{
  "name" : "1306.3108.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Guaranteed Classification via Regularized Similarity Learning",
    "authors" : [ "Zheng-Chu Guo" ],
    "emails" : [ "gzhengchu@gmail.com", "mathying@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n31 08\nv1 [\ncs .L"
    }, {
      "heading" : "1 Introduction",
      "text" : "The success of many machine learning algorithms heavily depends on how to specify the similarity or distance metric between examples. For instance, the k-nearest neighbor (k-NN) classifier depends on a distance (dissimilarity) function to identify the nearest neighbors for classification. Most information retrieval methods rely on a similarity function to identify the data points that are most similar to a given query. Kernel methods rely on the kernel function to represent the similarity between examples. Hence, how to learn an appropriate (dis)similarity function from the available\ndata is a central problem in machine learning, which we refer to as similarity metric learning throughout the paper.\nRecently, a considerable amount of research efforts are devoted to similarity metric learning and many approaches have been proposed. They can be briefly divided into three main approaches. The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x′ ∈ Rd, by dM (x, x′) = √\n(x− x′)TM(x− x′). Here, M is a positive semi-definite (PSD) matrix. The second approach focuses on learning a bilinear similarity function defined, for any x, x′ ∈ Rd, by sM(x, x′) = xTMx′ with M being a PSD matrix. This approach has been successfully applied to image searching [9] and object recognition [18]. The generalization bounds were recently established for metric and similarity learning [7, 14, 18] under different statistical assumptions on the data. The above methods are mainly motivated by the natural intuition that the similarity score between examples in the same class should be larger than that of examples from distinct classes. The k-NN classification using the similarity metric learnt from the above methods was empirically shown to achieve better accuracy than that using the standard Euclidean distance. However, there are no theoretical guarantees for such empirical success. In other words, it is not clear whether good generalization bounds for metric and similarity learning [14, 7] can lead to a good classification performance of the resultant k-NN classifiers.\nThe third approach for similarity metric learning is concerned with the problem of learning a general non-PSD similarity function (indefinite kernel). Kernel methods such as support vector machine (SVM) require the similarity function (kernel) to be PSD, which ensures the similarity function explicitly implies an embedding of the data into a high-dimensional Hilbert space and leads to an elegant convex optimization problem. However, many potential kernel matrices could be non-positive semi-definite. Such cases are quite common in applications such as hyperbolic tangent kernels [23], and the protein sequence similarity measures derived from SmithWaterman and BLAST score [22]. The problem of learning with an indefinite kernel has recently attracted considerable attention. Some methods [8, 30] learn a PSD kernel matrix from a prescribed indefinite kernel matrix, which are mostly restricted to the transductive settings. Recent methods [27, 28] analyzed regularization networks such as ridge regression and SVM given a prescribed indefinite kernel, instead of aiming to learn an indefinite kernel function from data.\nAlthough many approaches for similarity metric learning have been proposed, there is little theoretical study on the question whether similarity-based learning guarantees a good generalization of the resultant classification. Recently, Bellet et al. [3] proposed a regularized similarity learning approach, which is mainly motivated by the (ε, γ, τ)good similarity functions introduced in [1, 2]. In particular, they showed that the proposed similarity learning can theoretically guarantee a good generalization for classification. However, due to the techniques dependent on the notion of uniform stability [6], the generalization bounds only hold true for the Frobenius matrix-norm regularization which usually has a strong dependence on the dimensionality of the input space.\nIn this paper, we consider a new similarity learning formulation associated with general matrix-norm regularization terms. Its generalization bounds are established for various matrix regularization including the Frobenius norm, sparse L1-norm, and mixed (2, 1)-norm (see definitions below). The learnt similarity matrix is used to design a sparse classification algorithm and we prove the generalization error of its resultant linear separator can be bounded by the derived generalization bound for similarity learning. This implies that the proposed similarity learning with general matrix-norm regularization guarantees a good generalization for classification. Our techniques using the Rademacher complexity [5] and the important Khinchin-type inequality for the Rademacher variables, in the cases of sparse L1-norm, and mixed (2, 1)-norm regularization, enables us to derive bounds that have a mild dependence on the input dimensionality.\nThe remainder of this paper is organized as follows. In Section 2, we propose the similarity learning formulations with general matrix-norm regularization terms and state the main theorems. In particular, the results will be illustrated using various examples. The related work is discussed in Section 3. The generalization bounds for similarity learning are established in Section 4. In Section 5, we develop a theoretical link between the generalization bounds of the proposed similarity learning method and the generalization error of the linear classifier built from the learnt similarity function. Section 6 estimates the Rademacher averages and gives the proof for examples stated in Section 2. Section 7 summarizes this paper and discuss some possible directions for future research."
    }, {
      "heading" : "2 Regularization Formulation and Main Results",
      "text" : "In this section, we mainly introduce the regularized formulation of similarity learning and state our main results. Before we do that, let us introduce some notations and present some background material.\nDenote, for any n ∈ N, Nn = {1, 2, . . . , n}. Let z = {zi = (xi, yi) : i ∈ Nm} be a set of training samples, which is drawn identically and independently from a distribution ρ on Z = X × Y. Here, the input space X is a domain in Rd and Y = {−1, 1} is called the output space. For any x, x′ ∈ X , we consider KA(x, x′) = xTAx′ as a bilinear similarity score parameterized by a symmetric matrix A ∈ Sd×d. The symmetry of matrix A guarantees the symmetry of the similarity score KA, i.e. KA(x, x\n′) = KA(x ′, x).\nThe aim of similarity learning is to learn a matrix A from a given set of training samples z such that the similarity score KA between examples from the same label is larger than that between examples from different labels. A natural approach to achieve the above aim [1, 3] is to minimize the following empirical error\nEz(A) = 1\nm\n∑\ni∈Nm\n( 1− 1 mr ∑\nj∈Nm\nyiyjKA(xi, xj) )\n+ . (1)\nNote that ∑\nj∈Nm yiyjKA(xi, xj) =\n∑\n{j:yj=yi} KA(xi, xj)−\n∑\n{j:yj 6=yi} KA(xi, xj).Min-\nimizing the above empirical error encourages, for any i, that the average similarity scores between examples with the same class as yi are relatively larger than those between examples with distinct classes from yi. To avoid overfitting, we add a matrixregularized term to the above empirical error (1) and reach the following regularization formulation\nAz = arg min A∈Sd×d\n[ Ez(A) + λ‖A‖ ] , (2)\nwhere λ > 0 is the regularization parameter. Here, the notation ‖A‖ denotes a general matrix norm. For instance, it can be the sparse L1-norm ‖A‖1 = ∑\nk∈Nd\n∑\nℓ∈Nd |Akℓ|,\nthe (2, 1)-mixed norm ‖A‖(2,1) := ∑\nk∈Nd\n( ∑\nℓ∈Nd A2kℓ\n) 1\n2 , the Frobenius norm ‖A‖F = ( ∑\nk,ℓ∈Nd A2kℓ\n) 1\n2 or the trace norm ‖A‖tr := ∑ ℓ∈Nd σℓ(A), where {σℓ(A) : ℓ ∈ Nd}\ndenote the singular values of matrix A.\nThe first contribution of this paper is to establish generalization bounds for regularized similarity learning (1) with general matrix-norms. Specifically, define\nE(A) = ∫\nZ\n(\n1− 1 r\n∫\nZ yy′KA(x, x ′)dρ(x′, y′)\n)\n+\ndρ(x, y). (3)\nThe target of generalization analysis for similarity learning is to bound E(Az) − Ez(Az). Its special case with the Frobenius matrix norm was established in [3]. It used the uniform stability techniques [6], which, however, can not deal with non-strongly convex matrix-norms such as the L1-norm, (2, 1)-mixed norm and trace norm. Our new analysis techniques are able to deal with general matrix norms, which depend on the concept of Rademacher averages [5] defined as follows.\nDefinition 1. Let F be a class of uniformly bounded functions. For every integer n, we call\nRn(F ) := EzEσ\n[\nsup f∈F\n1\nn\n∑\ni∈Nn\nσif(zi)\n]\n,\nthe Rademacher average over F, where {zi : i ∈ Nn} are independent random variables distributed according to some probability measure and {σi : i ∈ Nn} are independent Rademacher random variables, that is, P (σi = 1) = P (σi = −1) = 12 .\nBefore stating our generalization bounds for similarity learning, we first introduce some notations. For any B,A ∈ Rn×d, let 〈B,A〉 = trace(BTA), where trace(·) denotes the trace of a matrix. For any matrix-norm ‖ · ‖, its dual norm ‖ · ‖∗ defined, for any B, by ‖B‖∗ = sup‖A‖≤1 trace(BTA). Denote ‖X‖∗ = supx,x′∈X ‖x′xT ‖∗. Let the Rademacher average with respect to the dual matrix norm be defined by\nRm := Ez,σ [\nsup x̃∈X\n∥ ∥ ∥ 1\nm\n∑\ni∈Nm\nσiyixix̃ T ∥ ∥ ∥\n∗\n]\n. (4)\nNow we can state the generalization bounds for similarity learning, which is closely related to the Rademacher averages with respect to the dual matrix-norm ‖ · ‖∗.\nTheorem 1. Let Az be the solution to algorithm (2). Then, for any 0 < δ < 1, with probability at least 1− δ, there holds\nE(Az)− Ez(Az) ≤ 6Rm rλ + 2X∗ rλ\n√\n2 ln ( 1 δ )\nm . (5)\nThe proof for Theorem 1 will be given in Section 4.\nThe second contribution of this paper is to investigate the theoretical relationship between similarity learning (2) and the generalization error of the linear classifier built from the learnt metric Az. We show that the generalization bound for the similarity learning gives an upper bound for the generalization error of a linear classifier produced by the linear Support Vector Machine (SVM) [24] defined as follows:\nfz = argmin { 1\nm\n∑\ni∈Nm\n( 1− yif(xi) ) + : f ∈ Fz, Ω(f) :=\n∑\nj∈Nm\n|αj | ≤ 1/r } . (6)\nwhere Fz = { f : f = ∑\nj∈Nm αjKAz(xj , ·), aj ∈ R\n}\nis the sample-dependent hypoth-\nesis space. The empirical error of f ∈ Fz associated with z is defined by\nEz(f) = 1\nm\n∑\ni∈Nm\n( 1− yif(xi) )\n+ .\nThe true generalization error is defined as\nE (f) =\n∫\nZ\n( 1− yf(x) )\n+ dρ(x, y).\nNow we are in a position to state the relationship between the generalization error of similarity learning and the generalization error of the liner classifier.\nTheorem 2. Let Az and fz be defined by (2) and (6), respectively. Then, for any 0 < δ < 1, with confidence at least 1− δ, there holds\nE (fz) ≤ Ez(Az) + 4Rm λr + 2X∗ λr\n√\n2 log 1δ m . (7)\nThe proof for Theorem 2 will be established in Section 5.\nTheorems 1 and 2 depend critically on two terms: the constantX∗ and the Rademacher average Rm. Below, we list the estimation of these two terms associated with different matrix norms. For any vector x = (x1, x2, . . . , xd) ∈ Rd, denote ‖x‖∞ = maxℓ∈Nd |xℓ|.\nExample 1. Consider the matrix norm be the sparse L1-norm defined, for any A ∈ S d×d, by ‖A‖ = ∑k,ℓ∈Nd |Akℓ|. Let Az and fz be defined respectively by (2) and (6). Then, we have the following results.\n(a) X∗ ≤ supx∈X ‖x‖2∞ and Rm ≤ 2 supx∈X ‖x‖2∞ √ e log(d+1) m .\n(b) For any 0 < δ < 1, with confidence at least 1− δ, there holds\nE(Az)−Ez(Az) ≤ 12 supx∈X ‖x‖2∞\nrλ\n√\ne log(d+ 1) m + 2 supx∈X ‖x‖2∞ rλ\n√\n2 ln ( 1 δ )\nm . (8)\n(c) For any 0 < δ < 1, with confidence at least 1− δ, there holds\nE (fz) ≤ Ez(Az)+ 4 supx∈X ‖x‖2∞\nλr\n√\n2e log(d+ 1)\nm + 2 supx∈X ‖x‖2∞ λr\n√\n2 log 1δ m . (9)\nFor any vector x ∈ Rd, let ‖x‖F be the standard Euclidean norm. Considering the regularized similarity learning with the Frobenius matrix norm, we have the following result.\nExample 2. Consider the Frobenius matrix norm defined, for any A ∈ Sd×d, by ‖A‖ = √ ∑\nk,ℓ∈Nd |Akℓ|2. Let Az and fz be defined by (2) and (6), respectively. Then,\nwe have the following estimation.\n(a) X∗ ≤ supx∈X ‖x‖2F and Rm ≤ 2 supx∈X ‖x‖2F √ 1 m .\n(b) For any 0 < δ < 1, with confidence at least 1− δ, there holds\nE(Az)− Ez(Az) ≤ 6 supx∈X ‖x‖2F\nrλ √ m\n+ 2 supx∈X ‖x‖2F\nrλ\n√\n2 ln ( 1 δ )\nm . (10)\n(c) For any 0 < δ < 1, with confidence at least 1− δ, there holds\nE (fz) ≤ Ez(Az) + 4 supx∈X ‖x‖2F\nλr √ m\n+ 2 supx∈X ‖x‖2F\nλr\n√\n2 log 1δ m . (11)\nWe end this section with two remarks. Firstly, the above theorem and examples means that a good similarity (i.e. a small generalization error Ez(Az) for similarity learning) can guarantee a good classification (i.e. a small classification error E (fz)). Secondly, the bounds in Example 2 is consistent with that in [3]. Comparing the results in Examples 1 and 2, the generalization bound of similarity learning using the L1-norm has a milder dependence on the input dimension d than using the Frobenius norm. For instance, if the input space X = [0, 1]d, then we have supx∈X ‖x‖∞ = 1 and supx∈X ‖x‖F = √ d. Hence, we can observe from inequalities (9) and (11) that the generalization bound with L1-norm only relies on log(1 + d), while the bound for the Frobenius norm heavily depends on d."
    }, {
      "heading" : "3 Related Work",
      "text" : "In this section, we discuss studies on similarity metric learning which are related to our work.\nMany similarity metric learning methods have been motivated by the intuition that the similarity score between examples in the same class should be larger than that of examples from distinct classes, see e.g. [4, 7, 9, 12, 14, 18, 26, 29]. Jin et al. [14] established generalization bounds for regularized metric learning algorithms via the concept of uniform stability [5], which, however, only works for strongly convex matrix regularization terms. A very recent work [7] established generalization bounds for the metric and similarity learning associated with general matrix norm regularization using techniques of Rademacher averages and U-statistics. However, there was no theoretical links between the similarity metric learning and the generalization performance of classifiers based on the learnt similarity matrix. Here, we focused on the problem how to learn a good linear similarity function KA such that it can guarantee a good classification error of the resultant classifier derived from the learnt similarity function. In addition, our formulation (2) is quite distinct from the available similarity metric learning methods, since most of them are based on pairwise or triplet-wise constraints. For instance, the following pairwise empirical objective function was considered in [9, 7]:\n1\nm(m− 1)\nm ∑\ni,j=1,i 6=j\n( 1− yiyj(KA(xi, xj)− r) )\n+ . (12)\nOur formulation (2) is less restrictive since the empirical objective function is defined over an average of similarity scores and it doesn’t require the positive semi-definiteness of the similarity function K.\nBalcan et al. [2] developed a theory of (ǫ, γ, τ)-good similarity function defined as follows. It attempts to investigate the theoretical relationship between the properties of a similarity function and its performance in linear classification.\nDefinition 2. A similarity function K is a (ǫ, γ, τ)-good similarity function in hinge loss for a learning problem P if there exists a random indicator function R(x) defining a probabilistic set of “reasonable points” such that the following conditions hold: 1. E(x,y)∼P [1− yg(x)/γ]+ ≤ ǫ, where g(x) = E(x′,y′)∼P [y′K(x, x′)|R(x′)], 2. Prx′ [R(x ′)] ≥ τ.\nThe first condition can be interpreted as “most points x are on average 2γ more similar to random reasonable points of the same class than to random reasonable points of the distinct classes” and the second condition as “at least a τ proportion of the points should be reasonable.” The following theorem implies that if given an (ǫ, γ, τ)-good similarity function and enough landmarks, there exists a separator α with error arbitrarily close to ǫ.\nTheorem 3. Let K be an (ǫ, γ, τ)-good similarity function in hinge loss for a learning problem P. For any ǫ1 > 0, and 0 < δ ≤ γǫ1/4, let S = {x′1, · · · , x′dland} be a poten-\ntially unlabeled sample of dland = 2 τ\n( log(2/δ) + 16 log(2/δ) (ǫ1γ)2 ) landmarks drawn from P.\nConsider the mapping φSi = K(x, x ′ i), i ∈ {1, · · · , dland}. Then, with probability at least 1− δ over the random sample S, the induced distribution φS(P ) in Rdland has a linear separator α of error at most ǫ+ ǫ1 at margin γ.\nIt was mentioned in [2] that the linear separator can be estimated by solving the following linear programming if we have du potentially unlabeled sample and dl labeled sample,\nmin α\n{ dl ∑\ni=1\n[ 1− du ∑\nj=1\nαjyiK(xi, x ′ j) ]\n+ :\ndu ∑\nj=1\n|αj | ≤ 1/γ } . (13)\nThe above algorithm (13) is quite similar to the linear SVM (6) used in our paper. Our work is distinct from Balcan et al. [2] in the following two aspects. Firstly, the similarity function K is predefined in algorithm (13), while we aim to learn a similarity function KAz from a regularized similarity learning formulation (2). Secondly, although the separators are both trained from the linear SVM, the classification algorithm (13) in [2] was designed using two different sets of examples, a set of labeled samples of size dl to train the classification algorithm and another set of unlabeled samples with size du to define the mapping φ\nS . In this paper, we used the same set of training samples for both similarity learning (2) and the classification algorithm (6), which could be more practically feasible.\nRecent work by Bellet et al. [3] is mostly close to ours. Specifically, they considered similarity learning formulation (2) with the Frobenius norm regularization. Generalization bounds for similarity learning were derived via uniform stability arguments [6] which can not deal with, for instance, the L1-norm and (2, 1)-norm regularization terms. In addition, the results about the relationship between the similarity learning and the performance of the learnt matrix in classification were quoted from [2] and hence requires two separate sets of samples to train the classifier.\nJain et al. [13] and Kar et al. [15] introduced an extended framework of [1, 2] in the general setting of supervised learning. The authors proposed a general goodness criterion for similarity functions, which can handle general supervised learning tasks and also subsumes the goodness of condition of [2]. There, efficient algorithms were constructed with provable generalization error bounds. The main distinction between these work and our work is that we aim to learn a similarity function while in their work a similarity function is defined in advance."
    }, {
      "heading" : "4 Generalization Bounds for Similarity Learning",
      "text" : "In this section, we establish generalization bounds for the similarity learning formulation (2) with general matrix-norm regularization terms. Recall that the true error for similarity learning is defined by\nE(A) = ∫\nZ\n( 1− 1 r\n∫\nZ yy′KA(x, x\n′)dρ(x′, y′) )\n+ dρ(x, y).\nThe target of generalization analysis for similarity learning is to bound the true error E(Az) by the empirical error Ez(Az).\nBy the definition (2) of Az, we know that Ez(Az) + λ‖Az‖ ≤ Ez(0) + λ‖0‖ = 1, which implies that ‖Az‖ ≤ 1/λ. Denote\nA = { A ∈ Sd×d : ‖A‖ ≤ 1/λ } .\nHence, one can easily see that the solution Az to algorithm (2) belongs to A. Now we are ready to prove generalization bounds for similarity learning which was stated as Theorem 1 in Section 2.\nProof of Theorem 1: Our proof is divided into two steps.\nStep 1: Let Ez denote the expectation with respect to samples z. Observe that E(Az) − Ez(Az) ≤ sup A∈A [ E(A) − Ez(A) ] . Also, for any z = (z1, . . . , zk, . . . , zm) and z̃ = (z1, . . . , z̃k, . . . , zm), 1 ≤ k ≤ m, there holds ∣\n∣ ∣ sup A∈A\n[ E(A) − Ez(A) ]\n− sup A∈A\n[ E(A)− Ez̃(A) ] ∣ ∣ ∣ ≤ sup\nA∈A |Ez(A)− Ez̃(A)|\n≤ 1 m2r\nsup A∈A\n{ m ∑\ni=1,i 6=k\n|yiykKA(xk, xi)− yiỹkKA(x̃k, xi)|\n+ |∑j∈Nm(ykyjKA(xk, xj)− ỹkyjKA(x̃k, xj))| }\n≤ 2 m2r\nsup A∈A\n∑\ni∈Nm\n( |yiykKA(xk, xi)|+ |yiỹkKA(x̃k, xi)| ) ≤ 4X∗ mrλ .\nApplying the McDiarmid’s inequality [19] (see Lemma 1 in the Appendix) to the term\nsup A∈A\n[ E(A)− Ez(A) ] , with probability at least 1− δ, there holds\nsup A∈A\n[ E(A)− Ez(A) ]\n≤ Ez sup A∈A\n[ E(A)− Ez(A) ] + 2X∗ rλ\n√\n2 ln ( 1 δ )\nm . (14)\nNow we are in a position to estimate the first term in the expectation form on the righthand side of the above equation by standard symmetrization techniques.\nStep 2: We divide the term Ez sup A∈A\n[ E(A)− EzA ] into two parts as follows,\nEz sup A∈A\n[ E(A)− Ez(A) ]\n= Ez sup A∈A\n{ E(A)− 1 m ∑\ni∈Nm\n[ 1− 1 mr ∑\nj∈Nm\nyiyjKA(xi, xj) ]\n+\n}\n= Ez sup A∈A\n{ E(A)− 1 m ∑\ni∈Nm\n[ 1− 1 r E(x′,y′)yiy ′KA(xi, x ′) ] +\n+ 1m\n∑\ni∈Nm\n[ 1− 1 r E(x′,y′)yiy ′KA(xi, x ′) ] + − 1 m ∑\ni∈Nm\n[ 1− 1 mr ∑\nj∈Nm\nyiyjKA(xi, xj) ]\n+\n}\n≤ I1 + I2,\nwhere\nI1 := Ez sup A∈A\n{ E(A)− 1 m ∑\ni∈Nm\n[ 1− 1 r E(x′,y′)yiy ′KA(xi, x ′) ] + } ,\nand\nI2 := Ez sup A∈A\n{ 1\nm\n∑\ni∈Nm\n[ 1−1 r E(x′,y′)yiy ′KA(xi, x ′) ] + − 1 m ∑\ni∈Nm\n( 1− 1 mr ∑\nj∈Nm\nyiyjKA(xi, xj) )\n+\n}\n.\nNow let z̄ = {z̄1, z̄2, . . . , z̄m} be an i.i.d. sample which is independent of z. We first estimate I1 using the standard symmetrization techniques, to this end, we rewrite E(A) as Ez̄ ( 1 m ∑\ni∈Nm\n[ 1− 1 r E(x′,y′)ȳiy ′KA(x̄i, x ′) ] + ) . Then we have\nI1 = Ez sup A∈A\n{\nEz̄\n( 1\nm\n∑\ni∈Nm\n[ 1− 1 r E(x′,y′)ȳiy ′KA(x̄i, x ′) ] + )\n− 1m ∑\ni∈Nm\n[ 1− 1 r E(x′,y′)yiy ′KA(xi, x ′) ] + }\n≤ Ez,z̄ sup A∈A\n{ 1\nm\n∑\ni∈Nm\n[ 1− 1 r E(x′,y′)ȳiy ′KA(x̄i, x ′) ] +\n− 1m ∑\ni∈Nm\n[ 1− 1 r E(x′,y′)yiy ′KA(xi, x ′) ] + } .\nBy the standard Rademacher symmetrization technique and the contraction property of the Rademacher average (see Lemma 2 in the Appendix), we further have\nI1 ≤ 2Ez,σ sup A∈A\n{ 1\nm\n∑\ni∈Nm\nσi\n[ 1− 1 r E(x′,y′)yiy ′KA(xi, x ′) ] + }\n≤ 4Ez,σ sup A∈A\n∣ ∣ ∣ 〈 1 mr ∑\ni∈Nm\nσiyixi\n∫\ny′x′Tdρ(x′, y′), A〉 ∣ ∣\n∣\n≤ 4rλEz,σ ∥ ∥ ∥ 1 m ∑\ni∈Nm\nσiyixi\n∫\ny′x′Tdρ(x′, y′) ∥ ∥\n∥ ∗ ≤ 4 rλ Ez,σ sup\nx̃\n∥ ∥ ∥ 1\nm\n∑\ni∈Nm\nσiyixix̃ T ∥ ∥ ∥\n∗ ,\nwhere the last inequality follows from the fact that 〈A,B〉 ≤ ‖A‖‖B‖∗ ≤ 1r‖B‖∗ for any A ∈ A and B ∈ Rd×d.\nSimilarly, we can estimate I2 as follows.\nI2 = Ez sup A∈A\n1\nm\n∑\ni∈Nm\n([ 1− 1 r E(x′,y′)yiy ′KA(xi, x ′) ] + − [ 1− 1 mr ∑\nj∈Nm\nyiyjKA(xi, xj) ]\n+\n)\n≤ Ez sup A∈A\n1\nm\n∑\ni∈Nm\n(∣\n∣ ∣\n1 r E(x′,y′)yiy ′KA(xi, x ′)− 1 mr ∑\nj∈Nm\nyiyjKA(xi, xj) ∣ ∣ ∣ )\n= Ez sup A∈A\n1\nmr\n∑\ni∈Nm\n(∣\n∣ ∣ 〈E(x′,y′)yiy′x′xTi −\n1\nm\n∑\nj∈Nm\nyiyjxjx T i , A〉\n∣ ∣ ∣ )\n≤ 1rλEz sup x∈X\n∥ ∥ ∥ E(x′,y′)y ′x′xT − 1 m ∑\nj∈Nm\nyjxjx T ∥ ∥ ∥\n∗\n= 1rλEz sup x∈X\n∥ ∥ ∥ Ez′ 1\nm\n∑\nj∈Nm\ny′jx ′ jx T − 1 m ∑\nj∈Nm\nyjxjx T ∥ ∥ ∥\n∗ .\nIn the above estimation, the first inequality follows from the Lipschitz continuity of the hinge loss function. Following the standard Rademacher symmetrization technique (see e.g. [5]), from the above estimation we can further estimate I2 as follows:\nI2 ≤ 1rλEz sup x∈X\n∥ ∥ ∥ Ez′ 1\nm\n∑\nj∈Nm\ny′jx ′ jx T − 1 m ∑\nj∈Nm\nyjxjx T ∥ ∥ ∥\n∗\n≤ 1rλEz,z′ sup x∈X\n∥ ∥ ∥ 1\nm\n∑\nj∈Nm\ny′jx ′ jx T − 1 m ∑\nj∈Nm\nyjxjx T ∥ ∥ ∥\n∗\n≤ 1rλEz,z′,σ sup x∈X\n∥ ∥ ∥ 1\nm\n∑\nj∈Nm\nσj\n(\ny′jx ′ jx T − yjxjxT )∥ ∥ ∥ ∗ ≤ 2 rλ Ez,σ sup\nx∈X\n∥ ∥ ∥ 1\nm\n∑\nj∈Nm\nσjyjxjx T ∥ ∥ ∥\n∗ .\nThe desired result follows by combining (14) with the above estimation for I1 and I2. This completes the proof for the theorem."
    }, {
      "heading" : "5 Guaranteed Classification Via Good Similarity",
      "text" : "In this section, we investigate the theoretical relationship between the generalization error for the similarity learning and that of the linear classifier built from the learnt similarity metric KAz . In particular, we will show that the generalization error of the similarity learning gives an upper bound for the generalization error of the linear classifier which was stated as Theorem 2 in Section 2.\nBefore giving the proof of Theorem 2, we first establish the generalization bounds for the linear SVM algorithm (6). Recalling that the linear SVM algorithm (6) was defined by\nfz = argmin { 1\nm\n∑\ni∈Nm\n( 1− yif(xi) ) + : f ∈ Fz, Ω(f) :=\n∑\nj∈Nm\n|αj | ≤ 1/r } ,\nwhere Fz = { f : f = ∑\nj∈Nm\nαjKAz(xj , ·), aj ∈ R } .\nThe generalization analysis of the linear SVM algorithm (6) aims to estimate the term E (fz)− Ez(fz). For any z, one can easily see that the solution to algorithm (6) belongs to the set Fz,r, where\nFz,r = { f = ∑\nj∈Nm\nαjKAz(xj , ·) : Ω(f) = ∑\nj∈Nm\n|αj | ≤ 1/r, aj ∈ R } .\nTo perform the generalization analysis, we seek a sample-independent set which contains, for any z, the sample-dependent hypothesis space Fz. Specifically, we define a sample independent hypothesis space by\nFm = { f = ∑\ni∈Nm\nαiKA(ui, ·) : ‖A‖ ≤ 1/λ, uj ∈ X, aj ∈ R } .\nRecalling that, for any z, ‖Az‖ ≤ λ−1, one can easily see that Fz is a subset of Fm. It follows that, for any z, the solution to the linear SVM algorithm (6) lies in the set Fm,r, which is given by\nFm,r = { f ∈ Fm : Ω(f) ≤ 1/r } .\nThe following theorem states the generalization bounds of the linear SVM for classification.\nTheorem 4. Let fz be the solution to the algorithm (6). For any 0 < δ < 1, with probability at least 1− δ, we have\nE (fz)− Ez(fz) ≤ 4\nλr Ez,σ sup\nx∈X\n∥ ∥ ∥ 1\nm\n∑\ni∈Nm\nσiyixix T ∥ ∥ ∥\n∗ + 2X∗ λr\n√\n2 log 1δ m . (15)\nProof. By McDiarmid’s inequality, for any 0 < δ < 1, with confidence 1 − δ, there holds\nE (fz)− Ez(fz) ≤ sup f∈Fz,r\n( E (f)− Ez(f) ) ≤ sup f∈Fm,r ( E (f)− Ez(f) )\n≤ Ez sup f∈Fm,r\n( E (f)− Ez(f) ) + 2X∗ λr\n√\n2 log 1δ m .\nNext, all we need is to estimate the first part of the right hand-side of the above inequality. Let z̄ be an independent sample (independent each other and z) and with the same distribution as z.\nEz sup f∈Fm,r\n( E (f)− Ez(f) )\n= Ez sup f∈Fm,r\n( Ez̄Ez̄(f)− Ez(f) ) ≤ Ez,z̄ sup f∈Fm,r ( Ez̄(f)− Ez(f) )\n≤ 2Ez,σ [\nsup ‖A‖≤1/λ sup∑ i∈Nm |αi|≤1/r\n( 1\nm\n∑\ni∈Nm\nσi [ 1− ∑\nj∈Nm\nαjyiKA(xi, uj)]+\n)]\n≤ 4Ez,σ [\nsup ‖A‖≤1/λ sup∑ i∈Nm |αi|≤1/r\n( 1\nm\n∑\ni∈Nm\nσi ∑\nj∈Nm\nαjyiKA(xi, uj) )]\n≤ 4rEz,σ sup A:‖A‖≤1/λ sup x∈X\n( ∣\n∣ ∣\n1\nm\n∑\ni∈Nm\nσiyi〈xixT , A〉 ∣ ∣ ∣ ) ≤ 4 λr Ez,σ sup x∈X ∥ ∥ ∥ 1 m ∑\ni∈Nm\nσiyixix T ∥ ∥ ∥\n∗ .\nHere we also use the standard Rademacher Symmetrization technique and the contractor property of the Rademacher average. Then the proof is completed.\nNow we are in a position to give the detailed proof of Theorem 2.\nProof of Theorem 2: If we take α0 = ( y1mr , · · · , ym mr ) T , then f0 z = 1mr ∑ j∈Nm yjKAz(xj, ·). One can easily see that Ω(f0 z ) = ∑ j∈Nm |α0j | = 1r , that means f0z ∈ Fz,r. From Theo-\nrem 4 and the definition of fz , we get\nE (fz) ≤ Ez(fz) + 4λrEz,σ sup x∈X\n∥ ∥ ∥ 1\nm\n∑\ni∈Nm\nσiyixix T ∥ ∥ ∥\n∗ + 2X∗ λr\n√\n2 log 1δ m\n≤ Ez(f0z ) + 4λrEz,σ sup x∈X\n∥ ∥ ∥ 1\nm\n∑\ni∈Nm\nσiyixix T ∥ ∥ ∥\n∗ + 2X∗ λr\n√\n2 log 1δ m\n= 1m\n∑\ni∈Nm\n( 1− 1 mr ∑\nj∈Nm\nyiyjKAz(xi, xj) )\n+ +\n4\nλr Ez,σ sup\nx∈X\n∥ ∥ ∥ 1\nm\n∑\ni∈Nm\nσiyixix T ∥ ∥ ∥\n∗\n+2X∗λr\n√\n2 log 1 δ\nm\n= Ez(Az) + 4λrEz,σ sup x∈X\n∥ ∥ ∥ 1\nm\n∑\ni∈Nm\nσiyixix T ∥ ∥ ∥\n∗ + 2X∗ λr\n√\n2 log 1δ m .\nThis completes the proof of the theorem."
    }, {
      "heading" : "6 Estimating Rademacher Averages",
      "text" : "The main theorems above critically depend on the estimation of the Rademacher average Rm. In this section, we establish its estimation and prove the examples listed in Section 2. For notational simplicity, denote by xℓi the ℓ-th variable of the i-th sample xi ∈ Rd.\nProof of Example 1: The dual norm of L1-norm is the L∞-norm. Hence,\nX∗ = sup x,x′∈X sup ℓ,k∈Nd |xℓ(x′)k| = sup x∈X ‖x‖2∞. (16)\nAlso, the Rademacher average can be rewritten as\nRm = Ez,σ sup x∈X ‖ 1 m ∑\nj∈Nm\nσjyjxjx T ‖∞ ≤ sup x∈X ‖x‖∞ Ez,σ max ℓ∈Nd\n∣ ∣ ∣ 1\nm\n∑\nj∈Nm\nσjyjx ℓ j\n∣ ∣ ∣ . (17)\nNow let Uℓ(σ) = 1 m\n∑\nj∈Nm\nσjyjx ℓ j, for any ℓ ∈ Nd. By Jensen’s inequality, for any η > 0,\nwe have eη 2(Eσ maxℓ∈Nd |Uℓ(σ)|) 2 − 1 ≤ Eσ[eη 2(maxℓ∈Nd |Uℓ(σ)|) 2 − 1]\n= Eσ[max ℓ∈Nd\neη 2|Uℓ(σ)| 2 − 1] ≤ ∑\nℓ∈Nd\nEσ[e η2(|Uℓ(σ)|) 2 − 1]. (18)\nFurthermore, for any ℓ ∈ Nd, there holds\nEσ[e η2(|Uℓ(σ)|) 2 − 1] = ∑\nk≥1\n1 k! η2kEσ|Uℓ|2k\n≤ ∑\nk≥1\n1 k! η2k(2k − 1)k(Eσ|Uℓ|2)k ≤ ∑\nk≥1\n(2eη2Eσ|Uℓ|2)k,\nwhere the first inequality follows from the Khinchin-type inequality (see Lemma 3 in the Appendix), and the second inequality holds due to the Stirling’s inequality:e−kkk ≤ k!. Now set η = [2\n√ emax ℓ∈Nd (Eσ|Uℓ|2) 1 2 ]−1. The above inequality can be upper bounded\nby\nE[eη 2(|Uℓ(σ)|) 2 − 1] ≤ ∑\nk≥1\n2−k = 1, ∀ℓ ∈ Nd.\nPutting the above estimation back into (18) implies that\neη 2(Emaxℓ∈Nd |Uℓ(σ)|) 2 − 1 ≤ d. That means\nEσ max ℓ∈Nd |Uℓ(σ)| = Eσ max ℓ∈Nd\n∣ ∣\n1\nm\n∑\nj∈Nm\nσjyjx ℓ j\n∣ ∣ ≤ √ log(d+ 1)η−2\n= 2 √\ne log(d+ 1)max ℓ∈Nd\n(Eσ|Uℓ|2) 1 2\n= 2 √\ne log(d+ 1)max ℓ∈Nd\n(\nEσ\n∣ ∣ ∣ 1\nm\nn ∑\nj∈Nm\nσjyjx ℓ j\n∣ ∣ ∣\n2) 1 2\n= 2 √\ne log(d+ 1)max ℓ∈Nd\n( Eσ 1\nm2\n∑\nj,k∈Nm\nσjσkyjykx ℓ jx ℓ k\n) 1\n2\n= 2 √\ne log(d+ 1)max ℓ∈Nd\n( 1\nm2\n∑\nj∈Nm\n(xℓj) 2 )\n1 2 ≤ 2 sup x∈X ‖x‖∞ √ e log(d+ 1) m .\n(19)\nPutting the above estimation back into (17) implies that\nRm ≤ 2 sup x∈X ‖x‖2∞\n√\ne log(d+ 1)\nm .\nThe other desired results in the example follow directly from combining the above estimation with Theorems 1 and 2.\nWe turn our attention to similarity learning formulation (2) with the Frobenius norm regularization.\nProof of Example 2: The dual norm of the Frobenius norm is itself. Consequently, X∗ = sup\nx,x′∈X ‖x′xT ‖F = sup x∈X ‖x‖2F . The Rademacher average can be rewritten as\nRn = Ez,σ sup x∈X\n∥ ∥ ∥ 1\nm\n∑\nj∈Nm\nσjyjxjx T ∥ ∥ ∥\nF .\nBy Cauchy’s Inequality, there holds\nRn ≤ Ez,σ sup x∈X\n‖x‖F ∥ ∥ ∥ 1\nm\n∑\nj∈Nm\nσjyjxj\n∥ ∥ ∥\nF = Ez,σ sup x∈X ‖x‖F\n( d ∑\nk=1\n( 1\nm\n∑\nj∈Nm\nσjyjx k j\n)2) 1 2\n≤ Ez sup x∈X\n‖x‖F (\nd ∑\nk=1\nEσ\n( 1\nm\n∑\nj∈Nm\nσjyjx k j\n)2) 1 2\n= Ez sup x∈X\n‖x‖F (\nd ∑\nk=1\n1\nm2\n∑\nj∈Nm\n(xkj ) 2 )\n1 2\n= Ez sup x∈X\n‖x‖F ( 1\nm2\n∑\nj∈Nm\n‖xj‖2F )\n1 2 ≤ sup x∈X ‖x‖2F 1√ m .\n.\nThen, the desired results can be derived by combining the above estimation with Theorems 1 and 2.\nThe above generalization bound for similarity learning formulation (2) with the Frobenius norm regularization is consistent with that given in [3], where the result holds true under the assumption that supx∈X ‖x‖F ≤ 1. Below, we provide the estimation of Rm respectively for the mixed (2, 1)-norm and the trace norm.\nExample 3. Consider similarity learning formulation (2) with the mixed (2, 1)-norm regularization ‖A‖(2,1) = ∑ k∈Nd ( ∑ ℓ∈Nd |Akℓ|2)1/2. Then, we have the following estimation.\n(a) X∗ ≤ [ supx∈X ‖x‖F ][ supx∈X ‖x‖∞ ] and\nRm ≤ 2 [\nsup x∈X\n‖x‖F ][\nsup x∈X\n‖x‖∞ ]\n√\ne log(d+ 1)\nm .\n(b) For any 0 < δ < 1, with confidence at least 1− δ, there holds\nE(Az)− Ez(Az) ≤ 12 [ supx∈X ‖x‖F ][ supx∈X ‖x‖∞ ]\nrλ\n√\ne log(d+1) m\n+ 2 [ supx∈X ‖x‖F ][ supx∈X ‖x‖∞ ]\nrλ\n√\n2 ln ( 1\nδ\n)\nm .\n(20)\n(c) For any 0 < δ < 1, with probability at least 1− δ there holds\nE (fz) ≤ Ez(Az) + 4 [ supx∈X ‖x‖F ][ supx∈X ‖x‖∞ ]\nλr\n√\n2e log(d+1) m\n+ 2 [ supx∈X ‖x‖F ][ supx∈X ‖x‖∞ ]\nλr\n√\n2 log 1 δ\nm .\nProof. The dual norm of the (2, 1)-norm is the (2,∞)-norm, which implies that X∗ = sup\nx,x′∈X ‖x′xT ‖(2,∞) = sup x∈X ‖x‖F sup x′∈X ‖x′‖∞ and\nEz,σ sup x∈X\n∥ ∥ ∥ 1\nm\n∑\nj∈Nm\nσjyjxjx T ∥ ∥ ∥\n∗ ≤ sup x∈X ‖x‖FEz,σ max ℓ∈Nd\n∣ ∣ ∣ 1\nm\n∑\nj∈Nm\nσjyjx ℓ j\n∣ ∣ ∣\n≤ 2 sup x∈X ‖x‖F sup x\n‖x‖∞ √ e log(d+ 1)\nm ,\nwhere the last inequality follows from estimation (19). We complete the proof by combining the above estimation with Theorems 1 and 2.\nWe briefly discuss the case of the trace norm regularization, i.e., ‖A‖ = ‖A‖tr. In this case, the dual norm of trace norm is the spectral norm defined, for any B ∈ Sd×d, by ‖B‖∗ = maxℓ∈Nd σℓ(B) where {σℓ : ℓ ∈ Nd} are the singular values of matrix B.\nObserve, for any u, v ∈ Rd, that ‖uvT ‖∗ = ‖u‖F ‖v‖F . Hence, the constant X∗ = sup\nx,x′∈X ‖x′xT ‖∗ = sup x∈X ‖x‖2F . In addition,\nRm = Ez,σ sup x∈X\n∥ ∥ ∥ 1\nm\n∑\nj∈Nm\nσjyjxjx T ∥ ∥ ∥\n∗ = Ez,σ sup x∈X ‖x‖F\n∥ ∥ ∥ 1\nm\n∑\nj∈Nm\nσjyjxj\n∥ ∥ ∥\nF ≤ sup x∈X ‖x‖2F 1√ m .\nThese estimations above mean that the estimation for X∗ and Rm are the same as those for the Frobenius norm regularization. Hence the generalization bounds for similarity learning and the relationship between similarity learning and the linear SVM are the same as those stated in Example 2. It is a bit disappointing that there is no improvement when using the trace norm. The possible reason is that the spectral norm of B and the Frobenius norm of B are the same when B takes the form B = xyT for any x, y ∈ Rd."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we considered a regularized similarity learning formulation (2). Its generalization bounds were established for various matrix-norm regularization terms such as the Frobenius norm, sparse L1-norm, and mixed (2, 1)-norm. We proved the generalization error of the linear separator based on the learnt similarity function can be bounded by the derived generalization bound of similarity learning. This guarantees the goodness of the generalization of similarity learning (2) with general matrix-norm regularization and thus the classification generalization of the resulting linear classifier. Our techniques using the Rademacher complexity [5] and the important Khinchin-type inequality for the Rademacher variables allows us to obtain new bounds for similarity learning that have a mild dependence on the input dimensionality.\nThere are several possible directions for future work. Firstly, we may consider similarity algorithms with general loss functions. It is expected that under some convexity conditions on the loss functions, better results could be obtained. Secondly, we usually focus on the excess misclassification error when considering classification problems. Hence, in the future, we would like to consider the theoretical link between the generalization bounds of the similarity learning and the excess misclassification error of the classifier built from the learnt similarity function.\nAcknowledgement:\nThis work is supported by the EPSRC under grant EP/J001384/1. The corresponding author is Yiming Ying."
    } ],
    "references" : [ {
      "title" : "A theory of learning with similarity functions",
      "author" : [ "M.-F. Balcan", "A. Blum" ],
      "venue" : "COLT,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Improved gaurantees for learning via similarity functions",
      "author" : [ "M.-F. Balcan", "A. Blum", "N. Srebro" ],
      "venue" : "COLT,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Similarity learning for provably accurate sparse linear classification",
      "author" : [ "A. Bellet", "A. Habrard", "M. Sebban" ],
      "venue" : "ICML,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Rademacher and Gaussian complexities: risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "J. of Machine Learning Research, 3: 463–482,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Stability and generalization",
      "author" : [ "O. Bousequet", "A. Elisseeff" ],
      "venue" : "J. of Machine Learning Research, 2: 499–526,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Generalization bounds for metric and similarity learning",
      "author" : [ "Q. Cao", "Z.-C. Guo", "Y. Ying" ],
      "venue" : "Preprint,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Similaritybased classification: concepts and algorithms",
      "author" : [ "Y. Chen", "E.K. Garcia", "M.R. Gupta", "A. Rahimi", "L. Cazzanti" ],
      "venue" : "J. of Machine Learning Research, 10:747-776,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Large scale online learning of image similarity through ranking",
      "author" : [ "G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio" ],
      "venue" : "J. of Machine Learning Research, 11: 1109-1135,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Is that you? Metric learning approaches for face identification",
      "author" : [ "M. Guillaumin", "J. Verbeek", "C. Schmid" ],
      "venue" : "ICCV",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Information-theoretic metric learning",
      "author" : [ "J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon" ],
      "venue" : "ICML,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning distance metrics with contextual constraints for image retrieval",
      "author" : [ "S.C.H. Hoi", "W. Liu", "M.R. Lyu", "W.-Y. Ma" ],
      "venue" : "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 2072–2078,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Supervised learning with similarity functions",
      "author" : [ "P. Jain", "P. Kar" ],
      "venue" : "NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Regularized distance metric learning: theory and algorithm",
      "author" : [ "R. Jin", "S. Wang", "Y. Zhou" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Similarity-based learning via data-driven embeddings",
      "author" : [ "P. Kar", "P. Jain" ],
      "venue" : "NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Empirical margin distributions and bounding the generalization error of combined classifiers",
      "author" : [ "V. Koltchinskii", "V. Panchenko" ],
      "venue" : "The Annals of Statistics, 30, 1–5,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Probability in Banach Spaces: Isoperimetry and Processes",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : "Springer Press, New York,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Learning similarity with operator-valued large-margin classifiers",
      "author" : [ "A. Maurer" ],
      "venue" : "J. of Machine Learning Research, 9: 1049–1082,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Surveys in Combinatorics, Chapter On the methods of bounded differences, 148–188",
      "author" : [ "C. McDiarmid" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1989
    }, {
      "title" : "Learning with non-positive kernels",
      "author" : [ "C.S. Ong", "X. Mary", "S. Canu", "A.J. Smola" ],
      "venue" : "ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Decoupling: from Dependence to Independence",
      "author" : [ "V.H. De La Peña", "E. Giné" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1999
    }, {
      "title" : "Protein homology detection using string alignment kernels",
      "author" : [ "H. Saigo", "J.P. Vert", "N. Ueda", "T. Akutsu" ],
      "venue" : "Bioinformatics, 20: 1682–1689.,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Regularization with dot-product kernels",
      "author" : [ "A.J. Smola", "Z.L. Óv́ari", "R.C. Williamson" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2000
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "Wiley, New York,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "On learning with dissimilarity functions",
      "author" : [ "L. Wang", "C. Yang", "J. Feng" ],
      "venue" : "ICML,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Distance metric learning for large margin nearest neighbour classification",
      "author" : [ "K.Q. Weinberger", "J. Blitzer", "L.K. Saul" ],
      "venue" : "NIPS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "SVM soft margin classifiers: linear programming versus quadratic programming",
      "author" : [ "Q. Wu", "D.X. Zhou" ],
      "venue" : "Neural computation, 17: 1160–1187,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Regularization networks with indefinite kernels",
      "author" : [ "Q. Wu" ],
      "venue" : "Journal of Approximation Theory, 166: 1–18,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distance metric learning with application to clustering with side information",
      "author" : [ "E. Xing", "A. Ng", "M. Jordan", "S. Russell" ],
      "venue" : "NIPS,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Analysis of SVM with indefinite kernels",
      "author" : [ "Y. Ying", "M. Girolami", "C. Campbell" ],
      "venue" : "NIPS,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sparse metric learning via smooth optimization",
      "author" : [ "Y. Ying", "K. Huang", "C. Campbell" ],
      "venue" : "NIPS,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "[3].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Due to the techniques dependent on the notion of uniform stability [6], the bound obtained there holds true only for the Frobenius matrixnorm regularization, which has a strong dependence on the dimensionality of the input space.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "Our techniques using the Rademacher complexity [5] and its related Khinchin-type inequality, in the cases of sparse L-norm and mixed (2, 1)-norm regularization, enables us to obtain bounds that have a mild dependence on the input dimensionality.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x′ ∈ Rd, by dM (x, x′) = √ (x− x′)TM(x− x′).",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x′ ∈ Rd, by dM (x, x′) = √ (x− x′)TM(x− x′).",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x′ ∈ Rd, by dM (x, x′) = √ (x− x′)TM(x− x′).",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 24,
      "context" : "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x′ ∈ Rd, by dM (x, x′) = √ (x− x′)TM(x− x′).",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 27,
      "context" : "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x′ ∈ Rd, by dM (x, x′) = √ (x− x′)TM(x− x′).",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 29,
      "context" : "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x′ ∈ Rd, by dM (x, x′) = √ (x− x′)TM(x− x′).",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "This approach has been successfully applied to image searching [9] and object recognition [18].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "This approach has been successfully applied to image searching [9] and object recognition [18].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "The generalization bounds were recently established for metric and similarity learning [7, 14, 18] under different statistical assumptions on the data.",
      "startOffset" : 87,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "The generalization bounds were recently established for metric and similarity learning [7, 14, 18] under different statistical assumptions on the data.",
      "startOffset" : 87,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "The generalization bounds were recently established for metric and similarity learning [7, 14, 18] under different statistical assumptions on the data.",
      "startOffset" : 87,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "In other words, it is not clear whether good generalization bounds for metric and similarity learning [14, 7] can lead to a good classification performance of the resultant k-NN classifiers.",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "In other words, it is not clear whether good generalization bounds for metric and similarity learning [14, 7] can lead to a good classification performance of the resultant k-NN classifiers.",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : "Such cases are quite common in applications such as hyperbolic tangent kernels [23], and the protein sequence similarity measures derived from SmithWaterman and BLAST score [22].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "Such cases are quite common in applications such as hyperbolic tangent kernels [23], and the protein sequence similarity measures derived from SmithWaterman and BLAST score [22].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : "Some methods [8, 30] learn a PSD kernel matrix from a prescribed indefinite kernel matrix, which are mostly restricted to the transductive settings.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 28,
      "context" : "Some methods [8, 30] learn a PSD kernel matrix from a prescribed indefinite kernel matrix, which are mostly restricted to the transductive settings.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 25,
      "context" : "Recent methods [27, 28] analyzed regularization networks such as ridge regression and SVM given a prescribed indefinite kernel, instead of aiming to learn an indefinite kernel function from data.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 26,
      "context" : "Recent methods [27, 28] analyzed regularization networks such as ridge regression and SVM given a prescribed indefinite kernel, instead of aiming to learn an indefinite kernel function from data.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "[3] proposed a regularized similarity learning approach, which is mainly motivated by the (ε, γ, τ)good similarity functions introduced in [1, 2].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[3] proposed a regularized similarity learning approach, which is mainly motivated by the (ε, γ, τ)good similarity functions introduced in [1, 2].",
      "startOffset" : 139,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "[3] proposed a regularized similarity learning approach, which is mainly motivated by the (ε, γ, τ)good similarity functions introduced in [1, 2].",
      "startOffset" : 139,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "However, due to the techniques dependent on the notion of uniform stability [6], the generalization bounds only hold true for the Frobenius matrix-norm regularization which usually has a strong dependence on the dimensionality of the input space.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "Our techniques using the Rademacher complexity [5] and the important Khinchin-type inequality for the Rademacher variables, in the cases of sparse L1-norm, and mixed (2, 1)-norm regularization, enables us to derive bounds that have a mild dependence on the input dimensionality.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "A natural approach to achieve the above aim [1, 3] is to minimize the following empirical error Ez(A) = 1 m ∑",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "A natural approach to achieve the above aim [1, 3] is to minimize the following empirical error Ez(A) = 1 m ∑",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "Its special case with the Frobenius matrix norm was established in [3].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "It used the uniform stability techniques [6], which, however, can not deal with non-strongly convex matrix-norms such as the L1-norm, (2, 1)-mixed norm and trace norm.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Our new analysis techniques are able to deal with general matrix norms, which depend on the concept of Rademacher averages [5] defined as follows.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "We show that the generalization bound for the similarity learning gives an upper bound for the generalization error of a linear classifier produced by the linear Support Vector Machine (SVM) [24] defined as follows: fz = argmin { 1 m ∑",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "Secondly, the bounds in Example 2 is consistent with that in [3].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "For instance, if the input space X = [0, 1]d, then we have supx∈X ‖x‖∞ = 1 and supx∈X ‖x‖F = √ d.",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "[4, 7, 9, 12, 14, 18, 26, 29].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "[4, 7, 9, 12, 14, 18, 26, 29].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "[4, 7, 9, 12, 14, 18, 26, 29].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "[4, 7, 9, 12, 14, 18, 26, 29].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "[4, 7, 9, 12, 14, 18, 26, 29].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 24,
      "context" : "[4, 7, 9, 12, 14, 18, 26, 29].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "[4, 7, 9, 12, 14, 18, 26, 29].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "[14] established generalization bounds for regularized metric learning algorithms via the concept of uniform stability [5], which, however, only works for strongly convex matrix regularization terms.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[14] established generalization bounds for regularized metric learning algorithms via the concept of uniform stability [5], which, however, only works for strongly convex matrix regularization terms.",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "A very recent work [7] established generalization bounds for the metric and similarity learning associated with general matrix norm regularization using techniques of Rademacher averages and U-statistics.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "For instance, the following pairwise empirical objective function was considered in [9, 7]: 1 m(m− 1) m ∑",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "For instance, the following pairwise empirical objective function was considered in [9, 7]: 1 m(m− 1) m ∑",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "[2] developed a theory of (ǫ, γ, τ)-good similarity function defined as follows.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "It was mentioned in [2] that the linear separator can be estimated by solving the following linear programming if we have du potentially unlabeled sample and dl labeled sample, min α { dl ∑",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "[2] in the following two aspects.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Secondly, although the separators are both trained from the linear SVM, the classification algorithm (13) in [2] was designed using two different sets of examples, a set of labeled samples of size dl to train the classification algorithm and another set of unlabeled samples with size du to define the mapping φ S .",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "[3] is mostly close to ours.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Generalization bounds for similarity learning were derived via uniform stability arguments [6] which can not deal with, for instance, the L1-norm and (2, 1)-norm regularization terms.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "In addition, the results about the relationship between the similarity learning and the performance of the learnt matrix in classification were quoted from [2] and hence requires two separate sets of samples to train the classifier.",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 11,
      "context" : "[13] and Kar et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] introduced an extended framework of [1, 2] in the general setting of supervised learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[15] introduced an extended framework of [1, 2] in the general setting of supervised learning.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "[15] introduced an extended framework of [1, 2] in the general setting of supervised learning.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "The authors proposed a general goodness criterion for similarity functions, which can handle general supervised learning tasks and also subsumes the goodness of condition of [2].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "Applying the McDiarmid’s inequality [19] (see Lemma 1 in the Appendix) to the term sup A∈A [ E(A)− Ez(A) ] , with probability at least 1− δ, there holds sup A∈A [ E(A)− Ez(A) ] ≤ Ez sup A∈A [ E(A)− Ez(A) ]",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "[5]), from the above estimation we can further estimate I2 as follows: I2 ≤ 1 rλEz sup x∈X ∥ ∥ ∥ Ez′ 1 m ∑",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "The above generalization bound for similarity learning formulation (2) with the Frobenius norm regularization is consistent with that given in [3], where the result holds true under the assumption that supx∈X ‖x‖F ≤ 1.",
      "startOffset" : 143,
      "endOffset" : 146
    } ],
    "year" : 2017,
    "abstractText" : "Learning an appropriate (dis)similarity function from the available data is a central problem in machine learning, since the success of many machine learning algorithms critically depends on the choice of a similarity function to compare examples. Despite many approaches for similarity metric learning have been proposed, there is little theoretical study on the links between similarity metric learning and the classification performance of the result classifier. In this paper, we propose a regularized similarity learning formulation associated with general matrix-norms, and establish their generalization bounds. We show that the generalization error of the resulting linear separator can be bounded by the derived generalization bound of similarity learning. This shows that a good generalization of the learnt similarity function guarantees a good classification of the resulting linear classifier. Our results extend and improve those obtained by Bellet at al. [3]. Due to the techniques dependent on the notion of uniform stability [6], the bound obtained there holds true only for the Frobenius matrixnorm regularization, which has a strong dependence on the dimensionality of the input space. Our techniques using the Rademacher complexity [5] and its related Khinchin-type inequality, in the cases of sparse L-norm and mixed (2, 1)-norm regularization, enables us to obtain bounds that have a mild dependence on the input dimensionality.",
    "creator" : "LaTeX with hyperref package"
  }
}