{
  "name" : "1510.00452.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Minimax Binary Classifier Aggregation with General Losses",
    "authors" : [ "Akshay Balsubramani", "Yoav Freund" ],
    "emails" : [ "abalsubr@ucsd.edu", "yfreund@ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "for a broad class of losses including but not limited to all convex surrogates. The result is a family of parameter-free ensemble aggregation algorithms, which are as efficient as linear learning and prediction for convex risk minimization but work without any relaxations whatsoever on many nonconvex losses like the 0-1 loss. The prediction algorithms take a familiar form, applying “link functions\" to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning – all this structure follows from a minimax interpretation of loss minimization."
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider a binary classification problem, in which we attempt to build the best predictor possible for data falling into two classes. At our disposal is an ensemble of individual classifiers which we can use in designing our predictor. The task is to predict with minimum error on a large unlabeled test set, on which we know the predictions of the ensemble classifiers but not the true test labels.\nWe know a priori that each classifier is constrained to perform well on the test data, based on generalization loss bounds from its performance on a labeled set. By examining the test predictions of each classifier and their patterns of disagreement, in addition to the constraints coupling the unknown test labels to each classifier’s predictions, we hope to predict the test labels well.\nThis problem was recently studied by the authors [BF15a], who gave a worst-case-optimal algorithm for it when the evaluation metric, and the constraints, are measured with zero-one classification error. In that work, the problem is formalized as a constrained optimization: the algorithm attempts to predict the true labels with minimum worst-case error, and these true labels are constrained by error bounds on individual ensemble classifiers.\nThe result is a convex optimization problem whose solution is a weighting over ensemble classifiers; this constitutes the learning phase of the algorithm. The prediction on each example in the test set turns out to be a sigmoid-like function of a linear combination of the ensemble predictions, using the learned weighting. The minimax structure ensures that this “link function,\" as well as the training algorithm, is completely data-dependent without parameter choices, relying merely on the structure of the zero-one loss function.\nHowever, this misclassification loss is inappropriate for other common binary classification tasks, such as estimating label probabilities, and handling false positives and false negatives differently. Such goals motivate the use of different losses like log loss and cost-weighted misclassification loss.\nIn this manuscript, we generalize the setup of [BF15a] to these loss functions and others. Like the earlier work, we show that the choice of loss function completely governs the learning and prediction phases of an efficient ensemble aggregation algorithm that is minimax optimal in our setting.\nIn the discussion of the work [BF15a] above, the loss function is used for two purposes: as an evaluation metric, and for the given performance bounds on classifiers in the ensemble (constraints). We give results for general evaluation metrics, but 0-1 loss constraints, in Section 2, along with some common concrete examples and extensions. Finally, we extend the framework to hold for more general loss constraints in Section 3.\nBut first, in the rest of this section, we give notational details of our setup, and formalize the loss functions encompassed by our treatment.\nar X\niv :1\n51 0.\n00 45\n2v 1\n[ cs\n.L G\n] 1\nO ct\n2 01"
    }, {
      "heading" : "1.1 Preliminaries",
      "text" : "Our setting generalizes that of Balsubramani and Freund [BF15a], in which we are given an ensemble H = {h1, . . . , hp} and unlabeled data x1, . . . , xn on which we wish to predict. The ensemble’s predictions on the unlabeled data are denoted by F:\nF = h1(x1) h1(x2) · · · h1(xn)... ... . . . ... hp(x1) hp(x2) · · · hp(xn)  ∈ [−1, 1]p×n (1) We use vector notation for the rows and columns of F: hi = (hi(x1), · · · , hi(xn))> and xj = (h1(xj), · · · , hp(xj))>. The test set has some binary labels (y1; . . . ; yn) ∈ {−1, 1}n. As in [BF15a], though, the test labels are allowed to be randomized, represented by values in [−1, 1] instead of just the two values {−1, 1}. So it is convenient to write the labels on the test data T as z = (z1; . . . ; zn) ∈ [−1, 1]n. 1 These true test set labels are unknown to the predictor.\nWrite [a]+ = max(0, a) and [n] = {1, 2, . . . , n}. All vector inequalities are componentwise."
    }, {
      "heading" : "1.2 Loss Functions",
      "text" : "On any single test point with randomized binary label zj ∈ [−1, 1], our expected performance upon predicting gj , with respect to the randomization of zj , is measured by a loss function `(zj , gj). It is apparent that\n`(zj , gj) =\n( 1 + zj\n2\n) `(1, gj) + ( 1− zj\n2\n) `(−1, gj) := ( 1 + zj\n2\n) `+(gj) + ( 1− zj\n2\n) `−(gj)\nwhere we conveniently write `+(gj) := `(1, gj) and `−(gj) := `(−1, gj). We call `± the partial losses, following earlier work [RW10].\nIn this manuscript, we make an assumption on `+(·) and `−(·):\nAssumption 1. Over the interval (−1, 1), `+(·) is decreasing 2 and `−(·) is increasing, and both are twice differentiable.\nWe view Assumption 1 as natural, because the loss function intuitively measures discrepancy to the true label ±1. (Differentiability is convenient for our proofs, but most of our arguments do not require it.) Notably, we do not require convexity or symmetry of the losses.\nThere has been much investigation into the nature of the loss ` and its partial losses, particularly on how to estimate the “conditional label probability\" zj using `(zj , gj). A natural operation to do this is to minimize the loss over gj ; accordingly, a loss ` such that arg min\ng∈[−1,1] `(zj , g) = zj (for all zj ∈ [−1, 1]) is called a\nproper loss [SJAM66, BSS05, RW10]."
    }, {
      "heading" : "2 Evaluation with General Losses",
      "text" : "The idea of [BF15a] is to formulate the ensemble aggregation problem as a two-player zero-sum game between a predictor and an adversary. In this game, the predictor is the first player, who plays g = (g1; g2; . . . ; gn), a randomized label gj ∈ [−1, 1] for each example {xj}nj=1. The adversary then sets the labels z ∈ [−1, 1]n.\nOf course, as mentioned previously, the classifiers are known to perform well on the test data. Accordingly, for this section we assume the predictor has knowledge of a correlation vector b ∈ (0, 1]p such that\n∀i ∈ [p], 1 n n∑ j=1 hi(xj)zj ≥ bi (2)\ni.e. 1nFz ≥ b. These p inequalities represent upper bounds on individual classifier error rates, which can be estimated from the training set w.h.p. when the training and test data are i.i.d. (statistical learning), in a\n1For example, a value of zi = 12 indicates yi = +1 w.p. 3 4 and −1 w.p. 1 4 . 2Our basic analysis holds more generally also, for functions that are not strictly decreasing but simply non-increasing.\nstandard way also used by ERM [BF15a]. So in our game-theoretic formulation, the adversary plays under ensemble classifier error constraints defined by b.\nThe predictor’s goal is to minimize the worst-case expected loss on the test data (w.r.t. the randomized labeling z), which we write\n`(z,g) := 1\nn n∑ j=1 `(zj , gj)\nThe predictor’s worst-case goal can be written as the following optimization problem, a game:\nV := min g∈[−1,1]n max z∈[−1,1]n,\n1 nFz≥b\n`(z,g) (3)\n= min g∈[−1,1]n max z∈[−1,1]n,\n1 nFz≥b\n1\nn n∑ j=1 [( 1 + zj 2 ) `+(gj) + ( 1− zj 2 ) `−(gj) ] (4)\n= 1\n2 min g∈[−1,1]n max\nz∈[−1,1]n, 1 nFz≥b\n1\nn n∑ j=1 [`+(gj) + `−(gj) + zj (`+(gj)− `−(gj))] (5)\nIn this manuscript, our goal is to solve the learning problem faced by the predictor, finding an optimal strategy g∗ realizing the minimum in (3). This strategy guarantees good worst-case performance on the unlabeled dataset, with an upper bound of V on the loss. This bound is perfectly tight, by virtue of the minimax argument above; there exists a z∗ obeying the ensemble loss constraints such that `(z∗,g∗) = V .\nNext, in Theorem 3, we exactly compute g∗ (and V ), and then give an efficient algorithm for learning it."
    }, {
      "heading" : "2.1 Results",
      "text" : "A few more quantities will be convenient to define before discussing our main results.\nThe loss-based score function Γ : [−1, 1] 7→ R is\nΓ(g) := `−(g)− `+(g)\n(We will also write the vector Γ(g) componentwise with [Γ(g)]j = Γ(gj) for convenience, so that Γ(hi) ∈ Rn and Γ(xj) ∈ Rp.) Observe that by our assumptions, Γ(g) is increasing on its domain. Therefore, we can discuss its inverse Γ−1(m). 3 This can be thought of as a sort of link function.\nWith these in mind, we can set up the solution to the game (3), which depends on the optimum of a convex function.\nDefinition 1 (Potential Well). Define the potential well\nΨ(m) =  −m+ 2`−(−1) if m ≤ Γ(−1) `+(Γ −1(m)) + `−(Γ −1(m)) if m ∈ (Γ(−1),Γ(1))\nm+ 2`+(1) if m ≥ Γ(1) (6)\nAs in [BF15a], we show that g∗ is a simple function of a particular weighting over the p hypotheses – a non-negative p-vector.\nDefinition 2 (Slack Function). Let σ ≥ 0p be a weight vector over H (not necessarily a distribution). The vector of ensemble predictions is F>σ = (x>1 σ, . . . ,x>n σ), whose elements’ magnitudes are the margins. The prediction slack function is\nγ(σ,b) := γ(σ) := −b>σ + 1 n n∑ j=1 Ψ(x>j σ) (7)\nAn optimal weight vector σ∗ is any minimizer of the slack function: σ∗ ∈ arg min σ≥0p [γ(σ)].\n3If Γ does not have a unique inverse, our arguments also work, mutatis mutandis, with the pseudoinverse Γ−1(m) = inf{g ∈ [−1, 1] : Γ(g) ≥ m}."
    }, {
      "heading" : "2.1.1 Solution of the Game",
      "text" : "These are used to describe the minimax equilibrium of the game (3), in our main result.\nTheorem 3. The minimax value of the game (3) is\nmin g∈[−1,1]n max z∈[−1,1]n,\n1 nFz≥b\n`(z,g) = V = 1\n2 γ(σ∗) =\n1 2 min σ≥0p −b>σ + 1 n n∑ j=1 Ψ(x>j σ)  The minimax optimal predictions are defined as follows: for all i ∈ [n],\ng∗j := gj(σ ∗) =  −1 if x>i σ∗ ≤ Γ(−1) Γ−1(x>i σ\n∗) if x>i σ∗ ∈ (Γ(−1),Γ(1)) 1 if x>i σ∗ ≥ Γ(1)\n(8)\nProof of Theorem 3. The main obstacle to solving (3) is the constrained maximization over z, which we handle first. Note that `(z,g) is linear in z, so that we are basically dealing with the constrained maximization\nmax z∈[−1,1]n,\n1 nFz≥b\n1\nn n∑ i=1 zj (`+(gj)− `−(gj)) = max z∈[−1,1]n,\n1 nFz≥b\n− 1 n z>[Γ(g)] = min σ≥0p\n[ −b>σ + 1\nn\n∥∥F>σ − Γ(g)∥∥ 1 ] (9)\nwhere the last equality uses Lemma 11, a basic application of Lagrange duality (from [BF15a], but proved in Section 4 for completeness).\nSubstituting (9) into (4) and simplifying,\nV = 1\n2 min g∈[−1,1]n  1 n n∑ j=1 [`+(gj) + `−(gj)] + max z∈[−1,1]n,\n1 nFz≥b\n1\nn n∑ j=1 zj (`+(gj)− `−(gj))  = 1\n2 min g∈[−1,1]n  1 n n∑ j=1 [`+(gj) + `−(gj)] + min σ≥0p [ −b>σ + 1 n ∥∥F>σ − Γ(g)∥∥ 1 ] = 1\n2 min σ≥0p −b>σ + min g∈[−1,1]n  1 n n∑ j=1 [`+(gj) + `−(gj)] + 1 n ∥∥F>σ − Γ(g)∥∥ 1  (10) = 1\n2 min σ≥0p −b>σ + 1 n n∑ j=1 min gj∈[−1,1] [ `+(gj) + `−(gj) + ∣∣x>j σ − Γ(gj)∣∣]  (11)\nThe absolute value breaks down into two cases, so the inner minimization’s objective can be simplified:\n`+(gj) + `−(gj) + ∣∣x>j σ − Γ(gj)∣∣ =\n{ 2`+(gj) + x > j σ if x>j σ ≥ Γ(gj)\n2`−(gj)− x>j σ if x>j σ < Γ(gj) (12)\nSuppose gj falls in the first case, so that x>j σ ≥ Γ(gj). From Assumption 1, 2`+(gj) + x>j σ is decreasing in gj , so it is minimized for the greatest g∗j ≤ 1 s.t. Γ(g∗j ) ≤ x>j σ. Since Γ(·) is increasing, exactly one of two subcases holds:\na) g∗j is such that Γ(g∗j ) = x>j σ, in which case the minimand (12) is `+(g∗j ) + `−(g∗j ) b) g∗j = 1 so that Γ(g∗j ) = Γ(1) < x>j σ, in which case the minimand (12) is 2`+(1) + x>j σ A precisely analogous argument holds if gj falls in the second case, where x>j σ < Γ(gj). Putting the cases together, (11) is equal to 12 minσ≥0p [γ(σ)]. We have proved the dependence of g∗j on x>j σ∗, where σ∗ is the minimizer of the outer minimization of (11). This completes the proof.\nThe optimization problem we constructed in Equation (11) of this proof warrants a comment. The constraints which do not explicitly appear with Lagrange parameters are all box, or L∞ norm, constraints. These decouple over the n test examples and are therefore fairly easy to handle – they reduce to the onedimensional optimization at the heart of Equation (11). We have introduced Lagrange parameters σ, λ for all the remaining constraints in the problem (only on z,a) which are “global\" over the n test examples and do not so decouple. This technique of optimizing halfway into the dual allows us to readily manipulate the problem exactly without using an approximation like weak duality, despite the lack of convexity in g.\nWe can also redo the proof of Theorem 3 when g ∈ [−1, 1]n is not left as a free variable set in the game, but instead is preset to g(σ) as in (8) for some (possibly suboptimal) weight vector σ.\nObservation 4. For any weight vector σ0 ≥ 0p, the worst-case loss after playing g(σ0) is bounded by\nmax z∈[−1,1]n,\n1 nFz≥b\n`(z,g(σ0)) ≤ 1\n2 γ(σ0)\nThe proof is a simpler version of that of Theorem 3; there is no minimum over g to deal with, and the minimum over σ ≥ 0p in Equation (10) is upper-bounded by using σ0. This weak duality result generalizes Obs. 4 of [BF15a].\nIt will also be useful to outline some properties of the potential well (and slack function).\nLemma 5. The potential well Ψ(m) is continuous and 1-Lipschitz. It is also convex under any of the following conditions: (A) The partial losses `±(·) are convex over (−1, 1). (B) The loss function `(·, ·) is a proper loss. (C) `′−(x)`′′+(x) ≥ `′′−(x)`′+(x) for all x ∈ (−1, 1).\n(Indeed, the proof shows that the last condition is both sufficient and necessary for convexity of Ψ, under 1.)\nNote that these conditions encompass convex surrogate losses commonly used in ERM, including all such “margin-based\" losses (convex univariate functions of zjgj). These constitute a large class of losses introduced primarily for their favorable computational properties relative to 0-1 loss ERM. We discuss this more in Section 2.3, but first introduce our counterpart learning algorithm and its computational properties."
    }, {
      "heading" : "2.2 The Ensemble Aggregation Algoritheorem",
      "text" : "Theorem 3 defines a prescription for aggregating the given ensemble predictions on the test set. This can be stated in terms of a learning algorithm and a prediction method.\nLearning. Minimize the slack function γ(σ), finding the minimizer σ∗ that achieves V . This is a convex optimization under broad conditions (Lemma 5), and when the test examples are i.i.d. it is a sum of n i.i.d. functions. As such it is readily amenable even to standard first-order optimization methods which require only O(1) test examples at once. In practice, learning employs such methods to approximately minimize γ, finding some σA such that γ(σA) ≤ γ(σ∗) + for some small . Standard convex optimization methods will do this because the slack function is Lipschitz, as Lemma 5 shows (combined with the observation that ‖b‖∞ ≤ 1).\nPrediction. Predict g(σ∗) on any test example, as indicated in (8). This decouples the prediction task on each test example, which is as efficient as p-dimensional linear prediction, requiring O(p) time and memory. After finding an -approximate minimizer σA in the learning step as above, Observation 4 tells us that the prediction g(σA) has loss guaranteed to be within 2 of V .\nIn particular, note that there is no algorithmic dependence on n in either step in a statistical learning setting, so our transductive formulation is no less tractable than a stochastic optimization setting in which i.i.d. data arrive one at a time."
    }, {
      "heading" : "2.3 Discussion and Extensions",
      "text" : "Some further comments are in order.\nThe work [BF15a] addresses a problem, 0-1 loss minimization, that is well known to be strongly NP-hard when solved directly. Formulating it in the transductive setting, in which the data distribution is known, is crucial. It gives the dual problem a special meaning, so the learning problem is on the always-convex Lagrange dual function and is therefore tractable. This work generalizes that idea, as the possibly nonconvex partial losses are minimized transductively via a straightforward convex optimization. A similar formal technique, including the use of L∞-norm constraints to decompose optimization efficiently over many examples, was used for a different purpose in the “drifting game\" analysis of boosting ([SF12], Sec. 13.4.1).\nOur transductive formulation involves no surrogates or relaxations of the loss, an advantage we believe is significant – it bypasses the consistency and agnostic-learning discussions [Zha04, BJM06] common to ERM methods that use convex risk minimization. Such methods are limited by the Bayes risk of the data, and (approximation) error analyses are generally ad hoc [Zha04], as compared to emerging directly from the convex optimization analysis in this work. However, such work does express the conclusion we explicitly derive – the learning problem is completely determined by the choice of loss function.\nAll our algorithms in this manuscript can be used in full generality with “specialist\" hypotheses in the ensemble that only predict on some subset of the test examples. This is done by merely changing F and b so that the loss bounds are only over these examples; see [BF15b]."
    }, {
      "heading" : "2.3.1 Uniform Convergence Bounds",
      "text" : "Given b as a lower bound on ensemble classifier losses, the slack function can be efficiently approximately optimized, translating into a worst-case prediction loss bound, as detailed in Section 2.2. This can also be extended to explicitly incorporate uniform convergence (L∞) bounds on b, which affect the dual (L1) norm of the dual vector σ.\nTheorem 6. We have\nmin g∈[−1,1]n max z∈[−1,1]n, ‖ 1nFz−b‖∞≤ `(z,g) = min σ∈Rp\n−b>σ + 1 n n∑ j=1 Ψ(x>j σ) + ‖σ‖1  Let σ∗s be the minimizer of the right-hand side above. Then the optimal g∗ = g(σ∗s ), the same function of the optimal weighting as in (8).\n(This is proved exactly like Theorem 3, but using Lemma 13 instead of Lemma 11.) So when the ensemble losses are uniformly bounded, we are now searching over all vectors σ (not just nonnegative ones) in an L1-regularized version of the original optimization problem in Theorem 3."
    }, {
      "heading" : "2.3.2 Weighted Test Sets, Covariate Shift, and Label Noise",
      "text" : "Though our results here deal with binary classification of a uniformly-weighted test set, note that our formulation deals with a weighted test set with only a modification to the slack function:\nTheorem 7. For any vector r ≥ 0n,\nmin g∈[−1,1]n max z∈[−1,1]n,\n1 nFz≥b\n1\nn n∑ j=1 rj`(zj , gj) = 1 2 min σ≥0p −b>σ + 1 n n∑ j=1 rjΨ ( x>j σ rj ) Such weighted classification can be parlayed into algorithms for general supervised learning problems via learning reductions [BLZ08]. Allowing weights on the test set for the evaluation is tantamount to accounting for covariate shift in our setting.\nIn addition, the weights used in Theorem 7 can be interpreted as changing box constraints on z; defining z̃ = r ◦ z, we have\nmin g∈[−1,1]n max z∈[−1,1]n,\n1 nFz≥b\n1\nn n∑ j=1 rj`(zj , gj) = min g∈[−1,1]n\nmax −r≤z̃≤r, 1 n F̃z̃≥b `(z̃,g)\nwhere F̃ is a suitably redefined version of F (s.t. x̃j = 1rj xj). The right-hand side here is formally equivalent to the original problem except for the box constraint on the adversary, which is now nonuniform. This was done for the 0-1 loss in ([BF15a], Prop. 5-6), where it was interpreted as constraining the adversary to act under a level of known label noise when ‖r‖∞ ≤ 1. It is clear from the above that when ‖r‖∞ ≤ 1,\nmin g∈[−1,1]n max −r≤z≤r, 1 nFz≥b\n`(z,g) = 1\n2 min σ≥0p −b>σ + 1 n n∑ j=1 rjΨ ( x>j σ ) ≤ 1 2 min σ≥0p [γ(σ)] = V\ni.e. knowing the noise level always helps in a minimax sense by further constraining z, as was seen for the 0-1 loss in [BF15a]."
    }, {
      "heading" : "2.4 Examples of Different Losses",
      "text" : "To further illuminate Theorem 3, we detail a few special cases in which `+, `− are explicitly defined. These losses may be found throughout the literature; for further information, see Reid and Williamson [RW10]. The key functions Ψ and g∗ are given for these losses in Table 1 and Figure 1. • 0-1 Loss: Here gj is taken to be a randomized binary prediction; this case was developed in [BF15a]. • Log Loss • Absolute Loss: The absolute loss can be defined as\n`abs− (gj) = 1 + gj and ` abs + (gj) = 1− gj (13)\nThe partial losses are the same as for 0-1 loss up to scaling, and therefore all our results are as well. • Square Loss • Cost-Weighted Misclassification (Quantile) Loss: This is defined with a parameter c ∈ [0, 1]\nrepresenting the relative cost of false positives vs. false negatives. • Exponential Loss • Logistic Loss • “AdaBoost Loss\": If the objective of AdaBoost [SF12] is interpreted as class probability estimation,\nthe implied loss is proper and given in [BSS05, RW10]."
    }, {
      "heading" : "3 Constraints on General Losses",
      "text" : "In the previous section, we make use of the fact that each hypothesis represents a linear constraint on the true labels z, in terms of zero-one loss. In fact, all the losses we consider are linear in z, as seen in (4):\n`(z,g) = 1\nn  n∑ j=1 1 2 [`+(gj) + `−(gj)]− 1 2 z>[Γ(g)]  (14) Therefore, each classifier hi can be used to constrain the test labels z, not with the zero-one loss of hi’s predictions, but rather with some other loss.\nAccordingly, recall that hi ∈ [−1, 1]n is the vector of test predictions of hi. Suppose we have an upper bound on the generalization loss of hi, i.e. `(z,hi) ≤ `i . If we define\nb`i := 1\nn n∑ j=1 [`+(hi(xj)) + `−(hi(xj))]− 2 `i (15)\nwe can use (14) to write\n`(z,hi) ≤ `i ⇐⇒ 1\nn z>[Γ(hi)] ≥ b`i (16)\nNow (16) is a linear constraint on z, just like each of the error constraints earlier considered in (2). Essentially the same analysis can be used to derive an aggregation algorithm with constraints like (16) as was used in Section 2 to solve the game (3).\nIn summary, any classifier can be used in our framework for aggregation if we have a generalization loss bound on it, where the loss can be any of the losses we have considered. This allows a huge variety of constraint sets, as each classifier considered can have constraints corresponding to any number of loss bounds on it, including 0 (it can be omitted), or > 1 (it can conceivably have multiple loss bounds using different losses). For instance, h1 can yield a constraint corresponding to a zero-one loss bound, h2 can yield one constraint corresponding to a square loss bound and another corresponding to a zero-one loss bound, and so on."
    }, {
      "heading" : "3.1 Matching Objective and Constraint Losses",
      "text" : "Despite this generality, we can glean some intuition about the aggregation method for general losses. To do so in the rest of this section, we only consider the case when each classifier contributes exactly one constraint to the problem, and the losses used for these constraints are all the same as each other and as the loss ` used in the objective function. In other words, the minimax prediction problem we consider is\nV ` := min g∈[−1,1]n\nmax z∈[−1,1]n,\n∀i∈[p]: `(z,hi)≤ `i\n`(z,g) = min g∈[−1,1]n\nmax z∈[−1,1]n,\n∀i∈[p]: 1nz >[Γ(hi)]≥b`i\n`(z,g) (17)\nThe matrix F and the slack function from (1) are therefore redefined:\nF `ij := Γ(hi(xj)) = `−(hi(xj))− `+(hi(xj)) (18)\nγ`(σ,b`) := γ`(σ) := −[b`]>σ + 1 n n∑ j=1 Ψ ( [Γ(xj)] >σ`∗ )\n(19)\nwhere b` = (b`1, . . . , b`p)>. The game (17) is clearly of the same form as the earlier formulation (3). Therefore, its solution has the same structure as in Thm. 3, proved using that theorem’s proof:\nTheorem 8. The minimax value of the game (17) is V := 12γ `(σ`∗) := minσ≥0p 1 2γ `(σ). The minimax optimal predictions are defined as follows: for all j ∈ [n],\ng∗j := gj(σ ∗) =  −1 if [Γ(xj)]>σ`∗ ≤ Γ(−1) Γ−1 ( [Γ(xj)] >σ`∗ )\nif [Γ(xj)]>σ`∗ ∈ (Γ(−1),Γ(1)) 1 if [Γ(xj)]>σ`∗ ≥ Γ(1)\n(20)"
    }, {
      "heading" : "3.2 Beating the Best Classifier and the Best Weighted Majority",
      "text" : "In minimizing the slack function over the dual parameters σ, we perform at least as well as the weighting σi ≥ 0p that puts weight 1 on hi and 0 on the remaining classifiers hi′ 6=i. In other words, our predictor always has the option of simply choosing the best single classifier i∗ and guaranteeing its loss bound `i∗ . Consequently, our predictor’s loss is always at most that of any single classifier, proving the following observation.\nProposition 9. V ` ≤ `i for any classifier i ∈ [p] and any loss `.\nThough the proposition is evident from the fact that we are minimizing over σ, we provide a short proof (in Section 4) using the definitions of this section to better illuminate how they fit together.\nSince optimizing the slack function involves searching over all σ ≥ 0p, our algorithm automatically admits superior worst-case loss bounds to any weighted majority vote as well, given the ensemble loss constraints b`.\nProposition 10. V ` is at most the loss of the best weighted majority vote, for any loss ` such that Γ(−1) and Γ(1) are finite.\nThe caveat merely excludes loss functions, like the log loss, which involve no “clipping\" (truncation to [−1, 1]) of the algorithm’s prediction g. For such losses, the prediction of the weighted majority vote can be infinitely penalized if it is incorrect even once, so it does not make sense to consider weighted majorities there."
    }, {
      "heading" : "4 Supporting Results and Proofs",
      "text" : "Lemma 11. For any a ∈ Rn,\nmax z∈[−1,1]n,\n1 nFz≥b\n1 n z>a = min σ≥0p\n[ −b>σ + 1\nn\n∥∥F>σ + a∥∥ 1\n]\nProof. We have\nmax z∈[−1,1]n, Fz≥nb\n1 n z>a = 1 n max z∈[−1,1]n min σ≥0p\n[ z>a + σ>(Fz− nb) ] (21)\n(a) =\n1 n min σ≥0p max z∈[−1,1]n\n[ z>(a + F>σ)− nb>σ ] (22)\n= 1\nn min σ≥0p\n[∥∥a + F>σ∥∥ 1 − nb>σ ] = min σ≥0p [ −b>σ + 1 n ∥∥F>σ + a∥∥ 1 ] (23)\nwhere (a) is by the minimax theorem ([SF12], p.144).\nProof of Lemma 5. Continuity follows by checking Ψ(m) at m = ±1. For Lipschitzness, note that for m ∈ (Γ(−1),Γ(1)), by (28),\nΨ′(m) = `′−(Γ −1(m)) + `′+(Γ −1(m))\n`′−(Γ −1(m))− `′+(Γ−1(m))\n(24)\n= −1 + 2`′−(Γ −1(m))\n`′−(Γ −1(m))− `′+(Γ−1(m))\n(25)\n= 1− 2(−`′+(Γ−1(m)))\n`′−(Γ −1(m))− `′+(Γ−1(m))\n(26)\nUsing Assumption 1 on the partial losses, equations (25) and (26) respectively make clear that Ψ′(m) ≥ −1 and Ψ′(m) ≤ 1 on this interval. Since Ψ′(m) is −1 for m < Γ(−1) and 1 for m > Γ(1), it is 1-Lipschitz.\nAs for convexity, since Ψ is linear outside the interval (Γ(−1),Γ(1)), it suffices to show that Ψ(m) is convex inside this interval, which is shown in Lemma 12.\nLemma 12. The function `+(Γ−1(m)) + `−(Γ−1(m)) is convex for m ∈ (Γ(−1),Γ(1)) under any of the conditions of Lemma 5.\nProof of Lemma 12. Define F (m) = `+(Γ−1(m)) + `−(Γ−1(m)). By basic properties of the derivative, d [ Γ−1 ] dm = 1 Γ′(Γ−1(m)) = 1\n`′−(Γ −1(m))− `′+(Γ−1(m))\n≥ 0 (27)\nwhere the last inequality follows by Assumption 1. Therefore, by the chain rule and (27),\nF ′(m) = `′−(Γ −1(m)) + `′+(Γ −1(m))\n`′−(Γ −1(m))− `′+(Γ−1(m))\n(28)\nFrom this, we calculate F ′′(m), writing `′±(Γ−1(m)) and `′′±(Γ−1(m)) as simply `′± and `′′± for clarity:\nF ′′(m) = d[Γ−1] dm(\n`′−(Γ −1(m))− `′+(Γ−1(m)) )2︸ ︷︷ ︸ (a)\n[( `′− − `′+ ) ( `′′− + ` ′′ + ) − ( `′− + ` ′ + ) ( `′′− − `′′+ )]\nFrom (27), observe that the term (a) = ( `′−(Γ −1(m))− `′+(Γ−1(m)) )−3 ≥ 0. Therefore, it suffices to show that the other term is ≥ 0. But this is equal to( `′− − `′+ ) ( `′′− + ` ′′ + ) − ( `′− + ` ′ + ) ( `′′− − `′′+ ) = 2(`′−` ′′ + − `′′−`′+) (29)\nThis proves that condition (C) of Lemma 5 is sufficient for convexity of F (and indeed necessary also, under 1 on the partial losses).\nWe now address the other conditions of Lemma 5. (A) implies (C), because by Assumption 1, `′−, `′′+, `′′− are ≥ 0 and `′+ ≤ 0, so (29) is ≥ 0 as desired.\nFinally we prove that (B) implies (C). If ` is proper, then it is well known (e.g. Thm. 1 of [RW10], and [BSS05]) that for all x ∈ (−1, 1),\n`′−(x) 1 + x = − `′+(x)\n1− x (This is a simple and direct consequence of stationary conditions from the properness definition.)\nDefine the function w(x) = ` ′ −(x) 1+x = − `′+(x)\n1−x ; we drop the argument and write it and its derivative as w and w′ for clarity. By direct computation,\n`′−` ′′ + − `′′−`′+ = [(1 + x)w (w + (x− 1)w′)]− [(w + (1 + x)w′)(x− 1)w] = [ (1 + x)w2 + (x2 − 1)ww′ ] − [ (x− 1)w2 + (x2 − 1)ww′ ] = 2w2 ≥ 0\nso (C) is true as desired.\nLemma 13. For any a ∈ Rn,\nmax z∈[−1,1]n, ‖ 1nFz−b‖∞≤\n1 n z>a = min σ∈Rp\n[ −b>σ + 1\nn\n∥∥F>σ + a∥∥ 1 + ‖σ‖1\n]\nProof.\nmax z∈[−1,1]n, ‖ 1nFz−b‖∞≤\n1 n z>a = max\nz∈[−1,1]n, 1 nFz−b≤ 1 n, − 1nFz+b≤ 1 n\n1 n z>a\n= 1\nn max z∈[−1,1]n min λ,ξ≥0p\n[ z>a + λ>(−Fz + nb + n 1n) + ξ>(Fz− nb + n 1n) ] = 1\nn min λ,ξ≥0p max z∈[−1,1]n\n[ z>(a + F>(ξ − λ)) + λ>(nb + n 1n) + ξ>(−nb + n 1n) ] = 1\nn min λ,ξ≥0p\n[∥∥a + F>(ξ − λ)∥∥ 1 − nb>(ξ − λ) + n 1n>(ξ + λ) ] Suppose for some j ∈ [n] that ξj > 0 and λj > 0. Then subtracting min(ξj , λj) from both does not affect the value [ξ−λ]j , but always decreases [ξ+λ]j , and therefore always decreases the objective function. Therefore, we can w.l.o.g. assume that ∀j ∈ [n] : min(ξj , λj) = 0, so defining σj = ξj − λj for all j, the last equality above becomes\n1 n min λ,ξ≥0p [∥∥a + F>(ξ − λ)∥∥ 1 − nb>(ξ − λ) + n 1n>(ξ + λ) ] = 1 n min σ∈Rp [∥∥a + F>σ∥∥ 1 − nb>σ + n ‖σ‖1 ]\nProof of Theorem 7. The proof is quite similar to that of Theorem 3, but generalizes it. First note that we have\nmax z∈[−1,1]n,\n1 nFz≥b\n1\nn n∑ i=1 rjzj (`+(gj)− `−(gj))\n= max z∈[−1,1]n,\n1 nFz≥b\n− 1 n z>[r ◦ Γ(g)] = min σ≥0p\n[ −b>σ + 1\nn\n∥∥F>σ − (r ◦ Γ(g))∥∥ 1 ] (30)\nwhere the last equality uses Lemma 11.\nTherefore, using (30) on the left-hand side of what we wish to prove,\nV = 1\n2 min g∈[−1,1]n  1 n n∑ j=1 rj [`+(gj) + `−(gj)] + max z∈[−1,1]n,\n1 nFz≥b\n1\nn n∑ i=1 rjzj (`+(gj)− `−(gj))  = 1\n2 min g∈[−1,1]n  1 n n∑ j=1 rj [`+(gj) + `−(gj)] + min σ≥0p −b>σ + 1 n n∑ j=1 ∣∣x>j σ − rjΓ(gj)∣∣ \n= 1\n2 min σ≥0p −b>σ + 1 n n∑ j=1 min gj∈[−1,1] ( rj [`+(gj) + `−(gj)] + ∣∣x>j σ − rjΓ(gj)∣∣)  (31)\nAs in the proof of Theorem 3, the inner minimization’s objective can be simplified:\nrj(`+(gj) + `−(gj)) + ∣∣x>j σ − rjΓ(gj)∣∣ =\n{ 2rj`+(gj) + x > j σ if x>j σ ≥ rjΓ(gj)\n2rj`−(gj)− x>j σ if x>j σ < rjΓ(gj) (32)\nSuppose gj falls in the first case, so that x>j σ ≥ rjΓ(gj). From Assumption 1, 2rj`+(gj) + x>j σ is\ndecreasing in gj , so it is minimized for the greatest g∗j ≤ 1 s.t. Γ(g∗j ) ≤ x>j σ rj . Since Γ(·) is increasing, exactly one of two subcases holds: a) g∗j is such that Γ(g∗j ) = x>j σ rj , in which case the minimand (32) is rj(`+(g∗j ) + `−(g∗j ))\nb) g∗j = 1 so that Γ(g∗j ) = Γ(1) < x>j σ rj , in which case the minimand (32) is 2rj`+(1) + x>j σ\nA precisely analogous argument holds if gj falls in the second case, where x>j σ < Γ(gj). So as before, we have proved the dependence of g∗j on x>j σ∗, where σ∗ is the minimizer of the outer minimization of (31). This completes the proof.\nProof of Proposition 9. Consider the weighting σi as above. Then\nV ` = 1\n2 min σ≥0p γ`(σ) ≤ 1 2 γ`(σi) = −b`i + 1 n n∑ j=1 Ψ(Γ(hi(xj))) (33)\nSince hi(xj) ∈ [−1, 1] ∀j, we have Γ(hi(xj)) ∈ [Γ(−1),Γ(1)]. Using the definitions of Ψ and b` (in Equation (15)), (33) can therefore be rewritten as\n2V ` ≤ −b`i + 1\nn n∑ j=1 [ `+(Γ −1(Γ(hi(xj)))) + `−(Γ −1(Γ(hi(xj)))) ] = − 1\nn n∑ j=1 [`+(hi(xj)) + `−(hi(xj))] + 2 ` i + 1 n n∑ j=1 [`+(hi(xj)) + `−(hi(xj))] = 2 ` i"
    } ],
    "references" : [ {
      "title" : "Optimally combining classifiers using unlabeled data",
      "author" : [ "Akshay Balsubramani", "Yoav Freund" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Balsubramani and Freund.,? \\Q2015\\E",
      "shortCiteRegEx" : "Balsubramani and Freund.",
      "year" : 2015
    }, {
      "title" : "Scalable semi-supervised classifier aggregation",
      "author" : [ "Akshay Balsubramani", "Yoav Freund" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Balsubramani and Freund.,? \\Q2015\\E",
      "shortCiteRegEx" : "Balsubramani and Freund.",
      "year" : 2015
    }, {
      "title" : "Convexity, classification, and risk bounds",
      "author" : [ "Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2006
    }, {
      "title" : "Machine learning techniques?reductions between prediction quality metrics",
      "author" : [ "Alina Beygelzimer", "John Langford", "Bianca Zadrozny" ],
      "venue" : "In Performance Modeling and Engineering,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2008
    }, {
      "title" : "Loss functions for binary class probability estimation and classification: Structure and applications",
      "author" : [ "Andreas Buja", "Werner Stuetzle", "Yi Shen" ],
      "venue" : null,
      "citeRegEx" : "Buja et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Buja et al\\.",
      "year" : 2005
    }, {
      "title" : "Composite binary losses",
      "author" : [ "Mark D Reid", "Robert C Williamson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Reid and Williamson.,? \\Q2010\\E",
      "shortCiteRegEx" : "Reid and Williamson.",
      "year" : 2010
    }, {
      "title" : "Boosting: Foundations and Algorithms",
      "author" : [ "Robert E. Schapire", "Yoav Freund" ],
      "venue" : null,
      "citeRegEx" : "Schapire and Freund.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schapire and Freund.",
      "year" : 2012
    }, {
      "title" : "Admissible probability measurement procedures",
      "author" : [ "Emir H Shuford Jr.", "Arthur Albert", "H Edward Massengill" ],
      "venue" : null,
      "citeRegEx" : "Jr et al\\.,? \\Q1966\\E",
      "shortCiteRegEx" : "Jr et al\\.",
      "year" : 1966
    }, {
      "title" : "Statistical behavior and consistency of classification methods based on convex risk minimization",
      "author" : [ "Tong Zhang" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Zhang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2004
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We develop a worst-case analysis of aggregation of binary classifier ensembles in a transductive setting, for a broad class of losses including but not limited to all convex surrogates. The result is a family of parameter-free ensemble aggregation algorithms, which are as efficient as linear learning and prediction for convex risk minimization but work without any relaxations whatsoever on many nonconvex losses like the 0-1 loss. The prediction algorithms take a familiar form, applying “link functions\" to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning – all this structure follows from a minimax interpretation of loss minimization.",
    "creator" : "LaTeX with hyperref package"
  }
}