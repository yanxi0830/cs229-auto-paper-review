{
  "name" : "1104.2097.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Vladimir Pestov" ],
    "emails" : [ "vpest283@uottawa.ca)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 4.\n20 97\nv1 [\ncs .L\nG ]\n1 2\nA pr\n2 01\n1\nI. INTRODUCTION\nThe following is a fundamental result of statistical learning theory.\nTheorem 1: For a concept class C the following three conditions are equivalent:\n1) C is distribution-free PAC learnable, 2) C is a uniform Glivenko–Cantelli class, and 3) the Vapnik–Chervonenkis dimension of C is finite.\nIt is in this form that the theorem is usually stated in textbooks on the subject, see [1], [2]. The condition 1) means the existence of a learning rule for C which is probably approximately correct.\nHowever, strictly speaking, the result is only true under a suitable measurability assumption on the concept class C . One such assumption is that of C being image admissible Souslin: the class C can be parametrized with elements of the unit interval so that pairs (x, t), x ∈ Ct, t ∈ [0, 1] form an analytic subset of Ω × [0, 1] [3]. Another measurability assumption, more difficult to state, is that of a well-behaved class C [2]. Under either of those conditions, the statement (1) in Theorem 1 can be replaced with\n1′) C is distribution-free consistently PAC learnable,\nmeaning that every consistent learning rule L for C is distribution-free probably approximately correct. In the proof, a measurability hypothesis on C has to be invoked twice, in order to deduce implications (3) ⇒ (2) and (1′ ⇒ (1).\nVladimir Pestov is with Departamento de Matemática, Universidade Federal de Santa Catarina, Campus Universitário Trindade, CEP 88.040-900 Florianópolis-SC, Brasil (CNPq Visiting Researcher) and the Department of Mathematics and Statistics, University of Ottawa, 585 King Edward Avenue, Ottawa, Ontario, K1N 6N5 Canada (permenent address, phone: 613-562- 5800 ext. 3523, fax: 613-562-5776, email: vpest283@uottawa.ca).\nIn particular, Theorem 1 holds for every countable class C or, more generally, for every universally separable class [4]. It is arguable that every concept class emerging in either theory or applications of statistical learning will be measurable in a sufficiently strong sense. For this reason, a measurability condition on C is typically not even mentioned.\nThe fact remains that Theorem 1 cannot be derived in full generality. An example of a concept class C of Vapnik– Chervonenkis (VC) dimension one which is not uniform Glivenko–Cantelli was constructed by Durst and Dudley [5], and a further modification of this example, also of VC dimension one, fails consistent PAC learnability [2].\nThis example has been constructed under Continuum Hypothesis (CH), which is arguably not a natural assumption in a probabilistic context [6]. However, the example remains valid under much more relaxed and natural set-theoretic hypothesis: Martin’s Axiom (MA). It is one of the most often used and best studied additional set-theoretic assumptions beyond the standard Zermelo-Frenkel set theory with the Axiom of Choice (ZFC). In particular, Martin’s Axiom follows from the Continuum Hypothesis (CH), but it is also compatible with the negation of CH, and in fact it is namely the combination MA+¬CH that is really interesting [7], [8], [9].\nIn this note we make the following, somewhat astonishing, observation: under the same assumption (Martin’s Axiom), the conditions (1) and (3) in Theorem 1 are equivalent. Here is our main result.\nTheorem 2: Assume the validity of Martin’s Axiom (MA). Then the following are equivalent for every concept class C consisting of universally measurable subsets of a Borel domain Ω:\n1) C is distribution-free PAC learnable, and 2) the Vapnik–Chervonenkis dimension of C is finite.\nOf course it is only the implication (2)⇒(1) that needs proving, because (1)⇒(2) is a well-known classical result from [2] which does not require any assumptions on C .\nWe review a precise formal setting for learnability, after which we proceed to analysis of a counter-example from [5], [2]. We observe that the concept class C in the example is in fact PAC learnable, and this observation provides a clue to a general result.\nThe construction of the learning rule L can be described as a “first in, first served” approach. The concept class C is given a minimal well-ordering, ≺, and L is constructed recursively, by assigning to a learning sample the ≺-smallest consistent concept C with regard to the the ordering. As a\nconsequence, for every concept C ∈ C , the image of all learning samples of the form (σ,C ∩ σ) under L forms a uniform Glivenko–Cantelli class. It is for establishing this property of L that we need Martin’s Axiom. Now the probable approximate correctness of L is straightforward.\nThe present approach goes back to present author’s earlier work [10], but the results are new and have never been stated explicitely before."
    }, {
      "heading" : "II. THE SETTING",
      "text" : "For obvious reasons, we need to be quite precise when fixing a general setting for learnability. The domain (instance space) Ω = (Ω,A ) is a standard Borel space, that is, a complete separable metric space equipped with the sigmaalgebra of Borel subsets (the smallest family of sets containing all open balls and closed under complements and countable intersections).\nMeasures on Ω mean Borel probability measures, that is, countably additive functions on A with values in the unit interval [0, 1], having the property µ(Ω) = 1. We will not distinguish between a measure µ and its Lebesgue completion, that is, an extension of µ over a larger sigma-algebra of Lebesgue µ-measurable subsets of Ω. Furthermore, recall that a subset A ⊆ Ω is universally measurable if it is Lebesgue µ-measurable for every probability measure µ on Ω.\nWith this caveat, a concept class, C , is a family of universally measurable subsets of Ω.\nIn the learning model, a set P of probability measures on Ω is fixed. Usually either P = P (Ω) is the set of all probability measures (distribution-free learning), or P = {µ} is a single measure (learning under fixed distribution). In our article, the case of interest is the former, although some of our results are valid in the case of a general family P ⊆ P (Ω).\nA learning sample is a pair (σ, τ) of finite subsets of Ω, where τ ⊆ σ is thought of as the set of points belonging to an unknown concept, C. The set of all samples of size n is usually identified with (Ω× {0, 1})n.\nA learning rule (for C ) is a mapping\nL : ∞ ⋃\nn=1\nΩn × {0, 1}n → C\nwhich satisfies the following measurability condition: for every C ∈ C , n ∈ N and µ ∈ P , the function\nΩn ∋ σ 7→ µ (L(σ,C ∩ σ)△ C) ∈ R (1)\nis measurable. A learning rule L is consistent (with a concept class C ) if for all C ∈ C , n ∈ N and σ ∈ Ωn one has\nL(σ,C ∩ σ) ∩ σ = C ∩ σ.\nA learning rule L is probably approximately correct (PAC) under P if for every ǫ > 0\nµ⊗n {σ ∈ Ωn : µ (L(σ,C ∩ σ)△ C) > ǫ} → 0 (2)\nas n → ∞, uniformly over all C ∈ C and µ ∈ P . Here µ⊗n denotes the product measure on Ωn.\nIn terms of sample complexity function s(ǫ, δ), a learning rule L is PAC if for each C ∈ C and every µ ∈ P an independent identically distributed (i.i.d.) sample σ = (x1, x2, . . . , xn) with n ≥ s(ǫ, δ) points has the property µ(C △L(σ,C ∩ σ)) < ǫ with confidence ≥ 1− δ.\nA concept class C is PAC learnable under P , if there exists a PAC learning rule for C under P . A class C is consistently learnable (under P) if every learning rule consistent with C is PAC under P . If P = P (Ω) is the set of all probability measures, then C is said to be distribution-free PAC learnable. If P = {µ} is a single probability measure, one is talking of learning under a single distribution. Learnability under intermediate families P is also receiving considerable attention, cf. Chapter 7 in [11].\nNotice that in this paper, we only talk of potential PAC learnability, adopting a purely information-theoretic viewpoint. As a consequence, our statements about learning rules are existential rather than constructive, and building learning rules by transfinite recursion is perfectly acceptable.\nA concept class C is uniform Glivenko–Cantelli with regard to a family of measures P , if for each ǫ > 0\nsup µ∈P\nµ⊗n {\nsup C∈C |µ(C) − µn(C)| ≥ ǫ\n}\n→ 0 as n → ∞.\n(3) Here µn stands for the empirical (uniform) measure on n points, sampled in an i.i.d. fashion from Ω according to the distribution µ. In this case, one also says that C has the property of uniform convergence of empirical measures (UCEM property) (with regard to P) [11].\nEvery uniform Glivenko–Cantelli concept class (with regard to P) is consistently PAC learnable (under P), as is easy to verify. In the distribution-free situation (P = P (Ω)) the converse holds under additional measurability conditions on the class mentioned in the Introduction, but, as we will see, not always.\nMore precisely, every distribution-free PAC learnable class has finite VC dimension (it was proved in [2], Theorem 2.1(i); see also e.g. [11], Lemma 7.2 on p. 279). Now the measurabilty conditions on C assure that a class C of finite VC dimension d is uniform Glivenko–Cantelli, with a sample complexity bound that does not depend on C , but only on ǫ, δ, and d. The following is a typical (and far from being optimal) such estimate, which can be deduced, for instance, along the lines of [12]:\ns(ǫ, δ, d) ≤ 128\nǫ2\n(\nd log\n(\n2e2\nǫ log\n2e\nǫ\n)\n+ log 8\nδ\n)\n. (4)\nFor our purposes, we will fix any such bound and refer to it as a “standard” sample complexity estimate for s(ǫ, δ, d).\nNow the consistent learnability for C , with the same sample complexity, follows. Of course in order to conclude that C is PAC learnable, it is necessary to prove the existence of a consistent learning rule satisfying Eq. (1). This is usually being done using subtle measurable selection theorems using the same measurability assumptions on C yet again.\nFinally, recall that a subset N ⊆ Ω is universal null if for every non-atomic probability measure µ on (Ω,A ) one has\nµ(N ′) = 0 for some Borel set N ′ containing N . Universal null Borel sets are just countable sets."
    }, {
      "heading" : "III. REVISITING AN EXAMPLE OF DURST AND DUDLEY",
      "text" : "The proof of the implication (3)⇒(2) in Theorem 1 depends in an essential way on the Fubini theorem, which is why some measurability restrictions on the class C are unavoidable. Without them, the conclusion is not true in general. Here is a classical example of a concept class having finite VC dimension which is not uniform Glivenko–Cantelli.\nExample 3 (Durst and Dudley [5], Proposition 2.2): Assume the validity of the Continuum Hypothesis (CH). Let Ω be an uncountable standard Borel space, that is, up to an isomorphism, a Borel space associated to the unit interval [0, 1]. The statement of CH is equivalent to the existence of a total order ≺ on Ω with the property that every half-open initial segment Iy = {x ∈ Ω: x ≺ y}, y ∈ Ω is countable, and ≺ is a well-ordering: every non-empty subset of Ω has the smallest element. Fix such an order.\nLet C consist of all half-open initial segments Iy , y ∈ Ω as above. Clearly, the VC dimension of the class C is one.\nNow let µ be a non-atomic Borel probability measure on Ω (e.g., the Lebesgue measure on [0, 1]). Under CH, every element of C is a countable set, therefore Borel measurable of measure zero. At the same time, for every n and each i.i.d. random n-sample σ, there is a countable initial segment C = Iy ∈ C containing all elements of σ. The empirical measure of C with regard to σ is one. Thus, no finite sample guesses the measure of all elements of C to within an accuracy ǫ < 1 with a non-vanishing confidence.\nSee also [13], p. 314; [3], pp. 170–171. A further modification of this construction gives an example of a concept class of finite VC dimension which is not consistently PAC learnable.\nExample 4 (Blumer et al. [2], p. 953): Again, assume CH. Add to the concept class C from Example 3 the set Ω as an element, forming a new concept class C ′ = C ∪ {Ω}. One still has VC(C ′) = 1. For a finite labelled sample (σ, τ) define\nL(σ, τ) = Iz , z = min{y ∈ (Ω,≺) : τ ⊆ Iy}. (5)\nThe learning rule L is consistent with the class C ′. At the same time, L is not probably approximately correct. Indeed, for the concept C = Ω the value of the learning rule L(σ,Ω ∩ σ) = L(σ, σ) will always return a countable concept Iy for some y ∈ Ω, and if µ is a non-atomic Borel probability measure on Ω, then µ(C△ Iy) = 1. The concept C = Ω cannot be learned to accuracy ǫ < 1 with a non-zero confidence.\nRemark 5: It is important to note that — again, under CH — the class C ′ is distribution-free PAC learnable.\nIndeed, redefine a well-ordering on C = {Ix : x ∈ Ω} ∪ {Ω} by making Ω the smallest element (instead of the largest one) and keeping the order relation between other elements the same. Denote the new order relation by ≺1, and define a learning rule L1 similarly to Eq. (5), but this time\nunderstanding the minimum with regard to the well-ordering ≺1:\nL1(σ, τ) = min (≺1)\n\n\n\nC ∈ C : C ∩ σ = ⋂\nτ⊆D\nD\n\n\n\n. (6)\nIn essence, L1 examines all the concepts following a transfinite order on them, and returns the first encountered concept consistent with the sample, provided it exists.\nTo see what difference it makes with Example 4, let µ be again a non-atomic probability measure on Ω. If C = Ω, then for every sample σ consistently labelled with C the rule L1 will return C, because this is the smallest consistent concept encountered by the algorithm. If C 6= Ω, then for µalmost all samples σ the labelling on σ produced by C will be empty, and the concept L1(σ, ∅) returned by L1, while possibly different from C, will be again a countable concept, meaning that µ(C △L(σ, ∅)) = 0.\nTo give a formal proof that L1 is PAC, notice that for every C ∈ C ′ and each n ∈ N the collection of pairwise distinct concepts L1(σ ∩C), σ ∈ Ωn is only countable (under CH), because they are all contained in the ≺1-initial segment of a minimally ordered set C of cardinality continuum, bounded by C itself. As a consequence, the concept class\nLC1 = {L1(σ ∩C) : σ ∈ Ω n, n ∈ N} ⊆ C ′ (7)\nis also countable (assuming CH). The VC dimension of the family LC1 ∪{C} is ≤ 1, and being countable, it is a uniform Glivenko–Cantelli class with a standard sample complexity as in Eq. (4). Consequently, given ǫ, δ > 0, and assuming that n is sufficiently large, one has for each probability measure µ on Ω and every σ ∈ Ωn\nµ(C △L(σ,C ∩ σ)) < ǫ\nprovided n ≥ s(ǫ, δ, 1), as required. Remark 6: Notice that the role of the Continuum Hypothesis in the above examples was merely to assure that every initial segment Iy , y ∈ Ω is a universally measurable set. As we will see, it can be achieved under a much milder assumption of Martin’s Axiom.\nRemark 7: Thus, under the Continuum Hypothesis, the example of Dudley and Durst as modified by Blumer, Ehrenfeucht, Haussler, and Warmuth gives an example of a PAC learnable concept class which is not uniform Glivenko– Cantelli (even if having finite VC dimension). As it will become clear in the next Section, the assumption of CH can be weakened to Martin’s Axiom. Still, it would be interesting to know whether an example with the same combination of properties can be constructed without additional set-theoretic assumptions.\nA basic observation of this Section is that in order for a learning rule L to be PAC, the assumption on C being uniform Glivenko–Cantelli can be weakened as follows.\nLemma 8: Let C be a concept class and P a family of probability measures on the domain Ω. Suppose there exists a function s(ǫ, δ) and a learning rule L for C with the property\nthat for every C ∈ C , the set LC ∪{C} is Glivenko–Cantelli with regard to P with the sample complexity s(ǫ, δ), where\nLC = {L(C ∩ σ) : σ ∈ Ωn, n ∈ N} .\nThen L is probably approximately correct under P with sample complexity s(ǫ, δ).\nThis simple fact becomes useful in combination with the technique of well-orderings. Of course the Continuum Hypothesis is a particularly unnatural assumption in a probabilistic context (cf. [6]). But it is unnecessary. Martin’s Axiom (MA) is a much weaker and natural additional settheoretic axiom, which works just as well."
    }, {
      "heading" : "IV. LEARNABILITY UNDER MARTIN’S AXIOM",
      "text" : "Martin’s Axiom (MA) says that no compact Hausdorff topological space with the countable chain condition is a union of strictly less than continuum nowhere dense subsets. Thus, it is a stronger statement than the Baire Category Theorem. In particular, the Continuum Hypothesis implies MA. However, MA is compatible with the negation of CH, and this is where the most interesting applications of MA are to be found. We need the following consequence of MA.\nTheorem 9 (Martin-Solovay): Let (Ω, µ) be a standard Lebesgue non-atomic probability space. Under MA, the Lebesgue measure is 2ℵ0 -additive, that is, if κ < 2ℵ0 and Aα, α < κ is family of pairwise disjoint measurable sets, then ∪α<κAα is Lebesgue measurable and\nµ\n(\n⋃\nα<κ\nAα\n)\n= ∑\nα<κ\nµ(Aα).\nIn particular, the union of strictly less than continuum null subsets of Ω is a null subset.\nFor the proof and more on MA, see [9], Theorem 2.21, or [7], or [8], pp. 563–565.\nLemma 10: Let C be a concept class and P a family of probability measures on a standard Borel domain Ω. Consider the following properties.\n1) Every countable subclass of C is uniform Glivenko– Cantelli with regard to P . 2) There is a function s(ǫ, δ) so that every countable subclass of C is uniform Glivenko–Cantelli with regard to P with sample complexity s(ǫ, δ). 3) Every subclass C ′ of C having cardinality < 2ℵ0 is uniform Glivenko–Cantelli with regard to P . 4) There is a function s(ǫ, δ) so that every subclass C ′\nof C having cardinality < 2ℵ0 is uniform Glivenko– Cantelli with regard to P with sample complexity s(ǫ, δ).\nThen (1)\nրւ տ (2) (3)\nտ ր (4)\nUnder Martin’s Axiom, all four conditions are equivalent.\nProof: The implications (2) ⇒ (1), (3) ⇒ (1), (4) ⇒ (2) and (4) ⇒ (3) are trivially true. To show (1) ⇒ (2), let δ, ǫ > 0 be artitrary but fixed. For each countable subclass C ′, choose the smallest value of sample complexity s = s(C ′, ǫ, δ). The function C ′ 7→ s(C ′, ǫ, δ) is monotone under inclusions: if C ′ ⊆ C ′′, then s(C ′, ǫ, δ) ≤ s(C ′′, ǫ, δ). If C ′n is a sequence of countable classes, then the union ∪∞n=1C ′ n is a countable class, whose sample complexity value bounds from above s(C ′, ǫ, δ), n = 1, 2, . . .. Thus, the function C\n′ 7→ s(C ′, ǫ, δ) for δ, ǫ > 0 fixed is bounded on countable sets of inputs, and therefore bounded.\nNow assume (MA). It is enough to prove (2) ⇒ (4). This is done by a transfinite induction on the cardinality κ = |C ′|, which never exceeds 2ℵ0 because C ′ consists of Borel subsets of a standard Borel domain. For κ = ℵ0 there is nothing to prove. Else, represent C as a union of an increasing transfinite chain of concept classes Cα, α < κ, for each of which the statement of (4) holds. For every ǫ > 0 and n ∈ N, the set\n{σ ∈ Ωn : supC∈C |µn(σ)(C) − µ(C)| < ǫ}\n= ⋂\nα<κ\n{ σ ∈ Ωn : supC∈Cα |µn(σ)(C) − µ(C)| < ǫ }\nis measurable by Martin-Solovay’s Theorem 9. Given δ > 0 and n ≥ s(ǫ, δ, d), another application of the same result leads to conclude that for every µ ∈ P (Ω):\nµ⊗n {\nσ ∈ Ωn : sup C∈C |µn(σ)(C) − µ(C)| < ǫ\n}\n= µ⊗n\n(\n⋂\nα<κ\n{\nσ ∈ Ωn : sup C∈Cα |µn(σ)(C) − µ(C)| < ǫ\n}\n)\n= inf α<κ\nµ⊗n {\nσ ∈ Ωn : sup C∈Cα |µn(σ)(C) − µ(C)| < ǫ\n}\n≥ 1− δ,\nas required. Lemma 11: Let C be a concept class whose countable subclasses are uniform Glivenko–Cantelli with regard to a family of probability measures P . Let L be a learning rule for C with the property that for every C ∈ C , the set\nLC,n = {L(C ∩ σ) : σ ∈ Ωn} (8)\nhas cardinality strictly less than continuum. Under Martin’s Axiom, the rule L is probably approximately correct under P . The common sample complexity bound of countable subclasses of C becomes the sample complexity bound for the learning rule L.\nProof: Recall that 2ℵ0 is a regular cardinal, and thus admits no countable cofinal subset. Therefore, under the assumptions of Lemma, the cardinality of LC = ∪∞n=1L C,n is still strictly less than continuum. Applying now Lemma 10 and then Lemma 8, we conclude.\nThe following result establishes existence of learning rules with the required property.\nLemma 12: Let C be an infinite concept class on a measurable space Ω. Denote κ = |C | the cardinality of C . There exists a consistent learning rule L for C with the\nproperty that for every C ∈ C and each n, the set LC,n (cf. Eq. (8)) has cardinality < κ. Under MA the rule L satisfies the condition in Eq. (1).\nProof: Choose a minimal well-ordering of elements of C :\nC = {Cα : α < κ},\nand set for every σ ∈ Ωn and τ ∈ {0, 1}n the value L(σ, τ) equal to Cβ , where\nβ = min{α < κ : Cα ∩ σ = τ},\nprovided such a β exists. Clearly, for each α < κ one has\nL(σ,Cα ∩ σ) ∈ {Cβ : β ≤ α},\nwhich assures (8). Besides, the learning rule L is consistent. Fix C = Cα ∈ C , α < κ. For every β ≤ α define Dβ = {σ ∈ Ωn : C ∩ σ = Cβ ∩ σ}. The sets Dβ are measurable, and the function\nΩn ∋ σ 7→ µ(L(C ∩ σ)△ C) ∈ R\ntakes a constant value µ(C△Cα) on each set Dβ \\∪γ<βDγ , β ≤ α. Such sets, as well as all their possible unions, are measurable under MA by force of Martin–Solovay’s Theorem 9, and their union is Ωn. This implies the validity of Eq. (1) for L.\nLemma 11 and Lemma 12 lead to the following result. Theorem 13 (Assuming MA): Let C be a concept class consisting of Borel measurable subsets of a standard Borel domain Ω, and let P be a family of probability measures on Ω. Suppose that every countable subclass of C is uniform Glivenko–Cantelli with regard to P . Then the concept class C is PAC learnable under P . In addition, there exists a common sample complexity bound for countable subclasses of C , and any such bound gives a sample complexity bound for PAC learnability of C .\nFinally, we can deduce our main result.\nProof of (2)⇒(1) in Theorem 2\nThe implication follows from Theorem 13 with P = P (Ω) and the common complexity bound (4)."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "As a footnote to the fundamental theorem of statistical learing, we have proved that in the presence of a mild settheoretic axiom (Martin’s Axiom), PAC learnability of a concept class C is equivalent to finiteness of VC dimension of C , without any extra assumptions on the measurability of the class C . The price to pay is giving up consistent PAC learnability, as well as constructive choice of a learning rule.\nIt would be interesting to know to what extent the results remain true in the usual ZFC model of set theory. In particular, can an example of a concept class C on a standard Borel domain which has finite VC dimension and still is not cosistently PAC learnable, be constructed without any additional set-theoretic axioms?"
    } ],
    "references" : [ {
      "title" : "Chervonenkis, On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "V.N. Vapnik", "A.Ya" ],
      "venue" : "Theory Probab. Appl. 16,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1971
    }, {
      "title" : "Learnability and the Vapnik-Chervonenkis dimension",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth" ],
      "venue" : "Journal of the ACM, 36(4) ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Uniform Central Limit Theorems",
      "author" : [ "R.M. Dudley" ],
      "venue" : "Cambridge Studies in Advanced Mathematics, 63, Cambridge University Press, Cambridge",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Convergence of Stochastic Processes",
      "author" : [ "D. Pollard" ],
      "venue" : "Springer-Verlag, New York",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Empirical processes",
      "author" : [ "M. Durst", "R.M. Dudley" ],
      "venue" : "Vapnik–Chervonenkis classes, and Poisson processes, Prob. and Math. Statistics 1 ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Axioms of symmetry: throwing darts at the real number line",
      "author" : [ "C. Freiling" ],
      "venue" : "J. Symbolic Logic 51 ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Consequences of Martin’s Axiom",
      "author" : [ "D.H. Fremlin" ],
      "venue" : "Cambridge Tracts in Mathematics, 84. Cambridge University Press, Cambridge",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Set Theory",
      "author" : [ "T. Jech" ],
      "venue" : "Academic Press, New York–London",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Set Theory",
      "author" : [ "K. Kunen" ],
      "venue" : "North-Holland, Amsterdam",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "PAC learnability of a concept class under non-atomic measures: a problem by Vidyasagar",
      "author" : [ "V. Pestov" ],
      "venue" : "in: Proc. 21st Intern. Conference on Algorithmic Learning Theory ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning and Generalization",
      "author" : [ "M. Vidyasagar" ],
      "venue" : "with Applications to Neural Networks, 2nd Ed., Springer-Verlag",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A few notes on statistical learning theory",
      "author" : [ "S. Mendelson" ],
      "venue" : "in: S. Mendelson, A.J. Smola, Eds., Advanced Lectures in Machine Learning, LNCS 2600, Springer",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Some special Vapnik–Chervonenkis classes",
      "author" : [ "R.S. Wenokur", "P.M. Dudley" ],
      "venue" : "Discrete Math. 33 ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1981
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It is in this form that the theorem is usually stated in textbooks on the subject, see [1], [2].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "It is in this form that the theorem is usually stated in textbooks on the subject, see [1], [2].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "One such assumption is that of C being image admissible Souslin: the class C can be parametrized with elements of the unit interval so that pairs (x, t), x ∈ Ct, t ∈ [0, 1] form an analytic subset of Ω × [0, 1] [3].",
      "startOffset" : 166,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "One such assumption is that of C being image admissible Souslin: the class C can be parametrized with elements of the unit interval so that pairs (x, t), x ∈ Ct, t ∈ [0, 1] form an analytic subset of Ω × [0, 1] [3].",
      "startOffset" : 204,
      "endOffset" : 210
    }, {
      "referenceID" : 2,
      "context" : "One such assumption is that of C being image admissible Souslin: the class C can be parametrized with elements of the unit interval so that pairs (x, t), x ∈ Ct, t ∈ [0, 1] form an analytic subset of Ω × [0, 1] [3].",
      "startOffset" : 211,
      "endOffset" : 214
    }, {
      "referenceID" : 1,
      "context" : "Another measurability assumption, more difficult to state, is that of a well-behaved class C [2].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "In particular, Theorem 1 holds for every countable class C or, more generally, for every universally separable class [4].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : "An example of a concept class C of Vapnik– Chervonenkis (VC) dimension one which is not uniform Glivenko–Cantelli was constructed by Durst and Dudley [5], and a further modification of this example, also of VC dimension one, fails consistent PAC learnability [2].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "An example of a concept class C of Vapnik– Chervonenkis (VC) dimension one which is not uniform Glivenko–Cantelli was constructed by Durst and Dudley [5], and a further modification of this example, also of VC dimension one, fails consistent PAC learnability [2].",
      "startOffset" : 259,
      "endOffset" : 262
    }, {
      "referenceID" : 5,
      "context" : "This example has been constructed under Continuum Hypothesis (CH), which is arguably not a natural assumption in a probabilistic context [6].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "In particular, Martin’s Axiom follows from the Continuum Hypothesis (CH), but it is also compatible with the negation of CH, and in fact it is namely the combination MA+¬CH that is really interesting [7], [8], [9].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 7,
      "context" : "In particular, Martin’s Axiom follows from the Continuum Hypothesis (CH), but it is also compatible with the negation of CH, and in fact it is namely the combination MA+¬CH that is really interesting [7], [8], [9].",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 8,
      "context" : "In particular, Martin’s Axiom follows from the Continuum Hypothesis (CH), but it is also compatible with the negation of CH, and in fact it is namely the combination MA+¬CH that is really interesting [7], [8], [9].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "Of course it is only the implication (2)⇒(1) that needs proving, because (1)⇒(2) is a well-known classical result from [2] which does not require any assumptions on C .",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "We review a precise formal setting for learnability, after which we proceed to analysis of a counter-example from [5], [2].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "We review a precise formal setting for learnability, after which we proceed to analysis of a counter-example from [5], [2].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "The present approach goes back to present author’s earlier work [10], but the results are new and have never been stated explicitely before.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "Measures on Ω mean Borel probability measures, that is, countably additive functions on A with values in the unit interval [0, 1], having the property μ(Ω) = 1.",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "Chapter 7 in [11].",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : "In this case, one also says that C has the property of uniform convergence of empirical measures (UCEM property) (with regard to P) [11].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "More precisely, every distribution-free PAC learnable class has finite VC dimension (it was proved in [2], Theorem 2.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "[11], Lemma 7.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "The following is a typical (and far from being optimal) such estimate, which can be deduced, for instance, along the lines of [12]:",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "Example 3 (Durst and Dudley [5], Proposition 2.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Let Ω be an uncountable standard Borel space, that is, up to an isomorphism, a Borel space associated to the unit interval [0, 1].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : ", the Lebesgue measure on [0, 1]).",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "See also [13], p.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : "314; [3], pp.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "[2], p.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "For the proof and more on MA, see [9], Theorem 2.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "21, or [7], or [8], pp.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 7,
      "context" : "21, or [7], or [8], pp.",
      "startOffset" : 15,
      "endOffset" : 18
    } ],
    "year" : 2011,
    "abstractText" : "A fundamental result of statistical learnig theory states that a concept class is PAC learnable if and only if it is a uniform Glivenko–Cantelli class if and only if the VC dimension of the class is finite. However, the theorem is only valid under special assumptions of measurability of the class, in which case the PAC learnability even becomes consistent. Otherwise, there is a classical example, constructed under the Continuum Hypothesis by Dudley and Durst and further adapted by Blumer, Ehrenfeucht, Haussler, and Warmuth, of a concept class of VC dimension one which is neither uniform Glivenko– Cantelli nor consistently PAC learnable. We show that, rather surprisingly, under an additional set-theoretic hypothesis which is much milder than the Continuum Hypothesis (Martin’s Axiom), PAC learnability is equivalent to finite VC dimension for every concept class.",
    "creator" : "LaTeX with hyperref package"
  }
}