{
  "name" : "1705.00522.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Regularized Residual Quantization: a multi-layer sparse dictionary learning approach",
    "authors" : [ "Sohrab Ferdowsi", "Slava Voloshynovskiy", "Dimche Kostadinov" ],
    "emails" : [ "Dimche.Kostadinov}@unige.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Regularized Residual Quantization: a multi-layer sparse dictionary learning approach\nSohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov Department of Computer Science, University of Geneva, Switzerland\n{Sohrab.Ferdowsi, svolos, Dimche.Kostadinov}@unige.ch\nI. INTRODUCTION\nQuantizing the residual errors from a previous level of quantization has been considered in signal processing for different applications, e.g., image coding. This problem was extensively studied in the 80’s and 90’s (e.g., see [1] and [4]). However, due to strong over-fitting, its efficiency was limited for more modern applications with larger scales. In practice, it was not feasible to train more than a couple of layers. Particularly at high dimensions, the codebooks learned on a training set were not able to quantize a statistically similar test set.\nInspired by an insight from rate-distortion theory, we introduce an effective regularization for the framework of Residual Quantization (RQ), making it capable to learn multiple layers of codebooks with many stages. Moreover, the introduced framework effectively deals with high dimensions making it feasible to go beyond patch level processing and deals with entire images. The proposed regularization makes use of the problem of optimal rate allocation for asymptotic case of Gaussian independent sources, which is reviewed next."
    }, {
      "heading" : "II. BACKGROUND: QUANTIZATION OF INDEPENDENT GAUSSIAN SOURCES",
      "text" : "Given n independent Gaussian sources Xj’s each with variance σ2j distributed as Xj∼N (0, σ2j ), the optimal rate allocation from the rate-distortion theory is derived for this setup as (Ch. 10 of [2]):\nDj = { γ, if σ2j > γ, σ2j , if σ 2 j < γ,\n(1)\nwhere γ should be chosen to guarantee that ∑n j=1Dj = D. Hence, the optimal codeword variance σ2Cj is soft-thresholding of σ 2 j :\nσ2Cj = ( σ2j − γ )+ = { σ2j − γ, if σ2j > γ, 0, if σ2j < γ.\n(2)\nThis means that sources with variances less than γ should not be assigned any rate at all. We next incorporate this phenomenon for codebook learning and enforce it as a regularization for the codebook variances. This, in fact, will be an effective way to reduce the gap between the train and test distortion errors. Moreover, the inactivity of the dimensions with variances less than γ will also lead to a natural sparsity of codewords, which lowers the computational complexity."
    }, {
      "heading" : "III. THE PROPOSED APPROACH: RRQ",
      "text" : "Instead of the standard K-means used in RQ, we first propose its regularized version and then use it as the building-block for RRQ.\nA. VR-Kmeans algorithm\nAfter de-correlating the data points, e.g., using the pre-processing proposed in Fig. 2, and gathering them in in columns of X with σ2j at each dimension, define S , diag([σ2C1 , · · · , σ 2 Cn ]) from Eq. 2. For codebook C, to regularize only the diagonal elements of CCT , define\nPj with all elements as zeros except at P(j,j)=1. We formulate the variance-regularized K-means algorithm with parameter λ as:\nminimize C,A\n1 2 ||X− CA||2F + 1 2 λ|| n∑ j=1 PjCC TPj − S||2F ,\ns.t. ||αi||0 = ||αi||1 = 1. (3)\nLike the standard K-means algorithm, we iterate between fixing C and updating A, and then fixing A and updating C.\nFix C, update A: Exactly like the standard K-means. Fix A, update C: Eq. 3 can be re-written as:\nminimize Tr C\n[ −XATCT + 1\n2 CAATCT\n+ 1\n2 λ( n∑ j=1 PjCC TPj)(CC T − 2S) ] .\n(4)\n∑n j=1 PjCC\nTPj , and due to its structure AAT , diag([a1, · · · , ak]) are diagonal. Therefore Eq. 4 will reduce to minimizing independent sub-problems at each (active) dimension:\nminimize c(j)\n[ − z(j)T c(j) + 1\n2\n( a1c1(j) 2 + · · ·+ akck(j)2 )\n+ 1\n2 λ||c(j)||2(||c(j)||2 − 2σ2Cj )\n] ,\n(5)\nwhere Z , XAT = [z(1), · · · , z(n)]T . These independent problems can be solved easily using the Newton’s algorithm, for which the derivation of the required gradient and Hessian is straightforward."
    }, {
      "heading" : "B. Regularized Residual Quantization (RRQ) algorithm",
      "text" : "For a fixed number of centroids K(l) at layer l and D(l−1)j the distortion of the previous stage of quantization for each dimension, the\nRRQ first specifies γ∗ = argmin γ\n( | log2K(l)− ∑ j∈Aγ 1 2 log+2 D (l−1) j γ | )\nfollowed by calculation of an active set of dimensions A(l)γ∗ = {j : 1 6 j 6 n|σ2j > γ∗}. The algorithm then continues with quantizing the residual of stage l − 1 with the VR-Kmeans algorithm described above, until a desired stage L which can be chosen based on distortion constraints or an overall rate budget allowed."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "Fig. 1 and Table I compare the performance of VR-Kmeans with K-means in quantization of high-dimensional variance-decaying independent data. In fact, in many practical cases, the correlated data behaves similarly after an energy-compacting and de-correlating transform. As is seen in this figure, the VR-Kmeans regularizes the variance resulting in a reduced train-test distortion gap.\nFig. 3 demonstrates the performance of the RRQ in superresolution of similar images. It is clear from this figure that the highfrequency content lost in down-sampling can be reconstructed from a multi-layer codebook learned from face images with full resolution.\nar X\niv :1\n70 5.\n00 52\n2v 1\n[ cs\n.L G\n] 1\nM ay\n2 01\n7"
    } ],
    "references" : [ {
      "title" : "Advances in residual vector quantization: a review",
      "author" : [ "C.F. Barnes", "S.A. Rizvi", "N.M. Nasrabadi" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Elements of Information Theory 2nd Edition",
      "author" : [ "T. Cover", "J. Thomas" ],
      "venue" : "Wiley-Interscience, 2 edition,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Acquiring linear subspaces for face recognition under variable lighting",
      "author" : [ "K. Lee", "J. Ho", "D. Kriegman" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Image coding using vector quantization: a review",
      "author" : [ "N.M. Nasrabadi", "R.A. King" ],
      "venue" : "IEEE Transactions on Communications,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", see [1] and [4]).",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : ", see [1] and [4]).",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : "10 of [2]):",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "3: Super-resolution using RRQ on the CroppedYale [3] set with L = 100 layers with K = 256 centroids each: After preprocessing as proposed in Fig.",
      "startOffset" : 49,
      "endOffset" : 52
    } ],
    "year" : 2017,
    "abstractText" : "Quantizing the residual errors from a previous level of quantization has been considered in signal processing for different applications, e.g., image coding. This problem was extensively studied in the 80’s and 90’s (e.g., see [1] and [4]). However, due to strong over-fitting, its efficiency was limited for more modern applications with larger scales. In practice, it was not feasible to train more than a couple of layers. Particularly at high dimensions, the codebooks learned on a training set were not able to quantize a statistically similar test set. Inspired by an insight from rate-distortion theory, we introduce an effective regularization for the framework of Residual Quantization (RQ), making it capable to learn multiple layers of codebooks with many stages. Moreover, the introduced framework effectively deals with high dimensions making it feasible to go beyond patch level processing and deals with entire images. The proposed regularization makes use of the problem of optimal rate allocation for asymptotic case of Gaussian independent sources, which is reviewed next.",
    "creator" : "LaTeX with hyperref package"
  }
}