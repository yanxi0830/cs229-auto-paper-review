{
  "name" : "1404.4108.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Lifelong Learning of Discriminative Representations",
    "authors" : [ "Ouais Alsharif", "Philip Bachman", "Joelle Pineau" ],
    "emails" : [ "ouais.alsharif@mail.mcgill.ca,", "phil.bachman@gmail.com,", "jpineau@cs.mcgill.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The machine learning community has long been aware of the importance of lifelong learning (Thrun 1995; 1996b; Carlson et al. 2010). A lifelong learner is a persistent agent facing a continuous stream of problems which, ideally, should be able to use information extracted from past tasks to reduce the amount of information required to generalize well on future tasks. Lifelong learning is distinct from multitask learning (Caruana 1997) in the sense that it focuses on improving generalization performance for new tasks with minimal amounts of training data, rather than focusing on improving generalization performance for a fixed collection of tasks defined a priori. Intuitively, a lifelong learner tries to propagate useful information forward in time, from past tasks to future tasks, while a multitask learner tries to share information a-temporally, among a static collection of tasks.\nMost of the algorithms proposed thus far for multitask and lifelong learning, e.g. (Kumar and Daume III 2012;\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nBaxter 2000; Saha et al. 2011; Ando and Zhang 2005), rely heavily on an assumption that seems to violate the premises motivating lifelong learning. We call this the assumption of task correspondence. To assume task correspondence, our envisioned machine learning service provider, upon being faced with a new task, must have access to a mapping between labels for the new task and labels encountered in previous tasks1. When such a mapping is available, the service provider can directly transfer label-specific models based on information aggregated over previous tasks to the new task, possibly adapting the transferred models using the labeled data from the new task. For example, in the standard supervised learning setting a labeled set of observations is used to train a model and the trained model is then used to infer labels for new observations. This corresponds to a lifelong learning problem with one past task, defined by the labeled training data, where all labels for future tasks are assumed to be in one-to-one correspondence with the training labels.\nTask correspondence is a strong assumption which may be inapplicable in some practical scenarios. For example, imagine a service provider whose service is face recognition. Clients present the provider with a small set of faces labelled as interesting and not interesting. The clients then request the provider to construct a hypothesis that allows them to recognize other face images they might find interesting. In this context, faces that are interesting to one person need not be interesting to another. Hence, task correspondence cannot be assumed. Another example is one where our ML service provider is a website which directs users to available information (e.g. blog posts, news articles, etc.) based on their observed history of likes/dislikes for information they’ve already seen. In this case, new users may continually join the site with only a minimal amount of like/dislike information made available a priori, and new information may continually become available. Intuitively, one might assume that there exists some “coarsest-common-partition (CCP)” of the set of available information sources such that the users’ binary partitions of information space into things they like or dislike can be approximated by binary partitions that don’t further divide any elements of the CCP. If our ML service provider could learn about the structure of the CCP from the likes and dislikes of its users, then its performance could\n1We use “label” for categorical and continuous target outputs.\nar X\niv :1\n40 4.\n41 08\nv1 [\ncs .L\nG ]\n2 4\nFe b\n20 14\nimprove dramatically, by providing better recommendations to existing users and by more rapidly accommodating new users and new information sources.\nIn this paper, we propose a novel model for lifelong learning which doesn’t require information about task correspondence. Our model is motivated by an idea adapted from (Baxter 2000): a lifelong learner should use information from past tasks to learn a representation that is simple enough to allow for effective generalization on new tasks with minimal training data, yet flexible enough to accommodate a relatively diverse set of tasks. In Section 2, we describe the lifelong learning problem more formally and further discuss the benefits of not relying on task correspondence. In Section 3, we briefly discuss some existing approaches to multitask and lifelong learning. In Section 4, we show how looking at lifelong learning as a representation learning problem leads to an intuitive, efficient algorithm which we call Lifelong Learning of Discriminative Representations. Section 5 presents empirical results showing how our algorithm fares in three different contexts: as a supervised representation learner, as a multitask learner, and as a lifelong learner."
    }, {
      "heading" : "2 The Lifelong Learning Problem",
      "text" : "We now convey our formulation of lifelong learning through a sequence of definitions. We begin by defining the environment E = (Q,X ), which comprises an input domain X and a task distributionQ. Q is a distribution over tasks Ti, where each task Ti = (Yi,Di,Li,Gi) comprises: • An output domain Yi. • A distribution Di over X × Yi. • A non-negative loss function Li : Yi × Yi → R+. • A generalization functional Gi(f), for all functions f : X → Yi, with Gi(f) = E(x,y)∼DiLi(f(x), y).\nFor the environments we consider, X = Rd for some finite d > 0. For classification tasks, Yi is the discrete space of relevant labels and Li(y, y′) is the indicator function I{y 6= y′}. For regression tasks, Yi = R and Li(y, y′) is the squared error (y − y′)2. For classification tasks, the generalization functional Gi(f) gives the expected misclassification rate of the “classifier” f w.r.t. Di. For regression tasks, the generalization functional Gi(f) gives the expected squared error of the “regressor” f , w.r.t. Di.\nLoosely speaking, a lifelong learner is a persistent machine learning agent faced with an environment E and tasked with producing hypotheses ĥi : X → Yi for any tasks Ti it encounters, so as to minimize ETi∼QGi(ĥi). To produce a hypothesis ĥi for task Ti, the agent first receives an msample (Xmi , Y m i ) = {(xj , yj)}mj=1 of training observations drawn from Di. The agent then applies an algorithm Ai to the m-sample, such that ĥi = Ai(Xmi , Y mi ) and:\nAi(Xmi , Y mi ) =\narg min h∈HAi\n1\nm ∑ (xj ,yj)∈(Xmi ,Ymi ) LAi (h(xj), yj) +RAi (h),\nwhere HAi gives the hypothesis space searched by Ai, LAi gives the loss function minimized by Ai, and RAi gives any regularization terms used by Ai to bias the search overHAi . We let Ai minimize a surrogate loss LAi , as the natural task loss Li may provide an intractable optimization objective.\nUsing the definitions presented thus far, the lifelong learning objective we consider can be written as:\nminimize θ∈Θ\nETi∼QE(Xmi ,Ymi )∼DiGi(A θ i (X m i , Y m i )), 2 (1)\nin which we have linked the per-task algorithms in some way through shared “structure” determined by the θ ∈ Θ. Sections 3 and 4 will give more detail on how one might define useful structure to be shared among the per-task estimation problems. Loosely speaking, the objective in (1) measures the ability of the structurally-biased per-task algorithms Aθi to find hypotheses ĥi = Aθi (Xmi , Y mi ) that generalize well w.r.t. Gi, in a small-sample setting."
    }, {
      "heading" : "3 Related Works",
      "text" : "The key motivation for lifelong learning is the idea that information extracted from past tasks may be used to improve the ĥi estimated for future tasks. A similar motivation (adapted for an a-temporal setting) motivates most work on multitask learning. Lifelong learning has deep connections with multitask learning, and can be seen as an online form of multitask learning where the goal is to generalize to new samples and new tasks, not just to new samples within existing tasks. In (Thrun 1996a), the authors provide a listing of multitask and lifelong learning methods. The listed methods all rely on some structure through which information is shared among tasks. The ubiquity of the shared-structure approach continues through to the present-day. The various structures used to induce information sharing include neural network-based feature transformations (Baxter 1995), learned distance metrics (Thrun 1996b), Bayesian priors (Xue et al. 2007), and many others.\nSome recent work has focused on creating efficient, online multitask learners. In particular, (Ruvolo and Eaton 2013) presents an algorithm for online multitask learning composed of two parts: a linear function approximator fw(x) = w\n>x where w ∈ Rd, and a dictionary L ∈ Rd×k containing k d-dimensional basis functions. Their algorithm, ELLA, relies on representing the parameters of fw for a new task using a sparse code over the dictionary elements, i.e. fw(x) = (Ls)>x, where ||s||1 is small and we assume w = Ls. While ELLA scales well with an increasing number of tasks, it doesn’t scale well as the dimension, d, of the function approximator increases. This results from the way ELLA updates the dictionary L after seeing each new task, which costs O(d3k2). Additionally, the linearity of fw assumed by ELLA, as presented in (Ruvolo and Eaton 2013), limits the space of tasks where ELLA might perform well to tasks on which a linear approximator could conceivably perform well given enough training observations.\n2While we always use m for the number of samples available as input to each Ai, in practice m will differ across tasks.\nIn (Saha et al. 2011), the authors also propose an online method for multitask learning, called OMTL, based on the perceptron algorithm. For OMTL, the target tasks are limited to classification tasks. It also does not scale well as the number of tasks increases since it must maintain a T × T matrix when presented with T tasks. Though, as pointed out by (Ruvolo and Eaton 2013), the perceptron algorithm tends to underperform other linear methods (e.g, logistic regression). In (Simm, Sugiyama, and Kato 2011), the authors proposed an efficient method for multitask learning, however their method is not an online method, making it unsuitable for the scenario considered in this paper.\nOn the theoretical side, (Baxter 2000) provides PAC bounds for a setting very similar to the one described in Section 2. Though motivated by lifelong learning, Baxter solves a multitask learning problem using a two-part model comprising a trainable feature extractor fθ, parameterized by θ ∈ Θ, shared by all tasks and t trainable task-specific function approximators {h1, ..., hT }. These function approximators receive the output of fθ as their inputs. Given anm-sample (Xmt , Y m t ) for each task, the objective used by Baxter’s approach can be written in our notation as:\nminimize θ∈Θ, {h1,...hT } T∑ t=1 m∑ j=1 LAt (ht(xj), yj), (2)\nin which LAt gives the surrogate loss function chosen for each task, to serve as an optimization-friendly proxy for Lt.\nWhen, as in his Baxter’s work, the function approximator fθ is a neural network and the per-task function approximators ht are linear approximators estimated to mimize, e.g., a logistic regression loss LAt , Baxter’s algorithm still constitutes the standard algorithm for multitask learning with a neural network. Modern examples of this approach include (Collobert and Weston 2008) and the wining entry in the Merck Molecular Activity Challenge in 2012.\nWe note that among the algorithms we’ve just mentioned, only ELLA (Ruvolo and Eaton 2013) can operate without the task correspondence assumption."
    }, {
      "heading" : "4 Method",
      "text" : "We now present the model we propose for addressing the objective in (1), which has the following key properties:\n1. Flexibility: our model is modular, easily accommodating different feature extractors and function approximators.\n2. Online: tasks can be presented to our model sequentially and need not be remembered in perpetuity.\n3. Scalability: our model scales well with respect to: the number of samples per task, the number of tasks, the dimension of the input domain, and the dimension of the extracted-feature domain.\n4. Task Correspondence: is not required by our model. The general idea of our model is to wrap a parametric feature extractor f and an unbounded collection of trainable function approximators {..., ht, ...} with a “meta” algorithm that tries to make the output of f more effective as input to the\nalgorithmAt used to train ht for task Tt3. Using the notation defined in Section 2, our objective is given by:\nminimize θ∈Θ\nETt∼QE(Xmt ,Ymt )∼DtGt (At(fθ(X m t ), Y m t )) ,\n(3) where fθ represents setting the parameters of the parametric function approximator f to θ. This objective is an instance of the objective in (1) in which the structure sharing among the per-task algorithms, i.e. the Aθi in (1), is effected by using the same parameterized feature extractor fθ to preprocess inputs to each At.\nWhere our method differs from existing methods is that we explicitly minimize an empirical proxy for the expected per-task small-sample generalization errors given by Gt (At(f(Xmt ), Y mt )). In contrast, typical approaches to multitask learning simultaneously learn a feature extractor fθ and a collection of task-adapted functions ĥt for a fixed set of tasks {T1, ..., Tn}. This approach seeks an f such that there exist functions ĥt with small error, w.r.t. LAt , on the training sets available for each Tt. In particular, these approaches do not explicitly account for the small-sample learnability of the functions ĥt4. This shift in perspective informs the structure of our method and produces improved empirical performance in practically-relevant scenarios.\nGiven T m-samples {(Xm1 , Y m1 ), ..., (XmT , Y mT )} drawn from its environment, our algorithm optimizes the following empirical approximation of (3):\nminimize θ∈Θ T∑ t=1 Ĝt(At, (fθ(Xmt ), Y mt )), (4)\nwhere the we use the “empirical generalization functional” Ĝt. To define Ĝt, we first define a process for sampling a pseudo-training/validation set pair (trnt , va m−n t ) from the m-sample (Xmt , Y m t ) available for Tt. We sample the pseudo-train/validate split (trnt , va m−n t ) by randomly sampling n < m observations (xi, yi) for trnt , from (X m t , Y m t ), and then let vam−nt contain the remaining m − n observations in (Xmt , Y m t ). Now, we compute Ĝt as follows:\nĜt(At, (fθ(Xmt , Y mt ))) =\nE(trnt ,vat)∼(Xmt ,Ymt )  ∑ (xj ,yj)∈vat LAt (ĥnt (fθ(xj)), yj)  , in which ĥnt = At(trnt ) indicates the hypothesis produced by applying algorithm At to the pseudo-training set trnt .\nAlgorithm 1 describes concretely how our algorithm operates. Our algorithm requires sampling a potentially large number of tasks from the relevant task distributionQ. While\n3The collection of function approximators may be unbounded, but we select each algorithm/approximator pairAt/ht from a finite set of methods, e.g. linear logistic regression for classification tasks and linear least-squares regression for regression tasks.\n4Here, learnability roughly means the probability that algorithm At finds ĥt ∈ HAt with small Gt(ĥt) given the small m-sample (f(Xmt ), Y m t ) used in the search overHAt .\nAlgorithm 1 LLDR Input: Source environment E = (X ,Q), pseudo-training batch size n, updates per task sample K, learning rate γ. while true do\nSample task Tt ∼ Q, with m-sample (Xmt , Y mt ) Sample train/validate split (trnt , va m−n t ) ∼ (Xmt , Y mt ) Apply At to trnt to get ĥnt . for k = 1 to K do\nSample a minibatch ṽant from va n t . Define (X̃, Ỹ ) such that ṽant = (X̃, Ỹ ). Let θ := θ − γ∇θLAt (ĥnt (fθ(X̃)), Ỹ )\nend for end while\nthis is reasonable in the lifelong learning setting we are concerned with, where the number of available tasks potentially exceeds computational capacity to process them, it may be unreasonable in a more restricted multitask learning setting where the number of available tasks is relatively small. When the number of available tasks is small, we approximate sampling a random task Tt from Q by sampling Tt at random from the fixed set of available tasks."
    }, {
      "heading" : "5 Evaluation",
      "text" : "We evaluate our method in three different contexts:\n1. As a representation learner: these experiments test if our algorithm can learn representations that are well-suited to function approximation in the small-sample setting.\n2. As a multitask learner: these experiments show how our algorithm compares to state-of-the-art multitask learning algorithms on common multitask learning benchmarks.\n3. As a lifelong learner: these experiments show how our algorithm performs at lifelong learning with a large number of training tasks and a large number of testing tasks that are distinct from the training tasks.\nExperiments in scenario 1 and 3 have three phases:\n1. Representation learning: algorithms encounter streams of tasks with which to train shared inter-task structure θ.\n2. Function approximation: each algorithm is presented with a small sample of labeled data from a new task and a function approximator is trained on the labeled data, biased by the shared structure θ learned in phase 1.\n3. Approximator testing: the function approximators from phase 2 are tested for generalization performance on data not seen in phase 1 or 2.\nMultitask learning experiments follow the standard procedures in the relevant literature discussed below."
    }, {
      "heading" : "Representation Learning Experiments",
      "text" : "We now compare LLDR to Baxter’s method, which is a standard approach to multitask learning with neural networks, e.g (Collobert and Weston 2008). Experiments in this section use the well-known MNIST dataset (LeCun et al. 1998).\nMNIST is a dataset of handwritten character images. The dataset contains 60k training samples and 10k test samples. We preprocess each sample with contrast normalization by transforming it to zero mean and unit variance.\nFor our method and Baxter’s, we use multinomial logistic regression for function appproximation. For the parametric feature extractor, we use two layer neural networks with 100 nodes in each hidden layer, with rectified linear activation functions, which are common in the deep learning community (Glorot, Bordes, and Bengio 2011). We train the logistic regressions and neural network with standard gradient descent and momentum. We optimized the learning rates over a validation set, using learning rates of 0.01/0.1 for the first experiment and 0.1/0.1 for the second experiment our method/Baxter’s method respectively. For our algorithm, we used K = 50 updates per task presentation and pseudo-training/validation sets of size 200/200. Tasks were sampled uniformly. The batch samplers were created such that they would sample an equal number of samples of every class for every batch. We train our algorithm for 1000 batches and Baxter’s for 50k such that they perform the same number of updates to their feature extractors. Neither method appeared to improve with more updates.\nIn experiment 1, we split the MNIST dataset into three disjoint sets, such that only one set is used in each phase of the evaluation procedure. As noted earlier, the first part is used for training the feature extractor, the second is for training a new function approximator on a new task and the third is for measuring generalization performance, with sizes of 50k, 10k and 10k respectively. We train our method and Baxter’s on just the standard 10-way MNIST classification task. At test time, we select a few samples from the function approximator training set and train a new logistic regression\n(for the 10-way all-digits problem) using inputs transformed by the feature extractor learned by each method in the first phase. We then measure performance with examples from the testing set. Results for each function approximator training sample size are averaged over 10 different initializations of the neural net and 10 different sample sets from the function approximator training set.\nExperiment 2, tested algorithms on their ability to learn representations that allow better performance on a task they didn’t see during training. We denote classes in MNIST as C. For this experiment we partition the MNIST dataset classwise into two sets Ctrain and Ctest. The training tasks are all k-way classification problems such that no problem contains more than one class from Ctest. Performance is measured on the |Ctest|-way classification problem over the classes in Ctest. For this experiment we set k = 5, |Ctrain| = 6, and |Ctest| = 4. This lead to 66 training tasks and a single testing task. For each size of the function approximator training set, we averaged the results over 10 initializations of the neural network, each with different classes in Ctest. For each network initialization, we averaged the results over 10 different samples from the function approximator training sets.\nIn the first experiment, as shown by Figure 1a, LLDR significantly outperforms Baxter’s method, especially when the number of samples used during function approximator training is small. Thus, our algorithm is learning representations allow one to learn better-generalizing function approximators with fewer samples. In the second experiment, Figure 1b shows that LLDR still maintains an advantage over Baxter’s method in the small-sample setting. However, this advantage seems to disappear when the number of available samples increases, indicating that our algorithm may not be particularly suited for regions where the number of samples available for new tasks is large and the number of training tasks is small. However, we note that Baxter’s method was permitted to assume task correspondence during training, which slightly disadvantages our algorithm."
    }, {
      "heading" : "Multitask Learning Experiments",
      "text" : "This section examines how our algorithm fares at multitask learning, where the number of training tasks is fixed and testing tasks are the same as training tasks. We compare our algorithm to state-of-the-art multitask learning algorithms on datasets that are common in the multitask learning community. We empirically show that, even though our model was not made for multitask learning and does not assume task correspondence, its compares favorably to state-of-the-art multitask learners on these benchmarks. We compare LLDR to three multitask learning algorithms: ELLA (Ruvolo and Eaton 2013), GOMTL (Kumar and Daume III 2012), and OMTL (Saha et al. 2011), and standard single task learning (STL), on the landmine (Xue et al. 2007) and London schools datasets (Kumar and Daume III 2012)."
    }, {
      "heading" : "Datasets",
      "text" : ""
    }, {
      "heading" : "Landmine Detection",
      "text" : "This dataset is a binary classification dataset (mine vs. no mine) that contains ∼ 15k points divided unevenly among\n29 regions with 9 features per sample. The features are four moment-based features, three correlation-based features, one energy-ratio feature and one spatial variance feature. Similar to (Ruvolo and Eaton 2013), we treat every region as a different task."
    }, {
      "heading" : "London Schools Data",
      "text" : "This dataset is a regression dataset which contains information about ∼ 15k students, divided unevenly among 139 schools, and their examination scores. The features for this task are as follows: four school-specific categorical variables and three student-specific categorical variables. Similar to (Ruvolo and Eaton 2013) and (Kumar and Daume III 2012), we encode the categorical variables as one-hot binary features and add examination year and bias terms leading to 27 features per student."
    }, {
      "heading" : "Training Methodology",
      "text" : "We follow the methodology used by (Ruvolo and Eaton 2013). In which each task is split 50/50 between a training set and a testing set. For OMTL and GOMTL we use the results from (Ruvolo and Eaton 2013) which optimize the hyperparameters for each method on a validation set. For ELLA we optimized the hyperparameters on a validation set. For LLDR, we optimized the learning rates on a validation set. LLDR’s general structure was as for the representation learning experiments, but with logistic regression replaced by linear regression for the London schools task. For the landmine task, performance is measured as the mean Area Under Curve (AUC) for OMTL and GOMTL in addition to accuracy for ELLA and LLDR5. For the London schools task, performance is measured as the mean RMSE. Despite the class-bias in the landmine dataset, we believe accuracy to be a more reasonable measure in our context than area under curve, as the methods use loss functions designed to minimize classification error, not AUC.\nAs table 1 shows, LLDR performs comparably to other state-of-the-art algorithms on these multitask learning benchmarks. We note that, due to the simplicity of these datasets in terms of task specification and observation dimensionality, most (reasonable) methods perform rather similarly, as there is little room to gain relative to basic single task learning. Our final experimental setting is intended to move past some of these limitations, to examine performance in a setting that more closely approaches the intent of lifelong learning."
    }, {
      "heading" : "Lifelong Learning Experiments",
      "text" : "We now examine how our algorithm performs in the context of lifelong learning with many training tasks, when tested on tasks not seen in training. We compare to ELLA (Ruvolo and Eaton 2013) and single-task multinomial logistic regression.\n5We were unable to reproduce results for OMTL and GOMTL, so we limit our accuracy-wise comparisons to ELLA and STL\nTable 1: Comparison of LLDR, ELLA, OMTL and GOMTL on the land mine and London schools datasets. The N/A indicates that OMTL does not work for regression problems. The AUC results for OMTL and GOMTL are the mean results from (Ruvolo and Eaton 2013). For the classification problems, the results are in area under curve and the parenthesized results are the mean accuracy. For regression, the results are in RMSE. STL is logistic regression for classification and random forests for regression.\nDataset Type OMTL GOMTL ELLA LLDR STL Land Mine Classification 0.63 0.78 0.776 (94.14) 0.765 (94.3) 76.24 (94.1) London Schools Regression N/A 10.10 10.20 10.466 11.06"
    }, {
      "heading" : "Datasets",
      "text" : "CIFAR 100 This dataset (Krizhevsky 2009) contains 50k training samples and 10k testing samples from 100 different classes. Samples are 32-by-32 color images. We preprocess the data by converting it to grayscale, applying contrast normalization and projecting onto the first 10 principal components. We would use higher dimensionality, but ELLA scales poorly in this regard."
    }, {
      "heading" : "20 Newsgroups",
      "text" : "This dataset contains 19k documents from 20 classes. We preprocessed the data by removing stop words, encoding the documents as bags of words and then representing them using LDA (Blei, Ng, and Jordan 2003) with 10 topics."
    }, {
      "heading" : "Training Methodology",
      "text" : "Similar to the representation learning experiments. Each dataset is divided into three different parts. The first part is used for training the feature extractor, the second part is used for training the function approximator during the testing phase and the third part is used for evaluating performance. The sizes of these datasets are (40,000, 10,000 , 10,000) and (9314, 2000, 7532) for the CIFAR 100 and the 20-Newsgroups dataset respectively. Both algorithms are trained on a set of 1000 k-way classification problems sampled randomly from the set of (|C| k ) problems where |C| is the number of classes available for the respective datasets and k = 5. Testing is done on 100 random k-way classification problems on the respective datasets with performance measured as the mean accuracy. Training problems are selected such that they don’t intersect with testing problems. Our algorithm had the same instantiation as in the representation learning section.\nAs depicted in Figure 2a/b, LLDR outperforms ELLA significantly on every sample size on the CIFAR 100 and 20-newsgroups datasets. We expect the gap between our algorithm and ELLA to widen when the observation dimension is less significantly reduced. Surprisingly, single-task logistic regression outperformed ELLA in this setting."
    }, {
      "heading" : "6 Discussion",
      "text" : "We introduced the problem of lifelong learning without task correspondence and presented a flexible framework that addresses this problem by optimizing a feature extractor whose output works well on future tasks. Our method allows us to learn new hypothesis functions efficiently with relatively few samples and allows us to avoid the assumption of task\ncorrespondence. The flexibility of the proposed framework is in its modularity, as the function approximator can be a classifier or a regressor. Also, the feature extractor can be changed to suit different domains and scenarios where possible feature extractors include convolutional neural networks, boosted stumps, etc. We tested our framework in three relevant scenarios. As a representation learner, a multitask learner and a lifelong learner. In the representation learning case, we empirically show that our model surpasses classical methods when trained on a single task and on multiple tasks in scenarios where the representation is utilized to perform classification on new tasks. In the multitask learning case, we show that our framework, despite not retaining information specific to any task it has seen before, performs comparably to state-of-the-art multitask learning algorithms. Further more, in the lifelong learning scenario, we empirically showed how our algorithm outperforms ELLA on two real world datasets: CIFAR 100 and 20 newsgroups."
    } ],
    "references" : [ {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "R.K. Ando", "T. Zhang" ],
      "venue" : "The Journal of Machine Learning Research 6:1817–1853.",
      "citeRegEx" : "Ando and Zhang,? 2005",
      "shortCiteRegEx" : "Ando and Zhang",
      "year" : 2005
    }, {
      "title" : "Learning internal representations",
      "author" : [ "J. Baxter" ],
      "venue" : "In Proceedings of the Eighth International Conference on Computational Learning Theory, 311–320. ACM Press.",
      "citeRegEx" : "Baxter,? 1995",
      "shortCiteRegEx" : "Baxter",
      "year" : 1995
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "J. Baxter" ],
      "venue" : "J. Artif. Intell. Res.(JAIR) 12:149–198.",
      "citeRegEx" : "Baxter,? 2000",
      "shortCiteRegEx" : "Baxter",
      "year" : 2000
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "the Journal of machine Learning research 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Toward an architecture for never-ending language learning",
      "author" : [ "A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R. Hruschka Jr", "T.M. Mitchell" ],
      "venue" : "AAAI.",
      "citeRegEx" : "Carlson et al\\.,? 2010",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2010
    }, {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine learning 28(1):41–75.",
      "citeRegEx" : "Caruana,? 1997",
      "shortCiteRegEx" : "Caruana",
      "year" : 1997
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning, 160–167. ACM.",
      "citeRegEx" : "Collobert and Weston,? 2008",
      "shortCiteRegEx" : "Collobert and Weston",
      "year" : 2008
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume, volume 15, 315–323.",
      "citeRegEx" : "Glorot et al\\.,? 2011",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky",
      "year" : 2009
    }, {
      "title" : "Learning task grouping and overlap in multi-task learning",
      "author" : [ "A. Kumar", "H. Daume III" ],
      "venue" : "arXiv preprint arXiv:1206.6417.",
      "citeRegEx" : "Kumar and III,? 2012",
      "shortCiteRegEx" : "Kumar and III",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE 86(11):2278–2324.",
      "citeRegEx" : "LeCun et al\\.,? 1998",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Ella: An efficient lifelong learning algorithm",
      "author" : [ "P. Ruvolo", "E. Eaton" ],
      "venue" : "ICML.",
      "citeRegEx" : "Ruvolo and Eaton,? 2013",
      "shortCiteRegEx" : "Ruvolo and Eaton",
      "year" : 2013
    }, {
      "title" : "Online learning of multiple tasks and their relationships",
      "author" : [ "A. Saha", "P. Rai", "H.D. Iii", "S. Venkatasubramanian" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, 643–651.",
      "citeRegEx" : "Saha et al\\.,? 2011",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2011
    }, {
      "title" : "Computationally efficient multi-task learning with least-squares probabilistic classifiers",
      "author" : [ "J. Simm", "M. Sugiyama", "T. Kato" ],
      "venue" : "Information and Media Technologies 6(2):508–515.",
      "citeRegEx" : "Simm et al\\.,? 2011",
      "shortCiteRegEx" : "Simm et al\\.",
      "year" : 2011
    }, {
      "title" : "Lifelong learning: A case study",
      "author" : [ "S. Thrun" ],
      "venue" : "Technical report, DTIC Document.",
      "citeRegEx" : "Thrun,? 1995",
      "shortCiteRegEx" : "Thrun",
      "year" : 1995
    }, {
      "title" : "Discovering structure in multiple learning tasks: The tc algorithm",
      "author" : [ "S. Thrun" ],
      "venue" : "ICML.",
      "citeRegEx" : "Thrun,? 1996a",
      "shortCiteRegEx" : "Thrun",
      "year" : 1996
    }, {
      "title" : "Learning to learn: Introduction",
      "author" : [ "S. Thrun" ],
      "venue" : "In Learning To Learn. Citeseer.",
      "citeRegEx" : "Thrun,? 1996b",
      "shortCiteRegEx" : "Thrun",
      "year" : 1996
    }, {
      "title" : "Multi-task learning for classification with dirichlet process priors",
      "author" : [ "Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram" ],
      "venue" : "The Journal of Machine Learning Research 8:35–63.",
      "citeRegEx" : "Xue et al\\.,? 2007",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "The machine learning community has long been aware of the importance of lifelong learning (Thrun 1995; 1996b; Carlson et al. 2010).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "The machine learning community has long been aware of the importance of lifelong learning (Thrun 1995; 1996b; Carlson et al. 2010).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "Lifelong learning is distinct from multitask learning (Caruana 1997) in the sense that it focuses on improving generalization performance for new tasks with minimal amounts of training data, rather than focusing on improving generalization performance for a fixed collection of tasks defined a priori.",
      "startOffset" : 54,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Our model is motivated by an idea adapted from (Baxter 2000): a lifelong learner should use information from past tasks to learn a representation that is simple enough to allow for effective generalization on new tasks with minimal training data, yet flexible enough to accommodate a relatively diverse set of tasks.",
      "startOffset" : 47,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "In (Thrun 1996a), the authors provide a listing of multitask and lifelong learning methods.",
      "startOffset" : 3,
      "endOffset" : 16
    }, {
      "referenceID" : 1,
      "context" : "The various structures used to induce information sharing include neural network-based feature transformations (Baxter 1995), learned distance metrics (Thrun 1996b), Bayesian priors (Xue et al.",
      "startOffset" : 111,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "The various structures used to induce information sharing include neural network-based feature transformations (Baxter 1995), learned distance metrics (Thrun 1996b), Bayesian priors (Xue et al.",
      "startOffset" : 151,
      "endOffset" : 164
    }, {
      "referenceID" : 17,
      "context" : "The various structures used to induce information sharing include neural network-based feature transformations (Baxter 1995), learned distance metrics (Thrun 1996b), Bayesian priors (Xue et al. 2007), and many others.",
      "startOffset" : 182,
      "endOffset" : 199
    }, {
      "referenceID" : 11,
      "context" : "In particular, (Ruvolo and Eaton 2013) presents an algorithm for online multitask learning composed of two parts: a linear function approximator fw(x) = w >x where w ∈ R, and a dictionary L ∈ Rd×k containing k d-dimensional basis functions.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "Additionally, the linearity of fw assumed by ELLA, as presented in (Ruvolo and Eaton 2013), limits the space of tasks where ELLA might perform well to tasks on which a linear approximator could conceivably perform well given enough training observations.",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "In (Saha et al. 2011), the authors also propose an online method for multitask learning, called OMTL, based on the perceptron algorithm.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Though, as pointed out by (Ruvolo and Eaton 2013), the perceptron algorithm tends to underperform other linear methods (e.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "On the theoretical side, (Baxter 2000) provides PAC bounds for a setting very similar to the one described in Section 2.",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "Modern examples of this approach include (Collobert and Weston 2008) and the wining entry in the Merck Molecular Activity Challenge in 2012.",
      "startOffset" : 41,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "We note that among the algorithms we’ve just mentioned, only ELLA (Ruvolo and Eaton 2013) can operate without the task correspondence assumption.",
      "startOffset" : 66,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "g (Collobert and Weston 2008).",
      "startOffset" : 2,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "Experiments in this section use the well-known MNIST dataset (LeCun et al. 1998).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "We compare LLDR to three multitask learning algorithms: ELLA (Ruvolo and Eaton 2013), GOMTL (Kumar and Daume III 2012), and OMTL (Saha et al.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "We compare LLDR to three multitask learning algorithms: ELLA (Ruvolo and Eaton 2013), GOMTL (Kumar and Daume III 2012), and OMTL (Saha et al. 2011), and standard single task learning (STL), on the landmine (Xue et al.",
      "startOffset" : 129,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "2011), and standard single task learning (STL), on the landmine (Xue et al. 2007) and London schools datasets (Kumar and Daume III 2012).",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "Similar to (Ruvolo and Eaton 2013), we treat every region as a different task.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "Similar to (Ruvolo and Eaton 2013) and (Kumar and Daume III 2012), we encode the categorical variables as one-hot binary features and add examination year and bias terms leading to 27 features per student.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "We follow the methodology used by (Ruvolo and Eaton 2013).",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : "For OMTL and GOMTL we use the results from (Ruvolo and Eaton 2013) which optimize the hyperparameters for each method on a validation set.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "We compare to ELLA (Ruvolo and Eaton 2013) and single-task multinomial logistic regression.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "The AUC results for OMTL and GOMTL are the mean results from (Ruvolo and Eaton 2013).",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "CIFAR 100 This dataset (Krizhevsky 2009) contains 50k training samples and 10k testing samples from 100 different classes.",
      "startOffset" : 23,
      "endOffset" : 40
    } ],
    "year" : 2017,
    "abstractText" : "We envision a machine learning service provider facing a continuous stream of problems with the same input domain, but with output domains that may differ. Clients present the provider with problems implicitly, by labeling a few example inputs, and then ask the provider to train models which reasonably extend their labelings to novel inputs. The provider wants to avoid constraining its users to a set of common labels, so it does not assume any particular correspondence between labels for a new task and labels for previously encountered tasks. To perform well in this setting, the provider needs a representation of the input domain which, in expectation, permits effective models for new problems to be learned efficiently from a small number of examples. While this bears a resemblance to settings considered in previous work on multitask and lifelong learning, our non-assumption of inter-task label correspondence leads to a novel algorithm: Lifelong Learner of Discriminative Representations (LLDR), which explicitly minimizes a proxy for the intra-task small-sample generalization error. We examine the relative benefits of our approach on a diverse set of real-world datasets in three significant scenarios: representation learning, multitask learning and lifelong learning.",
    "creator" : "TeX"
  }
}