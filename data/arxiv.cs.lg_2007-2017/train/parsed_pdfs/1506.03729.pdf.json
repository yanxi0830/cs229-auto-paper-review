{
  "name" : "1506.03729.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recovering communities in the general stochastic block model without knowing the parameters",
    "authors" : [ "Emmanuel Abbe", "Colin Sandon" ],
    "emails" : [ "eabbe@princeton.edu.", "sandon@princeton.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "∗Program in Applied and Computational Mathematics, and EE department, Princeton University, Princeton, USA, eabbe@princeton.edu. This research was partially supported by the 2014 Bell Labs Prize.\n†Department of Mathematics, Princeton University, USA, sandon@princeton.edu.\nar X\niv :1\n50 6.\n03 72\n9v 1\n[ m\nat h.\nPR ]\n1 1\nJu n\nContents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Related results on the general SBM with known parameters . . . . . . . . . 2 1.2 Estimating the parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"
    }, {
      "heading" : "2 Results 4",
      "text" : "2.1 Definitions and terminologies . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Partial recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 Exact recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    }, {
      "heading" : "3 Proof Techniques and Algorithms 6",
      "text" : "3.1 Partial recovery and the Agnostic-sphere-comparison algorithm . . . . . 6 3.2 Exact recovery and the Agnostic-degree-profiling algorithm . . . . . . 10"
    }, {
      "heading" : "4 An example with real data 11",
      "text" : ""
    }, {
      "heading" : "5 Open problems 12",
      "text" : "6 The Agnostic-sphere-comparison algorithm in details 16"
    }, {
      "heading" : "7 Appendix 24",
      "text" : "7.1 Partial Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n7.1.1 Formal results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 7.1.2 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7.2 Exact recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 7.2.1 Formal results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 7.2.2 Testing degree profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 7.2.3 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . 57"
    }, {
      "heading" : "1 Introduction",
      "text" : "This paper studies the problem of recovering communities in the general stochastic block model with linear size communities, for constant and slowly diverging degree regimes. In contrast to [AS15], this paper does not require knowledge of the SBM parameters. In particular, the problem of learning the model parameters is solved when average degrees are diverging. We next provide some motivations on the problem and further background on the model.\nDetecting communities (or clusters) in graphs is a fundamental problem in networks, computer science and machine learning. This applies to a large variety of complex networks (e.g., social and biological networks) as well as to data sets engineered as networks via similarly graphs, where one often attempts to get a first impression on the data by trying to identify groups with similar behavior. In particular, finding communities allows one to find like-minded people in social networks [GN02, NWS], to improve recommendation systems [LSY03, XWZ+14], to segment or classify images [SM97, SHB07], to detect protein complexes [CY06, MPN+99], to find genetically related sub-populations [PSD00, JTZ04], or discover new tumor subclasses [SPT+01].\nWhile a large variety of community detection algorithms have been deployed in the past decades, the understanding of the fundamental limits of community detection has only appeared more recently, in particular for the SBM [Co10, DKMZ11, Mas14, MNS14, ABH14, MNSb, YC14, AS15]. The SBM is a canonical model for community detection [HLL83, WBB76, FMW85, WW87, BC09, KN11, BCLS87, DF89, Bop87, JS98, CK99, CI01, SN97, McS01, BC09, RCY11, CWA12, CSX12], where n vertices are partitioned into k communities of relative size pi, i ∈ [k], and pairs of nodes in communities i and j connect independently with probability Wi,j .\nRecently the SBM came back to the center of the attention at both the practical level, due to extensions allowing overlapping communities [ABFX08] that have proved to fit well real data sets in massive networks [GB13], and at the theoretical level due to new phase transition phenomena [Co10, DKMZ11, Mas14, MNS14, ABH14, MNSb]. The latter works focus exclusively on the SBM with two symmetric communities, i.e., each community is of the same size and the connectivity in each community is identical. Denoting by p the intra- and q the extra-cluster probabilities, most of the results are concerned with two figure of merits: (i) recovery (also called exact recovery or strong consistency), which investigates the regimes of p and q for which there exists an algorithm that recovers with high probability the two communities completely [BCLS87, DF89, Bop87, JS98, CK99, CI01, SN97, McS01, BC09, RCY11, CWA12, CSX12, Vu14, YC14], (ii) detection, which investigates the regimes for which there exists an algorithm that recovers with high probability a positively correlated partition [Co10, DKMZ11, MNS12, Mas14, MNS14].\nThe sharp threshold for exact recovery was obtained in [ABH14, MNSb], showing1 that for p = a log(n)/n, q = b log(n)/n, a, b > 0, exact recovery is solvable if and only if√ a− √ b ≥ 2, with efficient algorithms achieving the threshold provided in [ABH14, MNSb]. In addition, [ABH14] introduces an SDP proved to achieve the threshold in [BH14, Ban15], while [YP14] shows that a spectral algorithm also achieves the threshold. Prior to these,\n1[MNSb] generalizes this to a, b = Θ(1).\nthe sharp threshold for detection was obtained in [Mas14, MNS14], showing that detection is solvable (and so efficiently) if and only if (a − b)2 > 2(a + b), when p = a/n, q = b/n, settling a conjecture made in [DKMZ11] and improving on [Co10].\nBesides the detection and the recovery properties, one may ask about the partial recovery of the communities, studied in [MNSa, GV14, Vu14, CRV15, AS15]. Of particular interest to this paper is the case of almost exact recovery (also called weak consistency), where only a vanishing fraction of the nodes is allowed to be misclassified. For two-symmetric communities, [MNSb] shows that almost exact recovery is possible if and only if n(p− q)2/(p+ q) diverges, generalized in [AS15] for general SBMs.\nIn the next section, we discuss the results for the general SBM of interest in this paper and the problem of learning the model parameters. We conclude this section by providing motivations on the problem of achieving the threshold with an efficient and universal algorithm.\nThreshold phenomena have long been studied in fields such as information theory (e.g., Shannon’s capacity) and constraint satisfaction problems (e.g., the SAT threshold). In particular, the quest of achieving the threshold has generated major algorithmic developments in these fields (e.g., LDPC codes, polar codes, survey propagation to name a few). Likewise, identifying thresholds in community detection models is key to benchmark and guide the development of clustering algorithms. Most reasonable algorithms may succeed in some regimes, while in others they may be doomed to fail due to computational barriers. However, it is particularly crucial to develop benchmarks that do not depend sensitively on the knowledge of the model parameters. A natural question is hence whether one can solve the various recovery problems in the SBM without having access to the parameters. This paper answers this question by the affirmative for the exact and almost exact recovery of the communities."
    }, {
      "heading" : "1.1 Related results on the general SBM with known parameters",
      "text" : "Most of the previous works are concerned with the SBM having symmetric communities (mainly 2 or sometimes k), with the exception of [Vu14] which provides some achievability results for the general SBM.2 Recently, [AS15] studied the fundamental limits for the general SBM, with results as follows (where SBM(n, p,W ) is the SBM with community prior p and connectivity matrix W ).\nI. Partial and almost exact recovery in the general SBM. The first result of [AS15] concerns the regime where the connectivity matrix scales as Q/n for a positive symmetric matrix Q (i.e., the node average degree is constant). The following notion of SNR is introduced3\nSNR = |λmin|2/λmax (1)\n2[GV14] also study variations of the k-symmetric model.\n3Note that this in a sense the “worst-case” notion of SNR, which ensures that all of the communities can be separated (when amplified); one could consider other ratios of the kind |λj |2/λmax, for subsequent eigenvalues (j = 2, 3, . . . ), if interested in separating only subset of the communities.\nwhere λmin and λmax are respectively the smallest 4 and largest eigenvalue of diag(p)Q.\nThe algorithm Sphere-comparison is proposed that solves partial recovery with exponential accuracy and quasi-linear complexity when the SNR diverges, solving in particular almost exact recovery.\nTheorem 1. [AS15] Given any k ∈ Z, p ∈ (0, 1)k with |p| = 1, and symmetric matrix Q with no two rows equal, let λ be the largest eigenvalue of PQ, and λ′ be the eigenvalue of PQ with the smallest nonzero magnitude. If ρ := |λ ′|2 λ > 4, λ\n7 < (λ′)8, and 4λ3 < (λ′)4, then for some ε = ε(λ, λ′) and C = C(p,Q) > 0, Sphere-comparison (see Section 3.1) detects with high probability communities in graphs drawn from SBM(n, p,Q/n) with accuracy 1−4ke− Cρ 16k /(1−exp(− Cρ16k ( (λ′)4 λ3 − 1 )\n)), provided that the above is larger than 1− mini pi2 ln(4k) , and runs in O(n1+ ) time. Moreover, ε can be made arbitrarily small with 8 ln(λ √ 2/|λ′|)/ ln(λ), and C(p, αQ) is independent of α.\nNote that for k symmetric clusters, SNR reduces to (a−b) 2\nk(a+(k−1)b) , which is the quantity of\ninterest for detection [DKMZ11, MNS12]. Moreover, the SNR must diverge to ensure almost exact recovery in the symmetric case [AS15]. The following is an important consequence of the previous theorem, as it shows that Sphere-comparison achieves almost exact recovery when the entries of Q are scaled.\nCorollary 1. [AS15] For any k ∈ Z, p ∈ (0, 1)k with |p| = 1, and symmetric matrix Q with no two rows equal, there exists (δ) = O(1/ ln(δ)) such that for all sufficiently large δ there exists an algorithm (Sphere-comparison) that detects communities in graphs drawn from SBM(n, p, δQ) with accuracy 1− e−Ω(δ) and complexity On(n1+ (δ)). II. Exact recovery in the general SBM. The second result in [AS15] is for the regime where the connectivity matrix scales as log(n)Q/n, Q fixed, where it is shown that exact recovery has a sharp threshold characterized by the divergence function\nD+(f, g) = max t∈[0,1] ∑ x∈[k] ( tf(x) + (1− t)g(x)− f(x)tg(x)1−t ) ,\nnamed the CH-divergence in [AS15]. Specifically, if all pairs of columns in diag(p)Q are at D+-distance at least 1 from each other, then exact recovery is solvable in the general SBM. This provides in particular an operational meaning to a new divergence function analog to the KL-divergence in the channel coding theorem (see Section 2.3 in [AS15]). Moreover, an algorithm (Degree-profiling) is developed that solves exact recovery down to the D+ limit in quasi-linear time, showing that exact recovery has no informational to computational gap (as opposed to the conjectures made for detection with more than 4 communities [DKMZ11]). The following gives a more general statement characterizing which subset of communities can be extracted — see Definition 3 for formal definitions.\nTheorem 2. [AS15] (i) Exact recovery is solvable in the stochastic block model G2(n, p,Q) for a partition [k] = tts=1As if and only if for all i and j in different subsets of the partition,5\nD+((PQ)i, (PQ)j) ≥ 1, (2)\n4The smallest eigenvalue of diag(p)Q is the one with least magnitude.\n5The entries of Q are assumed to be non-zero.\nIn particular, exact recovery is information-theoretically solvable in SBM(n, p,Q log(n)/n) if and only if mini,j∈[k],i 6=j D+((PQ)i||(PQ)j) ≥ 1. (ii) The Degree-profiling algorithm (see [AS15]) recovers the finest partition that can be recovered with probability 1 − on(1) and runs in o(n1+ ) time for all > 0. In particular, exact recovery is efficiently solvable whenever it is information-theoretically solvable.\nIn summary, exact or almost exact recovery is closed for the general SBM (and detection is closed for 2 symmetric communities). However this is for the case where the parameters of the SBM are assumed to be known, and with linear-size communities."
    }, {
      "heading" : "1.2 Estimating the parameters",
      "text" : "For the estimation of the parameter, some results are known for two-symmetric communities. In the logarithmic degree regime, since the SDP is agnostic to the parameters (it is a relaxation of the min-bisection), and the parameters can be estimated by recovering the communities [ABH14, BH14, Ban15]. For the constant-degree regime, [MNS12] shows that the parameters can be estimated above the threshold by counting cycles (which is efficiently approximated by counting non-backtracking walks). These are however for a fixed number of communities, namely 2. We also became recently aware of a parallel work [BCS15], which considers private graphon estimation (including SBMs). In particular, for the logarithmic degree regime, [BCS15] obtains a procedure to estimate parameters of graphons in an appropriate version of the L2 norm. This procedure is however not efficient.\nFor the general SBM, the results of [AS15] allow to find communities efficiently, however these rely on the knowledge of the parameters. Hence, a major open problem is to understand if these results can be extended without such a knowledge."
    }, {
      "heading" : "2 Results",
      "text" : "Agnostic algorithms are developed for the constant and diverging node degrees. These afford optimal accuracy scaling for large node degrees and achieve the CH-divergence limit for logarithmic node degrees in quasi-linear time. In particular, these solve the parameter estimation problems for SBM(n, p, ω(1)Q) without knowing the number of communities. An example with real data is provided in Section 4."
    }, {
      "heading" : "2.1 Definitions and terminologies",
      "text" : "The general stochastic block model SBM(n, p,W ) is a random graph ensemble defined on the vertex-set V = [n], where each vertex v ∈ V is assigned independently a hidden (or planted) label σv in [k] under a probability distribution p = (p1, . . . , pk) on [k], and each (unordered) pair of nodes (u, v) ∈ V ×V is connected independently with probability Wσu,σv , where Wσu,σv is specified by a symmetric k × k matrix W with entries in [0, 1]. Note that G ∼ SBM(n, p,W ) denotes a random graph drawn under this model, without the hidden (or planted) clusters (i.e., the labels σv ) revealed. The goal is to recover these labels by observing only the graph.\nThis paper focuses on p independent of n (the communities have linear size), W dependent on n such that the average node degrees are either constant or logarithmically growing, and\nk fixed. These assumptions on p and k could be relaxed, for example to slowly growing k, but we leave this for future work. As discussed in the introduction, the above regimes for W are both motivated by applications, as networks are typically sparse [LLDM08, Str01] though the average degrees may not be small, and by the fact that interesting mathematical phenomena take place in these regimes. For convenience, we attribute specific notations for the model in these regimes:\nDefinition 1. For a symmetric matrix Q ∈ Rk×k+ , G1(n, p,Q) denotes SBM(n, p,Q/n), and G2(n, p,Q) denotes SBM(n, p, ln(n)Q/n).\nDefinition 2. (Partial recovery.) An algorithm recovers or detects communities in SBM(n, p,W ) with an accuracy of α ∈ [0, 1], if it outputs a labelling of the nodes {σ′(v), v ∈ V }, which agrees with the true labelling σ on a fraction α of the nodes with probability 1− on(1). The agreement is maximized over relabellings of the communities.\nDefinition 3. (Exact recovery.) Exact recovery is solvable in SBM(n, p,W ) for a community partition [k] = tts=1As, where As is a subset of [k], if there exists an algorithm that takes G ∼ SBM(n, p,W ) and assigns to each node in G an element of {A1, . . . , At} that contains its true community6 with probability 1− on(1). Exact recovery is solvable in SBM(n, p,W ) if it is solvable for the partition of [k] into k singletons, i.e., all communities can be recovered.\nThe problem is solvable information-theoretically if there exists an algorithm that solves it, and efficiently if the algorithm runs in polynomial-time in n. Note that exact recovery for the partition [k] = {i} t ([k] \\ {i}) is equivalent to extracting community i. In general, recovering a partition [k] = tts=1As is equivalent to merging the communities that are in a common subset As and recovering the merged communities. Note also that exact recovery in SBM(n, p,W ) requires the graph not to have vertices of degree 0 in multiple communities with high probability (i.e., connectivity in the symmetric case). Therefore, for exact recovery, we focus below on W = ln(n)n Q where Q is fixed."
    }, {
      "heading" : "2.2 Partial recovery",
      "text" : "Our main result in the Appendix (Theorme 6) applies to SBM(n, p,Q/n) with arbitrary Q. We provided here a specific instance which is easier to parse.\nTheorem 3 (See Theorem 6). Given δ > 0 and for any k ∈ Z, p ∈ (0, 1)k with ∑ pi = 1 and 0 < δ ≤ min pi, and any symmetric matrix Q with no two rows equal such that every entry in Qk is positive (in other words, Q such that there is a nonzero probability of a path between vertices in any two communities in a graph drawn from G1(n, p, cQ)), there exists (c) = O(1/ ln(c)) such that for all sufficiently large c, Agnostic-sphere-comparison(G, δ) detects communities in graphs drawn from G1(n, p, cQ) with accuracy at least 1− e−Ω(c) in On(n 1+ (c)) time.\nNote that a vertex in community i has degree 0 with probability exponential in c, and there is no way to differentiate between vertices of degree 0 from different communities. So,\n6This is again up to relabellings of the communities.\nan error rate that decreases exponentially with c is optimal. The above gives in particular the parameter estimation in the case c = ω(1) (see also Lemma 17 in the Appendix).\nThe general result in the Appendix yields the following refined results in the k-block symmetric case.\nTheorem 4. Consider the k-block symmetric case. In other words, pi = 1 k for all i, and Qi,j is α if i = j and β otherwise. The vector whose entries are all 1s is an eigenvector of PQ with eigenvalue α+(k−1)βk , and every vector whose entries add up to 0 is an eigenvector of PQ with eigenvalue α−βk . So, λ = α+(k−1)β k and λ ′ = α−βk and (λ′)2 λ = (a−b)2 k(a+(k−1)β) . Then, as long as 1609 k(α+ (k − 1)β) 7 < (α− β)8 and 4k(α+ (k − 1)β)3 < (α− β)4, there exist a constant c > 0 such that Agnostic-sphere-comparison(G, 1/k) detects communities with an accuracy of 1−O(e−c(α−β)2/(α+(k−1)β)) for sufficiently large (α− β)2/(α+ (k − 1)β).\nWe refer to Section 4 for an example of implementation with real data."
    }, {
      "heading" : "2.3 Exact recovery",
      "text" : "Recall that from [AS15], exact recovery is information-theoretically solvable in the stochastic block model G2(n, p,Q) for a partition [k] = tts=1As if and only if for all i and j in different subsets of the partition,\nD+((PQ)i, (PQ)j) ≥ 1. (3)\nWe next show that this can be achieved without knowing the parameters. Recall that the finest partition is the largest partition of [k] that ensure (19).\nTheorem 5. (See Theorem 7) The Agnostic-degree-profiling algorithm (see Section 3.2) recovers the finest partition in any G2(n, p,Q), it uses no input except the graph in question, and runs in o(n1+ ) time for all > 0. In particular, exact recovery is efficiently and universally solvable whenever it is information-theoretically solvable.\nThe proof assumes that the entries of Q are non-zero, see Remark 1 for zero entries. To achieve this result we rely on a two step procedure. First an algorithm is developed to recover all but a vanishing fraction of nodes — this is the main focus of our partial recovery result — and then a procedure is used to “clean up” the leftover graphs using the node degrees of the preliminary classification. This turns out to be much more efficient than aiming for an algorithm that directly achieves exact recovery. We already used this technique in [AS15], but here we also deal with the difficulties resulting from not knowing the SBM’s parameters."
    }, {
      "heading" : "3 Proof Techniques and Algorithms",
      "text" : "3.1 Partial recovery and the Agnostic-sphere-comparison algorithm\nThe first key observation used to classify graphs’ vertices is that if v is a vertex in a graph drawn from G1(n, p,Q) then for all small r the expected number of vertices in community i that are r edges away from v is approximately ei · (PQ)reσv . So, we define:\nDefinition 4. For any vertex v, let Nr(v) be the set of all vertices with shortest path to v of length r. We also refer to the vector with i-th entry equal to the number of vertices in Nr(v) that are in community i as Nr(v). If there are multiple graphs that v could be considered a vertex in, let Nr[G](v) be the set of all vertices with shortest paths in G to v of length r.\nOne could probably determine PQ and eσ given the values of (PQ) reσv for a few different r, but using Nr(v) to approximate that would require knowing how many of the vertices in Nr(v) are in each community. So, we attempt to get information relating to how many vertices in Nr(v) are in each community by checking how it connects to Nr′(v\n′) for some vertex v′ and integer r′. The obvious way to do this would be to compute the cardinality of their intersection. Unfortunately, whether a given vertex in community i is in Nr(v) is not independent of whether it is in Nr′(v\n′), which causes the cardinality of |Nr(v) ∩Nr′(v′)| to differ from what one would expect badly enough to disrupt plans to use it for approximations.\nIn order to get around this, we randomly assign every edge in G to a set E with probability c. We hence define the following. Definition 5. For any vertices v, v′ ∈ G, r, r′ ∈ Z, and subset of G’s edges E, let Nr,r′[E](v · v′) be the number of pairs of vertices (v1, v2) such that v1 ∈ Nr[G\\E](v), v2 ∈ Nr′[G\\E](v′), and (v1, v2) ∈ E.\nNote that E and G\\E are disjoint; however, G is sparse enough that even if they were generated independently a given pair of vertices would have an edge between them in both with probability O( 1\nn2 ). So, E is approximately independent of G\\E. Thus, for any\nv1 ∈ Nr[G/E](v) and v2 ∈ Nr′[G/E](v′), (v1, v2) ∈ E with a probability of approximately cQσv1 ,σv2/n. As a result,\nNr,r′ [E](v · v′) ≈ Nr[G\\E](v) · cQ\nn Nr′[G\\E](v\n′)\n≈ ((1− c)PQ)reσv · cQ n ((1− c)PQ)r′eσv′ = c(1− c)r+r′eσv ·Q(PQ)r+r ′ eσv′/n\nLet λ1, ..., λh be the distinct eigenvalues of PQ, ordered so that |λ1| ≥ |λ2| ≥ ... ≥ |λh| ≥ 0. Also define h′ so that h′ = h if λh 6= 0 and h′ = h− 1 if λh = 0. If Wi is the eigenspace of PQ corresponding to the eigenvalue λi, and PWi is the projection operator on to Wi, then\nNr,r′[E](v · v′) ≈ c(1− c)r+r ′ eσv ·Q(PQ)r+r ′ eσv′/n (4)\n= c(1− c)r+r′\nn\n(∑ i PWi(eσv) ) ·Q(PQ)r+r′ ∑ j PWj (eσv′ )  (5) = c(1− c)r+r′\nn\n∑ i,j PWi(eσv) ·Q(PQ)r+r ′ PWj (eσv′ ) (6)\n= c(1− c)r+r′\nn\n∑ i,j PWi(eσv) · P−1(λj)r+r ′+1PWj (eσv′ ) (7)\n= c(1− c)r+r′\nn\n∑ i λr+r ′+1 i PWi(eσv) · P −1PWi(eσv′ ) (8)\nwhere the final equality holds because for all i 6= j,\nλiPWi(eσv) · P−1PWj (eσv′ ) = (PQPWi(eσv)) · P −1PWj (eσv′ )\n= PWi(eσv) ·QPWj (eσv′ ) = PWi(eσv) · P−1λjPWj (eσv′ ),\nand since λi 6= λj , this implies that PWi(eσv) · P−1PWj (eσv′ ) = 0. In order to simplify the terminology,\nDefinition 6. Let ζi(v · v′) = PWi(eσv) · P−1PWi(eσv′ ) for all i, v, and v ′.\nEquation (14) is dominated by the λr+r ′+1 1 term, so getting good estimate of the λ r+r′+1 2\nthrough λr+r ′+1\nh′ terms requires cancelling it out somehow. As a start, if λ1 > λ2 > λ3 then\nNr+2,r′[E](v · v′) ·Nr,r′[E](v · v′)−N2r+1,r′[E](v · v ′)\n≈ c 2(1− c)2r+2r′+2\nn2 (λ21 + λ 2 2 − 2λ1λ2)λr+r ′+1 1 λ r+r′+1 2 ζ1(v · v ′)ζ2(v · v′)\nNote that the left hand side of this expression is equal to det ∣∣∣∣ Nr,r′[E](v · v′) Nr+1,r′[E](v · v′)Nr+1,r′[E](v · v′) Nr+2,r′[E](v · v′) ∣∣∣∣.\nMore generally, in order to get an expression that can be used to estimate the λi and ζi(v ·v′), we consider the determinant of the following.\nDefinition 7. Let Mm,r,r′[E](v · v′) be the m × m matrix such that Mm,r,r′[E](v · v′)i,j = Nr+i+j,r′[E](v · v′) for each i and j.\nTo the degree that approximation 8 holds and c is small, each column of Mm,r,r′[E](v · v′) is a linear combination of the vectors\nc(1− c)r+r′\nn ζi(v · v′)λr+r\n′\ni [1, λi, λ 2 i , ..., λ m−1 i ] t\nwith coefficients that depend only on {λ1, ..., λh}. So, by linearity of the determinant in one column, det(Mm,r,r′[E](v · v′)) is a linear combination of these vector’s wedge products with coefficients that are independent of r and r′. By antisymmetry of wedge products, only the products that use m different such vectors contribute to the determinant, and the products involving the eigenvalues of highest magnitude will dominate. As a result, there exist constants γ(λ1, ..., λm) and γ ′(λ1, ..., λm) such that\ndet(Mm,r,r′[E](v · v′)) ≈ cm(1− c)m(r+r′)\nnm γ(λ1, ..., λm) m∏ i=1 λr+r ′+1 i ζi(v · v ′)\nif |λm| > |λm+1|, and\ndet(Mm,r,r′[E](v · v′)) ≈ c m(1− c)m(r+r′) nm · m−1∏ i=1 λr+r ′+1 i ζi(v · v ′)\n· ( γ(λ1, ..., λm)λ r+r′+1 m ζm(v · v′) + γ′(λ1, ..., λm)λr+r ′+1 m+1 ζm+1(v · v ′) )\nif |λm| = |λm+1|. These facts suggest the following plan for estimating the eigenvalues corresponding to a graph. First, pick several vertices at random. Then, use the fact that |Nr[G\\E](v)| ≈ ((1 − c)λ1)r for any good vertex v to estimate λ1. Next, use the formulas above about det(Mm,r,r′[E](v · v)) to get an approximation of h′ and all of PQ’s eigenvalues for each selected vertex. Finally, take the median of these estimates.\nNow, note that whether or not |λm| = |λm+1|, we have\ndet(Mm,r+1,r′[E](v · v′))− (1− c)mλm+1 m−1∏ i=1 λi det(Mm,r,r′[E](v · v′)) ≈ c m\nnm γ(λ1, ..., λm) λm − λm+1 (1− c)mλm m∏ i=1 ((1− c)λi)r+r ′+2ζi(v · v′)\nThat means that det(Mm,r+1,r′[E](v · v′))− (1− c)mλm+1 ∏m−1 i=1 λi det(Mm,r,r′[E](v · v′))\ndet(Mm−1,r+1,r′[E](v · v′))− (1− c)m−1λm ∏m−2 i=1 λi det(Mm−1,r,r′[E](v · v′)) ≈ c (1− c)n γ(λ1, ..., λm)\nγ(λ1, ..., λm−1) λm−1(λm − λm+1) λm(λm−1 − λm) ((1− c)λm)r+r ′+2ζm(v · v′)\nThis fact can be used in combination with estimates of PQ’s eigenvalues to approximate ζi(v · v′) for arbitrary v, v′, and i.\nOf course, this requires r and r′ to be large enough that\nc(1− c)r+r′\nn λr+r ′+1 i ζi(v · v ′)\nis large relative to the error terms for all i ≤ h′. At a minimum, that requires that |(1− c)λi|r+r\n′+1 = ω(n). On a different note, for any v and v′,\n0 ≤ PWi(eσv − eσv′ ) · P −1PWi(eσv − eσv′ ) = ζi(v · v)− 2ζi(v · v ′) + ζi(v ′ · v′)\nwith equality for all i if and only if σv = σv′ , so sufficiently good approximations of ζi(v · v), ζi(v · v′) and ζi(v′ · v′) can be used to determine which pairs of vertices are in the same community.\nOne could generate a reasonable classification based solely on this method of comparing vertices. However, that would require computing Nr,r′[E](v · v) for every vertex in the graph with fairly large r + r′, which would be slow. Instead, we use the fact that for any vertices v, v′, and v′′ with σv = σv′ 6= σv′′ ,\nζi(v ′ · v′)− 2ζi(v · v′) + ζi(v · v) = 0 ≤ ζi(v′′ · v′′)− 2ζi(v · v′′) + ζi(v · v)\nfor all i, and the inequality is strict for at least one i. So, subtracting ζi(v · v) from both sides gives us that\nζi(v ′ · v′)− 2ζi(v · v′) ≤ ζi(v′′ · v′′)− 2ζi(v · v′′)\nfor all i, and the inequality is still strict for at least one i. So, given a representative vertex in each community, we can determine which of them a given vertex, v, is in the same community as without needing to know the value of ζi(v · v). This runs fairly quickly if ζi(v · v′) is approximated using Nr,r′[E](v′ · v) such that r is large and r′ is small because the algorithm only requires focusing on |Nr′(v)| vertices. This leads to the following plan for partial recovery. First, randomly select a set of vertices that is large enough to contain at least one vertex from each community with high probability. Next, compare all of the selected vertices in an attempt to determine which of them are in the same communities. Then, pick one in each community. After that, use the algorithm referred to above to attempt to determine which community each of the remaining vertices is in. As long as there actually was at least one vertex from each community in the initial set and none of the approximations were particularly bad, this should give a reasonably accurate classification.\nThe risk that this randomly gives a bad classification due to a bad set of initial vertices can be mitigated as follows. First, repeat the previous classification procedure several times. Assuming that the procedure gives a good classification more often than not, the good classifications should comprise a set that contains more than half the classifications and which has fairly little difference between any two elements of the set. Furthermore, any such set would have to contain at least one good classification, so none of its elements could be too bad. So, find such a set and average its classifications together. This completes the Agnostic-Sphere-comparison-algorithm. We refer to Section 6 for a detailed version.\n3.2 Exact recovery and the Agnostic-degree-profiling algorithm\nThe exact recovery part is similar to [AS15] and uses the fact that once a good enough clustering has been obtained from Agnostic-sphere-comparison, the classification can be finished by making local improvements based on the nodes’ neighborhoods. The key result here is that, when testing between two multivariate Poisson distributions of means log(n)λ1 and log(n)λ2 respectively, where λ1, λ2 ∈ Zk+, the probability of error (of say maximum a posteriori decoding) is\nΘ ( n−D+(λ1,λ2)−o(1) ) . (9)\nThis is proved in [AS15]. In the case of unknown parameters, the algorithmic approach is largely unchanged, adding a step where the best known classification is used to estimate P and Q prior to any step in which vertices are classified based on their neighbors. The analysis of the algorithm requires however some careful handling.\nFirst, it is necessary to prove that given a labelling of the graph’s vertices with an error rate of x, one can compute approximations of P and Q that are within O(x+ log(n)/ √ n) of their true values with probability 1 − o(1). Secondly, one needs to modify the robust degree profiling lemma to show that attempting to determine vertices’ communities based on estimates of p and Q that are off by at most δ, p′ and Q′, and a classification of its neighbors that has an error rate of δ classifies the vertices with an error rate only eO(δ logn) times higher than it would be given accurate values of p and Q and accurate classifications of the vertices’ neighbors. Combining these yields the conclusion that any errors in the\nestimates of the SBM’s parameters do not disrupt vertex classification any worse than the errors in the preliminary classifications already were.\nThe Agnostic-degree-profiling algorithm. The inputs are (G, γ), where G is a graph, and γ ∈ [0, 1] (see Theorem 7 for how to set γ).\nThe algorithm outputs an assignment of each vertex to one of the groups of communities {A1, . . . , At}, where A1, . . . , At is the partition of [k] in to the largest number of subsets such that D+((pQ)i, (pQ)j) ≥ 1 for all i, j in [k] that are in different subsets (this is called the “finest partition”). It does the following:\n(1) Define the graph g′ on the vertex set [n] by selecting each edge in g independently with probability γ, and define the graph g′′ that contains the edges in g that are not in g′.\n(2) Run Agnostic-sphere-comparison on g′ to obtain the preliminary classification σ′ ∈ [k]n (see Section 7.1.)\n(3) Determine the size of each alleged community, and the edge density between each pair of alleged communities.\n(4) For each node v ∈ [n], determine in which community node v is most likely to belong to based on its degree profile computed from the preliminary classification σ′ (see Section 7.2.2), and call it σ′′v\n(5) Use σ′′v to get new estimates of p and Q. (6) For each node v ∈ [n], determine in which group A1, . . . , At node v is most likely to belong to based on its degree profile computed from the preliminary classification σ′′ (see Section 7.2.2)."
    }, {
      "heading" : "4 An example with real data",
      "text" : "We have tested a simplified version of our algorithm on the data from “The political blogosphere and the 2004 US Election” [AG05], which contains a list of political blogs that were classified as liberal or conservative, and links between the blogs.\nThe algorithm we used has a few major modifications relative to our standard algorithm. First of all, instead of using Nr,r′(v · v′) as its basic tool for comparing vertices, it uses a different measure, N ′r,r′(v · v′) which is defined as the fraction of pairs of an edge leaving the ball of radius r centered on v and an edge leaving the ball of radius r′ centered on v′ which hit the same vertex but are not the same edge. Making the measure a fraction of the pairs rather than a count of pairs was necessary to prevent N ′r,r′(v · v′) from being massively dependent on the degrees of v and v′, which would have resulted in the increased variance in vertex degree obscuring the effects of σv and σv′ on N ′ r,r′(v · v′). The other changes to the definition make the measure somewhat less reliable, but it is still useable as long as the average degree is fairly high and v 6= v′.\nSecondly, the version of Vertex-comparison-algorithm we used simply concludes that two vertices, v and v′, are in different communities if N ′r,r′(v · v′) is below average and the same community otherwise. This is reasonable because of the following facts. For one thing, because the normalization converts the dominant term to a constant, N ′r,r′(v · v′) is approximately affine in (λ2/λ1)\nr+r′ζ2(v · v′)/n. Also, as a result of the symmetry between communities, ζ2(v · v) is the same for all v. So, ζ2(v · v) − 2ζ2(v · v′) + ζ2(v′ · v′) is also affine in ζ2(v · v′). Furthermore, there are only two possible values of ζ2(v · v′) by symmetry,\nand λ2 > 0, so ζ2(v · v) − 2ζ2(v · v′) + ζ2(v′ · v′) > 0 iff (λ2/λ1)r+r ′ ζ2(v · v′)/n is below average. Finally, because both communities have the same average degree, ζ1(v, v ′) is independent of v and v′ so ζ1(v · v) − 2ζ1(v · v′) + ζ1(v′ · v′) is always 0. The version of Vertex-classification-algorithm we used is comparably modified.\nFinally, our algorithm generates reference vertices by repeatedly picking two vertices at random and comparing them. If it concludes that they are in different communities and they both have above-average degree, it accepts them as reference vertices; otherwise it tries again. Requiring above-average degree is useful because a higher degree vertex is less likely to have its neighborhood distorted by a couple of atypical neighbors.\nOut of 40 trials, the resulting algorithm gave a reasonably good classification 37 times. Each of these classified all but 56 to 67 of the 1222 vertices in the graph’s main component correctly. The state-of-the-art described in [CG15] gives a lowest value at 58, with the best algorithms around 60, while algorithms regularized spectral methods such as the one in [QR13] obtain about 80 errors."
    }, {
      "heading" : "5 Open problems",
      "text" : "The current result should also extend directly to a slowly growing number of communities (e.g., up to logarithmic). It would be interesting to extend the current approach to smaller sized communities or larger numbers of communities (watching the complexity scaling with the number of communities), as well as more general models with corrected-degrees, labeled-edges, or overlapping communities (though linear-sized overlapping communities can be treated with the approach of [AS15])."
    }, {
      "heading" : "7 Appendix",
      "text" : ""
    }, {
      "heading" : "7.1 Partial Recovery",
      "text" : ""
    }, {
      "heading" : "7.1.1 Formal results",
      "text" : "Theorem 6. For any δ > 0, there exists an algorithm (Agnostic-sphere-comparison) such that the following holds. Given any k ∈ Z, p ∈ (0, 1)k with |p| = 1, and symmetric matrix Q with no two rows equal, let λ be the largest eigenvalue of PQ, and λ′ be the eigenvalue of PQ with the smallest nonzero magnitude. For any x, x′, and such that x is either a unit reciprocal or an integer, is a rational number of the form 1z or 1− 1 z , and all of the following hold:\n2ke − .9x 2λ′2 min pi 16λk3/2((min pi) −1/2+x) / ( 1− e − .9x 2λ′2 min pi 16λk3/2((min pi) −1/2+x) ·(( .9λ ′4 4λ3 )−1) ) < 1\n2\n.9(λ′2/2)4 > λ7\n0 < x ≤ x′ < λk λ′min pi (2λ3/λ′2)1− /3 < λ (1 + /3) > log(λ)/ log(λ′2/2λ) 13(2x′(min pj) −1/2 + (x′)2) < min\n6=0 (wi({v})− wi({v′})) · P−1(wi({v})− wi({v′}))\nEvery entry of Qk is positive\n∃w ∈ Rk such that QPw = λw,w · Pw = 1, and x ≤ minwi/2. δ ≤ min pi\n8 ln(4b1/δc)b1/δce − x 2λ′2δ 16λb1/δc3/2(δ−1/2+x) / ( 1− e − x 2λ′2δ 16λb1/δc3/2(δ−1/2+x) ·(( λ ′4 4λ3 )−1) ) < δ\nmin pi > 8ke − .9x ′2λ′2 min pi 16λk3/2((min pi) −1/2+x′) / ( 1− e −.9 x ′2λ′2 min pi 16λk3/2((min pi) −1/2+x′) ·(( .9λ ′4 4λ3 )−1) )\nWith probability 1 − o(1), the algorithm runs in O(n1+ 2 3 log n) time and detects communities in graphs drawn from G1(n, p,Q) with accuracy at least 1− 3y′ without any input beyond δ and the graph, where\ny′ = 2ke − .9x ′2λ′2 min pi 16λk3/2((min pi) −1/2+x′) / ( 1− e −.9 x ′2λ′2 min pi 16λk3/2((min pi) −1/2+x′) ·(( .9λ ′4 4λ3 )−1) )\nConsidering the way δ, , x, and x′ scale when Q is multiplied by a scalar yields the following corollary.\nCorollary 2. For any k ∈ Z, p ∈ (0, 1)k with |p| = 1, and symmetric matrix Q with no two rows equal such that Qk has all positive entries, there exist (c) = O(1/ ln(c)) such that for all sufficiently large c, Agnostic-sphere-comparison detects communities in graphs drawn from G1(n, p, cQ) with accuracy at least 1− e−Ω(c) in On(n1+ (c)).\nIf instead of having constant average degree, one has an average degree which increases as n increases, one can slowly reduce b, δ, and as n increases, leading to the following corollary.\nCorollary 3. For any k ∈ Z, p ∈ [0, 1]k with |p| = 1, symmetric matrix Q with no two rows equa such that Qm has all positive entries for sufficiently large ml, and c(n) such that c = ω(1), Agnostic-sphere-comparison detects the communities with accuracy 1− o(1) in G1(n, p, c(n)Q) and runs in o(n1+ ) time for all > 0.\nThese corollaries are important as they show that if the entries of the connectivity matrix Q are amplified by a coefficient growing with n, almost exact recovery is achieved by (Agnostic-sphere-comparison) without parameter knowledge."
    }, {
      "heading" : "7.1.2 Proof of Theorem 6",
      "text" : "Proving Theorem 6 will require establishing some terminology. First, let λ1, ..., λh be the distinct eigenvalues of PQ, ordered so that |λ1| ≥ |λ2| ≥ ... ≥ |λh| ≥ 0 and if |λi| = |λi+1| then λi > 0 > λi+1. Also define h\n′ so that h′ = h if λh 6= 0 and h′ = h − 1 if λh = 0. In addition to this, let d be the largest sum of a column of PQ.\nDefinition 12. For any graph G drawn from G1(n, p,Q) and any set of vertices in G, V , let −→ V be the vector such that −→ V i is the number of vertices in V that are in community i. Define w1(V ), w2(V ), ..., wh(V ) such that −→ V = ∑ wi(V ) and wi(V ) is an eigenvector of PQ with eigenvalue λi for each i.\nw1(V ), ..., wh(V ) are well defined because Rk is the direct sum of PQ’s eigenspaces. The key intuition behind their importance is that if V ′ is the set of vertices adjacent to vertices in V then −→ V ′ ≈ PQ −→ V , so wi(V ′) ≈ PQ · wi(V ) = λiwi(V ).\nDefinition 13. For any vertex v, let Nr(v) be the set of all vertices with shortest path to v of length r. If there are multiple graphs that v could be considered a vertex in, let Nr[G′](v) be the set of all vertices with shortest paths in G′ to v of length r.\nWe also typically refer to −−−−−−→ Nr[G′](v) as simply Nr[G′](v), as the context will make it clear\nwhether the expression refers to a set or vector.\nDefinition 14. A vertex v of a graph drawn from G1(n, p,Q) is (R, x)-good if for all 0 ≤ r < R and w ∈ Rk with w · Pw = 1\n|w ·Nr+1(v)− w · PQNr(v)| ≤ xλh′\n2\n( λ2h′\n2λ1 )r and (R, x)-bad otherwise.\nNote that since any such w can be written as a linear combination of the ei, v is\n(R, x)-good if |ei · Nr+1(v) − ei · PQNr(v)| ≤ xλh′2\n( λ2 h′\n2λ1\n)r√ pi/k for all 1 ≤ i ≤ k and\n0 ≤ r < R.\nLemma 1. If v is a (R, x)-good vertex of a graph drawn from G1(n, p,Q), then for every 0 ≤ r ≤ R, |Nr(v)| ≤ λr1 √ k((min pi) −1/2 + x).\nProof. First, note that for any eigenvector of PQ, w, and r < R,\n|(P−1w) ·Nr+1(v)− (P−1w) · PQNr(v)| ≤ xλh′\n2\n( λ2h′\n2λ1\n)r√ w · P−1w\nSo, by the triangle inequality,\n|(P−1w) ·Nr+1(v)| ≤ |(P−1PQw) ·Nr(v)|+ xλh′\n2\n( λ2h′\n2λ1\n)r√ w · P−1w\n≤ λ1|(P−1w) ·Nr(v)|+ x ( λ1 2 )r+1√ w · P−1w\nThus, for any r ≤ R, it must be the case that\n|(P−1w) ·Nr(v)| ≤ λr1|(P−1w) ·N0(v)|+ r∑\nr′=1\nλr−r ′ 1 · x ( λ1 2 )r′ √ w · P−1w\n≤ λr1 ( |wσv/pσv |+ x √ w · P−1w ) Now, define w1,..., wh such that PQwi = λiwi for each i and p = ∑h i=1wi. For any i, j,\nλiwi · P−1wj = (PQwi) · P−1wj = wi · P−1PQwj = λjwi · P−1wj\nIf i 6= j, then λi 6= λj , so this implies that wi · P−1wj = 0. It follows from this that∑ i wi · P−1wi = ∑ i,j wi · P−1wj\n= (∑ i wi ) · P−1 ∑ j wj  = p · P−1p = 1\nAlso, for any i, it is the case that |(wi)σv/pσv | ≤ √ (wi)σv · p−1σv · (wi)σv/ √ pσv ≤ (min pi)−1/2 √ wi · P−1wi\nTherefore, for any r ≤ R, we have that\n|Nr(v)| = |(P−1p) ·Nr(v)| ≤ ∑ i |(P−1wi) ·Nr(v)|\n≤ λr1 ∑ i |(wi)σv/pσv |+ λr1x ∑ i √ wi · P−1wi ≤ λr1 √ k((min pi) −1/2 + x)\nThe following two lemmas are proved in [AS15].\nLemma 2. Let k ∈ Z, p ∈ (0, 1)k with |p| = 1, Q be a symmetric matrix such that λ4h′ > 4λ31, and 0 < x < λ1kλh′ min pi . Then there exists\ny < 2ke −\nx2λ2 h′ min pi\n16λ1k 3/2((min pi) −1/2+x) / 1− e− x2λ2h′ min pi16λ1k3/2((min pi)−1/2+x) ·(( λ4h′4λ31 )−1) \nand R(n) = ω(1) such that at least 1− y of the vertices of a graph drawn from G1(n, p,Q) are (R(n), x)-good with probability 1− o(1).\nLemma 3. Let k ∈ Z, p ∈ (0, 1)k with |p| = 1, Q be a symmetric matrix such that λ4h′ > 4λ31, R(n) = ω(1), and > 0 such that (2λ31/λ 2 h′) 1− /3 < λ1. A vertex of a graph drawn from G(p,Q, n) is (R(n), x)-good but (1− /3lnλ1 lnn, x)-bad with probability o(1). Definition 15. For any vertices v, v′ ∈ G, r, r′ ∈ Z, and subset of G’s edges E, let Nr,r′[E](v · v′) be the number of pairs of vertices (v1, v2) such that v1 ∈ Nr[G\\E](v), v2 ∈ Nr′[G\\E](v ′), and (v1, v2) ∈ E.\nNote that if Nr[G\\E](v) and Nr′[G\\E](v ′) have already been computed, Nr,r′[E](v · v′) can\nbe computed by means of the following algorithm, where E[v] = {v′ : (v, v′) ∈ E}\nCompute-Nr,r′[E](v · v′): for v1 ∈ Nr′[G\\E](v′):\nfor v2 ∈ E[v1] : if v2 ∈ Nr[G\\E](v) :\ncount=count+1 return count\nNote that this runs in O((d+1)|Nr′[G\\E](v′)|) average time. The plan is to independently put each edge in G in E with probability c. Then the probability distribution of G\\E will\nbe G1(n, p, (1− c)Q), so Nr[G\\E](v) ≈ ((1− c)PQ)reσv and Nr′[G\\E](v′) ≈ ((1− c)PQ)r ′ eσv′ . So, it will hopefully be the case that\nNr,r′[E](v ·v′) ≈ ((1−c)PQ)reσv ·cQ((1−c)PQ)r ′ eσv′/n = c(1−c) r+r′eσv ·Q(PQ)r+r ′ eσv′/n.\nMore rigorously, we have that:\nLemma 4. Choose p, Q, G drawn from G1(n, p,Q), E randomly selected from G’s edges such that each of G’s edges is independently assigned to E with probability c, and v, v′ ∈ G chosen independently from G’s vertices. Then with probability 1− o(1),\n|Nr,r′[E](v · v′)−Nr[G\\E](v) · cQNr′[G\\E](v′)/n| < (1 + √ |Nr[G\\E](v)| · |Nr′[G\\E](v′)|/n) log n\nProof. Roughly speaking, for each v1 ∈ Nr[G\\E](v) and v2 ∈ Nr′[G\\E](v′), (v1, v2) ∈ E with probability cQσv1 ,σv2/n. This is complicated by the facts that (v1, v1) is never in E and no edge is in G\\E and E. However, this changes the expected value of Nr,r′[E](v · v′) given G\\E by at most a constant unless G has more than double its expected number of edges, something that happens with probability o(1). Furthermore, whether (v1, v2) is in E is independent of whether (v′1, v ′ 2) is in E unless (v ′ 1, v ′ 2) = (v1, v2) or (v ′ 1, v ′ 2) = (v2, v1). So, the variance of Nr,r′[E](v · v′) is proportional to its expected value, which is\nO(|Nr[G\\E](v)| · |Nr′[G\\E](v′)|/n).\nNr,r′[E](v · v′) is within log n standard deviations of its expected value with probability 1− o(1), which completes the proof.\nNote that if −→v is an eigenvector of (1− c)PQ, √ PQ−→v is an eigenvector of the symmetric matrix (1−c) √ PQ √ P . So, since eigenvectors of a symmetric matrix with different eigenvalues are orthogonal, we have\nNr[G\\E](v) · cQNr′[G\\E](v′)/n = c\nn ∑ i wi(Nr[G\\E](v)) ·Qwi(Nr′[G\\E](v′))\nLemma 5 (Determinant Lemma). Let 0 < c < 1, x > 0, G be drawn from G1(n, p,Q), E be a subset of G’s edges that independently contains each edge with probability c, and m ∈ Z+. For any v, v′ ∈ G and r ≥ r′ ∈ Z+, such that ((1− c)λ2h′/2)r+r ′ > λr+r ′\n1 n let Mm,r,r′[E](v · v′) be the m × m matrix such that Mm,r,r′[E](v · v′)i,j = Nr+i+j,r′[E](v · v′) for each i and j. There exist γ = γ((1− c)λi,m) and γ′ = γ′((1− c)λi,m) such that γ is nonzero and for any r, r′, and vertices v, v′ ∈ G, then with probability 1− o(1), either v is (r + 2m+ 1, x)-bad, v′ is (r′ + 1, x)-bad, or\n|det(Mm,r,r′[E](v · v′))− cm\nnm m−1∏ i=1 wi(Nr[G\\E](v)) ·Qwi(Nr′[G\\E](v′))\n· (γwm(Nr[G\\E](v)) ·Qwm(Nr′[G\\E](v′)) + γ′wm+1(Nr[G\\E](v)) ·Qwm+1(Nr′[G\\E](v′)))|\n≤ c m\nnm lnm+1 n(1− c)m(r+r′)|λm+2|r+r ′ m−1∏ i=1 |λi|r+r ′\n+ cm\nnm lnm+1 n(1− c)m(r+r′)|λm|r+r ′ |λm+1|r+r ′ m−2∏ i=1 |λi|r+r ′\nwhere we temporarily adopt the convention that if i > h′, λi = λh′/ √\n2 and wi(S) = 0 for all S.\nAlternately, if m = h′ + 1 then with probability 1− o(1), either v is (r + 2m+ 1, x)-bad, v′ is (r′ + 1, x)-bad, or\n|det(Mm,r,r′[E](v · v′))|\n≤ c h′+1\nnh′+1 log2(n)(1− c)h′(r+r′) h′∏ i=1 |λi|r+r ′ ( ((1− c)λ1)2r n + ((1− c)λ1)r/2 ) · (1− c)r′λr′1\nProof. For each 1 ≤ l ≤ 2m and 1 ≤ i ≤ h, let\nxl(i) = c n (1− c)lλliwi(Nr[G\\E](v)) ·Qwi(Nr′[G\\E](v′))\nNext, for each 1 ≤ i ≤ h and 0 ≤ l ≤ m, let ul(i) be the column vector thats jth entry is xl+j(i) for 1 ≤ j ≤ m. Also, for each 1 ≤ l ≤ m, let ul(h + 1) be the length m column vector thats jth entry is Nr+l+j,r′[E](v · v′)− ∑h i=1 xl+j(i) for 1 ≤ j ≤ m. Note that for each\n1 ≤ l ≤ m, the lth column of Mm,r,r′[E](v · v′) is ∑h+1 i=1 ul(i). So,\ndet(Mm,r,r′[E](v · v′)) = ∑\ni∈(Z∩[1,h+1])m det([u1(i1), u2(i2), ..., um(im)])\nFor any i ∈ (Z ∩ [1, h + 1])m, if there exist j 6= j′ such that ij = ij′ ≤ h′, then uj(ij) = (1− c)j−j′λj−j ′\nij uj′(ij′), which implies that det([u1(i1), u2(i2), ..., um(im)]) = 0.\nIf m ≤ h and i is some permutation of the integers from 1 to m, then\ndet([u1(i1), u2(i2), ..., um(im)]) =  m∏ j=1 (1− c)jλjij  sgn(i) det([u0(1), u0(2), ..., u0(m)]) The jth column of this matrix is proportional to cnwj(Nr[G\\E](v)) ·Qwj(Nr′[G\\E](v\n′)), so there exists some γ = γ({(1− c)λj},m) such that the sum of all such terms is\ncm nm γ m∏ j=1 wj(Nr[G\\E](v)) ·Qwj(Nr′[G\\E](v′))\nAlternately, the sum of all such terms is equal to\ndet  m∑ j=1 u1(j), m∑ j=1 u2(j), ..., m∑ j=1 um(j)  If xl(0) 6= 0 for each 1 ≤ l ≤ m and u′ ∈ (R)m such that\n([∑m j=1 u1(j), ∑m j=1 u2(j), ..., ∑m j=1 um(j) ]) u′ = 0, then for each 1 ≤ i ≤ m,\nm∑ l=1 u′l m∑ j=1 xl+i(j) = 0\nm∑ j=1 m∑ l=1 u′l c n (1− c)l+iλl+ij wj(Nr[G\\E](v)) ·Qwj(Nr′[G\\E](v ′)) = 0\nm∑ j=1 wj(Nr[G\\E](v)) ·Qwj(Nr′[G\\E](v′))(1− c)iλij m∑ l=1 u′l · (1− c)lλlj = 0\nThat can only hold for all such i if ∑m\nl=1 u ′ l(1− c)lλlj = 0 for all 1 ≤ j ≤ m, and that can\nonly be the case if u′ = 0. Therefore, the determinant is nonzero unless xl(0) = 0 for some l, which implies that γ 6= 0. If m < h then by similar logic, there exists γ′ = γ′({(1− c)λi},m) such that the sum of all terms for which i is a permutation of the integers from 1 to m− 1 and m+ 1 is\ncm nm γ′ · wm+1(Nr[G\\E](v)) ·Qwm+1(Nr′[G\\E](v′)) m−1∏ i=1 wi(Nr[G\\E](v)) ·Qwi(Nr′[G\\E](v′))\nThat accounts for all i ∈ (Z ∩ [1, h+ 1])m except for some of those such that there exists j such that ij ≥ min(m+ 2, h+ 1) or there exist j, j′ such that ij = m and ij′ = m+ 1.\nIf v is (r + 2m+ 1, x)-good, then\nwi(Nr[G/E](v))P −1wi(Nr[G/E](v)) ≤ ((min pj)−1/2 + x)2(1− c)2rλ2ri\nfor all i. Similarly, if v′ is (r′ + 1, x)-good then\nwi(Nr′[G/E](v ′))P−1wi(Nr′[G/E](v ′)) ≤ ((min pj)−1/2 + x)2(1− c)2r ′ λ2r ′ i\nfor all i. If both hold, then |xl(i)| ≤ cn(1− c) r+r′+l|λi|r+r ′+l+1((min pj) −1/2 + x)2 for all\ni. Furthermore, for any l and j,\n|ul(h+ 1)j | = |Nr+l+j,r′[E](v · v′)− h∑ i=1 xl+j(i)|\n≤ |Nr+l+j,r′[E](v · v′)− c n Nr+l+j[G\\E](v) ·QNr′[G\\E](v′)|\n+ | c n Nr+l+j[G\\E](v) ·QNr′[G\\E](v′)− h∑ i=1 xl+j(i)|\n≤ (1 + √ |Nr+l+j[G\\E](v)| · |Nr′[G\\E](v′)|/n) log n\n+ c\nn h∑ i=1 |wi(Nr+l+j[G\\E](v)) ·Qwi(Nr′[G\\E](v′))\n− (1− c)l+jλl+ji wi(Nr[G\\E](v)) ·Qwi(Nr′[G\\E](v ′))|\nhence\n|ul(h+ 1)j | ≤ (1 + ((1− c)λ1)(r+r ′+l+j)/2) √ k((min pi) −1/2 + x)/ √ n) log n\n+ c\nn h∑ i=1 (1− c)r+l+j−1xλh′ ( λ2h′ 2λ1 )r |λi|l+j−1\n· √ wi(Nr′[G\\E](v′)) ·QPQwi(Nr′[G\\E](v′))\n≤ (1 + ((1− c)λ1)(r+r ′+l+j)/2) √ k((min pi) −1/2 + x)/ √ n) log n\n+ c\nn h∑ i=1 (1− c)r+l+j−1xλh′ ( λ2h′ 2λ1 )r |λi|l+j−1((min pj)−1/2 + x)(1− c)r ′ |λi|r ′+1\n≤ (1 + ((1− c)λ1)(r+r ′+l+j)/2) √ k((min pi) −1/2 + x)/ √ n) log n\n+ ch\nn (1− c)l+j−1xλh′\n( (1− c)2λ2h′\n2\n)(r+r′)/2 λl+j1 ((min pj) −1/2 + x)\nwith probability 1− o(1). In other words, under these circumstances |xl(i)| is upper bounded by a constant multiple of cn((1− c)|λi|) r+r′ if v and v′ are both good, and every entry of ul(h+ 1) has a magnitude that is upper bounded by a constant multiple of ((1 − c)λ1)(r+r ′)/2 log n/ √ n + cn((1 − c)2λ2h′/2) (r+r′)/2. Either way, every entry of ul(i) is upper bounded by a constant multiple of cn((1− c)|λi|) r+r′ log n.\nThat means that for any i ∈ (Z ∩ [1, h + 1])m such that ij ≥ m + 1 for some i, then det([u1(i1), u2(i2), ..., um(im)]) is upper bounded by a constant multiple of cm nm log m(n)(1− c)m(r+r ′)|λm+2|r+r ′∏m−1 i=1 |λi|r+r ′ Similarly, for i ∈ (Z ∩ [1, h + 1])m such that ij ≥ m− 1 and ij′ ≥ m− 1 with j 6= j′, det([u1(i1), u2(i2), ..., um(im)]) is upper bounded by a constant multiple of c m\nnm log m(n)(1 − c)m(r+r′)|λm|r+r ′ |λm+1|r+r ′∏m−2 i=1 |λi|r+r ′ . There are at most\nmm such i; therefore,\n| det(Mm,r,r′[E](v · v′))− cm\nnm m−1∏ i=1 wi(Nr[G\\E](v)) ·Qwi(Nr′[G\\E](v′))\n· (γwm(Nr[G\\E](v)) ·Qwm(Nr′[G\\E](v′)) + γ′wm+1(Nr[G\\E](v)) ·Qwm+1(Nr′[G\\E](v′)))|\n≤ c m\nnm lnm+1(n)(1− c)m(r+r′)|λm+2|r+r ′ m−1∏ i=1 |λi|r+r ′\n+ cm\nnm lnm+1(n)(1− c)m(r+r′)|λm|r+r ′ |λm+1|r+r ′ m−2∏ i=1 |λi|r+r ′\nwith probability 1− o(1), as desired. Alternately, recall that if v is (r + 2m + 1, x)-good then for any r′′ < r + 2m + 1 and\ni ≤ h,\n||E[Nr′′+1[G\\E](v))− (1− c)PQNr′′[G\\E](v)|| = O\n( |Nr′′[G\\E](v)|2\nn\n) = O ( ((1− c)λ1)2r ′′\nn\n)\nAlso, for fixed values of Nr′′′(v) for all r ′′′ ≤ r′′ ≤ r+2m+1, each element of Nr′′+1[G\\E](v)\nhas a variance of O(|Nr′′[G\\E](v)|) = O((1− c)λr ′′ 1 ). So,\n||wi(Nr′′+l+j[G\\E](v))− (1− c)l+jλ l+j i wi(Nr′′ [G\\E](v))||\n≤\n( ((1− c)λ1)2r ′′\nn + ((1− c)λ1)r\n′′/2 ) lnn\nwith probability 1−o(1) for all l, j ≤ m and i ≤ h. This implies that if v is (r+2m+1, x)-good and v′ is (r′ + 1, x)-good then |ul(h+ 1)j | = |Nr+l+j,r′[E](v · v′)− h∑ i=1 xl+j(i)|\n≤ |Nr+l+j,r′[E](v · v′)− c n Nr+l+j[G\\E](v) ·QNr′[G\\E](v′)|\n+ | c n Nr+l+j[G\\E](v) ·QNr′[G\\E](v′)− h∑ i=1 xl+j(i)|\n≤ (1 + √ |Nr+l+j[G\\E](v)| · |Nr′[G\\E](v′)|/n) log n\n+ c\nn h∑ i=1 |wi(Nr+l+j[G\\E](v)) ·Qwi(Nr′[G\\E](v′))\n− (1− c)l+jλl+ji wi(Nr[G\\E](v)) ·Qwi(Nr′[G\\E](v ′))|\n≤ (1 + ((1− c)λ1)(r+r ′+l+j)/2) √ k(((min pi) −1/2 + x)/ √ n) log n\n+ c\nn h∑ i=1 ( ((1− c)λ1)2r n + ((1− c)λ1)r/2 ) log n · ||Qwi(Nr′[G\\E](v′))||\n≤ (1 + ((1− c)λ1)(r+r ′+l+j)/2) √ k(((min pi) −1/2 + x)/ √ n) log n\n+ c\nn h∑ i=1 ( ((1− c)λ1)2r n + ((1− c)λ1)r/2 ) log n((min pj) −1/2 + x)(1− c)r′ |λi|r ′+1\n≤ (1 + ((1− c)λ1)(r+r ′+l+j)/2) √ k(((min pi) −1/2 + x)/ √ n) log n\n+ ch\nn\n( ((1− c)λ1)2r\nn + ((1− c)λ1)r/2\n) log n((min pj) −1/2 + x)(1− c)r′λr′+11\nfor any l and j with probability 1− o(1). If m = h′ + 1 then for any i ∈ (Z ∩ [1, h+ 1])m, either there exist j 6= j′ such that ij = ij′ ≤ h′, or there exists j such that ij > h′. Either\nway, det([u1(i1), u2(i2), ..., um(im)]) is upper bounded by a constant multiple of\nch ′+1\nnh′+1 log(n)(1− c)h′(r+r′) h′∏ i=1 |λi|r+r ′ ( ((1− c)λ1)2r n + ((1− c)λ1)r/2 ) · (1− c)r′λr′1\nwith probability 1 − o(1). There are only mm possible choices of i, so with probability 1− o(1), either v is (r + 2m+ 1, x)-bad, v′ is (r′ + 1, x)-bad, or\n|det(Mm,r,r′[E](v · v′))|\n≤ c h′+1\nnh′+1 log2(n)(1− c)h′(r+r′) h′∏ i=1 |λi|r+r ′ ( ((1− c)λ1)2r n + ((1− c)λ1)r/2 ) · (1− c)r′λr′1\nWhile this is a helpful result, it turns out to be more useful to have an expression that links the determinant to wi(Nr[G\\E](v)) ·Qwi(Nr′[G\\E](v′)) for fixed values of r and r′. So, we have the following:\nLemma 6. Let 0 < c < 1, x > 0, G be drawn from G1(n, p,Q), E be a subset of G’s edges that independently contains each edge with probability c, and m ≤ h′. Now, for any v, v′ ∈ G and √\nlnn ≤ r′ ≤ r ∈ Z+, such that ((1− c)λ2h′/2)r+r ′ > λr+r ′\n1 n, with probability 1− o(1), either v is (r + 2m+ 1, x)-bad, v′ is (r′ + 1, x)-bad, or\n|det(Mm,r,r′[E](v · v′))− cm\nnm m−1∏ i=1 ((1− c)λi)r+r ′−2 √ lnnwi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v ′))\n· (γ((1− c)λm)r+r ′−2 √ lnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v ′)) + γ′((1− c)λm+1)r+r ′−2 √ lnnwm+1(N√lnn[G\\E](v)) ·Qwm+1(N√lnn[G\\E](v ′)))|\n≤ 1 ln2m+2 n · c m nm m∏ i=1 |(1− c)λi|r+r ′\nProof. First, note that\n|wi(Nr[G/E](v)) ·Qwi(Nr′[G/E](v′))\n− ((1− c)λi)r+r ′−2 √ lnnwi(N√lnn[G/E](v)) ·Qwi(N√lnn[G/E](v ′))|\n≤ |λi[wi(Nr[G/E](v)) · P−1(wi(Nr′[G/E](v′))− ((1− c)λi)r ′− √ lnnwi(N√lnn[G/E](v ′)))\n+ (wi(Nr[G/E](v))− ((1− c)λi)r− √ lnnwi(N√lnn[G/E](v)))\n· P−1((1− c)λi)r ′− √ lnnwi(N√lnn[G/E](v ′))]|\n≤ |λi| √ wi(Nr[G/E](v)) · P−1wi(Nr[G/E](v)) · x|(1− c)λi|r ′\n2 √ lnn\n+ |λi| x|(1− c)λi|r\n2 √ lnn · |(1− c)λi|r\n′− √ lnn √ wi(N√lnn[G/E](v ′)) · P−1wi(N√lnn[G/E](v′))\n≤ 2x(1− c) r+r′ |λi|r+r ′+1\n2 √ lnn ((min pj)\n−1/2 + x) = o(|(1− c)λi|r+r ′ / ln2m+2(n))\nAlso,\n|λm+2|r+r ′ /|λm|r+r ′ ≤ n− ln(|λm/λm+2|)/ ln((1−c)λ 2 h′/(2λ1)) = o(1/ ln3m+3 n)\nand\n|λm+1|r+r ′ /|λm−1|r+r ′ ≤ n− ln(|λm−1/λm+1|)/ ln((1−c)λ 2 h′/(2λ1)) = o(1/ ln3m+3 n)\nCombining these inequalities with the determinant lemma yields the desired result.\nIn some sense, this establishes that\ndet(Mm,r,r′[E](v · v′)) ≈ cm\nnm m−1∏ i=1 ((1− c)λi)r+r ′−2 √ lnnwi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v ′))\n· (γ((1− c)λm)r+r ′−2 √ lnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v ′))\n+ γ′((1− c)λm+1)r+r ′−2 √ lnnwm+1(N√lnn[G\\E](v)) ·Qwm+1(N√lnn[G\\E](v ′)))\nHowever, in order to know this in a useful sense it is necessary to prove that these terms are large relative to the error terms. In order to do that, we need the following:\nLemma 7. For any p ∈ (0, 1)k with ∑ pi = 1 and k × k matrix Q with nonnegative entries such that every entry of Qk is positive, there exists a unique w ∈ (0,∞)k such that w is an eigenvector of PQ with eigenvalue λ1 and w·P−1w = 1. Now, let G be drawn from G1(n, p,Q), v ∈ G, and w′ be an eigenvector of PQ with eigenvalue λi such that λ2i > λ1. With probability 1− o(1) either v is ( √ lnn,min(P−1w)i/2)-bad, or |w′ · P−1N√lnn(v)| ≥ λ √ lnn i / lnn.\nProof. Every entry in (PQ)k is positive, so its eigenvector of largest eigenvalue is unique up to multiplication, and its entries all have the same sign. That means that it has a unique multiple, w, such that w · P−1w = 1 and w1 > 0. In this case all of w’s entries must be positive, and since (PQ)kw = λk1w, it must be the case that PQw = λ1w.\nGiven any ( √ lnn,min(P−1w)j/2)-good vertex v, and any r < √ lnn,\nw · P−1Nr(v) ≥ λr1w · P−1{v} − min(P−1w)j\n2\nr−1∑ j=0 2−j−1λr1 ≥ λr1 min(P−1w)j/2\nFor any eigenvector w′′ with eigenvalue λi′ and w ′′ · P−1w′′ = 1,\n|w′′·P−1Nr(v)| ≤ |λi′ |rw′′·P−1{v}+ min(P−1w)j\n2\nr−1∑ j=0 2−j−1|λi′ |r ≤ |λi′ |r(min p −1/2 j +min(P −1w)j/2)\nλ1 > λ2, so there exists a constant r0 such that for any r > r0,\nNr(v)j ≥ min(P−1w)j\n4 λr1wj\nfor all j. For r0 < r ≤ √\nlnn, 1 ≤ j ≤ k, and a fixed value of Nr(v), the probability distribution of Nr+1(v)j is within o(1) of a poisson distribution with expected value\n(PQNr(v))j . Furthermore, Nr+1(v)j′ has negligible dependence on Nr+1(v)j for all j ′ 6= j. So, w′ ·P−1Nr+1(v) has an expected value of λiw′ ·P−1Nr(v)+o(1) and a standard deviation of √√√√ k∑\nj=1\nw′2j p −2 j (PQNr(v))j + o(1) ≤ √√√√2λr+11 min(P−1w)j k∑ j=1 w′2j p −2 j wj + o(1)\nThis implies that E [ |w′ · P−1N√lnn(v)− λ √ lnn−r i w ′ · P−1Nr(v)| ] ≤\n√ lnn−1∑ r′=r E [ |λ √ lnn−1−r′ i w ′ · P−1Nr′+1(v)− λ √ lnn−r′ i w ′ · P−1Nr′(v)| ]\n≤ √ lnn−1∑ r′=r λ √ lnn−1−r′ i √√√√2λr′+11 min(P−1w)j k∑ j=1 w′2j p −2 j wj + o(1)  ≤ λ √ lnn−r\ni λ r/2 1 · 1√ λ2i /λ1 − 1 √√√√2 min(P−1w)j k∑ j=1 w′2j p −2 j wj + o(1) \nIn particular, this means that if there exists r0 < r < 2 ln lnn/ ln(λ 2 i /λ1) such that |w′ · P−1Nr(v)| > λr/21 ln ln ln lnn then with probability 1− o(1),\n|w′ · P−1N√lnn(v)| ≥ λ √ lnn−r i λ r/2 1 ln ln ln lnn− λ √ lnn−r i λ r/2 1 √ ln ln ln lnn\n≥ λ √\nlnn i · (λ 2 i /λ1)\n−r/2\n≥ λ √\nlnn i / lnn\nFurthermore, since the probability distribution of w′ · P−1Nr(v) for a fixed value of Nr−1(v) is a sum of constant multiples of poisson distributions with expected values of Ω(λ r 1), it must be the case that |w′ ·P−1Nr(v)| > λr/21 ln ln ln lnn with probability e−O(ln 2 ln ln lnn) = ω(1/ ln lnn). Therefore, there exists r0 < r < 2 ln lnn/ ln(λ 2 i /λ1) such that this holds with probability 1− o(1), and the lemma follows.\nThis also implies that for any v, v′, and i, either v is ( √ lnn,min(P−1w)i/2)-bad, v ′\nis ( √\nlnn,min(P−1w)i/2)-bad, or |wi(N√lnn(v)) · P −1wi(N√lnn(v\n′))| ≥ λ2 √\nlnn i / ln 2 n with probability 1 − o(1), since the degree of dependence between N√lnn(v) and N√lnn(v\n′) is negligible. This allows me to attempt to approximate PQ’s eigenvalues as follows.\nLemma 8. Let 0 < c < 1, min(P−1w)i/2 > x > 0, G be drawn from G1(n, p,Q), E be a subset of G’s edges that independently contains each edge with probability c, v ∈ G, such that (1− c)(λ2h′/2)4 > λ71.\nWith probability 1− o(1), either v is (23 · log n/ log((1− c)λ1), x)-bad or Basic-Eigenvalueapproximation-algorithm(E,c,v) runs in O(n) time and returns (λ′1, ..., λ ′ h′′) such that h ′ = h′′ and |λ′i − λi| < ln −3/2(n) for all i.\nBasic-Eigenvalue-approximation-algorithm(E,c,v):\nCompute Nr[G\\E](v) for each r until |Nr[G\\E](v)| > √ n, and then set λ′′1 = 2r √ n/(1− c).\nSet r = r′ = 23 log n/ log((1− c)λ ′′ 1)−\n√ lnn. Then, compute\n2r\n√ nmax(| detMm,r,r[E](v · v)|, | detMm,r+1,r[E](v · v)|)\ncmax(|detMm−1,r,r[E](v · v)|, |detMm−1,r+1,r[E](v · v)|)\nuntil an m is found for which this expression is less than ((1− c)λ′′1)3/4 + 1√lnn . Then, set h′′ = m− 1.\nThen, set\n|λ′i| = 1\n1− c\n√ det(Mi,r+3,r′[E](v · v′))/det(Mi,r+1,r′[E](v · v′))/ i−1∏ j=1 (1− c)|λ′j |\nunless | det(Mi,r+1,r′[E](v · v′))| < √ | det(Mi,r,r′[E](v · v′))| · | det(Mi,r+2,r′[E](v · v′))|, in\nwhich case set |λ′i| = 11−c √ det(Mi,r+2,r′[E](v · v′))/ det(Mi,r,r′[E](v · v′))/ ∏i−1 j=1(1 − c)|λ′j |. Repeat this for each i ≤ h′′\nNext, for each i < h′′, if ||λ′i| − |λ′i+1|| < 1lnn then set λ ′ i = |λ′i| and λ′i+1 = −|λ′i+1|. For each i ≤ h′′ such that ||λ′i| − |λ′i+1|| ≥ 1lnn and ||λ ′ i−1| − |λ′i|| ≥ 1lnn set\nλ′i = 1 1− c det(Mi,r+1,r′[E](v · v′))/ det(Mi,r,r′[E](v · v′))/ i−1∏ j=1 (1− c)λ′j\nReturn (λ′1, ..., λ ′ h′′)\nProof. If v is (23 · log n/ log((1− c)λ1, x)-good, there exists a constant r such that for any r < r′′ < 23 · log n/ log((1− c)λ1),\nmin(P−1w)j 4 ((1− c)λ1)r ′′ k∑ j=1 wj ≤ |Nr′′[G\\E](v[0])| ≤ ((1− c)λ1)r ′′√ k((min pi) −1/2 + x)\nSo, the minimum r′′ such that |Nr′′[G\\E](v)| > √ n is within a constant of log n/2 log((1−c)λ1). That means that |λ1 − λ′′1| is upper bounded by a constant multiple of 1/ log n, and that r = r′ is within a constant of the value it would have if λ′′1 were λ1. This also implies that if n is sufficiently large it is less than 23 · log n/ log((1− c)λ1)− 2h\n′ − 3. For m ≤ h′, r′′ ∈ {r, r + 1}, and v ∈ G if |λi| 6= |λi+1| then\n| det(Mm,r′′,r′′[E](v · v))\nγ({(1−c)λj},m)cm nm ∏m i=1((1− c)λi)2r ′′−2 √ lnnwi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v)) −1| = o(1)\nwith probability 1− o(1). If |λi| = |λi+1| then λi = −λi+1, so either γ((1− c)λm)2r−2 √ lnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v)) has the same sign as γ\n′((1− c)λm+1) 2r−2 √\nlnnwm+1(N√lnn[G\\E](v)) ·Qwm+1(N√lnn[G\\E](v)) or γ((1−c)λm)2r+1−2 √ lnnwm(N√lnn[G\\E](v))·Qwm(N√lnn[G\\E](v)) has the same sign as γ\n′((1− c)λm+1) 2r+1−2 √ lnnwm+1(N√lnn[G\\E](v)) ·Qwm+1(N√lnn[G\\E](v))). Either way, we have that\n(1− o(1))|γ({(1− c)λj},m)c m\nnm\nm∏ i=1 ((1− c)λi)2r−2 √ lnnwi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v))|\n≤ max(|det(Mm,r,r[E](v · v))|, |det(Mm,r+1,r[E](v · v))|)\n≤ (1 + o(1))| c m\nnm m−1∏ i=1 ((1− c)λi)2r+1−2 √ lnnwi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v))|\n· (|γ((1− c)λm)2r+1−2 √ lnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v))| + |γ′((1− c)λm+1)2r+1−2 √ lnnwm+1(N√lnn[G\\E](v)) ·Qwm+1(N√lnn[G\\E](v))|)\nwith probability 1− o(1). Furthermore, by lemma 8, these bounds are within a factor of O(ln2 n) of each other with probability 1− o(1).\nThat means that\nnmax(| detMm,r,r[E](v · v)|, | detMm,r+1,r[E](v · v)|) cmax(| detMm−1,r,r[E](v · v)|, |detMm−1,r+1,r[E](v · v)|)\nis within a factor of ln3(n) of |((1−c)λm)2r1−2 √\nlnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v))| with probability 1− o(1), and thus that\n2r1\n√ nmax(| detMm,r,r[E](v · v)|, |detMm,r+1,r[E](v · v)|)\ncmax(| detMm−1,r,r[E](v · v)|, |detMm−1,r+1,r[E](v · v)|) = |(1− c)λm| ± o(1)\nwith probability 1− o(1). |(1− c)λm| ≥ 4 √\n(4(1− c)3λ31, so with probability 1− o(1), this expression is not less than ((1− c)λ′′1)3/4 + 1√lnn .\nIf m = h′ + 1, r′′ ∈ {r, r + 1}, and v ∈ G, then with probability 1 − o(1) either v is (r′′ + 2m+ 1, x)-bad or\n|det(Mm,r,r′[E](v · v))|\n≤ c h′+1\nnh′+1 ln2(n)(1− c)2h′·r′′ h′∏ i=1 |λi|2r ′′\n( ((1− c)λ1)2r ′′\nn + ((1− c)λ1)r\n′′/2 ) · (1− c)r′′λr′′1\n≤ 2 c h′+1\nnh′+1 ln2(n)(1− c)2h′·r′′ h′∏ i=1 |λi|2r ′′ · ((1− c)λ1)3r ′′/2\nIf v falls under the later case,\nnmax(| detMm,r,r[E](v · v)|, | detMm,r+1,r[E](v · v)|) cmax(|detMm−1,r,r[E](v · v)|, |detMm−1,r+1,r[E](v · v)|) = O(ln2(n)((1− c)λ1)3r/2)\n, so\n2r\n√ nmax(|detMm,r,r[E](v · v)|, |detMm,r+1,r[E](v · v)|)\ncmax(|detMm−1,r,r[E](v · v)|, | detMm−1,r+1,r[E](v · v)|) = ((1−c)λ1)3/4+O(ln(ln(n))/ ln(n))\nTherefore, with probability 1− o(1), either v is (23 · log n/ log((1− c)λ1), x)-bad or h ′′ = h′.\nBy the previous two lemmas, with probability 1− o(1) either v is (r+ 2m+ 4, x)-bad, or\nwi(N√lnn(v)) · P −1wi(N√lnn(v\n′))| ≥ λ2 √\nlnn i / ln 2 n\nfor all i ≤ h and\n|det(Mm,r′′,r′[E](v · v′))\n− c m\nnm m−1∏ i=1 ((1− c)λi)r ′′+r′−2 √ lnnwi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v ′))\n· (γ((1− c)λm)r ′′+r′−2 √ lnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v ′)) + γ′((1− c)λm+1)r ′′+r′−2 √ lnnwm+1(N√lnn[G\\E](v)) ·Qwm+1(N√lnn[G\\E](v ′)))|\n≤ 1 ln2m+2 n · c m nm m∏ i=1 |(1− c)λi|r ′′+r′\nfor all m ≤ h′ + 1 and r ≤ r′′ < r + 4. Assume that the later holds. If |λm+1| < |λm| then if n is sufficiently large the γ′ term will be negligable relative to the γ term, while if |λm+1| = |λm| increasing r′′ by two would multiply both of these terms by (1− c)2λ2m. Either way, we have that for any k ≤ h′ and r ≤ r′′ ≤ r + 1,\n| det(Mm,r′′+2,r′[E](v · v′))/det(Mm,r′′,r′[E](v · v′))− k∏ j=1 ((1− c)λj)2| ≤ 1 ln7/8 n\nas long as n is sufficiently large, and\n|γ((1− c)λm)r ′′+r′−2 √ lnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v ′))\n+ γ′((1− c)λm+1)r ′′+r′−2 √ lnnwm+1(N√lnn[G\\E](v)) ·Qwm+1(N√lnn[G\\E](v ′))|\n≥ 1 2 |γ((1− c)λm)r\n′′+r′−2 √\nlnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v ′))|\nWe assumed that n is large, so that can only fail if |λm| = |λm+1|. In this case, λm = −λm+1, so increasing m by one would result in both terms having the same sign, at which point the condition is satisfied. So,\n||λ′i|2 − |λi|2| ≤ 4 ln−7/8 n\nfor any i ≤ h′. For any i < h′, if |λi| > |λi+1|, then for sufficiently large n, |λ′i|−|λ′i+1| will be greater than 1lnn . On the other hand, if |λi| = |λi+1| and n is sufficiently large, |λ ′ i| − |λ′i+1| will be less than 1lnn . So, the algorithm will suceed at determining which eigenvalues have the same absolute values and assign λ′i the same sign as λi for each i. So, |λ′i−λi| < ln −3/2(n) as desired. Assuming this all works, the slowest part of the algorithm is computing expressions of the form Nr′′,r′′′[E](v · v), each such expression can be computed in O(n) time, and only a constant number of them need to be computed. So, the algorithm runs in O(n) time.\nSo, this algorithm sometimes works, but it fails if v is bad. This risk can be mitigated by using multiple vertices as follows.\nImproved-Eigenvalue-approximation-algorithm(c):\nCreate a set of edges E, that each of G’s edges is independently assigned to with probability c.\nRandomly select √ lnn of G’s vertices, v[1], v[2],..., v[ √ lnn].\nRun Basic-Eigenvalue-approximation-algorithm(E,c,v[i]) for each i ≤ √\nlnn, stopping the algorithm prematurely if it takes more than O(n √ lnn) time.\nReturn (λ′1, ..., λ ′ h′′) where h ′′ and λ′i are the median outputs of the executions of BasicEigenvalue-approximation-algorithm for each i.\nLemma 9. Let 0 < c < 1, min(P−1w)i/2 > x > 0, and G be drawn from G1(n, p,Q). Improved-Eigenvalue-approximation-algorithm(c) runs in O(n log n) time. Furthermore, if (1− c)(λ2h′/2)4 > λ71, x < λ1k λh′ min pi , and\n2ke −\nx2(1−c)λ2 h′ min pi\n16λ1k 3/2((min pi) −1/2+x) / 1− e− x2(1−c)λ2h′ min pi16λ1k3/2((min pi)−1/2+x) ·(( (1−c)λ4h′4λ31 )−1)  < 1\n2\nthen Improved-Eigenvalue-approximation-algorithm(c) returns (λ′1, ..., λ ′ h′′) such that h ′ = h′′ and |λ′i − λi| < ln −3/2(n) for all i with probability 1− o(1).\nProof. Generating E takes O(n) time, picking v[i] takes o(n) time, each execution of Basic-Eigenvalue-approximation-algorithm(c) runs in O(n √ log n) time, and combining their outputs takes o(n) time. So, this algorithm runs in O(n log n) time. Assuming the conditions are satisfied, there exists y < 12 such that 1− y of G’s vertices are (23 · log n/ log((1− c)λ1, x)-good with probability 1− o(1). So, with probability 1− o(1), the majority of the selected vertices of G are (23 · log n/ log((1 − c)λ1), x)-good and the majority of the executions of Basic-Eigenvalue-approximation-algorithm give good output. If this happens, then the median value of h′′ is h′, and for each 1 ≤ i ≤ h′, the median value of λ′i is within ln −3/2(n) of λi for each i, as desired.\nGiven approximations of PQ’s eigenvalues, one can attempt to approximate Ni({v}) · P−1Ni({v′}) as follows.\nVertex-product-approximation-algorithm(v,v’,r,r’,E,c,(λ′1, ..., λ ′ h′′)): (Assumes that Nr′′[G\\E](v) has already been computed for r ′′ ≤ r + 2h′′ + 3 and that Nr′′[G\\E](v ′) has already been computed for r′′ ≤ r′)\nFor each i ≤ h′′, set\nzi(v · v′) = det(Mi,r+1,r′[E](v · v′)− (1− c)iλ′i+1\n∏i−1 j=1 λ ′ j det(Mi,r,r′[E](v · v′)\ndet(Mi−1,r+1,r′[E](v · v′)− (1− c)i−1λ′i ∏i−2 j=1 λ ′ j det(Mi−1,r,r′[E](v · v′)\n· n(λ′i−1 − λ′i)γ({(1− c)λ′j}, i− 1) cλ′i−1(λ ′ i − λ′i+1)γ({(1− c)λ′j}, i) ((1− c)λ′i)−r−r ′−1\nReturn (z1(v · v′), ..., zh′′(v · v′)).\nLemma 10. Let 0 < c < 1, min(P−1w)i/2 > x > 0, G be drawn from G1(n, p,Q), E be a subset of G’s edges that independently contains each edge with probability c, v, v′ ∈ G and √ lnn ≤ r′ ≤ r ∈ Z+, such that ((1 − c)λ2h′/2)r+r ′ > λr+r ′ 1 n. Also let (λ ′ 1, ..., λ ′ h′′) be a h′′-tuple which may depend on n such that h′′ = h′ and |λ′i − λi| ≤ ln −3/2(n) for all i. Assume that Nr′′[G\\E](v) has already been computed for r ′′ ≤ r + 2m + 3 and that Nr′′[G\\E](v ′) has already been computed for r′′ ≤ r′. Vertex-product-approximationalgorithm(v, v′, r, r′, E, c, (λ′1, ..., λ ′ h′′)) runs in O(((1− c)λ1)r ′ ) average time. Furthermore, with probability 1 − o(1), either v is (r + 2m + 4, x)-bad, v′ is (r′ + 1, x)-bad, or Vertexproduct-approximation-algorithm(v, v′, r, r′, E, c, (λ′1, ..., λ ′ h′′)) returns (z1(v · v′), ..., zh′(v · v′)) such that |zi(v · v′)− wi({v}) · P−1wi({v′})| < 2x(min pj)−1/2 + x2 + o(1) for all i.\nProof. The slowest part of the algorithm is computing the expressions of the form Nr′′,r′[E](v · v′). Each of these can be computed in O(E[|Nr′[G\\E](v′)|]) = O(((1− c)λ1)r ′ ) average time, and the number of these that need to be computed is constant in n. So, this algorithm runs in O(((1− c)λ1)r ′ ) average time.\nWith probability 1− o(1) either v is (r + 2m+ 4, x)-bad, v′ is (r′ + 1, x)-bad, or\nwi(N√lnn(v)) · P −1wi(N√lnn(v\n′))| ≥ λ2 √\nlnn i / ln 2 n\nfor all i ≤ h and\n|det(Mm,r′′,r′[E](v · v′))− cm\nnm m−1∏ i=1 ((1− c)λi)r ′′+r′−2 √ lnnwi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v ′))\n· (γ((1− c)λm)r ′′+r′−2 √ lnnwm(N√lnn[G\\E](v)) ·Qwm(N√lnn[G\\E](v ′)) + γ′((1− c)λm+1)r ′′+r′−2 √ lnnwm+1(N√lnn[G\\E](v)) ·Qwm+1(N√lnn[G\\E](v ′)))|\n≤ 1 ln2m+2 n · c m nm m∏ i=1 |(1− c)λi|r ′′+r′\nfor all m ≤ h′ + 1 and r ≤ r′′ < r + 4. Assume that the later holds.\n|det(Mi,r+1,r′[E](v · v′))− (1− c)iλ′i+1 i−1∏ j=1 λ′j det(Mi,r,r′[E](v · v′))\n− γλi − λi+1 λi · c i ni i∏ j=1 ((1− c)λj)r+1+r ′−2 √ lnnwj(N√lnn[G\\E](v)) ·Qwj(N√lnn[G\\E](v ′))|\n≤ 1 lnn · | c\ni\nni i∏ j=1 ((1− c)λj)r+1+r ′−2 √ lnnwj(N√lnn[G\\E](v)) ·Qwj(N√lnn[G\\E](v ′))|\nfor any i ≤ h with probability 1− o(1). So,\n| det(Mi,r+1,r′[E](v · v′)− (1− c)iλ′i+1\n∏i−1 j=1 λ ′ j det(Mi,r,r′[E](v · v′)\ndet(Mi−1,r+1,r′[E](v · v′)− (1− c)i−1λ′i ∏i−2 j=1 λ ′ j det(Mi−1,r,r′[E](v · v′) − γ({(1− c)λj}, i) γ({(1− c)λj}, i− 1) · λi−1(λi − λi+1) λi(λi−1 − λi)\n· c n\n((1− c)λi)r+r ′−2 √ lnn+1wi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v ′))|\n≤ 1 n √ lnn · |((1− c)λi)r+r ′+1|\nwith probability 1− o(1). Therefore,\n|zi(v · v′)− λ−1i ((1− c)λi) −2 √ lnnwi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v ′))| = o(1)\nwith probability 1− o(1). That implies that\n|zi − wi({v}) · P−1wi({v′})|\n≤ |zi − (1− c)((1− c)λi)−2 √ lnn−1wi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v ′))|\n+ (1− c) · |((1− c)λi)−2 √ lnn−1wi(N√lnn[G\\E](v)) ·Qwi(N√lnn[G\\E](v ′))\n− ((1− c)λi)− √ lnn−1wi({v}) ·Qwi(N√lnn[G\\E](v ′))|\n+ |(1− c) · ((1− c)λi)− √ lnn−1wi({v}) ·Qwi(N√lnn[G\\E](v ′))− wi({v}) · P−1wi({v′}))|\n≤ |((1− c)λi)− √ lnnwi(N√lnn[G\\E](v ′)) · P−1[((1− c)λi)− √ lnnwi(N√lnn[G\\E](v))− wi({v})]|\n+ |wi({v}) · P−1[((1− c)λi)− √ lnnwi(N√lnn[G\\E](v ′))− wi({v′})] + o(1)\nBy goodness of v and v′, this is less than or equal to ((1− c)λi)− √ lnn √ wi(N√lnn[G\\E](v ′)) · P−1wi(N√lnn[G\\E](v′)) · x\n+ √ wi({v}) · P−1wi({v}) · x+ o(1)\n≤ √ wi({v′}) · P−1wi({v′}) + 2wi({v′}) · P−1[((1− c)λi)− √ lnnwi(N√lnn[G\\E](v ′))− wi({v′})]+\n[((1− c)λi)− √ lnnwi(N√lnn[G\\E](v ′))− wi({v′})] ·P−1[((1− c)λi)− √ lnnwi(N√lnn[G\\E](v ′))− wi({v′})] · x\n+ x √\n1/min pj + o(1) ≤ √ 1/min pj + 2x/ √ min pj + x2x+ x/ √ min pj + o(1) = (x2 + 2x(min pj) −1/2) + o(1)\nwith probability 1− o(1), as desired.\nFor any two vertices in different communities, v and v′, the fact that Q’s rows are distinct\nimplies that Q( −→ {v} − −−→ {v′}) 6= 0. So, wi({v}) 6= wi({v′}) for some 1 ≤ i ≤ h′. That means that for any two vertices v and v′,\n(wi({v})− wi({v′})) · P−1(wi({v})− wi({v′})) = wi({v}) · P−1wi({v})− 2wi({v}) · P−1wi({v′}) + wi({v′}) · P−1wi({v′}) ≥ 0\nfor all 1 ≤ i ≤ h′, with equality for all i if and only if v and v′ are in the same community. This also implies that given a vertex v, another vertex in the same community v′, and a vertex in a different community v′′,\n2wi({v}) · P−1wi({v′})− wi({v′}) · P−1wi({v′}) ≥ 2wi({v}) · P−1wi({v′′})− wi({v′′}) · P−1wi({v′′})\nfor all 1 ≤ i ≤ h′ and the inequality is strict for at least one i. This suggests the following algorithms for classifying vertices.\nVertex-comparison-algorithm(v,v’, r,r’,E,x,c,(λ′1, ..., λ ′ h′′)): (Assumes that Nr′′[G\\E](v) and Nr′′[G\\E](v ′) have already been computed for r′′ ≤ r+2h′′+3)\nRun Vertex-product-approximation-algorithm(v, v′, r, r′, E, c, (λ′1, ..., λ ′ h′′)), Vertex-productapproximation-algorithm(v, v, r, r′, E, c, (λ′1, ..., λ ′ h′′)), and Vertex-product-approximationalgorithm(v′, v′, r, r′, E, c, (λ′1, ..., λ ′ h′′)).\nIf ∃i : zi(v · v)− 2zi(v · v′) + zi(v′ · v′) > 5(2x(min pj)−1/2 + x2) then conclude that v and v′ are in different communities.\nOtherwise, conclude that v and v′ are in the same community.\nLemma 11. Assuming that each of G’s edges was independently assigned to E with probability c, this algorithm runs in O(((1− c)λ1)max(r,r\n′)) average time. Furthermore, if each execution of Vertex-product-approximation-algorithm succeeds and 13(2x(min pj)\n−1/2 + x2) is less than the minimum nonzero value of (wi({v})− wi({v′})) · P−1(wi({v})− wi({v′})) then the algorithm returns the correct result with probability 1− o(1).\nProof. The slowest step of the algorithm is using Vertex-product-approximation-algorithm. This runs in an average time of O(((1− c)λ1)max(r,r\n′)) and must be done 3 times. If each execution of Vertex-product-approximation-algorithm succeeds then with probability 1− o(1) the zi are all within 6 5(2x(min pj)\n−1/2 + x2) of the products they seek to approximate, in which case\nzi(v · v)− 2zi(v · v′) + zi(v′ · v′) > 5(2x(min pj)−1/2 + x2)\nif and only if (wi({v})− wi({v′})) · P−1(wi({v})− wi({v′})) 6= 0,\nwhich is true for some i if and only if v and v′ are in different communities.\nVertex-classification-algorithm(v[],v’, r,r’,E,c,(λ′1, ..., λ ′ h′′)): (Assumes that Nr′′[G\\E](v[σ])have already been computed for 0 ≤ σ < k and r′′ ≤ r+2h′′+3, that Nr′′[G\\E](v\n′) has already been computed for all r′′ ≤ r′, and that zi(v[σ] · v[σ]) has already been computed for each i and σ)\nRun Vertex-product-approximation-algorithm(v[σ], v′, r, r′, E, c, (λ′1, ..., λ ′ h′′)) for each σ.\nFind a σ that minimizes the value of\nmax σ′ 6=σ,i≤h′′\nzi(v[σ] · v[σ])− 2zi(v[σ] · v′)− [zi(v[σ′] · v[σ′])− 2zi(v[σ′] · v′)]\nand conclude that v′ is in the same community as v[σ].\nLemma 12. Assuming that E was generated properly, this algorithm runs in O(((1−c)λ1)r ′ ) average time. Let x > 0 and assume that each execution of Vertex-product-approximationalgorithm succeeds (including the previous ones to compute zi(v[σ] · v[σ])), and that v[] contains exactly one vertex from each community. Also, assume that 13(2x(min pj)\n−1/2 +x2) is less than the minimum nonzero value of (wi({v})− wi({v′})) · P−1(wi({v})− wi({v′})). Then this algorithm classifies v′ correctly with probability 1− o(1).\nProof. Again, the slowest step of the algorithm is running Vertex-product-approximationalgorithm. This runs in an average time of O(((1− c)λ1)r ′ ) and must be done k times. If the conditions given above are satisifed, then each zi is within 21 20(2x(min pj)\n−1/2 + x2) of the product it seeks to approximate with probability 1− o(1). If this is the case, then\n2wi({v′}) · P−1wi({v[σ]})− wi({v[σ]}) · P−1wi({v[σ]}) ≥ 2wi({v′}) · P−1wi({v[σ′]})− wi({v[σ′]}) · P−1wi({v[σ′]})\nfor all i and σ′ if v′ is in the same community as v[σ], and\n2wi({v′}) · P−1wi({v[σ]})− wi({v[σ]}) · P−1wi({v[σ]}) ≤ 2wi({v′}) · P−1wi({v[σ′]})− wi({v[σ′]}) · P−1wi({v[σ′]})− 13(2x(min pj)−1/2 + x2)\nfor some i and σ otherwise. So,\nzi(v[σ] · v[σ])− 2zi(v[σ] · v′) ≤ zi(v[σ′] · v[σ])− 2zi(v[σ′] · v′) + 19 3 · (2x(min pj)−1/2 + x2)\nfor all i and σ iff v′ is in the same community as v[σ]. So,\nmax σ′ 6=σ,i≤h′′\nzi(v[σ]·v[σ])−2zi(v[σ]·v′)−[zi(v[σ′]·v[σ′])−2zi(v[σ′]·v′)] ≤ 19 3 ·(2x(min pj)−1/2+x2)\niff v′ is in the same community as v[σ]. Therefore, the algorithm returns the correct result with probability 1− o(1).\nAt this point, we can finally start giving algorithms for classifying a graph’s vertices.\nLemma 13. Let x′ > 0, and assume that all of the following hold:\n< 1\n(1− c)λ4h′ > 4λ31 0 < x ≤ x′ < λ1k λh′ min pi (2(1− c)λ31/λ2h′)1− /3 < (1− c)λ1 (1 + /3) > log((1− c)λ1)/ log((1− c)λ2h′/2λ1) 13(2x′(min pj)\n−1/2 + (x′)2) < min 6=0 (wi({v})− wi({v′})) · P−1(wi({v})− wi({v′}))\n∃k such that every entry of Qk is positive ∃w ∈ Rk such that QPw = λ1w,w · Pw = 1, and x ≤ minwi/2. h′′ = h′ |λi − λ′i| ≤ ln−3/2(n) for all i\nUnreliable-graph-classification-algorithm(G,c,m, ,x,(λ′1, ..., λ ′ h′′)):\nRandomly assign each edge in G to E independently with probability c.\nRandomly select m vertices in G, v[0], ..., v[m− 1].\nLet r = (1− 3) log n/ log((1− c)λ ′ 1)− √ lnn and r′ = 2 3 · log n/ log((1− c)λ ′ 1)\nCompute Nr′′[G\\E](v[i]) for each r ′′ ≤ r + 2h′′ + 3 and 0 ≤ i < m.\nRun vertex-comparison-algorithm(v[i], v[j], r, r′, E, x, (λ′1, ..., λ ′ h′′)) for every i and j\nIf these give consistent results, randomly select one alleged member of each community v′[σ]. Otherwise, fail.\nFor every v′′ in the graph, compute Nr′′[G\\E](v ′′) for each r′′ ≤ r′. Then, run vertex-classification-algorithm(v′[], v′′, r, r′, E, (λ′1, ..., λ ′ h′′)) in order to get a hypothesized classification of v′′\nReturn the resulting classification.\nLet y = 2ke −\nx2(1−c)λ2 h′ min pi\n16λ1k 3/2((min pi) −1/2+x) / 1− e− x2(1−c)λ2h′ min pi16λ1k3/2((min pi)−1/2+x) ·(( (1−c)λ4h′4λ31 )−1) \nand y′ = 2ke −\nx′2(1−c)λ2 h′ min pi\n16λ1k 3/2((min pi) −1/2+x′) / 1− e− x′2(1−c)λ2h′ min pi16λ1k3/2((min pi)−1/2+x′) ·(( (1−c)λ4h′4λ31 )−1) \nThis algorithm runs in O(m2n1− 3 + n1+ 2 3 ) time. Furthermore, with probability 1− o(1), G is such that Unreliable-graph-classification-algorithm(G, c,m, , x, (λ′1, ..., λ ′ h′′)) has at least a 1− k(1−min pi)m −my\nchance of classifying at least 1− y′ of G’s vertices correctly.\nProof. Let r0 = (1− 3) log n/ log((1−c)λ1). There exists y ′′ < y such that if these conditions hold, then with probability 1− o(1), at least 1− y′′ of G’s vertices are (r0, x)-good and the number of vertices in G in community σ is within √ n log n of pσn for all σ. If this is the case, then for sufficiently large n, it is at least 1− k(1−min pi)m −my likely that every one of the m randomly selected vertices is (r0, x)-good and at least one is selected from each community.\nIf v[i] is (r0, x)-good for all i, then with probability 1 − o(1), vertex-comparisonalgorithm(v[i], v[j], r, r′, E, x, c, (λ′1, ..., λ ′ h′′)) determines whether or not v[i] and v[j] are in the same community correctly for every i and j, allowing the algorithm to pick one member of each community. If that happens, then the algorithm will classify each (r′+h′, x′)-good\nvertex correctly with probability 1−o(1). So, as long as the initial selection of v[] is good, the algorithm classifies at least 1− y′ of the graph’s vertices correctly with probability 1− o(1).\nGenerating E and v[] takes O(n) time. Computing Nr′′[G\\E](v[i]) for all r ′′ ≤ r+ 2h′ + 3 takes O(m| ∪r′′ Nr′′[G\\E](v[i])) = O(mn) time, and computing Nr′′[G\\E](v′) for all r′′ ≤ r′ and v′ ∈ G takes\nO(n| ∪r′′≤r′ Nr′′[G\\E]) = O(n · ((1− c)λ1)r ′ ) = O(n1+ 2 3 )\ntime. Once these have been computed, running Vertex-comparisonalgorithm(v[i], v[j], r, r′, E, x, (λ′1, ..., λ ′ h′′)) for every i and j takes O(m\n2 · ((1 − c)λ1)r) = O(m2n1− 3 ) time, at which point an alleged member of each community can be found in O(m2) time. Running Vertex-classification-algorithm(v′[], v′′, r, r′, E, c, (λ′1, ..., λ ′ h′′)) for every v′′ ∈ G takes O(n · ((1− c)λ1)r ′ ) = O(n1+ 2 3 ) time. So, the overall algorithm runs in O(m2n1− 3 + n1+ 2 3 ) average time.\nSo, this algorithm can sometimes give a vertex classification that is nontrivially better than that obtained by guessing. However, it has an assymptotically nonzero failure rate and requires too much information about the graph’s parameters. In order to get around that, we combine the results of multiple executions of the algorithm and add in a parameter analysis procedure as follows.\nLemma 14. Assume that there exist x, x′, and such that x is either a unit reciprocal or\nReliable-graph-classification-algorithm(G,m,δ, T(n)) (i.e., Agnostic-sphere-comparison):\nRun Improved-Eigenvalue-approximation-algorithm(.1) in order to compute (λ′1, ..., λ ′ h′′)\nLet λ′′1 = λ ′ 1 + 2 ln −3/2(n), λ′′h′′ = λ ′ h′′ − 2 ln −3/2(n), and k′ = b1/δc\nLet x be the smallest rational number of minimal numerator such that\nk′(1− δ)m +m · 2k′e −\nx2(λ′′ h′′ ) 2δ\n16λ′′1 (k ′)3/2((δ)−1/2+x) / 1− e− x2(λ′′h′′ )2δ16λ′′1 (k′)3/2((δ)−1/2+x) ·(( (λ′′h′′ )44(λ′′1 )3 )−1)  < 1\n2\nLet be the smallest rational number of the form 1z or 1− 1 z such that (2(λ ′′ 1) 3/(λ′′h′′) 2)1− /3 < λ′′1 and (1 + /3) > log(λ ′′ 1)/ log((λ ′′ h′′) 2/2λ′′1)\nLet c be the largest unit reciprocal less than 1/9 such that all of the following hold:\n(1− c)(λ′′h′′)4 > 4(λ′′1)3 (2(1− c)(λ′′1)3/(λ′′h′)2)1− /3 < (1− c)λ′′1 (1 + /3) > log((1− c)λ′′1)/ log((1− c)(λ′′h′)2/2λ′′1)\nk′(1− δ)m +m · 2k′e −\nx2(1−c)(λ′′ h′ ) 2δ\n16λ′′1 (k ′)3/2((δ)−1/2+x) / 1− e− x2(1−c)(λ′′h′′ )2δ16λ′′1 (k′)3/2((δ)−1/2+x) ·(( (1−c)(λ′′h′′ )44(λ′′1 )3 )−1)  < 1\n2\nRun Unreliable-graph-classification-algorithm(G, c,m, , x, (λ′1, ..., λ ′ h′′)) T (n) times and record the resulting classifications.\nFind the smallest y′′ such that there exists a set of more than half of the classifications no two of which have more than y′′ disagreement, and discard all classifications not in the set. In this step, define the disagreement between two classifications as the minimum disagreement over all bijections between their communities.\nFor every vertex in G, randomly pick one of the remaining classifications and assert that it is in the community claimed by that classification, where a community from one classification is assumed to correspond to the community it has the greatest overlap with in each other classification.\nReturn the resulting combined classification.\nan integer, is a rational number of the form 1z or 1− 1 z , and all of the following hold:\n2ke −\n.9x2λ2 h′ min pi\n16λ1k 3/2((min pi) −1/2+x) / 1− e− .9x2λ2h′ min pi16λ1k3/2((min pi)−1/2+x) ·(( .9λ4h′4λ31 )−1)  < 1\n2\n.9(λ2h′/2) 4 > λ71 λ4h′ > 4λ 3 1 0 < x ≤ x′ < λ1k λh′ min pi (2λ31/λ 2 h′) 1− /3 < λ1 (1 + /3) > log(λ1)/ log(λ 2 h′/2λ1) 13(2x′(min pj) −1/2 + (x′)2) < min\n6=0 (wi({v})− wi({v′})) · P−1(wi({v})− wi({v′}))\nevery entry of Qk is positive\n∃w ∈ Rk such that QPw = λ1w,w · Pw = 1, and x ≤ minwi/2. δ ≤ min pi\nb1/δc · (1− δ)m +m · 2b1/δce −\nx2λ2 h′δ\n16λ1b1/δc3/2(δ−1/2+x) / 1− e− x2λ2h′δ16λ1b1/δc3/2(δ−1/2+x) ·(( λ4h′4λ31 )−1)  < 1\n2\nT (n) = w(1)\nT (n) ≤ ln(n)\nmin pi > 8ke −\n.9x′2λ2 h′ min pi\n16λ1k 3/2((min pi) −1/2+x′) / 1− e−.9 x′2λ2h′ min pi16λ1k3/2((min pi)−1/2+x′) ·(( .9λ4h′4λ31 )−1) \nWith probability 1− o(1), Reliable-graph-classification-algorithm(G,m,δ, T(n)) runs in O(m2n1− 3T (n) + n1+ 2 3 T (n)) time and classifies at least 1− 3y′ of G’s vertices correctly, where\ny′ = 2ke −\n.9x′2λ2 h′ min pi\n16λ1k 3/2((min pi) −1/2+x′) / 1− e−.9 x′2λ2h′ min pi16λ1k3/2((min pi)−1/2+x′) ·(( .9λ4h′4λ31 )−1) \nProof. With probability 1−o(1), Improved-Eigenvalue-approximation-algorithm gives output such that h′′ = h′ and |λi − λ′i| ≤ ln\n−3/2(n) for each i. Assuming that this holds, the algorithm finds the largest x and that satisfy the conditions above and the largest unit reciprocal less than 1/9, c, that satisifes the conditions for Unreliable-graph-classificationalgorithm(G, c,m, , x, (λ′1, ..., λ ′ h′′)) to have a greater than 1/2 success rate for any k ≤ b1/δc with probability 1− o(1). Since its success rate is greater than 1/2 and T (n) = ω(1), more than half of the executions of Unreliable-graph-classification-algorithm give classifications with error y′ or less with probability 1− o(1). That means that y′′ ≤ 2y′ and at least one of the classifications in the selected set must have error y′ or less. Thus, all of the selected classifications have error 3y′ or less. The requirement that min pi > 4y\n′ ensures that the bijection between any two of these classifications’ communities that minimizes disagreeement\nis the identity. Therefore, this algorithm classifies at least 1− 3y′ of the vertices correctly, as desired.\nAssuming this all works correctly, Improved-Eigenvalue-approximation-algorithm runs in O(n ln(n)) time. Finding x, , and c takes constant time, and running Unreliable-graphclassification-algorithm T (n) times takes O(m2n1− 3T (n) + n1+ 2 3 T (n)). Computing the degree of agreement between each pair of classifications takes O(n ln2(n)) time. T (n) < lnn, so the brute force algorithm finds y′′ and the corresponding set in O(n) time. Combining the classifications takes O(n) time. Therefore, this whole algorithm runs in O(m2n1− 3T (n) + n1+ 2 3 T (n)) time with probability 1− o(1), as desired.\nProof of Theorem 6. If the conditions hold, Reliable-graph-classificationalgorithm(G, ln(4b1/δc)/δ, δ, ln(n)) has the desired properties by the previous lemma."
    }, {
      "heading" : "7.2 Exact recovery",
      "text" : "Recall that p is a probability vector of dimension k, Q is a k × k symmetric matrix with positive entries, and G2(n, p,Q) denotes the stochastic block model with community prior p and connectivity matrix ln(n)Q/n. A random graph G drawn under G2(n, p,Q) has a planted community assignment, which we denote by σ ∈ [k]n and call sometime the true community assignment.\nRecall also that exact recovery is solvable for a community partition [k] = tts=1As, if there exists an algorithm that assigns to each node in G an element of {A1, . . . , At} that contains its true community7 with probability 1 − on(1). Exact recovery is solvable in SBM(n, p,W ) if it is solvable for the partition of [k] into k singletons, i.e., all communities can be recovered."
    }, {
      "heading" : "7.2.1 Formal results",
      "text" : "Definition 16. Let µ, ν be two positive measures on a discrete set X , i.e., two functions from X to R+. We define the CH-divergence between µ and ν by\nD+(µ, ν) := max t∈[0,1] ∑ x∈X ( tµ(x) + (1− t)ν(x)− µ(x)tν(x)1−t ) . (16)\nNote that for a fixed t,∑ x∈X ( tµ(x) + (1− t)ν(x)− µ(x)tν(x)1−t ) is an f -divergence. For t = 1/2, i.e., the gap between the arithmetic and geometric means, we have ∑\nx∈X tµ(x) + (1− t)ν(x)− µ(x)tν(x)1−t = 1 2 ‖√µ− √ ν‖22 (17)\n7Up to a relabelling of the communities.\nwhich is the Hellinger divergence (or distance), and the maximization over t of the part∑ x µ(x)\ntν(x)1−t is the exponential of the Chernoff divergence. We refer to Section 8.3 for further discussions on D+. Note also that we will often evaluate D+ as D+(x, y) where x, y are vectors instead of measures.\nDefinition 17. For the SBM G2(n, p,Q), where p has dimension k (i.e., there are k communities), the finest partition of [k] is the partition of [k] in to the largest number of subsets such that D+((PQ)i, (PQ)j) ≥ 1 for all i, j that are in different subsets.\nWe next present our main theorem for exact recovery. We first provide necessary and sufficient conditions for exact recovery of partitions, and then provide an agnostic algorithm that solves exact recovery efficiently, more precisely, in quasi-linear time.\nRecall that from [AS15] exact recovery is solvable in the stochastic block model G2(n, p,Q) for a partition [k] = tts=1As if and only if for all i and j in different subsets of the partition,\nD+((PQ)i, (PQ)j) ≥ 1, (18)\nwhere (PQ)i denotes the i-th row of the matrix PQ. In particular, exact recovery is solvable in G2(n, p,Q) if and only if mini,j∈[k],i 6=j D+((PQ)i, (PQ)j) ≥ 1.\nTheorem 7. Let k ∈ Z+ denote the number of communities, p ∈ (0, 1)k with |p| = 1 denote the community prior, P = diag(p), and let Q ∈ (0,∞)k×k symmetric with no two rows equal. For G ∼ G2(n, p,Q), the algorithm Agnostic-degree-profiling(G, ln lnn4 lnn ) recovers the finest partition, runs in o(n1+ ) time for all > 0, and does not need to know the parameters (it uses no input except the graph in question).\nNote that the second item in the theorem implies that Agnostic-degree-profiling solves exact recovery efficiently whenever the parameters p and Q allow for exact recovery to be solvable.\nRemark 1. If Qij = 0 for some i and j then the results above still hold, except that if for all i and j in different subsets of the partition,\nD+((PQ)i, (PQ)j) ≥ 1, (19)\nbut there exist i and j in different subsets of the partition such that D+((PQ)i, (PQ)j) = 1 and ((PQ)i,k ·(PQ)j,k ·((PQ)i,k−(PQ)j,k) = 0 for all k, then the optimal algorithm will have an assymptotically constant failure rate. The recovery algorithm also needs to be modified to accomodate 0’s in Q.\nThe algorithm Agnostic-degree-profiling is given in Section 3.1 and replicated below. The idea is to recover the communities with a two-step procedure, similarly to one of the algorithms used in [ABH14] for the two-community case. In the first step, we run Agnostic-sphere-comparison on a sparsified version of G2(n, p,Q) which has a slowly growing average degree. Hence, from Corollary 2, Agnostic-sphere-comparison recovers correctly a fraction of nodes that is arbitrarily close to 1 (w.h.p.). In the second step, we proceed to an improvement of the first step classification by making local checks for each node in the residue graph and deciding whether the node should be moved to another\ncommunity or not. This step requires solving a hypothesis testing problem for deciding the local degree profile of vertices in the SBM. The CH-divergence appears when resolving this problem, as the mis-classification error exponent. We present this result of self-interest in Section 7.2.2. The proof of Theorem 7 is given in Section 7.2.3.\nAgnostic-degree-profiling algorithm. Inputs: a graph G = ([n], E), and a splitting parameter γ ∈ [0, 1] (see Theorem 7 for the choice of γ). Output: Each node v ∈ [n] is assigned a community-list A(v) ∈ {A1, . . . , At}, where A1, . . . , At is intended to be the partition of [k] in to the largest number of subsets such that D+((pQ)i, (pQ)j) ≥ 1 for all i, j in [k] that are in different subsets. Algorithm: (1) Define the graph G′ on the vertex set [n] by selecting each edge in G independently with probability γ, and define the graph G′′ that contains the edges in G that are not in G′. (2) Run Agnostic-sphere-comparison on G′ to obtain the preliminary classification σ′ ∈ [k]n (see Section 7.1 and Corollary 2.) (3) Estimate p and Q based on the alleged communities’ sizes and the edge densities between them. (4) For each node v ∈ [n], determine in which community node v is most likely to belong to based on its degree profile computed from the preliminary classification σ′ and the estimates of p and Q (see Section 7.2.2), and call it σ′′v (5) Re-estimate p and Q based on the sizes of the communities claimed by σ′′ and the edge densities between them. (6) For each node v ∈ [n], determine in which group A1, . . . , At node v is most likely to belong to based on its degree profile computed from the preliminary classification σ′′ and the new estimates of P and Q (see Section 7.2.2)."
    }, {
      "heading" : "7.2.2 Testing degree profiles",
      "text" : "In this section, we consider the problem of deciding which community a node in the SBM belongs to based on its degree profile. The notions are the same as in [AS15], we repeat them to ease the reading and explain how the Agnostic-degree-profiling algorithm works.\nDefinition 18. The degree profile of a node v ∈ [n] for a partition of the graph’s vertices into k communities is the vector d(v) ∈ Zk+, where the j-th component dj(v) counts the number of edges between v and the vertices in community j. Note that d(v) is equal to N1(v) as defined in Definition 8.\nFor G ∼ G2(n, p,Q), community i ∈ [k] has a relative size that concentrates exponentially fast to pi. Hence, for a node v in community j, d(v) is approximately given by ∑ i∈[k]Xijei, where Xij are independent and distributed as Bin(npi, ln(n)Qi,j/n), and where Bin(a, b) denotes8 the binomial distribution with a trials and success probability b. Moreover, the Binomial is well-enough approximated by a Poisson distribution of the same mean in this\n8Bin(a, b) refers to Bin(bac, b) if a is not an integer.\nregime. In particular, Le Cam’s inequality gives∥∥∥∥Bin(na, ln(n)n b ) − P (ab ln(n)) ∥∥∥∥ TV ≤ 2ab 2 ln2(n) n , (20)\nhence, by the additivity of Poisson distribution and the triangular inequality,\n‖µd(v) − P(ln(n) ∑ i∈[k] piQi,jei)‖TV = O ( ln2(n) n ) . (21)\nWe will rely on a simple one-sided bound (see (44)) to approximate our events under the Poisson measure.\nConsider now the following problem. Let G be drawn under the G2(n, p,Q) SBM and assume that the planted partition is revealed except for a given vertex. Based on the degree profile of that vertex, is it possible to classify the vertex correctly with high probability? We have to resolve a hypothesis testing problem, which involves multivariate Poisson distributions in view of the previous observations. We next study this problem.\nTesting multivariate Poisson distributions. Consider the following Bayesian hypothesis testing problem with k hypotheses. The random variable H takes values in [k] with P{H = j} = pj (this is the a priori distribution of H). Under H = j, an observed random variable D is drawn from a multivariate Poisson distribution with mean λ(j) ∈ Rk+, i.e.,\nP{D = d|H = j} = Pλ(j)(d), d ∈ Zk+, (22)\nwhere\nPλ(j)(d) = ∏ i∈[k] Pλi(j)(di), (23)\nand\nPλi(j)(di) = λi(j)\ndi\ndi! e−λi(j). (24)\nIn other words, D has independent Poisson entries with different means. We use the following notation to summarize the above setting:\nD|H = j ∼ P(λ(j)), j ∈ [k]. (25)\nOur goal is to infer the value of H by observing a realization of D. To minimize the error probability given a realization of D, we must pick the most likely hypothesis conditioned on this realization, i.e.,\nargmaxj∈[k]P{D = d|H = j}pj , (26)\nwhich is the Maximum A Posteriori (MAP) decoding rule.9 To resolve this maximization, we can proceed to a tournament of k − 1 pairwise comparisons of the hypotheses. Each comparison allows us to eliminate one candidate for the maxima, i.e.,\nP{D = d|H = i}pi > P{D = d|H = j}pj ⇒ H 6= j. (27)\n9Ties can be broken arbitrarily.\nThe error probability Pe of this decoding rule is then given by, Pe = ∑ i∈[k] P{D ∈ Bad(i)|H = i}pi, (28)\nwhere Bad(i) is the region in Zk+ where i is not maximizing (26). Moreover, for any i ∈ [k], P{D ∈ Bad(i)|H = i} ≤ ∑ j 6=i P{D ∈ Badj(i)|H = i} (29)\nwhere Badj(i) is the region in Zk+ where P{D = x|H = i}pi ≤ P{D = x|H = j}pj . Note that with this upper-bound, we are counting the overlap regions where P{D = x|H = i}pi ≤ P{D = x|H = j}pj for different j’s multiple times, but no more than k − 1 times. Hence,∑\nj 6=i P{D ∈ Badj(i)|H = i} ≤ (k − 1)P{D ∈ Bad(i)|H = i}. (30)\nPutting (28) and (29) together, we have Pe ≤ ∑ i 6=j P{D ∈ Badj(i)|H = i}pi, (31)\n= ∑ i<j ∑ d∈Zk+ min(P{D = d|H = i}pi,P{D = d|H = j}pj) (32)\nand from (30),\nPe ≥ 1 k − 1 ∑ i<j ∑ d∈Zk+ min(P{D = d|H = i}pi,P{D = d|H = j}pj). (33)\nTherefore the error probability Pe can be controlled by estimating the terms ∑\nd∈Zk+ min(P{D =\nd|H = i}pi,P{D = d|H = j}pj). In our case, recall that\nP{D = d|H = i} = Pλ(i)(d), (34)\nwhich is a multivariate Poisson distribution. In particular, we are interested in the regime where k is constant and λ(i) = ln(n)ci, ci ∈ Rk+, and n diverges. Due to (32), (33), we can then control the error probability by controlling ∑ x∈Zk+\nmin(Pln(n)ci(x)pi,Pln(n)cj (x)pj), which we will want to be o(1/n) to classify vertices in the SBM correctly with high probability based on their degree profiles (see next section). The following lemma provides the relevant estimates.\nLemma 15. For any c1, c2 ∈ (R+ \\ {0})k with c1 6= c2 and p1, p2 ∈ R+ \\ {0},∑ x∈Zk+ min(Pln(n)c1(x)p1,Pln(n)c2(x)p2) = O ( n −D+(c1,c2)− ln ln(n)2 ln(n) ) , (35)\n∑ x∈Zk+ min(Pln(n)c1(x)p1,Pln(n)c2(x)p2) = Ω ( n −D+(c1,c2)− k ln ln(n)2 ln(n) ) , (36)\nwhere D+(c1, c2) is the CH-divergence as defined in (16).\nIn other words, the CH-divergence provides the error exponent for deciding among multivariate Poisson distributions. We did not find this result in the literature, but found a similar result obtained by Verdú [Ver86], who shows that the Hellinger distance (the special case with t = 1/2 instead of the maximization over t) appears in the error exponent for testing Poisson point-processes, although [Ver86] does not investigate the exact error exponent. This lemma was proven in [AS15].\nThis lemma together with previous bounds on Pe imply that if D+(ci, cj) > 1 for all i 6= j, the true hypothesis is correctly recovered with probability o(1/n). However, it may be that D+(ci, cj) > 1 only for a subset of (i, j)-pairs. What can we then infer? While we may not recover the true value of H with probability o(1/n), we may narrow down the search within a subset of possible hypotheses with that probability of error.\nTesting composite multivariate Poisson distributions. We now consider the previous setting, but we are no longer interested in determining the true hypothesis, but in deciding between two (or more) disjoint subsets of hypotheses. Under hypothesis 1, the distribution of D belongs to a set of possible distributions, namely P(λi) where i ∈ A, and under hypothesis 2, the distribution of D belongs to another set of distributions, namely P(λi) where i ∈ B. Note that A and B are disjoint subsets such that A ∪B = [k]. In short,\nD|H̃ = 1 ∼ P(λi), for some i ∈ A, (37) D|H̃ = 2 ∼ P(λi), for some i ∈ B, (38)\nand as before the prior on λi is pi. To minimize the probability of deciding the wrong hypothesis upon observing a realization of D, we must pick the hypothesis which leads to the larger probability between P{H̃ ∈ A|D = d} and P{H̃ ∈ B|D = d}, or equivalently,∑\ni∈A Pλ(i)(d)pi ≥ ∑ i∈B Pλ(i)(d)pi ⇒ H̃ = 1, (39)∑ i∈A Pλ(i)(d)pi < ∑ i∈B Pλ(i)(d)pi ⇒ H̃ = 2. (40)\nIn other words, the problem is similar to the previous one, using the above mixture distributions. If we denote by P̃e the probability of making an error with this test, we have\nP̃e = ∑ x∈Zk+ min (∑ i∈A Pλ(i)(x)pi, ∑ i∈B Pλ(i)(x)pi ) . (41)\nMoreover, applying bounds on the minima of two sums, P̃e ≤ ∑ x∈Zk+ ∑ i∈A,j∈B min ( Pλ(i)(x)pi,Pλ(j)(x)pj ) , (42)\nP̃e ≥ 1 |A||B| ∑ x∈Zk+ ∑ i∈A,j∈B min ( Pλ(i)(x)pi,Pλ(j).(x)pj ) . (43)\nTherefore, for constant k and λ(i) = ln(n)ci, ci ∈ Rk+, with n diverging, it suffices to control the decay of ∑ x∈Zk+ min(Pλ(i)(x)pi,Pλ(j)(x)pj) when i ∈ A and j ∈ B, in order to bound\nthe error probability of deciding whether a vertex degree profile belongs to a group of communities or not.\nThe same reasoning can be applied to the problem of deciding whether a given node belongs to a group of communities, with more than two groups. Also, for any p and p′ such that |pj − p′j | < lnn/ √ n for each j, Q, γ(n), and i,\n∑ x∈Zk+ max ( Bin( np′, (1−γ(n)) ln(n) n Qi )(x)− 2PPQi(1−γ(n)) ln(n)/n(x), 0 ) = O(1/n2). (44)\nSo, the error rate for any algorithm that classifies vertices based on their degree profile in a graph drawn from a sparse SBM is at most O(1/n2) more than twice what it would be if the probability distribution of degree profiles really was the poisson distribution.\nIn summary, we have proved the following.\nLemma 16. Let k ∈ Z+ and let A1, . . . , At be disjoint subsets of [k] such that ∪ti=1Ai = [k]. Let G be a random graph drawn under G2(n, p, (1 − γ(n))Q). Assigning the most likely community subset Ai to a node v based on its degree profile d(v) gives the correct assignment with probability\n1−O ( n−(1−γ(n))∆− 1 2 ln((1−γ(n)) lnn)/ lnn + 1\nn2\n) ,\nwhere\n∆ = min r,s∈[t] r 6=s min i∈Ar,j∈As D+((pQ)i, (pQ)j). (45)\nMoreover, in order to prove Theorem 7, we will need a version of this lemma that still holds when some of our information is inaccurate. First of all, consider attempting the previous testing procedure when one thinks the distributions are λ′ with probability p′ instead of λ with probability p. Assume that there exists such that for all i and j, |λ′(i)j−λ(i)j | ≤ min(λ′(i)j , λ(i)j) and |p′i−pi| ≤ min(p′i, pi). Then for any i ∈ A, x ∈ Zk+, the previous hypothesis testing procedure will not classify x as being in B unless∑\nj∈B (1 + )|x|+1Pλ(j)(x)pj ≥ (1 + )−|x|−1Pλ(i)(x)pi\nThat means that for any i ∈ A, x ∈ Zk+, the probability that x arises from Pλ(i) and is then misclassified as being in B is at most\nPλ(i)(x)pi ≤ ∑ j∈B (1 + )2|x|+2Pλ(j)(x)pj\nSo, this hypothesis testing procedure has an error probability of at most∑ x∈Zk+ (1 + )2|x|+2 ∑ i∈A,j∈B min ( Pλ(i)(x)pi,Pλ(j)(x)pj ) Furthermore,\nLemma 17. For any p and Q there exists c such that the following holds for all δ < min pi/2. Let G be a random graph drawn from G2(n, p,Q), and σ′ be a classification of G’s vertices that misclassifies δn vertices. Also, let p′ be the frequencies with which vertices are classified as being in the communities, and Q′ be n/ lnn times the edge densities between the alleged communities. Then with probability 1−o(1), every element of p′ or Q′ is within cδ+ √ lnn/n of the corresponding element of p or Q.\nProof. With probability 1 − o(1), the size of every community is within √\nlnn/n of its expected value, and the edge density between each pair of communities is within √ n lnn of its expected value. Also, there exists a constant c′ such that with probability 1− o(1), no vertex has degree more than c′ lnn. Assuming this is the case, the misclassification of vertices can not change the apparent number of edges between any two communities by more than c′δn lnn, and it can not change the apparent size of any community by more than δn. Thus, |p′i − pi| ≤ √ lnn/n+ δ and\n|Q′i,j −Qi,j | ≤ √ lnn/n+ 4 c′δ\npi · pj + 4Qi,j\nδpi + δpj + δ 2\npipj\nThis lets us prove the following “robust” version of this lemma to prove Theorem 7.\nLemma 18. Let k ∈ Z+ and let A1, . . . , At be disjoint subsets of [k] such that ∪ti=1Ai = [k]. Let G be a random graph drawn under G2(n, p, (1 − γ(n))Q). There exist c1, c2, and c3 such that with probability 1− o(1) G is such that for any sufficiently small δ, assigning the most likely community subset Ai to a node v based on a distortion of its degree profile that independently gets each node’s community wrong with probability at most δ and estimates of p and Q based on a classification of the graphs vertices that misclassifies each vertex with probability δ gives the correct assignment with probability at least\n1− c2 · (1 + c1δ)c3 lnn · ( n−(1−γ(n))∆− 1 2 ln((1−γ(n)) lnn)/ lnn ) − c2 n2 ,\nwhere\n∆ = min r,s∈[t] r 6=s min i∈Ar,j∈As D+((pQ)i, (pQ)j). (46)\nProof. First, note that by the previous lemma, with probability 1 − o(1), G is such that every element of the estimates of p and Q is within cδ + √ lnn/n of its true value. Choose c3 such that every vertex in the graph has degree less than c3 lnn with probability 1− o(1). If this holds, then the probability of misclassifying a vertex based on its true degree profile and the estimates of its parameters is at most\n2(1 + 2cδ/min(pi, (PQ)i,j)) 2c3 lnn+2 · ( n−(1−γ(n))∆− 1 2 ln((1−γ(n)) lnn)/ lnn ) +O ( 1\nn2 ) based on the previous bounds on the probability that classifying a vertex based on its degree profile fails.\nNow, let\nc′1 = max i,j\n∑ pi′qi′,j/(piqi,j).\nThe key observation is that v’s mth neighbor had at least a mini,j(piqi,j)/ ∑ pi′qi′,j chance of actually being in community σ for each σ, so its probability of being reported as being in community σ is at most 1 + c′1δ times the probability that it actually is. So, the probability that its reported degree profile is bad is at most (1 + c′1δ)\n|N1(v)| times the probability that its actual degree profile is bad. The conclusion follows."
    }, {
      "heading" : "7.2.3 Proof of Theorem 7",
      "text" : "We prove the possibility result here. The converse was proven in [AS15] for known parameters, hence also applies here.\nLet G ∼ G2(n, p,Q) and γ = ln lnn4 lnn , and A1, . . . , At be a partition of [k]. Agnostic-degreeprofiling(G, p,Q, γ) recovers the partition [k] = tts=1As with probability 1− on(1) if for all i, j in [k] that are in different subsets,\nD+((PQ)i, (PQ)j) ≥ 1. (47)\nThe idea behind Claim 1 is contained in Lemma 16. However, there are several technical steps that need to be handled:\n1. The graphs G′ and G′′ obtained in step 1 of the algorithm are correlated, since an edge cannot be both in G′ and G′′. However, this effect can be discarded since two independent versions would share edges with low enough probability.\n2. The classification in step 2 using Agnostic-sphere-comparison has a vanishing fraction of vertices which are wrongly labeled, and the SBM parameters are unknown. This requires using the robust version of Lemma 16, namely Lemma 18.\n3. In the case where D+((PQ)i, (PQ)j) = 1 a more careful classification is needed as carried in steps 3 and 4 of the algorithm.\nProof. With probability 1−O(1/n), no vertex in the graph has degree greater than c3 lnn. Assuming that this holds, no vertex’s set of neighbors in G′′ is more than\n(1−max qi,j lnn/n)−c3 lnn · (n/(n− c3 lnn))c3 lnn = 1 + o(1)\ntimes as likely to occur as it would be if G′′ were independent of G′. So, the fact that they are not has negligible impact on the algorithm’s error rate. Now, let\nδ = (e (1−γ) 2c3 mini6=j D+((PQ)i,(PQ)j) − 1)/c1.\nBy Lemma 18, if the classification in step 2 has an error rate of at most δ, then the classification in step 3 has an error rate of\nO(n−(1−γ) mini 6=j D+((PQ)i,(PQ)j)/2 + 1/n2),\nobserving that if σ′v 6= σv the error rate of σ′′v′ for v′ adjacent to v is at worst multiplied by a constant. That in turn ensures that the final classification has an error rate of at most\nO ( (1 +O(n−(1−γ) mini 6=j D+((PQ)i,(PQ)j)/2 + 1/n2))c3 lnn 1\nn lnn−1/4\n) = O ( 1\nn lnn−1/4\n) ."
    } ],
    "references" : [ {
      "title" : "Mixed membership stochastic blockmodels",
      "author" : [ "E.M. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing" ],
      "venue" : "J. Mach. Learn. Res",
      "citeRegEx" : "Airoldi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Airoldi et al\\.",
      "year" : 2008
    }, {
      "title" : "Exact recovery in the stochastic block model, Available at ArXiv:1405.3267",
      "author" : [ "E. Abbe", "A.S. Bandeira", "G. Hall" ],
      "venue" : null,
      "citeRegEx" : "Abbe et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Abbe et al\\.",
      "year" : 2014
    }, {
      "title" : "The political blogosphere and the 2004 u.s. election: Divided they blog",
      "author" : [ "Lada A. Adamic", "Natalie Glance" ],
      "venue" : "Proceedings of the 3rd International Workshop on Link Discovery (New York, NY, USA), LinkKDD ’05,",
      "citeRegEx" : "Adamic and Glance,? \\Q2005\\E",
      "shortCiteRegEx" : "Adamic and Glance",
      "year" : 2005
    }, {
      "title" : "Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms",
      "author" : [ "E. Abbe", "C. Sandon" ],
      "venue" : null,
      "citeRegEx" : "Abbe and Sandon,? \\Q2015\\E",
      "shortCiteRegEx" : "Abbe and Sandon",
      "year" : 2015
    }, {
      "title" : "Random laplacian matrices and convex relaxations",
      "author" : [ "A.S. Bandeira" ],
      "venue" : null,
      "citeRegEx" : "Bandeira,? \\Q2015\\E",
      "shortCiteRegEx" : "Bandeira",
      "year" : 2015
    }, {
      "title" : "A nonparametric view of network models and newmangirvan and other modularities",
      "author" : [ "P.J. Bickel", "A. Chen" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "Bickel and Chen,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel and Chen",
      "year" : 2009
    }, {
      "title" : "Graph bisection algorithms with good average case",
      "author" : [ "T.N. Bui", "S. Chaudhuri", "F.T. Leighton", "M. Sipser" ],
      "venue" : "behavior, Combinatorica",
      "citeRegEx" : "Bui et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Bui et al\\.",
      "year" : 1987
    }, {
      "title" : "Private graphon estimation for sparse graphs, In preparation",
      "author" : [ "C. Borgs", "J. Chayes", "A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Borgs et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Borgs et al\\.",
      "year" : 2015
    }, {
      "title" : "Achieving exact cluster recovery threshold via semidefinite programming, arXiv:1412.6156",
      "author" : [ "J. Xu B. Hajek", "Y. Wu" ],
      "venue" : null,
      "citeRegEx" : "Hajek and Wu,? \\Q2014\\E",
      "shortCiteRegEx" : "Hajek and Wu",
      "year" : 2014
    }, {
      "title" : "Eigenvalues and graph bisection: An average-case analysis",
      "author" : [ "R.B. Boppana" ],
      "venue" : "In 28th Annual Symposium on Foundations of Computer Science",
      "citeRegEx" : "Boppana,? \\Q1987\\E",
      "shortCiteRegEx" : "Boppana",
      "year" : 1987
    }, {
      "title" : "Achieving optimal misclassification proportion in stochastic block",
      "author" : [ "A.Y. Zhang H.H. Zhou C. Gao", "Z. Ma" ],
      "venue" : null,
      "citeRegEx" : "Gao and Ma,? \\Q2015\\E",
      "shortCiteRegEx" : "Gao and Ma",
      "year" : 2015
    }, {
      "title" : "Hill-climbing finds random planted bisections",
      "author" : [ "T. Carson", "R. Impagliazzo" ],
      "venue" : "Proc. 12th Symposium on Discrete Algorithms (SODA 01), ACM press,",
      "citeRegEx" : "Carson and Impagliazzo,? \\Q2001\\E",
      "shortCiteRegEx" : "Carson and Impagliazzo",
      "year" : 2001
    }, {
      "title" : "Algorithms for graph partitioning on the planted partition model, Lecture",
      "author" : [ "A. Condon", "R.M. Karp" ],
      "venue" : "Notes in Computer Science",
      "citeRegEx" : "Condon and Karp,? \\Q1999\\E",
      "shortCiteRegEx" : "Condon and Karp",
      "year" : 1999
    }, {
      "title" : "Graph partitioning via adaptive spectral techniques, Comb",
      "author" : [ "A. Coja-oghlan" ],
      "venue" : "Probab. Comput",
      "citeRegEx" : "Coja.oghlan,? \\Q2010\\E",
      "shortCiteRegEx" : "Coja.oghlan",
      "year" : 2010
    }, {
      "title" : "Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery",
      "author" : [ "P. Chin", "A. Rao", "V. Vu" ],
      "venue" : null,
      "citeRegEx" : "Chin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chin et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic blockmodels with a growing number of classes",
      "author" : [ "D.S. Choi", "P.J. Wolfe", "E.M. Airoldi" ],
      "venue" : null,
      "citeRegEx" : "Choi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2012
    }, {
      "title" : "Detecting functional modules in the yeast proteinprotein interaction",
      "author" : [ "J. Chen", "B. Yuan" ],
      "venue" : "network, Bioinformatics",
      "citeRegEx" : "Chen and Yuan,? \\Q2006\\E",
      "shortCiteRegEx" : "Chen and Yuan",
      "year" : 2006
    }, {
      "title" : "The solution of some random NP-hard problems in polynomial expected time",
      "author" : [ "M.E. Dyer", "A.M. Frieze" ],
      "venue" : "Journal of Algorithms",
      "citeRegEx" : "Dyer and Frieze,? \\Q1989\\E",
      "shortCiteRegEx" : "Dyer and Frieze",
      "year" : 1989
    }, {
      "title" : "Statistical analysis of multiple sociometric relations, Journal of The American Statistical Association",
      "author" : [ "S.E. Fienberg", "M.M. Meyer", "S.S. Wasserman" ],
      "venue" : null,
      "citeRegEx" : "Fienberg et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Fienberg et al\\.",
      "year" : 1985
    }, {
      "title" : "Efficient discovery of overlapping communities in massive networks",
      "author" : [ "P.K. Gopalan", "D.M. Blei" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "Gopalan and Blei,? \\Q2013\\E",
      "shortCiteRegEx" : "Gopalan and Blei",
      "year" : 2013
    }, {
      "title" : "Community structure in social and biological networks",
      "author" : [ "M. Girvan", "M.E.J. Newman" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "Girvan and Newman,? \\Q2002\\E",
      "shortCiteRegEx" : "Girvan and Newman",
      "year" : 2002
    }, {
      "title" : "Community detection in sparse networks via Grothendieck’s inequality",
      "author" : [ "O. Guédon", "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "Guédon and Vershynin,? \\Q2014\\E",
      "shortCiteRegEx" : "Guédon and Vershynin",
      "year" : 2014
    }, {
      "title" : "The metropolis algorithm for graph bisection",
      "author" : [ "Mark Jerrum", "Gregory B. Sorkin" ],
      "venue" : "Discrete Applied Mathematics",
      "citeRegEx" : "Jerrum and Sorkin,? \\Q1998\\E",
      "shortCiteRegEx" : "Jerrum and Sorkin",
      "year" : 1998
    }, {
      "title" : "Cluster analysis for gene expression data: a survey, Knowledge and Data Engineering",
      "author" : [ "D. Jiang", "C. Tang", "A. Zhang" ],
      "venue" : "IEEE Transactions on",
      "citeRegEx" : "Jiang et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2004
    }, {
      "title" : "Stochastic blockmodels and community structure in networks",
      "author" : [ "B. Karrer", "M.E.J. Newman" ],
      "venue" : "Phys. Rev. E",
      "citeRegEx" : "Karrer and Newman,? \\Q2011\\E",
      "shortCiteRegEx" : "Karrer and Newman",
      "year" : 2011
    }, {
      "title" : "Statistical properties of community structure in large social and information networks",
      "author" : [ "J. Leskovec", "K.J. Lang", "A. Dasgupta", "M.W. Mahoney" ],
      "venue" : "Proceedings of the 17th international conference on World Wide Web (New York, NY, USA),",
      "citeRegEx" : "Leskovec et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Leskovec et al\\.",
      "year" : 2008
    }, {
      "title" : "Community detection thresholds and the weak Ramanujan property",
      "author" : [ "L. Massoulié" ],
      "venue" : "STOC 2014: 46th Annual Symposium on the Theory of Computing (New York, United States),",
      "citeRegEx" : "Massoulié,? \\Q2014\\E",
      "shortCiteRegEx" : "Massoulié",
      "year" : 2014
    }, {
      "title" : "Spectral partitioning of random graphs",
      "author" : [ "F. McSherry" ],
      "venue" : "Annual Symposium on Foundations of Computer Science",
      "citeRegEx" : "McSherry,? \\Q2001\\E",
      "shortCiteRegEx" : "McSherry",
      "year" : 2001
    }, {
      "title" : "Stochastic block models and reconstruction, Available online at arXiv:1202.1499",
      "author" : [ "E. Mossel", "J. Neeman", "A. Sly" ],
      "venue" : null,
      "citeRegEx" : "Mossel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mossel et al\\.",
      "year" : 2012
    }, {
      "title" : "Detecting protein function and protein-protein interactions from genome",
      "author" : [ "E.M. Marcotte", "M. Pellegrini", "H.-L. Ng", "D.W. Rice", "T.O. Yeates", "D. Eisenberg" ],
      "venue" : "sequences, Science",
      "citeRegEx" : "Marcotte et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Marcotte et al\\.",
      "year" : 1999
    }, {
      "title" : "Inference of Population Structure Using Multilocus Genotype",
      "author" : [ "J.K. Pritchard", "M. Stephens", "P. Donnelly" ],
      "venue" : "Data, Genetics",
      "citeRegEx" : "Pritchard et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Pritchard et al\\.",
      "year" : 2000
    }, {
      "title" : "Regularized spectral clustering under the degree-corrected stochastic blockmodel, Advances in Neural Information Processing Systems",
      "author" : [ "T. Qin", "K. Rohe" ],
      "venue" : null,
      "citeRegEx" : "Qin and Rohe,? \\Q2013\\E",
      "shortCiteRegEx" : "Qin and Rohe",
      "year" : 2013
    }, {
      "title" : "Spectral clustering and the high-dimensional stochastic blockmodel",
      "author" : [ "K. Rohe", "S. Chatterjee", "B. Yu" ],
      "venue" : "The Annals of Statistics",
      "citeRegEx" : "Rohe et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rohe et al\\.",
      "year" : 2011
    }, {
      "title" : "Image processing, analysis, and machine vision, Thomson-Engineering",
      "author" : [ "M. Sonka", "V. Hlavac", "R. Boyle" ],
      "venue" : null,
      "citeRegEx" : "Sonka et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Sonka et al\\.",
      "year" : 2007
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "Shi and Malik,? \\Q1997\\E",
      "shortCiteRegEx" : "Shi and Malik",
      "year" : 1997
    }, {
      "title" : "Estimation and Prediction for Stochastic Blockmodels for Graphs with Latent Block Structure",
      "author" : [ "T.A.B. Snijders", "K. Nowicki" ],
      "venue" : "Journal of Classification",
      "citeRegEx" : "Snijders and Nowicki,? \\Q1997\\E",
      "shortCiteRegEx" : "Snijders and Nowicki",
      "year" : 1997
    }, {
      "title" : "Exploring complex networks, Nature",
      "author" : [ "S.H. Strogatz" ],
      "venue" : null,
      "citeRegEx" : "Strogatz,? \\Q2001\\E",
      "shortCiteRegEx" : "Strogatz",
      "year" : 2001
    }, {
      "title" : "Asymptotic error probability of binary hypothesis testing for poisson point-process observations (corresp.)",
      "author" : [ "S. Verdú" ],
      "venue" : "Information Theory, IEEE Transactions on",
      "citeRegEx" : "Verdú,? \\Q1986\\E",
      "shortCiteRegEx" : "Verdú",
      "year" : 1986
    }, {
      "title" : "A simple svd algorithm for finding hidden partitions, Available online at arXiv:1404.3918",
      "author" : [ "V. Vu" ],
      "venue" : null,
      "citeRegEx" : "Vu,? \\Q2014\\E",
      "shortCiteRegEx" : "Vu",
      "year" : 2014
    }, {
      "title" : "Stochastic blockmodels for directed graphs",
      "author" : [ "Y.J. Wang", "G.Y. Wong" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "Wang and Wong,? \\Q1987\\E",
      "shortCiteRegEx" : "Wang and Wong",
      "year" : 1987
    }, {
      "title" : "Jointly clustering rows and columns of binary matrices: Algorithms and trade-offs, SIGMETRICS Perform",
      "author" : [ "J. Xu", "R. Wu", "K. Zhu", "B. Hajek", "R. Srikant", "L. Ying" ],
      "venue" : "Eval. Rev",
      "citeRegEx" : "Xu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices",
      "author" : [ "J. Xu Y. Chen" ],
      "venue" : null,
      "citeRegEx" : "Chen,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen",
      "year" : 2014
    }, {
      "title" : "Accurate community detection in the stochastic block model via spectral algorithms",
      "author" : [ "S. Yun", "A. Proutiere" ],
      "venue" : null,
      "citeRegEx" : "Yun and Proutiere,? \\Q2014\\E",
      "shortCiteRegEx" : "Yun and Proutiere",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Bandeira, and G. Hall, Exact recovery in the stochastic block model, Available at ArXiv:1405.3267. (2014). 1, 4, 50",
      "startOffset" : 0,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Bandeira, Random laplacian matrices and convex relaxations, arXiv:1504.03987 (2015). 1, 4",
      "startOffset" : 0,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : "Chen, A nonparametric view of network models and newmangirvan and other modularities, Proceedings of the National Academy of Sciences (2009). 1",
      "startOffset" : 0,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "Boppana, Eigenvalues and graph bisection: An average-case analysis, In 28th Annual Symposium on Foundations of Computer Science (1987), 280–285.",
      "startOffset" : 0,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "Coja-oghlan, Graph partitioning via adaptive spectral techniques, Comb. Probab. Comput. 19 (2010), no.",
      "startOffset" : 0,
      "endOffset" : 98
    }, {
      "referenceID" : 38,
      "context" : "Vu, Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery, arXiv:1501.05021 (2015). 2",
      "startOffset" : 0,
      "endOffset" : 149
    }, {
      "referenceID" : 41,
      "context" : "Chen, S. Sanghavi, and H. Xu, Clustering Sparse Graphs, arXiv:1210.3335 (2012). 1",
      "startOffset" : 0,
      "endOffset" : 79
    }, {
      "referenceID" : 41,
      "context" : "Chen and B. Yuan, Detecting functional modules in the yeast proteinprotein interaction network, Bioinformatics 22 (2006), no.",
      "startOffset" : 0,
      "endOffset" : 121
    }, {
      "referenceID" : 27,
      "context" : "McSherry, Spectral partitioning of random graphs, In 42nd Annual Symposium on Foundations of Computer Science (2001), 529–537.",
      "startOffset" : 0,
      "endOffset" : 117
    }, {
      "referenceID" : 36,
      "context" : "Strogatz, Exploring complex networks, Nature 410 (2001), no.",
      "startOffset" : 0,
      "endOffset" : 56
    }, {
      "referenceID" : 37,
      "context" : "Verdú, Asymptotic error probability of binary hypothesis testing for poisson point-process observations (corresp.), Information Theory, IEEE Transactions on 32 (1986), no.",
      "startOffset" : 0,
      "endOffset" : 167
    }, {
      "referenceID" : 38,
      "context" : "[Vu14] V. Vu, A simple svd algorithm for finding hidden partitions, Available online at arXiv:1404.3918 (2014). 1, 2",
      "startOffset" : 1,
      "endOffset" : 111
    }, {
      "referenceID" : 41,
      "context" : "Chen, Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices, arXiv:1402.1267 (2014). 1",
      "startOffset" : 0,
      "endOffset" : 163
    } ],
    "year" : 2015,
    "abstractText" : "Most recent developments on the stochastic block model (SBM) rely on the knowledge of the model parameters, or at least on the number of communities. This paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in [AS15] for linear size communities. The results are three-fold: (i) in the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and detects communities with an optimal accuracy scaling for large degrees; (ii) in the regime where degrees are scaled by ω(1) (diverging degrees), this is enhanced into a fully agnostic algorithm that only takes the graph in question and simultaneously learns the model parameters (including the number of communities) and detects communities with accuracy 1− o(1), with an overall quasi-linear complexity; (iii) in the logarithmic degree regime, an agnostic algorithm is developed that learns the parameters and achieves the optimal CH-limit for exact recovery, in quasi-linear time. These provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the general SBM with linear size communities. ∗Program in Applied and Computational Mathematics, and EE department, Princeton University, Princeton, USA, eabbe@princeton.edu. This research was partially supported by the 2014 Bell Labs Prize. †Department of Mathematics, Princeton University, USA, sandon@princeton.edu. ar X iv :1 50 6. 03 72 9v 1 [ m at h. PR ] 1 1 Ju n 20 15",
    "creator" : "LaTeX with hyperref package"
  }
}