{
  "name" : "1509.02597.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Asynchronous Distributed ADMM for Large-Scale Optimization- Part I: Algorithm and Convergence Analysis",
    "authors" : [ "Tsung-Hui Chang", "Mingyi Hong", "Wei-Cheng Liao", "Xiangfeng Wang" ],
    "emails" : [ "tsunghui.chang@ieee.org.", "mingyi@iastate.edu", "liaox146@umn.edu", "xfwang@sei.ecnu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n02 59\n7v 2\n[ cs\n.D C\n] 1\n9 Fe\nb 20\nKeywords− Distributed optimization, ADMM, Asynchronous, Consensus optimization\nPart of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1]. Tsung-Hui Chang is supported by NSFC, China, Grant No. 61571385. Mingyi Hong is supported by NFS Grant No. CCF-1526078 , and AFOSR, Grant No. 15RT0767. Xiangfeng Wang is supported by Shanghai YangFan No. 15YF1403400 and NSFC No. 11501210.\n⋆Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172. E-mail: tsunghui.chang@ieee.org.\n†Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu\n†Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: liaox146@umn.edu\n‡Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, School of Computer Science and Software Engineering, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn\nFebruary 22, 2016 DRAFT\n2 I. INTRODUCTION"
    }, {
      "heading" : "A. Background",
      "text" : "Scaling up optimization algorithms for future data-intensive applications calls for efficient distributed and parallel implementations, so that modern multi-core high performance computing technologies can be fully utilized [2]–[4]. In this work, we are interested in developing distributed optimization methods for solving the following optimization problem\nmin x∈Rn\nN∑\ni=1\nfi(x) + h(x), (1)\nwhere each fi : Rn → R is a (smooth) cost function; h : Rn → R∪{∞} is a convex ( proper and lower semi-continuous) but possibly non-smooth regularization function. The latter is used to impose desired structures on the solution (e.g., sparsity) and/or used to enforce certain constraints. Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8]. In this paper, we focus on solving large-scale instances of these learning problems with either a large number of training samples or a large number of features (n is large) [3]. These are typical data-intensive machine learning scenarios in which the data sets are often distributedly located in a few computing nodes. Traditional centralized optimization methods, therefore, fails to scale well due to their inability to handle distributed data sets and computing resources.\nOur goal is to develop efficient distributed optimization algorithms over a computer network with a star topology, in which a master node coordinates the computation of a set of distributed workers (see Figure 1 for illustration). Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]–[16]. For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]–[17] parallelized the block coordinate descent (BCD) method. In these works, the distributed workers iteratively calculate the gradients related to their local data, while the master collects such information from the workers to perform SGD, PG or BCD updates.\nHowever, when scaling up these distributed algorithms, node synchronization becomes an important issue. Specifically, under the synchronous protocol, the master is triggered at each iteration only if it receives the required information from all the distributed workers. On the one hand, such synchronization is beneficial to make the algorithms well behaved; on the other hand, however, the speed of the algorithms\nFebruary 22, 2016 DRAFT\nwould be limited by the “slowest” worker especially when the workers have different computation and communication delays. To address such dilemma, a few recent works [10]–[14] have introduced “asynchrony” into the distributed algorithms, which allows the master to perform updates when not all, but a small subset of workers have returned their gradient information. The asynchronous updates would cause “delayed” gradient information. A few algorithmic tricks such as delay-dependent step-size selection have been introduced to ensure that the staled gradient information does not destroy the stability of the algorithm. In practice, such asynchrony does make a big difference. As has been consistently reported in [10]–[14], under such an asynchronous protocol, the computation time can decrease almost linearly with the number of workers."
    }, {
      "heading" : "B. Related Works",
      "text" : "A different approach for distributed and parallel optimization is based on the alternating direction method of multipliers (ADMM) [9, Section 7.1.1]. In the distributed ADMM, the original learning problem is partitioned into N subproblems, each containing a subset of training samples or the learning parameters. At each iteration, the workers solve the subproblems and send the up-to-date variable information to the master, who summarizes this information and broadcasts the result to the workers. In this way, a given large-scale learning problem can be solved in a parallel and distributed fashion. Notably, other than the standard convex setting [9], the recent analysis in [18] has shown that such distributed ADMM is provably convergent to a Karush-Kuhn-Tucker (KKT) point even for non-convex problems.\nRecently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]–[14]. Specifically, reference [19] has considered a version of AD-ADMM with bounded delay assumption and studied its theoretical and numerical performances. However, only convex cases are considered in [19]. Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]–[14], the workers compute gradient information only. This type of distributed optimization schemes, however, may not fully utilize the\nFebruary 22, 2016 DRAFT\n4 computation powers of distributed nodes. Besides, due to inexact update, such schemes usually require more iterations to converge and thus may have higher communication overhead. References [21]–[23] have respectively considered asynchronous ADMM methods for decentralized optimization over networks. These works consider network topologies beyond the star network, but their definition of asynchrony is different from what we propose here. Specifically, the asynchrony in [21] lies in that, at each iteration, the nodes are randomly activated to perform variable update. The method presented in [22] further allows that the communications between nodes can succeed or fail randomly. It is shown in [22] that such asynchronous ADMM can converge in a probability-one sense, provided that the nodes and communication links satisfy certain statistical assumption. Reference [23] has considered an asynchronous dual ADMM method. The asynchrony is in the sense that the nodes are partitioned into groups based on certain coloring scheme and only one group of nodes update variable in each iteration."
    }, {
      "heading" : "C. Contributions",
      "text" : "In this paper1, we generalize the state-of-the-art synchronous distributed ADMM [9], [18] to the asynchronous setting. Like [10]–[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.\nTheoretically, we show that, for general and possibly non-convex problems in the form of (1), the AD-ADMM converges to the set of KKT points if the algorithm parameters are chosen appropriately according to the maximum network delay. Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems. Therefore, the analysis and algorithm proposed here are applicable not only to standard convex learning problems but also to important non-convex problems such as the sparse PCA problem [8] and matrix factorization problems [24]. To the best of our knowledge, except the inexact version in [20], this is the first time that the distributed ADMM is rigorously shown to be convergent for non-convex problems under the asynchronous protocol. Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only. Furthermore, we demonstrate that the asynchrony of ADMM has to be handled with care – as a slight modification of the algorithm may\n1In contrast to the conference paper [1], the current paper presents detailed proofs of theorems and more simulation results.\nFebruary 22, 2016 DRAFT\n5 lead to completely different convergence conditions and even destroy the convergence of ADMM for convex problems. Some numerical results are presented to support our theoretical claims.\nIn the companion paper [25], the linear convergence conditions of the AD-ADMM is further analyzed. In addition, the numerical performance of the AD-ADMM is examined by solving a large-scale LR problem on a high-performance computer cluster.\nSynopsis: Section II presents the applications of problem (1) and reviews the distributed ADMM in [9]. The proposed AD-ADMM and its convergence conditions are presented in Section III. Comparison of the proposed AD-ADMM with an alternative scheme is presented in Section IV. Some simulation results are presented in Section V. Finally, concluding remarks are given in Section VI.\nII. APPLICATIONS AND DISTRIBUTED ADMM"
    }, {
      "heading" : "A. Applications",
      "text" : "We target at solving problem (1) over a star computer network (cluster) with one master node and N workers/slaves, as illustrated in Figure 1. Such distributed optimization approach is extremely useful in modern big data applications [3]. For example, let us consider the following regularized empirical risk minimization problem [7]\nmin w∈Rn\nm∑\nj=1\nℓ(aTj w, yj) + Ω(w), (2)\nwhere m is the number of training samples and ℓ(aTj w, yj) is a loss function (e.g., regression or classification error) that depends on the training sample aj ∈ Rn, label yj and the parameter vector w ∈ Rn. Here, n denotes the dimension of the parameters (features); Ω(w) is an appropriate convex regularizer. Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few. Obviously, solving (2) can be challenging when the number of training samples is very large. In that case, it is natural to split the training samples across the computer cluster and resort to a distributed optimization approach. Suppose that the m training samples are uniformly distributed and stored by the N workers, with each node i getting qi = xm/Ny samples. By defining fi(w) , ∑iqi\nj=(i−1)qi+1 ℓ(aTj w, yj),\ni = 1, . . . , N , and h(w) , Ω(w), it is clear that (2) is an instance of (1).\nWhen the number of training samples is moderate but the dimension of the parameters is very large (n ≫ m), problem (2) is also challenging to solve. By [9, Section 7.3], one can instead consider the Lagrangian dual problem of (2) provided that (2) has zero duality gap. Specifically, let the training matrix A , [a1, . . . ,am]T ∈ Rm×n be partitioned as A = [A1, . . . ,AN ], and let the parameter vector\nFebruary 22, 2016 DRAFT\n6 w be partitioned conformally as w = [wT1 , . . . ,w T N ]\nT ; moreover, assume that Ω is separable as Ω(w) = ∑N\ni=1 Ωi(wi). Then, following [9, Section 7.3], one can obtain the dual problem of (2) as\nmin ν∈Rm\nN∑\ni=1\nΩ∗i (A T i ν) + Φ ∗(ν), (3)\nwhere ν , [ν1, . . . , νm]T is a dual variable, Φ∗(ν) = ∑m j=1 ℓ ∗(νj , yj), and ℓ∗ and Ω∗i are respectively the conjugate functions of ℓ and Ωi. Note that (3) is equivalent to splitting the n parameters across the N workers. Clearly, problem (3) is an instance of (1).\nIt is interesting to mention that many emerging problems in smart power grid can also be formulated as problem (1); see, for example, the power state estimation problem considered in [27] is solved by employing the distributed ADMM. The energy management problems (i.e., demand response) in [28]–[30] can potentially be handled by the distributed ADMM as well."
    }, {
      "heading" : "B. Distributed ADMM",
      "text" : "In this section, we present the distributed ADMM [4], [9] for solving problem (1). Let us consider the\nfollowing consensus formulation of problem (1)\nmin x0,xi∈Rn, i=1,...,N\nN∑\ni=1\nfi(xi) + h(x0) (4a)\ns.t. xi = x0, ∀i = 1, . . . , N. (4b)\nIn (4), the N + 1 variables xi, i = 0, 1, . . . , N , are subject to the consensus constraint in (4b), i.e., x0 = x1 = · · · = xN . Thus, problem (4) is equivalent to (1). It has been shown that such a consensus problem can be efficiently solved by the ADMM [9]. To describe this method, let λ ∈ Rn denote the Lagrange dual variable associated with constraint (4b) and define the following augmented Lagrangian function\nLρ(x,x0,λ) = N∑\ni=1\nfi(xi) + h(x0)\n+\nN∑\ni=1\nλ T i (xi − x0) +\nρ 2\nN∑\ni=1\n‖xi − x0‖2, (5)\nwhere x , [xT1 , . . . ,x T N ] T , λ , [λT1 , . . . ,λ T N ] T and ρ > 0 is a penalty parameter. According to [4], the standard synchronous ADMM iteratively updates the primal variables xi, i = 0, 1, . . . , N, by minimizing (5) in a (one-round) Gauss-Seidel fashion, followed by updating the dual variable λ using an approximate gradient ascent method. The ADMM algorithm for solving (4) is presented in Algorithm 1,\nFebruary 22, 2016 DRAFT\n7 Algorithm 1 (Synchronous) Distributed ADMM for (4) [9]\n1: Given initial variables x0 and λ0; set x00 = x 0 and k = 0. 2: repeat 3: update\nx k+1 0 =arg min\nx0∈Rn\n{ h(x0)− xT0 ∑N i=1 λ k i\n+ ρ2 ∑N i=1 ‖xki − x0‖2 } , (6)\nx k+1 i =arg min\nxi∈Rn fi(xi) + x\nT i λ k i + ρ 2‖xi − xk+10 ‖2,\n∀ i = 1, . . . , N, (7)\nλ k+1 i = λ k i + ρ(x k+1 i − xk+10 ), ∀ i = 1, . . . , N. (8)\n4: set k ← k + 1. 5: until a predefined stopping criterion is satisfied.\nAs seen, Algorithm 1 is naturally implementable over the star computer network illustrated in Figure 1. Specifically, the master node takes charge of optimizing x0 by (6), and each worker i is responsible for optimizing (xi,λi) by (7)-(8). Through exchanging the up-to-date x0 and (xi,λi) between the master and the workers, Algorithm 1 solves problem (1) in a fully distributed and parallel manner. Convergence properties of the distributed ADMM have been extensively studied; see, e.g., [9], [18], [31]–[33]. Specifically, [31] shows that the ADMM, under general convex assumptions, has a worst-case O(1/k) convergence rate; while [32] shows that the ADMM can have a linear convergence rate given strong convexity and smoothness conditions on fi’s. For non-convex and smooth fi’s, the work [18] shows that Algorithm 1 can converge to the set of KKT points with a O(1/ √ k) rate as long as ρ is large enough.\nHowever, Algorithm 1 is a synchronous algorithm, where the operations of the master and the workers are “locked” with each other. Specifically, to optimize x0 at each iteration, the master has to wait until receiving all the up-to-date variables (xi,λi), i = 1, . . . , N , from the workers. Since the workers may\nFebruary 22, 2016 DRAFT\n8 have different computation and communication delays2, the pace of the optimization would be determined by the “slowest” worker. As an example illustrated in Figure 2(a), the master updates x0 only when it has received the variable information for the four workers at every iteration. As a result, under such synchronous protocol, the master and speedy workers (e.g., workers 1 and 3 in Figure 2) would spend most of the time idling, and thus the parallel computational resources cannot be fully utilized.\nIII. ASYNCHRONOUS DISTRIBUTED ADMM"
    }, {
      "heading" : "A. Algorithm Description",
      "text" : "In this section, we present an AD-ADMM. The asynchronism we consider is in the same spirit of [10]–[14], [19], [20], where the master does not wait for all the workers. Instead, the master updates x0 whenever it receives (xi,λi) from a partial set of the workers. For example, in Figure 2(b), the master updates x0 whenever it receives the variable information from at least two workers. This implies that none of the workers have to be synchronized with each other and the master does not need to wait for the slowest worker either. As illustrated in Figure 2(b), with the lock removed, both the master and speedy workers can update their variables more frequently.\nLet us denote k ≥ 0 as the iteration number of the master (i.e., the number of times for which the master updates x0), and denote Ak ⊆ V , {1, . . . , N} as the index subset of workers from which the master receives variable information during iteration k (for example, in Figure 2(b), A0 = {1, 3} and\n2In a heterogeneous network, the workers can have different computational powers, or the data sets can be non-uniformly distributed across the network. Thus, the workers can require different computational times in solving the local subproblems. Besides, the communication delays can also be different, e.g., due to probabilistic communication failures and message retransmission.\nFebruary 22, 2016 DRAFT\n9 A1 = {1, 2})3. We say that worker i is “arrived” at iteration k if i ∈ Ak and “unarrived” otherwise. Clearly, unbounded delay will jeopardize the algorithm convergence. Therefore throughout this paper, we will assume that the asynchronous delay in the network is bounded. In particular, we follow the popular partially asynchronous model [4] and assume:\nAssumption 1 (Bounded delay) Let τ ≥ 1 be a maximum tolerable delay. For all i ∈ V and iteration k ≥ 0, it must be that i ∈ Ak ∪ Ak−1 · · · ∪ Amax{k−τ+1,−1}.\nAssumption 1 implies that every worker i is arrived at least once within the period [k − τ + 1, k]. In another word, the variable information (xi,λi) used by the master must be at most τ iterations old. To guarantee the bounded delay, at every iteration the master should wait for the workers who have been inactive for τ − 1 iterations, if such workers exist. Note that, when τ = 1, one has i ∈ Ak for all i ∈ V (i.e., Ak = V), which corresponds to the synchronous case and the master always waits for all the workers at every iteration.\nIn Algorithm 2, we present the proposed AD-ADMM, which specifies respectively the steps for the master and the distributed workers. Here, Ack denotes the complementary set of Ak, i.e., Ak ∩ Ack = ∅ and Ak ∪ Ack = V . Algorithm 2 has five notable differences compared with Algorithm 1. First, the master is required to update {(xi,λi)}i∈V , and such update is only performed for those variables with i ∈ Ak. Second, x0 is updated by solving a problem with an additional proximal term γ2‖x0 − xk0‖2, where γ > 0 is a penalty parameter (cf. (12)). Adding such proximal term is crucial in making the algorithm well-behaved in the asynchronous setting. As will be seen in the next section, a proper choice of γ guarantees the convergence of Algorithm 2. Third, the variables di’s are introduced to count the delays of the workers. If worker i is arrived at the current iteration, then di is set to zero; otherwise, di is increased by one. So, to ensure Assumption 1 hold all the time, in Step 4 of Algorithm of the Master, the master waits if there exists at least one worker whose di ≥ τ − 1. Fourth, in addition to the bounded delay, we assume that the master proceeds to update the variables only if there are at least A ≥ 1 arrived workers, i.e., |Ak| ≥ A for all k [19]. Note that when A = N , the algorithm reduces to the synchronous distributed ADMM. Fifth, in Step 6 of Algorithm of the Master, the master sends the up-to-date x0 only to the arrived workers.\nWe emphasize again that both the master and fast workers in the AD-ADMM can have less idle time and update more frequently than its synchronous counterpart. As illustrated in Figure 2, during the\n3Without loss of generality, we let A−1 = V , as seen from Figure 2.\nFebruary 22, 2016 DRAFT\n10\nsame period of time, the synchronous algorithm only completes two updates whereas the asynchronous algorithm updates six times already. On the flip side, the asynchronous algorithm introduces delayed variable information and thereby requires a larger number of iterations to reach the same solution accuracy than its synchronous counterpart. In practice we observe that the benefit of improved update frequency can outweigh the cost of increased number of iterations, and as a result the asynchronous algorithm can still converge faster in time. This is particularly true when the workers have different computation and communication delays and when the computation and communication delays of the master for solving (12) is much shorter than the computation and communication delays of the workers for updating (13) and (14)4; e.g., see Figure 2. Detailed numerical results will be reported in Section V of the companion paper [25]."
    }, {
      "heading" : "B. Convergence Analysis",
      "text" : "In this subsection, we analyze the convergence conditions of Algorithm 2. We first make the following\nstandard assumption on problem (1) (or equivalently problem (4)):\nAssumption 2 Each function fi is twice differentiable and its gradient ∇fi is Lipschitz continuous with a Lipschitz constant L > 0; the function h is proper convex (lower semi-continuous, but not necessarily smooth) and dom(h) (the domain of h) is compact. Moreover, problem (1) is bounded below, i.e., F ⋆ > −∞ where F ⋆ denotes the optimal objective value of problem (1).\nNotably, we do not assume any convexity on fi’s. Indeed, we will show that the AD-ADMM can converge to the set of KKT points even for non-convex fi’s. Our main result is formally stated below.\nTheorem 1 Suppose that Assumption 1 and Assumption 2 hold true. Moreover, assume that there exists a constant S ∈ [1, N ] such that |Ak| < S for all k and that\n∞ > Lρ(x0,x00,λ0)− F ⋆ ≥ 0, (15) ρ > (1 + L+ L2) + √ (1 + L+ L2)2 + 8L2\n2 , (16)\nγ > S(1 + ρ2)(τ − 1)2 −Nρ\n2 . (17)\n4Note that, for many practical cases (such as h(x0) = ‖x0‖1) for which (12) has a closed-form solution, the computation delay of the master is negligible. For high-performance computer clusters connected by large-bandwidth fiber links, the communication delays between the master and the workers can also be short. However, for cases in which the computation and communication delays of the master is significant, the AD-ADMM could be less time efficient than the synchronous ADMM due to the increased number of iterations.\nFebruary 22, 2016 DRAFT\n11\nThen, ({xki }Ni=1,xk0 , {λki }Ni=1) generated by (9), (10) and (12) are bounded and have limit points which satisfy KKT conditions of problem (4).\nTheorem 1 implies that the AD-ADMM is guaranteed to converge to the set of KKT points as long as the penalty parameters ρ and γ are sufficiently large. Since 1/γ can be viewed as the step size of x0, (17) indicates that the master should be more cautious in moving x0 if the network allows a longer delay τ . In particular, the value γ in the worst case should increase with the order of τ2. When τ = 1 (the synchronous case), γ = −(Nρ)/2 < 0 and thus the proximal term γ2‖x0 − xk0‖2 can be removed from (12). On the other hand, we also see from (17) that γ should increase with N if τ > 1 is fixed5. This is because in the worst case the more workers, the more outdated information introduced in the network. Finally, we should mention that a large ρ may be essential for the AD-ADMM to converge properly, especially for non-convex problems, as we demonstrate via simulations in Section V.\nLet us compare Theorem 1 with the results in [19], [22]. First, the convergence conditions in [19], [22] are only applicable for convex problems, whereas our results hold for both convex and non-convex problems. Second, [19], [22] have made specific statistical assumptions on the behavior of the workers, and the convergence results presented therein are in an expectation sense. Therefore it is possible, at least theoretically, that a realization of the algorithm fails to converge despite satisfying the conditions given in [19]. On the contrary, our convergence results hold deterministically.\nNote that for non-convex fi’s, subproblem (13) is not necessarily convex. However, given ρ ≥ L in (16) and twice differentiability of fi (Assumption 2), subproblem (13) becomes a (strongly) convex problem6 and hence is globally solvable. When fi’s are all convex functions, Theorem 1 reduces to the following corollary.\nCorollary 1 Assume that fi’s are all convex functions. Under the same premises of Theorem 1, and for γ satisfying (17) and\nρ ≥ (1 + L 2) +\n√ (1 + L2)2 + 8L2\n2 , (18)\n({xki }Ni=1,xk0 , {λki }Ni=1) generated by (9), (10) and (12) are bounded and have limit points which satisfy KKT conditions of problem (4).\n5Note that, for a fixed τ , S should increase with N . 6By [34, Lemma 1.2.2], the minimum eigenvalue of the Hessian matrix of fi(xi) is no smaller than −L. Thus, for ρ > L,\nsubproblem (13) is a strongly convex problem.\nFebruary 22, 2016 DRAFT\n12"
    }, {
      "heading" : "C. Proof of Theorem 1 and Corollary 1",
      "text" : "Let us write Algorithm 2 from the master’s point of view. Define k̄i as the last iteration number before iteration k for which worker i ∈ Ak is arrived7, i.e., i ∈ Ak̄i . Then Algorithm 2 from the master’s point of view is as follows: for master iteration k = 0, 1, . . . ,\nx k+1 i =    argmin xi { fi(xi) + x T i λ k̄i+1 i + ρ2‖xi − x k̄i+1 0 ‖2 } , ∀i ∈ Ak\nx k i ∀i ∈ Ack\n, (19)\nλ k+1 i =\n{ λ k̄i+1 i + ρ(x k+1 i − xk̄i+10 ) ∀i ∈ Ak\nλ k i ∀i ∈ Ack\n, (20)\nx k+1 0 =arg min\nx0∈Rn\n{ h(x0)− xT0 ∑N i=1 λ k+1 i\n+ ρ2 ∑N i=1 ‖xk+1i − x0‖2 + γ2 ‖x0 − xk0‖2 } .\nNow it is relatively easy to see that the master updates x0 using the delayed (xi,λi)i∈Ak and the old (xi,λi)i∈Ack . Under Assumption 1, it must hold\nmax{k − τ,−1} ≤ k̄i < k ∀ k ≥ 0. (21)\nMoreover, by the definition of k̄i it holds that i /∈ Ak−1 ∪ · · · ∪ Ak̄i+1, therefore we have that\nλ k̄i+1 i = λ k̄i+2 i = · · · = λki , ∀ i ∈ Ak. (22)\nBy applying (22) to (19) and (20) (replacing λk̄i+1i with λ k i ), we rewrite the master-point-of-view algorithm in Algorithm 3.\nInspired by [18], our analysis for Theorem 1 investigates how the augmented Lagrangian function, i.e.,\nLρ(xk,xk0 ,λk) = N∑\ni=1\nfi(x k i ) + h(x k 0) +\nN∑\ni=1\n(λki ) T (xki − xk0)\n+ ρ\n2\nN∑\ni=1\n‖xki − xk0‖2 (26)\nevolves with the iteration number k, where xk , [(xk1) T , . . . , (xkN ) T ]T and λk , [(λk1) T , . . . , (λkN ) T ]T . The following lemma is one of the keys to prove Theorem 1.\n7Note that k̄i = −1 for k = 0 and k̄i ≥ −1 for k ≥ 0\nFebruary 22, 2016 DRAFT\n13\nLemma 1 Suppose that Assumption 2 holds and ρ ≥ L. Then, it holds that\nLρ(xk+1,xk+10 ,λk+1)− Lρ(xk,xk0 ,λk)\n≤ −2γ +Nρ 2 ‖xk+10 − xk0‖2\n+\n( 1\nρ +\n1 2\n) ∑\ni∈Ak\n‖λk+1i − λki ‖2\n+ 1 + ρ2\n2\n∑\ni∈Ak\n‖xk̄i+10 −xk0‖2\n+ (1− ρ) + L\n2\n∑\ni∈Ak\n‖xk+1i −xki ‖2. (27)\nProof: See Appendix A.\nEquation (27) shows that Lρ(xk,xk0 ,λk) is not necessarily decreasing due to the error terms ∑ i∈Ak ‖λk+1i −\nλ k i ‖2 and ∑ i∈Ak\n‖xk̄i+10 −xk0‖2. Next we bound the sizes of these two terms. First consider\n∑ i∈Ak ‖λk+1i −λki ‖2. Note from (24) and the optimality condition of (23) that, ∀ i ∈ Ak,\n0 = ∇fi(xk+1i ) + λki + ρ(xk+1i − xk̄i+10 )\n= ∇fi(xk+1i ) + λk+1i . (28)\nFor any i ∈ Ack, denote k̃i < k as the last iteration number for which worker i is arrived. Then, i ∈ Ak̃i and thus ∇fi(xk̃i+1i ) + λk̃i+1i = 0. Since xk̃i+1i = xk̃i+2i = · · · = xki = xk+1i and λk̃i+1i = λk̃i+2i = · · · = λki = λk+1i , we obtain that ∇fi(xk+1i ) + λk+1i = 0 ∀i ∈ Ack. Therefore, we conclude that\n∇fi(xk+1i ) + λk+1i = 0, ∀ i ∈ V and ∀ k. (29)\nBy (29) and the Lipschitz continuity of ∇fi (Assumption 2), we can bound\n‖λk+1i − λki ‖2 ≤ ‖∇fi(xk+1i )−∇fi(xki )‖2\n≤ L2‖xk+1i − xki ‖2, ∀ i ∈ V. (30)\nBy applying (30), we can further write (27) as\nLρ(xk+1,xk+10 ,λk+1)\n≤ Lρ(xk,xk0 ,λk) + ( 1 + ρ2\n2\n) ∑\ni∈Ak\n‖xk0 − xk̄i+10 ‖2\n− ( 2γ +Nρ\n2\n) ‖xk+10 − xk0‖2\n+\n( L+ L2 + (1− ρ)\n2 +\nL2\nρ\n) ∑\ni∈Ak\n‖xk+1i − xki ‖2. (31)\nFebruary 22, 2016 DRAFT\n14\nFrom (31), one can observe that the error term (1+ρ 2 2 ) ∑ i∈Ak ‖xk0 − xk̄i+10 ‖2 is present due to the\nasynchrony of the network. The next lemma bounds this error term:\nLemma 2 Suppose that Assumption 1 holds and assume that |Ak| < S for all k, for some constant S ∈ [1, N ]. Then, it holds that\nk∑\nj=0\n∑\ni∈Aj\n‖xj0 − x j̄i+1 0 ‖2 ≤ S(τ − 1)2\nk−1∑\nj=0\n‖xj+10 − x j 0‖2. (32)\nProof: See Appendix B.\nThe last lemma shows that Lρ(xk,xk0,λk) is bounded below:\nLemma 3 Under Assumption 2 and for ρ ≥ L, it holds that\nLρ(xk+1,xk+10 ,λk+1) ≥ F ⋆ > −∞. (33)\nProof: See Appendix C.\nGiven the three lemmas above, we are ready to prove Theorem 1.\nProof of Theorem 1: Note that any KKT point ({x⋆i }Ni=1,x⋆0, {λ⋆i }Ni=1) of problem (4) satisfies the following conditions\n∇fi(x⋆i ) + λ⋆i = 0, ∀ i ∈ V, (34a) s ⋆ 0 − ∑N i=1 λ ⋆ i = 0, (34b) x ⋆ i = x ⋆ 0, ∀ i ∈ V, (34c)\nwhere s⋆0 ∈ ∂h(x⋆0) denotes a subgradient of h at x⋆0 and ∂h(x⋆0) is the subdifferential of h at x⋆0. Since (34) also implies\nN∑\ni=1\n∇fi(x⋆)+s⋆0 = 0, (35)\nwhere x⋆ , x⋆0 = · · · = x⋆N , x⋆ is also a stationary point of the original problem (1).\nFebruary 22, 2016 DRAFT\n15\nTo prove the desired result, we take a telescoping sum of (31), which yields\nLρ(xk+1,xk+10 ,λk+1)− Lρ(x0,x00,λ0)\n≤ ( 1 + ρ2\n2\n) k∑\nj=0\n∑\ni∈Aj\n‖xj0 − x j̄i+1 0 ‖2\n+\n( L+ L2 + (1− ρ)\n2 +\nL2\nρ\n) k∑\nj=0\n∑\ni∈Aj\n‖xj+1i − x j i‖2\n− ( 2γ +Nρ\n2\n) k∑\nj=0\n‖xj+10 − xj0‖2. (36)\nBy substituting (32) in Lemma 2 into (36), we obtain ( 2γ +Nρ− S(1 + ρ2)(τ − 1)2\n2\n) k−1∑\nj=0\n‖xj+10 − x j 0‖2\n+\n( (1− ρ)− (L+ L2)\n2 − L\n2\nρ\n) k∑\nj=0\nN∑\ni=1\n‖xj+1i − x j i‖2\n≤ Lρ(x0,x00,λ0)− Lρ(xk+1,xk+10 ,λk+1) = (Lρ(x0,x00,λ0)− F ⋆)− (Lρ(xk+1,xk+10 ,λk+1)− F ⋆) ≤ Lρ(x0,x00,λ0)− F ⋆ < ∞, (37)\nwhere the second inequality is obtained by applying Lemma 3, and the last strict inequality is due to Assumption 2 where the optimal value F ⋆ is assumed to be lower bounded.\nThen, (16) and (17) imply that the left hand side (LHS) of (37) is positive and increasing with k. Since\nthe RHS of (37) is finite, we must have, as k → ∞,\nx k+1 0 − xk0 → 0, xk+1i − xki → 0, ∀ i ∈ V. (38)\nGiven (30), (38) infers\nλ k+1 i − λki → 0, ∀i ∈ V. (39)\nWe use (38) and (39) to show that every limit point of ({xki }Ni=1,xk0 , {λki }Ni=1) is a KKT point of problem (4). Firstly, by applying (39) to (24) and by (38), one obtains xk+10 − xk+1i → 0 ∀i ∈ Ak. For i ∈ Ack, note that i ∈ Ak̃i (see the definition of k̃i above (29)) and thus, by (24),\nλ k̃i+1 i = λ k̃i i + ρ(x k̃i+1 i − x (k̃i)i+1 0 ),\nFebruary 22, 2016 DRAFT\n16\nwhere (k̃i)i denotes the last iteration number before iteration k̃i for which worker i is arrived. Moreover, since xk̃i+1i = x k̃i+2 i = · · · = xki = xk+1i ∀i ∈ Ack, and by (24), (38) and (39), we have ∀i ∈ Ack,\n‖xk+10 − xk+1i ‖ = ‖xk+10 − xk̃i+1i ‖\n= ‖xk+10 − x (k̃i)i+1 0 + x (k̃i)i+1 0 − xk̃i+1i ‖ ≤ ‖xk+10 − x (k̃i)i+1 0 ‖+ 1\nρ ‖λk̃i+1i − λk̃ii ‖\n→ 0. (40)\nSo we conclude\nx k+1 0 − xk+1i → 0 ∀i ∈ V. (41)\nSecondly, the optimality condition of (25) gives\ns k+1 0 −\nN∑\ni=1\nλ k+1 i − ρ\nN∑\ni=1\n(xk+1i − xk+10 )\n+ γ(xk+10 − xk0) = 0, (42)\nfor some sk+10 ∈ ∂h(xk+10 ). By applying (41) and (38) to (42), we obtain that\ns k+1 0 −\nN∑\ni=1\nλ k+1 i → 0. (43)\nEquations (29), (41) and (43) imply that ({xki }Ni=1,xk0 , {λki }Ni=1) asymptotically satisfy the KKT conditions in (34).\nLastly, let us show that ({xki }Ni=1,xk0 , {λki }Ni=1) is bounded and has limit points. Since dom(h) is compact and xk0 ∈ dom(h), xk0 is a bounded sequence and thus has limit points. From (41), xki , i ∈ V , are bounded and have limit points. Moreover, by (29), λki , i ∈ V , are bounded and have limit points as well. In summary, ({xki }Ni=1,xk0 , {λki }Ni=1) converges to the set of KKT points of problem (4) .\nProof of Corollary 1: The proof exactly follows that of Theorem 1. The only difference is that the\ncoefficient of the term (1−ρ)+L2 ∑ i∈Ak ‖xk+1i −xki ‖2 in (27) reduces from (1−ρ)+L 2 to (1−ρ) 2 ; see the footnote in Appendix A.\nIV. COMPARISON WITH AN ALTERNATIVE SCHEME\nIn Algorithm 2, the workers compute (xi,λi), i ∈ V , and the master is in charge of computing x0. While such distributed implementation is intuitive and natural, one may wonder whether there exist\nFebruary 22, 2016 DRAFT\n17\nother valid implementations, and if so, how they compare with Algorithm 2. To shed some light on this question, we consider in this section an alternative scheme in Algorithm 4.\nAlgorithm 4 differs from Algorithm 2 in that the master handles not only the update of x0 but also that of {λi}i∈V ; so the workers only updates {xi}. In essence, in a synchronous network, Algorithm 2 and Algorithm 4 are equivalent up to a change of update order8 and have the same convergence conditions. However, intriguingly, in an asynchronous network, the two algorithms may require distinct convergence conditions and behave very differently in practice. To analyze the convergence of Algorithm 4, we make the following assumption.\nAssumption 3 Each function fi is strongly convex with modulus σ2 > 0 and the function h is convex.\nUnder the strong convexity assumption, we are able to show the following convergence result for Algorithm 4.\nTheorem 2 Suppose that Assumption 1 and Assumption 3 hold true. Moreover, let γ = 0 and\n0 < ρ ≤ σ 2\n(5τ − 3)max{2τ, 3(τ − 1)} , (48)\nand define x̄ki = 1 k ∑k ℓ=1 x k i ∀i = 0, 1, . . . , N , where ({xki }Ni=1,xk0) are generated by (44) and (45). Then, it holds that ∣∣∣∣ [ N∑\ni=1\nfi(x̄ k i ) + h(x̄ k 0)\n] − F ⋆ ∣∣∣∣+ N∑\ni=1\n‖x̄ki − x̄k0‖\n≤ (2 + δλ)C k\n(49)\nfor all k, where C < ∞ is a finite constant and δλ , max{‖λ⋆1‖, . . . , ‖λ⋆N‖}, in which {λ⋆i } denote the optimal dual variables of (4).\nThe proof is presented in Appendix D. Theorem 2 somehow implies that Algorithm 4 may require stronger convergence conditions than Algorithm 2 in the asynchronous network, as fi’s are assumed to be strongly convex. Besides, different from Theorem 1 where ρ is advised to be large for Algorithm 2, Theorem 2 indicates that ρ needs to be small for Algorithm 4. Since ρ is the step size of the dual gradient ascent in (46), (48) implies that the master should move λi’s slowly when τ is large. Such insight is reminiscent of the recent convergence results for multi-block ADMM in [33].\nInterestingly and surprisingly, our numerical results to be presented shortly suggest that the strongly\nconvex fi’s and a small ρ are necessary for the convergence of Algorithm 4.\n8Algorithm 2 under the synchronous protocol is the same as Algorithm 1 with the order of (6) and (7) interchanged.\nFebruary 22, 2016 DRAFT\n18\nV. SIMULATION RESULTS\nThe main purpose of this section is to examine the convergence behavior of the AD-ADMM with respect to the master’s iteration number k. So, the simulation results to be presented are obtained by implementing Algorithm 3 on a desktop computer. First, we present the simulation results of the ADADMM for solving the non-convex sparse PCA problem. Second, we consider the LASSO problem and compare Algorithm 4 with Algorithm 2."
    }, {
      "heading" : "A. Example 1: Sparse PCA",
      "text" : "Theorem 1 has shown that the AD-ADMM can converge for non-convex problems. To verify this point,\nlet us consider the following sparse PCA problem [8]\nmin w∈Rn\n− N∑\nj=1\nw T B T j Bjw+θ‖w‖1, (50)\nwhere Bj ∈ Rm×n, ∀j = 1, . . . , N, and θ > 0 is a regularization parameter. The sparse PCA problem above is not a convex problem. We display in Figure 3 the convergence performance of the AD-ADMM for solving (50). In the simulations, each matrix Bj ∈ Rn is a 1000 × 500 sparse random matrix with approximately 5000 non-zero entries; θ is set to 0.1 and N = 32. The penalty parameter ρ is set to ρ = βmaxj=1,...,N λmax(B T j Bj) and γ = 0. To simulate an asynchronous scenario, at each iteration, half of the workers are assumed to have a probability 0.1 to be arrived independently, and half of the workers are assumed to have a probability 0.8 to be “arrived” independently. At each iteration, the master proceeds to update the variables as long as there is at least one arrived worker, i.e., A = 1. The accuracy is defined as\naccuracy = |Lρ(xk,xk0,λk)− F̂ |\nF̂ (51)\nwhere F̂ denotes the optimal objective value for the synchronous case (τ = 1) which is obtained by running the distributed ADMM (with β = 3) for 10000 iterations (it is found in the experiments that the AD-ADMM converges to the same KKT point for different values of τ ). One can observe from Figure 3 that the AD-ADMM (with β = 3) indeed converges properly even though (50) is a non-convex problem.\nInterestingly, we note that for the example considered here, the AD-ADMM with γ = 0 works well for different values of τ , even though Theorem 1 suggests that γ should be a larger value in the worst-case. However, we do observe from Figure 3 that if one sets β = 1.5 (i.e., a smaller value of ρ), then the AD-ADMM diverges even in the synchronous case (τ = 1). This implies that the claim of a large enough ρ is necessary for the non-convex sparse PCA problem.\nFebruary 22, 2016 DRAFT\n19"
    }, {
      "heading" : "B. Example 2: LASSO",
      "text" : "In this example, we compare the convergence performance of Algorithm 4 with Algorithm 2. We\nconsider the following LASSO problem\nmin w∈Rn\nN∑\ni=1\n‖Aiw − bi‖2 + θ‖w‖1, (52)\nwhere Ai ∈ Rm×n, bi ∈ Rm, i = 1, . . . , N , and θ > 0. The elements of Ai’s are randomly generated following the Gaussian distribution with zero mean and unit variance, i.e., ∼ N (0, 1); each bi is generated by bi = Aiw0+νi where w0 ∈ Rn is an n×1 sparse random vector with approximately 0.05n non-zero entries and νi is a noise vector with entries following N (0, 0.01). A star network with 16 (N = 16) workers is considered. To simulate an asynchronous scenario, at each iteration, half of the workers are assumed to have a probability 0.1 to be arrived independently, 4 workers are assumed to have a probability 0.3 to be arrived independently, and the remaining 4 workers are assumed to have a probability 0.8 to be arrived independently.\nFigure 4(a) and Figure 4(b) respectively display the convergence curves (accuracy versus iteration number) of Algorithm 2 and Algorithm 4 for solving (52) with N = 16, m = 200, n = 100 and θ = 0.1. The accuracy is defined as\naccuracy = |Lρ(xk,xk0 ,λk)− F ⋆|\nF ⋆ (53)\nwhere F ⋆ denotes the optimal objective value of problem (52). One can see from Figure 4(a) that Algorithm 2 (with ρ = 500, γ = 0) converges well for various values of delay τ . From Figure 4(b),\nFebruary 22, 2016 DRAFT\n20\none can observe that, under the synchronous setting (i.e., τ = 1), Algorithm 4 (with ρ = 500) exhibits a similar behavior as Algorithm 2 in Figure 4(a). However, under the asynchronous setting of τ = 3, Algorithm 4 (with ρ = 500) diverges as shown in Figure 4(b); Algorithm 4 can become convergent if one decrease ρ to 10. Analogously, for τ = 10, one has to further reduce ρ to 1 in order to have Algorithm 4 convergent. However, the convergence speed of Algorithm 4 with ρ = 1 is much slower when comparing to Algorithm 2 in Figure 4(a).\nFigure 4(c) and Figure 4(d) show the comparison results of Algorithm 2 and Algorithm 4 for solving\nFebruary 22, 2016 DRAFT\n21\n(52) with n increased to 1000. Note that, given m = 200 and n = 1000, the cost functions fi(wi) , ‖Aiwi − bi‖2 in (52) are no longer strongly convex. One can observe from Figure 4(c) that Algorithm 2 (with ρ = 500, γ = 0) still converges properly for various values of τ . However, as one can see from Figure 4(d), Algorithm 4 always diverges for various values of ρ even when the delay τ is as small as two. As a result, the strong convexity assumed in Theorem 2 may also be necessary in practice. We conclude from these simulation results that Algorithm 2 significantly outperforms Algorithm 4 in the asynchronous network, even though the two have the same convergence behaviors in the synchronous network.\nVI. CONCLUDING REMARKS\nIn this paper, we have proposed the AD-ADMM (Algorithm 2) aiming at solving large-scale instances of problem (1) over a star computer network. Under the partially asynchronous model, we have shown (in Theorem 1) that the AD-ADMM can deterministically converge to the set of KKT points of problem (4), even in the absence of convexity of fi’s. We have also compared the AD-ADMM (Algorithm 2) with an alternative asynchronous implementation (Algorithm 4), and illustrated the interesting fact that a slight modification of the algorithm can significantly change the algorithm convergence conditions/behaviors in the asynchronous setting.\nFrom the presented simulation results, we have observed that the AD-ADMM may exhibit linear convergence for some structured instances of problem (1). The conditions under which linear convergence can be achieved are presented in the companion paper [25]. Numerical results which demonstrate the time efficiency of the proposed AD-ADMM on a high performance computer cluster are also presented in [25].\nAPPENDIX A\nPROOF OF LEMMA 1\nNotice that\nLρ(xk+1,xk+10 ,λk+1)− Lρ(xk,xk0 ,λk)\n= Lρ(xk+1,xk+10 ,λk+1)− Lρ(xk+1,xk0 ,λk+1)\n+ Lρ(xk+1,xk0 ,λk+1)− Lρ(xk+1,xk0 ,λk) + Lρ(xk+1,xk0 ,λk)− Lρ(xk,xk0 ,λk). (A.1)\nFebruary 22, 2016 DRAFT\n22\nWe bound the three pairs of the differences on the right hand side (RHS) of (A.1) as follows. Firstly, since −xT0 ∑N i=1 λ k+1 i + ρ 2 ∑N i=1 ‖xk+1i − x0‖2 + γ2‖x0 − xk0‖2 in (25) is strongly convex with respect to (w.r.t.) x0 with modulus γ +Nρ, by [34, Definition 2.1.2], we have ( − (xk0)T N∑\ni=1\nλ k+1 i +\nρ 2\nN∑\ni=1\n‖xk+1i − xk0‖2 )\n− ( − (xk+10 )T N∑\ni=1\nλ k+1 i\n+ ρ\n2\nN∑\ni=1\n‖xk+1i − xk+10 ‖2 + γ\n2 ‖xk+10 − xk0‖2\n)\n≥ ( − N∑\ni=1\nλ k+1 i + ρ\nN∑\ni=1\n(xk+10 − xk+1i )\n+ γ(xk+10 − xk0) )T (xk0 − xk+10 ) + γ +Nρ\n2 ‖xk+10 − xk0‖2. (A.2)\nBy the optimality condition of (25) and the convexity of h, we respectively have ( s k+1 0 − N∑\ni=1\nλ k+1 i + ρ\nN∑\ni=1\n(xk+10 − xk+1i )\n+ γ(xk+10 − xk0) )T (xk0 − xk+10 ) ≥ 0, (A.3)\nh(xk0) ≥ h(xk+10 ) + (sk+10 )T (xk0 − xk+10 ). (A.4)\nBy subsequently applying (A.3) and (A.4) to (A.2), we obtain ( h(xk0)− (xk0)T N∑\ni=1\nλ k+1 i +\nρ 2\nN∑\ni=1\n‖xk+1i − xk0‖2 )\n− ( h(xk+10 )− (xk+10 )T N∑\ni=1\nλ k+1 i\n+ ρ\n2\nN∑\ni=1\n‖xk+1i − xk+10 ‖2 + γ\n2 ‖xk+10 − xk0‖2\n)\n≥ γ +Nρ 2 ‖xk+10 − xk0‖2, (A.5)\nthat is,\nLρ(xk+1,xk+10 ,λk+1)− Lρ(xk+1,xk0 ,λk+1)\n≤ −2γ +Nρ 2 ‖xk+10 − xk0‖2. (A.6)\nFebruary 22, 2016 DRAFT\n23\nSecondly, it directly follows from (26) that\nLρ(xk+1,xk0 ,λk+1)− Lρ(xk+1,xk0 ,λk)\n=\nN∑\ni=1\n(λk+1i − λki )T (xk+1i − xk0)\n= ∑\ni∈Ak\n(λk+1i − λki )T (xk+1i − xk̄i+10 )\n+ ∑\ni∈Ak\n(λk+1i − λki )T (xk̄i+10 − xk0)\n= 1\nρ\n∑\ni∈Ak\n‖λk+1i − λki ‖2\n+ ∑\ni∈Ak\n(λk+1i − λki )T (xk̄i+10 − xk0), (A.7)\nwhere the second equality is due to the fact that λk+1i = λ k i ∀i ∈ Ack and the last equality is obtained by applying\nλ k+1 i = λ k i + ρ(x k+1 i − xk̄i+10 ) ∀i ∈ Ak (A.8)\nas shown in (24).\nThirdly, define Li(xi,xk0 ,λk) = fi(xi) + xTi λki + ρ2‖xi − xk0‖2 and assume that ρ ≥ L. Since, by [34, Lemma 1.2.2], the minimum eigenvalue of the Hessian matrix of fi(xi) is no smaller than −L, Li(xi,xk0 ,λk) is strongly convex w.r.t. xi and the convexity parameter is given by ρ−L ≥ 0 9. Therefore, one has\nLi(xki ,xk0 ,λk) ≥ Li(xk+1i ,xk0 ,λk)\n+ (∇fi(xk+1i ) + λki + ρ(xk+1i − xk0))T (xki − xk+1i ) + ρ− L 2 ‖xk+1i − xki ‖2. (A.9)\nAlso, by the optimality condition of (23), one has, ∀i ∈ Ak,\n0 = ∇fi(xk+1i ) + λki + ρ(xk+1i − xk̄i+10 ) (A.10)\n= (∇fi(xk+1i ) + λki + ρ(xk+1i − xk0))\n+ ρ(xk0 − xk̄i+10 ). (A.11)\n9When fi is a convex function, the minimum eigenvalue of the Hessian matrix of fi(xi) is zero. So, the convexity parameter\nof Li(xi,λk,xk0) is ρ instead.\nFebruary 22, 2016 DRAFT\n24\nBy substituting (A.11) into (A.9) and by (26), we have\nLρ(xk+1,xk0 ,λk)− Lρ(xk,xk0 ,λk)\n=\nN∑\ni=1\n(Li(xk+1i ,λk,xk0)− Li(xki ,λk,xk0))\n= ∑\ni∈Ak\n(Li(xk+1i ,λk,xk0)− Li(xki ,λk,xk0))\n≤ −ρ− L 2\n∑\ni∈Ak\n‖xk+1i − xki ‖2\n+ ρ ∑\ni∈Ak\n(xk̄i+10 − xk0)T (xk+1i − xki ), (A.12)\nwhere the second equality is due to xk+1i = x k i ∀i ∈ Ack from (23).\nAfter substituting (A.6), (A.7) and (A.12) into (A.1), we obtain\nLρ(xk+1,xk+10 ,λk+1)− Lρ(xk,xk0 ,λk)\n≤ −2γ +Nρ 2 ‖xk+10 − xk0‖2 + 1 ρ\n∑\ni∈Ak\n‖λk+1i − λki ‖2\n− ρ− L 2\n∑\ni∈Ak\n‖xk+1i −xki ‖2\n+ ∑\ni∈Ak\n(λk+1i − λki )T (xk̄i+10 − xk0)\n+ρ ∑\ni∈Ak\n(xk̄i+10 −xk0)T (xk+1i − xki ). (A.13)\nRecall the Young’s inequality, i.e.,\na T b ≤ 1 2δ ‖a‖2 + δ 2 ‖b‖2, (A.14)\nfor any a, b and δ > 0, and apply it to the fourth and fifth terms in the RHS of (A.13) with δ = 1 and δ = 1/ρ for some ǫ > 0, respectively. Then (27) is obtained.\nFebruary 22, 2016 DRAFT\n25\nAPPENDIX B\nPROOF OF LEMMA 2\nIt is easy to show that\nk∑\nj=0\n∑\ni∈Aj\n‖xj0 − x j̄i+1 0 ‖2 =\nk∑\nj=0\n∑\ni∈Aj\n‖ j−1∑\nℓ=j̄i+1\n(xℓ0 − xℓ+10 )‖2\n≤ k∑\nj=0\n∑\ni∈Aj\n(j − j̄i − 1) j−1∑\nℓ=j̄i+1\n‖xℓ0 − xℓ+10 ‖2\n≤ k∑\nj=0\n∑\ni∈Aj\n(τ − 1) j−1∑\nℓ=j−τ+1\n‖xℓ0 − xℓ+10 ‖2\n≤ S(τ − 1) k∑\nj=0\nj−1∑\nℓ=j−τ+1\n‖xℓ0 − xℓ+10 ‖2 (A.15)\nwhere, in the second inequality, we have applied the fact of j − τ ≤ j̄i < j from (21); in the last inequality, we have applied the assumption of |Ak| < S for all k. Notice that, in the summation ∑k\nj=0 ∑j−1 ℓ=j−τ+1 ‖xℓ0 − xℓ+10 ‖2, each ‖x j 0 − xj+10 ‖2, where j = 0, . . . , k − 1, appears no more than\nτ − 1 times. Thus, one can upper bound k∑\nj=0\nj−1∑\nℓ=j−τ+1\n‖xℓ0 − xℓ+10 ‖2 ≤ (τ − 1) k−1∑\nj=0\n‖xj+10 − xj0‖2, (A.16)\nwhich, combined with (A.15), yields (32).\nAPPENDIX C\nPROOF OF LEMMA 3\nThe proof is similar to [18, Lemma 2.3]. We present the proof here for completeness. By recalling\nequation (29) and applying it to (26), one obtains\nLρ(xk+1,xk+10 ,λk+1) = h(xk+10 ) + N∑\ni=1\nfi(x k+1 i )\n− N∑\ni=1\n(∇fi(xk+1i ))T (xk+1i − xk+10 ) + ρ\n2\nN∑\ni=1\n‖xk+1i − xk+10 ‖2. (A.17)\nAs ∇fi is Lipschitz continuous under Assumption 2, the descent lemma [36, Proposition A.24] holds\nfi(x k+1 0 ) ≤ fi(xk+1i ) + (∇fi(xk+1i ))T (xk+10 − xk+1i )\n+ L\n2 ‖xk+1i − xk+10 ‖2 ∀ i = 1, . . . , N. (A.18)\nFebruary 22, 2016 DRAFT\n26\nBy combining (A.17) and (A.18), one can lower bound Lρ(xk+1,xk+10 ,λk+1) as\nLρ(xk+1,xk+10 ,λk+1) ≥ h(xk+10 ) + N∑\ni=1\nfi(x k+1 0 )\n+ ρ− L 2\nN∑\ni=1\n‖xk+1i − xk+10 ‖2, (A.19)\nwhich implies (33) given ρ ≥ L and under Assumption 2.\nAPPENDIX D\nPROOF OF THEOREM 2\nFor ease of analysis, we equivalently write Algorithm 4 as follows: For iteration k = 0, 1, . . . ,\nx k+1 i =\n{ arg min xi fi(xi) + x T i λ k̄i+1 i + ρ 2‖xi − x k̄i+1 0 ‖2, ∀i ∈ Ak\nx k i ∀i ∈ Ack\n, (A.20)\nx k+1 0 = arg min\nx0\nh(x0)− xT0 ∑N i=1 λ k i + ρ 2 ∑N i=1 ‖xk+1i − x0‖2, (A.21)\nλ k+1 i = λ k i + ρ(x k+1 i − xk+10 ) ∀i ∈ V. (A.22)\nHere, k̄i is the last iteration number for which the master node receives message from worker i ∈ Ak before iteration k. For i ∈ Ack, let us denote k̃i (k−τ < k̃i < k) as the last iteration number for which the master node receives message from worker i before iteration k, and further denote k̂i (k̃i − τ ≤ k̂i < k̃i) as the last iteration number for which the master node receives message from worker i before iteration k̃i. Then, by (A.20), it must be\nx k̃i+1 i = arg min\nxi\nfi(xi) + x T i λ k̂i+1 i + ρ 2‖xi − x k̂i+1 0 ‖2 ∀i ∈ Ack, (A.23)\nx k+1 i = x k̃i+1 i , (A.24)\nwhere the second equation is due to xk̃i+1i = x k̃i+2 i = · · · = xki = xk+1i ∀i ∈ Ack.\nLet us consider the following update steps\nx k+1 i =\n{ arg min xi αfi(xi) + x T i λ̃ k̄i+1 i + β 2 ‖xi − x k̄i+1 0 ‖2, ∀i ∈ Ak\narg min xi\nαfi(xi) + x T i λ̃ k̂i+1 i + β 2 ‖xi − x k̂i+1 0 ‖2 ∀i ∈ Ack\n, (A.25)\nx k+1 0 = arg min\nx0\nαh(x0)− xT0 ∑N i=1 λ̃ k i + β 2 ∑N i=1 ‖xk+1i − x0‖2, (A.26)\nλ̃ k+1 i = λ̃ k i + β(x k+1 i − xk+10 ) ∀i ∈ V, (A.27)\nwhere α, β > 0. One can verify that (A.25)-(A.27) are equivalent to (A.20)-(A.22) and (A.23)-(A.24) if one considers the change of variables λi = λ̃i/α and ρ = β/α.\nFebruary 22, 2016 DRAFT\n27\nWe first consider the optimality condition of (A.25) for i ∈ Ak:\n0 ≥ α∂fi(xk+1i )T (xk+1i − x⋆i ) + (λ̃k̄i+1i + β(xk+1i − xk̄i+10 ))T (xk+1i − x⋆i )\n= α∂fi(x k+1 i ) T (xk+1i − x⋆i ) + (λ̃k+1i )T (xk+1i − x⋆i )\n+ (λ̃k̄i+1i − λ̃ki )T (xk+1i − x⋆i ) + β(xk+10 − xk̄i+10 )T (xk+1i − x⋆i ), (A.28)\nwhere we have applied (A.27) to obtain the equality. Since, under Assumption 3, fi is strongly convex, one has\nαfi(x ⋆ i ) ≥ αfi(xk+1i ) + α∂fi(xk+1i )T (x⋆i − xk+1i ) +\nασ2\n2 ‖xk+1i − x⋆i ‖2. (A.29)\nCombining (A.28) and (A.29) gives rise to\nαfi(x k+1 i )− αfi(x⋆i ) + λ̃Ti (xk+1i − x⋆i ) +\nασ2\n2 ‖xk+1i − x⋆i ‖2\n+ (λ̃k+1i − λ̃i)T (xk+1i − x⋆i ) + (λ̃k̄i+1i − λ̃ki )T (xk+1i − x⋆i ) + β(xk+10 − xk̄i+10 )T (xk+1i − x⋆i ) ≤ 0 ∀i ∈ Ak. (A.30)\nOn the other hand, consider the optimality condition of (A.25) for i ∈ Ack:\n0 ≥ α∇fi(xk+1i )T (xk+1i − x⋆i ) + (λ̃k̂i+1i + β(xk+1i − xk̂i+10 ))T (xk+1i − x⋆i )\n= α∇fi(xk+1i )T (xk+1i − x⋆i )\n+ (λ̃k̂i+1i + λ̃ k̃i+1 i − λ̃k̃ii − β(xk̃i+1i − xk̃i+10 ) + β(xk̃i+1i − xk̂i+10 ))T (xk+1i − x⋆i )\n= α∇fi(xk+1i )T (xk+1i − x⋆i ) + (λ̃k̃i+1i )T (xk+1i − x⋆i )\n+ (λ̃k̂i+1i − λ̃k̃i)T (xk+1i − x⋆i ) + β(xk̃i+10 − xk̂i+10 )T (xk+1i − x⋆i ), (A.31)\nwhere (A.27) with k = k̃i and (A.24) are used to obtain the first equality. By combining (A.29) with (A.31), one obtains\nαfi(x k+1 i )− αfi(x⋆i ) + λ̃Ti (xk+1i − x⋆i ) +\nασ2\n2 ‖xk+1i − x⋆i ‖2\n+ (λ̃k̃i+1i − λ̃i)T (xk+1i − x⋆i ) + (λ̃k̂i+1i − λ̃k̃i)T (xk+1i − x⋆i ) + β(xk̃i+10 − xk̂i+10 )T (xk+1i − x⋆i ) ≤ 0 ∀i ∈ Ack. (A.32)\nBy summing (A.30) for all i ∈ Ak and (A.32) for all i ∈ Ack and further summing the resultant two\nFebruary 22, 2016 DRAFT\n28\nterms, we obtain that\nα\nN∑\ni=1\nfi(x k+1 i )− α\nN∑\ni=1\nfi(x ⋆ i ) +\nN∑\ni=1\nλ̃ T i (x k+1 i − x⋆i ) +\nN∑\ni=1\nασ2\n2 ‖xk+1i − x⋆i ‖2\n+ ∑\ni∈Ak\n(λ̃k+1i − λ̃i)T (xk+1i − x⋆i ) + ∑\ni∈Ack (λ̃k̃i+1i − λ̃i)T (xk+1i − x⋆i ) ︸ ︷︷ ︸\n(a)\n+ ∑\ni∈Ak\n(λ̃k̄i+1i − λ̃ki )T (xk+1i − x⋆i ) + ∑\ni∈Ack\n(λ̃k̂i+1i − λ̃k̃i)T (xk+1i − x⋆i )\n+ ∑\ni∈Ak\nβ(xk+10 − xk̄i+10 )T (xk+1i − x⋆i ) + ∑\ni∈Ack β(xk̃i+10 − xk̂i+10 )T (xk+1i − x⋆i ) ︸ ︷︷ ︸\n(b)\n≤ 0. (A.33)\nThe term (a) in (A.33), after adding and subtracting ∑\ni∈Ack (λ̃k+1i − λ̃i)T (xk+1i −x⋆i ), can be written as\n(a) =\nN∑\ni=1\n(λ̃k+1i − λ̃i)T (xk+1i − x⋆i ) + ∑\ni∈Ack\n(λ̃k̃i+1i − λ̃k+1i )T (xk+1i − x⋆i ). (A.34)\nThe term (b) in (A.33) can be expressed as\n(b) = ∑\ni∈Ak\nβ(xk+10 − xk0 + xk0 − xk̄i+10 )T (xk+1i − x⋆i ) + ∑\ni∈Ack\nβ(xk̃i+10 − xk̂i+10 )T (xk+1i − x⋆i )\n=\nN∑\ni=1\nβ(xk+10 − xk0)T (xk+1i − x⋆i ) + ∑\ni∈Ack\nβ(xk̃i+10 − xk̂i+10 − xk+10 + xk0)T (xk+1i − x⋆i )\n+ ∑\ni∈Ak\nβ(xk0 − xk̄i+10 )T (xk+1i − x⋆i ). (A.35)\nNote that, by applying (A.27) and the fact of x⋆i = x ⋆ 0 ∀i ∈ V , one can write\nN∑\ni=1\nβ(xk+10 − xk0)T (xk+1i − x⋆i ) = N∑\ni=1\nβ(xk+10 − xk0)T (xk+1i − xk+10 + xk+10 − x⋆i )\n=\nN∑\ni=1\n(xk+10 − xk0)T (λ̃k+1i − λ̃ki ) +Nβ(xk+10 − xk0)T (xk+10 − x⋆0).\n(A.36)\nSo, The term (b) in (A.35) is given by\n(b) =\nN∑\ni=1\n(xk+10 − xk0)T (λ̃k+1i − λ̃ki ) +Nβ(xk+10 − xk0)T (xk+10 − x⋆0)\n+ ∑\ni∈Ack\nβ(xk̃i+10 − xk̂i+10 − xk+10 + xk0)T (xk+1i − x⋆i ) + ∑\ni∈Ak\nβ(xk0 − xk̄i+10 )T (xk+1i − x⋆i ).\n(A.37)\nFebruary 22, 2016 DRAFT\n29\nIt can be shown that N∑\ni=1\n(xk+10 − xk0)T (λ̃k+1i − λ̃ki ) ≥ 0. (A.38)\nTo see this, consider the optimality condition of (A.26): ∀x0 ∈ Rn,\n0 ≥ αh(xk+10 )− αh(x0)− N∑\ni=1\n(λ̃ki + β(x k+1 i − xk+10 ))T (xk+10 − x0)\n= αh(xk+10 )− αh(x0)− N∑\ni=1\n(λ̃k+1i ) T (xk+10 − x0), (A.39)\nwhere the equality is due to (A.27). By letting x0 = xk0 in (A.39) and also considering (A.39) for iteration k and x0 = x k+1 0 , we have\n0 ≥ αh(xk+10 )− αh(xk0)− N∑\ni=1\n(λ̃k+1i ) T (xk+10 − xk0),\n0 ≥ αh(xk0)− αh(xk+10 )− N∑\ni=1\n(λ̃ki ) T (xk0 − xk+10 ), (A.40)\nrespectively. By summing the above two equations, we obtain (A.38). Moreover, by letting x0 = x⋆i = x ⋆ 0 in (A.39), we have\nαh(xk+10 )− αh(x⋆0)− N∑\ni=1\nλ̃ T i (x k+1 0 − x⋆i )−\nN∑\ni=1\n(λ̃k+1i − λ̃i)T (xk+10 − x⋆i ) ≤ 0. (A.41)\nBy summing (A.41) and (A.33) followed by applying (A.34), (A.37) and (A.38), one obtains\nα\nN∑\ni=1\nfi(x k+1 i ) + αh(x k+1 0 )− α\nN∑\ni=1\nfi(x ⋆ i )− αh(x⋆0) +\nN∑\ni=1\nλ̃ T i (x k+1 i − xk+10 ) +\nN∑\ni=1\nασ2\n2 ‖xk+1i − x⋆i ‖2\n+ 1\nβ\nN∑\ni=1\n(λ̃k+1i − λ̃i)T (λ̃k+1i − λ̃ki ) +Nβ(xk+10 − xk0)T (xk+10 − x⋆0)\n+ ∑\ni∈Ack\n(λ̃k̃i+1i − λ̃k+1i + λ̃k̂i+1i − λ̃k̃i)T (xk+1i − x⋆i ) + ∑\ni∈Ak\n(λ̃k̄i+1i − λ̃ki )T (xk+1i − x⋆i )\n+ ∑\ni∈Ack\nβ(xk̃i+10 − xk̂i+10 − xk+10 + xk0)T (xk+1i − x⋆i ) + ∑\ni∈Ak\nβ(xk0 − xk̄i+10 )T (xk+1i − x⋆i ) ≤ 0,\n(A.42)\nwhere the seventh term in the LHS is obtained by applying (A.27).\nFebruary 22, 2016 DRAFT\n30\nWe sum (A.42) for k = 0, . . . ,K − 1 and take the average, which yields\nα K\nK−1∑\nk=0\n[ N∑\ni=1\nfi(x k+1 i ) + h(x k+1 0 )\n] − α [ N∑\ni=1\nfi(x ⋆ i ) + h(x ⋆ 0)\n] + 1\nK\nK−1∑\nk=0\nN∑\ni=1\nλ̃ T i (x k+1 i − xk+10 )\n+ 1\nβK\nK−1∑\nk=0\nN∑\ni=1 (λ̃k+1i − λ̃i)T (λ̃k+1i − λ̃ki ) ︸ ︷︷ ︸\n(a)\n+ Nβ\nK\nK−1∑\nk=0 (xk+10 − xk0)T (xk+10 − x⋆0) ︸ ︷︷ ︸\n(b)\n≤ − 1 K\nK−1∑\nk=0\nN∑\ni=1\nασ2\n2 ‖xk+1i − x⋆i ‖2\n+ 1\nK\nK−1∑\nk=0\n( − ∑\ni∈Ack\n(λ̃k̃i+1i − λ̃k+1i + λ̃k̂i+1i − λ̃k̃i)T (xk+1i − x⋆i )− ∑\ni∈Ak\n(λ̃k̄i+1i − λ̃ki )T (xk+1i − x⋆i ) )\n︸ ︷︷ ︸ (c)\n+ 1\nK\nK−1∑\nk=0\n( − ∑\ni∈Ack\nβ(xk̃i+10 − xk̂i+10 − xk+10 + xk0)T (xk+1i − x⋆i )− ∑\ni∈Ak\nβ(xk0 − xk̄i+10 )T (xk+1i − x⋆i ) )\n︸ ︷︷ ︸ (d)\n.\n(A.43)\nIt is easy to see that term (a)\n(a) = 1\n2\nK−1∑\nk=0\n( ‖λ̃k+1i − λ̃i‖2 − ‖λ̃ki − λ̃i‖2 + ‖λ̃k+1i − λ̃ki ‖2 )\n= 1\n2 ‖λ̃Ki − λ̃i‖2 −\n1 2 ‖λ̃0i − λ̃i‖2 + 1 2\nK−1∑\nk=0\n‖λ̃k+1i − λ̃ki ‖2, (A.44)\nand similarly, term (b)\n(b) = 1\n2\nK−1∑\nk=0\n( ‖xk+10 − x⋆0‖2 − ‖xk0 − x⋆0‖2 + ‖xk+10 − xk0‖2 )\n= 1\n2 ‖xK0 − x⋆0‖2 −\n1 2 ‖x00 − x⋆0‖2 + 1 2\nK−1∑\nk=0\n‖xk+10 − xk0‖2. (A.45)\nFebruary 22, 2016 DRAFT\n31\nNotice that one can bound the term ∑K−1\nk=0 ∑ i∈Ak\n(λ̃k̄i+1i − λ̃ki )T (xk+1i − x⋆i ) in (c) as follows K−1∑\nk=0\n∑\ni∈Ak\n(λ̃k̄i+1i − λ̃ki )T (xk+1i − x⋆i ) = K−1∑\nk=0\n∑\ni∈Ak\nk−1∑\nℓ=k̄i+1\n(λ̃ℓi − λ̃ℓ+1i )T (xk+1i − x⋆i )\n≤ K−1∑\nk=0\n∑\ni∈Ak\nk−1∑\nℓ=k−τ+1\n‖λ̃ℓi − λ̃ℓ+1i ‖ · ‖xk+1i − x⋆i ‖\n≤ N∑\ni=1\nK−1∑\nk=0\nk−1∑\nℓ=k−τ+1\n( 1\n2β2 ‖λ̃ℓi − λ̃ℓ+1i ‖2 +\nβ2\n2 ‖xk+1i − x⋆i ‖2\n) (A.46)\n≤ N∑\ni=1\nK−1∑\nk=0\n( τ − 1 2β2 ‖λ̃k+1i − λ̃ki ‖2 + (τ − 1)β2 2 ‖xk+1i − x⋆i ‖2 ) , (A.47)\nwhere the second inequality is obtained by applying the Young’s inequality:\na T b ≤ 1 2δ ‖a‖2 + δ 2 ‖b‖2 (A.48)\nfor any a, b and δ > 0; the last inequality is caused by the fact that the term ‖λ̃k+1i − λ̃ki ‖2 for each k does not appear more than τ − 1 times in the RHS of (A.46). By applying a similar idea to the first term of (c) and by (A.47), one eventually can bound (c) as follows\n(c) ≤ 3(τ − 1) 2β2\nN∑\ni=1\nK−1∑\nk=0\n‖λ̃k+1i − λ̃ki ‖2 + 3(τ − 1)β2\n2\nN∑\ni=1\nK−1∑\nk=0\n‖xk+1i − x⋆i ‖2. (A.49)\nSimilarly, the term ∑K−1\nk=0 ∑ i∈Ak\nβ(xk0 − xk̄i+10 )T (xk+1i − x⋆i ) in (d) can be upper bounded as follows K−1∑\nk=0\n∑\ni∈Ak\nβ(xk0 − xk̄i+10 )T (xk+1i − x⋆i ) ≤ K−1∑\nk=0\n∑\ni∈Ak\nk−1∑\nℓ=k−τ+1\nβ‖xk0 − xk̄i+10 ‖ · ‖xk+1i − x⋆i ‖\n≤ N∑\ni=1\nK−1∑\nk=0\nk−1∑\nℓ=k−τ+1\n( 1\n2 ‖xk+10 − xk0‖2 +\nβ2\n2 ‖xk+1i − x⋆i ‖2\n) (A.50)\n≤ N∑\ni=1\nK−1∑\nk=0\n( τ − 1 2 ‖xk+10 − xk0‖2 + (τ − 1)β2 2 ‖xk+1i − x⋆i ‖2 ) . (A.51)\nBy applying a similar idea to the first term of (d) and by (A.51), one can bound (d) as follows\n(d) ≤ N∑\ni=1\nK−1∑\nk=0\n( τ‖xk+10 − xk0‖2 + τβ2‖xk+1i − x⋆i ‖2 ) . (A.52)\nFebruary 22, 2016 DRAFT\n32\nAfter substituting (A.44), (A.45), (A.49) and (A.52) into (A.43), we obtain that\nα\n[ N∑\ni=1\nfi(x̄ K i ) + h(x̄ K 0 )\n] − α [ N∑\ni=1\nfi(x ⋆ i ) + h(x ⋆ 0)\n] + N∑\ni=1\nλ̃ T i (x̄ K i − x̄K0 )\n≤ α K\nK−1∑\nk=0\n[ N∑\ni=1\nfi(x k+1 i ) + h(x k+1 0 )\n] − α [ N∑\ni=1\nfi(x ⋆ i ) + h(x ⋆ 0)\n] + 1\nK\nK−1∑\nk=0\nN∑\ni=1\nλ̃ T i (x k+1 i − xk+10 )\n≤ 1 2βK\nN∑\ni=1\n‖λ̃0i − λ̃i‖2 − 1\n2βK\nN∑\ni=1\n‖λ̃Ki − λ̃i‖2 + Nβ\n2K ‖x00 − x⋆0‖2 −\nNβ 2K ‖xK0 − x⋆0‖2\n+ ( 3(τ − 1) 2Kβ2 − 1 2βK ) N∑\ni=1\nK−1∑\nk=0\n‖λ̃k+1i − λ̃ki ‖2 + ( Nτ\nK − Nβ 2K\n)K−1∑\nk=0\n‖xk+10 − xk0‖2\n+ 1\nK\nK−1∑\nk=0\nN∑\ni=1\n( 3(τ − 1)β2 + 2τβ2 − ασ2\n2\n) ‖xk+1i − x⋆i ‖2 (A.53)\nwhere the first inequality is by the convexity of fi’s and h.\nAccording to (A.53), by choosing\nβ ≥ max{2τ, 3(τ − 1)}, α ≥ (5τ − 3)β 2\nσ2 , (A.54)\nand recalling that λi = λ̃i/α and ρ = β/α, one can obtain [ N∑\ni=1\nfi(x̄ K i ) + h(x̄ K 0 )\n] − [ N∑\ni=1\nfi(x ⋆ i ) + h(x ⋆ 0)\n] + N∑\ni=1\nλ T i (x̄ K i − x̄K0 )\n≤ 1 2ρK\nN∑\ni=1\n‖λ0i − λi‖2 + Nρ\n2K ‖x00 − x⋆0‖2. (A.55)\nNote that (A.54) is equivalent to\nρ = β/α ≤ σ 2 (5τ − 3)β ≤ σ2 (5τ − 3)max{2τ, 3(τ − 1)} . (A.56)\nNow, let λi = λ⋆i + x̄\nK i −x̄ K 0\n‖x̄Ki −x̄ K 0 ‖ ∀i ∈ V in (A.57), and note that, by the duality theory [37],\n[ N∑\ni=1\nfi(x̄ K i ) + h(x̄ K 0 )\n] − [ N∑\ni=1\nfi(x ⋆ i ) + h(x ⋆ 0)\n] + N∑\ni=1\n(λ⋆i ) T (x̄Ki − x̄K0 ) ≥ 0.\nThus, we obtain that\nN∑\ni=1\n‖x̄Ki − x̄K0 ‖ ≤ 1\nK\n[ 1\n2ρ max ‖a‖≤1\n{ N∑\ni=1\n‖λ0i − λ⋆i + a‖2 } + Nρ\n2 ‖x00 − x⋆0‖2\n] ,\nC1 K . (A.57)\nFebruary 22, 2016 DRAFT\n33\nOn the other hand, let λi = λ⋆i in (A.57), and note that, [ N∑\ni=1\nfi(x̄ K i ) + h(x̄ K 0 )\n] − [ N∑\ni=1\nfi(x ⋆ i ) + h(x ⋆ 0)\n] + N∑\ni=1\n(λ⋆i ) T (x̄Ki − x̄K0 )\n≥ ∣∣∣∣ [ N∑\ni=1\nfi(x̄ K i ) + h(x̄ K 0 )\n] − [ N∑\ni=1\nfi(x ⋆ i ) + h(x ⋆ 0) ]∣∣∣∣− δλ N∑\ni=1\n‖x̄Ki − x̄K0 ‖ (A.58)\nwhere δλ , max{‖λ⋆1‖, . . . , ‖λ⋆N‖}. Thus, we obtain that ∣∣∣∣ [ N∑\ni=1\nfi(x̄ K i ) + h(x̄ K 0 )\n] − [ N∑\ni=1\nfi(x ⋆ i ) + h(x ⋆ 0) ]∣∣∣∣\n≤ δλC1 K + 1 2ρK\nN∑\ni=1\n‖λ0i − λ⋆i ‖2 + Nρ\n2K ‖x00 − x⋆0‖2 = δλC1 +C2 K . (A.59)\nFinally, combining (A.57) and (A.59) gives rise to (52).\nREFERENCES\n[1] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, “Asynchronous distributed alternating direction method of multipliers:\nAlgorithm and convergence analysis,” submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016.\n[2] V. Cevher, S. Becker, and M. Schmidt, “Convex optimization for big data,” IEEE Signal Process. Mag., pp. 32–43, Sept.\n2014.\n[3] R. Bekkerman, M. Bilenko, and J. Langford, Scaling up Machine Learning- Parallel and Distributed Approaches.\nCambridge University Press, 2012.\n[4] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and distributed computation: Numerical methods. Upper Saddle River, NJ,\nUSA: Prentice-Hall, Inc., 1989.\n[5] R. Tibshirani, “Regression shrinkage and selection via the LASSO,” J. Roy. Stat. Soc. B, vol. 58, pp. 267–288, 1996. [6] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in Proc. ACM Int. Conf. on Knowledge Discovery and\nData Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547–556.\n[7] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\nNew York, NY, USA: Springer-Verlag, 2001.\n[8] P. Richtárik, M. Takáč, and S. D. Ahipasaoglu, “Alternating maximization: Unifying framework for 8 sparse PCA\nformulations and efficient parallel codes,” [Online] http://arxiv.org/abs/1212.4137.\n[9] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating\ndirection method of multipliers,” Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011.\n[10] F. Niu, B. Recht, C. Re, and S. J. Wright, “Hogwild!: A lock-free approach to parallelizing stochastic gradient descent,”\nProc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730.\n[11] A. Agarwal and J. C. Duchi, “Distributed delayed stochastic optimization,” Proc. Advances in Neural Information Processing\nSystems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.\n[12] M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G. Andersen, and A. Smola, “Parameter server for distributed machine learning,”\n[Online] http://www.cs.cmu.edu/∼muli/file/ps.pdf.\nFebruary 22, 2016 DRAFT\n34\n[13] M. Li, D. G. Andersen, and A. Smola, “Distributed delayed proximal gradient methods,” [Online] http://www.cs.cmu.edu/\n∼muli/file/ddp.pdf.\n[14] J. Liu and S. J. Wright, “Asynchronous stochastic coordinate descent: Parallelism and convergence properties,” SIAM J.\nOptim.,, vol. 25, no. 1, pp. 351–376, Feb. 2015.\n[15] M. Razaviyayn, M. Hong, Z.-Q. Luo, and J. S. Pang, “Parallel successive convex approximation for nonsmooth nonconvex\noptimization,” in the Proceedings of the Neural Information Processing (NIPS), 2014.\n[16] G. Scutari, F. Facchinei, P. Song, D. P. Palomar, and J.-S. Pang, “Decomposition by partial linearization: Parallel\noptimization of multi-agent systems,” IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641–656, 2014.\n[17] A. Daneshmand, F. Facchinei, V. Kungurtsev, and G. Scutari, “Hybrid random/deterministic parallel algorithms for\nnonconvex big data optimization,” to appear in IEEE Trans. on Signal Processing [Online] http://www.eng.buffalo.edu/ ∼gesualdo/Papers/DanFaccKungTSPsub14.pdf.\n[18] M. Hong, Z.-Q. Luo, and M. Razaviyayn, “Convergence analysis of alternating direction method of multipliers for a family\nof nonconvex problems,” to appear in SIAM J. Opt.; available on http://arxiv.org/pdf/1410.1390.pdf.\n[19] R. Zhang and J. T. Kwok, “Asynchronous distributed ADMM for consensus optimization,” in Proc. 31th ICML, , 2014.,\nBeijing, China, June 21-26, 2014, pp. 1–9.\n[20] M. Hong, “A distributed, asynchronous and incremental algorithm for nonconvex optimization: An ADMM based approach,”\ntechnical report; available on http://arxiv.org/pdf/1412.6058.\n[21] F. Iutzeler, P. Bianchi, P. Ciblat, and W. Hachem, “Asynchronous distributed optimization using a randomized alternating\ndirection method of multipliers,” in Proc. IEEE CDC, Florence, Italy, Dec. 10-13, 2013, pp. 3671–3676.\n[22] E. Wei and A. Ozdaglar, “On the O(1/K) convergence of asynchronous distributed alternating direction method of\nmultipliers,” available on arxiv.org.\n[23] J. F. C. Mota, J. M. F. Xavier, P. M. Q. Aguiar, and M. Puschel, “D-ADMM: A communication-efficient distributed\nalgorithm for separable optimization,” IEEE. Trans. Signal Process., vol. 60, no. 10, pp. 2718–2723, May 2013.\n[24] Q. Ling, Y. Xu, W. Yin, and Z. Wen, “Decentralized low-rank matrix completion,” in Proc. IEEE ICASSP, Kyoto, Japan,\nMarch 25-30, 2012, pp. 2925–2928.\n[25] T.-H. Chang, W.-C. Liao, M. Hong, and X. Wang, “Asynchronous distributed ADMM for large-scale optimization- Part\nII: Linear convergence analysis and numerical performance,” submitted for publication.\n[26] R. Tibshirani and M. Saunders, “Sparisty and smoothness via the fused lasso,” J. R. Statist. Soc. B, vol. 67, no. 1, pp.\n91–108, 2005.\n[27] J. Zhang, S. Nabavi, A. Chakrabortty, and Y. Xin, “Convergence analysis of ADMM based power system mode estimation\nunder asynchronous wide-area communication delays,” in Proc. IEEE PES General Meeting, Denver, CO, USA, July 26-30, 2015, pp. 1–5.\n[28] T.-H. Chang, A. Nedić, and A. Scaglione, “Distributed constrained optimization by consensus-based primal-dual perturba-\ntion method,” IEEE. Trans. Auto. Control., vol. 59, no. 6, pp. 1524–1538, June 2014.\n[29] J.-Y. Joo and M. Ilic, “Multi-layered optimization of demand resources using Lagrange dual decomposition,” IEEE Trans.\nSmart Grid, vol. 4, no. 4, pp. 2081–2088, Dec 2013.\n[30] E. Dall’Anese, H. Zhu, and G. B. Giannakis, “Distributed optimal power flow for smart microgrids,” IEEE Trans. Smart\nGrid, vol. 4, no. 3, pp. 1464–1475, Sept. 2013.\n[31] B. He and X. Yuan, “On the o(1/n) convergence rate of Douglas-Rachford alternating direction method,” SIAM J. Num.\nAnal., vol. 50, 2012.\nFebruary 22, 2016 DRAFT\n35\n[32] W. Deng and W. Yin, “On the global and linear convergence of the generalized alternating direction method of multipliers,”\nRice CAAM technical report 12-14, 2012.\n[33] M. Hong and Z.-Q. Luo, “On the linear convergence of the alternating direction method of multipliers,” available on\narxiv.org.\n[34] Y. Nesterov, Introductory lectures on convex optimization: A basic course. Kluwer Academic Publishers, 2004. [35] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, “Electronic companion for “Asynchronous distributed ADMM for\nlarge-scale optimization- Part I: Algorithm and convergence analysis,” available on http://arxiv.org.\n[36] D. P. Bertsekas, Nonlinear Programming: 2nd Ed. Cambridge, Massachusetts: Athena Scientific, 2003. [37] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, UK: Cambridge University Press, 2004.\nFebruary 22, 2016 DRAFT\n36\nAlgorithm 2 Asynchronous Distributed ADMM for (4).\n1: Algorithm of the Master: 2: Given initial variable x0 and broadcast it to the workers. Set k = 0 and d1 = · · · = dN = 0; 3: repeat 4: wait until receiving {x̂i, λ̂i}i∈Ak from workers i ∈ Ak such that |Ak| ≥ A and di < τ − 1 ∀i ∈ Ack. 5: update\nx k+1 i = { x̂i ∀i ∈ Ak x k i ∀i ∈ Ack , (9)\nλ k+1 i = { λ̂i ∀i ∈ Ak λ k i ∀i ∈ Ack , (10)\ndi = { 0 ∀i ∈ Ak di + 1 ∀i ∈ Ack , (11)\nx k+1 0 =arg min\nx0∈Rn\n{ h(x0)− xT0 ∑N i=1 λ k+1 i\n+ ρ2 ∑N i=1 ‖xk+1i − x0‖2 + γ2‖x0 − xk0‖2 } , (12)\n6: broadcast xk+10 to the workers in Ak. 7: set k ← k + 1. 8: until a predefined stopping criterion is satisfied.\n1: Algorithm of the ith Worker: 2: Given initial λ0 and set ki = 0. 3: repeat 4: wait until receiving x̂0 from the master node. 5: update\nx ki+1 i = arg min\nxi∈Rn fi(xi) + x\nT i λ ki i + ρ 2‖xi − x̂0‖2, (13)\nλ ki+1 i = λ ki i + ρ(x ki+1 i − x̂0). (14)\n6: send (xki+1i ,λ ki+1 i ) to the master node. 7: set ki ← ki + 1. 8: until a predefined stopping criterion is satisfied.\nFebruary 22, 2016 DRAFT\n37\nAlgorithm 3 Asynchronous distributed ADMM from the master’s point of view.\n1: Given initial variables x0 and λ0; set x00 = x 0 and k = 0. 2: repeat 3: update\nx k+1 i =    arg min xi∈Rn { fi(xi) + x T i λ k i + ρ2‖xi − x k̄i+1 0 ‖2 } , ∀i ∈ Ak\nx k i ∀i ∈ Ack\n, (23)\nλ k+1 i =\n{ λ k i + ρ(x k+1 i − xk̄i+10 ) ∀i ∈ Ak\nλ k i ∀i ∈ Ack\n, (24)\nx k+1 0 =arg min\nx0∈Rn\n{ h(x0)− xT0 ∑N i=1 λ k+1 i\n+ ρ2 ∑N i=1 ‖xk+1i − x0‖2 + γ2‖x0 − xk0‖2 } . (25)\n4: set k ← k + 1. 5: until a predefined stopping criterion is satisfied.\nFebruary 22, 2016 DRAFT\n38\nAlgorithm 4 An Alternative Implementation of Asynchronous Distributed ADMM.\n1: Algorithm of the Master: 2: Given initial variable x0 and broadcast it to the workers. Set k = 0 and d1 = · · · = dN = 0; 3: repeat 4: wait until receiving {x̂i, λ̂i}i∈Ak from workers i ∈ Ak such that |Ak| ≥ A and di < τ − 1 ∀i ∈ Ack. 5: update\nx k+1 i = { x̂i ∀i ∈ Ak x k i ∀i ∈ Ack , (44)\ndi = { 0 ∀i ∈ Ak di + 1 ∀i ∈ Ack ,\nx k+1 0 =arg min\nx0∈Rn\n{ h(x0)− xT0 ∑N i=1 λ k i\n+ ρ2 ∑N i=1 ‖xk+1i − x0‖2 + γ2‖x0 − xk0‖2 } , (45)\nλ k+1 i = λ k i + ρ(x ki+1 i − xk+10 ) ∀i ∈ V. (46)\n6: broadcast xk+10 and {λk+1i }i∈Ak to the workers in Ak. 7: set k ← k + 1. 8: until a predefined stopping criterion is satisfied.\n1: Algorithm of the ith Worker: 2: Given initial λ0 and set ki = 0. 3: repeat 4: wait until receiving (x̂0, λ̂i) from the master node. 5: update\nx ki+1 i = arg min\nxi∈Rn fi(xi) + x\nT i λ̂i + ρ 2‖xi − x̂0‖2, (47)\n6: send xki+1i to the master node. 7: set ki ← ki + 1. 8: until a predefined stopping criterion is satisfied.\nFebruary 22, 2016 DRAFT"
    } ],
    "references" : [ {
      "title" : "Asynchronous distributed alternating direction method of multipliers: Algorithm and convergence analysis",
      "author" : [ "T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang" ],
      "venue" : "submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Convex optimization for big data",
      "author" : [ "V. Cevher", "S. Becker", "M. Schmidt" ],
      "venue" : "IEEE Signal Process. Mag., pp. 32–43, Sept. 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Scaling up Machine Learning- Parallel and Distributed Approaches",
      "author" : [ "R. Bekkerman", "M. Bilenko", "J. Langford" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Parallel and distributed computation: Numerical methods",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1989
    }, {
      "title" : "Regression shrinkage and selection via the LASSO",
      "author" : [ "R. Tibshirani" ],
      "venue" : "J. Roy. Stat. Soc. B, vol. 58, pp. 267–288, 1996.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Large-scale sparse logistic regression",
      "author" : [ "J. Liu", "J. Chen", "J. Ye" ],
      "venue" : "Proc. ACM Int. Conf. on Knowledge Discovery and Data Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547–556.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "Alternating maximization: Unifying framework for 8 sparse PCA formulations and efficient parallel codes",
      "author" : [ "P. Richtárik", "M. Takáč", "S.D. Ahipasaoglu" ],
      "venue" : "[Online] http://arxiv.org/abs/1212.4137.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1212
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "F. Niu", "B. Recht", "C. Re", "S.J. Wright" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "A. Agarwal", "J.C. Duchi" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Parameter server for distributed machine learning",
      "author" : [ "M. Li", "L. Zhou", "Z. Yang", "A. Li", "F. Xia", "D.G. Andersen", "A. Smola" ],
      "venue" : "[Online] http://www.cs.cmu.edu/∼muli/file/ps.pdf. February 22, 2016  DRAFT  34",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Distributed delayed proximal gradient methods",
      "author" : [ "M. Li", "D.G. Andersen", "A. Smola" ],
      "venue" : "[Online] http://www.cs.cmu.edu/ ∼muli/file/ddp.pdf.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Asynchronous stochastic coordinate descent: Parallelism and convergence properties",
      "author" : [ "J. Liu", "S.J. Wright" ],
      "venue" : "SIAM J. Optim.,, vol. 25, no. 1, pp. 351–376, Feb. 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Parallel successive convex approximation for nonsmooth nonconvex optimization",
      "author" : [ "M. Razaviyayn", "M. Hong", "Z.-Q. Luo", "J.S. Pang" ],
      "venue" : "the Proceedings of the Neural Information Processing (NIPS), 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Decomposition by partial linearization: Parallel optimization of multi-agent systems",
      "author" : [ "G. Scutari", "F. Facchinei", "P. Song", "D.P. Palomar", "J.-S. Pang" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641–656, 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Hybrid random/deterministic parallel algorithms for nonconvex big data optimization",
      "author" : [ "A. Daneshmand", "F. Facchinei", "V. Kungurtsev", "G. Scutari" ],
      "venue" : "to appear in IEEE Trans. on Signal Processing [Online] http://www.eng.buffalo.edu/ ∼gesualdo/Papers/DanFaccKungTSPsub14.pdf.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems",
      "author" : [ "M. Hong", "Z.-Q. Luo", "M. Razaviyayn" ],
      "venue" : "to appear in SIAM J. Opt.; available on http://arxiv.org/pdf/1410.1390.pdf.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1390
    }, {
      "title" : "Asynchronous distributed ADMM for consensus optimization",
      "author" : [ "R. Zhang", "J.T. Kwok" ],
      "venue" : "Proc. 31th ICML, , 2014., Beijing, China, June 21-26, 2014, pp. 1–9.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A distributed, asynchronous and incremental algorithm for nonconvex optimization: An ADMM based approach",
      "author" : [ "M. Hong" ],
      "venue" : "technical report; available on http://arxiv.org/pdf/1412.6058.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1412
    }, {
      "title" : "Asynchronous distributed optimization using a randomized alternating direction method of multipliers",
      "author" : [ "F. Iutzeler", "P. Bianchi", "P. Ciblat", "W. Hachem" ],
      "venue" : "Proc. IEEE CDC, Florence, Italy, Dec. 10-13, 2013, pp. 3671–3676.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the O(1/K) convergence of asynchronous distributed alternating direction method of multipliers",
      "author" : [ "E. Wei", "A. Ozdaglar" ],
      "venue" : "available on arxiv.org.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "D-ADMM: A communication-efficient distributed algorithm for separable optimization",
      "author" : [ "J.F.C. Mota", "J.M.F. Xavier", "P.M.Q. Aguiar", "M. Puschel" ],
      "venue" : "IEEE. Trans. Signal Process., vol. 60, no. 10, pp. 2718–2723, May 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Decentralized low-rank matrix completion",
      "author" : [ "Q. Ling", "Y. Xu", "W. Yin", "Z. Wen" ],
      "venue" : "Proc. IEEE ICASSP, Kyoto, Japan, March 25-30, 2012, pp. 2925–2928.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Asynchronous distributed ADMM for large-scale optimization- Part II: Linear convergence analysis and numerical performance",
      "author" : [ "T.-H. Chang", "W.-C. Liao", "M. Hong", "X. Wang" ],
      "venue" : "submitted for publication.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Sparisty and smoothness via the fused lasso",
      "author" : [ "R. Tibshirani", "M. Saunders" ],
      "venue" : "J. R. Statist. Soc. B, vol. 67, no. 1, pp. 91–108, 2005.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Convergence analysis of ADMM based power system mode estimation under asynchronous wide-area communication delays",
      "author" : [ "J. Zhang", "S. Nabavi", "A. Chakrabortty", "Y. Xin" ],
      "venue" : "Proc. IEEE PES General Meeting, Denver, CO, USA, July 26-30, 2015, pp. 1–5.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distributed constrained optimization by consensus-based primal-dual perturbation method",
      "author" : [ "T.-H. Chang", "A. Nedić", "A. Scaglione" ],
      "venue" : "IEEE. Trans. Auto. Control., vol. 59, no. 6, pp. 1524–1538, June 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-layered optimization of demand resources using Lagrange dual decomposition",
      "author" : [ "J.-Y. Joo", "M. Ilic" ],
      "venue" : "IEEE Trans. Smart Grid, vol. 4, no. 4, pp. 2081–2088, Dec 2013.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed optimal power flow for smart microgrids",
      "author" : [ "E. Dall’Anese", "H. Zhu", "G.B. Giannakis" ],
      "venue" : "IEEE Trans. Smart Grid, vol. 4, no. 3, pp. 1464–1475, Sept. 2013.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the o(1/n) convergence rate of Douglas-Rachford alternating direction method",
      "author" : [ "B. He", "X. Yuan" ],
      "venue" : "SIAM J. Num. Anal., vol. 50, 2012. February 22, 2016  DRAFT  35",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the global and linear convergence of the generalized alternating direction method of multipliers",
      "author" : [ "W. Deng", "W. Yin" ],
      "venue" : "Rice CAAM technical report 12-14, 2012.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the linear convergence of the alternating direction method of multipliers",
      "author" : [ "M. Hong", "Z.-Q. Luo" ],
      "venue" : "available on arxiv.org.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2004
    }, {
      "title" : "Electronic companion for “Asynchronous distributed ADMM for large-scale optimization- Part I: Algorithm and convergence analysis",
      "author" : [ "T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang" ],
      "venue" : "available on http://arxiv.org.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Nonlinear Programming: 2nd Ed",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Part of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "Background Scaling up optimization algorithms for future data-intensive applications calls for efficient distributed and parallel implementations, so that modern multi-core high performance computing technologies can be fully utilized [2]–[4].",
      "startOffset" : 235,
      "endOffset" : 238
    }, {
      "referenceID" : 3,
      "context" : "Background Scaling up optimization algorithms for future data-intensive applications calls for efficient distributed and parallel implementations, so that modern multi-core high performance computing technologies can be fully utilized [2]–[4].",
      "startOffset" : 239,
      "endOffset" : 242
    }, {
      "referenceID" : 4,
      "context" : "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].",
      "startOffset" : 243,
      "endOffset" : 246
    }, {
      "referenceID" : 2,
      "context" : "In this paper, we focus on solving large-scale instances of these learning problems with either a large number of training samples or a large number of features (n is large) [3].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]–[16].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 8,
      "context" : "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]–[16].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]–[16].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]–[17] parallelized the block coordinate descent (BCD) method.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]–[17] parallelized the block coordinate descent (BCD) method.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]–[17] parallelized the block coordinate descent (BCD) method.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]–[17] parallelized the block coordinate descent (BCD) method.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]–[17] parallelized the block coordinate descent (BCD) method.",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 16,
      "context" : "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]–[17] parallelized the block coordinate descent (BCD) method.",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 9,
      "context" : "To address such dilemma, a few recent works [10]–[14] have introduced “asynchrony” into the distributed algorithms, which allows the master to perform updates when not all, but a small subset of workers have returned their gradient information.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "To address such dilemma, a few recent works [10]–[14] have introduced “asynchrony” into the distributed algorithms, which allows the master to perform updates when not all, but a small subset of workers have returned their gradient information.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 9,
      "context" : "As has been consistently reported in [10]–[14], under such an asynchronous protocol, the computation time can decrease almost linearly with the number of workers.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "As has been consistently reported in [10]–[14], under such an asynchronous protocol, the computation time can decrease almost linearly with the number of workers.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "Notably, other than the standard convex setting [9], the recent analysis in [18] has shown that such distributed ADMM is provably convergent to a Karush-Kuhn-Tucker (KKT) point even for non-convex problems.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "Notably, other than the standard convex setting [9], the recent analysis in [18] has shown that such distributed ADMM is provably convergent to a Karush-Kuhn-Tucker (KKT) point even for non-convex problems.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]–[14].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]–[14].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]–[14].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]–[14].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : "Specifically, reference [19] has considered a version of AD-ADMM with bounded delay assumption and studied its theoretical and numerical performances.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "However, only convex cases are considered in [19].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]–[14], the workers compute gradient information only.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]–[14], the workers compute gradient information only.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]–[14], the workers compute gradient information only.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "References [21]–[23] have respectively considered asynchronous ADMM methods for decentralized optimization over networks.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 22,
      "context" : "References [21]–[23] have respectively considered asynchronous ADMM methods for decentralized optimization over networks.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 20,
      "context" : "Specifically, the asynchrony in [21] lies in that, at each iteration, the nodes are randomly activated to perform variable update.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 21,
      "context" : "The method presented in [22] further allows that the communications between nodes can succeed or fail randomly.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : "It is shown in [22] that such asynchronous ADMM can converge in a probability-one sense, provided that the nodes and communication links satisfy certain statistical assumption.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 22,
      "context" : "Reference [23] has considered an asynchronous dual ADMM method.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 8,
      "context" : "Contributions In this paper1, we generalize the state-of-the-art synchronous distributed ADMM [9], [18] to the asynchronous setting.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 17,
      "context" : "Contributions In this paper1, we generalize the state-of-the-art synchronous distributed ADMM [9], [18] to the asynchronous setting.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "Like [10]–[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : "Like [10]–[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 18,
      "context" : "Like [10]–[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "Like [10]–[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Therefore, the analysis and algorithm proposed here are applicable not only to standard convex learning problems but also to important non-convex problems such as the sparse PCA problem [8] and matrix factorization problems [24].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 23,
      "context" : "Therefore, the analysis and algorithm proposed here are applicable not only to standard convex learning problems but also to important non-convex problems such as the sparse PCA problem [8] and matrix factorization problems [24].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 19,
      "context" : "To the best of our knowledge, except the inexact version in [20], this is the first time that the distributed ADMM is rigorously shown to be convergent for non-convex problems under the asynchronous protocol.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 21,
      "context" : "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, we demonstrate that the asynchrony of ADMM has to be handled with care – as a slight modification of the algorithm may In contrast to the conference paper [1], the current paper presents detailed proofs of theorems and more simulation results.",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 24,
      "context" : "In the companion paper [25], the linear convergence conditions of the AD-ADMM is further analyzed.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "Synopsis: Section II presents the applications of problem (1) and reviews the distributed ADMM in [9].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "Such distributed optimization approach is extremely useful in modern big data applications [3].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "For example, let us consider the following regularized empirical risk minimization problem [7] min w∈R m ∑",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 6,
      "context" : "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 26,
      "context" : "It is interesting to mention that many emerging problems in smart power grid can also be formulated as problem (1); see, for example, the power state estimation problem considered in [27] is solved by employing the distributed ADMM.",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 27,
      "context" : ", demand response) in [28]–[30] can potentially be handled by the distributed ADMM as well.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : ", demand response) in [28]–[30] can potentially be handled by the distributed ADMM as well.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "Distributed ADMM In this section, we present the distributed ADMM [4], [9] for solving problem (1).",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "Distributed ADMM In this section, we present the distributed ADMM [4], [9] for solving problem (1).",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "It has been shown that such a consensus problem can be efficiently solved by the ADMM [9].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "According to [4], the standard synchronous ADMM iteratively updates the primal variables xi, i = 0, 1, .",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "Algorithm 1 (Synchronous) Distributed ADMM for (4) [9] 1: Given initial variables x0 and λ0; set x0 = x 0 and k = 0.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : ", [9], [18], [31]–[33].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 17,
      "context" : ", [9], [18], [31]–[33].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 30,
      "context" : ", [9], [18], [31]–[33].",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 32,
      "context" : ", [9], [18], [31]–[33].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 30,
      "context" : "Specifically, [31] shows that the ADMM, under general convex assumptions, has a worst-case O(1/k) convergence rate; while [32] shows that the ADMM can have a linear convergence rate given strong convexity and smoothness conditions on fi’s.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 31,
      "context" : "Specifically, [31] shows that the ADMM, under general convex assumptions, has a worst-case O(1/k) convergence rate; while [32] shows that the ADMM can have a linear convergence rate given strong convexity and smoothness conditions on fi’s.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "For non-convex and smooth fi’s, the work [18] shows that Algorithm 1 can converge to the set of KKT points with a O(1/ √ k) rate as long as ρ is large enough.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "The asynchronism we consider is in the same spirit of [10]–[14], [19], [20], where the master does not wait for all the workers.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "The asynchronism we consider is in the same spirit of [10]–[14], [19], [20], where the master does not wait for all the workers.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "The asynchronism we consider is in the same spirit of [10]–[14], [19], [20], where the master does not wait for all the workers.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "The asynchronism we consider is in the same spirit of [10]–[14], [19], [20], where the master does not wait for all the workers.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "In particular, we follow the popular partially asynchronous model [4] and assume: Assumption 1 (Bounded delay) Let τ ≥ 1 be a maximum tolerable delay.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : ", |Ak| ≥ A for all k [19].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 24,
      "context" : "Detailed numerical results will be reported in Section V of the companion paper [25].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "Let us compare Theorem 1 with the results in [19], [22].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "Let us compare Theorem 1 with the results in [19], [22].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "First, the convergence conditions in [19], [22] are only applicable for convex problems, whereas our results hold for both convex and non-convex problems.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "First, the convergence conditions in [19], [22] are only applicable for convex problems, whereas our results hold for both convex and non-convex problems.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "Second, [19], [22] have made specific statistical assumptions on the behavior of the workers, and the convergence results presented therein are in an expectation sense.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 21,
      "context" : "Second, [19], [22] have made specific statistical assumptions on the behavior of the workers, and the convergence results presented therein are in an expectation sense.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "Therefore it is possible, at least theoretically, that a realization of the algorithm fails to converge despite satisfying the conditions given in [19].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "Inspired by [18], our analysis for Theorem 1 investigates how the augmented Lagrangian function, i.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 32,
      "context" : "Such insight is reminiscent of the recent convergence results for multi-block ADMM in [33].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "To verify this point, let us consider the following sparse PCA problem [8] min w∈R − N ∑",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "The conditions under which linear convergence can be achieved are presented in the companion paper [25].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "Numerical results which demonstrate the time efficiency of the proposed AD-ADMM on a high performance computer cluster are also presented in [25].",
      "startOffset" : 141,
      "endOffset" : 145
    } ],
    "year" : 2016,
    "abstractText" : "Aiming at solving large-scale optimization problems, this paper studies distributed optimization methods based on the alternating direction method of multipliers (ADMM). By formulating the optimization problem as a consensus problem, the ADMM can be used to solve the consensus problem in a fully parallel fashion over a computer network with a star topology. However, traditional synchronized computation does not scale well with the problem size, as the speed of the algorithm is limited by the slowest workers. This is particularly true in a heterogeneous network where the computing nodes experience different computation and communication delays. In this paper, we propose an asynchronous distributed ADMM (AD-ADMM) which can effectively improve the time efficiency of distributed optimization. Our main interest lies in analyzing the convergence conditions of the AD-ADMM, under the popular partially asynchronous model, which is defined based on a maximum tolerable delay of the network. Specifically, by considering general and possibly non-convex cost functions, we show that the AD-ADMM is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points as long as the algorithm parameters are chosen appropriately according to the network delay. We further illustrate that the asynchrony of the ADMM has to be handled with care, as slightly modifying the implementation of the AD-ADMM can jeopardize the algorithm convergence, even under the standard convex setting. Keywords− Distributed optimization, ADMM, Asynchronous, Consensus optimization Part of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1]. Tsung-Hui Chang is supported by NSFC, China, Grant No. 61571385. Mingyi Hong is supported by NFS Grant No. CCF-1526078 , and AFOSR, Grant No. 15RT0767. Xiangfeng Wang is supported by Shanghai YangFan No. 15YF1403400 and NSFC No. 11501210. Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172. E-mail: tsunghui.chang@ieee.org. Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: liaox146@umn.edu Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, School of Computer Science and Software Engineering, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn February 22, 2016 DRAFT",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}