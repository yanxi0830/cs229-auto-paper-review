{
  "name" : "1507.00093.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Study of Gradient Descent Schemes for General-Sum Stochastic Games",
    "authors" : [ "H. L. Prasad", "Shalabh Bhatnagar" ],
    "emails" : [ "hlprasu@csa.iisc.ernet.in", "shalabh@csa.iisc.ernet.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 7.\n00 09\n3v 1\n[ cs\n.L G\n] 1\nZero-sum stochastic games are easy to solve as they can be cast as simple Markov decision processes. This is however not the case with general-sum stochastic games. A fairly general optimization problem formulation is available for general-sum stochastic games by Filar and Vrieze [2004]. However, the optimization problem there has a non-linear objective and non-linear constraints with special structure. Since gradients of both the objective as well as constraints of this optimization problem are well defined, gradient based schemes seem to be a natural choice. We discuss a gradient scheme tuned for two-player stochastic games. We show in simulations that this scheme indeed converges to a Nash equilibrium, for a simple terrain exploration problem modelled as a general-sum stochastic game. However, it turns out that only global minima of the optimization problem correspond to Nash equilibria of the underlying general-sum stochastic game, while gradient schemes only guarantee convergence to local minima. We then provide important necessary conditions for gradient schemes to converge to Nash equilibria in general-sum stochastic games.\nKeywords: Game theory, Nonlinear programming, Non-convex constrained problems, Discounted cost criteria, General-sum stochastic games, Nash equilibrium."
    }, {
      "heading" : "1 Introduction",
      "text" : "Game theory is seen as a useful means to handle multi-agent scenarios. Since the seminal work of Shapley [1953], stochastic games have been an important class of models for multi-agent systems. A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc., can be modelled as stochastic games, see Filar and Vrieze [2004]. One of the significant results is that every stochastic game has a Nash equilibrium and it can be characterised in terms of global minima of a suitable mathematical programming problem.\nAs an application of general-sum games to the multi-agent scenario, Singh et al. [2000] observed that in a two-agent iterated general-sum game, Nash convergence is assured either in strategies or in the very least in average payoffs. Later by Hu and Wellman [1999], stochastic game theory was observed to be a better framework for multi-agent scenarios as it could be viewed as an extension of the well studied Markov decision theory (see Bertsekas [1995]). However, in the stochastic game setting, general-sum games are difficult to solve as, unlike\n∗hlprasu@csa.iisc.ernet.in †shalabh@csa.iisc.ernet.in\nzero-sum games, they cannot be cast in the framework similar to Markov decision processes. Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986].\nA new type of approach based on homotopy is proposed by Herings and Peeters [2004]. In this approach, a homotopic path between equilibrium points of N independent MDPs and the N -player stochastic game in question, is traced numerically giving a Nash equilibrium point of the stochastic game of interest. Their result applies to both normal form as well as extensive form games. However, this approach has a complexity similar to that of typical gradient descent schemes discussed in this paper. For more recent developments in this direction, see the work by Herings and Peeters [2006] and Borkovsky et al. [2010].\nA recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed. Though their experiments do show convergence in a large group of randomly generated games, a formal proof of convergence has not been provided.\nFor general-sum stochastic games, [Breton et al., 1986, Section 4.3] provides an interesting optimization problem with non-linear objectives and linear constraints whose global minima correspond to Nash equilibria of the underlying general-sum stochastic game. However, since the objective is not guaranteed to be convex, simple gradient descent techniques might not converge to a global minimum. Mac Dermed and Isbell [2009] formulate intermediate optimization problems, called Multi-Objective Linear Programs (MOLPs), to compute Nash equilibria as well as Pareto optimal solutions. However, as mentioned in that paper, the complexity of their algorithm scales exponentially with the problem size. Thus, their algorithm is tractable only for small sized problems with a few tens of states.\nAnother non-linear optimization problem for computing Nash equilibria in general-sum stochastic games has been given by Filar and Vrieze [2004]. We begin with this optimization problem by discussing it in Section 2. Gradient based techniques are quite common for solving optimization problems. In the optimization problem, gradients of (both) the objective and all constraints w.r.t. the value vector v and strategy vector π, are well defined. A possible solution approach is to apply simple gradient based techniques to solve these optimization problems. We look at a possible gradient descent scheme in Section 3. In the process of construction of this scheme, several initial hurdles for gradient descent schemes are discussed and addressed. We then consider an example problem of terrain exploration, modelled as a general-sum stochastic game, in Section 4. In the same section, we show via simulation that the gradient descent scheme of Section 3 does indeed give a Nash equilibrium solution to the terrain exploration problem. In our case, the optimization problem at hand has only global minima correspond to Nash equilibria of the underlying general-sum stochastic game as discussed later in Section 2.4. It is well known that gradient descent schemes can only guarantee convergence to local minima. But, in the optimization problems that we consider, global minima are desired. So, a question remains: Are simple gradient descent schemes good enough to give Nash equilibria in the aforementioned optimization problems? In other words, are there only global minimum points in these optimization problems, so that simple gradient descent schemes can easily work? We\naddress this issue in Section 5. Finally, in Section 6 we provide the concluding remarks."
    }, {
      "heading" : "2 The Optimization Problem",
      "text" : "The framework of a general-sum stochastic game is described in Section 2.1. A basic idea of the optimization problem is given in Section 2.2. The full optimization problem is then formulated in Section 2.3 for the infinite horizon discounted reward setting. Some important results by Filar and Vrieze [2004] that are applicable here are then described."
    }, {
      "heading" : "2.1 Stochastic Games",
      "text" : "A two-agent scenario is considered in the following formulation. One can in general consider an N -agent scenario for N ≥ 2. We assume N = 2 only for notational simplicity. We interchangeably use the terms ‘agent’ and ‘player’ to mean the same entity in the description below. We assume that the stochastic game terminates in a finite but random time. Hence, a discounted value framework in dynamic programming has been chosen for the optimization problem.\nA stochastic game is described via a tuple < S,A, p, r >. The quantities in the tuple are explained through the description below. (i) S denotes the state space. (ii) Ai(x) denotes the action space for the ith agent, i = 1, 2. A(x) =\n2 × i=1\nAi(x), the Cartesian product, is the aggregate action space consisting of all possible actions of both agents when the state of the game is x ∈ S. (iii) p(y|x, a) denotes the probability of going from the state x ∈ S at the current instant to y ∈ S at the immediate next instant when the action a ∈ A(x) is chosen. (iv) Finally, r(x, a) denotes the vector of reward functions of both agents when the state is x ∈ S and the vector of actions a ∈ A(x) is chosen.\nFor an infinite horizon discounted reward setting, a discount factor 0 < β < 1 is also included in the tuple describing the game. As is clear from this definition, a stochastic game can be viewed as an extension of the single-agent Markov decision process.\nA strategy πi △ = 〈 πi1, π i 2, . . . , π i t, . . . 〉 of the ith player in a stochastic game prescribes the action to be performed in each state at each time instant t, by that player. We denote by πit(·) the action prescribed for the ith agent by the strategy πit at time instant t. The quantity ‘·’ in πit(·), in general, corresponds to the entire history of states and actions of all agents up to the (t − 1)st instant and the current system state at the tth instant. Let the set of all possible strategies for the ith player be denoted by F i. A strategy πi of player i, is said to be a Markov strategy if πit depends only on the current state xt ∈ S at time t. Thus, for a Markov strategy πi of player i, πit(x) ∈ Ai(x), ∀t ≥ 0, x ∈ S, i = 1, 2. If the action chosen in any state for a Markov strategy πi is independent of the time instant t, viz., πit ≡ π̄i, ∀t ≥ 0, i = 1, 2, for some π̄ such that π̄i(x) ∈ Ai(x), ∀x ∈ S, then the strategy is said be stationary. Henceforth, we shall restrict our attention to stationary strategies only. By abuse of notation, we denote by π itself the stationary strategy. Extending this to all players, we denote a Markov strategy-tuple by π △ = 〈 π1, π2 〉\nLet∆(A(x)) (resp. ∆(Ai(x))) denote the set of all probability measures onA(x) (resp. Ai(x)). A randomized Markov strategy is specified via the sequence of maps φit : S → ∆(Ai(x)), x ∈ S, t ≥ 0, i = 1, 2. Thus, φit(x) is a distribution on the set of actions Ai(x) and in general depends on time instant t. We say that φit is a stationary\nrandomized strategy or simply a randomized strategy for player i if φit ≡ φi. By an abuse of notation, we denote by π = 〈 π1, π2 〉 , a stationary randomized strategy-tuple that we also (many times) call a strategy, since from now on, we shall only work with randomized strategies. We use πi(x, a) to denote the probability of picking action a ∈ Ai(x) in state x ∈ S by agent i. [Filar and Vrieze, 2004, Theorem 3.8.1, pp. 130] states that a Nash equilibrium in stationary randomized strategies exists for general-sum discounted stochastic games. We will refer to such stationary randomized strategies as Nash strategies. Similar to MDPs [Bertsekas, 1995], one can define the value function as follows:\nviπ(x0) = E\n  ∑\nt\nβt ∑\na∈A(x)\n( ri(xt, a) 2∏\ni=1\nπi(xt, a) )  , ∀i = 1, 2. (1)\nLet π−i represent the strategy of the agent other than the ith agent, that is, π−1 = π2 and π−2 = π1 respectively. Formally, we define Nash strategies and Nash equilibrium below. Definition 1 (Nash Equilibrium) A stationary Markov strategy π∗ = 〈 π1∗, π2∗ 〉 is said to be Nash if\nviπ∗(x) ≥ vi〈πi,π−i∗〉(x), ∀πi, i = 1, 2, ∀x ∈ S.\nThe corresponding equilibrium of the game is said to be a Nash equilibrium.\nLike in normal-form games [Nash, 1950], pure strategy Nash equilibria may not exist in the case of stochastic games. Using dynamic programming, the Nash equilibrium condition can be written as:\nvi(x) = max πi(x)∈∆(Ai(x))   Eπ(x)  ri(x, a) + β ∑\ny∈U(x)\np(y|x, a)vi(y)      , ∀i = 1, 2. (2)\nUnlike MDPs, (2) involves two maximization equations. Note that the two equations are coupled because the reward of one agent is influenced by the strategy of the other agent and so is the state transition."
    }, {
      "heading" : "2.2 The Basic Formulation",
      "text" : "The dynamic programming equation (2) for finding optimal values can now be revised to:\nvi(x) = max πi(x)∈∆(Ai(x))\n{ Eπi(x)Q i(x, ai) } , ∀x ∈ S, ∀i = 1, 2, (3)\nwhere\nQi(x, ai) = Eπ−i(x)\n ri(x, a) + β ∑\ny∈U(x)\np(y|x, a)vi(y)   ,\nrepresents the marginal value associated with picking action ai ∈ Ai(x), in state x ∈ S for agent i. Also, ∆(Ai(x)) denotes the set of all possible probability distributions over Ai(x). We derive a possible optimization problem from (3) in Section 2.2.1 followed by a discussion of possible constraints on the feasible solutions in Section 2.2.2."
    }, {
      "heading" : "2.2.1 The objective",
      "text" : "Equation (3) says that vi(x) represents the maximum value of EπiQi(x, ai) over all possible convex combinations of policy of agent i, πi ∈ ∆(Ai(x)). However, neither the optimal value vi(x) nor the optimal policy πi are known apriori. So, a possible optimization objective would be\nf i(vi, πi) = ∑\nx∈S\n( vi(x)− EπiQi(x, ai) ) ,\nwhich will have to be minimized over all possible policies πi ∈ ∆(Ai(x)). But Qi(x, ai), by definition, is dependent on strategies of all other agents. So, an isolated minimization of f i(vi, πi) would really not make sense. Rather we need to consider the aggregate objective,\nf(v, π) =\n2∑\ni=1\nf i(vi, πi),\nwhich is minimized over all possible policies πi ∈ ∆(Ai(x)), i = 1, 2. Thus, we have an optimization problem with objective as f(v, π) along with natural constraints ensuring that the policy vectors πi(x) remain as probabilities over all possible actions Ai(x) for all states x ∈ S for both agents. Formally, we write this optimization problem as below:\nmin v,π\nf(v, π) = 2∑\ni=1 ∑ x∈S ( vi(x) − EπiQi(x, ai) ) s.t.\n(a)πi(x, ai) ≥ 0, ∀ai ∈ Ai(x), x ∈ S, i = 1, 2, (b) 2∑\ni=1\nπi(x, ai) = 1, ∀x ∈ S, i = 1, 2.\n  \n(4)\nIntuitively, all those (v, π) pairs which make f(v, π) as zero with π satisfying (4(a))-(4(b)), should correspond to Nash equilibria of the corresponding general-sum discounted stochastic game. The question is: Is this true? We address this question in two parts. First, if π∗ represents a Nash strategy-tuple with v∗ as the corresponding dynamic programming value obtained from (2), then we answer the question whether f(v∗, π∗) is zero? Second, if (v∗, π∗) is such that (4(a))-(4(b)) are satisfied and f(v∗, π∗) is zero, then whether π∗ is a Nash strategy-tuple? We address these two questions in Lemmas 2.1 and 2.2.\nLemma 2.1 Let (v∗, π∗) represent a possible solution for the dynamic programming equation (2). Then, (v∗, π∗) is a feasible solution of the optimization problem (4) and f(v∗, π∗) = 0.\nProof: Proof follows simply from the construction of the optimization problem (4).\nLemma 2.2 Let (v∗, π∗) be a feasible solution of the optimization problem (4) such that f(v∗, π∗) = 0. Then, π∗ need not be Nash strategy-tuple and v∗ need not correspond to the dynamic programming value obtained from (2).\nProof: We provide a proof by example. Choose a π∗ such that it is not a Nash strategy-tuple. Then, to make f(v∗, π∗) = 0, we need to compute a v∗ such that\nvi∗(x) − Eπi∗(x)Qi(x, ai) = 0, ∀x ∈ S, i = 1, 2. (5)\nLet Ri = 〈 Eπ∗(x)r i(x, a) : x ∈ S 〉 be a column vector over rewards to agent i in various states of the underlying game. Also, let P = [ Eπ∗(x)p(y|x, a) : x ∈ S, y ∈ S ] represent the state-transition matrix of the underlying Markov process. Then, (5) can be written in vector form as\nv i∗ − ( Ri + βPvi∗ ) = 0, ∀i = 1, 2. (6)\nSince P is a stochastic matrix, all its eigen-values are less than or equal to one. Thus, the matrix I − βP is invertible. So the system of equations (6) has a unique solution with\nv i∗ = (I − βP )−1 Ri, i = 1, 2.\nThus for any strategy-tuple π∗, which need not be Nash, there exists a corresponding v∗ such that f(v∗, π∗) = 0."
    }, {
      "heading" : "2.2.2 Constraints",
      "text" : "The basic optimization problem (4) has only a set of simple constraints ensuring that π remains a valid strategy. As shown in lemma 2.2, this optimization problem is not sufficient to accurately represent Nash equilibria of the underlying general-sum discounted stochastic game. Here, we look at a possible set of additional constraints which might make the optimization problem more useful. Note that the term being maximized in equation (3), i.e., EπiQ i(x, ai), represents a convex combination of the values of Qi(x, ai) over all possible actions ai ∈ Ai(x) in a given state x ∈ S for a given agent i. Thus, it is implicitly implied that\nQi(x, ai) ≤ vi(x), ∀ai ∈ Ai(x), x ∈ S, i = 1, 2, . . . , N.\nSo, we could consider a new optimization problem with these additional constraints. However, the previously posed question remains: Is this good enough to make f(v, π) = 0, for a feasible (v, π) to correspond to a Nash equilibrium? We show that this is indeed true in the next section."
    }, {
      "heading" : "2.3 Optimization Problem for two-player Stochastic Games",
      "text" : "An optimization problem on similar lines as in Section 2.2, for a two-player general-sum discounted stochastic game has been given by Filar and Vrieze [2004]. The optimization problem is as follows:\nmin v,π\nf(v, π) = 2∑\ni=1\n1T|S| [ v i − ri(π)− βP (π)vi ] s.t.\n(a)π2(x)T [ r 1(x) + β\n∑ y∈U(x)\nP (y|x)v1(y) ] ≤ v1(x)1Tm1(x) ∀x ∈ S\n(b) [ r 2(x) + β\n∑ y∈U(x)\nP (y|x)v2(y) ] π1(x) ≤ v2(x)1m2(x) ∀x ∈ S\n(c)π1(x)T 1m1(x) = 1 ∀x ∈ S (d)π2(x)T 1m2(x) = 1 ∀x ∈ S (e)π1(x, a1) ≥ 0 ∀a1 ∈ A1(x) ∀x ∈ S (f)π2(x, a2) ≥ 0 ∀a2 ∈ A2(x) ∀x ∈ S.\n  \n(7)\nwhere, (i) v = 〈 v i : i = 1, 2 〉 is the vector of value vectors of all agents with vi = 〈 vi(y) : y ∈ S 〉 being the value vector for the ith agent (over all states). Here, vi(x) is the value of the state x ∈ S for the ith agent. (ii) π = 〈 πi : i = 1, 2 〉 and πi = 〈 πi(x) : x ∈ S 〉 , where πi(x) = 〈 πi(x, a) : a ∈ Ai(x) 〉 is the randomized policy vector in state x ∈ S for the ith agent. Here πi(x, a) is the probability of picking action a by the ith agent in state x. (iii) ri(x) = [ ri(x, a1, a2) : a1 ∈ A1(x), a2 ∈ A2(x) ] is the reward matrix for the ith agent when in state x ∈ S\nwith rows corresponding to the actions of the second agent and columns corresponding to that of the first. Here, ri(x, a1, a2) is the reward obtained by the ith agent in state x ∈ S when the first agent has taken action a1 ∈ A1(x) and the second agent a2 ∈ A2(x). (iv) ri(x, a1,A2(x)) is the column in ri(x) corresponding to the action a1 ∈ A1(x) of the first player. Each entry in the column corresponds to one action of the second player which is why we use A2(x) as an argument above. Likewise, we have ri(x,A1(x), a2) is the row in ri(x) corresponding to the action a2 ∈ A2(x) of the second player. (v) ri(π) = ri( 〈 π1, π2 〉 ) = 〈 π2(x)T ri(x)π1(x) : x ∈ S 〉 , where π2(x)T ri(x)π1(x) represents the expected reward for the given state x when actions are selected by both agents according to policies π1 and π2 respectively. (vi) P (y|x) = [p(y|x, a) : a = 〈 a1, a2 〉 , a1 ∈ A1(x), a2 ∈ A2(x)] is a matrix representing the probabilities of transition from the current state x ∈ S to a possible next state y ∈ S at the next instant with rows representing the actions of the second player and columns representing those of the first player. (vii) P (y|x, a1,A2(x)) is the column in P (y|x) corresponding to the case when the first player picks action a1 ∈ A1(x). As with ri(x, a1,A2(x)), each entry in the above column corresponds to an action of the second player. Similarly, P (y|x,A1(x), a2) is the row in P (y|x) corresponding to the case when the second player picks an action a2 ∈ A2(x). (viii) P (π) = P ( 〈 π1, π2 〉 ) = [ π2(x)TP (y|x)π1(x) : x ∈ S, y ∈ S ] is a matrix with columns representing the possible current states x and rows representing the future possible states y. Here, π2(x)TP (y|x)π1(x) represents the transition probability from x to y under policy π. (ix) mi(x) = |Ai(x)|, and (recall that) (x) U(x) ⊆ S represents the set of next states for a given state x ∈ S.\nThe inequality constraints in the optimization problem are quadratic in v and π. The first set of inequality constraints (7(a)) on the first agent are quadratic in v1 and π2 and the second set (7(b)) on the second agent are quadratic in v2 and π1 respectively. However, in both cases, the quadratic terms are only cross products between the components of a value vector and a strategy vector.\nThe objective function is a non-negative cubic function of v and π. All the terms in the objective function consist of only cross terms. The cross terms between the value vector of an agent and the strategy vector of either agent are present in the term P (π)vi of the objective function and those between strategy vectors of the two agents are in the term ri(π).\nWe modify the constraints in the optimization problem (7) by eliminating all the equality constraints in it, as follows: One of the elements πi(x, ai), ai ∈ Ai(x), in each of the equations 1Tmi(x)πi(x, ai) = 1 can be automatically set, thereby resulting in inequality constraints over the remaining components as below. Let ai(x), i = 1, 2 denote the actions eliminated using the equality constraint. Then, the set of constraints can be re-written as in (8).\nπ2(x)T [ r1(x) + β\n∑ y∈U(x)\nP (y|x)v1(y) ] ≤ v1(x)1Tm1(x) ∀x ∈ S\n[ r2(x) + β\n∑ y∈U(x)\nP (y|x)v2(y) ] π1(x) ≤ v2(x)1m2(x) ∀x ∈ S\n∑ ai∈Ai(x)\\{ai(x)} πi(x, ai) ≤ 1 ∀x ∈ S, i = 1, 2, πi(x, ai) ≥ 0 ∀ai ∈ Ai(x)\\{ai(x)} ∀x ∈ S, i = 1, 2.\n  \n(8)\nThe variables πi(x, ai(x)) = 1 − ∑\nai∈Ai(x)\\{ai(x)} π i(x, ai), ∀x ∈ S, i = 1, 2 are implicitly assigned in the\nabove set of constraints. For the sake of simplicity, in the above, the equations related to the values v1 and v2\nare written without performing elimination of the quantities πi(x, ai). For further simplicity, we represent all the inequality constraints in (8) as gj(v, π) ≤ 0, j = 1, 2, . . . n, where n is the total number of constraints."
    }, {
      "heading" : "2.4 Theoretical Results on the Optimization Problem",
      "text" : "The optimization problem described above is applicable for general-sum two-agent discounted stochastic games. [Filar and Vrieze, 2004, Theorems 3.8.1–3.8.3] are given below as Theorems 2.3-2.5. See [Filar and Vrieze, 2004, pp. 130-132] for a proof of these results.\nTheorem 2.3 In a general-sum, discounted stochastic game, there exists a Nash equilibrium in stationary strategies.\nTheorem 2.4 Consider a tuple (v̂, π̂). The strategy π̂ forms a Nash equilibrium for the general-sum discounted game if and only if (v̂, π̂) is the global minimum of the optimization problem with f(v̂, π̂) = 0.\nThus, the optimization problem defined in (7) has at least one global optimum having value zero which corre-\nsponds to the Nash equilibrium for the stochastic game.\nTheorem 2.5 Let (v̂, π̂) be a feasible point for (7) with an objective function value γ > 0. Then π̂, forms an ǫ-Nash equilibrium with ǫ ≤ γ 1− β .\nThe above result in simple terms, says that, being in a small neighbourhood of a global optimal point of the optimization problem (7) corresponds to being in a small neighbourhood of the corresponding Nash equilibrium. Thus, there is a correspondence between global optima and Nash equilibria. Thus, this is an important result from the point of view of numerical convergence behaviour."
    }, {
      "heading" : "3 A Gradient Descent Scheme",
      "text" : "The optimization problem (7) for two-player general-sum stochastic games, has an interesting structure with only cross products between optimization variables appearing in both the objective function as well as the constraints. So, as the first naive way of handling this optimization problem, we see whether the same can be broken down into smaller problems via a uni-variate type scheme [Rao, 1996, Section 5.4, pp. 350]. It is possible to see that the original problem can be split into two sets of linear optimization problems with (i) the first set having two optimization problems in v1 and v2 separately. Here, π is held constant; and, (ii) the second having one in〈 π1(x), π2(x) 〉 for every possible state x ∈ S. In each of these cases, v is held constant. Thus, with a uni-variate type of break down of the original problem, we get several smaller problems that can be easily solved. However, a major drawback of this approach is the inherent deficiency of the uni-variate methods which do not have guaranteed convergence in general. In fact, we observed in simulations and also through numerical calculations that this approach does indeed fail because of the above mentioned deficiency. Hence, we look at devising a non-linear programming approach. The algorithm to be discussed is mainly based on an interior-point search algorithm by Herskovits [1986].\nWe first discuss the difficulties posed by the optimization problem (7). We try to address these issues in the subsequent sections by presenting a suitable gradient-based algorithm. With a suitable initial feasible point, the iterative procedure of Herskovits [1986] converges to a constrained local minimum of a given optimization problem. The unmodified algorithm of Herskovits [1986] is presented in Section 3.2. Section 3.3 discusses a\nscheme for finding an initial feasible point. Exploiting the knowledge about the functional forms of the objective and constraints, we present in Section 3.5 our modification in Herskovits algorithm to the procedure of selection of a suitable step length. And finally the modified algorithm in full, is provided in Section 3.6."
    }, {
      "heading" : "3.1 Difficulties",
      "text" : "We note that the optimization problem (7) presents the following difficulties.\n1. Dimensionality - The numbers of variables and constraints involved in the optimization problem are large.\nFor the two agent scenario, the number of variables can be shown to be twice the sum of the cardinalities of the state and action spaces. For instance, in the terrain exploration problem discussed in Section 4, for a simple 4 × 4 grid terrain with two agents and two objects, the number of variables is (647 × 2) + (4169 × 2) = 9632. The total number of inequality constraints for the same can also be computed to be (4169× 2) + (4169× 2) = 16676.\n2. Non-convexity - The constraint region in the optimization problem is not necessarily convex. In fact, during simulations related to the terrain exploration problem (Section 4), we observed that the condition does not\nhold for many constraints. So, in general, the optimization problem (7) has non-convex feasible region.\n3. Issue with steepest descent - As explained in Section 2.2, the objective function in the optimization problem (7) is obtained by averaging over strategies, the inequality constraint sets (7(a)) and (7(b)) respectively. This\nhas an effect on the steepest descent gradient directions at the constraint boundaries. The steepest descent direction has been found to be always opposing the constraint boundaries. As a result, a gradient method with the steepest descent direction as its search direction will get stuck when it hits a constraint boundary."
    }, {
      "heading" : "3.2 The Herskovits Algorithm",
      "text" : "We observed that in the optimization problem (7), steepest descent directions most often oppose the active constraint boundaries. Hence a steepest descent direction cannot be used as it would get stuck at one such boundary point which may not be an optimal point. Herskovits method offers two features which address this issue: (1) The search direction selected at each iteration, while being a strictly descent direction, makes use of the knowledge of the gradients of constraints as well as the gradient of the objective; and (2) the procedure is strictly feasible, i.e., at any iteration, the current best feasible point is not touching any constraint boundary.\nAssumptions: The assumptions required for the two-stage method are as follows:\n(i) The feasible region Ω has an interior Ωo and is equal to the closure of Ωo, i.e., Ω = Ω̄o.\n(ii) Each 〈v, π〉 ∈ Ωo, satisfies gi(v, π) < 0, i = 1, 2, . . . , n.\n(iii) There exists a real number a such that the level set Ωa = {〈v, π〉 ∈ Ω|f(v, π) ≤ a} is compact and has an interior.\n(iv) The function f is continuously differentiable and gj , j = 1, 2, . . . , n, is twice continuously differentiable in\nΩa.\n(v) At every 〈v, π〉 ∈ Ωa, the gradients of active constraints form an independent set of vectors.\nIt can be seen that assumptions (i)-(iv) are easily verified considering the functional forms of the objective and constraints of the optimization problem (7) and that state space, S and action space, A, are assumed to be finite. Assumption (v) is carried over as it is. We present the algorithm of Herskovits [1986] in two parts: First, we provide the two-stage feasible direction method in Algorithm 1 and then in Algorithm 2, we present the full algorithm.\nAlgorithm 1. Two-stage Feasible Direction Method Parameter: α ∈ (0, 1), ρ0 > 0 Parameter: wj(v0, π0) > 0, j = 1, 2, . . . , n, continuous functions Input: ∇f(v0, π0), ∇gj(v0, π0), j = 1, 2, . . . , n Output: S, a feasible direction\n1. Set ρ ← ρ0. 2. Compute γ0 ∈ ℜn, S0 ∈ ℜn by solving the linear system\nS0 = − [ ∇f(v0, π0) +\nn∑ j=1\nγ0j∇gj(v0, π0) ]\nST0 ∇gj(v0, π0) = −wj(v0, π0)γ0jgj(v0, π0), j = 1, 2, . . . , n\n  \n(9)\n3. Stop and output S ← 0 if S0 = 0. 4. Compute ρ1 = (1− α) n∑\ni=1\nγ0i\n, if n∑\ni=1\nγ0i > 0. Also, ρ ← ρ12 if ρ1 < ρ.\n5. Compute γ ∈ ℜn and S ∈ ℜn by solving the linear system\nS = − [ ∇f(v0, π0) +\nn∑ j=1\nγj∇gj(v0, π0) ]\nST∇gj(v0, π0) = − [ wj(v0, π0)γjgj(v0, π0) + ρ‖S0‖2 ] , j = 1, 2, . . . , n\n  \n(10)\nwhere ‖S0‖ is the Euclidean norm of the direction vector S0. 6. Output S.\nThis method computes a feasible direction in two stages. In the first stage (equation (9)), it computes a descent direction S0. By using its squared norm as a factor, a feasible direction S is computed in the second stage (equation (10)). Note that the second stage ensures that all the active constraints have ST∇gj(v0, π0) = −ρ‖S0‖2 where the right hand side is strictly negative. Thus, gradients of active constraints are maintained at obtuse angles with the direction S and hence, the vector S points away from the active constraint boundaries. Thus, feasibility of the direction S gets ensured. Let S = 〈 S1v , S 2 v , S 1 π, S 2 π 〉 be a descent search direction where Siv (resp. S i π) is the search direction in vi (resp. πi) for i = 1, 2. Also, let Sv = 〈 S1v , S 2 v 〉 and Sπ = 〈 S1π, S 2 π 〉 . We now present the original Herskovits algorithm as Algorithm 2.\nAlgorithm 2. The Herskovits Algorithm Parameter: ν > 1, δ0 ∈ (0, 1), η ∈ (0, 1) Input: 〈v0, π0〉: initial feasible point which is a strict interior point. Output: 〈v∗, π∗〉\n1. iteration ← 1. 2. 〈v̂, π̂〉 ← 〈v0, π0〉 begin loop\n3. Compute feasible direction S using the two-stage feasible direction method (algorithm 1).\n4. Stop algorithm if S = 0. Set 〈v∗, π∗〉 ← 〈v̂, π̂〉. Output 〈v∗, π∗〉. 5. Let γ = 〈γ1, γ2, . . . , γn〉, be the Lagrange multiplier vector computed in Algorithm 1. Define δj = δ0 if γj ≥ 0 and δj = 1 if γj < 0. 6. Find t, the first element in the sequence {1, 1/ν, 1/ν2, . . . } such that\nf(v + tSv, π + tSπ) ≤ f(v, π) + tηST∇f(v, π), and g(v + tSv, π + tSπ) ≤ δig(v, π), ∀i = 1, 2, . . . , n.\n(11)\n7. Stop algorithm if t = 0. Set 〈v∗, π∗〉 ← 〈v̂, π̂〉. Output 〈v∗, π∗〉. 8. 〈v̂, π̂〉 ← 〈v̂, π̂〉+ tS 9. iteration ← iteration + 1.\nend loop\nThis algorithm can be tuned by utilizing the knowledge about the structure of the optimization problem (7).\nWe present the following modifications in this direction:\n1. Computing the initial feasible point by a set of simple linear programs in Section 3.3;\n2. Exploiting the sparsity of the matrix involved in computing the two-stage feasible direction in Section 3.4;\nand\n3. Knowing the cubic form of the objective and the quadratic form of constraints, and computing an optimal\nstep-length in Section 3.5. However, we keep the condition 11 satisfied while selecting the step-length."
    }, {
      "heading" : "3.3 Initial Feasible Point",
      "text" : "The optimization problem given in (7) has a distinct separation between strategy probability terms and value vector terms. This can be exploited to find an initial feasible solution using the following procedure. First, a feasible strategy is selected, for instance, a uniform strategy, π0 = 〈 πi0 : i = 1, 2 〉 with\nπi0(x, a) = 1\nmi(x) ∀a ∈ Ai(x), x ∈ S, i = 1, 2. (12)\nIf this strategy is held constant, then it is easy to see that the main optimization problem (7) breaks down into two linear programming problems in v1 and v2, respectively, as given in (13). For the Herskovits algorithm, a strict interior point is desired to start with. That is, the initial point for the algorithm needs to be strictly away from all constraint boundaries. So, we introduce a small positive parameter, α > 0, in the left-hand side of the constraints given in (13).\nmin v1\n{ 1T|S| ( v1 − r1(π0)− βP (π0)v1 ) , }\ns.t. π20(x) T\n[ r1(x) + β\n∑ y∈U(x)\nP (y|x)v1(y) ] + α ≤ v1(x)1Tm1(x), ∀x ∈ S,\n \nmin v2\n{ 1T|S| ( v2 − r2(π0)− βP (π0)v2 )} ,\ns.t.\n[ r2(x) + β\n∑ y∈U(x)\nP (y|x)v2(y) ] π10(x) + α ≤ v2(x)1m2(x) ∀x ∈ S.\n    \n(13)\nThe two optimization problems in (13) can be solved readily with the popular method of revised simplex [Chvatal, 1983, Chapter 7]. Since the main purpose here is to just get an initial feasible point, the first phase of the revised simplex method in which the auxiliary problem in an artificial variable is solved for, is in itself sufficient. The relevant details related to the first phase of the revised simplex method have been described in [Chvatal, 1983, Chapter 7]. An initial feasible point (v0, π0) can thus be obtained."
    }, {
      "heading" : "3.4 Sparsity",
      "text" : "The two-stage feasible direction method given in Algorithm 1 requires inverting a matrix of dimension the same as the number of constraints. Note that the number of constraints in the optimization problem is large (See Section 3.1). Thus, in principle, our method would require a large memory and computational effort. However, we observe that the matrix to be inverted is sparse. Hence, we use efficient techniques for sparse matrices that result in a substantial reduction in the computational requirements. The matrix to be inverted is given by\nH =   ∇gT1 ∇g1 − w1g1 ∇gT1 ∇g2 . . . ∇gT1 ∇gn ∇gT2 ∇g1 ∇gT2 ∇g2 − w2g2 . . . ∇gT2 ∇gn ... ... . . . ...\n∇gTn∇g1 ∇gTn∇g2 . . . ∇gTn∇gn − wngn\n  . (14)\nThe elements of H can be seen mainly to be dot products of constraint gradients. In a typical stochastic game, the number of states which are related by non-zero transition probability, is less compared to the total number of states. The same is applicable when we consider the action set. The action set available at each state is usually less overlapping with corresponding (action) sets of other states. In some cases, these sets may be completely disjoint as well. For the simple terrain exploration problem which shall be discussed in Section 4, the above matrix for a 4× 4 grid scenario with two objects and two agents, is of size 16676× 16676, and is only about 4% full. We note that the two-stage feasible direction method does not really require an explicit inverse of the matrix. Rather, it requires the solution to the linear system of equations Hγ = b, where b is a vector of appropriate dimension. We target to use decomposition techniques for the purpose in which the matrix is decomposed as H = LDLT where L is a lower triangular matrix and D is a diagonal matrix. Since H is also sparse, we do sparse LDLT decomposition of the matrix H using techniques discussed by Davis et al. [2007], Davis [2007], using publicly available software on the internet. Upon decomposition of the matrix H , the solution to γ can be easily computed."
    }, {
      "heading" : "3.5 Computing the Optimal Step-length",
      "text" : "The objective function has been shown previously to be cubic and constraints quadratic in the optimization variables. This structure can be exploited to find the optimal step length, t∗, in any chosen direction."
    }, {
      "heading" : "3.5.1 Optimal Step Length, t∗",
      "text" : "Let (v0, π0) be the current point and (v, π) be the next point obtained from the previous by moving one step along the descent direction. Thus, v = v0+tSv and π = π0+tSπ. Upon substitution into the objective function f(v, π), one obtains,\nf(v, π) = f(v0 + tSv, π0 + tSπ) = d0 + d1t+ d2t 2 + d3t 3 (15)\nwhere\nd0 = 2 ∑\ni=1\n1T|S| { vi0 − r i(π0)− βP (π0)v i 0 } ,\nd1 = 2 ∑\ni=1\n1T|S|{S i v −\n( ri( 〈 π10 , S 2 π 〉 ) + ri( 〈 S1π, π 2 0 〉 ) )\n−β ( P (π0)S i π + P ( 〈 π10 , S 2 π 〉 )vi0 + P ( 〈 S1π, π 2 0 〉 )vi0 ) },\nd2 = 2 ∑\ni=1\n1T|S| { −ri(Sπ)− β ( P ( 〈 π10 , S 2 π 〉 )Siv + P ( 〈 S1π, π 2 0 〉 )Siv + P (Sπ)v i 0 )} ,\nd3 = 2 ∑\ni=1\n1T|S| { −βP (Sπ)S i v } .\n        \n        \n(16)\nIn the above equations, the search direction terms S1π and S 2 π have been used in places where strategy terms are\nexpected. Note that the search direction terms S1π and S 2 π do not form strategies. The usage here is purely in the functional sense.\nNow, from ∂f\n∂t = 0, one obtains d1 + 2d2t + 3d3t2 = 0. Hence the extreme points of f(v, π) are given\nby t∗ = −d2 ± √ d22 − 3d1d3 3d3\n. (i) If d22 − 3d1d3 < 0, then with increasing t, the function value shall decrease monotonically in the chosen direction S. So, any value t ≥ 0 is fine without considering the constraints (see Figure 1). (ii) If d22 − 3d1d3 ≥ 0, we get two extreme points in the chosen direction. If any of these points has a negative t value, it is ignored. Since the direction is known to be descent, if one extreme point is negative, then so will be the other extreme point as well (see Figure 1). Till now only the objective function was considered. The approach to handle the constraints will be explained next."
    }, {
      "heading" : "3.5.2 Constraints on step-length, t",
      "text" : "The constraints in the optimization problem (7) impose limits on the possible values that t can take. Consider the inequality constraint (7(a)) for a particular a1 ∈ A1(x). Let gj(·) ≤ 0 represent one of these constraints and let δi represent the corresponding parameter computed in step 6 of the Herskovits method (Algorithm 1). Apart from feasibility of step-size, we wish to ensure that the condition (11), i.e., gj(v, π) ≤ δjgj(v0, π0), is ensured as well. Now, substituting v = v0 + tSv and π = π0 + tSπ, and rearranging we get,\nb1(x, a1) + c1(x, a1)t+ d1(x, a1)t2 ≤ 0, (17)\nwhere b1(x, a1) = (1− δj)gj(v0, π0),\nc1(x, a1) = π20(x) T\n[\nβ ∑ y∈U(x) P (y|x, a1)S1v(y)\n]\n− S1v(x)\n+S2π(x) T\n[\nr1(x, a1) + β ∑\ny∈U(x)\nP (y|x, a1)v10(y)\n]\n,\nd1(x, a1) = S2π(x) T\n[\nβ ∑ y∈U(x) P (y|x, a1)S1v(y)\n]\n,\n        \n        \n(18)\nrespectively. Let D(x, a1) = c1(x, a1)c1(x, a1) − 4b1(x, a1)d1(x, a1). Consider the case where D < 0. This implies that the quadratic does not intersect the t-axis at any point i.e., it lies fully above the t-axis or fully below it. Clearly, d1(x, a1) < 0, implies that the quadratic lies fully below the t-axis and vice versa for d1(x, a1) > 0.\nProposition 3.6 If D(x, a1) < 0, then d1(x, a1) < 0.\nProof: Suppose this is not true. Then d1(x, a1) ≥ 0. Hence, b1(x, a1)d1(x, a1) ≤ 0 because by definition of δj and gj(v0, π0), we have that b1(x, a1) ≤ 0. Thus, we obtain c1(x, a1)c1(x, a1) − 4b1(x, a1)d1(x, a1) ≥ 0 which is a contradiction. Hence, d1(x, a1) < 0.\nThus, for the case when D(x, a1) < 0, any value of t ≥ 0 is fine as the quadratic is fully below the t-axis. Now consider the case where D(x, a1) ≥ 0. On solving the quadratic function, we get its two roots as\nt11(x, a 1) =\n−c1(x, a1) + √ D(x, a1)\n2d1(x, a1) and t12(x, a\n1) = −c1(x, a1)−\n√ D(x, a1)\n2d1(x, a1) , (19)\n∀a1 ∈ A1(x) ∀x ∈ S. If t11(x, a1) ≥ t12(x, a1), it can be shown that the region allowed by this constraint is given by the interval [t12(x, a 1), t11(x, a 1)] in the given direction S. Otherwise for t11(x, a 1) < t12(x, a 1), the region allowed by this constraint on the real line is given by the interval ( −∞, t11(x, a1) ] ⋃ [ t12(x, a 1),∞ ) . Note that this implies that the constraint is not convex. The above explanation can be easily adapted for constraints on the second agent as well. Thus, feasible value ranges for t imposed by the constraints (7(a)) and (7(b)) can be obtained. We formalize in Algorithm 3, this process of computing feasible value ranges for t imposed by quadratic constraints (17).\nAlgorithm 3. Feasible x from a Quadratic Constraint\nInput: b, c, d - Coefficients of quadratic constraint b+ cx+ dx2 ≤ 0. Output: L - Feasible x set\nif d = 0 then\nif c = 0 then\nif b > 0 then L = φ else L = ℜ\nend if else if c > 0 then\nL = [−b/c,∞) else\nL = (−∞,−b/c] end if\nelse\nD = c2 − 4bd if D < 0 then\nif d ≥ 0 then L = φ else L = ℜ\nend if else\nx1 = −c+\n√ D\n2d {Upper limit, x ≤ x1}\nx2 = −c−\n√ D\n2d {Lower limit, x ≥ x2}\nif x2 ≤ x1 then L = [x2, x1] else\nL = (−∞, x1] ⋃ [x2,∞)\nend if\nend if end if\nEquality constraints (7(c)) and (7(d)), on the values of π are 1Tmi(x)π i(x) = 1 ∀x ∈ S, i = 1, 2. On using\nπ = π0 + tSπ and 1Tmi(x)π i 0(x) = 1, ∀x ∈ S, we get,\nt× [ 1Tmi(x)S i π(x) ] = 0, ∀x ∈ S,\nwhich do not impose any condition on the value of t. However, the set of inequality constraints (7(e)) and (7(f)), for non-negativity of strategy vectors, i.e., πi(x, aj) ≥ 0, ∀aj ∈ Ai(x), x ∈ S, i = 1, 2 may impose an upper limit on the value of t. For example, consider πi(x, aj) ≥ 0. Let πi0(x, aj) represent the current best value of the probability of picking action aj in state x by agent i, and let δj represent the corresponding parameter computed in step 6 of the Herskovits algorithm, (see Algorithm 2). We wish to satisfy the condition (11), i.e., πi(x, aj) ≥ δjπi0(x, aj). Upon substituting πi(x, aj) = πi0(x, a j) + tSiπ(x, a j), we get, πi0(x, a j) + tSiπ(x, a j) ≥ δjπi0(x, aj). If Siπ(x, aj) < 0, then,\nt ≤ π i 0(x, a j)(1 − δj) −Siπ(x, aj)\nor t ∈ ( −∞, π i 0(x, a\nj)(1− δj) −Siπ(x, aj)\n] .\nNote that t ≥ 0 is implicitly assumed, else while S is a descent direction, tS would not be one. Thus, if Siπ(x, aj) ≥ 0, we get t ≥ π i 0(x, a\nj)(1 − δj) −Siπ(x, aj) which does not impose any additional constraint on t and hence can be ignored.\nIntersection of feasible regions given by all constraints in (7(a)), (7(b)), (7(e)) and (7(f)), gives the feasible set of values for t, from which a suitable step length t∗ ≥ 0 is selected. The procedure for doing so is explained next."
    }, {
      "heading" : "3.5.3 Selection of the optimal step length, t∗",
      "text" : "Along the chosen descent direction S, the objective function f(v, π) is a cubic function in the step length t. If the extreme points are real and both positive, then the first sub-point in the descent direction will be a minimum point and the next a maximum point as shown in Figure 1. So, under this condition, the best point is obtained by finding the best among the minimum point (or two feasible points near the minimum point) and the maximum step length point which is decided by the constraints. Otherwise, the cubic curve would be like the dashed curve in Figure 1. In such a case, the optimal step length is simply the maximum feasible step length."
    }, {
      "heading" : "3.5.4 Optimal Step Length Algorithm",
      "text" : "Algorithm 4. Step Length Calculation Parameter: β: discount factor Input: (v0, π0): current value strategy pair Input: S: selected descent direction Output: t: The best step length\n1. Calculate d1, d2 and d3 using (16). 2. (t1, t2) ← roots(d1, 2d2, 3d3) 3. F ← ℜ+, the set of all non-negative real numbers for x ∈ S, a1 ∈ A1(x), a2 ∈ A2(x), i = 1, 2 do\n4. F ← F ∩ quadraticfeasible(bi(x, ai), ci(x, ai), di(x, ai)) where bi(x, ai), ci(x, ai), di(x, ai) are from (18).\n5. F ← F ∩ [ 0, πi0(x, a i)\n−Siπ(x, ai)\n] if Siπ(x, a i) < 0.\nend for 6. If the extreme points t1 and t2 are real and both positive, the best step t is obtained by finding the best amongst the minimum point t1 (or two feasible points in F near the minimum point) and the maximum step in F . Otherwise, the best step t is the maximum step in F .\nIn the above, roots(a, b, c) gives the roots of a + bx + cx2 = 0. Algorithm 3 is being referred to as\nquadraticfeasible(a, b, c)."
    }, {
      "heading" : "3.6 The Complete Algorithm",
      "text" : "With the schemes discussed in Sections 3.3, 3.4 and 3.5, we present the modified Herskovits algorithm.\nAlgorithm 5. The Complete Algorithm Parameter: β: discount factor Input: π0: initial strategy (from (12)) Output: 〈v∗, π∗〉: An ǫ-Nash equilibrium with ǫ = f(v ∗, π∗)\n1− β 1. iteration ← 1. 2. π̂ ← π0. 3. Compute v̂ from linear programs in (13) using only the first phase of the revised simplex method (see Section 3.3). begin loop\n4. Compute feasible direction S using the two-stage feasible direction method (algorithm 1). 5. Stop algorithm if S = 0. 〈v∗, π∗〉 ← 〈v̂, π̂〉. Output 〈v∗, π∗〉 and ǫ = f(v ∗, π∗)\n1− β . Terminate the algorithm. 6. Compute the constrained optimal step length t by the procedure described in Section 3.5. 7. Stop algorithm if t = 0. 〈v∗, π∗〉 ← 〈v̂, π̂〉. Output 〈v∗, π∗〉 and ǫ = f(v ∗, π∗)\n1− β . Terminate the algorithm. 8. 〈v̂, π̂〉 ← 〈v̂, π̂〉+ tS\n9. iteration ← iteration + 1. end loop\nNote that in the above algorithm, equality of S to zero and also that of t to zero are to be considered with a small error bound around zero to handle numerical issues. The computational complexity per iteration of the algorithm is O(|A|3) multiplications contributed mainly from the steps involving formation and decomposition of the inner product matrix, G. However, the factor multiplying |A|3 can be shown to be far less than one in the actual implementation."
    }, {
      "heading" : "3.7 Convergence to a KKT point",
      "text" : "KKT conditions represent a set of necessary and sufficient conditions for a point to be a valid local minimum of an optimization problem. We write down the necessary conditions for a point 〈v∗, π∗〉 to be a local minimum of the optimization problem (7):\n(a)∇f(v∗, π∗) +∑nj=1 λj∇gj(v∗, π∗) = 0, (b)λjgj(v ∗, π∗) = 0, j = 1, 2, . . . , n, (c) gj(v ∗, π∗) ≥ 0, j = 1, 2, . . . , n, (d)λj ≥ 0, j = 1, 2, . . . , n,   \n(20)\nwhere λj , j = 1, 2, . . . , n, are the Lagrange multipliers associated with the constraints, gj(v, π) ≥ 0, j = 1, 2, . . . , n. Let I = {j|gj(v, π) = 0} be the set of active constraints. It can be easily shown that, the above set of conditions are sufficient as well if, the gradients of all active constraints form a linearly independent set.\nWe note here that the entire proof of convergence to KKT point presented in [Herskovits, 1986, Section 3] for the unmodified Herskovits algorithm, can easily be seen to be applicable as it is, to the modified Herskovits algorithm, i.e., Algorithm 5. In the next section, we apply this algorithm to a simple terrain exploration problem, modelled as a general-sum discounted stochastic game, and observe in the simulations that the convergence is also to a Nash equilibrium. However, later in Section 5, we show that in general, convergence to a KKT point is not sufficient to guarantee convergence to a Nash equilibrium."
    }, {
      "heading" : "4 A Simple Terrain Exploration Problem",
      "text" : "A simplified version of the general terrain exploration problem is presented below. Consider a pair of agents that are assigned the task of collecting a set of objects located at various positions in a terrain. We assume that the object positions are known aproiri. The game between the pair of agents terminates if all the objects are collected. The agent movements are considered to be stochastic. Modelling of this problem as a discounted stochastic game 〈S,A, p, r, β〉 is described as follows. (i) State Space, S - Let the entire terrain be discretized into a grid structure defined by SG = G × G where G = {0,±1,±2, . . . ,±M} . The position of an agent can be represented by a point in SG. Let the position of the ith agent be denoted by xi ∈ SG with xi(1), xi(2) ∈ G being its two co-ordinate components. So, the positional part of the overall state space considering the two agents, is given by Sp = SG×SG. The status regarding whether a particular object is collected or not is also a part of the state space. So, the overall state space would be given by S ′ = Sp × {0, 1}K , where K is the total number of objects to be collected from the terrain. Let oi represent the Boolean variable for the status of the ith object. Here oi = 0 implies that the ith object is not yet collected and the opposite is true for oi = 1. Thus, x = 〈 x1, x2, o1, o2, . . . , oK 〉 ∈ S ′ where xi ∈ SG, i = 1, 2. Let B = {y ∈ SG : an object is located at y}. The two sets S1 = { x ∈ S ′ : xi ∈ B and oxi = 0 for some i = 1, 2 }\nand S2 = {x ∈ S ′ : oj = 1 ∀j = 1 to K} , represent those combinations of states which are not feasible. Thus, the actual state space containing only feasible states is S = [S ′\\ (S1 ∪ S2)] ∪ T , where T represents the terminal state of the game. (ii) Action Space, A - The action space of the ith agent can be defined as\nAi(x) = { Go to y ∈ SG : d∞(xi, y) ≤ 1 } ,\nwhere xi ∈ SG is the position of the ith agent and d∞(xi, y) = max(|xi(1)−y(1)|, |xi(2)−y(2)|) is the L∞ distance metric. The aggregate action space of the two agents at state x ∈ S\\{T } is given by A(x) = A1(x) × A2(x). Note that x = 〈 x1, x2, o1, o2, . . . , oK 〉 . Thus, the action space does not depend upon the object state except for the termination state T . For the termination state T , the only action available is to stay in the termination state. The action related to the termination state T is ignored in subsequent discussions. (iii) Transition Probability, p(y|x, a) - The movements of each agent are assumed to be independent of other agents. The transition probability pi(yi|xi, ai) for the ith agent is given by pi(yi|xi, ai) = C(xi) 2−d1(ai,yi) ∀yi ∈ U i(xi) ⊆ SG, i = 1, 2, where C(xi) =\n∑ y∈Ui(xi) 2−d1(a i,y) is the normalization factor chosen to make this a\nprobability measure and d1(ai, y) = ( |ai(1) − y(1)|+ |ai(2) − y(2)| ) , the L1 norm distance between ai and y. The joint transition probability is given by p(y|x, a) = p1(y1|x1, a1)p2(y2|x2, a2). (iv) Reward function, r(x, a) - To ensure that the two agents do not get to the same position, a penalty may be imposed on the two agents when they attain the same position. Thus, the stochastic reward function for the ith agent can be defined accordingly as\nri(x, a, y) =    − 12 if yi = yj , j = 1, 2, j 6= i and Oy 6= 〈1, 1, . . . , 1〉 , 1 if object present at yi,\n0 otherwise,\n(21)\ni = 1, 2. The reward ri(x, a) is given by ri(x, a) = ∑ y∈S ri(x, a, y)p(y|x, a)."
    }, {
      "heading" : "4.1 Simulation Results",
      "text" : "Simulation results for G = {0, 1, 2, 3} with two objects situated at (0, 3) and (3, 3) and discount factor β = 0.75 are described below. The parameters given to the two-stage feasible direction method are wj(v0, π0) = 1, j = 1, 2, . . . , n, α = 0.5 and ρ0 = 0.9."
    }, {
      "heading" : "4.1.1 Objective Value",
      "text" : "The convergence of the objective value using Algorithm 5, to a value close to zero is shown in Figure 2. After getting an initial feasible solution, the objective value was ≈ 102.37."
    }, {
      "heading" : "4.1.2 Strategies",
      "text" : "The convergence behaviour of strategies of both agents with the initial position of the first agent being (2,1) and that of the second being (2,0), respectively, is shown in Figures 3 and 4 respectively. The arrows in the various grids in Figures 3 and 4 signify the feasible actions in each state and their lengths are proportional to the transition probabilities along the corresponding directions. With the given initial positions of agents and object locations, strategies pertaining only to those positions which an agent can visit with the other agent sticking to its own position\nare plotted. Consider for instance, Figure 3. The figure shows the strategy of the first agent with the second agent sticking to the position (2,0). At the start of the algorithm, all transition probabilities are chosen according to the uniform distribution. In Figures 3 and 4, we show the strategy profile of both the agents after the 1st, 11th and 100th iterations, and upon convergence of the algorithm. The algorithm converged in a total of 278 iterations.\nThe Nash strategies have an interesting structure here which is evident in Figures 3 and 4 as well. The strategies are deterministic except when both agents are in the vicinity of one another. This is expected from the structure of the reward function. Also, it is clear from Figures 3 and 4 that strategy components that are near to the two objects converge faster compared to those which are farther from the two objects. Note that strategy components of those positions which have no probability of being visited by an agent, with the agent being in a particular position, are not shown with arrow marks. For instance, in Figure 3(d), position (1,1) has no probability of being visited by the first agent located at (2,1)."
    }, {
      "heading" : "5 Non-Convergence to a Nash Equilibrium",
      "text" : "Theorem 2.4 showed that it is both necessary and sufficient for a feasible point 〈v∗, π∗〉 to correspond to a Nash equilibrium, if the objective value, f(v∗, π∗) = 0. However, for a gradient-based scheme, it would be apt to have conditions represented in terms of gradients of the objective and constraints. In this direction, we now present a series of results which ultimately give the desired set of necessary and sufficient conditions for a minimum point to be a global minimum. For a given point 〈v, π〉, let G = [∇gj(v, π) : j = 1, 2, . . . , N ] represent a matrix whose columns are gradients of all the constraints (8).\nProposition 5.7 At any given point 〈v, π〉, the gradient of the objective function f(v, π), can be expressed as a linear combination of the gradient of all the constraints (8). In other words, ∇f(v, π) = Gλ′ where λ′ is an appropriate vector.\nProof: Let h1(x, a1) = π2(x)T [ r 1(x, a1,A2(x)) + β ∑\ny∈U(x)\nP (y|x, a1,A2(x))v1(y) ] −v1(x). Then, h1(x, a1) ≤\n0 represents the set of constraints (7(a)). Similarly, let h2(x, a2) = [ r 2(x,A1(x), a2) + β ∑\ny∈U(x)\nP (y|x,A1(x), a2)v2(y) ] π1(x)−\nv2(x). Thus, h2(x, a2) ≤ 0 represents the set of constraints (7(b)). Now, we observe that the objective of the optimization problem (7) can be re-expressed in terms of hi(x, ai) and πi(x, ai), i = 1, 2, as follows:\nf(v, π) =\n2∑\ni=1\n∑\nx∈S\n−πi(x, ai)hi(x, ai). (22)\nSo, the objective can be visualized as the sum of products between LHS of (7(a)) and that of corresponding constraints in (7(e)). Note that the equality constraints are easily eliminated as expressed in (8). Thus, all the constraints of interest are inequality constraints which pair up, one from (7(a))-(7(b)) and the other from (7(e))(7(f)). It is now easy to see the desired result by considering the chain-rule of differentiation.\nNote that the vectorλ′ discussed in the proposition 5.7, is in value the same as the negative of the pair constraint. For instance, let for some j, gj(v, π) = hi(x, ai). Then, λ′j = −πi(x, ai). Similarly, for some j for which gj(v, π) = π i(x, ai), we have λ′j = −hi(x, ai).\nLet λ′ = [ λ′I λ′K ] , where λ′I is the part of λ ′ corresponding to active constraints and λ′K is that corresponding\nto inactive constraints. Similarly, let the set of Lagrange multipliers, λ =\n[ λI\nλK\n] , where λI is the part of λ\ncorresponding to active constraints and λK is that corresponding to inactive constraints.\nLemma 5.8 Under assumption (v) of Section 3.2, if λ′K = 0 at a KKT point 〈v∗, π∗〉, then λI = −λ′I .\nProof: Let G = [GI GK ] be the previously defined matrix of gradients of all constraints, where GI is the part of matrix G containing gradients of all active constraints and GK that containing gradients of all inactive\nconstraints. Now, from proposition 5.7, we have that ∇f(v∗, π∗) = Gλ′. From the KKT conditions (20), we have ∇f(v∗, π∗) = −Gλ. Combining the two, we get G(λ + λ′) = 0. Since G is full-rank from assumption (v), we have\nGTG(λ + λ′) = 0.\nThis can be re-written as [ GTI GI G T I GK\nGTKGI G T KGK\n][ λI + λ ′ I\nλK + λ ′ K\n] = 0.\nIn other words, we have a set of simultaneous equations as follows:\nGTI GI(λI + λ ′ I) +G T I GK(λK + λ ′ K) = 0, (23)\nGTKGI(λI + λ ′ I) +G T KGK(λK + λ ′ K) = 0. (24)\nNote that at a KKT point, it is easy to see that λK = 0. Also, we have λ′K = 0. So, from (23), we have,\nGTI GI(λI + λ ′ I) = 0.\nSince GI is of full rank, GTI GI is invertible. Hence, the result.\nCorollary 5.9 Under assumption (v) of Section 3.2, if λ′K = 0 at a KKT point 〈v∗, π∗〉, then λ = −λ′.\nProof: At a KKT point, λK = 0. Thus, the result follows.\nTheorem 5.10 A KKT point 〈v∗, π∗〉 corresponds to a Nash equilibrium of the underlying general-sum stochastic game, if and only if λ′K = 0.\nProof: We provide the proof in two parts below. If part: From corollary 5.9, we have λ = −λ′. Let us consider a pair of constraints, hi(x, ai) ≤ 0, and πi(x, ai) ≥ 0 for some x ∈ S, ai ∈ Ai(x), i = 1, 2. We consider the following cases in each of which we show that hi(x, ai)πi(x, ai) = 0 independent of the choice of x ∈ S, ai ∈ Ai(x), i = 1, 2. Thus from (22), it would follow that f(v∗, π∗) = 0. The result then follows from theorem 2.4.\n1. When hi(x, ai) < 0 and πi(x, ai) = 0 or hi(x, ai) = 0 and πi(x, ai) > 0 or hi(x, ai) = 0 and πi(x, ai) = 0\nthe result follows.\n2. The case when hi(x, ai) < 0 and πi(x, ai) < 0 does not occur. We prove this by contradiction. Suppose\nthis case holds. Since hi(x, ai) < 0, is an inactive constraint, by complementary slackness KKT condition (20(b)), we have that the corresponding λj = 0 = −λ′j = πi(x, ai). Thus, this case does not occur.\nOnly if part: If a KKT point 〈v∗, π∗〉 corresponds to a Nash equilibrium, then by theorem 2.4, we have that f(v∗, π∗) = 0. From equation (22), we have\n2∑\ni=1\n∑\nx∈S\n−πi(x, ai)hi(x, ai) = 0.\nSince a KKT point is always a feasible point of the optimization problem (7), every summand in this equation is non-negative. So, we have\nπi(x, ai)hi(x, a i) = 0, ∀x ∈ S, ai ∈ Ai(x), i = 1, 2. (25)\nNow from (25) and the complementary slackness KKT condition (20(b)), it is easy to see that λ′K = 0.\nDefinition 2 (KKT-N point) A KKT point 〈v∗, π∗〉 of the optimization problem (7) is defined to be a KKT-N (KKT-Nash) point, if the matrix G′ = GTK ( I −GI(GTI GI)−1GTI ) GK , computed at the KKT point, is full rank.\nLemma 5.11 Under assumption (v) of Section 3.2, a KKT-N point 〈v∗, π∗〉 of the optimization problem (7), corresponds to a Nash equilibrium of the underlying general-sum discounted stochastic game.\nProof: From assumption (v), we have that GI is full-rank and hence GTI GI is invertible. So, (23) can be simplified to get\n(λI + λ ′ I) = − ( GTI GI )−1 GTI GK(λK + λ ′ K).\nNote that λK = 0 by definition at a KKT point. The above can be substituted in (24) and simplified to get\nGTK ( I −GI(GTI GI)−1GTI ) GKλ ′ K = 0.\nSince at a KKT-N point, the matrix GTK ( I −GI(GTI GI)−1GTI ) GK is full-rank, we have λ′K = 0. Now we have the desired result from theorem 5.10.\nThus, we have a sufficient condition for a KKT point to correspond to a Nash equilibrium. Note that the matrix G′ that needs to be of full rank is dependent on (i) the reward function and state transition probabilities of the underlying stochastic game, (ii) the value function and strategy-pair at the current KKT point, and (iii) the set of active and inactive constraints. These dependencies are highly non-linear and difficult to separate. Using this sufficient condition, we can obtain a weak result on the convergence of gradient-based algorithms to Nash equilibrium solutions, as follows. Here by gradient-based algorithms, we mean those algorithms which assure convergence to a KKT point of a given optimization problem. For instance, the algorithm given in Section 3 is one such algorithm.\nTheorem 5.12 Under assumption (v) of Section 3.2, if every KKT point is also a KKT-N point, then any gradientbased algorithm when applied to the optimization problem (7) would converge to a point corresponding to a Nash equilibrium of the underlying general-sum discounted stochastic game.\nOn the contrary, if a general-sum discounted stochastic game is such that there is at least one KKT point which\nis not a KKT-N point, then convergence of plain gradient-based algorithms to Nash equilibrium is not assured."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We first proposed a simple gradient descent scheme for solution of general-sum stochastic games. During the construction of the scheme, we discussed the overall nature of the indefinite objective and non-convex constraints illustrating the fact that a simple steepest descent algorithm may not even converge to a local minimum of the optimization problem. The proposed scheme takes these issues while constructing both (i) feasible search direction as well as, (ii) optimal step-length. Also, it tries to address numerical efficiency by appropriately using sparsity techniques for an associated matrix inversion. We observed that the size of the optimization problem increases exponentially in the number of variables and the number of constraints. We showed that the proposed scheme converges to a KKT point of the optimization problem. This was seen to be sufficient in simulations performed for the example problem of terrain exploration. However, in general, we showed in Section 5 that it may not be sufficient for a scheme to converge to any KKT point as the same may not correspond to a Nash equilibrium. The\nresults discussed in Section 5 can be easily generalized to the case where there are more than two players. In summary, usual gradient schemes could possibly suffer from two issues: (i) Non-convergence to Nash equilibria which is the more serious of the two issues, and (ii) scalability to higher problem sizes. In would be interesting to derive gradient-based algorithms that provide guaranteed convergence to Nash equilibria."
    } ],
    "references" : [ {
      "title" : "Multi-agent reinforcement learning: Algorithm converging to Nash equilibrium in general-sum stochastic games",
      "author" : [ "N. Akchurina" ],
      "venue" : "In 8th International Conference on Autonomous Agents and Multi-agent Systems,",
      "citeRegEx" : "Akchurina.,? \\Q2009\\E",
      "shortCiteRegEx" : "Akchurina.",
      "year" : 2009
    }, {
      "title" : "Dynamic Programming and Optimal Control",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 1995
    }, {
      "title" : "Neuro-Dynamic Programming",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas and Tsitsiklis.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis.",
      "year" : 1996
    }, {
      "title" : "A user’s guide to solving dynamic stochastic games using the homotopy method",
      "author" : [ "R.N. Borkovsky", "U. Doraszelski", "Y. Kryukov" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Borkovsky et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Borkovsky et al\\.",
      "year" : 2010
    }, {
      "title" : "On the computation of equilibria in discounted stochastic dynamic games",
      "author" : [ "M. Breton", "J.A. Filar", "A. Haurie", "T.A. Schultz" ],
      "venue" : "Dynamic Games and Applications in Economics, Springer-Verlag Lecture Notes in Mathematical and Economic Systems,",
      "citeRegEx" : "Breton et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Breton et al\\.",
      "year" : 1986
    }, {
      "title" : "LDL, a Concise Sparse Cholesky Package",
      "author" : [ "T.A. Davis" ],
      "venue" : "URL http://www.cise.ufl. edu/research/sparse/ldl",
      "citeRegEx" : "Davis.,? \\Q2007\\E",
      "shortCiteRegEx" : "Davis.",
      "year" : 2007
    }, {
      "title" : "Competitive Markov Decision Processes. Springer-Verlag, New York, Inc., 1 edition, November",
      "author" : [ "J. Filar", "K. Vrieze" ],
      "venue" : null,
      "citeRegEx" : "Filar and Vrieze.,? \\Q2004\\E",
      "shortCiteRegEx" : "Filar and Vrieze.",
      "year" : 2004
    }, {
      "title" : "Stationary equilibria in stochastic games: Structure, selection, and computation",
      "author" : [ "P. Herings", "R.J.A.P. Peeters" ],
      "venue" : "Journal of Economic Theory,",
      "citeRegEx" : "Herings and Peeters.,? \\Q2004\\E",
      "shortCiteRegEx" : "Herings and Peeters.",
      "year" : 2004
    }, {
      "title" : "Homotopy methods to compute equilibria in game theory",
      "author" : [ "P.J. Herings", "R. Peeters" ],
      "venue" : "Research Memoranda 046, Maastricht : METEOR, Maastricht Research School of Economics of Technology and Organization,",
      "citeRegEx" : "Herings and Peeters.,? \\Q2006\\E",
      "shortCiteRegEx" : "Herings and Peeters.",
      "year" : 2006
    }, {
      "title" : "A two-stage feasible directions algorithm for nonlinear constrained optimization",
      "author" : [ "J. Herskovits" ],
      "venue" : "In Mathematical Programming",
      "citeRegEx" : "Herskovits.,? \\Q1986\\E",
      "shortCiteRegEx" : "Herskovits.",
      "year" : 1986
    }, {
      "title" : "Multiagent reinforcement learning: Theoretical framework and an algorithm",
      "author" : [ "J. Hu", "M.P. Wellman" ],
      "venue" : "In Proc. 15th International Conf. on Machine Learning,",
      "citeRegEx" : "Hu and Wellman.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hu and Wellman.",
      "year" : 1999
    }, {
      "title" : "Nash q-learning for general-sum stochastic games",
      "author" : [ "J. Hu", "M.P. Wellman" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "Hu and Wellman.,? \\Q2003\\E",
      "shortCiteRegEx" : "Hu and Wellman.",
      "year" : 2003
    }, {
      "title" : "Friend-or-Foe Q-Learning in General Sum Games",
      "author" : [ "M.L. Littman" ],
      "venue" : "In Proceedings of the 18th International Conference on Machine Learning,",
      "citeRegEx" : "Littman.,? \\Q2001\\E",
      "shortCiteRegEx" : "Littman.",
      "year" : 2001
    }, {
      "title" : "Solving stochastic games",
      "author" : [ "L. Mac Dermed", "C.L. Isbell" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dermed and Isbell.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dermed and Isbell.",
      "year" : 2009
    }, {
      "title" : "Equilibrium points in n-person games",
      "author" : [ "J.F. Nash" ],
      "venue" : "Proceedings of the national academy of sciences,",
      "citeRegEx" : "Nash.,? \\Q1950\\E",
      "shortCiteRegEx" : "Nash.",
      "year" : 1950
    }, {
      "title" : "Engineering Optimization: Theory and Practice",
      "author" : [ "S.S. Rao" ],
      "venue" : "New Age International (P) Ltd.,",
      "citeRegEx" : "Rao.,? \\Q1996\\E",
      "shortCiteRegEx" : "Rao.",
      "year" : 1996
    }, {
      "title" : "Stochastic games",
      "author" : [ "L.S. Shapley" ],
      "venue" : "In Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Shapley.,? \\Q1953\\E",
      "shortCiteRegEx" : "Shapley.",
      "year" : 1953
    }, {
      "title" : "Nash convergence of gradient dynamics in general-sum games",
      "author" : [ "S. Singh", "M. Kearns", "Y. Mansour" ],
      "venue" : "In Proc. of the 16th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Singh et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "A fairly general optimization problem formulation is available for general-sum stochastic games by Filar and Vrieze [2004]. However, the optimization problem there has a non-linear objective and non-linear constraints with special structure.",
      "startOffset" : 99,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "Since the seminal work of Shapley [1953], stochastic games have been an important class of models for multi-agent systems.",
      "startOffset" : 26,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc.",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc., can be modelled as stochastic games, see Filar and Vrieze [2004]. One of the significant results is that every stochastic game has a Nash equilibrium and it can be characterised in terms of global minima of a suitable mathematical programming problem.",
      "startOffset" : 88,
      "endOffset" : 251
    }, {
      "referenceID" : 5,
      "context" : "A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc., can be modelled as stochastic games, see Filar and Vrieze [2004]. One of the significant results is that every stochastic game has a Nash equilibrium and it can be characterised in terms of global minima of a suitable mathematical programming problem. As an application of general-sum games to the multi-agent scenario, Singh et al. [2000] observed that in a two-agent iterated general-sum game, Nash convergence is assured either in strategies or in the very least in average payoffs.",
      "startOffset" : 88,
      "endOffset" : 526
    }, {
      "referenceID" : 5,
      "context" : "A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc., can be modelled as stochastic games, see Filar and Vrieze [2004]. One of the significant results is that every stochastic game has a Nash equilibrium and it can be characterised in terms of global minima of a suitable mathematical programming problem. As an application of general-sum games to the multi-agent scenario, Singh et al. [2000] observed that in a two-agent iterated general-sum game, Nash convergence is assured either in strategies or in the very least in average payoffs. Later by Hu and Wellman [1999], stochastic game theory was observed to be a better framework for multi-agent scenarios as it could be viewed as an extension of the well studied Markov decision theory (see Bertsekas [1995]).",
      "startOffset" : 88,
      "endOffset" : 703
    }, {
      "referenceID" : 1,
      "context" : "Later by Hu and Wellman [1999], stochastic game theory was observed to be a better framework for multi-agent scenarios as it could be viewed as an extension of the well studied Markov decision theory (see Bertsekas [1995]).",
      "startOffset" : 205,
      "endOffset" : 222
    }, {
      "referenceID" : 2,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]).",
      "startOffset" : 113,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance.",
      "startOffset" : 113,
      "endOffset" : 330
    }, {
      "referenceID" : 0,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium.",
      "startOffset" : 113,
      "endOffset" : 500
    }, {
      "referenceID" : 0,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al.",
      "startOffset" : 113,
      "endOffset" : 984
    }, {
      "referenceID" : 0,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986]. A new type of approach based on homotopy is proposed by Herings and Peeters [2004].",
      "startOffset" : 113,
      "endOffset" : 1016
    }, {
      "referenceID" : 0,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986]. A new type of approach based on homotopy is proposed by Herings and Peeters [2004]. In this approach, a homotopic path between equilibrium points of N independent MDPs and the N -player stochastic game in question, is traced numerically giving a Nash equilibrium point of the stochastic game of interest.",
      "startOffset" : 113,
      "endOffset" : 1100
    }, {
      "referenceID" : 0,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986]. A new type of approach based on homotopy is proposed by Herings and Peeters [2004]. In this approach, a homotopic path between equilibrium points of N independent MDPs and the N -player stochastic game in question, is traced numerically giving a Nash equilibrium point of the stochastic game of interest. Their result applies to both normal form as well as extensive form games. However, this approach has a complexity similar to that of typical gradient descent schemes discussed in this paper. For more recent developments in this direction, see the work by Herings and Peeters [2006] and Borkovsky et al.",
      "startOffset" : 113,
      "endOffset" : 1604
    }, {
      "referenceID" : 0,
      "context" : "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986]. A new type of approach based on homotopy is proposed by Herings and Peeters [2004]. In this approach, a homotopic path between equilibrium points of N independent MDPs and the N -player stochastic game in question, is traced numerically giving a Nash equilibrium point of the stochastic game of interest. Their result applies to both normal form as well as extensive form games. However, this approach has a complexity similar to that of typical gradient descent schemes discussed in this paper. For more recent developments in this direction, see the work by Herings and Peeters [2006] and Borkovsky et al. [2010]. A recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed.",
      "startOffset" : 113,
      "endOffset" : 1632
    }, {
      "referenceID" : 0,
      "context" : "A recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "A recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed. Though their experiments do show convergence in a large group of randomly generated games, a formal proof of convergence has not been provided. For general-sum stochastic games, [Breton et al., 1986, Section 4.3] provides an interesting optimization problem with non-linear objectives and linear constraints whose global minima correspond to Nash equilibria of the underlying general-sum stochastic game. However, since the objective is not guaranteed to be convex, simple gradient descent techniques might not converge to a global minimum. Mac Dermed and Isbell [2009] formulate intermediate optimization problems, called Multi-Objective Linear Programs (MOLPs), to compute Nash equilibria as well as Pareto optimal solutions.",
      "startOffset" : 69,
      "endOffset" : 715
    }, {
      "referenceID" : 0,
      "context" : "A recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed. Though their experiments do show convergence in a large group of randomly generated games, a formal proof of convergence has not been provided. For general-sum stochastic games, [Breton et al., 1986, Section 4.3] provides an interesting optimization problem with non-linear objectives and linear constraints whose global minima correspond to Nash equilibria of the underlying general-sum stochastic game. However, since the objective is not guaranteed to be convex, simple gradient descent techniques might not converge to a global minimum. Mac Dermed and Isbell [2009] formulate intermediate optimization problems, called Multi-Objective Linear Programs (MOLPs), to compute Nash equilibria as well as Pareto optimal solutions. However, as mentioned in that paper, the complexity of their algorithm scales exponentially with the problem size. Thus, their algorithm is tractable only for small sized problems with a few tens of states. Another non-linear optimization problem for computing Nash equilibria in general-sum stochastic games has been given by Filar and Vrieze [2004]. We begin with this optimization problem by discussing it in Section 2.",
      "startOffset" : 69,
      "endOffset" : 1224
    }, {
      "referenceID" : 6,
      "context" : "Some important results by Filar and Vrieze [2004] that are applicable here are then described.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "Similar to MDPs [Bertsekas, 1995], one can define the value function as follows:",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "Like in normal-form games [Nash, 1950], pure strategy Nash equilibria may not exist in the case of stochastic games.",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "2, for a two-player general-sum discounted stochastic game has been given by Filar and Vrieze [2004]. The optimization problem is as follows:",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "The algorithm to be discussed is mainly based on an interior-point search algorithm by Herskovits [1986]. We first discuss the difficulties posed by the optimization problem (7).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "The algorithm to be discussed is mainly based on an interior-point search algorithm by Herskovits [1986]. We first discuss the difficulties posed by the optimization problem (7). We try to address these issues in the subsequent sections by presenting a suitable gradient-based algorithm. With a suitable initial feasible point, the iterative procedure of Herskovits [1986] converges to a constrained local minimum of a given optimization problem.",
      "startOffset" : 87,
      "endOffset" : 373
    }, {
      "referenceID" : 9,
      "context" : "The algorithm to be discussed is mainly based on an interior-point search algorithm by Herskovits [1986]. We first discuss the difficulties posed by the optimization problem (7). We try to address these issues in the subsequent sections by presenting a suitable gradient-based algorithm. With a suitable initial feasible point, the iterative procedure of Herskovits [1986] converges to a constrained local minimum of a given optimization problem. The unmodified algorithm of Herskovits [1986] is presented in Section 3.",
      "startOffset" : 87,
      "endOffset" : 493
    }, {
      "referenceID" : 9,
      "context" : "We present the algorithm of Herskovits [1986] in two parts: First, we provide the two-stage feasible direction method in Algorithm 1 and then in Algorithm 2, we present the full algorithm.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "Since H is also sparse, we do sparse LDL decomposition of the matrix H using techniques discussed by Davis et al. [2007], Davis [2007], using publicly available software on the internet.",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "Since H is also sparse, we do sparse LDL decomposition of the matrix H using techniques discussed by Davis et al. [2007], Davis [2007], using publicly available software on the internet.",
      "startOffset" : 101,
      "endOffset" : 135
    } ],
    "year" : 2015,
    "abstractText" : "Zero-sum stochastic games are easy to solve as they can be cast as simple Markov decision processes. This is however not the case with general-sum stochastic games. A fairly general optimization problem formulation is available for general-sum stochastic games by Filar and Vrieze [2004]. However, the optimization problem there has a non-linear objective and non-linear constraints with special structure. Since gradients of both the objective as well as constraints of this optimization problem are well defined, gradient based schemes seem to be a natural choice. We discuss a gradient scheme tuned for two-player stochastic games. We show in simulations that this scheme indeed converges to a Nash equilibrium, for a simple terrain exploration problem modelled as a general-sum stochastic game. However, it turns out that only global minima of the optimization problem correspond to Nash equilibria of the underlying general-sum stochastic game, while gradient schemes only guarantee convergence to local minima. We then provide important necessary conditions for gradient schemes to converge to Nash equilibria in general-sum stochastic games.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}