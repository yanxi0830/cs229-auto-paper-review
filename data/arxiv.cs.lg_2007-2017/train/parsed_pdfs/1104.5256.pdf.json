{
  "name" : "1104.5256.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Undirected Graphical Models with Structure Penalty",
    "authors" : [ "Shilin Ding" ],
    "emails" : [ "sding@stat.wisc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 4.\n52 56\nv1 [\ncs .A\nI] 2\n7 A"
    }, {
      "heading" : "1. Introduction",
      "text" : "In undirected graphical models (Markov Random Fields), a graph G is defined as G = (V,E), where V = {1, · · · ,K} is the set of nodes and E ⊂ V ×V is the set of links between the nodes. In fact, V is associated to a set of multivariate response variables Y1, · · · , YK , and E specifies the conditional independence structure among them. For example, a link between two nodes i, j indicates a pairwise interaction f ij. , and a clique between three nodes i, j, k indicates a third order interaction f ijk. These functions formulate the effects of predicative variables (features), X, on the responses and their interactions.\nGraphical models have been used in many applications. Conditional Random Fields (CRF) (Lafferty et al., 2001) and their extensions, e.g. dynamic CRF (Sutton et al., 2007), are well known in Natural Language Processing community. The CRFs achieve great success in sequentially structured text, by modeling the interaction of labels (Y ) on the nodes conditioned flexibly on the features (X). There are also numerous applications of graphical\nmodels to computer vision (Szeliski et al., 2007; Honorio and Samaras, 2010). Ising model is another classical example that draws great interest in image processing (Williams et al., 2004) and also has been recently applied to social networks (Banerjee et al., 2008).\nThe intuition of utilizing a graph structure is that some responses are related while others are not. However, in many cases, the graph is pre-determined by domain knowledge. For example, Duan et al. (2008) proposed a collective model for labeling music signals with fully connected graph, which they called collective conditional random fields. They have 10 semantic categories such as genre (blues, rap, . . . ), instrument (guitar, piano, . . . ), production (studio, live), rhythm(strong, weak, middle), and etc. It is possible that some links are not necessary: e.g. production and instrument. Estimating the parameters on these relations will lead to over-fitting. Therefore, graph structure learning is an important aspect of relation discovery in multivariate response applications and multi-task learning.\nMany papers have focused on the graphical model selection issue. Meinshausen and Buhlmann (2006) and Peng et al. (2009) studied sparse covariance estimation of Gaussian outcomes (Speed and Kiiveri, 1986) without input features. The covariance matrix determines the dependence structure in the Gaussian distribution and its sparsity specifies the linkage in Gaussian Markov Random Fields. This is not the case for non-elliptical distribution, such as the distribution of discrete random variables. Ravikumar et al. (2010) focused on graph structure selection of Ising model based on l1-regularized logistic regression. It gave sufficient conditions for consistently estimating the neighborhood of the nodes, without input features. However, two marginally independent response variables may become dependent after conditioning onX. So, ignoring the predicative variables may lead to inconsistent estimation of the graph structure. To the best of our knowledge, there is no previous work addressed the issue of learning the graph structure and the functions associated with the graph at the same time.\nIn this paper, our first contribution is the proof of the equivalence between the general graphical model with bivariate outcomes and multivariate Bernoulli (MVB) model. The functions that represent the effects of predicative variables on responses and their interactions (at all levels) can be formulated in MVB model, which is endowed with the advantage of interpreting the graph structure. It follows from the sparsity of links in the graphical models that some functions are constant zero, which means certain responses are conditionally independent. Therefore, we impose the structure penalty on groups of functions with overlaps to obtain sparse estimation of the graph structure. These groups are designed to enforce the sparsity on the functions and shrink higher order interactions so that they only appear after their lower components have entered the model.\nThe paper is organized as follows: Section 2 introduces graphical models and their relation with multivariate Bernoulli model. Section 3 and 4 discusses the model for learning the graph structure and the functions on the graph through structure penalty. The experiments are shown and discussed in Section 5. Section 6 gives the conclusion and future work."
    }, {
      "heading" : "2. Conditional Independence in Graphical Models",
      "text" : "The notations in this paper are summarized in Table 1.\nIn graphical models, G = (V,E), the distribution of multivariate discrete random variables Y1, . . . , YK is:\np(Y1 = y1, . . . , YK = yK |X) = 1\nZ(X)\n∏\nC∈C\nΦC(yC ;X) (1)\nwhere Z is the normalization factor. The distribution is factorized according to the cliques in the graph. A clique C ⊂ Ω = {1, . . . ,K} is the set of nodes of a fully connected subgraph. ΦC(yC ;X) is the potential function on C. It depends on yC = {yi | i ∈ C} and the predicative variables X which are shared across all response variables. One example of application is to model the relations of multiple clinical responses (hypertension, diabetes, etc.) and how they are affected by the person’s genetic variables and environmental variables (smoking, income, etc.).\nFor the purpose of efficient computation, C is usually the set of all maximal cliques of the graph. The maximal clique is a clique that is not properly contained in any other clique in the graph. Different representations with C as the set of non-maximal cliques can be converted to maximal clique representation by redefinition of the potential functions (Wainwright and Jordan, 2008). Furthermore, C does not have to reflect the graph structure, as long as it is sufficient. For example, the most general choice for any given\ngraphical model is C = {Ω}. The conditional independence between the response variables is implicitly formulated by the restrictions on the potential functions. See Theorem 2.2 and Example 2.1 for details.\nThe Markov property states that any two nodes not in a clique are conditionally independent given other nodes. For example, Ys, Yt are conditionally independent given all other variables that block the path from Ys to Yt. Therefore, C as the set of maximal cliques factorizes the graph and specifies the conditional independence in the model. In Figure 1(a), we have 2 cliques {1, 2, 3} and {3, 4}. In this case, {Y2, Y4} are conditionally independent given Y3, so are {Y1, Y4}.\nGiven the graph structure, the potential functions are convenient to characterize the distribution on the graph. However, if the graph is unknown in advance, estimating the potential functions does not give direct inference of the graph structure, because there are different representations with different choices of cliques in the same graph as mentioned above. Even if assuming the most general case where C = {Ω}, the conditional independence between the nodes cannot be represented by ΦC in a simple form (see Example 2.1), which\nmakes the learning of graph structure difficult. The multivariate Bernoulli (MVB) model can represent a graphical model whose nodes are Bernoulli random variables, i.e. Yi = 0 or 1. And the parameterization in MVB model is suitable for learning the graph structure. We will show later that it is equivalent to GM (1) with binary outcomes. The distribution of MVB model is:\np(Y1 = y1, . . . , YK = yk|X)\n= exp{y1f 1 + · · · + yKf K + · · ·+ y1y2f 1,2 + · · ·+ y1 . . . yKf 1,...,K − b(f)}\n= exp{ 2K−1 ∑\nω=1\nyωfω − b(f)} (2)\nHere, we use the following notations. Let ΨK be the power set of Ω = {1, . . . ,K}. Counting the empty set, there are 2K elements in ΨK . Where convenient in what follows, we will relabel these elements from 0 to 2K − 1, e.g. for K = 3, we will use f1,2,3 and f7 interchangeably without further specification. Because there are 2K − 1 free parameters in (2), denote ΨK = ΨK − {∅} for simple notation. Let ω denotes a set in ΨK , define Y = (y1, · · · , yω, · · · , yΩ) be the augmented response with\nyω = ∏\ni∈ω\nyi (3)\nAnd f = (f1, . . . , fω, . . . , fΩ) be the vector of natural parameters, where fω(x) is the conditional log odds ratio (Gao et al., 2001) to be estimated\nfω = logOR(Yi, i ∈ ω|Yj = 0, j /∈ ω;X) (4)\nHere, the odds ratios are calculated recursively\nOR(Yi) = P (Yi = 1)\n1− P (Yi = 1) , (5)\nOR(Yi, i ∈ ω ∪ {k}) = OR(Yi, i ∈ ω|Yk = 1)\nOR(Yi, i ∈ ω|Yk = 0) , suppose k /∈ ω (6)\nLater, we will call f1, · · · , fK main effects, and f1,2, · · · , f1,··· ,K measures the interactions between the response variables. We are interested in the sparse estimation of fω, which is in a Reproducing Kernel Hilbert Space (RKHS) Hω with kernel Kω (Wahba, 1990). Denote:\nSω(y;x) = ∑\nκ⊂ω\nyκfκ(x); Sω(x) = ∑\nκ⊂ω\nfκ(x); (7)\nThen the normalization factor is:\nexp(b(f(x))) = 1 + ∑\nω∈ΨK\nexp(Sω(x)) (8)\nAnd we have the following lemma:\nLemma 2.1 In multivariate Bernoulli model, define the odd-even partition of the power set of ω as: Ψωodd = {κ ⊂ ω | |κ| = |ω| − k,where k is odd}, and Ψ ω even{κ ⊂ ω | |κ| = |ω| − k, where k is even}. Note |Ψωodd| = |Ψ ω even| = 2\n|ω|−1, the natural parameters have the following property:\nfω = log\n∏\nκ∈Ψωeven p(Yi = 1, i ∈ κ, and Yj = 0, j ∈ Ω− κ|X)\n∏\nκ∈Ψω odd\np(Yi = 1, i ∈ κ, and Yj = 0, j ∈ Ω− κ|X) (9)\nand\nexp(Sω) = p(Yi = 1, i ∈ ω, and Yj = 0, j ∈ Ω− ω|X)\nP (Yi = 0, i ∈ Ω|X) (10)\nThe equivalence between graphical models and MVB model is given in the following theorem.\nTheorem 2.2 Graphical model of general form (1) with 0/1 nodes is equivalent to multivariate Bernoulli model (2). And the followings are equivalent:\n1. There is no |C|-order interaction in {Yi, i ∈ C}. 2. There is no clique C ∈ ΨK in the graph. 3. fω = 0 for all ω such that C ⊂ ω.\nA proof is given in Appendix A. Theorem 2.2 states that there is a clique C in the graphical model, if there is ω ⊃ C, fω 6= 0 in MVB model. The conditional independence specified in the graphical model can be fully formulated by MVB model.\nExample 2.1 For a graph with K nodes, the parameters in GM are {Φω | ω ∈ ΨK}, where Φω = ΦΩ(Yi = 1, i ∈ ω, and Yj = 0, j ∈ Ω−ω) is the potential function. We usually restrict Φ∅ = 1 to make the model identifiable. So there are 2\nK −1 free parameters. Similarly, there are also 2K − 1 free parameters in MVB model (f1, . . . , fΩ)\nWhen K = 2, Ω = {1, 2}, C = {Ω}, define Φ11 to be the potential function ΦΩ(Y1 = 1, Y2 = 1;X) for simplicity, and define Φ10,Φ01,Φ00 similarly. The probability distribution with the GM parameterization is\np(Y1 = 1, Y2 = 1|X) = 1\nZ Φ11, p(Y1 = 1, Y2 = 0|X) =\n1 Z Φ10,\np(Y1 = 0, Y2 = 1|X) = 1\nZ Φ01, p(Y1 = 0, Y2 = 0|X) =\n1 Z Φ00\nThe relation between GM and MVB model is\nf1 = log Φ10 Φ00 , f2 = log Φ01 Φ00 , f1,2 = log Φ11 · Φ00 Φ01 · Φ10\nNote, the independence between Y1 and Y2 implies\nf1,2 = 0 or log Φ11 · Φ00 Φ01 · Φ10 = 0 (11)\nTherefore, the sparseness in the conditional log odds ratios in MVB model gives a direct inference of the graph structure. But this property does not apply to GM."
    }, {
      "heading" : "3. Structure Penalty",
      "text" : "In many applications, the assumption of graphical models is that the graph structure has few large cliques. It is equivalent to the sparsity in higher order interactions in MVB model by Theorem 2.2. So we will impose a sparse penalty on the dependence structure to shrink higher order interactions.\nLet y(i) = (y1(i), . . . , yK(i)), x(i) = (x1(i), . . . , xp(i)) be the ith data point. The augmented representation of the multivariate responses is:\nY(i) = (y1(i), . . . , yω(i), . . . , yΩ(i)) (12)\nThere are |ΨK | = 2 K − 1 components in total1. Denote the number of components by q. In this paper, we consider the learning of the full model where q = |ΨK |. Suppose each function fω is in a Reproducing Kernel Hilbert Space (RKHS) Hω with kernel Kω (Wahba, 1990). The general penalized log likelihood model is:\nmin Iλ(f) = L(f) + λJ(f) (13)\nwhere the first term is the negative log-likelihood:\nL(f) =\nn ∑\ni=1\n( − Y(i)T f(x(i)) + b(f) )\n(14)\nand the second term J(·) is the structure penalty. The objective is to obtain sparse estimation of the cliques by structure penalty on f . Consider the pairwise links. No link between Ys, Yt in the graphical model means f ω = 0 for all ω ⊃ {s, t}. For example, in Figure 1(b), Y1, Y4 are conditionally independent means f1,4, f1,2,4,, f1,3,4, f1,2,3,4 are all zero. This objective is similar to sparse covariance matrix estimation in Gaussian data for neighborhood selection with lasso (Meinshausen and Buhlmann, 2006). However, our model will deal with higher order covariance structures that do not exist in Gaussian data. In addition, we not only consider the graph structure of responses Y alone, but also the functions of predicative variables X on Y .\nTo satisfy this intuition, the penalty is designed to shrink large cliques in the graph. Suppose in the true model, there is no interaction on clique C, then all fω should be zero, for C ⊂ ω. The penalty is designed to shrink such fω to zero. The idea can be viewed as group lasso with overlaps. Group lasso (Yuan and Lin, 2006) leads to selection of variables in groups. It has consistent estimation when the groups are exclusive and union to the whole set. Jacob et al. (2009) considered the penalty on groups with arbitrary overlaps. Zhao et al. (2009) set up the general framework for hierarchical variable selections with overlapping groups, which we adopt here for the functions.\nWe consider the penalty guided by the structure in Figure 2. The guiding graph T has 2K − 1 nodes: 1, . . . , ω, . . . ,Ω. With some abuse of notation, we use the element in Ψk to index the node in T . There is an edge from ω1 to ω2 if and only if ω1 ⊂ ω2 and |ω1|+1 = |ω2|. Domain knowledge can be applied here to design a different guiding structure. Jenatton et al. (2009) discussed how to define the groups to achieve different nonzero patterns.\n1. In applications with large graphs, we only consider up to m’th interactions. We truncate higher order\ninteractions and get ∑m\nk=1\n(\nK k\n)\nfunctions\nLet Tv = {ω ∈ ΨK |v ⊂ ω} be the subgraph rooted at v in T , including all the descendants of v. Denote fTv = (fω), ω ∈ Tv. All the functions are categorized into groups with overlaps as G = (T1, . . . , TΩ). The structure penalty on the group Tv of functions is:\nJ(fTv) = pv\n√\n∑\nω∈Tv\n‖fω‖2Hω (15)\nwhere pv is the weight for the penalty on Tv chosen empirically as 1\n|Tv| . Then the objective\nfunction is:\nmin f\nIλ(f) = L(f) + λ ∑\nv\npv\n√\n∑\nω∈Tv\n‖fω‖2Hω (16)\nThe following theorem shows that by solving the objective (16), fω1 will enter the model before fω2 if ω1 ⊂ ω2. That is to say, if f\nω1 does not exist, there will be no higher order interactions on ω2. The proof is given in Appendix A.\nTheorem 3.1 Objective (16) is convex, thus the minimal is attainable. Let ω1, ω2 ∈ ΨKand ω1 ⊂ ω2. If f ∗ is the minimizer, that is 0 ∈ ∂Iλ(f ∗) which is the subgradient of Iλ at f\n∗, then f∗ω2 = 0 almost sure if f∗ω1 = 0.\nExample 3.1 If K = 3, f = (f1, f2, f3, f1,2, f1,3, f2,3, f1,2,3). The grouped functions at node 1 in Figure 2 is fT1 = (f1, f1,2, f1,3, f1,2,3). The objective is:\nmin− l(y, f) + λ ( p1 √ ‖f1‖2 + ‖f1,2‖2 + ‖f1,3‖2 + ‖f1,2,3‖2\n+p2 √ ‖f2‖2 + ‖f1,2‖2 + ‖f2,3‖2 + ‖f1,2,3‖2 +p3 √ ‖f3‖2 + ‖f1,3‖2 + ‖f2,3‖2 + ‖f1,2,3‖2 (17) +p4 √ ‖f1,2‖2 + ‖f1,2,3‖2 + p5 √ ‖f1,3‖2 + ‖f1,2,3‖2 +p6 √ ‖f2,3‖2 + ‖f1,2,3‖2 + p7 √ ‖f1,2,3‖2 )\nAlgorithm 1 Proximal Linearization Algorithm\nInput: c0, α0, ζ > 1, tol > 0 repeat\nChoose αk ∈ [αmin, αmax] Solve Eq (19) for dk while δk = Iλ(ck)− Iλ(ck + dk) < ‖dk‖ 3 do\n// Insufficient decrease Set αk = max(αmin, ζαk) Solve Eq (19) for dk\nend while Set αk+1 = αk/ζ Set ck+1 = ck + dk\nuntil δk < tol"
    }, {
      "heading" : "4. Parameter Estimation",
      "text" : "In this paper, we focus on the situation where the ωth function space is Hω = {1}⊕Hω1 . {1} refers to the constant function space, and Hω1 is a RKHS with a linear kernel. The function fω ∈ Hω has the form: fω(x) = cω0 + ∑p j=1 c ω j xj . Its norm is ‖f ω‖2Hω = ‖c ω‖2. Here, we denote cω = (cω0 , . . . , c ω p ) T ∈ Rp+1 as a vector of length p+1 and c = (cω)ω∈ΨK ∈ R p̃ be the concatenated vector of all parameters of length p̃ = (p+ 1) · q. Hence, the objective (16) is now:\nmin c\nIλ(c) = L(c) + λ ∑\nv\npv‖c Tv‖ (18)\nwhere cTv = (cω)ω∈Tv is a (p + 1) · |T v| vector.\nTo solve (18), we iteratively solve the following proximal linearization problem (Wright, 2010):\nmin c\nLk +∇L T k (c− ck) + αk 2 ‖c− ck‖ 2 + λJ(c) (19)\nwhere Lk = L(ck), αk is a positive scalar chosen adaptively. With slight abuse of notation, we use ck to denote the vector of all parameters at kth step. Algorithm 1 summarized the framework of solving (18). Following the analysis in Wright (2010), we can show that the proximal linearization algorithm will converge for negative log-likelihood loss function plus group lasso type penalties with overlaps.\nHowever, solving group lasso with overlaps is not trivial due to the non-smoothness at the singular point. In recent years, several papers have addressed this problem. Jacob et al. (2009) duplicated the variables in the design matrix that appear in group overlaps, then solved the problem as group lasso without overlaps. Kim and Xing (2010) reparameterized the group norm with additional dummy variables. They alternatively optimized for the model parameters and the dummy variables at each iteration. The method performs efficiently on quadratic loss function for Gaussian data. But optimizing alternatively over two sets of parameters might not scale well on penalized logistic regression.\nIn this paper, we solve (19) by its smooth and convex dual problem proposed by Liu and Ye (2010). Let Z = {v ∈ ΨK |‖c\nTv‖ = 0}, and Z̄ = ΨK −Z be the complement. Define sv, v ∈ ΨK as:\nsv ∈ Sv = {s = (s ω)ω∈ΨK | s ∈ R p̃, ‖s‖ ≤ λpv, s ω = 0 if ω ∈ Tv} (20)\nthen the subgradient of (19) is:\n∇L+ αk(c− ck) + ∑\nv∈Z\nsv + ∑\nu∈Z̄\nru (21)\nwhere sv is the subgradient of λpv‖c Tv‖ for v ∈ Z and ru is the subgradient for u ∈ Z̄:\nru = argmaxsu〈su, c〉, for u ∈ Z̄ (22)\nThe subgradient sv is in a unit ball of certain subspace of R p̃. These subspaces are not perpendicular to each other. Thus, sv’s are not separable, and closed form solution of (19) cannot be obtained. We solve the proximal subproblem (19) by its smoothing and convex dual problem. Note (19) is equivalent to:\nmin c∈Rp̃ max S∈S φ(c, S) = (23)\n∇LTk (c− ck) + αk 2 ‖c− ck‖ 2 + ∑\nv∈Ω\n〈sv, c〉\nwhere S is a p̃ × q matrix whose columns are sv. S = {S|S = (s1, . . . , sv, . . . , sΩ), sv ∈ Sv for v ∈ ΨK} is the feasible region of S. Since φ(·, S) is lower semicontinuous and φ(c, ·) is upper semicontinuous, there exists a saddle point and the max and min are exchangeable. The solution of minimizing φ(c, S) is:\nc̃ = argmincφ(c, S) = ck − 1\nαk ∇Lk −\n1\nαk\n∑\nv\nsv (24)\nSubstitute c̃ back into (23), we have the dual problem of (19) as:\nmax S∈S\nη(S) = − 1 2 ‖ ∑\nv\nsv‖ 2 + (αkck −∇Lk)\nT ∑\nv\nsv (25)\nFollowing the proof in Liu and Ye (2010), we can show that η(S) is convex and Lipschitz continuous. The differential is αkc̃e\nT where e ∈ Rp̃ is a vector of ones. Hence, (25) can be solved by existing gradient methods. We use the accelerated gradient descent method implemented in (Liu et al., 2009)."
    }, {
      "heading" : "5. Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Simulation Study",
      "text" : "The simulations are performed to evaluate the learning accuracy of our method. The graphs of the response variables are depicted in Figure 1. In the simulation, we assume the most\ngeneral distribution family for each graphical model according to its graph structure. For example, in Model 1, we have 4 response variables, (Y1, Y2, Y3, Y4). There is a triangular clique (Y1, Y2, Y3), but Y4 is independent with the other response variables. In this case, we have a 15 conditional log odds ratios to estimate. In the true model, the non-zero functions are {f1, f2, f3, f1,2, f1,3, f2,3, f1,2,3, f4}. In Model 3, there are 255 functions and 25 of them are nonzero. In Model 4, there are 1023 functions and 25 of them are nonzero.\nThe predictive variables X = (X1, . . . ,X5) are independently generated from multivariate Gaussian distribution with mean 0 and variance 1. Each fω has 6 parameters to estimate (1 of them is the intercept). These parameters, cωj , j = 1, · · · , 5, are uniformly sampled from {−5,−4, · · · , 5}. We set the intercepts cω0 in main effects to 1, those in second or higher order interactions to 2. Each Y is randomly selected proportional to the probability in equation (2), where f = Xc. We generate 100 datasets for each graph structure in Figure 1 to evaluate the learning accuracy. The sample size in each dataset is 1000.\nThe tuning parameter λ is chosen by two different tuning methods: 1) GACV (generalized approximation cross validation), 2) BGACV (B-type GACV). The details of the tuning methods are discussed in Appendix B.\nIn Table 2, we count, for each function fω, the number of runs out of the 100 replications in which fω is recovered (‖cω‖ 6= 0). The recovered functions in the true model are considered as true positive; while the others not in the true model are false positive. Since the main effects are always detected correctly, they are not listed in the table. The structure penalty is efficient in recognizing strong interactions in the responses, such as the interaction between Y1, Y2. But its performance on higher order interactions will be affected in more complex graph structures, e.g. f1,2,3 in Model 3 and 4.\nCompared to GACV, BGACV tends to achieve more sparse results in general, because they have large penalties on the degrees of freedom of the estimated model. On the contrary, GACV will discover more true positive functions with the cost of higher false positive rate.\nIn Figure 3, we show the learning results in terms of true positive rate (TPR) with increasing sample size from 100 to 1000. The experimental setting is the same as before.\nThe TPRs are improving along with the increasing sample size. Compared to Model 1 and 2, the algorithm needs substantially larger sample size to achieve high TPR on Model 3 and 4. GACV achieves better true positive rate in all four graphical models. This tuning method will obtain less sparse models compared to BGACV."
    }, {
      "heading" : "5.2 Census Bureau County Data",
      "text" : "We use the county data from U.S. Census Bureau2 to validate our method. It includes demographic data for all counties in United States, covering population, employment, votes (Scammon et al., 2005), and etc. We delete the counties which have missing value in the columns we are interested, and get 2668 data entries in total. The outcomes for this\n2. http://www.census.gov/statab/www/ccdb.html\nstudy are summarized in Table 3. “Vote” is coded as 1 if the Republican candidate won in the 2004 presidential election. To dichotomize the rest response variables, the national mean is selected as a threshold. The third column in the table gives the percentage of positive in the data. The features in the model are: percentage of housing unit change; government expenditure; population percentage of 3 ethnic groups (White, African, Asian), people foreign born, people over 65, people under 18, people with high school education, and people with bachelor degree; birth rate; death rate.\nIn the experiment, We first standardize the data to be mean 0 variance 1. Then, we can get an estimated graphical model with every fixed λ. Adjusting the regularization parameter λ from 0.3289 to 0.1389, we will discover new interactions entering the model. The graph structure of λ = 0.1559 (chosen by cross validation) is shown in Figure 4. The first number on edge indicates the order of the link entering the model, while the second one is the corresponding λ. The unemployment rate plays an important role as a hub in the graph. It is strongly related to poverty and crimes. Population change is negatively related to violent crimes, as well as to unemployment rate.\nWe analyze the link between “Vote” and “PChange”, which is recovered by our method before any other links. The marginal correlation between them (without conditioning on X) is 0.0389, which is the second smallest absolute value in the correlation matrix. The partial correlation method (Peng et al., 2009) is taken as an example to show how the links of response variables are discovered without considering X. The link between “Vote” and “PChange” is the tenth recovered link using R package space. The reason is that after taking features into account, the dependence structure of response variables may change. The main contribution in this case is “percentage of housing unit change” (X1) and “population percentage of people over 65” (X2). Part of the fitted model is shown below:\nfV ote = 0.0463 ·X1 + 0.0877 ·X2 + · · · fPChange = 0.2315 ·X1 − 0.0942 ·X2 + · · · fV ote,PChange = 0.0211 ·X1 − 0.0115 ·X2 + · · ·\nThe main effects suggest that with increase of housing units, the counties tend to increase in population and vote for Republican. With increase of people over 65, the counties tend to lose population, but still more likely to vote for Republican. The interaction function reveals that as housing units increase, the counties are more likely to have both positive results for “Vote” and “PChange”. But this tendency will be counteracted by the increase of people over 65: the responses are less likely to take both positive values."
    }, {
      "heading" : "6. Conclusions and Future Work",
      "text" : "The structure penalty on the multivariate Bernoulli model can efficiently learn the graph structure, which indicates the conditional independence of the response variables. In this paper, we only consider linear models for the conditional log odds ratios. It can be extended to smoothing spline ANOVA model with more freedom by choosing the kernels. And Theorem 3.1 holds natually. It is also interesting to see how the penalty will improve the prediction power compared to large margin methods.\nAnother extension will be the selection of features for each fω. The sparsity in each function requires the sparsity within each group. But the graph structure would change with different selected features. One application is to discover the relations of multiple symptoms or clinical responses and how they are effected by the environmental and genetical covariates. Smoking could be significant for many diseases and their interactions, but other covariates, such as taking Vitamin might be only related to a subset of the symptoms. As a result, we will investigate sparse penalties within each function."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Research is supported in part by NIH Grant EY09946, NSF Grant DMS-0906818 and ONR Grant N0014-09-1-0655."
    }, {
      "heading" : "Appendix A. Proof",
      "text" : "A.1 Proof of Theorem 2.2\nProof Given graphical model (1), let yωC be a realization of yC such that y ω C = {y ω i | i ∈ C} where yωi = 1 if i ∈ ω and y ω i = 0 otherwise. Let the odd-even partition of the power set of ω defined as in Lemma 2.1. The conditional log odds ratios in MVB model are:\nfω(x) = log\n∏\nκ∈Ψωeven\n∏\nC∈C ΦC(y κ C ;x)\n∏\nκ∈Ψω odd\n∏\nC∈C ΦC(y κ C ;x)\n(26)\nb(f) = log Z(x) ∏\nC∈C ΦC(0;x)\nConversely, given the MVB model of 2, the cliques can be determined by the nonzero fω: clique C exists if C = ω and fω 6= 0. Then the maximal cliques can be inferred from the graph structure. And suppose they are C1, . . . , Cm. Let ωi be ψ(ωi) = Ci, i = 1, . . . ,m, and κ1 = ∅, κi be ψ(κi) = Ci∩(Ci−1∪· · ·∪C1), i = 2, . . . ,m. Then one possible parameterization is:\nΦCi(yCi ;x) = exp ( Sωi(y;x)− Sκi(y;x) )\n(27)\nZ(x) = exp(b(f))\nTherefore, graphical model (1) with bivariate outcomes is equivalent to the MVB Model (2).\nIn the latter part of the theorem, 1 ⇒ 2 and 3 ⇒ 1 follows naturally from the Markov property of graphical models. To show 2 ⇒ 3, notice that whenever κ∩C = κ′∩C, yκC = y κ′\nC . For any possible v = κ ∩ C, κ′ ∈ {κ|κ = v ∪ u, s.t. u ⊂ ω − v} will give κ′ ∩ C = v. There are 2|ω−v| such κ′ in total due to the choice of u. Also, they appear in the nominator and denominator of equation (26) equally. So, for any C ∈ C,\n∏\nκ∈Ψωeven\nΦC(y κ C ;x) =\n∏\nκ∈Ψω odd\nΦC(y κ C ;x) (28)\nIt follows that fω = 0 by (26).\nA.2 Proof of Theorem 3.1\nProof We give the proof for the linear case. The convexity of Iλ is easy to check, since L and J(fTv) are all convex in c. Suppose there is some ω2 ⊃ ω1 s.t. c\n∗ω2 6= 0, by the groups constructed through Figure 2, ‖c∗Tv‖ 6= 0 for all v ⊂ ω1. So the partial derivative of objective (18) with respect to cω1 is\n∂L\n∂cω1 + λ\n∑\nv⊂ω1\npv c∗ω1\n‖c∗Tv‖ = 0 (29)\nHence, the probability of {∃ω2 ⊃ ω1 s.t c ∗ω2 6= 0} equals to the probability that ∂L\n∂cω1 = 0,\nwhich is 0."
    }, {
      "heading" : "Appendix B. Tuning",
      "text" : "For i-th data, we have:\nSωi = S ω(x(i)) (30)\nbi = b(f(x(i))) = log (1 + ∑\nω\nexpSωi ) (31)\nThen, the mean of the augmented response Y(i) in the multivariate Bernoulli model is:\nµ(i) = E[Y(i)|x(i), f ] = (µ1(i), · · · , µκ(i), · · · , µΩ(i)) (32)\nwhere µκ(i) = ∂bi ∂fκ =\n∑\nω∈Tκ expSωi\nexp bi (33)\nThe q × q covariance matrix of the augmented response is:\nW (i) = var(Y(i)|x(i), f) (34)\nwhere the (α, β)-th element of W (i) is:\nWα,β(i) = ∂2bi\n∂fα(∂fβ)T =\n∑\nω∈Tα∩Tβ expSωi\nexp bi − µα(i) · µβ(i) (35)\nLet Rv be a p̃ × p̃ diagonal matrix whose (i, i)-th element is 1 if ci 6= 0. Then, the v-th group penalty in (18) can be written as:\nJ(fTv) = pv\n√\n∑\nω∈Tv\n‖fω‖2 = pv‖Rvc‖ (36)\nNote Rv is symmetric and Rv ·Rv = Rv, direct calculation yields the derivative and Hessian of the penalty term:\n∂J ∂c = ∑\nv:Rvc 6=0\npv Rvc\n‖Rvc‖ (37)\n∂2J\n∂c∂cT =\n∑\nv:Rvc 6=0\npvJv = ∑\nv:Rvc 6=0\npv Rv\n(\n‖Rvc‖ 2I − c · cT\n)\nRv\n‖Rvc‖3 (38)\nwhere Jv . = (Rv(‖Rvc‖ 2I − c · cT )Rv)/‖Rvc‖ 3. Denote the grand design matrix as:\nD = ( D(1)T · · · D(n)T )T\n(39)\nwhere D(i) =\n\n    x(i)T 0 · · · 0 0 x(i)T · · · 0 ... ... . . .\n... 0 0 · · · x(i)T\n\n   \n(40)\nSuppose there are N non-zero elements of c at location {a1, . . . , aN}. Let D̃ be the matrix composed by the a1, . . . , aN th column of D. Then, the Hessian matrix of Iλ in (16) is:\n∂2Iλ ∂c∂cT = ∂2L ∂c∂cT + λ ∂2J ∂c∂cT = D̃TWD̃ + λ ∑\nv:Rvc 6=0\npvJv (41)\n(42)\nLet H be the nq × nq influence matrix that implies\nfλ,ǫ − fλ ≈ Hǫ (43)\nwhere ǫ is a small perturbation on Y; fλ = Dcλ is the estimated function value with tuning parameter λ; and fλ,ǫ is the estimated function value with the perturbation. Then, the analysis of the first order Taylor expansion of ∂Iλ ∂c\n(Y + ǫ, cλ,ǫ) leads to the formulation of H as follows (refer to (Xiang and Wahba, 1996) and (Ma, 2010) Chapter 3 for more details)\nH = D̃ ( ∂2Iλ ∂c∂cT )−1 D̃T = D̃ ( D̃TWD̃ + λ ∑\nv:Rvc 6=0\npvJv\n)−1 D̃T (44)\nThe (i, j)-th q × q submatrix of H is\nH(i, j) = D̃(i)T ( ∂2Iλ ∂c∂cT )−1 D̃(j) (45)\nLet Q(i) = I −H(i, i)W (i) for i = 1, . . . , n, define the generalized average matrix, denoted as Q̄, of {Q(i), i = 1, . . . , n} as follows\nQ̄ = (δ − γ)Iq×q + γ · ee T =\n\n    δ γ · · · γ γ δ · · · γ ... ... . . .\n... γ γ · · · δ\n\n   \n(46)\nwhere e is the unit vector of length q and\nδ = 1\nnq ∑n i=1 tr(Q(i)) , γ =\n1\nnq(q − 1)\n[ eTQ(i)e− tr(Q(i)) ]\n(47)\nLet H̄ be the generalized average of {H(i, i), i = 1, · · · , n}, the GACV score is\nGACV (λ) = OBS(λ) + 1\nn\nn ∑\ni=1\nY(i)T Q̄−1H̄ ( Y(i) − µ(i) )\n(48)\nwhere\nOBS(λ) = 1\nn\n[ − Y(i)T fλ(x(i)) + b(fλ(x(i))) ]\n(49)\nis the observed log-likelihood.\nThe degrees of freedom of multivariate Bernoulli data is generally difficult to obtain. But we can have a good approximation from GACV (Shi et al., 2008) as\nd̂f(λ) =\nn ∑\ni=1\nY(i)T Q̄−1H̄ ( Y(i) − µ(i) )\n(50)\nSo the BGACV score can be defined as\nBGACV (λ) = OBS(λ) + 1\nn\nlog n\n2\nn ∑\ni=1\nY(i)T Q̄−1H̄ ( Y(i)− µ(i) )\n(51)"
    } ],
    "references" : [ {
      "title" : "d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data",
      "author" : [ "O. Banerjee", "L. El Ghaoui" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Banerjee et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2008
    }, {
      "title" : "Collective annotation of music from multiple semantic sategories",
      "author" : [ "Z. Duan", "L. Lu", "C. Zhang" ],
      "venue" : "In Proceedings of 9th International Conference on Music Information Retrieval,",
      "citeRegEx" : "Duan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2008
    }, {
      "title" : "Smoothing Spline ANOVA for multivariate Bernoulli observations, with application to ophthalmology data",
      "author" : [ "F. Gao", "G. Wahba", "R. Klein", "B. Klein" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Gao et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2001
    }, {
      "title" : "Multi-Task Learning of Gaussian Graphical Models",
      "author" : [ "J. Honorio", "D. Samaras" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine learning,",
      "citeRegEx" : "Honorio and Samaras.,? \\Q2010\\E",
      "shortCiteRegEx" : "Honorio and Samaras.",
      "year" : 2010
    }, {
      "title" : "Group Lasso with overlap and graph Lasso",
      "author" : [ "L. Jacob", "G. Obozinski", "J.P. Vert" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Jacob et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jacob et al\\.",
      "year" : 2009
    }, {
      "title" : "Structured variable selection with sparsityinducing norms",
      "author" : [ "R. Jenatton", "J.Y. Audibert", "F. Bach" ],
      "venue" : null,
      "citeRegEx" : "Jenatton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jenatton et al\\.",
      "year" : 2009
    }, {
      "title" : "Tree-guided group lasso for multi-task regression with structured sparsity",
      "author" : [ "S. Kim", "E.P. Xing" ],
      "venue" : "In Proceedings of 27th International Conference on Machine Learning,",
      "citeRegEx" : "Kim and Xing.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kim and Xing.",
      "year" : 2010
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J. Lafferty", "A. McCallum", "F. Pereira" ],
      "venue" : "In Proceedings of the 18th International Conference on Machine Learning,",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Fast overlapping group",
      "author" : [ "J. Liu", "J. Ye" ],
      "venue" : "lasso. arXiv:1009.0306v1,",
      "citeRegEx" : "Liu and Ye.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu and Ye.",
      "year" : 2010
    }, {
      "title" : "SLEP: Sparse Learning with Efficient Projections",
      "author" : [ "J. Liu", "S. Ji", "J. Ye" ],
      "venue" : "Arizona State University,",
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "High-dimensional graphs and variable selection with the lasso",
      "author" : [ "N. Meinshausen", "P. Buhlmann" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Meinshausen and Buhlmann.,? \\Q2006\\E",
      "shortCiteRegEx" : "Meinshausen and Buhlmann.",
      "year" : 2006
    }, {
      "title" : "Partial correlation estimation by joint sparse regression models",
      "author" : [ "J. Peng", "P. Wang", "N. Zhou", "J. Zhu" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Peng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2009
    }, {
      "title" : "High-dimensional Ising model selection using l1-regularized logistic regression",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "J. Lafferty" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2010
    }, {
      "title" : "LASSO-Patternsearch algorithm with application to ophthalmology and genomic data",
      "author" : [ "W. Shi", "G. Wahba", "S. Wright", "K. Lee", "R. Klein", "B. Klein" ],
      "venue" : "Statistics and its Interface,",
      "citeRegEx" : "Shi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2008
    }, {
      "title" : "Gaussian Markov distributions over finite graphs",
      "author" : [ "TP Speed", "HT Kiiveri" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Speed and Kiiveri.,? \\Q1986\\E",
      "shortCiteRegEx" : "Speed and Kiiveri.",
      "year" : 1986
    }, {
      "title" : "Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data",
      "author" : [ "C. Sutton", "A. McCallum", "K. Rohanimanesh" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2007
    }, {
      "title" : "A comparative study of energy minimization methods for markov random fields with smoothness-based priors",
      "author" : [ "R. Szeliski", "R. Zabih", "D. Scharstein", "O. Veksler", "V. Kolmogorov", "A. Agarwala", "M. Tappen", "C. Rother" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Szeliski et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Szeliski et al\\.",
      "year" : 2007
    }, {
      "title" : "Spline Models for Observational Data",
      "author" : [ "G. Wahba" ],
      "venue" : "Society for Industrial Mathematics,",
      "citeRegEx" : "Wahba.,? \\Q1990\\E",
      "shortCiteRegEx" : "Wahba.",
      "year" : 1990
    }, {
      "title" : "The Variational Ising Classifier (VIC) algorithm for coherently contaminated data",
      "author" : [ "O. Williams", "A. Blake", "R. Cipolla" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Williams et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2004
    }, {
      "title" : "Accelerated block-coordinate relaxation for regularized optimization",
      "author" : [ "S.J. Wright" ],
      "venue" : "Technical report, Department of Computer Science, University of Wisconsin-Madison,",
      "citeRegEx" : "Wright.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wright.",
      "year" : 2010
    }, {
      "title" : "A generalized approximate cross validation for smoothing splines with non-Gaussian data",
      "author" : [ "D. Xiang", "G. Wahba" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Xiang and Wahba.,? \\Q1996\\E",
      "shortCiteRegEx" : "Xiang and Wahba.",
      "year" : 1996
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Yuan and Lin.,? \\Q2006\\E",
      "shortCiteRegEx" : "Yuan and Lin.",
      "year" : 2006
    }, {
      "title" : "The composite absolute penalties family for grouped and hierarchical variable selection",
      "author" : [ "P. Zhao", "G. Rocha", "B. Yu" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Conditional Random Fields (CRF) (Lafferty et al., 2001) and their extensions, e.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "dynamic CRF (Sutton et al., 2007), are well known in Natural Language Processing community.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "models to computer vision (Szeliski et al., 2007; Honorio and Samaras, 2010).",
      "startOffset" : 26,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "models to computer vision (Szeliski et al., 2007; Honorio and Samaras, 2010).",
      "startOffset" : 26,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "Ising model is another classical example that draws great interest in image processing (Williams et al., 2004) and also has been recently applied to social networks (Banerjee et al.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : ", 2004) and also has been recently applied to social networks (Banerjee et al., 2008).",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "(2009) studied sparse covariance estimation of Gaussian outcomes (Speed and Kiiveri, 1986) without input features.",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : ", 2004) and also has been recently applied to social networks (Banerjee et al., 2008). The intuition of utilizing a graph structure is that some responses are related while others are not. However, in many cases, the graph is pre-determined by domain knowledge. For example, Duan et al. (2008) proposed a collective model for labeling music signals with fully connected graph, which they called collective conditional random fields.",
      "startOffset" : 63,
      "endOffset" : 294
    }, {
      "referenceID" : 0,
      "context" : ", 2004) and also has been recently applied to social networks (Banerjee et al., 2008). The intuition of utilizing a graph structure is that some responses are related while others are not. However, in many cases, the graph is pre-determined by domain knowledge. For example, Duan et al. (2008) proposed a collective model for labeling music signals with fully connected graph, which they called collective conditional random fields. They have 10 semantic categories such as genre (blues, rap, . . . ), instrument (guitar, piano, . . . ), production (studio, live), rhythm(strong, weak, middle), and etc. It is possible that some links are not necessary: e.g. production and instrument. Estimating the parameters on these relations will lead to over-fitting. Therefore, graph structure learning is an important aspect of relation discovery in multivariate response applications and multi-task learning. Many papers have focused on the graphical model selection issue. Meinshausen and Buhlmann (2006) and Peng et al.",
      "startOffset" : 63,
      "endOffset" : 999
    }, {
      "referenceID" : 0,
      "context" : ", 2004) and also has been recently applied to social networks (Banerjee et al., 2008). The intuition of utilizing a graph structure is that some responses are related while others are not. However, in many cases, the graph is pre-determined by domain knowledge. For example, Duan et al. (2008) proposed a collective model for labeling music signals with fully connected graph, which they called collective conditional random fields. They have 10 semantic categories such as genre (blues, rap, . . . ), instrument (guitar, piano, . . . ), production (studio, live), rhythm(strong, weak, middle), and etc. It is possible that some links are not necessary: e.g. production and instrument. Estimating the parameters on these relations will lead to over-fitting. Therefore, graph structure learning is an important aspect of relation discovery in multivariate response applications and multi-task learning. Many papers have focused on the graphical model selection issue. Meinshausen and Buhlmann (2006) and Peng et al. (2009) studied sparse covariance estimation of Gaussian outcomes (Speed and Kiiveri, 1986) without input features.",
      "startOffset" : 63,
      "endOffset" : 1022
    }, {
      "referenceID" : 0,
      "context" : ", 2004) and also has been recently applied to social networks (Banerjee et al., 2008). The intuition of utilizing a graph structure is that some responses are related while others are not. However, in many cases, the graph is pre-determined by domain knowledge. For example, Duan et al. (2008) proposed a collective model for labeling music signals with fully connected graph, which they called collective conditional random fields. They have 10 semantic categories such as genre (blues, rap, . . . ), instrument (guitar, piano, . . . ), production (studio, live), rhythm(strong, weak, middle), and etc. It is possible that some links are not necessary: e.g. production and instrument. Estimating the parameters on these relations will lead to over-fitting. Therefore, graph structure learning is an important aspect of relation discovery in multivariate response applications and multi-task learning. Many papers have focused on the graphical model selection issue. Meinshausen and Buhlmann (2006) and Peng et al. (2009) studied sparse covariance estimation of Gaussian outcomes (Speed and Kiiveri, 1986) without input features. The covariance matrix determines the dependence structure in the Gaussian distribution and its sparsity specifies the linkage in Gaussian Markov Random Fields. This is not the case for non-elliptical distribution, such as the distribution of discrete random variables. Ravikumar et al. (2010) focused on graph structure selection of Ising model based on l1-regularized logistic regression.",
      "startOffset" : 63,
      "endOffset" : 1423
    }, {
      "referenceID" : 2,
      "context" : ", f) be the vector of natural parameters, where f(x) is the conditional log odds ratio (Gao et al., 2001) to be estimated f = logOR(Yi, i ∈ ω|Yj = 0, j / ∈ ω;X) (4) Here, the odds ratios are calculated recursively OR(Yi) = P (Yi = 1) 1− P (Yi = 1) , (5) OR(Yi, i ∈ ω ∪ {k}) = OR(Yi, i ∈ ω|Yk = 1) OR(Yi, i ∈ ω|Yk = 0) , suppose k / ∈ ω (6) Later, we will call f, · · · , f main effects, and f, · · · , f1,··· ,K measures the interactions between the response variables.",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "We are interested in the sparse estimation of f, which is in a Reproducing Kernel Hilbert Space (RKHS) H with kernel K (Wahba, 1990).",
      "startOffset" : 119,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : "Suppose each function f is in a Reproducing Kernel Hilbert Space (RKHS) H with kernel K (Wahba, 1990).",
      "startOffset" : 88,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "This objective is similar to sparse covariance matrix estimation in Gaussian data for neighborhood selection with lasso (Meinshausen and Buhlmann, 2006).",
      "startOffset" : 120,
      "endOffset" : 152
    }, {
      "referenceID" : 21,
      "context" : "Group lasso (Yuan and Lin, 2006) leads to selection of variables in groups.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "Jacob et al. (2009) considered the penalty on groups with arbitrary overlaps.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "Jacob et al. (2009) considered the penalty on groups with arbitrary overlaps. Zhao et al. (2009) set up the general framework for hierarchical variable selections with overlapping groups, which we adopt here for the functions.",
      "startOffset" : 0,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "Jacob et al. (2009) considered the penalty on groups with arbitrary overlaps. Zhao et al. (2009) set up the general framework for hierarchical variable selections with overlapping groups, which we adopt here for the functions. We consider the penalty guided by the structure in Figure 2. The guiding graph T has 2 − 1 nodes: 1, . . . , ω, . . . ,Ω. With some abuse of notation, we use the element in Ψk to index the node in T . There is an edge from ω1 to ω2 if and only if ω1 ⊂ ω2 and |ω1|+1 = |ω2|. Domain knowledge can be applied here to design a different guiding structure. Jenatton et al. (2009) discussed how to define the groups to achieve different nonzero patterns.",
      "startOffset" : 0,
      "endOffset" : 602
    }, {
      "referenceID" : 19,
      "context" : "To solve (18), we iteratively solve the following proximal linearization problem (Wright, 2010): min c Lk +∇L T k (c− ck) + αk 2 ‖c− ck‖ 2 + λJ(c) (19) where Lk = L(ck), αk is a positive scalar chosen adaptively.",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "To solve (18), we iteratively solve the following proximal linearization problem (Wright, 2010): min c Lk +∇L T k (c− ck) + αk 2 ‖c− ck‖ 2 + λJ(c) (19) where Lk = L(ck), αk is a positive scalar chosen adaptively. With slight abuse of notation, we use ck to denote the vector of all parameters at kth step. Algorithm 1 summarized the framework of solving (18). Following the analysis in Wright (2010), we can show that the proximal linearization algorithm will converge for negative log-likelihood loss function plus group lasso type penalties with overlaps.",
      "startOffset" : 82,
      "endOffset" : 400
    }, {
      "referenceID" : 4,
      "context" : "Jacob et al. (2009) duplicated the variables in the design matrix that appear in group overlaps, then solved the problem as group lasso without overlaps.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "Jacob et al. (2009) duplicated the variables in the design matrix that appear in group overlaps, then solved the problem as group lasso without overlaps. Kim and Xing (2010) reparameterized the group norm with additional dummy variables.",
      "startOffset" : 0,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "In this paper, we solve (19) by its smooth and convex dual problem proposed by Liu and Ye (2010). Let Z = {v ∈ ΨK |‖c v‖ = 0}, and Z̄ = ΨK −Z be the complement.",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "We use the accelerated gradient descent method implemented in (Liu et al., 2009).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "Following the proof in Liu and Ye (2010), we can show that η(S) is convex and Lipschitz continuous.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 11,
      "context" : "The partial correlation method (Peng et al., 2009) is taken as an example to show how the links of response variables are discovered without considering X.",
      "startOffset" : 31,
      "endOffset" : 50
    } ],
    "year" : 2013,
    "abstractText" : "In undirected graphical models, learning the graph structure and learning the functions that relate the predictive variables (features) to the responses given the structure are two topics that have been widely investigated in machine learning and statistics. Learning graphical models in two stages will have problems because graph structure may change after considering the features. The main contribution of this paper is the proposed method that learns the graph structure and functions on the graph at the same time. General graphical models with binary outcomes conditioned on predictive variables are proved to be equivalent to multivariate Bernoulli model. The reparameterization of the potential functions in graphical model by conditional log odds ratios in multivariate Bernoulli model offers advantage in the representation of the conditional independence structure in the model. Additionally, we impose a structure penalty on groups of conditional log odds ratios to learn the graph structure. These groups of functions are designed with overlaps to enforce hierarchical function selection. In this way, we are able to shrink higher order interactions to obtain a sparse graph structure. Simulation studies show that the method is able to recover the graph structure. The analysis of county data from Census Bureau gives interesting relations between unemployment rate, crime and others discovered by the model.",
    "creator" : "LaTeX with hyperref package"
  }
}