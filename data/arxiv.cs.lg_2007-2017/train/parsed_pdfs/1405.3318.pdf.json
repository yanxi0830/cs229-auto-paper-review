{
  "name" : "1405.3318.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Adaptive Monte Carlo via Bandit Allocation",
    "authors" : [ "James Neufeld", "András György", "Dale Schuurmans", "Csaba Szepesvári" ],
    "emails" : [ "JNEUFELD@UALBERTA.CA", "GYORGY@UALBERTA.CA", "DAES@UALBERTA.CA", "CSABA.SZEPESVARI@UALBERTA.CA" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Monte Carlo methods are a pervasive approach to approximating complex integrals, which are widely deployed in all areas of science. Their widespread adoption has led to the development dozens of specialized Monte Carlo methods for any given task, each having their own tunable parameters. Consequently, it is usually difficult for a practitioner to know which approach and corresponding parameter setting might be most effective for a given problem.\nIn this paper we develop algorithms for sequentially allocating calls between a set of unbiased estimators to minimize the expected squared error (MSE) of a combined estimate. In particular, we formalize a new class of adaptive estimation problem: learning to combine Monte Carlo estimators. In this scenario, one is given a set of Monte Carlo estimators that can each approximate the expectation of some function of interest. We assume initially that each estimator is unbiased but has unknown variance. In practice, such estimators could include any unbiased method and/or variance reduction technique, such as unique instantiations of importance, stratified, or rejection sampling; antithetic Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nvariates; or control variates (Robert & Casella, 2005). The problem is to design a sequential allocation procedure that can interleave calls to the estimators and combine their outputs to produce a combined estimate whose MSE decreases as quickly as possible. To analyze the performance of such a meta-strategy we formalize the notion of MSEregret: the time-normalized excess MSE of the combined estimate compared to the best estimator selected in hindsight, i.e., with knowledge of the distribution of estimates produced by each base estimator.\nOur first main contribution is to show that this meta-task can be reduced to a stochastic multi-armed bandit problem, where bandit arms are identified with base estimators and the payoff of an arm is given by the negative square of its sampled estimate. In particular, we show that the MSEregret of any meta-strategy is equal to its bandit-regret when the procedure is used to play in the corresponding bandit problem. As a consequence, we conclude that existing bandit algorithms, as well as their bounds on banditregret, can be immediately applied to achieve new results for adaptive Monte Carlo estimation. Although the underlying reduction is quite simple, the resulting adaptive allocation strategies provide novel alternatives to traditional adaptive Monte Carlo strategies, while providing strong finite-sample performance guarantees.\nSecond, we consider a more general case where the alternative estimators require different (possibly random) costs to produce their sampled estimates. Here we develop a suitably designed bandit formulation that yields bounds on the MSE-regret for cost-aware estimation. We develop new algorithms for this generalized form of adaptive Monte Carlo, provide explicit bounds on their MSE-regret, and compare their performance to a state-of-the-art adaptive Monte Carlo method. By instantiating a set of viable base estimators and selecting from them algorithmically, rather than tuning parameters manually, we discover that both computation and experimentation time can be reduced.\nThis work is closely related, and complementary to work on adaptive stratified sampling (Carpentier & Munos, 2011), where a strategy is designed to allocate samples between fixed strata to achieve MSE-regret bounds relative\nar X\niv :1\n40 5.\n33 18\nv1 [\ncs .A\nI] 1\n3 M\nay 2\n01 4\nto the best allocation proportion chosen in hindsight. Such work has since been extended to optimizing the number (Carpentier & Munos, 2012a) and structure (Carpentier & Munos, 2012b) of strata for differentiable functions. The method proposed in this paper, however, can be applied more broadly to any set of base estimation strategies and potentially even in combination with these approaches."
    }, {
      "heading" : "2. Background on Bandit Problems",
      "text" : "The multi-armed bandit (MAB) problem is a sequential allocation task where an agent must choose an action at each step to maximize its long term payoff, when only the payoff of the selected action can be observed (Cesa-Bianchi & Lugosi, 2006; Bubeck & Cesa-Bianchi, 2012b). In the stochastic MAB problem (Robbins, 1952) the payoff for each action k ∈ {1, 2, . . . ,K} is assumed to be generated independently and identically (i.i.d.) from a fixed but unknown distribution νk. The performance of an allocation policy can then by analyzed by defining the cumulative regret for any sequence of n actions, given by\nRn . = max 1≤k≤K E [ n∑ t=1 Xk,t − n∑ t=1 XIt,TIt (t) ] , (1)\nwhere Xk,t ∈ R is the random variable giving the tth payoff of action k, It ∈ {1, . . . ,K} denotes the action taken by the policy at time-step t, and Tk(t) . = ∑t s=1 I{Is = k} denotes the number of times action k is chosen by the policy up to time t. Here, I{p} is the indicator function, set to 1 if the predicate p is true, 0 otherwise. The objective of the agent is to maximize the total payoff, or equivalently to minimize the cumulative regret. By rearranging (1) and conditioning, the regret can be rewritten\nRn = K∑ k=1 E[Tk(n)](µ∗ − µk), (2)\nwhere µk . = E[Xk,t] and µ∗ . = maxj=1,...,K µj .\nThe analysis of the stochastic MAB problem was pioneered by Lai & Robbins (1985) who showed that, when the payoff distributions are defined by a single parameter, the asymptotic regret of any sub-polynomially consistent policy (i.e., a policy that selects non-optimal actions only sub-polynomially many times in the time horizon) is lower bounded by Ω(log n). In particular, for Bernoulli payoffs\nlim inf n→∞ Rn log n\n≥ ∑\nk:∆k>0\n∆k KL(µk, µ∗) , (3)\nwhere ∆k . = µ∗ − µk and KL (p, q) . = p log(p/q) + (1 − p) log(1−p1−q ) for p, q ∈ [0, 1]. Lai & Robbins (1985) also presented an algorithm based on upper confidence bounds (UCB), which achieves a regret asymptotically matching the lower bound (for certain parametric distributions).\nLater, Auer et al. (2002a) proposed UCB1 (Algorithm 1), which broadens the practical use of UCB by dropping the\nAlgorithm 1 UCB1 (Auer et al., 2002a) 1: for k ∈ {1, . . . ,K} 2: Play k, observe Xk,1, set µ̄k,1 := Xk,1; Tk(1) := 1. 3: end for 4: for t ∈ {K + 1,K + 2, . . .} 5: Play action k that maximizes µ̄j,t−1 + √ 2 log t Tj(t−1) ;\nset Tk(t) = Tk(t − 1) + 1 and Tj(t) = Tj(t − 1) for j 6= k, observe payoff Xk,Tk(t), and compute µ̄k,t = (1− 1/Tk(t))µ̄k,t−1 +Xk,Tk(t)/Tk(t).\n6: end for\nrequirement that payoff distributions fit a particular parametric form. Instead, one need only make the much weaker assumption that the rewards are bounded; in particular, we let Xk,t ∈ [0, 1]. Auer et al. (2002a) proved that, for any finite number of actions n, UCB1’s regret is bounded by\nRn ≤ ∑\nk:∆k>0\n8 log n ∆k + ( 1 + π2 3 ) ∆k. (4)\nVarious improvements of the UCB1 algorithm have since been proposed. One approach of particular interest is the UCB-V algorithm (Audibert et al., 2009), which takes the empirical variances into account when constructing confidence bounds. Specifically, UCB-V uses the bound\nµ̄k,t +\n√ 2Vk,Tk(t−1)ETk(t−1),t\nTk(t− 1) + c 3ETk(t−1),t Tk(t− 1) ,\nwhere Vk,s denotes the empirical variance of arm k’s payoffs after s pulls, c > 0, and Es,t is an exploration function required to be a non-decreasing function of s or t (typically Es,t = ζ log(t) for a fixed constant ζ > 0). The UCB-V procedure can then be constructed by substituting the above confidence bound into Algorithm 1, which yields a regret bound that scales with the true variance of each arm\nRn ≤ cζ ∑\nk:∆k>0\n(V(Xk,1) ∆k + 2 ) log n. (5)\nHere cζ is a constant relating to ζ and c. In the worst case, when V(Xk,1) = 1/4 and ∆k = 1/2, this bound is slightly worse than UCB1’s bound; however, it is usually better in practice, particularly if some k has small ∆k and V(Xk,1).\nA more recent algorithm is KL-UCB (Cappé et al., 2013), where the confidence bound for arm k is based on solving\nsup { µ : KL (µ̄k,t, µ) ≤ f(t)\nTk(t)\n} ,\nfor a chosen increasing function f(·), which can be solved efficiently since KL (p, ·) is smooth and increasing on [p, 1]. By choosing f(t) = log(t) + 3 log log(t) for t ≥ 3 (and f(1)=f(2)=f(3)), KL-UCB achieves a regret bound\nRn ≤ ∑\nk:∆k>0\n( ∆k KL (µk, µ∗) ) log n+O( √ log(n)) (6)\nAlgorithm 2 Thompson Sampling (Agrawal & Goyal, 2012) Require: Prior parameters α and β Initialize: S1:K(0) := 0, F1:K(0) := 0 for t ∈ {1, 2, . . .}\nSample θt,k∼B(Sk(t−1)+α, Fk(t−1)+β), k = 1...K Play k = arg maxj=1,...,K θt,j ; observe Xt ∈ [0, 1] Sample X̂t ∼ Bernoulli(Xt) if X̂t = 1 then set Sk(t) = Sk(t − 1) + 1 else set Fk(t) = Fk(t) + 1\nend for\nfor n ≥ 3, with explicit constants for the “higher order” terms (Cappé et al., 2013, Corollary 1). Apart from the higher order terms, this bound matches the lower bound (3). In general, KL-UCB is expected to be better than UCB-V except for large sample sizes and small variances. Note that, given any set of UCB algorithms, one can apply the tightest upper confidence from the set, via the union bound, at the price of a small additional constant in the regret.\nAnother approach that has received significant recent interest is Thompson sampling (TS) (Thompson, 1933): a Bayesian method where actions are chosen randomly in proportion to the posterior probability that their mean payoff is optimal. TS is known to outperform UCB-variants when payoffs are Bernoulli distributed (Chapelle & Li, 2011; May & Leslie, 2011). Indeed, the finite time regret of TS under Bernoulli payoff distributions closely matches the lower bound (3) (Kaufmann et al., 2012):\nRn ≤ (1+ε) ∑\nk:∆k>0\n∆k(log(n) + log log(n))\nKL(µk, µ∗) +C(ε, µ1:K),\nfor every ε > 0, where C is a problem-dependant constant. However, since it is not possible to have Bernoulli distributed payoffs with the same mean and different variances, this analysis is not directly applicable to our setting. Instead, we consider a more general version of Thompson sampling (Agrawal & Goyal, 2012) that converts realvalued to Bernoulli-distributed payoffs through a resampling step (Algorithm 2), which has been shown to obtain\nRn ≤ ( ∑ k:∆k>0 1 ∆2k )2 log(n). (7)"
    }, {
      "heading" : "3. Combining Monte Carlo Estimators",
      "text" : "We now formalize the main problem we consider in this paper. Assume we are given a finite number of Monte Carlo estimators, k = 1, . . . ,K, where base estimator k produces a sequence of real-valued random variables (Xk,t)(t=1,2,...) whose mean converges to the unknown target quantity, µ ∈ R. Observations from the different estimators are assumed to be independent from each other. We assume, initially, that drawing a sample from each estimator takes constant time, hence the estimators differ only in terms of how\nfast their respective sample means Xk,n = 1n ∑n t=1Xk,t converge to µ. The goal is to design a sequential estimation procedure that works in discrete time steps: For each round t = 1, 2, . . ., based on the previous observations, the procedure selects one estimator It ∈ {1, . . . ,K}, whose observation is used by an outer procedure to update the estimate µ̂t ∈ R based on the values observed so far.\nAs is common in the Monte Carlo literature, we evaluate accuracy by the mean-squared error (MSE). That is, we define the loss of the sequential methodA at the end of round n by Ln(A) = E [ (µ̂n − µ)2 ] . A reasonable goal is to\nthen compare the loss, Lk,n = E [ (Xk,n − µ)2 ] , of each base estimator to the loss ofA. In particular, we propose to evaluate the performance of A by the (normalized) regret\nRn(A) = n2 ( Ln(A)− min\n1≤k≤K Lk,n\n) ,\nwhich measures the excess loss of A due to its initial ignorance of estimator quality. Implicit in this definition is the assumption thatA’s time to select the next estimator is negligible compared to the time to draw an observation. Note also that the excess loss is multiplied by n2, which ensures that, in standard settings, when Lk,n ∝ 1/n, a sublinear regret (i.e., |Rn(A|)/n → 0 as n → ∞) implies that the loss ofA asymptotically matches that of the best estimator.\nIn the next two sections we will adopt a simple strategy for combining the values returned from the base estimators: A simply returns their (unweighted) average as the estimate µ̂n of µ. A more sophisticated approach may be to weight each of these samples inversely proportional to their respective (sample) variances. However, if the adaptive procedure can quickly identify and ignore highly suboptimal arms the savings from the weighted estimator will diminish rapidly. Interestingly, this argument does not immediately translate to the nonuniform cost case considered in Section 5 as will be shown empirically in Section 6."
    }, {
      "heading" : "4. Combining Unbiased I.I.D. Estimators",
      "text" : "Our main assumption in this section will be the following: Assumption 4.1. Each estimator produces a sequence of i.i.d. random observations with common mean µ and finite variance; values from different estimators are independent.\nLet ψk denote the distribution of samples from estimator k. Note that Ψ = (ψk)1≤k≤K completely determines the sequential estimation problem. Since the samples coming from estimator k are i.i.d., we have V (Xk,1) = V (Xk,t). Let Vk = V (Xk,1) and V ∗ = min1≤k≤K Vk. Furthermore, let Lk,t = V (Xk,1) /t, hence min1≤k≤K Lk,t = V ∗/t. We then have the first main result of this section. Theorem 1 (Regret Identity). Consider K estimators for which Assumption 4.1 holds, and let A be an arbitrary allocation procedure. Then, for any n ≥ 1, the MSE-regret\nof the estimation procedure Aavg, estimating µ using the sample-mean of the observations obtained by A, satisfies\nRn(Aavg) = K∑ k=1 E [Tk(n)] (Vk − V ∗) . (8)\nThe proof follows from a simple calculation given in Appendix A. Essentially, one can rewrite the loss as Ln(A) = 1 n2E [∑K k=1 S 2 k,n + 2 ∑ k 6=j Sk,nSj,n ] , where Sk,n is the centered sum of observations for arm k. The cross-terms can be shown to cancel by independence, and Wald’s second identity with some algebra gives the result.\nThe tight connection between sequential estimation and bandit problems revealed by (8) allows one to reduce sequential estimation to the design of bandit strategies and vice versa. Furthermore, regret bounds transfer both ways. Theorem 2 (Reduction). Let Assumption 4.1 hold for Ψ. Define a corresponding bandit problem (νk) by assigning νk as the distribution of−X2k,1. Given an arbitrary allocation strategy A, let Bandit(A) be the bandit strategy that consults A to select the next arm after obtaining reward Yt (assumed nonpositive), based on feeding observations (−Yt)1/2 to A and copying A’s choices. Then, the banditregret of Bandit(A) in bandit problem (νk) is the same as the MSE-regret of A in estimation problem Ψ. Conversely, given an arbitrary bandit strategy B, let MC(B) be the allocation strategy that consults B to select the next estimator after observing−Y 2t , based on feeding rewards Yt to B and copying B’s choices. Then the MSE-regret of MC(B) in estimation problem Ψ is the same as the bandit-regret of B in bandit problem (νk) (where MC(B) uses the average of observations as its estimate).\nProof of Theorem 2. The result follows from Theorem 1 since Vk = E[X2k,1] − µ2 and V ∗ = E[X2k∗,1]− µ2 where k∗ is the lowest variance estimator, hence Vk − V ∗ = E[X2k,1] − min1≤k′≤K E[X2k′,1]. Furthermore, the bandit problem (νk) ensures the regret of a procedure that chooses arm k Tk(n) times is ∑K k=1 E[Tk(n)]∆k, where ∆k = max1≤k′≤K E[−X2k′,1]−E[−X2k,1] = Vk−V ∗.\nFrom this theorem one can also derive a lower bound. First, let V(ψ) denote the variance of X ∼ ψ and let V∗(Ψ) = min1≤k≤K V(ψk). For a family F of distributions over the reals, let Dinf(ψ, v,F) = infψ′∈F :V(ψ′)<vD(ψ,ψ′), where D(ψ, φ) = ∫ log dψdφ (x) dψ(x), if the RadonNikodym derivative dψ/dφ exists, and∞ otherwise. Note that Dinf(ψ, v,F) measures how distinguishable ψ is from distributions in F having smaller variance than v. Further, we let Rn(A,Ψ) denote the regret of A on the estimation problem specified using the distributions Ψ. Theorem 3 (MSE-Regret Lower Bound). Let F be the set of distributions supported on [0, 1] and assume that A allocates a subpolynomial fraction to suboptimal estimators\nfor any Ψ∈FK: i.e., EΨ[Tk(n)]=O(na) for all a > 0 and k such that V(ψk)>V∗(Ψ). Then, for any Ψ∈F where not all variances are equal and 0<Dinf(ψk,V∗(Ψ),F)<∞ holds whenever V(ψk)>V∗(Ψ), we have\nlim inf n→∞ Rn(A,Ψ) log n\n≥ ∑\nk:V(ψk)>V∗(Ψ)\nV(ψk)− V∗(Ψ) Dinf(ψk,V∗(Ψ),F) .\nProof. The result follows from Theorem 2 and (Burnetas & Katehakis, 1996, Proposition 1).\nUsing Theorem 2 we can also establish bounds on the MSE-regret for the algorithms mentioned in Section 2.\nTheorem 4 (MSE-Regret Upper Bounds). Let Assumption 4.1 hold for Ψ = (ψk) where (for simplicity) we assume each ψk is supported on [0, 1]. Then, after n rounds, MC(B) achieves the MSE-Regret bound of: (4) when using B= UCB1; (5) when using B= UCB-V with cζ = 10; (6) when using B=UCB-KL; and (7) when using B=TS.1\nAdditionally, due to Theorem 2, one can also obtain bounds on the minimax MSE-regret by exploiting the lower bound for bandits in (Auer et al., 2002b). In particular, the UCB-based bandit algorithms above can all be shown to achieve the minimax rate O( √ Kn) up to logarithmic factors, immediately implying that the minimax MSE-regret of MC(B) for B ∈ {UCB1,UCB-V,KL-UCB} is of order Ln(MC(B))− L∗n = Õ(K1/2n−(1+ 1 2 )).2\nAppendix B provides a discussion of how alternative ranges on the observations can be handled, and how the above methods can still be applied when the payoff distribution is unbounded but satisfies moment conditions."
    }, {
      "heading" : "5. Non-uniform Estimator Costs",
      "text" : "Next, we consider the case when the base estimators can take different amounts of time to generate observations. A consequence of non-uniform estimator times, which we refer to as non-uniform costs, is that the definitions of the loss and regret must be modified accordingly. Intuitively, if an estimator takes more time to produce an observation, it is less useful than another estimator that produces observations with (say) identical variance but in less time.\nTo develop an appropriate notion of regret for this case, we introduce some additional notation. Let Dk,m denote the time needed by estimator k to produce its mth observation, Xk,m. As before, we let Im ∈ {1, . . . ,K} denote the index of the estimator that A chooses in round m. Let Jm denote the time when A observes the mth sample,\n1 Note that to apply the regret bounds from Section 2, one has to feed the bandit algorithms with 1− Y 2t instead of −Y 2t in Theorem 2. This modification has no effect on the regret.\n2 Õ denotes the order up to logarithmic factors. To remove such factors one can exploit MOSS (Audibert & Bubeck, 2010).\nYm =XIm,TIm (m); thus, J1 =DI1,1, J2 =J1 +DI2,TI2 (2), and Jm+1 = Jm+DIm,TIm (m) = ∑m s=1DIs,TIs (s). For convenience, define J0 = 0. Note that round m starts at time Jm−1 with A choosing an estimator, and finishes at time Jm when the observation is received and A (instantaneously) updates it estimate. Thus, at time Jm a new estimate µ̂m becomes available: the estimate is “renewed”. Let µ̂(t) denote the estimate available at time t ≥ 0. Assuming A produces a default estimate µ̂0 before the first observation, we have µ̂(t) = µ̂0 on [0, J1), µ̂(t) = µ̂1 on [J1, J2), etc. IfN(t) denotes the round index at time t (i.e.,N(t)=1 on [0, J1), N(t)=2 on [J1, J2), etc.) then µ̂(t)= µ̂N(t)−1. The MSE of A at time t ≥ 0 is\nL(A, t) = E [ (µ̂(t)− µ)2 ] .\nBy comparison, the estimate for a single estimator k at time t is µ̂k(t) = I{Nk(t) > 1} ∑Nk(t)−1\nm=1 Xk,m Nk(t)−1 , where Nk(t) =\n1 on [0, Dk,1), Nk(t) = 2 on [Dk,1, Dk,1 + Dk,2), etc. We set µ̂k(t) = 0 on [0, Dk,1) to let µ̂k(t) be well-defined on [0, Dk,1). Thus, at time t ≥ 0 the MSE of estimator k is\nLk(t) = E [ (µ̂k(t)− µ)2 ] .\nGiven these definitions, it is natural to define the regret as R(A, t) = t2 ( L(A, t)− min\n1≤k≤K Lk(t)\n) .\nAs before, the t2 scaling is chosen so that, under the condition that Lk(t) ∝ 1/t, a sublinear regret implies that A is “learning”. Note that this definition generalizes the previous one: if Dk,m = 1∀k, m, then Rn(A) = R(A, n).\nIn this section we make the following assumption.\nAssumption 5.1. For each k, (Xk,m, Dk,m)(m=1,2,...) is an i.i.d. sequence such that E [Xk,1] =µ, Vk . =V (Xk,1)< ∞, P(Dk,m > 0) = 1 and δk . = E[Dk,1] = E [Dk,m]<∞. Furthermore, we assume that the sequences for different k are independent of each other.\nNote that Assumption 5.1 allows Dk,m to be a deterministic value; a case that holds when the estimators use deterministic algorithms to produce observations. Another situation arises when Dk,m is stochastic (i.e., the estimator uses a randomized algorithm) and (Xk,m, Dk,m) are correlated. In which case µ̂k(t) may be a biased estimate of µ. However, if (Xk,m)m and (Dk,m)m are independent and P(Nk(t) > 1) = 1 then µ̂k(t) is unbiased. Indeed, in such a case, (Nk(t))t is independent of the partial sums\n( ∑n m=1Xk,m)n, hence E[µ̂k(t)] = E [∑Nk(t)−1 m=1 Xk,m Nk(t)−1 ] =∑∞\nn=2 P(Nk(t) = n)E [∑n−1 m=1Xk,m n−1 ∣∣∣Nk(t) = n] = P(Nk(t)>1)E [X], because µ̂k(t)=0 when Nk(t)≤1.\nUsing Assumption 5.1, a standard argument of renewal reward processes gives Lk(t)∼ Vk/(t/δk) = Vkδk/t, where\nf(t)∼g(t) means limt→∞ f(t)/g(t) = 1. Intuitively, estimator k will produce approximately t/δk independent observations during [0, t); hence, the variance of their average is approximately Vk/(t/δk) (made more precise in the proof of Theorem 5). This implies min1≤k≤K Lk(t)∼ min1≤k≤K δkVk t . Thus, any allocation strategy A competing with the best estimator must draw most of its observations from k satisfying δkVk = δ∗V ∗ . = min1≤k≤K δkVk t . For simplicity, we assume k∗ = arg min1≤k≤KδkVk is unique, with δ∗=δk∗ and V ∗ = Vk∗ .\nAs before, we will consider adaptive strategies that estimate µ using the mean of the observations: µ̂m= Smm such that Sm . = ∑m s=1 Ys. Hence, the estimate at time t ≥ J1 is\nµ̂(t) = S(t)\nN(t)− 1 , where S(t) = SN(t)−1 . (9)\nOur aim is to bound regret of the overall algorithm by bounding the number of times the allocation strategy chooses suboptimal estimators. We will do so by generalizing (2) to the nonuniform-cost case, but unlike the equality obtained in Theorem 1, here we provide an upper bound.\nTheorem 5. Let Assumption 5.1 hold and assume that (Xk,m) are bounded and k∗ is unique. Let the estimate of A at time t be defined by the sample mean µ̂(t). Let t>0 be such that E [N(t)− 1]>0 and E [Nk∗(t)]>0, and assume that for any k 6= k∗, E [Tk(N(t))]≤ f(t) for some f : (0,∞)→ [1,∞) such that f(t)≤ cf t for some cf > 0 and any t > 0. Assume furthermore that P(Dk,1 > t) ≤ CDt −2 and E [ Nk∗(t) 2 ] ≤ CN t2 for all t > 0. Then, for\nany c < √ t/(8δmax) where δmax = maxk δk, the regret of A at time t is bounded by\nR(A, t) ≤ (c+ C) √ t + C ′f(t)\n+ C ′′t2 P ( Nk∗(t)>E [Nk∗(t)] + c √ E [Nk∗(t)− 1] ) + C ′′′t2 P ( N(t)<E [N(t)]− c √ E [N(t)− 1] ) , (10)\nfor some appropriate constants C,C ′, C ′′, C ′′′>0 that depend on the problem parameters δk, Vk, the upper bound on |Xk,m|, and the constants cf , CD and CN .\nThe proof of the theorem is given in Appendix C. Several comments are in order. First, recall that the optimal regret rate is order √ t in this setting, up to logarithmic factors. To obtain such a rate, one need only achieve f(t)=O( √ t), which can be attained by stochastic or even adversarial bandit algorithms (Bubeck & Cesa-Bianchi, 2012b) receiving rewards with expectation −δkVk and a well-concentrated number of samples. The moment condition on Nk∗(t) is also not restrictive; for example, if the estimators are rejection samplers, their sampling times will have a geometric distribution that satisfies the polynomial tail condition. Furthermore, ifDk,m≥δ− for some δ−>0 thenNk(t)<t/δ− for all k, which ensures the moment condition on Nk∗(t).\nAlthough it was sufficient to use the negative second moment −X2k,m instead of variance as the bandit reward under uniform costs, this simplification is no longer possible when costs are nonuniform, since δkVk=δk(E[X2k,1]−µ2) now involves the unknown expectation µ. Several strategies can be followed to bypass this difficulty. For example, given independent costs and observations, one can use each bandit algorithm decision twice, feeding rewards rk,m=− 14 (Dk,2m+Dk,2m+1)(Xk,2m−Xk,2m+1)\n2 whose expectation is δkVk. Similar constructions using slightly more data can be used for the dependent case.\nNote that ensuring E[Tk(N(t))] ≤ f(t) can be nontrivial. Typical guarantees for UCB-type algorithms ensure that the expected number of pulls to a suboptimal arm k in n rounds is bounded by a function gk(n). However, due to their dependence, E[Tk(N(t))] cannot generally be bounded by gk(E[N(t)]). Nevertheless, if, for example, Dk,m ≥ δ− for some δ−>0, then N(t)−1≤ t/δ−, hence fk(t)=gk(t/δ −) can be used.\nFinally, we need to ensure that the last two terms in (10) remain small, which follows if N(t) and Nk∗(t) concentrate around their means. In general, P(N < E[N ]− C √ E[N ] log(1/δ)) ≤ δ for some constant C, therefore\nc=C √\nlog(1/δ) can be chosen to achieve t2P(N<E[N ]− c √ E[N ]) ≤ t2δ, hence by chosing δ to be O(t−3/2) we achieve Õ( √ t) regret. However, to ensure concentration, the allocation strategy must also select the optimal estimator most of the time. For example, Audibert et al. (2009) show that with default parameters, UCB1 and UCB-V will select suboptimal arms with probability Ω(1/n), making t2P(N < E[N ]− c √ E[N ]) = Ω(t). However, by increasing the constant 2 in UCB1 and the parameter ζ in UCB-V, it follows from (Audibert et al., 2009) that the chance of using any suboptimal arm more than c log(t) √ t times can be made smaller than c/t (where c is some problem-dependent constant). Outside of this small probability event, the optimal arm is used t− cK log(t) √ t times, which is sufficient to show concentration of N(t). In summary, we conclude that Õ( √ t) regret can be achieved in Theorem 5 under reasonable assumptions."
    }, {
      "heading" : "6. Experiments",
      "text" : "We conduct experimental investigations in a number of scenarios to better understand the effectiveness of multi-armed bandit algorithms for adaptive Monte Carlo estimation."
    }, {
      "heading" : "6.1. Preliminary Investigation: A 2-Estimator Problem",
      "text" : "We first consider the performance of allocation strategies on a simple 2-estimator problem. Note that this evaluation differs from standard evaluations of stochastic bandits through the absence of single-parameter payoff distribu-\ntions, such as the Bernoulli, which cannot have identical means yet different variances. This is an important detail, since stochastic bandit algorithms such as KL-UCB and TS are often evaluated on single-parameter payoff distributions, but their advantages in such scenarios might not extend to adaptive Monte Carlo estimation.\nIn particular, we consider problems when Xk,t = µ + sk(Zt − 12 ), where Zt is standard Bernoulli and sk∈(0, 1) is a separate scale parameter for k∈{1, 2}. This design permits the maximum range for variance around a mean within a bounded interval. We evaluated the four bandit strategies detailed in Section 2: UCB1, UCB-V, KL-UCB, and TS, where for UCB-V we used the same settings as (Audibert et al., 2009), and for TS we used the uniform Beta prior, i.e., α0 = 1 and β0 = 1.\nThe relative performance of these approaches is reported in Figure 1. TS appears best suited for scenarios where either estimator has high variance, whereas UCB-V is more effective when faced with medium or low variance estimators. Additionally, KL-UCB out-performs UCB-V in high variance settings, but in all such cases was eclipsed by TS."
    }, {
      "heading" : "6.2. Option Pricing",
      "text" : "We next consider a more practical application of adaptive Monte Carlo estimation to the problem of pricing financial instruments. In particular, following (Douc et al., 2007; Arouna, 2004), we consider the problem of pricing European call options under the assumption that the interest rate evolves in time according to the Cox-Ingersoll-Ross (CIR) model (Cox et al., 1985), a popular model in mathematical finance (details provided in Appendix F). In a nutshell, this model assumes that the interest rate r(t), as a function of time t > 0, follows a square root diffusion model. The price of a European caplet option with “strike price”K>0, “nominee amount” M > 0 and “maturity” T > 0 is then given by P = M exp(− ∫ T 0 r(t)dt) max(r(T ) − K, 0). The problem is to determine the expected value of P .\nK = 0.06\nK = 0.08\nK = 0.07\nA naive approach to estimating E [P ] is to simulate independent realizations of r(t) for 0 < t ≤ T . However, any simulation where the interest rate r(T ) lands below the strike price can be ignored since the payoff is zero. Therefore, a common estimation strategy is to use importance sampling by introducing a “drift” parameter θ > 0 into the proposal density for r(t), with θ = 0 meaning no drift; this encourages simulations with higher interest rates. The importance weights for these simulations can then be efficiently calculated as a function of θ (see Appendix F).\nImportantly, the task of adaptively allocating trials between different importance sampling estimators has been previously studied on this problem, using an unrelated technique known as d-kernel Population Monte Carlo (PMC) (Douc et al., 2007). Space restrictions prevent us from providing a full description of the PMC method, but, roughly speaking, the method defines the proposal density as mixture over the set {θk} of drift parameters considered. At each time step, PMC samples a new drift value according to this mixture and then simulates an interest rate. After a fixed number of samples, say G (the population size), the mixture coefficient αk of each drift parameter θk is adjusted by setting it to be proportional to the sum of importance weights sampled from that parameter: αk = ∑G t=1 wtI{It=k}∑G\nt=1 wt . The new\nproposal is then used to generate the next population.\nWe approximated the option prices under the same parameter settings as (Douc et al., 2007), namely, ν = 0.016, κ = 0.2, r0 = 0.08, T = 1, M = 1000, σ = 0.02, and n= 100, for different strike prices K = {0.06, 0.07, 0.08} (see Appendix F). However, we consider a wider set of proposals given by θk = k/10 for k ∈ {0, 1, ..., 15}. The results averaged over 1000 simulations are given in Figure 2.\nThese results generally indicate that the more effective bandit approaches are significantly better suited to this allocation task than the PMC approach, particularly in the longer term. Among the bandit based strategies, TS is the clear winner, which, given the conclusions from the previous experiment, is likely due to high level of variance introduced by the option pricing formula. Despite this strong showing for the bandit methods, PMC remains surprisingly competi-\ntive at this task, doing uniformly better than UCB, and better than all other bandit allocation strategies early on for K = 0.06. However, we believe that this advantage of PMC stems from the fact that PMC explore the entire space of mixture distribution (rather than single θk). It remains an interesting area for future work in bandit-based allocation strategies to extend the existing methods to continuously parameterized settings."
    }, {
      "heading" : "6.3. Adaptive Annealed Importance Sampling",
      "text" : "Many important applications of Monte Carlo estimation occur in Bayesian inference, where a particularly challenging problem is evaluating the model evidence of a latent variable model. Evaluating such quantities is useful for a variety of purposes, such as Bayesian model comparison and testing/training set evaluation (Robert, 2012). However, the desired quantities are notoriously difficult to estimate in many important settings, due in no small part to the fact that popular high-dimensional Monte Carlo strategies, such as Markov Chain Monte Carlo (MCMC) methods, cannot be directly applied (Neal, 2005).\nNevertheless, a popular approach for approximating such values is annealed importance sampling (AIS) (Neal, 2001) (or more generally sequential Monte Carlo samplers (Del Moral et al., 2006)). In a nutshell, AIS combines the advantages of importance sampling with MCMC by defining a proposal density through a sequence of MCMC transitions applied to a sequence of annealed distributions, which slowly blend between the proposal (prior) and the target (un-normalized posterior). While such a technique can offer impressive practical advantages, it often requires considerable effort to set parameters; in particular, the practitioner must specify the number of annealing steps, the annealing rate or “schedule”, the underlying MCMC method (and its parameters), and the number of MCMC transitions to execute at annealing step. Even when these parameters have been appropriately tuned on preliminary data, there is no assurance that these choices will remain effective when deployed on larger or slightly different data sets.\nHere we consider the problem of approximating the normalization constant for a Bayesian logistic regression\nmodel on different sized subsets of the 8-dimensional Pima Indian diabetes UCI data set (Bache & Lichman, 2013). We consider the problem of allocating resources between three AIS estimators that differ only in the number of annealing steps they use; namely, 400, 2000, and 8000 steps. In each case, we fix the annealing schedule using the power of 4 heuristic suggested by (Kuss & Rasmussen, 2005), with a single slice sampling MCMC transition used at each step (Neal, 2003) (this entails one “step” in each of the 8 dimensions); see Appendix G for further details.\nA key challenge in this scenario is that the computational costs associated with each arm differ substantially, and, because slice sampling uses an internal rejection sampler, these costs are stochastic. To account for these costs we directly use elapsed CPU-time when drawing a sample from each estimator, as reported by the JAVA VM. This choice reflects the true underlying cost and is particularly convenient since it does not require the practitioner to implement special accounting functionality. Since we do not expect this cost to correlate with the sample returns, we use the independent costs payoff formulation from Section 5: − 14 (Dk,2m +Dk,2m+1)(Xk,2m −Xk,2m+1) 2.\nThe results for the different allocation strategies for training sets of size 5, 10, and 50 are shown in Figure 3. Perhaps the most striking result is the performance improvement achieved by the nonuniformly combined estimators, which are indicated by the dashed lines. These estimators do not change the underlying allocation; instead they improve the final combined estimate by weighting each observation inversely proportional to the sample variance of the estimator that produced it. This performance improvement is an artifact of the nonuniform cost setting, since arms that are very close in terms of Vkδk can still have considerably different variances, which is especially true for AIS. Also observe that no one arm is optimal for all three training set sizes, consequently, we can see that bandit allocation (Thompson sampling in particular) is able to outperform any static strategy. In practice, this implies that even after exhaus-\ntive parameter tuning, automated adaptation can still offer considerable benefits simply due to changes in the data."
    }, {
      "heading" : "7. Conclusion",
      "text" : "In this paper we have introduced a new sequential decision making strategy for competing with the best consistent Monte Carlo estimator in a finite pool. When each base estimator produces unbiased values at the same cost, we have shown that the sequential estimation problem maps to a corresponding bandit problem, allowing future improvements in bandit algorithms to be transfered to combining unbiased estimators. We have also shown a weaker reduction for problems where the different estimators take different (possibly random) time to produce an observation.\nWe expect this work to inspire further research in the area. For example, one may consider combining not only finitely many, but infinitely many estimators using appropriate bandit techniques (Bubeck et al., 2011), and/or exploit the fact that the observation from one estimator may reveal information about the variance of others. This is the case for example when the samplers use importance sampling, leading to the (new) stochastic variant of the problem known as “bandits with side-observations” (Mannor & Shamir, 2011; Alon et al., 2013). However, much work remains to be done, such studying in detail the use variance weighted estimators, dealing with continuous families of estimators, or a more thorough empirical investigation of the alternatives available."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by the Alberta Innovates Technology Futures and NSERC. Part of this work was done while Cs.Sz. was visiting Technion, Haifa and Microsoft Research, Redmond, whose support and hospitality are greatly acknowledged."
    }, {
      "heading" : "A. Proofs for Section 4",
      "text" : "In this section we provide the proof of Theorem 1. For the proof, we will need the following two lemmas:\nLemma 1 (Optional Sampling). Let (Xt)t∈N be a sequence of i.i.d. random variables, and (X ′t)t∈N be its subsequence such that the decision whether to include Xt in the subsequence is independent of future values in the sequence, i.e., Xs for s ≥ t. Then the sequence (X ′t)t∈N is an i.i.d. sequence with the same distribution as (Xt)t∈N.\nProof. See Theorem 5.2 in Chapter III on page 145 of (Doob, 1953).\nWe also need Wald’s second identity:\nLemma 2 (Wald’s Second Identity). Let (Xt)t∈N be a sequence of (Ft; t ≥ 1)-adapted random variables such that E [Xt|Ft−1] = µ and E [ (Xt − µ)2|Ft−1 ] = σ2 for any t ≥ 1. Let Sn = ∑n t=1Xt be the partial sum of the first n random variables (n ≥ 1) and τ > 0 be some stopping time w.r.t. (Ft; t ≥ 1).3 Then,\nE [ (Sτ − µτ)2 ] = σ2E [τ ] .\nProof. See Theorem 14.3 of (Gut, 2005).\nNow, let us return to the proof of Theorem 1. Let It ∈ {1, . . . ,K} denote the choice thatAmade at the beginning of round t and recall that Tk(t) = ∑t s=1 I{Is = k}. The observation at the end of round t is Yt = XIt,TIt (t) and the cumulative\nsum of returns after n rounds for arm k is Sk,n = ∑Tk(n) t=1 Xk,t. Likewise, Sn = ∑n t=1 Yt = ∑K k=1 Sk,n. By definition, the estimate after n rounds is\nµ̂n = Sn n = 1 n K∑ k=1 Sk,n .\nSince the variance of this estimate does not depend on the mean, without loss of generality we may consider the case when µ = 0, giving\nLn(A) = E [ µ̂2n ] = 1\nn2 E  K∑ k=1 S2k,n + 2 ∑ k 6=j Sk,nSj,n  (11) We may then observe that for any k 6= j,\nE [Sk,nSj,n] = E [(I{In = k}Yn + Sk,n−1)(I{In = j}Yn + Sj,n−1)]\n= E[ ((((\n(((( ((\nI{In = k}I{In = j}Y 2n + I{In = k}YnSj,n−1 + I{In = j}YnSk,n−1 + Sk,n−1Sj,n−1].\nBy considering the conditional expectation w.r.t. the history up to Yn (including In), i.e., w.r.t. Fn−1 = σ(Y1, . . . , Yn−1, I1, . . . , In), we have\nE[Sk,nSj,n] = E [E [Sk,nSj,n|Fn−1]] = E[(I{In = k}Sj,n−1 + I{In = j}Sk,n−1)E[Yn|Fn−1]] + E[Sk,n−1Sj,n−1].\nNow, since In ∈ {1, . . . ,K} and In is Fn−1]-measurable, E [Yn|Fn−1] = ∑K k=1 I{In = k}E [ Xk,Tk(n)|Fn−1, In = k ] . Further, by Lemma 1, (Xk,Tk(n))n is an i.i.d. sequence, sharing the same distribution as (Xk,t)t. Since σ(Xk,Tk(n)) is independent of σ(Y1, . . . , Yn−1, I1, . . . , In−1, In, In = k), E [ Xk,Tk(n)|Fn−1, In = k ] = E [ Xk,Tk(n) ] = µ = 0. Therefore, E [Sk,nSj,n] = E[Sk,n−1Sj,n−1] = . . . = E[Sk,0Sj,0] = 0.\nAs a result, (11) becomes\nLn(A) = 1\nn2 E [ K∑ k=1 S2k,n ] = 1 n2 K∑ k=1 E[S2k,n].\n3τ is called a stopping time w.r.t. (Ft; t ≥ 1) if {τ ≤ t} ∈ Ft for all t ≥ 1.\nFrom Wald’s second identity (cf. Lemma 2) we conclude E[S2k,n] = V(Xk)E[Tk(n)] and thus get\nLn(A) = 1\nn2 K∑ k=1 V(Xk)E[Tk(n)]. (12)\nUsing the definition of the normalized MSE-regret and that min1≤k≤K Lk,n = V ∗/n,\nRn(A) = n2 ( Ln(A)− V ∗\nn\n) = K∑ k=1 E[Tk(n)](Vk − V ∗),\nwhich was the result to be proven."
    }, {
      "heading" : "B. Handling Unknown Ranges",
      "text" : "The algorithms and bounds of Section 4 can be easily extended to the setting where Xk,t ∈ [ak, bk] (often a = 0) where ak < bk are a priori known. One option is to scale Xk,t to the common range [0, 1] using X̃k,t = (Xk,t − mink ak)/(maxk bk−mink ak) and then feed 1−X̃2k,t to the bandit algorithms (as the bandit algorithms expect the rewards in [0, 1] and constant translation of each reward does not change the regret). However, as these algorithms (prepared for “worst case” distributions) are sensitive to the overestimation of the range, this would lead to an unnecessary deterioration of the performance. A better option is to scale each variable separately. Then, the upper-confidence bound based algorithms must be modified by scaling each of the rewards with respect to its own range and then the bounds needs to be scaled back to the original range. Thus, the method that computes the reward upper bounds must be fed with (bk−amin)\n2−(Xk,m−amin)2 (bk−amin)2 ∈\n[0, 1], where amin = min1≤k≤K ak. Then, if the method returns an upper bound Bk,m, the bound to be used for selecting the best arm (which should be an upper bound for −E [ (Xk,t − amin)2 ] ) should be B′k,t = (bk − amin)2(Bk,t − 1).\nHere we exploit that −E [ (Xk,t − amin)2 ] = −E [ X2k,t ] + c where the constant c = 2µamin − a2min (which is neither known, nor needs to be known or estimated) is common to all the estimators, hence finding the arm that maximizes −E [ (Xk,t − amin)2 ] is equivalent to finding the arm that maximizes −E [ X2k,t ] . Since the confidence bounds will be loser if the range is larger, we thus see that the algorithm may be sensitive to the ranges rk = bk − amin. In particular, the 1/n-term in the bound used by UCB-V will scale with r2k and may dominate the bound for smaller values of n. In fact, UCB-V needs n ≈ r2k samples before this term becomes negligible, which means that for such estimators, the upper confidence bound of UCB-V will be above 1 for r2k steps. Even if we cut the upper confidence bounds at 1, UCB-V will keep using these arms with a large range rk defending against the possibility that the sample variance crudely underestimates the true variance. Since the bound of KL-UCB is in the range [0, 1], KL-UCB is less exposed to this problem.\nBased on Theorem 1, an alternative is to use bandit algorithms to minimize cost where the cost of an arm is defined as the variance of samples from that arm. This may be advantageous when the ranges become large because of unequal lower bounds (ak). To implement this idea one needs to develop new bandit algorithms for cumulative variance minimization, which in fact is explored in Section 5, in the context of estimation under nonuniform-costs.\nFinally, note that the assumption that the samples belong to a known bounded interval is not necessary (it was not used in the reduction results). In fact, the upper-confidence based bandit algorithms mentioned can also be applied when the payoffs are subgaussian with a known subgaussian coefficient,4 or even if the tail is heavier (Bubeck & Cesa-Bianchi, 2012a; Bubeck et al., 2013). In fact, the weaker assumptions under which the multi-armed bandit problem with finitely many arms has been analyzed assumes only that the 1 + ε moment of the payoff is finite for some known ε with a known moment bound (Bubeck et al., 2013). These strategies must replace the sample means with more robust estimators and also modify the way the upper confidence bounds are calculated. In our setting, the condition on the moment transfers to the assumption that the 2 + ε moment E [ |Xk,t|2+ε ] must be finite with a known upper bound for some ε."
    }, {
      "heading" : "C. Proofs for Section 5",
      "text" : "In this section we provide the proof of Theorem 5.\n4 A centered random variable X is subgaussian if P(|X| ≥ t) ≤ c exp(−t2/(2σ2)) for all real t with some c, σ > 0.\nWe can write the MSE at time t as L(A, t) = E [(\nS(t)−(N(t)−1)µ N(t)−1\n)2] . The first step is to replace the denominator with its\nexpectation. The price of this is calculated in the following result:\nLemma 3. Let S,N be random variables, N ≥ 0 such that E [N ] > 0, µ ∈ R. Let D = I{N = 0}(a − µ) + I{N 6= 0} ( S N − µ ) . Let d ∈ R be an almost sure upper bound on |D|I{N 6= 0}. Then, for any c ≥ 0,\nE [ D2 ] ≤ (a− µ)2P(N = 0) +\nE [ (S −Nµ)2 ] E [N ]2 ( 1 + 2c√ E [N ] )2 + d2 [ P(N < E [N ]− c √ E [N ]) + I{E [N ] < 4c2} ] .\n(13)\nFurther, assuming that S = 0 almost surely on {N = 0},\nE [ D2 ] ≥ (a− µ)2P(N = 0) +\nE [ (S −Nµ)2 ] E [N ]2 ( 1− 2c√ E [N ] ) − d2 E [ N2 ] E [N ]2 P(N > E [N ] + c √ E [N ]) .\nThe lemma is useful when N concentrates around its mean. Consider the upper bound on E [ D2 ] . In general, we expect\nP(N < E [N ] − C √ E [N ] log(1/δ)) ≤ δ with some numerical constant C. Thus, setting c = C √ log(1/δ) results in\nd2 [ P(N < E [N ]− c √ E [N ]) + I{E [N ] < 4c2} ] ≤ d2δ + d2I{E [N ] < 4C2 log(1/δ)}. Imagine now that E [N ] = rt\nand choose δ = t−q with q > 0. Then, as soon as rt > 4C2 √ q log(t), the last indicator of (13) becomes zero. Further,\n2c√ E[N ]\n= √\n2q log(t) rt → 0 as t→∞. Since this term converges at a 1/\n√ t rate, we should choose q ≥ 1/2, so that δ = δ(t)\nconverges at least as fast as this term. However, since later “D gets multiplied by t2”, to get a sublinear rate matching other terms, one should choose q ≥ 3/2.\nProof. Let b = c √ E [N ]. First we prove the upper bound. We have\nE [ D2 ] ≤ E [ D2I{N = 0} ] + E [ D2I{N 6= 0, N ≥ E [N ]− b, b ≤ 12E [N ]} ] + E [ D2I{N 6= 0, N < E [N ]− b} ] + E [ D2I{N 6= 0, b > 12E [N ]}\n] ≤ (a− µ)2P(N = 0) + E [ (S −Nµ)2\n(E [N ]− b)2 I{b ≤ 12E [N ]}\n] + d2P(N < E [N ]− b) + d2I{b > 12E [N ]}\n≤ (a− µ)2P(N = 0) + E [ (S −Nµ)2 ] (E [N ]− b)2 I{b ≤ 12E [N ]}+ d 2P(N < E [N ]− b) + d2I{b > 12E [N ]} .\nNoting that 1/(1− x) ≤ 1 + 2x when 0 ≤ x ≤ 1/2, we get that\n1\nE [N ]− b I{b ≤ 12E [N ]} =\n1 E [N ] 1\n1− bE[N ] I{b ≤ 12E [N ]} ≤\n1\nE [N ]\n( 1 + 2 b\nE [N ]\n) = 1\nE [N ]\n( 1 + 2\nc√ E [N ]\n) .\nPutting things together, we get the desired upper bound.\nThe lower bound is proved in a similar fashion. To simplify notation, letN denote the event {0 < N ≤ E [N ] + b}. Then,\nE [ D2 ] ≥ (a− µ)2P(N = 0) + E\n[( (S −Nµ)\nN\n)2 I{N} ]\n≥ (a− µ)2P(N = 0) + E [ (S −Nµ)2\n(E [N ] + b)2 I{N} ] = (a− µ)2P(N = 0) + E [ (S −Nµ)2\n(E [N ] + b)2\n] − E [ (S −Nµ)2\n(E [N ] + b)2 I{N = 0 or N > E [N ] + b} ] ≥ (a− µ)2P(N = 0) + E [N ] 2 (E [N ] + b)2 E [ (S −Nµ)2 ] E [N ]2 − E [ (S −Nµ)2 (E [N ] + b)2 I{N > E [N ] + b}\n] (because by assumption E [ (S −Nµ)2I{N = 0} ] = 0)\n= (a− µ)2P(N = 0) + E [N ] 2 (E [N ] + b)2 E [ (S −Nµ)2 ] E [N ]2 − E [ N2 ] (E [N ] + b)2 ( d2P(N > E [N ] + b) ) ≥ (a− µ)2P(N = 0) + ( 1− 2b\nE [N ]\n) E [ (S −Nµ)2 ] E [N ]2 − d2 E [ N2 ] P(N > E [N ] + b) E [N ]2 ,\nwhere in the last inequality we used\nE [N ]2\n(E [N ] + b)2 =\n( 1− b\nE [N ] + b\n)2 ≥ 1− 2b\nE [N ] + b\nand 1/(E [N ] + b) ≤ 1/E [N ]. The above lemma suggests that when bounding E [ (S/N − µ)2 ] , the main term is E [ (S −Nµ)2 ] /E [N ]2. First we develop a lower bound on this when only sampler k is used at every time step.\nLemma 4. Let Assumption 5.1 hold and assume that the random variables |Xk,m − µ|k,m are a.s. bounded by some constant B > 0. Consider the case when an individual sampler k ∈ {1, . . . ,K} is used up to time t: Let Nk(t) be the smallest integer such that ∑Nk(t) m=1 Dk,m ≥ t and Sk(t) = ∑Nk(t)−1 m=1 Xk,m. Then, for any t > 0 such that E [Nk(t)] > 1, and for any constant β > 0 such that P(Dk,1 ≤ β) > 0,\nE [ (Sk(t)− (Nk(t)− 1)µ)2 ] E [Nk(t)− 1]2 ≥ δkVk t+ β − 2Bδk √ Vkδk t3/2 − VkE [Dk,1I{Dk,1 > β}] t (14)\nand\nt\nδk ≤ E [Nk(t)] ≤\nt+ β\nδk − E [Dk,1I{Dk,1 > β}] , (15)\nIf the random variables (Dk,m) are a.s. bounded by a constant, we can choose β to be their common upper bound to cancel the third term of (14). Otherwise, we need to select β to strike a good balance between the first and third terms. Note that β = o(t) makes the first the same order as δkVk/t with an error term of order β/t2. Thus, a reasonable choice is β = t3/2, which makes the error term of the same order as the second term of the lower bound.\nProof. We start with the proving the upper bound on E [Nk(t)] in (15). Define D̂k,m = min{Dk,m, β} and let N̂k(t) be the smallest integer such that ∑N̂k(t) m=1 D̂k,m ≥ t. Then, clearly N̂k(t) ≥ Nk(t), and by Wald’s identity,\nt > E N̂k(t)−1∑ m=1 D̂k,m  ≥ E N̂k(t)∑ m=1 D̂k,m − β = E [ N̂k(t) ] E [ D̂k,1 ] − β ≥ E [Nk(t)] (δk − E [Dk,1I{Dk,1 > β}])− β.\nTherefore,\nE [Nk(t)] ≤ t+ β\nδk − E [Dk,1I{Dk,1 > β}] ,\nwhere we used that δk − E [Dk,1I{Dk,1 > β}] > 0, which follows from our assumption P(Dk,1 ≤ β) > 0, finishing the proof of the upper bound. As for the lower bound, by the definition ofNk(t) and Wald’s identity we have E [Nk(t)] δk ≥ t, thus finishing the proof of (15).\nLet us now turn to proving (14). We would like to apply Wald’s second identity to (Sk(t)−(Nk(t)−1)µ)2. However, while Nk(t) is a stopping time w.r.t. the (natural) filtration Fm = σ(Xk,1, Dk,1, . . . , Xk,m, Dk,m), Nk(t) − 1 is not. Define F = ∑Nk(t) m=1 Xk,m −Nk(t)µ: We can apply Wald’s identity to F . Since (Sk(t)− (Nk(t)− 1)µ)2 = F − (Xk,Nk(t) − µ) we get\nE [ (Sk(t)− (Nk(t)− 1)µ)2 ] = E [ F 2 ] + E [ (Xk,Nk(t) − µ) 2 ] − 2E [ (Xk,Nk(t) − µ)F ] ≥ E [Nk(t)]Vk − 2E [ (Xk,Nk(t) − µ)F\n] ≥ E [Nk(t)]Vk − 2B(E [ F 2 ] )1/2\n= E [Nk(t)]Vk − 2B √ E [Nk(t)]Vk , (16)\nwhere in the second and last equality we used Lemma 2.\nCombining (15) with (16), we obtain\nE [ (Sk(t)− (Nk(t)− 1)µ)2 ] E [Nk(t)− 1]2 ≥ E [Nk(t)]Vk E [Nk(t)]2 − 2B √ E [Nk(t)]Vk E [Nk(t)]2\n= Vk\nE [Nk(t)] −\n2B √ E [Nk(t)]Vk\nE [Nk(t)]2\n≥ δkVk t+ β − VkE [Dk,1I{Dk,1 > β}] t+ β\n− 2B √ E [Nk(t)]Vk\nE [Nk(t)]2\n≥ δkVk t+ β − VkE [Dk,1I{Dk,1 > β}] t+ β\n− 2Bδk √ Vkδk\nt3/2 .\nIn the next result we give an upper bound on E [ (S(t)− (N(t)− 1)µ)2 ] /E [N(t)− 1]2. Before stating this lemma, let us recall the definitions of (Im)m=1,2,..., (Tk(m))m=0,1,...,1≤k≤K , (Ym)m=1,2,..., (Jm)m=0,1,2,... and (N(t))t≥0: Im ∈ {1, . . . ,K} is the index of the sampler chosen by A for round m; Tk(m) = ∑m s=1 I{Is = k} is the number of samples obtained from sampler k by the end of round m (the empty sum is defined as zero); Ym = XIm,TIm (m) is the mth sample observed by A; J0 = 0 and Jm+1 = ∑m s=1DIs,TIs (s) is the time when A observes the (m + 1)th sample, and so the mth round lasts over the time period [Jm, Jm+1); and N(t) = min {m : Jm ≥ t} is the index of the round at time t (the indexing of rounds starts at one). Thus, N(t) − 1 is the number of samples observed over the time period [0, t]. Note that ∑K k=1 Tk(m) = m for any m ≥ 0 and thus, in particular, ∑K k=1 Tk(N(t)) = N(t). Further, remember that\nSm = ∑m s=1 Ym and S(t) = SN(t)−1. Lemma 5. Let Assumption 5.1 hold and assume that the random variables |Xk,m − µ|k,m are a.s. bounded by some constant B > 0 and that k∗ = arg min1≤k≤KδkVk is unique. For s ≥ 0, let f(s) = maxk 6=k∗ E [Tk(N(s)− 1)] and assume that cf = sups>0 f(s)/s < +∞. Suppose that t ≥ 2δmax and E [N(t)− 1] > 1. Then,\nE [ (S(t)− (N(t)− 1)µ)2 ] E [N(t)− 1]2 ≤ δ ∗V ∗ t + C t3/2 + C ′f(t) t2 (17)\nfor some constants C,C ′ > 0 that depend only on the problem parameters δk, Vk and cf . Furthermore,\nE [N(t)− 1] ≥ t δmax − 1 . (18)\nProof. We proceed similarly to the proof of Lemma 4.\nWe first prove (18). By the definition of N(t) and since Dk,m is nonnegative for all k and m,\nt ≤ JN(t) ≤ N(t)∑ s=1 DIs,TIs (s) = K∑ k=1 Tk(N(t))∑ m=1 Dk,m .\nNotice that (N(t))t≥0, (Tk(n))n=0,1,... are stopping times w.r.t. the filtration (Fm;m ≥ 1), where Fm = σ(I1, J1, . . . , Im, Ym). Therefore, Tk(N(t)) is also a stopping time w.r.t. (Fm). Defining T k(t) = E [Tk(N(t))], Wald’s identity yields\nt ≤ K∑ k=1 E [Tk(N(t))] δk = K∑ k=1 T k(t)δk . (19)\nThen,\nE [N(t)] = K∑ k=1 T k(t) ≥ 1 δmax K∑ k=1 T k(t)δk ≥ t δmax ,\nfinishing the proof of (18).\nNow, let us turn to showing that (17) holds. First, notice that\nS(t)− (N(t)− 1)µ = −(YN(t) − µ) + { SN(t) −N(t)µ } = −(YN(t) − µ) + K∑ k=1 Sk,Tk(N(t)) − Tk(N(t))µ.\nHence, with the same calculation as in the proof of Theorem 1 (the difference is that here N(t) is a stopping time while n is a constant in Theorem 1), we get\nE [ K∑ k=1 { Sk,Tk(N(t)) − Tk(N(t))µ }2] = K∑ k=1 T k(t)Vk\nand thus, introducing F = SN(t) −N(t)µ, Vmax = maxk Vk, and V̄ = ∑ k 6=k∗ Vk,\nE [ (S(t)− (N(t)− 1)µ)2 ] = −2E [ (YN(t) − µ)F ] + E [ (YN(t) − µ)2 ] + K∑ k=1 T k(t)Vk\n≤ 2B(E [ F 2 ] )1/2 + Vmax + K∑ k=1 T k(t)Vk\n≤ 2B √√√√ K∑ k=1 T k(t)Vk + Vmax + K∑ k=1 T k(t)Vk (20)\n≤ 2B √√√√Vmax K∑ k=1 T k(t) + Vmax + K∑ k=1 T k(t)Vk (21) = 2B √ VmaxE [N(t)] + Vmax +\nK∑ k=1 T k(t)Vk . (22)\nIntroduce the notation δ̄ = ∑ k 6=k∗ δk and recall that, by assumption, T k(t) ≤ f(t) for all k 6= k∗. On the other hand,\nT k∗(t) ≤ E [N(t)] holds trivially. Therefore, using (22), (19), and (18), we obtain\nE [ (S(t)− (N(t)− 1)µ)2 ] E [N(t)− 1]2 ≤ (∑ k=1 T k(t)δk ) · {∑K k=1 T k(t)Vk + 2B √ VmaxE [N(t)] + Vmax } tE [N(t)− 1]2\n≤\n( T k∗(t)δ ∗ + f(t)δ̄ ){ T k∗Vk∗ + f(t)V̄ + 2B √ VmaxE [N(t)− 1] + 1 + Vmax } tE [N(t)− 1]2\n≤\n( E [N(t)− 1] δ∗ + δ∗ + f(t)δ̄ ){ E [N(t)− 1]Vk∗ + f(t)V̄ + 2B √ VmaxE [N(t)− 1] + C0 } tE [N(t)− 1]2\n≤ δ ∗V ∗\nt +\nC1 t √ E [N(t)− 1] + C2f(t) + C3 tE [N(t)− 1] + C4f 2(t) + C5f(t) + C6 tE [N(t)− 1]2\n≤ δ ∗V ∗\nt +\nδ 1/2 maxC1\nt √ t− δmax\n+ δmax(C2f(t) + C3) t(t− δmax) + δ2max(C4f 2(t) + C5f(t) + C6) t(t− δmax)2\n≤ δ ∗V ∗\nt +\nC t3/2 + C ′′f(t) t2 + C ′′′f2(t) t3\n≤ δ ∗V ∗\nt +\nC t3/2 + C ′f(t) t2\nwhere C0, C1, C2, C3, C4, C5, C6, C, C ′, C ′′, C ′′′ > 0 are appropriate constants depending on the problem parameters and cf , and in the first to last step we used t ≥ 2δmax and f(t) ≤ cf t in the last step. This finishes the proof of the second part of the lemma.\nCombining the above two lemmas the proof of Theorem 5 is straightforward. We apply the first inequality of Lemma 3 to D̂ = µ̂(t)−µ = S(t)/(N(t)−1)−µ and the second inequality of the lemma to D̂∗ = µ̂k∗(t)−µ = Sk(t)/(Nk∗(t)−1)−µ. It is easy to see that P(N(t) = 1) and P(Nk∗(t) = 1) both decay exponentially fast, so the difference of the first terms is negligible. The difference of the second terms can be handled by Lemmas 4 and 5 and the bounds (15), (18) on E [Nk∗(t)], E [N(t)− 1] (note that the lower bound on E [N(t)− 1] can be simplified to t/(2δmax) using t ≥ 2δmax): introducing λ∗(β) = E [Dk∗,1I{Dk∗,1 > β}] we obtain E [ (S(t)− (N(t)− 1)µ)2\n] E [N(t)− 1]2 ( 1 + 2c√ E [N(t)− 1] )2 − E [ (Sk∗(t)− (Nk∗(t)− 1)µ)2 ] E [Nk∗(t)− 1]2 ( 1− 2c√ E [Nk∗(t)− 1] )\n≤ ( δ∗V ∗\nt +\nC t3/2 + C ′f(t) t2\n)( 1 + 2c √\n2δmax√ t\n)2 − ( δ∗V ∗\nt+ β − 2Bδ\n∗ √ δ∗V ∗\nt3/2 − V\n∗λ∗(β)\nt\n) ( 1− c\n√ 2δ∗√ t\n)\n=\n( δ∗V ∗\nt +\nC t3/2 + C ′f(t) t2\n)( 1 + 2c √\n2δmax√ t + 8c2δmax t\n) − ( δ∗V ∗\nt+ β − 2Bδ\n∗ √ δ∗V ∗\nt3/2 − V\n∗λ∗(β)\nt\n) ( 1− c\n√ 2δ∗√ t\n)\n≤ β δ ∗V ∗\nt(t+ β) + (c+ C ′′)t−3/2 + C ′′′f(t)t−2 + C ′′′′λ∗(β)t−1\n≤ δ∗V ∗ βt−2 + (c+ C ′′)t−3/2 + C ′′′f(t)t−2 + C ′′′′λ∗(β)t−1 ,\nwhere we used that by assumption c < √ t/(2δ∗). Using E [Nk∗(t)− 1] ≥ t/(2δ∗), the last term in the bound of Lemma 3\nfor E [ (µ̂k∗(t)− µ)2 ] becomes C1t−2E [ Nk∗(t) 2 ] P(Nk∗(t) > E [Nk∗(t)] + c √ E [Nk∗(t)− 1]). By E [N(t)− 1] ≥\nt/(2δmax), the indicator in the third line of the bound for E [ (µ̂(t)− µ)2 ] can be bounded as I{E [N(t)− 1] < 4c2} ≤\nI{t < 8δmaxc2} which is zero since by assumption c < √ t/(8δmax). Summing up our bounds and keeping the probability\nterms gives E [ (µ̂(t)− µ)2 ] − E [ (µ̂k∗(t)− µ)2 ] ≤ µ2 {P(N(t) = 1)− P(Nk∗(t) = 1)}\n+ δ∗V ∗ βt−2 + (c+ C ′′)t−3/2 + C ′′′f(t)t−2 + C ′′′′λ∗(β)t−1\n+ C1t −2E [ Nk∗(t) 2 ] P(Nk∗(t) > E [Nk∗(t)] + c √ E [Nk∗(t)− 1])\n+ C2P(N(t) < E [N(t)]− c √ E [N(t)− 1]) ,\nthus, finishing the proof."
    }, {
      "heading" : "D. KL-Based Confidence Bound on Variance",
      "text" : "The following lemma gives a variance estimate based on Kullback-Leibler divergence.\nLemma 6. Let Y1, . . . , Y2n and Q1, . . . , Qn be two independent sequences of independent and identically distributed random variables taking values in [0, 1]. Furthermore, for any t = 1, . . . , n, let\nV̄2t = 1\n2t t∑ s=1 Qt(Y2s − Y2s−1)2. (23)\nThen, for any δ > 0,\nP [ ∪nt=1 { KL ( 2V̄2t, 2E [Q1]V[Y1] ) ≥ δ t }] ≤ 2edδ log nee−δ .\nProof. Equation (5) of Garivier (2013) states that if Z1, . . . , Zn are independent, identically distributed random variables taking values in [0, 1] and Z̄t = 1t ∑t s=1 Zs for t = 1, . . . , n, then for any δ > 0,\nP [ ∪nt=1 { KL ( Z̄t,E [Z1] ) ≥ δ t }] ≤ 2edδ log nee−δ .\nDefining Zt = Qt(Y2t − Y2t−1)2, we see that the above conditions on Zt are satisfied since they are clearly independent and identically distributed for t = 1, . . . , n, and Zt ∈ [0, 1] since 0 ≤ Qt, Y2t−1, Y2t ≤ 1. Now the statement of the lemma follows since E [Zt] = E [ Qt(Y2t − Y2t−1)2 ] = E [Qt]E [ ((Y2t − E [Y2t])− (Y2t−1 − EY2t−1))2 ] = 2E [Q1]V[Y1].\nCorollary 1. Let Y1, . . . , Y2n and Q1, . . . , Qn be two independent sequences of independent and identically distributed random variables taking values in [0, 1]. For any t = 2, . . . , n, let V̄t be defined by (23) if t is even, and let V̄t = V̄t−1 if t is odd. Furthermore, let\nV̄t,min = inf{µ : KL ( 2V̄t, 2µ ) ≤ δ/bt/2c}\nand V̄t,max = sup{µ : KL ( 2V̄t, 2µ ) ≤ δ/bt/2c}.\nThen for any δ > 0, with probability at least 1− 2edδ logbn/2cee−δ ,\nmax 2≤t≤n V̄t,min ≤ E [Q]V[Y ] ≤ min 2≤t≤n V̄t,max ."
    }, {
      "heading" : "E. Details on Synthetic Experiments",
      "text" : "In considering alternative bounded payout distributions we evaluated 3 natural choices, truncated normal, uniform, and scaled-Bernoulli. The latter non-standard distribution is a transformation of a Bernoulli described by three parameters, a midpoint m ∈ R, a scale s ∈ R+, as well as a Bernoulli parameter p. Specifically, if X ∈ {0, 1} is a Bernoulli random variable with parameter p then the transformation m + (X − 0.5)s provides the corresponding scaled-Bernoulli sample. We conducted experiments using these different distributions, keeping their variances the same, in an attempt to ascertain whether the shape of the distribution might affect the performance of the various bandit allocation algorithms. However, we were unable to uncover an instance where the shape of the distribution had a non-negligible effect on performance. As a result, we decided to conduct our experiments using scaled-Bernoulli distributions as they permit the maximum range for the variance in an bounded interval."
    }, {
      "heading" : "F. The Cox-Ingersoll-Ross Model",
      "text" : "In the Cox-Ingersoll-Ross (CIR) model the interest rate at time t, denoted r(t), follows a square-root diffusion given by a stochastic differential equation of the form\ndr(t) = (η − κr(t))dt+ σ √ r(t)dW (t),\nwhere η, κ and σ are fixed, problem-specific, constants andW is a standard one-dimensional Brownian motion. The payoff of an option for this rate at time T (maturity) is given as\nM max(r(T )−K, 0),\nwhere the strike price K and nominee amount M are parameters of the option. The actual quantify of interest, the price of an option at time 0, is given as\nP̄ . = E [ exp(− ∫ T 0 r(t)dt)M max(r(T )−K, 0) ] .\nThe above integrals may approximated with the Monte Carlo method by first discretizing the interest rate trajectory, r(t), into n points r1:n and simulating\nrt = rt−1 + (η − κrt−1) T\nn + σ\n√ rt−1 T\nn εt,\nwhere εt ∼ N (0, 1), and r0 is given. Given a set of N sampled trajectories, r(1:N)1:n , we may let\npi . = exp ( −T n ( r (i) 1 + r (i) n 2 + n∑ t=1 r (i) t )) M max(r(i)n −K, 0),\nand approximate using P̄ ≈ 1N ∑N i=1 pi.\nAn important detail is that the value of the option is exactly zero when the interest rate is below the strike price at maturity. Consequently, when it comes to Monte Carlo simulation, we are less interested in simulating trajectories with lower interest rates. A standard way of exploiting this intuition is an importance sampling variant known as exponential twisting (see (Glasserman, 2003)). Here, instead of sampling the noise εt directly from the target density, N (0, 1), one uses a skewed proposal density, N (θ, 1), defined by a single drift parameter, θ ∈ R. Deriving the importance weights, wi . = exp(− ∑n t=1 ε (i) t − nθ 2 2 ), then, we arrive with the unbiased approximation P̄ ≈ 1 N ∑N i=1 wipi."
    }, {
      "heading" : "G. Bayesian Logistic Regression Example",
      "text" : "We consider a standard Bayesian logistic regression model using a multivariate Gaussian prior. To set up the model, for y ∈ {0, 1}, x ∈ Rd, θ ∈ Rd, let p(y|x, θ) = σ(θ>x)y(1 − σ(θ>x))1−y , where σ denotes the logit function and let p(θ) = N (θ; 0, 0.05I), i.e., the density of d dimensional Gaussian with mean 0 and covariance 0.05I . The labeled training examples (x1, y1), . . . , (xT , yT ) are assumed to be generated as follows: First, θ∗ ∼ p(·) is chosen. The sequence (x1, y1), . . . , (xT , yT ) is assumed to be i.i.d. given θ∗ and the labels are assumed to satisfy\nP(yt = 1|xt, θ∗) = p(yt|xt, θ∗) ,\ni.e., yt ∼ Ber(σ(θ>∗ xt)). The posterior distribution of θ∗ given x1:T and y1:T then follows\npT (θ) . =\n1 Z p0(θ) T∏ t=1 p(yt|xt, θ)\nwhere the normalization constant is evaluated by integrating out θ, that is\nZ = ∫ p0(θ) T∏ t=1 p(yt|xt, θ) dθ.\nIn our demonstrations we consider approximating this integral using Monte Carlo integration by drawing i.i.d. samples over θi for i = 1, ..., N using annealed importance sampling. To do so we first define an annealed target distribution\nfj(θ) = f0(θ) βjfn(θ)\n1−βj\nfor some 1 = β1 > β2 > ... > βn = 0, where fn(θ) = p0(θ), and posterior f0(θ) = pT (θ). The βj values are set using the power of 4 heuristic suggested by (Kuss & Rasmussen, 2005), that is, given a fixed number of annealing steps n we let β1:n = {0, 1n−1 , 2 n−1 , ..., 1} 4.\nAdditionally, AIS requires that we specify a sequence of MCMC transitions to use at each annealing step. For this we found that slice sampling (Neal, 2003) moves were most effective (compared to Metropolis-Hastings, Langevin, and Hamiltonian MCMC (Neal, 2011) moves) in additional to being effectively parameter-free. If we let Ti(x, x′) denote the slice sampling transition operator that meets detailed balance w.r.t. target fj , then the AIS algorithm is given by the following procedure:\n• generate θ(n−1) ∼ fn(θ);\n• generate θ(n−2) from θ(n−1) using Tn−1;\n• ...\n• generate θ(1) from θ(2) using T2;\n• generate θ(0) from θ(1) using T1.\nDefining\nw = fn−1(θ\n(n−1))\nfn(θ(n−1))\nfn−2(θ (n−2))\nfn−1(θ(n−2)) · · · f1(θ\n(1))\nf2(θ(1))\nf0(θ (0) f1(θ(0))\nwe get that Z = E [w]."
    } ],
    "references" : [ {
      "title" : "Analysis of Thompson sampling for the multi-armed bandit problem",
      "author" : [ "S. Agrawal", "N. Goyal" ],
      "venue" : "In Conference on Learning Theory, pp",
      "citeRegEx" : "Agrawal and Goyal,? \\Q2012\\E",
      "shortCiteRegEx" : "Agrawal and Goyal",
      "year" : 2012
    }, {
      "title" : "From bandits to experts: A tale of domination and independence",
      "author" : [ "N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour" ],
      "venue" : null,
      "citeRegEx" : "Alon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive Monte Carlo technique, a variance reduction technique",
      "author" : [ "B. Arouna" ],
      "venue" : "Monte Carlo Methods and Applications,",
      "citeRegEx" : "Arouna,? \\Q2004\\E",
      "shortCiteRegEx" : "Arouna",
      "year" : 2004
    }, {
      "title" : "Regret bounds and minimax policies under partial monitoring",
      "author" : [ "J. Audibert", "S. Bubeck" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Audibert and Bubeck,? \\Q2010\\E",
      "shortCiteRegEx" : "Audibert and Bubeck",
      "year" : 2010
    }, {
      "title" : "Exploration– exploitation tradeoff using variance estimates in multi-armed bandits",
      "author" : [ "J. Audibert", "R. Munos", "Szepesvári", "Cs" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Audibert et al\\.,? \\Q1876\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 1876
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi",
      "year" : 2012
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "arXiv preprint arXiv:1204.5721,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi",
      "year" : 2012
    }, {
      "title" : "Bandits with heavy tail",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimal adaptive policies for sequential allocation problems",
      "author" : [ "A. Burnetas", "M. Katehakis" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Burnetas and Katehakis,? \\Q1996\\E",
      "shortCiteRegEx" : "Burnetas and Katehakis",
      "year" : 1996
    }, {
      "title" : "Kullback-Leibler upper confidence bounds for optimal sequential decision making",
      "author" : [ "O. Cappé", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Cappé et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cappé et al\\.",
      "year" : 2013
    }, {
      "title" : "Finite time analysis of stratified sampling for Monte Carlo",
      "author" : [ "A. Carpentier", "R. Munos" ],
      "venue" : "In NIPS-24,",
      "citeRegEx" : "Carpentier and Munos,? \\Q2011\\E",
      "shortCiteRegEx" : "Carpentier and Munos",
      "year" : 2011
    }, {
      "title" : "Minimax number of strata for online stratified sampling given noisy samples",
      "author" : [ "A. Carpentier", "R. Munos" ],
      "venue" : "In Algorithmic Learning Theory, pp",
      "citeRegEx" : "Carpentier and Munos,? \\Q2012\\E",
      "shortCiteRegEx" : "Carpentier and Munos",
      "year" : 2012
    }, {
      "title" : "Adaptive stratified sampling for Monte-Carlo integration of differentiable functions",
      "author" : [ "A. Carpentier", "R. Munos" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Carpentier and Munos,? \\Q2012\\E",
      "shortCiteRegEx" : "Carpentier and Munos",
      "year" : 2012
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2006
    }, {
      "title" : "An empirical evaluation of Thompson sampling",
      "author" : [ "O. Chapelle", "L. Li" ],
      "venue" : "In NIPS-24,",
      "citeRegEx" : "Chapelle and Li,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle and Li",
      "year" : 2011
    }, {
      "title" : "A theory of the term structure of interest rates",
      "author" : [ "J. Cox", "J. Ingersoll Jr.", "S. Ross" ],
      "venue" : "Econometrica: Journal of the Econometric Society,",
      "citeRegEx" : "Cox et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Cox et al\\.",
      "year" : 1985
    }, {
      "title" : "Sequential Monte Carlo samplers",
      "author" : [ "P. Del Moral", "A. Doucet", "A. Jasra" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Moral et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Moral et al\\.",
      "year" : 2006
    }, {
      "title" : "Minimum variance importance sampling via population Monte Carlo",
      "author" : [ "R. Douc", "A. Guillin", "J. Marin", "C. Robert" ],
      "venue" : "ESAIM: Probability and Statistics,",
      "citeRegEx" : "Douc et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Douc et al\\.",
      "year" : 2007
    }, {
      "title" : "Informational confidence bounds for self-normalized averages and applications",
      "author" : [ "A. Garivier" ],
      "venue" : "IEEE Information Theory Workshop",
      "citeRegEx" : "Garivier,? \\Q2013\\E",
      "shortCiteRegEx" : "Garivier",
      "year" : 2013
    }, {
      "title" : "Probability: a graduate course",
      "author" : [ "A. Gut" ],
      "venue" : null,
      "citeRegEx" : "Gut,? \\Q2005\\E",
      "shortCiteRegEx" : "Gut",
      "year" : 2005
    }, {
      "title" : "Thompson sampling: An asymptotically optimal finite time analysis",
      "author" : [ "E. Kaufmann", "N. Korda", "R. Munos" ],
      "venue" : "In Algorithmic Learning Theory, pp",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2012
    }, {
      "title" : "Assessing approximate inference for binary gaussian process classification",
      "author" : [ "M. Kuss", "C. Rasmussen" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Kuss and Rasmussen,? \\Q2005\\E",
      "shortCiteRegEx" : "Kuss and Rasmussen",
      "year" : 2005
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins",
      "year" : 1985
    }, {
      "title" : "From bandits to experts: On the value of side-observations",
      "author" : [ "S. Mannor", "O. Shamir" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Mannor and Shamir,? \\Q2011\\E",
      "shortCiteRegEx" : "Mannor and Shamir",
      "year" : 2011
    }, {
      "title" : "Simulation studies in optimistic Bayesian sampling in contextual-bandit problems",
      "author" : [ "B. May", "D. Leslie" ],
      "venue" : "Technical report,",
      "citeRegEx" : "May and Leslie,? \\Q2011\\E",
      "shortCiteRegEx" : "May and Leslie",
      "year" : 2011
    }, {
      "title" : "Annealed importance sampling",
      "author" : [ "R. Neal" ],
      "venue" : "Technical report, University of Toronto,",
      "citeRegEx" : "Neal,? \\Q2001\\E",
      "shortCiteRegEx" : "Neal",
      "year" : 2001
    }, {
      "title" : "Estimating ratios of normalizing constants using linked importance sampling",
      "author" : [ "R. Neal" ],
      "venue" : "Technical report, University of Toronto,",
      "citeRegEx" : "Neal,? \\Q2005\\E",
      "shortCiteRegEx" : "Neal",
      "year" : 2005
    }, {
      "title" : "Handbook of Markov Chain Monte Carlo, chapter MCMC using Hamiltonian dynamics, pp. 113–162",
      "author" : [ "R. Neal" ],
      "venue" : null,
      "citeRegEx" : "Neal,? \\Q2011\\E",
      "shortCiteRegEx" : "Neal",
      "year" : 2011
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "Bulletin of the American Mathematical Society,",
      "citeRegEx" : "Robbins,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins",
      "year" : 1952
    }, {
      "title" : "Bayesian computational methods",
      "author" : [ "C. Robert" ],
      "venue" : null,
      "citeRegEx" : "Robert,? \\Q2012\\E",
      "shortCiteRegEx" : "Robert",
      "year" : 2012
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W. Thompson" ],
      "venue" : null,
      "citeRegEx" : "Thompson,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson",
      "year" : 1933
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "In the stochastic MAB problem (Robbins, 1952) the payoff for each action k ∈ {1, 2, .",
      "startOffset" : 30,
      "endOffset" : 45
    }, {
      "referenceID" : 30,
      "context" : "The analysis of the stochastic MAB problem was pioneered by Lai & Robbins (1985) who showed that, when the payoff distributions are defined by a single parameter, the asymptotic regret of any sub-polynomially consistent policy (i.",
      "startOffset" : 66,
      "endOffset" : 81
    }, {
      "referenceID" : 30,
      "context" : "Lai & Robbins (1985) also presented an algorithm based on upper confidence bounds (UCB), which achieves a regret asymptotically matching the lower bound (for certain parametric distributions).",
      "startOffset" : 6,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "Later, Auer et al. (2002a) proposed UCB1 (Algorithm 1), which broadens the practical use of UCB by dropping the Algorithm 1 UCB1 (Auer et al.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "Auer et al. (2002a) proved that, for any finite number of actions n, UCB1’s regret is bounded by",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "A more recent algorithm is KL-UCB (Cappé et al., 2013), where the confidence bound for arm k is based on solving",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 32,
      "context" : "Another approach that has received significant recent interest is Thompson sampling (TS) (Thompson, 1933): a Bayesian method where actions are chosen randomly in proportion to the posterior probability that their mean payoff is optimal.",
      "startOffset" : 89,
      "endOffset" : 105
    }, {
      "referenceID" : 22,
      "context" : "Indeed, the finite time regret of TS under Bernoulli payoff distributions closely matches the lower bound (3) (Kaufmann et al., 2012):",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "For example, Audibert et al. (2009) show that with default parameters, UCB1 and UCB-V will select suboptimal arms with probability Ω(1/n), making tP(N < E[N ]− c √ E[N ]) = Ω(t).",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "In particular, following (Douc et al., 2007; Arouna, 2004), we consider the problem of pricing European call options under the assumption that the interest rate evolves in time according to the Cox-Ingersoll-Ross (CIR) model (Cox et al.",
      "startOffset" : 25,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "In particular, following (Douc et al., 2007; Arouna, 2004), we consider the problem of pricing European call options under the assumption that the interest rate evolves in time according to the Cox-Ingersoll-Ross (CIR) model (Cox et al.",
      "startOffset" : 25,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : ", 2007; Arouna, 2004), we consider the problem of pricing European call options under the assumption that the interest rate evolves in time according to the Cox-Ingersoll-Ross (CIR) model (Cox et al., 1985), a popular model in mathematical finance (details provided in Appendix F).",
      "startOffset" : 188,
      "endOffset" : 206
    }, {
      "referenceID" : 19,
      "context" : "Importantly, the task of adaptively allocating trials between different importance sampling estimators has been previously studied on this problem, using an unrelated technique known as d-kernel Population Monte Carlo (PMC) (Douc et al., 2007).",
      "startOffset" : 224,
      "endOffset" : 243
    }, {
      "referenceID" : 19,
      "context" : "We approximated the option prices under the same parameter settings as (Douc et al., 2007), namely, ν = 0.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 31,
      "context" : "Evaluating such quantities is useful for a variety of purposes, such as Bayesian model comparison and testing/training set evaluation (Robert, 2012).",
      "startOffset" : 134,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : "However, the desired quantities are notoriously difficult to estimate in many important settings, due in no small part to the fact that popular high-dimensional Monte Carlo strategies, such as Markov Chain Monte Carlo (MCMC) methods, cannot be directly applied (Neal, 2005).",
      "startOffset" : 261,
      "endOffset" : 273
    }, {
      "referenceID" : 27,
      "context" : "Nevertheless, a popular approach for approximating such values is annealed importance sampling (AIS) (Neal, 2001) (or more generally sequential Monte Carlo samplers (Del Moral et al.",
      "startOffset" : 101,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "This is the case for example when the samplers use importance sampling, leading to the (new) stochastic variant of the problem known as “bandits with side-observations” (Mannor & Shamir, 2011; Alon et al., 2013).",
      "startOffset" : 169,
      "endOffset" : 211
    } ],
    "year" : 2014,
    "abstractText" : "We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.",
    "creator" : "LaTeX with hyperref package"
  }
}