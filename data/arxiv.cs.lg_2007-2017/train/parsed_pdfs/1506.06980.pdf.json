{
  "name" : "1506.06980.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Strategic Classification",
    "authors" : [ "Moritz Hardt", "Nimrod Megiddo", "Christos Papadimitriou", "Mary Wootters" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We model classification as a sequential game between a player named “Jury” and a player named “Contestant.” Jury designs a classifier, and Contestant receives an input to the classifier drawn from a distribution. Before being classified, Contestant may change his input based on Jury’s classifier. However, Contestant incurs a cost for these changes according to a cost function. Jury’s goal is to achieve high classification accuracy with respect to Contestant’s original input and some underlying target classification function, assuming Contestant plays best response. Contestant’s goal is to achieve a favorable classification outcome while taking into account the cost of achieving it.\nFor a natural class of separable cost functions, and certain generalizations, we obtain computationally efficient learning algorithms which are near optimal, achieving a classification error that is arbitrarily close to the theoretical minimum. Surprisingly, our algorithms are efficient even on concept classes that are computationally hard to learn. For general cost functions, designing an approximately optimal strategy-proof classifier, for inverse-polynomial approximation, is NP-hard.\nar X\niv :1\n50 6.\n06 98\n0v 1\n[ cs\n.L G\n] 2\n3 Ju\nn 20\n15"
    }, {
      "heading" : "1 Introduction",
      "text" : "Studies have found that a student’s success at school is highly correlated with the number of books in the parents’ household [EKST10]. Therefore, in theory, this attribute should be of great value when using machine-learning techniques for student admission. However, this statistical pattern is obviously open to manipulation: books are relatively cheap and, knowing that their number matters, parents can easily buy an attic full of unread books in preparation for admission decisions.\nThis behavior is often called gaming: the strategic use of methods that, while not dishonest or against the rules, give the individual an unintended advantage.1 The problem of gaming is well known and can be seen as a consequence of a classical principle in financial policy making known as Goodhart’s law:\n“If a measure becomes the public’s goal, it is no longer a good measure.”\nGoodhart’s law is highly relevant for the practice of machine learning today. Machine learning relies on the idea that patterns observed in training data translate to accurate predictions about unseen instances of a classification problem. Machine learning is increasingly used to make decisions about individuals in areas such as employment, health, education and commerce. In each such application, an individual may try to achieve a more favorable classification outcome with little effort by exploiting information that may be available about the classifier. Goodhart’s law suggests that if a classifier is exposed to public scrutiny, its prediction accuracy vanishes and it becomes useless. Indeed, concerns of gaming and manipulation are often used as a reason for keeping classification mechanisms secret, which is a major concern in credit scoring (cf. [CP14]). Secrecy is not a robust solution to the problem; information about a classifier may leak, and it is often possible for an outsider to learn such information from classification outcomes. Moreover, transparency is highly desirable and sometimes even mandated by regulation in applications of public interest.\nOur goal in this work is to formalize gaming in classification and to develop approaches and techniques for designing classifiers that are near optimal in the presence of public scrutiny and gaming. The hope is that this analysis may lead, in certain cases, to classifiers with performance comparable to ones that rely on secrecy. In other cases, our analysis may lead to the realization that secrecy is necessary for a good classification performance.\nAs gaming entails strategic behavior, any attempt to formalize it must incorporate the strategic response of an individual to a classifier. We propose a general model for strategic classification, based on a sequential two-player game between a party that wishes to learn a classifier and a party that is being classified. This is different from the standard supervisedlearning setup, which is commonly viewed as a one-shot learning process, in which an algorithm produces a classifier from labeled training examples. Our model combines the statistical elements of learning theory—namely, seeking a small generalization error given a finite number of training data—with a game-theoretic notion of equilibrium. This combination allows us to build classifiers that achieve high classification accuracy at equilibrium, when both parties respond strategically to each other.\nInformal description of our model and results. We model learning and classification as a sequential two-player game. The first player, named “Jury,\" has a learning task: she is given\n1See, for instance, http://www.thefreedictionary.com/gamesmanship.\nlabeled examples from some true classifier h, and must publish a classifier f . The second player, named “Contestant,\" receives an input to the classifier, and is given a chance to “game\" it. That is, Contestant may change his input based on f . However, Contestant incurs a cost for these changes according to a cost function known to both players. Jury’s goal is to achieve high classification accuracy with respect to Contestant’s original input and the true classifier h. Contestant’s goal is to be accepted by Jury, without paying too much to change his input. The cost function plays an important role in our framework as it determines the flexibility of Contestant in changing his input. Ideally, the cost function should capture ground truth or our best approximation thereof.\nOur contributions are the following:\n– For certain cost functions, we give an efficient strategy for Jury which approaches the optimal payoff. Surprisingly, this result holds even for concept classes which are computationally intractable to learn. The intuitive reason is that Contestant’s changes to his input “smooth out” any intractability.\n– Those cost functions for which Jury has near-optimal algorithms include separable cost functions. This is a natural class of cost functions which generalize our introductory example of school admissions and books. We also obtain results for a broad generalization of these separable functions.\n– In contrast, we show that, for general cost functions—even for cost functions which are metrics, another nice class—it is hard to approximate the optimum classification score with reasonable accuracy.\n– We observe through experiments on real data that our approach leads to higher classification accuracy compared with standard classifiers in situations where even a small amount of gaming occurs. We also experimentally demonstrate the robustness of our framework to inaccuracies in our modeling assumptions and the modeling of the cost function."
    }, {
      "heading" : "1.1 Our model",
      "text" : "We first describe an idealized version of the game, where Jury has perfect information. This will serve as a reference point for how well Jury may hope to do. We will later relax this to a version where Jury knows neither h nor D, and only sees labeled examples.\nDefinition 1.1 (Full information game). The players are Jury and Contestant. Fix a population X, and a probability distribution D over X. Fix a cost function c : X ×X → R+ and a target classifier h : X→ {−1,1}.\n1. Jury (who knows the cost function c, the distributionD, and the true classifier h) publishes a classifier f : X→ {−1,1}.\n2. Contestant (who knows c,h,D, and f ), produces a function ∆ : X→ X.\nThe payoff to Jury is Prx∼D {h(x) = f (∆(x))}. The payoff to Contestant is Ex∼D [f (∆(x))− c(x,∆(x))].\nDefinition 1.1 is an example of a Stackelberg competition, which means that the first player (Jury) has the ability to commit to her strategy (a classifier f ) before the second player (Contestant) responds. We wish to find a Stackelberg equilibrium, that is, a highest-payoff strategy\nfor Jury, assuming best response of Contestant; equivalently, a perfect equilibrium in the corresponding strategic-form game.\nNotice that designing the optimum f , given h, D and c, for a finite X, is a conventional combinatorial optimization problem. We seek to label the points in X with ±1 so that the expectation, over D, of h(x) · f (∆(x)) is maximized. Here, ∆(x) is a best move of Contestant, that is,\n∆(x) = argmaxy∈Xf (y)− c(x,y). (1)\nWe note that ∆(x) may not be well-defined, if there are multiple y which attain the maximum. In the following, we assume that Contestant may move to any of them; for simplicity, we do assume that if one of the maximum-attaining y is x itself, then ∆(x) = x. That is, if Contestant is indifferent between moving and not moving, he will default to not moving. We refer to the best payoff for Jury in the above full-information game at the “strategic maximum\" of the game:\nDefinition 1.2 (Strategic Maximum). The strategic maximum in the full-information game is defined as\nOPTh(D, c) = max f : X→{−1,1} Pr x∼D [h(x) = f (∆(x))] ,\nwhere ∆(x) is defined as in (1). Notice that ∆(x) depends on f .\nRemark 1.3. For intuition, notice that if c(x,x) = 0 (that is, it costs nothing for Contestant to stay where he is), then ∆(x) has the following characterization:\n– if f (x) = 1, then ∆(x) = x;\n– if f (x) = −1, let y = argminy∈X : f (y)=1c(x,y); then\n∆(x) = y c(x,y) < 2x c(x,y) > 2. Indeed, since Contestant is best-responding, he only makes a move from input x to point y if c(x,y) is strictly less than 2, which is the payoff he obtains by improving his outcome from “rejected\" to “accepted.\" In this case, the quantity f (∆(x)) in the definition of the strategic maximum becomes\nf (∆(x)) = max y:c(x,y)<2 f (y).\nIn Section 4 we show that, for general cost functions, the strategic maximum is NP-hard to approximate. However, we will also show that for a natural class of cost functions, it is possible to to design a classifier for which Jury’s payoff is arbitrarily close to the strategic maximum, even when Jury has incomplete information. To formalize this, we introduce a second game, which we call the statistical classification game. In this game, Jury does not know the target classifier h for every point in X, but instead is given a few labeled examples from an unknown distribution D. Contestant best-responds to Jury’s published classifier f .\nDefinition 1.4 (Statistical Classification Game). The players are Jury and Contestant. Fix a population X and a probability distribution D over X. Fix a cost function c : X ×X→R+ and a target classifier h : X→ {−1,1}.\n1. Jury (who knows only the cost function c) can request labeled examples of the form (x,h(x)), with x being drawn from D. She publishes a classifier f : X→ {−1,1}.\n2. Contestant (who knows c and f ), produces a function ∆ : X→ X.\nThe payoff to Jury is Prx∼D {h(x) = f (∆(x))}. The payoff to Contestant is Ex∼D[f (∆(x))−c(x,∆(x))]."
    }, {
      "heading" : "1.2 Strategy-robust learning",
      "text" : "A learning algorithm in our setting has to accomplish two goals. First, it needs to learn the unknown target classifier from labeled examples. Second, it needs to achieve high payoff for Jury in the statistical classification game, by anticipating Contestant’s best response. Below, we give two definitions of stategy-robust learning which combine these goals; the second is a stronger requirement than the first. In our first definition, we fix an unknown target classifier h, and demand an algorithm which, with high probability over the samples, returns a classifier f guaranteeing a near-optimal payoff to Jury in the statistical classification game. In our second definition, we present a uniform notion: the learning algorithm must, with high probability, return a classifier that is guaranteed to work on any target classifier h in some concept class H.\nDefinition 1.5 (Strategy-robust learning). Let C be a class of cost functions. We say that an algorithm A is a strategy-robust learning algorithm for C if the condition that follows holds. For all distributions D, for all classifiers h, all c ∈ C and for all ε and δ, given a description of c and access to labeled examples of the form (x,h(x)), where x ∼ D, A produces a classifier f : X→ {−1,1} so that, with probability at least 1− δ over the samples,\nPr x∼D [h(x) = f (∆(x))] >OPTh(D, c)− ε. (2)\nwhere ∆(x) is defined as in (1).\nOne might expect, in line with PAC-learning [Val84], that Definition 1.5 might restrict h to be in some concept class H. However, we will show that for a natural class C of cost functions, in fact it is possible to achieve strategy-robust learning with no dependence on h!\nHowever, we may want to ask a bit more. Suppose that Jury builds a classifier for some property, and later wants to re-use the data to build a classifier for a slightly different property. For example, returning to the scenario from the introduction, suppose that the school admissions board collects data on students and tries to predict academic success. Later, the board is charged with recruiting to maximize the quality of the basketball team; they would like to use the same dataset to predict who will be a good student-athlete. Later still, suppose that the this data set is made public, and many other schools try to use it to predict many things. If enough different classifiers are trained on this data, the guarantee of Definition 1.5 starts to degrade. A strategy-robust learning algorithm should succeed with high probability on a single classifier, but there are no guarantees (beyond what the union bound gives) if it is used repeatedly. This situation motivates the following definition.\nDefinition 1.6 (Uniform strategy-robust learning). Let H be a concept class and C be a class of cost functions. We say that an algorithm A is a uniform strategy-robust learning algorithm for (H,C) if the condition that follows holds. For all distributions D, for all c ∈ C and for all ε and δ, with probability at least 1−δ over draws x ∼ D, the following holds simultaneously for all h ∈ H.\nGiven a description of c and access to labels (x,h(x)), A produces a classifier f : X→ {−1,1} so that\nPr x∼D [h(x) = f (∆(x))] >OPTh(D, c)− ε, (3)\nwhere ∆(x) is defined as in (1).\nWe will typically specify the number of labeled examples that the algorithm requires as a function of ε, δ and a parameter that depends on the domain size (e.g., the number of features)."
    }, {
      "heading" : "1.3 Our contributions",
      "text" : "Our main result is a strategy-robust learning algorithm, which comes with both uniform and non-uniform guarantees. Our algorithm is computationally efficient when the cost function comes from a broad class of functions that we call separable. In the non-uniform case, the target classifier h can be anything. In the uniform case, the algorithm is efficient as long as the concept class H is statistically learnable, but it notably does not require that H be efficiently learnable.\nSeparable cost functions are functions of the form c(x,y) = max{0, c2(y)− c1(x)}, where c1 and c2 are arbitrary functions, mapping the domain X into the real numbers. We take the maximum with 0 to obtain a nonnegative cost function. We will later see and discuss a number of natural examples of separable cost functions.\nOur main theorem, and our stronger result, is about uniform strategy-robust learning.\nTheorem 1.7 (Informal). LetH be a concept class that is learnable fromm examples up to error ε and confidence 1− δ, and let S be the class of separable cost functions. Then, there is a uniform strategyrobust learning algorithm for (H,S) with running time and sample complexity poly(m,1/ε, log(1/δ)).\nIn fact, (the formal statement of) this theorem implies a non-uniform result:\nTheorem 1.8 (Informal). Let S be the class of separable cost functions. There is a non-uniform strategy-robust learning algorithm for S with polynomial running time and sample complexity.\nOur main theorem (and the non-uniform corollary) can be extended to a more general class of cost functions, which are obtained by taking the minimum of k separable cost functions. We state only the uniform version here, the non-uniform version follows similarly.\nTheorem 1.9 (Informal). LetH be a concept class that is learnable fromm examples up to error ε and confidence 1−δ, and let S (k) be the class of minima of k separable cost functions. Then, there is a uniform strategy-robust learning algorithm for (H,S (k)) with sample complexity poly(m,k,1/ε, log(1/δ)) and running time poly(m,exp(k),1/ε, log(1/δ)).\nTheorem 1.9 applies to a broad class of cost functions: it is not hard to see that any cost function on a finite domain X can be written as a minimum of separable cost functions. Of course, the sample complexity in Theorem 1.9 depends on k, the number of cost functions involved. For general cost functions, k grows with |X | and might be quite large. However, many spaces admit a more efficient representation—for instance, if the cost function defines a metric that admits a small ε-net, k depends only on the size of the net. Thus, k is a parameter that interpolates nicely between tractable cases where k is small and the general case where k is unrestricted.\nThe fact that the sample complexity in Theorem 1.9 might be large is unavoidable: for general cost functions, we have the following negative result.\nTheorem 1.10 (Informal). There is a class of metrics S such that, unless P = NP, there is no efficient strategy-robust learning algorithm for S that achieves expected payoff within ε = 1/ |X |η of the optimum, for any constant η > 0.\nRecall that a distance function is a metric if it is non-negative, symmetric, and satisfies the triangle inequality. This result is an immediate corollary of the fact (which we will prove in Section 4) that approximating the strategic maximum for metrics is NP-complete."
    }, {
      "heading" : "1.3.1 Experimental evaluation",
      "text" : "We experimentally evaluate our framework on real data from a Brazilian social network called Apontador. The data set deals with instances of review spam and was recently studied in the context of spam fighting [CdCMBB14]. Classification of spammers is a natural setting for our methods, because spammers will of course try to game any automated attempt to identify them. We model a cost function that roughly reflects the loss in revenue that a spammer experiences when changing certain attributes. For instance, when a spam message contains a URL pointing to malware, it is costly for the spammer to remove this URL from his message as his message loses its intended purpose. Acknowledging that the modeling of a cost function can never be perfectly realistic, we evaluate our approach while explicitly taking into account several types of modeling inaccuracies. Specifically, we only assume that our cost function is roughly correct and that the amount of gaming is possibly below or above the threshold predicted by our theoretical framework. Our empirical observations demonstrate that even in the presence of significant modeling errors and only a small amount of gaming, our algorithm already outperforms a standard SVM classifier. Complementing our robustness analysis, we explore an approach for creating hybrid classifiers that interpolate between our classifier and standard classifiers that aren’t by themselves strategy-robust. We observe that such hybrids often achieve an excellent trade-off between resilience to gaming and classification accuracy."
    }, {
      "heading" : "1.4 Related work",
      "text" : "The deterioration of prediction accuracy due to unforeseen events is often described as concept drift and arises in a number of contexts. A sequence of works on adversarial learning is motivated by the question of learning in the presence of an adversary that tampers with the examples of a learning algorithm. Typical application examples in this line of work include intrusion detection and spam fighting. Early works considered zero-sum games [DDM+04] which are not very applicable to our problem as there are almost always cases where the payoff should be high for both players (e.g, a good student being admitted to a good college). More recent work considers alternative game-theoretic notions [BS09, BS11, BKS12, GSBS13]. The most closely related is the work by Brückner and Scheffer [BS11], which considered a Stackelberg competition for adversarial learning. A notable difference with our setup is that they define the equilibrium with respect to the sample, while we define it with respect to the underlying distribution. Our definition requires us to provide generalization bounds. Beyond this difference, Brückner and Scheffer focus on learning centered linear classifiers when the Euclidean squared norm is the cost function. The Euclidean norm is not separable and so our results are incomparable. Stackelberg competitions have also been studied extensively in the context of security games [KYK+11, KCP10]."
    }, {
      "heading" : "2 Separable cost functions",
      "text" : "We begin by studying the class of separable cost functions, which arise naturally in the context of gaming. To motivate the definition, recall the example of the school board which wants to exploit the correlation between parents’ books and students’ performance. In this (admittedly rather stylized) example, the cost to Contestant from moving from a household x ∈ X with 50 books to a household y ∈ X with 100 books is simply the cost of the the additional books.\nMore generally, this logic applies to any situation where Contestant can assign a cost to each state x ∈ X, independently of how it was reached. If the cost of a state x is g(x), then the cost to Contestant of moving from x to y is simply any additional cost: c(x,y) = max {0, g(y)− g(x)}. For example, suppose that Jury is designing a spam filter, and Contestant wishes to send an email. Independently of the spam filter, Contestant wants his message to serve a purpose such as advertising or distributing malware. We can assign a score g(x) to each message in x ∈ X that expresses how much utility the spammer experiences when this message is delivered without being classified as spam. For example, a message is significantly less useful for the spammer after the URL pointing to malware has been removed. The expression max{0, g(y)− g(x)} then captures the loss in utility (or expected revenue) when moving from x to y. We will return to this example in detail in our experimental evaluation in Section 5.\nWith these examples in mind, we define a separable cost function as follows.\nDefinition 2.1. A cost function c(x,y) is called separable if it can be written as\nc(x,y) = max {0, c2(y)− c1(x)} ,\nfor functions c1, c2 : X→R satsifying c1(X) ⊂ c2(X).\nAbove, the term “separable\" is a slight abuse of terminology, because the cost function cannot be negative, and because of the assumption about c1(X) ⊂ c2(X); a truly “separable\" function would be of the form c2(y)−c1(x), for arbitrary c1, c2. However, we will stick with it for simplicity of exposition. The two extra conditions are natural for cost functions. The maximum with 0 ensures that the cost function is non-negative. The condition c1(X) ⊂ c2(X) means that there is always a 0-cost option (that is, Contestant can opt not to game, and can pay nothing).\nAnother important special case of a separable cost functions are linear cost functions of the form\nc(x,y) = 〈 α, (y − x) 〉 + ,\nfor α ∈ Rn. With this cost function, each attribute can be increased independently at some linear cost, and can be decreased for free. For our arguments that follow, a linear cost function is helpful for intuition.\nOur main result is that for separable cost functions, there is a nearly optimal algorithm for Jury, with a uniform guarantee. The sample complexity and running time of this algorithm depend on the Rademacher complexity of the class H of classifiers.\nDefinition 2.2. For a class F of functions f : X → R, the Rademacher complexity of F with sample size m is defined as\nRm(F ) := Ex1,...,xm∼DEσ1,...,σm\n[ sup { 1 m m∑ i=1 σif (xi) : f ∈ F }] ,\nwhere σ1, . . . ,σm are i.i.d. Rademacher random variables.\nOur algorithm, given below as Algorithm 1, has the following uniform guarantee.\nTheorem 2.3. Suppose the cost function c is separable, i.e., c(x,y) = max{0, c2(y) − c1(x)} and c1(X) ⊆ c2(X) . Let H be a concept class. Let m denote the number of samples in Algorithm 1, and suppose\nRm(H) + 2 √ ln(m+1) m + √ ln(2/δ) 8m 6 ε 8 .\nUnder these conditions, with probability at least 1−δ, (3) holds for all h ∈ H and for all distributionsD. In particular, Algorithm 1 is a uniform strategy-robust learning algorithm for H and is efficient whenever the concept class H is statistically learnable, i.e., Rm(H) decays inverse polynomially with m.\nIt is worth pointing out that Algorithm 1 is computationally efficient as long as H has low sample complexity—even if H itself is not computationally efficiently learnable! As we mentioned above, the proof of Theorem 2.3 also implies that our algorithm satisfies the following non-uniform guarantee.\nCorollary 2.4. Suppose the cost function c is separable. Let m denote the number of samples in Algorithm 1, and suppose that 2 √\nln(m+1) m +\n√ ln(2/δ)\n8m 6 ε 8 .\nThen with probability at least 1− δ, (3) holds for all distributions D. In particular, Algorithm 1 is an efficient (non-uniform) strategy-robust learning algorithm.\nCorollary 2.4 follows from Theorem 2.3 by setting H = {h}, the singleton containing the fixed target classifier h. Indeed, in this case Rm(H) = 0.\nBefore proving Theorem 2.3, we state the algorithm and discuss the intuition behind it. In Figure 1, we illustrate the idea for a linear cost function, c(x,y) = 〈 α,y − x 〉 +. Because moving perpendicularly to α is free for Contestant, Jury may as well choose a classifier f that accepts some affine halfspace whose normal is equal to α (see Figure 1). Thus, the only issue is finding the correct shift for this halfspace. Because the calculated shift can only be based on samples, we choose the shift that is empirically the best. The latter can be calculated quickly because it is a one-dimensional problem.\nFor a more general separable cost function\nc(x,y) = max{0, c2(y)− c1(x)} ,\nby the same argument, Jury may as well return a classifier c2[t] of the form:\nc2[t](x) =  1 if c2(x) > t−1 if c2(x) < t (x ∈ X) for some t. Algorithm 1 gives the details, and we proceed with the proof below.\nRemark 2.5 (Input to Algorithm 1). Algorithm 1 takes a cost function c(x,y) = max{0, c2(y)− c1(x)} as an input, and it returns some threshold function based on c2. We have been a little sloppy about how exactly c should be represented. A quick inspection of the algorithm shows that in order to compute the threshold, A needs only black-box access to c1, and enough access to c2 to determine c2(X)∩ [ti , ti + 2]. In order to return the classifier f , A additionally needs\nα f\nf ′\ny\nx\nx′\nwhatever access to c2 it is expected to return. For example, if we only ask that A be able to provide black-box access to f , then black-box access to c2 suffices for this step. If we ask that A return a short description of f , then a short description of c2 suffices for this step.\nProof of Theorem 2.3. Assume for simplicity that the cost function satisfies c(x,y) , 2, for all x,y ∈ X. First, for any mapping f : X→ {−1,1}, define\nΓ (f ) := { x : max{f (y) : c(x,y) < 2} = 1 } = { x : (∃y ∈ X)(f (y) = 1, c(x,y) < 2)\n} = { x : c1(x) >min{c2(y) : f (y) = 1} − 2 } .\nClaim 2.6. Γ (f ) is the set of x ∈ X such that f (∆(x)) = 1 when ∆ is a best response of Contestant.\nProof. Indeed, for x ∈ Γ (f ), there exists some y such that f (y) = 1, so that the payoff to Contestant when he plays ∆(x) = y is equal to 1 − c(x,y) > −1. On the other hand, suppose that Contestant plays ∆(z) ∈ X for some with f (z) , 1. Then the best payoff of Contestant is equal to −1− c(x,z) 6 −1, because c(x,z) > 0. So, the best response of Contestant is to choose ∆(x) = y for some y with f (y) = 1. This establishes that Γ (f ) ⊆ {x ∈ X : f (∆(x)) = 1}.\nFor the other direction, suppose that f (∆(x)) = 1. Then there is some y ∈ X so that\n1− c(x,y) > −1−min z∈X c(x,z) = −1,\nusing from the definition of separability that c1(X) ⊆ c2(X), and hence for all x,\nmin z∈X c(x,z) = min z∈X max {0, c2(z)− c1(x)} = 0.\nIn particular, c(x,y) < 2, and so x ∈ Γ (x). This establishes that\n{x ∈ X : f (∆(x)) = 1} ⊆ Γ (f ),\nand proves the claim.\nClaim 2.6 is the only place in the proof where we need either of the extra conditions in Definition 2.1 (that c(x,y) > 0 and c1(X) ⊆ c2(X)).\nGiven this characterization of Γ (f ), we next argue that we may replace f by a much more structured function f ′ so that Γ (f ) = Γ (f ′); in particular, the payoff to Jury under f will be the same as under f ′, and so we can restrict our attention to these more structured functions. For any f , let\nf ′(y) =  1 if c2(y) >min{c2(z) : f (z) = 1}−1 otherwise . (4) Then we have\nΓ (f ) = {x : c1(x) >min{c2(y) : f (y) = 1 } − 2 } = { x : c1(x) >min{c2(y) : f ′(y) = 1 } − 2 } = Γ (f ′) .\nIn particular, for any true classifier h ∈ H, the payoff to Jury if she plays f is the same as if she plays f ′:\nP {h(x) = max{f (y) : c(x,y) < 2 } } = P { x ∈ (Γ (f )4{y : h(y) = 1})c } = P { x ∈ (Γ (f ′)4{y : h(y) = 1})c\n} = P { h(x) = max{f ′(y) : c(x,y) < 2 } } .\nAbove, 4 denotes symmetric difference. Thus, it suffices to consider classifiers of the form of (4). That is, our classifier may as well be equal to c2[s], for some s ∈ c2(X)∪ {∞}, where s plays the role of min{c2(z) : f (z) = 1 }, and s =∞means that there is no z such that f (z) = 1. Let\nS := c2(X)∪ {∞}\nbe the set of these relevant values of s. Recall the definition of si from Algorithm 1. For s ∈ S, we have2\nΓ (c2[s]) = {x : c1(x) > s − 2 } .\nThe best possible payoff to Jury is obtained by finding the best threshold s, i.e.,\nOPTh(D, c) = 1− inf{err(s) : s ∈ S } ,\nwhere err(s) := P {h(x) , c1[s − 2](x)} . In Algorithm 1, Jury returns f = c2[si∗], and as above the payoff to Jury from this f is equal to\nP {h(x) , c1[si∗ − 2](x)} = 1− err(si∗) .\nThus, to prove Theorem 2.3, it suffices to show that for all h ∈ H,\nerr(si∗) 6 inf{err(s) : s ∈ S }+ ε . (5)\nTo establish this, we first observe that there is no loss of generality in Algorithm 1 by considering only the si , i = 1, . . . ,m+ 1, where as in Algorithm 1 we set sm+1 =∞.\nClaim 2.7.\nêrr(si∗) = min{ êrr(si) : i = 1, . . . ,m+ 1 } = inf{ êrr(s) : s ∈ S } .\nProof. The first equality is just the definition of i∗. The second equality follows from the fact that\nêrr(s) = 1m m∑ j=1 1 { h(xj ) , c1[s − 2](xj ) } only changes when c1[s − 2](xj) changes for some j. Thus, by construction, this sum takes on every possible value (as s ranges over S = c2(X)∩ {∞}) at the points si , i = 1, . . . ,m+ 1.\n2 As usual,∞− 2 =∞.\nClaim 2.8. With probability at least 1− δ, for all h ∈ H and for all s ∈ S, |êrr(s)− err(s)| 6 4Rm(H) + 8 √ ln(m+1) m + √ 2ln(2/δ) m ."
    }, {
      "heading" : "In particular, under the conditions of Theorem 2.3, with probability at least 1− δ,",
      "text" : "sup { |êrr(s)− err(s) | : h ∈ H, s ∈ S } 6 ε/2 .\nProof. Writing out the definition of êrr and err, we need to bound the absolute value of the difference\nêrr(s)− err(s) = 1m m∑ j=1 1 { h(xj ) , c1[s − 2](xj ) } −Ex∼D [1 {h(x) , c1[s − 2](x)}]\nsimultaneously for all h ∈ H, s ∈ S. By standard arguments (see, for example, Theorem 3.2 in [BBL05]), for all h ∈ C, s ∈ S,\n| êrr(s)− err(s) | 6 2Rm(X ) + √ 2ln(2/δ) m , (6)\nwhere X = {h · c1[s − 2] : h ∈ H, s ∈ S } .\nThus, it suffices to control the Rademacher complexity of X , which is in turn controlled by\nRm(X ) 6 2(Rm(H) +Rm(Y )) , (7)\nwhere Y = {c1[s − 2] : s ∈ S}. Note that, because all the functions in H∪Y are ±1-valued,\nh(x) · c1[s − 2](x) = |h(x) + c1[s − 2](x) | − 1\nfor every x. Inequality (7) follows from a contraction principle (see, e.g., Theorem 4.2 in [LT91]) and the definition of the Rademacher complexity.\nIt remains to bound Rm(Y ). Fix x1, . . . ,xm ∈ X and sign flips σi ∈ {−1,1}. As in the proof of Claim 2.7, all of the values that ∑m i=1σic1[s − 2](xi) takes on as s ranges over S are attained at {s1, . . . , sm+1}. Thus, for fixed x1, . . . ,xm ∈ X, using a Chernoff bound and the union bound, and integrating to bound the expectation, we obtain\nEσ [ sup { 1 m m∑ i=1 σic1[s − 2](xi) : s ∈ S }]\n= Eσ [ sup { 1 m m∑ i=1 σic1[sj − 2](xi) : j = 1, . . . ,m+ 1 }]\n6 2 √\nln(m+1) m .\nThus, we have\nRm(Y ) 6 2 √ ln(m+1) m ,\nand altogether inequality (6) implies that for all h ∈ H and s ∈ S, | êrr(s)− err(s) | 6 4 ( Rm(H) + 2 √ ln(m+1) m ) + √ 2ln(2/δ) m ,\nwhich completes the proof of the claim.\nClaims 2.7 and 2.8 establish Theorem 2.3. Indeed, we have, with probability at least 1− δ, for all h ∈ C,\nerr(si∗) 6 êrr(si∗) + ε/2\n= inf {êrr(s) : s ∈ S}+ ε/2 6 inf {err(s) : s ∈ S}+ ε ,\nestablishing inequality (5) and completing the proof."
    }, {
      "heading" : "3 General cost functions",
      "text" : "While separable cost functions are quite reasonable, they do not capture everything. In this section, we consider more general cost functions. We extend Algorithm 1 to work for a cost function that is the minimum of an arbitrary set of separable cost functions. This is a much broader class. In fact, every cost function can be represented as the minimum of separable cost functions, although not necessarily very parsimoniously.\nProposition 3.1. Let X be any finite set and let c : X ×X→R be any mapping. Suppose\nD >max{c(x,y) : x,y ∈ X} .\nUnder these conditions,\nc(x,y) = min {c(w,z) +D · 1 {x , w}+D · 1 {y , z} : w,z ∈ X} .\nSince each of the cost functions cw,z(x,y) = c(w,z) +D · 1 {x , w}+D · 1 {y , z} is a separable cost function, Proposition 3.1 implies that any c can be written as the minimum of |X |2 cost functions. The sample complexity of our extension depends on the number of cost functions; since |X |may be quite large (possibly exponential in the parameter of interest), Proposition 3.1 might not help. However, a smaller number of cost functions can be used ifX has nice geometric structure.\nProposition 3.2. Let X be any finite set and let c : X ×X → R be a metric. Let S be an ε-net of X: that is, for every x ∈ X, there is some s ∈ S so that c(x,s) 6 ε. Under these conditions, for every x,y ∈ X,\nc(x,y) 6min {c(x,w) + c(w,z) + c(z,y) : w,z ∈ S} 6 c(x,y) + 4ε .\nThus, when c is a metric, our problem is very close to a problem where the cost function is the minimum of separable cost functions, and the number of cost functions we need to consider depends essentially on the covering number of the metric space (X,c).\nAlgorithm 2 is an adaptation of Algorithm 1 for cost functions of the form\nc(x,y) = min{b(x,y) : b ∈ B} ,\nwhere each function b ∈ B is separable, i.e.,\nb(x,y) = max{0,b2(y)− b1(x)} .\nAlgorithm 2: A: gaming-robust classification algorithm for minima of separable cost functions 1 Inputs: Labeled examples (x1,h(x1)), . . . , (xm,h(xm)) from xi ∼ D i.i.d.. Also, a description\nof k separable cost functions b(x,y) = max{0,b2(y)− b1(x)} for b ∈ B. 2 For i = 1, . . . ,m and b ∈ B, set\nti,b = b1(xi)\nsi,b = max{b2(X)∩ [ti,b, ti,b + 2]} if b2(X)∩ [ti,b, ti,b + 2] , ∅∞ if b2(X)∩ [ti,b, ti,b + 2] = ∅ and set sm+1,b =∞ for all b ∈ B.\n3 For each s ∈ ⊕ b∈B { si,b : i = 1, . . . ,m+ 1 } , compute\nêrr(s) := 1m m∑ j=1 1 { h(xj ) ,min{b1[sb − 2](xj ) : b ∈ B} } .\n4 Find a s∗ that minimizes êrr(s). 5 Return: f (x) = min{b2[s∗b](x) : b ∈ B}.\nTheorem 3.3. Suppose the cost function c is the minimum of separable functions,\nc(x,y) = min{b(x,y) : b ∈ B} ,\nwhere each b : X ×X→R is separable. Suppose that Algorithm 2 uses m samples, so that m satisfies\nRm(H) + 2 √ |B| ln(m+1) m + √ ln(2/δ) 8m 6 ε 8 .\nUnder these conditions, with probability at least 1−δ, (3) holds for all h ∈ H and for all distributions D. The running time of Algorithm 2 is O(m|B|).\nThe intuition for Algorithm 2 is similar to that for Algorithm 1, and is illustrated in Figure 2 for the minimum of two linear cost functions. The proof of Theorem 3.3 is also similar to that of Theorem 2.3; for completeness, we give it in Appendix A.\nRemark 3.4 (Improvements for structured classes B). When the size of B is small, Theorem 3.3 gives a nice bound. However, if B is large (as in our extreme example of the beginning of this section), these guarantees are not so good. An inspection of the proof (in Appendix A) shows that the term√ |B| ln(m+1)\nm may be replaced by Rm(H), where\nH = minb∈B b1[sb − 2] : s ∈⊕ b∈B (b2(X)∪ {∞})  . For some sets B of separable cost functions, this may be much smaller."
    }, {
      "heading" : "4 NP-completeness",
      "text" : "What happens when the cost function c is not separable? It turns out that for general cost functions, any algorithm for Jury requires more than polynomial time to obtain a near-optimum classifier, unless P =NP . This holds true\n(a) even if the underlying distance function is a metric (another very natural class of cost function), and\n(b) even if the learning algorithm were given correct labels h(x) for all members x ∈ X of the population,\nwhen the desired deviation ε is inverse-polynomially small and the distribution D is uniform. The above statements are consequences of the following result:\nTheorem 4.1. Given a finite population X with the uniform distribution, a metric c on X, and a target labeling h : X 7→ {−1,+1}, it is NP-hard to compute the strategic optimum within ε = 1|X |η for any constant η > 0.\nProof of Theorem 4.1. We will reduce from 3Sat. Suppose we are given a 3Sat Boolean formula with n variables x1, . . . ,xn and m clauses C1, . . . ,Cm, where Ci has three literal occurrences Li1,Li2,Li3. We now construct our instance of Strategic Optimum as follows. We need to specify X, h, and c. We begin by constructing a weighted population Y , which will consist of points y and positive integer weights w(y) for each y ∈ Y . Our population X will simply consist of w(y) identical copies of each y ∈ Y . Thus, |X | = ∑ y∈Y w(y). We will also specify labels h(y) for each y ∈ Y , which the points x ∈ X will inherit. Fix a number K (polynomial in m) to be chosen later. Our weighted population Y consists of:\n– 3m points Lik for 1 6 i 6m and k ∈ {1,2,3}, corresponding to the literal occurrences in the clauses. These points each have weight w(Lik) = K(m− 1− 1m ) and label h(Lik) = −1.\n– (m 2 )\npoints Pij for 1 6 i < j 6m corresponding to unordered pairs {Ci ,Cj} of clauses. These points each have weight w(Pij ) = 2K and label h(Pij ) = +1.\n– 9 · (m 2 )\npoints Qikj`, for 1 6 i < j 6m, and for k,` ∈ {1,2,3} so that Lik is not the negation of Lj`. These points correspond to unordered pairs of literal occurrences {Lik ,Lj`} of literal occurrences in different clauses which are not contradictory. They have weight w(Qijk`) = 1 and label h(Qijk`) = −1. (Actually, their label does not matter).\n– One other point R with a huge weight w(R) = KM, for a very large value M, and label h(R) = −1. Choose M = 2 (m 2 ) .\nWe next define a metric c : X ×X→ R+. It will take only two nonzero values, 1.5 and 2.5. Notice that this guarantees c satisfies the triangle inequality. We will choose c so that c(x,x) = 0 and c(x,y) = c(y,x), and so c will indeed be a metric. To describe c, it suffices to describe the points which are “close,\" that is, which have distance 1.5. Further, it suffices to define c for points in Y , and we will extend it to X in a natural way: for points x,x′ ∈ X, if they come from the same y ∈ Y , they will have distance 1.5; if x,x′ come from y , y′ respectively, then c(x,x′) = c(y,y′). The close pairs of points in Y are:\n– All pairs of the form {Pij ,Qijkl};\n– All pairs of the form {Pij ,R};\n– All pairs of the form {Qijk`,Lik} or {Qijk`,Lj`}.\nClaim 4.2. If the given formula is unsatisfiable, the number of points labeled +1 by the Jury’s optimum f is equal to\nb = K ( M + 3m ( m− 1− 1\nm\n)) + 9 ( m 2 ) ,\nwhich we call the baseline payoff. Otherwise, if the given formula is satisfiable, then there is a labeling f of the points with payoff at least b+K − 9(m2). Proof. In the following, we will consider a graph with vertices Y . Two vertices x,y are neighbors in this graph if c(x,y) = 1.5. Let Γ (x) denote the neighbors of x in this graph. Thus, the bestresponse ∆ to a classifier f is\n∆(x) =  x f (x) = 1 x f (x) = −1 and f (y) = −1∀y ∈ Γ (x) y f (x) = −1 and f (y) = 1, y ∈ Γ (x) ,\nwhere above if y in the last case is not uniquely defined Contestant can pick any such y. First observe that the baseline payoff is obtained by the classifier f (x) = −1 for all x ∈ X, and so it is certainly acheivable. We now argue that Jury can do better if and only if the original formula was satisfiable. We make a few observations about Jury’s optimal classifier f .\n– First, because of our choice of M, we must have f (Pij ) = −1 for all i, j. Indeed, our choice implies that KM > |X | − KM; thus, if f (Pij) = 1 for some i, j, then Contestant will set ∆(R) = Pij , and Jury will mis-classify the point R, and get a payoff worse than the baseline.\n– Next, f (Lik) = −1 for all i,k. Indeed, since h(Lik) = −1 and h(x) = −1 for all of the (Q-type) neighbors of Lik , there can be no benefit to Jury for making f (Lik) = +1.\n– For each Pij , at most one Q-point Qikj` in Γ (Pij) has f (Qikj`) = +1. Indeed, each Q-point is connected to exactly one Pij , and once one of them is accepted by Jury, she can gain nothing by accepting additional points of Γ (Pij ).\nThus, the optimal f only assigns positive weights to Q points, and it does so to at most one Q-point in each Γ (Pij ). Suppose that f (x) = +1 for the set A of Q-points, and let B = ΓL(A) be the set of L-points adjacent to A. Now, the size of B can vary based on how the literals overlap with the clauses. It satisfies\n2|A| m− 1 6 |B| 6 2|A|,\nwhere the lower end is attained when there are complete collisions, and the upper end is attained when there are no collisions. Now consider the number of points of X that Jury classifies correctly under such an f . It is\nK ( M + (3m− |B|) ( m− 1− 1\nm\n) + 2|A| ) + ( 9 ( m 2 ) − |A| ) = b+ δ,\nwhere δ = K ( |B| ( m− 1− 1\nm\n) + 2|A| ) − |A|.\nConsider this first term, which is multiplied by K . This is only positive when |B| = 2|A|m−1 is as small as it can possibly be, which happens only if |A| = (m 2 )\nand |B| =m. In this case, the first term is equal to K , and we have δ = K − |A| > K −9 (m 2 ) . But this happens if and only if we can choose m different literals Lik, one from each clause, so that no pair of them contradict each other; that is, if and only if the original formula was satisfiable.\nNow the theorem follows quickly from the claim. We choose K to be a large polynomial in m, say m2/η for some small constant η. Thus, |X | is on the order of m2/η+2. Suppose there is a polynomial-time algorithm which approximates the strategic optimum up to ε. Claim 4.2 implies a contradiction for any\nε < K − 9\n(m 2 )\n|X | =\nK − 9 (m 2 )\nK ( 3m+ (m 2 ) +M ) + 9 (m 2 ) .\nUsing our choice of K , for sufficiently large m the right hand side is at least |X |−η . Thus, we have a contradiction whenever ε < |X |−η .\nThe metric constructed in the proof has “separability dimension” (the smallest number of separable functions needed to achieve it as a minimum) that grows linearly with the population. The same dimension appears in the exponent of the running time of the algorithm of the previous section. It is an interesting open problem to determine whether this exponential dependence is inherent; the other possibility is that the problem is fixed-parameter tractable with respect to the “separability dimension” parameter. We suspect that exponential dependence is necessary."
    }, {
      "heading" : "5 Experiments",
      "text" : "We conducted experiments on real data from a Brazilian social network called Apontador that provides location-based recommendations and reviews. The data set was introduced in the context of spam fighting in a recent work by Costa et al. [CdCMBB14] and is available from the authors upon request. The data set consists of 7076 instances of so-called “tips” half of which are labeled as “spam”. Tips are pieces of user-provided content associated with the places listed on Apontador. The paper distinguishes between different types of spam, but the distinction does not matter for us, so we will only consider one category. There are 60 features in total, but to facilitate the modeling of a cost function we restricted our attention to the 15 most discriminative features as indicated by previous work [CdCMBB14]. We normalized all features of the data to have zero mean and unit standard deviation.\nThe goal of our cost function is not primarily to capture monetary cost of changing certain attributes. Apart from attributes like “number of followers”, most attributes are technically easy to change. Rather the goal of a cost function is to capture the loss in expected revenue that a spammer experiences when changing certain parts of the spam message. If, for instance, it is essential for the spam message to contain a URL or contact information, then the spammer experiences lost revenue when such information is omitted. Similarly, the spammer could choose to post his messages on the pages of lower-rated places, but such pages are less frequented and hence his utility decreases. Similar reasoning applies to the modeling of the other attributes. Cheap attributes are those that can be changed without a loss in utility for the spammer. For example, the “number of words” is not robust as the spammer can freely choose to write longer or shorter messages.\nWith this intuition in mind, we model our cost function as a simple linear function truncated at 0 to make it non-negative. That is we consider a cost function of the form c(x,y) = 〈α,y − x〉+. Truncation at 0 is a meaningful modeling decision, since a spammer doesn’t derive any utility from, say, decreasing the number of his followers even though it is costly to increase this attribute.\nThe cost vector α specifies for each attribute a coefficient quantifying the cost of changing that attribute. We do not attempt to construct as realistic a cost function as possible. We only distinguish between three types of cost: somewhat costly to increase (coefficient 1), somewhat costly to decrease (coefficient −1.0) and cheap to increase (coefficient 0.1). The concrete values of these coefficients are rather arbitrary and different choices may be more suitable. The next table details each feature with its description and its associated cost. For a more detailed explanation of these features, the reader is referred to [CdCMBB14].\nDescription Cost coefficient 1 Number of tips on the place −1 2 Place rating −1 3 Number of emails −1 4 Number of contact information −1 5 Number of URLs −1 6 Number of phone numbers −1 7 Number of numeric characters −1 8 SentiStrength score 1 9 Combined-method 1 10 Number of words 0.1 11 Ratio of followers to followees 1 12 Number of distinct 1-grams 0.1 13 Number of tips posted by user 0.1 14 Number of followers 1 15 Number of capital letters 0.1\nWe made no attempt to arrive at a perfectly-realistic cost function. Instead our focus is on a qualitative comparison of our approach with a standard SVM classifier, which does not take gaming into account. We selected SVM as a representative classifier as it was shown in previous work [CdCMBB14] to achieve high classification accuracy on this data set compared with other standard classifiers. For simplicity and increased interpretability, we use a linear SVM which still achieves high accuracy.\nIf we were to assume that our model of gaming and choice of cost function were perfectly correct, then a standard SVM would perform very poorly when compared with our algorithm. To obtain a more balanced comparison, we take modeling inaccuracies into account in our experiments. Specifically, we account for two potential inaccuracies in our model:\n1. The true cost function is not the one on which we train our algorithm.\n2. The amount of gaming varies and does not necessarily correspond to the threshold predicted by our theoretical framework.\nFinally, we explore a convenient way to interpolate between the classifier suggested by our approach and standard classifiers. This leads to different trade-offs which are more favorable in certain settings."
    }, {
      "heading" : "5.1 Comparison with SVM under robustness to modeling errors",
      "text" : "We now show that our method is robust to significant modeling errors while simultaneously outperforming SVM even if only a small amount of gaming occurs.\nTo formalize our error model, we assume that there is a true underlying cost function which differs from the cost function we feed into Algorithm 1. We imagine that the true cost function is some mixture of the linear cost function described above, plus a squared Euclidean distance term:\nctrue(x,y) = (1− ε) 〈 α,y − x 〉 + + ε ∥∥∥x − y∥∥∥2 2 . (8)\nOn the other hand, we run our algorithm on a cost function which is incorrect in two ways. First, it is separable, so it necessarily ignores the squared-distance term. Second, we do not\nimagine that we have correctly identified α, and we replace it with some α′:\ncassumed(x,y) = 〈 α′ , y − x 〉 + .\nThe addition of the Euclidean norm in (8) reflects the possibility that our separability assumption does not exactly hold. The difference between α and α′ reflects the possibility that we may not even have accurately identified the separable part. We stress that not only does our algorithm not know the true cost function, it also does not know the parameter ε, or how much α differs from α′.\nFor our experiments, we considered a range of values of ε, and we generated α′ from α at random by adding Gaussian noise and re-normalizing. We develop our classifier using cassumed, but then for tests allow Contestant to best-respond to the classifier given the cost function ctrue. We note that finding the best response to a linear classifier given the cost function ctrue is a simple calculus problem.\nThe other parameter we varied is the amount of gaming allowed. In our theoretical framework above, the Contestant is always willing to pay a cost of up to 2, since his payoff for switching is 1− (−1) = 2. To relax this assumption and vary the amount of gaming allowed, we multiply both ctrue and cassumed by 2/t; we say that this allows t units of gaming. Notice that by the definition of ctrue, this means that the Contestant is willing to move distance t in the direction of α, and possibly more in other directions. As mentioned above, we have normalized the standard deviation of all attributes to be 1.\nWithin the above error model, we compare our algorithm with SVM as a representative standard classifier. Figures 3 show that our algorithm outperforms SVM, even under a small amount of gaming, and even in the presence of significant modeling errors."
    }, {
      "heading" : "5.2 A hybrid approach for higher accuracy",
      "text" : "In practice it is convenient to start from a standard classifier and make it more robust to gaming and as opposed to adopting an entirely new classifier. Our framework gives a convenient way to incorporate a set of known classifiers into the design of a strategy-robust classifier. As we show below this can lead to more favorable trade-offs between gaming and accuracy.\nThe basic idea is to use each known classifier as a feature to which we assign a positive weight in the cost function. In other words, we stipulate that the classifier is by itself a somewhat reliable attribute of the data. Below we try out this hybrid approach by combining our classifier with the standard SVM classifier. Indeed, we find in our experiments that the hybrid has higher accuracy in a robust range of parameters. This is shown in Figure 4.\nIn the case of a linear SVM, the decision boundary is given by a vector β and we can simple add this vector to our cost function. We assume that the true cost function ctrue is as above, but we modify cassumed as:\ncassumed(x,y) = 〈 (1−γ)α′ +γβ,y − x 〉 + ,\nwhere β are the SVM coefficients learned from the training data set."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are grateful for stimulating discussions with Cynthia Dwork, Brendan Juba, Omer Reingold and Aaron Roth. We are also grateful to Fabricio Benevenuto for pointing us to the Apondator\ndata set and sharing it with us, and to an anonymous reviewer for pointing out that our uniform result implied the non-uniform corollary."
    }, {
      "heading" : "A Proof of Theorem 3.3",
      "text" : "Proof of Theorem 3.3. Fix h ∈ C. As in the proof of Theorem 2.3, we begin by defining the set Γ (f ) of x ∈ X so that f (∆(x)) = 1 when ∆ is a best response to f . For every f ∈ C, we have\nΓ (f ) := {x : max{f (y) : y ∈ Γ (x)} = 1} = {x : (∃y ∈ X, ∃b ∈ B)(f (y) = 1, b(x,y) < 2)} = {x : (∃b ∈ B)(b1(x) >min{b2(y) : f (y) = 1} − 2)}\n= ∪b∈B { x : b1(x) >min{b2(y) : f (y) = 1} − 2 } .\nNow we can again restrict our attention to nicely structured functions. For any f , let\nf ′(y) =  1 if (∀b ∈ B)(b2(y) >min{b2(z) : f (z) = 1})−1 otherwise . (9) Then, as in the proof of Theorem 2.3, we have\nmin{b2(y) : f (y) = 1} = min{b2(y) : f ′(y) = 1}\nfor all b ∈ B. Indeed,\nmin{b2(y) : f ′(y) = 1} > min{b2(z) : f (z) = 1}\nby definition of f ′, and\nmin{b2(y) :f ′(y) = 1} = min {b2(y) : y ∈ ∩b∈B {w : b2(w) >min{b2(z) : f (z) = 1}}} 6 min {b2(y) : f (y) = 1} ,\nusing the fact that\n{y : f (y) = 1} ⊂ {w : b2(w) >min{b2(z) : f (z) = 1}}\nfor all b ∈ B. Thus, Γ (f ) = ∪b∈B { x : b1(x) >min{b2(y) : f (y) = 1} − 2 } = ∪b∈B { x : b1(x) >min{b2(y) : f ′(y) = 1} − 2 } = Γ (f ′) .\nThus, as before, the payoff to Jury if she plays f is the same as if she plays f ′:\nP (h(x) = max{f (y) : y ∈ Γ (x) ) = P ((Γ (f )4h)c)\n= P ( (Γ (f ′)4h)c ) = P (h(x) = max{f ′(y) : y ∈ Γ (x)}) .\nThus, it suffices to consider classifiers f of the form (9). Moving the quantifiers around, it suffices to consider classifiers of the form\nf = min{b2[sb] : b ∈ B} (10)\nfor SB := s ∈ ⊕ b∈B (b2(X)∪ {∞}) .\nHere, sb plays the role of min{b2(z) : f (z) = 1}, and sb =∞ means that f (z) = −1 for all z ∈ X. For f as in (10), we have\nΓ (min{b2[sb] : b ∈ B}) = ⋃ b∈B {x : b1(x) > sb − 2} ,\nand a best-possible payoff to Jury is obtained by finding the best thresholds s:\nOPTh(D, c) = 1− inf s∈SB\n{ P (h(x) ,min{b1[sb − 2](x) : b ∈ B}) } =: 1− inf\ns∈SB {err(s)} .\nIn Algorithm 2, Jury returns f = min{b2[s∗b] : b ∈ B} ,\nand as above the payoff to Jury from this f is\nP (h(x) , c1[s ∗ − 2](x)) = 1− err(s∗) .\nAs in the proof of Theorem 2.3, to prove Theorem 3.3 it suffices to show that for all h ∈ C,\nerr(s∗) 6 inf{err(s) : s ∈ SB}+ ε . (11)\nAs before, we have êrr(s∗) = min{êrr(s) : s ∈ SB} , (12)\nso it suffices to establish that êrr(s) is close to err(s), uniformly over s ∈ SB .\nClaim A.1. With probability at least 1− δ, for all h ∈ C and s ∈ SB , |êrr(s)− err(s)| 6 4Rm(C) + 8 √ |B| ln(m+1) m + √ 2ln(2/δ) m .\nIn particular, if the hypotheses of the lemma are met, with probability at least 1− δ, sup { |êrr(s)− err(s)| : h ∈ C,s ∈ SB } 6 ε/2 .\nProof. Again, this follows very similarly to the proof of Theorem 2.3. We need to bound the absolute value of the following difference:\n1 m m∑ j=1 1 { h(xj ) ,min{b1[sb − 2](xj ) : b ∈ B} } −Ex∼D [ 1 {h(x) ,min{b1[sb − 2](x) : b ∈ B}} ]\nfor all h ∈ C and s ∈ SB . As before (via, say, Theorem 3.2 in [BBL05]), we have for all h ∈ C,s ∈ SB ,∣∣∣∣∣∣∣∣ 1m m∑ j=1 1 { h(xj ) ,min b∈B b1[tb](xj ) } −Ex∼D1 { h(x) ,min b∈B b1[tb](x) }∣∣∣∣∣∣∣∣ 6 2Rm(X ) + √ 2ln(2/δ) m , (13)\nwhere X = { h ·min\nb∈B b1[sb − 2] : h ∈ C,s ∈ SB\n} .\nAs before, Rm(X ) 6 2(Rm(C) +Rm(H)) , (14)\nwhere H = {minb∈B b1[sb − 2] : s ∈ SB}, so it remains to bound Rm(H). For fixed x1, . . . ,xm ∈ X, we have\nEσ\n[ sup { 1 m m∑ i=1 σi min b∈B b1[sb − 2](xi) : s ∈ SB }]\n= Eσ\n[ sup { 1 m m∑ i=1 σi min b∈B b1[sb − 2](xi) : s ∈ ⊕ b∈B { sj,b : j ∈ [m+ 1] }}] 6 2 √ ln((m+1)|B|)\nm .\nThus, we have\nRm(H) 6 2 √ |B| ln(m+ 1)\nm ,\nand, along with Equation 13, this finishes the claim.\nAs in the proof of Theorem 2.3, Equation 12 and Claim A.1 finish the proof of Theorem 3.3."
    } ],
    "references" : [ {
      "title" : "Theory of classification: A survey of some recent advances",
      "author" : [ "Stéphane Boucheron", "Olivier Bousquet", "Gábor Lugosi" ],
      "venue" : "ESAIM: probability and statistics,",
      "citeRegEx" : "Boucheron et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Boucheron et al\\.",
      "year" : 2005
    }, {
      "title" : "Static prediction games for adversarial learning problems",
      "author" : [ "Michael Brückner", "Christian Kanzow", "Tobias Scheffer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Brückner et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Brückner et al\\.",
      "year" : 2012
    }, {
      "title" : "Nash equilibria of static prediction games",
      "author" : [ "Michael Brückner", "Tobias Scheffer" ],
      "venue" : "In Proc. 23rd NIPS",
      "citeRegEx" : "Brückner and Scheffer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Brückner and Scheffer.",
      "year" : 2009
    }, {
      "title" : "Stackelberg games for adversarial prediction problems",
      "author" : [ "Michael Brückner", "Tobias Scheffer" ],
      "venue" : "In Proc 17th ACM SIGKDD,",
      "citeRegEx" : "Brückner and Scheffer.,? \\Q2011\\E",
      "shortCiteRegEx" : "Brückner and Scheffer.",
      "year" : 2011
    }, {
      "title" : "Pollution, bad-mouthing, and local marketing: The underground of location-based social networks",
      "author" : [ "Helen Costa", "Fabrício Barth", "Fabrício Benevenuto" ],
      "venue" : "Inf. Sci.,",
      "citeRegEx" : "Costa et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Costa et al\\.",
      "year" : 2014
    }, {
      "title" : "The scored society: Due process for automated predictions",
      "author" : [ "Danielle Keats Citron", "Frank Pasquale" ],
      "venue" : "Washington Law Review,",
      "citeRegEx" : "Citron and Pasquale.,? \\Q2014\\E",
      "shortCiteRegEx" : "Citron and Pasquale.",
      "year" : 2014
    }, {
      "title" : "Adversarial classification",
      "author" : [ "Nilesh N. Dalvi", "Pedro Domingos", "Mausam", "Sumit K. Sanghai", "Deepak Verma" ],
      "venue" : "In Proc 10th ACM SIGKDD,",
      "citeRegEx" : "Dalvi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2004
    }, {
      "title" : "Family scholarly culture and educational success: Evidence from 27 nations",
      "author" : [ "M.D.R. Evans", "J. Kelley", "J. Sikora", "D.J. Treiman" ],
      "venue" : "Research in Social Stratification and Mobility,",
      "citeRegEx" : "Evans et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Evans et al\\.",
      "year" : 2010
    }, {
      "title" : "Bayesian games for adversarial regression problems",
      "author" : [ "Michael Großhans", "Christoph Sawade", "Michael Brückner", "Tobias Scheffer" ],
      "venue" : "In Proc. 30th ICML,",
      "citeRegEx" : "Großhans et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Großhans et al\\.",
      "year" : 2013
    }, {
      "title" : "Complexity of computing optimal Stackelberg strategies in security resource allocation games",
      "author" : [ "Dmytro Korzhyk", "Vincent Conitzer", "Ronald Parr" ],
      "venue" : "In Proc. AAAI,",
      "citeRegEx" : "Korzhyk et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Korzhyk et al\\.",
      "year" : 2010
    }, {
      "title" : "Stackelberg vs. Nash in security games: An extended investigation of interchangeability, equivalence, and uniqueness",
      "author" : [ "Dmytro Korzhyk", "Zhengyu Yin", "Christopher Kiekintveld", "Vincent Conitzer", "Milind Tambe" ],
      "venue" : "J. Artif. Intell. Res.(JAIR),",
      "citeRegEx" : "Korzhyk et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Korzhyk et al\\.",
      "year" : 2011
    }, {
      "title" : "Probability in Banach Spaces: isoperimetry and processes, volume 23",
      "author" : [ "Michel Ledoux", "Michel Talagrand" ],
      "venue" : null,
      "citeRegEx" : "Ledoux and Talagrand.,? \\Q1991\\E",
      "shortCiteRegEx" : "Ledoux and Talagrand.",
      "year" : 1991
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "Leslie Valiant" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Valiant.,? \\Q1984\\E",
      "shortCiteRegEx" : "Valiant.",
      "year" : 1984
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Machine learning relies on the assumption that unseen test instances of a classification problem follow the same distribution as observed training data. However, this principle can break down when machine learning is used to make important decisions about the welfare (employment, education, health) of strategic individuals. Knowing information about the classifier, such individuals may manipulate their attributes in order to obtain a better classification outcome. As a result of this behavior—often referred to as gaming—the performance of the classifier may deteriorate sharply. Indeed, gaming is a well-known obstacle for using machine learning methods in practice; in financial policy-making, the problem is widely known as Goodhart’s law. In this paper, we formalize the problem, and pursue algorithms for learning classifiers that are robust to gaming. We model classification as a sequential game between a player named “Jury” and a player named “Contestant.” Jury designs a classifier, and Contestant receives an input to the classifier drawn from a distribution. Before being classified, Contestant may change his input based on Jury’s classifier. However, Contestant incurs a cost for these changes according to a cost function. Jury’s goal is to achieve high classification accuracy with respect to Contestant’s original input and some underlying target classification function, assuming Contestant plays best response. Contestant’s goal is to achieve a favorable classification outcome while taking into account the cost of achieving it. For a natural class of separable cost functions, and certain generalizations, we obtain computationally efficient learning algorithms which are near optimal, achieving a classification error that is arbitrarily close to the theoretical minimum. Surprisingly, our algorithms are efficient even on concept classes that are computationally hard to learn. For general cost functions, designing an approximately optimal strategy-proof classifier, for inverse-polynomial approximation, is NP-hard. ar X iv :1 50 6. 06 98 0v 1 [ cs .L G ] 2 3 Ju n 20 15",
    "creator" : "LaTeX with hyperref package"
  }
}