{
  "name" : "1606.04142.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula",
    "authors" : [ "Jean Barbier", "Mohamad Dia", "Florent Krzakala" ],
    "emails" : [ "firstname.lastname@epfl.ch", "florent.krzakala@ens.fr", "lesieur.thibault@gmail.com", "lenka.zdeborova@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Consider the following probabilistic rank-one matrix estimation problem: one has access to noisy observations w=(wij)ni,j=1 of the pair-wise product of the components of a vector s=(s1, . . . , sn)ᵀ∈Rn with i.i.d components distributed as Si∼P0, i=1, . . . , n. The entries of w are observed through a noisy element-wise (possibly non-linear) output probabilistic channel Pout(wij |sisj/ √ n). The goal is to estimate the vector s from w assuming that both P0 and Pout are known and independent of n (noise is symmetric so that wij =wji). Many important problems in statistics and machine learning can be expressed in this way, such as sparse PCA [Zou et al. (2006)], the Wigner spike model [Johnstone and Lu (2012); Deshpande and Montanari (2014)], community detection [Deshpande et al. (2015)] or matrix completion [Candès and Recht (2009)].\nProving a result initially derived by a heuristic method from statistical physics, we give an explicit expression for the mutual information and the information theoretic minimal mean-square-error (MMSE) in the asymptotic n→+∞ limit. Our results imply that for\nar X\niv :1\n60 6.\n04 14\n2v 1\n[ cs\n.I T\n] 1\na large region of parameters, the posterior marginal expectations of the underlying signal components (often assumed intractable to compute) can be obtained in the leading order in n using a polynomial-time algorithm called approximate message-passing (AMP) [Rangan and Fletcher (2012); Deshpande and Montanari (2014); Deshpande et al. (2015); Lesieur et al. (2015b)]. We also demonstrate the existence of a region where both AMP and spectral methods [Baik et al. (2005)] fail to provide a good answer to the estimation problem, while it is nevertheless information theoretically possible to do so. We illustrate our theorems with examples and also briefly discuss the implications in terms of computational complexity."
    }, {
      "heading" : "1. Setting and main results",
      "text" : ""
    }, {
      "heading" : "1.1 The additive white Gaussian noise setting",
      "text" : "A standard and natural setting is the case of additive white Gaussian noise (AWGN) of known variance ∆,\nwij = sisj√ n\n+ zij √ ∆, (1)\nwhere z=(zij)ni,j=1 is a symmetric matrix with i.i.d entries Zij∼N (0, 1), 1≤ i≤j≤n. Perhaps surprisingly, it turns out that this Gaussian setting is sufficient to completely characterize all the problems discussed in the introduction, even if these have more complicated output channels. This is made possible by a theorem of channel universality [Krzakala et al. (2016)] (already proven for community detection in [Deshpande et al. (2015)] and conjectured in [Lesieur et al. (2015a)]). This theorem states that given an output channel Pout(w|y), such that logPout(w|y = 0) is three times differentiable with bounded second and third derivatives, then the mutual information satisfies I(S;W)=I(S;SSᵀ/ √ n+Z √ ∆)+O( √ n),\nwhere ∆ is the inverse Fisher information (evaluated at y = 0) of the output channel: ∆−1 := EPout(w|0)[(∂y logPout(W |y)|y=0)2]. Informally, this means that we only have to compute the mutual information for an AWGN channel to take care of a wide range of problems, which can be expressed in terms of their Fisher information. In this paper we derive rigorously, for a large class of signal distributions P0, an explicit one-letter formula for the mutual information per variable I(S;W)/n in the asymptotic limit n→+∞."
    }, {
      "heading" : "1.2 Main result",
      "text" : "Our central result is a proof of the expression for the asymptotic n→+∞ mutual information per variable via the so-called replica symmetric potential function iRS(E; ∆) defined as\niRS(E; ∆) := (v − E)2 + v2\n4∆ − ES,Z\n[ ln (∫ dxP0(x)e − x 2 2Σ(E;∆)2 +x ( S Σ(E;∆)2 + Z Σ(E;∆) ))] , (2)\nwith Z∼N (0, 1), S∼P0, E[S2]=v and Σ(E; ∆)2 :=∆/(v−E), E∈ [0, v]. Here we will assume that P0 is a discrete distribution over a finite bounded real alphabet P0(s)= ∑ν α=1 pαδ(s−aα). Thus the only continuous integral in (2) is the Gaussian over z. Our results can be extended to mixtures of discrete and continuous signal distributions at the expense of technical complications in some proofs.\nIt turns out that both the information theoretical and algorithmic AMP thresholds are determined by the set of stationary points of (2) (w.r.t E). It is possible to show that for all ∆> 0 there always exist at least one stationary minimum. Note E = 0 is never a stationary point (except for P0 a single Dirac mass) and E=v is stationary only if E[S]=0. In this contribution we suppose that at most three stationary points exist, corresponding to situations with at most one phase transition. We believe that situations with multiple transitions can also be covered by our techniques.\nTheorem 1 (One letter formula for the mutual information) Fix ∆>0 and assume P0 is a discrete distribution such that iRS(E; ∆) given by (2) has at most three stationary points. Then\nlim n→+∞\n1 n I(S;W) = min E∈[0,v] iRS(E; ∆). (3)\nThe proof of the existence of the limit does not require the above hypothesis on P0. Also, it was first shown in [Krzakala et al. (2016)] that for all n, I(S;W)/n≤minE∈[0,v] iRS(E; ∆), an inequality that we will use in the proof section. It is conceptually useful to define the following threshold:\nDefinition 2 (Information theoretic threshold) Define ∆Opt as the first non-analyticity point of the asymptotic mutual information per variable as ∆ increases, that is formally ∆Opt :=sup{∆| limn→+∞ I(S;W)/n is analytic in ]0,∆[}.\nWhen P0 is such that (2) has at most three stationary points, as discussed below, then minE∈[0,v] iRS(E; ∆) has at most one non-analyticity point denoted ∆RS (if minE∈[0,v] iRS(E; ∆) is analytic over all R+ we set ∆RS = +∞). Theorem 1 gives us a mean to compute the information theoretical threshold ∆Opt = ∆RS. A basic application of theorem 1 is the expression of the MMSE:\nCorollary 3 (Exact formula for the MMSE) For all ∆ 6= ∆RS, the matrix-MMSE Mmmsen :=ES,W‖SSᵀ−E[XXᵀ|W]‖2F/n2 (‖−‖F being the Frobenius norm) is asymptotically limn→+∞Mmmsen(∆\n−1)=v2−(v−argminE∈[0,v]iRS(E; ∆))2. Moreover, if ∆<∆AMP (where ∆AMP is the algorithmic threshold, see definition 4) or ∆>∆RS, then the usual vector-MMSE Vmmsen :=ES,W‖S−E[X|W]‖22/n satisfies limn→+∞Vmmsen=argminE∈[0,v]iRS(E; ∆).\nIt is natural to conjecture that the vector-MMSE is given by argminE∈[0,v]iRS(E; ∆) for all ∆ 6=∆RS, but our proof does not quite yield the full statement.\nA fundamental consequence concerns the performance of the AMP algorithm [Rangan and Fletcher (2012)] for estimating s. AMP has been analysed rigorously in [Bayati and Montanari (2011); Javanmard and Montanari (2013); Deshpande et al. (2015)] where it is shown that its asymptotic performance is tracked by state evolution. Let Et :=limn→+∞ ES,Z[‖S−ŝt‖22]/n be the asymptotic average vector-MSE of the AMP estimate ŝt at time t. Define mmse(Σ−2) := ES,Z [(S−E[X|S+ΣZ])2] as the usual scalar mmse function associated to a scalar AWGN channel of noise variance Σ2, with S∼P0 and Z∼N (0, 1). Then\nEt+1 = mmse(Σ(Et; ∆)−2), E0 = v, (4)\nis the state evolution recursion. Monotonicity properties of the mmse function imply that Et is a decreasing sequence such that limt→+∞Et=E∞ exists. Note that when E[S] = 0 and v is an unstable fixed point, as such, state evolution “does not start”. While this is not really a problem when one runs AMP in practice, for analysis purposes one can slightly bias P0 and remove the bias at the end of the proofs.\nDefinition 4 (AMP algorithmic threshold) For ∆ > 0 small enough, the fixed point equation corresponding to (4) has a unique solution for all noise values in ]0,∆[. We define ∆AMP as the supremum of all such ∆.\nCorollary 5 (Performance of AMP) In the limit n→+∞, AMP initialized without any knowledge other than P0 yields upon convergence the asymptotic matrix-MMSE as well as the asymptotic vector-MMSE iff ∆<∆AMP or ∆>∆RS, namely E∞=argminE∈[0,v]iRS(E; ∆).\n∆AMP can be read off the replica potential (2): by differentiation of (2) one finds a fixed point equation that corresponds to (4). Thus ∆AMP is the smallest solution of ∂iRS/∂E= ∂2iRS/∂E\n2 = 0; in other words it is the “first” horizontal inflexion point that appears in iRS(E; ∆) when we increase ∆."
    }, {
      "heading" : "1.3 Discussion",
      "text" : "With our hypothesis on P0 there are only three possible scenarios: ∆AMP < ∆RS (one “first order” phase transition); ∆AMP = ∆RS <+∞ (one “higher order” phase transition); ∆AMP = ∆RS = +∞ (no phase transition). In the sequel we will have in mind the most interesting case, namely one first order phase transition, where we determine the gap between the algorithmic AMP and information theoretic performance. The cases of no phase transition or higher order phase transition, which present no algorithmic gap, are basically covered by the analysis of [Deshpande and Montanari (2014)] and follow as a special case from our proof. The only cases that would require more work are those where P0 is such that (2) develops more than three stationary points and more than one phase transition is present.\nFor ∆AMP < ∆RS the structure of stationary points of (2) is as follows1 (figure 1). There exist three branches Egood(∆), Eunstable(∆) and Ebad(∆) such that: 1) For 0 < ∆<∆AMP there is a single stationary point Egood(∆) which is a global minimum; 2) At ∆AMP a horizontal inflexion point appears, for ∆∈ [∆AMP,∆RS] there are three stationary points satisfying Egood(∆AMP)<Eunstable(∆AMP)=Ebad(∆AMP), Egood(∆)<Eunstable(∆)< Ebad(∆) otherwise, and moreover iRS(Egood; ∆)≤ iRS(Ebad; ∆) with equality only at ∆RS; 3) for ∆>∆RS there is at least the stationary point Ebad(∆) which is always the global minimum, i.e. iRS(Ebad; ∆)<iRS(Egood; ∆). (For higher ∆ the Egood(∆) and Eunstable(∆) branches may merge and disappear); 4) Egood(∆) is analytic for ∆∈]0,∆′[, ∆′>∆RS, and Ebad(∆) is analytic for ∆>∆AMP.\nWe note for further use in the proof section that E∞ =Egood(∆) for ∆<∆AMP and E∞=Ebad(∆) for ∆>∆AMP. Definition 4 is equivalent to ∆AMP=sup{∆|E∞=Egood(∆)}. Moreover we will also use that iRS(Egood; ∆) is analytic on ]0,∆′[, iRS(Ebad; ∆) is analytic on ]∆AMP,+∞[, and the only non-analyticity point of minE∈[0,v] iRS(E; ∆) is at ∆RS.\n1. We take E[S] 6= 0. Once theorem 1 is proven for this case a limiting argument allows to extend it to E[S]=0."
    }, {
      "heading" : "1.4 Relation to other works",
      "text" : "Explicit single-letter characterization of the mutual information in the rank-one problem has attracted a lot of attention recently. Particular cases of (3) have been shown rigorously in a number of situations. A special case when si=±1∼Ber(1/2) already appeared in [Korada and Macris (2009)] where an equivalent spin glass model is analysed. Very recently, [Krzakala et al. (2016)] has generalized the results of [Korada and Macris (2009)] and, notably, obtained a generic matching upper bound. The same formula has been also rigorously computed following the study of AMP in [Deshpande and Montanari (2014)] for spike models (provided, however, that the signal was not too sparse) and in [Deshpande et al. (2015)] for strictly symmetric community detection.\nFor rank-one symmetric matrix estimation problems, AMP has been introduced by [Rangan and Fletcher (2012)], who also computed the state evolution formula to analyse its performance, generalizing techniques developed by [Bayati and Montanari (2011)] and [Javanmard and Montanari (2013)]. State evolution was further studied by [Deshpande and Montanari (2014)] and [Deshpande et al. (2015)]. In [Lesieur et al. (2015b,a)], the generalization to larger rank was also considered.\nThe general formula proposed by [Lesieur et al. (2015a)] for the conditional entropy and the MMSE on the basis of the heuristic cavity method from statistical physics was not demonstrated in full generality. Worst, all existing proofs could not reach the more interesting regime where a gap between the algorithmic and information theoretic perfomances appears,\nleaving a gap with the statistical physics conjectured formula (and rigorous upper bound from [Krzakala et al. (2016)]). Our result closes this conjecture and has interesting non-trivial implications on the computational complexity of these tasks.\nOur proof technique combines recent rigorous results in coding theory along the study of capacity-achieving spatially coupled codes [Hassani et al. (2010); Kudekar et al. (2011); Yedla et al. (2014); Barbier et al. (2016)] with other progress, coming from developments in mathematical physics putting on a rigorous basis predictions of spin glass theory [Guerra (2005)]. From this point of view, the theorem proved in this paper is relevant in a broader context going beyond low-rank matrix estimation. Hundreds of papers have been published in statistics, machine learning or information theory using the non-rigorous statistical physics approach. We believe that our result helps setting a rigorous foundation of a broad line of work. While we focus on rank-one symmetric matrix estimation, our proof technique is readily extendable to more generic low-rank symmetric matrix or low-rank symmetric tensor estimation. We also believe that it can be extended to other problems of interest in machine learning and signal processing, such as generalized linear regression, features/dictionary learning, compressed sensing or multi-layer neural networks."
    }, {
      "heading" : "2. Two examples: Wigner spike model and community detection",
      "text" : "In order to illustrate the consequences of our results we shall present two examples. In the first one we are given data distributed according to the spiked Wigner model where the vector s is a Bernoulli random vector, Si∼Ber(ρ). For large enough densities (i.e. ρ>0.041(1)), [Deshpande and Montanari (2014)] computed the matrix-MMSE and proved that AMP is a computationally efficient algorithm that asymptotically achieves the matrix-MMSE for any value of the noise ∆. Our results allow to close the gap left open by [Deshpande and Montanari (2014)]: on one hand we now obtain rigorously the MMSE for ρ≤0.041(1), and on the other one, we observe that for such values of ρ, and as ∆ decreases, there is a small region where two local minima coexist in iRS(E; ∆). In particular for ∆AMP<∆<∆Opt = ∆RS the global minimum corresponding to the MMSE differs from the local one that traps AMP, and a computational gap appears (see figure 1). While the region where AMP is Bayes optimal is quite large, the region where is it not, however, is perhaps the most interesting one. While this is by no means evident, statistical physics analogies with physical phase transitions in nature suggest that this region should be hard for a very broad class of algorithms.\nFor small ρ our results are consistent with the known optimal and algorithmic thresholds predicted in sparse PCA [Amini and Wainwright (2008); Berthet and Rigollet (2013)], that treats the case of sub-extensive ρ=O(1) values. Another interesting line of work for such probabilistic models appeared in the context of random matrix theory (see [Baik et al. (2005)] and references therein) and predicts that a sharp phase transition occurs at a critical value of the noise ∆spectral=ρ2 below which an outlier eigenvalue (and its principal eigenvector) has a positive correlation with the hidden signal. For larger noise values the spectral distribution of the observation is indistinguishable from that of the pure random noise.\nWe now consider the problem of detecting two communities (groups) with different sizes ρn and (1−ρ)n, that generalizes the one considered in [Deshpande et al. (2015)]. One is given a graph where the probability to have a link between nodes in the first group is p+µ(1− ρ)/(ρ √ n), between those in the second group is p+µρ/( √ n(1− ρ)), while\npoint) and when ∆>1, it is information theoretically impossible to find any overlap with the true communities and the matrix-MMSE is 1, while it becomes possible for ∆<1. In this region, AMP is always achieving the matrix-MMSE and spectral methods can find a non-trivial overlap with the truth as well, starting from ∆< 1. For ρ < 1/2− √ 1/12, however, it is information theoretically possible to find an overlap with the hidden communities for ∆>1 (below the blue line) but both AMP and spectral methods miss this information. Inset: matrix-MMSE (blue) at ρ=0.05 as a function of ∆. AMP (dashed red) again provably achieves the matrix-MMSE except in the region ∆AMP<∆<∆Opt.\ninterconnections appear with probability p−µ/ √ n. With this peculiar “balanced” setting,\nthe nodes in each group have the same degree distribution with mean pn, making them harder to distinguish. According to the universality property described in section 1.1, this is equivalent to a model with AWGN of variance ∆=p(1−p)/µ2 where each variable si is chosen according to P0(s)=ρδ(s− √ (1−ρ)/ρ)+(1−ρ)δ(s+ √ ρ/(1−ρ)). Our results for this\nproblem2 are summarized on the right hand side of figure 2. For ρ> ρc=1/2− √\n1/12 (black point), it is asymptotically information theoretically possible to get an estimation better than chance if and only if ∆<1. When ρ<ρc, however, it becomes possible for much larger values of the noise. Interestingly, AMP and spectral methods have the same transition and can find a positive correlation with the hidden communities for ∆<1, regardless of the value of ρ. Again, a region [∆AMP,∆Opt=∆RS] exists where a computational gap appears when ρ<ρc.\nOne can investigate the very low ρ regime where we find that the information theoretic transition goes as ∆Opt(ρ→ 0) = 1/(4ρ| log ρ|). Now if we assume that this result stays\n2. Note that here since E=v=1 is an extremum of iRS(E; ∆), one must introduce a small bias in P0 and let it then tend to zero at the end of the proofs.\ntrue even for ρ=O(1) (which is a speculation at this point), we can choose µ→(1−p)ρ √ n\nsuch that the small group is a clique. Then the problem corresponds to a “balanced” version of the famous planted clique problem [d’Aspremont et al. (2007)]. We find that the AMP/spectral approach finds the hidden clique when it is larger than √ np/(1−p), while the information theoretic transition translates into size of the clique 4p log(n)/(1−p). This is indeed reminiscent of the more classical planted clique problem at p= 1/2 with its gap between log(n) (information theoretic), √ n/e (AMP [Deshpande and Montanari (2015)]) and √ n (spectral [d’Aspremont et al. (2007)]). Since in our balanced case the spectral and\nAMP limits match, this suggests that the small gain of AMP in the standard clique problem is simply due to the information provided by the distribution of local degrees in the two groups (which is absent in our balanced case). We believe this correspondence strengthens the claim that the AMP gap is actually a fundamental one."
    }, {
      "heading" : "3. Proofs",
      "text" : "The crux of our proof rests on an auxiliary “spatially coupled system”. The hallmark of spatially coupled models is that one can tune them so that the gap between the algorithmic and information theoretical limits can be eliminated, while at the same time the mutual information is maintained unchanged for the coupled and original models. Roughly speaking, this means that it is possible to algorithmically compute the information theoretical limit of the original model because a suitable algorithm is optimal on the coupled system.\nThe spatially coupled construction used here is very similar to the one used for the coupled Curie-Weiss model [Hassani et al. (2010)]. We consider a ring of length L+1 (L even) with blocks positioned at µ∈{0, . . . , L} and coupled to neighboring blocks {µ−w, . . . , µ+w}. The positions µ are taken modulo L+1 and w∈{0, . . . , L/2} is an integer equal to the size of the coupling window. The coupled model is\nwiµjν = siµsjν √ Λµν n + ziµjν √ ∆, (5)\nwhere the index iµ∈{1, . . . , n} (resp. jν) belongs to the block µ (resp. ν) along the ring, Λ is an (L+1)×(L+1) matrix which describes the strength of the coupling between blocks, and Ziµjν ∼N (0, 1) are i.i.d. For the proof to work, the matrix elements have to be chosen appropriately. We assume that: i) Λ is a doubly stochastic matrix; ii) Λµν depends on |µ−ν|; iii) Λµν is not vanishing for |µ−ν| ≤ w and vanishes for |µ−ν|>w; iv) Λ is smooth in the sense |Λµν−Λµ+1ν |=O(w−2); v) Λ has a non-negative Fourier transform. All these conditions can easily be met, the simplest example being a triangle of base 2w+1 and height 1/(w+1). The construction of the coupled system is completed by introducing a seed in the ring: we assume perfect knowledge of the signal components {siµ} for µ∈B := {−w−1, . . . , w−1} mod L+1. This seed is what allows to close the gap between the algorithmic and information theoretical limits and therefore plays a crucial role. Note it can also be viewed as an “opening” of the chain with pinned boundary conditions.\nOur first crucial result states that the mutual information Iw,L(S;W) of the coupled and original systems are the same in a suitable asymptotic limit.\nLemma 6 (Equality of mutual informations) For any w ∈ {0, . . . , L/2} the following limits exist and are equal: limL→+∞ limn→+∞ Iw,L(S;W)/(n(L+1))=limn→+∞ I(S;W)/n.\nAn immediate corollary is that non-analyticity points (w.r.t ∆) of the mutual informations are the same in the coupled and original models. In particular, defining ∆Opt,coup :=sup{∆ | limL→+∞ limn→+∞ Iw,L(S;W)/(n(L+1)) is analytic in ]0,∆[}, we have ∆Opt,coup=∆Opt.\nThe second crucial result states that the AMP threshold of the spatially coupled system is at least as good as ∆RS. The analysis of AMP applies to the coupled system as well [Bayati and Montanari (2011); Javanmard and Montanari (2013)] and it can be shown that the performance of AMP is assessed by state evolution. Let Etµ :=limn→+∞ ES,Z[‖Sµ−ŝtµ‖22]/n be the asymptotic average vector-MSE of the AMP estimate ŝtµ at time t for the µ-th “block” of S. We associate to each position µ∈{0, . . . , L} an independent scalar system with AWGN noise of the form Y =S+Σµ(E; ∆)Z with Σµ(E; ∆)2 := ∆/(v− ∑L ν=0 ΛµνEν) and S ∼P0, Z∼N (0, 1). Taking into account knowledge of the signal in B, state evolution reads:\nEt+1µ = mmse(Σµ(E t; ∆)−2), E0µ = v for µ ∈ {0, . . . , L} \\ B, Etµ = 0 for µ ∈ B, t ≥ 0, (6)\nwhere the mmse function is defined as in section 1.2. From the monotonicity of the mmse function we have Et+1µ ≤ Etµ for all µ ∈ {0, . . . , L}, a partial order which implies that limt→+∞Et = E∞ exists. This allows to define an algorithmic threshold: ∆AMP,w,L := sup{∆|E∞µ ≤Egood(∆) ∀ µ}. We show (equality holds but is not directly needed)\nLemma 7 (Threshold saturation) Let ∆AMP,coup :=lim infw→+∞ lim infL→+∞∆AMP,w,L. We have ∆AMP,coup≥∆RS.\nProof sketch of theorem 1 First we prove (3) for ∆ ≤ ∆Opt. It is known [Deshpande and Montanari (2014)] that the matrix-MSE of AMP when n→+∞ is equal to v2−(v−Et)2. This cannot improve the matrix-MMSE, hence\n1 4 (v2 − (v − E∞)2) ≥ lim sup\nn→+∞\n1\n4n2 ES,W‖SSᵀ − E[XXᵀ|W]‖2F. (7)\nFor ∆≤∆AMP we have E∞=Egood(∆) which is the global minimum of (2) so the left hand side of (7) is equal to the derivative of minE∈[0,v] iRS(E; ∆) w.r.t ∆−1. Thus using a matrix version of the well known I-MMSE relation [Guo et al. (2005)] we get\nd\nd∆−1 min E∈[0,v] iRS(E; ∆) ≥ lim sup n→+∞\n1\nn dI(S;W) d∆−1 . (8)\nIntegrating this relation on [0,∆]⊂ [0,∆AMP] and checking that minE∈[0,v] iRS(E; 0)=H(S) (the Shannon entropy of P0) we obtain minE∈[0,v] iRS(E; ∆)≤ lim infn→+∞ I(S;W)/n. But we know I(S;W)/n≤minE∈[0,v] iRS(E; ∆) [Krzakala et al. (2016)], thus we already get (3) for ∆≤∆AMP. We notice that ∆AMP≤∆Opt. While this might seem intuitively clear, it follows from ∆RS≥∆AMP (by their definitions) which together with ∆AMP>∆Opt would imply from (3) that limn→+∞ I(S;W)/n is analytic at ∆Opt, a contradiction. The next step is to extend (3) to the range [∆AMP,∆Opt]. Suppose for a moment ∆RS≥∆Opt. Then both functions on each side of (3) are analytic on the whole range ]0,∆Opt[ and since they are equal for ∆≤∆AMP, they must be equal on their whole analyticity range and by continuity, they must also be equal at ∆Opt (that the functions are continuous follows from independent arguments on the existence of the n→+∞ limit of concave functions). It remains to show that ∆RS ∈ ]∆AMP,∆Opt[ is impossible. We proceed by contradiction, so suppose this is\ntrue. Then both functions on each side of (3) are analytic on ]0,∆RS[ and since they are equal for ]0,∆AMP[⊂]0,∆RS[ they must be equal on the whole range ]0,∆RS[ and also at ∆RS by continuity. For ∆>∆RS the fixed point of state evolution is E∞=Ebad(∆) which is also the global minimum of iRS(E; ∆), hence (8) is verified. Integrating this inequality on ]∆RS,∆[⊂]∆RS,∆Opt[ and using I(S;W)/n≤minE∈[0,v] iRS(E; ∆) again, we find that (3) holds for all ∆∈ [0,∆Opt]. But this implies that minE∈[0,v] iRS(E; ∆) is analytic at ∆RS, a contradiction.\nWe now prove (3) for ∆≥∆Opt. Note that the previous arguments showed that necessarily ∆Opt≤∆RS. Thus by lemmas 6 and 7 (and the sub-optimality of AMP as shown as before) we obtain ∆RS ≤ ∆AMP,coup≤∆Opt,coup=∆Opt≤∆RS. This shows that ∆Opt=∆RS (this is the point where spatial coupling came in the game and we do not know of other means to prove such an equality). For ∆>∆RS we have E∞=Ebad(∆) which is the global minimum of iRS(E; ∆). Therefore we again have (8) in this range and the proof can be completed by using once more the integration argument, this time over the range [∆RS,∆]=[∆Opt,∆].\nProof sketch of corollaries 3 and 5 Let E∗(∆)=argminEiRS(E; ∆) for ∆ 6=∆RS. By explicit calculation one checks that diRS(E∗,∆)/d∆−1=(v2−(v−E∗(∆))2)/4, so from theorem 1 and the matrix form of the I-MMSE relation we find Mmmsen→v2−(v−E∗(∆))2 as n→+∞ which is the first part of the statement of corollary 3. Let us now turn to corollary 5. For n→+∞ the vector-MSE of the AMP estimator at time t equals Et, and since the fixed point equation corresponding to state evolution is precisely the stationarity equation for iRS(E; ∆), we conclude that for ∆ /∈ [∆AMP,∆RS] we must have E∞=E∗(∆). It remains to prove that E∗(∆)=limn→+∞Vmmsen(∆) at least for ∆ /∈ [∆AMP,∆RS] (we believe this is in fact true for all ∆). This will settle the second part of corollary 3 as well as 5. Using (Nishimori) identities ES,W[SiSjE[XiXj |W]]=ES,W[E[XiXj |W]2] (see e.g. [Krzakala et al. (2016)]) and the law of large numbers we can show limn→+∞Mmmsen≤ limn→+∞(v2−(v−Vmmsen(∆))2). Concentration techniques similar to [Korada and Macris (2009)] suggest that the equality in fact holds (for ∆ 6= ∆RS) but there are technicalities that prevent us from completing the proof of equality. However it is interesting to note that this equality would imply E∗(∆)=limn→+∞Vmmsen(∆) for all ∆ 6=∆RS. Nevertheless, another argument can be used when AMP is optimal. On one hand the right hand side of the inequality is necessarily smaller than v2− (v−E∞)2. On the other hand the left hand side of the inequality is equal to v2−(v−E∗(∆))2. Since E∗(∆) = E∞ when ∆ /∈ [∆AMP,∆RS], we can conclude limn→+∞Vmmsen(∆)=argminEiRS(E; ∆) for this range of ∆.\nProof sketch of lemma 6 Here we prove the lemma for a ring that is not seeded. An easy argument shows that a seed of size w does not change the mutual information per variable when L→+∞. The statistical physics formulation is convenient: up to a trivial additive term equal to n(L+1)v2/4, the mutual information Iw,L(S;W) is equal to the free energy −ES,Z[lnZw,L], where Zw,L := ∫ dxP0(x) exp(−H(x, z,Λ)) is the partition function\nwith Hamiltonian\nH(x, z,Λ) = 1 ∆ L∑ µ=0 Λµµ ∑ iµ≤jµ ( x2iµx 2 jµ 2n − siµsjµxiµxjµ n − xiµxjµziµjµ √ ∆√ nΛµµ )\n+ 1\n∆ L∑ µ=0 µ+w∑ ν=µ+1 Λµν ∑ iµ,jν ( x2iµx 2 jν 2n − siµsjνxiµxjν n − xiµxjνziµjν √ ∆√ nΛµν ) . (9)\nConsider a pair of systems with coupling matrices Λ and Λ′ and i.i.d noize realizations z, z′, an interpolated Hamiltonian H(x, z, tΛ)+H(x, z′, (1−t)Λ′), t ∈ [0, 1], and the corresponding partition function Zt. The main idea of the proof is to show that for suitable choices of matrices, − ddtES,Z,Z′ [lnZt] is negative for all t∈ [0, 1] (up to negligible terms), so that by the fundamental theorem of calculus, we get a comparison between the free energies of H(x, z,Λ) and H(x, z′,Λ′). Performing the t-derivative brings down a Gibbs average of a polynomial in all variables siµ , xiµ , ziµjν and z′iµjν . This expectation over S, Z, Z\n′ of this Gibbs average can be greatly simplified using integration by parts over the Gaussian noise ziµjν , z′iµjν and Nishimori identities (see e.g. proof of corollary 3 for one of them). This algebra leads to\n− 1 n(L+ 1) d dt ES,Z,Z′ [lnZt] =\n1\n4∆(L+ 1) ES,Z,Z′ [〈qᵀΛq− qᵀΛ′q〉t] +O(1/(nL)), (10)\nwhere 〈−〉t is the Gibbs average w.r.t the interpolated Hamiltonian, q is the vector of overlaps qµ := ∑n iµ=1\nsiµxiµ/n. If we can choose matrices such that Λ′>Λ, the difference of quadratic forms in the Gibbs bracket is negative and we obtain an inequality in the large size limit. We use this scheme to interpolate between the fully decoupled system w=0 and the coupled one 1≤w<L/2 and then between 1≤w <L/2 and the fully connected system w=L/2. The w=0 system has Λµν =δµν with eigenvalues (1, 1, . . . , 1). For the 1≤w<L/2 system, we take any stochastic translation invariant matrix with non-negative discrete Fourier transform (of its rows): such matrices have an eigenvalue equal to 1 and all others in [0, 1[ (the eigenvalues are precisely equal to the discrete Fourier transform). For w=L/2 we choose Λµν =1/(L+1) which is a projector with eigenvalues (0, 0, . . . , 1). With these choices we deduce that the free energies and mutual informations are ordered as Iw=0,L+O(1)≤Iw,L+O(1)≤Iw=L/2,L+O(1). To conclude the proof we divide by n(L+1) and note that the limits of the leftmost and rightmost mutual informations are equal, provided the limit exists. Indeed the leftmost term equals L times I(S;W) and the rightmost term is the same mutual information for a system of n(L+1) variables. Existence of the limit follows by a subadditivity inequality which itself is proven by a similar interpolation [Guerra (2005)].\nProof sketch of lemma 7 Fix ∆<∆RS. We show that, for w large enough, the coupled state evolution recursion (6) must converge to a fixed point E∞µ ≤Egood(∆) for all µ. The main intuition behind the proof is to use a “potential function” whose “energy” can be lowered by small perturbation of a fixed point that would go above Egood(∆) [Yedla et al. (2014); Barbier et al. (2016)]. The relevant potential function iw,L(E,∆) is in fact the replica potential of the coupled system, and equals up to a constant (2w+1)Lv2/4∆∑ µ { µ+w∑ ν=µ−w Λµν 4∆ (v−Eµ)(v−Eν)−ES,Z [ ln (∫ dxP0(x)e − x 2 2Σµ(E;∆)2 +x ( S Σµ(E;∆)2 + Z Σµ(E;∆) ))]} .\nWe note that the stationarity condition for this potential is precisely (6) (without the seeding condition). Monotonicity properties of state evolution ensure that any fixed point has a “unimodal” shape (and recall that it vanishes for µ∈B= {0, . . . , w−1} ∪ {L−w, . . . , L}). Consider a position µmax∈{w, . . . , L−w−1} where it is maximal and suppose that E∞µmax> Egood(∆). We associate to the fixed point E∞ a so-called saturated profile Es defined on the whole of Z as follows: Esµ=Egood(∆) for all µ≤µ∞ where µ∞+1 is the smallest position such that E∞µ > Egood(∆); Esµ = E∞µ for µ ∈ {µ∞+1, . . . , µmax−1}; Esµ = E∞µmax for all µ≥ µmax. We show that Es cannot exist for w large enough. To this end define a shift operator by [S(Es)]µ :=Esµ−1. On one hand the shifted profile is a small perturbation of Es which matches a fixed point, except where it is constant, so if we Taylor expand, the first order vanishes and the second order and higher orders can be estimated as |iw,L(S(Es); ∆)− iw,L(Es; ∆)|=O(1/w) uniformly in L. On the other hand, by explicit cancellation of telescopic sums iw,L(S(Es); ∆)−iw,L(Es; ∆)= iRS(Egood; ∆)−iRS(E∞µmax ; ∆). Now one can show from monotonicity properties of state evolution that if E∞ is a fixed point then E∞µmax cannot be in the basin of attraction of Egood(∆) for the uncoupled recursion. Consequently as can be seen on the plot of iRS(E; ∆) (e.g. figure 1) we must have iRS(E∞µmax ; ∆)≥ iRS(Ebad; ∆). Therefore iw,L(S(Es); ∆)−iw,L(Es; ∆)≤−|iRS(Ebad; ∆)−iRS(Egood; ∆)| which is an energy gain independent of w, and for large enough w we get a contradiction with the previous estimate coming from the Taylor expansion."
    }, {
      "heading" : "Acknowledgments",
      "text" : "J.B and M.D acknowledge funding from the Swiss National Science Foundation (grant num. 200021-156672). Part of the research has received funding from the European Research Council under the European Union’s 7th Framework Programme (FP/2007-2013/ERC Grant Agreement 307087-SPARCS). This work was done in part while F.K and L.Z were visiting the Simons Institute for the Theory of Computing."
    } ],
    "references" : [ {
      "title" : "High-dimensional analysis of semidefinite relaxations for sparse principal components",
      "author" : [ "A.A. Amini", "M.J. Wainwright" ],
      "venue" : "In IEEE Int. Symp. on Inf. Theory, page 2454,",
      "citeRegEx" : "Amini and Wainwright.,? \\Q2008\\E",
      "shortCiteRegEx" : "Amini and Wainwright.",
      "year" : 2008
    }, {
      "title" : "Péché. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices",
      "author" : [ "J. Baik", "G. Ben Arous" ],
      "venue" : "Annals of Probability,",
      "citeRegEx" : "Baik et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Baik et al\\.",
      "year" : 2005
    }, {
      "title" : "Threshold saturation of spatially coupled sparse superposition codes for all memoryless channels",
      "author" : [ "J. Barbier", "M. Dia", "N. Macris" ],
      "venue" : "CoRR, abs/1603.04591,",
      "citeRegEx" : "Barbier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Barbier et al\\.",
      "year" : 2016
    }, {
      "title" : "The dynamics of message passing on dense graphs, with applications to compressed sensing",
      "author" : [ "M. Bayati", "A. Montanari" ],
      "venue" : "IEEE Trans. on Inf. Theory,",
      "citeRegEx" : "Bayati and Montanari.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bayati and Montanari.",
      "year" : 2011
    }, {
      "title" : "Computational lower bounds for sparse pca",
      "author" : [ "Q. Berthet", "P. Rigollet" ],
      "venue" : null,
      "citeRegEx" : "Berthet and Rigollet.,? \\Q2013\\E",
      "shortCiteRegEx" : "Berthet and Rigollet.",
      "year" : 2013
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E.J. Candès", "B. Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "Asymptotic mutual information for the two-groups stochastic block model",
      "author" : [ "Y. Deshpande", "E. Abbe", "A. Montanari" ],
      "venue" : null,
      "citeRegEx" : "Deshpande et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Deshpande et al\\.",
      "year" : 2015
    }, {
      "title" : "An introduction to mean field spin glass theory: methods and results",
      "author" : [ "F. Guerra" ],
      "venue" : "Mathematical Statistical Physics,",
      "citeRegEx" : "Guerra.,? \\Q2005\\E",
      "shortCiteRegEx" : "Guerra.",
      "year" : 2005
    }, {
      "title" : "Mutual information and minimum mean-square error in gaussian channels",
      "author" : [ "D. Guo", "S. Shamai", "S. Verdú" ],
      "venue" : "IEEE Trans. on Inf. Theory,",
      "citeRegEx" : "Guo et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2005
    }, {
      "title" : "Coupled graphical models and their thresholds",
      "author" : [ "S.H. Hassani", "N. Macris", "R. Urbanke" ],
      "venue" : "In IEEE Information Theory Workshop (ITW),",
      "citeRegEx" : "Hassani et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hassani et al\\.",
      "year" : 2010
    }, {
      "title" : "State evolution for general approximate message passing algorithms, with applications to spatial coupling",
      "author" : [ "A. Javanmard", "A. Montanari" ],
      "venue" : "J. Infor. & Inference,",
      "citeRegEx" : "Javanmard and Montanari.,? \\Q2013\\E",
      "shortCiteRegEx" : "Javanmard and Montanari.",
      "year" : 2013
    }, {
      "title" : "On consistency and sparsity for principal components analysis in high dimensions",
      "author" : [ "I.M. Johnstone", "A.Y. Lu" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Johnstone and Lu.,? \\Q2012\\E",
      "shortCiteRegEx" : "Johnstone and Lu.",
      "year" : 2012
    }, {
      "title" : "Exact solution of the gauge symmetric p-spin glass model on a complete graph",
      "author" : [ "S.B. Korada", "N. Macris" ],
      "venue" : "Journal of Statistical Physics,",
      "citeRegEx" : "Korada and Macris.,? \\Q2009\\E",
      "shortCiteRegEx" : "Korada and Macris.",
      "year" : 2009
    }, {
      "title" : "Mutual information in rank-one matrix estimation",
      "author" : [ "F. Krzakala", "J. Xu", "L. Zdeborová" ],
      "venue" : null,
      "citeRegEx" : "Krzakala et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krzakala et al\\.",
      "year" : 2016
    }, {
      "title" : "Threshold saturation via spatial coupling: Why convolutional ldpc ensembles perform so well over the bec",
      "author" : [ "S. Kudekar", "T.J. Richardson", "R. Urbanke" ],
      "venue" : "IEEE Trans. on Inf. Theory,",
      "citeRegEx" : "Kudekar et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kudekar et al\\.",
      "year" : 2011
    }, {
      "title" : "Mmse of probabilistic low-rank matrix estimation: Universality with respect to the output channel",
      "author" : [ "T. Lesieur", "F. Krzakala", "L. Zdeborová" ],
      "venue" : "In Annual Allerton Conference,",
      "citeRegEx" : "Lesieur et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lesieur et al\\.",
      "year" : 2015
    }, {
      "title" : "Phase transitions in sparse pca",
      "author" : [ "T. Lesieur", "F. Krzakala", "L. Zdeborová" ],
      "venue" : "In IEEE Int. Symp. on Inf. Theory,",
      "citeRegEx" : "Lesieur et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lesieur et al\\.",
      "year" : 2015
    }, {
      "title" : "Iterative estimation of constrained rank-one matrices in noise",
      "author" : [ "S. Rangan", "A.K. Fletcher" ],
      "venue" : "In IEEE Int. Symp. on Inf. Theory,",
      "citeRegEx" : "Rangan and Fletcher.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rangan and Fletcher.",
      "year" : 2012
    }, {
      "title" : "A simple proof of maxwell saturation for coupled scalar recursions",
      "author" : [ "A. Yedla", "Y.Y. Jian", "P.S. Nguyen", "H.D. Pfister" ],
      "venue" : "IEEE Trans. on Inf. Theory,",
      "citeRegEx" : "Yedla et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yedla et al\\.",
      "year" : 2014
    }, {
      "title" : "Sparse principal component analysis",
      "author" : [ "H. Zou", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of computational and graphical statistics,",
      "citeRegEx" : "Zou et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Many important problems in statistics and machine learning can be expressed in this way, such as sparse PCA [Zou et al. (2006)], the Wigner spike model [Johnstone and Lu (2012); Deshpande and Montanari (2014)], community detection [Deshpande et al.",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "(2006)], the Wigner spike model [Johnstone and Lu (2012); Deshpande and Montanari (2014)], community detection [Deshpande et al.",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "(2006)], the Wigner spike model [Johnstone and Lu (2012); Deshpande and Montanari (2014)], community detection [Deshpande et al.",
      "startOffset" : 33,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "(2006)], the Wigner spike model [Johnstone and Lu (2012); Deshpande and Montanari (2014)], community detection [Deshpande et al. (2015)] or matrix completion [Candès and Recht (2009)].",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "(2015)] or matrix completion [Candès and Recht (2009)].",
      "startOffset" : 30,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "a large region of parameters, the posterior marginal expectations of the underlying signal components (often assumed intractable to compute) can be obtained in the leading order in n using a polynomial-time algorithm called approximate message-passing (AMP) [Rangan and Fletcher (2012); Deshpande and Montanari (2014); Deshpande et al.",
      "startOffset" : 259,
      "endOffset" : 286
    }, {
      "referenceID" : 13,
      "context" : "a large region of parameters, the posterior marginal expectations of the underlying signal components (often assumed intractable to compute) can be obtained in the leading order in n using a polynomial-time algorithm called approximate message-passing (AMP) [Rangan and Fletcher (2012); Deshpande and Montanari (2014); Deshpande et al.",
      "startOffset" : 259,
      "endOffset" : 318
    }, {
      "referenceID" : 5,
      "context" : "a large region of parameters, the posterior marginal expectations of the underlying signal components (often assumed intractable to compute) can be obtained in the leading order in n using a polynomial-time algorithm called approximate message-passing (AMP) [Rangan and Fletcher (2012); Deshpande and Montanari (2014); Deshpande et al. (2015); Lesieur et al.",
      "startOffset" : 319,
      "endOffset" : 343
    }, {
      "referenceID" : 5,
      "context" : "a large region of parameters, the posterior marginal expectations of the underlying signal components (often assumed intractable to compute) can be obtained in the leading order in n using a polynomial-time algorithm called approximate message-passing (AMP) [Rangan and Fletcher (2012); Deshpande and Montanari (2014); Deshpande et al. (2015); Lesieur et al. (2015b)].",
      "startOffset" : 319,
      "endOffset" : 367
    }, {
      "referenceID" : 1,
      "context" : "We also demonstrate the existence of a region where both AMP and spectral methods [Baik et al. (2005)] fail to provide a good answer to the estimation problem, while it is nevertheless information theoretically possible to do so.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "This is made possible by a theorem of channel universality [Krzakala et al. (2016)] (already proven for community detection in [Deshpande et al.",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "(2016)] (already proven for community detection in [Deshpande et al. (2015)] and conjectured in [Lesieur et al.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "(2016)] (already proven for community detection in [Deshpande et al. (2015)] and conjectured in [Lesieur et al. (2015a)]).",
      "startOffset" : 52,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "Also, it was first shown in [Krzakala et al. (2016)] that for all n, I(S;W)/n≤minE∈[0,v] iRS(E; ∆), an inequality that we will use in the proof section.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "A fundamental consequence concerns the performance of the AMP algorithm [Rangan and Fletcher (2012)] for estimating s.",
      "startOffset" : 73,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "AMP has been analysed rigorously in [Bayati and Montanari (2011); Javanmard and Montanari (2013); Deshpande et al.",
      "startOffset" : 37,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "AMP has been analysed rigorously in [Bayati and Montanari (2011); Javanmard and Montanari (2013); Deshpande et al.",
      "startOffset" : 37,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "AMP has been analysed rigorously in [Bayati and Montanari (2011); Javanmard and Montanari (2013); Deshpande et al. (2015)] where it is shown that its asymptotic performance is tracked by state evolution.",
      "startOffset" : 37,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "A special case when si=±1∼Ber(1/2) already appeared in [Korada and Macris (2009)] where an equivalent spin glass model is analysed.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "A special case when si=±1∼Ber(1/2) already appeared in [Korada and Macris (2009)] where an equivalent spin glass model is analysed. Very recently, [Krzakala et al. (2016)] has generalized the results of [Korada and Macris (2009)] and, notably, obtained a generic matching upper bound.",
      "startOffset" : 56,
      "endOffset" : 171
    }, {
      "referenceID" : 9,
      "context" : "A special case when si=±1∼Ber(1/2) already appeared in [Korada and Macris (2009)] where an equivalent spin glass model is analysed. Very recently, [Krzakala et al. (2016)] has generalized the results of [Korada and Macris (2009)] and, notably, obtained a generic matching upper bound.",
      "startOffset" : 56,
      "endOffset" : 229
    }, {
      "referenceID" : 9,
      "context" : "A special case when si=±1∼Ber(1/2) already appeared in [Korada and Macris (2009)] where an equivalent spin glass model is analysed. Very recently, [Krzakala et al. (2016)] has generalized the results of [Korada and Macris (2009)] and, notably, obtained a generic matching upper bound. The same formula has been also rigorously computed following the study of AMP in [Deshpande and Montanari (2014)] for spike models (provided, however, that the signal was not too sparse) and in [Deshpande et al.",
      "startOffset" : 56,
      "endOffset" : 398
    }, {
      "referenceID" : 5,
      "context" : "The same formula has been also rigorously computed following the study of AMP in [Deshpande and Montanari (2014)] for spike models (provided, however, that the signal was not too sparse) and in [Deshpande et al. (2015)] for strictly symmetric community detection.",
      "startOffset" : 195,
      "endOffset" : 219
    }, {
      "referenceID" : 5,
      "context" : "The same formula has been also rigorously computed following the study of AMP in [Deshpande and Montanari (2014)] for spike models (provided, however, that the signal was not too sparse) and in [Deshpande et al. (2015)] for strictly symmetric community detection. For rank-one symmetric matrix estimation problems, AMP has been introduced by [Rangan and Fletcher (2012)], who also computed the state evolution formula to analyse its performance, generalizing techniques developed by [Bayati and Montanari (2011)] and [Javanmard and Montanari (2013)].",
      "startOffset" : 195,
      "endOffset" : 370
    }, {
      "referenceID" : 3,
      "context" : "For rank-one symmetric matrix estimation problems, AMP has been introduced by [Rangan and Fletcher (2012)], who also computed the state evolution formula to analyse its performance, generalizing techniques developed by [Bayati and Montanari (2011)] and [Javanmard and Montanari (2013)].",
      "startOffset" : 220,
      "endOffset" : 248
    }, {
      "referenceID" : 3,
      "context" : "For rank-one symmetric matrix estimation problems, AMP has been introduced by [Rangan and Fletcher (2012)], who also computed the state evolution formula to analyse its performance, generalizing techniques developed by [Bayati and Montanari (2011)] and [Javanmard and Montanari (2013)].",
      "startOffset" : 220,
      "endOffset" : 285
    }, {
      "referenceID" : 3,
      "context" : "For rank-one symmetric matrix estimation problems, AMP has been introduced by [Rangan and Fletcher (2012)], who also computed the state evolution formula to analyse its performance, generalizing techniques developed by [Bayati and Montanari (2011)] and [Javanmard and Montanari (2013)]. State evolution was further studied by [Deshpande and Montanari (2014)] and [Deshpande et al.",
      "startOffset" : 220,
      "endOffset" : 358
    }, {
      "referenceID" : 3,
      "context" : "For rank-one symmetric matrix estimation problems, AMP has been introduced by [Rangan and Fletcher (2012)], who also computed the state evolution formula to analyse its performance, generalizing techniques developed by [Bayati and Montanari (2011)] and [Javanmard and Montanari (2013)]. State evolution was further studied by [Deshpande and Montanari (2014)] and [Deshpande et al. (2015)].",
      "startOffset" : 220,
      "endOffset" : 388
    }, {
      "referenceID" : 3,
      "context" : "For rank-one symmetric matrix estimation problems, AMP has been introduced by [Rangan and Fletcher (2012)], who also computed the state evolution formula to analyse its performance, generalizing techniques developed by [Bayati and Montanari (2011)] and [Javanmard and Montanari (2013)]. State evolution was further studied by [Deshpande and Montanari (2014)] and [Deshpande et al. (2015)]. In [Lesieur et al. (2015b,a)], the generalization to larger rank was also considered. The general formula proposed by [Lesieur et al. (2015a)] for the conditional entropy and the MMSE on the basis of the heuristic cavity method from statistical physics was not demonstrated in full generality.",
      "startOffset" : 220,
      "endOffset" : 532
    }, {
      "referenceID" : 10,
      "context" : "leaving a gap with the statistical physics conjectured formula (and rigorous upper bound from [Krzakala et al. (2016)]).",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "Our proof technique combines recent rigorous results in coding theory along the study of capacity-achieving spatially coupled codes [Hassani et al. (2010); Kudekar et al.",
      "startOffset" : 133,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "Our proof technique combines recent rigorous results in coding theory along the study of capacity-achieving spatially coupled codes [Hassani et al. (2010); Kudekar et al. (2011); Yedla et al.",
      "startOffset" : 133,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "Our proof technique combines recent rigorous results in coding theory along the study of capacity-achieving spatially coupled codes [Hassani et al. (2010); Kudekar et al. (2011); Yedla et al. (2014); Barbier et al.",
      "startOffset" : 133,
      "endOffset" : 199
    }, {
      "referenceID" : 2,
      "context" : "(2014); Barbier et al. (2016)] with other progress, coming from developments in mathematical physics putting on a rigorous basis predictions of spin glass theory [Guerra (2005)].",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "(2014); Barbier et al. (2016)] with other progress, coming from developments in mathematical physics putting on a rigorous basis predictions of spin glass theory [Guerra (2005)].",
      "startOffset" : 8,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "For small ρ our results are consistent with the known optimal and algorithmic thresholds predicted in sparse PCA [Amini and Wainwright (2008); Berthet and Rigollet (2013)], that treats the case of sub-extensive ρ=O(1) values.",
      "startOffset" : 114,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "For small ρ our results are consistent with the known optimal and algorithmic thresholds predicted in sparse PCA [Amini and Wainwright (2008); Berthet and Rigollet (2013)], that treats the case of sub-extensive ρ=O(1) values.",
      "startOffset" : 114,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "For small ρ our results are consistent with the known optimal and algorithmic thresholds predicted in sparse PCA [Amini and Wainwright (2008); Berthet and Rigollet (2013)], that treats the case of sub-extensive ρ=O(1) values. Another interesting line of work for such probabilistic models appeared in the context of random matrix theory (see [Baik et al. (2005)] and references therein) and predicts that a sharp phase transition occurs at a critical value of the noise ∆spectral=ρ below which an outlier eigenvalue (and its principal eigenvector) has a positive correlation with the hidden signal.",
      "startOffset" : 114,
      "endOffset" : 362
    }, {
      "referenceID" : 0,
      "context" : "For small ρ our results are consistent with the known optimal and algorithmic thresholds predicted in sparse PCA [Amini and Wainwright (2008); Berthet and Rigollet (2013)], that treats the case of sub-extensive ρ=O(1) values. Another interesting line of work for such probabilistic models appeared in the context of random matrix theory (see [Baik et al. (2005)] and references therein) and predicts that a sharp phase transition occurs at a critical value of the noise ∆spectral=ρ below which an outlier eigenvalue (and its principal eigenvector) has a positive correlation with the hidden signal. For larger noise values the spectral distribution of the observation is indistinguishable from that of the pure random noise. We now consider the problem of detecting two communities (groups) with different sizes ρn and (1−ρ)n, that generalizes the one considered in [Deshpande et al. (2015)].",
      "startOffset" : 114,
      "endOffset" : 891
    }, {
      "referenceID" : 9,
      "context" : "The spatially coupled construction used here is very similar to the one used for the coupled Curie-Weiss model [Hassani et al. (2010)].",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "The analysis of AMP applies to the coupled system as well [Bayati and Montanari (2011); Javanmard and Montanari (2013)] and it can be shown that the performance of AMP is assessed by state evolution.",
      "startOffset" : 59,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "The analysis of AMP applies to the coupled system as well [Bayati and Montanari (2011); Javanmard and Montanari (2013)] and it can be shown that the performance of AMP is assessed by state evolution.",
      "startOffset" : 59,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "Thus using a matrix version of the well known I-MMSE relation [Guo et al. (2005)] we get",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "But we know I(S;W)/n≤minE∈[0,v] iRS(E; ∆) [Krzakala et al. (2016)], thus we already get (3) for ∆≤∆AMP.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "[Krzakala et al. (2016)]) and the law of large numbers we can show limn→+∞Mmmsen≤ limn→+∞(v−(v−Vmmsen(∆))).",
      "startOffset" : 1,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "Concentration techniques similar to [Korada and Macris (2009)] suggest that the equality in fact holds (for ∆ 6= ∆RS) but there are technicalities that prevent us from completing the proof of equality.",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "Existence of the limit follows by a subadditivity inequality which itself is proven by a similar interpolation [Guerra (2005)].",
      "startOffset" : 112,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "The main intuition behind the proof is to use a “potential function” whose “energy” can be lowered by small perturbation of a fixed point that would go above Egood(∆) [Yedla et al. (2014); Barbier et al.",
      "startOffset" : 168,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "(2014); Barbier et al. (2016)].",
      "startOffset" : 8,
      "endOffset" : 30
    } ],
    "year" : 2016,
    "abstractText" : "Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available. Consider the following probabilistic rank-one matrix estimation problem: one has access to noisy observations w=(wij)i,j=1 of the pair-wise product of the components of a vector s=(s1, . . . , sn)∈R with i.i.d components distributed as Si∼P0, i=1, . . . , n. The entries of w are observed through a noisy element-wise (possibly non-linear) output probabilistic channel Pout(wij |sisj/ √ n). The goal is to estimate the vector s from w assuming that both P0 and Pout are known and independent of n (noise is symmetric so that wij =wji). Many important problems in statistics and machine learning can be expressed in this way, such as sparse PCA [Zou et al. (2006)], the Wigner spike model [Johnstone and Lu (2012); Deshpande and Montanari (2014)], community detection [Deshpande et al. (2015)] or matrix completion [Candès and Recht (2009)]. Proving a result initially derived by a heuristic method from statistical physics, we give an explicit expression for the mutual information and the information theoretic minimal mean-square-error (MMSE) in the asymptotic n→+∞ limit. Our results imply that for 1 ar X iv :1 60 6. 04 14 2v 1 [ cs .I T ] 1 3 Ju n 20 16 a large region of parameters, the posterior marginal expectations of the underlying signal components (often assumed intractable to compute) can be obtained in the leading order in n using a polynomial-time algorithm called approximate message-passing (AMP) [Rangan and Fletcher (2012); Deshpande and Montanari (2014); Deshpande et al. (2015); Lesieur et al. (2015b)]. We also demonstrate the existence of a region where both AMP and spectral methods [Baik et al. (2005)] fail to provide a good answer to the estimation problem, while it is nevertheless information theoretically possible to do so. We illustrate our theorems with examples and also briefly discuss the implications in terms of computational complexity. 1. Setting and main results 1.1 The additive white Gaussian noise setting A standard and natural setting is the case of additive white Gaussian noise (AWGN) of known variance ∆, wij = sisj √ n + zij √ ∆, (1) where z=(zij)i,j=1 is a symmetric matrix with i.i.d entries Zij∼N (0, 1), 1≤ i≤j≤n. Perhaps surprisingly, it turns out that this Gaussian setting is sufficient to completely characterize all the problems discussed in the introduction, even if these have more complicated output channels. This is made possible by a theorem of channel universality [Krzakala et al. (2016)] (already proven for community detection in [Deshpande et al. (2015)] and conjectured in [Lesieur et al. (2015a)]). This theorem states that given an output channel Pout(w|y), such that logPout(w|y = 0) is three times differentiable with bounded second and third derivatives, then the mutual information satisfies I(S;W)=I(S;SST/ √ n+Z √ ∆)+O( √ n), where ∆ is the inverse Fisher information (evaluated at y = 0) of the output channel: ∆−1 := EPout(w|0)[(∂y logPout(W |y)|y=0)]. Informally, this means that we only have to compute the mutual information for an AWGN channel to take care of a wide range of problems, which can be expressed in terms of their Fisher information. In this paper we derive rigorously, for a large class of signal distributions P0, an explicit one-letter formula for the mutual information per variable I(S;W)/n in the asymptotic limit n→+∞.",
    "creator" : "LaTeX with hyperref package"
  }
}