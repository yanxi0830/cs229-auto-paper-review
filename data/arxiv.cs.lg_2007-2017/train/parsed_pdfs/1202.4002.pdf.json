{
  "name" : "1202.4002.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalized Principal Component Analysis (GPCA)",
    "authors" : [ "René Vidal" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "unknown and varying dimensions from sample data points. We represent the subspaces with a set of homogeneous polynomials whose degree is the number of subspaces and whose derivatives at a data point give normal vectors to the subspace passing through the point. When the number of subspaces is known, we show that these polynomials can be estimated linearly from data; hence, subspace segmentation is reduced to classifying one point per subspace. We select these points optimally from the data set by minimizing certain distance function, thus dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recovered by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA that deal with data in a highdimensional space and with an unknown number of subspaces are also presented. Our experiments on low-dimensional data show that GPCA outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspaces and Expectation Maximization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3Dmotion segmentation from point correspondences in multiple affine views.\nIndex Terms—Principal component analysis (PCA), subspace segmentation, Veronese map, dimensionality reduction, temporal\nvideo segmentation, dynamic scenes and motion segmentation."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "PRINCIPAL Component Analysis (PCA) [12] refers to theproblem of fitting a linear subspace S IRD of unknown dimension d < D to N sample points fxjgNj¼1 in S. This problem shows up in a variety of applications inmany fields, e.g., pattern recognition, data compression, regression, image processing, etc., and can be solved in a remarkably simple way from the singular value decomposition (SVD) of the (mean-subtracted) data matrix ½x1; x2; . . . ; xN 2 IRD N .1 With noisy data, this linear algebraic solution has the geometric interpretation of minimizing the sum of the squared distances from the (noisy) data points xj to their projections ~xj in S.\nIn addition to these algebraic and geometric interpretations, PCA can also be understood in a probabilistic manner. In Probabilistic PCA [20] (PPCA), the noise is assumed to be drawn from an unknown distribution and the problem becomes one of identifying the subspace and distribution parameters in a maximum-likelihood sense. When the noise\ndistribution is Gaussian, the algebro-geometric and prob-\nabilistic interpretations coincide [2]. However, when the\nnoise distribution is non-Gaussian, the solution toPPCA is no\nlonger linear, as shown in [2], where PCA is generalized to\narbitrary distributions in the exponential family. Another extension of PCA is nonlinear principal compo-\nnents (NLPCA) or Kernel PCA (KPCA),which is the problem of identifying a nonlinear manifold from sample points. The standard solution toNLPCA [16] is based on first embedding the data into a higher-dimensional feature space F and then applying standard PCA to the embedded data. Since the dimension of F can be large, a more practical solution is obtained from the eigenvalue decomposition of the so-called kernel matrix; hence, the name KPCA. One of the disadvantages of KPCA is that, in practice, it is difficult to determine which kernel function to use because the choice of the kernel naturally depends on the nonlinear structure of the manifold to be identified. In fact, learning kernels is an active topic of research in machine learning. To the best of our knowledge, our work is the first one to prove analytically that the Veronese map (a polynomial embedding) is the natural embedding for data lying in a union of multiple subspaces.\nIn this paper, we consider the following alternative\nextension of PCA to the case of data lying in a union of subspaces, as illustrated in Fig. 1 for two subspaces of IR3.\nProblem (Subspace Segmentation). Given a set of pointsX ¼ fxj2 IRDgNj¼1 drawn from n 1 different linear subspaces fSi IRDgni¼1 of dimension di ¼ dimðSiÞ, 0 < di < D, without knowing which points belong to which subspace:\n1. find the number of subspaces n and their dimensions fdigni¼1, 2. find a basis for each subspace Si (or for S ? i ), and 3. group the N sample points into the n subspaces.\n. R. Vidal is with the Center for Imaging Science, Department of Biomedical Engineering, The Johns Hopkins University, 308B Clark Hall, 3400 N. Charles Street, Baltimore, MD 21218. E-mail: rvidal@cis.jhu.edu. . Y. Ma is with the Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, 145 Coordinated Science Laboratory, 1308 West Main Street, Urbana, IL 61801. E-mail: yima@uiuc.edu. . S. Sastry is with the Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, 514 Cory Hall, Berkeley, CA 94720. E-mail: sastry@eecs.berkeley.edu.\nManuscript received 21 May 2004; revised 11 Mar. 2005; accepted 10 May 2005; published online 13 Oct. 2005. Recommended for acceptance by Y. Amit. For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number TPAMI-0253-0504.\n1. In the context of stochastic signal processing, PCA is also known as the Karhunen-Loeve transform [18]; in the applied statistics literature, SVD is also known as the Eckart and Young decomposition [4].\n0162-8828/05/$20.00 2005 IEEE Published by the IEEE Computer Society"
    }, {
      "heading" : "1.1 Previous Work on Subspace Segmentation",
      "text" : "Subspace segmentation is a fundamental problem in many applications in computer vision (e.g., image/motion/video segmentation), image processing (e.g., image representation and compression), and systems theory (e.g., hybrid system identification), which is usually regarded as “chicken-andegg.” If the segmentation of the data was known, one could easily fit a single subspace to each group of points using standard PCA. Conversely, if the subspace bases were known, one could easily find the data points that best fit each subspace. Since, in practice, neither the subspace bases nor the segmentation of the data are known, most existing methods randomly choose a basis for each subspace and then iterate between data segmentation and subspace estimation. This can be done using, e.g., K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19]. Unfortunately, most iterative methods are, in general, very sensitive to initialization; hence, they may not converge to the global optimum [21].\nThe need for initialization methods has motivated the recent development of algebro-geometric approaches to subspace segmentation that do not require initialization. In [13] (see, also, [3]), it is shown that when the subspaces are orthogonal, of equal dimension d, and intersect only at the origin, which implies thatD nd, one can use the SVD of the data to define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques.Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28]. The latter case has been addressed in an ad hoc fashion by using clustering algorithms such as K-means, spectral clustering, or EM [14], [28] to segment the data and PCA to obtain a basis for each group. The only algebraic approaches that deal with arbitrary intersections are [17], which studies the case of two planes in IR3 and [24] which studies the case of subspaces of codimension one, i.e., hyperplanes, and shows that hyperplane segmentation is equivalent to homogeneous polynomial factorization. Our previous work [23] extended this framework to subspaces of unknown and possibly different dimensions under the additional assumption that the number of subspaces is known. This paper unifies the results of [24] and [23] and extends to the case inwhich both the number anddimensions of the subspaces are unknown."
    }, {
      "heading" : "1.2 Paper Organization and Contributions",
      "text" : "In this paper, we propose an algebro-geometric approach to subspace segmentation called Generalized Principal Component Analysis (GPCA), which is based on fitting, differentiating, and dividing polynomials. Unlike prior work, we do not restrict the subspaces to be orthogonal, trivially intersecting, or with known and equal dimensions. Instead, we address the most general case of an arbitrary number of subspaces of unknown and possibly different dimensions (e.g., Fig. 1) and with arbitrary intersections among the subspaces.\nIn Section 2,wemotivate andhighlight the key ideas of our approach by solving the simple example shown in Fig. 1.\nIn Section 3, we generalize this example to the case of data lying in a known number of subspaces with unknown and possibly different dimensions. We show that one can represent the union of all subspaces as the zero set of a collection of homogeneous polynomials whose degree is the number of subspaces and whose factors encode normal vectors to the subspaces. The coefficients of these polynomials can be linearly estimated from sample data points on the subspaces and the set of normal vectors to each subspace can be obtained by evaluating the derivatives of these polynomials at any point lying on the subspace. Therefore, subspace segmentation is reduced to the problem of classifying onepoint per subspace.When those points are given (e.g., in semisupervised learning), this means that in order to learn the mixture of subspaces, it is sufficient to have one positive example per class. When all the data points are unlabeled (e.g., in unsupervised learning), we use polynomial division to recursively select points in the data set that minimize their distance to the algebraic set; hence, dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recoveredby applying standardPCA to thederivatives of thepolynomials (normalvectors) at those points. The final result is a global, noniterative subspace segmentation algorithm based on simple linear and polynomial algebra.\nIn Section 4, we discuss some extensions of our approach. We show how to deal with low-dimensional subspaces of a high-dimensional space via a linear projection onto a lowdimensional subspace that preserves the number and dimensions of the subspaces.Wealso showhow togeneralize the basic GPCA algorithm to the case in which the number of subspaces is unknownvia a recursive partitioning algorithm.\nIn Section 5, we present experiments on low-dimensional data showing thatGPCAgives about half the error of existing algebraic algorithms based on polynomial factorization, and improves the performance of iterative techniques, such as K-subspaces and EM, by about 50 percent with respect to random initialization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3D motion segmentation from point correspondences in multiple affine views."
    }, {
      "heading" : "2 AN INTRODUCTORY EXAMPLE",
      "text" : "Imagine that we are given data in IR3 drawn from a line S1 ¼ fx : x1 ¼ x2 ¼ 0g and a plane S2 ¼ fx : x3 ¼ 0g, as shown in Fig. 1. We can describe the two subspaces as\nS1 [ S2 ¼ fx : ðx1 ¼ x2 ¼ 0Þ _ ðx3 ¼ 0Þg ¼ fx : ðx1x3 ¼ 0Þ ^ ðx2x3 ¼ 0Þg:\nTherefore, even though each individual subspace is de-\nscribedwithpolynomialsofdegreeone (linear equations), the\nmixture of two subspaces is described with two polynomials of degree two, namely,p21ðxÞ ¼ x1x3 andp22ðxÞ ¼ x2x3.More generally, any two linear subspaces in IR3 can be represented\nas the set of points satisfying some polynomials of the form\nc1x 2 1 þ c2x1x2 þ c3x1x3 þ c4x22 þ c5x2x3 þ c6x23 ¼ 0:\nAlthough these polynomials are nonlinear in each data point ½x1; x2; x3 T , they are actually linear in the coefficient vector c ¼ ½c1; . . . ; c6 T . Therefore, given enough data points, one can linearly fit these polynomials to the data.\nGiven the collection of polynomials that vanish on the\ndata points, we would like to compute a basis for each subspace. In our example, let P2ðxÞ ¼ ½p21ðxÞ; p22ðxÞ and consider the derivatives of P2ðxÞ at two points in each of the subspaces y1 ¼ ½0; 0; 1 T 2 S1 and y2 ¼ ½1; 1; 0 T 2 S2:\nDP2ðxÞ ¼ x3 0 0 x3\nx1 x2\n2 64\n3 75)\nDP2ðy1Þ ¼ 1 0 0 1\n0 0\n2 64\n3 75; DP2ðy2Þ ¼ 0 0 0 0\n1 1\n2 64\n3 75:\nNote that the columns of DP2ðy1Þ span S?1 and the columns of DP2ðy2Þ span S?2 (see Fig. 1). Also, the dimension of the line is d1 ¼ 3 rankðDP2ðy1ÞÞ ¼ 1 and the dimension of the plane is d2 ¼ 3 rankðDP2ðy2ÞÞ ¼ 2. Thus, if we are given one point in each subspace, we can obtain the subspace bases\nand their dimensions from the derivatives of the polynomials at\nthese points. The final question is to find one point per subspace, so that\nwe can obtain the normal vectors from the derivatives ofP2 at\nthose points.With perfect data,wemay choose a first point as\nany of the points in the data set.With noisy data, wemay first define a distance from any point in IR3 to one of the subspaces, e.g., the algebraic distance d2ðxÞ2 ¼ p21ðxÞ2 þ p22ðxÞ2 ¼ ðx21 þ x22Þx23, and then choose a point in the data set that minimizes this distance. Say, we pick y2 2 S2 as such point. We can then compute the normal vector b2 ¼ ½0; 0; 1 T to S2 fromDP ðy2Þ. As it turns out, we can pick a second point in S1 but not in S2 by polynomial division. We can just divide the original polynomials of degree n ¼ 2 by ðbT2 xÞ to obtain polynomials of degree n 1 ¼ 1:\np11ðxÞ ¼ p21ðxÞ bT2 x ¼ x1 and p12ðxÞ ¼ p22ðxÞ bT2 x ¼ x2:\nSince these new polynomials vanish on S1 but not on S2, we can find a point y1 in S1 but not in S2, as a point in the data set that minimizes d1ðxÞ2 ¼ p11ðxÞ2 þ p12ðxÞ2 ¼ x21 þ x22. As wewill show in the next section, one can also solve the\nmore general problem of segmenting a union of n subspaces fSi IRDgni¼1 of unknown and possibly different dimensions fdigni¼1 by polynomial fitting (Section 3.3), differentiation (Section 3.4), and division (Section 3.5)."
    }, {
      "heading" : "3 GENERALIZED PRINCIPAL COMPONENT ANALYSIS",
      "text" : "In this section, we derive a constructive algebro-geometric solution to the subspace segmentation problem when the number of subspaces n is known. The case in which the number of subspaces is unknown will be discussed in Section 4.2. Our algebro-geometric solution is summarized in the following theorem:\nTheorem 1 (Generalized Principal Component Analysis). A union of n subspaces of IRD can be represented with a set of homogeneous polynomials of degree n in D variables. These polynomials can be estimated linearly given enough sample points in general position in the subspaces. A basis for the complement of each subspace can be obtained from the derivatives of these polynomials at a point in each of the subspaces. Such points can be recursively selected via polynomial division. Therefore, the subspace segmentation problem ismathematically equivalent to fitting, differentiating and dividing a set of homogeneous polynomials."
    }, {
      "heading" : "3.1 Notation",
      "text" : "Let x be a vector in IRD. A homogeneous polynomial of degree n in x is a polynomial pnðxÞ such that pnð xÞ ¼ npnðxÞ for all in IR. The space of all homogeneous polynomials of degree n in D variables is a vector space of dimension MnðDÞ ¼ nþD 1D 1 . A particular basis for this space is given by all the monomials of degree n in D variables, that is xI ¼ xn11 x n2 2 x nD D with 0 nj n for j ¼ 1; . . . ; D, and n1 þ n2 þ þ nD ¼ n. Thus, each homogeneous polynomial can be written as a linear combination of the monomials xI with coefficient vector cn 2 IRMnðDÞ as\npnðxÞ ¼ cTn nðxÞ ¼ X cn1;n2;...;nDx n1 1 x n2 2 x nD D ; ð1Þ\nwhere n : IR D ! IRMnðDÞ is the Veronese map of degree n [7], also known as the polynomial embedding in machine learning, defined as n : ½x1; . . . ; xD T 7!½. . . ; xI ; . . . T with I chosen in the degree-lexicographic order.\nExample1(TheVeronesemapofdegree2inthreevariables). If x ¼ ½x1; x2; x3 T 2 IR3, the Veronese map of degree 2 is given by:\n2ðxÞ ¼ ½x21; x1x2; x1x3; x22; x2x3; x23 T 2 IR6: ð2Þ\n3.2 Representing a Union of n Subspaces with a Set of Homogeneous Polynomials of Degree n We represent a subspace Si IRD of dimension di, where 0 < di < D, by choosing a basis\nBi¼: ½bi1; . . . ; biðD diÞ 2 IRD ðD diÞ ð3Þ\nfor its orthogonal complement S?i . One could also choose a basis forSi directly, especially if di D. Section 4.1will show that the problem can be reduced to the caseD ¼ maxfdig þ 1; hence, the orthogonal representation is more convenient if maxfdig is small.With this representation, each subspace can be expressed as the set of points satisfying D di linear equations (polynomials of degree one), that is,\nSi ¼ fx 2 IRD : BTi x ¼ 0g ¼ n x 2 IRD: D̂ di\nj¼1 ðbTijx ¼ 0Þ\no : ð4Þ\nFor affine subspaces (which do not necessarily pass through the origin), we use homogeneous coordinates so that they become linear subspaces.\nWe now demonstrate that one can represent the union of n subspaces fSi IRDgni¼1 with a set of polynomials whose degree is n rather than one. To see this, notice that x 2 IRD belongs to [ni¼1Si if and only if it satisfies ðx 2 S1Þ _ . . . _ ðx 2 SnÞ. This is equivalent to\n_n i¼1 ðx 2 SiÞ , _n i¼1 D̂ di j¼1 ðbTijx ¼ 0Þ , ^ _n i¼1 ðbTi ðiÞx ¼ 0Þ; ð5Þ\nwhere the right-hand side is obtained by exchanging ands and ors using De Morgan’s laws and is a particular choice of one normal vector bi ðiÞ from each basis Bi. Since each one of the Qn i¼1ðD diÞ equations in (5) is of the form\n_n i¼1 ðbTi ðiÞx ¼ 0Þ , Yn i¼1 ðbTi ðiÞxÞ ¼ 0 , ðpn ðxÞ ¼ 0Þ; ð6Þ\ni.e., a homogeneouspolynomial ofdegreen inDvariables,we can write each polynomial as a linear combination of monomials xI with coefficient vector cn 2 IRMnðDÞ, as in (1). Therefore, we have the following result.\nTheorem 2 (Representing Subspaces with Polynomials). A union of n subspaces can be represented as the set of points satisfying a set of homogeneous polynomials of the form\npnðxÞ ¼ Yn i¼1 ðbTi xÞ ¼ cTn nðxÞ ¼ 0; ð7Þ\nwhere bi 2 IRD is a normal vector to the ith subspace.\nThe importance of Theorem 2 is that it allows us to solve the “chicken-and-egg” problem described in Section 1.1 algebraically, because the polynomials in (7) are satisfied by all data points, regardless of which point belongs to which subspace. We can then use all the data to estimate all the subspaces,withoutprior segmentationandwithouthaving to iterate between data segmentation and model estimation, as we will show in Sections 3.3, 3.4, and 3.5."
    }, {
      "heading" : "3.3 Fitting Polynomials to Data Lying in Multiple Subspaces",
      "text" : "Thanks to Theorem 2, the problem of identifying a union of n subspaces fSigni¼1 from a set of data pointsX¼\n: fxjgNj¼1 lying in the subspaces is equivalent to solving for the normal bases fBign1¼1 from the set of nonlinear equations in (6). Although these polynomial equations are nonlinear in each data point x, they are actually linear in the coefficient vector cn. Indeed, since each polynomial pnðxÞ ¼ cTn nðxÞ must be satisfied by every data point, we have cTn nðxjÞ ¼ 0 for all j ¼ 1; . . . ; N . We use In to denote the space of coefficient vectors cn of all homogeneous polynomials that vanish on the n subspaces. Obviously, the coefficient vectors of the factorizable polynomials defined in (6) span a (possibly proper) subspace in In:\nspan fpn g In: ð8Þ\nAs every vector cn in In represents a polynomial that vanishes on all the data points (on the subspaces), the vector must satisfy the system of linear equations\ncTnV nðDÞ¼ : cTn ½ nðx1Þ . . . nðxNÞ ¼ 0T : ð9Þ\nV nðDÞ 2 IRMnðDÞ N is called the embedded data matrix. Obviously, we have the relationship\nIn nullðV nðDÞÞ:\nAlthough we know that the coefficient vectors cn of vanishing polynomials must lie in the left null space of V nðDÞ, we do not know if every vector in the null space corresponds to a polynomial that vanishes on the subspaces. Therefore, we would like to study under what conditions on the data points, we can solve for the unique mn¼: dimðInÞ independent polynomials that vanish on the subspaces from the null space of V n. Clearly, a necessary condition is to have N Pn i¼1 di points in[ni¼1Si, with at least di points in general positionwithin each subspace Si, i.e., the di points must span Si. However, because we are representing each polynomial pnðxÞ linearly via the coefficient vector cn, we need a number of samples such that a basis for In can be uniquely recovered from nullðV nðDÞÞ. That is, the number of samplesN must be such that\nrankðV nðDÞÞ ¼MnðDÞ mn MnðDÞ 1: ð10Þ\nTherefore, if the number of subspaces n is known, we can recover In from nullðV nðDÞÞ given N MnðDÞ 1 points in general position. A basis of In can be computed linearly as the set ofmn left singular vectors of V nðDÞ associatedwith itsmn zero singular values. Thus, we obtain a basis of polynomials of degree n, say fpn‘gmn‘¼1, that vanish on the n subspaces. Remark 1 (GPCA and Kernel PCA). KernelPCAidentifiesa\nmanifold from sample data by embedding the data into a higher-dimensional feature space F such that the embedded data points lie in a linear subspace of F . Unfortunately, there isnogeneralmethodology for finding the appropriate embedding for a particular problem becausetheembeddingnaturallydependsonthegeometry of the manifold. The above derivation shows that the commonly used polynomial embedding n is the appropriate embedding to use in KPCAwhen the original data lie in a union of subspaces, because the embedded data points f nðxjÞgNj¼1 lie in a subspace of IRMnðDÞ of dimension MnðDÞ mn, where mn ¼ dimðInÞ. Notice also that the matrix C ¼ V nðDÞV nðDÞT 2 IRMnðDÞ MnðDÞ is exactly the covariance matrix in the feature space and K ¼ V nðDÞTV nðDÞ 2 IRN N is the kernel matrix associatedwith theN embedded samples.\nRemark 2 (Estimation from Noisy Data). In the presence of moderatenoise,wecanstillestimate thecoefficientsofeach polynomial in a least-squares sense as the singular vectors of V nðDÞ associated with its smallest singular values. However, we cannot directly estimate the number of polynomials fromtherankofV nðDÞbecauseV nðDÞmaybe of full rank.We use model selection to determinemn as\nmn ¼ argmin m 2mþ1ðV nðDÞÞPm j¼1 2 j ðV nðDÞÞ þ m; ð11Þ\nwith jðV nðDÞÞ the jth singular vector of V nðDÞ and a parameter. An alternative way of selecting the correct linear model (in feature space) for noisy data can be found in [11].\nRemark 3 (Suboptimality in the Stochastic Case). Notice that, in the case of hyperplanes, the least-squares solution for cn is obtained by minimizing kcTV nðDÞk2 subject to kcnk ¼ 1. However, when n > 1 the so-found cn does not minimize the sum of least-square errorsP\nj mini¼1;...;nðbTi xjÞ 2. Instead, it minimizes a “weighted\nversion” of the least-square errors X j j min i¼1;...;n ðbTi xjÞ 2¼: X j Yn i¼1 ðbTi xjÞ 2 ¼ kcTV nðDÞk2; ð12Þ\nwhere the weight j is conveniently chosen so as to eliminate the minimization over i ¼ 1; . . . ; n. Such a “softening” of the objective function permits a global algebraic solution because the softened errordoes notdependon the membership of one point to one of the hyperplanes. This least-squares solution for cn offers a suboptimal approximation for the original stochastic objective when the variance of the noise is small. This solution can be used to initializeother iterativeoptimizationschemes(suchasEM) to further minimize the original stochastic objective."
    }, {
      "heading" : "3.4 Obtaining a Basis and the Dimension of Each Subspace by Polynomial Differentiation",
      "text" : "In this section, we show that one can obtain the bases fBigni¼1 for the complement of the n subspaces and their dimensions fdigni¼1 by differentiating all the polynomials obtained from the left null space of the embedded data matrix V nðDÞ. For the sake of simplicity, let us first consider the case of hyperplanes, i.e., subspaces of equal dimension di ¼ D 1, for i ¼ 1; . . . ; n. In this case, there is only one vector bi 2 IRD normal to subspace Si. Therefore, there is only one polynomial representing the n hyperplanes, namely, pnðxÞ ¼ ðbT1 xÞ ðbTnxÞ ¼ cTn nðxÞ and its coefficient vector cn can be computed as the unique vector in the left null space of V nðDÞ. Consider now the derivative of pnðxÞ\nDpnðxÞ ¼ @pnðxÞ @x ¼ @ @x Yn i¼1 ðbTi xÞ ¼ Xn i¼1 ðbiÞ Y ‘ 6¼i ðbT‘ xÞ; ð13Þ\nat a point yi 2 Si, i.e., yi is such that bTi yi ¼ 0. Then, all terms in (13), except the ith, vanish, because Q ‘ 6¼iðbT‘ yjÞ ¼ 0 for j 6¼ i, so that we can immediately obtain the normal vectors as\nbi ¼ DpnðyiÞ kDpnðyiÞk ; i ¼ 1; . . . ; n: ð14Þ\nTherefore, in a semisupervised learning scenario in which we are given only one positive example per class, the hyperplane segmentation problem can be solved analytically by evaluating the derivatives of pnðxÞ at the points with known labels.\nAs it turns out, the same principle applies to subspaces of arbitrary dimensions. This fact should come at no surprise. The zero set of each vanishing polynomial pn‘ is just a surface in IRD; therefore, the derivative of pn‘ at a point yi 2 Si,Dpn‘ðyiÞ, gives a vector normal to the surface. Since a union of subspaces is locally flat, i.e., in a neighborhood of yi the surface is merely the subspace Si, then the derivative at yi lies in the orthogonal complement S ? i of Si. By evaluating\nthe derivatives of all the polynomials in In at the same point yi, we obtain a set of normal vectors that span the orthogonal complement of Si, as stated in Theorem 3. Fig. 2 illustrates the theorem for the case of a plane and a line described in Section 2.\nTheorem 3 (Obtaining Subspace Bases and Dimensions\nby Polynomial Differentiation). Let In be (the space of coefficient vectors of) the set of polynomials of degree n that vanish on the n subspaces. If the data set X is such that dimðnullðV nðDÞÞÞ ¼ dimðInÞ ¼ mn and one point yi 2 Si but yi =2Sj for j 6¼ i is given for each subspace Si, then we have\nS?i ¼ span n @ @x cTn nðxÞ x¼yi ; 8cn 2 nullðV nðDÞÞ o : ð15Þ\nTherefore, the dimensions of the subspaces are given by\ndi ¼ D rank DPnðyiÞ for i ¼ 1; . . . ; n; ð16Þ\nwith PnðxÞ ¼ ½pn1ðxÞ; . . . ; pnmnðxÞ 2 IR1 mn and DPnðxÞ ¼ ½Dpn1ðxÞ; . . . ; DpnmnðxÞ 2 IRD mn .\nAs a consequence of Theorem 3, we already have the sketch of an algorithm for segmenting subspaces of arbitrary dimensions in a semisupervised learning scenario in which we are given one positive example per class fyi 2 Signi¼1:\n1. Compute a basis for the left null space of V nðDÞ using, for example, SVD. 2. Evaluate the derivatives of the polynomial cTn nðxÞ at yi for each cn in the basis of nullðV nðDÞÞ to obtain a set of normal vectors in S?i . 3. Compute a basis Bi for S ? i by applying PCA to the\nnormal vectors obtained in Step 2. PCA automatically gives the dimension of each subspace di ¼ dimðSiÞ. 4. Cluster the data by assigning point xj to subspace i if\ni ¼ arg min ‘¼1;...;n kBT‘ xjk: ð17Þ\nRemark 4 (Estimating the Bases from Noisy Data Points). Withamoderate levelofnoise inthedata,wecanstillobtain abasis for eachsubspaceandcluster thedataasabove.This is because we are applying PCA to the derivatives of the polynomials and both the coefficients of the polynomials and their derivatives depend continuously on the data.\nNotice also that we can obtain the dimension of each\nsubspace by looking at the singular values of thematrix of\nderivatives, similarly to (11).\nRemark 5 (Computing Derivatives of Homogeneous\nPolynomials). Notice that given cn the computation of the derivatives of pnðxÞ ¼ cTn nðxÞ does not involve taking derivatives of the (possibly noisy) data. For instance, one may compute the derivatives as @pnðxÞ@xk ¼ c T n nðxÞ @xk ¼ cTnEnk n 1ðxÞ, where Enk2 IRMnðDÞ Mn 1ðDÞ is a constant matrix that depends on the exponents of the different monomials in the Veronese map nðxÞ."
    }, {
      "heading" : "3.5 Choosing One Point per Subspace by Polynomial Division",
      "text" : "Theorem 3 demonstrates that one can obtain a basis for each S?i directly from the derivatives of the polynomials representing the union of subspaces. However, in order to\nproceed we need to have one point per subspace, i.e., we need to know the vectors fyigni¼1. In this section, we show how to select these n points in the\nunsupervised learning scenario in which we do not know the\nlabel for any of the data points. To this end, notice thatwe can\nalways choose a point yn lying on one of the subspaces, say Sn, by checking that PnðynÞ ¼ 0T . Since we are given a set of data pointsX ¼ fxjgnj¼1 lying on the subspaces, in principle, we could choose yn to be any of the data points. However, in the presence of noise and outliers, a random choice of yn may be far from the true subspaces. In Section 2, we chose a point in the data set X that minimizes kPnðxÞk. However, such a choice has the following problems:\n1. The value kPnðxÞk is merely an algebraic error, i.e., it does not represent the geometric distance from x to its closest subspace. In principle, finding the geometric distance from x to its closest subspace is a difficult problem because we do not know the normal bases fBigni¼1. 2. Points x lying close to the intersection of two or more subspaces could be chosen. However, at a point x in the intersection of two or more subspaces, we often have DpnðxÞ ¼ 0. Thus, one should avoid choosing such points, as they give very noisy estimates of the normal vectors.\nAs it turns out, one can avoid both of these problems\nthanks to the following lemma:\nLemma 1. Let ~x be the projection of x 2 IRD onto its closest subspace. The Euclidean distance from x to ~x is\nkx ~xk ¼ n ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi PnðxÞ DPnðxÞTDPnðxÞ y PnðxÞT r\nþO kx ~xk2 ;\nwhere PnðxÞ ¼ ½pn1ðxÞ; . . . ; pnmnðxÞ 2 IR1 mn , DPnðxÞ ¼ Dpn1ðxÞ; . . . ; DpnmnðxÞ½ 2 IRD mn , and Ay is the MoorePenrose inverse of A.\nProof. The projection ~x of a point x onto the zero set of the\npolynomials fpn‘gmn‘¼1 can be obtained as the solution of the following constrained optimization problem\nmin k~x xk2 subject to pn‘ð~xÞ ¼ 0 ‘ ¼ 1; . . . ;mn:\nð18Þ\nBy using Lagrange multipliers 2 IRmn , we can convert this problem into theunconstrained optimizationproblem\nmin ~x; k~x xk2 þ Pnð~xÞ : ð19Þ\nFrom the first order conditions with respect to ~x, we have 2ð~x xÞ þDPnð~xÞ ¼ 0. After multiplying on the left by ð~x xÞT and ðDPnð~xÞÞT , respectively, we obtain\nk~x xk2 ¼ 1 2 xTDPnð~xÞ ; and ð20Þ\n¼ 2 DPnð~xÞTDPnð~xÞ y DPnð~xÞTx; ð21Þ\nwherewe have used the fact that ðDPnð~xÞÞT ~x ¼ nPnð~xÞ ¼ 0 because D nð~xÞT ~x ¼ n nð~xÞ. After replacing (21) on (20), the squared distance from x to its closest subspace is given by\nk~x xk2 ¼ xTDPnð~xÞ DPnð~xÞTDPnð~xÞ y DPnð~xÞTx: ð22Þ\nAfter expanding in Taylor series about ~x ¼ x and noticing that DPnðxÞTx ¼ nPnðxÞT , we obtain\nk~x xk2 n2PnðxÞ DPnðxÞTDPnðxÞ y PnðxÞT ; ð23Þ\nwhich completes the proof. tu Thanks to Lemma 1, we can immediately choose a point\nyn lying in (close to) one of the subspaces and not in (far from) the other subspaces as\nyn ¼ arg min x2X :DPnðxÞ6¼0 PnðxÞ DPnðxÞTDPnðxÞ\ny PnðxÞT ; ð24Þ\nand then compute the basis Bn 2 IRD ðD dnÞ for S?n by applying PCA to DPnðynÞ.\nIn order to find a point yn 1 lying in (close to) one of the remaining ðn 1Þ subspaces but not in (far from) Sn, we find a new set of polynomials fpðn 1Þ‘ðxÞg defining the algebraic set [n 1i¼1 Si. In the case of hyperplanes, there is only one such polynomial, namely,\npn 1ðxÞ¼: ðb1xÞ ðbTn 1xÞ ¼ pnðxÞ bTnx ¼ cTn 1 n 1ðxÞ:\nTherefore, we can obtain pn 1ðxÞ by polynomial division. Notice thatdividingpnðxÞby bTnx is a linear problemof the form cTn 1RnðbnÞ ¼ cTn , where RnðbnÞ2 IRMn 1ðDÞ MnðDÞ. This is because solving for the coefficients of pn 1ðxÞ is equivalent to solving the equations ðbTnxÞðcTn 1 nðxÞÞ ¼ cTn nðxÞ, where bn and cn are already known. Example 2. If n ¼ 2 and b2 ¼ ½b1; b2; b3 T , then the matrix R2ðb2Þ is given by\nR2ðb2Þ ¼ b1 b2 b3 0 0 0 0 b1 0 b2 b3 0 0 0 b1 0 b2 b3\n2 4\n3 5 2 IR3 6:\nIn the case of subspaces of varying dimensions, in principle, we cannot simply divide the entries of the polynomial vector PnðxÞ by bTnx for any column bn of Bn\nbecause the polynomials fpn‘ðxÞg may not be factorizable.2 Furthermore, they do not necessarily have the common factor bTnx. The following theorem resolves this difficulty by showing how to compute the polynomials associated with the remaining subspaces [n 1i¼1 Si: Theorem 4 (Obtaining Points by Polynomial Division). Let\nIn be (the space of coefficient vectors of) the set of polynomials vanishing on the n subspaces. If the data set X is such that dimðnullðV nðDÞÞÞ ¼ dimðInÞ, then the set of homogeneous polynomials of degree ðn 1Þ that vanish on the algebraic set [n 1i¼1 Si is spanned by fcTn 1 n 1ðxÞg, where the vectors of coefficients cn 12 IRMn 1ðDÞ must satisfy\ncTn 1RnðbnÞV nðDÞ ¼ 0T ; for all bn 2 S?n : ð25Þ\nProof. We first show the necessity. That is, any polynomial of degree n 1, cTn 1 n 1ðxÞ, that vanishes on[n 1i¼1 Si satisfies the above equation. Since a point x in the original algebraic set [ni¼1Si belongs to either [n 1i¼1 Si or Sn, we have cTn 1 n 1ðxÞ ¼ 0 or bTnx ¼ 0 for all bn 2 S?n . Hence, pnðxÞ¼: ðcTn 1 n 1ðxÞÞðbTnxÞ ¼ 0. If we denote pnðxÞ as cTn nðxÞ, then the coefficient vector cn must be in nullðV nðDÞÞ. From cTn nðxÞ ¼ ðcTn 1 n 1ðxÞÞðbTnxÞ, the relationship between cn and cn 1 can be written as cTn 1RnðbnÞ ¼ cTn . Since cTnV nðDÞ ¼ 0T , cn 1 needs to satisfy the following linear system of equations cTn 1RnðbnÞV nðDÞ ¼ 0T .\nWe now show the sufficiency. That is, if cn 1 is a solution to (25), then for all bn 2 S?n , cTn ¼ cTn 1RnðbnÞ is in nullðV nðDÞÞ. From the construction of RnðbnÞ, we have cTn nðxÞ ¼ ðcTn 1 n 1ðxÞÞðbTnxÞ. Then, for every x 2 [n 1i¼1 Si but not in Sn, we have c T n 1 n 1ðxÞ ¼ 0 because there is a bn such that b T nx 6¼ 0. Therefore, cTn 1 n 1ðxÞ is a homogeneous polynomial of degree ðn 1Þ that vanishes on [n 1i¼1 Si. tu Thanks to Theorem 4, we can obtain a collection of polynomials fpðn 1Þ‘ðxÞgmn 1‘¼1 representing [n 1i¼1 Si from the intersection of the left null spaces of RnðbnÞV nðDÞ 2 IRMn 1ðDÞ N for all bn 2 S?n . We can then repeat the same procedure to find a basis for the remaining subspaces. We thus obtain the following Generalized Principal Component Analysis (GPCA) algorithm (Algorithm 1) for segmenting n subspaces of unknown and possibly different dimensions.\nAlgorithm 1 (GPCA: Generalized Principal Component Analysis) set V n ¼½ nðx1Þ; . . . ; nðxNÞ 2 IRMnðDÞ N ; for i ¼ n : 1 do solve cTV i ¼ 0 to obtain a basis fci‘gmi‘¼1 of nullðV iÞ, where the number of polynomials mi is obtained as in (11); set PiðxÞ ¼ ½pi1ðxÞ; . . . ; pimiðxÞ 2 IR1 mi , where pi‘ðxÞ ¼ cTi‘ iðxÞ for ‘ ¼ 1; . . . ;mi; do\nyi ¼ arg min x2X :DPiðxÞ6¼0 PiðxÞ DPiðxÞTDPiðxÞ\ny PiðxÞT ;\nBi ¼ PCA DPiðyiÞ ;\nV i 1 ¼ Riðbi1ÞV i; . . . ; Riðbi;D diÞV i\n;with bij columns of Bi;\nend do end for for j ¼ 1 : N do assign point xj to subspace Si if i ¼ argmin‘ kBT‘ xjk; end for\nRemark 6 (Avoiding Polynomial Division).Notice that one may avoid computing Pi for i < n by using a heuristic distance function to choose the points fyigni¼1. Since a point in [n‘¼iS‘ must satisfy kBTi xk kBTnxk ¼ 0, we can choose a point yi 1 on [i 1‘¼1S‘ as:\nyi 1¼ arg min x2X :DPnðxÞ6¼0\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi PnðxÞðDPnðxÞTDPnðxÞÞyPnðxÞT q þ\nkBTi xk kBTnxk þ ;\nwhere a small number > 0 is chosen to avoid cases in which both the numerator and the denominator are zero (e.g., with perfect data).\nRemark 7 (Robustness and Outlier Rejection). In practice, there could be points in X that are far away from any of the subspaces, i.e., outliers. By detecting and rejecting outliers, we can typically ensure a much better estimate of the subspaces. Many methods from robust statistics can be deployed to detect and reject the outliers [5], [11]. For instance, the function\nd2ðxÞ ¼ PnðxÞ DPnðxÞTDPnðxÞ y PnðxÞT\napproximates the squared distance of a point x to the subspaces. From the d2-histogram of the sample setX , we may exclude from X all points that have unusually large d2 values and use only the remaining sample points to reestimate the polynomials before computing the normals. For instance, if we assume that the sample points are drawn around each subspace from independent Gaussian distributions with a small variance 2, then d 2\n2 is approximately a 2-distribution with P iðD diÞ degrees of freedom. We can apply standard 2-test to reject sampleswhichdeviate significantly from this distribution. Alternatively, one can detect and reject outliers using Random Sample Consensus (RANSAC) [5]. One can chooseMnðDÞdata points at random, estimate a collection of polynomials passing through those points, determine their degree of support among the other points, and then choose the set of polynomials giving a large degree of support. This method is expected to be effective when MnðDÞ is small. An open problem is how to combine GPCA with methods from robust statistics in order to improve the robustness of GPCA to outliers."
    }, {
      "heading" : "4 EXTENSIONS TO THE BASIC GPCA ALGORITHM",
      "text" : "In this section, we discuss some extensions of GPCA that deal with practical situations such as low-dimensional subspaces of a high-dimensional space and unknown number of subspaces."
    }, {
      "heading" : "4.1 Projection and Minimum Representation",
      "text" : "When the dimension of the ambient space D is large, the complexity of GPCA becomes prohibitive because MnðDÞ is of the order nD. However, in most practical situations, we are interested in modeling the data as a union of subspaces\n2. Recall that we can only compute a basis for the null space of V nðDÞ, and that linear combinations of factorizable polynomials are not necessarily factorizable. For example, x21 þ x1x2 and x22 x1x2 are both factorizable, but their sum x21 þ x22 is not.\nof relatively small dimensions fdi Dg. In such cases, it seems rather redundant to use IRD to represent such a lowdimensional linear structure. One way of reducing the dimensionality is to linearly project the data onto a lowerdimensional (sub)space. An example is shown in Fig. 3, where two lines L1 and L2 in IR3 are projected onto a plane P. In this case, segmenting the two lines in the threedimensional space IR3 is equivalent to segmenting the two projected lines l1 and l2 in the plane P.\nIngeneral,wewilldistinguishbetweentwodifferentkinds of linear “projections.” The first kind corresponds to the case in which the span of all the subspaces is a proper subspace of the ambient space, i.e., spanð[ni¼1SiÞ IRD. In this case, one may simply apply the classic PCA algorithm to the original data to eliminate the redundant dimensions. The secondkind corresponds to the case in which the largest dimension of the subspaces, denoted by dmax, is strictly less thanD 1. When dmax is known, one may choose a ðdmax þ 1Þ-dimensional subspace P such that, by projecting onto this subspace:\nP : x 2 IRD 7! x 0 ¼ PðxÞ 2 P;\nthe dimension of each original subspace Si is preserved, 3 and the number of subspaces is preserved,4 as stated in the following theorem:\nTheorem 5 (Segmentation-Preserving Projections). If a set of vectors fxjg lie in n subspaces of dimensions fdigni¼1 in IRD and if P is a linear projection into a subspace P of dimension D0, then the points f PðxjÞg lie in n0 n linear subspaces of P of dimensions fd0i dig n i¼1. Furthermore, ifD > D\n0 > dmax, then there is an open and dense set of projections that preserve the number and dimensions of the subspaces, i.e.,n0 ¼ n and d0i ¼ di for i ¼ 1; . . . ; n. Thanks to Theorem 5, if we are given a data set X drawn from a union of low-dimensional subspaces of a highdimensional space, we can cluster the data set by first projecting X onto a generic subspace of dimension D0 ¼ dmax þ 1 and then applying GPCA to the projected subspaces, as illustrated with the following sequence of steps:\nX ! P X 0 !GPCA [ni¼1 PðSiÞ ! 1P [ni¼1 Si:\nHowever, even though we have shown that the set of ðdmax þ 1Þ-dimensional subspaces P IRD that preserve the\nnumber and dimensions of the subspaces is an open and dense set, it remains unclear what a “good” choice for P is, especiallywhen there is noise in thedata. Inpractice, onemay simply select a few random projections and choose the one that results in the smallest fitting error. Another alternative is to apply classic PCA to project onto a ðdmax þ 1Þ-dimensional affine subspace. The reader may refer to [1] for alternative ways of choosing a projection."
    }, {
      "heading" : "4.2 Identifying an Unknown Number of Subspaces of Unknown Dimensions",
      "text" : "The solution to the subspace segmentationproblemproposed in Section 3 assumes prior knowledge of the number of subspacesn. In practice, however, the number of subspaces n maynot be knownbeforehand, hence,we cannot estimate the polynomials representing the subspaces directly.\nFor the sake of simplicity, let us first consider the problem of determining the number of subspaces from a generic data set lying in a union of n different hyperplanes Si ¼ fx : bTi x ¼ 0g. From Section 3, we know that in this case there is a unique polynomial of degree n that vanishes in Z ¼ [ni¼1Si, namely, pnðxÞ ¼ ðbT1 xÞ ðbTnxÞ ¼ cTn nðxÞ and that its coefficient vector cn lives in the left null space of the embedded data matrix V nðDÞ defined in (9), hence, rankðV nÞ ¼MnðDÞ 1. Clearly, there cannot be a polynomial of degree i < n that vanishes in Z; otherwise, the data would lie in a union of i < n hyperplanes. This implies that V iðDÞmust be full rank for all i < n. In addition, notice that there is more than one polynomial of degree i > n that vanishes on Z, namely, any multiple of pn, hence, rankðV iðDÞÞ < MiðDÞ 1 if i > n. Therefore, the number of hyperplanes can be determined as the minimum degree such that the embedded data matrix drops rank, i.e.,\nn ¼ minfi : rankðV iðDÞÞ < MiðDÞg: ð26Þ\nConsider now the case of data lying in subspaces of equal dimension d1 ¼ d2 ¼ dn ¼ d < D 1. For example, consider a set of pointsX ¼ fxig lying in two lines in IR3, say,\nS1 ¼ fx : x2 ¼ x3 ¼ 0g and S2 ¼ fx : x1 ¼ x3 ¼ 0g: ð27Þ\nIfwe construct thematrix of embeddeddata points V nðDÞ for n ¼ 1, we obtain rankðV 1ð3ÞÞ ¼ 2 < 3 because all the points lie also in the plane x3 ¼ 0. Therefore, we cannot determine the number of subspaces as in (26) because we would obtain n ¼ 1, which is not correct. In order to determine the correct number of subspaces, recall from Section 4.1 that a linear projection onto a generic ðdþ 1Þ-dimensional subspace P preserves the number and dimensions of the subspaces. Therefore, if we project the data onto P, then the projected data lies in a union ofnhyperplanes of IRdþ1. By applying (26) to the projected data, we can obtain the number of subspaces from the embedded (projected) data matrix V iðdþ 1Þ as\nn ¼ minfi : rankðV iðdþ 1ÞÞ < Miðdþ 1Þg: ð28Þ\nOf course, in order to apply this projection, we need to know the common dimension d of all the subspaces. Clearly, ifwe project onto a subspace of dimension ‘þ 1 < dþ 1, then the number and dimension of the subspaces are no longer preserved. In fact, the projected data points lie in one subspace of dimension ‘þ 1, and V ið‘þ 1Þ is of full rank for all i (as long asMiðDÞ < N). Therefore, we can determine the dimension of the subspaces as the minimum integer ‘ such that there is a degree i for which V ið‘þ 1Þ drops rank, that is,\n3. This requires that P be transversal to each S?i , i.e., spanfP; S?i g ¼ IRD for every i ¼ 1; . . . ; n. Since n is finite, this transversality condition can be easily satisfied. Furthermore, the set of positions for P which violate the transversality condition is only a zero-measure closed set [9].\n4. This requires that all PðSiÞ be transversal to each other in P, which is guaranteed if we require P to be transversal to S?i \\ S?j for i; j ¼ 1; ::; n. All Ps which violate this condition form again only a zero-measure set.\nd ¼ minf‘ : 9 i 1 such rankðV ið‘þ 1ÞÞ < Mið‘þ 1Þg: ð29Þ\nIn summary, when the subspaces are of equal dimension d, both the number of subspaces n and their common dimension d can be retrieved from (28) and (29) and the subspace segmentation problem can be subsequently solved by first projecting the data onto a ðdþ 1Þ-dimensional subspace and then applying GPCA (Algorithm 1) to the projected data points.\nRemark 8. In the presence of noise, one may not be able to estimate d and n from (29) and (28), respectively, because the matrix V ið‘þ 1Þ may be of full rank for all i and ‘. Similarly to Remark 2, one can use model selection techniques to determine the rank of V ið‘Þ. However, in practice this requires searching for up to possibly ðD 1Þ values ford and dN=ðD 1Þevalues forn.Onemay refer to [11] for a more detailed discussion on selecting the best multiple-subspace model from noisy data, using modelselection criteria such as MML, MDL, AIC, and BIC.\nUnfortunately, the situation is not so simple for subspaces of different dimensions. For instance, imagine that in addition to the two lines S1 and S2 we are also given data points on a plane S3 ¼ fx : x1 þ x2 ¼ 0g, so that the overall configuration is similar to that shown in Fig. 4. In this case, we have rankðV 1ð3ÞÞ ¼ 3 6< 3, rankðV 2ð3ÞÞ ¼ 5 < 6, and rankðV 3ð3ÞÞ ¼ 6 < 10. Therefore, if we try to determine the number of subspaces as the degree of the embedding for which the embedded data matrix drops rank we would obtain n ¼ 2, which is incorrect again. The reason for this is clear: We can fit the data either with one polynomial of degree n ¼ 2, which corresponds to the plane S3 and the plane P spanned by the two lines, or with four polynomials of degree n ¼ 3, which vanish precisely on the two lines S1, S2, and the plane S3.\nTo resolve thedifficulty insimultaneouslydetermining the number and dimension of the subspaces, notice that the algebraic set Z ¼ [nj¼1Sj can be decomposed into irreducible subsets Sjs—an irreducible algebraic set is also called a variety—and that the decomposition of Z into fSjgnj¼1 is always unique [8]. Therefore, as long as we are able to correctly determine from the given sample points the underlying algebraic set Z or the associated radical ideal IðZÞ,5 in principle, the number of subspaces n and their dimensions fdjgnj¼1 can always be uniquely determined in a purely algebraic fashion. InFig. 4, for instance, the first interpretation (2 linesand1plane)wouldbe the right oneand the secondone (two planes) would be incorrect because the two lines, which span one of the planes, are not an irreducible algebraic set.\nHaving established that the problem of subspace segmentation is equivalent to decomposing the algebraic ideal\nassociated with the subspaces, we are left with deriving a computable scheme to achieve the goal of decomposing algebraic sets into varieties. To this end, notice that the set of all homogeneous polynomials that vanish inZ can be graded by degree as\nIðZÞ ¼ Im Imþ1 In ; ð30Þ where m n is the degree of the polynomial of minimum degree that fits all the data points. For each degree i m, we can evaluate the derivatives of the polynomials in I i at points in subspace Sj and denote the collection of derivatives as\nDi;j¼: span f[x2Sjfrf jx ; 8f 2 I igg; j ¼ 1; 2; . . . ; n: ð31Þ\nObviously, we have the following relationship:\nDi;j Diþ1;j S?j ; 8i m: ð32Þ\nTherefore, for each degree i m, we may compute a union of up to n subspaces,\nZi¼: D?i;1 [D?i;2 [ [D?i;n Z; ð33Þ\nwhich contains the original n subspaces. Therefore, we can further partition Zi to obtain the original subspaces. More specifically, in order to segment an unknown number of subspacesofunknownandpossiblydifferentdimensions,we can first search for the minimum degree i and dimension ‘ such that V ið‘þ 1Þ drops rank. In our example in Fig. 4, we obtain i ¼ 2 and ‘ ¼ 2. By applying GPCA to the data set projected onto an ð‘þ 1Þ-dimensional space, we partition the data into up to n subspaces Zi which contain the original n subspaces. In our example, we partition the data into two planes P and S3. Once these subspaces have been estimated, we can reapply the same process to each reducible subspace. In our example, theplanePwill be separated into two linesS1 and S2, while the plane S3 will remain unchanged. This recursive process stopswhen every subspace obtained canno longer be separated into lower-dimensional subspaces, or whenaprespecifiedmaximumnumber of subspacesnmax has been reached.\nWe summarize the above derivation with the recursive GPCA algorithm (Algorithm 2).\nAlgorithm 2 Recursive GPCA Algorithm n ¼ 1; repeat build a data matrix V nðDÞ¼: ½ nðx1Þ; . . . ; nðxNÞ 2 IRMnðDÞ N via the Veronese map n of degree n; if rankðV nðDÞÞ < MnðDÞ then\ncompute the basis fcn‘g of the left null space of V nðDÞ; obtain polynomials fpn‘ðxÞ¼: cTn‘ nðxÞg; Y ¼ ;; for j ¼ 1 : n do select a point xj from X nY (similar to Algorithm 1); obtain the subspace S?j spanned by the derivatives spanfDpn‘ðxjÞg; find the subset of points Xj X that belong to the subspace Sj; Y Y [Xj; Recursive-GPCA(Xj); (with Sj now as the ambient space) end for n nmax;\nelse n nþ 1;\nend if until n nmax.\n5. The ideal of an algebraic set Z is the set of all polynomials that vanish in Z. An ideal I is called radical if f 2 I whenever f s 2 I for some integer s."
    }, {
      "heading" : "5 EXPERIMENTAL RESULTS AND APPLICATIONS IN",
      "text" : "COMPUTER VISION\nIn this section, we first evaluate the performance of GPCA on synthetically generated data by comparing and combining it with the following approaches:\n1. Polynomial Factorization Algorithm (PFA). This algorithm is only applicable to the case of hyperplanes. It computes the normal vectors fbigni¼1 to the n hyperplanes by factorizing the homogeneous polynomial pnðxÞ ¼ ðbT1 xÞðbT2 xÞ ðbTnxÞ into a product of linear factors. See [24] for further details.\n2. K-subspaces. Given an initial estimate for the subspace bases, this algorithmalternates between clustering the data points using the distance residual to the different subspaces and computing a basis for each subspace using standard PCA. See [10] for further details.\n3. Expectation Maximization (EM). This algorithm assumes that the data is corrupted with zero-mean Gaussian noise in the directions orthogonal to the subspace. Given an initial estimate for the subspace bases, EM alternates between clustering the data points (E-step) and computing a basis for each subspace (M-step) by maximizing the log-likelihood of the corresponding probabilistic model. See [19] for further details.\nWe then apply GPCA to various problems in computer vision such as face clustering under varying illumination, temporal video segmentation, two-view segmentation of linear motions, and multiview segmentation of rigid-body motions. However, it is not our intention to convince the reader that the proposed GPCA algorithm offers an optimal solution to each of these problems. In fact, one can easily obtain better segmentation results by using algorithms/ systems specially designed for each of these tasks.Wemerely wish to point out that GPCA provides an effective tool to automatically detect themultiple-subspace structure present in these data sets in a noniterative fashion and that it provides a good initial estimate for any iterative algorithm."
    }, {
      "heading" : "5.1 Experiments on Synthetic Data",
      "text" : "The experimental setup consists of choosing n ¼ 2; 3; 4 collections of N ¼ 200n points in randomly chosen planes in IR3. Zero-mean Gaussian noise with s.t.d. from 0 percent\nto 5 percent along the subspace normals is added to the sample points. We run 1,000 trials for each noise level. For each trial, the error between the true (unit) normal vectors fbigni¼1 and their estimates fb̂ig n i¼1 is computed as the mean angle between the normal vectors:\nerror¼: 1 n Xn i¼1 acos bTi b̂i ðdegreesÞ: ð34Þ\nFig. 5a plots themean error as a function of noise forn ¼ 4. Similar results were obtained for n ¼ 2; 3, though with smaller errors. Notice that the estimates of GPCA with the choice of ¼ 0:02 (see Remark 6) have an error that is only about 50 percent the error of the PFA. This is because GPCA deals automatically with noisy data by choosing the points fyigni¼1 in an optimal fashion. The choice of was not important (results were similar for 2 ½0:001; 0:1 ). Notice also that both the K-subspaces and EM algorithms have a nonzero error in the noiseless case, showing that they frequently converge to a local minimum when a single randomly chosen initialization is used.When initializedwith GPCA, both the K-subspaces and EM algorithms reduce the error to approximately 35-50 percent with respect to random initialization. The best performance is achieved by using GPCA to initialize the K-subspaces and EM algorithms.\nFig. 5b plots the estimation error of GPCA as a function of the number of subspaces n, for different levels of noise. As expected, the error increases rapidly as a function of n because GPCA needs a minimum of Oðn2Þ data points to linearly estimate the polynomials (see Section 4.1).\nTable 1 shows the mean computing time and the mean number of iterations for a MATLAB implementation of each one of the algorithms over 1,000 trials. Among the algebraic algorithms, the fastest one is PFAwhich directly factors pnðxÞ given cn. The extra cost of GPCA relative to the PFA is to compute thederivativesDpnðxÞ forallx 2 X andtodivide the polynomials.Overall, GPCAgives about half the error of PFA in about twice as much time. Notice also that GPCA reduces the number of iterations of K-subspaces and EM to approximately 1/3 and 1/2, respectively. The computing times for K-subspacesandEMarealsoreducedincluding theextra time spent on initialization with GPCA or GPCA + K-subspaces."
    }, {
      "heading" : "5.2 Face Clustering under Varying Illumination",
      "text" : "Given a collection of unlabeled images fIj 2 IRDgNj¼1 of n different faces taken under varying illumination, wewould like to cluster the imagescorresponding to the faceof the same person. For aLambertian object, it has been shown that the set of all images taken under all lighting conditions forms a cone in the image space,which can bewell approximated by a lowdimensional subspace [10]. Therefore, we can cluster the collection of images by estimating a basis for eachoneof those subspaces, because images of different faces will lie in different subspaces. Since, in practice, the number of pixels D is large comparedwith the dimension of the subspaces, we first apply PCA to project the images onto IRD 0 with D0 D (see Section 4.1). More specifically, we compute the SVD of the data I1 I2 IN½ D N¼ U V T and consider a matrix X 2 IRD 0 N consisting of the first D0 columns of V T . We obtain a\nnew set of data points in IRD 0 from each one of the columns of X. We use homogeneous coordinates fxj 2 IRD 0þ1gNj¼1 so that eachprojected subspacegoes through theorigin.We consider a subset of the Yale Face Database B consisting of N ¼ 64n frontal views of n ¼ 3 faces (subjects 5, 8, and 10) under 64 varying lighting conditions. For computational efficiency,we downsampled each image to D ¼ 30 40 pixels. Then, we projected the data onto the firstD0 ¼ 3principal components, as shown in Fig. 6. We applied GPCA to the data in homogeneous coordinates and fitted three linear subspaces of dimensions 3, 2, and 2. GPCA obtained a perfect segmentation as shown in Fig. 6b."
    }, {
      "heading" : "5.3 Temporal Segmentation of Video Sequences",
      "text" : "Consider a news video sequence in which the camera is switching among a small number of scenes. For instance, the host could be interviewing a guest and the camera may be switching between the host, the guest, and both of them, as shown in Fig. 7a. Given the frames fIj 2 IRDgNj¼1, we would like to cluster them according to the different scenes. We assume that all the frames corresponding to the same scene live in a low-dimensional subspace of IRD and that different scenes correspond to different subspaces. As in the case of face clustering, we may segment the video sequence into different scenes by applying GPCA to the image data projected onto the first few principal components. Fig. 7b shows the segmentation results for two video sequences. In both cases, a perfect segmentation is obtained."
    }, {
      "heading" : "5.4 Segmentation of Linearly Moving Objects",
      "text" : "In this section, we apply GPCA to the problem of\nsegmenting the 3D motion of multiple objects undergoing\na purely translational motion. We refer the reader to [25],\n[26], where for the case of arbitrary rotation and translation\nvia the segmentation of a mixture of fundamental matrices.\nWe assume that the scene can be modeled as a mixture of purely translational motion models, fTigni¼1, where Ti 2 IR3 represents the translation of object i relative to the camera\nbetween the two consecutive frames. Given the images x1 and x2 of a point in object i in the first and second frame, respectively, the rays x1, x2 and Ti are coplanar. Therefore x1, x2 and Ti must satisfy the well-known epipolar constraint for linear motions\nxT2 ðTi x1Þ ¼ 0: ð35Þ\nIn the case of an uncalibrated camera, the epipolar constraint reads xT2 ðei x1Þ ¼ 0, where ei 2 IR3 is known as the epipole and is linearly related to the translation vector Ti 2 IR3. Since the epipolar constraint can be conveniently rewritten as\neTi ðx2 x1Þ ¼ 0; ð36Þ\nwhere ei 2 IR3 represents the epipole associated with the ith motion, i ¼ 1; . . . ; n, if we define the epipolar line ‘ ¼ ðx2 x1Þ 2 IR3 as a data point, then we have that eTi ‘ ¼ 0. Therefore, the segmentation of a set of images fðxj1; x j 2ÞgNj¼1 of a collection of N points in 3D undergoing n distinct linear motions e1; . . . ; en 2 IR3, can be interpreted as a subspace segmentation problem with d ¼ 2 and D ¼ 3, where the\nepipoles feigni¼1 are the normal to the planes and the epipolar lines f‘jgNj¼1 are the data points. One can use (26) and Algorithm 1 to determine the number of motions n and the\nepipoles ei, respectively.\nFig. 8a shows the first frame of a 320 240 video sequence containing a truck and a car undergoing two 3D translational motions. We applied GPCA with D ¼ 3, and ¼ 0:02 to the epipolar lines obtained from a total of N ¼ 92 features, 44 in the truck and 48 in the car. The algorithm obtained a perfect\nsegmentation of the features, as shown in Fig. 8b, and\nestimated the epipoles with an error of 5.9 degrees for the\ntruck and 1.7 degrees for the car.\nWe also tested the performance of GPCA on synthetic\npoint correspondences corrupted with zero-mean Gaussian\nnoise with s.t.d. between 0 and 1 pixels for an image size of\n500 500 pixels. For comparison purposes, we also implemented the PFA and the EM algorithm for segmenting hyperplanes in IR3. Figs. 8c and 8d show the performance of all the algorithms as a function of the level of noise for n ¼ 2 moving objects. The performance measures are the mean\nerror between the estimated and the true epipoles (in\ndegrees) and the mean percentage of correctly segmented\nfeature points using 1,000 trials for each level of noise. Notice\nthat GPCA gives an error of less than 1.3 degrees and a\nclassification performance of over 96 percent. Thus, GPCA\ngives approximately 1/3 the error of PFA and improves the\nclassification performance by about 2 percent. Notice also\nthat EM with the normal vectors initialized at random (EM)\nyields a nonzero error in the noise free case, because it\nfrequently converges to a local minimum. In fact, our\nalgorithm outperforms EM. However, if we use GPCA to\ninitialize EM (GPCA + EM), the performance of both\nalgorithms improves, showing that our algorithm can be\neffectively used to initialize iterative approaches to motion\nsegmentation. Furthermore, the number of iterations of\nGPCA + EM is approximately 50 percent with respect to\nEM randomly initialized; hence, there is also a gain in\ncomputing time. Figs. 8e and 8f show the performance of\nGPCA as a function of the number of moving objects for\ndifferent levels of noise. As expected, the performance\ndeteriorates as the number of moving objects increases,\nthough the translation error is still below 8 degrees and the\npercentage of correct classification is over 78 percent."
    }, {
      "heading" : "5.5 Three-Dimensional Motion Segmentation from Multiple Affine Views",
      "text" : "Let fxfp 2 IR2gp¼1;...;Nf¼1;...;F be a collection of F images of N 3D points fXp 2 IR3gNj¼1 taken by a moving affine camera. Under the affine camera model, which gener-\nalizes orthographic, weak perspective, and paraperspec-\ntive projection, the images satisfy the equation\nxfp ¼ AfXp; ð37Þ\nwhere Af 2 IR2 4 is the affine camera matrix for frame f , which depends on the position and orientation of the camera as well as the internal calibration parameters. Therefore, if we stack all the image measurements into a 2F N matrix W , we obtain\nW ¼MST\nx11 x1N .. . .. .\nxF1 xFN\n2 664\n3 775 2F N ¼ A1 .. . AF 2 664 3 775 2F 4 X1 XN½ 4 N:\nð38Þ\nIt follows from (38) that rankðWÞ 4; hence, the 2D trajectories of the image points across multiple frames, that is, the columns of W , live in a subspace of IR2F of\ndimension 2, 3, or 4 spanned by the columns of the motion matrix M 2 IR2F 4.\nConsider now the case in which the set of points fXpgNp¼1 corresponds to n moving objects undergoing n different\nmotions. In this case, each moving object spans a different d-dimensional subspace of IR2F , where d ¼ 2, 3, or 4. Solving the motion segmentation problem is hence equivalent to\nfinding a basis for each one of such subspaces without\nknowing which points belong to which subspace. Therefore,\nwe can apply GPCA to the image measurements projected onto a subspace of IR2F of dimensionD ¼ dmax þ 1 ¼ 5. That is, if W ¼ U V T is the SVD of the data matrix, then we can solve themotion segmentationproblembyapplyingGPCAto the first five columns of V T .\nWe tested GPCA on two outdoor sequences taken by a\nmoving camera tracking a car moving in front of a parking\nlot and a building (sequences A and B), and one indoor\nsequence taken by a moving camera tracking a person\nmoving his head (sequence C), as shown in Fig. 9. The data\nfor these sequences are taken from [14] and consist of point\ncorrespondences in multiple views, which are available at\nhttp://www.suri.it.okayama-u.ac.jp/data.html. For all\nsequences, the number of motions is correctly estimated from (11) as n ¼ 2 for all values of 2 ½2; 20 10 7. Also, GPCA gives a percentage of correct classification of\n100.0 percent for all three sequences, as shown in Table 2.\nThe table also shows results reported in [14] from existing\nmultiframe algorithms for motion segmentation. The com-\nparison is somewhat unfair, because our algorithm is purely\nalgebraic, while the others use iterative refinement to deal\nwith noise. Nevertheless, the only algorithm having a\ncomparable performance to ours is Kanatani’s multistage\noptimization algorithm, which is based on solving a series of\nEM-like iterative optimization problems, at the expense of a\nsignificant increase in computation."
    }, {
      "heading" : "6 CONCLUSIONS AND OPEN ISSUES",
      "text" : "We have proposed an algebro-geometric approach to sub-\nspace segmentation called Generalized Principal Component\nAnalysis (GPCA). Our approach is based on estimating a\ncollection of polynomials fromdata and then evaluating their\nderivatives at a data point in order to obtain a basis for the\nsubspace passing through that point. Our experiments\nshowed that GPCA gives about half of the error with respect\nto existing algebraic algorithms based on polynomial\nfactorization, and significantly improves the performance of\niterative techniques such as K-subspaces and EM. We also\ndemonstrated the performance of GPCA on vision problems\nsuch as face clustering and video/motion segmentation. At present, GPCA works well when the number and the dimensions of the subspaces are small, but the performance deteriorates as the number of subspaces increases. This is because GPCA starts by estimating a collection of polynomials in a linear fashion, thus neglecting the nonlinear constraints among the coefficients of those polynomials, the so-called Brill’s equations [6]. Another open issue has to do with the estimation of the number of subspaces n and their dimensions fdigni¼1 by harnessing additional algebraic properties of the vanishing ideals of subspace arrangements (e.g., the Hilbert function of the ideals). Throughout the paper, we hinted at a connection between GPCA and Kernel Methods, e.g., the Veronese map gives an embedding that satisfies the modeling assumptions of KPCA (see Remark 1). Further connections between GPCA and KPCA are worthwhile investigating. Finally, the current GPCA algorithm does not assume the existence of outliers in the given sample data, though one can potentially incorporate statistical methods such as influence theory and random sampling consensus to improve its robustness.Wewill investigate these problems in future research."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors would like to thank Drs. Jacopo Piazzi and Kun Huang for their contributions to this work, and Drs. Frederik Schaffalitzky and Robert Fossum for insightful discussions on the topic. This work was partially supported by Hopkins WSE startup funds, UIUC ECE startup funds, and by grants NSF CAREER IIS-0347456, NSF CAREER IIS0447739, NSF CRS-EHS-0509151, ONR YIP N00014-05-10633, ONR N00014-00-1-0621, ONR N000140510836, and DARPA F33615-98-C-3614."
    } ],
    "references" : [ {
      "title" : "A New Approach to Dimensionality Reduction Theory and Algorithms",
      "author" : [ "D.S. Broomhead", "M. Kirby" ],
      "venue" : "SIAM J. Applied Math., vol. 60, no. 6, pp. 2114-2142, 2000.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A Generalization of Principal Component Analysis to the Exponential Family",
      "author" : [ "M. Collins", "S. Dasgupta", "R. Schapire" ],
      "venue" : "Advances on Neural Information Processing Systems, vol. 14, 2001.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A Multibody Factorization Method for Independently Moving Objects",
      "author" : [ "J. Costeira", "T. Kanade" ],
      "venue" : "Int’l J. Computer Vision, vol. 29, no. 3, pp. 159-179, 1998.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The Approximation of One Matrix by Another of Lower Rank",
      "author" : [ "C. Eckart", "G. Young" ],
      "venue" : "Psychometrika, vol. 1, pp. 211-218, 1936.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1936
    }, {
      "title" : "RANSAC Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography",
      "author" : [ "M.A. Fischler", "R.C. Bolles" ],
      "venue" : "Comm. ACM, vol. 26, pp. 381-395, 1981.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Zelevinsky, Discriminants, Resultants, and Multidimensional Determinants",
      "author" : [ "I.M. Gelfand", "M.M. Kapranov", "A.V" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1994
    }, {
      "title" : "Algebraic Geometry: A",
      "author" : [ "J. Harris" ],
      "venue" : "First Course. Springer-Verlag,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1992
    }, {
      "title" : "Clustering Apperances of Objects under Varying Illumination Conditions",
      "author" : [ "J. Ho", "M.-H. Yang", "J. Lim", "K.-C. Lee", "D. Kriegman" ],
      "venue" : "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 1, pp. 11-18, 2003.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Minimum Effective Dimension for Mixtures of Subspaces: A Robust GPCA Algorithm and Its Applications",
      "author" : [ "K. Huang", "Y. Ma", "R. Vidal" ],
      "venue" : "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, pp. 631-638, 2004.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Principal Component Analysis",
      "author" : [ "I. Jolliffe" ],
      "venue" : "New York: Springer- Verlag,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1986
    }, {
      "title" : "Motion Segmentation by Subspace Separation and Model Selection",
      "author" : [ "K. Kanatani" ],
      "venue" : "Proc. IEEE Int’l Conf. Computer Vision, vol. 2, pp. 586-591, 2001.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Multi-Stage Optimization for Multi- Body Motion Segmentation",
      "author" : [ "K. Kanatani", "Y. Sugaya" ],
      "venue" : "Proc. Australia-Japan Advanced Workshop Computer Vision, pp. 335-349, 2003.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Multiple Eigenspaces",
      "author" : [ "A. Leonardis", "H. Bischof", "J. Maver" ],
      "venue" : "Pattern Recognition, vol. 35, no. 11, pp. 2613-2627, 2002.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Nonlinear Component Analysis as a Kernel Eigenvalue Problem",
      "author" : [ "B. Scholkopf", "A. Smola", "K.-R. Muller" ],
      "venue" : "Neural Computation, vol. 10, pp. 1299-1319, 1998.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A Unified Computational Theory for Motion Transparency and Motion Boundaries Based on Eigenenergy Analysis",
      "author" : [ "M. Shizawa", "K. Mase" ],
      "venue" : "Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 289-295, 1991.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Probability and Random Processes with Applications to Signal Processing, third ed",
      "author" : [ "H. Stark", "J.W. Woods" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2001
    }, {
      "title" : "Mixtures of Probabilistic Principal Component Analyzers,”Neural",
      "author" : [ "M. Tipping", "C. Bishop" ],
      "venue" : "Computation, vol. 11,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1999
    }, {
      "title" : "Probabilistic Principal Component Analysis",
      "author" : [ "M. Tipping", "C. Bishop" ],
      "venue" : "J. Royal Statistical Soc., vol. 61, no. 3, pp. 611-622, 1999.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "An Integrated Bayesian Approach to Layer Extraction from Image Sequences",
      "author" : [ "P. Torr", "R. Szeliski", "P. Anandan" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 23, no. 3, pp. 297-303, Mar. 2001.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Motion Segmentation with Missing Data by PowerFactorization and Generalized PCA",
      "author" : [ "R. Vidal", "R. Hartley" ],
      "venue" : "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. II, pp. 310-316, 2004.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A New GPCA Algorithm for Clustering Subspaces by Fitting, Differentiating, and Dividing Polynomials",
      "author" : [ "R. Vidal", "Y. Ma", "J. Piazzi" ],
      "venue" : "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. I, pp. 510-517, 2004.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Generalized Principal Component Analysis (GPCA)",
      "author" : [ "R. Vidal", "Y. Ma", "S. Sastry" ],
      "venue" : "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. I, pp. 621-628, 2003.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Two-View Multibody Structure from Motion",
      "author" : [ "R. Vidal", "Y. Ma", "S. Soatto", "S. Sastry" ],
      "venue" : "Int’l J. Computer Vision, to be published in 2006.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A Unified Algebraic Approach to 2-D and 3- D Motion Segmentation",
      "author" : [ "R. Vidal", "Y. Ma" ],
      "venue" : "Proc. European Conf. Computer Vision, pp. 1-15, 2004.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multibody Grouping via Orthogonal Subspace Decomposition",
      "author" : [ "Y. Wu", "Z. Zhang", "T.S. Huang", "J.Y. Lin" ],
      "venue" : "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, pp. 252-257, 2001.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "PRINCIPAL Component Analysis (PCA) [12] refers to the problem of fitting a linear subspace S IR of unknown dimension d < D to N sample points fxjgNj1⁄41 in S.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "In Probabilistic PCA [20] (PPCA), the noise is assumed to be drawn from an unknown distribution and the problem becomes one of identifying the subspace and distribution parameters in a maximum-likelihood sense.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "When the noise distribution is Gaussian, the algebro-geometric and probabilistic interpretations coincide [2].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "noise distribution is non-Gaussian, the solution toPPCA is no longer linear, as shown in [2], where PCA is generalized to",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "The standard solution toNLPCA [16] is based on first embedding the data into a higher-dimensional feature space F and then applying standard PCA to the embedded data.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "In the context of stochastic signal processing, PCA is also known as the Karhunen-Loeve transform [18]; in the applied statistics literature, SVD is also known as the Eckart and Young decomposition [4].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "In the context of stochastic signal processing, PCA is also known as the Karhunen-Loeve transform [18]; in the applied statistics literature, SVD is also known as the Eckart and Young decomposition [4].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 18,
      "context" : "Unfortunately, most iterative methods are, in general, very sensitive to initialization; hence, they may not converge to the global optimum [21].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "In [13] (see, also, [3]), it is shown that when the subspaces are orthogonal, of equal dimension d, and intersect only at the origin, which implies thatD nd, one can use the SVD of the data to define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : "In [13] (see, also, [3]), it is shown that when the subspaces are orthogonal, of equal dimension d, and intersect only at the origin, which implies thatD nd, one can use the SVD of the data to define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 19,
      "context" : "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : "The latter case has been addressed in an ad hoc fashion by using clustering algorithms such as K-means, spectral clustering, or EM [14], [28] to segment the data and PCA to obtain a basis for each group.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "The only algebraic approaches that deal with arbitrary intersections are [17], which studies the case of two planes in IR and [24] which studies the case of subspaces of codimension one, i.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "The only algebraic approaches that deal with arbitrary intersections are [17], which studies the case of two planes in IR and [24] which studies the case of subspaces of codimension one, i.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "Our previous work [23] extended this framework to subspaces of unknown and possibly different dimensions under the additional assumption that the number of subspaces is known.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 21,
      "context" : "This paper unifies the results of [24] and [23] and extends to the case inwhich both the number anddimensions of the subspaces are unknown.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 20,
      "context" : "This paper unifies the results of [24] and [23] and extends to the case inwhich both the number anddimensions of the subspaces are unknown.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : ";nDx n1 1 x n2 2 x nD D ; ð1Þ where n : IR D ! IRMnðDÞ is the Veronese map of degree n [7], also known as the polynomial embedding in machine learning, defined as n : 1⁄2x1; .",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "An alternative way of selecting the correct linear model (in feature space) for noisy data can be found in [11].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "Many methods from robust statistics can be deployed to detect and reject the outliers [5], [11].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "Many methods from robust statistics can be deployed to detect and reject the outliers [5], [11].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "Alternatively, one can detect and reject outliers using Random Sample Consensus (RANSAC) [5].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "The reader may refer to [1] for alternative ways of choosing a projection.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "Onemay refer to [11] for a more detailed discussion on selecting the best multiple-subspace model from noisy data, using modelselection criteria such as MML, MDL, AIC, and BIC.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 21,
      "context" : "See [24] for further details.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "See [10] for further details.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "See [19] for further details.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "For aLambertian object, it has been shown that the set of all images taken under all lighting conditions forms a cone in the image space,which can bewell approximated by a lowdimensional subspace [10].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 22,
      "context" : "We refer the reader to [25],",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "[26], where for the case of arbitrary rotation and translation",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "for these sequences are taken from [14] and consist of point",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "The table also shows results reported in [14] from existing",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Segmenting the point correspondences of sequences A (left), B (center), and C (right) in [14] for each pair of consecutive frames by segmenting subspaces in IR.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "This is because GPCA starts by estimating a collection of polynomials in a linear fashion, thus neglecting the nonlinear constraints among the coefficients of those polynomials, the so-called Brill’s equations [6].",
      "startOffset" : 210,
      "endOffset" : 213
    } ],
    "year" : 2005,
    "abstractText" : "This paper presents an algebro-geometric solution to the problem of segmenting an unknown number of subspaces of unknown and varying dimensions from sample data points. We represent the subspaces with a set of homogeneous polynomials whose degree is the number of subspaces and whose derivatives at a data point give normal vectors to the subspace passing through the point. When the number of subspaces is known, we show that these polynomials can be estimated linearly from data; hence, subspace segmentation is reduced to classifying one point per subspace. We select these points optimally from the data set by minimizing certain distance function, thus dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recovered by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA that deal with data in a highdimensional space and with an unknown number of subspaces are also presented. Our experiments on low-dimensional data show that GPCA outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspaces and Expectation Maximization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3Dmotion segmentation from point correspondences in multiple affine views.",
    "creator" : "3B2 Total Publishing System 7.51c/W"
  }
}