{
  "name" : "1605.06955.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Beyond the Low-density Separation Principle: A Novel Approach to Semi-supervised Learning",
    "authors" : [ "Tomoya Sakai", "Masashi Sugiyama" ],
    "emails" : [ "sakai@ms.k.u-tokyo.ac.jp", "christo@ms.k.u-tokyo.ac.jp", "gang@ms.k.u-tokyo.ac.jp", "sugi@k.u-tokyo.ac.jp" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the last decades, learning from labeled and unlabeled data called semi-supervised learning has been extensively studied [1–3]. The basic principle of semi-supervised learning is the low-density separation principle: the decision boundary for classification does not go through high-density regions. Following this principle, various semi-supervised learning methods based on the cluster assumption (samples in the same cluster share the same class label) and the manifold assumption (the learning target function is smooth over the input data manifolds) have been developed.\nHowever, in many real-world problems, such cluster and manifold assumptions are not necessarily satisfied, which causes standard semi-supervised learning methods to perform poorly. Furthermore, there is no practical way to assess whether the cluster and manifold assumptions are satisfied, which is a critical limitation when applying semi-supervised learning to real-world problems. The purpose of this paper is to go beyond the low-density separation principle: we propose a novel approach to semi-supervised learning that does not require the cluster and manifold assumptions.\nOur key idea is based on learning only from positive and unlabeled data (PU learning), which has gathered a great deal of attention recently [4–8]. An interesting property of such PU learning methods is that they are theoretically guaranteed to perform well without restrictive cluster and manifold assumptions. In other words, unlabeled data can be effectively used irrespective of the low-density separation principle.\nBased on this useful fact, we propose to combine PU learning and PN learning (learning from positive and negative data, i.e. standard supervised learning) — thus, we use all positive, negative, and\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 5.\n06 95\n5v 1\n[ cs\n.L G\n] 2\nunlabeled data as standard semi-supervised learning. In our companion paper [9], we have theoretically proved that PU learning (or its counterpart, NU learning, learning from negative and unlabeled data) can outperform PN learning under some condition. Thus, it is naturally expected that our proposed methods, a convex combination of PN learning and PU (or NU) learning, can further improve the performance.\nWe validate this expectation by theoretically analyzing the stability and generalization error of the proposed combined estimators. We first prove, given that the combined classifiers are unbiased, their variance is almost always lower than the classifier trained by PN learning, without the low-density separation principle. We then provide the generalization error bounds for the combined classifiers, showing that they achieve the optimal parametric convergence rate. Finally, we numerically illustrate the behavior of the proposed semi-supervised classifiers.\nThe rest of this paper is structured as follows. In Section 2, we formulate our target classification problem and review PN learning and PU (NU) learning methods. We then describe our proposed semi-supervised learning methods in Section 3 and give optimization algorithms in Section 3.2. We theoretically analyze the behavior of the proposed estimators in Section 4 and experimentally evaluate the performance of the proposed methods in Section 5. Finally, we conclude in Section 6."
    }, {
      "heading" : "2 Background",
      "text" : "In this section, we first formulate our target classification problem and then briefly review PN learning and PU (NU) learning methods that will be used for developing a novel semi-supervised learning method in the following sections."
    }, {
      "heading" : "2.1 Problem Settings",
      "text" : "Let X ∈ Rd for d ∈ N and Y = ±1, which are equipped with underlying joint probability density p(x, y). Let p+(x) = p(x | y = +1), p−(x) = p(x | y = −1), and π = p(y = +1). Suppose that we are given no data from p(x, y), but instead we observe the following three data sets: X+ = {x+i}n+i=1 ∼ p+(x),X− = {x − i } n− i=1 ∼ p−(x), andXu = {xui } nu i=1 ∼ pu(x). Each of the three data sets contains independent and identically distributed data from the corresponding marginal density. We call X+, X− and Xu positive (P), negative (N) and unlabeled (U) data, respectively. A learning problem using P and N data is called PN learning, and that using P and U data, or N and U data, is PU learning, or NU learning. Finally, a learning problem using all of P, N and U data is referred to as PNU learning. See Figure 1."
    }, {
      "heading" : "2.2 PN Learning",
      "text" : "Let g : Rd 7→ R be an arbitrary real-valued decision function for binary classification and ` : R 7→ R be a bounded Lipschitz-continuous loss function. Let\nR+(g) = E+[`(g(X))], R−(g) = E−[`(−g(X))], where E±[·] = EX∼p± [·]. Then the risk of g w.r.t. ` under p(x, y) is given by\nR(g) = E(X,Y )[`(Y g(X))] = πR+(g) + (1− π)R−(g). (1) Approximating R(g) based on Eq. (1), we obtain an unbiased estimator of the risk in PN learning:\nR̂pn(g) = π\nn+ n+∑ i=1 `(g(x+i )) + 1− π n− n−∑ i=1 `(−g(x−i )),\nwhose convergence rate is Op(1/ √ n+ + 1/ √ n−) [10]. Here, Op(·) denotes the asymptotic order in probability."
    }, {
      "heading" : "2.3 PU Learning",
      "text" : "Let Ru,−(g) = EX [`(−g(X))], where EX = EX∼pu [·]. du Plessis et al. [7] has shown that with the following symmetric condition `(t) + `(−t) = 1, (2)\nwe have Ru,−(g) = π(1−R+(g)) + (1− π)R−(g), and hence R(g) = 2πR+(g) +Ru,−(g)− π. (3) Approximating R(g) based on Eq. (3), we obtain an unbiased estimator to the risk in PU learning:\nR̂pu(g) = −π + 2π\nn+ n+∑ i=1 `(g(x+i)) + 1 nu nu∑ i=1 `(−g(xui )),\nwhose convergence rate is Op(1/ √ n+ + 1/ √ nu) [7]."
    }, {
      "heading" : "2.4 NU Learning",
      "text" : "Likewise, R(g) could be estimated using NU data: R(g) = Ru,+(g) + 2(1− π)R−(g)− (1− π), (4)\nwhere Ru,+(g) = EX [`(g(X))]. Approximating R(g) based on Eq. (4), we obtain an unbiased estimator to the risk in NU learning:\nR̂nu(g) = 1\nnu nu∑ i=1 `(g(xui )) + 2(1− π) n− n−∑ i=1 `(−g(x−i ))− (1− π),\nwhose convergence rate is Op(1/ √ n− + 1/ √ nu)."
    }, {
      "heading" : "3 Learning from PNU Data",
      "text" : "In this section, we describe our semi-supervised learning methods which combine PN learning and PU learning (or NU learning)."
    }, {
      "heading" : "3.1 PNPU and PNNU Estimators",
      "text" : "Let Rpn(g) denote R(g) in Eq. (1), Rpu(g) denote R(g) in Eq. (3), and γ be a real scalar such that 0 ≤ γ ≤ 1. Then the convex combination of Rpn(g) and Rpu(g) gives\nR(g) = γRpn(g) + (1− γ)Rpu(g) = (2π − γπ)R+(g) + γ(1− π)R−(g) + (1− γ)Ru,−(g)− (1− γ)π. (5)\nApproximating R(g) based on Eq. (5), we obtain an unbiased estimator to the risk:\nR̂γpnpu(g) = γR̂pn(g) + (1− γ)R̂pu(g)\n= c+ n+∑ i=1 `(g(x+i)) + c− n−∑ i=1 `(−g(x−i )) + cu nu∑ i=1 `(−g(xui ))− (1− γ)π, (6)\nwhere c+ = (2 − γ)π/n+, c− = γ(1 − π)n−, and cu = (1 − γ)/nu. R̂γpnpu(g) converges in Op(1/ √ n+ + 1/ √ n− + 1/ √ nu).\nLikewise, the risk of PNNU estimator is obtained by the convex combination ofRpn(g) andRpu(g), R(g) = γπR+(g) + (2− γ)(1− π)R−(g) + (1− γ)Ru,+(g)− (1− γ)(1− π), (7) whose empirical approximator R̂γpnnu(g) = γR̂pn(g) + (1− γ)R̂nu(g) converges in Op(1/ √ n+ + 1/ √ n− + 1/ √ nu)."
    }, {
      "heading" : "3.2 Algorithm",
      "text" : "Here, we give an algorithm to minimize the empirical loss (6) for PNPU learning. PNNU learning can be performed in the same manner. For simplicity, we introduce a set of triple {(xi, κi, ci)}ni=1, where n = n+ +n−+nu, κ takes +1 for positive sample and−1 for negative and unlabeled sample (for PNNU, κ takes +1 for unlabeled sample), and c is the cost for each sample, i.e., ci = c+ for positive, ci = c− for negative, and ci = cu for unlabeled sample. We use the ramp loss, which satisfies the symmetric condition (2): `R(z) := 12 max(0,min(2, 1−z)). Since the ramp loss can be separated into convex and concave parts, we use the ConCave-Convex Procedure (CCCP) [11, 12]. The CCCP method iteratively tightens a linear upper-bound for the concave part, and minimizes the convex part and the linear upper bound. The procedure is discussed in detail below.\nConvex-Concave Partitioning The scaled ramp loss function can be split into a convex and concave part:\n`R(z) = H1(z)︸ ︷︷ ︸ convex −H−1(z)︸ ︷︷ ︸ concave ,\nwhere Hs(z) := 12 max(0, s− z) is the hinge loss. Assuming that the function gw(x) is parameterized by w, we can express the objective function (6) as\nJ(w) = n∑ i=1 ci`R(κigw(xi)) + λ 2 w>w,\nwhere the last term is an additional `2-regularization term with regularization parameter λ ≥ 0. This objective function can then be partitioned into a convex part and a concave part:\nJ(w) = n∑ i=1 ciH1(κigw(xi)) + λ\n2 w>w︸ ︷︷ ︸\nJvex(w)\n− n∑ i=1\nciH−1(κigw(xi))︸ ︷︷ ︸ Jcave(w) .\nUsing the Fenchel inequality −H−1(z) ≤ H∗−1(t)− zt, the concave part can be upper-bounded as\nJ̄cave(w,u) = n∑ i=1 ci(H ∗ −1(ui)− uiκigw(xi)),\nwhere u is the bound parameter, and H∗−1(t) is −t if −1/2 ≤ t ≤ 0, ∞ otherwise. H∗−1(t) is known as the convex conjugate or Fenchel dual. The upper-bound J̄cave(w,u) can be analytically minimized w.r.t. u by ui = −1/2 if κigw(xi) ≤ −1, and 0 otherwise.\nOptimization problems The tightened upper-bound of the objective function J̄(w) = Jvex(w) + J̄cave(w) can be minimized as\nminimize w∈Rb\n1\n2 n∑ i=1 ci max(0, 1− κigw(xi)) + 1 2 ∑ i∈V ciκigw(xi) + λ 2 w>w,\nwhere V = {i ∈ [n] | κigw(xi) ≤ −1} is the active set and [n] = {1, . . . , n}.\nLet us use a model gw(x) = w>φ(x) + b, then the dual problem is obtained by\nmaximize α∈Rn\nα>1n − 1\n2λ α>[(κκ>) ◦K]α\nsubject to α>κ = 0 − η α c− η,\nwhere denotes element-wise inequality for vectors, ◦ denotes element-wise multiplication of matrices, Ki,j = φ(xi)>φ(xj), κ := (κ1, . . . , κn)>, c := (c1/2, . . . , cn/2)>, η := (η1, . . . , ηn)>, and ηi = ci if i ∈ V and ηi = 0 if i 6∈ V . Those problems can be solved as the quadratic program by an off-the-shelf optimizer such as Gurobi Optimizer [13].\nThis algorithm includes PN, PU, and NU learning with the ramp loss. For example, when γ = 1 and κi = yi, the above optimization problems is equivalent to that of ramp loss SVMs with R−1(z) = H1(z)−H−1(z) proposed by [12]. Thus, this formulation can be regarded as a more general version of that algorithm."
    }, {
      "heading" : "4 Theoretical Analyses",
      "text" : "In this section, we theoretically analyze the behavior of the proposed method."
    }, {
      "heading" : "4.1 Variance Analysis",
      "text" : "All estimators to R(g) so far are unbiased. A natural question would be raised: Could R̂γpnpu(g) and R̂γpnnu(g) have smaller variances than R̂pn(g)?\nTo answer this question, let g be fixed and assume that\nn+ nu, n− nu,\nwhich should be natural in PU and NU learning. Then we have the following lemmas (their proofs are given in Appendix A and Appendix B): Lemma 1. For any fixed g, if\n1\nn+ π2σ2+(g) <\n1\nn− (1− π)2σ2−(g) (8)\nis true, the combination R̂γpnpu(g) at\nγ∗pnpu = 2\n( 1 +\n(1− π)2σ2−(g)/n− π2σ2+(g)/n+\n)−1 (9)\nis guaranteed to reduce the variance of R̂pn as nu → ∞; otherwise, R̂pn has already achieved the minimal variance among all possible combinations implied by R̂γpnpu(g). Lemma 2. For any fixed g, if\n1\nn+ π2σ2+(g) >\n1\nn− (1− π)2σ2−(g) (10)\nis true, the combination R̂γpnnu(g) at\nγ∗pnnu = 2\n( 1 +\nπ2σ2+(g)/n+\n(1− π)2σ2−(g)/n− )−1 is guaranteed to reduce the variance of R̂pn as nu → ∞; otherwise, R̂pn has already achieved the minimal variance among all possible combinations implied by R̂γpnnu(g).\nObserving that (8) and (10) are essentially complimentary, we immediately obtain the following theorem: Theorem 3. For any fixed g, either R̂γpnpu(g) or R̂γpnnu(g) could almost always reduce the variance of R̂pn. The only exception is\n1\nn+ π2σ2+(g) =\n1\nn− (1− π)2σ2−(g),\nwith probability zero over choices of n+, n−, and g.\nThis theorem shows that either PNPU learning and PNNU learning almost always improves the variance over PN learning without the low-density separation principle."
    }, {
      "heading" : "4.2 Generalization Error Bounds",
      "text" : "Next, we analyze the generalization error of PNPU learning and PNNU learning.\nLet G be a function class of hyperplanes with bounded normals and feature maps:\nG = {g(x) = 〈w,φ(x)〉 | ‖w‖ ≤ Cw, ‖φ(x)‖ ≤ Cφ},\nwhere Cw and Cφ are certain positive constants. Since we have added the `2-regularization, we can naturally assume that the optimal g returned by our CCCP solver always belongs to certain G. Let `01(z) = (1− sign(z))/2 be the zero-one loss and I(g) = E(X,Y )[`01(Y g(X))] be the risk of g w.r.t. `01 under p(x, y). We then can obtain two generalization error bounds. Theorem 4. For any δ > 0, with probability at least 1−δ, the following generalization error bounds hold for any g ∈ G separately:\nI(g) ≤ 2R̂γpnpu(g) + ( 2CwCφ + 3 √ 2 ln(6/δ) )( (2− γ)π\n√ n+ + γ(1− π) √ n− + 1− γ √ nu\n) , (11)\nI(g) ≤ 2R̂γpnnu(g) + ( 2CwCφ + 3 √ 2 ln(6/δ) )( γπ √ n+ + (2− γ)(1− π) √ n− + 1− γ √ nu ) . (12)\nSketch of proof (See Appendix C for full proof): A key observation is that `01(z) ≤ 2`R(z) for any z, and consequently I(g) ≤ 2R(g). Then, according to the decomposition of R(g) in Eq. (5) and the definition of R̂γpnpu(g) in Eq. (6), the bound in (11) can be proved using the Rademacher complexity [14], Talagrand’s contraction lemma [15], and the fact that the Lipschitz constant of our ramp loss is 1/2. Similarly, according to the decomposition of R(g) in Eq. (7) and the definition of R̂γpnnu(g), the bound in (12) can be proved.\nTheorem 4 guarantees that for any g, the generalization error I(g) can be bounded from above by two times the empirical risks, i.e., 2R̂γpnpu(g) and 2R̂ γ pnnu(g), plus a confidence term of order\nOp(1/ √ n+ + 1/ √ n− + 1/ √ nu).\nAs n+, n− and nu can increase independently, this is already the optimal convergence rate of the confidence term without any additional assumption."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we experimentally evaluate the performance of the proposed method."
    }, {
      "heading" : "5.1 Numerical illustration",
      "text" : "We numerically illustrate the behavior of the proposed method using simple Gaussian data. The PNU data is generated from the following distributions:\np+(x) = N(x; (1, 0) >, I2), p−(x) = N(x; (−1, 0)>, I2),\nwhere N(x;µ,Σ) denotes the normal density with mean µ and covariance matrix Σ, and I2 is the 2 × 2 identity matrix. We generate 6 positive, 2 negative samples, and 30 unlabeled samples using class priors of π = 0.2, 0.5. The class prior is assumed to be known in this and subsequent experiments, but in practice it can be accurately estimated with methods proposed in [6, 16, 17]. A linear model g(x) = w>x + b and a fixed regularization parameter of λ = 0.001 is used for training. The misclassification rate is evaluated on a dataset of 100, 000 samples.\nFigure 2(a) shows the mean and standard error of the PNPU classifier over 100 trials when π = 0.2 and π = 0.5, for different γ values. When γ = 0, it corresponds to pure PU learning, and when γ = 1, it corresponds to pure PN learning. By assuming that the unknown variances are equal, i.e., σ2+(g) = σ 2 −(g), we can select γ according to (9) (the γ values selected according to this rule is indicated by a cross on the figure).\nWhen π = 0.2, PU learning is much better than PN learning. In this situation, the γ selected according to our heuristic is close to the value for PU learning. When π = 0.5, we see that PU learning is worse than PN learning. However, the convex combination of both objective functions, with γ = 0.5 (as selected by the rule), still improves on PN learning.\nFigures 2(b) and 2(c) show typical examples of two cases of PNPU learning. Here, the black dotted, green dashed, and red solid lines show the true decision boundary and boundaries obtained by PN learning and PNPU learning with γ selected according to the rule. When π = 0.2, the PNPU decision boundary is close to the optimal decision boundary and differs from the PN boundary. When π = 0.5, the PU and PNPU decision boundaries are very similar. In both cases, the PN boundary is far from ideal."
    }, {
      "heading" : "5.2 Benchmark data sets",
      "text" : "We compare our method against two standard semi-supervised (i.e. PNU) learning methods, Laplacian SVM (LapSVM) [3] and logistic regression with entropy regularization (ER) [18], on various benchmarks. 1 In the experiments, we fix n+ and n− and vary π. Since the class-prior of the labeled data may differ from the target class prior, we reweighted SVM for LapSVM and reweighted logistic regression for ER.\nTo ensure fair comparison, all methods used a Gaussian kernel model\ng(x) = n∑ i=1 αiK(x,xi), where K(x,x′) = exp ( −‖x− x ′‖2 2σ2 ) .\nThe bandwidth of the kernel was selected from {10−3, 10−2, . . . , 101}. The hyper-parameters in all methods were selected via 5-fold cross-validation. In LapSVM and ER, unlabeled data is incorporated as a regularization term. Therefore the parameter controlling the strength of unlabeled data was selected (as was all other hyper-parameters) based on cross-validation score calculated solely from the PN data. For LapSVM, the number of nearest neighbors for constructing the Laplacian matrix is chosen from {5, 6, . . . , 10}, and other options are the same as default value of implementation provided on the author’s website [3]. ER has a single hyper-parameter term that controls the strength of L2-regulation on the model and entropy regularization term. This parameter is selected from {10−3, 10−2, . . . , 101}. In PNPU and PNNU learning, the unlabeled samples are incorporated in the objective function. Therefore, if γ is fixed a priori, the cross-validation is based on the misclassification rate estimated from both the PN and PU data (i.e., empirical versions of (5) and (7) with the zero-one loss). If γ is appropriately set, PNPU and PNPU learning may potentially have estimates of the misclassification rate with a lower variance than other semi-supervised methods (cf. Appendix A), which in turn may translate to better hyper-parameter selection.\nTable 1 reports the mean and standard error of misclassification rate over 50 trials. Overall, our proposed method works well compared to the conventional methods on most data sets. Note that when π = 0.2, 0.8, the error rate in the competing methods may be in some instances higher than the\n1The benchmark data sets are from the semi-supervised learning book [2], UCI machine learning repository [19], IDA benchmark repository [20], and the European ESPRIT 5516 project. The data for the ESPRIT project is available from: https://www.elen.ucl.ac.be/neural-nets/Research/ Projects/ELENA/elena.htm\nchance rate. One possible explanation on why this occurs is that the assumptions on unlabeled data, such as the manifold assumption in LapSVM, are violated in the datasets. This may for instance occur in the banana dataset, since the two classes are highly overlapping.\nAccording to our rule for selecting γ, when the sample sizes, are the same (n+ = n−) and the target class prior is π = 0.5, our semi-supervised method is reduced to PN learning. This is due to the assumption used in the rule that the variances of the two classes are equal. It may however still be possible to select γ that will cause an improvement, since the variances of the two classes may differ (this may occur in situations such as one-versus-rest). Developing such a rule is an important avenue for future research."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we proposed a novel approach to semi-supervised learning that is not based on the low-density separation principle. Our key idea was to combine PN learning and PU learning or NU learning. We theoretically showed that either PNPU learning and PNNU learning almost always improves the variance over PN learning without the low-density separation principle. We further proved that PNPU learning and PNNU learning achieve the optimal parametric convergence rate in terms of the generalization error. Numerical experiments illustrated the effectiveness of the proposed methods."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "Proof. Due to the independence of X+, X− and Xu and the assumptions n+ nu and n− nu, we have\nVar[R̂γpnpu(g)] = (2π − γπ)2\nn+ σ2+(g) +\nγ2(1− π)2\nn− σ2−(g) +\n(1− γ)2\nnu σ2u,−(g)\n≈ (2π − γπ) 2\nn+ σ2+(g) +\nγ2(1− π)2\nn− σ2−(g) (13)\nas nu → ∞, where σ2+(g) = VarX∼p+(x)[`(g(X))], σ 2 −(g) = VarX∼p−(x)[`(−g(X))], and σ 2 u,−(g) = VarX∼p(x)[`(−g(X))]. Then,\nγ∗pnpu = argmin γ\n(2π − γπ)2\nn+ σ2+(g) +\nγ2(1− π)2\nn− σ2−(g) = 2\n( 1 +\n(1− π)2σ2−(g)/n− π2σ2+(g)/n+\n)−1 .\nIt is clear that 0 < γ∗pnpu < 1 if and only if\n1\nn+ π2σ2+(g) <\n1\nn− (1− π)2σ2−(g).\nNote that Eq. (13) is quadratic in γ, and thus we have Lemma 1."
    }, {
      "heading" : "B Proof of Lemma 2",
      "text" : "Proof. With the assumptions n+ nu and n− nu, we have\nVar[R̂γpnnu(g)] ≈ γ2π2\nn+ σ2+(g) +\n(2− γ)2(1− π)2\nn− σ2−(g) (14)\nas nu →∞, and then\nγ∗pnnu = argmin γ\nγ2π2\nn+ σ2+(g) +\n(2− γ)2(1− π)2\nn− σ2−(g) = 2\n( 1 +\nπ2σ2+(g)/n+ (1− π)2σ2−(g)/n−\n)−1 .\nIt is clear that 0 < γ∗pnnu < 1 if and only if\n1\nn+ π2σ2+(g) >\n1\nn− (1− π)2σ2−(g).\nEq. (14) is also quadratic in γ, and thus we have Lemma 2."
    }, {
      "heading" : "C Proof of Theorem 4",
      "text" : "Let R̂+(g), R̂−(g), R̂u,+(g) and R̂u,−(g) be empirical approximations of R+(g), R−(g), Ru,+(g) and Ru,−(g) of sizes n+, n−, nu and nu, respectively. Then we have the following concentration results, which will be used to prove Theorem 4:\nLemma 5. For any δ > 0, with probability at least 1 − δ/3, the following uniform deviation bounds hold separately:\nsupg∈G(R+(g)− R̂+(g)) ≤ CwCφ√ n+ + 3\n√ ln(6/δ)\n2n+ ,\nsupg∈G(R−(g)− R̂−(g)) ≤ CwCφ√ n− + 3\n√ ln(6/δ)\n2n− ,\nsupg∈G(Ru,+(g)− R̂u,+(g)) ≤ CwCφ√ nu + 3\n√ ln(6/δ)\n2nu ,\nsupg∈G(Ru,−(g)− R̂u,−(g)) ≤ CwCφ√ nu + 3\n√ ln(6/δ)\n2nu .\nAll four bounds in Lemma 5 are from the basic uniform deviation bound using the Rademacher complexity [14], Talagrand’s contraction lemma [15], and the fact that the Lipschitz constant of our ramp loss is 1/2. For these reasons, the detailed proof of Lemma 5 is omitted.\nProof. Recall that\nRγpnpu(g) = (2− γ)πR+(g) + γ(1− π)R−(g) + (1− γ)Ru,−(g)− (1− π)γ,\nR̂γpnpu(g) = (2− γ)πR̂+(g) + γ(1− π)R̂−(g) + (1− γ)R̂u,−(g)− (1− π)γ.\nThen, we have\nsupg∈G(R γ pnpu − R̂γpnpu) = (2− γ)π supg∈G(R−(g)− R̂−(g)) + γ(1− π) supg∈G(R+(g)− R̂+(g))\n+ γ supg∈G(Ru,−(g)− R̂u,−(g)).\nFor any δ > 0, with probability at least 1− δ, we have the following uniform deviation bounds\nsupg∈G(R γ pnpu − R̂γpnpu) ≤ (2− γ)π ( CwCφ√ n+ + 3 √ ln(6/δ) 2n+ ) + γ(1− π) ( CwCφ√ n− + 3 √ ln(6/δ) 2n− )\n+ (1− γ) ( CwCφ√ nu + 3 √ ln(6/δ) 2nu )\n= 1\n2\n( 2CwCφ + 3 √ 2 ln(6/δ) )( (2− γ)π √ n+ + γ(1− π) √ n− + 1− γ√ nu ) ,\nwhere each supremum is bounded with probability 1− δ/3 based on Lemma 5. Since I(g) ≤ 2R(g), we have Eq. (11). Likewise, the generalization error bounds in Eq.(12) holds."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Semi-supervised learning based on the low-density separation principle such as<lb>the cluster and manifold assumptions has been extensively studied in the last<lb>decades. However, such semi-supervised learning methods do not always per-<lb>form well due to violation of the cluster and manifold assumptions. In this paper,<lb>we propose a novel approach to semi-supervised learning that does not require<lb>such restrictive assumptions. Our key idea is to combine learning from positive<lb>and negative data (standard supervised learning) and learning from positive and<lb>unlabeled data (PU learning), the latter is guaranteed to be able to utilize unla-<lb>beled data without the cluster and manifold assumptions. We theoretically and<lb>experimentally show the usefulness of our approach.",
    "creator" : "LaTeX with hyperref package"
  }
}