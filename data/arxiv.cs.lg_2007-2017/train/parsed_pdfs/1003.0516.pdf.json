{
  "name" : "1003.0516.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Model Selection with the Loss Rank Principle",
    "authors" : [ "Marcus Hutter" ],
    "emails" : [ "RSISE@ANU", "SML@NICTA,", "marcus@hutter1.net", "ngoctm@nus.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 3.\n05 16\nv1 [\ncs .L\nG ]\nContents"
    }, {
      "heading" : "1 Introduction 2",
      "text" : ""
    }, {
      "heading" : "2 The Loss Rank Principle 4",
      "text" : ""
    }, {
      "heading" : "3 LoRP for y-Linear Models 8",
      "text" : ""
    }, {
      "heading" : "4 Optimality Properties of LoRP for Variable Selection 11",
      "text" : ""
    }, {
      "heading" : "5 Experiments 13",
      "text" : ""
    }, {
      "heading" : "6 Comparison to Gaussian Bayesian Linear Regression 17",
      "text" : ""
    }, {
      "heading" : "7 Comparison to other Model Selection Schemes 19",
      "text" : ""
    }, {
      "heading" : "8 Loss Functions and their Selection 23",
      "text" : ""
    }, {
      "heading" : "9 Self-Consistent Regression 24",
      "text" : ""
    }, {
      "heading" : "10 Nearest Neighbors Classification 26",
      "text" : "11 Conclusion and Outlook 28 References 30\nKeywords\nModel selection, loss rank principle, non-parametric regression, classification, general loss function, k nearest neighbors."
    }, {
      "heading" : "1 Introduction",
      "text" : "Regression. Consider a regression or classification problem in which we want to determine the functional relationship yi≈ftrue(xi) from dataD={(x1,y1),...,(xn,yn)}, i.e., we seek a function r(.|D)≡ r(D)(.) such that r(x|D)≡ r(D)(x) is close to the unknown ftrue(x) for all x. One may define r(.|D) directly, e.g., “average the y values of the k nearest neighbors (kNN) of x in D”, or select r(.|D) from a class of functions F that has smallest (training) error on D. If the class F is not too large, e.g., the polynomials of fixed reasonable degree d, this often works well.\nModel selection. What remains is to select the right model complexity c, like k or d. This selection cannot be based on the training error, since the more complex the model (large d, small k) the better the fit on D (perfect for d=n and k=1). This problem is called overfitting, for which various remedies have been suggested.\nThe most popular ones in practice are based on a test set D′ used for selecting the c for which the function rc(.|D) has smallest (test) error on D′, or improved versions like cross-validation [All74]. Typically D′ is cut from D, thus reducing the sample size available for regression. Test set methods often work well in practice, but the reduced sample decreases accuracy, which can be a serious problem if n is small. We will not discuss empirical test set methods any further. See [Mac92] for a comparison of cross-validation with Bayesian model selection.\nThere are also various model selection methods that allow to use all data D for regression. The most popular ones can be regarded as penalized versions of Maximum Likelihood (ML). In addition to the function class Fc (subscript c belonging to some set indexing the complexity), one has to specify a sampling model P(D|f), e.g., that the yi have independent Gaussian distribution with mean f(xi). ML chooses rc(D)=argmaxf∈FcP(D|f), Penalized ML (PML) then chooses ĉ=argminc{−logP(D|rc(D))+Penalty(c)}, where the penalty depends on the used approach (MDL [Ris78], BIC [Sch78], AIC [Aka73]). All PML variants rely on a proper sampling model (which may be difficult to establish), ignore (or at least do not tell how to incorporate) a potentially given loss function (see [Yam99, Grü04] for exceptions), are based on distribution-independent penalties (which may result in bad performance for specific distributions), and are typically limited to (semi)parametric models.\nMain idea. The main goal of the paper is to establish a criterion for selecting the “best” model complexity c based on regressors rc given as a black box without insight into the origin or inner structure of rc, that does not depend on things often not given (like a stochastic noise model), and that exploits what is/should be given (like the loss function, note that the criterion can also be used for loss-function selection, see Section 8). The key observation we exploit is that large classes Fc or more flexible regressors rc can fit more data well than more rigid ones. We define the loss rank of rc as the number of other (fictitious) data D\n′ that are fitted better by rc(D\n′) than D is fitted by rc(D), as measured by some loss function. The loss rank is large for regressors fitting D not well and for too flexible regressors (in both\ncases the regressor fits many other D′ better). The loss rank has a minimum for not too flexible regressors which fit D not too bad. We claim that minimizing the loss rank is a suitable model selection criterion, since it trades off the quality of fit with the flexibility of the model. Unlike PML, our Loss Rank Principle (LoRP) works without a noise (stochastic sampling) model, and is directly applicable to any non-parametric regressor, like kNN.\nRelated ideas. There are various other ideas that somehow count fictitious data. In normalized ML [Grü04], the complexity of a stochastic model class is defined as the log sum over all D′ of maximum likelihood probabilities. In the luckiness framework for classification [Her02, Chp.4], the loss rank is related to the level of a hypothesis, if the empirical loss is used as an unluckiness function. The empirical Rademacher complexity [Kol01, BBL02] averages over all possible relabeled instances. Finally, instead of considering all D′ one could consider only the set of all permutations of {y1,...,yn}, like in permutation tests [ET93]. The test statistic would here be the empirical loss.\nContents. In Section 2, after giving a brief introduction to regression, we formally state LoRP for model selection. Explicit expressions for the loss rank for the important class of linear regressors are derived in Section 3; this class includes kNN, polynomial, linear basis function (LBFR), kernel, projective regression, and some others. In Section 4, we establish optimality properties of LoRP for linear regression, namely model consistency and asymptotic mean efficiency. Experiments are presented in Section 5: We compare LoRP to other selection methods and demonstrate the use of LoRP for some specific problems like choosing tuning parameters in kNN and spline regression. In Section 6 we compare linear LoRP to Bayesian model selection for linear regression with Gaussian noise and prior, and in Section 7 to PML, in particular MDL, BIC, and AIC, and then discuss two trace formulas for the effective dimension. Sections 8-10 can be considered as extension sections. In Section 8 we show how to generalize linear LoRP to non-quadratic loss, in particular to other norms. We also discuss how LoRP can be used to select the loss function itself, in case it is not part of the problem specification. In Section 9 we briefly discuss interpolation. LoRP only depends on the regressor on data D and not on x 6∈ {x1,...,xn}. We construct canonical regressors for off-data interpolation from regressors given only on-data, in particular for kNN, Kernel, and LBFR, and show that they are canonical. In Section 10 we derive exact expressions for kNN when {x1,...,xn} forms a discrete d-dimensional hypercube, and discuss the limits n→∞, k→∞, and d→∞. Section 11 contains the conclusions of our work and further considerations that could be elaborated on in the future.\nThe main idea of LoRP has already been presented at the COLT 2007 conference [Hut07]. In this paper we present LoRP more thoroughly, discover its theoretical properties and evaluate the method through some experiments."
    }, {
      "heading" : "2 The Loss Rank Principle",
      "text" : "After giving a brief introduction to regression, classification, model selection, overfitting, and some reoccurring examples, we state our novel Loss Rank Principle for model selection. We first state it for classification (Principle 3 for discrete values), and then generalize it for regression (Principle 5 for continuous values), and exemplify it on two (over-simplistic) artificial Examples 4 and 6. Thereafter we show how to regularize LoRP for realistic regression problems.\nSetup and notation. We assume data D = (x,y) := {(x1,y1),...,(xn,yn)} ∈ (X × Y)n=:D has been observed. We think of the y as having an approximate functional dependence on x, i.e., yi ≈ ftrue(xi), where ≈ means that the yi are distorted by noise from the unknown “true” values ftrue(xi). We will write (x,y) for generic data points, use vector notation x=(x1,...,xn) ⊤ and y=(y1,...,yn) ⊤, and D′=(x′,y′) for generic (fictitious) data of size n. A full list of abbreviations and notations used throughout the paper is placed in the appendix.\nRegression and classification. In regression problems Y is typically (a subset of) the real set IR or some more general measurable space like IRm. In classification, Y is a finite set or at least discrete. We impose no restrictions on X . Indeed, x will essentially be fixed and plays only a spectator role, so we will often notationally suppress dependencies on x. The goal of regression/classification is to find a function fD ∈F ⊂X →Y “close” to ftrue based on the past observations D. Or phrased in another way: we are interested in a mapping r :D→F such that ŷ := r(x|D)≡ r(D)(x)≡fD(x)≈ftrue(x) for all x∈X . Example 1 (polynomial regression) For X = Y = IR, consider the set Fd := {fw(x) = wdxd−1+ ...+w2x+w1 :w ∈ IRd} of polynomials of degree d−1. Fitting the polynomial to data D, e.g., by least squares regression, we estimate w with ŵD. The regression function ŷ= rd(x|D) = fŵD(x) can be written down in closed form (see Example 9). ♦\nExample 2 (k nearest neighbors) Let Y be some vector space like IR and X be a metric space like IRm with some (e.g., Euclidean) metric d(·,·). kNN estimates ftrue(x) by averaging the y values of the k nearest neighbors Nk(x) of x in D, i.e., rk(x|D)= 1k ∑\ni∈Nk(x)yi with |Nk(x)|=k such that d(x,xi)≤d(x,xj) for all i∈Nk(x) and j 6∈Nk(x). ♦\nParametric versus non-parametric regression. Polynomial regression is an example of parametric regression in the sense that rd(D) is the optimal function from a family of functions Fd indexed by d<∞ real parameters (w). In contrast, the kNN regressor rk is directly given and is not based on a finite-dimensional family of functions. In general, r may be given either directly or be the result of an optimization process.\nLoss function. The quality of fit to the data is usually measured by a loss function Loss(y,ŷ), where ŷi = f̂D(xi) is an estimate of yi. Often the loss is additive:\nLoss(y,ŷ)= ∑n i=1Loss(yi,ŷi). If the class F is not too large, good regression functions r(D) can be found by minimizing the loss w.r.t. all f ∈ F . For instance, rd(D)=argminf∈Fd ∑n i=1(yi−f(xi))2 and ŷ=rd(x|D) in Example 1. Regression class and loss. In the following we assume a class of regressors R (whatever their origin), e.g., the kNN regressors {rk : k ∈ IN} or the least squares polynomial regressors {rd : d ∈ IN0 := IN∪{0}}. Each regressor r can be thought of as a model. Throughout the paper, we use the terms “regressor” and “model” interchangeably. Note that unlike f ∈F , regressors r ∈R are not functions of x alone but depend on all observations D, in particular on y. Like for functions f , we can compute the empirical loss of each regressor r∈R:\nLossr(D) ≡ Lossr(y|x) := Loss(y, ŷ) = n ∑\ni=1\nLoss(yi, r(xi|x,y))\nwhere ŷi= r(xi|D) in the third expression, and the last expression holds in case of additive loss.\nOverfitting. Unfortunately, minimizing Lossr w.r.t. r will typically not select the “best” overall regressor. This is the well-known overfitting problem. In case of polynomials, the classes Fd⊂Fd+1 are nested, hence Lossrd is monotone decreasing in d with Lossrn ≡ 0 perfectly fitting the data. In case of kNN, Lossrk is more or less an increasing function in k with perfect regression on D for k=1, since no averaging takes place. In general, R is often indexed by a “flexibility” or smoothness or complexity parameter, which has to be properly determined. The more flexible r is, the closer it can fit the data. Hence such r has smaller empirical loss, but is not necessarily better since it has higher variance. Clearly, too inflexible r also lead to a bad fit (“high bias”).\nMain goal. The main goal of the paper is to establish a selection criterion in order to specify the smallest model to which ftrue belongs or is close to, and simultaneously determine the “best” fitting function r(D). The criterion\n• is based on r given as a black box that does not require insight into the origin or inner structure of r; • does not depend on things often not given (like a stochastic noise model); and • exploits what is or should be given (like the loss function).\nDefinition of loss rank. We first consider discrete Y (i.e., classification), fix x, y is the observed data and y′ are fictitious others. The key observation we exploit is that a more flexible r can fit more data D′∈D well than a more rigid one. The more flexible r is, the smaller the empirical loss Lossr(y|x) is. Instead of minimizing the unsuitable Lossr(y|x) w.r.t. r, we could ask how many y′∈Yn lead to smaller Lossr than y. We define the loss rank of r (w.r.t. y) as the number of y\n′∈Yn with smaller or equal empirical loss than y:\nRankr(y|x)≡ Rankr(L) := #{y′∈Yn : Lossr(y′|x)≤L} with L := Lossr(y|x) (1)\nWe claim that the loss rank of r is a suitable model selection measure. For (1) to make sense, we have to assume (and will later assure) that Rankr(L)<∞, i.e., there are only finitely many y′∈Yn having loss smaller than L.\nSince the logarithm is a strictly monotone increasing function, we can also consider the logarithmic rank LRr(y|x) := logRankr(y|x), which will be more convenient.\nPrinciple 3 (LoRP for classification) For discrete Y, the best classifier/regressor r : D×X → Y in some class R for data D = (x,y) is the one with the smallest loss rank:\nrbest = argmin r∈R LRr(y|x) ≡ argmin r∈R Rankr(y|x) (2)\nwhere Rankr is defined in (1).\nWe give a simple example for which we can compute all ranks by hand to help the reader better grasp how the principle works.\nExample 4 (simple discrete) Consider X = {1,2}, Y = {0,1,2}, and two points D={(1,1),(2,2)} lying on the diagonal x=y, with polynomial (zero, constant, linear) least squares regressors R= {r0,r1,r2} (see Ex.1). r0 is simply 0, r1 the y-average, and r2 the line through points (1,y1) and (2,y2). This, together with the quadratic Loss for generic y′ and observed y=(1,2) and fixed x=(1,2), is summarized in the following table\nd rd(x|x,y′) Lossd(y′|x) Lossd(D) 0 0 y′1 2 + y′2 2 5 1 1 2 (y′1 + y ′ 2) 1 2 (y′2 − y′1)2 12 2 (y′2 − y′1)(x− 1) + y′1 0 0\nFrom the Loss we can easily compute the Rank for all nine y′∈{0,1,2}2. Equal rank due to equal loss is indicated by a “=” in the table below. Whole equality groups are actually assigned the rank of their right-most member, e.g., for d=1 the ranks of (y′1,y ′ 2)=(0,1),(1,0),(2,1),(1,2) are all 7 (and not 4,5,6,7).\nRankrd(y ′ 1y ′ 2|12)\nd 1 2 3 4 5 6 7 8 9 Rankrd(D) 0 y′1y ′ 2 = 00 < 01 = 10 < 11 < 02 = 20 < 21 = 12 < 22 8 1 y′1y ′ 2 = 00 = 11 = 22 < 01 = 10 = 21 = 12 < 02 = 20 7 2 y′1y ′ 2 = 00 = 01 = 02 = 10 = 11 = 20 = 21 = 22 = 12 9\nSo LoRP selects r1 as best regressor, since it has minimal rank on D. r0 fits D too badly and r2 is too flexible (perfectly fits all D\n′). ♦ LoRP for continuous Y. We now consider the case of continuous or measurable spaces Y , i.e., normal regression problems. We assume Y = IR in the following exposition, but the idea and resulting principle hold for more general measurable\nspaces like IRm. We simply reduce the model selection problem to the discrete case by considering the discretized space Yε=εZZ for small ε>0 and discretize y;yε∈εZZn (“;” means “is replaced by”). Then Rankεr(L) :=#{y′ε∈Ynε :Lossr(y′ε|x)≤L} with L=Lossr(yε|x) counting the number of ε-grid points in the set\nVr(L) := {y′ ∈ Yn : Lossr(y′|x) ≤ L} (3)\nwhich we assume (and later assure) to be finite, analogous to the discrete case. Hence Rankεr(L) ·εn is an approximation of the loss volume |Vr(L)| of set Vr(L), and typically Rankεr(L) ·εn = |Vr(L)| ·(1+O(ε)) → |Vr(L)| for ε → 0. Taking the logarithm we get LRεr(y|x)=logRankεr(L)=log|Vr(L)|−nlogε+O(ε). Since nlogε is independent of r, we can drop it in comparisons like (2). So for ε→0 we can define the log-loss “rank” simply as the log-volume\nLRr(y|x) := log |Vr(L)|, where L := Lossr(y|x) (4)\nPrinciple 5 (LoRP for regression) For measurable Y, the best regressor r :D× X →Y in some class R for data D=(x,y) is the one with the smallest loss volume:\nrbest = argmin r∈R LRr(y|x) ≡ argmin r∈R |Vr(L)|\nwhere LR, Vr, and L are defined in (3) and (4), and |Vr(L)| is the volume of Vr(L)⊆ Yn.\nFor discrete Y with counting measure we recover the discrete LoRP (Principle 3).\nExample 6 (simple continuous) Consider Example 4 but with interval Y=[0,2]. The first table remains unchanged, while the second table becomes\nd Vd(L) = {y′ ∈ [0, 2]2 : ...} |Vd(L)| Lossd(D) |Vd(Lossd(D))| 0 y′1 2 + y′2 2 ≤ L π 4 L if L≤4; 4 if L≥8;\n2 √ L−4+L(π\n4 −cos−1( 2√ L )) else\n5 . = 3.6\n1 1 2 (y′2 − y′1)2 ≤ L 4 √ 2L−2L if L≤2; 4 if L≥2\n1 2\n3\n2 0 ≤ L 4 0 4\nSo LoRP again selects r1 as best regressor, since it has smallest loss volume on D. ♦\nInfinite rank or volume. Often the loss rank/volume will be infinite, e.g., if we had chosen Y = ZZ in Ex.4 or Y = IR in Ex.6. There are various potential remedies. We could modify (a) the regressor r or (b) the Loss to make LRr finite, (c) the Loss Rank Principle itself, or (d) find problem-specific solutions. Regressors r with infinite rank might be rejected for philosophical or pragmatic reasons. We will briefly consider (a) for linear regression later, but to fiddle around with r in a generic (blackbox way) seems difficult. We have no good idea how to tinker with LoRP (c), and also a patched LoRP may be less attractive. For kNN on a grid we\nlater use remedy (d). While in (decision) theory, the application’s goal determines the loss, in practice the loss is often more determined by convenience or rules of thumb. So the Loss (b) seems the most inviting place to tinker with. A very simple modification is to add a small penalty term to the loss.\nLossr(y|x) ; Lossαr (y|x) := Lossr(y|x) + α‖y‖2, α > 0 “small” (5)\nThe Euclidean norm ‖y‖2 :=∑ni=1y2i is default, but other (non)norm regularizations are possible. The regularized LRαr (y|x) based on Lossαr is always finite, since {y : ‖y‖2≤L} has finite volume. An alternative penalty αŷ⊤ŷ, quadratic in the regression estimates ŷi=r(xi|x,y) is possible if r is unbounded in every y→∞ direction.\nA scheme trying to determine a single (flexibility) parameter (like d and k in the above examples) would be of no use if it depended on one (or more) other unknown parameters (α), since varying through the unknown parameter leads to any (non)desired result. Since LoRP seeks the r of smallest rank, it is natural to also determine α=αmin by minimizing LR α r w.r.t. α. The good news is that this leads to meaningful results. Interestingly, as we will see later, a clever choice of α may also result in alternative optimalities of the selection procedure."
    }, {
      "heading" : "3 LoRP for y-Linear Models",
      "text" : "In this section we consider the important class of y-linear regressions with quadratic loss function. By “y-linear regression”, we mean the linearity is only assumed in y and the dependence on x can be arbitrary. This class is richer than it may appear. It includes the normal linear regression model, kNN (Example 7), kernel (Example 8), and many other regression models. For y-linear regression and Y=IR, the loss rank is the volume of an n-dimensional ellipsoid, which can efficiently be computed in time O(n3) (Theorem 10). For the special case of projective regression, e.g., linear basis function regression (Example 9), we can even determine the regularization parameter α analytically (Theorem 11).\ny-Linear regression. We assume Y = IR in this section; generalization to IRm is straightforward. A y-linear regressor r can be written in the form\nŷ = r(x|x,y) = n ∑\nj=1\nmj(x,x)yj ∀x ∈ X and some mj : X × X n → IR (6)\nParticularly interesting is r for x=x1,...,xn.\nŷi = r(xi|x,y) = ∑\nj\nMij(x)yj with M : X n → IRn×n (7)\nwhere matrix Mij(x)=mj(xi,x). Since LoRP needs r only on the training data x, we only need M .\nExample 7 (kNN ctd.) For kNN of Ex.2 we have mj(x,x)= 1 k if j∈Nk(x) and 0 else, and Mij(x)= 1 k if j∈Nk(xi) and 0 else. ♦\nExample 8 (kernel regression) Kernel regression takes a weighted average over y, where the weight of yj to y is proportional to the similarity of xj to x, measured by a kernel K(x,xj), i.e., mj(x,x)=K(x,xj)/ ∑n j=1K(x,xj). For example the Gaussian kernel for X =IRm is K(x,xj)=e−‖x−xj‖22/2σ2 . The width σ controls the smoothness of the kernel regressor, and LoRP selects the real-valued “complexity” parameter σ. ♦ Example 9 (linear basis function regression, LBFR) Let φ1(x),...,φd(x) be a set or vector of “basis” functions often called “features”. We place no restrictions on X or φ :X →IRd. Consider the class of functions linear in φ:\nFd = {fw(x) = ∑d a=1waφa(x) = w ⊤φ(x) : w ∈ IRd}\nFor instance, for X=IR and φa(x)=xa−1 we would recover the polynomial regression Example 1. For quadratic loss function Loss(yi,ŷi)=(yi−ŷi)2 we have\nLossw(y|φ) := n ∑\ni=1\n(yi − fw(xi))2 = y⊤y − 2y⊤Φw +w⊤Bw\nwhere matrix Φ is defined by Φia = φa(xi) and B is a symmetric matrix with Bab = ∑n i=1φa(xi)φb(xi) = [Φ\n⊤Φ]ab. The loss is quadratic in w with minimum at w=B−1Φ⊤y. So the least squares regressor is ŷ= y⊤ΦB−1φ(x), hence mj(x,x) = (ΦB−1φ(x))j and M(x)=ΦB\n−1Φ⊤. ♦ Consider now a general linear regressor M with quadratic loss and quadratic\npenalty LossαM(y|x) = n ∑\ni=1\n(\nyi − ∑n j=1Mijyj\n)2\n+ α‖y‖2 = y⊤Sαy,\nwhere1 Sα = (I −M)⊤(I −M) + αI (8) (I is the identity matrix). Sα is a symmetric matrix. For α>0 it is positive definite and for α=0 positive semidefinite. If λ1,...,λn ≥ 0 are the eigenvalues of S0, then λi+α are the eigenvalues of Sα. V (L)={y′∈IRn :y′⊤Sαy′≤L} is an ellipsoid with the eigenvectors of Sα being the main axes and √\nL/(λi+α) being their length. Hence the volume is\n|V (L)| = vn n ∏\ni=1\n√\nL\nλi + α =\nvnL n/2\n√ detSα\nwhere vn=π n/2/n\n2 ! is the volume of the n-dimensional unit sphere, z! :=Γ(z+1), and\ndet is the determinant. Taking the logarithm we get\nLRαM(y|x) = log |V (LossαM(y|x))| = n2 log(y⊤Sαy)− 12 log detSα + log vn (9) Since vn is independent of α and M it is possible to drop vn. Consider now a class of linear regressors M= {M}, e.g., the kNN regressors {Mk : k ∈ IN} or the d-dimensional linear basis function regressors {Md :d∈IN0}.\n1The mentioned alternative penalty α‖ŷ‖2 would lead to Sα =(I−M)⊤(I−M)+αM⊤M . For LBFR, penalty α‖ŵ‖2 is popular (ridge regression). Apart from being limited to parametric regression, it has the disadvantage of not being reparametrization invariant. For instance, scaling φa(x);γaφa(x) does not change the class Fd, but changes the ridge regressor.\nTheorem 10 (LoRP for y-linear regression) For Y=IR, the best linear regressor M :X n→IRn×n in some class M for data D=(x,y) is\nM best = argmin M∈M,α≥0 {n 2 log(y⊤Sαy)− 12 log detSα} = argmin M∈M α≥0\n{ y⊤Sαy\n(detSα)1/n\n}\n(10)\nwhere Sα=Sα(M) is defined in (8).\nThe last expression shows that linear LoRP minimizes the Loss times the geometric average of the squared axes lengths of ellipsoid V (1). Note that M best depends on y unlike the M ∈M. Nullspace of S0. If M has an eigenvalue 1, then S0 = (I−M)⊤(I−M) has a zero eigenvalue and α > 0 is necessary, since detS0 = 0. Actually this is true for most practical M . Most linear regressors are invariant under a constant shift of y, i.e., r(x|x,y+c) = r(x|x,y)+c, which implies that M has eigenvector (1,...,1)⊤ with eigenvalue 1. This can easily be checked for kNN (Ex.2), kernel (Ex.8), and LBFR (Ex.9). Such a generic 1-eigenvector effecting all M ∈M could easily and maybe should be filtered out by considering only the orthogonal space or dropping these λi=0 when computing detS0. The 1-eigenvectors that depend on M are the ones where we really need a regularizer α > 0. For instance, Md in LBFR has d eigenvalues 1, and MkNN has as many eigenvalues 1 as there are disjoint components in the graph determined by the edges Mij>0. In general we need to find the optimal α numerically. If M is a projection we can find αm analytically.\nNumerical approximation of (detSα) 1/n and the computational complexity of linear LoRP. For each α and candidate model, the determinant of Sα in the general case can be computed in time O(n3). Often M is a very sparse matrix (like in kNN) or can be well approximated by a sparse matrix (like for kernel regression), which allows us to approximate detSα sometimes in linear time [Reu02]. To search the optimal α and M , the computational cost depends on the range of α we search and the number of candidate models we have.\nProjective regression. Consider a projection matrix M =P =P 2 with d(= trP ) eigenvalues 1, and n−d zero eigenvalues. For instance, M=ΦB−1Φ⊤ of LBFR Ex.9 is such a matrix. This implies that Sα has d eigenvalues α and n−d eigenvalues 1+α, thus detSα=α d(1+α)n−d. Let ρ=‖y−ŷ‖2/‖y‖2, then y⊤Sαy=(ρ+α)y⊤y and\nLRαP = n 2 logy⊤y + n 2 log(ρ+ α)− d 2 logα− n−d 2 log(1 + α). (11)\nSolving ∂LRαP/∂α=0 w.r.t. α we get a minimum at α=αm := ρd\n(1−ρ)n−d provided that 1−ρ>d/n. After some algebra we get\nLRαmP = n 2 logy⊤y− n 2 KL( d n ‖1−ρ), where KL(p‖q) := plog p q +(1−p)log 1−p 1−q (12)\nis the relative entropy or the Kullback-Leibler divergence. Note that (12) is still valid without the condition 1−ρ>d/n (the term log((1−ρ)n−d) has been canceled\nin the derivation). What we need when using (12) is that d<n and ρ< 1, which are very reasonable in practice. Interestingly, if we use the penalty α‖ŷ‖2 instead of α‖y‖2, the loss rank then has the same expression as (12) without any condition2.\nMinimizing LRαmP w.r.t. P is equivalent to maximizing KL( d n ‖1−ρ). The term ρ is a measure of fit. If d increases, then ρ decreases and otherwise. We are seeking a tradeoff between the model complexity d and the measure of fit ρ, and LoRP suggests the optimal tradeoff by maximizing KL.\nTheorem 11 (LoRP for projective regression) The best projective regressor P :X n→IRn×n with P =P 2 in some projective class P for data D=(x,y) is\nP best = argmax P∈P KL( trP (x) n ‖y⊤P (x)y y⊤y ). (13)"
    }, {
      "heading" : "4 Optimality Properties of LoRP for Variable Se-",
      "text" : "lection\nIn the previous sections, LoRP was stated for general-purpose model selection. By restricting attention to linear regression models, we will point out in this section some theoretical properties of LoRP for variable (also called feature or attribute) selection.\nVariable selection is probably the most fundamental and important topic in linear regression analysis. At the initial stage of modeling, a large number of potential covariates are often introduced; one then has to select a smaller subset of the covariates to fit/interpret the data. There are two main goals of variable selection, one is model identification, the other is regression estimation. The former aims at identifying the true subset generating the data, while the latter aims at estimating efficiently the regression function, i.e., selecting a subset that has the minimum mean squared error loss. Note that whether or not there is a selection criterion achieving simultaneously these two goals is still an open question [Yan05, Grü04]. We show that with the optimal parameter α (defined as αm that minimizes the loss rank LR α M in α), LoRP satisfies the first goal, while with a suitable choice of α, LoRP satisfies the second goal.\nGiven d+1 potential covariates X0≡1,X1,...,Xd and a response variable Y , let X=x be a non-random design matrix of size n×(d+1) and y be a response vector respectively (if y and X are centered, then the covariate 1 can be omitted from the models). Denote by S={0,j1,...j|S|−1} the candidate model that has covariates X0,Xj1,...,Xj|S|−1. Under a proposed model S, we can write\ny = XSβS + σSǫ\n2 Then Sα=(In−P )⊤(In−P )+αP⊤P = In+(α−1)P has d eigenvalues α and n−d eigenvalues 1, thus det(Sα)=α d. The loss rank LRαP = n 2 logy\n⊤y+ n2 log(1+(α−1)(1−ρ))− d2 logα is minimized at αm= ρd (1−ρ)(n−d) . After some algebra we get the same expression of LR αm P as (12).\nwhere ǫ is noise with expectation E[ǫ] = 0 and covariance Cov(ǫ) = In, σS > 0, βS = (β0,βj1 ,...,βj|S|−1)\n⊤, and XS is the n×|S| design matrix obtained from X by removing the (j+1)st column for all j 6∈S. Model consistency of LoRP for variable selection. The ordinary least squares (OLS) fitted vector under model S is ŷS=MSy with MS=XS(X⊤SXS)−1X⊤S being a projection matrix. From Theorem 11 the best subset chosen by LoRP is\nŜn = argminS LR αm S = argmaxS {KL( |S| n ‖1− ρS)}, ρS = ‖y−ŷS‖ 2 ‖y‖2 .\nThe term ρS is a measure of fit. It will be very close to 0 if model S is big, otherwise, it will be close to 1 if S is too small. Therefore, it is reasonable to consider only cases in which ρS is bounded away from 0 and 1. In order to prove the theoretical properties of LoRP, we need the following technical assumption.\n(A) For each candidate model S, ρS is bounded away from 0 and 1, i.e., there are constants c1 and c2 such that 0<c1≤ρS ≤c2<1 with probability 1 (w.p.1).\nLet σ̂2S=‖y−ŷS‖2/n and Snull={0}. It is easy to see that for every S 1−ρS = ‖ŷS‖2/‖y‖2, nσ̂2S = ρS‖y‖2, n ȳ2 = ‖ŷSnull‖2 ≤ ‖ŷS‖2 ≤ ‖y‖2 (14) where ȳ denotes the arithmetic mean ∑n\ni=1yi/n. Assumption (A) follows from\n(A’) 0< lim inf n→∞ (ȳ)2≤ lim sup n→∞ ( 1 n ‖y‖2)<∞ and ∀S : σ̂2S→σ2S>0 w.p.1.\nThe first condition of (A’) is obviously very mild and satisfied in almost all cases in practice. The second one is routinely used to derive asymptotic properties of model selection criteria (e.g., Theorem 2 of [Sha97] and Condition 1 of [WLT07]).\nLemma 12 (LoRP for variable selection) The loss rank of model S is LRS ≡ LRαmS = n2 log(nσ̂2S) + n2H( |S| n ) + d 2 log 1−ρS ρS (15) where ρS and σ̂2S are defined in (14), and H(p) :=−plogp−(1−p)log(1−p) is the entropy of p. Under Assumption (A) or (A’), after neglecting constants independent of S, the loss rank of model S has the form\nLRS = n 2 log σ̂2S + |S| 2 log n+OP(1), (16)\nwhere OP(1) denotes a bounded random variable w.p.1.\nProof. Inserting y⊤y = nσ̂2S/ρS into (12) and rearranging terms gives (15). By Assumption (A) the last term in (15) is bounded w.p.1. Taylor expansion log(1−p) =−p+O(p2) implies H(p)/p+logp→ 1, hence n\n2 H( |S| n ) = |S| 2 logn+O(1).\nFinally, dropping the S-independent term n 2 logn from (15) gives (16).\nThis lemma implies that the loss rank LRS here is a BIC-type criterion, thus we immediately can state without proof the following theorem which is the well-known model consistency of BIC-type criteria (interested readers can find the routine proof in, for example, [Cha06]).\nTheorem 13 (Model consistency) Under Assumption (A) or (A’), LoRP is model consistent for variable selection in the sense that the probability of selecting the true model goes to 1 for data size n→∞.\nThe optimal regression estimation of LoRP.The second goal of model selection is often measured by the (asymptotic) mean efficiency [Shi83] which is briefly defined as follows. Let ST denote the true model (which may contain an infinite number of covariates). For a candidate model S, let Ln(S)=‖XSTβST−XSβ̂S‖2 be the squared loss where β̂S is the OLS estimate, and Rn(S)=E[Ln(S)] be the risk. The mean efficiency of a selection criterion δ is defined by the ratio\neff(δ) = infS Rn(S) E[Ln(Sδ)] ≤ 1\nwhere Sδ is the model selected by δ. δ is said to be asymptotically mean efficient if lim infn→∞eff(δ)=1.\nBy minimizing the loss rank in α we have shown in the previous paragraph that LoRP satisfies the first goal of model selection. We now show that with a suitable choice of α, LoRP also satisfies the second goal.\nFrom (11), we have\nLRαS(y|x) = n2 log(σ̂2S + αny⊤y) + n2 log n− |S| 2 log(α)− n−|S| 2 log(1 + α).\nBy choosing α=α̃=exp(− n(n+|S|)|S|(n−|S|−2)), under Assumption (A), the loss rank of model S (neglecting the common constant n\n2 logn) is proportional to\nLRα̃S(y|x) = n log σ̂2S + n(n+|S|)n−|S|−2 + oP(1),\nwhich is the corrected AIC of [HT89]. As a result, LoRP(α̃) is optimal in terms of regression estimation, i.e., it is asymptotically mean efficient ([Shi83], 1983; [Sha97], 1997).\nTheorem 14 (Asymptotic mean efficiency) Under Assumption (A) or (A’), with a suitable choice of α, the loss rank is proportional to the corrected AIC. As a result, LoRP is asymptotically mean efficient."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section we present a simulation study for LoRP, compare it to other methods and also demonstrate how LoRP can be used for some specific problems like choosing tuning parameters for kNN and spline regression. All experiments are conducted by using MATLAB software and the source code is freely available at http://www.hutter1.net/ai/lorpcode.zip.\nComparison to AIC and BIC for model identification. Samples are generated from the model\ny = β0 + β1X1 + ...+ βdXd + ǫ, ǫ ∼ N(0, σ2) (17)\nwhere β is the vector of coefficients with some zero entries. Without loss of generality, we assume that β0=0, otherwise, we can center the response vector y and standardize the design matrix X to exclude β0 from the model. We shall compare the performance of LoRP to that of BIC and AIC with various factors n, d and signal-to-noise ratio (SNR) which is ‖β‖2/σ2 (‖β‖2 is often called the length of the signal).\nFor a given set of factors (n, d, SNR), the way we simulate a dataset from model (17) is as follows. Entries ofX are sampled from a uniform distribution on [−1,1]. To generate β, we first create a vector u=(u1,...,ud)\n⊤ whose entries are sampled from a uniform distribution on [−1,1]. The number of true covariates d∗ is randomly selected from {1,2,...,d}, the last d−d∗ entries of u are set to zero, then coefficient vector β is computed by βi = {length of signal}∗ui/||u||. In our simulation, the length of signal was fixed to be 10. n observation errors ǫ1,...,ǫn are sampled from a normal distribution with mean 0 and variance σ2=||β||2/SNR. Finally, the response vector is computed by y=Xβ+ǫ. For each set of factors (n, d, SNR), 1000 datasets are simulated in the same manner to assess the average performance of the methods. For simplicity, a candidate model is specified by its order, i.e., we search the best model among only d models {1},{1,2}...,{1,2,...,d}. For the general case, an efficient branch-and-bound algorithm [Mil02, Chp.3] can be used to exhaustively search for the best subsets.\nTable 1 presents percentages of correctly-fitted models with various factors n, d and SNR. As shown, LoRP outperforms the others. The better performance of LoRP over BIC, which is the most popular criterion for model identification, is very encouraging. This is probably because LoRP is a selection criterion with a datadependent penalty. This improvement needs a theoretical justification which we intend to do in the future.\nComparison to AIC and BIC for regression estimation. Consider the following model which is from [Shi83]\ny = y(x) = log 1 1−x + ǫ, ǫ ∼ N(0, σ2), x ∈ [0, 1). (18)\nWe approximate the true function by a Fourier series and consider the problem of choosing a good order among models\ny = β0 +\nk−1 ∑\nl=1\ncos(πlx/δ) l+1 βl + ǫ, k = 1, ..., K.\nIn the present context, a model in Section 4 is completely specified by the order K of the Fourier series. Samples are created from (18) at the points xi=δ i n+1\n, i=1,...,n. As in [Shi83], we take δ= .99, and K=163 with various n and σ. The performance is measured by the estimate of mean efficiency over 1000 replications.\nTable 2 represents the simulation results. In general, LoRP (with α= α̃ as in Section 4) outperforms the others, except for cases with unrealistically high noise level. For cases with high noise, mean efficiency of BIC is often larger than that of AIC and LoRP. This was also shown in the simulation study of [Shi83], Table 1. This phenomenon can be explained as follows.\nThe risk of model k (the model specified by its order k) is Rn(k) = ‖(I− Mk)ytrue‖2+kσ2 where Mk is the regression matrix under model k and ytrue is the vector of true values y(xi). When σ→∞, the ideal k⋆=arginfkRn(k)→1. Because BIC penalizes the model complexity more strongly than AIC and LoRP do, the order chosen by BIC is closer to k⋆=1 than the ones chosen by AIC and LoRP. As a result, mean efficiency of BIC is larger than that of the others.\nLoRP for selecting a good number of neighbors in kNN. Let us now see how LoRP can be applied to select a good parameter k in kNN regression.\nWe created a dataset of n=100 observations (xi,yi) from the model:\ny = f(x) + ε, with f(x) = sin(12(x+0.2)) x+0.2 , x ∈ [0, 1] (19)\nwhere ε∼N(0,σ2) with σ=0.5. The regression matrix M (k) for kNN regression is determined by M\n(k) ij = 1 k if j∈Nk(xi) and 0 else. Then, the loss rank is\nLR(k) = inf α≥0\n{n 2 log(y⊤S(k)α y)− 12 log detS(k)α },\nwhere S (k) α = (I−M (k))⊤(I−M (k))+αI. The most widely-used method to select a good k is probably Generalized Cross-Validation (GCV) [CW79]: GCV(k)=n‖(I− M (k))y‖2/[tr(I−M (k))]2. To judge how well GCV and LoRP work, we compare them to the expected prediction error defined as\nEPE(k) =\nn ∑\ni=1\nE(yi − ŷi)2 = n ∑\ni=1\n[\nσ2 + (f(xi)− 1k ∑\nj∈Nk(xi) f(xj))\n2 + σ 2\nk\n]\n.\nFigure 1(a) shows the curves LR(k), GCV(k), EPE(k) for k = 2,...,20 (the trivial case k=1 is omitted), in which k=7-nearest neighbors is chosen by LoRP and k=8 is chosen by GCV. The “ideal” k is 5. Both LoRP and GCV do a reasonable job. LoRP works slightly better than GCV.\nLoRP for selecting a good smoothing parameter. We now further demonstrate the use of LoRP in selecting a good smoothing parameter for spline regression. Consider the following problem: find a function belonging to the class of functions with continuous 2nd derivative that minimizes the following penalized residual sum\nof squares:\nRSS(f) = n ∑\ni=1\n(yi − f(xi))2 + λ ∫ (f ′′(t))2dt,\nwhere λ is called the smoothing parameter. The second term penalizes the curvature of function f and the smoothing parameter λ controls the amount of penalty. Our goal is to choose a good λ.\nIt is well known (see, e.g., [HTF01], Section 5.4) that the solution is a natural spline f(x)=\n∑n j=1Nj(x)θj where N1(x),...,Nn(x) are the basis functions of the\nnatural cubic spline:\nN1(x) = 1, N2(x) = x, Nk+2(x) = dk(x)− dn−1(x) with dk(x) = (x−xk) 3 +−(x−xn)3+ xn−xk .\nThe problem thus reduces to finding a vector θ∈IRn that minimizes\nRSS(θ) = (y −Nθ)⊤(y −Nθ) + λθ⊤Ωθ\nwhere Nij =Nj(xi) and Ωij = ∫ N ′′i (x)N ′′ j (x)dx. It is easy to see that the solution is θ̂λ = (N ⊤N+λΩ)−1N⊤y, and the fitted vector is ŷ = N θ̂λ = Mλy with Mλ = N(N⊤N+λΩ)−1N⊤y. The fitted vector is linear in y, thus the loss rank is\nLR(λ) = argmin α≥0\n{n 2 log(y⊤Sαλy)− 12 log detSαλ}\nwhere Sαλ =(I−Mλ)⊤(I−Mλ)+αI. Let us consider again the dataset generated from model (19). Figure 1(b) shows the curves LR(λ), GCV(λ) and EPE(λ). The derivation of expressions for GCV(λ) and EPE(λ) is similar to the previous example. λ≈ 3×10−4 is the optimal value selected by the “ideal” criterion EPE. λ≈5×10−4 and λ≈7×10−4 are selected by LoRP and GCV, respectively. One again, like the previous example, LoRP selects a better λ than GCV does."
    }, {
      "heading" : "6 Comparison to Gaussian Bayesian Linear Re-",
      "text" : "gression\nWe now consider LBFR from a Bayesian perspective with Gaussian noise and prior, and compare it to LoRP. In addition to the noise model as in PML, one also has to specify a prior. Bayesian model selection (BMS) proceeds by selecting the model that has largest evidence. In the special case of LBFR with Gaussian noise and prior and a type II maximum likelihood estimate for the noise variance, the expression for the evidence has a similar structure as the expression of the loss rank.\nGaussian Bayesian LBFR / MAP. Recall from Sec.3 Ex.9 that Fd is the class of functions fw(x)=w ⊤φ(x) (w∈IRd) that are linear in feature vector φ. Let\nGaussN(z|µ, σ) := exp(−1 2 (z − µ)⊤σ−1(z − µ)) (2π)N/2 √ det σ\n(20)\ndenote a general N -dimensional Gaussian distribution with mean µ and covariance matrix σ. We assume that observations y are perturbed from fw(x) by independent additive Gaussian noise with variance β−1 and zero mean, i.e., the likelihood of y under model w is P(y|w) =Gaussn(y|Φw,β−1I), where Φia =φa(xi). A Bayesian assumes a prior (before seeing y) distribution on w. We assume a centered Gaussian with covariance matrix (αC)−1, i.e., P(w)=Gaussd(w|0,α−1C−1). From the prior and the likelihood one can compute the evidence and the posterior\nEvidence: P(y) =\n∫\nP(y|w)P(w)dw = Gaussn(y|0, β−1S−1) (21)\nPosterior: P(w|y) = P(y|w)P(w)/P (y) = Gaussd(w|ŵ, A−1)\nB := Φ⊤Φ, A := αC + βB, M := βΦA−1Φ⊤, S := I −M, (22) ŵ := βA−1Φ⊤y, ŷ := Φŵ = My\nA standard Bayesian point estimate for w for fixed d is the one that maximizes the posterior (MAP) (which in the Gaussian case coincides with the mean) ŵ = argmaxwP(w|y) = βA−1Φ⊤y. For α → 0, MAP reduces to Maximum Likelihood (ML), which in the Gaussian case coincides with the least squares regression of Ex.9. For α>0, the regression matrix M is not a projection anymore.\nBayesian model selection. Consider now a family of models {F1,F2,...}. Here the Fd are the linear regressors with d basis functions, but in general they could be completely different model classes. All quantities in the previous paragraph implicitly depend on the choice of F , which we now explicate with an index. In particular, the evidence for model class F is PF (y). BMS chooses the model class (here d) F of highest evidence:\nFBMS = argmax F PF(y)\nOnce the model class FBMS is determined, the MAP (or other) regression function fw\nFBMS or MFBMS are chosen. The data variance β −1 may be known or estimated from the data, C is often chosen I, and α has to be chosen somehow. Note that while α→0 leads to a reasonable MAP=ML regressor for fixed d, this limit cannot be used for BMS.\nComparison to LoRP. Inserting (20) into (21) and taking the logarithm we see that BMS minimizes\n− log PF (y) = β2y⊤Sy − 12 log detS − n2 log β 2π\n(23)\nw.r.t. F . Let us estimate β by ML: We assume a broad prior α≪β so that β ∂S ∂β = O(α β ) can be neglected. Then −∂logPF (y) ∂β = 1 2 y⊤Sy− n 2β +O(α β n) = 0 ⇔ β ≈ β̂ := n/(y⊤Sy). Inserting β̂ into (23) we get\n− log PF(y) = n2 logy⊤Sy − 12 log detS − n2 log n2πe (24)\nTaking an improper prior P(β)∝β−1 and integrating out β leads for small α to a similar result. The last term in (24) is a constant independent of F and can be ignored. The first two terms have the same structure as in linear LoRP (10), but the matrix S is different. In both cases, α act as regularizers, so we may minimize over α in BMS like in LoRP. For α=0 (which neither makes sense in BMS nor in LoRP), M in BMS coincides with M of Ex.9, but still the S0 in LoRP is the square of the S in BMS. For α>0, M of BMS may be regarded as a regularized regressor as suggested in Sec.2 (a), rather than a regularized loss function (b) used in LoRP. Note also that BMS is limited to (semi)parametric regression, i.e., does not cover the non-parametric kNN Ex.2 and kernel Ex.8, unlike LoRP.\nSince B only depends on x (and not on y), and all P are implicitly conditioned on x, one could choose C = B. In this case, M = γΦB−1Φ⊤, with γ = β\nα+β < 1\nfor α> 0, is a simple multiplicative regularization of projection ΦB−1Φ⊤, and (24) coincides with (11) for suitable α, apart from an irrelevant additive constant, hence minimizing (24) over α also leads to (12)."
    }, {
      "heading" : "7 Comparison to other Model Selection Schemes",
      "text" : "In this section we give a brief introduction to PML for (semi)parametric regression, and its major instantiations, AIC, BIC, and MDL principle, whose penalty terms are all proportional to the number of parameters d. The effective number of parameters is often much smaller than d, e.g., if there are soft constraints like in ridge regression. We compare MacKay’s trace formula [Mac92] for Gaussian Bayesian LBFR and Hastie’s et al. trace formula [HTF01] for general linear regression with LoRP.\nPenalized ML (AIC, BIC, MDL). Consider a d-dimensional stochastic model class like the Gaussian Bayesian linear regression example of Section 6. Let Pd(y|w) be the data likelihood under d-dimensional model w∈IRd. The maximum likelihood (ML) estimator for fixed d is\nŵ = argmax w Pd(y|w) = argmin w {− log Pd(y|w)} (25)\nSince −logPd(y|w) decreases with d, we cannot find the model dimension by simply minimizing over d (overfitting). Penalized ML adds a complexity term to get reasonable results\nd̂ = argmin d {− log Pd(y|ŵ) + Penalty(d)} (26)\nThe penalty introduces a tradeoff between the first and second term with a minimum at d̂<∞. Various penalties have been suggested: AIC [Aka73] uses d, BIC [Sch78] and the (crude) MDL [Ris78, Grü04] use d\n2 logn for Penalty(d). There are at least\nthree important conceptual differences to LoRP:\n• In order to apply PML one needs to specify not only a class of regression functions, but a full probabilistic model Pd(y|w),\n• PML ignores or at least does not tell how to incorporate a potentially given loss-function, • PML is mostly limited to selecting between (semi)parametric models.\nWe discuss two approaches to the last item in the remainder of this section (where AIC, BIC, and MDL are not directly applicable): (a) for non-parametric models like kNN or kernel regression, or (b) if d does not reflect the “true” complexity of the model. [Mac92] suggests an expression for the effective number of parameters deff as a substitute for d in case of (b), while [HTF01] introduce another expression which is applicable for both (a) and (b).\nThe trace penalty for parametric Gaussian LBFR. We continue with the Gaussian Bayesian linear regression example (see Section 6 for details and notation). Performing the integration in (21), [Mac92, Eq.(21)] derives the following expression for the Bayesian evidence for C=I\n− log P(y) = (αÊW + βÊD) + (12 log detA− d2 logα)− n2 log β 2π\n(27)\nÊD = 1 2 ‖Φŵ − y‖22, ÊW = 12‖ŵ‖22\n(the first bracket in (27) equals β 2 y⊤Sy and the second equals −1 2 logdetS, cf. (23)). Minimizing (27) w.r.t. α leads to the following relation:\n0 = −∂ log P(y) ∂α = ÊW + 1 2 trA−1 − d 2α ( ∂ ∂α log detA = trA−1)\nHe argues that α‖ŵ‖22 corresponds to the effective number of parameters, hence\ndMcKeff := α‖ŵ‖22 = 2αÊW = d− αtrA−1 (28)\nThe trace penalty for general linear models. We now return to general linear regression ŷ=M(x)y (7). LBFR is a special case of a projection matrix M =M2 with rank d = trM being the number of basis functions. M leaves d directions untouched and projects all other n−d directions to zero. For general M , [HTF01, Sec.5.4.1] argue to regard a direction that is only somewhat shrunken, say by a factor of 0< β < 1, as a fractional parameter (β degrees of freedom). If β1,...,βn are the shrinkages = eigenvalues of M , the effective number of parameters could be defined as [HTF01, Sec.7.6]\ndHTFeff :=\nn ∑\ni=1\nβi = trM,\nwhere HTF stands for Hastie-Tibshirani-Friedman, which generalizes the relation d=trM beyond projections. For MacKay’s M (22), trM=d−αtrA−1, i.e., dHTFeff is consistent with and generalizes dMcKeff .\nProblems. Though nicely motivated, the trace formula is not without problems. First, since for projections, M=M2, one could have argued equally well for dHTFeff =\ntrM2. Second, for kNN we have trM= n k (since M is 1 k on the diagonal), which does not look unreasonable. Consider now kNN’, which is defined as follows: we average over the k nearest neighbors excluding the closest neighbor. For sufficiently smooth functions, kNN’ for suitable k is still a reasonable regressor, but trM=0 (since M is zero on the diagonal). So dHTFeff =0 for kNN’, which makes no sense and would lead one to always select the k=1 model.\nRelation to LoRP. In the case of kNN’, trM2 would be a better estimate for the effective dimension. In linear LoRP, −logdetSα serves as complexity penalty. Ignoring the nullspace of S0=(I−M)⊤(I−M) (8), we can Taylor expand −12 logdetS0 in M\n−1 2 log detS0 = −tr log(I−M) =\n∞ ∑\ns=1\n1 s tr(Ms) = trM + 1 2 trM2 + ...\nFor BMS (24) with S= I−M (22) we get half of this value. So the trace penalty may be regarded as a leading order approximation to LoRP. The higher order terms prevent peculiarities like in kNN’.\nCoding/MDL interpretation of LoRP. The basic idea of MDL is as follows [Grü04]: “The goal of statistical inferences may be cast as trying to find regularity in the data. ‘Regularity’ may be identified with ‘ability to compress’. MDL combines these two insights by viewing learning as data compression: it tells us that, for a given set of hypotheses H and data set D, we should try to find the hypothesis or combination of hypotheses in H that compress D most.”\nThe standard incarnation of (crude) MDL is as follows: If H is a stochastic model of (discrete) data D, we can code D (by Shannon-Fano) in ⌈−log2P(D|H)⌉ bits. If we have a class of models H, we also have to code H (somehow in, say, L(H) bits) in order to be able to decode D. MDL chooses the hypothesis HMDL= argminH∈H{−log2P(D|H)+L(H)} of minimal two-part code. For instance, if H is the class of all polynomials of all degrees with each coefficient coded to 1\n2 log2n bits\n(i.e., O(n−1/2) accuracy) and we condition on x, i.e., D;y|x, MDL takes the form (25) and (26), i.e., HMDL=(ŵ,d̂).\nWe now give LoRP (for discrete D) a data compression/MDL interpretation. For simplicity, we will first assume that all loss values are different, i.e., if Lossr(y\n′|x) 6=Lossr(y′′|x) for y′ 6= y′′ (adding infinitesimal random noise to Lossr easily ensures this). In this case, Rankr(·|x) :Yn→ IN is an order preserving bijection, i.e., Rankr(y\n′|x)<Rankr(y′′|x) iff Lossr(y′|x)<Lossr(y′′|x) with no gaps in the range of Rankr(·|x).\nPhrased differently, Rankr(·|x) codes each y′ ∈ Yn as a natural number m in increasing loss-order. The natural number m can itself be coded in ⌈log2m⌉ bits (using plain not prefix coding). Let us call this code of y′ the Loss Rank Code (LRC). LRC has a nice characterization: LRC is the shortest loss-order preserving code. Ignoring the rounding, the Length of LRCr(y ′|x) is LRr(y′|x):\nProposition 15 (Minimality property) If all loss values are different, i.e., if\nLossr(y ′|x) 6= Lossr(y′′|x) for all y′ 6= y′′\nthen the loss rank (code) of y is the smallest/shortest among all loss-order preserving rankings/codes C in the sense that\nRank(y) = min{C(y) : C ∈ Yn→IN ∧ (⋆) } ⌊LR(y)/ log 2⌋ = min{Length(C(y)) : C ∈ Yn→{0, 1}∗ ∧ (⋆) }\n(⋆) := [Loss(y′) < Loss(y′′) ⇔ C(y′) < C(y′′), ∀y′,y′′]\nThe proof follows from the fact that if a discrete injection (code) is order preserving, there exists a “smallest” one without gaps in the range. So LoRP minimizes the Loss Rank Code, where LRC itself is the shortest among all loss-order preserving codes. From this perspective, LoRP is just a different (non-stochastic, non-parametric, loss-based) incarnation of MDL. The MDL philosophy provides a justification of LoRP (2), its regularization (5), and loss function selection (Section 8). This identification should also allow to apply or adapt the various consistency results of MDL, implying that LoRP is consistent under some mild conditions.\nIf some losses are equal, Rankr(·|x) :Yn→IN still preserves the order ≤, but the mapping is neither surjective nor injective anymore.\nLarge regression classes R. The classes R of regressors we considered so far were discrete and “small”, often indexed by an integer complexity index (like k in kNN or d in LBFR). But large classes are also thinkable.\nAs an extreme case, consider the class of all regressors. Clearly, there is an r=rD which “knows” D and perfectly fits D (r(xi|D)= yi, ∀i), but is the worst possible on all other D′ (r(xi|D′)=∞, ∀i, ∀D′ 6=D). This r has (discrete) Rank 1, so is best according to LoRP. So if R is too large, LoRP can overfit too.\nConsider a more realistic example by not taking all of the first d basis functions in LBFR, but selecting some basis functions φi1 ,...,φid, i.e., R is indexed by d integers, and d may be variable too.\nOne solution approach is to group more regressors in R into one function class F , e.g., the class of functions Fk,d={w1φi1+...wdφid :w∈IRd, 1≤i1<...<id≤k} that are linear in d of the first k bases. Now R is a small class indexed by d and k only.\nLooking at the coding interpretation of LRr and the MDL philosophy, suggests to assign a code to r∈IR in order to get a complete code for D:\nrbest = argmin r {LRr(y|x) + L(r)}\nwhere r is the length of a code for r (given R). For R ≃ IN a single integer has to be coded, e.g., k in L(r) = L(k)≈ log2k bits, which can usually be safely dropped/ignored. For more complex classes like the (ungrouped) LBFR subset selection above, L(r)=L(i1,...,id,d)≈dlog2k+log2d can become important."
    }, {
      "heading" : "8 Loss Functions and their Selection",
      "text" : "General additive loss. Linear LoRP ŷ=M(x)y of Section 3 can easily be generalized to non-quadratic loss. Let us consider the ρ>0 loss\nLossM(y|x) := ( ∑n i=1(yi − ŷi)ρ) 1/ρ = ‖y − ŷ‖ρ = ‖(I−M)y‖ρ\nV (L) = {y′ ∈ IRn : ‖(I−M)y′‖ρ ≤ L} = {(I−M)−1z ∈ IRn : ‖z‖ρ ≤ L} Let vρn := |{z ∈ IRn : ‖z‖ρ ≤ 1}| = 2n ∏n−1 i=1 i ρ !1 ρ !/ i+1 ρ !,\nwhere i ρ ! :=Γ( i ρ +1), be the volume of the unit d-dimensional ρ-norm “ball”. Since V (L) is a linear transformation of this ball with transformation matrix (I−M)−1 and scaling L, we have |V (L)|=vρnLn/det(I−M), hence\nLRM(y|x) = log |V (LossM(y|x))| = n log ‖(I−M)y‖ρ − log det(I−M) + log vρn (29) For the ρ=2 norm, (29) reduces to LR0M (9). Note that LossM :=g(‖y−ŷ‖ρ) leads to the same result (29) for any monotone increasing g, i.e., only the order of the loss matters, not its absolute value. More generally LossM = g( ∑\nih(yi− ŷi)) for any h implies\nLRM(y|x) = n log vhn( ∑ i h(yi − ŷi))− log det(I−M), where vhn(l) := |{z ∈ IRn : ∑ i h(zi) ≤ l}| 1/n\nis a one-dimensional function of l (independent D and M), once to be determined (e.g., vhn(l) = l ·(vρn)\n1/n ∝ l for ρ-norm loss). Regularization may be performed by M;γM with optimization over γ<1.\nLoss-function selection. In principle, the loss function should be part of the problem specification, since it characterizes the ultimate goal. For instance, whether a test should more likely classify a healthy person as sick than a sick person as healthy, depends on the severity of a misclassification (loss) in each direction. In reality, though, having to specify the loss function can be a nuisance. Sure, the loss has to respect some general features, e.g., that it increases with the deviation of ŷi from yi. Otherwise it is chosen by convenience or rules of thumb, rather than by elicitation of the real goal, for instance preferring the Euclidean norm over ρ 6= 2 norms. If we subscribe to the procedure of choosing the loss function, we could ask whether this may be done in a more principled way. Consider a (not too large) class of loss functions Lossα, indexed by some parameter α. For instance, Lossα= ‖y−ŷ‖α from the previous paragraph. The regularized loss (5) also constitutes a class of losses. In this case we minimized over the regularization parameter α. This suggests to choose in general the loss function that has minimal loss rank LRαr . The justifications are similar to the ones for minimizing LRαr w.r.t. r. Note that the term logvρn cannot be dropped anymore, unlike in (10)."
    }, {
      "heading" : "9 Self-Consistent Regression",
      "text" : "So far we have considered only “on-data” regression. LoRP only depends on the regressor r on data D and not on x 6∈ {x1,...,xn}. We now construct canonical regressors for off-data x from regressors given only on-data. First, this may ease the specification of the regression functions, second, it is a canonical way for interpolation (LoRP can’t distinguish between r that are identical on D), and third, we show that many standard regressors (kNN, Kernel, LBFR) are self-consistent in the sense that they are canonical. We limit our exposition to linear regression.\nOff-data regression. A linear regressor is completely determined by the n functions mj (6), but not by the matrix function M (7). Indeed, two sets {mj} and {m′j} that coincide on D=(x,y), i.e. mj(xi|x)=m′j(xi|x) ∀i,j but possibly differ for x 6∈x, lead to the same matrix Mij(x)=mj(xi|x)=m′j(xi|x). LoRP has the advantage of only depending on M , but this also means that it cannot distinguish between an mj that behaves well on x 6∈x and one that, e.g., wildly oscillates outside x.\nTypically, the mj are given and, provided the model complexity is chosen appropriately e.g. by LoRP, they properly interpolate x. Nevertheless, a canonical extension from M to mj would be nice. In this way LoRP would not be vulnerable to bad mj , and we could interpolate D (predict y for any x∈X ) even without mj given a-priori.\nWe define a self-consistent regression scheme based only on M (for all n). We ask for an estimate ŷ of y for x 6∈x. We add a virtual data point (x0,y0) to D, where x0=x. If we knew y0= y we could estimate ŷ0= r(x0|{(x0,y0)}∪D), but we don’t know y0. But we could require a self-consistency condition, namely that ŷ0=y0 for x0 6∈x.\nDefinition 16 (canonical and self-consistent regressors) Let M ′ij(x ′)0≤i,j≤n be the regression matrix for the data set D′={(x0,y0)}∪D=((x0,x),(y0,y))=(x′,y′) of size n+1.\n(i) A linear regressor ỹ0 = r̃(x0|D) is called a canonical regressor for M ′ if the consistency condition ỹ0=r(x0|D′)≡ ∑n j=0M ′ 0jyj holds ∀x0,D.\n(ii) A regressor r is called self-consistent if r̃=r, i.e. if r(x0|{(x0,r(x0|D))}∪D)=r(x0|D) ∀x0,D.\n(iii) A class of regressors R={r} is called self-consistent if R̃={r̃}⊆R.\nWe denote the solution of the self-consistency condition y0= ∑n j=0M ′ 0jyj by ỹ0.\nSo we have to solve\nỹ0 =\nn ∑\nj=1\nM ′0jyj +M ′ 00ỹ0 =⇒ ỹ0 =\n∑n j=1M ′ 0jyj\n1−M ′00 =\n∑n j=1M ′ 0jyj\n∑n j=1M ′ 0j\nwhere the last equality only holds if ∑n j=0M ′ 0j =1, which is often the case, in particular for kNN and Kernel regression, but not necessarily for LBFR.\nProposition 17 (canonical regressor) The linear regressor\ny0 = r̃(x0|D) := n ∑\nj=1\nm̃j(x0|x)yj, where m̃j(x0|x) := M ′0j(x ′)\n1−M ′00(x′)\nis the unique canonical regressor for M ′ (if M ′00<1).\nExample 18 (self-consistent kNN, ↑Ex.2) M ′0j(x′) = 1k for xj ∈ N ′k(x0) and 0 else. The k nearest neighbors N ′k(x0) of x0 among x′ consist of x0 and the k−1 nearest neighbors Nk−1(x0)=:J of x0 among x, i.e. N ′k(x0)={x0}∪Nk−1(x0). Hence\nỹ0 =\n∑n j=1M ′ 0jyj\n∑n j=1M ′ 0j\n=\n∑\nj∈J 1 k yj\n∑\nj∈J 1 k\n= ∑\nj∈J\n1 k−1yj =\nn ∑\nj=1\nM (k−1) 0j yj = rk−1(x0|D) = ŷ0\nCanonical kNN is equivalent to standard (k–1)NN, so the class of canonical kNN regressors coincides with the standard kNN class. ♦ Example 19 (self-consistent kernel)\nM ′0j(x ′) = K(x0, xj) ∑n\nj=0K(x0, xj) =⇒ ỹ0 =\n∑n j=1K(x0, xj)yj\n∑n j=1K(x0, xj)\n= r(x0|D) = ŷ0\nCanonical kernel regression coincides with the standard kernel smoother. ♦ Example 20 (self-consistent LBFR)\nB′ = n ∑\ni=0\nφ(xi)φ(xi) ⊤= B + φ(x0)φ(x0) ⊤\n⇒ M ′0j = φ(x0)⊤B′−1φ(xj) = φ(x0)⊤ [ B−1 − B −1φ(x0)φ(x0)⊤B−1\n1 + φ(x0)⊤B−1φ(x0)\n]\nφ(xj)\n= M0j − M00M0j 1 +M00 = M0j 1 +M00 ⇒ 1−M ′00 =\n1\n1 +M00\nIn the first line we used the Sherman-Morrison formula for inverting B′. In the second line we defined M0j=φ(x0) ⊤B−1φ(xj), extending M .\n⇒ ỹ0 = ∑n j=1M ′ 0jyj\n1−M ′00 =\nn ∑\nj=1\nM0jyj =\nn ∑\nj=1\nmj(x0,x)yj = ŷ0\nCanonical LBFR coincides with standard LBFR. ♦\nProposition 21 (self-consistent regressors) Kernel regression and linear basis function regression are self-consistent. kNN is not self-consistent but the class of kNN regressors R={rkNN :k∈IN} is self-consistent.\nTo summarize, we expect LoRP to select good regressors with proper interpolation behavior for canonical and self-consistent regressors."
    }, {
      "heading" : "10 Nearest Neighbors Classification",
      "text" : "We now consider k-nearest neighbors classification in more detail. In order to get more insight into LoRP we seek a case that allows analytic solution. In general, the determinant detSα cannot be computed analytically, but for x lying on a hypercube of the regular grid X =ZZd we can. We derive exact expressions, and consider the limits n→∞, k→∞, and d→∞.\nkNN on one-dimensional grid. We consider the d=1 dimensional case first. We assume x= (1,2,3,...,n), a circular metric d(xi,xj) = d(i,j) =min{|i−j|,n−|i−j|}, and odd k≤n. The kNN regression matrix\nMij = bi−j with bi−j = 1 k if d(i, j) ≤ k−1 2 and 0 otherwise\nis a diagonal-constant (Toeplitz) matrix with circularity property bi−j=bi−j+n. For instance, for k=3 and n=5\nM = 1\n3\n\n 1 1 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 1\n\n\nFor every circulant matrix, the eigenvectors v1,...,vn are waves vlj = θ jl with θ = e2π √ −1/n. The eigenvalues are the fourier transform b̂l = ∑n j=1bjθ\n−jl of b, since ∑\njMijv l j=\n∑\njbi−jθ jl=\n∑ jbjθ (i−j)l=vli ∑ jbjθ −jl= b̂lvli, where we exploited circularity\nof b and θjl. For MkNN in particular we get\nb̂l =\n↑ circularity\n1\nk\nk−1 2 ∑\nj=− k−1 2\nθ−jl =\n↑ geometric sum\n1\nk θlk/2 − θ−lk/2 θl/2 − θ−l/2 =↑\ninsert θ\nsin(πlk/n) k sin(πl/n) < 1 for l 6= n\nand b̂n=1. The only 1-vector v n=1 corresponds to a constant shift yi;yi+c under which kNN (like many other regressors) is invariant. Instead of regularizing LoRP with α> 0 we can restrict V (L)⊂ IRn to the space orthogonal to vn, which means dropping b̂n=1 in the determinant. Intuitively, since this invariant direction is the same for all k, we can drop the same additive infinite constant from LR for every k, which is irrelevant for comparisons (formally we should compute limα→0{LRαk1− LRαk2}). The exact expression for the restricted log-determinant (denoted by a prime) is\n−1 2 log det′S0 = − log det′(1−M) = −\nn−1 ∑\nl=1\nlog(1−b̂l) =: nk c1nk = c1nktrM\nFor large n (and large k) the expression can be simplified. The exact, large n, and\nlarge k≪n expressions are\nc1nk = − k\nn\nn−1 ∑\nl=1\nlog ( 1− sin(πlk/n) k sin(πl/n) )\nc1∞k = − k\nπ\n∫ π/2\n−π/2 log\n( 1− sin(kz) k sin(z) ) dz\n(\nz = πl/n for l < n 2\nz = πl/n− π else\n)\nc1∞∞ = − 1\nπ\n∫ ∞\n−∞ log\n( 1− sin t t ) dt =̇ 3.202 (t = kz, sin(z) ∼ z)\nFurther, c1∞3=3log3=̇3.295. Since c 1 ∞k is decreasing in k, c 1 ∞k equals 3.2 within 3% for all k.\nkNN on d-dimensional grid. We now consider x = X d = {1,...,n1}d on a ddimensional complete hypercube grid with n= nd1 points and Manhattan distance d(xi,xj)=d(i,j)= ∑d a=1d1(ia,ja) for all xi= i∈X d and xj =j∈X d, where d1 is the one-dimensional circular distance defined above (so actually X d is a discrete torus). For k= kd1, the neighborhood Nk(x) of x is a cube of side-length k1. In this case, M=M1⊗...⊗M1 is a d-fold tensor product of the 1d k1NN matrices M1 of sample size n1. The eigenvectors of M are v l1⊗...⊗vld with eigenvalues b̂l1 ·...· b̂ld. We get\n− log det′(1 −M) = − n1−1 ∑\nl1=1\n...\nnd−1 ∑\nld=1\nlog(1− b̂l1 · ... · b̂ld) (30)\nn≫k→∞−→ − 1 πd\n∫\nIRd log\n( 1− d ∏\na=1\nsin ta ta ) ddt =: n k cd∞∞\nFor instance, for d=2, numerical integration gives c2∞∞=̇2.2 compared to 3.2 in one dimension. For higher dimensions, evaluation of the d-dimensional integral becomes cumbersome, and we resort to a different approximation.\nTaylor series in M . We can also (not only for kNN) expand logdetS0 in a Taylor series in M :\n− log det′(1−M) = −tr′ log(1−M) = ∞ ∑\ns=1\n1 s tr′(Ms)\n= ∞ ∑\ns=1\n1 s (tr′Ms1 ) d = n k\n∞ ∑\ns=1\n1 s (An1k1s) d =: n k cdnk\nwhere we used tr(A⊗B)=tr(A)·tr(B) and (A⊗B)s=As⊗Bs and defined\nAn1k1s := k1 n1 tr′(Ms1 ) = k1 n1\nn1−1 ∑\nl=1\n(b̂l) s n≫k→∞−→ 1\nπ\n∫ ∞\n−∞\n(sin t\nt\n)s\ndt\nThe one-dimensional integral can be expressed as a finite sum with s terms or evaluated numerically. For any n and k one can show that Ank1=Ank2=1>Anks for\ns>2. So the expansion above is useful for large d. Note also that cdnk is monotone decreasing in d. For d→∞ we have\nc∞nk = ∞ ∑\ns=1\n1 s (Anks) ∞ = 1 + 1 2 + 0 + ... = 3 2\ni.e. cdnk decreases monotone in d from about 3.2 to 3 2 .\nThe practical implication of this observation, though, is limited, since k=kd1→∞ is actually not fixed for d→∞. Indeed, in practical high-dimensional problems, k≪n≪3d, but in our grid example k=kd1≥3d. Real data do not form full grids but sparse neighborhoods if d is large."
    }, {
      "heading" : "11 Conclusion and Outlook",
      "text" : "We introduced a new method, the Loss Rank Principle, for model selection. The loss rank of a model is defined as the number of other data that fit the model better than the training data. The model chosen by LoRP is the one of smallest loss rank. The loss rank has an explicit expression in case of linear models. Model consistency and asymptotic efficiency of LoRP were considered. The numerical experiments suggest that LoRP works well in practice. A comparison between LoRP and other methods for model selection was also presented.\nIn this paper, we have only scratched at the surface of LoRP. LoRP seems to be a promising principle with a lot of potential, leading to a rich field. In the following we briefly summarize miscellaneous considerations.\nComparison to Rademacher complexities. For a (binary) classification problem, the rank (1) of classifier r can be re-formulated as the probability that a randomly relabeled sample y′ behaves better than the actual y. The more flexible r is, the larger its rank is. The Rademacher complexity [Kol01, BBL02] of r is the expectation of the difference between the misclassifying loss under the actual y and the misclassifying loss under a randomly relabeled sample y′. The more flexible r is, the larger its Rademacher complexity is. Therefore, there is a close connection between LoRP and Rademacher complexities. Model selection based on Rademacher complexities has a number of attractive properties and has been attracting many researchers, thus it’s worth discovering this connection. Some results have been recently already obtained, however, to keep the present paper not so long, we decide to present the results in another paper.\nMonte Carlo estimates for non-linear LoRP. For non-linear regression we did not present an efficient algorithm for the loss rank/volume LRr(y|x). The high-dimensional volume |Vr(L)| (3) may be computed by Monte Carlo algorithms. Normally Vr(L) constitutes a small part of Yn, and uniform sampling over Yn is not feasible. Instead one should consider two competing regressors r and r′ and compute |V ∩V ′|/|V | and |V ∩V ′|/|V ′| by uniformly sampling from V and V ′ respectively e.g., with a Metropolis-type algorithm. Taking the ratio we get |V ′|/|V | and hence the\nloss rank difference LRr−LRr′, which is sufficient for LoRP. The usual tricks and problems with sampling apply here too.\nLoRP for hybrid model classes. LoRP is not restricted to model classes indexed by a single integral “complexity” parameter, but may be applied more generally to selecting among some (typically discrete) class of models/regressors. For instance, the class could contain kNN and polynomial regressors, and LoRP selects the complexity and type of regressor (non-parametric kNN versus parametric polynomials).\nGenerative versus discriminative LoRP. We have concentrated on counting y’s given fixed x, which corresponds to discriminative learning. LoRP might equally well be used for counting (x,y), as alluded to in the introduction. This would correspond to generative learning. Both regimes are used in practice. See [LJ08] for some recent results on their relative benefit, and further references.\nAcknowledgement. We would like to thank two anonymous reviewers for their detailed and helpful comments. The second author would like to thank the SML@NICTA for supporting a visit which led to the present paper."
    }, {
      "heading" : "Appendix: List of Abbreviations and Notations",
      "text" : "AIC= Akaike Information Criterion. BIC= Bayesian Information Criterion. BMS= Bayesian Model Selection kNN= k Nearest Neighbors. LBFR= Linear Basis Function Regression. LoRP= Loss Rank Principle. LRC = Loss Rank Code. MAP= Maximum a Posterior. MDL= Minimum Description Length. ML= Maximum Likelihood. PML= Penalized Maximum Likelihood. D={(x1,y1),...,(xn,yn)}= observed data. D={D}= set of all possible data D. X×Y=observation space. x=(x1,...,xn)= vector of x-observations, similarly y. f :X →Y= functional dependence between x and y. F= (“small”) class of functions f . H= class of stochastic hypotheses/models. r :D→F= regressor/model. ŷi=r(xi|D)= r-estimate of yi. R= (“small”) class of regressors/models. w∈IRd= parametrization of Fd. Nk(x)= set of indices of the k nearest neighbors of x in D. L=Lossr(D)=Loss(y,ŷ)= empirical loss of r on D.\nRankr(L)=#{y′∈Yn :Lossr(y′|x)≤L}= loss rank of r. V (L)= volume of D under r. LRr(y|x)= log rank/volume of D. LRαr= regularized LRr. deff= effective dimension. mj(x,x)= coefficients of linear regressor. M(x)= linear regression matrix or “hat” matrix. log= natural logarithm. a;b: a is replaced by b."
    } ],
    "references" : [ {
      "title" : "Information theory and an extension of the maximum likelihood principle",
      "author" : [ "H. Akaike" ],
      "venue" : "In Proc. 2nd International Symposium on Information Theory,",
      "citeRegEx" : "Akaike.,? \\Q1973\\E",
      "shortCiteRegEx" : "Akaike.",
      "year" : 1973
    }, {
      "title" : "The relationship between variable selection and data augmentation and a method for prediction",
      "author" : [ "D. Allen" ],
      "venue" : null,
      "citeRegEx" : "Allen.,? \\Q1974\\E",
      "shortCiteRegEx" : "Allen.",
      "year" : 1974
    }, {
      "title" : "Model selection and error estimation",
      "author" : [ "P. Bartlett", "S. Boucheron", "G. Lugosi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2002
    }, {
      "title" : "Testing the order of a model",
      "author" : [ "A. Chambaz" ],
      "venue" : "Ann. Stat.,",
      "citeRegEx" : "Chambaz.,? \\Q2006\\E",
      "shortCiteRegEx" : "Chambaz.",
      "year" : 2006
    }, {
      "title" : "Smoothing noisy data with spline functions: estimating the correct degree of smoothing by the methods of generalized cross-validation",
      "author" : [ "P. Craven", "G. Wahba" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Craven and Wahba.,? \\Q1979\\E",
      "shortCiteRegEx" : "Craven and Wahba.",
      "year" : 1979
    }, {
      "title" : "An Introduction to the Bootstrap",
      "author" : [ "B. Efron", "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Efron and Tibshirani.,? \\Q1993\\E",
      "shortCiteRegEx" : "Efron and Tibshirani.",
      "year" : 1993
    }, {
      "title" : "Tutorial on minimum description length. In Minimum Description Length: recent advances in theory and practice, page Chapters 1 and 2",
      "author" : [ "P.D. Grünwald" ],
      "venue" : "http://www.cwi.nl/∼pdg/ftp/mdlintro.pdf",
      "citeRegEx" : "Grünwald.,? \\Q2004\\E",
      "shortCiteRegEx" : "Grünwald.",
      "year" : 2004
    }, {
      "title" : "Learning Kernel Classifiers",
      "author" : [ "R. Herbrich" ],
      "venue" : null,
      "citeRegEx" : "Herbrich.,? \\Q2002\\E",
      "shortCiteRegEx" : "Herbrich.",
      "year" : 2002
    }, {
      "title" : "Regression and time series model selection in small samples",
      "author" : [ "C.M. Hurvich", "C.L. Tsai" ],
      "venue" : null,
      "citeRegEx" : "Hurvich and Tsai.,? \\Q1989\\E",
      "shortCiteRegEx" : "Hurvich and Tsai.",
      "year" : 1989
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "T. Hastie", "R. Tibshirani", "J.H. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2001
    }, {
      "title" : "The loss rank principle for model selection",
      "author" : [ "M. Hutter" ],
      "venue" : "In Proc. 20th Annual Conf. on Learning Theory (COLT’07),",
      "citeRegEx" : "Hutter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2007
    }, {
      "title" : "Rademacher penalties and structural risk minimization",
      "author" : [ "V. Koltchinskii" ],
      "venue" : "IEEE Trans. Inform. Theory,",
      "citeRegEx" : "Koltchinskii.,? \\Q2001\\E",
      "shortCiteRegEx" : "Koltchinskii.",
      "year" : 2001
    }, {
      "title" : "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators",
      "author" : [ "P. Liang", "M. Jordan" ],
      "venue" : "In Proc. 25th International Conf. on Machine Learning (ICML-2008),",
      "citeRegEx" : "Liang and Jordan.,? \\Q2008\\E",
      "shortCiteRegEx" : "Liang and Jordan.",
      "year" : 2008
    }, {
      "title" : "Subset Selection in Regression",
      "author" : [ "A. Miller" ],
      "venue" : "Chapman & Hall/CRC,",
      "citeRegEx" : "Miller.,? \\Q2002\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 2002
    }, {
      "title" : "Approximation of the determinant of large sparse symmetric positive definite matrices",
      "author" : [ "A. Reusken" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Reusken.,? \\Q2002\\E",
      "shortCiteRegEx" : "Reusken.",
      "year" : 2002
    }, {
      "title" : "Modeling by shortest data",
      "author" : [ "J.J. Rissanen" ],
      "venue" : "description. Automatica,",
      "citeRegEx" : "Rissanen.,? \\Q1978\\E",
      "shortCiteRegEx" : "Rissanen.",
      "year" : 1978
    }, {
      "title" : "Estimating the dimension of a model",
      "author" : [ "G. Schwarz" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Schwarz.,? \\Q1978\\E",
      "shortCiteRegEx" : "Schwarz.",
      "year" : 1978
    }, {
      "title" : "An asymptotic theory for linear model selection",
      "author" : [ "J. Shao" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Shao.,? \\Q1997\\E",
      "shortCiteRegEx" : "Shao.",
      "year" : 1997
    }, {
      "title" : "Asymptotic mean efficiency of a selection of regression variables",
      "author" : [ "R. Shibata" ],
      "venue" : "Annals of the Institute of Statistical Mathematics,",
      "citeRegEx" : "Shibata.,? \\Q1983\\E",
      "shortCiteRegEx" : "Shibata.",
      "year" : 1983
    }, {
      "title" : "Tuning parameter selectors for the smoothly clipped absolute deviation method",
      "author" : [ "H. Wang", "R. Li", "C.L. Tsai" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "Extended stochastic complexity and minimax relative loss analysis",
      "author" : [ "K. Yamanishi" ],
      "venue" : "Proc. 10th International Conference on Algorithmic Learning Theory - ALT’",
      "citeRegEx" : "Yamanishi.,? \\Q1999\\E",
      "shortCiteRegEx" : "Yamanishi.",
      "year" : 1999
    }, {
      "title" : "Can the strengths of aic and bic be shared? a conflict between model identification and regression estimation",
      "author" : [ "Y. Yang" ],
      "venue" : null,
      "citeRegEx" : "Yang.,? \\Q2005\\E",
      "shortCiteRegEx" : "Yang.",
      "year" : 2005
    } ],
    "referenceMentions" : [ ],
    "year" : 2010,
    "abstractText" : "A key issue in statistics and machine learning is to automatically select the “right” model complexity, e.g., the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle the Loss Rank Principle (LoRP) for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.",
    "creator" : "LaTeX with hyperref package"
  }
}