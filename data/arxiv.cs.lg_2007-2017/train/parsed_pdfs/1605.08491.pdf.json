{
  "name" : "1605.08491.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Provable Algorithms for Inference in Topic Models",
    "authors" : [ "Sanjeev Arora", "Rong Ge", "Frederic Koehler", "Tengyu Ma", "Ankur Moitra" ],
    "emails" : [ "ARORA@CS.PRINCETON.EDU", "RONGGE@CS.DUKE.EDU", "FKOEHLER@PRINCETON.EDU", "TENGYU@CS.PRINCETON.EDU", "MOITRA@MIT.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Generative models of data are ubiquitous in unsupervised learning, and lead to two types of computational problems: In parameter learning, the goal is find the parameters of\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nthe model that best fits a given collection of data. In inference, the goal is to learn the values of latent variables for a specific datapoint. A wide range of approaches are empirically effective for both tasks, including Gibbs sampling and variational inference. However, for the most part we lack strong provable guarantees — on running time, or quality of solution — for these approaches.\nRecently, there has been considerable progress on designing new algorithms for parameter learning with such provable guarantees. Since the usual maximum likelihood estimator is often NP-hard to compute even in simple models, these new algorithms use alternative estimators based on the method of moments and linear algebra. Their analysis usually involves making a structural assumption about the parameters of the problem, which can often be justified in applications. Some highlights include algorithms for topic modeling (Arora et al., 2013b; Anandkumar et al., 2012), learning mixture models (Moitra & Valiant, 2010; Hsu & Kakade, 2013; Ge et al., 2015), community detection (Anandkumar et al., 2014) and (special cases of) deep learning (Arora et al., 2014; Janzamin et al., 2015).\nBut there has been comparatively much less progress on designing algorithms with provable guarantees for inference. The current paper takes a first step in this direction, in context of topic models. Our algorithms leverage a property of topic models (Definition 3.1) that turns out to hold in many datasets — the existence of a good approximate inverse matrix. We also give empirical results that demonstrate that our algorithm works on realistic topic models. On synthetic data, its error is competitive with state-of-theart approaches (which have no such provable guarantees).\nar X\niv :1\n60 5.\n08 49\n1v 1\n[ cs\n.L G\nIt obtains somewhat weaker results on real data."
    }, {
      "heading" : "1.1. Setup and Overview",
      "text" : "Here we describe topic modeling, and why inference appears more difficult than parameter learning. In topic modeling, each document is represented as a bag of words where we ignore the order in which words occur. The model assumes there is a fixed set of k topics, each of which is a distribution on words. Thus the ith topic is a vector Ai ∈ RD (where D is the number of words in the language) whose coordinates are nonnegative and sum to 1. Each document is generated by first picking its topic proportions from some distribution; say xi is the proportion of topic i, so that ∑ i xi = 1. The model assumes a distribution on x that favors sparse or approximately sparse vectors; a popular choice is the Dirichlet distribution (Blei et al., 2003). Then the document {w1, w2, . . . , wn} is generated by drawing n words independently from the distribution A · x where A is the matrix whose columns are the topics. It is important to note that the document size n can be quite small (e.g., n may be 400, and D may be 50, 000) so the empirical distribution of words in a document is in general a very inaccurate approximation toAx. With some abuse of notation we also think of y as a vector in RD, whose jth coordinate is the number of occurences of word j in the document.\nParameter learning involves recovering the bestA for a corpus of documents; this can be seen as a the latent structure in the corpus. Recent (provable) algorithms for this problem (Anandkumar et al., 2012; Arora et al., 2013b) use the method of moments, leveraging the fact that some form of averaging over the corpus yields a linear algebraic problem for recovering A. For example the word-word cooccurence matrix (whose i, j entry is the probability that words i, j co-occur in a document) is given by\nEx[Axx TAT ] = AZAT\nwhere Z is the 2nd moment matrix of the prior distribution on x. It is possible to recoverA from this expression, under natural conditions like separability (Arora et al., 2013b). Alternatively, one can use a co-occurrence tensor and recover A under weaker assumptions (Anandkumar et al., 2012).\nIn the inference problem, we know the topic matrix A and are given a single document y generated using this matrix. The goal is to find the posterior distribution x|y. This can be seen as labeling or categorizing this document, which is important in applications. Inference is reminiscent of classical regression problems where the goal is to find x given y = Ax + noise vector. The key difference here is the nature of noise —for each word coordinate j is 1 with probability (Ax)j , and 0 otherwise— which means that\nthe noise on a coordinate-by-coordinate basis can be much larger than the signal. In particular the vector y ∈ RD is very sparse even though Ax is dense. This problem can be seen as an analog of sparse linear regression when the target (regression) vector x has nonnegative coordinate and∑ i xi = 1. (This is distinct from usual `1-regression where regression vector is in `2 even though the loss function is `1.) The difficulty here, in addition to the issue of high coordinate-wise error already mentioned, is that the usual sparsity-enforcing `1-regularization buys nothing since the solution needs to exactly satisfy ‖x‖1 = 1.\nInference seems more difficult than parameter learning because averaging over many documents is no longer an option. Furthermore, the solution x is not unique in general, and in some cases the posterior distribution on x is not well concentrated around any particular value. (In practice Gibbs Sampling can be used to sample from the posterior (Griffiths & Steyvers, 2004; Yao et al., 2009), but as mentioned, a rigorous analysis has proved difficult. The inference is actually NP-hard.) We will view inference as a problem of recovering some ground truth x∗ that was used to generate the document, and we show that with probability close to 1 our estimate x̂ is close to x∗ in `1 norm.\nBayesian vs Frequentist Views. So far we have not differentiated between Bayesian and frequentist approaches to frame the inference problem, and now we show that the two are closely related here. The above description is frequentist, assuming an unknown “ground truth”vector x∗ of topic proportions (which is r-sparse for some small r) was used to generate a document y, using a distribution y|x∗. Let Ex∗ be the event that our algorithm recovers a vector x̂ such that ‖x̂ − x∗‖1 ≤ . For our algorithm Pry|x∗ [Ex∗ ] ≥ 1 − δ2 for some δ > 0. By contrast, in the Bayesian view, one assumes a prior distribution on x∗ and seeks to output a sample from the conditional distribution x∗|y. Now we show that the success of our frequentist algorithm implies that the posterior x∗|y must also be concentrated, and place most probability mass on set of x such that ‖x − x̂‖1 ≤ . By law of total expectation, we have Prx∗,y [Ex∗ ] = Prx∗ [ Pry|x∗ [Ex∗ |x∗] ] ≥ 1−δ2. Switching the order of expectation, we obtain\nPry [ Prx∗|y [Ex∗ | y] ] ≥ 1− δ2 .\nThen it follows by Markov argument that Pry [ Prx∗|y [Ex∗ | y] ≥ 1− δ ] ≥ 1− δ .\nNote that the inner probability is over the posterior distribution px∗|y . But the event Ex∗ only depends on the output x̂ of the algorithm given y. Thus the probability is at least 1 − δ over choice of y, that 1 − δ of the probability mass of x∗|y is concentrated in the `1 ball of radius around the algorithm’s answer x̂.\nFrom now on the goal of our algorithm is to recover x∗ given y, and we identify conditions under which the event has probability close to 1.\nMinimum Variance Estimators (with Bias). Having set up the problem as above, next we consider how to recover an approximation to x∗ given a document y generated with topic proportions x∗.\nSince A has orders of magnitude more rows than columns, it has many left inverses to choose from. If we find any matrix B where BA is equal to the identity matrix, then By is an unbiased estimate for x∗. However this estimate has high variance if B has large entries, necessitating working with only very large documents. Motivated by applications to collaborative filtering, Kleinberg & Sandler (2008) introduce the notion of the `1 condition number (see Definition 2.1) of A, which allows them to construct a left inverse B with a much smaller maximum entry. We introduce a weaker notion of condition number called the `∞- to-`1 condition number, which leverages the observation that even if BA is close to the identity matrix it still yields a good linear estimator for x∗. We call B an approximate inverse of A. Moreover it has the benefit that the condition number as well as the approximate left inverseB with minimum variance can also be computed in polynomial time using a linear program (Proposition 3.2)!\nIn our experiments, we compute the exact condition number of word-topic matrices that were found using standard topic modeling algorithms on real-life corpora. (By contrast, we do not know the `1 condition number of these matrices.) In all of the examples, we found that the condition number is at most a small constant, which allows us to compute good approximate left inverses to the topic matrix A to enable us to estimate x∗ even with relatively short documents.\nMain results. Our main result (Theorem 4.1) shows that when the condition number is small, it is possible to estimate x∗ using a combination of thresholding and a left inverse B of minimum variance. Our overall algorithm runs efficiently and requires time O(nk) and Õ(r2) samples to achieve o(1) error in `1 norm and o(1/r) error in `∞ norm, where r is the number of topics represented in the document. Note that we do not need to assume a particular model (e.g. uniform random) for the r topics, the algorithm works even when the topics may be correlated with each other.\nAs an intermediate step, we are able to recover the support of x∗ when each of its non-zero coordinates is suitably bounded away from zero. We complement this result by showing that maximizing the log-likelihood function over the recovered support can further reduce the estimation er-\nror (measured in the `1-norm) to Õ( √ r/n) (see Section 5). The experiments show that it indeed yields estimates for x∗ with smaller error (see Section 7).\nFinally we show that in order to recover support of x∗, it is necessary to observe Ω(r2) words, even if A is perfectly conditioned and x∗ is promised to have all non-zero coordinates larger than Ω(1/r) (see Lemma 6.2 for a family of such perfectly conditioned but hard instances of A, and Lemma 6.3 for hard instance of x∗).\nThus to sum up, our overall approach involves simple linear algebraic primitives followed by convex programming. For a topic model with k topics, the sample complexity of our algorithms depend on log k instead of k. This is important in practice as k is often at least 100. The accuracy on synthetic data is good for sparse x, though not quite as good as Gibbs sampling. However, if we forgo the convex programming step we can compute a reasonable estimate for x from a single matrix vector multiplication plus thresholding, which is an order of magnitude faster than finding an estimate of the same quality via Gibbs sampling. And of course, our approach comes with a performance guarantee."
    }, {
      "heading" : "2. Notations and Setup",
      "text" : "In addition to the description of topic model in Section 1.1, we introduce the following notations. We use Sk = {z ∈ Rk≥0 : |z|1 = 1} to denote the k-dimensional probability simplex. We assume that the true topic proportion vector x∗ ∈ Sk is r-sparse throughout the paper. Sometimes we also abuse notations and use y as a D dimensional vector instead of a set, in this case yi is the number of times word i appears in the document. We will use a>i to denote the i-th row of A. We will use cat(p) to denoted the categorical distribution defined by probability vector p. Euclidean norm, `1, `∞ norm of a vector is denoted by ‖ · ‖, ‖ · ‖1 and ‖ · ‖∞ respectively.\nCondition Numbers of Matrices Condition number of a matrix usually represents the ratio of the largest and smallest singular values. However, this concept is tied to `2 norm, and for probability distributions the most natural norms are `1 and `∞.\nNext we define various matrix norms that we will utilize. Let |A|∞ = maxi,j |Aij | denotes the maximum absolute value of the entries of the matrix A, and |A|1 = ∑ i,j |Aij | denotes the sum of the absolute value of the entries of the matrix A. Let Idk denotes the identity matrix of dimension k. For a matrix, let ‖ · ‖ denote the spectral norm, and ‖ · ‖Q denote the norm defined by ‖x‖Q = √ x>Qx where Q is a positive semidefinite matrix. We will use this norm particularly with Q being fisher information matrix.\nWe will also work with various notions of condition num-\nber, that we will use in our guarantees.\nDefinition 2.1 (`1-condition number). For a nonnegative matrix A, define its `1-condition number κ(A) to be the minimum κ such that for any x ∈ Rk,\n‖Ax‖1 ≥ ‖x‖1/κ (1)\nThis condition number was introduced by Kleinberg & Sandler (2008) in analyzing various algorithms for collaborative filtering. We will use a weaker (i.e. smaller) notion of condition number. Empirically, it seems that most of the word-topic matrices that we have encountered have a reasonably small `1-condition number, and have an even smaller `∞ → `1-condition number. Definition 2.2 (`∞ → `1-condition number). Let λ(A) be the minimum number λ such that for any x ∈ Rk,\n‖Ax‖1 ≥ ‖x‖∞/λ (2)\nRemark 1. Based on the relationship between `1 and `∞ norm, we have that λ(A) ≤ κ(A) ≤ kλ(A). In Section 6 we give an example where the `1 → `1 condition number is significantly worse: κ(A) ≥ Ω( √ k)λ(A)."
    }, {
      "heading" : "3. δ-Biased Minimum Variance Estimators",
      "text" : "Let y ∈ RD be the document vector whose i-th entry yi is the number of times word i appears. We try to estimate the true topic vector x∗ by left multiplying y with some matrix B. Intuitively, E[By] = BAx∗, so we want BA to be close to the identity matrix. On the other hand, when we apply B to the document vector, each word will select a column of B, and its variance on any entry is bounded by the maximum entry in B. Therefore we would like to optimize over two things: first, we want BA to be close to identity; second, we want the matrixB to have small |B|∞. This inspires the following linear program:\nDefinition 3.1. For A ∈ RD×k and δ ≥ 0, define λδ(A) to be the solution of the following convex program:\nλδ(A) = min |B|∞ s.t. |BA− Idk|∞ ≤ δ\nB ∈ Rk×D (3)\nWe will refer to the minimizer B of the above convex program as the δ-biased minimum variance inverse for A. The solution to the above convex program will help minimize our sample complexity both theoretically and empirically.\nAllowing a nonzero δ can potentially reduce the variance of the estimator while introducing a small bias. Such biasvariance trade-off has been studied in other settings (Moitra & Saks, 2013; Javanmard & Montanari, 2014).\nWhat is the optimal |B|∞? To answer this question we get the dual of the LP 3 (with variable Q ∈ Rk×k),\nmaximize tr(Q)− δ|Q|1 s.t. |AQ|1 ≤ 1 (4)\nWe can further show that (4) is equivalent to the following (non-convex) program with vector variables x ∈ Rk (see Appendix B for the proof):\nmaximize ‖x‖∞ − δ‖x‖1 s.t. ‖Ax‖1 ≤ 1\nNote that this is very closely related to the condition number λ in Definition 2.2. In particular, the optimal value is exactly λ(A) when δ = 0! When δ > 0 this can be viewed as a relaxation of the `∞ → `1 condition number. This is summarized in the following Proposition whose proof is deferred to appendix.\nProposition 3.2. For any δ ≥ 0, we have that\nλδ(A) ≤ λ0(A) = λ(A) ≤ κ(A) .\n4. Recovery Guarantees in the `1-Norm In this section we show how to estimate the topic proportion vector using a δ-biased minimum variance inverse B of word-topic matrix A (Definition 3.1). For a small δ (that is 1/r), given a solution B of program (3) with entries of absolute value at most λδ(A), the following Thresholded Linear Inverse estimator (Algorithm 1) is guaranteed to be close to the true x∗ in both `1 and `∞ norm.\nAlgorithm 1 Thresholded Linear Inverse Algorithm (TLI) Input: Document y with n words, and δ-biased inverse matrix B of matrix A. Output: Topic vector estimator x.\n1. Compute x̂ = 1nBy. 2. Let τ = 2λδ(A) √\nlog k/n + δ. For all i ∈ [k], , if x̂i < τ , set xi = 0, otherwise set xi = x̂i.\nTheorem 4.1. Suppose document y is generated from rsparse topic vector x∗. For any > 4δr, given n = Ω(λδ(A)\n2r2 log k/ 2) samples, with high probability Algorithm 1 returns a vector that has `1-distance at most with x∗.\nOur first step is to bound the variance of the partial estimator x̂ before thresholding. Our bound will utilize the maximum entry in B, which is why we tried to find B that minimizes this quantity in the first place. In particular we can show:\nLemma 4.2. With probability at least 1 − 1/k2, for every i we have |x̂i − x∗i | ≤ δ + 2λδ(A) √ (log k)/n. Proof of Lemma 4.2. By definition, x̂i = 1n ∑n j=1(B1wj )i where 1wj is the indicator vector for the jth word in the document. By summing over words in the document as opposed to words in the vocabulary, we have written x̂j as a sum of independent random variables, and we will use Bernstein’s inequality to show that it is concentrated around its mean. This is straightforward, but the key is the way we have chosenB ensures that the estimator is at most δ-based. To elaborate, we can compute\nE[x̂i] = (BAx∗)i = k∑ j=1 (BA)i,jx ∗ j\n= x∗j + k∑ j=1 ((BA)i,j − 1i=j)x∗j ,\nwhere 1i=j = 1 if i = j and 1i=j = 0 otherwise. Now by construction we have that for all i and j, |(BA)i,j−1i=j | ≤ δ. Hence, | ∑k j=1((BA)i,j − 1i=j)x∗j | ≤ δ ∑k j=1 x ∗ j = δ . Therefore we conclude that |E[x̂i]− x∗i | ≤ δ which shows that our estimator has bias at most δ.\nNow we can appeal to standard concentration arguments. Recall that x̂i is a sum of independent random variables x̂i = 1n ∑n j=1(B1wj )i, and each summand here is bounded by max(B1wj )i ≤ λδ(A). We apply Hoeffding’s inequality and obtain that with probability at least 1−1/k2, |x̂i − E[x̂i]| ≤ 2λδ(A) √ (log k)/n and this completes the proof of the lemma.\nLemma above shows that the vector x̂ is close to the true x∗ in infinity norm. As a corollary, we know the algorithm finds the correct support if x∗ does not have very small entries\nCorollary 4.3. With high probability, x output by Algorithm 1 satisfies that for every i ∈ [k], if x∗i = 0 then xi = 0, and if x∗i ≥ 4λδ(A) √ (log k)/n+ 2δ then xi > 0. In particular, if all the nonzero entries of x∗ are at least /r for some > 4δr, the algorithm finds the correct support with O(λδ(A)2r2 log k/ 2) samples.\nUsing the corollary above we can then prove Theorem 4.1. The key intuition is x can only incur error on non-zero coordinates of x∗, and a fixed amount of error on non-zero coordinates of x∗.\nProof of Theorem 4.1. By Lemma 4.2 and union bound, we have that with probability at least 1 − 1/k, for every i ∈ [k] |x̂i − x∗i | ≤ δ + 2λδ(A) √ (log k)/n). Thus in Step 2 of the algorithm we are guaranteed that if x∗i = 0 then x̂i must be smaller than the threshold and therefore xi = 0.\nOn the other hand, if x∗i > 0, we know\n|x∗i−xi| ≤ |x∗i−x̂i|+|x̂i−xi| ≤ 2δ+4λδ(A) √ (log k)/n)\nAgain appealing to the fact that x̂ and x∗ are entry-wise close, there can be at most r entries where we set x∗i > 0 for δ ≤ /4r. When n = 64κ2r2 log k/ 2 we also have 4λδ(A) √ (log k)/n) ≤ /2r. Combining these two facts\nwe conclude |x∗−x|1 ≤ r(2δ+4λδ(A) √\n(log k)/n)) ≤ , which completes the proof."
    }, {
      "heading" : "5. Rate of MLE estimator",
      "text" : "In this section, we show that given the correct support R of x∗, we can optimize the log-likelihood function over the variables inR and obtain a finer solution with smaller `1 error. We make the following two assumptions: first, that the non-zero coordinates of x∗ are bounded away from zero; second, that the word-topic matrix has small restricted `1 → `1 condition number. Assumption 5.1. We assume that x∗ ∈ Sk satisfies that R = supp(x∗) is of size at most r and x∗i ≥ τ/r for any i ∈ R. Assumption 5.2 (restricted `1 → `1 condition number). We assume that word-topic matrix A satisfies that for any r-sparse vector v ∈ Rd,\n‖Av‖1 ≥ ‖v‖1/κ̄ .\nWe note that by definition κ̄ ≤ κ(A). Moreover, the restricted `1 → `1 condition number can be viewed as `1 analog of the restricted isometry property (Candes & Tao, 2005) or restricted eigenvalue conditions (Bickel et al., 2009; Meinshausen & Yu, 2009) associated to `2 norm. This type of assumption is particularly useful (and somewhat necessary) for the estimation problem.\nWe will restricted our attention to support R throughout this section. Let âw ∈ Rr be the restriction of aw to the support R, and Â be the word-topic matrix restricted to columns indexed by R. Let f(x) be the log-likelihood function restricted to the support R. That is, for x ∈ Rr,\nf(x) = log Pr[y | x] = ∑ w∈y log(〈âw, x〉), . (5)\nThe main theorem in this section below shows that when n = Ω(r2), the maximum likelihood estimator (MLE) restricted to the supportR has `1 rate Õ(κ̄ √ r/n). Moreover,\nthe error on predicting Ax∗ is Õ( √ r/n) which doesn’t depend on the condition number.\nTheorem 5.3. Under assumption 5.1 and 5.2, suppose n ≥ cκ̄2r2 log k/τ2 for a sufficiently large constant c. Let xMLE be the maximizer of the log-likelihood function f(x)\nrestricted to support R. Then with high probability xMLE satisfies that\n‖AxMLE −Ax∗‖1 ≤ Õ (√ r\nn\n) ,\nand,\n‖xMLE − x∗‖1 ≤ Õ ( κ̄ √ r\nn\n) .\nAsymptotically, we know that the error vector xMLE − x∗ converges to standard normal with covariance matrixQ being the Fisher information matrix (see Equation (7)). This means that Q−1/2(xMLE − x∗) is bounded in `2 norm. Therefore the keys towards proving Theorem 5.3 consists of a) converting the above to a non-asymptotic bound with careful concentration inequality b) understanding how Q−1/2 converts `1 space to `2 space so that an `1 norm of the error can be obtained.\nWe give intuitions for the proofs here, which mostly follows from the classical asymptotic normality of Maximum Likelihood Estimator, and our main contribution here is to give a finite sample bound using concentration inequalities.\nFirst we consider the gradients and Hessians of the likelihood function.\n∇f(x) = ∑ w∈y âw 〈âw, x〉 , ∇2f(x) = − ∑ w∈y âwâ > w 〈âw, x〉2 . (6)\nLet Q be the Fisher information matrix as defined below, Q = E [ âwâ > w\n〈âw, x〉2 ] = ∑ i∈[D] 〈âi, x∗〉 âiâ > i 〈âi, x∗〉2 . (7)\nNote that we have E[∇2f(x∗)] = −nQ. When n is sufficiently large and xMLE is sufficiently close to x∗, we have,\n−∇f(x∗) = ∇f(xMLE)−∇f(x∗) ≈ ∇2f(x∗)(xMLE−x∗) .\nTherefore, it follows that\nx∗ − xMLE ≈ ∇2f(x∗)−1∇f(x∗) .\nIt can be shown that the covariance of the gradient is E[∇f(x∗)∇f(x∗)>] = nQ. Therefore when ∇2f(x∗)−1 is sufficiently close to its expectation nQ, the covariance of the error x∗ − xMLE is approximately equal to 1nQ −1.\nTowards establishing a non-asymptotic result, we bound from above ∇f(x∗) and lower from below ∇2f(x) in a proper Euclidean norm – the norm defined by the Fisher information matrix Q in the following three lemmas. Lemma 5.4 below controls the `2 and `∞ norm of Q−1/2∇f(x∗) (which is supposed to be spherical Gaussian with covariance matrix √ nIdr asymptotically).\nLemma 5.4. Under assumption 5.1 and 5.2, suppose n ≥ cκ̄2r2/τ2 · log k for a sufficiently large constant c. Then, with high probability we have\n‖Q−1/2(∇f(x∗)− n1r)‖ ≤ Õ( √ nr) , (8)\nand,\n‖Q−1/2(∇f(x∗)− n1r)‖∞ ≤ Õ( √ n) . (9)\nLemma 5.5 relates −∇2f(x) with the Fisher information matrix Q spectrally around a neighborhood of x∗ which MLE will be proved to fall in. We essentially show that −∇f(x∗) concentrates around its expectation nQ, and moreover we can effectively approximate the Hessian −∇2f(x) around a neighborhood of x∗ by nQ as well. Lemma 5.5. Under assumption 5.1 and 5.2, suppose n = c0κ̄\n2r2 log k/τ2 for sufficiently large constant c0. Then, with high probability over the randomness of y, it holds that for all x such that x ≤ Cx∗,\n−∇2f(x) n 2C2 ·Q . (10)\nFinally, as alluded before, Lemma 5.6 characterizes the distortion caused by Q−1/2 transforming `2 to `1 space. We note that the square-root of Fisher information matrixQ1/2 converts naturally `1 to `2. Therefore we can get the desired `1 error bound in Theorem 5.3.\nLemma 5.6. Under assumption 5.1 and 5.2, Fisher information matrix Q satisfies that\n‖ÂQ−1/2‖2→1 ≤ 1, and ‖Q−1/2‖2→1 ≤ κ̄ . (11)"
    }, {
      "heading" : "As a corollary,",
      "text" : "Q 1 κ̄2 · Idr . (12)"
    }, {
      "heading" : "6. Sample Complexity Lower Bounds",
      "text" : "In this section we construct a natural (distribution of) wordtopic matrix A with low Λδ(A) value for very small δ for which given document with o(r2) words, it is impossible to determine the support x∗ even if all the nonzero coordinates of x∗ are roughly 1/r. This shows that for the task of support recovery, our algorithm in Section 4 achieves optimal sample complexity up to logarithmic factor.\nTheorem 6.1. There exists a (distribution of) word-topic matrix A with Λδ(A) = 1 for δ = O( √ log k/D) such that any algorithm A that takes document of o(r2) words as input cannot recover the support of the topic vector x∗ that is used to generate y with probability 3/4. This is still true when x∗ is promised to have only non-zero entries that are larger than 1/r.\nThe hard instance that we constructed is pretty simple: we consider a word-topic matrix where every topic contains roughly half of words in the vocabulary, and gives probability roughly 2/D to each of the words. The words in the topic are uniformly randomly selected. To make this more precise, let S1, S2, ..., Sk ⊂ [D] be k independent subsets that are uniformly chosen among all subsets of [D]. Let matrix Ai,j = 1/|Sj | if i ∈ Sj and Ai,j = 0 otherwise.\nWe first show that indeed Λδ(A) is very small, and therefore it has a good δ-biased minimum variance estimator and our algorithm works well on this matrix. Lemma 6.2. With high probability over the randomness of A, we have 1− δ ≤ Λδ(A) ≤ 1 for any δ ≥ c √ (log k)/D where c is a sufficiently large constant.\nProof of Lemma 6.2. For any matrix A with columns having `1 norm 1, we have that Λδ(A) ≥ 1 − δ. We show the upper bound by constructing the linear inverse matrix B with small |B|∞ ≤ 1 explicitly: Bj,i = 1 if Ai,j > 0, and Bj,i = −1 ifAi,j = 0. Let bi be the i-th row ofB and ai be the i-th column of A, to verify |BA − I|∞ ≤ δ, it suffices to show that 1) 〈bi, ai〉 = 1 for all i, and 2) |〈bi, aj〉| ≤ δ for all i 6= j.\nThe first equation is easy because bi is 1 on the support of ai, so 〈bi, ai〉 = |ai|1 = 1.\nFor the second equation, consider an arbitrary pair bi, aj (i 6= j). Consider Si, Sj , the supports of ai and aj respectively. By the construction of A,B we know that all entries in Si ∩ Sj contribute +1/|Sj | to the inner product 〈bi, aj〉, that all entries in Sj\\Si contribute −1/|Sj | to the inner product, and that all other entries contribute 0. Therefore, 〈bi, aj〉 = 1|Sj | (|Si ∩ Sj | − |Sj\\Si|) .\nStandard concentration bounds show that |Si∩Sj |, |Sj\\Si| are within D/4± O( √ D log k), and |Sj | is within D/2± O( √ D log k) with probability at least 1− 1/k4. Therefore by union bound with probability at least 1 − 1/k2 for all pairs i, j we have |〈bi, aj〉| ≤ O( √ (log k)/D).\nLemma 6.2 in particular implies that if nonzero entries in the true topic vector x∗ are at least 1/r, our algorithm can detect the support with O(r2 log k) samples. We show below that no algorithm can do much better by constructing the following hard instance: Lemma 6.3. With high probability over the choice of A, there exists vector r-sparse vectors x, x− with non-zero entries bounded below from 1/r, such that no algorithm that only takes document of o(r2) words can distinguish distributions cat(Ax) and cat(Ax−) with probability better than 1/2 + o(1).\nThe full proof is deferred to Section C. Here we sketch the main idea. We construct x, x− randomly as follows. Let\nR ⊂ [k] be a random subset of size r, and let R− ⊂ R be a random subset with one item removed from R (so |R−| = r − 1). Let xi = 1/r if i ∈ R, and xi = 0 otherwise. Let x−i = 1/(r − 1) if i ∈ R− and 0 otherwise. The key here is to show that with high probability over the choice of x, x− and the choice of A, the KL-divergence between the two distribution cat(Ax) and cat(Ax−) isO(1/r2), and therefore Ω(r2) is need for distinguishing them.\nLet p = Ax and q = Ax−. We will show that pj/qj is between 1±O(1/r) for most of the word j, and the rest of words are negligible. To see this, we observe that for most of the words, pj ≥ Ω(1/D) , and |pj − qj | = |(Ax)j − (Ax−)j | ≤ O(1/r) maxiAj,i = O(1/(rD)). Finally, we can bound KL-divergence of p, q by O(1/r2), KL(p‖q) ≤ χ2(p, q) = ∑ j(pj − qj)2/qj ≤ O(1/r2), as desired.\nIn fact, using the same matrix we can bound the difference between `1 → `1 condition number κ(A) and `1 → `∞ condition number λ(A).\nLemma 6.4. When D k log k, with high probability, the `1 → `1 condition number κ(A) ≥ Ω( √ k). When D k2 log k, with high probability λ(A) ≤ 2.\nWe give the proof in Appendix C. Intuitively, for κ(A) we show that the uniform mixture of first half of topics is very similar to the uniform mixture of the last half of topics. For λ(A), we use the construction for the δ-biased linear inverse, and show that “fixing” the bias does not change the condition number by too much."
    }, {
      "heading" : "7. Experiments",
      "text" : "The corpora we use consist of New York Times articles (295,000 documents, average document length 298), Enron emails (39,861 documents, average document length 136), and NIPS papers (1500 documents, average length 1042). We compute the word-topic matrices using the algorithm in (Arora et al., 2013a), using 100 topics for NIPS, 100 topics for Enron, and 400 topics for NYTimes.\nCondition Numbers of Matrices First we empirically verify the assumption that the word-topic matrix have small `∞ → `1 condition number (see Table 1). Solving LP (3) on 16 processors in parallel using the Mosek LP solver takes 1 minute and a half for the NIPS dataset, 4 minutes for the Enron dataset, and roughly 4 hours for the NYTimes dataset (this is partly the result of using more topics for this dataset).\nNote that we only need to compute the inverse matrix once and then it can be used to do inference on all the documents, so the running time for computing the inverse is not a major concern. The procedure can also be easily parallelized because the LP for different rows of B are independent.\nFor comparison, we also list lower bounds on the `1 → `1 condition number κ(·), the condition used by (Kleinberg & Sandler, 2008). We see that it’s at least 2 times larger than `∞ → `1 condition number. There is no efficient algorithm known for computing `1 → `1 condition number, and therefore we only compute provable lower bounds for various dataset (see Section D for the approach).\nSynthetic Experiments We first verify the recovery guarantee of our algorithm on synthetic documents. For each document, we sample r = 5 topics uniformly at random, and choose weights for these topics uniformly from the r-dimensional probability simplex. The results1 are listed in Figure 1.\nWe compare our TLI algorithm (Algorithm 1)2 with the collapsed Gibbs sampling algorithm implemented in MALLET (McCallum, 2002) and its anchor word compatible extension3; we use 200 iteration of burn-in and 1000 further iterations of sampling. Note that when applying Gibbs sampling for the topic vectors, we treat the word-topic matrix as a fixed constant. It is not easy to do Gibbs Sampling for the prior we specified, so we compare against a Dirichlet prior with α = rk1 which encourages r-sparse vectors. Note that TLI-Unnormalized is the output of the TLI algorithm as described, whereas TLI is the result after normalizing the entries to give a probability distribution. We can also improve the quality of the TLI estimate by using gradient ascent on the likelihood (see Section 5), restricted to the top r entries of our initial estimate; this is denoted TLI+MLE in the figure. Finally, we give the result of gradient ascent on the posterior (treating the prior as Dirichlet, similarly to Gibbs sampling) starting from the TLI estimate, and denote this TLI+MAP.\nWe can also replace the uniform sparse prior with a Dirichlet prior with α = rk1 and get similar results; see Figure 2.\nWhen using a uniform sparse prior, we can improve the recovery score of TLI by replacing the thresholding step in algorithm TLI with one that drops all but the top-r values; see Figure 3.\nThe performance of the TLI algorithm is 3 to 5 times worse than Gibbs sampling in terms of `1 or `∞ error with same number of words. This is mostly because a simple linear estimator cannot capture the correlations between weights of different topics. The post processing using maximum like-\n1Code to reproduce the results is available at: https:// github.com/frytvm/topic-inference\n2For documents of the length found in the corpuses, the thresholding value τ used in the theoretical section is too conservative (large). As a more practical alternative we replace τ as given in the theoretical section by τ/4.5. We use unbiased pseudoinverses, taking δ = 0.\n3https://github.com/mimno/anchor\nlihood improves the performance significantly. The performance of the algorithms seems to be related to the `∞ → `1 condition number (with NIPS being best and Enron being worst).\nA virtue of our algorithm is its speed. On the NYTimes dataset, computing the TLI estimate for a single document, which is just a matrix multiplication and a single thresholding step, takes approximately 0.8 milliseconds. In contrast, on a document of length 1600, a single iteration of Gibbs sampling takes approximately 1.0 ms (and to get the result in the plot we used 1000 iterations). On the Enron semisynthetic data with uniform sparse prior and documents of length 1600, we find it takes about 20 total iterations of Gibbs sampling (15 of them as burn in) to return a result of similar accuracy to TLI. The speed of these methods for different length documents is illustrated in Figure 4.\nInference on Real Documents We run both our algorithm TLI and Gibbs sampling on a subsample of real documents and test the similarity of results. See Table 2. Since we don’t have the ground truth in this setting, we focus on trying to recover a small number of high-weight topics: we take the top-3 scoring topics from each estimated topic vector, and then the overlap is the cardinality of the intersection divided by 3. If we treat the Gibbs sampling result as the “ground truth”, this gives the fraction of the highprobability topics our algorithm correctly finds. We also observe that by taking 5 instead of 3 topics from TLI, we can improve recall (fraction of Gibbs topics found by TLI) at the expense of precision (fraction of TLI topics that are also from Gibbs)4."
    }, {
      "heading" : "8. Conclusion",
      "text" : "This work takes a step towards designing algorithms with provable guarantees for inference in topic modeling, building upon earlier work of Kleinberg and Sandler (Kleinberg & Sandler, 2008) in collaborative filtering. We use a notion of the approximate inverse of a topic matrix (as opposed to its exact inverse) and characterize the mathematical con-\n4We only list recall values because in this setting precision = 3/5 · recall.\ndition — namely, the `∞ → `1 condition number — that determines how well it behaves as an estimator. Furthermore, we showed that this algorithm approximately solves inference in a document with as few as O(r2 log k) words where k is the number of topics and r is the sparsity of the topic vector generating the document. We have showed that such guarantees are optimal in the sense that there are word-topic matrices for which it is information theoretically impossible to make meaningful conclusions about the topic vector with fewer than r2 samples. We also show that our linear estimator identifies a reasonable set of topics, which allows us to solve (via convex programming) the maximum likelihood problem restricted to this set of topics and get better estimations in theory and practice. We also find that in practice, the standard pseudoinverse of the topic matrix is a good choice for B, though we do not have theory to support it.\nThe experiments show that topic model matrices associated with real-life corpora have good `∞ → `1 condition number, and that the above method works well with synthetic documents generated using these topic matrices. Moreover the running time is comparable to a single iteration of Gibbs sampling. Topic recovery on real-life documents seems to be slightly weaker, and seems to require further modifications to be more robust to modelmisspecification.\nAcknowledgments. Sanjeev Arora would like to thank the support in part by NSF grants CCF-1527371, DMS1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR- N00014-16-1-2329. Tengyu Ma would like to thank the support in part by Sanjeev Arora’s grants, Simons Award in Theoretical Computer Science and IBM PhD Fellowship. Ankur Moitra would like to thank the support in part by NSF CAREER Award CCF1453261, a grant from the MIT NEC Corporation and a Google Faculty Research Award."
    }, {
      "heading" : "A. Missing proofs in Section 5",
      "text" : "A.1. Proof of Main Theorem\nWe start by claiming that we only need to consider a reasonable neighborhood of x∗ which contains xMLE.\nClaim A.1. Under the setting of Theorem 5.3, we have xMLE ≤ 2x∗.\nProof of Claim A.1. For the sake of contradiction, assume this is not true. Then since x∗i ≥ τ/r, we have that xMLE 6∈ B where B = B(x∗, τ/r, ‖ · ‖∞) be the τ/rinfinity norm ball around x∗. For simplicity we do not write\nthe projection to the orthogonal subspace of 1, but all the gradients will be in that subspace while `1/`∞ norms are measured in original space.\nLet u = x∗ − xMLE. By simple integration along the line of u, we have\n∇f(x∗) = ∇f(x∗)−∇f(xMLE)\n= (∫ 1 0 ∇2f(tu+ xMLE)dt ) u = −Hu\nwhere H = − (∫ 1\n0 ∇2f(tu)dt ) . Let Ht = −∇2f(tu),\nand therefore H = ∫ 1\n0 Htdt.\nLet x = su + xMLE be the point in B with maximum s. Therefore we have that x is on the boundary of B and therefore ‖x− x∗‖∞ = τ/r. Then for any 1 ≥ t ≥ s, we claim that Hs satisfies that v>Htv ≥ Ω(n/κ̄2)‖v‖21 for any vector v. Indeed, let x = tu+ xMLE, we can verify that\nv>Htv ≥ 1\n4 · v>H1v ≥\nn 8 v>Qv ≥ Ω(n/κ̄2)‖v‖21 .\nTherefore we obtain that\nv>Hv = ∫ 1 0 v>Htvdt\n≥ ∫ 1 s Ω(n/κ̄2)‖v‖21dt = Ω((1− s)n/κ̄2)‖v‖21 .\nTherefore we obtain that ‖H−1/2‖2→1 ≤ O (√ κ̄2\n(1−s)n\n) .\nSimilarly we have that since Ht Ω(n) · Q for 1 ≥ t ≥ s, and Ht 0 for any t ∈ [0, 1] , we have that H =∫ t\n0 Htdt (1 − s)nQ and therefore ‖H−1/2Q1/2‖2→2 ≤\nO\n( 1√\n(1−s)n\n) . Then we have that\n‖H−1Q1/2‖2→1 ≤ ‖H−1/2‖2→1‖H−1/2Q1/2‖2→2 ≤ O ( κ̄\n(1− s)n\n) .\nwhich in turn implies (by Lemma A.4) that ‖H−1Q1/2‖∞→∞ ≤ O(\n√ κ̄2\n(1−s)n ). Therefore,\n‖x∗ − x‖∞ = ‖(s− 1)(x∗ − xMLE)‖∞ = (1− s)‖H−1Q1/2Q−1/2∇f(x∗)‖∞\n≤ O( κ̄ n ) · ‖Q−1/2∇f(x∗)‖∞\nThen by equation (9) of Lemma 5.4, we have that\n‖x∗ − x‖∞ ≤ O( κ̄\nn ) · ‖Q−1/2∇f(x∗)‖∞ ≤ O( κ̄√ n ) .\nThis contradicts with the fact that ‖x∗ − x‖∞ = τ/r for some n ≥ Ω(κ̄2r2/τ2). Hence we showed that xMLE indeed satisfies that xMLE ≤ 2x∗, which completes the proof of the claim.\nProof of Theorem 5.3. We restrict out attention to the convex region C = {x : x ≤ 2x∗} which contains both xMLE and x∗. We change the basis of the space according to the Fisher information matrix. Let g(z) = f(Q−1/2z), and let z∗ = Q1/2x∗, and zMLE = Q1/2xMLE. Then zMLE is also the maximizer of g(z) and therefore ∇g(zMLE) = 0. Moreover, we have that ∇g(z) = Q−1/2∇f(z) and ∇2g(z) = Q−1/2∇2f(z)Q−1/2. By Theorem 5.5, for any z ∈ Q1/2C, we have that ∇2g(z) −Q−1/2 · nQ/8 · Q−1/2 = −n/8 · Idr. Therefore g(z) is Ω(n)-strongly concave in Q1/2C. Then by strong concavity of g, we obtain that\n‖∇g(z∗)‖ = ‖∇g(z∗)−∇g(zMLE)‖ ≥ n/2 ·‖z∗−zMLE‖ .\nBy Theorem 5.4, we have ‖∇g(z∗) − nQ−1/21r‖ = ‖Q−1/2(∇f(x∗)−n1r)‖ ≤ Õ( √ nr). Here we are projecting out the all 1’s direction 1r in the original space because x is a probability simplex and 〈1r, x〉 is fixed by constraint (similarly the constraint will fix 〈Q−1/21r, z〉). Let Proj be the projection to the orthogonal subspace of Q−1/21r, we know\n‖z∗ − zMLE‖ ≤ O(1/n) · ‖Proj∇g(z∗)‖ ≤ Õ (√ r\nn\n) .\nConverting z to x, and observing that by Lemma 5.6 ‖AQ−1/2‖2→1 ≤ 1 and ‖Q−1/2‖2→1 ≤ κ̄, we obtain that\n‖Ax∗ −AxMLE‖1 = ‖AQ−1/2z∗ −AQ−1/2zMLE‖1 ≤ ‖z∗ − zMLE‖ ≤ Õ (√ r\nn\n) .\nSimilarly, we have ‖x∗ − xMLE‖1 ≤ ‖Q−1/2(z∗ − zMLE)‖1 ≤ κ̄‖z∗ − zMLE‖ ≤ Õ ( κ̄ √ r n ) .\nA.2. Proof of Lemma 5.5 and Lemma 5.6\nWe establish equation (10) by first showing that with high probability, −∇2f(x∗) ≥ nQ/2. Then using the fact that x ≤ Cx∗, we obtain that −∇2f(x) −∇2f(x∗)/C2 nQ/(2C2). Towards obtaining the lower bound for −∇2f(x∗), we first consider its expectation, which is lowerbounded in Lemma 5.6, whose proof is as follows.\nProof of Lemma 5.6. For any unit vector v ∈ Rr, we con-\nsider the quadratic form of v over Q,\nv>Qv = ∑ i∈[D] 〈âi, v〉2 〈âi, x∗〉\n= ∑ i∈[D] 〈âi, v〉2 〈âi, x∗〉 ∑ i∈[D] 〈âi, x∗〉  ≥\n∑ i∈[D] |〈âi, v〉| 2 = ‖Âv‖21 (13) ≥ ‖v‖21/κ̄2 (14)\nwhere the second line uses the fact that ∑ i〈âi, x∗〉 = 1, the third line uses Cauchy-Schwartz inequality, and the last line uses that Â has `1-condition number κ̄. (Note that v could take negative entries and therefore ‖Âv‖1 could be smaller than ‖v‖1. )\nReplacing v in equation (14) by Q−1/2z, we have ‖z‖ ≥ ‖Q−1/2z‖1/κ̄ for any z ∈ Rr. Therefore, ‖Q−1/2‖2→1 ≤ κ̄. It follows that ‖Q−1/2‖2→2 ≤ κ̄, which is equivalent to Q 1κ̄2 · Idr.\nFinally, using the intermediate result equation (13), plugging v = Q−1/2z, we obtain that\n‖z‖2 ≥ ‖ÂQ−1/2z‖21\nwhich implies that ‖ÂQ−1/2‖2→1 ≤ 1.\nThe following Lemma gives high probability (lower) bound for the spectrum of E[−∇2f(x∗)], from which Theorem 5.5 follows straightforwardly.\nLemma A.2. Let Q̂ = − 1n∇ 2f(x∗) be the empirical fisher information matrix. Suppose n = c0κ̄2r2/β2 · log k for sufficiently large constant c0. Then with probability at least 1− k−10, we have that\nQ̂ 1 2 ·Q .\nMoreover, it holds that\n‖Q̂−1/2‖2→1 ≤ √\n2κ̄ , and Q̂ 1 2κ̄2 · Idr\nSince we have shown that Q 1/κ̄2 · Idr, it would suffice to prove (by matrix concentration inequality) that with high probability, Q̂ concentrates around its expectation with variance significantly less than n2. However, it turns out that the variance Q̂ could have spectral norm as large as O(r4n). Then it will require n r4 to guarantee that the spectral norm of the variance can be smaller n2, which turns out to be a suboptimal bound.\nWe weaken the requirement to n r2 by observing the following inefficiency in the argument above: Though the\nvariance matrix E[Q̂2] could have a large eigenvalue with some eigenvector v, when this happens the expectation Q = E[Q̂] also have eigenvalue much larger than n/κ̄2 around some direction correlated with v (in the meantime it might also have a small eigenvalue n/κ̄2 in some other direction so that the lower bound Q n/κ̄2 · Idr cannot be improved). This suggests us to first whiten the expectation matrix Q to identity matrix, so that we can avoid the inefficiency caused by using spectral norm to measure the size of a very skew matrix.\nProof of Lemma A.2. For w ∈ y, let Zw = âwâ > w\n〈âw,x∗〉2 be the single term in the sum of∇2f(x∗) (see equation (6) for the representation of Hessian). That is, we have\nQ̂ = − 1 n · ∇2f(x∗) = 1 n ∑ w∈y Zw\nWe change basis usingQ−1/2. Let Z ′w = Q −1/2ZwQ −1/2. Consider the random variable Q̂′ = Q−1/2Q̂Q−1/2 ∈ Rr×r, which is a sum of independent random matrices,\nQ̂′ = 1\nn ∑ w∈y Z ′w .\nToward bounding the fluctuation of Q̂, we apply Bernstein inequality on Q̂′. By Lemma A.3, we know have that Zw is almost surely bounded by ‖Zw‖ ≤ r2/β2. Then using equation (12),\nZ ′w = Q −1/2ZwQ −1/2 Q−1/2·r2/β2Idr·Q−1/2 = r2/β2·Q−1\nTherefore it follows that Z ′w is almost surely bounded by ‖Z ′w‖ ≤ r2/β2‖Q−1‖ ≤ r2κ̄2/β2. Next we bound the variance of Q̂′:\nE[(Z ′w)2] = 1\nn ∑ i∈[D] 〈âi, x∗〉 · ( Q−1/2 âiâ > i 〈âi, x∗〉2 Q−1/2 )2\n= 1\nn ∑ i∈[D] â>i Q −1âi 〈âi, x∗〉2 ·Q−1/2 âiâ > i 〈âi, x∗〉 Q−1/2\n1 n ∑ i∈[D] ‖âi‖22‖Q−1‖ 〈âi, x∗〉2 ·Q−1/2 âiâ > i 〈âi, x∗〉 Q−1/2\n1 n · 1/κ̄2 · r2/β2 · ∑ i∈[D] Q−1/2 âiâ > i 〈âi, x∗〉 Q−1/2\n= κ̄2 · r2/β2 ·Q−1/2QQ−1/2 = κ̄2r2/β2 · Idr·\nwhere the second line is just a re-arrangement of the first line, and the third line uses the definition of spectral norm, and the fourth line uses that ‖âi‖ 2 2\n〈âi,x∗〉2 ≤ r 2/β2 (see\nLemma A.3) and Q 1/κ̄2 · Idr, and finally the fifth line uses the definition of Q.\nNow we are ready to apply Bernstein inequality on Q̂′. We conclude that with high probability,\n∥∥∥Q̂′ − E[Q̂′]∥∥∥ ≤ O(√κ̄2r2/β2 · n log k+r2κ̄2/β2 ·log k) Recall the definition of Q̂′ = Q−1/2Q̂Q−1/2 and that E[Q̂′] = Q−1/2 E[Q]Q−1/2 = Idr , we obtain that for n ≥ c0r2κ̄2/β2 · log k with sufficiently large constant c0,\n1 2 · Idr Q̂′ 3 2 · Idr (15)\nTherefore towards upperbounding the operator norm ‖Q̂−1/2‖2→1, we consider the quadratic form v>Q̂v and have that\nv>Q̂v = (Q1/2v)> · Q̂′ · (Q1/2v) ≥ 1 2 ‖Q1/2v‖2 ≥ 1 2κ̄2 ‖v‖21\nwhere the first inequality uses (15) and the second uses Lemma 5.6 (or equation (14)). It follows easily that ‖Q̂−1/2‖2→1 ≤ √ 2κ̄.\nLemma A.3. Suppose x ∈ Sr such that x ≥ β/r ·1r, then Then for any vector a ∈ Rr≥0, ‖a‖2 ≤ β/r · 〈a, x〉 and∥∥∥ aa>〈a,x〉2 ∥∥∥ ≤ r2/β2. Proof of Lemma A.3. Then we have that 〈a, x〉 ≥ β/r · ‖a‖1 ≥ β/r · ‖a‖2, and therefore\n∥∥∥ aa>〈a,x〉2 ∥∥∥ ≤ ‖a‖22〈a,x〉2 ≤ r2/β2.\nLemma A.4. For any symmetric positive semidefinite matrix G, it holds that ‖G‖∞→∞ ≤ ‖G‖2→1.\nProof of Lemma A.4. We have that since `1 norm is larger than `2, we have that ‖G‖1→1 ≤ ‖G‖2→2. For symmetric matrix, we have ‖G‖∞→∞ = ‖G‖1→1 which completes the proof.\nA.3. Proof of Lemma 5.4\nBefore proving the concentration of ∇f(x∗), we start by calculating its mean and variance.\nLemma A.5. The mean and variance of∇f(x∗) are equal to\nE [∇f(x∗)] = n · 1r ,\nand E [ ‖∇f(x∗)‖2Q−1 ] = nr .\nProof. Plugging x = x∗ into equation (6), we have that\n∇f(x∗) = ∑ w∈y âw 〈âw, x∗〉 .\nIt follows straightforwardly that\nE [∇f(x∗)] = ∑ w∈y E [ âw 〈âw, x∗〉 ] = n\n∑ i∈[D] 〈âi, x∗〉 âi 〈âi, x∗〉 = n ∑ i∈[D] âi = n1r .\nTowards computing the variance of ∇f(x∗) (under the Q−1 norm), we observe that\nE [ ∇f(x∗)∇f(x∗)> ] = ∑ i∈[D] 〈âi, x∗〉 âiâ > i 〈âi, x∗〉2\n= −E [ ∇2f(x∗) ] ] = Q .\nTherefore\nE[‖∇f(x∗)‖2Q−1 ] = E[tr(Q −1∇f(x∗)∇f(x∗)>)] = nr .\nNow we are ready to prove Lemma 5.4 by using Bernstein inequality on∇f(x∗).\nProof of Lemma 5.4. Note that ‖∇f(x∗)‖2Q−1 = ‖Q−1/2∇f(x∗)‖2. We apply concentration inequality on\nQ−1/2∇f(x∗) = ∑ w∈y Q−1/2âw 〈âw, x∗〉 ,\nwhich is a sum of independent random variables. Toward using Bernstein inequality, we first verify that∥∥∥∥Q−1/2âw〈âw, x∗〉 ∥∥∥∥ ≤ σ−1/2min (Q) · ∥∥∥∥ âw〈âw, x∗〉 ∥∥∥∥ ≤ κ̄ · r/τ\nwhere the last inequality uses the As bounded in Lemma A.5, the variance of Q−1/2∇f(x∗) is at most nr, and the mean of Q−1/2∇f(x∗) is nQ−1/21r. Therefore, by Bernstein inequality, we obtain that with high probability∥∥∥Q−1/2(∇f(x∗)− n1r)∥∥∥ ≤ Õ(κ̄r/τ+√nr) = Õ(√nr) . for n ≥ Ω(κ̄2r/τ2).\nFor the infinity norm bound, we fix a coordinate j and have that the j-th coordinate of Q\n−1/2âw 〈âw,x∗〉 is smaller than\nits `2 norm, which has been shown to be smaller than κ̄r/τ . Therefore applying Bernstein’s inequality, and taking union bound over all coordinates, we have that∥∥∥Q−1/2(∇f(x∗)− n1r)∥∥∥\n∞ ≤ O(κ̄r/τ · log2 k +\n√ n log3 k)\n= O( √ n log1.5 k) .\nfor some n ≥ Ω((κ̄2r2 log2 k)/τ2)"
    }, {
      "heading" : "B. Proof of Proposition 3.2",
      "text" : "Proof of Proposition 3.2. Let J be the all 1’s matrix. We rewrite the program (3) as an LP by introducing auxiliary variable t.\nλδ(A) = min t s.t. B ≤ tJ\n−B ≤ −tJ BA− Id ≤ δJ −BA+ Id ≤ −δJ\nLet P1, P2 ∈ Rk×D, Q1, Q2 ∈ Rk×k be the dual variables for the four (set of) constraints. Let 〈X,Y 〉 = tr(XTY ) denote the inner product of the two matrices. Then, the dual of the program above is\nmaximize 〈Q2 −Q1, Id〉 − δ〈Q2 +Q1, J〉 s.t. (P1 − P2) + (Q1 −Q2)A> = 0\n〈P1 + P2, J〉 = 1 P1, P2, Q1, Q2 ≥ 0 (16)\nLet Q = Q2 −Q1 and P = P1 − P2. Observe that 〈P1 + P2, J〉 ≥ |P |1 and 〈Q1 +Q2, J〉 ≥ |Q|1, it is easy to verify that program (16) is equivalent to the program below\nmaximize tr(Q)− δ|Q|1 s.t. P +QA> = 0\n|P |1 ≤ 1 (17)\nTowards further simplification, we claim that program (17) is equivalent to the following (non-convex) program with vector variables x ∈ Rk:\nmaximize ‖x‖∞ − δ‖x‖1 s.t. ‖Ax‖1 ≤ 1 (18)\nIndeed, suppose program (4) has optimal value λ and program (18) has optimal value λ′ with optimal solution xopt. We first show that for any x,\n‖x‖∞ − δ‖x‖1 ≤ λ′‖Ax‖1 , (19)\nwhich is due to the homogeneity of the equation. Then, consider any P,Q that satisfies the constraint of (4). Let Pj and Qj be the rows of P and Q. We have\ntr(Q)− δ|Q|1 ≤ ∑ j (‖Qj‖∞ − δ‖Qj‖1)\n≤ ∑ j (‖AQj‖1) = |P |1\nwhere the second inequality is by equation (19). Therefore λ ≤ λ′.\nOn the other hand, suppose the xopt has coordinate i with the largest absolute value. Then let Q be the matrix which has it’s j-th row as xopt and 0 elsewhere, and P = −QA>. Then it’s straightforward to check P,Q satisfy the constraint of (4) and have objective value λ′. Therefore λ′ ≤ λ. Hence we obtained that λ = λ′. Finally, from (18) it’s easy to see λ0(A) = λ(A), and λδ(A) ≤ λ0(A)."
    }, {
      "heading" : "C. Missing proofs in Section 6",
      "text" : "Proof of Lemma 6.3. We construct x, x− randomly as follows. Let R ⊂ [k] be a random subset of size r, and let R− ⊂ R be a random subset with one item removed from R (so |R−| = r − 1). Let xi = 1/r if i ∈ R, and xi = 0 otherwise. Let x−i = 1/(r − 1) if i ∈ R− and 0 otherwise.\nEach word of the document is from a multinomial distribution whose probabilities are specified by either Ax or Ax−. We shall show the two distributions have small KLdivergence so no algorithm can distinguish between the two.\nFirst we observe that most rows will have between r/4 and 3r/4 entries with value roughly 2/D in the subset R− of coordinates . We say a row is biased if it has less than r/4 or more than 3r/4 nonzero entries in R−, otherwise it’s balanced.\nClaim C.1. With high probability when r is larger than a fixed constant, and when D r2, there are at most D/r2 biased rows.\nProof. For every row, when we look at the entries in R−, these entries are independent and has probability 1/2 of being nonzero. Therefore the probability that the row is biased is e−Ω(r\n2) which is much smaller than 1/2r2. Different rows are also independent, so by Chernoff bound we know with high probability there are at most D/r2 biased rows.\nWe know the entries in i-th column of A are all equal to 1/|Si|, and with high probability 1/|Si| is within (2 ±\no(1))/D. Therefore the probability of every word in Ax or Ax− is at most (2 + o(1))/D, and the probability that we see a biased row is less than o(1) if we have o(r2) samples. Therefore we can condition on the event that the algorithm does not see any biased row.\nSuppose row i that is balanced, let {j} = R\\R−, we know |(Ax)i−(Ax−)i| = | 1rAi,j− 1 r(r−1) ∑ t∈R− Ai,t| ≤ maxAi,j · 2r < 5 rD . On the other hand, we know (Ax−)i ∈ [(1 − o(1))/2D, (3 + o(1))/2D] because row i is balanced. Let pi be the probability of seeing word i fromAx, conditioned on that we don’t see any biased row(word). Similarly let qi be the probability of seeing word i from Ax− with the same conditioning. The probabilities pi and qi are within 1 + 1/r2 multiplicative factor with (Ax)i and (Ax−)i by the rule of conditioning, and in particular we know (pi − qi) ≤ 10qi/r. Therefore the χ2-distance between p and q is χ2(p, q) = ∑ i (pi−qi)2 qi ≤ ∑ i 100qi/r\n2 = 100/r2 , where the sum is over all balanced row i. This implies that the KL-divergence between p and q is less than 100/r2 and therefore it is impossible to distinguish between p and q with more than 1/2 + o(1) accuracy with o(r2) samples.\nNext we prove Lemma 6.4.\nProof of Lemma 6.4. For the lowerbound of κ(A), we will construct a vector x = (1, 1, 1, ..., 1,−1, ...,−1). That is, xi = 1 for i ≤ k/2 and xi = −1 if i > k/2 (without loss of generality here we assume k is even).\nNow consider the `1 norm of Ax. To do that, consider a different matrix Â where Âi,j = 2/D if Ai,j > 0, and Âi,j = 0 if Ai,j = 0. Basically, Â has the same support as A, except its row may not normalized.\nUsing Chernoff bound and Union bound, we know with high probability, the size of the sets Si’s are bounded by D/2±O( √ D log k). Therefore the maximum entry in Â−\nA is at most 2D · √\n(log k)/D. Therefore |(Â − A)x|1 ≤ D · 2D · √ (log k)/D · |x|1 ≤ O( √ (log k)/D)|x|1.\nOn the other hand, we know for the x that we constructed, E[Âx] = 0, and E[|(Âx)i|] = O( √ k/D) because entries in Â are independent of each other. By Chernoff bounds we know with high probability |Âx|1 = O( √ k). However, |x|1 = k, so when D k log k we have\n|Ax|1 ≤ |Âx|1 + |(Â−A)x|1 ≤ O( √ k) = O(\n1√ k )|x|1.\nTherefor κ(A) ≥ Ω( √ k).\nFor λ(A), first notice that by Lemma 6.2, we know Λδ(A) ≤ 1 when D is larger than Ω((log k)/δ2. That means there is a matrix B with maximum entry at most 1 and BA = I −∆ where ∆ has maximum entry δ. Now let δ < 1/2k, then the rows of δ have `1 norm at most 1/2. By Gershgorin’s Disk Theorem we know ‖∆‖ < 1/2. Therefore we can write\n(I −∆)−1 = I + ∞∑ i=1 ∆i =: I + C.\nThe matrix C is defined to be ∑∞ i=1 ∆\ni. The maximum entry |∆i|∞ is bounded by (1/2)i−1δ. Therefore the maximum entry |C|∞ is bounded by 2δ. Now let B̂ = (I+C)B, we know B̂A = (I + C)BA = (I − ∆)−1(I − ∆) = I . On the other hand,\n|B̂|∞ ≤ |B|∞+|CB|∞ ≤ 1+|C|∞|B|∞·k ≤ 1+2δk ≤ 2.\nSo A has a pseudoinverse with maximum entry at most 2. By Proposition 3.2 the condition number λ(A) ≤ 2."
    }, {
      "heading" : "D. Lower bound on `1 → `1 condition number",
      "text" : "Here we describe how we bound from below the `1 → `1 condition number in Table 1. Fix δ > 0 and letBδ ∈ Rk×D be a minimizer of LP (3) with given δ, i.e. |Bδ|∞ = λδ and |BA−I|∞ ≤ δ. By compactness, there exists a v such that |v|∞/|Av|1 = λ(A). Then,\nλδ(A) ≥ |BδAv|∞ |Av|1\n≥ (|v|∞ − |(BA− I)v|∞)/|Av|1 ≥ (|v|∞ − |BA− I|∞|v|1)/|Av|1 ≥ λ(A)− δκ(A).\nHence, we can bound from below κ(A) by,\nκ(A) ≥ λδ(A)− λ(A) δ ."
    } ],
    "references" : [ {
      "title" : "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation",
      "author" : [ "A. Anandkumar", "D. Foster", "D. Hsu", "S. Kakade", "Y. Liu" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "A tensor approach to learning mixed membership community models",
      "author" : [ "Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2014
    }, {
      "title" : "A practical algorithm for topic modeling with provable guarantees",
      "author" : [ "Zhu", "Michael" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Zhu and Michael.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhu and Michael.",
      "year" : 2013
    }, {
      "title" : "New algorithms for learning incoherent and overcomplete dictionaries",
      "author" : [ "Arora", "Sanjeev", "Ge", "Rong", "Moitra", "Ankur" ],
      "venue" : null,
      "citeRegEx" : "Arora et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2013
    }, {
      "title" : "Provable bounds for learning some deep representations",
      "author" : [ "Arora", "Sanjeev", "Bhaskara", "Aditya", "Ge", "Rong", "Ma", "Tengyu" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,",
      "citeRegEx" : "Arora et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2014
    }, {
      "title" : "Simultaneous analysis of lasso and dantzig selector",
      "author" : [ "Bickel", "Peter J", "Ritov", "Yaacov", "Tsybakov", "Alexandre B" ],
      "venue" : "Ann. Statist., 37(4):1705–1732,",
      "citeRegEx" : "Bickel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel et al\\.",
      "year" : 2009
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research, pp. 993–1022,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Decoding by linear programming",
      "author" : [ "E.J. Candes", "T. Tao" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candes and Tao,? \\Q2005\\E",
      "shortCiteRegEx" : "Candes and Tao",
      "year" : 2005
    }, {
      "title" : "Learning mixtures of gaussians in high dimensions",
      "author" : [ "Ge", "Rong", "Huang", "Qingqing", "Kakade", "Sham M" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,",
      "citeRegEx" : "Ge et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2015
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "Griffiths", "Thomas L", "Steyvers", "Mark" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Griffiths et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Griffiths et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
      "author" : [ "Hsu", "Daniel", "Kakade", "Sham M" ],
      "venue" : "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2013
    }, {
      "title" : "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods",
      "author" : [ "M. Janzamin", "H. Sedghi", "A. Anandkumar" ],
      "venue" : null,
      "citeRegEx" : "Janzamin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Janzamin et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Some highlights include algorithms for topic modeling (Arora et al., 2013b; Anandkumar et al., 2012), learning mixture models (Moitra & Valiant, 2010; Hsu & Kakade, 2013; Ge et al.",
      "startOffset" : 54,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : ", 2012), learning mixture models (Moitra & Valiant, 2010; Hsu & Kakade, 2013; Ge et al., 2015), community detection (Anandkumar et al.",
      "startOffset" : 33,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : ", 2015), community detection (Anandkumar et al., 2014) and (special cases of) deep learning (Arora et al.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : ", 2014) and (special cases of) deep learning (Arora et al., 2014; Janzamin et al., 2015).",
      "startOffset" : 45,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : ", 2014) and (special cases of) deep learning (Arora et al., 2014; Janzamin et al., 2015).",
      "startOffset" : 45,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "The model assumes a distribution on x that favors sparse or approximately sparse vectors; a popular choice is the Dirichlet distribution (Blei et al., 2003).",
      "startOffset" : 137,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "Recent (provable) algorithms for this problem (Anandkumar et al., 2012; Arora et al., 2013b) use the method of moments, leveraging the fact that some form of averaging over the corpus yields a linear algebraic problem for recovering A.",
      "startOffset" : 46,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "Alternatively, one can use a co-occurrence tensor and recover A under weaker assumptions (Anandkumar et al., 2012).",
      "startOffset" : 89,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "Moreover, the restricted `1 → `1 condition number can be viewed as `1 analog of the restricted isometry property (Candes & Tao, 2005) or restricted eigenvalue conditions (Bickel et al., 2009; Meinshausen & Yu, 2009) associated to `2 norm.",
      "startOffset" : 170,
      "endOffset" : 215
    } ],
    "year" : 2016,
    "abstractText" : "Recently, there has been considerable progress on designing algorithms with provable guarantees — typically using linear algebraic methods — for parameter learning in latent variable models. But designing provable algorithms for inference has proven to be more challenging. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a single iteration of Gibbs sampling.",
    "creator" : "LaTeX with hyperref package"
  }
}