{
  "name" : "1511.08681.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Christos Dimitrakakis" ],
    "emails" : [ "chrdimi}@chalmers.se" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n08 68\n1v 1\n[ st\nat .M\nL ]\n2 7\nN ov\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "The well-known stochastic K-armed bandit problem (Thompson 1933; Robbins and others 1952) involves an agent sequentially choosing among a set of arms A = {1, . . . ,K}, and obtaining a sequence of scalar rewards {rt}, such that, if the agent’s action at time t is at = i, then it obtains reward rt drawn from some distribution Pi with expectation µi , E(rt | at = i). The goal of the decision maker is to draw arms so as to maximize the total reward\n∑T t=1 rt obtained.\nThis problem is a model for many applications where there is a need for trading-off exploration and exploitation. This occurs because we only see the reward of the arm we pull. An example is clinical trials, where arms correspond to different treatments or tests, and the goal can be to maximise the number of cured patients over time while being uncertain about the effects of treatments. Other problems, such as search engine advertisement and movie recommendations can be formalised similarly (Pandey and Olston 2006).\nIt has been previously noted (Jain, Kothari, and Thakurta 2012; Thakurta and Smith 2013; Mishra and Thakurta 2015; Zhao et al. 2014) that privacy is an important consideration for many multi-armed bandit applications. Indeed, privacy can be easily violated by observing changes in\nCopyright c© 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nthe prediction of the bandit algorithm. This has been demonstrated for recommender systems such as Amazon by (Calandrino et al. 2011) and for user-targeted advertising such as Facebook by (Korolova 2010). In both cases, with a moderate amount of side information and by tracking changes in the output of the system, it was possible to learn private information of any targeted user.\nDifferential privacy (DP) (Dwork 2006) provides an answer to this privacy issue by making the output of an algorithm almost insensitive to any single user information. That is, no matter what side information is available to an outside observer, he can not have more information about a user than he already had by observing the outputs released by the algorithm. This goal is achieved by formally bounding the loss in privacy through the use of two parameters (ǫ, δ) as shown in Definition 2.1.\nFor bandit problems, differential privacy implies that the actions taken by the bandit algorithm do not reveal information about the sequence of rewards obtained. In the context of clinical trials and diagnostic tests, it guarantees that even an adversary with arbitrary side information, such as the identity of each patient, cannot learn anything from the output of the learning algorithm about patient history, condition, or test results."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Differential privacy (DP) was introduced by (Dwork et al. 2006); a good overview is given in (Dwork and Roth 2013). While initially the focus in DP was static databases, interest in its relation to online learning problems has increased recently. In the full information setting, (Jain, Kothari, and Thakurta 2012) obtained differentially private algorithms with near-optimal bounds. In the bandit setting, (Thakurta and Smith 2013) were the first to present a differentially private algorithm, for the adversarial case, while (Zhao et al. 2014) present an application to smart grids in this setting. Then, (Mishra and Thakurta 2015) provided a differentially private algorithm for the stochastic bandit problem. Their algorithms are based on two non private stochastic bandit algorithms: Upper Confidence Bound (UCB, (Auer, Cesa-Bianchi, and Fischer 2002)) and Thompson sampling (Thompson 1933). Their results are sub-optimal: although sim-\nple index-based algorithms achieving O(log T ) regret exist (Burnetas and Katehakis 1996; Auer, Cesa-Bianchi, and Fischer 2002), these differentially private algorithms additional poly-log terms in time T , as well further linear terms in the number of arms compared to the non-private optimal regret O(log T ).\nWe provide a significantly different and improved UCBstyle algorithm whose regret only adds a constant, privacydependent term to the optimal. We also improve upon previous algorithms by relaxing the need to know the horizon T ahead of time, and as a result we obtain a uniform bound. Finally, we also obtain significantly improved bounds for a variant of the original algorithm of (Mishra and Thakurta 2015), by using a different proof technique and confidence intervals. Let’s note that similarly to their result, we only make distributional assumptions on the data for the regret analysis. To ensure privacy, our algorithms do not make any assumption on the data. We summarize our contributions in the next section."
    }, {
      "heading" : "1.2 Our Contributions",
      "text" : "• We present a novel differentially private algorithm (DP-\nUCB-INT) in the stochastic bandit setting that is almost optimal and only add an additive constant term (depending on the privacy parameter) to the optimal non private version. Previous algorithms had in large multiplicative factors to the optimal.\n• We also provide an incremental but important improvement to the regret of existing differentially private algorithm in the stochastic bandit using the same family of algorithms as previously presented in the literature. This is done by using a simpler confidence bound and a more sophisticated proof technique. These bounds are achieved by DP-UCB-BOUND and DP-UCB algorithms.\n• We present the first set of differentially private algorithm in the bandit setting which are unbounded and do not require the knowledge of the horizon T . Furthermore, all our regret analysis holds for any time step t."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Multi-Armed Bandit",
      "text" : "The well-known stochastic K-armed bandit problem (Thompson 1933; Lai and Robbins 1985; Auer, Cesa-Bianchi, and Fischer 2002) involves an agent sequentially choosing among a set of K arms A = {1, . . . ,K}. At each time step t, the player selects an action at = i ∈ A and obtains a reward rt ∈ [0, 1]. The reward rt is drawn from some fixed but unknown distribution Pi such that E(rt | at) = µi. The goal of the decision maker is to draw arms so as to maximize the total reward obtained after T interactions. An equivalent notion is to minimize the total regret against an agent who knew the arm with the maximum expectation before the game starts and always plays it. This is defined by:\nR , Tµ∗ − Eπ T ∑\nt=1\nrt. (2.1)\nwhere µ∗ , maxa∈A µa is the mean reward of the optimal arm and π(at | a1:t−1, r1:t−1) is the policy of the decision maker, defining a probability distribution on the next actions at given the history of previous actions a1:t−1 = a1, . . . , at−1 and rewards r1:t−1 = r1, . . . rt−1. Our goal is to bound the regret uniformly over T ."
    }, {
      "heading" : "2.2 Differential Privacy",
      "text" : "Differential privacy was originally proposed by (Dwork 2006), as a way to formalise the amount of information about the input of an algorithm, that is leaked to an adversary observing its output, no matter what the adversary’s side information is. In the context of our setup, the algorithm’s input is the sequence of rewards, and its output the actions. Consequently, we use the following definition of differentially private bandit algorithms.\nDefinition 2.1 ((ǫ, δ)-differentially private bandit algorithm). A bandit algorithm π is (ǫ, δ)-differentially private if for all sequences r1:t−1 and r′1:t−1 that differs in at most one time step, we have for all S ⊆ A: π(at ∈ S | a1:t−1, r1:t−1) ≤ π(at ∈ S | a1:t−1, r′1:t−1)eǫ+δ where A is the set of actions. When δ = 0, the algorithm is said to be ǫ-differential private.\nIntuitively, this means that changing any reward rt for a given arm, will not change too much the best arm released at time t or later on. If each rt is a private information or a point associated to a single individual, then the definition aboves means that the presence or absence of that individual will not affect too much the output of the algorithm. Hence, the algorithm will not reveal any extra information about this individual leading to a privacy protection. The privacy parameters (ǫ, δ) determines the extent to which an individual entry affects the output; lower values of (ǫ, δ) imply higher levels of privacy.\nA natural way to obtain privacy is to add a noise such as Laplace noise (Lap ) to the output of the algorithm. The main challenge is how to get the maximum privacy while adding a minimum amount of noise as possible. This leads to a trade off between privacy and utility. In our paper, we demonstrated how to optimally trade-off this two notions."
    }, {
      "heading" : "2.3 Hybrid Mechanism",
      "text" : "The hybrid mechanism is an online algorithm used to continually release the sum of some statistics while preserving differential privacy. More formally, there is a stream σt = r1, r2 . . . rt of statistics with ri in [0, 1]. At each time step t a new statistic rt is given. The goal is to output the partial sum (yt = ∑t i=1 ri) of the statistics from time step 1 to t without compromising privacy of the statistics. In other words, we wish to find a randomised mechanism M(yt | σt, y1:t−1) that is (ǫ, δ)-differential private.\nThe hybrid mechanism solves this problem by combining the Logarithm and Binary Noisy Sum mechanisms. Whenever t = 2k for some integer k, it uses the Logarithm mechanism to release a noisy sum by adding Laplace noise of scale ǫ−1. It then builds a binary tree B, which is used to release noisy sums until t = 2k+1 via the Binary mechanism.\nThis uses the leaf nodes of B to store the inputs ri, while all other nodes store partial sums, with the root containing the sum from 2k to 2k+1 − 1. Since the tree depth is logarithmic, there is only a logarithmic amount of noise added for any given sum, more specifically Laplace noise of scale log tǫ and mean 0 which is denoted by Lap( log tǫ ).\n(Chan, Shi, and Song 2010) proves that the hybrid mechanism is ǫ-differential private for any n where n is the number of statistics seen so far. They also show that with probability at least 1 − γ, the error in the released sum is upper bounded by 1ǫ log( 1 γ ) log\n1.5 n. In this paper, we derived and used a tighter bound for this same mechanism (see Appendix in Supplementary Material) which is:\n√ 8\nǫ log(\n4 γ ) log n+\n√ 8\nǫ log(\n4 γ ) (2.2)"
    }, {
      "heading" : "3 Private Stochastic Multi-Armed Bandits",
      "text" : "We describe here the general technique used by our algorithms to obtain differential privacy. Our algorithms are based on the non-private UCB algorithm by (Auer, Cesa-Bianchi, and Fischer 2002). At each time step, UCB based its action according to an optimistic estimate of the expected reward of each arm. This estimate is the sum of the empirical mean and an upper bound confidence equal to √\n2 log t na,t where t is the time step and na,t the number of\ntimes arm a has been played till time t. We can observe that the only quantity using the value of the reward is the empirical mean. To achieve differential privacy, it is enough to make the player based its action on differentially private empirical means for each arm. This is so, because, once the mean of each arm is computed, the action which will be played is a deterministic function of the means. In particular, we can see the differentially private mechanism as a black box, which keeps track of the vector of non-private empirical means Y for the player, and outputs a vector of private empirical means X . This is then used by the player to select an action, as shown in Figure 1.\nWe provide three different algorithms that use different techniques to privately compute the mean and calculate the index of each arm. The first, DP-UCB-BOUND, employs the Hybrid mechanism to compute a private mean and then adds a suitable term to the confidence bound to take into account the additional uncertainty due to privacy. The second, DP-UCB employs the same mechanism, but in such a way so as all arms have the same privacy-induced uncertainty; consequently the algorithm then uses the same index as standard UCB. The final one, employs a mechanism that only releases a new mean once at the beginning of each interval. This allows us to obtain the optimal regret rate."
    }, {
      "heading" : "3.1 The DP-UCB-BOUND Algorithm",
      "text" : "In Algorithm 1, we compute the sum of the rewards of each arm using the hybrid mechanism (Chan, Shi, and Song 2010). However, the number and the variance of Laplace noise added by the hybrid mechanism increases as we keep pulling an arm. This means that\nAlgorithm 1 DP-UCB-BOUND\nInput ǫ, the differential privacy parameter. Instantiate K Hybrid Mechanisms (Chan, Shi, and Song 2010); one for each arm a. for t ← 1 to T do\nif t ≤ K then play arm a = t and observe the reward rt Insert rt to the hybrid mechanism a else for all arm a do\nsa(t) ← total sum computed using the hybrid mechanism a if na,t is a power of 2 then νa ← √ 8 ǫ log(4t\n4) else\nνa ← √ 8 ǫ log(4t 4) logna,t + √ 8 ǫ log(4t\n4) end if\nend for Pull arm at = argmaxa sa(t) na,t + √ 2 log t na,t\n+ νana,t Observe the reward rt Insert rt to the hybrid mechanism for arm at\nend if end for\nthe sum of each arm get added different amount of noise bigger than the original confidence bound used by UCB. This makes it difficult to identify the best arms. To solve this issue, we add a tight upper bound defined in equation (2.2) on the noise added by the hybrid mechanism.\nTheorem 3.2 validates this choice by showing that only O(ǫ−1 log log t) factors are added to the optimal non private regret. In theorem 3.1, we demonstrate that Algorithm 1 is indeed ǫ-differential private.\nTheorem 3.1. Algorithm 1 is ǫ-differential private after any number of t of plays.\nProof. This follows directly from the fact that the hybrid mechanism is ǫ-DP after any number t of plays and a single flip of one reward in the sequence of rewards only affect one mechanism. Furthermore, the whole algorithm is a random mapping from the output of the hybrid mechanism to the action taken and using Proposition 2.1 of\n(Dwork and Roth 2013) completes the proof.\nTheorem 3.2 gives the regret for algorithm 1. While here we give only a sketch proof, the complete derivation can be found in the supplementary material (Tossou and Dimitrakakis 2016).\nTheorem 3.2. If Algorithm 1 is run with K arms having arbitrary reward distributions, then, its expected regret R after any number t of plays is bounded by:\nR ≤ ∑\na:µa<µ∗\nmax\n(\nB (lnB + 7) , 8\nλ20∆a log t\n)\n+ ∑\na:µa<µ∗\n(\n∆a + 2π2∆a\n3\n)\n(3.1)\nB =\n√ 8\nǫ(1− λ0) · ln(4t4)\nfor any λ0 such that 0 < λ0 < 1 where µ1, . . . , µK are the expected values of P1, . . . , PK and ∆a = µ∗ − µa. Proof Sketch. We used the bound on the hybrid mechanism defined in equation 2.2 together with the union and Chernoff-Hoeffding bounds. We then select the error probability γ at each step to be t−4. This leads to a transcendental inequality solved using the Lambert W function and approximated using section 3.1 of (Barry et al. 2000)."
    }, {
      "heading" : "3.2 The DP-UCB Algorithm",
      "text" : "The key observation used in Algorithm 2 is that if at each time step we insert a reward to all hybrid mechanisms, then the scale of the noise will be the same. This means that there is no need anymore to compensate an additional bound. More precisely, every time we play an arm at = a and receive the reward rt, we not only add it to the hybrid mechanism corresponding to arm a but we also add a reward of 0 to the hybrid mechanism of all other arms. As these calculate a sum, it doesn’t affect subsequent calculations.\nTheorem 3.3 shows the validity of this approach by demonstrating a regret bound with only an additional factor of O(ǫ−2 log2 log t) to the optimal non private regret.\nAlgorithm 2 DP-UCB\nRun Algorithm 1 and set νa to 0 When arm at = a is played, insert 0 to all hybrid mechanisms corresponding to arm a′ 6= a (Do not increase na′,t)\nTheorem 3.3. If Algorithm 2 is run with K arms having arbitrary reward distributions, then, its expected regret R after any number t of plays is bounded by:\nR ≤ ∑\na:µa<µ∗\n∆a\n[\nmax\n(\nC2 (lnC + 7)2 , 8\n∆2a log t\n)]\n+ ∑\na:µa<µ∗\n(∆a + 4∆aζ(1.5))\nC = 56(2 +\n√ 3.5) √ log t\nǫ\nwhere ζ denotes the Riemann zeta function.\nProof Sketch. The proof is similar to the one for Theorem 3.2, but we have to choose the error probability to be t−3.5."
    }, {
      "heading" : "3.3 The DP-UCB-INT Algorithm",
      "text" : "Both Algorithms 1 and 2 enjoy a logarithmic regret with only a small additional factor in the time step t to the optimal non-private regret. However, this includes a multiplicative factor of ǫ−1 and ǫ−2 respectively. Consequently, increasing privacy scales the total regret proportionally. A natural question is whether or not it is possible to get a differentially private algorithm with only an additive constant to the optimal regret. Algorithm 3 answers positively to this question by using novel tricks to achieve differential privacy. Looking at regret analysis of Algorithms 1 and 2, we observe that by adding noise proportional to ǫ, we will get a multiplicative factor to the optimal. In other words, to remove this factor, the noise should not depend on ǫ. But how can we get ǫ-DP in this case?\nNote that if we compute and use the mean at each time step with an ǫ′na,t-DP algorithm, then after time step t, our overall privacy is roughly the sum E ′ of all ǫ′na,t . We then change the algorithm so that it only uses a released mean once every 1ǫ times, making privacy ǫE ′. In any case, ǫ′na,t needs to decrease, at least as n−1a,t , for the sum to be bounded by logna,t. However, ǫ′na,t should also be big enough such that the noise added keeps the UCB confidence interval used at the same order, otherwise, the regret will be higher.\nA natural choice for ǫ′na,t is a p-series. Indeed, by mak-\ning ǫ′na,t to be of the form 1\nn v/2 a,t\n, where na,t is the num-\nber of times action a has been played until time t, its sum will converge to the Riemann zeta function when v is appropriately chosen. This choice of ǫ′na,t leads to the addition of a Laplace noise of scale 1\nn 1−v/2 a,t\nto the mean (See\nLemma 3.1). Now our trade-off issue between high privacy and low regret is just reduced into choosing a correct value for v. Indeed, we can pick v > 2, for the privacy to converge; but the noise added at each time step will be increasing and greater than the UCB bound; which is not desirable. To overcome this issue, we used the more sophisticated k-fold adaptive composition theorem (III-3 in (Dwork, Rothblum, and Vadhan 2010)). Roughly speaking, this theorem shows that our overall privacy after releasing the mean a number of times depends on the sum of the square of each individual privacy parameter ǫ′na,t . So, v > 1 is enough for convergence and with v ≤ 1.5, the noise added will be decreasing and will eventually become lower than the UCB bound.\nIn summary, we just need to lazily update the mean of each arm every 1ǫ times. However, we show that the interval of release is much better than 1ǫ and follows a series f as defined by Lemma (B.1) in the supplements\n(Tossou and Dimitrakakis 2016). Algorithm 3 summarizes the idea developed in this section.\nThe next lemma establishes the privacy ǫ′ each time a new mean is released for a given arm a.\nAlgorithm 3 DP-UCB-INT (ǫ, v, K , A) Input ǫ ∈ (0, 1] v ∈ (1, 1.5]; privacy rate. K is the number of arms and A the set of all arms. f ← ⌈ 1ǫ ⌉; x̂ ← 0 (For simplicity, we take the interval f to be ⌈ 1ǫ ⌉ here) for t ← 1 to T do\nif t ≤ Kf then play arm a = (t− 1) mod K +1 and observe rt else for all a ∈ A do\nif na,t mod f = 0 then\nx̂a ← sana,t + Lap(0, 1\nn 1−v/2 a,t\n) + √\n2 log t na,t\nend if end for Pull arm at = argmaxa x̂a and observe rt Update sum sa ← sa + rt.\nend if end for\nLemma 3.1. The mean x̂a computed by Algorithm 3 for a given arm a at each interval is n−v/2a,t -differential private with respect to the reward sequence observed by that arm.\nProof. Sketch This follows directly from the fact that we add Laplace noise of scale nv/2−1a,t .\nThe next theorem establishes the overall privacy after having played for t time steps.\nTheorem 3.4. After playing for any t time steps, Algorithm 3 is (ǫ′, δ′)-differential private with\nǫ′ ≤ min\n\n\nt ∑\nn=1\nǫ√ nv , ǫ\nt ∑\nn=1\ne 1√ nv − 1√ tv +\n√ √ √ √ǫ t ∑\nn=1\n2 ln 1δ′\nnv\n\n\nfor any δ′ ∈ (0, 1], ǫ ∈ (0, 1] Proof Sketch. We begin by using similar observations as in Theorem 3.1. Then, we compute the privacy of the mean of an arm using the k-fold adaptive composition theorem in (Dwork, Rothblum, and Vadhan 2010) (see the supplements (Tossou and Dimitrakakis 2016)).\nThe next corollary gives a nicer closed form for the privacy parameter which is needed in practice.\nCorollary 3.1. After playing for t time steps, Algorithm 3 is (ǫ′, δ′)-differential private with\nǫ′ ≤ min ( ǫ t1−v/2 − v/2\n1− v/2 , 2ǫζ(v) + √ 2ǫζ(v) ln(1/δ′)\n)\nwith ζ the Riemann Zeta Function for any δ′ ∈ (0, 1], ǫ ∈ (0, 1], v ∈ (1, 1.5].\nProof Sketch. We upper bounded the first term in theorem 3.4 by the integral test, then for the second term we used ex ≤ 1 + 2x for all x ∈ [0, 1] to conclude the proof.\nThe following corollary gives the parameter ǫ with which one should run Algorithm 3 to achieve a given ǫ′ privacy.\nCorollary 3.2. If you run Algorithm 3 with parameter ǫ = (\n√\nlog 1 δ′ +4ǫ′ 8ζ(v) − √ log 1 δ′ 8ζ(v)\n)2\nfor any δ′ ∈ (0, 1], ǫ′ ∈ (0, 1],\nv ∈ (1, 1.5], you will be at least (ǫ′, δ′)-differential private.\nProof. The proof is obtained by inverting the term using the Riemann zeta function in corollary 3.1.\nFinally, we present the regret of Algorithm 3 in theorem 3.5 . A simple observation shows us that it has the same regret as the non private UCB with just an additive constant.\nTheorem 3.5. If Algorithm 3 is run with K arms having arbitrary reward distributions, then, its expected regret R after any number t of plays is bounded by:\nR ≤ ∑\na:µa<µ∗\n∆a\n[\nf0 + 8\n∆2a log t+ 1 + 4ζ(1.5)\n]\nwhere f0 ≤ 1ǫ . More precisely, f0 is the first value of the series f defined in Lemma B.1 in the supplements (Tossou and Dimitrakakis 2016).\nSketch. This is proven using a Laplace concentration inequality to bound the estimate of the mean then we selected the error probability to be t−3.5."
    }, {
      "heading" : "4 Experiments",
      "text" : "We perform experiments using arms with rewards drawn from independent Bernoulli distribution. The plot, in logarithmic scale, shows the regret of the algorithms over 100, 000 time steps averaged over 100 runs. We targeted 2 different ǫ privacy levels : 0.1 and 1. For DP-UCB-INT, we pick ǫ such that the overall privacy is (ǫ′, δ′)-DP with ǫ as defined in corollary 3.2 and δ′ = e−10, ǫ′ ∈ {0.1, 1}, v = 1.1. We put in parenthesis the input privacy of each algorithm.\nWe compared against the non private UCB algorithm and the algorithm presented in (Mishra and Thakurta 2015) (Private-UCB ) with a failure probability chosen to be t−4.\nWe perform two scenarios. Firstly we used two arms: one with expectation 0.9 and the other 0.6. The second scenario is a more challenging one with 10 arms having all an expectation of 0.1 except two with 0.55 and 0.2.\nAs expected, the performance of DP-UCB-INT is significantly better than all other private algorithms. More importantly, the gap between the regret of DP-UCB-INT and the non private UCB does not increase with time confirming the theoretical regret. We can notice that DP-UCB is better than DP-UCB-BOUND for small time steps. However, as the time step increases DP-UCB-BOUND outperforms DPUCB and eventually catches its regret. The reason for that is: DP-UCB spends less time to distinguish between arms with close rewards due to the fact that the additional factor\nin its regret depends on ∆a = µ∗ −µa which is not the case for DP-UCB. Private-UCB performs worse than all other algorithms which is not surprising.\nMoreover, we noticed that the difference between the best regret (after 100 runs) and worst regret is very consistent for all ours algorithms and the non private UCB (it is under 664.5 for the 2 arms scenario). However, this gap reaches 30, 000 for Private-UCB. This means that our algorithms are able to correctly trade-off between exploration and exploitation which is not the case for Private-UCB."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we have proposed and analysed differentially private algorithms for the stochastic multi-armed bandit problem, significantly improving upon the state of the art. The first two, (DP-UCB and DP-UCBBOUND) are variants of an existing private UCB algorithm (Mishra and Thakurta 2015), while the third one uses an interval-based mechanism.\nThose first two algorithms are only within a factor of O(ǫ−1 log log t) and O(ǫ−2 log2 log t) to the non-private algorithm. The last algorithm, DP-UCB-INT, efficiently trades off the privacy level and the regret and is able to achieve the same regret as the non-private algorithm up to an additional additive constant. This has been achieved by using two key tricks: updating the mean of each arm lazily with a frequency proportional to the privacy ǫ−1 and adding a noise independent of ǫ. Intuitively, the algorithm achieves better privacy without increasing regret, because its output is less dependent on individual reward.\nPerhaps it is possible to improve our bounds further if we are willing to settle for asymptotically low regret (Cowan and Katehakis 2015). A natural future work is to study if we can use similar methods for other mechanisms such as Thompson sampling (known to be differentially private (Dimitrakakis et al. 2014)) instead of UCB. Another question is whether a similar analysis can be performed for adversarial bandits.\nWe would also like to connect more to applications by two extensions of our algorithms. The first natural extension is to consider some side-information. In the drug testing example, this could include some information about the drug, the test performed and the user examined or treated. The second extension would relate to generalising the notion of neighbouring databases to take into account the fact that multiple observations in the sequence (say m) can be associated with a single individual. Our algorithms can be easily extended to deal with this setting (by re-scaling the privacy parameter to ǫ m ). However, in practice, m could be quite large and it will be an interesting future work to check if we could get sub linearity in the parameter m under certain conditions."
    }, {
      "heading" : "A Collected proofs",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Theorem 3.2",
      "text" : "a will be used to indicate the index of an arm. ǫ is the differential privacy parameter. t is used to denote the time step.\nFrom Lemma (B.2), we know that the error between the empirical and private mean is bounded as |Y −X | ≤ hn with probability at least 1 − γ, X is the empirical mean returned by the private mechanism, Y the true empirical mean, hn the error due to the differentially private mechanism. It is\ndefined as: hn = 1ǫ · √ 8(logn) · ln 4γ · 1n + 1ǫ · √ 8 · ln 4γ · 1n . We can rewrite this bound into equations A.1 and A.2.\nPr(X ≥ Y + hn) ≤ γ (A.1) Pr(X ≤ Y − hn) ≤ γ. (A.2)\nLet Ta(s) be the number of times arm a is played in the first s time steps. Let’s ct,n , √\n(2 ln t)/n denote the original UCB confidence index.\nBy following similar steps as in the demonstration of UCB in (Auer, Cesa-Bianchi, and Fischer 2002), we have\nTa(s) = 1 +\ns ∑\nt=K+1\n{at = a}\n≤ ℓ+ ∞ ∑\nt=1\nt−1 ∑\nn=1\nt−1 ∑\nna=ℓ\n{X∗n + ct,n + hn ≤ Xa,na + ct,na + hna} (A.3) In equation A.3, X∗n is the mean returned by the private mechanism for the best arm when it has been played n times. Now we can observe that X∗n+ct,n+hn ≤ Xa,na +ct,na + hna implies that at least one of the following must hold\nX∗n ≤ µ∗ − ct,n − hn (A.4) Xa,na ≥ µa + ct,na + hna (A.5)\nµ∗ < µa + 2ct,n + 2hn (A.6)\nWe can bound the probability of events (A.4) using equation (A.2), the union bound and the Chernoff-Hoeffding bound.\nPr(A.4) = Pr(X∗n ≤ µ∗ − ct,n − hn) = Pr(X∗n ≤ Y ∗n − hn ∨ Y ∗n ≤ µ∗ − ct,n) ≤ Pr(X∗n ≤ Y ∗n − hn) + Pr(Y ∗n ≤ µ∗ − ct,n) ≤ γ + exp(−4 log t) = γ + t−4. (A.7)\nSimilarly, to prove a bound on the probability (A.5) we use (A.1) , the union bound and the Chernoff-Hoeffding bound.\nPr(A.5) = Pr(Xa,na ≥ µa + ct,na + hna) = Pr(Xa,na ≥ Ya,na + hna ∨ Ya,na ≥ µa + ct,na) ≤ Pr(Xa,na ≥ Ya,na + hna) + Pr(Ya,na ≥ µa + ct,na) ≤ γ + exp(−4 log t) = γ + t−4. (A.8)\nLet’s choose γ = t−4; this leads respectively to\nPr(A.4) ≤ 2t−4 (A.9) Pr(A.5) ≤ 2t−4 (A.10)\nNow consider the last condition (A.6). For this, we want to find the minimum number n for which event (A.6) is always false. Event (A.6) is false, implies that ∆a > 2ct,n + 2hn where ∆a = µ∗−µa. We observe that for ∆a > 2ct,n+2hn to hold, it is enough that the following two conditions hold for any λ0 such that 0 < λ0 < 1.\nλ0∆a > 2ct,n (A.11)\n(1 − λ0)∆a > 2hn (A.12)\nFrom inequality (A.11), we have\nn ≥ 8 λ20∆ 2 a log(t). (A.13)\nEquation (A.12) leads to\nn ≥ B · log(n) +B with\nB =\n√ 8\nǫ(1− λ0)∆a · ln 4 γ\n=\n√ 8\nǫ(1− λ0)∆a · ln(4t4)\nWe can rewrite the inequality in a more familiar form:\ne−(− 1 B )n ≥ e · n This is a standard transcendental algebraic inequality\nwhose solution is given by the Lambert W function. So,\nn ≥ −B ·W ( −1 e ·B ,−1 )\nwhere W (x, k) is the Lambert function of x on branch k. Note here that the branch is -1 and because −1e < −1 e·B < 0 ∀t > 1, we are always guaranteed to find a real number. By using the approximation of the Lambert function provided in section 3.1 of (Barry et al. 2000), we can conclude that\nn ≥ −B ·W ( −1 e · B ,−1 )\n≈ −B ( ln( 1 e ·B )− 2 0.3361 )\n≥ B (lnB + 7)\n=\n√ 8 · ln(4t4)\nǫ(1− λ0)∆a\n(\nln\n( √ 8 · ln(4t4)\nǫ(1− λ0)∆a\n)\n+ 7\n)\n(A.14)\nCombining inequalities (A.13) and (A.14) yields,\nn ≥ max [ B (ln (B) + 7) , 8\nλ20∆ 2 a\nlog(t)\n]\nIn summary,\nTa(s) ≤ ⌈ max [ B (lnB + 7) , 8\nλ20∆ 2 a\nlog(t)\n]⌉\n+\n∞ ∑\nt=1\nt−1 ∑\nn=1\nt−1 ∑\nna=ℓ\n4t−4\n≤ 1 + max ( B (lnB + 7) , 8\nλ20∆ 2 a\nlog t\n) + ∞ ∑\nt=1\n4t−2\n≤ 1 + max ( B (lnB + 7) , 8\nλ20∆ 2 a\nlog t\n)\n+ 2π2\n3 (A.15)\nwhich concludes the proof."
    }, {
      "heading" : "A.2 Proof for Theorem 3.3",
      "text" : "The proof of this theorem is similar to the one for theorem 3.5 with hn = 1ǫ · √ 8(log n) · ln 4γ · 1n + 1ǫ · √ 8 · ln 4γ · 1n . We make the same choice of λ and γ. However, the minimum number n compatible with these choices lead a transcendental equations in the same form as the one in the proof of theorem 3.2.\nB Proofs for UCB-Interval Algorithm\nFact B.1. Differential privacy of the Laplace mechanism (See Theorem 4 in (Dwork and Roth 2013)) For any real function g of the data, a mechanism adding Laplace noise with scale parameter β is ∆g/β-differentially private, where ∆g is the L1 sensitivity of g."
    }, {
      "heading" : "B.1 Proof of Lemma 3.1",
      "text" : "Proof. Indeed, for each arm, we are adding a Laplace noise of mean 0 and scale nv/2−1a,t where na,t is the number of times this arm has been played. As the sensitivity of the mean is 1na,t , we use the differential privacy of the Laplace Mechanism (Fact B.1) to conclude the proof."
    }, {
      "heading" : "B.2 Proof of Theorem 3.4",
      "text" : "Similarly to the proof of Theorem 3.1, the overall privacy of Algorithm 3 will be the same as the overall privacy of the Mechanism computing the mean of the rewards received from a single arm.\nA new Laplace Mechanism is used to compute the mean of each arm. However a new mean is only released t/f times (every f time steps) after t times steps where f is the interval used.\nAccording to the k-fold adaptive composition theorem (III-3 in (Dwork, Rothblum, and Vadhan 2010)), Algorithm 3 will be (ǫ′, δ′) differential private for any δ′ ∈ (0, 1] with\nǫ′ ≤ min {C,D}\nC = ∑\nn∈Ina 1\nǫ\n1√ nv\nD = ∑\nn∈Ina 1\nǫ\n1√ nv (e 1√ nv − 1) +\n√ √ √ √ ∑\nn∈Ina 1\nǫ\n2\nnv ln(1/δ′)\nwhere Ina1 ǫ = { 1ǫ , 2ǫ , 3ǫ , · · ·na} We have\nD =\n\n\n\n∑\nn∈Ina 1\nǫ\n1√ nv (e 1√ nv − 1) +\n√ √ √ √ ∑\nn∈Ina 1\nǫ\n2\nnv ln(1/δ′)\n\n\n\n(B.1)\n≤\n\n\n\n∑\nn∈It 1\nǫ\n1√ nv (e 1√ nv − 1) +\n√ √ √ √ ∑\nn∈It 1\nǫ\n2\nnv ln(1/δ′)\n\n\n\n(B.2)\n≤\n\nǫ\nt ∑\nn=1\n1√ nv (e 1√ nv − 1) +\n√ √ √ √ǫ t ∑\nn=1\n2\nnv ln(1/δ′)\n\n\n(B.3)\nC = ∑\nn∈Ina 1\nǫ\n1√ nv\n(B.4)\n≤ ∑\nn∈It 1\nǫ\n1√ nv\n(B.5)\n≤ ǫ t ∑\nn=1\n1√ nv\n(B.6)\nwhich concludes the proof.\nLemma B.1 (Values of the interval of release of the means). The interval f by which we should update the mean follows a series such that fn = Wn+1 −Wn with: \n         \n        \nW0 = 0 Wn+1 is either x or y with\nx = inf x′∈N\n\n\n\nx′ ≥ Wn + 1 : x′ ∑\ni=Wn+1\n1√ iv ≥ 1 ǫ √ x′v\n\n\n\ny = inf y′∈N\n\n\n\ny′ ≥ Wn + 1 : y′ ∑\ni=Wn+1\n1 iv ≥ 1 ǫy′v\n\n\n\nFurthermore fn is always such that fn ≤ ⌈ 1 ǫ ⌉\nNote that only one of the expression for Wn+1 should be used up to the horizon T.\nProof. From the proof of theorem 3.4, we can easily see that we can pass from lines (B.2) to (B.3) if the conditions on y in Lemma B.1 is verified. Similarly, we can pass from lines (B.5) to (B.6) if the conditions on y in Lemma B.1 is verified. In both cases, 1ǫ is an upper bound on the number x − Wn and y − Wn as the original interval used in both lines (B.2) and (B.5) is 1ǫ ."
    }, {
      "heading" : "B.3 Proof of Corollary 3.1",
      "text" : "From theorem 3.4, we know that Algorithm 3 will be (ǫ′, δ′) differential private for any δ′ ∈ (0, 1], 1 < v ≤ 1.5 with\nǫ′ ≤ min {C,D}\nC =\nt ∑\nn=1\nǫ√ nv\nD = ǫ\nt ∑\nn=1\ne 1√ nv − 1√ tv +\n√ √ √ √ǫ t ∑\nn=1\n2 ln 1δ′\nnv\nIt is easy to get an upper bound for C using the integral test inequality giving:\nC ≤ ǫ t 1−v/2−v/2 1−v/2\nWe will now simplify D using a standard approximation for exponential function.\nD = ǫ\nt ∑\nn=1\ne 1√ nv − 1√ tv +\n√ √ √ √ǫ t ∑\nn=1\n2 ln 1δ′\nnv (B.7)\n≤\n\nǫ\n∞ ∑\nt=1\n1√ tv (e 1√ tv − 1) +\n√ √ √ √ǫ ∞ ∑\nt=1\n2 tv ln(1/δ′)\n\n\n(B.8)\n≤\n\nǫ\n∞ ∑\nt=1\n2( 1√ tv )2 +\n√ √ √ √ǫ ∞ ∑\nt=1\n2 tv ln(1/δ′)\n\n (B.9)\n= ( 2ǫζ(v) + √ 2ǫζ(v) ln(1/δ′) )\n(B.10)\nwhere ζ is the Riemann zeta function."
    }, {
      "heading" : "B.4 Proof of Corollary 3.2",
      "text" : "The proof is immediate from corollary 3.1. It is obtained by inverting the term using the Riemann zeta function in corollary 3.1."
    }, {
      "heading" : "B.5 Proof of Theorem 3.5",
      "text" : "Fact B.2 (Fact 3.7 in (Dwork and Roth 2013)). If Y ∼ Lap(b) then\nPr(|Y |≥ b ln 1 γ ) = γ. (B.11)\nFirst, let’s ignore the effect of computing the means per interval and assume that we compute it at each time step after playing each arm f0 times where f0 ≤ 1ǫ is the first value of the series f defined in lemma (B.1). As much of the proof is quite similar to that of Theorem 3.2, we shall omit some steps.\nSimilarly to the proof of Theorem 3.2, we will take a bad arm when one of the following 3 events happens:\nX∗n ≤ µ∗ − ct,n (B.12) Xa,na ≥ µa + ct,na (B.13)\nµ∗ < µa + 2ct,n (B.14)\nSince we are adding Laplace noise, we can use Fact B.2 to show that:\nPr{|Y −X | ≥ log( 1 γ )nv/2−1} ≤ γ (B.15)\nLet hn = log( 1γ )n v/2−1. Now we can bound the probability of events (B.13) using equation (B.15), the union bound and the Chernoff-Hoeffding bound. In order to be more precise, we use Xa,na for x̂a and Y to denote the empirical mean x̄a at time t, while ct,na = √ 2 log(t)/na as before.\nPr(B.13) = Pr(Xa,na ≥ µa + ct,na) = Pr(Xa,na ≥ Ya,na + hna ∨ Ya,na ≥ µa + ct,na − hna) ≤ Pr(Xa,na ≥ Ya,na + hna) + Pr(Ya,na ≥ µa + ct,na − hn ≤ γ + Pr(Ya,na ≥ µa + ct,na − hna). (B.16) ≤ γ + exp(−2na(ct,na − hna)2) (B.17) ≤ γ + t−3.5 (B.18) ≤ 2t−3.5 (B.19)\nIn the above, we choose hna ≤ λ4ct,na with λ4 = 1− √ 3.5 2 and γ = t−3.5\nSimilarly, we prove a bound on the probability (B.12):\nPr(B.12) = Pr(X∗n ≤ µ∗ − ct,n) = Pr(X∗n ≤ Y ∗n − hna ∨ Y ∗n ≤ µ∗ − ct,n + hna) ≤ Pr(X∗n ≤ Y ∗n − hna) + Pr(Y ∗n ≤ µ∗ − ct,n + hna) ≤ γ + t−3.5 = 2t−3.5. (B.20)\nNow, Let’s compute the minimum value for na which is consistent with our choice of λ4 and γ. It is easy to show that na ≥ ( 2− √ 3.5\n3.5 √ 2 log t\n) 2\nv−1 . And this number converges to 0 as t increases.\nThe final event B.14 is exactly the standard UCB event and B.14 will be false for all na ≥ 8∆2a log t.\nNow, we can easily see that the effect of the interval in the algorithm will not change this number. Indeed, because we are only updating the mean each f steps, the number na should be divided by f . Now, after updating the mean, we play this arm for a number of steps lower or equal to f . So, we should multiply na by f which cancel out the effect of the division.\nIn summary, (and similarly to Theorem 3.2)\nTa(s) ≤ f0 + ⌈ max ( ( 2−\n√ 3.5\n3.5 √ 2 log t ) 2\nv−1 , 8\n∆2a log t\n)⌉\n+\n∞ ∑\nt=1\nt−1 ∑\nn=1\nt−1 ∑\nna=ℓ\n4t−3.5\n≤ f0 + ⌈ 8\n∆2a log t\n⌉\n+\n∞ ∑\nt=1\n4t−1.5\n≤ f0 + 1 + 8\n∆2a log t+ 4ζ(1.5)\n≤ f0 + 1 + 8\n∆2a log t+ 4ζ(1.5) (B.21)\nLemma B.2 (Improved (Chan, Shi, and Song 2010) bound). For any γ ≤ n−b, where b > 0, Chan’s hybrid mechanism is ǫ-differential private and has an error bounded with probability at least 1− γ by √ 8 ǫ log( 4 γ ) logn+ √ 8 ǫ log( 4 γ ).\nProof. Our improvement is based on two observations. Firstly, if γ ≤ n−b then we can use the alternative concentration bound for the sum of the Laplace distribution. Secondly, we note that given all the previous released sums, the current sum is only drawn from one of the two mechanisms. Thus there is no need to apply a composition theorem."
    } ],
    "references" : [ {
      "title" : "Finite time analysis of the multiarmed bandit problem. Machine Learning 47(2/3):235–256",
      "author" : [ "Cesa-Bianchi Auer", "P. Fischer 2002] Auer", "N. CesaBianchi", "P. Fischer" ],
      "venue" : null,
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Analytical approximations for real values of the lambert w-function",
      "author" : [ "Barry" ],
      "venue" : "Mathematics and Computers in Simulation",
      "citeRegEx" : "Barry,? \\Q2000\\E",
      "shortCiteRegEx" : "Barry",
      "year" : 2000
    }, {
      "title" : "Optimal adaptive policies for sequential allocation problems",
      "author" : [ "Burnetas", "A.N. Katehakis 1996] Burnetas", "M.N. Katehakis" ],
      "venue" : "Advances in Applied Mathematics",
      "citeRegEx" : "Burnetas et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Burnetas et al\\.",
      "year" : 1996
    }, {
      "title" : "you might also like: ” privacy risks of collaborative filtering",
      "author" : [ "Calandrino" ],
      "venue" : "IEEE Symposium on Security and Privacy,",
      "citeRegEx" : "Calandrino,? \\Q2011\\E",
      "shortCiteRegEx" : "Calandrino",
      "year" : 2011
    }, {
      "title" : "Private and continual release of statistics",
      "author" : [ "Shi Chan", "T.H. Song 2010] Chan", "E. Shi", "D. Song" ],
      "venue" : "In Automata, Languages and Programming",
      "citeRegEx" : "Chan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2010
    }, {
      "title" : "Asymptotic behavior of minimal-exploration allocation policies: Almost sure, arbitrarily slow growing regret",
      "author" : [ "Cowan", "W. Katehakis 2015] Cowan", "M.N. Katehakis" ],
      "venue" : "arXiv preprint arXiv:1505.02865",
      "citeRegEx" : "Cowan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cowan et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust and private Bayesian inference",
      "author" : [ "Dimitrakakis" ],
      "venue" : "In Algorithmic Learning Theory",
      "citeRegEx" : "Dimitrakakis,? \\Q2014\\E",
      "shortCiteRegEx" : "Dimitrakakis",
      "year" : 2014
    }, {
      "title" : "The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science 9(34):211–407",
      "author" : [ "Dwork", "C. Roth 2013] Dwork", "A. Roth" ],
      "venue" : null,
      "citeRegEx" : "Dwork et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2013
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Dwork" ],
      "venue" : "In Proceedings of the Third Conference on Theory of Cryptography,",
      "citeRegEx" : "Dwork,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork",
      "year" : 2006
    }, {
      "title" : "Boosting and differential privacy",
      "author" : [ "Rothblum Dwork", "C. Vadhan 2010] Dwork", "G.N. Rothblum", "S. Vadhan" ],
      "venue" : "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2010
    }, {
      "title" : "Differentially private online learning",
      "author" : [ "Kothari Jain", "P. Thakurta 2012] Jain", "P. Kothari", "A. Thakurta" ],
      "venue" : "COLT 2012 - The 25th Annual Conference on Learning Theory,",
      "citeRegEx" : "Jain et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2012
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules. Advances in applied mathematics 6(1):4–22",
      "author" : [ "Lai", "T.L. Robbins 1985] Lai", "H. Robbins" ],
      "venue" : null,
      "citeRegEx" : "Lai et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 1985
    }, {
      "title" : "nearly) optimal differentially private stochastic multi-arm bandits",
      "author" : [ "Mishra", "N. Thakurta 2015] Mishra", "A. Thakurta" ],
      "venue" : "Proceedings of the 31th International Conference on Conference on Uncertainty in Artificial Intelligence (UAI-2015)",
      "citeRegEx" : "Mishra et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2015
    }, {
      "title" : "Handling advertisements of unknown quality in search advertising",
      "author" : [ "Pandey", "S. Olston 2006] Pandey", "C. Olston" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Pandey et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Pandey et al\\.",
      "year" : 2006
    }, {
      "title" : "Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society 58(5):527–535",
      "author" : [ "H Robbins" ],
      "venue" : null,
      "citeRegEx" : "Robbins,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins",
      "year" : 1952
    }, {
      "title" : "nearly) optimal algorithms for private online learning in full-information and bandit settings",
      "author" : [ "Thakurta", "A.G. Smith 2013] Thakurta", "A.D. Smith" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Thakurta et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Thakurta et al\\.",
      "year" : 2013
    }, {
      "title" : "Supplementary Materials for Algorithms for Differentially Private Multi-Armed Bandits",
      "author" : [ "Tossou", "A. Dimitrakakis 2016] Tossou", "C. Dimitrakakis" ],
      "venue" : null,
      "citeRegEx" : "Tossou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tossou et al\\.",
      "year" : 2016
    }, {
      "title" : "Achieving differential privacy of data disclosure in the smart grid",
      "author" : [ "Zhao" ],
      "venue" : "IEEE Conference on Computer Communications,",
      "citeRegEx" : "Zhao,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhao",
      "year" : 2014
    }, {
      "title" : "bound). For any γ ≤ n−b, where b > 0, Chan’s hybrid mechanism is ǫ-differential private and has an error bounded with probability at least",
      "author" : [ "Improved (Chan", "Shi", "Song" ],
      "venue" : null,
      "citeRegEx" : ".Chan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : ".Chan et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Differential privacy (DP) (Dwork 2006) provides an answer to this privacy issue by making the output of an algorithm almost insensitive to any single user information.",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "2 Differential Privacy Differential privacy was originally proposed by (Dwork 2006), as a way to formalise the amount of information about the input of an algorithm, that is leaked to an adversary observing its output, no matter what the adversary’s side information is.",
      "startOffset" : 71,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "[Dwork 2006] Dwork, C.",
      "startOffset" : 0,
      "endOffset" : 12
    } ],
    "year" : 2015,
    "abstractText" : "We present differentially private algorithms for the stochastic Multi-Armed Bandit (MAB) problem. This is a problem for applications such as adaptive clinical trials, experiment design, and user-targeted advertising where private information is connected to individual rewards. Our major contribution is to show that there exist (ǫ, δ) differentially private variants of Upper Confidence Bound algorithms which have optimal regret, O(ǫ + log T ). This is a significant improvement over previous results, which only achieve poly-log regret O(ǫ log T ), because of our use of a novel intervalbased mechanism. We also substantially improve the bounds of previous family of algorithms which use a continual release mechanism. Experiments clearly validate our theoretical bounds.",
    "creator" : "LaTeX with hyperref package"
  }
}