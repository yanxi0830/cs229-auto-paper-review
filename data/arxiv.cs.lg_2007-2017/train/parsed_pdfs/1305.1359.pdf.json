{
  "name" : "1305.1359.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Differential Equations Approach to Optimizing Regret Trade-offs",
    "authors" : [ "Alexandr Andoni", "Rina Panigrahy" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ T ) is known, we argue that it important\nto ask what is the provably optimal algorithm for this problem — both because it leads to natural algorithms, as well as because regret is in fact often comparable in magnitude to the final payoffs and hence is a non-negligible term.\nIn the basic setting, the result essentially follows from a classical result of Cover from ’65. Here instead, we focus on another standard setting, of time-discounted payoffs, where the final “stopping time” is not specified. We exhibit an explicit characterization of the optimal regret for this setting.\nTo obtain our main result, we show that the optimal payoff functions have to satisfy the Hermite differential equation, and hence are given by the solutions to this equation. It turns out that characterization of the payoff function is qualitatively different from the classical (non-discounted) setting, and, namely, there’s essentially a unique optimal solution."
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider the following classical game of predicting a binary ±1 sequence. The player (predictor) sees a binary sequence {bt}t≥1, one bit at a time, and attempts to predict the next bit bt from the past history b1, . . . bt−1. The payoff (score) of the algorithm is then the count of correct guesses minus the number of the wrong guesses, formally defined as follows, for some target time T > 0, and where b̃t is the prediction at time t:\nAT = ∑\n1≤t≤T\nbtb̃t.\nOne can view this game as an idealized “stock prediction” problem as follows. Each day, the stock price goes up or down by precisely one dollar, and the player bets on this event. If the bet is right, the player wins one dollar, and otherwise she looses one dollar. Not surprisingly, in general, it is impossible to guarantee a positive payoff for all possible scenarios (sequences), even for randomized algorithms. However, one could hope to give some guarantees when the sequence has some additional property.\nThe above sequence prediction problem is in fact precisely equivalent to the two experts problem (or multi-armed bandits problem), where one considers two experts, via a reduction: one side of the reduction follows simply by using two experts, one always predicts “+1” and another always predicts “-1”. Then one measures the regret of an algorithm: how much worse one’s algorithm does as opposed to the best of the two experts (in hindsight, after seeing the sequence), which is equal to | ∑ 1≤t≤T bt|. We will henceforth will\nrefer to ∑\n1≤t≤T bt as the “height” of the sequence (as in the height of a growth chart of a stock). Regret has been studied in a number of papers, including [12, 22, 13, 5, 4]. A classical result says that one can obtain a regret of Θ( √ T ) for a sequence of length T , via, say, the weighted majority algorithm of [22]. Note that\nar X\niv :1\n30 5.\n13 59\nv1 [\ncs .L\nG ]\n7 M\nay 2\n01 3\nthe payoff per time step btb̃t is essentially equivalent to the well known absolute loss function |bt − b̃t| (see for example [10], chapter 8)1.\nObtaining a regret of Θ( √ T ) has since become the golden standard for many similar expert learning problem. But is this the best possible guarantee? While there is a lower bound of Ω( √ T ), it is natural to ask what is the optimal algorithm for minimizing the regret, departing from asymptotic notation. Note that the weighted majority algorithm may not be optimal, even if it obtains the “right order of magnitude”. More generally, one can ask what exactly are all possible payoff functions one can achieve as a function of the total payoffs of the two arms (height, in our case).\nIn this paper, we undertake precisely this task, of studying the algorithms that obtain the optimal, minimal regret possible and characterize the possible payoff functions. Our results also lead to optimal regret trade-offs between two experts in the experts problem from the equivalence between the two problems. The latter problem has been previously studied by [14], and later by [20], to address, say, an investment scenario where there may be two experts one risk taking and another conservative and one may be willing to take different regrets with respect to these two experts. In particular, it is known that it is possible to get regret O( √ T log T ) with respect to one expert and 1/TΩ(1) with respect to the other.\nThere are several reasons to study such optimal algorithms and compute the exact trade-off curves. First of all, such an optimal algorithm may be viewed as more “natural”, for example, because if an autonomous system has the same optimization criteria (of minimizing regret), it would arrive at such an “optimal” solution. Second, it is worthwhile to go beyond the asymptotics of a Θ( √ T ) bound. Specifically, often the final value of a sequence is actually of the order of √ T , such as for a random sequence. Although, we do not expect to obtain a positive payoff for a random sequence, a large fraction of all sequences still have O( √ T ) value. In such a scenario, it is critical to obtain the best possible constant in front of the √ T regret bound. When the value of the sequence is indeed around Θ( √ T ), an algorithm with a regret of Θ( √ T ) achieves a constant factor approximation, and improving the leading constants leads to an approximation factor which is a better constant. For example, in several investment scenarios it is known that the payoffs of the experts (or stocks) in time T is barely more than O( √ T ) (see, for example, the Hurst coefficient measurements of financial markets in [6, 26]). In such settings, the precise constant in regret term can translate into a difference between gain and loss. Indeed, we find that our algorithm can have a regret that is about 10% lower than that of the well known weighted majority algorithm and, at several positions on the curve, our payoff is improved by as much as 0.3 √ T (see figure 1(a)). We also obtain the exact trade-off curve between the regrets with respect to two arms (see figure 2). We note that, in the vanilla setting, when there is a time bound T , the solution already follows from the results of [12] (see also [9, 10]), who gave a characterization of all possible payoffs back in 1965. One can also obtain the optimal algorithm by computing a certain dynamic programming, similar to an approach from [21]. Yet, the resulting algorithm has a betting strategy and payoff function that are time-dependent as well as depend on the final stopping time T . These dependencies introduce issues and parameters that are hard to control in reality (often the predictor does not really know when the time “stops”). To understand the time-independent strategies, we are led to consider the another classic setting of time-discounted payoffs (see [17, 27]).\nThus we focus our study on regret-optimal algorithms in the time-discounted setting, where payoff is discounted, and there is no apriori time bound. Formally, we define a ρ−discounted version of payoff at some moment of time T , for a discount factor ρ ∈ (0, 1), as\nAρT = ∑ t≥0 bT−tb̃T−t · ρt\nThe question then is to minimize the regret with respect to this quantity, as a function of (discounted) height. One can also see this scenario as capturing the situation where we care about a certain “attention” window\n1since when |bt| = 1, |bt − b̃t| = |bt||bt − b̃t| = |1 − btb̃t| = 1 − btb̃t. Thus the absolute loss function is the negative of our payoff in one step plus a shift of 1. Also bt values from {−1, 1} or {0, 1} are equivalent by a simple scaling and shifting transform.\nof time (given by ρ). One of the consequences of our study is that, when the strategies are time-independent, the characterization of the optimal regret/algorithms becomes quite different."
    }, {
      "heading" : "1.1 Statement of Results",
      "text" : "In general, we study the optimal regret curves. Namely, we measure the payoff and regret as a function of the “height” of the sequence (the sum of the bits of the sequence, as defined above; one can also take a discounted sum). Note that comparing against height amounts to comparing the performance of our algorithm against that of two static experts: one that always predicts +1 (“long the stock”), and another that always predicts -1 (“short the stock”). The former obtains a payoff equal to the height and the latter obtains a payoff equal to negative height.\nWe use the notion of a payoff function — a real function f , which assigns algorithm’s payoff f(x) for each height value x. In particular, for fixed algorithm and a height x, let fT (x) denote the minimum payoff over all sequences with height x at time T . For a certain function fT , we will say that fT (x) is feasible if there is an algorithm with payoff at least fT (x) over all possible sequences {bt} such that h({bt}) = x. In the discounted scenario, the notion of height becomes the discounted height: hρT ({bt}t≤T ) = ∑ t≥0 bT−tρ\nt. More importantly, for time-independent strategies (in the discounted setting) we will say that f(x) is feasible if the payoff is at least f(x) for (discounted) height x at all times (feasible in steady state).\nOur goal will be to optimize the regret, defined for a payoff function f , as follows:\nR(f) = max x |x| − f(x),\nwhere x ranges over all possible (discounted) heights. Note that |x| is the maximum of the payoff of the two constant experts. In general, we allow bets b̃t to be bounded reals in the interval [−1, 1]. In such a case, it is sufficient to consider deterministic strategies only. One can also consider the version of the problem when there is no restriction on the range of values for b̃t. We will refer to this case as the sequence prediction problem with unbounded bets. This will be useful in deriving bounds for the standard case with bounded bets.\nFor starters, we remind the result for the vanilla, non-discounted, fixed stopping time setting, which follows from [12], and is related to Rademacher complexity of the predictions of the two experts (see [9, 10]). The theorem below also extends to the discounted scenario, with fixed stopping time T . See Appendix A.2 for discussion of this settings.\nTheorem 1.1. Consider the problem of prediction of binary sequence. The minimal possible regret is\nR = min f R(f) =\n√ 2\nπ\n√ T +O(1).\nThere is a prediction algorithm (betting strategy) achieving this optimal regret and has f(x) = |x| −R. The actual corresponding betting strategy may be computing via dynamic programming.\nFurthermore, f is feasible iff ∑ f(x)p(x) = 0 where p(x) is the probability of a random walk of length T to end at x (i.e., E [f(x)] = 0 for x being the height of a random sequence). For bounded bet value, we have the additional constraint that f is 1-Lipschitz.\nTime-independent strategies. Our main result is for optimal regret curves in the setting of discounted and time-independent strategies. We characterize the set of all-time feasible f ’s. For this, we define a certain “optimal” curve function, which will be central to our claims. For constants c1, c2, define the following function:\nFc1,c2(x) = c1\n( x · erfi(x)− ex 2 / √ π )\n+ c2x,\nwhere erfi(x) = i·erf(ix) is the imaginary error function. We also define F̂c1,c2 to be the function obtained by bounding the derivative of F to lie in [−1, 1]. That is F̂ = F when |F ′| ≤ 1 and F̂ ′ = sign(F ′) when |F ′| > 1.\nTheorem 1.2 (Main). Consider the problem of discounted prediction of binary sequence with the discount factor of ρ = 1− 1/n (corresponding to a “window size” of n). A payoff function f is feasible in the steady state if there exist constants c1, c2 such that for all x ∈ [−n, n]:\nf(x) ≤ √ n · F̂c1,c2(x/ √ n)−O(1).\nConversely, if there exists a function g such that f(x) = √ n · g(x/ √ n) for infinitely many n and g is\npiecewise analytic2 then g(x) ≤ F̂c1,c2(x) for some constants c1, c2. Hence, the minimum ρ-discounted regret is, for C = minα≥1\n1√ π · α erfi( √ lnα) :\nmin fρ\nR(fρ) = C √ n+O(1).\nWe note that the above characterization follows from a “limit view” of the corresponding dynamic programming characterizing the payoff function, which leads to a differential equation formulation of the question. Such an approach has been previously undertaken by [21] to show that many differential equations can be realized as two-person games, as is also the case in our scenario.\nIn particular, to prove Theorem 1.2, we show that f needs to satisfy the inequality\nf(x) ≥ f(ρx+ 1) + f(ρx− 1) 2ρ . (1)\nIt turns out that, after the correct rescaling, and taking the process to the limit, we obtain a differential equation. Namely, let g(x) = f( √ nx)/ √ n denote a normalized version of f where the axes are scaled down by a factor or √ n (the standard deviation of the height). We will assume that g is (piece-wise) analytic3. Then, as n approaches infinity, the above inequality implies the following differential inequality:\ng′′ − 2xg′ + 2g ≤ 0\nIf we replace the inequality by equality, we obtain the Hermite differential equation which has as its solution the aforementioned functions Fc1,c2 . While our solutions are close to these differential equation solutions Fc1,c2 , we also point out the curious fact that if we insist on the constraint (1) being an equality, then the only solution is f(x) = 0. Thus the relaxation into an inequality seems necessary to capture the feasible set of functions.\nThe algorithm from the above theorem is explicitly given. In particular, it computes the current discounted height x, and then outputs the bet b̃(x) = f(ρx+1)−f(ρx−1)2 for the next time step, for f from Theorem 1.2. Surprisingly, the characterization of the feasible payoff functions f is very different when the strategies are time-independent as opposed to the time-dependent case. In particular, in the time-independent case, there are only two degrees of freedom as compared to the time-dependent case when there were infinite (or ≈ n) degrees of freedom.\nSee figure 1(a) for the plots of the resulting betting strategy as compared to the one resulting from the multiplicative weights update algorithm (which also happens to be a time-independent strategy). Also, see figure 1(b) for the resulting payoff function f (where the axes have been scaled down by √ n). After scaling x down by √ n, we obtain that b̃(x) tends to F ′(x) as n→∞.\nTrading off regrets between two experts. We also relate our problem to experts problem with two experts (or the multi-armed bandit problem in the full information model with two arms/experts). Here, in each round, each expert has a payoff in the range [0, 1] that is unknown to the algorithm. For two experts, let b1,t, b2,t denote the payoffs of the two experts at time t. The algorithm pulls each arm (expert) with probability b̃1,t, b̃2,t ∈ [0, 1] respectively where b̃1,t + b̃2,t = 1. The payoff of the algorithm in this setting is A′T := ∑T t=1 b1,tb̃1,t + b2,tb̃2,t. The objective of the algorithm is to obtain low regret with respect to the two experts. We note that this was first studied in [14].\n2In fact, it suffices to assume that the first three derivatives of g exist instead of requiring it to be analytic. 3In fact all we will need is that it is twice differentiable.\nWe achieve the optimal tradeoff between the regrets with respect to the two experts by reducing it to an instance of the sequence prediction problem. In particular, define the loss of a payoff function f as the negative of the minimum value of f . Then we show that the regret/loss trade-off for the sequence prediction problem is tightly connected to the trade-off of the regrets with respect to the two experts. Hence, we also derive the regret trade-off for the case of two experts. (See figure 2 for the trade-off curve for the regrets in the two experts problem.)\nTheorem 1.3. Consider the problem of trading off regrets R1, R2 with respect to two experts. Regrets R1, R2 are achievable in the time-discounted setting if and only if there exists an α > 0 such that T (αR1/ √ n) + T (αR2/ √ n) ≥ α/ √ π, where T (x) = erfi( √ lnx) for x ≥ 1.\nMultiple scales. Finally, we investigate the possible payoff functions at multiple time scales ρ (window sizes). Several earlier papers considered regrets at different time scales; see [8, 16, 18, 29, 20]. We consider\ntwo different time scales, ρ1 = 1− 1/n1 and ρ2 = 1− 1/n2, although a similar result can be obtained for a larger number of time scales.\nWe exhibit the necessary and sufficient condition for a feasible payoff function, as window size goes to infinity. In particular, suppose that n1 = a1n and n2 = a2n where n tends to infinity. Let x1 and x2 be the time discounted heights for the two different time scales. We can ask if it is possible to get (time discounted) payoff functions f1(x1, x2) and f2(x1, x2) at time scales n1 and n2 respectively. Again we apply the coordinate rescaling by √ n for both x1, f1 and x2, f2.\nTheorem 1.4. For n ≥ 1, fix two windows n1 = a1n and n2 = a2n. As n goes to infinity, there is are payoff function f1(x1, x2) = √ ng1(x1/ √ n, x2/ √ n) for the discount rate ρ1 = 1 − 1/n1 and f2(x1, x2) =√\nng2(x1/ √ n, x2/ √ n) for the discount rate ρ2 = 1− 1/n2, as n goes to infinity, if and only if the following\nsystem of partial differential inequalities is satisfied: E1 , − 12 ( ∂ ∂x1 + ∂∂x2 )2 g1 + ( a21x1 ∂ ∂x1 + a22x2 ∂ ∂x2 ) g1 − a21 · g1 ≥ 0\nE2 , − 12 ( ∂ ∂x1 + ∂∂x2 )2 g2 + ( a21x1 ∂ ∂x1 + a22x2 ∂ ∂x2 ) g2 − a22 · g2 ≥ 0\nE1 + E2 ≥ ∣∣∣( ∂∂x1 + ∂∂x2) (g1 − g2)∣∣∣ .\nWe do not seem to have explicit analytical solution for the above system of inequations, and so perhaps one would have to rely on numerical simulations to solve it. This part is deferred to Appendix C due to space limitation."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "There is a large body on work on regret style analysis for prediction. Numerous works including [12, 9] have examined the optimal amount of regret achievable with respect to multiple experts. Many of the results in this body of work can be found in [10]. It is well known that in the case of static experts, the optimal regret is exactly equal to the Rademacher complexity of the predictions of the experts (chapter 8 in [10]). Recent works, including [1, 2, 23], have extended this analysis to other settings. Measures other than the standard regret measure have been studied in [25]. Also related is the NormalHedge algorithm [11], though it differs in both the setting and the precise algorithm. Namely, NormalHedge looks at undiscounted payoffs and obtains strong regret guarantees to the epsilon-quantile of best experts. We look at two experts case (where epsilon-quantile is not applicable) and seek to obtain provably optimal regret.\nAlgorithms with performance guarantees within each interval have been studied in [8, 16, 29] and, more recently, in [18, 20]. The question of what can be achieved if one would like to have a significantly better guarantee with respect to a fixed arm or a distribution on arms was asked before in [14, 20]. Tradeoffs between regret and loss were also examined in [28], where the author studied the set of values of a, b for which an algorithm can have payoff aOPT + b logN , where OPT is the payoff of the best arm and a, b are constants. The problem of bit prediction was also considered in [15], where several loss functions are considered. Numerous papers ([7, 19, 3]) have implemented algorithms inspired from regret syle analysis and applied it on financial and other types of data."
    }, {
      "heading" : "2 Time-Independent Prediction Algorithms",
      "text" : "In this section we study the optimal regret and algorithms for the time-independent strategies and regret curves. We consider the time-discounted setting, thereby proving Theorem 1.2.\nAs mentioned in the introduction, we consider a payoff f to be feasible if there is a prediction algorithm that achieves a payoff of at least f(x) for the discounted height x at all times t ≥ 1. We will argue that, without loss of generality, we can assume that the betting strategy b̃(x) is time independent and the payoff always dominates the function f(x).\nObserve that for a time independent betting function, the payoff function it achieves is also time independent in the limit.\nClaim 2.1. If f is feasible (in steady state), then there is a time-independent betting strategy b̃ that achieves payoff function f .\nProof of Claim 2.1. Remember that we use discount factor of ρ = 1 − 1/n, where n ≥ 1 is the “window” size. Assume there is a time-dependent betting strategy b̃t(x) that achieves payoff at least f(x) in the steady state. We consider the average of these betting strategies over a long interval and argue that it changes only slightly over time. Note that the time shifted strategy b̃t−i also achieves payoff at least f(x) at all times. This means that an average of a large number of such shifted betting strategies also achieves this. Consider the average strategy µt(x) = 1 N ∑N i=1 b̃t−i(x), and note it is essentially constant over a small window for a sufficienly large N . For example, if we choose N > exp(n) the differences in µt over a window of size poly(n) are exponentially small. Since we are time discounting at rate 1− 1/n, it suffices to ignore anything outside such a window of size poly(n).\nWe will characterize the payoff functions that are feasible for time-independent betting strategies.\nLemma 2.2. If there is a time-independent betting strategy with payoff function f(x) then\nf(x) ≥ f(ρx+ 1) + f(ρx− 1) 2ρ . (2)\nConversely if f satisfies the above inequality and f(0) ≤ 0, then it is feasible with unbounded bets. In particular the betting strategy b̃(x) = f(ρx+1)−f(ρx−1)2 achieves a payoff function at least f . For the bounded bets case, we need the additional constraint that b̃(x) computed thus satisfies |b̃(x)| ≤ 1\nProof. Note that since the payoff at time t is b̃(xt)b (where b = bt), we have ρf(x) + bb̃(x) ≥ f(ρx+ b) where b ∈ {±1}. This is because at time t− 1 there is some sequence of height x with payoff f(x). Thus, we have ρf(x) + b̃(x) ≥ f(ρx+ 1) and, similarly, ρf(x)− b̃(x) ≥ f(ρx− 1). Averaging the two we get inequality (2).\nTo prove the converse we can use induction on time t to show that the stated betting strategy achieves payoff at least f(x). Clearly at t = 0, x = 0 and since f(0) ≤ 0 the condition is satisfied. Further, if the height is x at time t− 1 then at the next step the payoff is at least ρf(x) + bb̃(x) ≥ f(ρx+ b) for b ∈ {−1, 1} which follows from the inequality (2).\nWe now proceed to proving the main claims of Theorem 1.2. In particular, we start by showing the “converse” direction. For this, we will show that, in the limit, the payoff function has to satisfy a certain differential equation, when property scaled. The next lemma proves precisely this switch. Lemma 2.3. Let g(x) = f( √ nx)/ √ n, and assume it is piece-wise analytic. Then as n→∞, condition (2) becomes g′′ − 2xg′ + 2g ≤ 0. (3) Proof. Rescaling and setting δ = 1/ √ n (i.e., ρ = 1− δ2) in inequality (2) gives us:\n(1− δ2)g(x) ≥ g((1− δ 2)x+ δ) + g((1− δ2)x− δ)\n2 .\nUsing Taylor expansion on g, we obtain\n(1− δ2)g(x) ≥ g(x)− δ2xg′(x) + (1/2)δ2(1 + δ2x2)g′′(x) +O(δ3)g′′′(x− δ2x± δ) 0 ≥ g(x)− xg′(x) + (1/2)(1 + δ2x2)g′′(x) +O(δ)g′′′(x− δ2x± δ)\nAs δ = 1/ √ n→ 0, we obtain that g′′ − 2xg′ + 2g ≤ 0.\nWe note that if we replace the inequality (2) with the equality, we obtain the Hermite differential equation:\ng′′ − 2xg′ + 2g = 0. (4)\nDifferential equation (4) has a general solution of the form Fc1,c2 = c1(x erfi(x) − ex 2 / √ π) + c2x, where erfi(x) = i · erf(ix) is the imaginary error function.\nRemark 2.4. Note that, for example, this “limiting payoff function” Fc1,c2 satisfies the “limiting” T →∞ characterization similar to Theorem A.3. Namely, for any constants c1, c2, we have that ∫ pρ(x)Fc1,c2(x) dx =\n0, where pρ(x) is the distribution of the ρ-decayed random walk to be at height √ nx, at the limit of n, T →∞. (Note that pρ converges to N(0, 1) when n→∞.)\nIn the following, we show that the solutions for the steady-state payoffs are essentially characterized by functions Fc1,c2 . Note that we thus obtain solutions that have only two degrees of freedom. This is in stark contrast to the time-dependent strategies, where there is an infinite number of degrees of freedom (see Appendix A).\nThe next lemma shows that if g(x) = f( √ nx)/ √ n satisfies the differential inequality, then g must be\ndominated by Fc1,c2 , i.e., a solution to the Hermite differential equation.\nLemma 2.5. Suppose g satisfies g′′ − 2xg′ + 2g ≤ 0. Then there exist some c1, c2, such that g ≤ Fc1,c2 .\nProof. There is a unique solution y = Fc1,c2 such that y(0) = g(0) and y ′(0) = g′(0). Now look at h = g− y. We will show that h ≤ 0. Observe that h satisfies h′′ − 2xh′ + 2h ≤ 0 and h(0) = h′(0) = 0. We will make the substitution u = xh′ − h. Hence we have that u′ = xh′′, and thus u′/x − 2u ≤ 0. or u′ ≤ 2ux for x ≥ 0 and u′ ≥ 2ux for x ≤ 0 and u(0) = 0. This implies that u ≤ 0. This means that xh′ − h ≤ 0 which implies h ≤ 0.\nSo far we have ignored the condition that the |b| ≤ 1 thus allowing unbounded bets. In the following claim, we consider the case of bounded bets and show that in this case the function g has a bounded derivative.\nClaim 2.6. With bounded bets |b| ≤ 1, the function g must also satisfy the constraint |g′(x)| ≤ 1 as n→∞. Proof. For δ = 1/ √ n, we have that\n(1− δ2)g(x) + δ ≥ g((1− δ2)x+ δ) =⇒ −δg(x) + 1 ≥ g((1− δ 2)x+ δ)− g(x)\nδ\nConsidering δ → 0 gives g′(x) ≤ 1. Similarly we get −g′(x) ≤ 1.\nSuppose we choose a solution g = Fc1,c2 , this would correspond to the betting strategy b(x) = c1 ·erfi(x)+ c2. Note that F doesn’t satisfy |g′(x)| ≤ 1, but a simple capping of its growth when |F ′| ≥ 1 gives a alternate function F̂ (see figure 1(b)) that satisfies the extra condition. This essentially corresponds to capping b(x) so that |b(x)| ≤ 1. Let b̂(x) denote the capped version of b(x) that can be used for bounded bets. This concludes the “converse” part of Theorem 1.2. Next, we switch to showing the forward direction, that if (a properly scaled) f is dominated by F , then it is also a valid payoff function. In particular, in the next lemma, we show that the solutions to the differential inequality can be made to satisfy the original recursive inequality (2) with a small error term. Lemma 2.7. For any constants c1, c2, for the bounded bets case, there is a function ĝ(x) = F̂ (x)−O(1/ √ n) such that √ n · ĝ( √ nx) satisfies the inequality (2).\nWith unbounded bets, there is a function g(x) = F (x) · e−O(x2/n+1/n) such that √ ng( √ nx) satisfies the\ninequality (2). Proof. Let δ = 1/ √ n. We will argue that the O(δ) slack is sufficient to account for the error in the Taylor approximation in the bounded bets case. To see this note that the error in the Taylor approximation is δ2x2g′′(x) + O(δ)ḡ′′′((1 − δ2)x ± δ), where ḡ′′′(x ± ) denotes the average of g′′′ at two points in the range x± . We will look at the interval where |F ′(x)| ≤ 1. For constant c1, c2 the end points of this interval are also constants which implies all the terms in the error expression are constants (since f is independent of δ). Thus the error is at most O(δ). So it suffices to satisfy the condition g′′ − 2xg′ + 2g < −O(δ) which is satisfied by F −O(δ). For the region where |F ′(x)| ≥ 1, note that in F̂ we are capping |F̂ ′(x)| = 1 and since ĝ is F̂ shifted down, it also satisfies the inequality.\nFor the case of unbounded bets, observe that the recursive inequality holds if we satisfy the following, per the approximation of the Taylor series:\nḡ′′(x− x2δ2 ± δ)(1 + x2δ2)− 2xg′(x) + 2g(x) ≤ 0.\nFor simplicity of explanation, consider x ≥ 0. Note that δx ≤ 1. We have F ′′(x) = c1ex 2 . Suppose we look for a function g that satisfies: g′′(x) is even and is increasing in x when x ≥ 0 and g′′(x± ) ≤ g′′(x)/e4(x2 2+ 2) for a big enough constant in the O — we will later verify that our resulting g indeed satisfies this (note that this is satisfied for F ). Then, since (1 + x2δ2) ≤ eO(x2δ2), the above inequality is satisfied as along as\ng′′(x) ≤ e−O(x 2δ2+δ2)2(xg′(x)− g(x))\nAgain, for u = xg′ − g, we get: u ′ x ≤ e −O(x2δ2+δ2)2u which holds if u ≤ ex2−O(δ2+δ2).\nSo it suffices that xg′ − g = ex2−O(x2δ2+δ2). Dividing by x2, we get: (g/x)′ = ex2−O(x2δ2+δ2)/x2. Note that without the correction terms the earlier differential equation (g/x)′ = ex 2 /x2 has the solution g = F , and so for the new equation there is a solution g = Fe−O(x 2δ2+δ2). Note that this g also satisfies g′′(x± ) ≤ g′′(x)/e4(x2 2+ 2) that we had assumed.\nOur Theorem 1.2 is hence concluded by Lemma 2.2 and Lemma 2.7. In the following we remark that obtaining a (non-trivial) solution that preserves the equation (4) precisely is impossible.\nRemark 2.8. If we convert the condition (2) into an equality then the only satisfying analytic solution f is f(x) = cx for a constant c. Thus the relaxation into an inequality seems to be necessary to find all the feasible payoff functions\nProof. If we require the equality f(x) = f(ρx+1)+f(ρx−1)2ρ then applying this recursively i times gives:\nf(x) = ρ−i ∑ b1,···bi∈{−1,1} f(ρ ix+ ρi−1b1 + ρ i−2 · · ·+ bi)\n2i\nWe apply Taylor series to f(ρix + ρi−1b1 + · · · + bi) around the point y = ρi−1b1 + · · · + bi to conclude that f(y + ρix) = f(y) + ρixf ′(y) + ρ2ix2f ′′(y ± ρix). We now consider the following difference\nf(x)− f(0) = ρ−i ∑ b1,···bi∈{−1,1} f(ρ\nix+ ρi−1b1 + · · ·+ bi)− f(ρi−1b1 + · · ·+ bi) 2i\nand using the above expansion, we have f(x)− f(0) = ∑ b1,···bi∈{−1,1},y=ρi−1b1+···+bi xf\n′(y) + ρix2f ′′(y ± ρix) 2i .\nTaking i tend to infinity, we conclude that f(x)− f(0) = x · ∫ pρ(y)f(y) dy. This implies that f is of the\nform f(x) = cx+ a. Moreover substituting f into the equality condition, we obtain that a = 0."
    }, {
      "heading" : "A Prediction Algorithms for Fixed Stopping Time",
      "text" : "In this section we discuss the optimal regret and the corresponding betting algorithm for a fixed stopping time T , which leads to strategies that depend on current time t and the stopping time T . We consider the classical non-discounted setting (Theorem 1.1) and the time-discounted setting (Theorem A.3), both with fixed stopping time.\nIn the non-discounted setting, we show that the optimal regret and algorithm follow easily from the existing work of [12].\nWe note that the resulting prediction algorithms depend on the current time t and the stopping time T . We will consider the admittedly more interesting case — of time-independent strategies — in the next section.\nA.1 Non-discounted setting\n[12] gave a precise characterization of possible payoff curves attainable. First of all, he showed that, if, for a sequence b̄ ∈ {±1}T , we denote g(b̄) to be the payoff/score obtained for sequence b̄, then ∑ b̄ g(b̄) = 0 for all possible algorithms. Cover proves the following characterization of the curve as a function of the height of the sequence:\nTheorem A.1 ([12]). Let f : N→ R be the payoff function of an algorithm, where f(x) is the payoff of an algorithm for sequences of height x precisely. Then f is feasible if and only if: 1) ∑T x=0 ( T x ) f(T − 2x) = 0 and 2) |f(x+ 1)− f(x)| ≤ 1 (f is Lipschitz).\nFrom the above theorem we have the following corollary. Corollary A.2. f(x) = |x| − R is feasible for R = √\n2 π\n√ T + O(1), and this is the minimum R for which\nthis is feasible. Proof. Note that Theorem A.1 holds for the payoff function f(x) = |x| −R, where R = Eb̄∈{±1}T [ | ∑ i b̄i| ] .\nTo compute this value R, we use following standard approximation: ∣∣∣Eb̄∈{±1}T [|∑i b̄i|]− Ex∼φ [|√Tx|]∣∣∣ ≤ O(1), where φ(x) is the normal distribution (see, e.g., [24], Theorem 3.4). Furthermore, we have that\nEx∼φ [ | √ Tx| ] = √ 2 π √ T . The corollary follows.\nTo recover the actual prediction algorithm, we employ the following standard dynamic programming. Namely, define st(x) to be the minimal necessary algorithm payoff, after t\nth time step for height x, in order to obtain payoffs of sT (y) = f(y) = |y|−R. In particular, if b̃t(x) denotes the prediction (bet) at time t assuming the current height is x, we have that st(x) = min|bt(x)|≤1 max{st+1(x+ 1)− b̃t+1(x), st+1(x− 1) + b̃t+1(x)}. Suppose we ignore the boundedness of bt(x), then the minimum is achieved for bt+1(x) = 1 2 (st+1(x + 1) −\nst+1(x− 1)). Note that this way we obtain s0(0) = Eb̄∈{±1}T [ f( ∑ i b̄i) ] = 0 (which gives a different proof of the above theorem). But these values of b̃ actually satisfy |b̃t(x)| ≤ 1, since if the Lipschitz condition holds at time t, then it also holds at time t− 1. Hence there was no loss of generality of dropping the boundedness of b̃t(x)’s. In particular, we have that b̃t(x) = 1 2T−t+1 ∑ b̄∈{±1}T−t+1 ( |x+ 1 + ∑ j b̄j | − |x− 1 + ∑ j b̄j | ) .\nThis concludes the proof of Theorem 1.1 to show the optimal regret and prediction algorithm for the vanilla fixed stopping time setting. Note that the prediction algorithm b̃t(x) depends on the current time: for example, for t close to T the all bet values are close to 1, whereas for small t’s we obtain very small values of b̃t(x).\nA.2 Time-discounted setting\nWe prove the following theorem for the time-discounted setting with fixed stopping time T , by extending the characterization given in Section A.1.\nTheorem A.3. Consider the problem of time-discounted prediction of binary sequence for “window size” n. Fix the discount factor ρ = 1− 1/n. For any fixed time T , fT is feasible iff ∫ f(x)pT (x) dx = 0 where pT (x) is the probability of a (decayed) random walk to end at height x and f is 1-Lipshitz (for bounded bet value). There is an algorithm (betting strategy) achieving this optimal regret and has f(x) = |x| − Rρ, where\nRρ = minf RT (f) = √ 2 π √ α + O(1), and α = 1−ρ 2T\n1−ρ2 . Note that α → T when T n, and α → n/2 when T n. The betting strategy may be computing via dynamic programming.\nFirst, we need to count the number of random walks achieving a certain discounted height x. When the height was not discounted, this was simply a binomial distribution, which we approximated by a normal distribution. It turns out that, in the discounted height case, the height distribution is also approaches normal distribution at the limit. Specifically, we show the following lemma.\nLemma A.4. Consider the time-discounted setting, with discount ρ = 1 − 1/n for some n ≥ 1. Let pT (x) be the probability that a random binary sequence of length T has discounted height x ∈ [−n, n]. Then, as T goes to infinity, the probability distribution of the discounted height, scaled down by √ α, converges to the normal distribution N(0, 1), where α = 1−ρ 2T\n1−ρ2 . Furthermore, Eb̄∈{±1}T [ | ∑ i≥1 b̄iρ T−i| ] = √ 2 π √ α±O(1).\nProof. Note that the height is distributed as x = ∑T i=1 b̄iρ\nT−i where b̄i are random ±1. Then, by Lyapunov central limit theorem, we have that 1√\nα\n∑T i=1 b̄iρ T−i tends to N(0, 1) as long as T = ωn(1).\nAgain, we have that (see, e.g., [24], Theorem 3.4) ∣∣∣Eb̄∈{±1}T [|∑i≥1 b̄iρT−i|]− Ex∼φ [|√α · x|]∣∣∣ = O(α−1)·∑T\ni=0 ρ 3i = O(1). Hence, we obtain that Eb̄∈{±1}T [ | ∑ i≥1 b̄iρ T−i| ] = √ 2 π √ α±O(1).\nThe rest of the proof of Theorem A.3 follows along the same lines of Theorem 1.1. Specifically, one can employ the same dynamic programming (for all possible discounted heights). We again have that s0(0) = Ex∼pT [f(x)] for any desired target function f . The only way s0(0) = 0 is when Ex∼pT [f(x)] = 0. As long as f is also Lipschitz, the dynamic programming will recover the betting strategy with bounded bets |b̃t(x)| ≤ 1. As in the previous setting, note that the betting strategy b̃t depends on the time t: it is small at the beginning, and gets closer to 1 for large values of t (close to T )."
    }, {
      "heading" : "B Trade-off with two experts",
      "text" : "In this section we will prove Theorem 1.3 by proving an equivalence between the sequence prediction problem and the two-experts problem. In each round of the experts problem, each expert has a payoff in the range [0, 1] that is unknown to the algorithm. For two experts, let b1,t, b2,t denote the payoffs of the two experts. The algorithm pulls the each arm (expert) with probability b̃1,t, b̃2,t ∈ [0, 1] respectively where b̃1,t+ b̃2,t = 1. The payoff of the algorithm is A = ∑T t=1 b1,tb̃1,t + b2,tb̃2,t. Let X1 = ∑T t=1 b1,t We will study the regret trade-off R1, R2 with respect to these two experts which means that A ≥ X1 −R1 and A ≥ X2 −R2. For this we we translate it into an instance of the sequence prediction problem where we show how we can obtain a tradeoff between regret R and loss L, which is defined as the minumum payoff of the algorithm. With two experts, the regret/loss tradeoff in the sequence prediction problem is related to regret trade-off for the two experts problem. Let R, L be feasible upper bounds on the regret and loss in the sequence prediction problem in the worst case; Let Ro, Lo be feasible upper bounds on the regret and loss with version of the sequence prediction problem with one sided bets (that is b̃t cannot be negative; the feasible payoff curves for this case is a simple variant of Fc1,c2 where F ′ is capped to lie in [0, 1].) Let R1, R2 be feasible\nupper bounds in regret with respect to expert one and expert two in the worst case. Another variant that has been asked before is a tradeoff between regret to the average and regret to the max (see [14, 20]). Let Rm, Ra be feasible upper bounds on the regret to the max and regret to the average with two experts in the worst case.\nTheorem 1.3 follows from the following two lemmas.\nLemma B.1. Regret and loss R,L is feasible in the sequence prediction problem if and only if Rm = R/2, Ra = L/2 is feasible for regret to the max and regret to the average in the two experts problem.\nRo, Lo is feasible in the sequence prediction problem (with one sided bets) if and only if R1 = Lo, R2 = Ro is feasible for regret to the first expert and regret to the second expert in the two experts setting.\nFor x ≥ 0, let T (x) = h(g−1(x)) where g(x) = ex2 , h(x) = erfi(x). Note that T (x) = erfi( √ lnx).\nProof of Lemma B.1. First we look at reduction from the regret to the average and regret to the max problem. We can reduce this problem to our sequence prediction problem by producing at time t, bt = (b1,t − b2,t)/2. A bet b̃t in our sequence prediction problem can be translated back into probabilities b̃1,t = (1 + b̃t)/2 and (1− b̃t)/2 for the two experts. A payoff A in the original problem gets translated into payoff∑ t b1,t(1 + b̃t)/2 + b2,t(1 − b̃t)/2 = (X1 + X2)/2 + A in the two experts case. In this reduction the loss L gets mapped to Ra and the regret R gets mapped to Rm. However note that bt is now in the range [0, 1/2]. Therefore we need to scale it by 2 to reduce it to the standard version of the original problem. Conversely, given an sequence bt of the prediction problem we can convert it into two experts with payoffs b1,t = (1 + bt)/2, b2,t = (1− bt)/2. The average expert has payoff T/2. A payoff of A in prediction problem can be obtained from a sequence of arm pulling probabilities with payoff T/2 +A/2 by interpreting the arm\npulling probabilities as (1± b̃t)/2 since ∑ t (1+bt) 2 (1+b̃t) 2 + (1−bt) 2 (1−b̃t) 2 = T/2 +A/2.\nNext we look at regrets R1, R2 with respect to the two experts. Given a sequence of payoffs to for the two experts we can reduce it to a sequence for the (one sided ) prediction problem by setting bt = b2,t− b1,t. A bet b̃t in the prediction problem can be translated to probabilities b̃1,t = 1 − b̃t and b̃2,t = b̃t for the two experts. A payoff A in the prediction problem gets translated into payoff ∑ t(1 − b̃t)b1,t + b̃tb2,t = X1 + A in the two experts case where a zero regret in the prediction would correspond to A = X2 − X1. Thus a loss of Lo translates to a regret R1 = Lo with respect to the first arm. And regret Ro translates to regret R2 = Ro with respect to the second arm. Thus if Ro, Lo is feasible then so is R1 = Ro, R2 = Lo. Conversely, given an instance of the prediction problem with one sided bets, we can convert it to a version of the two armed problem by setting b2,t = bt, b1,t = 0 if bt ≥ 0 and b2,t = 0, b1,t = −bt otherwise. A bet b̃t is used in our original problem if the arms are pulled with probabilities 1 − b̃t and b̃t respectively. The payoff in the experts problem is X1 + ∑ t b̃t(b2,t − b1,t). So regrets R1, R2 will translate to Lo = R1, Ro = R2 in the prediction problem with one sided bets. The above reduction also works for the time-discounted case. Lemma B.2. Let R,L,R0, L0 be normalized by a factor √ n (scaled down). R,L is feasible in the original problem if and only if T (R/L) = 1/( √ πL).\nRo, Lo is feasible in the original problem (with one sided bets) if and only if there is an α > 0 so that T (αLo) + T (αRo) ≥ α/ √ π.\nProof of Lemma B.2. The best tradeoffs for R,L is attained when F is symmetric; that is, F = c1(x erfi(x)− ex 2 / √ π) with the slope capped in the interval [1,−1]. Here L = c1/ √ π corresponds to the minimum value attained at x = 0. R is obtained by looking at x − F at the point x0 where F ′ = 1 giving c1 erfi(x0) = 1 implying R = x− F = c1ex 2 0/ √ π. Thus 1/( √ πL) = erfi(x0) and R/L = e x20 , implying T (R/L) = 1/(L √ π).\nIn the case of one sided bets, we look at the curve F = c1(x erfi(x)− ex 2 / √ π) + c2x where additionally the derivative is capped in the interval [0, 1]. Loss Lo is maximized at the minimum point x1 where F ′ = 0 giving c1 erfi(x1) + c2 = 0 implying Lo = −F (x1) = c1ex 2 / √ π. Regret Ro is maximized at x0 where F ′ = 1 (which means c1 erfi(x1) + c2 = 1) giving Ro = x − F = c1ex 2 1/ √ π. Since ex 2\nis even and erfi(x) is odd, T (Lo √ π/c1) = |c2/c1| and T (Ro √ π/c1) = |(1− c2)/c1|. For a given c1 ≥ 0 (as otherwise regret is infinity), a c2 exists if and only if T (Lo √ π/c1) + T (Ro √ π/c1) ≥ 1/c1."
    }, {
      "heading" : "C Multi-scale Optimal Regret",
      "text" : "We now show how the framework can be extended to the multiple time scales. The sequence bt may have trends at some unknown time scale and therefore it is important that the algorithm has small regret not just at one time scale but simultaneously at many timescales. We will now prove that (with unbounded bets) there are (normalized) payoff functions g1(x1, x2) and g2(x1, x2) at time scales an and bn if and only if it satisfies the conditions in Theorem 1.4.\nProof of Theorem 1.4. If b̃(x1, x2) is the betting function. then as before we get ρ1f1(x1, x2) + bb̃(x1, x2) ≥ f1(ρ1x1 + b, ρ2x2 + b) for b ∈ {−1, 1} and ρ2f1(x1, x2) + bb̃(x1, x2) ≥ f2(ρ1x1 + b, ρ2x2 + b) for b ∈ {−1, 1}\nFurther these conditions are sufficient. Simplifying we get\nρ1f1(x1, x2) + b̃(x1, x2) ≥ f1(ρ1x1 + 1, ρ2x2 + 1) ρ1f1(x1, x2)− b̃(x1, x2) ≥ f1(ρ1x1 − 1, ρ2x2 − 1)\nThis is satisfied if and only if\nρ1f1(x1, x2)− (1/2)(f1(ρ1x1 + 1, ρ2x2 + 1) + f1(ρ1x1 − 1, ρ2x2 − 1)) ≥\n|(1/2)(f1(ρ1x1 + 1, ρ2x2 + 1)− f1(ρ1x1 − 1, ρ2x2 − 1))− b̃(x1, x2)|\nTo see this, note that if b̃(x1, x2) = (1/2)(f1(ρ1x1 + 1, ρ2x2 + 1) − f1(ρ1x1 − 1, ρ2x2 − 1)) then the two inequalities become identical. Otherwise we can denote the difference by ∆ and we get that the left hand side has to be ≥ ±∆.\nSimilarly we get ρ2f2(x1, x2)− (1/2)(f1(ρ1x1 + 1, ρ2x2 + 1) + f1(ρ1x1 − 1, ρ2x2 − 1)) ≥ |(1/2)(f2(ρ1x1 + 1, ρ2x2 + 1)− f2(ρ1x1 − 1, ρ2x2 − 1))− b̃(x1, x2)|\nWe can write these as L1 ≥ |R1 − b̃| and L2 ≥ |R2 − b̃|. Note that for such a b̃ to exist it is necessary and sufficient that L1 + L2 ≥ |R1 − R2| and L1 ≥ 0 and\nL2 ≥ 0. Now rescaling into functions g1 and g2 we get\nL1 = ρ1f1(x1, x2)− (1/2)(f1(ρ1x1 + 1, ρ2x2 + 1) + f1(ρ1x1 − 1, ρ2x2 − 1)) = (1− a21δ2)g1(x1, x2)− 12 (g1((1− a 2 1δ\n2)x1 + δ, (1− b2δ2)x2 + δ) + g1((1− a2δ2)x1 − δ, (1− b2δ2)x2 − δ)) = −a21δ2g1(x1, x2) + δ2(a21 ∂∂x1 + a 2 2 ∂ ∂x2 )g1 + (1/2)((−a21δ2 + δ) ∂∂x1 + ((−a 2 2δ 2 + δ) ∂∂x2 ) 2\n+(1/2)((−a21δ2 − δ) ∂∂x1 + ((−a 2 2δ 2 − δ) ∂∂x2 ) 2)\nDividing by δ2 and taking limit as δ → 0 we get −a21g1(x1, x2) + (a21x1 ∂∂x1 + a 2 2x2 ∂ ∂x2 )g1 − (1/2)( ∂∂x1 + ∂ ∂x2 )2g1.\nThus we have E1 = −a21g1(x1, x2) + (a21x1 ∂∂x1 + a 2 2x2 ∂ ∂x2 )g1 − (1/2)( ∂∂x1 + ∂ ∂x2 )2g1 ≥ 0 and E2 = −a22g2(x1, x2) + (a21x1 ∂∂x1 + a 2 2x2 ∂ ∂x2 )g2 − (1/2)( ∂∂x1 + ∂ ∂x2\n)2g2 ≥ 0. Now R1 = (1/2)(f1(ρ1x1 + 1, ρ2x2 + 1)− f1(ρ1x1 − 1, ρ2x2 − 1)) After scaling this becomes in the limit.\n= (1/2)(g1((1− a21δ2)x1 + δ, (1− b2δ2)x2 + δ)− g1((1− a2δ2)x1 − δ, (1− b2δ2)x2 − δ)) = δ2( ∂∂x1 + ∂ ∂x2 )g1.\nDividing by δ2 we get: E1 + E2 ≥ |( ∂∂x1 + ∂ ∂x2 )(g1 − g2)|."
    } ],
    "references" : [ {
      "title" : "Continuous experts and the binning algorithm",
      "author" : [ "J. Abernethy", "J. Langford", "M. Warmuth" ],
      "venue" : "Learning Theory pp",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Optimal strategies from random walks",
      "author" : [ "J. Abernethy", "M. Warmuth", "J. Yellin" ],
      "venue" : "Proceedings of The 21st Annual Conference on Learning Theory",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Algorithms for portfolio management based on the newton method",
      "author" : [ "A. Agarwal", "E. Hazan", "S. Kale", "R. Schapire" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Minimax policies for adversarial and stochastic bandits",
      "author" : [ "J.Y. Audibert", "S. Bubeck" ],
      "venue" : "COLT",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "The nonstochastic multi-armed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire" ],
      "venue" : "SIAM J. Comput",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Estimating the fractal dimension of the S&P500 index using wavelet analysis. International joirnal of theoretical and applied finance",
      "author" : [ "E. Bayraktar", "H. Poor", "K. Sircar" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Empirical support for winnow and weighted-majority algorithms: Results on a calendar scheduling domain",
      "author" : [ "A. Blum" ],
      "venue" : "Machine Learning 26(1),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "From external to internal regret",
      "author" : [ "A. Blum", "Y. Mansour" ],
      "venue" : "Journal of Machine Learning Research pp",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "How to use expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D. Helmbold", "R. Schapire", "M. Warmuth" ],
      "venue" : "Journal of the ACM (JACM) 44(3),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Prediction, Learning and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "A parameter free hedging algorithm",
      "author" : [ "K. Chaudhuri", "Y. Freund", "D. Hsu" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Behaviour of sequential predictors of binary sequences. Transactions of the Fourth Prague Conference on Information Theory, Statistical Decision Functions, Random Processes",
      "author" : [ "T. Cover" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1965
    }, {
      "title" : "Universal portfolios",
      "author" : [ "T. Cover" ],
      "venue" : "Mathematical Finance",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1991
    }, {
      "title" : "Regret to the best vs. regret to the average",
      "author" : [ "E. Even-Dar", "M. Kearns", "Y. Mansour", "J. Wortman" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Predicting a binary sequence almost as well as the optimal biased coin",
      "author" : [ "Y. Freund" ],
      "venue" : "COLT",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1996
    }, {
      "title" : "Using and combining predictors that specialize",
      "author" : [ "Y. Freund", "R.E. Schapire", "Y. Singer", "M.K. Warmuth" ],
      "venue" : "STOC pp",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1997
    }, {
      "title" : "Multi-armed Bandit Allocation Indices",
      "author" : [ "J.C. Gittins" ],
      "venue" : "John Wiley",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1989
    }, {
      "title" : "Efficient learning algorithms for changing environments",
      "author" : [ "E. Hazan", "C. Seshadhri" ],
      "venue" : "ICML pp",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "On-line portfolio selection using multiplicative updates",
      "author" : [ "D. Helmbold", "R. Schapire", "Y. Singer", "M. Warmuth" ],
      "venue" : "Mathematical Finance 8(4),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1998
    }, {
      "title" : "A deterministic-control-based approach to fully nonlinear parabolic and elliptic equations",
      "author" : [ "R. Kohn", "S. Serfaty" ],
      "venue" : "Comm. Pure Appl. Math. 63(10),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "The weighted majority",
      "author" : [ "N. Littlestone", "M. Warmuth" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1989
    }, {
      "title" : "Learning with continuous experts using drifting games",
      "author" : [ "I. Mukherjee", "R. Schapire" ],
      "venue" : "Algorithmic Learning Theory",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Normal approximation with Steins method",
      "author" : [ "M. Raič" ],
      "venue" : "Proceedings of the Seventh Young Statisticians Meeting",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2003
    }, {
      "title" : "Online learning: Beyond regret",
      "author" : [ "A. Rakhlin", "K. Sridharan", "A. Tewari" ],
      "venue" : "COLT",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Fractional brownian motion, random walks and binary market models",
      "author" : [ "T. Sottinen" ],
      "venue" : "Finance and Stochastics 5(3),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2001
    }, {
      "title" : "A short proof of the gittins index theorem",
      "author" : [ "J. Tsitsiklis" ],
      "venue" : "Annals of Applied Probability",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1994
    }, {
      "title" : "A game of prediction with expert advice",
      "author" : [ "V. Vovk" ],
      "venue" : "Journal of Computer and System Sciences",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "The question turns out to be also equivalent to the problem of optimal trade-offs between the regrets of two experts in an “experts problem”, studied before by [14].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 11,
      "context" : "Regret has been studied in a number of papers, including [12, 22, 13, 5, 4].",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "Regret has been studied in a number of papers, including [12, 22, 13, 5, 4].",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "Regret has been studied in a number of papers, including [12, 22, 13, 5, 4].",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "Regret has been studied in a number of papers, including [12, 22, 13, 5, 4].",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Regret has been studied in a number of papers, including [12, 22, 13, 5, 4].",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "A classical result says that one can obtain a regret of Θ( √ T ) for a sequence of length T , via, say, the weighted majority algorithm of [22].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "the payoff per time step btb̃t is essentially equivalent to the well known absolute loss function |bt − b̃t| (see for example [10], chapter 8).",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "The latter problem has been previously studied by [14], and later by [20], to address, say, an investment scenario where there may be two experts one risk taking and another conservative and one may be willing to take different regrets with respect to these two experts.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "For example, in several investment scenarios it is known that the payoffs of the experts (or stocks) in time T is barely more than O( √ T ) (see, for example, the Hurst coefficient measurements of financial markets in [6, 26]).",
      "startOffset" : 218,
      "endOffset" : 225
    }, {
      "referenceID" : 24,
      "context" : "For example, in several investment scenarios it is known that the payoffs of the experts (or stocks) in time T is barely more than O( √ T ) (see, for example, the Hurst coefficient measurements of financial markets in [6, 26]).",
      "startOffset" : 218,
      "endOffset" : 225
    }, {
      "referenceID" : 11,
      "context" : "We note that, in the vanilla setting, when there is a time bound T , the solution already follows from the results of [12] (see also [9, 10]), who gave a characterization of all possible payoffs back in 1965.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "We note that, in the vanilla setting, when there is a time bound T , the solution already follows from the results of [12] (see also [9, 10]), who gave a characterization of all possible payoffs back in 1965.",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 9,
      "context" : "We note that, in the vanilla setting, when there is a time bound T , the solution already follows from the results of [12] (see also [9, 10]), who gave a characterization of all possible payoffs back in 1965.",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 19,
      "context" : "One can also obtain the optimal algorithm by computing a certain dynamic programming, similar to an approach from [21].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "To understand the time-independent strategies, we are led to consider the another classic setting of time-discounted payoffs (see [17, 27]).",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 25,
      "context" : "To understand the time-independent strategies, we are led to consider the another classic setting of time-discounted payoffs (see [17, 27]).",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "For starters, we remind the result for the vanilla, non-discounted, fixed stopping time setting, which follows from [12], and is related to Rademacher complexity of the predictions of the two experts (see [9, 10]).",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "For starters, we remind the result for the vanilla, non-discounted, fixed stopping time setting, which follows from [12], and is related to Rademacher complexity of the predictions of the two experts (see [9, 10]).",
      "startOffset" : 205,
      "endOffset" : 212
    }, {
      "referenceID" : 9,
      "context" : "For starters, we remind the result for the vanilla, non-discounted, fixed stopping time setting, which follows from [12], and is related to Rademacher complexity of the predictions of the two experts (see [9, 10]).",
      "startOffset" : 205,
      "endOffset" : 212
    }, {
      "referenceID" : 19,
      "context" : "Such an approach has been previously undertaken by [21] to show that many differential equations can be realized as two-person games, as is also the case in our scenario.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Here, in each round, each expert has a payoff in the range [0, 1] that is unknown to the algorithm.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "The algorithm pulls each arm (expert) with probability b̃1,t, b̃2,t ∈ [0, 1] respectively where b̃1,t + b̃2,t = 1.",
      "startOffset" : 70,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "We note that this was first studied in [14].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "Several earlier papers considered regrets at different time scales; see [8, 16, 18, 29, 20].",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Several earlier papers considered regrets at different time scales; see [8, 16, 18, 29, 20].",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Several earlier papers considered regrets at different time scales; see [8, 16, 18, 29, 20].",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "Numerous works including [12, 9] have examined the optimal amount of regret achievable with respect to multiple experts.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "Numerous works including [12, 9] have examined the optimal amount of regret achievable with respect to multiple experts.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "Many of the results in this body of work can be found in [10].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "It is well known that in the case of static experts, the optimal regret is exactly equal to the Rademacher complexity of the predictions of the experts (chapter 8 in [10]).",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "Recent works, including [1, 2, 23], have extended this analysis to other settings.",
      "startOffset" : 24,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "Recent works, including [1, 2, 23], have extended this analysis to other settings.",
      "startOffset" : 24,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : "Recent works, including [1, 2, 23], have extended this analysis to other settings.",
      "startOffset" : 24,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : "Measures other than the standard regret measure have been studied in [25].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "Also related is the NormalHedge algorithm [11], though it differs in both the setting and the precise algorithm.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 7,
      "context" : "Algorithms with performance guarantees within each interval have been studied in [8, 16, 29] and, more recently, in [18, 20].",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "Algorithms with performance guarantees within each interval have been studied in [8, 16, 29] and, more recently, in [18, 20].",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "Algorithms with performance guarantees within each interval have been studied in [8, 16, 29] and, more recently, in [18, 20].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "The question of what can be achieved if one would like to have a significantly better guarantee with respect to a fixed arm or a distribution on arms was asked before in [14, 20].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 26,
      "context" : "Tradeoffs between regret and loss were also examined in [28], where the author studied the set of values of a, b for which an algorithm can have payoff aOPT + b logN , where OPT is the payoff of the best arm and a, b are constants.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "The problem of bit prediction was also considered in [15], where several loss functions are considered.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "Numerous papers ([7, 19, 3]) have implemented algorithms inspired from regret syle analysis and applied it on financial and other types of data.",
      "startOffset" : 17,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "Numerous papers ([7, 19, 3]) have implemented algorithms inspired from regret syle analysis and applied it on financial and other types of data.",
      "startOffset" : 17,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "Numerous papers ([7, 19, 3]) have implemented algorithms inspired from regret syle analysis and applied it on financial and other types of data.",
      "startOffset" : 17,
      "endOffset" : 27
    } ],
    "year" : 2013,
    "abstractText" : "We consider the classical question of predicting binary sequences and study the optimal algorithms for obtaining the best possible regret and payoff functions for this problem. The question turns out to be also equivalent to the problem of optimal trade-offs between the regrets of two experts in an “experts problem”, studied before by [14]. While, say, a regret of Θ( √ T ) is known, we argue that it important to ask what is the provably optimal algorithm for this problem — both because it leads to natural algorithms, as well as because regret is in fact often comparable in magnitude to the final payoffs and hence is a non-negligible term. In the basic setting, the result essentially follows from a classical result of Cover from ’65. Here instead, we focus on another standard setting, of time-discounted payoffs, where the final “stopping time” is not specified. We exhibit an explicit characterization of the optimal regret for this setting. To obtain our main result, we show that the optimal payoff functions have to satisfy the Hermite differential equation, and hence are given by the solutions to this equation. It turns out that characterization of the payoff function is qualitatively different from the classical (non-discounted) setting, and, namely, there’s essentially a unique optimal solution.",
    "creator" : "LaTeX with hyperref package"
  }
}