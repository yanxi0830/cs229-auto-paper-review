{
  "name" : "1605.09346.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs",
    "authors" : [ "Anton Osokin", "Jean-Baptiste Alayrac", "Isabella Lukasewitz", "Puneet K. Dokania", "Simon Lacoste-Julien" ],
    "emails" : [ "FIRST.LASTNAME@INRIA.FR" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "One of the most popular learning objectives for structured prediction is the structured support vector machine (Taskar et al., 2003; Tsochantaridis et al., 2005), which generalizes the classical binary SVM to problems with structured outputs. In this paper, we consider the `2-regularized `1-slack structured SVM, to which we will simply refer as SSVM. The SSVM method consists in the minimization of the regularized structured hinge-loss on the labeled training set. The optimization problem of SSVM is of significant complexity and, thus, hard to scale up. In the literature, multiple optimization methods have been applied to tackle this problem, including cutting-plane methods (Tsochantaridis et al., 2005; Joachims et al., 2009) and stochastic subgradient methods (Ratliff et al., 2007), among others.\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nRecently, Lacoste-Julien et al. (2013) proposed the blockcoordinate Frank-Wolfe method (BCFW), which is currently one of the state-of-the-art algorithms for SSVM.1 In contrast to the classical (batch) Frank-Wolfe algorithm (Frank & Wolfe, 1956), BCFW is a randomized block-coordinate method that works on block-separable convex compact domains. In the case of SSVM, BCFW operates in the dual domain and iteratively applies FrankWolfe steps on the blocks of dual variables corresponding to different objects of the training set. Distinctive features of BCFW for SSVM include optimal step size selection leading to the absence of the step-size parameter, convergence guarantees for the primal objective, and ability to compute the duality gap as a stopping criterion.\nNotably, the duality gap obtained by BCFW can be written as a sum of block gaps, where each block of dual variables corresponds to one training example. In this paper, we exploit this property and improve the BCFW algorithm in multiple ways. First, we substitute the standard uniform sampling of objects at each iteration with an adaptive nonuniform sampling. Our procedure consists in sampling objects with probabilities proportional to the values of their block gaps, giving one of the first fully adaptive sampling approaches in the optimization literature that we are aware of. This choice of sampling probabilities is motivated by the intuition that objects with higher block gaps potentially can provide more improvement to the objective. We analyze the effects of the gap-based sampling on convergence and discuss the practical trade-offs.\nSecond, we apply pairwise (Mitchell et al., 1974) and away (Wolfe, 1970) steps of Frank-Wolfe to the blockcoordinate setting. This modification is motivated by the fact that batch algorithms based on these steps have linear convergence rates (Lacoste-Julien & Jaggi, 2015) whereas convergence of standard Frank-Wolfe is sublinear.\nThird, we cache oracle calls and propose a gap-based criterion for calling the oracle (cache miss vs. cache hit). Caching the oracle calls was shown do deliver significant speed-ups when the oracle is expensive, e.g., in the case of\n1Independently, Branson et al. (2013) proposed their SVM-IS algorithm which is equivalent to BCFW in some scenarios.\nar X\niv :1\n60 5.\n09 34\n6v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\n01 6\ncutting-plane methods (Joachims et al., 2009).\nFinally, we propose an algorithm to approximate the regularization path of SSVM, i.e., solve the problem for all possible values of the regularization parameter. Our method exploits block gaps to construct the breakpoints of the path and leads to an ε-approximate path.\nContributions. Overall, we make the following contributions: (i) adaptive non-uniform sampling of the training objects; (ii) pairwise and away steps in the blockcoordinate setting; (iii) gap-based criterion for caching the oracle calls; (iv) regularization path for SSVM. The first three contributions are general to BCFW and thus could be applied to other block-separable optimization problems where BCFW could or have been used such as video colocalization (Joulin et al., 2014), multiple sequence alignment (Alayrac et al., 2016, App. B) or structured submodular optimization (Jegelka et al., 2013), among others.\nThis paper is organized as follows. In Section 2, we describe the setup and review the BCFW algorithm. In Section 3, we describe our contributions: adaptive sampling (Section 3.1), pairwise and away steps (Section 3.2), caching (Section 3.3). In Section 4, we explain our algorithm to compute the regularization path. We discuss the related work in the relevant sections of the paper. Section 5 contains the experimental study of the methods. The code and datasets are available at our project webpage.2"
    }, {
      "heading" : "2. Background",
      "text" : ""
    }, {
      "heading" : "2.1. Structured Support Vector Machine (SSVM)",
      "text" : "In structured prediction, we are given an input x ∈ X , and the goal is to predict a structured object y ∈ Y(x) (such as a sequence of tags). In the standard setup for structured SVM (SSVM) (Taskar et al., 2003; Tsochantaridis et al., 2005), we assume that prediction is performed with a linear model hw(x) = argmaxy∈Y(x)〈w,φ(x,y)〉 parameterized by the weight vectorw where the structured feature map φ(x,y) ∈ Rd encodes the relevant information for input/output pairs. We reuse below the notation and setup from Lacoste-Julien et al. (2013). Given a labeled training set D = {(xi,yi)}ni=1, the parameters w are estimated by solving a convex non-smooth optimization problem\nmin w\nλ 2 ‖w‖ 2 + 1n n∑ i=1 H̃i(w) (1)\nwhere λ is the regularization parameter and H̃i(w) is the structured hinge loss defined using the loss-augmented decoding subproblem (or maximization oracle):\n‘max oracle’ H̃i(w) := max y∈Yi Li(y)− 〈w,ψi(y)〉︸ ︷︷ ︸ =:Hi(y;w) . (2)\nHere ψi(y) := φ(xi,yi) − φ(xi,y), Yi := Y(xi), and Li(y) := L(yi,y) denotes the task-dependent structured\n2 http://www.di.ens.fr/sierra/research/gapBCFW/\nAlgorithm 1 Block-Coordinate Frank-Wolfe (BCFW) algorithm for structured SVM\n1: Let w(0) :=wi(0) :=0; `(0) :=`i(0) :=0 2: for k := 0, . . . ,∞ dos 3: Pick i at random in {1, . . . , n} 4: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(k))\n5: Let ws := 1λnψi(y ∗ i ) and `s := 1 nLi(y ∗ i ) 6: Let g(k)i := λ(w (k) i −ws)Tw(k) − ` (k) i + `s 7: Let γ := g (k) i\nλ‖w(k)i −ws‖ 2\nand clip to [0, 1]\n8: Update wi(k+1) := (1− γ)wi(k) + γws 9: and `i(k+1) := (1− γ)`i(k) + γ `s\n10: Update w(k+1) := w(k) +wi(k+1) −wi(k) 11: and `(k+1) := `(k) + `i(k+1) − `i(k) 12: end for\nerror of predicting an output y instead of the observed output yi (e.g., a Hamming distance between two sequences).\nDual formulation. The negative of a Fenchel dual for objective (1) can be written as\nmin α∈Rm α<0\nf(α) := λ2 ∥∥Aα∥∥2 − bTα (3)\ns.t. ∑ y∈Yi αi(y) = 1 ∀i ∈ [n]\nwhere αi(y), i ∈ [n] := {1, . . . , n}, y ∈ Yi are the dual variables. The matrix A ∈ Rd×m consists of the m := ∑ imi = ∑ i |Yi| columns A :={\n1 λnψi(y) ∈ R d ∣∣ i ∈ [n],y ∈ Yi}, and the vector b ∈\nRm is given by b := (\n1 nLi(y) ) i∈[n],y∈Yi .\nIn SSVM (as for the standard SVM), the Karush-KuhnTucker (KKT) optimality conditions can give the primal variables w(α) = Aα = ∑ i,y∈Yi αi(y) ψi(y) λn corresponding to the dual variables α (see, e.g., (Lacoste-Julien et al., 2013, App. E)). The gradient of f then takes a simple form ∇f(α) = λATAα − b = λATw − b; its (i,y)-th component equals − 1nHi(y;w)."
    }, {
      "heading" : "2.2. Block Coordinate Frank-Wolfe method (BCFW)",
      "text" : "We give in Alg. 1 the BCFW algorithm from Lacoste-Julien et al. (2013) applied to problem (3). It exploits the blockseparability of the domainM := ∆|Y1| × . . . ×∆|Yn| for problem (3) and sequentially applies the Frank-Wolfe steps to the blocks of the dual variables α(i) ∈M(i) := ∆|Yi|. While BCFW works on the dual (3) of SSVM, it only maintains explicitly the primal variables via the relationship w(α). Most importantly, the Frank-Wolfe linear oracle on block i at iterate α(k) is equivalent to the max oracle (2) at the corresponding weight vector w(k) := Aα(k) (Lacoste-Julien et al., 2013, App. B.1) (see line 4 of Alg. 1):\nmax s(i)∈M(i)\n〈 s(i),−∇(i)f(α(k)) 〉 = 1n maxy∈Yi Hi(y;w (k)). (4)\nHere, the operator ∇(i) denotes the partial gradient corresponding to the block i, i.e., ∇f = (∇(i)f)ni=1. Note that each argmax of the r.h.s. of (4), y∗(i), corresponds to a corner s∗(i) of the polytopeM (i) maximizing the l.h.s. of (4).\nAs the objective (3) is quadratic, the optimal step size that yields the maximal improvement in the chosen direction s∗(i) −α (k) (i) can be found analytically (Line 7 of Alg. 1)."
    }, {
      "heading" : "2.3. Duality gap",
      "text" : "At each iteration, the batch Frank-Wolfe algorithm (Frank & Wolfe, 1956), (Lacoste-Julien et al., 2013, Section 3) computes the following quantity, known as the linearization duality gap or Frank-Wolfe gap:\ng(α) := max s∈M\n〈α− s,∇f(α)〉 = 〈α− s∗,∇f(α)〉. (5)\nIt turns out that this Frank-Wolfe gap exactly equals the Lagrange duality gap between the dual objective (3) at a point α and the primal objective (1) at the point w(α) = Aα (Lacoste-Julien et al., 2013, App. B.2).\nBecause of the separability ofM, the Frank-Wolfe gap (5) can be represented here as a sum of block gaps gi(α), g(α) = ∑n i=1 gi(α), where\ngi(α) := max s(i)∈M(i)\n〈 α(i) − s(i),∇(i)f(α) 〉 . (6)\nBlock gaps can be easily computed using the quantities maintained by Alg. 1 (see line 6).\nFinally, we can rewrite the block gap in the form\ngi(α)= 1 n ( max y∈Yi Hi(y;w)− ∑ y∈Yi αi(y)Hi(y;w) ) (7)\nproviding understandable intuition of when the block gap equals zero. This is the case when all the support vectors, i.e., labelings corresponding to αi(y) > 0, are tied solutions of the max oracle (4)."
    }, {
      "heading" : "2.4. Convergence of BCFW",
      "text" : "Lacoste-Julien et al. (2013) prove the convergence of the BCFW algorithm at a rate O( 1k ). Theorem 1 (Lacoste-Julien et al. (2013), Theorem 2). For each k ≥ 0, the iterate3 α(k) of Alg. 1 satisfies IE [ f(α(k)) ] − f(α∗) ≤ 2nk+2n ( C⊗f +h0 ) , where α∗ ∈M is a solution of the problem (3), h0 := f(α(0)) − f(α∗) is the suboptimality at the starting point of the algorithm, C⊗f := ∑n i=1 C (i) f is the sum of the curvature constants\n4 of f with respect to the domains M(i) of individual blocks. The expectation is taken over the random choice of the block i at iterations 1, . . . , k of the algorithm.\n3Note that Alg. 1 does not maintain iterates α(k) explicitly. They are stored in the form ofw(k) = Aα(k).\n4For the definition of curvature constant, see Definition 2 in App. B or (Lacoste-Julien & Jaggi, 2015, App. A)\nThe proof of Theorem 1 crucially depends on a standard descent lemma applied to a block, stating that at each iteration of BCFW, for any picked block i and any scalar γ ∈ [0, 1], the following inequality holds:\nf(α(k+1)) ≤ f(α(k))− γgi(α(k)) + γ 2 2 C (i) f . (8)\nWe rederive inequality (8) as Lemma 3 in App. B. Note that α(k+1) ∈M is defined by a line search, which is why the bound (8) holds for any scalar γ ∈ [0, 1]. Taking the expectation of (8) w.r.t. the random choice of block i (sampled uniformly on [n]), we get the inequality\nIE [ f(α(k+1)) |α(k) ] ≤ f(α(k))− γng(α (k)) + γ 2 2nC ⊗ f (9)\nwhich can be used to get the convergence theorem."
    }, {
      "heading" : "3. Block gaps in BCFW",
      "text" : "In this section, we propose three ways to improve the BCFW algorithm: adaptive sampling (Sec. 3.1), pairwise and away steps (Sec. 3.2) and caching (Sec. 3.3)."
    }, {
      "heading" : "3.1. Adaptive non-uniform sampling",
      "text" : "Motivation. When optimizing finite sums such as (1), it is often the case that processing some summands does not lead to significant progress of the algorithm. At each iteration, the BCFW algorithm selects a training object and performs the block-coordinate step w.r.t. the corresponding dual variables. If these variables are already close to being optimal, then BCFW does not make significant progress at this iteration. Usually, it is hard to identify whether processing the summand would lead to an improvement without actually doing computations on it. The BCFW algorithm obtains at each iteration the block gap (6) quantifying the suboptimality on the block. In what follows, we use the block gaps to randomly choose a block (an object of the training set) at each iteration in such a way that the blocks with larger suboptimality are sampled more often (the sampling probability of a block is proportional to the value of the current gap estimate).\nConvergence. Assume that at iteration k of Alg. 1, we have the probability p(k)i of sampling block i. By minimizing the descent lemma bound (8) w.r.t. γ for each i independently under the assumption that gi(α(k)) ≤ C(i)f , and then taking the conditional expectation w.r.t. i, we get\nIE [ f(α(k+1)) |α(k) ] ≤ f(α(k))− 12 n∑ i=1 p (k) i g2i (α (k)) C (i) f . (10)\nIntuitively, by adapting the probabilities p(k)i , we can obtain a better bound on the expected improvement of f . In the ideal scenario, one would choose deterministically the block i with the maximal value of g2i (α (k))/C (i) f .\nIn practice, the curvature C(i)f is unknown, and having access to all gi(α(k))’s at each step is prohibitively expensive. However, the values of the block gaps obtained at the previous iterations can serve as estimates of the block gaps at the current iteration. We use them in the following nonuniform gap sampling scheme: p(k)i ∝ gi(α(ki)). where ki records the last iteration at which the gap i was computed. Alg. 2 in App. D summarizes the method.\nWe also motivate this choice by Theorem 2 below which shows that BCFW with (exact) gap sampling converges with a better constant in the rate than BCFW with uniform sampling when the gaps are non-uniform enough (and is always better when the curvatures C(i)f ’s are uniform). See the proof and discussion in App. E.\nTheorem 2. Consider the same notation as in Theorem 1. Assume that at each iterate α(k), BCFW with gap sampling (Alg. 2) has access to the exact values of the block gaps. Then, at each iteration, it holds that IE [ f(α(k)) ] −\nf(α∗) ≤ 2nk+2n ( C⊗f χ ⊗ + h0 ) where the constant χ⊗ is\nan upper bound on IE [ χ(C (:) f )\nχ(g:(α(k)))3\n] . The non-uniformity\nmeasure χ(x) of a vector x ∈ Rn+ is defined as χ(x) :=√ 1 + n2 Var [ p ]\nwhere p := x‖x‖1 is the probability vector obtained by normalizing x.\nAdaptive procedure. Note that this procedure is adaptive, meaning that the criterion for choosing an object to optimize changes during the optimization process. Our adaptive approach differs from more standard techniques that sample proportional to the Lipschitz constants, as e.g., in Nesterov (2012). In App. C, we illustrate the advantage of this property by constructing an example where the convergence of gap sampling can be shown tightly to be n times faster than when using Lipschitz sampling.\nExploitation versus staleness trade-off. In practice, having access to the exact block gaps is intractable because it requires a full pass over the dataset after every block update. However, we have access to the estimates of the block gaps computed from past oracle calls on each block. No-\ntice that such estimates are outdated, i.e., might be quite far from the current values of the block gaps. We call this effect “staleness”. One way to compensate staleness is to refresh the block gaps by doing a full gap computation (a pass over the dataset) after several block-coordinate passes. These gap computations were often already done during the optimization process, e.g., to monitor convergence.\nWe demonstrate the exploitation/staleness trade-off in our exploratory experiment reported in Figure 1. On the OCR dataset (Taskar et al., 2003), we run the gap sampling algorithm with a gap computation pass after 1, 5, 10 and 100 block-coordinate passes (Gap 1, Gap 5, Gap 10, Gap 100) and without any gap computation passes (Gap Inf). As a baseline, we use BCFW with uniform sampling (Uniform). Figure 1a reports the duality gap after each number of effective passes over the data.5 Figure 1b shows the ratio of the exact value of the duality gap to the heuristic gap estimate defined as the sum of the current gap estimates. We observe that when the gap computation is never run, the gap becomes significantly underestimated and the algorithm does not converge. On another extreme, when performing the gap computation after each pass of BCFW, the algorithm wastes too many computations and converges slowly. Between the two extremes, the method is not very sensitive to the parameter (we have tried 5, 10, 20, 50) allowing us to always use the value of 10.\nComparing adaptive methods to BCFW with uniform sampling, we observe a faster convergence. Figure 1c reports the improvement of gap sampling at each iteration w.r.t. uniform sampling that is predicted by Theorem 2. Specifically, we report the quantity χ(g:(α(k)))3/χ(C(:)f ) with the block gaps estimated at the runs of BCFW with both uniform and gap sampling schemes. To estimate the curvature constants C(i)f , we use the upper bounds proposed by Lacoste-Julien et al. (2013, App. A): 4R 2 i\nλn2 where Ri := maxy∈Yi ‖ψi(y)‖2. We approximate Ri by picking the largest value ‖ψi(y)‖2 corresponding to a labeling y observed within the run of BCFW.\n5An effective pass consists in n calls to the max oracle.\nRelated work. Non-uniform sampling schemes have been used over the last few years to improve the convergence rates of well known randomized algorithms (Nesterov, 2012; Needell et al., 2014; Zhao & Zhang, 2015). Most of these approaches use the Lipschitz constants of the gradients to sample more often functions for which gradient changes quickly. This approach has two main drawbacks. First, Lipschitz constants are often unknown and heuristics are needed to estimate them. Second, such schemes are not adaptive to the current progress of the algorithm. To the best of our knowledge, the only other approach that uses an adaptive sampling scheme to guide the optimization with convergence guarantees is the one from Csiba et al. (2015), in the context of the stochastic dual coordinate ascent (SDCA) algorithm. A cyclic version of BCFW has been analyzed by Beck et al. (2015) while Wang et al. (2014) analyzed its mini-batch form."
    }, {
      "heading" : "3.2. Pairwise and away steps",
      "text" : "Motivation. In the batch setting, the convergence rate of the Frank-Wolfe algorithm is known to be sublinear when the solution is on the boundary (Wolfe, 1970), as is the case for SSVM. Several modifications have been proposed in the literature to address this issue. All these methods replace (or complement) the FW step with a step of another type: pairwise step (Mitchell et al., 1974), away step (Wolfe, 1970), fully-corrective step (Holloway, 1974) (see Lacoste-Julien & Jaggi (2015) for a recent review and the proof that all these methods have a linear rate on the objective (3) despite not being strongly convex). A common feature of these methods is the ability to remove elements of the active set (support vectors in the case of SSVM) in order to reach the boundary, unlike FW which oscillates while never completely reaching the boundary. As we expect the solution of SSVM to be sparse, these variants seem natural in our setting. In the rest of this section, we present the pairwise steps in the block-coordinate setting (the away-step version is described in Alg. 4 of App. 4).\nPairwise steps. A (block) pairwise step consists in removing mass from the away corner on block i and transferring it to the FW corner obtained by the max oracle (4). The away corner is the element of the active set Si := {y ∈ Yi | αi(y) > 0} ⊆ Yi worst aligned with the current descent direction, which can be found by solving yai := argminy∈Si Hi(y;w). This does not require solving a combinatorial optimization problem because the size of the active set is typically small, e.g., bounded by the number of iterations performed on the block i. Analogously to the case of BCFW, the optimal step size γ for the pairwise step can be computed explicitly by clipping λ(wa−ws)\nTw(k)+`s−`a λ‖wa−ws‖2 to the seg-\nment [0, α(k)i (y a i )] where the upper bound α (k) i (y a i ) corresponds to the mass of the away corner before the step and the quantities ws := 1λnψi(y ∗ i ), `s := 1 nLi(y ∗ i ) and\nwa := 1 λnψi(y a i ), `a := 1 nLi(y a i ) represent the FW and away corners. Alg. 3 in App. D summarizes the blockcoordinate pairwise Frank-Wolfe (BCPFW) algorithm.\nIn contrast to BCFW, the steps of BCPFW cannot be expressed in terms of the primal variables w only, thus it is required to explicitly store the dual variables αi. Storing the dual variables is feasible, because they are extremely sparse, but still can lead to computational overheads caused by the maintenance of the data structure.\nThe standard convergence analysis for pairwise and awaystep FW cannot be easily extended to BCFW. We show the geometric decrease of the objective in Theorem 4 of App. G only when no block would have a drop step (a.k.a. ‘bad step’); a condition that cannot be easily analyzed due to the randomization of the algorithm. We believe that novel proof techniques are required here, even though we did observe empirically a linear convergence rate when λ is big enough.\nRelated work. Ñanculef et al. (2014, Alg. 4) used the pairwise FW algorithm on the dual of binary SVM (in batch mode, however). It is related to classical working set algorithms, such as the SMO algorithm used to train SVMs (Platt, 1999), also already applied on SSVMs in Taskar (2004, Ch. 6). Franc (2014) recently proposed a version of pairwise FW for the block-coordinate setting. Their SDA-WSS2 algorithm uses a different criterion for choosing the away corner than BCPFW: instead of minimizing Hi over the active set Si, they compute the improvement for all possible away corners and pick the best one. Their FASOLE algorithm also contains a version of gap sampling in the form of variable shrinking: if a block gap becomes small enough, the block is not visited again, until all the counters are reset."
    }, {
      "heading" : "3.3. Caching",
      "text" : "Motivation. At each step, the BCFW and BCPFW algorithms call the max oracle to find the Frank-Wolfe corner. In cases where the max oracle is expensive, this step becomes a computational bottleneck. A natural idea to overcome this problem consists in using a “cheaper oracle” most of the time hoping that the resulting corner would be good enough. Caching the results of the max oracle implements this idea by reusing the previous calls of the max oracle to store potentially promising corners.\nCaching. The main principle of caching consists in maintaining a working set Ci ⊂ Yi of labelings/corners for each block i, where |Ci| |Yi|. A cache oracle obtains the cache corner defined as a corner from the working set best aligned with the descent direction, i.e., yci := argmaxy∈Ci Hi(y;w). If the obtained cache corner passes a cache hit criterion, i.e., there is a cache hit, we do a Frank-Wolfe (or pairwise) step based on the cache corner. A step defined this way is equivalent to the corresponding\nstep on the convex hull of the working set, which is a subset of the block domain Yi. If a cache hit criterion is not satisfied, i.e., there is a cache miss, we call the (possibly expensive) max oracle to obtain a Frank-Wolfe corner over the full domain Yi. Alg. 5 in App. D summarizes the BCFW method with caching.\nNote that, in the case of BCPFW, the working set Ci is closely related to the active set Si. On the implementation side, we maintain both sets in the same data structure and keep Si ⊆ Ci.\nCache hit criterion. An important part of a caching scheme is the criterion deciding whether the cache look up is sufficient or the max oracle needs to be called. Intuitively, we want to use the cache whenever it allows optimization to make large enough progress. We use as measure of potential progress the inner product between the candidate direction and the negative gradient (which would give the block gap gi (6) if the FW corner is used). For a cache step, it gives ĝ(k)i := λ(w (k) i −wc)Tw(k)−`i\n(k)+`c, which is defined by quantitieswc = ψi(y c i ) λn , `c = 1 nLi(y c i ) similar to the ones defining the block gap. The quantity ĝ(k)i is then compared to a cache hit threshold defined as max(Fg(ki)i , ν ng\n(k0)) where ki identifies the iteration when the max oracle was last called for the block i, k0 is the index of the iteration when the full batch gap was computed, F > 0 and ν > 0 are cache parameters.\nThe following theorem gives a safety convergence result for BCFW with caching (see App. F for the proof). Theorem 3. Consider the same notation as in Theorem 1. Let ν̃ := 1nν ≤ 1. The iterate α\n(k) of Alg. 5 satisfies IE [ f(α(k)) ] − f(α∗) ≤ 2nν̃k+2n ( 1 ν̃C ⊗ f + h0 ) for k ≥ 0.\nNote that the convergence rate of Theorem 3 differs from the original rate of BCFW (Theorem 1) by the constant ν̃. If ν̃ equals one the rate is the same, but the criterion effectively prohibits cache hits. If ν̃ < 1 then the convergence is slower, meaning that the method with cache needs more iterations to converge, but the oracles calls might be cheaper because of the cache hits.\nEffect of F and ν. The parameter ν controls the global component and acts as a safety parameter to ensure convergence (Theorem 3). The parameter F controls, instead, the local (block-dependent) component of the criterion. Figure 2 illustrates the effect of the parameters on OCR dataset (Taskar et al., 2003) and motivates their choice. At one extreme, if either F or ν are too large the cache is almost never hit. At another extreme, if both values are small the cache is hit almost always, thus the method almost stops calling the oracle and does not converge. Between the two extremes, one of the components usually dominates. We observe empirically that the regime with the local component dominating leads to faster convergence. Our experiments show that the method is not very sensitive to the\nchoice of the parameters, so, in what follows, we use values F = 0.25 and ν = 0.01.\nRelated work. In the context of SSVM, the idea of caching was successfully applied to the cutting plane methods by Joachims et al. (2009), and, recently, to BCFW by Shah et al. (2015). In contrast to Shah et al. (2015), our method chooses whether to call the oracle or to use the cache in an adaptive way by looking at the gap estimates of the current blocks. In the extreme case, when just one block is hard and requires computation and all the rest are easy, our method would be able to call an oracle on the hard block and to use the cache everywhere else. This will result to n times less oracle calls, compared to their strategy."
    }, {
      "heading" : "4. Regularization path",
      "text" : "According to the definition of Efron et al. (2004), a regularization path is a set of minimizers of a regularized objective in the form of (1) for all possible values of the regularization parameter λ. Similarly to LASSO and binary SVM, the general result of Rosset & Zhu (2007, Proposition 1) is applicable to the case of SSVM and implies that the exact regularization path is piecewise linear in 1/λ. However, recovering the exact path is, up to our knowledge, intractable in the case of SSVM. In this paper, we construct an εapproximate regularization path, meaning that, for each feasible λ, we have a corresponding primal variables w which is ε-approximate, i.e., the suboptimality fλ(w)−f∗λ does not exceed ε. We use a piecewise constant approximation except for the first piece which is linear. The approximation is represented by a set of breakpoints {λj}J+1j=0 , λ0 = +∞, λJ+1 = 0, λj+1 ≤ λj , and a set of parameter vectors {wj}Jj=1 with the following properties: for each λ ∈ [λj+1, λj ], j ≥ 1, the vector wj is ε-approximate; for λ ≥ λ1, the vector λ1λ w 1 is ε-approximate. Our algorithm consists of two steps: (1) at the initialization step, we find the maximal finite breakpoint λ∞ := λ1 and the vector w∞ := w1; (2) at the induction step, we compute a value λj+1 and a vector wj+1 given quantities λj and wj . At both steps of our algorithm, we explic-\nitly maintain dual variables α that correspond tow. Alg. 7 in App. D presents the complete procedure.\nInitialization of the regularization path. First, note that, for λ = ∞, the KKT conditions for (1) and (3) imply that w = 0 is a solution of the problem (1). In what follows, we provide a finite value for λ∞ and explicitly construct α∞ and w∞ such that λ ∞\nλ w ∞ is ε-approximate\nfor λ ≥ λ∞. Let ỹi = argmaxy∈Yi Hi(y;0) = argmaxy∈Yi Li(y) be the output of the max oracle forw = 0. First, we construct a dual point α∞ ∈ M by setting α∞i (ỹi) = 1. For any value of λ∞, the corresponding weight vector can be easily computed: w∞ = 1λ∞n ∑n i=1ψi(ỹi). Identity (7) provides the duality gap:\ng(α∞, λ∞,w∞) = 1n n∑ i=1 ( max y∈Yi ( Li(y)− 〈w∞,ψ(y)〉 ) − Li(ỹi) + 〈(w∞,ψ(ỹi)〉 ) .\nThe inequality maxx(f(x) + g(x)) ≤ maxx f(x) + maxx g(x) and the equality maxy∈Yi Li(y) = Li(ỹi) bound the gap:\ng(α∞, λ∞,w∞) ≤ 1n ∑ i ( max y∈Yi (−〈w∞,ψ(y)〉)+\n〈w∞,ψ(ỹi)〉 ) = 1nλ∞ n∑ i=1 θi + 1 λ∞ ∥∥∥ψ̃∥∥∥2 where the quantities θi = maxy∈Yi ( − 〈ψ̃,ψ(y)〉 ) and\nψ̃ := 1n ∑ iψi(ỹi) are easily computable. To ensure that g(α∞, λ, λ ∞\nλ w ∞) ≤ ε for λ ≥ λ∞, we can now set\nλ∞ := 1ε ( ‖ψ̃‖2 + 1n n∑ i=1 θi ) .\nInduction step. We utilize the intuition that the expression (7) provides control on the Frank-Wolfe gap for different values of λ if the primal variablesw and, consequently, the results of the max oracles stay unchanged. Proposition 1 formalizes this intuition. Proposition 1. Assume that Li(yi) = 0, i = 1, . . . , n, i.e., the loss on the ground truth equals zero. Let ρ := λ new\nλold < 1.\nThen, setting αi(y) := ραoldi (y), y 6= yi, and αi(yi) := 1− ∑ y 6=yi αi(y), we then have w new = wold and\ng(α, λnew) = g(αold, λold) + (1− ρ)∆(αold, λold) (11)\nwhere\n∆(αold, λold) := 1n n∑ i=1 ∑ y∈Yi αoldi (y)Hi(y;w old).\nProof. Consider the problem (3) for both λnew and λold. Since ψi(yi) = 0 and Anew = 1ρA\nold, we have that wold = Aoldαold = Anewα = wnew. The assumption Li(yi) = 0 implies equalities Hi(yi;wold) = 0. Under these conditions, the equation (11) directly follows from the computation of g(α, λnew)− g(αold, λold) and the equality (7).\nAssume that for the regularization parameter λold the primal-dual pair αold, wold is κε-approximate, 0 < κ < 1, i.e., g(αold, λold) ≤ κε. Proposition 1 ensures that g(α, λnew) ≤ ε whenever\nρ = 1− ε−g(α old, λold) ∆(αold, λold) ≤ 1− ε(1−κ) ∆(αold, λold) . (12)\nHaving κ < 1 ensures that ρ < 1, i.e., we get a new break point λnew < λold. If the equation (12) results in ρ ≤ 0 then we reach the end of the regularization path, i.e., wold is ε-approximate for all 0 ≤ λ < λold. To be able to iterate the induction step, we apply one of the algorithms for the minimization of the SSVM objective for λnew to obtain κε-approximate pair αnew, wnew. Initializing from α, wold provides fast convergence in practice.\nRelated work. Due to space constraints, see App. A."
    }, {
      "heading" : "5. Experiments",
      "text" : "The experimental evaluation consists of two parts: Section 5.1 compares the different algorithms presented in Section 3; Section 5.2 evaluates our approach on the regularization path estimation.\nDatasets. We evaluate our methods on four datasets for different structured prediction tasks: OCR (Taskar et al., 2003) for handwritten character recognition, CoNLL (Tjong Kim Sang & Buchholz, 2000) for text chunking, HorseSeg (Kolesnikov et al., 2014) for binary image segmentation and LSP (Johnson & Everingham, 2010) for pose estimation. The models for OCR and CoNLL were provided by Lacoste-Julien et al. (2013). We build our model based on the one by Kolesnikov et al. (2014) for HorseSeg, and the one by Chen & Yuille (2014) for LSP. For OCR and CoNLL, the max oracle consists of the Viterbi algorithm (Viterbi, 1967); for HorseSeg – in graph cut (Boykov & Kolmogorov, 2004), for LSP – in belief propagation on a tree with messages passed by a generalized distance transform (Felzenszwalb & Huttenlocher, 2005). Note that the oracles of HorseSeg and LSP require positivity constraints on a subset of the weights in order to be tractable. The BCFW algorithm with positivity constraints is derived in App. H. We provide a detailed description of the datasets in App. I with a summary in Table 1.\nThe problems included in our experimental study vary in the number of objects n (from 100 to 25,000), in the number of features d (from 102 to 106), and in the computational cost of the max oracle (from 10−4 to 2 seconds)."
    }, {
      "heading" : "5.1. Comparing the variants of BCFW",
      "text" : "In this section, we evaluate the three modifications of BCFW presented in Section 3. We compare 8 methods obtained by all the combinations of three binary dimensions: gap-based vs. uniform sampling of objects, BCFW vs. BCPFW, caching oracle calls vs. no caching.\nWe report the results of each method on 6 datasets (including 3 sizes of HorseSeg) for three values of the regularization parameter λ: the value leading to the best test performance, a smaller and a larger value. For each setup, we report the duality gap against both number of oracle calls and elapsed time. We run each method 5 times with different random seeds influencing the order of sampled objects and report the median (bold line), minimum and maximum values (shaded region). We summarize the results in Figure 3 and report the rest in App. J.\nFirst, we observe that, aligned with our theoretical results, gap sampling always leads to faster convergence (both in terms of time and the number of effective passes). The effect is stronger when n is large (Figure 3b). Second, caching always helps in terms of number of effective passes, but an overhead caused by maintaining the cache is significant when the max oracle is fast (Figure 3a). In the case of expensive oracle (Figure 3c), the cache overhead is negligible. Third, the pairwise steps (BCPFW) lead to an improvement to get smaller values of duality gaps. The effect is stronger when the problem is more strongly convex, i.e., λ is bigger. However, maintaining the active sets results in computational overheads, which sometimes are significant. Note that the overhead of cache and active sets are shared, because they are maintained in the same data structure. Using a cache also greatly limits the memory requirements of BCPFW, because, when the cache is hit, the active set is guaranteed not to grow.\nRecommendation. For off-the-shelf usage, we recommend to use the BCPFW + gap sampling + cache method when oracle calls are expensive, and the BCFW + gap sampling method when oracle calls are cheap."
    }, {
      "heading" : "5.2. Regularization path",
      "text" : "In this section, we evaluate our regularization path algorithm presented in Section 4. We compare an εapproximate regularization path with ε = 0.1 against the standard grid search approach with/without warm start (we use a grid of 31 values of λ: 215, 214, . . . , 2−15). In Figure 4, we report the cumulative elapsed time and cumulative number of effective passes over the data required by the three methods to reach a certain value of λ on the HorseSeg-small dataset (starting from the initialization value for the path method and the maximum values of the grid for the grid search methods). The methods and additional experiments are detailed in App. K.\nInterpretation. First, we observe that warm start speeds up the grid search. Second, the cost of computing the full regularization path is comparable with the cost of grid search. However, the regularization path algorithm finds solutions for all values of λ without the need to predefine the grid."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by the MSR-Inria Joint Center and a Google Research Award."
    }, {
      "heading" : "A. Related work for regularization path",
      "text" : "Efron et al. (2004), in their seminal paper, introduced the notion of regularization path and showed that the regularization path of LASSO (Tibshirani, 1996) is piecewise linear. Hastie et al. (2004) proposed the path following method to compute the exact regularization path for the binary SVM with L2-regularization. Exact path following algorithms suffer from numerical instabilities as they repeatedly invert a potentially badly-conditioned matrix (Allgower & Georg, 1993). In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant εapproximate path with at most O(1/ √ ε) break points and applied it, e.g., to binary SVM.\nIn contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM. Finally, Jun-Tao & Ying-Min (2010) constructed the regularization path for the multi-class SVM with huberized loss. We are not aware of any work computing the regularization path for SSVM or, for its predecessor multi-class SVM in the formulation of Crammer & Singer (2001).\nThe induction step of our method is similar to Alg. 1 of (Giesen et al., 2012) applied to the case of binary SVM. They also construct a piecewise linear ε-approximate path by alternating the SVM solver and a procedure to identify the region where the output of the solver is accurate enough.\nIn contrast to our method, Giesen et al. (2012) construct the path only for the predefined segment of the values of λ. We do not require such a segment as input and are able to find the largest and smallest value automatically. Another difference to (Giesen et al., 2012) consists in using the λ-formulation of SVM instead of the C-formulation. In the two formulations, the accuracy parameter ε is scaled differently for the different values of the regularization parameters. The λ-formulation requests higher accuracy for the small values of λ and, thus, creates more break points in that region."
    }, {
      "heading" : "B. Block descent lemma for BCFW",
      "text" : "Definition 2 (Block curvature constant). Consider a convex function f defined on a separable domain M = M(1) × · · · × M(n). The curvature constant C(i)f of the function f w.r.t. the individual block of coordinatesM(i) is defined by\nC (i) f := sup 2 γ2 ( f(β)− f(α)− 〈β(i) −α(i),∇(i)f(α)〉 ) s.t. α ∈M, s(i) ∈M(i), γ ∈ [0, 1], β = α+ γ(s[i] −α[i]). (13)\nHere s[i] ∈ Rm and α[i] ∈ Rm are the zero-padded versions of s(i) ∈ M(i) and α(i) ∈ M(i), respectively. Note that, although s[i] 6∈ M and α[i] 6∈ M, we have that β := α+ γ(s[i] −α[i]) ∈M.\nIn the case of SSVM, the curvature constant C(i)f can be\nupper bounded (tightly in the worst case) with 4R 2 i\nλn2 where Ri := maxy∈Yi ‖ψi(y)‖2 (Lacoste-Julien et al., 2013, Appendix A).6\nFor reference, we restate below the key descent lemma used for the proof of convergence of BCFW and its variants. We note in passing that this is an affine invariant analog of the standard descent lemmas that use the Lipschitz continuity of the gradient function to show progress during first order optimization algorithms.\nLemma 3 (Block descent lemma). For any α ∈ M and block i, let s(i) ∈ M(i) be the Frank-Wolfe corner selected by the max oracle of block i at α. Let αLS be obtained by the line search between α(i) ∈ M(i) and s(i), i.e., f(αLS) = minγ∈[0,1] f(αγ) where αγ := α + γ(s[i] − α[i]). Then, it holds that for each γ ∈ [0, 1]:\nf(αLS) ≤ f(α)− γgi(α) + γ 2 2 C (i) f (14)\nwhere C(i)f is the curvature constant of the function f over the factorM(i) and gi(α) is the block gap at the point α w.r.t. the block i.\nProof. From Definition 2 of the curvature constant and the expression (6) for the block gap, we have\nf(αγ) =f(α+ γ(s[i] −α[i]))\n≤f(α) + γ〈s(i) −α(i),∇(i)f(α)〉+ γ 2 2 C (i) f =f(α)− γgi(α) + γ 2 2 C (i) f .\nThe inequality f(αLS) ≤ f(αγ) completes the proof."
    }, {
      "heading" : "C. Toy example for gap sampling",
      "text" : "In this section, we construct a toy example of the structured SVM problem where the adaptive gap-based sampling is n times faster than non-adaptive sampling schemes such as uniform sampling or curvature-based sampling (the latter being the affine invariant analog of Lipschitz-based sampling).\nGeneral idea. The main idea is to consider a training set where there are n − 1 “easy” objects that need to be visited only once to learn to classify them, and one “hard” object that requires at least K 1 visits in order to get the optimal parameter. We can design the example in such a way that the curvature or Lipschitz constants are noninformative about which example is hard, and which is\n6More generally, let ‖ · ‖i be some norm defined on M(i). Then suppose that Li is the Lipschitz-continuity constant with respect to this norm for ∇(i)f(α) when only α(i) varies, i.e., ‖∇(i)f(α) − ∇(i)f(α + s[i] − α[i])‖∗i ≤ Li‖s(i) − α(i)‖i for all α ∈ M, s(i) ∈ M(i), where ‖ · ‖∗i is the dual norm of ‖ · ‖i. Then similarly to Lemma 7 in Jaggi (2013), we have C\n(i) f ≤ Li ( diam‖·‖iM (i) )2.\neasy. The non-adaptive sampling schemes will thus have to visit the easy objects as often as the hard object, whereas the gap sampling technique can adapt to focus only on the single hard object after having visited the easy objects once, thus yielding an overall min{n,K}-times speedup. Note that large-scale learning datasets could have analogous features as this toy example: a subgroup of objects might be easier to learn than another, and moreover, they might share similar information, so that after visiting a subset, we do not need to linger on the other ones from the same subset as all the information has already been extracted. We cannot know in advance which subgroups are these subsets, and thus an adaptive scheme is needed.\nC.1. Explicit construction For simplicity, we set the weight of the regularizer λ to 1/n so that the scaling factor defining A in Problem (3) is 1/λn = 1. The matrix A thus consists of the difference feature maps, i.e., A :={ ψi(y) := φ(xi,yi)− φ(xi,y) ∈ Rd\n∣∣ i ∈ [n],y ∈ Yi}. In our example, we use feature maps of dimensionality d := K + 1 := |Yi|. Let Yi := {0, 1, . . . ,K} be the set of labels for the object i and the label 0 be the correct label. We consider the zero-one loss, i.e., Li(0) = 0 and Li(k) = 1 for k ≥ 1. In the following, let {ej}dj=1 be the standard basis vectors for Rd.\nHard and easy objects. We construct the feature map φ(x,y) so that only the last coordinate of the parameter vector is needed to classify correctly the easy object, whereas all the other coordinates are needed to classify correctly the hard object. By using a different set of coordinates between the easy and the hard objects, we simplify the analysis as the optimization for both block types decouples (become independent). Specifically, we set the feature map for the correct label to be φ(xi, 0) := 0 for all objects i. We let i = 1 be the hard object and we set φ(xi, k) := − 1√2ek for k = 1, . . . ,K. For the easy object, i ∈ {2, . . . , n}, we use the constant φ(xi, k) := −eK+1 for all k ≥ 1. The normalization of the feature maps is made so that the curvature constants for all the objects are equal (see below). Note also here that ψ1(k) ⊥ ψi(l) for any labels k, l, and thus the optimization over block 1 decouples with the one for the other blocks i = 2, . . . , n. The SSVM dual (3) takes here the following simple form:\nmin α∈Rm α<0\n1 n K∑ k=1 ( 1 4α1(k) 2 − α1(k) ) + 1n ( 1 2u 2 − u) (15)\ns.t. K∑ k=0 αi(k) = 1 ∀i ∈ [n] , u = n∑ i=2 K∑ k=1 αi(k) ,\nwhere we have introduced the auxiliary variable u to highlight the simple structure for the optimization over the easy\nblocks. The unique7 solution for the first (hard) block is easily seen to be α∗1(k) = 1 K for k ≥ 1 and α ∗ 1(0) = 0. For the easy blocks, any feasible combination of dual variables that gives u∗ = 1 is a solution. This gives the optimal parameter w∗ = Aα∗ = eK+1 + 1K ∑K k=1 ek.\nOptimization on the hard object. The objective for the hard object (block 1) in (15) is similar to the one used to show a lower bound of Ω(1/t) suboptimality error after t iterations for the Frank-Wolfe algorithm for t smaller than the dimensionality (e.g., see Lemma 3 and 4 in Jaggi (2013)), hence showing that the optimization is difficult on this block. The BCFW algorithm is initialized withw = 0, which corresponds to putting all the mass on the correct label, i.e., αi(0) = 1 and αi(k) = 0, k ≥ 1. At each iteration of BCFW, the mass can be moved only towards one corner, and all the corners (of the simplex) have exactly one non-zero coordinate. This means that after t iterations of BCFW on the first block, at most t non-ground truth dual variables can be non-zero. Minimizing the objective (15) over the first block with the constraint that at most t of these variables are non-zero give the similar solution α1(k) = 1/t for k = 1, . . . , t, which gives a suboptimality of 14n ( 1 t − 1 K ) for t ≤ K. Similarly, this also yields the smallest FW gap8 possible for this block after t iterations, which is 1n 1 2t . This means that in order to get a suboptimality error smaller than ε, one needs at least\nt ≥ Ω(min{K, 1nε}) (16)\nBCFW iterations on the first block.9\nOptimization on easy objects. Finally, we now show that after one iteration on any easy object, the gaps gi on all easy objects become zero (i.e., they are all optimal and then stay optimal as the optimization is decoupled with the first block). After this iteration, BCFW with gap sampling visits all the easy objects exactly once and sets their gap estimates to zero, thus never revisiting them again.\nNote that before visiting any easy object i, we have 〈w,ψi(k)〉 = 0 for all k as the features for the hard object are orthogonal and w is initialized to zero. Thus, at the first visit of an easy object i ∈ {2, . . . , n}, we have Hi(0;w) = 0 and Hi(k;w) = 1, k ≥ 1, and the max oracle returns some (any) label k ∈ {1, . . . ,K}. Following the steps of Algorithm 1, we havews := 1λnψ(k) = eK+1 and `s = 1nLi(k) = 1 n . Then gi = 1 n and γ = 1 as\n7Uniqueness can be proved by noticing that the objective is strongly convex in α1 after removing α1(0) and replacing the equality constraint with an inequality.\n8Recall that the FW gap here is the same as the Lagrangian duality gap (see Section 2.3), and so if one cares about the SSVM primal suboptimality, one needs a small FW gap.\n9In fact, BCFW also has a O( 1 nt ) gap after t iterations on the first block by the standard FW convergence theorem, asC(1)f = 1 n as we show in (18).\nwi = 0. The assignment wi = eK+1 implies the update w ← w + eK+1 of the parameter vector. After such an update, at all iterations, for all easy objects i ∈ {2, . . . , n} and for all labels k ∈ {1, . . . ,K}, we have\nHi(k;w) = Li(k)− 〈w, eK+1〉 = 0 (17)\nbecause the coordinate wK+1 is never updated again. According to (7), the equalities (17) imply that the block gaps gi equal zero for all the easy objects.\nCurvature constants. The simple structure of the matrix A allows us to explicitly compute the curvature constants C(i)f corresponding to both easy and hard objects.\nThe SSVM dual (3) is a quadratic function with a constant Hessian H := λATA, so the second-order Taylor expansion of f at a point α allows us to rewrite the definition 13 as\nC (i) f = sup\nα∈M s(i)∈M(i)\n(s[i] −α[i])TH(s[i] −α[i])\n= λ sup α∈M\ns(i)∈M(i)\n‖A(s[i] −α[i])‖22\n= λmax k , l ‖φ(xi, k)− φ(xi, l)‖22.\nThe last line uses the property that the maximum of a convex function over a convex set is obtained at a vertex.\nIn the case of the hard object, we can get\nC (1) f = λ‖φ(x1, 1)− φ(x1, 2)‖ 2 2 = 1 n . (18)\nIn the case of an easy object, we can get\nC (i) f = λ‖φ(xi, 1)− φ(xi, 0)‖ 2 2 = 1 n . (19)\nAdaptive and non-adaptive sampling. Let t be the number of steps needed on the hard block. By (16), we need t ≥ Ω(min{K, 1nε}) to get a suboptimality smaller than ε. The uniform sampling scheme visits all the objects with the same probability. In the setting constructed above, it makes, on average, t visits to each easy object prior to visiting the hard object t times. Thus, the overall scheme will call the max-oracle O(nt) times. All the curvature constants C(i)f are equal, so the sampling proportional to the curvature constants is equivalent to uniform sampling.\nThe adaptive sampling scheme visits each easy object only once after the first visit to any of them. After such a visit to any easy object, its local gap estimate equals zero and this object is never visited again. The gap sampling scheme thus makes an overall O(n + t) oracle calls. The adaptive scheme is thus approximately min{n, t} = min{n,K, 1nε} times faster than the non-adaptive ones. The speed-up can be made arbitrary large by setting both n and K large enough, and ε small enough.\nLipschitz and curvature constants. The non-uniform sampling techniques used in the work of Nesterov (2012); Needell et al. (2014); Zhao & Zhang (2015) use Lipschitz constants of partial derivatives to obtain the sampling probabilities. In our discussion above, we use the curvature constants. Lacoste-Julien & Jaggi (2015, Appendix C) note that the curvature constants are affine invariant quantities and, thus, are more suited for the analysis of Frank-Wolfe methods compared to Lipschitz constants (which depend on a choice of norm). We illustrate this point on our toy example by explicitly computing the Lipschitz constants over blocks for the `2 and `1 norm. For both easy and hard blocks, the Lipschitz constant of the gradient with respect to the `2 norm equals the largest eigenvalues of the corresponding block Hessians. For an easy object, the block Hessian is a rank one matrix with the only non-zero eigenvalue equal to λ(|Yi| − 1) = Kn . For the hard object, the block Hessian is a diagonal matrix with non-zero entries equal to λ2 = 1 2n . Here the Lipschitz constant for the easy block is about K times bigger than the one for the hard block, and thus for a large number of labels K, sampling according to Lipschitz constants can be much slower than sampling according to the curvature constants, which was itself slower than the adaptive sampling scheme.\nThis poor scaling of the Lipschitz constants is partly due to the bad choice of norm in relationship to the optimization domain. d’Aspremont et al. (2013) suggests to use the atomic norm of the domain M for the analysis. In the case of the simplex, we get the `1 norm to measure the diameter of the domain, and its dual norm (`∞) to measure the Lipschitz constant of the gradient. With this norm, the Lipschitz constant stays as 12n for the hard block, but decreases to the more reasonable 1n for the easy blocks. As explained in footnote 6, we can use the bound C\n(i) f ≤ Li ( diam‖·‖iM(i) )2 for the curvature constant. As the diameter for the simplex measured with the `1-norm is 2, we get the bound C(1)f ≤ 2 n for the hard block, very close to its exact value of 1n as derived in (18). The `1 norm thus appears as a more appropriate choice for this problem."
    }, {
      "heading" : "D. Detailed algorithms.",
      "text" : "In this section, we give the detailed versions of our BCFW variants applied to the SSVM objective presented in the main paper. Algorithm 2 describes BCFW with adaptive gap sampling. We give the block-coordinate version of pairwise FW (BCPFW) in Algorithm 3, and of awaystep FW (BCAFW) in Algorithm 4. We note that these two algorithms are simply the blockwise application of the PFW and AFW algorithms as described in Lacoste-Julien & Jaggi (2015), but in the context of SSVM which complicates the notation. Algorithm 5 presents the BCFW algorithm with caching. Algorithm 7 presents our method for computing the regularization path and Algorithm 6 presents the initialization of the regularization path.\nAlgorithm 2 Block-coordinate Frank-Wolfe (BCFW) algorithm with gap sampling for structured SVM\n1: Let w(0):=wi(0):=0; `(0):=`i(0):=0; g (0) i :=+∞; 2: ki :=0 // the last time gi was computed 3: for k := 0, . . . ,∞ do 4: Pick i at random with probability ∝ g(ki)i 5: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(k))\n6: Let ki := k 7: Let ws := 1λnψi(y ∗ i ) and `s := 1 nLi(y ∗ i ) 8: Let g(ki)i := λ(w (k) i −ws)Tw(k) − ` (k) i + `s 9: Let γ := g (ki) i\nλ‖w(k)i −ws‖ 2\nand clip to [0, 1]\n10: Update wi(k+1) := (1− γ)wi(k) + γws 11: and `i(k+1) := (1− γ)`i(k) + γ `s 12: Update w(k+1) := w(k) +wi(k+1) −wi(k) 13: and `(k+1) := `(k) + `i(k+1) − `i(k) 14: if update global gap then 15: for i := 1, . . . , n do 16: Let ki := k + 1 17: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(ki))\n18: Let ws := 1λnψi(y ∗ i ) and `s := 1 nLi(y ∗ i ) 19: g(ki)i := λ(w (ki) i −ws)Tw(ki) − ` (ki) i + `s 20: end for 21: end if 22: end for\nNote that the three modifications proposed in our paper (gap sampling, caching, pairwise/away steps) can be straightforwardly put together in any combination. In our experimental study, we evaluate all the possibilities.\nWhen using gap sampling or caching and to guarantee convergence, we have to do a full pass over the data every so often to refresh the global gap estimates and to compensate for the staleness effect. In the experiments, we perform this computation every 10 passes over the data (this is the “update global gap” condition in the algorithms). This global gap can also be used as a certificate (upper bound) on the current suboptimality. We thus use the same frequency of global gap computation (every 10 passes) when we run a SSVM solver with a specific convergence tolerance threshold. This is used in our regularization path algorithm which runs a SSVM solver up to a fixed convergence tolerance at each breakpoint.\nIn our description of the regularization path algorithms (Algorithm 6 and Algorithm 7), we explicitly describe how to update the active sets over the dual variables when the regularization parameter is updated. This is needed when using a SSVM solver that requires the active set over the dual variables (such as BCPFW or BCAFW). When using the simpler BCFW solver, then lines 14–17 of Algorithm 6 and lines 17–21 of Algorithm 7 can simply be omitted.\nAlgorithm 3 Block-coordinate pairwise Frank-Wolfe (BCPFW) algorithm for structured SVM\n1: Let w(0) := wi(0) := 0; `(0) := `i(0) := 0; 2: Si(0) := {yi}; // active sets 3: α(0)i (y) := 0, y 6= yi; α (0) i (yi) := 1 4: for k := 0, . . . ,∞ do 5: Pick i at random in {1, . . . , n} 6: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(k)) // FW corner\n7: Let ws := 1λnψi(y ∗ i ) and `s := 1 nLi(y ∗ i ) 8: Solve yai := argmin y∈S(k)i Hi(y;w (k)) // away corner 9: Let wa := 1λnψi(y a i ) and `a := 1 nLi(y a i )\n10: Let wd := ws −wa and `d = `s − `a 11: Let γ := −λwd\nTw(k)+`d λ‖wd‖2 and clip to [0, α (k) i (y a i )]\n12: Update wi(k+1) := wi(k) + γwd 13: and `i(k+1) := `i(k) + γ `d 14: Update w(k+1) := w(k) +wi(k+1) −wi(k) 15: and `(k+1) := `(k) + `i(k+1) − `i(k) 16: Update Si(k+1) := Si(k) ∪ {y∗i } 17: and α(k+1)i (y a i ) := α (k) i (y a i )− γ 18: and α(k+1)i (y ∗ i ) := α (k) i (y ∗ i ) + γ 19: if γ = α(k)i (yai ) then 20: Set Si(k+1) := Si(k+1) \\ {yai } // drop step 21: end if 22: end for"
    }, {
      "heading" : "E. Proof of Theorem 2 (convergence of BCFW with gap sampling)",
      "text" : "Lemma 4 (Expected block descent lemma). Let gj(α(k)) be the block gap for block j for the iterateα(k). Letα(k+1) be obtained by sampling a block i with probability pi and then doing a (block) FW step with line-search on this block, starting from α(k). Consider any set of scalars γj ∈ [0, 1], j = 1, . . . , n, which do not depend on the chosen block i. Then in conditional expectation over the random choice of block i with probabilities pi, it holds:\nIE [ f(α(k+1)) |α(k) ] ≤ f(α(k))− n∑ i=1 γipigi(α (k))\n+ 12 n∑ i=1 γ2i piC (i) f . (20)\nProof. The proof is analogous to the proof of Lemma 3, but being careful with the expectation. Let block i be the chosen one that defined α(k+1) and let αγ := α + γ(s[i] − α[i]), where s(i) ∈ M(i) is the FW corner on block i and s[i] ∈ Rm is its zero-padded version. By the linesearch, we have f(α(k+1)) ≤ f(αγ) for any γ ∈ [0, 1]. By using γ = γi in the bound (13) provided in the curvature Definition 2, and by the definition of the Frank-Wolfe\nAlgorithm 4 Block-coordinate away-step Frank-Wolfe (BCAFW) algorithm for structured SVM\n1: Let w(0) := wi(0) := 0; `(0) := `i(0) := 0; 2: S(0)i := {yi}; // active sets 3: α(0)i (y) := 0, y 6= yi; α (0) i (yi) := 1 4: for k := 0, . . . ,∞ do 5: Pick i at random in {1, . . . , n} 6: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(k)) // FW corner\n7: Let ws := 1λnψi(y ∗ i ) and `s := 1 nLi(y ∗ i ) 8: Solve yai := argmin y∈S(k)i Hi(y;w (k)) // away corner 9: Let wa := 1λnψi(y a i ) and `a := 1 nLi(y a i )\n10: Let gFWi := λ(w (k) i −ws)Tw(k) − ` (k) i + `s 11: Let gAi := λ(wa −w (k) i ) Tw(k) + ` (k) i − `a 12: if gFWi > gAi then // FW step 13: Let γ := g FW i\nλ‖w(k)i −ws‖2 and clip to [0, 1]\n14: Update Si(k+1) := Si(k) ∪ {y∗i } 15: and α(k+1)i (y) := (1− γ)α (k) i (y) 16: and α(k+1)i (y ∗ i ) := α (k+1) i (y ∗ i ) + γ 17: Set Si(k+1) := {y∗i } if γ = 1 18: else // away step 19: Let γ := g A i\nλ‖w(k)i −wa‖2 and clip to [0, αi(y\na i )\n1−αi(yai ) ]\n20: Update α(k+1)i (y) := (1 + γ)α (k) i (y) 21: and α(k+1)i (y a i ) := α (k+1) i (y a i )− γ 22: and Si(k+1) := Si(k) \\ {yai } if α (k+1) i (y a i ) = 0 23: end if 24: Update wi(k+1) := wi(k) + γwd 25: and `i(k+1) := `i(k) + γ `d 26: Update w(k+1) := w(k) +wi(k+1) −wi(k) 27: and `(k+1) := `(k) + `i(k+1) − `i(k) 28: end for\ngap, we get:\nf(α(k+1)) ≤ f(αγi) = f(α(k) + γi(s[i] −α (k) [i] ))\n≤ f(α(k)) + γigi(α(k)) + γ 2 i 2 C (i) f .\nTaking the expectation of the bound with respect to i, conditioned on α(k), proves the lemma.\nDefinition 5. The nonuniformity measure χ(x) of a vector x ∈ Rn+ is defined as:\nχ(x) := √ 1 + n2 Var [ p ]\nwhere p := x‖x‖1 is the probability vector obtained by normalizing x. Lemma 6. Let x ∈ Rn+. The following relation between its `1-norm and `2-norm holds:\n‖x‖2 = χ(x)√n ‖x‖1 .\nProof. We have that Var [ p ] = IE [ p2 ] − IE [ p ]2\n= 1n‖p‖ 2 2 − 1n2 . (21)\nCombining (21) and Definition 5 we prove the lemma.\nRemark. For any x ∈ Rn+, the quantity χ(x) always belongs to the segment [1, √ n]. We have χ(x) = 1 when all the elements of x are equal and χ(x) = √ n when all the elements, except one, equal zero.\nTheorem 2. Assume that at each iterate α(k), k ≥ 0, BCFW with gap sampling (Algorithm 2) has access to the exact values of the block gaps. Then, at each iteration, it holds that IE [ f(α(k)) ] − f(α∗) ≤ 2nk+2n ( C⊗f χ ⊗ + h0 ) where α∗ ∈ M is a solution of problem (3), h0 := f(α(0)) − f(α∗) is the suboptimality at the starting point of the algorithm, the constant C⊗f := ∑n i=1 C (i) f is the sum of the curvature constants, and the constant χ⊗ is an up-\nper bound on IE [ χ(C (:) f )\nχ(g:(α(k)))3\n] , which quantities the amount\nof non-uniformity of the C(i)f ’s in relationship to the nonuniformity of the gaps obtained during the algorithm. The expectations are taken over the random choice of the sampled block at iterations 1, . . . , k of the algorithm.\nProof. Starting from Lemma 4 with γi := γ for some γ to be determined later and pi := gig where gi := gi(α (k)) and g := g(α(k)), we get\nIE [ f(α(k+1)) |α(k) ] ≤ f(α(k))− γ n∑ i=1 g2i g\n+ γ 2\n2 n∑ i=1 C (i) f gi g . (22)\nThe Cauchy-Schwarz inequality bounds the dot product between the vectors of curvature constants c := C(:)f := (C (i) f ) n i=1 and block gaps g := g:(α (k)) := (gi) n i=1\nn∑ i=1 C (i) f gi ≤ ‖c‖2 ‖g‖2 . (23)\nCombining (23) and the result of Lemma 6 for the vectors of curvature constants and block gaps (with ‖g‖1 = g and ‖c‖1 = C⊗f ), we can further bound (22):\nIE [ f(α(k+1)) |α(k) ] ≤ f(α(k))− γgn χ(g) 2\n+ γ 2 2nχ(g)χ(c)C ⊗ f . (24)\nSubtracting the minimal function value f(α∗) from both sides of (24) and by using h(α(k)) := f(α(k))− f(α∗) ≤ g, we bound the conditional expectation of the suboptimality h with\nIE[h(α(k+1)) | α(k)] ≤ h(α(k))− γnχ(g) 2 h(α(k))\n+ γ 2 2nχ(g)χ(c)C ⊗ f (25)\nwhich is analogous to (Lacoste-Julien et al., 2013, Eq. (20)). In what follows, we use the modified induction technique of (Lacoste-Julien et al., 2013, Proof of Theorem C.1).\nBy induction, we are going to prove the following upper bound on the unconditional expectation of the suboptimality h:\nIE [ h(α(k)) ] ≤ 2nCk+2n , for k ≥ 0, (26)\nthat corresponds to the statement of the theorem with C := C⊗f χ ⊗ + h0.\nThe basis of the induction k = 0 follows immediately from the definition of C, given that C⊗f ≥ 0 and χ⊗ > 0. Consider the induction step. Assume that (26) is satisfied for k ≥ 0. With a particular choice of step size γ := 2nχ(g)2(k+2n) ∈ [0, 1] (which does not depend on the picked i), we rewrite the bound (25) on the conditional expectation as\nIE[h(α(k+1)) | α(k)] ≤ ( 1− 2k+2n ) h(α(k))\n+ 2n(k+2n)2 χ(c)C⊗f χ(g)3 . (27)\nTaking the unconditional expectation of (27), then the induction assumption (26) and the definition of χ⊗ give us the deterministic inequality\nIE[h(α(k+1))] ≤ ( 1− 2k+2n ) 2nC k+2n\n+ 2n(k+2n)2χ ⊗ C⊗f . (28)\nBounding χ⊗ C⊗f by C and rearranging the terms gives IE[h(α(k+1))] ≤ 2nCk+2n ( 1− 2k+2n + 1 k+2n ) = 2nCk+2n k+2n−1 k+2n\n≤ 2nCk+2n k+2n k+2n+1\n= 2nC(k+1)+2n ,\nwhich completes the induction proof.\nComparison with uniform sampling. We now compare the rates obtained by Theorem 2 for BCFW with gap sampling and by Theorem 1 for BCFW with uniform sampling. The only difference is in the constants: Theorem 2 has C⊗f χ ⊗ and Theorem 1 has C⊗f .\nRecall that by definition\nχ⊗ = max k\nIE [ χ(C (:) f )\nχ(g:(α(k)))3 ] In the best case for gap sampling, the curvature constants are uniform, χ(C(:)f ) = 1, and the gaps are nonuniform χ(g:(α(k))) ≈ √ n. Thus, χ⊗ ≈ 1\nn √ n .\nIn the worst case for gap sampling, the curvature constants are very non-uniform, χ(C(:)f ) ≈ √ n. The constant for gap\nsampling is still better if the gaps are non-uniform enough, i.e., χ(g:(α(k))) ≥ n 1 6 .\nWe note that to design a sampling scheme that always dominates uniform sampling (in terms of bounds at least), we would need to include the C(i)f ’s in the sampling scheme (as was essentially done by Csiba et al. (2015) for SDCA). Unfortunately, computing good estimates for C(i)f ’s is too expensive for structured SVM, thus motivating our simpler yet practically efficient scheme. See also the discussion after (10)."
    }, {
      "heading" : "F. Proof of Theorem 3 (convergence of BCFW with caching)",
      "text" : "Theorem 3. Let ν̃ := 1nν ≤ 1. Then, for each k ≥ 0, the iterateα(k) of Algorithm 5 satisfies IE [ f(α(k)) ] −f(α∗) ≤\n2n ν̃k+2n ( 1 ν̃C ⊗ f + h0 ) where α∗ ∈ M is a solution of problem (3), h0 := f(α(0))− f(α∗) is the suboptimality at the starting point of the algorithm, C⊗f := ∑n i=1 C (i) f is the sum of the curvature constants (see Definition 2) of f with respect to the domainsM(i) of individual blocks. The expectation is taken over the random choice of the sampled blocks at iterations 1, . . . , k of the algorithm.\nProof. The key observation of the proof consists in the fact that the combined oracle (the cache oracle in the case of a cache hit and the max oracle in the case of a cache miss) closely resembles an oracle with multiplicative approximation error (Lacoste-Julien et al., 2013, Eq. (12) of Appendix C).\nIn the case of a cache hit, Definition 2 of curvature constant for any step size γ ∈ [0, 1] gives us\nf(α(k+1)γ ) := f(α (k) + γ(c[i] −α (k) [i] ))\n≤ f(α(k)) + γ〈c(i)−α (k) (i) ,∇(i)f(α (k))〉+ γ 2 2 C (i) f\n= f(α(k))− γĝ(k)i + γ2 2 C (i) f ≤ f(α(k))− γν̃g(k0) + γ 2\n2 C (i) f\nwhere the corner c(i) ∈ M(i) and its zero-padded version c[i] ∈ Rm are provided by the cache oracle, and ν̃ = 1nν is the constant controlling the global part of the cache-hit criterion. In the case of a cache miss, similarly to Lemma 3, we get\nf(α(k+1)γ ) ≤ f(α(k))− γg (k) i +\nγ2\n2 C (i) f .\nCombining the two cases we get\nf(α(k+1)γ ) ≤ f(α(k))− γg̃ (k) i +\nγ2\n2 C (i) f (29)\nwhere\ng̃ (k) i := [i is a cache miss]g (k) i + [i is a cache hit]ν̃g (k0).\nAlgorithm 5 Block-coordinate Frank-Wolfe (BCFW) algorithm with cache for structured SVM\n1: Let w(0) :=wi(0) :=0; `(0) :=`i(0) :=0; Ci :={yi}; 2: g(0) :=g(0)i =+∞ 3: k0 :=ki :=0 ; // the last time g / gi was computed 4: for k := 0, . . . ,∞ do 5: Pick i at random in {1, . . . , n} // either uniform or 6: with probability ∝ g(ki)i for gap sampling 7: Solve yci := argmaxy∈Ci Hi(y;w) // cache corner 8: Let wc := ψi(y c i ) λn and `c := 1 nLi(y c i ) 9: Let ĝ(k)i := λ(w (k) i −wc)Tw(k) − `i (k) + `c\n10: if ĝ(k)i ≥ max(Fg (ki) i , ν ng (k0)) then // cache hit 11: ws := wc, `s := `c, ĝi := ĝ (k) i 12: else // cache miss 13: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(k)) // FW corner\n14: Let ws := 1λnψi(y ∗ i ) and `s := 1 nLi(y ∗ i ) 15: Let g(k)i := λ(w (k) i −ws)Tw(k) − ` (k) i + `s 16: Set ki := k, ĝi := g (k) i 17: Update Ci := Ci ∪ {y∗i } 18: end if 19: Let γ := ĝi\nλ‖w(k)i −ws‖2 and clip to [0, 1]\n20: Update wi(k+1) := (1− γ)wi(k) + γws 21: and `i(k+1) := (1− γ)`i(k) + γ `s 22: Update w(k+1) := w(k) +wi(k+1) −wi(k) 23: and `(k+1) := `(k) + `i(k+1) − `i(k) 24: if update global gap then 25: Let g(k0) := 0, k0 := k + 1 26: for i := 1, . . . , n do 27: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(k0))\n28: Let ws := 1λnψi(y ∗ i ) and `s := 1 nLi(y ∗ i ) 29: g(k0)+= λ(w(k0)i −ws)Tw(k0) − ` (k0) i + `s 30: Set ki := k0 31: end for 32: end if 33: end for\nSubtracting f(α∗) from both sides of (29) and taking the expectation of (29) w.r.t. the block index i we get\nIE [ h(α(k+1)γ ) |α(k) ] ≤ h(α(k))− γn g̃ (k) + γ 2 2nC ⊗ f (30)\nwhere h(α) := f(α) − f(α∗) is the suboptimality of the function f and g̃(k) := ∑n i=1 g̃ (k) i . We know that the duality gap upper-bounds the suboptimality, i.e., g(α) ≥ h(α), and that cache miss steps, as well as cache hit steps, always decrease suboptimality, i.e., h(α(k)) ≤ h(α(k0)). If at iteration k there is at least one cache hit, then we can bound the quantity g̃(k) from below:\ng̃(k) ≥ ν̃g(α(k0)) ≥ ν̃h(α(k0)) ≥ ν̃h(α(k)). (31)\nAlgorithm 6 INIT-REG-PATH: Initialization of the regularization path for structured SVM\ninput κ, tolerance ε 1: w := wi := 0; ` := `i := 0; ψ̃ := 0 2: for i := 1, . . . , n do 3: ỹi := argmaxy∈Yi Hi(y;0) 4: `i := 1 nL(yi, ỹi)\n5: ` := `+ `i 6: ψ̃ := ψ̃ + 1nψ(ỹi) 7: end for 8: for i := 1, . . . , n do 9: θi := maxy∈Yi ( − ψ̃Tψ(y)\n) 10: end for 11: Let λ∞ := 1κε ( ‖ψ̃‖2 + 1n ∑n i=1 θi\n) 12: Let w := 1λ∞ ψ̃; wi := 1 nλ∞ψ(ỹi) 13: for i := 1, . . . , n do gi := 1nλ∞ θi + λ ∞wTi w 14: for i := 1, . . . , n do // optional 15: Si := {ỹi} 16: αi(ỹi) := 1 and αi(y) := 0 for y 6= ỹi 17: end for 18: return w, wi, `, `i, gi, λ∞, Si, α\nIn the case of no cache hits, we have\ng̃(k) = g(α(k)) ≥ h(α(k)) ≥ ν̃h(α(k))\nwhere the last inequality holds because ν̃ ≤ 1. Applying the lower bound on g̃(k) to (30), we get\nIE [ h(α(k+1)γ ) |α(k) ] ≤ h(α(k))− γν̃n h(α (k))\n+ γ 2 2nC ⊗ f .\n(32)\nInequality (32) is identical to the inequality (Lacoste-Julien et al., 2013, Eq. (20)) in the proof of convergence of BCFW with a multiplicative approximation error in the oracle. We recopy their argument below for reference to finish the proof. First, we take the expectation of (32) w.r.t. the choice of previous blocks:\nIE [ h(α(k+1)γ )] ≤ (1− γν̃ n )IE [ h(α(k))] + γ 2 2nC ⊗ f . (33)\nFollowing the proof of Theorem C.1 in Lacoste-Julien et al. (2013), we prove the bound of Theorem 3 by induction. The induction hypothesis consists in inequality\nIE [ h(α(k))] ≤ 2nCν̃k+2n for k ≥ 0\nwhere C := (\n1 ν̃C ⊗ f + h0\n) .\nThe base-case k = 0 follows directly from C ≥ h0. We now prove the induction step. Assume that the hypothesis is true for a given k ≥ 0. Let us now prove that the hypothesis is true for k + 1. We use inequality (32) with the step\nAlgorithm 7 Regularization path for structured SVM input κ, tolerance ε, λmin\n1: Initialize regularization path using Algorithm 6. {w0,w0i , `0, `0i , g0i , λ0,S0i ,α0}:= INIT-REG-PATH (κ, ε) 2: J := 0 3: repeat 4: For i := 1, . . . , n do δi := `Ji − λJ〈wJ ,wJi 〉 5: Compute excess gap τ := ε− ∑n i=1 g J i\n6: Let ∆ := ∑n i δi 7: if ∆ ≤ τ then 8: Let ρ := 1− τ∆ 9: else\n10: wJ is ε-approximate for any λ < λJ 11: return {λj}Jj=0, {wj}Jj=0 12: end if 13: Let λJ+1 := ρλJ , `J+1 := ρ`J , 14: for i := 1, . . . , n do // update gaps using (11) 15: Let gJ+1i := g J i + (1− ρ)δi and ` J+1 i := ρ` J i 16: end for 17: for i := 1, . . . , n do // optional: update duals 18: SJ+1i := SJi ∪ {yi} 19: αJ+1i (y) := ρα J i (y) for y ∈ S J+1 i \\ {yi}\n20: αJ+1i (yi) := 1− ∑ y∈SJ+1i \\{yi} αJ+1i (y) 21: end for 22: Run SSVM-optimizer with tolerance κ ε 23: to updatewJ+1,wJ+1i , ` J+1, `J+1i , g J+1 i , S J+1 i , α J+1\n// to have ε-appr. path, gaps gJ+1i have to be exact 24: J := J + 1 25: until λJ+1 < λmin 26: return {λj}Jj=0, {wj}Jj=0\nsize γk := 2nν̃k+2n ∈ [0, 1]: IE [ h(α(k+1)γk )] ≤ (1− γkν̃ n )IE [ h(α(k))] + (γk) 2Cν̃ 2n\n= (1− 2ν̃ν̃k+2n )IE [ h(α(k))] + ( 2nν̃k+2n ) 2Cν̃ 2n\n≤ (1− 2ν̃ν̃k+2n ) 2nC ν̃k+2n + ( 1 ν̃k+2n ) 22nCν̃\nwhere, in the first line, we use inequality C⊗f ≤ Cν̃, and, in the last line, we use the induction hypothesis for IE [ h(α(k))].\nBy rearranging the terms, we have\nIE [ h(α(k+1))] ≤ 2nCν̃k+2n ( 1− 2ν̃ν̃k+2n + ν̃ ν̃k+2n ) = 2nCν̃k+2n ν̃k+2n−ν̃ ν̃k+2n\n≤ 2nCν̃k+2n ν̃k+2n ν̃k+2n+ν̃\n= 2nCν̃(k+1)+2n ,\nwhich finishes the proof."
    }, {
      "heading" : "G. Convergence of BCPFW and BCAFW",
      "text" : "In this section, we prove Theorem 4 that states that the suboptimality error on (3) decreases geometrically in expectation for BCPFW and BCAFW for the iterates at which no block would have a drop step, i.e., when no atom would be removed from the active sets. We follow closely the notation and the results from Lacoste-Julien & Jaggi (2015) where the global linear convergence of the (batch) pairwise FW (PFW) and away-step FW (AFW) algorithms was shown. The main insight to get our result is that the “pairwise FW gap” decomposes also as a sum of block gaps. We give our result for the following more general setting (the block-separable analog of the setup in Appendix F of Lacoste-Julien & Jaggi (2015)):\nmin α∈M\nf(α) with f(α) := q(Aα) + b>α\nand M =M(1) × · · · ×M(n), (34)\nwhere q is a strongly convex function, and M(i) := conv(A(i)) for each i, where A(i) ⊆ Rmi is a finite set of vectors (called atoms). In other words, each M(i) is a polytope. For the example of the dual SSVM objective (3), q(·) := λ2 ‖ · ‖\n2 and A(i) are the corners of a probability simplex in mi := |Yi| dimensions. Suppose that we maintain an active set Si for each block (as in the BCPFW algorithm). We first relate the batch PFW direction with the block PFW directions, as well as their respective batch and blockwise PFW gaps (the PFW gap is replacing the FW gap (5) in the analysis of PFW). Definition 7 (Block PFW gaps). Consider the problem (34) and suppose that the pointα has each of its blockα(i) with current active set Si ⊆ A(i).10 We define the corresponding batch PFW gap at α with active set S := S1×· · ·×Sn as:\ngPFW(α;S) := max s∈M,v∈S 〈−∇f(α) , s− v〉 (35)\n= max s∈M,v∈S ∑ i 〈−∇(i)f(α) , s(i) − v(i)〉\n= ∑ i\nmax s(i)∈M(i) v(i)∈Si\n〈−∇(i)f(α) , s(i) − v(i)〉\n︸ ︷︷ ︸ =:\n∑ i\ngPFWi (α ; Si), (36)\nwhere gPFWi is the PFW gap for block i. We recognize that the maximizing arguments for gPFWi are the FW corner s(i) and the away corner v(i) for block i that one would obtain when running BCPFW on this block.\nWe note that by maintaining independent active sets Si for each block, the number of potential away corner combinations is exponential in the number of blocks, yielding\n10That is, α(i) is a convex combination of all the elements of S(i) with non-zero coefficients.\nmany more possible directions of movement than in the batch PFW algorithm where the number of away corners is bounded by the number of iterations. Moreover, suppose that we have an explicit expansion for each block α(i) as a convex combination of atoms in the active set: α(i) = ∑ v(i)∈Si βi(v(i))v(i), where βi(v(i)) > 0 is the convex combination coefficient associated with atom v(i). Then we can also express α as an explicit convex combination of the (exponential size) active set S as follows: α = ∑ v∈S β(v)v, where β(v) := ∏n i=1 βi(v(i)).\nWe can now prove an analog of the expected block descent lemma (Lemma 4 for BCFW) in the case of BCPFW and BCAFW. For technical reasons, we need a slightly different block curvature constantCA (i)f (cf. Eq. (26) in LacosteJulien & Jaggi (2015)).\nLemma 8 (Expected BCPFW descent lemma). Consider running the BCPFW algorithm on problem (34). Let α(k) be the current iterate, and suppose that Sj is the current active set for each blockα(k)(j) . Let S\n(k) := S1×· · ·×Sn be the current (implicit) active set forα(k). Suppose that there is no drop set at α(k), that is, that for each possible block i that could be picked at this stage, the PFW step with linesearch on block i will not have its step size truncated (we say that the line-search will succeed). Then, conditioned on the current state, in expectation over the random choice of block i with uniform probability and for any γ ∈ [0, 1], it holds for the next iterate α(k+1) of BCPFW:\nIE [ f(α(k+1)) |α(k),S(k) ] ≤\nf(α(k))− γng PFW(α(k) ; S(k)) + γ\n2\n2nC A⊗ f , (37)\nwhere CA⊗f := ∑n i=1 C A (i) f is the total (away) curvature constant, and where CA (i)f is defined as in Definition 2, but allowing the reference point α(i) in (13) to be any point v(i) ∈ M(i) instead, thus allowing a pairwise FW direction s[i] − v[i] to be used in its definition. Moreover, (37) also holds for BCAFW (again under the assumption of no drop step), but with an extra 1/2 factor in front of gPFW(α(k) ; S(k)) in the bound.\nProof. Let block i be the chosen one that defined α(k+1) and let αγ := α(k) + γ(s[i] − v[i]), where s(i) ∈ M(i) is the FW corner on block i with s[i] ∈ Rm its zero-padded version, and similarly v(i) ∈ Si is the chosen away corner on block i. By assumption, we have that the linesearch succeeds, i.e., the minimum of minγ∈[0,γmax] f(αγ) is achieved for γ∗ < γmax, where γmax is the maximum step size for this block for the PFW direction (this is because the optimal step size for the line-search cannot be truncated at γmax, as otherwise it would be a drop step). As f is a convex function, this means that f(α(k+1)) = minγ∈[0,γmax] f(αγ) = minγ≥0 f(αγ) (removing inactive constraints does not change its minimum). By definition\nof CA (i)f , we thus have for any γ ∈ [0, 1]:\nf(α(k+1)) ≤ f(αγ) = f(α(k) + γ(s[i] − v[i]))\n≤ f(α(k))+γ〈∇(i)f(α), s(i)−v(i)〉+ γ 2 2 C A (i) f = f(α(k))− γ gPFWi (α(k);Si) + γ2 2 C A (i) f . (38)\nTaking the expectation of the bound with respect to i, conditioned on α(k) and S(k), yields (37) by using the block-decomposition relationship (36) in the definition of gPFW(α(k) ; S(k)). This completes the proof for BCPFW.\nIn the case of BCAFW, let di be the chosen direction for block i (either a FW direction or an away direction). Then since di is chosen to maximize the inner product with −∇(i)f(α(k)), we have 〈−∇(i)f(α(k)),di〉 ≥ 1 2g PFW i (α\n(k) ; Si) (with a similar argument as used to get Eq. (6) in Lacoste-Julien & Jaggi (2015) for AFW). We then follow the same argument to derive (38), but using di instead of (s(i)− v(i)), which gives an extra 1/2 factor as 〈−∇(i)f(α(k)),di〉 is potentially only half of gPFWi (α\n(k) ; Si). Taking again the expectation of (38) completes the proof.\nRemark 9. The important condition that there is no drop step at α(k) in the BCPFW descent lemma 8 is to allow the bound (38) to hold for any γ ∈ [0, 1]. Otherwise, let I be the (non-empty) set of blocks for which there would be a drop step at α(k) and let γI := mini∈I γ (i) max, where γ (i) max is the maximum step size for block i. Then in this case we could only show the bound (38) for γ ≤ γI . But γI could be arbitrarily small,11 and so no significant progress is guaranteed in expectation in this case.\nWe also note that CA (i)f is used instead of C (i) f in the lemma because C(i)f can only be used with a feasible step from α(k), and thus again, the bound would only be valid for γ ≤ γmax (as bigger step sizes can take you outside of M(i)). If the gradient of f is Lipschitz continuous, one can bound CA (i)f ≤ L̃i ( diam‖·‖iM(i) )2 , which is almost the same bound as for C(i)f explained in footnote 6, but with L̃i being the Lipschitz constant of ∇(i)f for variations in the slightly extended domainM(i) + (M(i)−M(i)) (with set addition in the Minkowski sense).\nTheorem 4 (Geometric convergence of BCPFW). Consider running BCPFW (or BCAFW) on problem (34) where q is a strongly convex function andM is a block-separable polytope. Let hk := f(α(k))− f(α∗) be the suboptimality of the iterate k, where α∗ is any optimal solution to (34).\n11Small maximum step sizes happen when the current coordinate value for an away corner is small (perhaps because a small step size was used by the line-search when they were added as a FW corner previously).\nConditioned on any iterate α(k) with active set S(k) such that no block could give a drop set (as defined in the conditions for Lemma 8), then the expected new suboptimality decreases geometrically, that is:\nIE [ hk+1 |α(k),S(k) ] ≤ (1− ρ)hk, (39)\nwith rate:\nρ := 12n min{1, 2 µ̃f CA⊗f } for the BCPFW algorithm, (40)\nρ := 14n min{1, µ̃f CA⊗f } for the BCAFW algorithm, (41)\nwhere CA⊗f := ∑n i=1 C A (i) f is the total (away) curvature constant for problem (34) as defined in Lemma 8, and µ̃f is the generalized strong convexity constant for problem (34) as defined in Eq. (39) of Lacoste-Julien & Jaggi (2015) (µ̃f is strictly greater than zero when q is strongly convex and M is a polytope).\nProof. We first do the argument for BCPFW. Let gk := gPFW(α(k) ; S(k)), and notice that gk ≥ hk always. Because we assume that there is no drop step at α(k), we can use the expected BCPFW descent lemma 8. By subtracting f(α∗) on both side of the descent inequality (37), we get (for any γ ∈ [0, 1]):\nIE [ hk+1 |α(k),S(k) ] ≤ hk − γngk + γ2 2nC A⊗ f . (42)\nWe can minimize the RHS of (42) with γ∗ = gk CA⊗f . If gk > C A⊗ f (i.e. γ ∗ > 1), then use γ = 1 in (42) to get:\nIE [ hk+1 |α(k),S(k) ] ≤ hk − 12ngk ≤ (1− 1 2n )hk. (43)\nThis gives a geometric rate of ρ = 12n . So now suppose that gk ≤ CA⊗f (so that γ∗ ≤ 1); putting γ = γ∗ in (42), we get:\nIE [ hk+1 |α(k),S(k) ] ≤ hk − 1\n2nCA⊗f gk\n2. (44)\nWe now use the key relationship between the suboptimality hk and the PFW gap gk derived in inequality (43) of Lacoste-Julien & Jaggi (2015) (which is true for any function f by definition of µ̃f if we allow it to be zero):\nhk ≤ gk 2\n2µ̃f . (45)\nSubstituting (45) into (44), we get:\nIE [ hk+1 |α(k),S(k) ] ≤ (1− µ̃f\nnCA⊗f )hk, (46)\nwhich gives the ρ = µ̃f/nCA⊗f rate. Taking the worst rate of (43) and (46) gives the rate (40), completing the proof for BCPFW.\nIn the case of BCAFW, Lemma 8 yields the inequality (42) but with an extra 1/2 factor in front of gk. Re-using the same argument as above, we get a rate of ρ = 1/4n when γ∗ > 1, and ρ = µ̃f/4nCA⊗f when γ\n∗ ≤ 1, showing (41) as required.\nFinally, the fact that µ̃f > 0 when q is µ-strongly convex and M is a polytope comes from the lower bound given in Theorem 10 of Lacoste-Julien & Jaggi (2015) in terms of the pyramidal width ofM (a strictly positive geometric quantity for polytopes), and the generalized strong convexity of f as defined in Lemma 9 of Lacoste-Julien & Jaggi (2015). The generalized strong convexity of f is simply µ if f is µ-strongly convex. In the more general case of problem (34) where only q is µ-strongly convex, the generalized strong convexity depends both on µ and the Hoffman constant (Hoffman, 1952) associated with the linear system of problem (34). See Lacoste-Julien & Jaggi (2015) for more details, as well as Lemma 2.2 of Beck & Shtern (2015).\nInterpretation. Theorem 4 only guarantees progress of BCPFW or BCAFW when there would not be any drop step for any block i for the current iterate. For the batch AFW algorithm, one can easily lower bound the number of times that these “good steps” can happen as a drop step reduces the size of the active set and thus cannot happen more than half of the time. On the other hand, in the block coordinate setting, we can be unlucky and always have one block that could give a drop step (while we pick other blocks during the algorithm, this bad block affects the expectation). This means that without a refined analysis of the drop step possibility, we cannot guarantee any progress in the worst case for BCPFW or BCAFW. As a safeguard, one can modify BCPFW or BCAFW so that it also has the option to do a standard BCFW step on a block if it yields better progress on f – this way, the algorithm inherits at least the (sublinear) convergence guarantees of BCFW.\nEmpirical linear convergence. In our experiments, we note that BCPFW always converged empirically, and had an empirical linear convergence rate for the SSVM objective when λ was big enough (q(·) = λ2 ‖ · ‖\n2 for the SSVM objective (3)). See Figure 5 for OCR-large (c) for example. We also tried the modified BCPFW algorithm where a choice is made between a FW step, a pairwise FW step or an away step on a block by picking the one which gives the biggest progress. We did not notice any significant speedup for this modified method.\nOn the dimension of SSVM. Finally, we note that the rate constant ρ in Theorem 4 has an implicit dependence on the dimensionality (in particular, through the pyramidal width ofM). Lacoste-Julien & Jaggi (2015) showed that the largest possible pyramidal width of a polytope in dimension m (for a fixed diameter) is achieved by the probability simplex and is Θ(1/ √ m). For the SSVM in the general form (3), the dimensionality ofM(i) is the number\nof possible structured outputs for input i, which is typically an exponentially large number, and thus the pyramidal width lower bound would be useless in this case. Fortunately, the matrix A (feature map) and vector b (loss function) are highly structured, and thus many α’s are mapped to the same objective value. For a feature mapping ψi(y) representing the sufficient statistics for an energy function associated with a graphical model (as for a conditional random field (Lafferty et al., 2001)), then the SSVM objective is implicitly optimizing over the marginal polytope for the graphical model (Wainwright & Jordan, 2008). More specifically, let Ai be the d × mi submatrix of A associated with example i. Then we can write Ai = BiMi where Mi is a p ×mi marginalization matrix, that is, µ = Miα is an element of the marginal polytope for the graphical model, where p is the dimensionality of the marginal polytope – which is a polynomial number in the size of the graph, rather than exponential. By the affine invariance property of the FW-type algorithms, we can thus instead use the pyramidal width of the marginal polytope for the convergence analysis (and similarly for the Hoffman constant). Lacoste-Julien & Jaggi (2015) conjectured that the pyramidal width of a marginal polytope in dimension pwas also Θ(1/ √ p), thus giving a more reasonable bound for the convergence rate of BCPFW for SSVM."
    }, {
      "heading" : "H. BCFW for SSVM with box constraints",
      "text" : "H.1. Problem with box constraints Problem (1) can be equivalently rewritten as a quadratic program (QP) with an exponential number of constraints:\nmin w, ξ\nλ 2 ‖w‖ 2 + 1n n∑ i=1 ξi (47)\ns.t. 〈w,ψi(y)〉 ≥ L(yi,y)− ξi ∀i, ∀y ∈ Yi where the slack variable ξi measures the surrogate loss for the i-th datapoint. Problem (47) is often referred to as the n-slack structured SVM with margin-rescaling (Joachims et al., 2009, Optimization Problem 2).\nIn this section, we consider the problem (47) with additional box constraints on the parameter vector w:\nmin w, ξ\nλ 2 ‖w‖ 2 + 1n n∑ i=1 ξi (48)\ns.t. 〈w,ψi(y)〉 ≥ L(yi,y)− ξi ∀i, ∀y ∈ Yi, l 4 w 4 u,\nwhere l ∈ Rd and u ∈ Rd denote the lower and upper bounds, respectively, and the symbol “4” is the elementwise “less or equal to” sign. In the following, we assume that the box constraints are feasible, i.e., l 4 u. Note that the following discussion can be directly extended to the case where only some dimension of the weight vector have to respect the box constraints. The Lagrangian of\nproblem (48) can be written as\nL(w, ξ,α,βl,βu) = λ 2 〈w,w〉+ 1 n n∑ i=1 ξi\n+ ∑\ni∈[n],y∈Yi\n1 nαi(y) (−ξi + 〈w,−ψi(y)〉+ Li(y))\n+ λ〈βu,w − u〉+ λ〈βl,−w + l〉 (49)\nwhere βl ∈ Rd and βu ∈ Rd are the dual variables associated with the lower and upper bound constraints, respectively. From the KKT conditions, we obtain\nw = Aα− (βu − βl), (50)∑ y∈Yi αi(y) = 1 ∀i ∈ [n]. (51)\nFinally, the dual of problem (48) (here written in a minimization form) can be written as follows:\nmin α∈Rm α<0\nf(α,βl,βu) := λ 2 ∥∥Aα− (βu − βl)∥∥2 − bTα + λ(βTuu− βTl l)\ns.t. ∑ y∈Y αi(y) = 1 ∀i ∈ [n],\nand βu < 0,βl < 0. (52)\nA modified block optimization method. Ideally, we should optimize f(α,βl,βu) jointly w.r.t. all the dual variables. This task is not directly suitable for the Frank-Wolfe approach as the domain for βl and βu is unbounded. However, joint optimization w.r.t. βl and βu with α kept fixed can be done in closed form. After that, optimization w.r.t.α can be performed using the Frank-Wolfe blockwise approach. Therefore, we resort to optimizing in a blockwise fashion: we iterate either a batch FW or a BCFW step on α with an exact block-update on (βu,βl). As we will see below, this principled approach is similar to a commonly used heuristic of truncating the value of w to make it feasible during an algorithm which works on the dual. In fact, our approach will be equivalent to run FW or BCFW with a truncation makingw(α) feasible after each FW step, but with a change in the optimal step-size computation (line 8 in Algorithm 8 for FW; line 7 in Algorithm 9 for BCFW) due to the different nature of the optimization problem.\nH.2. Optimizing w.r.t βu and βl while fixing α The optimization w.r.t.βu withα andβl fixed can be easily solved in closed form via a simple thresholding operation:\nβ∗u = [Aα+ βl − u]+ . (53)\nThe optimization w.r.t. βl with α and βu fixed is analogous:\nβ∗l = [−Aα+ βu + l]+ . (54)\nAlgorithm 8 Batch Frank-Wolfe algorithm for structured SVM with box constraints\n1: Let v(0) := 0; `(0) := 0 2: w(0) := [v(0)]ul // truncation to the feasible set 3: for k := 0, . . . ,∞ do 4: for i := 1, . . . , n do 5: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(k))\n6: end for 7: Let vs := n∑ i=1 1 λnψi(y ∗ i ) and `s := 1 n n∑ i=1 Li(y ∗ i ) 8: Let γ := λ(v (k)−vs)Tw(k)−`(k)+`s\nλ‖v(k)−vs‖2 and clip to [0, 1]\n9: Update v(k+1) := (1− γ)v(k) + γ vs 10: and `(k+1) := (1− γ)`(k) + γ `s 11: and w(k+1) := [v(k+1)]ul 12: end for\nAlgorithm 9 Block-coordinate Frank-Wolfe algorithm for structured SVM with box constraints\n1: Let v(0) := vi(0) := 0; `(0) := `i(0) := 0; 2: w(0) := [v(0)]ul // truncation to the feasible set 3: for k := 0, . . . ,∞ do 4: Pick i at random in {1, . . . , n} 5: Solve y∗i := argmax\ny∈Yi Hi(y;w\n(k))\n6: Let vs := 1λnψi(y ∗ i ) and `s := 1 nLi(y ∗ i ) 7: Let γ := λ(v (k) i −vs)\nTw(k)−`(k)i +`s λ‖v(k)i −vs‖2 and clip to [0, 1]\n8: Update vi(k+1) := (1− γ)vi(k) + γ vs 9: and `i(k+1) := (1− γ)`i(k) + γ `s\n10: Update v(k+1) := v(k) + vi(k+1) − vi(k) 11: and `(k+1) := `(k) + `i(k+1) − `i(k) 12: Let w(k+1) := [v(k+1)]ul 13: end for\nDenote the p-th variable of βu and βl with βu(p) and βl(p), respectively. For any index p, both βu(p) and βl(p) cannot be nonzero simultaneously, because if one of the constraints is violated (either the upper or the lower bound), then the other constraint must be satisfied. Hence, βu(p) 6= 0 implies βl(p) = 0 and vice versa. Therefore, the final update equations can equivalently be written as\nβ∗u(α) = [Aα− u]+ , (55) β∗l (α) = [−Aα+ l]+ . (56)\nIntroducing v(α) := Aα, we get β∗u = [v − u]+ and β∗l = [−v+ l]+. Hence, the operationw = v− (β∗u−β∗l ) is simply the projection (truncation) of v on the feasible set defined by the upper and lower bounds. In the final algorithm, we maintain v(α) and directly update the primal variables w without updating βu and βl.\nH.3. Batch setting: optimizing w.r.t α with βu and βl fixed\nWhen the variables βu and βl are fixed, the convex problem (52) has a compact domain and so we can use the Frank-Wolfe algorithm on it. In the following, we highlight the differences with the setting without box constraints. We denote by w(α) the truncation of v(α) on the box constraints, i.e.,\nw(α) := v(α)− (β∗u(α)− β∗l (α)) . (57)\nThe derivations below assume that βu and βl are fixed to their optimal values β∗u(α) and β ∗ l (α) for a specific α.\nLinear subproblem. The Frank-Wolfe linear subproblem can be written as\ns = argmin s′∈M\n〈s′,∇αf(α,βl,βu)〉 (58)\nwhere∇αf(α,βl,βu) can be easily computed:\n∇αf(α,βl,βu) = λAT(Aα− (βu − βl))− b = λATw − b. (59)\nAnalogously to the problem without box constraints, the linear subproblem used by the Frank-Wolfe algorithm is equivalent to the loss-augmented decoding subproblem (2). The update of α can be made using the corner s. In what follows, we show that this update can be performed without explicitly keeping the dual variables at the cost of storing the extra vector v.\nThe duality gap. The Frank-Wolfe gap for problem (52) can be written as\ng(α) := max s′∈M 〈α− s′,∇αf(α,βl,βu)〉 = (α− s)T(λAT(Aα− (βu − βl))− b) = λ(v − vs)Tw − bTα+ bTs\nwhere vs := As. Below, we prove that the Frank-Wolfe duality gap g(α) for the problem (52) when βu and βl are fixed at their current optimal value for the current α equals to a Lagrange duality gap, analogously to the case without box constraints (Lacoste-Julien et al., 2013, Appendix B.2).12\nProof. Consider the difference between the primal objective of (48) at w := Aα − (βu − βl) with the optimal slack variables ξ and the dual objective of (52) at α (in the\n12We stress that this relationship is only valid for the pairw = w(α) in the primal, and βu = β∗u(α),βl = β∗l (α) in the dual.\nmaximization form). We get\ngLag.(w,α) = λ 2w Tw + 1n n∑ i=1 H̃i(w)\n− ( bTα− λ2w Tw − λ(βTuu− βTl l) )\n= λwTw − bTα+ 1n n∑ i=1 max y∈Yi Hi(y;w)\n+ λ(βTuu− βTl l) .\nRecalling\n1 n n∑ i=1 max y∈Yi Hi(y;w) = max s′∈M −s′T∇αf(α,βl,βu)\n= −sT∇αf(α,βl,βu),\nwe can write\ngLag.(w,α) = λw T(Aα− (βu − βl))− bTα − sT∇αf(α,βl,βu) + λ(βTuu− βTl l)\n= (λwTA− bT)α− sT∇αf(α,βl,βu) − λ(wTβu −wTβl) + λ(βTuu− βTl l)\n= (α− s)T∇αf(α,βl,βu) + λ(βTu (u−w)− βTl (l−w)).\nAs we assumed that βu = β∗u(α) and βl = β ∗ l (α), we have β∗Tu (u−w) = 0 and β∗Tl (l−w) = 0, and thus\ngLag(w,α,β ∗ u,β ∗ l ) = g(α).\nLine-Search. Line search can be performed efficiently using γopts := 〈α−s,∇f(α)〉 λ‖A(α−s)‖2 = g(α) λ‖v−vs‖2 .\nAlgorithm. Algorithm 8 contains the batch Frank-Wolfe algorithm with box constraints. The main idea consists in maintaining the vector v := Aα in order to perform all the updates of α using only the primal variables. Optimization w.r.t. βl and βu corresponds to the truncation of v. Given these variables, the gap and the optimal step size are easy to compute.\nH.4. The block-coordinate setting Algorithm 9 describes the block-coordinate version of the Frank-Wolfe algorithm with box constraints. The algorithm is obtained from the batch version (Algorithm 8) in exactly the same way as BCFW (Algorithm 1) is obtained from the batch Frank-Wolfe method (Lacoste-Julien et al., 2013, Algorithm 2)."
    }, {
      "heading" : "I. Dataset description",
      "text" : "In our experiments, we use four structured prediction datasets: OCR (Taskar et al., 2003) for character recognition; CoNLL (Tjong Kim Sang & Buchholz, 2000) for text chunking; HorseSeg (Kolesnikov et al., 2014) for binary image segmentation; LSP (Johnson & Everingham, 2010) for pose estimation. In this section, we provide the description of the datasets and the corresponding models. Table 1 summarizes quantitative statistics for all the datasets. For the OCR and CoNLL datasets, the features and models described below are exactly the same as used by LacosteJulien et al. (2013); we give a detailed description for reference. For HorseSeg and LSP, we had to build the models ourselves from previous work referenced in the relevant section.\nI.1. OCR The Optical Character Recognition (OCR) dataset collected by Taskar et al. (2003) poses the task of recognizing English words from sequences of handwritten symbols represented by binary images. The average length of sequences equals 7.6 symbols. For a sequence of length T , the input feature representation x consists of T binary images of size 16 × 8. The output object y is a sequence of length T with each symbol taking 26 possible values.\nThe OCR dataset contains 6, 877 words. In the small version, 626 words are used for training and the rest for testing. In the large version, 6, 251 words are used for training and the rest for testing.\nThe prediction model is a chain. The feature map φ(x,y) contains features of three types: emission, transition and bias. The 16× 8× 26 emission features count the number of times along the chain a specific position of the 16 × 8 binary image equals 1 when associated with a specific output symbol. The 26 × 26 transition features count the number of times one symbol follows another. The 26 × 3 bias features represent three biases for each element of the output alphabet: one model bias, and a bias for when the letter appears at the beginning or at the end of the sequence.\nAs the structured error between output vectors L(yi,y), the OCR dataset uses the Hamming distance normalized by the length of the sequences. The loss-augmented structured score Hi(y;w) is a function with unary and pairwise potentials defined on a chain and is exactly maximized with the dynamic programming algorithm of Viterbi (1967).\nI.2. CoNLL The CoNLL dataset released by Tjong Kim Sang & Buchholz (2000) poses the task of text chunking. Text chunking, also known as shallow parsing (Sha & Pereira, 2003), consists in dividing the input text into syntactically related non-overlapping groups of words, called phrase or chunks. The task of text chunking can be cast as a sequence labeling where a sequence of labels y is predicted from an input sequence of tokens x. For a given token xt (a word with its corresponding part-of-speech tag), the associated label yt gives the type of phrase the token belongs to, i.e., says whether or not it corresponds to the beginning of a chunk, or encodes the fact that the token does not belong to a chunk.\nThe CoNLL dataset contains 8, 936 training English sentences extracted from the Wall Street Journal part of the Penn Treebank II (Marcus et al., 1993). Each output label yt can take up to 22 different values.\nWe use the feature mapφ(x,y) proposed by Sha & Pereira (2003). First, for each position t of the input sequence x, we construct a unary feature representation, containing the local information. We start with extracting several attributes representing the words and the part-of-speech tags at the positions neighboring to t.13 Each attribute is encoded with an indicator vector of length equal to either the dictionary size or the number of part-of-speech tags. We concatenate these vectors to get a unary feature representation, which is a sparse binary vector of dimensional-\n13We extract the attributes with the CRFsuite library (Okazaki, 2007) and refer to its documentation for the exact list of attributes: http://www.chokkan.org/software/ crfsuite/tutorial.html.\nity 74, 658. Note that these representation can be precomputed outside of the training process.\nGiven a labeling y and the unary representations, the feature map φ is constructed by concatenating features of three types (as in the chain model for OCR): emission, transition and bias. The 74, 658 × 22 emission features count the number of times each coordinate of the unary representation of token xt is nonzero and the corresponding output variable yt is assigned a particular value. The transition map of size 22 × 22 encodes the number of times one label follows another in the output y. The 22 × 3 bias features encode biases for all the possible values of the output variables and, specifically, biases for the first and the last variables.\nAs in the OCR task, the structured error L(yi,y) is the normalized Hamming distance and thus the max-oracle can be efficiently implemented using the dynamic programming algorithm of Viterbi (1967).\nI.3. HorseSeg The HorseSeg dataset14 released by Kolesnikov et al. (2014) poses the task of object/background segmentation of images containing horses, i.e., assigning a label “horse” or “background” to each pixel of the image. HorseSeg contains 25, 438 training images, 147 of which are manually annotated, 5, 974 annotations are constructed from object bounding boxes by the automatic method of Guillaumin et al. (2014), while the remaining 19, 317 annotations were constructed by the same method but without any human supervision. The test set of HorseSeg consists of 241 images with manual annotations. In our experiments, we use training sets of three different sizes: 147 images for HorseSegsmall, 6, 121 images for HorseSeg-medium and 25, 438 for HorseSeg-large.\nIn addition to images and their pixel-level annotations, Kolesnikov et al. (2014) released15 oversegmentations (superpixels) of the images precomputed with the SLIC algorithm (Achanta et al., 2012) and the unary features of each superpixel computed similarly to the work of Lempitsky et al. (2011). On average, each image contains 147 superpixels. The 1, 969 unary features include 1 constant feature, 512-bin histograms of densely sampled visual SIFT words (Lowe, 2004), 128-bin histograms of RGB colors, 16-bin histograms of locations (each pixel of a region of interest is matched to a cell of the 4× 4 uniform grid). The three aforementioned histograms are computed on the superpixels themselves, on the superpixels together with their neighboring superpixels, and on the second-order neighborhoods.\nFor each pair of adjacent superpixels, we construct 100\n14https://pub.ist.ac.at/˜akolesnikov/ HDSeg/HDSeg.tar\n15https://pub.ist.ac.at/˜akolesnikov/ HDSeg/data.tar\npairwise features: a constant feature; and quantities exp (−ηdpq) where dpq is a χ2-distance between 9 pairs of the corresponding histograms of each type for neighbors p and q, and η is a parameter taking 11 values from the set 2−5, 2−4, . . . , 25.\nThe structured feature map is defined in such a way that the corresponding structured score function contains unary and pairwise Potts potentials\n〈w,φ(xi,y)〉 = ∑ p∈Vi 〈wU ,xUi,p〉([yp = 1]− [yp = 0])\n+ ∑\n{p,q}∈Ei\n〈wP ,xPi,pq〉[yp 6= yp]\nwhere the vector xi = ((xUi,p)p∈Vi , (x P i,pq){p,q}∈Ei) denotes all the features of image i, the vector y = (yp)p∈Vi ∈ {0, 1}Vi is a feasible labeling, the set Vi is the set of the superpixels of the image i, and the set Ei represents the adjacency graph.\nThe structured error is measured with a Hamming loss with class-normalized penalties\nL(yi,y) = ∑ p∈Vi ωyi,p [yi,p 6= yp]\nwhere yi = (yi,p)p∈Vi is the labeling of superpixels closest to the ground-truth annotation and the weights ω0 and ω1 are proportional to the ground-truth area of each class.\nThe loss-augmented score function\nL(yi,y)− 〈w,φ(xi,y)〉\nis a discrete function defined w.r.t. a cyclic graph and can be maximized in polynomial time when it is supermodular. By construction, all our pairwise features are nonnegative, so we can ensure supermodularity by adding positivity constraints on the weights corresponding to the pairwise features wP < 0. The version of BCFW with positivity constraints is described in Appendix H.\nThe discrete optimization problem arising in the maxoracle is solved by the min-cut/max-flow algorithm of Boykov & Kolmogorov (2004). The running time or the max-flow is small compared to the operations with the features required to compute the potentials.\nI.4. LSP The Leeds Sports Pose (LSP) dataset introduced by (Johnson & Everingham, 2010) poses the tasks of full body pose estimation from still images containing sports activities. Based on the input image with a centered prominent person, the task is to predict the locations of 14 body-parts (joints), e.g., “left knee” or “right ankle”.\nWe cast the task of pose estimation as a structured prediction problem and build our model based on the work\nof Chen & Yuille (2014), which is one of the state-of-theart methods for pose estimation. First, we construct an acyclic graph where the nodes p ∈ V correspond to the different body-parts. The set of body parts is extended from the original 14 parts of interest by the midway points to get the 26 nodes of the graph. Second, the graph is converted into the directed one by utilizing the arcs of both orientations for each original edge. We denote the resulting graph by G = (V, ~E). In the model of Chen & Yuille (2014), each node p ∈ V has a variable lp ∈ P ⊂ R2 denoting the spatial position of the corresponding joint that belongs to a finite set of possibilities P; each arc (p, q) ∈ ~E has a variable tpq ∈ T = {1, . . . , 13} representing the type of spacial relationship between the two nodes. The output variable y is constructed by concatenating the unary and pairwise variables y = ( (lp)p∈V , (tpq)(p,q)∈~E ) .\nThe structure score function is a function of discrete variables lp and tpq that is defined w.r.t. the graph G:\n〈w,φ(xi,y)〉 = ∑ p∈V wU,pφ U p (Ii, lp)\n+ ∑\n(p,q)∈E\nwT,pqφ P pq(Ii, lp, tpq)\n+ ∑\n(p,q)∈E\n〈wP,pq,tpq ,∆(lp − lq − rtpqpq )〉.\nHere, the input xi = ( Ii, (r t pq) t∈T (p,q)∈~E ) consists of the original image Ii and the mean relative positions rtpq ∈ R2 of each type of spatial relationship corresponding to each arc (p, q) ∈ ~E . Functions φUp (Ii, ·) and φPpq(Ii, ·, ·) compute the scores for each possible value of the discrete variables lp and (lp, tpq), respectively. The vector-valued function ∆(l) = (l1, l21, l2, l 2 2), l ∈ R2, measures different types of mismatch between the preferred relative displacement rtpqpq and the displacement lp − lq coming from the labeling y. The vector w =( (wU,p)p∈V , (wT,pq)(p,q)∈~E , (wP,pq,t) t∈T (p,q)∈~E ) is the joint vector of parameters learned by structured SVM. Overall, this setup has 2, 676 parameters.\nDisplacements rtpq and functions φ U p , φ P are computed at the preprocessing stage of the structured SVM. We follow Chen & Yuille (2014) and obtain the displacements rtpq with the K-means clustering of the displacements of the training set. Functions φUp , φ\nP consists in a Convolutional Neural Network (CNN) and are also learned from the training set. We refer the reader to the work of Chen & Yuille (2014) and their project page16 for further details. Note that the last training stage of Chen & Yuille (2014) is\n16http://www.stat.ucla.edu/˜xianjie.chen/ projects/pose_estimation/pose_estimation. html\ndifferent from ours, i.e., it consists in binary SVM on the carefully sampled sets of positive and negative examples.\nTo run SSVM, we define the structured error L(yi,y) as a decomposable function w.r.t. the positions of the joints\nL(yi,y) = 1 |V| ∑ p∈V max ( 1, ‖li,p−lp‖22 s2i ) where li,p belongs to the ground-truth labeling yi and lp belongs to the labeling y. The quantity si ∈ R is the scaling factor and is defined by the distance between the left shoulder and the right hip in the ground-truth labeling yi. Similarly to Osokin & Kohli (2014), the complex dependence of the loss on the ground-truth labeling does not influence the complexity of the max oracle.\nFor the defined structured score and loss, the optimization problem of the max oracle can be exactly solved by the max-sum belief propagation algorithm on an acyclic graph with messages computed with the generalized distance transform (GDT) (Felzenszwalb & Huttenlocher, 2005). The usage of GDTs allows to significantly reduce the oracle running time, but requires positivity constraints on the connection weights wP,pq,t. BCFW with positivity constraints is described in Appendix H.\nThe resulting max oracle is quite slow (2 seconds per image) even when the belief propagation algorithm is optimized with GDTs. Slow running time made it intractable for us to run the experiments on the full original training set consisting of 1, 000 images. We use only the first 100 images and refer to this dataset as LSP-small. However, both the CNN training and clustering of the displacements were done on the original training set."
    }, {
      "heading" : "J. Full experimental evaluation: comparing BCFW variants",
      "text" : "In Figures 5 and 6, we give the detailed results of the experiments described in section 5.1. As a reminder, we are comparing different methods (caching vs no caching, gap sampling vs uniform sampling, pairwise FW steps vs regular FW steps) on different datasets in three main regimes w.r.t. λ. For each dataset, we use the good value of λ (“good” meaning the smallest possible test error) together with its smaller and larger values. The three regimes are displayed in the middle (b), top (a) and bottom (c) of each subfigure, respectively."
    }, {
      "heading" : "K. Full experimental evaluation: regularization path",
      "text" : "In this section, we evaluate the regularization path method proposed in Section 4. Our experiments are organized in three stages. First, we choose the κ parameter for Algorithm 7 computing the ε-approximate regularization path. Second, we define and evaluate the heuristic regularization path. Finally, we compare both ε-approximate and heuristic paths against the grid search on multiple datasets.\nε-approximate path. Algorithm 7 for computing the εapproximate regularization path has one parameter κ controlling how large are the induction steps in terms of λ. This parameter provides the trade-off between the number of breakpoints and the accuracy of optimization for each breakpoint. We explore this trade-off in Figure 7a. For several values of κ, we report the cumulative number of effective passes (bottom plots) and cumulative time (top plots) required to get ε-approximate solution for each λ. We report both plots for the two methods used as the SSVM solver: BCPFW with gap sampling and caching; BCFW with gap sampling and without caching. We conclude that both too small (< 0.5) and too large (≈ 1) values of parameter κ result in slower methods, but overall the method is not too sensitive to κ. In all remaining experiments, we use κ = 0.9 when computing the ε-approximate path.\nHeuristic path. When computing an ε-approximate regularization path, Algorithm 7 needs to perform at least one pass over the dataset at each breakpoint to check the convergence criterion, i.e., that the gap is not larger than ε. When having many breakpoints, this extra pass can be of significant cost, especially for large values of λ where the SSVM solver converges very quickly. However, in practice, we observe that the stale gap estimates are often good enough to determine convergence and actually checking the convergence criterion is not necessary. At the cost of loosing the guarantees, we can expect computational speedup. We refer to the result of Algorithm 7 when the SSVM solver (at line 22) uses stale gap estimates instead of the exact gaps to check the convergence as a heuristic path. In\nthe case of heuristic path, the parameter κ provides a tradeoff between the running time and accuracy of the path. In Figure 7b, we illustrate this trade-off on the OCR-small dataset. For several values of κ, we report the true value of the duality gap at each breakpoint (the SSVM solver terminates when the stale gap estimate is below κε, ε = 0.1) and the cumulative time. We use BCFW + gap sampling and BCPFW + gap sampling + cache as the SSVM solvers. For large κ, we observe that the solutions for small λ are not ε-approximate and that the method in this regime requires less number of passes over the data, i.e., runs faster. In what follows we always use κ = 0.7 for the heuristic path as it is the largest value providing accurate enough results.\nComparison of paths against grid search. Finally, we compare the regularization path methods to the standard grid search approach. We define a grid of 31 values chosen to cover all the values of λ used in the experiments of Section J (215, 214, . . . , 2−15). For each value of the grid, we optimize the SSVM objective optimize either independently or by warm-starting from the nearest larger value. We have considered two variants of warm start: keeping the primal variables and rescaling the dual variables, or keeping the dual variables and rescaling the primal variables. In our experiments, we do not notice any consistent difference between the two approaches, so we only use the first type of warm start.\nFigure 8 presents the results on the four datasets: HorseSeg-small (Figure 8a), OCR-small (Figure 8b), HorseSeg-medium (Figure 8c), OCR-large (Figure 8d).\nFor both the OCR-large and HorseSeg-medium datasets, neither heuristic, nor ε-approximate path methods did not reach their stopping criterion and were terminated at the time limit of 24 hours. In the case of OCR-large, the regularization path reached a value of λ smaller than the lower limit of the predefined grid, i.e., 2−15. In the case of HorseSeg-medium, the grid search methods did not reach the lower bound of the grid and were terminated at the 24- hour time limit.\nValue of 6 10 -310 -210 -110 010 110 210 310 4\nT im\ne in\n( s)\n10 1\n10 2\n10 3\n5=0.10 5=0.50 5=0.70 5=0.90 5=0.95\nValue of 6 10 -310 -210 -110 010 110 210 310 4\nT im\ne in\n( s)\n10 1\n10 2\n10 3\n5=0.10 5=0.50 5=0.70 5=0.90 5=0.95\nValue of 6 10 -310 -210 -110 010 110 210 310 4\nN um\np as\nse s\n10 2\n200 300 400\n5=0.10 5=0.50 5=0.70 5=0.90 5=0.95\nValue of 6 10 -310 -210 -110 010 110 210 310 4\nN um\np as\nse s\n10 2\n200 300 400\n5=0.10 5=0.50 5=0.70 5=0.90 5=0.95\nBCFW + gap sampling BCPFW + gap sampling + caching\n(a) ε-approximate regularization paths. For different values of κ, we report the cumulative number of effective passes (bottom) and the cumulative time (top) required to get an ε-approximate solution for each λ. We analyze the two methods: BCFW + gap sampling (left) and BCPFW + gap sampling + caching (right).\nValue of 6 10 -410 -310 -210 -110 010 110 210 310 4\nT im\ne in\n( s)\n10 0\n10 1\n10 2\n300 600\n5=0.50 5=0.70 5=0.90 5=0.99\nValue of 6 10 -410 -310 -210 -110 010 110 210 310 4\nT im\ne in\n( s)\n10 0\n10 1\n10 2\n300 600\n5=0.50 5=0.70 5=0.90 5=0.99\nValue of 6 10 -410 -310 -210 -110 010 110 210 310 4\nD ua\nlit y\nga p\n10 -2\n0.05\n10 -1\n0.2\nTarget 0 5=0.50 5=0.70 5=0.90 5=0.99\nValue of 6 10 -410 -310 -210 -110 010 110 210 310 4\nD ua\nlit y\nga p\n10 -2\n0.05\n10 -1\n0.2\nTarget 0 5=0.50 5=0.70 5=0.90 5=0.99\nBCFW + gap sampling BCPFW + gap sampling + caching\n(b) Heuristic regularization paths. For different values of κ, we report the cumulative time required to compute a path until λ (top) along with the true value of the duality gap obtained for each λ (bottom).We analyze the two methods: BCFW + gap sampling (left) and BCPFW + gap sampling + caching (right).\nFigure 7. Experiment exploring the effect of κ for regularization paths computed on the OCR-small dataset.\nSupplementary References Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P.,\nand Süsstrunk, S. SLIC superpixels compared to stateof-the-art superpixel methods. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 34 (11):2274–2282, 2012.\nAllgower, E. and Georg, K. Continuation and path following. Acta Numerica, 2:1–64, 1993.\nBeck, A. and Shtern, S. Linearly convergent away-step conditional gradient for non-strongly convex functions. arXiv:1504.05002v1, 2015.\nCrammer, K. and Singer, Y. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research (JMLR), 2:265–292, 2001.\nd’Aspremont, A., Guzmán, C., and Jaggi, M. An optimal affine invariant smooth minimization algorithm. arXiv:1301.0465v2, 2013.\nGärtner, B., Jaggi, M., and Maria, C. An exponential lower bound on the complexity of regularization paths. Journal of Computational Geometry, 3(1):168–195, 2012.\nGiesen, J., Mueller, J., Laue, S., and Swiercy, S. Approximating concavely parameterized optimization problems. In Advances in Neural Information Processing Systems (NIPS), 2012.\nGuillaumin, M., Küttel, D., and Ferrari, V. ImageNet auto-annotation with segmentation propagation. International Journal of Computer Vision (IJCV), 110(3):328– 348, 2014.\nHastie, T., Rosset, S., Tibshirani, R., and Zhu, J. The entire regularization path for the support vector machine. Journal of Machine Learning Research (JMLR), 2004.\nHoffman, A. J. On approximate solutions of systems of linear inequalities. Journal of Research of the National Bureau of Standards, 49(4):263–265, 1952.\nJaggi, M. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Proceedings of the International Conference on Machine Learning (ICML), 2013.\nJun-Tao, L. I. and Ying-Min, J. I. A. Huberized multiclass support vector machine for microarray classification. Acta Automatica Sinica, 36(3):399–405, 2010.\nKarasuyama, M. and Takeuchi, I. Suboptimal solution path algorithm for support vector machine. In Proceedings of the International Conference on Machine Learning (ICML), 2011.\nLafferty, J. D., McCallum, A., and Pereira, F. C. N. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning (ICML), 2001.\nLee, Y. and Cui, Z. Characterizing the solution path of multicategory support vector machines. Statistica Sinica, 16: 391–409, 2006.\nLee, Y., Lin, Y., and Wahba, G. Multicategory support vector machines, theory, and application to the classification of microarray data and satellite radiance data. Journal of the American Statistical Association, 99:67–81, 2004.\nLempitsky, V., Vedaldi, A., and Zisserman, A. A Pylon model for semantic segmentation. In Advances in Neural Information Processing Systems (NIPS), 2011.\nLowe, D. G. Distinctive image features from scaleinvariant keypoint. International Journal of Computer Vision (IJCV), 60(2):91–110, 2004.\nMarcus, M. P., Marcinkiewicz, M. A., and Santorini, B. Building a large annotated corpus of English: The Penn treebank. Computational linguistics, 19(2):313–330, 1993.\nOkazaki, N. CRFsuite: a fast implementation of conditional random fields (CRFs), 2007. URL http:// www.chokkan.org/software/crfsuite/.\nOsokin, A. and Kohli, P. Perceptually inspired layoutaware losses for image segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), 2014.\nSentelle, C. G., Anagnostopoulos, G. C., and Georgiopoulos, M. A simple method for solving the SVM regularization path for semidefinite kernels. IEEE Transactions on Neural Networks and Learning Systems, 2015.\nSha, F. and Pereira, F. Shallow parsing with conditional random fields. In NAACL, 2003.\nTibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, 1996.\nWainwright, M. J. and Jordan, M. I. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1–305, 2008.\nWang, L. and Shen, X. Multi-category support vector machines, feature selection and solution path. Statistica Sinica, 16:617–633, 2006."
    } ],
    "references" : [ {
      "title" : "Continuation and path following",
      "author" : [ "E. Allgower", "K. Georg" ],
      "venue" : "Acta Numerica,",
      "citeRegEx" : "Allgower and Georg,? \\Q1993\\E",
      "shortCiteRegEx" : "Allgower and Georg",
      "year" : 1993
    }, {
      "title" : "Linearly convergent away-step conditional gradient for non-strongly convex functions",
      "author" : [ "A. Beck", "S. Shtern" ],
      "venue" : null,
      "citeRegEx" : "Beck and Shtern,? \\Q2015\\E",
      "shortCiteRegEx" : "Beck and Shtern",
      "year" : 2015
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Crammer and Singer,? \\Q2001\\E",
      "shortCiteRegEx" : "Crammer and Singer",
      "year" : 2001
    }, {
      "title" : "An optimal affine invariant smooth minimization algorithm",
      "author" : [ "A. d’Aspremont", "C. Guzmán", "M. Jaggi" ],
      "venue" : null,
      "citeRegEx" : "d.Aspremont et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "d.Aspremont et al\\.",
      "year" : 2013
    }, {
      "title" : "An exponential lower bound on the complexity of regularization paths",
      "author" : [ "B. Gärtner", "M. Jaggi", "C. Maria" ],
      "venue" : "Journal of Computational Geometry,",
      "citeRegEx" : "Gärtner et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gärtner et al\\.",
      "year" : 2012
    }, {
      "title" : "Approximating concavely parameterized optimization problems",
      "author" : [ "J. Giesen", "J. Mueller", "S. Laue", "S. Swiercy" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Giesen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Giesen et al\\.",
      "year" : 2012
    }, {
      "title" : "ImageNet auto-annotation with segmentation propagation",
      "author" : [ "M. Guillaumin", "D. Küttel", "V. Ferrari" ],
      "venue" : "International Journal of Computer Vision (IJCV),",
      "citeRegEx" : "Guillaumin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Guillaumin et al\\.",
      "year" : 2014
    }, {
      "title" : "The entire regularization path for the support vector machine",
      "author" : [ "T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Hastie et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2004
    }, {
      "title" : "On approximate solutions of systems of linear inequalities",
      "author" : [ "A.J. Hoffman" ],
      "venue" : "Journal of Research of the National Bureau of Standards,",
      "citeRegEx" : "Hoffman,? \\Q1952\\E",
      "shortCiteRegEx" : "Hoffman",
      "year" : 1952
    }, {
      "title" : "Revisiting Frank-Wolfe: Projection-free sparse convex optimization",
      "author" : [ "M. Jaggi" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Jaggi,? \\Q2013\\E",
      "shortCiteRegEx" : "Jaggi",
      "year" : 2013
    }, {
      "title" : "Huberized multiclass support vector machine for microarray classification",
      "author" : [ "L.I. Jun-Tao", "J.I.A. Ying-Min" ],
      "venue" : "Acta Automatica Sinica,",
      "citeRegEx" : "Jun.Tao and Ying.Min,? \\Q2010\\E",
      "shortCiteRegEx" : "Jun.Tao and Ying.Min",
      "year" : 2010
    }, {
      "title" : "Suboptimal solution path algorithm for support vector machine",
      "author" : [ "M. Karasuyama", "I. Takeuchi" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Karasuyama and Takeuchi,? \\Q2011\\E",
      "shortCiteRegEx" : "Karasuyama and Takeuchi",
      "year" : 2011
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J.D. Lafferty", "A. McCallum", "F.C.N. Pereira" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Characterizing the solution path of multicategory support vector machines",
      "author" : [ "Y. Lee", "Z. Cui" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Lee and Cui,? \\Q2006\\E",
      "shortCiteRegEx" : "Lee and Cui",
      "year" : 2006
    }, {
      "title" : "Multicategory support vector machines, theory, and application to the classification of microarray data and satellite radiance data",
      "author" : [ "Y. Lee", "Y. Lin", "G. Wahba" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Lee et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2004
    }, {
      "title" : "A Pylon model for semantic segmentation",
      "author" : [ "V. Lempitsky", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Lempitsky et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lempitsky et al\\.",
      "year" : 2011
    }, {
      "title" : "Distinctive image features from scaleinvariant keypoint",
      "author" : [ "D.G. Lowe" ],
      "venue" : "International Journal of Computer Vision (IJCV),",
      "citeRegEx" : "Lowe,? \\Q2004\\E",
      "shortCiteRegEx" : "Lowe",
      "year" : 2004
    }, {
      "title" : "Building a large annotated corpus of English: The Penn treebank",
      "author" : [ "M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "CRFsuite: a fast implementation of conditional random fields (CRFs)",
      "author" : [ "N. Okazaki" ],
      "venue" : "URL http:// www.chokkan.org/software/crfsuite/",
      "citeRegEx" : "Okazaki,? \\Q2007\\E",
      "shortCiteRegEx" : "Okazaki",
      "year" : 2007
    }, {
      "title" : "Perceptually inspired layoutaware losses for image segmentation",
      "author" : [ "A. Osokin", "P. Kohli" ],
      "venue" : "In Proceedings of the European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Osokin and Kohli,? \\Q2014\\E",
      "shortCiteRegEx" : "Osokin and Kohli",
      "year" : 2014
    }, {
      "title" : "A simple method for solving the SVM regularization path for semidefinite kernels",
      "author" : [ "C.G. Sentelle", "G.C. Anagnostopoulos", "M. Georgiopoulos" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems,",
      "citeRegEx" : "Sentelle et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sentelle et al\\.",
      "year" : 2015
    }, {
      "title" : "Shallow parsing with conditional random fields",
      "author" : [ "F. Sha", "F. Pereira" ],
      "venue" : "In NAACL,",
      "citeRegEx" : "Sha and Pereira,? \\Q2003\\E",
      "shortCiteRegEx" : "Sha and Pereira",
      "year" : 2003
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "Tibshirani,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani",
      "year" : 1996
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Wainwright and Jordan,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright and Jordan",
      "year" : 2008
    }, {
      "title" : "Multi-category support vector machines, feature selection and solution path",
      "author" : [ "L. Wang", "X. Shen" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Wang and Shen,? \\Q2006\\E",
      "shortCiteRegEx" : "Wang and Shen",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : ", 1974), away step (Wolfe, 1970), fully-corrective step (Holloway, 1974) (see Lacoste-Julien & Jaggi (2015) for a recent review and the proof that all these methods have a linear rate on the objective (3) despite not being strongly convex).",
      "startOffset" : 95,
      "endOffset" : 108
    }, {
      "referenceID" : 22,
      "context" : "(2004), in their seminal paper, introduced the notion of regularization path and showed that the regularization path of LASSO (Tibshirani, 1996) is piecewise linear.",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 20,
      "context" : "Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases.",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "1 of (Giesen et al., 2012) applied to the case of binary SVM.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "Another difference to (Giesen et al., 2012) consists in using the λ-formulation of SVM instead of the C-formulation.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "Hastie et al. (2004) proposed the path following method to compute the exact regularization path for the binary SVM with L2-regularization.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost.",
      "startOffset" : 13,
      "endOffset" : 342
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant εapproximate path with at most O(1/ √ ε) break points and applied it, e.",
      "startOffset" : 13,
      "endOffset" : 470
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant εapproximate path with at most O(1/ √ ε) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al.",
      "startOffset" : 13,
      "endOffset" : 750
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant εapproximate path with at most O(1/ √ ε) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM.",
      "startOffset" : 13,
      "endOffset" : 848
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant εapproximate path with at most O(1/ √ ε) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM.",
      "startOffset" : 13,
      "endOffset" : 868
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant εapproximate path with at most O(1/ √ ε) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM. Finally, Jun-Tao & Ying-Min (2010) constructed the regularization path for the multi-class SVM with huberized loss.",
      "startOffset" : 13,
      "endOffset" : 964
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant εapproximate path with at most O(1/ √ ε) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM. Finally, Jun-Tao & Ying-Min (2010) constructed the regularization path for the multi-class SVM with huberized loss. We are not aware of any work computing the regularization path for SSVM or, for its predecessor multi-class SVM in the formulation of Crammer & Singer (2001). The induction step of our method is similar to Alg.",
      "startOffset" : 13,
      "endOffset" : 1203
    }, {
      "referenceID" : 4,
      "context" : "In addition, Gärtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant εapproximate path with at most O(1/ √ ε) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM. Finally, Jun-Tao & Ying-Min (2010) constructed the regularization path for the multi-class SVM with huberized loss. We are not aware of any work computing the regularization path for SSVM or, for its predecessor multi-class SVM in the formulation of Crammer & Singer (2001). The induction step of our method is similar to Alg. 1 of (Giesen et al., 2012) applied to the case of binary SVM. They also construct a piecewise linear ε-approximate path by alternating the SVM solver and a procedure to identify the region where the output of the solver is accurate enough. In contrast to our method, Giesen et al. (2012) construct the path only for the predefined segment of the values of λ.",
      "startOffset" : 13,
      "endOffset" : 1544
    }, {
      "referenceID" : 9,
      "context" : "Then similarly to Lemma 7 in Jaggi (2013), we have C (i) f ≤ Li ( diam‖·‖iM (i) )2.",
      "startOffset" : 29,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : ", see Lemma 3 and 4 in Jaggi (2013)), hence showing that the optimization is difficult on this block.",
      "startOffset" : 23,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "d’Aspremont et al. (2013) suggests to use the atomic norm of the domain M for the analysis.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "We note that these two algorithms are simply the blockwise application of the PFW and AFW algorithms as described in Lacoste-Julien & Jaggi (2015), but in the context of SSVM which complicates the notation.",
      "startOffset" : 134,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "We follow closely the notation and the results from Lacoste-Julien & Jaggi (2015) where the global linear convergence of the (batch) pairwise FW (PFW) and away-step FW (AFW) algorithms was shown.",
      "startOffset" : 69,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "We follow closely the notation and the results from Lacoste-Julien & Jaggi (2015) where the global linear convergence of the (batch) pairwise FW (PFW) and away-step FW (AFW) algorithms was shown. The main insight to get our result is that the “pairwise FW gap” decomposes also as a sum of block gaps. We give our result for the following more general setting (the block-separable analog of the setup in Appendix F of Lacoste-Julien & Jaggi (2015)):",
      "startOffset" : 69,
      "endOffset" : 447
    }, {
      "referenceID" : 9,
      "context" : "(26) in LacosteJulien & Jaggi (2015)).",
      "startOffset" : 24,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "(6) in Lacoste-Julien & Jaggi (2015) for AFW).",
      "startOffset" : 24,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "(39) of Lacoste-Julien & Jaggi (2015) (μ̃f is strictly greater than zero when q is strongly convex and M is a polytope).",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "We now use the key relationship between the suboptimality hk and the PFW gap gk derived in inequality (43) of Lacoste-Julien & Jaggi (2015) (which is true for any function f by definition of μ̃f if we allow it to be zero):",
      "startOffset" : 127,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "In the more general case of problem (34) where only q is μ-strongly convex, the generalized strong convexity depends both on μ and the Hoffman constant (Hoffman, 1952) associated with the linear system of problem (34).",
      "startOffset" : 152,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "Finally, the fact that μ̃f > 0 when q is μ-strongly convex and M is a polytope comes from the lower bound given in Theorem 10 of Lacoste-Julien & Jaggi (2015) in terms of the pyramidal width ofM (a strictly positive geometric quantity for polytopes), and the generalized strong convexity of f as defined in Lemma 9 of Lacoste-Julien & Jaggi (2015).",
      "startOffset" : 146,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "Finally, the fact that μ̃f > 0 when q is μ-strongly convex and M is a polytope comes from the lower bound given in Theorem 10 of Lacoste-Julien & Jaggi (2015) in terms of the pyramidal width ofM (a strictly positive geometric quantity for polytopes), and the generalized strong convexity of f as defined in Lemma 9 of Lacoste-Julien & Jaggi (2015). The generalized strong convexity of f is simply μ if f is μ-strongly convex.",
      "startOffset" : 146,
      "endOffset" : 348
    }, {
      "referenceID" : 8,
      "context" : "In the more general case of problem (34) where only q is μ-strongly convex, the generalized strong convexity depends both on μ and the Hoffman constant (Hoffman, 1952) associated with the linear system of problem (34). See Lacoste-Julien & Jaggi (2015) for more details, as well as Lemma 2.",
      "startOffset" : 135,
      "endOffset" : 253
    }, {
      "referenceID" : 8,
      "context" : "In the more general case of problem (34) where only q is μ-strongly convex, the generalized strong convexity depends both on μ and the Hoffman constant (Hoffman, 1952) associated with the linear system of problem (34). See Lacoste-Julien & Jaggi (2015) for more details, as well as Lemma 2.2 of Beck & Shtern (2015).",
      "startOffset" : 135,
      "endOffset" : 316
    }, {
      "referenceID" : 12,
      "context" : "For a feature mapping ψi(y) representing the sufficient statistics for an energy function associated with a graphical model (as for a conditional random field (Lafferty et al., 2001)), then the SSVM objective is implicitly optimizing over the marginal polytope for the graphical model (Wainwright & Jordan, 2008).",
      "startOffset" : 159,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "Lacoste-Julien & Jaggi (2015) showed that the largest possible pyramidal width of a polytope in dimension m (for a fixed diameter) is achieved by the probability simplex and is Θ(1/ √ m).",
      "startOffset" : 17,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "By the affine invariance property of the FW-type algorithms, we can thus instead use the pyramidal width of the marginal polytope for the convergence analysis (and similarly for the Hoffman constant). Lacoste-Julien & Jaggi (2015) conjectured that the pyramidal width of a marginal polytope in dimension pwas also Θ(1/ √ p), thus giving a more reasonable bound for the convergence rate of BCPFW for SSVM.",
      "startOffset" : 182,
      "endOffset" : 231
    }, {
      "referenceID" : 17,
      "context" : "The CoNLL dataset contains 8, 936 training English sentences extracted from the Wall Street Journal part of the Penn Treebank II (Marcus et al., 1993).",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 17,
      "context" : "The CoNLL dataset contains 8, 936 training English sentences extracted from the Wall Street Journal part of the Penn Treebank II (Marcus et al., 1993). Each output label yt can take up to 22 different values. We use the feature mapφ(x,y) proposed by Sha & Pereira (2003). First, for each position t of the input sequence x, we construct a unary feature representation, containing the local information.",
      "startOffset" : 130,
      "endOffset" : 271
    }, {
      "referenceID" : 18,
      "context" : "We extract the attributes with the CRFsuite library (Okazaki, 2007) and refer to its documentation for the exact list of attributes: http://www.",
      "startOffset" : 52,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "The 1, 969 unary features include 1 constant feature, 512-bin histograms of densely sampled visual SIFT words (Lowe, 2004), 128-bin histograms of RGB colors, 16-bin histograms of locations (each pixel of a region of interest is matched to a cell of the 4× 4 uniform grid).",
      "startOffset" : 110,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "HorseSeg contains 25, 438 training images, 147 of which are manually annotated, 5, 974 annotations are constructed from object bounding boxes by the automatic method of Guillaumin et al. (2014), while the remaining 19, 317 annotations were constructed by the same method but without any human supervision.",
      "startOffset" : 169,
      "endOffset" : 194
    }, {
      "referenceID" : 6,
      "context" : "HorseSeg contains 25, 438 training images, 147 of which are manually annotated, 5, 974 annotations are constructed from object bounding boxes by the automatic method of Guillaumin et al. (2014), while the remaining 19, 317 annotations were constructed by the same method but without any human supervision. The test set of HorseSeg consists of 241 images with manual annotations. In our experiments, we use training sets of three different sizes: 147 images for HorseSegsmall, 6, 121 images for HorseSeg-medium and 25, 438 for HorseSeg-large. In addition to images and their pixel-level annotations, Kolesnikov et al. (2014) released15 oversegmentations (superpixels) of the images precomputed with the SLIC algorithm (Achanta et al.",
      "startOffset" : 169,
      "endOffset" : 624
    }, {
      "referenceID" : 6,
      "context" : "HorseSeg contains 25, 438 training images, 147 of which are manually annotated, 5, 974 annotations are constructed from object bounding boxes by the automatic method of Guillaumin et al. (2014), while the remaining 19, 317 annotations were constructed by the same method but without any human supervision. The test set of HorseSeg consists of 241 images with manual annotations. In our experiments, we use training sets of three different sizes: 147 images for HorseSegsmall, 6, 121 images for HorseSeg-medium and 25, 438 for HorseSeg-large. In addition to images and their pixel-level annotations, Kolesnikov et al. (2014) released15 oversegmentations (superpixels) of the images precomputed with the SLIC algorithm (Achanta et al., 2012) and the unary features of each superpixel computed similarly to the work of Lempitsky et al. (2011). On average, each image contains 147 superpixels.",
      "startOffset" : 169,
      "endOffset" : 840
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an adaptive criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gapbased sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}