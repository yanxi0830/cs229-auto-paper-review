{
  "name" : "1702.07444.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bandits with Movement Costs and Adaptive Pricing",
    "authors" : [ "Tomer Koren" ],
    "emails" : [ "tkoren@google.com", "rlivni@cs.princeton.edu", "mansour@tau.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n07 44\n4v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n? kT ` T {kq, where k is the number of actions and T is the time\nhorizon. When the set of actions corresponds to whole r0, 1s interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of rΘpT 2{3q, which is the same rate one obtains when there is no penalty for movements.\nAs our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of rOpT 2{3q compared to the best fixed price in hindsight, which outperform the previous regret bound of rOpT 3{4q for the problem."
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-Armed Bandit (MAB) is a well studied model in computational learning theory and operations research. In MAB a learner repeatedly selects actions and observes their rewards. The goal of the learner is to minimize the regret, which is the difference between her loss and the loss of the best action in hindsight. This simple model already abstracts beautifully the exploration-exploitation tradeoff, and allows for a systematic study of this important issue in decision making. The basic results for MAB show that even when an adversary selects the sequence of losses, the learner can guarantee a regret of Θp ? kT q, where k is the number of actions and T is the number of time steps (5, 3; see also 13). The simplicity of the MAB comes at a price. Essentially, the system is stateless, and previous actions have no influence on the losses assigned to actions in the future. A more involved model of sequential decision making is Markov Decision Processes (MDPs) where the environment is modeled by a finite set of states, and actions are not only associated with losses but also with stochastic transitions between states. Unfortunately, for the adversarial setting there are mostly hardness results even in limited cases [1].\nIntroducing switching costs is a step of incorporating dependencies in the learner’s action selection. The unit switching cost has a unit cost per each changing of actions. In such a setting a tight bound of rΘpk1{3T 2{3q is known [18]. Our main goal is to extend this basic model to the case of MAB with movement costs, where the cost associated with switching between arms is given by a metric that determines the distance between any pair of arms. Such a model already introduces a very interesting dependency in the action selection process for the learner. Specifically, we study a metric between actions which is modeled by a complete binary tree, where the distance between two actions is proportional to the number of nodes in the subtree of their least common ancestor. This abstracts the case where the arms are associated with k points on the real line and the switching cost between arms is the absolute difference between the corresponding points (actually, the tree metric only upper bounds distances on a line, but this upper bound is sufficient for our applications). Note that we do not assume that pairs of actions with low movement cost have similar losses: our model retains the full generality of the loss functions, and only imposes a metric structure on the cost of movement between arms.\nOur main result is an efficient MAB algorithm, called the Slowly Moving Bandit (SMB) algorithm, that guarantees expected regret of at most rOp ? kT `T {kq. As we elaborate later, this result implies that for k ď T 1{3 we can achieve an optimal regret rΘpT 2{3q, and for k ě T 1{3 we obtain an optimal regret rate of rΘp ? kT q. It is worth discussing the implication of our bound. The bound of rΘpT 2{3q for k ď T 1{3 is tight due to the lower bound of Dekel et al. [18], which applies already for k “ 2 actions. The bound of rΘp ? kT q for k ě T 1{3 is tight due to the classic lower bound for MAB even without movement costs [5]. Surprising, for a large action set (i.e., k ě T 1{3) we lose nothing in the regret by introducing movement costs to the problem! Another surprising consequence of our bound is that there is no loss in the regret by increasing the number of actions from k “ 2 to k “ ΘpT 1{3q when movement costs are present.\nThe main application of our SMB algorithm is for adaptive pricing with patient buyers [20]. In this adaptive pricing problem, we have a seller which would like to maximize his revenue. He is faced with a stream of patient buyers. Each buyer has a private value and a window of time in which she would like to purchase the item. The buyer buys at the lowest price in its window, in case it is below its value. (The seller publishes sufficient prices into the future, such that the buyer can observe all the relevant prices.) The adaptive price setting is related to the MAB problem with movement costs in the following way. The prices are continuous (say, r0, 1s) and the reward is the revenue gain by the seller. The rewards are given by a one–sided Lipschitz function (specifically, we receive\nthe reward whenever we post a price which is at most the private value, and zero otherwise). This allows us to apply our bandit algorithm via discretization of the continuous space. The challenge, though, remains to control the cost the seller pays which stems from the buyer’s patience.\nThe seller benchmark is the best single price. Using a single price implies that the buyers either buy immediately, or never buy. The movement cost models the loss due to having the buyer patient, which can be thought as the difference between the price of the item when the buyer arrives and the price at which it buys. (Note that there might be a gain, since it might be that when the buyer arrives the price is too high, but later lower prices make him buy. We ignore this effect for now.) Our main result is that the seller can use our SMB algorithm and guarantee a regret of at most rOpT 2{3q, using T 1{3 equally-spaced prices. This is in contrast to a regret of rOpT 3{4q which is achieved by applying a standard switching cost technique together with a discretization argument [20].\nIt is interesting to observe qualitatively how our algorithm performs. It is much more likely to make small changes than large ones; roughly speaking, the probability of a change drops exponentially in the magnitude of the change. Conceptually, this is a highly desirable property of a pricing algorithm, and arguably, of any regret minimization algorithm: we would like to slightly perturb the prices over time without a sever impact on the buyers, and only rarely make very large changes in the pricing.\nFinally, another application of our algorithm is for the case that we have continuous actions on an interval, and the losses of the actions are Lipschitz. Our algorithm can handle movement cost which are also Lipschitz on the interval. (We stress that in our application the losses are deterministic and not stochastic.)"
    }, {
      "heading" : "1.1 Related Work",
      "text" : "With a uniform unit switching cost (i.e., when switching between any two actions has a unit cost), it is known that there is a tight rΩpk1{3T 2{3q lower bound for the MAB problem [18], which is in contrast to the Op ? kT q regret upper bound without switching costs.\nClassical MAB algorithms such as Exp3 [5] guarantee a regret of rOp ? kT q without movement costs. However, they are not guaranteed to move slowly between actions, and in fact, it is known that Exp3 might make rΩpT q switches between actions in the worst case (see 18), which makes it inappropriate to directly handle movement costs.\nOur adaptive pricing application follows the model of Feldman et al. [20]. There, for a finite set of k prices show a matching bound of rΘpT 2{3q on the regret. For continuous prices they remark that their upper bound can be used to derive an rOpT 3{4q regret bound. Our SMB algorithm improves this regret bound to rOpT 2{3q. There is a slight difference in the exact feedback model between [20] and here: in both models when a buyer arrives, the sell time is uniquely determined; however, in [20] the seller observes the purchase only at the actual time of the sell, whereas here we assume the seller observes the sell when the buyer arrives and decides when to purchase. We remark, though, that as discussed in [20] all lower bounds derived there apply to the current feedback model too.\nThere is a vast literature on online pricing (e.g., 6, 8, 7, 9, 11). The main difference of our adaptive pricing model is the patience of our buyers, which correlates between the prices at nearby time steps.\nFor the case of continuous prices and a single seller, when one consider impatient buyers, a simple discretization argument can be used to achieve a regret of rOpT 2{3q, and there exists a similar lower bound of ΩpT 2{3q [25]. More generally, learning Lipschitz functions on a closed interval has been studied by Kleinberg [24], where an optimal rΘpT 2{3q regret bound is shown via discretization. Our results show that even if one adds a movement cost (which is the distance) to the problem, there\nis no change in the regret. There are many works on continuous action MAB [24, 16, 4, 12, 28]. Most of the works relate the change in the payoff to the change in the action in various ways. Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property. We differ from that line of work. Our assumption is about the switching cost (rather than the losses) being related to the distance between the actions.\nThe work of Guha and Munagala [21] discusses a stochastic MAB, in the spirit of the Gittins index, where there is both a switching cost and a play cost, and gives a constant approximation algorithm. We differ from that work both in the model, their model is stochastic and our is adversarial, and in the result, their is a multiplicative approximation and our is a regret.\nApproximating an arbitrary metric using randomized trees (i.e., k-HST) has a long history in the online algorithms literature, starting with the work of Bartal [10]. The main goal is to derive a simpler metric representation (using randomized trees) that will both upper and lower bound the given metric. In this work we need only an upper bound on the metric, and therefore we can use a deterministic complete binary tree."
    }, {
      "heading" : "2 Setup and Formal Statement of Results",
      "text" : ""
    }, {
      "heading" : "2.1 Bandits with Movement Costs",
      "text" : "In this section we consider the Multi-Armed Bandit (MAB) problem with movement costs. In this problem, that can be described as a game between an online learner and an adversary continuing for T rounds, where there is a set K “ t1, . . . , ku of k ě 2 arms (or actions) that the learner can choose from. The set of arms is equipped with a metric ∆pi, jq P r0, 1s that determines the movement distance between any pair of arms i, j P K.\nFirst, before the game begins, the adversary fixes a sequence ℓ1, . . . , ℓT P r0, 1sk of loss vectors assigning loss values in r0, 1s to the arms.1 Then, on each round t “ 1, . . . , T , the learner picks an arm it P K, possibly at random, and suffer the associated loss ℓtpitq. In addition to incurring this loss, the learner also pays a cost of ∆pit, it´1q that results from her movement from arm it´1 to arm it. At the end of each round t, the learner receives bandit feedback : she gets to observe the single number ℓtpitq, and this number only. (The movement cost is common knowledge.)\nThe goal of the learner, over the course of T rounds of the game, is to minimize her expected movement-regret, which is defined as the difference between her (expected) total costs—including both the losses she has incurred as well as her movement costs—and the total costs of the best fixed action in hindsight (that incur no movement costs, since it is the same action in all time steps); namely, the movement regret with respect to a sequence ℓ1:T of loss vectors and the metric ∆ equals\nRegretMCpℓ1:T ,∆q “ E « Tÿ\nt“1\nℓtpitq ` Tÿ\nt“2\n∆pit, it´1q ff ´ min\ni‹PK\nTÿ\nt“1\nℓtpi‹q .\nHere, the expectation is taken with respect to the player’s randomization in choosing the actions i1, . . . , iT .\nMAB with a tree metric. Our focus in this paper is on a metric induced over the actions by a complete binary tree T with k leaves. We consider the MAB setting where each action i is associated with a leaf of the tree T . (For simplicity, we assume that k is a power of two.)\n1Throughout, we assume that the adversary is oblivious, namely, that it cannot react to the learner’s actions.\nWe number the levels of the tree T from the leaves to the root. Let levelpvq be the level of node v in T , where the level of the leaves is 0. Given two leaves i and j, let lcapi, jq be their least common ancestor in T . Then, given actions i and j let dT pi, jq be the level of their least common ancestor in T , i.e., dT pi, jq “ levelplcapi, jqq. The movement cost between i and j is then\n∆T pi, jq “ 1k2 dT pi,jq P r0, 1s . (1)\nOur first main result bounds that movement cost with respect to the given metric:\nTheorem 1. There exists an algorithm (see Algorithm 1 in Section 4) that for any sequence of loss functions ℓ1, . . . , ℓT guarantees that\nRegretMCpℓ1:T ,∆T q “ rO ˆ?\nkT ` T k\n˙ .\nFor k ě T 1{3 the theorem gives an optimal regret bound of rOp ? kT q. For k ď T 1{3, we can extend a binary tree with k leaves by turning each leaf into a node whose subtree is a balanced binary tree and we obtain a new tree with at most 2T 1{3 leaves. We then associate with each new leaf as its action the action induced by its parent at the level of original leaves. One can show that the movements between the level of the original actions is then controlled by OpT 2{3q and we can then exploit this construction to achieve a regret bound of rOpT 2{3q. In any movement cost problem with at least two arms of fixed constant distance, a lower bound regret of 2-arm switching cost applies, hence we observe that these rates are optimal for every k ď T [18].\nContinuum-armed bandit with movement cost. We can apply Algorithm 1 to the problem of learning Lipschitz functions over the real line with movement regret associated with standard metric over the interval. In this setting we assume an arbitrary sequence of functions f1, . . . , fT : r0, 1s ÞÑ r0, 1s where each function ft is L-Lipschitz. i.e.,\n|ftpxq ´ ftpyq| ď L|x´ y| @ x, y P r0, 1s .\nLet xt be the action selected by the player at time t. The objective is then to minimize the movement regret, defined:\nRegretMCpf1:T , | ¨ |q “ E « Tÿ\nt“1\nftpxtq ` Tÿ\nt“1\n|xt ´ xt`1| ff ´ min\nxPr0,1s\nTÿ\nt“1\nftpxq .\nOne application of our algorithm is a regret bound for Lipschitz functions:\nTheorem 2. There exists an algorithm (based on Algorithm 1) that for every sequence of LLipschitz loss functions f1, . . . , fT , with L ě 1, achieves:\nRegretMCpf1:T , | ¨ |q “ rO ` L1{3T 2{3 ˘ .\nWe emphasize that even without movement costs, there is an rΩpT 2{3q lower bound in this setting [24]; hence, the regret bound of Theorem 2 is essentially optimal.\nWe also note that the result, in fact, holds for any metric ∆ that is L-Lipschitz (for exact statement see Theorem 12)."
    }, {
      "heading" : "2.2 Adaptive Pricing",
      "text" : "We consider the following model of online learning, with respect to a stream of patient buyers with patience at most τ . In our setting the seller posts at time t “ 1 prices ρ1, . . . , ρτ`1 for the next τ days in advance. Then at each time step t the seller posts price for the t` τ day ρt`τ and receives as feedback her revenue for day t. The revenue at time t depends on buyer bt and the sequence of prices ρt, ρt`1, . . . , ρt`τ in the manner described below.\nEach buyer bt, in our setting, is a mapping from a sequence of prices to revenues, parameterized by her value vt and her patience τt. The buyer proceed by observing prices ρt, . . . , ρt`τt , and purchases the item at the lowest price among these prices, if it does not exceed her value. Thus the revenue from the buyer at time t is described as follows:\nbtpρt, . . . , ρt`τ q “ # mintρt, . . . , ρt`τtu if mintρt, . . . , ρt`τtu ď vt, 0 otherwise.\nNote that at time t the buyer decides whether it will purchase and when. Here, we assume that the buyer also gets to order the good at day of arrival (at price and time decided by him according to his patience and private value), thus the seller observes the buyer’s decision at time t, namely the feedback at time t is given by btpρt, . . . , ρt`τ q. We note that this feedback model differs from Feldman et al. [20] where the buyer buy at day of purchase. However, we note that both lower and upper bounds derived by Feldman et al. apply to our feedback model as noted there in the discussion.\nOur objective is to construct an algorithm that minimizes the regret which is the difference between revenue obtained by the best fixed price in hindsight and the expected revenue obtained by the seller, given a sequence b1:T of buyers:\nRegretpb1:T q “ max ρ˚PP\nTÿ\nt“1\nbtpρ˚, . . . , ρ˚q ´ E « Tÿ\nt“1\nbtpρt, . . . ρt`τ q ff .\nOur main result with respect to adaptive pricing is as follows:\nTheorem 3. There exists an algorithm (see Algorithm 2 in Section 5) that for any sequence of buyers b1, . . . ,bT with maximum patience τ achieves the following regret bound:\nRegretpb1:T q “ rOpτ 1{3T 2{3q . It is interesting to note that even though a lower bound of ΩpT 2{3q stems from two different sources we can still achieve a regret rate of rOpT 2{3q. Indeed, Kleinberg and Leighton [25] showed that optimizing over the continuum r0, 1s leads to a lower bound of ΩpT 2{3q, irrespective of the patience of the buyers. Second, Feldman et al. [20] showed that whenever the seller wishes to optimize between more than two prices, a lower bound of ΩpT 2{3q holds for patient buyers.\nIn this work we deal with both obstacles together—patient buyers and optimization over the r0, 1s interval—yet the two obstacles can be dealt without leading to a regret bound that is necessarily worse then each obstacle alone.\nOur solution to the adaptive pricing problem is based on employing a MAB with movement costs algorithm that allows small change in the prices. The reason one needs to employ an algorithm with small movement cost stems from the memory of the buyers: roughly speaking, whenever the seller encounters a buyer with patience, the potential revenue of the seller will be the revenue at time t minus any discount price that buyer may encounter on future days. Indeed, for the case of two prices, Feldman et al. [20] constructed a sequence of buyers that reduces the problem to MAB with switching cost: a step in demonstrating a ΩpT 2{3q regret bound: thus a fluctuation in prices is indeed a cause for a high regret."
    }, {
      "heading" : "3 Overview of the approach and techniques",
      "text" : "In this section we give an informal overview of the main ideas in the paper and describe the techniques used in our solution. We begin with the main ideas behind our main result: an optimal and efficient algorithm for MAB problems with movement costs. Later we continue with the adaptive pricing problem, and show how it is abstracted as an instance of the MAB problem with movement costs.\nFrom continuum-armed to multi-armed. In our main applications, we consider actions that are associated to points on the interval r0, 1s equipped with the natural metric ∆px, yq “ |x ´ y|. As a preliminary step, we use discretization in order to make the action space finite and capture the setting by the MAB framework. That is, we reduce the problem of minimizing regret over the entire r0, 1s interval to regret minimization over k actions associated with the equally-spaced points K “ t 1\nk , 2 k , . . . , 1u. Our challenge is to then to design a regret minimization algorithm over\nA whose cumulative movement cost with respect to the metric r∆pi, jq “ |i´ j|{k is bounded. Our approach builds upon the basic techniques underlying the Exp3 algorithm for the basic MAB problem, which we recall here. Exp3maintains over rounds a distribution pt over the k actions and chooses an action it „ pt; thereafter, it updates its sampling distribution multiplicatively via pt`1piq 9 ptpiq ¨ expp´ηℓ̄tpiqq, where ℓ̄t is an unbiased estimator of true loss vector ℓt constructed using only the observed feedback ℓtpitq. Specifically, the estimator used by Exp3 is\nℓ̄tpiq “ 1tit “ iu ptpiq ℓtpitq @ i P K .\nA simple computation shows that ℓ̄t is indeed an unbiased estimator of ℓt, namely that Erℓ̄ts “ ℓt, and the crucial bound for Exp3 is then obtained by controlling a variance term of the form Erpt ¨ ℓ̄2t s, and showing that it is of the order rOpkq at all rounds t. This in turn implies the rOp ? kT q bound of Exp3.\nControlling movements with a tree. As a first step in controlling the movement costs of our algorithm, one can think of an easier problem of controlling the number of times the algorithm switches between actions in the left part of the interval, namely in AL “ t 1k , . . . , 12u, and actions in the right part of the interval, AR “ t12 ` 1k , . . . , 1u. Indeed, since each such switch might incur a high movement cost (potentially close to 1), any algorithm for MAB with movement costs must avoid making such switches too often. In principle, a solution to this simpler problem can be then lifted to a solution to the actual movement costs problem by applying it recursively to each side of the interval.\nThe thought experiment above motivates our tree-based metric: this metric assigns a fixed cost of 1 to any movement between the left and right parts of the interval—that correspond to the topmost left and right subtrees—and recursively, a cost of 2d{k for any movement between subtrees in level d of the tree. The tree metric is always an upper bound on the natural metric on the interval, namely r∆pi, jq ď 1\nk 2dT pi,jq “ r∆T pi, jq, so that controlling movement costs with respect to\nr∆T suffices for controlling movement costs with respect to the natural distance on r0, 1s. While this upper bound might occasionally be very loose,2 the tree-metric effectively captures the difficulties of the original movement costs problem with the natural metric over r0, 1s.\nHence, we can henceforth focus on constructing an algorithm with low movement costs with respect to a tree-based metric over a full binary tree. To accomplish this, we will regulate the\n2For example, the distance between 1 2 ´ 1 k and 1 2 ` 1 k according to the metric ∆T is 1.\nprobability of switching the ancestral node. Namely, if we denote by Adpiq the subtree at level d of the tree containing action i, our goal is to design an algorithm that switches between actions i and j such that Adpiq ‰ Adpjq with probability at most 2´d. This would ensure that the expected contribution of level d in the tree to the movement cost of the algorithm is Op1{kq per round. Indeed, switching between subtrees at level d (while not making a switch at higher levels) results with a movement cost of roughly 2d{k. Overall, the contribution of all layers in the tree to the total movement cost would then be OppT {kq log kq, as required.\nLazy sampling. Our challenge now is to construct an algorithm that switches infrequently between subtrees at higher levels of the tree. However, recall that typical bandit algorithms choose their actions i1, . . . , iT at random from sampling distributions p1, . . . , pT maintained throughout the evolution of game. In order to guarantee that consecutive actions it and it´1 will belong to the same subtree with high probability, the algorithm would have to sample ii in a way which is highly correlated with the preceding action it´1.\nSuppose that the marginals of the subtrees at some level d does not change between the distributions pt´1 and pt; namely, that the cumulative probability assigned to the leaves of each such subtree by both pt´1 and pt is the same. In this case, we argue that we can sample our new action it at time t, based on the preceding action it´1, from the conditional distribution ptp¨ | Adpit´1qq. In other words, if we think of sampling an action i from pt as sampling a path in the tree leading to the leaf associated with i, then for determining it on round t we copy the top d edges from the path at time t ´ 1, and only sample the remaining bottom edges (those contained in the subtree Adpit´1q) according to the new distribution pt. Intuitively, this can be justified because the distribution of the top d edges in the path leading to it is the same as that of the top d edges in the path leading to it´1, so we may as well keep the random bits associated with them and only resample bits associated with the remaining edges from fresh.\nThe lazy sampling scheme sketched above raises a major difficulty in the analysis: since it is sampled from a conditional of pt that might be very different from pt itself, it is no longer clear that it is distributed according to the “correct” distribution. In other words, conditioned on pt (which intuitively is a summary of the past), the random variable it is certainly not distributed according to pt. Nevertheless, our analysis demonstrates a crucial property of the distributions pt maintained the sampling scheme, which is sufficient for the regret analysis: we show that for all subtrees A at all levels of the tree, it holds that\nE „ 1ti P Au ptpAq  “ 1 .\nThat is, even though it is sampled indirectly from pt, it is still distributed according to pt in a certain sense.\nRebalancing the marginals. The lazy sampling we described above reduced the problem of controlling the frequency of movements in the actions i1, . . . , iT , to controlling the frequency in which the marginal distribution of p1, . . . , pT over subtrees is updated by our algorithm. Next, we describe how the latter is accomplished (where the frequency of update is exponentially-decreasing with the level of the subtree). To illustrate the technique, let us consider an easier problem: instead of demanding infrequent updates for subtrees in all levels, we shall only attempt to rebalance the marginals at the topmost level, with the goal of making them being updated with probability at most 2´D “ 1{k. We will demonstrate how the estimator rℓt can be modified in a way that induces such infrequent updates at the top level. Denote the left subtree at the top level by AL (containing actions 1\nk , . . . , 1 2 ) and the right topmost subtree by AR (containing actions 1 2 ` 1 k , . . . , 1). First, we\nchoose\nσt “ # 1´ 1 δ with probability δ;\n1 with probability 1´ δ.\nThen, for A P tAL, ARu we set\nrℓtpiq “ ℓ̄tpiq ´ σt\nη log\n˜ ÿ\njPA\nptpjq ptpAq e´ηℓ̄tpjq\n¸ @ i P A .\nHere, ℓ̄t is the basic Exp3 estimator discussed earlier. In terms of estimation, rℓt is still an unbiased estimator of the true vector ℓt: since Erσts “ 0 it follows that Errℓts “ ℓt. However, the added term has a balancing effect at the top level of the tree: a simple computation reveals that if σt “ 1 (which occurs with high probability), the multiplicative update of the algorithm applied on the vector rℓt ensures that ptpALq “ pt`1pALq and ptpARq “ pt`1pARq. In other words, with probability 1´ δ, the cumulative (i.e., marginal) probability of both subtrees at the top level is remained fixed between rounds t and t` 1.\nThe balancing effect we achieved comes at a price: for small values of δ the magnitude of rℓt becomes large, as it might be the case that σt « ´1{δ. Nevertheless, it is not hard to show that the variance term Erpt ¨ rℓ2t s is bounded by Opk ` 1{δq. In particular, for δ “ 1{k we retain a variance bound of Opkq, while changing the marginals of the two top subtrees with probability no larger than 1{k. As a result, by sampling accordingly from the slowly-changing distributions pt we can ensure that the movements at the top level contribute at most OpT {kq to the total movement cost of the algorithm.\nEvidently, the estimator described above only remedies the problem at the top level, and the movement costs at lower levels of the tree might still be very large (effectively, within each subtree the algorithm does nothing but simulating Exp3 on the leaves). Still, using a similar yet more involved technique we can induce a balancing effect at all levels simultaneously and ensure that the marginal probabilities of the subtrees at level d are modified by the algorithm with probability at most 2´d. The construction adds a balancing term corresponding to each level of the tree in a recursive manner that takes into account the balancing terms at lower levels.\nFrom adaptive pricing to bandits. We now discuss how to reduce adaptive pricing with patient buyers to a MAB problem with movement costs. We employ a reduction similar to the one used by [25]; however, the patience of the buyers introduce some difficulties, as we discuss below. For now, we ignore the buyers’ patience and give the idea of the reduction in the simplest case.\nIntuitively, in order to adaptively pick prices from the interval r0, 1s so as to minimize regret with respect to the best fixed price in hindsight, we could directly apply a standard MAB algorithm, e.g., Exp3, over a discretization A “ t 1\nk , 2 k , . . . , 1u of the interval, treating each of the k prices as an\narm that generates a reward whenever it is pulled. Furthermore, since the buyers’ valuations are not disclosed after purchase, the feedback observed by the seller is very limited and nicely captured by the MAB abstraction. Since the buyers’ valuations are one-sided Lipschitz, the best price in A will lose at most OpT {kq in total revenue as compared to the best fixed price in the entire r0, 1s interval. Thus, provided an algorithm that achieves rOp ? kT q expected regret with respect to the best price in A, we could pick k “ ΘpT 1{3q and obtain the optimal rOpT 2{3q regret for the pricing problem.\nPatient buyers and movement costs. A main complication in the above MAB approach arises from the buyers’ patience: the revenue extracted from a single buyer is determined not only by the price posted by the seller on the day of the buyer’s arrival, but also by prices posted on the subsequent days subject to the buyer’s patience. As a result, if the seller change prices abruptly on consecutive days, a strategic buyer—that purchases in the minimal price, if at all—could make use of this fact to gain the item at a lower price, which lowers the revenue of the seller. Roughly speaking, the latter additional cost to the seller is controlled by the absolute difference between the prices she posted at consecutive days. Thus, the pricing problem with patient buyers can be reduced to a MAB problem with movement costs, where the online player suffers an additional movement cost each time she changes actions, and the movement cost is determined by the metric (absolute value distance) between the respective actions.\nThe reduction sketched above is made precise in Section 5, where we also address an additional difficulty stemming from the adaptivity of the feedback signal observed by the seller: the latter is contaminated by the effect of prices posted at earlier rounds on the buyers, and has to be treated carefully."
    }, {
      "heading" : "4 The Slowly Moving Bandit Algorithm",
      "text" : "In this section we present the Slowly Moving Bandit (SMB) algorithm: our optimal algorithm for the Multi-armed bandit problem with movement costs.\nIn order to present the algorithm we require few additional notations. Recall that in our setting, we consider a complete binary tree of depth D “ log2 k whose leaves are identified with the actions 1, . . . , k (in this order). For any level 0 ď d ď D and arm i P K, let Adpiq be the set of leaves that share a common ancestor with i at level d (where level d “ 0 are the singletons). We denote by Ad the collection of all k{2d subsets of leaves:\nAd “ ! t1, . . . , 2du, t2d ` 1, . . . , 2 ¨ 2du, . . . , tk ´ 2d ` 1, . . . , ku ) @ 0 ď d ď D .\nThe SMB algorithm is presented in Algorithm 1. The algorithm is based on the multiplicative update method, and in that sense is reminiscent of the Exp3 algorithm [5]. Similarly to Exp3, the algorithm computes at each round t an estimator rℓt to the true, unrevealed loss vector ℓt using the single loss value ℓtpitq observed on that round.\nAs discussed in Section 3, in addition to being an (almost) unbiased estimate for the true loss vector, the estimator rℓt used by SMB has the additional property of inducing slowly-changing sampling distributions pt, that allow for sampling the actions it in a way that the overall movement cost is controlled. This is achieved by choosing at random, at each round t, a level dt of the tree to be rebalanced by the algorithm using the balancing vectors ℓ̄t,d. For reasons that will become apparent later on, the level dt is determined by choosing a random sign σt,d for each level d in the tree and identifying the bottommost level with a negative sign. Then, as we show in the analysis, the terms ℓ̄t,d defined using the signs σt,d have a balancing effect at levels d ě dt.\nA major difficulty inherent to our approach, also common to many bandit optimization settings (e.g., 17, 2, 14), is the fact that the estimated losses rℓtpiq might receive negative values that are very high in absolute value. Indeed, the balancing term ℓ̄t,d corresponding to level d is roughly as large as 2d{ptpitq, and might appear in negative sign in rℓt. Algorithm 1 resolves this issue by zeroing-out the estimator rℓt whenever it chooses an action whose probability is too small, which ensures that the ℓ̄t,d terms never become too large. We remark that the standard approaches used to resolve such issues (the simplest of which is mixing the distribution pt with the uniform distribution over\nthe k actions) fail in our case, as they break the rebalancing effect which is tailored to the specific multiplicative update of the algorithm.\nInitialize p1 “ u, d0 “ D and i0 „ p1; for t “ 1, . . . , T : (1) Choose action it „ ptp ¨ | Adt´1pit´1qq, observe loss ℓtpitq (2) Choose σt,0, . . . , σt,D´1 P t´1,`1u uniformly at random;\nlet dt “ mint0 ď d ď D : σt,d ă 0u where σt,D “ ´1 (3) Compute vectors ℓ̄t,0, . . . , ℓ̄t,D´1 recursively via\nℓ̄t,0piq “ 1tit “ iu ptpiq ℓtpitq ,\nand for all d ě 1:\nℓ̄t,dpiq “ ´ 1\nη log\n¨ ˝ ÿ\njPAdpiq\nptpjq ptpAdpiqq e´ηp1`σt,d´1qℓ̄t,d´1pjq\n˛ ‚\n(4) Define Bt “ tptpAdpitqq ă 2dη for some 0 ď d ă Du and set\nrℓt “ #\n0 if it P Bt; ℓ̄t,0 ` řD´1 d“0 σt,dℓ̄t,d otherwise\n(5) Update:\npt`1piq “ ptpiq e´ηrℓtpiqřk\nj“1 ptpjq e´η rℓtpjq\n@ i P K\nAlgorithm 1: The SMB algorithm.\nThe following theorem is the main result of this section. Theorem 1 is an immediate corollary.\nTheorem 4. For any sequence of loss functions ℓ1, . . . , ℓT , The SMB algorithm (Algorithm 1) guarantees that\nRegretpℓ1:tq “ O ˆ log k\nη ` ηTk log k\n˙ .\nIn particular, by setting η “ 1{ ? kT the expected regret of the algorithm is bounded by Op ? Tk log kq. Furthermore, for the metric ∆T (see Eq. (1)), the expected total movement cost of the algorithm is ErřTt“2 ∆T pit, it´1qs “ OppT {kq log kq.\nThe rest of the section focuses on proving Theorem 4. We begin by stating a useful technical bound that we use throughout our analysis to control the magnitude of the balancing vectors ℓ̄t,d. For a proof of the lemma, see Section 4.5 below.\nLemma 5. For all t and 0 ď d ă D the following holds almost surely:\n0 ď ℓ̄t,dpiq ď 1tit P Adpiqu ptpAdpiqq\nd´1ź\nh“0\np1` σt,hq @ i P K . (2)\nIn particular, if σt,h “ ´1 then ℓ̄t,d “ 0 for all d ą h.\nOne useful implication of the lemma is that, since ℓ̄t,d “ 0 for all d ą dt, we can express our estimator rℓt in the following equivalent form:\nrℓt “ ℓ̄t,0 ´ ℓ̄t,dt ` dt´1ÿ\nh“0\nℓ̄t,h . (3)"
    }, {
      "heading" : "4.1 Rebalancing the marginals",
      "text" : "Our first step is to show that the marginals of the distributions pt over subtrees of actions are not modified by the algorithm with high probability, as a result of adding the balancing vectors ℓ̄t,d.\nLemma 6. For all d ě dt we have that pt`1pAq “ ptpAq for all A P Ad.\nFor the proof, we require the next technical result about the balancing vectors ℓ̄t,d computed by the algorithm.\nLemma 7. If σt,0 “ . . . “ σt,d´1 “ 1 then: ÿ\niPA\nptpiqe´ηℓ̄t,dpiq “ ÿ\niPA\nptpiqe´η rℓt,dpiq @ A P Ad ,\nwhere rℓt,d “ ℓ̄t,0 ` řd´1 h“0 ℓ̄t,h.\nProof. The proof proceeds by induction on d. For the base case d “ 0, the claim follows trivially as ℓ̄t,0 “ rℓt,0. Next, we assume the claim is true for some value of d ě 0 and prove it for d ` 1. Pick any A P Ad`1 and write A “ A1 Y A2 where A1, A2 are disjoint sets from Ad. Notice that the vector ℓ̄t,d is uniform over A1 and A2, namely ℓ̄t,dpiq “ cA1 for all i P A1 for some cA1 ě 0, and similarly ℓ̄t,dpiq “ cA2 for all i P A2 for some cA2 ě 0. Hence, we have\nÿ\niPA\nptpiqe´η rℓt,d`1piq “\nÿ\niPA\nptpiqe´η rℓt,dpiqe´ηℓ̄t,dpiq\n“ e´ηcA1 ÿ\niPA1\nptpiqe´η rℓt,dpiq ` e´ηcA2\nÿ\niPA2\nptpiqe´η rℓt,dpiq\n“ e´ηcA1 ÿ\niPA1\nptpiqe´ηℓ̄t,dpiq ` e´ηcA2 ÿ\niPA2\nptpiqe´ηℓ̄t,dpiq\n“ ÿ\niPA\nptpiqe´ηℓ̄t,dpiqe´ηℓ̄t,dpiq\n“ ÿ\niPA\nptpiqe´2ηℓ̄t,dpiq ,\nwhere the third equality uses the induction hypothesis. On the other hand, by the recursive definition of ℓ̄t,d`1 and the fact that ℓ̄t,d`1 is uniform over A, we have\nÿ\niPA\nptpiqe´ηℓ̄t,d`1piq “ ptpAq ÿ\niPA\nptpiq ptpAq e´ηp1`σt,dqℓ̄t,dpiq “ ÿ\niPA\nptpiqe´2ηℓ̄t,dpiq .\nCombining both observations, we obtain ÿ\niPA\nptpiqe´ηℓ̄t,d`1piq “ ÿ\niPA\nptpiqe´η rℓt,d`1piq\nwhich concludes the inductive argument.\nWe can now prove Lemma 6.\nProof of Lemma 6. It is enough to prove that pt`1pAq “ ptpAq for all A P Adt , as each set in Ad for d ą dt is a disjoint union of sets from Adt .\nObserve that if it P Bt (see Algorithm 1 for the definition of Bt) then rℓt “ 0 and the claim is certainly true as pt`1 “ pt in this case. Thus, we henceforth assume that it R Bt, in which case rℓt “ rℓt,dd ´ ℓ̄t,dd where rℓt,dt “ ℓ̄t,0 ` řdt´1 h“0 ℓ̄t,h (recall Eq. (3)). Now, pick any A P Adt and j P A. Since ℓ̄t,dtpiq “ cA for all i P A for some cA ě 0, and using Lemma 7 we obtain\ne´ηcA “ ÿ\niPA\nptpiq ptpAq e´ηℓ̄t,dt piq “ ÿ\niPA\nptpiq ptpAq e´η rℓt,dtpiq . (4)\nOn the other hand, from rℓt “ rℓt,dt ´ ℓ̄t,dt it follows that e´η rℓtpiq “ e´ηrℓt,dt piq{e´ηcA for all i P A, and by Eq. (4) we have\nÿ\niPA\nptpiqe´η rℓtpiq “\nř iPA ptpiqe´η rℓt,dtpiq\ne´ηcA “ ptpAq .\nIn words, the multiplicative update does not change the probabilities of the sets in Adt , hence pt`1pAq “ ptpAq for all A P Adt as required."
    }, {
      "heading" : "4.2 Lazy sampling",
      "text" : "Our next step is to show that the sampling scheme employed by Algorithm 1 is valid and gives rise to low movement costs on expectation. Specifically, we would like to show that in a certain sense, the action it on round t is distributed in expectation according to the distribution pt, even though it is sampled from a conditional of pt in a way that is highly correlated with the preceding action it´1. Furthermore, we will show that the correlations in the sampling scheme are designed in a way that the expected movement between consecutive actions is small. These properties are formalized in the following lemma.\nLemma 8. For all t and 0 ď d ă D the following hold:\n• for all A P Ad we have\nE „ 1tit P Au ptpAq  “ 1 ; (5)\n• with probability at least 1´ 2´pd`1q, we have that Adpitq “ Adpit´1q.\nEq. (5) is central to our analysis below, and virtually all of our probabilistic arguments involving the random variables it and pt will be based on this property. We remark that if we were to sample it directly from the distribution specified by pt, then Eq. (5) would have been trivially true. However, the it are sampled from a conditional of pt that might be very different from pt itself; nevertheless, the lemma shows that Eq. (5) still continues to hold under the skewed sampling process.\nLemma 8 also implies the slow-movement property of the algorithm: at the high levels of the tree, where the subtrees are “wide”, the actions it and it´1 are very likely to belong to the same subtree. The probability of switching subtrees increases exponentially with the level in the tree: at the lower levels, where the subtrees are “narrow”, subtree switches may occur more often as the movement cost incurred by such switches is low.\nProof of Lemma 8. The second statement is true since we pick it`1 „ ptpi | Adtpitqq, so that Adpit`1q ‰ Adpitq can occur only if d ă dt. This happens with probability 2´pd`1q.\nNext, we show Eq. (5) by induction on t. For t “ 1 the statement is true since i1 „ p1. For the induction step, condition on dt and fix any d ě dt and A P Ad. By Lemma 6 we have that ptpAq “ pt`1pAq. Also it P A if and only if it`1 P A, since d ě dt implies that it P A if and only if Adtpitq Ď A and Adtpit`1q “ Adtpitq. Hence, we have\nE „ 1tit`1 P Au pt`1pAq ˇ̌ ˇ̌ dt  “ E „ 1tit P Au ptpAq ˇ̌ ˇ̌ dt  “ E „ 1tit P Au ptpAq  “ 1 , (6)\nwhere the last equality holds true since dt depends solely on σt,0, . . . , σt,D´1 which are independent of it and pt (note that this equality then holds for any set A, regardless of the fact that A P Ad).\nNext, we consider any d ă dt and A P Ad. Let A1 P Adt be the subtree such that A Ď A1, and recall that it`1 „ pt`1pi | Adtpitqq. Hence,\nE „ 1tit`1 P Au pt`1pAq ˇ̌ ˇ̌ it P A1, pt`1, dt  “ E „ 1tit`1 P Au pt`1pA | A1qpt`1pA1q ˇ̌ ˇ̌ it P A1, pt`1, dt  “ 1 pt`1pA1q . (7)\nSince it P A1 implies that it`1 P A1, we have\nE „ 1tit`1 P Au pt`1pAq ˇ̌ ˇ̌ dt, pt`1  “ E „ 1tit`1 P A1u ¨ E „ 1tit`1 P Au pt`1pAq ˇ̌ ˇ̌ it P A1, pt`1, dt  ˇ̌ ˇ̌ dt, pt`1  . (8)\nTaking Eqs. (7) and (8) together and taking the expectation over pt`1, we obtain that for every d ă dt:\nE „ 1tit`1 P Au pt`1pAq ˇ̌ ˇ̌ dt  “ E „ 1tit`1 P A1u pt`1pA1q ˇ̌ ˇ̌ dt  “ 1 ,\nwhere last equality follows from Eq. (6) as A1 P Adt . To conclude, we showed that for all d we have:\nE „ 1tit`1 P Au pt`1pAq ˇ̌ ˇ̌ dt  “ 1 .\nTaking the expectation over dt, we obtain the desired result."
    }, {
      "heading" : "4.3 Bounding the bias and variance",
      "text" : "Next, we turn to bound the variance of the loss estimates rℓt and the bias of their expectations from the true loss vectors. These bounds would become useful for controlling the expected regret of the underlying multiplicative updates scheme.\nWe begin with analyzing the bias of our estimator. The following lemma shows that our estimates are “optimistic”, in the sense that they always bound the true losses from below, yet they do not overly underestimate the losses incurred by the algorithm. The proof is somewhat involved, as a result of the “bad events” Bt under which the estimated loss vectors rℓt are being zeroed-out, thereby introducing biases into the estimation.\nLemma 9. For all t, we have Errℓtpiqs ď ℓtpiq and Erℓtpitqs ď Erpt ¨ rℓts ` ηk log2 k.\nProof. Observe that, by Eq. (5) of Lemma 8,\nErℓ̄t,0piqs “ ℓtpiqE „ 1tit “ iu ptpiq  “ ℓtpiq .\nWe now prove that Errℓt,0piqs ď Erℓ̄t,0piqs for all i, which would imply the first claim. Denote Bt “ i | ptpAdpiqq ă 2dη for some 0 ď d ă D ( . Then, by construction we have Errℓtpiq | it P Bts “ 0 ď Erℓ̄t,0piq | it P Bts. Also, since Erσt,ds “ 0 and σt,d is independent of it and ℓ̄t,d (the latter only depends on σt,0, . . . , σt,d´1), we have\nErrℓt | it R Bts “ Erℓ̄t,0 | it R Bts ` D´1ÿ\nd“0\nErσt,dsErℓ̄t,d | it R Bts “ Erℓ̄t,0 | it R Bts . (9)\nTogether, we obtain Errℓt,0piqs ď Erℓ̄t,0piqs as required. Next, to bound Erℓtpitqs observe that Erpt ¨ rℓt | it P Bts “ 0 and, similarly to Eq. (9),\nErpt ¨ rℓt | it R Bts “ Erpt ¨ ℓ̄t,0 | it R Bts “ Erℓtpitq | it R Bts .\nDenote βt “ P rit P Bts. Then\nErℓtpitqs “ βtErℓtpitq | it P Bts ` p1´ βtqErℓtpitq | it R Bts ď βt ` p1´ βtqErpt ¨ rℓt | it R Bts “ βt ` Erpt ¨ rℓts ,\nwhere for the inequality we used the fact that ℓtpitq ď 1. To complete the proof, we have to show that βt ď ηk log2 k. To this end, write\nPrit P Bts ď D´1ÿ\nd“0\nPrptpAdpitqq ă 2dηs .\nUsing Eq. (5) to write\nE\n„ 1\nptpAdpitqq\n “ kÿ\ni“1\n1\n|Adpiq| E „ 1tit P Adpiqu ptpAdpiqq  “ kÿ\ni“1\n1\n|Adpiq| “ |Ad| “\nk\n2d\ntogether with Markov’s inequality, we obtain\nP ” ptpAdpitqq ă 2dη ı “ P\n„ 1\nptpAdpitqq ą 1 2dη\n ď k\n2d ¨ 2dη “ kη .\nWe conclude that βt “ Prit P Bts ď ηk log2 k, as required.\nOur next step is to bound the relevant variance term of the estimator rℓt.\nLemma 10. For all t, we have Erpt ¨ rℓ2t s ď 2k log2 k.\nProof. Observe that\nrℓ2t piq ď ˜ ℓ̄t,0piq ` D´1ÿ\nd“0\nσt,dℓ̄t,dpiq ¸2 .\nSince Erσt,ds “ 0 and Erσt,dσt,d1s “ 0 for all d ‰ d1, we have for all i that\nErrℓ2t piqs “ Errℓ2t,0piqs ` D´1ÿ\nd“0\nErℓ̄2t,dpiqs ď 2 D´1ÿ\nd“0\nErℓ̄2t,dpiqs . (10)\nOn the other hand, for all d we have by Lemma 5 that\npt ¨ ℓ̄2t,d ď řk\ni“1 ptpiq1tit P Adpiqu ptpAdpitqq2\nd´1ź\nh“0\np1` σt,hq2\n“ 1 ptpAdpitqq\nd´1ź\nh“0\np1` σt,hq2\n“ kÿ\ni“1\n1 |Adpiq| 1tit P Adpiqu ptpAdpiqq\nd´1ź\nh“0\np1` σt,hq2 .\nSince it is independent of the σt,h, and recalling Eq. (5), we get\nEtrpt ¨ ℓ̄2t,ds ď kÿ\ni“1\n1\n|Adpiq| E „ 1tit P Adpiqu ptpAdpiqq  d´1ź\nh“0\nErp1` σt,hq2s “ kÿ\ni“1\n2d\n|Adpiq| “ 2d|Ad| “ k .\nTogether with Eq. (10), this gives\nErpt ¨ rℓ2t s ď 2 D´1ÿ\nd“0\nErpt ¨ ℓ̄2t,ds ď 2k log2 k."
    }, {
      "heading" : "4.4 Concluding the proof",
      "text" : "To conclude the proof and obtain a regret bound, we will use the following well-known second-order regret bound for the multiplicative weights (MW) method, essentially due to [15] (see also [2] for the version given here). For completeness, we give a proof of this bound in Section 4.5 below.\nLemma 11 (Second-order regret bound for MW). Let η ą 0 and let c1, . . . , cT P Rk be real vectors such that ctpiq ě ´1{η for all t and i. Consider a sequence of probability vectors q1, . . . , qT P ∆k defined by q1 “ p 1k , . . . , 1k q, and for all t ą 1:\nqt`1piq “ qtpiq e´ηctpiqřk\nj“1 qtpjq e´ηctpjq @ i P rks .\nThen, for all i˚ P rks we have that Tÿ\nt“1\nqt ¨ ct ´ Tÿ\nt“1\nctpi˚q ď log k\nη ` η\nTÿ\nt“1\nqt ¨ c2t .\nWe now have all we need in order to prove our main result.\nProof of Theorem 4. First, we bound the expected movement cost. Lemma 8 says that with probability at least 1´ 2´pd`1q, the actions it and it´1 belong to the same subtree on level d of the tree, which means that ∆pit, it´1q ď 2d{k with the same probability. Hence,\nEr∆pit, it´1qs ď D´1ÿ\nd“0\n2d\nk P\n„ ∆pit, it´1q ą 2d\nk\n ď D´1ÿ\nd“0\n1 2k “ log2 k 2k ,\nand the cumulative movement cost is then OppT {kq log kq. We turn to analyze the cumulative loss of the algorithm. We begin by observing that rℓtpiq ě ´1{η for all t and i. To see this, notice that rℓt “ 0 unless it R Bt, in which case we have, by Lemma 5 and the definition of Bt,\n0 ď ℓ̄t,dpiq ď 2d ptpAdpitqq ď 1 η @ 0 ď d ă D ,\nand since rℓt has the form rℓt “ ℓ̄t,0 ` řdt´1\nh“0 ℓ̄t,h ´ ℓ̄t,dt (recall Eq. (3)), we see that rℓtpiq ě ´1{η. Hence, we can use second-order bound of Lemma 11 on the vectors rℓt to obtain\nTÿ\nt“1\npt ¨ rℓt ´ Tÿ\nt“1\nrℓtpi˚q ď log k\nη ` η\nTÿ\nt“1\npt ¨ rℓ2t\nfor any fixed i˚ P rks. Taking expectations and using Lemmas 9 and 10, we have\nE\n« Tÿ\nt“1\nℓtpitq ff ´ Tÿ\nt“1\nℓtpi˚q ď log2 k\nη ` 2ηTk log2 k .\nChoosing η “ 1{ ? Tk, we get a regret bound of Op ? Tk log kq."
    }, {
      "heading" : "4.5 Additional technical proofs",
      "text" : "Here we give a proof of our technical lemma bounding the magnitude of the balancing terms ℓ̄t,d.\nProof of Lemma 5. We will prove the claim by induction on d. For the base case d “ 0, Eq. (2) follows directly from our definitions and the fact that 0 ď ℓtpiq ď 1 for all i. Next, we prove that Eq. (2) holds for some d assuming it hold for all d1 ă d. Since p1 ` σt,d´1qℓ̄t,d´1piq ě 0 for all i by the induction hypothesis, the recursive definition of ℓ̄t,d implies that\nℓ̄t,dpiq ě ´ 1\nη log\n˜ ÿ\njPAdpiq\nptpjq ptpAdpjqq\n¸ “ 0 .\nFurthermore, the definition of ℓ̄t,d together with the convexity of ´ log x and Jensen’s inequality give\nℓ̄t,dpiq ď p1` σd´1q ÿ\njPAdpiq\nptpjq ptpAdpjqq ℓ̄t,d´1pjq\nď 1tit P Adpiqu ptpAdpiqq\nÿ\njPAd´1piq\nptpjq ptpAd´1pjqq\nd´1ź\nh“0\np1` σt,hq\n“ 1tit P Adpiqu ptpAdpiqq\nd´1ź\nh“1\np1` σt,hq ,\nwhere in the second inequality we used the induction hypothesis. This concludes the inductive argument.\nFinally, for completeness, we give a proof of Lemma 11 being central to our regret analysis.\nProof of Lemma 11. The proof follows the standard analysis of exponential weighting schemes: let wtpiq “ exp ` ´ ηřt´1s“1 cspiq ˘ and let Wt “ ř iPV wtpiq. Then qtpiq “ wtpiq{Wt and we can write\nWt`1\nWt “\nkÿ\ni“1\nwt`1piq Wt\n“ kÿ\ni“1\nwtpiq exp ` ´η ctpiq ˘\nWt\n“ kÿ\ni“1\nqtpiq exp ` ´η ctpiq ˘\nď kÿ\ni“1\nqtpiq ` 1´ ηctpiq ` η2ctpiq2 ˘\n“ 1´ η kÿ\ni“1\nqtpiqctpiq ` η2 kÿ\ni“1\nqtpiqctpiq2 ,\nwhere the inequality uses the inequality ex ď 1` x` x2 valid for x ď 1. Taking logarithms, using logp1´ xq ď ´x for all x ď 1, and summing over t “ 1, . . . , T yields\nlog WT`1\nW1 ď\nTÿ\nt“1\nkÿ\ni“1\n` ´η qtpiqctpiq ` η2 qtpiqctpiq2 ˘ .\nMoreover, for any fixed action i˚, we also have\nlog WT`1 W1 ě log wT`1pkq W1 “ ´η\nTÿ\nt“1\nctpi˚q ´ log k .\nPutting together and rearranging gives the result."
    }, {
      "heading" : "4.6 Learning Continuum–Arm Bandit with Lipschitz Loss Functions",
      "text" : "In this section we turn to show how to reduce the problem of learning Lipschitz functions to MAB with tree-metric movement costs. Specifically we aim at proving Theorem 2. Specifically we prove the following statement, Theorem 12. Set k “ L2{3T 1{3 and η “ 1{ ? kT . Consider a procedure that receives actions from Algorithm 1 and returns as feedback ftp itk q then for every sequence of L-Lipschitz loss functions f1, . . . , fT and an L-Lipschitz metric ∆, we have that:\nRegretMCpf1:T ,∆q “ rO ` L1{3T 2{3 ˘ .\nIn particular, the result holds for L ě 1 and ∆pxt, xt`1q “ |xt ´ xt`1|.\nProof. First note that for every x˚ P r0, 1s we can find x “ t 1 k , 2 k , . . . , 1u such that ftpxq ´ ftpx˚q ď L{k “ L1{3T´1{3, hence\nTÿ\nt“1\n` ftpxq ´ ftpx˚q ˘ “ L1{3T 2{3.\nTherefore if we can show that the regret against every x˚ P t 1 k , 2 k , . . . 1u is bounded by OpL1{3T 2{3q we obtain that the same regret bound is true for every x P r0, 1s. Next, we apply Algorithm 1 on the a fully balanced tree where we associate with the leaves t1, . . . , ku the actions t 1 k , 2 k . . . , 1u. One can then show that |i´j| k ď ∆T pi, jq. We then obtain by Theorem 4 that for every x P t 1 k , 2 k . . . , 1u:\nE\n« Tÿ\nt“1\nftpxtq ff ´min\nx\nTÿ\nt“1\nftpxq “ OpηkT q “ rOpL1{3T 2{3q .\nAs to the second term in the regret we obtain that\nE\n« Tÿ\nt“1\n∆pxt, xt`1q ff ď L Tÿ\nt“1\n|xt ´ xt`1| ď E « L Tÿ\nt“1\n∆T pit, it`1q ff “ rO ˆ L T\nk\n˙ “ rOpL1{3T 2{3q .\nTaken together we obtain that\nE\n« Tÿ\nt“1\nftpxtq ` Tÿ\nt“1\n∆pxt, xt`1q ff ´ min\nxPt 1 k ,...,1u\nTÿ\nt“1\nftpxq “ rOpL1{3T 2{3q ."
    }, {
      "heading" : "5 Online Pricing with Patient Buyers",
      "text" : "In this section we present our reduction of adaptive pricing with patient buyers to a MAB with movement costs.\nThe reduction is presented in Algorithm 2 and uses our algorithm for MAB with movement costs (Algorithm 1) as a black-box. The algorithm divides the time interval T into τ blocks and the updates the price on T “ T {τ rounds. At each round t the algorithm publishes a fixed price for the whole block of τ consecutive days. Then, as feedback, the algorithm receives the mean revenue for those days, which we denote by\nr1t “ 1\nτ\ntτÿ\nk“pt´1qτ`1\nbkpρk, . . . , ρk`τ q .\nThus, we can consider the algorithm as an online algorithm over T rounds: where at each round t the algorithm announces a fixed action ρ1t`1 (the price for the next τ days) and receives at the end of the round as feedback r1t. Note that prices are always announced τ days in advance, as required.\nThe algorithm draws β1, . . . , βT unbiased Bernoulli random variables, and this sequence determines the switches in prices and updates. The algorithm posts a new price only on rounds where βt “ 0 and βt`1 “ 1, and invoke the update rule of Algorithm 1 only on rounds where βt`1 “ 0 and βt`2 “ 1. Note that these two events never co-occur, and further the algorithm exploits the feedback only on days prior to a switch, thus guaranteeing that the feedback is always on days when prices are fixed throughout the present and future block.\nAs discussed briefly in Section 3, the main difficulty in reducing the adaptive pricing problem to MAB, which Algorithm 2 overcomes, is in that the feedback function is not only a function of the current posted price (which is in fact the price tomorrow) but also of past prices. For example, for τ “ 1 the revenue at time t is a function of ρt and ρt`1, where only ρt`1 needs be posted at time t. Algorithm 2 overcomes this issue by employing techniques from [19] for handling adaptive feedback. The tools developed there allow regret minimization when feedback is taken only in time steps when the price is fixed for a period of time. Relying on these techniques, we construct an\nParameters: horizon T , and maximal patience τ Initialize, T “ T {p2τq, k “ T 1{3, η “ 2{ ? Tk Initialize an instance B of SMBpk, ηq Draw i.i.d. unbiased Bernoulli r.v. β0, . . . , βT Sample i1 „ B, set ρ11 “ i1{k Announce prices ρ1 “ ρ2 “ . . . , pτ “ ρ11 For t “ 1, . . . , T (1) If βt “ 0 and βt`1 “ 1, sample it`1 „ B; otherwise set it`1 “ it (2) Set ρ1\nT`1 “ it`1{k and announce prices: ρtτ`1 “ ¨ ¨ ¨ “ ρpt`1qτ “ ρ1t`1\n(3) Collect revenues rpt´1qτ`1, . . . , rtτ and set\nr1tpρ1tq “ 1\nτ\ntτÿ\nk“pt´1qτ`1\nrk\n(4) If βt`1 “ 0, βt`2 “ 1, update B with feedback ft “ 1´ r1tpρ1tq\nAlgorithm 2: Adaptive pricing with patient buyers.\nalgorithm that produces a sequence of prices with low regret if each buyer bt would observe price ρt. However, in our setting, a buyer may buy at a consecutive time steps; the additional cost we suffer is bounded by the potential cost of switching to lower prices, namely, by the movement cost of the algorithm.\nThe main result of this section, stated earlier in Theorem 3, shows that Algorithm 2 attains a regret bound of Opτ1{3T 2{3q against any sequence of buyers with patience at most τ :\nThe remainder of the section is devoted to proving Theorem 3. We begin by establishing additional notation required for the proof. We will denote the expected revenue from the buyers at each block as follows:\nbtpρ1t, ρ1t`1q “ 1\nτ\npt`1qτÿ\nk“tτ`1\nbkpρk, . . . , ρk`τtq .\nNote that since the blocks are of size τ , each buyer can see at most prices that are published on the next block, hence ρk`τt either equals ρ 1 t or ρ 1 t`1. In turn, this means that the expected revenue is indeed a function of ρ1t and ρ 1 t`1 alone.\nWe will further denote the expected revenue from buyers if they observe only the price at time of arrival as follows:\nbtpρ1tq “ 1\nτ\npt`1qτÿ\nk“tτ`1\nbkpρ1t, . . . , ρ1tq .\nFirst, we are estimating the performance on the subsequence of rounds where the algorithm exploits the received feedback.\nLemma 13. Let β1, . . . , βT be a sequence of unbiased Bernoulli random variables, denote\nS “ tt P rT s : βt`1 “ 0, βt`2 “ 1u,\nand denote the elements of S in increasing order S “ tts1 ď ts2 , . . . ,ď ts|S|u. For any price\nρ˚ P t 1 k , 2 k , . . . , 1u, Algorithm 2 enjoys the following guarantee:\nE\n« ÿ\ntPS\nbtpρ˚q ´ btpρ1tq ff “ rOpT 2{3q ,\nand\nE\n» – |S|ÿ\ns“1\n|ρ1ts ´ ρ1ts`1 |\nfi fl “ rOpT 2{3q .\nProof. For each sequence of buyers b1, . . . ,bT , define a sequence of loss functions ℓ1 . . . , ℓT according to:\nℓtpiq “ 1´ bt ˆ i\nk\n˙ .\nFirst note that for every t P S we have ρ1t “ ρ1t`1. The algorithm, in turn, announces the same price ρ1t for all days: tpt ´ 1qτ ` 1, . . . , pt` 1qτu, hence the revenue obtained from buyer bk for every pt ´ 1qτ ` 1 ď k ď tτ is given by btpρ1t, ρ1tq. Hence, the feedback used to update the algorithm B at round t is\nft “ 1´ r1t “ 1´ 1\nτ\ntτÿ\nk“pt´1qτ`1\nbkpρk, . . . , ρk`τ q “ 1´ tτÿ\nk“pt´1qτ`1\n1 τ bkpρ1tq “ ℓtpitq .\nIn words, we have shown that at every step t P S, Algorithm 2 receive action it and return to Algorithm 1 as feedback ℓtpitq. Thus Algorithm 2 applies Algorithm 1 on the sequence of losses tℓtutPS . As a corollary we have that:\nE\n« ÿ\ntPS\nbtpρ˚q ´ btpρ1tq ˇ̌ ˇ̌ ˇ S ff “ E « ÿ\ntPS\nℓtpi˚q ´ ℓtpitq ˇ̌ ˇ̌ ˇ S ff “ Opηk|S|q .\nTaking expectation over S and noting Er|S|s “ 1 4 T we get that\nE\n« ÿ\ntPS\nbtpρ˚q ´ btpρ1t, q ff “ OpT 2{3q .\nAs in Section 4.6, note that if we associate with the prices the corresponding actions on the tree we obtain that |ρ1t´ ρ1t`1| ď ∆T pit, it`1q hence we obtain as a second guarantee that the movement cost of the algorithm is given by\nE\n» – |S|ÿ\ns“1\n|ρ1ts ´ ρ1ts´1 | ˇ̌ ˇ̌ ˇ̌ S fi fl “ E » – |S|ÿ\ns“1\n1 k |its ´ its´1 | ˇ̌ ˇ̌ ˇ̌ S fi fl ď E » – |S|ÿ\ns“1\n1 k ∆pits , its´1q ˇ̌ ˇ̌ ˇ̌ S fi fl “ O p 1 k |S|q .\nAgain taking expectation over S we get that\nE\n» – |S|ÿ\ns“1\n|ρ1ts ´ ρ1ts´1 |\nfi fl “ rO ` 1 k T ˘ .\nNext, we upper bound the regret over the expected regret over the blocks of buyers, b̄t:\nLemma 14. For every ρ˚ P t 1 k , 2 k , . . . , 1u we have that\nE\n» – Tÿ\nt“1\nbtpρ˚q ´ btpρ1t, ρ1t`1q\nfi fl ď 4E « ÿ\ntPS\nbtpρ˚q ´ btpρ1tq ff ` E » – |S|ÿ\ns“1\n|ρ1ts ´ ρ1ts´1 |\nfi fl .\nProof. First note that for every ρ˚ we have\nE\n« ÿ\ntPS\nbtpρ˚q ff “ E « Tÿ\nt“1\nbtpρ˚qβt`2p1´ βt`1q ff .\nSince the Bernoulli random variables are independent of bt and ρ ˚ we get that\nE\n« ÿ\ntPS\nbtpρ˚q ff “ E « Tÿ\nt“1\nbtpρ˚qβt`2p1´ βt`1q ff “ 1\n4 E\n« Tÿ\nt“1\nbtpρ˚q ff . (11)\nSimilarly we have that\nE\n« ÿ\ntPS\nbtpρ1tq ff “ E » – Tÿ\nt“1\nbtpρ1tqβt`2p1´ βt`1q\nfi fl “ 1\n4 E\n» – Tÿ\nt“1\nbtpρ1tq\nfi fl ,\nwhere the equality holds since ρ1t is independent of βt`1 and βt`2. We can bound btpρ1t, ρ1t`1q ě btpρ1t, ρ1tq ´ |ρ1t ´ ρ1t`1|. Hence btpρ1t, ρ1t`1q ě bpρ1tq ´ |ρ1t ´ ρ1t`1|, and we obtain:\nE\n» – Tÿ\nt“1\nbtpρ1t, ρ1t`1q\nfi fl ě E » – Tÿ\nt“1\nbtpρ1tq ´ |ρ1t ´ ρ1t`1|\nfi fl\n“ 4E « ÿ\ntPS\nbtpρ1tq ff ´ Tÿ\nt“1\nE “ |ρ1t ´ ρ1t`1| ‰\n“ 4E « ÿ\ntPS\nbtpρ1tq ff ´ E » – |S|ÿ\nt“s\n|ρ1ts ´ ρ1ts´1 |\nfi fl , (12)\nwhere last equality is true since, we have that ρ1t “ ρ1t`1 unless ρ1t´1 P S in which case we have that ρ1t´1 “ ρ1t “ ρ1ts for some s and ρ1t`1 “ ρ1ts`1 . Taken together with Eqs. (11) and (12) we obtain the desired result.\nWe are now ready to prove the main result of this section.\nProof of Theorem 3. First, for any ρ P t 1 k , . . . , 1u, by employing Lemma 14 we have the following:\nE\n« Tÿ\nt“1\nbtpρ, . . . , ρq ´ btpρt, . . . , ρt`τ q ff “ Tÿ\nt1“1\nt1τÿ\nt“pt1´1qτ`1\n` b1tpρ, . . . , ρq ´ btpρt, . . . , ρt`τ q ˘\n“ τE\n» – Tÿ\nt“1\nbtpρq ´ btpρ1t, ρ1t`1q\nfi fl\nď τ 4 E\n« ÿ\ntPS\nbtpρq ´ btpρ1tq ff ` τE » – |S|ÿ\ns“1\n|ρ1ts ´ ρ1ts´1 |\nfi fl .\nNext, for any ρ˚ P r0, 1s there exist ρ P t 1 k , . . . , 1u such that ρ˚ ą ρ and btpρ˚, . . . , ρ˚q ă btpρ, . . . , ρq ` 1k . Hence, for every ρ˚ P r0, 1s we obtain that\nTÿ\nt“1\nbtpρ˚, . . . , ρ˚q ´ E « Tÿ\nt“1\nbtpρt, . . . ρt`τ q ff\nď τ 4 E\n« ÿ\ntPS\nbtpρq ´ btpρ1tq ff ` τE » – |S|ÿ\ns“1\n|ρ1ts ´ ρ1ts´1 |\nfi fl`OpT\nk q .\nBy Lemma 13 we now obtain\nTÿ\nt“1\nbtpρ˚, . . . , ρ˚q ´ E « Tÿ\nt“1\nbtpρt, . . . , ρt`τ q ff “ O ˆa τkT ` τT\nk ` T k\n˙ “ Opτ 1{3T 2{3q ,\nand using our choice of k gives the result."
    } ],
    "references" : [ {
      "title" : "Online learning in markov decision processes with adversarially chosen transition probability distributions",
      "author" : [ "Yasin Abbasi", "Peter L Bartlett", "Varun Kanade", "Yevgeny Seldin", "Csaba Szepesvari" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Online learning with feedback graphs: Beyond bandits",
      "author" : [ "Noga Alon", "Nicolò Cesa-Bianchi", "Ofer Dekel", "Tomer Koren" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Minimax policies for adversarial and stochastic bandits",
      "author" : [ "Jean-Yves Audibert", "Sébastien Bubeck" ],
      "venue" : "In COLT,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Improved rates for the stochastic continuum-armed bandit problem",
      "author" : [ "P. Auer", "R. Ortner", "C. Szepesvári" ],
      "venue" : "Proceedings of the 20th Annual Conference on Learning Theory, pages 454– 468",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Approximation algorithms and online mechanisms for item pricing",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum" ],
      "venue" : "In Proceedings of the 7th ACM Conference on Electronic Commerce,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Sequential item pricing for unlimited supply",
      "author" : [ "Maria-Florina Balcan", "Florin Constantin" ],
      "venue" : "In International Workshop on Internet and Network Economics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Item pricing for revenue maximization",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Yishay Mansour" ],
      "venue" : "In Proceedings of the 9th ACM conference on Electronic commerce,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Dynamic pricing for impatient bidders",
      "author" : [ "Nikhil Bansal", "Ning Chen", "Neva Cherniavsky", "Atri Rurda", "Baruch Schieber", "Maxim Sviridenko" ],
      "venue" : "ACM Transactions on Algorithms (TALG),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Probabilistic approximations of metric spaces and its algorithmic applications",
      "author" : [ "Yair Bartal" ],
      "venue" : "Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1996
    }, {
      "title" : "Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms",
      "author" : [ "Omar Besbes", "Assaf Zeevi" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "X -armed bandits",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesvári" ],
      "venue" : "Journal of Machine Learning Research, 12:1587–1627",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Kernel-based methods for bandit convex optimization",
      "author" : [ "Sébastien Bubeck", "Ronen Eldan", "Yin Tat Lee" ],
      "venue" : "arXiv preprint arXiv:1607.03084,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "Nicolo Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Regret and convergence bounds for a class of continuum-armed bandit problems",
      "author" : [ "E.W. Cope" ],
      "venue" : "IEEE Transactions on Automatic Control, 54(6):1243–1253",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "Varsha Dani", "Sham M Kakade", "Thomas P Hayes" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Bandits with switching costs: T2/3 regret",
      "author" : [ "Ofer Dekel", "Jian Ding", "Tomer Koren", "Yuval Peres" ],
      "venue" : "In Symposium on Theory of Computing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "The blinded bandit: Learning with adaptive feedback",
      "author" : [ "Ofer Dekel", "Elad Hazan", "Tomer Koren" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Online pricing with strategic and patient buyers",
      "author" : [ "Michal Feldman", "Tomer Koren", "Roi Livni", "Yishay Mansour", "Aviv Zohar" ],
      "venue" : "In Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Multi-armed bandits with metric switching costs",
      "author" : [ "Sudipto Guha", "Kamesh Munagala" ],
      "venue" : "In International Colloquium on Automata, Languages, and Programming,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Sharp dichotomies for regret minimization in metric spaces. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 827–846",
      "author" : [ "Robert Kleinberg", "Aleksandrs Slivkins" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal" ],
      "venue" : "In Proceedings of the fortieth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Nearly tight bounds for the continuum-armed bandit problem",
      "author" : [ "Robert D. Kleinberg" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "The value of knowing a demand curve: Bounds on regret for online posted-price auctions",
      "author" : [ "Robert D. Kleinberg", "Frank Thomson Leighton" ],
      "venue" : "In 44th Symposium on Foundations of Computer Science FOCS,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2003
    }, {
      "title" : "Multi-armed bandits on implicit metric spaces",
      "author" : [ "Aleksandrs Slivkins" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Ranked bandits in metric spaces: learning diverse rankings over large document collections",
      "author" : [ "Aleksandrs Slivkins", "Filip Radlinski", "Sreenivas Gollapudi" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Unimodal bandits",
      "author" : [ "J.Y. Yu", "S. Mannor" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Unfortunately, for the adversarial setting there are mostly hardness results even in limited cases [1].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 17,
      "context" : "In such a setting a tight bound of r Θpk1{3T 2{3q is known [18].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "[18], which applies already for k “ 2 actions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "The bound of r Θp ? kT q for k ě T 1{3 is tight due to the classic lower bound for MAB even without movement costs [5].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : "The main application of our SMB algorithm is for adaptive pricing with patient buyers [20].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "This is in contrast to a regret of r OpT 3{4q which is achieved by applying a standard switching cost technique together with a discretization argument [20].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 17,
      "context" : ", when switching between any two actions has a unit cost), it is known that there is a tight r Ωpk1{3T 2{3q lower bound for the MAB problem [18], which is in contrast to the Op ? kT q regret upper bound without switching costs.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "Classical MAB algorithms such as Exp3 [5] guarantee a regret of r Op ? kT q without movement costs.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "[20].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "There is a slight difference in the exact feedback model between [20] and here: in both models when a buyer arrives, the sell time is uniquely determined; however, in [20] the seller observes the purchase only at the actual time of the sell, whereas here we assume the seller observes the sell when the buyer arrives and decides when to purchase.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "There is a slight difference in the exact feedback model between [20] and here: in both models when a buyer arrives, the sell time is uniquely determined; however, in [20] the seller observes the purchase only at the actual time of the sell, whereas here we assume the seller observes the sell when the buyer arrives and decides when to purchase.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 19,
      "context" : "We remark, though, that as discussed in [20] all lower bounds derived there apply to the current feedback model too.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "For the case of continuous prices and a single seller, when one consider impatient buyers, a simple discretization argument can be used to achieve a regret of r OpT 2{3q, and there exists a similar lower bound of ΩpT 2{3q [25].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 23,
      "context" : "More generally, learning Lipschitz functions on a closed interval has been studied by Kleinberg [24], where an optimal r ΘpT 2{3q regret bound is shown via discretization.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 23,
      "context" : "There are many works on continuous action MAB [24, 16, 4, 12, 28].",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "There are many works on continuous action MAB [24, 16, 4, 12, 28].",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "There are many works on continuous action MAB [24, 16, 4, 12, 28].",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "There are many works on continuous action MAB [24, 16, 4, 12, 28].",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 27,
      "context" : "There are many works on continuous action MAB [24, 16, 4, 12, 28].",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property.",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 25,
      "context" : "Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property.",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : "Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property.",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property.",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "The work of Guha and Munagala [21] discusses a stochastic MAB, in the spirit of the Gittins index, where there is both a switching cost and a play cost, and gives a constant approximation algorithm.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : ", k-HST) has a long history in the online algorithms literature, starting with the work of Bartal [10].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 17,
      "context" : "In any movement cost problem with at least two arms of fixed constant distance, a lower bound regret of 2-arm switching cost applies, hence we observe that these rates are optimal for every k ď T [18].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 23,
      "context" : "We emphasize that even without movement costs, there is an r ΩpT 2{3q lower bound in this setting [24]; hence, the regret bound of Theorem 2 is essentially optimal.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "[20] where the buyer buy at day of purchase.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "Indeed, Kleinberg and Leighton [25] showed that optimizing over the continuum r0, 1s leads to a lower bound of ΩpT 2{3q, irrespective of the patience of the buyers.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 19,
      "context" : "[20] showed that whenever the seller wishes to optimize between more than two prices, a lower bound of ΩpT 2{3q holds for patient buyers.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] constructed a sequence of buyers that reduces the problem to MAB with switching cost: a step in demonstrating a ΩpT 2{3q regret bound: thus a fluctuation in prices is indeed a cause for a high regret.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "We employ a reduction similar to the one used by [25]; however, the patience of the buyers introduce some difficulties, as we discuss below.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "The algorithm is based on the multiplicative update method, and in that sense is reminiscent of the Exp3 algorithm [5].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "4 Concluding the proof To conclude the proof and obtain a regret bound, we will use the following well-known second-order regret bound for the multiplicative weights (MW) method, essentially due to [15] (see also [2] for the version given here).",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 1,
      "context" : "4 Concluding the proof To conclude the proof and obtain a regret bound, we will use the following well-known second-order regret bound for the multiplicative weights (MW) method, essentially due to [15] (see also [2] for the version given here).",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 18,
      "context" : "Algorithm 2 overcomes this issue by employing techniques from [19] for handling adaptive feedback.",
      "startOffset" : 62,
      "endOffset" : 66
    } ],
    "year" : 2017,
    "abstractText" : "We extend the model of Multi-armed Bandit with unit switching cost to incorporate a metric between the actions. We consider the case where the metric over the actions can be modeled by a complete binary tree, and the distance between two leaves is the size of the subtree of their least common ancestor, which abstracts the case that the actions are points on the continuous interval r0, 1s and the switching cost is their distance. In this setting, we give a new algorithm that establishes a regret of r Op ? kT ` T {kq, where k is the number of actions and T is the time horizon. When the set of actions corresponds to whole r0, 1s interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of r ΘpT 2{3q, which is the same rate one obtains when there is no penalty for movements. As our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of r OpT 2{3q compared to the best fixed price in hindsight, which outperform the previous regret bound of r OpT 3{4q for the problem.",
    "creator" : "LaTeX with hyperref package"
  }
}