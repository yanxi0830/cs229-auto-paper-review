{
  "name" : "1706.05148.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Veiled Attributes of the Variational Autoencoder",
    "authors" : [ "Bin Dai", "Yu Wang" ],
    "emails" : [ "daib13@mails.tsinghua.edu.cn", "yw323@cam.ac.uk", "j.aston@statslab.cam.ac.uk", "ganghua@microsoft.com", "davidwip@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We begin with a datasetX = {x(i)}ni=1 composed of n iid samples of some random variable x ∈ Rd of interest, with the goal of estimating a tractable approximation for pθ(x), knowledge of which would allow us to generate new samples of x. Moreover we assume that each sample is governed by unobserved latent variables z ∈ Rκ, such that pθ(x) = ∫ pθ(x|z)p(z)dz, where θ are the parameters defining the distribution we would like to estimate.\nGiven that this integral is intractable in all but the simplest cases, variational autoencoders (VAE) represent a powerful means of optimizing with respect to θ a tractable upper bound on − log pθ(x) [16, 21]. Once these parameters are obtained, we can then generate new samples from pθ(x) by first drawing some z(i) from p(z), and then a new x(i) from pθ(x|z(i)). The VAE upper bound itself is constructed as\nL(θ,φ) = ∑i { KL [ qφ ( z|x(i) ) ||pθ ( z|x(i) )] − log pθ(x(i)) } ≥ −∑i log pθ(x(i)), (1)\nwhere qφ ( z|x(i) ) defines an arbitrary approximating distribution, parameterized by φ, and KL [·||·] denotes the KL divergence between two distributions, which is always a non-negative quantity. For\nar X\niv :1\n70 6.\n05 14\n8v 1\n[ cs\n.L G\n] 1\noptimization purposes, it is often convenient to re-express this bound as\nL(θ,φ) ≡ ∑i ( KL [ qφ ( z|x(i) ) ||p(z) ] − Eqφ(z|xi) [ log pθ ( x(i)|z )]) . (2)\nIn these expressions, qφ (z|x) can be viewed as an encoder model that defines a conditional distribution over the latent ‘code’ z, while pθ (x|z) can be interpreted as a decoder model since, given a code z it quantifies the distribution over x.\nBy far the most common distributional assumptions are that p(z) = N (z; 0, I) and the encoder model satisfies qφ (z|x) = N (z;µz,Σz), where the mean µz and covariance Σz are some function of model parameters φ and the random variable x. Likewise, for the decoder model we assume pθ (x|z) = N (x;µx,Σx) for continuous data, with means and covariances defined analogously.1\nFor arbitrarily parameterized moments µz , Σz , µx, and Σx, the KL divergence in (2) computes to\n2KL [qφ (z|x) ||p(z)] ≡ tr [Σz] + ‖µz‖22 − log |Σz| , (3) excluding irrelevant constants. However, the remaining integral from the expectation term admits no closed-form solution, making direct optimization over θ and φ intractable. Likewise, any detailed analysis of the underlying objective function becomes problematic as well.\nAt least for practical purposes, one way around this is to replace the troublesome expectation with a Monte Carlo stochastic approximation [16, 21]. More specifically we utilize\nEqφ(z|x(i)) [ log pθ ( x(i)|z )] ≈ ∑τt=1 log pθ ( x(i)|z(i,t) ) (4)\nwhere z(i,t) are samples drawn from qφ ( z|x(i) ) . Using a simple reparameterization trick, these samples can be constructed such that gradients with respect to µz and Σz can be propagated through the righthand side of (4). Therefore, assuming all the required moments µz , Σz , µx, and Σx are differentiable with respect to φ and θ, the entire model can be updated using SGD [2].\nWhile quite effective in numerous application domains, certain important mechanisms which dictate the behavior of the VAE are obfuscated by the required stochastic approximation and the opaque underlying objective with high-dimensional integrals. Moreover, it remains unclear to what extent minima remain anchored at desirable locations in the non-convex energy landscape. We take a step towards better quantifying such issues via the following contributions:\n1. By probing the basic VAE model under a few simplifying assumptions of increasing complexity whereby closed-form integrations are (partially) possible, we unveil a number of interesting connections with more transparent, established generative models, each of which shed light on how the VAE may perform under more challenging conditions. This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].\n2. We demonstrate that the canonical form of the VAE, including the Gaussian distributional assumptions described above, harbors an innate agency for robust outlier removal. In fact, when the decoder mean µx is restricted to an affine function of z, we prove that the VAE model collapses to a form of robust PCA [4, 5], a recently celebrated technique for separating data into low-rank (low-dimensional) and sparse outlier components. However, the VAE maintains noteworthy advantages, such as a natural extensibility to handle outlier-robust low-dimensional manifold learning.\n3. We expose that a central, albeit underappreciated role of the VAE encoder covariance Σz is to smooth out undesirable minima in the energy landscape of what would otherwise amount to a traditional deterministic autoencoder (AE) [1], even if at times it may not alter the globally optimal solution, or contribute to sample diversity when operating as a generative model.\nThe remainder of this paper is organized as follows. In Section 2 we consider two affine decoder models and connections with past probabilistic PCA-like approaches. Next we examine various partially affine decoder models in Section 3, whereby only the mean µx is affine while Σx has potentially unlimited complexity. We precisely characterize how minimizers of the VAE cost, although not available in closed form, nonetheless are capable of optimally decomposing data into\n1For discrete data, a Bernoulli distribution is sometimes adopted instead.\nlow-rank and sparse factors while avoiding bad local optima. This section also discusses extensions as well as interesting behavioral properties of the VAE. Section 4 then considers degeneracies in the full VAE model that can arise even with a trivially simple encoder model and latent representation. Section 5 concludes with experiments that directly corroborate a number of interesting, practically-relevant hypotheses generated by our prior theoretical analysis, suggesting novel usages and potential pitfalls. We provide final conclusions in Section 6.\nNotation: We use a superscript (i) to denote quantities associated with the i-th sample, which at times may correspond with the columns of a matrix, such as the data X or related. For a general matrixM , we refer to the i-th row asmi· and the j-th column asm·j . Although technically speaking posterior moments are functions of the parameters {θ,φ}, the random variables x, and the latent z, i.e., µx ≡ µx (z;θ), Σx ≡ Σx (z;θ), µz ≡ µz (x;φ), and Σz ≡ Σz (x;φ), except in cases where some ambiguity exists regarding the arguments, these dependencies are omitted to avoid undue clutter; likewise for µ(i)z , µz(x(i);φ) and Σ(i)z , Σz(x(i);φ). Also, with some abuse of notation, we will use L to denote a number of different VAE-related objective functions and bounds, with varying arguments and context serving as differentiating factors. Finally, the diag[·] operator converts vectors to a diagonal matrix, and vice versa as in Matlab."
    }, {
      "heading" : "2 Affine Decoder and Probabilistic PCA",
      "text" : "If we assume that Σx is fixed at some λI , and force Σz = 0 (while removing the now undefined log |Σz| term), then it is readily apparent that the resultant VAE model reduces to a traditional AE with squared-error loss function [1], a common practical assumption. To see this, note that if Σz = 0, then qφ ( z|x(i) ) collapses to δ(µz), i.e., a delta function at the posterior mean, and Eqφ(z|x(i)) [ log pθ ( x(i)|z )] = log pθ ( x(i)|µ(i)z ) , which is just a standard AE with quadratic loss and representation µx (µz [x]). Moreover, the only remaining (non-constant) regularization from the KL term is ∑ i ‖µ (i) z ‖22. However, given scaling ambiguities that may arise in the decoder when Σz = 0, µ (i) z can often be made arbitrarily small, and therefore the effect of this quadratic penalty is infinitesimal. With affine encoder and decoder models, the resulting deterministic network will simply learn principle components like vanilla PCA, a well-known special case of the AE [3].\nTherefore to understand the VAE, it is crucial to explore the role of non-trivial selections for the encoder and decoder covariances, that serve as both enlightening and differentiating factors. As a step in this direction, we will explore several VAE reductions that lead to more manageable (yet still representative) objective functions and strong connections to existing probabilistic models. In this section we consider the following simplification:\nLemma 1 Suppose that the decoder moments satisfy µx = Wz + b and Σx = λI for some parameters θ = {W , b, λ} of appropriate dimensions. Furthermore, we assume for the encoder we have µz = f(x;φ), Σz = SzS > z , and Sz = g(x;φ), where f and g are any parameterized functional forms that include arbitrary affine transformations for some arrangement of parameters. Under these assumptions, the objective from (2) admits optimal, closed-form solutions for µz and Σz in terms ofW , b, and λ such that the resulting VAE cost collapses to\nL(W , b, λ) = ∑i Ω(i)(W , b, λI) + n log ∣∣∣λI +WW> ∣∣∣, (5)\nwhere\nΩ(i)(W , b,Ψ) , ( x(i) − b )> ( Ψ +WW> )−1 ( x(i) − b ) . (6)\nAdditionally, if we enforce that off-diagonal elements of Σz must be equal to zero (i.e., [Σz]ij = 0 for i 6= j), then (5) further decouples/separates to\nLsep(W , b, λ) = ∑ i Ω (i)(W , b, λI) + n [∑ j log ( λ+ ‖w·j‖22 ) + (d− κ) log λ ] . (7)\nAll proofs are deferred to the appendices. The objective (5) is the same as that used by certain probabilistic PCA models [23], even though the latter is originally derived in a completely different manner. Moreover, it can be shown that any minimum of this objective represents a globally optimal\nsolution (i.e, no minima with suboptimal objective function value exist). And with b and λ fixed, the optimalW will be such that span[W ] equals the span of the singular vectors ofX − b1> associated with singular values greater than √ λ. So the global optimum produces a principal subspace formed by soft-thresholding the singular values ofX − b1>, with the rank one offset often used to normalize samples to have zero mean.2\nIn contrast, the alternative cost (7), which arises from the oft-used practical assumption that Σz is diagonal, represents a rigorous upper bound to (5), since\n∑ j log ( λ+ ‖w·j‖22 ) + (d− κ) log λ ≥ log ∣∣∣λI +WW> ∣∣∣ (8)\nby virtue of Hadamard’s inequality (see proof of Theorem 1 below), with equality iff W>W is diagonal. Interestingly, all minima of the modified cost nonetheless retain global optimality of the original; however, it can be shown that there will be a combinatorial increase in the actual number of distinct (disconnected) minima:3\nTheorem 1 Let R ∈ Rκ×κ denote an arbitrary rotation matrix and P ∈ Rκ×κ an arbitrary permutation matrix. Furthermore letW ∗ be a minimum of (5) andW ∗∗ any minimum of (7) with b and λ fixed. Then the following three properties hold:\n1. L(W ∗, b, λ) = L(W ∗R, b, λ) = Lsep(W ∗∗, b, λ) = L(W ∗∗P , b, λ) = Lsep(W ∗∗P , b, λ). (9)\n2. For anyW ∗∗ (W ∗∗)> with distinct nonzero eigenvalues, there will exist at least κ!(κ−r)! distinct\n(disconnected) minima of (7) located at some UΛP , where UΛ2U> represents the SVD of W ∗∗ (W ∗∗)> and r = rank [W ∗∗].\n3. W ∗∗ will have at most r nonzero columns, whileW ∗ can have any number in {r, . . . , κ}.\nAlthough this result applies to relatively simplistic affine decoders, it nonetheless highlights a couple interesting principles. First, the diagonalization of Σz collapses the space of globally minimizing solutions to a subset of the original. While the consequences of this may be muted with the fully affine decoder model where any of these solutions are equally good, we surmise that with more sophisticated parameterizations this partitioning of the energy landscape to distinct basins-of-attraction could potentially introduce suboptimal local extrema.\nBut there is a second, potentially-advantageous counter-affect as well. Specifically, even if W is overparameterized, meaning that κ is unnecessarily large, there exists an inherent mechanism to prune superfluous columns to zero. Regardless, we have shown that both variants of the affine decoder model lead to reasonable probabilistic PCA-like objectives regardless of how overparametered µz and Σz happen to be."
    }, {
      "heading" : "3 Partially Affine Decoder and Robust PCA",
      "text" : "Thus far we have considered tight limitations on the complexity allowable in the functional forms of both µx and Σx, while µz and Σz were free-range variables granted arbitrary flexibility. We now turn our gaze to the case where Σx can be any parameterized, diagonal matrix4 while µx remains restricted. Although this administers considerable capacity to the model at the potential risk of overfitting, we will soon see that the VAE is nonetheless able to self-regularize in a very precise sense: Global minimizers of the VAE objective will ultimately correspond with optimal solutions to\nmin L,S\nn · rank [L] + ‖S‖0, s.t. X = L+ S, (10)\n2While we omit details here, optimal solutions for both b and λ can be analyzed as well. 3By disconnected we mean that, to traverse from one minimum to another, we must ascend the objective function at some point along the way. 4A full covariance over x is infeasible given the high dimension, and can lead to undesirable degeneracies anyway.\nwhere ‖ · ‖0 denotes the `0 norm, or a count of the number of nonzero elements in a vector or matrix. This problem represents the canonical form of robust principal component analysis (RPCA) [4, 5], decomposing a data matrixX into low-rank principal factors L = UV and a sparse outlier component S. In fact, if we were to replace the `0 penalty with an `2 norm, we would exactly recover one formulation of vanilla PCA, where by replacing the weighting factor n with different arbitrary scalars, the optimal solution will simply reflect principal subspaces of varying dimensions. That the complex, probabilistic VAE model shares any kinship with (10) is seemingly quite remarkable.\nBefore elucidating this relationship, we require one additional technical caveat. Specifically, since log 0 and 10 are both undefined, and yet we will soon require an alliance with degenerate (or nearly so) covariance matrices that mimic the behavior of sparse and low-rank factors through log-det and inverse terms, we must place the mildest of restrictions on the minimal allowable singular values of Σx and Σz . For this purpose we define Smα as the set of m × m covariance matrices with singular values all greater than or equal to α, and likewise S̄mα as the subset of Smα containing only diagonal matrices. We also define suppα(x) = {i : |xi| > α}, noting that per this definition, supp0(x) = supp(x), meaning we recover the standard definition of support: the set of indices associated with nonzero elements.\nGiven the affine assumption from above, and the mild restriction Σx ∈ S̄dα and Σz ∈ Sκα for some small α > 0, the resulting constrained VAE minimization problem can be expressed as\nmin θ,φ\nL ( W , b = 0,Σx ∈ S̄dα,µz,Σz ∈ Sκα ) , (11)\nwhere now θ includes W as well as all the parameters embedded in Σx, while µz and Σz are parameterized as in Lemma 1. We have also set b = 0 merely for ease of presentation as its role is minor. We then have the following:\nTheorem 2 Suppose that X = {x(i)}ni=1 admits a feasible decomposition X = UV + S that uniquely5 optimizes (10). Then for some ᾱ sufficiently small, and all α ∈ (0, ᾱ], any global minimum {Ŵ , Σ̂x, µ̂z, Σ̂z} of (11) will be such that6\nspan[Ŵ ] = span[U ] and suppα ( diag [ Σ̂x ( µ̂z [ x(i) ])]) = supp[s(i)] (12)\nfor all i provided that the latent representation satisfies κ ≥ rank [U ].\nSeveral important remarks are warranted here regarding this result:\n• Given Ŵ , Σ̂x, and µ̂z as described, we can directly recover the generating low-rank L and sparse S by solving a simple linear system. Therefore if we can globally optimize the VAE objective, we can recover the correct latent representation. • The requirements Σx ∈ S̄dα and Σz ∈ Sκα do not portend the need for specialized tuning\nor brittleness of the result; these are merely technical conditions for dealing with degenerate covariances that occur near optimal solutions. While it might seem natural that Σx has diagonal elements pushed to zero in regions where near perfect data fit is possible, less intuitively, global optima of (11) can be achieved with an arbitrarily small Σz , e.g., Σz = αI (see proof construction). Perhaps counterintuitively, this implies that in areas surrounding a global optimum, the VAE objective can resemble that of a regular AE. As we will discuss more below, desirable smoothing effects of integration over Σz occur elsewhere in the energy landscape while preserving extrema anchored at the correct latent representation. • Even if κ is large, meaningW is possibly overcomplete, the VAE will not overfit in the sense\nthat there exists an inherent regulatory effect pushing span[W ] towards span[U ]. • If the globally optimal solution to (10) is not unique (this is different from uniqueness regarding\nthe VAE objective), then a low-rank-plus-sparse model may not be the most reasonable, parsimonious representation of the data to begin with, and exact recovery of L and S will not be\n5Obviously only L and S will be unique; the actual decomposition of L into U and V is indeterminate up to an inconsequential invertible transform.\n6Although somewhat cumbersome in print, the expression Σ̂x ( µ̂z [ x(i) ]) refers to Σ̂x evaluated at µ̂z ,\nwhere the latter is evaluated at x(i), the i-th sample.\npossible by any algorithm without further assumptions. More concretely, an arbitrary data point x(i) ∈ Rd requires d degrees-of-freedom to represent; however, if the data succinctly adheres to the RPCA model, then for properly chosen U , V , and S, we can have x(i) = Uv(i) + s(i), where ‖v(i)‖0 + ‖s(i)‖0 < d. Arbitrary data in general position will never admit such a unique decomposition, and we should only expect such structure in data well-represented by our VAE model, or the original RPCA predecessor from (10). • A number of celebrated results have stipulated conditions [4, 5] whereby global solutions of the\nconvex relaxation into nuclear and `1 norm components given by\nmin L,S\n√ n · rank ‖L‖∗ + ‖S‖1, s.t. X = L+ S, (13)\nwill equal global solutions of (10). While elegant in theory, and practically relevant given that (10) is discontinuous, non-convex, and difficult to optimize, the required conditions for this equivalence to hold place strong restrictions on the allowable structure in L and support pattern in S. In practice these conditions can never be verified and are unlikely to hold, so an alternative modeling approach such as the VAE, which can be viewed as a smoothed version of (10) when an affine decoder mean is used (more on this later), remains attractive. Additionally, there is no clear way to modify (13) to handle nonlinear manifolds, which is obviously the bread and butter of the VAE.\nWe emphasize that these conclusions are not the product of an overly contrived situation, given that a significant restriction is only placed on µx; all other posterior quantities are essentially unconstrained provided a sufficient lower complexity bound is exceeded, implying that the result will hold whenever a sufficiently complex deep network is used for each respective quantity. Moreover, although we must defer to a longer journal version to present a formal treatment, with some mild additional conditions, Theorem 2 can naturally be extended to the case where the deocoder mean function is generalized to subsume non-linear, union-of-subspace models as commonly used in subspace clustering problems [9, 20]. This furthers the argument that the analysis presented here transitions to broader scenarios.\nMoving forward, as a point of comparison it is also interesting to examine how a traditional AE, which emerges when Σz is forced to zero, behaves under analogous conditions to Theorem 2.\nCorollary 1 Under the same conditions as Theorem 2, if we remove the log |Σz| term and assume Σz = 0 elsewhere, then (11) admits a closed-form solution for Σx in terms ofW and µz such that minimizers of the VAE cost are minimizers of\nL (W ,µz) = ∑\ni\n∥∥∥x(i) −Wµz ( x(i) )∥∥∥ 0 in the limit α→ 0. (14)\nFrom this result we immediately observe that minimization of (14) is just a constrained version of (10), exactly equivalent to solving\nmin L,S\n‖S‖0, s.t. X = L+ S, rank [L] = κ. (15)\nThis expression immediately exposes one weakness of the AE; namely, if κ is too large, there is no longer any operation in place to prune away unnecessary dimensions, and the trivial solution L = X will be produced. In the large-κ regime then, global VAE and global AE solutions do in fact deviate, ultimately because of the removal of the log |Σz| term in the latter. But there is also a more important, yet subtle, advantage of the VAE over both (15) and the original unconstrained RPCA model from (10). For both RPCA constructions, any feasible support pattern, even the trivial ones associated with non-interesting decompositions satisfying ‖v(i)‖0 + ‖s(i)‖0 ≥ d for some i, will necessarily represent a local minimum, since there is an infinite gradient to overcome to move from a zero-valued element of S to a nonzero one.\nUnlike these deterministic approaches, the behavior of the VAE reflects a form of differential smoothing that rids the model of many of these pitfalls while retaining desirable minima that satisfy (12).7 Based on details of the proof of Theorem 2, it can be shown that, excluding small-order terms\n7A more rudimentary form of this smoothing has been observed in much simpler empirical Bayesian models derived using Fenchel duality [24].\ndependent on other variables and a constant scale factor of − logα, then a lower bound on the VAE objective associated with each sample index i behaves like\nrank[W ] + suppα ( diag [ Σx ( µz [ x(i) ])]) , (16)\nbut crucially, this behavior lasts only as long as (16) is strictly less than d and Σz is forced to be small or degenerate. In contrast, when the value is at or above d, (16) no longer reflects the energy function, which becomes relatively flat because of smoothing via Σz , avoiding the pitfalls described above.\nTo situate things in the narrative of (10), the VAE can be viewed (at least to first order approximation) as minimizing the alternative objective function\n∑ i rank [ LL> + diag ( s(i) )2] ≥ n · rank [L] + ‖S‖0, (17)\nor a smooth surrogate thereof, over the constraint set X = L + S. The advantages of this upper bound are substantial: As long as a unique solution exists to the RPCA problem, the globally optimal solution with ‖v(i)‖0 + ‖s(i)‖0 < d for all i will be unchanged; however, any feasible solution with ‖v(i)‖0 + ‖s(i)‖0 ≥ d will have a constant cost via the expression on the left of the inequality, truncating the many erratic peaks that will necessarily occur with the energy on the righthand side.\nIn fact, away from the strongly attractive basins of optimal VAE solutions, the KL term from (2) is likely to push Σz more towards\narg min Σz 0 KL [qφ (z|x) ||p(z)] ≡ arg min Σz 0 tr [Σz]− log |Σz| = I. (18)\nExperiments presented in Section 5 confirm that this is indeed the case. And once Σz moves away from zero, it will generally contribute a strong smoothing effect via the expectation in (2). However, there exists an important previously unobserved caveat here: If the decoder mean function is excessively complex, it can potentially squash all effects from Σz , leading to undesirable degenerate solutions with no function value as described next."
    }, {
      "heading" : "4 Degeneracies Arising from a Flexible Decoder Mean",
      "text" : "In this section we consider the case where µx is finally released from its affine captivity to join with posterior colleagues in the wild. That simultaneously granting µx, Σx, µz , and Σz unlimited freedom leads to overfitting may not come as a surprise; however, it turns out that even if the latter three are severely constrained, overfitting will not be avoided when µx is over-parameterized. This is because, at least at a high level, the regulatory effect of Σz can be squashed in these situations leading to the following:\nTheorem 3 Suppose κ = 1 (i.e., a latent dimension of only one), Σz = λz (a scalar), µz = a>x for some fixed vector a, Σx = λxI , and µx is an arbitrary piecewise linear function with n segments. Then the VAE objective is unbounded from below at a trivial solution {λ̂z, â, λ̂x, µ̂x} such that the resulting posterior mean µ̂x(z;θ) will satisfy µ̂x(z;θ) ∈ {x(i)}ni=1 with probability one for any z.\nIn this special case, Σx, Σz , and µz are all simple affine functions and the latent dimension is minimal, and yet an essentially useless, degenerate solution can arbitrarily optimize the VAE objective. This occurs because the VAE has limited power to coral certain types of heavily over-parameterized decoder mean functions, even when all other degrees of freedom are constrained, and in this regime the VAE essentially has no advantage whatsoever over a traditional autoencoder (its natural selfregulatory agency may sometimes break down). In contrast, as we saw in a previous section, there is no problem taming the influences of an unlimited latent representation (meaning κ is large, e.g., even κ > n) and its huge, attendant parameterized mean function, provided the latter is affine, as in µx = Wz + b.\nIndeed then, the issue is clearly not the degree of over-parameterization in µx per se, but the actual structures in place. And the key problem is that, at least in some situations, the model can completely override the regulatory mechanism of the KL term, pushing the latent variances towards zero. For example, in the context of Theorem 3, the piecewise linear structure of µx allows the decoder to\nact much like a vector quantization process, pushing z towards training samples x(i). And because this will lead to perfect reconstruction error if the correct sample index i is found for a particular z(i), Σx = λxI ≈ 0 serves as a reasonable characterization of posterior uncertainty, pushing p(x(i)|z(i)) → δ ( x(i) ) provided that z(i) ≈ µz ( x(i);a ) = a>x(i), or Σz = λz is not too large. This in turn leads to a useless, degenerate solution, either for the purposes of generating representative samples, or for outlier removal as we have described herein.\nOf course an analogous issue exists with generative adversarial networks (GAN) as well, a popular competing deep generative model composed of a generator network analogous to the VAE decoder, and a discriminator network that replaces the VAE encoder in a loose sense [12]. If the generator network merely learns a segmentation of z-space such that all points in the i-th partition map to x(i), the discriminator will be helpless to avert this degenerate situation even in principle. But there is an asymmetry when it comes to the GAN discriminator network and the VAE encoder: Over-parameterization of the former can be problematic (e.g., it can easily out-wit an affine or other proportionally simple generator), but the latter not so, at least in the sense that a highly flexible VAE encoder need not bully a simple decoder into trivial solutions as we have shown in previous sections."
    }, {
      "heading" : "5 Experiments",
      "text" : "Theoretical analysis of simplified cases can be viewed as a powerful vehicle for generating accessible hypotheses that describe likely behavior in more complex, practical situations. In this section we empirically evaluate three concrete hypotheses that directly emanate from our previous analyses. Each of these have wide-ranging consequences in terms of how VAE models should be applied and interpreted in practice:\n(i) When the decoder mean function is allowed to be nonlinear, the VAE should behave like a nonlinear extension of RPCA, but with natural regularization mechanisms in place that help to avoid local minima and/or overfitting to outliers. It is therefore likely to outperform either RPCA algorithms or an AE on diverse manifold recovery/outlier discovery problems unrelated to the generative modeling tasks for which the VAE was originally designed.\n(ii) If the VAE latent representation z is larger than needed (meaning its dimension κ is higher than the underlying data manifold dimension), we have proven that unnecessary columns of W in a certain affine decoder mean model µx = Wz + b will automatically be pruned as desired. Analogously, in the arbitrary nonlinear case we would then expect that columns of the weight matrix from the first layer of the decoder mean function should be pushed to zero, again effectively pruning away the impact of any superfluous elements of z.\n(iii) Although originally promoted as a probabilistic generative model, when granted sufficient capacity in both µx (µz [x]) and Σx to model inliers and outliers respectively, the VAE should have a tendency to push elements of the encoder covariance Σz to exactly zero, overriding the KL regularizer that would otherwise push these values towards one. In doing so, the VAE’s value as a true (or non-degenerate) generative model is undermined; however, its utility as a nonlinear outlier removal tool is dramatically enhanced per Hypothesis (i) above."
    }, {
      "heading" : "5.1 Hypothesis (i) Evaluation",
      "text" : "If our theory is generally applicable, then a VAE with suitable parameterization should be able to significantly outperform an analogous deterministic AE (i.e., the VAE with Σz = 0) on the task of recovering data points drawn from a low-dimensional nonlinear manifold, but corrupted with gross outliers. We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24]. These plots evaluate the reconstruction quality of competing algorithms for every pairing of subspace dimension and outlier ratio, creating a heat map that differentiates success and failure regions.\nOf course explicit knowledge of the ground-truth low-dimensional manifold is required to accomplish this. With linear subspaces it is trivial to generate appropriate synthetic data by simply creating two low-rank random matricesU ∈ Rd×κ and V ∈ Rκ×n, a sparse outlier matrix S, and then computing X = L + S with L = UV . Algorithms are presented with X and attempt to reconstruct L. We generalize this process to the nonlinear regime in the following way.\nData Generation: First we draw n low-dimensional samples z(i) ∈ Rκ from N (z; 0, I) and pass them through a 3-layer network with ReLU activations [19]. The layer sizes are 1000, 2000, and d respectively, each created using the initialization procedure from [13]. The d-dimensional output is denoted as l(i), the collection of which form a matrix L, with columns effectively lying on a κ-dimensional manifold. This network can be viewed as a ground-truth decoder, projecting z(i) to clean samples l(i).\nBut we must also ensure that there exists a ground-truth encoder that can correctly invert the decoder (otherwise we cannot be sure that our VAE structure is even an appropriate model, unlike the linear RPCA case where this is trivially satisfied). We learn this inverse mapping by training something like an inverted autoencoder. Basically, the decoder described above now acts as an encoder, to which we append a new 3-layer ReLU network of sizes 2000, 1000, and κ, such that the entire structure is κ-1000-2000-d-2000-1000-κ. If any z(i) passes through this network with zero reconstruction error, it implies that the corresponding l(i) can pass through the flipped network with zero reconstruction error, and we have found our ground truth network.\nWe could train the entire system end-to-end to accomplish this, which should be easy since κ d; however, we found that although z(i) = ẑ(i) is obviously not difficult to achieve, the corresponding learned samples l(i) are pushed to very near a low-rank matrix when assembled into L. This would imply that non-linear manifold learning is not actually required. To circumvent this issue, we instead held the initial κ-1000-2000-d structure fixed, which ensures that the rank of L cannot be altered, and only train the second half using a standard `2 loss. In doing so we are able to obtain an L matrix, extracted from the middle layer, that is both (i) not well-represented by a low-rank approximation, and (ii) does lies on a known low-dimensional non-linear manifold. And the learned decoder from this process implicitly becomes the ground-truth encoder underlying the data structure.\nOnce L has been created in this manner, we then generate noisy samplesX by randomly corrupting 100 · ν% of the entries, replacing the original value with samples from a standardized Gaussian distribution. The resulting data matrix is fed to a VAE formed from the inverted d-2000-1000-κ1000-2000-d structure to form the encoder and decoder mean functions, but randomly initialized so as not to copy the ground-truth template. The encoder covariance shares the first two layers, and also has an exponential layer appended at the output to produce only positive values, consistent with the design in [16]. We choose d = 100, and vary the manifold dimension from κ = 2, 4, . . . , 20 while the outlier ratio ranges as ν = 0.05, 0.10, . . . , 0.50. In each case we measure the normalized MSE recovering the true L.\nResults: We compare the VAE against the corresponding AE, and include the ground-truth manifold within the parameterization of the cascaded encoder/decoder mean networks µx (µz [x]) for all models. Therefore perfect reconstruction is theoretically possible by either approach provided that outlier contributions can be mitigated. Note that once Σz = 0 for the AE, at every sample Σx can be solved for in closed form as [ Σ(i)x ] jj = ( x (i) j − µ (i) xj )2 for j = 1, . . . , d assuming sufficient capacity\nper Corollary 1. We then plug this value into the AE cost, effectively optimizing Σ(i)x out of the model making it entirely deterministic. For direct comparison, we apply the same procedure to the VAE. As an additional baseline, we also apply the convex RPCA formulation from 13 to the same corrupted data.\nResults are shown in Figure 1, where the VAE outperforms both RPCA and AE by a wide margin. Perhaps most notably, the VAE outperforms the regular AE, supporting our theory that the smoothing effect of integrating over Σz has immense practical value in avoiding bad minimizing solutions through its unique form of differential regularization. Even though by design both network structures are equivalent in terms of their predictive capacity, only the VAE is able to capitalize on the regularizing effect of Σz to actually reach a good solution in challenging conditions. In contrast, we found that in many cases the AE had a lower MSE with respect to the noisy matrixX than the clean L, a sign that it was stuck at a useless extrema which overfit the data. This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation."
    }, {
      "heading" : "5.2 Hypothesis (ii) Evaluation",
      "text" : "We train analogous AE and VAE models with nonlinear decoder mean networks on the MNIST dataset of handwritten digit images [17] as κ is varied. We use all n = 70000 samples, each of size 28× 28. The encoder is a 4-layer ReLU network (d-1000-500-250-κ), where d = 28× 28 = 784. We draw values of κ from {3, 5, 8, 10, 15, 20, 25, 30, 35, 40}. Also, µz and Σz share the first three hidden layers, but have separate output layers, similar to [16]. The decoder is a 4-layer network which simply reverses the encoder structure.\nThe first layer of the decoder mean network can be expressed as h1 = W 1z + b1 (before the nonlinearity), which in isolation is equivalent to the affine decoder mean model. A nonzero column inW 1 implies that the corresponding dimension in z is actually useless for any subsequent representation. Therefore, we can estimate the intrinsic dimension of the latent code by counting the number of nonzero columns in W 1. Of course in practice, it is unlikely that a column of W 1 converges all the way to exactly 0 via SGD. So we define a simple threshold by thr = 0.01×maxκj=1 ||w·j ||2. If ||w·j ||2 < thr, we regard it as a zero column. Figure 2 displays the number of nonzero columns produced by each model averaged across 10 trials. We observe that when κ > 15, the number of nonzero columns plateaus for the VAE consistent with Hypothesis (ii). In contrast, with the AE model the number is always equal to κ since there is no agency for self-regularization."
    }, {
      "heading" : "5.3 Hypothesis (iii) Evaluation",
      "text" : "We create histograms of all diagonal elements of {\nΣ(i)z }n i=1\nas noise ratios and manifold dimensions vary under the same conditions described in Section 5.1. The results are plotted in Figure 3, where we observe that on easier problems, meaning smaller outlier ratios and/or lower manifold dimension, the diagonal elements of Σ(i)z are forced towards zero. For this to happen, µx (µz [x]) must accurately fit the inlier positions while Σx must reflect the outlier support where needed. This situation allows the data fit term to overpower the KL regularizer that would otherwise prevent Σ(i)z from ever being rank-deficient in the region surrounding a global optima of the VAE energy function. In contrast, for harder problems (i.e., higher outlier ratio and/or manifold dimension), tightly fitting the data becomes difficult and the KL regularizer pushes Σ(i)z diagonal values more towards one given the influence of (18) in affecting global solutions.\nConsistent with our theory, these results then imply a rather curious behavior of the VAE: If µx (µz [x]) is suitably parameterized to model inlier samples, and Σx is sufficiently complex to model outlier locations, then Σz will be pushed towards zero possibly compromising the VAE model’s ability to act as a non-degenerate generative model. It may still capture manifold structure, but when attempting to generate new samples they may not be distributed on or around the manifold in the same way as the training set. However, we can nonetheless enjoy perfect recovery of original clean training data samples. In contrast, if µx (µz [x]) and/or Σx are inadequate, or alternatively, we provide a lower bound for allowable values of Σz , then such degeneracies are avoided at the expense of exact recovery of L fromX ."
    }, {
      "heading" : "6 Discussion",
      "text" : "Although originally developed as a viable deep generative model or tractable bound on the data likelihood, in this work we have revealed certain behaviors of the VAE that are not obvious from first inspection. For example, in addition to its putative role in driving diversity into the learned generative process, the latent covariance Σz also serves as an important smoothing mechanism that aids in the robust recovery of corrupted samples, even if sometimes this requires exhibiting behavior (i.e., convergence towards zero) that may be at odds with its original design purpose."
    }, {
      "heading" : "Appendix A − Additional MNIST Dataset Experiment",
      "text" : "Here we examine practical denoising of MNIST data corrupted with outliers using a VAE model. Outliers are added to MNIST handwritten digit data [17] by randomly replacing from 5% to 50% of the pixels with a value uniformly sampled from [0, 255] to createX . The encoder is the same as in Section ??, i.e., a 4-layer ReLU network (d-1000-500-250-κ), where we choose κ = 30 for the dimension of z. We train a VAE using both τ = 1 and τ = 5 latent samples {z(i,t)}τt=1 for each x(i), observing that the latter, which more closely approximates the posterior, should perform significantly better.\nWe compare the VAE against convex RPCA on the task of recovering the original, uncorrupted digits. Note that RPCA is commonly used for unsupervised cleaning of this type of data [9], and MNIST is known to have significant low-rank structure [18] as shown in Figure 4(a). Regardless, we observe in Figure 4(b) that the VAE performs significantly better in terms of normalized MSE by capturing additional manifold details that deviate from a purely low-rank representation. Furthermore, we hypothesize that using extra latent samples (the τ = 5 case) may work better on outlier removal tasks given the strong need for accurate smoothing of the VAE objective as described previously."
    }, {
      "heading" : "Appendix B − Proof of Lemma 1",
      "text" : "Under the stated assumptions, the VAE cost can be simplified as\nL(θ,φ) = ∑\ni\n{ Eqφ(z|x(i)) [ 1 λ ∥∥∥x(i) −Wz − b ∥∥∥ 2\n2\n] + d log λ\n+ tr [ Σ(i)z ] − log ∣∣∣Σ(i)z ∣∣∣+ ‖µ(i)z ‖22 } .\n= ∑\ni\n{ 1 λ ∥∥∥x(i) −Wµ(i)z − b ∥∥∥ 2\n2 + 1λ tr\n[ Σ(i)z W >W ] + d log λ\n+ tr [ Σ(i)z ] − log ∣∣∣Σ(i)z ∣∣∣+ ‖µ(i)z ‖22 } , (19)\nwhere µ(i)z , µz ( x(i);φ ) and Σ(i)z , Σz ( x(i);φ ) . Given that\nlog ∣∣∣AA> ∣∣∣ = arg inf Γ 0 tr [ AA>Γ−1 ] + log |Γ| , (20)\nwhen optimization is carried out over positive definite matrices Γ, minimization of (19) with respect to Σ(i)z leads to the revised objective\nL(θ,φ) ≡ ∑\ni\n{ 1 λ ∥∥∥x(i) −Wµ(i)z − b ∥∥∥ 2\n2 + log\n∣∣∣ 1λW >W + I ∣∣∣+ d log λ+ ‖µ(i)z ‖22 } ,\n= ∑\ni\n{ 1 λ ∥∥∥x(i) −Wµ(i)z − b ∥∥∥ 2\n2 + log\n∣∣∣WW> + λI ∣∣∣+ ‖µ(i)z ‖22 } , (21)\nignoring constant terms. This expression only requires that Σ(i)z = [ 1 λW >W + I ]−1 , or a constant\nparameterization, independent of x(i). Similarly we can optimize over µ(i)z in terms of the other variables. This is just a ridge regression problem, with optimal solution\nµ(i)z = W > ( λI +WW> )−1 ( x(i) − b ) , (22)\nor a simple linear function of x(i). Hence as long as the parameterization of both µ(i)z and Σ(i)z allows for arbitrary affine functions as stipulated in the lemma statement, these optimal solutions are feasible. Plugging (22) into (21) and applying some basic linear algebra, we arrive at\nL(θ,φ) ≡ ∑\ni\n( x(i) − b )> ( WW> + λI )−1 ( x(i) − b ) + n log ∣∣∣WW> + λI ∣∣∣ . (23)\nFinally, in the event that we enforce that Σ(i)z be diagonal, (21) must be modified via\nΣ(i)z = [ 1 λdiag ( diag [ W>W ]) + I ]−1 = κ∑\nj=1\nlog ( λ+ ‖w·j‖22 ) − κ log λ, (24)\nwhere the diag[·] operator converts vectors to diagonal matrices, and a matrix to a vector formed from its diagonal (just as in the Matlab computing environment), leading to the stated result."
    }, {
      "heading" : "Appendix C − Proof of Theorem 1",
      "text" : "First, for part 1 on the theorem, given thatWRR>W> = WPP>W> = WW> for any rotation R and permutation P , then obviously ifW ∗ is a minimum of (5),W ∗R andW ∗P must also be. Likewise, since ∑κ j=1 log ( λ+ ‖w·j‖22 ) is invariant to the order of the summation, then ifW ∗∗ is a minimum of ( 7),W ∗∗P must be as well.\nWe also have that\nLsep(W ∗∗, b, λ) = ∑\ni\nΩ(i)(W ∗∗, b, λI) + n\n ∑\nj\nlog ( λ+ ‖w∗∗·j ‖22 ) + (d− κ) log λ\n \n= ∑\ni\nΩ(i)(W ∗∗, b, λI) + n\n ∑\nj\nlog ( 1 + 1λ‖w∗∗·j ‖22 ) + d log λ\n \n≥ ∑\ni\nΩ(i)(W ∗∗, b, λI) + n [ log ∣∣∣ 1λ (W ∗∗)>W ∗∗ + I ∣∣∣+ d log λ ]\n= ∑\ni\nΩ(i)(W ∗∗, b, λI) + n log ∣∣∣λI +W ∗∗R (W ∗∗R)> ∣∣∣\n≥ ∑\ni\nΩ(i)(W ∗, b, λI) + n log ∣∣∣λI +W ∗R (W ∗R)> ∣∣∣ (25)\nwhere the the second inequality follows from the fact thatW ∗ is an optimal solution to ( 5). The first inequality stems from Hadamard’s inequality [10] applied to\n1 λ (W\n∗∗)>W ∗∗ + I = M>M (26)\nfor some square matrixM of appropriate dimension. This results in\nlog ∣∣∣ 1λ (W ∗∗)>W ∗∗ + I ∣∣∣ = 2 log |M | ≤ 2 log\n ∏\nj\n‖m·j‖2   = ∑\nj\nlog ( 1 + 1λ‖w∗∗·j ‖22 ) ,\n(27) with equality iffM>M is diagonal. We can further manipulate the log-det term in (25) via\nn log ∣∣∣λI +W ∗R (W ∗R)> ∣∣∣ = log ∣∣∣ 1λ (W ∗R)>W ∗R+ I ∣∣∣+ d log λ\n= log ∣∣∣∣ 1λ ( UΛV >R )> UΛV >R+ I ∣∣∣∣+ d log λ = log ∣∣∣ 1λR >V Λ2V >R+ I ∣∣∣+ d log λ,\nwhere UΛV > is the SVD of W ∗. Now if we choose R = V and define W̄ , WV , then Λjj = ‖w̄·j‖22 and this expression further reduces via\nlog ∣∣∣ 1λR >V Λ2V >R+ I ∣∣∣+ d log λ = ∑\nj\nlog ( 1 + 1λΛjj ) + d log λ\n= ∑\nj\nlog ( λ+ ‖w̄·j‖22 ) + (d− κ) log λ. (28)\nOf course we cannot have\n∑\ni\nΩ(i)(W ∗∗, b, λI) + n\n ∑\nj\nlog ( λ+ ‖w∗∗·j ‖22 ) + (d− κ) log λ   (29)\n> ∑\ni\nΩ(i)(W̄ , b, λI) + n\n ∑\nj\nlog ( λ+ ‖w̄·j‖22 ) + (d− κ) log λ   ,\notherwise W ∗∗ would not be a minimum of ( 7). Therefore, W̄ must also be a minimum of ( 7), from which the remaining parts of ( 9) immediately follows.\nWe next confront the arrangement of disconnected minima for part 2 of the theorem. It is not difficult to show that ( 5), and by virtue of the analysis above ( 7), will be uniquely minimized by U and Λ arising from the SVD of either W ∗ or equivalently W ∗∗. Let W ∗∗ = UΛV > via such\na decomposition. So any partitioning into disconnected minimizers must come at the hands of V , which only influences the ∑ j log ( λ+ ‖w·j‖22 ) term in ( 7).\nAt stated above, ifW ∗∗ is a minimum, thenW ∗∗P must also be a minimum. Assume for the moment thatW ∗∗ is full column rank. There will obviously be r! unique permutations of its columns, with r = rank[W ∗∗]. Moreover, any transition from some permutation P ′ to another P ′′ will necessarily involve some non-permutation-matrix rotation V . Given our assumption of distinct eigenvalues, this will ensure that\n1 λ (W ∗∗)>W ∗∗ + I = 1λV Λ 2V > + I (30)\nis non-diagonal. While this will not increase ( 5), it must increase ( 7) when diagonalized by Hadamard’s inequality. Therefore every permutation will reflect a distinct, disconnected minimizer. If W ∗∗ also has κ− r zero-valued columns, then the resulting number of unique permutations increases to κ!κ−r by standard rules of combinatorics.\nFinally, part 3 of the theorem follows directly from part 2: Given that any minimizer of ( 7) must be of the form UΛP , then there cannot be more than r nonzero columns. In contrast, for ( 5) we may apply any arbitrary rotation toW ∗, and hence all columns can be nonzero even if the rank is smaller than κ."
    }, {
      "heading" : "Appendix D − Proof of Theorem 2",
      "text" : "The basic high-level strategy here is as follows: We first present a candidate solution that satisfies (12) and carefully quantify the achievable objective function value for α ∈ (0, ᾱ], and ᾱ small. We then analyze a lower bound on the VAE cost and demonstrate that no solution can do significantly better, namely, any solution that can match the performance of our original proposal must necessarily also satisfy (12). Given that this is a lower bound, this implies that no other solution can both minimize the VAE objective and not satisfy (12). We now proceed to the details. Define µ(i)z , µz ( x(i);φ ) and Σ(i)z , Σz ( x(i);φ ) . We first note that if z = µ(i)z + S(i)z , with\nS(i)z satisfying Σ (i) z = S (i) z ( S(i)z )> , and ∼ p( ) = N ( ; 0, I), then z ∼ qφ ( z|x(i) ) . With this\nreparameterization and\nµ(i)x , Wµ(i)z +WS(i)z , diag[Σ(i)x ] , ν ( µ(i)z + S (i) z ;θ ) for some function ν\nµ(i)z , f(x(i);φ) for some function f (31) S(i)z , g(x(i);φ) for some function g,\nthe equivalent VAE objective becomes L(θ,φ) = ∑\ni\n{ Ep( ) [( x(i) −Wµ(i)z −WS(i)z )> ( Σ(i)x )−1 ( x(i) −Wµ(i)z −WS(i)z )]\n+ Ep( ) [ log ∣∣∣Σ(i)x ∣∣∣ ] + tr [ Σ(i)z ] − log ∣∣∣Σ(i)z ∣∣∣+ ‖µ(i)z ‖22 } (32)\nwhen b = 0 as stipulated.8 For now assume that κ, the dimension of the latent z, satisfies κ = rank[U ] (later we will relax this assumption).\nD.1 A Candidate Solution\nHere we consider a candidate solution that, by design, satisfies (12). For the encoder parameters we choose\nµ̂(i)z = π (i), Σ̂\n(i)\nz = αI. (33) where α is a non-negative scalar and π(i) is defined in conjunction with a matrix Ψ such that\nsuppα [ x(i) −Ψπ(i) ] = supp [ s(i) ]\nspan [U ] = span [Ψ] . (34) 8The extension to arbitrary b is trivial but clutters the presentation.\nAll quantities in (33) can be readily computed via X applied to an encoder module provided that κ = dim[z] = rank [U ] as stipulated, and sufficient representational complexity for µz and Σz . Additionally, for the encoder we only need to define the posterior moments at specific points x(i), hence the indexing via i in (33).\nIn contrast, for the decoder we consider the solution defined over any z given by\nŴ = Ψ\nµ̂x = Ŵz\ndiag [ Σ̂x ] = Λ(hπ(z)), (35)\nwhere Λ(i) ∈ Rd×d is a diagonal matrix with [ Λ(i)\n] jj = { α, if s(i)j = 0, 1, otherwise, ∀j. (36)\nand hπ : Rκ → {1, . . . , n} is a function satisfying hπ(z) , arg min\ni∈{1,...,n} ‖z − π(i)‖2. (37)\nAgain, given sufficient capacity, this function can always be learned by the decoder such that (35) is computable for any z. Given these definitions, then the index-specific moments µ̂(i)x and Σ̂ (i)\nx are of course reduced to functions of given by\nµ̂(i)x = µ̂x ( µ̂(i)z + Ŝ (i) z ;θ )\nΣ̂ (i)\nx = Σ̂x ( µ̂(i)z + Ŝ (i) z ;θ ) . (38)\n.\nWe next analyze the behavior of (32) at this specially parameterized solution as ¯alpha becomes small, in which case by design all covariances will be feasible by design. For this purpose, we first consider the integration across all cases where Σ̂ (i)\nx does not reflect the correct support, meaning /∈ S(i), where\nS(i) , { : [ Σ(i)x ] jj = α iff s(i)j = 0, ∀j } . (39)\nWith this segmentation in mind, the VAE objection naturally partitions as\nL(θ,φ) = ∑\ni\n{ L(i)(θ,φ; /∈ S(i)) + L(i)(θ,φ; ∈ S(i)) } (40)\nwhere L(i)(θ,φ; /∈ S(i)) denotes the cost for the i-th sample when integrated across those samples not in S(i), and L(i)(θ,φ; ∈ S(i)) is the associated complement.\nD.2 Evaluation of L(i)(θ,φ; /∈ S(i))\nFirst we define ρ = min\ni,j∈{1,...,n},i6=j 1 2‖π(i) − π(j)‖2, (41)\nwhich is just half the minimum distance between any two distinct coefficient expansions. If any z is within this distance of π(i), it will necessarily be quantized to this value per our previous definitions. Therefore if ‖Ŝ(i)z ‖2 < ρ, we are guaranteed that the correct generating support pattern will be mapped to Σ̂ (i)\nx , and so it follows that\nP ( /∈ S(i) ) ) ≤ P (∥∥∥Ŝ(i)z ∥∥∥ 2 > ρ ) = P (‖√α ‖2 > ρ) (42)\nat our candidate solution. We also make use of the quantity\nη , max i∈{1,...,n} ‖x(i) −Ψπ(i)‖22, (43)\nwhich represents the maximum data-fitting error. Then for the i-th sample we have\nL(i)(θ,φ; /∈ S(i))\n=\n∫\n/∈S(i)\n[( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )> ( Σ̂ (i) x )−1 ( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )\n+ log ∣∣∣Σ̂(i)x ∣∣∣+ tr [ Σ̂ (i) z ] − log ∣∣∣Σ̂(i)z ∣∣∣+ ‖µ̂(i)z ‖22 ] p( )d\n≤ ∫\n‖√α ‖2>ρ\n[( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )> ( Σ̂ (i) x )−1 ( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )\n+ log ∣∣∣Σ̂(i)x ∣∣∣+ tr [ Σ̂ (i) z ] − log ∣∣∣Σ̂(i)z ∣∣∣+ ‖µ̂(i)z ‖22 ] p( )d\n≤ ∫\n‖√α ‖2>ρ\n[ 1 α ( x(i) −Ψπ(i) −√αΨ )> ( x(i) −Ψπ(i) −√αΨ )\n+ κα− κ logα+ ‖π(i)‖22 ] p( )d , (44)\nwhere the second inequality comes from setting Σ̂ (i)\nx = αI (its smallest possible value) in the inverse\nterm and Σ̂ (i)\nx = I (its largest value) in the log-det term. Next, given that\n‖x(i) −Ψπ(i)‖22 ≤ η∫\n‖√α ‖2>ρ\n( π(i) )> Ψ>Ψ · p( )d = 0 (45)\n∫\n‖√α ‖2>ρ ‖Ψ ‖22p( )d ≤ tr\n[ Ψ>Ψ ] ,\nit follows that the bound from (44) can be further reduced via\nL(i)(θ,φ; /∈ S(i)) ≤ tr [ Ψ>Ψ ] +\n∫\n‖√α ‖2>ρ\n[ 1 αη + κα− κ logα+ ‖π(i)‖22 ] p( )d\n= Θ(1) + [\n1 αη + κα− κ logα+ ‖π(i)‖22\n] ∫\n‖√α ‖2>ρ p( )d\n≤ Θ(1) + [\n1 αη + κα− κ logα+ ‖π(i)‖22 ] α ρ2\n= Θ(1) + Θ(α2)−Θ(α logα) = Θ(1) as α→ 0, (46)\nwhere the second inequality holds based on the vector version of Chebyshev’s inequality, which ensures that ∫\n‖√α ‖2>ρ p( )d = P (‖√α ‖2 > ρ) ≤ αρ2 . (47)\nClearly then, as α becomes small, we have established that\nL(i)(θ,φ; /∈ S(i))→ O (1) . (48)\nD.3 Evaluation of L(i)(θ,φ; ∈ S(i))\nIn analyzing L(i)(θ,φ; ∈ S(i)), we note that ∫\n∈S(i)\n( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )> ( Σ̂ (i) x )−1 ( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z ) p( )d\n≤ ∫ ( x(i) −Ψπ(i) −√αΨ )> ( Λ(i) )−1 ( x(i) −Ψπ(i) −√αΨ ) p( )d ≤ ∫ ( x(i) −Ψπ(i) )> ( Λ(i) )−1 ( x(i) −Ψπ(i) ) p( )d + tr [ Ψ>Ψ ] ≤ η + tr [ Ψ>Ψ ]\n= Θ(1) (49)\ngiven the alignment of Λ(i) with zero-valued elements in x(i) −Ψπ(i). Furthermore, the remaining terms in L(i)(θ,φ; ∈ S(i)) are independent of giving\n∫\n∈S(i)\n[ log ∣∣∣Σ̂(i)x ∣∣∣+ tr [ Σ̂ (i) z ] − log ∣∣∣Σ̂(i)z ∣∣∣+ ‖µ̂(i)z ‖22 ] p( )d\n= [ log ∣∣∣Λ(i) ∣∣∣+ κα− κ logα+ ‖π(i)‖22 ] ∫\n‖√α ‖2<ρ p( )d\n= [ (r(i) − κ) logα+ κα+ ‖π(i)‖22\n] ∫\n‖√α ‖2<ρ p( )d\n= [ (r(i) − κ) logα\n] ∫\n‖√α ‖2<ρ p( )d +O(α) +O(1), (50)\nwhere r(i) , ∣∣∣ { j : Λ (i) jj = α }∣∣∣ = d− ‖s(i)‖0. (51) Therefore, since\n∫ ‖√α ‖2<ρ p( )d → 1 as α becomes small, we may conclude that\nL(i)(θ,φ; ∈ S(i))→ ( d− κ− ‖s(i)‖0 ) logα+O(1), (52)\nD.4 Compilation of Candidate Solution Cost\nAfter combining (48) and (52) across all i we find that\nL(θ,φ)→ ∑\ni\n( d− κ− ‖s(i)‖0 ) logα+O(1) (53)\nfor any α ∈ (0, ᾱ] as ᾱ becomes small. If d > κ+ ‖s(i)‖0, then this expression will tend towards minus infinity, indicative of an objective value that is unbounded from below, certainly a fertile region for candidate minimizers. Note that per the theorem statement, L = UV and S must represent a unique feasible solution to\nmin L,S\nd · rank[L] + ‖S‖0 s.t. X = L+ S. (54)\nGiven that each column x(i) has d degrees of freedom, then with U fixed there will be an infinite number of feasible solutions x(i) = Uv(i) + s(i) such that dim[v(i)] + ‖s(i)‖0 = κ+ ‖s(i)‖0 > d and a combinatorial number such that k + ‖s(i)‖0 = d. Therefore for uniqueness we require that k + ‖s(i)‖0 < d, so it follows that indeed L(θ,φ) will be unbounded from below as ᾱ and therefore α becomes small, with cost given by (53) as a candidate solution satisfying the conditions of the theorem.\nOf course it still remains possible that some other candidate solution could exist that violates one of these conditions and yet still achieves (53) or an even lower cost. We tackle this issue next. For this purpose our basic strategy will be to examine a lower bound on L(θ,φ) and show that essentially any candidate solution violating the theorem conditions will be worse than (53).\nD.5 Evaluation of Other Candidate Solutions\nTo begin, we first observe that if granted the flexibility to optimize Σ(i)x independently over all values of inside the integral for computing L(θ,φ), we immediately obtain a rigorous lower bound.9 For this purpose we must effectively solve decoupled problems of the form\ninf γ>α\nc γ + log γ, (55)\nto which the optimal solution is just\nγ∗ = ξα(x) , [c− α]+ + α, (56) 9Note that this is never possible in practice, even with an infinite capacity network for computing Σ(i)x , since it would require a unique network for each data sample; however, it nonetheless serves as a useful analysis tool.\nwhere the operator [·]+ retains only the positive part of its argument, setting negative values to zero. Plugging this solution back into (55), we find that\ninf γ>α\nc γ + log γ = log ξα(c) +O(1). (57)\nIn the context of our bound, this leads to\nL(θ,φ) ≥ ∑\ni\n  Ep( )  ∑\nj\nlog ξα\n([ x\n(i) j −wj·µ(i)z −wj·S(i)z\n]2)  \n+ tr [ Σ(i)z ] − log ∣∣∣Σ(i)z ∣∣∣+ ‖µ(i)z ‖22 } +O(1). (58)\nFrom this expression, it is clear that the lowest objective value we could ever hope to obtain cannot involve arbitrarily large values of Σ(i)z and µ (i) z since the respective trace and quadratic terms grow faster than log-det terms. Likewise µ(i)z cannot be unbounded for analogous reasons. Therefore, optimal solutions to (58) that will be unbounded from below must involve the first term becoming small, at least over a range of values with significant probability measure. Although the required integral admits no closed-form solution, we can simplify things further using refinements of the above bound.\nFor this purpose consider any possible candidate solution Ŵ = Ψ and µ̂(i)z = π (i) (not necessarily one that coincides with U and the optimal subspace), and define\n∆(i)α (Ψ,π) , suppα [ x(i) −Ψπ ] . (59)\nWithout loss of generality we also specify that\nS(i)z , Ξ(i)D(i), (60)\nwhere Ξ(i) ∈ Rd×κ has orthonormal columns andD(i) is a diagonal matrix with [ D(i)\n] kk = ξ√α ( σ (i) k ) , (61)\nand σ(i) = [σ(i)1 , . . . , σ (i) κ ]> ∈ Rκ+ is an arbitrary non-negative vector. Any general Σ(i)z =\nS(i)z ( S(i)z )> , with singular values bounded by α, is expressible via this format. We then reexpress\n(58) as\nL(θ,φ) ≥ ∑\ni\n   Ep( )  \n∑\nj∈∆(i)α (Ψ,π(i))\nlog ξα\n([ x\n(i) j −ψj·π(i) −ψj·Ξ(i)D(i) ]2)  \n+ Ep( )  \n∑\nj /∈∆(i)α (Ψ,π(i))\nlog ξα ([ ψj·Ξ (i)D(i) ]2)   (62)\n+ tr [ Ξ(i) ( D(i) )2 ( Ξ(i) )>] − log ∣∣∣∣Ξ (i) ( D(i) )2 ( Ξ(i) )>∣∣∣∣+ ‖π(i)‖22 } +O(1),\n= ∑\ni\n   Ep( )  \n∑\nj∈∆(i)α (Ψ,π(i))\nlog ξα\n  [ x\n(i) j −ψj·π(i) −\n∑\nk\nψ̄jk · ξ√α ( σ (i) k ) · k ]2   \n+ Ep( )  \n∑\nj /∈∆(i)α (Ψ,π(i))\nlog ξα\n  [∑\nk\nψ̄ (i) jk · ξ√α\n( σ\n(i) k ) · k ]2    (63)\n+ ∑\nk\nξα\n[( σ\n(i) k\n)2] − ∑\nk\nlog ξα\n[( σ\n(i) k )2] + ‖π(i)‖22 } +O(1),\nwhere ψ̄(i)jk is the k-th element of the vector ψj·Ξ (i). We can now analyze any given point {Ψ,π(i),Ξ(i),σ(i)}ni=1 as α becomes small. The first term can be shown to be Θ(1) with all other variables fixed,10 leading to the revised bound\nL(θ,φ) ≥ ∑\ni\n   Ep( )  \n∑\nj /∈∆(i)α (Ψ,π(i))\nlog ξα\n  [∑\nk\nψ̄ (i) jk · ξ√α\n( σ\n(i) k ) · k ]2    (64)\n− ∑\nk\nlog ξα\n[( σ\n(i) k\n)2] }\n+ Θ(1).\nwhere the terms ∑ k ξα [( σ (i) k )2] and ‖π(i)‖22 have also been absorbed into Θ(1).\nGiven that\nEp( ) [ log ξα ([ a> ]2)] = log ξα [ max k (ak) 2 ] +O (1) ≥ logα+O (1) (65)\nfor any vector a, we have the new bound\nL(θ,φ) (66)\n≥ ∑\ni\n  \n∑\nj /∈∆(i)α (Ψ,π(i))\nlog ξα ( max k [ ψ̄ (i) jk · ξ√α ( σ (i) k )]2) − ∑\nk\nlog ξα\n[( σ\n(i) k )2]    + Θ(1).\nIf then we choose σ(i)k = 0 for all i = 1, . . . n and k = 1, . . . , κ, then\nmax k\n[ ψ̄\n(i) jk · ξ√α\n( σ\n(i) k\n)]2 = logα+ Θ(1) (67)\nand we obtain the lower bound\nL(θ,φ) ≥ ∑\ni\n( d− κ− ∣∣∣∆(i)α ( Ψ,π(i) )∣∣∣ ) logα+ Θ(1). (68)\nAdditionally, if any set ∆(i)α ( Ψ,π(i) ) exists such that\n∑\ni\n( d− κ− ∣∣∣∆(i)α ( Ψ,π(i) )∣∣∣ ) ≤ ∑\ni\n( d− κ− ‖s(i)‖0 ) , (69)\nthen s(i) cannot be part of the unique, feasible solution to (10), i.e., we could use the support pattern from each ∆(i)α ( Ψ,π(i) ) to find a different feasible solution with equal or lower value of n · rank [L] + ‖S‖0, which would violate either the uniqueness or optimality of the original solution. Therefore, we have established that with σ(i)k = O(α) for all i and k, the resulting bound on L(θ,φ) is essentially no better than (53), or the same bound we had before from our feasible trial solution. Moreover, the resulting Ŵ = Ψ that maximizes this bound, as well as the implicit\nΣ̂ (i)\nx ( µ̂z [ x(i) ]) = diag [( x(i) −Ψπ(i) )2] , (70)\nwill necessarily satisfy (12). We then only need consider whether other choices for σ(i)k can do better.\nLet Ψ̃ (i) denote the the rows of Ψ(i) associated with row indeces j /∈ ∆(i)α ( Ψ,π(i) ) , meaning the indices at which we assume no sparse corruption term exists. Additionally, defineB(i) , Ψ̃(i)Ξ(i). 10This is ultimately because\n∫ |c1+x|>α log |c1 + x|N (x; 0, 1)dx < c2 ∫ |c1+x|>α log |c1 + x|dx = Θ(1) for\nany c1 and a c2 sufficiently large.\nThis implies that ∑\nj /∈∆(i)α (Ψ,π(i))\nlog ξα ( max k [ ψ̄ (i) jk · ξ√α ( σ (i) k )]2) − ∑\nk\nlog ξα\n[( σ\n(i) k\n)2] (71)\n= ∑\nj\nlog ξα ( max k [ B (i) jk · ξ√α ( σ (i) k )]2) − ∑\nk\nlog ξα\n[( σ\n(i) k\n)2] .\nContrary to our prior assumption σ(i) = 0, now consider any solution with ‖σ(i)‖0 = β > 0. For the time being, we also assume thatB(i) is full column rank. These conditions imply that\n∑\nj\nlog ξα ( max k [ B (i) jk · ξ√α ( σ (i) k )]2) ≥ ( d− β − ∣∣∣∆(i)α ( Ψ,π(i) )∣∣∣ ) logα+ Θ(1) (72)\nsince at least β elements of the summation over j must now be order Θ(1). By assumption we also have ∑ k log ξα [( σ (i) k )2] = (κ − β) logα + Θ(1). Combining with (72), we see that such\na solution is equivalent or worse than (68). So the former is the best we can do at any value of {Ψ,π(i),Ξ(i),σ(i)}ni=1, provided that B(i) is full rank, and obtaining the optimal value of ∆ (i) α ( Ψ,π(i) ) implies that (12) holds.\nHowever, if B(i) is not full rank it would indeed entail that (71) could be reduced further, since a nonzero element of σ(i) would not increase the first summation, while it would reduce the second. But if such a solution were to exist, it would violate the uniqueness assumption of the theorem statement. To see this, note that rank[B(i)] = rank[Ψ̃ (i) ] since Ξ(i) is orthogonal, so if the former is not full column rank, neither is the latter. And if Ψ̃ (i)\nis not full column rank, there will exist multiple solutions such that ‖x(i) −Ψπ(i)‖0 = ‖s(i)‖0 or equivalently ‖x(i) −Uv(i)‖0 = ‖s(i)‖0 in direct violation of the uniqueness clause.\nTherefore to conclude, a lower bound on the VAE cost is in fact the same order as that obtainable by our original trial solution. If this lower bound is not achieved, we cannot be at a minimizing solution, and any solution achieving this bound must satisfy (12).\nD.6 Generalization to Case where κ > rank[U ]\nFinally, we briefly consider the case where κ > rank[U ] , τ , meaning thatW contains redundant columns that are unnecessary in producing an optimal solution to ( ??). The candidate solution described in Section D.1 can be expanded via Ŵ = [ Ψ, 0[d×(κ−τ)] ] ,µ(i)z = [ (π(i))>, 0[1×(κ−τ)] ]> , and Σ̂ (i) z = diag [ α1>[τ×1], 1 > [(κ−τ)×1] ] such that the same objective function value is obtained.\nNow consider the general case where κ ≥ rank[Ŵ ] > τ . If we review the lower bound described in Section D.5, with this general Ŵ replacing Ψ, it can be shown that Σ̂ (i)\nz will be forced to have additional diagonal elements lowered to α, increasing the achievable objective by at least − logα per sample. The details are not especially enlightening and we omit them here for brevity. Consequently, at any minimizer we must have rank[Ŵ ] = τ ."
    }, {
      "heading" : "Appendix E − Proof of Corollary 1",
      "text" : "Under the stated conditions, the partially-affine VAE cost simplifies to the function L(W ,Σx,µz) = ∑\ni\n{( x(i) −Wµ(i)z )> ( Σ(i)x )−1 ( x(i) −Wµ(i)z ) + log ∣∣∣Σ(i)x ∣∣∣+ ‖µ(i)z ‖22 }\n= ∑\ni\n{( x(i) −Wβ−1βµ(i)z )> ( Σ(i)x )−1 ( x(i) −Wβ−1βµ(i)z )\n+ log ∣∣∣Σ(i)x ∣∣∣+ β2‖µ(i)z ‖22 } , (73)\nwhere β > 0 is an arbitrary scaler, Σ(i)x , Σx ( µ (i) z ;θ ) , and µ(i)z , µz(x(i);φ). Taking the limit\nas β → 0+, we can minimize (73) while ignoring the β2‖µ(i)z ‖22 regularization factor. Consequently, we can without loss of generality consider minimization of\nL(W ,Σx,µz) ≡ ∑\ni\n{( x(i) −Wµ(i)z )> ( Σ(i)x )−1 ( x(i) −Wµ(i)z ) + log ∣∣∣Σ(i)x ∣∣∣ } , (74)\nignoring any explicit reparameterization by β for convenience. If we optimize over Σ(i)x in the feasible region S̄dα and plug in the resulting value, then (74) reduces to the new cost\nL(W ,µz) ≡ ∑\ni,j\nlog ξα\n([ x\n(i) j −wj·µ(i)z\n]2) , (75)\nan immaterial constant notwithstanding. Given that limt→0 1t (|x|t − 1) = log |x|, and limt→0 ∑ j |xj |p = ‖x‖0, then up to an irrelevant scaling factor and additive constant, the stated result follows."
    }, {
      "heading" : "Appendix F − Proof of Theorem 3",
      "text" : "Based on the stated conditions, the VAE objective simplifies to\nL(θ,φ) = ∑\ni\n{ Eqφ(z|x(i)) [ 1 λx ∥∥∥x(i) − µx(z;θ) ∥∥∥ 2 ] + d log λx + λz − log λz + ( a>x(i) )2} . (76)\nNow choose some â such that µ̂(i)z = â>x(i) has a unique value for every sample x(i) (here we assume that each sample is unique, although this assumption can be relaxed). We then define the function h : R→ {1, . . . , n} as\nh(z) , arg min i∈{1,...,n} ‖z − µ(i)z ‖2. (77)\nand the piecewise linear decoder mean function\nµx(z;θ) = x (h(z)). (78)\nGiven these definitions, (76) becomes\nL(θ,φ) = ∑\ni\n{ Eqφ(z|x(i)) [ 1 λx ∥∥∥x(i) − x(h(z)) ∥∥∥ 2 ] + d log λx + λz − log λz + ( µ̂(i)z )2}\n= ∑\ni\n{ Ep( ) [ 1 λx ∥∥∥x(i) − x(h[µ̂(i)z + √ λz ]) ∥∥∥ 2 ] + d log λx + λz − log λz + ( µ̂(i)z )2} . (79)\nNow define the set S(i) , { : h ( µ̂(i)z + √ λz ) = i } , (80)\nwhich represents the set of that quantize to the correct index. We then have Ep( ) [\n1 λx\n∥∥∥x(i) − x(h[µ̂(i)z + √ λz ]) ∥∥∥ 2 ]\n=\n∫\n∈S(i)\n[ 1 λx ∥∥∥x(i) − x(h[µ̂(i)z + √ λz ]) ∥∥∥ 2 ] p( )d + ∫\n/∈S(i)\n[ 1 λx ∥∥∥x(i) − x(h[µ̂(i)z + √ λz ]) ∥∥∥ 2 ] p( )d\n=\n∫\n/∈S(i)\n[ 1 λx ∥∥∥x(i) − x(h[µ̂(i)z + √ λz ]) ∥∥∥ 2 ] p( )d\n≤ ∫\n/∈S(i) η λx p( )d (81)\n= ηλxP ( /∈ S(i) ) ,\nwhere η , max\ni,j∈{1,...,n},i6=j ‖x(i) − x(j)‖22, (82)\nthe maximal possible quantization error. Now we also define\nρ , max i,j∈{1,...,n},i6=j\n1 2 ‖µ̂(i)z − µ̂(j)z ‖22, (83)\nwhich is half the minimum distance between any two µ̂(i)z and µ̂ (j) z , with i 6= j. Then\nP ( /∈ S(i) ) ≤ P (√ λz > ρ )\n≤ λzρ2 (84) by Chebyshev’s inequality as was used in proving Theorem 2. This implies that (79) can be bounded via\nL(θ,φ) ≤ ∑\ni\n{ η λx P ( /∈ S(i) ) + d log λx + λz − log λz + ( µ̂(i)z )2}\n= ∑\ni\n{ η ρ2 + (d− 1) logα+ α+ ( µ̂(i)z )2} (85)\nassuming we are at the trial solution λ̂x = λ̂z = α. As we allow α→ 0, this expression is unbounded from below, and as an upper bound on the VAE objective, the theorem follows. Incidentally, it should also be possible to prove that for α sufficiently small, no other solution can do appreciably better in terms of the dominate (d− 1) logα factor, but we will reserve this for a future extended journal article."
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "International Conference on Computational Statistics, page 177–187,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Auto-association by multilayer perceptrons and singular value decomposition",
      "author" : [ "H. Bourlard", "Y. Kamp" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Robust principal component analysis? J",
      "author" : [ "E. Candès", "X. Li", "Y. Ma", "J. Wright" ],
      "venue" : "ACM, 58(2), May",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Rank-sparsity incoherence for matrix decomposition",
      "author" : [ "V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "SIAM J. Optimization, 21(3), June",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "A. Choromanska", "M. Henaff", "M. Mathieu", "G. Ben Arous", "Y. LeCun" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, page 192–204,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Open problem: The landscape of the loss surfaces of multilayer networks",
      "author" : [ "A. Choromanska", "Y. LeCun", "G. Ben Arous" ],
      "venue" : "Conference on Learning Theory, page 1756–1760,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bayesian robust principal component analysis",
      "author" : [ "X. Ding", "L. He", "L. Carin" ],
      "venue" : "IEEE Trans. Image Processing, 20(12), Dec.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sparse subspace clustering: Algorithm, theory, and applications",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, 35(11),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A Journey into Linear Analysis",
      "author" : [ "D.J.H. Garling" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Deep Learning",
      "author" : [ "I. Goodfellow", "Y. Bengio", "A. Courville" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Generative adversarial networks",
      "author" : [ "I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "arXiv:1406.2661,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "IEEE International Conference on Computer Vision,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep learning without poor local minima",
      "author" : [ "K. Kawaguchi" ],
      "venue" : "Advances in Neural Information Processing Systems 29, pages 586–594,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Partial sum minimization of singular values in RPCA for low-level vision",
      "author" : [ "T.H. Ohand H. Kim", "Y.W. Tai", "J.C. Bazin", "I.S. Kweon" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D. Kingma", "M. Welling" ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, 86(11):2278–2324, Nov",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Correlation adaptive subspace segmentation by trace lasso",
      "author" : [ "C. Lu", "J. Feng", "Z. Lin", "S. Yan" ],
      "venue" : "IEEE International Conference on Computer Vision,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Rectified linear units improve restricted Boltzman machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "International conference on Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories",
      "author" : [ "S. Rao", "R. Tron", "R. Vidal", "Y. Ma" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, 32(10),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D.J. Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "A.M. Saxe", "J.L. McClelland", "S. Ganguli" ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Probabilistic principal component analysis",
      "author" : [ "M. Tipping", "C. Bishop" ],
      "venue" : "J. Royal Statistical Society, Series B, 61(3):611–622,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Non-convex rank minimization via an empirical Bayesian approach",
      "author" : [ "D. Wipf" ],
      "venue" : "Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Given that this integral is intractable in all but the simplest cases, variational autoencoders (VAE) represent a powerful means of optimizing with respect to θ a tractable upper bound on − log pθ(x) [16, 21].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 20,
      "context" : "Given that this integral is intractable in all but the simplest cases, variational autoencoders (VAE) represent a powerful means of optimizing with respect to θ a tractable upper bound on − log pθ(x) [16, 21].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 15,
      "context" : "At least for practical purposes, one way around this is to replace the troublesome expectation with a Monte Carlo stochastic approximation [16, 21].",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "At least for practical purposes, one way around this is to replace the troublesome expectation with a Monte Carlo stochastic approximation [16, 21].",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "Therefore, assuming all the required moments μz , Σz , μx, and Σx are differentiable with respect to φ and θ, the entire model can be updated using SGD [2].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "In fact, when the decoder mean μx is restricted to an affine function of z, we prove that the VAE model collapses to a form of robust PCA [4, 5], a recently celebrated technique for separating data into low-rank (low-dimensional) and sparse outlier components.",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "In fact, when the decoder mean μx is restricted to an affine function of z, we prove that the VAE model collapses to a form of robust PCA [4, 5], a recently celebrated technique for separating data into low-rank (low-dimensional) and sparse outlier components.",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "We expose that a central, albeit underappreciated role of the VAE encoder covariance Σz is to smooth out undesirable minima in the energy landscape of what would otherwise amount to a traditional deterministic autoencoder (AE) [1], even if at times it may not alter the globally optimal solution, or contribute to sample diversity when operating as a generative model.",
      "startOffset" : 227,
      "endOffset" : 230
    }, {
      "referenceID" : 0,
      "context" : "If we assume that Σx is fixed at some λI , and force Σz = 0 (while removing the now undefined log |Σz| term), then it is readily apparent that the resultant VAE model reduces to a traditional AE with squared-error loss function [1], a common practical assumption.",
      "startOffset" : 228,
      "endOffset" : 231
    }, {
      "referenceID" : 2,
      "context" : "With affine encoder and decoder models, the resulting deterministic network will simply learn principle components like vanilla PCA, a well-known special case of the AE [3].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 22,
      "context" : "The objective (5) is the same as that used by certain probabilistic PCA models [23], even though the latter is originally derived in a completely different manner.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "This problem represents the canonical form of robust principal component analysis (RPCA) [4, 5], decomposing a data matrixX into low-rank principal factors L = UV and a sparse outlier component S.",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "This problem represents the canonical form of robust principal component analysis (RPCA) [4, 5], decomposing a data matrixX into low-rank principal factors L = UV and a sparse outlier component S.",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "• A number of celebrated results have stipulated conditions [4, 5] whereby global solutions of the convex relaxation into nuclear and `1 norm components given by min L,S √ n · rank ‖L‖∗ + ‖S‖1, s.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "• A number of celebrated results have stipulated conditions [4, 5] whereby global solutions of the convex relaxation into nuclear and `1 norm components given by min L,S √ n · rank ‖L‖∗ + ‖S‖1, s.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "Moreover, although we must defer to a longer journal version to present a formal treatment, with some mild additional conditions, Theorem 2 can naturally be extended to the case where the deocoder mean function is generalized to subsume non-linear, union-of-subspace models as commonly used in subspace clustering problems [9, 20].",
      "startOffset" : 323,
      "endOffset" : 330
    }, {
      "referenceID" : 19,
      "context" : "Moreover, although we must defer to a longer journal version to present a formal treatment, with some mild additional conditions, Theorem 2 can naturally be extended to the case where the deocoder mean function is generalized to subsume non-linear, union-of-subspace models as commonly used in subspace clustering problems [9, 20].",
      "startOffset" : 323,
      "endOffset" : 330
    }, {
      "referenceID" : 23,
      "context" : "7 Based on details of the proof of Theorem 2, it can be shown that, excluding small-order terms A more rudimentary form of this smoothing has been observed in much simpler empirical Bayesian models derived using Fenchel duality [24].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 11,
      "context" : "Of course an analogous issue exists with generative adversarial networks (GAN) as well, a popular competing deep generative model composed of a generator network analogous to the VAE decoder, and a discriminator network that replaces the VAE encoder in a loose sense [12].",
      "startOffset" : 267,
      "endOffset" : 271
    }, {
      "referenceID" : 3,
      "context" : "We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24].",
      "startOffset" : 231,
      "endOffset" : 245
    }, {
      "referenceID" : 7,
      "context" : "We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24].",
      "startOffset" : 231,
      "endOffset" : 245
    }, {
      "referenceID" : 14,
      "context" : "We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24].",
      "startOffset" : 231,
      "endOffset" : 245
    }, {
      "referenceID" : 23,
      "context" : "We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24].",
      "startOffset" : 231,
      "endOffset" : 245
    }, {
      "referenceID" : 18,
      "context" : "Data Generation: First we draw n low-dimensional samples z ∈ R from N (z; 0, I) and pass them through a 3-layer network with ReLU activations [19].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "The layer sizes are 1000, 2000, and d respectively, each created using the initialization procedure from [13].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "The encoder covariance shares the first two layers, and also has an exponential layer appended at the output to produce only positive values, consistent with the design in [16].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation.",
      "startOffset" : 114,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation.",
      "startOffset" : 114,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation.",
      "startOffset" : 114,
      "endOffset" : 128
    }, {
      "referenceID" : 23,
      "context" : "This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation.",
      "startOffset" : 114,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "We train analogous AE and VAE models with nonlinear decoder mean networks on the MNIST dataset of handwritten digit images [17] as κ is varied.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "Also, μz and Σz share the first three hidden layers, but have separate output layers, similar to [16].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "Outliers are added to MNIST handwritten digit data [17] by randomly replacing from 5% to 50% of the pixels with a value uniformly sampled from [0, 255] to createX .",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "Note that RPCA is commonly used for unsupervised cleaning of this type of data [9], and MNIST is known to have significant low-rank structure [18] as shown in Figure 4(a).",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "Note that RPCA is commonly used for unsupervised cleaning of this type of data [9], and MNIST is known to have significant low-rank structure [18] as shown in Figure 4(a).",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "The first inequality stems from Hadamard’s inequality [10] applied to",
      "startOffset" : 54,
      "endOffset" : 58
    } ],
    "year" : 2017,
    "abstractText" : "Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds obscured by gross corruptions. However, this previously unexplored feature comes with the cost of potential model collapse to a degenerate distribution that may be less suitable as the basis for generating new samples.",
    "creator" : "LaTeX with hyperref package"
  }
}