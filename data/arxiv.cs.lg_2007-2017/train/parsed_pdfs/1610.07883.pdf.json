{
  "name" : "1610.07883.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalization Bounds for Weighted Automata",
    "authors" : [ "B. Balle", "M. Mohri" ],
    "emails" : [ "b.deballepigem@lancaster.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Weighted finite automata (WFAs) provide a general and highly expressive framework for representing functions mapping strings to real numbers. The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25]. WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.\nThe recent developments in spectral learning [31, 6] have triggered a renewed interest in the use of WFAs in machine learning, with several recent successes in\n∗Corresponding author: b.deballepigem@lancaster.ac.uk\nar X\niv :1\n61 0.\n07 88\n3v 1\n[ cs\n.L G\n] 2\nnatural language processing [8, 9] and reinforcement learning [16, 30]. The interest in spectral learning algorithms for WFAs is driven by the many appealing theoretical properties of such algorithms, which include their polynomial-time complexity, the absence of local minima, statistical consistency, and finite sample bounds à la PAC [31]. However, the typical statistical guarantees given for the hypotheses used in spectral learning only hold in the realizable case. That is, these analyses assume that the labeled data received by the algorithm is sampled from some unknown WFA. While this assumption is a reasonable starting point for theoretical analyses, the results obtained in this setting fail to explain the good performance of spectral algorithms in many practical applications where the data is typically not generated by a WFA. See [11] for a recent survey of algorithms for learning WFAs with a discussion of the different assumptions and learning models.\nThere exists of course a vast literature in statistical learning theory providing tools to analyze generalization guarantees for different hypothesis classes in classification, regression, and other learning tasks. These guarantees typically hold in an agnostic setting where the data is drawn i.i.d. from an arbitrary distribution. For spectral learning of WFAs, an algorithm-dependent agnostic generalization bound was proven in [10] using a stability argument. This seems to have been the first analysis to provide statistical guarantees for learning WFAs in an agnostic setting. However, while [10] proposed a broad family of algorithms for learning WFAs parametrized by several choices of loss functions and regularizations, their bounds hold only for one particular algorithm within this family.\nIn this paper, we start the systematic development of algorithm-independent generalization bounds for learning with WFAs, which apply to all the algorithms proposed in [10], as well as to others using WFAs as their hypothesis class. Our approach consists of providing upper bounds on the Rademacher complexity of general classes of WFAs. The use of Rademacher complexity to derive generalization bounds is standard [37] (see also [13] and [49]). It has been successfully used to derive statistical guarantees for classification, regression, kernel learning, ranking, and many other machine learning tasks (e.g. see [49] and references therein). A key benefit of Rademacher complexity analyses is that the resulting generalization bounds are data-dependent.\nOur main results consist of upper bounds on the Rademacher complexity of three broad classes of WFAs. The main difference between these classes is the quantities used for their definition: the norm of the transition weight matrix or initial and final weight vectors of a WFA; the norm of the function computed by a WFA; and, the norm of the Hankel matrix associated to the function computed by a WFA. The formal definitions of these classes is given in Section 3. Let us point out that our analysis of the Rademacher complexity of the class of WFAs described in terms of Hankel matrices directly yields theoretical guarantees for a variety of spectral learning algorithms. We will return to this point when discussing the application of our results. As an application of our Rademacher complexity bounds we provide a variety of generalizations bounds for learning with WFAs using a bounded Lipschitz loss function; our bounds include both\ndata-dependent and data-independent bounds.\nRelated Work. To the best of our knowledge, this paper is the first to provide general tools for deriving learning guarantees for broad classes of WFAs. However, there exists some related work providing complexity bounds for some sub-classes of WFAs in agnostic settings. The VC-dimension of deterministic finite automata (DFAs) with n states over an alphabet of size k was shown by [33] to be in O(kn log n). This can be used to show that the Rademacher complexity of this class of DFA is bounded by O( √ nk log n/m). For probabilistic finite automata (PFAs), it was shown by [1] that, in an agnostic setting, a sample of size Õ(kT 2n2/ε2) is sufficient to learn a PFA with n states and k symbols whose log-loss error is at most ε away from the optimal one in the class when the error is measured on all strings of length T . New learning bounds on the Rademacher complexity of DFAs and PFAs follow as straightforward corollaries of the general results we present in this paper.\nAnother recent line of work, which aims to provide guarantees for spectral learning of WFAs in the non-realizable setting, is the so-called low-rank spectral learning approach [40]. This has led to interesting upper bounds on the approximation error between minimal WFAs of different sizes [39]. See [12] for a polynomial-time algorithm for computing these approximations. This approach, however, is more limited than ours for two reasons. First, because it is algorithm-dependent. And second, because it assumes that the data is actually drawn from some (probabilistic) WFA, albeit one that is larger than any of the WFAs in the hypothesis class considered by the algorithm.\nThe rest of this paper is organized as follows. Section 2 introduces the notation and technical concepts used throughout. Section 3 describes the three classes of WFAs for which we provide Rademacher complexity bounds. The bounds are formally stated and proven in Sections 4, 5, and 6. In Section 7 we provide additional bounds required for converting some sample-dependent bounds from Sections 5 and 6 into sample-independent bounds. Finally, the generalizations bounds obtained using the machinery developed in previous sections are given in Section 8."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Weighted Automata, Rational Functions, and Hankel Matrices",
      "text" : "Let Σ be a finite alphabet of size k. Let denote the empty string and Σ? the set of all finite strings over the alphabet Σ. The length of u ∈ Σ? is denoted by |u|. Given an integer L ≥ 0, we denote by Σ≤L the set of all strings with length at most L: Σ≤L = {x ∈ Σ? : |x| ≤ L}. Given two strings u, v ∈ Σ? we write uv for their concatenation.\nA WFA over the alphabet Σ with n ≥ 1 states is a tuple A = 〈α,β, {Aa}a∈Σ〉 where α,β ∈ Rn are the initial and final weights, and Aa ∈ Rn×n the transition\nmatrix whose entries give the weights of the transitions labeled with a. Every WFA A defines a function fA : Σ ? → R defined for all x = a1 · · · at ∈ Σ? by\nfA(x) = fA(a1 · · · at) = α>Aa1 · · ·Aatβ = α>Axβ , (1)\nwhere Ax = Aa1 · · ·Aat . This algebraic expression in fact corresponds to summing the weights of all possible paths in the automaton indexed by the symbols in x, where the weight of a single path (q0, q1, . . . , qt) ∈ [n]t+1 is obtained by multiplying the initial weight of q0, the weights of all transitions from qs−1 to qs labeled by xs, and the final weight if state qt. That is:\nfA(x) = ∑\n(q0,...,qt)∈[n]t+1 α(q0)\n( t∏\ns=1\nAxs(qs−1, qs) ) β(qt) .\nSee Figure 1 for an example of WFA with 3 states given in terms of its algebraic representation and the equivalent representation as a weighted transition diagram between states.\nAn arbitrary function f : Σ? → R is said to be rational if there exists a WFA A such that f = fA. The rank of f is denoted by rank(f) and is defined as the minimal number of states of a WFA A such that f = fA. Note that minimal WFAs are not unique. In fact, it is not hard to see that, for any minimal WFA A = 〈α,β, {Aa}〉 with f = fA and any invertible matrix Q ∈ Rn×n, AQ = 〈Q>α,Q−1β, {Q−1AaQ}〉 is also another minimal WFA computing f . We sometimes write A(x) instead of fA(x) to emphasize the fact that we are considering a specific parametrization of fA. Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings). Functions mapping strings to real numbers can also be viewed as non-commutative formal power series, which often helps deriving rigorous proofs in formal language theory [53, 15, 38]. We will not favor that point of view here, however, since we will not need to make explicit mention of the algebraic properties offered by that perspective.\nAn alternative method to represent rational functions independently of any WFA parametrization is via their Hankel matrices. The Hankel matrix Hf ∈ RΣ?×Σ? of a function f : Σ? → R is the infinite matrix with rows and columns indexed by all strings with Hf (u, v) = f(uv) for all u, v ∈ Σ?. By the theorem of Fliess [29] (see also [18] and [15]), Hf has finite rank n if and only if f is rational and there exists a WFA A with n states computing f , that is, rank(f) = rank(Hf )."
    }, {
      "heading" : "2.2 Learning Scenario",
      "text" : "Let Z denote a measurable subset of R. We assume a standard supervised learning scenario where training and test points are drawn i.i.d. according to some unknown distribution D over Σ? × R.\nLet F be a subset of the family of functions mapping from X to Y, with Y ⊆ R, and let ` : Y × Z → R+ be a loss function measuring the divergence between the prediction y ∈ Y made by a function in F and the target label z ∈ Z. The learner’s objective consists of using a labeled training sample S = ((x1, z1), . . . , (xm, zm)) of size m to select a function f ∈ F with small expected loss, that is\nLD(f) = E (x,z)∼D [`(f(x), z)] .\nOur objective is to derive learning guarantees for broad families of weighted automata or rational functions used as hypothesis sets in learning algorithms. To do so, we will derive upper bounds on the Rademacher complexity of different classes of rational functions f : Σ? → R. Thus, we start with a brief introduction to the main definitions and results regarding the Rademacher complexity of an arbitrary class of functions F = {f : X → Y} where X is the input space and Y ⊆ R the output space. Let D be a probability distribution over X × Z for some Z ⊆ R and denote by DX the marginal distribution over X . Suppose S = (x1, . . . , xm)\niid∼ DmX is a sample of m i.i.d. examples drawn from D. The empirical Rademacher complexity of F on S is defined as follows:\nR̂S(F) = E [ sup f∈F 1 m m∑ i=1 σif(xi) ] ,\nwhere the expectation is taken over the m independent Rademacher random variables σi ∼ Unif({+1,−1}). The Rademacher complexity of F is defined as the expectation of R̂S(F) over the draw of a sample S of size m:\nRm(F) = E S∼DmX\n[ R̂S(F) ] .\nThe Rademacher complexity of a hypothesis class can be used to derive generalization bounds for a variety of learning tasks [37, 13, 49]. To do so, we need to bound the Rademacher complexity of the associated loss class, for a given loss function ` : Y × Z → R+.\nFor a given hypothesis class F the corresponding loss class ` ◦ F is given by the set of all functions ` ◦ f : X × Z → R+ of the form (x, z) 7→ `(f(x), z). By Talagrand’s contraction lemma [41], the empirical Rademacher complexity of ` ◦ F can be bounded in terms of R̂S(F), when ` is µ-Lipschitz with respect to its first argument for some µ > 0, that is when\n|`(y, z)− `(y′, z)| ≤ µ|y − y′|\nfor all y, y′ ∈ Y and z ∈ Z. In that case, the following inequality holds: R̂S′(` ◦ F) ≤ µR̂S(F), where S′ = ((x1, z1), . . . , (xm, zm)) is a sample of size m with (xi, zi) ∈ X × Z and S = (x1, . . . , xm) denotes the sample of elements in X obtained from S′. When taking expectations over S′\niid∼ Dm and S iid∼ DmX we obtain the same bound for the Rademacher complexities Rm(`◦F) ≤ µRm(F). A typical example of a loss function that is µ-Lipschitz with respect to its first argument is the absolute loss `(y, z) = |y−z|, which satisfies the condition with µ = 1 for Y = Z = R."
    }, {
      "heading" : "3 Classes of Rational Functions",
      "text" : "In this section we introduce several classes of rational functions. Each of these classes is defined in terms of a different way to measure the complexity of rational functions. The first one is based on the weights of an explicit WFA representation, while the other two are based on intrinsic quantities associated to the function: the norm of the function, and the norm of the corresponding Hankel matrix when viewed as a linear operator on a certain Hilbert space. These three points of view measure different aspects of the complexity of a rational function, and each of them provides distinct benefits in the analysis of learning with WFAs. The Rademacher complexity of each of these classes will be analyzed in Sections 4, 5, and 6.\n3.1 The Class An,p,r We start by considering the case where each rational function is given by a fixed WFA representation. Our learning bounds would then naturally depend on the number of states and the weights of the WFA representations.\nFix an integer n > 0 and let An denote the set of all WFAs with n states. Note that any A ∈ An is identified by the d = n(kn+ 2) parameters required to specify its initial, final, and transition weights. Thus, we can identify An with the vector space Rd by suitably defining addition and scalar multiplication. In particular, given A,A′ ∈ An and c ∈ R, we define:\nA+A′ = 〈α,β, {Aa}〉+ 〈α′,β′, {A′a}〉 = 〈α + α′,β + β ′, {Aa + A′a}〉\ncA = c〈α,β, {Aa}〉 = 〈cα, cβ, {cAa}〉 .\nWe can view An as a normed vector space by endowing it with any norm from the following family. Let p, q ∈ [1,+∞] be Hölder conjugates, i.e. p−1 +q−1 = 1.\nIt is easy to check that the following defines a norm on An: ‖A‖p,q = max { ‖α‖p, ‖β‖q,max\na ‖Aa‖q\n} ,\nwhere ‖A‖q denotes the matrix norm induced by the corresponding vector norm, that is ‖A‖q = sup‖v‖q=1 ‖Av‖q. Given p ∈ [1,+∞] and q = 1/(1 − 1/p), we denote by An,p,r the set of all WFAs A with n states and ‖A‖p,q ≤ r. Thus, An,p,r is the ball of radius r at the origin in the normed vector space (An, ‖·‖p,q)."
    }, {
      "heading" : "3.1.1 Examples",
      "text" : "We consider first the class of deterministic finite automata (DFA). A DFA can be represented by a WFA where: α is the indicator vector of the initial state; the entries of β are values in {0, 1} indicating whether a state is accepting or rejecting; and, for any a ∈ Σ and any i ∈ [n] we have that the ith row of Aa is either the all zero vector if there is no transition from the ith state labeled by a, or an indicator vector with a one on the jth position if taking an a-transition from state i leads to state j. Therefore, a DFA A = 〈α,β, {Aa}〉 satisfies ‖A‖1,∞ ≤ 1 and An,1,1 contains all DFA with n states.\nAnother important class of WFA contained in An,1,1 is that of probabilistic finite automata (PFA). To represent a PFA as a WFA we consider automata where: α is a probability distribution over possible initial states; the vector β contains stopping probabilities for every state; and for every a ∈ Σ and i, j ∈ [n] the entry Aa(i, j) represents the probability of transitioning from state i to state j while outputting the symbol a. Any WFA satisfying these constraints clearly has ‖α‖1 = 1, ‖β‖∞ ≤ 1, and ‖Aa‖∞ = maxi ∑ j |Aa(i, j)| ≤ 1. The function fA computed by a PFA A defines a probability distribution over Σ ?; i.e. we have\nfA(x) ≥ 0 for all x ∈ Σ? and ∑ x∈Σ? fA(x) = 1.\n3.2 The Class Rp,r Next, we consider an alternative quantity measuring the complexity of rational functions that is independent of any WFA representation: their norm. Given p ∈ [1,∞] and f : Σ? → R we use ‖f‖p to denote the p-norm of f given by\n‖f‖p = [ ∑ x∈Σ? |f(x)|p ] 1 p ,\nwhich in the case p =∞ amounts to ‖f‖∞ = supx∈Σ? |f(x)|. Let Rp denote the class of rational functions with finite p-norm: f ∈ Rp if and only if f is rational and ‖f‖p < +∞. Given some r > 0 we also define Rp,r, the class of functions with p-norm bounded by r:\nRp,r = {f : Σ? → R | f rational and ‖f‖p ≤ r} .\nNote that this definition is independent of the WFA used to represent f ."
    }, {
      "heading" : "3.2.1 Examples and Membership Testing",
      "text" : "If A is a PFA, then the function fA is a probability distribution and we have fA ∈ R1,1 and by extension Rp,1 for all p ∈ [1,+∞]. On the other hand, if A is a DFA such that fA(x) = 1 for infinitely many x ∈ Σ?, then fA ∈ R∞,1, but fA /∈ Rp for any p < +∞. In fact, it is easy to see that for any n ≥ 0 we have An,1,1 ⊆ R∞. These examples show that An,1,1 ∩ R1 6= ∅ and An,1,1 ∩ (R∞ \\ R1) 6= ∅. Thus, the classes Rp yield a more fine grained characterization of the complexity of rational functions than what the classes An,p,r can provide in general.\nHowever, while testing membership of a WFA in An,p,r is a straightforward task, testing membership in any of the Rp can be challenging. Membership in R1,r was shown to be semi-decidable in [7]. On the other hand, membership in R2,r can be decided in polynomial time [22]. The inclusion An,1,1 ⊆ R∞ gives an easy to test sufficient condition for membership in R∞.\n3.3 The Class Hp,r Here, we introduce a third class of rational functions described via their Hankel matrices, a quantity that is also independent of their WFA representations. To do so, we represent a function f using its Hankel matrix Hf , interpret this matrix as a linear operator on a Hilbert space contained in the free vector space RΣ? , and consider the Schatten p-norm of Hf as a measure of complexity of f . To make this more precise we start by noting that the set\nL2 = {f : Σ? → R | ‖f‖2 <∞} together with the inner product 〈f, g〉 = ∑ x∈Σ? f(x)g(x) forms a separable Hilbert space. Note we have the obvious inclusionR2 ⊂ L2, but not all functions in L2 are rational. Given an arbitrary function f : Σ? → R we identify the Hankel matrix Hf with a (possibly unbounded) linear operator Hf : L2 → L2 defined by\n(Hfg)(x) = ∑ y∈Σ? f(xy)g(y) .\nRecall that an operator Hf is bounded when its operator norm is finite; i.e. ‖Hf‖ = sup‖g‖2≤1 ‖Hfg‖2 < ∞. Furthermore, a bounded operator is compact if it can be obtained as the limit of a sequence of bounded finite-rank operators under an adequate notion of convergence. In particular, bounded finite-rank operators are compact. Our interest in compact operators on Hilbert spaces stems from the fact that these are precisely the operators for which a notion equivalent to the SVD for finite matrices can be defined. Thus, if f is a rational function of rank n such that Hf is bounded (note this implies compactness by Fliess’ theorem), then we can use the singular values s1 ≥ . . . ≥ sn of Hf as a measure of the complexity of f . The following result follows from [12] and gives a useful condition for the boundedness of Hf .\nLemma 1. Suppose the function f : Σ? → R is rational. Then Hf is bounded if and only if ‖f‖2 <∞.\nWe see that every Hankel matrix Hf with f ∈ R2 has a well-defined SVD. Therefore, for any f ∈ R2 it makes sense to define its Schatten–Hankel p-norm as the Schatten p-norm of its Hankel matrix: ‖f‖H,p = ‖Hf‖S,p = ‖(s1, . . . , sn)‖p, where si = si(Hf ) is the ith singular value of Hf and rank(Hf ) = n. Using this notation, we can define several classes of rational functions. For a given p ∈ [1,+∞], we denote by Hp the class of rational functions with ‖f‖H,p <∞ and, for any r > 0, we write Hp,r the for class of rational functions with ‖f‖H,p ≤ r.\nNote that the discussion above implies Hp = R2 for every p ∈ [1,+∞], and therefore we can see the classes Hp,r as providing an alternative stratification of R2 than the classes R2,r. As a consequence of this containment we also have R1 ⊂ Hp for every p, and therefore the classesHp include all functions computed by probabilistic automata. Since membership in R2 is efficiently testable [22], a polynomial time algorithm from [12] can be used to compute ‖f‖H,p and thus test membership in Hp,r.\n4 Rademacher Complexity of An,p,r In this section, we present an upper bound on the Rademacher complexity of the class of WFAs An,p,r. To bound Rm(An,p,r), we will use an argument based on covering numbers. We first introduce some notation, then state our general bound and related corollaries, and finally prove the main result of this section.\nLet S = (x1, . . . , xm) ∈ (Σ?)m be a sample of m strings with maximum length LS = maxi |xi|. The expectation of this quantity over a sample of m strings drawn i.i.d. from some fixed distribution D will be denoted by Lm = ES∼Dm [LS ]. It is interesting at this point to note that Lm appears in our bound and introduces a dependency on the distribution D which will exhibit different growth rates depending on the behavior of the tails of D. For example, it is well known that if the random variable |x| for x ∼ D is sub-Gaussian,1 then Lm = O( √ logm). Similarly, if the tail of D is sub-exponential, then Lm = O(logm) and if the tail is a power-law with exponent s + 1, s > 0, then Lm = O(m\n1/s). Note that in the latter case the distribution of |x| has finite variance if and only if s > 1.\nTheorem 2. The following inequality holds for every sample S ∈ (Σ?)m:\nR̂S(An,p,r) ≤ inf η>0 η + rLS+2 √√√√2n(kn+ 2) log (2r + rLS+2(LS+2)η ) m  . By considering the case r = 1 and choosing η = (LS + 2)/m we obtain the\nfollowing corollary.\n1Recall that a non-negative random variable X is sub-Gaussian if P[X > k] ≤ exp(−Ω(k2)), sub-exponential if P[X > k] ≤ exp(−Ω(k)), and follows a power-law with exponent (s + 1) if P[X > k] ≤ O(1/ks+1).\nCorollary 3. For any m ≥ 1 and n ≥ 1 the following inequalities holds: Rm(An,p,1) ≤ √ 2n(kn+ 2) log(m+ 2)\nm + Lm + 2 m ,\nR̂S(An,p,1) ≤ √ 2n(kn+ 2) log(m+ 2)\nm + LS + 2 m ."
    }, {
      "heading" : "4.1 Proof of Theorem 2",
      "text" : "We begin the proof by recalling several well-known facts and definitions related to covering numbers (see e.g. [24]). Let V ⊂ Rm be a set of vectors and S = (x1, . . . , xm) ∈ (Σ?)m a sample of size m. Given a WFA A, we define A(S) ∈ Rm by A(S) = (A(x1), . . . , A(xm)) ∈ Rm. We say that V is an (`1, η)-cover for S with respect to An,p,r if for every A ∈ An,p,r there exists some v ∈ V such that\n1 m ‖v −A(S)‖1 = 1 m m∑ i=1 |vi −A(xi)| ≤ η .\nThe `1-covering number of S at level η with respect to An,p,r is defined as follows:\nN1(η,An,p,r, S) = min {|V | : V ⊂ Rm is an (`1, η)-cover for S w.r.t. An,p,r} .\nA typical analysis based on covering numbers would now proceed to obtain a bound on the growth of N1(η,An,p,r, S) in terms of the number of strings m in S. Our analysis requires a slightly finer approach where the size of S is characterized by m and LS . Thus, we also define for every integer L ≥ 0 the following covering number\nN1(η,An,p,r,m,L) = max S∈(Σ≤L)m N1(η,An,p,r, S) .\nThe first step in the proof of Theorem 2 is to bound N1(η,An,p,r,m,L). In order to derive such a bound, we will make use of the following technical results.\nLemma 4 (Corollary 4.3 in [57]). A ball of radius R > 0 in a real d-dimensional Banach space can be covered by Rd(2 + 1/ρ)d balls of radius ρ > 0.\nLemma 5. Let A,B ∈ An,p,r. Then the following hold for any x ∈ Σ?:\n1. |A(x)| ≤ r|x|+2 ,\n2. |A(x)−B(x)| ≤ r|x|+1(|x|+ 2)‖A−B‖p,q .\nProof. The first bound follows from applying Hölder’s inequality and the submultiplicativity of the norms in the definition of ‖A‖p,q to (1). The second bound was proven in [10].\nCombining these lemmas yields the following bound on the covering number N1(η,An,p,r,m,L).\nLemma 6. N1(η,An,p,r,m,L) ≤ rn(kn+2) ( 2 + rL+1(L+ 2)\nη\n)n(kn+2) .\nProof. Let d = n(kn+2). By Lemma 4 and Lemma 5, for any ρ > 0, there exists a finite set Cρ ⊂ An,p,r with |Cρ| ≤ rd(2 + 1/ρ)d such that: for every A ∈ An,p,r there exists B ∈ Cρ satisfying |A(x)−B(x)| ≤ r|x|+1(|x|+ 2)ρ for every x ∈ Σ?. Thus, taking ρ = η/(rL+1(L + 2)) we see that for every S ∈ (Σ≤L)m the set V = {B(S) : B ∈ Cρ} ⊂ Rm is an η-cover for S with respect to An,p,r.\nThe last step of the proof relies on the following well-known result due to Massart.\nLemma 7 (Massart [42]). Given a finite set of vectors V = {v1, . . . ,vN} ⊂ Rm, the following holds\n1 m E [ max v∈V 〈σ,v〉 ] ≤ ( max v∈V ‖v‖2 ) √ 2 log(N) m ,\nwhere the expectation is over the vector σ = (σ1, . . . , σm) whose entries are independent Rademacher random variables σi ∼ Unif({+1,−1}).\nFix η > 0 and let VS,η be an (`1, η)-cover for S with respect to An,p,r. By Massart’s lemma, we can write\nR̂S(An,p,r) ≤ η + (\nmax v∈VS,η\n‖v‖2 ) √\n2 log |VS,η| m . (2)\nSince |A(xi)| ≤ rLS+2 by Lemma 5, we can restrict the search for (`1, η)-covers for S to sets VS,η ⊂ Rm where all v ∈ VS,η must satisfy ‖v‖∞ ≤ rLS+2. By construction, such a covering satisfies maxv∈VS,η ‖v‖2 ≤ rLS+2 √ m. Finally, plugging in the bound for |VS,η| given by Lemma 6 into (2) and taking the infimum over all η > 0 yields the desired result.\n5 Rademacher Complexity of Rp,r In this section, we study the complexity of rational functions from a different perspective. Instead of analyzing their complexity in terms of the parameters of WFAs computing them, we consider an intrinsic associated quantity: their norm. We present upper bounds on the Rademacher complexity of the classes of rational functions Rp,r for any p ∈ [1,+∞] and r > 0.\nIt will be convenient for our analysis to identify a rational function f ∈ Rp,r with an infinite-dimensional vector f ∈ RΣ? with ‖f‖p ≤ r. That is, f is an infinite vector indexed by strings in Σ? whose xth entry is fx = f(x). An important observation is that using this notation, for any given x ∈ Σ?, we can write f(x) as the inner product 〈f , ex〉, where ex ∈ RΣ ?\nis the indicator vector corresponding to string x.\nTheorem 8. Let p−1 +q−1 = 1. Let S = (x1, . . . , xm) be a sample of m strings. Then, the following holds for any r > 0:\nR̂S(Rp,r) = r\nm E [∥∥∥∥ m∑ i=1 σiexi ∥∥∥∥ q ] ,\nwhere the expectation is over the m independent Rademacher random variables σi ∼ Unif({+1,−1}).\nProof. In view of the notation just introduced described, we can write\nR̂S(Rp,r) = E [ sup\nf∈Rp,r\n1\nm m∑ i=1 〈f , σiexi〉\n] = 1\nm E\n[ sup\nf∈Rp,r\n〈 f , m∑ i=1 σiexi 〉]\n= r\nm E [∥∥∥∥ m∑ i=1 σiexi ∥∥∥∥ q ] ,\nwhere the last inequality holds by definition of the dual norm.\nThe next corollaries give non-trivial bounds on the Rademacher complexity in the case p = 1 and the case p = 2.\nCorollary 9. For any m ≥ 1 and any r > 0, the following inequalities hold: r√ 2m ≤ Rm(R2,r) ≤ r√ m .\nProof. The upper bound follows directly from Theorem 8 and Jensen’s inequality:\nE [∥∥∥∥ m∑ i=1 σiexi ∥∥∥∥ 2 ] ≤ √√√√E[∥∥∥∥ m∑ i=1 σiexi ∥∥∥∥2 2 ] = √ m .\nThe lower bound is obtained using Khintchine–Kahane’s inequality (see appendix of [49]):\nE [∥∥∥∥ m∑ i=1 σiexi ∥∥∥∥ 2 ]2 ≥ 1 2 E [∥∥∥∥ m∑ i=1 σiexi ∥∥∥∥2 2 ] = m 2 ,\nwhich completes the proof.\nThe following definition will be needed to present our next corollary. Given a sample S = (x1, . . . , xm) and a string x ∈ Σ? we denote by sx = |{i : xi = x}| the number of times x appears in S. Let CS = maxs∈Σ? sx and note we have the straightforward bounds 1 ≤ CS ≤ m.\nCorollary 10. For any m ≥ 1, any S ∈ (Σ?)m, and any r > 0, the following upper bound holds:\nR̂S(R1,r) ≤ r √ 2CS log(2m)\nm .\nProof. Let S = (x1, . . . , xm) be a sample with m strings. For any x ∈ Σ? define the vector vx ∈ Rm given by vx(i) = Ixi=x. Let V be the set of vectors vx which are not identically zero, and note we have |V | ≤ m. Also note that by construction we have maxvx∈V ‖vx‖2 = √ CS . Now, by Theorem 8 we have\nR̂S(R1,r) = r\nm E [∥∥∥∥ m∑ i=1 σiexi ∥∥∥∥ ∞ ] = r m E [ max vx∈V ∪(−V ) 〈σ,vx〉 ] .\nTherefore, using Massart’s Lemma we get R̂S(R1,r) ≤ r √ 2CS log(2m)\nm .\nNote in this case we cannot rely on the Khintchine–Kahane inequality to obtain lower bounds because there is no version of this inequality for the case q =∞.\nWe can easily convert the above empirical bound into a standard Rademacher complexity bound by defining the expectation Cm = ES∼Dm [CS ] over a distribution D on Σ?. Note that Cm is the expected maximum number of collisions (repeated strings) in a sample of size m drawn from D. We shall provide a bound for Cm in terms of m in Section 7.\n6 Rademacher Complexity of Hp,r In this section, we present our last set of upper bounds on the Rademacher complexity of WFAs. Here, we characterize the complexity of WFAs in terms of the spectral properties of their Hankel matrix.\nThe Hankel matrix of a function f : Σ? → R is the bi-infinite matrix Hf ∈ RΣ?×Σ? whose entries are defined by Hf (u, v) = f(uv). Note that any string x ∈ Σ? admits |x|+ 1 decompositions x = uv into a prefix u ∈ Σ? and a suffix v ∈ Σ?. Thus, Hf contains a high degree of redundancy: for any x ∈ Σ?, f(x) is the value of at least |x| + 1 entries of Hf and we can write f(x) = e>uHfev for any decomposition x = uv.\nLet si(M) denote the ith singular value of a matrix M. For 1 ≤ p ≤ ∞, let ‖M‖S,p denote the p-Schatten norm of M defined by ‖M‖S,p = [∑ i≥1 si(M) p ] 1 p .\nTheorem 11. Let p, q ≥ 1 with p−1 + q−1 = 1 and let S = (x1, . . . , xm) be a sample of m strings in Σ?. For any decomposition xi = uivi of the strings in S and any r > 0, the following inequality holds:\nR̂S(Hp,r) ≤ r\nm E [∥∥∥∥ m∑ i=1 σieuie > vi ∥∥∥∥ S,q ] .\nProof. For any 1 ≤ i ≤ m, let xi = uivi be an arbitrary decomposition and let R = ∑m i=1 σieuie > vi . Then, in view of the identity f(xi) = e > uiHfevi =\nTr(evie > uiHf ), we can use the linearity of the trace to write\nR̂S(Hp,r) = E [ sup\nf∈Hp,r\n1\nm m∑ i=1 σie > uiHfevi\n]\n= 1\nm E\n[ sup\nf∈Hp,r m∑ i=1 Tr ( σievie > uiHf\n)] = 1\nm E\n[ sup\nf∈Hp,r 〈R,Hf 〉\n] .\nThen, by von Neumann’s trace inequality [43] and Hölder’s inequality, the following holds:\nE [ sup\nf∈Hp,r 〈R,Hf 〉\n] ≤ E  sup f∈Hp,r ∑ j≥1 sj(R) · sj(Hf )  ≤ E [ sup\nf∈Hp,r ‖R‖S,q‖Hf‖S,p\n] = rE [ ‖R‖S,q ] ,\nwhich completes the proof.\nNote that, in this last result, the equality condition for von Neumann’s inequality cannot be used to obtain a lower bound on R̂S(Hp,r) since it requires the simultaneous diagonalizability of the two matrices involved, which is difficult to control in the case of Hankel matrices.\nAs in the previous sections, we now proceed to derive specialized versions of the bound of Theorem 11 for the cases p = 1 and p = 2. First, note that the corresponding q-Schatten norms have given names: ‖R‖S,2 = ‖R‖F is the Frobenius norm, and ‖R‖S,∞ = ‖R‖op is the operator norm.\nCorollary 12. For any m ≥ 1 and any r > 0, the Rademacher complexity of H2,r can be bounded as follows:\nRm(H2,r) ≤ r√ m .\nProof. In view of Theorem 11 and using Jensen’s inequality, we can write\nRm(H2,r) ≤ r m E [ ‖R‖F ] ≤ r m √ E [ ‖R‖2F ] = r\nm √√√√E [ m∑ i,j=1 σiσj〈euie>vi , euje>vj 〉 ]\n= r\nm √√√√E [ m∑ i=1 〈euie>vi , euie>vi〉 ] = r√ m ,\nwhich concludes the proof.\nTo bound the Rademacher complexity of Hp,r in the case p = 1 we will need the following moment bound for the operator norm of a random matrix from [56].\nTheorem 13 (Corollary 7.3.2 [56]). Suppose M = ∑ iMi is a sum of i.i.d.\nrandom matrices with E[Mi] = 0 and ‖Mi‖op ≤ M . Let ∑ i E[MiM>i ] 4 V1,∑\ni E[M>i Mi] 4 V2, and V = diag(V1,V2). If d = Tr(V)/‖V‖op and ν = ‖V‖op, then we have\nE[‖M‖op] ≤ 2\n3\n( 1 + 4\nlog 2\n) M log(d+ 1) + ( 1 +\n4√ 2 log 2\n)√ 2ν log(d+ 1) .\nWe now introduce a combinatorial number depending on S and the decomposition selected for each string xi. Let US = maxu∈Σ? |{i : ui = u}| and VS = maxv∈Σ? |{i : vi = v}|. Then, we define WS = min max{US , VS}, where then minimum is taken over all possible decompositions of the strings in S. It is easy to show that we have the bounds 1 ≤ WS ≤ m. Indeed, for the case WS = m consider a sample with m copies of the empty string, and for the case WS = 1 consider a sample with m different strings of length m. The following result can be stated using this definition.\nCorollary 14. For any m ≥ 1, any S ∈ (Σ?)m, and any r > 0, the following upper bound holds:\nR̂S(H1,r) ≤ r\nm\n[ 2\n3\n( 1 + 4\nlog 2\n) log(2m+ 1) + ( 1 +\n4√ 2 log 2\n)√ 2WS log(2m+ 1) ] .\nProof. First note that we can apply Theorem 13 to the random matrix R by letting V1 = ∑ i euie > ui and V2 = ∑ i evie > vi . In this case we have d = 2m,\nν = max{‖ ∑ i euie > ui‖op, ‖ ∑ i evie > vi‖op}, and we get:\nE[‖R‖op] ≤ ( 2\n3 +\n8\n3 log 2\n) log(2m+ 1) + (√ 2 +\n4√ log 2\n)√ ν log(2m+ 1) .\nNext, observe that V1 = ∑ i euie > ui ∈ R\nΣ?×Σ? is a diagonal matrix with V1(u, u) =∑ i Iu=ui . Thus, ‖V1‖op = maxuV1(u, u) = maxu∈Σ? |{i : ui = u}| = US . Similarly, we have ‖V2‖op = VS . Thus, since the decomposition of the strings in S is arbitrary, we can choose it such that µ = WS . Applying Theorem 11 now yields the desired bound.\nWe can again convert the above empirical bound into a standard Rademacher complexity bound by defining the expectation Wm = ES∼Dm [WS ] over a distribution D on Σ?. We provide a bound for Wm in terms of m in next section."
    }, {
      "heading" : "7 Distribution-Dependent Rademacher Complex-",
      "text" : "ity Bounds\nThe bounds for the Rademacher complexity of R1,r and H1,r we give above identify two important distribution-dependent parameters Cm = ES [CS ] and\nWm = ES [WS ] that reflect the impact of the distribution D on the complexity of learning these classes of rational functions. We now use upper bounds on Cm and Wm in terms of m to give bounds for the Rademacher complexities Rm(R1,r) and Rm(H1,r).\nWe start by rewriting CS in a convenient way. Let E = {ex : Σ? → R|x ∈ Σ?} be the class of all indicator on Σ? given by ex(y) = 1 if x = y and ex(y) = 0 otherwise. Recall that given S = (x1, . . . , xm) we defined sx = |{i : xi = x}| and CS = supx∈Σ? sx. Using E we can rewrite these as sx = ∑m i=1 ex(xi) and\nCS = sup ex∈E m∑ i=1 ex(xi) .\nLet Dmax = maxx∈Σ? PD[x] be the maximum probability of any strings with respect to the distribution D.\nLemma 15. mDmax ≤ Cm ≤ mDmax +O( √ m) .\nProof. We can bound Cm = ES [CS ] as follows:\nCm = E S∼Dm [ sup ex∈E m∑ i=1 ex(xi) ]\n= E S∼Dm [ sup ex∈E m∑ i=1 ( ex(xi) + E x′i∼D [ex(x ′ i)]− E x′i∼D [ex(x ′ i)] )]\n≤ E S∼Dm [ sup ex∈E m∑ i=1 E x′i∼D [ex(x ′ i)] ] + E S∼Dm [ sup ex∈E m∑ i=1 ( ex(xi)− E x′i∼D [ex(x ′ i)] )]\n= m sup ex∈E E x′∼D\n[ex(x ′)] + E\nS∼Dm [ sup ex∈E m∑ i=1 ( ex(xi)− E x′i∼D [ex(x ′ i)] )]\n≤ m sup ex∈E E x′∼D [ex(x ′)] + E S∼Dm [ sup ex∈E ∣∣∣∣∣ m∑ i=1 ( ex(xi)− E x′i∼D [ex(x ′ i)] )∣∣∣∣∣ ] .\nNow note on the one hand we can write supex∈E Ex′∼D[ex(x ′)] = supx∈Σ? Px′∼D[x′ = x] = Dmax. On the other hand, a standard symmetrization argument yields:\nE S∼Dm [ sup ex∈E ∣∣∣∣∣ m∑ i=1 ( ex(xi)− E x′i∼D [ex(x ′ i)] )∣∣∣∣∣ ] ≤ 2mRm(E) = O( √ m) ,\nwhere in the last inequality we used that the VC-dimension of E is 1, in which case Dudley’s chaining method [26] yields Rm(E) ≤ C √ 1/m for some universal constant C > 0. Note that by Jensen’s inequality we also have\nm sup ex∈E E x′∼D\n[ex(x ′)] = sup\nex∈E E S∼Dm [ m∑ i=1 ex(xi) ] ≤ E S∼Dm [ sup ex∈E m∑ i=1 ex(xi) ] ,\nand therefore the bound is tight up to the lower order terms.\nA straightforward application of Jensen’s inequality now yields the following.\nCorollary 16. For any m ≥ 1 and any r > 0 we have:\nRm(R1,r) ≤ r√ m\n√ 2(Dmax +O( √ 1/m)) log(2m).\nNext we provide bounds for Wm. Given a sample S = (x1, . . . , xm) we will say that the tuples of pairs of strings S′ = ((u1, v1), . . . , (um, vm)) ∈ (Σ?×Σ?)m form a split of S if xi = uivi for all 1 ≤ i ≤ m. We denote by S∨ the set of all possible splits of a sample S. We also define coordinate projections πj : Σ\n? × Σ? → Σ? given by π1(u, v) = u and π2(u, v) = v. Now recall that Wm = ES [WS ] and note we can rewrite the definition of WS as\nWS = min S′∈S∨ max j=1,2 sup ex∈E m∑ i=1 ex(πj(ui, vi))\n= min S′∈S∨ sup e∈E∨ m∑ i=1 e(ui, vi) ,\nwhere E∨ = (E ◦ π1) ∪ (E ◦ π2) and E ◦ πj is the set of functions of the form ex(πj(u, v)). Finally, given a distribution D over Σ ? we define the parameter\nD∨max = sup x∈Σ? max {∑ v∈Σ?\n1\n|x|+ |v|+ 1 PD[xv], ∑ u∈Σ?\n1\n|x|+ |u|+ 1 PD[ux]\n} .\nWith these definitions we have the following result.\nLemma 17. Wm ≤ mD∨max +O( √ m) .\nProof. We start by upper bounding the minS′∈S∨ with the expectation ES′∼Unif(S∨) over a split chosen uniformly at random:\nWm = E S∼Dm [ min S′∈S∨ sup e∈E∨ m∑ i=1 e(ui, vi) ]\n≤ E S∼Dm E S′∼Unif(S∨) [ sup e∈E∨ m∑ i=1 e(ui, vi) ]\n≤ sup e∈E∨ E S∼Dm E S′∼Unif(S∨) [ m∑ i=1 e(ui, vi) ]\n+ E S∼Dm E S′∼Unif(S∨) [ sup e∈E∨ ∣∣∣∣∣ m∑ i=1 ( e(ui, vi)− E x′i∼D E (u′i,v ′ i)∼Unif({x′i}∨) [e(u′i, v ′ i)] )∣∣∣∣∣ ] .\nThe same standard argument we used above shows that the second term in the last sum above can be bounded by 2mRm(E∨) = O( √ m). To compute the\nfirst term in the sum note that given a string y and a random split (u, v) ∼ Unif({y}∨), the probability that u = x for some fixed x ∈ Σ? is 1/(|y|+ 1) if x is a prefix of y and 0 otherwise. Thus, we let e = ex ◦ π1 ∈ E∨ and write\nE S∼Dm E S′∼Unif(S∨) [ m∑ i=1 e(ui, vi) ] = m E x′∼D E (u,v)∼Unif({x′}∨) ex(u)\n= mPx′∼D,(u,v)∼Unif({x′}∨)[u = x] = m ∑\nx′∈xΣ?\n1\n|x′|+ 1 PD[x′]\n= m ∑ v∈Σ?\n1\n|x|+ |v|+ 1 PD[xv] .\nSimilarly, if we have e = ex ◦ π2 ∈ E∨ then\nE S∼Dm E S′∼Unif(S∨) [ m∑ i=1 e(ui, vi) ] = m ∑ u∈Σ?\n1\n|x|+ |u|+ 1 PD[ux] .\nThus, we can combine these equations to show that Wm ≤ mD∨max+O( √ m).\nUsing Jensen’s inequality we now obtain the following bound.\nCorollary 18. For any m ≥ 1 and any r > 0 we have: Rm(H1,r) ≤ ( 2\n3 +\n8\n3 log 2\n) r log(2m+ 1)\nm\n+ (√ 2 +\n4√ log 2 ) r√ m √ (D∨max +O( √ 1/m)) log(2m+ 1) ."
    }, {
      "heading" : "8 Learning and Sample Complexity Bounds",
      "text" : "We now have all the ingredients to give generalization bounds for learning with weighted automata. In particular, we will give bounds for learning with a Lipschitz bounded loss function on all the classes of weighted automata and rational functions considered above. In cases where we have different bounds for the empirical and expected Rademacher complexities we also give two versions of the bound. All these bounds can be used to derive learning algorithms for weighted automata provided the right-hand side can be optimized over the corresponding hypothesis class. We will discuss in the next section what are the open problems related to obtaining efficient algorithms to solve these optimization problems. The proofs of these theorems are a straightforward combination of the bounds on the Rademacher complexity with well-known generalization bounds [49].\nTheorem 19. Let D be a probability distribution over Σ? × R and let S = ((xi, yi)) m i=1 be a sample of m i.i.d. examples from D. Assume that the loss ` : R×R→ R+ is M -bounded and µ-Lipschitz with respect to its first argument. Fix δ > 0. Then, the following holds:\n1. For all n ≥ 1 and p ∈ [1,+∞], with probability at least 1− δ the following holds simultaneously for all A ∈ An,p,1:\nLD(A) ≤ L̂S(A)+ √ 8µ2n(kn+ 2) log(m+ 2)\nm +\n2µ(Lm + 2)\nm +M\n√ log(1/δ)\n2m .\n2. For all r > 0, with probability at least 1− δ the following holds simultaneously for all f ∈ R2,r:\nLD(f) ≤ L̂S(f) + 2µr√ m +M\n√ log(1/δ)\n2m .\n3. For all r > 0, with probability at least 1− δ the following holds simultaneously for all f ∈ R1,r:\nLD(f) ≤ L̂S(f) + 2µr√ m\n√ 2(Dmax +O( √ 1/m)) log(2m) +M √ log(1/δ)\n2m .\n4. For all r > 0, with probability at least 1− δ the following holds simultaneously for all f ∈ H2,r:\nLD(f) ≤ L̂S(f) + 2µr√ m +M\n√ log(1/δ)\n2m .\n5. For all r > 0, with probability at least 1− δ the following holds simultaneously for all f ∈ H1,r:\nLD(f) ≤ L̂S(f) + (√ 2 + 4√\nlog 2 ) 2µr√ m √ (D∨max +O( √ 1/m)) log(2m+ 1)\n+\n( 2\n3 +\n8\n3 log 2\n) 2µr log(2m+ 1)\nm +M\n√ log(1/δ)\n2m .\nTheorem 20. Let D be a probability distribution over Σ? × R and let S = ((xi, yi)) m i=1 be a sample of m i.i.d. examples from D. Suppose the loss ` : R × R → R+ is M -bounded and µ-Lipschitz with respect to its first argument. Fix δ > 0. Then, the following hold:\n1. For all n ≥ 1 and p ∈ [1,+∞], with probability at least 1− δ the following holds simultaneously for all A ∈ An,p,1:\nLD(A) ≤ L̂S(A)+ √ 8µ2n(kn+ 2) log(m+ 2)\nm +\n2µ(LS + 2)\nm +3M\n√ log(2/δ)\n2m .\n2. For all r > 0, with probability at least 1− δ the following holds simultaneously for all f ∈ R1,r:\nLD(f) ≤ L̂S(f) + 2µr √ 2CS log(2m)\nm + 3M\n√ log(2/δ)\n2m .\n3. For all r > 0, with probability at least 1− δ the following holds simultaneously for all f ∈ H1,r:\nLD(f) ≤ L̂S(f) + (√ 2 + 4√\nlog 2\n) 2µr √ WS log(2m+ 1)\nm\n+\n( 2\n3 +\n8\n3 log 2\n) 2µr log(2m+ 1)\nm + 3M\n√ log(2/δ)\n2m ."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We presented the first algorithm-independent generalization bounds for learning with wide classes of WFAs. We introduced three ways to parametrize the complexity of WFAs and rational functions, each described by a different natural quantity associated with the automaton or function. We pointed out the merits of each description in the analysis of the problem of learning with WFAs, and proved upper bounds on the Rademacher complexity of several classes defined in terms of these parameters. An interesting property of these bounds is the appearance of different combinatorial parameters that tie the sample to the convergence rate: the length of the longest string LS for An,p,r; the maximum number of collisions CS for Rp,r; and, the minimum number of prefix or suffix collisions over all possible splits WS for Hp,r.\nAnother important feature of our bounds for the classes Hp,r is that they depend on spectral properties of Hankel matrices, which are commonly used in spectral learning algorithms for WFAs [31, 10]. We hope to exploit this connection in the future to provide more refined analyses of these learning algorithms. Our results can also be used to improve some aspects of existing spectral learning algorithms. For example, it might be possible to use the analysis of Theorem 11 for deriving strategies to help choose which prefixes and suffixes to consider in algorithms working with finite sub-blocks of an infinite Hankel matrix. This is a problem of practical relevance when working with large amounts of data which require balancing trade-offs between computation and accuracy [8].\nIt is possible to see that through a standard argument about the risk of the empirical risk minimizer, our generalization bounds can be used to establish that samples of size polynomial in the relevant parameters are enough to learn in all the classes considered. Nonetheless, the computational complexity of learning from such a sample might be hard, since we know this is the case for DFAs and PFAs [52, 36, 19]. In the case of DFAs, several authors have analyzed special cases which are tractable in polynomial time (e.g. [20] show DFAs are learnable from positive data generated by “easy” distributions, and [55] showed that exact learning can be done efficiently when the sample contains short witnesses distinguishing every pair of states). For PFAs, spectral methods show that polynomial learnability is possible if a new parameter related to spectral properties of the Hankel matrix is added to the complexity [31]. In the case of general WFAs, there is no equivalent result identifying settings in which the problem is tractable. In [10], we proposed an efficient algorithm for\nlearning WFAs that works in two steps: a matrix completion procedure applied to Hankel matrices followed by a spectral method to obtain a WFA from such Hankel matrix. Although each of these two steps solves an optimization problem without local minima, it is not clear from the analysis that the solution of the combined procedure is close to the empirical risk minimizer of any of the classes introduced in this paper. Nonetheless, we expect that the tools developed in this paper will prove useful in analyzing variants of this algorithm and will also help design new algorithms for efficiently learning interesting classes of WFA."
    } ],
    "references" : [ {
      "title" : "On the computational complexity of approximating distributions by probabilistic automata",
      "author" : [ "Naoki Abe", "Manfred K Warmuth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1992
    }, {
      "title" : "Statistical modeling for unit selection in speech synthesis",
      "author" : [ "Cyril Allauzen", "Mehryar Mohri", "Michael Riley" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Sequence kernels for predicting protein essentiality",
      "author" : [ "Cyril Allauzen", "Mehryar Mohri", "Ameet Talwalkar" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Formal analysis of online algorithms",
      "author" : [ "Benjamin Aminof", "Orna Kupferman", "Robby Lampert" ],
      "venue" : "In Proceedings of ATVA,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Model checking linear-time properties of probabilistic systems",
      "author" : [ "C. Baier", "M. Größer", "F. Ciesinski" ],
      "venue" : "In Handbook of Weighted automata. Springer,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Grammatical inference as a principal component analysis problem",
      "author" : [ "R. Bailly", "F. Denis", "L. Ralaivola" ],
      "venue" : "In ICML,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Absolute convergence of rational series is semi-decidable",
      "author" : [ "Raphaël Bailly", "François Denis" ],
      "venue" : "Inf. Comput.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Spectral learning of weighted automata: A forward-backward perspective",
      "author" : [ "B. Balle", "X. Carreras", "F.M. Luque", "A. Quattoni" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Methods of moments for learning stochastic languages: Unified presentation and empirical comparison",
      "author" : [ "B. Balle", "W.L. Hamilton", "J. Pineau" ],
      "venue" : "In ICML,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Spectral learning of general weighted automata via constrained matrix completion",
      "author" : [ "Borja Balle", "Mehryar Mohri" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Learning weighted automata",
      "author" : [ "Borja Balle", "Mehryar Mohri" ],
      "venue" : "In CAI,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "A canonical form for weighted automata and applications to approximate minimization",
      "author" : [ "Borja Balle", "Prakash Panangaden", "Doina Precup" ],
      "venue" : "In Logic in Computer Science (LICS),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L Bartlett", "Shahar Mendelson" ],
      "venue" : "In COLT,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "Rational Series and Their Languages",
      "author" : [ "Jean Berstel", "Christophe Reutenauer" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1988
    }, {
      "title" : "Noncommutative rational series with applications",
      "author" : [ "Jean Berstel", "Christophe Reutenauer" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Closing the learning-planning loop with predictive state representations",
      "author" : [ "B. Boots", "S. Siddiqi", "G. Gordon" ],
      "venue" : "In RSS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "The OCRopus open source OCR system",
      "author" : [ "Thomas M. Breuel" ],
      "venue" : "In Proceedings of IS&T/SPIE,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Realizations by stochastic finite automata",
      "author" : [ "Jack W. Carlyle", "Azaria Paz" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1971
    }, {
      "title" : "Pre-reduction graph products: Hardnesses of properly learning dfas and approximating edp on dags",
      "author" : [ "P. Chalermsook", "B. Laekhanukit", "D. Nanongkai" ],
      "venue" : "In Proceedings of FOCS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Partially distribution-free learning of regular languages from positive samples",
      "author" : [ "Alexander Clark", "Franck Thollard" ],
      "venue" : "In Proceedings of the 20th international conference on Computational Linguistics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Rational kernels: Theory and algorithms",
      "author" : [ "Corinna Cortes", "Patrick Haffner", "Mehryar Mohri" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2004
    }, {
      "title" : "Lp distance and equivalence of probabilistic automata",
      "author" : [ "Corinna Cortes", "Mehryar Mohri", "Ashish Rastogi" ],
      "venue" : "International Journal of Foundations of Computer Science,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars",
      "author" : [ "A. de Gispert", "G. Iglesias", "G. Blackwood", "E.R. Banga", "W. Byrne" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Combinatorial methods in density estimation",
      "author" : [ "Luc Devroye", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2001
    }, {
      "title" : "Handbook of weighted automata",
      "author" : [ "Manfred Droste", "Werner Kuich", "Heiko Vogler", "editors" ],
      "venue" : "EATCS Monographs on Theoretical Computer Science. Springer,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Uniform central limit theorems, volume 23",
      "author" : [ "Richard M Dudley" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1999
    }, {
      "title" : "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids",
      "author" : [ "Richard Durbin", "Sean R. Eddy", "Anders Krogh", "Graeme J. Mitchison" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1998
    }, {
      "title" : "Automata, Languages and Machines, volume A",
      "author" : [ "Samuel Eilenberg" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1974
    }, {
      "title" : "Matrices de Hankel",
      "author" : [ "M. Fliess" ],
      "venue" : "Journal de Mathématiques Pures et Appliquées,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1974
    }, {
      "title" : "Modelling sparse dynamical systems with compressed predictive state representations",
      "author" : [ "W.L. Hamilton", "M.M. Fard", "J. Pineau" ],
      "venue" : "In ICML,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    }, {
      "title" : "A spectral algorithm for learning hidden Markov models",
      "author" : [ "D. Hsu", "S.M. Kakade", "T. Zhang" ],
      "venue" : "In COLT,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Image compression using weighted finite automata",
      "author" : [ "Karel Culik II", "Jarkko Kari" ],
      "venue" : "Computers & Graphics,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1993
    }, {
      "title" : "Vc-dimensions of finite automata and commutative finite automata with k letters and n states",
      "author" : [ "Yoshiyasu Ishigami", "Sei’ichi Tani" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1997
    }, {
      "title" : "Regular models of phonological rule systems",
      "author" : [ "Ronald M. Kaplan", "Martin Kay" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1994
    }, {
      "title" : "The replace operator",
      "author" : [ "Lauri Karttunen" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1995
    }, {
      "title" : "Cryptographic limitations on learning boolean formulae and finite automata",
      "author" : [ "Michael J. Kearns", "Leslie G. Valiant" ],
      "venue" : "Journal of ACM,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1994
    }, {
      "title" : "Rademacher processes and bounding the risk of function learning",
      "author" : [ "Vladimir Koltchinskii", "Dmitry Panchenko" ],
      "venue" : "In High Dimensional Probability II,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2000
    }, {
      "title" : "Semirings, Automata, Languages. Number 5 in EATCS Monographs on Theoretical Computer Science",
      "author" : [ "Werner Kuich", "Arto Salomaa" ],
      "venue" : null,
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1986
    }, {
      "title" : "Low-rank spectral learning with weighted loss functions",
      "author" : [ "A. Kulesza", "N. Jiang", "S. Singh" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2015
    }, {
      "title" : "Low-Rank Spectral Learning",
      "author" : [ "Alex Kulesza", "N Raj Rao", "Satinder Singh" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2014
    }, {
      "title" : "Some applications of concentration inequalities to statistics",
      "author" : [ "Pascal Massart" ],
      "venue" : "Annales de la Faculté des Sciences de Toulouse,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2000
    }, {
      "title" : "A trace inequality of John von Neumann",
      "author" : [ "L. Mirsky" ],
      "venue" : "Monatshefte fr Mathematik,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1975
    }, {
      "title" : "Finite-state transducers in language and speech processing",
      "author" : [ "Mehryar Mohri" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1997
    }, {
      "title" : "Weighted automata algorithms. In Handbook of Weighted Automata, Monographs in Theoretical Computer Science, pages 213–254",
      "author" : [ "Mehryar Mohri" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2009
    }, {
      "title" : "Weighted automata in text and speech processing",
      "author" : [ "Mehryar Mohri", "Fernando Pereira", "Michael Riley" ],
      "venue" : "In Proceedings of ECAI-96 Workshop on Extended finite state models of language,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 1996
    }, {
      "title" : "Dynamic compilation of weighted context-free grammars",
      "author" : [ "Mehryar Mohri", "Fernando C.N. Pereira" ],
      "venue" : "In Proceedings of COLING-ACL,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1998
    }, {
      "title" : "Speech recognition with weighted finite-state transducers",
      "author" : [ "Mehryar Mohri", "Fernando C.N. Pereira", "Michael Riley" ],
      "venue" : "In Handbook on Speech Processing and Speech Comm. Springer,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2008
    }, {
      "title" : "Foundations of machine learning",
      "author" : [ "Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar" ],
      "venue" : "MIT press,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2012
    }, {
      "title" : "An efficient compiler for weighted rewrite rules",
      "author" : [ "Mehryar Mohri", "Richard Sproat" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1996
    }, {
      "title" : "Speech recognition by composition of weighted finite automata. In Finite-State Language Processing",
      "author" : [ "Fernando Pereira", "Michael Riley" ],
      "venue" : null,
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1997
    }, {
      "title" : "The minimum consistent DFA problem cannot be approximated within any polynomial",
      "author" : [ "Leonard Pitt", "Manfred K. Warmuth" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 1993
    }, {
      "title" : "Automata-Theoretic Aspects of Formal Power Series",
      "author" : [ "Arto Salomaa", "Matti Soittola" ],
      "venue" : null,
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 1978
    }, {
      "title" : "A finite-state architecture for tokenization and graphemeto-phoneme conversion in multilingual text analysis",
      "author" : [ "Richard Sproat" ],
      "venue" : "In Proceedings of the ACL SIGDAT Workshop",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1995
    }, {
      "title" : "Finite Automata: Behavior and Synthesis",
      "author" : [ "B Trakhtenbrot", "Y Barzdin" ],
      "venue" : null,
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 1973
    }, {
      "title" : "An introduction to matrix concentration inequalities",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "Foundations and Trends R  © in Machine Learning,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2015
    }, {
      "title" : "Lectures in Geometrical Functional Analysis",
      "author" : [ "Roman Vershynin" ],
      "venue" : null,
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 51,
      "context" : "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 37,
      "context" : "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 31,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 209,
      "endOffset" : 229
    }, {
      "referenceID" : 44,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 209,
      "endOffset" : 229
    }, {
      "referenceID" : 49,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 209,
      "endOffset" : 229
    }, {
      "referenceID" : 42,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 209,
      "endOffset" : 229
    }, {
      "referenceID" : 46,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 209,
      "endOffset" : 229
    }, {
      "referenceID" : 52,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 304,
      "endOffset" : 311
    }, {
      "referenceID" : 1,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 304,
      "endOffset" : 311
    }, {
      "referenceID" : 33,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 364,
      "endOffset" : 376
    }, {
      "referenceID" : 34,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 364,
      "endOffset" : 376
    }, {
      "referenceID" : 48,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 364,
      "endOffset" : 376
    }, {
      "referenceID" : 45,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 389,
      "endOffset" : 393
    }, {
      "referenceID" : 22,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 415,
      "endOffset" : 419
    }, {
      "referenceID" : 26,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 436,
      "endOffset" : 443
    }, {
      "referenceID" : 2,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 436,
      "endOffset" : 443
    }, {
      "referenceID" : 20,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 478,
      "endOffset" : 482
    }, {
      "referenceID" : 4,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 523,
      "endOffset" : 529
    }, {
      "referenceID" : 3,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 523,
      "endOffset" : 529
    }, {
      "referenceID" : 16,
      "context" : "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.",
      "startOffset" : 564,
      "endOffset" : 568
    }, {
      "referenceID" : 30,
      "context" : "The recent developments in spectral learning [31, 6] have triggered a renewed interest in the use of WFAs in machine learning, with several recent successes in ∗Corresponding author: b.",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "The recent developments in spectral learning [31, 6] have triggered a renewed interest in the use of WFAs in machine learning, with several recent successes in ∗Corresponding author: b.",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "natural language processing [8, 9] and reinforcement learning [16, 30].",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "natural language processing [8, 9] and reinforcement learning [16, 30].",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "natural language processing [8, 9] and reinforcement learning [16, 30].",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : "natural language processing [8, 9] and reinforcement learning [16, 30].",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 30,
      "context" : "The interest in spectral learning algorithms for WFAs is driven by the many appealing theoretical properties of such algorithms, which include their polynomial-time complexity, the absence of local minima, statistical consistency, and finite sample bounds à la PAC [31].",
      "startOffset" : 265,
      "endOffset" : 269
    }, {
      "referenceID" : 10,
      "context" : "See [11] for a recent survey of algorithms for learning WFAs with a discussion of the different assumptions and learning models.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "For spectral learning of WFAs, an algorithm-dependent agnostic generalization bound was proven in [10] using a stability argument.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 9,
      "context" : "However, while [10] proposed a broad family of algorithms for learning WFAs parametrized by several choices of loss functions and regularizations, their bounds hold only for one particular algorithm within this family.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "In this paper, we start the systematic development of algorithm-independent generalization bounds for learning with WFAs, which apply to all the algorithms proposed in [10], as well as to others using WFAs as their hypothesis class.",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 36,
      "context" : "The use of Rademacher complexity to derive generalization bounds is standard [37] (see also [13] and [49]).",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "The use of Rademacher complexity to derive generalization bounds is standard [37] (see also [13] and [49]).",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 47,
      "context" : "The use of Rademacher complexity to derive generalization bounds is standard [37] (see also [13] and [49]).",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 47,
      "context" : "see [49] and references therein).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 32,
      "context" : "The VC-dimension of deterministic finite automata (DFAs) with n states over an alphabet of size k was shown by [33] to be in O(kn log n).",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "For probabilistic finite automata (PFAs), it was shown by [1] that, in an agnostic setting, a sample of size Õ(kT n/ε) is sufficient to learn a PFA with n states and k symbols whose log-loss error is at most ε away from the optimal one in the class when the error is measured on all strings of length T .",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 39,
      "context" : "Another recent line of work, which aims to provide guarantees for spectral learning of WFAs in the non-realizable setting, is the so-called low-rank spectral learning approach [40].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 38,
      "context" : "This has led to interesting upper bounds on the approximation error between minimal WFAs of different sizes [39].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "See [12] for a polynomial-time algorithm for computing these approximations.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 51,
      "context" : "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 37,
      "context" : "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 43,
      "context" : "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 51,
      "context" : "Functions mapping strings to real numbers can also be viewed as non-commutative formal power series, which often helps deriving rigorous proofs in formal language theory [53, 15, 38].",
      "startOffset" : 170,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "Functions mapping strings to real numbers can also be viewed as non-commutative formal power series, which often helps deriving rigorous proofs in formal language theory [53, 15, 38].",
      "startOffset" : 170,
      "endOffset" : 182
    }, {
      "referenceID" : 37,
      "context" : "Functions mapping strings to real numbers can also be viewed as non-commutative formal power series, which often helps deriving rigorous proofs in formal language theory [53, 15, 38].",
      "startOffset" : 170,
      "endOffset" : 182
    }, {
      "referenceID" : 28,
      "context" : "By the theorem of Fliess [29] (see also [18] and [15]), Hf has finite rank n if and only if f is rational and there exists a WFA A with n states computing f , that is, rank(f) = rank(Hf ).",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "By the theorem of Fliess [29] (see also [18] and [15]), Hf has finite rank n if and only if f is rational and there exists a WFA A with n states computing f , that is, rank(f) = rank(Hf ).",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "By the theorem of Fliess [29] (see also [18] and [15]), Hf has finite rank n if and only if f is rational and there exists a WFA A with n states computing f , that is, rank(f) = rank(Hf ).",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 36,
      "context" : "The Rademacher complexity of a hypothesis class can be used to derive generalization bounds for a variety of learning tasks [37, 13, 49].",
      "startOffset" : 124,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "The Rademacher complexity of a hypothesis class can be used to derive generalization bounds for a variety of learning tasks [37, 13, 49].",
      "startOffset" : 124,
      "endOffset" : 136
    }, {
      "referenceID" : 47,
      "context" : "The Rademacher complexity of a hypothesis class can be used to derive generalization bounds for a variety of learning tasks [37, 13, 49].",
      "startOffset" : 124,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "Membership in R1,r was shown to be semi-decidable in [7].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : "On the other hand, membership in R2,r can be decided in polynomial time [22].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "The following result follows from [12] and gives a useful condition for the boundedness of Hf .",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "Since membership in R2 is efficiently testable [22], a polynomial time algorithm from [12] can be used to compute ‖f‖H,p and thus test membership in Hp,r.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "Since membership in R2 is efficiently testable [22], a polynomial time algorithm from [12] can be used to compute ‖f‖H,p and thus test membership in Hp,r.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 23,
      "context" : "[24]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 55,
      "context" : "3 in [57]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 9,
      "context" : "The second bound was proven in [10].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 40,
      "context" : "Lemma 7 (Massart [42]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 47,
      "context" : "The lower bound is obtained using Khintchine–Kahane’s inequality (see appendix of [49]):",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 41,
      "context" : "Then, by von Neumann’s trace inequality [43] and Hölder’s inequality, the following holds:",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 54,
      "context" : "To bound the Rademacher complexity of Hp,r in the case p = 1 we will need the following moment bound for the operator norm of a random matrix from [56].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 54,
      "context" : "2 [56]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 25,
      "context" : "where in the last inequality we used that the VC-dimension of E is 1, in which case Dudley’s chaining method [26] yields Rm(E) ≤ C √ 1/m for some universal constant C > 0.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 47,
      "context" : "The proofs of these theorems are a straightforward combination of the bounds on the Rademacher complexity with well-known generalization bounds [49].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 30,
      "context" : "Another important feature of our bounds for the classes Hp,r is that they depend on spectral properties of Hankel matrices, which are commonly used in spectral learning algorithms for WFAs [31, 10].",
      "startOffset" : 189,
      "endOffset" : 197
    }, {
      "referenceID" : 9,
      "context" : "Another important feature of our bounds for the classes Hp,r is that they depend on spectral properties of Hankel matrices, which are commonly used in spectral learning algorithms for WFAs [31, 10].",
      "startOffset" : 189,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "This is a problem of practical relevance when working with large amounts of data which require balancing trade-offs between computation and accuracy [8].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 50,
      "context" : "Nonetheless, the computational complexity of learning from such a sample might be hard, since we know this is the case for DFAs and PFAs [52, 36, 19].",
      "startOffset" : 137,
      "endOffset" : 149
    }, {
      "referenceID" : 35,
      "context" : "Nonetheless, the computational complexity of learning from such a sample might be hard, since we know this is the case for DFAs and PFAs [52, 36, 19].",
      "startOffset" : 137,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "Nonetheless, the computational complexity of learning from such a sample might be hard, since we know this is the case for DFAs and PFAs [52, 36, 19].",
      "startOffset" : 137,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "[20] show DFAs are learnable from positive data generated by “easy” distributions, and [55] showed that exact learning can be done efficiently when the sample contains short witnesses distinguishing every pair of states).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 53,
      "context" : "[20] show DFAs are learnable from positive data generated by “easy” distributions, and [55] showed that exact learning can be done efficiently when the sample contains short witnesses distinguishing every pair of states).",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 30,
      "context" : "For PFAs, spectral methods show that polynomial learnability is possible if a new parameter related to spectral properties of the Hankel matrix is added to the complexity [31].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : "In [10], we proposed an efficient algorithm for",
      "startOffset" : 3,
      "endOffset" : 7
    } ],
    "year" : 2016,
    "abstractText" : "This paper studies the problem of learning weighted automata from a finite labeled training sample. We consider several general families of weighted automata defined in terms of three different measures: the norm of an automaton’s weights, the norm of the function computed by an automaton, or the norm of the corresponding Hankel matrix. We present new data-dependent generalization guarantees for learning weighted automata expressed in terms of the Rademacher complexity of these families. We further present upper bounds on these Rademacher complexities, which reveal key new data-dependent terms related to the complexity of learning weighted automata.",
    "creator" : "TeX"
  }
}