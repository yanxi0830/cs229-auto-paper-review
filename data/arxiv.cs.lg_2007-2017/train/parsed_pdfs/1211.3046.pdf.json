{
  "name" : "1211.3046.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recovering Optimal Solution by Dual Random Projection",
    "authors" : [ "Lijun Zhang", "Rong Jin", "Tianbao Yang", "Zhang Jin Yang" ],
    "emails" : [ "zhanglij@msu.edu", "rongjin@cse.msu.edu", "tyang@ge.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n21 1.\n30 46\nv1 [\ncs .L\nG ]\n1 3\nKeywords: Random projection, Primal solution, Dual solution, Low rank"
    }, {
      "heading" : "1. Introduction",
      "text" : "Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008), and information retrieval (Goel et al., 2005). In this work, we focus on random projection for classification.\nMany studies were devoted to analyzing the classification performance using random projection. In this paper, we examine the effect of random projection for data classification from a very different aspect. In particular, we are interested in accurately recovering the optimal solution to the original optimization problem related to data classification using random projection. This is particularly useful for feature selection (Guyon and Elisseeff, 2003), where important features are often selected based on their weights in the linear prediction model learned from the training data. In this case, it is insufficient to simply guarantee a low classification error for the learned prediction model based on random projection. In order to ensure that similar features are selected by the prediction model based on random projection, it is important to guarantee that the recovered solution based on random projection is close to the one obtained by solving the original optimization problem without random projection.\nc© 2012 L. Zhang, R. Jin & T. Yang.\nThe rest of the draft is arranged as follows: Section 2 describes the problem of recovering optimal solution by random projection, the center of this work. Section 3 describes the dual random projection approach for reconstructing optimal solutions. Section 4 presents the main theoretical results for the proposed algorithm. Section 5 presents the proof for the theorems stated in Section 4. Section 6 concludes this work with open questions."
    }, {
      "heading" : "2. The Problem of Recovering Optimal Solutions from Random Projection",
      "text" : "Let (xi, yi), i = 1, . . . , n be a set of training examples, where xi ∈ Rd is a vector of d dimension and yi ∈ {−1,+1} is the binary class assignment for xi. Let X = (x1, . . . ,xn) and y = (y1, . . . , yn)\n⊤ include input patterns and the class assignments of all training examples. A classifier w ∈ Rd is learned from the training examples by solving the following optimization problem:\nmin w∈Rd\nλ 2 ‖w‖2 +\nn∑\ni=1\nℓ(yix ⊤ i w) (1)\nwhere ℓ(z) is a convex loss function that is differentiable 1. By writing ℓ(z) in its convex conjugate form, i.e.\nℓ(z) = min α∈Ω\nαz − ℓ∗(α),\nwhere ℓ∗(α) is the convex conjugate of ℓ(z) and Ω is a domain for dual variable α, we have the dual optimization problem\nmax α∈Ωn\n− n∑\ni=1\nℓ∗(αi)− 1\n2λ α\n⊤Gα (2)\nwhere α = (α1, · · · , αn)⊤ and D(y) = diag(y) and G is the Gram matrix given by\nG = D(y)X⊤XD(y) (3)\nIn the following, we denote by w∗ the optimal primal solution to (1), and by α∗ the optimal dual solution to (2). The following proposition connects w∗ and α∗.\nProposition 1 Let w∗ be the optimal primal solution to (1), and α∗ be the optimal dual solution to (2), we have\nw∗ = − 1\nλ XD(y)α∗, and [α∗]i = ∇ℓ\n( yix ⊤ i w∗ ) , i = 1, . . . , n (4)\nThe proof of Proposition 1 and other omitted proofs are deferred to the Appendix. When dimension d is high and the number of training examples n is large, solving either the primal problem in (1) or the dual problem in (2) can be computationally expensive. To reduce the computational cost, one common approach is to significantly reduce the dimensionality by\n1. For non differentiable loss functions such as hinge loss, we could apply the smoothing technique (Nesterov, 2005) to make it differentiable.\nrandom projection. Let S ∈ Rd×m be a Gaussian random matrix, where each entry Si,j is independently drawn from a Gaussian distribution N (0, 1) and m is significantly smaller than d. Using random matrix S, we generate a new data representation for input data points by\nx̂i = 1√ m S⊤xi, (5)\nand we solve the following problem in the projected space:\nmin z∈Rm\nλ 2 ‖z‖2 +\nn∑\ni=1\nℓ(yiz ⊤x̂i) (6)\nThe corresponding dual problem is written as\nmin α∈Ωn\n− n∑\ni=1\nℓ∗(αi)− 1\n2λ α\n⊤Ĝα (7)\nwhere\nĜ = D(y)X⊤ SS⊤\nm XD(y) (8)\nRemark 2 Initially, the choice of Guassian random matrix S is justified by that the expectation of dot-product of any two examples in the projected space is equal to the dot-product in the original space, i.e.,\nE[x̂⊤i x̂j] = x ⊤ i E\n[ 1\nm SS⊤\n] xj = x ⊤ i xj\nwhere the last equality follows that E [ 1 m SS⊤ ] = I.\nLet z∗ denote the optimal solution to the primal problem (6) in the projected space, and α̂ denote the optimal dual solution to (7). Similar to Proposition 1, the following proposition, connects z∗ and α̂.\nProposition 3 We have\nz∗ = − 1\nλ 1√ m S⊤XD(y)α̂, and [α̂∗]i = ∇ℓ ( yi√ m x⊤i Sz∗ ) , i = 1, . . . , n (9)\nGiven the optimal solution z∗ ∈ Rm, the data point x ∈ Rd is classified by x⊤Sz∗/ √ m, which is equivalent to defining a new solution ŵ ∈ Rd given below, to which we refer as the naive solution,\nŵ = 1√ m Sz∗ (10)\nThe classification performance of ŵ has been examined by many studies (e.g. (Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)). The general conclusion is that when the original data is linearly separable with a large margin, the classification error for the solution based on random projection is usually small.\nAlthough these studies show that ŵ can achieve a small classification error under appropriate assumption, it is unclear if solution ŵ is a good approximation of the true optimal solution w∗. In fact, as we will see the result in Section 4, ŵ is almost guaranteed to be a BAD approximation of w∗ (i.e., ‖ŵ −w∗‖2 = Ω(‖w∗‖2)). This observation leads to an interesting question, Is it possible to accurately recover the optimal solution w∗ based on z∗, the random projection based solution. We refer to this problem as Recovery of Optimal Solution.\nRelationship to Compression Sensing The proposed problem is closely related to compressive sensing (Candès and Wakin, 2008; Donoho, 2006) where the goal is to recover a high dimensional but sparse vector using a small number of random projections. The key difference between our work and compressive sensing is that we don’t have the direct access to the random measurement of the target vector (which in our case is w∗). Instead, z∗ is the optimal solution to (6), the primal problem using random projection. However, the following Theorem shows that z∗ is a good approximation of S ⊤w∗/ √ m, which includes m random measurements of w∗, if the data matrix X is of low rank and the number of random measurements m is sufficiently large.\nTheorem 1 With a probability at least 1− δ − exp(−m/32), we have\n‖√mz∗ − S⊤w∗‖2 ≤ 2 √ 2ε√\n1− ε‖S ⊤w∗‖2\nprovided\nm ≥ r(log(r 2 + r) + log(1/δ))\ncε2\nwhere constant c is at least 1/32, and r is the rank of X.\nGiven the approximation bound in Theorem 1, it is appealing to reconstruct w∗ using the compressive sensing algorithm provided that w∗ is sparse to certain bases. We note that the low rank assumption for data matrix X implies that w∗ is sparse with respect to the singular vector system of X. However, since z∗ only provides an approximation to the random measurements of w∗, running the compressive sensing algorithm will not be able to perfectly recover w∗ from z∗. In Section 3, we present an algorithm, that recovers w∗ with a small error provided that the data matrix X is of low rank. Compared to the compressive sensing algorithm, the main advantage of the proposed algorithm is its computational simplicity because it does not need to compute the eigenvectors of X and solve an optimization problem that minimizes the ℓ1 norm."
    }, {
      "heading" : "3. Algorithm",
      "text" : "To motivate our algorithm, let us revisit the optimal primal solution w∗ to (1), which is given in Proposition 1, i.e.,\nw∗ = − 1\nλ XD(y)α∗, (11)\nAlgorithm 1 A Dual Random Projection Approach for Recovering Optimal Solution\n1: Input: input patterns X ∈ Rd×n, binary class assignment y ∈ {−1,+1}n, and sample size m 2: Sample a Gaussian random matrix S ∈ Rd×m 3: Compute the projected data matrix as X̂ = S⊤X/ √ m. 4: Compute α̂ by solving the primal problem (6) and constructing α̂ by Proposition 3. 5: Output: the recovered solution w̃ = −XD(y)α̂/λ\nwhere α∗ is the optimal solution to the dual problem (2). Given the projected data x̂ = S⊤x/ √ m, we have reached an approximate dual problem in (7). Comparing it with the dual problem in (2), and noticing that E[SS⊤/m] = I. As a result, when the number of random projections m is sufficiently large, we would expect α̂ to be close to α∗. As a result, we can use α̂ as an approximate of α∗ in (11), which yields a recovered prediction model, denoted by w̃:\nw̃ = − 1 λ XD(y)α̂ = −\nn∑\ni=1\n1 λ yi[α̂]ixi (12)\nRemark 4 Note that the key difference between the recovered solution w̃ and the naive solution ŵ is that ŵ is computed by projecting the optimal primal solution z∗ in the projected space back to the original space via S, while w̃ is computed directly in the original space using the approximate dual solution α̂. As a result, the naive solution ŵ lies in the subspace spanned by the column vectors in random matrix S (denoted by AS), while the recovered solution w̃ lies in the subspace that also contains the optimal solution w∗, i,e., the subspace spanned by columns of X (denoted by A). The mismatch between spaces AS and A leads to the large approximation error for ŵ.\nAlgorithm 1 shows the steps of the proposed method. We note that although dual variables have been widely used in the analysis of convex optimization (Boyd and Vandenberghe, 2004; Hazan et al., 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.\nTo further reduce the recovery error, we develop an iterative method shown in Algorithm 2. The intuition comes from that if ‖w∗ − w̃‖2 ≤ ǫ‖w∗‖2 with a small ǫ, we can apply the same dual random projection algorithm again to recover ∆w = w∗ − w̃, which should result in a recovery error of ‖∆w‖2 ≤ ǫ2‖w∗‖2. This simple intuition leads to an iterative method shown in Algorithm 2. If we repeat the process with T iterations, we should be able to obtain a solution with a recovery error of ǫT . In Algorithm 2, at iteration t, given the recovered solution w̃t−1 obtained from the previous iteration, we then solve the optimization problem in (13) that is designed to recover w∗ − w̃t−1.\nRemark 5 It is important to note that although Algorithm 2 is consisted of multiple iterations, the random projection of the data matrix is only computed once before the start of the iterations. This important feature makes the iterative algorithm computationally attractive\nAlgorithm 2 An Iterative Dual Random Projection Approach for Recovering Optimal Solution\n1: Input: input patterns X ∈ Rd×n, binary class assignment y ∈ {−1,+1}n, sample size m, and number of iterations T 2: Sample a Gaussian random matrix S ∈ Rd×m 3: Compute the projected data matrix as X̂ = (x̂1, . . . , x̂n) = S ⊤X/ √ m. 4: Initialize w̃0 = 0 5: for t = 1, . . . , T do 6: Obtain zt∗ ∈ Rm by solving the following optimization problem\nzt∗ = argmin z∈Rm\nλ\n2\n∥∥∥z+ S⊤w̃t−1/ √ m ∥∥∥ 2\n2 +\nn∑\ni=1\nℓ ( yiz ⊤x̂i + yiw̃ ⊤ t−1xi ) (13)\n7: Compute the dual solution ãt using\n[ãt]i = ∇ℓ ( yix̂ ⊤ i z t ∗ + yiw̃ ⊤ t−1xi )\n8: Update the solution by w̃t = − ∑n i=1 yi[ã t]ixi/λ\n9: end for 10: Output the recovered solution w̃T\nas calculating random projections of large data matrix is computationally expensive and has been the subject of many studies, e.g., (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010). However, it is worth noting that in Algorithm 2 at each iteration, we need to compute the dot-product w̃⊤t xi for all training data in the original space. We also note that Algorithm 2 is related to the Epoch gradient descent algorithm (Hazan and Kale, 2011) for stochastic optimization in that the solution obtained in the previous iteration is served as the center to the optimization problem of the current iteration. Unlike the algorithm in (Hazan and Kale, 2011), we do not shrink the domain size over the iterations in Algorithm 2."
    }, {
      "heading" : "4. Main Results",
      "text" : "In this section, we will present a bound of the recovery error ‖w∗−w̃‖2 for the dual random projection algorithm. We will then extend the result to the iterative algorithm. Similar to compressive sensing, we need to assume certain sparse structure for the recovery problem. In our case, we assume that the data matrix X is of low rank. We note that the low rank assumption is closely related to the sparsity assumption made in compressive sensing. This is because w∗ lies in the subspace spanned by the column vectors of X and the low rank assumption of X directly implies that w∗ is sparse with respect to the eigen system of X.\nWe denote by r the rank of matrix X. The following theorem shows that the recovery error of Algorithm 1 is small provided that (1) X is of low rank (i.e., r ≪ min(d, n)), and (2) sufficiently large number of random projections.\nTheorem 2 Let w∗ be the optimal solution to (1) and let w̃ be the solution recovered by Algorithm 1. Then, with a probability at least 1− δ, we have\n‖w∗ − w̃‖2 ≤ 2ε\n1− ε‖w∗‖2\nprovided\nm ≥ r(log(r 2 + r) + log(1/δ))\ncε2\nwhere constant c is at least 1/32.\nRemark 6 According to Theorem 2, the number of required random projections is Ω(r log r). This is similar to compressive sensing result if we view rank r as the sparsity measure used in compressive sensing. Following the same arguments as compressive sensing, it may be possible to argue that Ω(r log r) is optimal due to the result of coupon collector’s problem (Mowani and Raghavan, 1995), although the rigorous analysis remains to be developed.\nAs a comparison, the following theorem shows that with a high probability, the naive solution ŵ given in (10) (i.e., the solution based on random projection without exploiting the dual variables) does not accurately recover the true optimal solution w∗.\nTheorem 3 With a probability 1− exp(−(d− r)/32) − exp(−m/32) − δ, we have\n‖ŵ −w∗‖2 ≥ √\nd− r m\n( 1 2 − ε √ 2(1 + ε) 1− ε ) ‖w∗‖2\nprovided\nm ≥ r(log(r 2 + r) + log(1/δ))\ncε2\nRemark 7 As indicated by Theorem 3, when m is sufficiently larger than r but significantly smaller than d, we have ‖ŵ −w∗‖2 = Ω( √ d/m‖w∗‖2), indicating that ŵ does not approximate w∗ well.\nIt is important to note that Theorem 3 does not contradict with the previous results showing that the random projection method could result in a small classification error if the data set is almost linearly separable with a large margin. This is because, to decide if ŵ carries similar classification performance as w∗, we need to measure the following term\nmax x∈span(X),‖x‖2≤1\n|x⊤(ŵ −w∗)| (14)\nSince ‖ŵ −w∗‖2 can also be written as\n‖ŵ −w∗‖2 = max ‖x‖2≤1 x⊤(ŵ −w∗)\nthe quantity defined in (14) could be significantly smaller than ‖ŵ −w∗‖2 if data matrix X is of low rank. The following theorem quantifies this statement.\nTheorem 4 With a probability at least 1− δ, we have\nmax x∈span(X),‖x‖2≤1\nx⊤(w∗ − ŵ) ≤ ε ( 1 + 2\n1− ε\n) ‖w∗‖2\nprovided\nm ≥ r(log(r 2 + r) + log(1/δ))\ncε2\nwhere constant c is at least 1/32.\nThe proof of Theorem 4 can be found in Appendix. We note that Theorem 4 directly implies the result of margin classification error for random projection (Blum, 2006). This is because when a data point (xi, yi) can be separated by w∗ with a margin γ, i.e. yiw ⊤ ∗ xi ≥ γ|w∗|, it\nwill be classified by ŵ with a margin at least γ− ( 1 + 2(1+ε)1−ε ) ε provided γ > ( 1 + 2(1+ε)1−ε ) ε.\nUsing Theorem 2, we now state the recovery result for the iterative method in Algorithm 2.\nTheorem 5 Let w∗ be the optimal solution to (1) and let w̃T be the solution recovered by Algorithm 2. Then, with a probability at least 1− δ, we have\n‖w∗ − w̃T ‖2 ≤ ( 2ε\n1− ε\n)T ‖w∗‖2\nprovided\nm ≥ r(log(r 2 + r) + log(T/δ))\ncε2\nwhere constant c is at least 1/32."
    }, {
      "heading" : "5. Analysis",
      "text" : "Before presenting the analysis, we first establish some notations and facts. Let the SVD of X be\nX = UΣV ⊤ =\nr∑\ni=1\nλiuiv ⊤ i\nwhere λi is the ith singular value of X, ui and vi are the corresponding left and right singular vectors of X. Let U = (u1, . . . ,ur), V = (v1, . . . ,vr). Using the singular value decomposition of X, we define\nγ∗ = −ΣV ⊤D(y)α∗, γ̂ = −ΣV ⊤D(y)α̂ It is straightforward to show that\nw∗ = 1\nλ Uγ∗, w̃ =\n1 λ U γ̂\nSince U is an othorgonal matrix, we have\n‖w∗‖2 = 1\nλ ‖γ∗‖2, ‖w̃‖2 =\n1 λ ‖γ̂‖2\nLet us define A = U⊤S ∈ Rr×m. It is easy to verify that A is an Gaussian matrix of size r ×m."
    }, {
      "heading" : "5.1. Proof of Theorem 2",
      "text" : "The key to our analysis is to show that Ĝ in (7) is close to G in (2) when the number of random projections is sufficiently large. To this end, we need the following concentration inequality for Gaussian random matrix.\nCorollary 6 Let M ∈ Rr×m be a standard Gaussian random matrix. Then, with a probability at least 1− δ, we have ∥∥∥∥ 1\nm MM⊤ − I ∥∥∥∥ 2 ≤ ε\nprovided that\nm ≥ r ( log(r2 + r) + log(1/δ) )\ncε2\nwhere ‖M‖2 is the spectral norm of matrix M and c is a constant whose value is at least 1/32.\nRemark 8 The above corollary serves the key to our analysis, which enable us to bound G− Ĝ and furthermore to bound α∗ − α̂.\nUsing Corollary 6, we have the following theorem that bounds the difference between Ĝ and G.\nTheorem 7 With a probability 1− δ, we have\n(1 + ε)G Ĝ (1− ε)G\nprovided\nm ≥ r ( log(r2 + r) + log(1/δ) )\ncε2\nProof We rewrite G and Ĝ as\nG = D(y)V ΣU⊤UΣV ⊤D(y) Ĝ = D(y)V ΣU⊤ SS⊤\nm UΣV ⊤D(y) = D(y)V Σ\nAA⊤\nm ΣV ⊤D(y)\nThen with a probability 1− δ under the given condition on m, we can show that\nĜ− (1 + ε)G = D(y)V Σ ( AA⊤\nm − (1 + ε)I\n) ΣV ⊤D(y) 0\nand\nĜ− (1− ε)G = D(y)V Σ ( AA⊤\nm − (1− ε)I\n) ΣV ⊤D(y) 0\nusing the result in Corollary 6 since A is a Guassian matrix of size r ×m.\nWe now give the proof for Theorem 2. The basic logic is straightforward. Since Ĝ is close to G, we would expect α̂, the optimal solution to (7), to be close to α∗, the optimal\nsolution to (2). Since w∗ = XD(y)α∗/λ and w̃ = XD(y)α̂/λ, we would then expect w̃ to be close to w∗. Proof [Theorem 2] Define L(α) and L̂(α) as\nL(α) = − n∑\ni=1\nℓ∗(αi)− 1\n2λ α\n⊤Gα, L̂(α) = − n∑\ni=1\nℓ∗(αi)− 1\n2λ α\n⊤Ĝα\nSince α̂ maximizes L̂(α) over the domain Ωn, we have\nL̂(α̂) ≥ L̂(α∗) + 1\n2λ (α̂−α∗)⊤Ĝ(α̂−α∗) (15)\nUsing the concaveness of L̂(α), we have L̂(α̂) ≤ L̂(α∗) + (α̂−α∗)⊤∇L̂(α∗) = L̂(α∗) + (α̂−α∗)⊤ ( ∇L̂(α∗)−∇L(α∗) +∇L(α∗) )\n≤ L̂(α∗) + 1\nλ (α̂−α∗)⊤(G− Ĝ)α∗ (16)\nwhere the last inequality follows from the fact that (α̂ − α∗)⊤∇L(α∗) ≤ 0 since α∗ maximizes L(α) over the domain Ωn. Combining the inequalities in (15) and (16), we have\n1 λ (α̂−α∗)⊤(G − Ĝ)α∗ ≥ 1 2λ (α̂−α∗)⊤Ĝ(α̂−α∗)\nTherefore\n(γ̂ − γ∗)⊤ ( I − AA ⊤\nm\n) γ∗ ≥ 1\n2 (γ̂ − γ∗)⊤\nAA⊤\nm (γ̂ − γ∗) (17)\nUsing Corollary 6, with a probability 1− δ, we have ‖I −AA⊤/m‖2 ≤ ε and therefore\n(1− ε)‖γ̂ − γ∗‖2 ≤ 2ε‖γ∗‖2\nWe complete the proof using the fact that\nw∗ = 1\nλ Uγ∗, w̃ =\n1 λ U γ̂."
    }, {
      "heading" : "5.2. Proof of Theorem 3",
      "text" : "As indicated before, the key reason for the large difference between ŵ and w∗ is because they do not lie in the same subspace: w∗ lies in the subspace spanned by the columns in U while ŵ lies in the subspace spanned by the column vectors in a random matrix. Before presenting our analysis, we first state a version of John Linderstrauss theorem that is useful to our analysis.\nTheorem 8 (Theorem 2 (Blum, 2006)) Let x ∈ Rd, and x̂ = S⊤x/√m, where S ∈ Rd×m is a random matrix whose entries are chosen independently from N (0, 1). Then\nPr { (1− ǫ)‖x‖22 ≤ ‖x̂‖22 ≤ (1 + ǫ)‖x‖22 } ≥ 1− 2 exp ( −m\n4 (ǫ2 − ǫ3)\n)\nIn the subspace orthogonal to u1, . . . ,ur, we randomly choose a subset of d−r orthogonal bases, denoted by ur+1, . . . ,ud. Let U⊥ = (ur+1, . . . ,ud). Since\n‖w∗ − ŵ‖2 = max ‖x‖2≤1 x⊤(w∗ − ŵ),\nto facilitate our analysis, we restrict the choice of x to the subspace span(ur+1, . . . ,ud) and have\n‖w∗ − ŵ‖2 ≥ max x∈span(ur+1,...,ud),‖x‖2≤1 x⊤ŵ\nwhere we use the fact w∗ ⊥ span(ur+1, . . . ,ud). Write x as x = U⊥a, where a ∈ Rd−r. Define\nΛ = U⊤⊥S ∈ R(d−r)×m\nAs a result, we bound ‖w∗ − ŵ‖2 by\nmax x∈span(ur+1,...,um),‖x‖2≤1 x⊤ŵ = max ‖a‖2≤1\n1\nmλ a⊤U⊤⊥SS\n⊤U γ̂ = 1\nmλ ‖ΛA⊤γ̂‖2 (18)\nwhere γ̂ is given by γ̂ = ΣV ⊤D(y)α̂\nIt is easy to verify that A and Λ are two independent Gaussian random matrices. Therefore, we can fix the vector A⊤γ̂ and estimate how the random matrix Λ affect the norm of vector A⊤γ̂. According to the John Linderstrauss Theorem (i.e. Theorem 8), for a fixed vector A⊤γ̂, with a probability 1− exp(−(ǫ2 − ǫ3)(d− r)/4), we have\n1√ d− r\n‖ΛA⊤γ̂‖2 ≥ √ 1− ǫ‖A⊤γ̂‖2\nBy choosing ε = 1/2, we have, with a probability 1− exp(−(d− r)/32),\n1√ d− r ‖ΛA⊤γ̂‖2 ≥ 1√ 2 ‖A⊤γ̂‖2 (19)\nWe now bound ‖A⊤γ̂‖2. Note that we cannot directly apply the John Linderstrauss Theorem to bound the length of A⊤γ̂ because γ̂ is a random variable depending on the random matrix A. To decouple the dependence between A and γ̂, we expand ‖A⊤γ̂‖2 as\n‖A⊤γ̂‖2 ≥ ‖A⊤γ∗‖2 − ‖A⊤(γ∗ − γ̂)‖2 (20)\nwhere γ∗ = ΣV ⊤D(y)α∗\nWe bound the two terms on the right side of the inequality in (20) separately. Using the John Linderstrauss Theorem, with a probability 1− exp(−m/32), we bound ‖A⊤γ∗‖ by\n1√ m ‖A⊤γ∗‖2 ≥ 1√ 2 ‖γ∗‖2 = λ√ 2 ‖w∗‖2 (21)\nTo bound the second term ‖A⊤(γ∗ − γ̂)‖, with a probability 1− δ, we have\n1√ m ‖A⊤(γ∗ − γ̂)‖2 ≤\n√ λmax(AA⊤/m)‖γ∗ − γ̂‖2 ≤ √ 1 + ελ‖w∗ − w̃‖2\nwhere we use the result in Corollary 6. According to Theorem 2, with a probability 1− δ, we have\n‖w∗ − w̃‖2 ≤ 2ε\n1− ε‖w∗‖2\nAs a result, with probability 1− δ, we have 1√ m ‖A⊤(γ∗ − γ̂)‖2 ≤ λ √ 1 + ε 2ε 1− ε‖w∗‖2 (22)\nWe complete the proof by putting together (18), (19), (20), (21), and (22)."
    }, {
      "heading" : "5.3. Proof of Theorem 5",
      "text" : "Given a solution w̃t obtained at iteration t, we consider the following optimization problem\nmin w∈Rd\nLt(w;X,y) = λ\n2 ‖w + w̃t‖22 +\nn∑\ni=1\nℓ(yi(w + w̃t) ⊤xi) (23)\nIt is straightforward to show that ∆t+1∗ = w∗− w̃t is the optimal solution to (23). Then we can use the dual random projection approach to recover ∆t+1∗ by ∆̃t+1. If we can similarly show that\n‖∆̃t+1 −∆t+1∗ ‖2 ≤ 2ε\n1− ε‖∆ t+1 ∗ ‖2\nthen we define the updated recovered solution by w̃t+1 = w̃t + ∆̃t+1 and have\n‖w̃t+1 −w∗‖2 ≤ 2ε\n1− ε‖∆ t+1 ∗ ‖2 =\n2ε\n1− ε‖w̃t −w∗‖2\nContinously, if we repeat the above process for t = 1, . . . , T , the recovery error of w̃T is given by\n‖w̃T −w∗‖2 ≤ ( 2ε\n1− ε\n)T−1 ‖w̃1 −w∗‖2 ≤ ( 2ε\n1− ε\n)T ‖w∗‖2\nThe remaining question is how to compute the ∆̃t+1 using the dual random projection approach. In order to make the previous analysis remain valid for the recovered solution ∆̃t+1 to the problem (23), we need to write the primal optimization problem in the same\nform as in (1). To this end, we first note that w̃t lies in the subspace spanned by x1, . . . ,xn, thus we write w̃t as\nw̃t = − 1\nλ\nn∑\ni=1\n[ãt]iyixi\nThus, Lt(w;X,y) can be written as\nLt(w;X,y) = λ\n2 ‖w̃t‖22 +\nλ 2 ‖w‖22 + λw⊤w̃t +\nn∑\ni=1\nℓ(yiw ⊤xi + yiw̃ ⊤ t xi)\n= λ\n2 ‖w̃t‖22 +\nλ 2 ‖w‖22 +\nn∑\ni=1\nℓ(yiw ⊤xi + yiw̃ ⊤ t xi)− [ãt]iyiw⊤xi\n= λ\n2 ‖w̃t‖22 +\nλ 2 ‖w‖22 +\nn∑\ni=1\nℓit(yiw ⊤xi)\nwhere the new loss function ℓit(z), i = 1, . . . , n is defined as\nℓit(z) = ℓ(z + yiw̃ ⊤ t xi)− [ãt]iz (24)\nTherefore ∆t+1∗ is the solution to the following problem\n∆t+1∗ = arg min w∈Rd\nλ 2 ‖w‖22 +\nn∑\ni=1\nℓit(yiw ⊤xi)\nTo apply the dual random projection approach to recover ∆t+1∗ , we solve the following optimization problem in the projected space:\nmin z∈Rm\nλ 2 ‖z‖22 +\nn∑\ni=1\nℓit(yiz ⊤x̂i)\nThe following derivation signifies that the above problem is equivalent to the problem in (13).\nminz∈Rm λ\n2 ‖z‖22 +\nn∑\ni=1\nℓit(yiz ⊤x̂i)\n= λ\n2 ‖z‖22 +\nn∑\ni=1\nℓ(yiz ⊤x̂i + yiw̃ ⊤ t xi)− [ãt]iyiz⊤x̂i\n= λ\n2 ‖z‖22 + λ√ m z⊤(S⊤w̃t) +\nn∑\ni=1\nℓ(yiz ⊤x̂i + yiw̃ ⊤ t xi)\n= λ\n2 ‖z+ S⊤w̃t/\n√ m‖22 +\nn∑\ni=1\nℓ(yiz ⊤x̂i + yiw̃ ⊤ t xi)−\nλ 2 ‖S⊤w̃t/\n√ m‖2\nwhere we use w̃t = − ∑ i[ã t]iyixi. Given the optimal solution z t+1 ∗ to the above problem, we can recover ∆t+1∗ by\n∆̃t+1 = − 1\nλ XD(y)α̂t+1\nwhere α̂t+1 is computed by\n[α̂t+1]i = ∇ℓi(yix̂⊤zt∗) = ∇ℓ(yix̂⊤zt∗ + yiw̃⊤t xi)− [ãt]i The updated solution w̃t+1 is computed by\nw̃t+1 = w̃t + ∆̃t = − 1\nλ\nT∑\ni=1\n[ãt+1]ixiyi\nwhere [ãt+1]i = [α̂t+1]i + [ã t]i = ∇ℓ(yix̂⊤zt∗ + yiw̃⊤t xi)."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we discuss the problem of recovering optimal solutions through random projection. Our goal is to first efficiently obtain an approximate solution z∗ for a given optimization problem by using random projection and then reconstruct the true optimal solution w∗ from the random projection based solution z∗. We developed a dual random projection approach and show that under the assumption that the data matrix X is of low rank, the proposed approach is able to accurately recover the true optimal solution w∗ with a small error.\nThere are several open questions that need to be addressed in the future. The first open question is to analyze the behavior of the proposed algorithm when X can be well approximated by a low rank matrix, an assumption that is significantly weaker than the low rank assumption. The second open question is to develop a parallel version of the proposed algorithm by running it independently over multiple machines. The challenge is to design an effective approach for combining multiple sets of random projection based solutions into one solution with significantly smaller recovering error. This is an important question when we need to adapt the proposed algorithm to a distributed computing environment."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank a bunch of people."
    }, {
      "heading" : "Appendix A. Proof of Proposition 1 and Proposition 3",
      "text" : "Since the two propositions can be proved similarly, we only present the proof of Proposition 1. First if α∗ is the optimal dual solution, the optimal primal solution can be solved by\nw∗ = arg min w∈Rd\nλ 2 ‖w‖22 +\nn∑\ni=1\n[α∗]iyix ⊤ i\nBy setting the gradient with respect to w to zero, we obtain w∗ = − ∑n\ni=1[α]iyixi/λ = −XD(y)α/λ\nSecond, to prove the dual solution α∗ given the primal solution w∗, we note that\nℓ(yix ⊤ i w∗) = [α∗]i(yix ⊤ i w∗)− ℓ∗([α]i\nBy the Fenchel conjugate theory (e.g., Theorem 11.4 in(Cesa-Bianchi and Lugosi, 2006)) we have α satisfying\n[α∗]i = ∇ℓ(yixiw∗)."
    }, {
      "heading" : "Appendix B. Proof of Corollary 6",
      "text" : "In the proof, we make use of the following concentration inequality regarding the eigenvalues of Gaussian random matrix.\nTheorem 9 (Corollary 7.2 (Gittens and Tropp, 2011)) Let C ∈ Rp×p be a positive definite matrix. Let ηj ∈ Rp, j = 1, . . . , n be i.i.d. samples drawn from a N (0, C) distribution. Define\nĈn = 1\nn\nn∑\nj=1\nηjη ⊤ j .\nWrite λk for the kth eigenvalue of C, and write λ̂k for the kth eigenvalue of Ĉn. Then, for k = 1, . . . , p,\nPr { λ̂k ≥ (1 + ε)λk } ≤ (p− k + 1) exp ( − cnε 2\n∑p i=k λi/λk\n) , for ε ≤ 4n\nand\nPr { λ̂k ≤ (1− ε)λk } ≤ k exp ( − cnε 2\n∑k i=1 λ1λi/λ 2 k\n) , for ε ∈ (0, 1]\nwhere constant c is at least 1/32.\nWe write M = (η1, . . . ,ηm), where ηi ∈ Rr is i.i.d sample from a Gaussian distribution N (0, I) and write MM⊤/m as\nCm = 1\nm MM⊤ =\n1\nm\nm∑\ni=1\nηiη ⊤ i\nUsing Theorem 9, we have,\n1− ε ≤ λk(Cm) ≤ 1 + ε, k = 1, . . . , r\nwith the failure probability at most r∑\nk=1\n(r − k + 1) exp ( −cmε 2\nr\n) + k exp ( −cmε 2\nr\n) = (r2 + r) exp ( −cmε 2\nr\n)\nWe complete proof by setting the above failure probability to be less than δ."
    }, {
      "heading" : "Appendix C. Proof of Theorem 4",
      "text" : "We write ŵ in terms of w̃ as ŵ = SS⊤w̃/m and therefore\nmax ‖x‖2≤1,x∈span(X) x⊤(w∗ − ŵ) ≤ ‖w∗ − w̃‖2 + max ‖x‖2≤1,x∈span(X) x⊤(w̃ − ŵ)\n= ‖w∗ − w̃‖2 + max ‖a‖2≤1\na⊤ ( I − 1\nm U⊤SS⊤U\n) γ̂/λ\n≤ ‖w∗ − w̃‖2 + λmax ( I − 1\nm U⊤SS⊤U\n) ‖w̃‖2\n≤ ‖w∗ − w̃‖2 + λmax ( I − 1\nm AA⊤\n) ‖w∗‖\nThe last but one inequality uses the fact ‖w̃‖2 = ‖γ̂‖2/λ. Using Corollary 6, we have, with a probability 1− δ,\nλmax\n( I − 1\nm AA⊤\n) ≤ ε\nWe complete the proof by using the bound for ‖w∗ − w̃‖2 stated in Theorem 2."
    }, {
      "heading" : "Appendix D. Proof of Theorem 1",
      "text" : "According to (17) in the proof of Theorem 2, we have\n2(γ̂ − γ∗)⊤ ( I − AA ⊤\nm\n) γ∗ ≥ (γ̂ − γ∗)⊤ AA⊤\nm (γ̂ − γ∗)\nUsing the fact √ mz∗ = A ⊤ γ̂/λ and S⊤w∗ = A ⊤ γ∗/λ, we have\nλ2 m ‖√mŵ′ − S⊤w∗‖22 ≤ 2(γ̂ − γ∗)⊤\n( I − AA ⊤\nm\n) γ∗\nUsing Corollary 6, with a probability 1− δ, we have 1\nm ‖√mz∗ − S⊤w∗‖22 ≤ 2ε‖w∗‖2‖w̃ −w∗‖2\nUsing Theorem 2, with a probability 1− δ, we have 1\nm ‖√mz∗ − S⊤w∗‖22 ≤ 4ε2 1− ε‖w∗‖ 2 2 (25)\nTo replace w∗ on R. H. S. of the above inequality with S ⊤w∗, we make use of Theorem 8. As a result, with a probability 1− exp(−m/32), we have 1\nm ‖S⊤w∗‖22 ≥\n1 2 ‖w∗‖22 (26)\nWe complete the proof by combining the two inequalities in (25) and (26)."
    } ],
    "references" : [ {
      "title" : "Database-friendly random projections: Johnson-lindenstrauss with binary coins",
      "author" : [ "Dimitris Achlioptas" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Achlioptas.,? \\Q2003\\E",
      "shortCiteRegEx" : "Achlioptas.",
      "year" : 2003
    }, {
      "title" : "An algorithmic theory of learning: robust concepts and random projection",
      "author" : [ "Rosa I. Arriaga", "Santosh Vempala" ],
      "venue" : "In Proceedings of the 40th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Arriaga and Vempala.,? \\Q1999\\E",
      "shortCiteRegEx" : "Arriaga and Vempala.",
      "year" : 1999
    }, {
      "title" : "A pac-style model for learning from labeled and unlabeled data",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum" ],
      "venue" : "In Proceedings of the 18th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Balcan and Blum.,? \\Q2005\\E",
      "shortCiteRegEx" : "Balcan and Blum.",
      "year" : 2005
    }, {
      "title" : "Kernels as features: On kernels, margins, and low-dimensional mappings",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2006
    }, {
      "title" : "Random projection in dimensionality reduction: applications to image and text data",
      "author" : [ "Ella Bingham", "Heikki Mannila" ],
      "venue" : "In Proceedings of the 7th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Bingham and Mannila.,? \\Q2001\\E",
      "shortCiteRegEx" : "Bingham and Mannila.",
      "year" : 2001
    }, {
      "title" : "Random projection, margins, kernels, and feature-selection",
      "author" : [ "Avrim Blum" ],
      "venue" : "In Proceedings of the 2005 international conference on Subspace, Latent Structure and Feature Selection,",
      "citeRegEx" : "Blum.,? \\Q2006\\E",
      "shortCiteRegEx" : "Blum.",
      "year" : 2006
    }, {
      "title" : "Random projections for kmeans clustering",
      "author" : [ "Christos Boutsidis", "Anastasios Zouzias", "Petros Drineas" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2010
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Rademacher chaos, random eulerian graphs and the sparse johnson-lindenstrauss transform",
      "author" : [ "V. Braverman", "R. Ostrovsky", "Y. Rabani" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Braverman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Braverman et al\\.",
      "year" : 2010
    }, {
      "title" : "An introduction to compressive sampling",
      "author" : [ "Emmanuel J. Candès", "Michael B. Wakin" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Candès and Wakin.,? \\Q2008\\E",
      "shortCiteRegEx" : "Candès and Wakin.",
      "year" : 2008
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gabor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Random projection trees and low dimensional manifolds",
      "author" : [ "Sanjoy Dasgupta", "Yoav Freund" ],
      "venue" : "In Proceedings of the 40th annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Dasgupta and Freund.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dasgupta and Freund.",
      "year" : 2008
    }, {
      "title" : "Compressed sensing",
      "author" : [ "David L. Donoho" ],
      "venue" : "IEEE Transaction on Information Theory,",
      "citeRegEx" : "Donoho.,? \\Q2006\\E",
      "shortCiteRegEx" : "Donoho.",
      "year" : 2006
    }, {
      "title" : "Relative-error cur matrix decompositions",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2008
    }, {
      "title" : "Random projection for high dimensional data clustering: a cluster ensemble approach",
      "author" : [ "Xiaoli Zhang Fern", "Carla E. Brodley" ],
      "venue" : "In Proceedings of the 20th International Conference on Machine Learning,",
      "citeRegEx" : "Fern and Brodley.,? \\Q2003\\E",
      "shortCiteRegEx" : "Fern and Brodley.",
      "year" : 2003
    }, {
      "title" : "Experiments with random projections for machine learning",
      "author" : [ "Dmitriy Fradkin", "David Madigan" ],
      "venue" : "In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Fradkin and Madigan.,? \\Q2003\\E",
      "shortCiteRegEx" : "Fradkin and Madigan.",
      "year" : 2003
    }, {
      "title" : "Learning the structure of manifolds using random projections",
      "author" : [ "Yoav Freund", "Sanjoy Dasgupta", "Mayank Kabra", "Nakul Verma" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Freund et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 2008
    }, {
      "title" : "Tail bounds for all eigenvalues of a sum of random matrices",
      "author" : [ "Alex Gittens", "Joel A. Tropp" ],
      "venue" : "CoRR, abs/1104.4513v2,",
      "citeRegEx" : "Gittens and Tropp.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gittens and Tropp.",
      "year" : 2011
    }, {
      "title" : "Face recognition experiments with random projection",
      "author" : [ "Navin Goel", "George Bebis", "Ara Nefian" ],
      "venue" : "In Proceedings of SPIE,",
      "citeRegEx" : "Goel et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Goel et al\\.",
      "year" : 2005
    }, {
      "title" : "An introduction to variable and feature selection",
      "author" : [ "Isabelle Guyon", "André Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Guyon and Elisseeff.,? \\Q2003\\E",
      "shortCiteRegEx" : "Guyon and Elisseeff.",
      "year" : 2003
    }, {
      "title" : "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "In Proceedings of the 24th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Hazan and Kale.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2011
    }, {
      "title" : "Beating sgd: Learning svms in sublinear time",
      "author" : [ "Elad Hazan", "Tomer Koren", "Nati Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hazan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2011
    }, {
      "title" : "Dimensionality reduction by random mapping: fast similarity computation for clustering",
      "author" : [ "Samuel Kaski" ],
      "venue" : "In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks,",
      "citeRegEx" : "Kaski.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kaski.",
      "year" : 1998
    }, {
      "title" : "Dense fast random projections and lean walsh transforms",
      "author" : [ "Edo Liberty", "Nir Ailon", "Amit Singer" ],
      "venue" : "Proceedings of the 12th International Workshop on Randomization and Computation (RANDOM),",
      "citeRegEx" : "Liberty et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Liberty et al\\.",
      "year" : 2008
    }, {
      "title" : "Linear regression with random projections",
      "author" : [ "Oldalric-Ambrym Maillard", "Remi Munos" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Maillard and Munos.,? \\Q2012\\E",
      "shortCiteRegEx" : "Maillard and Munos.",
      "year" : 2012
    }, {
      "title" : "Randomized Algorithms",
      "author" : [ "Rajeev Mowani", "Prabhakar Raghavan" ],
      "venue" : null,
      "citeRegEx" : "Mowani and Raghavan.,? \\Q1995\\E",
      "shortCiteRegEx" : "Mowani and Raghavan.",
      "year" : 1995
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nesterov.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2005
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "Ali Rahimi", "Benjamin Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Rahimi and Recht.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rahimi and Recht.",
      "year" : 2008
    }, {
      "title" : "Online learning meets optimization in the dual",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer" ],
      "venue" : "In Proceedings of 19th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Shalev.Shwartz and Singer.,? \\Q2006\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Singer.",
      "year" : 2006
    }, {
      "title" : "Is margin preserved after random projection",
      "author" : [ "Qinfeng Shi", "Chunhua Shen", "Rhys Hill", "Anton van den Hengel" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Shi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2012
    }, {
      "title" : "The Random Projection Method",
      "author" : [ "Santosh S. Vempala" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "Vempala.,? \\Q2004\\E",
      "shortCiteRegEx" : "Vempala.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.",
      "startOffset" : 109,
      "endOffset" : 237
    }, {
      "referenceID" : 30,
      "context" : "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.",
      "startOffset" : 109,
      "endOffset" : 237
    }, {
      "referenceID" : 15,
      "context" : "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.",
      "startOffset" : 109,
      "endOffset" : 237
    }, {
      "referenceID" : 2,
      "context" : "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.",
      "startOffset" : 109,
      "endOffset" : 237
    }, {
      "referenceID" : 5,
      "context" : "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.",
      "startOffset" : 109,
      "endOffset" : 237
    }, {
      "referenceID" : 27,
      "context" : "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.",
      "startOffset" : 109,
      "endOffset" : 237
    }, {
      "referenceID" : 24,
      "context" : "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al.",
      "startOffset" : 250,
      "endOffset" : 298
    }, {
      "referenceID" : 13,
      "context" : "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al.",
      "startOffset" : 250,
      "endOffset" : 298
    }, {
      "referenceID" : 22,
      "context" : ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.",
      "startOffset" : 20,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.",
      "startOffset" : 20,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.",
      "startOffset" : 20,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.",
      "startOffset" : 34,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.",
      "startOffset" : 34,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008), and information retrieval (Goel et al.",
      "startOffset" : 94,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008), and information retrieval (Goel et al.",
      "startOffset" : 94,
      "endOffset" : 142
    }, {
      "referenceID" : 18,
      "context" : ", 2008), and information retrieval (Goel et al., 2005).",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : "This is particularly useful for feature selection (Guyon and Elisseeff, 2003), where important features are often selected based on their weights in the linear prediction model learned from the training data.",
      "startOffset" : 50,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : "For non differentiable loss functions such as hinge loss, we could apply the smoothing technique (Nesterov, 2005) to make it differentiable.",
      "startOffset" : 97,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).",
      "startOffset" : 0,
      "endOffset" : 92
    }, {
      "referenceID" : 29,
      "context" : "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).",
      "startOffset" : 0,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).",
      "startOffset" : 0,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).",
      "startOffset" : 0,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "Relationship to Compression Sensing The proposed problem is closely related to compressive sensing (Candès and Wakin, 2008; Donoho, 2006) where the goal is to recover a high dimensional but sparse vector using a small number of random projections.",
      "startOffset" : 99,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "Relationship to Compression Sensing The proposed problem is closely related to compressive sensing (Candès and Wakin, 2008; Donoho, 2006) where the goal is to recover a high dimensional but sparse vector using a small number of random projections.",
      "startOffset" : 99,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "We note that although dual variables have been widely used in the analysis of convex optimization (Boyd and Vandenberghe, 2004; Hazan et al., 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.",
      "startOffset" : 98,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "We note that although dual variables have been widely used in the analysis of convex optimization (Boyd and Vandenberghe, 2004; Hazan et al., 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.",
      "startOffset" : 98,
      "endOffset" : 147
    }, {
      "referenceID" : 28,
      "context" : ", 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.",
      "startOffset" : 28,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 66
    }, {
      "referenceID" : 23,
      "context" : ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : "We also note that Algorithm 2 is related to the Epoch gradient descent algorithm (Hazan and Kale, 2011) for stochastic optimization in that the solution obtained in the previous iteration is served as the center to the optimization problem of the current iteration.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : "Unlike the algorithm in (Hazan and Kale, 2011), we do not shrink the domain size over the iterations in Algorithm 2.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "Following the same arguments as compressive sensing, it may be possible to argue that Ω(r log r) is optimal due to the result of coupon collector’s problem (Mowani and Raghavan, 1995), although the rigorous analysis remains to be developed.",
      "startOffset" : 156,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "We note that Theorem 4 directly implies the result of margin classification error for random projection (Blum, 2006).",
      "startOffset" : 104,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "Theorem 8 (Theorem 2 (Blum, 2006)) Let x ∈ R, and x̂ = S⊤x/√m, where S ∈ Rd×m is a random matrix whose entries are chosen independently from N (0, 1).",
      "startOffset" : 21,
      "endOffset" : 33
    } ],
    "year" : 2017,
    "abstractText" : "In this work, we address the problem of how to recover the optimal solution to the optimization problem related to high dimensional data classification using random projection, to which we refer as Recovery of Optimal Solution. This is in contrast to the previous studies that were focused on analyzing the classification performance using random projection. We reveal the relationship between compressive sensing and the problem of recovering optimal solution using random projection. We also present a simple algorithm, termed as Dual Random Projection, that recovers the optimal solution with a small error by computing dual solution provided that the data matrix is of low rank.",
    "creator" : "LaTeX with hyperref package"
  }
}