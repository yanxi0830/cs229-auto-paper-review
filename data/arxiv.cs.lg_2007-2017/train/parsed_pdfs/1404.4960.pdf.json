{
  "name" : "1404.4960.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Agent Behavior Prediction and Its Generalization Analysis",
    "authors" : [ "Fei Tian", "Haifang Li", "Wei Chen", "Tao Qin", "Enhong Chen", "Tie-Yan Liu" ],
    "emails" : [ "tianfei@mail.ustc.edu.cn", "lihaifang@amss.ac.cn", "wche@microsoft.com", "taoqin@microsoft.com", "cheneh@ustc.edu.cn", "tyliu@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 4.\n49 60\nv1 [\ncs .L\nG ]\n1 9\nA pr\nMachine learning algorithms have been applied to predict agent behaviors in real-world dynamic systems, such as advertiser behaviors in sponsored search and worker behaviors in crowdsourcing, and the prediction models have been used for the optimization of these systems. The behavior data in these systems are generated by live agents: once the systems change due to the adoption of the prediction models learnt from the behavior data, agents will observe and respond to these changes by changing their own behaviors accordingly. As a result, the behavior data will evolve and will not be identically and independently distributed, posing great challenges to the theoretical analysis on the machine learning algorithms for behavior prediction. To tackle this challenge, in this paper, we propose to use Markov Chain in Random Environments (MCRE) to describe the behavior data, and perform generalization analysis of the machine learning algorithms on its basis. Since the one-step transition probability matrix of MCRE depends on both previous states and the random environment, conventional techniques for generalization analysis cannot be directly applied. To address this issue, we propose a novel technique that transforms the original MCRE into a higher-dimensional time-homogeneous Markov chain. The new Markov chain involves more variables but is more regular, and thus easier to deal with. We prove the convergence of the new Markov chain when time approaches infinity. Then we prove a generalization bound for the machine learning algorithms on the behavior data generated by the new Markov chain, which depends on both the Markovian parameters and the covering number of the function class compounded by the loss function for behavior prediction and the behavior prediction model. To the best of our knowledge, this is the first work that performs the generalization analysis on data generated by complex processes in real-world dynamic systems.\n∗This work was done when the first two authors were visiting Microsoft Research Asia. Copyright c© 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
    }, {
      "heading" : "1 Introduction",
      "text" : "In this Internet era, more and more data are generated by self-interested agents in interactive systems. For example, in sponsored search, advertisers generate a large volume of bidding log data in their daily competitions with each other in attracting search users to click their ads; in crowdsourcing, workers generate a lot of behavior data when competing with other workers in getting tasks from the employers, and when completing the tasks assigned to them.\nIn many real cases including the aforementioned ones, there are three kinds of players in the systems: platform, users, and self-interested agents. Platform is the owner of the system, who designs the mechanism of the system and takes care of its execution. Users arrive at the platform in random, with their particular needs to be fulfilled. Agents behave strategically in order to attract the attention of the users so as to realize their own utilities. Taking both user needs and agent behaviors into consideration, the platform matches users with agents, extracts revenue from this procedure, and gives agents feedback on their performances (which depend on both the behaviors of agents and randomness in users). Upon the feedback, agents will adjust their behaviors in order to be better off in the future. To design a good mechanism, it is very important for the platform to understand and predict agent behaviors. With accurate prediction of agent behaviors, the platform can also provide tools to help agents to optimize their performances and therefore attract more agents to the system. Thus, predicting agent behaviors is an important task for the platform. For ease of reference, we call the problem of predicting agent behaviors in an interactive system “agent behavior prediction (ABP)”."
    }, {
      "heading" : "1.1 Examples of ABP",
      "text" : "Here we take sponsored seaerch systems as an example for illustration.\nIn a sponsored search system, platform, users, and agents correspond to the search engine, search users, and advertisers respectively. Advertiser behaviors are the bid prices on their ads. When a search user issues a query, the search engine will run a GSP auction (Edelman, Ostrovsky, and Schwarz 2005) among all advertisers who bid on the query, rank their ads according to the\nproduct of the bid price and predicted click-through rate, and then charge the winning advertisers if the user clicks on their ads. After a period of time, the search engine will provide feedback to the advertisers about their performances (which we usually call Key Performance Indicators, or KPIs for short). The KPIs usually contain the numbers of impressions and clicks, the average rank positions, and the costs per click of their ads. Many advertisers will adjust their bid prices based on the feedback they receive, either by themselves or with the help of third-party search engine marketing companies. By logging the bid prices in a sufficiently long period of time, the search engine can predict how advertisers behave, and consequently enhance its click prediction algorithm and auction mechanism.\nIn crowdsourcing systems, platform, users, and agents correspond to crowdsourcing platform (e.g., Amazon Mechanical Turk), employers, and workers respectively. A worker’ behaviors usually include the claimed profile and expertise, the committed available time slots, the minimum payment requirement, and the quality of fulfilling a task. When an employer submits his/her task, the crowdsourcing platform will select workers according to some criteria, and then dispatch the tasks to them. After the workers finish the tasks, the employer will pay certain amount of money, part of which goes to the platform as commission and the rest goes to the workers. The employer will also rate the quality of the task completion. After a period of time, the platform will provide workers with necessary feedback, which usually includes the number of task assigned to them, the average ratings they receive, the total payments they obtain, etc. Some workers will adjust their behaviors based on the feedback in hope to be assigned more tasks and to get better rating/payment. With the logs of the behaviors of the workers, the platform can predict how workers behave, and enhance the worker engagement tool and task assignment mechanism on its basis.\nIn app stores, platform, users and agents correspond to app store, app users, and app developers respectively. The behaviors of the app developers include creating, describing, upgrading, and pricing their apps. When an app user searches for apps with certain functionalities, the app store will recommend a ranked list of related apps according to their claimed functionalities, popularities, reviews and ratings, prices, etc. The user might choose one or several apps from the list to install. For paid apps, the user needs to pay a certain amount of money, part of which goes to the app store as its commission, and the rest goes to the app developers. After using these apps, the user may comment on the quality of the apps by submitting reviews and ratings. After a period of time, the app store will provide app developers with feedback, which usually includes the number of users who view their apps, the number of users who purchase and install their apps, the average rating and the number of reviews of their apps, and the total revenue they obtained from the installations of their apps. Many app developers will upgrade their apps (e.g., adjust the functionalities, descriptions, and prices) according to the feedback in hope to get a better performance in the future. With the logs of the behaviors of app developers, the app store can predict how develop-\ners behave and enhance its recommendation algorithm, and revenue sharing mechanism on its basis."
    }, {
      "heading" : "1.2 Generalization Analysis for ABP",
      "text" : "Because of its importance, ABP has been studied in many works, including (Cary et al. 2007; Pin and Key. 2011; Zhou and Lukose. 2007; Xu et al. 2013; He et al. 2013). Some of them (Xu et al. 2013; He et al. 2013) have adopted machine learning techniques and attempted to learn an agent behavior model by means of empirical risk minimization (ERM) on the behavior logs. Empirical results have shown that these machine learning techniques can significantly outperform previous non-learning approaches. However, despite the experimental success, it still remains an open question whether the use of ERM algorithms in behavior prediction is theoretically sound, and whether certain generalization ability of such algorithms can be guaranteed.\nAs far as we know, the answers to the above questions are unclear yet. This is mainly because of the complication of the corresponding theoretical analysis. As aforementioned, the behavior data are generated by self-interested agents, and dependent on both their previous behaviors and the random user factors in the system. As a result, the behavior data have quite complex statistical properties, and the generalization analysis in such a setting goes beyond the state of the art of statistical learning theory (Vapnik 1998; Devroye 1996; Yu 1994)."
    }, {
      "heading" : "1.3 Related Work",
      "text" : "There have been extensive game-theoretic models on advertisers’ bidding prediction (Cary et al. 2007)(Chakrabarty, Zhou, and Lukose. 2007)(Zhou and Lukose. 2007). These models generally assume advertisers are fully rational and have full information access. This assumption is far beyond reality and thus recently machine learning methods (Cui et al. 2011)(Xu et al. 2013)(He et al. 2013), based on minimizing prediction loss on advertisers’ historical bidding data, have been employed in this task. For example, in (Xu et al. 2013), the authors propose models to describe different levels of advertisers’ rationalities and fit the model parameters by learning from bidding data. The work most relevant to ours is (He et al. 2013), in which a Markov bidding model is introduced and a linear prediction function is learnt using Maximum Likelihood Estimation. However, whether these machine learning algorithms enjoy generalization ability remains an open theoretical problem since historical behavioral data are non i.i.d.\nPreviously machine learning researchers have established several theories for learning from non i.i.d data (Yu 1994)(Vidyasagar 2003)(Mohri and Rostamizadeh 2007). In (Yu 1994) and (Zou, Li, and Xu 2009), the authors establish uniform convergence bounds for stationary process and strongly mixing sequence. There are also theories specially built for learning under Markov chains such as (Zou, Li, and Xu 2009)(Zhang and Tao 2012), however they are not qualified to answer our question because they do not cover the case of learning under MCRE, a special Markov chain with time-variant transition probabilities.\nThere are also works on MCRE in statistics literature such as (Cogburn 1980)(Cogburn 1984) and (Cogburn 1991), but they mainly focus on statistical properties of MCRE such as ergodicity and central limit theorem. To the best of our knowledge there has been no previous works on generalization analysis of learning on data from MCRE."
    }, {
      "heading" : "1.4 Our Results",
      "text" : "In order to analyze the ERM algorithms on agent behavior prediction, we propose a set of new techniques in this paper.\nFirst, we model the generation process of the behavior data using a so-called Markov Chain in Random Environments (MCRE), whose transition matrix is time-variant (depending on the random environments). Take sponsored search as an example. After the current-round auction, the advertiser observes his/her KPIs, which depend on both the bids of all the advertisers and the random clicks of the users. Based on the KPIs, the advertiser will determine how to set his/her own bid for the next round of auction and for different KPI values, the conditional distribution of his/her bid at the next round will be different. In this sense, the sequence of advertiser bids can be regarded as an MCRE.\nSecond, considering that it is difficult to perform generalization analysis on MCRE, we propose an equivalence transformation that maps the original MCRE to a higher dimensional time-homogenous Markov chain. Although the new Markov chain involves more variables, it is more regular and thus easier to deal with from the perspective of generalization analysis. We prove that the new Markov chain will converge when time approaches infinity and a Hoeffding-style inequality holds for the empirical process associated with it.\nThird, by exploiting the covering number technique, we derive a uniform convergence bound for the ERM algorithms on the data generated by the new Markov chain (and thus by the original MCRE due to the equivalence transformation). As a consequence, we prove that the ERM algorithms on the data generated by MCRE have their theoretical guarantees, which explains their good empirical performances reported in previous work. To the best of our knowledge, this is the first work in the literature that performs formal generalization analysis on the agent behavior prediction problem."
    }, {
      "heading" : "2 Agent Behavior Prediction",
      "text" : "In this section we give a formal description of the Agent Behavior Prediction (ABP) problem. We first show that the generation process for the behaviors of self-interested agents can be described by a Markov Chain in Random Environments (MCRE), and then formulate ABP as an optimization problem."
    }, {
      "heading" : "2.1 Agent Behaviors: Markov Chain in Random Environments",
      "text" : "The dynamic interactive systems mentioned in the introduction share some common properties. (1) The behaviors of an agent only depend on a finite number of his/her historical actions due to the limited memory of human being. In other words, the behaviors have Markovian properties.\n(2)Random users behaviors are independent and identically distributed (i.i.d), for example, in sponsor search, there are two aspects associated with users: queries issued by users, and users click patterns on ad ranking lists. It is clear that queries can be regarded as i.i.d. random variables. Click patterns are defined with respect to all possible ad ranking lists and they are also independent of agent behaviors(which only determine the selected ranking list), and can be regarded as i.i.d. (3) The behavior change of an agent is mainly affected by the feedback given by the platform. Since the feedback depends on the users randomly arriving at the system, the behavior change is not governed by a constant rule, but instead by some random factors.\nTaking both all the three properties into consideration, we can regard agent behaviors as generated by a Markov Chain in Random Environments (MCRE)(Cogburn 1980). Note that this generation process is much more complicated than a simple i.i.d. sampling process or a time-homogeneous Markov process.\nBefore formally describing the MCRE process for agent behaviors, we make some assumptions. First, we assume that both the behavior space and feedback space are finite. This assumption is reasonable, since in many applications the behaviors of the agents are either characterized by categorical profiles (e.g., expertise and functionalities) or bounded and associated with a minimum unit (e.g., bidding price, payment requirements, and available time slots). The same reason holds for the finiteness of the feedback space, since feedback usually takes discrete values (e.g., number of clicks, ratings, and number of reviews) as well. Second, we assume there is a deterministic function that generates the feedback for a given agent i based on the behaviors of all the agents and the random arrival of the users. This assumption is also reasonable because the feedback is usually provided by the platform using a deterministic algorithm.\nWith the above assumptions, now let us describe the generation process of the agent behavior according to (He et al. 2013). Suppose there are N agents. Let B and H be the behavior space and feedback space of a single agent respectively, and U be the random factor space induced by users. We use the mapping ηi : U×BN → H to denote the deterministic function that generates the feedback for agent i, and η : U×BN → HN to denote the joint feedback for all the agents. At the beginning of the (t +1)-th time period, agent i receives feedback hit = ηi(ut ,bt) about his/her behaviors in the t-th time period. Based on the feedback, agent i may change his/her behavior to bt+1i in order to be better off. That is, given the Markov property,\nP(bit+1|b1, ...,bt ;u1, ...,ut) = P(b i t+1|b i t ;h i t). (1)\nNote that the above equation implies the one-step Markov property, which is motivated here mainly due to ease of statement. Our analysis in this paper can be extended to higher order Markov chain as well, without too many modifications.\nGiven the feedback ht , all the agents change their behav-\niors independently, so we have\nP(bt+1|b1, ...,bt ;u1, ...,ut) = N\n∏ i=1 P(bit+1|b i t ;h i t) = P(bt+1|bt ,ht).\n(2) This indicates that {bt} is a MCRE (Cogburn 1980), where\nthe environmental process is {ht}. According to (2), the onestep transition matrix of an MCRE is time-variant and depends on the environmental variable."
    }, {
      "heading" : "2.2 Learning Agent Behavior Model",
      "text" : "There exist some related works that leverage the empirical risk minimization (ERM) framework to learn the agent behavior model. Mathematically speaking, given a training set containing the behaviors of agents and the feedback they received in T rounds, {(h1,b1) ;(h2,b2) ; ...;(hT ,bT )}, one aims to learn a function f : HN × BN → BN , which takes the behaviors and feedback at the current round as inputs and predicts the behavior at the next round. To this end, one minimizes the empirical risk on the training set: min f∈F ∑Tt=1 l ( f (ht ,bt) ;bt+1), where l measures the loss between the predicted behavior and the real behavior in the training data. For example, l can be the 0− 1 classification loss: l ( f (bt ,ht) ;bt+1) = 1 f (ht ,bt) 6=bt+1 , and can also be some surrogate loss functions.\nThis ERM framework covers the algorithms for predicting advertiser behaviors in many previous works including (He et al. 2013) and (Xu et al. 2013). For example, in (He et al. 2013), a truncated Gaussian function is used to model the Markov transition probabilities and a negative likelihood function is used as the loss function l. For another example, in (Xu et al. 2013), a compound function that considers the willingness, capability, and constraint of an advertiser is used as the model f , and again the negative likelihood of the historical behaviors is used as the loss function f .\nAs mentioned in the introduction, the ERM algorithms led to experimental success in the ABP tasks. However, it is unclear whether these algorithms have desired theoretical guarantee. In particular, it is unknown (1) qualitatively whether the ABP problem is learnable through an ERM process, and (2) quantitatively what is the relationship between generalization error and the size of the training data. The reason why the answers to these important questions are missing lies in that statistical learning theory mainly addresses learning problems with data generated by an i.i.d. sampling or a β -mixing Markov chain. This motivates us to formally study the learning theory with respect to the data generated by more complicated stochastic processes like MCRE."
    }, {
      "heading" : "3 Generalization Bounds for ABP",
      "text" : "In this section, we perform generalization analysis on the ERM algorithms for agent behavior prediction. Our main result is stated in Theorem (3.7). We prove the theorem in three steps: (1) constructing a new Markov chain of higher dimensionality but with more regular properties than the original MCRE; (2) proving the convergence of the empirical loss to the expected loss when the data are generated by this new Markov chain; (3) proving a uniform convergence bound by further leveraging the techniques of covering number. For\nease of reference, we use a Notation Table 1 in the end of the paper to summarize all new notations used in this section."
    }, {
      "heading" : "3.1 Constructing a Higher-Dimensional Markov Chain",
      "text" : "The difficulty of analyzing the ERM algorithms when the data are generated by an MCRE lies in its time-variant transition probabilities. To tackle the challenge, we construct a higher-dimensional chain M = {(ht ,bt ,bt+1) ;t ≥ 0}, by grouping correlated variables together. Let Mk(m,n) be onestep transition probability of {bt} from state m to n under the environment k. For convenience, we set zt = (ht ,bt ,bt+1) to be the t-th state of M. Since the state space HN ×B2N of the chain is finite, we label all the state values as 1,2...Z, i.e., M takes value in state space [Z] := {1,2, ...,Z}.\nThe following Lemma (3.1) and Theorem (3.2) show that the new Markov chain is time-homogeneous and has a stationary distribution under some mild assumptions.\nLemma 3.1 We assume the random factors caused by users ut are i.i.d., then the constructed Markov chain M = {(ht ,bt ,bt+1) ;t ≥ 0} is time-homogeneous.\nProof Since both feedback space and behavior space are finite, the set of their three dimensional Cartesian product values is also finite.\nTo show that the process is a time-homogeneous Markov chain, we consider any two states ( j, p,q),(k,m,n) ∈ HN × B 2N and write their one-step transition probability as:\nP(ht = k,bt = m,bt+1 = n|\n(hs,bs,bs+1) t−2 s=1; ht−1 = j,bt−1 = p,bt = q)\n=P(ht = k,bt = m,bt+1 = n|ht−1 = j,bt−1 = p,bt = q)\n=\n{\n0 for m 6= q P(u ∈ U : η(u,m) = k)Mk(m,n) for m = q\n(3) From (3), we know that the value of (ht ,bt ,bt+1) only depends on (ht−1,bt−1,bt). Therefore the process is a Markov chain. Furthermore, the one-step transition probability only depends on the states ( j, p,q),(k,m,n) and is independent of time index t, and therefore the transition probability is time-invariant.\nFor ease of statement, we will use M to denote the probability transition matrix in (3). As we can see from (3), lots of elements in M are zero. Therefore it is not straightforward to judge whether Markov chain M will converge or not. Theorem (3.2) shows that under some mild assumptions the convergence can be achieved.\nTheorem 3.2 The Markov chain M composed by {zt}Tt=1 := {ht ,bt ,bt+1}Tt=1 has a stationary distribution under the following assumptions: (A.1) for every fixed value of the feedback ht = k, the Markov process with transition matrix Mk is irreducible and aperiodic; (A.2) the feedback function η and the random user distribution satisfy ∀m∈B,k ∈H,P(u∈U : η(u,m) = k)> 0. 1\n1Condition (A.2), which seems not very intuitive, basically says\nTo prove this theorem, since the state of (ht ,bt ,bt+1) is finite and M is time-homogeneous, we only need to prove that Markov chain M is irreducible and aperiodic, which is shown in the following two lemmas respectively.\nLemma 3.3 Under the assumption (A.1) and (A.2), the Markov chain M is irreducible.\nProof According to the definition of irreducibility, we need to show that any two states (k,m,n) ∈ HN × B2N and ( j, p,q) ∈ HN ×B2N are accessible to each other. To prove that, we show three simple facts in the following:\n• According to assumption (A.2), it is possible to produce feedback signals k by a one step transition from state ( j, p,q). i.e. ∃x ∈ B,s.t.P(zt+1 = (k,q,x)|zt = ( j, p,q)) > 0.\n• In Markov chain Mk where the feedback signal is fixed to be k, there exists d ∈ {1,2, · · ·}, such that we can build a d-step transition path from behavior profile x to behavior profile m, followed by a one step transition to behavior profile n. The existence of the d-step path from q to m is due to the irreducibility of Mk in assumption (A.1). To see that it is possible to transit from state m to n by one step in Markov chain Mk, note that if Mk(m,n) = 0, we can simply erase the state (k,m,n) from the state space of Markov chainM, which does not affect our results, therefore we only need to consider the case in which Mk(m,n)> 0.\n• According to assumtion (A.2), in Marokov chain M, it is possible to observe d + 1 consecutive states in which the d+ 1 feedback signals are all k. Combining the three points, we can obtain:\nP(zt+d+2 = (k,m,n)|zt = ( j, p,q))\n≥P(zt+d+2 = (k,m,n)|zt+1 = (k,q,x))\n×P(zt+1 = (k,q,x)|zt = ( j, p,q))\n=Mk(m,n) ·P(u ∈ U : η(u,m) = k) ×·· ·×Mk(q,x) ·P(u ∈ U : η(u,q) = k)\n>0.\n(4)\nTherefore state (k,m,n) is reachable from state ( j, p,q). Similarly, we can also prove that state ( j, p,q) is reachable from state (k,m,n). Since (k,m,n) and ( j, p,q) are arbitrarily chosen, we actually prove the irreducibility of the Markov chain M.\nLemma 3.4 Under the assumption (A.1) and (A.2), the Markov chain M is aperiodic.\nProof Since the Markov chain is irreducible, all states in the chain have the same period. Therefore, in order to prove the aperiodicity, we just need to show that for a given state (k,m,m)∈HN ×B2N , its period is one. According to the first assumption in the lemma, ∀d ≥ 1 satisfying M(d)k (m,m) > 0. Therefore we can build a d-step path in Markov chain\nthat every possible value of the feedback is reachable from every value of the behavior, if there are a very large number of random users arriving at the platform and they have very high dynamics and variety.\nMk: m → m1 → m2 · · · → md−1 → m, such that the transition probability in every step is positive, i.e., Mk(m,m1) > 0, Mk(m1,m2)> 0, ..., Mk(md−1,m)> 0. As a result,\nP(zt+d = (k,m,m)|zt = (k,m,m))\n≥P(zt+d = (k,m,m)|zt+d−1 = (k,md−1,m))\n×P(zt+d−1 = (k,md−1,m)|zt+d−2 = (k,md−2,md−1))\n×·· ·×P(zt+1 = (k,m,m1)|zt = (k,m,m))\n=Mk(md−1,m) ·P(u ∈ U : η(u,md−1) = k) ×Mk(md−2,md−1) ·P(u ∈ U : η(u,md−2) = k) ×·· ·×Mk(m,m1) ·P(u ∈ U : η(u,m) = k)\n>0. (5)\nHence, a d-step transition path with positive probability in Markov chain M can be built as (k,m,m)→ (k,m,m1)→ (k,m1,m2) · · · → (k,md−1,m)→ (k,m,m). Then by the definition of period 2, Markov chain M has the same period as Markov chain Mk, which is one."
    }, {
      "heading" : "3.2 Convergence Bound",
      "text" : "In the previous subsection, we have constructed a new Markov chain and proved its convergence. In this subsection, we show that these results can be used to analyze the convergence rate of the empirical risk for a specified behavior prediction model, namely f .\nLet us start from a formal definition of the problem. Given the T -round training data S = {(h1,b1,b2);(h2,b2,b3); · · · ;(hT ,bT ,bT+1)} = (z1,z2, · · · ,zT ), we define the T -round empirical risk of f with respect to S as follows:\nerrTS ( f ) = 1 T\nT\n∑ t=1 l( f (bt ,ht),bt+1) = 1 T\nT\n∑ t=1 l( f ,zt ), (6)\nwhere we assume l to be upper bounded by a constant B> 0. We then define the T -round expected risk of f as errTM( f ) = 1 T ∑ T t=1 Ezt∼π0Mt l( f ,zt ), where π0 is the initial distribution of Markov chain M. According to Theorem(3.2), the limit of errTM( f ) exists:\nerrπ ( f ) = lim T→∞ errTM( f ) = ∑ z∈[Z] l( f ,z) ·πz, (7)\nwhere π is the stationary distribution of M, and errπ( f ) is the real expected risk of f .\nNext we investigate how well the T -round empirical risk errTS ( f ) approximates errπ( f ). For this purpose, we leverage the Hoeffding Inequality for uniformly ergodic Markov Chains (Glynn and Ormoneit 2002), which is rephrased as below for completeness.\nProposition 3.5 (Hoeffding Inequality for uniformly ergodic Markov Chains) Let X = (Xn : n ≥ 0) be a Markov Chain taking values in a state space S, if the following assumption holds: (A.3) there exists a probability measure φ\n2In Markov chain P, state i’s period is defined as the g.c.d. of all d ∈ 1,2, · · · satisfying P(d)(i, i)> 0.\non S, λ > 0, and an integer m ≥ 1 s.t. ∀x ∈ S, P(Xm ∈ ·|X0 = x) ≥ λ φ(·), then for a function g : S → R with its norm defined as ||g|| = sup{|g(x)| : x ∈ S} < ∞, define ST = 1T ∑ T t=1 g(Xt), for T > 2||g||m/(λ ε), we have\nP(|ST −E(ST )| ≥ ε)≤ 2exp(− λ 2(T ε −2||g||m/λ )2\n2T ||g||2m2 ), (8)\nwhere the expectation E(ST ) is taken on the stationary distribution of X.\nIn order to leverage Proposition (3.5), we need to check whether its assumption (A.3) holds in our problem. For this purpose, we note the fact that for an irreducible, aperiodic, and finite-state Markov chain with time-invariant transition probability matrix P, there must exist N such that ∀n ≥ N, all elements of n-step transition matrix P(n) are non-zero( Lemma 6.6.3 in (Durrett 2010) ). Accordingly in our setting, for Markov chain M, there exists N0 such that ∀1 ≤ i, j ≤ Z,M(N0)i, j > 0. Denote δ as the minimum element in M(N0), i.e., δ = min\n1≤i, j≤Z M(N0)i, j > 0. Then if we set m = N0, λ = Zδ\nand set φ to be the uniform distribution on [Z], it is easy to see that (A.3) holds. Further noticing that ||g|| in (8) is B in our setting, where g = l( f ;zt ) and B is the upperbound of function l, we can leverage Proposition (3.5) to obtain desired convergence bound as Theorem (3.6) shows.\nTheorem 3.6 Convergence of Empirical Loss to Expected Loss Let f :HN ×BN →BN be the behavior prediction function, errTS ( f ) and errπ( f ) be the empirical loss and expected loss respectively, as defined in equation (6) and (7). For any ε > 0 and T > 2BN0/(Zδε), we have\nP( ∣ ∣\n∣ errTS ( f )−errπ ( f )\n∣ ∣ ∣ ≥ ε)≤ 2exp\n(\n− Z2δ 2 (T ε −2BN0/(Zδ ))2\n2T B2N20\n)\n(9)\nTheorem (3.6) basically states that when the sample size T is large enough, the empirical risk errTS ( f ) will converge to the long-term expected risk errπ ( f ), and the convergence rate is exponential in the sample size."
    }, {
      "heading" : "3.3 Uniform Convergence Bound",
      "text" : "In this subsection, we prove a uniform convergence bound for the ABP problem based on covering number,3 as shown in the following theorem.\nTheorem 3.7 Uniform Convergence Theorem. Let F = { f : HN ×BN → BN} be the behavior function space and l ◦F be the composite function set of the loss function l acting on F . For a behavior function f ∈ F , denote errTS ( f )\n3Covering number is one of the common ways to measure the complexity of a function class. Specifically, covering number N (ε, l ◦F ,T ) is defined as max\n{zt}Tt=1∈[Z] T N (ε, l ◦F |{zt}Tt=1 ,dT ) in\nwhich N (ε, l ◦F |{zt}Tt=1 ,dT ) is the minimum capacity of ε-cover of (l ◦F )’s projection on data {zt}Tt=1, w.r.t. the distance metric between x ∈ RT ,y ∈ RT defined as dT (x,y) := max\n1≤t≤T |xt −yt |.\nand errπ( f ) as its empirical loss and expected loss respectively, as defined in equation (6) and (7). For any ε > 0, we have\nP\n(\nsup f∈F\n|errTS ( f )−errπ ( f )| ≥ ε\n)\n≤8N ( ε 8 , l ◦F ,2T )exp\n(\n− Z2δ 2 (T ε −16BN0/(Zδ ))2\n128T B2N20\n) (10)\nfor T ≥ max(T0, 16BN0/(Zδε)), where T0 satisfies Z2δ 2(T0ε−4BN0/(Zδ ))2\n8T0B2N20 ≥ ln4, and N ( ε8 , l ◦F ,2T ) is the ε/8-\ncovering number of l ◦F . N0 and δ are the parameters of Markov chain M (see Notation Table 1), B is the upper bound of loss function l( f ,z), Z is the state number of Markov chain M.\nThe proof of the theorem contains two steps. For the first step, we employ the symmetrization technique to reduce the probability of the uniform convergence bound to a probability involving only two sample sets. For the second step, we further reduce the case to a finite function class by using the covering number theory.\nLemma 3.8 Symmetrization Lemma. Denote S̃ = (z̃1, z̃2, ..., z̃T ) as a set of ghost samples from Markov chain M, then we have,\nP\n(\nsup f∈F\n∣ ∣ ∣ errTS ( f )−errπ ( f ) ∣ ∣ ∣ ≥ ε\n)\n≤ 2P\n(\nsup f∈F\n∣ ∣ ∣ errTS ( f )−err T S̃ ( f ) ∣ ∣ ∣ ≥ ε 2\n)\n(11)\nfor T large enough to satisfy Z 2δ 2(T ε−4BN0/(Zδ ))2\n8T B2N20 ≥ ln4.\nProof For the fixed T -rounds sample S, let fS be the behavior prediction function achieving the supremum sup f∈F ∣ ∣errTS ( f )− errπ( f ) ∣ ∣, then we have\nP\n(\nsup f∈F\n∣ ∣ ∣ errTS ( f )−err T S̃ ( f ) ∣ ∣ ∣ ≥ ε/2\n)\n≥P ( ∣ ∣\n∣ errTS ( fS)−err T S̃ ( fS)\n∣ ∣ ∣ ≥ ε/2 )\n≥P ({ ∣ ∣\n∣ errTS ( fS)−errπ ( fS)\n∣ ∣ ∣ ≥ ε } ∩ { ∣ ∣\n∣ errTS̃ ( fS)−errπ ( fS)\n∣ ∣ ∣ ≤ ε/2 })\n=E [\n1|errTS ( fS)−errπ ( fS)|≥ε ·PS̃|S\n( ∣\n∣ ∣ errTS̃ ( fS)−errπ ( fS)\n∣ ∣ ∣ ≤ ε/2 )] .\n(12)\nConsidering that S̃ is independent of S, by using Theorem 3.6, the conditional probability\nPS̃|S\n(∣\n∣ ∣ errT\nS̃ ( fS)− errπ( fS)\n∣ ∣ ∣ ≤ ε/2 ) can be bounded as\nbelow.\nPS̃|S\n( ∣\n∣ ∣ errTS̃ ( fS)−errπ ( fS)\n∣ ∣ ∣ ≤ ε/2 )\n=P ( ∣ ∣\n∣ errTS̃ ( fS)−errπ ( fS)\n∣ ∣ ∣ ≤ ε/2 )\n≥1−2exp\n(\n− Z2δ 2 (T ε −4BN0/(Zδ ))2\n8T B2N20\n)\n.\n(13)\nThen we can obtain the result state in inequality 11 by choosing T large enough such that exp ( − Z 2δ 2(T ε−4BN0/(Zδ ))2\n8T B2N20\n)\n≤ 1/4.\nProof of Theorem (3.7) Fixing any sample data (S, S̃), we set zT+t = z̃t , t = 1,2, ...,T for simplicity. Pick a subset of F G ⊆ F such that l ◦ G is an ε/8-cover of l ◦ F with respect to the metric d2T (l ◦ f1, l ◦ f2) := max\n1≤t≤2T |l( f1,zt )− l( f2,zt )|. Pick\nf ∈ F such that |errTS ( f )− err T S̃ ( f )| ≥ ε/2. According to the definition of G , there exists a function g ∈ G such that max\n1≤t≤2T |l( f ,zt )− l(g,zt)|< ε8 holds. In fact, as shown below, such a g satisfies |errTS (g)− err T S̃ (g)| ≥ ε/4:\nε 2 ≤ ∣ ∣ ∣ errTS ( f )−err T S̃ ( f ) ∣ ∣ ∣\n≤|errTS ( f )−err T S (g)|+ |err T S (g)−err T S̃ (g)|+ |err T S̃ (g)−err T S̃ ( f )|\n≤|errTS (g)−err T S̃ (g)|+ 1 T\n2T ∑ t=1 |l( f ,zt)− l(g,zt )|\n≤|errTS (g)−err T S̃ (g)|+2 max1≤t≤2T |l( f ,zt)− l(g,zt )|\n<|errTS (g)−err T S̃ (g)|+ ε 4\n(14)\nThe above result enables us to reduce the problem to the case of finite function class. Combining it with Inequality (11), we obtain\nP\n(\nsup f∈F\n∣ ∣ ∣ errTS ( f )−err T S̃ ( f ) ∣ ∣ ∣ ≥ ε/2\n)\n≤P\n(\nmax g∈G\n∣ ∣ ∣ errTS (g)−err T S̃ (g) ∣ ∣ ∣ ≥ ε/4\n)\n≤N (ε/8, l ◦F ,2T )max g∈G\nP ( ∣ ∣\n∣ errTS (g)−err T S̃ (g)\n∣ ∣ ∣ ≥ ε/4 )\n(15)\nFurther considering the result stated in Theorem (3.6), we obtain\nP ( ∣ ∣\n∣ errTS (g)−err T S̃ (g)\n∣ ∣ ∣ ≥ ε/4 )\n≤P ( ∣ ∣\n∣ errTS (g)−errπ (g)\n∣ ∣ ∣ ≥ ε/8 ) +P ( ∣ ∣\n∣ errTS̃ (g)−errπ (g)\n∣ ∣ ∣ ≥ ε/8 )\n≤4exp\n(\n− Z2δ 2 (T ε −16BN0/(Zδ ))2\n128T B2N20\n)\n(16) for T > 16BN0/(Zδε).\nBy combining inequalities (11), (15), and (16), we finally prove the theorem.\nRemark: Please note that for most regular function class, the covering number N (ε, l ◦F ,T ) defined in Theorem 3.7 can be polynomially bounded. For example,\n• If the loss function l is Lipschitz-continuous in its first argument with Lipschitz constant L > 0, then for any T , we have N (ε, l ◦F ,T )≤ N (ε/L,F ,T ).\n• Since we assume the behavior set to be finite, the ABP problem can actually be regarded as a multi-class\nclassification problem, where the class number is |B|. In this case, the covering number N ( ε8/L,F ,2T ) can be bounded by the growth function of F , defined as\nmax {zt}Tt=1∈[Z] 2T\n∣ ∣F |{zt}2Tt=1 ∣ ∣. Moreover, the growth function is\nbounded by ( 2Te(|B|+1) 2\n2d ) d (Bendavid et al. 1995), where\nd is the Natarajan dimension of F (Natarajan 1989). Thus N ( ε8/L,F ,2T ) is at most in T ’s polynomial order."
    }, {
      "heading" : "4 Conclusion and Future Work",
      "text" : "In this paper, we have studied the generalization ability of ERM algorithms for agent behavior prediction. In particular, we first develop a new technique that transforms MCRE to a higher-dimensional but more regular Markov chain and then give a uniform generalization bound based on the new Markov chain. As for the future work, we plan to investigate the joint learning problem of the optimal mechanism of the platform and the optimal prediction model of agent behaviors. Generalization analyses for these two cases will be even more challenging, and the corresponding results will also have more profound impact on adopting machine learning techniques in real-world interactive systems."
    }, {
      "heading" : "5 Acknowledgement",
      "text" : "We thank Di He for his valuable suggestions on the detailed proof technique. Thanks to the AAAI anonymous reviewers for their comments to make the paper more exact and clear. This work is partially supported by grant from the National Science Foundation for Distinguished Young Scholars of China (Grant No. 61325010)."
    } ],
    "references" : [ {
      "title" : "P",
      "author" : [ "S. Bendavid", "N. Cesabianchi", "D. Haussler", "Long" ],
      "venue" : "M.",
      "citeRegEx" : "Bendavid et al. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A",
      "author" : [ "M. Cary", "A. Das", "B. Edelman", "I. Giotis", "K. Heimerl", "Karlin" ],
      "venue" : "R.; Mathieu, C.; and Schwarz., M.",
      "citeRegEx" : "Cary et al. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Budget constrained bidding in keyword auctions and online knapsack problems",
      "author" : [ "Chakrabarty", "Zhou", "Lukose" ],
      "venue" : "In WWW ’07 Proceedings of the 16th international conference on World Wide Web. ACM Press",
      "citeRegEx" : "Chakrabarty et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chakrabarty et al\\.",
      "year" : 2007
    }, {
      "title" : "Bid landscape forecasting in online ad exchange marketplace",
      "author" : [ "Cui" ],
      "venue" : "In KDD ’11 Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM Press",
      "citeRegEx" : "Cui,? \\Q2011\\E",
      "shortCiteRegEx" : "Cui",
      "year" : 2011
    }, {
      "title" : "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords",
      "author" : [ "Ostrovsky Edelman", "B. Schwarz 2005] Edelman", "M. Ostrovsky", "M. Schwarz" ],
      "venue" : null,
      "citeRegEx" : "Edelman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Edelman et al\\.",
      "year" : 2005
    }, {
      "title" : "and Ormoneit",
      "author" : [ "P.W. Glynn" ],
      "venue" : "D.",
      "citeRegEx" : "Glynn and Ormoneit 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A game-theoretic machine learning approach for revenue maximization in sponsored search",
      "author" : [ "He" ],
      "venue" : null,
      "citeRegEx" : "He,? \\Q2013\\E",
      "shortCiteRegEx" : "He",
      "year" : 2013
    }, {
      "title" : "and Rostamizadeh",
      "author" : [ "M. Mohri" ],
      "venue" : "A.",
      "citeRegEx" : "Mohri and Rostamizadeh 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "B",
      "author" : [ "Natarajan" ],
      "venue" : "K.",
      "citeRegEx" : "Natarajan 1989",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Pin and Key",
      "author" : [ "F. Pin", "P. Key" ],
      "venue" : "In EC ’11 Proceedings of the 12th ACM conference on Electronic commerce. ACM Press",
      "citeRegEx" : "Pin and Key.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pin and Key.",
      "year" : 2011
    }, {
      "title" : "V",
      "author" : [ "Vapnik" ],
      "venue" : "N.",
      "citeRegEx" : "Vapnik 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Predicting advertiser bidding behaviors in sponsored search by rationality modeling",
      "author" : [ "Xu" ],
      "venue" : "In Proceedings of the 22nd international conference on World Wide Web, 1433–1444",
      "citeRegEx" : "Xu,? \\Q2013\\E",
      "shortCiteRegEx" : "Xu",
      "year" : 2013
    }, {
      "title" : "Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability 94–116",
      "author" : [ "B. Yu" ],
      "venue" : null,
      "citeRegEx" : "Yu,? \\Q1994\\E",
      "shortCiteRegEx" : "Yu",
      "year" : 1994
    }, {
      "title" : "and Tao",
      "author" : [ "C. Zhang" ],
      "venue" : "D.",
      "citeRegEx" : "Zhang and Tao 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Vindictive bidding in keyword auctions",
      "author" : [ "Zhou", "Y. Lukose. 2007] Zhou", "R. Lukose" ],
      "venue" : "In ICEC ’07 Proceedings of the ninth international conference on Electronic commerce. ACM Press",
      "citeRegEx" : "Zhou et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2007
    }, {
      "title" : "The generalization performance of erm algorithm with strongly mixing observations",
      "author" : [ "Li Zou", "B. Xu 2009] Zou", "L. Li", "Z. Xu" ],
      "venue" : null,
      "citeRegEx" : "Zou et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Machine learning algorithms have been applied to predict agent behaviors in real-world dynamic systems, such as advertiser behaviors in sponsored search and worker behaviors in crowdsourcing, and the prediction models have been used for the optimization of these systems. The behavior data in these systems are generated by live agents: once the systems change due to the adoption of the prediction models learnt from the behavior data, agents will observe and respond to these changes by changing their own behaviors accordingly. As a result, the behavior data will evolve and will not be identically and independently distributed, posing great challenges to the theoretical analysis on the machine learning algorithms for behavior prediction. To tackle this challenge, in this paper, we propose to use Markov Chain in Random Environments (MCRE) to describe the behavior data, and perform generalization analysis of the machine learning algorithms on its basis. Since the one-step transition probability matrix of MCRE depends on both previous states and the random environment, conventional techniques for generalization analysis cannot be directly applied. To address this issue, we propose a novel technique that transforms the original MCRE into a higher-dimensional time-homogeneous Markov chain. The new Markov chain involves more variables but is more regular, and thus easier to deal with. We prove the convergence of the new Markov chain when time approaches infinity. Then we prove a generalization bound for the machine learning algorithms on the behavior data generated by the new Markov chain, which depends on both the Markovian parameters and the covering number of the function class compounded by the loss function for behavior prediction and the behavior prediction model. To the best of our knowledge, this is the first work that performs the generalization analysis on data generated by complex processes in real-world dynamic systems. ∗This work was done when the first two authors were visiting Microsoft Research Asia. Copyright c © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
    "creator" : "LaTeX with hyperref package"
  }
}