{
  "name" : "1310.5665.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Theory and Algorithms for Revenue Optimization in Second-Price Auctions with Reserve",
    "authors" : [ "Mehryar Mohri" ],
    "emails" : [ "mohri@cims.nyu.edu", "munoz@cims.nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Second-price auctions with reserve play a critical role for modern search engine and popular online sites since the revenue of these companies often directly depends on the outcome of such auctions. The choice of the reserve price is the main mechanism through which the auction revenue can be influenced in these electronic markets. We cast the problem of selecting the reserve price to optimize revenue as a learning problem and present a full theoretical analysis dealing with the complex properties of the corresponding loss function. We further give novel algorithms for solving this problem and report the results of several experiments demonstrating their effectiveness."
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the past few years, advertisement has gradually moved away from the traditional printed promotion to the more tailored and directed online publicity. The advantages of online advertisement are clear: since most modern search engine and popular online sites companies such as as Microsoft, Facebook, Google, eBay, or Amazon, may collect information about the users’ behavior, advertisers can better target the population sector their brand is intended for.\nMore recently, a new method for selling advertisements has gained momentum. Unlike the standard contracts between publishers and advertisers where some amount of impressions is required to be fulfilled by the publisher, an Ad Exchange works in a way similar to a financial exchange where advertisers bid and compete between each other for an ad slot. The winner then pays the publisher and his ad is displayed.\nThe design of such auctions and their properties are crucial since they generate a large fraction of the revenue of popular online sites. These questions have motivated extensive research on the topic of auctioning in the last decade or so, particularly in the theoretical computer science and economic theory communities. Much of this work has focused on the analysis of mechanism design, either to prove some useful property of an existing auctioning mechanism, to analyze its computational efficiency, or to search for an optimal revenue maximization truthful mechanism (see [14] for a good discussion of key research problems related to Ad Exchange and references to a fast growing literature therein).\nOne important problem is that of determining an auction mechanism that achieves optimal revenue [14]. In the ideal scenario where the distribution of all bidders is given, this is known to be achievable (see for example [15]), but even good approximations of such distributions are not known in practice. Game theoretical approaches to the design of auctions have resulted in a series of interesting results including [18, 12, 15, 16], all of them based on some assumptions about the distribution of the bidders, e.g., the monotone hazard rate assumption.\nar X\niv :1\n31 0.\n56 65\nv1 [\ncs .L\nG ]\n2 1\nO ct\nThe results of the recent publications have nevertheless set the basis for most Ad Exchanges in practice: the mechanism widely adopted for selling ad slots is that of a Vickrey [24] auctions or second-price auctions with reserve price r [7]. In such auctions, the winning bidder (if any) pays the maximum of the second-place bid and the reserve price r. The reserve price can be set by the publisher or automatically by the exchange. It is clear that the revenue of the publisher depends greatly on how the reserve price is set: if set too low, the winner of the auction might end up paying only a small amount even if his bid was really high; on the other hand, if it is set too high, then bidders may not bid higher than the reserve price and the ad slot will not be sold.\nWe propose a machine learning approach to the problem of determining the reserve price to optimize revenue in such auctions. The general idea is to leverage the information gained from past auctions to predict a beneficial reserve price. Since every transaction on an Exchange is logged, it is natural to seek to exploit that data. This could be used to estimate the probability distribution of the bidders, which can then be used indirectly to come up with the optimal reserve price [15, 17]. Instead, we will seek a discriminative method making use of the loss function related to the problem and taking advantage of existing user features.\nMachine learning methods have already been used for the related problems of designing incentive compatible auction mechanisms [2, 4], for algorithmic bidding [25, 1] and even for predicting bid landscapes [6]. But, to our knowledge, no prior work has used historical data in combination with user features for the sole purpose of revenue optimization in this context. In fact, the only publications we are aware of that are directly related to our objective are [17] and the stimulating work of Cesa-Bianchi et al. [5] which considers a more general case than [17] . The scenario studied by [5] is that of censored information, which motivates their use of a bandit model to optimize the revenue of the seller. Our analysis assumes instead access to full information. We argue that this is in fact a more realistic scenario since most companies do have access to the full historical data.\nThe learning scenario we consider is more general since it includes the use of features, as is standard in supervised learning. Since user information is sent to advertisers and bids are made based on this information, it is only natural to include user features in our learning solution. A special case of our analysis coincides with the no-feature scenario considered in [5], assuming full information. But, our results further extend those of [5] even in that scenario. In particular, we present an O(m logm) algorithm for solving a key optimization problem used as a subroutine by [5], for which they do not seem to give an algorithm. We also do not require an i.i.d. assumption about the bidders, although this is needed in [5] only for the bandit approach.\nThe theoretical and algorithmic analysis of this learning problem raises several non-trivial technical issues. This is because, unlike some common problems in machine learning, here, the use of a convex surrogate loss cannot be successful. Instead, we must derive an alternative non-convex surrogate requiring novel theoretical guarantees (Section 3) and a new algorithmic solution (Section 4). We present a detailed analysis of possible surrogate losses and select a continuous loss that we prove to be calibrated and for which we give generalization bounds. This leads to an optimization problem cast as a DC-programming problem whose solutions are examined in detail: we first present an efficient combinatorial algorithm for solving that optimization in the no-feature case, next we combine that solution with the DC algorithm (DCA) [21] to solve the general case. Section 5 reports the results of our experiments with synthetic data in both the no-feature case and the general case. We first introduce the problem of selecting the reserve price to optimize revenue and cast it as a learning problem (Section 2)."
    }, {
      "heading" : "2 Reserve price selection problem",
      "text" : "As already discussed, the choice of the reserve price r is the main mechanism through which a seller can influence the auction revenue. To specify the results of a second-price auction we need only the vector of first and second highest bids which we denote by b = (b1, b2) ∈ B ⊂ R2. For a given reserve price r and bid pair b, the revenue of an auction is given by\nRevenue(r,b) = b21r<b2 + r1b2≤r≤b1 . (1)\nThe simplest setup is one where there are no features associated with the auction. In that case, the objective is to select r to optimize the expected revenue:\nE b [Revenue(r,b)] = ∫ ∞ r P[b2 > t]dt+ r P[b1 ≥ r]. (2)\nThe derivation of this equality can be obtained using integration by parts and can be found in [5]. In fact this expression is precisely the one optimized by the authors. If we now associate with each auction a feature vector x ∈ X , the so-called public information, and set the reserve price to h(x), where h : X → R+ is our reserve price hypothesis function, the problem can be formulated as that of selecting out of some hypothesis set H a hypothesis h with large expected revenue:\nE (x,b)∼D [Revenue(h(x),b)] (3)\nwhere D is the unknown distribution according to which the pairs (x,b) are drawn. Instead of the revenue, we will consider a loss function L defined by L(r,b) = −Revenue(r,b) for all (r,b) and will seek a hypothesis h with small expected loss L(h) = E(x,b)∼D[L(h(x),b)]. As in standard supervised learning scenarios, we assume access to a training sample S = ((x1,b1), . . . , (xm,bm)) of size m ≥ 1 drawn i.i.d. according to D and denote by L̂S(h) the empirical loss 1m ∑m i=1 L(h(xi,bi). In the next sections, we present a detailed study of this learning problem."
    }, {
      "heading" : "3 Learning guarantees",
      "text" : "Let X denote the set of feature vectors and B that of bid vectors. To derive generalization bounds for the learning problem formulated, we need to analyze the complexity of the family of functions LH mapping X ×B to R defined by LH = {(x,b) 7→ L(h(x),b) : h ∈ H}. The loss function L is neither Lipschitz continuous nor convex (see Figure 1). To analyze its complexity, we decompose L as a sum of two loss functions l1 and l2 with more convenient properties. We have L = l1 + l2 with l1 and l2 defined for all (x,b) ∈ X × B by\nl1(r,b) = −b21r<b2 − r1b2≤r≤b1 − b11r>b1 and l2(r,b) = b11r>b1 . Note that for a fixed b, the function r 7→ l1(r,b) is 1-Lipschitz since the slope of the lines defining the function is at most 1. We will consider the corresponding family of loss functions: l1H = {(x,b) 7→ l1(h(x),b) : h ∈ H} and l2H = {(x,b) 7→ l2(h(x),b) : h ∈ H} and use the notions of empirical and average Rademacher complexity. For a family of functions G and finite sample S = (z1, . . . , zm) of size m, the empirical Rademacher complexity is defined by R̂S(G) = Eσ [ supg∈G 1 m ∑m i=1 σig(zi) ] , where σ = (σ1, . . . , σm)>, with σis independent uniform random variables taking values in {−1,+1}. The Rademacher complexity of G is defined as Rm(G) = ES∼Dm [R̂S(G)]. Proposition 1. For any hypothesis set H and any sample S = ((x1,b1), . . . , (xm,bm)), the empirical Rademacher complexity of l1H can be bounded as follows:\nR̂S(l1H) ≤ R̂S(H).\nProof. By definition of the empirical Rademacher complexity, we can write R̂S(l1H) = 1 m Eσ[suph∈H ∑m i=1 σil1(h(xi),bi)] = 1 m Eσ[suph∈H ∑m i=1 σi(ψi ◦ h)(xi)], where, for all i ∈ [1,m], ψi is the function defined by ψi : r 7→ l1(r,bi). For any i ∈ [1,m], ψi is 1-Lipschitz, thus, by the contraction lemma 12, a variant of Talagrand’s contraction lemma [11][p. 112] with our definition of Rademacher complexity, we have R̂S(l1H) ≤ 1m Eσ[suph∈H ∑m i=1 σih(xi)] = R̂S(H).\nProposition 2. Let M = supb∈B b1. Then, for any hypothesis set H with pseudo-dimension d = Pdim(H) and any sample S = ((x1,b1), . . . , (xm,bm)), the empirical Rademacher complexity of l2H can be bounded as follows:\nR̂S(l2H) ≤ √\n2d log emd m .\nProof. By definition of the empirical Rademacher complexity, we can write\nR̂S(l2H) = 1\nm E σ [ sup h∈H m∑ i=1 σib 1 i1h(xi)>b1i ] = 1 m E σ [ sup h∈H m∑ i=1 σiΨi(1h(xi)>b1i ) ] ,\nwhere for all i ∈ [1,m], Ψi is the M -Lipschitz function x 7→ b1ix. Thus, by Lemma 12 combined with Massart’s lemma (see for example [13]):\nR̂S(l2H) ≤ M\nm E σ [ sup h∈H m∑ i=1 σi1h(xi)>b1i ] ≤M √ 2d′ log emd′ m ,\nwhere d′ = VCdim({(x,b) 7→ 1h(x)−b1>0 : (x,b) ∈ X × B}). Since the second bid component b2 plays no role in this definition, d′ coincides with the VCdim({(x, b1) 7→ 1h(x)−b1>0 : (x, b1) ∈ X × B1}), where B1 is the projection of B ⊆ R2 onto its first component, and is upper-bounded by VCdim({(x, t) 7→ 1h(x)−t>0 : (x, t) ∈ X ×R}), which is exactly the pseudo-dimension of H .\nTheorem 3. Let M = supb∈B b1 and let H be a hypothesis set with pseudo-dimension d = Pdim(H). Then, for any δ > 0, with probability at least 1 − δ over the choice of a sample S of size m, the following inequality holds for all h ∈ H:\nL(h) ≤ L̂S(h) + 2Rm(H) + 2M √\n2d log emd m +M √ log 1δ 2m . (4)\nProof. By Propositions 1 and 2, since L = l1 + l2, the Rademacher complexity of LH can be bounded as follows:\nRm(LH) ≤ Rm(l1H) + Rm(l2H) ≤ Rm(H) +M √\n2d log emd m .\nThe result then follows by the application of a standard Rademacher complexity bound [10].\nThis learning bound invites us to consider an algorithm minimizing the empirical loss L̂S(h), h ∈ H , while controlling the complexity, Rademacher complexity and pseudo-dimension, of the hypothesis setH . However, as in the familiar case of binary classification, in general, minimizing this empirical loss is a computationally hard problem. Thus, in the next section, we study the question of using a surrogate loss instead of the original loss L."
    }, {
      "heading" : "3.1 Surrogate loss",
      "text" : "As pointed out earlier, the loss function L does not admit some common useful properties: for any fixed b, L(·,b) is not differentiable at two points, is not convex, and is not Lipschitz, in fact it is discontinuous. For any fixed b, L(·,b) is quasi-convex, a property that is often desirable since there exist several solutions for quasi-convex optimization problems. However, unfortunately, in general, a sum of quasi-convex functions, such as the sum ∑m i=1 L(·,bi) appearing in the definition of the empirical loss, is not quasi-convex and a fortiori not convex.1 In fact, in general, such a sum may admit exponentially many local minima. This leads us to seek a surrogate loss function with more favorable optimization properties.\nA standard method in machine learning consists of replacing the loss function Lwith a convex upper bound [3]. A natural candidate in our case is the piecewise linear convex function shown in Figure 1(b). However, while this convex loss function is convenient for optimization, it is not calibrated\n1It is known that under some separability condition if a finite sum of quasi-convex functions on an open convex set is quasi-convex then all but perhaps one of them is convex [8].\nand does not provide a useful surrogate. The calibration problem is illustrated by Figure 2(a) in dimension one, where the true objective function to be minimized ∑m i=1 L(r,bi) is compared with the sum of the surrogate losses. As can be seen from the figure, the convex surrogate loss is in fact a very poor upper bound on the sum of the true losses. This is mainly due to the fact that the bid vectors bi, i ∈ [1,m], may be in general quite diverse, which is in fact observed in practice. Other convex surrogate losses seem to share the same issue or not even offer a good approximation for a single term. This leads us to consider alternative non-convex loss functions, thus not ideal for optimization, but that can still lead to simpler optimization problems than the original one using L. Perhaps, the most natural surrogate loss is then L′γ , an upper bound on L defined for all γ > 0 by:\nL′γ(r,b) = −b21r≤b2 − r1b2<r≤(1−γ)b1 + 1− γ γ (r − b1)1(1−γ)b1<r≤b1 . (5)\nand shown in Figure 3(b). However, this turns out to be also a poor choice because L′γ is a loose upper bound of L in the most critical region, that is around the minimum of the loss L. This furthermore results in a loss that in general is not calibrated, that is, for a hypothesis set H , we may not have in general infh∈H E(x,b)∼D[L′γ(h(x),b)] → infh∈H E(x,b)∼D[L(h(x),b)] as γ → 0, depending on the distribution D.2 Thus, instead, we will consider, for any γ > 0, the loss function Lγ defined as follows:\nLγ(r,b) = −b21r≤b2 − r1b2<r≤b1 + 1\nγ (r − (1 + γ)b1)1b1<r≤(1+γ)b1 , (6)\nand shown in Figure 3(a).3 A comparison between the sum of L-losses and the sum of Lγ-losses is shown in Figure 2(b). Observe that the fit is considerably better than when using a piecewise linear convex surrogate loss. A possible concern associated with the loss function Lγ is that it is a lower bound for L. One might think then that minimizing it would not lead to an informative solution. However, we argue that this problem arises significantly with upper bounding losses such as the convex surrogate, which we showed not to lead to a useful minimizer, or L′γ , which is a poor approximation of L near its minimum. By matching the original loss L in the region of interest, around the minimal value, the loss function Lγ leads to more informative solutions in this problem. We further analyze the difference of the expectations of L and Lγ and show that Lγ is calibrated. We will use for any h ∈ H , the notation Lγ(h) = E(x,b)∼D[Lγ(h(x),b)]. Theorem 4. Let H be a reproducing kernel Hilbert space associated with a positive definite kernel K. Denote by h∗γ the solution of min‖h‖K≤Λ Lγ(h). If supb∈B = M <∞, then\nL(h∗γ)− Lγ(h∗γ) ≤ γM.\nTo prove this proposition we require some definitions first.\n2Technically, this is related to the fact that the difference E[L′γ(h(x),b) − L(h(x),b)] can be expressed and bounded in terms of P [ h(x) ∈ ((1− γ)b1, b1] ] and to the closeness of the interval ((1− γ)b1, b1] at b1.\n3Note that, technically, the theoretical and algorithmic results we present for Lγ could be developed in a somewhat similar way for L′γ with the absence of the calibration property and other related properties.\nDefinition 5. For any h ∈ H define the following subsets of X × B: I1(h) = {(x,b)|h(x) ≤ b2} I2(h) = {(x,b)|h(x) ∈ (b2, b1]} I3(h) = {(x,b)|h(x) ∈ (b1, (1 + γ)b1]} I4(h) = {(x,b)|h(x) > (1 + γ)b1}\nThis sets represent the different regions where Lγ is defined. In each region the function is affine. When the reference to h is clear we will simply denote this sets by I1, I2, I3 and I4. We will now prove a technical lemma necessary for the proof of Theorem 4.\nLemma 6. Under the conditions of Theorem 4,\nE x,b\n[ h∗γ(x)1I2 ] ≥ 1 γ E x,b [ h∗γ(x)1I3 ] .\nProof. Let 0 < λ < 1, because ‖λh∗γ‖K < Λ and because h∗γ is a minimizer we must have:\nEx,b [ Lγ(h ∗ γ(x),b) ] ≤ Ex,b [ Lγ(λh ∗ γ(x),b) ] . (7)\nIf h∗(x) < 0, then Lγ(h∗(x),b) = Lγ(λh∗(x)) = −b2 by definition. If on the other hand h∗(x) > 0, because λh∗γ(x) < h ∗ γ(x) we must have that for (x,b) ∈ I1(h∗γ) Lγ(h∗γ(x),b) = Lγ(λh ∗ γ(x),b) = −b2 too. Also because Lγ ≤ 0 and Lγ(h∗γ(x),b) = 0 for (x,b) ∈ I4(h∗γ) it is immediate that Lγ(h∗γ(x),b) ≥ Lγ(λh∗γ(x),b) for (x,b) ∈ I4(h∗γ). The following inequality then trivially holds:\nE x,b\n[ Lγ(h ∗ γ(x),b)(1I1 + 1I4) ] ≥ E x,b [ Lγ(λh ∗ γ(x),b)(1I1 + 1I4) ] . (8)\nSubstracting (8) from (7) we obtain\nE x,b\n[ Lγ(h ∗ γ(x),b)(1I2 + 1I3) ] ≤ E x,b [ Lγ(λh ∗ γ(x),b)(1I2 + 1I3) ] .\nBy rearranging terms we can see this inequality is equivalent to\nE x,b\n[ (Lγ(h ∗ γ(x),b)− Lγ(λh∗γ(x),b))1I2 ] ≤ E x,b [ (Lγ(λh ∗ γ(x),b)− Lγ(h∗γ(x),b))1I3 ] (9)\nNotice that if (x,b) ∈ I2(h∗γ), then Lγ(h∗γ(x),b) = −h∗γ(x). If λh∗γ(x) > b2 too then Lγ(λh ∗ γ(x),b) = −λh∗γ(x). On the other hand if λh∗γ(x) ≤ b2 then Lγ(λh∗γ(x),b) = −b2 ≤ −λh∗γ(x). Thus\n(Lγ(h ∗ γ(x),b)− Lγ(λh∗γ(x),b))1I2 ≥ (λ− 1)h∗γ(x)1I2 (10)\nThis gives a lower bound for the left hand side of inequality (9). We will attempt to obtain an upper bound on the right hand side now. To do this we analyze two different cases: 1)λh∗γ(x) ≤ b1 and 2) λh∗γ(x) > b 1.\nFor the first case we know that Lγ(h∗γ(x),b) = 1 γ (h ∗ γ(x)−(1+γ)b1) > −b1 (because (x,b) ∈ I3). Furthermore if λh∗γ(x) ≤ b1 then by definitionLγ(λh∗γ(x),b) = min(−b2,−λh∗γ(x)) ≤ −λh∗γ(x). Thus, we must have:\nLγ(λh ∗ γ(x),b)− Lγ(h∗γ(x),b) < b1 − λh∗γ(x) < (1− λ)b1 ≤ (1− λ)M. (11)\nWhere we used the fact that h∗γ(x) > b 1 for the second inequality.\nWe analyze the second case now. If λh∗γ(x) > b 1, then for (x, b) ∈ I3 we have Lγ(λh∗γ(x),b) − Lγ(h ∗ γ(x),b) = 1 γ (λ− 1)h∗γ(x). Thus we can bound the right hand side of (9) as:\nE x,b\n[ (Lγ(λh ∗ γ(x),b)− Lγ(h∗γ(x),b))1I3 ] (12)\n= E x,b\n[ (Lγ(λh ∗ γ(x),b)− Lγ(h∗γ(x),b))1I31{λh∗γ(x)>b1} ] (13)\n+ E x,b\n[ (Lγ(λh ∗ γ(x),b)− Lγ(h∗γ(x),b))1I31{λh∗γ(x)≤b1} ] ≤ λ− 1\nγ E x,b\n[ h∗γ(x)1I3(λh∗γ) ] + (1− λ)BP [ h∗γ(x) > b 1 ≥ λh∗γ(x) ]\n(14)\nWhere we have used (11) to bound the second summand. Combining inequalities (9), (10) and (14) and dividing by (λ− 1) we obtain the bound\nEx,b [ h∗γ(x)1I2 ] ≥ 1 γ Ex,b [ h∗γ(x)1I3(λh∗γ) ] −BP (h∗γ(x) > b1 ≥ λh∗γ(x))\nWhere we have reverted the inequality since (λ − 1) < 0. Finally, taking the limit as λ → 1 we obtain\nEx,b [ h∗γ(x)1I2 ] ≥ 1 γ Ex,b [ h∗γ(x)1I3(h∗γ) ] .\nWhere the passing of the limit inside the expectation is justified by the bounded convergence theorem and P (h∗γ(x) > b 1 ≥ λh∗γ(x))→ 0 by the continuity of probability measures.\nWe now proceed to prove the original theorem.\nProof. Since the functions L and Lγ agree everywhere except in I3, the following holds:\nE x,b\n[ L(h∗γ(x),b)− Lγ(h∗γ(x),b) ] = 4∑ k=1 E x,b [ (L(h∗γ(x),b)− Lγ(h∗γ(x),b))1Ik ] = E\nx,b\n[ (L(h∗γ(x),b)− Lγ(h∗γ(x),b))1I3 ] = E\nx,b [ 1 γ ((1 + γ)b1 − h∗γ(x))1I3) ] . (15)\nFurthermore, for (x,b) ∈ I3 we know that b1 < h∗γ(x). Hence, we can bound (15) by Ex,b[h∗γ(x)1I3 ]. According to Lemma 6 this quantity is less than γ Ex,b [ h∗γ(x)1I2 ] . Thus, we\ncan write\nE x,b\n[ L(h∗γ(x),b) ] − E\nx,b\n[ Lγ(h ∗ γ(x),b) ] ≤ γ E\nx,b\n[ h∗γ(x)1I2 ] ≤ γ E\nx,b\n[ b11I2 ] ≤ γM.\nsince for (x,b) ∈ I2 h∗γ(x) ≤ b1.\nNotice that, since L ≥ Lγ for all γ ≥ 0, it follows easily from the proposition that Lγ(h∗γ) → Lγ(h∗). Where h∗ is the best hypothesis in class for the real loss. This shows that the loss Lγ is indeed consistent.\nThe 1/γ-Lipschitzness of Lγ can be used to prove the following generalization bound (see supplementary material). Theorem 7. If γ ∈ (0, 1] is fixed and S denotes a sample of size m, then for any δ > 0, with probability at least 1− δ over the choice of the sample S\nLγ(h) ≤ L̂γ(h) + 2\nγ Rm(H) +M √ log 1δ 2m , (16)\nfor all h ∈ H . The theorem can be used to derive a learning bound that holds uniformly for all γ ∈ (0, 1], at the price of an additional term of the form O( √ log log(1/γ)/m) (see Corollary 13 in the Appendix). These results are reminiscent of the standard margin bounds with γ playing the role of a margin. The situation here is however somewhat different. Our learning bounds suggest, for a fixed γ ∈ (0, 1], to seek a hypothesis h minimizing the empirical loss L̂γ(h) while controlling a complexity term upper bounding Rm(H), which in the case of a family of linear hypotheses could be ‖h‖2K for some PSD kernel K. Since the bound of Corollary 13 holds uniformly for all γ, we can use it to select γ out of a finite set of possible grid search values. Alternatively, γ can be set via cross-validation.\nWe conclude this section with a stronger consistency result than Theorem 4 presenting a stronger form of consistency. We will show that we can lower bound the risk of the best hypothesis in class L∗ := L(h∗). By the risk of the empirical minimizer of Lγ , ĥγ := argmin‖h‖K L̂γ(h). Theorem 8. Let M = supb∈B b1 and let H be a hypothesis set with pseudo-dimension d = Pdim(H). Then for any δ > 0 and a fixed value of γ > 0, with probability at least 1 − δ over the choice of a sample S of size m, the following inequality holds:\nL(ĥγ) ≤ L∗ + 2γ + 2\nγ Rm(H) + γM2M\n√ 2d log md\nm + 2M √ log 2δ 2m\nProof. According to Theorem 3 with probability at least 1− δ/2 the following is true:\nL(ĥγ) ≤ L̂S(ĥγ) + 2Rm(H) + 2M √\n2d log md m +M √ log 2δ 2m . (17)\nFurthermore, applying Lemma 6 for the empirical measure induced by the sample we can bound L̂S(ĥγ) by L̂γ(ĥγ)+γM . The first term of the previous expression is less than L̂γ(h∗γ) by definition of ĥγ . Finally, the same analysis used in Theorem 7 shows that with probability 1− δ/2\nL̂γ(h∗γ) ≤ Lγ(h∗γ) + 2\nγ Rm(H) +M √ log 2δ 2m .\nAgain, by definition of h∗γ we know that Lγ(h∗γ) ≤ Lγ(h∗) ≤ L(h∗). Where the last inequality is true since L is an upperbound on Lγ . Hence\nL̂S(ĥγ) ≤ L(h∗γ) + 1\nγ Rm(H) +M √ log 2δ 2m + γM. (18)\nUsing the union bound and combining inequalities (17) and (18) we obtain the result.\nThis bound can be made to hold uniformly on γ as in Corollary 13. Thus, we have shown that for appropiate choices of γ and m (for instance γ 1/m1/4) we have convergence of Lγ(ĥγ) to L∗, which is a stronger form of consistency."
    }, {
      "heading" : "4 Algorithms",
      "text" : "In this section we present algorithms for solving the optimization problem for selecting the reserve price. We start with no-feature case and then treat the general case."
    }, {
      "heading" : "4.1 No feature case",
      "text" : "We present a general algorithm to optimize sums of functions similar to Lγ or L in the onedimensional case.\nDefinition 9. We will say that function V : R+ × B → R is a v-function if it admits the following form: V (r,b) = −a11r≤b2 − a2r1b2<r≤b1 + (a3r − a4)1b1<r<(1+η)b1 , with a1 > 0 and η > 0 constants and a1, a2, a4 defined by a1 = ηa3b2, a2 = ηa3, and a4 = a3(1 + η)b1.\nFigure 4(a) illustrates this family of loss functions. A v-function is a generalization of Lγ . Indeed, any v-function V satisfies V (r,b) ≤ 0 and attains its minimum at b1. Finally, as can be seen straightforwardly from Figure 3, Lγ is a v-function for any γ ≥ 0. We consider the following general problem of minimizing a sum of v-functions:\nmin r≥0 F (r) := m∑ i=1 Vi(r,bi). (19)\nObserve that this is not a trivial problem since, for any fixed bi, Vi(·,bi) is non-convex and that, in general, a sum of m such functions may admit many local minima. The following proposition shows that the minimum is attained at one of the highest bids, which matches the intuition.\nProposition 10. Problem (19) admits a solution r∗ that satisfies r∗ = b1i for some i ∈ [1,m].\nThe problem can thus be reduced to examining the value of the function for the m arguments b1i , i ∈ [1,m]. This yields a straightforward method for solving the optimization which consists of computing F (b1i ) for all i and taking the minimum. But, since the computation of each F (b 1 i ) takes O(m), the overall computational cost is in O(m2), which can be prohibitive for even moderately large values of m.\nInstead, we have devised a more efficient combinatorial algorithm that can be used to solve the problem in O(m logm) time. The algorithm consists of first sorting all boundary points, that is the points in N = ⋃i{b1i , b2i , (1 + η)b1i } associated with the functions Vi(·,bi), i ∈ [1,m]. We then show that for the ordered sequence (n1, . . . , n3m), F (nk+1) can be computed from F (nk) in constant time, using the fact that Vi(·, bi) can only change at boundary points (see Figure 4(b)). A more detailed description and the proof of the correctness of the algorithm are given in the Appendix.\nFurthermore, the algorithm can be straightforwardly extended to solve the minimization of F over a set of r-values bounded by Λ, that is {r : 0 ≤ r ≤ Λ}. Indeed, we then only need to compute F (b1i ) for i ∈ [1,m] such that b1i < Λ and of course also F (Λ), thus the computational complexity in that regularized case remains O(m logm).\nProposition 11. There exists an algorithm to solve the optimization problem (19) in O(m logm).\nProof. The pseudocode for the desired algorithm is presented in Algorithm 1. Where a1i , ..., a 4 i denote the parameters defining the functions Vi(r,bi).\nAlgorithm 1 Sorting N := ⋃mi=1{b1i , b2i , (1 + η)b1i }; (n1, ..., n3m) =Sort(N ); Set ci := (c1i , c 2 i , c 3 i , c 4 i ) = 0 for i = 1, ..., 3m;\nSet c11 = − ∑m i=1 a 1 i ; for j = 2, ..., 3m do Set cj = cj−1; if nj−1 = b2i for some i then c1j = c 1 j + a 1 i ;\nc2j = c 2 j − a2i ;\nelse if nj−1 = b1i for some i then c2j = c 1 j + a 2 i ;\nc3j = c 3 j + a 3 i ; c4j = c 1 j − a4i ;\nelse c3j = c 3 j − a3i ;\nc4j = c 1 j + a 4 i ;\nend if end for\nWe will prove that after running the algorithm 1 we can evaluate F (nj) in constant time in the following way:\nF (nj) = c 1 j + c 2 jnj + c 3 jnj + c 4 j . (20) This trivial for n1 as by construction n1 ≤ b2i for all i and by definition then F (n1) = − ∑m i=1 a 1 i . Now assume that equation (20) is true for j, we prove that then it must be true for j + 1. Suppose nj = b 2 i for some i (the cases nj = b 1 i and nj = (1 + η)b 1 i can be handled in the same way). Then Vi(nj ,bi) = −a1i and we can write∑ k 6=i Vk(nj ,bk) = F (nj)− V (nj ,bi) = (c1j + c2jnj + c3jnj + c4j ) + a1i .\nThus, by construction we would have:\nc1j+1 + c 2 j+1nj+1 + c 3 j+1nj+1 + c 4 j+1 = c 1 j + a 1 i + (c 2 j − a2i )nj+1 + c3jnj+1 + c4j\n= (c1j + c 2 jnj+1 + c 3 jnj+1 + c 4 j ) + a 1 i − a2inj+1 = ∑ k 6=i Vk(nj+1,bk)− a2inj+1,\nwhere the last equality holds since the definition of Vk(r,bk) does not change for r ∈ [nj , nj+1]. Finally, since nj was a boundary point, the definition of Vi(r,bi) must change from −a1i to −a2i r, thus the last equation is indeed equal to F (nj+1) as we wanted. The reader should be convinced that a similar argument can be given if nj = b1i or nj = (1 + η)b 1 i . Let us analyze the complexity of the algorithm: sorting the set N can be done in O(m logm) whereas every iteration of the cycle takes constant time. Thus the evaluation of all points can be done in linear time. Once all evaluations are done, finding the minimum can again be done in linear time. Hence the total complexity of the algorithm is O(m logm)."
    }, {
      "heading" : "4.2 General case",
      "text" : "We first consider the case of a hypothesis set H of linear functions x 7→ w · x with bounded norm, ‖w‖ ≤ Λ, for some Λ ≥ 0. This is not a loss of generality since for any kernel K, we can consider hypotheses of the form h = ∑n i=1 αiK(xi, ·) which is a linear function of α. Thus any result in this section can be extended to arbitrary PSD kernels.\nThe results of Theorem 7 and Corollary 13 suggest seeking, for a fixed γ ≥ 0, the vector w solution of the following optimization problem: min‖w‖≤Λ ∑m i=1 Lγ(w ·xi,bi). Replacing the original loss L with Lγ helped us remove the discontinuity of the loss. But, we still face an optimization problem based on a sum of non-convex functions. This problem can be formulated as a DC-programming (difference of convex functions programming) problem. Indeed, Lγ can be decomposed as follows for all (r,b) ∈ X × B: Lγ(r,b) = u(r,b)− v(r,b), with the convex functions u and v defined by\nu(r,b) = −r1r<b1 + r − (1 + γ)b1\nγ 1r≥b1\nv(r,b) = (−r + b2)1r<b2 + r − (1 + γ)b1\nγ 1r>b1 .\nUsing the decomposition Lγ = u− v, our optimization problem can be formulated as follows: min w∈RN U(w)− V (w) subject to ‖w‖ ≤ Λ, (21)\nwhere U(w) = ∑m i=1 u(w · xi,bi) and V (w) = ∑m i=1 v(w · xi,bi), which shows it can be formulated as a DC-programming problem. The global minimum of the optimization problem (21) can be found using a cutting plane method [9], but that method only converges in the limit and does not admit known algorithmic convergence guarantees.4 There exists also a branch-and-bound algorithm with exponential convergence for DC-programming [9] for finding the global minimum. Nevertheless, in [20], it is pointed out that this type of combinatorial algorithms fail to solve real life DC programs in high dimensions. In fact our implementation of this algorithm shows that the convergence of the algorithm in practice is extremely slow for even moderately high-dimensional problems. An other attractive solution for finding the global solution of a DC-programming problem over a polyhedral convex set is the combinatorial solution of Hoang Tuy [22]. However, casting the problem as an instance of that problem requires in our context explicitly specifying the slope and offsets for the piecewise linear function corresponding to a sum of Lγ losses, which requires exponential time and space.\nAn alternative consists of using the DC algorithm, a primal-dual sub-differential method of Dinh Tao and Hoai An [21], (see also [20] for a good survey). This algorithm is applicable when u and v are proper lower semi-continuous convex functions as in our case. When v is differentiable, the DC algorithm coincides with the CCCP algorithm of Yuille and Rangarajan [27], which has been used in several contexts in machine learning and analyzed by [19].\nThe general proof of convergence of the DC algorithm was given by [21]. In some special cases, the DC algorithm can be used to find the global minimum of the problem as in the trust region problem [21], but, in general, the DC algorithm or its special case CCCP are only guaranteed to converge to a local minimum [21, 19]. Nevertheless, the number of iterations of the DC algorithm is relatively small. Its convergence has been shown to be in fact linear for DC-programming problems such as ours [26].\nThe algorithm we are proposing goes one step further than that of [21]: we use DCA to find a local minimum but then restart our algorithm with a new seed that is guaranteed to reduce the objective function. Unfortunately, we are not in the same regime as in the trust region problem of Dinh Tao and Hoai An [21] where the number of local minima is linear in the size of the input. Indeed, here the number of local minima can be exponential in the number of dimensions of the feature space and it is not clear to us how the combinatorial structure of the problem could help us rule out some local minima faster and make the optimization more tractable.\n4Some claims of [9], e.g., Proposition 4.4 used in support of the cutting plane algorithm, are incorrect [23].\nIn the following, we describe more in detail the solution we propose for solving the DCprogramming problem (21). The functions v and V are not differentiable in our context but they admit a sub-gradient at all points. We will denote by δV (w) an arbitrary element of the sub-gradient ∂V (w), which coincides with∇V (w) at points w where V is differentiable. The DC algorithm then coincides with CCCP, modulo the replacement of the gradient of V by δV (w). It consists of starting with a weight vector w0 ≤ Λ and of iteratively solving a sequence of convex optimization problems obtained by replacing V with its linear approximation giving wt as a function of wt−1, for t = 1, . . . , T : wt ∈ argmin‖w‖≤Λ U(w) − δV (wt−1) ·w. This problem can be rewritten in our context as the following:\nmin ‖w‖≤Λ,s m∑ i=1 si − δV (wt−1) ·w (22)\nsubject to (si ≥ −w · xi) ∧ [ si ≥ 1\nγ\n( w · xi − (1 + γ)b1i )] .\nThe problem is equivalent to a QP (quadratic-programming) problem since the quadratic constraint can be replaced by a term of the form λ‖w‖2 in the objective and thus can be tackled using any standard QP solver. We propose an algorithm that iterates along different local minima, but with the guarantee of reducing the function at every change of local minimum. The algorithm is simple and is based on the observation that the function Lγ is positive homogeneous: for any η > 0 and (r,b),\nLγ(ηr, ηb) = −ηb21ηr<ηb2 − ηr1ηb2≤ηr≤ηb1\n+ ηr − (1 + γ)ηb1\nγ 1ηb1<ηr<η(1+γ)b1 = ηLγ(r,b).\nMinimizing the objective function of (21) in a fixed direction u, ‖u‖ = 1, can be reformulated as follows: min0≤η≤Λ ∑m i=1 Lγ(ηu · xi,bi). Since for u · xi ≤ 0 the function η → Lγ(ηu · xi,bi)\nis constant equal to −b2i the problem is equivalent to solving min0≤η≤Λ ∑\nu·xi>0 Lγ(ηu · xi,bi). Moreover, because Lγ is positive homogeneous, for all i ∈ [1,m] with u ·xi > 0, Lγ(ηu ·xi,bi) = (u · xi)Lγ(η,bi/(u · xi)). But η 7→ Lγ(η,bi/(u · xi)) is a v-function and thus the problem can efficiently optimized using the combinatorial algorithm for the no-feature case (Section 4.1). This leads to the optimization algorithm shown in Figure 5. The last step each iteration of our algorithm can be viewed as a line search and this is in fact the step that reduces the objective function the most in practice. This is because we are then precisely minimizing the objective function even though this is for some fixed direction. Since in general this line search does not find a local minimum (we are likely to decrease the objective value in other directions that are not the one in which the line search was performed) running DCA helps us find a better direction for the next iteration of the line search."
    }, {
      "heading" : "5 Experiments",
      "text" : "Here, we report the results of some preliminary experiments demonstrating the benefits of our algorithm. All of our experiments are done on synthetic data, this is becasue despite the fact that experiments with data from online auctions exist in the literature [6], this data is not available to the public for confidentiality reasons. To the best of our knowledge no publicly available data set is available for online auctions and none has available features.\nThe first test we perform evaluates the speed of our algorithm in the simple non-feature case. Figure 6 depicts the time needed for our sorting algorithm to find the optimal solution compared to the naı̈ve approach of evaluating the loss at each point on a 4-Core 2.6 GHz AMD processor with 7GB of RAM. The time our algorithm takes to solve the problem with 2000 points is less than a second, whereas the naı̈ve approach required more than 10 minutes to find the solution. This shows the potential for scalability of our algorithm. Running our algorithm to solve the problem using 20,000 points required only 2.26 seconds.\nSince there is no published baseline to compare our algorithm with when using features, we instead present the results of experiments carried out with different loss functions. We propose the following experimental setup: points xi are sampled from a standard Gaussian distribution in R200. A “labeling” vector w ∈ R200, also sampled from a standard Gaussian, is used to generate bi = (|w · xi|, 12 |w · xi|). Notice that the use of the absolute value makes the dependency between\nfeatures and bids no longer linear. This setup does not introduce any noise in the bids, we deal with this scenario later on.\nWe compare our algorithm against three other natural ways of solving this problem.\n1. Using ridge regression to learn the highest bid b1. 2. Using the convex surrogate depicted in Figure 1(b):\nLα(r,b) =\n{ −r r < b1 + α(b2 − b1)(\n(1−α)b1+αb2 α(b1−b2)\n) (r − b1) otherwise.\n3. Minimizing the loss ignoring the values of xi, i.e. solve the following problem minr≤Λ ∑n i=1 L(r,bi).\nIt is worth mentioning that the third approach is really similar to what advertisement exchanges currently do to suggest reserve prices to publishers. In fact, in view of equation (1), this is equivalent to estimating the empirical distribution of bids and optimizing the expected revenue with respect to this empirical distribution as it is done in [17] and [5].\nFor all our experiments, the parameters Λ, γ and α were tuned via 10-fold cross validation. The test set was a collection of 20, 000 examples drawn from the same distribution. The experiment was repeated 20 times and the results shown in Figure 7 are the mean performance of each learning algorithm.\nBecause the algorithm presented in this paper converges to a local minimum, the choice of a good starting point is important. The initial point used here was the solution to\nmin ‖w‖≤Λ n∑ i=1 Lα(w · xi,bi).\nBecause the square loss used in ridge regression is clearly not calibrated with respect to L (it is symmetric around b1 whereas L is not), we would expect the performance of this algorithm to be bad. Indeed, it can be seen in Figure 7 that the test error is in fact the worst of the four algorithms. What is surprising is that all the competing algorithms behave in a similar way for big values of m. This emprical evaluation supports the argument against convex surrogate losses presented in Section 3.1, it also presents a convincing argument in favor of the use of features.\nBy using our better calibrated loss Lγ we see an improvement on performance of as much as 30% . This results are impressive considering that we are optimizing a non-convex function with potentially an exponential number of local minima.\nWe also present results on the performance of the DC algorithm as a function of the parameter γ. As the theory suggestes, smaller values of γ account for a greater variance whereas greater values of γ reduce the variance of the test error but on average perform worse. Our algorithm also converges faster for bigger values of γ. This is also expected because a greater value of γ makes the function Lγ smoother and hence easier to optimize.\nFinally, to analyze how our algorithm performs in the presence of noise we sampled the feature vectors xi from a uniform distribution in the cube [0, 1]50 we chose our “labeling” functions to be w1 =\n1√ 50 1, where 1 is the vector with all coordinates equal to 1 and w2 = 12w1. The bids for our\nexperiments are generated as follows: b1i = max ( (w1 · xi + σ )+, (wT2 · xi + σ )+ ) b2i = min ( (w1 · xi + σ )+, (w2 · xi + σ )+ ) ,"
    }, {
      "heading" : "100 10−1 10−2 10−3 10−4 10−5",
      "text" : "size 6400\nσ 0 .1 .2 .3 .4 .5\nwhere z+ := max(z, 0), ∼ N (0, 1) is a Gaussian random variable, and σ takes values in the set {0, .1, . . . , .5}.. We measured the performance of our learning algorithm as a function of the noise added to the features. Our algorithm was trained on a sample of size 8,000 points and tested on a sample of 8,000 points. Table 1 shows the mean revenue for different algorithm. The first row corresponds to the case where no reserve price is set. The second row represents the revenue made when using our simple sorting algorithm that ignores the feature vectors. Row three shows the performance of our algorithm and the last row shows the maximal possible mean revenue, i.e. the mean of the highest bids. The results of Table 1 show that in the separable case our algorithm achieves perfect performance, as it was expected. Furthermore, when using features to learn, the revenue obtained is increased by 20%, confirming the idea that learning with with features is beneficial.\nAs the amount of noise increases in the sample, the features become less relevant and the improvement in performance becomes smaller but never zero. This problem affects all learning algorithms, as no learning algorithm can perform well if the features are irrelevant to the learning task at hand."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a comprehensive theoretical and algorithmic analysis of the learning problem of revenue optimization in second-price auctions with reserve. The specific properties of the loss function for this problem required a new analysis and new learning guarantees. The algorithmic solutions we presented are practically applicable to revenue optimization problems for this type of auctions in most realistic settings. Our experimental results further demonstrate their effectiveness. Much of the analysis, in particular our calibration study, and algorithms presented can also be of interest in other learning areas."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Afshin Rostamizadeh and Umar Syed for numerous discussions about the topic of this work.This work was partly funded by the NSF award IIS-1117591."
    }, {
      "heading" : "A Proofs for learning guarantees",
      "text" : "A.1 Contraction lemma\nThe following is a version of Talagrand’s contraction lemma [11]. Since our definition of Rademacher complexity does not use absolute values as in [11], we give an explicit proof below.\nLemma 12. Let H be a hypothesis set of functions mapping X to R and Ψ1, . . . ,Ψm, µ-Lipschitz functions for some µ > 0. Then, for any sample S of m points x1, . . . , xm ∈ X , the following inequality holds\n1 m E σ [ sup h∈H m∑ i=1 σi(Ψi ◦ h)(xi) ] ≤ µ m E σ [ sup h∈H m∑ i=1 σih(xi) ] = µ R̂S(H).\nProof. The proof is similar to the case where the functions Ψi are all equal. Fix a sample S = (x1, . . . , xm). Then, we can rewrite the empirical Rademacher complexity as follows:\n1 m E σ [ sup h∈H m∑ i=1 σi(Ψi ◦ h)(xi) ] = 1 m E σ1,...,σm−1 [ E σm [ sup h∈H um−1(h) + σm(Ψm ◦ h)(xm) ]] ,\nwhere um−1(h) = ∑m−1 i=1 σi(Ψi ◦h)(xi). Assume that the suprema can be attained and let h1, h2 ∈ H be the hypotheses satisfying\num−1(h1) + Ψm(h1(xm)) = sup h∈H um−1(h) + Ψm(h(xm))\num−1(h2)−Ψm(h2(xm)) = sup h∈H um−1(h)−Ψm(h(xm)).\nWhen the suprema are not reached, a similar argument to what follows can be given by considering instead hypotheses that are -close to the suprema for any > 0.\nBy definition of expectation and because σm is uniform in {−1,+1} we have\nE σm [ sup h∈H um−1(h) + σm(Ψm ◦ h)(xm) ]\n= [1\n2 sup h∈H\num−1(h) + (Ψm ◦ h)(xm) + 1\n2 sup h∈H\num−1(h)− (Ψm ◦ h)(xm) ]\n= 1\n2 [um−1(h1) + (Ψm ◦ h1)(xm)] +\n1 2 [um−1(h2)− (Ψm ◦ h2)(xm)].\nLet s = sgn(h1(xm)− h2(xm)). Then, the previous equality implies\nE σm [ sup h∈H um−1(h) + σm(Ψm ◦ h)(xm) ]\n= 1\n2 [um−1(h1) + um−1(h2) + sµ(h1(xm)− h2(xm))]\n= 1\n2 [um−1(h1) + sµh1(xm)] +\n1 2 [um−1(h2)− sµh2(xm)]\n≤ 1 2 sup h∈H [um−1(h) + sµh(xm)] + 1 2 sup h∈H [um−1(h)− sµh(xm)]\n= E σm [ sup h∈H um−1(h) + σmµh(xm) ] .\nWhere we have used the fact that Ψm is µ−Lipchitz for the first equality , whereas the last equality is true by definition of expectation over σm.\nProceeding in the same way for all other σis (i 6= m) proves the lemma.\nA.2 Margin bounds\nTheorem 7. If γ ∈ (0, 1] is fixed and S denotes a sample of size m, then for any δ > 0, with probability at least 1− δ over the choice of the sample S\nL(h) ≤ L̂γ(h) + 2\nγ Rm(H) +M √ log 1δ 2m , (23)\nfor all h ∈ H .\nProof. Let Lγ,H denote the family of functions {(x,b)→ Lγ(h(x), b) : h ∈ H}. The loss function Lγ is 1γ -Lipschitz since the slope of the lines defining it is at most 1 γ . Thus, using the contraction lemma (Lemma 12) as in the proof of Proposition 1 gives Rm(Lγ,H) ≤ 1γRm(H). The application of a standard Rademacher complexity bound to the family of functions Lγ,H then shows that for any δ > 0, with probability at least 1− δ, for any h ∈ H , the following holds:\nLγ(h) ≤ L̂γ(h) + 2\nγ Rm(H) +M √ log 1δ 2m .\nCorollary 13. For any δ > 0, with probability at least 1 − δ over the choice of a sample S of size m, the following holds for all γ ∈ (0, 1] and h ∈ H:\nLγ(h) ≤ L̂γ(h) + 2\nγ Rm(H) +M K(γ)√ m + √ log 1δ 2m  . (24) With K(γ) = √ log log2 1/γ.\nProof. Consider two sequences (γk)k≥1 and ( k)k≥1, with k ∈ (0, 1). By theorem 7, for any fixed k ≥ 1,\nP [ L(h)− L̂γk(h)> 2\nγk Rm(H) +M k\n] ≤ exp(−2m 2k). (25)\nChoose k = + √ log k m , then, by the union bound,\nP [ ∃k : L(h)− L̂γk(h) > 1\nγk Rm(H) +M k ] ≤ ∑ k≥1 exp [ − 2m( + √ (log k)/m)2 ] ≤ (∑ k≥1 1/k2 ) exp(−2m 2)\n= π2\n6 exp(−2m 2) ≤ 2 exp(−2m 2).\nFor any γ ∈ (0, 1], there exists k ≥ 1 such that γ ∈ (γk, γk−1) with γk = 1/2k. For such a k, 1 γk−1 ≤ 1γ , γk−1 ≤ γ 2 , and √ log(k − 1) = √ log log2(1/γk−1) ≤ √ log log2(1/γ). Since for any h ∈ H , Lγk−1(h) ≤ Lγ(h), we can write\nP [ ∃k : L(h)−L̂γ(h)> 2\nγ Rm(H)+M\n( K(γ) + )] ≤ exp(−2m 2),\nwhich concludes the proof."
    }, {
      "heading" : "B Combinatorial algorithm",
      "text" : "B.1 Solution property\nWe will show that problem (19) admits a solution r∗ = b1i for some i. We will need the following definition. Definition 14. For a value of r ∈ R, define the following subset of R:\nΩ(r) = { |r < b1i ↔ r + ≤ b1i }\nWe will drop the dependency on r when it is understood what value of r we are referring to. Lemma 15. Let r 6= b1i for all i. If > 0 is such that [− , ] ⊂ Ω(r) then F (r + ) < F (r) or F (r − ) ≤ F (r). The condition that r 6= b1i for all i implies that there exists small enough that satisfies ∈ Ω(r).\nProof. Let vi = Vi(r,bi) and vi( ) = Vi(r+ ,bi). For ∈ Ω(r) define the setsD( ) = {i | vi( ) ≤ vi} and I( ) = {i | vi( ) > vi}. If∑\ni∈D( ) vi + ∑ i∈I( ) vi > ∑ i∈D( ) vi( ) + ∑ i∈I( ) vi( )\nThen by definition, F (r) > F (r + ) and the result is proven. If this inequality is not satisfied, then by grouping indices in D( ) and I( ) we must have that∑\ni∈D( ) vi − vi( ) ≤ ∑ i∈I( ) vi( )− vi (26)\nNotice that vi( ) ≤ vi if and only if vi(− ) ≥ vi. Indeed, the function Vi(r + η,bi) is monotone for η ∈ [− , ] as long as [− , ] ⊂ Ω which is true by the choice of . This fact can easily be seen in Figure 8. Hence D( ) = I(− ), similarly I( ) = D(− ) Furthermore, because Vi(r + η,bi) is also concave for η ∈ [− , ]. We must have\n1 2 (vi(− ) + vi( )) ≤ vi. (27)\nFrom equation (27) we can obtain the following inequalities:\nvi(− )− vi ≤ vi − vi( ) for i ∈ D( ) (28) vi( )− vi ≤ vi − vi(− ) for i ∈ I( ) (29)\nCombining inequalities (28), (26) and (29) we obtain∑ i∈I(− ) vi(− )− vi ≤ ∑ i∈D(− ) vi − vi(−e).\nBy rearranging the terms in the inequality we can easily see that F (r − ) ≤ F (r). Lemma 16. Under the conditions of Lemma 15, if F (r + ) ≤ F (r) then F (r + λ ) ≤ F (r) for every λ that satisfies λ ∈ Ω if and only if ∈ Ω.\nProof. The proof follows the same ideas used in the previous lemma. By hypothesis we know that∑ D( ) vi − vi( ) ≥ ∑ i∈I( ) vi( )− vi. (30)\nIt is also clear that I( ) = I(λ ) and D( ) = D(λe). Furthermore, the same concavity argument of lemma 15 also yields:\nvi( ) ≥ λ− 1 λ vi + 1 λ vi(λ ). (31)\nApplying inequality (31) in (30) we obtain\n1\nλ ∑ D(λ ) vi − vi(λ ) ≥ 1 λ ∑ I(λ ) vi(λ )− vi\nBecause λ > 0 we can multiply the inequality by λ to get an inequality similar to (30) which implies that F (r + λ ) ≤ F (r).\nProposition 10. Problem (19) admits a solution r∗ that satisfies r∗ = b1i for some i ∈ [1,m].\nProof. Let r 6= b1i for every i. By Lemma 15, we can choose 6= 0 small enough with F (r + ) ≤ F (r). Furthermore if λ = mini b 1 i−r | | then λ satisfies the hypotheses of Lemma 16. Hence, F (r) ≥ F (r + λ ) = F (bi∗), where i∗ is the minimizer of b 1 i−r ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Second-price auctions with reserve play a critical role for modern search engine<lb>and popular online sites since the revenue of these companies often directly de-<lb>pends on the outcome of such auctions. The choice of the reserve price is the<lb>main mechanism through which the auction revenue can be influenced in these<lb>electronic markets. We cast the problem of selecting the reserve price to optimize<lb>revenue as a learning problem and present a full theoretical analysis dealing with<lb>the complex properties of the corresponding loss function. We further give novel<lb>algorithms for solving this problem and report the results of several experiments<lb>demonstrating their effectiveness.",
    "creator" : "LaTeX with hyperref package"
  }
}