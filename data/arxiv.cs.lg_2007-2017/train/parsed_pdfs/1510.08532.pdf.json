{
  "name" : "1510.08532.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "zhihua@sjtu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n08 53\n2v 1\n[ cs\n.L G\n] 2\n9 O\nct 2\n01 5\nThe Singular Value Decomposition, Applications\nand Beyond\nZhihua Zhang Shanghai Jiao Tong University\nzhihua@sjtu.edu.cn\nContents"
    }, {
      "heading" : "1 Introduction 2",
      "text" : "1.1 Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Notation and Definitions . . . . . . . . . . . . . . . . . . 5"
    }, {
      "heading" : "2 Preliminaries 7",
      "text" : "2.1 Kronecker Products and Vectorization Operators . . . . . 7 2.2 Majorization . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Derivatives and Optimality . . . . . . . . . . . . . . . . . 9"
    }, {
      "heading" : "3 The Singular Value Decomposition 12",
      "text" : "3.1 Formulations . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 Matrix Properties via SVD . . . . . . . . . . . . . . . . . 19 3.3 Matrix Concepts via SVD . . . . . . . . . . . . . . . . . . 21 3.4 Generalized Singular Value Decomposition . . . . . . . . . 23"
    }, {
      "heading" : "4 Applications of SVD: Case Studies 29",
      "text" : "4.1 The Matrix MP Pseudoinverse . . . . . . . . . . . . . . . 30 4.2 The Procrustes Problem . . . . . . . . . . . . . . . . . . . 32 4.3 Subspace Methods: PCA, MDS, FDA, and CCA . . . . . . 33"
    }, {
      "heading" : "5 The QR and CUR Decompositions 37",
      "text" : "5.1 The QR Factorization . . . . . . . . . . . . . . . . . . . . 37\nii\niii\n5.2 The CUR Decomposition . . . . . . . . . . . . . . . . . . 38"
    }, {
      "heading" : "6 Variational Principles 41",
      "text" : "6.1 Variational Properties for Eigenvalues . . . . . . . . . . . 42 6.2 Variational Properties for Singular Values . . . . . . . . . 46 6.3 Appendix: Application of Matrix Differentials . . . . . . . 48"
    }, {
      "heading" : "7 Unitarily Invariant Norms 52",
      "text" : "7.1 Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . 53 7.2 Symmetric Gauge Functions . . . . . . . . . . . . . . . . . 56 7.3 Unitarily Invariant Norms via SGFs . . . . . . . . . . . . . 58 7.4 Properties of Unitarily Invariant Norms . . . . . . . . . . . 60"
    }, {
      "heading" : "8 Subdifferentials of Unitarily Invariant Norms 67",
      "text" : "8.1 Subdifferentials . . . . . . . . . . . . . . . . . . . . . . . 68 8.2 Applications . . . . . . . . . . . . . . . . . . . . . . . . . 73"
    }, {
      "heading" : "9 Matrix Low Rank Approximation 77",
      "text" : "9.1 Basic Results . . . . . . . . . . . . . . . . . . . . . . . . . 78 9.2 Approximate Matrix Multiplication . . . . . . . . . . . . . 82"
    }, {
      "heading" : "10 Large-Scale Matrix Approximation 86",
      "text" : "10.1 Randomized SVD . . . . . . . . . . . . . . . . . . . . . . 87 10.2 Kernel Approximation . . . . . . . . . . . . . . . . . . . . 93 10.3 The CUR Approximation . . . . . . . . . . . . . . . . . . 96\nAcknowledgements 99\nReferences 100\nAbstract\nThe singular value decomposition (SVD) is not only a classical theory in matrix computation and analysis, but also is a powerful tool in machine learning and modern data analysis. In this tutorial we first study the basic notion of SVD and then show the central role of SVD in matrices. Using majorization theory, we consider variational principles of singular values and eigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss unitarily invariant norms, which are then used to formulate general results for matrix low rank approximation. We study the subdifferentials of unitarily invariant norms. These results would be potentially useful in many machine learning problems such as matrix completion and matrix data classification. Finally, we discuss matrix low rank approximation and its recent developments such as randomized SVD, approximate matrix multiplication, CUR decomposition, and Nyström approximation. Randomized algorithms are important approaches to large scale SVD as well as fast matrix computations.\n1 Introduction\nThe singular value decomposition (SVD) is a classical matrix theory and a key computational technique, and it has also received wide applications in science and engineering. Compared with an eigenvalue decomposition (EVD) which only works on some of square matrices, SVD applies to all matrices. Moreover, many matrix concepts and properties such as matrix pseudoinverses, variational principles and unitarily invariant norms can be induced from SVD. Thus, SVD plays a fundamental role in matrix computation and analysis.\nFurthermore, due to recent great developments of machine learning, data mining and theoretical computer science, SVD has been found to be more and more important. It is not only a powerful tool and theory but also an art. SVD makes matrices become a “Language\" of data science.\nThe terminology of singular values has been proposed by Horn in 1950 and 1954 [Horn, 1951, 1954]. The first proof of the SVD for general m × n matrices might be given by Eckart and Young [1939]. But the theory of singular values can date back to the 19th century when it had been studied by the Italian differential geometer E. Beltrami, the French algebraist C. Jordan, the English mathematician J. J. Sylvester,\n2"
    }, {
      "heading" : "1.1. Roadmap 3",
      "text" : "the French mathematician L. Autonne, etc. Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values.\nThere is a rich literature involving singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides exhaustive studies about inequalities of singular values as well as unitarily invariant norms, and the primary focus is on the matrix theory. The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.\nThis tutorial is motivated by recent successful applications of SVD in machine learning and theoretical computer science [Hastie et al., 2001, Burges, 2010, Halko et al., 2011, Woodruff, 2014b, Mahoney, 2011, Blum et al., 2015]. The primary focus is on a perspective of machine learning. The main purpose of the tutorial includes two aspects. First, it provides a systematic tutorial to the SVD theory and illustrates its functions in matrix and data analysis. Second, it provides an advanced review about recent developments of the SVD theory in applications of machine learning and theoretical computer science."
    }, {
      "heading" : "1.1 Roadmap",
      "text" : "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997]. In Chapter 2 we review some preliminaries such as Kronecker produces and vectorization operators, majorization theory, and derivatives."
    }, {
      "heading" : "4 Introduction",
      "text" : "In Chapter 3 we introduce the basic notion of SVD, including the existence, construction, and uniqueness. We then rederive some important matrix concepts and properties via SVD. We also study generalized SVD problems, which are concerned with joint decomposition of two matrices. In Chapter 4 we further illustrate the application of SVD in definition of the matrix pseudoinverse and solution of the Procrustes analysis problem. We discuss the role that SVD plays in subspace machine learning methods.\nFrom the viewpoint of computation and modern data analysis, matrix factorization techniques should be the most important issue of matrices. In Table 1.1 we summary matrix factorization methods, which are categorized into three types. In particular, the Polar decomposition, SVD, and spectral decomposition consider geometric representation of a data matrix, whereas the CX, CUR, and Nyström dcompositions consider a compact representation of the data themselves. That is, the latters use a portion of the data to represent the whole data. The primary focus of the QR and Cholesky decomposition is on fast computation. In Chapter 5 we give reviews about the QR and CUR decompositions.\nIn Chapter 6 we consider variational principles for singular values and eigenvalues. Specifically, we apply matrix differential calculus to rederive the von Neumann theorem [Neumann, 1937] and the Ky Fan theorem [Fan, 1951]. Accordingly, we give some inequalities for singular values and eigenvalues.\nBuilt on the inequalities for singular values, Chapter 7 discusses unitarily invariant norms. Unitarily invariant norms include the nuclear norm, Frobenius norm and spectral norm as their special cases. There is a one-to-one correspondence between a unitarily invariant norm of a matrix and a symmetric gauge function on the singular values of the matrix. This helps us to establish properties of unitarily invariant norms.\nIn Chapter 8 we study subdifferentials of unitarily invariant norms. We especially present the subdifferentials of the spectral norm and the nuclear norm as well as the applications in matrix low rank approximation. We illustrate several examples in optimization, which are solved via the subdifferentials of the spectral and nuclear norms. The subdif-"
    }, {
      "heading" : "1.2. Notation and Definitions 5",
      "text" : "ferentials of unitarily invariant norms would have potentially useful in machine learning and optimization.\nMatrix low rank approximation is a promising theme in machine learning and theoretical computer science. Chapter 9 gives two important theorems about matrix low rank approximation based on errors of unitarily invariant norms. The first one is an extension of the ordinal least squares estimation problem. The second one was proposed by Mirsky [1960], which is an extension of the novel Eckart Young theorem [Eckart and Young, 1936]. We also discuss approximate matrix multiplication, which can be regarded as an inverse process of matrix low rank approximation.\nIn Chapter 10 we study randomized SVD, CUR approximation, and Nyström methods to make the applications scalable. The randomized SVD and CUR approximation can be also viewed as matrix low rank approximation techniques. The Nyström approximation is a special case of the CUR decomposition and has been widely used to speed up kernel methods."
    }, {
      "heading" : "1.2 Notation and Definitions",
      "text" : "Throughout this tutorial, vectors and matrices are denoted by boldface lowercase letters and boldface uppercase letters, respectively. Rn+ = {u = (u1, . . . , un)T ∈ Rn : uj ≥ 0 for j = 1, . . . , n} and Rn++ = {u = (u1, . . . , un)\nT ∈ Rn : uj > 0 for j = 1, . . . , n}. Furthermore, if u ∈ Rn+ (or u ∈ Rn++), we also denote u ≥ 0 (or u > 0).\nGiven a vector x = (x1, . . . , xn) T ∈ Rn, let |x| = (|x1|, . . . , |xn|)T ,\nlet ‖x‖p = ( ∑n\ni=1 |xi|p)1/p for p ≥ 1 be the ℓp-norm of x, and let diag(x) be an n× n diagonal matrix with the ith diagonal element as xi.\nLet [m] = {1, 2, . . . ,m}, Im be the m × m identity matrix, 1m be the m × 1 vector of ones, and 0 be the zero vector or matrix with appropriate size. Let A⊕B = [ A 0\n0 B\n]\n.\nFor a matrix A = [a1,a2, . . . ,an] = [aij ] ∈ Rm×n, AT denotes the transpose of A, rank(A) denotes the rank, range(A) represents the range which is the space spanned by the columns (i.e., range(A) ="
    }, {
      "heading" : "6 Introduction",
      "text" : "{y ∈ Rm : y = Ax for some x ∈ Rn} = span{a1,a2, . . . ,an}), null(A) is the null space (i.e., null(A) = {x : Ax = 0}), and for p = min{m,n} dg(A) denotes the p-vector with aii as the ith element. Sometimes we also use Matlab Colon to represent a submatrix of A. For example, let I ⊂ [m] and J ⊂ [n]. AI,J denotes the submatrix of A with rows indexed by I and columns indexed by J , AI,: consists of those rows of A in I, and A:,J consists of those columns of A in J .\nLet ‖A‖F = √ ∑ ij a 2 ij denote the Frobenius norm, ‖A‖2 denote the spectral norm, and ‖A‖∗ denote the nuclear norm. When A is square, we let A−1 be the inverse (if exists) of A, tr(A) =\n∑n i=1 aii be the\ntrace, and det(A) be the determinant of A.\nAn m × m real matrix U is symmetric if AT = A, and skewsymmetric if AT = −A, and normal if AAT = ATA. Clearly, symmetric and skew-symmetric matrices are normal. An m×m real matrix U is said to be orthonormal (or orthogonal) if UTU = UUT = Im. An m× n real matrix Q for m > n is column orthonormal (or column orthogonal) if QTQ = In, and a column orthonormal Q is always able to be extended to an orthonormal matrix. A matrix M ∈ Rm×m is said to be positive semidefinite (PSD) or positive definite if for any nonzero vector x ∈ Rm xTMx ≥ 0 or xTMx > 0.\n2 Preliminaries\nIn this chapter we present some preliminaries, including Kronecker products and vectorization operators, majorization theory, and derivatives. We list some basic results that will be used in this monograph but omit their detailed derivations."
    }, {
      "heading" : "2.1 Kronecker Products and Vectorization Operators",
      "text" : "Given two matricesA ∈ Rm×n and B ∈ Rp×q, the the Kronecker product of A and B is defined by\nA⊗B ,\n\n   a11B · · · a1nB ... . . . ...\nam1B · · · amnB\n\n   ,\nwhich is mp× nq. The following properties can be found in Muirhead [1982].\nProposition 2.1. The Kronecker product has the following properties.\n(a) (αA) ⊗ (βB) = αβ(A⊗B) for any scalars α, β ∈ R. (b) (A⊗B)T = AT ⊗BT .\n7"
    }, {
      "heading" : "8 Preliminaries",
      "text" : "(c) (A⊗B)⊗C = A⊗ (B⊗C). (d) If A and C are both m × n and B is p × q, then (A+C) ⊗ B =\nA⊗B+C⊗B and B⊗ (A+C) = B⊗A+B⊗C. (e) If A is m× n, B is p× q, C is n× r, and D is q × s, then\n(A⊗B)(C⊗D) = (AC)⊗ (BD).\n(f) If U and V are both orthogonal matrices, so is U⊗V. (g) If A and B are symmetric positive semidefinite (SPSD), so is A⊗B.\nKronecker products often work with vectorization operators together. Let vec(A) = (a11, . . . , am1, a12, . . . , amn) T ∈ Rmn be vectorization of the matrix A ∈ Rm×n. The following lemma gives the connection between Kronecker products and vectorization operators.\nLemma 2.1.\n(1) If B is p×m, X is m× n, and C is n× q, then\nvec(BXC) = (CT ⊗B)vec(X).\n(2) If A ∈ Rm×n, B ∈ Rn×p, and C ∈ Rp×m, then\ntr(ABC) = (vec(AT ))T (Im ⊗B)vec(C).\n(3) If A ∈ Rm×p, X ∈ Rn×p, B ∈ Rn×n, and C ∈ Rp×m, then\ntr(AXTBXC) = (vec(X))T ((CA)T ⊗B)vec(X) = (vec(X))T ((CA)⊗BT )vec(X)."
    }, {
      "heading" : "2.2 Majorization",
      "text" : "Given a vector x = (x1, . . . , xn) T ∈ Rn, let x↓ = (x↓1, . . . , x↓n) be such a permutation of x that x↓1 ≥ x ↓ 2 ≥ · · · ≥ x↓n. Given two vectors x and y ∈ R n, x ≥ y means xi − yi ≥ 0 for all i ∈ [n]. We say that x is majorized by y (denoted x ≺ y) if ∑ki=1 x↓i ≤ ∑k i=1 y ↓ i for k = 1, . . . , n − 1 and ∑n\ni=1 x ↓ i = ∑n i=1 y ↓ i . Similarly, x ≻ y if ∑k i=1 x ↓ i ≥ ∑k i=1 y ↓ i for\nk = 1, . . . , n−1 and ∑ni=1 x↓i = ∑n i=1 y ↓ i ."
    }, {
      "heading" : "2.3. Derivatives and Optimality 9",
      "text" : "We say that x is weakly submajorized by y (denoted x ≺w y) if ∑k\ni=1 x ↓ i ≤ ∑k i=1 y ↓ i for k = 1, . . . , n, and x is weakly superrmajorized\nby y (denoted x ≺w y) if ∑ki=1 x↓i ≥ ∑k i=1 y ↓ i for k = 1, . . . , n,\nAn n × n matrix W = [wij ] is said to be doubly stochastic if the wij ≥ 0, ∑n j=1wij = 1 for all i ∈ [n], and ∑n i=1 wij = 1 for all j ∈ [n]. Note that if Q = [qij ] ∈ Rn×n is orthonormal, then W , [q2ij ] is a doubly stochastic matrix. It is thus called orthostochastic.\nThe following three lemmas are classical results in majorization\ntheory. They will be used in investigating unitarily invariant norms.\nLemma 2.2. [Hardy et al., 1951] Given two vectors x,y ∈ Rn, then x ≺ y if and only if there exists a doubly stochastic matrix W such that x = Wy.\nLemma 2.3 (Birkhoff). Let W ∈ Rn×n. Then it is a doubly stochastic matrix if and only if it can be expressed as a convex combination of a set of permutation matrices.\nLemma 2.4. Let u1, . . . , un and v1, . . . , vn be given nonnegative real numbers such that u1 ≥ · · · ≥ un and v1 ≥ · · · ≥ vn. If k∏\ni=1\nui ≤ k∏\ni=1\nvi for k ∈ [n],\nthen k∑\ni=1\nui ≤ k∑\ni=1\nvi for k ∈ [n].\nMore generally, assume f is a real-valued function such that f(exp(u)) is increasing and convex. Then\nk∑\ni=1\nf(ui) ≤ k∑\ni=1\nf(vi) for k ∈ [n]."
    }, {
      "heading" : "2.3 Derivatives and Optimality",
      "text" : "First let f : X ⊂ Rn → R be a continuous function. The directional derivative of f at x̄ in a direction u ∈ X is defined as\nf ′(x̄;u) = lim t↓0 f(x̄+ tu)− f(x̄) t ,"
    }, {
      "heading" : "10 Preliminaries",
      "text" : "when this limit exists. When the directional derivative f ′(x̄;u) is linear in u (that is, f ′(x̄;u) = 〈a,u〉 for some a ∈ X ) then we say f is (Gâbeaux) differentiable at x̄, with derivative ∇f(x̄) = a. If f is differentiable at every point in X then we say f is differentiable on X .\nWhen f is not differentiable but convex, we consider a notion of\nsubdifferentials. We say z is the subgradient of f at x̄ if it satisfies\nf(x̄) ≤ f(x) + 〈z, x̄ − x〉 for all points z ∈ X .\nThe set of subgradients is called the subdifferential, and denoted by ∂f(x̄). The subdifferential is always a closed convex set. The following result shows a connection between subgradients and directional derivatives.\nLemma 2.5 (Max Formula). If the function f : X → (−∞,+∞] is convex, then any point x̄ in core(domf) and any direction u in X satisfy\nf ′(x̄;u) = max { 〈z,u〉 : z ∈ ∂f(x̄) } .\nThe further details of these results can be found from Borwein and Lewis [2006]. The following lemma then shows the fundamental role of subgradients in optimization.\nLemma 2.6. For any proper convex function f : X → (−∞,+∞], the point x̄ is a minimizer of f if and only if the condition 0 ∈ ∂f(x̄) holds.\nNow let f be a differentiable function from Rm×n to R. For a matrix X = [xij ] ∈ Rm×n, df(X)dX = ( df dxij )\n(m× n) defines the derivative of f w.r.t. X. The Hessian matrix of f w.r..t. X is defined as d\n2f(X) dvec(X)dvec(X)T ,\nwhich is an mn×mn matrix. Let us see an example.\nExample 2.1. We define the function f as\nf(X) = tr(XTMX),\nwhere M = [mij ] ∈ Rm×m is a given constant matrix. It is directly computed that dfdxij = ∑m l=1(mil+mli)xlj . This implies that df dX = (M+ MT )X. In fact, the derivative can be computed as follows. Compute\ndf = tr(dXTMX+XTMdX) = tr((M+MT )XdXT )."
    }, {
      "heading" : "2.3. Derivatives and Optimality 11",
      "text" : "We thus have that dfdX = (M+M T )X.\nAdditionally, it follows from Lemma 2.1 that f(X) = vec(X)T (In⊗ M)vec(X). Thus, we have\ndf\ndvec(X) = vec\n( df\ndX\n)\n= [In ⊗ (M+MT )]vec(X),\nand hence, d2f(X)\ndvec(X)dvec(X)T = In ⊗ (M+MT ).\n3 The Singular Value Decomposition\nThe singular value decomposition (SVD) is a classical matrix theory and computational tool. In modern data computation and analysis, SVD becomes more and more important. In this chapter we aim to provide a systematical review about the basic principle of SVD.\nWe will see that there are four approaches to SVD. The first approach is depart from the spectral decomposition of a symmetric positive semidefinite (SPSD) matrix. The second approach gives a construction process via induction. In the third approach the SVD problem is equivalently formulated into an eigenvalue decomposition problem of a symmetric matrix (see Theorem 3.5). The fourth approach is based on the equivalent relationship between the SVD and polar decomposition (see Theorem 3.6).\nWe also study uniqueness of SVD (see Theorem 3.2 and Corollary 3.3). These results will be used in derivation of subdifferentials of unitarily invariant norms (see Chapter 8). Additionally, we present a generalized SVD (GSVD), which addresses joint decomposition problems of two matrices. When the two matrices form a column orthonormal matrix, the resulting GSVD process is called the CS decomposition.\n12"
    }, {
      "heading" : "3.1. Formulations 13",
      "text" : ""
    }, {
      "heading" : "3.1 Formulations",
      "text" : "Given a nonzero SPSD matrix M ∈ Rn×n, let γi for i = 1, . . . , n be the eigenvalues of M and xi be the corresponding eigenvectors. That is,\nMxi = γixi, i = 1, . . . , n. (3.1)\nIt is well known that the xi can be assumed to be mutually orthonormal. Let Γ = diag(γ1, . . . , γn) and X = [x1, . . . ,xn] such that X TX = In. We write (3.1) in matrix form as\nMX = XΓ.\nThis gives rise to an eigenvalue decomposition (EVD) of M:\nM = XΓXT .\nSince the γi are nonnegative, this decomposition is also called a spectral decomposition of the SPSD matrix M.\nNote that the above EVD always exists when M is symmetric but not PSD. However, the current eigenvalues γi are not necessarily nonnegative. Let Γ̂ = diag(|γ1|, . . . , |γn|) and Y = [y1, . . . ,yn] with yi = sgn(γi)xi where sgn(0) = 1. Then the decomposition is reformulated as\nM = XΓ̂Y,\nwhere XTX = In, Y TY = In, and Γ̂ is a nonnegative diagonal matrix. This new formulation defines a singular value decomposition (SVD) of the symmetric matrix M.\nNaturally, a question emerges: does an SVD exist for an arbitrary matrix? Let A ∈ Rm×n of rank r where r ≤ min{m,n}. Without loss of generality, we assume m ≥ n for ease of exposition, because we can consider AT when m < n.\nConsider that AAT is SPSD, so it has the spectral decomposition,\nwhich is defined as\nAAT = UΛUT ,\nwhere Λ = diag(λ1, . . . , λm) and U TU = Im. Since rank(AA T ) = rank(A) = r, AAT has and only has r positive eigenvalues and the corresponding eigenvectors can form a column orthonormal matrix."
    }, {
      "heading" : "14 The Singular Value Decomposition",
      "text" : "Assume that Λr = diag(λ1, λ2, . . . , λr) and Ur = [u1,u2, . . . ,ur] where λ1 ≥ λ2 ≥ · · · ≥ λr are the positive eigenvalues of AAT and Ur is the m × r matrix of the corresponding eigenvectors such that UTr Ur = Ir. Thus, it follows from the spectral decomposition that\nUTr AA TUr = Λr\nand UT−rAA TU−r = 0 where U−r consists of the last m−r columns of U. Thus, we have ATU−r = 0. Let Vr = [v1, . . . ,vr] , ATUrΛ −1/2 r . Then it satisfies VTr Vr = Ir. Note that\nATU(Λ−1/2r ⊕ Im−r) = [Vr,ATU−r] = [Vr,0],\nwhich implies that AT = [Vr,0](Λ 1 2 r ⊕ Im−r)UT = VrΛ 1 2 r U T r . Hence,\nA = UrΣrV T r , (3.2)\nwhere Σr = diag(σ1, σ2, . . . , σr) with σi = λ 1/2 i for i = 1, . . . , r. Clearly, σ1 ≥ σ2 ≥ · · · ≥ σr > 0. We refer to (3.2) as the condensed SVD of A, where σi’s are called the singular values, the columns ui of Ur and the columns vi of Vr are called respectively the left and right singular vectors of A.\nRecall that we always assume that σ1 ≥ σ2 ≥ · · · ≥ σr > 0. Let Σn = diag(σ1, . . . , σr, 0, . . . , 0) be the n × n diagonal matrix, and Un be an m × n column-orthonormal matrix consisting of Ur in the first m×r block. In this case, we can equivalently write the condensed SVD of A as\nA = UnΣnV T , (3.3)\nwhich is called a thin (or reduced) SVD of A. Furthermore, we extend Un to a square orthonormal matrix (denoted U), and Σn to an m× n matrix Σ by adding m−n rows of zeros below. Then SVD can be also expressed as\nA = UΣVT , (3.4)\nwhich is called a full SVD of A.\nAs we have seen, these three expressions are mutually equivalent. We will sometimes use A = UΣVT for the thin SVD for notational simplicity. In a thin SVD version, let us always keep it in mind that"
    }, {
      "heading" : "3.1. Formulations 15",
      "text" : "Σ is square and U or V is column orthonormal. We now present the formal formation of SVD of an arbitrary A ∈ Rm×n in which m ≥ n is not necessarily required.\nTheorem 3.1. Given an arbitrary A ∈ Rm×n, its full SVD defined in (3.4) always exists. Furthermore, the singular values σi are uniquely determined.\nBased on the spectral decomposition of AAT , we have previously shown the existence proof of the SVD theorem. Here we present a constructive proof, which has been widely given in the literature.\nProof. IfA is zero, the result is trivial. Thus, letA be a nonzero matrix. Define σ1 , max‖x‖2=1 ‖Ax‖2, which exists because x 7→ ‖Ax‖2 is continuous and the set {x ∈ Rn : ‖x‖2 = 1} is compact. Moreover, σ1 > 0. Let v1 ∈ Rn be such a vector that σ1 = ‖Av1‖2. Define u1 = Av1/σ1, which satisfies ‖u1‖2 = 1.\nWe extend u1 and v1 to orthonormal matrices U = [u1,U−1] and V = [v1,V−1], respectively. Then\nUTAV =\n[\nσ1 u T 1 AV−1\n0 UT−1AV−1\n]\n, B,\nwhere we use the fact UT−1Av1 = σ1U T −1u1 = 0. Note that\nmax ‖x‖2=1 ‖Bx‖22 = max‖x‖2=1 ‖UTAVx‖22 = max‖x‖2=1 ‖Ax‖22 = σ21 .\nHowever,\n1\nσ21 + z T z\n∥ ∥ ∥ ∥ ∥ B [ σ1 z ]∥ ∥ ∥ ∥ ∥ 2\n2\n≥ σ21 + zTz,\nwhere z = VT−1A Tu1. This implies that z must be zero.\nThe proof is completed by induction. In particular, assume (m − 1) × (n − 1) matrix UT−1AV−1 has a full SVD UT−1AV−1 = ŨΣ̃ṼT . Then A has a full SVD:\nA = [u1,U−1]\n[\n1 0\n0 Ũ\n] [\nσ1 0\n0 Σ̃\n] [\n1 0 0 ṼT\n] [\nvT1 VT−1\n]\n= [u1,U−1Ũ]\n[\nσ1 0\n0 Σ̃\n] [\nvT1 (V−1Ṽ)T\n]\n,"
    }, {
      "heading" : "16 The Singular Value Decomposition",
      "text" : "because the matrices [u1,U−1Ũ] and [v1,V−1Ṽ] are orthonormal.\nAs for the uniqueness of the singular values is due to that the σ2i are eigenvalues of AAT which are unique. Unfortunately, the left and right singular matrices Ur and Vr are not unique. However, we have the following result.\nTheorem 3.2. Let A = UrΣrV T r be a given condensed SVD of A. Assume there are ρ distinct values among the nonzero singular values σ1, . . . , σr, with respective multiplicities ri (satisfying ∑ρ i=1 ri = r). Then A = ŨrΣrṼ T r is a condensed SVD if and only if\nŨr = Ur(Q1 ⊕Q2 ⊕ . . . ⊕Qρ) and Ṽr = Vr(Q1 ⊕Q2 ⊕ . . .⊕Qρ),\nwhere Qi is an arbitrary ri × ri orthonormal matrix. Furthermore, if all the nonzero singular values are distinct, then the Qi are either 1 or −1. In other words, the left and right singular vectors are uniquely determined up to signs.\nProof. Let δ1 > δ2 > . . . > δρ be the ρ distinct values among the σ1, . . . , σr. This implies that\nΣr = δ1Ir1 ⊕ δ2Ir2 ⊕ . . .⊕ δρIrρ. (3.5)\nThe sufficiency follows from the fact that\n(Q1 ⊕ . . .⊕Qρ)(δ1Ir1 ⊕ . . . ⊕ δρIrρ)(QT1 ⊕ . . . ⊕QTρ ) = Σr.\nWe now prove the necessary condition. Consider that range(Ur) = range(A) = range(Ũr) and range(Vr) = range(A T ) = range(Ṽr). Thus, we have\nŨr = UrS and Ṽr = VrT,\nwhere S and T are some r × r orthonormal matrices. Hence, Σr = SΣrT T , or equivalently, ΣrT = SΣr. As in (3.5) for Σ, partition S and T into\nS =\n\n   S11 . . . S1ρ ... . . . ...\nSρ1 . . . Sρρ\n\n   and T =\n\n   T11 . . . T1ρ ... . . . ...\nTρ1 . . . Tρρ\n\n   ,"
    }, {
      "heading" : "3.1. Formulations 17",
      "text" : "where Sij and Tij are ri × rj. It follows from ΣrT = SΣr that δiTii = δiSii for i = 1, . . . , ρ and δiTij = δjSij . As a result, we obtain that Sii = Tii for i = 1, . . . , ρ. Since S and T are orthonormal, we have\nρ ∑\nj=1\nSijS T ij = Iri =\nρ ∑\nj=1\nTijT T ij.\nNote that ∑ρ\nj=1TρjT T ρj = ∑ρ j=1 δ2 j\nδ2ρ SρjS\nT ρj, which implies that\n∑\nj<ρ\n[ 1− δ2j δ2ρ ] SρjS T ρj = 0. (3.6)\nSince 1 − δ 2 j\nδ2ρ < 0 for j < ρ and SρjS\nT ρj is always PSD, we must have\nSρj = 0 for all j < ρ, for otherwise, if there were a k < ρ such that Sρk 6= 0, there would exist a nonzero x ∈ Rrρ such that xTSρkSTρkx > 0. It would lead to\n∑\nj<ρ\n[ 1− δ2j δ2ρ ] xTSρjS T ρjx < 0,\nwhich conflicts with (3.6). Accordingly, Sρj = Tρj = 0 for all j < ρ, and hence, SρρS T ρρ = TρρT T ρρ = Irρ . It also follows from the orthogonality of S and of T that for any i < ρ,\n0 = ρ ∑\nj=1\nSijS T ρj = SiρS T ρρ and 0 =\nρ ∑\nj=1\nTijT T ρj = TiρT T ρρ,\nwhich leads to Siρ = Tiρ = 0 for i < ρ.\nSimilarly, consider the ρ−1, ρ−2, . . . , 2 cases. We have Sij = Tij = 0 for i 6= j, Sii = Tii and SiiSTii = TiiTTii = Iri for i ∈ [ρ]. As a result, setting Qi = Sii completes the proof.\nWe now extend the result in Theorem 3.2 to the full SVD and thin\nSVD of A. The following corollary is immediately obtained.\nCorollary 3.3. Let A = UΣVT be a given full SVD of A ∈ Rm×n. Then A = ŨΣṼT is a full SVD if and only if Ũ = UQ and Ṽ = VP where Q = Q1 ⊕ · · · ⊕ Qρ ⊕ Q0 and P = Q1 ⊕ · · · ⊕ Qρ ⊕ P0. Here Q1, . . . ,Qρ are defined as in Theorem 3.2, and Q0 ∈ R(m−r)×(m−r) and"
    }, {
      "heading" : "18 The Singular Value Decomposition",
      "text" : "P0 ∈ R(n−r)×(n−r) are any orthonormal matrices. Obviously, QΣPT = Σ and QTΣP = Σ hold.\nAssume m ≥ n and A = UΣVT is a given thin SVD of A ∈ Rm×n. Then A = ŨΣṼT is a thin SVD if and only if Ũ = UQ and Ṽ = VP where Q = Q1⊕· · ·⊕Qρ⊕Q0 and P = Q1⊕· · ·⊕Qρ⊕P0. Currently, Q0 ∈ R(n−r)×(n−r) is any orthonormal matrix. Obviously, QΣ = ΣQ, ΣPT = PTΣ, and QΣPT = Σ hold.\nTheorem 3.2 and Corollary 3.3 will be used in derivation of subdifferentials of unitarily invariant norms (see Chapter 8). When the matrix in question is SPSD, the spectral decomposition and SVD are identical. That is, U = V in this case. Moreover, the eigenvalues and singular values are identical.\nThe construction proof of Theorem 3.1 shows that\nσ1(A) = max{‖Av‖2 : v ∈ Rn, ‖v‖2 = 1}, so there exists a unit vector v1 ∈ Rn such that σ1(A) = ‖Av1‖2;\nσ2(A) = max{‖Av‖2 : v ∈ Rn, ‖v‖2 = 1,vTv1 = 0}, so there exists a unit vector v2 ∈ Rn such that vT2 v1 = 0 and σ2(A) = ‖Av2‖2;\n...\nσk(A) = max{‖Av‖2 : v ∈ Rn, ‖v‖2 = 1,vT [v1, . . . ,vk−1] = 0}, so there exists a unit vector vk ∈ Rn such that vTk [v1, . . . ,vk−1] = 0 and σk(A) = ‖Avk‖2;\n...\nThe following theorem is the generalization of the Courant-Fischer\ntheorem for singular values.\nTheorem 3.4. Given a matrix A ∈ Rm×n, let σ1 ≥ σ2 ≥ · · · ≥ σp be"
    }, {
      "heading" : "3.2. Matrix Properties via SVD 19",
      "text" : "the singular values of A where p = min{m,n}. For any k ∈ [p], then\nσk = min v1,...,vk−1∈Rn\nmax v ∈ Rn, ‖v‖2 = 1\nvT [v1, . . . ,vk−1] = 0\n‖Av‖2\n= max v1,...,vn−k∈Rn\nmin v ∈ Rn, ‖v‖2 = 1\nvT [v1, . . . ,vn−k] = 0\n‖Av‖2."
    }, {
      "heading" : "3.2 Matrix Properties via SVD",
      "text" : "In what follows, we list some matrix properties which can be induced from SVD. These properties show that SVD is fundamental not only in matrix computation but also in matrix analysis.\nProposition 3.1. Let A = UΣVT be a full SVD of m × n matrix A, and A = UrΣrV T r be a condensed SVD. Let p = min{m,n}. Then\n(1) The rank of A is equal to the number of the nonzero singular values σi of A. (2) ‖A‖2 = σ1 is the spectral norm and ‖A‖F = √ ∑ i,j a 2 ij = √ ∑p i=1 σ 2 i is\nthe Frobenius norm.\n(3) range(A) = range(AAT ) = range(Ur) = span(u1, . . . ,ur) and\nnull(A) = range(V−r) = span(vr+1, . . . ,vn).\n(4) range(AT ) = range(ATA) = range(Vr) = span(v1, . . . ,vr) and\nnull(AT ) = range(U−r) = span(ur+1, . . . ,um).\n(5) The eigenvalues ofATA are σ2i for i = 1, . . . , r and n−r zeros. The right singular vectors vi are the corresponding orthonormal eigenvectors. (6) The eigenvalues of AAT are σ2i for i = 1, . . . , r and m − r zeros. The left singular vectors ui are the corresponding orthonormal eigenvectors. (7) Let B = UBΣBVB be the condensed SVD of B. Then A ⊕ B = (U ⊕ UB)(Σ ⊕ ΣB)(VT ⊕ VTB) is the condensed SVD of A⊕B, and A⊗B = (U⊗UB)(Σ⊗ΣB)(VT ⊗VTB) is the condensed SVD of A⊗B. (8) If A is square and invertible, then A−1 = VΣ−1UT and |det(A)| = ∏n\ni=1 σi(A)."
    }, {
      "heading" : "20 The Singular Value Decomposition",
      "text" : "Theorem 3.5. Given a matrix A ∈ Rm×n, let H = [ 0 AT\nA 0\n]\n. If\nA = UrΣrV T r be the condensed SVD, then H has 2r nonzero eigenvalues, which are ±σi, with the corresponding orthonormal eigenvectors 1√ 2 [ vi\n±ui\n]\n, i = 1, . . . , r.\nConversely, if γi is the eigenvalue of H, with the corresponding\neigenvector zi =\n[\nz (1) i z (2) i\n]\nwhere z (1) i ∈ Rn and z (2) i ∈ Rm, then −γi is\nthe eigenvalue of H, with the corresponding eigenvector zi =\n[\nz (1) i\n−z(2)i\n]\n.\nFurthermore, let the σi denote the r positive values among the ±γi,\nand 1√ 2\n[\nvi ui\n]\ndenote the corresponding orthonormal eigenvectors. Then\nA = UrΣrV T r , where Ur = [u1, . . . ,ur], Vr = [v1, . . . ,vr], and Σr = diag(σ1, . . . , σr), is a condensed SVD of A.\nProof. The first part is directly obtained from the fact that\nH =\n[\n0 AT\nA 0\n]\n=\n[\n0 VrΣrU T r\nUrΣrV T r 0\n]\n= 1\n2\n[\nVr Vr Ur −Ur\n] [\nΣr 0\n0 −Σr\n] [\nVTr U T r VTr −UTr\n]\n.\nConversely, consider that [\n0 AT\nA 0\n] [\nz (1) i\n−z(2)i\n]\n=\n[\n−ATz(2)i Az\n(1) i\n]\n=\n[\n−γiz(1)i γiz (2) i\n] = −γi [ z (1) i\n−z(2)i\n]\n,\nwhich shows that −γi is the eigenvalue of H, with the corresponding\neigenvector\n[\nz (1) i\n−z(2)i\n]\n. Now using the notation of Σr, Ur, and Vr, we\nhave the EVD of H:\nH =\n[\n0 AT\nA 0\n]\n= 1\n2\n[\nVr Vr Ur −Ur\n] [\nΣr 0\n0 −Σr\n] [\nVTr U T r VTr −UTr\n]\n.\nIt also follows from the orthogonality of the eigenvectors that UTr Ur + VTr Vr = 2Ir and U T r Ur − VTr Vr = 0. This implies that UTr Ur = VTr Vr = Ir. Thus, A = UrΣrV T r is a condensed SVD of A."
    }, {
      "heading" : "3.3. Matrix Concepts via SVD 21",
      "text" : "Theorem 3.5 establishes an interesting connection of the SVD of a general matrix with the EVD of a symmetric matrix. This provides an approach to handling an SVD problem of an arbitrary matrix. That is, one transforms the SVD problem into an EVD problem of an associated symmetric matrix. The theorem also gives an alternative proof for the SVD theory.\nThe following theorem shows that the Polar Decomposition of a matrix can be induced from its SVD. Note that SVD can be also derived from the Polar decomposition. Here we do not give the detail of this derivation.\nTheorem 3.6 (Polar Decomposition). Let A ∈ Rm×n be a given matrix where m ≥ n. Then its polar decomposition exists; that is, there are a column orthonormal matrix Q and a unique SPSD matrix S such that A = QS. Furthermore, if A is full column rank, then Q is unique.\nProof. Let A = UΣVT be a thin SVD of A. Then\nA = UVTVΣVT , QS,\nwhere Q , UVT is column orthonormal and S , VΣVT is SPSD.\nAssume that A has two Polar decompositions: A = Q1S1 and Q2S2. Make the full SVDs (spectral decomposition) of S1 and S2 as S1 = V1Σ1V T 1 and S2 = V2Σ2V T 2 , respectively. Then A = (Q1V1)Σ1V T 1 and A = (Q2V2)Σ2V T 2 be two thin SVDs of A. This implies that Σ1 = Σ2 , Σ. Moreover, it follows from Corollary 3.3 that V2 = V1P1 and Q2V2 = Q1V1P2 where P1 and P2 are orthonormal matrices such that ΣPT1 = P T 1 Σ. Thus, S2 = V2ΣV T 2 = V1P1ΣP T 1 V T 1 = V1ΣV T 1 = S1.\nIf A is full column rank, then S is invertible. Hence, Q1 = Q2.\nAs we see from the proof, S = VΣVT = (ATA)1/2; that is, S is\nidentical to the square root of the matrix ATA."
    }, {
      "heading" : "3.3 Matrix Concepts via SVD",
      "text" : "All matrices have SVD, so SVD plays a central role in matrix analysis and computation. As we have seen in the previous section, many"
    }, {
      "heading" : "22 The Singular Value Decomposition",
      "text" : "matrix concepts and properties can be induced from SVD. Here we present other several matrix notions, which are used in modern matrix computations.\nDefinition 3.1. Assume A ∈ Rm×n and B ∈ Rm×n are of rank k and rank l, respectively, and l ≥ k. Let A = UA,kΣA,kVTA,k and B = UB,lΣB,lV T B,l be the condensed SVDs of A and B. The cosines of the canonical angles between A and B are defined as\ncos θi(A,B) = σi(U T A,kUB,l), i = 1, . . . , k.\nConsider that\nσ2(UTA,kUB,l) = λ(U T A,kUB,lU T B,lUA,k)\nand UTA,kUB,lU T B,lUA,k +U T A,kUB,−lU T B,−lUA,k = Ik, where UB,−l ∈ R m×n−l is an orthonormal complement of UB,l. Thus, we have that\nλ(UTA,kUB,lU T B,lUA,k) = 1− λ(UTA,kUB,−lUTB,−lUA,k).\nIn other words, σ2(UTA,kUB,l) = 1− σ2(UTA,kUB,−l). Hence,\nsin θi(A,B) = σk+1−i(U T A,kUB,−l), i = 1, . . . , k.\nNote that σ1(U T A,kUB,−l) = ‖UTA,kUB,−l‖2, which is also cased the distance between two subspaces spanned by UA,k and UB,l.\nDefinition 3.2. Given a nonzero matrix A ∈ Rm×n, let σ1 ≥ · · · ≥ σp where p = min{m,n}. The stable rank of A is defined as ∑pi=1 σ2 i\nσ2 1\n, and\nthe nuclear rank is defined as ∑p\ni=1 σi σ1 .\nClearly, ∑p i=1 σ2 i\nσ2 1 ≤ ∑pi=1 σiσ1 ≤ rank(A). The concepts have been recently proposed for describing error bounds of matrix multiplication approximation [Magen and Zouzias, 2011, Cohen et al., 2015, Kyrillidis et al., 2014].\nDefinition 3.3 (Statistical Leverage Score). Given an m× n matrix A with m > n, let A have a thin SVD A = UΣVT , and let u(i) be the ith row of U. Then the statistical leverage scores of the rows of A are defined as\nli = ‖u(i)‖22 for i = 1, . . . ,m."
    }, {
      "heading" : "3.4. Generalized Singular Value Decomposition 23",
      "text" : "The coherence of the rows of A is defined as\nγ , max i li.\nThe (i, j)-cross leverage scores are defined as\ncij = (u (i))Tu(j).\nThe statistical leverage [Hoaglin and Welsch, 1978] measures the extent to which the singular vectors of a matrix are correlated with the standard basis. Recently, it has found usefulness in large-scale data analysis and in the analysis of randomized matrix algorithms [Drineas et al., 2008, Mahoney and Drineas, 2009, Ma et al., 2014]. A related notion is that of matrix coherence, which has been of interest in matrix completion and Nyström-based low rank matrix approximation [Candès and Recht, 2009, Talwalkar and Rostamizadeh, 2010, Wang and Zhang, 2013, Nelson and Nguyên, 2013]."
    }, {
      "heading" : "3.4 Generalized Singular Value Decomposition",
      "text" : "This section studies simultaneous SVD of two given matrices A and B. This leads us to a generalized SVD (GSVD) problem.\nTheorem 3.7 (GSVD). Suppose two matrices A ∈ Rm×p and B ∈ R n×p with n ≥ p are given. Let q = min{m, p}. Then there exist two orthonormal matrices UA ∈ Rm×m and UB ∈ Rn×n, and an invertible matrix X ∈ Rp×p such that\nUTAAX = diag(α1, . . . , αq) and U T BBX = diag(β1, . . . , βp),\nwhere α1 ≥ · · ·αq ≥ 0, and 0 ≤ β1 ≤ · · · βp.\nThe GSVD theorem was originally proposed by Loan [1976], in which n ≥ p (or m ≥ p) is required. Later on, Paige and Saunders [1981] developed a more general formulation for GSVD in which matrix pencil A and B are required only to have the same number of columns. Paige and Saunders [1981] also studied a GSVD of submatrices of a column orthonormal matrix. That is a so-called CS decomposition [Golub et al., 1999] given as follows."
    }, {
      "heading" : "24 The Singular Value Decomposition",
      "text" : "Theorem 3.8 (The CS Decomposition). Let Q ∈ R(m+n)×p be a column orthonormal matrix. Partition it as QT = [QT1 ,Q T 2 ] where Q1 and Q2 arem×p and n×p. Then there exist orthonormal matrices U1 ∈ Rm×m, U2 ∈ Rn×n, and V1 ∈p×p such that\nUT1 Q1V1 = C and U T 2 Q2V1 = S,\nwhere\nC =\n\n \nr s p−r−s r Ir 0 0 s 0 C1 0 m−r−s 0 0 0   ,\nS =\n\n  r s p−r−s n+r−p 0 0 0 s 0 S1 0 p−r−s 0 0 Ip−r−s   ,\nC1 = diag(α1, . . . , αs) and S1 = diag( √ 1− α21, . . . , √\n1− α2s), and 1 > α1 ≥ α2 ≥ · · ·αs > 0.\nProof. Since QT1 Q1 + Q T 2 Q2 = Q TQ = Ip, the largest eigenvalue of QT1 Q1 (reps. Q T 2Q2) is at most 1. This implies ‖Q1‖2 = σ1(Q1) ≤ 1 (resp. ‖Q2‖2 ≤ 1). Let q = min{m, p}. Make a full SVD of Q1 as\nQ1 = U1CV T 1 ,\nwhere C = diag(c1, . . . , cq) is an m×p diagonal matrix. Assume\n1 = c1 = · · · = cr > cr+1 ≥ · · · ≥ cr+s > cr+s+1 = · · · cp = 0.\nLet D = diag(cr+1, . . . , cr+s)⊕ 0, which is (m− r)× (p− r), and\nQ2V1 = [W1 ︸︷︷︸\nr\n,W2 ︸︷︷︸\np−r\n].\nThen [\nU1 0\n0 In\n]T [\nQ1 Q2\n]\nV1 =\n\n \nIr 0 0 D\nW1 W2\n\n "
    }, {
      "heading" : "3.4. Generalized Singular Value Decomposition 25",
      "text" : "is column orthonormal. This implies that W1 = 0 and\nWT2 W2 = Ip−r −DTD = diag(1− c2r+1, . . . , 1− c2p)\nis nonsingular. Define si = √ 1− c2i for i ∈ [p]. Then\nZ , W2diag(1/sr+1, . . . , 1/sp)\nis column orthonormal. We now extend Z to an n × n orthonormal matrix U2, the last p− r columns of which constitute Z. When setting α1 = cr+1, · · · , αs = cr+s, we have\nUT2 Q1V1 = S.\nThus, the theorem follows.\nRemarks It is worth pointing out that Q1 = U2SV T 1 is not certainly a full SVD of Q1, because some of the nonzero elements of S might not lie on the principal diagonal. However, if n ≥ p, then we can move the first n− p rows of S to be the last n− p rows by pre-multiplying some permutation matrix P. That is,\nPTUT2 Q1V1 =\n\n   \nr s p−r−s r 0 0 0 s 0 S1 0 p−r−s 0 0 Ip−r−s n−p 0 0 0      .\nThis is the reason why the restriction n ≥ p is required in Theorem 3.7 (A and B correspond to Q1 and Q2, respectively).\nThe following theorem gives a more general version of Theorem 3.7 as well as Theorem 3.8. Compared with Theorem 3.7, m ≥ p or n ≥ p are no longer restricted. Compared with Theorem 3.8, the submatrices in question do not necessarily form a column orthonormal matrix.\nTheorem 3.9. Suppose two matrices A ∈ Rm×p and B ∈ Rn×p are given. Let KT , [AT ,BT ] with the rank t. Then exist orthonormal matrices UA ∈ Rm×m, UB ∈ Rn×n, W ∈ Rt×t, and V ∈ Rp×p such that\nUTAAV = ΣA[W TR\n︸ ︷︷ ︸\nt\n, 0 ︸︷︷︸ p−t ] and UTBBV = ΣB [W TR ︸ ︷︷ ︸ t , 0 ︸︷︷︸ p−t ],"
    }, {
      "heading" : "26 The Singular Value Decomposition",
      "text" : "whereR ∈ Rt×t is a positive diagonal matrix with its diagonal elements equal to the nonzero of singular values of K,\nΣA =\n\n \nr s t−r−s r Ir 0 0 s 0 DA 0 m−r−s 0 0 0   , (3.7)\nΣB =\n\n  r s t−r−s n+r−t 0 0 0 s 0 DB 0 t−r−s 0 0 It−r−s   . (3.8)\nHere r and s depend on the context, DA = diag(αr+1, . . . , αr+s) and DB = diag( √ 1−α2r+1, . . . , √ 1−α2r+s),\nand 1 > αr+1 ≥ · · · ≥ αr+s > 0.\nTheorem 3.9 implies that\nUTAAX = [ΣA,0] and U T BBX = [ΣB ,0],\nwhere X , V(R−1W ⊕ Ip−t). With the above remarks, Theorem 3.7 follows. Thus, we now present the proof of Theorem 3.9.\nProof. Since rank(K) = t, making a full SVD of K yields\nPTKV =\n[\nR 0\n0 0\n]\n,\nwhere P ∈ R(m+n)×(m+n) and V ∈ Rp×p are orthonormal matrices, R is a t × t diagonal matrix with the diagonal elements as the nonzero singular values of K. Partition P as\nP = [P1 ︸︷︷︸\nt\n, P2 ︸︷︷︸\nm+n−t\n] =\n[\nP11 P12 P21 P22\n]\nwhere P11 ∈ Rm×t and P21 ∈ Rn×t.\nObviously, PT1 P1 = P T 11P11 +P T 21P21 = It. Moreover, we have\nKV = [P1R,0]."
    }, {
      "heading" : "3.4. Generalized Singular Value Decomposition 27",
      "text" : "Applying Theorem 3.8 to P1 yields that there exist orthonormal matrices UA ∈ Rm×m, UB ∈ Rn×n, and W ∈ Rt×t such that [\nUTA 0\n0 UTB\n] [\nP11 P21\n]\nW =\n[\nΣA ΣB\n]\nwhere ΣA and ΣB are defined in (3.7) and (3.8). Hence, [\nUTA 0\n0 UTB\n] [\nA\nB\n]\nV =\n[\nΣAW TR 0 ΣBW TR 0\n]\n.\nThat is, UTAAV = ΣA[W TR,0] and UTBBV = ΣB [W TR,0].\nIn terms of Theorem 3.7, if βi 6= 0, then the column xi of X satisfies\nATAxi = λiB TBxi,\nwhere λi = α2 i\nβ2 i\n. This implies GSVD can be used to solve general-\nized eigenvalue problems. Based on this observation, Howland et al. [2003], Park and Park [2005] applied GSVD for solving Fisher linear discriminant analysis (FLDA) and generalized Fisher discriminant analysis [Baudat and Anouar, 2000, Mika et al., 2000].\nRecall that the above GSVD procedure requires to implementing an SVD on the (m+n) × p matrix K. The computational cost is O((m+n)p ∗min{m+n, p}). Thus, when both m+n and p are very large, the GSVD is less efficient. We now consider a special case in which B = ZA where Z ∈ Rn×m is some given matrix. We will see that it is no longer necessary to perform the SVD on K.\nTheorem 3.10. Let A ∈ Rm×p and B ∈ Rn×p be two given matrices. Assume that B = ZA where Z ∈ Rn×m is some matrix, rank(B) = s, and rank(A) = t. Let A = UtΣtV T t be a condensed SVD of A, and Y = UY ΣYV T Y be a full SVD of Y , ZUt. Then\n(UtVY ) TAVtΣ −1 t VY = It and U T Y BVtΣ −1 t VY = ΣY .\nThe proof is direct. AssumeUt andVt are extended to orthonormal\nmatrices U (m×m) and V (p× p). Let\nX = V(Σ−1t VY ⊕ Ip−t)."
    }, {
      "heading" : "28 The Singular Value Decomposition",
      "text" : "We now have that\nAX = UUTAV(Σ−1t VY ⊕ Ip−t) = [UtVY ,0] = U(VY ⊕ Im−t)(It⊕0)\nand\nBX = ZUUTAV(Σ−1t VY ⊕ Ip−t) = ZUt[VY ,0] = UY ΣYV T Y [VY ,0] = UY [ΣY ,0].\nThus, (VTY ⊕ Im−t)UTAX = [It ⊕ 0] and\nUTY BX = [ΣY ,0].\nIn this special case, we only need to implement two SVDs on two matrices with smaller sizes. The diagonal elements of ΣY and the columns of VtΣ −1 t VY are the generalized eigenvalues and eigenvectors of the corresponding generalized eigenvalue problem.\nRemarks Assume that A ∈ Rm×n and B ∈ Rm×n have the same size. Gibson [1974] proved that they have joint factorizations A = UΣAV T and B = UΣBV T if and only if ABT and BTA are both normal. Here U and V are orthonormal matrices, and bothΣA andΣB are diagonal but their diagonal elements are perhaps complex. These diagonal elements are nonnegative only if both ABT and BTA are SPSD.\n4 Applications of SVD: Case Studies\nIn the previous chapter we present the basic notion and some important properties of SVD. Meanwhile, we show that many matrix properties can be rederived via SVD. In this chapter, we further illustrate applications of SVD in matrices, including in the definition of the MoorePenrose pseudoinverse of an arbitrary matrix and in the analysis of the Procrustes problem.\nFor any matrix, the Moore-Penrose pseudoinverse exists and is unique. Moreover, it has been found to have many applications. Thus, it is an important matrix notion. In this chapter we exploit the matrix pseudoinverse to solve least squares estimation, giving rise to a more general result. We also show that the matrix pseudoinverse can be used to deal with a class of generalized eigenvalue problems.\nIn fact, SVD has also wide applications in machine learning and data analysis. For example, SVD is an important tool in spectral analysis [Azar et al., 2001], latent semantic indexing [Papadimitriou et al., 1998], spectral clustering, and projective clustering [Feldman et al., 2013]. We specifically show that SVD plays a fundamental role in subspace methods such as PCA, MDS, FDA and CCA.\n29"
    }, {
      "heading" : "30 Applications of SVD: Case Studies",
      "text" : ""
    }, {
      "heading" : "4.1 The Matrix MP Pseudoinverse",
      "text" : "Given a matrix A ∈ Rm×n and a vector b ∈ Rm, we are concerned with the least squares estimation problem:\nx̂ = argmin x∈Rn\n‖Ax− b‖22. (4.1)\nThe minimizer should satisfy the Karush-Kuhn-Tucker (KKT) condition: that is, it is the solution of the following normal equation:\nATAx = ATb. (4.2)\nLet A = UrΣrV T r be the condensed SVD of A. Then VrΣ 2 rV T r x = VrΣrU T r b. Define A † = VrΣ −1 r U T r ∈ Rn×m. Obviously,\nx̂ = A†b\nis a minimizer. It is clear that if A is invertible, then the minimizer is x̂ = A−1b. Thus, A† is a generalization of A−1 in the case that A is an arbitrary matrix, i.e., it is not necessarily invertible and even non-square. This leads us to the notion of the matrix Moore-Penrose (MP) pseudoinverse [Ben-Israel and Greville, 2003].\nDefinition 4.1. Given a matrix A ∈ Rm×n, a real n × m matrix B is called the MP pseudoinverse of A if it satisfies the following four conditions: (1) ABA = A, (2) BAB = B, (3) (AB)T = AB, and (4) (BA)T = BA.\nIt is easily verified that A† = VrΣ −1 r U T r is a pseudoinverse of A. Moreover, when A is invertible, A† is identical to A−1. The following theorem then shows that A† is the unique pseudoinverse of A.\nTheorem 4.1. LetA = UrΣrV T r be the condensed SVD of A ∈ Rm×n. Then B is the pseudoinverse of A if and only if B = A† , VrΣ −1 r U T r .\nProof. To complete the proof, it suffices to prove the uniqueness of the pseudoinverse. Assume that B and C are two pseudoinverses of A. Then\nAB = (AB)T = BTAT = BT (ACA)T = BTATCTAT\n= (AB)T (AC)T = (ABA)C = AC."
    }, {
      "heading" : "4.1. The Matrix MP Pseudoinverse 31",
      "text" : "Similarly, it also holds that BA = CA. Thus,"
    }, {
      "heading" : "B = BAB = BAC = CAC = C.",
      "text" : "The matrix pseudoinverse also has wide applications. Let us see its application in solving generalized eigenproblems. Given two matrices M and N ∈ Rm×m, we refer to (Λ,X) where Λ = diag(λ1, . . . , λq) and X = [x1, . . . ,xq] as q eigenpairs of the matrix pencil (M,N) if MX = NXΛ; namely,\nMxi = λiNxi, for i = 1, . . . , q.\nThe problem of finding eigenpairs of (M,N) is known as a generalized eigenproblem. Clearly, when N = Im, the problem becomes the conventional eigenvalue problem.\nUsually, we are interested in the problem with the nonzero λi for i = 1, . . . , q and refer to (Λ,X) as the nonzero eigenpairs of (M,N). If N is nonsingular, (Λ,X) is also referred to as the (nonzero) eigenpairs of N−1M because the generalized eigenproblem is equivalent to the eigenproblem:\nN−1MX = XΛ.\nHowever, when N is singular, Zhang et al. [2010] suggested to use a pseudoinverse eigenproblem:\nN†MX = XΛ.\nMoreover, Zhang et al. [2010] established a connection between the solutions of the generalized eigenproblem and its corresponding pseudoinverse eigenproblem. That is,\nTheorem 4.2. Let M and N be two matrices in Rm×m. Assume range(M) ⊆ range(N). Then, if (Λ,X) are the nonzero eigenpairs of N†M, we have that (Λ,X) are the nonzero eigenpairs of the matrix pencil (M,N). Conversely, if (Λ,X) are the nonzero eigenpairs of the matrix pencil (M,N), then (Λ,N†NX) are the nonzero eigenpairs of N†M."
    }, {
      "heading" : "32 Applications of SVD: Case Studies",
      "text" : "Proof. Let M = U1Γ1V T 1 and N = U2Γ2V T 2 be the condensed SVD of M and N. Thus, we have range(M) = range(U1) and range(N) = range(U2). Moreover, we have N † = V2Γ −1 2 U T 2 and NN\n† = U2UT2 . It follows from range(M) ⊆ range(N) that range(U1) ⊆ range(U2). This implies that U1 can be expressed as U1 = U2Q where Q is some matrix of appropriate order. As a result, we have\nNN†M = U2U T 2 U2QΓ1V T 1 = M.\nIt is worth noting that the condition NN†M = M is not only necessary but also sufficient for range(M) ⊆ range(N).\nIf (Λ,X) are the eigenpairs of N†M, then it is easily seen that (Λ,X) are also the eigenpairs of (M,N) due to NN†M = M.\nConversely, suppose (Λ,X) are the eigenpairs of (M,N). Then we have NN†MX = NXΛ. This implies that (Λ,N†NX) are the eigenpairs of N†M due to NN†M = M and N†NN† = N†.\nFisher discriminant analysis (FDA) is a classical method for classification and dimension reduction simultaneously [Mardia et al., 1979]. It is essentially a generalized eigenvalue problem in which the matrices N and M correspond to a pooled scatter matrix and a between-class scatter matrix [Ye and Xiong, 2006, Zhang et al., 2010]. Moreover, the condition range(M) ⊆ range(N) meets. Thus, Theorem 4.2 provides a solution when the pooled scatter matrix is singular or nearly singular. We will present more details about FDA in Section 4.3."
    }, {
      "heading" : "4.2 The Procrustes Problem",
      "text" : "Assume that X ∈ Rn×p and Y ∈ Rn×p are two configurations of n data points. The orthogonal Procrustes analysis aims to move Y relative into X through rotation [Gower and Dijksterhuis, 2004].\nIn particular, the Procrustes problem is defined as\nmin Q∈Rp×p\n‖X−YQ‖2F s.t. QTQ = Ip. (4.3)\nTheorem 4.3. Let the full SVD of YTX be YTX = UΣVT . Then UVT is the minimizer of the Procrustes problem in (4.3)."
    }, {
      "heading" : "4.3. Subspace Methods: PCA, MDS, FDA, and CCA 33",
      "text" : "Proof. Since ‖X − YQ‖2F = tr((X − YQ)T (X −YQ)) = tr(XTX) + tr(YTY)− 2tr(YTXQT ), the original problem is equivalent to\nmax tr(YTXQT ) s.t. QTQ = Ip.\nRecall that the constants QTQ = Ip are equivalent to that q T i qi = 1 for i = 1, . . . , p, and qTi qj = 0 for i 6= j. Here the qi are the columns of Q. Thus, the Lagrangian function is\ntr(YTXQT )− 1 2\np ∑\ni=1\ncii(q T i qi − 1)−\n1\n2\n∑\ni>j\ncij(q T i qj − 0),\nwhich is written in matrix form as\nL(Q,C) = tr(YTXQT )− 1 2 tr[C(QTQ− Ip)],\nwhere C = [cij ] is a symmetric matrix of the Lagrangian multipliers.\nSince\ndL = tr(YTXdQT )− 1 2 tr(C(dQTQ+QTdQ)),\nwe have dLdQ = Y TX − QC. Letting the first-order derivative be zero yields\nYTX−QC = 0.\nLet Q̂ = UVT and Ĉ = VΣVT , which are obviously the solutions of the above equation systems.\nThe Hessian matrix of L w.r.t. Q at Q = Q̂ and C = Ĉ is −(VΣVT ) ⊗ Ip, which is negative definite. Thus, Q = UVT is the minimizer of the Procrustes problem."
    }, {
      "heading" : "4.3 Subspace Methods: PCA, MDS, FDA, and CCA",
      "text" : "Subspace methods, such as principal component analysis (PCA), multidimensional scaling (MDS), Fisher discriminant analysis (FDA), and canonical correlation analysis (CCA), are a class of important machine learning methods. SVD plays a fundamental role in subspace learning methods."
    }, {
      "heading" : "34 Applications of SVD: Case Studies",
      "text" : "PCA [Jolliffe, 2002, Kittler and Young, 1973] and MDS [Cox and Cox, 2000] are two classical dimension reduction methods. Let A = [a1, . . . ,an] T be a given data matrix in which each vector ai represents a data instance in R p. Let m = 1n ∑n i=1 ai = 1 nA T1n be the sample mean and Cn = In− 1n1nITn be a so-called centered matrix. The pooled scatter matrix is defined as (a multiplier 1/n omitted)\nS = n∑\ni=1\n(ai −m)(ai −m)T = ATCnCnA = ATCnA.\nIt is well known that PCA computes the spectral decomposition of S, while the classical MDS or principal coordinate analysis (PCO) computes the spectral decomposition of the Gram matrix CnAA TCn. Proposition 3.1-(5)-(6) show that it is equivalent to computing SVD directly on the centerized data matrix CnA. Thus, SVD bridges PCA and PCO. That is, there is a duality relationship between PCA and PCO [Mardia et al., 1979]. This relationship has found usefulness in latent semantic analysis, face classification, and microarray data analysis [Deerwester et al., 1990, Turk and Pentland, 1991, Golub et al., 1999, Belhumeur et al., 1997, Muller et al., 2004].\nFDA is a joint approach for dimension reduction and classification. Assume that the ai are to be grouped into c disjoint classes and that each ai belongs to one and only one class. Let V = {1, 2, . . . , n} denote the index set of the data points ai and partition V into c disjoint subsets Vj; that is, Vi ∩ Vj = ∅ for i 6= j and ∪cj=1Vj = V , where the cardinality of Vj is nj so that ∑c j=1 nj = n. We also make use of a matrix representation for the partitions. In particular, we let E = [eij ] be an n×c indicator matrix with eij = 1 if input ai is in class j and eij = 0 otherwise.\nLet mj = 1 nj\n∑\ni∈Vj ai be the jth class mean for j = 1, . . . , c. The between-class scatter matrix is defined as Sb = ∑c j=1 nj(mj−m)(mj− m)T . Conventional FDA solves the following generalized eigenproblem:\nSbxj = λjSxj, λ1 ≥ λ2 ≥ · · · ≥ λq > λq+1 = 0,\nwhere q ≤ min{p, c−1} and where we refer to xj as the jth discriminant direction. The above generalized eigenproblem can can be"
    }, {
      "heading" : "4.3. Subspace Methods: PCA, MDS, FDA, and CCA 35",
      "text" : "expressed in matrix form:\nSbX = SXΛ, (4.4)\nwhere X = [x1, . . . ,xq] (n×q) and Λ = diag(λ1, . . . , λq) (q×q). Let Π = diag(n1, . . . , nc). Then Sb can be rewritten as\nSb = A TCnEΠ −1ETCnA.\nRecall that S = ATCnCnA. Given these representations of S and Sb, the problem in (4.4) can be solved by using the GSVD method [Loan, 1976, Paige and Saunders, 1981, Golub and Van Loan, 2012, Howland et al., 2003]. Moreover, it is obvious that range(Sb) ⊆ range(ATCn) = range(S). Thus, Theorem 4.2 provides a solution when S is singular or nearly singular. Moreover, the method given in Theorem 3.10 is appropriate for solving the FDA problem.\nCCA is another subspace learning model [Hardoon et al., 2004]. The primary focus is on the relationship between two groups of variables (or features), whereas PCA considers interrelationships within a set of variable. Mathematically, CCA is defined as a generalized eigenvalue problem, so its solution can be borrowed from that of FDA."
    }, {
      "heading" : "4.3.1 Nonlinear Extensions",
      "text" : "Reproducing kernel theory [Aronszajn, 1950] provides an approach for nonlinear extensions of subspace methods. For example, kernel PCA [Schölkopf et al., 1998], kernel FDA [Baudat and Anouar, 2000, Mika et al., 2000, Roth and Steinhage, 2000], kernel CCA [Akaho, 2001, Van Gestel et al., 2001, Bach and Jordan, 2002] have been successively proposed and received wide applications in data analysis.\nKernel methods work in a feature space F , which is related to the original input space X ⊂ Rp by a mapping,\nϕ : X → F .\nThat is, ϕ is a vector-valued function which gives a vector ϕ(a), called a feature vector, corresponding to an input a ∈ X . In kernel methods, we are given a reproducing kernel K : X × X → R such that"
    }, {
      "heading" : "36 Applications of SVD: Case Studies",
      "text" : "K(a,b) = ϕ(a)Tϕ(b) for a,b ∈ X . The mapping ϕ(·) itself is typically not given explicitly. Rather, there exist only inner products between feature vectors in F . In order to implement a kernel method without referring to ϕ(·) explicitly, one resorts to the so-called kernel trick [Schölkopf and Smola, 2002, Shawe-Taylor and Cristianini, 2004].\nLet L2(X ) be the square integrable Hilbert space of functions whose elements are functions defined on X . It is a well-known result that if K is a reproducing kernel for the Hilbert space L2(X ), then {K(·,b)} spans L2(X ). Here K(·,b) represents a function that is defined on X with values at a ∈ X equal to K(a,b). There are some common kernel functions:\n(a) Linear kernel: K(a,b) = aTb,\n(b) Gaussian kernel or radial basis function (RBF): K(a,b) = exp ( −\n∑p j=1 (aj−bj)2 βj ) with βj > 0,\n(c) Laplacian kernel: K(a,b) = exp ( −∑pj=1 |aj−bj | βj ) with βj > 0,\n(d) Polynomial kernel: K(a,b) = (aTb+ 1)d of degree d.\nGiven a training set of input vectors {a1, . . . ,an}, the kernel matrix K = [K(ai,aj)] is an n× n SPSD matrix.\n5 The QR and CUR Decompositions\nThe QR factorization and CUR decomposition are the two most important counterparts of SVD. These three factorizations apply to all matrices. In Table 1.1 we have compared their primary focuses. The SVD and QR factorization are two classical matrix theories. The CUR decomposition aims to represent a data matrix in terms of a small number part of the matrix, which makes it easy for us to understand and interpret the data in question. Here we present very brief introductions to the QR factorization and CUR decomposition."
    }, {
      "heading" : "5.1 The QR Factorization",
      "text" : "The QR factorization is another decomposition method applicable all matrices. Given a matrix A ∈ Rm×n, the QR factorization is given by\nA = QR,\nwhere Q ∈ Rm×m is orthonormal and R ∈ Rm×n is upper triangular (or low triangular). Let D be an m ×m diagonal matrix whose diagonal elements are either 1 or −1. Then A = (QD)(DR) is still a QR factorization of A. Thus, we always assume that R has nonnegative diagonal elements.\n37"
    }, {
      "heading" : "38 The QR and CUR Decompositions",
      "text" : "Assume m ≥ n. The matrix A also has a thin QR factorization:\nA = QR,\nwhere Q ∈ Rm×n is currently column orthonormal, and R ∈ Rn×n is upper triangular with nonnegative diagonal elements. If A is of rank n, R is uniquely determined. In this case, Q = AR−1 is also uniquely determined.\nAsume A has rank r (≤ min{m,n}). Then there exists an m ×m orthonormal matrix Q and an n× n permutation matrix P such that\nQTAP =\n[\nR11 R12\n0 0\n]\n,\nwhere R11 is an r × r upper triangular matrix with positive diagonal elements. This is called a rank revealing QR factorization.\nComputation of the QR factorization can be arranged by the novel Gram-Schmidt orthogonalization process or the modified GramSchmidt which is numerically more stable [Trefethen and Bau III, 1997]. Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992]. Stewart [1999] devised efficient computational algorithms of truncated pivoted QR approximations to a sparse matrix."
    }, {
      "heading" : "5.2 The CUR Decomposition",
      "text" : "As we have see, SVD leads us to a geometrical representation, and the QR factorization facilitates computations. They have little concrete meaning. This makes it difficult for us to understand and interpret the data in question.\nKuruvilla et al. [2002] have claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.” Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. Matrix column selection and CUR matrix decomposition provide such techniques."
    }, {
      "heading" : "5.2. The CUR Decomposition 39",
      "text" : "Column selection yields a so-called CX decomposition, and the CUR decomposition can be be regarded as a special CX decomposition. The CUR decomposition problem has been widely discussed in the literature [Goreinov et al., 1997a,b, Stewart, 1999, Tyrtyshnikov, 2000, Berry et al., 2005, Drineas and Mahoney, 2005, Bien et al., 2010], and it has been shown to be very useful in high dimensional data analysis.\nThe CUR was originally called a skeleton decomposition [Goreinov et al., 1997a]. Let A ∈ Rm×n be a given matrix of rank r. Then there exists a nonsingular r×r submatrix in A. Without loss of generality, assume this nonsingular matrix is the first r × r principal submatrix of A. That is, A can be partioned into the following form:\nA =\n[\nA11 A12 A21 A22\n]\n,\nwhere A11 is a r × r nonsingular matrix. Consider that [A21,A22] = B[A11,A12] for some B ∈ R(m−r)×r. It follows from A21 = BA11 that B = A21A −1 11 . Hence, A22 = A21A −1 11 A12. So it is obtained that\nA =\n[\nA11 A21\n]\nA−111 [A11,A12].\nIn general case, let AI,J be the nonsingular submatrix where I = {i1, . . . , ir} ⊂ [m] and J = {j1, . . . , jr} ⊂ [n]. Then it also hods that A = CA−1I,JR,\nwhere C = A:,J and R = AI,: are respectively a subset of columns and a subset of rows, of A.\nIn practical applications, however, it is intractable to select AI,J . Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005]. The SCA approximation is of the form A ≈ XTY, where X and Y consist of columns and rows of A, and T minimizes ‖A−XTY‖2F . This algorithm is a deterministic peocedure but computationally expensive.\nThe terminology of the CUR decomposition has been proposed by Drineas and Mahoney [2005], Mahoney et al. [2008]. They reformulated the idea based on random selection. A CUR decomposition"
    }, {
      "heading" : "40 The QR and CUR Decompositions",
      "text" : "algorithm seeks to find a subset of c columns of A to form a matrix C ∈ Rm×c, a subset of r rows to form a matrix R ∈ Rr×n, and an intersection matrix U ∈ Rc×r such that ‖A−CUR‖ξ is small. Accordingly, Ã = CUR is used to approximate A.\nSince there are (nc ) possible choices of constructing C and ( m r ) possible choices of constructing R, obtaining the best CUR decomposition is a hard problem. In Chapter 10 we will further study the CUR decomposition problem via random approximation.\nThe CUR decomposition is also an extension of the novel Nyström approximation to a general matrix. The Nyström method approximates an SPSD matrix only using a subset of its columns, so it can alleviate computation and storage costs when the SPSD matrix in question is large in size. Thus, the Nyström method and its variants [Halko et al., 2011, Gittens and Mahoney, 2013, Kumar et al., 2009, Wang and Zhang, 2013, 2014, Wang et al., 2014b, Si et al., 2014] have been extensively used in the machine learning community. For example, they have been applied to Gaussian processes [Williams and Seeger, 2001], kernel classification [Zhang et al., 2008, Jin et al., 2013], spectral clustering [Fowlkes et al., 2004], kernel PCA and manifold learning [Talwalkar et al., 2008, Zhang et al., 2008, Zhang and Kwok, 2010], determinantal processes [Affandi et al., 2013], etc.\n6 Variational Principles\nVariational principles correspond to matrix perturbation theory [Stewart and Sun, 1990], which is the theoretical foundation to characterize stability or sensitivity of a matrix computation algorithm. Thus, variational principles are important in analysis for error bounds of matrix approximate algorithms (see Chapters 9 and 10).\nIn this chapter we specifically study variational properties for eigenvalues of a symmetric matrix as well as for singular values of a general matrix. We will see that these results for eigenvalues and for singular values are almost parallel. The cornerstones are the novel von Neumann theorem [Neumann, 1937] and Ky Fan theorem [Fan, 1951]. We present new proofs for them by using theory of matrix differentials. Additionally, we present some majorization inequalities. They will be used in the latter chapters, especially in investigating unitarily invariant norms (see Chapter 7).\nGiven a matrix A ∈ Rm×n, we always let σ1(A) ≥ · · · ≥ σp(A) be the singular values of A where p = min{m,n}. When A is symmetric, let λ1(A) ≥ · · · ≥ λn(A) be the eigenvalues of A. These eigenvalues or singular values are always arranged in deceasing order. Note that the eigenvalues are real but could be negative. Let\n41"
    }, {
      "heading" : "42 Variational Principles",
      "text" : "λ(M) = (λ1(M), . . . , λn(M)) T denote the eigenvalues of an n× n real square matrix M, and σ(A) = (σ1(A), . . . , σp(A)) T denote the singular values of an m × n real matrix A. Sometimes we also write them the σi or the λi when they are explicit in the context for notational simplicity."
    }, {
      "heading" : "6.1 Variational Properties for Eigenvalues",
      "text" : "In this section we consider variational properties for eigenvalues of a real symmetric matrix. It is well known that for an arbitrary symmetric matrix, its eigenvalues are all real. The following cornerstone theorem was originally established by von Neumann [1937].\nTheorem 6.1 (von Neumann Theorem). Assume M ∈ Rn×n and N ∈ R n×n are symmetric. Then\nn∑\ni=1\nλi(M)λi(N) = max QQT=In\ntr(QMQTN).\nMoreover,\nn∑\ni=i\nλi(M)λn−i+1(N) = min QQT=In\ntr(QMQTN).\nProof. The second part directly follows from the first part because\nmin QQT=In tr(QMQTN) = − max QQT=In tr(QMQT (−N)).\nWe now present the proof of the first part. Make full EVDs of M and N as M = UMΛMU T M and N = UNΛNU T N , where ΛM = diag(λ1(M), . . . , λn(M)) and ΛN = diag(λ1(N), . . . , λn(N)), and UM and UN are orthonormal. It is easily seen that\nmax QQT=In tr(QMQTN) = max QQT=In tr((UTNQUM )ΛM (U T NQUM ) TΛN )\n= max QQT=In\ntr(QΛMQ TΛN )."
    }, {
      "heading" : "6.1. Variational Properties for Eigenvalues 43",
      "text" : "Let Q = [qij] = [q1, . . . ,qn] T . We now have\ntr(QΛMQ TΛN )\n= n∑\ni=1\nqTi ΛMqiλi(N)\n= n−1∑\ni=1\ni∑\nj=1\nqTj ΛMqj[λi(N)− λi+1(N)] + λn(N) n∑\nj=1\nqTj ΛMqj\n= n−1∑\ni=1\n[λi(N)− λi+1(N)] i∑\nj=1\nn∑\nk=1\nq2jkλk(M) + λn(N) n∑\nj=1\nλj(M).\nDefine W , [q2ij ] which is doubly stochastic, and u = [u1, . . . , un] T where uj = ∑n k=1 q 2 jkλk(M). That is, u = Wλ(M). By Lemma 2.2, we know that u ≺ λ(M). Accordingly,\ntr(QΛMQ TΛN ) ≤\nn−1∑\ni=1\n[λi(N)−λi+1(N)] i∑\nj=1\nλj(M) + λn(N) n∑\nj=1\nλj(M)\n= n∑\ni=1\nλi(M)λi(N).\nWhen Q = In, the equality holds. That is, U T NQUM = In in the original problem. The theorem follows.\nThe following theorem is a corollary of Theorem 6.1 when taking\nN =\n[\nIk 0 0 0\n]\n.\nTheorem 6.2 (von Neumann Theorem). Assume M ∈ Rn×n is symmetric. Then for k ∈ [n],\nk∑\ni=1\nλi = max QT Q=Ik\ntr(QTMQ),\nwhich is arrived when Q is the n×k matrix of the orthonormal vectors associated with λ1, . . . , λk. Moreover,\nn∑\ni=n−k+1 λi = min QT Q=Ik tr(QTMQ)."
    }, {
      "heading" : "44 Variational Principles",
      "text" : "In the appendix we give an other proof based on theory of matrix differentials. The von Neumann theorem describes the variational principle of eigenvalues of a symmetric matrix. Using Theorems 6.2, we have the following variational properties.\nProposition 6.1. Given two n× n real symmetric matrices M and N, we have that\n(1) λ(M+N) ≺ λ(M) + λ(N) and λ(M)− λ(N) ≺ λ(M−N). (2)\n∑k i=1 λi(M+N) ≥ ∑k i=1 λi(M) + ∑n j=n−k+1 λj(N) for k ∈ [n].\n(3) (m11, . . . ,mnn) ≺ (λ1(M), . . . , λn(M)).\nProof. The proof is based on Theorem 6.2. First, for k ∈ [n − 1], k∑\ni=1\nλi(M+N) = max QT Q=Ik\n{ tr(QTMQ) + tr(QTNQ) }\n≤ max QT Q=Ik tr(QTMQ) + max QT Q=Ik tr(QTNQ) = k∑\ni=1\nλi(M) + k∑\ni=1\nλi(N).\nNote that tr(M+N) = tr(M) + tr(N), so λ(M+N) ≺ λ(M) + λ(N). Hence, λ(M) − λ(N) ≺ λ(M−N). Second,\nk∑\ni=1\nλi(M+N) = max QT Q=Ik\n{ tr(QTMQ) + tr(QTNQ) }\n≥ max QT Q=Ik\n{\ntr(QTMQ) + min QT Q=Ik\ntr(QTNQ) }\n= k∑\ni=1\nλi(M) + n∑\nj=n−k+1 λj(N).\nTo prove the third part, we assume that m11 ≥ · · · ≥ mnn without loss of generality. Now the result is obtained via\nk∑\ni=1\nλi(M) = max QT Q=Ik\ntr(QTMQ) ≥ tr(HTkMHk) = k∑\ni=1\nmii,\nwhere Hk consists of the first k columns of In for all k ∈ [n]."
    }, {
      "heading" : "6.1. Variational Properties for Eigenvalues 45",
      "text" : "Proposition 6.1-(3) is sometimes referred to as Schur’s theorem. The second part of the following proposition is an extension of Schur’s theorem.\nProposition 6.2. Let M =\n[\nM11 M12 M21 M22\n]\nbe n×n real symmetric. Here\nM11 is k × k. Then\n(1) λi(M) ≥ λi(M11) ≥ λn−k+i(M) for i = 1, . . . , k;\nand (2) (λ(M11),λ(M22)) ≺ λ(M). Furthermore, for any column-orthonormal matrix Q ∈ Rn×k, we have\n(3) λi(M) ≥ λi(QTMQ) ≥ λn−k+i(M) for i = 1, . . . , k.\nProof. The first result directly follows from the well known interlacing theorem [Horn and Johnson, 1985]. As for the third part, we can extend Q to an orthonormal matrix Q̃ = [Q,Q⊥]. Consider that\nQ̃TMQ̃ =\n[\nQTMQ QTMQ⊥\n(Q⊥)TMQ (Q⊥)TMQ⊥\n]\n.\nThus,\nλi(M) = λi(Q̃ TMQ̃) ≥ λi(QTMQ) ≥ λn−k+i(Q̃TMQ̃) = λn−k+i(M).\nWe now consider the proof of the second part. Let the EVDs of\nM11 and M22 be M11 = U1Λ1U T 1 and M22 = U2Λ2U T 2 . Then\n[\nUT1 0\n0 UT2\n] [\nM11 M12 M21 M22\n] [\nU1 0\n0 U2\n]\n=\n[\nΛ1 U T 1 M12U2\nUT2 M21U1 Λ2\n]\n.\nSince U1 and U2 are orthonormal, we have that λ(M11) = λ(Λ1), λ(M22) = λ(Λ2), and\nλ\n([\nUT1 0\n0 UT2\n] [\nM11 M12 M21 M22\n] [\nU1 0\n0 U2\n])\n= λ(M).\nApplying Proposition 6.1-(3) completes the proof."
    }, {
      "heading" : "46 Variational Principles",
      "text" : ""
    }, {
      "heading" : "6.2 Variational Properties for Singular Values",
      "text" : "Theorems 6.1 and 6.2 can be extended to a general matrix. In this case, we investigate singular values of the matrix instead. Theorems 6.3 and 6.4 correspond to Theorems 6.1 and 6.2, respectively.\nTheorem 6.3 (Ky Fan Theorem). Given two matrices A ∈ Rm×n and B ∈ Rm×n, let A and B have full SVDs A = UAΣAVTA and B = UBΣBV T B , respectively. Let p = min{m,n}. Then\np ∑\ni=1\nσi(A)σi(B) = max XT X=Im,YT Y=In\n|tr(XTAYBT )|\n= max XT X=Im,YT Y=In\ntr(XTAYBT ),\nwhich is achieved at X = UAU T B and Y = VAV T B.\nProof. Note that\ntr(XTAYBT ) = 1\n2 tr\n([\nYT 0\n0 XT\n] [\n0 AT\nA 0\n] [\nY 0\n0 X\n] [\n0 BT\nB 0\n])\n.\nThe theorem is directly obtained from Theorems 6.1 and 3.5.\nTheorem 6.4 (Ky Fan Theorem). Given an m × n real matrix A, let p = min{m,n}, and let the singular values of A be σ1, . . . , σp which are arranged in descending order, with the corresponding left and right singular vectors ui and vi. Then for any k ∈ [p], k∑\ni=1\nσi = max XT X=Ik ,YT Y=Ik |tr(XTAY)| = max XT X=Ik ,YT Y=Ik tr(XTAY),\nwhich is achieved at X = [u1, . . . ,uk] and Y = [v1, . . . ,vk].\nThe theorem can be obtained from Theorems 6.2 and 3.5 or from\nTheorem 6.3. In the appendix we give the third proof.\nProposition 6.3. Given two matrices A ∈ Rm×n and B ∈ Rm×n, let p = min{m,n}. Let Â be obtained by replacing the last r rows and/or columns of A by zeros. Then"
    }, {
      "heading" : "6.2. Variational Properties for Singular Values 47",
      "text" : "(1) σ(A+B) ≺w σ(A) + σ(B). (2) σi+j−1(A+B) ≤ σi(A) + σj(B) for i, j ≥ 1 and i+ j − 1 ≤ p. (3) a ≺w σ(A) where a = (a11, . . . , app)T . (4) For i ∈ [p− r], σr+i(A) ≤ σi(Â) ≤ σi(A). (5) Let P ∈ Rm×r and Q ∈ Rn×r be column orthonormal matrices where\nr ≤ p. Then σr+i(A) ≤ σi(PTA) ≤ σi(A) and σr+i(A) ≤ σi(AQ) ≤ σi(A) for i = 1, . . . , p− r.\nProof. The proof of Proposition 6.3-(1) and (3) is parallel to that of Proposition 6.1-(1) and (3). Part-(2) is Weyl’s monotonicity theorem. It can be proven by the Courant-Fischer theorem (see Theorem 3.4). Consider that σi(A) = √ λi(ATA) = √ λi(AAT ) and σi(Â) = √ λi(ÂT Â) = √ λi(ÂÂT ). Part (4) follows from Proposition 6.2-(1). Part (5) follows then from Proposition 6.2-(3).\nTheorem 6.5. Given two matrices A ∈ Rm×n and B ∈ Rm×n, let si(A−B) = |σi(A)− σi(B)| for i ∈ [p] where p = min{m,n}. Then\nk∑\ni=1\ns↓i (A−B) ≤ k∑\ni=1\nσi(A−B) for k = 1, . . . , p.\nProof. Consider the following two (m+n)×(m+n) symmetric matrices:\nÃ =\n[\n0 A AT 0\n]\nand B̃ =\n[\n0 B BT 0\n]\n.\nBy Theorem 3.5, the eigenvalues of Ã are ±σ1(A), . . . ,±σp(A), together with m+n− 2p zeros; and similarly for B̃ as well as for Ã− B̃. Thus, the p largest entries of λ(Ã− B̃) are σ1(A−B), . . . , σp(A−B). Note that both σi(A) − σi(B) and σi(B) − σi(A) are the entries of λ(Ã)−λ(B̃), so the p largest entries of λ(Ã)−λ(B̃) comprise the set {s1(A −B), . . . , sp(A −B)}. Proposition 6.1 shows that λ(Ã − B̃) ≺ λ(Ã)− λ(B̃). This implies the result of the theorem.\nTheorem 6.6. Let A ∈ Rm×n and B ∈ Rn×p be given, and let q = min{m,n, p}. Then for k = 1, . . . , q,\nk∏\ni=1\nσi(AB) ≤ k∏\ni=1\nσi(A)σi(B)."
    }, {
      "heading" : "48 Variational Principles",
      "text" : "If n = p = m, then equality holds for k = n. And\nk∑\ni=1\nσi(AB) ≤ k∑\ni=1\nσi(A)σi(B) ≤ ( k∑\ni=1\nσi(A) )( k∑\ni=1\nσi(B) ) .\nProof. Let AB = UΣVT be a full SVD of AB, and for k ≤ q let Uk and Vk be the first k columns of U and V, respectively. Now take a polar decomposition of BVk as BVk = QS. Since S 2 = VTkB TBVk and by Proposition 6.3-(4), we obtain\ndet(S2) = det(VTkB TBVk) ≤\nk∏\ni=1\nσ2i (B)\nWe further have that\nk∏\ni=1\nσi(AB) = |det(UTkABVk)| = |det(UTkAQ) det(S)|\n≤ k∏\ni=1\nσi(A)σi(B).\nThe above inequality again follows from Proposition 6.3-(4), When n = p = m, then\nn∏\ni=1\nσi(AB) = |det(AB)| = |det(A)| × |det(B)| = n∏\ni=1\nσi(A)σi(B).\nThe second part follows from the first part and Lemma 2.4."
    }, {
      "heading" : "6.3 Appendix: Application of Matrix Differentials",
      "text" : "Here we present alternative proofs for Theorem 6.2 and Theorem 6.4, which are based on matrix differentials. It aims at further illustrating how to use matrix differentials.\nThe Second Proof of Theorem 6.2. To solve the problem, we define the Lagrangian function:\nL(Q,C) = tr(QTMQ)− tr(C(QTQ− Ik)),"
    }, {
      "heading" : "6.3. Appendix: Application of Matrix Differentials 49",
      "text" : "where C is a k × k symmetric matrix of Lagrangian multipliers. Since\ndL = tr(dQTMQ+QTMdQ)− tr(C(dQTQ+QTdQ)),\nthis shows that dLdQ = 2MQ− 2QC. The KKT condition is now\nMQ−QC = 0.\nClearly, if Ĉ , diag(λ1, . . . , λk) and Q̂ consists of the corresponding orthonormal eigenvectors, they are a solution of the above equation. In this setting, we see that tr(Q̂TMQ̂) = ∑k\ni=1 λi.\nThus, we only need to prove that Q̂ is indeed the maximizer of the original problem. We now compute the Hessian matrix of L w.r.t. Q at Q = Q̂ and C = Ĉ. Since vec(MQ−QC) = (Ik⊗M−C⊗ In)vec(Q), the Hessian matrix is given as\nH = 2(Ik ⊗M− Ĉ⊗ In).\nFor any X ∈ Rn×k such that XT Q̂ = 0, it suffices for our purpose to prove xTHx/2 ≤ 0 where x = vec(X). Take the full EVD of M as M = UΛUT , where Λ = diag(λ1, . . . , λn) and U = [Q̂, Q̂\n⊥] such that UTU = In. Denote Λ2 = diag(λk+1, . . . , λn) and Y = (Q̂\n⊥)TX = [y1, . . . ,yk]. Then,\n1 2 xTHx = tr(XTMX)− tr(XĈXT )\n= tr(XT Q̂⊥Λ2(Q̂ ⊥)TX)− tr(ĈXT (Q̂Q̂T + Q̂⊥(Q̂⊥)T )X) = tr(YTΛ2Y)− tr(ĈYTY) = k∑\ni=1\nyTi Λ2yi − k∑\ni=1\nλiy T i yi\n= k∑\ni=1\nyTi (Λ2 − λiIn−k)yi ≤ 0.\nThe Third Proof of Theorem 6.4. To solve the constrained problem in the theorem, we now define the Lagrangian function:\nL(X,Y,C1,C2) = tr(X TAY)−1\n2 tr(C1(X\nTX−Ik))− 1\n2 tr(C2(Y\nTY−Ik)),"
    }, {
      "heading" : "50 Variational Principles",
      "text" : "where C1 and C2 are two k × k symmetric matrix of Lagrange multipliers. Since\ndL = tr(dXTAY)− 1 2 tr(C1(dX TX+XTdX)), dL = tr(XTAdY)− 1 2 tr(C2(dY TY+YTdY)),\nwhich yield that dLdX = AY−XC1 and dLdY = XAT −YC2. The KKT condition is now\nAY−XC1 = 0 and ATX−YC2 = 0.\nIt then follows from XTX = Ik and Y TY = Ik that C1 = C2. We denote C , C1 = C2. So,\nAY−XC = 0, ATX−YC = 0.\nThat is, [\n0 A AT 0\n] [\nX\nY\n]\n=\n[\nX\nY\n]\nC.\nClearly, if Ĉ , Σk = diag(λ1, . . . , λk), X̂ , Uk = [u1, . . . ,uk], and Ŷ , Vk = [v1, . . . ,vk], then they are a solution of the above equation. In this setting, we see that tr(X̂TAŶ) = ∑k\ni=1 σi.\nThus, we only need to prove that (X̂, Ŷ) is the maximizer of the original problem. We now compute the Hessian matrix of L w.r.t. (X,Y) at (X,Y) = (X̂, Ŷ), and C = Ĉ. The Hessian matrix is given as\nH ,\n\n\n∂2L ∂vec(X̂)∂vec(X̂)T ∂2L ∂vec(X̂)∂vec(Ŷ)T ∂2L ∂vec(Ŷ)∂vec(X̂)T ∂2L ∂vec(Ŷ)∂vec(Ŷ)T\n\n =\n[\n−Σk ⊗ Im Ik ⊗A Ik ⊗AT −Σk ⊗ In\n]\n,\nbecause vec(AY − XC) = (Ik ⊗ A)vec(Y) − (CT ⊗ Im)vec(X) and vec(ATX−YC) = (Ik ⊗AT )vec(X)− (CT ⊗ In)vec(Y).\nNote that [ X̂T 0\n0 ŶT\n] [\nX̂ 0\n0 Ŷ\n]\n= I2k."
    }, {
      "heading" : "6.3. Appendix: Application of Matrix Differentials 51",
      "text" : "Thus, for any Z1 ∈ Rm×k and Z2 ∈ Rn×k such that ZT1 X̂ = 0 and ZT2 Ŷ = 0, it suffices for our purpose to prove z\nTHz ≤ 0 where zT = (vec(Z1) T , vec(Z2) T ). Compute\nzTHz = [vec(Z1) T , vec(Z2) T ]\n[\n−Σk ⊗ Im Ik ⊗A Ik ⊗AT −Σk ⊗ In\n] [\nvec(Z1) vec(Z2)\n]\n= vec(Z2) T (Ik⊗AT )vec(Z1) + vec(Z1)T (Ik⊗A)vec(Z2)\n− vec(Z1)T (Σk⊗Im)vec(Z1)− vec(Z2)T (Σk⊗In)vec(Z2) = −tr(ZT1 Z1Σk)− tr(ZT2 Z2Σk) + 2tr(ZT1 AZ2) , ∆.\nTake a thin SVD of A as A = UΣVT , where Σ = Σk ⊕ Σ−k, U = [Uk,U−k], and V = [Vk,V−k]. Denote R1 = UT−kZ1 and and R2 = V T −kZ2. Then tr(Z T 1 AZ2) = tr(Z T 1 U−kΣ−kV T −k). And hence,\n−∆ = tr(ZT1 ΣkZ1) + tr(ZT2 ΣkZ2)− 2tr(ZT1 U−kΣ−kVT−kZ2) ≥ tr(ZT1 U−kUT−kZ1Σk) + tr(ZT2 U−kUT−kZ2Σk) − 2tr(ZT1 U−kΣ−kVT−kZ2)\n= tr(RT1 R1Σk) + tr(R T 2 R2Σk)− 2tr(RT1 Σ−kR2) ≥ tr(RT1 Σ−kR1) + tr(RT2 Σ−kR2)− 2tr(RT1 Σ−kR2) = tr[(R1 −R2)TΣ−k(R1 −R2)] ≥ 0.\nThe last inequality uses the fact that tr(RT1 R1Σk) ≥ tr(RT1 Σ−kR1) and tr(RT2 R2Σk) ≥ tr(RT2 Σ−kR2).\n7 Unitarily Invariant Norms\nIn this chapter we study unitarily invariant norms of a matrix, which can be defined via singular values of the matrix. Unitarily invariant norms were contributed by J. von Neumann, Robert Schatten, and Ky Fan. J. von Neumann established an equivalent relationship between unitarily invariant norms and symmetric gauge functions. There are two popular classes of unitarily invariant norms: the Ky Fan norms and Schatten p-norms.\nParallel with the vector p-norms, the Schatten p-norms are defined on singular values of a matrix. Their special cases include the spectral norm, Frobenius norm, and nuclear norm. They have wide applications in modern data analysis and computation. For example, the Frobenius norm is used to measure approximation errors in regression and reconstruction problems because it essentially equivalent to the ℓ2-norm of a vector. The spectral norm is typically used to describe convergence and convergence rate of an iteration procedure. The nuclear norm provides an effective approach to matrix low rank modeling.\nWe first briefly review matrix norms, and then present the notion of symmetric gauge functions. Symmetric gauge functions facilitate us to study unitarily invariant norms. First, it transforms a unitarily invari-\n52"
    }, {
      "heading" : "7.1. Matrix Norms 53",
      "text" : "ant norm on matrices to a norm on vectors equivalently. Second, it can incorporate majorization theory. Accordingly, we give some important properties of unitarily invariant norms."
    }, {
      "heading" : "7.1 Matrix Norms",
      "text" : "A function f : Rm×n → R is said to be a matrix norm if the following conditions are satisfied:\n(1) f(A) > 0 for all nonzero matrix A ∈ Rm×n;\n(2) f(αA) = |α|f(A) for any α ∈ R and any A ∈ Rm×n;\n(3) f(A+B) ≤ f(A) + f(B) for any A and B ∈ Rm×n.\nWe denote the norm of a matrix A by ‖A‖. Furthermore, if\n(4) ‖AB‖ ≤ ‖A‖‖B‖ where A ∈ Rm×n and B ∈ Rn×p,\nthe matrix norm is said to be consistent. In some literature, when one refers to a matrix norm on Rn×n. it is required to be consistent. Here we do not make this requirement.\nThere is an equivalence between any two norms. Let ‖·‖α and ‖·‖β be two norms on Rm×n. Then there exist positive numbers α1 and α2 such that for all A ∈ Rm×n,\nα1‖A‖α ≤ ‖A‖β ≤ α2‖A‖α.\nConditions (2) and (3) tell us that the norm is convex. Moreover, it is continuous because\n|‖A‖ − ‖B‖| ≤ ‖A−B‖ ≤ α‖A−B‖F , where α > 0.\nA norm always companies with its dual. The dual is a norm. Moreover, the dual of the dual norm is the original norm.\nDefinition 7.1. Let ‖ · ‖ be a given norm on Rm×n. Its dual (denoted ‖ · ‖∗) is defined as\n‖A‖∗ = max { tr(ABT ) : B ∈ Rm×n, ‖B‖ = 1 } ."
    }, {
      "heading" : "54 Unitarily Invariant Norms",
      "text" : "Proposition 7.1. The dual ‖ · ‖∗ has the following properties:\n(1) The dual is a norm.\n(2) (‖A‖∗)∗ = ‖A‖. (3) tr(ABT ) ≤ |tr(ATB)| ≤ ‖A‖‖B‖∗ (or ‖A‖∗‖B‖).\nThere are two approaches for definition of a matrix norm. In the first approach, the norm of matrix A is defined via its vectorization vec(A); that is, ‖A‖ = ‖vec(A)‖, which obviously satisfies Conditions (1)-(3). We refer to this class of the matrix norms as matrix vectorization norms for ease of exposition. Note that the Frobenius norm is a matrix vectorization norm because ‖A‖F = ‖vec(A)‖2. However, this class of matrix norms are not always consistent. For example, let\nA = B =\n[\n1 1 1 1\n]\n.\nSince AB =\n[\n2 2 2 2\n]\nand\n2 = ‖vec(AB)‖∞ > ‖vec(A)‖∞‖vec(B)‖∞ = 1,\nthis implies that the corresponding matrix norm is not consistent.\nIn the second approach, the matrix norm is defined by\n‖A‖ = max ‖x‖=1 ‖Ax‖,\nwhich is also called the induced or operator norm.\nTheorem 7.1. The operator norm on Rm×n is a consistent matrix norm.\nProof. Given a matrixA ∈ Rm×n, the result is trivial IfA = 0. Assume that A 6= 0. Then there exists a nonzero vector z ∈ Rn for which Az 6= 0. So we have ‖Az‖ > 0 and ‖z‖ > 0. Hence,\n‖A‖ = max x 6=0 ‖Ax‖ ‖x‖ ≥ ‖Az‖ ‖z‖ > 0."
    }, {
      "heading" : "7.1. Matrix Norms 55",
      "text" : "Conditions (2)-(3) are directly obtained from the definition of the vector norm. As for Condition (4), it can be established by\n‖ABx‖ ≤ ‖A‖‖Bx‖ ≤ ‖A‖‖B‖‖x‖ for any x 6= 0. Thus,\n‖AB‖ = max x 6=0 ‖ABx‖ ‖x‖ ≤ ‖A‖‖B‖.\nAs we have shown, ‖A‖2 = max‖x‖2=1 ‖Ax‖2 = σ1(A). It is thus called the spectral norm.\nNote that ‖UAV‖2 = ‖A‖2 and ‖UAV‖F = ‖A‖F for any m×m orthonormal matrix U and any n× n orthonormal matrix V. In other words, they are unitarily invariant.\nDefinition 7.2. A matrix norm is said to be unitarily invariant if ‖UAV‖ = ‖A‖ for any unitary matrices U and V. In this tutorial, we only consider real matrices. Thus, a unitarily invariant norm should be termed as “orthogonally invariant norm.” However, we still follow the term of the unitarily invariant norm and denote it by ||| · |||. Theorem 7.2. Let ‖ · ‖ be a given norm on Rm×n. Then it is unitarily invariant if and only if its dual is unitarily invariant.\nProof. Suppose ‖ · ‖ is unitarily invariant, and let U ∈ Rm×m and V ∈ Rn×n be orthonormal. Then\n‖UAV‖∗ = max { tr(UAVBT ) : B ∈ Rm×n, ‖B‖ = 1 }\n= max { tr(A(UTBVT )T ) : B ∈ Rm×n, ‖B‖ = 1 } = max { tr(ACT ) : C ∈ Rm×n, ‖UCV‖ = 1 } = max { tr(ACT ) : C ∈ Rm×n, ‖C‖ = 1 } = ‖A‖∗.\nThe converse follows from the fact that (‖A‖∗)∗ = ‖A‖.\nWe find that ‖A‖2 = ‖σ(A)‖∞ and ‖A‖F = ‖σ(A)‖2; that is, they correspond the norms on the vector σ(A) of the singular values of A. This sheds light on the relationship of a unitarily invariant norm of a matrix with its singular values."
    }, {
      "heading" : "56 Unitarily Invariant Norms",
      "text" : ""
    }, {
      "heading" : "7.2 Symmetric Gauge Functions",
      "text" : "In order to investigate the unitarily invariant norm, we first present the notion of symmetric gauge functions.\nDefinition 7.3. A real function φ : Rn → R is called a symmetric gauge function if it satisfies the following four conditions:\n(1) φ(u) > 0 for all nonzero u ∈ Rn.\n(2) φ(αu) = |α|φ(u) for any constant α ∈ R.\n(3) φ(u+ v) ≤ φ(u) + φ(v) for all u,v ∈ Rn.\n(4) φ(Duπ) = φ(u) where uπ = (uπ1 , . . . , uπn) with π as a permutation of\n[n] and D is an n× n diagonal matrix with ±1 diagonal elements.\nFurthermore, the gauge function is called normalized if it satisfies the condition:\n(5) φ(1, 0, . . . , 0) = 1.\nConditions (1)-(3) show that that the gauge function is a vector norm. Thus, it is convex and continuous. Condition (4) says that the gauge function is symmetric.\nLemma 7.3. [Schatten, 1950] Let u,v ∈ Rn. If |u| ≤ |v|, then φ(u) ≤ φ(v) for every symmetric gauge function φ.\nProof. In terms of Condition (4), we can directly assume that u ≥ 0 and v ≥ 0. Currently, the argument is equivalent to\nφ(ω1v1, . . . , ωnvn) ≤ φ(v1, . . . , vn)\nfor ωi ∈ [0, 1]. Thus, by induction, it suffices to prove\nφ(v1, . . . , vn−1, ωvn) ≤ φ(v1, . . . , vn)\nwhere ω ∈ [0, 1] for every symmetric gauge function φ. It follows from"
    }, {
      "heading" : "7.2. Symmetric Gauge Functions 57",
      "text" : "the following direct computation:\nφ(v1, . . . , vn−1, ωvn)\n= φ (1+ω\n2 v1+ 1−ω 2 v1, . . . , 1+ω 2 vn−1+ 1−ω 2 vn−1, 1+ω 2 vn − 1−ω 2 vn )\n≤ 1 + ω 2 φ(v1, . . . , vn−1, vn) + 1− ω 2 φ(v1, . . . , vn−1,−vn) = φ(v1, . . . , vn−1, vn).\nTheorem 7.4. [Fan, 1951] Given two nonnegative vectors u,v ∈ Rn+, then u ≺w v if and only if φ(u) ≤ φ(v) for every symmetric gauge function φ.\nProof. The necessity is obtained by setting a set of special symmetric gauge functions φk for k ∈ [n]. Specifically, they are defined as\nφk(x) = max 1≤i1≤···≤ik≤n\nk∑\nl=1\n|xil |.\nwhere x = (x1, . . . , xn).\nIt remains to prove the sufficiency. Without loss of generality, we assume that u1 ≥ · · · ≥ un and v1 ≥ · · · ≥ vn. Let z = (z1, . . . , zn)T where zi = vi for i ∈ [n − 1] and zn = vn − ∑n i=1(vi − ui). Obviously, z ≤ v. And it follows from u ≺w v that u ≺ z. In terms of the theorem of Hardy, Littlewood, and Pólya (see Lemma 2.2), there exists a doubly stochastic matrix (say W) such that u = Wz. Since W(v − z) ≥ 0, we have u ≤ Wv. Thus, by Lemma 7.3, φ(u) ≤ φ(Wv) for every symmetric gauge function. Consider that a doubly stochastic matrix can be expressed a convex combination of a set of permutation matrices (see Lemma 2.3). We write W = ∑\nj=1 αjPj where αj ≥ 0 and ∑ j αj =\n1, and the Pj are permutation matrices. Accordingly,\nφ(u) ≤ φ( ∑\nj\nαjPjv) ≤ ∑\nj\nαjφ(Pjv) = ∑\nj\nαjφ(v) = φ(v)."
    }, {
      "heading" : "58 Unitarily Invariant Norms",
      "text" : "It is worth noting that the proof of Theorem 7.4 implies that if φk(u) ≤ φk(v) for k ∈ [n], then φ(u) ≤ φ(v) for every symmetric gauge function φ. In other words, an infinite family of norm inequalities follows from a finite one.\nDefinition 7.4. The dual of a symmetric gauge function φ on Rn is defined as\nφ∗(u) , max { uTv : v ∈ Rn, φ(v) = 1 } .\nProposition 7.2. Let φ∗ be the dual of the symmetric gauge function φ. Then φ∗ is also a symmetric gauge function. Moreover, (φ∗)∗ = φ.\nProof. For a nonzero vector u ∈ Rn, then φ(u) > 0. Hence,\nmax φ(v)=1\nuTv ≥ u Tu\nφ(u) > 0.\nIt is also seen that\nφ∗(u+v) = max φ(z)=1 (u+v)Tz ≤ max φ(z)=1 uT z+ max φ(z)=1 vTz ≤ φ∗(u) + φ∗(v).\nAs for the symmetry of φ∗ can be directly obtained from that of φ. Finally, note that φ∗ is a norm on Rn. Thus, (φ∗)∗ = φ."
    }, {
      "heading" : "7.3 Unitarily Invariant Norms via SGFs",
      "text" : "There is a one-to-one correspondence between a unitarily invariant norm and a symmetric gauge function (SGF).\nTheorem 7.5. If ||| · ||| is a given unitarily invariant norm on Rm×n, then there is a symmetric gauge function φ on Rq where q = min{m,n} such that |||A||| = φ(σ(A)) for all A ∈ Rm×n.\nConversely, if φ is a symmetric gauge function on Rq, then |||A||| , φ(σ(A)) is a unitarily invariant norm on Rm×n.\nProof. Given a unitarily invariant norm ||| · ||| on Rm×n and a vector x ∈ Rq, define φ(x) , |||X||| where X = [xij ] ∈ Rm×n satisfying that xii = xi for i ∈ [q] and all other elements are zero. That φ is a norm on R q follows from the fact that ||| · ||| is a norm. The unitary invariance of"
    }, {
      "heading" : "7.3. Unitarily Invariant Norms via SGFs 59",
      "text" : "||| · ||| then implies that φ satisfies the symmetry. Now let A = UΣVT be the full SVD of A. Then |||A||| = |||UΣVT ||| = |||Σ||| = φ(σ(A)).\nConversely, if φ is a symmetric gauge function, for any A ∈ Rm×n define |||A||| = φ(σ(A)). We now prove that ||| · ||| is a unitarily invariant norm. First, that |||A||| > 0 for A 6= 0 and |||αA||| = |α||||A||| for any constant α follows the fact that φ is a norm. The unitary invariance of ||| · ||| follows from that for any orthonormal matrices U (m×m) and V (n× n), UAV and A have the same singular values. Finally,\n|||A+B||| = φ(σ(A+B)) ≤ φ(σ(A) + σ(B)) ≤ φ(σ(A)) + φ(σ(B)) = |||A|||+ |||B|||.\nHere the first inequality follows Proposition 6.3 and Theorem 7.4.\nThe following theorem implies that there is also a one-one correspondence between the dual of a symmetric gauge function and a dual unitarily invariant norm. Theorem 7.6. Let φ∗ be the dual of symmetric gauge function φ. Then |||A||| = φ(σ(A)) if and only if |||A|||∗ = φ∗(σ(A)). Proof. Assume that |||A||| = φ(σ(A)). Then\n|||A|||∗ = max { tr(ATB) : B ∈ Rm×n, |||B||| = 1 }\n= max { tr(ΣTAU T ABVA) : φ(σ(B)) = 1 } ,\nwhere A = UAΣAV T A is a full SVD of A. By Theorem 6.3, we have\ntr(VTAB TUAΣA) ≤ max UT U=Im,VT V=In tr(VTBTUΣA) =\nq ∑\ni=1\nσi(A)σi(B).\nWhen letting B = UAΣBV T A as a full SVD of B, we can obtain that\n|||A|||∗ = max { tr(ΣTAΣB), φ(σ(B)) = 1 } = φ∗(σ(A)).\nConversely, the result follows from the fact that (φ∗)∗ = φ.\nGiven a matrix A ∈ Rm×n, let it have a full SVD: A = UΣVT . Then |||A||| = |||Σ|||. As we have seen, for x ∈ Rn the function\nφ(x) , max 1≤i1≤···≤ik≤n\nk∑\nl=1\n|xil |"
    }, {
      "heading" : "60 Unitarily Invariant Norms",
      "text" : "is a symmetric gauge function. Thus, ∑k\ni=1 σi(A) defines also a class of\nunitarily invariant norms which are the so-called Ky Fan k-norms.\nClearly, the vector p-norm ‖ · ‖p for p ≥ 1 is a symmetric gauge function. Thus, Theorem 7.5 shows that |||A|||p , ‖σ(A)‖p for p ≥ 1 are a class of unitarily invariant norms. They are well known as the Schatten p-norms. Thus, ‖A‖F = ‖σ(A)‖2 = |||A|||2 and ‖A‖2 = ‖σ(A)‖∞ = |||A|||∞.\nWhen p = 1, ‖A‖∗ , |||A|||1 = ‖σ(A)‖1 = ∑min{m,n} i=1 σi(A) is called the nuclear norm or trace norm, which has been widely used in many machine learning problems such as matrix completion, matrix data classification, multi-task learning, etc. [Srebro et al., 2004, Cai et al., 2010, Mazumder et al., 2010, Liu et al., 2013, Luo et al., 2015, Kang et al., 2011, Pong et al., 2010, Zhou and Li, 2014]. Parallel with the ℓ1-norm which is used as convex relaxation of the ℓ0-norm [Tibshirani, 1996], the nuclear norm is a convex alternative of the matrix rank. Since the nuclear norm is the best convex approximation of the matrix rank over the unit ball of matrices, this makes it more tractable to solve the resulting optimization problem (see Example 8.1 below)."
    }, {
      "heading" : "7.4 Properties of Unitarily Invariant Norms",
      "text" : "Theorem 7.5 opens an approach for exploring unitarily invariant norms by using symmetric gauge functions and majorization theory. We will see that this makes things more tractable.\nTheorem 7.7. Let ||| · ||| be a unitarily invariant norm on Rn×n. Then it is consistent.\nTheorem 7.7 follows immediately from Theorem 6.6. However, when the norm is defined on Rm×n, Theorem 6.6 can not help to establish the consistency of the corresponding unitarily invariant norm.\nAs an immediate corollary of Theorem 7.5, we have the following\nresult, which shows that unitarily invariant norms are monotone.\nTheorem 7.8. Let ||| · ||| be a given unitarily invariant norm on Rm×n. Then |||A||| ≤ |||B||| if and only if σ(A) ≺w σ(B)."
    }, {
      "heading" : "7.4. Properties of Unitarily Invariant Norms 61",
      "text" : "Proposition 7.3. Given a matrix A ∈ Rm×n, let [A]r be obtained by replacing the last r rows and r columns of A with zeros, and 〈A〉r by replacing the last r rows or columns of A with zeros. Let q = min{m,n}. Then for any r ∈ [q],\n|||[A]r||| ≤ |||〈A〉r||| ≤ |||A|||.\nProof. Part (1) directly follows from Proposition 6.3 which shows that σ([A]r) ≺w σ(〈A〉r) ≺w σ(A).\nProposition 7.4. Given two matrices A ∈ Rm×n and B ∈ Rm×n, we have that |||diag(σ(A)− σ(B))||| ≤ |||A−B|||. Furthermore, if both A and B are symmetric matrixes in Rm×m, then\n|||diag(σ(A)− σ(B)) ≤ |||diag(λ(A)− λ(B))|||||| ≤ |||A−B|||.\nProof. The first part of the proposition is immediately obtained from Theorem 6.5. As for the second part, Proposition 6.1-(i) says that λ(A)−λ(B) ≺ λ(A−B). It then follows from Lemmas 2.2 and 2.3 that λ(A) − λ(B) = ∑j αjPjλ(A −B) where the αj ≥ 0 and ∑ j αj = 1, and the Pj are some permutation matrices. Accordingly, for every symmetric gauge function φ on Rm, we have that\nφ(λ(A)− λ(B)) = φ( ∑\nj\nαjPjλ(A−B)) ≤ ∑\nj\nαjφ(Pjλ(A−B))\n= ∑\nj\nαjφ(λ(A−B)) = φ(λ(A−B)),\nwhich implies that |||diag(λ(A) − λ(B))|||||| ≤ |||A − B|||. Additionally, consider that for a symmetric matrixM, it holds that σi(M) = |λi(M)|. Hence, we have that\n|λi(A)− λi(B)| ≥ ∣ ∣|λi(A)| − |λi(B)| ∣ ∣ = |σi(A)− σi(B)|.\nThis concludes the proof.\nAs a direct corollary of Proposition 6.5, we have that\n|σi(A)− σi(B)| ≤ ‖A−B‖2, for i = 1, . . . , q,"
    }, {
      "heading" : "62 Unitarily Invariant Norms",
      "text" : "where q = min{m,n}, and √ √ √ √ q ∑\ni=1\n(σi(A)− σi(B))2 ≤ ‖A−B‖F .\nWhen A and B are both symmetric, we also have that\n|λi(A)− λi(B)| ≤ ‖A−B‖2, for i = 1, . . . ,m, √ √ √ √ m∑\ni=1\n(λi(A)− λi(B))2 ≤ ‖A−B‖F .\nThe latter result is well known as the Hoffman-Wielandt theorem. Note that the Hoffman-Wielandt theorem still hods when A and B are normal [Stewart and Sun, 1990].\nTheorem 7.9. Let ||| · ||| be an arbitrary unitarily invariant norm on R m×n, and E11 ∈ Rm×n have the entry 1 in the (1, 1)th position and zeroes elsewhere. Then\n(a) |||A||| = |||AT |||. (b) σ1(A)|||E11||| ≤ |||A||| ≤ ‖A‖∗|||E11|||. (c) If the symmetric gauge function φ corresponding to the norm ||| · ||| is\nnormalized (i.e., φ(1, 0, 0, . . . , 0) = 1), then\n‖A‖2 ≤ |||A||| ≤ ‖A‖∗. Proof. Part (a) is due to that φ(σ(A)) = φ(σ(AT )).\nIf φ(1, 0, . . . , 0) = 1, then |||E11||| = 1. Thus, we can have Part (c) from Part (b). Assume A is nonzero. Otherwise, the result is trivial. Let q = min{m,n}. First, |||A||| = φ(σ1(A), . . . , σq(A)) = σ1(A)φ(1, σ2(A)/σ1(A), . . . , σq(A)/σ1(A)) ≥ σ1(A)φ(1, 0, . . . , 0) = σ1(A)|||E11|||. Since ( σ1(A)/ ∑q i=1 σi(A), . . . , σq(A)/ ∑q i=1 σi(A) ) ≺ (1, 0, . . . , 0), we have\n|||A||| = ( q ∑\ni=1\nσi(A))φ ( σ1(A)/\nq ∑\ni=1\nσi(A), . . . , σq(A)/ q ∑\ni=1\nσi(A) )\n≤ ‖A‖∗φ(1, 0, . . . , 0) = ‖A‖∗|||E11|||."
    }, {
      "heading" : "7.4. Properties of Unitarily Invariant Norms 63",
      "text" : "Note that a norm ‖ · ‖ on Rm×n is said to be self adjoint if ‖A‖ = ‖AT ‖ for any A ∈ Rm×n. Thus, Theorem 7.9-(a) shows that the unitarily invariant norm is self-adjoint.\nIt is worth mentioning that |||Eij||| = |||E11||| where Eij ∈ Rm×n has entry 1 in the (i, j)th position and zeros elsewhere. Moreover, the Schatten p-norms satisfy |||E11|||p = 1. Theorem 7.9 says that for any unitarily invariant norm ||| · ||| such that |||E11||| = 1,\n1 ≤ |||A|||‖A‖2 ≤ ‖A‖∗‖A‖ 2 ≤ rank(A).\nRecall that\n∑q i=1 σ2 i (A)\nσ2 1 (A)\n= ‖A‖2 F\n‖A‖2 2\nand\n∑q i=1 σi(A)\nσ1(A) = ‖A‖∗‖A‖2 , so called\nstable rank and nuclear rank (see Definition 3.2). They have been found usefulness in the analysis of matrix multiplication approximation [Magen and Zouzias, 2011, Cohen et al., 2015, Kyrillidis et al., 2014].\nTheorem 7.10. Let M ∈ Rm×m, N ∈ Rn×n, and A ∈ Rm×n such that the block matrix [\nM A\nAT N\n]\nis SPSD. Then\n|||M|||+ |||N||| ≥ 2|||A|||.\nProof. Without loss of generality, we assume m ≥ n. Let A = UΣVT be a thin SVD of A. Consider that [UT ,−VT ] [ M A\nAT N\n] [\nU\n−V\n]\n= UTMU+VTNV−UTAV−VTATU\nis PSD. Hence, |||UTMU+VTNV||| ≥ 2|||Σ|||. That is,\n|||VTUTMUV+N||| ≥ 2|||A|||.\nNote that\n|||VTUTMUV+N||| ≤ |||VTUTMUV|||+ |||N||| ≤ |||M|||+ |||N|||."
    }, {
      "heading" : "64 Unitarily Invariant Norms",
      "text" : "Proposition 7.5. Given a matrix A ∈ Rm×n, then the following holds\n|||A||| = min X,Y:XYT=A\n1\n2\n{ |||XXT |||+ |||YYT ||| } .\nIf rank(A) = r ≤ min{m,n}, then the minimum above is attained at a rank decomposition A = X̂ŶT where X̂ = UrΣ 1/2 r and Ŷ = VrΣ 1/2 r , and A = UrΣrV T r is a condensed SVD of A.\nProof. Let A = XYT be any decomposition of A. Then [\nX\nY\n]\n[XT ,XT ] =\n[\nXXT XYT YXT YYT\n]\nis SPSD. Thus, 1\n2\n[ |||XXT |||+ |||YYT ||| ] ≥ |||A|||.\nWhen X , X̂ = UrΣ 1/2 r and Y , Ŷ = VrΣ 1/2 r , it holds that |||A||| = 1 2 [ |||X̂X̂T |||+ |||ŶŶT ||| ] .\nSince 12\n[ |||XXT |||+ |||YYT ||| ] ≥ √ |||XXT ||| √ |||YYT |||,\n|||A||| ≥ min X,Y:XYT=A\n√ |||XXT ||| √ |||YYT |||.\nWhen taking X̂ = UrΣ 1/2 r V T r and Ŷ = VrΣ 1/2 r V T r , one has\n|||A||| = √ |||X̂X̂T ||| √ |||ŶŶT |||.\nThis thus leads us to the following proposition.\nProposition 7.6. Given a matrix A ∈ Rm×n, then the following holds\n|||A||| = min X,Y:XYT=A\n√ |||XXT ||| √ |||YYT |||.\nAccordingly, the following inequality hods:\n|||XYT ||| ≤ |||XXT |||1/2|||YYT |||1/2. (7.1)\nThis is a form of the Cauchy-Schwarz inequality under the unitarily\ninvariant norms."
    }, {
      "heading" : "7.4. Properties of Unitarily Invariant Norms 65",
      "text" : "As a corollary of Proposition 7.5, the following proposition immediately follows. Moreover, this proposition was widely used in matrix completion problems, because an optimization problem regularized by the Frobenius norm is solved more easily than that regularized by the nuclear norm [Hastie et al., 2014].\nProposition 7.7. [Srebro et al., 2004, Mazumder et al., 2010] Given a matrix A ∈ Rm×n, then the following holds\n‖A‖∗ = min X,Y:XYT=A\n1 2\n{ ‖X‖2F + ‖Y‖2F } .\nIf rank(A) = k ≤ min{m,n}, the minimum above is attained at some rank decomposition.\nThe following theorem shows that the Frobenius norm has a so-called matrix-Pythagoras’ property. However, for other Schatten norms, there needs a strong condition to make the property hold.\nTheorem 7.11. Let A,B ∈ Rm×n. If ABT = 0 or ATB = 0, then\n‖A+B‖2F = ‖A‖2F + ‖B‖2F ,\nmax{‖A‖22, ‖B‖22} ≤ ‖A+B‖22 ≤ ‖A‖22 + ‖B‖22. If both ABT = 0 and ATB = 0 are satisfied, then\n|||A+B|||pp = |||A||| p p + |||B||| p p\nfor 1 ≤ p < ∞ and ‖A+B‖2 = max{‖A‖2, ‖B‖2}.\nProof. Since (A +B)T (A + B) = ATA + BTB when ATB = 0, the Pythagorean property for the Frobenius norm is obvious. As for the spectral norm, it is easily seen that\n‖A+B‖22 = max‖x‖2=1 xT (A+B)T (A+B)x\n= max ‖x‖2=1\nxT (ATA+BTB)x\n≤ max ‖x‖2=1 xTATAx+ max ‖x‖2=1 xTBTBx = ‖A‖22 + ‖B‖22."
    }, {
      "heading" : "66 Unitarily Invariant Norms",
      "text" : "Let the condensed SVDs of A and B be A = UAΣAV T A and B = UBΣBV T B . If A TB = 0 and ABT = 0, then VTAVB = 0 and U T AUB = 0. Note that\nA+B = [UA,UB ]\n[\nΣA 0\n0 ΣB\n] [\nVTA VTB\n]\nis the condensed SVD of A+B. So the nonzero singular values of A+B consist of those of A and of B. The theorem accordingly follows.\nLet us end this chapter by showing a relationship among the matrix\noperator, matrix vectorization, and unitarily invariant norms.\nTheorem 7.12. Let f be a matrix norm on Rm×n.\n(a) The norm f is both unitarily invariant and operator norm if and only\nif f(A) = ‖A‖2 for any A ∈ Rm×n. In other words, the spectral norm is only one operator norm that satisfies the self-adjoint property. (b) Given a matrix A ∈ Rm×n, f(A) , ‖vec(A)‖ is unitarily invariant if and only if it is the norm γ‖A‖F for some γ > 0.\nProof. The proof of Part (a) can be found in Corollary 5.6.35 of Horn and Johnson [1985]. As for Part (b), it is obvious that the Frobenius norm is both unitarily invariant and vectorization norm. Conversely, given any A ∈ Rm×n, the vectorization norm is defined as ‖a‖ where a = vec(A). Recall that the vector a can be regarded as an mn × 1 matrix. Let a = UaΣavTa be the full SVD of a. Then it is easily seen that Σa = (‖A‖F , 0, . . . , 0)T . Moreover, we can set va = 1. For any orthonormal matrices U ∈ Rm×m and V ∈ Rn×n, we have that f(UAVT ) = ‖vec(UAVT )‖ = ‖(V ⊗U)vec(A)‖ = ‖a‖ due to the unitary invariance. Moreover, we have that ‖a‖ = ‖Σa‖ = ‖A‖F ‖(1, 0, . . . , 0)‖. Letting γ = ‖(1, 0, . . . , 0)‖ > 0, we complete the proof. Notice that if the norm is normalized, then γ = 1.\n8 Subdifferentials of Unitarily Invariant Norms\nIn the previous chapters, we have used matrix differential calculus. Let f : Rm×n → R. We have discussed the gradient and Hessian of f w.r.t. X ∈ Rm×n. Especially, the function f : Rm×n → R is defined as a trace function. Such a function is differentiable. In this chapter we consider f to be a unitarily invariant norm.\nNorm functions are not necessarily differentiable. For example, the spectral norm and nuclear norm are not differentiable. But norm functions are convex and continuous, so we can resort to theory of subdifferentials [Rockafellar, 1970, Borwein and Lewis, 2006]. Indeed, the subdifferentials of unitarily invariant norms have been studied by Watson [1992] and Lewis [2003].\nUsing the properties of unitarily invariant norms and the SVD theory, we present directional derivatives and subdifferentials of unitarily invariant norms. As two special cases, we report the subdifferentials of the spectral norm and nuclear norm. These two norms have been widely used in machine learning such as matrix low rank approximation. We illustrate applications of the subdifferentials in optimization problems regularized by either the spectral norm or the nuclear norm. We also study the use of the subdifferentials of unitarily invariant norms in solv-\n67"
    }, {
      "heading" : "68 Subdifferentials of Unitarily Invariant Norms",
      "text" : "ing least squares estimation problems, whose loss function is defined as any unitarily invariant norm."
    }, {
      "heading" : "8.1 Subdifferentials",
      "text" : "Let ‖ · ‖ be a given norm on Rm×n, and A be a given matrix in Rm×n. The subdifferential, a set of subgradients, of ‖A‖ is defined as\n{ G ∈ Rm×n : ‖B‖ ≥ ‖A‖+ tr((B−A)TG) for all B ∈ Rm×n } ,\nand denoted by ∂‖A‖. When the norm ‖ · ‖ is differentiable, the subgradient degenerates to the gradient. That is, the subdifferential is a singleton. For example, when taking the squared Frobenius norm ‖A‖2F = tr(ATA), ∂‖A‖2F = {2A}.\nLemma 8.1. Let A ∈ Rm×n be a given matrix. Then G ∈ ∂‖A‖ if and only if ‖A‖ = tr(GTA) and ‖G‖∗ ≤ 1.\nProof. The sufficiency is immediate. Now assume thatG ∈ ∂‖A‖. Then taking B = 2A yields ‖G‖ ≥ tr(ATG) and taking B = 12A yields 1 2‖A‖ ≤ 12 tr(ATG), which implies that ‖A‖ = tr(ATG). Subsequently, ‖B‖ ≥ tr(GTB) for all matrices B. Thus, the dual norm satisfies\n‖G‖∗ = max{tr(GTB) : ‖B‖ = 1} ≤ 1.\nWe especially consider the subdifferential of unitarily invariant norms. Given a unitarily invariant norm ||| · ||| on Rm×n, let p = min{m,n}. Theorem 7.5 shows there exists a symmetric gauge function φ : Rp → R associated with the norm ||| · |||. Thus, this encourages us to define the subdifferential of unitarily invariant norms via the subdifferential of symmetric gauge functions.\nThe subdifferential of the symmetric gauge function φ at x ∈ Rp is\n∂φ(x) , {z ∈ Rp : φ(y) ≥ φ(x) + (y− x)Tz for all y ∈ Rp}.\nIn terms of Lemma 8.1, that z ∈ ∂φ(x) is equivalent to that φ(x) = xTz and φ∗(z) ≤ 1. Here φ∗ is the dual of φ (see Definition 7.4) which is a"
    }, {
      "heading" : "8.1. Subdifferentials 69",
      "text" : "symmetric gauge function for the dual norm ||| · |||∗. That is, φ∗(σ(A)) = |||A|||∗ (see Theorem 7.6).\nLet us return to the subdifferential of unitarily invariant norms.\nThe following lemma gives the directional derivative of |||A|||.\nLemma 8.2. Let ||| · ||| be a given unitarily invariant norm on Rm×n, and φ be the corresponding symmetric gauge function. Then the directional derivative of the norm at A ∈ Rm×n in a direction R ∈ Rm×n is\nlim t↓0 |||A+tR||| − |||A||| t = max d∈∂φ(σ(A))\np ∑\ni=1\ndiu T i Rvi = max G∈∂|||A||| tr(RTG).\nHere p = min{m,n}, U = [u1, . . . ,um], V = [v1, . . . ,vn], Σ = diag(σ(A)), and A = UΣVT is a full SVD of A.\nProof. By Lemma 2.5, we immediately have\nlim t↓0 |||A+ tR||| − |||A||| t = max G∈∂|||A||| tr(RTG).\nWe now prove the first equality. Let z = (uT1 Rv1, . . . ,u T p Rvp) T . Consider that\n|||A+ tR||| = |||Σ+ tUTRV||| = φ(σ(Σ+ tUTRV)) ≥ φ(σ(A) + tz)\nbecause σ(A)+tz ≺w σ(Σ+tUTRV) by Proposition 6.3. Accordingly, we have that\nlim t↓0 |||A+tR||| − |||A||| t ≥ lim t↓0 φ(σ(A)+tz)− φ(σ(A)) t = max d∈∂φ(σ(A)) dTz.\nThe above equality follows from Lemma 2.5, when applied to the symmetric gauge function φ.\nOn the other hand, let σ(t) , σ(A+tR) = σ(Σ+tUTRV). Now\nwe have\n|||A||| − |||A+tR||| t = |||A+tR−tR||| − |||A+tR||| t\n= φ(σ(Σ+tUTRV−tUTRV)) − φ(σ(t))\nt\n≥ φ(σ(t)− tz)− φ(σ(t)) t ≥ −d(t)Tz [where d(t) ∈ ∂φ(σ(t))]."
    }, {
      "heading" : "70 Subdifferentials of Unitarily Invariant Norms",
      "text" : "The above first inequality follows from σ(t)− tz ≺w σ(A). The second inequality is based on the property of the subgradient of φ at σ(t). Note that φ is a continuous function. By the definition of ∂φ(σ(t)), it is directly verified that lim t→0+ d(t) → d0 ∈ ∂φ(σ(A)). Thus,\nlim t↓0 |||A+tR||| − |||A||| t ≤ lim t↓0 d(t)T z = dT0 z ≤ max d∈∂φ(σ(A)) dTz.\nThis implies that the first equality also holds.\nTheorem 8.3. Let A ∈ Rm×n have a full SVD A = UΣVT , and let σ = dg(Σ). Then\n∂|||A||| = conv { UDVT : d ∈ ∂φ(σ),D = diag(d) } .\nwhere φ is a symmetric gauge function corresponding to the norm ||| · |||. Here the notation “conv{·}” represents the convex hull of a set, which is closed and convex. If G ∈ ∂|||A|||, Theorem 8.3 says that G can be expressed as\nG = ∑\ni\nαiU (i)D(i)(V(i))T ,\nwhere αi ≥ 0, ∑ i αi = 1, A = U (i)Σ(V(i))T is a full SVD, di ∈ φ(σ), and D(i) = diag(di). According to Corollary 3.3, we can rewrite G as\nG = ∑\ni\nαiUQ (i)D(i)(P(i))TVT , (8.1)\nwhere P(i) and Q(i) are defined as P and Q in Corollary 3.3; i.e., they satisfy that Q(i)Σ(P(i))T = Σ and (Q(i))TΣP(i) = Σ.\nProof. First of all, we denote the convex hull on the right-hand side by G(A). Assume that G ∈ G(A). We now prove G ∈ ∂|||A|||. Based on Lemma 8.1, we try to show that |||A||| = tr(ATG) and |||G|||∗ ≤ 1. In terms of the above discussion, we can express G as in (8.1). Thus,\ntr(ATG) = ∑\ni=1\nαitr(A TUQ(i)D(i)(P(i))TVT )\n= ∑\ni=1\nαitr((P (i))TΣTQ(i)D(i)) =\n∑\ni=1\nαitr(Σ TD(i))\n= ∑\ni=1\nαid T i σ = φ(σ) = |||A|||."
    }, {
      "heading" : "8.1. Subdifferentials 71",
      "text" : "Additionally,\n|||G|||∗ = max |||R|||≤1 tr(GTR) = max |||R|||≤1 tr ( RT\n∑\ni=1\nαiU (i)D(i)(V(i))T ) .\nSince for each i, |||U(i)D(i)(V(i))T |||∗ = |||D(i)|||∗ = φ∗(di) ≤ 1, and by Proposition 7.1 we have tr(RTU(i)D(i)(V(i))T ) ≤ |||R||| × |||U(i)D(i)(V(i))T |||∗ ≤ |||R|||. Thus, |||G|||∗ ≤ 1. In summary, we have G ∈ ∂|||A|||.\nConversely, assume that G ∈ ∂|||A||| but G /∈ G(A). Then by the well-known separation theorem [Borwein and Lewis, 2006, see Theorem 1.1.1] there exists a matrix R ∈ Rm×n such that tr(RTX) < tr(RTG) for all X ∈ G(A).\nThis implies that\nmax d∈∂φ(σ)\n∑\ni=1\ndiu T i Rvi = max X∈G(A) tr(RTX) < max G∈∂|||A||| tr(RTG).\nThis contradicts with Lemma 8.2. Thus, the theorem follows.\nWe are especially interested in the spectral norm ‖ · ‖2 and the nuclear norm ‖·‖∗. As corollaries of Theorem 8.3, we have the following the results.\nCorollary 8.4. Let A have rank r ≤ p = min{m,n} and A = UrΣrVTr be a condensed SVD. Then the subdifferential of ‖A‖∗ is give as ∂‖A‖∗ = { UrV T r +W : W ∈ Rm×n s.t. UTr W = 0,WVr = 0, ‖W‖2 ≤ 1 } .\nProof. For the nuclear norm, the corresponding symmetric gauge function is φ(σ) = ‖σ‖1 = ∑p i=1 σi. Moreover,\n∂‖σ‖1 = { u ∈ Rp : ‖u‖∞ ≤ 1 and ui = 1 for i = 1, . . . , r } .\nLet G ∈ ∂‖A‖∗. By Theorem 8.3 and Corollary 3.3, we have G = ∑\ni=1\nαiUQ (i)D(i)(P(i))TVT\n= UrV T r +\n∑\ni=1\nαiU−rQ (i) 0 D (i) −r(P (i) 0 ) TVT−r,"
    }, {
      "heading" : "72 Subdifferentials of Unitarily Invariant Norms",
      "text" : "where the αi ≥ 0 and ∑ i=1 αi = 1,D (i) = dg(di), di ∈ ∂φ(σ), andD(i)−r is the last (m − r) × (n − r) principal submatrix of D(i). Here Q(i) ∈ R m×m, P(i) ∈ Rn×n, Q(i)0 ∈ R(m−r)×(m−r), and P (i) 0 ∈ R(n−r)×(n−r) are orthonormal matrices, which are defined in Corollary 3.3. Let\nW , U−r [∑\ni=1\nαiQ (i) 0 D (i) −r(P (i) 0 )\nT ]\nVT−r. (8.2)\nObviously, UTr W = 0 and WVr = 0. Moreover,\n‖W‖2 ≤ ∑\ni=1\nαi‖D(i)−r‖2 ≤ 1.\nWe can also see that any matrix W satisfying the above three conditions always has an expression as in (8.2).\nCorollary 8.5. Let the largest singular value σ1 of A ∈ Rm×n have multiplicity t, and Ut and Vt consist of the first t columns of U and V respectively. Then\n∂‖A‖2 = { UtHV T t : H ∈ Rt×t s.t. H is SPSD, tr(H) = 1 } .\nProof. The corresponding symmetric gauge function is φ(σ) = ‖σ‖∞, and its subdifferential is\n∂‖σ‖∞ = conv{ei : i = 1, . . . , t},\nwhere ei is the ith column of the identity matrix. It then follows from Theorem 8.3 that for any G ∈ ∂‖A‖2, it can be written as\nG = ∑\ni=1\nαiUtQ (i)D (i) t (Q (i))TVTt ,\nwhere the αi ≥ 0 and ∑ i=1 αi = 1, and Q (i) is an arbitrary t × t orthonormal matrix (see Theorem 3.2). Here Di = dg(di), di ∈ ∂φ(σ), and D\n(i) t is the first t× t principal submatrix of D(i). Let\nH = ∑\ni=1\nαiQ (i)D (i) t (Q (i))T , (8.3)\nwhich is SPSD and satisfies tr(H) = 1. Conversely, any SPSD matrix H satisfying tr(H) = 1 can be always expressed as the form of (8.3)."
    }, {
      "heading" : "8.2. Applications 73",
      "text" : ""
    }, {
      "heading" : "8.2 Applications",
      "text" : "In this section we present several examples to illustrate the application of the subdifferential of unitarily invariant norms in solving an optimization problem regularized by a unitarily invariant norm or built on any unitarily invariant norm loss.\nExample 8.1. Given a nonzero matrix A ∈ Rm×n, consider the following optimization problem:\nmin X∈Rm×n\nf(X) , 1\n2 ‖X−A‖2F + τ‖X‖∗, (8.4)\nwhere τ > 0 is a constant. Clearly, the problem is convex in X. This problem is a steppingstone of matrix completion. Let A = UrΣrV T r be a given condensed SVD of A, and define\nX̂ = Ur[Σr − τIr]+Vr,\nwhere [Σr−τIr]+ = diag([σ1−τ ]+, . . . , [σr−τ ]+) and [z]+ = max(z, 0). Now it can be directly checked that\n∂f(X̂) = X̂−A+ τ∂‖X̂‖.\nAssume that the first k singular values σi are greater than τ . Then,\n1 r (A− X̂) = UkVTk + 1 τ Uk+1:rdiag(σk+1, . . . , σr)V T k+1:r,\nwhich belongs to ∂‖X̂‖. In other words, 0 ∈ ∂f(X̂) (see Corollary 8.4). Thus, X̂ is a minimizer of the optimization problem. It is called the singular value thresholding (SVT) operator [Cai et al., 2010]. We can see that the parameter τ controls the rank of the matrix X̂ and the problem is able to yield a low rank solution to the matrix X. That is, X̂ is a low rank approximation to the matrix A.\nExample 8.2. Given a nonzero matrix A ∈ Rm×n, consider the following optimization problem:\nmin X∈Rm×n\nf(X) , 1\n2 ‖X−A‖2F + τ‖X‖2, (8.5)"
    }, {
      "heading" : "74 Subdifferentials of Unitarily Invariant Norms",
      "text" : "where τ > 0 is a constant. Also, this problem is convex in X. Let A have the k distinct positive singular values δ1 > δ2 > · · · > δk among the σi, with respective multiplicities r1, . . . , rk. Thus, the rank of A is r = ∑k i=1 ri. Let mt = ∑t i=1 ri and µt = ∑t i=1 riδi for t = 1, . . . , k. So mk = r and µk = tr(Σr) = ∑r\ni=1 σi. Assume that τ ≤ µk. We now consider two cases.\nIn the first case, assume l ∈ [k− 1] is the smallest integer such that l∑\ni=1\nri(δi − δl+1) = µl − δl+1ml > τ,\nand hence, δl ≥ µl−τml > δl+1. Note that l+1∑\ni=1\nri(δi − δl+2) = l∑\ni=1\nri(δi−δl+1)+ l+1∑\ni=1\nri(δl+1−δl+2)\n> l∑\ni=1\nri(δi−δl+1) > τ.\nThis implies that l is identifiable. Denoting δ = µl−τml , we define Σ̂ by replacing the first ml diagonal elements of Σr by δ, and then set X̂ = UrΣ̂rV T r . Now note that\n1 τ (A− X̂) = UmlHVTml ,\nwhere H = diag ( (σ1 − δ)/τ, . . . , (σml − δ)/τ ) . Clearly, H is PSD and tr(H) = ∑ml\ni=1 σi−δ τ = ∑l i=1 rl(δi−δ) τ = 1. It follows from Corollary 8.5\nthat 1τ (A− X̂) ∈ ∂‖X̂‖2. Thus, X̂ is a minimizer. In the second case, otherwise,\n∑k−1 i=1 ri(δi − δk) = µk−1 −mk−1δk ≤\nτ ≤ µk. Let δ = µk−τmk such that\n0 ≤ δ ≤ µk − µk−1 + δkmk−1 mk = δk.\nDefine X̂ = UrδIrV T . Then\n1 τ (A− X̂) = 1 τ Ur(Σr − δIr)VTr .\nSince 1τ (Σr− δIr) is PSD and 1τ tr(Σr− δIr) = 1, we obtain 0 ∈ ∂f(X̂). This implies that X̂ is a minimizer of the problem."
    }, {
      "heading" : "8.2. Applications 75",
      "text" : "As we have seen, the minimizer X̂ has the same rank with A. Thus, the problem in (8.5) can not give a low-rank solution. However, this problem makes the singular values of X̂ more well-conditioned because the top singular values decay to δ. Thus, we call it a singular value averaging (SVA) operator.\nExample 8.3. Given a nonzero matrix A ∈ Rm×n, consider the following convex optimization problem:\nmin X∈Rm×n\nf(X) , ‖X−A‖2 + τ‖X‖∗, (8.6)\nwhere τ > 0 is a constant. In the above model the loss function and regularization term are respectively defined as the spectral norm and the nuclear norma, which are mutually dual. Moreover, this model can be regarded as a parallel version of the Dantzig selector [Candès and Tao, 2007]. Thus, this model might be potentially interesting.\nLet A = UrΣrV T r be a condensed SVD. Assume that rτ > 1. Assume there are the k distinct positive singular values δ1 > δ2 > · · · > δk among the σi, with respective multiplicities r1, . . . , rk. Let mt = ∑t i=1 ri for t = 1, . . . , k.\nLet l ∈ [k] be the smallest integer such that mlτ ≥ 1 > ml−1τ . Define X̂ = Ur[Σr−δlIr]+VTr = Uml−1diag(σ1−δl, . . . , σml−1 −δl)VTml−1 . Then A− X̂ has the maximum singular value δl with multiplicity ml. It follows from Corollaries 8.4 and 8.5 that ∂‖X̂‖∗ = { Uml−1V T ml−1 +W : WTUml−1 = 0,WVml−1 = 0, ‖W‖2 ≤ 1 }\nand\n∂‖A− X̂‖2 = { −UmlHVTml : H is PSD, tr(H) = 1 } .\nTake W0 = U[ml−1+1:ml] (1−ml−1τ) rlτ IrlV T [ml−1+1:ml] . Note that W0Vml−1 = 0, W T 0 Uml−1 = 0, and ‖W0‖2 = (1−ml−1τ) rlτ\n≤ 1 due to ml−1τ + rlτ = mlτ ≥ 1 and ml−1τ < 1. Hence,\nτ∂‖X̂‖∗ ∋ τ(Uml−1VTml−1 +W0) = UmlH0V T ml ,\nwhereH0 = τ(Iml−1⊕ (1−ml−1τ) rlτ Irl). Clearly,H0 is PSD and tr(H0) = 1. Thus,\n−UmlH0VTml ∈ ∂‖A− X̂‖2."
    }, {
      "heading" : "76 Subdifferentials of Unitarily Invariant Norms",
      "text" : "As a result, 0 ∈ ∂‖A− X̂‖2+ τ∂‖X̂‖∗. Consequently, X̂ is a minimizer of the problem in (8.6). Compared with SVT in the model (8.4) which uses the tuning parameter τ as the thresholding value, the current model uses δl as the thresholding value.\nWe also consider the following convex optimization problem:\nmin X∈Rm×n\nf(X) , ‖X−A‖∗ + 1\nτ ‖X‖2. (8.7)\nClearly, the minimizer of the problem isA−X̂ where X̂ is the minimizer of the problem (8.6).\nExample 8.4. Finally, we consider the following optimization problem:\nmin X∈Rn×p\nf(X) , |||AX−B|||,\nwhereA ∈ Rm×n and B ∈ Rm×p are two given matrices. This is a novel matrix low rank approximation problem. We will further discuss this problem in Theorem 9.1 of Chapter 9. Here we are concerned with the use of Theorem 8.3 in solving the problem based on unitarily invariant norm loss functions.\nLet A = UrΣrV T r be a condensed SVD of A, and U−r and V−r be respective orthonormal complements of Ur and Vr. Now B−AA†B = U−rUT−rB. Thus, when taking X̂ = A †B, one has\n∂f(X̂) = AT∂|||U−rUT−rB|||.\nLet U0Σ0V T 0 = U T −rB be a thin SVD of U T −rB, D be a diagonal matrix, and φ be a symmetric gauge function associated with the norm ||| · |||. It follows from Theorem 8.3 that\n∂|||U−rUT−rB||| = conv{U−rU0DVT0 : U0,V0, dg(D) ∈ φ(dg(Σ0))}.\nThus, for any G ∈ ∂|||U−rUT−rB|||, it holds that ATG = 0. This implies that ∂f(X̂) = {0}. Hence, 0 ∈ ∂f(X̂). This implies that X̂ is a minimizer of the problem. In other words,\nmin X∈Rn×p\n|||AX−B||| = |||AA†B−B|||.\n9 Matrix Low Rank Approximation\nMatrix low rank approximation is very important, because it has received wide applications in machine learning and data mining. On the one hand, many machine learning methods involve computing linear equation systems, matrix decomposition, matrix determinants, matrix inverses, etc. How to compute them efficiently is challenging in big data scenarios. Matrix low rank approximation is a potentially powerful approach for addressing computational challenge. On the other hand, many machine learning tasks can be modeled as matrix low rank approximation problems such as matrix completion, spectral clustering, and multi-task learning.\nApproximate matrix multiplication is an inverse process of the matrix low rank approximation problem. Recently, many approaches to approximate matrix multiplication [Drineas et al., 2006a, Sarlos, 2006, Cohen and Lewis, 1999, Magen and Zouzias, 2011, Kyrillidis et al., 2014, Kane and Nelson, 2014] have been developed. Meanwhile, they are used to obtain fast solutions for the ℓ2 regression and SVD problems [Drineas et al., 2006b, 2011b, Nelson and Nguyên, 2013, Halko et al., 2011, Clarkson and Woodruff, 2013, Martinsson et al., 2011, Woolfe et al., 2008]. This makes matrix low rank approximation\n77"
    }, {
      "heading" : "78 Matrix Low Rank Approximation",
      "text" : "also become increasingly popular in the theoretical computer science community [Sarlos, 2006, Drineas et al., 2006a].\nIn this chapter we first present some important theoretical results in matrix low rank approximation. We then discuss approximate matrix multiplication. In the following chapter we are concerned with large scale matrix approximation. We will study randomized SVD and CUR approximation. They can be also cast into the matrix low rank approximation framework."
    }, {
      "heading" : "9.1 Basic Results",
      "text" : "Usually, matrix low rank approximation is formulated as a least squares estimation problem based on the Frobenius norm loss. However, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest. He even said “Frobenius-norm error bounds are typically vacuous.” Thus, spectral norm as a loss function is also employed. In this chapter, we present several basic results, some of which hold even for every unitarily invariant norm.\nTheorem 9.1. Let A ∈ Rm×n and C ∈ Rm×c. Then for any X ∈ Rc×n and any unitarily invariant norm ||| · |||,\n|||A−CC†A||| ≤ |||A−CX|||.\nIn other words,\nC†A = argmin X∈Rc×n |||CX−A|||. (9.1)\nAs we have seen, Theorem 9.1 was discussed in Example 8.4, where the problem is solved via the subdifferentials of unitarily invariant norms given in Theorem 8.3. Here, we present an alternative proof.\nProof. Let E1 = A−CC†A, E2 = CC†A−CX, and E = E1 +E2 = A−CX. Since\nET1 E2 = A T (I −CC†)C(C†A−X) = AT0(C†A−X) = 0,\nwe have ETE = ET1 E1+E T 2 E2, and thus λi(E1) ≤ λi(E). It then follows that σi(E1) ≤ σi(E), and thereby σ(E1) ≺w σ(E). It then follows from"
    }, {
      "heading" : "9.1. Basic Results 79",
      "text" : "Theorems 7.4 and 7.5 that\n|||E1||| ≤ |||E|||\nfor any unitarily invariant norm ||| · |||.\nRecall that Problem (9.1) gives an extension to the least squares problem (4.1) in Section 4.1. Theorem 9.1 shows that there is an identical solution w.r.t. all unitarily invariant norm errors. The following theorem shows the solution of a more complicated problem. However, the theorem holds only for the Frobenius norm loss.\nTheorem 9.2. Let A ∈ Rm×n, C ∈ Rm×c, and R ∈ Rr×n. Then for all X ∈ Rc×r, ‖A−CC†AR†R‖F ≤ ‖A−CXR‖F . Equivalently, X⋆ = C†AR† minimizes the following problem:\nmin X∈Rc×n\n‖CXR −A‖2F . (9.2)\nProof. Let E1 = (Im − CC†)A, E2 = CC†A(In − R†R), E3 = CC†AR†R − CXR, and E = E1 + E2 + E3. Then E1 + E2 = A − CC†AR†R and E = A − CXR. Since ET1 E2 = 0, E3ET2 = 0, ET1 E3 = 0, it follows from the matrix Pythagorean theorem that\n‖E‖2F = ‖E1‖2F + ‖E2‖2F + ‖E3‖2F = ‖E1 +E2‖2F + ‖E3‖2F .\nThus, ‖E1 +E2‖2F ≤ ‖E‖2F .\nTheorem 9.3. [Eckart and Young, 1936, Mirsky, 1960] Given an m×n real matrix A of rank r (≤ min{m,n}), let A = UΣVT be the full SVD of A. DefineAk = UkΣkV T k , whereUk and Vk consist of the first k columns of U and V respectively, and Σk is the first k × k principal submatrix of Σ. Then for all m× n real matrices B of rank at most k,\n|||A−Ak||| ≤ |||A−B|||\nholds for all unitarily invariant norm ||| · |||. In other words,\nAk = argmin B∈Rm×n,rank(B)≤k\n|||A−B|||. (9.3)"
    }, {
      "heading" : "80 Matrix Low Rank Approximation",
      "text" : "Theorem 9.3 shows that the rank k truncated SVD produces the best rank k approximation. The theorem was originally proposed by Eckart and Young [1936] under the setting of the Frobenius norm, and generalized to any unitarily invariant norms by Mirsky [1960].\nProof. For any m×n real matrix B of rank at most k, we can write it as B = QC where Q is an m × k column orthonormal matrix and C is some k × n matrix. Thus,\n|||A−B||| = |||A−QC||| ≥ |||A−QQTA||| = |||Q⊥(Q⊥)TA|||,\nwhere Q⊥ (m × (m−k)) is the orthogonal complement of Q. By Proposition 6.3, we have σi(Q\n⊥(Q⊥)TA) = σi((Q⊥)TA) ≥ σk+i for i = 1, . . . , p− k. This implies that\nσ(A−Ak) = (σk+i, σp, 0, . . . , 0)T ≺w σ(Q⊥(Q⊥)TA).\nHence, |||A−B||| ≥ |||A−Ak|||.\nThe above proof procedure also implies that for all m × k column orthonormal matrices Q,\n|||A−UkUTkA||| ≤ |||A−QQTA|||\nholds for every unitarily invariant norm ||| · |||. When k < r, Ak is called a truncated SVD of A and the closest rank-k approximation of A. Note that when the Frobenius norm is used, Ak is the unique minimizer of the problem in (9.3). However, when other unitarily invariant norms are used, the case does not always hold. For example, let us take the spectral norm. Clearly, if\nΣ̃ = diag(σ1 − ωσk+1, σ2 − ωσk+1, . . . , σk − ωσk+1, 0, . . . , 0)\nfor any ω ∈ [0, 1], then UΣ̃VT is also a minimizer of the corresponding problem.\nTheorem 9.4. Given a matrix A ∈ Rm×n and a column orthonormal matrix Q ∈ Rm×p, let Bk be the rank-k truncated SVD of QTA for 1 ≤ k ≤ p. Then Bk is an optimal solution of the following problem:\nmin B∈Rl×n,rank(B)≤k\n‖A−QB‖2F = ‖A−QBk‖2F . (9.4)"
    }, {
      "heading" : "9.1. Basic Results 81",
      "text" : "Proof. Note that (A−QQTA)T (QB−QQTA) = 0, so\n‖A−QB‖2F = ‖A−QQTA‖2F + ‖QB−QQTA|2F = ‖A−QQTA‖2F + ‖B−QTA|2F .\nThe result of the theorem follows from Theorem 9.3.\nTheorem 9.4 is a variant of Theorem 9.3 and of Theorem 9.1. Unfortunately, Bk might not be the solution to the above problem in every unitarily invariant norm, even in the spectral norm error. The reason is that the matrix Pythagorean identity hods only for the Frobenius norm (see Theorem 7.11).\nHowever, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest. He even said “Frobenius-norm error bounds are typically vacuous” [Tropp, 2015]. The following theorem was proposed by Gu [2015], which relates the approximation error in the Frobenius norm to that in the spectral norm.\nTheorem 9.5. [Gu, 2015] Given any matrix A ∈ Rm×n, let p = min{m,n} and B be a matrix with rank at most k such that\n‖A−B‖F ≤ √ √ √ √η2 + p ∑\nj=k+1\nσ2j (A)\nfor some η ≥ 0. Then we must have √ ∑k\nj=1(σj(A)− σj(B))2 ≤ η and\n‖A−B‖2 ≤ √ η2 + σ2k+1(A).\nProof. By Proposition 6.3-(2), we have\nσi+k(A) ≤ σi(A−B) + σk+1(B) = σi(A−B) for i ∈ [p− k]\ndue to rank(B) ≤ k. It then follows that\n‖A−B‖2F = p ∑\ni=1\nσ2i (A−B) ≥ σ21(A−B) + p−k ∑\ni=2\nσ2i (A−B)\n≥ σ21(A−B) + p−k ∑\ni=2\nσ2i+k(A)."
    }, {
      "heading" : "82 Matrix Low Rank Approximation",
      "text" : "We thus obtain\n‖A−B‖22 = σ21(A−B) ≤ η2 + σ2k+1(A).\nAdditionally, it follows from Theorem 6.5 that\nk∑\ni=1\n(σi(A)− σi(B))2 + p ∑\nj=k+1\nσ2j (B) ≤ ‖A−B‖2F ≤ η2 + p ∑\nj=k+1\nσ2j (A),\nwhich leads to the result.\nLet us apply Theorem 9.5 to Theorem 9.4 to establish a spectral\nnorm error bound. It follows from Theorem 9.4 that\n‖A−Ak‖F ≤ ‖A−QBk‖F ≤ ‖A−QQTAk‖F .\nConsider that\n‖A−QQTAk‖2F = ‖A−Ak +Ak −QQTAk‖2F = ‖(Im −QQT )Ak‖2F + ‖A−Ak‖2F\ndue to (A−Ak)ATk (Im −QQT ) = 0. Thus,\n‖A−QBk‖2F ≤ ‖(Im −QQT )Ak‖2F + n∑\ni=k+1\nσ2i (A).\nBy Theorem 9.5, we have that\n‖A−QBk‖22 ≤ ‖(Im −QQT )Ak‖2F + σ2k+1(A),\nwhich can give an error bound in the spectral norm."
    }, {
      "heading" : "9.2 Approximate Matrix Multiplication",
      "text" : "Given matrices A ∈ Rn×d and B ∈ Rn×p, it is well known that the complexity of computing ATB is O(dnp). Approximate matrix multiplication aims to obtain a matrix C ∈ Rd×p with o(dnp) time complexity such that for a small ε > 0,\n‖ATB−C‖ ≤ ε‖A‖‖B‖."
    }, {
      "heading" : "9.2. Approximate Matrix Multiplication 83",
      "text" : "This shows that approximate matrix multiplication can be viewed as an inverse process of the conventional matrix low rank approximation problem.\nApproximate matrix multiplication is a potentially important approach for fast matrix multiplication [Drineas et al., 2006a, Clarkson and Woodruff, 2009, Cohen and Lewis, 1999, Kane and Nelson, 2014, Drineas et al., 2011b, Nelson and Nguyên, 2013, Clarkson and Woodruff, 2013]. It is the foundation of approximate least square methods and matrix low rank approximation methods [Sarlos, 2006, Halko et al., 2011, Kyrillidis et al., 2014, Martinsson et al., 2011, Woolfe et al., 2008, Magdon-Ismail, 2011, Magen and Zouzias, 2011, Cohen and Lewis, 1999, Kane and Nelson, 2014, Drineas et al., 2011b, Nelson and Nguyên, 2013, Clarkson and Woodruff, 2013]. Moreover, it can be also used in large scalable k-means clustering [Cohen et al., 2014], approximate leverage scores [Drineas et al., 2011a], etc.\nMost of work for matrix approximations is based on error bounds w.r.t. the Frobenius norm [Drineas et al., 2006a, Sarlos, 2006, Cohen and Lewis, 1999, Kane and Nelson, 2014, Drineas et al., 2011b, Nelson and Nguyên, 2013, Clarkson and Woodruff, 2013]. In contrast, there is a few work based on spectral-norm error bounds [Halko et al., 2011, Kyrillidis et al., 2014, Martinsson et al., 2011, Woolfe et al., 2008, Magdon-Ismail, 2011, Magen and Zouzias, 2011]. As we have mentioned earlier, spectral-norm error bounds are also of great interest.\nIn approximate matrix multiplication, oblivious subspace embedding matrix is a key ingredient. For example, gaussian matrix and random sign matrix are oblivious matrix. However, leverage score sketching matrix depends on data matrix, hence, it is not an oblivious subspace embedding matrix.\nDefinition 9.1. [Woodruff, 2014b] Given ε > 0 and δ > 0, let Π be a distribution on l× n matrices, where l relies on n, d, ε and δ. Suppose that with probability at lest 1 − δ, for any fixed n × d matrix A, a matrix S drawn from distribution Π is a (1+ε) ℓ2-subspace embedding for A, that is, for all x ∈ Rd, ‖SAx‖22 = (1± ε)‖Ax‖22 with probability 1− δ. Then we call Π an (ε, δ)-oblivious ℓ2-subspace embedding,"
    }, {
      "heading" : "84 Matrix Low Rank Approximation",
      "text" : "Recently, Cohen et al. [2015] proved optimal approximate matrix multiplication in terms of stable rank by using subspace embedding [Batson et al., 2014].\nTheorem 9.6. [Cohen et al., 2015] Given ε, δ ∈ (0, 1/2), let A and B be two conforming matrices, and Π be a (ε, δ) subspace embedding for the 2r̃-dimensional subspace, where r̃ is the maximum of the stable ranks of A and B. Then,\n||(ΠA)T (ΠB) −ATB|| ≤ ε||A||||B||\nholds with at least 1− δ.\nTo analyze approximate matrix multiplication with the Frobenius\nerror, Kane and Nelson [2014] introduced the JL-moment property.\nDefinition 9.2. A distribution D over Rn×d has the (ε, δ, ℓ)-JL moment property if for all x ∈ Rd with ‖x‖2 = 1,\nEΠ∼D ∣ ∣ ∣‖Πx‖22 − 1 ∣ ∣ ∣ ℓ ≤ εℓ · δ\nBased on the JL-moment property, these is an approximate matrix\nmultiplication method with the Frobenius error.\nTheorem 9.7. Given ε, δ ∈ (0, 1/2), let A and B be two conforming matrices, and Π be a matrix satisfying the (ε, δ, ℓ)-JL moment property for some ℓ ≥ 2. Then,\n||(ΠA)T (ΠB) −ATB||F ≤ ε||A||F ||B||F\nholds with at least 1− δ.\nNote that both the subspace embedding property and the JL moment property have close relationships. More specifically, they can be converted into each other [Kane and Nelson, 2014].\nThere are other methods, which do not use subspace embedding matrices, in the literature. Magen and Zouzias [2011] gave a method based on columns selection. Bhojanapalli et al. [2015] proposed a new method with sampling and alternating minimization to directly compute a low-rank approximation to the product of two given matrices."
    }, {
      "heading" : "9.2. Approximate Matrix Multiplication 85",
      "text" : "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].\n10\nLarge-Scale Matrix Approximation\nIn this chapter we discuss fast computational methods of the SVD, kernel methods, and CUR decomposition via randomized approximation. The goal is to make the matrix factorizations fill the use on large scale data matrices.\nIt is notoriously difficult to compute SVD because the exact SVD of an m × n matrix takes O(mnmin{m,n}) time. Fortunately, many machine learning methods such as latent semantic indexing [Deerwester et al., 1990], spectral clustering [Shi and Malik, 2000], manifold learning [Tenenbaum et al., 2000, Belkin and Niyogi, 2003] are interested in only the top singular value triples. The Krylov subspace method computes the top k singular value triples in Õ(mnk) time [Saad, 2011, Musco and Musco, 2015], where the Õ notation hides the logarithm factors and the data dependent condition number. If a low precision solution suffices, the time complexity can be even lower. Here we will make main attention on randomized approximate algorithms that demonstrate high scalability. Randomized algorithms are a feasible approach for large scale machine learning models [Rokhlin et al., 2009, Mahoney, 2011, Tu et al., 2014]. In particular, we will consider randomized SVD methods [Halko et al., 2011].\n86"
    }, {
      "heading" : "10.1. Randomized SVD 87",
      "text" : "In contrast to the randomized SVD which is based on random projection, the CUR approximation mainly employs column selection. Column selection has been extensively studied in the theoretical computer science (TCS) and numerical linear algebra (NLA) communities. The work in TCS mainly focuses on choosing good columns by randomized algorithms with provable error bounds [Frieze et al., 2004, Deshpande et al., 2006, Drineas et al., 2008, Deshpande and Rademacher, 2010, Boutsidis et al., 2014, Guruswami and Sinop, 2012]. The focus in NLA is then on deterministic algorithms, especially the rank-revealing QR factorizations, that select columns by pivoting rules [Foster, 1986, Chan, 1987, Stewart, 1999, Bischof and Hansen, 1991, Hong and Pan, 1992, Chandrasekaran and Ipsen, 1994, Gu and Eisenstat, 1996, Berry et al., 2005]."
    }, {
      "heading" : "10.1 Randomized SVD",
      "text" : "All the randomized SVD algorithms essentially have the same idea: first draw a random projection matrix Ω ∈ Rn×c, then form the sketch C = AΩ ∈ Rm×c and compute its orthonormal bases Q ∈ Rm×c, and finally compute a rank k matrix X ∈ Rc×n such that ‖A − QX‖2ξ is small compared to ‖A−Ak‖2ξ . Here ‖ · ‖ξ denotes either the Frobenius norm or the spectral norm.\nThe following lemma is the foundation in theoretical analysis of the\nrandomized SVD [Halko et al., 2011, Gu, 2015]. Lemma 10.1. Let A ∈ Rm×n be a given matrix, and Z ∈ Rn×k be column orthonormal. Let Ω ∈ Rn×c be any matrix such that rank(ZTΩ) = rank(Z) = k, and define C = AΩ ∈ Rm×c . Then\n‖A−ΠξC,k(A)‖2ξ ≤ ‖E‖2ξ + ‖EΩ(ZTΩ)†‖2ξ ,\nwhere E = A−AZZT , and ΠξC,k(A) ∈ Rm×n denotes the best approximation to A within the column space of C that has rank at most k w.r.t. the norm ‖ · ‖ξ loss. Proof. In terms of definition of ΠξC,k(A), we have\n‖A−ΠξC,k(A)‖2ξ ≤ ‖A−X‖2ξ"
    }, {
      "heading" : "88 Large-Scale Matrix Approximation",
      "text" : "for all matrices X ∈ Rm×n of rank at most k in the column space of C. Obviously, C(ZTΩ)†ZT is such a matrix. Thus,\n‖A−ΠξC,k(A)‖2ξ ≤ ‖A−C(ZTΩ)†ZT ‖2ξ = ‖A−AZZT +AZZT −C(ZTΩ)†ZT ‖2ξ = ‖E + (AZZT −A)Ω(ZTΩ)†ZT ‖2ξ = ‖E +EΩ(ZTΩ)†ZT ‖2ξ .\nHere we use the fact that ZTΩ(ZTΩ)† = Ik because rank(ZTΩ) = k. Consider that\nEΩ(ZTΩ)†ZTET = EΩ(ZTΩ)†ZT (AT − ZZTAT ) = 0.\nThe theorem follows from Theorem 7.11.\nConsider the rank-k truncated SVD Ak = UkΣkV T k . Then we can\nwrite A as\nA = AVkV T k + (A−Ak).\nLet Z = Vk and E = A − Ak in Lemma 10.1. Then the following theorem is an immediate corollary of Lemma 10.1.\nTheorem 10.2. Let A = UΣVT be the full SVD of A ∈ Rm×n, fix k ≥ 0, and let Ak = UkΣkV T k be the best at most rank k approximation of A. Choose a test matrix Ω and construct the sketch C = AΩ.\nPartition Σ =\n[\nΣk 0\n0 Σ−k\n]\nand V = [Vk,V−k]. Define Ω1 = VTkΩ and\nΩ2 = V T −kΩ. Assume that Ω1 has full row rank. Then\n‖(Im −CC†)A‖2ξ ≤ ‖A−ΠξC,k(A)‖2ξ ≤ ‖Σ−k‖2ξ + ‖Σ−kΩ2Ω † 1‖2ξ .\nIn Lemma 10.1 and Theorem 10.2, the condition rank(VTkΩ) = rank(Vk) = k is essential for an effective randomized SVD algorithm. An idealized case for meeting this condition is that range(Vk) ⊂ range(Ω). In this case, the randomized SVD degenerates an exact truncated SVD procedure. Thus, the above condition aims to relax this idealized case. Moreover, the key for an effective randomized SVD is to select a test matrix Ω such that the condition rank(VTkΩ) = rank(Vk) = k holds as much as possible. Lemma 10.1 and Theorem 10.2 are also fundamental in random column selection [Boutsidis et al., 2014]."
    }, {
      "heading" : "10.1. Randomized SVD 89",
      "text" : ""
    }, {
      "heading" : "10.1.1 Randomized SVD: Frobenius Norm Bounds",
      "text" : "In this subsection, we describe two randomized SVD algorithms which have (1 + ǫ) relative-error bound.\nRandom Projection. In order to reduce computational expenses, randomized algorithms [Frieze et al., 2004, Vempala, 2000] have been introduced to truncated SVD and low-rank approximation. The Johnson & Lindenstrauss (JL) transform [Johnson and Lindenstrauss, 1984, Dasgupta and Gupta, 2003] is known to keep isometry in expectation or with high probability. Halko et al. [2011], Boutsidis et al. [2014] used the JL transform for sketching and showed relative-error bounds. However, the Gaussian test matrix is dense and cannot efficiently apply to matrices. Several improvements have been proposed to make the sketching matrix sparser; see the review [Woodruff, 2014b] for the complete list of the literature. In particular, the count sketch [Clarkson and Woodruff, 2013] applies to A in only O(nnz(A)) time and exhibits very similar properties as the JL transform. Specifically, Woodruff [2014b] showed that an m×O(k/ǫ) sketch C = AΩ can be obtained in O(nnz(A)) time and\nmin rank(X)≤k\n∥ ∥A−QX ∥ ∥ 2\nF ≤ (1 + ǫ) ‖A−Ak‖2F (10.1)\nholds with high probability.\nThe Prototype Algorithm. Halko et al. [2011] proposed to directly solve the left-hand side of (10.1), which has closed-form solution X⋆ = (QTA)k. This leads to the prototype algorithm shown in Algorithm 1. The optimality of X⋆ is given in Theorem 9.4.\nThe prototype algorithm is not time efficient because the matrix product QTA costs O(mnc) time, which is not lower than the exact solutions. Nevertheless, the prototype algorithm is still useful in largescale applications because it is pass-efficient—it goes only two passes through A.\nFaster Randomized SVD. The bottleneck of the prototype algorithm is the matrix product in computing X⋆. Notice that (9.4) is a strongly over-determined system, so it can be approximately solved by once more random projection. Let P = P1P2 ∈ Rm×p be another random projection matrix, where P1 is a count sketch and P2 is a JL"
    }, {
      "heading" : "90 Large-Scale Matrix Approximation",
      "text" : "Algorithm 1 Randomized SVD: The Prototype Algorithm.\n1: Input: a matrix A ∈ Rm×n with m ≥ n, target rank k, the size of sketch c where 0 < k ≤ c < n; 2: Draw a sketching matrix Ω ∈ Rn×c, e.g. a Gaussian test matrix or a count sketch 3: Compute C = AΩ ∈ Rm×c and its orthonormal bases Q ∈ Rm×c; 4: Compute the rank k truncated SVD: QTA ≈ ŪkΣ̃kṼTk ; 5: return Ũk = QŪk, Σ̃k, Ṽk—an approximate rank-k truncated\nSVD of A.\ntransform matrix. Then we solve\nX̃ = min rank(X)≤k\n‖PT (A−QX)‖2F\ninstead of (9.4), and X̃ has closed-form solution\nX̃ = R̃†(Q̃TPTA)k,\nwhere Q̃R̃ be the economy size QR decomposition of (PTQ) ∈ Rp×c. Finally, the rank k matrix QX̃ is the obtained approximation to A, and its SVD can be very efficiently computed. Clarkson and Woodruff [2013], Woodruff [2014b] showed that\n∥ ∥A−QR̃†(Q̃TPTA)k ∥ ∥ 2 F ≤ (1 + ǫ) ‖A−Ak‖2F\nfor a large enough p, and the overall time cost is O(nnz(A) + (m + n)poly(k/ǫ))."
    }, {
      "heading" : "10.1.2 Randomized SVD: Spectral Norm Bounds",
      "text" : "The previous section shows that the approximate truncated SVD can be computed highly efficiently, with the (1+ǫ) Frobenius relative-error guaranteed. The Frobenius norm bound tells that the total elementwise distance is small, but it does not inform us the closeness of their singular vectors. Therefore, we need spectral norm bounds or even stronger principal angle bounds; here we only consider the former. We seek to find an m× k column orthogonal matrix Ũ such that\n∥ ∥A− ŨŨTA ∥ ∥ 2\n2 ≤ η‖A−Ak‖22,"
    }, {
      "heading" : "10.1. Randomized SVD 91",
      "text" : "where η will be specified later. The Prototype Algorithm. Unlike the Frobenius norm bound, the prototype algorithm is unlikely to attain a constant factor bound (i.e., η is independent of m, n), letting alone the 1 + ǫ bound. It is because the lower bounds [Witten and Candès, 2013, Boutsidis et al., 2014] showed that if Ω ∈ Rn×c in Algorithm 1 is the Gaussian test matrix or any column selection matrix, the order of η must be at least n/c. We apply Gu’s theorem [Gu, 2015] (Theorem 9.5) to obtain an O(n)factor spectral norm bound, and then introduce iterative algorithms with the (1+ǫ) spectral norm bound.\nLet Ũk, Σ̃k, and Ṽk be the outputs of Algorithm 1. We have that ∥ ∥A− ŨkŨTkA ∥ ∥ 2 F ≤ ∥ ∥A− ŨkΣ̃kṼTk ∥ ∥ 2 F\n= ∥ ∥A−QX⋆ ∥ ∥ 2\nF ≤ (1 + ǫ) ‖A−Ak‖2F ,\nwhere the first inequality follows from Theorem 9.1, the equality follows from the definitions, and the second inequality follows from (10.1) provided that c = O(k/ǫ) and Ω is the Gaussian test matrix or the count sketch. We let ǫ = 1 and c = O(k) and apply Theorem 9.5 to obtain\n∥ ∥A− ŨkŨTkA ∥ ∥ 2\n2 ≤ ‖A−Ak‖22 + ‖A−Ak‖2F ≤ (n − k + 1)‖A−Ak‖22. (10.2)\nHere the second inequality follows from that ‖A−Ak‖2F = ∑n i=k+1 σ 2 i ≤ (n − k)σ2k+1 = (n − k)‖A − Ak‖22. To this end, we have shown that the prototype algorithm 1 satisfies O(n)-factor spectral norm bound. However, the result itself has little meaning.\nThe Simultaneous Power Iteration can be used to refine the sketch [Halko et al., 2011, Gu, 2015]. The algorithm is described in Algorithm 2 and analyzed in the following. LetΩ ∈ Rn×c be a Gaussian test matrix or count sketch andB = (AAT )tA. Let us takeB instead of A as the input of the prototype algorithm 1 and obtain the approximate left singular vectors Ũk. It is easy to verify that Ũk is the same to the output of Algorithm 2. We will show that when t = O( lognǫ ),\n∥ ∥A− ŨkŨTkA ∥ ∥ 2 2 ≤ (1 + ǫ)‖A−Ak‖22. (10.3)\nTo show this result, we need the lemma of Halko et al. [2011]."
    }, {
      "heading" : "92 Large-Scale Matrix Approximation",
      "text" : "Algorithm 2 Subspace Iteration Methods.\n1: Input: any matrix A ∈ Rm×n, the target rank k, the size of sketch c where 0 < k ≤ c < n; 2: Generate an n × c Gaussian test matrix Ω and perform sketching C(0) = AΩ; 3: for i = 1 to t do 4: Optional: orthogonalize C(i−1); 5: Compute C(i) = AATC(i−1); 6: end for 7: The Power Method: orthonalize C(t) to obtain Q ∈ Rm×c; 8: The Krylov Subspace Method: orthonalize K =\n[C(0), · · · ,C(t)] to obtain Q ∈ Rm×(t+1)c; 9: Compute the rank k truncated SVD: QTA ≈ ŪkΣ̃kṼTk ;\n10: return Ũk = QŪk, Σ̃k, Ṽk—an approximate rank-k truncated SVD of A.\nLemma 10.3 (Halko, Martinsson, & Tropp). Let A be any matrix and U have orthonormal columns. Then for any positive integer t,\n∥ ∥(I−UUT )A ∥ ∥ 2 ≤ ∥ ∥(I−UUT )(AAT )tA ∥ ∥ 1/(2t+1) 2 .\nBy Lemma 10.3, we have that\n∥ ∥(I− ŨkŨTk )A ∥ ∥ 2 2 ≤ ∥ ∥(I − ŨkŨTk )B ∥ ∥ 2/(2t+1) 2\n≤ (n− k + 1)1/(2t+1)σ2/(2t+1)k+1 (B) = (1 + ǫ)σ2k+1(A).\nHere the second inequality follows from (10.2) and the definitions of B and Ũk, and we show the equality in the following. Let 2t + 1 = log(n−k+1)\n0.5ǫ . We have that 1 2t+1 log(n− k+1) = 0.5ǫ ≤ log(1+ ǫ), where the inequality holds for all for all ǫ ∈ [0, 1]. Taking the exponential of both sides, we have (n − k + 1)1/(2t+1) ≤ 1 + ǫ. Finally, (10.3) follows from that σ2k+1(A) = ‖A−Ak‖22.\nThe Krylov Subspace Method. From Algorithm 2 we can see that the power iteration repeats t times, but only the output of the last iteration C(t) is used. In fact, the intermediate resultsC(0), · · · ,C(t) are"
    }, {
      "heading" : "10.2. Kernel Approximation 93",
      "text" : "also useful. The matrix K = [C(0), · · · ,C(t)] ∈ Rm×(t+1)c is well known as the Krylov matrix, and range(K) is called the Krylov subspace. We show the Krylov subspace method in Algorithm 2, which differs from simultaneous power iteration in only one line. It turns out that the Krylov subspace method converges much faster than the power iteration [Saad, 2011]. Very recently, Musco and Musco [2015] showed that with t = logn√\nǫ power iteration, the 1+ǫ spectral norm bound (10.3)\nholds with high probability. This result is evidently stronger than the simultaneous power iteration.\nIt is worth mentioning that the Krylov subspace method described in Algorithm 2 is a simplified version, and it may be instable when t is large. This is because the columns of C(0), · · · ,C(t) tend to be linearly dependent as t grows. In practice, re-orthogonalization or partial re-orthogonalization are employed to prevent the instability from happening [Saad, 2011]."
    }, {
      "heading" : "10.2 Kernel Approximation",
      "text" : "Kernel methods are important tools in machine learning, computer vision, and data mining [Schölkopf and Smola, 2002, Shawe-Taylor and Cristianini, 2004, Vapnik, 1998, Rasmussen and Williams, 2006]. For example, kernel ridge regression (KRR), Gaussian processes, kernel support vector machine (KSVM), spectral clustering, and kernel principal component analysis (KPCA) are classical nonlinear models for regression, classification, clustering, and dimensionality regression. Unfortunately, the lack of scalability has always been the major drawback of kernel methods. The three steps of most kernel methods—forming the kernel matrix, training, generalization—can all be prohibitive in big-data applications.\nSpecifically, suppose we are given n training data and m test data, all of d dimension. Firstly, it takes O(n2d) time to form an n × n kernel matrix K, e.g., the Gaussian RBF kernel matrix. Secondly, the training requires either SVD or matrix inversion of the kernel matrix. For example, spectral clustering, KPCA, Isomap [Tenenbaum et al., 2000], and Laplacian eigenmaps [Belkin and Niyogi, 2003] compute the"
    }, {
      "heading" : "94 Large-Scale Matrix Approximation",
      "text" : "top k singular vectors of the (normalized) kernel matrix, where k is the number of classes or the target dimensionality. This costs O(n2k) time and O(n3) memory. Thirdly, to generalize the trained model to the test data, kernel methods such as KRR, KSVM, KPCA cost O(nmd) time to form an n × m cross kernel matrix between the training and test data. If m is as large as n, generalization is as challenging as training.\nLow rank approximation is the most popular approach to scalable kernel approximation. If we have the low rank approximation K ≈ CXCT , then the approximate eigenvalue decomposition can be immediately obtained by\nK ≈ CXCT = UC (ΣCVTCXVCΣC) ︸ ︷︷ ︸\n=Z\nUTC = (UCUZ)ΛZ(UCUZ) T .\nHere C = UCΣCVTC is the SVD and Z = UZΛZU T Z is the spectral decomposition. Since the tall-and-skinny matrix UCUZ has orthonormal columns and the diagonal entries of ΛZ are in the descending order, the leftmost columns of UCUZ are approximately the top singular vectors of K. This approach only costs O(nc2) time, where c is the number of columns of C. Our objective is thereby to find such a low rank approximation.\nDifference from Randomized SVD. Why cannot we directly use the randomized SVD to approximate the kernel matrix? The randomized SVD assumes that the matrix is fully observed; unfortunately, this is not true for kernel methods. When the number of data samples is million scale, even forming the kernel matrix is impossible. Therefore, the primary objective of kernel approximation is to avoid forming the whole kernel matrix. The existing random projection methods all require the full observation of the matrix, so random projection is not a feasible option. We must use column selection in the kernel approximation problem.\nThe Prototype Algorithm. Let S be an n × c sketching matrix and let C = KS. It remains to find the c × c intersection matrix X. The most intuitive approach is to minimize the approximation error by\nX⋆ = argmin X\n∥ ∥K−CXCT ∥ ∥ 2 F = C†K(C†)T , (10.4)\nwhere the second equality follows from Theorem 9.2. This method was"
    }, {
      "heading" : "10.2. Kernel Approximation 95",
      "text" : "proposed by Halko et al. [2011] for approximating symmetric matrix. Wang et al. [2014a] showed that by randomly samplingO(k/ǫ) columns of K to form C by a certain algorithm, the approximation is high accurate:\n∥ ∥K−CX⋆CT ∥ ∥ 2\nF ≤ (1 + ǫ)\n∥ ∥K−Kk ∥ ∥ 2\nF .\nThis upper bound matches the lower bound c ≥ 2k/ǫ up to a constant factor [Wang et al., 2014a]. Unfortunately, the prototype algorithm has two obvious drawbacks. Firstly, to compute the intersection matrix X⋆, every entry of K must be known. As is discussed, it takes O(n2d) time to form the kernel matrix K. Secondly, the matrix multiplication C†K costs O(n2c) time. In sum, the prototype algorithm costs O(n2c + n2d) time. Although it is substantially faster than the exact solution, the prototype algorithm has the same time complexity as the exact solution.\nFaster SPSD Matrix Sketching. Since C = KS has much more rows than columns, the optimization problem (10.4) is strongly overdetermined. Wang et al. [2015b] proposed to use sketching to approximately solve (10.4). Specifically, let P be a certain n×p column selection matrix with p ≥ c and compute\nX̃ = argmin X\n∥ ∥PT (K−CXCT )P ∥ ∥ 2 F = (PTC)†(PTKP)(CTP)†.\nIn this way, we need only nc+p2 entries of K to form the approximation K ≈ CX̃CT . The intersection matrix X̃ can be computed in O(ncd+ p2d+ p2c) time, given S and n data points of d dimension. Wang et al. [2015b] devised an algorithm that sets p = √ nc/ √ ǫ and very efficiently forms the column selection matrix P; and the following error bound holds with high probability:\n∥ ∥K−CX̃CT ∥ ∥ 2\nF ≤ (1 + ǫ) min\nX\n∥ ∥K−CXCT ∥ ∥ 2\nF .\nBy this choice of p, the overall time cost is linear in n. Motivated by the matrix ridge approximation of Zhang [2014], Wang et al. [2014b] proposed a spectral shifting kernel approximation method. When the spectrum of K decays slowly, the shifting term helps to improve the approximation accuracy and numerical stability. Wang et al. [2014a] also showed that the spectral shifting approach"
    }, {
      "heading" : "96 Large-Scale Matrix Approximation",
      "text" : "can be used to improve other kernel approximation models such as the memory efficient kernel approximation (MEKA) model [Si et al., 2014].\nThe Nyström Method is the most popular kernel approximation approach. It is named after its inventor Nyström [1930] and gained its popularity in the machine learning society after its application in Gaussian procession regression [Williams and Seeger, 2001]. Let S be a column selection matrix,C = KS, andW = STKS. The Nyström method approximates K by CW†CT . In fact, the Nyström method is a special case of the faster SPSD matrix sketching where P and S are equal. This also indicates that the Nyström method is an approximate solution to (10.4). Gittens and Mahoney [2013] offered comprehensive error analysis of the Nyström method. The Nyström method has been applied to solve million scale kernel methods [Talwalkar et al., 2013]. But unlike the faster SPSD matrix sketching, the Nyström method cannot generate high quality approximation. The lower bound [Wang and Zhang, 2013] indicates that the Nyström method cannot attain (1+ǫ) relativeerror bound unless it is willing to spend Ω(n2k/ǫ) time.\nTo this end, we have shown how to efficiently approximate any kernel matrix and use the obtained low rank approximation to speed up training. We will introduce efficient generalization using the CUR matrix decomposition in the next section."
    }, {
      "heading" : "10.3 The CUR Approximation",
      "text" : "Let A by any m×n matrix. The CUR matrix decomposition is formed by selecting c columns of A to form C ∈ Rm×c, r rows to form R ∈ Rr×n, and computing an intersection matrix U ∈ Rc×r such that CUR ≈ A. In this section, we first discussion the motivations and then describe algorithms and error analyses.\nMotivations. Firstly, let us continue the generalization problem of kernel methods which remains unsolved in the previous section. Suppose we are given n training data andm test data, all of d dimension. To generalize the trained model to the test data, supervised kernel methods such as Gaussian processes and KRR require evaluating the kernel function of every train and test data pair—that is to form an m × n"
    }, {
      "heading" : "10.3. The CUR Approximation 97",
      "text" : "cross kernel matrixK∗—which costs O(mnd) time. By the fast CUR algorithm described later in this section, the approximation K∗ ≈ CUR can be obtained in time linear in d(m+n). With such a decomposition at hand, the matrix product K∗M ≈ CURM can be computed in O(nrk+mck) time. In this way, the overall time cost of generalization is linear in m+ n.\nSecondly, CUR forms a compressed representation of the data matrix, as well as the truncated SVD, and it can be very efficiently converted to the SVD-like form:\nA ≈ CUR = UC ΣCVTCUURΣR ︸ ︷︷ ︸\n=B\nVTR = (UCUB)ΣB(VRVB) T .\nHere C = UCΣCVTC , R = URΣRV T R, B = UBΣBVR are the SVD. Since CUR is formed by sampling columns and rows, it preserves the sparsity and nonnegativity of the original data matrix. The sparsity makes CUR cheaper to store than SVD, and the nonnegativity makes CUR a nonnegative matrix factorization.\nThirdly, CUR consists of the actual columns and rows, and thus it enables human to to understand and interpret the data. In comparison, the basis vectors of SVD has little concrete meaning. An example of Drineas et al. [2008] and Mahoney and Drineas [2009] has well shown this viewpoint; that is, the vector [(1/2)age − (1/ √ 2)height + (1/2)income], the sum of the significant uncorrelated features from a data set of people’s features, is not particularly informative. Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix.\nColumn Selection. Several different column selection strategies have been devised, among which the leverage score sampling [Drineas et al., 2008] and the adaptive sampling [Wang and Zhang, 2013, Boutsidis and Woodruff, 2014] attain relative error bounds. In particular, Boutsidis and Woodruff [2014] showed that with c = O(k/ǫ) columns and r = O(k/ǫ) rows selected by adaptive sampling to form C and R,\nmin X ‖A−CXR‖2F ≤ (1 + ǫ)‖A−Ak‖2F holds in expectation. A further refinement was developed by Woodruff"
    }, {
      "heading" : "98 Large-Scale Matrix Approximation",
      "text" : "[2014b]. We will not go to the details of the leverage score sampling or adaptive sampling. The users only need to know that such algorithms randomly sample columns/rows according to some non-uniform distributions. Unfortunately, it requires observing the whole matrix A to compute such non-uniform distributions, thus such column selection algorithms cannot be applied to speed up computation. It remains an open problem whether there is a relative-error sampling algorithm that needs not observing the whole of A. In practice, the users can simply sample columns/rows uniformly without replacement, which usually has acceptable empirical performance.\nThe Intersection Matrix. With the selected columns C and rows R at hand, we can simply compute the intersection matrix by\nU⋆ = argmin U\n∥ ∥A−CUR ∥ ∥ 2 F = C†AR†. (10.5)\nHere the second equality follows from Theorem 9.2. This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014]. This approach is very similar to the prototype SPSD matrix approximation method in the previous section, and it costs at least O(mn·min{c, r}) time and requires observing every entry of A. Apparently, it cannot help speed up matrix computation.\nWang et al. [2015a] proposed a more practical CUR decomposition method which solves (10.5) approximately. The method first draws two column selection matrices PC ∈ Rm×pc and PR ∈ Rn×pr (pc, pr ≥ c, r), which costs O(mc2+nr2) time. It then computes the intersection matrix by\nŨ = argmin U\n∥ ∥PTC(A−CUR)PR ∥ ∥ 2 F = (PTCC) †(PTCAPR)(RPR) †.\nThis method needs observing only pc× pr entries of A, and the overall time cost is O(pcpr ·min{c, r}+mc2 + nr2). When\npc ≥ O ( c √ min{m,n}/ǫ ) and pr ≥ O ( r √ min{m,n}/ǫ ) ,\nthe following inequality holds with high probability: ∥ ∥A−CŨR ∥ ∥ 2\nF ≤ (1 + ǫ) min\nU\n∥ ∥A−CUR ∥ ∥ 2\nF .\nIn sum, a high quality CUR decomposition can be computed in time linear in min{m,n}."
    }, {
      "heading" : "Acknowledgements",
      "text" : "I would like to thank my graduate students Cheng Chen, Luo Luo, Shusen Wang, Haishan Ye, and Qiaomin Ye. Specifically, Cheng Chen, Luo Luo and Qiaomin Ye helped to proofread the whole manuscript. Haishan Ye helped to revise Chapter 9.2, and Shusen Wang helped to revise Chapter 10. I would also like to thank other students who took my course “ Matrix Methods in Massive Data Analysis” in the summer term 2015. They helped to improve the lecture notes, which provide the main materials for this tutorial.\n99"
    } ],
    "references" : [ {
      "title" : "Nyström approximation for large-scale determinantal processes",
      "author" : [ "Raja Hafiz Affandi", "Alex Kulesza", "Emily B. Fox", "Ben Taskar" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Affandi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Affandi et al\\.",
      "year" : 2013
    }, {
      "title" : "A kernel method for canonical correlation analysis",
      "author" : [ "S. Akaho" ],
      "venue" : "In International Meeting of Psychometric Society,",
      "citeRegEx" : "Akaho.,? \\Q2001\\E",
      "shortCiteRegEx" : "Akaho.",
      "year" : 2001
    }, {
      "title" : "Spectral analysis of data",
      "author" : [ "Yossi Azar", "Amos Fiat", "Anna Karlin", "Frank McSherry", "Jared Saia" ],
      "venue" : "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Azar et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Azar et al\\.",
      "year" : 2001
    }, {
      "title" : "Kernel independent component analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bach and Jordan.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bach and Jordan.",
      "year" : 2002
    }, {
      "title" : "Srivastave. Twice-Ramanujan sparsifiers",
      "author" : [ "J. Batson", "D. Spielman" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Batson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Batson et al\\.",
      "year" : 2014
    }, {
      "title" : "Generalized discriminant analysis using a kernel approach",
      "author" : [ "G. Baudat", "F. Anouar" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Baudat and Anouar.,? \\Q2000\\E",
      "shortCiteRegEx" : "Baudat and Anouar.",
      "year" : 2000
    }, {
      "title" : "Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection",
      "author" : [ "P. Belhumeur", "J. Hespanha", "D. Kriegman" ],
      "venue" : "IEEE Trans. PAMI,",
      "citeRegEx" : "Belhumeur et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Belhumeur et al\\.",
      "year" : 1997
    }, {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Belkin and Niyogi.,? \\Q2003\\E",
      "shortCiteRegEx" : "Belkin and Niyogi.",
      "year" : 2003
    }, {
      "title" : "Generalized Inverses: Theory and Applications",
      "author" : [ "A. Ben-Israel", "T.N.E. Greville" ],
      "venue" : "Second Edition. Springer,",
      "citeRegEx" : "Ben.Israel and Greville.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ben.Israel and Greville.",
      "year" : 2003
    }, {
      "title" : "Algorithm 844: computing sparse reduced-rank approximations to sparse matrices",
      "author" : [ "M.W. Berry", "S.A. Pulatova", "G.W. Stewart" ],
      "venue" : "ACM Transactions on Mathematical Software,",
      "citeRegEx" : "Berry et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Berry et al\\.",
      "year" : 2005
    }, {
      "title" : "Tighter low-rank approximation via sampling the leveraged element",
      "author" : [ "Srinadh Bhojanapalli", "Prateek Jain", "Sujay Sanghavi" ],
      "venue" : "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Bhojanapalli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bhojanapalli et al\\.",
      "year" : 2015
    }, {
      "title" : "CUR from a sparse optimization viewpoint",
      "author" : [ "J. Bien", "Y. Xu", "M.W. Mahoney" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Bien et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bien et al\\.",
      "year" : 2010
    }, {
      "title" : "Structure-preserving and rank-revealing QRfactorizations",
      "author" : [ "C.H. Bischof", "P.C. Hansen" ],
      "venue" : "SIAM Journal on Scientific and Statistical Computing,",
      "citeRegEx" : "Bischof and Hansen.,? \\Q1991\\E",
      "shortCiteRegEx" : "Bischof and Hansen.",
      "year" : 1991
    }, {
      "title" : "Convex Analysis and Nonlinear Optimization: Theory and Examples",
      "author" : [ "Jonathan M. Borwein", "Adrian S. Lewis" ],
      "venue" : null,
      "citeRegEx" : "Borwein and Lewis.,? \\Q2006\\E",
      "shortCiteRegEx" : "Borwein and Lewis.",
      "year" : 2006
    }, {
      "title" : "Optimal CUR matrix decompositions",
      "author" : [ "Christos Boutsidis", "David P. Woodruff" ],
      "venue" : "STOC, pages 353–362,",
      "citeRegEx" : "Boutsidis and Woodruff.,? \\Q2014\\E",
      "shortCiteRegEx" : "Boutsidis and Woodruff.",
      "year" : 2014
    }, {
      "title" : "Near-optimal column-based matrix reconstruction",
      "author" : [ "Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2014
    }, {
      "title" : "Dimension reduction: A guided tour",
      "author" : [ "Christopher J.C. Burges" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Burges.,? \\Q2010\\E",
      "shortCiteRegEx" : "Burges.",
      "year" : 2010
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "Jian-Feng Cai", "Emmanuel J Candès", "Zuowei Shen" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Cai et al\\.,? \\Q1956\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 1956
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "B. Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "The dantzig selector: Statistical estimation when p is much larger than n",
      "author" : [ "Emmanuel J Candès", "Terence Tao" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Candès and Tao.,? \\Q2007\\E",
      "shortCiteRegEx" : "Candès and Tao.",
      "year" : 2007
    }, {
      "title" : "Rank revealing QR factorizations",
      "author" : [ "T.F. Chan" ],
      "venue" : "Linear Algebra and Its Applications,",
      "citeRegEx" : "Chan.,? \\Q1987\\E",
      "shortCiteRegEx" : "Chan.",
      "year" : 1987
    }, {
      "title" : "On rank-revealing factorisations",
      "author" : [ "S. Chandrasekaran", "I.C.F. Ipsen" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Chandrasekaran and Ipsen.,? \\Q1994\\E",
      "shortCiteRegEx" : "Chandrasekaran and Ipsen.",
      "year" : 1994
    }, {
      "title" : "Numerical linear algebra in the streaming model",
      "author" : [ "Kenneth L Clarkson", "David P Woodruff" ],
      "venue" : "In Proceedings of the forty-first annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2009\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2009
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "Kenneth L Clarkson", "David P Woodruff" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2013
    }, {
      "title" : "Approximating matrix multiplication for pattern recognition tasks",
      "author" : [ "Edith Cohen", "David D Lewis" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "Cohen and Lewis.,? \\Q1999\\E",
      "shortCiteRegEx" : "Cohen and Lewis.",
      "year" : 1999
    }, {
      "title" : "Dimensionality reduction for k-means clustering and low rank approximation",
      "author" : [ "Michael Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu" ],
      "venue" : "arXiv preprint arXiv:1410.6801,",
      "citeRegEx" : "Cohen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal approximate matrix product in terms of stable rank",
      "author" : [ "Michael B. Cohen", "Jelani Nelson", "David P. Woodruff" ],
      "venue" : "arXiv preprint arXiv:1507.02268,",
      "citeRegEx" : "Cohen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2015
    }, {
      "title" : "An elementary proof of a theorem of Johnson and Lindenstrauss",
      "author" : [ "S. Dasgupta", "A. Gupta" ],
      "venue" : "Random Structure & Algorithms,",
      "citeRegEx" : "Dasgupta and Gupta.,? \\Q2003\\E",
      "shortCiteRegEx" : "Dasgupta and Gupta.",
      "year" : 2003
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman" ],
      "venue" : "Journal of The American Society for Information Science,",
      "citeRegEx" : "Deerwester et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Applied Numerical Linear Algebra",
      "author" : [ "J. Demmel" ],
      "venue" : "SIAM, Philadelphia,",
      "citeRegEx" : "Demmel.,? \\Q1997\\E",
      "shortCiteRegEx" : "Demmel.",
      "year" : 1997
    }, {
      "title" : "Efficient volume sampling for row/column subset selection",
      "author" : [ "A. Deshpande", "L. Rademacher" ],
      "venue" : "In Proceedings of the 51st IEEE Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Deshpande and Rademacher.,? \\Q2010\\E",
      "shortCiteRegEx" : "Deshpande and Rademacher.",
      "year" : 2010
    }, {
      "title" : "Matrix approximation and projective clustering via volume sampling",
      "author" : [ "A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "Deshpande et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Deshpande et al\\.",
      "year" : 2006
    }, {
      "title" : "On the Nyström method for approximating a gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M.W. Mahoney" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Drineas and Mahoney.,? \\Q2005\\E",
      "shortCiteRegEx" : "Drineas and Mahoney.",
      "year" : 2005
    }, {
      "title" : "Relative-error CUR matrix decompositions",
      "author" : [ "P. Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2008
    }, {
      "title" : "Fast monte carlo algorithms for matrices I: Approximating matrix multiplication",
      "author" : [ "Petros Drineas", "Ravi Kannan", "Michael W Mahoney" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "Sampling algorithms for l2 regression and applications",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "Fast approximation of matrix coherence and statistical leverage",
      "author" : [ "Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2011
    }, {
      "title" : "Faster least squares approximation",
      "author" : [ "Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tamás Sarlós" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2011
    }, {
      "title" : "The approximation of one matrix by another of lower rank",
      "author" : [ "C. Eckart", "G. Young" ],
      "venue" : null,
      "citeRegEx" : "Eckart and Young.,? \\Q1936\\E",
      "shortCiteRegEx" : "Eckart and Young.",
      "year" : 1936
    }, {
      "title" : "A principal axis transformation for non-Hermitian matrices",
      "author" : [ "C. Eckart", "G. Young" ],
      "venue" : "Bulletin of the American Mathematical Society,",
      "citeRegEx" : "Eckart and Young.,? \\Q1939\\E",
      "shortCiteRegEx" : "Eckart and Young.",
      "year" : 1939
    }, {
      "title" : "Maximum properties and inequalities for the eigenvalues of completely continuous operators",
      "author" : [ "Ky Fan" ],
      "venue" : "Proc. Nat. Acad. Sci. USA,",
      "citeRegEx" : "Fan.,? \\Q1951\\E",
      "shortCiteRegEx" : "Fan.",
      "year" : 1951
    }, {
      "title" : "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering",
      "author" : [ "Dan Feldman", "Melanie Schmidt", "Christian Sohler" ],
      "venue" : "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2013
    }, {
      "title" : "Rank and null space calculations using matrix decomposition without column interchanges",
      "author" : [ "L.V. Foster" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "Foster.,? \\Q1986\\E",
      "shortCiteRegEx" : "Foster.",
      "year" : 1986
    }, {
      "title" : "Spectral grouping using the Nyström method",
      "author" : [ "C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Fowlkes et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Fowlkes et al\\.",
      "year" : 2004
    }, {
      "title" : "Fast Monte Carlo algorithms for finding low-rank approximation",
      "author" : [ "A. Frieze", "K. Kannan", "Rademacher S. Vempala" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Frieze et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Frieze et al\\.",
      "year" : 2004
    }, {
      "title" : "Relative errors for deterministic low-rank matrix approximations",
      "author" : [ "Mina Ghashami", "Jeff M Phillips" ],
      "venue" : "In Proceedings of the Twenty-Fifth Annual ACMSIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Ghashami and Phillips.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ghashami and Phillips.",
      "year" : 2014
    }, {
      "title" : "Simultaneous diagonalization of rectangular complex matrices",
      "author" : [ "P.M. Gibson" ],
      "venue" : "Linear Algebra and Its Applications,",
      "citeRegEx" : "Gibson.,? \\Q1974\\E",
      "shortCiteRegEx" : "Gibson.",
      "year" : 1974
    }, {
      "title" : "Revisiting the Nyström method for improved large-scale machine learning",
      "author" : [ "A. Gittens", "M.W. Mahoney" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Gittens and Mahoney.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gittens and Mahoney.",
      "year" : 2013
    }, {
      "title" : "Molecular classification of cancer: class discovery and class prediction by gene expression monitoring",
      "author" : [ "T. Golub", "D. Slonim", "P. Tamayo", "C. Huard", "M. Gaasenbeek", "J. Mesirov", "H. Coller", "M. Loh", "J. Downing", "M. Caligiuri" ],
      "venue" : "Science, 286:531–536,",
      "citeRegEx" : "Golub et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Golub et al\\.",
      "year" : 1999
    }, {
      "title" : "A theory of pseudoskeleton approximations",
      "author" : [ "S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin" ],
      "venue" : "Linear Algebra and Its Applications,",
      "citeRegEx" : "Goreinov et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Goreinov et al\\.",
      "year" : 1997
    }, {
      "title" : "Pseudo-skeleton approximations by matrices of maximal volume",
      "author" : [ "S.A. Goreinov", "N.L. Zamarashkin", "E.E. Tyrtyshnikov" ],
      "venue" : "Mathematical Notes,",
      "citeRegEx" : "Goreinov et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Goreinov et al\\.",
      "year" : 1997
    }, {
      "title" : "Subspace iteration randomization and singular value problems",
      "author" : [ "Ming Gu" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Gu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gu.",
      "year" : 2015
    }, {
      "title" : "Efficient algorithms for computing a strong rank-revealing QR factorization",
      "author" : [ "Ming Gu", "S.C. Eisenstat" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Gu and Eisenstat.,? \\Q1996\\E",
      "shortCiteRegEx" : "Gu and Eisenstat.",
      "year" : 1996
    }, {
      "title" : "Optimal column-based low-rank matrix reconstruction",
      "author" : [ "V. Guruswami", "A.K. Sinop" ],
      "venue" : "In Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Guruswami and Sinop.,? \\Q2012\\E",
      "shortCiteRegEx" : "Guruswami and Sinop.",
      "year" : 2012
    }, {
      "title" : "Finding Structure with Randomness : Probabilistic Algorithms for Matrix Decompositions",
      "author" : [ "N Halko", "P G Martinsson", "J A Tropp" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Halko et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Halko et al\\.",
      "year" : 2011
    }, {
      "title" : "Canonical correlation analysis: An overview with application to learning methods",
      "author" : [ "D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hardoon et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hardoon et al\\.",
      "year" : 2004
    }, {
      "title" : "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2001
    }, {
      "title" : "Matrix completion and low-rank svd via fast alternating least squares",
      "author" : [ "Trevor Hastie", "Rahul Mazumder", "Jason Lee", "Reza Zadeh" ],
      "venue" : "arXiv preprint arXiv:1410.2596,",
      "citeRegEx" : "Hastie et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2014
    }, {
      "title" : "The hat matrix in regression and ANOVA",
      "author" : [ "D.C. Hoaglin", "R.E. Welsch" ],
      "venue" : "The American Statistician,",
      "citeRegEx" : "Hoaglin and Welsch.,? \\Q1978\\E",
      "shortCiteRegEx" : "Hoaglin and Welsch.",
      "year" : 1978
    }, {
      "title" : "Rank-revealing QR factorizations and the singular value decomposition",
      "author" : [ "Y.P. Hong", "C.T. Pan" ],
      "venue" : "Mathematics of Computation,",
      "citeRegEx" : "Hong and Pan.,? \\Q1992\\E",
      "shortCiteRegEx" : "Hong and Pan.",
      "year" : 1992
    }, {
      "title" : "On the singular values of a product of completely continuous operators",
      "author" : [ "A. Horn" ],
      "venue" : "Proc. Nat. Acad. Sci. USA,",
      "citeRegEx" : "Horn.,? \\Q1951\\E",
      "shortCiteRegEx" : "Horn.",
      "year" : 1951
    }, {
      "title" : "On the eigenvalues of a matrix with prescribed singular values",
      "author" : [ "A. Horn" ],
      "venue" : "Proc. Amer. Math. Soc.,",
      "citeRegEx" : "Horn.,? \\Q1954\\E",
      "shortCiteRegEx" : "Horn.",
      "year" : 1954
    }, {
      "title" : "Matrix Analysis",
      "author" : [ "Roger A. Horn", "Charles R. Johnson" ],
      "venue" : null,
      "citeRegEx" : "Horn and Johnson.,? \\Q1985\\E",
      "shortCiteRegEx" : "Horn and Johnson.",
      "year" : 1985
    }, {
      "title" : "Topics in Matrix Analysis",
      "author" : [ "Roger A. Horn", "Charles R. Johnson" ],
      "venue" : null,
      "citeRegEx" : "Horn and Johnson.,? \\Q1991\\E",
      "shortCiteRegEx" : "Horn and Johnson.",
      "year" : 1991
    }, {
      "title" : "Structure preserving dimension reduction for clustered text data based on the generalized singular value decomposition",
      "author" : [ "P. Howland", "M. Jeon", "H. Park" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Howland et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Howland et al\\.",
      "year" : 2003
    }, {
      "title" : "Improved bound for the Nyström method and its application to kernel classification",
      "author" : [ "R. Jin", "T. Yang", "M. Mahdavi", "Y.F. Li", "Z.H. Zhou" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Jin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2013
    }, {
      "title" : "Extensions of Lipschitz mapping into a Hilbert space",
      "author" : [ "W.B. Johnson", "J. Lindenstrauss" ],
      "venue" : "Contemporary Mathematics,",
      "citeRegEx" : "Johnson and Lindenstrauss.,? \\Q1984\\E",
      "shortCiteRegEx" : "Johnson and Lindenstrauss.",
      "year" : 1984
    }, {
      "title" : "Principal component analysis. Springer, New York, second edition",
      "author" : [ "I.T. Jolliffe" ],
      "venue" : null,
      "citeRegEx" : "Jolliffe.,? \\Q2002\\E",
      "shortCiteRegEx" : "Jolliffe.",
      "year" : 2002
    }, {
      "title" : "Sparser johnson-lindenstrauss transforms",
      "author" : [ "Daniel M Kane", "Jelani Nelson" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Kane and Nelson.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kane and Nelson.",
      "year" : 2014
    }, {
      "title" : "Learning with whom to share in multi-task feature learning",
      "author" : [ "Zhuoliang Kang", "Kristen Grauman", "Fei Sha" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Kang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2011
    }, {
      "title" : "A new approach to feature selection based on the Karhunen-Loève expansion",
      "author" : [ "J. Kittler", "P.C. Young" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Kittler and Young.,? \\Q1973\\E",
      "shortCiteRegEx" : "Kittler and Young.",
      "year" : 1973
    }, {
      "title" : "Ensemble Nyström method",
      "author" : [ "S. Kumar", "M. Mohri", "A. Talwalkar" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Kumar et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2009
    }, {
      "title" : "Vector algebra in the analysis of genome-wide expression data",
      "author" : [ "F.G. Kuruvilla", "P.J. Park", "S.L. Schreiber" ],
      "venue" : "Genome Biology,",
      "citeRegEx" : "Kuruvilla et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kuruvilla et al\\.",
      "year" : 2002
    }, {
      "title" : "Approximate matrix multiplication with application to linear embeddings",
      "author" : [ "Anastasios Kyrillidis", "Michail Vlachos", "Anastasios Zouzias" ],
      "venue" : "In Information Theory (ISIT),",
      "citeRegEx" : "Kyrillidis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kyrillidis et al\\.",
      "year" : 2014
    }, {
      "title" : "The mathematics of eigenvalue optimization",
      "author" : [ "Adrian S Lewis" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Lewis.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lewis.",
      "year" : 2003
    }, {
      "title" : "Simple and deterministic matrix sketching",
      "author" : [ "Edo Liberty" ],
      "venue" : "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Liberty.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liberty.",
      "year" : 2013
    }, {
      "title" : "Tensor completion for estimating missing values in visual data",
      "author" : [ "Ji Liu", "Przemyslaw Musialski", "Peter Wonka", "Jieping Ye" ],
      "venue" : "In Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Liu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "Generalizing the singular value decomposition",
      "author" : [ "C.F. Van Loan" ],
      "venue" : "SIAM Journal on numerical Analysis,",
      "citeRegEx" : "Loan.,? \\Q1976\\E",
      "shortCiteRegEx" : "Loan.",
      "year" : 1976
    }, {
      "title" : "Support matrix machines",
      "author" : [ "Luo Luo", "Yubo Xie", "Zhihua Zhang", "Wu-Jun Li" ],
      "venue" : "In The International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Luo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2015
    }, {
      "title" : "A statistical perspective on algorithmic leveraging",
      "author" : [ "Ping Ma", "Michael Mahoney", "Bin Yu" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2014
    }, {
      "title" : "Matrix Differential Calculus with Applications in Statistics and Econometrics",
      "author" : [ "Jan R. Macnus", "Heinz Neudecker" ],
      "venue" : null,
      "citeRegEx" : "Macnus and Neudecker.,? \\Q2000\\E",
      "shortCiteRegEx" : "Macnus and Neudecker.",
      "year" : 2000
    }, {
      "title" : "Using a non-commutative Bernstein bound to approximate some matrix algorithms in the spectral norm",
      "author" : [ "Malik Magdon-Ismail" ],
      "venue" : "arXiv preprint arXiv:1103.5453,",
      "citeRegEx" : "Magdon.Ismail.,? \\Q2011\\E",
      "shortCiteRegEx" : "Magdon.Ismail.",
      "year" : 2011
    }, {
      "title" : "Low rank matrix-valued chernoff bounds and approximate matrix multiplication",
      "author" : [ "Avner Magen", "Anastasios Zouzias" ],
      "venue" : "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,",
      "citeRegEx" : "Magen and Zouzias.,? \\Q2011\\E",
      "shortCiteRegEx" : "Magen and Zouzias.",
      "year" : 2011
    }, {
      "title" : "CUR matrix decompositions for improved data analysis",
      "author" : [ "M.W. Mahoney", "P. Drineas" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Mahoney and Drineas.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mahoney and Drineas.",
      "year" : 2009
    }, {
      "title" : "Tensor-CUR decompositions for tensor-based data",
      "author" : [ "M.W. Mahoney", "M. Maggioni", "P. Drineas" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Mahoney et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mahoney et al\\.",
      "year" : 2008
    }, {
      "title" : "Randomized algorithms for matrices and data",
      "author" : [ "Michael W. Mahoney" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Mahoney.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mahoney.",
      "year" : 2011
    }, {
      "title" : "Multivariate Analysis",
      "author" : [ "K.V. Mardia", "J.T. Kent", "J.M. Bibby" ],
      "venue" : null,
      "citeRegEx" : "Mardia et al\\.,? \\Q1979\\E",
      "shortCiteRegEx" : "Mardia et al\\.",
      "year" : 1979
    }, {
      "title" : "Inequalities: Theory of Majorization and Its Applications",
      "author" : [ "Albert W. Marshal", "Ingram Olkin", "Barry C. Arnold" ],
      "venue" : null,
      "citeRegEx" : "Marshal et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Marshal et al\\.",
      "year" : 2010
    }, {
      "title" : "A randomized algorithm for the decomposition of matrices",
      "author" : [ "Per-Gunnar Martinsson", "Vladimir Rokhlin", "Mark Tygert" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "Martinsson et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Martinsson et al\\.",
      "year" : 2011
    }, {
      "title" : "Spectral regularization algorithms for learning large incomplete matrices",
      "author" : [ "Rahul Mazumder", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : "Journal of machine learning research,",
      "citeRegEx" : "Mazumder et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mazumder et al\\.",
      "year" : 2010
    }, {
      "title" : "Invariant feature extraction and classification in kernel space",
      "author" : [ "S. Mika", "G. Rätsch", "J. Weston", "B. Schölkopf", "A. Smola", "K.R. Müller" ],
      "venue" : "In Advances in Neural Information Processing Systems 12,",
      "citeRegEx" : "Mika et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Mika et al\\.",
      "year" : 2000
    }, {
      "title" : "Symmetric gauge functions and unitarily invariant norms",
      "author" : [ "L. Mirsky" ],
      "venue" : "Quarterly Journal of Mathemathics,",
      "citeRegEx" : "Mirsky.,? \\Q1960\\E",
      "shortCiteRegEx" : "Mirsky.",
      "year" : 1960
    }, {
      "title" : "Aspects of Multivariate Statistical Theory",
      "author" : [ "R.J. Muirhead" ],
      "venue" : null,
      "citeRegEx" : "Muirhead.,? \\Q1982\\E",
      "shortCiteRegEx" : "Muirhead.",
      "year" : 1982
    }, {
      "title" : "Singular value decomposition, eigenfaces, and 3 D reconstruction",
      "author" : [ "N. Muller", "L. Magaia", "B.M. Herbst" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Muller et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Muller et al\\.",
      "year" : 2004
    }, {
      "title" : "Stronger approximate singular value decomposition via the block lanczos and power methods",
      "author" : [ "Cameron Musco", "Christopher Musco" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Musco and Musco.,? \\Q2015\\E",
      "shortCiteRegEx" : "Musco and Musco.",
      "year" : 2015
    }, {
      "title" : "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings",
      "author" : [ "Jelani Nelson", "Huy L Nguyên" ],
      "venue" : "In IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Nelson and Nguyên.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nelson and Nguyên.",
      "year" : 2013
    }, {
      "title" : "Some matrix-inequalities and metrication of matrix-space",
      "author" : [ "J. von Neumann" ],
      "venue" : "Tomsk University Review,",
      "citeRegEx" : "Neumann.,? \\Q1937\\E",
      "shortCiteRegEx" : "Neumann.",
      "year" : 1937
    }, {
      "title" : "Über die praktische auflösung von integralgleichungen mit anwendungen auf randwertaufgaben",
      "author" : [ "Evert J. Nyström" ],
      "venue" : "Acta Mathematica,",
      "citeRegEx" : "Nyström.,? \\Q1930\\E",
      "shortCiteRegEx" : "Nyström.",
      "year" : 1930
    }, {
      "title" : "Towards a generalized singular value decomposition",
      "author" : [ "C.C. Paige", "M.A. Saunders" ],
      "venue" : "SIAM Journal on Numerical Analysis,",
      "citeRegEx" : "Paige and Saunders.,? \\Q1981\\E",
      "shortCiteRegEx" : "Paige and Saunders.",
      "year" : 1981
    }, {
      "title" : "Latent semantic indexing: A probabilistic analysis",
      "author" : [ "Christos H Papadimitriou", "Hisao Tamaki", "Prabhakar Raghavan", "Santosh Vempala" ],
      "venue" : "In Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,",
      "citeRegEx" : "Papadimitriou et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Papadimitriou et al\\.",
      "year" : 1998
    }, {
      "title" : "Nonlinear discriminant analysis using kernel functions and the generalized singular value decomposition",
      "author" : [ "C.H. Park", "H. Park" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Park and Park.,? \\Q2005\\E",
      "shortCiteRegEx" : "Park and Park.",
      "year" : 2005
    }, {
      "title" : "Trace norm regularization: reformulations, algorithms, and multi-task learning",
      "author" : [ "Ting Kei Pong", "Paul Tseng", "Shuiwang Ji", "Jieping Ye" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Pong et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pong et al\\.",
      "year" : 2010
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen and Williams.,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen and Williams.",
      "year" : 2006
    }, {
      "title" : "Convex Analysis",
      "author" : [ "T. Rockafellar" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar.,? \\Q1970\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1970
    }, {
      "title" : "A randomized algorithm for principal component analysis",
      "author" : [ "V. Rokhlin", "A. Szlam", "M. Tygert" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Rokhlin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rokhlin et al\\.",
      "year" : 2009
    }, {
      "title" : "Nonlinear discriminant analysis using kernel functions",
      "author" : [ "V. Roth", "V. Steinhage" ],
      "venue" : "In Advances in Neural Information Processing Systems 12,",
      "citeRegEx" : "Roth and Steinhage.,? \\Q2000\\E",
      "shortCiteRegEx" : "Roth and Steinhage.",
      "year" : 2000
    }, {
      "title" : "Numerical methods for large eigenvalue problems. preparation",
      "author" : [ "Yousef Saad" ],
      "venue" : "Available from: http://www-users. cs. umn. edu/saad/books. html,",
      "citeRegEx" : "Saad.,? \\Q2011\\E",
      "shortCiteRegEx" : "Saad.",
      "year" : 2011
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "Tamas Sarlos" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Sarlos.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sarlos.",
      "year" : 2006
    }, {
      "title" : "A Theory of Cross-Space",
      "author" : [ "Robert Schatten" ],
      "venue" : null,
      "citeRegEx" : "Schatten.,? \\Q1950\\E",
      "shortCiteRegEx" : "Schatten.",
      "year" : 1950
    }, {
      "title" : "Learning with Kernels",
      "author" : [ "B. Schölkopf", "A. Smola" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf and Smola.,? \\Q2002\\E",
      "shortCiteRegEx" : "Schölkopf and Smola.",
      "year" : 2002
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "B. Schölkopf", "A. Smola", "K.-R. Müller" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1998
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "Shawe.Taylor and Cristianini.,? \\Q2004\\E",
      "shortCiteRegEx" : "Shawe.Taylor and Cristianini.",
      "year" : 2004
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "Jianbo Shi", "Jitendra Malik" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Shi and Malik.,? \\Q2000\\E",
      "shortCiteRegEx" : "Shi and Malik.",
      "year" : 2000
    }, {
      "title" : "Memory efficient kernel approximation",
      "author" : [ "Si Si", "Cho-Jui Hsieh", "Inderjit Dhillon" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Si et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Si et al\\.",
      "year" : 2014
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2004
    }, {
      "title" : "Four algorithms for the efficient computation of truncated pivoted QR approximations to a sparse matrix",
      "author" : [ "G.W. Stewart" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Stewart.,? \\Q1999\\E",
      "shortCiteRegEx" : "Stewart.",
      "year" : 1999
    }, {
      "title" : "Matrix Perturbation Theory",
      "author" : [ "G.W. Stewart", "J.G. Sun" ],
      "venue" : null,
      "citeRegEx" : "Stewart and Sun.,? \\Q1990\\E",
      "shortCiteRegEx" : "Stewart and Sun.",
      "year" : 1990
    }, {
      "title" : "Matrix coherence and the Nyström method",
      "author" : [ "A. Talwalkar", "A. Rostamizadeh" ],
      "venue" : "Proceedings of the 26th Conference in Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Talwalkar and Rostamizadeh.,? \\Q2010\\E",
      "shortCiteRegEx" : "Talwalkar and Rostamizadeh.",
      "year" : 2010
    }, {
      "title" : "Large-scale manifold learning",
      "author" : [ "A. Talwalkar", "S. Kumar", "H. Rowley" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Talwalkar et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Talwalkar et al\\.",
      "year" : 2008
    }, {
      "title" : "Largescale SVD and manifold learning",
      "author" : [ "Ameet Talwalkar", "Sanjiv Kumar", "Mehryar Mohri", "Henry Rowley" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Talwalkar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Talwalkar et al\\.",
      "year" : 2013
    }, {
      "title" : "A global geometric framework for nonlinear dimensionality reduction",
      "author" : [ "Joshua B Tenenbaum", "Vin De Silva", "John C Langford" ],
      "venue" : null,
      "citeRegEx" : "Tenenbaum et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Tenenbaum et al\\.",
      "year" : 2000
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "An introduction to matrix concentration inequalities",
      "author" : [ "Joel A Tropp" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Tropp.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tropp.",
      "year" : 2015
    }, {
      "title" : "Making fisher discriminant analysis scalable",
      "author" : [ "Bojun Tu", "Zhihua Zhang", "ShusenWang", "Hui Qiani" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning",
      "citeRegEx" : "Tu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2014
    }, {
      "title" : "Face recognition using eigenfaces",
      "author" : [ "M.A. Turk", "A.P. Pentland" ],
      "venue" : "In Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Turk and Pentland.,? \\Q1991\\E",
      "shortCiteRegEx" : "Turk and Pentland.",
      "year" : 1991
    }, {
      "title" : "Incomplete cross approximation in the mosaic-skeleton",
      "author" : [ "E.E. Tyrtyshnikov" ],
      "venue" : "method. Computing,",
      "citeRegEx" : "Tyrtyshnikov.,? \\Q2000\\E",
      "shortCiteRegEx" : "Tyrtyshnikov.",
      "year" : 2000
    }, {
      "title" : "Kernel canonical correlation analysis and least squares support vector machines",
      "author" : [ "T. Van Gestel", "J.A.K. Suykens", "J. De Brabanter", "B. De Moor", "J. Vandewalle" ],
      "venue" : "In The International Conference on Artificial Neural Networks (ICANN),",
      "citeRegEx" : "Gestel et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Gestel et al\\.",
      "year" : 2001
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik.,? \\Q1998\\E",
      "shortCiteRegEx" : "Vapnik.",
      "year" : 1998
    }, {
      "title" : "The Random Projection Method",
      "author" : [ "Santosh S. Vempala" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "Vempala.,? \\Q2000\\E",
      "shortCiteRegEx" : "Vempala.",
      "year" : 2000
    }, {
      "title" : "Improving CUR matrix decomposition and the Nyström approximation via adaptive sampling",
      "author" : [ "Shusen Wang", "Zhihua Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Wang and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang and Zhang.",
      "year" : 2013
    }, {
      "title" : "Efficient algorithms and error analysis for the modified nyström method",
      "author" : [ "Shusen Wang", "Zhihua Zhang" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Wang and Zhang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang and Zhang.",
      "year" : 2014
    }, {
      "title" : "The modified Nyström method: Theories, algorithms, and extension",
      "author" : [ "Shusen Wang", "Luo Luo", "Zhihua Zhang" ],
      "venue" : "CoRR, abs/1406.5675,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving the modified nyström method using spectral shifting",
      "author" : [ "Shusen Wang", "Chao Zhang", "Hui Qian", "Zhihua Zhang" ],
      "venue" : "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved analyses of the randomized power method and block Lanczos method",
      "author" : [ "Shusen Wang", "Zhihua Zhang", "Tong Zhang" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards more efficient symmetric matrix sketching and cur matrix decomposition",
      "author" : [ "Shusen Wang", "Zhihua Zhang", "Tong Zhang" ],
      "venue" : "arXiv preprint arXiv:1503.08395,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Fundamentals of Matrix Computations",
      "author" : [ "D.S. Watkins" ],
      "venue" : null,
      "citeRegEx" : "Watkins.,? \\Q1991\\E",
      "shortCiteRegEx" : "Watkins.",
      "year" : 1991
    }, {
      "title" : "Characterization of the subdifferential of some matrix norms",
      "author" : [ "G.A. Watson" ],
      "venue" : "Linear Algebra and Its Applications,",
      "citeRegEx" : "Watson.,? \\Q1992\\E",
      "shortCiteRegEx" : "Watson.",
      "year" : 1992
    }, {
      "title" : "Using the Nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Williams and Seeger.,? \\Q2001\\E",
      "shortCiteRegEx" : "Williams and Seeger.",
      "year" : 2001
    }, {
      "title" : "Randomized algorithms for low-rank matrix factorizations: sharp performance",
      "author" : [ "Rafi Witten", "Emmanuel Candès" ],
      "venue" : "bounds. Algorithmica,",
      "citeRegEx" : "Witten and Candès.,? \\Q2013\\E",
      "shortCiteRegEx" : "Witten and Candès.",
      "year" : 2013
    }, {
      "title" : "Low rank approximation lower bounds in row-update streams",
      "author" : [ "David Woodruff" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Woodruff.,? \\Q2014\\E",
      "shortCiteRegEx" : "Woodruff.",
      "year" : 2014
    }, {
      "title" : "Sketching as a tool for numerical linear algebra",
      "author" : [ "David P Woodruff" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "Woodruff.,? \\Q2014\\E",
      "shortCiteRegEx" : "Woodruff.",
      "year" : 2014
    }, {
      "title" : "A fast randomized algorithm for the approximation of matrices",
      "author" : [ "Franco Woolfe", "Edo Liberty", "Vladimir Rokhlin", "Mark Tygert" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "Woolfe et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Woolfe et al\\.",
      "year" : 2008
    }, {
      "title" : "Computational and theoretical analysis of null space and orthogonal linear discriminant analysis",
      "author" : [ "J. Ye", "T. Xiong" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ye and Xiong.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ye and Xiong.",
      "year" : 2006
    }, {
      "title" : "Clustered Nyström method for large scale manifold learning and dimension reduction",
      "author" : [ "K. Zhang", "J.T. Kwok" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Zhang and Kwok.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang and Kwok.",
      "year" : 2010
    }, {
      "title" : "Improved Nyström low-rank approximation and error analysis",
      "author" : [ "K. Zhang", "I.W. Tsang", "J.T. Kwok" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Zhang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2008
    }, {
      "title" : "The matrix ridge approximation: algorithms and applications",
      "author" : [ "Zhihua Zhang" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Zhang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2014
    }, {
      "title" : "Regularized discriminant analysis, ridge regression and beyond",
      "author" : [ "Zhihua Zhang", "Guang Dai", "Congfu Xu", "Michael I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    }, {
      "title" : "Regularized matrix regression",
      "author" : [ "Hua Zhou", "Lexin Li" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Zhou and Li.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhou and Li.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 38,
      "context" : "The first proof of the SVD for general m × n matrices might be given by Eckart and Young [1939]. But the theory of singular values can date back to the 19th century when it had been studied by the Italian differential geometer E.",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 55,
      "context" : "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 55,
      "context" : "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values. There is a rich literature involving singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides exhaustive studies about inequalities of singular values as well as unitarily invariant norms, and the primary focus is on the matrix theory.",
      "startOffset" : 29,
      "endOffset" : 264
    }, {
      "referenceID" : 55,
      "context" : "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values. There is a rich literature involving singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides exhaustive studies about inequalities of singular values as well as unitarily invariant norms, and the primary focus is on the matrix theory. The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.",
      "startOffset" : 29,
      "endOffset" : 442
    }, {
      "referenceID" : 28,
      "context" : "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.",
      "startOffset" : 28,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.",
      "startOffset" : 28,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.",
      "startOffset" : 28,
      "endOffset" : 99
    }, {
      "referenceID" : 60,
      "context" : "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions.",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 60,
      "context" : "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al.",
      "startOffset" : 61,
      "endOffset" : 268
    }, {
      "referenceID" : 60,
      "context" : "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997].",
      "startOffset" : 61,
      "endOffset" : 291
    }, {
      "referenceID" : 60,
      "context" : "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997].",
      "startOffset" : 61,
      "endOffset" : 308
    }, {
      "referenceID" : 60,
      "context" : "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997]. In Chapter 2 we review some preliminaries such as Kronecker produces and vectorization operators, majorization theory, and derivatives.",
      "startOffset" : 61,
      "endOffset" : 323
    }, {
      "referenceID" : 96,
      "context" : "Specifically, we apply matrix differential calculus to rederive the von Neumann theorem [Neumann, 1937] and the Ky Fan theorem [Fan, 1951].",
      "startOffset" : 88,
      "endOffset" : 103
    }, {
      "referenceID" : 40,
      "context" : "Specifically, we apply matrix differential calculus to rederive the von Neumann theorem [Neumann, 1937] and the Ky Fan theorem [Fan, 1951].",
      "startOffset" : 127,
      "endOffset" : 138
    }, {
      "referenceID" : 38,
      "context" : "The second one was proposed by Mirsky [1960], which is an extension of the novel Eckart Young theorem [Eckart and Young, 1936].",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 89,
      "context" : "The second one was proposed by Mirsky [1960], which is an extension of the novel Eckart Young theorem [Eckart and Young, 1936].",
      "startOffset" : 31,
      "endOffset" : 45
    }, {
      "referenceID" : 92,
      "context" : "The following properties can be found in Muirhead [1982].",
      "startOffset" : 41,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "The further details of these results can be found from Borwein and Lewis [2006]. The following lemma then shows the fundamental role of subgradients in optimization.",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 58,
      "context" : "The statistical leverage [Hoaglin and Welsch, 1978] measures the extent to which the singular vectors of a matrix are correlated with the standard basis.",
      "startOffset" : 25,
      "endOffset" : 51
    }, {
      "referenceID" : 48,
      "context" : "That is a so-called CS decomposition [Golub et al., 1999] given as follows.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 76,
      "context" : "The GSVD theorem was originally proposed by Loan [1976], in which n ≥ p (or m ≥ p) is required.",
      "startOffset" : 44,
      "endOffset" : 56
    }, {
      "referenceID" : 76,
      "context" : "The GSVD theorem was originally proposed by Loan [1976], in which n ≥ p (or m ≥ p) is required. Later on, Paige and Saunders [1981] developed a more general formulation for GSVD in which matrix pencil A and B are required only to have the same number of columns.",
      "startOffset" : 44,
      "endOffset" : 132
    }, {
      "referenceID" : 76,
      "context" : "The GSVD theorem was originally proposed by Loan [1976], in which n ≥ p (or m ≥ p) is required. Later on, Paige and Saunders [1981] developed a more general formulation for GSVD in which matrix pencil A and B are required only to have the same number of columns. Paige and Saunders [1981] also studied a GSVD of submatrices of a column orthonormal matrix.",
      "startOffset" : 44,
      "endOffset" : 289
    }, {
      "referenceID" : 63,
      "context" : "Based on this observation, Howland et al. [2003], Park and Park [2005] applied GSVD for solving Fisher linear discriminant analysis (FLDA) and generalized Fisher discriminant analysis [Baudat and Anouar, 2000, Mika et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 63,
      "context" : "Based on this observation, Howland et al. [2003], Park and Park [2005] applied GSVD for solving Fisher linear discriminant analysis (FLDA) and generalized Fisher discriminant analysis [Baudat and Anouar, 2000, Mika et al.",
      "startOffset" : 27,
      "endOffset" : 71
    }, {
      "referenceID" : 46,
      "context" : "Gibson [1974] proved that they have joint factorizations A = UΣAV T and B = UΣBV T if and only if ABT and BTA are both normal.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 2,
      "context" : "For example, SVD is an important tool in spectral analysis [Azar et al., 2001], latent semantic indexing [Papadimitriou et al.",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 99,
      "context" : ", 2001], latent semantic indexing [Papadimitriou et al., 1998], spectral clustering, and projective clustering [Feldman et al.",
      "startOffset" : 34,
      "endOffset" : 62
    }, {
      "referenceID" : 41,
      "context" : ", 1998], spectral clustering, and projective clustering [Feldman et al., 2013].",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "This leads us to the notion of the matrix Moore-Penrose (MP) pseudoinverse [Ben-Israel and Greville, 2003].",
      "startOffset" : 75,
      "endOffset" : 106
    }, {
      "referenceID" : 144,
      "context" : "However, when N is singular, Zhang et al. [2010] suggested to use a pseudoinverse eigenproblem:",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 144,
      "context" : "Moreover, Zhang et al. [2010] established a connection between the solutions of the generalized eigenproblem and its corresponding pseudoinverse eigenproblem.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 86,
      "context" : "Fisher discriminant analysis (FDA) is a classical method for classification and dimension reduction simultaneously [Mardia et al., 1979].",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 86,
      "context" : "That is, there is a duality relationship between PCA and PCO [Mardia et al., 1979].",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 55,
      "context" : "CCA is another subspace learning model [Hardoon et al., 2004].",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 110,
      "context" : "For example, kernel PCA [Schölkopf et al., 1998], kernel FDA [Baudat and Anouar, 2000, Mika et al.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 59,
      "context" : "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992].",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 51,
      "context" : "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992].",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 51,
      "context" : "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992]. Stewart [1999] devised efficient computational algorithms of truncated pivoted QR approximations to a sparse matrix.",
      "startOffset" : 14,
      "endOffset" : 153
    }, {
      "referenceID" : 72,
      "context" : "Kuruvilla et al. [2002] have claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005].",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 111,
      "context" : "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al.",
      "startOffset" : 15,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005]. The SCA approximation is of the form A ≈ XTY, where X and Y consist of columns and rows of A, and T minimizes ‖A−XTY‖F . This algorithm is a deterministic peocedure but computationally expensive. The terminology of the CUR decomposition has been proposed by Drineas and Mahoney [2005], Mahoney et al.",
      "startOffset" : 147,
      "endOffset" : 453
    }, {
      "referenceID" : 9,
      "context" : "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005]. The SCA approximation is of the form A ≈ XTY, where X and Y consist of columns and rows of A, and T minimizes ‖A−XTY‖F . This algorithm is a deterministic peocedure but computationally expensive. The terminology of the CUR decomposition has been proposed by Drineas and Mahoney [2005], Mahoney et al. [2008]. They reformulated the idea based on random selection.",
      "startOffset" : 147,
      "endOffset" : 476
    }, {
      "referenceID" : 137,
      "context" : "For example, they have been applied to Gaussian processes [Williams and Seeger, 2001], kernel classification [Zhang et al.",
      "startOffset" : 58,
      "endOffset" : 85
    }, {
      "referenceID" : 43,
      "context" : ", 2013], spectral clustering [Fowlkes et al., 2004], kernel PCA and manifold learning [Talwalkar et al.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : ", 2008, Zhang and Kwok, 2010], determinantal processes [Affandi et al., 2013], etc.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 116,
      "context" : "Variational principles correspond to matrix perturbation theory [Stewart and Sun, 1990], which is the theoretical foundation to characterize stability or sensitivity of a matrix computation algorithm.",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 96,
      "context" : "The cornerstones are the novel von Neumann theorem [Neumann, 1937] and Ky Fan theorem [Fan, 1951].",
      "startOffset" : 51,
      "endOffset" : 66
    }, {
      "referenceID" : 40,
      "context" : "The cornerstones are the novel von Neumann theorem [Neumann, 1937] and Ky Fan theorem [Fan, 1951].",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 96,
      "context" : "The following cornerstone theorem was originally established by von Neumann [1937].",
      "startOffset" : 68,
      "endOffset" : 83
    }, {
      "referenceID" : 62,
      "context" : "The first result directly follows from the well known interlacing theorem [Horn and Johnson, 1985].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 108,
      "context" : "[Schatten, 1950] Let u,v ∈ Rn.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 40,
      "context" : "[Fan, 1951] Given two nonnegative vectors u,v ∈ R+, then u ≺w v if and only if φ(u) ≤ φ(v) for every symmetric gauge function φ.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 121,
      "context" : "Parallel with the l1-norm which is used as convex relaxation of the l0-norm [Tibshirani, 1996], the nuclear norm is a convex alternative of the matrix rank.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 116,
      "context" : "Note that the Hoffman-Wielandt theorem still hods when A and B are normal [Stewart and Sun, 1990].",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 57,
      "context" : "Moreover, this proposition was widely used in matrix completion problems, because an optimization problem regularized by the Frobenius norm is solved more easily than that regularized by the nuclear norm [Hastie et al., 2014].",
      "startOffset" : 204,
      "endOffset" : 225
    }, {
      "referenceID" : 60,
      "context" : "35 of Horn and Johnson [1985]. As for Part (b), it is obvious that the Frobenius norm is both unitarily invariant and vectorization norm.",
      "startOffset" : 6,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "But norm functions are convex and continuous, so we can resort to theory of subdifferentials [Rockafellar, 1970, Borwein and Lewis, 2006]. Indeed, the subdifferentials of unitarily invariant norms have been studied by Watson [1992] and Lewis [2003].",
      "startOffset" : 113,
      "endOffset" : 232
    }, {
      "referenceID" : 13,
      "context" : "But norm functions are convex and continuous, so we can resort to theory of subdifferentials [Rockafellar, 1970, Borwein and Lewis, 2006]. Indeed, the subdifferentials of unitarily invariant norms have been studied by Watson [1992] and Lewis [2003]. Using the properties of unitarily invariant norms and the SVD theory, we present directional derivatives and subdifferentials of unitarily invariant norms.",
      "startOffset" : 113,
      "endOffset" : 249
    }, {
      "referenceID" : 19,
      "context" : "Moreover, this model can be regarded as a parallel version of the Dantzig selector [Candès and Tao, 2007].",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 122,
      "context" : "However, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest.",
      "startOffset" : 9,
      "endOffset" : 22
    }, {
      "referenceID" : 38,
      "context" : "The theorem was originally proposed by Eckart and Young [1936] under the setting of the Frobenius norm, and generalized to any unitarily invariant norms by Mirsky [1960].",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 38,
      "context" : "The theorem was originally proposed by Eckart and Young [1936] under the setting of the Frobenius norm, and generalized to any unitarily invariant norms by Mirsky [1960].",
      "startOffset" : 39,
      "endOffset" : 170
    }, {
      "referenceID" : 122,
      "context" : "He even said “Frobenius-norm error bounds are typically vacuous” [Tropp, 2015].",
      "startOffset" : 65,
      "endOffset" : 78
    }, {
      "referenceID" : 51,
      "context" : "[Gu, 2015] Given any matrix A ∈ Rm×n, let p = min{m,n} and B be a matrix with rank at most k such that",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 121,
      "context" : "However, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest.",
      "startOffset" : 9,
      "endOffset" : 22
    }, {
      "referenceID" : 51,
      "context" : "The following theorem was proposed by Gu [2015], which relates the approximation error in the Frobenius norm to that in the spectral norm.",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "Moreover, it can be also used in large scalable k-means clustering [Cohen et al., 2014], approximate leverage scores [Drineas et al.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "[2015] proved optimal approximate matrix multiplication in terms of stable rank by using subspace embedding [Batson et al., 2014].",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "Recently, Cohen et al. [2015] proved optimal approximate matrix multiplication in terms of stable rank by using subspace embedding [Batson et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 26,
      "context" : "[Cohen et al., 2015] Given ε, δ ∈ (0, 1/2), let A and B be two conforming matrices, and Π be a (ε, δ) subspace embedding for the 2r̃-dimensional subspace, where r̃ is the maximum of the stable ranks of A and B.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 68,
      "context" : "To analyze approximate matrix multiplication with the Frobenius error, Kane and Nelson [2014] introduced the JL-moment property.",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 68,
      "context" : "More specifically, they can be converted into each other [Kane and Nelson, 2014].",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 67,
      "context" : "More specifically, they can be converted into each other [Kane and Nelson, 2014]. There are other methods, which do not use subspace embedding matrices, in the literature. Magen and Zouzias [2011] gave a method based on columns selection.",
      "startOffset" : 58,
      "endOffset" : 197
    }, {
      "referenceID" : 10,
      "context" : "Bhojanapalli et al. [2015] proposed a new method with sampling and alternating minimization to directly compute a low-rank approximation to the product of two given matrices.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches.",
      "startOffset" : 58,
      "endOffset" : 87
    }, {
      "referenceID" : 22,
      "context" : "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].",
      "startOffset" : 58,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].",
      "startOffset" : 58,
      "endOffset" : 270
    }, {
      "referenceID" : 22,
      "context" : "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].",
      "startOffset" : 58,
      "endOffset" : 321
    }, {
      "referenceID" : 28,
      "context" : "Fortunately, many machine learning methods such as latent semantic indexing [Deerwester et al., 1990], spectral clustering [Shi and Malik, 2000], manifold learning [Tenenbaum et al.",
      "startOffset" : 76,
      "endOffset" : 101
    }, {
      "referenceID" : 112,
      "context" : ", 1990], spectral clustering [Shi and Malik, 2000], manifold learning [Tenenbaum et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 54,
      "context" : "In particular, we will consider randomized SVD methods [Halko et al., 2011].",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "2 are also fundamental in random column selection [Boutsidis et al., 2014].",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "In particular, the count sketch [Clarkson and Woodruff, 2013] applies to A in only O(nnz(A)) time and exhibits very similar properties as the JL transform.",
      "startOffset" : 32,
      "endOffset" : 61
    }, {
      "referenceID" : 24,
      "context" : "The Johnson & Lindenstrauss (JL) transform [Johnson and Lindenstrauss, 1984, Dasgupta and Gupta, 2003] is known to keep isometry in expectation or with high probability. Halko et al. [2011], Boutsidis et al.",
      "startOffset" : 77,
      "endOffset" : 190
    }, {
      "referenceID" : 15,
      "context" : "[2011], Boutsidis et al. [2014] used the JL transform for sketching and showed relative-error bounds.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "[2011], Boutsidis et al. [2014] used the JL transform for sketching and showed relative-error bounds. However, the Gaussian test matrix is dense and cannot efficiently apply to matrices. Several improvements have been proposed to make the sketching matrix sparser; see the review [Woodruff, 2014b] for the complete list of the literature. In particular, the count sketch [Clarkson and Woodruff, 2013] applies to A in only O(nnz(A)) time and exhibits very similar properties as the JL transform. Specifically, Woodruff [2014b] showed that an m×O(k/ǫ) sketch C = AΩ can be obtained in O(nnz(A)) time and",
      "startOffset" : 8,
      "endOffset" : 526
    }, {
      "referenceID" : 54,
      "context" : "Halko et al. [2011] proposed to directly solve the left-hand side of (10.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "Clarkson and Woodruff [2013], Woodruff [2014b] showed that",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 22,
      "context" : "Clarkson and Woodruff [2013], Woodruff [2014b] showed that",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 51,
      "context" : "We apply Gu’s theorem [Gu, 2015] (Theorem 9.",
      "startOffset" : 22,
      "endOffset" : 32
    }, {
      "referenceID" : 51,
      "context" : ", 2011, Gu, 2015]. The algorithm is described in Algorithm 2 and analyzed in the following. LetΩ ∈ Rn×c be a Gaussian test matrix or count sketch andB = (AAT )tA. Let us takeB instead of A as the input of the prototype algorithm 1 and obtain the approximate left singular vectors Ũk. It is easy to verify that Ũk is the same to the output of Algorithm 2. We will show that when t = O( logn ǫ ), ∥ ∥A− ŨkŨkA ∥ ∥ 2 2 ≤ (1 + ǫ)‖A−Ak‖2. (10.3) To show this result, we need the lemma of Halko et al. [2011].",
      "startOffset" : 8,
      "endOffset" : 502
    }, {
      "referenceID" : 106,
      "context" : "It turns out that the Krylov subspace method converges much faster than the power iteration [Saad, 2011].",
      "startOffset" : 92,
      "endOffset" : 104
    }, {
      "referenceID" : 106,
      "context" : "In practice, re-orthogonalization or partial re-orthogonalization are employed to prevent the instability from happening [Saad, 2011].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 94,
      "context" : "Very recently, Musco and Musco [2015] showed that with t = logn √ ǫ power iteration, the 1+ǫ spectral norm bound (10.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 120,
      "context" : "For example, spectral clustering, KPCA, Isomap [Tenenbaum et al., 2000], and Laplacian eigenmaps [Belkin and Niyogi, 2003] compute the",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : ", 2000], and Laplacian eigenmaps [Belkin and Niyogi, 2003] compute the",
      "startOffset" : 33,
      "endOffset" : 58
    }, {
      "referenceID" : 54,
      "context" : "proposed by Halko et al. [2011] for approximating symmetric matrix.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 54,
      "context" : "proposed by Halko et al. [2011] for approximating symmetric matrix. Wang et al. [2014a] showed that by randomly samplingO(k/ǫ) columns of K to form C by a certain algorithm, the approximation is high accurate: ∥ ∥K−CXC ∥ ∥ 2 F ≤ (1 + ǫ) ∥ ∥K−Kk ∥ ∥ 2 F .",
      "startOffset" : 12,
      "endOffset" : 88
    }, {
      "referenceID" : 54,
      "context" : "proposed by Halko et al. [2011] for approximating symmetric matrix. Wang et al. [2014a] showed that by randomly samplingO(k/ǫ) columns of K to form C by a certain algorithm, the approximation is high accurate: ∥ ∥K−CXC ∥ ∥ 2 F ≤ (1 + ǫ) ∥ ∥K−Kk ∥ ∥ 2 F . This upper bound matches the lower bound c ≥ 2k/ǫ up to a constant factor [Wang et al., 2014a]. Unfortunately, the prototype algorithm has two obvious drawbacks. Firstly, to compute the intersection matrix X⋆, every entry of K must be known. As is discussed, it takes O(n2d) time to form the kernel matrix K. Secondly, the matrix multiplication C†K costs O(n2c) time. In sum, the prototype algorithm costs O(n2c + n2d) time. Although it is substantially faster than the exact solution, the prototype algorithm has the same time complexity as the exact solution. Faster SPSD Matrix Sketching. Since C = KS has much more rows than columns, the optimization problem (10.4) is strongly overdetermined. Wang et al. [2015b] proposed to use sketching to approximately solve (10.",
      "startOffset" : 12,
      "endOffset" : 973
    }, {
      "referenceID" : 131,
      "context" : "Wang et al. [2015b] devised an algorithm that sets p = √ nc/ √ ǫ and very efficiently forms the column selection matrix P; and the following error bound holds with high probability:",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 141,
      "context" : "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al.",
      "startOffset" : 47,
      "endOffset" : 60
    }, {
      "referenceID" : 131,
      "context" : "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al. [2014b] proposed a spectral shifting kernel approximation method.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 131,
      "context" : "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al. [2014b] proposed a spectral shifting kernel approximation method. When the spectrum of K decays slowly, the shifting term helps to improve the approximation accuracy and numerical stability. Wang et al. [2014a] also showed that the spectral shifting approach",
      "startOffset" : 61,
      "endOffset" : 284
    }, {
      "referenceID" : 113,
      "context" : "can be used to improve other kernel approximation models such as the memory efficient kernel approximation (MEKA) model [Si et al., 2014].",
      "startOffset" : 120,
      "endOffset" : 137
    }, {
      "referenceID" : 137,
      "context" : "It is named after its inventor Nyström [1930] and gained its popularity in the machine learning society after its application in Gaussian procession regression [Williams and Seeger, 2001].",
      "startOffset" : 160,
      "endOffset" : 187
    }, {
      "referenceID" : 119,
      "context" : "The Nyström method has been applied to solve million scale kernel methods [Talwalkar et al., 2013].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 129,
      "context" : "The lower bound [Wang and Zhang, 2013] indicates that the Nyström method cannot attain (1+ǫ) relativeerror bound unless it is willing to spend Ω(n2k/ǫ) time.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 95,
      "context" : "The Nyström Method is the most popular kernel approximation approach. It is named after its inventor Nyström [1930] and gained its popularity in the machine learning society after its application in Gaussian procession regression [Williams and Seeger, 2001].",
      "startOffset" : 4,
      "endOffset" : 116
    }, {
      "referenceID" : 47,
      "context" : "Gittens and Mahoney [2013] offered comprehensive error analysis of the Nyström method.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 33,
      "context" : "Several different column selection strategies have been devised, among which the leverage score sampling [Drineas et al., 2008] and the adaptive sampling [Wang and Zhang, 2013, Boutsidis and Woodruff, 2014] attain relative error bounds.",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "An example of Drineas et al. [2008] and Mahoney and Drineas [2009] has well shown this viewpoint; that is, the vector [(1/2)age − (1/ √ 2)height + (1/2)income], the sum of the significant uncorrelated features from a data set of people’s features, is not particularly informative.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 32,
      "context" : "An example of Drineas et al. [2008] and Mahoney and Drineas [2009] has well shown this viewpoint; that is, the vector [(1/2)age − (1/ √ 2)height + (1/2)income], the sum of the significant uncorrelated features from a data set of people’s features, is not particularly informative.",
      "startOffset" : 14,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : ", 2008] and the adaptive sampling [Wang and Zhang, 2013, Boutsidis and Woodruff, 2014] attain relative error bounds. In particular, Boutsidis and Woodruff [2014] showed that with c = O(k/ǫ) columns and r = O(k/ǫ) rows selected by adaptive sampling to form C and R, min X ‖A−CXR‖F ≤ (1 + ǫ)‖A−Ak‖F holds in expectation.",
      "startOffset" : 57,
      "endOffset" : 162
    }, {
      "referenceID" : 114,
      "context" : "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014].",
      "startOffset" : 31,
      "endOffset" : 46
    }, {
      "referenceID" : 114,
      "context" : "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014].",
      "startOffset" : 31,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014]. This approach is very similar to the prototype SPSD matrix approximation method in the previous section, and it costs at least O(mn·min{c, r}) time and requires observing every entry of A.",
      "startOffset" : 70,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014]. This approach is very similar to the prototype SPSD matrix approximation method in the previous section, and it costs at least O(mn·min{c, r}) time and requires observing every entry of A. Apparently, it cannot help speed up matrix computation. Wang et al. [2015a] proposed a more practical CUR decomposition method which solves (10.",
      "startOffset" : 70,
      "endOffset" : 366
    } ],
    "year" : 2015,
    "abstractText" : "The singular value decomposition (SVD) is not only a classical theory in matrix computation and analysis, but also is a powerful tool in machine learning and modern data analysis. In this tutorial we first study the basic notion of SVD and then show the central role of SVD in matrices. Using majorization theory, we consider variational principles of singular values and eigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss unitarily invariant norms, which are then used to formulate general results for matrix low rank approximation. We study the subdifferentials of unitarily invariant norms. These results would be potentially useful in many machine learning problems such as matrix completion and matrix data classification. Finally, we discuss matrix low rank approximation and its recent developments such as randomized SVD, approximate matrix multiplication, CUR decomposition, and Nyström approximation. Randomized algorithms are important approaches to large scale SVD as well as fast matrix computations.",
    "creator" : "LaTeX with hyperref package"
  }
}