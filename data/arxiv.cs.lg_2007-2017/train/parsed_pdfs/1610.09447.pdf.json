{
  "name" : "1610.09447.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Asynchronous Stochastic Block Coordinate Descent with Variance Reduction",
    "authors" : [ "Bin Gu", "Zhouyuan Huo", "Heng Huang" ],
    "emails" : [ "jsgubin@gmail.com", "zhouyuan.huo@mavs.uta.edu", "heng@uta.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n09 44\n7v 3\n[ cs\n.L G\n] 1\nKeywords: stochastic optimization, block coordinate descent, parallel computing, lockfree\n1. Introduction\nStochastic optimization technologies in theory and practice are emerging recently due to the demand of handling large scale data. Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems. Also, stochastic coordinate descent (SCD) algorithms (Takác, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed. For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014). Basically, these algorithms are sequential algorithms which can not be directly used in parallel environment.\nTo scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.\nAmong these asynchronous parallel implementations, the ones with lock-free are more efficient than the ones with writing or reading lock, because they can achieve near-linear speedup which is the ultimate goal of the parallel computation. In these paper, we focus on the asynchronous parallel implementations with lock-free.\nThere have been several asynchronous parallel implementations for the stochastic optimization algorithms which are totally free of reading and writing locks. For example, Zhao and Li (2016) proposed an asynchronous parallel algorithm for SVRG and proved the linear convergence. Liu and Wright (2015) proposed an asynchronous parallel algorithm for SCD and proved the linear convergence. Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG. Huo and Huang (2016a) proposed an asynchronous SGD algorithm with variance reduction on non-convex optimization problems and proved the convergence. Lian et al. (2016) proposed an asynchronous stochastic optimization algorithm with zeroth order and proved the convergence.\nIn this paper, we focus on a composite objective function as follows.\nmin x∈Rn F (x) = f(x) + g(x) (1)\nwhere f(x) = 1l ∑l i=1 fi(x), fi : R n 7→ R is a smooth convex function. g : Rn 7→ R ∪ {∞} is a block separable, closed, convex, and extended real-valued function. Given a partition {G1, · · · ,Gk} of n coordinates of x, we can write g(x) as g(x) = ∑k j=1 gGj (xGj ). The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al., 2013), and so on. Note that, Liu and Wright (2015) consider the formulation (1) with the constraints that |Gj | = 1 for all j. Thus, the formulation in (Liu and Wright, 2015) is a special case of (1). Each iteration in (Liu and Wright, 2015) only modifies a single component of x which is an atomic operation in the parallel system with shared memory. However, we need to modify a block coordinate Gj of x with lockfree for each iteration, which are more complicated in the asynchronous parallel analysis than the atomic updating in (Liu and Wright, 2015). Due to the complication induced by the block representation of g(x), there have been no asynchronous stochastic block coordinate descent algorithm with lock-free proposed for handle formulation (1), especially on the theoretical analysis.\nIn this paper, we propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lockfree in the implementation and analysis. We prove that AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained.\nAsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. In the big data era, both of the sample size and dimension could be huge at the same time as modern data collection technologies evolve, which demands that the learning algorithms can process large scale datasets with large sample size and high dimension. AsySBCDVR is based on a doubly stochastic scheme which randomly\nchoose a set of samples and a block coordinate for each iteration. Thus, AsySBCDVR can process large scale datasets. Especially, the technology of variance reduction is used to accelerate AsySBCDVR such that AsySBCDVR has the linear or sublinear convergence in different conditions. Thus, AsySBCDVR can scale well with the sample size and dimension simultaneously.\nWe organize the rest of the paper as follows. In Section 2, we give some preliminaries. In section 3, we propose our AsySBCDVR algorithm. In Section 4, we prove the convergence rate for AsySBCDVR. Finally, we give some concluding remarks in Section 5.\n2. Preliminaries\nIn this section, we introduce the condition of optimal strong convexity and three different Lipschitz constants and give the corresponding assumptions, which are critical to the analysis of AsySBCDVR. Optimal Strong Convexity: Let F ∗ denote the optimal value of (1), and let S denote the solution set of F such that F (x) = F ∗, ∀x ∈ S. Firstly, we assume that S is nonempty (i.e., Assumption 1), which is reasonable to (1).\nAssumption 1 The solution set S of (1) is nonempty.\nBased on S, we define PS(x) = argminy∈S ‖y − x‖2 as the Euclidean-norm projection of a vector x onto S. Then, we assume that the convex function f is with the optimal strong convexity (i.e., Assumption 2).\nAssumption 2 (Optimal strong convexity) The convex function f has the condition of optimal strong convexity with parameter l > 0 with respect to the optimal set S, which means that, ∃l such that, ∀x, we have\nF (x)− F (PS(x)) ≥ l\n2 ‖x−PS(x)‖2 (2)\nAs mentioned in Liu and Wright (2015), the condition of optimal strong convexity is significantly weaker than the normal strong convexity condition. And several examples of optimally strongly convex functions that are not strongly convex are provided in (Liu and Wright, 2015). Lipschitz Smoothness: Let ∆j denote the zero vector in R\nn except that the block coordinates indexed by the set Gj. We define the normal Lipschitz constant (Lnor), block restricted Lipschitz constant (Lres) and block coordinate Lipschitz constant (Lmax) as follows.\nDefinition 1 (Normal Lipschitz constant) Lnor is the normal Lipschitz constant for ∇fi (∀i ∈ {1, · · · , l}) in (1), such that, ∀x and ∀y, we have\n‖∇fi(x)−∇fi(y)‖ ≤ Lnor‖x− y‖ (3)\nDefinition 2 (Block Restricted Lipschitz constant) Lres is the block restricted Lipschitz constant for ∇fi (∀i ∈ {1, · · · , l}) in (1), such that, ∀x, and ∀j ∈ {1, · · · , k}, we have\n‖∇fi(x+∆j)−∇fi(x)‖ ≤ Lres ∥∥(∆j)Gj ∥∥ (4)\nDefinition 3 (Block Coordinate Lipschitz constant) Lmax is the block coordinate Lipschitz constant for ∇fi (∀i ∈ {1, · · · , l}) in (1), such that, ∀x, and ∀j ∈ {1, · · · , k}, we have\nmax j=1,··· ,k\n‖∇fi(x+∆j)−∇fi(x)‖ ≤ Lmax ∥∥(∆j)Gj ∥∥ (5)\n(5) is equvilent to the formulation (6).\nfi(x+∆j) ≤ fi(x) + 〈∇Gjfi(x), (∆j)Gj 〉+ Lmax\n2\n∥∥(∆j)Gj ∥∥2 (6)\nBased on Lnor Lres and Lmax as defined above, we assume that the function fi (∀i ∈ {1, · · · , l} is Lipschitz smooth with Lnor Lres and Lmax (i.e., Assumption 3). In addition, we define Λres =\nLres Lmax , Λnor = Lnor Lmax .\nAssumption 3 (Lipschitz smoothness) The function fi (∀i ∈ {1, · · · , l} is Lipschitz smooth with the normal Lipschitz constant Lnor, block restricted Lipschitz constant Lres and block coordinate Lipschitz constant Lmax.\n3. Algorithm\nIn this section, we propose our AsySBCDVR. AsySBCDVR is designed for the parallel environment with shared memory, such as multi-core processors and GPU-accelerators, but it can also work in the parallel environment with distributed memory.\nIn the parallel environment with shared memory, all cores in CPU or GPU can read and write the vector x in the shared memory simultaneously without any lock. Besides randomly choosing a sample set and a block coordinate, AsySBCDVR is also accelerated by the variance reduction. Thus, AsySBCDVR has two-layer loops. The outer layer is to parallelly compute the full gradient ∇f(xs) = 1l ∑l i=1∇fi(xs), where the superscript s denotes the s-th outer loop. The inner layer is to parallelly and repeatedly update the vector x in the shared memory. Specifically, all cores repeat the following steps independently and concurrently without any lock:\n1. Read: Read the vector x from the shared memory to the local memory without reading lock. We use x̂s+1t to denote its value, where the subscript t denotes the t-th inner loop.\n2. Compute: Randomly choose a mini-batch B and a block coordinate j from {1, ..., k}, and locally compute v̂s+1Gj = 1 |B| ∑ i∈B ∇Gjfi(x̂s+1t )− 1|B| ∑ i∈B ∇Gjfi(x̃s) +∇Gjf(x̃s).\n3. Update: Update the block j of the vector x in the shared memory as (xs+1t+1 )Gj ← PGj , γLmax gj ( (xs+1t )Gj − γLmax v̂ s+1 Gj ) without writing lock.\nThe detailed description of AsySBCDVR is presented in Algorithm 1. Note that v̂s+1Gj computed locally is the approximation of ∇Gjf(x̂s+1t ), and the expectation of v̂s+1t on B is equal to ∇f(x̂s+1t ) as follows.\nEv̂s+1t = E\n( 1\n|B| ∑\ni∈B\n∇fi(x̂s+1t )− 1 |B| ∑\ni∈B\n∇fi(x̃s) +∇f(x̃s) )\n(7)\n= ∇f(x̂s+1t )−∇f(x̃s) +∇f(x̃s) = ∇f(x̂s+1t )\nThus, v̂s+1t is called a stochastic gradient of f(x) at x̂ s+1 t .\nBecause AsySBCDVR does not use the reading lock, the vector x̂s+1t read into the local memory may be inconsistent to the vector xs+1t in the shared memory, which means that some components of x̂s+1t are same with the ones in x s+1 t , but others are different to the ones in xs+1t . However, we can define a set K(t) of inner iterations, such that,\nxs+1t = x̂ s+1 t +\n∑\nt′∈K(t)\nBs+1t′ ∆ s+1 t′ (8)\nwhere t′ ≤ t − 1, (∆s+1t′ )Gj(t′) = PGj(t′), γLmax gj(t′) ( (xs+1t′ )Gj(t′) − γ Lmax v̂s+1t′,Gj(t′) ) − (xs+1t′ )Gj(t′) , (∆s+1t′ )\\Gj(t′) = 0, B s+1 t′ is a diagonal matrix with diagonal entries either 1 or 0. It is reasonable to assume that Bst′ 6= 0, ∀t′ ∈ K(t) (i.e., Assumption 4), and there exists an upper bound τ such that τ ≥ t−min{t′|t′ ∈ K(t)} (i.e., Assumption 5).\nAssumption 4 (Non zero of Bst′) For all inner iterations t in AsySBCDVR, ∀t′ ∈ K(t), we have that Bst′ 6= 0.\nAssumption 5 (Bound of delay) There exists a upper bound τ such that τ ≥ t−min{t′|t′ ∈ K(t)} for all inner iterations t in AsySBCDVR.\nAlgorithm 1 Asynchronous Stochastic Block Coordinate Descent with Variance Reduction (AsySBCDVR)\nInput: γ, S, and m. Output: xS. 1: Initialize x0 ∈ Rd, p threads. 2: for s = 0, 1, 2, S − 1 do 3: x̃s ← xs 4: All threads parallelly compute the full gradient ∇f(x̃s) = 1l ∑l i∇fi(x̃s)\n5: For each thread, do: 6: for t = 0, 1, 2,m − 1 do 7: Randomly sample a mini-batch B from {1, ..., l} with equal probability. 8: Randomly choose a block j(t) from {1, ..., k} with equal probability. 9: Compute v̂s+1Gj(t) = 1 |B| ∑ i∈B ∇Gj(t)fi(x̂s+1t )− 1|B| ∑ i∈B ∇Gj(t)fi(x̃s) +∇Gj(t)f(x̃s).\n10: (xs+1t+1 )Gj(t) ← PGj(t), γLmax gj(t) ( (xs+1t )Gj(t) − γLmax v̂ s+1 t,Gj(t) ) . 11: (xs+1t+1 )\\Gj(t) ← (xs+1t )\\Gj(t) . 12: end for 13: xs+1 ← xs+1m 14: end for\n4. Convergence Analysis\nIn this section, we follow the analysis of (Liu and Wright, 2015) and prove the convergence rate of AsyDSCDVR (Theorem 8). Specifically, AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity.\nBefore providing the theoretical analysis, we give the definitions of x̂st,t′+1, x s t+1 and the\nexplanation of xst used in the analysis as follows.\n1. x̂st,t′ : Assume the indices in K(t) are sorted in the increasing order, we use K(t)t′ to denote the t′-th index in K(t). For t′ = 0, 1, · · · , |K(t)|, we define\nx̂st,t′ = x̂ s t +\nt′∑\nt′′=1\n( BsK(t)t′′ ∆sK(t)t′′ ) = x̂st + t′∑\nt′′=1\n( xK(t)t′′+1 − xK(t)t′′ ) ) (9)\nThus, we have that\nx̂st = x̂ s t,0 (10) xst = x̂ s t,|K(t)| (11)\nxst − x̂st = |K(t)|−1∑\nt′′=0\n( Bs+1K(t)t′′ ∆s+1K(t)t′′ ) = |K(t)|−1∑\nt′=0\n( x̂st,t′+1 − x̂st,t′ ) (12)\n∇f(xst)−∇f(x̂st) = |K(t)|−1∑\nt′=0\n( ∇f(x̂st,t′+1)−∇f(x̂t,t′) ) (13)\n2. xst+1: x s t+1 is defined as:\nxst+1 def = P γ\nLmax g\n( xst − γ\nLmax v̂st\n) (14)\nBased on (14), it is easy to verify that (xst+1)Gj(t) = (x s+1 t+1 )Gj(t) . Thus, we have Ej(t)(x s t+1 − xst ) = 1k ( xst+1 − xst ) . It means that xst+1 − xst captures the expectation of xst+1 − xst .\n3. xst : As mentioned previously, AsySBCDVR does not use any locks in the reading and writing. Thus, in the line 10 of Algorithm 1, xst (left side of ‘←’) updated in the shared memory may be inconsistent with the idea one (right side of ‘←’) computed by the proximal operator. In the analysis, we use xst to denote the idea one computed by the proximal operator. Same as mentioned in (Mania et al., 2015), there might not be an actual time the idea ones exist in the shared memory, except the first and last iterates for each outer loop. It is noted that, xs0 and x s m are exactly what is stored in\nshared memory. Thus, we only consider the idea xst in the analysis.\nThen, we give two inequalities in Lemma 4 and 5 respectively. Based on Lemma 4 and 5, we prove that E‖xst−1 − xst‖2 ≤ ρE‖xst − xst+1‖2 (Lemma 6), where ρ > 1 is a user defined parameter. Then, we prove the monotonicity of the expectation of the objectives EF (xst+1) ≤ EF (xst ) (Lemma 7). Note that the analyses only consider the case |B| = 1 without loss of generality. The case of |B| > 1 can be proved similarly.\nLemma 4 For ‖∇f(xst )−∇f(x̂st)‖ in each iteration of AsySBCDVR, we have its upper bound as\n‖∇f(xst)−∇f(x̂st)‖ ≤ Lres ∑\nt′∈K(t)\n‖∆st′‖ (15)\nProof Based on , we have that\n‖∇f(xst)−∇f(x̂st)‖ = ∥∥∥∥∥∥ |K(t)|−1∑\nt′=0\n∇f(x̂st,t′+1)−∇f(x̂st,t′) ∥∥∥∥∥∥ (16)\n≤ |K(t)|−1∑\nt′=0\n∥∥∇f(x̂st,t′+1)−∇f(x̂st,t′) ∥∥ ≤ Lres |K(t)|−1∑\nt′=0\n∥∥x̂st,t′+1 − x̂st,t′ ∥∥\n= Lres\n|K(t)|−1∑\nt′=0\n∥∥∥BsK(t)t′∆ s K(t)t′ ∥∥∥ ≤ Lres |K(t)|−1∑\nt′=0\n∥∥∥BsK(t)t′ ∥∥∥ ∥∥∥∆sK(t)t′ ∥∥∥ ≤ Lres ∑\nt′∈K(t)\n‖∆st′‖\nThis completes the proof.\nLemma 5 In each iteration of AsySBCDVR, ∀x, we have the following inequality. 〈 (v̂st )Gj(t) + Lmax\nγ ∆st , (x s t+1 − x)Gj(t)\n〉 + gGj(t) ( (xst+1)Gj(t) ) − gGj(t) ( (x)Gj(t) ) ≤ 0 (17)\nProof The problem solved in lines 8 of Algorithm 1 is as follows\nxst+1 = argminx\n〈 (v̂st )Gj(t) , (x− xst)Gj(t) 〉 + Lmax\n2γ\n∥∥∥(x− xst )Gj(t) ∥∥∥ 2\n(18)\n+gGj(t) ( (x)Gj(t) )\ns.t. x\\Gj(t) = (x s t )\\Gj(t)\nIf xst+1 is the solution of (18), the solution of optimization problem (19) is also x s t+1 according to the subdifferential version of Karush-Kuhn-Tucker (KKT) conditions (Ruszczyński., 2006).\nP (x) = min x\n〈 (v̂st )Gj(t) + Lmax\nγ\n( xst+1 − xst ) Gj(t) , (x− xst)Gj(t) 〉 + gGj(t) ( (x)Gj(t) ) (19)\ns.t. x\\Gj(t) = (x s t )\\Gj(t)\nThus, we have that P (x) ≥ P (xst+1), ∀x, which leads to (17). This completes the proof.\nLemma 6 Let ρ be a constant that satisfies ρ > 1, and define the quantities θ1 = ρ 1 2−ρ τ+1 2\n1−ρ 1 2\nand θ2 = ρ 1 2 −ρ m 2\n1−ρ 1 2\n. Suppose the nonnegative steplength parameter γ > 0 satisfies γ ≤\nmin {\nk1/2(1−ρ−1)−4 4(Λres(1+θ1)+Λnor(1+θ2)) , k 1/2\n1 2 k1/2+2Λnorθ2+Λresθ1\n} , we have\nE‖xst−1 − xst‖2 ≤ ρE‖xst − xst+1‖2 (20)\nProof According to (A.8) in Liu and Wright (2015), we have\n‖xst−1 − xst‖2 − ‖xst − xst+1‖2 ≤ 2‖xst−1 − xst‖‖xst − xst+1 − xst−1 + xst‖ (21)\nThe second part in the right half side of (21) is bound as follows if B = {it} and J(t) = {j(t)}.\n‖xst − xst+1 − xst−1 + xst‖ (22)\n= ∥∥∥∥x s t − P γLmax g ( xst − γ Lmax v̂st ) − xst−1 + P γLmax g ( xst−1 − γ Lmax v̂st−1 )∥∥∥∥ ≤ ‖xst − xst−1‖+ ∥∥∥∥P γLmax g ( xst − γ Lmax v̂st ) − P γ Lmax g ( xst−1 − γ Lmax v̂st−1 )∥∥∥∥ ≤ 2‖xst − xst−1‖+ γ\nLmax\n∥∥v̂st − v̂st−1 ∥∥\n= 2‖xst − xst−1‖+ γ\nLmax\n∥∥∇fit(x̂st )−∇fit(x̃s) +∇f(x̃s)−∇fit−1(x̂st−1) +∇fit−1(x̃s)−∇f(x̃s) ∥∥\n= 2‖xst − xst−1‖+ γ\nLmax\n∥∥∇fit(x̂st )−∇fit(x̃s)−∇fit−1(x̂st−1) +∇fit−1(x̃s) ∥∥\n≤ 2‖xst − xst−1‖+ γ\nLmax ‖∇fit(x̂st )−∇fit(xst ) +∇fit(xst )−∇fit(x̃s)‖\n+ γ\nLmax\n∥∥∇fit−1(x̂st−1)−∇fit−1(xst−1) +∇fit−1(xst−1)−∇fit−1(x̃s) ∥∥\n≤ 2‖xst − xst−1‖+ γ\nLmax ‖∇fit(x̂st )−∇fit(xst )‖+\nγ\nLmax ‖∇fit(xst )−∇fit(x̃s)‖\n+ γ\nLmax\n∥∥∇fit−1(x̂st−1)−∇fit−1(xst−1) ∥∥+ γ\nLmax\n∥∥∇fit−1(xst−1)−∇fit−1(x̃s) ∥∥\n≤ 2‖xst − xst−1‖+ γΛres\n  ∑\nt′∈K(t−1)\n‖∆st′‖+ ∑\nt′∈K(t)\n‖∆st′‖\n \n+γΛnor ( ‖xst − x̃s‖+ ‖xst−1 − x̃s‖ )\n≤ 2‖xst − xst−1‖+ 2γ ( Λres t−1∑\nt′=t−1−τ\n‖∆st′‖+ Λnor t−1∑\nt′=0\n‖∆st′‖ )\nwhere the first inequality use the nonexpansive property of P γ Lmax g, the fifth inequality use\nA.7 of Liu and Wright (2015), the sixth inequality comes from ‖xst − x̃s‖ = ‖ ∑t−1\nt′=0∆ s t′‖ ≤∑t−1\nt′=0 ‖∆st′‖. If t = 1, we have that K(0) = ∅ and K(1) ⊆ {0}. Thus, according to (22), we have\n‖xs1 − xs2 − xs0 + xs1‖ ≤ 2‖xs1 − xs0‖+ 2γ (Λres + Λnor) ‖∆s0‖ (23)\nSubstituting (23) into (21), and takeing expectations, we have\nE‖xs0 − xs1‖2 − E‖xs1 − xs2‖2 ≤ 2E (‖xs0 − xs1‖‖xs1 − xs2 − xs0 + xs1‖) (24) ≤ 4E (‖xs0 − xs1‖‖xs1 − xs0‖) + 4γ (Λres + Λnor)E (‖xs0 − xs1‖‖∆s0‖) ≤ 4k− 12E ( ‖xs0 − xs1‖2 ) + 4γ (Λres + Λnor)E (‖xs0 − xs1‖‖∆s0‖)\nwhere the last inequality uses A.13 in (Liu and Wright, 2015). Further, we have the upper bound of E ( ‖xst − xst+1‖‖∆st‖ ) as\nE ( ‖xst − xst+1‖‖∆st‖ ) ≤ 1\n2 E\n( k− 1 2 ‖xst − xst+1‖2 + k 1 2 ‖∆st‖2 ) (25)\n= 1\n2 E\n( k− 1 2 ‖xst − xst+1‖2 + k 1 2Ej(t)‖∆st‖2 ) = 1\n2 E\n( k− 1 2‖xst − xst+1‖2 + k− 1 2E‖xst − xst+1‖2 )\n= k− 1 2E‖xst − xst+1‖2\nSubstituting (25) into (24), we have\nE‖xs0 − xs1‖2 − E‖xs1 − xs2‖2 ≤ k− 1 2 (4 + 4γ (Λres + Λnor))E ( ‖xs0 − xs1‖2 ) (26)\nwhich implies that\nE‖xs0 − xs1‖2 ≤ ( 1− 4 + 4γ (Λres + Λnor)√\nk\n)−1 E‖xs1 − xs2‖2 ≤ ρE‖xs1 − xs2‖2 (27)\nwhere the last inequality follows from . Thus, we have (20) for t = 1.\nρ−1 ≤ 1− 4 + 4γ (Λres + Λnor)√ k ⇔ γ ≤ k 1/2(1− ρ−1)− 4 4 (Λres + Λnor)\n(28)\nNext, we consider the cases for t > 1. For t− 1− τ ≤ t′ ≤ t− 1 and any β > 0, we have\nE ( ‖xst − xst+1‖‖∆st′‖ ) ≤ 1\n2 E\n( k−\n1 2β−1‖xst − xst+1‖2 + k 1 2β‖∆st′‖2\n) (29)\n= 1\n2 E\n( k− 1 2β−1‖xst − xst+1‖2 + k 1 2βEj(t)‖∆st′‖2 )\n= 1\n2 E\n( k− 1 2β−1‖xst − xst+1‖2 + k− 1 2βE‖xst′ − xst′+1‖2 )\n≤ 1 2 E\n( k−\n1 2β−1‖xst − xst+1‖2 + k− 1 2 ρt−t ′ βE‖xst − xst+1‖2 )\nβ=ρ t′−t 2\n≤ k− 12ρ t−t ′\n2 E‖xst − xst+1‖2\nWe assume that (20) holds ∀t′ < t. By substituting (22) into (21) and taking expectation on both sides of (21), we can have\nE ( ‖xst−1 − xst‖2 − ‖xst − xst+1‖2 ) (30)\n≤ 2E ( ‖xst−1 − xst‖‖xst − xt+1 − xst−1 + xst‖ ) ≤ 2E ( ‖xst−1 − xst‖ ( 2‖xst − xst−1‖+ 2γ ( Λres t−1∑\nt′=t−1−τ\n‖∆st′‖+ Λnor t−1∑\nt′=0\n‖∆st′‖ )))\n= 4E ( ‖xst−1 − xst‖‖xst−1 − xst‖ ) +\n4γE ( Λres t−1∑\nt′=t−1−τ\n‖xst−1 − xst‖‖∆st′‖+ Λnor t−1∑\nt′=0\n‖xst−1 − xst‖‖∆st′‖ )\n≤ 4k−1/2E ( ‖xst−1 − xst‖2 ) +\n4γk−1/2E ( ‖xst−1 − xst‖2 ) · ( Λres t−1∑\nt′=t−1−τ\nρ t−1−t′\n2 + Λnor\nt−1∑\nt′=0\nρ t−1−t′ 2\n)\n= (4 + 4γ (Λres + Λnor))k −1/2 E ( ‖xst−1 − xst‖2 ) +\n4γk −1 2 E ( ‖xst−1 − xst‖2 ) ( Λres τ∑\nt′=1\nρ t′\n2 + Λnor\nt−1∑\nt′=1\nρ t′ 2\n)\n= k−1/2E ( ‖xst−1 − xst‖2 ) ( 4 + 4γΛres ( 1 +\nρ 1 2 − ρ τ+12 1− ρ 12\n) + 4γΛnor ( 1 +\nρ 1 2 − ρm2 1− ρ 12\n))\n= k−1/2E ( ‖xst−1 − xst‖2 ) · (4 + 4γ (Λres (1 + θ1) + Λnor (1 + θ2)))\nwhere the third inequality uses (29). Based on (30), we have that\nE ( ‖xst−1 − xst‖2 ) (31)\n≤ ( 1− k−1/2 (4 + 4γ (Λres (1 + θ1) + Λnor (1 + θ2))) )−1 · E ( ‖xst − xst+1‖2 ) ≤ ρE ( ‖xst − xst+1‖2 )\nwhere the last inequality follows from\nρ−1 ≤ 1− k−1/2 (4 + 4γ (Λres (1 + θ1) + Λnor (1 + θ2))) (32)\n⇔ γ ≤ k 1/2(1− ρ−1)− 4\n4 (Λres (1 + θ1) + Λnor (1 + θ2))\nThis completes the proof.\nLemma 7 Let ρ be a constant that satisfies ρ > 1, and define the quantities θ1 = ρ 1 2−ρ τ+1 2\n1−ρ 1 2\nand θ2 = ρ 1 2 −ρ m 2\n1−ρ 1 2\n. Suppose the nonnegative steplength parameter γ > 0 satisfies γ ≤\nmin {\nk1/2(1−ρ−1)−4 4(Λres(1+θ1)+Λnor(1+θ2)) , k 1/2\n1 2 k1/2+2Λnorθ2+Λresθ1\n} . The expectation of the objective func-\ntion EF (xst ) is monotonically decreasing, i.e., EF (x s t+1) ≤ EF (xst ).\nProof Take expectation F (xst+1) on j(t), we have that\nEj(t)F (x s t+1) = Ej(t) ( f(xst +∆ s t) + g(x s t+1) ) (33)\n≤ Ej(t) ( f(xst ) + 〈 ∇Gj(t)f(xst), (∆st )Gj(t) 〉 + Lmax\n2\n∥∥∥(∆st )Gj(t) ∥∥∥ 2\n+gGj(t) ( (xst+1)Gj(t) ) + ∑\nj′ 6=j(t)\ngGj′ ( (xst+1)Gj′\n)  \n= f(xst) + k − 1 k g(xst ) + Ej(t) (〈 ∇Gj(t)f(xst), (∆st )Gj(t) 〉 + Lmax 2 ∥∥∥(∆st )Gj(t) ∥∥∥ 2\n+gGj(t) ( (xst+1)Gj(t) ))\n= F (xst ) + Ej(t) (〈 (v̂st )Gj(t) , (∆ s t )Gj(t) 〉 + Lmax\n2\n∥∥∥(∆st )Gj(t) ∥∥∥ 2 + gGj(t) ( (xst+1)Gj(t) )\n−gGj(t) ( (xst )Gj(t) ) + 〈 ∇Gj(t)f(xst )− (v̂st )Gj(t) , (∆st )Gj(t) 〉)\n≤ F (xst ) + Ej(t) ( −Lmax\nγ\n∥∥∥(∆st )Gj(t) ∥∥∥ 2 + Lmax\n2\n∥∥∥(∆st )Gj(t) ∥∥∥ 2 +\n+ 〈 ∇Gj(t)f(xst )− (v̂st )Gj(t) , (∆st )Gj(t) 〉)\n= F (xst ) + Ej(t)\n(( Lmax\n2 − Lmax γ\n)∥∥∥(∆st )Gj(t) ∥∥∥ 2 ) + Ej(t) 〈 ∇Gj(t)f(xst)− (v̂st )Gj(t) , (∆st )Gj(t) 〉\n= F (xst ) + Lmax\nk\n( 1\n2 − 1 γ\n) ‖xst+1 − xst‖2 + Ej(t) 〈 ∇Gj(t)f(xst)− (v̂st )Gj(t) , (∆st )Gj(t) 〉\nwhere the first inequality uses (6), and the second inequality uses (17) in Lemma 5. Consider the expectation of the last term on the right-hand side of (33), we have\nE 〈 ∇Gj(t)f(xst )− (v̂st )Gj(t) , (∆st )Gj(t) 〉 (34)\n= E 〈 ∇Gj(t)f(xst )− (∇fit(x̂st )−∇fit(x̃s) +∇f(x̃s))Gj(t) , (∆ s t )Gj(t) 〉 = E 〈 ∇Gj(t)f(xst )− (∇fit(x̂st )−∇fit(xst ) +∇fit(xst )−∇fit(x̃s) +∇f(x̃s))Gj(t) , (∆st )Gj(t) 〉 = E 〈 ∇Gj(t)f(xst )−∇Gj(t)f(x̃s), (∆st )Gj(t) 〉 + E 〈 ∇Gj(t)fit(xst )−∇Gj(t)fit(x̂st ), (∆st )Gj(t) 〉\n+E 〈 ∇Gj(t)fit(x̃s)−∇Gj(t)fit(xst ), (∆st )Gj(t) 〉\n≤ E (∥∥∥∇Gj(t)f(xst )−∇Gj(t)f(x̃s) ∥∥∥ ‖∆st‖ ) + E (∥∥∥∇Gj(t)fit(xst )−∇Gj(t)fit(x̂st ) ∥∥∥ ‖∆st‖ )\n+E (∥∥∥∇Gj(t)fit(x̃s)−∇Gj(t)fit(xst ) ∥∥∥ ‖∆st‖ )\n= 1 k E ( ‖∇f(xst)−∇f(x̃s)‖ ∥∥xst+1 − xst ∥∥)+ 1 k E ( ‖∇fit(xst )−∇fit(x̂st )‖ ∥∥xst+1 − xst ∥∥)\n+ 1 k E ( ‖∇fit(x̃s)−∇fit(xst )‖ ∥∥xst+1 − xst ∥∥)\n≤ 1 k E\n 2Lnor‖xst − x̃s‖‖xst+1 − xst‖+ Lres ∑\nt′∈K(t)\n‖∆st′‖ ‖xst+1 − xst‖\n \n≤ 1 k E\n( 2Lnor t−1∑\nt′=0\n‖∆st′‖‖xst+1 − xst‖+ Lres t−1∑\nt′=t−τ\n‖∆st′‖‖xst+1 − xst‖ )\n≤ 2Lnor t−1∑\nt′=0\nρ t−t′ 2 k3/2 E‖xst+1 − xst‖2 + Lres t−1∑\nt′=t−τ\nρ t−t′ 2 k3/2 E‖xst+1 − xst‖2\n= k−3/2 ( 2Lnor\nρ 1 2 − ρm2 1− ρ 12 + Lres ρ 1 2 − ρ τ+12 1− ρ 12\n) · E‖xst+1 − xst‖2\n= k−3/2 (2Lnorθ2 + Lresθ1)E‖xst+1 − xst‖2\nwhere the first inequality uses the Cauchy-Schwarz inequality (Callebaut, 1965), the third inequality uses (3) and (4), the sixth inequality uses (29).\nBy taking expectations on both sides of (33) and substituting (34), we have\nEF (xst+1) (35)\n≤ EF (xst ) + Lmax\nk\n( 1\n2 − 1 γ\n) E‖xst+1 − xst‖2 + E 〈 ∇Gj(t)f(xst)− (v̂st )Gj(t) , (∆st )Gj(t) 〉\n≤ EF (xst )− 1 k · ( Lmax ( 1 γ − 1 2 ) − 2Lnorθ2 + Lresθ1 k1/2 ) E‖xst+1 − xst‖2\nwhere Lmax ( 1 γ − 12 ) − 2Lnorθ2+Lresθ1 k1/2 ≥ 0 because that γ−1 ≥ 12 + 2Λnorθ2+Λresθ1k1/2 . This completes the proof.\nTheorem 8 Let ρ be a constant that satisfies ρ > 1, and define the quantity θ′ = ρ τ+1−ρ ρ−1 . Suppose the nonnegative steplength parameter γ > 0 satisfies 1−Λnorγ−γτθ ′ n − 2(Λresθ1+Λnorθ2)γ n1/2 ≥ 0. If the optimal strong convexity holds for f with l > 0, we have\nEF (xs)− F ∗ ≤ Lmax 2γ\n( 1\n1 + 2mγl2k(lγ+Lmax)\n)s · ( ‖x0 − PS(x0)‖2 + 2γ\nLmax\n( EF (x0)− F ∗ )) (36)\nIf f is a general smooth convex function, we have\nEF (xs)− F ∗ ≤ nLmax‖x 0 − PS(x0)‖2 + 2γk\n( F (x0)− F ∗ )\n2γk + 2mγs (37)\nProof We have that\n‖xst+1 −PS(xst+1)‖2 ≤ ‖xst+1 − PS(xst )‖2 = ‖xst +∆st − PS(xst )‖2 (38) = ‖xst −PS(xst )‖2 − ‖∆st‖2 − 2〈(PS(xst )− xst −∆st )Gj(t) , (∆ s t )Gj(t)〉 = ‖xst −PS(xst )‖2 − ‖∆st‖2 − 2〈 ( PS(xst )− xst+1 ) Gj(t) , (∆st )Gj(t)〉 ≤ ‖xst −PS(xst )‖2 − ‖∆st‖2 + 2γ\nLmax\n( 〈 ( PS(xst )− xst+1 ) Gj(t) , (v̂st )Gj(t)〉 ) +\n2γ\nLmax\n( gGj(t)(PS(xst )Gj(t))− gj(t)(xst+1)Gj(t)) )\n= ‖xst −PS(xst )‖2 − ‖∆st‖2 + 2γ\nLmax\n( 〈(PS(xst )− xst )Gj(t) , (v̂ s t )Gj(t)〉 ) ︸ ︷︷ ︸ T1 +\n2γ\nLmax\n( 〈(∆st)Gj(t) , (v̂ s t )Gj(t)〉 ) ︸ ︷︷ ︸ T2 + 2γ Lmax ( gj(t)(PS(xst )Gj(t))− gGj(t)(xst+1)Gj(t)) )\nwhere the first inequality comes from the definition of function PS(x) = argminy∈S ‖y−x‖2, and the second inequality uses (17) in Lemma 5. For the expectation of T1, we have\nE(T1) = E ( 〈(PS(xst )− xst )Gj(t) , (v̂ s t )Gj(t)〉 ) (39)\n= 1\nk E〈PS(xst )− xst , v̂st 〉\n= 1\nk E〈PS(xst )− xst ,∇fit(x̂st )−∇fit(x̃s) +∇f(x̃s)〉\n= 1\nk E〈PS(xst )− xst ,∇fit(x̂st )〉+\n1 k 〈E(PS(xst )− xst ),E(−∇fit(x̃s) +∇f(x̃s))〉\n= 1\nk E〈PS(xst )− x̂st ,∇fit(x̂st )〉+\n1 k E〈x̂st − xst ,∇fit(x̂st )〉\n= 1\nk E〈PS(xst )− x̂st ,∇fit(x̂st )〉+\n1 k E\n|K(t)|−1∑\nt′=0\n〈x̂st,t′ − x̂st,t′+1,∇fit(x̂st )〉\n= 1\nk E〈PS(xst )− x̂st ,∇fit(x̂st )〉+\n1 k E\n|K(t)|−1∑\nt′=0\n〈x̂st,t′ − x̂st,t′+1,∇fit(x̂st,t′)〉\n+ 1\nk E\n|K(t)|−1∑\nt′=0\n〈x̂st,t′ − x̂st,t′+1,∇fit(x̂st )−∇fit(x̂st,t′)〉\n≤ 1 k E (fit(PS(xst ))− fit(x̂st )) + 1 k E\n|K(t)|−1∑\nt′=0\n〈x̂st,t′ − x̂st,t′+1,∇fit(x̂st )−∇fit(x̂st,t′)〉\n+ 1\nk E\n|K(t)|−1∑\nt′=0\n( fit(x̂ s t,t′)− fit(x̂st,t′+1) + Lmax\n2 ‖x̂st,t′ − x̂st,t′+1‖2\n)\n= 1\nk E (fit(PS(xst ))− fit(xst )) +\n1 k E\n|K(t)|−1∑\nt′=0\n〈x̂st,t′ − x̂st,t′+1,∇fit(x̂st )−∇fit(x̂st,t′)〉\n+ Lmax\n2k E\n|K(t)|−1∑\nt′=0\n( ‖x̂st,t′ − x̂st,t′+1‖2 )\n= 1\nk E (fit(PS(xst ))− fit(xst )) +\nLmax\n2k E\n|K(t)|−1∑\nt′=0\n( ‖x̂st,t′ − x̂st,t′+1‖2 )\n+ 1\nk E\n|K(t)|−1∑\nt′=0\n〈x̂st,t′ − x̂st,t′+1, t′−1∑\nt′′=0\n∇fit(x̂st′′)−∇fit(x̂st,t′′+1)〉\n≤ 1 k E (fit(PS(xst ))− fit(xst )) + Lmax 2k E\n|K(t)|−1∑\nt′=0\n( ‖x̂st,t′ − x̂st,t′+1‖2 )\n+ Lmax\nk E\n|K(t)|−1∑\nt′=0\n( ∥∥x̂st,t′ − x̂st,t′+1 ∥∥ t′−1∑\nt′′=0\n∥∥x̂st′′ − x̂st,t′′+1 ∥∥ )\n= 1\nk E (fit(PS(xst ))− fit(xst )) +\nLmax\n2k E\n|K(t)|−1∑\nt′=0\n( ‖x̂st,t′ − x̂st,t′+1‖2 )\n+ Lmax\n2k E\n    |K(t)|−1∑\nt′=0\n∥∥x̂st,t′ − x̂st,t′+1 ∥∥   2 − |K(t)|−1∑\nt′=0\n( ‖x̂st,t′ − x̂st,t′+1‖2\n)  \n= 1\nk E (fit(PS(xst ))− fit(xst )) +\nLmax\n2k E\n  |K(t)|−1∑\nt′=0\n∥∥x̂st,t′ − x̂st,t′+1 ∥∥   2\n≤ 1 k E (fit(PS(xst ))− fit(xst )) + Lmaxτ 2k E\n|K(t)|−1∑\nt′=0\n∥∥x̂st,t′ − x̂st,t′+1 ∥∥2\n= 1\nk E (fit(PS(xst ))− fit(xst )) +\nLmaxτ\n2k\n|K(t)|−1∑\nt′=0\nE ‖Bst′∆st′‖2\n≤ 1 k E (fit(PS(xst ))− fit(xst )) + Lmaxτ 2k2\n|K(t)|−1∑\nt′=0\nE ∥∥xst′+1 − xst′ ∥∥2\n≤ 1 k E (fit(PS(xst ))− fit(xst )) + Lmaxτ 2k2\nτ∑\nt′=1\nρt ′ E ∥∥xst+1 − xst ∥∥2\n≤ 1 k E (f(PS(xst ))− f(xst)) +\nLmaxτθ ′\n2k2 E(‖xst − xst+1‖2)\nwhere the fifth equality comes from that xst is independent to it, the sixth equality uses Lemma 4, the first inequality uses the convexity of fi and (6), the second inequality uses (5). For the expectation of T2, we have\nE(T2) = E〈(∆st )Gj(t) , (v̂ s t )Gj(t)〉 (40)\n= E〈(∆t)Gj(t) , (∇fit(x̂ s t )−∇fit(x̃s) +∇f(x̃s))Gj(t)〉 = E〈(∆t)Gj(t) , (∇fit(x̂ s t )−∇fit(xst ) +∇fit(xst )−∇fit(x̃s) +∇f(x̃s))Gj(t)〉 = E〈(∆t)Gj(t) , (∇fit(x̂ s t )−∇fit(xst ))Gj(t)〉+\nE〈(∆t)Gj(t) , (∇fit(x s t )−∇fit(x̃s))Gj(t)〉+ E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\n≤ 1 k E ( ‖xst+1 − xst‖ ‖∇fit(x̂st )−∇fit(xst )‖ ) +\n1 k E ( ‖xst+1 − xst‖ ‖∇fit(xst )−∇fit(x̃s)‖ ) + E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\n≤ Lres k\n  ∑\nt′∈K(t)\n‖xst+1 − xst‖‖∆st′‖\n \nLnor k E ( ‖xst+1 − xst‖‖xst − xs‖ ) + E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\n≤ Lres k\n  ∑\nt′∈K(t)\n‖xst+1 − xst‖‖∆st′‖\n \nLnor\nk E\n( t−1∑\nt′=0\n‖xst+1 − xst‖‖∆st′‖ ) + E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\n≤ Lres k3/2\nt−1∑\nt′=t−τ\nρ(t−t ′)/2 E(‖xst − xst+1‖2)\n+ Lnor\nk3/2\nt−1∑\nt′=0\nρ(t−t ′)/2 E(‖xst − xst+1‖2) + E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\n= 1\nk3/2 (Lresθ1 + Lnorθ2)E(‖xst − xst+1‖2) + E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\n≤ Lresθ1 + Lnorθ2 k3/2 E(‖xst − xst+1‖2) + E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\nwhere the second inequality uses Lemma 4, the fourth inequality uses (29). By substituting the upper bounds from (39) and (40) into (38), we have\nE‖xst+1 − PS(xst+1)‖2 (41)\n≤ E‖xst − PS(xst )‖2 − 1\nk E(‖xst − xst+1‖2) +\n2γ\nLmaxk E (f(PS(xst ))− f(xst)) +\nγτθ′\nk2 E(‖xst − xst+1‖2)\n+ 2γ\nLmax\n( Lresθ1 + Lnorθ2\nk3/2 θE(‖xst − xst+1‖2) + E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\n1 k Eg(PS(xst ))− Eg(xst+1) + k − 1 k Eg(xst )\n)\n= E‖xst − PS(xst )‖2 + 2γ\nLmaxk E (f(PS(xst ))− f(xst ))−\n1\nk\n( 1− γτθ ′\nk − 2(Lresθ1 + Lnorθ2)γ\nk1/2Lmax\n) E(‖xst − xst+1‖2) + 2γ\nLmax\n( E〈(∆t)Gj(t) ,∇Gj(t)f(x̃ s)〉\n1 k Eg(PS(xst ))− Eg(xst+1) + k − 1 k Eg(xst )\n)\nWe consider a fixed stage s+ 1 such that xs+10 = x s m. By summing the the inequality (42) over t = 0, · · · ,m− 1, we obtain\nE‖xs+1 − PS(xs+1)‖2 (42)\n≤ E‖xs − PS(xs)‖2 + m−1∑\nt′=0\n2γ Lmaxk E ( f(PS(xs+1t′ ))− f(xs+1t′ ) ) −\nm−1∑\nt′=0\n1\nk\n( 1− γτθ ′\nk − 2(Lresθ1 + Lnorθ2)γ\nk1/2Lmax\n) · E(‖xs+1t′ − xs+1t′+1‖2)\n+ 2γ\nLmax\nm−1∑\nt′=0\nE 〈 (∆t′)Gj(t′) ,∇Gj(t′)f(x̃ s) 〉\n+ 2γ\nLmax\nm−1∑\nt′=0\n( 1\nk Eg(PS(xs+1t′ ))− Eg(xs+1t′+1) + n− 1 k Eg(xs+1t′ )\n)\n= E‖xs − PS(xs)‖2 + m−1∑\nt′=0\n2γ Lmaxk E ( f(PS(xs+1t′ ))− f(xs+1t′ ) ) −\nm−1∑\nt′=0\n1\nk\n( 1− γτθ ′\nk − 2(Lresθ1 + Lnorθ2)γ\nk1/2Lmax\n) · E(‖xs+1t′ − xs+1t′+1‖2)\n+ 2γ Lmax E 〈 xs+1 − xs,∇f(x̃s) 〉\n+ 2γ\nLmax\nm−1∑\nt′=0\n( 1\nk Eg(PS(xs+1t′ ))− Eg(xs+1t′+1) + k − 1 k Eg(xs+1t′ )\n)\n≤ E‖xs − PS(xs)‖2 + m−1∑\nt′=0\n2γ Lmaxk E ( f(PS(xs+1t′ ))− f(xs+1t′ ) ) −\nm−1∑\nt′=0\n1\nk\n( 1− γτθ ′\nk − 2(Lresθ1 + Lnorθ2)γ\nk1/2Lmax\n) · E(‖xs+1t′ − xs+1t′+1‖2)\n+ 2γ\nLmax E\n( f(xs)− f(xs+1) + Lnor\n2 ‖xs+1 − xs‖2\n)\n+ 2γ\nLmax\nm−1∑\nt′=0\n( 1\nk Eg(PS(xs+1t′ ))− Eg(xs+1t′+1) + k − 1 k Eg(xs+1t′ )\n)\n= E‖xs − PS(xs)‖2 + m−1∑\nt′=0\n2γ Lmaxk E ( f(PS(xs+1t′ ))− f(xs+1t′ ) ) −\nm−1∑\nt′=0\n1\nk\n( 1− γτθ ′\nk − 2(Lresθ1 + Lnorθ2)γ\nk1/2Lmax\n) · E(‖xs+1t′ − xs+1t′+1‖2)\n+ 2γ\nLmax\nm−1∑\nt′=0\nE ( f(xs+1t′ )− f(xs+1t+1 ) ) + Lnorγ\nLmax E ∥∥∥∥∥ m−1∑\nt′=0\n( xs+1t′ − xs+1t′+1 ) ∥∥∥∥∥ 2\n+ 2γ\nLmax\nm−1∑\nt′=0\n( 1\nk Eg(PS(xs+1t′ ))− Eg(xs+1t′+1) + k − 1 k Eg(xs+1t′ )\n)\n≤ E‖xs − PS(xs)‖2 + 2γ\nLmaxk\nm−1∑\nt′=0\n( F ∗ − EF (xs+1t′ ) ) + 2γ\nLmax\nm−1∑\nt′=0\n( EF (xs+1t′ )− EF (xs+1t′+1) )\n− m−1∑\nt′=0\n1\nk\n( 1− Λnorγ − γτθ′\nn − 2(Λresθ1 +Λnorθ2)γ k1/2\n) · E(‖xs+1t′ − xs+1t′+1‖2)\n≤ E‖xs − PS(xs)‖2 + 2γ\nLmaxk\nm−1∑\nt′=0\n( F ∗ − EF (xs+1t′ ) ) + 2γ\nLmax\n( EF (xs)− EF (xs+1) )\nwhere the second inequality uses (3), the final inequality comes from 1 − Λnorγ − γτθ ′ n − 2(Λresθ1+Λnorθ2)γ\nn1/2 ≥ 0. Define S(xs) = E‖xt −PS(xs)‖2 + 2γLmaxE (F (x s)− F ∗). According to\n(42), we have\nS(xs+1) ≤ S(xs)− 2γ Lmaxk\nm−1∑\nt′=0\nE ( F (xs+1t′ )− F ∗ ) ≤ S(xs)− 2mγ Lmaxk E ( F (xs+1)− F ∗ ) (43)\nwhere the second inequality comes from the monotonicity of EF (xst ). According to (43), we have\nS(xs) ≤ S(x0)− 2mγs Lmaxk E (F (xs)− F ∗) (44)\nThus, the sublinear convergence rate (37) for general smooth convex function f can be obtained from (44).\nIf the optimal strong convexity for the smooth convex function f holds with l > 0, we have (45) as proved in (A.28) of Liu and Wright (2015).\nE (F (xs)− F ∗) ≥ Lmaxl 2(lγ + Lmax) S(xs) (45)\nThus, substituting (46) into (43), we have\nS(xs+1) ≤ S(xs)− 2mγl 2k(lγ + Lmax) S(xs+1) (46)\nBased on (46), we have (47) by induction.\nS(xs) ≤ (\n1\n1 + 2mγl2k(lγ+Lmax)\n)s S(x0) (47)\nThus, the linear convergence rate (36) for the optimal strong convexity on f can be obtained from (47).\n5. Conclusion\nIn this paper, we propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lockfree in the implementation and analysis. AsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. We prove that AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained.\nReferences\nHaim Avron, Alex Druinsky, and Anshul Gupta. Revisiting asynchronous linear solvers: Provable convergence rate through randomization. Journal of the ACM (JACM), 62(6): 51, 2015.\nMathieu Blondel, Kazuhiro Seki, and Kuniaki Uehara. Block coordinate descent algorithms for large-scale sparse multiclass classification. Machine learning, 93(1):31–52, 2013.\nDK Callebaut. Generalization of the cauchy-schwarz inequality. Journal of mathematical analysis and applications, 12(3):491–494, 1965.\nSorathan Chaturapruek, John C Duchi, and Christopher Ré. Asynchronous stochastic convex optimization: the noise is in the noise and sgd don’t care. In Advances in Neural Information Processing Systems, pages 1531–1539, 2015.\nSaeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.\nCho-Jui Hsieh, Hsiang-Fu Yu, and Inderjit S Dhillon. Passcode: Parallel asynchronous stochastic dual co-ordinate descent. arXiv preprint, 2015.\nZhouyuan Huo and Heng Huang. Asynchronous stochastic gradient descent with variance reduction for non-convex optimization. arXiv preprint arXiv:1604.03584, 2016a.\nZhouyuan Huo and Heng Huang. Distributed asynchronous dual-free stochastic dual coordinate ascent. arXiv preprint arXiv:1605.09066, 2016b.\nZhouyuan Huo, Bin Gu, and Heng Huang. Decoupled asynchronous proximal stochastic gradient descent with variance reduction. arXiv preprint arXiv:1609.06804, 2016.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315– 323, 2013.\nXiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2737–2745, 2015.\nXiangru Lian, Huan Zhang, Cho-Jui Hsieh, Yijun Huang, and Ji Liu. A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zerothorder to first-order. arXiv preprint arXiv:1606.00498, 2016.\nQihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated proximal coordinate gradient method. In Advances in Neural Information Processing Systems, pages 3059–3067, 2014.\nJi Liu and Stephen J Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIAM Journal on Optimization, 25(1):351–376, 2015.\nJi Liu, Stephen J Wright, Christopher Ré, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. Journal of Machine Learning Research, 16(285-322):1–5, 2015.\nZhaosong Lu and Lin Xiao. On the complexity analysis of randomized block-coordinate descent methods. Mathematical Programming, 152(1-2):615–642, 2015.\nHoria Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. arXiv preprint arXiv:1507.06970, 2015.\nAtsushi Nitanda. Stochastic proximal gradient descent with acceleration techniques. In Advances in Neural Information Processing Systems, pages 1574–1582, 2014.\nBenjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693–701, 2011.\nSashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex J Smola. On variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in Neural Information Processing Systems, pages 2647–2655, 2015.\nPeter Richtárik and Martin Takáč. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156(1-2):433–484, 2016.\nVolker Roth and Bernd Fischer. The group-lasso for generalized linear models: uniqueness of solutions and efficient algorithms. In Proceedings of the 25th international conference on Machine learning, pages 848–855. ACM, 2008.\nAndrzej P Ruszczyński. Nonlinear Optimization. Number 13 in Nonlinear optimization. Princeton University Press, 2006.\nMark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.\nShai Shalev-Shwartz. SDCA without duality, regularization, and individual convexity. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 747–754, 2016. URL http://jmlr.org/proceedings/papers/v48/shalev-shwartza16.html.\nShai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In ICML, pages 64–72, 2014.\nMartin Takác. Randomized coordinate descent methods for big data optimization. 2014.\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267–288, 1996.\nLin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014.\nTong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the twenty-first international conference on Machine learning, page 116. ACM, 2004.\nShen-Yi Zhao and Wu-Jun Li. Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\nTuo Zhao, Mo Yu, Yiming Wang, Raman Arora, and Han Liu. Accelerated mini-batch randomized block coordinate descent method. In Advances in neural information processing systems, pages 3329–3337, 2014."
    } ],
    "references" : [ {
      "title" : "Revisiting asynchronous linear solvers: Provable convergence rate through randomization",
      "author" : [ "Haim Avron", "Alex Druinsky", "Anshul Gupta" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Avron et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Avron et al\\.",
      "year" : 2015
    }, {
      "title" : "Block coordinate descent algorithms for large-scale sparse multiclass classification",
      "author" : [ "Mathieu Blondel", "Kazuhiro Seki", "Kuniaki Uehara" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Blondel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Blondel et al\\.",
      "year" : 2013
    }, {
      "title" : "Generalization of the cauchy-schwarz inequality",
      "author" : [ "DK Callebaut" ],
      "venue" : "Journal of mathematical analysis and applications,",
      "citeRegEx" : "Callebaut.,? \\Q1965\\E",
      "shortCiteRegEx" : "Callebaut.",
      "year" : 1965
    }, {
      "title" : "Asynchronous stochastic convex optimization: the noise is in the noise and sgd don’t care",
      "author" : [ "Sorathan Chaturapruek", "John C Duchi", "Christopher Ré" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chaturapruek et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chaturapruek et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic first-and zeroth-order methods for nonconvex stochastic programming",
      "author" : [ "Saeed Ghadimi", "Guanghui Lan" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Ghadimi and Lan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ghadimi and Lan.",
      "year" : 2013
    }, {
      "title" : "Passcode: Parallel asynchronous stochastic dual co-ordinate descent",
      "author" : [ "Cho-Jui Hsieh", "Hsiang-Fu Yu", "Inderjit S Dhillon" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "Hsieh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous stochastic gradient descent with variance reduction for non-convex optimization",
      "author" : [ "Zhouyuan Huo", "Heng Huang" ],
      "venue" : "arXiv preprint arXiv:1604.03584,",
      "citeRegEx" : "Huo and Huang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huo and Huang.",
      "year" : 2016
    }, {
      "title" : "Distributed asynchronous dual-free stochastic dual coordinate ascent",
      "author" : [ "Zhouyuan Huo", "Heng Huang" ],
      "venue" : "arXiv preprint arXiv:1605.09066,",
      "citeRegEx" : "Huo and Huang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huo and Huang.",
      "year" : 2016
    }, {
      "title" : "Decoupled asynchronous proximal stochastic gradient descent with variance reduction",
      "author" : [ "Zhouyuan Huo", "Bin Gu", "Heng Huang" ],
      "venue" : "arXiv preprint arXiv:1609.06804,",
      "citeRegEx" : "Huo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huo et al\\.",
      "year" : 2016
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Johnson and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2013
    }, {
      "title" : "Asynchronous parallel stochastic gradient for nonconvex optimization",
      "author" : [ "Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lian et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lian et al\\.",
      "year" : 2015
    }, {
      "title" : "A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zerothorder to first-order",
      "author" : [ "Xiangru Lian", "Huan Zhang", "Cho-Jui Hsieh", "Yijun Huang", "Ji Liu" ],
      "venue" : "arXiv preprint arXiv:1606.00498,",
      "citeRegEx" : "Lian et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lian et al\\.",
      "year" : 2016
    }, {
      "title" : "An accelerated proximal coordinate gradient method",
      "author" : [ "Qihang Lin", "Zhaosong Lu", "Lin Xiao" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Asynchronous stochastic coordinate descent: Parallelism and convergence properties",
      "author" : [ "Ji Liu", "Stephen J Wright" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Liu and Wright.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu and Wright.",
      "year" : 2015
    }, {
      "title" : "An asynchronous parallel stochastic coordinate descent algorithm",
      "author" : [ "Ji Liu", "Stephen J Wright", "Christopher Ré", "Victor Bittorf", "Srikrishna Sridhar" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "On the complexity analysis of randomized block-coordinate descent methods",
      "author" : [ "Zhaosong Lu", "Lin Xiao" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Lu and Xiao.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lu and Xiao.",
      "year" : 2015
    }, {
      "title" : "Perturbed iterate analysis for asynchronous stochastic optimization",
      "author" : [ "Horia Mania", "Xinghao Pan", "Dimitris Papailiopoulos", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan" ],
      "venue" : "arXiv preprint arXiv:1507.06970,",
      "citeRegEx" : "Mania et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mania et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic proximal gradient descent with acceleration techniques",
      "author" : [ "Atsushi Nitanda" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Nitanda.,? \\Q2014\\E",
      "shortCiteRegEx" : "Nitanda.",
      "year" : 2014
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Recht et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2011
    }, {
      "title" : "On variance reduction in stochastic gradient descent and its asynchronous variants",
      "author" : [ "Sashank J Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnabas Poczos", "Alex J Smola" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Reddi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Reddi et al\\.",
      "year" : 2015
    }, {
      "title" : "Parallel coordinate descent methods for big data optimization",
      "author" : [ "Peter Richtárik", "Martin Takáč" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Richtárik and Takáč.,? \\Q2016\\E",
      "shortCiteRegEx" : "Richtárik and Takáč.",
      "year" : 2016
    }, {
      "title" : "The group-lasso for generalized linear models: uniqueness of solutions and efficient algorithms",
      "author" : [ "Volker Roth", "Bernd Fischer" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Roth and Fischer.,? \\Q2008\\E",
      "shortCiteRegEx" : "Roth and Fischer.",
      "year" : 2008
    }, {
      "title" : "Nonlinear Optimization. Number 13 in Nonlinear optimization",
      "author" : [ "Andrzej P Ruszczyński" ],
      "venue" : null,
      "citeRegEx" : "Ruszczyński.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ruszczyński.",
      "year" : 2006
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2013
    }, {
      "title" : "SDCA without duality, regularization, and individual convexity",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shalev.Shwartz.",
      "year" : 2016
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2014
    }, {
      "title" : "Randomized coordinate descent methods for big data optimization",
      "author" : [ "Martin Takác" ],
      "venue" : null,
      "citeRegEx" : "Takác.,? \\Q2014\\E",
      "shortCiteRegEx" : "Takác.",
      "year" : 2014
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "Lin Xiao", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Xiao and Zhang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiao and Zhang.",
      "year" : 2014
    }, {
      "title" : "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
      "author" : [ "Tong Zhang" ],
      "venue" : "In Proceedings of the twenty-first international conference on Machine learning,",
      "citeRegEx" : "Zhang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2004
    }, {
      "title" : "Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee",
      "author" : [ "Shen-Yi Zhao", "Wu-Jun Li" ],
      "venue" : "In Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Zhao and Li.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao and Li.",
      "year" : 2016
    }, {
      "title" : "Accelerated mini-batch randomized block coordinate descent method",
      "author" : [ "Tuo Zhao", "Mo Yu", "Yiming Wang", "Raman Arora", "Han Liu" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.",
      "startOffset" : 107,
      "endOffset" : 210
    }, {
      "referenceID" : 9,
      "context" : "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.",
      "startOffset" : 107,
      "endOffset" : 210
    }, {
      "referenceID" : 23,
      "context" : "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.",
      "startOffset" : 107,
      "endOffset" : 210
    }, {
      "referenceID" : 4,
      "context" : "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.",
      "startOffset" : 107,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.",
      "startOffset" : 107,
      "endOffset" : 210
    }, {
      "referenceID" : 26,
      "context" : "Also, stochastic coordinate descent (SCD) algorithms (Takác, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.",
      "startOffset" : 53,
      "endOffset" : 104
    }, {
      "referenceID" : 31,
      "context" : "Also, stochastic coordinate descent (SCD) algorithms (Takác, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.",
      "startOffset" : 53,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "Also, stochastic coordinate descent (SCD) algorithms (Takác, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.",
      "startOffset" : 53,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : ", 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.",
      "startOffset" : 76,
      "endOffset" : 130
    }, {
      "referenceID" : 24,
      "context" : ", 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.",
      "startOffset" : 76,
      "endOffset" : 130
    }, {
      "referenceID" : 28,
      "context" : "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).",
      "startOffset" : 80,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).",
      "startOffset" : 80,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).",
      "startOffset" : 80,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 3,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 14,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 18,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 19,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 11,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 13,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 30,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 10,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 0,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 5,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 16,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 8,
      "context" : "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richtárik and Takáč, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.",
      "startOffset" : 90,
      "endOffset" : 403
    }, {
      "referenceID" : 23,
      "context" : "For example, Zhao and Li (2016) proposed an asynchronous parallel algorithm for SVRG and proved the linear convergence.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "Liu and Wright (2015) proposed an asynchronous parallel algorithm for SCD and proved the linear convergence.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG.",
      "startOffset" : 0,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG. Huo and Huang (2016a) proposed an asynchronous SGD algorithm with variance reduction on non-convex optimization problems and proved the convergence.",
      "startOffset" : 0,
      "endOffset" : 256
    }, {
      "referenceID" : 0,
      "context" : "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG. Huo and Huang (2016a) proposed an asynchronous SGD algorithm with variance reduction on non-convex optimization problems and proved the convergence. Lian et al. (2016) proposed an asynchronous stochastic optimization algorithm with zeroth order and proved the convergence.",
      "startOffset" : 0,
      "endOffset" : 402
    }, {
      "referenceID" : 27,
      "context" : "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al.",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al.",
      "startOffset" : 130,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al., 2013), and so on.",
      "startOffset" : 189,
      "endOffset" : 211
    }, {
      "referenceID" : 13,
      "context" : "Thus, the formulation in (Liu and Wright, 2015) is a special case of (1).",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "Each iteration in (Liu and Wright, 2015) only modifies a single component of x which is an atomic operation in the parallel system with shared memory.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "However, we need to modify a block coordinate Gj of x with lockfree for each iteration, which are more complicated in the asynchronous parallel analysis than the atomic updating in (Liu and Wright, 2015).",
      "startOffset" : 181,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al., 2013), and so on. Note that, Liu and Wright (2015) consider the formulation (1) with the constraints that |Gj | = 1 for all j.",
      "startOffset" : 190,
      "endOffset" : 257
    }, {
      "referenceID" : 13,
      "context" : "And several examples of optimally strongly convex functions that are not strongly convex are provided in (Liu and Wright, 2015).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "As mentioned in Liu and Wright (2015), the condition of optimal strong convexity is significantly weaker than the normal strong convexity condition.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "Convergence Analysis In this section, we follow the analysis of (Liu and Wright, 2015) and prove the convergence rate of AsyDSCDVR (Theorem 8).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "Same as mentioned in (Mania et al., 2015), there might not be an actual time the idea ones exist in the shared memory, except the first and last iterates for each outer loop.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "x\\Gj(t) = (x s t )\\Gj(t) If xt+1 is the solution of (18), the solution of optimization problem (19) is also x s t+1 according to the subdifferential version of Karush-Kuhn-Tucker (KKT) conditions (Ruszczyński., 2006).",
      "startOffset" : 196,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "8) in Liu and Wright (2015), we have ‖xt−1 − xt‖ − ‖xt − xt+1‖ ≤ 2‖xt−1 − xt‖‖xt − xt+1 − xt−1 + xt‖ (21) The second part in the right half side of (21) is bound as follows if B = {it} and J(t) = {j(t)}.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "7 of Liu and Wright (2015), the sixth inequality comes from ‖xt − x̃s‖ = ‖ ∑t−1 t′=0∆ s t′‖ ≤ ∑t−1 t=0 ‖∆t′‖.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "13 in (Liu and Wright, 2015).",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "where the first inequality uses the Cauchy-Schwarz inequality (Callebaut, 1965), the third inequality uses (3) and (4), the sixth inequality uses (29).",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "28) of Liu and Wright (2015). E (F (x)− F ) ≥ Lmaxl 2(lγ + Lmax) S(x) (45)",
      "startOffset" : 7,
      "endOffset" : 29
    } ],
    "year" : 2016,
    "abstractText" : "Asynchronous parallel implementations for stochastic optimization have received huge successes in theory and practice recently. Asynchronous implementations with lock-free are more efficient than the one with writing or reading lock. In this paper, we focus on a composite objective function consisting of a smooth convex function f and a block separable convex function, which widely exists in machine learning and computer vision. We propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lock-free in the implementation and analysis. AsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. We prove that AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained.",
    "creator" : "LaTeX with hyperref package"
  }
}