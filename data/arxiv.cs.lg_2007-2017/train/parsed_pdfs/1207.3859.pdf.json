{
  "name" : "1207.3859.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning",
    "authors" : [ "Ulugbek S. Kamilov", "Sundeep Rangan", "Alyson K. Fletcher" ],
    "emails" : [ "ulugbek.kamilov@epfl.ch)", "srangan@poly.edu)", "afletcher@ucsc.edu)", "michael.unser@epfl.ch)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION Consider the estimation of a random vector x ∈ Rn from the measurement model illustrated in Figure 1. The random vector x, which is assumed to have independent and identically distributed (i.i.d.) components xj ∼ PX , is passed through a known linear transform that outputs z = Ax. The components of y ∈ Rm are generated by a componentwise transfer function PY |Z . This work addresses the cases where the distributions PX and PY |Z have some unknown parameters,\nThe work of S. Rangan was supported by the National Science Foundation under Grant No. 1116589. U. S. Kamilov and M. Unser were supported by the European Commission under Grant ERC-2010-AdG 267439-FUN-SP.\nU. S. Kamilov (email: ulugbek.kamilov@epfl.ch) is with Biomedical Imaging Group, École Polytechnique Fédérale de Lausanne, Switzerland\nS. Rangan (email: srangan@poly.edu) is with Polytechnic Institute of New York University, Brooklyn, NY.\nA. K. Fletcher (email: afletcher@ucsc.edu) is with the Department of Electrical Engineering, University of California, Santa Cruz.\nM. Unser (email: michael.unser@epfl.ch) is with Biomedical Imaging Group, École Polytechnique Fédérale de Lausanne, Switzerland\nλx and λz , that must be learned in addition to the estimation of x.\nSuch joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]–[5]. Also, since the parameters in the output transfer function PY |Z can model unknown nonlinearities, this problem formulation can be applied to the identification of linear-nonlinear cascade models of dynamical systems, in particular for neural spike responses [6]–[8].\nWhen the distributions PX and PY |Z are known, or reasonably bounded, there are a number of methods available that can be used for the estimation of the unknown vector x. In recent years, there has been significant interest in so-called approximate message passing (AMP) and related methods based on Gaussian approximations of loopy belief propagation (LBP) [9]–[18]. These methods originate from CDMA multiuser detection problems in [9]–[11], and have received considerable recent attention in the context of compressed sensing [13]–[17]. See, also the survey article [19]. The Gaussian approximations used in AMP are also closely related to standard expectation propagation techniques [20], [21], but with additional simplifications that exploit the linear coupling between the variables x and z. The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17]. This paper considers the so-called generalized AMP (GAMP) method of [18] that extends the algorithm in [13] to arbitrary output distributions PY |Z (many original formulations assumed additive white Gaussian noise (AWGN) measurements).\nHowever, although the current formulation of AMP and GAMP methods is attractive conceptually, in practice, one often does not know the prior and noise distributions exactly.\nar X\niv :1\n20 7.\n38 59\nv3 [\ncs .I\nT ]\n1 D\nec 2\n01 2\nTo overcome this limitation, Vila and Schniter [22], [23] and Krzakala et al. [24], [25] have recently proposed extension of AMP and GAMP based on Expectation Maximization (EM) that enable joint learning of the parameters (λx, λz) along with the estimation of the vector x. While simulations indicate excellent performance, the analysis of these methods is difficult. This work provides a unifying analytic framework for such AMP-based joint estimation and learning methods. The main contributions of this paper are as follows: • Generalization of the GAMP method of [18] to a class\nof algorithms we call adaptive GAMP that enables joint estimation of the parameters λx and λz along with vector x. The methods are computationally fast and general with potentially large domain of application. In addition, the adaptive GAMP methods include the EM-GAMP algorithms of [22]–[25] as special cases. • Exact characterization of the asymptotic behavior of adaptive GAMP. We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]– [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations. • Demonstration of asymptotic consistency of adaptive GAMP with maximum-likelihood (ML) parameter estimation. Our main result shows that when the ML parameter estimation is computed exactly, the estimated parameters converge to the true values and the performance of adaptive GAMP asymptotically coincides with the performance of the oracle GAMP algorithm that knows the correct parameter values. Remarkably, this result applies to essentially arbitrary parameterizations of the unknown distributions PX and PY |Z , thus enabling provably consistent estimation on non-convex and nonlinear problems. • Experimental evaluation of the algorithm for the problems of learning of sparse priors in compressed sensing and identification of linear-nonlinear cascade models in neural spiking processes. Our simulations illustrate the performance gain of adaptive GAMP and its asymptotic consistency. Adaptive GAMP thus provides a computationallyefficient method for a large class of joint estimation and learning problems with a simple, exact performance characterization and provable conditions for asymptotic consistency."
    }, {
      "heading" : "A. Related Literature",
      "text" : "As mentioned above, the adaptive GAMP method proposed here can be seen as a generalization of the EM methods in [22]–[25]. In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26]. The “expectation” or E-step is performed by GAMP, which can approximately determine the marginal posterior distributions of the components xj given the observations y and the current parameter estimates of the GM distribution PX . A related EM-GAMP algorithm has also appeared in [24], [25] for the case of certain sparse priors and AWGN outputs. Simulations in [22], [23] show remarkably\ngood performance and computational speed for EM-GAMP over a wide class of distributions, particularly in the context of compressed sensing. Also, using arguments from statistical physics, [24], [25] presents state evolution (SE) equations for the joint evolution of the parameters and vector estimates and confirms them numerically.\nAs discussed in Section III-B, EM-GAMP is a special case of adaptive GAMP with a particular choice of the adaptation functions. Therefore, one contribution of this paper is to provide a rigorous theoretical justification of the EMGAMP methodology. In particular, the current work provides a rigorous justification of the SE analysis in [24], [25] along with extensions to more general input and output channels and adaptation methods. However, the methodology in [24], [25] in other ways is more general in that it can also study “seeded” or “spatially-coupled” matrices as proposed in [24], [25], [27]. An interesting open question is whether the analysis methods in this paper can be extended to these scenarios as well.\nAn alternate method for joint learning and estimation has been presented in [28], which assumes that the distributions on the source and output channels are themselves described by graphical models with the parameters λx and λz appearing as unknown variables. The method in [28], called Hybrid-GAMP, iteratively combines standard loopy BP with AMP methods. One avenue of future work is to see if methodology in this paper can be applied to analyze the Hybrid-GAMP methods as well.\nFinally, it should be pointed out that while simultaneous recovery of unknown parameters is appealing conceptually, it is not a strict requirement. An alternate solution to the problem is to assume that the signal belongs to a known class of distributions and to minimize the maximal mean-squared error (MSE) for the class. This minimax approach [29] was proposed for AMP recovery of sparse signals in [13]. Although minimax approach results in the estimators that are uniformly good over the entire class of distributions, there may be a significant gap between the MSE achieved by the minimax approach and the oracle algorithm that knows the distribution exactly. Indeed, this gap was the main justification of the EMGAMP methods in [22], [23]. Due to its asymptotic consistency, adaptive GAMP provably achieves the performance of the oracle algorithm."
    }, {
      "heading" : "B. Outline of the Paper",
      "text" : "The paper is organized as follows: In Section II, we review the non-adaptive GAMP and corresponding state evolution equations. In Section III, we present adaptive GAMP and describe ML parameter learning. In Section IV, we provide the main theorems characterizing asymptotic performance of adaptive GAMP and demonstrating its consistency. In Section V, we provide numerical experiments demonstrating the applications of the method. Section VI concludes the paper. A conference version of this paper has appeared in [30]. This paper contains all the proofs, more detailed descriptions and additional simulations."
    }, {
      "heading" : "II. REVIEW OF GAMP",
      "text" : ""
    }, {
      "heading" : "A. GAMP Algorithm",
      "text" : "Before describing the adaptive GAMP algorithm, it is useful to review the basic (non-adaptive) GAMP algorithm of [18]. Consider the estimation problem in Fig. 1 where the componentwise distributions on the inputs and outputs have some parametric form,\nPX(x|λx), PY |Z(y|z, λz), (1)\nwhere λx ∈ Λx and λz ∈ Λz represent parameters of the distributions and Λx and Λz some parameter sets.\nThe GAMP algorithm of [18] can be seen as a class of methods for estimating the vectors x and z for the case when the parameters λx and λz are known. In contrast, the adaptive GAMP method that is discussed in Section III enables joint estimation of the parameters λx and λz along with the vectors x and z. In order that to understand how the adaptation works, it is best to describe the basic GAMP algorithm as a special case of the more general adaptive GAMP procedure.\nThe basic GAMP algorithm corresponds to the special case of Algorithm 1 when the adaptation functions Htx and H t z output fixed values\nHtz(p t,y, τ tp) = λ\nt z, H t x(r t, τ tr) = λ t x, (2)\nfor some pre-computed sequence of parameters λ t x and λ t\nz . By “pre-computed”, we mean that the values do not depend on the data through the vectors pt, yt, and rt. In the oracle scenario λ t x and λ t\nz are set to the true values of the parameters and do not change with the iteration number t.\nThe estimation functions Gtx, G t z and G t s determine the estimates for the vectors x and z, given the parameter values λ̂tx and λ̂ t z . As described in [18], there are two important sets of choices for the estimation functions, resulting in two variants of GAMP: • Sum-product GAMP: In this case, the estimation func-\ntions are selected so that GAMP provides a Gaussian approximation of sum-product loopy BP. The estimates x̂t and ẑt then represent approximations of the MMSE estimates of the vectors x and z. • Max-sum GAMP: In this case, the estimation functions are selected so that GAMP provides a quadratic approximation of max-sum loopy BP and x̂t and ẑt represent approximations of the MAP estimates.\nThe estimation functions of the sum-product GAMP are equivalent to scalar MMSE estimation problems for the components of the vectors x and z observed in Gaussian noise. For max-sum GAMP, the estimation functions correspond to scalar MAP problems. Thus, for both versions, the GAMP method reduces the vector-valued MMSE and MAP estimation problems to a sequence of scalar AWGN problems combined with linear transforms by A and AT . GAMP is thus computationally simple, with each iteration involving only scalar nonlinear operations followed by linear transforms. The operations are similar in form to separable and proximal minimization methods widely used for such problems [31]– [35]. Appendix A reviews the equations for the sum-product\nGAMP. More details, as well as the equations for max-sum GAMP can be found in [18]."
    }, {
      "heading" : "B. State Evolution Analysis",
      "text" : "In addition to its computational simplicity and generality, a key motivation of the GAMP algorithm is that its asymptotic behavior can be precisely characterized when A is a large i.i.d. Gaussian transform. The asymptotic behavior is described by what is known as a state evolution (SE) analysis. By now, there are a large number of SE results for AMP-related algorithms [9], [11]–[18]. Here, we review the particular SE analysis from [18] which is based on the framework in [16].\nAssumption 1: Consider a sequence of random realizations of the GAMP algorithm, indexed by the dimension n, satisfying the following assumptions: (a) For each n, the matrix A ∈ Rm×n has i.i.d. components\nwith Aij ∼ N (0, 1/m) and the dimension m = m(n) is a deterministic function of n satisfying n/m→ β for some β > 0 as n→∞. (b) The input vectors x and initial condition x̂0 are deterministic sequences whose components converge empirically with bounded moments of order s = 2k − 2 as\nlim n→∞\n(x, x̂0) PL(s) = (X, X̂0), (3)\nto some random vector (X, X̂0) for some k ≥ 2. See Appendix B for the precise definition of this form of convergence.\n(c) The output vectors z and y ∈ Rm are generated by\nz = Ax, and yi = h(zi, wi) for all i = 1, . . . ,m, (4)\nfor some scalar function h(z, w) and vector w ∈ Rm representing an output disturbance. It is assumed that the output disturbance vector w is deterministic, but empirically converges as\nlim n→∞\nw PL(s) = W, (5)\nwhere s = 2k−2 is as in Assumption 1 (b) and W is some random variable. We let PY |Z denote the conditional distribution of the random variable Y = h(Z,W ). (d) The estimation function Gtx(r, τr, λx) and its derivative with respect to r, is Lipschitz continuous in r at (τr, λx) = (τ t r, λ t x), where τ t r is a deterministic parame-\nter from the SE equations below. A similar assumptions holds for Gtz(p, τp, λz). (e) The adaptation functions Htx and H t z are set to (2) for\nsome deterministic sequence of parameters λ t x and λ t\nz . Also, in the estimation steps in lines 7, 8 and 15 of Algorithm 1, the values of the τ tp and τ t r are replaced with the deterministic parameters τ tp and τ t r from the SE\nequations defined below. Assumption 1(a) simply states that we are considering large, Gaussian i.i.d. matrices A. Assumptions (b) and (c) state that the input vector x and output disturbance w are modeled as deterministic, but whose empirical distributions asymptotically appear as i.i.d. This deterministic model is one of key features\nof Bayati and Montanari’s analysis in [16]. Assumption (d) is a mild continuity condition. Assumption (e) defines the restriction of adaptive GAMP to the non-adaptive GAMP algorithm. We will remove this final assumption later.\nNote that, for now, there is no assumption that the “true” distribution of X or the true conditional distribution of Y given Z must belong to the class of distributions (1) for any parameters λx and λz . The analysis can thus model the effects of model mismatch.\nNow, given the above assumptions, define the sets of vectors\nθtx := {(xj , rtj , x̂t+1j ), j = 1, . . . , n}, (6a) θtz := {(zi, ẑti , yi, pti), i = 1, . . . ,m}. (6b)\nThe first vector set, θtx, represents the components of the the “true,” but unknown, input vector x, its GAMP estimate x̂t as well as rt. The second vector, θtz , contains the components the “true,” but unknown, output vector z, its GAMP estimate ẑt, as well as pt and the observed output y. The sets θtx and θtz are implicitly functions of the dimension n.\nThe main result of [18] shows that if we fix the iteration t, and let n→∞, the asymptotic joint empirical distribution of the components of these two sets θtx and θ t z converges to random vectors of the form\nθ t\nx := (X,R t, X̂t+1), θ\nt z := (Z, Ẑ t, Y, P t). (7)\nWe precisely state the nature of convergence momentarily (see Theorem 1). In (7), X is the random variable in Assumption 1(b), while Rt and X̂t+1 are given by\nRt = αtX + V t, V t ∼ N (0, ξtr), (8a)\nX̂t+1 = Gtx(R t, τ tr, λ\nt x) (8b)\nfor some deterministic constants αtr, ξ t r, and τ t r that are defined below. Similarly, (Z,P t) ∼ N (0,Ktp) for some covariance matrix Ktp, and\nY = h(Z,W ), Ẑt = Gtz(P t, Y, τ tp, λ\nt z), (9)\nwhere W is the random variable in (5) and Ktp contains deterministic constants.\nThe deterministic constants αtr, ξ t r, τ t r and K t p represent\nparameters of the distributions of θ t x and θ t\nz and depend on both the distributions of the input and outputs as well as the choice of the estimation and adaptation functions. The SE equations provide a simple method for recursively computing these parameters. The equations are best described algorithmically as shown in Algorithm 2. In order that we do not repeat ourselves, in Algorithm 2, we have written the SE equations for adaptive GAMP: For non-adaptive GAMP, the updates (19b) and (20a) can be ignored as the values of λ t\nz\nand λ t\nx are pre-computed. With these definitions, we can state the main result from\n[18].\nTheorem 1 ( [18]): Consider the random vectors θtx and θ t z generated by the outputs of GAMP under Assumption 1. Let θ t x and θ t z be the random vectors in (7) with the parameters\nAlgorithm 1 Adaptive GAMP\nRequire: Matrix A, estimation functions Gtx, Gts and Gtz and adaptation functions Htx and H t z .\n1: Set t← 0, st−1 ← 0 and select some initial values for x̂0 and τ0x . 2: repeat 3: {Output node update} 4: τ tp ← ‖A‖2F τ tx/m 5: pt ← Ax̂t − st−1τ tp 6: λ̂tz ← Htz(pt,y, τ tp) 7: ẑti ← Gtz(pti, yi, τ tp, λ̂tz) for all i = 1, . . . ,m 8: sti ← Gts(pti, yi, τ tp, λ̂tz) for all i = 1, . . . ,m 9: τ ts ← −(1/m) ∑ i ∂G t s(p t i, yi, τ t p, λ̂ t z)/∂p t i\n10: 11: {Input node update} 12: 1/τ tr ← ‖A‖2F τ ts/n 13: rt = xt + τ trA Tst 14: λ̂tx ← Htx(rt, τ tr) 15: x̂t+1j ← Gtx(rtj , τ tr , λ̂tx) for all j = 1, . . . , n 16: τ t+1x ← (τ tr/n) ∑ j ∂G t x(r t j , τ t r , λ̂ t x)/∂rj 17: until Terminated\ndetermined by the SE equations in Algorithm 2. Then, for any fixed t, the components of θtx and θ t z converge empirically with bounded moments of order k as\nlim n→∞\nθtx PL(k) = θ t\nx, lim n→∞\nθtz PL(k) = θ t z. (10)\nwhere θ t x and θ t\nz are given in (7). In addition, for any t, the limits\nlim n→∞\nτ tr = τ t r, lim n→∞ τ tp = τ t p, (11)\nalso hold almost surely. The theorem shows that the behavior of any component of the vectors x and z and their GAMP estimates x̂t and ẑt are distributed identically to a simple scalar equivalent system with random variables X , Z, X̂t and Ẑt. This scalar equivalent model appears in several analyses and can be thought of as a single-letter characterization [36] of the system. Remarkably, this limiting property holds for essentially arbitrary distributions and estimation functions, even ones that arise from problems that are highly nonlinear or noncovex. From the singleletter characterization, one can compute the asymptotic value of essentially any componentwise performance metric such as mean-squared error or detection accuracy. Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]–[40]."
    }, {
      "heading" : "III. ADAPTIVE GAMP",
      "text" : "As described in the previous section, the standard GAMP algorithm of [18] considers the case when the parameters λx and λz in the distributions in (1) are known. The adaptive GAMP method proposed in this paper, and shown in Algorithm 1, is an extension of the standard GAMP procedure that enables simultaneous identification of the parameters λx and λz along with estimation of the vectors x and z. The\nkey modification is the introduction of the two adaptation functions: Htz(p t,y, τ tp) and H t x(r\nt, τ tr). In each iteration, these functions output estimates, λ̂tz and λ̂ t x of the parameters based on the data pt, y, rt, τ tp and τ t r . We saw the standard GAMP method corresponds to the adaptation functions in (2) which outputs fixed values λ t z and λ t\nx that do not depend on the data, and can be used when the true parameters are known. For the case when the true parameters are not known, we will see that a simple maximum likelihood (ML) can be used to estimate the parameters from the data."
    }, {
      "heading" : "A. ML Parameter Estimation",
      "text" : "To understand how to estimate parameters via the adaptation functions, observe that from Theorem 1, we know that the distribution of the components of rt are distributed identically to the scalar Rt in (8). Now, the distribution of Rt only depends on three parameters – αtr, ξ t r and λx. It is thus natural to attempt to estimate these parameters from the empirical distribution of the components of rt.\nTo this end, let φx(r, λx, αr, ξr) be the log likelihood\nφx(r, λx, αr, ξr) = log pR(r|λx, αr, ξr), (12)\nwhere the right-hand side is the probability density of a random variable R with distribution\nR = αrX + V, X ∼ PX(·|λx), V ∼ N (0, ξr).\nThen, at any iteration t, we can attempt to perform a maximum-likelihood (ML) estimate\nλ̂tx = H t x(r t, τ tr)\n= arg max λx∈Λx max (αr,ξr)∈Sx(τtr)  1n n∑ j=1 φx(r t j , λx, αr, ξr)  .(13) Here, the set Sx(τ tr) is a set of possible values for the parameters αr, ξr. The set may depend on the measured variance τ tr . We will see the precise role of this set below.\nSimilarly, the joint distribution of the components of pt and y are distributed according to the scalar (P t, Y ) which depend only on the parameters Kp and λz . Thus, we can define the likelihood\nφz(p, y, λz,Kp) = log pP,Y (p, y|λz,Kp), (14)\nwhere the right-hand side is the joint probability density of (P, Y ) with distribution\nY ∼ PY |Z(·|Z, λz), (Z,P ) ∼ N (0,Kp).\nThen, we can attempt to estimate λz via the ML estimate\nλ̂tz = H t z(p t,y, τ tp)\n= arg max λz∈Λz max Kp∈Sz(τtp)\n{ 1\nm m∑ i=1 φz(p t i, yi,Kp)\n} . (15)\nAgain, the set Sz(τ tp) is a set of possible covariance matrices Kp."
    }, {
      "heading" : "B. Relation to EM-GAMP",
      "text" : "It is useful to briefly compare the above ML parameter estimation with the EM-GAMP method proposed by Vila and Schniter in [22], [23] and Krzakala et. al. in [24], [25]. Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows. First, the algorithms use the sum-product version of the AMP / GAMP algorithms, so that the outputs can provide an estimate of the posterior distributions on the components of x given the current parameter estimates. Specifically, at any iteration t, define the distribution\nP̂ tj (xj |rtj , τ tr , λ̂t−1x )\n= 1\nZ exp\n[ − 1\n2τ tr |xj − rtj |2\n] PX(xj |λ̂t−1x ). (16)\nFor the sum-product AMP or GAMP algorithms, it is shown in [18] that the SE equations simplify so that αtr = 1 and ξtr = τ t r, if the parameters were selected correctly. Therefore, from Theorem 1, the conditional distribution P (xj |rtj) should approximately match the distribution (16) for large n. If, in addition, we treat rtj and τ t r as sufficient statistics for estimating xj given y and A, then P̂ tj can be treated as an approximation for the posterior distribution of xj given the current parameter estimate λ̂t−1x . Some justification for this last step can be found in [11], [12], [17]. Using the approximation, we can approximately implement the EM procedure to update the parameter estimate via a maximization\nλ̂tx = H t x(r t, τ tr)\n:= arg max λx∈Λx\n1\nn n∑ j=1 E [ logPX(xj |λx)|P̂ tj ] , (17)\nwhere the expectation is with respect to the distribution in (16). In [22], [23], the parameter update (17) is performed only once every few iterations to allow P̂ t to converge to the approximation of the posterior distribution of xj given the current parameter estimates. In [24], [25], the parameter estimate is updated every iteration. A similar procedure can be performed for the estimation of λz .\nWe thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions Htx and H t z . As a result, our analysis in Theorem 2 below can be applied to these algorithms to provide rigorous asymptotic characterizations of the EM-GAMP performance. However, at the current time, we can only prove the asymptotic consistency result, Theorem 3, for the ML adaptation functions (13) and (15) described above.\nThat being said, it should be pointed out that EM-GAMP update (17) is generally computationally much simpler than the ML updates in (13) and (15). For example, when PX(x|λx) is an exponential family, the optimization in (17) is convex. Also, the optimizations in (13) and (15) require searches over additional parameters such as αr and ξr. Thus, an interesting avenue of future work is to apply the analysis result, Theorem 3 below, to see if the EM-GAMP method or some similarly computationally simple technique can be developed which also provides asymptotic consistency.\nAlgorithm 2 Adaptive GAMP State Evolution Given the distributions in Assumption 1, compute the sequence of parameters as follows: • Initialization Set t = 0 with\nK0x = cov(X, X̂ 0), τ0x = τ 0 x , (18)\nwhere the expectation is over the random variables (X, X̂0) in Assumption 1(b) and τ0x is the initial value in the GAMP algorithm. • Output node update: Compute the variables associated with the output nodes Compute the variables\nτ tp = βτ t x, K t p = βK t x, (19a) λ t\nz = H t z(P t, Y, τ tp), (19b) τ tr = −E−1 [ ∂\n∂p Gts(P t, Y, τ tp, λ t z)\n] , (19c)\nξtr = (τ t r) 2E [ Gts(P t, Y, τ tp, λ t z) ] , (19d)\nαtr = τ t rE [ ∂\n∂z Gts(P t, h(Z,W ), τ tp, λ t z)\n] , (19e)\nwhere the expectations are over the random variables (Z,P t) ∼ N (0,Ktp) and Y is given in (9).\n• Input node update: Compute\nλ t\nx = H t x(R t, τ tr), (20a) τ t+1x = τ t rE [ ∂\n∂r Gtx(R t, τ tr, λ t x)\n] , (20b)\nKt+1x = cov(X, X̂ t+1), (20c)\nwhere the expectations are over the random variables in (8)."
    }, {
      "heading" : "IV. CONVERGENCE AND ASYMPTOTIC CONSISTENCY WITH GAUSSIAN TRANSFORMS",
      "text" : ""
    }, {
      "heading" : "A. General State Evolution Analysis",
      "text" : "Before proving the asymptotic consistency of the adaptive GAMP method with ML adaptation, we first prove the following more general convergence result.\nAssumption 2: Consider the adaptive GAMP algorithm running on a sequence of problems indexed by the dimension n, satisfying the following assumptions: (a) Same as Assumption 1(a) to (c) with k = 2. (b) For every t, the adaptation function Htx(r, τr) can be\nregarded as a functional over r satisfying the following weak pseudo-Lipschitz continuity property: Consider any sequence of vectors r = r(n) and sequence of scalars τr = τ (n) r , indexed by n satisfying\nlim n→∞\nr(n) PL(k)\n= Rt, lim n→∞ τ (n)r = τ t r,\nwhere Rt and τ tr are the outputs of the state evolution equations defined below. Then,\nlim n→∞\nHtx(r (n), τ (n)r ) = H t x(R t, τ tr).\nSimilarly, Htz(y,p, τp) satisfies analogous continuity conditions in τp and (y,p). See Appendix B for a\ngeneral definition of weakly pseudo-Lipschitz continuous functionals. (c) The scalar function Gtx(r, τr, λx) and its derivative G′\nt x(r, τr, λx) with respect to r are continuous in λx uniformly over r in the following sense: For every > 0, t, τ∗r and λ ∗ x ∈ Λx, there exists an open neighborhood U of (τ∗r , λ ∗ x) such that for all (τr, λx) ∈ U and r,\n|Gtx(r, τr, λx)−Gtx(r, τ∗r , λ∗x)| < , |G′tx(r, τr, λx)−G′ t x(r, τ ∗ r , λ ∗ x)| < .\nIn addition, the functions Gtx(r, τr, λx) and G ′t x(r, τr, λx) must be Lipschitz continuous in r with a Lipschitz constant that can be selected continuously in τr and λx. The functions Gts(p, y, τp, λz), G t z(p, y, τp, λz) and their derivatives G′ts(p, y, τp, λz) G ′t z(p, y, τp, λz) satisfy analogous continuity assumptions with respect to p, y, τp and λz .\nAssumptions (b) and (c) are somewhat technical, but mild, continuity conditions that can be satisfied by a large class of adaptation functionals and estimation functions. For example, from the definitions in Appendix B, the continuity assumption (d) will be satisfied for any functional given by an empirical average\nHtx(r, τr) = 1\nn n∑ j=1 φtx(rj , τr),\nwhere, for each t, φtx(rj , τr) is pseudo-Lipschitz continuous in r of order p and continuous in τr uniformly over r. A similar functional can be used for Htz . As we will see in Section IV-B, the ML functionals (13) and (15) will also satisfy the conditions of this assumption.\nTheorem 2: Consider the random vectors θtx and θ t z generated by the outputs of the adaptive GAMP under Assumption 2. Let θ t x and θ t\nz be the random vectors in (7) with the parameters determined by the SE equations in Algorithm 2. Then, for any fixed t, the components of θtx and θ t z converge empirically with bounded moments of order k = 2 as\nlim n→∞\nθtx PL(k) = θ t\nx, lim n→∞\nθtz PL(k) = θ t z. (21)\nwhere θ t x and θ t\nz are given in (7). In addition, for any t, the limits\nlim n→∞\nλtx = λ t\nx, lim n→∞\nλtz = λ t z, (22a)\nlim n→∞\nτ tr = τ t r, lim n→∞ τ tp = τ t p, (22b)\nalso hold almost surely. Proof: See Appendix C.\nThe result is a natural generalization of Theorem 1 and provides a simple extension of the SE analysis to incorporate the adaptation. The SE analysis applies to essentially arbitrary adaptation functions. It particular, it can be used to analyze both the behavior of the adaptive GAMP algorithm with either ML and EM-GAMP adaptation functions in the previous section.\nThe proof is straightforward and is based on a continuity argument also used in [41]."
    }, {
      "heading" : "B. Asymptotic Consistency with ML Adaptation",
      "text" : "We cam now use Theorem 2 to prove the asymptotic consistency of the adaptive GAMP method with the ML parameter estimation described in Section III-A. The following two assumptions can be regarded as identifiability conditions.\nDefinition 1: Consider a family of distributions, {PX(x|λx), λx ∈ Λx}, a set Sx of parameters (αr, ξr) of a Gaussian channel and function φx(r, λx, αr, ξr). We say that PX(x|λx) is identifiable with Gaussian outputs with parameter set Sx and function φx if: (a) The sets Sx and Λx are compact. (b) For any “true” parameters λ∗x ∈ Λx, and (α∗r , ξ∗r ) ∈ Sx,\nthe maximization\nλ̂x = arg max λx∈Λx max (αr,ξr)∈Sx\nE [φx(α∗rX + V, λx, αr, ξr)|λ∗x, ξ∗r ] , (23)\nis well-defined, unique and returns the true value, λ̂x = λ∗x. The expectation in (23) is with respect to X ∼ PX(·|λ∗x) and V ∼ N (0, ξ∗r ). (c) For every λx and αr, ξr, the function φx(r, λx, αr, ξr) is pseudo-Lipschitz continuous of order k = 2 in r. In addition, it is continuous in λx, αr, ξr uniformly over r in the following sense: For every > 0 and λ̂x, α̂r, ξ̂r, there exists an open neighborhood U of λ̂x, α̂r, ξ̂r, such that for all (λx, αr, ξr) ∈ U and all r,\n|φx(r, λx, αr, ξr)− φx(r, λ̂x, α̂r, ξ̂r)| < .\nDefinition 2: Consider a family of conditional distributions, {PY |Z(y|z, λz), λz ∈ Λz} generated by the mapping Y = h(Z,W, λz) where W ∼ PW is some random variable and h(z, w, λz) is a scalar function. Let Sz be a set of covariance matrices Kp and let φz(y, p, λz,Kp) be some function. We say that conditional distribution family PY |Z(·|·, λz) is identifiable with Gaussian inputs with covariance set Sz and function φz if: (a) The parameter sets Sz and Λz are compact. (b) For any “true” parameter λ∗z ∈ Λz and true covariance\nK∗p, the maximization\nλ̂z = arg max λz∈Λz max Kp∈Sz E [ φz(Y, P, λz,Kp)|λ∗z,K∗p ] , (24)\nis well-defined, unique and returns the true value, λ̂z = λ∗z , The expectation in (24) is with respect to Y |Z ∼ PY |Z(y|z, λ∗z) and (Z,P ) ∼ N (0,K∗p). (c) For every λz and Kp, the function φz(y, p, λz,Kp) is pseudo-Lipschitz continuous in (p, y) of order k = 2. In addition, it is continuous in λp,Kp uniformly over p and y.\nDefinitions 1 and 2 essentially require that the parameters λx and λz can be identified through a maximization. The functions φx and φz can be the log likelihood functions (12) and (14), although we permit other functions as well, since the maximization may be computationally simpler. Such functions are sometimes called pseudo-likelihoods. The existence of a such a function is a mild condition. Indeed, if such a function\ndoes not exists, then the distributions on R or (Y, P ) must be the same for at least two different parameter values. In that case, one cannot hope to identify the correct value from observations of the vectors rt or (y,pt).\nAssumption 3: Let PX(x|λx) and PY |Z(y|z, λz) be families of distributions and consider the adaptive GAMP algorithm, Algorithm 1, run on a sequence of problems, indexed by the dimension n satisfying the following assumptions: (a) Same as Assumption 1(a) to (c) with k = 2. In addition,\nthe distributions for the vector X is given by PX(·|λ∗x) for some “true” parameter λ∗x ∈ Λx and the conditional distribution of Y given Z is given by PY |Z(y|z, λ∗z) for some “true” parameter λ∗z ∈ Λz .\n(b) Same as Assumption 2(c). (c) The adaptation functions are set to (13) and (15).\nTheorem 3: Consider the outputs of the adaptive GAMP algorithm with ML adaptation as described in Assumption 3. Then, for any fixed t, (a) The components of θtx and θ t z in (6) converge empirically\nwith bounded moments of order k = 2 as in (21) and the limits (22) hold almost surely. (b) In addition, if (αtr, ξ t r) ∈ Sx(τ tr), and the family of dis-\ntributions PX(·|λx), λx ∈ Λx is identifiable in Gaussian noise with parameter set Sx(τ tr) and pseudo-likelihood φx (see Definition 1), then\nlim n→∞\nλ̂tx = λ t x = λ ∗ x (25)\nalmost surely. (c) Similarly, if Ktp ∈ Sz(τ tp) for some t, and the family\nof distributions PY |Z(·|λz), λz ∈ Λz is identifiable with Gaussian inputs with parameter set Sz(τ tp) and pseudolikelihood φz (see Definition 2) then\nlim n→∞\nλ̂tz = λ t z = λ ∗ z (26)\nalmost surely. Proof: See Appendix D.\nThe theorem shows, remarkably, that for a very large class of the parameterized distributions, the adaptive GAMP algorithm is able to asymptotically estimate the correct parameters. Moreover, there is asymptotically no performance loss between the adaptive GAMP algorithm and a corresponding oracle GAMP algorithm that knows the correct parameters in the sense that the empirical distributions of the algorithm outputs are described by the same SE equations.\nThere are two key requirements: First, that the optimizations in (13) and (15) can be computed. These optimizations may be non-convex. Secondly, that the optimizations can be performed are over sufficiently large sets of Gaussian channel parameters Sx and Sz such that it can be guaranteed that the SE equations eventually enter these sets. In the examples below, we will see ways to reduce the search space of Gaussian channel parameters."
    }, {
      "heading" : "V. NUMERICAL RESULTS",
      "text" : ""
    }, {
      "heading" : "A. Estimation of a Gauss-Bernoulli input",
      "text" : "Recent results suggest that there is considerable value in learning of priors PX in the context of compressed sensing\n[42], which considers the estimation of sparse vectors x from underdetermined measurements (m < n) . It is known that estimators such as LASSO offer certain optimal minmax performance over a large class of sparse distributions [43]. However, for many particular distributions, there is a potentially large performance gap between LASSO and MMSE estimator with the correct prior. This gap was the main motivation for [22], [23] which showed large gains of the EMGAMP method due to its ability to learn the prior.\nHere, we illustrate the performance and asymptotic consistency of adaptive GAMP in a simple compressed sensing example. Specifically, we consider the estimation of a sparse vector x ∈ Rn from m noisy measurements\ny = Ax + w = z + w,\nwhere the additive noise w is random with i.i.d. entries wi ∼ N (0, σ2). Here, the “output” channel is determined by the statistics on w, which are assumed to be known to the estimator. So, there are no unknown parameters λz .\nAs a model for the sparse input vector x, we assumed the components are i.i.d. with the Gauss-Bernoulli distribution,\nxj ∼ {\n0 prob = 1− ρ, N (0, σ2x) prob = ρ\n(27)\nwhere ρ represents the probability that the component is nonzero (i.e. the vector’s sparsity ratio) and σ2x is the variance of the non-zero components. The parameters λx = (ρ, σ2x) are treated as unknown.\nIn the adaptive GAMP algorithm, we use the estimation functions Gx, Gs, and Gz corresponding to the sum-product GAMP algorithm. As described in Appendix A, for the sumproduce GAMP the SE equations simplify so that αtr = 1 and ξtr = τ t r. Since the noise variance is known, the initial output noise variance τ0r obtained by adaptive GAMP in Algorithm 1 exactly matches that of oracle GAMP. Therefore, for t = 0, the parameters αtr and ξ t r do not need to be estimated, and (13)\nconveniently simplifies to\nHx(r, τr) = arg max λx∈Λx  1n n∑ j=1 log pR(rj |λx, τr)  , (28) where Λx = [0, 1]× [0,+∞). For iteration t > 0, we rely on asymptotic consistency, and assume that the maximization (28) yields the correct parameter estimates, so that λ̂tx = λx. Then, in principle, for t > 0 adaptive GAMP uses the correct parameter estimates and we expect it to match the performance of oracle GAMP. In our implementation, we run EM update (17) until convergence to approximate the ML adaptation (28).\nFig. 2 illustrates the performance of adaptive GAMP on signals of length n = 400 generated with the parameters λx = (ρ = 0.2, σ 2 x = 5). The performance of adaptive GAMP is compared to that of LASSO with MSE optimal regularization parameter, and oracle GAMP that knows the parameters of the prior exactly. For generating the graphs, we performed 1000 random trials by forming the measurement matrix A from i.i.d. zero-mean Gaussian random variables of variance 1/m. In Figure 2(a), we keep the variance of the noise fixed to σ2 = 0.1 and plot the average MSE of the reconstruction against the measurement ratio m/n. In Figure 2(b), we keep the measurement ratio fixed to m/n = 0.75 and plot the average MSE of the reconstruction against the noise variance σ2. For completeness, we also provide the asymptotic MSE values computed via SE recursion. The results illustrate that GAMP significantly outperforms LASSO over the whole range of m/n and σ2. Moreover, the results corroborate the consistency of adaptive GAMP which achieves nearly identical quality of reconstruction with oracle GAMP. The performance results indicate that adaptive GAMP can be an effective method for estimation when the parameters of the problem are difficult to characterize and must be estimated from data.\nKAMILOV, RANGAN, FLETCHER AND UNSER 9 (a) (b)\nNoise variance ( )Measurement ratio ( )\nM S E (d B )\nM S E (d B )\n0.5 1 1.5 2 14\n13 12 11 10 9 8 7\nMeasurement ratio (m/n)\nM SE (d B)\nState Evolution LASSO Oracle GAMP Adaptive GAMP\n10 3 10 2 10 1 35\n30 25 20 15 10\ni V ri nce ( 2\nM SE (d B)"
    }, {
      "heading" : "B. Estimation of a Nonlinear Output Classification Function",
      "text" : "As a second example, we consider the estimation of the linear-nonlinear-Poisson (LNP) cascade model [8]. The model has been successfully used to characterize neural spike responses in early sensory pathways of the visual system. In the context of LNP cascade model, the vector x ∈ Rn represents the linear filter, which models the linear receptive field of the neuron. AMP techniques combined with the parameter estimation have been recently proposed for neural receptive field estimation and connectivity detection in [44].\nAs in Section V-A, we model x as a Gauss-Bernoulli vector of unknown parameters λx = (ρ, σ2x). To obtain the measurements y, the vector z = Ax is passed through a componentwise nonlinearity u to result in\nu(z) = 1\n1 + e−z . (29)\nLet the function ψ ∈ Rr denote a vector ψ(z) = ( 1, u(z), . . . , u(z)r−1 )T . (30)\nThen, the final measurement vector y is generated by a measurement channel with a conditional density of the form\npY |Z(yi|zi, λz) = f(zi)\nyi\ny! e−f(zi), (31)\nwhere f denotes the nonlinearity given by f(z) = exp ( λTz ψ(z) ) .\nAdaptive GAMP can now be used to also estimate vector of polynomial coefficients λz , which together with x, completely characterizes the LNP system.\nThe estimation of λz is performed with ML estimator described in Section III-A. We assume that the mean and variance of the vector x are known at iteration t = 0. This implies that for sum-product GAMP the covariance K0p is initially known and the optimization (15) simplifies to\nHz(p,y, τp) = arg max λz∈Λz\n{ 1\nm m∑ i=1 log pY (yi|λz)\n} , (32)\nwhere Λz ⊂ Rr. The estimation of λx is performed as in Section V-A. As before, for iteration t > 0, we assume that the maximizations (28) and (32) yield correct parameter estimates λ̂tx = λx and λ̂ t z = λz , respectively. Thus we can conclude by induction that for t > 0 the adaptive GAMP algorithm should continue matching oracle GAMP for large enough n. In our simulations, we implemented (32) with a gradient ascend algorithm and run it until convergence.\nIn Fig. 3, we compare the reconstruction performance of adaptive GAMP against the oracle version that knows the true parameters (λx, λz) exactly. We consider the vector x generated with true parameters λx = (ρ = 0.1, σ2x = 30). We consider the case r = 3 and set the parameters of the output channel to λz = [−4.88, 7.41, 2.58]. To illustrate the asymptotic consistency of the adaptive algorithm, we consider the signals of length n = 1000 and n = 10000. We perform 10 and 100 random trials for long and short signals, respectively, and plot the average MSE of the reconstruction against m/n. As expected, for large n, the performance of adaptive GAMP is nearly identical (within 0.15) to that of oracle GAMP."
    }, {
      "heading" : "VI. CONCLUSIONS AND FUTURE WORK",
      "text" : "We have presented an adaptive GAMP method for the estimation of i.i.d. vectors x observed through a known linear transforms followed by an arbitrary, componentwise random transform. The procedure, which is a generalization of EMGAMP methodology of [22]–[25] that estimates both the vector x as well as parameters in the source and componentwise output transform. In the case of large i.i.d. Gaussian transforms, it is shown that the adaptive GAMP method is provably asymptotically consistent in that the parameter estimates converge to the true values. This convergence result holds over a large class of models with essentially arbitrarily complex parameterizations. Moreover, the algorithm is computationally efficient since it reduces the vector-valued estimation problem to a sequence of scalar estimation problems in Gaussian noise. We believe that this method is applicable to a large class of linear-nonlinear models with provable guarantees can have applications in a wide range of problems. We have mentioned the use of the method for learning sparse priors in compressed sensing. Future work will include learning of parameters of output functions as well as possible extensions to non-Gaussian matrices."
    }, {
      "heading" : "APPENDIX A SUM-PRODUCT GAMP EQUATIONS",
      "text" : "As described in [18], the sum-product estimation can be implemented with the estimation functions\nGtx(r, τr, λ̂x) := E[X|R = r, τr, λ̂x], (33a) Gtz(p, y, τp, λ̂z) := E[Z|P = p, Y = y, τp, λ̂z], (33b) Gts(p, y, τp, λ̂z) := 1\nτp\n( Gtz(p, y, τp, λ̂z)− p ) , (33c)\nwhere the expectations are with respect to the scalar random variables\nR = X + Vx, Vx ∼ N (0, τr), X ∼ PX(·|λ̂x), (34a) Z = P + Vz, Vz ∼ N (0, τp), Y ∼ PY |Z(·|Z, λ̂z). (34b)\nThe paper [18] shows that the derivatives of these estimation functions for lines 9 and 16 are computed via the variances:\nτ r ∂Gtx(r, τr, λ̂x)\n∂r = var[X|R = r, τr, λ̂x] (35a)\n−∂G t s(p, y, τp, λ̂z)\n∂p\n= 1\nτp\n( 1− var[Z|P = p, Y = y, τp, λ̂z]\nτp\n) . (35b)\nThe estimation functions (33) correspond to scalar estimates of random variables in additive white Gaussian noise (AWGN). A key result of [18] is that, when the parameters are set to the true values (i.e. (λ̂x, λ̂z) = (λx, λz)), the outputs x̂t and ẑt can be interpreted as sum products estimates of the conditional expectations E(x|y) and E(z|y). The algorithm thus reduces the vector-valued estimation problem to a computationally simple sequence of scalar AWGN estimation problems along with linear transforms.\nMoreover, the SE equations in Algorithm 2 reduce to a particularly simple forms, where τ tr and ξ t r in (19) are given by\nτ tr = ξ t r = E−1\n[ ∂2\n∂p2 log pY |P (Y |P t)\n] , (36a)\nwhere the expectations are over the random variables (Z,P t) ∼ N (0,Ktp) and Y is given in (9). The covariance matrix Ktp has the form\nKtp =\n[ βτx0 βτx0 − τ tp\nβτx0 − τ tp βτx0 − τ tp\n] , (36b)\nwhere τx0 is the variance of X and β > 0 is the asymptotic measurement ratio (see Assumption 1 for details). The scaling constant (19e) becomes αtr = 1. The update rule for τ t+1 x also simplifies to τ t+1x = E [ var ( X|Rt )] , (36c)\nwhere the expectation is over the random variables in (8)."
    }, {
      "heading" : "APPENDIX B CONVERGENCE OF EMPIRICAL DISTRIBUTIONS",
      "text" : "Bayati and Montanari’s analysis in [16] employs certain deterministic models on the vectors and then proves convergence properties of related empirical distributions. To apply the same analysis here, we need to review some of their definitions. We say a function φ : Rr → Rs is pseudo-Lipschitz of order k > 1, if there exists an L > 0 such for any x, y ∈ Rr,\n‖φ(x)− φ(y)‖ ≤ L(1 + ‖x‖k−1 + ‖y‖k−1)‖x− y‖.\nNow suppose that for each n = 1, 2, . . ., v(n) is a set of vectors\nv(n) = {vi(n), i = 1, . . . , `(n)}, (37)\nwhere each element vi(n) ∈ Rs and `(n) is the number of elements in the set. Thus, v(n) can itself be regarded as a vector with s`(n) components. We say that v(n) empirically converges with bounded moments of order k as n → ∞ to a\nrandom vector V on Rs if: For all pseudo-Lipschitz continuous functions, φ, of order k,\nlim n→∞\n1\nn n∑ i=1 φ(vi(n)) = E(φ(V)) <∞.\nWhen the nature of convergence is clear, we may write (with some abuse of notation)\nv(n) PL(k)→ V as n→∞,\nor lim n→∞ v(n) PL(k) = V.\nFinally, let Psk be the set of probability distributions on Rs with bounded kth moments, and suppose that H : Psk → Λ is a functional Psk to some topological space Λ. Given a set v(n) as in (37), write H(v) for H(Pv) where Pv is the empirical distribution on the components of v. Also, given a random vector V with distribution PV write H(V) for H(PV). Then, we will say that the functional H is weakly pseudo-Lipschitz continuous of order k if\nlim n→∞\nv(n) PL(k)\n= V =⇒ lim n→∞ H(v(n)) = H(V),\nwhere the limit on the right hand side is in the topology of Λ."
    }, {
      "heading" : "APPENDIX C PROOF OF THEOREM 2",
      "text" : "The proof follows along the adaptation argument of [41]. We use the tilde superscript on quantities such as x̃t, r̃t, τ̃ tr , p̃ t, τ tp, s̃ t, and z̃t to denote values generated via a non-adaptive version of the GAMP. The non-adaptive GAMP algorithm has the same initial conditions as the adaptive algorithm (i.e. x̃0 = x̂0, τ̃0p = τ 0 p , s̃ −1 = s−1 = 0), but with λ̂tx and λ̂ t z replaced by their deterministic limits λ̄ t x and λ̄ t z , respectively. That is, we replace lines 7, 8 and 15 with\nz̃ti = G t z(p t i, yi, τ t p, λ\nt z), s̃ t i = G t s(p t i, yi, τ t p, λ t z),\nx̃t+1j = G t x(r t j , τ t r , λ\nt x).\nThis non-adaptive algorithm is precisely the standard GAMP method analyzed in [18]. The results in that paper show that the outputs of the non-adaptive algorithm satisfy all the required limits from the SE analysis. That is,\nlim n→∞\nθ̃tx PL(k) = θ t\nx, lim n→∞\nθ̃tz PL(k) = θ t z,\nwhere θ̃tx and θ̃ t z are the sets generated by the non-adaptive GAMP algorithm:\nθ̃tx := { (xj , r̃ t j , x̃ t+1 j ) : j = 1, . . . , n } ,\nθ̃tz = { (zi, z̃ t i , yi, p̃ t i) : i = 1, . . . ,m } .\nThe limits (21) are now proven through a continuity argument that shows that the adaptive and non-adaptive quantities must asymptotically agree with one another. Specifically, we will start by proving that the following limits holds almost surely for all t ≥ 0\nlim n→∞ ∆tx = lim n→∞\n1 n ‖x̂t − x̃t‖kk = 0, , (38a)\nlim n→∞ ∆tτp = limn→∞ |τ tp − τ̃ tp| = 0 (38b)\nwhere ‖ · ‖k is usual the k-norm. Moreover, in the course of proving (38), we will also show that the following limits hold almost surely\nlim m→∞ ∆tp = lim m→∞\n1 m ‖pt − p̃t‖kk = 0, (39a)\nlim n→∞ ∆tr = lim n→∞\n1 n ‖rt − r̃t‖kk = 0, (39b)\nlim m→∞ ∆ts = lim m→∞\n1 m ‖st − s̃t‖kk = 0, (39c)\nlim m→∞ ∆tz = lim m→∞\n1 m ‖ẑt − z̃t‖kk = 0, (39d)\nlim n→∞ ∆tτr = limn→∞ |τ tr − τ̃ tr | = 0, (39e)\nlim n→∞\nλ̂tx = λ̄ t x, (39f)\nlim n→∞\nλ̂tz = λ̄ t z, (39g)\nThe proof of the limits (38) and (39) is achieved by an induction on t. Although we only need to show the above limits for k = 2, most of the arguments hold for arbitrary k ≥ 2. We thus present the general derivation where possible.\nTo begin the induction argument, first note that the nonadaptive algorithm has the same initial conditions as the adaptive algorithm. Thus the limits (38) and (39c) hold for t = 0 and t = −1, respectively.\nWe now proceed by induction. Suppose that t ≥ 0 and the limits (38) and (39c) hold for some t and t− 1, respectively. Since A has i.i.d. components with zero mean and variance 1/m, it follows from the Marčenko-Pastur Theorem [45] that that its 2-norm operator norm is bounded. That is, there exists a constant CA such that\nlim n→∞ ‖A‖k ≤ CA, lim n→∞ ‖AT‖k ≤ CA. (40)\nThis bound is the only part of the proof that specifically requires k = 2. From (40), we obtain\n‖pt − p̃t‖k = ‖Ax̂t − τ tpst−1 −Ax̃t + τ̃ tps̃t−1‖k = ‖A(x̂t − x̃t) + τ tp(s̃t−1 − st−1) + (τ̃ tp − τ tp)s̃t−1‖k ≤ ‖A(x̂t − x̃t)‖k + |τ tp|‖s̃t−1 − st−1‖k + |τ̃ tp − τ tp|‖s̃t−1‖k\n(a) ≤‖A‖k‖x̂t − x̃t‖k + |τ tp|‖s̃t−1 − st−1‖k + |τ̃ tp − τ tp|‖s̃t−1‖k ≤ CA‖x̂t − x̃t‖k + |τ tp|‖st−1 − s̃t−1‖k + |τ tp − τ̃ tp|‖s̃t−1‖k\n(41)\nwhere (a) is due to the norm inequality ‖Ax‖k ≤ ‖A‖k‖x‖k. Since k ≥ 1, we have that for any positive numbers a and b\n(a+ b)k ≤ 2k(ak + bk). (42)\nApplying the inequality (42) into (41), we obtain\n1 m ‖pt − p̃t‖kk ≤ 1 m ( CA‖x̂t − x̃t‖k + |τ tp|‖st−1 − s̃t−1‖k + ∆tτp‖s̃ t−1‖k )k ≤ 2kCA n\nm ∆tx + 2 k|τ tp|k∆t−1s + 2k(∆tτp) k\n( 1\nm ‖s̃t−1‖kk\n) .\n(43)\nNow, since s̃t and τ̃ tp are the outputs of the non-adaptive algorithm they satisfy the limits\nlim n→∞\n1 m ‖s̃t‖kk = lim n→∞ 1 m m∑ i=1 |s̃ti|k = E [ |St|k ] <∞, (44a)\nlim n→∞\nτ̃ tp = τ t p <∞. (44b)\nNow, the induction hypotheses state that ∆tx, ∆ t−1 s and ∆ t τp → 0. Applying these induction hypotheses along the bounds (44a), and the fact that n/m→ β we obtain (39a).\nTo prove (39g), we first prove the empirical convergence of (pt,y) to (P t, Y ). Towards this end, let φ(p, y) be any pseudo-Lipschitz continuous function φ of order k. Then∣∣∣∣∣ 1m m∑ i=1 φ(pti, yi)− E [ φ(P t, Y )\n]∣∣∣∣∣ ≤ 1 m m∑ i=1\n∣∣φ(pti, yi)− φ(p̃ti, yi)∣∣ +\n∣∣∣∣∣ 1m m∑ i=1 φ(p̃ti, yi)− E [ φ(P t, Y ) ]∣∣∣∣∣ (a) ≤ L m m∑ i=1 ( 1 + |pti|k−1 + |p̃ti|k−1 + |yi|k−1 ) |pti − p̃ti|\n+ ∣∣∣∣∣ 1m m∑ i=1 φ(p̃ti, yi)− E [ φ(P t, Y ) ]∣∣∣∣∣ (b) ≤LC∆tp + ∣∣∣∣∣ 1m m∑ i=1 φ(p̃ti, yi)− E [ φ(P t, Y )\n]∣∣∣∣∣ . (45) In (a) we use the fact that φ is pseudo-Lipschitz, and in (b) we use Hölder’s inequality |x̂Ty| = ‖x‖k‖y‖q with q = p/(p−1) and define C as\nC :=\n[ 1\nm m∑ i=1 ( 1 + |pti|k−1 + |p̃ti|k−1 + |yi|k−1\n)]k/(k−1)\n≤ 1 m m∑ i=1 ( 1 + |pti|k−1 + |p̃ti|k−1 + |yi|k−1 )k/(k−1) ≤ const× [ 1 + ( 1\nm\n∥∥pt∥∥k k\n) k−1 k\n+\n( 1\nm\n∥∥p̃t∥∥k k\n) k−1 k\n+\n( 1\nm ‖y‖kk\n) k−1 k ] , (46)\nwhere the first step is from Jensen’s inequality. Since (p̃t,y) satisfy the limits for the non-adaptive algorithm we have:\nlim n→∞\n1 m ‖p̃t‖kk = lim n→∞ 1 m m∑ i=1 |p̃ti|k = E [ |P t|k ] <∞ (47a)\nlim n→∞\n1 m ‖y‖kk = lim n→∞ 1 m m∑ i=1 |yi|k = E [ |Y |k ] <∞ (47b)\nAlso, from the induction hypothesis (39a), it follows that the adaptive output must satisfy the same limit\nlim n→∞\n1 m ‖pt‖kk = lim n→∞ 1 m m∑ i=1 |pti|k = E [ |P t|k ] <∞. (48)\nCombining (45), (46), (47), (48), (39a) we conclude that for all t ≥ 0\nlim n→∞\n(pt,y) PL(k) = (P t, Y ). (49)\nThe limit (49) along with (38b) and the continuity condition on Htz in Assumption 1(d) prove the limit in (39g).\nThe limit (39a) together with continuity conditions on Gtz in Assumptions 1 show that (39c), (39d) and (39e) hold for t. For example, to show (39d), we consider the limit m → ∞ of the following expression\n1 m ‖ẑt − z̃t‖kk = 1 m ‖Gtz(pt,y, τ tp, λ̂tz)−Gtz(p̃t,y, τ tp, λ̄tz)‖kk\n(a) ≤ L m ‖pt − p̃t‖kk = L∆tp,\nwhere at (a) we used the Lipschitz continuity assumption. Similar arguments can be used for (39c) and (39e).\nTo show (39b), we proceed exactly as for (39a). Due to the continuity assumptions on Hx, this limit in turn shows that (39f) holds almost surely. Then, (38a) and (38b) follow directly from the continuity of Gx in Assumptions 1, together with (39b) and (39f). We have thus shown that if the limits (38) and (39) hold for some t, they hold for t+1. Thus, by induction they hold for all t.\nFinally, to show (21), let φ be any pseudo-Lipschitz continuous function φ(x, r, x̂), and define\nt = ∣∣∣∣∣∣ 1n m∑ j=1 φ(xj , r̃ t j , x̃ t+1 j )− E [ φ(X,Rt, X̂t+1) ]∣∣∣∣∣∣ , (50) which, due to convergence of non-adaptive GAMP, can be made arbitrarily small by choosing n large enough. Then, consider∣∣∣∣∣∣ 1n m∑ j=1 φ(xj , r̂ t j , x̂ t+1 j )− E [ φ(X,Rt, X̂t+1)\n]∣∣∣∣∣∣ ≤ tn + 1\nn n∑ j=1 ∣∣φ(xj , r̂tj , x̂t+1j )− φ(xj , r̃tj , x̃t+1j )∣∣ (a) ≤ tn + L‖rt − r̃t‖1 + L‖x̂t+1 − x̃t+1‖1\n+ L′\nn n∑ j=1 ( |r̂tj |k−1 + |r̃tj |k−1 ) (|r̂tj − r̃tj |+ |x̂t+1j − x̃ t+1 j |)\n+ L′\nn n∑ j=1 ( |x̂t+1j | k−1 + |x̃t+1j | k−1) (|r̂tj − r̃tj |+ |x̂t+1j − x̃t+1j |)\n(b) ≤ tn + L ( ∆tr ) 1 k + L ( ∆tx ) 1 k\n+ L′ ( ∆tr ) 1 k ( (M̃ t+1x ) k−1 k + (M̂ t+1x ) k−1 k + (M̃ tr) k−1 k + (M̂ tr) k−1 k ) + L′ ( ∆tx ) 1 k ( (M̃ t+1x ) k−1 k + (M̂ t+1x ) k−1 k + (M̃ tr) k−1 k + (M̂ tr) k−1 k\n) (51)\nwhere L, L′ are constants independent of n and\nM̂ t+1x = 1\nn\n∥∥x̂t+1∥∥k k , M̂ tr = 1\nn\n∥∥rt∥∥k k ,\nM̃ t+1x = 1\nn\n∥∥x̃t+1∥∥k k , M̃ tr = 1\nn\n∥∥r̃t∥∥k k\nIn (a) we use the fact that φ is pseudo-Lipshitz, in (b) we use `p-norm equivalence ‖x‖1 ≤ n1−1/p‖x‖k and Hölder’s inequality |x̂Ty| = ‖x‖k‖y‖q with q = p/(p−1). By applying of (38a), (39b) and since, M̂ t+1x , M̃ t+1 x , M̂ t r , and M̃ t r converge to a finite value we can obtain the first equation of (21) by taking n→∞. The second equation in (21) can be shown in a similar way. This proves the limits (21).\nAlso, the first two limits in (22) are a consequence of (39f) and (39f). The second two limits follow from continuity assumptions in Assumption 1(e) and the convergence of the empirical distributions in (21). This completes the proof."
    }, {
      "heading" : "APPENDIX D PROOF OF THEOREM 3",
      "text" : "Part (a) of Theorem 3 is a direct application of the general result, Theorem IV-A. To apply the general result, first observe that Assumptions 3(a) and (c) immediately imply the corresponding items in Assumptions 2. So, we only need to verify the continuity condition in Assumption 2(b) for the adaptation functions in (13) and (15).\nWe begin by proving the continuity of Htz . Fix t, and let (y(n),p(n)) be a sequence of vectors and τ (n)p be a sequence of scalars such that\nlim n→∞\n(y(n),p(n)) PL(p)\n= (Y, P t) lim n→∞ τ (n)p = τ t p, (52)\nwhere (Y, P t) and τ tp are the outputs of the state evolution equations. For each n, let\nλ̂(n)z = H t z(y (n),p(n), τ (n)p ). (53)\nWe wish to show that λ̂(n)z → λ∗z , the true parameter. Since λ̂ (n) z ∈ Λz and Λz is compact, it suffices to show that, any limit point of any convergent subsequence is equal to λ∗z . So, suppose that λ̂(n)z → λ̂z to some limit point λ̂z on some subsequence λ̂(n)z .\nFrom λ̂(n)z and the definition (15) it follows that\n1\nm m∑ i=1 φz(y (n) i , p (n) i , τ (n) p , λ̂ (n) z )\n≥ 1 m m∑ i=1 φz(y (n) i , p (n) i , τ (n) p , λ ∗ z). (54)\nNow, since τ (n)p → τ tp and λ̂ (n) z → λ̂z , we can apply the continuity condition in Definition 2(c) to obtain\nlim inf n→∞\n1\nm m∑ i=1 [ φz(y (n) i , p (n) i , τ t p, λ̂z)\n−φz(y(n)i , p (n) i , τ t p, λ ∗ z) ] ≥ 0. (55)\nAlso, the limit (52) and the fact that φz is psuedo-Lipschitz continuous of order k implies that\nE[φz(Y, P t, τ tp, λ̂z)] ≥ E[φz(Y, P t, τ tp, λ∗z)]. (56)\nBut, property (b) of Definition 2 shows that λ∗z is the maxima of the right-hand side, so\nE[φz(Y, P t, τ tp, λ̂z)] = E[φz(Y, P t, τ tp, λ∗z)]. (57)\nSince, by Definition 2(b), the maxima is unique, λ̂z = λ∗z . Since this limit point is the same for all convergent subsequences, we see that λ̂(n)z → λ∗z over the entire sequence. We have thus shown that given limits (52), the outputs of the adaptation function converge as\nHtz(y (n),p(n), τ (n)p ) = λ̂ (n) z → λ∗z = Htz(Y, P t, τ (n)p ).\nThus, the continuity condition on Htz in Assumption 2(b) is satisfied. The analogous continuity condition on Htx can be proven in a similar manner.\nTherefore, all the conditions of Assumption 2 are satisfied and we can apply Theorem 2. Part (a) of Theorem 3 immediately follows from Theorem 2.\nSo, it remains to show parts (b) and (c) of Theorem 3. We will only prove (b); the proof of (c) is similar. Also, since we have already established (22), we only need to show that the output of the SE equations matches the true parameter. That is, we need to show λ t\nx = λ ∗ x. This fact follows immediately\nfrom the selection of the adaptation functions:\nλ t\nx (a) = Htx(R t, τ tr) (b) = arg max\nλx∈Λx max (αr,ξr)∈Sx(τtr) E [ φx(R t, λx, αr, ξr) ]\n(c) = arg max\nλx∈Λx max (αr,ξr)∈Sx(τtr) E [ φx(α t rX + V t, λx, αr, ξr)|λ∗x, ξtr ]\n(58) (d) = λ ∗ x (59)\nwhere (a) follows from the SE equation (20a); (b) is the definition of the ML adaptation function Htx(·) when interpreted as a functional on a random variable Rt; (c) is the definition of the random variable Rt in (8) where V t ∼ N (0, ξtr); and (d) follows from Definition 1(b) and the hypothesis that (α∗r , ξ ∗ r ) ∈ Sx(τ tr). Thus, we have proven that λ t x = λ ∗ x, and this completes the proof of part (b) of Theorem 3. The proof of part (c) is similar."
    } ],
    "references" : [ {
      "title" : "Sparse Bayesian learning and the relevance vector machine",
      "author" : [ "M. Tipping" ],
      "venue" : "J. Machine Learning Research, vol. 1, pp. 211–244, Sep. 2001.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Bayesian factor regressionm models in the “large p, small n” paradigm",
      "author" : [ "M. West" ],
      "venue" : "Bayesian Statistics, vol. 7, 2003.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Sparse Bayesian learning for basis selection",
      "author" : [ "D. Wipf", "B. Rao" ],
      "venue" : "IEEE Trans. Signal Process., vol. 52, no. 8, pp. 2153–2164, Aug. 2004.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Bayesian compressive sensing",
      "author" : [ "S. Ji", "Y. Xue", "L. Carin" ],
      "venue" : "IEEE Trans. Signal Process., vol. 56, pp. 2346–2356, Jun. 2008.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning with compressible priors",
      "author" : [ "V. Cevher" ],
      "venue" : "Proc. NIPS, Vancouver, BC, Dec. 2009.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Identification of systems containing linear dynamic and static nonlinear elements",
      "author" : [ "S. Billings", "S. Fakhouri" ],
      "venue" : "Automatica, vol. 18, no. 1, pp. 15–26, 1982.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "The identification of nonlinear biological systems: Wiener and Hammerstein cascade models",
      "author" : [ "I.W. Hunter", "M.J. Korenberg" ],
      "venue" : "Biological Cybernetics, vol. 55, no. 2–3, pp. 135–144, 1986.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Spiketriggered neural characterization",
      "author" : [ "O. Schwartz", "J.W. Pillow", "N.C. Rust", "E.P. Simoncelli" ],
      "venue" : "J. Vision, vol. 6, no. 4, pp. 484–507, Jul. 2006.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Iterative multiuser joint decoding: Unified framework and asymptotic analysis",
      "author" : [ "J. Boutros", "G. Caire" ],
      "venue" : "IEEE Trans. Inform. Theory, vol. 48, no. 7, pp. 1772–1793, Jul. 2002.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Approximate belief propagation, density evolution, and neurodynamics for CDMA multiuser detection",
      "author" : [ "T. Tanaka", "M. Okada" ],
      "venue" : "IEEE Trans. Inform. Theory, vol. 51, no. 2, pp. 700–706, Feb. 2005.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Asymptotic mean-square optimality of belief propagation for sparse linear systems",
      "author" : [ "D. Guo", "C.-C. Wang" ],
      "venue" : "Proc. IEEE Inform. Theory Workshop, Chengdu, China, Oct. 2006, pp. 194–198.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Random sparse linear systems observed via arbitrary channels: A decoupling principle",
      "author" : [ "——" ],
      "venue" : "Proc. IEEE Int. Symp. Inform. Theory, Nice, France, Jun. 2007, pp. 946–950.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Message-passing algorithms for compressed sensing",
      "author" : [ "D.L. Donoho", "A. Maleki", "A. Montanari" ],
      "venue" : "Proc. Nat. Acad. Sci., vol. 106, no. 45, pp. 18 914–18 919, Nov. 2009.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Message passing algorithms for compressed sensing I: motivation and construction",
      "author" : [ "——" ],
      "venue" : "Proc. Info. Theory Workshop, Jan. 2010.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Message passing algorithms for compressed sensing II: analysis and validation",
      "author" : [ "——" ],
      "venue" : "Proc. Info. Theory Workshop, Jan. 2010.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The dynamics of message passing on dense graphs, with applications to compressed sensing",
      "author" : [ "M. Bayati", "A. Montanari" ],
      "venue" : "IEEE Trans. Inform. Theory, vol. 57, no. 2, pp. 764–785, Feb. 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Estimation with random linear mixing, belief propagation and compressed sensing",
      "author" : [ "S. Rangan" ],
      "venue" : "Proc. Conf. on Inform. Sci. & Sys., Princeton, NJ, Mar. 2010, pp. 1–6.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Generalized approximate message passing for estimation with random linear mixing",
      "author" : [ "——" ],
      "venue" : "Proc. IEEE Int. Symp. Inform. Theory, Saint Petersburg, Russia, Jul.–Aug. 2011, pp. 2174–2178.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Graphical model concepts in compressed sensing",
      "author" : [ "A. Montanari" ],
      "venue" : "Compressed Sensing: Theory and Applications, Y. C. Eldar and G. Kutyniok, Eds. Cambridge Univ. Press, Jun. 2012, pp. 394–438.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A family of algorithms for approximate Bayesian inference",
      "author" : [ "T.P. Minka" ],
      "venue" : "Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, 2001.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Bayesian inference and optimal design for the sparse linear model",
      "author" : [ "M. Seeger" ],
      "venue" : "J. Machine Learning Research, vol. 9, pp. 759–813, Sep. 2008.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Expectation-maximization Bernoulli-Gaussian approximate message passing",
      "author" : [ "J.P. Vila", "P. Schniter" ],
      "venue" : "Conf. Rec. 45th Asilomar Conf. Signals, Syst. & Comput., Pacific Grove, CA, Nov. 2011, pp. 799–803.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Expectation-maximization Gaussian-mixture approximate message passing",
      "author" : [ "——" ],
      "venue" : "Proc. Conf. on Inform. Sci. & Sys., Princeton, NJ, Mar. 2012.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Statistical physics-based reconstruction in compressed sensing",
      "author" : [ "F. Krzakala", "M. Mézard", "F. Sausset", "Y. Sun", "L. Zdeborová" ],
      "venue" : "arXiv:1109.4424, Sep. 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic reconstruction in compressed sensing: Algorithms, phase diagrams, and threshold achieving matrices",
      "author" : [ "——" ],
      "venue" : "arXiv:1206.3953, Jun. 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Maximum-likelihood from incomplete data via the EM algorithm",
      "author" : [ "A. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "J. Roy. Statist. Soc., vol. 39, pp. 1–17, 1977.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Informationtheoretically optimal compressed sensing via spatial coupling and approximate message passing",
      "author" : [ "D.L. Donoho", "A. Javanmard", "A. Montanari" ],
      "venue" : "December 2011, arXiv:1112.0708v1 [cs.IT].",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Hybrid generalized approximation message passing with applications to structured sparsity",
      "author" : [ "S. Rangan", "A.K. Fletcher", "V.K. Goyal", "P. Schniter" ],
      "venue" : "Proc. IEEE Int. Symp. Inform. Theory, Cambridge, MA, Jul. 2012, pp. 1241–1245.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Minimax risk over `p-balls for `qerror.",
      "author" : [ "D.L. Donoho", "I.M. Johnstone" ],
      "venue" : "Probab. Theory and Relat. Fields, vol. 99,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1994
    }, {
      "title" : "Approximate message passing with consistent parameter estimation and applications to sparse learning",
      "author" : [ "U.S. Kamilov", "S. Rangan", "A.K. Fletcher", "M. Unser" ],
      "venue" : "Proc. NIPS, Lake Tahoe, NV, Dec. 2012.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sparse reconstruction by separable approximation",
      "author" : [ "S.J. Wright", "R.D. Nowak", "M. Figueiredo" ],
      "venue" : "IEEE Trans. Signal Process., vol. 57, no. 7, pp. 2479–2493, Jul. 2009.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Augmented Lagrangian and Operator- Splitting Methods in Nonlinear Mechanics, ser",
      "author" : [ "R. Glowinski", "P.L. Tallec" ],
      "venue" : "SIAM Studies in Applied Mathematics. Philadelphia, PA: SIAM,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1989
    }, {
      "title" : "A new inexact alternating directions method for monotone variational inequalities",
      "author" : [ "B. He", "L.-Z. Liao", "D. Han", "H. Yang" ],
      "venue" : "Math. Program., vol. 92, no. 1, Ser A, pp. 103–108, 2002.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Alternating direction augmented Lagrangian methods for semidefinite programming",
      "author" : [ "Z. Wen", "D. Goldfarb", "W. Yin" ],
      "venue" : "Math. Program. Comp., vol. 2, no. 3–4, pp. 203–230, 2010.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bayesian compressive sensing via belief propagation",
      "author" : [ "D. Baron", "S. Sarvotham", "R.G. Baraniuk" ],
      "venue" : "IEEE Trans. Signal Process., vol. 58, no. 1, pp. 269–280, Jan. 2010.  14  APPROXIMATE MESSAGE PASSING WITH CONSISTENT PARAMETER ESTIMATION",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A statistical-mechanics approach to large-system analysis of CDMA multiuser detectors",
      "author" : [ "T. Tanaka" ],
      "venue" : "IEEE Trans. Inform. Theory, vol. 48, no. 11, pp. 2888–2910, Nov. 2002.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Randomly spread CDMA: Asymptotics via statistical physics",
      "author" : [ "D. Guo", "S. Verdú" ],
      "venue" : "IEEE Trans. Inform. Theory, vol. 51, no. 6, pp. 1983–2010, Jun. 2005.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Support recovery in compressed sensing: Information-theoretic bounds",
      "author" : [ "G. Caire", "S. Shamai", "A. Tulino", "S. Verdú" ],
      "venue" : "Proc. UCSD Workshop Inform. Theory & Its Applications, La Jolla, CA, Jan. 2011.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Asymptotic analysis of MAP estimation via the replica method and applications to compressed sensing",
      "author" : [ "S. Rangan", "A. Fletcher", "V.K. Goyal" ],
      "venue" : "IEEE Trans. Inform. Theory, vol. 58, no. 3, pp. 1902–1923, Mar. 2012.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1902
    }, {
      "title" : "Iterative estimation of constrained rank-one matrices in noise",
      "author" : [ "S. Rangan", "A.K. Fletcher" ],
      "venue" : "Proc. IEEE Int. Symp. Inform. Theory, Cambridge, MA, Jul. 2012.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Compressed sensing over `p-balls: Minimax mean square error",
      "author" : [ "D. Donoho", "I. Johnstone", "A. Maleki", "A. Montanari" ],
      "venue" : "Proc. ISIT, St. Petersburg, Russia, Jun. 2011.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Neural reconstruction with approximate message passing (NeuRAMP)",
      "author" : [ "A.K. Fletcher", "S. Rangan", "L. Varshney", "A. Bhargava" ],
      "venue" : "Proc. Neural Information Process. Syst., Granada, Spain, Dec. 2011.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distribution of eigenvalues for some sets of random matrices",
      "author" : [ "V.A. Marčenko", "L.A. Pastur" ],
      "venue" : "Math. USSR–Sbornik, vol. 1, no. 4, pp. 457– 483, 1967.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 1967
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]–[5].",
      "startOffset" : 253,
      "endOffset" : 256
    }, {
      "referenceID" : 1,
      "context" : "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]–[5].",
      "startOffset" : 258,
      "endOffset" : 261
    }, {
      "referenceID" : 2,
      "context" : "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]–[5].",
      "startOffset" : 380,
      "endOffset" : 383
    }, {
      "referenceID" : 4,
      "context" : "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]–[5].",
      "startOffset" : 384,
      "endOffset" : 387
    }, {
      "referenceID" : 5,
      "context" : "Also, since the parameters in the output transfer function PY |Z can model unknown nonlinearities, this problem formulation can be applied to the identification of linear-nonlinear cascade models of dynamical systems, in particular for neural spike responses [6]–[8].",
      "startOffset" : 259,
      "endOffset" : 262
    }, {
      "referenceID" : 7,
      "context" : "Also, since the parameters in the output transfer function PY |Z can model unknown nonlinearities, this problem formulation can be applied to the identification of linear-nonlinear cascade models of dynamical systems, in particular for neural spike responses [6]–[8].",
      "startOffset" : 263,
      "endOffset" : 266
    }, {
      "referenceID" : 8,
      "context" : "In recent years, there has been significant interest in so-called approximate message passing (AMP) and related methods based on Gaussian approximations of loopy belief propagation (LBP) [9]–[18].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "In recent years, there has been significant interest in so-called approximate message passing (AMP) and related methods based on Gaussian approximations of loopy belief propagation (LBP) [9]–[18].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "These methods originate from CDMA multiuser detection problems in [9]–[11], and have received considerable recent attention in the context of compressed sensing [13]–[17].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "These methods originate from CDMA multiuser detection problems in [9]–[11], and have received considerable recent attention in the context of compressed sensing [13]–[17].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : "These methods originate from CDMA multiuser detection problems in [9]–[11], and have received considerable recent attention in the context of compressed sensing [13]–[17].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 16,
      "context" : "These methods originate from CDMA multiuser detection problems in [9]–[11], and have received considerable recent attention in the context of compressed sensing [13]–[17].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 18,
      "context" : "See, also the survey article [19].",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "The Gaussian approximations used in AMP are also closely related to standard expectation propagation techniques [20], [21], but with additional simplifications that exploit the linear coupling between the variables x and z.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "The Gaussian approximations used in AMP are also closely related to standard expectation propagation techniques [20], [21], but with additional simplifications that exploit the linear coupling between the variables x and z.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 11,
      "context" : "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 15,
      "context" : "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].",
      "startOffset" : 235,
      "endOffset" : 239
    }, {
      "referenceID" : 16,
      "context" : "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 17,
      "context" : "This paper considers the so-called generalized AMP (GAMP) method of [18] that extends the algorithm in [13] to arbitrary output distributions PY |Z (many original formulations assumed additive white Gaussian noise (AWGN) measurements).",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "This paper considers the so-called generalized AMP (GAMP) method of [18] that extends the algorithm in [13] to arbitrary output distributions PY |Z (many original formulations assumed additive white Gaussian noise (AWGN) measurements).",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "To overcome this limitation, Vila and Schniter [22], [23] and Krzakala et al.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "To overcome this limitation, Vila and Schniter [22], [23] and Krzakala et al.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : "[24], [25] have recently proposed extension of AMP and GAMP based on Expectation Maximization (EM) that enable joint learning of the parameters (λx, λz) along with the estimation of the vector x.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[24], [25] have recently proposed extension of AMP and GAMP based on Expectation Maximization (EM) that enable joint learning of the parameters (λx, λz) along with the estimation of the vector x.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "• Generalization of the GAMP method of [18] to a class of algorithms we call adaptive GAMP that enables joint estimation of the parameters λx and λz along with vector x.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "In addition, the adaptive GAMP methods include the EM-GAMP algorithms of [22]–[25] as special cases.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "In addition, the adaptive GAMP methods include the EM-GAMP algorithms of [22]–[25] as special cases.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]– [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]– [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]– [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]– [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "As mentioned above, the adaptive GAMP method proposed here can be seen as a generalization of the EM methods in [22]–[25].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "As mentioned above, the adaptive GAMP method proposed here can be seen as a generalization of the EM methods in [22]–[25].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 25,
      "context" : "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 23,
      "context" : "A related EM-GAMP algorithm has also appeared in [24], [25] for the case of certain sparse priors and AWGN outputs.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "A related EM-GAMP algorithm has also appeared in [24], [25] for the case of certain sparse priors and AWGN outputs.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "Simulations in [22], [23] show remarkably good performance and computational speed for EM-GAMP over a wide class of distributions, particularly in the context of compressed sensing.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 22,
      "context" : "Simulations in [22], [23] show remarkably good performance and computational speed for EM-GAMP over a wide class of distributions, particularly in the context of compressed sensing.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "Also, using arguments from statistical physics, [24], [25] presents state evolution (SE) equations for the joint evolution of the parameters and vector estimates and confirms them numerically.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 24,
      "context" : "Also, using arguments from statistical physics, [24], [25] presents state evolution (SE) equations for the joint evolution of the parameters and vector estimates and confirms them numerically.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "In particular, the current work provides a rigorous justification of the SE analysis in [24], [25] along with extensions to more general input and output channels and adaptation methods.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "In particular, the current work provides a rigorous justification of the SE analysis in [24], [25] along with extensions to more general input and output channels and adaptation methods.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 23,
      "context" : "However, the methodology in [24], [25] in other ways is more general in that it can also study “seeded” or “spatially-coupled” matrices as proposed in [24], [25], [27].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 24,
      "context" : "However, the methodology in [24], [25] in other ways is more general in that it can also study “seeded” or “spatially-coupled” matrices as proposed in [24], [25], [27].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "However, the methodology in [24], [25] in other ways is more general in that it can also study “seeded” or “spatially-coupled” matrices as proposed in [24], [25], [27].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 24,
      "context" : "However, the methodology in [24], [25] in other ways is more general in that it can also study “seeded” or “spatially-coupled” matrices as proposed in [24], [25], [27].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 26,
      "context" : "However, the methodology in [24], [25] in other ways is more general in that it can also study “seeded” or “spatially-coupled” matrices as proposed in [24], [25], [27].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 27,
      "context" : "An alternate method for joint learning and estimation has been presented in [28], which assumes that the distributions on the source and output channels are themselves described by graphical models with the parameters λx and λz appearing as unknown variables.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 27,
      "context" : "The method in [28], called Hybrid-GAMP, iteratively combines standard loopy BP with AMP methods.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 28,
      "context" : "This minimax approach [29] was proposed for AMP recovery of sparse signals in [13].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 12,
      "context" : "This minimax approach [29] was proposed for AMP recovery of sparse signals in [13].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "Indeed, this gap was the main justification of the EMGAMP methods in [22], [23].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "Indeed, this gap was the main justification of the EMGAMP methods in [22], [23].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "A conference version of this paper has appeared in [30].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "Before describing the adaptive GAMP algorithm, it is useful to review the basic (non-adaptive) GAMP algorithm of [18].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "The GAMP algorithm of [18] can be seen as a class of methods for estimating the vectors x and z for the case when the parameters λx and λz are known.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "As described in [18], there are two important sets of choices for the estimation functions, resulting in two variants of GAMP:",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 30,
      "context" : "The operations are similar in form to separable and proximal minimization methods widely used for such problems [31]– [35].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 33,
      "context" : "The operations are similar in form to separable and proximal minimization methods widely used for such problems [31]– [35].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "More details, as well as the equations for max-sum GAMP can be found in [18].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "By now, there are a large number of SE results for AMP-related algorithms [9], [11]–[18].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "By now, there are a large number of SE results for AMP-related algorithms [9], [11]–[18].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "By now, there are a large number of SE results for AMP-related algorithms [9], [11]–[18].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "Here, we review the particular SE analysis from [18] which is based on the framework in [16].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : "Here, we review the particular SE analysis from [18] which is based on the framework in [16].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "of Bayati and Montanari’s analysis in [16].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "The main result of [18] shows that if we fix the iteration t, and let n→∞, the asymptotic joint empirical distribution of the components of these two sets θ x and θ t z converges to random vectors of the form",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "With these definitions, we can state the main result from [18].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "Theorem 1 ( [18]): Consider the random vectors θ x and θ t z generated by the outputs of GAMP under Assumption 1.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 34,
      "context" : "This scalar equivalent model appears in several analyses and can be thought of as a single-letter characterization [36] of the system.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]–[40].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 35,
      "context" : "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]–[40].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 38,
      "context" : "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]–[40].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : "As described in the previous section, the standard GAMP algorithm of [18] considers the case when the parameters λx and λz in the distributions in (1) are known.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "It is useful to briefly compare the above ML parameter estimation with the EM-GAMP method proposed by Vila and Schniter in [22], [23] and Krzakala et.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "It is useful to briefly compare the above ML parameter estimation with the EM-GAMP method proposed by Vila and Schniter in [22], [23] and Krzakala et.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "in [24], [25].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "in [24], [25].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "For the sum-product AMP or GAMP algorithms, it is shown in [18] that the SE equations simplify so that α r = 1 and ξ r = τ t r, if the parameters were selected correctly.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "Some justification for this last step can be found in [11], [12], [17].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "Some justification for this last step can be found in [11], [12], [17].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : "Some justification for this last step can be found in [11], [12], [17].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "In [22], [23], the parameter update (17) is performed only once every few iterations to allow P̂ t to converge to the approximation of the posterior distribution of xj given the current parameter estimates.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "In [22], [23], the parameter update (17) is performed only once every few iterations to allow P̂ t to converge to the approximation of the posterior distribution of xj given the current parameter estimates.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 23,
      "context" : "In [24], [25], the parameter estimate is updated every iteration.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "In [24], [25], the parameter estimate is updated every iteration.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 21,
      "context" : "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 39,
      "context" : "The proof is straightforward and is based on a continuity argument also used in [41].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 40,
      "context" : "It is known that estimators such as LASSO offer certain optimal minmax performance over a large class of sparse distributions [43].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 21,
      "context" : "This gap was the main motivation for [22], [23] which showed large gains of the EMGAMP method due to its ability to learn the prior.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "This gap was the main motivation for [22], [23] which showed large gains of the EMGAMP method due to its ability to learn the prior.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "where Λx = [0, 1]× [0,+∞).",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "As a second example, we consider the estimation of the linear-nonlinear-Poisson (LNP) cascade model [8].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 41,
      "context" : "AMP techniques combined with the parameter estimation have been recently proposed for neural receptive field estimation and connectivity detection in [44].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 21,
      "context" : "The procedure, which is a generalization of EMGAMP methodology of [22]–[25] that estimates both the vector x as well as parameters in the source and componentwise output transform.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "The procedure, which is a generalization of EMGAMP methodology of [22]–[25] that estimates both the vector x as well as parameters in the source and componentwise output transform.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "As described in [18], the sum-product estimation can be implemented with the estimation functions",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "The paper [18] shows that the derivatives of these estimation functions for lines 9 and 16 are computed via the variances:",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 17,
      "context" : "A key result of [18] is that, when the parameters are set to the true values (i.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "Bayati and Montanari’s analysis in [16] employs certain deterministic models on the vectors and then proves convergence properties of related empirical distributions.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 39,
      "context" : "The proof follows along the adaptation argument of [41].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "This non-adaptive algorithm is precisely the standard GAMP method analyzed in [18].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 42,
      "context" : "components with zero mean and variance 1/m, it follows from the Marčenko-Pastur Theorem [45] that that its 2-norm operator norm is bounded.",
      "startOffset" : 88,
      "endOffset" : 92
    } ],
    "year" : 2012,
    "abstractText" : "We consider the estimation of an i.i.d. (possibly non-Gaussian) vector x ∈ R from measurements y ∈ R obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. A novel method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector x is presented. The proposed algorithm is a generalization of a recently-developed EM-GAMP that uses expectationmaximization (EM) iterations where the posteriors in the E-steps are computed via approximate message passing. The methodology can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes. We prove that for large i.i.d. Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. In addition, we show that when a certain maximum-likelihood estimation can be performed in each step, the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values. Remarkably, this result applies to essentially arbitrary parametrizations of the unknown distributions, including ones that are nonlinear and non-Gaussian. The adaptive GAMP methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.",
    "creator" : "LaTeX with hyperref package"
  }
}