{
  "name" : "1701.01722.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Follow the Compressed Leader: Faster Algorithms for Matrix Multiplicative Weight Updates",
    "authors" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
    "emails" : [ "zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ d\nworse than the optimal regret of MMWU for dimension-d matrices. In this paper, we propose a followed-the-compressed-leader framework which, not only matches the optimal regret of MMWU (up to polylog factors), but runs even faster than FTPL. Our main idea is to “compress” the matrix exponential computation to dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting. This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16].\nar X\niv :1\n70 1.\n01 72\n2v 1\n[ cs\n.L G\n] 6\nJ an"
    }, {
      "heading" : "1 Introduction",
      "text" : "The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]). Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].\nTo discuss MMWU on an abstract level, let us consider the online linear optimization problem.\nOnline Matrix Optimization. Let ∆d = {U ∈ Rd×d |TrU = 1 ∧ U 0} be the set of density matrices in dimension d.1 Consider the following online game between a player and an adversary. The player plays T actions W1, . . . ,WT ∈ ∆d in a row; after playing Wk, the adversary picks a feedback matrix Ak ∈ Rd×d that is symmetric and satisfies 0 A I (both these assumptions are for the sake of simplicity and can be removed2); this Ak may depend on W1, . . . ,Wk. The player receives a gain Ak •Wk def= Tr(AkWk) ∈ [0, 1]. The regret minimization problem asks us the player to design a strategy to minimize regret, that is, the difference between the total gain obtained by the player and that by the a posteriori best fixed strategy U ∈ ∆d:\nminimize max U∈∆d\n∑T k=1Ak • (U−Wk) = λmax ( A1 + · · ·+ AT )− ∑T k=1Ak •Wk .\nThe MMWU strategy chooses Wk = exp(−ηΣk−1) Tr exp(−ηΣk−1) where Σk−1 def = A1 + · · · + Ak−1 and η >\n0 is the so-called learning rate. The best choice η = √ log d/ √ T yields a total regret at most O( √ T log d) [26], and this is optimal up to constant [9]. Some authors also refer to MMWU as the follow-the-regularized-leader strategy or FTRL for short, because MMWU can be analyzed from a mirror-descent view with the matrix entropy function as its regularizer [7].\nOnline Eigenvector Problem. If instead of playing an arbitrary matrix in ∆d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:\nminimize λmax ( A1 + · · ·+ AT )− ∑T k=1w > k Akwk .\nThe name comes from the fact that the player chooses only vectors in a row, but wants to compete against the leading eigenvector in hindsight. To make this problem meaningful, the feedback matrix Ak, is not allowed to depend on wk but can depend on w1, . . . , wk−1. This more challenging setting is very desirable for multiple reasons:\n• in many applications —such as graph problems [7, 22]— Ak does not depend on wk; • vector-based strategies wk can be cheaper to compute and more efficient to communicate. • as we shall see next, online eigenvector is more general than online matrix optimization because\nthe player can achieve the same regret by playing a full rank matrix Wk or by simply playing wkw > k , as long as Ak does not depend on Wk.\n1We denote by A B spectral dominance that is equivalent to saying that A−B is positive semidefinite (PSD). 2Firstly, all the results cited and stated in this paper, after scaling, generalize to the scenario when the eigenvalues of Ak are in the range [l, r] for arbitrary l, r ∈ R. For notational simplicity, we have assumed l = 0 and r = 1 in this paper. Secondly, if Ak is not symmetric or even rectangular, classical reductions can turn such a problem into an equivalent online game with only symmetric matrices [16, Sec 2.1].\naThe total time complexity of the first Tε rounds where Tε is the earliest round to achieve an ε average regret.\nKnown Approach 1 (Better Regret). Applying the same MMWU strategy but randomly yields the same total O( √ T log d) regret for the online eigenvector problem. Indeed, given an eigendecomposition Wk = exp(−ηΣk−1) Tr exp(−ηΣk−1) = ∑d\nj=1 pj · yjy>j where vectors yj are normalized eigenvectors, the player can play each yj with probability pj . This gives O( √ T log d) total regret in expectation.3 Unfortunately, the per-iteration running time of this method is O(d3) due to eigendecomposition.4\nSome researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable. They define a sketch matrix Y = W 1/2 k Q using a random Q ∈ Rd×m and then use YY> to approximate Wk. The average regret loss is σ if the dimension m = Õ(1/σ2). We call this method MMWU-JL for short. Since σ must be around T−1/2, MMWU-JL still runs slowly per iteration, see Table 1.5\nKnown Approach 2 (Faster Run Time). Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21]. Most notably, Garber, Hazan and Ma [16] proposed to compute an (approximate) leading eigenvector of the matrix Σk−1 +rr> at iteration k, where r is a random vector whose norm is carefully chosen. They showed that the total regret of FTPL is Õ( √ dT ), which is interesting only when T ≥ d and is a factor √ d worse than the optimum regret. The per-iteration cost of FTPL is only Õ ( T 3 4d− 1 4 nnz(ΣT ) ) because computing the leading eigenvector is faster than eigendecomposition. (We use nnz(M) to denote the time needed to multiply M to a vector.)"
    }, {
      "heading" : "1.1 Our Main Results",
      "text" : "We propose a follow-the-compressed-leader (FTCL) strategy that, at a high level, compresses the MMWU strategy to dimension m = 3 as opposed to dimension m = Θ̃(1/σ2) = Θ̃(T ) in MMWU-\n3It requires some additional, but standard, effort to turn this into a high-confidence result. 4The best eigendecomposition algorithm runs in time O(d3) as opposed to O(dω). 5More specifically, the computation W\n1/2 k Q becomes m computations of exp(−ηΣk−1/2) applied to vectors, and\nthis can be done using Chebyshev approximation with a total running time Õ ( σ−2 · ‖ηΣk−1‖1/22 · nnz(ΣT ) ) . The optimal choice η will imply ‖ηΣk−1‖2 ≤ T 1/2, so this totals to Õ(T 5/4nnz(ΣT )) in the per-iteration running time.\nJL. Our FTCL strategy has significant advantages over previous results because:\n• FTCL has regret Õ( √ T ) which is optimal up to poly-log factors (as opposed to √ d in FTPL). • FTCL, in its basic form, has a per-iteration complexity Õ ( T 1 4 nnz(ΣT ) ) which is already faster\nthan MMWU-JL by a factor Ω̃(T ) and than FTPL by a factor Ω̃(T 1/2d−1/4) ≥ Ω̃(T 1/4). • FTCL, after using optimization techniques to speed it up, has a per-iteration complexity\nÕ ( T 1 4 nnz(ΣT ) 3 4 nnz(A) 1 4 + nnz(ΣT ) ) , where nnz(A) def = maxi∈[T ]{nnz(Ai)} ≤ nnz(ΣT ) .\nWe compare our running time to known results in Table 1 in full. We stress here that a direct comparison in per-iteration complexity between FTCL (or MMWU) and FTPL is unfair, because FTPL requires d times more iterations in order to achieve the same average regret as FTCL (or MMWU). For this reason, in the last column of Table 1, we also summarize the minimum total time complexity needed to achieve an ε average regret.\nExamples. If nnz(ΣT ) = d 2 and nnz(A) = O(d), the per-iteration running time comparison is\nÕ(d2+d1.75T 0.25) (by us) vs. Õ(T 0.75d1.75) (by FTPL, only T ≥ d) or O(d3)(by MMWU) . We also compare the total complexity needed to achieve an ε average regret:\nÕ(d2ε−2 + d1.75ε−2.5) (by us) vs. Õ(d2ε−4.5) (by MMWU-JL) or Õ(d3ε−2) (by MMWU) ."
    }, {
      "heading" : "1.2 Our Side Result: Stochastic Online Eigenvector",
      "text" : "Our compression idea also gives rise to a faster algorithm for the special case of the online eigenvector problem where the adversary is stochastic, meaning that A1, . . . ,AT are chosen i.i.d. from a common distribution whose expectation equals some matrix B, independent of the player’s actions.\nFor this problem, Garber, Hazan, and Ma [16] showed that a block power method matches the optimum regret and enjoys an efficient O(nnz(ΣT ))-time implementation per iteration. Shamir [30] analyzed the so-called Oja’s algorithm but his total regret is O( √ dT log(T )) which is a factor √ d worse than optimum.6\nIn this paper, we show by showing that Oja’s algorithm in fact only has a total regretO( √ T log d)\nfor the stochastic online eigenvector problem, which is optimal up to a √\nlog d factor. Most importantly, the per-iteration complexity of Oja’s is only O(nnz(A)).\nExample. If nnz(ΣT ) = d 2 and nnz(A) = O(d), our running time is O(d) times faster than [16].\nOur proof relies on a FTCL view of Oja’s algorithm which compresses MMWU to dimension m = 1. Our proof is less than one page, and essentially three-lined. This indicates that FTCL might be a better way of thinking about these type of problems."
    }, {
      "heading" : "1.3 Our Stronger Results in a More Refined Language",
      "text" : "Denoting by λ def = 1T λmax(A1 + · · ·AT ), we have λ ≤ 1 according to the normalization Ak I. In general, the smaller the value λ is, the better a learning algorithm should behave. In the previous subsections, we have followed the tradition and discussed our results and prior works assuming the worst possibility of λ. This has indeed simplified notations.\n6In the special case of Ak being rank-1, the Õ( √ T ) regret for Oja’s algorithm was recently shown by [6], using\ndifferent techniques from us.\nIf λ is much smaller than 1, our complexity bounds can be improved to quantities that depend on λ. We call this the λ-refined language. We restate our FTCL results for this language in Table 2. At a high level, for our FTCL, in both the adversarial and stochastic settings,\n• the total regret formula improves from Õ( √ T ) to Õ( √ λT ); and\n• for obtaining the same average regret, – the necessary number of iterations reduces by a factor λ; and – the total running time improves by a factor between λ and λ3/2.\nWe emphasize that there is an information-theoretic lower bound of Ω( √ λT ) for the total regret in this λ-refined language, see Appendix I. This lower bound even holds for the simpler stochastic online eigenvector problem, even when the matrices Ak are of rank 1.\nAs for prior works, it has been recorded that (cf. [7, Theorem 3.1]) the MMWU method (or the MMWU-JL) has a total regret of only O( √ λT log d) as opposed to O( √ T log d) in this λrefined language. The running time of MMWU therefore improves by a factor of λ. The total time complexity of MMWU-JL improves only by a factor of λ1/4.7\nThe block power method (for the stochastic online eigenvector problem) can also be analyzed in this λ-refined language, for instance by modifying the proof in [16]. This improves the total regret to Õ( √ λT ). To the best of our knowledge, FTPL has not been analyzed in the λ-refined language (and even if it has, the resulting time complexity must be outperformed by ours). We compare our results with prior work in Table 2 for this λ-refined language."
    }, {
      "heading" : "1.4 Other Related Works",
      "text" : "For the online eigenvector problem, if the feedback matrices Ak are only of rank-1, then the Õ( √ dT ) total regret of FTPL can be improved to Õ(d1/4T 1/2). This is first shown by Dwork et al. [13] and independently shown by Kot lowski and Warmuth [21]. However, this d1/4 factor for the rank-1 case and the d1/2 factor for the high-rank case are tight at least for their proposed FTPL methods [18]. Abernethy et al. showed that FTPL strategies can also be analyzed using a FTRL framework [1].\n7This is so because, in the same notations of Footnote 5, the per-iteration running time is Õ ( σ−2 · ‖ηΣk−1‖1/22 ·\nnnz(ΣT ) ) . This time, the optimal choice η will imply ‖ηΣk−1‖2 ≤ (λT )1/2 which is λ1/4 smaller than before; however, the error tolerance σ must satisfy σ2 ≈ T/λ, so this totals to a per-iteration complexity of Õ(T 5/4λ−3/4nnz(ΣT )).\nResearchers also put efforts to understand high-rank variants of the online eigenvector problem. Nie et al. studied the high-rank variant using MMWU [25], but their per-iteration running time is still O(d3) due to eigendecomposition. Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k · poly(1/ε)) vectors instead of k but with a good PCA reconstruction error.\nThe stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17].8 In streaming PCA, we are given i.i.d. random matrices with an expectation B and asked to approximately find a unit vector w with large w>Bw in the end. The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.\nFor the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition)."
    }, {
      "heading" : "1.5 Roadmap",
      "text" : "We introduce necessary notations in Section 2, and discuss the high-level difficulties and our techniques in Section 3. We introduce a new trace inequality in Section 4 that shall be used in our main proof. In Section 5 we prove our main FTCL result for an oblivious adversary, and then extend it to the adversarial setting in Section 6. We discuss how to implement FTCL fast in Section 7. Finally, in Section 8 we provide our FTCL result for a stochastic adversary.\nAll of our results are stated and proved directly in the λ-refined language."
    }, {
      "heading" : "2 Notations and Preliminaries",
      "text" : "Define Σk def = ∑k\ni=1 Ai for every k = 0, 1, . . . , T . Since each Ak is positive semi-definite (PSD), we can find Pk ∈ Rd×d such that Ak = PkP>k ; we only use Pk for analysis purpose only. Given two matrices A,B ∈ Rd×d, we write A •B def= Tr(A>B). We write A B if A,B are symmetric matrices and A − B is PSD. We write [A]i,j the (i, j)-th entry of A. We use nnz(M) to denote time needed to multiply matrix M ∈ Rd×d with an arbitrary vector in Rd. In particular, nnz(M) is at most d plus the number of non-zero elements in M. We denote nnz(A) def = maxk∈[T ] { nnz(Ak) } .\nSuppose x1, · · · , xt ∈ R are drawn i.i.d. from the standard Gaussian N (0, 1), then χ = ∑t i=1 x 2 i has a chi-squared distribution of t-degree freedom. χ−1 is called inverse-chi-squared distribution of t-degree freedom. It is known that E[χ−1] = 1t−2 for t ≥ 3.\nFor a polynomial f : R → R, we use f (k) to denote the k-th order derivative of f at point x. We use Markov brothers’ inequality: if polynomial f is of degree n, then ∀k ∈ N∗ and ∀a > 0:\nmax x∈[0,a]\n|f (k)(x)| ≤ ( 2\na )i n2(n2 − 12)(n2 − 22) . . . (n2 − (k − 1)2) (2k − 1)!! maxx∈[0,a] |f(x)| . (2.1)"
    }, {
      "heading" : "3 High-Level Discussion of Our Techniques",
      "text" : "Let us first revisit the high-level idea behind MMWU. Recall Wk = exp(ckI + ηΣk−1) where ck is the unique constant such that TrWk = 1. Now, the key idea behind the analysis of MMWU is to use the Golden-Thompson inequality:\nTr ( eckI+ηΣk ) ≤ Tr ( eckI+ηΣk−1eηAk ) = Tr ( Wke ηAk ) ≈ Tr ( eckI+ηΣk−1 ) + ηWk •Ak .\n8This “equivalence” is not a black-box reduction; one usually needs open up the analysis and turn the solution of one to the other.\nIn other words, the gain value Wk •Ak, up to a factor η, is proportional to the change of the trace function. One can also use convexity to show Tr ( eck+1I+ηΣk ) − Tr ( eckI+ηΣk ) ≤ ck+1 − ck. Using these two inequalities plus a little more work, one can obtain the total regret bound. In the rest of this section, let us perform a thought experiment to “modify” the MMWU analysis step-by-step. In the end, our FTCL method and its intuition shall become clear to the reader.\nThinking Step 1. Choose a random Gaussian vector u ∈ Rd and “compress” MMWU to dimension 1 in the direction of u. More specifically, we define Wk = exp(ckI + ηΣk−1) but this time ck is the unique constant such that Tr(Wkuu >) = u>Wku = 1. In such a case, we wish to say that\nTr ( eckI+ηΣkuu> ) = Tr ( eckI+ηΣk−1+ηAkuu> ) (?) ≤ Tr ( e(ckI+ηΣk−1)/2uu>e(ckI+ηΣk−1)/2eηAk )\n= Tr ( W\n1/2 k uu >W1/2k e ηAk ) ≈ Tr(Wkuu>) + ηW1/2k uu>W 1/2 k •Ak .\nIf the above inequality were true, then we could define wk def = W 1/2 k u which is a unit vector (because Tr(Wkuu >) = 1) and the gain w>k Akwk = wkw > k •Ak would again be proportional to the change\nof this new potential function Tr ( eckI+ηΣk−1uu> ) . This idea almost worked except that inequality (?) is false due to the non-commutativity of matrices.9\nPerhaps the most “immediate” idea to fix this issue is to use the randomness of uu>. Recall that E[uu>] can be made I and therefore it “seems like” we have E[Tr(Wkuu>)] = Tr(Wk) and the inequality will go through. Unfortunately, this idea fails for a fundamental reason: the normalization constant ck depends on u, so Wk is not independent from the randomness of u. 10\nThinking Step 2. Since Gaussian vectors are rotationally invariant, we temporarily switch to the eigenbasis of Σk−1 so Wk is a diagonal matrix. We make an important observation:11\nck depends only on |u1|, . . . , |ud|, but not on the 2d possible signs of u1, . . . , ud. For this reason, we can fix a diagonal matrix D and consider all random uu> which agrees with D on its diagonal,12 All of such vectors u give the same normalization constant ck, and it satisfies E[uu>|D] = D. This implies that we can now study the conditional expected potential change\nE [ Tr ( eckI+ηΣkuu> ) −Tr ( eckI+ηΣk−1uu> )∣∣D ] = Tr ( eckI+ηΣkD ) −Tr ( eckI+ηΣk−1D ) ,\nor if we denote by B = ckI + ηΣk−1, we want to study the difference Tr ( eB+ηAkD ) − Tr ( eBD ) only in the special case that D and B are simultaneously diagonalizable. Thinking Step 3. A standard way to bound Tr ( eB+ηAkD ) − Tr ( eBD ) is to define f(η) def = Tr ( eB+ηAkD ) and bound f(η) by its Taylor series f(0) + f ′(0)η + 12f\n′′(0)η2 + · · · . The zero-order derivative f(0) is Tr ( eBD ) . The first-order derivative f ′(0) = Tr(AkeBD) = eB/2DeB/2 • Ak behaves exactly in the way we hope, and this strongly relies on the commutativity between B and D. Unfortunately, higher-order derivatives f (k)(0) benefit less and less from the commutativity between B and D due to the existence of terms such as Ake\nBDeBAkD. For this reason, we need to (1) truncate the Taylor series and (2) use different analytic tools. This motivates us to use the following regime that can be viewed as a “low-degree” version of MMWU:\nA Quick Detour. In a recent result, the authors of [7] generalized MMWU to `1−1/q regularized strategies. For every q ≥ 2, they define Xk = (ckI−ηΣk−1)−q where ck is the unique constant such\n9A analogy for this effect can be found in the inequality Tr(eA) ≤ Tr(eB) for every A B. This inequality becomes false when multiplied with uu> and in general eA eB is false.\n10In fact, ck can be made almost independent from u if we replace uu > with some QQ> where Q is a random\nd×m matrix for some very large m. That was the main idea behind MMWU-JL. 11This is because, Tr(eckI−ηΣk−1uu>) = ∑d i=1 ( |ui|2/(ck − ηλi) ) where λi is the i-th eigenvalue of Σk−1.\n12That is, all random uu> such that ‖ui‖22 = Di,i for each i ∈ [d]. For simplicity we also denote this event as D.\nthat ckI−ηΣk−1 0 and TrXk = 1.13 This is a generalization of MMWU because when q ≈ log d, the matrix Xk behaves nearly the same as Wk; in particular, it gives the same regret bound. The analysis behind this new strategy is to keep track of the potential change in Tr ( (ckI−ηΣk−1)−(q−1) ) , and then use the so-called Lieb-Thirring inequality (see Section 4) to replace the use of GoldenThompson. (Note that ck is choosen with respect to q but the potential is with respect to q − 1.) Thinking Step 4. Let us now replace MMWU strategies in our Thinking Steps 1,2,3 with `1−1/q regularized strategies. Such strategies have two advantages: (1) they help us overcome the issue for higher-order terms in Thinking Step 3, and (2) matrix inversions are more efficient than matrices exponentials in terms of computation. We shall choose q = Θ(log(dT )) in the end.\nSpecifically, we prepare a random vector u and define the normalization constant ck to be the unique one satisfying Tr ( (ckI−ηΣk−1)−quu> ) = Tr(Xkuu >) = 1. At iteration k, we let the player choose strategy X 1/2 k u which is a unit vector.\nIf one goes through all the math carefully (using Woodbury formula), this time we are entitled to upper bound the trace difference of the form Tr ( (B + ηC)q−1D ) − Tr ( Bq−1D ) where D is simultaneously diagonalizable with D but not C. Similar to Thinking Step 3, we can define f(η) def = Tr ( (B + ηC)q−1D ) and bound this polynomial f(η) using its Taylor expansion at point 0. Commutativity between B and D helps us compute f ′(0) = (q − 1)Tr(Bq−2CD) but again we cannot bound higher-derivatives directly. Fortunately, this time f(η) is a degree q − 1 polynomial so we can use Markov brothers’ inequality to give an upper bound on its higher-order terms. This is the place we lose a few extra polylogarithmic factors in the total regret.\nThinking Step 5. Somehow necessarily, even the second-order derivative f ′′(0) can depend on terms such as 1/Dii where Dii = |ui|2 is the i-th diagonal entry of D. This quantity, over the Gaussian random choice of ui, does not have a bounded mean. More generally, the inverse chisquared distribution with degree t (recall Section 2) has a bounded mean only when t ≥ 3. For this reason, instead of picking a single random vector u ∈ Rd, we need pick three random vectors u1, u2, u3 ∈ Rd and replace all the occurrences of uu> with 13 ( u1u > 1 +u2u > 2 +u3u > 3 ) in the previous thinking steps. As a result, each Dii becomes a chi-squared distribution of degree 3 so the issue goes away. This is why we claimed in the introduction that\nwe can compress MMWU to dimension 3.\nRemark. By losing a polylog factor in regret, one can compress it further to dimension 2. This is because the mean of the inverse chi-squared distribution with degree 2, if truncated at some large value v, is only log(v). However, this “truncated mean” becomes Ω( √ v) for degree 1.\nThinking Step 6. Putting together previous steps, we obtain a FTCL strategy with total regret O( √ T log3(dT )), which is worse than MMWU only by a factor O(log2.5(dT )). We call this method FTCLobl and include its analysis in Section 5. However, FTCLobl only works for an oblivious adversary (i.e., when A1, . . . ,AT are fixed a priori) and gives an expected regret. To turn it into a robust strategy against adversarial A1, . . . ,AT , and to make the regret bound work with high confidence, we need to re-sample u1, u2, u3 every iteration. We call this method FTCL\nadv. A careful but standard analysis with Azuma inequality helps us reduce FTCLadv to FTCLobl. We state this result in Section 6.\nRunning Time. As long as q is an even integer, the computation of “(ckI − ηΣk−1)−1 applied to a vector” becomes the bottleneck of each iteration of FTCLobl and FTCLadv. However, as long\n13The name of such strategies come from the following fact. Recall that MMWU naturally arises as the strategy in follow-the-regularized-leader when the regularizer is the matrix entropy. If that entropy function is replaced with a negative `1−1/q norm, the resulting strategy becomes the so-defined matrix Xk. We encourage interested readers to see the introduction of [7] for more background information, but we shall make this present paper self-contained.\nas q ≥ Ω(log(dT )), we show that the condition number of the matrix ckI − ηΣk−1 is at most ηT = Θ(T 1/2). Using conjugate gradient, we can compute this inversion in time Õ(T 1/4) times O(nnz(Σk−1)). This gives the FTCL (basic) running time in Table 1. As for the faster FTCL (opt) running time, one need to use more advanced optimization tools —namely, accelerated variance reduction— to perform inversion. We discuss the details in Section 7.\nCompress MMWU to Dimension 1 in Stochastic Online Eigenvector. If the adversary is stochastic, we observe that Oja’s algorithm corresponds to a potential function Tr ( (I+ηAk) · · · (I+ ηA1)uu >(I + ηA1) · · · (I + ηAk) ) . Because the matrices are drawn from a common distribution, this potential behaves similar to the matrix exponential but compressed to dimension 1, namely Tr ( eη(A1+···+Ak)uu> ) . In fact, just using linearity of expectation carefully, one can both upper and lower bound this potential. We state this result in Section 8 (and it can be proved in one page!)"
    }, {
      "heading" : "4 A New Trace Inequality",
      "text" : "Prior work on MMWU and its extensions relies heavily on one of the following trace inequalities [7]:\nGolden-Thompson inequality : Tr(eA+ηB) ≤ Tr ( eAeηB )\nLieb-Thirring inequality : Tr ( (A + ηB)k ) ≤ Tr ( Ak/2(I + ηA−1/2BA−1/2)kAk/2 ) .\nDue to our compression framework in this paper, we need inequalities of type\n“ Tr(eA+ηBD) ≤ Tr ( eηBeA/2DeA/2 ) ” “ Tr ( (A + ηB)kD ) ≤ Tr ( (I + ηA−1/2BA−1/2)kAk/2DAk/2 ) . ” (4.1)\nwhich look almost like “generalizations” of Golden-Thompson and Lieb-Thirring (by seeting D = I). Unfortunately, such generalizations do not hold for an arbitrary D. For instance, if the first “generalization” holds for every PSD matrix D then it would imply “ eA+ηB eA/2eηBeA/2 ” which is a false inequality due to matrix non-commutativity.\nIn this paper, we show that if D is commutative with A, then the “generalization” (4.1) holds for the zeroth and first order terms with respect to η. As for the second and higher order terms, we can control it using Markov brothers’ inequality. (Proof in Appendix A.)\nLemma 4.1. For every symmetric matrices A,B,D ∈ Rd×d, every integer k ≥ 1, every η∗ ≥ 0, and every η ∈ [0, η∗/k2], if A and D are commutative, then\n(A + ηB)k •D−Ak •D ≤ kηB •Ak−1D + ( ηk2\nη∗\n)2 max\nη′∈[0,η∗]\n{∣∣(A + η′B)k •D−Ak •D ∣∣ } ."
    }, {
      "heading" : "5 Oblivious Online Eigenvector + Expected Regret",
      "text" : "In this section we first focus on a simpler oblivious setting. A1, . . . ,AT are T PSD matrices chosen by the adversary in advance, and they do not depend on the player’s actions in the T iterations. We are interested in upper bounding the total expected regret\nλmax (∑T k=1 Ak ) −∑Tk=1 E[w>k Akwk] ,\nwhere the expectation is over player’s random choices wk ∈ Rd. Recall ‖wk‖2 = 1. (In Section 6 we generalize this result to the full adversarial setting along with high-confidence regret.)\nOur algorithm FTCLobl is presented in Algorithm 1. This algorithm is parameterized by an even integer q ≥ 2 and a learning rate η > 0. It initializes with a rank-3 Wishart random matrix U. For\nevery k ∈ [T + 1], we denote by Xk def= ( ckI− ηΣk−1 )−q where\nck > 0 is the unique constant s.t. ckI− ηΣk−1 0 and Tr ( XkU ) = 1 .\nAt iteration k ∈ [T ], the player plays a random vector among the three eigenvectors of X1/2k UX 1/2 k .\nWe prove the following theorem in this paper for the total regret of FTCLobl(T, q, η).\nTheorem 1. In the online eigenvector problem with an oblivious adversary, there exists absolute constant C > 1 such that if q ≥ 3 log(2dT ) and η ∈ [ 0, 1\n11q3\n] , then FTCLobl(T, q, η) satisfies\nT∑\nk=1\nE [ w>k Akwk ] = T∑\nk=1\nE [ Ak •X1/2k UX 1/2 k ] ≥ ( 1− C · ηq5 log(dT ) ) λmax(ΣT )− 4\nη .\nCorollary 5.1. Choosing q = 3 log(2dT ) and η = Θ(log−3(dT )/ √ λmax(ΣT )), we have\n∑T k=1 E [ w>k Akwk ] ≥ λmax(ΣT )−O (√ λmax(ΣT ) log 3(dT ) ) , (λ-refined language)\nor choosing the same q but η = Θ(log−3(dT )/ √ T ) we have\n∑T k=1 E [ w>k Akwk ] ≥ λmax(ΣT )−O (√ T log3(dT ) ) . (general language)\nAs discussed in Section 3, our proof of Theorem 1 relies on a careful analysis on how the potential\nfunction Tr(X 1−1/q k U) = Tr ( (ckI − ηΣk−1)−(q−1)U ) changes across iterations. We analyze this potential increase in two steps: in the first step we replace Σk−1 with Σk, and in the second step we replace ck with ck+1. After appropriate telescoping, we can derive the result of Theorem 1.\nWe now discuss the details in the subsequent sections.\nAlgorithm 1 FTCLobl(T, q, η) Input: T , number of iterations; q ≥ 2, an even integer, theory-predicted choice q = Θ(log(dT )) η, the learning rate. theory-predicted choice η = log−3(dT )/√λmax(ΣT )\n1: Choose 3 vectors u1, u2, u3 ∈ Rd where the 3d coordinates are i.i.d. drawn from N (0, 1). 2: U← 13 ( u1u > 1 + u2u > 2 + u3u > 3 ) . 3: for k ← 1 to T do 4: Σk−1 ← ∑k−1 i=1 Ai.\n5: Denote by Xk ← ( ckI− ηΣk−1 )−q where ck is the unique constant satisfying that\nckI− ηΣk−1 0 and Tr ( XkU ) = 1 .\n6: Compute X 1/2 k UX 1/2 k = ∑3 j=1 pj · yjy>j where y1, y2, y3 are orthogonal unit vectors in Rd. This is an eigendecomposition and it satisfies p1, p2, p3 ≥ 0 and p1 + p2 + p3 = 1. 7: Choose wk ← yj with probability pj . 8: Play strategy wk and receive matrix Ak. 9: end for"
    }, {
      "heading" : "5.1 Well-Conditioning Events",
      "text" : "Due to concentration reasons, the potential increase could only be “reasonably” bounded for wellconditioned matrices U. We now make this definition formal. Given some parameter δ > 0 that we shall later choose to be 1/T 3, we introduce the following event:\nDefinition 5.2. For every k ∈ {0, 1, . . . , T}, define event\nEk(U) def= { ν>1 Uν1 ≥ δ\n2 and ∀i ∈ [d] : ν>i Uνi ≤ 2 log\ned\nδ\n}\nwhere ν1, . . . , νd are the eigenvectors of Σk with non-increasing eigenvalues. Let E<j(U) def= ∧j−1 k=0 Ek(U).\nIntuitively, event Ek(U) makes sure that the matrix U is “well-conditioned” in the eigenbasis of Σk: (1) it has a not-so-small first coordinate ν > 1 Uν1, and (2) each coordinate ν > i Uνi is no more than logarithmic. Using tail bounds for Gaussian distributions, it is not hard to show that this event occurs with probability at least 1− δ (see Appendix B): Lemma 5.3. For every k = 0, 1, . . . , T , we have PrU[Ek(U)] ≥ 1− δ.\nUnder event Ek−1(U), the barrier ck and the matrix Xk satisfy the following nice properties. (Their proofs are simple manipulations of matrix algebra and included in Appendix B.)\nProposition 5.4. If q ≥ max{log 2δ , log(3d log edδ )}, then\nevent Ek−1(U) implies 1\ne ≤ ck − ηλmax(Σk−1) ≤ e .\nIn particular, Ek−1(U) implies (recall Ak = PkP>k )\n(a) : ckI− ηΣk−1 1\ne I (b) : Tr(X\n1−1/q k U) ≤ ck ≤ ηλmax(Σk−1) + e (c) : ηP>k X 1/q k Pk eηI ."
    }, {
      "heading" : "5.2 First Potential Increase",
      "text" : "We next lemma bounds the potential increase if we replace Σk−1 with Σk: Lemma 5.5. There exists constant C > 1 such that, if q ≥ max{log 2δ , log(3d log edδ )} and η ≤ 13q3 ,\nE [ Tr (( ckI− ηΣk )−(q−1) U ) · 1E<k(U) −Tr (( ckI− ηΣk−1 )−(q−1) U ) · 1E<k(U) ]\n≤ (q − 1)η(1 + C · ηq5 log(d/δ))E [ Ak •X1/2k UX 1/2 k ] + (ηT + e)Tδ .\nThe proof of Lemma 5.5 is the main technical contribution of this paper, and deviates the most from classical analysis of MMWU. It makes use of our trace inequality in Section 4, and is the only place in our analysis that relies on rank(U) ≥ 3. We include the details in Appendix C. Remark 5.6. We have slightly abused notations here. In principle, the quantity Tr (( ckI−ηΣk )−(q−1) U ) can be unbounded if ckI − ηΣk is not invertible. However, as we shall see in the proof of Lemma 5.5, this necessarily implies 1E<k(U) = 0 because of Proposition 5.4. Therefore, we de-\nfine Tr (( ckI− ηΣk )−(q−1) U ) · 1E<k(U) to be zero if this happens."
    }, {
      "heading" : "5.3 Second Potential Increase",
      "text" : "The following lemma bounds the potential increase if we replace ck with ck+1. Its proof is included in Appendix D and is reasonably straightforward.\nLemma 5.7. For all q ≥ 2 and η > 0, E [ Tr (( ck+1I− ηΣk )−(q−1) U ) · 1E<(k+1)(U) ] − E [ Tr (( ckI− ηΣk )−(q−1) U ) · 1E<k(U) ] ≤ −(q − 1)(E[ck+1]− E[ck]) Finally, we prove in Appendix E that Theorem 1 is a direct consequence of our two potential increase lemmas above."
    }, {
      "heading" : "6 Adversarial Online Eigenvector + Regret in High-Confidence",
      "text" : "In this section, we switch to the more challenging adversarial setting: in each iteration k, the adversary picks Ak after seeing the player’s strategies w1, . . . , wk−1. In other words, Ak may depend on the randomness used in generating w1, . . . , wk−1 as well.\nIn such a case, denoting by D the same rank-3 Wishart distribution we generate U from in FTCLobl, we consider a variant of FTCLobl where a new random Uk is generated from D per iteration. In other words, instead of choosing U ∼ D only once at the beginning, we choose U1, . . . ,UT i.i.d. from D. Then, the normalization constant ck is defined to satisfy Tr((ckI− ηΣk−1)−qUk) = 1. We call this algorithm FTCLadv and present it in the appendix for completeness’ sake.\nOur next theorem shows that, algorithm FTCLadv gives the same regret bound as Theorem 1 even in the adversarial setting; in addition, it elevates the regret bound to a high-confidence level.\nTheorem 2. In the online eigenvector problem with an adversarial adversary, there exists constant C > 1 such that for every p ∈ (0, 1), q ≥ 3 log(2dT ) and η ∈ [ 0, 1\n11q3\n] , our FTCLadv(T, q, η) satisfies\nw.p. ≥ 1− p : T∑\nk=1\nw>k Akwk ≥ ( 1− C · η ( q5 log(dT ) + log(1/p) )) λmax(ΣT )− 5\nη .\nCorollary 6.1. Let q = 3 log(2dT ) and η = Θ ( log3(dT )+log1/2(1/p))−1√\nλmax(ΣT )\n) , then with prob. ≥ 1− p:\n∑T k=1w > k Akwk ≥ λmax(ΣT )− √ λmax(ΣT ) ·O ( log3(dT ) + √ log(1/p) ) , (λ-refined language)\nor choosing the same q but η = Θ ( log3(dT )+log1/2(1/p))−1√\nT\n) we have with prob. ≥ 1− p:\n∑T k=1w > k Akwk ≥ λmax(ΣT )− √ T ·O ( log3(dT ) + √ log(1/p) ) . (general language)\nProof of Theorem 2 relies on a reduction to the oblivious setting, and is included in Appendix F."
    }, {
      "heading" : "7 Efficient Implementation",
      "text" : "Recall that our regret theorems were based on the assumption that in each iteration k, the three vectors vj def = X 1/2 k uj = ( ckI− ηΣk−1 )−q/2 uj for j = 1, 2, 3 can be computed exactly. If this is the\ncase, then one can compute the 3× 3 matrix ( u>i Xkuj ) i,j∈[3] explicitly, and then we can obtain its rank-3 eigendecomposition X 1/2 k UX 1/2 k = ∑3 j=1 pj · yjy>j in O(d) time.\nTo make such computations efficient, we need to deal with three important issues:\n(a) We need to allow v1, v2, v3 to be computed approximately.\n(b) We need to find the unique normalization constant ck efficiently. (c) We need to compute ( ckI− ηΣk−1 )−1 b efficiently for any vector b ∈ Rd.\nAt a high level, issue (a) is not a big deal because if v′j satisfies ‖vj − v′j‖2 ≤ ε̃/poly(d, T ) and we use v′j instead of vj , then the final regret is affected by less than ε̃; issue (b) can be dealt as long as we perform a careful binary search to find ck, similar to prior work [7]; issue (c) can be done as long as we have a good control on the condition number of the matrix ( ckI− ηΣk−1 ) .\nWe discuss the details in Appendix G, and state below our final running-time theorem:\nTheorem 3. As long as q ≥ 3 log(2dT/p), with probability at least 1− p, each of the T iterations of FTCLobl and FTCLadv can be implemented to run in time\nÕ (√ ηλmax(ΣT ) + 1 · nnz(ΣT ) ) and Õ ( nnz(ΣT ) + √ ηT · nnz(ΣT )3/4nnz(A)1/4 ) .\nCorollary 7.1. Let q = 3 log(2dT/p) and η = Θ ( log−3(dT/p)/ √ T ) , then with prob. ≥ 1 − p, each iteration runs in time Õ ( T 1/4 · nnz(ΣT ) ) and Õ ( nnz(ΣT ) + T 1/4 · nnz(ΣT )3/4nnz(A)1/4 ) .\nAlternatively, if we choose η = Θ ( log−3(dT/p)/ √ λmax(ΣT ) ) , then each iteration runs in time\nÕ ( (λmax(ΣT ) + 1) 1/4 · nnz(ΣT ) ) and Õ ( nnz(ΣT ) + (T/ √ λmax(ΣT )) 1/2 · nnz(ΣT )3/4nnz(A)1/4 ) ."
    }, {
      "heading" : "8 Stochastic Online Eigenvector",
      "text" : "Consider the simplest setting when the matrices A1, . . . ,AT are generated i.i.d. from a common distribution whose expectation equals B. This is known as the stochastic online eigenvector problem, and we wish to minimize the regret ∑T k=1w > k Akwk − T · λmax(B).14\nIn this setting, we revisit Oja’s algorithm: beginning with a random vector u ∈ Rd where each ui is i.i.d. drawn from N (0, 1), at each iteration k, play wk to be (I + ηAk−1) · · · (I + ηA1)u after normalization. It is clear that wk can be computed from wk−1 with an additional time nnz(A).\nWe include in Appendix H a one-paged proof of the following theorem:\nTheorem 4. There exists C > 1 such that, for every p ∈ (0, 1), if η ∈ [ 0, √ p/(60Tλmax(B)) ] in Oja’s algorithm, we have with probability at least 1− p: ∑T\nk=1w > k Akwk ≥ (1− 2η)T · λmax(B)− C · log(d+log(1/p)) η .\nCorollary 8.1. Choosing η = √ p/ √\n60Tλmax(B), we have with prob. ≥ 1− p: ∑T\nk=1w > k Akwk ≥ T · λmax(B)−O (√T ·λmax(B)√ p · log(d+ log(1/p)) ) . (λ-refined language)\nChoosing η = √ p/ √\n60T , we have with prob. ≥ 1− p: ∑T\nk=1w > k Akwk ≥ T · λmax(B)−O (√ T√ p · log(d+ log(1/p)) ) . (general language)\nThe proof of Theorem 4 uses a potential function analysis which is similar to the matrix exponential potential used in MMWU, but compressed to dimension 1."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank Yin Tat Lee for discussing the problem regarding how to compress MMWU to constant dimension in 2015. We thank Elad Hazan for suggesting us the problem and for several insightful discussions. We thank Dan Garber and Tengyu Ma for clarifying some results of prior work [16].\n14In principle, one can also ask to minimize regret where T ·B is replaced with A1 + · · · + AT . However, due to simple concentration results, there is no big difference between the two different notions. [16]\nAppendix"
    }, {
      "heading" : "A Proof of Lemma 4.1",
      "text" : "Lemma 4.1. For every symmetric matrices A,B,D ∈ Rd×d, every integer k ≥ 1, every η∗ ≥ 0, and every η ∈ [0, η∗/k2], if A and D are commutative, then\n(A + ηB)k •D−Ak •D ≤ kηB •Ak−1D + ( ηk2\nη∗\n)2 max\nη′∈[0,η∗]\n{∣∣(A + η′B)k •D−Ak •D ∣∣ } .\nProof. Consider a degree-k polynomial\nf(η) def = (A + ηB)k •D−Ak •D =\nk∑\ni=1\nηi ∑\nj0,...,ji∈Z≥0 j0+···+ji=k−i\nAj0BAj1B · · ·BAji •D\nIts first order derivative\nf ′(0) = ∑\nj0,j1∈Z≥0 j0+j1=k−1\nAj0BAj1 •D = ∑\nj0,j1∈Z≥0 j0+j1=k−1\nA(k−1)/2BA(k−1)/2 •D = kB •A(k−1)/2DA(k−1)/2 .\nAbove, the first equality is due to the commutativity between A and D. Letting f∗ def= maxη′∈[0,η∗] |f(η′)|, we can apply Markov brothers’ inequality (2.1) and obtain for every i ≥ 2,\n|f (i)(0)| ≤ ( 2\nη∗\n)i · k\n2(k2 − 1) · · · (k2 − (i− 1)2) 1 · 3 · 5 · · · (2i− 1) maxη′∈[0,η∗] |f(η ′)| ≤ k 2i (η∗)i f∗ .\nTherefore, as long as η ≤ η∗ k2 , we have\nf(η) = f(0) + f ′(0) · η + k∑\ni=2\nηi · f (i)(0)\ni! ≤ f(0) + f ′(0) · η +\nk∑\ni=2\n( ηk2\nη∗\n)i · f ∗\ni!\n≤ f(0) + f ′(0) · η + ( ηk2\nη∗\n)2 f∗ .\nSince f(0) = 0 we complete the proof."
    }, {
      "heading" : "B Proof for Section 5.1",
      "text" : "B.1 Proof of Lemma 5.3\nLemma 5.3. For every k = 0, 1, . . . , T , we have PrU[Ek(U)] ≥ 1− δ. Proof. Let ν1, . . . , νd be the eigenvectors of Σk with non-increasing eigenvalues. Because Gaussian random vectors are rotationally invariant, we can view each u1, u2, u3 as drawn in the basis of ν1, . . . , νd, so each ν > i uj is drawn i.i.d. from N (0, 1) for every i ∈ [d], j ∈ [3].\nSince ν>1 Uν1 = 1 3 ( (ν>1 u1) 2 + (ν>1 u2) 2 + (ν>1 u3) 2 ) , we immediately know that 3ν>1 Uν1 is distributed according to chi-square distribution χ2(3). The probability density function of χ2(3) is f(x) = e −x/2√x√\n2π (for x ∈ [0,∞)) and therefore Pr [ ν>1 Uν1 ≤ δ/2 ] ≤ ∫ 3δ/2\n0\ne−x/2 √ x√\n2π dx ≤\n∫ 3δ/2\n0\n√ x√ 2π dx = 1 2 √ 3 π δ3/2 ≤ δ 2 .\nAs for the second condition, for every t ≥ 0 and i ∈ [d],\nPr [ ν>i Uνi ≥ t/3 ] ≤ ∫ ∞\nt\ne−x/2 √ x√\n2π dx = 1− Erf (√ t√ 2 ) + √ 2 π e−t/2 √ t ≤ e−t/2 + √ 2 π e−t/2 √ t ,\nwhere Erf(x) is the Gauss error function. Picking t = 4 log edδ , we have\ne−t/2 +\n√ 2\nπ e−t/2\n√ t ≤ δ 2\ne2d2 +\n√ 2\nπ\nδ2 e2d2 · 2 √ ed δ < δ 2d .\nTherefore, we have Pr [ ∀i ∈ [d] : ν>i Uνi ≥ 2 log edδ ] ≤ δ2 and we conclude by union bound PrU[Ek(U)] ≤ δ2 + δ2 = δ .\nB.2 Proof of Proposition 5.4\nProposition 5.4. If q ≥ max{log 2δ , log(3d log edδ )}, then\nevent Ek−1(U) implies 1\ne ≤ ck − ηλmax(Σk−1) ≤ e . (B.1)\nIn particular, Ek−1(U) implies (recall Ak = PkP>k )\n(a) : ckI− ηΣk−1 1\ne I (b) : Tr(X\n1−1/q k U) ≤ ck ≤ ηλmax(Σk−1) + e (c) : ηP>k X 1/q k Pk eηI .\nProof. Let ν1, . . . , νd be the eigenvectors of Σk−1 with non-increasing eigenvalues λ1, . . . , λd. Then,∑d i=1 ν>i Uνi (ck−ηλi)q = Tr(XkU) = 1. However, event Ek(U) tells us ν > i Uνi ≥ δ2 which implies (ck − ηλ1) q ≥ δ2 . Under our choice of q, we have ck − ηλ1 ≥ 1e which proves the first inequality in (B.1).\nOn the other hand, letting c = ηλmax(Σk−1) + e, our choice of q implies\nTr((cI− ηΣk−1)−qU) = d∑\ni=1\nν>i Uνi (c− ηλi)q ≤ d∑\ni=1\n2 log(ed/δ)\neq ≤ 1 .\nSince the left hand side of the above inequality is an decreasing function in c, and since Tr((ckI− ηΣk−1)−qU) = 1, we must have ck ≤ c which proves the second inequality in (B.1).\nFinally, (a) is a simple corollary of the first inequality of (B.1). As for (b), it simply comes from the following upper bound\nTr(X 1−1/q k U) =\nd∑\ni=1\nν>i Uνi (ck − ηλi)q−1 ≤ ck d∑\ni=1\nν>i Uνi (ck − ηλi)q = ckTr(XkU) = ck .\nAs for (c), it follows from P>k Pk I so ηP>k X 1/q k Pk ηP>k (eI)Pk eηI."
    }, {
      "heading" : "C Proof for Section 5.2",
      "text" : "Lemma 5.5. There is constant C > 1 such that, if q ≥ max{log 2δ , log(3d log edδ )} and η ≤ 111q3 ,\nE [ Tr (( ckI− ηΣk )−(q−1) U ) · 1E<k(U) −Tr (( ckI− ηΣk−1 )−(q−1) U ) · 1E<k(U) ]\n≤ (q − 1)η(1 + C · ηq5 log(d/δ))E [ Ak •X1/2k UX 1/2 k ] + (ηT + e)Tδ .\nProof. Let ν1, . . . , νd be the eigenvectors of Σk−1 with non-increasing eigenvalues. In this proof, let us assume without loss of generality that all vectors and matrices are written in this eigenbasis (so Σk−1 and Xk are both diagonal matrix).\nSince Gaussian random vectors are rotationally invariant, we assume that u1, u2, u3 are generated according to the following procedure: first, the absolute values of their 3d coordinates u1, u2, u3 are determined; then, their signs are determined.\nDenoting by D = diag{U11, . . . ,Udd} the diagonal part of U, we immediately notice that D is determined completely at the first step of the above procedure. This has two important consequences that we shall rely crucially in the proof:\n• fixing the randomness of D, it satisfies EU[U|D] = D;15 • ck is completely determined by D. 16\nIn addition, since the event Ek−1(U) only depends on the diagonal entry of U, slightly abusing notation, we also use Ek−1(D) to denote this event on diagonal matrices D. We also use Di to represent the i-th diagonal entry of D. Our proof now has three parts:\nPart I: Potential Increase for D. For every PSD matrix D, denoting by Ak = PkP > k ,\nTr (( ckI− ηΣk )−(q−1) D ) −Tr (( ckI− ηΣk−1 )−(q−1) D )\n¬ = Tr (( X −1/q k − ηPkP>k )−(q−1) D ) −Tr ( X 1−1/q k D )\n = Tr (( X\n1/q k + ηX 1/q k Pk(I− ηP>k X 1/q k Pk) −1P>k X 1/q k\n)q−1 D ) −Tr ( X\n1−1/q k D\n) (C.1)\nAbove, ¬ follows from the definition of Xk and  uses the Woodbury formula for matrix inversion. Now, unlike the classical proof for MMWU, our matrix D here is not identity so we cannot rely on the Lieb-Thirring trace inequality to bound the right hande side of (C.1) like it was used in [7]. We can instead consult our new trace inequality Lemma 4.1 because D and Xk are both diagonal matrices so they are commutative. Recall that Lemma 4.1 requires a crude upper bound on the first trace quantity on the term “ ∣∣(A+η′B)k •D−Ak •D ∣∣”, and we shall provide this crude upper bound in Lemma C.1. Formally, choosing η∗ def= 111q , we that for every D satisfying Ek−1(D),\nTr (( ckI− ηΣk )−(q−1) D ) −Tr (( ckI− ηΣk−1 )−(q−1) D )\n® ≤ (q − 1)ηX1/qk Pk(I− ηP>k X 1/q k Pk) −1P>k X 1/q k •X (q−2)/q k D\n+\n( η(q − 1)2\nη∗\n)2 · 4(q − 1)η∗‖D‖2PkP>k •Xk\n¯ ≤ (q − 1)η\n1− eη PkP > k •XkD +O\n( η2q6 ) · ‖D‖2 ·PkP>k •Xk\n° ≤ (q − 1)ηPkP>k •XkD +O ( η2q6 log(d/δ) ) PkP > k •Xk . (C.2)\nAbove, ® follows from Lemma 4.1 (with η ≤ η∗/q2) together with Lemma C.1 (for η = η∗); ¯ follows from I − ηP>k X 1/q k Pk (1 − eη)I (see Proposition 5.4), the fact that Tr(AC) ≤ Tr(BC) for A B and C symmetric, and the choice of η∗; ° follows from our assumption η ≤ 16 as well as ‖D‖2 ≤ 2 log 2dδ which comes from the definition of event Ek−1(D). Part II: Potential Increase for All U That Agrees With D. For every fixed D that satisfies\n15More specifically, since the off-diagonal entries of U can still randomly flip signs in the second step of the random procedure, their expectations are all equal to zero.\n16This is because ck is defined as the constant satisfying 1 = Tr((ckI− ηΣk−1)U) = Tr((ckI− ηΣk−1)D).\nEk−1(D), taking expectation over all matrices U that agrees with D:17\nE [ Tr (( ckI− ηΣk )−(q−1) U ) · 1E<(k−1)(U) −Tr (( ckI− ηΣk−1 )−(q−1) U ) · 1E<(k−1)(U) ∣∣∣D ]\n¬ ≤ E [ Tr (( ckI− ηΣk )−(q−1) U ) −Tr (( ckI− ηΣk−1 )−(q−1) U )\n+ Tr (( ckI− ηΣk−1 )−(q−1) U ) · (1− 1E<(k−1)(U)) ∣∣∣D ]\n ≤ Tr (( ckI− ηΣk )−(q−1) D ) −Tr (( ckI− ηΣk−1 )−(q−1) D ) + E [ (ηT + e) · (1− 1E<(k−1)(U)) ∣∣∣D ] = Tr (( ckI− ηΣk )−(q−1) D ) −Tr (( ckI− ηΣk−1 )−(q−1) D ) + (ηT + e) ·Pr [ E<(k−1)(U) ∣∣∣D ]\n® ≤ (q − 1)ηPkP>k •XkD +O ( η2q6 log(d/δ) ) ·PkP>k •Xk + (ηT + e)Tδ ¯ = (q − 1)η E [ PkP > k •XkU | D ] +O ( η2q6 log(d/δ) ) ·PkP>k •Xk + (ηT + e)Tδ. (C.3)\nAbove, ¬ is because indicator functions are never greater than 1;  uses Tr(X 1−1/q k U) ≤ ηλmax(Σk−1)+ e ≤ ηT + e which follows from Proposition 5.4; ® follows from (C.2) as well as Lemma 5.3; and ¯ follows from the observation EU[U|D] = D together with the fact that Xk only depends on D. Part III: Potential Increase for All U. We now claim for all possible diagonal D, it satisfies\nE [ Tr (( ckI− ηΣk )−(q−1) U ) · 1E<k(U) −Tr (( ckI− ηΣk−1 )−(q−1) U ) · 1E<k(U) ∣∣∣D ]\n≤ (q − 1)η E [ PkP > k •XkU|D ] +O ( η2q6 log(d/δ) ) ·PkP>k •Xk + (ηT + e)Tδ. (C.4)\nThis is because, if D satisfies Ek−1(D) then (C.4) comes from (C.3); or if D does not satisfy Ek−1(D) then the left hand side of (C.4) is zero (see Remark 5.6) but the right hand side is non-negative.\nTaking expectation with respect to the randomness of D in (C.4), and using Lemma C.2 which upper bounds ED[PkP>k •Xk] by ED[PkP>k •XkD] = EU[PkP>k •XkU] we get the desired inequality. (Note that PkP > k XkU = AkXkU = AkX 1/2 k UX 1/2 k .)\nC.1 Missing Auxiliary Lemmas\nIn this subsection we prove the following two auxiliary lemmas. The first one shall be used to bound the higher-order terms in Lemma 4.1.\nLemma C.1. For every q ≥ 2 and every η ∈ [ 0, 14e(q−1) ] , event Ek−1(D) implies that\n∣∣∣∣Tr (( X 1/q k + ηX 1/q k Pk(I− ηP>k X 1/q k Pk) −1P>k X 1/q k )q−1 D ) −Tr ( X q−1 q k D )∣∣∣∣ ≤ 4η(q − 1)‖D‖2Tr(XkPkP>k ) .\nThe second one upper bounds the expectation of the right hand side of Lemma C.1. We highlight that the proof of Lemma C.2 is the only place in this paper that we have assumed k(U) = 3.\nLemma C.2. We have ED[Tr(PkP>k Xk)] ≤ 9 · ED[Tr(PkP>k XkD)].\nNote that we can assume without loss of generality that Σk−1, Xk and D are all diagonal matrices, which has been argued in the proof of Lemma 5.5. Therefore, all the proofs in this subsection will be given under this assumption.\n17Note when D satisfies Ek−1(D) we have ckI− ηΣk−1 1e I according to Proposition 5.4. This implies, as long as η ≤ e−1, it satisfies ckI− ηΣk 0 so Tr (( ckI− ηΣk )−(q−1) U ) > 0.\nTo prove Lemma C.1 we need the following lemma:\nLemma C.3 (Monotonicity of Diagonal entries). Let A,D ∈ Rd×d be two diagonal positive definite matrices,18 let B ∈ Rd×d be PSD, then for every q ∈ N∗ such that q‖A−1/2BA−1/2‖2 < 1:\n0 ≤ Tr((A + B)qD)−Tr(AqD) ≤ ‖D‖2 1− q‖A−1/2BA−1/2‖2 Tr ( Aq−1B ) .\nProof of Lemma C.3. For every i ∈ [D], let P be a matrix with all zero entries except Pi,i = 1. Then we have:\n[(A + B)q]i,i = Tr(P q(A + B)qPq) ≥ Tr ((P(A + B)P)q)\n= ([A + B]i,i) q ≥ [A]qi,i = [Aq]i,i .\nWhere the first inequality is due to the Lieb-Thirring inequality, and the last equality is because A is diagonal. Since D is a diagonal PSD matrix, we can conclude that19\nTr((A + B)qD)−Tr(AqD) = d∑\ni=1\n[D]i,i ([(A + B) q −Aq]i,i) ≥ 0 .\nand\nTr((A + B)qD)−Tr(AqD) ≤ max i∈[d] [D]i,i\nd∑\ni=1\n[(A + B)q −Aq]i,i = ‖D‖2Tr((A + B)q −Aq) .(C.5)\nWe focus on the term (A+B)q. We can re-write it as (A+B)q = ( A1/2(I + A−1/2BA−1/2)A1/2 )q .\nThen by Lieb-Thirring again, we have:\nTr((A + B)q) ≤ Tr ( Aq/2 ( I + A−1/2BA−1/2 )q Aq/2 )\n≤ Tr ( Aq/2 ( I + 1\n1− q‖A−1/2BA−1/2‖2 A−1/2BA−1/2\n) Aq/2 )\n≤ Tr(Aq) + q 1− q‖A−1/2BA−1/2‖2\nTr ( Aq−1B ) . (C.6)\nWhere the second inequality uses (I + X)q I + q1−q‖X‖2 X for every PSD matrix X with q‖X‖2 < 1. Putting together (C.5) and (C.6), we obtain:\nTr((A + B)qD)−Tr(AqD) ≤ q‖D‖2 1− q‖A−1/2BA−1/2‖2 Tr ( Aq−1B ) .\nProof of Lemma C.1. Under event Ek−1(D) , we know I−ηP>k X 1/q k Pk (1−eη)I (see Proposition 5.4) and thus\n0 ηX1/2qk Pk(I− ηP>k X 1/2q k Pk) −1P>k X 1/q k\neη\n1− eη I .\nWe now apply Lemma C.3 with A = X 1/q k , B = ηX 1/q k Pk(I−ηP>k X 1/q k Pk) −1P>k X 1/q k , and q = q−1. We can do so because A and D are both diagonal and (q−1)eη1−eη < 1 under our assumption of η. The\n18In fact, we have only required them to be simultaneously diagonalizable. 19The authors would like to thank Elliott Lieb who has helped us obtain the inequality of the next line.\nconclusion of Lemma C.3 tells us that:∣∣∣∣Tr (( X 1/q k + ηX 1/q k Pk(I− ηP>k X 1/q k Pk) −1P>k X 1/q k )q−1 D ) −Tr ( X q−1 q k D )∣∣∣∣\n≤ q − 1 1− (q−1)eη1−eη\n‖D‖2Tr(Aq−2B) ≤ ( 2(q − 1)‖D‖2 )( η 1− eηTr(XkPkP > k ) )\n≤ 4η(q − 1)‖D‖2Tr(XkPkP>k ) . Above, the second and third inequalities have respectively used (q−1)eη1−eη < 1 2 and 1 1−eη ≤ 2, which are both true by our assumption on η. Proof of Lemma C.2. Let λ1 ≥ · · · ≥ λd ≥ 0 be the eigenvalues of Σk−1 and ν1, . . . , νd be the corresponding eigenvectors. Let D1, · · · , Dd be the diagonals of D. Recall that Σk−1,Xk,D are all diagonal matrices. Define function f : Rd → R\nf(r1, · · · , rd) def= d∑\ni=1\n[PkP > k ]i,i · ri\n(ck − λi)q (recall that ck depends on (D1, . . . , Dd))\nWe shall prove that for some γ ∈ (0, 1) that shall be chosen later, it satisfies for every i ∈ [d], E[f(γ, · · · , γ,Di, · · · , Dd)] ≥ E[f(γ, · · · , γ,Di+1, · · · , Dd)]\nwhere recall that both expectations are only over the randomness of D1, . . . , Dd. Let D−i def = (D1, . . . , Di, Di+2, · · · , Dd). Then, it is sufficient to prove that for every fixed possibility of D−i, the following inequality holds:\nE Di [f(γ, · · · , γ,Di, · · · , Dd) | D−i] ≥ E Di [f(γ, · · · , γ,Di+1, · · · , Dd) | D−i] .\nTherefore, in the remaining proofs, we shall consider Di as the only random variable, and thus ck only depends on Di. For a fixed value s ≥ 1 that we shall choose later, we can let c be the (unique) value of ck when Di = sγ.\nLetting g(x) def = x(ck−λi)q , we make three quick observations:\n1. g(γ) = γ(ck−λi)q is a monotone decreasing function of Di.\nThis is so because ck is a monotone increasing function of Di.\n2. g(Di) = Di\n(ck−λi)q is a monotone decreasing function of Di.\nThis is because g(Di) = 1− ∑ j 6=i Dj (ck−λj)q = 1 but ck is a monotone increasing function of Di.\n3. When Di ≤ sγ, we have g(γ) ≤ sγDi γ (c−λi)q .\nThis is because g(γ) = γDi ( 1−∑j 6=i Dj (ck−λj)q ) ≤ γDi ( 1−∑j 6=i Dj (c−λj)q ) = γDi sγ (c−λj)q , where the first inequality is because ck ≤ c when Di ≤ sγ (by the monotone increasing of ck with respect to Di), and the second equality is according to the definition of c.\nCombining the above three observations, we have:\nE[g(Di)] ≥ Pr[Di ≥ sγ]E[g(Di) | Di ≥ sγ] ≥ Pr[Di ≥ sγ] sγ\n(c− λi)q\nE[g(γ)] ≤ Pr[Di ≥ sγ]E[g(γ) | Di ≥ sγ] + E [ 1\nDi\n] sγ2\n(c− λi)q\n≤ γ (c− λi)q\n+ E [ 1\nDi\n] sγ2\n(c− λi)q ≤ sγ (c− λi)q ( 1 s + E [ γ Di ]) .\nRecall that each Di = 1 3(〈νi, u1〉2 + 〈νi, u2〉2 + 〈νi, u3〉2) where u1, u2, u3 are three normal Gaussian random vectors. Therefore, each 3Di has a chi-square distribution of degree 3, which implies E[ 1Di ] = 3 and Pr[Di ≥ 1 3 ] > 2 3 . In sum, if we take γ = 1 9 and s = 3, we have:\nE Di [g(Di)] ≥ E Di [g(γ)] .\nFinally, this implies\nE Di [f(γ, · · · , γ,Di, · · · , Dd)− f(γ, · · · , γ,Di+1, · · · , Dd) | D−i] = [PkP>k ]i,i E Di [g(Di)− g(γ)|D−i] ≥ 0 .\nso we have\nE D [f(γ, · · · , γ,Di, · · · , Dd)] ≥ E D [f(γ, · · · , γ,Di+1, · · · , Dd)] . In particular,\nE[Tr(PkP>k XkD)] = E[f(D1, · · · , Dd)] ≥ E[f(γ, · · · , γ)] = γ E[Tr(PkP>k Xk)] ."
    }, {
      "heading" : "D Proof for Section 5.3",
      "text" : "Lemma 5.7. For all q ≥ 2 and η > 0,\nE [ Tr (( ck+1I− ηΣk )−(q−1) U ) · 1E<(k+1)(U) ] − E [ Tr (( ckI− ηΣk )−(q−1) U ) · 1E<k(U) ]\n≤ −(q − 1)(E[ck+1]− E[ck])\nProof. Recall that ck+1 ≥ ck because all matrices Ak are PSD. Denoting by ν1, . . . , νd the eigenvectors of Σk with non-increasing eigenvalues λ1 ≥ · · · ≥ λd,20 we have for every U,\nTr (( ck+1I− ηΣk )−(q−1) U ) −Tr (( ckI− ηΣk )−(q−1) U )\n= d∑\ni=1\nν>i Uνi (ck+1 − ηλi)q−1 − d∑\ni=1\nν>i Uνi (ck − ηλi)q−1 ¬ ≤ −(q − 1)(ck+1 − ck) · d∑\ni=1\nν>i Uνi (ck+1 − ηλi)q\n= −(q − 1)(ck+1 − ck) ·Tr (( ck+1I− ηΣk )−q U )\n= −(q − 1)(ck+1 − ck)Tr(Xk+1U) = −(q − 1)(ck+1 − ck) . (D.1)\nAbove, ¬ is derived from inequality 1 (c+x)q−1 − 1xq−1 ≤ − (q−1)c (c+x)q (for every c ≥ 0, x > 0) which follows from the convexity of function f(x) = 1 xq−1 .\nNext, we observe that for every U that does not satisfy E<k(U), the very right hand side of (D.1) is still non-negative. Therefore, we conclude that for all U, Tr (( ck+1I− ηΣk )−(q−1) U ) · 1E<k(U) −Tr (( ckI− ηΣk )−(q−1) U ) · 1E<k(U) ≤ −(q − 1)(ck+1 − ck) . Finally, since 1E<(k+1)(U) ≤ 1E<k(U) and Tr (( ckI− ηΣk )−(q−1) U ) ≥ 0, we have Tr (( ck+1I− ηΣk )−(q−1) U ) · 1E<(k+1)(U) −Tr (( ckI− ηΣk )−(q−1) U ) · 1E<k(U) ≤ −(q − 1)(ck+1 − ck)\nand taking expectation we finish the proof of Lemma 5.7. 20This is different from the proof of Lemma 5.5 where we defined them to be eigenvectors of Σk−1."
    }, {
      "heading" : "E Proof of Theorem 1: Oblivious Online Eigenvector",
      "text" : "Proof of Theorem 1. Combining Lemma 5.5 and Lemma 5.7, we have\nE [ Tr ( X\n1−1/q k+1 U ) · 1E<k+1(U) ] − E [ Tr ( X 1−1/q k U ) · 1E<k(U) ]\n≤ −(q − 1)(E[ck+1]− E[ck]) + (q − 1)η(1 +O(ηq5 log(d/δ))) · E [ Ak •X1/2k UX 1/2 k ] + (ηT + e)Tδ .\nTelescoping it for all k = 1, . . . , T , we have\nE [ Tr ( X\n1−1/q T+1 U ) · 1E<T+1(U) ] − E [ Tr ( X 1−1/q 1 U ) · 1E<1(U) ] (E.1)\n≤ −(q − 1)(E[cT+1]− E[c1]) + (q − 1)η(1 +O(ηq5 log(d/δ))) · E [ T∑\nk=1\nAk •X1/2k UX 1/2 k\n] + (ηT + e)T 2δ .\nWe make four quick observations:\n• Regardless of the randomness of U, we have Tr ( X\n1−1/q T+1 U ) · 1E<T+1(U) ≥ 0.\n• Regardless of the randomness of U, we have cT+1 ≥ ηλmax(ΣT ). • We have E[c1] ≤ e. To derive that, we use 1cq1 TrU = Tr(X1U) = 1 which implies c1 =\n(TrU)1/q. Notice that TrU = 13 ∑ i∈[d],j∈[3](uj,i) 2 so 3TrU is distributed according to chisquared distribution χ2(3d) whose PDF is p(x) = 2 − 3d2 e− x 2 x 3d 2 −1\nΓ(3d/2) . We thus have\nE[c1] = ∫ ∞\n0 x1/qp(x)dx =\n21/qΓ (\n3d 2 + 1 q\n)\nΓ (\n3d 2\n) ≤ 21/q · (3d\n2\n)1/q = (3d)1/q ≤ e .\nAbove, the first inequality uses Γ(x+a)Γ(x) ≤ xa for a ∈ (0, 1) and x > 0 (cf. Wendell [32]), and the second inequality uses our assumption on q.\n• E [ Tr ( X\n1−1/q 1 U ) ·1E<1(U) ] ≤ e. This is because Tr(X1−1/q1 U) = 1cq−11 TrU = c1 and E[c1] ≤ e.\nSubstituting the four observations above into the telescoping sum (E.1), we have\n(q − 1)ηλmax(ΣT ) ≤ e+ (q − 1)e+ (q − 1)η(1 +O(ηq5 log(d/δ))) · E [ T∑\nk=1\nAk •X1/2k UX 1/2 k\n] + (ηT + e)T 2δ .\nUsing the inequality (ηT + e)T 2δ ≤ (1 + e)T 3δ, we conclude that if we choose δ = 11+eT−3, then\n(q − 1)ηλmax(ΣT ) ≤ (q − 1)η ( 1 +O(ηq5 log(dT )) ) · E [ T∑\nk=1\nAk •X1/2k UX 1/2 k\n] + 4(q − 1) .\nDividing both sides by (q − 1)η we arrive at the desired inequality."
    }, {
      "heading" : "F Proof of Theorem 2: Adversarial Online Eigenvector",
      "text" : "Proof of Theorem 2. Before beginning our proof, let us emphasize that in this adversarial setting,\n• Ak and Σk can depend on the randomness of U1, . . . ,Uk−1. • Xk and ck depend on the randomness of Uk and Σk−1 (and thus also on U1, . . . ,Uk−2).\nAlgorithm 2 FTCLadv(T, q, η)\nInput: T , number of iterations; q ≥ 2, an even integer, theory-predicted choice q = Θ(log(dT )) η, the learning rate. theory-predicted choice η = log−3(dT )/√λmax(ΣT )\n1: for k ← 1 to T do 2: Choose 3 vectors u1, u2, u3 ∈ Rd where the 3d coordinates are i.i.d. drawn from N (0, 1). 3: Uk ← 13 ( u1u > 1 + u2u > 2 + u3u > 3 ) .\n4: Σk−1 ← ∑k−1 i=1 Ai.\n5: Denote by Xk ← ( ckI− ηΣk−1 )−q where ck is the unique constant satisfying that\nckI− ηΣk−1 0 and Tr ( XkUk ) = 1 .\n6: Compute X 1/2 k UkX 1/2 k = ∑3 j=1 pj · yjy>j where y1, y2, y3 are orthogonal unit vectors in Rd. This is an eigendecomposition and it satisfies p1, p2, p3 ≥ 0 and p1 + p2 + p3 = 1. 7: Choose wk ← yj with probability pj . 8: Play strategy wk and receive matrix Ak. 9: end for\nConsider for analysis purpose only another random matrix Ũ drawn from distribution D, independent of the randomness of U1, . . . ,UT . Define c̃k to be the unique constant satisfying c̃kI− ηΣk−1 0 and Tr((c̃kI− ηΣk−1)−qU) = 1, and define X̃k = (c̃kI− ηΣk−1)−q.\nNow, if we fix the randomness of U1, . . . ,Uk−1, the matrices Σk−1 and Ak become fixed. The fact that Uk and Ũ are both drawn from the same distribution D (and the fact that Xk and X̃k are computed from Uk and Ũ in the same way) implies\nE Uk\n[ Ak •X1/2k UkX 1/2 k ∣∣∣U1, . . . ,Uk−1 ]\n= E Ũ\n[ Ak • X̃k 1/2 ŨX̃k 1/2 ∣∣∣U1, . . . ,Uk−1 ] (F.1)\nNow, consider random variables Zk = w > k Akwk. We have that Zk is Fk-measurable for Fk generated by U1, ...,Uk, w1, ..., wk. According to the martingale concentration Lemma F.1, we have\nPr\n[ T∑\nk=1\nZk ≤ (1− µ) T∑\nk=1\nE[Zk | Fk−1]− log 1p µ\n] ≤ p .\nAt the same time, we have\nE[Zk | Fk−1] = E wk,Uk\n[ Ak • wkw>k | U1, . . . ,Uk−1 ] = E\nUk\n[ Ak •X1/2k UkX 1/2 k | U1, . . . ,Uk−1 ]\n= E Ũ\n[ Ak • X̃k 1/2 ŨX̃k 1/2 | U1, . . . ,Uk−1 ] ,\nwhere the last inequality comes from (F.1). In sum, with probability at least 1 − p (over the randomness of U1, . . . ,UT , w1, . . . , wT ), we have\nT∑\nk=1\nw>k Akwk ≥ (1− µ)E Ũ\n[ T∑\nk=1\nAk • X̃k 1/2 ŨX̃k 1/2 ∣∣∣U1, . . . ,UT−1 ] − log 1p µ .\nApplying Theorem 1 we have (more specifically, fixing each possible sequence U1, . . . ,UT , we have a fixed sequence of A1, . . . ,AT and can apply Theorem 1):\nT∑\nk=1\nw>k Akwk ≥ (1− µ) ( 1−O(ηq5 log(dT )) ) λmax(ΣT )− 4\nη − log 1p µ .\nChoosing µ = η · log(1/p), we finish the proof of Theorem 2.\nF.1 A Concentration Inequality for Martingales\nWe show the following (simple) martingale concentration lemma that we believe is classical but have not found anywhere else.\nLemma F.1 (Concentration). Let {Zt}Tt=1 be a random process with respect to a filter {0,Ω} = F0 ⊂ F1 ⊂ · · · ⊂ FT and each Zt ∈ [0, 1] is Ft-measurable. For every p, µ ∈ (0, 1),\nPr\n[ T∑\nt=1\nZt ≤ (1− µ) T∑\nt=1\nE[Zt | Ft−1]− log 1p µ\n] ≤ p .\nWe emphasize here that E[Zt | Ft−1] is Ft−1-measurable and thus not a constant.\nProof of Lemma F.1. Like in classical concentration proofs, we have\nPr [∑T t=1 Zt ≤ (1− µ) ∑T t=1 E[Zt | Ft−1]− log 1 p µ ]\n= Pr [∑T t=1 ((1− µ)E[Zt | Ft−1]− Zt) ≥ log 1 p µ ] = Pr [ exp { µ (∑T t=1 ((1− µ)E[Zt | Ft−1]− Zt) )} ≥ 1p ] ≤ pE [ exp { µ (∑T t=1 ((1− µ)E[Zt | Ft−1]− Zt) )} ] . (F.2)\nDenote by Yt = µ(1− µ)E[Zt | Ft−1]− µZt, we know that each Yt ∈ [−1, 1] is Ft-measurable.\nE [ exp {∑T t=1 Yt }] = E [ E [ exp {∑T t=1 Yt } ∣∣FT−1 ]]\n= E [ exp {∑T−1 t=1 Yt } E [ eYT ∣∣FT−1 ]] ≤ E [ exp {∑T−1 t=1 Yt } E [ 1 + YT + Y 2 T ∣∣FT−1 ]] .\nNow, we focus on the term YT + Y 2 T :\nYT + Y 2 T ≤ µ(1− µ)E[ZT | FT−1]− µZT + µ2(1− µ)2 E[ZT | FT−1]2 + µ2Z2T ≤ µ(1− µ)E[ZT | FT−1]− µZT + µ2(E[ZT | FT−1] + µZT ) .\n(The first inequality has used (a− b)2 ≤ a2 + b2 when a, b ≥ 0, and the second has used Zt ∈ [0, 1].) Taking the conditional expectation, we obtain E[YT + Y 2T | FT−1] ≤ 0 and this implies\nE [ exp {∑T t=1 Yt }] ≤ E [ exp {∑T−1 t=1 Yt }] ≤ · · · ≤ e0 = 1 .\nPlugging this into (F.2) completes the proof of Lemma F.1."
    }, {
      "heading" : "G Proof of Theorem 3: Implementation Details",
      "text" : "Resolution to Issue (a). We first point out that the final regret blows up by an additive value ε̃ as long as the eigendecomposition ∑3 j=1 pj · yjy>j is computed to satisfy21\n∥∥∥ 3∑\nj=1\nX 1/2 k uju > j X 1/2 k −\n3∑\nj=1\npj · yjy>j ∥∥∥\n2 ≤ ε̃ poly(d, T ) .\nMoreover, this can be done in time O(d) as long as we can compute the three vectors { X1/2uj } j∈[3] to an additive ε̃/poly(d, T ) error in Euclidean norm. This can be done by applying ( ckI−ηΣk−1\n)−1 a number q/2 times to vector uj , each again to an error ε̃/poly(d, T ). In sum, we can repeatedly apply Lemma G.1 and the final running time only logarithmically depends on ε̃/poly(d, T ).\nResolution to Issue (c). We choose δ = p/T and revisit the event Ek(U) defined in Def. 5.2. According to Lemma 5.3 and union bound, it satisfies with probability at least 1 − p, all the T events E0(U1), . . . , ET−1(UT ) are satisfied. If we apply Proposition 5.4, we immediately have that\nq ≥ 3 log(2dT/p) =⇒ ∀k ∈ [T ] : (ηλmax(Σk−1) + e)I ckI ckI− ηΣk−1 1\ne I . (G.1)\nThis implies, throughout the algorithm, whenever we want to compute ( ckI−ηΣk−1 )−1 , the matrix under inversion has a bounded condition number. We have the following lemma which relies on classical results from convex optimization: Lemma G.1. Given any b ∈ Rd, the computation of a ∈ Rd satisfying ∥∥a− ( ckI− ηΣk−1 )−1 b ∥∥ 2 ≤ ε‖b‖2 can be done in running time • Õ (√ ηλmax(Σk−1) + 1 · nnz(Σk−1) · log ε−1 ) if conjugate gradient or accelerated gradient descent\nis used;\n• Õ (( nnz(Σk−1)+ √ ηk·maxi∈[k−1] { nnz(Σk−1)3/4nnz(Ai)1/4 }) log ε−1 ) if accelerated SVRG is used.\nProof. This inverse operation is the same as minimizing a convex function f(x) def = 12x >(ckI − ηΣk−1 ) x− b>x. The condition number of Hessian matrix ∇2f(x) is at most O(ηλmax(Σk−1) + 1) according to (G.1), so one can apply conjugate gradient [31] or Nesterov’s accelerated gradient descent [24] to minimize this objective.\nAs for the SVRG type of result, one can write f(x) = 1k−1 ∑k−1 i=1 fi(x) where fi(x) = x >(ckI−\nη(k − 1)Ai ) x− b>x. Each computation of ∇f(x) requires time O(nnz(Σk−1)) and that of ∇fi(x) requires time O(nnz(Ai)). Since ‖∇2fi(x)‖2 ≤ ηk for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time Õ ( nnz(Σk−1) + (ηk)2 maxi∈[k−1]{nnz(Ai)} ) . Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to Õ ( nnz(Σk−1) + √ ηk ·maxi∈[k−1]{nnz(Σk−1)3/4nnz(Ai)1/4} ) .\nResolution to Issue (b). In each iteration, we need to compute some constant ck such that Tr(X1/2UX1/2) = 1. This can be done via a “binary search” procedure which was used widely for shift-and-invert based methods [15]:\n1. Begin with c = ηk + e which is a safe upper bound on ck according to (G.1).\n21We refrain from doing this precisely here because because MMWU analysis is generally “robust against noise”. The authors of [7] have shown that the potential Tr(X\n1−1/q k ) is robust against noise and a completely analogous (but\nlengthy) proof of theirs applies to this paper.\n2. Repeatedly compute some value σ̃ which is a 9/10 approximation of σ def = c − ηλmax(Σk−1).\n(This requires O(1) iterations of power method applied to (cI− ηΣk−1)−1 [15].) 3. If σ̃ ≤ 1e · 910 (which implies σ ≤ 1e ), we end the procedure; otherwise we update c ← c − σ̃/2\nand go to Step 2.\nIt is a simple exercise (with details given in [15]) to show that when the procedure ends, it satisfies 1 2e ≤ c − ηΣk−1 ≤ 1e so c is a lower bound on ck. At this point, it suffices to perform a binary search between [ c, ηk + e ] to find ck. Note that, according to resolution to issue (a), it suffices to compute ck to an additive error of ε̃/poly(d, T ). In sum, the above binary search procedure requires only a logarithmic number of oracle calls to (cI− ηΣk−1)−1, and each time we do so it satisfies c ≤ ηk + e and (ηk + e)I cI− ηΣk−1 12eI. For this reason, the same computational complexity in Lemma G.1 applies.\nThe three resolutions above, combined together, imply that the running time statements in Theorem 3 hold."
    }, {
      "heading" : "H Proof of Theorem 4: Stochastic Online Eigenvector",
      "text" : "Proof of Theorem 4. Define Φk def = (I + ηAk) · · · (I + ηA1)uu>(I + ηA1) · · · (I + ηAk) and Ψk def= (I + ηAk) · · · (I + ηAT )ν1ν>1 (I + ηAT ) · · · (I + ηAk). Let ν1 and λ1 respectively denote the largest eigenvector and eigenvalue of B. We first make three simple calculations:\nTr(ΦT ) = Tr ( (I + ηAT )ΦT−1(I + ηAT ) ) = Tr(ΦT−1) + 2ηTr ( ATΦT−1 ) + η2Tr ( ATΦT−1AT )\n¬ ≤ Tr(ΦT−1) · (1 + (2η + η2)Tr(ATwTw>T ))  ≤ Tr(ΦT−1) · e(2η+η 2)Tr(ATwTw > T ) ≤ · · · ≤ ‖u‖22 · e(2η+η 2) ∑T k=1 w > k Akwk . (H.1)\nE[ν>1 ΦT ν1] = E [ Tr ( ν1ν > 1 (I + ηAT )ΦT−1(I + ηAT ) )] = E[Tr(ν1ν>1 (I + 2ηAT )ΦT−1) + η2ν>1 ATΦT−1AT ν1]\n≥ E[Tr(ν1ν>1 (I + 2ηB)ΦT−1) = (1 + 2ηλ1)E[ν>1 ΦT−1ν1] ® ≥ e2ηλ1−2η2λ21 E[ν>1 ΦT−1ν1] ≥ · · · ¯ ≥ e(2ηλ1−2η2λ21)T . (H.2)\nE[(ν>1 ΦT ν1)2] = E [ Tr(Ψ21) ] = E [ Tr((I + ηA1) 2Ψ2(I + ηA1) 2Ψ2) ] ° ≤ E [ Tr((I + ηA1) 4Ψ22) ]\n± ≤ E [ Tr ( (I + (4η + 11η2)A1)Ψ 2 2 )] = Tr ( (I + (4η + 11η2)B)E[Ψ22] ) ≤ e4ηλ1+11η2λ1 E[Tr(Ψ22)]\n≤ · · · ≤ e(4ηλ1+11η2λ1)T . (H.3) Above, ¬ uses Tr(ATΦT−1AT ) ≤ Tr(ATΦT−1) as well as wkw>k = Φk−1/Tr(Φk−1),  uses 1+x ≤ ex, ® uses 1+2x ≥ e2x−2x2 for x ∈ [0, 1],  uses E[ν>1 Φ0ν1] = 1, ° uses the Lieb-Thirring inequality Tr(ABAB) ≤ Tr(A2B2),22 ± uses (I + ηA1)4 I + (4η + 11η2)A1.\nNow, we can combine (H.2) and (H.3) and apply Chebyshev’s inequality: for every p ∈ (0, 1)\nPr [ ν>1 ΦT ν1 ≤ e(2ηλ1−2η\n2λ21)T − 1√ p\n√ e(4ηλ1+11η2λ1)T − (e(2ηλ1−2η2λ21)T )2 ] ≤ p .\nIn other words, as long as λ1η 2T ≤ p/60, we have with probability at least 1− p,\nTr(ΦT ) ≥ ν>1 ΦT ν1 ≥ e(2ηλ1−2η 2λ21)T · (1− p−1/2\n√ e15η2λ1T − 1) ≥ 1\n2 e(2ηλ1−2η 2λ21)T . (H.4)\n22In fact, we do not need the full power of Lieb-Thirring here because one of the two matrices is rank-1.\nAt the same time, using tail bound for chi-squared distribution, it is easy to derive that with probability at least 1− p we have ‖u‖22 ≤ d+O( √ d log(1/p)) ≤ O(d+ log(1/p)).23 Combining this with (H.1) and (H.4) we have\n(2η + η2) T∑\nk=1\nw>k Akwk ≥ 2ηTλ1 − 2η2λ21T −O ( log(d+ log(1/p)) ) ,\nwhich after dividing both sides by 2η + η2 finishes the proof of Theorem 4."
    }, {
      "heading" : "I A Simple Lower Bound for the λ-Refined Language",
      "text" : "We sketch the proof that for the stochastic online eigenvector problem, for every λ ∈ (0, 1), there exists a constant C > 0, a PSD matrix B satisfying B λI, and a distribution D of (even rank-1) matrices with spectral norm at most 1 and expectation equal to B, such that for every learning algorithm Learner, the total regret must be at least C · √ λT .\nSuch a lower bound naturally translates to the harder adversarial or oblivious settings. We prove this lower bound by reducing the problem to an information-theoretic lower bound that has appeared in our separate paper [6].\nThe lower bound in [6] states that, for every 1 ≥ λ ≥ λ2 ≥ 0, there exists a PSD matrix B with the largest two eigenvalues being λ and λ2, and a distribution D of rank-1 matrices with spectral norm at most 1 and expectation equal to D. Furthermore, for any algorithm Alg that takes T samples from D and outputs a unit vector v ∈ Rd, it must satisfy\nE[1− 〈v, ν1〉2] ≥ Ω ( λ (λ− λ2)2T )\nfor every T ≥ Ω(λ/(λ− λ2)2) ,\nwhere ν1 is the first eigenvector of B, and the expectation is over the randomness of Alg and the T samples from D. After rewriting, we have\nE[v>Bv] ≤ E[λ〈v, ν1〉2 + λ2(1− 〈v, ν1〉2)] = E[λ− (λ− λ2)(1− 〈v, ν1〉2)] ≤ λ− Ω ( λ (λ− λ2)T ) .\nIf we choose λ2 such that T = Θ(λ/(λ− λ2)2), then the above inequality becomes E[v>Bv] ≤ λ− Ω( √ λ/T ) .\nFinally, for any algorithm Learner for the stochastic online eigenvector problem, suppose Learner takes T samples A1, . . . ,AT from D and outputs unit vectors v1, . . . , vT , we can define a corresponding algorithm Alg that outputs v = vk each with probability 1/T . In this way, we have\nE [ T∑\nk=1\nv>k Akvk ] = E [ T∑\nk=1\nv>k Bvk ] = T E [ vBv ] ≤ λT − Ω( √ λT ) .\nIn other words, the total regret of Learner must be at least Ω( √ λT )."
    } ],
    "references" : [ {
      "title" : "Online linear optimization via smoothing",
      "author" : [ "Jacob Abernethy", "Chansoo Lee", "Abhinav Sinha", "Ambuj Tewari" ],
      "venue" : "In COLT, pages 807–823,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Spectral smoothing via random matrix perturbations",
      "author" : [ "Jacob Abernethy", "Chansoo Lee", "Ambuj Tewari" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Using optimization to obtain a widthindependent, parallel, simpler, and faster positive SDP solver",
      "author" : [ "Zeyuan Allen-Zhu", "Yin Tat Lee", "Lorenzo Orecchia" ],
      "venue" : "In Proceedings of the 27th ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition",
      "author" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Even Faster SVD Decomposition Yet Without Agonizing Pain",
      "author" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "First Efficient Convergence for Streaming k-PCA: a Global, Gap- Free, and Near-Optimal Rate",
      "author" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Spectral Sparsification and Regret Minimization Beyond Multiplicative Updates",
      "author" : [ "Zeyuan Allen-Zhu", "Zhenyu Liao", "Lorenzo Orecchia" ],
      "venue" : "In Proceedings of the 47th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives",
      "author" : [ "Zeyuan Allen-Zhu", "Yang Yuan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "The Multiplicative Weights Update Method: a Meta- Algorithm and Applications",
      "author" : [ "Sanjeev Arora", "Elad Hazan", "Satyen Kale" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "A combinatorial, primal-dual approach to semidefinite programs",
      "author" : [ "Sanjeev Arora", "Satyen Kale" ],
      "venue" : "In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing - STOC",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "Online principal components analysis",
      "author" : [ "Christos Boutsidis", "Dan Garber", "Zohar Karnin", "Edo Liberty" ],
      "venue" : "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gabor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Analyze gauss: optimal bounds for privacy-preserving principal component analysis",
      "author" : [ "Cynthia Dwork", "Kunal Talwar", "Abhradeep Thakurta", "Li Zhang" ],
      "venue" : "In STOC,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization",
      "author" : [ "Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford" ],
      "venue" : "In ICML,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Fast and simple PCA via convex optimization",
      "author" : [ "Dan Garber", "Elad Hazan" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Online learning of eigenvectors",
      "author" : [ "Dan Garber", "Elad Hazan", "Tengyu Ma" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "The noisy power method: A meta algorithm with applications",
      "author" : [ "Moritz Hardt", "Eric Price" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "QIP = PSPACE",
      "author" : [ "Rahul Jain", "Zhengfeng Ji", "Sarvagya Upadhyay", "John Watrous" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Online pca with spectral bounds",
      "author" : [ "Zohar Karnin", "Edo Liberty" ],
      "venue" : "In Proceedings of the 28th Annual Conference on Computational Learning Theory (COLT),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Pca with gaussian perturbations",
      "author" : [ "Wojciech Kot  lowski", "Manfred K. Warmuth" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Constructing linear-sized spectral sparsification in almost-linear time",
      "author" : [ "Yin Tat Lee", "He Sun" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "A Universal Catalyst for First-Order Optimization",
      "author" : [ "Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Introductory Lectures on Convex Programming Volume: A Basic course, volume I",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Online pca with optimal regrets",
      "author" : [ "Jiazhong Nie", "Wojciech Kot  lowski", "Manfred K Warmuth" ],
      "venue" : "In International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Fast Approximation Algorithms for Graph Partitioning using Spectral and Semidefinite-Programming Techniques",
      "author" : [ "Lorenzo Orecchia" ],
      "venue" : "PhD thesis, EECS Department,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Approximating the exponential, the lanczos method and an Õ(m)-time spectral algorithm for balanced separator",
      "author" : [ "Lorenzo Orecchia", "Sushant Sachdeva", "Nisheeth K. Vishnoi" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Faster and simpler width-independent parallel algorithms for positive semidefinite programming",
      "author" : [ "Richard Peng", "Kanat Tangwongsan" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "SDCA without Duality, Regularization, and Individual Convexity",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "In ICML,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Convergence of stochastic gradient descent for pca",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In ICML,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "An introduction to the conjugate gradient method without the agonizing pain",
      "author" : [ "Jonathan Richard Shewchuk" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1994
    }, {
      "title" : "Note on the gamma function",
      "author" : [ "J.G. Wendel" ],
      "venue" : "The American Mathematical Monthly,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1948
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Matrix multiplicative weight update (MMWU) [26] is an extremely powerful algorithmic tool for computer science and related fields.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 8,
      "context" : "The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]).",
      "startOffset" : 237,
      "endOffset" : 240
    }, {
      "referenceID" : 11,
      "context" : "The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]).",
      "startOffset" : 254,
      "endOffset" : 258
    }, {
      "referenceID" : 24,
      "context" : "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 26,
      "context" : "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 25,
      "context" : "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 6,
      "context" : "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 20,
      "context" : "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 17,
      "context" : "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
      "startOffset" : 282,
      "endOffset" : 286
    }, {
      "referenceID" : 0,
      "context" : "The player receives a gain Ak •Wk def = Tr(AkWk) ∈ [0, 1].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 24,
      "context" : "The best choice η = √ log d/ √ T yields a total regret at most O( √ T log d) [26], and this is optimal up to constant [9].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "The best choice η = √ log d/ √ T yields a total regret at most O( √ T log d) [26], and this is optimal up to constant [9].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "Some authors also refer to MMWU as the follow-the-regularized-leader strategy or FTRL for short, because MMWU can be analyzed from a mirror-descent view with the matrix entropy function as its regularizer [7].",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "If instead of playing an arbitrary matrix in ∆d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:",
      "startOffset" : 202,
      "endOffset" : 221
    }, {
      "referenceID" : 12,
      "context" : "If instead of playing an arbitrary matrix in ∆d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:",
      "startOffset" : 202,
      "endOffset" : 221
    }, {
      "referenceID" : 15,
      "context" : "If instead of playing an arbitrary matrix in ∆d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:",
      "startOffset" : 202,
      "endOffset" : 221
    }, {
      "referenceID" : 19,
      "context" : "If instead of playing an arbitrary matrix in ∆d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:",
      "startOffset" : 202,
      "endOffset" : 221
    }, {
      "referenceID" : 23,
      "context" : "If instead of playing an arbitrary matrix in ∆d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:",
      "startOffset" : 202,
      "endOffset" : 221
    }, {
      "referenceID" : 6,
      "context" : "This more challenging setting is very desirable for multiple reasons: • in many applications —such as graph problems [7, 22]— Ak does not depend on wk; • vector-based strategies wk can be cheaper to compute and more efficient to communicate.",
      "startOffset" : 117,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "This more challenging setting is very desirable for multiple reasons: • in many applications —such as graph problems [7, 22]— Ak does not depend on wk; • vector-based strategies wk can be cheaper to compute and more efficient to communicate.",
      "startOffset" : 117,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "MMWU [7, 9] Õ( √ T ) O(d) Õ ( d ε2 )",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "MMWU [7, 9] Õ( √ T ) O(d) Õ ( d ε2 )",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "MMWU with JL [7, 28] Õ( √ T ) Õ ( T 5 4 nnz(Σ) ) Õ ( 1 ε4.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 26,
      "context" : "MMWU with JL [7, 28] Õ( √ T ) Õ ( T 5 4 nnz(Σ) ) Õ ( 1 ε4.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "FTPL (T ≥ d only) [16] Õ( √ dT ) Õ ( T 3 4 d− 1 4 nnz(Σ) ) Õ ( d ε3.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "block power method [16] Õ( √ T ) O ( nnz(Σ) ) Õ ( 1 ε2 nnz(Σ) )",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 20,
      "context" : "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 26,
      "context" : "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "Most notably, Garber, Hazan and Ma [16] proposed to compute an (approximate) leading eigenvector of the matrix Σk−1 +rr> at iteration k, where r is a random vector whose norm is carefully chosen.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : "For this problem, Garber, Hazan, and Ma [16] showed that a block power method matches the optimum regret and enjoys an efficient O(nnz(ΣT ))-time implementation per iteration.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 28,
      "context" : "Shamir [30] analyzed the so-called Oja’s algorithm but his total regret is O( √ dT log(T )) which is a factor √ d worse than optimum.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "If nnz(ΣT ) = d 2 and nnz(A) = O(d), our running time is O(d) times faster than [16].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "In the special case of Ak being rank-1, the Õ( √ T ) regret for Oja’s algorithm was recently shown by [6], using different techniques from us.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "MMWU [7, 9] Õ( √ λT ) O(d) Õ ( λd ε2 )",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "MMWU [7, 9] Õ( √ λT ) O(d) Õ ( λd ε2 )",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "MMWU with JL [7, 28] Õ( √ λT ) Õ ( T 5 4 λ− 3 4 nnz(Σ) ) Õ ( λ ε4.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 26,
      "context" : "MMWU with JL [7, 28] Õ( √ λT ) Õ ( T 5 4 λ− 3 4 nnz(Σ) ) Õ ( λ ε4.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "block power method [16] Õ( √ λT ) O ( nnz(Σ) ) Õ ( 1 ε2 nnz(Σ) )",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "We denote by Σ = A1 + · · ·+ AT , by nnz(A) = maxk∈[T ] { nnz(Ak) } , and by λ = 1 T λmax(Σ) ∈ [0, 1].",
      "startOffset" : 95,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "7 The block power method (for the stochastic online eigenvector problem) can also be analyzed in this λ-refined language, for instance by modifying the proof in [16].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "[13] and independently shown by Kot lowski and Warmuth [21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[13] and independently shown by Kot lowski and Warmuth [21].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "showed that FTPL strategies can also be analyzed using a FTRL framework [1].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "studied the high-rank variant using MMWU [25], but their per-iteration running time is still O(d3) due to eigendecomposition.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k · poly(1/ε)) vectors instead of k but with a good PCA reconstruction error.",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k · poly(1/ε)) vectors instead of k but with a good PCA reconstruction error.",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "The stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "The stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "For the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition).",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "For the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition).",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "In a recent result, the authors of [7] generalized MMWU to `1−1/q regularized strategies.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "We encourage interested readers to see the introduction of [7] for more background information, but we shall make this present paper self-contained.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "Prior work on MMWU and its extensions relies heavily on one of the following trace inequalities [7]: Golden-Thompson inequality : Tr(e) ≤ Tr ( ee )",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "If this is the case, then one can compute the 3× 3 matrix ( ui Xkuj ) i,j∈[3] explicitly, and then we can obtain its rank-3 eigendecomposition X 1/2 k UX 1/2 k = ∑3 j=1 pj · yjy> j in O(d) time.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "At a high level, issue (a) is not a big deal because if v′ j satisfies ‖vj − v′ j‖2 ≤ ε̃/poly(d, T ) and we use v′ j instead of vj , then the final regret is affected by less than ε̃; issue (b) can be dealt as long as we perform a careful binary search to find ck, similar to prior work [7]; issue (c) can be done as long as we have a good control on the condition number of the matrix ( ckI− ηΣk−1 ) .",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 15,
      "context" : "We thank Dan Garber and Tengyu Ma for clarifying some results of prior work [16].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : "[16]",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "from N (0, 1) for every i ∈ [d], j ∈ [3].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "1) like it was used in [7].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "Notice that TrU = 13 ∑ i∈[d],j∈[3](uj,i) 2 so 3TrU is distributed according to chisquared distribution χ2(3d) whose PDF is p(x) = 2 − 3d 2 e− x 2 x 3d 2 −1 Γ(3d/2) .",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 30,
      "context" : "Wendell [32]), and the second inequality uses our assumption on q.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "Let {Zt}t=1 be a random process with respect to a filter {0,Ω} = F0 ⊂ F1 ⊂ · · · ⊂ FT and each Zt ∈ [0, 1] is Ft-measurable.",
      "startOffset" : 100,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "(The first inequality has used (a− b)2 ≤ a2 + b2 when a, b ≥ 0, and the second has used Zt ∈ [0, 1].",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Moreover, this can be done in time O(d) as long as we can compute the three vectors { Xuj } j∈[3] to an additive ε̃/poly(d, T ) error in Euclidean norm.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 29,
      "context" : "1), so one can apply conjugate gradient [31] or Nesterov’s accelerated gradient descent [24] to minimize this objective.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "1), so one can apply conjugate gradient [31] or Nesterov’s accelerated gradient descent [24] to minimize this objective.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "Since ‖∇fi(x)‖2 ≤ ηk for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time Õ ( nnz(Σk−1) + (ηk)2 maxi∈[k−1]{nnz(Ai)} ) .",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : "Since ‖∇fi(x)‖2 ≤ ηk for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time Õ ( nnz(Σk−1) + (ηk)2 maxi∈[k−1]{nnz(Ai)} ) .",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to Õ ( nnz(Σk−1) + √ ηk ·maxi∈[k−1]{nnz(Σk−1)nnz(Ai)} ) .",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to Õ ( nnz(Σk−1) + √ ηk ·maxi∈[k−1]{nnz(Σk−1)nnz(Ai)} ) .",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "This can be done via a “binary search” procedure which was used widely for shift-and-invert based methods [15]:",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "The authors of [7] have shown that the potential Tr(X 1−1/q k ) is robust against noise and a completely analogous (but lengthy) proof of theirs applies to this paper.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "(This requires O(1) iterations of power method applied to (cI− ηΣk−1)−1 [15].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "It is a simple exercise (with details given in [15]) to show that when the procedure ends, it satisfies 1 2e ≤ c − ηΣk−1 ≤ 1e so c is a lower bound on ck.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "Above, ¬ uses Tr(ATΦT−1AT ) ≤ Tr(ATΦT−1) as well as wkw> k = Φk−1/Tr(Φk−1), ­ uses 1+x ≤ ex, ® uses 1+2x ≥ e2x−2x for x ∈ [0, 1], ­ uses E[ν> 1 Φ0ν1] = 1, ° uses the Lieb-Thirring inequality Tr(ABAB) ≤ Tr(A2B2),22 ± uses (I + ηA1) I + (4η + 11η)A1.",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "We prove this lower bound by reducing the problem to an information-theoretic lower bound that has appeared in our separate paper [6].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "The lower bound in [6] states that, for every 1 ≥ λ ≥ λ2 ≥ 0, there exists a PSD matrix B with the largest two eigenvalues being λ and λ2, and a distribution D of rank-1 matrices with spectral norm at most 1 and expectation equal to D.",
      "startOffset" : 19,
      "endOffset" : 22
    } ],
    "year" : 2017,
    "abstractText" : "Matrix multiplicative weight update (MMWU) [26] is an extremely powerful algorithmic tool for computer science and related fields. However, it comes with a slow running time due to the matrix exponential and eigendecomposition computations. For this reason, many researchers studied the followed-the-perturbed-leader (FTPL) framework which is faster, but a factor √ d worse than the optimal regret of MMWU for dimension-d matrices. In this paper, we propose a followed-the-compressed-leader framework which, not only matches the optimal regret of MMWU (up to polylog factors), but runs even faster than FTPL. Our main idea is to “compress” the matrix exponential computation to dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting. This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16]. ar X iv :1 70 1. 01 72 2v 1 [ cs .L G ] 6 J an 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}