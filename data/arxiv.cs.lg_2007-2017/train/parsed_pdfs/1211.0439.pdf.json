{
  "name" : "1211.0439.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning curves for multi-task Gaussian process regression",
    "authors" : [ "Simon R F Ashton", "Peter Sollich" ],
    "emails" : [ "peter.sollich@kcl.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction and motivation",
      "text" : "Gaussian processes (GPs) [1] have been popular in the NIPS community for a number of years now, as one of the key non-parametric Bayesian inference approaches. In the simplest case one can use a GP prior when learning a function from data. In line with growing interest in multi-task or transfer learning, where relatedness between tasks is used to aid learning of the individual tasks (see e.g. [2, 3]), GPs have increasingly also been used in a multi-task setting. A number of different choices of covariance functions have been proposed [4, 5, 6, 7, 8]. These differ e.g. in assumptions on whether the functions to be learned are related to a smaller number of latent functions or have free-form inter-task correlations; for a recent review see [9].\nGiven this interest in multi-task GPs, one would like to quantify the benefits that they bring compared to single-task learning. PAC-style bounds for classification [2, 3, 10] in more general multi-task scenarios exist, but there has been little work on average case analysis. The basic question in this setting is: how does the Bayes error on a given task depend on the number of training examples for all tasks, when averaged over all data sets of the given size. For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20]. Already two-task GP regression is much more difficult to analyse, and progress was made only very recently at NIPS 2009 [21], where upper and lower bounds for learning curves were derived. The tightest of these bounds, however, either required evaluation by Monte Carlo sampling, or assumed knowledge of the corresponding single-task learning curves. Here our aim is to obtain accurate learning curve approximations that apply to an arbitrary number T of tasks, and that can be evaluated explicitly without recourse to sampling.\nar X\niv :1\n21 1.\n04 39\nv1 [\ncs .L\nG ]\n2 N\nov 2\nWe begin (Sec. 2) by expressing the Bayes error for any single task in a multi-task GP regression problem in a convenient feature space form, where individual training examples enter additively. This requires the introduction of a non-trivial tensor structure combining feature space components and tasks. Considering the change in error when adding an example for some task leads to partial differential equations linking the Bayes errors for all tasks. Solving these using the method of characteristics then gives, as our primary result, the desired learning curve approximation (Sec. 3). In Sec. 4 we discuss some of its predictions. The approximation correctly delineates the limits of pure transfer learning, when all examples are from tasks other than the one of interest. Next we compare with numerical simulations for some two-task scenarios, finding good qualitative agreement. These results also highlight a surprising feature, namely that asymptotically the relatedness between tasks can become much less useful. We analyse this effect in some detail, showing that it is most extreme for learning of smooth functions. Finally we discuss the case of many tasks, where there is an unexpected separation of the learning curves into a fast initial error decay arising from “collective learning”, and a much slower final part where tasks are learned almost independently."
    }, {
      "heading" : "2 GP regression and Bayes error",
      "text" : "We consider GP regression for T functions fτ (x), τ = 1, 2, . . . , T . These functions have to be learned from n training examples (x`, τ`, y`), ` = 1, . . . , n. Here x` is the training input, τ` ∈ {1, . . . , T} denotes which task the example relates to, and y` is the corresponding training output. We assume that the latter is given by the target function value fτ`(x`) corrupted by i.i.d. additive Gaussian noise with zero mean and variance σ2τ` . This setup allows the noise level σ 2 τ to depend on the task.\nIn GP regression the prior over the functions fτ (x) is a Gaussian process. This means that for any set of inputs x` and task labels τ`, the function values {fτ`(x`)} have a joint Gaussian distribution. As is common we assume this to have zero mean, so the multi-task GP is fully specified by the covariances 〈fτ (x)fτ ′(x′)〉 = C(τ, x, τ ′, x′). For this covariance we take the flexible form from [5], 〈fτ (x)fτ ′(x′)〉 = Dττ ′C(x, x′). Here C(x, x′) determines the covariance between function values at different input points, encoding “spatial” behaviour such as smoothness and the lengthscale(s) over which the functions vary, while the matrixD is a free-form inter-task covariance matrix.\nOne of the attractions of GPs for regression is that, even though they are non-parametric models with (in general) an infinite number of degrees of freedom, predictions can be made in closed form, see e.g. [1]. For a test point x for task τ , one would predict as output the mean of fτ (x) over the (Gaussian) posterior, which is yTK−1kτ (x). Here K is the n × n Gram matrix with entries K`m = Dτ`τmC(x`, xm)+σ 2 τ` δ`m, while kτ (x) is a vector with the n entries kτ,` = Dτ`τC(x`, x). The error bar would be taken as the square root of the posterior variance of fτ (x), which is\nVτ (x) = DττC(x, x)− kTτ (x)K−1kτ (x) (1) The learning curve for task τ is defined as the mean-squared prediction error, averaged over the location of test input x and over all data sets with a specified number of examples for each task, say n1 for task 1 and so on. As is standard in learning curve analysis we consider a matched scenario where the training outputs y` are generated from the same prior and noise model that we use for inference. In this case the mean-squared prediction error ̂τ is the Bayes error, and is given by the average posterior variance [1], i.e. ̂τ = 〈Vτ (x)〉x. To obtain the learning curve this is averaged over the location of the training inputs x`: τ = 〈̂τ 〉. This average presents the main challenge for learning curve prediction because the training inputs feature in a highly nonlinear way in Vτ (x). Note that the training outputs, on the other hand, do not appear in the posterior variance Vτ (x) and so do not need to be averaged over.\nWe now want to write the Bayes error ̂τ in a form convenient for performing, at least approximately, the averages required for the learning curve. Assume that all training inputs x`, and also the test input x, are drawn from the same distribution P (x). One can decompose the input-dependent part of the covariance function into eigenfunctions relative to P (x), according to C(x, x′) =∑ i λiφi(x)φi(x\n′). The eigenfunctions are defined by the condition 〈C(x, x′)φi(x′)〉x′ = λiφi(x) and can be chosen to be orthonormal with respect to P (x), 〈φi(x)φj(x)〉x = δij . The sum over i here is in general infinite (unless the covariance function is degenerate, as e.g. for the dot product kernel C(x, x′) = x · x′). To make the algebra below as simple as possible, we let the eigenvalues λi be arranged in decreasing order and truncate the sum to the finite range i = 1, . . . ,M ; M is then some large effective feature space dimension and can be taken to infinity at the end.\nIn terms of the above eigenfunction decomposition, the Gram matrix has elements K`m = Dτ`τm ∑ i λiφi(x`)φi(xm)+σ 2 τ` δ`m = ∑ i,τ,j,τ ′ δτ`,τφi(x`)λiδijDττ ′φj(xm)δτ ′,τm+σ 2 τ` δ`m\nor in matrix formK = ΨLΨT + Σ where Σ is the diagonal matrix from the noise variances and\nΨ`,iτ = δτ`,τφi(x`), Liτ,jτ ′ = λiδijDττ ′ (2)\nHere Ψ has its second index ranging over M (number of kernel eigenvalues) times T (number of tasks) values; L is a square matrix of this size. In Kronecker (tensor) product notation, L = D⊗Λ if we define Λ as the diagonal matrix with entries λiδij . The Kronecker product is convenient for the simplifications below; we will use that for generic square matrices, (A ⊗ B)(A′ ⊗ B′) = (AA′)⊗ (BB′), (A⊗B)−1 = A−1 ⊗B−1, and tr (A⊗B) = (trA)(trB). In thinking about the mathematical expressions, it is often easier to picture Kronecker products over feature spaces and tasks as block matrices. For example, L can then be viewed as consisting of T ×T blocks, each of which is proportional to Λ.\nTo calculate the Bayes error, we need to average the posterior variance Vτ (x) over the test input x. The first term in (1) then becomesDττ 〈C(x, x)〉 = Dττ trΛ. In the second one, we need to average\n〈kτ,`(x)kτ,m〉x = Dττ`〈C(x`, x)C(x, xm)〉xDτmτ = Dττ` ∑ ij λiλjφi(x`)〈φi(x)φj(x)〉xφj(xm)Dτmτ\n= ∑\ni,τ ′,j,τ ′′\nDττ ′Ψl,iτ ′λiλjδijΨm,jτ ′′Dτ ′′τ\nIn matrix form this is 〈kτ (x)kTτ (x)〉x = Ψ[(DeτeTτD) ⊗ Λ2]ΨT = ΨMτΨT Here the last equality defines Mτ , and we have denoted by eτ the T -dimensional vector with τ -th component equal to one and all others zero. Multiplying by the inverse Gram matrix K−1 and taking the trace gives the average of the second term in (1); combining with the first gives the Bayes error on task τ\n̂τ = 〈Vτ (x)〉x = Dττ trΛ− trΨMτΨT(ΨLΨT + Σ)−1\nApplying the Woodbury identity and re-arranging yields\n̂τ = Dττ trΛ− trMτΨTΣ−1Ψ(I +LΨTΣ−1Ψ)−1\n= Dττ trΛ− trMτL−1[I − (I +LΨTΣ−1Ψ)−1]\nBut\ntrMτL −1 = tr {[(DeτeTτD)⊗Λ2][D ⊗Λ]−1}\n= tr {[DeτeTτ ]⊗Λ} = eTτDeτ trΛ = Dττ trΛ\nso the first and second terms in the expression for ̂τ cancel and one has\n̂τ = trMτL −1(I +LΨTΣ−1Ψ)−1 = trL−1MτL −1(L−1 + ΨTΣ−1Ψ)−1\n= tr [D ⊗Λ]−1[(DeτeTτD)⊗Λ2][D ⊗Λ]−1(L−1 + ΨTΣ−1Ψ)−1 = tr [eτe T τ ⊗ I](L−1 + ΨTΣ−1Ψ)−1\nThe matrix in square brackets in the last line is just a projector Pτ onto task τ ; thought of as a matrix of T ×T blocks (each of size M ×M ), this has an identity matrix in the (τ, τ) block while all other blocks are zero. We can therefore write, finally, for the Bayes error on task τ ,\n̂τ = trPτ (L −1 + ΨTΣ−1Ψ)−1 (3)\nBecause Σ is diagonal and given the definition (2) of Ψ, the matrix ΨTΣ−1Ψ is a sum of contributions from the individual training examples ` = 1, . . . , n. This will be important for deriving the learning curve approximation below. We note in passing that, because ∑ τ Pτ = I , the sum of the\nBayes errors on all tasks is ∑ τ ̂τ = tr (L\n−1+ΨTΣ−1Ψ)−1, in close analogy to the corresponding expression for the single-task case [13]."
    }, {
      "heading" : "3 Learning curve prediction",
      "text" : "To obtain the learning curve τ = 〈̂τ 〉, we now need to carry out the average 〈. . .〉 over the training inputs. To help with this, we can extend an approach for the single-task scenario [13] and define a response or resolvent matrix G = (L−1 + ΨTΣ−1Ψ + ∑ τ vτPτ )\n−1 with auxiliary parameters vτ that will be set back to zero at the end. One can then ask how G = 〈G〉 and hence τ ′ = trPτ ′G changes with the number nτ of training points for task τ . Adding an example at position x for task τ increases ΨTΣ−1Ψ by σ−2τ φτφ T τ , where φτ has elements (φτ )iτ ′ = φi(x)δττ ′ . Evaluating the difference (G−1 + σ−2τ φτφTτ )−1 − G with the help of the Woodbury identity and approximating it with a derivative gives\n∂G ∂nτ = − Gφτφ T τ G σ2τ + φ T τ Gφτ\nThis needs to be averaged over the new example and all previous ones. If we approximate by averaging numerator and denominator separately we get\n∂G ∂nτ =\n1\nσ2τ + trPτG\n∂G ∂vτ (4)\nHere we have exploited for the average over x that the matrix 〈φτφTτ 〉x has (i, τ ′), (j, τ ′′)-entry 〈φi(x)φj(x)〉xδττ ′δττ ′′ = δijδττ ′δττ ′′ , hence simply 〈φτφTτ 〉x = Pτ . We have also used the auxiliary parameters to rewrite −〈GPτG〉 = ∂〈G〉/∂vτ = ∂G/∂vτ . Finally, multiplying (4) by Pτ ′ and taking the trace gives the set of quasi-linear partial differential equations\n∂ τ ′ ∂nτ =\n1\nσ2τ + τ\n∂ τ ′ ∂vτ (5)\nThe remaining task is now to find the functions τ (n1, . . . , nT , v1, . . . , vT ) by solving these differential equations. We initially attempted to do this by tracking the τ as examples are added one task at a time, but the derivation is laborious already for T = 2 and becomes prohibitive beyond. Far more elegant is to adapt the method of characteristics to the present case. We need to find a 2T -dimensional surface in the 3T -dimensional space (n1, . . . , nT , v1, . . . , vT , 1, . . . , T ), which is specified by the T functions τ (. . .). A small change (δn1, . . . , δnT , δv1, . . . , δvT , δ 1, . . . , δ T ) in all 3T coordinates is tangential to this surface if it obeys the T constraints (one for each τ ′)\nδ τ ′ = ∑ τ ( ∂ τ ′ ∂nτ δnτ + ∂ τ ′ ∂vτ δvτ ) From (5), one sees that this condition is satisfied whenever δ τ = 0 and δnτ = −δvτ (σ2τ + τ ) It follows that all the characteristic curves given by τ (t) = τ,0 = const., vτ (t) = vτ,0(1 − t), nτ (t) = vτ,0(σ 2 τ + τ,0) t for t ∈ [0, 1] are tangential to the solution surface for all t, so lie within this surface if the initial point at t = 0 does. Because at t = 0 there are no training examples (nτ (0) = 0), this initial condition is satisfied by setting\nτ,0 = trPτ\n( L−1 +\n∑ τ ′ vτ ′,0Pτ ′ )−1 Because τ (t) is constant along the characteristic curve, we get by equating the values at t = 0 and t = 1\nτ,0 = trPτ\n( L−1 +\n∑ τ ′ vτ ′,0Pτ ′ )−1 = τ ({nτ ′ = vτ ′,0(σ2τ ′ + τ ′,0)}, {vτ ′ = 0})\nExpressing vτ ′,0 in terms of nτ ′ gives then\nτ = trPτ\n( L−1 +\n∑ τ ′ nτ ′ σ2τ ′ + τ ′ Pτ ′\n)−1 (6)\nThis is our main result: a closed set of T self-consistency equations for the average Bayes errors τ . Given L as defined by the eigenvalues λi of the covariance function, the noise levels σ2τ and the\nnumber of examples nτ for each task, it is straightforward to solve these equations numerically to find the average Bayes error τ for each task.\nThe r.h.s. of (6) is easiest to evaluate if we view the matrix inside the brackets as consisting of M ×M blocks of size T ×T (which is the reverse of the picture we have used so far). The matrix is then block diagonal, with the blocks corresponding to different eigenvalues λi. Explicitly, because L−1 = D−1 ⊗Λ−1, one has\nτ = ∑ i ( λ−1i D −1 + diag({ nτ ′ σ2τ ′ + τ ′ }) )−1 ττ\n(7)"
    }, {
      "heading" : "4 Results and discussion",
      "text" : "We now consider the consequences of the approximate prediction (7) for multi-task learning curves in GP regression. A trivial special case is the one of uncorrelated tasks, where D is diagonal. Here one recovers T separate equations for the individual tasks as expected, which have the same form as for single-task learning [13]."
    }, {
      "heading" : "4.1 Pure transfer learning",
      "text" : "Consider now the case of pure transfer learning, where one is learning a task of interest (say τ = 1) purely from examples for other tasks. What is the lowest average Bayes error that can be obtained? Somewhat more generally, suppose we have no examples for the first T0 tasks, n1 = . . . = nT0 = 0, but a large number of examples for the remaining T1 = T − T0 tasks. Denote E = D−1 and write this in block form as\nE = ( E00 E01 ET01 E11 ) Now multiply by λ−1i and add in the lower right block a diagonal matrix N = diag({nτ/(σ2τ + τ )}τ=T0+1,...,T ). The matrix inverse in (7) then has top left block λi[E−100 + E −1 00 E01(λiN + E11 −ET01E−100 E01)−1ET01E −1 00 ]. As the number of examples for the last T1 tasks grows, so do all (diagonal) elements of N . In the limit only the term λiE−100 survives, and summing over i gives 1 = trΛ(E −1 00 )11 = 〈C(x, x)〉(E −1 00 )11. The Bayes error on task 1 cannot become lower than this, placing a limit on the benefits of pure transfer learning. That this prediction of the approximation (7) for such a lower limit is correct can also be checked directly: once the last T1 tasks fτ (x) (τ = T0 + 1, . . . T ) have been learn perfectly, the posterior over the first T0 functions is, by standard Gaussian conditioning, a GP with covariance C(x, x′)(E00)−1. Averaging the posterior variance of f1(x) then gives the Bayes error on task 1 as 1 = 〈C(x, x)〉(E−100 )11, as found earlier. This analysis can be extended to the case where there are some examples available also for the first T0 tasks. One finds for the generalization errors on these tasks the prediction (7) withD−1 replaced by E00. This is again in line with the above form of the GP posterior after perfect learning of the remaining T1 tasks."
    }, {
      "heading" : "4.2 Two tasks",
      "text" : "We next analyse how well the approxiation (7) does in predicting multi-task learning curves for T = 2 tasks. Here we have the work of Chai [21] as a baseline, and as there we choose\nD = ( 1 ρ ρ 1 ) The diagonal elements are fixed to unity, as in a practical application where one would scale both task functions f1(x) and f2(x) to unit variance; the degree of correlation of the tasks is controlled by ρ. We fix π2 = n2/n and plot learning curves against n. In numerical simulations we ensure integer values of n1 and n2 by setting n2 = bnπ2c, n1 = n − n2; for evaluation of (7) we use directly n2 = nπ2, n1 = n(1− π2). For simplicity we consider equal noise levels σ21 = σ22 = σ2. As regards the covariance function and input distribution, we analyse first the scenario studied in [21]: a squared exponential (SE) kernel C(x, x′) = exp[−(x − x′)2/(2l2)] with lengthscale l, and one-dimensional inputs x with a Gaussian distributionN (0, 1/12). The kernel eigenvalues λi\nare known explicitly from [22] and decay exponentially with i. Figure 1(left) compares numerically simulated learning curves with the predictions for 1, the average Bayes error on task 1, from (7). Five pairs of curves are shown, for ρ2 = 0, 0.25, 0.5, 0.75, 1. Note that the two extreme values represent single-task limits, where examples from task 2 are either ignored (ρ = 0) or effectively treated as being from task 1 (ρ = 1). Our predictions lie generally below the true learning curves, but qualitatively represent the trends well, in particular the variation with ρ2. The curves for the different ρ2 values are fairly evenly spaced vertically for small number of examples, n, corresponding to a linear dependence on ρ2. As n increases, however, the learning curves for ρ < 1 start to bunch together and separate from the one for the fully correlated case (ρ = 1). The approximation (7) correctly captures this behaviour, which is discussed in more detail below.\nFigure 1(middle) has analogous results for the case of inputs x uniformly distributed on the interval [0, 1]; the λi here decay exponentially with i2 [17]. Quantitative agreement between simulations and predictions is better for this case. The discussion in [17] suggests that this is because the approximation method we have used implicitly neglects spatial variation of the dataset-averaged posterior variance 〈Vτ (x)〉; but for a uniform input distribution this variation will be weak except near the ends of the input range [0, 1]. Figure 1(right) displays similar results for an OU kernel C(x, x′) = exp(−|x − x′|/l), showing that our predictions also work well when learning rough (nowhere differentiable) functions."
    }, {
      "heading" : "4.3 Asymptotic uselessness",
      "text" : "The two-task results above suggest that multi-task learning is less useful asymptotically: when the number of training examples n is large, the learning curves seem to bunch towards the curve for ρ = 0, where task 2 examples are ignored, except when the two tasks are fully correlated (ρ = 1). We now study this effect.\nWhen the number of examples for all tasks becomes large, the Bayes errors τ will become small and eventually be negligible compared to the noise variances σ2τ in (7). One then has an explicit prediction for each τ , without solving T self-consistency equations. If we write, for T tasks, nτ = nπτ with πτ the fraction of examples for task τ , and set γτ = πτ/σ2τ , then for large n\nτ = ∑ i ( λ−1i D −1 + nΓ )−1 ττ = ∑ i(Γ −1/2[λ−1i (Γ 1/2DΓ1/2)−1 + nI]−1Γ−1/2)ττ (8)\nwhere Γ = diag(γ1, . . . , γT ). Using an eigendecomposition of the symmetric matrix Γ1/2DΓ1/2 =∑T a=1 δavav T a , one then shows in a few lines that (8) can be written as\nτ ≈ γ−1τ ∑ a(va,τ ) 2δag(nδa) (9)\nwhere g(h) = tr (Λ−1 + h)−1 = ∑ i(λ −1 i + h)\n−1 and va,τ is the τ -th component of the a-th eigenvector va. This is the general asymptotic form of our prediction for the average Bayes error for task τ .\nTo get a more explicit result, consider the case where sample functions from the GP prior have (mean-square) derivatives up to order r. The kernel eigenvalues λi then decay as1 i−(2r+2) for large i, and using arguments from [17] one deduces that g(h) ∼ h−α for large h, with α = (2r+1)/(2r+ 2). In (9) we can then write, for large n, g(nδa) ≈ (δa/γτ )−αg(nγτ ) and hence\nτ ≈ g(nγτ ){ ∑ a(va,τ ) 2(δa/γτ ) 1−α} (10)\nWhen there is only a single task, δ1 = γ1 and this expression reduces to 1 = g(nγ1) = g(n1/σ21). Thus g(nγτ ) = g(nτ/σ2τ ) is the error we would get by ignoring all examples from tasks other than τ , and the term in {. . .} in (10) gives the “multi-task gain”, i.e. the factor by which the error is reduced because of examples from other tasks. (The absolute error reduction always vanishes trivially for n→∞, along with the errors themselves.) One observation can be made directly. Learning of very smooth functions, as defined e.g. by the SE kernel, corresponds to r → ∞ and hence α → 1, so the multi-task gain tends to unity: multi-task learning is asymptotically useless. The only exception occurs when some of the tasks are fully correlated, because one or more of the eigenvalues δa of Γ1/2DΓ1/2 will then be zero.\nFig. 2(left) shows this effect in action, plotting Bayes error against ρ2 for the two-task setting of Fig. 1(left) with n = 500. Our predictions capture the nonlinear dependence on ρ2 quite well, though the effect is somewhat weaker in the simulations. For larger n the predictions approach a curve that is constant for ρ < 1, signifying negligible improvement from multi-task learning except at ρ = 1. It is worth contrasting this with the lower bound from [21], which is linear in ρ2. While this provides a very good approximation to the learning curves for moderate n [21], our results here show that asymptotically this bound can become very loose.\nWhen predicting rough functions, there is some asymptotic improvement to be had from multi-task learning, though again the multi-task gain is nonlinear in ρ2: see Fig. 2(left, inset) for the OU case, which has r = 1). A simple expression for the gain can be obtained in the limit of many tasks, to which we turn next.\n1 See the discussion of Sacks-Ylvisaker conditions in e.g. [1]; we consider one-dimensional inputs here though the discussion can be generalized."
    }, {
      "heading" : "4.4 Many tasks",
      "text" : "We assume as for the two-task case that all inter-task correlations, Dτ,τ ′ with τ 6= τ ′, are equal to ρ, while Dτ,τ = 1. This setup was used e.g. in [23], and can be interpreted as each task having a component proportional to √ ρ of a shared latent function, with an independent task-specific signal\nin addition. We assume for simplicity that we have the same number nτ = n/T of examples for each task, and that all noise levels are the same, σ2τ = σ\n2. Then also all Bayes errors τ = will be the same. Carrying out the matrix inverses in (7) explicitly, one can then write this equation as\n= gT (n/(σ 2 + ), ρ) (11)\nwhere gT (h, ρ) is related to the single-task function g(h) from above by\ngT (h, ρ) = T − 1 T\n(1− ρ)g(h(1− ρ)/T ) + ( ρ+\n1− ρ T\n) g(h[ρ+ (1− ρ)/T ]) (12)\nNow consider the limit T → ∞ of many tasks. If n and hence h = n/(σ2 + ) is kept fixed, gT (h, ρ) → (1 − ρ) + ρg(hρ); here we have taken g(0) = 1 which corresponds to trΛ = 〈C(x, x)〉x = 1 as in the examples above. One can then deduce from (11) that the Bayes error for any task will have the form = (1−ρ)+ρ̃, where ̃ decays from one to zero with increasing n as for a single task, but with an effective noise level σ̃2 = (1− ρ+ σ2)/ρ. Remarkably, then, even though here n/T → 0 so that for most tasks no examples have been seen, the Bayes error for each task decreases by “collective learning” to a plateau of height 1−ρ. The remaining decay of to zero happens only once n becomes of order T . Here one can show, by taking T →∞ at fixed h/T in (12) and inserting into (11), that = (1 − ρ)̄ where ̄ again decays as for a single task but with an effective number of examples n̄ = n/T and effective noise level σ̄2/(1− ρ). This final stage of learning therefore happens only when each task has seen a considerable number of exampes n/T . Fig. 2(right) validates these predictions against simulations, for a number of tasks (T = 200) that is in the same ballpark as in the many-tasks application example of [24]. The inset for T = 1000 shows clearly how the two learning curve stages separate as T becomes larger.\nFinally we can come back to the multi-task gain in the asymptotic stage of learning. For GP priors with sample functions with derivatives up to order r as before, the function ̄ from above will decay as (n̄/σ̄2)−α; since = (1 − ρ)̄ and σ̄2 = σ2/(1 − ρ), the Bayes error is then proportional to (1 − ρ)1−α. This multi-task gain again approaches unity for ρ < 1 for smooth functions (α = (2r + 1)/(2r + 2) → 1). Interestingly, for rough functions (α < 1), the multi-task gain decreases for small ρ2 as 1 − (1 − α) √ ρ2 and so always lies below a linear dependence on ρ2 initially. This shows that a linear-in-ρ2 lower error bound cannot generally apply to T > 2 tasks, and indeed one can verify that the derivation in [21] does not extend to this case."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have derived an approximate prediction (7) for learning curves in multi-task GP regression, valid for arbitrary inter-task correlation matrices D. This can be evaluated explicitly knowing only the kernel eigenvalues, without sampling or recourse to single-task learning curves. The approximation shows that pure transfer learning has a simple lower error bound, and provides a good qualitative account of numerically simulated learning curves. Because it can be used to study the asymptotic behaviour for large training sets, it allowed us to show that multi-task learning can become asymptotically useless: when learning smooth functions it reduces the asymptotic Bayes error only if tasks are fully correlated. For the limit of many tasks we found that, remarkably, some initial “collective learning” is possible even when most tasks have not seen examples. A much slower second learning stage then requires many examples per task. The asymptotic regime of this also showed explicitly that a lower error bound that is linear in ρ2, the square of the inter-task correlation, is applicable only to the two-task setting T = 2.\nIn future work it would be interesting to use our general result to investigate in more detail the consequences of specific choices for the inter-task correlationsD, e.g. to represent a lower-dimensional latent factor structure. One could also try to deploy similar approximation methods to study the case of model mismatch, where the inter-task correlations D would have to be learned from data. More challenging, but worthwhile, would be an extension to multi-task covariance functions where task and input-space correlations to not factorize."
    } ],
    "references" : [ {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C K I Williams", "C Rasmussen" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "J Baxter" ],
      "venue" : "J. Artif. Intell. Res.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2000
    }, {
      "title" : "A notion of task relatedness yielding provable multiple-task learning guarantees",
      "author" : [ "S Ben-David", "R S Borbely" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Semiparametric latent factor models",
      "author" : [ "Y W Teh", "M Seeger", "M I Jordan" ],
      "venue" : "In Workshop on Artificial Intelligence and Statistics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Kernel multi-task learning using task-specific features",
      "author" : [ "E V Bonilla", "F V Agakov", "C K I Williams" ],
      "venue" : "In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics (AISTATS). Omni Press,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Multi-task Gaussian process prediction",
      "author" : [ "E V Bonilla", "K M A Chai", "C K I Williams" ],
      "venue" : "editors, NIPS",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "Sparse convolved Gaussian processes for multi-output regression",
      "author" : [ "M Alvarez", "N D Lawrence" ],
      "venue" : "editors, NIPS",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Focused multi-task learning using Gaussian processes",
      "author" : [ "G Leen", "J Peltonen", "S Kaski" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Kernels for vector-valued functions: a review",
      "author" : [ "M A Álvarez", "L Rosasco", "N D Lawrence" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Bounds for linear multi-task learning",
      "author" : [ "A Maurer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "General bounds on Bayes errors for regression with Gaussian processes",
      "author" : [ "M Opper", "F Vivarelli" ],
      "venue" : "editors, NIPS",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1999
    }, {
      "title" : "Finite-dimensional approximation of Gaussian processes",
      "author" : [ "G F Trecate", "C K I Williams", "M Opper" ],
      "venue" : "editors, NIPS",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Learning curves for Gaussian processes",
      "author" : [ "P Sollich" ],
      "venue" : "NIPS 11,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Learning curves for Gaussian processes regression: A framework for good approximations",
      "author" : [ "D Malzahn", "M Opper" ],
      "venue" : "editors, NIPS",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2001
    }, {
      "title" : "A variational approach to learning curves",
      "author" : [ "D Malzahn", "M Opper" ],
      "venue" : "editors, NIPS",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "Statistical mechanics of learning: a variational approach for real data",
      "author" : [ "D Malzahn", "M Opper" ],
      "venue" : "Phys. Rev. Lett.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Learning curves for Gaussian process regression: approximations and bounds",
      "author" : [ "P Sollich", "A Halees" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2002
    }, {
      "title" : "Gaussian process regression with mismatched models",
      "author" : [ "P Sollich" ],
      "venue" : "editors, NIPS",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "Can Gaussian process regression be made robust against model mismatch? In Deterministic and Statistical Methods in Machine Learning, volume 3635 of Lecture Notes in Artificial Intelligence, pages 199–210",
      "author" : [ "P Sollich" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Exact larning curves for Gaussian process regression on large random graphs",
      "author" : [ "M Urry", "P Sollich" ],
      "venue" : "editors, NIPS",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Generalization errors and learning curves for regression with multi-task Gaussian processes",
      "author" : [ "K M A Chai" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Gaussian regression and optimal finite dimensional linear models",
      "author" : [ "H Zhu", "C K I Williams", "R J Rohwer", "M Morciniec" ],
      "venue" : "Neural Networks and Machine Learning. Springer,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    }, {
      "title" : "One-shot learning of object categories using dependent Gaussian processes",
      "author" : [ "E Rodner", "J Denzler" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Solving a huge number of similar tasks: a combination of multi-task learning and a hierarchical Bayesian approach",
      "author" : [ "T Heskes" ],
      "venue" : "In Proceedings of the Fifteenth International Conference on Machine Learning",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Gaussian processes (GPs) [1] have been popular in the NIPS community for a number of years now, as one of the key non-parametric Bayesian inference approaches.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "[2, 3]), GPs have increasingly also been used in a multi-task setting.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "[2, 3]), GPs have increasingly also been used in a multi-task setting.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "A number of different choices of covariance functions have been proposed [4, 5, 6, 7, 8].",
      "startOffset" : 73,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "A number of different choices of covariance functions have been proposed [4, 5, 6, 7, 8].",
      "startOffset" : 73,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "A number of different choices of covariance functions have been proposed [4, 5, 6, 7, 8].",
      "startOffset" : 73,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "A number of different choices of covariance functions have been proposed [4, 5, 6, 7, 8].",
      "startOffset" : 73,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "A number of different choices of covariance functions have been proposed [4, 5, 6, 7, 8].",
      "startOffset" : 73,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "in assumptions on whether the functions to be learned are related to a smaller number of latent functions or have free-form inter-task correlations; for a recent review see [9].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "PAC-style bounds for classification [2, 3, 10] in more general multi-task scenarios exist, but there has been little work on average case analysis.",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "PAC-style bounds for classification [2, 3, 10] in more general multi-task scenarios exist, but there has been little work on average case analysis.",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "PAC-style bounds for classification [2, 3, 10] in more general multi-task scenarios exist, but there has been little work on average case analysis.",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 11,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 12,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 13,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 14,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 15,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 16,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 18,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 19,
      "context" : "For a single regression task, this learning curve has become relatively well understood since the late 1990s, with a number of bounds and approximations available [11, 12, 13, 14, 15, 16, 17, 18, 19] as well as some exact predictions [20].",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 20,
      "context" : "Already two-task GP regression is much more difficult to analyse, and progress was made only very recently at NIPS 2009 [21], where upper and lower bounds for learning curves were derived.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "For this covariance we take the flexible form from [5], 〈fτ (x)fτ ′(x′)〉 = Dττ ′C(x, x′).",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "In this case the mean-squared prediction error \u000F̂τ is the Bayes error, and is given by the average posterior variance [1], i.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "We note in passing that, because ∑ τ Pτ = I , the sum of the Bayes errors on all tasks is ∑ τ \u000F̂τ = tr (L −1+ΨTΣ−1Ψ)−1, in close analogy to the corresponding expression for the single-task case [13].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "To help with this, we can extend an approach for the single-task scenario [13] and define a response or resolvent matrix G = (L−1 + ΨTΣ−1Ψ + ∑ τ vτPτ ) −1 with auxiliary parameters vτ that will be set back to zero at the end.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : ", vτ (t) = vτ,0(1 − t), nτ (t) = vτ,0(σ 2 τ + τ,0) t for t ∈ [0, 1] are tangential to the solution surface for all t, so lie within this surface if the initial point at t = 0 does.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Here one recovers T separate equations for the individual tasks as expected, which have the same form as for single-task learning [13].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "Here we have the work of Chai [21] as a baseline, and as there we choose",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "As regards the covariance function and input distribution, we analyse first the scenario studied in [21]: a squared exponential (SE) kernel C(x, x′) = exp[−(x − x′)2/(2l2)] with lengthscale l, and one-dimensional inputs x with a Gaussian distributionN (0, 1/12).",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "are known explicitly from [22] and decay exponentially with i.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "Figure 1(middle) has analogous results for the case of inputs x uniformly distributed on the interval [0, 1]; the λi here decay exponentially with i [17].",
      "startOffset" : 102,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "Figure 1(middle) has analogous results for the case of inputs x uniformly distributed on the interval [0, 1]; the λi here decay exponentially with i [17].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "The discussion in [17] suggests that this is because the approximation method we have used implicitly neglects spatial variation of the dataset-averaged posterior variance 〈Vτ (x)〉; but for a uniform input distribution this variation will be weak except near the ends of the input range [0, 1].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "The discussion in [17] suggests that this is because the approximation method we have used implicitly neglects spatial variation of the dataset-averaged posterior variance 〈Vτ (x)〉; but for a uniform input distribution this variation will be weak except near the ends of the input range [0, 1].",
      "startOffset" : 287,
      "endOffset" : 293
    }, {
      "referenceID" : 16,
      "context" : "The kernel eigenvalues λi then decay as1 i−(2r+2) for large i, and using arguments from [17] one deduces that g(h) ∼ h−α for large h, with α = (2r+1)/(2r+ 2).",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "It is worth contrasting this with the lower bound from [21], which is linear in ρ.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "While this provides a very good approximation to the learning curves for moderate n [21], our results here show that asymptotically this bound can become very loose.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "[1]; we consider one-dimensional inputs here though the discussion can be generalized.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 22,
      "context" : "in [23], and can be interpreted as each task having a component proportional to √ ρ of a shared latent function, with an independent task-specific signal in addition.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 23,
      "context" : "2(right) validates these predictions against simulations, for a number of tasks (T = 200) that is in the same ballpark as in the many-tasks application example of [24].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 20,
      "context" : "This shows that a linear-in-ρ lower error bound cannot generally apply to T > 2 tasks, and indeed one can verify that the derivation in [21] does not extend to this case.",
      "startOffset" : 136,
      "endOffset" : 140
    } ],
    "year" : 2012,
    "abstractText" : "We study the average case performance of multi-task Gaussian process (GP) regression as captured in the learning curve, i.e. the average Bayes error for a chosen task versus the total number of examples n for all tasks. For GP covariances that are the product of an input-dependent covariance function and a free-form intertask covariance matrix, we show that accurate approximations for the learning curve can be obtained for an arbitrary number of tasks T . We use these to study the asymptotic learning behaviour for large n. Surprisingly, multi-task learning can be asymptotically essentially useless, in the sense that examples from other tasks help only when the degree of inter-task correlation, ρ, is near its maximal value ρ = 1. This effect is most extreme for learning of smooth target functions as described by e.g. squared exponential kernels. We also demonstrate that when learning many tasks, the learning curves separate into an initial phase, where the Bayes error on each task is reduced down to a plateau value by “collective learning” even though most tasks have not seen examples, and a final decay that occurs once the number of examples is proportional to the number of tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}