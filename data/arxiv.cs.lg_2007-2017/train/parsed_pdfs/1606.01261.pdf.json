{
  "name" : "1606.01261.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Minimizing Regret on Reflexive Banach Spaces and Learning Nash Equilibria in Continuous Zero-Sum Games",
    "authors" : [ "Maximilian Balandat", "Walid Krichene", "Claire Tomlin", "Alexandre Bayen" ],
    "emails" : [ "BALANDAT@EECS.BERKELEY.EDU", "WALID@EECS.BERKELEY.EDU", "TOMLIN@EECS.BERKELEY.EDU", "BAYEN@BERKELEY.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Regret analysis is a general technique for designing and analyzing algorithms for sequential decision problems in adversarial or stochastic settings (Audibert and Bubeck, 2009; Bubeck and CesaBianchi, 2012). Online learning algorithms have many applications, including in machine learning (Xiao, 2010), portfolio optimization (Cover, 1991), and online convex optimization (Hazan et al., 2007). A particularly interesting role that regret plays manifests in the study of repeated play of finite games (Hart and Mas-Colell, 2001). It is well known, for example, that in a two-player zero-sum finite game, if both players play according to a Hannan-consistent strategy (Hannan, 1957), their (marginal) empirical distributions of play almost surely converge to the set of Nash equilibria of the game (Cesa-Bianchi and Lugosi, 2006). Moreover, it can be shown that playing a strategy that achieves sublinear regret almost surely guarantees Hannan-consistency.\nar X\niv :1\n60 6.\n01 26\n1v 1\n[ cs\n.L G\n] 3\nJ un\nA natural question to ask is whether a similar result holds for games in which the action sets are infinite. In this paper we show that this is in fact true. In particular, we prove that in a continuous two-player zero sum game over compact (not necessarily convex) metric spaces, if both players follow a Hannan-consistent strategy, then with probability 1, their empirical distributions of play weakly converge to the set of Nash equilibria of the game. This in turn raises another important question: Do algorithms that ensure Hannan-consistency exist for such games? More generally, can one develop algorithms that guarantee sub-linear growth of the worst-case regret? We answer these questions affirmatively as well in this article. To this end, we develop a general framework to study the Dual Averaging (or Follow the Regularized Leader) method on reflexive Banach spaces. This framework subsumes a wide range of existing results in the literature, including algorithms for online learning on finite sets (e.g. the Hedge Algorithm (Arora et al., 2012)), and finite-dimensional online convex optimization (e.g. Exponentially Weighted Online Optimization (Hazan et al., 2007)). Our results are related to (Lehrer, 2003) and (Sridharan and Tewari, 2010). Lehrer (2003) conducts an abstract analysis, providing necessary geometric conditions for Blackwell approachability in infinite-dimensional spaces, but no implementable algorithm that guarantees Hannan-consistency. Sridharan and Tewari (2010) provide general regret bounds for Mirror Descent (MD) under the assumption that the strategy set is uniformly bounded in the norm of the Banach space. We make no such assumption. In fact, for our applications in Section 3 this is typically not the case.\nGiven a convex subset X of a reflexive Banach space X , the generalized Dual Averaging (DA) method maximizes, at each iteration, the cumulative past rewards (which are elements of X∗, the dual space ofX) plus a regularization term h. We show that under certain conditions, the maximizer in the DA update is the Fréchet gradientDh∗ of the regularizer’s conjugate function. In doing so, we develop a novel characterization of the duality between essential strong convexity of h and essential Fréchet differentiability of h∗ in reflexive Banach spaces, which may be of independent interest.\nWe apply these general results to the problem of minimizing regret when the rewards are uniformly continuous functions over a compact metric space S. Importantly, we do not assume convexity of either S or the reward functions. Under the assumption that S admits a locally Q-regular Borel measure µ, we lift this non-convex finite dimensional problem on S, to the convex infinitedimensional problem on the set of probability distributions on S that are absolutely continuous with respect to µ, and whose Radon-Nikdoym derivatives are in X = Lp(S, µ) for some p > 1. We provide explicit bounds for a class of regularizers, which guarantee sublinear growth of the worst-case regret. We also prove a general lower bound on the regret for any online algorithm.\nThe paper is organized as follows: In Section 2 we introduce and provide a general analysis of Dual Averaging in reflexive Banach spaces. In Section 3 we apply these results to obtain explicit regret bounds on compact metric spaces with uniformly continuous reward functions. We use these results in Section 4 in the context of learning Nash equilibria in continuous two-player zero sum games, for which we provide numerical examples in Section 5. All proofs are given in Appendix E."
    }, {
      "heading" : "2. Regret Minimization on Reflexive Banach Spaces",
      "text" : "Consider a sequential decision problem in which we are to choose a sequence (x1, x2, . . . ) of actions from some feasible subset X of a reflexive Banach space X , and seek to maximize a sequence (u1(x1), u2(x2), . . . ) of rewards, where the uτ : X → R are elements of a given subset U ⊂ X∗, with X∗ the dual space of X . We assume that xt, the action chosen at time t, may only depend on the sequence of previously observed reward vectors (u1, . . . , ut−1). We call any such algorithm\nan online algorithm. We consider the adversarial setting, i.e., we do not make any distributional assumptions on the rewards. In particular, they could be picked maliciously by some adversary.\nThe notion of regret is a standard measure of performance for such a sequential decision problem. For a sequence (u1, . . . , ut) of reward vectors, and a sequence of decisions (x1, . . . , xt) produced by an algorithm, the regret of the algorithm with respect to a (fixed) decision x ∈ X is the gap between the realized reward and the reward under x. In other words,\nRt(x) := t∑\nτ=1\nuτ (x)− t∑\nτ=1\nuτ (xτ ) (1)\nThe worst-case regret is defined as\nRt := sup x∈X Rt(x) (2)\nAn algorithm is said to have sublinear regret if for any sequence (ut)t≥1 in the set of admissible reward functions U , the worst-case regret grows sublinearly, i.e. lim suptRt/t ≤ 0.\nExample 1 (Finite Action Sets) Consider a finite action set S = {1, . . . , n}, let X = X∗ = Rn, and let X = ∆n−1, the probability simplex in Rn. In this case, a reward function on S is simply a vector u ∈ Rn, such that the i-th element ui is the reward of action i. A choice x ∈ X corresponds to a randomization over the n discrete actions in S. This is the classic setting of many regretminimizing algorithms in the literature.\nExample 2 (Online Optimization on Compact Metric Spaces) Let S be a compact metric space, and let µ be a finite measure on S. Consider the Hilbert Space X = X∗ = L2(S, µ) and let X = {x ∈ X : x ≥ 0 a.e., ‖x‖1 = 1}. The set S is again the set of feasible actions. A reward function is an L2-integrable function on S, and each choice x ∈ X corresponds to a probability distribution (absolutely continuous w.r.t. to µ) over the set of actions. We will further explore a more general variant of this problem in Section 3.\nIn this Section, we prove a general bound on the worst-case regret for the Dual Averaging method. Dual Averaging was introduced by Nesterov (2009) for (finite dimensional) convex optimization, and was since applied to the online learning setting, for example by Xiao (2010). In the finite dimensional case, the method works by solving, at each iteration, the maximization problem\nxt+1 = arg max x∈X\n〈 ηt ∑t τ=1 uτ , x 〉 − h(x)\nwhere h is a strongly convex regularizer defined on X ⊂ Rn and (ηt)t≥0 is a sequence of learning rates. The regret analysis of the method relies on the duality between strong convexity and smoothness for a convex function and its conjugate (Nesterov, 2009, Lemma 1), see also (Rockafellar, 1997, Theorem 26.3). To generalize the Dual Averaging method to our Banach space setting, we first need an analogous duality result. We develop such a result in Theorem 4. In particular, we show that the correct notion of strong convexity in our setting is (uniform) essential strong convexity. Equipped with this duality result, we can analyze the regret of the Dual Averaging method and derive a general bound in Theorem 6."
    }, {
      "heading" : "2.1. Preliminaries",
      "text" : "Let (X, ‖ · ‖) be a reflexive Banach space, and denote by 〈 · , · 〉 : X × X∗ → R the canonical pairing between X and its dual space X∗, so that 〈x, ξ〉 := ξ(x) for all x ∈ X, ξ ∈ X∗. By the effective domain of an extended real-valued function f : X → [−∞,+∞] we mean the set dom f = {x ∈ X : f(x) < +∞}. A function f is proper if f > −∞ and dom f is non-empty. The conjugate or Legendre-Fenchel transform of f is the function f∗ : X∗ → [−∞,+∞] given by\nf∗(ξ) = sup x∈X 〈x, ξ〉 − f(x) (3)\nfor all ξ ∈ X∗. If f is proper, lower semicontinuous and convex, its subdifferential ∂f is the set-valued mapping ∂f(x) = { ξ ∈ X∗ : f(y) ≥ f(x) + 〈y − x, ξ〉 for all y ∈ X } . We define dom ∂f := {x ∈ X : ∂f(x) 6= ∅}. Let Γ denote the set of all convex, lower semicontinuous functions γ : [0,∞)→ [0,∞] such that γ(0) = 0, and let\nΓU := { γ ∈ Γ : γ(r) > 0, for r > 0 } (4a)\nΓL := { γ ∈ Γ : γ(r)/r → 0, as r → 0 } (4b)\nWe now introduce the appropriate definitions of strong convexity, differentiability and smoothness for our setting. Some related results are reviewed in Appendix A.\nDefinition 1 (Essential strong convexity (Strömberg, 2011)) A proper convex lower semicontinuous function f : X → (−∞,∞] is essentially strongly convex if\n(i) f is strictly convex on every convex subset of dom ∂f\n(ii) (∂f)−1 is locally bounded on its domain\n(iii) for every x0 ∈ dom ∂f there exists ξ0 ∈ X∗ and γ ∈ ΓU such that\nf(x) ≥ f(x0) + 〈x− x0, ξ0〉+ γ(‖x− x0‖), ∀x ∈ X (5)\nIf (5) holds with γ independent of x0, then f is said to be uniformly essentially strongly convex with modulus γ.\nDefinition 2 (Essential Fréchet differentiability (Strömberg, 2011)) A proper convex lower semicontinuous function f : X → (−∞,∞] is essentially Fréchet differentiable if int dom f 6= ∅, f is Fréchet differentiable on int dom f with Fréchet derivative D, and ‖Df(xj)‖∗ → ∞ for any sequence (xj)j in int dom f converging to some boundary point of dom f .\nDefinition 3 (Essential strong smoothness) A proper Fréchet differentiable function f : X → (−∞,∞] is essentially strongly smooth if ∀x0 ∈ dom ∂f, ∃ ξ0 ∈ X∗, κ ∈ ΓL such that\nf(x) ≤ f(x0) + 〈ξ0, x− x0〉+ κ(‖x− x0‖), ∀x ∈ X (6)\nIf (6) holds with κ independent of x0, then f is said to be uniformly essentially strongly smooth with modulus κ.\nWe are now ready to give our main duality result:\nTheorem 4 Let f : X → (−∞,+∞] be proper, lower semicontinuous and uniformly essentially strongly convex with modulus γ ∈ ΓU . Then\n(i) f∗ is proper and essentially Fréchet differentiable with Fréchet derivative\nDf∗(ξ) = arg max x∈X 〈x, ξ〉 − f(x) (7)\nIf, in addition, γ̃(r) := γ(r)/r is strictly increasing, then ‖Df∗(ξ1)−Df∗(ξ2)‖ ≤ γ̃−1 ( ‖ξ1 − ξ2‖∗/2 ) (8)\nIn other words, Df∗ is uniformly continuous with modulus of continuity χ(r) = γ̃−1(r/2).\n(ii) f∗ is uniformly essentially smooth with modulus γ∗.\nCorollary 5 If γ(r) ≥ C r1+κ, ∀ r ≥ 0 then ‖Df∗(ξ1) − Df∗(ξ2)‖ ≤ (2C)−1/κ‖ξ1 − ξ2‖1/κ∗ . In particular, with γ(r) = K2 r\n2, Definition 1 becomes the classic definition of K-strong convexity, and (8) yields the result familiar from the finite-dimensional case that the gradient Df∗ is 1/K Lipschitz with respect to the dual norm (Nesterov, 2009, Lemma 1)."
    }, {
      "heading" : "2.2. Dual Averaging in Reflexive Banach Spaces",
      "text" : "We call a proper convex function h : X → (−∞,+∞] a regularizer function on a set X ⊂ X if h is essentially strongly convex and domh = X . We emphasize that we do not assume h to be Fréchet-differentiable. Definition 1 in conjunction with Lemma 29 implies that for any regularizer function h, the supremum of any function of the form 〈 · , ξ〉 − h( · ) over X , where ξ ∈ X∗, will be attained at a unique element of X , namely Dh∗(ξ), the Fréchet gradient of h∗ at ξ.\nThe Dual Averaging method with regularizer h and a sequence of learning rates (ηt)t≥1 generates a sequence of decisions using the simple update rule:\nxt+1 = Dh ∗(ηtUt) where Ut = ∑t τ=1 uτ and U0 := 0. The following theorem provides a general regret bound.\nTheorem 6 (Dual Averaging Regret) Let h be a uniformly essentially strongly convex regularizer on X with modulus γ. Let (ηt)t≥1 be a positive non-increasing sequence of learning rates. Then, for any sequence of payoff functions (ut)t≥1 in X∗, the sequence of plays (xt)t≥0 given by\nxt+1 = Dh ∗(ηt∑tτ=1 uτ) (9)\nensures that\nRt(x) := t∑\nτ=1\n〈uτ , x〉 − t∑\nτ=1\n〈uτ , xτ 〉 ≤ h(x)− h\nηt + t∑ τ=1 ‖uτ‖∗ γ̃−1 (ητ−1 2 ‖uτ‖∗ ) (10)\nwhere h = infx∈X h(x), γ̃(r) := γ(r)/r and η0 := η1.\nNote that it is possible to obtain a regret bound similar to (10) also in a continuous-time setting. In fact, following Kwon and Mertikopoulos (2014), we derive the bound (10) by first proving a bound on a suitably defined notion of continuous-time regret, and then bounding the difference between the continuous-time and discrete-time regrets. This analysis is detailed in Appendix B.\nTheorem 6 provides a bound on the regret Rt(x) with respect to a particular choice x ∈ X . Recall that the worst-case regret is defined asRt := supx∈X Rt(x). In the finite-dimensional seting of Example 1 the set X is compact, so any continuous regularizer h will be bounded, and hence taking the supremum over x in (10) poses no issue. However, this is not the case in our general setting, as the regularizer may be unbounded on X . For instance, consider Example 2 with the entropy regularizer h(x) = ∫ S x(s) log(x(s))ds, which is easily seen to be unbounded on X . As a consequence, obtaining a worst-case bound will in general require additional assumptions on the reward functions and the decision set X . This will be investigated in detail in Section 3.\nCorollary 7 Suppose that γ(r) ≥ C r1+κ, ∀ r ≥ 0 for some C > 0 and κ > 0. Then\nRt(x) ≤ h(x)− h\nηt + (2C)−1/κ t∑ τ=1 η 1/κ τ−1‖uτ‖ 1+1/κ ∗ (11)\nIn particular, if ‖ut‖∗ ≤M for all t and ηt = η t−β , then\nRt(x) ≤ h(x)− h\nη tβ +\nκ\nκ− β ( η 2C )1/κ M1+1/κ t1−β/κ (12)\nAssuming h is bounded, optimizing over β yields a rate of Rt(x) = O(t κ\n1+κ ). In particular, if γ(r) = K2 r 2, which corresponds to the classic definition of strong convexity, then Rt(x) = O( √ t). For non-vanishing uτ we will need that ηt ↘ 0 for the sum in (11) to converge. Thus we could get potentially tighter control over the rate of this term for κ < 1, at the expense of larger constants."
    }, {
      "heading" : "3. Online Optimization on Compact Metric Spaces with Uniformly Continuous Rewards",
      "text" : "Motivated by Example 2, in this section we apply our results to the problem of regret minimization on compact metric spaces under the additional assumption of uniformly continuous reward functions. Importantly, we make no assumptions on convexity of either the feasible set or the reward functions. Essentially, this can be seen as lifting the non-convex problem of minimizing a sequence of functions over the (possibly non-convex) set S to the convex (albeit infinite-dimensional) problem of minimizing a sequence of linear functionals over the convex subset X of probability measures over the vector space of measures on S. This correspondance is illustrated in Figure 1."
    }, {
      "heading" : "3.1. An Upper Bound on the Worst-Case Regret",
      "text" : "Let (S, d) be a compact metric space, and let µ be a Borel measure on S. Suppose that the reward vectors uτ are given by elements in Lq(S, µ), where q > 1. Let X = Lp(S, µ), where p and q are Hölder conjugates, i.e., 1p + 1 q = 1. Consider X = {x ∈ X : x ≥ 0 a.e., ‖x‖1 = 1}, the set of probability measures on S that are absolutely continuous w.r.t. to µ with p-integrable RadonNikodym derivatives. Moreover, denote by Z the class of non-decreasing χ : [0,∞)→ [0,∞] such that limr→0 χ(r) = χ(0) = 0. The following assumption will be made throughout this section:\nAssumption 1 The reward vectors ut have modulus of continuity χ on S, uniformly in t. That is, there exists χ ∈ Z such that |ut(s)− ut(s′)| ≤ χ(d(s, s′)) for all t and for all s, s′ ∈ S.\nDenote by B(s, r) = {s′ ∈ S : d(s, s′) < r} the open ball of radius r centered at s and by B(s, δ) ⊂ X the set of elements of X with support contained in B(s, δ). Furthermore, let DS := sups,s′∈S d(s, s ′) the diameter of S. Then we have the following:\nTheorem 8 (Dual Averaging Regret on Metric Spaces with Uniformly Continuous Rewards) Let (S, d) be compact, and suppose that Assumption 1 holds. Let h be a uniformly essentially strongly convex regularizer on X with modulus γ, and let (ηt)t≥1 be a positive non-increasing sequence of learning rates. Then, under (9), for any positive sequence (ϑt)t≥1,\nRt ≤ sups∈S infx∈B(s,ϑt) h(x)− h\nηt + t χ(ϑt) + t∑ τ=1 ‖uτ‖∗ γ̃−1 (ητ−1 2 ‖uτ‖∗ ) (13)\nRemark 9 The sequence (ϑt)t≥1 that appears in Theorem 8 is not a parameter of the Dual Averaging algorithm, but rather a parameter in the regret bound. In particular, (13) holds true for any such positive sequence, and we will use this fact later on to obtain explicit bounds by instantiating (13) with a particular choice of (ϑt)t≥1.\nIt is important to realize that the infimum over B(s, ϑt) in (13) may be infinite, in which case the bound is meaningless. This happens for example if s is an isolated point of a compact subset S ⊂ Rn and µ is the Lebesgue measure, in which case B(s, ϑt) = ∅. However, under an additional regularity assumption on the measure µ we can avoid such degenerate situations.\nDefinition 10 (Q-regularity (Heinonen. et al., 2015)) A Borel measure µ on a metric space (S, d) is (Ahlfors) Q-regular if there exist 0 < c0 ≤ C0 <∞ such that for any open ball B(s, r)\nc0r Q ≤ µ(B(s, r)) ≤ C0rQ (14)\nWe say that µ is r0-locally Q-regular if (14) holds for all 0 < r ≤ r0.\nIntuitively, under an r0-locally Q-regular measure, the mass in the neighborhood of any point of S is uniformly bounded from above and below. This will allow, at each iteration t, to assign sufficient probability mass around the maximizer(s) of the cumulative reward function.\nExample 3 (Regularity of the Lebesgue measure λ) The canonical example for aQ-regular measure is the Lebesgue measure λ on Rn. If d is the metric induced by the standard Euclidean norm,\nthenQ = n and the bound (14) is tight with c0 = C0, a dimensional constant. However, for general sets S ⊂ Rn, λ need not be locally Q-regular. This is for example the case if S includes isolated points. One sufficient condition for local regularity of the Lebesgue measure is that S is v-uniformly fat, as defined by Krichene et al. (2015), i.e., that for all s ∈ S, there exists a convex set K ⊆ S that contains s with λ(K) ≥ v. In this case one can to prove that λ|S is r0-locally n-regular, with r0, c0 and C0 depending on the geometry of S. Importantly, S need not be convex or even connected.\nAssumption 2 The measure µ is r0-locally Q-regular on (S, d).\nRecall that he worst-case regretRt is defined as the supremum of Rt(x) over all elements of X . In the setting of this section, we can in fact say more:\nProposition 11 Suppose Assumption 2 holds. Then\nRt = sup x∈X Rt(x) = sup x∈P Rt(x) = sup s∈S\nUt(s)− ∑t τ=1〈uτ , xτ 〉 (15)\nUnder Assumption 2, B(s, ϑt) 6= ∅ for all s ∈ S and ϑt > 0, and hence there is hope for a bound on infx∈B(s,ϑt) h(x) that is uniform in s. To get explicit rates of convergence, we have to consider a more specific class of regularizers.\n3.2. Explicit Rates for f -Divergences on Lp(S)\nIn this section we consider a particular class of regularizers called f -divergences or Csiszár divergences (Csiszár, 1967) and provide explicit bounds on the worst-case regret. Following Audibert et al. (2014), we define ω-potentials and the associated f -divergence.\nDefinition 12 (ω-Potential) Let ω ≤ 0 and a ∈ (−∞,+∞]. A continuous increasing diffeomorphism φ : (−∞, a) → (ω,∞), is an ω-potential if limz→−∞ φ(z) = ω, limz→a φ(z) = +∞ and φ(0) ≤ 1. Associated to φ is the convex function fφ : [0,∞)→ R defined by\nfφ(x) = ∫ x 1 φ−1(z) dz,\nand the fφ-divergence, defined by\nhφ(x) = ∫ S fφ ( x(s) ) dµ(s) + ιX (x).\nwhere ιX is the indicator function of X (i.e. ιX (x) = 0 if x ∈ X and ιX (x) = +∞ if x /∈ X ).\nA remarkable fact is that for regularizers based on ω potentials, the Dual Averaging update (9) can be computed efficiently. More precisely, it can be shown (Krichene and Balandat, 2016) that the maximizer in this case has a simple expression in terms of the dual problem, and the problem of computing xt+1 = Dh∗(ηt ∑t τ=1 uτ ) reduces to computing a scalar dual variable ν ∗ t . In Proposition 34 in Appendix C we provide a bound on ν∗t+1 that depends on the value of ν ∗ t and other parameters of the problem. In practice, these bounds greatly speed up computation. Note that the measure µ plays the role of a design variable, and its choice will affect our bounds through the constantsQ,C0 and c0. The problem of finding a “good” measure µ is a very interesting problem for future studies. For now we will assume that µ(S) = 1, which due to compactness of S is without loss of generality if we want arbitrarily small balls to have finite measure.\nProposition 13 Suppose that µ(S) = 1, and that Assumption 2 holds with constants r0 > 0 and 0 < c0 ≤ C0 < ∞. Under the Assumptions of Theorem 8, with h = hφ the regularizer associated to an ω-potential φ, we have that, for any positive sequence (ϑt)t≥1 with ϑt ≤ r0,\nRt t ≤ min(C0ϑ Q t , µ(S)) t ηt fφ ( c−10 ϑ −Q t ) + χ(ϑt) + 1 t t∑ τ=1 ‖uτ‖∗ γ̃−1 (ητ−1 2 ‖uτ‖∗ ) (16)\nFor particular choices of the learning rates (ηt)t≥1 and the sequence (ϑt)t≥1, we can derive explicit regret rates. To intuit, suppose for simplicity that the reward functions are uniformly bounded in the dual norm. Then it is clear that in order for the last term in (16) to vanish asymptotically, ηt must be vanishing. Similarly, for the second term to vanish, ϑt must be vanishing as well. Thus, if both (ηt)t≥1 and (ϑt)t≥1 are decreasing sequences, their respective rates of decay must be carefully chosen so that the first term also vanishes. These tradeoffs will become clear in the statement of Corollary 14 and in the numerical examples presented in Appendix D."
    }, {
      "heading" : "3.3. Analysis for Entropy Dual Averaging (The Generalized Hedge Algorithm)",
      "text" : "Taking φ(z) = ez−1, we have that fφ(x) = ∫ x 1 φ −1(z)dz = x log x, and hence the regularizer is\nhφ(x) = ∫ S x(s) log x(s)dµ(s). Then Dh\n∗(ξ)(s) = exp ξ(s)‖ exp ξ(s)‖1 . This corresponds to a generalized version of the Hedge algorithm (Arora et al., 2012; Krichene et al., 2015). The regularizer hφ can be shown to be essentially strongly convex with modulus γ(r) = 12r 2.\nCorollary 14 (Regret Bound for Entropy Dual Averaging) Suppose that µ(S) = 1, that µ is r0-locally Q-regular with constants c0, C0, that ‖ut‖∗ ≤ M for all t, and that χ(r) = Cαrα for 0 < α ≤ 1 (that is, the rewards are α-Hölder continuous). Then, under Entropy Dual Averaging, choosing ηt = η √ log t/t with η = 1M (C0Q 2c0 log(c−10 ϑ −Q/α) + Q2α\n)1/2 and ϑ > 0, we have that Rt t ≤ ( 2M √ 2C0 c0 ( log(c−10 ϑ −Q/α) + Q 2α ) + Cαϑ )√ log t t (17)\nwhenever √\nlog t/t < rα0 ϑ −1.\nOne can further optimize over the choice of ϑ to obtain the best constant in the bound. Note also that the case α = 1 corresponds to Lipschitz continuity."
    }, {
      "heading" : "3.4. A General Lower Bound",
      "text" : "We also prove the following general lower bound for any online algorithm:\nTheorem 15 (General Lower Bound) Let (S, d) be compact, suppose that Assumption 2 holds, and let χ ∈ Z . Then for any online algorithm, there exist a sequence (uτ )tτ=1 of reward vectors uτ ∈ X∗ with ‖uτ‖∗ ≤M and modulus of continuity χτ < χ such that\nRt ≥ w(DS)\n2 √ 2\n√ t (18)\nwhere w : R → R is any function with modulus of continuity χ such that ‖w(d( · , s′))‖q ≤ M for some s′ ∈ S for which there exists s ∈ S with d(s, s′) = DS .\nMaximizing the constant in (18) is of interest in order to benchmark the bound against the upper bounds obtained in the previous sections. This problem is however quite challenging, and we will defer this analysis to future work. For Hölder-continuous functions, we have the following result:\nProposition 16 (General Lower Bound for Hölder-Continuous Functions) In the setting of Theorem 15, suppose that µ(S) = 1 and that χ(r) = Cαrα for some 0 < α ≤ 1. Then\nRt ≥ min\n( C\n1/α α DαS , M ) 2 √ 2 √ t (19)\nObserve that, up to a √\nlog t factor, the asymptotic rate of this general lower bound for any online algorithm matches that of the upper bound (17) of Entropy Dual Averaging."
    }, {
      "heading" : "3.5. Consistency of Dual Averaging",
      "text" : "It is quite intuitive to see that Dual Averaging would recover the greedy algorithm as the regularizer h “approaches a constant”. In the following, we make this intuition precise.\nDefinition 17 (Consistency of a Sequence of Regularizers) A sequence (h1, h2, . . . ) of regularizers on X is consistent if there exists C ∈ R such that hi(x)→ C as i→∞ for all x ∈ X .\nFor s ∈ S, A ⊂ S, let d(s,A) = infs′∈A d(s, s′). For δ > 0, let B∗δ := {s ∈ S : d(s, S∗) < δ}. Moreover, let ν|A denote the restriction of ν ∈ P(S) to A.\nProposition 18 Suppose Assumption 2 holds and that (hi)i≥1 is a sequence of regularizers that is consistent. Fix t and let U∗ := maxs∈S Ut(s) and S∗ := {s ∈ S : Ut(s) = U∗}. For i ≥ 1 let x∗t,i := Dh ∗ i (Ut) Then, for any δ > 0, we have that x ∗ t,i|(B∗δ )c → 0|(B∗δ )c (strongly) as i → ∞.\nEquivalently, ∫ S∗ x ∗ i (s) ds→ 1 as i→∞.\nProposition 18 shows that if the sequence of regularizers is consistent, the optimizers, in the limit, collapse to distributions supported on the set of maximizers of Ut (as illustrated numerically in Example 5 in Appendix D). If the maximizer of Ut is unique, we can say the following:\nCorollary 19 In the setting of Proposition 18, suppose that Ut admits a unique maximizer s∗t ∈ S. Then x∗i weakly converges to the Dirac measure on s ∗ t as i→∞. We write x∗t,i ⇀ δs∗t ."
    }, {
      "heading" : "4. Learning in Continuous Two-Player Zero-Sum Games",
      "text" : "In this section we apply our esults on Dual Averaging on Lp-spaces in the context of repeated play of continuous games. In particular, we focus on continuous two-player zero-sum games. In the finite case similar results exist for non-zero sum games, and we believe that they can be extended to our setting, however this is outside the scope of this article (see for example (Stoltz and Lugosi, 2007) for related work on learning correlated equilibria under additional convexity assumptions)."
    }, {
      "heading" : "4.1. Static Two-Player Zero-Sum Games",
      "text" : "Consider a two-player zero sum game G = (S1, S2, u), in which the strategy spaces S1 and S2 of player 1 and 2, respectively, are Hausdorff spaces, and u : S1 × S2 → R is the payoff function of player 1. As the game is zero-sum, the payoff function of player 2 is −u. For each i, denote by Pi := P(Si) the set of Borel probability measures on Si. Denote S := S1×S2 and P := P1×P2. For a (joint) mixed strategy x ∈ P , we define the natural extension ū : P → R by ū(x) := Ex[u] =∫ S u(s\n1, s2) dx(s1, s2), which is the expected payoff of player 1 under x. A continuous zero-sum game G is said to have value V if\nsup x1∈P1 inf x2∈P2 ū(x1, x2) = inf x2∈P2 sup x1∈P1 ū(x1, x2) = V (20)\nThe elements x1×x2 ∈ P at which (20) holds are the (mixed) Nash Equilibria of G. We denote the set of Nash equilibria of G by N (G). In the case of finite games, it is well known that every twoplayer zero-sum game has a value. This is not true in general for continuous games, and additional conditions on strategy sets and payoffs are required. A classic result is the following:\nTheorem 20 (Glicksberg, 1950) Let S1 and S2 be compact, and suppose that u : S1×S2 → R is semi-continuous (upper or lower). Then G has a value."
    }, {
      "heading" : "4.2. Repeated Play",
      "text" : "We consider repeated play of the continuous two-player zero-sum game. Given a game G and a sequence of plays (s1t )t≥1 and (s 2 t )t≥1, we say that player i has sublinear (realized) regret if\nlim sup t→∞\n1\nt ( sup si∈Si t∑ τ=1 ui(s i, s−iτ )− t∑ τ=1 ui(s i τ , s −i τ ) ) = 0 (21)\nwhere we use −i to denote the other player (e.g. −1 = 2). A strategy σi for player i is, loosely speaking, a (possibly random) mapping from past observations to its actions. Of primary interest to us are Hannan-consistent strategies (Hannan, 1957):\nDefinition 21 (Hannan Consistency) A strategy σi of player i is Hannan consistent if, for any sequence (st−i)t≥1, the sequence of plays (s t i)t≥1 generated by σ i has sublinear regret almost surely.\nNote that the almost sure statement in Definition 21 is with respect to the randomness in the strategy σi. The following results are generalizations of their counterparts for discrete games:\nProposition 22 Suppose G has value V and consider a sequence of plays (s1t )t≥1, (s2t )t≥1 and suppose that player 1 has sublinear realized regret. Then\nlim inf t→∞\n1\nt t∑ τ=1 u(s1τ , s 2 τ ) ≥ V (22)\nCorollary 23 Suppose G has value V and consider a sequence of plays (s1t )t≥1, (s2t )t≥1 and assume that both players have sublinear realized regret. Then\nlim t→∞\n1\nt t∑ τ=1 u(s1τ , s 2 τ ) = V (23)\nAs in the discrete case (Cesa-Bianchi and Lugosi, 2006), we can also say something about convergence of the empirical distributions of play to the set of Nash Equilibria. Since these distributions have finite support for every t, we can at best hope for convergence in the weak sense as follows:\nTheorem 24 (Weak Convergence of the Empirical Distributions of Play) Suppose that in a repeated two-player zero sum game G that has a value both players follow a Hannan-consistent strategy, and denote by x̂it = 1 t ∑t τ=1 δsiτ the marginal empirical distribution of play of player i at iteration t. Let x̂t := (x̂1t , x̂ 2 t ). Then x̂t ⇀ N (G) almost surely, that is, with probability 1 the sequence (x̂t)t≥1 weakly converges to the set of Nash equilibria of G.\nCorollary 25 If G has a unique Nash equilibrium x∗, then with probability 1, x̂t ⇀ x∗."
    }, {
      "heading" : "4.3. Hannan-Consistent Strategies",
      "text" : "By Theorem 24, if each player follows a Hannan-consistent strategy, then the empirical distributions of play weakly converge to the set of Nash equilibria of the game. But do such strategies exist? Regret minimizing strategies are intuitive candidates, and the intimate connection between regret minimization and learning in games is well studied for special cases such as for finite games (CesaBianchi and Lugosi, 2006) or potential games (Monderer and Shapley, 1996). Using our results from Section 3, we will show that, under an additional assumption on the underlying information structure, no-regret learning based on Dual Averaging leads to Hannan consistency in our setting.\nSuppose that after each iteration t, each player i observes a partial payoff function ũit : Si → R describing their payoff as a function of only their own action, si, holding the action played by the other player fixed. That is,\nũ1t (s 1) := u(s1, s2t ) ũ 2 t (s 2) := −u(s1t , s2) (24)\nRemark 26 Note that we do not assume that the players have knowledge of the the joint utility function u. However, we do assume that the player has full information feedback, in the sense that they observe partial reward functions u( · , s−iτ ) on their entire action set, as opposed to only observing the reward u(s1τ , s 2 τ ) of the action played (the latter corresponds to the bandit setting).\nWe denote by Ũ it = {ũiτ}tτ=1 the sequence of partial payoff functions observed by player i. We use U it to denote the set of all possible such histories, and define U i0 := ∅. A strategy σi of player i is a collection {σit}∞t=1 of (possibly random) mappings σit : U it−1 → Si, such that at iteration t, player i plays sit = σ i t(U i t−1). We make the following assumption on the payoff function:\nAssumption 3 The payoff function u is uniformly continuous in si with modulus of continuity independent of s−i for i = 1, 2. That is, for each i there exists χi ∈ Z such that |u(s, s−i) − u(s′, s−i)| ≤ χi(di(s, s′)) for all s−i ∈ S−i.\nIt is easy to see that Assumption 3 implies that the game has a value (see e.g. the argument in the proof of Lemma 39). It also makes our setting compatible with that of our Dual Averaging algorithm from Section 3. Suppose therefore that each player randomizes their play according to the sequence of probability distributions on Si generated by Dual Averaging with regularizer hi. That is, suppose that for i ∈ {1, 2}, σit is a random variable with the following distribution:\nσit ∼ Dh∗i ( ηt−1 ∑t−1 τ=1 ũ i τ ) . (25)\nTheorem 27 Suppose that player i uses strategy σi according to (25). If the Dual Averaging algorithm ensures sublinear regret (i.e. lim suptRt/t ≤ 0), then σi is Hannan-consistent.\nCorollary 28 If both players use strategies according to (25) with the respective Dual Averaging ensuring that lim suptRt/t ≤ 0, then with probability 1 the sequence (x̂t)t≥1 of empirical distributions of play weakly converges to the set of Nash equilibria of G.\nInterestingly, even though Dual Averaging is performed on Lp(Si), a strict subset of P(Si), Corollary 28 still ensures weak convergence of the empirical distributions of play to N (G)."
    }, {
      "heading" : "5. Examples",
      "text" : ""
    }, {
      "heading" : "5.1. A Game With Unique Mixed Strategy Equilibrium",
      "text" : "Consider the zero-sum game G1 between two players playing on the unit interval Si = [0, 1] with payoff function given by\nu(s1, s2) = (1 + s1)(1 + s2)(1− s1s2)\n(1 + s1s2)2 (26)\nSince |Dsiu| ≤ 8 for any s−i ∈ [0, 1] the payoff function is Lipschitz. It can be shown that V = 4/π and that this game has no pure and a unique mixed Nash equilibrium, with equilibrium density xi(s) = 2\nπ √ s(1+s) the same for both players (Glicksberg and Gross, 1953). Note that xi is un-\nbounded and that xi ∈ Lp(Si, λ) for any 1 ≤ p < 2. This unboundedness is the reason for the slow convergence of the empirical distributions to xi near zero that we can observe in Figure 2."
    }, {
      "heading" : "5.2. A Game With Explicit Dual Averaging Updates",
      "text" : "Consider a zero-sum game G2 between two players on the unit interval with payoff function\nu(s1, s2) = s2s2 − a1s1 − a2s2\nwhere a1 = e−2e−1 and a 2 = 1e−1 . It is easy to verify that the pair (x 1, x2) given by x1(s) = exp(s)e−1 and x2(s) = exp(1−s)e−1 is a mixed-strategy Nash equilibrium of G2. For sequences (s 1 τ ) t τ=1 and (s 2 τ ) t τ=1, the cumulative payoff functions for fixed action s ∈ [0, 1] are given, respectively, by\nU1t (s 1) = ( Σtτ=1s 2 τ − a1t ) s1 − a2Σtτ=1s2τ U2t (s2) = ( a2t− Σtτ=1s1τ ) s2 − a1Σtτ=1s1τ\nIf each player i uses the Generalized Hedge Algorithm with a sequence of learning rates (ητ )tτ=1 to minimize their respective regret, then their strategy in period t is given by sampling from the distribution xit(s) ∝ exp(αits), where α1t = ηt ( Σtτ=1s 2 τ − a1t ) and α2t = ηt ( a1t − Σtτ=1s1τ ) . Interestingly, in this case the sum of the opponent’s past plays is a sufficient statistic, in the sense that it completely determines the mixed strategy at time t.\nFigure 3 shows normalized histograms of the empirical distributions of play at different iterations t. As t grows the histograms approach the equilibrium densities x1 and x2, respectively. Note however, that this does not mean that the individual strategies xit converge. Indeed, Figure 4 shows that the parameters αit keep oscillating around the equilibrium parameters 1 and −1, respectively, even for very large t. We do, however, observe that the time-averaged parameters ᾱit converge to the equilibrium values 1 and −1.\n100 101 102 103 104 105 106 1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nα1t ᾱ1t\nα2t ᾱ2t\nFigure 4: Evolution of parameters αit and ᾱ i t := 1 t\n∑t τ=1 α i τ in G2"
    }, {
      "heading" : "5.3. A Game on a Non-Convex Domain",
      "text" : "One of the most interesting features of the Dual Averaging algorithms discussed in Section 3 is that they are applicable also in case of non-convex domains. We may therefore utilize them as a tool to compute approximate Nash equilibria in continuous zero-sum games on non-convex domains. In particular, consider a game G3 in which each Si = [0, 2]2 \\ [0.4, 1]2 is an L-shaped subset of R2. It is easy to see that the Lebesgue measure on this set is Q-regular with Q = 2, c0 = π4 and C0 = π. We define the metric d̃ on S1 between any two points a, b ∈ Si as the length (in the Euclidean distance) of the shortest path between a and b that is entirely contained in Si. The payoff function u is given as u(s1, s2) = d̃(s1, s2)− 110 d̃(s 1, 0), which can be interpreted as a “hide and seek” game\nin which player 1 would like to get as far away from player 2 as possible, while at the same time having a preference for being near the origin. Player 2 instead wants to be as close to player 1 as possible. Intuitively, this game will not admit a pure Nash equilibrium. Given the geometry of the problem, computing a mixed Nash equilibrium (whose existence follows from Theorem 20) poses a challenge. Instead, having both players play Entropy Dual Averaging on Lp(Si, λ), we observe in Figure 5 that they indeed incur sublinear regret, and that the empirical distributions of play do converge. Figure 6 shows Kernel Density Estimates (KDE) of x̂1t and x̂ 2 t after t = 7500 iterations."
    }, {
      "heading" : "Appendix A. Review of Some Results From Convex Analysis",
      "text" : "In this section we collect some results from infinite-dimensional convex analysis that will play an important role in our analysis of the Dual Averaging algorithm.\nLemma 29 (Asplund, 1968) Let f : X → (−∞,+∞] be proper lower semicontinuous. For a pair (x0, ξ0) ∈ X ×X∗ the following are equivalent:\n(i) f∗ is finite and Fréchet differentiable at ξ0 with Fréchet derivative Df∗(ξ0) = x0.\n(ii) For some γ∗ ∈ ΓL,\nf∗(ξ) ≤ f∗(ξ0) + 〈x0, ξ − ξ0〉+ γ∗(‖ξ − ξ0‖), ∀ ξ ∈ X∗ (27)\nand f∗(ξ0) ∈ R.\n(iii) For some γ ∈ ΓU ,\nf(x) ≥ f(x0) + 〈x− x0, ξ0〉+ γ(‖x− x0‖), ∀x ∈ X (28)\nand f(x0) ∈ R.\n(iv) f∗ is finite at ξ0, dom f∗ is radial at ξ0, and xj → x0 in norm whenever\nlim j→∞ 〈xj , ξ0〉 − f(xj) = f∗(ξ0) (29)\nAny of the above conditions implies that 〈x0, ξ0〉 = f(x0) + f∗(ξ0) (in other words: the Fenchel-Young inequality holds with equality) and that f(x0) = f∗∗(x0). The functions γ and γ∗ in (ii) and (iii) form a pair of mutually dual functions.\nNote that the function f in Lemma 29 need not be convex. The following result will be essential to our analysis:\nTheorem 30 (Strömberg, 2011) Let f : X → (−∞,+∞] be lower semicontinuous. Then f∗ is proper and essentially Fréchet differentiable if and only if f is a convex proper function that is essentially strongly convex."
    }, {
      "heading" : "Appendix B. Dual Averaging in Continuous Time",
      "text" : "In this section we use ideas from Kwon and Mertikopoulos (2014) and introduce a continuous-time regret minimization problem related to the one in discrete-time discussed in Section 2.2. In fact, this analysis will be crucial in proving the discrete-time regret bound (9) in Theorem 6."
    }, {
      "heading" : "B.1. Regret Minimization in Continuous Time on Reflexive Banach Spaces",
      "text" : "Consider a reflexive Banach space X with dual X∗ and regularizer h on X . Furthermore, suppose that uc : [0,∞)→ X∗ is a continuous-time reward process satisfying the following assumptions:\nAssumption 4 The reward process uc is locally integrable for any x ∈ X . That is, for all x ∈ X , rx : t 7→ 〈uct , x〉 is Lebesgue-integrable on any compact set K ⊂ [0,∞).\nAssumption 5 There exists M <∞ such that supx∈X |〈uct , x〉| ≤M for all t.\nLet ηc : [0,∞) → (0,∞) be a non-increasing and piece-wise continuous learning rate process. Furthermore, let U ct = ∫ t 0 ucτ dτ be the cumulative reward function. We consider the continuous-time process xc : [0,∞)→ X given by\nxct := Dh ∗(ηct U c t ) (30)\nTheorem 31 (Continuous-Time Regret Bound) Let h be a regularizer function on X , let ηc be nondecreasing and locally piecewise continuous. Suppose that the reward process uc satisfies Assumptions 4 and 5. Then under (30) we have, for any x ∈ X , that\nRct(x) := ∫ t 0 〈ucτ , x〉 dτ − ∫ t 0 〈ucτ , xcτ 〉 dτ ≤ h(x)− h ηct (31)\nwhere h := infx∈X h(x).\nProof [Theorem 31] Let yct = ηct ∫ t 0 ucτ dτ . By linearity,\nηct ∫ t 0 〈ucτ , x〉 dτ = ηct 〈∫ t 0 ucτ dτ, x 〉 = 〈yct , x〉\nAssume for now that ηc ∈ C1. If h is proper, then∫ t 0 〈ucτ , x〉 dτ = 〈yct , x〉 ηct ≤ h ∗(yct ) + h(x) ηct = h∗(yct ) ηct + h(x) ηct (32)\nby the Fenchel-Young inequality. By Theorem 4, h∗ is essentially Fréchet differentiable with Fréchet gradient Dh∗(y). Furthermore, yct is differentiable. Thus, applying the chain rule and using that x c t = Dh\n∗(yct ) = arg maxx∈X ( 〈x, yct 〉 − h(x) ) = arg maxx∈X ( 〈x, yct 〉 − h(x) ) we obtain\nd\ndt\nh∗(yct ) ηct = ηct 〈Dh∗(yct ), ddty c t 〉 − h∗(yct ) η̇c(t) (ηct ) 2\n=\n〈 xct , ( ηctu c t + η̇ c t ∫ t 0 uc(s, τ) dτ )〉 ηct − η̇ c t (ηct ) 2 h∗(yct )\n= 〈xct , uct〉+ η̇ct\n(ηct ) 2\n( 〈xct , yct 〉 − h∗(yct ) ) = 〈xct , uct〉+\nη̇ct (ηct ) 2 h(xct)\nNow η̇ct ≤ 0 by assumption, and hence\nd\ndt\nh∗(yct )\nηct ≤ 〈xct , uct〉+ η̇ct (ηct ) 2 h\nIntegrating from t = 0 to t = t yields\nh∗(yct )\nηct − h\n∗(yc0) ηc(0) ≤ ∫ t 0 〈xcτ , ucτ 〉 dτ + h ∫ t 0 η̇cτ (ηcτ ) 2 dτ\n= ∫ t 0 〈xcτ , ucτ 〉 dτ − h ( 1 ηct − 1 ηc0 ) Now yc0 = 0, and hence h ∗(yc0) = supx∈X −h(x) = −h, and so\nh∗(yct ) ηct ≤ ∫ t 0 〈xcτ , ucτ 〉 dτ − h ηct\nPlugging this into (32), collecting terms and rearranging yields (31). Now suppose that ηc is only piecewise continuous. Then there exists a sequence (ηc,i)∞i=1 of positive nonincreasing C1 functions such that ηc,i → ηc pointwise a.e.. Let xc,it := Dh∗(η c,i t U c t ). Note that Dh\n∗ is continuous by Theorem 4 and thus xc,it → xct pointwise. By Assumption 5 we have that |〈ucτ , xc,iτ 〉| < M for all τ, i and thus ∫ t 0 〈ucτ , xc,iτ 〉 dτ → ∫ t 0 〈ucτ , xcτ 〉 dτ by Dominated Convergence."
    }, {
      "heading" : "B.2. Online Optimization in Continuous Time on Compact Metric Spaces",
      "text" : "One can also obtain bounds on the regret in continuous time by using similar arguments as in Section 3. While we do not make use of them in the main part of this article, these bounds may be of independent interest.\nWe consider the setting of Section 3. Specifically, let (S, d) be a compact metric space, and let µ ∈ P , the set of Borel measures on S. Denote by B(s, r) = {s′ ∈ S : d(s, s′) < r} the open ball of radius r centered at s. For p > 1 consider X = Lp(S, µ) and X = {x ∈ X : x ≥ 0 a.e., ‖x‖1 = 1}, the set of probability measures on S that are absolutely continuous w.r.t. to µ and whose Radon-Nikodym densities are p-integrable. Denote by DS := sups,s′∈S d(s, s\n′) the diameter of S and by B(s, ϑct) ⊂ X the set of elements of X with support contained in B(s, ϑct). We need the following continuous-time variant of Assumption 1:\nAssumption 6 The reward process uc has modulus of continuity χ on S, uniformly in t. That is, there exists χ ∈ Z such that |uct(s)− uct(s′)| ≤ χ(d(s, s′)) for all s, s′ ∈ S for all t.\nTheorem 32 (Continuous-Time Regret Bound on Metric Spaces) Let (S, d) be compact, and suppose that Assumption 6 holds. Let h be a regularizer function on X , and let ηc be non-decreasing and locally piecewise continuous. Suppose further that ϑc : [0,∞) → (0,∞) is a non-negative function and that the reward process uc satisfies Assumptions 4 and 5. Then, under the process (30),\nRct ≤ sups∈S infx∈B(s,ϑct) h(x)\nηct + t χ(ϑct)−\nh ηct (33)\nProof [Theorem 32] Similar to the proof of Theorem 8.\nProposition 33 Suppose that Assumption 2 holds with constants c0 > 0 and C0 < ∞. Under the Assumptions of Theorem 32, with essentially strongly convex regularizer hφ the f -divergence of an ω-potential φ, we have the following regret bound:\nRct t ≤ min(C0 (ϑ c t) Q, µ(S)) t ηct fφ ( c−10 (ϑ c t) −Q)+ χ(ϑct) (34)\nProof [Proposition 33] Similar to the proof of Proposition 13."
    }, {
      "heading" : "Appendix C. Computing the Dual Averaging Optimizer",
      "text" : "In this section we discuss some aspects concerning the computation of the optimizer in the Dual Averaging update in the setting of online optimization on compact metric spaces with uniformly continuous rewards. The results of this section are used for generating the Hannan-consistent strategies in the repeated games in Section 5, and for performing the numerical benchmarks of the algorithms in Appendix D.\nAs pointed out in Section 3.2, it can be shown that for f -Divergences of ω-potentials, the Fréchet differential Dh∗ in this case has a simple expression in terms of the dual problem, and the problem of computing xt+1 = Dh ∗(ηt ∑t τ=1 uτ ) reduces to computing a scalar dual variable ν ∗ t . In particular, one can show the following:\nProposition 34 (Krichene and Balandat, 2016) Let φ be an ω-potential with associated f -Divergence hφ on X . Then\nDh∗φ(ξ) = φ(ξ + ν ?)+ (35)\nwhere ( · )+ denotes the positive part of ( · ), and ν? satisfies ∫ S φ(ξ + ν?)+ dµ(s) = 1.\nBy Proposition 34, the Fréchet derivativeDh∗φ at ξ = ηtUt is entirely determined by the dual variable ν ?, the unique ν such that f(ν) = 1, where f(ν) = ∫ S φ(ηt(Ut(s) + ν\n?))+ dµ(s). Since f is increasing by assumption on φ, ν? can be determined using a simple bisection method. To guide the search for ν?t for t > 0 we can make use of the following result:\nProposition 35 Suppose φ is convex and let ν?t the optimal dual variable determining Dh∗φ(ηtUt). Then\nηt ηt+1 ν?t −M ≤ ν?t+1 ≤ ηt ηt+1 ν?t + ηt − ηt+1 ηt+1 tM (36)\nwhere ν?0 = η −1 0 φ −1(1). Moreover, for ηt = η t−β this interval has length ≈ (1 + β)M .\nProof [Proposition 35] Since Ut ≡ 0, we have ν?0 = η−10 φ−1(1). Moreover, by definition we have∫ S φ ( ηt ( Ut(s) + ν ? t )) + dµ(s) = ∫ S φ ( ηt+1 ( Ut+1(s) + ν ? t+1 )) + dµ(s) = 1\nIf φ is convex, then so is φ( · )+ as z 7→ z+ is convex and nondecreasing. Therefore\n1 = ∫ S φ ( ηt ( Ut(s) + ν ? t ) + (ηt+1 − ηt)Ut(s)− ηtν?t + ηt+1ν?t+1 + ηt+1ut+1(s) ) + dµ(s)\n≤ ∫ S φ ( ηt ( Ut(s) + ν ? t )) + + φ ( ηt ( Ut(s) + ν ? t ))′ + ( (ηt+1 − ηt)Ut(s)\n− ηtν?t + ηt+1ν?t+1 + ηt+1ut+1(s) ) dµ(s)\n≤ 1 + ∫ S φ ( ηt ( Ut(s) + ν ? t ))′ + ( ηt+1ν ? t+1 − ηtν?t + ηt+1M ) dµ(s)\nand hence, since φ′ ≥ 0, we must have that ηt+1ν?t+1 − ηtν?t + ηt+1M ≥ 0. Rearranging yields the lower bound on ν?t+1. The other inequality is proven in a similar fashion by reversing the roles of t and t+1. Finally, to show that the interval has length ≈ (1 + β)M independent of t, note that ηtηt+1 = (1 + 1 t ) β ≈ 1 + βt , and so ηt−ηt+1ηt+1 tM ≈ βM .\nHaving determined ν?t , we then have an explicit form of the distribution over S from which to sample st+1. For this, a variety of established methods can be used, from simple rejection sampling in low dimensions (employed in our simulations) to MCMC methods (e.g. slice sampling) in higher dimensions. In cetain special cases, sampling from xt may be done very efficiently. For example, if the losses are affine, the domain S is a hyperrectangle, and the potential is a generalized Exponential Potential, then st+1 can be obtained by sampling from n independent truncated exponential random variables. The main computational challenge is then to compute the integral in f . Off-the-shelf numerical integration schemes work well if n is small, but are typically not applicable in higher dimensions. Instead, one has to resort to other methods, such as Monte Carlo methods or sparse grids."
    }, {
      "heading" : "Appendix D. Numerical Results and Comparison With Other Methods",
      "text" : "In this section, we review some algorithms for online convex optimization over subsets of Rn that have been proposed in the literature, and compare them with our Dual Averaging method for online optimization on compact metric spaces with uniformly continuous rewards from Section 3. Such algorithms are often formulated in terms of loss functions `τ , but clearly these algorithms apply just as well by setting `τ = −uτ , as long as the set S is convex and the rewards are concave and satisfy the additional assumptions made by the algorithms. Table 1 summarizes the regret bounds of each method, with the corresponding assumptions on the feasible set and the loss functions.\nThe bound on Dual Averaging in Table 1 is obtained by assuming the regularizer to be the f -divergence associated to an ω-potential and making an assumption on the asymptotic growth rate of the function fφ as follows:\nCorollary 36 Suppose that fφ(x) ≤ Cφ x1+κ for some κ > 0 and Cφ < ∞. Suppose further the rewards are α-Hölder continuous, i.e. χ(r) = Cα rα, and that hφ is uniformly essentially strongly convex with modulus γ(r) = K2 r 2. Then the learning rate ηt = η t−β with η = 1M ( 1+ καQ 2+ καQ C0Cφ c1+κ0 ϑ κQ )1/2 and β = 12+ καQ yields the following bound:\nR t ≤ ( 2MC̃ϑ− κQ 2 + Cαϑ α ) t − 1 2+ κ α Q (37)\nfor any ϑ < r0, where C̃ = √ 2+ καQ\n1+ καQ C0Cφ c1+κ0 ."
    }, {
      "heading" : "D.1. Optimizing Sequences of Convex Functions over Convex Sets",
      "text" : "Zinkevich (2003) formalized the online convex optimization problem, in which the feasible set S and the loss functions are assumed to be convex. He proposed a Greedy Projection method (GP), summarized in Algorithm 1, which we will also refer to as Online Gradient Descent (OGD). Theorem 1 in (Zinkevich, 2003) shows that when ‖∇`t‖ is uniformly bounded, the regret of GP with learning rates ηt = 1/ √ t grows as\nO( √ t). Hazan et al. (2007) show that it is possible to obtain logarithmic regret under additional assumptions on the loss functions. In particular, if the losses are H-strongly convex then GP with learning rates ηt = 1Ht has regret Rt ≤ M 2\n2H (1 + log t). They also propose methods for uniformly exp-concave losses, that is,\nwhen there exists α > 0 such that exp(−α`t) is concave for all t. These methods, Exponentially Weighted Online Optimization (EWOO) and Follow The Approximate Leader (FTAL), are summarized in Algorithm 2 and 3 (their Online Newton Step (ONS) algorithm is very similar to FTAL and and therefore omitted). The respective regret bounds are given in Theorems 4 and 7 in (Hazan et al., 2007) and are summarized in Table 1.\nAlgorithm 1 Greedy Projection method (GP) a.k.a. Online Gradient Descent (OGD), with input sequence (`t) and learning rates (ηt)\n1: for t ∈ N do 2: Let s̃t+1 = st − ηt+1∇`t(st) 3: Update: xt+1 = δst+1 , where\nst+1 = arg min s∈S\n‖s− s̃t+1‖\nAlgorithm 2 Exponentially Weighted Online Optimization method (EWOO), with input sequence (`t) and learning rate α.\n1: for t ∈ N do 2: Let Lt = ∑t τ=1 `τ 3: Let x̃t+1(s) = e −αLt(s)∫\nS e −αLt(s)λ(ds)\n4: Update: xt+1 = δst+1 , where\nst+1 = Es∼x̃t+1 [s]\nAlgorithm 3 Follow The Approximate Leader (FTAL) with input sequence (`t) and parameter β.\n1: for t ∈ N do 2: Let gτ = ∇`τ (sτ ) 3: Let At = ∑t τ=1 gτ (gτ )\nT and s̃t+1 = (At) †(∑t τ=1 gτ (gτ ) T sτ − 1β gτ ) ,\nand define ‖s‖At = 〈s,Ats〉. 4: Update: xt+1 = δst+1 , where\nst+1 = arg min s∈S ‖s− s̃t+1‖At\nAlgorithm 4 Dual Averaging (DA) with input sequence (ut), learning rates (ηt), and regularizer h.\n1: for t ∈ N do 2: Let Ut = ∑t τ=1 uτ 3: Update\nx(t+1) = Dh ∗(ηtUt)\n= arg max x∈X\n〈 ηtUt, x 〉 − h(x)\nExample 4 (Convex Quadratics on a Hypercube) As a first example, we consider quadratic reward functions of the form ut(s) = − 12 (s−µt)\nTQt(s−µt)− ct, where Qt is p.d. symmetric, and ct ≥ 0. The domain is S = {‖s‖∞ ≤ 0.5} with DS = √ n, and the rewards are generated randomly, L-Lipschitz with L = 5 and uniformly bounded by ‖ut‖∞ ≤ 3.75 and ‖ut‖4 ≤ 1.6. Figure 7 shows the time-average regretsRt/t in dimensions n = 2 and n = 3 for time horizons of T = 104 and T = 4 · 103, respectively. Displayed are the empirical means over N = 2500 runs of the algorithm (solid), the associated theoretical bounds1 (dashed), and the regions between the associated 10% and 90% quantiles (shaded).\nNot surprisingly, those algorithms that exploit the strong convexity of the problem (OGD, FTAL, EWOO) achieve better asymptotic rates than GP (which requires only convexity) or DA (which makes no convexity assumptions at all). Still, the regret of DA is not significantly higher than that of GP and OGD, and is competitive with FTAL over the simulation horizon. We note that the theoretical regret bounds for both DA instances are much closer to the actual regret of the algorithm.\nTable 2 shows the decay rates (which correspond to the slopes in the log-log plots) of empirical means and theoretical bounds in Figure 7 at the end of the simulation horizon. There is a relatively good match between bounds and simulations. Except for FTAL and EWOO, all algorithms exhibit a decay that is faster than that of the associated bound2. When making this comparison, one must keep in mind that all these bounds are worst-case in nature, and that it is not entirely clear what characterizes a worst-case sequence of\n1. For easier readability we omitted the bound on FTAL, which in this example is much higher than the others. 2. For EWOO this discrepancy is likely due to numerical inaccuracies at the very small regrets for large t, while for\nFTAL the simulation may not have reached the asymptotic regime yet.\nreward functions (see Example 5 for a partial remedy).\nn = 2 n = 3 Algorithm simulation theory simulation theory GP -0.564 -0.497 -0.515 -0.495 OGD -0.920 -0.900 -0.892 -0.888 FTAL -0.780 -0.900 -0.705 -0.888 EWOO -0.809 -0.900 -0.676 -0.888 DA, Exp -0.519 -0.446 -0.481 -0.439 DA, 1.5-Norm -0.452 -0.333 -0.396 -0.286\nTable 2: Rates in Figure 7\nPotential simulation theory ExpPot -0.557 -0.446 1.01-Norm -0.546 -0.495 1.05-Norm -0.477 -0.476 1.5-Norm -0.307 -0.333 1.75-Norm -0.279 -0.286\nTable 3: Rates in Figure 8\nExample 5 (Alternating Affine Losses on a Hypercube) In this example we consider a situation in which the greedy algorithm mentioned in Section 3 fails3, and offer a simulation that illustrates the result of Proposition 18. We consider a sequence of affine reward functions on S = {‖s‖∞ ≤ 0.5} in R2, alternating in such a way that any maximizer s?t of Ut is in fact a minimizer of Ut+1. Specifically, we choose ut(s) = −〈at, s〉−ct, where\na0 = [L/2, 0], c0 = L/4, at = [(−1)tL, 0], ct = L/2\nfor t ≥ 1. It is easy to see that in this case the greedy algorithm incurs time-average regretRt/t = L + o(1). Figure 8 shows regrets for the greedy algorithm and DA with Exponential and different ρ-Norm potentials. Besides the obvious failure of the greedy algorithm, we observe that for p-Norm potentials performance decreases as ρ ↘ 1, which can be explained by Proposition 18. Nevertheless, DA guarantees sublinear regret for any ρ > 1 (with theoretical asymptotic rate approaching t−1/2 as ρ → 1), though at the cost of much higher constants in the bound as ρ ≈ 1. Table 3 shows that empirical and theoretical rates in this instance (which is intuitively hard) are very close, providing further support for the theoretical analysis of DA. Finally, Figure 9 for each potential shows the negative entropy DKL(xt||λ) of xt. From this we observe that the minimizers x?[ρ] are indeed more and more concentrated around their mode as ρ↘ 1.\n3. In fact, any deterministic policy will incur linear regret in a nontrivial adversarial setting."
    }, {
      "heading" : "Appendix E. Proofs Omitted in the Main Part",
      "text" : ""
    }, {
      "heading" : "PROOF OF THEOREM 4",
      "text" : "Proof [Theorem 4] Essential Fréchet differentiability, the characterization (7) of the Fréchet gradient in (i) and (ii) follow from Theorem 30, Lemma 29, and the definition of uniform essential strong convexity. To prove (8), let ξ1, ξ2 ∈ X∗ and let xi = Df∗(ξi) = arg maxx∈X〈ξi, x〉−f(x). Then, by first-order optimality, 〈z − ξi, x− xi〉 ≥ 0, ∀ z ∈ ∂f(xi),∀x ∈ X . In particular,\n〈z1 − ξ1, x2 − x1〉 ≥ 0 〈z2 − ξ2, x1 − x2〉 ≥ 0\nfor all zi ∈ ∂f(xi), i = 1, 2. Summing these inequalities we find that\n〈ξ1 − ξ2, x1 − x2〉 ≥ 〈z1 − z2, x1 − x2〉\nBy uniform strong convexity, we further have that f(x) ≥ f(xi) + 〈x− xi, zi〉+ γ(‖x− xi‖) for all x ∈ X . In particular,\nf(x1) ≥ f(x2) + 〈x1 − x2, z2〉+ γ(‖x1 − x2‖) f(x2) ≥ f(x1) + 〈x2 − x1, z1〉+ γ(‖x2 − x1‖)\nand summing these inequalities yields\n〈z1 − z2, x1 − x2〉 ≥ 2γ(‖x1 − x2‖)\nOn the other hand, 〈ξ1 − ξ2, x1 − x2〉 ≤ ‖ξ1 − ξ2‖∗‖x1 − x2‖ by definition of the dual norm, so\nγ̃(‖x1 − x2‖) ≤ 1\n2 ‖ξ1 − ξ2‖∗\nusing the definition of γ̃. If γ̃ is strictly increasing it admits a (strictly increasing) inverse γ̃−1. Applying γ̃−1 to both sides then yields (8)."
    }, {
      "heading" : "PROOF OF THEOREM 6",
      "text" : "Proof [Theorem 6] We consider the continuous-time reward and learning rate processes uc and ηc given by uct := udte and η\nc(t) := ηbtc∨1, respectively, where dre := inf{n ∈ Z : n ≥ r} and brc = sup{n ∈ Z : n ≤ r} for all r ∈ R and a ∨ b = min(a, b). In doing so we follow the ideas of the analysis of Kwon\nand Mertikopoulos (2014) (our problem is, however, different as our reward vectors are infinite-dimensional). With this\nxk = Dh ∗ ( ηk−1 k−1∑ j=1 uj ) = Dh∗ ( ηc(k − 1) ∫ k−1 0 ucτ dτ ) = xck−1\nand thus, for j ≥ 1 and t ∈ (j − 1, j), we have\n|〈uct | xct〉 − 〈uj , xj〉| = |〈uj , xct − xcj−1〉| ≤ ‖uj‖∗‖xct − xcj−1‖ (38)\nby definition of the dual norm. Therefore\n|〈uct , xct〉 − 〈uj , xj〉| ≤ ‖uj‖∗‖Dh∗(yct )−Dh∗(ycj−1)‖ ≤ ‖uj‖∗ γ̃−1 ( ‖yct − ycj−1‖∗/2 ) (39)\nwhere the second inequality follows from Theorem 4. From the definition of yct , we have\n‖yct − ycj−1‖∗ = ∥∥∥∥ηc(j − 1)∫ t\nj−1 ucτ dτ ∥∥∥∥ ∗ ≤ ηj−1‖uj‖∗(t− j + 1)\nand therefore∣∣∣∣∫ k 0 〈ucτ , xcτ 〉dτ − k∑ j=1 〈uj , xj〉 ∣∣∣∣ ≤ k∑ j=1 ∫ j j−1 |〈ucτ , xcτ 〉 − 〈uj , xj〉| dτ\n≤ k∑ j=1 ‖uj‖∗ ∫ j j−1 γ̃−1 ( ηj−1(t− j + 1) 2 ‖uj‖∗ ) dτ ≤ k∑ j=1 ‖uj‖∗γ̃−1 (ηj−1 2 ‖uj‖∗\n) where the last equality follows since γ̃−1 is non-decreasing (a consequence of γ being sublinear). Finally, we note that\nk∑ j=1 〈uj , x〉 − k∑ j=1 〈uj , xj〉 = ∫ k 0 〈ucτ , x〉 dτ − k∑ j=1 〈uj , xj〉\n≤ ∣∣∣∣∫ k\n0 〈ucτ , x〉 dτ − ∫ k 0 〈ucτ , xcτ 〉dτ ∣∣∣∣+ ∣∣∣∣∫ k 0 〈ucτ , xcτ 〉dτ − k∑ j=1 〈uj , xj〉 ∣∣∣∣\nThe bound (10) then follows from Theorem 31 and the above."
    }, {
      "heading" : "PROOF OF COROLLARY 7",
      "text" : "Proof [Corollary 7] It is easy to show that γ̃−1 (ηj−1 2 ‖uj‖∗ ) ≤ (2C)−1/κ η1/κj−1 ‖uj‖ 1/κ ∗ . If ‖uj‖∗ ≤ M for all j, then Rt(x) ≤ h(x)−hηct + (2C) −1/κM1+1/κ ∑t τ=1 η 1/κ τ−1. In particular, if ηt = η t −β , then (12) follows\nfrom the bound ∑t τ=1 (j − 1) −β/κ ≤ ∫ t 0 v−β/κdv = κκ−β t 1− βκ"
    }, {
      "heading" : "PROOF OF THEOREM 8",
      "text" : "Proof [Theorem 8] The spaceX = Lp(S) is uniformly convex (Clarkson, 1936), and thus reflexive (Milman, 1938). Its dual is X∗ = Lq(S, µ) for q = pp−1 and 〈x, ξ〉 = ∫ S x(s)ξ(s)µ(ds) for x ∈ X and ξ ∈ X∗. Fix t <∞. Then for any s ∈ S and all x ∈ B(s, ϑt)\n〈Ut, x〉 = ∫ B(s,ϑt) Ut(s ′)x(s′) dµ(s′) = ∫ B(s,ϑt) t∑ τ=1 uτ (s ′)x(s′) dµ(s′) dτ\n≥ t∑\nτ=1\n∫ B(s,ϑt) ( uτ (s)− χ(ϑt) ) x(s′) dµ(s′) dτ = Ut(s)− t χ(ϑt)\nand therefore\nRt = sup s∈S\nUt(s)− t∑\nτ=1\n〈uτ , xτ 〉\n≤ sup s∈S inf x∈B(s,ϑt)\n〈Ut, x〉+ t χ(ϑt)− t∑\nτ=1\n〈uτ , xτ 〉\n= sup s∈S inf x∈B(s,ϑt) Rt(x) + t χ(ϑt)\nand thus (33) follows from (31) in Theorem 31."
    }, {
      "heading" : "PROOF OF PROPOSITION 11",
      "text" : "Denote by 1A the indicator function of the set A, i.e. 1A(s) = 1 if s ∈ A and 1A(s) = 0 if s 6∈ A. In this proof we will make use of the following Lemma:\nLemma 37 Let (S, d) be a compact metric space and let µ be an r0-locally Q-regular measure on S. For p ≥ 1 let X p := {x ∈ Lp(S, µ) : x ≥ 0 a.s., ‖x‖1 = 1}. Suppose further that f : S → R is continuous. Then\nsup s∈S f(s) = sup x∈P ∫ S f(s) dx(s) = sup x∈Xp ∫ S f(s) dx(s), ∀ p ∈ [0,∞] (40)\nProof [Lemma 37] The first equality follows directly by observing that Borel measures measures include measures with finite support. Clearly supx∈P ∫ S f(s) dx(s) ≥ supx∈Xp ∫ S f(s) dx(s) since X p ⊂ P for all p ∈ [1,∞]. Since Lp ⊂ Lq for all q ≥ p it suffices to show the reverse inequality holds for p =∞. Since S is compact and f is continuous, there exists a maximizer s? of f on S. Let > 0. By continuity, there exists δ > 0 such that |f(s)− f(s′)| ≤ whenever d(s, s′) < δ. Moreover, by local Q-regularity of µ we have that µ(B(s?, δ)) > 0. Now let x(s) = 1µ(B(s?,δ)) 1B(s?,δ)(s). Clearly x ∈ X\n∞, and∫ Si f(s) dx(s) = 1 µ(B(s?, δ)) ∫ B(s?,δ) f(s) dλ(s) ≥ 1 µ(B(s?, δ)) ∫ B(s?,δ) (f(s?)− ) dλ(s) = f(s∗)−\nNow let ↘ 0.\nProof [Proposition 11] Recall that\nRt(x) = t∑ τ=1 〈uτ , x〉 − t∑ τ=1 〈uτ , xτ 〉 = ∫ S Ut(s) dx(s)− t∑ τ=1 〈uτ , xτ 〉\nClearly Ut is continuous (in fact, with modulus of continuity t χ(r)) on S for any t < ∞. The equivalence of the suprema then follows from a direct application of Lemma 37."
    }, {
      "heading" : "PROOF OF PROPOSITION 13",
      "text" : "Proof [Proposition 13] By convexity of f , we have that h(x) = hφ(x) ≥ fφ (∫ S dx dµdµ(s) ) = fφ(1) = 0 for all x ∈ X , and thus h = 0. Furthermore, choosing x as the uniform Radon-Nikodym density w.r.t. µ on B(s, ϑt), i.e.,\nx(s′) = 1B(s,ϑt)(s\n′)\nµ(B(s, ϑt))\nwe have that\nh(x) = ∫ Si fφ(x(s ′))µ(ds′) = ∫ B(s,ϑt) fφ ( 1 µ(B(s, ϑt)) ) µ(ds′)\n≤ min ( C0(ϑt) Q, µ(S) ) fφ\n( 1\nµ(B(s, ϑt)) ) where we used the assumption of r0-local Q-regularity and the fact that ϑt ≤ r0. It is easy to see that fφ is increasing on [1,∞). Indeed, f ′φ(x) = φ−1(x), and φ−1(x) is increasing by assumption with φ−1(1) ≥ 0. Moreover, since µ(S) = 1 by assumption, we have that µ(B(s, ϑt)) ≤ 1 for any s, so\nh(x) ≤ min ( C0(ϑt) Q, µ(S) ) fφ ( c−10 (ϑt) −Q) Plugging this into the general bound (13) of Theorem 8 yields (16)."
    }, {
      "heading" : "PROOF OF COROLLARY 14",
      "text" : "Proof [Corollary 14] Plugging γ̃(r) = 2r, fφ(x) = x log x and χ(r) = Cαrα into (16) we find that\nRt t ≤ C0 c0 t ηt\nlog ( c−10 ϑ −Q t ) + Cαϑ α t + M2\nt t∑ τ=1 ητ−1\nLetting ηt = η √ log t t−β we have that\nt∑ τ=1 ητ−1 ≤ η √ log t t∑ τ=1 t−β ≤ η √ log t t∑ τ=1 ∫ τ τ−1 z−βdz = η √ log t ∫ t 0 z−βdz = η √ log t 1− β t1−β\nand therefore\nRt t ≤ C0 c0η tβ−1√ log t\nlog ( c−10 ϑ −Q t ) + Cαϑ α t + ηM2 1− β √ log t t−β\nChoosing β = 1/2 and ϑt = ϑ 1 α (log t) 1 2α t− β α this becomes, after dropping a 1/ log t term,\nRt t ≤ ( C0 c0 η ( log(c−10 ϑ −Q/α) + Q 2α ) + Cαϑ+ 2ηM 2 )√ log t t\nas ϑt < r0 since √ log t/t < ϑ−1rα0 . Then choosing η = 1 M √ C0Q 2c0 log(c−10 ϑ −Q/α) + Q2α gives\nRt t ≤ ( 2M √ 2C0 c0 ( log(c−10 ϑ −Q/α) + Q 2α ) + Cαϑ )√ log t t"
    }, {
      "heading" : "PROOF OF THEOREM 15",
      "text" : "Proof [Theorem 15] Since S is compact there exist sa, sb ∈ S such that d(sa, sb) = DS . Let xa = δsa and xb = δsb , where δs denotes the Dirac measure on S at s. Let w : R → R be any function with modulus of continuity χ such that ‖w(d( · , sb))‖q ≤ M . Define v : S → R by v(s) = w(d(s, sb)). Using the triangle inequality it is easy to see that v also has modulus of continuity χ. Now observe that\n〈v, xa − xb〉 = v(sa)− v(sb) = w(d(sa, sb)) = w(DS)\nLet V1, . . . , V2 a sequence of i.i.d. Rademacher random variables, i.e. P(Vi = +1) = P(Vi = −1) = 12 , and consider the (random) sequence of reward vectors (uτ )tτ=1 with ut = Vtv. By Proposition 11 we have that Rt = supx∈P Rt(x), and thus\nE[Rt] = E [\nsup x∈P t∑ τ=1 〈uτ , x〉 − t∑ τ=1 〈uτ , xτ 〉 ] ≥ E [ max x∈{xa,xb} t∑ τ=1 〈uτ , x〉 ] − E [ t∑ τ=1 〈uτ , xτ 〉 ]\n= E [\nmax x∈{xa,xb} t∑ τ=1 Vτ 〈v, x〉 ] − E [ t∑ τ=1 Vτ 〈v, xτ 〉 ]\nObserve that the second expectation is zero for any sequence of (xτ )tτ=1 with xτ measurable with respect to σ(V1, . . . , Vτ−1), i.e. any online algorithm. Noting that max(a, b) = 12 (a+ b) + 1 2 |a− b| we thus have that\nE[Rt] ≥ 1 2 E [ t∑ τ=1 Vτ 〈v, xa + xb〉 ] + 1 2 E [ ∣∣∣∣ t∑ τ=1 Vτ 〈v, xa − xb〉 ∣∣∣∣]\n= w(DS) 2 E [ ∣∣∣∣ t∑\nτ=1\nVτ ∣∣∣∣] ≥ w(DS)2√2 √t where the last step follows from an application of Khintchine’s inequality (Haagerup, 1981)."
    }, {
      "heading" : "PROOF OF PROPOSITION 16",
      "text" : "Lemma 38 Let C ∈ R and 0 < β ≤ 1. The function v : [0,∞) given by v(r) = Crβ is Hölder continuous with modulus of continuity χ(r) = |C|βrβ .\nProof [Lemma 38] Noting that |x + y|β ≤ |x|β + |y|β for any x, y ∈ R we find with x = Cr1 − Cr2 and y = Cr2 for any r1, r2 ≥ 0 that |C|rβ1 − |C|r β 2 ≤ |C|β |r1 − r2|β . Exchanging the roles of r1 and r2 then\nyields ∣∣Crβ1 − Crβ2 ∣∣ ≤ |C|β |r1 − r2|β .\nProof [Proposition 16] With sa, sb as in the proof of Theorem 15, choose w(r) = min ( C1/αα , M‖d( · , sb)α‖−1q ) rα\nThen clearly ‖w(d( · , sb))‖q ≤ M by construction. Moreover, w has modulus of continuity χ̃(r) ≤ Cαrα by Lemma 38. The result follows from observing that ‖d( · , sb)α‖q ≤ ‖DαS ‖q = DαS ."
    }, {
      "heading" : "PROOF OF PROPOSITION 18",
      "text" : "Proof [Proposition 18] Fix t < ∞ and let δ > 0. Consider x ∈ X with := ∫ (B∗δ )\nc x(s)µ(ds) > 0. and define the function κ : R+ → R+ as κ(u) = sups∈(B∗u)c Ut(s). Clearly, κ is decreasing, κ(u) < U\n∗ for u > 0 by definition of S∗, and continuous (by continuity of Ut). We then have that Ut(s) < U∗−κ(d(s, S∗)) for all s ∈ S. Let 0 < δ′ < χ−1(κ(δ)2 t ) such that µ(B ∗ δ′) > 0. Such a δ\n′ always exists by Q-regularity of µ. Consider\nx̃(s) = x(s)1B∗δ (s) + µ(B∗δ′) 1B∗ δ′ (s)\nClearly, x̃ ∈ X . Furthermore, (∗) := ∫ S ηtUt(v)x̃(v)µ(dv)− hi(x̃)− ∫ S ηkUk(v)x(v)µ(dv) + hi(x)\n=\nµ(B∗δ′) ∫ B∗ δ′ ηkUk(v)µ(dv)− ∫ (B∗δ ) c ηtUk(v)x(v)µ(dv)− (hi(x̃)− hi(x))\n≥ ηk(U∗ − tχ(δ′))− ηt(U∗ − κ(δ))− (hi(x̃)− hi(x)) ≥ ηt(κ(δ)− tχ(δ′))− (hi(x̃)− hi(x))\n> ηt κ(δ)\n2 t − (hi(x̃)− hi(x))\nNow hi(x̃) − hi(x) → 0 as i → ∞ by consistency of (hi)i≥0. Hence there exists j < ∞ such that (∗) > 0 and thus x 6= x∗j for all i ≥ j. Since was arbitrary, this shows that ∫ (B∗δ ) c x ∗ i (s)µ(ds)→ 0 as i→∞."
    }, {
      "heading" : "PROOF OF COROLLARY 19",
      "text" : "Proof [Corollary 19] Let f : S → R be continuous and bounded, say |f(s)| ≤ M for all s ∈ S. Let > 0. Since S is compact, f is uniformly continuous, i.e. ∃ δ > 0 such that |f(s) − f(s∗)| < /2 for all s ∈ B∗δ . By Corollary 18 there exists j <∞ such that x∗i ((B∗δ )c) < 4M for all i > j. Hence∫\nS |f(s)− f(s∗)|x∗i (s)λ(ds) < /2 ∫ B∗δ x∗i (s)µ(ds) + 2M ∫ (B∗δ ) c x∗i (s)µ(ds) <\nfor all i > j."
    }, {
      "heading" : "PROOF OF PROPOSITION 22",
      "text" : "Proof [Proposition 22] This proof uses similar arguments as Theorem 7.2 in Cesa-Bianchi and Lugosi (2006), with modifications to accommodate our more general setting of functions on metric spaces.\nSince player 1 has sublinear (realized) regret, by (21) it suffices to show that\nsup s1∈S1\n1\nt t∑ τ=1 u(s1, s2τ ) ≥ V.\nNow clearly sups1∈S1 f(s 1) = supx1∈P1 ∫ Si f(s) dx1(s) for any f measurable, thus we may equivalently show that supx1∈P1 1 t ∑t τ=1 ∫ S1 u(s1, s2τ ) dx 1(s1) ≥ V . Observe that, for all x1 ∈ P1,\n1\nt t∑ τ=1 ∫ S1 u(s1, s2τ ) dx 1(s1) = ∫ S1 1 t t∑ τ=1 u(s1, s2τ ) dx 1(s1)\n= ∫ S1 1 t t∑ τ=1 (∫ S2 u(s1, s) dδs2τ (s) ) dx1(s1) = ū(x1, x̂2t )\nwhere x̂2t (B) := 1 t ∑t τ=1 1B(s 2 τ ) for any Borel set B ⊂ S2. Since x̂2t ∈ P2 we thus have that\nsup x1∈P1 ū(x1, x̂2t ) ≥ inf x2∈P2 sup x1∈P1 ū(x1, x2) = V"
    }, {
      "heading" : "PROOF OF COROLLARY 23",
      "text" : "Proof [Corollary 23] Using the fact that the payoff of player 2 is the negative of player 1, we have from Theorem 22 and the fact that the game has a value that\nlim inf t→∞\n1\nt t∑ τ=1 −u(s1τ , s2τ ) ≥ −V\nand thus\nlim sup t→∞\n1\nt t∑ τ=1 u(s1τ , s 2 τ ) ≤ V\nCombining this with (22) proves (23)."
    }, {
      "heading" : "PROOF OF THEOREM 24",
      "text" : "In the proof of the theorem we will use the following Lemma:\nLemma 39 The functions g1(x2) := supx1∈P1 ū(x 1, x2) and g2(x1) := infx2∈P2 ū(x 1, x2) are continuous with respect to the weak topology.\nProof [Lemma 39] It suffices to show that g−11 ((−∞, a)) and g−1((b,∞)) are open, since the sets of the form (−∞, a) and (b,∞) form a subbase for the topology of R. Observe first that u is continuous. Indeed, by Assumption 3, we have for any s, t ∈ S1 × S2 that\n|u(s1, s2)− u(t1, t2)| ≤ |u(s1, s2)− u(s1, t2)|+ |u(s1, t2)− u(t1, t2)| ≤ χ2(d2(s2, t2)) + χ1(d1(s1, t1))\nand so for any > 0 there exists δ > 0 such that |u(s1, s2) − u(t1, t2)| < whenever (d1 × d2)(s, t) < δ. Since u is continuous on the compact set S1 × S2 it is bounded, i.e. there exists M < ∞ such that |u(s1, s2)| ≤ M for all s ∈ S. This implies that ū(x1, x2) is 2M -Lipschitz w.r.t the Lévy-Prokhorov metric\nonP1×P2, hence in particular (jointly) continuous w.r.t. the weak (product) topology. Let π2 : P1×P2 → P2 denote the canonical projection onto P2, which by definition of the product topology is continuous. Together with the continuity of ū this implies that g−11 ((b,∞)) = π2 ◦ ū−1((b,∞)) is open. Furthermore, note that ū(x1, x2) < a, ∀x1 ∈ P1 whenever g1(x) < a, and hence for any x2 ∈ P2, the set (x1, x2) ∈ g−11 ((−∞, a)) is open. That is, there exists an open cover of P1 × {x2}. Now P1 is compact in the weak topology, which means we can find a finite subcover {U jx2} nx2 j=1 such that ⋂nx2 j=1 U j x2 ⊃ P1 × {x2}. Taking the union over all\nx2 ∈ g−1((−∞, a)) we have that g−1((−∞, a)) = ⋃ x2∈g−1((−∞,a)) ⋂nx2 j=1 U j x2 , which is an open set. This shows that g1 is continuous. The argument for showing continuity of g2 is essentially the same.\nProof [Theorem 24] Note that both Pi are metrizable and compact in the weak topology (as each Si is compact), and hence P1 × P2 by Tychonoff’s theorem. Therefore it suffices to show that with probability 1, the weak limit of any weakly converging subsequence of (x̂t)∞t=0 is a Nash equilibrium. Let (x̂ 1 θ, x̂ 2 θ) ∞ θ=1 be such weakly convergent subsequence, and (z1, z2) ∈ P1 × P2 its weak limit. We will show that whenever a given realization of plays (s1t ), (s 2 t ) has sublinear regret for both players, (z 1, z2) is a Nash Equilibrium, i.e.,\nsup x1∈P1 ū(x1, z2) = V = inf x2∈P2 ū(z1, x2). (41)\nLet g1(x2) := supx1∈P1 ū(x 1, x2) and g2(x1) := infx2∈P2 ū(x 1, x2), which by Lemma 39 are continuous w.r.t. the weak topology. Hence, using that x̂iθ ⇀ z i for i = 1, 2, (41) is equivalent to\nlim θ→∞ sup x1∈P1\nū(x1, x̂2θ) = V, (42a)\nlim θ→∞ inf x2∈P2\nū(x̂1θ, x 2) = V. (42b)\nWe first show (42a). By assumption, the game has value V , i.e. it holds that infx2∈P2 supx1∈P1 ū(x 1, x2) = V and thus, in particular, that\nlim inf θ→∞ sup x1∈P1\nū(x1, x̂2θ) ≥ V. (43)\nNow, suppose that for a realization (s1τ ), (s 2 τ ), the regret of the second player is sublinear, i.e.\nlim sup t→∞\n1\nt ( sup x1∈P1 t∑ τ=1 ∫ S1 u(s1, s2τ ) dx 1(s1)− n∑ τ=1 u(s1τ , s 2 τ ) ) ≤ 0.\nThen by Corollary 23, limt→∞ 1t ∑t τ=1 u(s 1 τ , s 2 τ ) = V , and we have\nV ≥ lim sup t→∞ sup x1∈P1\n1\nt t∑ τ=1 ∫ S1 u(s1, s2τ ) dx 1(s1)\n= lim sup t→∞ sup x1∈P1 ∫ S1 1 t t∑ τ=1 u(s1, s 2 τ ) dx 1(s1)\n= lim sup t→∞ sup x1∈P1\nū(x1, x̂2t )\n≥ lim sup θ→∞ sup x1∈P1 ū(x1, x̂2θ).\nCombining the last inequality with (43) proves (42a). The argument for (42b) is essentially the same, modulo some sign changes.\nThis proves that for any realization with sublinear regret for both players, all weak limit points of the sequence (x̂1t , x̂ 2 t ) lie in the set of Nash equilibria. But by definition of Hannan consistency, this happens with probability 1."
    }, {
      "heading" : "PROOF OF THEOREM 27",
      "text" : "Proof [Theorem 27] To start, note that for any p > 1 the space X as a closed subset of Lp(S, µ) is a complete metric space, hence Polish and thus there exists a Borel isomorphism between X and the Lebesgue measure on the unit interval. Consequently, to randomize its plays according to a sequence of probability measures in X , it suffices that player i has access to a sequence of i.i.d. random variables drawn from the uniform distribution on [0, 1]. Denote this sequence by Zi = (Zi1, Z i 2, . . . ).\nThe key observation is that if player −i plays a non-oblivious strategy, then the partial rewards will not be some a priori fixed sequence of reward functions, but will depend on the history of play. Indeed, since ũit( · ) = ∑t τ=1 ui( · , s−iτ ) and since s−iτ is itself some function of past plays si1, . . . , siτ−1, the partial reward functions ũit are measurable w.r.t. the σ field generated by (Z i 1, . . . , Z i t). Note that this implicitly assumes\nthat any randomization performed by player −i is independent of that of player i. Let Eit[X] := E [ X |\nZi1, . . . , Z i t−1 ] denote the conditional expectation of X given the past plays of player i. Then\nt∑ τ=1 ui(si, s−iτ )− t∑ τ=1 Eiτ [ ui(siτ , s −i τ ) ] ≤ t∑ τ=1 sup s−iτ Eiτ [ ui(s i, s−iτ )− ui(siτ , s−iτ ) ]\n= t∑ τ=1 sup ũiτ Eiτ [ ũiτ (s i)− ũiτ (siτ ) ]\n= sup ũi1,...,ũ i t t∑ τ=1 Eiτ [ ũiτ (s i)− ũiτ (siτ ) ]\n(44)\nwhere the last step uses the fact that siτ ∼ xiτ := Dh∗i ( ητ−1 ∑t−1 θ=1 ũ i θ ) , which depends on the sequence {siθ} τ−1 θ=1 only through the sequence {ũiθ} τ−1 θ=1 of observed partial loss functions.\nFrom Proposition 11 we have that\nRt = sup si∈Si sup ũi1,...,ũ i t t∑ τ=1 ũiτ (s i)− t∑ τ=1 〈ũτi , xiτ 〉 = sup si∈Si sup ũi1,...,ũ i t t∑ τ=1 Eiτ [ ũiτ (s i)− ũiτ (siτ ) ]\n(45)\nNow let W iτ = ũ i τ (s i τ )− 〈ũiτ , xiτ 〉 and observe that W iτ is a martingale. Indeed,\nE[W iτ |W iτ , . . . ,W τ−1i ] = E[W i τ | Zτi , . . . , Zτ−1i ] = 0 a.s.\nMoreover, since by assumption ui is continuous on the compact set S1 × S2, we have that ui is bounded and therefore |W iτ −W iτ−1| ≤M for some M <∞. Noting that W iτ = 0 it follows from the Azuma-Hoeffding inequality that, for every > 0, P(W iτ ≤ ) ≥ 1− exp(− 2 2τM2 ) and thus\nP (∑t\nτ=1W i τ ≤M\n√ 2t log(t/ ) ) ≥ 1− ∀ > 0\nNow ∑t τ=1W i τ = ∑t τ=1 ũ i τ (s i τ ) − ∑t τ=1〈ũiτ , xiτ 〉, and hence, using (45) and (44), we have for all t < ∞ that\nsup si∈Si\n1\nt ( t∑ τ=1 ui(s i, s−iτ )− t∑ τ=1 ui(s i τ , s −i τ ) ) ≤ Rt t +M √ 2 log(t/ ) t\nNowRt/t→ 0 by assumption, and √ log(t/ ) t → 0 for any > 0, which proves Hannan consistency."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "<lb>We study a general version of the adversarial online learning problem. We are given a decision<lb>set X in a reflexive Banach space X and a sequence of reward vectors in the dual space of X . At<lb>each iteration, we choose an action from X , based on the observed sequence of previous rewards.<lb>Our goal is to minimize regret, defined as the gap between the realized reward and the reward<lb>of the best fixed action in hindsight. Using results from infinite dimensional convex analysis, we<lb>generalize the method of Dual Averaging (or Follow the Regularized Leader) to our setting and<lb>obtain general upper bounds on the worst-case regret that subsume a wide range of results from the<lb>literature. Under the assumption of uniformly continuous rewards, we obtain explicit anytime regret<lb>bounds in a setting where the decision set is the set of probability distributions on a compact metric<lb>space S whose Radon-Nikodym derivatives are elements of L(S) for some p > 1. Importantly,<lb>we make no convexity assumptions on either the set S or the reward functions. We also prove<lb>a general lower bound on the worst-case regret for any online algorithm. We then apply these<lb>results to the problem of learning in repeated continuous two-player zero-sum games, in which<lb>players’ strategy sets are compact metric spaces. In doing so, we first prove that if both players<lb>play a Hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly<lb>converge to the set of Nash equilibria of the game. We then show that, under mild assumptions,<lb>Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves<lb>Hannan-consistency. Finally, we illustrate our results through numerical examples.<lb>",
    "creator" : "LaTeX with hyperref package"
  }
}