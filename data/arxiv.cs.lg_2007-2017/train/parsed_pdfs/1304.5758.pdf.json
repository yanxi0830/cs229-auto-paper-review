{
  "name" : "1304.5758.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Prior-free and prior-dependent regret bounds for Thompson Sampling",
    "authors" : [ "Sébastien Bubeck", "Che-Yu Liu" ],
    "emails" : [ "sbubeck@princeton.edu,", "cheliu@princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 4.\n57 58\nv2 [\nst at\n.M L\n√ nK. This result is unimprovable in the sense that there exists a prior distribution\nsuch that any algorithm has a Bayesian regret bounded from below by 120 √ nK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.\n1 Introduction\nIn this paper we are interested in the Bayesian multi-armed bandit problem which can be described as follows. Let π0 be a known distribution over some set Θ, and let θ be a random variable distributed according to π0. For i ∈ [K], let (Xi,s)s≥1 be identically distributed random variables taking values in [0, 1] and which are independent conditionally on θ. Denote µi(θ) := E(Xi,1|θ). Consider now an agent facing K actions (or arms). At each time step t = 1, . . . n, the agent pulls an arm It ∈ [K]. The agent receives the reward Xi,s when he pulls arm i for the sth time. The arm selection is based only on past observed rewards and potentially on an external source of randomness. More formally, let (Us)s≥1 be an i.i.d. sequence of random variables uniformly distributed on [0, 1], and let Ti(s) = ∑s t=1 1It=i, then It is a random variable measurable with respect to σ(I1, X1,1, . . . , It−1, XIt−1,TIt−1(t−1), Ut). We measure the performance of the agent through the\nBayesian regret defined as\nBRn = E n∑\nt=1\n( max i∈[K] µi(θ)− µIt(θ) ) ,\nwhere the expectation is taken with respect to the parameter θ, the rewards (Xi,s)s≥1, and the external source of randomness (Us)s≥1. We will also be interested in the individual regret Rn(θ) which is defined similarly except that θ is fixed (instead of being integrated over π0). When it is clear from the context we drop the dependency on θ in the various quantities defined above.\nGiven a prior π0 the problem of finding an optimal strategy to minimize the Bayesian regret BRn is a well defined optimization problem and as such it is merely a computational problem. On the other hand the point of view initially developed in Robbins [1952] leads to a learning problem. In this latter view the agent’s strategy must have a low regret Rn(θ) for any θ ∈ Θ. Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute). The product assumption on the prior means that the reward processes (Xi,s)s≥1 are independent across arms. In the present paper we are precisely interested in the situations where this assumption is not satisfied. Indeed we believe that one of the strength of the Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way. A prototypical example that we shall consider later on in this paper is when one knows the distributions of the arms up to a permutation, in which case the reward processes are strongly dependent.\nIn general without the product assumption on the prior it seems hopeless (from a computational perspective) to look for the optimal Bayesian strategy. Thus, despite being in a Bayesian setting, it makes sense to view it as a learning problem and to evaluate the agent’s performance through its Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling strategy which was proposed in the very first paper on the multi-armed bandit problem Thompson [1933]. This strategy can be described very succinctly: let πt be the posterior distribution on θ given the history Ht = (I1, X1,1, . . . , It−1, XIt−1,TIt−1 (t−1)) of the algorithm up to the beginning of round t. Then Thompson Sampling first draws a parameter θ(t) from πt (independently from the past given πt) and it pulls It ∈ argmaxi∈[K] µi(θ(t)).\nRecently there has been a surge of interest in this simple policy, mainly because of its flexibility to incorporate prior knowledge on the arms, see for example Chapelle and Li [2011]. For a long time the theoretical properties of Thompson Sampling remained elusive. The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b]. However as we pointed out above here we are interested in proving regret bounds for the more realistic scenario where one runs Thompson Sampling with a hand-tuned prior distribution, possibly very different from a Beta prior. The first result in this spirit was obtained very recently by Russo and Roy [2013] who showed that for any prior distribution π0 Thompson Sampling always satisfies BRn ≤ 5 √ nK logn. A similar\nbound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009].\nOur second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where µ∗ and ε > 0 are known values such that for any θ ∈ Θ, first there is a unique best arm {i∗(θ)} = argmaxi∈[K] µi(θ), and furthermore\nµi∗(θ)(θ) = µ ∗, and ∆i(θ) := µi∗(θ)(θ)− µi(θ) ≥ ε, ∀i 6= i∗(θ).\nIn other words the value of the best arm is known as well as a non-trivial lower bound on the gap between the values of the best and second best arms. For this problem a new algorithm was proposed in Bubeck et al. [2013] (which we call the BPR policy), and it was shown that the BPR policy satisfies\nRn(θ) = O\n  ∑\ni 6=i∗(θ)\nlog(∆i(θ)/ε)\n∆i(θ) log log(1/ε)\n  , ∀θ ∈ Θ, ∀n ≥ 1.\nThus the BPR policy attains a regret uniformly bounded over time in the BPR setting, a feature that standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve. It is natural to view the assumptions of the BPR setting as a prior over the reward distributions and to ask what regret guarantees attains Thompson Sampling in that situation. More precisely we consider Thompson Sampling with Gaussian reward distributions and uniform prior over the possible range of parameters. We then prove individual regret bounds for any sub-Gaussian distributions (similarly to Bubeck et al. [2013]). We obtain that Thompson Sampling uses optimally the prior information in the sense that it also attains uniformly bounded over time regret. Furthermore as an added bonus we remove the extraneous log-log factor of the BPR policy’s regret bound.\nThe results presented in Section 3 and 4 can be viewed as a first step towards a better understanding of prior-dependent regret bounds for Thompson Sampling. Generalizing these results to arbitrary priors is a challenging open problem which is beyond the scope of our current techniques.\n2 Optimal prior-free regret bound for Thompson Sampling\nIn this section we prove the following result.\nTheorem 1 For any prior distribution π0 over reward distributions in [0, 1], Thompson Sampling satisfies\nBRn ≤ 14 √ nK.\nRemark that the above result is unimprovable in the sense that there exist prior distributions π0 such that for any algorithm one has Rn ≥ 120 √ nK (see e.g. [Theorem 3.5, Bubeck and Cesa-Bianchi\n1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(θ) while the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.\n[2012]]). This theorem also implies an optimal rate of identification for the best arm, see Bubeck et al. [2009] for more details on this.\nProof We decompose the proof into three steps. We denote i∗(θ) ∈ argmaxi∈[K] µi(θ), in particular one has It = i∗(θ(t)).\nStep 1: rewriting of the Bayesian regret in terms of upper confidence bounds. This step is given by [Proposition 1, Russo and Roy [2013]] which we reprove for the sake of completeness. Let Bi,t be a random variable measurable with respect to σ(Ht). Note that by definition θ(t) and θ are identically distributed conditionally on Ht. This implies by the tower rule:\nEBi∗(θ),t = EBi∗(θ(t)),t = EBIt,t.\nThus we obtain:\nE ( µi∗(θ)(θ)− µIt(θ) ) = E ( µi∗(θ)(θ)− Bi∗(θ),t ) + E (BIt,t − µIt(θ)) .\nInspired by the MOSS strategy of Audibert and Bubeck [2009] we will now take\nBi,t = µ̂i,Ti(t−1) +\n√√√√ log+ ( n KTi(t−1) )\nTi(t− 1) ,\nwhere µ̂i,s = 1s ∑s t=1Xi,t, and log+(x) = log(x)1x≥1. In the following we denote δ0 = 2 √ K n\n. From now on we work conditionally on θ and thus we drop all the dependency on θ.\nStep 2: control of E ( µi∗(θ)(θ)−Bi∗(θ),t|θ ) . By a simple integration of the deviations one has\nE (µi∗ −Bi∗,t) ≤ δ0 + ∫ 1\nδ0\nP(µi∗ −Bi∗,t ≥ u)du.\nNext we extract the following inequality from Audibert and Bubeck [2010] (see p2683–2684), for any i ∈ [K],\nP(µi −Bi,t ≥ u) ≤ 4K\nnu2 log\n(√ n\nK u\n) +\n1\nnu2/K − 1 .\nNow an elementary integration gives\n∫ 1\nδ0\n4K nu2 log\n(√ n\nK u\n) du = [ −4K\nnu log\n( e √ n\nK u\n)]1\nδ0\n≤ 4K nδ0 log\n( e √ n\nK δ0\n) = 2(1+log 2) √ K\nn ,\nand\n∫ 1\nδ0\n1 nu2/K − 1du = [ −1 2 √ K n log (√ n K u+ 1√\nn K u− 1\n)]1\nδ0\n≤ 1 2\n√ K\nn log\n(√ n K δ0 + 1√\nn K δ0 − 1\n) = log 3\n2\n√ K\nn .\nThus we proved: E ( µi∗(θ)(θ)− Bi∗(θ),t|θ ) ≤ ( 2 + 2(1 + log 2) + log 3\n2 )√ K n ≤ 6 √ K n .\nStep 3: control of ∑n\nt=1 E (BIt,t − µIt(θ)|θ). We start again by integrating the deviations:\nE\nn∑\nt=1\n(BIt,t − µIt) ≤ δ0n+ ∫ +∞\nδ0\nn∑\nt=1\nP(BIt,t − µIt ≥ u)du.\nNext we use the following simple inequality:\nn∑\nt=1\n1{BIt,t − µIt ≥ u} ≤ n∑\ns=1\nK∑\ni=1\n1   µ̂i,s + √ log+ ( n Ks ) s − µi ≥ u    ,\nwhich implies\nn∑\nt=1\nP(BIt,t − µIt ≥ u) ≤ K∑\ni=1\nn∑\ns=1\nP  µ̂i,s + √ log+ ( n Ks )\ns − µi ≥ u\n  .\nNow for u ≥ δ0 let s(u) = ⌈3 log ( nu2\nK\n) /u2⌉ where ⌈x⌉ is the smallest integer large than x. Let\nc = 1− 1√ 3 . It is easy to see that one has:\nn∑\ns=1\nP  µ̂i,s + √ log+ ( n Ks )\ns − µi ≥ u\n  ≤ 3 log ( nu2 K )\nu2 +\nn∑\ns=s(u)\nP (µ̂i,s − µi ≥ cu) .\nUsing an integration already done in Step 2 we have\n∫ +∞\nδ0\n3 log ( nu2\nK\n)\nu2 ≤ 3(1 + log(2))\n√ n\nK ≤ 5.1\n√ n\nK .\nNext using Hoeffding’s inequality and the fact that the rewards are in [0, 1] one has for u ≥ δ0 n∑\ns=s(u)\nP (µ̂i,s − µi ≥ cu) ≤ n∑\ns=s(u)\nexp(−2sc2u2)1u≤1/c ≤ exp(−12c2 log 2) 1− exp(−2c2u2)1u≤1/c.\nNow using that 1− exp(−x) ≥ x− x2/2 for x ≥ 0 one obtains ∫ 1/c\nδ0\n1 1− exp(−2c2u2)du = ∫ 1/(2c)\nδ0\n1 1− exp(−2c2u2)du+ ∫ 1/c\n1/(2c)\n1\n1− exp(−2c2u2)du\n≤ ∫ 1/(2c)\nδ0\n1 2c2u2 − 2c4u4du+ 1 2c(1− exp(−1/2))\n≤ ∫ 1/(2c)\nδ0\n2\n3c2u2 du+\n1\n2c(1− exp(−1/2))\n= 2 3c2δ0 − 4 3c +\n1\n2c(1− exp(−1/2))\n≤ 1.9 √ n\nK .\nPutting the pieces together we proved\nE\nn∑\nt=1\n(BIt,t − µIt) ≤ 7.6 √ nK,\nwhich concludes the proof together with the results of Step 1 and Step 2.\n3 Thompson Sampling in the two-armed BPR setting\nFollowing [Section 2, Bubeck et al. [2013]] we consider here the two-armed bandit problem with sub-Gaussian reward distributions (that is they satisfy Eeλ(X−µ) ≤ eλ2/2 for all λ ∈ R) and such that one reward distribution has mean µ∗ and the other one has mean µ∗ −∆ where µ∗ and ∆ are known values.\nIn order to derive the Thompson Sampling strategy for this problem we further assume that the reward distributions are in fact Gaussian with variance 1. In other words let Θ = {θ1, θ2}, π0(θ1) = π0(θ2) = 1/2, and under θ1 one has X1,s ∼ N (µ∗, 1) and X2,s ∼ N (µ∗ − ∆, 1) while under θ2 one has X2,s ∼ N (µ∗, 1) and X1,s ∼ N (µ∗ −∆, 1). Then a straightforward computation (using Bayes rule and induction) shows that one has for some normalizing constant c > 0:\nπt(θ1) = c exp\n −1\n2\nT1(t−1)∑\ns=1\n(µ∗ −X1,s)2 − 1\n2\nT2(t−1)∑\ns=1\n(µ∗ −∆−X2,s)2   ,\nπt(θ2) = c exp\n −1\n2\nT1(t−1)∑\ns=1\n(µ∗ −∆−X1,s)2 − 1\n2\nT2(t−1)∑\ns=1\n(µ∗ −X2,s)2   .\nRecall that Thompson Sampling draws θ(t) from πt and then pulls the best arm for the environment θ(t). Observe that under θ1 the best arm is arm 1 and under θ2 the best arm is arm 2. In other words Thompson Sampling draws It at random with the probabilities given by the posterior πt. This leads to a general algorithm for the two-armed BPR setting with sub-Gaussian reward distributions that we summarize in Figure 1. The next result shows that it attains optimal performances in this setting up to a numerical constant (see Bubeck et al. [2013] for lower bounds), for any subGaussian reward distribution (not necessarily Gaussian) with largest mean µ∗ and gap ∆.\nTheorem 2 The policy of Figure 1 has regret bounded as Rn ≤ ∆+ 578∆ , uniformly in n.\nNote that we did not try to optimize the numerical constant in the above bound. Figure 2 shows an empirical comparison of the policy of Figure 1 with Policy 1 of Bubeck et al. [2013]. Note in particular that a regret bound of order 16/∆ was proved for the latter algorithm and the (limited) numerical simulation presented here suggests that Thompson Sampling outperforms this strategy. Proof Without loss of generality we assume that arm 1 is the optimal arm, that is µ1 = µ∗ and µ2 = µ ∗ − ∆. Let µ̂i,s = 1s ∑s\nt=1Xi,t, γ̂1,s = µ1 − µ̂1,s and γ̂2,s = µ̂2,s − µ2. Note that large (positive) values of γ̂1,s or γ̂2,s might mislead the algorithm into bad decisions, and we will need\nto control what happens in various regimes for these γ coefficients. We decompose the proof into three steps.\nStep 1. This first step will be useful in the rest of the analysis, it shows how the probability ratio of a bad pull over a good pull evolves as a function of the γ coefficients introduced above. One has:\npt(2) pt(1) = exp\n −1\n2\nT1(t−1)∑\ns=1\n[ (µ2 −X1,s)2 − (µ1 −X1,s)2 ] − 1\n2\nT2(t−1)∑\ns=1\n[ (µ1 −X2,s)2 − (µ2 −X2,s)2 ] \n= exp ( −T1(t− 1)\n2\n[ µ22 − µ21 − 2(µ2 − µ1)µ̂1,T1(t−1) ] − T2(t− 1)\n2\n[ µ21 − µ22 − 2(µ1 − µ2)µ̂2,T2(t−1) ])\n= exp ( −T1(t− 1)\n2\n[ ∆2 − 2∆(µ1 − µ̂1,T1(t−1)) ] − T2(t− 1)\n2\n[ ∆2 − 2∆(µ̂2,T2(t−1) − µ2) ])\n= exp ( −t∆ 2\n2 + T1(t− 1)∆γ̂1,T1(t−1) + T2(t− 1)∆γ̂2,T2(t−1)\n) .\nStep 2. We decompose the regret Rn as follows:\nRn ∆ = 1 + E\nn∑\nt=3\n1{It = 2}\n= 1 + E n∑\nt=3\n1 { γ̂2,T2(t−1) > ∆\n4 , It = 2\n} + E n∑\nt=3\n1 { γ̂2,T2(t−1) ≤ ∆\n4 , γ̂1,T1(t−1) ≤\n∆ 4 , It = 2\n}\n+E n∑\nt=3\n1 { γ̂2,T2(t−1) ≤ ∆\n4 , γ̂1,T1(t−1) >\n∆ 4 , It = 2\n} .\nµ* = 0, ∆ = 0.2\nµ* = 0, ∆ = 0.05\nWe use Hoeffding’s inequality to control the first term:\nE\nn∑\nt=3\n1 { γ̂2,T2(t−1) > ∆\n4 , It = 2\n} ≤ E n∑\ns=1\n1 { γ̂2,s > ∆\n4\n} ≤ n∑\ns=1\nexp ( −s∆ 2\n32\n) ≤ 32\n∆2 .\nFor the second term, using the rewriting of Step 1 as an upper bound on pt(2), one obtains:\nE\nn∑\nt=3\n1 { γ̂2,T2(t−1) ≤ ∆\n4 , γ̂1,T1(t−1) ≤\n∆ 4 , It = 2\n} = n∑\nt=3\nE ( pt(2)1 { γ̂2,T2(t−1) ≤ ∆\n4 , γ̂1,T1(t−1) ≤\n∆\n4\n})\n≤ n∑\nt=3\nexp ( −t∆ 2\n4\n) ≤ 4\n∆2 .\nThe third term is more difficult to control, and we further decompose the corresponding event as follows:\n{ γ̂2,T2(t−1) ≤ ∆\n4 , γ̂1,T1(t−1) >\n∆ 4 , It = 2\n}\n⊂ { γ̂1,T1(t−1) > ∆\n4 , T1(t− 1) > t/4\n} ∪ { γ̂2,T2(t−1) ≤ ∆\n4 , It = 2, T1(t− 1) ≤ t/4\n} .\nThe cumulative probability of the first event in the above decomposition is easy to control thanks to Hoeffding’s maximal inequality2 which states that for any m ≥ 1 and x > 0 one has\nP(∃ 1 ≤ s ≤ m s.t. s γ̂1,s ≥ x) ≤ exp ( − x 2\n2m\n) .\n2It is an easy exercise to verify that Azuma-Hoeffding holds for martingale differences with sub-Gaussian increments, which implies Hoeffding’s maximal inequality for sub-Gaussian distributions.\nIndeed this implies\nP ( γ̂1,T1(t−1) > ∆\n4 , T1(t− 1) > t/4\n) ≤ P ( ∃ 1 ≤ s ≤ t s.t. s γ̂1,s > ∆t\n16\n) ≤ exp ( −t∆ 2\n512\n) ,\nand thus\nE\nn∑\nt=3\n1 { γ̂1,T1(t−1) > ∆\n4 , T1(t− 1) > t/4\n} ≤ 512\n∆2 .\nIt only remains to control the term\nE\nn∑\nt=3\n1 { γ̂2,T2(t−1) ≤ ∆\n4 , It = 2, T1(t− 1) ≤ t/4\n} = n∑\nt=3\nE ( pt(2)1 { γ̂2,T2(t−1) ≤ ∆\n4 , T1(t− 1) ≤ t/4\n})\n≤ n∑\nt=3\nE exp ( −t∆ 2\n4 + ∆ max 1≤s≤t/4 sγ̂1,s\n) ,\nwhere the last inequality follows from Step 1. The last step is devoted to bounding from above this last term.\nStep 3. By integrating the deviations and using again Hoeffding’s maximal inequality one obtains\nE exp ( ∆ max\n1≤s≤t/4 sγ̂1,s\n) ≤ 1+ ∫ +∞\n1\nP ( max 1≤s≤ t\n4\nsγ̂1,s ≥ log x\n∆\n) dx ≤ 1+ ∫ +∞\n1\nexp ( −2(log x) 2\n∆2t\n) dx.\nNow, straightforward computation gives n∑\nt=3\nexp ( −t∆ 2\n4\n)( 1 + ∫ +∞\n1\nexp ( −2(log x) 2\n∆2t\n) dx ) ≤ n∑\nt=3\nexp ( −t∆ 2\n4\n)( 1 + √ π∆2t\n2 exp\n( t∆2\n8\n))\n≤ 4 ∆2 +\n∫ +∞\n0\n√ π∆2t\n2 exp\n( −t∆ 2\n8\n) dt\n≤ 4 ∆2\n+ 16 √ π\n∆2\n∫ +∞\n0\n√ u exp(−u) du\n≤ 30 ∆2 .\nwhich concludes the proof by putting this together with the results of the previous step.\n4 Optimal strategy for the BPR setting inspired by Thompson Sampling\nIn this section we consider the general BPR setting. That is the reward distributions are subGaussian (they satisfy Eeλ(X−µ) ≤ eλ2/2 for all λ ∈ R), one reward distribution has mean µ∗, and all the other means are smaller than µ∗ − ε where µ∗ and ε are known values.\nSimilarly to the previous section we assume that the reward distributions are Gaussian with variance 1 for the derivation of the Thompson Sampling strategy (but we do not make this assumption for the analysis of the resulting algorithm). Then the set of possible parameters is described as follows:\nΘ = ∪Ki=1Θi where Θi = {θ ∈ RK s.t. θi = µ∗ and θj ≤ µ∗ − ε for all j 6= i}.\nAssuming a uniform prior over the index of the best arm, and a prior λ over the mean of a suboptimal arm one obtains by Bayes rule that the probability density function of the posterior is given by:\ndπt(θ) ∝ exp  −1\n2\nK∑\nj=1\nTj(t−1)∑\ns=1\n(Xj,s − θj)2  \nK∏\nj=1,j 6=i∗(θ) dλ(θj).\nNow remark that with Thompson Sampling arm i is played at time t if and only if θ(t) ∈ Θi. In other words It is played at random from probability pt where\npt(i) = πt(Θi) ∝ exp  −1\n2\nTi(t−1)∑\ns=1\n(Xi,s − µ∗)2  ∏\nj 6=i\n  ∫ µ∗−ε\n−∞ exp\n −1\n2\nTj(t−1)∑\ns=1\n(Xj,s − v)2   dλ(v)  \n∝ exp\n( −1\n2 ∑Ti(t−1) s=1 (Xi,s − µ∗)2\n)\n∫ µ∗−ε −∞ exp ( −1 2 ∑Ti(t−1) s=1 (Xi,s − v)2 ) dλ(v) .\nTaking inspiration from the above calculation we consider the following policy, where λ is the Lebesgue measure and we assume a slightly larger value for the variance (this is necessary for the proof).\nThe following theorem shows that this policy attains the best known performance for the BPR setting, shaving off a log-log term in the regret bound of the BPR policy.\nTheorem 3 The policy of Figure 3 has regret bounded as Rn ≤ ∑\ni:∆i>0\n( ∆i +\n80+log(∆i/ε) ∆i\n) ,\nuniformly in n.\nProof The general structure of the proof is superficially similar to the proof of Theorem 2 but many details are different. Without loss of generality we assume that arm 1 is the optimal arm, that is µ1 = µ∗ and ∀i ≥ 2, µi = µ∗ − ∆i. Let γ̂1,s = µ1 − µ̂1,s and γ̂i,s = µ̂i,s − µi for i ≥ 2. We decompose the proof into four steps.\nStep 1: Rewriting of the ratio pi,t p1,t . Let i ≥ 2, the following rewriting will be useful in the rest of the proof:\npt(i) pt(1) =\n∫ µ1−ε −∞ exp ( −1 3 ∑T1(t−1) s=1 (X1,s − v)2 − (X1,s − µ1)2 ) dv ∫ µ1−ε −∞ exp ( −1 3 ∑Ti(t−1) s=1 (Xi,s − v)2 − (Xi,s − µ1)2 ) dv\n=\n∫ µ1−ε −∞ exp ( −T1(t−1) 3 (µ̂1,T1(t−1) − v)2 − (µ̂1,T1(t−1) − µ1)2 ) dv\n∫ µ1−ε −∞ exp ( −Ti(t−1) 3 (µ̂i,Ti(t−1) − v)2 − (µ̂i,Ti(t−1) − µ1)2 ) dv\n=\n∫ +∞ −γ̂1,T1(t−1)+ε exp ( −T1(t−1) 3 (v2 − γ̂21,T1(t−1)) ) dv\n∫ +∞ γ̂i,Ti(t−1)−∆i+ε exp ( −Ti(t−1) 3 (v2 − (γ̂i,Ti(t−1) −∆i)2) ) dv ,\nwhere the last step follows by a simple change of variable.\nStep 2: Decomposition of Rn. For i ≥ 2. Let Ai = ⌈ 6∆2i log( e6∆i ε )⌉ where ⌈x⌉ is the smallest integer larger than x. We decompose the regret Rn as follows.\nRn =\nK∑\ni=2\n( ∆i +∆iE n∑\nt=K+1\n1{It = i} )\n≤ K∑\ni=2\n∆i ( Ai + E n∑\nt=K+1\n1{Ti(t− 1) ≥ Ai, It = i} )\n= K∑\ni=2\n∆i ( Ai + E n∑\nt=K+1\n1 { γ̂i,Ti(t−1) >\n∆i 4 , Ti(t− 1) ≥ Ai, It = i\n}\n+ E n∑\nt=K+1\n1 { γ̂i,Ti(t−1) ≤\n∆i 4 , Ti(t− 1) ≥ Ai, It = i\n}) .\nThe first expectation can be bounded by using Hoeffding’s inequality.\nE\nn∑\nt=K+1\n1 { γ̂i,Ti(t−1) >\n∆i 4 , Ti(t− 1) ≥ Ai, It = i\n} ≤ E n∑\ns=1\n1 { γ̂i,s >\n∆i 4\n} ≤ n∑\ns=1\nexp ( −s∆ 2 i\n32\n) ≤ 32\n∆2i .\nThe second expectation is more difficult to bound from above and the next two steps are dedicated to this task.\nStep 3: Analysis of ∑n t=K+1 E 1 { γ̂i,Ti(t−1) ≤ ∆i4 , Ti(t− 1) ≥ Ai, It = i } . Clearly by definition of the policy one has\nn∑\nt=K+1\nE 1 { γ̂i,Ti(t−1) ≤\n∆i 4 , Ti(t− 1) ≥ Ai, It = i\n} = n∑\nt=K+1\nE\n[ pt(i)\npt(1) 1\n{ γ̂i,Ti(t−1) ≤\n∆i 4 , Ti(t− 1) ≥ Ai, It = 1\nWe have now to control the term pt(i) pt(1) on the event {γ̂i,Ti(t−1) ≤ ∆i4 , Ti(t−1) ≥ Ai}. The following bounds on the tail of the standard Gaussian distribution will be useful, for any x > 0 one has\n1 x e− 1 2 x2 ≥\n∫ +∞\nx\ne− 1 2 v2 dv ≥ 1\nx\n( 1− 1\nx2\n) e− 1 2 x2 .\nNow one has ∫ +∞\nγ̂i,Ti(t−1)−∆i+ε e− 1 3 Ti(t−1)(v2−(γ̂i,Ti(t−1)−∆i) 2) dv\n= e 1 3 Ti(t−1)(γ̂i,Ti(t−1)−∆i) 2\n∫ +∞\nγ̂i,Ti(t−1)−∆i+ε e− 1 3 Ti(t−1)v2 dv\n≥ e 316Ti(t−1)∆2i ∫ +∞\n∆i 4\ne− 1 3 Ti(t−1)v2 dv\n= e 3 16 Ti(t−1)∆2i · √\n3 2Ti(t− 1) · ∫ +∞\n∆i 4\n√ 2Ti(t−1)\n3\ne− 1 2 v2 dv\n≥ e 316Ti(t−1)∆2i · 6 ∆iTi(t− 1)\n( 1− 24\n∆2iTi(t− 1)\n) e− 1 48 Ti(t−1)∆2i\n≥ e 16Ti(t−1)∆2i · 2 ∆iTi(t− 1) ,\nwhere the last step follows from\nTi(t− 1) ≥ Ai ≥ 6\n∆2i log ( e6∆i ε ) ≥ 36 ∆2i .\nNext, using the fact that the function x → 1 x e 1 6 x∆2i is increasing on [ 6 ∆2i ,+∞), we get (∫ +∞\nγ̂i,Ti(t−1)−∆i+ε e− 1 3 Ti(t−1)(v2−(γ̂i,Ti(t−1)−∆) 2) dv\n)−1 ≤ ( e 1 6 Ti(t−1)∆2 · 2\n∆iTi(t− 1)\n)−1\n≤ e− 1 6 ∆2i\n( 6\n∆2 i\nlog\n( e6∆i\nε )) ∆i 2 6 ∆2i log ( e6∆ ε )\n= 3 e6 ε ∆2i log ( e6∆i ε ) .\nPlugging into the expression of pt(i) pt(1) , we obtain\nn∑\nt=K+1\nE 1{γ̂i,Ti(t−1)≤ ∆i 4 ,Ti(t−1)≥Ai,It=i}\n=\nn∑\nt=K+1\nE [ pi,t p1,t 1{γ̂i,Ti(t−1)≤ ∆i 4 ,Ti(t−1)≥Ai,It=1} ]\n≤ ( n∑\nt=K+1\nE\n[∫ +∞\n−γ̂1,T1(t−1)+ε e − 1 3 T1(t−1)(v2−γ̂21,T1(t−1)) dv1{It=1}\n]) 3\ne6 ε ∆2i log ( e6∆i ε ) .\n≤ ( +∞∑\nt=1\nE\n[∫ +∞\n−γ̂1,t+ε e− 1 3 t(v2−γ̂21,t) dv\n]) 3\ne6 ε ∆2i log ( e6∆i ε ) .\nStep 4: Control of ∑+∞\nt=1 E [∫ +∞ −γ̂1,t+ε e − 1 3 t(v2−γ̂21,t) dv ] .\nFirst, observe that\n+∞∑\nt=1\nE\n[∫ +∞\n−γ̂1,t+ε e− 1 3 t(v2−γ̂21,t) dv\n]\n≤ +∞∑\nt=1\nE\n[∫ +∞\n−γ̂1,t+ε e− 1 3 tv2 dv · e 13 tγ̂21,t1{|γ̂1,t|≤ ε3}\n] + +∞∑\nt=1\nE\n[∫ +∞\n−γ̂1,t+ε e− 1 3 tv2 dv · e 13 tγ̂21,t1{|γ̂1,t|≥ ε3}\n]\n≤ +∞∑\nt=1 ∫ +∞ 2 3 ε e− 1 3 tv2 dv · e 127 tε2 + +∞∑ t=1 ∫ +∞ −∞ e− 1 3 tv2 dv · E [ e 1 3 tγ̂21,t1{|γ̂1,t|≥ ε3} ] .\nThe first term is straightforward to compute:\n+∞∑\nt=1 ∫ +∞ 2 3 ε e− 1 3 tv2 dv · e 127 tε2 = ∫ +∞ 2 3 ε +∞∑ t=1 e− 1 3 t(v2− 1 9 ε2) dv\n≤ ∫ +∞\n2 3 ε\n3\nv2 − 1 9 ε2\ndv ≤ 9 log 3 2ε\nFor the second term, we first integrate the deviations and we use Hoeffding’s inequality to obtain\nE [ e\n1 3 tγ̂21,t1{|γ̂1,t|≥ ε3} ] ≤ e 13 t( ε3 )2P(|γ̂1,t| ≥ ε\n3 ) +\n∫ +∞\ne 1 3 t( ε 3 )\n2 P(e\n1 3 tγ̂21,t ≥ x) dx\n≤ 2e− 154 tε2 + ∫ +∞\ne 1 27 tε\n2 P\n( |γ̂1,t| ≥ √ 3 log x\nt\n) dx\n≤ 2e− 154 tε2 + 2 ∫ +∞\ne 1 27 tε\n2 e− 3 2 log x dx\n≤ 6e− 154 tε2 ,\nwhich yields\n+∞∑\nt=1\n∫ +∞\n−∞ e− 1 3 tv2 dv · E\n[ e\n1 3 tγ̂21,t1{|γ̂1,t|≥ ε3}\n] ≤\n∫ +∞\n−∞ 6\n+∞∑\nt=1\ne− 1 3 t(v2+ 1 18 ε2) dv\n≤ ∫ +∞\n−∞ 18\n1\nv2 + 1 18 ε2\ndv = 54 √ 2π\nε .\nPutting together all the steps finishes the proof.\nReferences\nS. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012a.\nS. Agrawal and N. Goyal. Further optimal regret bounds for thompson sampling, 2012b. arXiv:1209.3353.\nJ.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT), 2009.\nJ.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. Journal of Machine Learning Research, 11:2635–2686, 2010.\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning Journal, 47(2-3):235–256, 2002.\nS. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.\nS. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.\nS. Bubeck, V. Perchet, and P. Rigollet. Bounded regret in stochastic multi-armed bandits. In Proceedings of the 26th Annual Conference on Learning Theory (COLT), 2013.\nO. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Advances in Neural Information Processing Systems (NIPS), 2011.\nJ.C. Gittins. Bandit processes and dynamic allocation indices. Journal Royal Statistical Society Series B, 14:148–167, 1979.\nE. Kaufmann, N. Korda, and R. Munos. Thompson sampling: an asymptotically optimal finitetime analysis. In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT), 2012.\nH. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematics Society, 58:527–535, 1952.\nD. Russo and B. Van Roy. Learning to optimize via posterior sampling, 2013. arXiv:1301.2609.\nW. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Bulletin of the American Mathematics Society, 25:285–294, 1933."
    } ],
    "references" : [ {
      "title" : "Analysis of Thompson sampling for the multi-armed bandit problem",
      "author" : [ "S. Agrawal", "N. Goyal" ],
      "venue" : "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Agrawal and Goyal.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agrawal and Goyal.",
      "year" : 2012
    }, {
      "title" : "Further optimal regret bounds for thompson sampling, 2012b. arXiv:1209.3353",
      "author" : [ "S. Agrawal", "N. Goyal" ],
      "venue" : null,
      "citeRegEx" : "Agrawal and Goyal.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agrawal and Goyal.",
      "year" : 2012
    }, {
      "title" : "Minimax policies for adversarial and stochastic bandits",
      "author" : [ "J.-Y. Audibert", "S. Bubeck" ],
      "venue" : "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Audibert and Bubeck.,? \\Q2009\\E",
      "shortCiteRegEx" : "Audibert and Bubeck.",
      "year" : 2009
    }, {
      "title" : "Regret bounds and minimax policies under partial monitoring",
      "author" : [ "J.-Y. Audibert", "S. Bubeck" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Audibert and Bubeck.,? \\Q2010\\E",
      "shortCiteRegEx" : "Audibert and Bubeck.",
      "year" : 2010
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning Journal,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "Bounded regret in stochastic multi-armed bandits",
      "author" : [ "S. Bubeck", "V. Perchet", "P. Rigollet" ],
      "venue" : "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2013
    }, {
      "title" : "An empirical evaluation of Thompson sampling",
      "author" : [ "O. Chapelle", "L. Li" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Chapelle and Li.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle and Li.",
      "year" : 2011
    }, {
      "title" : "Bandit processes and dynamic allocation indices",
      "author" : [ "J.C. Gittins" ],
      "venue" : "Journal Royal Statistical Society Series B,",
      "citeRegEx" : "Gittins.,? \\Q1979\\E",
      "shortCiteRegEx" : "Gittins.",
      "year" : 1979
    }, {
      "title" : "Thompson sampling: an asymptotically optimal finitetime analysis",
      "author" : [ "E. Kaufmann", "N. Korda", "R. Munos" ],
      "venue" : "In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2012
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "Bulletin of the American Mathematics Society,",
      "citeRegEx" : "Robbins.,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins.",
      "year" : 1952
    }, {
      "title" : "Learning to optimize via posterior sampling",
      "author" : [ "D. Russo", "B. Van Roy" ],
      "venue" : null,
      "citeRegEx" : "Russo and Roy.,? \\Q2013\\E",
      "shortCiteRegEx" : "Russo and Roy.",
      "year" : 2013
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W. Thompson" ],
      "venue" : "Bulletin of the American Mathematics Society,",
      "citeRegEx" : "Thompson.,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson.",
      "year" : 1933
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 √ nK.",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 √ nK.",
      "startOffset" : 30,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 √ nK. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by 1 20 √ nK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.",
      "startOffset" : 30,
      "endOffset" : 486
    }, {
      "referenceID" : 5,
      "context" : "On the other hand the point of view initially developed in Robbins [1952] leads to a learning problem.",
      "startOffset" : 59,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting.",
      "startOffset" : 91,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute).",
      "startOffset" : 91,
      "endOffset" : 271
    }, {
      "referenceID" : 3,
      "context" : "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute). The product assumption on the prior means that the reward processes (Xi,s)s≥1 are independent across arms. In the present paper we are precisely interested in the situations where this assumption is not satisfied. Indeed we believe that one of the strength of the Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way. A prototypical example that we shall consider later on in this paper is when one knows the distributions of the arms up to a permutation, in which case the reward processes are strongly dependent. In general without the product assumption on the prior it seems hopeless (from a computational perspective) to look for the optimal Bayesian strategy. Thus, despite being in a Bayesian setting, it makes sense to view it as a learning problem and to evaluate the agent’s performance through its Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling strategy which was proposed in the very first paper on the multi-armed bandit problem Thompson [1933]. This strategy can be described very succinctly: let πt be the posterior distribution on θ given the history Ht = (I1, X1,1, .",
      "startOffset" : 91,
      "endOffset" : 1487
    }, {
      "referenceID" : 3,
      "context" : "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute). The product assumption on the prior means that the reward processes (Xi,s)s≥1 are independent across arms. In the present paper we are precisely interested in the situations where this assumption is not satisfied. Indeed we believe that one of the strength of the Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way. A prototypical example that we shall consider later on in this paper is when one knows the distributions of the arms up to a permutation, in which case the reward processes are strongly dependent. In general without the product assumption on the prior it seems hopeless (from a computational perspective) to look for the optimal Bayesian strategy. Thus, despite being in a Bayesian setting, it makes sense to view it as a learning problem and to evaluate the agent’s performance through its Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling strategy which was proposed in the very first paper on the multi-armed bandit problem Thompson [1933]. This strategy can be described very succinctly: let πt be the posterior distribution on θ given the history Ht = (I1, X1,1, . . . , It−1, XIt−1,TIt−1 (t−1)) of the algorithm up to the beginning of round t. Then Thompson Sampling first draws a parameter θ from πt (independently from the past given πt) and it pulls It ∈ argmaxi∈[K] μi(θ). Recently there has been a surge of interest in this simple policy, mainly because of its flexibility to incorporate prior knowledge on the arms, see for example Chapelle and Li [2011]. For a long time the theoretical properties of Thompson Sampling remained elusive.",
      "startOffset" : 91,
      "endOffset" : 2011
    }, {
      "referenceID" : 0,
      "context" : "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al.",
      "startOffset" : 103,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b].",
      "startOffset" : 103,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b]. However as we pointed out above here we are interested in proving regret bounds for the more realistic scenario where one runs Thompson Sampling with a hand-tuned prior distribution, possibly very different from a Beta prior.",
      "startOffset" : 103,
      "endOffset" : 180
    }, {
      "referenceID" : 0,
      "context" : "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b]. However as we pointed out above here we are interested in proving regret bounds for the more realistic scenario where one runs Thompson Sampling with a hand-tuned prior distribution, possibly very different from a Beta prior. The first result in this spirit was obtained very recently by Russo and Roy [2013] who showed that for any prior distribution π0 Thompson Sampling always satisfies BRn ≤ 5 √ nK logn.",
      "startOffset" : 103,
      "endOffset" : 490
    }, {
      "referenceID" : 0,
      "context" : "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1.",
      "startOffset" : 20,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees.",
      "startOffset" : 20,
      "endOffset" : 280
    }, {
      "referenceID" : 0,
      "context" : "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where μ∗ and ε > 0 are known values such that for any θ ∈ Θ, first there is a unique best arm {i∗(θ)} = argmaxi∈[K] μi(θ), and furthermore μi∗(θ)(θ) = μ ∗, and ∆i(θ) := μi∗(θ)(θ)− μi(θ) ≥ ε, ∀i 6= i∗(θ).",
      "startOffset" : 20,
      "endOffset" : 526
    }, {
      "referenceID" : 0,
      "context" : "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where μ∗ and ε > 0 are known values such that for any θ ∈ Θ, first there is a unique best arm {i∗(θ)} = argmaxi∈[K] μi(θ), and furthermore μi∗(θ)(θ) = μ ∗, and ∆i(θ) := μi∗(θ)(θ)− μi(θ) ≥ ε, ∀i 6= i∗(θ). In other words the value of the best arm is known as well as a non-trivial lower bound on the gap between the values of the best and second best arms. For this problem a new algorithm was proposed in Bubeck et al. [2013] (which we call the BPR policy), and it was shown that the BPR policy satisfies",
      "startOffset" : 20,
      "endOffset" : 983
    }, {
      "referenceID" : 4,
      "context" : "Thus the BPR policy attains a regret uniformly bounded over time in the BPR setting, a feature that standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve.",
      "startOffset" : 142,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "Thus the BPR policy attains a regret uniformly bounded over time in the BPR setting, a feature that standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve. It is natural to view the assumptions of the BPR setting as a prior over the reward distributions and to ask what regret guarantees attains Thompson Sampling in that situation. More precisely we consider Thompson Sampling with Gaussian reward distributions and uniform prior over the possible range of parameters. We then prove individual regret bounds for any sub-Gaussian distributions (similarly to Bubeck et al. [2013]).",
      "startOffset" : 142,
      "endOffset" : 600
    }, {
      "referenceID" : 0,
      "context" : "5, Bubeck and Cesa-Bianchi 1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(θ) while the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.",
      "startOffset" : 60,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "5, Bubeck and Cesa-Bianchi 1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(θ) while the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.",
      "startOffset" : 60,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "This theorem also implies an optimal rate of identification for the best arm, see Bubeck et al. [2009] for more details on this.",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "This theorem also implies an optimal rate of identification for the best arm, see Bubeck et al. [2009] for more details on this. Proof We decompose the proof into three steps. We denote i∗(θ) ∈ argmaxi∈[K] μi(θ), in particular one has It = i∗(θ(t)). Step 1: rewriting of the Bayesian regret in terms of upper confidence bounds. This step is given by [Proposition 1, Russo and Roy [2013]] which we reprove for the sake of completeness.",
      "startOffset" : 82,
      "endOffset" : 387
    }, {
      "referenceID" : 2,
      "context" : "Inspired by the MOSS strategy of Audibert and Bubeck [2009] we will now take",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "Next we extract the following inequality from Audibert and Bubeck [2010] (see p2683–2684), for any i ∈ [K], P(μi −Bi,t ≥ u) ≤ 4K nu2 log (√ n K u ) + 1 nu2/K − 1 .",
      "startOffset" : 46,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "3 Thompson Sampling in the two-armed BPR setting Following [Section 2, Bubeck et al. [2013]] we consider here the two-armed bandit problem with sub-Gaussian reward distributions (that is they satisfy Eeλ(X−μ) ≤ eλ2/2 for all λ ∈ R) and such that one reward distribution has mean μ∗ and the other one has mean μ∗ −∆ where μ∗ and ∆ are known values.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "The next result shows that it attains optimal performances in this setting up to a numerical constant (see Bubeck et al. [2013] for lower bounds), for any subGaussian reward distribution (not necessarily Gaussian) with largest mean μ∗ and gap ∆.",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "The next result shows that it attains optimal performances in this setting up to a numerical constant (see Bubeck et al. [2013] for lower bounds), for any subGaussian reward distribution (not necessarily Gaussian) with largest mean μ∗ and gap ∆. Theorem 2 The policy of Figure 1 has regret bounded as Rn ≤ ∆+ 578 ∆ , uniformly in n. Note that we did not try to optimize the numerical constant in the above bound. Figure 2 shows an empirical comparison of the policy of Figure 1 with Policy 1 of Bubeck et al. [2013]. Note in particular that a regret bound of order 16/∆ was proved for the latter algorithm and the (limited) numerical simulation presented here suggests that Thompson Sampling outperforms this strategy.",
      "startOffset" : 107,
      "endOffset" : 516
    }, {
      "referenceID" : 6,
      "context" : "Policy 1 from Bubeck et al.[2013] Policy of Figure 1",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "Policy 1 from Bubeck et al.[2013] Policy of Figure 1",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "Figure 2: Empirical comparison of the policy of Figure 1 and Policy 1 of Bubeck et al. [2013] on Gaussian reward distributions with variance 1.",
      "startOffset" : 73,
      "endOffset" : 94
    } ],
    "year" : 2013,
    "abstractText" : "We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit as the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 √ nK. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by 1 20 √ nK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.",
    "creator" : "LaTeX with hyperref package"
  }
}