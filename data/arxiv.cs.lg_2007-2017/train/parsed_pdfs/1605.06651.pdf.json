{
  "name" : "1605.06651.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Gambler’s Ruin Bandit Problem",
    "authors" : [ "Nima Akbarzadeh", "Cem Tekin" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nMulti-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2]. In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward. The goal of the learner is to maximize its long-term expected reward by choosing actions that yield high rewards. This is a non-trivial task, since the reward distributions are not known beforehand. Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]–[6]. These rules act myopically by choosing the action with the maximum index in each round.\nSituations that require multiple actions to be taken in each round cannot be modeled using conventional MAB. As an example, consider medical treatment administration. At the beginning of each round a patient arrives to the intensive care unit (ICU) with a random initial health state. The goal state is defined as discharge and dead-end state is defined as death. Actions correspond to treatment options that move the patient randomly over the state space. The objective is to maximize the expected number of patients that are discharged by learning the optimal treatment policy using the observations gathered from the previous patients. In the example given above, each round corresponds to a goaloriented Markov Decision Process (MDP) with dead-ends\nCem Tekin is supported by TUBITAK 2232 Fellowship (116C043).\n[7]. The learner knows the state space, goal and dead-end states, but does not know the state transition probabilities a priori. At each round, the learner chooses a sequence of actions and only observes the state transitions that result from the chosen actions. In the literature, this kind of feedback information is called bandit feedback [8].\nMotivated by the application described above, we propose a new MAB problem in which multiple arms are selected in each round until a terminal state is reached. Due to its resemblance to the Gambler’s Ruin Problem [9]–[11], we call this new MAB problem the Gambler’s Ruin Bandit Problem (GRBP). In GRBP, the system proceeds in a sequence of rounds ρ ∈ {1, 2, . . .}. Each round is modeled as an MDP (as in Fig. 1 ) with unknown state transition probabilities and terminal (absorbing) states. The set of terminal states includes a goal state G and a dead-end state D, and the non-terminal states are ordered between the goal and dead-end states. In each non-terminal state, there are two possible actions: a continuation action (action C) that moves the learner randomly over the state space around the current state; and a terminal action (action F ) that moves the learner directly into a terminal state. Starting from a random, non-terminal initial state, the learner chooses a sequence of actions and observes the resulting state transitions until a terminal state is reached. The learner incurs a unit reward if the goal state is reached. Otherwise, it incurs no reward. The goal of the learner is to maximize its cumulative expected reward over the rounds.\nIf the state transition probabilities were known beforehand, an omnipotent oracle with unlimited computational power could calculate the optimal policy that maximizes the probability of hitting the goal state from any initial state, and then select its actions according to the optimal policy. We define the regret of the learner by round ρ as the difference in the expected number of times the goal state is reached by the omnipotent oracle and the learner by round ρ.\nFirst, we show that the optimal policy for GRBP can be computed in a straightforward manner: there exists a threshold state above which it is always optimal to take action C and on or below which it is always optimal to take action F . Then, we propose an online learning algorithm for the learner, and bound its regret for two different regions that the actual state transition probabilities can lie in. The regret is bounded (finite) in one region, while it is logarithmic in the number of rounds in the other region. These bounds are problem-specific, in the sense that they are functions of the state transition probabilities. Finally, we illustrate the\nar X\niv :1\n60 5.\n06 65\n1v 3\n[ cs\n.L G\n] 2\n9 Se\np 20\n16\nbehavior of the regret as a function of the state transition probabilities through numerical experiments.\nThe contributions of this paper can be summarized as follows: • We define a new MAB problem, called GRBP, in which\nthe learner takes a sequence of actions in each round with the objective of reaching to the goal state. • We show that using conventional MAB algorithms such as UCB1 [4] in GRBP by enumerating all deterministic Markov policies is very inefficient and results in high regret. • We prove that the optimal policy for GRBP has a threshold form and the value of the threshold can be calculated in a computationally efficient way. • We derive bounds on the regret of the learner with respect to an omnipotent oracle that acts optimally. Unlike conventional MAB where the regret growth is at least logarithmic in the number of rounds [3], in GRBP regret can be either logarithmic or bounded, based on the values of the state transition probabilities. We explicitly characterize the region of state transition probabilities in which the regret is bounded.\nRemainder of the paper is organized as follows. Related work is given in Section II. GRBP is defined in Section III. Form of the optimal policy for the GRBP is given in Section IV. The learning algorithm for GRBP is given in Section V together with its regret analysis. Numerical results are shown in Section VI. Conclusion is given in Section VII."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : ""
    }, {
      "heading" : "A. Gambler’s Ruin Problem",
      "text" : "If action F is removed from the GRBP, it becomes the Gambler’s Ruin Problem. In the model of Hunter et al. [10] of the Gambler’s Ruin Problem, in addition to the standard outcome of moving one state to the left or right, two extra outcomes are also considered. One outcome changes the state immediately to G, while the other outcome changes the state immediately to D. These outcomes are referred to as Windfall and Catastrophe outcomes, respectively. The ruin and winning probabilities and the duration of the game are calculated based on these additional outcomes. In another model [11], modifications such as the chance of absorption in states other than G and D and staying in the same state are\nconsidered. The ruin and winning probabilities are calculated according to the proposed state transition model. Unlike GRBP which is an MDP, the Gambler’s Ruin Problem is a Markov chain. Moreover, the ruin and winning probabilities in the models above can be calculated exactly since the transition probabilities are assumed to be known."
    }, {
      "heading" : "B. MDPs",
      "text" : "GRBP is closely related to goal oriented MDPs and stochastic shortest path problems [12]. For these problems, in each state (or time epoch), an action has to be taken with the aim of reaching to the goal state (G) with minimum cost. For this task, the optimal policy have to be determined beforehand using the set of known transition probabilities. Recently, progress has been made in obtaining solutions for MDPs that have dead-end (D) states in addition to goal (G) states [7], [13]. These solutions require value iteration and heuristic search methods to be performed using the knowledge of transition probabilities. To the best of our knowledge, a reinforcement learning algorithm that works without knowing the transition probabilities a priori and that achieves logarithmic regret bounds, has not been developed yet for these problems.\nReinforcement learning in MDPs is considered by numerous researchers [14], [15]. In these works, it is assumed that the underlying MDP is unknown but ergodic, i.e., it is possible to reach from any state to all other states with a positive probability under any policy. These works adopt the principle of optimism under uncertainty to choose an action that maximizes the expected reward among a set of MDP models that are consistent with the estimated transition probabilities. Unlike these works, in GRBP (i) the MDP is not ergodic, and (ii) the reward is obtained only in the terminal state and not after each chosen action."
    }, {
      "heading" : "C. Multi-armed Bandits",
      "text" : "Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]– [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem). The performance of a learning algorithm for a MAB problem is computed using the notion of regret. For the stochastic MAB problem [3], the regret is defined as the difference between the total (expected) reward of the learning algorithm and an oracle which acts optimally based on complete knowledge of the problem parameters. It is shown that the regret grows logarithmically in the number of rounds for this problem.\nGRBP can be viewed as a MAB problem in which each arm corresponds to a policy. Since the set of possible deterministic policies for the GRBP is exponential in the number of states, it is infeasible to use algorithms developed for MAB problems to directly learn the optimal policy by experimenting with different policies over different rounds.\nIn addition, GRBP model does not fit into the combinatorial models proposed in prior works [18]. Due to these differences, existing MAB solutions cannot solve GRBP in an efficient way. Therefore, a new learning methodology that exploits the structure of the GRBP is needed."
    }, {
      "heading" : "III. PROBLEM FORMULATION",
      "text" : ""
    }, {
      "heading" : "A. Definition of the GRBP",
      "text" : "In the GRBP, the system is composed of a finite set of states S := {D, 1, . . . , G}, where integer D = 0 denotes the dead-end state and G denotes the goal state. The set of initial (starting) states is denoted by S̃ := {1, . . . , G−1}. The system operates in rounds (ρ = 1, 2, . . .). The initial state of each round is drawn from a probability distribution q(s), s ∈ S̃ over the set of initial states S̃, such that 1 − q(1) > 0. The current round ends and the next round starts when the learner hits state D or G. Because of this, D and G are called terminal states. All other states are called non-terminal states. Each round is divided into multiple time slots in which the learner takes an action in each time slot from the action set A := {C,F} with the aim of reaching to state G. Here, C denotes the continuation action and F is the terminal action. According to Fig. 1, action C moves the learner one state to the right or to the left of the current state. Action F moves the learner directly to one of the terminal states. Possible outcomes of each action in a non-terminal state s is shown in Fig. 1. Let sρt denote the state at the beginning of the tth time slot of round ρ and aρt denote the action taken at the tth time slot of round ρ. The state transition probabilities for action C are given by\nPr(sρt+1 = s+ 1|sρt = s, aρt = C) = pC , t ≥ 1, s ∈ S̃ Pr(sρt+1 = s− 1|sρt = s, aρt = C) = pD, t ≥ 1, s ∈ S̃\nwhere pC + pD = 1. The state transition probabilities for action F are given by\nPr(sρt+1 = G|sρt = s, aρt = F ) = pF , t ≥ 1, s ∈ S̃ Pr(sρt+1 = D|sρt = s, aρt = F ) = 1− pF , t ≥ 1, s ∈ S̃\nwhere 0 < pF < 1. If the state transition probabilities are known, each round can be modeled as a MDP and an optimal policy can be found by dynamic programming [12], [19].\nB. Value Functions, Rewards and the Optimal Policy\nLet π = (π1, π2, . . .), where πt : S̃ → A, t ≥ 1 represent a deterministic Markov policy. π is a stationary policy if πt = πt′ for all t and t′. For this case we will simply use π : S̃ → A to denote a stationary deterministic Markov policy. Since the time horizon is infinite within a round and the state transition probabilities are time-invariant, it is sufficient to search for the optimal policy within the set of stationary deterministic Markov policies, which is denoted by Π. Let V π(s) denote the probability of reaching to G by using policy π given that the system is in state s. Let Qπ(s, a) denote the\nprobability of reaching to G by taking action a in state s, and then continuing according to policy π. We have\nQπ(s, C) = pCV π(s+ 1) + pDV π(s− 1), Qπ(s, F ) = pF\nfor s ∈ S̃. Hence, V π(s), s ∈ S̃ can be computed by solving the following set of equations:\nV π(G) = 1, V π(D) = 0, V π(s) = Qπ(s, π(s)), ∀s ∈ S̃ where π(s) denotes the action selected by π in state s. The value of policy π is defined as\nV π := ∑ s∈S̃ q(s)V π(s).\nThe optimal policy is denoted by\nπ∗ := arg max π∈Π V π\nand the value of the optimal policy is denoted by\nV ∗ := max π∈Π V π.\nThe optimal policy is characterized by Bellman optimality equations for all s ∈ S̃ V ∗(s) = max{pFV ∗(G), pCV ∗(s+ 1) + pDV ∗(s− 1)},\n= max{pF , pCV ∗(s+ 1) + pDV ∗(s− 1)}. (1) As it is sufficient to search for the optimal policy within stationary deterministic Markov policies and since there are only two actions that can be taken in each s ∈ S̃, the number of all such policies is 2G−1. In Section IV, we will prove that the optimal policy for GRBP has a simple threshold form, which reduces the number of policies to learn from 2G−1 to 2."
    }, {
      "heading" : "C. Online Learning in the GRBP",
      "text" : "As we described in the previous subsection, when the state transition probabilities are known, optimal solution and its probability of reaching to the goal can be found by solving Bellman optimality equations. When the learner does not know pC and pF , the optimal policy cannot be computed a priori, and hence needs to be learned. We define the learning loss of the learner, who is not aware of the optimal policy a priori, with respect to an oracle, who knows the optimal policy from the initial round, as the regret given by\nReg(T ) := TV ∗ − T∑ ρ=1 V π̂ρ\nwhere π̂ρ denotes the policy that is used by the learner in round ρ. Let Nπ(T ) denote the number of times policy π is used by the learner by round T . For any policy π, let ∆π := V\n∗−V π denote the suboptimality gap of that policy. The regret can be rewritten as\nReg(T ) = ∑ π∈Π Nπ(T )∆π. (2)\nIn this paper, we will design learning algorithms that minimize the growth rate of the expected regret, i.e., E[Reg(T )]. A straightforward way to do this will be to employ UCB1 algorithm [4] or its variants [6] by taking each policy as an arm. The result below state a logarithmic bound on the expected regret when UCB1 is used.\nTheorem 1. When UCB1 in [4] is used to select the policy to follow at the beginning of each round (with set of arms Π), we have\nE[Reg(T )] = 8 ∑\nπ:V π<V ∗\nlog T\n∆π +\n( 1 + π2\n3 )∑ π∈Π ∆π.\nProof: See [4]. As shown in Theorem 1, the expected regret of UCB1 depends linearly on the number of suboptimal policies. For GRBP, the number of policies can be very large. For instance, we have 2G−1 different stationary deterministic Markov policies for the defined problem. These imply that using UCB1 to learn the optimal policy is highly inefficient for the GRBP. The learning algorithm we propose in Section V exploits a result on the form of the optimal policy that will be derived in Section IV to learn the optimal policy in a fast manner. This learning algorithm calculates an estimated optimal policy using the estimated transition probabilities, and hence learns much faster than applying UCB1 naively. Moreover, it can even achieve bounded regret (instead of logarithmic regret) under some special cases."
    }, {
      "heading" : "IV. FORM OF THE OPTIMAL POLICY",
      "text" : "In this section, we prove that the optimal policy for GRBP has a threshold form. The value of the threshold depends only on the state transition probabilities and the number of states. First, we give the definition of a stationary threshold policy.\nDefinition 1. π is a stationary threshold policy if there exists τ ∈ {0, 1, . . . , G− 1} such that π(s) = C for all s > τ and π(s) = F for all s ≤ τ . We use πtrτ to denote the stationary threshold policy with threshold τ . The set of stationary threshold policies is given by Πtr := {πtrτ }τ={0,1,...,G−1}.\nThe next lemma constrains the set of policies that the optimal policy lies in.\nLemma 1. In the GRBP it is always optimal to select action C at s ∈ S̃ − {1}.\nProof: By (1), for s ∈ S̃ − {1} we have V ∗(s) = max{pF , pCV ∗(s+ 1) + pDV ∗(s− 1)}.\nIf V ∗(s) = pF , this implies that\npCV ∗(s+ 1) + pDV ∗(s− 1) ≤ pF ⇒\nV ∗(s− 1) ≤ p F − pCV ∗(s+ 1)\npD . (3)\nBy definition,\npF ≤ V ∗(s),∀s ∈ S̃. (4)\nTherefore,\npF − pCV ∗(s+ 1) pD ≤ p F − pCpF pD = pF\nwhich in combination with (3) implies that V ∗(s− 1) ≤ pF . According to (4) we find that V ∗(s − 1) = pF . Then, we conclude that\nV ∗(s) = pF ⇒ V ∗(s− 1) = pF ,∀s ∈ S̃ − {1}. This also implies that\nV ∗(s+ 1) ≤ p F − pDV ∗(s− 1)\npC = pF .\nConsequently, if V ∗(s) = pF for some s ∈ S̃ − {1}, then V ∗(s) = pF ,∀s ∈ S̃ − {1}. (5)\nBy (5), if V ∗(s) = pF for some s ∈ S̃ − {1}, then this implies that V ∗(G− 1) = pF . Since V ∗(G) = 1, we have V ∗(G− 1) = max{pF , pC + pDpF } = pF\n⇒ pF ≥ pC + pDpF\n⇒ pF (1− pD) ≥ pC ⇒ pF ≥ 1⇒ pF = 1. This shows that unless pF = 1, it is suboptimal to select action F in states S̃ −{1} and since pF = 1 is a trivial case, we disregard that. Hence, it is always optimal to select action C at s ∈ S̃ − {1}.\nThe result of Lemma 1 holds independently from the set of transition probabilities and the number of states. Lemma 1 leaves out only two candidates for the optimal policy. The first candidate is the policy which selects action C at any state s ∈ S̃. The second candidate selects action C in all states except state 1. Hence, the optimal policy is always in set {πtr0 , πtr1 }. This reduces the set of policies to consider from 2G−1 to 2. Let r := pD/pC denote the failure ratio of action C. The next lemma gives the value functions for πtr1 and πtr0 .\nLemma 2. In the GRBP we have\n(i) V π tr 1 (s) =  pF + (1− pF ) 1− r s−1 1− rG−1 , when r 6= 1\npF + (1− pF ) s− 1 G− 1 , when r = 1\n(ii) V π tr 0 (s) =  1− rs 1− rG , when r 6= 1 s\nG , when r = 1\nfor s ∈ S̃. Proof: (i):\nFor πtr1 we have: V π tr 1 (G) = 1 V π tr 1 (G− 1) = pCV πtr1 (G) + pDV πtr1 (G− 2) . . . V π tr 1 (2) = pCV π tr 1 (3) + pDV π tr 1 (1)\nV π tr 1 (1) = pF\n⇒  (pC + pD)V π tr 1 (G− 1) = pC + pDV πtr1 (G− 2) . . .\n(pC + pD)V π tr 1 (2) = pCV π tr 1 (3) + pDpF\n⇒  pC(V π tr 1 (G− 1)− 1) = pD(V π tr 1 (G− 2)− V πtr1 (G− 1)) . . . pC(V π tr 1 (s+ 1)− V πtr1 (s+ 2)) = pD(V π tr 1 (s)− V πtr1 (s+ 1)) . . .\npC(V π tr 1 (2)− V πtr1 (3)) = pD(pF − V πtr1 (2))\n⇒  V π tr 1 (G− 1)− 1 = rG−2(pF − V πtr1 (2)) . . . V π tr 1 (s)− V πtr1 (s+ 1) = rs−1(pF − V πtr1 (2)) . . .\nV π tr 1 (2)− V πtr1 (3) = r(pF − V πtr1 (2))\n⇒\n(6)\nSummation of all the terms results in\n1− V πtr1 (2) = (V πtr1 (2)− pF )( G−2∑ i=1 ri)⇒ (7)\nV π tr 1 (2)( G−2∑ i=0 ri) = 1 + pF ( G−2∑ i=1 ri)⇒\nV π tr 1 (2)( G−2∑ i=0 ri) = 1− pF + pF ( G−2∑ i=0 ri)⇒\nV π tr 1 (2) = pF + 1− pF ( ∑G−2 i=0 r i) ⇒ V π tr 1 (2) = pF + (1− pF ) 1− r\n1− rG−1 .\nThen, for sth state, we have to sum up to (s− 1)th equation in (6):\nV π tr 1 (s)− V πtr1 (2) = (V πtr1 (2)− pF )( s−2∑ i=1 ri)⇒\nV π tr 1 (s) = pF + (V π tr 1 (2)− pF )( s−2∑ i=0 ri)⇒ (8) V π tr 1 (s) = pF + (1− pF ) 1− r s−1\n1− rG−1 . (9)\nFor the fair case, r has to be set to 1 in (7) and (8). Then,\nV π tr 1 (2) = pF + (1− pF ) 1\nG− 1 and\nV π tr 1 (s) = pF + (1− pF ) s− 1\nG− 1 .\nCase (ii): Since action F is never selected by πtr0 , for this case, standard analysis of the gambler’s ruin problem applies.\nThus, the probability of hitting G from state s is\n(1− rs)/(1− rG) (10) for r 6= 1 and s/G for r = 1 [20].\nThe form of the optimal policy is given in the following theorem.\nTheorem 2. In the GRBP, the optimal policy is πtrτ∗ , where\nτ∗ =  sign(pF − 1− r 1− rG ), when r 6= 1\nsign(pF − 1 G ), when r = 1\nwhere sign(x) = 1 if x is nonnegative and 0 otherwise.\nProof: Since we have found in Lemma 1 that it is always optimal to select action C when the state is in {2, . . . , G−1}, to find the optimal policy, it is sufficient to compare the value functions of the two policies for s = 1. When r 6= 1, this gives π∗ = πtr1 if\n1− r 1− rG < p F\nand π∗ = πtr0 otherwise. 1 Similarly, if r = 1 and 1/G < pF , then π∗ = πtr1 . Otherwise, π ∗ = πtr0 . Using these, the value of the optimal threshold is given as\nτ∗ =  sign(pF − 1− r 1− rG ) if r 6= 1\nsign(pF − 1 G ) if r = 1\nwhich completes the proof. When r 6= 1, the term (1 − r)/(1 − rG) represents probability of hitting G starting from state 1 by always selecting action C. This probability is equal to 1/G when r = 1. Because of this, it is optimal to take the terminal action in some cases for which pC > pF . Although the continuation action can move the system state in the direction of the goal state for some time, the long term chance of hitting the goal state by taking the continuation action can be lower than the chance of hitting the goal state by immediately taking the terminal action at state 1.\nEquation of the boundary for which the optimal policy changes from πtr0 to π tr 1 is\npF = B(r) := (1− r)/(1− rG) (11) when r 6= 1. This decision boundary is illustrated in Fig. 2 for different values of G. We call the region of transition probabilities for which πtr0 is optimal as the exploration region, and the region for which πtr1 is optimal as the noexploration region. In exploration region, the optimal policy does not take action F in any round. Therefore, any learning algorithm that needs to learn how well action F performs, needs to explore action F . As the value of G increases, area of the exploration region decreases due to the fact that probability of hitting the goal state by only taking action C decreases.\n1When (1− r)/(1− rG) = pF both πtr1 and πtr0 are optimal. For this case, we favor πtr1 because it always ends the current round."
    }, {
      "heading" : "V. AN ONLINE LEARNING ALGORITHM AND ITS REGRET ANALYSIS",
      "text" : "In this section, we propose a learning algorithm that minimizes the regret when the state transition probabilities are unknown. The proposed algorithm forms estimates of state transition probabilities based on the history of state transitions, and then, uses these estimates together with the form of the optimal policy obtained in Section IV to calculate an estimated optimal policy at each round."
    }, {
      "heading" : "A. Greedy Exploitation with Threshold Based Exploration",
      "text" : "The learning algorithm for the GRBP is called Greedy Exploitation with Threshold Based Exploration (GETBE) and its pseudocode is given in Algorithm 1. Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability. GETBE achieves this by utilizing the form of the optimal policy derived in the previous section. Although GETBE does not require all policies to be explored, it requires exploration of action F when the estimated optimal policy never selects action F . This forced exploration is done to guarantee that GETBE does not get stuck in the suboptimal policy.\nGETBE keeps counters NGF (ρ), NF (ρ), N u C(ρ) and NC(ρ): (i) NGF (ρ) is the number of times action F is selected and terminal state G is entered upon selection of action F by the beginning of round ρ, (ii) NF (ρ) is the number of times action F is selected by the beginning of round ρ, (iii) NuC(ρ) is the number of times transition from some state s to s+ 1 happened (i.e., the state moved up) after selecting action C by the beginning of round ρ, (iv) NC(ρ) is the number of times action C is selected by the beginning of round ρ. Let TF (ρ) and TC(ρ) represent the number of times action F and action C is selected in round ρ, respectively. Since, action F is a terminal action, it can be selected at most once in each round. However, action C can be selected multiple times in the same round. Let TGF (ρ) and T u C(ρ) represent the number of times state G is reached after the selection of action F and\nthe number of times the state moved up after the selection of action C in round ρ, respectively.\nAt the beginning of round ρ, GETBE forms the transition probability estimates p̂Fρ := N G F (ρ)/NF (ρ) and p̂ C ρ := NuC(ρ)/NC(ρ) that correspond to actions F and C, respectively. Then, it computes the estimated optimal policy π̂ρ by using the form of the optimal policy given in Theorem 2 for the GRBP. If π̂ρ = πtr1 , then GETBE operates in greedy exploitation mode by acting according to πtr1 for the entire round. Else if π̂ρ = πtr0 , then GETBE operates in triggered exploration mode and selects action F in the first time slot of that round if NF (ρ) < D(ρ), where D(ρ) is a non-decreasing control function that is an input of GETBE. This control function helps GETBE to avoid getting stuck in the suboptimal policy by forcing the selection of action F , although it is suboptimal according to π̂ρ. When NF (ρ) ≥ D(ρ), GETBE employs π̂ρ for the entire round.\nAt the end of round ρ the values of counters are updated as follows:\nNF (ρ+ 1) = NF (ρ) + TF (ρ) NGF (ρ+ 1) = N G F (ρ) + T G F (ρ)\nNC(ρ+ 1) = NC(ρ) + TC(ρ) NuC(ρ+ 1) = N u C(ρ) + T u C(ρ). (12)\nThese values are used to estimate the transition probabilities that will be used at the beginning of round ρ+ 1, for which the above procedure repeats. In the analysis of GETBE, we will show that when NF (ρ) ≥ D(ρ), the probability that GETBE selects the suboptimal policy is very small, which implies that the regret incurred is very small."
    }, {
      "heading" : "B. Regret Analysis",
      "text" : "In this section, we bound the (expected) regret of GETBE. We show that GETBE achieves bounded regret when the unknown transition probabilities lie in no-exploration region and logarithmic (in number of rounds) regret when the unknown transition probabilities lie in exploration region. Based on Theorem 2, GETBE only needs to learn the optimal policy from the set of policies {πtr0 , πtr1 }. Using this fact and taking the expectation of (2), the expected regret of GETBE can be written as\nE[Reg(T )] = ∑\nπ∈{πtr0 ,πtr1 }\nE[Nπ(T )]∆π. (13)\nLet ∆(s) := |V πtr1 (s)−V πtr0 (s)|, s ∈ S̃ be the suboptimality gap when the initial state is s. For any π ∈ {πtr0 , πtr1 }, we have ∆π ≤ ∆max, where ∆max := maxs∈S̃ ∆(s). The next lemma gives closed-form expressions for ∆(s) and ∆max.\nLemma 3. We have\n∆(s) =  G−s G−1 |pF − 1 G | if r = 1\nrG−1−rs−1 rG−1−1 |pF − 1− r 1− rG | if r 6= 1\n2I(·) denotes the indicator function which is 1 if the expression inside evaluates true and 0 otherwise.\nAlgorithm 1 GETBE Algorithm 1: Input : G,D(ρ) 2: Initialize: Take action C and then action F once to form initial\nestimates: NGF (1), NF (1) = 1, N u C(1), NC(1) = 1 (Round(s) to form the initial estimates (at most 2 rounds) are ignored in the regret analysis). ρ = 1\n3: while ρ ≥ 1 do 4: Get initial state sρ1 ∈ S̃, t = 1 5: p̂Fρ = NGF (ρ)\nNF (ρ) , p̂Cρ =\nNuC(ρ) NC(ρ) , r̂ρ = 1− p̂Cρ p̂Cρ\n6: if p̂uρ = 0.5 then 7: τ̂ρ = sign(p̂Fρ − 1/G) 8: else 9: τ̂ρ = sign(p̂Fρ −\n1− r̂ρ 1− (r̂ρ)G )\n10: end if 11: Set π̂ρ = πtrτ̂ρ 12: while sρt 6= G or D do 13: if (π̂ρ = πtr0 && NF (ρ) < D(ρ)) || (sρt ≤ τ̂ρ) then 14: Select action F , observe state sρt+1 15: TF (ρ) = TF (ρ) + 1, TGF (ρ) = I(s ρ t+1 = G) 2 16: else 17: Select action C, observe state sρt+1 18: TC(ρ) = TC(ρ) + 1 19: TuC(ρ) = T u C(ρ) + I(s ρ t+1 = s ρ t + 1) 20: t = t+ 1 21: end if 22: end while 23: Update the counters according to (12) 24: ρ = ρ+ 1 25: end while\nand\n∆max =  |pF − 1 G | if r = 1\n|pF − 1− r 1− rG | if r 6= 1\nProof: According to Lemma 2 we have Case (i) r = 1:\n∆(s) = |V πtr1 (s)− V πtr0 (s)|= |pF + (1− pF ) s− 1 G− 1 − s G |\n= |pF (G− s G− 1) + s− 1 G− 1 − s G |= G− s G− 1 |p F − 1 G |.\nThe above equation is maximized when s = 1. Therefore, when r = 1,\n∆max = max s∈S̃ ∆(s) = |pF − 1 G |.\nCase (ii) r 6= 1: ∆(s) = |V πtr1 (s)− V πtr0 (s)|\n= |pF + (1− pF ) r s−1 − 1 rG−1 − 1 − rs − 1 rG − 1 | = |pF (r G−1 − rs−1 rG−1 − 1 ) + rs−1 − 1 rG−1 − 1 − rs − 1 rG − 1 | = |pF (r G−1 − rs−1 rG−1 − 1 ) + rs − rs−1 + rG−1 − rG (1− rG−1)(1− rG) | = ( rG−1 − rs−1 rG−1 − 1 )|p F − 1− r 1− rG |.\nAgain, the above equation is maximized when s = 1. Therefore, when r 6= 1,\n∆max = max s∈S̃ ∆(s) = |pF − 1− r 1− rG |.\nNext, we bound E[Nπ(T )] for the suboptimal policy in a series of lemmas. From (11), it is clear that the boundary is a function of r. Let r = 1−xx . Then, the boundary becomes a function of x by which we have\nB(x) = (1− 1− x x )/(1− (1− x x )G).\nLet δ be the minimum Euclidean distance of pair (pC , pF ) from the boundary (x,B(x)) given in Fig. 2. The value of δ specifies the hardness of GRBP. When δ is small, it is harder to distinguish the optimal policy from the suboptimal policy. If the pair of estimated transition probabilities (p̂Cρ , p̂ F ρ ) in round ρ lies within a ball around (pC , pF ) with radius less than δ, then GETBE will select the optimal policy in that round. The probability that GETBE selects the optimal policy is lower bounded by the probability that the estimated transition probabilities lie in a ball centered at (pC , pF ) with radius δ.\nThe following lemma provides a lower bound on the expected number of times each action is selected by GETBE. This result will be used when bounding the regret of GETBE.\nLemma 4. (i) Let pF,1 be the probability of taking action F in round ρ when π̂ρ = πtr1 and pC,1 be the probability of taking action C at least once in round ρ when π̂ρ = πtr1 . Then,\npC,1 = 1− q(1)\npF,1 =\n{∑G−1 s=1\nG−s G−1q(s) if r = 1∑G−1\ns=1 rs−1−rG−1 1−rG−1 q(s) if r 6= 1 .\n(ii) Let D(ρ) := γ log ρ where γ > 1/p2F,1, and\nfa(ρ) := { 0.5pC,1ρ, for a = C 0.5(pF,1γ −√γ) log ρ, for a = F\nLet ρ′C be the first round in which 0.5pC,1ρ− pC,1dD(ρ)e−√ ρ− dD(ρ)e log ρ becomes positive and ρ′F be the first\nround in which both 0.5(pF,1γ − √γ) log ρ − √\nlog ρ and ρ − 2 − dD(ρ)e becomes positive. Then for a ∈ {F,C} we have\nPr (Na(ρ) < fa(ρ)) ≤ 1\nρ2 , for ρ ≥ ρ′a.\nProof: The following expressions will be used in the proof: • N0(ρ) : Number of rounds by ρ for which π̂ρ = πtr0 . • N1(ρ) : Number of rounds by ρ for which π̂ρ = πtr1 . • NF,1(ρ) : Number of rounds by ρ for which action F\nis taken when π̂ρ = πtr1 . • NC,1(ρ) : Number of rounds by ρ for which action C\nis taken when π̂ρ = πtr1 .\n• na(ρ) : Indicator function of the event that action a is selected for at least once in round ρ.\n(i) When π̂ρ = πtr1 , action C is not taken only if the initial state is 1. Hence,\npC,1 = 1− Pr(sρ1 = 1) = 1− q(1).\nLet H1 denote the event that state 1 is reached before state G when π̂ρ = πtr1 . We have\npF,1 = G−1∑ s=1 Pr(H1|sρ1 = s)q(s).\nWhen r = 1, pF,1 is equivalent to the ruin probability (probability of hitting the terminal state 1) of a fair gambler’s ruin problem over G−1 states, where states 1 and G are the terminal states. For this problem, the probability of hitting G from state s is s−1G−1 . Hence, probability of hitting state 1 from state s is\nPr(H1|sρ1 = s) = 1− s− 1 G− 1 = G− s G− 1 .\nWhen r 6= 1, the problem is equivalent to an unfair gambler’s ruin problem with G−1 states in which probability of hitting G from state s is 1−r s−1\n1−rG−1 . Then, the probability of hitting state 1 from state s becomes\nPr(H1|sρ1 = s) = 1− 1− rs−1 1− rG−1 = rs−1 − rG−1 1− rG−1 .\n(ii) Since action C might be selected for more than once in a round, we have Na(ρ) ≥ na(1)+na(2)+ · · ·+na(ρ). This holds because in the initialization of GETBE, each action is selected once. Basically, we derive the lower bounds for Na(ρ + 1), but these lower bounds also hold for Na(ρ) because of the way GETBE is initialized. For a set of rounds ρ ∈ T ⊂ {1, . . . , T}, na(ρ)s are in general not identically distributed. But if π̂ρ is same for all rounds ρ ∈ T , then na(ρ)s are identically distributed.\nFirst, assume that N1(ρ) = k, 0 ≤ k ≤ ρ. Then, the probability that action C is selected at least once in each of these k rounds is pC,1. Let ji denote the index of the round in which the estimated optimal policy is πtr1 for the ith time. The sequence of Bernoulli random variables nC(ji), i = 1, . . . , k are independent and identically distributed. Hence, the Hoeffding bound given in Appendix A can be used to upper-bound the deviation probability of sum of these random variables from the expected sum. Since the estimated optimal policy will be πtr0 for the remaining ρ−k rounds, the number of times action F is selected in all of these rounds will be at most dD(ρ)e. Therefore, the probability of taking action C is zero for at most dD(ρ)e rounds. Let ρD := ρ−dD(ρ)e and N ′C(ρ) denote the sum of ρ random variables that are drawn from an independent identically distributed Bernoulli random process with parameter pC,1. Then,\nNC(ρ) ≥ ρ− k − dD(ρ)e+ k∑ i=1 nC(ji)\n≥ ρ−dD(ρ)e∑\ni=1\nnC(ji) = N ′ C(ρ− dD(ρ)e)\n= N ′C(ρD). (14)\nAccording to the Hoeffding bound in Appendix A, we have for z > 0\nPr (N ′C(ρD)− E(N ′C(ρD)) ≤ −z) ≤ e−2z 2/ρD .\nWhen z = √ ρD log ρ the above bound becomes\nPr(N ′C(ρD) ≤ E(N ′C(ρD))− √ ρD log ρ) ≤ 1\nρ2\n⇒Pr(N ′C(ρD) ≤ pC,1(ρ− dD(ρ)e)−√ (ρ− dD(ρ)e) log ρ) ≤ 1\nρ2 .\nThen, by using (14) we obtain\nPr(NC(ρ) ≤ pC,1(ρ− dD(ρ)e)−√ (ρ− dD(ρ)e) log ρ) ≤ 1\nρ2 .\nSince ρ′C is the first round in which 0.5pC,1ρ−pC,1dD(ρ)e−√ ρD log ρ becomes positive, on or after ρ′C , we have\npC,1ρD − √ ρD log ρ > 0.5pC,1ρ. Therefore, we replace\npC,1ρD − √ ρD log ρ with 0.5pC,1ρ and then\nPr(NC(ρ) ≤ 0.5pC,1ρ) ≤ 1\nρ2 , for ρ ≥ ρ′C\nwhich is equivalent to\nPr(NC(ρ) ≤ fC(ρ)) ≤ 1\nρ2 , for ρ ≥ ρ′C . (15)\nAgain, assume that N1(ρ) = k. Then, the probability of selecting action F is pF,1 in each of these k rounds. Let R denote the set of the remaining ρ−k rounds. For a round ρr ∈ R, action F is selected only if NF (ρr) ≤ D(ρr). Among the rounds in R, the number of rounds in which action F is selected is bounded below by min{ρ−k, dD(ρ−k)e}. Then, nF (ji), i = 1, 2, . . . is a sequence of i.i.d. Bernoulli random variables with parameter pF,1. From the argument above, we obtain\nNF (ρ) ≥ min{ρ− k, dD(ρ− k)e}+ k∑ i=1 nF (ji)\n≥ k+min{ρ−k,dD(ρ−k)e}∑\ni=1\nnF (ji)\nWhen min{ρ− k, dD(ρ− k)e} = ρ− k, we have\nNF (ρ) ≥ ρ∑ i=1 nF (ji) ≥ dD(ρ)e∑ i=1 nF (ji), for ρ ≥ ρ′F .\nWhen min{ρ− k, dD(ρ− k)e} = dD(ρ− k)e, we have\nNF (ρ) ≥ k+dD(ρ−k)e∑\ni=1\nnF (ji).\nNext, we will show that D(ρ− k) + k ≥ D(ρ) when ρ is sufficiently large. First, min{ρ−k, dD(ρ−k)e} = dD(ρ−k)e implies that\nρ ≥ dD(ρ− k)e+ k ≥ D(ρ− k) + k. (16) Also, D(ρ− k) + k ≥ D(ρ) should imply that\nD(ρ)−D(ρ− k) ≤ k ⇒ γ log(ρ/(ρ− k)) ≤ k ⇒\nρ/(ρ− k) ≤ ek/γ ⇒ ρ ≥ ke k/γ\nek/γ − 1 (17)\nUsing the results in (16) and (17), we conclude that D(ρ− k) + k ≥ D(ρ) holds when\nD(ρ− k) + k ≥ ke k/γ\nek/γ − 1 . (18)\nBy setting D(ρ) = γ log ρ and manipulating (18) we get\nγ log(ρ− k) + k ≥ ke k/γ\nek/γ − 1 ⇒\nγ log(ρ− k) ≥ k( e k/γ\nek/γ − 1 − 1)⇒\nlog(ρ− k) ≥ k/γ ek/γ − 1 ⇒ ρ− k ≥ e k/γ ek/γ−1 ⇒\nρ ≥ k + e k/γ ek/γ−1 . (19)\nFirst, we evaluate the term h(k) := e k/γ\nek/γ−1 . We will show that h(k) ∈ [1, e] for all k ∈ R+. By applying L’Hopital’s rule we get\nlim k→0\nk/γ ek/γ − 1 = limk→0 1/γ (1/γ)ek/γ = 1\nand\nlim k→∞\nk/γ\nek/γ − 1 = 0\nThese two conditions and using the fact that exponential function is continuous we conclude that\nlim k→0\ne k/γ ek/γ−1 = e limk→0 k/γ ek/γ−1 = e\nand\nlim k→∞\ne k/γ ek/γ−1 = e limk→∞ k/γ ek/γ−1 = 1.\nNext, we will show that e k/γ\nek/γ−1 is decreasing in k. Since this is a monotonically increasing function of k/γ\nek/γ−1 , it is sufficient to show that k/γ\nek/γ−1 is decreasing in k. We have\nd\ndk\nk/γ ek/γ − 1 = 1 γ (e k/γ − 1)− kγ (ek/γ/γ) (ek/γ − 1)2\nThe denominator is always positive for k > 0. Therefore, we only consider the numerator and write it as\n1 γ (ek/γ − 1)− k γ (ek/γ/γ) = (γ − k)ek/γ − γ γ2 .\nAs the denominator is positive, we only need to show that (γ−k)ek/γ−γ is always negative. The derivative of the above expression is −(k/γ)ek/γ , which is negative for k > 0. We also have (γ−k)ek/γ−γ = 0 at k = 0. These two conditions imply that (γ − k)ek/γ − γ is always negative for k > 0, by which we conclude that e k/γ\nek/γ−1 is decreasing in k. Hence, we have\n1 ≤ e k/γ ek/γ−1 ≤ e\nThis implies that k + e k/γ\nek/γ−1 ≤ k + e. Hence (19) holds when ρ ≥ k+ e. This implies that when k ≤ ρ− e, we have\nk+dD(ρ−k)e∑ i=1 nF (ji) ≥ dD(ρ)e∑ i=1 nF (ji).\nThe only cases that are left out are k = ρ, k = ρ − 1 and k = ρ − 2. But we know from the definition of ρ′F that for ρ ≥ ρ′F , ρ − 2 − dD(ρ)e is positive. Hence for these cases we also have\nk+dD(ρ−k)e∑ i=1 nF (ji) ≥ ρ−2∑ i=1 nF (ji) ≥ dD(ρ)e∑ i=1 nF (ji).\nLet N ′F (ρ) denote the sum over nF (ji) for ρ rounds. From all of the cases we derived above, we obtain\nNF (ρ) ≥ dD(ρ)e∑ i=1 nF (ji) = N ′ F (dD(ρ)e) for ρ ≥ ρ′F (20)\nNow, by using Hoeffding bound we have\nPr (N ′F (dD(ρ)e)− E(N ′F (dD(ρ)e)) ≤ −z) ≤ e−2z 2/dD(ρ)e\nand if z = √ dD(ρ)e log ρ then,\nPr(N ′F (dD(ρ)e) < E(N ′F (dD(ρ)e))− √ dD(ρ)e log ρ) ≤ 1\nρ2 Pr(N ′F (dD(ρ)e) < pF,1dD(ρ)e − √ dD(ρ)e log ρ) ≤ 1\nρ2 .\nBy using (20), we get Pr(NF (ρ) < pF,1dD(ρ)e − √ dD(ρ)e log ρ) ≤ 1\nρ2 . (21)\nThen, by using D(ρ) = γ log ρ, γ > 1/p2F,1, we have pF,1dD(ρ)e − √ dD(ρ)e log ρ\n= pF,1dγ log ρe − √ dγ log ρe log ρ\n≥ pF,1γ log ρ− √ (γ log ρ+ 1) log ρ\n= pF,1γ log ρ− √ γ log2 ρ+ log ρ\n≥ pF,1γ log ρ− √ γ log2 ρ− √ log ρ (22) = (pF,1γ − √ γ) log ρ− √ log ρ (23)\nwhere (22) occurs due to the subadditivity3 of the square root. Next, we will show that (23) becomes positive when ρ\n3For a, b > 0 we have √ a+ √ b > √ a+ b since ( √ a+ √ b)2 > a+ b.\nis large enough. To do this, we first show that the first term in (23) is always positive. This is proven by observing that\nγ > 1/p2F,1 ⇒ pF,1 √ γ − 1 > 0⇒ pF,1γ − √ γ > 0. (24)\nSince log ρ increases at a higher rate than √\nlog ρ, it can be shown that 0.5(pF,1γ − √γ) log ρ − √ log ρ will always increase after some round. Since limρ→∞ 0.5(pF,1γ −√ γ) log ρ−√log ρ =∞, this term is expected to be positive after some round. From the statement of the lemma, it is known that ρ′F is greater than or equal to this round. Therefore, for ρ ≥ ρ′F , (pF,1γ − √ γ) log ρ − √log ρ >\n0.5(pF,1γ −√γ) log ρ. Using this and (23), we obtain\npF,1dD(ρ)e − √ dD(ρ)e log ρ\n≥ (pF,1γ − √ γ) log ρ− √ log ρ ≥ 0.5(pF,1γ − √ γ) log ρ.\nThen, we use this result and (21) to get\nPr(NF (ρ) ≤ 0.5(pF,1γ − √ γ) log ρ) ≤ 1\nρ2 , for ρ ≥ ρ′F\nwhich is equivalent to\nPr(NF (ρ) ≤ fF (ρ)) ≤ 1\nρ2 , for ρ ≥ ρ′F . (25)\nThe (expected) regret given in (13) can be decomposed into two parts: (i) regret in rounds in which the suboptimal policy is selected, (ii) regret in rounds in which the optimal policy is selected and GETBE explores. Let IR(T ) denote the number of rounds by round T in which the suboptimal policy is selected. The first part of the regret is upper bounded by E(IR(T )), since the reward in a round can be either 0 or 1. Similarly, the second part of the regret is upper bounded by the number of explorations when the optimal policy is πtr0 . When the optimal policy is π tr 1 , exploration will only be performed when the suboptimal policy is selected. Hence, there is no additional regret due to explorations, since all the regret is accounted for in the first part of the regret.\nLet Aρ denote the event that the suboptimal policy is selected in round ρ. Let\nCρ := {|pC − p̂Cρ |≥ δ/ √ 2} ∪ {|pF − p̂Fρ |≥ δ/ √ 2}.\nIt can be shown that on event Ccρ the Euclidian distance between (pC , pF ) and (p̂Cρ , p̂ F ρ ) is less than δ. This implies that on event Ccρ, the optimal policy is selected. Therefore, Cρ contains the event that the optimal policy is not selected. Using the linearity of expectation and the union bound, we obtain\nE[IR(T )] = E[ T∑ ρ=1 I(Aρ)]\n≤ T∑ ρ=1 ∑ a∈{F,C} Pr ( |pa − p̂aρ|≥ δ/ √ 2 ) . (26)\nLet Iexpρ be the indicator function of the event that GETBE explores. By the above discussion we have\nE[Reg(T )|π∗ = πtr1 ] ≤ E[IR(T )] (27) E[Reg(T )|π∗ = πtr0 ] ≤ ∆maxE[IR(T )]\n+ E[ T∑ ρ=1 Iexpρ ]. (28)\nNext, we bound the expected regret of GETBE for the GRBP using (27) and (28).\nTheorem 3. Let x1 := ( 1 + √ (24pF,1/δ2) + 1 ) /2pF,1. Assume that the control function is\nD(ρ) = γ log ρ where γ > max{(x1)2, 1\n(pF,1)2 }.\nLet ρ′′ be the first round in which δ2 ≥ 3 log ρfa(ρ) for both actions, ρ′ := max{ρ′C , ρ′F , ρ′′}, and\nw := 4ρ′ + 2π2\n3 (1 +\n1\n1− e−δ2 )\nThen, the regret of GETBE is bounded by\nE[Reg(T )|π∗ = πtr1 ] ≤ w\nand E[Reg(T )|π∗ = πtr0 ] ≤ dD(T )e+ w∆max.\nProof: First, we bound E[IR(T )]. For this, we replace the order of summations in (26) and we have\nE[IR(T )] ≤ ∑\na∈{F,C} T∑ ρ=1 Pr ( |pa − p̂aρ|≥ δ/ √ 2 ) . (29)\nLet N∗F (ρ) := N G F (ρ) and N ∗ C(ρ) := N u C(ρ). By using the law of total probability and Hoeffding inequality, we obtain for a ∈ {F,C} T∑ ρ=1 Pr ( |pa − p̂aρ|≥ δ√ 2 )\n= T∑ ρ=1 ∞∑ n=1 Pr ( |pa − N ∗ a (ρ) Na(ρ) |≥ δ√ 2 |Na(ρ) = n ) Pr (Na(ρ) = n)\n= T∑ ρ=1 ∞∑ n=1 Pr ( |npa −N∗a (ρ)|≥ n δ√ 2 ) Pr (Na(ρ) = n)\n≤ T∑ ρ=1 ∞∑ n=1 2e−2 (nδ/ √ 2)2 n Pr (Na(ρ) = n)\n= T∑ ρ=1 ∞∑ n=1 2e−nδ 2 Pr (Na(ρ) = n). (30)\nFor each action, we use the result of Lemma 4 and divide the summation in (30) into two summations. Note that the\nbounds on Na(ρ) given in Lemma 4 hold when ρ ≥ ρ′ ≥ max{ρ′C , ρ′F }. Therefore, we have\nT∑ ρ=1 ∞∑ n=1 2e−nδ 2 Pr (Na(ρ) = n) = ρ′−1∑ ρ=1 ∞∑ n=1 2e−nδ 2 Pr (Na(ρ) = n)+\nT∑ ρ=ρ′ ∞∑ n=1 2e−nδ 2 Pr (Na(ρ) = n)\n= K ′ + T∑ ρ=ρ′ ∞∑ n=1 2e−nδ 2 Pr (Na(ρ) = n) (31)\nwhere K ′ = ρ′−1∑ ρ=1 ∞∑ n=1 2e−nδ 2 Pr (Na(ρ) = n) which is finite since ρ′ is finite. Since ∞∑ n=1 Pr (Na(ρ) = n) = 1\nand as e−nδ 2 ≤ 1, then we have ∞∑ n=1 e−nδ 2 Pr (Na(ρ) = n) ≤ 1.\nTherefore, an upper bound on K ′ can be given as\nK ′ = ρ′−1∑ ρ=1 ∞∑ n=1 2e−nδ 2 Pr (Na(ρ) = n) ≤ ρ′−1∑ ρ=1 2 < 2ρ′.\n(32)\nWe have T∑\nρ=ρ′ ∞∑ n=1 2e−nδ 2 Pr (Na(ρ) = n) = T∑ ρ=ρ′ fa(ρ)∑ n=1 2e−nδ 2 Pr (Na(ρ) = n)+\nT∑ ρ=ρ′ ∞∑ n=fa(ρ)+1 2e−nδ 2 Pr (Na(ρ) = n). (33)\nFor the first summation in (33), we use (15) and (25) for each action as an upper bound since it is the case when n ≤ fa(ρ). Therefore,\nT∑ ρ=ρ′ fa(ρ)∑ n=1 2e−nδ 2 Pr (Na(ρ) = n) ≤ T∑ ρ=1 fa(ρ)∑ n=1 2e−nδ 2 ρ2\n≤ ∞∑ ρ=1 ∞∑ n=1 2e−nδ 2 ρ2 =\nπ2\n3(1− e−δ2) (34)\nFor the second summation in (33), we first show that δ2 ≥ 3 log ρ fa(ρ)\nfor each action when ρ ≥ ρ′. For a = F , we have δ2 ≥ 3 log ρfF (ρ) = 6 pF,1γ− √ γ since γ ≥\n(x1) 2. The proof is as follows. Note that the term pF,1γ−√γ\nis positive because of (24). In order to have δ2 ≥ 6pF,1γ−√γ , we must have pF,1γ−√γ−6/δ2 ≥ 0. This can be re-written as a second order polynomial function, which is given by\ng(x) = ax2 + bx+ c ≥ 0\nwhere a = pF,1, b = −1,c = −6/δ2 and x = √γ. Since γ is positive, we will find positive values of x for which g(x) is non-negative. Also, g(x) is a convex function since its second derivative is 2a, which is positive. Hence, g(x) is non-negative for positive x’s which are greater than the largest root. The roots of g(x) are given as\nx1 = 1 +\n√ 1 +\n24pF,1 δ2\n2pF,1 , x2 =\n1− √\n1 + 24pF,1 δ2\n2pF,1 .\nIt is clear that only x1 is positive. Thus, g(x) is non-negative for x = √ γ ≥ x1. Therefore, γ has to be greater than (x1)2 so that δ2 ≥ 6pF,1γ−√γ . For a = C we have 3 log ρfC(ρ) = 3 log ρ 0.5pC,1ρ\n. This quantity decreases as ρ increases and converges to zero in the limit ρ goes to infinity. Hence, this quantity becomes smaller than δ2 after some round. Hence, for ρ ≥ ρ′, we have δ2 ≥ 3 log ρfa(ρ) for both actions. Thus,\nT∑ ρ=ρ′ ∞∑ n=fa(ρ)+1 2e−nδ 2 Pr (Na(ρ) = n)\n≤ T∑\nρ=ρ′\n2e−fa(ρ)δ 2 ∞∑ n=fa(ρ)+1 Pr (Na(ρ) = n)\n≤ T∑\nρ=ρ′\n2e−fa(ρ)δ 2 ≤ T∑ ρ=ρ′ 2e−3 log ρ\n≤ ∞∑ ρ=1 2 ρ3 ≤ π 2 3 . (35)\nFinally, we combine the results of (35), (34) and (32) together with the result of (31) and sum the final result over the two actions to get a bound for the expression in (29). This results in\nE[IR(T )] ≤ 2(2ρ′ + π 2 3(1− e−δ2) + π2 3 )\n= 4ρ′ + 2π2\n3 (1 +\n1\n1− e−δ2 ) = w. (36)\nAssume the optimal policy is πtr1 . Then, the expected number of rounds in which the suboptimal policy is selected is finite and bounded by w (independent of T ) in (36). In this case, the exploration is done only when the suboptimal policy is selected and there will be no extra regret term due to exploration. Therefore,\nE[Reg(T )|π∗ = πtr1 ] ≤ 4ρ′ + 2π2\n3 (1 +\n1\n1− e−δ2 ) = w.\nAssume the optimal policy is πtr0 . Similar to the previous case, the expected number of rounds in which the suboptimal policy is selected is at most w. Since the suboptimal policy for this case is πtr1 , it will always be played if it is selected (no exploration). Hence, the regret in these rounds is at most ∆max. However, the learner will explore action F when the optimal policy is selected. This results in additional regret. Since, the number of explorations of GETBE by round T is bounded by dD(T )e, the regret that will result from explorations is also bounded by dD(T )e. Therefore,\nE[Reg(T )|π∗ = πtr0 ] ≤ dD(T )e+ w∆max.\nTheorem 3 bounds the expected regret of GETBE. When π∗ = πtr1 , Reg(T ) = O(1) since both actions will be selected with positive probability by the optimal policy at each round. When π∗ = πtr0 , Reg(T ) = O(D(T )) since GETBE forces to explore action F logarithmically many times to avoid getting stuck in the suboptimal policy."
    }, {
      "heading" : "VI. NUMERICAL RESULTS",
      "text" : "We create a synthetic medical treatment selection problem based on [21]. Each state is assumed to be a stage of gastric cancer (G = 4, D = 0). The goal state is defined as at least three years of survival. Action C is assumed to be chemotherapy and action F is assumed to be surgery. For action C, pC is determined by using the average survival rates for young and old groups at different stages of cancer given in [21]. For each stage, the survival rate at three years is taken to be the probability of hitting G by taking action C continuously. With this information, we set pC = 0.45. Also, the five-year survival rate of surgery given in [22] (29%) is used to set pF = 0.3.\nThe regrets shown in Fig. 3 and 4 correspond to different variants of GETBE, named as GETBE-SM, GETBE-PS and GETBE-UCB. Each variant updates the state transition probabilities in a different way. GETBE-SM uses the control function together with sample mean estimates of the state transition probabilities. Unlike GETBE-SM, GETBE-UCB and GETBE-PS do not use the control function. GETBEPS uses posterior sampling from the Beta distribution [17] to sample and update pF and pC . GETBE-UCB adds an inflation term that is equal to √ 2 log(NF (ρ)+NC(ρ))\nNa(ρ) to the\nsample mean estimates of the state transition probabilities that correspond to action a. PS-PolSelection and UCBPolSelection algorithms treat each policy as a super-arm, and use PS and UCB methods to select the best policy among the two threshold policies. Instead of updating the state transition probabilities, they directly update the rewards of the policies.\nInitial state distribution is taken to be the uniform distribution. Initial estimates of the transition probabilities are formed by setting NF (1) = 1, NGF (1) ∼ Unif[0, 1], NC(1) = 1, NuC(1) ∼ Unif[0, 1]. The time horizon is taken to be 5000 rounds, and the control function is set to be D(ρ) = 15 log ρ. Reported results are averaged over 200 iterations.\nIn Fig. 3 the regrets of GETBE and other algorithms are shown for pF and pC values given above. For this case, the the optimal policy is πtr1 and all variants of GETBE achieve finite regret, as expected. However, the regrets of UCBPolSelection and PS-PolSelection increase logarithmically, since they sample each policy logarithmically many times.\nNext, we set pC = 0.65 and pF = 0.3, in order to show how the algorithms perform when the optimal policy is πtr0 .\nThe result for this case is given in Fig. 4. As expected, the regret grows logarithmically over the rounds for all variants of GETBE, PS-PolSelection and UCB-PolSelection. GETBEPS achieves the lowest regret for this case.\nFig. 5 illustrates the regret of GETBE-SM as a function of pF and pC for T = 1000. As the state transition probabilities shift from the no-exploration region to the exploration region the regret increases as expected."
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "In this paper, we introduced the Gambler’s Ruin Bandit Problem. We characterized the form of the optimal policy for this problem, and then developed a learning algorithm called GETBE that operates on the GRBP to learn the optimal policy when the transition probabilities are unknown. We proved that the regret of this algorithm is either bounded (finite) or logarithmic in the number of rounds based on the region that the true transition probabilities lie in. In addition to the regret bounds, we illustrated the performance of our algorithm via numerical experiments."
    } ],
    "references" : [ {
      "title" : "Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges",
      "author" : [ "S.S. .Villar", "J. Bowden", "J. Wason" ],
      "venue" : "Statistical Science, vol. 30, no. 2, pp. 199–215, 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "RELEAF: An algorithm for learning and exploiting relevance",
      "author" : [ "C. Tekin", "M. van der Schaar" ],
      "venue" : "IEEE J. Sel. Topics Signal Process., vol. 9, no. 4, pp. 716–727, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "Robbins", "Herbert" ],
      "venue" : "Advances in Applied Mathematics, vol. 6, no. 1, pp. 4–22, 1985.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "Cesa-bianchi", "N. o", "P. Fischer" ],
      "venue" : "Machine Learning, vol. 47, pp. 235–256, 2002.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The KL-UCB algorithm for bounded stochastic bandits and beyond",
      "author" : [ "Garivier", "Aurelien", "Cappe", "Olivier" ],
      "venue" : "COLT, 2011, pp. 359–376.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem",
      "author" : [ "P. Auer", "R. Ortner" ],
      "venue" : "Periodica Mathematica Hungarica, vol. 61, no. 1-2, pp. 55–65, 2010.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A theory of goal-oriented mdps with dead ends",
      "author" : [ "A. Kolobov", "Mausam", "D. Weld" ],
      "venue" : "UAI, 2012, pp. 438–447.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Regret analysis of stochastic and non-stochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning, vol. 5, no. 1, pp. 1–122, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the classical ruin problems",
      "author" : [ "L. Takacs" ],
      "venue" : "J. Amer. Statisistical Association, vol. 64, pp. 889–906, 1969.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Gambler’s ruin with catastrophe and windfalls",
      "author" : [ "B. Hunter", "A.C. Krinik", "C. Nguyen", "J.M. Switkes", "H.F. von Bremen" ],
      "venue" : "Statistical Theory and Practice, vol. 2, no. 2, pp. 199–219, 2008.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Maximum and minimum of modified gamblers ruin problem, arxiv:1301.2702",
      "author" : [ "T. van Uem" ],
      "venue" : "2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dynamic programming and optimal control",
      "author" : [ "D. Bertsekas" ],
      "venue" : "Athena Scientific.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Stochastic safest and shortest path problems",
      "author" : [ "F. Teichteil-Konigsbuch" ],
      "venue" : "AAAI, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Optimistic linear programming gives logarithmic regret for irreducible MDPs",
      "author" : [ "A. Tewari", "P. Bartlett" ],
      "venue" : "Advances in Neural Information Processing Systems, vol. 20, pp. 1505–1512, 2008.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "P. Auer", "T. Jaksch", "R. Ortner" ],
      "venue" : "Advances in Neural Information Processing Systems, 2009, pp. 89–96.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A dynamic allocation index for the sequential design of experiments",
      "author" : [ "J.C. Gittins", "Jones", "D.M." ],
      "venue" : "Progress in Statistics Gani, J. (ed.), pp. 241–266, 1974.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Analysis of Thompson sampling for the multi-armed bandit problem",
      "author" : [ "S. Agrawal", "N. Goyal" ],
      "venue" : "The Journal of Machine Learning Research, vol. 23, no. 39, pp. 285–294, 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Combinatorial bandits",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Journal of Computer and System Sciences, vol. 78, no. 5, pp. 1404–1422, 2012.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Dynamic programming and modern control theory",
      "author" : [ "R. Bellman", "R.E. Kalaba" ],
      "venue" : "Citeseer,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1965
    }, {
      "title" : "On the gambler’s ruin problem for a finite markov chain",
      "author" : [ "M.A. El-Shehawey" ],
      "venue" : "Statistics and Probability Letters, vol. 79, pp. 1590– 1595, 2009.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Characteristics and prognosis of gastric cancer in young patients",
      "author" : [ "T. Isobe", "K. Hashimoto", "J. Kizaki", "M. Miyagi", "K. Aoyagi", "K. Koufuji", "K. Shirouzu" ],
      "venue" : "International Journal of Oncology, vol. 30, no. 1, pp. 43–49, 2013.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2].",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : "In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]–[6].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]–[6].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "[7].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "In the literature, this kind of feedback information is called bandit feedback [8].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "Due to its resemblance to the Gambler’s Ruin Problem [9]–[11], we call this new MAB problem the Gambler’s Ruin Bandit Problem (GRBP).",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "Due to its resemblance to the Gambler’s Ruin Problem [9]–[11], we call this new MAB problem the Gambler’s Ruin Bandit Problem (GRBP).",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "• We show that using conventional MAB algorithms such as UCB1 [4] in GRBP by enumerating all deterministic Markov policies is very inefficient and results in high regret.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "Unlike conventional MAB where the regret growth is at least logarithmic in the number of rounds [3], in GRBP regret can be either logarithmic or bounded, based on the values of the state transition probabilities.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "[10] of the Gambler’s Ruin Problem, in addition to the standard outcome of moving one state to the left or right, two extra outcomes are also considered.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "In another model [11], modifications such as the chance of absorption in states other than G and D and staying in the same state are considered.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "MDPs GRBP is closely related to goal oriented MDPs and stochastic shortest path problems [12].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "Recently, progress has been made in obtaining solutions for MDPs that have dead-end (D) states in addition to goal (G) states [7], [13].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "Recently, progress has been made in obtaining solutions for MDPs that have dead-end (D) states in addition to goal (G) states [7], [13].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "Reinforcement learning in MDPs is considered by numerous researchers [14], [15].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "Reinforcement learning in MDPs is considered by numerous researchers [14], [15].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]– [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 3,
      "context" : "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]– [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).",
      "startOffset" : 241,
      "endOffset" : 244
    }, {
      "referenceID" : 5,
      "context" : "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]– [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).",
      "startOffset" : 246,
      "endOffset" : 249
    }, {
      "referenceID" : 3,
      "context" : "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]– [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).",
      "startOffset" : 288,
      "endOffset" : 291
    }, {
      "referenceID" : 16,
      "context" : "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]– [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).",
      "startOffset" : 314,
      "endOffset" : 318
    }, {
      "referenceID" : 7,
      "context" : "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]– [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).",
      "startOffset" : 324,
      "endOffset" : 327
    }, {
      "referenceID" : 2,
      "context" : "For the stochastic MAB problem [3], the regret is defined as the difference between the total (expected) reward of the learning algorithm and an oracle which acts optimally based on complete knowledge of the problem parameters.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : "In addition, GRBP model does not fit into the combinatorial models proposed in prior works [18].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "If the state transition probabilities are known, each round can be modeled as a MDP and an optimal policy can be found by dynamic programming [12], [19].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 18,
      "context" : "If the state transition probabilities are known, each round can be modeled as a MDP and an optimal policy can be found by dynamic programming [12], [19].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "A straightforward way to do this will be to employ UCB1 algorithm [4] or its variants [6] by taking each policy as an arm.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "A straightforward way to do this will be to employ UCB1 algorithm [4] or its variants [6] by taking each policy as an arm.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "When UCB1 in [4] is used to select the policy to follow at the beginning of each round (with set of arms Π), we have",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "Proof: See [4].",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "Thus, the probability of hitting G from state s is (1− r)/(1− r) (10) for r 6= 1 and s/G for r = 1 [20].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "NUMERICAL RESULTS We create a synthetic medical treatment selection problem based on [21].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "For action C, p is determined by using the average survival rates for young and old groups at different stages of cancer given in [21].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "GETBEPS uses posterior sampling from the Beta distribution [17] to sample and update p and p .",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Initial estimates of the transition probabilities are formed by setting NF (1) = 1, N F (1) ∼ Unif[0, 1], NC(1) = 1, N C(1) ∼ Unif[0, 1].",
      "startOffset" : 98,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Initial estimates of the transition probabilities are formed by setting NF (1) = 1, N F (1) ∼ Unif[0, 1], NC(1) = 1, N C(1) ∼ Unif[0, 1].",
      "startOffset" : 130,
      "endOffset" : 136
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we propose a new multi-armed bandit problem called the Gambler’s Ruin Bandit Problem (GRBP). In the GRBP, the learner proceeds in a sequence of rounds, where each round is a Markov Decision Process (MDP) with two actions (arms): a continuation action that moves the learner randomly over the state space around the current state; and a terminal action that moves the learner directly into one of the two terminal states (goal and dead-end state). The current round ends when a terminal state is reached, and the learner incurs a positive reward only when the goal state is reached. The objective of the learner is to maximize its long-term reward (expected number of times the goal state is reached), without having any prior knowledge on the state transition probabilities. We first prove a result on the form of the optimal policy for the GRBP. Then, we define the regret of the learner with respect to an omnipotent oracle, which acts optimally in each round, and prove that it increases logarithmically over rounds. We also identify a condition under which the learner’s regret is bounded. A potential application of the GRBP is optimal medical treatment assignment, in which the continuation action corresponds to a conservative treatment and the terminal action corresponds to a risky treatment such as surgery. I. INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2]. In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward. The goal of the learner is to maximize its long-term expected reward by choosing actions that yield high rewards. This is a non-trivial task, since the reward distributions are not known beforehand. Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]–[6]. These rules act myopically by choosing the action with the maximum index in each round. Situations that require multiple actions to be taken in each round cannot be modeled using conventional MAB. As an example, consider medical treatment administration. At the beginning of each round a patient arrives to the intensive care unit (ICU) with a random initial health state. The goal state is defined as discharge and dead-end state is defined as death. Actions correspond to treatment options that move the patient randomly over the state space. The objective is to maximize the expected number of patients that are discharged by learning the optimal treatment policy using the observations gathered from the previous patients. In the example given above, each round corresponds to a goaloriented Markov Decision Process (MDP) with dead-ends Cem Tekin is supported by TUBITAK 2232 Fellowship (116C043). [7]. The learner knows the state space, goal and dead-end states, but does not know the state transition probabilities a priori. At each round, the learner chooses a sequence of actions and only observes the state transitions that result from the chosen actions. In the literature, this kind of feedback information is called bandit feedback [8]. Motivated by the application described above, we propose a new MAB problem in which multiple arms are selected in each round until a terminal state is reached. Due to its resemblance to the Gambler’s Ruin Problem [9]–[11], we call this new MAB problem the Gambler’s Ruin Bandit Problem (GRBP). In GRBP, the system proceeds in a sequence of rounds ρ ∈ {1, 2, . . .}. Each round is modeled as an MDP (as in Fig. 1 ) with unknown state transition probabilities and terminal (absorbing) states. The set of terminal states includes a goal state G and a dead-end state D, and the non-terminal states are ordered between the goal and dead-end states. In each non-terminal state, there are two possible actions: a continuation action (action C) that moves the learner randomly over the state space around the current state; and a terminal action (action F ) that moves the learner directly into a terminal state. Starting from a random, non-terminal initial state, the learner chooses a sequence of actions and observes the resulting state transitions until a terminal state is reached. The learner incurs a unit reward if the goal state is reached. Otherwise, it incurs no reward. The goal of the learner is to maximize its cumulative expected reward over the rounds. If the state transition probabilities were known beforehand, an omnipotent oracle with unlimited computational power could calculate the optimal policy that maximizes the probability of hitting the goal state from any initial state, and then select its actions according to the optimal policy. We define the regret of the learner by round ρ as the difference in the expected number of times the goal state is reached by the omnipotent oracle and the learner by round ρ. First, we show that the optimal policy for GRBP can be computed in a straightforward manner: there exists a threshold state above which it is always optimal to take action C and on or below which it is always optimal to take action F . Then, we propose an online learning algorithm for the learner, and bound its regret for two different regions that the actual state transition probabilities can lie in. The regret is bounded (finite) in one region, while it is logarithmic in the number of rounds in the other region. These bounds are problem-specific, in the sense that they are functions of the state transition probabilities. Finally, we illustrate the ar X iv :1 60 5. 06 65 1v 3 [ cs .L G ] 2 9 Se p 20 16 D\t\r   D+1\t\r   s-­‐1\t\r   s G-­‐1\t\r   G ... ... s+1",
    "creator" : "LaTeX with hyperref package"
  }
}