{
  "name" : "1503.00255.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "shimkin@ee.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n00 25\n5v 1\n[ cs\nThe notion of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions for approachability and corresponding strategies that rely on computing steering directions as projections from the current average payoff vector to the (convex) target set. Recently, Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on the no-regret properties of Online Linear Programming for computing a suitable sequence of steering directions. This is first carried out for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone. In this paper we present a more direct formulation that relies on the support function of the set, along with suitable Online Convex Optimization algorithms, which leads to a general class of approachability algorithms. We further show that Blackwell’s original algorithm and its convergence follow as a special case."
    }, {
      "heading" : "1 Introduction",
      "text" : "Both Blackwell’s theory of approachability and the no-regret framework of online learning address a repeated decision problem in the presence of on an arbitrary (namely, unpredictable) adversary. The concept of approachability, introduced in [4], addresses a fundamental feasibility issue in for repeated matrix games with vector-valued payoffs. Referring to one player as the agent and to the other as Nature, a set S in the payoff space is approachable by the agent if he can ensure that the average payoff vector converges (with probability 1) to S,\nirrespectively of Nature’s strategy. Blackwell provided in his paper geometric conditions for approachability, which are both necessary and sufficient for convex target sets S, and a corresponding approachability strategy for the agent. An extensive recent survey of approachability and its implications can be found in [12], and a textbook exposition is available in [11].\nConcurrently, Hannan [7] introduced the concept of no-regret play for repeated matrix games. The regret of the agent is the shortfall of the cumulative payoff that was actually obtained relative to the one that could have been obtained with the best (fixed) action in hindsight, given Nature’s observed actions. A no-regret strategy, or algorithm, should ensure that the regret grows sub-linearly in time. The no-regret criterion has been widely adopted during the last two decades by the machine learning community as a standard measure for the performance of online learning algorithms, and its scope has been greatly extended. Of specific relevance here is the Online Convex Optimization (OCO) framework, where Nature’s discrete action is replaced by the choice of a convex function at each stage, and the agent’s decision is a point in a convex set. The textbook [6] offers a broad overview of regret and online learning. Recent surveys of OCO algorithms may be found in [15, 9].\nIt is well known that no-regret strategies for repeated games can be obtained as a special case of the approachability problem. This was already observed in [3]; an alternative formulation that leads to more explicit strategies was proposed in [8]. More recently, it was shown in [1] that any no-regret algorithm for the online linear optimization problem can be used as a basis for an approachability strategy for convex target sets. The online algorithm is used here compute a sequence of steering vectors, that replace the projection directions used in Blackwell’s original algorithm.\nThe scheme suggested in [1] first considers target sets S that are convex cones. The generalization to any convex set is carried out by embedding the original target set in a convex cone in a higher dimensional payoff space. The present paper proposes a more direct scheme that avoids the above-mentioned embedding. This is done by invoking the support function of the target set, along with well-known relations between this function and the Euclidean distance to the set. As the support function is convex, the full arsenal of OCO algorithms may be applied to provide the required sequence of steering vectors.\nA natural question concerns the relation between Blackwell’s original algorithm and the present framework. We first observe that Blackwell’s algorithm is recovered when the standard Follow the Leader (FTL) algorithm is used for the OCO part. Establishing the (known) convergence of this algorithm via the proposed OCO framework is a bit more intricate. First, when the target set has a smooth boundary, we show that FTL guarantees logarithmic rate, which ”fast” approachability at a rate of O( log TT ). To address the general case, we further observe that Blackwell’s algorithm is still obtained when a regularized version of FTL is employed,\nfrom which the standard O(t−1/2) convergence rate may be deduced.\nThe paper proceeds as follows. In Section 2 we recall the relevant background on Blackwell’s approachability and Online Convex Optimization. Section 3 presents the proposed scheme, in the form of a meta-algorithm that relies on a generic OCO algorithm, discusses the relation to the scheme of [1], and demonstrates a specific algorithm that is obtained by using Generalized Gradient Descent for the OCO algorithm. In Section 4 we outline the relations with Blackwell’s original algorithm, and provide some concluding remarks.\nNotation: The standard inner product in Rd is denoted by 〈·, ·〉, ‖ · ‖ is the Euclidean norm, and d(r, S) = infs∈S ‖r − s‖ denotes the corresponding point-to-set distance. Further, B2 = {w ∈ Rd : ‖w‖ ≤ 1} denotes the Euclidean unit ball, ∆(I) is the set of probability distributions over a finite set I, diam(S) = sups,s′∈S ‖s − s′‖ is the diameter of the set S, and ‖R − S‖ = supr∈R,s∈S ‖r − s‖ denotes the maximal distance between points in the sets R and S."
    }, {
      "heading" : "2 Model and Background",
      "text" : "We start with a brief of review of Blackwell’s approachability and of Online Convex Programming, focusing on those aspects that are relevant to this paper."
    }, {
      "heading" : "2.1 Approachability",
      "text" : "Consider a repeated game with vector-valued rewards that is played by two players, the agent and Nature. Let I and J denote the finite action sets of these players, with corresponding mixed actions x = (x(1), . . . , x(|I|)) ∈ ∆(I) and y = (y(1), . . . , y(|J |)) ∈ ∆(J). Let r : I × J → Rd be the vector-valued reward function of the single-stage game, which is extended to mixed action as usual through the bilinear function\nr(x, y) = ∑\ni,j\nx(i)y(j)r(i, j) .\nSimilarly, we denote\nr(x, j) = ∑\ni\nx(i)r(i, j) .\nThe game is repeated in stages t = 1, 2, . . . , where at stage t actions it and jt are chosen by the players, and the reward vector r(it, jt) is obtained. A pure strategy for the agent is a mapping from each possible history (i1, j1, . . . , it−1, jt−1) to an action it, and a mixed strategy is a probability distribution over the pure strategies. Nature’s strategies may be similarly defined.\nAs usual, we restrict attention to so-called behavior strategies of the agent, where the action it is drawn randomly according to a mixed action xt, using independent draws across stages. Furthermore, to simplify the presentation, we shall state our results and algorithms in terms of the smoothed reward vectors r(xt, jt), where the reward r(i, t, jt) is averaged over the mixed action xt. This will allow us to state the results in simpler sample-path terms, rather than probabilistic ones; we further discuss this formulation below after Theorem 1.\nLet\nr̄T = 1\nT\nT ∑\nt=1\nr(xt, jt)\ndenote the T -stage average reward vector.\nDefinition 2.1 (Approachability) A closed set S ⊂ Rd is approachable if there exists a strategy for the agent and a sequence ǫ(T ) → 0 such that\nlim T→∞\nd(r̄T , S) ≤ ǫ(T ) (1)\nholds (w.p. 1) for any strategy of Nature. A strategy of the agent that satisfies this property is an approachability strategy for S.\nTheorem 1 (Blackwell, 1956) A closed and convex set S ⊂ Rd is approachable if and only if either one of the following equivalent conditions holds:\n(i) For each unit vector u ∈ Rd, there exists a mixed action x = xS(u) ∈ ∆(I) such that\n〈u, r(x, j)〉 ≤ sup s∈S 〈u, s〉 , for all j ∈ J . (2)\n(ii) For each y ∈ ∆(J) there exists x ∈ ∆(I) such that r(x, y) ∈ S.\nIf S is approachable, then the following strategy is an approachability strategy for S: For v 6∈ S, let uS(v) be the unit vector that points to v from ProjS(v), the closet point to v in S. Then, for t ≥ 1, if r̄t 6∈ S, choose xt+1 = xS(uS(r̄t)); otherwise, choose an arbitrarily action.\nThe approachability strategy introduced by Blackwell has been generalized in [8], that essentially allow different norms to be used for the projection unto S. Several recent papers have proposed approachability algorithms that depend on Blackwell’s dual condition (condition (ii) in the above Theorem) and avoid the projection step altogether (see [2] and references therein). The current paper again proposes a generalization of Blackwell’s strategy, but from a different viewpoint.\nLet us elaborate on the use of the smoothed rewards r(xt, jt). This offers several useful benefits:\n1. As noted, we obtain sample-path bounds rather than probabilistic ones.\n2. We can state results that hold for any sequence (jt), rather than any (mixed) strategy\nof Nature. This is closer to the spirit of Online Algorithms, where the notion of a randomized choice by Nature may not be meaningful.\n3. As is well known, the difference ∑T t=1 r(xt, jt)− ∑T t=1 r(it, jt) is a Martingale difference\nsequence, hence of order √ T . Thus, the difference in the means is of order 1√\nT , and\nconvergence results derived for the smoothed mean are valid for the non-smoothed one up to that order.\nWe note that the results in [1] are developed for the rewards r(xt, yt), with the mean taken over yt as well, and the agent is allowed to observe Nature’s mixed action yt (or at least the mean reward r(xt, yt)). We avoid making that extra step and assume that the agent only observes Nature’s pure actions {jt}.\nAs the pure actions it of the agent do not affect the rewards rt = r(xt, jt), we may suppress them in the following discussion and focus on the mixed actions xt. In particular, we restrict attention to strategies of the agent that assign a mixed action xt to each sequence (j1, . . . , jt−1) of Nature’s actions. (Note that there is no need to include the past mixed actions x1, . . . , xt−1 in the history sequence, since they may be computed recursively; in practice, however, we will express xt as a function of past the reward vector sequence (r(xk, jk))k<t.) Since there is no randomization involved, it may be seen that Definition 2.1 is equivalent to the requirement that the bound (1) holds (deterministically) for any sequence (j1, j2, . . . ) of Nature’s actions."
    }, {
      "heading" : "2.2 Online Convex Optimization (OCO)",
      "text" : "OCO extends the framework of no-regret learning to function minimization. Let W be a convex and compact set in Rd, and let F be a set of convex and uniformly bounded functions f : W → R. Consider a sequential decision problem, where at each stage t ≥ 1 the agent chooses a point wt ∈ W , and then observes a function ft ∈ F . An Algorithm for the agent is a rule for choosing wt, t ≥ 1, based on the history {fk, wk}k≤t−1. The regret of an algorithm A is defined as\nRegretT (A) = sup f1,...,fT∈F\n{\nT ∑\nt=1\nft(wt)− min w∈W\nT ∑\nt=1\nft(w)\n}\n, (3)\nwhere the supremum is taken over all possible functions ft ∈ F . An effective algorithm should guarantee a small regret, and in particular one that grows sub-linearly in T .\nThe OCO problem was introduced in this generality in [16], along with the following Online\nGradient Descent algorithm:\nwt+1 = ProjW (wt − ηtgt) . (4)\nHere gt is an arbitrary element of ∂ft(wt), the subdifferential of ft at wt, (ηt) is a diminishing gain sequence, and ProjW denotes the Euclidean projection onto the convex set W . To state a regret bound for this algorithm, let diam(W ) denote the diameter of W , and suppose that all subgradients of the functions ft are uniformly bounded in norm by a constant G.\nProposition 2 (Zinkevich, 2003) For the Online Gradient Descent algorithm in (4) with gain sequence ηt = η√ t , η > 0, the regret is upper bounded by\nRegretT (OGD) ≤ ( diam(W )2\nη + 2ηG2)\n√ T . (5)\nSeveral classes of OCO algorithms are now known, as surveyed in [6, 15, 9]. Of particular relevance here is the Regularized Follow the Leader (RTFL) algorithm, specified by\nwt+1 = argmin w∈W\n(\nt ∑\nk=1\nfk(w) +Rt(w)\n)\n, (6)\nwhere Rt(w), t ≥ 1 is a sequence of regularization functions. With Rt ≡ 0, the algorithm reduces to the basic Follow the Leader (FTL) algorithm, which does not generally lead to sub-linear regret, unless additional requirements such as strong convexity are imposed on the functions ft (we will revisit the convergence of FTL in Section 4). For RFTL, we will require the following standard convergence result. Recall that a function R(w) over a convex set W is called ρ-strongly convex if R(w)− ρ2‖w‖2 is convex there.\nProposition 3 Suppose that each function ft is Lischitz-continuous over W , with Lipschitz coefficient Lf . Let Rt(w) = ρtR(w), where 0 < ρt < ρt+1, and the function R : W → [0, Rmax] is Lipschitz continuous with coefficient LR, and is 1-strongly convex. Then,\nRegretT (RFTL) ≤ 2Lf T ∑\nt=1\nLf + (ρt − ρt−1)LR ρt + ρt−1 + ρTRmax . (7)\nThe last bound can be established along the lines of Theorem 2.11 in [15], which considers the case of fixed regularization parameters, ρt ≡ ρ0. The proof is outlined in the Appendix."
    }, {
      "heading" : "3 OCO-Based Approachability",
      "text" : "This section presents the proposed OCO-based approachability algorithm. We start by introducing the support function and some of its properties, and expressing Blackwell’s separation\ncondition in terms of this function. We continue to present the proposed meta-algorithm that employs a generic OCO algorithm, and then provide as an example the specific algorithm that is obtained when Online Gradient Descent is used as the OCO algorithm."
    }, {
      "heading" : "3.1 The Support Function",
      "text" : "Let set S ⊂ Rd be a closed and convex set. The support function hS : Rd → R ∪ {∞} of S is defined as\nhS(w) , sup s∈S\n〈w, s〉, w ∈ Rd.\nIt it is evident that hS is a convex function (as a pointwise supremum over linear functions), and is positive homogeneous: hS(aw) = ahS(w) for a ≥ 0. Furthermore, the Euclidean distance from a point r to S can be expressed as\nd(r, S) = max w∈B2\n{〈w, r〉 − hS(w)} , (8)\nwhere B2 is the closed Euclidean unit ball (see, e.g., [5], Section 8.1.3; this equality may be readily verified using the minimax theorem). It follows that\nargmax w∈B2\n{〈w, r〉 − hS(w)} = { 0 : r ∈ S uS(r) : r 6∈ S\n(9)\nwith uS(r) as defined in Theorem 1, namely the unit vector pointing to r from ProjS(r).\nBlackwell’s separation condition in (2) can now be written in terms of the support function, as\n〈w, r(x, j)〉 ≤ sup s∈S 〈w, s〉 ≡ hS(w) .\nWe thus obtain the following Corollary to Theorem 1.\nCorollary 4 A closed and convex set S is approachable if and only if for every vector w ∈ B2 there exists x ∈ ∆(I) so that\n〈w, r(x, j)〉 − hS(w) ≤ 0, ∀j ∈ J. (10)\nNote that the last condition can be written as val(w · r) ≤ hS(w), where\nval(w · r) △= min x∈∆(I) max j∈J 〈w, r(x, j)〉 ,\nthe minimax value of the game with the scalar payoff that is obtained by projection the reward vectors r(i, j) onto w. Consequently, a mixed action x that satisfies (10) can be computed as the minimax strategy for the agent in this game."
    }, {
      "heading" : "3.2 The General Algorithm",
      "text" : "The proposed algorithm builds on the following idea. First, we employ an OCO algorithm to generate a sequence of steering vectors wt ∈ B2, so that T ∑\nt=1\n(〈wt, rt〉 − hS(wt)) ≥ T max w∈B2 {〈w, r̄T 〉 − hS(w)} − a(T ), (11)\nwhere rt = r(xt, jt) is considered an arbitrary vector that is revealed after wt is specified, and a(T ) = o(T ). Next, given wt, we choose xt that satisfies (10), so that 〈wt, rt〉 − hS(wt) ≤ 0. Using this inequality in (11), and observing the distance formula (8), yields\nd(r̄t, S) ≤ a(T )\nT → 0 .\nTo secure (11), observe that the function f(w; r) = −〈w, r〉 + hS(w) is convex in w for each vector r. Therefore, an OCO algorithm can be applied to the sequence of convex functions ft(w) = −〈w, rt〉 + hS(w), where rt = r(xt, jt) is considered an arbitrary vector which is revealed only after wt is specified. Applying an OCO algorithm A with RegretT (A) ≤ a(T ) to this setup, we obtain a sequence (wt) such that\nT ∑\nt=1\nft(wt) ≤ min w∈B2\nT ∑\nt=1\nft(w) + a(T ) ,\nwhere\nT ∑\nt=1\nft(wt) = − T ∑\nt=1\n(〈wt, rt〉 − hS(wt)) ,\nT ∑\nt=1\nft(w) = − T ∑\nt=1\n(〈w, rt〉 − hS(w)) = −T (〈w, r̄T 〉 − hS(w)) .\nThis clearly implies (11).\nThe discussion above leads to the following approachability meta-algorithm.\nAlgorithm 1 (Approachability Meta-Algorithm Based on OCO)\nGiven: A closed, convex and approachable set S; a procedure (e.g., a linear program) to compute x, for a given vector w, so that (10) is satisfied; an OCO algorithm A for the functions ft(w) = −〈wt, rt〉+ hS(w), with RegretT (A) ≤ a(T ).\nRepeat for t = 1, 2, . . . :\n1. Obtain wt from the OCO algorithm applied to the convex functions fk(w) = −〈w, rk〉 + hk(w), k ≤ t− 1, so that inequality (11) is satisfied.\n2. Choose xt according to (10), so that 〈wt, r(xt, j)〉 − hS(wt) ≤ 0 holds for all j ∈ J .\n3. Observe Nature’s action jt, and set rt = r(xt, jt).\nProposition 5 For the algorithm above,\nd(r̄T , S) ≤ a(T )\nT\nis satisfied for all T ≥ 1 and any sequence (j1, j2, . . . ) of Nature’s actions.\nProof: As observed above, application of the OCO algorithm implies (11), so that\nd(r̄T , S) = max w∈B2\n{〈w, r̄T 〉 − hS(w)}\n≤ 1 T\nT ∑\nt=1\n(〈wt, rt〉 − hS(wt)) + a(T ) T ≤ a(T ) T .\nTo recap, any OCO algorithm that guarantees (11) with a(T )T → 0, induces an approachability strategy with rate of convergence a(T )T .\nRemark 1 (Convex Cones) The approachability algorithm developed in [1] starts with a target sets S that are restricted to be convex cones. For S a closed convex cone, the support function is given by\nhS(w) =\n{\n0 : w ∈ So ∞ : w 6∈ So\nwhere So is the polar cone of S. The required inequality in (11) therefore reduces to\nT ∑\nt=1\n〈wt, rt〉 ≥ T max w∈B2∩So 〈w, r̄T 〉 − a(T ) .\nThe sequence (wt) can be obtained in this case by applying an online linear optimization algorithm restricted to wt ∈ B2 ∩ So. This is the algorithm proposed in [1].\nThe extension to general convex sets is handled there by lifting the problem to a (d + 1)- dimensional space, with payoff vector r′(x, y) = (κ, r(x, y)) and target set S′ = cone({κ} × S), where κ = maxs∈S ‖s‖, for which it holds that d(u, S) ≤ 2d(u′, S′). For further details see [1]."
    }, {
      "heading" : "3.3 An OGD-based Approachability Algorithm",
      "text" : "As a concrete example, let us apply the Online Gradient Descent algorithm specified in (4) to our problem. With W = B2 and ft(w) = −(〈w, rt〉−hS(w)), we obtain in step 1 of Algorithm 1,\nwt+1 = ProjB2{wt + ηt(rt − yt)} , yt ∈ ∂hS(wt) .\nObserve that ProjB2(v) = v/max{1, ‖v‖}, and (e.g., Corollary 8.25 in [14])\n∂hS(w) = argmax s∈S\n〈s,w〉 .\nTo evaluate the convergence rate in (5), observe that diam(B2) = 2, and, since yt ∈ S, ‖gt‖ = ‖rt − yt‖ ≤ ‖R− S‖, where R = {r(x, y)}x∈∆(I),y∈∆(J) is the reward set. Assuming for the moment that the goal set S is bounded, we obtain\nd(r̄T , S) ≤ b(η)√ T , with b(η) = 4 η + 2η‖R − S‖2 .\nFor η = √ 2/‖R − S‖, we thus obtain b(η) = 4 √ 2‖R − S‖.\nIf S is not bounded, it can always be intersected with R (without affecting its approachability), yielding ‖R − S‖ ≤ diam(R). This amounts to modifying the choice of yt in the algorithm to\nyt ∈ ∂hS∩R(wt) = argmax y∈S∩R (y,w) .\nAlternatively, one may restrict attention (by projection) to vectors wt in the set {w ∈ B2 : hS(w) < ∞}, similarly to the case of convex cones mentioned in Remark 1 above; we will not go into further details here."
    }, {
      "heading" : "4 Blackwell’s Algorithm and (R)FTL",
      "text" : "We next examine the relation between Blackwell’s approachability algorithm and the present OCO-based framework. We first show that Blackwell’s algorithm coincides with OCO-based approachability when FTL is used as the OCO algorithm. We use this equivalence to establish fast (logarithmic) convergence rates for Blackwell’s algorithm when the target set S has a smooth boundary. Interestingly, this equivalence does not provide a convergence result for general convex sets. To complete the picture, we show that Blackwell’s algorithm can more generally be obtained via a regularized version of FTL, which leads to an alternative proof of convergence of the algorithm in the general case."
    }, {
      "heading" : "4.1 Blackwell’s algorithm as FTL",
      "text" : "Recall Blackwell’s algorithm as specified in Theorem 1, namely xt+1 is chosen as a mixed action that satisfies (2) for u = uS(r̄t).\nLemma 6 For ft(w) = −〈w, rt〉+ hS(w),\nargmin w∈B2\nt ∑\nk=1\nfk(w) =\n{\nuS(r̄t) : r̄t 6∈ S 0 : r̄t ∈ S .\nProof: Observe that ∑t k=1 fk(w) = −t(〈w, r̄t〉 − hS(w)), so that\nargmin w∈B2\nt ∑\nk=1\nfk(w) = argmax w∈B2\n{〈w, r̄t〉 − hS(w)} .\nThe required equality now follows from (9).\nComparing to (6), with Rt ≡ 0, it may be seen that the sequence of projection directions uS(r̄t) in Blackwell’s algorithm coincides with the sequence (wt) that is obtained by applying the FTL algorithm to the functions (ft) over w ∈ B2. It follows that Blackwell’s algorithm is identical to Algorithm 1 with this choice of the OCO algorithm.\nTo establish convergence of Blackwell’s algorithm via this equivalence, one needs to show that FTL guarantees the regret bound in (11) for an arbitrary reward sequence (rt) ⊂ R, with a sublinear rate sequence a(T ). It is well know, however, that (unregularized) FTL does not guarantee sublinear regret, without some additional assumptions on the function ft. A simple counter-example, reformulated to the present case, is devised as follows: Let S = {0} ⊂ R, so that hS(w) = 0, and suppose that r1 = −1 and rt = 2(−1)t for t > 1. Since wt = sign(r̄t−1) and sign(rt) = −sign(r̄t−1), we obtain that ft(wt) = −rtwt = 1, leading to a linearly-increasing regret.\nThe failure of FTL in this example is clearly due to the fast changes in the predictors wt. We now add some smoothness assumptions on the set S that can mitigate such abrupt changes.\nAssumption 1 Let S be a compact and convex set. Suppose that the boundary ∂S of S is smooth with curvature bounded by κ0, namely:\n‖~n(s1)− ~n(s2)‖ ≤ κ0‖s1 − s2‖ for all s1, s2 ∈ ∂S , (12)\nwhere ~n(s) is the unique unit outer normal to S at s ∈ ∂S.\nFor example, for a closed Euclidean ball of radius ρ, (12) is satisfied with equality for κ0 = ρ −1. The assumed smoothness property may in fact be formulated in terms of an interior sphere\ncondition: For any point in s ∈ S there exists a ball B(ρ) ⊂ S with radius ρ = κ−10 such that s ∈ B(ρ).\nProposition 7 Let Assumption 1 hold. Consider Blackwell’s algorithm as specified in Theorem 1, and denote wt = uS(r̄t−1) (with w1 arbitrary). Then, for any time T ≥ 1 such that r̄T 6∈ S, (11) holds with\na(T ) = C0(1 + lnT ), (13)\nwhere C0 = diam(R) ‖R − S‖κ0, C1 = ‖R − S‖, and ln(·) is the natural logarithm. Consequently,\nd(r̄T , S) ≤ C0 1 + lnT\nT , T ≥ 1 . (14)\nProof: See the Appendix.\nThe last result establishes a fast convergence rate (of order log T/T ) for Blackwell’s approachability algorithm, under the assumed smoothness of the target set. We observe that in the stochastic version of the algorithm, which is based on the rewards r(it, jt) rather than r(xt, jt), the convergence is still of order T−1/2 due to the added stochastic effect (unless all mixed actions xt happen to be pure). We also note that logarithmic convergence rates for OCO algorithms were derived in [10], under strong convexity conditions on the function ft. Finally, conditions for fast approachability (of order T−1) were derived in [13], but are of different nature than the above."
    }, {
      "heading" : "4.2 Blackwell’s algorithm as RFTL",
      "text" : "The smoothness requirement in Assumption 1 precludes such important target sets as polyhedra and cones. As observed above, in absence of such additional smoothness properties the interpretation of Blackwell’s algorithm through an FTL scheme does not imply its convergence, as the regret of FTL (and the corresponding bound a(T ) in (11)) might increase linearly in general.\nTo address the general case, we show next that the Blackwell’s algorithm can be identified more generally with a regularized version of FTL. This algorithm does guarantee an O( √ T ) regret in (11), and consequently leads to the standard O(T−1/2) rate of convergence of Blackwell’s approachability algorithm.\nOur starting point is the following observation:\nLemma 8 For fk(w) = −〈w, rk〉+ hS(w), 1 ≤ k ≤ t, and any ρt > 0,\nwt+1 △ = argmin\nw∈B2\n{\nt ∑\nk=1\nfk(w) + ρt 2 ‖w‖2 } =\n{\nβtuS(r̄t) : r̄t 6∈ S 0 : r̄t ∈ S . (15)\nwhere βt = min{1, tρtd(r̄t, S)} > 0.\nProof: Recall that ∑t k=1 fk(w) = −t(〈w, r̄t〉 − hS(w)), so that\nargmin w∈B2\n{\nt ∑\nk=1\nfk(w) + ρt 2 ‖w‖2 } = argmax w∈B2 {〈w, r̄t〉 − hS(w) − ρt 2t ‖w‖2} .\nTo compute the right-hand side, we first maximize over {w : ‖w‖ = β}, and then optimize over β ∈ [0, 1]. Denote r = r̄t, and η = ρt/t. Similarly to Lemma 6,\nargmax ‖w‖=β\n{〈w, r〉 − hS(w) − η\n2 ‖w‖2} = argmax ‖w‖=β {〈w, r〉 − hS(w)} =\n{\nβuS(r) : r 6∈ S 0 : r ∈ S .\nNow, for r 6∈ S, max ‖w‖=β {〈w, r〉 − hS(w)− η 2 ‖w‖2} = βd(r, S) − η 2 β2 . Maximizing the latter over 0 ≤ β ≤ 1 gives β∗ = min{1, d(r,S)η }. Substituting back r and η gives (15).\nEquation (15) defines an RFTL algorithm with quadratic regularization. When used for the OCO part in Algorithm 1, the resulting scheme turns out to be equivalent to Blackwell’s algorithm. Indeed, the minimum in (15) is attained by the same unit vector uS(r̄t) that appears in Theorem 1, scaled by a positive constant. That scaling does not affect the choice of xt according to (10), as the support function hS(w) is positive homogeneous. However, this scaling does induce sublinear-regret for the OLO algorithm, and consequently convergence of the approachability algorithm. This is summarized as follows.\nProposition 9 Let S be a convex and compact set. Consider the RTFL algorithm specified in equation (15), with ρt = ρ √ t, ρ > 0. The regret of this algorithm is bounded by\nRegretT (RTFL) ≤ ( 2L2f ρ + ρ) √ T + 2L2f ρ + Lf ln(4T − 3) △ = a0(T ) ,\nwhere Lf = ‖R − S‖. Consequently, if this RTFL algorithm is used in step 1 of Algorithm 1 to provide wt, we obtain\nd(r̄T , S) ≤ a0(T ) T = O(T− 1 2 ) , T ≥ 1 . (16)\nProof: The regret bound follows from the one in Proposition 3, evaluated for ft(w) = −〈rt, w〉 + hS(s), W = B2, R(w) = ‖w‖2, and ρt = ρ0 √ t. Recalling that ∂ft(w) = −rt + argmaxs∈S〈w, s〉, the Lipschitz constant of ft is upper bounded by ‖R − S‖ △ = Lf . Furthermore, Rmax = 1 and LR = 2. Therefore,\nRegretT (RTFL) ≤ 2Lf T ∑\nt=1\nLf + 2ρ( √ t− √ t− 1)\nρ( √ t+ √ t− 1)\n+ ρ √ T .\nUpper bounding the sums with corresponding integrals gives the stated regret bound. The second part now follows directly from Proposition 5. With ρ = √ 2Lf , we obtain in (16) the convergence rate\nd(r̄T , S) ≤ 2 √ 2‖R − S‖√\nT + o( 1√ T ) .\nWe emphasize that the algorithm discussed in this section is equivalent to Blackwell’s algorithm, hence its convergence is well known. The proof of convergence here is certainly not the simplest, nor does it lead to the best constants in the convergence rate. Indeed, Blackwell’s proof (which recursively bounds the square distance d(r̄T , S) 2) leads to the bound d(r̄T , S) ≤ ‖R−S‖√T . Rather, our main purpose here was to provide an alternative view and analysis of Blackwell’s algorithm, which rely on a standard OCO algorithm. Nonetheless, the logarithmic convergence rate that was obtained under the smoothness Assumption 1 appears to be new."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The author wishes to thank Elad Hazan for helpful comments on a preliminary version of this work. This research was supported by the Israel Science Foundation grant No. 1319/11."
    } ],
    "references" : [ {
      "title" : "Blackwell approachability and low-regret learning are equivalent",
      "author" : [ "J. Abernethy", "P.L. Bartlett", "E. Hazan" ],
      "venue" : "Proceedings of the 24th Conference on Learning Theory (COLT’11), pages 27–46, Budapest, Hungary, June",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Response-based approachability with applications to generalized no-regret problems",
      "author" : [ "A. Bernstein", "N. Shimkin" ],
      "venue" : "To appear in Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Controlled random walks",
      "author" : [ "D. Blackwell" ],
      "venue" : "Proceedings of the International Congress of Mathematicians, volume III, pages 335–338,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1954
    }, {
      "title" : "An analog of the minimax theorem for vector payoffs",
      "author" : [ "D. Blackwell" ],
      "venue" : "Pacific Journal of Mathematics, 6:1–8,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1956
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press, Cambridge, UK,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Cambridge University Press, New York, NY,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Approximation to Bayes risk in repeated play",
      "author" : [ "J. Hannan" ],
      "venue" : "Contributions to the Theory of Games, 3:97–139,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1957
    }, {
      "title" : "A general class of adaptive strategies",
      "author" : [ "S. Hart", "A. Mas-Colell" ],
      "venue" : "Journal of Economic Theory, 98:26–54,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The convex optimization approach to regret minimization",
      "author" : [ "E. Hazan" ],
      "venue" : "S. Sra et al., editor, Optimization for Machine Learning, chapter 10. MIT Press, Cambridge, MA,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Agarwal", "S. Kale" ],
      "venue" : "Machine Learning, 69(2-3):169–192,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Game Theory",
      "author" : [ "M. Maschler", "E. Solan", "S. Zamir" ],
      "venue" : "Cambridge University Press, Cambridge, UK,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Approachability, regret and calibration: Implications and equivalences",
      "author" : [ "V. Perchet" ],
      "venue" : "Journal of Dynamics and Games, 1:181–254,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Approachability, fast and slow",
      "author" : [ "V. Perchet", "S. Mannor" ],
      "venue" : "Proc. COLT 2013: JMLR Workshop and Conference Proceedings, volume 30, pages 474–488,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Variational Analysis",
      "author" : [ "R.T. Rockafellar", "R. Wets" ],
      "venue" : "Springer-Verlag,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning, 4:107–194,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "Proceedings of the 20th International Conference on Machine Learning (ICML ’03), pages 928–936,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The concept of approachability, introduced in [4], addresses a fundamental feasibility issue in for repeated matrix games with vector-valued payoffs.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "An extensive recent survey of approachability and its implications can be found in [12], and a textbook exposition is available in [11].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "An extensive recent survey of approachability and its implications can be found in [12], and a textbook exposition is available in [11].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "Concurrently, Hannan [7] introduced the concept of no-regret play for repeated matrix games.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "The textbook [6] offers a broad overview of regret and online learning.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 14,
      "context" : "Recent surveys of OCO algorithms may be found in [15, 9].",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "Recent surveys of OCO algorithms may be found in [15, 9].",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "This was already observed in [3]; an alternative formulation that leads to more explicit strategies was proposed in [8].",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "This was already observed in [3]; an alternative formulation that leads to more explicit strategies was proposed in [8].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "More recently, it was shown in [1] that any no-regret algorithm for the online linear optimization problem can be used as a basis for an approachability strategy for convex target sets.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "The scheme suggested in [1] first considers target sets S that are convex cones.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Section 3 presents the proposed scheme, in the form of a meta-algorithm that relies on a generic OCO algorithm, discusses the relation to the scheme of [1], and demonstrates a specific algorithm that is obtained by using Generalized Gradient Descent for the OCO algorithm.",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "The approachability strategy introduced by Blackwell has been generalized in [8], that essentially allow different norms to be used for the projection unto S.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Several recent papers have proposed approachability algorithms that depend on Blackwell’s dual condition (condition (ii) in the above Theorem) and avoid the projection step altogether (see [2] and references therein).",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : "We note that the results in [1] are developed for the rewards r(xt, yt), with the mean taken over yt as well, and the agent is allowed to observe Nature’s mixed action yt (or at least the mean reward r(xt, yt)).",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "The OCO problem was introduced in this generality in [16], along with the following Online",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Several classes of OCO algorithms are now known, as surveyed in [6, 15, 9].",
      "startOffset" : 64,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "Several classes of OCO algorithms are now known, as surveyed in [6, 15, 9].",
      "startOffset" : 64,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Several classes of OCO algorithms are now known, as surveyed in [6, 15, 9].",
      "startOffset" : 64,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "11 in [15], which considers the case of fixed regularization parameters, ρt ≡ ρ0.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : ", [5], Section 8.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 0,
      "context" : "Remark 1 (Convex Cones) The approachability algorithm developed in [1] starts with a target sets S that are restricted to be convex cones.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "This is the algorithm proposed in [1].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "For further details see [1].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "25 in [14]) ∂hS(w) = argmax s∈S 〈s,w〉 .",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 9,
      "context" : "We also note that logarithmic convergence rates for OCO algorithms were derived in [10], under strong convexity conditions on the function ft.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "Finally, conditions for fast approachability (of order T−1) were derived in [13], but are of different nature than the above.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "To compute the right-hand side, we first maximize over {w : ‖w‖ = β}, and then optimize over β ∈ [0, 1].",
      "startOffset" : 97,
      "endOffset" : 103
    } ],
    "year" : 2015,
    "abstractText" : "The notion of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions for approachability and corresponding strategies that rely on computing steering directions as projections from the current average payoff vector to the (convex) target set. Recently, Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on the no-regret properties of Online Linear Programming for computing a suitable sequence of steering directions. This is first carried out for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone. In this paper we present a more direct formulation that relies on the support function of the set, along with suitable Online Convex Optimization algorithms, which leads to a general class of approachability algorithms. We further show that Blackwell’s original algorithm and its convergence follow as a special case.",
    "creator" : "LaTeX with hyperref package"
  }
}