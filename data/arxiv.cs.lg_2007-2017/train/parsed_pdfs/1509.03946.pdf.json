{
  "name" : "1509.03946.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Parametric Maxflows for Structured Sparse Learning with Convex Relaxations of Submodular Functions",
    "authors" : [ "Yoshinobu Kawahara", "Yutaro Yamaguchi" ],
    "emails" : [ "ykawahara@sanken.osaka-u.ac.jp,", "yamaguchi@mist.i.u-tokyo.ac.jp," ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Learning with structural information in data has been a primary interest in machine learning. Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].\nRecently, it has been revealed that many of the existing structured sparsity-inducing penalties can be interpreted as convex relaxations of submodular functions [3, 41]. Based on this result, the calculation of the proximal operators for such penalties is known to be reduced to the minimization of separable convex functions over the corresponding submodular polyhedra, which can be solved via the iteration of submodular minimization. However, minimizing a submodular function is not effectively scalable (due to its generality); thus, an unavoidable next step is to clarify when the problem is solvable as a special case that can be calculated faster, especially cases that are solvable as an efficiently solvable class of network flow optimization. Several specific problems are known to be solvable via such network flow optimization. For example, a class of the total variation, which is equivalent to generalized fused Lasso (GFL), is known to be solved via parametric maxflows [10, 19]. Mairal et al. (2011) [36] and Mairal & Yu (2013) [37] proposed parametric maxflow algorithms for l1/l∞-regularization and the path-coding, respectively. In addition, Takeuchi et al. (2015) [49] recently proposed a generalization of GFL to a hyper-graph case, which they call higher-order fused Lasso, with a parametric maxflow algorithm.\nIn this paper, we first develop sufficient conditions for estimating whether a submodular function corresponding to a given structured penalty is graph-representable, i.e., realizable as a projection of a graph-cut function with auxiliary nodes. Several existing structured penalties from submodular functions, such as (overlapping) grouped penalty and (generalized) fused penalty, satisfy these conditions. Then, we show that the parametric maxflow algorithm proposed by Gallo et al. [17] and its variants (hereafter, we call those the GGT-type algorithms) is applicable to calculate the proximal problems for penalties obtained via convex\nar X\niv :1\n50 9.\n03 94\n6v 1\n[ cs\n.L G\n] 1\n4 Se\np 20\nrelaxation of such submodular functions, which runs at the cost of only a constant factor in the worst-case time bound of the corresponding maxflow optimization. Also, we empirically investigate the comparative performance of the proposed framework against existing algorithms.\nThus, the main contribution of this work is two-fold: (i) we develop sufficient conditions (with concrete ways of constructing the corresponding networks) for the class of structured penalties that can be solved via a parametric maxflow algorithm and (ii) we show that an efficient parametric flow algorithm can be applied to the proximal problem for such penalties. Note that the first one is closely related to the class of energy minimization problems that can be solved with the so-called graph-cut algorithm, which has been discussed actively in computer vision [30, 29]. Similar discussions are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 38, 16]. Our current work would give a relation to such discussions to structured regularized learning. And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.\nThe remainder of this paper is organized as follows. We first define notations and describe preliminaries in Section 2. Then, in Section 3, we give a brief review of structured penalties obtained as convex relaxations of submodular functions. In Section 4, we describe the sufficient condition for estimating whether the proximal problem for a given penalty is solvable via network flow optimization. In Section 5, we develop the parametric flow algorithm to proximal problems for penalties satisfying this condition. In Section 6, we describe related work. Finally, we show runtime comparisons for calculating the proximal problem for the penalties by the proposed and existing algorithms in Section 7, and conclude the paper in Section 8. All proofs are given in Appendix C."
    }, {
      "heading" : "2 Notations and Preliminaries",
      "text" : "In this section, we introduce notations used in this paper, and give brief reviews on submodular functions in Section 2.1 and network flow optimization in Section 2.2."
    }, {
      "heading" : "2.1 Submodular Functions",
      "text" : "Let d be a positive integer and V := {1, 2, . . . , d}. We denote the complement of A by A for A ⊆ V , i.e., A = V \\ A. For a real vector w = (wi)i∈V ∈ RV and a subset A ⊆ V , define w(A) := ∑ i∈A wi. A set function F : 2V → R is called submodular if\nF (A) + F (B) ≥ F (A ∩B) + F (A ∪B)\nfor any A,B ⊆ V [11, 14].\nWe denote by F̂ the Lovász extension of a set function F with F (∅) = 0, i.e., F̂ : RV → R is a continuous function defined as, for each w ∈ RV ,\nF̂ (w) := d∑ i=1 wji ( F ({j1, . . . , ji})− F ({j1, . . . , ji−1}) ) ,\nwhere j1, j2, . . . , jd ∈ V are the distinct indices corresponding to a permutation that arranges the entries of w in nonincreasing order, i.e., wj1 ≥ wj2 ≥ · · · ≥ wjd [33]. For a submodular function F with F (∅) = 0, the submodular polyhedron P (F ) ⊆ RV and the base polyhedron B(F ) ⊆ RV are respectively defined as\nP (F ) : = {x ∈ RV | x(A) ≤ F (A) (∀A ⊆ V ) } and B(F ) : = {x ∈ P (F ) | x(V ) = F (V ) }.\nWe define P+(F ) := RV+ ∩ P (F ). For an integer i with 0 ≤ i ≤ d, let ( V i ) denote the set of i-element subsets of V . For any set function F ,\nthere uniquely exist functions F (i) : ( V i ) → R (i = 0, 1, . . . , d) such that\nF (A) = |A|∑ i=0 ∑ Y ∈(Ai ) F (i)(Y ) (A ⊆ V ),\nwhere, for each i = 0, 1, . . . , d, F (i)(A) = ∑ Y⊆A (−1)|A−Y |F (Y ) (A ∈ ( V i ) )\nby the Möbius inversion formula (see, for example, [1]). A set function F is said to be of order k for an integer k with 0 ≤ k ≤ d if F (k) 6= 0 and F (i) = 0 (k + 1 ≤ i ≤ d)."
    }, {
      "heading" : "2.2 Flow Terminology",
      "text" : "Suppose we are given a directed network N = (U,E) with a finite vertex set U and an edge set E ⊆ U × U , a distinguished source vertex s ∈ U , a distinguished sink vertex t ∈ U , and a nonnegative capacity c(u, v) for each edge (u, v) ∈ E. Define c(u, v) := 0 for each pair (u, v) ∈ (U × U) \\ E. A flow f on N is a real-valued function on vertex pairs satisfying the following three constraints:\nf(u, v) ≤ c(u, v) for (u, v) ∈ U × U (capacity), f(u, v) = −f(v, u) for (u, v) ∈ U × U (antisymmetry), and∑\nu∈Uf(u, v) = 0 for v ∈ U \\ {s, t} (conservation). The value of flow f is ∑ v∈U f(v, t). A maximum flow is a flow of maximum value. For disjoint A,B ⊆ V ,\nthe capacity of pair (A,B) is defined as c(A,B) := ∑ u∈A,v∈B c(u, v). A cut (C,C) is a vertex partition (i.e., C ∪ C = U , C ∩ C = ∅) such that s ∈ C and t ∈ C. A minimum cut is a cut of minimum capacity. The capacity constraint implies that for any flow f and any cut (C,C), we have f(C,C) ≤ c(C,C), which implies that the value of a maximum flow is at most the capacity of a minimum cut. The max-flow min-cut theorem of [13] states that these two quantities are equal."
    }, {
      "heading" : "3 Penalties via Convex Relaxation of Submodular Functions",
      "text" : "We briefly review structured penalties through convex relaxations of submodular functions, which cover several known structured sparsity-inducing penalties, in Subsection 3.1, and then the existing optimization methods for those proximal problems in Subsection 3.2."
    }, {
      "heading" : "3.1 Structured Penalties from Submodular Functions",
      "text" : "Structured penalties obtained via convex relaxations of submodular functions can be categorized into two types. Here, we review these respectively in Sections 3.1.1 and 3.1.2.\n3.1.1 Penalty via `p-relaxation of Nondecreasing Submodular Function\nThe first type of the penalty from a submodular function is defined through convex relaxation with the `p-norm [3, 41, 5]. For this type, a submodular function F is required to be non-decreasing. To define this penalty, we first consider a function h : RV → R that penalizes both supports and lp-norm on the supports;\nh(w) = 1\np ‖w‖pp +\n1 r F (supp(w)), (1)\nwhere 1/p + 1/r = 1. Note that when p tends to infinity, function g tends to F (supp(w)) restricted to the l∞-ball. The following is known for any p ∈ (1,+∞).\nProposition 1 ([41]). Let F be a non-decreaing function s.t. F ({i}) > 0 for all i ∈ V . The tightest convex homogeneous lower-bound of h(w) is a norm, denoted by Ω̃F,p, such that its dual norm equals to, for s ∈ RV ,\nΩ̃∗F,p(s) = sup A⊆V,A6=∅ ‖sA‖r F (A)1/r . (2)\nNote that, if F is submodular, then only stable inseparable sets may be kept in the definition of Ω̃∗F,p in Eq. (2). From the above definition, we obtain, for any w ∈ RV ,\nΩ̃F,p(w) = sup s∈RV\nw>s such that Ω∗F,p(w) ≤ 1\n= sup s∈RV\nw>s such that ∀A ⊆ V, ‖sA‖rr ≤ F (A)\n= sup t∈P+(F )\n∑ i∈V t 1/r i |wi|, (3)\nwhere we change the variables as ti = s r i . The first equality is obtained using the Fenchel duality. Consequently, the norm Ω̃F,p is computed with a separable form over (the positive part of) the corresponding submodular polyhedron.\nIt is easy to check that, if we use F (A) = |A|, the Ω̃F,p is equivalent to the `p-regularization. And, if we use F (A) = ∑ g∈G min{|A∩g|, 1} for a group of variables G, then Ω̃f,p is equivalent to the (possibly, overlapping) `1/`∞ and non-overlapping `1/`p group regularizations or provides group sparsity similar to the overlapping `1/`p group regularization [41, 5]."
    }, {
      "heading" : "3.1.2 Penalty by the Lovász Extension of Submodular Function",
      "text" : "The other type of penalty is defined as the Lovász extension, i.e., `∞-relaxation, of a submodular function F with F (∅) = F (V ) = 0. This is known to make some of the components of w equal when used as a regularizer [4]. A representative example of this type of penalty is the generalized fused Lasso (GFL), which is defined for a given undirected network N = (V,E) as\nΩfl(w) = ∑\n(i,j)∈E\naij |wi − wj |,\nwhere aij is the weight on each pair (i, j). This penalty is known to be equivalent to the Lovász extension of a cut function on N , i.e., F (A) = ∑ i∈A,j∈V \\A aij [4, 54]. This can be extended to a hypergraph H = (V,E) with non-negative weight ae for each hyperedge e ∈ E, where the Lovász extension of a hypergraph cut function F (A) = ∑ e∈E:e∩A6=∅,e∩A 6=∅ ae gives the hypergraph regularization Ωhr(w) = ∑ e∈E ae(maxi∈e wi − mini∈e wj) p [22].\nFrom the definition, the Lovász extension of a submodular function with F (∅) = 0 can be represented as a greedy solution over the submodular polyhedron [33], i.e.,\nF̂ (w) = sup t∈P+(F ) ∑ i∈V ti|wi|.\nwhich is in fact the equivalent form with Eq. (3) for r = 1 (i.e., p =∞)."
    }, {
      "heading" : "3.2 Proximal Problem for Submodular Penalties",
      "text" : "The above penalties have a common form, for a (normalized) submodular function F ,\nΩF,p(w) := sup t∈P+(F ) ∑ i∈V t 1/r i |wi|, (4)\nwhere p ∈ (1,+∞) and 1/p + 1/r = 1. However, note that, if F is not nondecreasing, then ΩF,p(w) does not necessarily has the duality as described in Section 3.1. When using the norm ΩF,p as a regularizer, we solve the following problem for some (convex and smooth) loss l : RV → R that corresponds to the respective learning task:\nmin w∈RV\nl(w) + λ · ΩF,p(w) (λ > 0).\nSince the objective of this problem is the sum of smooth and non-smooth convex functions, a major option for its optimization is the proximal gradient method, such as FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) [7]. Thus, our necessary step is to compute iteratively the proximal operator\nproxλΩF,p(z) := argmin w∈Rd\n1 2 ‖z −w‖22 + λ · ΩF,p(w), (5)\nwhere z ∈RV . From the definition (4), we can calculate proxλΩF,p by solving\nmin w∈RV max t∈P+(F )\n1 2 ‖w − z‖22 + λ ∑ i∈V t 1/r i |wi| = max t∈P+(F ) ∑ i∈V min wi∈R { 1 2 (wi − zi)2 + λt1/ri |wi| } = − min\nt∈P+(F ) ∑ i∈V ψi(ti), (6)\nwhere ψi(ti) = −minwi∈R{ 12 (wi − zi) 2 + λt 1/r i |wi|}. Thus, solving the proximal problem equals minimizing a separable convex function over the submodular polyhedron.\nBased on the above formulation, Obozinski & Bach (2012) [41] recently suggested a divide-and-conquer algorithm as an adaptation of the decomposition algorithm by Groenevelt (1991) [21] for penalties from general submodular functions (for the case of p = 2). A more general version of this approach was also developed by Bach (2013) [5]. However, a straightforward implementation of this approach yields O(d)-time calculation of submodular minimization, which could be time-consuming especially in large problems.\nWe address this issue by considering it from the following two perspectives. First, in Section 4, we develop an explicit sufficient conditions for determining whether the proximal problem for a given penalty can be solved through maximum flow optimization rather than submodular minimization. Maximum flow optimization can be regarded as an efficiently-solvable special case of submodular minimization, and is known to be much faster than submodular minimization in general; thus, this could be useful to judge whether a given penalty can be dealt with in a scalable manner as a regularizer. The respective structured penalties from submodular functions mentioned above are in fact instances of this case. On that basis, in Section 5, we develop a procedure for problem (5) that runs at the cost of only a constant factor in its worst-case time bound of the maxflow calculation rather than the O(d)-time calculation of the straightforward implementation. In other words, we discuss whether an efficient parametric maxflow algorithm is applicable to the current problem."
    }, {
      "heading" : "4 Graph-Representable Penalties",
      "text" : "In this section, we develop sufficient conditions for determining whether the proximal problem for a given structured penalty is solvable through an efficiently-solvable class of network flow optimization. We also describe a concrete procedure to construct the corresponding network."
    }, {
      "heading" : "4.1 Graph-Representable Set Functions",
      "text" : "The currently-known best complexity of minimizing a general submodular function is O(d6 + d5 EO), where EO is the cost of evaluating a function value [42]. Although there exist practically faster algorithms, such as the minimum-norm-point algorithm [15] as well as faster algorithms for special cases (e.g., Queyranne’s algorithm for symmetric submodular functions [44]), their scalability would not be practically sufficient, especially if we must solve submodular minimization several times, which is the current case. In addition,\nit is well known that a cut function (which is almost equivalent to a second order submodular function [16]) can be minimized much faster through calculation of maxflows over the corresponding network. Given a directed network N = (V,E) with nonnegative capacity c(e) on each edge e ∈ E, a cut function κN : V → R is defined as\nκN (A) := ∑ { c(e) | e ∈ δoutN (A) } (A ⊆ V ),\nwhere δoutN (A) denotes the set of edges leaving A in N . If N consists of d nodes and m edges, the currently best runtime bound for the minimization is O(md) [43]. Albeit it is a better run-time bound, the empirical complexity is often much better with practical fast algorithms, e.g., [18, 9].\nHowever, the expressive power of a cut function is limited. Therefore, in order to balance between expressiveness and computational simplicity, using a higher-order function that is represented as a cut function with auxiliary nodes is often helpful. Such a function is sometimes referred to as graph-representable [26],1 and defined as follows. Let U = V ∪W ∪ {s, t} for some finite set W with W ∩ V = ∅ and distinct elements s, t 6∈ V ∪W , and let Ñ = (U, Ẽ) be a directed network with nonnegative capacity c(e) on each edge e ∈ Ẽ. Then, define a set function F : 2V → R as\nF (A) := min Y⊆W κÑ ({s} ∪A ∪ Y ) + CF (A ⊆ V ),\nwhere CF ∈ R is an arbitrary constant, and such F is said to be graph-representable. If W is empty, this function coincides with a cut function. The submodularity of this function is derived from the classical result of Megiddo (1974) [38] on network flow problems with multiple terminals (see, for the proof, [40])."
    }, {
      "heading" : "4.2 Sufficient Conditions and Network Construction",
      "text" : "As described in Section 5, if the corresponding set function F for norm ΩF,p is graph-representable, then its proximal problem (5) can be efficiently solved through a parametric maxflow computation. Hereafter, we refer to such a penalty as a graph-representable penalty, which is defined as follows.\n1This class of functions is closely related to the class of energy minimization problems that can be solved by the so-called graph-cut algorithm [9, 30]. Related results are also found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].\nDefinition 2 (Graph-representable penalty). A penalty defined in Proposition 1 is said to be graph-representable if the set function F on supports is graph-representable.\nHere, we present three types of sufficient conditions for a penalty ΩF,p from a given submodular function F (as described in Section 3.1) to be graph-representable by constructing networks representing F . The first one is mentioned as “truncations,” where a function F is graph-representable by just one additional node (see Figure 1(a) and also refer [26] or [40]). The second one is closely related to [8], and the third one is derived from [16], for which we describe concrete procedures to construct networks (see Figure 1 for the construction).\nTheorem 3. A set function F with one of the following conditions is graph-representable.\n(i) F (A) = min{w(A), y} for some w ∈ RV+ and y ∈ R+. (ii) F is submodular and of order at most three, i.e., F (i) = 0 for i = 4, 5, . . . , d.\n(iii) F has no positive term of order at least two, i.e., F (i) ≤ 0 for i = 2, 3, . . . , d.\nRemark. It should be noted that the sum of graph-representable submodular functions is also graphrepresentable by considering the union of the corresponding networks."
    }, {
      "heading" : "4.3 Examples",
      "text" : "A submodular function F (A) = ∑ g∈G min{|A∩g|, 1}, which gives the grouped-type regularization, is graphrepresentable since each term min{|A∩g|, 1} = min{eg(A), 1} is guaranteed to be so from Condition (i). The cost for constructing the corresponding network for this function is O(|G|) and the number of the additional nodes is |G|. Condition (ii) is a generalization of the condition that a cut function F (A) = ∑ i∈A,j∈V \\A aij for a network (V,E) can be solved with maximum flows, i.e., positive weights aij for all i, j ∈ E. Besides, a hypergraph cut function F (A) = ∑ e∈E:e∩A 6=∅,e∩A6=∅ ae is also confirmed as graph-representable as follows. For each hyperedge e ∈ E, define Fe,1(A) := ae ·min{|A ∩ e|, 1}, Fe,2(A) := −ae if e ⊆ A, and Fe,2(A) := 0 otherwise. Then, F (|e|) e,2 (e) = −ae and F (|A|) e,2 (A) = 0 for A 6= e. Hence, Fe,1 and Fe,2 satisfy\nConditions (i) and (iii), respectively, and it is easy to see F = ∑ e∈E(Fe,1 +Fe,2). The network construction\nrequires O(‖E‖) time, where ‖E‖ = ∑ e∈E |e|."
    }, {
      "heading" : "5 Parametric Maxflows for proxΩF,p(z)",
      "text" : "We describe how the proximal problem (5) for a network representable penalty is solvable with an adaptation of the GGT-type algorithms. We first derive a parametric formulation of this problem in Subsection 5.1, and then develop the algorithm in Subsection 5.2."
    }, {
      "heading" : "5.1 Parametric Formulation",
      "text" : "We address a parametric formulation of problem (6). To this end, we first consider\nmin τ∈B+(F )\n∑ i∈V ψi(τi). (7)\nNote that the above optimization is over B+(F ) in place of P+(F ). In the following parts of this section, we suppose that F is non-decreasing (thus, B+(F ) coincides with B(F )). Although this does not necessarily hold for our case, we can show the following:\nLemma 4. Let b ∈ RV and F be submodular, and set β := supi∈V {0, F (V \\ {i})−F (V )}/bi. Then, F +βb is a nondecreasing submodular function. Also, τ ∗ is optimal to problem (7) for F if and only if τ ∗ + βb is optimal to problem (7) for F + βb.\nThus, for F that is not non-decreasing, we can apply the algorithm developed below and recover an optimal solution to the original problem by transforming it to a non-decreasing one as in this lemma.\nFirst, we define an interval J ∈ R as\nJ := ⋂ i∈V {ψ′i(τi) | τi ∈ (domψi ∩ R+) } (= (−∞, 0]).\nLet τ ∗ be an optimal solution to problem (7). Denote the distinct values of ψ′i(τ ∗ i ) by ξ ∗ 1 < · · · < ξ∗k, and let ξ∗0 := −∞ and ξ∗k+1 := +∞. Let A∗j := { i ∈ V | ψi(τ∗i ) ≤ ξ∗j } for j = 0, 1, . . . , k + 1. Also, let\nFα(A) := F (A)− ∑ i∈Aφi(α) (α ∈ J),\nwhere φi(α) = ψ ′−1 i (α) (α ∈ J \\ {0}) or (|zi|/λ)r (α = 0), and •−1 means an inverse function.\nLemma 5. Let α ∈ J . If ξ∗j < α < ξ∗j+1, A∗j is a minimizer of Fα. If α = ξ∗j , A∗j−1 is a minimal minimizer and A∗j is a maximal minimizer of Fα.\nThis is obtained, in Lemma 4 of [39], by replacing the assumption on the strict convexity of ψ′i with the monotonisity of the function in the region under consideration. As discussed in [39], this lemma implies that problem (6) can be reduced to the following parametric problem:\nmin A⊆V\nFα(A) for all α ∈ J. (8)\nThat is, once we have the chain of solutions A∗0 ⊂ · · · ⊂ A∗k+1 to problem (8) for all α ∈ J , we can obtain an optimal solution to problem (7) as for j = 0, . . . , k\nτ∗i = φi(α ∗ j+1) (i ∈ A∗j+1 \\A∗j ) with α∗j+1 s.t. F (A∗j+1)− F (A∗j ) = ∑ i∈A∗j+1\\A∗j φi(α). (9)\nThe key here is that, if function F is graph-representable, problem (8) can be solved as a parametric minimum-cut (equivalently, a parametric maxflow) problem on Ñ , where c(s, v) for v ∈ V are functions of α (since φi(α) ≥ 0 for α ∈ J), as will be stated in the next subsection. Once we have a solution τ ∗ to problem (7), we can then obtain a solution to problem (5) as follows.\nCorollary 6. If τ ∗ be an optimal solution to problem (7), then the one to problem (5) is given by\nw∗i = { zi − sign(zi)λ(max(τ∗i , 0))1/r if 0 ≤ τ∗i ≤ (|zi|/λ)r, 0 otherwise."
    }, {
      "heading" : "5.2 Algorithm Description",
      "text" : "As mentioned above, if the penalty is network representable, then problem (8) is solved as a parametric maxflow problem on network Ñ , where capacities cα(s, v) for v ∈ V are cα(s, v) = (φi(α) + const.) and the others are constants for α (note that φi(α) ≥ 0 for α ∈ J). Since ψi is convex, those capacities satisfy the conditions of the monotone source-sink class of problems, i.e.,\n1. c(s, v) is a non-decreasing function of α for all v ∈ U ,\n2. c(v, t) is a non-increasing function of α for all v ∈ U , and\n3. c(u, v) is constant for all u, v ∈ U \\ {s, t}.\nTherefore, for a given on-line sequence of parameter values α1 < · · · < αk, there exists a parametric maxflow algorithm that computes minimum cuts (A1, A1), · · · , (Ak, Ak) on the network such that A1 ⊆ · · · ⊆ Ak, and runs at the cost of only a constant factor in the worst-case time bound of a single maxflow computation.\nAlgorithm 1 Parametric preflow algorithm for the computation of proxλΩF,p(z). Input: z ∈Rd, Ñ = (U,E). Output: w∗= proxλΩF,p(z). 1: Compute α0 as in Eq. (10) and set αk+1 ← 0. Compute maximum flows f0 and fk+1, and minimum\ncuts (C0, C0) and (Ck+1, Ck+1) for α0 and αk+1 such that |C0| and |Ck+1| are maximum and minimum by applying the preflow algorithm to Ñ , respectively. Form N ′ from Ñ by shrinking the nodes in C0 and in Ck+1 to single nodes respectively, eliminating loops, and combining multiple arcs by adding their capacities. 2: If N ′ has at least three vertices, let f ′0, f ′k+1 be respectively the flows in N ′ corresponding to f0, fl+1. Then, perform Slice(N ′, α0, αk+1, f ′0, f ′k+1, C0, Ck+1). 3: Compute w∗ as in Corollary 6 and return w∗.\nProcedure Slice(N , αl, αu, fl, fu, Al, Au) 1: Find α̃ such that cα̃({s}, U \\ {s}) = c(U \\ {t}, {t}) (cf. Lemma 7). 2: Run the preflow algorithm for α̃ on N starting with the preflow f ′l formed by increasing fl on arcs (s, v)\nto saturate them and decreasing fl on arcs (v, t) to meet the capacity constraints for v ∈ U . As an initial valid labeling, use d(v)=min{df ′l (v, t), df ′l (v, s)+(|U |−2)}. Find the minimal and maximal minimum cuts (C,C) and (C ′, C ′ ) for α̃, respectively.\n3: If C ′ = {t}, set τ ∗Au\\Al←F (Au)−F (Al). Otherwise, run Slice(N (C ′), α̃, αu, f̃ , fu, C,Au). And if C 6= {s}, then run Slice(N (C), αl, α̃, fl, f̃ , Al, C ′ ).\nIf parametric capacities in the monotone source-sink class of problems are linear for α, all breakpoints, i.e., a value of parameter α at which the capacity for the corresponding cut changes, can also be found at the cost of a constant factor in the worst-case time bound of a single maxflow computation using the GGT-type algorithms. However, this is generally not true for non-linear capacities because we must solve nonlinear equations to identify such a parameter value [20]. Although this is the case for our situation in general, we can find such a value in closed-form for the important cases p = 2,+∞ due to its specific form of the problem.\nLemma 7. For network Ñ corresponding to a graph-representative penalty, the value of α such that∑ v∈V cα(s, v) = ∑ v∈U\\{t}c(v, t)− ∑ v∈U\\V c(s, v)\nis found in close form for p = 2,+∞.\nThe concrete derivations of these closed-forms are described in Appendix B. For the other cases, we can at least apply some line search for finding such value of α due to the monotonicity of φi. Thus, we can adapt the procedure of the GGT-type algorithms to find the chain of solutions A1 ⊆ · · · ⊆ Ak, which results in giving an optimal solution to problem (5), as shown in Algorithm 1 (a brief review on the preflow-push algorithm used in Algorithm 1 is given in Appendix A).\nTheorem 8. Algorithm 1 is correct, and runs at the cost of a constant factor in the worst-case time bound of a single maxflow computation. For example, it runs in O(dm log(d2/m)) with dynamic trees.\nThat is, although the preflow algorithm is applied several times, the total runtime of Algorithm 1 is equivalent to that of a single application of the preflow algorithm to the original network.\nThe interval (α0, αk+1) is chosen such that it covers all possible breakpoints α1, . . . , αk. In other words, it suffices to select a sufficiently small α0 so that for each vertex v such that (s, v) is of nonconstant capacity, cα0(s, v) + ∑ u∈U\\{s,t}c(u, v)<c(v, t), which is given as\nα0 ← ψ′i(minv∈V {c(v, t)− ∑ u∈V \\{s,t}c(u, v)})− 1. (10)\nSimilarly, it suffices to select αk+1 sufficiently large so that for each vertex v such that (s, v) is of nonconstant capacity, c(v, t)+ ∑ u∈U\\{s,t} c(v, u)<cαk+1(s, v), which is obtained as αk+1 ← 0.\nBy following the above results, any GGT-type algorithms can be adapted to solve the problem (8). Algorithm 1 shows an adaptation of the simplified version [2] of the original GGT algorithm."
    }, {
      "heading" : "6 Related Work",
      "text" : "Learning with structured sparsity-inducing regularization has been actively discussed in machine learning for a decade. Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6]. Generalized fused Lasso is closely related to the so-called total variation, which has often been discussed in computer vision [45]. Recently, group penalties have been applied to more complex groups, such as hierarchical penalty [57, 27] and path penalty [37]. The total variation regularization is known to be solvable with an efficient parametric maxflow algorithm [10, 19]. In addition, the proximal problem for l1/l∞-group penalty is calculated via parametric maxflow optimization [36]. The proposed optimization formulation includes these formulations as special cases. Bach (2010) [3] and Bach & Obozinski (2012) [41] revealed that many of the existing structured penalties are obtained as convex relaxations of submodular functions, and those proximal problems are formulated as separable convex minimization.\nThe sufficient condition in Section 4 is closely related to the class of energy minimization problems solvable by graph-cut algorithm [9, 30]. Energy minimization is a formulation of the maximum a posteriori (MAP) estimation on MRFs (see, for example, [53]). Similar results are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].\nAlgorithm 1 is a divide-and-conquer implementation of the preflow algorithm proposed by Gallo & Tarjan (1988) [18]. Bach (2010) [3] and Bach & Obozinski (2012) [41] have mentioned an application of a divide-and-conquer approach to separable convex minimization proposed by Groenevelt (1991) [21] for proximal problem (5), which takes O(d) times of the cost for submodular minimization. Algorithm 1 takes the cost for only a single run of the preflow algorithm by adapting Gallo et al. (1989) [17]’s algorithm to the current problem."
    }, {
      "heading" : "7 Runtime Comparisons",
      "text" : "Here, we show empirical runtime comparisons of our algorithm with some existing ones based on different principles to see the scalabilities of the algorithms. The experiments were run on a 2.6 GHz 64-bit workstation using C++. We applied our algorithm (we refer it as ‘PARA’) to the proximal problems for the penalty from F (A) = ∑ g min{|A ∩ g|, 1} (p = 2,∞) (as a typical example of penalties described in 3.1.1) and the (generalized) fused penalty (as one described in 3.1.2). We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (‘DA-MNP’/’DA-MF’) and the algorithm by [36] (‘MJOB’)2 for the penalties from F (A) = ∑ g min{|A ∩ g|, 1} (MJOB is applicable only for p=∞), and the MNP algorithm (‘MNP’), the algorithm by Tibshirani & Taylor (2011) [51] (‘TT’) and the one by Liu et al. (2010) [32] (‘LYY’)3 for the (generalized) fused penalty (LYY is applicable only to the 1d fused case).\nWe generated data as follows. First, we generated a random vector z ∈ Rd from the uniform distribution in [−1, 1]d. For generalized fused penalty, we randomly generated a directed network over nodes corresponding to V using GENRMF from DIMACS Challenge.4 And for generating overlapping groups, we randomly generated d/20–d/10 groups of size 30–100. The graphs in Figure 2 show the empirical runtimes (in logarithm scale) for the algorithms. The plotted points are the averaged values over 10 randomly generated datasets."
    }, {
      "heading" : "8 Conclusions",
      "text" : "In this paper, we provided a comprehensive class of structured penalties for which the proximal problem can be solved via an efficiently-solvable class of parametric maxflow optimization. Then, we showed that the parametric maxflow algorithm by Gallo et al. (1989) [17] and its variants, which runs at the cost of a constant\n2We used the code modified from the one available at http://spams-devel.gforge.inria.fr/ 3We used the code available at http://www.yelab.net/software/SLEP/ 4The first DIMACS Int’l Algorithm Implementation Challenge (http://dimacs.rutgers.edu/Challenges/).\nfactor in the worst-case time bound of the corresponding maxflow optimization, is applicable to solve this problem. The runtime of the proposed algorithm was empirically compared to those of the state-of-the-art ones.\nSeveral avenues would be worth investigating: First, our formulation does not include the type of sparsity by the latent group penalties, such as [25]. As mentioned in [37], the proximal problem for the penalties of Jacob et al. (2009) [25] and its generalization can be solved as a minimum-cost flow problem, which is known to be calculated as a parametric maxflow problem if the costs are quadratic and only on edges connected to source/sink [23]. It would be important to consider an unified framework connecting the current and such problems in the future work. Also, it would be interesting to address a new structured penalty satisfying the developed condition for some specific application."
    }, {
      "heading" : "A Review of the Preflow-Push Algorithm",
      "text" : "The preflow algorithm computes a maximum flow in a directed network N [18]. We first define terminology to describe the algorithm. A preflow f on N is a real-valued function on vertex pairs satisfying the capacity constant, the antisymmetry constraint, and the following relaxation of the conservation constraint∑\nv1∈U f(v1, v2) ≥ 0 for all v2 ∈ V \\ {s, t}. (11)\nFor a given preflow, we define the excess e(v) of a vertex v to be ∑ u∈U f(u, v) if v 6= s, or infinity if v = s. We call a vertex v 6= {s, t} active if e(v) > 0. A preflow is a flow if and only if Eq. (11) holds with equality for all v 6= {s, t}, i.e., e(v) = 0 for all v 6= {s, t}. A vertex pair (v, u) is a residual arc for f if (v, u) < c(v, u).\nA path of residual arcs is a residual path. A valid labeling d for a preflow f is a function from the vertices to the nonnegative integers and infinity, such that d(t) = 0, d(s) = n, and d(v) ≤ d(u) + 1 for every residual arc (v, u). The residual distance df (v, u) from v to u is the minimum number of arcs on a residual path from v to u, or infinity if there is no such a path.\nTo implement the preflow algorithm, we use the incidence list I(v) for each vertex v. The elements of I(v) are the unordered pairs {v, u} such that (v, u) ∈ E or (u, v) ∈ E. The algorithm consists of repeating the following procedure until no active vertices exist. Select any active vertex v1. Let (v1, v2) be the current edge of v1. Then, apply the appropriate one of the following three cases.\nPush: If d(v1) > d(v2) and f(v1, v2) < c(v1, v2), send δ = min{e(v1), c(v1, v2)− f(v1, v2)} units of flow from v1 to v2, by increasing f(v1, v2) and e(v2) by δ, and by decreasing f(v1, v2) and e(v1) by δ.\nGet Next Edge: If d(v1) ≤ d(v2) or f(v1, v2) = c(v1, v2), and (v1, v2) is not the last edge in I(v1), replace (v1,v2) as the current edge of v1 with the next in I(v1).\nRelabel: If d(v1) ≤ d(v2) or f(v1, v2) = c(v1, v1), and (v1, v2) is the last edge in I(v1), replace d(v1) by min{(v1, v2) ∈ I(v1), f(v1, v2) < c(v1, v2)}+1 and make the first edge in I(v1) the current edge of v1.\nWhen the algorithm terminates, f is a maximum flow. A minimum cut can be computed, after replacing d(v) by min{df (v, s) +n, df (v, t)} for each v ∈ V , as (A,A) such that A = {v|d(v) ≥ n}, where the sink side A is of minimum. The worst-case total time is O(dm log(d2/m)) if we use dynamic trees for the selection of active vertices."
    }, {
      "heading" : "B Details of Algorithm 1",
      "text" : "In this appendix, we describe the details of Algorithm 1 for solving the proximal problem (5). Especially, we give the closed-form solutions for finding α described in Lemma 7 for p = 2,∞ (i.e., r = 1, 2), which is the key to make the complexity of Algorithm 1 equivalent to the original GGT-type algorithm.\nFirst, from the definition (see, Eq. (6)), function ψi(τi) is represented as\nψi(τi) =\n{ 1 2λ 2τ 2/r i − λτ 1/r i |zi| (0 ≤ τi ≤ (|zi|/λ) r )\n− 12z 2 i ((|zi|/λ) r < τi).\nNote that this function is non-increasing for τi (for τi such that 0 ≤ τi ≤ (|zi|/λ)r, it is monotone). The derivative is given by\nψ′i(τi) =  λ2τ 1/r i −λ|zi| rτ 1−1/r i (0 ≤ τi ≤ (|zi|/λ)r)\n0 ((|zi|/λ)r < τi). (12)\nThis derivative is a non-decreasing function for τi (for τi such that 0 ≤ τi ≤ (|zi|/λ)r, it is monotone). Hence, ψ′i has an inverse function for 0 ≤ τi ≤ (|zi|/λ) r .\nTo give an closed-form solution for α as in Eq. (9) and in Lemma 7, it is sufficient to describe how we can find α̃ satisfies for S ⊆ V ∑\ni∈Sφi(α̃) = c̃,\nwhere c̃ is some constant, which is stated in following parts for p = 2,∞, respectively.\nCase for p = 2 (r = 2) By substituting r = 2 into Eq. (12), we have for 0 ≤ τi ≤ (|zi|/λ)2\nψ′i(τi) = λ\n2\n( λ− |zi|/τ1/2i ) .\nTherefore, τ̃i and τ̃j such that ψ ′ i(τ̃i) = ψ ′ j(τ̃j) satisfy\n|zi|2τ̃j = |zj |2τ̃i.\nThis means that, if α̃ satisfies ∑ i∈S φi(α̃) = c̃, then we have\nφi(α̃) = |zi|2∑ j∈S |zj |2 c̃.\nThus, we can calculate such α̃ as\nα̃ = ψ′i ( |zi|2c̃/ ∑ j∈S |zj | 2 ) .\nCase for p = +∞ (r = 1) By substituting r = 1 into Eq. (12), we have for 0 ≤ τi ≤ |zi|/λ\nψ′i(τi) = λ(λτi − |zi|).\nThus, τ̃i and τ̃j such that ψ ′ i(τ̃i) = ψ ′ j(τ̃j) satisfy\n|zi| − |zj | = λ(τ̃i − τ̃j).\nThis means that, if α̃ satisfies ∑ i∈S φi(α̃) = c̃, then we have\nφi(α̃) = c̃ d + |zi| − ∑ j∈S |zj |/|S| λ .\nHence, we can calculate such α̃ as\nα̃ = ψ′i\n( c̃\nd + |zi| − ∑ j∈S |zj |/|S| λ ) ."
    }, {
      "heading" : "C Proofs",
      "text" : "Theorem 3\n(i) Let Ñ be the constructed network (see Figure 1-(a)) with the additional node u. Then, for each A ⊆ V , we have κÑ ({s}∪A) = w(A) and κÑ ({s}∪A∪{u}) = y. Hence, the constructed network indeed represents F (A) = min{w(A), y}.\n(ii) Let Ñ = (U = V ∪W ∪ {s, t}, Ẽ) be the constructed network (see Figures 1-(c),(d)). We show that F (A) = minY⊆W κÑ ({s} ∪ A ∪ Y ) − κÑ ({s}) + F (∅) for every A ⊆ V . It is easy to confirm that, for each A ⊆ V , the set {wB ∈ W | B ⊆ A } attains the minimum of minY⊆W κÑ ({s} ∪ A ∪ Y ). When A = ∅, the minimum value is indeed κÑ ({s}). Besides, when ∅ 6= A ⊆ V , it increases by ∑ v∈V max{0, F (1)(v)}\nand decreases by ∑ v∈V max{0,−F (1)(v)} and ∑ A{−F (|A|)(A) | A ⊆ V with |A| ≥ 2 }, which implies that\nminY⊆W κÑ ({s} ∪A ∪ Y ) = κÑ ({s}) + ∑ A{F (|A|)(A) | ∅ 6= A ⊆ V } = κÑ ({s}) + F (A)− F (∅).\n(iii) For a fixed set function F satisfying Condition (iii), we construct a directed network Ñ = (U = V ∪W ∪ {s, t}, Ẽ) with nonnegative capacity c : Ẽ → R+ as follows. Then, Ñ coincides with the network just before Step 4 in the construction procedure in Section 4.2 (up to modular terms), and we have F (A) = minY⊆W κÑ ({s} ∪A ∪ Y ) for every A ⊆ V . First, we define W as the union of the following:\nW2 := {wA | A ∈ ( V 2 ) },\nW+3 := {wA | A ∈ ( V 3 ) with F (3)(A) > 0 },\nW−3 := {wA | A ∈ ( V 3 ) with F (3)(A) < 0 },\nwhere each wA is an additional node adjacent to the nodes in A. Next, we define Ẽ as the union of the following:\nE+1 := V × {t}, E − 1 := {s} × V, E2 := {s} ×W2, E21 := { (wA, v) | wA ∈W2, v ∈ A }, E+3 := W + 3 × {t}, E13 := { (v, wA) | wA ∈W + 3 , v ∈ A }, E−3 := {s} ×W − 3 , E31 := { (wA, v) | wA ∈W − 3 , v ∈ A }.\nLet us define a set function H : 2V → R as\nH(A) := ∑ B{F (3)(B) | A ⊆ B ⊆ V, wB ∈W+3 } (A ⊆ V ),\nand the capacity function c : E → R+ as, for each e ∈ E,\nc(e) :=  max{0, F (1)({v})−H({v})} (e = (v, t) ∈ E+1 ) max{0,−F (1)({v}) +H({v})} (e = (s, v) ∈ E−1 ) −F (2)(A)−H(A) (e = (s, wA) ∈ E2) F (3)(A) (e = (wA, t) ∈ E+3 ) −F (3)(A) (e = (s, wA) ∈ E−3 ) +∞ (e ∈ E21 ∪ E13 ∪ E31).\nThe nonnegativity of c is guaranteed by the submodularity of F as follows: for any A = {u, v} ⊆ V with |A| = 2, we have\n0 ≤ min B {F (B \\ {u}) + F (B \\ {v})− F (B)− F (B \\ {u, v}) | A ⊆ B ⊆ V }\n= min B { − ∑ B′ {F (|B ′|)(B′) | A ⊆ B′ ⊆ B } ∣∣∣∣∣ A ⊆ B ⊆ V }\n= min B\n{ −F (2)(A)−\n∑ B′ { F (3)(B′) ∣∣∣ A ⊆ B′ ∈ (B 3 )} ∣∣∣∣∣ A ⊆ B ⊆ V }\n= −F (2)(A)− max B̃⊆V \\A ∑ v∈B̃ F (3)(A ∪ {v}) = −F (2)(A)−H(A).\nWe first check the value of minY⊆W κN (Y ∪{s}). If Y ∩ (W2 ∪W−3 ) 6= ∅, then at least one edge in E21 ∪E31 contributes to the cut capacity, which makes it +∞. Otherwise (i.e., if Y ⊆W+3 ), as no edge in E is from s to W+3 , we have κN (Y ∪ {s}) ≥ κN ({s}) for any Y ⊆ W + 3 , which means that Y = ∅ attains the minimum value. Without loss of generality, we assume F (0)(∅) = F (∅) = κN ({s}) (i.e., CF = 0), Then, it suffices to show that F (A) = minY⊆W κÑ (A ∪ Y ∪ {s}) for each nonempty A ⊆ V . For any B ⊆ V with wB ∈ W2 ∪W−3 , only the edge (s, wB) enters wB , and the edges (wB , v) (v ∈ B) with c(wB , v) = +∞ leave wB . Therefore, if B ⊆ A, we have\nκN (A ∪ Y ∪ {s, wB}) = κN (A ∪ Y ∪ {s})− c(s, wB) ≤ κN (A ∪ Y ∪ {s})\nfor every Y ⊆ W \\ {wB}. Moreover, for any B ⊆ V with wB ∈ W+3 , only the edge (wB , t) leaves wB , and the edges (v, wB) (v ∈ B) with c(v, wB) = +∞ enter wB . Thus, if B ∩A 6= ∅, we have wB ∈ Y and the edge (wB , t) contributes to the cut capacity. Thus, the minimum value is attained by Y := {wB ∈ W2 ∪W−3 |\nB ⊆ A } ∪ {wB ∈W+3 | B ∩A 6= ∅ }, and we have\nκÑ (A ∪ Y ∪ {s})− κÑ ({s}) = ∑ v∈A (c(v, t)− c(s, v))− ∑\nwB∈W2∪W−3 : B⊆A\nc(s, wB) + ∑\nwB∈W+3 : B∩A6=∅\nc(wB , t)\n= ∑ v∈A (F (1)(v)−H(v)) + ∑ wB∈W2 : B⊆A (F (2)(B) +H(B)) + ∑\nwB∈W−3 : B⊆A\nF (3)(B) + ∑\nwB∈W+3 : B∩A 6=∅\nF (3)(B)\n= ∑\nB : ∅6=B⊆A\nF (|B|)(B) + ∑\nwB∈W+3 : B∩A6=∅6=B\\A\nF (3)(B) − ∑ v∈A H(v) + ∑ wB∈W2 : B⊆A H(B)\n= F (A)− F (0)(∅),\nwhich means κÑ (A∪Y ∪{s}) = F (A). To see the last equality, it suffices to count the contribution of F (3)(B′) to the second to the last line, which is easily seen to be totally zero, for each B′ ⊆ V with wB′ ∈W+3 .\nLemma 4\nThe first is shown in Lemma 2 and 3 in [40] or Proposition 2.5 in [5]. The equivalence of optimal solutions to the two problems is obvious.\nCorollary 6\nFirst, from Proposition 8.8 in [5], we obtain a solution to problem (6) as\nt∗i = { τ∗i if (|zi|/λ)r > τ∗i , sign(zi)(|zi|/λ)r otherwise.\n(13)\nAlthough the proposition assumes the strict convexity on separable functions, the above can be obtain since (−ψi)′ is monotone for τi s.t. (|zi|/λ)r > τ∗i . Then, the corollary follows by solving analytically the minimization w.r.t. w in the definition of ψi.\nLemma 7\nThe statement of this lemma is shown by Appendix B.\nTheorem 8\nThe correctness follows the monotone source-sink property of the current network. The runtime follows the analysis in [17] from Lemma 7."
    } ],
    "references" : [ {
      "title" : "Combinatorial Theory",
      "author" : [ "M. Aigner" ],
      "venue" : "Springer–Verlag,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Experimental evaluation of parametric max-flow algorithms",
      "author" : [ "M. Babenko", "J. Derryberry", "A. Goldberg", "R. Tarjan", "Y. Zhou" ],
      "venue" : "Proc. of the 6th Int’l WS on Experimental Algorithms, pages 256–269,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Structured sparsity-inducing norms through submodular functions",
      "author" : [ "F. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 23, pages 118–126.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Shaping level sets with submodular functions",
      "author" : [ "F. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 24, pages 10–18.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning with submodular functions: A convex optimization perspective",
      "author" : [ "F. Bach" ],
      "venue" : "Foundations and Trends in Machine Learning, 6(2–3):145–373,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Structured sparsity through convex optimization",
      "author" : [ "F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski" ],
      "venue" : "Statistical Science, 27(4):450–468,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal of Image Science, 2(1):183–202,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Maximizing a supermodular pseudoboolean function: A polynomial algorithm for supermodular cubic functions",
      "author" : [ "A. Billionnet", "M. Minoux" ],
      "venue" : "Discrete Applied Mathematics, 12(1):1–11,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Fast approximate energy minimization via graph cuts",
      "author" : [ "Y. Boykov", "O. Veksler", "R. Zabih" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(9):1222–1239,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "On total variation minimization and surface evolution using parametric maximum flows",
      "author" : [ "A. Chambolle", "J. Darbon" ],
      "venue" : "International Journal of Computer Vision, 84:288–307,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Submodular functions, matroids, and certain polyhedra",
      "author" : [ "J. Edmonds" ],
      "venue" : "Combinatorial structures and their applications, pages 69–87,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Discovering sociolinguistic associations with structured sparsity",
      "author" : [ "J. Eisenstein", "N.A. Smith", "E.P. Xing" ],
      "venue" : "Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT’11), pages 1365–1374,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Flows in Networks",
      "author" : [ "L.R. Ford", "D.R. Fulkerson" ],
      "venue" : "Princeton University Press,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "Submodular Functions and Optimization",
      "author" : [ "S. Fujishige" ],
      "venue" : "Elsevier, 2nd edition,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The minimum-norm-point algorithm applied to submodular function minimization and linear programming",
      "author" : [ "S. Fujishige", "T. Hayashi", "S. Isotani" ],
      "venue" : "Report RIMS-1571, Kyoto University,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Realization of set functions as cut functions of graphs and hypergraphs",
      "author" : [ "S. Fujishige", "S.B. Patkar" ],
      "venue" : "Discrete Mathematics, 226(1-3):199–210,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A fast parametric maximum flow algorithm and applications",
      "author" : [ "G. Gallo", "M.D. Grigoriadis", "R.E. Tarja" ],
      "venue" : "SIAM Journal of Computing, 18(1):30–55,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A new approach to the maximum-flow problem",
      "author" : [ "A. Goldberg", "R. Tarjan" ],
      "venue" : "J. ACM, 35(4):921–940,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Parametric maximum flow algorithms for fast total variation minimization",
      "author" : [ "D. Goldfarb", "W. Yin" ],
      "venue" : "SIAM journal of Scientific Computing, 31(5):3712–3743,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Structural and algorithmic properties for parametric minimum cuts",
      "author" : [ "F. Granot", "S.T. McCormick", "M. Queyranne", "F. Tardella" ],
      "venue" : "Math. Prog., 135(1-2):337–367,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Two algorithms for maximizing a separable concave function over a polymatroid feasible region",
      "author" : [ "H. Groenevelt" ],
      "venue" : "European Journal of Operational Research, 54:227–236,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "The total variation on hypergraphs – Learning on hypergraphs revisited",
      "author" : [ "M. Hein", "S. Setzer", "L. Jost", "S.S. Rangapuram" ],
      "venue" : "Adv. in NIPS, 26:2427–2435,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Complexity and algorithms for nonlinear optimization problems",
      "author" : [ "D.S. Hochbaum" ],
      "venue" : "Annals of Operations Research, 153(1):257–296,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning with structured sparsity",
      "author" : [ "J. Huang", "T. Zhang", "D. Metaxas" ],
      "venue" : "Journal of Machine Learning Research, 12:3371–3412,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Group Lasso with overlaps and graph Lasso",
      "author" : [ "L. Jacob", "G. Obozinski", "J.P. Vert" ],
      "venue" : "Proc. of the 26th Int’l Conf. on Machine Learning (ICML’09), pages 433–440,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On fast approximate submodular minimization",
      "author" : [ "S. Jegelka", "H. Liu", "J. Bilmes" ],
      "venue" : "NIPS, 24:460–468,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Proximal methods for hierarchical sparse coding",
      "author" : [ "R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach" ],
      "venue" : "Journal of Machine Learning Research, 12:2297–2334,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eqtl mapping",
      "author" : [ "S. Kim", "E.P. Xing" ],
      "venue" : "Annals of Applied Statistics, 6(3):1095–1117,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robust higher order potentials for enforcing label consistency",
      "author" : [ "P. Kohli", "L.u. Ladický", "P.H.S. Torr" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "What energy functions can be minimized via graph cuts? IEEE Trans",
      "author" : [ "V. Kolmogorov", "R. Zabih" ],
      "venue" : "on Pattern Analysis and Machine Intelligence, 26(2):147–159,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Network-constrained regularization and variable selection for analysis of genomic data",
      "author" : [ "C. Li", "H. Li" ],
      "venue" : "Bioinformatics, 24(9):1175–1182,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An efficient algorithm for a class of fused lasso problems",
      "author" : [ "J. Liu", "L. Yuan", "J. Ye" ],
      "venue" : "Proc. of KDD’10, pages 323–332,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Submodular functions and convexity",
      "author" : [ "L. Lovász" ],
      "venue" : "Math. Prog.–The State of the Art, pages 235–257,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Supervised group lasso with applications to microarray data analysis",
      "author" : [ "S. Ma", "X. Song", "J. Huang" ],
      "venue" : "BMC Bioinformatics, 8:60,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Sparse modeling for image and vision processing",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce" ],
      "venue" : "Foundations and Trends in Computer Graphics and Vision, 8(2-3):85–283,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Convex and network flow optimization for structured sparsity",
      "author" : [ "J. Mairal", "R. Jenatton", "G. Obozinski", "F. Bach" ],
      "venue" : "Journal of Machine Learning Research, 12:2681–2720,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Supervised feature selection in graphs with path coding penalties and network flows",
      "author" : [ "J. Mairal", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research, 14:2449–2485,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Optimal flows in networks with multiple sources and sinks",
      "author" : [ "N. Megiddo" ],
      "venue" : "Mathematical Programming, 7:97–107,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Equivalent of convex minimization problems over base polytopes",
      "author" : [ "K. Nagano", "K. Aihara" ],
      "venue" : "Japan Journal of Industrial and Applied Mathematics, 29:519–534,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Structured convex optimization under submodular constraints",
      "author" : [ "K. Nagano", "Y. Kawahara" ],
      "venue" : "Proc. of the 29th Ann. Conf. on Uncertainty in Artificial Intelligence (UAI’13), pages 459–468,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convex relaxation for combinatorial penalties",
      "author" : [ "G. Obozinski", "F. Bach" ],
      "venue" : "Report HAL 00694765,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A faster strongly polynomial time algorithm for submodular function minimization",
      "author" : [ "J.B. Orlin" ],
      "venue" : "Mathematicl Programming, 118:237–251,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Max flows in o(nm) time, or better",
      "author" : [ "J.B. Orlin" ],
      "venue" : "Proc. of STOC’13, pages 765–774,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Minimizing symmetric submodular functions",
      "author" : [ "M. Queyranne" ],
      "venue" : "Mathematicl Programming, 82(1):3–12,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Nonlinear total variation based noise removal algorithms",
      "author" : [ "L.I. Rudin", "S. Osher", "E. Fatemi" ],
      "venue" : "Physica D, 60(1-4):259–268,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Toward probabilistic diagnosis and understanding of depression based on functional MRI data analysis with logistic group LASSO",
      "author" : [ "Y. Shimizu", "J. Yoshimoto", "S. Toki", "M. Takamura", "S. Yoshimura", "Y. Okamoto", "S. Yamawaki", "K. Doya" ],
      "venue" : "PLoS ONE, 10(5):e0123524,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Structured sparsity for audio signals",
      "author" : [ "K. Siedenburg", "M. Dörfler" ],
      "venue" : "Proc. of the 14th Int’l Conf. on Digital Audio Effects (DAFx-11),, pages 23–26,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Pathwaysdriven sparse regression identifies pathways and genes associated with high-density lipoprotein cholesterol in two Asian cohorts",
      "author" : [ "M. Silver", "P. Chen", "R. Li", "C.-Y. Cheng", "T.-Y. Wong", "E.-S. Tai", "Y.-Y. Teo", "G. Montana" ],
      "venue" : "PLoS Genetics, 9(11):e1003939,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sparsity and smoothness via the fused Lasso",
      "author" : [ "R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight" ],
      "venue" : "Journal of the Royal Statistical Society: Series B, 67(1):91–108,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The solution path of the generalized lasso",
      "author" : [ "R. Tibshirani", "J. Taylor" ],
      "venue" : "Ann. Stat., 39(3):1335–1371,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Wavelet shrinkage using adaptive structured sparsity constraints",
      "author" : [ "D. Tomassia", "D. Miloned", "J.D.B. Nelson" ],
      "venue" : "Signal Processing, 106:73–87,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning, 1(1–2):1–305,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficient generalized fused Lasso with application to the diagnosis of Alzheimer’s disease",
      "author" : [ "B. Xin", "Y. Kawahara", "Y. Wang", "W. Gao" ],
      "venue" : "Proc. of AAAI′14, pages 2163–2169,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Linguistic structured sparsity in text categorization",
      "author" : [ "D. Yogatama", "N.A. Smith" ],
      "venue" : "Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL’14), pages 786–796,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B, 68(1):49–67,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The composite absolute penalties family for grouped and hierarchical variable selection",
      "author" : [ "P. Zhao", "G. Rocha", "B. Yu" ],
      "venue" : "Annals of Statistics, 37(6A):3468–3497,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "[17] and its variants, which runs, in the worst-case, at the cost of only a constant factor of a single computation of the corresponding maxflow optimization, can be adapted to solve the proximal problems for those penalties.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 54,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 48,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 49,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 33,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 319,
      "endOffset" : 339
    }, {
      "referenceID" : 30,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 319,
      "endOffset" : 339
    }, {
      "referenceID" : 24,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 319,
      "endOffset" : 339
    }, {
      "referenceID" : 27,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 319,
      "endOffset" : 339
    }, {
      "referenceID" : 47,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 319,
      "endOffset" : 339
    }, {
      "referenceID" : 52,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 357,
      "endOffset" : 369
    }, {
      "referenceID" : 34,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 357,
      "endOffset" : 369
    }, {
      "referenceID" : 45,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 357,
      "endOffset" : 369
    }, {
      "referenceID" : 11,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 399,
      "endOffset" : 407
    }, {
      "referenceID" : 53,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 399,
      "endOffset" : 407
    }, {
      "referenceID" : 46,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 430,
      "endOffset" : 438
    }, {
      "referenceID" : 50,
      "context" : "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].",
      "startOffset" : 430,
      "endOffset" : 438
    }, {
      "referenceID" : 2,
      "context" : "Recently, it has been revealed that many of the existing structured sparsity-inducing penalties can be interpreted as convex relaxations of submodular functions [3, 41].",
      "startOffset" : 161,
      "endOffset" : 168
    }, {
      "referenceID" : 40,
      "context" : "Recently, it has been revealed that many of the existing structured sparsity-inducing penalties can be interpreted as convex relaxations of submodular functions [3, 41].",
      "startOffset" : 161,
      "endOffset" : 168
    }, {
      "referenceID" : 9,
      "context" : "For example, a class of the total variation, which is equivalent to generalized fused Lasso (GFL), is known to be solved via parametric maxflows [10, 19].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "For example, a class of the total variation, which is equivalent to generalized fused Lasso (GFL), is known to be solved via parametric maxflows [10, 19].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 35,
      "context" : "(2011) [36] and Mairal & Yu (2013) [37] proposed parametric maxflow algorithms for l1/l∞-regularization and the path-coding, respectively.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 36,
      "context" : "(2011) [36] and Mairal & Yu (2013) [37] proposed parametric maxflow algorithms for l1/l∞-regularization and the path-coding, respectively.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "[17] and its variants (hereafter, we call those the GGT-type algorithms) is applicable to calculate the proximal problems for penalties obtained via convex",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "Note that the first one is closely related to the class of energy minimization problems that can be solved with the so-called graph-cut algorithm, which has been discussed actively in computer vision [30, 29].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 28,
      "context" : "Note that the first one is closely related to the class of energy minimization problems that can be solved with the so-called graph-cut algorithm, which has been discussed actively in computer vision [30, 29].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 7,
      "context" : "Similar discussions are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 38, 16].",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 37,
      "context" : "Similar discussions are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 38, 16].",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "Similar discussions are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 38, 16].",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.",
      "startOffset" : 283,
      "endOffset" : 310
    }, {
      "referenceID" : 18,
      "context" : "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.",
      "startOffset" : 283,
      "endOffset" : 310
    }, {
      "referenceID" : 35,
      "context" : "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.",
      "startOffset" : 283,
      "endOffset" : 310
    }, {
      "referenceID" : 40,
      "context" : "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.",
      "startOffset" : 283,
      "endOffset" : 310
    }, {
      "referenceID" : 36,
      "context" : "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.",
      "startOffset" : 283,
      "endOffset" : 310
    }, {
      "referenceID" : 4,
      "context" : "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.",
      "startOffset" : 283,
      "endOffset" : 310
    }, {
      "referenceID" : 52,
      "context" : "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.",
      "startOffset" : 283,
      "endOffset" : 310
    }, {
      "referenceID" : 10,
      "context" : "A set function F : 2 → R is called submodular if F (A) + F (B) ≥ F (A ∩B) + F (A ∪B) for any A,B ⊆ V [11, 14].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "A set function F : 2 → R is called submodular if F (A) + F (B) ≥ F (A ∩B) + F (A ∪B) for any A,B ⊆ V [11, 14].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : ", wj1 ≥ wj2 ≥ · · · ≥ wjd [33].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "by the Möbius inversion formula (see, for example, [1]).",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "The max-flow min-cut theorem of [13] states that these two quantities are equal.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "The first type of the penalty from a submodular function is defined through convex relaxation with the `p-norm [3, 41, 5].",
      "startOffset" : 111,
      "endOffset" : 121
    }, {
      "referenceID" : 40,
      "context" : "The first type of the penalty from a submodular function is defined through convex relaxation with the `p-norm [3, 41, 5].",
      "startOffset" : 111,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "The first type of the penalty from a submodular function is defined through convex relaxation with the `p-norm [3, 41, 5].",
      "startOffset" : 111,
      "endOffset" : 121
    }, {
      "referenceID" : 40,
      "context" : "Proposition 1 ([41]).",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 40,
      "context" : "And, if we use F (A) = ∑ g∈G min{|A∩g|, 1} for a group of variables G, then Ω̃f,p is equivalent to the (possibly, overlapping) `1/`∞ and non-overlapping `1/`p group regularizations or provides group sparsity similar to the overlapping `1/`p group regularization [41, 5].",
      "startOffset" : 262,
      "endOffset" : 269
    }, {
      "referenceID" : 4,
      "context" : "And, if we use F (A) = ∑ g∈G min{|A∩g|, 1} for a group of variables G, then Ω̃f,p is equivalent to the (possibly, overlapping) `1/`∞ and non-overlapping `1/`p group regularizations or provides group sparsity similar to the overlapping `1/`p group regularization [41, 5].",
      "startOffset" : 262,
      "endOffset" : 269
    }, {
      "referenceID" : 3,
      "context" : "This is known to make some of the components of w equal when used as a regularizer [4].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : ", F (A) = ∑ i∈A,j∈V \\A aij [4, 54].",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 52,
      "context" : ", F (A) = ∑ i∈A,j∈V \\A aij [4, 54].",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : "This can be extended to a hypergraph H = (V,E) with non-negative weight ae for each hyperedge e ∈ E, where the Lovász extension of a hypergraph cut function F (A) = ∑ e∈E:e∩A6=∅,e∩A 6=∅ ae gives the hypergraph regularization Ωhr(w) = ∑ e∈E ae(maxi∈e wi − mini∈e wj) p [22].",
      "startOffset" : 268,
      "endOffset" : 272
    }, {
      "referenceID" : 32,
      "context" : "From the definition, the Lovász extension of a submodular function with F (∅) = 0 can be represented as a greedy solution over the submodular polyhedron [33], i.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Since the objective of this problem is the sum of smooth and non-smooth convex functions, a major option for its optimization is the proximal gradient method, such as FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) [7].",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 40,
      "context" : "Based on the above formulation, Obozinski & Bach (2012) [41] recently suggested a divide-and-conquer algorithm as an adaptation of the decomposition algorithm by Groenevelt (1991) [21] for penalties from general submodular functions (for the case of p = 2).",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "Based on the above formulation, Obozinski & Bach (2012) [41] recently suggested a divide-and-conquer algorithm as an adaptation of the decomposition algorithm by Groenevelt (1991) [21] for penalties from general submodular functions (for the case of p = 2).",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 4,
      "context" : "A more general version of this approach was also developed by Bach (2013) [5].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : "1 Graph-Representable Set Functions The currently-known best complexity of minimizing a general submodular function is O(d + d EO), where EO is the cost of evaluating a function value [42].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "Although there exist practically faster algorithms, such as the minimum-norm-point algorithm [15] as well as faster algorithms for special cases (e.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 43,
      "context" : ", Queyranne’s algorithm for symmetric submodular functions [44]), their scalability would not be practically sufficient, especially if we must solve submodular minimization several times, which is the current case.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "it is well known that a cut function (which is almost equivalent to a second order submodular function [16]) can be minimized much faster through calculation of maxflows over the corresponding network.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 42,
      "context" : "If N consists of d nodes and m edges, the currently best runtime bound for the minimization is O(md) [43].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : ", [18, 9].",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : ", [18, 9].",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 25,
      "context" : "Such a function is sometimes referred to as graph-representable [26], and defined as follows.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 37,
      "context" : "The submodularity of this function is derived from the classical result of Megiddo (1974) [38] on network flow problems with multiple terminals (see, for the proof, [40]).",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 39,
      "context" : "The submodularity of this function is derived from the classical result of Megiddo (1974) [38] on network flow problems with multiple terminals (see, for the proof, [40]).",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : "1This class of functions is closely related to the class of energy minimization problems that can be solved by the so-called graph-cut algorithm [9, 30].",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 29,
      "context" : "1This class of functions is closely related to the class of energy minimization problems that can be solved by the so-called graph-cut algorithm [9, 30].",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "Related results are also found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "Related results are also found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 25,
      "context" : "The first one is mentioned as “truncations,” where a function F is graph-representable by just one additional node (see Figure 1(a) and also refer [26] or [40]).",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 39,
      "context" : "The first one is mentioned as “truncations,” where a function F is graph-representable by just one additional node (see Figure 1(a) and also refer [26] or [40]).",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "The second one is closely related to [8], and the third one is derived from [16], for which we describe concrete procedures to construct networks (see Figure 1 for the construction).",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : "The second one is closely related to [8], and the third one is derived from [16], for which we describe concrete procedures to construct networks (see Figure 1 for the construction).",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 38,
      "context" : "This is obtained, in Lemma 4 of [39], by replacing the assumption on the strict convexity of ψ′ i with the monotonisity of the function in the region under consideration.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 38,
      "context" : "As discussed in [39], this lemma implies that problem (6) can be reduced to the following parametric problem:",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "However, this is generally not true for non-linear capacities because we must solve nonlinear equations to identify such a parameter value [20].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "Algorithm 1 shows an adaptation of the simplified version [2] of the original GGT algorithm.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 48,
      "context" : "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 49,
      "context" : "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 54,
      "context" : "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 44,
      "context" : "Generalized fused Lasso is closely related to the so-called total variation, which has often been discussed in computer vision [45].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 55,
      "context" : "Recently, group penalties have been applied to more complex groups, such as hierarchical penalty [57, 27] and path penalty [37].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "Recently, group penalties have been applied to more complex groups, such as hierarchical penalty [57, 27] and path penalty [37].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 36,
      "context" : "Recently, group penalties have been applied to more complex groups, such as hierarchical penalty [57, 27] and path penalty [37].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "The total variation regularization is known to be solvable with an efficient parametric maxflow algorithm [10, 19].",
      "startOffset" : 106,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "The total variation regularization is known to be solvable with an efficient parametric maxflow algorithm [10, 19].",
      "startOffset" : 106,
      "endOffset" : 114
    }, {
      "referenceID" : 35,
      "context" : "In addition, the proximal problem for l1/l∞-group penalty is calculated via parametric maxflow optimization [36].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "Bach (2010) [3] and Bach & Obozinski (2012) [41] revealed that many of the existing structured penalties are obtained as convex relaxations of submodular functions, and those proximal problems are formulated as separable convex minimization.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 40,
      "context" : "Bach (2010) [3] and Bach & Obozinski (2012) [41] revealed that many of the existing structured penalties are obtained as convex relaxations of submodular functions, and those proximal problems are formulated as separable convex minimization.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "The sufficient condition in Section 4 is closely related to the class of energy minimization problems solvable by graph-cut algorithm [9, 30].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 29,
      "context" : "The sufficient condition in Section 4 is closely related to the class of energy minimization problems solvable by graph-cut algorithm [9, 30].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 51,
      "context" : "Energy minimization is a formulation of the maximum a posteriori (MAP) estimation on MRFs (see, for example, [53]).",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "Similar results are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "Similar results are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "Algorithm 1 is a divide-and-conquer implementation of the preflow algorithm proposed by Gallo & Tarjan (1988) [18].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Bach (2010) [3] and Bach & Obozinski (2012) [41] have mentioned an application of a divide-and-conquer approach to separable convex minimization proposed by Groenevelt (1991) [21] for proximal problem (5), which takes O(d) times of the cost for submodular minimization.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 40,
      "context" : "Bach (2010) [3] and Bach & Obozinski (2012) [41] have mentioned an application of a divide-and-conquer approach to separable convex minimization proposed by Groenevelt (1991) [21] for proximal problem (5), which takes O(d) times of the cost for submodular minimization.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "Bach (2010) [3] and Bach & Obozinski (2012) [41] have mentioned an application of a divide-and-conquer approach to separable convex minimization proposed by Groenevelt (1991) [21] for proximal problem (5), which takes O(d) times of the cost for submodular minimization.",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 16,
      "context" : "(1989) [17]’s algorithm to the current problem.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 40,
      "context" : "We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (‘DA-MNP’/’DA-MF’) and the algorithm by [36] (‘MJOB’) for the penalties from F (A) = ∑ g min{|A ∩ g|, 1} (MJOB is applicable only for p=∞), and the MNP algorithm (‘MNP’), the algorithm by Tibshirani & Taylor (2011) [51] (‘TT’) and the one by Liu et al.",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (‘DA-MNP’/’DA-MF’) and the algorithm by [36] (‘MJOB’) for the penalties from F (A) = ∑ g min{|A ∩ g|, 1} (MJOB is applicable only for p=∞), and the MNP algorithm (‘MNP’), the algorithm by Tibshirani & Taylor (2011) [51] (‘TT’) and the one by Liu et al.",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 35,
      "context" : "We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (‘DA-MNP’/’DA-MF’) and the algorithm by [36] (‘MJOB’) for the penalties from F (A) = ∑ g min{|A ∩ g|, 1} (MJOB is applicable only for p=∞), and the MNP algorithm (‘MNP’), the algorithm by Tibshirani & Taylor (2011) [51] (‘TT’) and the one by Liu et al.",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 49,
      "context" : "We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (‘DA-MNP’/’DA-MF’) and the algorithm by [36] (‘MJOB’) for the penalties from F (A) = ∑ g min{|A ∩ g|, 1} (MJOB is applicable only for p=∞), and the MNP algorithm (‘MNP’), the algorithm by Tibshirani & Taylor (2011) [51] (‘TT’) and the one by Liu et al.",
      "startOffset" : 380,
      "endOffset" : 384
    }, {
      "referenceID" : 31,
      "context" : "(2010) [32] (‘LYY’) for the (generalized) fused penalty (LYY is applicable only to the 1d fused case).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 16,
      "context" : "(1989) [17] and its variants, which runs at the cost of a constant 2We used the code modified from the one available at http://spams-devel.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 24,
      "context" : "Several avenues would be worth investigating: First, our formulation does not include the type of sparsity by the latent group penalties, such as [25].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 36,
      "context" : "As mentioned in [37], the proximal problem for the penalties of Jacob et al.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : "(2009) [25] and its generalization can be solved as a minimum-cost flow problem, which is known to be calculated as a parametric maxflow problem if the costs are quadratic and only on edges connected to source/sink [23].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 22,
      "context" : "(2009) [25] and its generalization can be solved as a minimum-cost flow problem, which is known to be calculated as a parametric maxflow problem if the costs are quadratic and only on edges connected to source/sink [23].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 17,
      "context" : "The preflow algorithm computes a maximum flow in a directed network N [18].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 39,
      "context" : "The first is shown in Lemma 2 and 3 in [40] or Proposition 2.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "5 in [5].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : "8 in [5], we obtain a solution to problem (6) as",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "The runtime follows the analysis in [17] from Lemma 7.",
      "startOffset" : 36,
      "endOffset" : 40
    } ],
    "year" : 2015,
    "abstractText" : "The proximal problem for structured penalties obtained via convex relaxations of submodular functions is known to be equivalent to minimizing separable convex functions over the corresponding submodular polyhedra. In this paper, we reveal a comprehensive class of structured penalties for which penalties this problem can be solved via an efficiently solvable class of parametric maxflow optimization. We then show that the parametric maxflow algorithm proposed by Gallo et al. [17] and its variants, which runs, in the worst-case, at the cost of only a constant factor of a single computation of the corresponding maxflow optimization, can be adapted to solve the proximal problems for those penalties. Several existing structured penalties satisfy these conditions; thus, regularized learning with these penalties is solvable quickly using the parametric maxflow algorithm. We also investigate the empirical runtime performance of the proposed framework.",
    "creator" : "LaTeX with hyperref package"
  }
}