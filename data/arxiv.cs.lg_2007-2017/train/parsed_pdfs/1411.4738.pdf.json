{
  "name" : "1411.4738.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cross-Modal Similarity Learning : A Low Rank Bilinear Formulation",
    "authors" : [ "Cuicui Kang", "Shengcai Liao", "Yonghao He", "Jian Wang", "Wenjia Niu", "Shiming Xiang" ],
    "emails" : [ "kangcuicui@iie.ac.cn", "scliao@nlpr.ia.ac.cn", "yhhe@nlpr.ia.ac.cn", "jian.wang@nlpr.ia.ac.cn", "niuwenjia@iie.ac.cn", "smxiang@nlpr.ia.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords Multimedia Retrieval; Cross-Modality; Similarity Learning; Nuclear Norm; Accelerated Proximal Gradient."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "A large amount of multimedia data has been released on the Internet during the past decade, and it is still rapidly increasing. From this big amount of media data, there exist some popular but different modalities, such as images, audios and documents. As a result, the requirement of crossmodal retrieval becomes a new problem to both researchers and the industry. Examples include the heterogenous face recognition (i.e. sketches and photos), cross-lingual retrieval, and the cross-media retrieval. Take the image-document\n*Notice: To appear in the 24th ACM International Conference on Information and Knowledge Management (CIKM 2015).\ncross retrieval as an example, given an image, the task is to find the documents that best describe it, or on the contrary, given several words, the task is to find the most related images.\nRecently, Rasiwasia et al. [29] proposed a new approach to the multimedia information retrieval problem. In the research, the authors want to match features of different modalities directly, although the cross-modal data is in rather different feature spaces. The key problem to this research is to best reduce the heterogeneity among the different modal features, or find a common latent space in which the different modal features can be matched to each other.\nClassical algorithms have been applied to solve the problem, such as the Canonical Correlation Analysis (CCA) [15, 29] and the Partial Least Squares (PLS) [30, 32]. Specifically, the CCA aims at learning a latent space where the correlations between the projected features of the two modalities have been mutually maximized. Similar to the CCA, the PLS also learns a latent space, but with different formulations to extract latent vectors [30]. In the work of [29, 8], Rasiwasia et al. proposed a semantic correlation matching (SCM) algorithm to deal with the problem. In the algorithm, a semantic level matching is suggested to combine with the linear correlations among the features. They also pointed out that, the correlation matching (such as CCA) is not enough for cross-modal matching, and the semantic level matching brings much benefit in working out the problem. This is consistent with that the class label information is very helpful to reduce the semantic gap, which is well known in the image retrieval field denoting the gap between the high level documents and low level images. Beyond these methods, there are some other algorithms to deal with the problem, such as the generalized multiview analysis (GMA) [32], the weakly paired maximum covariance analysis (WMCA) [23], the deep neural network [27, 34], etc. [4, 28, 19, 42, 43, 41, 13].\nRecently, metric learning algorithms have been popularly studied. The target of metric learning is to find a distance metric by using the similar constraints and the dissimilar constraints [39]. Following the work of Xing et al. [39], there are some popular algorithms like the Large Margin Nearest Neighbor (LMNN) [37], the Information-Theoretic Metric Learning (ITML) [9], the Relevance Component Anal-\nar X\niv :1\n41 1.\n47 38\nv2 [\ncs .M\nM ]\n1 7\nD ec\nysis (RCA) [2], and the Discriminative Component Analysis (DCA) which is based on the RCA [16]. Specifically, the LMNN was proposed to use a large margin setting to improve the k-NN classification. The ITML algorithm proposed by Davis et al. aims at minimizing the differential relative entropy between two multivariate Gaussians. However, these traditional metric learning algorithms are designed for a single modality. They suffer the difficulty of learning the similarity/distance metrics between different modalities, especially with totaly different features.\nInspired by metric learning, we propose a novel logistic loss based cross-modality similarity function learning algorithm for the cross-media retrieval. The similarity formulation is in a bilinear form of two modalities, and a fast optimization algorithm is imported to find the optimal solution with a convergence rate of O(1/t2), where t is the number of iterations. Besides, the nuclear-norm regularization is imported in the formulation to explore the structures of the different modalities for robust learning. The proposed algorithm is applied to multimedia information retrieval on three popularly used multimedia databases for experiments, namely the Pascal VOC2007 database [12, 32], the NUSWIDE database [7], and the Wikipedia database [29]. Experimental results show that the proposed algorithm is effective for cross-media retrieval. Especially, on the Wikipedia database the proposed algorithm outperforms the second best one for more than 6%.\nThe rest of the paper is organized as following. In Section 2, a concise review is given for the related works in the cross-modal information retrieval field and the metric learning field. Then, in Section 3, after a brief introduction of the Metric Learning problem, the formulation of the proposed algorithm and how to find the optimal solution are introduced in detail. The experiments are described in Section 4 to validate the effectiveness of the proposed algorithm. Finally, Section 5 gives the conclusion of the paper."
    }, {
      "heading" : "2. RELATED WORK",
      "text" : "Information Retrieval (IR) is an important problem in the multimedia field. However, many traditional methods to the problem belong to the unimodal algorithms, such as the document retrieval and the content based image retrieval. Due to the rapid development of Internet applications, the crossmodality information retrieval becomes a common scene. To retrieve an image, the query terms may be of various different modalities, such as paragraphs, sketches and audios. Thus, cross-modal matching and learning algorithms which can match different modality data directly become a new research interest in the IR field.\nAmong these cross-modal matching algorithms, the CCA algorithm is the most widely used method in the multimedia field [15, 17, 29, 24]. The target of CCA is to learn a latent space by maximizing the correlating relationships between two modality features. Thus the different modal features can be projected to the latent space for similarity computation. The algorithm is also used as the correlation matching (CM) method by Rasiwasia et al. [29]. Beyond CCA, the PLS algorithm is another classical method for cross-modal data [30, 31, 32]. The core idea of it is very similar with that of CCA, which is to extract the latent vectors with maximal correlations.\nIn [29] where the cross-modal IR was suggested, Rasiwasia et al. proposed a supervised algorithm for the image-text\ncross-modal retrieval problem, namely the SCM algorithm. The SCM is one of the most famous and the current stateof-the-art algorithm. To reduce the semantic gap between images and documents, the sematic level matching is developed based on the learned maximal correlation latent space by CCA. Thus, the algorithm can be separated into two steps. The correlational matching between different modalities by CCA is done in the first step. Then, based on it a semantic space is learned in the second step. As indicated in [29, 8], the class information is an important information to reduce the semantic gap. Thus, in order to use the class labels, Sharma et al. proposed a GMA algorithm to learn a discriminative latent space for cross-modal data and treat it as an eigenvalue problem. The algorithm shows great performance to the pose and lighting invariant face recognition and cross-modal retrieval problems.\nIn fact, some methods targeting to the heterogenous face recognition problem are available to deal with the crossmodality IR problem, such as the Multiview Discriminant Analysis (MvDA) method [20]. The MvDA algorithm aims at learning a common space where the between-class variations from both inter-view and intra-view are maximized, and the within-class variations from both inter-view and intra-view are minimized. In the transfer learning field, some algorithms are also related [11, 22, 23, 33]. Such as in [23], Lampert and Krömer proposed a weakly-paired maximum covariance analysis method to deal with the not fully paired (not one-by-one paired) training data. Besides, Wang et al. [36] proposed an iterative algorithm based on sparsity to learn the coupled feature spaces for the different modalities. The work in [41] also proposed a greedy dictionary construction approach to select dictionary atoms for constructing a modality-adaptive dictionary pair. In the deep learning field, the works [27] and [34] both used the restricted boltzmann machine for the cross-modal feature learning.\nIn the literature of the metric learning field, Chechik et al. [6] imported an online similarity function learning for large-scale images. But the algorithm is designed for singlemodality in a triplet ranking formulation. Kulis et al. also proposed to learn an asymmetric transformation matrix for domain adaption [22]. Besides, a metric learning algorithm for different modalities was also realized by Wu et al. [38]. The objective function in their work learns the projections for the different modalities, respectively, to best separate the similar points set and the dissimilar points set. However, the pair-wise information is ignored in the algorithm. In [40] and [25], the authors also proposed to learn two projections for each modality to minimize the distances of the two modalities in the target feature space. Zhai et al. also used the semantic information in the second step based on a unified k-NN graph. However, both of the algorithms are not convex, thus the optimal solution is not guaranteed. In contrast, the proposed formulation in this paper is a strict convex problem, and the optimal solution is achieved by the accelerated proximal gradient (APG) algorithm [26, 35, 3]."
    }, {
      "heading" : "3. SIMILARITY LEARNING FOR CROSS MODALITIES",
      "text" : ""
    }, {
      "heading" : "3.1 Traditional Metric Learning",
      "text" : "In this section, we give a brief review of the traditional metric learning problem, which was firstly studied by Xing\net al. [39] to learn a distance metric according to the similar points and dissimilar points.\nSuppose that there is a set of m data points {xi ∈ Rd}mi=1, where each data point is with a d dimensional feature. Besides, we are also given the binary similarity information indicating whether two data points are similar or not. Specifically, there are two sets of paired points, which can be described as follows:{ S = {(xi,xj) | xi and xj are in the same class}, D = {(xi,xj) | xi and xj are not in the same class},\n(1) where S is called the must-link set containing the similar pairs and the D is called the cannot-link set containing the dissimilar pairs.\nGiven the two sets, the task of metric learning is to learn a distance metric in the following form,\nd(xi,xj)M = ‖xi − xj‖M = √ (xi − xj)TM(xi − xj), (2)\nwhere M ∈ Rd×d is a Mahalanobis distance metric that should be learned. It can be easily found that M is symmetric. Furthermore, to be a valid metric, M must satisfy the non-negativity and the triangle inequality. Thus M is required to be positive semi-definite, namely M 0.\nInspired by the metric learning formulation, we want to learn a similarity function to evaluate two modality features which are in different dimensions, for example the 〈image, text〉 pairs. In this way, the different modality features can be matched directly with the learned similarity function. The proposed formulation is introduced as follows."
    }, {
      "heading" : "3.2 Problem Definition and Formulation",
      "text" : "Suppose that there is a multimedia databaseA = {(x1, lx1 ), (x2, l x 2 ), . . . , (xm, l x m), (z1, l z 1), . . . , (zn, l z n)} constructed from two-modality data sets X and Z. Sample xi ∈ Rd1 is from the modal X , such as the image, document, or audio, and zi ∈ Rd2 belongs to the modality Z which is different from the modality X , where d1 and d2 are dimensions of the two modality features, respectively. The lxi and l z j are the class labels of the samples xi and zj respectively. The connection between the two modalities is that they share c classes, for example, dog, shoes and buildings. The cross matching problem is how to match the cross-modal samples xi and zj directly.\nSimilar with the metric learning, we also define two pairwise sets on the cross-modal samples,{\nS = {(xi, zj) | lxi = lzj }, D = {(xi, zj) | lxi 6= lzj },\n(3)\nwhere S is the must-link set with similar pairs from the two different modalities, and D is the cannot-link set with dissimilar pairs from the two modalities.\nBased on these definitions, we want to learn a similarity function which can measure the similarity between any two different modal features. The formulation of the proposed algorithm is\nmin M m,n∑ i,j=1 wij log(1 + exp(−yijSM(xi, zj))) + λ‖M‖∗, (4)\nwhere SM(xi, zj) is the similarity function parameterized by a matrix M ∈ Rd1×d2 , which is to be learned in a bilinear\nform,\nSM(xi, zj) = x T i Mzj . (5)\nThe formulation in Eqn.(4) contains a loss function item and a regularization item. The loss function is based on the logistic sigmoid function [14], δ(x) = 1/(1 + exp(−x)), which is often integrated in − log(δ(x)) to approximate the hinge loss function to avoid the discontinuity of the gradient. Here, yij ∈ {+1,−1} is a sign indicating whether the pair is similar (positive) or dissimilar (negative). Specifically,\nyij = { +1 (xi, zj) ∈ S, −1 (xi, zj) ∈ D.\n(6)\nFor wij in the formulation, it stands for the weight of the (xi, zj) pair, which is used to deal with unbalanced positive and negative samples. In our experiments, we set the weights for positive (negative) pairs to be the reciprocal of the number of positive (negative) pairs.\nAs for the regularization term in Eqn.(4), the nuclear norm ‖ · ‖∗ is used, which is defined as the sum of the singular values of a matrix. The functions of the nuclear norm in the proposed formulation lie in two folds. On one fold, the constraint can be treated as a scale regularization of M. It makes the solution to Eqn.(4) in a constrained domain. On the other fold, the nuclear norm can make the learned metric matrix M with a low rank, which is a desirable property and has been widely used in the machine learning field. In the problem Eqn.(4), the nuclear norm may help to discover the connections between the two modalities and result in a robust learning."
    }, {
      "heading" : "3.3 Optimization",
      "text" : "In this subsection, the accelerated proximal gradient algorithm (APG) [26, 35] is utilized to find the optimal solution of Eqn.(4). The APG algorithm is a kind of first order gradient descent method [3, 18, 26, 35], which has received popular attentions in recent years due to its fast convergence rate of O(1/t2) when dealing with the problem of convex and smooth loss function regularized by non-smooth constraints.\nFor convenience, the formulation in Eqn.(4) can be rewritten as\nmin M\nf(M) = l(M) + λ‖M‖∗, (7)\nwhere\nl(M) = m,n∑ i,j=1 wij log(1 + exp(−yijxTi Mzj)). (8)\nNote that l(M) is a smooth and convex function, and the objective function is convex. Accordingly, the APG algorithm can be used to solve the proposed formulation.\nTo find the optimal minimizer, we first construct a proximal operator of the Eqn. (7), which is\nmin M\ng(M) = l(Qt) + 〈l′(Qt),M−Qt〉\n+ 1\n2ηt ‖M−Qt‖22 + λ‖M‖∗,\n(9)\nwhere {Qt} is a sequence of the search points, l′(Qt) is the gradient of l(·) at the Qt point, and ηt is the update step size at the t-th iteration. The l′(Qt) is\n∂l(M)\n∂M |Qt = − m,n∑ i,j=1 wijyij 1 + exp(yijxTi Qtzj) xiz T j , (10)\nBy defining a matrix T with each entry Tij as\nTij = wijyij\n1 + exp(yijxTi Qtzj) , (11)\nwe can conveniently compute T as\nT = W Y 1 1 + exp(Y XTQtZ) , (12)\nwhere is the element-wise product of two matrices. W ∈ Rm×n is the weight matrix with wij for each pair of the cross-modal samples, and Y ∈ Rm×n is the sign matrix with yij defined in Eqn.(6). The X ∈ Rd1×m is the data matrix containing the m samples of the modality X , and Z ∈ Rd2×n consists of the n samples of the modality Z.\nWith these definitions, Eqn.(10) can be simplified as\nl′(Qt) = ∂l(M)\n∂M |Qt = − m∑ i=1 n∑ j=1 Tijxiz T j\n= −XTZT (13)\nBy removing the constant term l(Qt) and adding another constant term ηt‖l′(Qt)‖22/2 with respect to M, the proximal operator of Eqn. (9) is equal to\nmin M\n1 2 ‖M− M̃t‖2F + ληt‖M‖∗, (14)\nwhere M̃t = Qt − ηtl′(Qt). (15) According to [5], the minimization of the objective function Eqn.(14) can be solved by a soft-thresholding technique. Firstly, the singular value decomposition (SVD) is\nused to find the eigenvalues for the matrix M̃. Then, a soft-thresholding is applied on the computed eigenvalues. In summary,\nTheorem 1. Assume matrix L ∈ Rm×n and the SVD decomposition of it is L = UΣVT , where U ∈ Rd1×r and V ∈ Rd2×r are constructed by orthogonal vectors, and Σ is a diagonal matrix with diagonal values Σii ≥ 0 and rank(Σ) = r. Then the following objective function [5]\nCγ(M) = min M\n1 2 ‖M− L‖2F + γ‖M‖∗ (16)\nhas a closed-form solution Cγ(M) = UΣγV T , where (Σγ)ii = max{0,Σii − γ} is the soft-thresholding result of the matrix Σ by the regularization parameter γ .\nThe proof can be found in the Appendix. Accordingly, we get the Mt+1 = UΣληtV\nT . Then, the APG algorithm accelerates the proximal gradient descent by updating Qt+1 with\nQt+1 = Mt+1 + αt − 1 αt+1 (Mt+1 −Mt) (17)\nwhere αt+1 = (1 + √\n1 + 4α2t )/2 and α1 = 1. In fact, the αt can be updated in other ways if the certain conditions are satisfied [26, 35]. As for the step size ηt, it can be estimated in the algorithm by comparing the objective function and its proximal operator [3], whose derivation is omitted here due to the paper length.\nIt turns out that the APG algorithm updates Mt and Qt iteratively to find the optimal solution, and the searching point Qt is a linear combination of the latest two solutions of Mt and Mt−1. The steps of the proposed algorithm is summarized in Algorithm 1.\nAlgorithm 1: Optimization for Eqn. (4).\nInput: X ∈ Rd1×m, Z ∈ Rd2×n, and Y ∈ Rm×n. Output: M ∈ Rd1×d2\n1 Initialization Qt = 0, t = 1; 2 repeat\n3 Compute M̃t as in Eqn. (12) (13) (15); 4 Decompose M̃t = UΣV T ; 5 Update Mt+1 = UΣληtV T ; 6 Update Qt+1 as in Eqn. (17). 7 until convergence;"
    }, {
      "heading" : "4. EXPERIMENT",
      "text" : "In this section, the proposed low rank bilinear similarity learning algorithm (denoted as LRBS) is compared with several popular and state-of-the-art algorithms by experiments on three famous image-text databases.\nCompared Methods: To evaluate the performance of the proposed algorithm, several famous algorithms in the cross-media retrieval field were compared in the experiments, including the popular methods CCA [15, 29] and PLS [30, 31], and the state-of-the-art methods SCM [29], the Microsoft algorithm (MsAlg) [38], and the Generalized Multiview Analysis methods, including the GMLDA (Generalized Multiview Linear Discriminant Analysis) and the GMMFA (Generalized Multiview Marginal Fisher Analysis) [32]. A brief introduction of these algorithms can be found in the related work section.\nEvaluation Metrics: For the evaluation, the mean average precision (MAP) and the precision-recall curve metrics were used in the experiments. In fact, both of them are popularly used in evaluation of information retrieval systems. Given one query and its N retrieved documents in a rank, the average precision is computed by AveP = 1 T ∑N r=1 P (r)rel(r), where T is the number of the relevant documents in the retrieval, P (r) is the precision of the top r retrieved documents, and rel(r) is a binary function denoting whether the r-th retrieved document is relevant to the query or not. Having the AveP calculated for each query, the MAP is computed as the average AveP score of all queries. Besides of the MAP and precision-recall curve, we also displayed the precision-scope curve for a better visualization, where the scope denotes the number of retrieved documents. Therefore, by the precision-scope curve it is more easily to see the precision of the top r documents presented to the users."
    }, {
      "heading" : "4.1 Databases",
      "text" : "In the experiments, three famous images and texts databases in the multimedia retrieval field are used to evaluate the performance of the proposed algorithm, namely the Pascal VOC2007 database, the Wikipedia database, and the NUSWIDE database.\nThe Pascal VOC2007 dataset consists of 9963 images from 20 categories [12], which was split into a training set with 5011 images and a test set with 4952 images. Since some of the images are with multi-labels, the images containing only one object were selected in the experiments [32]. As a result, there are 2808 images for the training set and 2841 images for the test set. For the features, the 399-dimensional word frequency feature [17] was used for the\ntext, and the convolutional neural network (CNN) feature, which was trained on the ImageNet [21], was used as the image feature. The CNN source code, namely Decaf [10], can be freely downloaded on the web for research purpose1.\n1http://daggerfs.com/\nIn the experiments, only the outputs of the sixth layer were used as the image feature, resulting in 4096 dimensions.\nThe Wikipedia dataset2 is constructed from the Wikipedia’s “featured articles”, which is a continually updated collection selected and reviewed by the Wikipedia’s editors since 2009. Among the articles in the collection, Rasiwasia et al. built\n2http://www.svcl.ucsd.edu/projects/crossmodal/\na dataset by selecting ten popular categories [29]. Each image in the dataset is associated with a section of at least 70 words. In total, the dataset consists of 2866 image-document pairs. In [29], the dataset was randomly split into a training set of 2173 image-document pairs and a test set of the remaining 693 pairs. The text feature was derived from the Latent Dirichlet Allocation model (LDA) with 200 topics [8]. Similar as on the Pascal VOC2007 database, the CNN feature was also used for the images on this dataset.\nThe NUS-WIDE dataset is a large-scale real-world database where the images were crawled from the “Flickr” website by the Media Search Lab in the National University of Singapore [7]. The database is constructed with 269,648 images associated with tags. The organizers has labeled the images with 81 concepts for evaluation. They also provided all the URLs of the images on the web to facilitate further research3. In the experiment, we used 40 concepts which contained the most uniquely labeled images, and 250 images per concept were randomly selected for evaluation. The selected images were further randomly separated into a training set and a test set, which contain 6000 images and 3630 images respectively. Again, the CNN feature for the images was used in the experiment. As for the text feature, the provided 1000-dimensional word frequency feature by the database organizers [7] was imported.\n4.2 Experimental Results\n3http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm\nIn the experiments, we found that the CCA, PLS and GMA algorithms all performed better with PCA dimensional reduction than without it. Thus, we displayed both the experimental results with PCA and without PCA for all the algorithms. With PCA, about 99% energy were preserved for the features. As a result, the dimension of the CNN image feature was reduced to 1000 on the three databases. As for the text feature, it was reduced to 200\non the Pascal VOC2007 database, 180 on the Wikipedia and 800 on the NUS-WIDE database. The MAPs of the experimental results are shown in Table 1, Table 2 and Ta-\nble 3 for the VOC2007, the Wikipedia and the NUS-WIDE databases respectively, where the black bold numbers are the best performances, and the blue bold ones are the second\nbest across the performances with PCA and not with PCA. The precision-recall and precision-scope curves are shown in Figure 1, 2 and Figure 3 for the three datasets, respectively.\nFrom Table 1, Table 2 and Table 3 it can be observed that the proposed LRBS algorithm achieves the best performance on the three databases. On the Wikipedia dataset, no matter with PCA and without PCA, the proposed method performs about 6% higher than the second best algorithm (the GMMFA algorithm with PCA and the MsAlg algorithm without PCA). The followings are MsAlg with PCA, GMLDA with PCA and the SCM with PCA, which perform comparable to each other. This ranking is similar with that on the NUS-WIDE database. On the VOC2007 dataset, LRBS outperforms the second best one, the GMLDA algorithm with PCA, by 1%. The followings are the SCM with PCA and the MsAlg algorithms. It can be also observed that the GMMFA and the GMLDA perform closely to each other on the three datasets.\nIt should be noted that the CCA without dimensional reduction cannot be applied since the feature matrix is not with full rank. This is the same with the SCM which needs CCA as the first step for correlation matching. Obviously, the GMA methods with PCA dimensional reduction perform better than without PCA. In contrast, for the PLS, MsAlg, and LRBS algorithms, they perform almost the same with PCA dimensional reduction and without it. The reason may lie in that the PCA dimension reduction removes part of the redundant information and noises. However, PCA may also discard some useful information at the same time. This may be the reason that the performances of the MsAlg and LRBS algorithms do not improve with PCA.\nTable 1, 2 and 3 also show that the supervised algorithms (the MsAlg, GMMFA, GMLDA, SCM and LRBS algorithms) clearly outperform unsupervised ones (the CCA and the PLS). It is because the class information used in supervised learning could partly reduce the semantic gap between low-level image features and high-level semantic meanings. The work in [8] also found that the combination of correlation matching and semantic matching can work better.\nFigure 1, Figure 2 and Figure 3 display the precisionrecall curves and the precision-scope curves of the compared algorithms for the image-text cross retrieval task. The performances of the GMA methods without PCA dimensional reduction are not shown in the figures due to the low performances. Note that the figures on show the best performance of each compared algorithm. For example, since the performance of the MsAlg method with PCA is slightly lower than without PCA, the figures just display the performance of the MsAlg without PCA. In these figures, it is also observed that the proposed algorithm has the best performance on all databases, followed by GMLDA, GMMFA and MsAlg methods, which are comparable to each other. From the precision-recall figures, it is clearly that with the same recall rate, the proposed method gets the highest precision in all compared algorithms, especially on the Wikipedia dataset. This is similar with the precision-scope curves, which plot the precision under different numbers of the top r retrieved samples. In summary, the experimental results show that the proposed algorithm successfully learns a similarity function for cross-modal matching.\nIn Figure 6, the singular values of the learned similarity metric matrix are shown, which contains the top 80 singular\nvalues. From the figure we can see that the learned M has a low rank, indicating that the proposed LRBS algorithm succeeds to find the similar structures in the data. It can be seen that the similarity metric of the Wikipeda dataset has the lowest rank. This is partly because the Wikipedia dataset only contains 10 categories. In contrast, the NUSWIDE dataset contains the most categories, thus its similarity metric has a relatively high rank. In Figure 4 and Figure 5, some retrieved samples by the proposed LRBS algorithm in the experiment are shown. The retrieved samples also indicate that the LRBS is an effective cross-modality information retrieval algorithm."
    }, {
      "heading" : "5. CONCLUSION",
      "text" : "In this paper, we proposed a novel similarity function learning method inspired by metric learning algorithms. The similarity function is learned in a bilinear form of two different modality features, which can handle cross-modal features with different dimensions. The accelerated proximal gradient method is successfully imported to find the optimal solution with a fast convergence rate. Besides, we also imported the nuclear-norm to explore the structures and connections of the two modalities. The experiments are evaluated on three famous multimedia databases for the image-text cross-modal information retrieval problem, showing that the proposed algorithm has the best performance compared to the state-of-the-art algorithms."
    }, {
      "heading" : "6. ACKNOWLEDGMENTS",
      "text" : "This work was supported in part by the National Basic Research Program of China (Grant 2012CB316304), the National Natural Science Foundation of China (Grants 61272331, 91338202 and 91438105), the Strategic Priority Research Program of the Chinese Academy of Sciences Grant (XDA06030200), Beijing Key Lab of Intelligent Telecommunication Software and Multimedia (ITSM201502), Guangxi Key Laboratory of Trusted Software (KX201418)."
    }, {
      "heading" : "7. ADDITIONAL AUTHORS",
      "text" : "Additional author: Chunhong Pan (Institute of Automation, Chinese Academy of Sciences, email: chpan@nlpr.ia.ac.cn)."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] F. R. Bach. Consistency of trace norm minimization.\nJournal of Machine Learning Research, 9:1019–1048, 2008.\n[2] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learning distance functions using equivalence relations. In ICML, pages 11–18, 2003.\n[3] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sciences, 2(1):183–202, 2009.\n[4] M. Bronstein, A. Bronstein, F. Michel, and N. Paragios. Data fusion through cross-modality metric learning using similarity-sensitive hashing. In CVPR, pages 3594–3601, 2010.\n[5] J.-F. Cai, E. J. Candès, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.\n[6] G. Chechik, U. Shalit, V. Sharma, and S. Bengio. An online algorithm for large scale image similarity learning. In NIPS, pages 306–314, 2009.\n[7] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.-T. Zheng. NUS-WIDE: A Real-World Web Image Database from National University of Singapore. In ACM Conf. on Image and Video Retrieval, 2009.\n[8] J. Costa Pereira, E. Coviello, G. Doyle, N. Rasiwasia, G. Lanckriet, R. Levy, and N. Vasconcelos. On the role of correlation and abstraction in cross-modal multimedia retrieval. TPAMI, 36(3):521–535, 2014.\n[9] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. Information-theoretic metric learning. In ICML, volume 227, pages 209–216, 2007.\n[10] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013.\n[11] L. Duan, D. Xu, and I. W. Tsang. Learning with augmented features for heterogeneous domain adaptation. In ICML, pages 711–718, 2012.\n[12] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 Results, 2007.\n[13] Y. Gong, Q. Ke, M. Isard, and S. Lazebnik. A multi-view embedding space for modeling internet images, tags, and their semantics. International Journal of Computer Vision, 106(2):210–233, 2014.\n[14] M. Guillaumin, J. J. Verbeek, and C. Schmid. Is that you? metric learning approaches for face identification. In ICCV, pages 498–505, 2009.\n[15] D. R. Hardoon, S. Szedmák, and J. Shawe-Taylor. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16(12):2639–2664, 2004.\n[16] S. C. H. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma. Learning distance metrics with contextual constraints for image retrieval. In CVPR, pages 2072–2078, 2006.\n[17] S. J. Hwang and K. Grauman. Accounting for the relative importance of objects in image retrieval. In BMVC, pages 1–12, 2010.\n[18] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In ICML, pages 457–464, 2009.\n[19] Y. Jia, M. Salzmann, and T. Darrell. Learning cross-modality similarity for multinomial data. In ICCV, pages 2407–2414, 2011.\n[20] M. Kan, S. Shan, H. Zhang, S. Lao, and X. Chen. Multi-view discriminant analysis. In ECCV (1), volume 7572, pages 808–821, 2012.\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1106–1114, 2012.\n[22] B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using\nasymmetric kernel transforms. In CVPR, pages 1785–1792, 2011.\n[23] C. H. Lampert and O. Krömer. Weakly-paired maximum covariance analysis for multimodal dimensionality reduction and transfer learning. In ECCV, pages 566–579, 2010.\n[24] A. Li, S. Shan, X. Chen, and W. Gao. Face recognition based on non-corresponding region matching. In International Conference on Computer Vision, pages 1060–1067, 2011.\n[25] A. Mignon and F. Jurie. CMML: a New Metric Learning Approach for Cross Modal Matching. In Asian Conference on Computer Vision, 2012.\n[26] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers, 2003.\n[27] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In ICML, pages 689–696, 2011.\n[28] Y. Pan, T. Yao, T. Mei, H. Li, C.-W. Ngo, and Y. Rui. Click-through-based cross-view learning for image search. In ACM conference on Research and Development in Information Retrieval (SIGIR), 2014.\n[29] N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos. A new approach to cross-modal multimedia retrieval. In ACM Multimedia, pages 251–260, 2010.\n[30] R. Rosipal and N. Krämer. Overview and recent advances in partial least squares. In SLSFS, pages 34–51. Springer, 2006.\n[31] A. Sharma and D. W. Jacobs. Bypassing synthesis: PLS for face recognition with pose, low-resolution and sketch. In CVPR, pages 593–600, 2011.\n[32] A. Sharma, A. Kumar, H. D. III, and D. W. Jacobs. Generalized multiview analysis: A discriminative latent space. In CVPR, pages 2160–2167, 2012.\n[33] R. Socher, M. Ganjoo, C. D. Manning, and A. Y. Ng. Zero-shot learning through cross-modal transfer. In NIPS, pages 935–943, 2013.\n[34] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In NIPS, pages 2231–2239, 2012.\n[35] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted to SIAM Journal on Optimization, 2008.\n[36] K. Wang, R. He, W. Wang, L. Wang, and T. Tan. Learning coupled feature spaces for cross-modal matching. International Conference on Computer Vision, 2013.\n[37] K. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neighbor classification. NIPS, pages 1473–1480, 2006.\n[38] W. Wu, J. Xu, and H. Li. Learning similarity function between objects in heterogeneous spaces. Technical Report MSR-TR-2010-86, 2010.\n[39] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell. Distance metric learning with application to clustering with side-information. In NIPS, pages 505–512, 2002.\n[40] X. Zhai, Y. Peng, and J. Xiao. Heterogeneous metric learning with joint graph regularization for crossmedia retrieval. In AAAI, pages 1198–1204, 2013.\n[41] F. Zhu, L. Shao, and M. Yu. Cross-modality submodular dictionary learning for information retrieval. In CIKM, pages 1479–1488, 2014.\n[42] J. Zhuang and S. C. H. Hoi. A two-view learning approach for image tag ranking. In WSDM, pages 625–634, 2011.\n[43] Y. Zhuang, Y. F. Wang, F. Wu, Y. Zhang, and W. Lu. Supervised coupled dictionary learning with group structures for multi-modal retrieval. In AAAI, pages 1070–1076, 2013.\nAPPENDIX"
    }, {
      "heading" : "A. PROOF OF THEOREM 1",
      "text" : "Proof. Considering that the objective function in Eqn. (16) is a strongly convex function, a unique solution exists. Thus we just need to prove that the optimal solution is equal to Cγ(M) [5]. Considering that M̂ is the optimal solution of Eqn.(16) if and only if 0 is a subgradient of the function\nat the point M̂, we have\n0 ∈ M̂− L + γ∂‖M̂‖∗, (18)\nwhere the ∂‖M̂‖∗ is the subgradient of the nuclear norm. Let the SVD decomposition of an arbitrary matrix A ∈ Rm×n is A = P1ΛP T 2 , then the subgradient of its nuclear norm is [1, 5]\n∂‖A‖∗ = {P1PT2 + S : S ∈ Rm×n,PT1 S = 0, SP2 = 0, ‖S‖2 ≤ 1}.\n(19)\nDenote the SVD decomposition of L in Eqn.(16) as\nL = U0Σ0V T 0 + U1Σ1V T 1 ,\nwhere U0Σ0V T 0 is the part of SVD with singular values greater than γ, and the U0Σ0V T 0 corresponds to the remaining part. By denoting M̂ = Cγ(M), we have\nM̂ = U0(Σ0 − γI)VT0 .\nTherefore,\nL− M̂ = L− Cγ(M) = γ(U0VT0 + S),\nwhere\nS = γ−1U1Σ1V T 1 .\nIt turns out that UT0 S = 0, SV0 = 0, and ‖S‖2 ≤ 1 since Σ1 is bounded by γ. Finally, we have proved that L−Cγ(M) ∈ γ∂‖Cγ(M)‖∗, which shows that Cγ(M) is the optimal solution of Eqn.(16)."
    } ],
    "references" : [ {
      "title" : "Consistency of trace norm minimization",
      "author" : [ "F.R. Bach" ],
      "venue" : "Journal of Machine Learning Research, 9:1019–1048,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning distance functions using equivalence relations",
      "author" : [ "A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall" ],
      "venue" : "ICML, pages 11–18,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM J. Imaging Sciences, 2(1):183–202,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Data fusion through cross-modality metric learning using similarity-sensitive hashing",
      "author" : [ "M. Bronstein", "A. Bronstein", "F. Michel", "N. Paragios" ],
      "venue" : "CVPR, pages 3594–3601,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "J.-F. Cai", "E.J. Candès", "Z. Shen" ],
      "venue" : "SIAM Journal on Optimization, 20(4):1956–1982,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An online algorithm for large scale image similarity learning",
      "author" : [ "G. Chechik", "U. Shalit", "V. Sharma", "S. Bengio" ],
      "venue" : "NIPS, pages 306–314,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "NUS-WIDE: A Real-World Web Image Database from National University of Singapore",
      "author" : [ "T.-S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y.-T. Zheng" ],
      "venue" : "ACM Conf. on Image and Video Retrieval,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the role of correlation and abstraction in cross-modal multimedia retrieval",
      "author" : [ "J. Costa Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G. Lanckriet", "R. Levy", "N. Vasconcelos" ],
      "venue" : "TPAMI, 36(3):521–535,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Information-theoretic metric learning",
      "author" : [ "J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon" ],
      "venue" : "ICML, volume 227, pages 209–216,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1310.1531,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning with augmented features for heterogeneous domain adaptation",
      "author" : [ "L. Duan", "D. Xu", "I.W. Tsang" ],
      "venue" : "ICML, pages 711–718,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The PASCAL Visual Object Classes Challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "A multi-view embedding space for modeling internet images, tags, and their semantics",
      "author" : [ "Y. Gong", "Q. Ke", "M. Isard", "S. Lazebnik" ],
      "venue" : "International Journal of Computer Vision, 106(2):210–233,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Is that you? metric learning approaches for face identification",
      "author" : [ "M. Guillaumin", "J.J. Verbeek", "C. Schmid" ],
      "venue" : "ICCV, pages 498–505,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Canonical correlation analysis: An overview with application to learning methods",
      "author" : [ "D.R. Hardoon", "S. Szedmák", "J. Shawe-Taylor" ],
      "venue" : "Neural Computation, 16(12):2639–2664,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning distance metrics with contextual constraints for image retrieval",
      "author" : [ "S.C.H. Hoi", "W. Liu", "M.R. Lyu", "W.-Y. Ma" ],
      "venue" : "CVPR, pages 2072–2078,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Accounting for the relative importance of objects in image retrieval",
      "author" : [ "S.J. Hwang", "K. Grauman" ],
      "venue" : "BMVC, pages 1–12,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An accelerated gradient method for trace norm minimization",
      "author" : [ "S. Ji", "J. Ye" ],
      "venue" : "ICML, pages 457–464,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning cross-modality similarity for multinomial data",
      "author" : [ "Y. Jia", "M. Salzmann", "T. Darrell" ],
      "venue" : "ICCV, pages 2407–2414,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multi-view discriminant analysis",
      "author" : [ "M. Kan", "S. Shan", "H. Zhang", "S. Lao", "X. Chen" ],
      "venue" : "ECCV (1), volume 7572, pages 808–821,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS, pages 1106–1114,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "What you saw is not what you get: Domain adaptation using  asymmetric kernel transforms",
      "author" : [ "B. Kulis", "K. Saenko", "T. Darrell" ],
      "venue" : "CVPR, pages 1785–1792,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Weakly-paired maximum covariance analysis for multimodal dimensionality reduction and transfer learning",
      "author" : [ "C.H. Lampert", "O. Krömer" ],
      "venue" : "ECCV, pages 566–579,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Face recognition based on non-corresponding region matching",
      "author" : [ "A. Li", "S. Shan", "X. Chen", "W. Gao" ],
      "venue" : "International Conference on Computer Vision, pages 1060–1067,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "CMML: a New Metric Learning Approach for Cross Modal Matching",
      "author" : [ "A. Mignon", "F. Jurie" ],
      "venue" : "Asian Conference on Computer Vision,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A Basic Course",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Multimodal deep learning",
      "author" : [ "J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng" ],
      "venue" : "ICML, pages 689–696,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Click-through-based cross-view learning for image search",
      "author" : [ "Y. Pan", "T. Yao", "T. Mei", "H. Li", "C.-W. Ngo", "Y. Rui" ],
      "venue" : "ACM conference on Research and Development in Information Retrieval (SIGIR),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A new approach to cross-modal multimedia retrieval",
      "author" : [ "N. Rasiwasia", "J.C. Pereira", "E. Coviello", "G. Doyle", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos" ],
      "venue" : "ACM Multimedia, pages 251–260,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Overview and recent advances in partial least squares",
      "author" : [ "R. Rosipal", "N. Krämer" ],
      "venue" : "SLSFS, pages 34–51. Springer,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Bypassing synthesis: PLS for face recognition with pose, low-resolution and sketch",
      "author" : [ "A. Sharma", "D.W. Jacobs" ],
      "venue" : "CVPR, pages 593–600,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Generalized multiview analysis: A discriminative latent space",
      "author" : [ "A. Sharma", "A. Kumar", "H.D. III", "D.W. Jacobs" ],
      "venue" : "CVPR, pages 2160–2167,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Zero-shot learning through cross-modal transfer",
      "author" : [ "R. Socher", "M. Ganjoo", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "NIPS, pages 935–943,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multimodal learning with deep boltzmann machines",
      "author" : [ "N. Srivastava", "R. Salakhutdinov" ],
      "venue" : "NIPS, pages 2231–2239,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On accelerated proximal gradient methods for convex-concave optimization",
      "author" : [ "P. Tseng" ],
      "venue" : "submitted to SIAM Journal on Optimization,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning coupled feature spaces for cross-modal matching",
      "author" : [ "K. Wang", "R. He", "W. Wang", "L. Wang", "T. Tan" ],
      "venue" : "International Conference on Computer Vision,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K. Weinberger", "J. Blitzer", "L. Saul" ],
      "venue" : "NIPS, pages 1473–1480,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning similarity function between objects in heterogeneous spaces",
      "author" : [ "W. Wu", "J. Xu", "H. Li" ],
      "venue" : "Technical Report MSR-TR-2010-86,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Distance metric learning with application to clustering with side-information",
      "author" : [ "E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell" ],
      "venue" : "NIPS, pages 505–512,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Heterogeneous metric learning with joint graph regularization for crossmedia retrieval",
      "author" : [ "X. Zhai", "Y. Peng", "J. Xiao" ],
      "venue" : "AAAI, pages 1198–1204,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Cross-modality submodular dictionary learning for information retrieval",
      "author" : [ "F. Zhu", "L. Shao", "M. Yu" ],
      "venue" : "CIKM, pages 1479–1488,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A two-view learning approach for image tag ranking",
      "author" : [ "J. Zhuang", "S.C.H. Hoi" ],
      "venue" : "WSDM, pages 625–634,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Supervised coupled dictionary learning with group structures for multi-modal retrieval",
      "author" : [ "Y. Zhuang", "Y.F. Wang", "F. Wu", "Y. Zhang", "W. Lu" ],
      "venue" : "AAAI, pages 1070–1076,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "[29] proposed a new approach to the multimedia information retrieval problem.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "Classical algorithms have been applied to solve the problem, such as the Canonical Correlation Analysis (CCA) [15, 29] and the Partial Least Squares (PLS) [30, 32].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 28,
      "context" : "Classical algorithms have been applied to solve the problem, such as the Canonical Correlation Analysis (CCA) [15, 29] and the Partial Least Squares (PLS) [30, 32].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 29,
      "context" : "Classical algorithms have been applied to solve the problem, such as the Canonical Correlation Analysis (CCA) [15, 29] and the Partial Least Squares (PLS) [30, 32].",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 31,
      "context" : "Classical algorithms have been applied to solve the problem, such as the Canonical Correlation Analysis (CCA) [15, 29] and the Partial Least Squares (PLS) [30, 32].",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 29,
      "context" : "Similar to the CCA, the PLS also learns a latent space, but with different formulations to extract latent vectors [30].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 28,
      "context" : "In the work of [29, 8], Rasiwasia et al.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "In the work of [29, 8], Rasiwasia et al.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 31,
      "context" : "Beyond these methods, there are some other algorithms to deal with the problem, such as the generalized multiview analysis (GMA) [32], the weakly paired maximum covariance analysis (WMCA) [23], the deep neural network [27, 34], etc.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "Beyond these methods, there are some other algorithms to deal with the problem, such as the generalized multiview analysis (GMA) [32], the weakly paired maximum covariance analysis (WMCA) [23], the deep neural network [27, 34], etc.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 26,
      "context" : "Beyond these methods, there are some other algorithms to deal with the problem, such as the generalized multiview analysis (GMA) [32], the weakly paired maximum covariance analysis (WMCA) [23], the deep neural network [27, 34], etc.",
      "startOffset" : 218,
      "endOffset" : 226
    }, {
      "referenceID" : 33,
      "context" : "Beyond these methods, there are some other algorithms to deal with the problem, such as the generalized multiview analysis (GMA) [32], the weakly paired maximum covariance analysis (WMCA) [23], the deep neural network [27, 34], etc.",
      "startOffset" : 218,
      "endOffset" : 226
    }, {
      "referenceID" : 3,
      "context" : "[4, 28, 19, 42, 43, 41, 13].",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "[4, 28, 19, 42, 43, 41, 13].",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "[4, 28, 19, 42, 43, 41, 13].",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 41,
      "context" : "[4, 28, 19, 42, 43, 41, 13].",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 42,
      "context" : "[4, 28, 19, 42, 43, 41, 13].",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 40,
      "context" : "[4, 28, 19, 42, 43, 41, 13].",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 12,
      "context" : "[4, 28, 19, 42, 43, 41, 13].",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 38,
      "context" : "The target of metric learning is to find a distance metric by using the similar constraints and the dissimilar constraints [39].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 38,
      "context" : "[39], there are some popular algorithms like the Large Margin Nearest Neighbor (LMNN) [37], the Information-Theoretic Metric Learning (ITML) [9], the Relevance Component Analar X iv :1 41 1.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[39], there are some popular algorithms like the Large Margin Nearest Neighbor (LMNN) [37], the Information-Theoretic Metric Learning (ITML) [9], the Relevance Component Analar X iv :1 41 1.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "[39], there are some popular algorithms like the Large Margin Nearest Neighbor (LMNN) [37], the Information-Theoretic Metric Learning (ITML) [9], the Relevance Component Analar X iv :1 41 1.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "ysis (RCA) [2], and the Discriminative Component Analysis (DCA) which is based on the RCA [16].",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "ysis (RCA) [2], and the Discriminative Component Analysis (DCA) which is based on the RCA [16].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "The proposed algorithm is applied to multimedia information retrieval on three popularly used multimedia databases for experiments, namely the Pascal VOC2007 database [12, 32], the NUSWIDE database [7], and the Wikipedia database [29].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 31,
      "context" : "The proposed algorithm is applied to multimedia information retrieval on three popularly used multimedia databases for experiments, namely the Pascal VOC2007 database [12, 32], the NUSWIDE database [7], and the Wikipedia database [29].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 6,
      "context" : "The proposed algorithm is applied to multimedia information retrieval on three popularly used multimedia databases for experiments, namely the Pascal VOC2007 database [12, 32], the NUSWIDE database [7], and the Wikipedia database [29].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 28,
      "context" : "The proposed algorithm is applied to multimedia information retrieval on three popularly used multimedia databases for experiments, namely the Pascal VOC2007 database [12, 32], the NUSWIDE database [7], and the Wikipedia database [29].",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 14,
      "context" : "Among these cross-modal matching algorithms, the CCA algorithm is the most widely used method in the multimedia field [15, 17, 29, 24].",
      "startOffset" : 118,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "Among these cross-modal matching algorithms, the CCA algorithm is the most widely used method in the multimedia field [15, 17, 29, 24].",
      "startOffset" : 118,
      "endOffset" : 134
    }, {
      "referenceID" : 28,
      "context" : "Among these cross-modal matching algorithms, the CCA algorithm is the most widely used method in the multimedia field [15, 17, 29, 24].",
      "startOffset" : 118,
      "endOffset" : 134
    }, {
      "referenceID" : 23,
      "context" : "Among these cross-modal matching algorithms, the CCA algorithm is the most widely used method in the multimedia field [15, 17, 29, 24].",
      "startOffset" : 118,
      "endOffset" : 134
    }, {
      "referenceID" : 28,
      "context" : "[29].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "Beyond CCA, the PLS algorithm is another classical method for cross-modal data [30, 31, 32].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 30,
      "context" : "Beyond CCA, the PLS algorithm is another classical method for cross-modal data [30, 31, 32].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 31,
      "context" : "Beyond CCA, the PLS algorithm is another classical method for cross-modal data [30, 31, 32].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 28,
      "context" : "In [29] where the cross-modal IR was suggested, Rasiwasia et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "As indicated in [29, 8], the class information is an important information to reduce the semantic gap.",
      "startOffset" : 16,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "As indicated in [29, 8], the class information is an important information to reduce the semantic gap.",
      "startOffset" : 16,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "In fact, some methods targeting to the heterogenous face recognition problem are available to deal with the crossmodality IR problem, such as the Multiview Discriminant Analysis (MvDA) method [20].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 10,
      "context" : "In the transfer learning field, some algorithms are also related [11, 22, 23, 33].",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "In the transfer learning field, some algorithms are also related [11, 22, 23, 33].",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "In the transfer learning field, some algorithms are also related [11, 22, 23, 33].",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 32,
      "context" : "In the transfer learning field, some algorithms are also related [11, 22, 23, 33].",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "Such as in [23], Lampert and Krömer proposed a weakly-paired maximum covariance analysis method to deal with the not fully paired (not one-by-one paired) training data.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 35,
      "context" : "[36] proposed an iterative algorithm based on sparsity to learn the coupled feature spaces for the different modalities.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "The work in [41] also proposed a greedy dictionary construction approach to select dictionary atoms for constructing a modality-adaptive dictionary pair.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 26,
      "context" : "In the deep learning field, the works [27] and [34] both used the restricted boltzmann machine for the cross-modal feature learning.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 33,
      "context" : "In the deep learning field, the works [27] and [34] both used the restricted boltzmann machine for the cross-modal feature learning.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "[6] imported an online similarity function learning for large-scale images.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 21,
      "context" : "also proposed to learn an asymmetric transformation matrix for domain adaption [22].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 37,
      "context" : "[38].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "In [40] and [25], the authors also proposed to learn two projections for each modality to minimize the distances of the two modalities in the target feature space.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "In [40] and [25], the authors also proposed to learn two projections for each modality to minimize the distances of the two modalities in the target feature space.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 25,
      "context" : "In contrast, the proposed formulation in this paper is a strict convex problem, and the optimal solution is achieved by the accelerated proximal gradient (APG) algorithm [26, 35, 3].",
      "startOffset" : 170,
      "endOffset" : 181
    }, {
      "referenceID" : 34,
      "context" : "In contrast, the proposed formulation in this paper is a strict convex problem, and the optimal solution is achieved by the accelerated proximal gradient (APG) algorithm [26, 35, 3].",
      "startOffset" : 170,
      "endOffset" : 181
    }, {
      "referenceID" : 2,
      "context" : "In contrast, the proposed formulation in this paper is a strict convex problem, and the optimal solution is achieved by the accelerated proximal gradient (APG) algorithm [26, 35, 3].",
      "startOffset" : 170,
      "endOffset" : 181
    }, {
      "referenceID" : 38,
      "context" : "[39] to learn a distance metric according to the similar points and dissimilar points.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "The loss function is based on the logistic sigmoid function [14], δ(x) = 1/(1 + exp(−x)), which is often integrated in − log(δ(x)) to approximate the hinge loss function to avoid the discontinuity of the gradient.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 25,
      "context" : "In this subsection, the accelerated proximal gradient algorithm (APG) [26, 35] is utilized to find the optimal solution of Eqn.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 34,
      "context" : "In this subsection, the accelerated proximal gradient algorithm (APG) [26, 35] is utilized to find the optimal solution of Eqn.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "The APG algorithm is a kind of first order gradient descent method [3, 18, 26, 35], which has received popular attentions in recent years due to its fast convergence rate of O(1/t) when dealing with the problem of convex and smooth loss function regularized by non-smooth constraints.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "The APG algorithm is a kind of first order gradient descent method [3, 18, 26, 35], which has received popular attentions in recent years due to its fast convergence rate of O(1/t) when dealing with the problem of convex and smooth loss function regularized by non-smooth constraints.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "The APG algorithm is a kind of first order gradient descent method [3, 18, 26, 35], which has received popular attentions in recent years due to its fast convergence rate of O(1/t) when dealing with the problem of convex and smooth loss function regularized by non-smooth constraints.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 34,
      "context" : "The APG algorithm is a kind of first order gradient descent method [3, 18, 26, 35], which has received popular attentions in recent years due to its fast convergence rate of O(1/t) when dealing with the problem of convex and smooth loss function regularized by non-smooth constraints.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "According to [5], the minimization of the objective function Eqn.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 4,
      "context" : "Then the following objective function [5]",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : "In fact, the αt can be updated in other ways if the certain conditions are satisfied [26, 35].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 34,
      "context" : "In fact, the αt can be updated in other ways if the certain conditions are satisfied [26, 35].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "As for the step size ηt, it can be estimated in the algorithm by comparing the objective function and its proximal operator [3], whose derivation is omitted here due to the paper length.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "Compared Methods: To evaluate the performance of the proposed algorithm, several famous algorithms in the cross-media retrieval field were compared in the experiments, including the popular methods CCA [15, 29] and PLS [30, 31], and the state-of-the-art methods SCM [29], the Microsoft algorithm (MsAlg) [38], and the Generalized Multiview Analysis methods, including the GMLDA (Generalized Multiview Linear Discriminant Analysis) and the GMMFA (Generalized Multiview Marginal Fisher Analysis) [32].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 28,
      "context" : "Compared Methods: To evaluate the performance of the proposed algorithm, several famous algorithms in the cross-media retrieval field were compared in the experiments, including the popular methods CCA [15, 29] and PLS [30, 31], and the state-of-the-art methods SCM [29], the Microsoft algorithm (MsAlg) [38], and the Generalized Multiview Analysis methods, including the GMLDA (Generalized Multiview Linear Discriminant Analysis) and the GMMFA (Generalized Multiview Marginal Fisher Analysis) [32].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 29,
      "context" : "Compared Methods: To evaluate the performance of the proposed algorithm, several famous algorithms in the cross-media retrieval field were compared in the experiments, including the popular methods CCA [15, 29] and PLS [30, 31], and the state-of-the-art methods SCM [29], the Microsoft algorithm (MsAlg) [38], and the Generalized Multiview Analysis methods, including the GMLDA (Generalized Multiview Linear Discriminant Analysis) and the GMMFA (Generalized Multiview Marginal Fisher Analysis) [32].",
      "startOffset" : 219,
      "endOffset" : 227
    }, {
      "referenceID" : 30,
      "context" : "Compared Methods: To evaluate the performance of the proposed algorithm, several famous algorithms in the cross-media retrieval field were compared in the experiments, including the popular methods CCA [15, 29] and PLS [30, 31], and the state-of-the-art methods SCM [29], the Microsoft algorithm (MsAlg) [38], and the Generalized Multiview Analysis methods, including the GMLDA (Generalized Multiview Linear Discriminant Analysis) and the GMMFA (Generalized Multiview Marginal Fisher Analysis) [32].",
      "startOffset" : 219,
      "endOffset" : 227
    }, {
      "referenceID" : 28,
      "context" : "Compared Methods: To evaluate the performance of the proposed algorithm, several famous algorithms in the cross-media retrieval field were compared in the experiments, including the popular methods CCA [15, 29] and PLS [30, 31], and the state-of-the-art methods SCM [29], the Microsoft algorithm (MsAlg) [38], and the Generalized Multiview Analysis methods, including the GMLDA (Generalized Multiview Linear Discriminant Analysis) and the GMMFA (Generalized Multiview Marginal Fisher Analysis) [32].",
      "startOffset" : 266,
      "endOffset" : 270
    }, {
      "referenceID" : 37,
      "context" : "Compared Methods: To evaluate the performance of the proposed algorithm, several famous algorithms in the cross-media retrieval field were compared in the experiments, including the popular methods CCA [15, 29] and PLS [30, 31], and the state-of-the-art methods SCM [29], the Microsoft algorithm (MsAlg) [38], and the Generalized Multiview Analysis methods, including the GMLDA (Generalized Multiview Linear Discriminant Analysis) and the GMMFA (Generalized Multiview Marginal Fisher Analysis) [32].",
      "startOffset" : 304,
      "endOffset" : 308
    }, {
      "referenceID" : 31,
      "context" : "Compared Methods: To evaluate the performance of the proposed algorithm, several famous algorithms in the cross-media retrieval field were compared in the experiments, including the popular methods CCA [15, 29] and PLS [30, 31], and the state-of-the-art methods SCM [29], the Microsoft algorithm (MsAlg) [38], and the Generalized Multiview Analysis methods, including the GMLDA (Generalized Multiview Linear Discriminant Analysis) and the GMMFA (Generalized Multiview Marginal Fisher Analysis) [32].",
      "startOffset" : 494,
      "endOffset" : 498
    }, {
      "referenceID" : 11,
      "context" : "The Pascal VOC2007 dataset consists of 9963 images from 20 categories [12], which was split into a training set with 5011 images and a test set with 4952 images.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 31,
      "context" : "Since some of the images are with multi-labels, the images containing only one object were selected in the experiments [32].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "For the features, the 399-dimensional word frequency feature [17] was used for the",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "text, and the convolutional neural network (CNN) feature, which was trained on the ImageNet [21], was used as the image feature.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "The CNN source code, namely Decaf [10], can be freely downloaded on the web for research purpose.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "a dataset by selecting ten popular categories [29].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "In [29], the dataset was randomly split into a training set of 2173 image-document pairs and a test set of the remaining 693 pairs.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "The text feature was derived from the Latent Dirichlet Allocation model (LDA) with 200 topics [8].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "The NUS-WIDE dataset is a large-scale real-world database where the images were crawled from the “Flickr” website by the Media Search Lab in the National University of Singapore [7].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 6,
      "context" : "As for the text feature, the provided 1000-dimensional word frequency feature by the database organizers [7] was imported.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "The work in [8] also found that the combination of correlation matching and semantic matching can work better.",
      "startOffset" : 12,
      "endOffset" : 15
    } ],
    "year" : 2015,
    "abstractText" : "The cross-media retrieval problem has received much attention in recent years due to the rapid increasing of multimedia data on the Internet. A new approach to the problem has been raised which intends to match features of different modalities directly. In this research, there are two critical issues: how to get rid of the heterogeneity between different modalities and how to match the cross-modal features of different dimensions. Recently metric learning methods show a good capability in learning a distance metric to explore the relationship between data points. However, the traditional metric learning algorithms only focus on single-modal features, which suffer difficulties in addressing the cross-modal features of different dimensions. In this paper, we propose a cross-modal similarity learning algorithm for the crossmodal feature matching. The proposed method takes a bilinear formulation, and with the nuclear-norm penalization, it achieves low-rank representation. Accordingly, the accelerated proximal gradient algorithm is successfully imported to find the optimal solution with a fast convergence rate O(1/t). Experiments on three well known image-text crossmedia retrieval databases show that the proposed method achieves the best performance compared to the state-of-theart algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}