{
  "name" : "0912.3995.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Gaussian Process Bandits without Regret: An Experimental Design Approach",
    "authors" : [ "Niranjan Srinivas", "Andreas Krause", "Matthias Seeger" ],
    "emails" : [ "niranjan@caltech.edu", "krausea@caltech.edu", "sham@tti-c.org", "mseeger@mmci.uni-saarland.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In many real-world problems, one needs to optimize a noisy function which is expensive to evaluate. Recent examples of interest include choosing the advertisements in sponsored search to maximize profit in a click-through model [2] and learning optimal control strategies for robots [3]. A common approach is to use a probabilistic model to estimate the function response on the inputs [4, 5, 6, 7]. One natural choice that has proven effective [3] is to use Gaussian process models for the function response. However, so far, analysis of algorithms for Gaussian process optimization has proven very challenging, and there are no results on convergence rates. A key challenge is that any algorithm for GP optimization must trade off exploration – sampling to determine the shape of the function – and exploitation – sampling where the function is expected to achieve high values. We tackle this challenge by casting GP optimization as a multi-armed bandit problem [8] with a large (possibly infinite) number of arms – one for each possible input. There has been some recent success on analyzing bandit problems with many arms. For example, regret bounds are known in the case of linear payoff functions [9] and Lipschitz-continuous payoff functions [10, 11]. However, linearity is often a strong assumption, since many real-world problems (such as ad revenue and robot control) exhibit non-linear behavior. In contrast, Lipschitz-continuity is a rather weak assumption, and captures many phenomena. However, the regret bounds for Lipschitz-continuous payoff functions degrade quickly with the dimensionality of the problem (Ω(T d+1 d+2 )). By varying the covariance function, the framework of GP optimization allows us to very naturally encode various levels of smoothness or other properties of the payoff function. In fact, there is a large amount of work on kernels for objects other than Euclidean vectors, such as graphs, sets, lists, etc. [12].\n1An earlier version of this paper appeared in the NIPS 2009 workshop on Adaptive Sensing, Active Learning and Experimental Design, Whistler, B.C., Canada. See [1].\nar X\niv :0\n91 2.\n39 95\nv1 [\ncs .L\nG ]\n2 1\nD ec"
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Brochu et al. [13] provide a comprehensive review of and motivation for Bayesian optimization. The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions with the goal of minimizing the number of function evaluations is proposed in [6]; and extended for noisy black-box functions using Gaussian processes in [14]. Convergence of EGO using multivariate Gaussian processes (without an analysis of rates) is established in [15]. In fact, Brochu et al. [13] clearly point out one of the major problems with Bayesian optimization - It is also often unclear how to handle the trade-off between exploration and exploitation in the utility function. Too much exploration, and many iterations can go by without improvement. Too much exploitation leads to local maximization. Gaussian Processes are used for estimating the value function in Reinforcement Learning problems with continuous state spaces in [16, 17].\nIn the classic bandit optimization literature with independent arms, [18, 19] achieve a regret bound of O( √ KT ) for the K-armed bandit case. This bound is clearly vacuous forK =∞ orK = Ω(T ). In the infinitely many arms case, [9] and [20] assume a linear payoff function and derive regret bounds. However, the linearity assumption is very strong and not useful in many practical applications with nonlinear response functions. [10] and [11] assume the payoff function to be Lipschitz, which captures a large class of applications. However, this weak assumption comes at the price that the regret bounds scale poorly with dimension. Also, [10] analyzes the bandit problem in a metric space, while [21] allows the arms to be from a generic topological space.\nOur observation that linearity assumptions are too strong and that Lipschitz assumptions seem too weak motivates our choice to model the dependencies in the infinite-armed bandit using a Gaussian Process, since the choice of Kernel function allows one to model several levels of intermediate smoothness. Now we proceed to describe our main contributions toward GP bandit optimization."
    }, {
      "heading" : "1.2 Main Contributions",
      "text" : "We consider the problem of adaptively choosing inputs x1, x2, ..., xT to an unknown function f sampled from a Gaussian Process, such that we maximize ∑ f(xt). We cast the problem as a bandit problem with infinitely many arms, where the dependencies between the arms are modeled by the Gaussian Process. We analyze a simple and intuitive Upper-Confidence based algorithm and prove that this algorithm achieves sublinear no-regret for many popular classes of kernel functions.\nOur proof demonstrates a novel general bound for the cumulative regret in terms of maximal information gain, thus connecting GP optimization and optimal experimental design. This result does not assume any structure on the decision space D (apart from the existence of a suitable kernel function). We bound this maximal information gain for compact decision sets in Rd by exploiting known spectral properties of popular classes of kernels. Moreover, we show that the regret bounds for the squared exponential kernel depend only very weakly on the dimensionality d of the decision set. Lastly, we experimentally demonstrate that our information-gain based regret bounds accurately capture the growth of cumulative regret on several important classes of kernels."
    }, {
      "heading" : "2 Gaussian Processes",
      "text" : "A Gaussian process (c.f., [22]) is a collection of random variables such that every finite subset is distributed according to a multivariate Gaussian distribution. A GP F (x) ∼ GP (m(x), k(x, x′)) is completely specified by its mean function\nm(x) = E[F (x)]\nand its covariance function\nk(x, x′) = E[(F (x)−m(x))(F (x′)−m(x′))] A GP is therefore a distribution over functions. Moreover, the choice of covariance function is crucial since it controls the smoothness properties of the sample functions drawn from the GP.\nOne advantage of using Gaussian processes is that well-known closed-form formulae [22] exist for the mean and variance of the posterior distribution, which enables exact inference in an efficient manner. Suppose we have n noisy samples from F i.e., data D = {(xs, ys)|s = 1, ..., n}, where\nA = {1, 2, ..., n} is the set of indices of the sampling points XA = {xs|s = 1, ..., n}. To be precise, ys = f(xs) + s where s ∼ N(0, σ2) is iid Gaussian noise and f is a particular realization of F . Let XV be the set of test points and y be the vector of noisy function values ys. Define K(C,D) to be the |C| × |D| matrix of covariances between the points in set C and the points in set D, where C and D could each be either XA or XV .\nThen, the posterior predictive distribution of FV = F (XV ) is also Gaussian, with mean.\nF̄V = K(XV , XA)(K(XA, XA) + σ2In)−1y\nand variance\nV(FV ) = K(XV , XV )−K(XV , XA)(K(XA, XA) + σ2In)−1K(XA, XV )\nSome popular covariance functions include: finite-dimensional Bayesian Linear Regression (BLR), the squared exponential kernel and the Matérn class of covariance functions [22]. The simplest of these is the BLR case, where the function F is assumed to arise as a linear combination of basis functions; i.e., F (x) = ∑ αiφi(x) where φ1(x), φ2(x), ..., φq(x) are the basis functions, and α ∼ N(0, I). Then F ∼ GP (0, k), and the kernel funcion k is given by k(x, x′) = φ(x)†φ(x′), where φ(x)† = (φ1(x), φ2(x), ..., φq(x)).(Throughout this paper, Object† refers to the transpose of Object).\nAlso, the Gram covariance matrix over the sampling points x1, x2, ..., xn becomes\nG =  φ†(x1) φ†(x2)\n... φ†(xn)\n (φ(x1), φ(x2), ..., φ(xn))\nThe squared exponential kernel is k(x, x′) = exp(− |x−x ′|2\n2l2 ), where l is the lengthscale parameter. Samples from this kernel are analytic (infinitely often differentiable) with probability 1 [22].\nThe Matérn class of covariance functions is given by k(x, x′) = 2 1−ν Γ(ν) ( √ 2ν|x−x′| l ) νKν( √ 2ν|x−x′| l ), where ν is a parameter that controls smoothness and Kν is a modified Bessel function. A function arising from the Matérn class is r times mean-square differentiable if and only if r < ν. Therefore, the higher the ν, the stronger the smoothness assumptions and as ν → ∞ the Matérn covariance function reduces to the squared exponential kernel. An example plot of functions sampled from each of the three kernels is provided in Figure 1."
    }, {
      "heading" : "3 GP Optimization and the UCB algorithm",
      "text" : "Our objective while sampling is to maximize the sum of the ‘rewards’ we get, i.e., to choose our sampling points in order to maximize the sum of the function values obtained. This problem is analogous to the classical multi-armed bandit problem, where we have one arm for each possible decision. In bandit problems, we have the exploration-exploitation trade-off between the need to\nexplore various possible options and the desire to constantly choose the empirically best option. At each play t, the Upper Confidence Bound (UCB) [23, 24] algorithm chooses the arm xt by maximizing an upper confidence index; for each arm, this index is calculated by adding the current estimate of the mean of that arm and the weighted standard deviation of that arm:\nxt = argmax x∈D\nµ̂t(x) + √ βtσ̂(x, x) (1)\nSo, if an arm lies in an unexplored region, its index is likely to be high even if its initial mean estimate is low. The algorithm promises to converge to the optimal choice by ensuring that it never misses out on a ‘good’ arm due to an initial ‘bad’ return obtained for that arm. In this way, the UCB algorithm implicitly negotiates the exploration-exploitation trade-off. Classically, the UCB algorithm would keep independent statistics about mean and standard deviation for all arms. It is naturally extended to the GP optimization setting, by performing Bayesian inference, conditioning previous observations, and thereby exploiting correlation between the arms. The Gaussian Bandit UCB algorithm is provided in Figure 2. Figure 3 illustrates the performance of the UCB algorithm in two subsequent iterations.\nIf the decision set D is infinite, solving for xt in (1) is non-trivial; therefore, we discretize D into a set V = VT in our algorithm. For compact decision spaces in Rd and many kernels (with a fixed lengthscale i.e., independent of the discretization), a polynomial increase of n = |VT | = O(T τ ) suffices to ensure that the discretization error vanishes “quickly enough”. For each fixed d, τ is set appropriately. We discuss discretization details in Section 5.3.\nAnalyzing the UCB algorithm: Why does it work? UCB algorithms have been used as a heuristic in several applications [24, 2, 3]; but our current theoretical understanding of its success in GP optimization is unsatisfactory. To the best of our knowledge, we now provide the first analysis of the UCB algorithm in this context. Algorithms for bandit problems are usually analyzed in terms of their ‘cumulative regret’. For a particular choice of arm xt, and a particular realization f of F , the instantaneous regret is\nrt = f(x∗)− f(xt)\nwhere f(x) is maximized at x∗. Essentially, rt it is the mean (w.r.t noise) opportunity cost or loss of reward we incur due to our lack of knowledge of the best arm to play. The cumulative regret RT at time T is the sum of instantaneous regrets:\nRT = T∑ t=1 rt\nIn a multi-armed bandit problem, the quest is to find a no-regret algorithm for choosing the arms to play; an algorithm A is said to be no-regret if\nlim T→∞ RA(T ) T = 0\nFor the classicalK-armed bandit problem, the UCB algorithm is known to be no-regret; however the best bound available isO( √ KT ) [18, 19] and therefore vacuous for infinitely many arms (or ifK = Ω(T )). In the sequel, we will show that we can replace K in the bound by the maximum possible information gain due to sampling, thus connecting GP bandit optimization and optimal experimental design. Coupled with a proof of diminishing returns for the information gain, this will yield a noregret bound for GP bandit optimization. The key insight is the following: in the classical setting, each arm is assumed to be independent; i.e, playing one arm gives us no information at all about any other arm. Thus, the regret grows unboundedly with the number of available arms. However, the GP setting imposes smoothness through the covariance structure, and therefore playing one ‘arm’ does give us information about other ‘arms’ in its neighbourhood – therefore the information gain grows sublinearly during the sampling process."
    }, {
      "heading" : "4 Regret bounds",
      "text" : "We will now establish a bound on the cumulative regret for GP bandit optimization. First, in Section 4.1, we will show that the growth of the cumulative regret is bounded by the information due to sampling. In Section 4.2, we will then bound the information gain for important examples of kernel functions."
    }, {
      "heading" : "4.1 A general bound",
      "text" : "Throughout the discussion, wherever appropriate, upper case notation (F , Y ) refers to random variables, whereas lower case notation (f ,y) refers to their corresponding realization. Let V represent the discretization of the decision set. We observe points ys = u†sfV + s, where fV is the realization of the random unknown reward function F at the points in V , i is a Gaussian white noise process with variance σ2 and us is an indicator vector which refers to the particular member s of V we choose to observe. Further, fV is the vector of f -values at the discretization points V . We can think of picking the set of vectors ui in terms of choosing the matrix A with ui as the columns. Then, we observe YA = A†FV + , where ∼ N(0, σ2I). We define the information gain to be\nγT = max A I(FV ;YA) = max A (H(YA)−H(YA|FV )) (2)\nwhere I(·, ·) stands for mutual information [25]. Fortunately, for multivariate Gaussians, the mutual information can be readily computed in closed form, since for an n-dimensional multivariate Gaussian random variable with covariance matrix Σ, the entropy is simply 12 log((2πe) n det Σ).\nOur main result is the following regret bound:\nTheorem 1 Let 0 < δ < 1. If we run the Gaussian Bandit UCB algorithm with discretization V , |V | = T τ and parameter δ, the cumulative regret RT after T plays is bounded as:\nProb(∀T,RT ≤ √ 8TβT γT ) ≥ 1− δ\nwhere βT = 4(log( 2T τ+1 δ )) 2.\nThe proof of all our theoretical results are given in the Appendix. Thus, with probability arbitrarily close to 1, we can bound the cumulative regret in terms of the maximum possible information gain due to sampling. This connects the GP bandit problem with the problem of experimental design, where points of measurement need to be chosen in order to maximize the information gain."
    }, {
      "heading" : "4.2 Bounding the maximum possible information gain",
      "text" : "Since the bound in Theorem 1 depends on the information gain, the key remaining question is how to bound the quantity maxA I(A;FV ) ≡ I(A) for practical classes of kernels. Intuitively, due to correlation between the arms, observations for one input imply observations of “nearby” arms, and thus we expect diminishing returns in the information obtained through sampling. Thus, in order to bound γT , we exploit the fact that information gain is submodular, a natural formalization of the notion of diminishing returns. Submodularity guarantees that the information gain on sampling from one point is larger when our number of earlier sample points is low than when it is high [26]. That is, I(A ∪ {s})− I(A) ≥ I(A′ ∪ {s})− I(A′) whenever A ⊆ A′ and s /∈ A. Submodularity implies a performance guarantee on the ‘greedy’ algorithm for maximizing the information gain: A fundamental result by Nemhauser et al. [27] states that the simple greedy algorithm which always picks the input smaximizing the increase in information gain I(A∪{s})−I(A) over all possible choices of inputs s obtains at least a constant fraction of the maximum information gain: For the matrix AT associated with the set of T points chosen by the greedy algorithm it holds that\nI(AT ) ≥ (1− 1/e) max A I(A).\nThus, in order to bound the worst-case information gain incurred from T samples, it suffices to analyze the choices that the greedy algorithm makes.\nSince F is sampled from the GP, FV ∼ N(0,Σ), we have YA ∼ N(0, A†ΣA+σ2IT ) and YA|FV ∼ N(A†FV , σ2IT ). Therefore the the last term in (2) is independent of A and depends only on σ.\nSo, if we allow selecting arbitrary vectors of unit norm (instead of just point evaluations), maximizing the RHS of (2) reduces to\nmax A H(YA) = max A\nlog(detA†ΣA) + const.\nWhen A = v is just a vector, i.e., we sample just one point at a time, we have\nargmax A H(YA) = argmax ‖v‖2≤1\nv†Σv (3)\nWe know that (3) is maximized by selecting the eigenvector v of Σ with maximal absolute eigenvalue. This insight shows that the worst case bound occurs when the UCB algorithm is allowed to sample eigenvectors of Σ. Further, the submodularity of information gain allows us to choose the sampling points greedily – picking eigenvectors with maximal eigenvalues maximizes information gain. The following result bounds the information gain by how frequently each eigenvector has been sampled:\nTheorem 2 The maximal information gain γT is bounded as follows:\nγT ≤ (1− 1 e )−1 max m1,...,mn n∑ t=1 log(1 + mtλt σ2 )\nwhere mi is the number of times the eigenvector with eigenvalue λi is picked. That is, m1,m2, ...,mn with ∑ imi = T is an allocation of the T plays.\nThus, in order to bound the maximal information gain, we need to understand which allocation m1, . . . ,mn maximizes the bound in Theorem 2. This optimal allocation will depend on the spectral properties of the kernel matrices involved. If we know the decay of the eigenvalues λi, we can bound the optimal allocation using a fractional relaxation, i.e., by allowing mi be arbitrary nonnegative real numbers. Therefore, maximizing the RHS of the bound in Theorem 2 subject to the constraints∑n t=1mt = T and mt ≥ 0 using the Lagrange multiplier method, we arrive at the following optimality conditions: whenever mi,mj 6= 0 λi\nσ2 + λimi = λj σ2 + λjmj\n(4)\nExploiting the spectral decay properties of the kernel in question, we use (4) to get a recursive relation between the allocations mi and mj . From [28, 29] we know the asymptotic relationship between the eigenvalues µi of the operator defined by a kernel K and the eigenvalues λi of the Gram matrix of dimension n corresponding to the same kernel; we have λi ≈ nµi. For the squared exponential and Matérn kernels, the spectral decay results for the d dimensional case are provided in [30]. Using this general strategy, in the next section, we will derive regret bounds for three popular classes of kernels."
    }, {
      "heading" : "5 Bounds for common kernel functions",
      "text" : "We now show that if we choose to discretize such that n = Θ(T τ ), where τ is fixed in accordance with the dimension d of the decision set D, the average regret vanishes asymptotically with T . We justify this choice and discuss the details at the end of the section. Also, note that βT = O(log2(T ))."
    }, {
      "heading" : "5.1 Finite dimensional Bayesian linear regression",
      "text" : "Consider finite-dimensional Bayesian linear regression, with piecewise Lipschitz-continuous basis functions φ(x)† = (φ1(x), φ2(x), ..., φq(x)). As we saw in Section 2, the Gram covariance matrix for sample points x1, x2, ..., xn is\nG =  φ†(x1) φ†(x2)\n... φ†(xn)  (φ(x1), φ(x2), ..., φ(xn)) The maximal number of non-zero eigenvalues that G can have is q. Therefore, we have\nγT ≤ (1− 1 e )−1 q∑ t=1 log(1 + λtmt σ2 )\n≤ (1− 1 e )−1q log(1 + λmaxT σ2 )\nwhere λmax is the maximal eigenvalue of G. It is easy to see that λmax = O(n). Therefore, we have\nγT = O(log(nT ))\nFrom Theorem 1, we have the upper bound RT = O( √ TβT γT )\nFor Bayesian linear regression, since γT = O(log(nT )) and n = Θ(T τ ) , we have γT = O(log(T )), which yields\nRT = O( √ T log3(T ))\nwhich is the no-regret bound for the Bayesian linear regression case. Note that the bound is independent of the dimension of the decision set."
    }, {
      "heading" : "5.2 Squared exponential and Matérn kernels",
      "text" : "For the squared exponential kernel, we know µs ≤ b d 2 c−s 1/d∀ s ≥ 0, where b < 1 and c > 1 are constants and d is the dimension of the decision space. Therefore, for the eigenvalues of the Gram matrix, we have\nλi ≤ nb d 2 c−s 1/d .\nTherefore, equation (4) implies\nmt = m1 − σ2c\nn b d 2 (ct 1/d−1 − 1)\nAs is intuitive, the allocations decrease as the eigenvalues decay. Let N0 be the number of non-zero allocations. We want to enforce 0 ≤ mt for t ≤ N0. We have, since m1 ≤ T ,\n0 ≤ mt ≤ T − σ2c\nn b d 2 (ct 1/d−1 − 1) (5)\nSolving (5) for t gives us\nt ≤ (1 + log(1 + nTσ2cb d 2 )\nlog(c) )d\nwhich yields N0 = O((log(nT ))d)\nTherefore, we have\nγT ≤ (1− 1 e )−1 n∑ t=1 log(1 + mtλt σ2 )\n= (1− 1 e\n)−1 (log(nT ))d∑\nt=1\nlog(1 + mtλt σ2 )\n≤ (1− 1 e )−1(log(nT ))d log(1 + nT σ2c b d 2 ) ≈ (log(nT ))d+1\nUsing Theorem 1 and γT = O((log(nT ))d+1),\nRT = O( √ T (log(T )) d+3 2 )\nThis forces the average regret to go to zero asymptotically wth T . Moreover, note that the regret bound has very weak dependence on the dimension; d is important only as an index for a term polylog in T .\nFor the Matérn kernels, we have µs = O(s− 2ν+d d ) ∀ s ≥ 0, where ν is the Matérn smoothness parameter and d, the dimension of the decision space. The analysis is similar to the squared exponential case:\nCorollary. For the Matérn kernel with smoothness parameter ν, we have\nRT = O(T 1 2 + (τ+1)d 2(2ν+d) log(T )). (6)\nEquation (6) suggests that the average regret vanishes asymptotically wth T provided we have ν > τd 2 . This is intuitive – the higher the dimensionality of the problem, the stronger our smoothness assumptions need to be to guarantee zero asymptotic average regret. However, we actually need to ensure ν > min( τd2 , 2) since the lower bound of 2 ensures Lipschitz continuity, as demonstrated below."
    }, {
      "heading" : "5.3 Discretization of the decision set",
      "text" : "We need to ensure that the discretization error vanishes asympotically – that is, that the optimal point over the discretization converges to the optimum over the decision set. First, we show that if f is Lipschitz continuous, this requirement is satisfied.\nLet x∗ = argmaxD f and xV = argmaxV f . Then, we have |F (x∗) − F (xV )| ≤ κ‖x∗ − xV ‖ for some κ > 0. Moreover, if we choose n = Θ(T τ ), we have ‖x∗ − xV ‖ ≤ √ d\nT τ/d . Therefore, an\nappropriate choice of τ (for example, d2 ) is sufficient to ensure that the discretization error vanishes asymptotically sufficiently quicky (i.e. faster than 1√\nT ) since d is fixed and known.\nTherefore, we just need to prove Lipschitz continuity of our payoff function f . For the BLR and squared exponential cases the sample functions are infinitely smooth and so the Lipschitz argument follows trivially. For all Matérn kernels, ν > 2 implies continuity of the derivative of the sample function f - which is sufficient for Lipschitz continuity through the mean value theorem, since D is compact. Therefore, we conclude that appropriate choice of discretization parameter τ guarantees zero asymptotic average regret."
    }, {
      "heading" : "6 Experiments",
      "text" : "We perform some preliminary experiments to demonstrate that our bounds accurately capture the growth of the cumulative regret. We discretize the unit interval D = [0, 1] uniformly into V ⊆ D where |V | = n = 1000. We ran the UCB algorithm for T = 1000 (results averaged over 30 random runs) for the Squared Exponential and Matérn (with ν = 2.5) kernels, each using lengthscale param-\neter 0.01. The sampling noise variance σ2 was set to .001 and the precision/confidence parameter delta to 0.01.\nWe computed the actual cumulative regret incurred and information gained by the UCB algorithm. We also ran the greedy algorithm in order to compute the regret bound from Theorem 1 and the bound on the information gain from Theorem 2. Figure 4 shows the results for the squared exponential kernel, and Figure 5 shows the results for the Matèrn kernel. While there is a gap between the actual incurred regret and our bound (Figures 4(a) and 5(a)), we find that the actual cumulative regret correlates strongly with the regret bound (Figures 4(b) and 5(b)), indicating that our constants may be loose, but the rates correspond accurately with the actual growth of the regret. Similarly, there is strong correlation between the actual information gain incurred and the bound on information gain (Figures 4(d) and 5(d))."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we prove the first sublinear regret bounds for the UCB algorithm for GP optimization for several popular kernel functions. Our regret bounds are independent of the dimension of the decision set for Bayesian Linear Regression and have very weak dependence for the squared exponential kernel. For the Matèrn Kernel, we demand higher smoothness for ensuring zero asymptotic average regret as the dimensionality of the decision space increases, which is very intuitive.\nMoreover, our proof proceeds by bounding the cumulative regret in terms of maximal information gain without assuming any structure on the decision space D, thus yielding a natural connection between Gaussian process bandit optimization and optimal experimental design. Our preliminary\nexperiments indicate that our bounds accurately capture the growth of the cumulative regret for the UCB algorithm."
    }, {
      "heading" : "A Proofs",
      "text" : "First, we prove the corollary stated for the Matérn class of kernels in Section 5.2.\nProof [Regret bounds for Matérn class of kernels] For the Matérn kernels, we have µs = O(s− 2ν+dd ) ∀ s ≥ 0, where ν is the Matérn smoothness parameter and d, the dimension of the decision space. Therefore, for the eigenvalues of the Gram matrix, we have\nλs = O(ns− 2ν+d d )\n(4) implies\nmt = m1 − σ2\nn (t\n2ν+d d − 1) (7)\nCalculations directly analogous to the squared exponential kernel case yield\nN0 ≤ (1 + nT\nσ2 )\nd 2ν+d\nwhich implies N0 = O((nT ) d 2ν+d )\nTherefore, bounding γT similarly, we have\nγT = O((nT ) d 2ν+d log(nT )). (8)\nUsing Theorem 1 and (8),\nRT = O(T 1 2 + (τ+1)d 2(2ν+d) log(T )). (9)\nNow, we state a theorem that is crucial for proving our main result Theorem 1.\nTheorem 3 (Sum of Squares Regret Bound) Let δ > 0. Then, if rt = f(x∗)− f(xt) is the instantaneous regret for Gaussian Bandit UCB on round t, we have, with probability 1− δ\nT∑ t=1 r2t ≤ 8βT γT\nwhere βt = 4(log( 2T τ+1 δ )) 2.\nNow, we prove that Theorem 1 follows from Theorem 3. Proof [Proof of Theorem 1] By Theorem 3, we know that with probability at least 1−δ, ∑T t=1 r 2 t ≤ 8βT γT . Applying the Cauchy-Schwarz inequality, we have, with probability at least 1− δ\nRT = T∑ t=1 rt\n≤ ( T\nT∑ t=1 r2t )1/2 which completes the proof.\nBefore we prove Theorem 3, we prove a series of useful lemmas. First, we prove a lemma that helps us understand how the covariance/precision matrices are updated after each round.\nLemma 1 Suppose FV , Y , ys, us and s are defined as before. We know FV |Y = ys is normally distributed with mean 0; suppose it has the covariance matrix ΣFV |Y . Then, we have\nΣF |Y = (P1 + σ−2usu†s) −1\nwhere P1 = Σ−1.\nProof For simplicity, we drop the indexing subscripts. We know( F\nY\n) = (\n1 0 u† 1\n)( F ) and therefore (\nF\nY\n) ∼ N(0, V )\nwhere\nV = (\n1 0 u† 1 )( Σ 0 0 σ2 )( 1 u 0 1 ) = (\nΣ Σu u†Σ u†Σu+ σ2 ) Also, by using the standard expression for posterior variance on Gaussian updation using Bayes’ rule,\nΣFV |Y = Σ− (Σu)(u †Σu+ σ2)−1(u†Σ) (10)\nUsing the Matrix-Inversion lemma, the RHS of (10) simplifies directly to (Σ−1 + σ−2uu†)−1. Therefore, we have ΣF |Y = (P1 + σ−2uu†)−1 (11) The result follows for more than one observation by induction.\nCorollary. From the Bayesian update rule we have just proved in Lemma 1, it follows that\nγT = max u1,u2,...uT log\n( detP1 + σ−2 ∑T t=1 utu † t\ndetP1\n)\nOur next key insight is that on any round t, with probability 1−δ, the instantaneous regret is at most the “width” of the confidence bound in the direction of the chosen decision. We now formalize this.\nDefine wt := σ−1 √ x†tP −1 t xt\nwhich we interpret as the “normalized width” at time t in the direction of the chosen decision. The true width, 2 √ βtwt, turns out to be an upper bound for the instantaneous regret.\nLemma 2 For any 0 < δ < 1, choosing βt = 4(log( 2nTδ )) 2 yields P ( ∀ s ∈ V, t ≤ T, |fs − f̂s,t| ≤ √ βtσ2s,t ) ≥ 1− δ ∀ T.\nwhere fs is the true mean marginal payoff function at s, f̂s,t is our estimate of fs at step t of the algorithm, and σ2s,t is the marginal posterior variance at s at step t.\nProof By assumption, f is sampled from a GP with known prior mean and covariance function. Suppose that f̂s,t is our posterior distribution along the marginal s at step t after t observations. Then, because of the posterior mean is Gaussian, we have\nP (|fs − f̂s,t| > cσs,t) ≤ 2e− c 2\nUsing the union bound, we have\nP (∃ s, t | |fs − f̂s,t| > cσs,t) ≤ 2nTe− c 2\nTherefore, P (∀ s, t, |fs − f̂s,t| ≤ cσs,t) ≥ 1− 2nTe− c 2 Choosing c = √ βt, we have\nP (∀ s, t, |fs − f̂s,t| ≤ √ βtσs,t) ≥ 1− 2nTe− √ βt 2\nNow choose βt = 4(log( 2nTδ )) 2, which yields\nP (∀ s, t, |fs − f̂s,t| ≤ √ βtσs,t) ≥ 1− 2nT δ\n2nT ≥ 1− δ\nIn the next lemma we provide a probabilistic bound for the instantaneous regret incurred by the algorithm, hence formalizing our insight that with probability 1 − δ, the instantaneous regret is at most the “width” of the confidence bound in the direction of the chosen decision.\nLemma 3 Let wt be the standard deviation in the direction of the input s chosen by our algorithm at step t. Then, we show that the instantaneous regret rt is bounded by 2 min( √ βtwt, 1), provided the high probability event (∀ s′ ∈ V, t ≤ T, |fs′ − f̂s′,t| ≤ √ βtσs′,t) is true.\nProof Clearly, rt = maxs fs − fs. Therefore we have\nrt ≤ |max s fs − fs|\n= |max s fs − f̂s,t + f̂s,t − fs|\n≤ |max s fs − f̂s,t|+ |f̂s,t − fs| ≤ √ βtwt + √ βtwt\n= 2 √ βtwt\nFor the last step, use Lemma 2 and observe that the choice of s by the algorithm implies f̂s,t + √ βTwt > maxs fs. Also, notice that without loss of generality we can assume that the loss at any step is bounded by 1, which yields the other part of the bound.\nThe following fact is useful.\nLemma 4 For every t ≤ T , detPt+1 detP1 = t∏\nτ=1\n(1 + w2t ).\nProof By the way the precision matrices are updated i.e. from Lemma 1, we have\ndetPt+1 = det(Pt + σ−2xtx † t)\n= det(P 1/2t (I + P −1/2 t σ −2xtx † tP −1/2)P 1/2) = detPt det(I + P −1/2 t σ −1xt(P −1/2 t σ −1xt)†) = detPt det(I + vtv † t ),\nwhere vt := σ−1P −1/2 t xt. Now observe that v † t vt = w2t and\n(I + vtv † t )vt = vt + vt(v † t vt) = (1 + w 2 t )vt\nHence (1+w2t ) is an eigenvalue of I+vtv † t . Since vtv † t is a rank one matrix, all the other eigenvalues of I + vtv † t equal 1. It follows that det(I + vtv † t ) is (1 + w2t ), and so\ndetPt+1 = (1 + w2t ) detPt.\nThe result follows now by induction.\nFinally, we are ready to prove Theorem 3.\nProof [Proof of Theorem 3] First, by using the fact that for any 0 ≤ z ≤ 1, ln(1 + z) ≥ z/2, we have min(w2τ , 1) ≤ 2 ln(1 + w2τ ) Also, we have the following probabilistic bounds:\nT∑ t=1 r2t ≤ T∑ t=1 4βt min(w2t , 1) by Lemma 3\n≤ 4βT T∑ t=1 min(w2t , 1) since 1 < β1 < · · · < βT\n≤ 8βT T∑ t=1 ln(1 + w2τ ) since 1 < β1 < · · · < βT\n≤ 8βT ln (\ndetPt+1 detP1\n)\nThe proof follows from the “maximal” nature of the definition of γ.\nOur next lemma shows that the information gain is submodular in our setting. This allows us to bound the information gain by using a greedy algorithm.\nLemma 5 Information gain in our setting is submodular.\nProof We need to show that the information gain satisfies the weak assumptions in [26]. Let A be the index set of points we observe. Then, our information measure is G(A) = I(F ;YA). We have\nYs = u†sFV + s and therfore Yi and Yj are clearly conditionally independent given FV . Hence, information gain is submodular.\nNext, we compute the information gain of the greedy algorithm when it samples an eigenvector.\nLemma 6 Suppose the ith decision point is chosen and Σi is updated to Σi+1. Then, if λ1 is the eigenvalue of Σi corresponding to the eigenvector v1 picked by Gaussian Bandit UCB, then Σi+1 has the same eigenvalues and eigenvectors as Σi with the exception of λ1, which is reduced.\nProof Without loss of generality, assume i = 1. Then\nΣ1 = n∑ i=1 λiviv † i\nand Σ2 = (Σ−11 + σ −2v1v † 1) −1 and since\nΣ−11 = N∑ i=1 1 λi viv † i\nΣ2 = n∑ i=2 λiviv † i + ( 1 λ1 + 1 σ2 )−1v1v † 1\nHence, all eigenvectors are exactly the same, and all eigenvalues with the exception of λ1 which has been reduced to λ1( σ 2\nσ2+λ1 ).\nLemma 7 The Information gain after sampling mt times at each of the n eigenvectors vt corresponding to eigenvalues λt is ∑n t=1 log(1 + mtλt σ2 ).\nProof We shall show that the information gain on sampling once from eigenvector v1 corresponding to eigenvalue λ1 is log(1 + λ1σ2 ), the rest follows by induction.\nSuppose we are sampling at Fs ∼ N(0, λ1). However, since our sampling is noisy, we actually observe from Y = Fs + s, where s ∼ N(0, σ2). We know that F |Y = ys is normally distributed with variance λ1σ 2\nλ1+σ2 .\nTherefore, the information gain obtained is log λ1 − log( λ1σ 2\nλ1+σ2 ) = log(1 + λ1σ2 ).\nProof [Proof of Theorem 2] Follows from Lemma 6 and Lemma 7. Lemma 6 implies that the eigenvectors of the covariance matrix contain independent information; sampling an eigenvector gives Gaussian Bandit UCB information only about that eigenvector and not about any others. This demonstrates that the information gained by Gaussian Bandit UCB just adds after each round. Lemma 7 computes the information gained by Gaussian Bandit UCB at each round, and therefore Theorem 2 follows.\nNow, we derive the recurrence relation λiσ2+λimi = λj σ2+λjmj by maximizing the RHS of (2) subject to the constraints ∑n t=1mt = T and mt ≥ 0.\nLemma 8 mi,mj 6= 0 =⇒ λiσ2+λimi = λj σ2+λjmj\nProof Writng the Lagrangian, we need to maximize\nL = n∑ t=1 log(1 + mtλt σ2 )− k0( n∑ t=1 mt − T )− n∑ t=1 ktmt\nsubject to kt ≤ 0 ∀ i, and the complimentary slackness conditions ktmt = 0 ∀ t, and k0( ∑n t=1mt− T ) = 0.\nDifferentiating L with respect to mt and setting it to 0, we have\nkt + k0 = λt\nσ2 +mtλt ∀ t (12)\nMultiplying (12) by mt and using the complimentary slackness conditions, we have\nmtk0 = mtλt\nmtλt + σ2\nwhich implies either mt = 0 or k0 = λtσ2+λtmt . Therfore, whenever mi,mj 6= 0 we have\nλi σ2 + λimi = λj σ2 + λjmj"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>We consider the problem of optimizing an unknown, noisy function that is ex-<lb>pensive to evaluate. We cast this problem as a multiarmed bandit problem where<lb>the payoff function is sampled from a Gaussian Process. We resolve an important<lb>open problem on deriving regret bounds for this setting. In particular, we ana-<lb>lyze an upper confidence algorithm and bound its cumulative regret in terms of<lb>the maximal information gain due to sampling, thus connecting Gaussian Process<lb>bandits and optimal experimental design. Moreover, we bound the maximal infor-<lb>mation gain by exploiting known spectral properties of popular classes of kernels<lb>and obtain sub-linear regret bounds for our algorithm. In particular, we show that,<lb>perhaps surprisingly, the regret bounds for the squared exponential kernel depend<lb>only very weakly on the dimensionality of the problem.",
    "creator" : "LaTeX with hyperref package"
  }
}