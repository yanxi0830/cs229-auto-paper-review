{
  "name" : "1505.05798.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret",
    "authors" : [ "Haitham Bou Ammar", "Rasul Tutunov", "Eric Eaton" ],
    "emails" : [ "HAITHAMB@SEAS.UPENN.EDU", "TUTUNOV@SEAS.UPENN.EDU", "EEATON@CIS.UPENN.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1. Introduction Reinforcement learning (RL) (Busoniu et al., 2010; Sutton & Barto, 1998) often requires substantial experience before achieving acceptable performance on individual control problems. One major contributor to this issue is the tabula-rasa assumption of typical RL methods, which learn from scratch on each new task. In these settings, learning performance is directly correlated with the quality of the acquired samples. Unfortunately, the amount of experience necessary for high-quality performance increases exponentially with the tasks’ degrees of freedom, inhibiting the application of RL to high-dimensional control problems.\nWhen data is in limited supply, transfer learning can significantly improve model performance on new tasks by reusing previous learned knowledge during training (Taylor & Stone, 2009; Gheshlaghi Azar et al., 2013; Lazaric, 2011; Ferrante et al., 2008; Bou Ammar et al., 2012). Multitask learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultane-\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008).\nIn the lifelong learning setting (Thrun & O’Sullivan, 1996a;b), which can be framed as an online MTL problem, agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Recently, based on the work of Ruvolo & Eaton (2013) on supervised lifelong learning, Bou Ammar et al. (2014) developed a lifelong learner for policy gradient RL. To ensure efficient learning over consecutive tasks, these works employ a second-order Taylor expansion around the parameters that are (locally) optimal for each task without transfer. This assumption simplifies the MTL objective into a weighted quadratic form for online learning, but since it is based on single-task learning, this technique can lead to parameters far from globally optimal. Consequently, the success of these methods for RL highly depends on the policy initializations, which must lead to near-optimal trajectories for meaningful updates. Also, since their objective functions average loss over all tasks, these methods exhibit non-vanishing regrets of the form O(R), where R is the total number of rounds in a non-adversarial setting.\nIn addition, these methods may produce control policies with unsafe behavior (i.e., capable of causing damage to the agent or environment, catastrophic failure, etc.). This is a critical issue in robotic control, where unsafe control policies can lead to physical damage or user injury. This problem is caused by using constraint-free optimization over the shared knowledge during the transfer process, which may lead to uninformative or unbounded policies.\nIn this paper, we address these issues by proposing the first safe lifelong learner for policy gradient RL operating in an adversarial framework. Our approach rapidly learns highperformance safe control policies based on the agent’s previously learned knowledge and safety constraints on each task, accumulating knowledge over multiple consecutive tasks to optimize overall performance. We theoretically analyze the regret exhibited by our algorithm, showing sublinear dependency of the form O( √ R) for R rounds, thus outperforming current methods. We then evaluate our approach empirically on a set of dynamical systems.\nar X\niv :1\n50 5.\n05 79\n8v 1\n[ cs\n.L G\n] 2\n1 M\nay 2\n01 5\n2. Background 2.1. Reinforcement Learning\nAn RL agent sequentially chooses actions to minimize its expected cost. Such problems are formalized as Markov decision processes (MDPs) 〈X ,U ,P, c, γ〉, where X ⊂ Rd is the (potentially infinite) state space, U ∈ Rda is the set of all possible actions, P : X × U × X → [0, 1] is a state transition probability describing the system’s dynamics, c : X × U × X → R is the cost function measuring the agent’s performance, and γ ∈ [0, 1] is a discount factor. At each time step m, the agent is in state xm ∈ X and must choose an action um ∈ U , transitioning it to a new state xm+1 ∼ P (xm+1|xm,um) and yielding a cost cm+1 = c(xm+1,um,xm). The sequence of state-action pairs forms a trajectory τ = [x0:M−1,u0:M−1] over a (possibly infinite) horizon M . A policy π : X ×U → [0, 1] specifies a probability distribution over state-action pairs, where π (u|x) represents the probability of selecting an actionu in state x. The goal of RL is to find an optimal policy π? that minimizes the total expected cost.\nPolicy search methods have shown success in solving high-dimensional problems, such as robotic control (Kober & Peters, 2011; Peters & Schaal, 2008a; Sutton et al., 2000). These methods represent the policy πα(u|x) using a vector α ∈ Rd of control parameters. The optimal policy π? is found by determining the parameters α? that minimize the expected average cost:\nl(α) = n∑ k=1 pα ( τ (k) ) C ( τ (k) ) , (1)\nwhere n is the total number of trajectories, and pα ( τ (k) ) andC ( τ (k) ) are the probability and cost of trajectory τ (k):\npα\n( τ (k) ) = P0 ( x (k) 0 )M−1∏ m=0 P ( x (k) m+1|x(k)m ,u(k)m ) × πα ( u(k)m |x(k)m\n) (2) C ( τ (k) ) = 1\nM M−1∑ m=0 c ( x (k) m+1,u (k) m ,x (k) m ) , (3)\nwith an initial state distribution P0 : X → [0, 1]. We handle a constrained version of policy search, in which optimality not only corresponds to minimizing the total expected cost, but also to ensuring that the policy satisfies safety constraints. These constraints vary between applications, for example corresponding to maximum joint torque or prohibited physical positions.\n2.2. Online Learning & Regret Analysis\nIn this paper, we employ a special form of regret minimization games, which we briefly review here. A regret minimization game is a triple 〈K,F , R〉, where K is a nonempty decision set, F is the set of moves of the adversary\nwhich contains bounded convex functions from Rn to R, and R is the total number of rounds. The game proceeds in rounds, where at each round j = 1, . . . , R, the agent chooses a prediction θj ∈ K and the environment (i.e., the adversary) chooses a loss function lj ∈ F . At the end of the round, the loss function lj is revealed to the agent and the decision θj is revealed to the environment. In this paper, we handle the full-information case, where the agent may observe the entire loss function lj as its feedback and can exploit this in making decisions. The goal is to minimize the cumulative regret ∑R j=1 lj(θj)−infu∈K [∑R j=1 lj(u) ] . When analyzing the regret of our methods, we use a variant of this definition to handle the lifelong RL case:\nRR = R∑ j=1 ltj (θj)− inf u∈K  R∑ j=1 ltj (u)  , where ltj (·) denotes the loss of task t at round j. For our framework, we adopt a variant of regret minimization called “Follow the Regularized Leader,” which minimizes regret in two steps. First, the unconstrained solution θ̃ is determined (see Sect. 4.1) by solving an unconstrained optimization over the accumulated losses observed so far. Given θ̃, the constrained solution is then determined by learning a projection into the constraint set via Bregman projections (see Abbasi-Yadkori et al. (2013)).\n3. Safe Lifelong Policy Search We adopt a lifelong learning framework in which the agent learns multiple RL tasks consecutively, providing it the opportunity to transfer knowledge between tasks to improve learning. Let T denote the set of tasks, each element of which is an MDP. At any time, the learner may face any previously seen task, and so must strive to maximize its performance across all tasks. The goal is to learn optimal policies π?α?1 , . . . , π ? α?|T | for all tasks, where policy π?α?t for task t is parameterized by α?t ∈ Rd. In addition, each task is equipped with safety constraints to ensure acceptable policy behavior: Atαt ≤ bt, with At ∈ Rd×d and bt ∈ Rd representing the allowed policy combinations. The precise form of these constraints depends on the application domain, but this formulation supports constraints on (e.g.) joint torque, acceleration, position, etc.\nAt each round j, the learner observes a set of ntj trajectories { τ (1) tj , . . . , τ (ntj ) tj } from a task tj ∈ T , where each trajectory has length Mtj . To support knowledge transfer between tasks, we assume that each task’s policy parameters αtj ∈ Rd at round j can be written as a linear combination of a shared latent basis L ∈ Rd×k with coefficient vectors stj ∈ Rk; therefore, αtj = Lstj . Each column of L represents a chunk of transferrable knowledge; this task construction has been used successfully in previous\nmulti-task learning work (Kumar & Daumé III, 2012; Ruvolo & Eaton, 2013; Bou Ammar et al., 2014). Extending this previous work, we ensure that the shared knowledge repository is “informative” by incorporating bounding constraints on the Frobenius norm ‖ · ‖F of L. Consequently, the optimization problem after observing r rounds is:\nmin L,S r∑ j=1 [ ηtj ltj ( Lstj )] + µ1 ||S||2F + µ2 ||L|| 2 F (4)\ns.t. Atjαtj ≤ btj ∀tj ∈ Ir λmin ( LLT ) ≥ p and λmax ( LLT ) ≤ q ,\nwhere p and q are the constraints on ‖L‖F, ηtj ∈ R are design weighting parameters1, Ir = {t1, . . . , tr} denotes the set of all tasks observed so far through round r, and S is the collection of all coefficients\nS(:, h) = { sth if th ∈ Ir 0 otherwise ∀h ∈ {1, . . . , |T |} .\nThe loss function ltj (αtj ) in Eq. (4) corresponds to a policy gradient learner for task tj , as defined in Eq. (1). Typical policy gradient methods (Kober & Peters, 2011; Sutton et al., 2000) maximize a lower bound of the expected cost ltj ( αtj ) , which can be derived by taking the logarithm and applying Jensen’s inequality: log [ ltj ( αtj )] = log ntj∑ k=1 p (tj) αtj ( τ (k) tj ) C(tj) ( τ (k) tj\n) (5) ≥ log [ ntj ] + E Mtj−1∑ m=0 log [ παtj ( u(k,tj)m | x(k,tj)m )]ntj k=1 +const .\nTherefore, our goal is to minimize the following objective:\ner = r∑ j=1 − ηtj ntj ntj∑ k=1 Mtj−1∑ m=0 log [ παtj ( u(k,tj)m | x(k,tj)m )] (6)\n+ µ1 ‖S‖2F + µ2 ‖L‖ 2 F\ns.t. Atjαtj ≤ btj ∀tj ∈ Ir λmin ( LLT ) ≥ p and λmax ( LLT ) ≤ q .\n3.1. Online Formulation\nThe optimization problem above can be mapped to the standard online learning framework by unrolling L and S into a vector θ = [vec(L) vec(S)]T ∈ Rdk+k|T |. Choosing Ω0(θ) = µ2 ∑dk i=1 θ 2 i + µ1 ∑dk+k|T | i=dk+1 θ 2 i , and Ωj(θ) = Ωj−1(θ) + ηtj ltj (θ), we can write the safe lifelong policy search problem (Eq. (6)) as:\nθr+1 = argmin θ∈K Ωr(θ) , (7)\nwhere K ⊆ Rdk+k|T | is the set of allowable policies under the given safety constraints. Note that the loss for task tj\n1We describe later how to set the η’s later in Sect. 5 to obtain regret bounds, and leave them as variables now for generality.\ncan be written as a bilinear product in θ:\nltj (θ) = − 1\nntj ntj∑ k=1 Mtj−1∑ m=0 log [ π (tj) ΘLΘstj ( u(k, tj)m | x(k, tj)m )]\nΘL =  θ1 . . . θd(k−1)+1... ... ... θd . . . θdk  , Θstj =  θdk+1... θ(d+1)k+1  . We see that the problem in Eq. (7) is equivalent to Eq. (6) by noting that at r rounds, Ωr = ∑r j=1 ηtj ltj (θ)+Ω0(θ).\n4. Online Learning Method We solve Eq. (7) in two steps. First, we determine the unconstrained solution θ̃r+1 when K = Rdk+k|T | (see Sect. 4.1). Given θ̃r+1, we derive the constrained solution θ̂r+1 by learning a projection ProjΩr,K ( θ̃r+1 ) to the constraint set K ⊆ Rdk+k|T |, which amounts to minimizing the Bregman divergence over Ωr(θ) (see Sect. 4.2)2. The complete approach is given in Algorithm 1 and is available as a software implementation on the authors’ websites.\n4.1. Unconstrained Policy Solution\nAlthough Eq. (6) is not jointly convex in both L and S, it is separably convex (for log-concave policy distributions). Consequently, we follow an alternating optimization approach, first computing L while holding S fixed, and then updating S given the acquiredL. We detail this process for two popular PG learners, eREINFORCE (Williams, 1992) and eNAC (Peters & Schaal, 2008b). The derivations of the update rules below can be found in Appendix A.\nThese updates are governed by learning rates β and λ that decay over time; β and λ can be chosen using line-search methods as discussed by Boyd & Vandenberghe (2004). In our experiments, we adopt a simple yet effective strategy, where β = cj−1 and λ = cj−1, with 0 < c < 1.\nStep 1: UpdatingL HoldingS fixed, the latent repository can be updated according to:\nLβ+1 = Lβ − ηβL∇Ler(L,S) (eREINFORCE) Lβ+1 = Lβ − ηβLG −1(Lβ ,Sβ)∇Ler(L,S) (eNAC)\nwith learning rate ηβL ∈ R, and G−1(L,S) as the inverse of the Fisher information matrix (Peters & Schaal, 2008b).\nIn the special case of Gaussian policies, the update for L\n2In Sect. 4.2, we linearize the loss around the constrained solution of the previous round to increase stability and ensure convergence. Given the linear losses, it suffices to solve the Bregman divergence over the regularizer, reducing the computational cost.\ncan be derived in a closed form as Lβ+1 = Z−1L vL, where\nZL =2µ2Idk×dk+ r∑ j=1 ηtj ntjσ 2 tj ntj∑ k=1 Mtj−1∑ m=0 vec ( ΦsTtj )( ΦT⊗sTtj )\nvL = ∑ j ηtj ntjσ 2 tj ntj∑ k=1 Mtj−1∑ m=0 vec ( u(k, tj)m Φs T tj ) ,\nσ2tj is the covariance of the Gaussian policy for a task tj , and Φ = Φ ( x (k, tj) m ) denotes the state features.\nStep 2: Updating S Given the fixed basis L, the coefficient matrix S is updated column-wise for all tj ∈ Ir:\ns (tj) λ+1 = s (tj) λ+1 − η λ S∇stj er(L,S) (eREINFORCE)\ns (tj) λ+1 = s (tj) λ+1 − η λ SG −1(Lβ ,Sβ)∇stj er(L,S) (eNAC) with learning rate ηλS ∈ R. For Gaussian policies, the closed-form of the update is stj = Z −1 stj vstj , where\nZstj = 2µ1Ik×k + ∑ tk=tj ηtj ntjσ 2 tj ntj∑ k=1 Mtj−1∑ m=0 LTΦΦTL\nvtj = ∑ tk=tj ηtj ntjσ 2 tj ntj∑ k=1 Mtj−1∑ m=0 u(k, tj)m L TΦ .\n4.2. Constrained Policy Solution\nOnce we have obtained the unconstrained solution θ̃r+1 (which satisfies Eq. (7), but can lead to policy parameters in unsafe regions), we then derive the constrained solution to ensure safe policies. We learn a projection ProjΩr,K ( θ̃r+1 ) from θ̃r+1 to the constraint set:\nθ̂r+1 = argmin θ∈K BΩr,K\n( θ, θ̃r+1 ) , (8)\nwhereBΩr,K ( θ, θ̃r+1 ) is the Bregman divergence over Ωr:\nBΩr,K ( θ, θ̃r+1 ) = Ωr(θ)−Ωr(θ̃r+1)\n− trace ( ∇θΩr (θ) ∣∣∣ θ̃r+1 ( θ − θ̃r+1 )) .\nSolving Eq. (8) is computationally expensive since Ωr(θ) includes the sum back to the original round. To remedy this problem, ensure the stability of our approach, and guarantee that the constrained solutions for all observed tasks lie within a bounded region, we linearize the current-round loss function ltr (θ) around the constrained solution of the previous round θ̂r:\nltr (û) = f̂tr ∣∣∣T θ̂r û , (9)\nwhere\nf̂tr ∣∣∣ θ̂r =\n ∇θltr (θ) ∣∣∣ θ̂r\nltr (θ) ∣∣∣ θ̂r −∇θltr (θ) ∣∣∣ θ̂r θ̂r\n , û = [ u 1 ] .\nGiven the above linear form, we can rewrite the optimization problem in Eq. (8) as:\nθ̂r+1 = argmin θ∈K BΩ0,K\n( θ, θ̃r+1 ) . (10)\nConsequently, determining safe policies for lifelong policy search reinforcement learning amounts to solving:\nmin L,S\nµ1‖S‖2F + µ2‖L‖2F\n+ 2µ1trace ( ST ∣∣∣ θ̃r+1 S ) + 2µ2trace ( L ∣∣∣ θ̃r+1 L ) s.t.AtjLstj ≤ btj ∀tj ∈ Ir LLT ≤ pI and LLT ≥ qI .\nTo solve the optimization problem above, we start by converting the inequality constraints to equality constraints by introducing slack variables ctj ≥ 0. We also guarantee that these slack variables are bounded by incorporating ‖ctj‖ ≤ cmax, ∀tj ∈ {1, . . . , |T |}:\nmin L,S,C\nµ1‖S‖2F + µ2‖L‖2F\n+ 2µ2trace ( LT ∣∣∣ θ̃r+1 L ) + 2µ1trace ( ST ∣∣∣ θ̃r+1 S ) s.t.AtjLstj = btj − ctj ∀tj ∈ Ir ctj > 0 and ‖ctj‖2 ≤ cmax ∀tj ∈ Ir LLT ≤ pI and LLT ≥ qI .\nWith this formulation, learning ProjΩr,K ( θ̃r+1 ) amounts to solving second-order cone and semi-definite programs.\n4.2.1. SEMI-DEFINITE PROGRAM FOR LEARNING L\nThis section determines the constrained projection of the shared basisL given fixedS andC. We show thatL can be acquired efficiently, since this step can be relaxed to solving a semi-definite program in LLT (Boyd & Vandenberghe, 2004). To formulate the semi-definite program, note that\ntrace ( LT ∣∣∣ θ̃r+1 L ) = k∑ i=1 l (i) r+1 T∣∣∣ θ̃r+1 li\n≤ k∑ i=1 ∥∥∥∥l(i)r+1∣∣∣θ̃r+1 ∥∥∥∥ 2 ‖li‖2\n≤ √√√√ k∑ i=1 ∥∥∥∥l(i)r ∣∣∣ θ̃r+1 ∥∥∥∥2 2 √√√√ k∑ i=1 ||li||22\n= ∣∣∣∣∣∣∣∣L∣∣∣ θ̃r+1 ∣∣∣∣∣∣∣∣ F √ trace (LLT) .\nFrom the constraint set, we recognize:\nsTtjL T = ( btj − ctj )T ( A†tj )T =⇒ sTtjL TLstj = a T tjatj with atj = A † tj ( btj − ctj ) .\nAlgorithm 1 Safe Online Lifelong Policy Search 1: Inputs: Total number of rounds R, weighting factor η = 1/ √ R, regularization parameters µ1 and µ2, con-\nstraints p and q, number of latent basis vectors k. 2: S = zeros(k, |T |), L = diagk(ζ) with p ≤ ζ2 ≤ q 3: for j = 1 to R do 4: tj ← sampleTask(), and update Ij 5: Compute unconstrained solution θ̃j+1 (Sect. 4.1) 6: Fix S and C, and update L (Sect. 4.2.1) 7: Use updated L to derive S and C (Sect. 4.2.2) 8: end for 9: Output: Safety-constrained L and S\nSince spectrum ( LLT ) = spectrum ( LTL ) , we can write:\nmin X⊂S++ µ2trace(X) + 2µ2 ∣∣∣∣∣∣∣∣L∣∣∣ θ̃r+1 ∣∣∣∣∣∣∣∣ F √ trace (X)\ns.t. sTtjXstj = a T tjatj ∀tj ∈ Ir\nX ≤ pI and X ≥ qI , with X = LTL .\n4.2.2. SECOND-ORDER CONE PROGRAM FOR LEARNING TASK PROJECTIONS\nHaving determined L, we can acquire S and update C by solving a second-order cone program (Boyd & Vandenberghe, 2004) of the following form:\nmin st1 ,...,stj ,ct1 ,...,ctj µ1 r∑ j=1 ‖stj‖22 + 2µ1 r∑ j=1 sTtj ∣∣∣ θ̂r stj\ns.t. AtjLstj = btj − ctj ctj > 0 ‖ctj‖22 ≤ c2max ∀tj ∈ Ir .\n5. Theoretical Guarantees This section quantifies the performance of our approach by providing formal analysis of the regret after R rounds. We show that the safe lifelong reinforcement learner exhibits sublinear regret in the total number of rounds. Formally, we prove the following theorem:\nTheorem 1 (Sublinear Regret). After R rounds and choosing ∀tj ∈ IR ηtj = η = 1√R , L ∣∣∣ θ̂1 = diagk(ζ), with diagk(·) being a diagonal matrix among the k columns of L, p ≤ ζ2 ≤ q, and S\n∣∣∣ θ̂1 = 0k×|T |, the safe lifelong rein-\nforcement learner exhibits sublinear regret of the form: R∑ j=1 ltj ( θ̂j ) − ltj (u) = O (√ R ) for any u ∈ K.\nProof Roadmap: The remainder of this section completes our proof of Theorem 1; further details are given in Appendix B. We assume linear losses for all tasks in the constrained case in accordance with Sect. 4.2. Although linear\nlosses for policy search RL are too restrictive given a single operating point, as discussed previously, we remedy this problem by generalizing to the case of piece-wise linear losses, where the linearization operating point is a resultant of the optimization problem. To bound the regret, we need to bound the dual Euclidean norm (which is the same as the Euclidean norm) of the gradient of the loss function, then prove Theorem 1 by bounding: (1) task tj’s gradient loss (Sect. 5.1), and (2) linearized losses with respect to L and S (Sect. 5.2).\n5.1. Bounding tj’s Gradient Loss\nWe start by stating essential lemmas for Theorem 1; due to space constraints, proofs for all lemmas are available in the supplementary material. Here, we bound the gradient of a loss function ltj (θ) at round r under Gaussian policies\n3. Assumption 1. We assume that the policy for a task tj is Gaussian, the action set U is bounded by umax, and the feature set is upper-bounded by Φmax.\nLemma 1. Assume task tj’s policy at round r is given by\nπ (tj) αtj\n( u (k, tj) m |x(k, tj)m )∣∣∣ θ̂r = N ( αTtj ∣∣∣ θ̂r Φ ( x (k, tj) m ) ,σtj ) ,\nfor states x(k, tj)m ∈ Xtj and actions u (k, tj) m ∈ Utj . For ltj ( αtj ) = − 1ntj ntj∑ k=1 Mtj−1∑ m=0 log [ π (tj) αtj ( u(k, tj)m |x(k, tj)m )] , the\ngradient ∇αtj ltj ( αtj )∣∣∣ θ̂r satisfies ∣∣∣∣∣∣∣∣∇αtj ltj(αtj)∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2 ≤\nMtj σ2tj\n( umax + max\ntk∈Ir−1\n{∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax)}Φmax ) Φmax\nfor all trajectories and all tasks, with umax = max k,m {∣∣∣u(k, tj)m ∣∣∣} and Φmax=max k,m {∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣ 2 } .\n5.2. Bounding Linearized Losses\nAs discussed previously, we linearize the loss of task tr around the constraint solution of the previous round θ̂r. To acquire the regret bounds in Theorem 1, the next step is to\nbound the dual norm, ∥∥∥∥f̂tr ∣∣∣\nθ̂r ∥∥∥∥? 2 = ∥∥∥∥f̂tr ∣∣∣ θ̂r ∥∥∥∥ 2 of Eq. (9). It\ncan be easily seen∥∥∥∥f̂tr ∣∣∣ θ̂r ∥∥∥∥ 2 ≤ ∣∣∣∣ltr (θ) ∣∣∣ θ̂r ∣∣∣∣︸ ︷︷ ︸ constant + ∥∥∥∥∇θltr (θ) ∣∣∣ θ̂r ∥∥∥∥ 2︸ ︷︷ ︸\nLemma 2\n(11)\n+ ∥∥∥∥∇θltr (θ)∣∣∣ θ̂r ∥∥∥∥ 2 × ∥∥∥θ̂r∥∥∥\n2︸ ︷︷ ︸ Lemma 3 .\n3Please note that derivations for other forms of log-concave policy distributions could be derived in similar manner. In this work, we focus on Gaussian policies since they cover a broad spectrum of real-world applications.\nSince ∣∣∣∣ltr (θ) ∣∣∣\nθ̂r ∣∣∣∣ can be bounded by δltr (see Sect. 2), the next step is to bound ∥∥∥∥∇θltr (θ) ∣∣∣ θ̂r ∥∥∥∥ 2 , and ‖θ̂r‖2. Lemma 2. The norm of the gradient of the loss function evaluated at θ̂r satisfies∣∣∣∣∣∣∣∣∇θltr (θ) ∣∣∣\nθ̂r ∣∣∣∣∣∣∣∣2 2 ≤ ∣∣∣∣∣∣∇αtr ltr (θ) ∣∣∣ θ̂r ∣∣∣∣∣∣2 2 ( q × d\n( 2d/p2 max\ntk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||22 + c2max) } + 1 )) .\nTo finalize the bound of ∥∥∥∥f̂tr ∣∣∣\nθ̂r ∥∥∥∥ 2 as needed for deriving\nthe regret, we must derive an upper-bound for ‖θ̂r‖2: Lemma 3. The L2 norm of the constraint solution at round r − 1, ‖θ̂r‖22 is bounded by\n‖θ̂r‖22 ≤ q × d [ 1 + |Ir−1| 1\np2\nmax tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||2 + cmax)2 }] ,\nwhere |Ir−1| is the number of unique tasks observed so far.\nGiven the previous two lemmas, we can prove the bound for ∥∥∥∥f̂tr ∣∣∣\nθ̂r ∥∥∥∥ 2 :\nLemma 4. The L2 norm of the linearizing term of ltr (θ) around θ̂r, ∥∥∥∥f̂tr ∣∣∣\nθ̂r ∥∥∥∥ 2\n, is bounded by∥∥∥∥f̂tr ∣∣∣ θ̂r ∥∥∥∥ 2 ≤ ∥∥∥∥∇θltr(θ)∣∣∣ θ̂r ∥∥∥∥ 2 ( 1+‖θ̂r‖2 ) + ∣∣∣∣ltr(θ)∣∣∣ θ̂r ∣∣∣∣ (12) ≤ γ1(r) (1 + γ2(r)) + δltr ,\nwhere δltr is the constant upper-bound on ∣∣∣∣ltr (θ)∣∣∣\nθ̂r ∣∣∣∣, and γ1(r) = 1\nntjσ 2 tj\n[( umax\n+ max tk∈Ir−1\n{∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax)}Φmax ) Φmax ]\n× ( d\np\n√ 2q √ max\ntk∈Ir−1\n{ ‖A†tk‖ 2 2 (‖btk‖22 + c2max) } + √ qd ) γ2(r) ≤ √ q × d\n+ √ |Ir−1| √ 1+ 1\np2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22(||btk ||2 + cmax)2 } .\n5.3. Completing the Proof of Sublinear Regret\nGiven the lemmas in the previous section, we now can derive the sublinear regret bound given in Theorem 1. Using\nresults developed by Abbasi-Yadkori et al. (2013), it is easy to see that ∇θΩ0 ( θ̃j ) −∇θΩ0 ( θ̃j+1 ) = ηtj f̂tj ∣∣∣ θ̂j . From the convexity of the regularizer, we obtain:\nΩ0 ( θ̂j ) ≥ Ω0 ( θ̂j+1 ) + 〈 ∇θΩ0 ( θ̂j+1 ) , θ̂j − θ̂j+1 〉 + 1\n2 ∣∣∣∣∣∣θ̂j − θ̂j+1∣∣∣∣∣∣2 2 .\nWe have: ∥∥∥θ̂j − θ̂j+1∥∥∥ 2 ≤ ηtj ∥∥∥∥f̂tj ∣∣∣ θ̂j ∥∥∥∥ 2 . Therefore, for any u ∈ K r∑ j=1 ηtj ( ltj ( θ̂j ) − ltj (u) ) ≤ r∑ j=1 ηtj ∥∥∥∥f̂tj ∣∣∣ θ̂j ∥∥∥∥2 2\n+ Ω0(u)−Ω0(θ̂1) . Assuming that ∀tj ηtj = η, we can derive: r∑ j=1 ( ltj ( θ̂j ) − ltj (u) ) ≤ η r∑ j=1 ∥∥∥∥f̂tj ∣∣∣ θ̂j ∥∥∥∥2 2\n+ 1/η ( Ω0(u)−Ω0(θ̂1) ) .\nThe following lemma finalizes the proof of Theorem 1:\nLemma 5. AfterR rounds with ∀tj ηtj = η = 1√R , for any u ∈ K we have that ∑R j=1 ltj (θ̂j)− ltj (u) ≤ O (√ R ) .\nProof. From Eq. (12), it follows that∥∥∥∥f̂tj ∣∣∣ θ̂r ∥∥∥∥2 2 ≤ γ3(R) + 4γ21(R)γ22(R)\n≤ γ3(R) + 8 d\np2 γ21(R)qd\n( 1 + |IR−1|\n× max tk∈IR−1\n{ ‖A†tk‖2 (‖btk‖2 + cmax) 2 })\nwith γ3(R) = 4γ21(R) + 2maxtj∈IR−1 δ 2 tj . Since |IR−1| ≤ |T |, we have that ∥∥∥∥f̂tj ∣∣∣\nθ̂r ∥∥∥∥2 2 ≤ γ5(R)|T | with\nγ5 = 8d/p 2qγ21(R) max\ntk∈IR−1\n{ ‖A†tk‖ 2 2 (‖btk‖2 + cmax) 2 } .\nGiven that Ω0(u) ≤ qd + γ5(R)|T |, with γ5(R) being a constant, we have: r∑ j=1 ( ltj ( θ̂j ) −ltj(u) ) ≤ η r∑ j=1 γ5(R)|T |\n+ 1\nη\n( qd+ γ5(R)|T | −Ω0(θ̂1) ) .\nInitializing L and S: We initialize L ∣∣∣ θ̂1 = diagk(ζ), with\np ≤ ζ2 ≤ q and S ∣∣∣ θ̂1 = 0k×|T | to ensure the invertibility\nof L and that the constraints are met. This leads to r∑ j=1 ( ltj ( θ̂j ) −ltj(u) ) ≤ η r∑ j=1 γ5(R)|T |\n+ 1/η (qd+ γ5(R)|T | − µ2kζ) . Choosing ∀tj ηtj = η = 1/ √ R, we acquire sublinear regret, finalizing the statement of Theorem 1: r∑ j=1 ( ltj ( θ̂j ) −ltj(u) ) ≤ 1/√Rγ5(R)|T |R\n+ √ R (qd+ γ5(R)|T | − µ2kζ)\n≤ √ R ( γ5(R)|T |+ qdγ5(R)|T | − µ2kζ ) ≤ O (√ R ) .\n6. Experimental Validation To validate the empirical performance of our method, we applied our safe online PG algorithm to learn multiple consecutive control tasks on three dynamical systems (Figure 1). To generate multiple tasks, we varied the parameterization of each system, yielding a set of control tasks from each domain with varying dynamics. The optimal control policies for these systems vary widely with only minor changes in the system parameters, providing substantial diversity among the tasks within a single domain.\nSimple Mass Spring Damper: The simple mass (SM) system is characterized by three parameters: the spring constant k in N/m, the damping constant d in Ns/m and the mass m in kg. The system’s state is given by the position x and ẋ of the mass, which varies according to a linear force F . The goal is to train a policy for controlling the mass in a specific state gref = 〈xref, ẋref〉. Cart Pole: The cart-pole (CP) has been used extensively as a benchmark for evaluating RL methods (Busoniu et al., 2010). CP dynamics are characterized by the cart’s mass mc in kg, the pole’s mass mp in kg, the pole’s length in meters, and a damping parameter d in Ns/m. The state is given by the cart’s position x and velocity ẋ, as well as the pole’s angle θ and angular velocity θ̇. The goal is to train a policy that controls the pole in an upright position.\n6.1. Experimental Protocol\nWe generated 10 tasks for each domain by varying the system parameters to ensure a variety of tasks with diverse op-\ntimal policies, including those with highly chaotic dynamics that are difficult to control. We ran each experiment for a total of R rounds, varying from 150 for the simple mass to 10, 000 for the quadrotor to train L and S, as well as for updating the PG-ELLA and PG models. At each round j, the learner observed a task tj through 50 trajectories of 150 steps and updated L and stj . The dimensionality k of the latent space was chosen independently for each domain via cross-validation over 3 tasks, and the learning step size for each task domain was determined by a line search after gathering 10 trajectories of length 150. We used eNAC, a standard PG algorithm, as the base learner.\nWe compared our approach to both standard PG (i.e., eNAC) and PG-ELLA (Bou Ammar et al., 2014), examining both the constrained and unconstrained variants of our algorithm. We also varied the number of iterations in our alternating optimization from 10 to 100 to evaluate the effect of these inner iterations on the performance, as shown in Figures 2 and 3. For the two MTL algorithms (our approach and PG-ELLA), the policy parameters for each task tj were initialized using the learned basis (i.e., αtj = Lstj ). We configured PG-ELLA as described by Bou Ammar et al. (2014), ensuring a fair comparison. For the standard PG learner, we provided additional trajectories in order to ensure a fair comparison, as described below.\nFor the experiments with policy constraints, we generated a set of constraints (At, bt) for each task that restricted the policy parameters to pre-specified “safe” regions, as shown in Figures 2(c) and 2(d). We also tested different values for the constraints on L, varying p and q between 0.1 to 10; our approach showed robustness against this broad range, yielding similar average cost performance.\n6.2. Results on Benchmark Systems\nFigure 2 reports our results on the benchmark simple mass and cart-pole systems. Figures 2(a) and 2(b) depicts the performance of the learned policy in a lifelong learning setting over consecutive unconstrained tasks, averaged over all 10 systems over 100 different initial conditions. These results demonstrate that our approach is capable of outperforming both standard PG (which was provided with 50 additional trajectories each iteration to ensure a more fair comparison) and PG-ELLA, both in terms of initial performance and learning speed. These figures also show that the performance of our method increases as it is given more alternating iterations per-round for fitting L and S.\nWe evaluated the ability of these methods to respect safety constraints, as shown in Figures 2(c) and 2(d). The thicker black lines in each figure depict the allowable “safe” region of the policy space. To enable online learning per-task, the same task tj was observed on each round and the shared basis L and coefficients stj were updated using alternating optimization. We then plotted the change in the policy pa-\nrameter vectors per iterations (i.e., αtj = Lstj ) for each method, demonstrating that our approach abides by the safety constraints, while standard PG and PG-ELLA can violate them (since they only solve an unconstrained optimization problem). In addition, these figures show that increasing the number of alternating iterations in our method causes it to take a more direct path to the optimal solution.\n6.3. Application to Quadrotor Control\nWe also applied our approach to the more challenging domain of quadrotor control. The dynamics of the quadrotor system (Figure 1) are influenced by inertial constants around e1,B , e2,B , and e3,B , thrust factors influencing how the rotor’s speed affects the overall variation of the system’s state, and the lengths of the rods supporting the rotors. Although the overall state of the system can be described by a 12-dimensional vector, we focus on stability and so consider only six of these state-variables. The quadrotor system has a high-dimensional action space, where the goal is to control the four rotational velocities {wi}4i=1 of the rotors to stabilize the system. To ensure realistic dynamics, we used the simulated model described by (Bouabdallah, 2007; Voos & Bou Ammar, 2010), which has been verified and used in the control of physical quadrotors.\nWe generated 10 different quadrotor systems by varying the inertia around the x, y and z-axes. We used a linear quadratic regulator, as described by Bouabdallah (2007), to initialize the policies in both the learning and testing phases. We followed a similar experimental procedure to that discussed above to update the models.\nFigure 3 shows the performance of the unconstrained solution as compared to standard PG and PG-ELLA. Again, our approach clearly outperforms standard PG and PG-ELLA in both the initial performance and learning speed. We also evaluated constrained tasks in a similar manner, again showing that our approach is capable of respecting constraints. Since the policy space is higher dimensional, we cannot visualize it as well as the benchmark systems, and so instead report the number of iterations it takes our approach\nto project the policy into the safe region. Figure 4 shows that our approach requires only one observation of the task to acquire safe policies, which is substantially lower then standard PG or PG-ELLA (e.g., which require 545 and 510 observations, respectively, in the quadrotor scenario).\n7. Conclusion We described the first lifelong PG learner that provides sublinear regret O( √ R) with R total rounds. In addition, our approach supports safety constraints on the learned policy, which are essential for robust learning in real applications. Our framework formalizes lifelong learning as online MTL with limited resources, and enables safe transfer by sharing policy parameters through a latent knowledge base that is efficiently updated over time.\nReferences Yasin Abbasi-Yadkori, Peter Bartlett, Varun Kanade,\nYevgeny Seldin, & Csaba Szepesvári. Online learning in Markov decision processes with adversarially chosen transition probability distributions. Advances in Neural Information Processing Systems 26, 2013.\nHaitham Bou Ammar, Karl Tuyls, Matthew E. Taylor, Kurt Driessen, & Gerhard Weiss. Reinforcement learning transfer via sparse coding. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2012.\nHaitham Bou Ammar, Eric Eaton, Paul Ruvolo, & Matthew Taylor. Online multi-task learning for policy gradient methods. In Proceedings of the 31st International Conference on Machine Learning (ICML), 2014.\nSamir Bouabdallah. Design and Control of Quadrotors with Application to Autonomous Flying. PhD Thesis, École polytechnique fédérale de Lausanne, 2007.\nStephen Boyd & Lieven Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, 2004.\nLucian Busoniu, Robert Babuska, Bart De Schutter, & Damien Ernst. Reinforcement Learning and Dynamic Programming Using Function Approximators. CRC Press, Boca Raton, FL, 2010.\nEliseo Ferrante, Alessandro Lazaric, & Marcello Restelli. Transfer of task representation in reinforcement learning using policy-based proto-value functions. In Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2008.\nMohammad Gheshlaghi Azar, Alessandro Lazaric, & Emma Brunskill. Sequential transfer in multi-armed bandit with finite set of models. Advances in Neural Information Processing Systems 26, 2013.\nRoger A. Horn & Roy Mathias. Cauchy-Schwarz inequalities associated with positive semidefinite matrices. Linear Algebra and its Applications 142:63–82, 1990.\nJens Kober & Jan Peters. Policy search for motor primitives in robotics. Machine Learning, 84(1–2):171–203, 2011.\nAbhishek Kumar & Hal Daumé III. Learning task grouping and overlap in multi-task learning. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.\nAlessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In M. Wiering & M. van Otterlo, editors, Reinforcement Learning: State of the Art. Springer, 2011.\nJan Peters & Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural Networks, 2008a.\nJan Peters & Stefan Schaal. Natural Actor-Critic. Neurocomputing 71, 2008b.\nPaul Ruvolo & Eric Eaton. ELLA: An Efficient Lifelong Learning Algorithm. In Proceedings of the 30th International Conference on Machine Learning (ICML), 2013.\nRichard S. Sutton & Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, 1998.\nRichard S. Sutton, David Mcallester, Satinder Singh, & Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems 12, 2000.\nMatthew E. Taylor & Peter Stone. Transfer learning for reinforcement learning domains: a survey. Journal of Machine Learning Research, 10:1633–1685, 2009.\nSebastian Thrun & Joseph O’Sullivan. Discovering structure in multiple learning tasks: the TC algorithm. In Proceedings of the 13th International Conference on Machine Learning (ICML), 1996a.\nSebastian Thrun & Joseph O’Sullivan. Learning more from less data: experiments in lifelong learning. Seminar Digest, 1996b.\nHolger Voos & Haitham Bou Ammar. Nonlinear tracking and landing controller for quadrotor aerial robots. In Proceedings of the IEEE Multi-Conference on Systems and Control, 2010.\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8(3–4):229–256, 1992.\nAaron Wilson, Alan Fern, Soumya Ray, & Prasad Tadepalli. Multi-task reinforcement learning: a hierarchical Bayesian approach. In Proceedings of the 24th International Conference on Machine Learning (ICML), 2007.\nJian Zhang, Zoubin Ghahramani, & Yiming Yang. Flexible latent variable models for multi-task learning. Machine Learning, 73(3):221–242, 2008.\nA. Update Equations Derivation In this appendix, we derive the update equations forL and S in the special case of Gaussian policies. Please note that these derivations can be easily extended to other policy forms in higher dimensional action spaces.\nFor a task tj , the policy π (tj) αtj\n( u (k,tj) m |x(k,tj)m ) is given by:\nπ (tj) αtj ( u(k,tj)m |x(k,tj)m ) =\n1√ 2πσ2tj exp ( − 1 2σ2tj ( u(k,tj)m − ( Lstj )T Φ ( x(k,tj)m ))2) .\nTherefore, the safe lifelong reinforcement learning optimization objective can be written as:\ner(L,S) = r∑ j=1 ηtj 2σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 ( u(k,tj)m − ( Lstj )T Φ ( x(k,tj)m ))2 + µ1||S||2F + µ2||L||2F . (13)\nTo arrive at the update equations, we need to derive Eq. (13) with respect to each L and S.\nA.1. Update Equations for L\nStarting with the derivative of er(L,S) with respect to the shared repository L, we can write:\n∇Ler(L,S) = ∇L  r∑ j=1 ηtj 2σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 ( u(k,tj)m − ( Lstj )T Φ ( x(k,tj)m ))2 + µ1||S||2F + µ2||L||2F  = −\nr∑ j=1  ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 ( u(k,tj)m − ( Lstj )T Φ ( x(k,tj)m )) Φ ( x(k,tj)m ) sTtj + 2µ2L . To acquire the minimum, we set the above to zero:\nr∑ j=1  ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 ( u(k,tj)m − ( Lstj )T Φ ( x(k,tj)m )) Φ ( x(k,tj)m ) sTtj + 2µ2L = 0 r∑ j=1  ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 sTtjL TΦ ( x(k,tj)m ) Φ ( x(k,tj)m ) sTtj + 2µ2L = r∑ j=1 ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 u(k,tj)m Φ ( x(k,tj)m ) sTtj .\nNoting that sTtjL TΦ ( x (k,tj) m ) ∈ R, we can write:\nr∑ j=1  ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 Φ ( x(k,tj)m ) sTtjΦ T ( x(k,tj)m ) Lstj + 2µ2L = r∑ j=1 ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 u(k,tj)m Φ ( x(k,tj)m ) sTtj .\n(14) To solve Eq. (14), we introduce the standard vec(·) operator leading to:\nvec  r∑ j=1  ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 Φ ( x(k,tj)m ) sTtjΦ T ( x(k,tj)m ) Lstj + 2µ2L \n= vec  r∑ j=1 ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 u(k,tj)m Φ ( x(k,tj)m ) sTtj  r∑ j=1 ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 vec ( Φ ( x(k,tj)m ) sTtj ) vec ( ΦT ( x(k,tj)m ) Lstj ) + 2µ2vec(L)\n= r∑ j=1 ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 vec ( u(k,tj)m Φ ( x(k,tj)m ) sTtj ) .\nKnowing that for a given set of matricesA,B, andX , vec(AXB) = ( BT ⊗A ) vec(X), we can write r∑ j=1 ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 vec ( Φ ( x(k,tj)m ) sTtj )( sTtj ⊗Φ T ( x(k,tj)m )) vec(L) + 2µ2vec(L)\n= r∑ j=1 ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 vec ( u(k,tj)m Φ ( x(k,tj)m ) sTtj ) .\nBy choosing ZL = 2µ2Idk×dk + ∑r j=1\nηtj ntjσ 2 tj\n∑ntj k=1 ∑Mtj−1 m=0 vec ( Φ ( x (k,tj) m ) sTtj )( Φ ( x (k,tj) m ) ⊗ sTtj ) , and vL =∑r\nj=1 ηtj ntjσ 2 tj\n∑ntj k=1 ∑Mtj−1 m=0 vec ( u (k,tj) m Φ ( x (k,tj) m ) sTtj ) , we can update L = Z−1L vL.\nA.2. Update Equations for S\nTo derive the update equations with respect to S, similar approach to that ofL can be followed. The derivative of er(L,S) with respect to S can be computed column-wise for all tasks observed so far:\n∇stj er(L,S) = ∇stj  r∑ j=1 ηtj 2σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 ( u(k,tj)m − ( Lstj )T Φ ( x(k,tj)m ))2 + µ1||S||2F + µ2||L||2F  = −\n∑ tk=tj  ηtj σ2tjntj ntj∑ k=1 Mtj−1∑ m=0 ( u(k,tj)m − ( Lstj )T Φ ( x(k,tj)m )) LTΦ ( x(k,tj)m )+ 2µ2stj . Using a similar analysis to the previous section, choosing\nZstj = 2µ1Ik×k + ∑ tk=tj ηtj ntjσ 2 tj ntj∑ k=1 Mtj−1∑ m=0 LTΦ ( x(k,tj)m ) ΦT ( x(k,tj)m ) L ,\nvstj = ∑ tk=tj ηtj ntjσ 2 tj ntj∑ k=1 Mtj−1∑ m=0 u(k,tj)m L TΦ ( x(k,tj)m ) ,\nwe can update stj = Z −1 stj vstj .\nB. Proofs of Theoretical Guarantees In this appendix, we prove the claims and lemmas from the main paper, leading to sublinear regret (Theorem 1).\nLemma 1. Assume the policy for a task tj at a round r to be given by π (tj) αtj\n( u (k, tj) m |x(k, tj)m ) ∣∣∣ θ̂r =\nN ( αTtj ∣∣∣ θ̂r Φ ( x (k, tj) m ) ,σtj ) , for x(k, tj)m ∈ Xtj and u (k, tj) m ∈ Utj with Xtj and Utj representing the state and action\nspaces, respectively. The gradient∇αtj ltj ( αtj ) ∣∣∣ θ̂r , for ltj ( αtj ) = −1/ntj ∑ntj k=1 ∑Mtj−1 m=0 log [ π (tj) αtj ( u (k, tj) m |x(k, tj)m )] satisfies ∣∣∣∣∣∣∣∣∇αtj ltj (αtj) ∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2 ≤ Mtj σ2tj [( umax + max tk∈Ir−1\n{∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax)}Φmax)Φmax] , with umax = maxk,m {∣∣∣u(k, tj)m ∣∣∣} and Φmax = maxk,m {∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣ 2 } for all trajectories and all tasks.\nProof. The proof of the above lemma will be provided as a collection of claims. We start with the following: Claim: Given π(tj)αtj ( u (k) m |x(k)m ) ∣∣∣ θ̂r = N ( αTtj ∣∣∣ θ̂r Φ ( x (k, tj) m ) ,σtj ) , for x(k, tj)m ∈ Xtj and u (k, tj) m ∈ Utj , and\nltj ( αtj ) = −1/ntj ∑ntj k=1 ∑Mtj−1 m=0 log [ π (tj) αtj ( u (k, tj) m |x(k, tj)m )] , ∣∣∣∣∣∣∣∣∇αtj ltj (αtj) ∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2\nsatisfies∣∣∣∣∣∣∣∣∇αtj ltj (αtj) ∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2 ≤ Mtj σ2tj [( umax + ∣∣∣∣∣∣∣∣αtj ∣∣∣ θ̂r ∣∣∣∣∣∣∣∣ 2 Φmax ) Φmax ] . (15)\nProof: Since π(tj)αtj ( u (k, tj) m |x(k, tj)m ) ∣∣∣ θ̂r = N ( αTtj ∣∣∣ θ̂r Φ ( x (k, tj) m ) ,σtj ) , we can write\nlog [ π (tj) αtj ( u(k, tj)m |x(k, tj)m ) ∣∣∣ θ̂r ] = − log [√ 2πσ2tj ] − 1\n2σ2tj\n( u(k, tj)m −αTtj ∣∣∣ θ̂r Φ ( x(k, tj)m ))2 .\nTherefore:\n∇αtj ltj ( αtj ) ∣∣∣ θ̂r = − 1 ntj ntj∑ k=1 Mtj−1∑ m=0 1 σ2tj ( u(k, tj)m −αTtj ∣∣∣ θ̂r Φ ( x(k, tj)m )) Φ ( x(k, tj)m ) ∣∣∣∣∣∣∣∣∇αtj ltj (αtj) ∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2 ≤ Mtj σ2tj [ max k,m {∣∣∣∣u(k, tj)m −αTtj ∣∣∣ θ̂r Φ ( x(k, tj)m )∣∣∣∣× ∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣ 2 }]\n≤ Mtj σ2tj [ max k,m {∣∣∣u(k, tj)m ∣∣∣× ∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣ 2 } +max\nk,m {∣∣∣∣αTtj ∣∣∣ θ̂r Φ ( x(k, tj)m )∣∣∣∣× ∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣ 2 }]\n≤ Mtj σ2tj [ max k,m {∣∣∣u(k, tj)m ∣∣∣}max k,m {∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣ 2 } +max\nk,m {∣∣∣∣〈αtj ∣∣∣ θ̂r ,Φ ( x(k, tj)m )〉∣∣∣∣}maxk,m {∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣2} ] .\nDenoting maxk,m {∣∣∣u(k, tj)m ∣∣∣} = umax and maxk,m {∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣\n2\n} = Φmax for all trajectories and all tasks, we can\nwrite ∣∣∣∣∣∣∣∣∇αtj ltj (αtj) ∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2 ≤ Mtj σ2tj [( umax +max k,m {∣∣∣∣〈αtj ∣∣∣ θ̂r ,Φ ( x(k, tj)m )〉∣∣∣∣})Φmax] . Using the Cauchy-Shwarz inequality (Horn & Mathias, 1990), we can upper bound maxk,m {∣∣∣∣〈αtj ∣∣∣ θ̂r ,Φ ( x (k, tj) m )〉∣∣∣∣} as\nmax k,m {∣∣∣∣〈αtj ∣∣∣ θ̂r ,Φ ( x(k, tj)m )〉∣∣∣∣} ≤ maxk,m {∣∣∣∣∣∣∣∣αtj ∣∣∣ θ̂r ∣∣∣∣∣∣∣∣ 2 ∣∣∣∣∣∣Φ(x(k, tj)m )∣∣∣∣∣∣ 2 } ≤ max k,m {∣∣∣∣∣∣∣∣αtj ∣∣∣ θ̂r ∣∣∣∣∣∣∣∣ 2 } Φmax\n≤ ∣∣∣∣∣∣∣∣αtj ∣∣∣\nθ̂r ∣∣∣∣∣∣∣∣ 2 Φmax .\nFinalizing the statement of the claim, the overall bound on the norm of the gradient of ltj (αtj ) can be written as∣∣∣∣∣∣∣∣∇αtj ltj (αtj) ∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2 ≤ Mtj σ2tj [( umax + ∣∣∣∣∣∣∣∣αtj ∣∣∣ θ̂r ∣∣∣∣∣∣∣∣ 2 Φmax ) Φmax ] . (16)\nClaim: The norm of the gradient of the loss function satisfies:∣∣∣∣∣∣∣∣∇αtj ltj (αtj) ∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2 ≤ Mtj σ2tj [( umax + max tk∈Ir−1 { ‖A+tk‖2 (‖btk‖2 + cmax) } Φmax ) Φmax ] .\nProof: As mentioned previously, we consider the linearization of the loss function ltj around the constraint solution of the previous round, θ̂r. Since θ̂r satisfiesAtkαtk = btk − ctk ,∀tk ∈ Ir−1. Hence, we can write\nAtkαtk + ctk = btk ∀tk ∈ Ir−1\n=⇒ αtk = A + tk (btk − ctk) withA + tk = ( ATtkAtk )−1 ATtk being the left pseudo-inverse.\nTherefore\n||αtk ||2 ≤ ∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + ||ctk ||2)\n≤ ∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax) .\nCombining the above results with those of Eq. (16) we arrive at∣∣∣∣∣∣∇αtj ltj (αtj)∣∣∣∣∣∣2 ≤ Mtjσ2tj [( umax + max tk∈Ir−1 {∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax)}Φmax)Φmax] .\nThe previous result finalizes the statement of the lemma, bounding the gradient of the loss function in terms of the safety constraints.\nLemma 2. The norm of the gradient of the loss function evaluated at θ̂r satisfies∣∣∣∣∣∣∣∣∇θltj (θ) ∣∣∣ θ̂r ∣∣∣∣∣∣∣∣2 2 ≤ ∣∣∣∣∣∣∇αtj ltj (θ) ∣∣∣θ̂r ∣∣∣∣∣∣2 2 ( q × d ( 2d/p2 max tk∈Ir−1 {∣∣∣∣∣∣A†tj ∣∣∣∣∣∣2 2 (∣∣∣∣btj ∣∣∣∣22 + c2max)}+ 1) ) . Proof. The derivative of ltj (θ) ∣∣∣ θ̂r can be written as\n∇θltj (θ) ∣∣∣ θ̂r =  ∇αtj l T tj (θ) ∣∣∣ θ̂r  ∂α (1) tj ∂θ1 ∣∣∣ θ̂r ... ∂α (d) tj ∂θ1 ∣∣∣ θ̂r  ... ∇αtj l T tj (θ) ∣∣∣ θ̂r  ∂α (1) tj ∂θdk+k|T | ∣∣∣ θ̂r ... ∂α\n(d) tj\n∂θdk+k|T | ∣∣∣ θ̂r\n  =  ∇αtj l T tj (θ) ∣∣∣ θ̂r  θdk+1 ∣∣∣ θ̂r 0 ... 0  ... ∇αtj l T tj (θ) ∣∣∣ θ̂r  0 ... θ(d+1)k+1 ∣∣∣ θ̂r  ... ∇αtj l T tj (θ) ∣∣∣ θ̂r  θd(k+1)+1 ∣∣∣ θ̂r ...\nθdk ∣∣∣ θ̂r\n  =⇒ ∥∥∥∥∇θltj (θ)∣∣∣ θ̂r ∥∥∥∥2 2 ≤ ∥∥∥∥∇αtj ltj (αtj )∣∣∣θ̂r ∥∥∥∥2 2 [ d ∥∥∥∥stj ∣∣∣ θ̂r ∥∥∥∥2 2 + ∥∥∥∥L∣∣∣ θ̂r ∥∥∥∥2 F ] .\nThe results of Lemma 1 bound ∥∥∥∥∇αtj ltj (θ)∣∣∣θ̂r ∥∥∥∥2 2 .\nNow, we target to bound each of ∣∣∣∣∣∣stj ∣∣∣\nθ̂r\n∣∣∣∣∣∣2 2 and ∣∣∣∣∣∣L∣∣∣\nθ̂r\n∣∣∣∣∣∣2 F .\nBounding ∥∥∥∥stj ∣∣∣\nθ̂r ∥∥∥∥2 2 and ‖L ∣∣∣ θ̂r ‖2F: Considering the constraint AtjLstj + ctj = btj for a task tj , we realize that\nstj = L + ( A+tj ( btj − ctj )) . Therefore,∥∥∥∥stj ∣∣∣\nθ̂r ∥∥∥∥ 2 ≤ ∣∣∣∣∣∣L+ (A+tj (btj − ctj))∣∣∣∣∣∣ 2 ≤ ∣∣∣∣L+∣∣∣∣ 2 ∣∣∣∣∣∣A+tj ∣∣∣∣∣∣ 2 (∣∣∣∣btj ∣∣∣∣2 + ∣∣∣∣ctj ∣∣∣∣2) . (17) Noting that ∣∣∣∣L+∣∣∣∣ 2 = ∣∣∣∣∣∣(LTL)−1LT∣∣∣∣∣∣ 2 ≤ ∣∣∣∣∣∣(LTL)−1∣∣∣∣∣∣ 2 ∣∣∣∣LT∣∣∣∣ 2 ≤ ∣∣∣∣∣∣(LTL)−1∣∣∣∣∣∣ 2 ∣∣∣∣LT∣∣∣∣ F\n= ∣∣∣∣∣∣(LTL)−1∣∣∣∣∣∣\n2 ||L||F .\nTo relate ||L+||2 to ||L||F, we need to bound ∣∣∣∣∣∣(LTL)−1∣∣∣∣∣∣\n2 in terms of ‖L‖F. Denoting the spectrum of LTL as spec ( LTL ) = {λ1, . . . ,λk} such that 0 < λ1 ≤ · · · ≤ λk, then spect (( LTL )−1) = {1/λ1, . . . , 1/λk} such\nthat 1/λk ≤ · · · ≤ 1/λk. Hence, ∣∣∣∣∣∣(LTL)−1∣∣∣∣∣∣\n2 = max\n{ spec (( LTL )−1)} = 1/λ1 = 1/λmin ( LTL ) . Noticing that\nspec ( LTL ) = spec ( LLT ) , we recognize ∣∣∣∣∣∣(LTL)−1∣∣∣∣∣∣ 2 = 1/λmin ( LLT\n) ≤ 1/p. Therefore∣∣∣∣L+∣∣∣∣\n2 ≤ 1 p ||L||F . (18)\nPlugging the results of Eq. (18) into Eq. (17), we arrive at∥∥∥∥stj ∣∣∣ θ̂r ∥∥∥∥ 2 ≤ 1/p ∥∥∥∥L∣∣∣ θ̂r ∥∥∥∥ F max tk∈Ir−1 {∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax)} . (19) Finally, since θ̂r satisfies the constraints, we note that ∥∥∥∥L∣∣∣ θ̂r ∥∥∥∥2 F ≤ q × d. Consequently,\n∣∣∣∣∣∣∇θltj (θ) ∣∣∣ θ̂r ∣∣∣∣∣∣2 2 ≤ ∣∣∣∣∣∣∇αtj ltj (θ) ∣∣∣θ̂r ∣∣∣∣∣∣2 2 ( q × d ( 2d p2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||22 + c2max) } + 1 )) .\nLemma 3. The L2 norm of the constraint solution at round r − 1, ‖θ̂r‖22 is bounded by ‖θ̂r‖22 ≤ q × d [ 1 + |Ir−1| 1\np2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||2 + cmax)2 }] .\nwith |Ir−1| being the cardinality of Ir−1 representing the number of different tasks observed so-far. Proof. Noting that θ̂r = [ θ1, . . . ,θdk︸ ︷︷ ︸\nL ∣∣∣ θ̂r\n,θdk+1, . . .︸ ︷︷ ︸ si1 ∣∣∣ θ̂r , . . . , . . .︸ ︷︷ ︸ sir−1 ∣∣∣ θ̂r , . . . ,θdk+kT?︸ ︷︷ ︸ 0’s: unobserved tasks\n]T , it is easy to see\n‖θ̂r‖22 ≤ ∥∥∥∥L∣∣∣\nθ̂r ∥∥∥∥2 F + |Ir−1| max tk∈Ir−1 {∥∥∥∥stk ∣∣∣ θ̂r ∥∥∥∥2 2 }\n≤ q × d+ |Ir−1| max tk∈Ir−1 [ q × d p2 ∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||2 + cmax)2 ]\n≤ q × d [ 1 + |Ir−1| 1\np2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||2 + cmax)2 }] .\nLemma 4. The L2 norm of the linearizing term of ltj (θ) around θ̂r, ∥∥∥∥f̂tj ∣∣∣\nθ̂r ∥∥∥∥ 2\n, is bounded by∥∥∥∥f̂tj ∣∣∣ θ̂r ∥∥∥∥ 2 ≤ ∥∥∥∥∇θltj (θ)∣∣∣ θ̂r ∥∥∥∥ 2 ( 1 + ‖θ̂r‖2 ) + ∣∣∣∣ltj (θ)∣∣∣ θ̂r ∣∣∣∣ ≤ γ1(r) (1 + γ2(r)) + δltj , with δltj being the constant upper-bound on ∣∣∣∣ltj (θ)∣∣∣ θ̂r\n∣∣∣∣, and γ1(r) = 1\nntjσ 2 tj\n[( umax + max\ntk∈Ir−1 {∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax)}Φmax)Φmax] × ( d/p √ 2q √ max\ntk∈Ir−1\n{ ‖A†tk‖ 2 2 (‖btk‖22 + c2max) } + √ qd ) .\nγ2(r) ≤ √ q × d+ √ |Ir−1| √[ 1 + 1\np2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||2 + cmax)2 }] .\nProof. We have previously shown that ∣∣∣∣∣∣f̂tj ∣∣∣\nθ̂r\n∣∣∣∣∣∣ 2 ≤ ∣∣∣∣∣∣∇θltj (θ) ∣∣∣\nθ̂r\n∣∣∣∣∣∣ 2 + ∣∣∣ltj (θ̂r) ∣∣∣+ ∣∣∣∣∣∣∇θltj (θ)∣∣∣\nθ̂r\n∣∣∣∣∣∣ 2 × ∣∣∣∣∣∣θ̂r∣∣∣∣∣∣ 2 . Using\nthe previously derived lemmas we can upper-bound ∣∣∣∣∣∣f̂tj ∣∣∣\nθ̂r\n∣∣∣∣∣∣ 2\nas follows∣∣∣∣∣∣∇θltj (θ) ∣∣∣ θ̂r ∣∣∣∣∣∣2 2 ≤ ∣∣∣∣∣∣∇αtj ltj (θ) ∣∣∣θ̂r ∣∣∣∣∣∣2 2 ( q × d ( 2d p2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||22 + c2max) } + 1 )) ∣∣∣∣∣∣∇θltj (θ) ∣∣∣\nθ̂r ∣∣∣∣∣∣ 2 ≤ ∣∣∣∣∣∣∇αtj ltj (θ) ∣∣∣θ̂r ∣∣∣∣∣∣ 2 ( d/p √ 2q √ max tk∈Ir−1 { ‖A†tk‖ 2 2 (‖btk‖22 + c2max) } + √ qd ) ≤ 1 ntjσ 2 tj [( umax + max tk∈Ir−1\n{∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax)}Φmax)Φmax] × ( d/p √ 2q √ max\ntk∈Ir−1\n{ ‖A†tk‖ 2 2 (‖btk‖22 + c2max) } + √ qd ) .\nFurther, ∣∣∣∣∣∣θ̂r∣∣∣∣∣∣2 2 ≤ q × d+ |Ir−1| max tk∈Ir−1 [ 1 + 1 p2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||2 + cmax)2 }]\n=⇒ ∣∣∣∣∣∣θ̂r∣∣∣∣∣∣ 2 ≤ √ q × d+ √ |Ir−1| √[ 1 + 1 p2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||2 + cmax)2 }] .\nTherefore ∣∣∣∣∣∣∣∣f̂tj ∣∣∣ θ̂r ∣∣∣∣∣∣∣∣ ≤ ∣∣∣∣∣∣∣∣∇θltj (θ)∣∣∣θ̂r ∣∣∣∣∣∣∣∣ 2 ( 1 + ∣∣∣∣∣∣θ̂r∣∣∣∣∣∣ 2 ) + ∣∣∣∣ltj (θ)∣∣∣ θ̂r ∣∣∣∣ (20) ≤ γ1(r) (1 + γ2(r)) + δltj ,\nwith δltj being the constant upper-bound on ∣∣∣∣ltj (θ)∣∣∣\nθ̂r ∣∣∣∣, and γ1(r) = 1\nntjσ 2 tj\n[( umax + max\ntk∈Ir−1 {∣∣∣∣A+tk ∣∣∣∣2 (||btk ||2 + cmax)}Φmax)Φmax] × ( d/p √ 2q √ max\ntk∈Ir−1\n{ ‖A†tk‖ 2 2 (‖btk‖22 + c2max) } + √ qd ) .\nγ2(r) ≤ √ q × d+ √ |Ir−1| √[ 1 + 1\np2 max tk∈Ir−1 {∣∣∣∣∣∣A†tk ∣∣∣∣∣∣22 (||btk ||2 + cmax)2 }] .\nTheorem 1 (Sublinear Regret; restated from the main paper). After R rounds and choosing ηt1 = · · · = ηtj = η = 1√R , L ∣∣∣ θ̂1 = diagk(ζ), with diagk(·) being a diagonal matrix among the k columns of L, p ≤ ζ2 ≤ q, and S ∣∣∣ θ̂1 = 0k×|T |, for any u ∈ K our algorithm exhibits a sublinear regret of the form R∑ j=1 ltj ( θ̂r ) − ltj (u) = O (√ R ) .\nProof. Given the ingredients of the previous section, next we derive the sublinear regret results which finalize the statement of the theorem. First, it is easy to see that\n∇θΩ0 ( θ̃j ) −∇θΩ0 ( θ̃j+1 ) = ηtj f̂tj ∣∣∣ θ̂j .\nFurther, from strong convexity of the regularizer we obtain:\nΩ0 ( θ̂j ) ≥ Ω0 ( θ̂j+1 ) + 〈 ∇θΩ0 ( θ̂j+1 ) , θ̂j − θ̂j+1 〉 + 1\n2 ∣∣∣∣∣∣θ̂j − θ̂j+1∣∣∣∣∣∣2 2 .\nIt can be seen that ∥∥∥θ̂j − θ̂j+1∥∥∥ 2 ≤ ηtj ∥∥∥∥f̂tj ∣∣∣ θ̂j ∥∥∥∥ 2 .\nFinally, for any u ∈ K, we have: r∑ j=1 ηtj ( ltj ( θ̂j ) − ltj (u) ) ≤ r∑ j=1 [ ηtj (∥∥∥∥f̂tj ∣∣∣ θ̂j ∥∥∥∥ 2 )2] + Ω0(u)−Ω0(θ̂1) .\nAssuming ηt1 = · · · = ηtj = η, we can derive r∑ j=1 ( ltj ( θ̂j ) − ltj (u) ) ≤ η r∑ j=1 (∥∥∥∥f̂tj ∣∣∣ θ̂j ∥∥∥∥ 2 )2 + 1/η ( Ω0(u)−Ω0(θ̂1) ) .\nThe following lemma finalizes the statement of the theorem:\nLemma 5. After T rounds and for ηt1 = · · · = ηtj = η = 1√R , our algorithm exhibits, for any u ∈ K, a sublinear regret of the form\nR∑ j=1 ltj (θ̂j)− ltj (u) ≤ O (√ R ) .\nProof. It is then easy to see∥∥∥∥f̂tj ∣∣∣ θ̂r ∥∥∥∥2 2 ≤ γ3(R) + 4γ21(R)γ22(R) with γ3(R) = 4γ21(R) + 2 max tj∈IR−1 δ2tj\n≤ γ3(R) + 8 d\np2 γ21(R)qd+ 8\nd\np2 γ21(R)qd |IR−1| max tk∈IR−1\n{ ‖A†tk‖2 (‖btk‖2 + cmax) 2 } .\nSince |IR−1| ≤ |T | with |T | being the total number of tasks available, then we can write∥∥∥∥f̂tj ∣∣∣ θ̂r ∥∥∥∥2 2 ≤ γ5(R)|T | ,\nwith γ5 = 8d/p2qγ21(R)maxtk∈IR−1 { ‖A†tk‖ 2 2 (‖btk‖2 + cmax) 2 }\n. Further, it is easy to see that Ω0(u) ≤ qd+ γ5(R)|T | with γ5(R) being a constant, which leads to\nr∑ j=1 ( ltj ( θ̂j ) − ltj (u) ) ≤ η r∑ j=1 γ5(R)|T |+ 1/η ( qd+ γ5(R)|T | −Ω0(θ̂1) ) .\nInitializing L and S: We initialize L ∣∣∣ θ̂1 = diagk(ζ), with p ≤ ζ2 ≤ q and S ∣∣∣ θ̂1 = 0k×|T | ensures the invertability of L and that the constraints are met. This leads us to r∑ j=1 ( ltj ( θ̂j ) − ltj (u) ) ≤ η r∑ j=1 γ5(R)|T |+ 1/η (qd+ γ5(R)|T | − µ2kζ) . Choosing ηt1 = · · · = ηtj = η = 1/ √ R, we acquire sublinear regret, finalizing the statement of the theorem:\nr∑ j=1 ( ltj ( θ̂j ) − ltj (u) ) ≤ 1/√Rγ5(R)|T |R+ √ R (qd+ γ5(R)|T | − µ2kζ)\n≤ √ R (γ5(R)|T |+ qdγ5(R)|T | − µ2kζ) ≤ O (√ R ) ,\nwith γ5(R) being a constant."
    } ],
    "references" : [ {
      "title" : "Online learning in Markov decision processes with adversarially chosen transition probability distributions",
      "author" : [ "Yasin Abbasi-Yadkori", "Peter Bartlett", "Varun Kanade", "Yevgeny Seldin", "Csaba Szepesvári" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning transfer via sparse coding",
      "author" : [ "Haitham Bou Ammar", "Karl Tuyls", "Matthew E. Taylor", "Kurt Driessen", "Gerhard Weiss" ],
      "venue" : "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),",
      "citeRegEx" : "Ammar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2012
    }, {
      "title" : "Online multi-task learning for policy gradient methods",
      "author" : [ "Haitham Bou Ammar", "Eric Eaton", "Paul Ruvolo", "Matthew Taylor" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ammar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2014
    }, {
      "title" : "Design and Control of Quadrotors with Application to Autonomous Flying",
      "author" : [ "Samir Bouabdallah" ],
      "venue" : "PhD Thesis, École polytechnique fédérale de Lausanne,",
      "citeRegEx" : "Bouabdallah.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bouabdallah.",
      "year" : 2007
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Reinforcement Learning and Dynamic Programming Using Function Approximators",
      "author" : [ "Lucian Busoniu", "Robert Babuska", "Bart De Schutter", "Damien Ernst" ],
      "venue" : null,
      "citeRegEx" : "Busoniu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Busoniu et al\\.",
      "year" : 2010
    }, {
      "title" : "Sequential transfer in multi-armed bandit with finite set of models",
      "author" : [ "Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Azar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Azar et al\\.",
      "year" : 2013
    }, {
      "title" : "Cauchy-Schwarz inequalities associated with positive semidefinite matrices",
      "author" : [ "Roger A. Horn", "Roy Mathias" ],
      "venue" : "Linear Algebra and its Applications 142:63–82,",
      "citeRegEx" : "Horn and Mathias.,? \\Q1990\\E",
      "shortCiteRegEx" : "Horn and Mathias.",
      "year" : 1990
    }, {
      "title" : "Policy search for motor primitives in robotics",
      "author" : [ "Jens Kober", "Jan Peters" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kober and Peters.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kober and Peters.",
      "year" : 2011
    }, {
      "title" : "Learning task grouping and overlap in multi-task learning",
      "author" : [ "Abhishek Kumar", "Hal Daumé III" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Kumar and III.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kumar and III.",
      "year" : 2012
    }, {
      "title" : "Transfer in reinforcement learning: a framework and a survey",
      "author" : [ "Alessandro Lazaric" ],
      "venue" : "Reinforcement Learning: State of the Art. Springer,",
      "citeRegEx" : "Lazaric.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lazaric.",
      "year" : 2011
    }, {
      "title" : "Reinforcement learning of motor skills with policy gradients",
      "author" : [ "Jan Peters", "Stefan Schaal" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Peters and Schaal.,? \\Q2008\\E",
      "shortCiteRegEx" : "Peters and Schaal.",
      "year" : 2008
    }, {
      "title" : "ELLA: An Efficient Lifelong Learning Algorithm",
      "author" : [ "Paul Ruvolo", "Eric Eaton" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ruvolo and Eaton.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ruvolo and Eaton.",
      "year" : 2013
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S. Sutton", "David Mcallester", "Satinder Singh", "Yishay Mansour" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sutton et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2000
    }, {
      "title" : "Transfer learning for reinforcement learning domains: a survey",
      "author" : [ "Matthew E. Taylor", "Peter Stone" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Taylor and Stone.,? \\Q2009\\E",
      "shortCiteRegEx" : "Taylor and Stone.",
      "year" : 2009
    }, {
      "title" : "Discovering structure in multiple learning tasks: the TC algorithm",
      "author" : [ "Sebastian Thrun", "Joseph O’Sullivan" ],
      "venue" : "In Proceedings of the 13th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Thrun and O.Sullivan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Thrun and O.Sullivan.",
      "year" : 1996
    }, {
      "title" : "Learning more from less data: experiments in lifelong learning",
      "author" : [ "Sebastian Thrun", "Joseph O’Sullivan" ],
      "venue" : "Seminar Digest,",
      "citeRegEx" : "Thrun and O.Sullivan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Thrun and O.Sullivan.",
      "year" : 1996
    }, {
      "title" : "Nonlinear tracking and landing controller for quadrotor aerial robots",
      "author" : [ "Holger Voos", "Haitham Bou Ammar" ],
      "venue" : "In Proceedings of the IEEE Multi-Conference on Systems and Control,",
      "citeRegEx" : "Voos and Ammar.,? \\Q2010\\E",
      "shortCiteRegEx" : "Voos and Ammar.",
      "year" : 2010
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Multi-task reinforcement learning: a hierarchical Bayesian approach",
      "author" : [ "Aaron Wilson", "Alan Fern", "Soumya Ray", "Prasad Tadepalli" ],
      "venue" : "In Proceedings of the 24th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Wilson et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2007
    }, {
      "title" : "Flexible latent variable models for multi-task learning",
      "author" : [ "Jian Zhang", "Zoubin Ghahramani", "Yiming Yang" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Introduction Reinforcement learning (RL) (Busoniu et al., 2010; Sutton & Barto, 1998) often requires substantial experience before achieving acceptable performance on individual control problems.",
      "startOffset" : 41,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "When data is in limited supply, transfer learning can significantly improve model performance on new tasks by reusing previous learned knowledge during training (Taylor & Stone, 2009; Gheshlaghi Azar et al., 2013; Lazaric, 2011; Ferrante et al., 2008; Bou Ammar et al., 2012).",
      "startOffset" : 161,
      "endOffset" : 275
    }, {
      "referenceID" : 20,
      "context" : "ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008).",
      "startOffset" : 60,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : "ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008).",
      "startOffset" : 60,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : ", 2008; Bou Ammar et al., 2012). Multitask learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultaneProceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008). In the lifelong learning setting (Thrun & O’Sullivan, 1996a;b), which can be framed as an online MTL problem, agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Recently, based on the work of Ruvolo & Eaton (2013) on supervised lifelong learning, Bou Ammar et al.",
      "startOffset" : 12,
      "endOffset" : 658
    }, {
      "referenceID" : 1,
      "context" : ", 2008; Bou Ammar et al., 2012). Multitask learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultaneProceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008). In the lifelong learning setting (Thrun & O’Sullivan, 1996a;b), which can be framed as an online MTL problem, agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Recently, based on the work of Ruvolo & Eaton (2013) on supervised lifelong learning, Bou Ammar et al. (2014) developed a lifelong learner for policy gradient RL.",
      "startOffset" : 12,
      "endOffset" : 715
    }, {
      "referenceID" : 14,
      "context" : "Policy search methods have shown success in solving high-dimensional problems, such as robotic control (Kober & Peters, 2011; Peters & Schaal, 2008a; Sutton et al., 2000).",
      "startOffset" : 103,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "Given θ̃, the constrained solution is then determined by learning a projection into the constraint set via Bregman projections (see Abbasi-Yadkori et al. (2013)).",
      "startOffset" : 132,
      "endOffset" : 161
    }, {
      "referenceID" : 14,
      "context" : "Typical policy gradient methods (Kober & Peters, 2011; Sutton et al., 2000) maximize a lower bound of the expected cost ltj ( αtj ) , which can be derived by taking the logarithm and applying Jensen’s inequality:",
      "startOffset" : 32,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "We detail this process for two popular PG learners, eREINFORCE (Williams, 1992) and eNAC (Peters & Schaal, 2008b).",
      "startOffset" : 63,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "We detail this process for two popular PG learners, eREINFORCE (Williams, 1992) and eNAC (Peters & Schaal, 2008b). The derivations of the update rules below can be found in Appendix A. These updates are governed by learning rates β and λ that decay over time; β and λ can be chosen using line-search methods as discussed by Boyd & Vandenberghe (2004). In our experiments, we adopt a simple yet effective strategy, where β = cj−1 and λ = cj−1, with 0 < c < 1.",
      "startOffset" : 64,
      "endOffset" : 351
    }, {
      "referenceID" : 0,
      "context" : "Using results developed by Abbasi-Yadkori et al. (2013), it is easy to see that ∇θΩ0 ( θ̃j ) −∇θΩ0 ( θ̃j+1 ) = ηtj f̂tj ∣∣∣ θ̂j .",
      "startOffset" : 27,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Cart Pole: The cart-pole (CP) has been used extensively as a benchmark for evaluating RL methods (Busoniu et al., 2010).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : ", eNAC) and PG-ELLA (Bou Ammar et al., 2014), examining both the constrained and unconstrained variants of our algorithm. We also varied the number of iterations in our alternating optimization from 10 to 100 to evaluate the effect of these inner iterations on the performance, as shown in Figures 2 and 3. For the two MTL algorithms (our approach and PG-ELLA), the policy parameters for each task tj were initialized using the learned basis (i.e., αtj = Lstj ). We configured PG-ELLA as described by Bou Ammar et al. (2014), ensuring a fair comparison.",
      "startOffset" : 25,
      "endOffset" : 525
    }, {
      "referenceID" : 3,
      "context" : "To ensure realistic dynamics, we used the simulated model described by (Bouabdallah, 2007; Voos & Bou Ammar, 2010), which has been verified and used in the control of physical quadrotors.",
      "startOffset" : 71,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "To ensure realistic dynamics, we used the simulated model described by (Bouabdallah, 2007; Voos & Bou Ammar, 2010), which has been verified and used in the control of physical quadrotors. We generated 10 different quadrotor systems by varying the inertia around the x, y and z-axes. We used a linear quadratic regulator, as described by Bouabdallah (2007), to initialize the policies in both the learning and testing phases.",
      "startOffset" : 72,
      "endOffset" : 356
    } ],
    "year" : 2015,
    "abstractText" : "Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.",
    "creator" : "LaTeX with hyperref package"
  }
}