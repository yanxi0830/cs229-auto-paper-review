{
  "name" : "1411.3224.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence",
    "authors" : [ "Nathaniel Korda", "Prashanth L.A" ],
    "emails" : [ "nathaniel.korda@eng.ox.ac.uk", "prashla@isr.umd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n32 24\nv2 [\ncs .L\nG ]\n1 S\n1 Introduction\nMany stochastic control problems can be cast within the framework of Markov decision processes (MDP). Reinforcement learning (RL) is a popular approach to solve MDPs, when the underlying transition mechanism is unknown. An important problem in RL is to estimate the value function V π for a given stationary policy π. We focus on discounted reward MDPs with a high-dimensional state space S . In this setting, one can only hope to estimate the value function approximately and this constitutes the policy evaluation step in several approximate policy iteration methods, e.g. actor-critic algorithms [Konda and Tsitsiklis, 2003], [Bhatnagar et al., 2009].\nTemporal difference learning is a well-known policy evaluation algorithm that is both online and works with a single sample path obtained by simulating the underlying MDP. However, the classic TD(0) algorithm uses full-state representations (i.e. it stores an entry for each state s ∈ S) and hence, suffers from the curse of dimensionality. A standard trick to alleviate this problem is to approximate the value function within a linearly parameterized space of functions, i.e., V π(s) ≈ θTφ(s). Here θ is a tunable parameter and φ(s) is a column feature vector with dimension d << |S|. This approximation allows for efficient implementation of TD(0) even on large state spaces.\nThe update rule for TD(0) that incorporates linear function approximators is as follows: Starting with an arbitrary θ0,\nθn+1 = θn + γn ( r(sn, π(sn)) + βθ T nφ(sn+1)− θTnφ(sn) ) φ(sn). (1)\n∗nathaniel.korda@eng.ox.ac.uk †prashla@isr.umd.edu\nIn the above, the quantities γn are step sizes, chosen in advance ,and satisfying standard stochastic approximation conditions (see assumption (A5)). Further, r(s, a) is the reward recieved in state s on choosing action a, β ∈ (0, 1) is a discount factor, and sn is the state of the MDP at time n.\nAsymptotic convergence of TD(0). In [Tsitsiklis and Van Roy, 1997], the authors establish that θn governed by (1) converges almost surely to the fixed point, θ∗, of the projected Bellman equation given by\nΦθ∗ = ΠT π(Φθ∗). (2)\nIn the above, T π is the Bellman operator, Π is the orthogonal projection onto the linearly parameterized space within which we approximate the value function, and Φ is the feature matrix with rows φ(s)T,∀s ∈ S denoting the features corresponding to state s ∈ S (see Section 2 for more details). Let P denote the transition probability matrix with components p(s, π(s), s′) that denote the probability of transitioning from state s to s′ under the action π(s). Let r be a vector with components r(s, π(s)), and Ψ be a diagonal matrix whose diagonal forms the stationary distribution (assuming it exists) of the Markov chain for the underlying policy π. Then, θ∗ can be written as the solution to the following system of equations (see Section 6.3 of [Bertsekas, 2011])\nAθ∗ = b, where A = ΦTΨ(I − βP )Φ and b = ΦTΨr. (3)\nOur work. We derive non-asymptotic bounds on ‖θn − θ∗‖2, both in high-probability and in expectation, to quantify the rate of convergence of TD(0) with linear function approximators. To the best of our knowledge, there are no non-asymptotic bounds for TD(0) with function approximation, while there are asymptotic convergence and rate results available.\nFinite time analysis is challenging because: (1) The asymptotic limit θ∗ is the fixed point of the Bellman operator, which assumes that the underlying MDP is begun from the stationary distribution Ψ (whose influence is evident in (3)). However, the samples provided to the algorithm come from simulations of the MDP that are not begun from Ψ. This is a problem for a finite time analysis, since we do not know exactly the number of steps after which mixing of the underlying Markov chain has occurred, and the distribution of the samples that TD(0) sees has become the stationary distribution. Moreover, an assumption on this mixing rate amounts to assuming (partial) knowledge of the transition dynamics of the Markov chain underlying the policy π. (2) Standard results from stochastic approximation theory suggest that in order to obtain the optimal rate of convergence for a step size choice of γn = c/(c + n), one has to chose the constant c carefully. In the case of TD(0), we derive this condition and point out the optimal choice for c requires knowledge of the mixing rate of the underlying Markov chain for policy π. We handle the first problem by establishing that under a mixing assumption (the same as that used to establish asymptotic convergence for TD(0) in [Tsitsiklis and Van Roy, 1997]), the mixing error can be handled in the non-asymptotic bound. This assumption is broad enough to encompass a reasonable range of MDP problems. We alleviate the second problem by using iterate averaging.\nVariance reduction. One inherent problem with iterative schemes that use a single sample to update the iterate at each time step, is that of variance. This is the reason why it is necessary to carefully choose the step-size sequence: too large and the variance will force divergence; too small and the algorithm will converge, but not to the solution intended. Indeed, iterate averaging is a technique that aims to allow for larger step-sizes, while producing the same overall rate of convergence (and we show that it succeeds in eliminating the necessity to know properties of the stationary distribution of the underlying Markov chain). A more direct approach is to center the updates, and this was pioneered recently for solving batch problems via stochastic gradient descent in convex optimization [Johnson and Zhang, 2013]. We propose a variant of\nTD(0) that uses this approach, though our setting is considerably more complicated as samples arrive online and the function being optimized is not accessible directly.\nOur contributions can be summarized as follows: (1) Concentration bounds. Under assumptions similar to [Tsitsiklis and Van Roy, 1997], we provide nonasymptotic bounds, both in high probability as well as in expectation and these quantify the convergence rate of TD(0) with function approximation. (2) Centered TD. We propose a variant of TD(0) that incorporates a centering sequence and we show that it converges faster than the regular TD(0) algorithm in expectation.\nThe key insights from our finite-time analysis are: (1) Choosing γn = c0c(c+n) , with c0 < µ(1 − β)/(2(1 + β)2) and c such that µ(1 − β)c0c > 1, we obtain the optimal rate of convergence of the order O (1/ √ n), both in high-probability as well as in expectation. Here µ is the smallest eigenvalue of the matrix ΦTΨΦ (see Theorem 1). However, obtaining this rate is problematic as it implies (partial) knowledge (via µ) of the transition dynamics of the MDP. (2) With iterate averaging, one can get rid of the step-size dependency and still obtain the optimal rate of convergence, both in high probability as well as in expectation (see Theorem 2). (3) For the centered variant of TD(0), we obtain an exponential convergence rate when the underlying Markov chain mixes fast (see Theorem 3). (4) We illustrate the usefulness of our bounds on two simple synthetic experimental setups. In particular, using the step-sizes suggested by our bounds in Theorems 1–3, we are able to establish convergence empirically for TD(0), and both its averaging, as well as centered variants.\nRelated work. Concentration bounds for general stochastic approximation schemes have been derived in [Frikha and Menozzi, 2012] and later expanded to include iterate averaging in [Fathi and Frikha, 2013]. Unlike the aforementioned reference, deriving convergence rate results for TD(0), especially of non-asymptotic nature, requires sophisticated machinery as it involves Markov noise that impacts the mixing rate of the underlying Markov chain. An asymptotic normality result for TD(λ) is available in [Konda, 2002]. The authors establish there that TD(λ) converges asymptotically to a multi-variate Gaussian distribution with a covariance matrix that depends on A (see (3)). This rate result holds true for TD(λ) when combined with iterate averaging, while the non-averaged case does not result in the optimal rate of convergence. Our results are consistent with this observation, as we establish from a finite time analysis that the non-averaged TD(0) can result in optimal convergence only if the step-size constant c in γn = c/(c+n) is set carefully (as a function of a certain quantity that depends on the stationary distribution - see (A3) below), while one can get rid of this dependency and still obtain the optimal rate with iterate averaging. Least squares temporal difference methods are popular alternatives to the classic TD(λ). Asymptotic convergence rate results for LSTD(λ) and LSPE(λ), two popular least squares methods, are available in [Konda, 2002] and [Yu and Bertsekas, 2009], respectively. However, to the best of our knowledge, there are no concentration bounds that quantify the rate of convergence through a finite time analysis. A related work in this direction is the finite time bounds for LSTD in [Lazaric et al., 2010]. However, the analysis there is under a fast mixing rate assumption, while we provide non-asymptotic rate results without making any such assumption. We note here that assuming a mixing rate implies partial knowledge of the transition dynamics of the MDP under a stationary policy and in typical RL settings, this information is not available.\n2 TD(0) with Linear Approximation\nWe consider an MDP with state space S and action space A. The aim is to estimate the value function V π for any given stationary policy π : S → A, where\nV π(s) := E\n[\n∞ ∑\nt=0\nβtr(st, π(st)) | s0 = s ] . (4)\nRecall that β ∈ (0, 1) is the discount factor, st denotes the state of the MDP at time t, and r(s, a) denotes the reward obtained in state s under action a. The expectation in (4) is taken with respect to the transition dynamics P . It is well-known that V π is the solution to the fixed point relation V = T π(V ), where the Bellman operator T π is defined as\nT π(V )(s) := r(s, π(s)) + β ∑\ns′\np(s, π(s), s′)V (s′), (5)\nTD(0) [Sutton and Barto, 1998] performs a fixed point-iteration using stochastic approximation: Starting with an arbitrary V0, update\nVn(sn) := Vn−1(sn) + γn ( r(sn, π(sn)) + βVn−1(sn+1)− Vn−1(sn) ) , (6)\nwhere γn are step-sizes that satisfy standard stochastic approximation conditions. As discussed in the introduction, while TD(0) algorithm is simple and provably convergent to the fixed point of T π for any policy, it suffers from the curse of dimensionality associated with high-dimensional state spaces, and popular method to allieviate this is to parameterize the value function using a linear function approximator, i.e. for every s ∈ S , approximate V π(s) ≈ φ(s)Tθ. Here φ(s) is a d-dimensional feature vector with d << |S|, and θ is a tunable parameter. Incorporating function approximation, an update rule for TD(0) analogous to (6) is given in (1).\n3 Concentration bounds for TD(0)\n3.1 Assumptions\n(A1) Ergodicity: The Markov chain induced by the policy π is irreducible and aperiodic. Moreover, there exists a stationary distribution Ψ(= Ψπ) for this Markov chain. Let EΨ denote the expectation w.r.t. this distribution.\n(A2) Bounded rewards: |r(s, π(s))| ≤ 1, for all s ∈ S .\n(A3) Linear independence: The feature matrix Φ has full column rank. This assumption implies that the matrix ΦTΨΦ has smallest eigenvalue µ > 0.\n(A4) Bounded features: ‖φ(s)‖2 ≤ 1, for all s ∈ S .\n(A5) The step sizes satisfy ∑ n γn = ∞, and ∑ n γ 2 n < ∞.\n(A6) Combined step size and mixing assumption: There exists a non-negative function B′(·) such that: For all s ∈ S and m ≥ 0,\n∞ ∑\nτ=0\ne3(1+β) ∑τ−1 j=1 γτ ‖E(r(sτ , π(sτ ))φ(sτ ) | s0 = s)− EΨ(r(sτ , π(sτ ))φ(sτ ))‖ ≤ B′(s),\n∞ ∑\nτ=0\ne3(1+β) ∑τ−1 j=1 γτ ‖E(φ(sτ )φ(sτ+m)T | s0 = s)− EΨ(φ(sτ )φ(sτ+m)T)‖ ≤ B′(s),\n(A6’) Uniform mixing bound: (A6) holds, and there exists a constant B′ that is an uniformly bound on B(s),∀s ∈ S .\nIn comparison to the assumptions in [Tsitsiklis and Van Roy, 1997], (A1), (A3), (A5) have exact counterparts in [Tsitsiklis and Van Roy, 1997], while (A2), (A4) and (A6) are simplified versions of the corresponding boundedness assumptions in [Tsitsiklis and Van Roy, 1997].\nRemark 1. (Geometric ergodicity) A Markov chain is mixing at a geometric rate if\nP (st = s | s0)− ψ(s)| ≤ Cρt. (7) For finite state space settings, the above condition holds and hence (A6) is easily satisfied. Moreover, B′ = Θ ( 1/(1 − (1− ρ)1−ǫ )\n, for any ǫ > 0. Here ρ is an unknown quantity that relates to the second eigenvalue of the transition probability matrix. See Chapters 15 and 16 of [Meyn and Tweedie, 2009] for a detailed treatment of the subject matter.\n3.2 Non-averaged case\nTheorem 1. Under (A1)-(A6), choosing γn = c0c(c+n) , with c0 < µ(1 − β)/(2(1 + β)2) and c such that µ(1− β)c0c > 1, we have,\nE ‖θn − θ∗‖2 ≤ K1(n)√ n+ c .\nIn addition, assuming (A6’), we have. for any δ > 0,\nP\n(\n‖θn − θ∗‖2 ≤ K2(n)√ n+ c\n)\n≥ 1− δ, where\nK1(n) :=\n(\nc(‖θ0 − θ∗‖2 + C) (n + c)µ(1−β)c0c−1 + (1 + ‖θ∗‖2)c20c2 + Cc0c µ(1− β)c0c− 1\n) 1 2\n,\nK2(n) := c0cB\n′ (2 [2 + c0c] [1 + β(3− β)] ln(1/δ)) 1 2\n(µ(1− β)c0c− 1) 1 2\n+K1(n),\nC := 6dB(s0) (‖θ0‖2 + d+ ‖θ∗‖2 1− β )2 .\nProof. See Section 5.1.\nRemark 2. K1(n) and K2(n) above are O(1), i.e., they can be upper bounded by a constant. Thus, one can indeed get the optimal rate of convergence of the order O (1/ √ n) with a step-size γn = c(c+n) . However, this rate is contingent upon on the constant c in the step-size being chosen correctly. This is problematic because the right choice of c requires the knowledge of eigenvalue µ for expectation bound and and knowing µ would imply knowledge about the transition probability matrix of the underlying Markov chain. The latter information is unavailable in a typical RL setting. The next section derives bounds for the iterate averaged variant that overcomes this problematic step-size dependency.\n3.3 Iterate Averaging\nThe idea here is to employ larger step-sizes and combine it with averaging of the iterates, i.e., θ̄n+1 := (θ1 + . . . + θn)/n. This principle was introduced independently by Ruppert [Ruppert, 1991] and Polyak [Polyak and Juditsky, 1992], for accelerating stochastic approximation schemes. The following theorem establishes that iterate averaging results in the optimal rate of convergence without any step-size dependency:\nTheorem 2. Under (A1)-(A6), choosing γn = c0 ( c c+n )α , with α ∈ (1/2, 1) and c ∈ (0,∞), we have, for all n > n0 := (cµ(1− β)/(2c0(1 + β)2))−1/α,\nE ∥ ∥θ̄n − θ∗ ∥ ∥ 2 ≤ K\nIA 1 (n)\n(n+ c)α/2 .\nIn addition, assuming (A6)’, we have, for any δ > 0,\nP\n(\n∥ ∥θ̄n − θ∗ ∥ ∥ 2 ≤ K\nIA 2 (n)\n(n+ c)α/2\n)\n≥ 1− δ,\nwhere\nKIA1 (n) := ((1 + dc0c\nα(c+ n0) 1−α)e(1+β)c0c α(c+n0)1−α) + ‖θ∗‖+ C)C ′′ (n+ c)(1−α)/2\n+ n0\n[\n(1 + dc0c α(c+ n0) 1−α)e(1+β)c0c α(c+n0)1−α) + ‖θ∗‖\n]\nn1−α/2\n+ cαc0\n[\n1 + ‖θ∗‖ 1 2 2 +\n(\nC\ncαc0\n) 1 2\n]\n(\nµ(1− β)c0cα 1− α\n)− α+2α 2\n2(1−α)\n,\nKIA2 (n) := 4 √ (1 + C ′)B′ µ(1− β) C ′′′ n(1−α)/2 + KIA1 (n− n0),\nC ′ :=\n(\n3α +\n[\n4α\nµ(1− β)c0cα +\n2α\nα\n]2 ) 1 2\n, C ′′ :=\n∞ ∑\nk=1\nk−2α, and\nC ′′′ :=\n∞ ∑\nk=1\ne −\nµcα(1−β)c0 2(1−α)\n((n+c)1−α−((c+n0)1−α).\nProof. See Section 5.2.\nRemark 3. The step-size exponent α can be chosen arbitrarily close to 1, resulting in a convergence rate of the order O (1/ √ n). However although the constants KIA1 (n) and K IA 2 (n) remain O(1), there is a minor tradeoff here since a choice of α close to 1 would result in their bounding constants blowing up. One cannot choose c too large or too small for the same reasons.\n4 TD(0) with Centering (CTD)\nCTD is a control variate solution to reduce the variance of the updates of normal TD(0). This is achieved by adding a zero-mean, centering term to the TD(0) update.\nLet Xn = (sn, sn+1). Then, the TD(0) algorithm can be seen to perform the following fixed-point iteration:\nθn = θn−1 + γnfXn(θn). (8)\nwhere fXn(θ) := (r(sn, π(sn)) + βθ Tφ(sn+1) − θTφ(sn))φ(sn). The limit of (8) is the solution, θ∗, of F (θ) = 0, where F (θ) := ΠT π(Φθ) − Φθ. The idea behind the CTD algorithm is to reduce the variance of the increments fXn(θn), in order that larger step sizes can be used. This is achieved by choosing an extra iterate θ̄n, centred over the previous θn, and using an increment approximating fXn(θn)−fXn(θ̄n)+F (θ̄n). The intuitive motivation for this choice is that when the CTD algorithm arrives close to θ∗, the centering term alone ensures the updates become small, while with regular TD(0), one has to rely on a decaying step size to keep the iterates close to θ∗.\nThe approach is inspired by the SVRG algorithm, proposed in [Johnson and Zhang, 2013], for a optimising a strongly-convex function. However, the setting for TD(0) with function approximation that we have is considerably more complicated owing to the following reasons: (i) Unlike [Johnson and Zhang, 2013], we are not optimising a function that is a finite-sum of smooth functions in a batch setting. Instead, we are estimating a value function which is an infinite (discounted) sum, with the individual functions making up the sum being made available in an online fashion (i.e. as new samples are generated from the simulation of the underlying MDP for policy π). (ii) The centering term in SVRG directly uses F (·), which in our case is a limit function that is neither directly accessible nor can be simulated for any given θ. (iii) Obtaining the exponential convergence rate is also difficult owing to the fact that TD(0) does not initially see samples from the stationary distribution and there is an underlying mixing term that affects the rate. (iv) Finally, there are extra difficulties owing to the fact that we have a fixed point iteration, while the corresponding algorithm in [Johnson and Zhang, 2013] is stochastic gradient descent (SGD).\nThe CTD algorithm that we propose overcomes the difficulties mentioned above and the overall scheme of this epoch-based algorithm is presented in Figure 1. At the start of the mth epoch, a random iterate is picked from the previous epoch, i.e. θ̄(m) = θin , where in is drawn uniformly at random in {(m − 1)M, . . . ,mM}. Thereafter, for the epoch length M , CTD performs the following iteration: Set θmM = θ̄(m) and for n = mM, . . . , (m+ 1)M − 1 update\nθn+1 =Υ\n(\nθn + γ ( fXin (θn)− fXin (θ̄ (m)) + F̂ (m)(θ̄(m)) )\n)\n, (9)\nwhere F̂ (m)(θ) := M−1 ∑mM\ni=(m−1)M fXi(θ) and Υ is a projection operator that ensures that the iterates stay within a H-ball. Unlike TD(0), one can choose a large (constant) stepsize γ in (9). This choice in conjunction with iterate averaging via the choice of θ̄(m) results in an exponential convergence rate for CTD (see Remark 4 below).\n4.1 Finite time bound\nTheorem 3 below presents a finite time bound in expectation for CTD under the following mixing assumption:\n(A6”) There exists a non-negative function B′(·) such that: For all s ∈ S and m ≥ 0, ∞ ∑\nτ=0\n‖E(r(sτ , π(sτ ))φ(sτ ) | s0 = s)− EΨ(r(sτ , π(sτ ))φ(sτ ))‖ ≤ B′(s),\n∞ ∑\nτ=0\n‖E[φ(sτ )φ(sτ+m)T | s0 = s]− EΨ[φ(sτ )φ(sτ+m)T]‖ ≤ B′(s),\nThe above is weaker than assumption (A6) used earlier for regular TD(0), and this is facilitated by the fact that we project the CTD iterates onto a H-ball.\nTheorem 3. Assume (A1)-(A4) and (A6”) and let θ∗ denote the solution of F (θ) = 0. Let the epoch length M of the CTD algorithm (9) be chosen such that C1 < 1, where\nC1 := ((2µγM) −1 + γd2/2)/((1 − β)− d2γ/2))\n(i) Geometrically ergodic chains: Here the Markov chain underlying policy π mixes fast (see (7)) and we obtain1\n‖Φ(θ̄(m) − θ∗)‖2Ψ ≤ Cm1 ( ‖Φ(θ̄(0) − θ∗)‖2Ψ ) + CMC2H(5γ + 4)max{C1, ρM}(m−1), (10)\nwhere C2 = γ/(M((1 − β)− d2γ/2)). (ii) General Markov chains:\n‖Φ(θ̄(m) − θ∗)‖2Ψ ≤ Cm1 ( ‖Φ(θ̄(0) − θ∗)‖2Ψ ) + C2H(5γ + 4)\nm−1 ∑\nk=1\nC (m−2)−k 1 B kM (k−1)M (s0), (11)\nwhere BkM(k−1)M is an upper bound on the partial sums ∑kM i=(k−1)M (E(φ(si) | s0) − EΨ(φ(si))) and ∑kM\ni=(k−1)M (E(φ(si)φ(si+l) | s0)− EΨ(φ(si)φ(si+l)T)), for l = 0, 1.\nProof. See Section 5.3.\nFor finite state space settings, we obtain exponential convergence rate (10) since they are geometrically ergodic, while for MDPs that do not mix exponentially fast, the second (mixing) term in (11) will dominate and decide the rate of the CTD algorithm.\nRemark 4. Combining the result in (10) with the bound in statement (4) of Theorem 1 in [Tsitsiklis and Van Roy, 1997], we obtain\n‖Φθ̄(m) − V π‖Ψ ≤ 1\n1− β ‖ΠV π − V π‖Ψ + C m/2 1\n( ‖Φ(θ̄(0) − θ∗)‖Ψ ) + √ CC2max{C1, ρ}(m−1)/2.\nThe first term on the RHS above is an artifact of function approximation, while the second and third terms reflect the convergence rate of the CTD algorithm.\nRemark 5. As a consequence of the fact that (θ̄(m)−θ∗)T I(θ̄(m)−θ∗) ≤ 1 µ (θ̄(m)−θ∗)TΦTΨΦ(θ̄(m)−θ∗), one can obtain the following bound on the parameter error for CTD:\n‖θ̄(m) − θ∗‖2 ≤ (1/µ) ( Cm1 ( ‖Φ(θ̄(0) − θ∗)‖2Ψ ) + C2H(5γ + 4)\nm−1 ∑\nk=1\nC (m−2)−k 1 B kM (k−1)M (s0)\n)\n.\nComparing the above bound with those in Theorems 1–2, we can infer that CTD exhibits an exponential convergence rate of order O(Cm1 ), while TD(0) with/without averaging can converge only at a sublinear rate of order O(n−1/2).\n1For any v ∈ Rd, we take ‖v‖Ψ := √ v TΨv.\n5 Convergence proofs\n5.1 Non-averaged case: Proof of Theorem 1\nWe split the analysis in two, first considering the bound in high probability, and second the bound in expectation. Both bounds involve a martingale decomposition, the former of the centered error, and the latter of the iteration (1).\n5.1.1 High probability bound\nWe first state and prove a result bounding the error with high probability for general step-sizes:\nProposition 1. (High probability bound) Under (A1)-(A5) and (A6’), we have,\nP (‖θn − θ∗‖2 − E ‖θn − θ∗‖2 ≥ ǫ) ≤ e−ǫ 2(2 ∑n i=1 L 2 i )\n−1\n,\nwhere Li := γi[e−µ(1−β) ∑n k=i γk(1 + [γi + ∑n−1 k=i [γk − γk+1]eµ(1−β) ∑k+1 j=i γj ][1 + β(3− β)]B′)] 12 .\nProof. Recall that zn := θn − θ∗. First, we rewrite ‖zn‖22 − E ‖zn‖ 2 2 as a telescoping sum of martingale differences:\n‖zn‖2 − E ‖zn‖2 = n ∑\ni=1\ngi − E[gi |Fi−1 ] (12)\nwhere Di := gi −E[gi |Fi−1 ], gi := E[‖zn‖2 |θi,Fi−1 ], and Fi denotes the sigma algebra generated by the states of the underlying Markov chain, {σ1, . . . , σi}. Note that θi is Fi-measurable.\nThe above establishes that the centered error, ‖zn‖2 − E ‖zn‖2 can be written down as a sum of martingale differences with respect to the filtration {Fi}ni=0. The proof procedes by establishing that these martingale differences are Lipschitz functions of the random inovation at each time, with Lipschitz constants Li. This is the content of Lemma 4, and it makes use of assumption (A3), (A4), and (A7). This is the since the random innovation is, through assumptions (A2)-(A4), bounded, it is subgaussian, and we can invoke a standard concentration argument in Lemma 5 to finish the bound.\nLemma 4. Recall that Xn = (sn, sn+1) and fXn(θ) := (r(sn, π(sn)) + βθ Tφ(sn+1)−T φ(sn))φ(sn). Then, conditioned on Fi−1, the functions gi are Lipschitz continuous in fXi(θi−1), the random innovation at time i, with constants\nLi := γi\n[\ne−µ(1−β) ∑n k=i γk\n(\n1 +\n[\nγi +\nn−1 ∑\nk=i\n[γk − γk+1] ] [1 + β(3 − β)]B′ )]\n1 2\n.\nProof. Let Θij(θ) denote the mapping that returns the value of the iterate θj at instant j, given that θi = θ.\nΘij+1(θ)−Θij+1(θ′) = Θij(θ)−Θij(θ′)− γj [fXj(Θij(θ))− fXj(Θij(θ′))] = Θij(θ)−Θij(θ′)− γj [φ(sj)φ(sj)T − βφ(sj)φ(sj+1)T](Θij(θ)−Θij(θ′))\n= [I − γjaj ](Θij(θ)−Θij(θ′)) = ( j ∏\nk=i\n[I − γkak] ) (θ − θ′),\nwhere aj := φ(sj)φ(sj)T − βφ(sj)φ(sj+1)T. So we have\nE\n(\n∥ ∥Θin+1(θ)−Θin+1(θ′) ∥ ∥\n2 2 | Fi\n) = (θ − θ′)TE [( n ∏\nk=i\n[I − γkak] ) T ( n ∏\nk=i\n[I − γkak] ) | Fi ] (θ − θ′)\n(13)\nand from the tower property of conditional expectations, it follows that\nE\n[(\nn ∏\nk=i\n[I − γkak] ) T ( n ∏\nk=i\n[I − γkak] ) | Fi ]\n=E\n[(\nn−1 ∏\nk=i\n[I − γkak] )T ( I − 2γnE (\nan − γn 2 aTnan | Fn ))\n(\nn−1 ∏\nk=i\n[I − γkak] ) | Fi ]\n=E\n[(\nn−1 ∏\nk=i\n[I − γkak] )T ( I − 2γnEΨ (\nan − γn 2 aTnan ))\n(\nn−1 ∏\nk=i\n[I − γkak] ) | Fi ]\n+ E\n[(\nn−1 ∏\nk=i\n[I − γkak] )T 2γnE (ǫn | Fn) ( n−1 ∏\nk=i\n[I − γkak] ) | Fi ]\n(14)\nwhere\nǫ′n := E [(an − γnaTnan/2) | Fn]− EΨ [(an − γnaTnan/2)]\nTo deal with the term in the second last line of (14), let ∆ be the diagonal matrix with entries ∆i,i = Φi,1:dΦ T i,1:d. Then we find that, for any vector θ,\nθTEΨ\n[ aj+1 − γj+1 2 aTj+1aj+1 ] θ = θTΦT ( I − βΨP − γj+1 2 (∆− βP T (2I − β∆)ΨP ) ) Φθ\n= θTΦT ( I − βΨΠP − γj+1 2 (∆− βP TΠT (2I − β∆)ΨΠP ) ) Φθ (15) ≥ ‖Φθ‖2Ψ − β ‖Φθ‖Ψ − γj+1 2 θTΦT (∆− βP TΠT (2I − β∆)ΨΠP ) Φθ (16) ≥ ‖Φθ‖2Ψ − β ‖Φθ‖2Ψ − γj+1 2 ‖Φθ‖2Ψ + γj+1 2 θTΦTβP TΠT (2I − β∆)ΨΠPΦθ (17) ≥ ‖Φθ‖2Ψ − β ‖Φθ‖ 2 Ψ −\nγj+1 2 ‖Φθ‖2Ψ + γj+1 2 β(2− β) ‖ΠPΦθ‖2Ψ (18)\n≥ µ ( 1− β − γj+1 2 ) ‖θ‖22 , (19)\nwhere (15) follows from the fact that θTΦTΨ(I − Π)x = 0 since Π is a projection, (16) by an application of Cauchy-Schwarz inequality and from the non-expansiveness property of Π and P . (17) and (18) follow from the fact that, by (A4), the matrix ∆− I is positive semi-definite. The final inequality (19) follows from (A3).\nFor the term on the last line of (14) we notice, that by boundedness of the features, ∥\n∥ ∥ ∥ ∥\nn−1 ∏\nk=i\n[I − γkak] ∥ ∥ ∥ ∥\n∥\n2\n≤ n−1 ∏\nk=i\n(1 + γk(1 + β)) .\nUsing this observation and (19), we can expand (13) with (14) to obtain\nE\n(\n∥ ∥Θin+1(θ)−Θin+1(θ′) ∥ ∥\n2 2 | Fi\n)\n≤ xn(θ − θ′)E [( n−1 ∏\nk=i\n[I − γkak] )T (n−1 ∏\nk=i\n[I − γkak] ) | Fi ] (θ − θ′) (20)\n+ 2γn\n\n\nn−1 ∏\nj=i\n(1 + γj(1 + β))\n\n\n2\n‖E (ǫn | Fi)‖2 ∥ ∥θ − θ′ ∥ ∥ 2 2 (21)\n≤\n\n\n(\nn ∏\nk=i\nxk\n)\n+ 2 n ∑\nk=i\nγk\n\n\nn ∏\nj=k+1\nxj\n\n\n\n\nk−1 ∏\nj=i\n(1 + γj(1 + β))\n\n\n2\n‖E (ǫk | Fi)‖2\n\n\n∥ ∥θ − θ′ ∥ ∥ 2\n2\n≤ [ e−2 ∑n k=i γkµ(1−β− γk 2 ) + 2 n ∑\nk=i\nγke −2\n∑n j=k+1 γjµ\n(\n1−β− γj 2\n)\n+2 ∑k−1\nj=i γj(1+β)) ‖E (ǫk | Fi)‖2\n]\n∥ ∥θ − θ′ ∥ ∥ 2\n2\n(22)\n≤ e−2 ∑n k=i γkµ(1−β− γk 2 )\n[\n1 + 2 n ∑\nk=i\nγke 2 ∑k\nj=i γj(µ ( 1−β− γj 2 )\n+(1+β)) ‖E (ǫk | Fi)‖2\n]\n∥ ∥θ − θ′ ∥ ∥ 2\n2 (23)\nwhere xn := 1− 2γnµ ( 1− β − γn2 ) . Now, using discrete integration by parts,\nn ∑\nk=i\nγke 2 ∑k\nj=i γj(µ ( 1−β− γj 2 )\n+(1+β)) ‖E (ǫk | Fi)‖2\n=γi\nn ∑\nk=i\ne 2 ∑k\nj=i γj(µ ( 1−β− γj 2 )\n+(1+β)) ‖E (ǫk | Fi)‖2\n+ n−1 ∑\nk=i\n[γk+1 − γk] n ∑\nj=k+1\ne2 ∑j l=i γl(µ(1−β− γl 2 )+(1+β)) ‖E (ǫk | Fi)‖2\n≤ [ γi + n−1 ∑\nk=i\n[γk+1 − γk] ] [1 + β(3 − β)]B′(si)\nwhere for the inequality we have used (A6) with the following bound:\n‖E(ǫk | Fi)‖2 ≤(1− γk/2) ‖E(βφ(sk)φ(sk)T | Fi)− EΨ(βφ(sk)φ(sk)T)‖2 + β ‖E(βφ(sk)φ(sk+1)T | Fi)− EΨ(βφ(sk)φ(sk+1)T)‖2 + β(2 − β) ‖E(φ(sk+1)φ(sk+1)T | Fi)− EΨ(φ(sk+1)φ(sk+1)T)‖2 .\nFrom the foregoing, we have\nE [∥ ∥Θin (θ)−Θin ( θ′ )∥ ∥\n2\n]\n(24)\n≤ [ e−µ(1−β) ∑n k=i γk ( 1 + [\nγi +\nn−1 ∑\nk=i\n[γk − γk+1] ] [1 + β(3− β)]B′(si) )]\n1 2 ∥\n∥θ − θ′ ∥ ∥\n2 (25)\nFinally, we have ∣\n∣E [‖θn − θ∗‖2 | Fi−1, θi = θ]− E [ ‖θn − θ∗‖2 | Fi−1, θi = θ′ ]∣ ∣ 2 ≤ E [∥ ∥Θin (θ)−Θin ( θ′ )∥ ∥ 2 ]\n≤ [ e−µ(1−β) ∑n k=i γk ( 1 + [ γi + n−1 ∑\nk=i\n[γk − γk+1] ] [1 + β(3 − β)]B′(si) )]\n1 2\nγi ∥ ∥f − f ′ ∥ ∥\n2\n≤ Li ∥ ∥f − f ′ ∥ ∥\n2 .\nwhere f = θ − θi−1 and f ′ = θ′ − θi−1.\nIn the following lemma, we invoke a standard martingale concentration bound using the Li-Lipschitz property of the gi functions and the assumption (A3).\nLemma 5. Under the conditions of Theorem 1, we have\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) ≤ exp(−λǫ) exp ( αλ2\n2\nn ∑\ni=1\nL2i\n)\n. (26)\nProof. Note that\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) = P ( n ∑\ni=1\nDi ≥ ǫ ) ≤ exp(−λǫ)E ( exp ( λ n ∑\ni=1\nDi\n)\n)\n= exp(−λǫ)E ( exp ( λ n−1 ∑\ni=1\nDi\n)\nE\n( exp(λDn) |Fn−1 )\n)\n.\nThe first equality above follows from (12), while the inequality follows from Markov inequality. Now for any bounded random variable f , and L-Lipschitz function g we have\nE (exp(λg(f))) ≤ exp ( λ2L2/2 ) .\nNote that each fi(θi−1) is a bounded random variable by (A2) and (A4), and, conditioned on Fi−1, gi is Lipschitz in fi(θi−1) with constant Li (Lemma 4). So we obtain\nE (exp(λDn) |Fn−1 ) ≤ exp ( λ2L2n 2 ) ,\nand so\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) ≤ exp(−λǫ) exp ( αλ2\n2\nn ∑\ni=1\nL2i\n)\n.\nThe proof of Proposition 1 follows by optimizing over λ in (26).\n5.1.2 Bound in expectation\nNow we state and prove a result bounding the expected error for general step-size sequences:\nProposition 2. (Bound in expectation) Under (A1)-(A6) and assuming that γn ≤ µ(1 − β)/(2(1 + β)2) for all n, we have,\nE(‖θn+1 − θ∗‖2 ‖s0 ) ≤ [ e−µ(1−β) ∑n k=1 γk(‖z0‖2 + C) + (1 + ‖θ∗‖2) n ∑\nk=1\nγ2ke −µ(1−β)\n∑n j=k γj\n+ C\nn−1 ∑\nk=1\n(γk+1 − γk)e−µ(1−β) ∑n j=k+1 γj\n] 1 2\n, (27)\nwhere C = 2(2 + β)(d+ 4)B(s0) ( ‖θ0‖2+d+‖θ ∗‖2\n1−β\n)2 .\nProof. The update rule (1) can be re-written as follows:\nzn+1 =θn − θ∗ + γn[EΨ(fXn(θn)) + E(fXn(θn) | Fn)− EΨ(fXn(θn)) + fXn(θn)− E(fXn(θn) | Fn)], =zn + γn[EΨ(fXn(θn)− fXn(θ∗)) + E(fXn(θn)− fXn(θ∗) | Fn)− EΨ(fXn(θn)− fXn(θ∗))\n+ fXn(θn)− fXn(θ∗)− E(fXn(θn)− fXn(θ∗) | Fn) + fXn(θ∗)] (28)\nWe notice that\nEΨ(fXn(θn)− fXn(θ∗)) =EΨ ( βθTnφ(sn+1)− βθ∗Tφ(sn+1)− (θTnφ(sn)− θ∗Tφ(sn)) )\n=EΨ ((θn − θ∗)T[βφ(sn+1)− φ(sn)]φ(sn)) =EΨ (φ(sn)[βφ(sn+1)\nT − φ(sn)T](θn − θ∗)) =−Azn, (29)\nwhere A := ΦTΨ(I − βP )Φ (here P = Pπ denotes the one-step transition probability matrix of the underlying Markov chain induced under a stationary policy π). Noticing also that for any vector x\nxTΦTΨPΦx = 〈xTΦT, PΦx〉2Ψ ≤ ‖xTΦT‖Ψ‖PΦx‖Ψ = ‖xTΦT‖2Ψ,\nwhere we have used Lemma 1 in Tsitsiklis and Van Roy [1997] for the final equality. So we deduce that A− (1− β)µI is positive definite by (A3).\nFurthermore, letting\nan := βφ(sn)φ(sn+1) T − φ(sn)φ(sn)T, ǫn := E(an | Fn)− EΨ(an), and ∆Mn := an − E(an | Fn)),\nthen, using (29), we can rewrite (28) as\nzn+1 = [I − γn(A+ ǫn +∆Mn)] zn + γnǫ′n.\nwhere ǫ′n = fXn(θ ∗)−EΨ (fXn(θ∗)). Notice that ∆Mn is a martingale difference sequence with respect to the filtration F = {Fn}n≥1, and E(ǫn | s0) is mixing-error term. Taking the expectation of the square, and expanding, we have\nE\n( ‖zn+1‖22 | Fn ) = E ( ‖[I − γn(A+ ǫn +∆Mn)] zn‖22 | Fn )\n(30)\n+ 2γnE (fXn(θ ∗)T [I − γn(A+ ǫn +∆Mn)] zn | Fn) + γ2n(1 + (1 + β) ‖θ∗‖2)2\n= [zTn[I − 2γn(A+ E (ǫn | Fn)) + γ2nE ( (A+ ǫn +∆Mn) 2 | Fn ) ]zn] (31)\n+ 2γnE ( (fXn(θ ∗)− EΨ (fXn(θ∗)))T [I − γn(A+ ǫn +∆Mn)] zn | Fn ) + γ2n(1 + (1 + β) ‖θ∗‖2)2\n≤ [1− 2γn(µ(1− β)− 2γn(1 + β)2)] ‖zn‖22 (32)\n+ γnz T nE (ǫn | Fn) zn + 2γnE ( ( ǫ′n ) T [I − γn(A+ ǫn +∆Mn)] zn | Fn ) + γ2n(1 + (1 + β) ‖θ∗‖2)2\nwe have used that\n‖A+ ǫn +∆Mn‖2 = ‖βφ(sn)φ(sn+1)T − φ(sn)φ(sn)T)‖2 ≤ (1 + β).\nBefore unrolling (32) using the tower property of expectations, we bound the terms involving ǫn and ǫ′n. We notice first that, given a fixed orthonomal basis of vectors {ei}di=1. and a the associated coordinate mapping αi(·) for which φ = ∑d i=1 αi(φ)ei holds true,\nθn+1 = θn + γn (r(sn, π(sn)) + βθ T nφ(sn+1)− θTnφ(sn))φ(sn)\n= (1 + γnan)θn + γnr(sn, π(sn))φ(sn)\n=\nn ∏\nk=1\n(1 + γkan)θ0 +\nn ∑\nk=1\nγk\n\n\nn ∏\nj=k\n(1 + γjaj)\n\n r(sk, π(sk))φ(sk)\n=\n(\nn ∏\nk=1\n(1 + γkan)\n)\nθ0 + d ∑\ni=1\n\n\nn ∑\nk=1\nαi(φ(sk))r(sk, π(sk))γk\n\n\nn ∏\nj=k\n(1 + γjaj)\n\n\n\n ei.\nNow, if A is a possibly random matrix such that ‖A‖2 ≤ C , and θ is a fixed vector, then\nE (θTATE(ǫn | Fn)Aθ | s0) ≤ E ( C2θTE(ǫn | Fn)θ | s0 ) ≤ C2θTE (ǫn | s0) θ.\nFurthermore, for any norm ‖ · ‖, ‖∑di=1 xi‖2 ≤ d ∑d i=1 ‖xi‖2. From these observations we can deduce that\nE (zn+1E(ǫn | Fn)Tzn+1 | s0)\n≤ (d+ 2)\n\ne2(1+β) ∑n k=1 γk ‖θ0‖22 + ( n ∑\nk=1\nγke (1+β)\n∑n j=k γj\n)2\n+ ‖θ∗‖22\n\n ‖E(ǫn | s0)‖2\n≤ (d+ 2)\n\ne2(1+β) ∑n k=1 γk ‖θ0‖2 + e2(1+β) ∑n k=1 γk\n(\nn ∑\nk=1\nγke −(1+β)\n∑k−1 j=1 γj\n)2\n+ ‖θ∗‖22\n\n ‖E(ǫn | s0)‖2\n≤ (d+ 2)‖θ0‖ 2 2 + 1 + ‖θ∗‖22 (1− β)2 e2(1+β) ∑n k=1 γk ‖E(ǫn | s0)‖2 .\nSimilarly, using Cauchy-Schwarz,\nE\n(\nE\n(\n( ǫ′n ) T [I − γn(A+ ǫn +∆Mn)] zn | Fn ) | s0 )\n= E\n(\n( ǫ′n ) T [I − γn(A+ ǫn +∆Mn)] n ∏\nk=1\n(1 + γkan) | s0 ) θ0\n+\nd ∑\ni=1\nE\n\n\n( ǫ′n ) T [I − γn(A+ ǫn +∆Mn)] n ∑\nk=1\nγkαi(φ(sk))r(sj , π(sj))\n\n\nn ∏\nj=k\n(1 + γjaj)\n\n | s0\n\n ei\n− [I − γn(A+ ǫn +∆Mn)]E ( ( ǫ′n ) T | s0 ) θ∗\n≤ 2e(1+β) ∑n k=1 γk ∥ ∥E (( ǫ′n ) | s0 )∥ ∥ 2 ‖θ0‖2\n+ 2d ∥ ∥E (( ǫ′n ) | s0 )∥ ∥ 2 e(1+β) ∑n k=1 γk\nn ∑\nk=1\nγke −(1+β) ∑k−1 j=1 γj + 2 ∥ ∥E (( ǫ′n ) | s0 )∥ ∥ 2 ‖θ∗‖2\n≤ 2‖θ0‖2 + d+ ‖θ ∗‖2\n1− β e (1+β)\n∑n k=1 γk ∥ ∥E (( ǫ′n ) | s0 )∥ ∥\n2\nSo we can unroll (32) using the tower property of conditional expectations to obtain:\nE(‖zn+1‖22 | s0) ≤ Πnz0 + (1 + (1 + β) ‖θ∗‖2)2 n ∑\nk=1\nγ2kΠnΠ −1 k\n+ (d+ 2) ‖θ0‖22 + d+ ‖θ∗‖ 2 2 (1− β)2 n ∑\nk=1\nγkΠnΠ −1 k e\n2(1+β) ∑k\nj=1 γj ‖E(ǫk | s0)‖2\n+ 4 ‖θ0‖2 + d+ ‖θ∗‖2\n1− β\nn ∑\nk=1\nγkΠnΠ −1 k e\n(1+β) ∑k\nj=1 γj ∥ ∥E(ǫ′k | s0) ∥ ∥\n2\n≤ e− ∑n k=1 2γk(µ(1−β)−2γk(1+β) 2)z0 + (1 + (1 + β) ‖θ∗‖2)2\nn ∑\nk=1\nγke −\n∑n j=k γj(µ(1−β)−2γj (1+β) 2)\n+ (d+ 2) ‖θ0‖22 + d+ ‖θ∗‖ 2 2 (1− β)2 n ∑\nk=1\nγke −\n∑n j=k γj(µ(1−β)−2γj (1+β) 2)e2(1+β) ∑k\nj=1 γj ‖E(ǫk | s0)‖2\n+ 4 ‖θ0‖2 + d+ ‖θ∗‖2\n1− β\nn ∑\nk=1\nγke −\n∑n j=k γj(µ(1−β)−2γj (1+β) 2)e(1+β) ∑k j=1 γj ∥ ∥E(ǫ′k | s0) ∥ ∥\n2\nwhere Πn := ∏n\nk=1\n( 1− 2γn(µ(1− β)− 2γn(1 + β)2) )\n. Using that γn < µ(1− β)/(2(1 + β)2) for all n, we have that\nE(‖zn+1‖ | s0) ≤ [ E(‖zn+1‖22 | s0) ] 1 2\n≤ [ e−µ(1−β) ∑n k=1 γkz0 + (1 + (1 + β) ‖θ∗‖2)2 n ∑\nk=1\nγ2ke −µ(1−β)\n∑n j=k γj\n+ (d+ 2) ‖θ0‖22 + d+ ‖θ∗‖ 2 2 (1− β)2 n ∑\nk=1\nγke −µ(1−β) ∑n j=k γje2(1+β) ∑k j=1 γj ‖E(ǫk | s0)‖2\n+ 4 ‖θ0‖2 + d+ ‖θ∗‖2\n1− β\nn ∑\nk=1\nγke −µ(1−β) ∑n j=k γje(1+β) ∑k j=1 γj ∥ ∥E(ǫ′k | s0) ∥ ∥\n2\n] 1 2\nUsing discrete integration by parts, we have that\nn ∑\nk=1\ne−µ(1−β) ∑n j=k γje2(1+β) ∑k\nj=1 γj ‖E(ǫk | s0)‖2\n≤e−µ(1−β) ∑n k=1 γj\nn ∑\nj=1\ne2(1+β) ∑k\nj=1 γj ‖E(ǫk | s0)‖2\n+ n−1 ∑\nk=1\n(γk+1 − γk)e−µ(1−β) ∑n\nj=k+1 γj k ∑\nj=1\ne2(1+β) ∑j\nl=1 γl ‖E(ǫj | s0)‖2\n≤(1 + β)B(s0) [ e−µ(1−β) ∑n k=1 γj + n−1 ∑\nk=1\n(γk+1 − γk)e−µ(1−β) ∑n j=k+1 γj\n]\n.\nTreating the mixing term involving ǫ′k similarly, we find that:\nE( ‖zn+1‖2 | s0) ≤ [ E(‖zn+1‖22 | s0) ]\n1 2\n≤ [ e−µ(1−β) ∑n k=1 γk ‖z0‖2 + (1 + ‖θ∗‖2) n ∑\nk=1\nγ2ke −µ(1−β)\n∑n j=k γj\n+\n[\n(d+ 2) ‖θ0‖22 + d+ ‖θ∗‖22\n(1− β)2 (1 + β)B(s0) + 4 ‖θ0‖2 + d+ ‖θ∗‖2 1− β (2 + β)B(s0)\n]\n× [ e−µ(1−β) ∑n k=1 γj + n−1 ∑\nk=1\n(γk+1 − γk)e−µ(1−β) ∑n j=k+1 γj\n] ] 1 2\n≤ [ e−µ(1−β) ∑n k=1 γk ‖z0‖2 + (1 + ‖θ∗‖2) n ∑\nk=1\nγ2ke −µ(1−β)\n∑n j=k γj\n+ 2(2 + β)(d + 4)B(s0) (‖θ0‖2 + d+ ‖θ∗‖2 1− β\n)2 [\ne−µ(1−β) ∑n k=1 γj + n−1 ∑\nk=1\n(γk+1 − γk)e−µ(1−β) ∑n j=k+1 γj\n]] 1 2\n≤ [ e−µ(1−β) ∑n\nk=1 γk(‖z0‖2 + C)\n+ (1 + ‖θ∗‖2) n ∑\nk=1\nγ2ke −µ(1−β)\n∑n j=k γj + C\nn−1 ∑\nk=1\n(γk+1 − γk)e−µ(1−β) ∑n j=k+1 γj\n] 1 2\nwhere C = 2(2 + β)(d+ 4)B(s0) ( ‖θ0‖2+d+‖θ ∗‖2\n1−β\n)2 .\n5.1.3 Derivation of rates\nProof. (High probability bound in Theorem 1) Note that when γn = c0c (c+n) ,\nn ∑\ni=1\nL2i ≤ n ∑\ni=1\nc20c 2\n(c+ i)2\n[\ne −µ(1−β)c0c\nn ∑\nj=i\n1 (c+j)\n[\n1 + c0c\nc+ i +\nn−1 ∑\nk=i\n(\nc0c c+ k − c0c c+ k + 1\n)\n] [1 + β(3− β)]B′ ]\n≤ [1 + β(3− β)]B′ n ∑\ni=1\nc20c 2\n(c+ i)2\n(\nc+ i\nc+ n\n)µ(1−β)c0c [\n2 +\nn−1 ∑\nk=i\nc0c\n(c+ k)(c + k + 1)\n]\n≤ [2 + c0c] [1 + β(3− β)]B ′\n(c+ n)µ(1−β)c0c\nn ∑\ni=1\nc20c 2\n(c+ i)2−µ(1−β)c0c\nWe now find three regimes for the rate of convergence, based on the choice of c: (i) ∑n\ni=1 L 2 i = O\n( (n+ c)µ(1−β) 2c )\nwhen µ(1− β)c0c ∈ (0, 1), (ii)\n∑n i=1 L 2 i = O\n( n−1(ln n)2 ) when µ(1− β)c0c = 1, and (iii)\n∑n i=1 L 2 i ≤\n[2+c0c][1+β(3−β)]c20c 2B′\nµ(1−β)c0c−1 (n+ c)−1 when µ(1− β)c0c ∈ (1,∞).\n(We have used comparisons with integrals to bound the summations.) Setting c so that we are in regime (iii), the high probability bound from Proposition 1 gives\nP(‖θn − θ∗‖2 − E ‖θn − θ∗‖2 ≥ ǫ) ≤ exp ( −ǫ 2(n+ c)\n2Kµ,c,β\n)\n(33)\nwhere Kµ,c,β := [2+c0c][1+β(3−β)]c20c 2B′\nµ(1−β)c0c−1 (n+ c)−1.\nProof. (Expectation bound in Theorem 1) For the expectation bound, the rate derivation involves bounding each term on the RHS in (27) after choosing step-sizes γn = c0c (c+n) . Supposing that c is again chosen so that (1− β)c0µc > 1 we have:\nn ∑\nk=1\nγ2ke −µ(1−β)Γnk ≤\nn ∑\nk=1\nc20c\n(c+ k)2 e−µ(1−β)c0c\n∑n i=k 1 c+i\n≤ c 2 0c 2\n(c+ n)(µ(1− β)c0c− 1)\nn ∑\nk=1\nc20c\n(c+ k)2−µ(1−β)c0c ≤ c\n2 0c 2\nµ(1− β)c0c− 1 1 c+ n\nand similarly\nn−1 ∑\nk=1\n(γk+1 − γk)e−µ(1−β)Γ n k+1 ≤ 2c0c µ(1− β)c0c− 1 1 c+ n\nwhere we have again compared the sums with integrals. Also\ne−(1−β)µΓ n 1 ≤\n(\nc\nn+ c\n)(1−β)µc0c ≤ ( c\nn+ c\n) 1 2\n.\nSo from Proposition 2\nE ‖θn − θ∗‖2 ≤ ( c(‖θ0 − θ∗‖2 + C) (n + c)µ(1−β)c0c−1 + (1 + ‖θ∗‖2)c20c2 + Cc0c µ(1− β)c0c− 1 ) (c+ n)− 1 2 , (34)\nand the result in Theorem 1 follows.\n5.2 Iterate Averaging: Proof of Theorem 2\nIn order to prove the results in Theorem 2 we again consider the case of a general step sequence. Recall that θ̄n+1 := (θ1+ . . .+θn)/n and let zn = θ̄n+1−θ∗. We directly give a bound on the error in high probability for the averaged iterates (the bound in expectation can be obtained directly from the bound in Theorem 2):\nProposition 3. Suppose that ∀n > n0, γn ≤ µ(1− β)/(2(1 + β)2). Then, under (A1)-(A5) and (A6’), and we have, ∀ǫ ≥ 0 and ∀n > n0,\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) ≤ e−ǫ 2(2 ∑n i=1 L 2 i )\n−1\n,\nwhere Li := γi n\n(\n1 + ∑n−1\nl=i+1\n[\ne−µ(1−β) ∑l k=i γk(1 + [γi + ∑l k=i[γk − γk+1]][1 + β(3− β)]B′) ] 1 2\n)\n.\nProof. The overall schema of this proof is similar to that used for proving Proposition 1, which is used for establishing a high probability bound of non-averaged TD(0).\nWe first decompose the centered error ‖zn‖2 −E ‖zn‖2 into a sum of martingale differences as follows:\n‖zn‖2 − E ‖zn‖2 = n ∑\nk=1\nDk, (35)\nwhere Dk := gk − E[gk |Fk−1 ] and gk := ∑l k=1 E[‖zn‖2 |Fk ]. We need to prove that the functions gk are Lipschitz continuous in the random innovation at time k with the new constants Lk. Let ζ1k is the value of the averaged iterate θ̄k−1 at instant k, ζ 2 k is the value of the iterate θk at instant k, and let Θ̄kj (ζ) denote the mapping that returns the value of the averaged iterate at instant j, θ̄j , given that θ̄k−1 = ζ1 and θk = ζ2. Then, we have\nE\n∥ ∥ ∥ Θ̄kn (ζ)− Θ̄kn ( ζ ′ ) ∥ ∥ ∥\n2 ≤ E\n∥ ∥ ∥ ∥ ∥ k − 1 n ( ζ1 − ζ1′ ) + 1 n n ∑\nl=k\n(\nΘkl ( ζ2 ) −Θkl ( ζ2 ′ ))\n∥ ∥ ∥ ∥ ∥\n2\n≤ k − 1 n ∥ ∥ζ1 − ζ ′1 ∥ ∥ 2\n+ 1\nn\nn ∑\nl=k\n[\ne−µ(1−β) ∑l j=k γj (1 + [γk +\nl ∑\nj=k\n[γj − γj+1]][1 + β(3− β)]B′) ] 1 2 ∥ ∥ζ2 − ζ ′2 ∥ ∥\n2 . (36)\nIn the above, we have used (25) derived in the proof of Proposition 1 to bound ∥ ∥\n∥Θkl ( ζ2 ) −Θkl ( ζ2 ′ )∥ ∥ ∥\n2 .\nThe rest of the proof amounts to showing that gk (and also Dk) is Lk-Lipschitz in the random innovation at time k and this follows in a similar manner as the proof of Proposition 1.\n5.2.1 Derivation of rates: High Probability Bound in Theorem 2\nFor the rate of the bound in high probability, one has to again separately bound the influence of the first n0 steps, and then use the expectation bound together with 3.\nProof. (High probability bound in Theorem 2) We perform the calculation:\nn ∑\ni=n0\nL2i =\nn ∑\ni=n0\n\n γi n\n\n1 +\nn−1 ∑\nl=i+1\n[\ne−µ(1−β)Γ n i\n(\n1 +\n[\nγi +\nn−1 ∑\nk=i\n[γk − γk+1] ] [1 + β(3− β)]B′ )]\n1 2 \n\n\n\n2\n≤ n ∑\ni=1\n[\nγi n\n(\n1 + ( 1 + [ 1 +C ′ ] [1 + β(3− β)]B′ )\nn−1 ∑\nl=i+1\ne− µ(1−β) 2 Γni\n)]2\n= 1\nn2\nn ∑\ni=1\n[\nc0\n(\nc\nc+ i\n)α (\n1 + ( 1 + [ 1 + C ′ ] [1 + β(3− β)]B′ )\nn−1 ∑\nl=i+1\ne− µ(1−β) 2 Γni\n)]2\n≤ [ 1 + C ′ ] [1 + β(3 − β)]B′ ( 2c0 µ(1− β)c0n\n)2 n ∑\ni=1\n[(\nc+ i+ 2\nc+ i\n)α\n+\n1\n(c+ i)α\nn−1 ∑\nl=i\ne− µ(1−β)c0 2 cα ((c+l) 1−α−(c+i)1−α) 1−α .((c + l + 2)α − (c+ l + 1)α)\n]2\n≤ [ 1 + C ′ ] [1 + β(3 − β)]B′ (\n2\nµ(1− β)n\n)2 {\n3α +\n[\n4α\nµ(1− β)c0cα +\n2α\nα\n]2 }\nn\nwhere C ′ := ∑∞ k=1 k −2α. In the second equality we have substituted γi = (1− β)\n(\nc c+n\n)α . For the second\ninequality we have used discrete integration by parts (see page 15 in Fathi and Frikha [2013], display (2.2), for details). For the last inequality we have noted, as in page 15 in Fathi and Frikha [2013], that\nn−1 ∑\nl=i+1\ne− µ(1−β)c0 2 cα(1−β)2((c+l)1−α−(c+i)1−α)/(1−α) ((c+ l + 2)α − (c+ l + 1)α)\n≤ 1 1− αe µ(1−β)c0 2 cα(1−β)2(c+i)1−α/(1−α)\n∫ (c+n)1−α\n(c+i+1)1−α e\nµ(1−β)c0 2 cα(1−β)2l/(1−α)l 2α−1 1−α dl. (37)\nNow, by taking the derivative and setting it to zero, we find that\nl 7→ e µ(1−β)c0 2 c(1−β)l/(1−α)l 2α 1−α\nis decreasing on [4α/(µ(1 − β)c0)cα(1 − β),∞), and so we deduce that (37)≤ (c + i + 1)α/α when c+ i ≥ 4α/ (µ(1− β)c0). When c+ i < 4α/ (µ(1− β)c0) we use that the summand is bounded by 1.\n5.2.2 Derivation of rates: Expectation bound in Theorem 2\nTo bound the expected error we average the errors of the non-averaged iterates. However, this averaging this is not straightforward as the bound in Theorem 2 holds only if n > n0 (which ensures that γn is sufficiently small). Note that n0 can be easily derived from the specific form of the step sequece. Hence we analyse the initial phase (n < n0) and later phase n ≥ n0 separately as follows:\nE ‖zn‖2 ≤ ∑n0 k=1 E ‖θk − θ∗‖2 n + ∑n k=n0+1 E ‖θk − θ∗‖2 n\nThe last term on RHS above is bounded using Theorem 1, while the first term is bounded by unrolling TD(0) recursion for the first n0 steps and bounding the individual terms that arise using (A6).\nProof. (Expectation bound in Theorem 2) Let n > n0. Then, with γn = c0 (c/(c + n)) −α, from the statement of Proposition 1, we have\nE ‖θn − θ∗‖2 ≤ e −\nµcα(1−β)c0 2(1−α)\n((n+c)1−α−(c+n0)1−α)(‖θ0 − θ∗‖2 + C)\n+ (1 + ‖θ∗‖2) 1 2\n\n\nn ∑\nk=n0\nc20\n(\nc\nk + c+ 1\n)2α\n2e− µ(1−β)c0c\nα\n1−α ((n+c)1−α−(k+1+c)1−α\n\n\n1 2\n+ C 1 2\n\n\nn ∑\nk=n0\nc0\n((\nc\nk + c\n)α − (\nc\nk + c+ 1\n)α)\n2e− µ(1−β)c0c\nα\n1−α ((n+c)1−α−(k+1+c)1−α\n\n\n1 2\n≤e− µcα(1−β)c0 2(1−α) ((n+c)1−α−(c+n0)1−α)\n[\n(‖θ0 − θ∗‖2 + C)\n+ [ (1 + ‖θ∗‖2) 1 2 cαc0 + (Cc0c α) 1 2 ]\n{ ∫ n\n0 x−2α2e\nµ(1−β)c0c α\n1−α x1−αdx\n} 1 2\n]\n≤e− µcα(1−β)c0 2(1−α) ((n+c)1−α−(c+n0)1−α)\n[\n(‖θ0 − θ∗‖2 + C)\n+ [ (1 + ‖θ∗‖2) 1 2 cαc0 + (Cc0c α) 1 e ]\n\n\n\n(\nµ(1− β)c0cα 1− α\n)−2α ∫ ( µ(1−β)c0c α\n1−α\n)1/(1−α) n\n0 y−2α2ey 1−α dy\n\n\n\n1 2 ]\n≤e− µcα(1−β)c0 2(1−α) ((n+c)1−α−(c+n0)1−α)\n[\n(‖θ0 − θ∗‖2 + C)\n+ [ (1 + ‖θ∗‖2) 1 2 cαc0 + (Cc0c α) 1 2 ]\n{\n(\nµ(1− β)c0cα 1− α\n)−2α\n.\n∫\n( µ(1−β)c0c α\n1−α\n)1/(1−α) n\n0 ((1− α)y−2α − αy−(1+α))2ey1−αdy\n} 1 2 ]\n≤e− µcα(1−β)c0\n2(1−α) ((n+c)1−α−(c+n0)1−α)(‖θ0 − θ∗‖2 + C)\n+ [ (1 + ‖θ∗‖2) 1 2 cαc0 + (Cc0c α) 1 2 ]\n(\nµ(1− β)c0cα 1− α\n)−α 1+2α 2(1−α)\n(n+ c)− α 2\nSo, recalling that\nθn0 − θ∗ = ( n0 ∏\nk=1\n(1 + γkan)\n)\nθ0 +\nd ∑\ni=1\n\n\nn0 ∑\nk=1\nαi(φ(sk))r(sk, π(sk))γk\n\n\nn ∏\nj=k\n(1 + γjaj)\n\n\n\n ei − θ∗\nwe deduce that\nE ∥ ∥θ̄n − θ∗ ∥ ∥\n2\n≤ ∑n0 k=1 E ‖θk − θ∗‖2 n + ∑n k=n0+1 E ‖θk − θ∗‖2 n\n≤ n0\n[\n(1 + dc0c α(c+ n0) 1−α)e2(1+β)c0c α(c+n0)1−α) + ‖θ∗‖\n]\nn +\n∑n k=n0+1 E ‖θk − θ∗‖2 n\n≤ n0\n[\n(1 + dc0c α(c+ n0) 1−α)e2(1+β)c0c α(c+n0)1−α) + ‖θ∗‖\n]\nn\n+ (1 + dc0c\nα(c+ n0) 1−α)e2(1+β)c0c α(c+n0)1−α) + ‖θ∗‖+ C n ∞ ∑\nk=1\ne −\nµcα(1−β)c0 2(1−α) ((k+c)1−α−(c+n0)1−α)\n+ [ (1 + ‖θ∗‖2) 1 2 cαc0 + (Cc0c α) 1 2 ]\n(\nµ(1− β)c0cα 1− α\n)−α 1+2α 2(1−α)\n(n+ c)− α 2\n5.3 TD(0) with centering: Proof of Theorem 3\nProof. The proof proceeds along the following steps:\nStep 1: One-step expansion of the recursion (9).\n‖θn+1 − θ∗‖22 (38)\n=‖Υ [\n(θn − θ∗) + γ(fXin (θn)− fXin (θ̄ (m)) + E(fXin (θ̄ (m)) | Fn) ]\n‖22 ≤‖(θn − θ∗) + γ(fXin (θn)− fXin (θ̄ (m)) + E(fXin (θ̄ (m)) | Fn)‖22 ≤ ‖θn − θ∗‖22 + 2γ(θn − θ∗)T [ fXin (θn)− fXin (θ̄ (m)) + E(fXin (θ̄ (m)) | Fn) ]\n+ γ2 [ ∥ ∥\n∥ fXin (θn)− fXin (θ̄ (m)) + E(fXin (θ̄ (m)) | Fn)\n∥ ∥ ∥ 2\n2\n]\n(39)\nwhere Υ(·) denotes the orthogonal projection onto the H-ball.\nStep 2: Bounding the variance using the centering term. Let\nf̄Xin (θn) :=fXin (θn)− fXin (θ̄ (m)) + E(fXin (θ̄ (m)) | Fn) =fXin (θn)− fXin (θ ∗)− (fXin (θ̄ (m))− fXin (θ ∗)) + E(fXin (θ̄ (m))− fXin (θ\n∗) | Fn) + (E(fXin (θ ∗) | Fn)− EΨ,θn(fXin (θ ∗)))\nwhere we have used that EΨ,θn(fXin (θ ∗)) = 0. Then, letting\nen(θ) := E [ fXin (θ) ∣ ∣Fn ] − EΨ,θn [ fXin (θ) ]\nwe have,\nE\n(\n∥ ∥f̄Xin (θn) ∥ ∥ 2 2 | Fn\n)\n≤ E ( ∥\n∥fXin (θn)− fXin (θ ∗) ∥ ∥ 2 2\n+ ∥ ∥\n∥ (fXin (θ̄ (m))− fXin (θ ∗)) + E(fXin (θ̄ (m))− fXin (θ ∗) | Fn)\n∥ ∥ ∥ 2\n2 + ‖en(θ∗)‖22 | Fn\n)\n≤ E ( ∥\n∥fXin (θn)− fXin (θ ∗) ∥ ∥ 2 2\n∣ ∣ ∣Fn ) + E\n(\n∥ ∥ ∥fXin (θ̄ (m))− fXin (θ ∗) ∥ ∥ ∥ 2\n2\n∣ ∣ ∣ ∣ Fn ) + E ( ‖en(θ∗)‖22 | Fn ) ,\n(40)\nwhere we have used that for any random variable X, E(‖X − EX‖2) ≤ E(‖X‖2). Let\nǫn(θ) = E ( ∥ ∥fXin (θ)− fXin (θ ∗) ∥ ∥ 2 2\n∣ ∣ ∣Fn ) − EΨ,θn( ∥ ∥fXin (θ)− fXin (θ ∗) ∥ ∥ 2 2 ).\nThe second term in ǫn(θ) can be bounded as follows: For any θ, we have\nEΨ,θn( ∥ ∥fXin (θ)− fXin (θ ∗) ∥ ∥ 2 2 )\n=(θ − θ∗)TEΨ,θn [(βφ(sn+1)− φ(sn))φ(sn)Tφ(sn)(βφ(sn+1)− φ(sn))T] (θ − θ∗) =(θ − θ∗)T (ΦT(I − βP )ΨΦ)T ΦTΨ(I − βP )Φ(θ − θ∗) ≤(θ − θ∗)T (ΦTΨΦ)T ΦTΨΦ(θ − θ∗) ≤d2‖Φ(θ − θ∗)‖2Ψ. (41)\nIn the final inequality, we have used ‖ΦTΨΦ‖2 ≤ d2. Plugging the above in (40), we obtain\nE\n(\n∥ ∥f̄Xin (θn) ∥ ∥ 2 2\n∣ ∣ ∣Fn ) ≤d2 ( ‖Φ(θn − θ∗)‖2Ψ + ‖Φ(θ̄(m) − θ∗)‖2Ψ )\n+ ǫn(θn) + ǫn(θ̄ (m)) + E\n( ‖en(θ∗)‖22 | Fn )\nStep 3: Analysis for a particular epoch. Using the last display, we calculate,\nE ( ‖θn+1 − θ∗‖22 ∣ ∣Fn ) ≤ ‖θn − θ∗‖22 + 2γ(θn − θ∗)TE [ f̄Xin (θn) ∣ ∣Fn ]\n+ γ2 ( d2 ( ‖Φ(θn − θ∗)‖2Ψ + ‖Φ(θ̄(m) − θ∗)‖2Ψ ) + ǫn(θn) + ǫn(θ̄ (m)) + E ( ‖en(θ∗)‖22 | Fn ) )\n= ‖θn − θ∗‖22 + 2γ(θn − θ∗)TE [ fXin (θn) ∣ ∣Fn ]\n(42)\n+ γ2 ( d2 ( ‖Φ(θn − θ∗)‖2Ψ + ‖Φ(θ̄(m) − θ∗)‖2Ψ ) + ǫn(θn) + ǫn(θ̄ (m)) + E ( ‖en(θ∗)‖22 | Fn ) ) .\nThe equality above uses the fact that E [ f̄Xin (θn) ∣ ∣Fn ] = E [ fXin (θn) ∣ ∣Fn ] , since\nfXin (θ̄ (m))− E\n[\nfXin (θ̄ (m))\n∣ ∣ ∣Fn ]\nis a martingale difference w.r.t. Fn. We can rewrite (42) as\nE ( ‖θn+1 − θ∗‖22 ∣ ∣Fn ) ≤ ‖θn − θ∗‖22 + 2γ(θn − θ∗)TEΨ,θn [ fXin (θn) ] + 2γ(θn − θ∗)Ten(θn)\n+ γ2 ( d2 ( ‖Φ(θn − θ∗)‖2Ψ + ‖Φ(θ̄(m) − θ∗)‖2Ψ ) + ǫn(θn) + ǫn(θ̄ (m)) + E ( ‖en(θ∗)‖22 | Fn )) (43)\nNotice that EΨ,θn [ fXin (θ ∗) ] = 0 and hence\nEΨ,θn\n[ fXin (θn) ] = EΨ,θn((βφ(sn+1)− φ(sn))φ(sn)T)(θn − θ∗) = (ΦTΨ(I − βP )Φ)(θn − θ∗).\nUsing the above, we can simplify (43) as follows:\nE ( ‖θn+1 − θ∗‖22 ∣ ∣Fn )\n≤‖θn − θ∗‖22 + γ2d2 ( ‖Φ(θn − θ∗)‖2Ψ + ‖Φ(θ̄(m) − θ∗)‖2Ψ ) − 2γ(θn − θ∗)T [ΦTΨ(I − βP )Φ]\n+ 2γ(θn − θ∗)Ten(θn) + γ2ǫn(θn) + γ2ǫn(θ̄(m)) + E ( ‖en(θ∗)‖22 | Fn )\n≤‖θn − θ∗‖22 − 2γ((1 − β)− d2γ\n2 )‖Φ(θn − θ∗)‖2Ψ + γ2d2‖Φ(θ̄(m) − θ∗)‖2Ψ\n+ 2γ(θn − θ∗)Ten(θn) + γ2ǫn(θn) + γ2ǫn(θ̄(m)) + E ( ‖en(θ∗)‖22 | Fn )\n(44)\nUnrolling the above recursion within an epoch, i.e., from n = (m+ 1)M − 1 to n = mM , we obtain\nE ( ‖θ(m+1)M−1 − θ∗‖22 ∣ ∣Fn ) ≤ ‖θmM − θ∗‖22\n− (m+1)M−2 ∑\nn=mM\n2γ((1− β)− d 2γ\n2 )E\n( ‖Φ(θn − θ∗)‖2Ψ ∣ ∣FmM−1 ) +Mγ2d2‖Φ(θ̄(m) − θ∗)‖2Ψ\n+ E\n\n\n(m+1)M−2 ∑\nn=mM\n2γ(θn − θ∗)Ten(θn) + γ2 ( ǫn(θn) + ǫn(θ̄ (m)) + E ( ‖en(θ∗)‖22 | Fn ))\n∣ ∣ ∣ ∣ ∣ ∣ FmM−1  \n(45)\nNotice that (θ̄(m) − θ∗)TI(θ̄(m) − θ∗) ≤ 1 µ (θ̄(m) − θ∗)TΦTΨΦ(θ̄(m) − θ∗) and hence we obtain the\nfollowing by setting θmM = θ̄(m) and ignoring the non-negative LHS term:\n2γM ((1− β)− d 2γ\n2 )E\n( ‖Φ(θ̄(m+1) − θ∗)‖2Ψ ∣ ∣ ∣ FmM )\n≤ ( 1\nµ +Mγ2d2\n) ‖θ̄(m) − θ∗‖22 + γ2 (m+1)M−2 ∑\nn=mM\nE\n(\nǫn(θn) + ǫn(θ̄ (m)) + ‖en(θ∗)‖22\n∣ ∣ ∣FmM )\n+ E\n\n2γ\n(m+1)M−1 ∑\nn=mM\n(θn − θ∗)Ten(θn)\n∣ ∣ ∣ ∣ ∣ ∣ FmM  \nStep 4: Combining across epochs.\nLet C1 :=\n\n  \n1\n2µγM((1 − β)− d 2γ\n2 )\n+ γd2\n2((1 − β)− d 2γ\n2 )\n\n   . Then, we have\nE ( ‖Φ(θ̄(m) − θ∗)‖2Ψ ∣ ∣ ∣F0 )\n≤ Cm1 ‖θ̄(0) − θ∗‖22 + γC2 2\nm−1 ∑\nk=1\nC (m−2−k) 1\nkM−1 ∑\ni=(k−1)M\nE\n(\nǫi(θi) + ǫi(θ̄ ((k−1)M)) + ‖ei(θ∗)‖22\n∣ ∣ ∣F0 )\n+ C2\nm−1 ∑\nk=1\nC (m−2−k) 1\nkM−1 ∑\ni=(k−1)M\nE ((θi − θ∗)Tei(θi)| F0) ,\nwhere C2 = γ\n2M((1 − β)− d 2γ\n2 )\n.\nStep 5: Controlling the error terms. First note that\nǫ(θ) =E ( ‖(βφ(sn)φ(sn+1)T − φ(sn)φ(sn))(θ − θ∗)T‖22 | Fn )\n− EΨ,θn ( ‖(βφ(sn)φ(sn+1)T − φ(sn)φ(sn))(θ − θ∗)T‖22 )\n=(θ − θ∗)T [E(vTv | Fn)− EΨ,θn(vTv)] (θ − θ∗)\nwhere v = βφ(sn)φ(sn+1)T −φ(sn)φ(sn). Supposing that ‖(θ − θ∗)‖2 ≤ H , and using that ‖φ(sn)‖2 ≤ 1 together with the convexity of matrix inner products in the ‖·‖2-matrix norm, we have that\n‖ǫ(θ)‖2 ≤ 2H ‖E(v | Fn)− EΨ,θn(v)‖2 So by (A6), and the fact the projection step of the algorithm, we have\nkM−1 ∑\ni=(k−1)M\nE\n(\n∥ ∥ ∥ ǫi(θi) + ǫi(θ̄ ((k−1)M)) ∥ ∥ ∥ 2\n2\n∣ ∣ ∣ ∣ F0 ) ≤ 8HBkM(k−1)M (s0). (46)\nSimilarly\nkM−1 ∑\ni=(k−1)M\nE\n( ‖ei(θ∗)‖22 ∣ ∣ ∣ F0 ) , kM−1 ∑\ni=(k−1)M\nE ((θi − θ∗)Tei(θi)| F0) ≤ 2HBkM(k−1)M (s0)\n6 Numerical Experiments\nWe test the performance of TD(0), TD(0) with averaging and CTD algorithms.\nExample 1. This is a two-state toy example, which is borrowed from [Yu and Bertsekas, 2009]. The setting has the transition structure P = [0.2, 0.8; 0.3, 0.7] and the rewards given by r(1, j) = 1, r(2, j) = 2, for j = 1, 2. The features are one-dimensional, i.e., Φ = (1 2)T.\nFig. 2(a) presents the results obtained on this example. For setting the step-sizes of TD(0), we used the guideline from Theorem 1. Note that this results in convergence for TD(0), with the caveat that setting the step-size constant c requires knowledge of underlying transition structure through µ. It is evident that TD(0) with averaging gives performance on par with TD(0) and unlike TD(0), the setting of c is not constrained\nhere. Given that convergence is rapid for TD(0) on this example, we do not plot CTD in Fig 2(a) as the epoch length suggested by Theorem 3 is 100 and this is already enough for TD(0) itself to converge. CTD resulted in a normalized value difference of about 0.03 on this example, but the effect of averaging across epochs for CTD will be seen better in the next example.\nExample 2. Here the number of states are 100, the transitions are governed by a random stochastic matrix and the rewards are random and bounded between 0 and 1. Features are 3-dimensional and are picked randomly in (0, 1). The results obtained for the three algorithms are presented in Fig. 2(b). It is evident that all algorithms converge, with CTD showing the lowest variance. As in example 1, the setting parameters for TD(0) was dictated by Theorem 1, while for CTD, the step-size and epoch length were set such that the constant C1 in Theorem 3 is less than 1.\n7 Conclusions\nTD(0) with linear function approximators is a well-known policy evaluation algorithm. While asymptotic convergence rate results are available for this algorithm, there are no finite-time bounds that quantify the rate of convergence. In this paper, we derived non-asymptotic bounds, both in high-probability as well as in expectation. From our results, we observed that iterate averaging is necessary to obtain the optimal O (1/ √ n) rate of convergence. This is because, to obtain the optimal rate with the classic step-size choice ∝ 1/n, it is necessary to know properties of the stationary distribution of the underlying Markov chain. We also proposed a fast variant of TD(0) that incorporates a centering sequence and established that the rate of convergence of this algorithm is exponential. We established the practicality of our bounds by using them to guide the step-size choices in two synthetic experimental setups.\nReferences\nDimitri P Bertsekas. Approximate dynamic programming. 2011.\nS. Bhatnagar, R. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor-critic algorithms. Automatica, 45 (11):2471–2482, 2009.\nMax Fathi and Noufel Frikha. Transport-entropy inequalities and deviation estimates for stochastic approximation schemes. arXiv preprint arXiv:1301.7740, 2013.\nNoufel Frikha and Stéphane Menozzi. Concentration Bounds for Stochastic Approximations. Electron. Commun. Probab., 17:no. 47, 1–15, 2012.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems (NIPS), pages 315–323, 2013.\nVijay R Konda. Actor-Critic Algorithms. PhD thesis, Department of Electrical Engineering and Computer Science, MIT, 2002.\nVijay R Konda and John N Tsitsiklis. On Actor-Critic Algorithms. SIAM journal on Control and Optimization, 42(4):1143–1166, 2003.\nAlessandro Lazaric, Mohammad Ghavamzadeh, and Rémi Munos. Finite-sample analysis of lstd. In ICML, pages 615–622, 2010.\nSean P Meyn and Richard L Tweedie. Markov chains and stochastic stability. Cambridge university press, 2009.\nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838–855, 1992.\nDavid Ruppert. Stochastic approximation. Handbook of Sequential Analysis, pages 503–529, 1991.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. Cambridge Univ Press, 1998.\nJohn N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997.\nHuizhen Yu and Dimitri P Bertsekas. Convergence results for some temporal difference methods based on least squares. IEEE Transactions on Automatic Control, 54(7):1515–1531, 2009."
    } ],
    "references" : [ {
      "title" : "Approximate dynamic programming",
      "author" : [ "Dimitri P Bertsekas" ],
      "venue" : null,
      "citeRegEx" : "Bertsekas.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2011
    }, {
      "title" : "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes",
      "author" : [ "Max Fathi", "Noufel Frikha" ],
      "venue" : "arXiv preprint arXiv:1301.7740,",
      "citeRegEx" : "Fathi and Frikha.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fathi and Frikha.",
      "year" : 2013
    }, {
      "title" : "Concentration Bounds for Stochastic Approximations",
      "author" : [ "Noufel Frikha", "Stéphane Menozzi" ],
      "venue" : "Electron. Commun. Probab.,",
      "citeRegEx" : "Frikha and Menozzi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Frikha and Menozzi.",
      "year" : 2012
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Johnson and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2013
    }, {
      "title" : "Actor-Critic Algorithms",
      "author" : [ "Vijay R Konda" ],
      "venue" : "PhD thesis, Department of Electrical Engineering and Computer Science,",
      "citeRegEx" : "Konda.,? \\Q2002\\E",
      "shortCiteRegEx" : "Konda.",
      "year" : 2002
    }, {
      "title" : "On Actor-Critic Algorithms",
      "author" : [ "Vijay R Konda", "John N Tsitsiklis" ],
      "venue" : "SIAM journal on Control and Optimization,",
      "citeRegEx" : "Konda and Tsitsiklis.,? \\Q2003\\E",
      "shortCiteRegEx" : "Konda and Tsitsiklis.",
      "year" : 2003
    }, {
      "title" : "Finite-sample analysis of lstd",
      "author" : [ "Alessandro Lazaric", "Mohammad Ghavamzadeh", "Rémi Munos" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Lazaric et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lazaric et al\\.",
      "year" : 2010
    }, {
      "title" : "Markov chains and stochastic stability",
      "author" : [ "Sean P Meyn", "Richard L Tweedie" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Meyn and Tweedie.,? \\Q2009\\E",
      "shortCiteRegEx" : "Meyn and Tweedie.",
      "year" : 2009
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "Boris T Polyak", "Anatoli B Juditsky" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Polyak and Juditsky.,? \\Q1992\\E",
      "shortCiteRegEx" : "Polyak and Juditsky.",
      "year" : 1992
    }, {
      "title" : "Stochastic approximation. Handbook of Sequential Analysis, pages 503–529",
      "author" : [ "David Ruppert" ],
      "venue" : null,
      "citeRegEx" : "Ruppert.,? \\Q1991\\E",
      "shortCiteRegEx" : "Ruppert.",
      "year" : 1991
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "John N Tsitsiklis", "Benjamin Van Roy" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Tsitsiklis and Roy.,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy.",
      "year" : 1997
    }, {
      "title" : "Convergence results for some temporal difference methods based on least squares",
      "author" : [ "Huizhen Yu", "Dimitri P Bertsekas" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Yu and Bertsekas.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yu and Bertsekas.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "actor-critic algorithms [Konda and Tsitsiklis, 2003], [Bhatnagar et al.",
      "startOffset" : 24,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "3 of [Bertsekas, 2011]) Aθ = b, where A = ΦTΨ(I − βP )Φ and b = ΦTΨr.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "A more direct approach is to center the updates, and this was pioneered recently for solving batch problems via stochastic gradient descent in convex optimization [Johnson and Zhang, 2013].",
      "startOffset" : 163,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "Concentration bounds for general stochastic approximation schemes have been derived in [Frikha and Menozzi, 2012] and later expanded to include iterate averaging in [Fathi and Frikha, 2013].",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "Concentration bounds for general stochastic approximation schemes have been derived in [Frikha and Menozzi, 2012] and later expanded to include iterate averaging in [Fathi and Frikha, 2013].",
      "startOffset" : 165,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "An asymptotic normality result for TD(λ) is available in [Konda, 2002].",
      "startOffset" : 57,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "Asymptotic convergence rate results for LSTD(λ) and LSPE(λ), two popular least squares methods, are available in [Konda, 2002] and [Yu and Bertsekas, 2009], respectively.",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "Asymptotic convergence rate results for LSTD(λ) and LSPE(λ), two popular least squares methods, are available in [Konda, 2002] and [Yu and Bertsekas, 2009], respectively.",
      "startOffset" : 131,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "A related work in this direction is the finite time bounds for LSTD in [Lazaric et al., 2010].",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "s′ p(s, π(s), s)V (s), (5) TD(0) [Sutton and Barto, 1998] performs a fixed point-iteration using stochastic approximation: Starting with an arbitrary V0, update Vn(sn) := Vn−1(sn) + γn ( r(sn, π(sn)) + βVn−1(sn+1)− Vn−1(sn) ) , (6) where γn are step-sizes that satisfy standard stochastic approximation conditions.",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "See Chapters 15 and 16 of [Meyn and Tweedie, 2009] for a detailed treatment of the subject matter.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "This principle was introduced independently by Ruppert [Ruppert, 1991] and Polyak [Polyak and Juditsky, 1992], for accelerating stochastic approximation schemes.",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "This principle was introduced independently by Ruppert [Ruppert, 1991] and Polyak [Polyak and Juditsky, 1992], for accelerating stochastic approximation schemes.",
      "startOffset" : 82,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "The approach is inspired by the SVRG algorithm, proposed in [Johnson and Zhang, 2013], for a optimising a strongly-convex function.",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "However, the setting for TD(0) with function approximation that we have is considerably more complicated owing to the following reasons: (i) Unlike [Johnson and Zhang, 2013], we are not optimising a function that is a finite-sum of smooth functions in a batch setting.",
      "startOffset" : 148,
      "endOffset" : 173
    }, {
      "referenceID" : 3,
      "context" : "(iv) Finally, there are extra difficulties owing to the fact that we have a fixed point iteration, while the corresponding algorithm in [Johnson and Zhang, 2013] is stochastic gradient descent (SGD).",
      "startOffset" : 136,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "For the second inequality we have used discrete integration by parts (see page 15 in Fathi and Frikha [2013], display (2.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "For the second inequality we have used discrete integration by parts (see page 15 in Fathi and Frikha [2013], display (2.2), for details). For the last inequality we have noted, as in page 15 in Fathi and Frikha [2013], that n−1 ∑",
      "startOffset" : 85,
      "endOffset" : 219
    }, {
      "referenceID" : 12,
      "context" : "This is a two-state toy example, which is borrowed from [Yu and Bertsekas, 2009].",
      "startOffset" : 56,
      "endOffset" : 80
    } ],
    "year" : 2015,
    "abstractText" : "We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume (partial) knowledge of the stationary distribution for the Markov chain underlying the policy considered. We also provide bounds for the iterate averaged TD(0) variant, which gets rid of the step-size dependency while exhibiting the optimal rate of convergence. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and establish that it exhibits an exponential rate of convergence in expectation. We demonstrate the usefulness of our bounds on two synthetic experimental settings.",
    "creator" : "LaTeX with hyperref package"
  }
}