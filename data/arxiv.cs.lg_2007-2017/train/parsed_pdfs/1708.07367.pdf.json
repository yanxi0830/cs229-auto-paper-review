{
  "name" : "1708.07367.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 8.\n07 36\n7v 1\n[ m\nat h.\nST ]\n2 4\nA ug\n(\n1 γ⋆π⋆\n)\n, then γ can be estimated to\nwithin multiplicative constants with high probability. When π is uniform on d states, this matches (up to logarithmic correction) a lower bound of Ω̃ (\nd γ⋆\n)\nsteps required for precise estimation of γ⋆. Moreover, we provide the first procedure for computing a fully data-dependent interval, from a single finitelength trajectory of the chain, that traps the mixing time tmix of the chain at a prescribed confidence level. The interval does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time trelax = 1/γ⋆, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a 1/ √ n rate, where n is the length of the sample path."
    }, {
      "heading" : "1. Introduction",
      "text" : "This work tackles the challenge of constructing confidence intervals for the mixing time of reversible Markov chains based on a single sample path. Let (Xt)t=1,2,... be an irreducible, aperiodic time-homogeneous Markov chain on a finite state space [d] := {1, 2, . . . , d} with transition matrix P . Under this assumption, the chain converges to its unique stationary distribution π = (πi) d i=1 regardless of the initial state distribution q:\nlim t→∞ Prq (Xt = i) = lim t→∞\n(qP t)i = πi for each i ∈ [d].\nThe mixing time tmix of the Markov chain is the number of time steps required for the chain to be within a fixed threshold of its stationary distribution:\ntmix := min\n{ t ∈ N : sup\nq\nmax A⊂[d]\n|Prq (Xt ∈ A)− π(A)| ≤ 1/4 } .(1) Here, π(A) = ∑\ni∈A πi is the probability assigned to set A by π, and the supremum is over all possible initial distributions q. The problem studied in this work is the construction of a non-trivial confidence interval Cn = Cn(X1, X2, . . . , Xn, δ) ⊂ [0,∞], based only on the observed sample path (X1, X2, . . . , Xn) and δ ∈ (0, 1), that succeeds with probability 1− δ in trapping the value of the mixing time tmix.\nThis problem is motivated by the numerous scientific applications and machine learning tasks in which the quantity of interest is the mean π(f) = ∑ i πif(i) for\n1\nsome function f of the states of a Markov chain. This is the setting of the celebrated Markov Chain Monte Carlo (MCMC) paradigm (J. S. Liu 2001), but the problem also arises in performance prediction involving time-correlated data, as is common in reinforcement learning (Sutton and Barto 1998). Observable, or a posteriori bounds on mixing times are useful in the design and diagnostics of these methods; they yield effective approaches to assessing the estimation quality, even when a priori knowledge of the mixing time or correlation structure is unavailable.\n1.1. Main results. Consider a reversible ergodic Markov chain on d states with absolute spectral gap γ⋆ and stationary distribution minorized by π⋆. As is wellknown (see, for example, Levin, Peres, andWilmer (2009, Theorems 12.3 and 12.4)),\n(2) (trelax − 1) ln 2 ≤ tmix ≤ trelax ln 4\nπ⋆\nwhere trelax := 1/γ⋆ is the relaxation time. Hence, it suffices to estimate γ⋆ and π⋆. Our main results are summarized as follows.\n(1) In Section 3.1, we show that in some problems n = Ω((d log d)/γ⋆ + 1/π⋆) observations are necessary for any procedure to guarantee constant multiplicative accuracy in estimating γ⋆ (Theorems 3.1 and 3.2). Essentially, in some problems every state may need to be visited about log(d)/γ⋆ times, on average, before an accurate estimate of the mixing time can be provided, regardless of the actual estimation procedure used. (2) In Section 3.2, we give a point estimator γ̂⋆ for γ⋆, based an a single sample\npath, and prove in Theorem 3.4 that | γ̂⋆γ⋆ −1| < ε with high probability if the path is of length Õ(1/(π⋆γ⋆ε\n2)). (The Õ(·) notation suppresses logarithmic factors.) We also provide and analyze a point estimator for π⋆. This establishes the feasibility of estimating the mixing time in this setting, and the dependence on π⋆ and γ⋆ in the path length matches our lower bound (up to logarithmic factors) in the case where 1/π⋆ = Ω(d). We note, however, that these results give only a priori confidence intervals that depend on the unknown quantities π⋆ and γ⋆. As such, the results do not lead to a universal (chain-independent) stopping rule for stopping the chain when the relative error is below the prescribed accuracy. (3) In Section 4, we propose a procedure for a posteriori constructing confidence intervals for π⋆ and γ⋆ that depend only on the observed sample path and not on any unknown parameters. We prove that the intervals shrink at a Õ(1/ √ n) rate (Theorems 4.1 and 4.2). These confidence intervals trivially\nlead to a universal stopping rule to stop the chain when a prescribed relative error is achieved.\n1.2. Related work. There is a vast statistical literature on estimation in Markov chains. For instance, it is known that under the assumptions on (Xt)t from above, the law of large numbers guarantees that the sample mean πn(f) := 1 n ∑n t=1 f(Xt) converges almost surely to π(f) (Meyn and Tweedie 1993), while the central limit theorem tells us that as n → ∞, the distribution of the deviation √n(πn(f)−π(f)) will be normal with mean zero and asymptotic variance limn→∞ nVar (πn(f)) (Kipnis and Varadhan 1986).\nAlthough these asymptotic results help us understand the limiting behavior of the sample mean over a Markov chain, they say little about the finite-time nonasymptotic behavior, which is often needed for the prudent evaluation of a method\nor even its algorithmic design (Kontoyiannis, Lastras-Montaño, and Meyn 2006; Flegal and Jones 2011; Gyori and Paulin 2014). To address this need, numerous works have developed Chernoff-type bounds on Pr(|πn(f) − π(f)| > ǫ), thus providing valuable tools for non-asymptotic probabilistic analysis (Gillman 1998; León and Perron 2004; Kontoyiannis, Lastras-Montaño, and Meyn 2006; Kontorovich and Weiss 2014; Paulin 2015). These probability bounds are larger than the corresponding bounds for independent and identically distributed (iid) data due to the temporal dependence; intuitively, for the Markov chain to yield a fresh draw Xt′ that behaves as if it was independent of Xt, one must wait Θ(tmix) time steps. Note that the bounds generally depend on distribution-specific properties of the Markov chain (e.g., P , tmix, γ⋆), which are often unknown a priori in practice. Consequently, much effort has been put towards estimating these unknown quantities, especially in the context of MCMC diagnostics, in order to provide data-dependent assessments of estimation accuracy (e.g., Garren and R. L. Smith 2000; Jones and Hobert 2001; Flegal and Jones 2011; Atchadé 2016; Gyori and Paulin 2014). However, these approaches generally only provide asymptotic guarantees, and hence fall short of our goal of empirical bounds that are valid with any finite-length sample path. In particular, they also fail to provide universal stopping rules that allow the estimation of (for example) the mixing time with a fixed relative accuracy.\nLearning with dependent data is another main motivation to our work. Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent data (e.g., Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study.\nIt is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a “reset” device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform. Finally, when one only has a circuit-based description of the transition probabilities of a Markov chain over an exponentially-large state space, there are complexity-theoretic barriers for many MCMC diagnostic problems (Bhatnagar, Bogdanov, and Mossel 2011).\nThis paper is based on the conference paper of Hsu, Kontorovich, and Szepesvári (2015), combined with the results in the unpublished manuscript of Levin and Peres (2016)."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "2.1. Notations. We denote the set of positive integers by N, and the set of the first d positive integers {1, 2, . . . , d} by [d]. The non-negative part of a real number x is [x]+ := max{0, x}, and ⌈x⌉+ := max{0, ⌈x⌉}. We use ln(·) for natural logarithm, and log(·) for logarithm with an arbitrary constant base > 1. Boldface symbols are used for vectors and matrices (e.g., v, M), and their entries are referenced by subindexing (e.g., vi, Mi,j). For a vector v, ‖v‖ denotes its Euclidean norm; for a matrix M , ‖M‖ denotes its spectral norm. We use Diag(v) to denote the diagonal matrix whose (i, i)-th entry is vi. The probability simplex is denoted by ∆d−1 = {p ∈ [0, 1]d : ∑di=1 pi = 1}, and we regard vectors in ∆d−1 as row vectors.\n2.2. Setting. Let P ∈ (∆d−1)d ⊂ [0, 1]d×d be a d × d row-stochastic matrix for an ergodic (i.e., irreducible and aperiodic) Markov chain. This implies there is a unique stationary distribution π ∈ ∆d−1 with πi > 0 for all i ∈ [d] (Levin, Peres, and Wilmer 2009, Corollary 1.17). We also assume that P is reversible (with respect to π):\nπiPi,j = πjPj,i, i, j ∈ [d].(3) The minimum stationary probability is denoted by π⋆ := mini∈[d] πi.\nDefine the matrices\nM := Diag(π)P and L := Diag(π)−1/2M Diag(π)−1/2 .\nThe (i, j)th entry of the matrix Mi,j contains the doublet probabilities associated with P : Mi,j = πiPi,j is the probability of seeing state i followed by state j when the chain is started from its stationary distribution. The matrix M is symmetric on account of the reversibility of P , and hence it follows that L is also symmetric. (We will strongly exploit the symmetry in our results.) Further, L = Diag(π)1/2P Diag(π)−1/2, hence L and P are similar and thus their eigenvalue systems are identical. Ergodicity and reversibility imply that the eigenvalues of L are contained in the interval (−1, 1], and that 1 is an eigenvalue of L with multiplicity 1 (Levin, Peres, and Wilmer 2009, Lemmas 12.1 and 12.2). Denote and order the eigenvalues of L as\n1 = λ1 > λ2 ≥ · · · ≥ λd > −1. Let λ⋆ := max{λ2, |λd|}, and define the (absolute) spectral gap to be γ⋆ := 1− λ⋆, which is strictly positive on account of ergodicity.\nLet (Xt)t∈N be a Markov chain whose transition probabilities are governed by P . For each t ∈ N, let π(t) ∈ ∆d−1 denote the marginal distribution of Xt, so\nπ(t+1) = π(t)P , t ∈ N. Note that the initial distribution π(1) is arbitrary, and need not be the stationary distribution π.\nThe goal is to estimate π⋆ and γ⋆ from the length n sample path (Xt)t∈[n], and also to construct confidence intervals that π⋆ and γ⋆ with high probability; in particular, the construction of the intervals should be fully empirical and not depend on any unobservable quantities, including π⋆ and γ⋆ themselves. As mentioned in the introduction, it is well-known that the mixing time of the Markov chain tmix (defined in Eq. (1)) is bounded in terms of π⋆ and γ⋆, as shown in Eq. (2). Moreover, convergence rates for empirical processes on Markov chain sequences are also often\ngiven in terms of mixing coefficients that can ultimately be bounded in terms of π⋆ and γ⋆ (as we will show in the proof of our first result). Therefore, valid confidence intervals for π⋆ and γ⋆ can be used to make these rates fully observable."
    }, {
      "heading" : "3. Point estimation",
      "text" : "In this section, we present lower and upper bounds on achievable rates for estimating the spectral gap as a function of the length of the sample path n.\n3.1. Lower bounds. The purpose of this section is to show lower bounds on the number of observations necessary to achieve a fixed multiplicative (or even just additive) accuracy in estimating the spectral gap γ⋆. By Eq. (2), the multiplicative accuracy lower bound for γ⋆ gives the same lower bound for estimating the mixing time. Our first result holds even for two state Markov chains and shows that a sequence length of Ω(1/π⋆) is necessary to achieve even a constant additive accuracy in estimating γ⋆.\nTheorem 3.1. Pick any π̄ ∈ (0, 1/4). Consider any estimator γ̂⋆ that takes as input a random sample path of length n ≤ 1/(4π̄) from a Markov chain starting from any desired initial state distribution. There exists a two-state ergodic and reversible Markov chain distribution with spectral gap γ⋆ ≥ 1/2 and minimum stationary probability π⋆ ≥ π̄ such that\nPr [|γ̂⋆ − γ⋆| ≥ 1/8] ≥ 3/8. Next, considering d state chains, we show that a sequence of length Ω(d log(d)/γ⋆) is required to estimate γ⋆ up to a constant multiplicative accuracy. Essentially, the sequence may have to visit all d states at least log(d)/γ⋆ times each, on average. This holds even if π⋆ is within a factor of two of the largest possible value of 1/d that it can take, i.e., when π is nearly uniform.\nTheorem 3.2. There is an absolute constant c > 0 such that the following holds. Pick any positive integer d ≥ 3 and any γ̄⋆ ∈ (0, 1/2). Consider any estimator γ̂⋆ that takes as input a random sample path of length n < cd log(d)/γ̄⋆ from a d-state reversible Markov chain starting from any desired initial state distribution. There is an ergodic and reversible Markov chain distribution with spectral gap γ⋆ ∈ [γ̄⋆, 2γ̄⋆] and minimum stationary probability π⋆ ≥ 1/(2d) such that\nPr [|γ̂⋆ − γ⋆| ≥ γ̄⋆/2] ≥ 1/4. The proofs of Theorems 3.1 and 3.2 are given in Section 5.\n3.2. A plug-in based point estimator and its accuracy. Let us now consider the problem of estimating γ⋆. For this, we construct a natural plug-in estimator. Along the way, we also provide an estimator for the minimum stationary probability, allowing one to use the bounds from Eq. (2) to trap the mixing time.\nDefine the random matrix M̂ ∈ [0, 1]d×d and random vector π̂ ∈ ∆d−1 by\nM̂i,j := |{t ∈ [n− 1] : (Xt, Xt+1) = (i, j)}|\nn− 1 , i, j ∈ [d] ,\nπ̂i := |{t ∈ [n] : Xt = i}|\nn , i ∈ [d] .\nFurthermore, define\nSym(L̂) := 1\n2 (L̂+ L̂\n⊤\n)\nto be the symmetrized version of the (possibly non-symmetric) matrix\nL̂ := Diag(π̂)−1/2M̂ Diag(π̂)−1/2.\nLet λ̂1 ≥ λ̂2 ≥ · · · ≥ λ̂d be the eigenvalues of Sym(L̂). Our estimator of the minimum stationary probability π⋆ is π̂⋆ := mini∈[d] π̂i, and our estimator of the spectral gap γ⋆ is γ̂⋆ := 1 −min{1,max{λ̂2, |λ̂d|}} ∈ [0, 1]. The astute reader may notice that our estimator is ill-defined when π̂ is not positive valued. In this case, we can simply set γ̂⋆ = 0.\nThese estimators have the following accuracy guarantees:\nTheorem 3.3. There exists an absolute constant C ≥ 1 such that the following holds. Let (Xt) n t=1 be an ergodic and reversible Markov chain with spectral gap γ⋆ and minimum stationary probability π⋆ > 0. Let π̂⋆ = π̂⋆((Xt) n t=1) and γ̂⋆ = γ̂⋆((Xt) n t=1) be the estimators described above. For any δ ∈ (0, 1), with probability at least 1− δ,\n(4) |π̂⋆ − π⋆| ≤ C\n  √ π⋆ log 1 π⋆δ\nγ⋆n + log 1π⋆δ γ⋆n\n \nand\n(5) |γ̂⋆ − γ⋆| ≤ C\n√ log dδ · log nπ⋆δ\nπ⋆γ⋆n .\nTheorem 3.3 implies that the sequence lengths sufficient to estimate π⋆ and γ⋆ to within constant multiplicative factors are, respectively,\nÕ\n( 1\nπ⋆γ⋆\n) and Õ ( 1\nπ⋆γ3⋆\n) .\nThe proof of Theorem 3.3 is based on analyzing the convergence of the sample\naverages M̂ and π̂ to their expectation, and then using perturbation bounds for eigenvalues to derive a bound on the error of γ̂⋆. However, since these averages are formed using a single sample path from a (possibly) non-stationary Markov chain, we cannot use standard large deviation bounds; moreover applying Chernoff-\ntype bounds for Markov chains to each entry of M̂ would result in a significantly worse sequence length requirement, roughly a factor of d larger. Instead, we adapt probability tail bounds for sums of independent random matrices (Tropp 2015) to our non-iid setting by directly applying a blocking technique of Bernstein (1927) as described in the article of Yu (1994). Due to ergodicity, the convergence rate can be bounded without any dependence on the initial state distribution π(1). The proof of Theorem 3.3 is given in Section 6.\n3.3. Improving the plug-in estimator. We can bootstrap the plug-in estimator in Eq. (5) to show that in fact, to obtain any prescribed multiplicative accuracy,\nÕ(1/(π⋆γ⋆)) steps suffice to estimate γ⋆. The idea is to apply the estimator γ̂⋆ from Eq. (5) to the a-skipped chain (Xas) n/a s=1 for some a ≥ 1. This chain has spectral gap γ⋆(a) = 1 − (1 − γ⋆)a. Thus, letting γ̂⋆(a) be the plug-in estimator for γ⋆(a) based on the a-skipped chain, a natural estimator of γ⋆ is 1− (1− γ̂⋆(a))1/a.\nWhy may this improve on the original plug-in estimator from Section 3.2? Observe that γ⋆(a) = Ω(γ⋆a) for a ≤ 1/γ⋆, so the additive accuracy bound from\nEq. (5) for the plug-in estimator on (Xas) n/a s=1 is roughly the same for all a ≤ 1/γ⋆. However, when γ⋆(a) is bounded away from 0 and 1, a small additive error in estimating γ⋆(a) with γ̂⋆(a) translates to a small multiplicative error in estimating γ⋆ using 1− (1− γ̂⋆(a))1/a. So it suffices to use the skipped chain estimator with some a = O(1/γ⋆). Since γ⋆ is not known (of course), we use a doubling trick to find a suitable value of a.\nThe estimator is defined as follow. For simplicity, assume n is a power of two. Initially, set k := 0. Let a := 2k and γ̂⋆(a) := γ̂⋆((Xas) n/a s=1). If γ̂⋆(a) > 0.31 or a = n, then set A := a and return γ̃⋆ := 1− (1− γ̂⋆(A))1/A. Otherwise, increment k by one and repeat.\nTheorem 3.4. There exists a polynomial function L of the logarithms of γ−1⋆ , π−1⋆ , δ−1, and d such that the following holds. Let (Xt) n t=1 be an ergodic and reversible Markov chain with spectral gap γ⋆ and minimum stationary probability π⋆ > 0. Let γ̃⋆ = γ̃⋆((Xt) n t=1) be the estimator defined above. For any ε, δ ∈ (0, 1), if n ≥ L/(π⋆γ⋆ε2), then with probability at least 1− δ,∣∣∣∣ γ̃⋆ γ⋆ − 1 ∣∣∣∣ ≤ ε.\nThe definition of L is in Eq. (34). The proof of Theorem 3.4 is given in Section 7. The result shows that to estimate both π⋆ and γ⋆ to within constant multiplicative factors, a single sequence of length Õ(1/(π⋆γ⋆)) suffices."
    }, {
      "heading" : "4. A posteriori confidence intervals",
      "text" : "In this section, we describe and analyze a procedure for constructing confidence intervals for the stationary probabilities and the spectral gap γ⋆.\n4.1. Procedure. We first note that the point estimators from Theorem 3.3 and Theorem 3.4 fall short of being directly suitable for obtaining a fully empirical, a posteriori confidence interval for γ⋆ and π⋆. This is because the deviation terms themselves depend inversely both on γ⋆ and π⋆, and hence can never rule out 0 (or an arbitrarily small positive value) as a possibility for γ⋆ or π⋆.\n1 In effect, the fact that the Markov chain could be slow mixing and the long-term frequency of some states could be small makes it difficult to be confident in the estimates provided by γ̂⋆ and π̂⋆.\nThe main idea behind our procedure, given as Algorithm 1, is to use the Markov property to eliminate the dependence of the confidence intervals on the unknown quantities (including π⋆ and γ⋆). Specifically, we estimate the transition probabilities from the sample path using simple state visit counts: as a consequence of the Markov property, for each state, the frequency estimates converge at a rate that depends only on the number of visits to the state, and in particular the rate (given the visit count of the state) is independent of the mixing time of the chain.\nWith confidence intervals for the entries of P in hand, it is possible to form a confidence interval for γ⋆ based on the eigenvalues of an estimated transition probability matrix by appealing to the Ostrowski-Elsner theorem (cf. Theorem 1.4 on Page 170 of Stewart and Sun (1990).) However, directly using this perturbation\n1Using Theorem 3.3, it is possible to trap γ⋆ in the union of two empirical confidence intervals— one around γ̂⋆ and the other around zero, both of which shrink in width as the sequence length increases.\nAlgorithm 1 Confidence intervals\nInput: Sample path (X1, X2, . . . , Xn), confidence parameter δ ∈ (0, 1). 1: Compute state visit counts and smoothed transition probability estimates:\nNi := |{t ∈ [n− 1] : Xt = i}| , i ∈ [d]; Ni,j := |{t ∈ [n− 1] : (Xt, Xt+1) = (i, j)}| , (i, j) ∈ [d]2;\nP̂i,j := Ni,j + 1/d\nNi + 1 , (i, j) ∈ [d]2.\n2: Let Â# be the group inverse of Â := I − P̂ . 3: Let π̂ ∈ ∆d−1 be the unique stationary distribution for P̂ . 4: Compute eigenvalues λ̂1≥λ̂2≥ · · · ≥λ̂d of Sym(L̂), where L̂ :=\nDiag(π̂)1/2P̂ Diag(π̂)−1/2. 5: Spectral gap estimate:\nγ̂⋆ := 1−max{λ̂2, |λ̂d|}.\n6: Bounds for |P̂i,j−Pi,j | for (i, j) ∈ [d]2: c := 1.1, τn,δ := inf{t ≥ 0 : 2d2(1 + ⌈logc 2nt ⌉+)e−t ≤ δ}, and\nB̂i,j :=   √\ncτn,δ 2Ni + √√√√cτn,δ 2Ni + √ 2cP̂i,j(1− P̂i,j)τn,δ Ni + 4 3τn,δ + |P̂i,j − 1d | Ni   2 .\n7: Relative sensitivity of π:\nκ̂ := 1\n2 max\n{ Â#j,j −min { Â#i,j : i ∈ [d] } : j ∈ [d] } .\n8: Bounds for maxi∈[d] |π̂i − πi| and max ⋃ i∈[d]{| √ πi/π̂i − 1|, | √ π̂i/πi − 1|}:\nb̂ := κ̂max { B̂i,j : (i, j) ∈ [d]2 } , ρ̂ := 1\n2 max\n⋃\ni∈[d]\n{ b̂\nπ̂i ,\nb̂\n[π̂i − b̂]+\n} .\n9: Bounds for |γ̂⋆ − γ⋆|:\nŵ := 2ρ̂+ ρ̂2 + (1 + 2ρ̂+ ρ̂2)\n( ∑\n(i,j)∈[d]2\nπ̂i π̂j B̂2i,j\n)1/2 .\nresult leads to very wide intervals, shrinking only at a rate of O(n−1/(2d)). We avoid this slow rate by constructing confidence intervals for the symmetric matrix L, so that we can use a stronger perturbation result (namely Weyl’s inequality, as in the proof of Theorem 3.3) available for symmetric matrices.\nTo form an estimate of L based on an estimate of the transition probabilities, one possibility is to estimate π using state visit counts as was done in Section 3, and appeal to the relation L = Diag(π)1/2P Diag(π)−1/2 to form a plug-in estimate of L. However, it is not clear how to construct a confidence interval for the entries of π because the accuracy of this estimator depends on the unknown mixing time.\nWe adopt a different strategy for estimating π. We form the matrix P̂ using smoothed frequency estimates of P (Step 1), then compute the group inverse Â# of Â = I − P̂ (Step 2), followed by finding the unique stationary distribution π̂ of P̂ (Step 3), this way decoupling the bound on the accuracy of π̂ from the mixing time. The group inverse Â# of Â is uniquely defined; and if P̂ defines an ergodic chain (which is the case here due to the use of the smoothed estimates), Â# can be computed at the cost of inverting an (d−1)×(d−1) matrix (Meyer Jr. 1975, Theorem 5.2).2 Further, given Â#, the unique stationary distribution π̂ of P̂ can be read out from the last row of Â# (Meyer Jr. 1975, Theorem 5.3). The\ngroup inverse is also used to determine the relative sensitivity of π̂ to P̂ , which is quantified by\n(6) κ̂ := 1\n2 max\n{ Â#j,j −min { Â#i,j : i ∈ [d] } : j ∈ [d] } .\nWe can regard κ̂ as a plug-in estimator for κ, which is defined by substituting the group inverse A# of A in for Â #\nin Eq. (6). We can now follow the strategy based on estimating L alluded to above. Using\nπ̂ and P̂ , we construct the plug-in estimate L̂ of L, and use the eigenvalues of its symmetrization to form the estimate γ̂⋆ of the spectral gap (Steps 4 and 5). In the remaining steps, we use matrix perturbation analyses to relate π̂ and π, viewing P\nas the perturbation of P̂ ; and also to relate γ̂⋆ and γ⋆, viewingL as a perturbation of Sym(L̂). Both analyses give error bounds entirely in terms of observable quantities (e.g., κ̂), tracing back to empirical error bounds for the estimate of P .\nThe most computationally expensive step in Algorithm 1 is the computation of the group inverse Â#, which, as noted earlier, reduces to matrix inversion. Thus, with a standard implementation of matrix inversion, the algorithm’s time complexity is O(n+ d3), while its space complexity is O(d2).\n4.2. Main result. We now state our main theorems. Below, the big-O notation should be interpreted as follows. For a random sequence (Yn)n≥1 and a (nonrandom) positive sequence (εθ,n)n≥1 parameterized by θ, we say “Yn = O(εθ,n) holds almost surely as n → ∞” if there is some universal constant C > 0 such that for all θ, lim supn→∞ Yn/εθ,n ≤ C holds almost surely.\nTheorem 4.1. Suppose Algorithm 1 is given as input a sample path of length n from an ergodic and reversible Markov chain and confidence parameter δ ∈ (0, 1). Let γ⋆ > 0 denote the spectral gap, π the unique stationary distribution, and π⋆ > 0 the minimum stationary probability. Then, on an event of probability at least 1− δ,\nπi ∈ [π̂i − b̂, π̂i + b̂] for all i ∈ [d], and γ⋆ ∈ [γ̂⋆ − ŵ, γ̂⋆ + ŵ]. Moreover,\nb̂ = O ( max\n(i,j)∈[d]2 κ\n√ Pi,j log logn\nπin\n) , ŵ = O ( κ\nπ⋆\n√ log log n\nπ⋆n +\n√ d log logn\nπ⋆n\n)\nalmost surely as n → ∞.\n2 The group inverse of a square matrix A, a special case of the Drazin inverse, is the unique matrix A# satisfying AA#A = A, A#AA# = A# and A#A = AA#.\nThe proof of Theorem 4.1 is given in Section 8. As mentioned above, the obstacle encountered in Theorem 3.3 is avoided by exploiting the Markov property. We establish fully observable upper and lower bounds on the entries of P that converge at a √ (log logn)/n rate using standard martingale tail inequalities; this justifies the validity of the bounds from Step 6. Properties of the group inverse (Meyer Jr. 1975; Cho and Meyer 2001) and eigenvalue perturbation theory (Stewart and Sun 1990) are used to validate the empirical bounds on πi and γ⋆ developed in the remaining steps of the algorithm.\nThe first part of Theorem 4.1 provides valid empirical confidence intervals for each πi and for γ⋆, which are simultaneously valid at confidence level δ. The second part of Theorem 4.1 shows that the width of the intervals decrease as the sequence length increases. The rate at which the widths shrink is given in terms of P , π, κ, and n. We show in Section 8.5 (Lemma 8.8) that\nκ ≤ 1 γ⋆ min{d, 8 + log(4/π⋆)},\nand hence\nb̂ = O ( max\n(i,j)∈[d]2 min{d, log(1/π⋆)} γ⋆\n√ Pi,j log logn\nπin\n) ,\nŵ = O\n( min{d, log(1/π⋆)}\nπ⋆γ⋆\n√ log logn\nπ⋆n\n) .\nIt is easy to combine Theorems 3.3 and 4.1 to yield intervals whose widths shrink at least as fast as both the non-empirical intervals from Theorem 3.3 and the empirical intervals from Theorem 4.1. Specifically, determine lower bounds on π⋆ and γ⋆ using Algorithm 1, π⋆ ≥ mini∈[d][π̂i − b̂]+ , γ⋆ ≥ [γ̂⋆ − ŵ]+; then plug-in these lower bounds for π⋆ and γ⋆ in the deviation bounds in Eq. (5) from Theorem 3.3. This yields a new interval centered around the estimate of γ⋆ from Theorem 3.3 and the new interval no longer depends on unknown quantities. The interval is a valid 1 − 2δ probability confidence interval for γ⋆, and for sufficiently large n, the width shrinks at the rate given in Eq. (5). We can similarly construct an empirical confidence interval for π⋆ using Eq. (4), which is valid on the same 1− 2δ probability event.3 Finally, we can take the intersection of these new intervals with the corresponding intervals from Algorithm 1. This is summarized in the following theorem, which we prove in Section 9.\nTheorem 4.2. The following holds under the same conditions as Theorem 4.1. For any δ ∈ (0, 1), the confidence intervals Û and V̂ described above for π⋆ and γ⋆, respectively, satisfy π⋆ ∈ Û and γ⋆ ∈ V̂ with probability at least 1−2δ. Furthermore,\n|Û | = O (√ π⋆ log d π⋆δ\nγ⋆n\n) and |V̂ | = O ( min {√ log d δ ·log(n)\nπ⋆γ⋆n , ŵ\n}) almost surely as\nn → ∞, where ŵ is the width from Algorithm 1. Finally, note that a stopping rule that stops when γ⋆ and π⋆ are estimated with a given relative error ǫ can be obtained as follows. At time n:\n3For the π⋆ interval, we only plug-in lower bounds on π⋆ and γ⋆ only where these quantities appear as 1/π⋆ and 1/γ⋆ in Eq. (4). It is then possible to “solve” for observable bounds on π⋆. See Section 9 for details.\n1: if n = 2k for an integer k then 2: Run Algorithm 1 (or the improved variant from Theorem 4.2) with inputs (X1, X2, . . . , Xn) and δ/(k(k + 1)) to obtain intervals for π⋆ and γ⋆. 3: Stop if, for each interval, the interval width divided by the lower bound on estimated quantity falls below ǫ. 4: end if It is easy to see then that with probability 1− δ, the algorithm only stops when the relative accuracy of its estimate is at least ǫ. Combined with the lower bounds, we conjecture that the expected stopping time of the resulting procedure is optimal up to log factors."
    }, {
      "heading" : "5. Proofs of Theorems 3.1 and 3.2",
      "text" : "In this section, we prove Theorem 3.1 and Theorem 3.2.\n5.1. Proof of Theorem 3.1. Fix π̄ ∈ (0, 1/4). Consider two Markov chains given by the following stochastic matrices:\nP (1) := [ 1− π̄ π̄ 1− π̄ π̄ ] , P (2) := [ 1− π̄ π̄ 1/2 1/2 ] .\nEach Markov chain is ergodic and reversible; their stationary distributions are, respectively, π(1) = (1 − π̄, π̄) and π(2) = (1/(1 + 2π̄), 2π̄/(1 + 2π̄)). We have π⋆ ≥ π̄ in both cases. For the first Markov chain, λ⋆ = 0, and hence the spectral gap is 1; for the second Markov chain, λ⋆ = 1/2− π̄, so the spectral gap is 1/2+ π̄.\nIn order to guarantee |γ̂⋆ − γ⋆| < 1/8 < |1− (1/2+ π̄)|/2, it must be possible to distinguish the two Markov chains. Assume that the initial state distribution has mass at least 1/2 on state 1. (If this is not the case, we swap the roles of states 1 and 2 in the constructions above.) With probability at least half, the initial state is 1; and both chains have the same transition probabilities from state 1. The chains are indistinguishable unless the sample path eventually reaches state 2. But with probability at least 3/4, a sample path of length n < 1/(4π̄) starting from state 1 always remains in the same state (this follows from properties of the geometric distribution and the assumption π̄ < 1/4).\n5.2. Proof of Theorem 3.2. We consider d-state Markov chains of the following form:\nPi,j = { 1− εi if i = j; εi\nd− 1 if i 6= j\nfor some ε1, ε2, . . . , εd ∈ (0, 1). Such a chain is ergodic and reversible, and its unique stationary distribution π satisfies\nπi = 1/εi∑d j=1 1/εj .\nWe fix ε := d−1d/2 γ̄ and set ε ′ := d/2−1d−1 ε < ε. Consider the following d + 1 different Markov chains of the type described above:\n• P (0): ε1 = · · · = εd = ε. For this Markov chain, λ2 = λd = λ⋆ = 1− dd−1ε. • P (i) for i ∈ [d]: εj = ε for j 6= i, and εi = ε′. For these Markov chains, λ2 = 1− ε′ − 1d−1ε = 1− d/2 d−1ε, and λd = 1− dd−1ε. So λ⋆ = 1− d/2 d−1ε.\nThe spectral gap in each chain satisfies γ⋆ ∈ [γ̄, 2γ̄]; in P (i) for i ∈ [d], it is half of what it is in P (0). Also πi ≥ 1/(2d) for each i ∈ [d].\nIn order to guarantee |γ̂⋆−γ⋆| < γ̄/2, it must be possible to distinguish P (0) from each P (i), i ∈ [d]. But P (0) is identical to P (i) except for the transition probabilities from state i. Therefore, regardless of the initial state, the sample path must visit all states in order to distinguish P (0) from each P (i), i ∈ [d]. For any of the d+ 1 Markov chains above, the earliest time in which a sample path visits all d states stochastically dominates a generalized coupon collection time T = 1 + ∑d−1\ni=1 Ti, where Ti is the number of steps required to see the (i + 1)-th distinct state in the sample path beyond the first i. The random variables T1, T2, . . . , Td−1 are independent, and are geometrically distributed, Ti ∼ Geom(ε − (i − 1)ε/(d − 1)). We have that\nE[Ti] = d− 1 ε(d− i) , var(Ti) = 1− ε d−id−1( ε d−id−1 )2 .\nTherefore\nE[T ] = 1 + d− 1 ε Hd−1, var(T ) ≤ ( d− 1 ε )2 π2 6\nwhere Hd−1 = 1 + 1/2 + 1/3 + · · ·+ 1/(d− 1). By the Paley-Zygmund inequality,\nPr ( T > 1\n3 E[T ]\n) ≥ 1\n1 + var(T )(1−1/3)2E[T ]2 ≥ 1 1 + ( d−1ε ) 2 π2 6\n(4/9)( d−1ε H2) 2\n≥ 1 4 .\nSince n < cd log(d)/γ̄ ≤ (1/3)(1 + (d− 1)Hd−1/(2γ̄)) = E[T ]/3 (for an appropriate absolute constant c), with probability at least 1/4, the sample path does not visit all d states."
    }, {
      "heading" : "6. Proof of Theorem 3.3",
      "text" : "In this section, we prove Theorem 3.3.\n6.1. Accuracy of π̂⋆. We start by proving the deviation bound on π⋆ − π̂⋆, from which we may easily deduce Eq. (4) in Theorem 3.3.\nLemma 6.1. Pick any δ ∈ (0, 1), and let (7) εn := ln ( d δ √ 2 π⋆ )\nγ⋆n .\nWith probability at least 1− δ, the following inequalities hold simultaneously: |π̂i − πi| ≤ √ 8πi(1− πi)εn + 20εn for all i ∈ [d];(8) |π̂⋆ − π⋆| ≤ 4 √ π⋆εn + 47εn.(9)\nProof. We use the following Bernstein-type inequality for Markov chains of Paulin (2015, Theorem 3.3): letting Pπ denote the probability with respect to the stationary chain (where the marginal distribution of each Xt is π), we have for every ǫ > 0,\nP π (|π̂i − πi| > ǫ) ≤ 2 exp ( − nγ⋆ǫ 2\n4πi(1− πi) + 10ǫ\n) , i ∈ [d].\nTo handle possibly non-stationary chains, as is our case, we combine the above inequality with Paulin (2015, Proposition 3.10), to obtain for any ǫ > 0,\nP (|π̂i − πi| > ǫ) ≤ √ 1\nπ⋆ Pπ (|π̂i − πi| > ǫ) ≤\n√ 2\nπ⋆ exp\n( − nγ⋆ǫ 2\n8πi(1− πi) + 20ǫ\n) .\nUsing this tail inequality with ǫ := √ 8πi(1− πi)εn +20εn and a union bound over all i ∈ [d] implies that the inequalities in Eq. (8) hold with probability at least 1−δ. Now assume this 1− δ probability event holds; it remains to prove that Eq. (9) also holds in this event. Without loss of generality, we assume that π⋆ = π1 ≤ π2 ≤ · · · ≤ πd. Let j ∈ [d] be such that π̂⋆ = π̂j . By Eq. (8), we have |πi − π̂i| ≤√ 8πiεn + 20εn for each i ∈ {1, j}. Since π̂⋆ ≤ π̂1,\nπ̂⋆ − π⋆ ≤ π̂1 − π1 ≤ √ 8π⋆εn + 20εn ≤ π⋆ + 22εn\nwhere the last inequality follows by the AM/GM inequality. Furthermore, using the fact that a ≤ b√a+ c ⇒ a ≤ b2 + b√c+ c for nonnegative numbers a, b, c ≥ 0 (see, e.g., Bousquet, Boucheron, and Lugosi 2004) with the inequality πj ≤ √ 8εn √ πj + (π̂j + 20εn) gives\nπj ≤ π̂j + √ 8(π̂j + 20εn)εn + 28εn.\nTherefore π⋆−π̂⋆ ≤ πj−π̂j ≤ √ 8(π̂⋆ + 20εn)εn+28εn ≤ √ 8(2π⋆ + 42εn)εn+28εn ≤ 4 √ π⋆εn+47εn\nwhere the second-to-last inequality follows from the above bound on π̂⋆ − π⋆, and the last inequality uses √ a+ b ≤ √a+ √ b for nonnegative a, b ≥ 0.\n6.2. Accuracy of γ̂⋆. Let us now turn to proving Eq. (5), i.e., the bound on the error of the spectral gap estimate γ̂⋆. The accuracy of γ̂⋆ is based on the accuracy of Sym(L̂) in approximating L via Weyl’s inequality:\n|λ̂i − λi| ≤ ‖ Sym(L̂)−L‖ for all i ∈ [d].\nMoreover, the triangle inequality implies that symmetrizing L̂ can only help:\n‖ Sym(L̂)−L‖ ≤ ‖L̂−L‖.\nTherefore, we can deduce Eq. (5) in Theorem 3.3 from the following lemma.\nLemma 6.2. There exists an absolute constant C > 0 such that the following holds. For any δ ∈ (0, 1), if (10) n ≥ C ( log 1π⋆δ π⋆γ⋆ + logn γ⋆ ) ,\nthen with probability at least 1− δ, the bounds from Lemma 6.1 hold, and\n‖L̂−L‖ ≤ C (√ ε+ ε+ ε2 ) ,\nwhere\nε :=\n( log dδ )( log nπ⋆δ )\nπ⋆γ⋆n .\nWe briefly describe how to obtain the bound on |γ̂⋆ − γ⋆| that appears in Eq. (5), which is of the form C′ √ ε. Observe that if ε > 1/C′, then, owing to C′ ≥ 1, the bound on |γ̂⋆ − γ⋆| is trivial. So we may assume that ε ≤ 1/C′, which implies n/ logn ≥ C′(log(d/δ))/(π⋆γ⋆) (and thus n ≥ 2), and also n ≥ C′(log(d/δ))(log(1/(π⋆δ)))/(π⋆γ⋆). These inequalities imply that n satisfies the condition in Eq. (10), so by Lemma 6.2, we have |γ̂⋆ − γ⋆| ≤ ‖L̂ − L‖ ≤ C( √ ε+ ε+ ε2) ≤ C′√ε.\nThe remainder of this section is devoted to proving this lemma. When π̂ is positive valued, the error L̂−L may be written as L̂−L = EM + EπL+LEπ + EπLEπ + EπEM + EMEπ + EπEMEπ ,\nwhere\nEπ := Diag(π̂) −1/2 Diag(π)1/2 − I and\nEM := Diag(π) −1/2 ( M̂ −M ) Diag(π)−1/2 .\nTherefore\n‖L̂−L‖ ≤ ‖EM‖+ (‖EM‖+ ‖L‖) ( 2‖Eπ‖+ ‖Eπ‖2 ) .\nIf ‖Eπ‖ ≤ 1 also holds, then, thanks to ‖L‖ ≤ 1, ‖L̂−L‖ ≤ ‖EM‖+ ‖EM‖2 + 3‖Eπ‖.(11)\n6.3. A bound on ‖Eπ‖. Since Eπ is diagonal,\n‖Eπ‖ = max i∈[d]\n∣∣∣∣ √\nπi π̂i\n− 1 ∣∣∣∣ .\nAssume that\n(12) n ≥ 108 ln\n( d δ √ 2 π⋆ )\nπ⋆γ⋆ ,\nin which case √ 8πi(1− πi)εn + 20εn ≤\nπi 2 ,\nwhere εn is as defined in Eq. (7). Therefore, on the 1 − δ probability event from Lemma 6.1, we have |πi− π̂i| ≤ πi/2 for each i ∈ [d], and moreover, 2/3 ≤ πi/π̂i ≤ 2 for each i ∈ [d]. In particular, it also holds that π̂ is positive valued. Further, for this range of πi/π̂i, we have ∣∣∣∣ √ πi π̂i − 1 ∣∣∣∣ ≤ ∣∣∣∣ π̂i πi − 1 ∣∣∣∣ . We conclude that if n satisfies Eq. (12), then on this 1 − δ probability event from Lemma 6.1, π̂ is positive valued and\n(13) ‖Eπ‖ ≤ max i∈[d] ∣∣∣∣ π̂i πi − 1 ∣∣∣∣ ≤ maxi∈[d] √ 8πi(1− πi)εn + 20εn πi\n≤ √\n8εn π⋆ + 20εn π⋆ =\n√√√√8 ln ( d δ √ 2 π⋆ )\nπ⋆γ⋆n +\n20 ln (\nd δ √ 2 π⋆ )\nπ⋆γ⋆n ≤ min{C′(\n√ ε+ ε), 1}\nfor some suitable constant C′ > 0, where ε as defined in Lemma 6.2.\n6.4. Accuracy of doublet frequency estimates (bounding ‖EM‖). In this section we prove a bound on ‖EM‖. For this, we decompose EM = Diag(π)−1/2(M̂− M)Diag(π)−1/2 into E (EM ) and EM − E (EM ), the first measuring the effect of a non-stationary start of the chain, while the second measuring the variation due to randomness.\n6.4.1. Bounding ‖E (EM ) ‖: The price of a non-stationary start. Let π(t) be the distribution of states at time step t. We will make use of the following proposition, which can be derived by following Montenegro and Tetali (2006, Proposition 1.12):\nProposition 6.3. For t ≥ 1, let Υ(t) be the vector with Υ(t)i = π (t) i πi and let ‖ · ‖2,π denote the π-weighted 2-norm\n‖v‖2,π := ( d∑\ni=1\nπiv 2 i\n)1/2 .(14)\nThen,\n(15) ‖Υ(t) − 1‖2,π ≤ (1 − γ⋆)t−1√\nπ⋆ .\nAn immediate corollary of this result is that ∥∥∥Diag(π(t))Diag(π)−1 − I ∥∥∥ ≤ (1− γ⋆) t−1\nπ⋆ .(16)\nNow note that\nE(M̂ ) = 1\nn− 1\nn−1∑\nt=1\nDiag(π(t))P\nand thus\nE (EM ) = Diag(π) −1/2 ( E(M̂ )−M ) Diag(π)−1/2\n= 1\nn− 1\nn−1∑\nt=1\nDiag(π)−1/2(Diag(π(t))−Diag(π))P Diag(π)−1/2\n= 1\nn− 1\nn−1∑\nt=1\nDiag(π)−1/2(Diag(π(t))Diag(π)−1 − I)M Diag(π)−1/2\n= 1\nn− 1\nn−1∑\nt=1\n(Diag(π(t))Diag(π)−1 − I)L .\nCombining this, ‖L‖ ≤ 1 and Eq. (16), we get\n‖E(EM )‖ ≤ 1\n(n− 1)π⋆\nn−1∑\nt=1\n(1− γ⋆)t−1 ≤ 1\n(n− 1)γ⋆π⋆ .(17)\n6.4.2. Bounding ‖EM − E (EM ) ‖: Application of a matrix tail inequality. In this section we analyze the deviations of EM − E (EM ). By the definition of EM ,\n‖EM − E (EM ) ‖ = ‖Diag(π)−1/2 ( M̂ − EM̂ ) Diag(π)−1/2‖ .(18)\nThe matrix M̂−E ( M̂ ) is defined as a sum of dependent centered randommatrices.\nWe will use the blocking technique of Bernstein (1927) to relate the likely deviations of this matrix to that of a sum of independent centered random matrices. The deviations of these will then bounded with the help of a Bernstein-type matrix tail inequality due to Tropp (2015).\nWe divide [n − 1] into contiguous blocks of time steps; each has size a ≤ n/3 except possibly the first block, which has size between a and 2a− 1. Formally, let a′ := a+ ((n− 1) mod a) ≤ 2a− 1, and define\nF := [a′],\nHs := {t ∈ [n− 1] : a′ + 2(s− 1)a+ 1 ≤ t ≤ a′ + (2s− 1)a}, Ts := {t ∈ [n− 1] : a′ + (2s− 1)a+ 1 ≤ t ≤ a′ + 2sa},\nfor s = 1, 2, . . . . Let µH (resp., µT ) be the number of non-empty Hs (resp., Ts) blocks. Let nH := aµH (resp., nT := aµT ) be the number of time steps in ∪sHs (resp., ∪sTs). We have\nM̂ = 1\nn− 1\nn−1∑\nt=1\neXte ⊤\nXt+1\n= a′ n− 1 · 1 a′\n∑\nt∈F\neXte ⊤\nXt+1\n︸ ︷︷ ︸ M̂F\n+ nH n− 1 · 1\nµH\nµH∑\ns=1\n( 1\na\n∑\nt∈Hs\neXte ⊤\nXt+1\n)\n︸ ︷︷ ︸ M̂H\n+ nT n− 1 · 1\nµT\nµT∑\ns=1\n( 1\na\n∑\nt∈Ts\neXte ⊤\nXt+1\n)\n︸ ︷︷ ︸ M̂T\n.(19)\nHere, ei is the i-th coordinate basis vector, so eie ⊤ j ∈ {0, 1}d×d is a d× d matrix of all zeros except for a 1 in the (i, j)-th position.\nThe contribution of the first block is easily bounded using the triangle inequality:\n(20) a′ n− 1 ∥∥∥Diag(π)−1/2 ( M̂F − E(M̂F ) ) Diag(π)−1/2 ∥∥∥\n≤ 1 n− 1\n∑\nt∈F\n{∥∥∥∥∥ eXte ⊤ Xt+1√ πXtπXt+1 ∥∥∥∥∥+ ∥∥∥∥∥E ( eXte ⊤ Xt+1√ πXtπXt+1 )∥∥∥∥∥ } ≤ 2a ′ π⋆(n− 1) .\nIt remains to bound the contributions of the Hs blocks and the Ts blocks. We just focus on the the Hs blocks, since the analysis is identical for the Ts blocks.\nLet\nY s := 1\na\n∑\nt∈Hs\neXte ⊤ Xt+1 , s ∈ [µH ],\nso\nM̂H = 1\nµH\nµH∑\ns=1\nY s,\nan average of the random matrices Y s. For each s ∈ [µH ], the random matrix Y s is a function of\n(Xt : a ′ + 2(s− 1)a+ 1 ≤ t ≤ a′ + (2s− 1)a+ 1)\n(note the +1 in the upper limit of t), so Y s+1 is a time steps ahead of Y s. When a is sufficiently large, we will be able to effectively treat the random matrices Y s as if they were independent. In the sequel, we shall always assume that the block length a satisfies\n(21) a ≥ aδ := 1\nγ⋆ ln 2(n− 2) δπ⋆\nfor δ ∈ (0, 1). Define\nπ(Hs) := 1\na\n∑\nt∈Hs\nπ(t), π(H) := 1\nµH\nµH∑\ns=1\nπ(Hs).\nObserve that\nE(Y s) = Diag(π (Hs))P\nso\nE\n( 1\nµH\nµH∑\ns=1\nY s\n) = Diag(π(H))P .\nDefine\nZs := Diag(π) −1/2 (Y s − E(Y s)) Diag(π)−1/2.\nWe apply a matrix tail inequality to the average of independent copies of the Zs’s. More precisely, we will apply the tail inequality to independent copies Z̃s, s ∈ [µH ] of the random variables Zs and then relate the average of Z̃s to that of Zs. The following probability inequality is from Tropp (2015, Theorem 6.1.1.).\nTheorem 6.4 (Matrix Bernstein inequality). Let Q1,Q2, . . . ,Qm be a sequence of independent, random d1 × d2 matrices. Assume that E (Qi) = 0 and ‖Qi‖ ≤ R for each 1 ≤ i ≤ m. Let S = ∑mi=1 Qi and let\nv = max { ‖E∑i QiQ⊤i ‖, ‖E ∑ iQ ⊤ i Qi‖ } .\nThen, for all t ≥ 0,\nP (‖S‖ ≥ t) ≤ 2(d1 + d2) exp ( − t 2/2\nv +Rt/3\n) .\nIn other words, for any δ ∈ (0, 1),\nP ( ‖S‖ > √ 2v ln 2(d1 + d2)\nδ +\n2R\n3 ln\n2(d1 + d2)\nδ\n) ≤ δ .\nTo apply Theorem 6.4, it suffices to bound the spectral norms of Zs (almost surely), E(ZsZ ⊤ s ), and E(Z ⊤ s Zs).\nRange bound. By the triangle inequality,\n‖Zs‖ ≤ ‖Diag(π)−1/2Y s Diag(π)−1/2‖+ ‖Diag(π)−1/2E(Y s)Diag(π)−1/2‖ . For the first term, we have\n‖Diag(π)−1/2Y s Diag(π)−1/2‖ ≤ 1\nπ⋆ .(22)\nFor the second term, we use the fact ‖L‖ ≤ 1 to bound\n‖Diag(π)−1/2(E(Y s)−M)Diag(π)−1/2‖ = ‖ ( Diag(π(Hs))Diag(π)−1 − I ) L‖\n≤ ‖Diag(π(Hs))Diag(π)−1 − I‖ .\nThen, using Eq. (16),\n(23) ‖Diag(π(Hs))Diag(π)−1 − I‖ ≤ (1 − γ⋆) a′+2(s−1)a π⋆ ≤ (1− γ⋆) a π⋆ ≤ 1 ,\nwhere the last inequality follows from the assumption that the block length a satisfies Eq. (21). Combining this with ‖Diag(π)−1/2M Diag(π)−1/2‖ = ‖L‖ ≤ 1, it follows that\n‖Diag(π)−1/2E(Y s)Diag(π)−1/2‖ ≤ 2(24)\nby the triangle inequality. Therefore, together with Eq. (22), we obtain the range bound\n‖Zs‖ ≤ 1\nπ⋆ + 2.\nVariance bound. We now determine bounds on the spectral norms of E(ZsZ ⊤ s ) and E(Z⊤s Zs). Observe that\nE(ZsZ ⊤ s )\n= 1\na2\n∑\nt∈Hs\nE ( Diag(π)−1/2eXte ⊤ Xt+1 Diag(π) −1eXt+1e ⊤ Xt Diag(π) −1/2 ) (25)\n+ 1\na2\n∑\nt6=t′\nt,t′∈Hs\nE ( Diag(π)−1/2eXte ⊤ Xt+1 Diag(π) −1eXt′+1e ⊤ Xt′ Diag(π)−1/2 ) (26)\n−Diag(π)−1/2E(Y s)Diag(π)−1E(Y ⊤s )Diag(π)−1/2.(27)\nThe first sum, Eq. (25), easily simplifies to the diagonal matrix\n1\na2\n∑\nt∈Hs\nd∑\ni=1\nd∑\nj=1\nPr(Xt = i,Xt+1 = j) · 1\nπiπj eie\n⊤ j eje ⊤ i\n= 1\na2\n∑\nt∈Hs\nd∑\ni=1\nd∑\nj=1\nπ (t) i Pi,j ·\n1\nπiπj eie\n⊤ i = 1\na\nd∑\ni=1\nπ (Hs) i\nπi\n  d∑\nj=1\nPi,j πj\n eie⊤i .\nFor the second sum, Eq. (26), a symmetric matrix, consider\nu⊤   1\na2\n∑\nt6=t′\nt,t′∈Hs\nE ( Diag(π)−1/2eXte ⊤ Xt+1 Diag(π) −1eXt′+1e ⊤ Xt′ Diag(π)−1/2 )  u\nfor an arbitrary unit vector u. By Cauchy-Schwarz and AM/GM, this is bounded from above by\n1\n2a2\n∑\nt6=t′\nt,t′∈Hs\n[ E ( u⊤ Diag(π)−1/2eXte ⊤ Xt+1 Diag(π) −1eXt+1e ⊤ Xt Diag(π) −1/2u )\n+ E ( u⊤ Diag(π)−1/2eXt′e ⊤ Xt′+1 Diag(π)−1eXt′+1e ⊤ Xt′ Diag(π)−1/2u )] ,\nwhich simplifies to\na− 1 a2 u⊤E\n(∑\nt∈Hs\nDiag(π)−1/2eXte ⊤ Xt+1 Diag(π) −1eXt+1e ⊤ Xt Diag(π) −1/2\n) u .\nThe expectation is the same as that for the first term, Eq. (25). Finally, the spectral norm of the third term, Eq. (27), is bounded using Eq. (24):\n‖Diag(π)−1/2E(Y s)Diag(π)−1/2‖2 ≤ 4.\nTherefore, by the triangle inequality, the bound π (H) i /πi ≤ 2 from Eq. (23), and\nsimplifications,\n‖E(ZsZ⊤s )‖ ≤ max i∈[d]\n  d∑\nj=1\nPi,j πj\n  π (H) i\nπi + 4 ≤ 2max i∈[d]\n  d∑\nj=1\nPi,j πj\n + 4.\nWe can bound E(Z⊤s Zs) in a similar way; the only difference is that the reversibility needs to be used at one place to simplify an expectation:\n1\na2\n∑\nt∈Hs\nE ( Diag(π)−1/2eXt+1e ⊤ Xt Diag(π) −1eXte ⊤ Xt+1 Diag(π) −1/2 )\n= 1\na2\n∑\nt∈Hs\nd∑\ni=1\nd∑\nj=1\nPr(Xt = i,Xt+1 = j) · 1\nπiπj eje\n⊤ j\n= 1\na2\n∑\nt∈Hs\nd∑\ni=1\nd∑\nj=1\nπ (t) i Pi,j ·\n1\nπiπj eje\n⊤ j\n= 1\na2\n∑\nt∈Hs\nd∑\nj=1\n( d∑\ni=1\nπ (t) i\nπi · Pj,i πi\n) eje ⊤\nj\nwhere the last step uses Eq. (3). As before, we get\n‖E(Z⊤s Zs)‖ ≤ max i∈[d]\n  d∑\nj=1\nPi,j πj\n· π (H) j\nπj\n + 4 ≤ 2max\ni∈[d]\n  d∑\nj=1\nPi,j πj\n + 4\nagain using the bound π (H) i /πi ≤ 2 from Eq. (23). Independent copies bound. Let Z̃s for s ∈ [µH ] be independent copies of Zs for s ∈ [µH ]. Applying Theorem 6.4 to the average of these random matrices, we have\n(28) P   ∥∥∥∥∥ 1\nµH\nµH∑\ns=1\nZ̃s ∥∥∥∥∥ > √ 4 (dP + 2) ln 4d δ µH + 2 ( 1 π⋆ + 2 ) ln 4dδ 3µH   ≤ δ\nwhere\ndP := max i∈[d]\nd∑\nj=1\nPi,j πj ≤ 1 π⋆ .\nThe actual bound. To bound the probability that ‖ ∑µH\ns=1 Zs/µH‖ is large, we appeal to the following result (a consequence of Yu 1994, Corollary 2.7). For each s ∈ [µH ], let X(Hs) := (Xt : a′ + 2(s − 1)a + 1 ≤ t ≤ a′ + (2s − 1)a + 1), which are the random variables determining Zs. Let P denote the joint distribution of (X(Hs) : s ∈ [µH ]); let Ps be its marginal over X(Hs), and let P1:s+1 be its marginal over (X(H1), X(H2), . . . , X(Hs+1)). Let P̃ be the product distribution formed from the marginals P1,P2, . . . ,PµH , so P̃ governs the joint distribution of (Z̃s : s ∈ [µH ]). The result from Yu (1994, Corollary 2.7) implies for any event E,\n|P(E)− P̃(E)| ≤ (µH − 1)β(P)\nwhere\nβ(P) := max 1≤s≤µH−1 E (∥∥∥P1:s+1(· |X(H1), X(H2), . . . , X(Hs))− Ps+1 ∥∥∥ tv ) .\nHere, ‖ · ‖tv denotes the total variation norm. The number β(P) can be recognized to be the β-mixing coefficient of the stochastic process {X(Hs)}s∈[µH ]. This result implies that the bound from Eq. (28) for ‖∑µHs=1 Z̃s/µH‖ also holds for ‖∑µHs=1 Zs/µH‖, except the probability bound increases from δ to δ+(µH−1)β(P): (29)\nP   ∥∥∥∥∥ 1\nµH\nµH∑\ns=1\nZs ∥∥∥∥∥ > √ 4 (dP + 2) ln 4d δ µH + 2 ( 1 π⋆ + 2 ) ln 4dδ 3µH   ≤ δ + (µH − 1)β(P).\nBy the triangle inequality,\nβ(P) ≤ max 1≤s≤µH−1 E (∥∥∥P1:s+1(· |X(H1), X(H2), . . . , X(Hs))− Pπ ∥∥∥ tv + ‖Ps+1 − Pπ‖tv )\nwhere Pπ is the marginal distribution of X(H1) under the stationary chain. Using the Markov property and integrating out Xt for t > minHs+1 = a ′ + 2sa+ 1, ∥∥∥P1:s+1(· |X(H1), X(H2), . . . , X(Hs))− Pπ ∥∥∥ tv = ∥∥L(Xa′+2sa+1 |Xa′+(2s−1)a+1)− π ∥∥ tv\nwhere L(Y |Z) denotes the conditional distribution of Y given Z. We bound this distance using standard arguments for bounding the mixing time in terms of the relaxation time 1/γ⋆ (see, e.g., the proof of Theorem 12.3 of Levin, Peres, and Wilmer 2009): for any i ∈ [d],\n∥∥L(Xa′+2sa+1 |Xa′+(2s−1)a+1 = i)− π ∥∥ tv = ‖L(Xa+1 |X1 = i)− π‖tv ≤ exp (−aγ⋆)\nπ⋆ .\nThe distance ‖Ps+1 − Pπ‖tv can be bounded similarly:\n‖Ps+1 − Pπ‖tv = ‖L(Xa′+2sa+1)− π‖tv\n= ∥∥∥∥∥ d∑\ni=1 P(X1 = i)L(Xa′+2sa+1 |X1 = i)− π ∥∥∥∥∥ tv\n≤ d∑\ni=1\nP(X1 = i) ‖L(Xa′+2sa+1 |X1 = i)− π‖tv\n≤ exp (−(a ′ + 2sa)γ⋆) π⋆ ≤ exp (−aγ⋆) π⋆ .\nWe conclude\n(µH − 1)β(P) ≤ (µH − 1) 2 exp(−aγ⋆) π⋆ ≤ 2(n− 2) exp(−aγ⋆) π⋆ ≤ δ\nwhere the last step follows from the block length assumption Eq. (21). We return to the decomposition from Eq. (19). We apply Eq. (29) to both the Hs blocks and the Ts blocks, and combine with Eq. (20) to obtain the following probabilistic bound. Pick any δ ∈ (0, 1), let the block length be\na := ⌈aδ⌉ = ⌈ 1\nγ⋆ ln 2(n− 2) π⋆δ\n⌉ ,\nso\nmin{µH , µT } = ⌊ n− 1− a′\n2a\n⌋ ≥ n− 1\n2 ( 1 + 1γ⋆ ln 2(n−2) π⋆δ ) − 2 =: µ.\nIf\n(30) n ≥ 7 + 6 γ⋆ ln 2(n− 2) π⋆δ ≥ 3a,\nthen with probability at least 1− 4δ, ∥∥∥Diag(π)−1/2 ( M̂ − E[M̂ ] ) Diag(π)−1/2 ∥∥∥\n≤ 4 ⌈ 1 γ⋆ ln 2(n−2)π⋆δ ⌉\nπ⋆(n− 1) +\n√ 4 (dP + 2) ln 4d δ\nµ +\n2 (\n1 π⋆ + 2 ) ln 4dδ\n3µ .\n6.4.3. The bound on ‖EM‖. Combining the probabilistic bound from above with the bound on the bias from Eq. (17), we obtain the following. Assuming the condition on n from Eq. (30), with probability at least 1− 4δ,\n(31) ‖EM‖ ≤ 1\n(n− 1)γ⋆π⋆ +\n4 ⌈\n1 γ⋆ ln 2(n−2)π⋆δ\n⌉\nπ⋆(n− 1)\n+\n√ 4 (dP + 2) ln 4d δ\nµ +\n2 (\n1 π⋆ + 2 ) ln 4dδ\n3µ ≤ C′\n(√ ε+ ε ) ,\nfor some suitable constant C′ > 0, where ε as defined in Lemma 6.2.\n6.5. Overall error bound. Observe that the assumption on the sequence length in Eq. (10) implies the conditions in Eq. (12) and Eq. (30) for a suitable choice of C > 0. With this assumption, there is a 1− 5δ probability event in which Eqs. (8), (9) and (31) hold; in particular, we have the bound on ‖EM‖ from Eq. (31). In this event, the bound on ‖Eπ‖ in Eq. (13) also holds, and the claimed bound on ‖L̂− L‖ follows from combining the bound in Eq. (11) with the bounds on ‖Eπ‖ and ‖EM‖:\n‖L̂−L‖ ≤ ‖EM‖+ ‖EM‖2 + 3‖Eπ‖ ≤ 4C′ (√ ε+ ε ) + C′ 2 (√ ε+ ε )2 ≤ C (√ ε+ ε+ ε2 ) ,\nwhere ε is defined in the statement of Lemma 6.2. The proof of Lemma 6.2 now follows by replacing δ with δ/5."
    }, {
      "heading" : "7. Proof of Theorem 3.4",
      "text" : "In this section, we prove Theorem 3.4. We call γ̂⋆ of Theorem 3.3 the initial estimator. Let C be the constant from\nTheorem 3.3, and define\nn1 = n1(ε; δ, γ⋆) := 3C2 ε2π⋆γ⋆ · ( log d δ ) · ( log 3C2 ε2π2⋆γ⋆δ )\nand\nM(n; δ, γ⋆) := C\n√ log dδ · log nπ⋆δ\nπ⋆γ⋆n ,\nwhich is the right-hand side of Eq. (5). Observe that\nM(n1; δ, γ⋆) ≤ ε\n√√√√ log 3C2 ε2π2⋆γ⋆δ + log log dδ + log log 3C2 ε2π2⋆γ⋆δ\n3 log 3C 2\nε2π2⋆γ 2 ⋆δ\n≤ ε.\n(Each term in the numerator under the radical is at most a third of the denominator. We have used that π⋆ ≤ 1/d in comparing the second term in the numerator to the denominator.)\nFor a > 0, the spectral gap of the chain with transition matrix P a is denoted by γ⋆(a), and the initial estimator of γ⋆(a), based on n/a steps of P\na, is denoted by γ̂⋆(a). Note that\nγ⋆(a) = 1− (1− γ⋆)a . Define Kγ⋆ := ⌊log2(1/γ⋆)⌋ and, for any δ ∈ (0, 1), δγ⋆ = δγ⋆(δ) := δ/(Kγ⋆ + 1).\nProposition 7.1. Fix ε ∈ (0, 0.01) and δ ∈ (0, 1). Let A be the random variable defined in the estimator of Theorem 3.4 (which depends on (Xt) n t=1). If\nn > n1(ε/ √ 2; δγ⋆ , γ⋆), then there is an event G(ε) having probability at least 1− δ, such that on G(ε),\n0.30 < γ⋆(A) < 0.54 if γ⋆ < 1/2 ,\nA = 1 if γ⋆ ≥ 1/2 .\nMoreover, on G(ε), the initial estimator γ̂⋆(A) applied to the chain (XAs) n/A s=1 satisfies\n|γ̂⋆(A) − γ⋆(A)| ≤ ε .(32)\nThe proof of Proposition 7.1 is based on the following lemma.\nLemma 7.2. Fix n ≥ n1(ε/ √ 2; δ, γ⋆). If aγ⋆ ≤ 1, then\nPr(|γ⋆(a)− γ̂⋆(a)| ≤ ε) > 1− δ. Proof. Recall the bound M(n; δ, γ⋆) on the right-hand side of Eq. (5). If γ⋆(a) ≥ γ⋆a/2, then\nM(n/a; δ, γ⋆(a)) ≤ √ 2M(n; aδ, γ⋆) ≤ √ 2M(n; δ, γ⋆) ≤ √ 2 · ε√\n2 = ε ,\nand the lemma follows from applying Theorem 3.3 to the P a-chain. We now show that γ⋆(a) ≥ γ⋆a/2. A Taylor expansion of (1 − γ⋆)a implies that there exists ξ ∈ [0, γ⋆] ⊆ [0, 1/a] such that\nγ⋆(a) = 1− (1− γ⋆)a = γ⋆a− a(a− 1)(1− ξ)a−2γ2⋆ 2 ≥ γ⋆a 2 .\n(We have used the hypothesis aγ⋆ ≤ 1 in the inequality.)\nProof of Proposition 7.1. Define the events G(a; ε) := {|γ⋆(a) − γ̂⋆(a)| ≤ ε}, and G = G(ε) := ⋂Kγ⋆ k=0 G(2\nk; ε). If k ≤ Kγ⋆ , then γ⋆2k ≤ γ⋆2log2(1/γ⋆) ≤ 1 and Lemma 7.2 implies that\nPr(Gc) ≤ Kγ⋆∑\nk=0\nPr(G(2k; ε)c) ≤ (Kγ⋆ + 1) · δ\nKγ⋆ + 1 = δ .\nOn G, if γ⋆ ≥ 1/2, then |γ̂⋆ − γ⋆| ≤ 0.01, and consequently γ̂⋆ ≥ 0.49 > 0.31. In this case, A = 1 on G.\nOn the event G, if the algorithm has not terminated by step k − 1, then the following hold:\n(1) If γ⋆(2 k) ≤ 0.30, then the algorithm does not terminate at step k. (2) If γ⋆(2 k) > 0.32, then the algorithm terminates at step k.\nAlso, assuming γ⋆ ≤ 1/2,\nγ⋆(2 Kγ⋆ ) ≥ 1− (1− γ⋆) 1 2γ⋆ ≥ 1− e−1/2 ≥ 0.39 ,\nso the algorithm always terminates before k = Kγ⋆ on G and thus (32) holds on G. Finally, on G, if A > 1, then γ⋆(A/2) ≤ 0.32, whence\nγ⋆(A) = 1− (1− γ⋆(A/2))2 ≤ 1− (0.68)2 < 0.54 . If γ⋆ < 1/2 and A = 1, then γ⋆(A) = γ⋆ ≤ 1/2.\nWe now prove Theorem 3.4.\nProof of Theorem 3.4. Let (33) n0(ε; δ, γ⋆, π⋆) = n0(ε) := L\nπ⋆γ⋆ε2 ,\nwhere (34) L := 3·(16 √ 2)2· ( log\nd(⌊log2(1/γ⋆)⌋+ 1) δ\n) · ( log 3 · (16 √ 2)2 · C2(⌊log2(1/γ⋆)⌋+ 1)\nε2π2⋆γ⋆δ\n) ,\nand C is the constant in Eq. (5).\nFix n > n0(ε) = n1(ε/(16 √ 2); δγ⋆ , γ⋆). Let A and G be as defined in Proposi-\ntion 7.1. Assume we are on the event G = G(ε/16) for the rest of this proof. Suppose first that γ⋆ < 1/2. We have 0.30 < γ⋆(A) < 0.54, and\n|γ̂⋆(A)− γ⋆(A)| ≤ ε\n16 < 0.01 ,\nso both γ⋆(A) and γ̂⋆(A) are in [0.29, 0.55], say. Let h(x) = 1−(1−x)1/A, so γ⋆ = h(γ⋆(A)) and γ̃⋆ = h(γ̂⋆(A)). Since (1−x)1/A ≤ 1− x/A, we have 1\n1− (1 − x)1/A ≤ A x .\nConsequently, on [0.29, 0.55], ∣∣∣∣ d\ndx log h(x) ∣∣∣∣ = 1 A (1− x)1/A−1 1− (1 − x)1/A ≤\n1 A(1 − x) A x =\n1 (1− x)x ≤ 1 (0.45)(0.29) < 8 .\nThus, | ddx log h(x)| is bounded (by 8) on [0.29, 0.55]. We have | log(h(γ̂⋆(A))/γ⋆)| = | log h(γ⋆(A))− log h(γ̂⋆(A))| ≤ 8|γ⋆(A)− γ̂⋆(A)| ≤ 8 ε\n16 ≤ ε 2 .\nThus, γ̃⋆ γ⋆ = h(γ̂⋆(A)) γ⋆ ≤ eε/2 ≤ 1 + ε . Similarly, γ⋆h(γ̂⋆(A)) ≤ e ε/2, so\nγ̃⋆ γ⋆ = h(γ̂⋆(A)) γ⋆ ≥ e−ε/2 ≥ 1− ε .\nNow instead suppose that γ⋆ ≥ 1/2. Then A = 1 on the event G, and\n|γ̃⋆ − γ⋆| < ε\n16 ,\nso ∣∣∣∣ γ̃⋆ γ⋆ − 1 ∣∣∣∣ < ε 16γ⋆ ≤ ε ."
    }, {
      "heading" : "8. Proof of Theorem 4.1",
      "text" : "In this section, we derive Algorithm 1 and prove Theorem 4.1.\n8.1. Estimators for π and γ⋆. The algorithm forms the estimator P̂ of P using Laplace smoothing:\nP̂i,j := Ni,j + α\nNi + dα\nwhere\nNi,j := |{t ∈ [n− 1] : (Xt, Xt+1) = (i, j)}| , Ni := |{t ∈ [n− 1] : Xt = i}| and α > 0 is a positive constant, which we set beforehand as α := 1/d for simplicity.\nAs a result of the smoothing, all entries of P̂ are positive, and hence P̂ is a transition probability matrix for an ergodic Markov chain. We let π̂ be the unique\nstationary distribution for P̂ . Using π̂, we form an estimator Sym(L̂) of L using:\nSym(L̂) := 1\n2 (L̂+ L̂\n⊤\n), L̂ := Diag(π̂)1/2P̂ Diag(π̂)−1/2.\nLet λ̂1 ≥ λ̂2 ≥ · · · ≥ λ̂d be the eigenvalues of Sym(L̂) (and in fact, we have 1 = λ̂1 > λ̂2 and λ̂d > −1). The algorithm estimates the spectral gap γ⋆ using\nγ̂⋆ := 1−max{λ̂2, |λ̂d|}.\n8.2. Empirical bounds for P . We make use of a simple corollary of Freedman’s inequality for martingales (Freedman 1975, Theorem 1.6).\nTheorem 8.1 (Freedman’s inequality). Let (Yt)t∈N be a bounded martingale difference sequence with respect to the filtration F0 ⊂ F1 ⊂ F2 ⊂ · · · ; assume for some b > 0, |Yt| ≤ b almost surely for all t ∈ N. Let Vk := ∑k t=1 E ( Y 2t |Ft−1 ) and Sk := ∑k t=1 Yt for k ∈ N. For all s, v > 0, Pr [∃k ∈ N s.t. Sk > s ∧ Vk ≤ v] ≤ ( v/b2\ns/b+ v/b2\n)s/b+v/b2 es/b = exp ( − v b2 · h ( bs v )) ,\nwhere h(u) := (1 + u) ln(1 + u)− u.\nObserve that in Theorem 8.1, for any x > 0, if s := √ 2vx+bx/3 and z := b2x/v,\nthen the probability bound on the right-hand side becomes\nexp ( −x · h (√ 2z + z/3 )\nz\n) ≤ e−x\nsince h( √ 2z + z/3)/z ≥ 1 for all z > 0 (see, e.g., Audibert, Munos, and Szepesvári (2009, proof of Lemma 5)).\nCorollary 8.2. Under the same setting as Theorem 8.1, for any n ≥ 1, x > 0, and c > 1,\nPr [ ∃k ∈ [n] s.t. Sk > √ 2cVkx+ 4bx/3 ] ≤ (1 + ⌈logc(2n/x)⌉+) e−x.\nProof. Define vi := c ib2x/2 for i = 0, 1, 2, . . . , ⌈logc(2n/x)⌉+, and let v−1 := −∞. Then, since Vk ∈ [0, b2n] for all k ∈ [n], Pr [ ∃k ∈ [n] s.t. Sk > √ 2max{v0, cVk}x+ bx/3 ]\n=\n⌈logc(2n/x)⌉+∑\ni=0\nPr [ ∃k ∈ [n] s.t. Sk > √ 2max{v0, cVk}x+ bx/3 ∧ vi−1 < Vk ≤ vi ]\n≤ ⌈logc(2n/x)⌉+∑\ni=0\nPr [ ∃k ∈ [n] s.t. Sk > √ 2max{v0, cvi−1}x+ bx/3 ∧ vi−1 < Vk ≤ vi ]\n≤ ⌈logc(2n/x)⌉+∑\ni=0\nPr [ ∃k ∈ [n] s.t. Sk > √ 2vix+ bx/3 ∧ Vk ≤ vi ]\n≤ (1 + ⌈logc(2n/x)⌉+) e−x , where the final inequality uses Theorem 8.1. The conclusion now follows because\n√ 2cVkx+ 4bx/3 ≥ √ 2max{v0, cVk}x+ bx/3\nfor all k ∈ [n].\nLemma 8.3. The following holds for any constant c > 1 with probability at least 1− δ: for all (i, j) ∈ [d]2, (35) |P̂i,j −Pi,j | ≤ √(\nNi Ni + dα\n) 2cPi,j(1− Pi,j)τn,δ\nNi + dα + (4/3)τn,δ Ni + dα + |α− dαPi,j | Ni + dα ,\nwhere (36)\nτn,δ := inf { t ≥ 0 : 2d2 (1 + ⌈logc(2n/t)⌉+) e−t ≤ δ } = O ( log ( d log(n)\nδ\n)) .\nProof. Let Ft be the σ-field generated by X1, X2, . . . , Xt. Fix a pair (i, j) ∈ [d]2. Let Y1 := 0, and for t ≥ 2, Yt := 1 {Xt−1 = i} (1 {Xt = j} − Pi,j), so that\nn∑\nt=1\nYt = Ni,j −NiPi,j .\nThe Markov property implies that the stochastic process (Yt)t∈[n] is an (Ft)-adapted martingale difference sequence: Yt is Ft-measurable and E (Yt|Ft−1) = 0, for each t. Moreover, for all t ∈ [n],\nYt ∈ [−Pi,j , 1− Pi,j ] , and for t ≥ 2,\nE ( Y 2t |Ft−1 ) = 1 {Xt−1 = i}Pi,j(1− Pi,j) .\nTherefore, by Corollary 8.2 and union bounds, we have\n|Ni,j −NiPi,j | ≤ √ 2cNiPi,j(1− Pi,j)τn,δ +\n4τn,δ 3\nfor all (i, j) ∈ [d]2. Equation (35) can be viewed as constraints on the possible value that Pi,j may have (with high probability). Since Pi,j is the only unobserved quantity in the bound from Eq. (35), we can numerically maximize |P̂i,j − Pi,j | subject to the constraint in Eq. (35) (viewing Pi,j as the optimization variable). Let B ∗ i,j be this maximum value, so we have\nPi,j ∈ [ P̂i,j −B∗i,j , P̂i,j +B∗i,j ]\nin the same event where Eq. (35) holds. In the algorithm, we give a simple alternative to computing B∗i,j that avoids numerical optimization, derived in the spirit of empirical Bernstein bounds (Audibert, Munos, and Szepesvári 2009). Specifically, with c := 1.1 (an arbitrary choice), we compute (37)\nB̂i,j :=   √\ncτn,δ 2Ni + √√√√cτn,δ 2Ni + √ 2cP̂i,j(1 − P̂i,j)τn,δ Ni + (4/3)τn,δ + |α− dαP̂i,j | Ni   2\nfor each (i, j) ∈ [d]2, where τn,δ is defined in Eq. (36). We show in Lemma 8.4 that\nPi,j ∈ [ P̂i,j − B̂i,j , P̂i,j + B̂i,j ]\nagain, in the same event where Eq. (35) holds. The observable bound in Eq. (37) is not too far from the unobservable bound in Eq. (35).\nLemma 8.4. In the same 1 − δ event as from Lemma 8.3, we have Pi,j ∈ [P̂i,j − B̂i,j , P̂i,j + B̂i,j ] for all (i, j) ∈ [d]2, where B̂i,j is defined in Eq. (37). Proof. Recall that in the 1 − δ probability event from Lemma 8.3, we have for all (i, j) ∈ [d]2,\n|P̂i,j − Pi,j | = ∣∣∣∣ Ni,j −NiPi,j\nNi + dα + α− dαPi,j Ni + dα\n∣∣∣∣\n≤ √\n2cNiPi,j(1 − Pi,j)τn,δ (Ni + dα)2 + (4/3)τn,δ Ni + dα + |α− dαPi,j | Ni + dα .\nApplying the triangle inequality to the right-hand side, we obtain\n|P̂i,j − Pi,j | ≤\n√ 2cNi(P̂i,j(1− P̂i,j) + |P̂i,j − Pi,j |)τn,δ\n(Ni + dα)2 + (4/3)τn,δ Ni + dα\n+ |α− dαP̂i,j |+ dα|P̂i,j − Pi,j |\nNi + dα .\nSince √ A+B ≤ √ A + √ B for non-negative A,B, we loosen the above inequality and rearrange it to obtain ( 1− dα\nNi + dα\n) |P̂i,j − Pi,j | ≤ √ |P̂i,j − Pi,j | · √ 2cNiτn,δ\n(Ni + dα)2\n+\n√ 2cNiP̂i,j(1 − P̂i,j)τn,δ\n(Ni + dα)2 + (4/3)τn,δ + |α− dαP̂i,j | Ni + dα .\nWhenever Ni > 0, we can solve a quadratic inequality to conclude |P̂i,j − Pi,j | ≤ B̂i,j .\n8.3. Empirical bounds for π. Recall that π̂ is obtained as the unique stationary distribution for P̂ . Let Â := I − P̂ , and let Â# be the group inverse of Â—i.e., the unique square matrix satisfying the following equalities:\nÂÂ#Â = Â, Â#ÂÂ# = Â#, Â#Â = ÂÂ#.\nThe matrix Â#, which is well defined no matter what transition probability matrix\nP̂ we start with (Meyer Jr. 1975), is a central quantity that captures many prop-\nerties of the ergodic Markov chain with transition matrix P̂ (Meyer Jr. 1975). We denote the (i, j)-th entry of Â# by Â#i,j . Define\nκ̂ := 1\n2 max\n{ Â\n# j,j −min\n{ Â\n# i,j : i ∈ [d]\n} : j ∈ [d] } .\nAnalogously define\nA := I − P , A# := group inverse of A,\nκ := 1\n2 max\n{ A\n# j,j −min\n{ A\n# i,j : i ∈ [d]\n} : j ∈ [d] } .\nWe now use the following perturbation bound from Cho and Meyer (2001, Section 3.3) (derived from Haviv and Van der Heyden (1984) and Kirkland, Neumann, and Shader (1998)).\nLemma 8.5 (Haviv and Van der Heyden 1984; Kirkland, Neumann, and Shader 1998). If |P̂i,j − Pi,j | ≤ B̂i,j for each (i, j) ∈ [d]2, then\nmax {|π̂i − πi| : i ∈ [d]} ≤ min{κ, κ̂}max{B̂i,j : (i, j) ∈ [d]2} ≤ κ̂max{B̂i,j : (i, j) ∈ [d]2}.\nThis establishes the validity of the confidence intervals for the πi in the same event from Lemma 8.3.\nWe now establish the validity of the bounds for the ratio quantities √ π̂i/πi and√\nπi/π̂i.\nLemma 8.6. If max{|π̂i − πi| : i ∈ [d]} ≤ b̂, then\nmax ⋃\ni∈[d]\n{| √ πi/π̂i − 1|, | √ π̂i/πi − 1|} ≤ 1\n2 max\n⋃\ni∈[d]\n{ b̂\nπ̂i ,\nb̂\n[π̂i − b̂]+\n} .\nProof. By Lemma 8.5, we have for each i ∈ [d], |π̂i − πi|\nπ̂i ≤ b̂ π̂i , |π̂i − πi| πi ≤ b̂ πi ≤ b̂ [π̂i − b̂]+ .\nTherefore, using the fact that for any x > 0,\nmax { | √ x− 1|, | √ 1/x− 1| } ≤ 1\n2 max {|x− 1|, |1/x− 1|}\nwe have for every i ∈ [d],\nmax { | √ πi/π̂i − 1|, | √ π̂i/πi − 1| } ≤ 1\n2 max {|πi/π̂i − 1|, |π̂i/πi − 1|}\n≤ 1 2 max\n{ b̂\nπ̂i ,\nb̂\n[π̂i − b̂]+\n} .\n8.4. Empirical bounds for L. By Weyl’s inequality and the triangle inequality,\nmax i∈[d]\n|λi − λ̂i| ≤ ‖L− Sym(L̂)‖ ≤ ‖L− L̂‖.\nIt is easy to show that |γ̂⋆ − γ⋆| is bounded by the same quantity. Therefore, it remains to establish an empirical bound on ‖L− L̂‖.\nLemma 8.7. If |P̂i,j − Pi,j | ≤ B̂i,j for each (i, j) ∈ [d]2 and max{|π̂i − πi| : i ∈ [d]} ≤ b̂, then\n‖L̂−L‖ ≤ 2ρ̂+ ρ̂2 + (1 + 2ρ̂+ ρ̂2) ( ∑\n(i,j)∈[d]2\nπ̂i π̂j B̂2i,j\n)1/2 ,\nwhere\nρ̂ := 1\n2 max\n⋃\ni∈[d]\n{ b̂\nπ̂i ,\nb̂\n[π̂i − b̂]+\n} .\nProof. We use the following decomposition of L− L̂: L− L̂ = EP + Eπ,1L̂+ L̂Eπ,2 + Eπ,1EP + EPEπ,2 + Eπ,1L̂Eπ,2 + Eπ,1EPEπ,2\nwhere\nEP := Diag(π̂) 1/2(P − P̂ )Diag(π̂)−1/2,\nEπ,1 := Diag(π) 1/2 Diag(π̂)−1/2 − I,\nEπ,2 := Diag(π̂) 1/2 Diag(π)−1/2 − I.\nTherefore\n‖L− L̂‖ ≤ ‖Eπ,1‖+ ‖Eπ,2‖+ ‖Eπ,1‖‖Eπ,2‖ + (1 + ‖Eπ,1‖+ ‖Eπ,2‖+ ‖Eπ,1‖‖Eπ,2‖) ‖EP ‖.\nObserve that for each (i, j) ∈ [d]2, the (i, j)-th entry of EP is bounded in absolute value by\n|(EP )i,j | = π̂1/2i π̂ −1/2 j |Pi,j − P̂i,j | ≤ π̂ 1/2 i π̂ −1/2 j B̂i,j .\nSince the spectral norm of EP is bounded above by its Frobenius norm,\n‖EP ‖ ≤ ( ∑\n(i,j)∈[d]2\n(EP ) 2 i,j\n)1/2 ≤ ( ∑\n(i,j)∈[d]2\nπi πj B̂2i,j\n)1/2 .\nFinally, the spectral norms of Eπ,1 and Eπ,2 satisfy\nmax {‖Eπ,1‖, ‖Eπ,2‖} = max ⋃\ni∈[d]\n{| √ πi/π̂i − 1|, | √ π̂i/πi − 1|},\nwhich can be bounded using Lemma 8.6.\nThis establishes the validity of the confidence interval for γ⋆ in the same event from Lemma 8.3.\n8.5. Asymptotic widths of intervals. Let us now turn to the asymptotic behavior of the interval widths (regarding b̂, ρ̂, and ŵ all as functions of n). A simple calculation gives that, almost surely, as n → ∞,\n√ n\nlog logn b̂ = O ( max i,j κ √ Pi,j πi ) ,\n√ n\nlog logn ρ̂ = O\n( κ\nπ 3/2 ⋆\n) .\nHere, we use the fact that κ̂ → κ as n → ∞ since Â# → A# as P̂ → P (Li and Wei 2001; Beńıtez and X. Liu 2012).\nFurther, since\n√ n\nlog logn\n(∑\ni,j\nπ̂i π̂j B̂2i,j\n)1/2 = O   (∑\ni,j\nπi πj · Pi,j(1− Pi,j) πi\n)1/2  = O (√ d\nπ⋆\n) ,\nwe thus have √\nn\nlog logn ŵ = O\n( κ\nπ 3/2 ⋆\n+\n√ d\nπ⋆\n) .\nThis completes the proof of Theorem 4.1.\nThe following lemma provides a bound on κ in terms of the number of states and the spectral gap.\nLemma 8.8. κ ≤ 1γ⋆ min{d, 8 + ln(4/π⋆)}\nBefore proving this, we prove a lemma of independent interest.\nLemma 8.9. Let τj be the first positive time that state j is visited by the Markov chain. Then (38) Eiτj ≤ 2 ( tmix + 8\ntrelax πj\n) .\nProof. By taking f to be the indicator of state j in Theorem 12.19 of Levin, Peres, and Wilmer (2009), for any i, if t = tmix + 8trelax/πj , then\nPri(τj > t) ≤ 1\n2 .\nThus, Pri(τj > tk) ≤ 2−k, whence Eq. (38) follows.\nProof of Lemma 8.8. It is established by Cho and Meyer (2001) that\nκ ≤ max i,j |A#i,j | ≤ sup ‖v‖1=1,〈v,1〉=0 ‖v⊤A#‖1\n(our κ is the κ4 quantity from Cho and Meyer (2001)), and Seneta (1993) establishes\nsup ‖v‖1=1,〈v,1〉=0\n‖v⊤A#‖1 ≤ d\nγ⋆ .\nSince it is shown in Cho and Meyer (2001) that\nκ = 1\n2 max j [ max i6=j Ei(τj) ] πj ,\nit follows from Lemma 8.9 that\nκ ≤ tmix + 8trelax ≤ trelax(8 + ln(4/π⋆)) ."
    }, {
      "heading" : "9. Proof of Theorem 4.2",
      "text" : "Let π̂⋆,lb and γ̂⋆,lb be the lower bounds on π⋆ and γ⋆, respectively, computed from Algorithm 1. Let π̂⋆ and γ̂⋆ be the estimates of π⋆ and γ⋆ computed using the estimators from Theorem 3.3. By a union bound, we have by Theorems 3.3 and 4.1 that with probability at least 1− 2δ,\n(39) |π̂⋆ − π⋆| ≤ C\n  √ π⋆ log d π̂⋆,lbδ\nγ̂⋆,lbn +\nlog dπ̂⋆,lbδ\nγ̂⋆,lbn\n \nand\n(40) |γ̂⋆ − γ⋆| ≤ C\n  √\nlog dδ · log nπ̂⋆,lbδ π̂⋆,lbγ̂⋆,lbn + log dδ · log nπ̂⋆,lbδ π̂⋆,lbγ̂⋆,lbn + log 1γ̂⋆,lb γ̂⋆,lbn\n  .\nThe bound on |γ̂⋆− γ⋆| in Eq. (40)—call it ŵ′—is fully observable and hence yields a confidence interval for γ⋆. The bound on |π̂⋆−π⋆| in Eq. (39) depends on π⋆, but from it one can derive\n|π̂⋆ − π⋆| ≤ C′   √ π̂⋆ log d π̂⋆,lbδ\nγ̂⋆,lbn +\nlog dπ̂⋆,lbδ\nγ̂⋆,lbn\n \nusing the approach from the proof of Lemma 8.4. Here, C′ > 0 is an absolute constant that depends only on C. This bound—call it b̂′—is now also fully observable. We have established that in the 1− 2δ probability event from above,\nπ⋆ ∈ Û := [π̂⋆ − b̂′, π̂⋆ + b̂′], γ⋆ ∈ V̂ := [γ̂⋆ − ŵ′, γ̂⋆ + ŵ′]. It is easy to see that almost surely (as n → ∞),\n√ n\nlogn ŵ′ = O\n(√ log(d/δ)\nπ⋆γ⋆\n)\nand\n√ nb̂′ = O   √ π⋆ log d π⋆δ\nγ⋆\n  .\nThis completes the proof of Theorem 4.2.\n10. Discussion\nThe construction used in Theorem 4.2 applies more generally: Given a confidence interval of the form In = In(γ⋆, π⋆, δ) for some confidence level δ and a confidence set En(δ) for (γ⋆, π⋆) for the same level, I ′ n = En(δ) ∩ ∪(γ,π)∈En(δ)In(γ, π, δ) is a valid 2δ-level confidence interval whose asymptotic width matches that of In up to lower order terms under reasonable assumptions on En and In. In particular, this suggests that future work should focus on closing the gap between the lower and upper bounds on the accuracy of point-estimation. The bootstrap estimator of Theorem 3.4 closes most of the gap when π is uniform. Another interesting direction is to reduce the computation cost: the current cubic cost in the number of states can be too high even when the number of states is only moderately large.\nPerhaps more important, however, is to extend our results to large state space Markov chains. In most practical applications the state space is continuous or is exponentially large in some natural parameters. To subvert our lower bounds, we must restrict attention to Markov chains with additional structure. Parametric classes, such as Markov chains with factored transition kernels with a few factors, are promising candidates for such future investigations. The results presented here are a first step in the ambitious research agenda outlined above, and we hope that they will serve as a point of departure for further insights on the topic of fully empirical estimation of Markov chain parameters based on a single sample path."
    } ],
    "references" : [ {
      "title" : "Rates of uniform convergence",
      "author" : [ "R.L. Karandikar", "M. Vidyasagar" ],
      "venue" : null,
      "citeRegEx" : "Karandikar and Vidyasagar,? \\Q2002\\E",
      "shortCiteRegEx" : "Karandikar and Vidyasagar",
      "year" : 2002
    }, {
      "title" : "An improvement on the perturbation of the group inverse",
      "author" : [ "ical Society", "Providence", "X. RI. Li", "Y. Wei" ],
      "venue" : null,
      "citeRegEx" : "Society et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Society et al\\.",
      "year" : 2001
    }, {
      "title" : "The Role of the Group Generalized Inverse in the Theory",
      "author" : [ "C.D. Meyer Jr." ],
      "venue" : "ficients”. In: AISTATS,",
      "citeRegEx" : "Jr.,? \\Q1975\\E",
      "shortCiteRegEx" : "Jr.",
      "year" : 1975
    }, {
      "title" : "Stability bounds for non-iid processes",
      "author" : [ "M. Springer. Mohri", "A. Rostamizadeh" ],
      "venue" : null,
      "citeRegEx" : "Mohri and Rostamizadeh,? \\Q2008\\E",
      "shortCiteRegEx" : "Mohri and Rostamizadeh",
      "year" : 2008
    }, {
      "title" : "Sensitivity of finite Markov chains under perturbation",
      "author" : [ "E. Seneta" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1993
    }, {
      "title" : "Fast Learning from Non-i.i.d",
      "author" : [ "I. Steinwart", "A. Christmann" ],
      "venue" : "Statistics & Probability Letters",
      "citeRegEx" : "Steinwart and Christmann,? \\Q2009\\E",
      "shortCiteRegEx" : "Steinwart and Christmann",
      "year" : 2009
    }, {
      "title" : "Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning)",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "A Bradford Book. isbn:",
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Liu 2001), but the problem also arises in performance prediction involving time-correlated data, as is common in reinforcement learning (Sutton and Barto 1998).",
      "startOffset" : 136,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent data (e.g., Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.",
      "startOffset" : 131,
      "endOffset" : 287
    }, {
      "referenceID" : 3,
      "context" : "Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent data (e.g., Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.",
      "startOffset" : 131,
      "endOffset" : 287
    }, {
      "referenceID" : 5,
      "context" : "Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent data (e.g., Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.",
      "startOffset" : 131,
      "endOffset" : 287
    }, {
      "referenceID" : 0,
      "context" : ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients.",
      "startOffset" : 11,
      "endOffset" : 471
    }, {
      "referenceID" : 0,
      "context" : ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a “reset” device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W.",
      "startOffset" : 11,
      "endOffset" : 1182
    }, {
      "referenceID" : 0,
      "context" : ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a “reset” device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015).",
      "startOffset" : 11,
      "endOffset" : 1239
    }, {
      "referenceID" : 0,
      "context" : ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a “reset” device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform.",
      "startOffset" : 11,
      "endOffset" : 1276
    }, {
      "referenceID" : 0,
      "context" : ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a “reset” device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform.",
      "startOffset" : 11,
      "endOffset" : 1336
    }, {
      "referenceID" : 0,
      "context" : ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a “reset” device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform. Finally, when one only has a circuit-based description of the transition probabilities of a Markov chain over an exponentially-large state space, there are complexity-theoretic barriers for many MCMC diagnostic problems (Bhatnagar, Bogdanov, and Mossel 2011). This paper is based on the conference paper of Hsu, Kontorovich, and Szepesvári (2015), combined with the results in the unpublished manuscript of Levin and Peres (2016).",
      "startOffset" : 11,
      "endOffset" : 1945
    }, {
      "referenceID" : 0,
      "context" : ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a “reset” device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform. Finally, when one only has a circuit-based description of the transition probabilities of a Markov chain over an exponentially-large state space, there are complexity-theoretic barriers for many MCMC diagnostic problems (Bhatnagar, Bogdanov, and Mossel 2011). This paper is based on the conference paper of Hsu, Kontorovich, and Szepesvári (2015), combined with the results in the unpublished manuscript of Levin and Peres (2016).",
      "startOffset" : 11,
      "endOffset" : 2028
    } ],
    "year" : 2017,
    "abstractText" : "The spectral gap γ⋆ of a finite, ergodic, and reversible Markov chain is an important parameter measuring the asymptotic rate of convergence. In applications, the transition matrix P may be unknown, yet one sample of the chain up to a fixed time n may be observed. We consider here the problem of estimating γ⋆ from this data. Let π be the stationary distribution of P , and π⋆ = minx π(x). We show that if n = Õ ( 1 γ⋆π⋆ ) , then γ can be estimated to within multiplicative constants with high probability. When π is uniform on d states, this matches (up to logarithmic correction) a lower bound of Ω̃ ( d γ⋆ ) steps required for precise estimation of γ⋆. Moreover, we provide the first procedure for computing a fully data-dependent interval, from a single finitelength trajectory of the chain, that traps the mixing time tmix of the chain at a prescribed confidence level. The interval does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time trelax = 1/γ⋆, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a 1/ √ n rate, where n is the length of the sample path.",
    "creator" : "Creator"
  }
}