{
  "name" : "1705.05067.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning Via Regularized Frequent Directions",
    "authors" : [ "Luo Luo", "Cheng Chen", "Zhihua Zhang", "Wu-Jun Li" ],
    "emails" : [ "zhzhang@math.pku.edu.cn", "liwujun@nju.edu.cn" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Online learning is a typical approach for making an algorithm scalable, which constructs a learner incrementally from a sequence of examples. The online Newton step algorithm is the online analogue of the Newton-Raphson method [1, 2, 3], which incorporates the gradient information in earlier iterations. Compared with first order methods, the online Newton step algorithm has logarithmical regret bound without requirement of strongly convex assumption on loss function. Also, it needs less iterations than first order methods for the same prediction error. However, the online Newton step algorithm require quadratical space and time with respect to the number of dimensions, which is expensive for large scale problems.\nIn a recent study, Luo et al. [4] proposed sketched online Newton (SON) algorithms to accelerate second order online learning methods. The authors discussed several sketching strategies [5]\nar X\niv :1\n70 5.\n05 06\n7v 1\nfor approximating the second order information, including random projection [6, 7, 8], frequent direction [9, 10] and Oja’s algorithm [11, 12]. The SON achieves regret bounds nearly as good as the standard online Newton algorithms and performs well in real applications.\nIn this paper, we improve the SON methods with a novel sketching methods that we call regularized frequent directions (RFD). The RFD is a variant of frequent direction (FD) [9, 10], but reduces half of the approximation error bound with almost the same computational cost. Rather than approximating the matrix with a low-rank structure, RFD introduces an additional regularization term. Note that Zhang [13] proposed matrix ridge approximation (MRA) to a positive semi-definite matrix by the similar idea with RFD. However, there are two main differences between RFD and MRA. First, RFD is designed to the case that data samples come sequentially, while MRA relies on the whole dataset. Hence RFD is more suitable to online learning. Second, MRA aims to minimize the approximation error with respect to the Frobenius norm while RFD tries to minimize the spectral norm of the approximation error. In general, the spectral norm error bounds is more meaningful than the Frobenius norm [14].\nIn contrast to existing online Newton algorithms which usually use a fixed regularization term for each update, RFD provides an adaptive way to compute this term. This strategy achieves comparable regret bound with existing sketched online Newton methods, but makes the algorithm less sensitive to the hyperparameter.\nThe remainder of the paper is organized as follows. Firstly, we review the background of second order online learning and its sketched variants. Then we propose our regularized frequent direction (RFD) method with applications in online learning and provide some related theoretical analysis. Finally, we demonstrate empirical comparisons with baselines on serval real-world datasets to show the superiority of our algorithms."
    }, {
      "heading" : "2. Online Learning by Sketching",
      "text" : "In this section, we first describe the setup of convex online learning and some classical algorithms. Then we introduce the connection between online learning and sketching second order methods."
    }, {
      "heading" : "2.1 Convex Online Learning",
      "text" : "Online learning is performed in a sequence of consecutive rounds [15]. We consider the problem of online optimization as follows. For a sequence of examples {x(t) ∈ Rd}, and convex smooth loss functions {ft : Kt → R} where the ft(w) , `t(w>x(t)) and the Kt ⊂ Rd are convex compact set, the learner must decide a predictor w(t) and suffers loss ft(w(t)) for the t-th round. The regret at round T is then defined as:\nRT (w ∗) = T∑ t=1 ft(w (t))− T∑ t=1 ft(w ∗),\nwhere w∗ = argminw∈K ∑T t=1 ft(w) and K = ⋂T t=1Kt.\nWe make the following assumptions on the loss functions [4].\nAssumption 1 The loss function `t satisfies |`′t(z)| ≤ L whenever |z| ≤ C, where L and C are positive constant scalars.\nAssumption 2 There exists a µt ≥ 0 such that for all u,w ∈ K, we have\nft(w) ≥ ft(u) +∇ft(u)>(w − u) + µt 2\n( ∇ft(u)>(w − u) )2 .\nNote that for bounded diameter of the domain and gradient, holding Assumption 2 only requires the exp-concave property of ft, which is more general than strong convexity [3].\nOne typical online learning algorithm is online gradient descent (OGD) [2, 16]. At round t+ 1, OGD exploits the following update rule:\nu(t+1) = w(t) − βtg(t), w(t+1) = argmin\nw∈Kt ‖w − u(t+1)‖,\nwhere g(t) = ∇ft(w(t)) and βt is the stepsize. The algorithm has linear computation cost and achieves O(L2H log T ) regret bound for the H-strongly convex loss.\nIn this paper, we are more interested in online Newton step algorithms [2, 4]. The notation H-norm for given positive definite matrix H ∈ Rd×d and vector z ∈ Rd is defined as ‖z‖H =√\nz>Hz. The standard online Newton step keeps the curvature information in the matrix H(t) ∈ Rd×d sequentially and iterates as follows:\nu(t+1) = w(t) − βt(H(t))−1g(t), w(t+1) = argmin\nw∈Kt ‖w − u(t+1)‖H(t) . (1)\nThe matrix H(t) is constructed by the outer product of historical gradients [4, 17]:\nH(t) = α0I + t∑ i=1 g(i)(g(i))>, or H(t) = α0I + t∑ i=1 (µt + ηt)g (i)(g(i))>, (2)\nwhere α0 ≥ 0 is a fixed regularization parameter, µt is the constant in Assumption 2 and ηt is a learning rate which is typical chosen by O(1/t). The second order algorithms enjoy logarithmical regret bound without the strongly convex assumption but require quadratical space and computation cost."
    }, {
      "heading" : "2.2 Efficient Algorithms by Sketching",
      "text" : "To make the online Newton step scalable, it is natural to use sketching techniques [5]. Since the matrix H(t) in online learning has the form H(t) = α0I + (A(t))>A(t), where A(t) ∈ Rt×d is the corresponding term in (2) such as\nA(t) = [g(1), . . . ,g(t)]>, or A(t) = [ √ µ1 + η1g (1), . . . , √ µt + ηtg (t)]>.\nThe sketching algorithm employs an approximation of A(t) by B(t) ∈ Rm×d where m d. Then we can use α0I + (B(t))>B(t) to replace H(t) in update (1). By the Woodbury identity formula, we can reduce the computation of the update from O(d2) to O(m2d). There are several choices of sketching techniques, such as random projection [6, 7, 8], frequent direction [9, 10] and Oja’s algorithm [11, 12]. Luo et al. [4] discussed how to improve online learning by these\ntechniques. In practice, the performance of sketched online Newton methods is sensitive to the choice of hyperparamter α0 and they only achieve good result when α0 is set appropriately.\nWe now give a brief review of frequent directions (FD) [9, 10], because it is closely related to our proposed method. Frequent Directions is a deterministic matrix sketching in the row-updates model. For any input matrix A ∈ RT×d which comes from sequentially row by row, it maintains a sketch matrix B ∈ Rm×d to approximate A>A by B>B. FD has following properties for any k < m,\nA>A−B>B 0, (3) ‖A>A−B>B‖2 ≤ T−1∑ i=1 (σ(t)m ) 2 ≤ 1 m− k ‖A−Ak‖2F , (4)\nwhere ‖ · ‖2 denotes the spectral norm of the matrix and Ak is the best rank-k approximation to A in both the Frobenius and spectral norms.\nWe present the detailed implementation of FD in Algorithm 1, where σ(t−1)m is the m-th largest singular values of B(t−1). The dominated computation of the algorithm is computation of svd(B(t)) which requires O(m2d) by the standard SVD implementation, while one can reduce the total cost from O(Tm2d) into O(Tmd) by doubling the space [5, 9] as in Algorithm 2.\nAlgorithm 1 Frequent Directions 1: Input: A = [a(1), . . . ,a(T )]> ∈ RT×d, B(0) = 0m×d\n2: for t = 1, . . . , T do 3: Insert (a(t))> into the m-th row of B(t−1) 4: [U(t−1),Σ(t−1),V(t−1)] = svd(B(t−1))\n5: B(t) = √( Σ(t−1) )2 − (σ(t−1)m )2I · (V(t−1))> 6: end for 7: Output: B = B(T )"
    }, {
      "heading" : "3. Regularized Frequent Directions",
      "text" : "Sketching usually leads to a low-rank matrix. To make the matrix H(t) invertible and well-conditioned, both the standard online Newton and sketched online Newton methods usually require a fixed regularization term α0I. The hyperparameter α0 typically needs to be tuned manually. On the other hand, the conventional sketching algorithms such as frequent direction only consider the gradient part (A(t))>A(t) in H(t) and do not cover the regularization term. Intuitively, it is better to increase the factor of the identity matrix as iteration goes because there are more and more rows of A(t). These motivate us to propose a new sketching algorithm that is more suitable to online learning."
    }, {
      "heading" : "3.1 The Algorithm",
      "text" : "The regularized frequent direction (RFD) is a variant of frequent direction. For a given matrix A ∈ RT×d whose rows come from sequentially and a fixed scalar α0 ∈ R, RFD computes the sketch matrix B ∈ Rm×d and α ∈ R (m d) to approximate α0I + A>A by αI + B>B.\nAlgorithm 2 Fast Frequent Directions 1: Input: A = [a(1), . . . ,a(T )]> ∈ RT×d, B(0) = 02m×d\n2: for t = 1, . . . , T do 3: Insert (a(t))> into a zero-valued row of B(t−1) 4: if B(t−1) has no zero-valued row 5: [U(t−1),Σ(t−1),V(t−1)] = svd(B(t−1))\n6: B(t) = √( Σ(t−1) )2 − (σ(t−1)m )2I · (V(t−1))> 7: else 8: B(t) = B(t−1)\n9: end if 10: end for 11: Output: B = B(T )\nAlgorithm 3 Regularized Frequent Directions 1: Input: A = [a(1), . . . ,a(T )]> ∈ RT×d, B(0) = 0m×d, α(0) = α0 ∈ R 2: for i = 1, . . . , T do 3: Insert (a(t))> into a zero-valued row of B(t−1)\n4: [U(t−1),Σ(t−1),V(t−1)] = svd(B(t−1))\n5: B(t) = √( Σ(t−1) )2 − (σ(t−1)m )2I · (V(t−1))> 6: α(t) = α(t−1) + (σ (t−1) m )2 /2\n7: end for 8: Output: B = B(T ) and α = α(T ).\nWe demonstrate the detailed implementation of RFD in Algorithm 3. Compared with the standard FD, RFD only maintains an extra variable α(t) by scalar operations for each iteration, hence the cost of RFD is almost the same as FD. In real applications, α(t) is typically increasing as iteration from the (m + 1)-th round, which makes αI + B>B positive definite even the initial α(0) is zero. Also, we can further accelerate it by doubling the space as shown in Algorithm 4."
    }, {
      "heading" : "3.2 Theoretical Analysis",
      "text" : "We now explain why the regularization term is updated by the rule of Algorithm 3 and provide the approximation error bound of RFD. Firstly, we give the following theorem about matrix approximation.\nTheorem 1 Given a positive semi-definite matrix M ∈ Rd×d and a positive integer k < d, let M = UΣU> be the SVD of M. Let Uk denote the matrix of the first k columns of U and σk be the top k-th singular value of M. Then the pair (Ĉ, δ̂), defined as\nĈ = Uk(Σk − δ̂I)1/2V and δ̂ = (σk+1 + σd)/2\nAlgorithm 4 Fast Regularized Frequent Directions 1: Input: A = [a(1), . . . ,a(T )]> ∈ RT×d, B(0) = 02m×d, α(0) = α0 ∈ R 2: for t = 1, . . . , T do 3: Insert (a(t))> into a zero-valued row of B(t−1)\n4: if B(t−1) has no zero-valued row 5: [U(t−1),Σ(t−1),V(t−1)] = svd(B(t−1))\n6: B(t) = √( Σ(t−1) )2 − (σ(t−1)m )2I · (V(t−1))> 7: α(t) = α(t−1) + (σ (t−1) m )2 /2\n8: else 9: B(t) = B(t−1)\n10: end if 11: end for 12: Output: B = B(T )\nwhere V is an arbitrary k × k orthonormal matrix, is the global minimizer of\nmin C∈Rd×k,δ∈R\n‖M−CC> − δI‖2.\nTheorem 1 provides the closed form optimal solution for matrix approximation with regularization term. Zhang [13] has established the Frobenius norm based result in optimization view, while our analysis relies on the properties of unitary invariant norms. Moreover, our derivation is more concise. Addiotionally, we prove the solution is global optimal while Zhang’s analysis is local (see Appendix A).\nAt the t-th round, our goal is to approximate the concentration of historical approximation and current data. The following theorem shows that our update is optimal with respect to the spectral norm.\nTheorem 2 Based on the updates in Algorithm 3 and letting B(t) refer to the matrix obtained by Line 5 of Algorithm 3 at the t-th iteration (without inserting a(t) in the next iteration), and B̃(t) be the matrix consists of the first m− 1 rows of B(t), we have\n(B̃(t), α(t)) = argmin B̃∈Rd×(m−1),\nα∈R\n∥∥α(t−1)I + (B(t−1))>B(t−1) − B̃>B̃− αI∥∥ 2 . (5)\nTheorem 2 provides a natural explanation to RFD, making the algorithm reasonable intuitively, while the standard FD sketching is only an extension of approximating item frequencies in streams [18] but lacks optimization view in the matrix case.\nRFD also enjoys a tighter approximation error shown in the following theorem.\nTheorem 3 For any k < m and using the notation of Algorithm 3, we have∥∥A>A− (B>B + αI)∥∥ 2 ≤ 1 2(m− k) ‖A−Ak‖2F , (6)\nwhere Ak is the best rank-k approximation to A in both the Frobenius and spectral norm.\nThe right-hand side of inequality (6) is the half of the one in (4), which means RFD reduces the approximation error significantly with only one extra scalar.\nThe approximation Hessian of sketched online Newton step typically has a regularization term. Hence we are more interested in approximating the matrix that can be expressed as M = α0I + A>A where α0 > 0. Suppose that the standard FD approximates A>A by B>B. Then it estimates M as MFD = α0I + B>B, but RFD uses MRFD = αI + B>B. Theorem 4 shows that MRFD is well-conditioned no worse than MFD and M. In general, the equality in the theorem usually can not be hold for t > m unless (a(t))> lies in the row space of B(t−1) exactly or the first t rows of A have perfect low rank structure, which means that RFD results in a well-conditioned approximation more than others in practice.\nTheorem 4 With the notation of Algorithms 1 and 3, let M = α0I + A>A, MFD = α0I + B>B and MRFD = αI + B>B, where α0 > 0 is a fixed scalar. Define the condition number of any nonsingular matrix H as κ(H) = σmax(H)/σmin(H), where σmax(H) and σmin(H) are the largest and smallest singular values of H respectively. Then we have κ(MRFD) ≤ κ(MFD) and κ(MRFD) ≤ κ(M)."
    }, {
      "heading" : "4. The Online Newton Step by RFD",
      "text" : "We now present the online Newton step by Regularized frequent directions (RFD-SON). The procedure is shown in Algorithm 5, which is similar to sketched online Newton step (SON) [4] but with the new sketching method RFD.\nAlgorithm 5 RFD for Online Newton Step 1: Input: α(0) = α0, m < d, ηt = O(1/t) and B(0) = 0m×d. 2: for t = 1, . . . , T do\n3: Receive example x(t), and loss function ft(w)\n4: g(t) = ∇ft(w(t)) 5: Insert ( √ µt + ηtg (t))> into the m-th row of B(t−1) 6: [U(t−1),Σ(t−1),V(t−1)] = svd(B(t−1))\n7: B(t) = √( Σ(t−1) )2 − (σ(t−1)m )2I · (V(t−1))> 8: α(t) = α(t−1) + (σ (t−1) m )2 /2\n9: H(t) = α(t)I + (B(t))>B(t)\n10: u(t+1) = w(t) − (H(t))−1g(t)\n11: w(t+1) = argminw∈Kt ‖w − u (t+1)‖H(t) 12: end for\nTheorem 5 Let µ = maxTt=1{µt} and K = ⋂T t=1Kt. Then under Assumptions 1 and 2 for any w ∈ K, Algorithm 5 has the following regret\nRT (w) ≤ α0 2 ‖w‖2 + 2(CL)2 T∑ t=1 ηt + m 2(µ+ ηT ) ln ( tr ( (B(T ))>B(T ) ) + α(T ) α0 ) + ΩRFD (7)\nwhere\nΩRFD = d−m 2(µ+ ηT ) ln α(T ) α0 +\nm\n4(µ+ ηT ) T∑ t=1 (σ (t) m )2 α(t) + C2 T∑ t=1 (σ(t−1)m ) 2.\nWe present the regret bound of RFD-SON in Theorem 5. The last term in (7) is the main gap between RFD-SON and the standard online Newton step without sketching, and the terms are logarithmic to T . ΩRFD is dominated by the last term which can be bounded as (4). If we exploit the standard FD to sketched online Newton step [4] (RFD-SON), the gap will be\nΩFD = m\n2(µ+ ηT ) T∑ t=1 (σ (t−1) m )2 α0 ,\nwhich looks comparable to our result because it is also dependent on ∑T\nt=1(σ (t−1) m )2. However, The\nbound of FD-SON is highly related to the hyperparameter α0. If we increase the value of α0, the gap ΩFD can be reduced, but the term α02 ‖w‖\n2 will increase. For RFD-SON, we can set α0 be sufficient small to reduce α02 ‖w‖ 2 and it affects the term ΩRFD limited. The reason is that the first term of ΩRFD contains 1α0 in the logarithmic term and the second term contains α (t) = α0+ 1 2 ∑t−1 i=1 (σ (i) m )2\nin the denominator. For large t, α(t) is mainly dependent on ∑t−1\ni=1 (σ (i) m )2, rather than α0."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section, we evaluate the performance of regularized frequent directions (RFD) and online Newton step by RFD (RFD-SON) on three real-world datasets “a9a”, “gisette” and “sido0” whose details are listed in Table 1. The “sido0” dataset comes from Causality Workbench1 and the others can be downloaded from LIBSVM repository2. The experiments are conducted in Matlab and run on a server with Intel (R) Core (TM) i7-3770 CPU 3.40GHz×2, 8GB RAM and Windows Server 2012 64-bit system."
    }, {
      "heading" : "5.1 Matrix Approximation",
      "text" : "We first compare the approximation error of FD and RFD. For a given dataset A ∈ Rn×d of n samples with d features, we use FD (Algorithm 2) and RFD (Algorithm 4) to approximate the covariance matrix A>A by B>B and αI + B>B respectively; that is,\nError-FD = ‖A>A−B>B‖2 ‖A>A‖2 , and Error-RFD = ‖A>A−B>B− αI‖2 ‖A>A‖2 .\nThe regularization term α0 is not important for this evaluation because it does not change the absolute error in the numerator of Error-FD or Error-RFD. We report the relative spectral norm error by varying the sketch size m. Figure 1 shows the performance of the two Algorithms. The relative error of RFD is always lower than the one of FD and the decrement is nearly the half in most cases. The results match the theoretical analysis (Theorem 3) very well. We do not include the comparison of the running time because they are almost the same."
    }, {
      "heading" : "5.2 Online Learning",
      "text" : "We now evaluate the performance of RFD-SON. We use the least squares loss (that is, ft(w) = (w>x(t) − y(t))2), and set Kt = {w : |w>x(t)| ≤ 1}. In the experiments, we use the the doubling space strategy on RFD sketching, which is shown in Algorithm 6.\nBy the Woodbury formula, the parameter w(t) can be updated in O(md) cost as follows\nu(t+1) = w(t) − 1 α(t)\n( g(t) − (B(t))>H(t)B(t)g(t) ) ,\nw(t+1) = u(t+1) − γ(t) ( x(t) − (B(t))>H(t)B(t)x(t) ) ,\nwhere γ(t) = τ ( (u(t))>x(t) ) (x(t))>x(t) − (x(t))>(B(t))>H(t)B(t)x(t) and τ(z) = sgn(z) max{|z| − 1, 0}.\nThe detailed derivation of the projection step can be found in Luo et al. [4]. We use 70% data for training and the rest for test. The algorithms in the experiments include ADAGRAD; standard online Newton step with the full Hessian [17] (FULL-ON); sketched online Newton step with frequent directions (FD-SON), random projections (RP-SON), Oja’s algorithms\nAlgorithm 6 Fast RFD for Online Newton Step 1: Input: α(0) = α0, m < d, ηt = O(1/t) and B(0) = 0m×d. 2: for t = 1, . . . , T do\n3: Receive example x(t), and loss function ft(w)\n4: g(t) = ∇ft(w(t)) 5: Insert ( √ µt + ηtg (t))> into the m-th row of B(t−1) 6: if B(t−1) has no zero valued rows 7: [U(t−1),Σ(t−1),V(t−1)] = svd(B(t−1))\n8: B(t) = √( Σ(t−1) )2 − (σ(t−1)m )2I · (V(t−1))> 9: α(t) = α(t−1) + (σ (t−1) m )2 /2\n10: else 11: B(t) = B(t−1)\n12: end if 13: H(t) = α(t)I + (B(t))>B(t)\n14: u(t+1) = w(t) − (H(t))−1g(t)\n15: w(t+1) = argminw∈Kt ‖w − u (t+1)‖H(t) 16: end for\n(Oja-SON) [4], and our proposed sketched online Newton step with regularized frequent directions (RFD-SON). The hyperparameter α0 is tuned from {10−10, 10−9 . . . 109, 1010} and the sketch size for FD-SON, RP-SON, Oja-SON and RFD-SON is chosen from {5, 10, 20}.\nTable 2 reports the accuracy for all the above algorithms at one epoch with the best α0, and Table 3 includes the corresponding running times. ADAGRAD has apparently less accuracy than the second order methods although it is the fastest. The SON methods are more efficient than FULL-ON as we expected. The costs of FD-SON and RFD-SON are not identical here because the approximation methods will generate different g(t) at each step which makes the matrices to be estimated not same. The accuracy of RFD-SON withm = 20 is (one of) the best on all the datasets.\nWe are also interested in how the hyperparameter α0 affects the performance of the algorithm. Figures 2, 3 and 4 show the results. We display the results of baselines when α0 is near to the best one for all the baseline algorithms. Note that only FULL-ON is robust to the hyperparameter on “a9a” and “sido0”. In most cases, the baselines are sensitive to the choice of the hyperparameter. But the proposed RFD-SON is very robust to the hyperparameter. We demonstrate the behaviors of RFD-SON with a large range of α0. The algorithm typically achieves comparable good performance when α0 is not too large (the accuracy of all the second order methods will decrease apparently when α0 reaches 104). Specifically, RFD-SON with α0 = 10−10 has almost the same performance as the best choice of the hyperparameter on “a9a” and “sido0”. RFD-SON is even more stable than the standard online Newton step algorithm. The reason is that the estimate of the Hessian in our method is typically more well-conditioned."
    }, {
      "heading" : "6. Conclusions",
      "text" : "In this paper, we have studied the second order online learning algorithms. We have proposed a novel sketching method regularized frequent directions (RFD) with several nice properties. Both the theoretical analysis and experiments show RFD is much better than FD. The online learning algorithm with RFD achieves better accuracy than baselines and more robust to the regularization parameters. The application of RFD is not limited to convex online optimization. We will try to exploit RFD to improve other optimziation algorithms including stochastic or non-convex cases."
    }, {
      "heading" : "Appendix A: The Proof of Theorem 1",
      "text" : "In this section, we firstly provide serval lemmas from the book “Topics in matrix analysis” [22], then we prove Theorem 1. The proof of Lemma 1 and 2 can be found in the book and we give the proof of Lemma 3 here.\nLemma 1 (Theorem 3.4.5 of [22]) Let A,B ∈ Rm×n be given, and suppose A, B and A − B have decreasingly ordered singular values, σ1(A) ≥ · · · ≥ σq(A), σ1(B) ≥ · · · ≥ σq(B), and σ1(A − B) ≥ · · · ≥ σq(A − B), where q = min{m,n}. Define si(A,B) ≡ |σi(A) − σi(B)|, i = 1, . . . , q and let s[1](A,B) ≥ · · · ≥ s[i](A,B) denote a decreasingly ordered rearrangement of the values si(A,B). Then\nk∑ i=1 s[i](A,B) ≤ k∑ i=1 σi(A−B) for k = 1, . . . , q.\nLemma 2 (Corollary 3.5.9 of [22]) Let A,B ∈ Rm×n be given, and let q = min{m,n}. The following are equivalent\n1. |||A||| ≤ |||B||| for every unitarily invariant norm ||| · ||| on Rm×n.\n2. Nk(A) ≤ Nk(B) for k = 1, . . . , q whereNk(X) ≡ ∑k i=1 σk(X) denotes by Ky Fan k-norm.\nLemma 3 (Page 215 of [22]) Let A,B ∈ Rm×n be given, and let q = min{m,n}. Define the diagonal matrix Σ(A) = [σij ] ∈ Rm×n by σii = σi(A), all other σij = 0, where σ1(A) ≥, . . . ,≥ σq(A) are the decreasingly ordered singular values of A. We define Σ(B) similarly. Then we have |||A−B||| ≥ |||Σ(A)−Σ(B)||| for every unitarily invariant norm ||| · |||.\nProof Using the notation of Lemma 1 and 2, matrices A−B and Σ(A)−Σ(B) have the decreasingly ordered singular values σ1(A−B) ≥ · · · ≥ σq(A−B) and s[1](A,B) ≥ · · · ≥ s[q](A,B). Then we have\nNk(A−B) = k∑ i=1 σi(A−B) ≥ k∑ i=1 s[i](A,B) = Nk(Σ(A)−Σ(B)), (8)\nwhere the inequality is obtain by Lemma 1. The Lemma 2 implies (8) is equivalent to |||A−B||| ≥ |||Σ(A)−Σ(B)||| for every unitarily invariant norm ||| · |||.\nThen we give the proof of Theorem 1. Proof Using the notation of above lemmas, we can bound the objective function as follow. For any C ∈ Rd×k and δ ∈ R, we have ∥∥M−CC> − δI∥∥\n2 ≥ ∥∥Σ(M)−Σ(CC> + δI)∥∥\n2\n= max i∈{1,...,d} ∣∣σi(M)− σi(CC>)− δ∣∣ ≥ max i∈{k+1,...,d}\n∣∣σi(M)− σi(CC>)− δ∣∣ = max i∈{k+1,...,d}\n∣∣σi(M)− δ∣∣ ≥ max i∈{k+1,...,d}\n∣∣σi(M)− δ̂∣∣. The first inequality is obtained by Lemma 3 since the spectral norm is unitarily invariant, and the second inequality is the property of maximization operator. The last inequality can be checked easily by the property of medium and the equivalence of SVD and eigenvector decomposition for positive semi-definite matrix.\nThe first equality is based on the definition of spectral norm. The second equality holds due to the fact rank(CC>) ≤ k which leads σi(CC>) = 0 for any i > k.\nNote that all above equalities occurs for C = Ĉ and δ = δ̂. Hence we prove the result of this theorem.\nWe also demonstrate the similar result with respect to Frobenius norm in Corollary 1. This analysis includes the global optimality of the problem, while Zhang [13]’s analysis only prove the solution is local optimal. Additionally, our proof is more concise.\nCorollary 1 Using the same notation of Theorem 1, we have the pair (C̃, δ̃) defined as\nC̃ = Uk(Σk − δ̃I)1/2V and δ̃ = 1\nd− k d∑ i=j+1 σi\nis the global minimizer of\nmin C∈Rd×k,δ∈R\n‖M−CC> − δI‖2F ,\nwhere V is an arbitrary k × k orthogonal matrix.\nProof We have the result similarly to Theorem 1.\n‖M−CC> − δI‖2F\n≥‖Σ(M)−Σ(CC> + δI)‖2F = d∑ i=1 (σi(M)− σi(CC>)− δ)2 ≥ d∑\ni=k+1\n(σi(M)− σi(CC>)− δ)2\n= d∑ i=k+1 (σi(M)− δ)2 ≥ d∑\ni=k+1\n(σi(M)− δ̃)2\nThe first four steps are similar to the ones of Theorem 1, but replace the spectral norm and absolute operator with Frobenius norm and square function . The last step comes from the property of the mean value.\nWe can check that all above equalities occurs for C = C̃ and δ = δ̃, which completes the proof."
    }, {
      "heading" : "Appendix B: The Proof of Theorem 2",
      "text" : "Proof The Algorithm 3 implies the singular values of α(t−1)I + (B(t−1))>B(t−1) are\n(σ (t) 1 ) 2 + α(t−1) ≥ · · · ≥ (σ(t)m )2 + α(t−1) ≥ α(t−1) = · · · = α(t−1).\nThen we can use the Theorem 1 by taking\nM =α(t−1)I + (B(t−1))>B(t−1),\nĈ =(V(t−1))> √( Σ(t−1) )2 − (σ(t−1)m )2I = (B(t))>,\nδ̂ =[(σ(t−1)m ) 2 + α(t−1) + α(t−1)]/2 = α(t).\nDue to the last column of B(t) is zero and (B̃(t))>B̃ = (B(t))>B(t), we have (B̃(t), α(t)) is the minimizer of the problem."
    }, {
      "heading" : "Appendix C: The Proof of Theorem 3",
      "text" : "Proof In this proof, the notation B(t) refer to the matrix obtain by the line 5 of Algorithm 3 at t-th iteration, and B(t) is the same one for (t − 1)-th iteration (without inserting a(t)). In other words, we have V(t−1)(Σ(t−1))2V(t−1) = (B(t−1))>B(t−1) +a(t)(a(t))>. Then we can deserve the lower bound as follow∥∥A>A− (B>B + λI)∥∥\n2\n= ∥∥∥∥∥ T∑ t=1 [ (a(i))>a(t) − (B(t))>B(t) + (B(t−1))>B(t−1) − 1 2 (σ(t−1)m ) 2I ]∥∥∥∥∥\n2 ≤ T∑ t=1 ∥∥∥(a(t))>a(t) − (B(t))>B(t) + (B(t−1))>B(t−1) − 1 2 (σ(t−1)m ) 2I ∥∥∥ 2\n= T∑ t=1 ∥∥∥V(t−1)(Σ(t−1))2(V(t−1))> −V(t−1)[(Σ(t−1))2 − (σ(t−1)m )2I](V(t−1))> − 12(σ(t−1)m )2I∥∥∥2 =\nT∑ t=1 ∥∥∥(σ(t−1)m )2V(t−1)(V(t−1))> − 12(σ(t−1)m )2I∥∥∥2 = 1\n2 T∑ t=1 (σ(t)m ) 2 ≤ 1 2(m− k) ‖A−Ak‖2F .\nThe first three equalities are direct from the procedure of the algorithm, and the last one is due to V(t−1) is column orthonormal. The first inequality comes from the triangle inequality of spectral norm. The last one can be obtain by the properties of (4)."
    }, {
      "heading" : "Appendix D: The Proof of Theorem 4",
      "text" : "Proof We can compare κ(MRFD) and κ(MFD) by the fact α ≥ α0 as follow\nκ(MRFD) = σmax(B\n>B) + α\nα ≤ σmax(B >B) + α0 α0 = κ(MFD).\nThe other inequality can be derived as\nκ(MRFD) = σmax(B\n>B) + α\nα ≤ σmax(A\n>A) + α\nα ≤ σmax(A >A) + α0 α0 = κ(M),\nwhere the first inequality comes from (3) and the others are easy to obtain."
    }, {
      "heading" : "Appendix E: The Proof of Theorem 5",
      "text" : "Proof Suppose that V(t)⊥ is the orthogonal complement of V (t)’s column space, that is V(t)(V(t))>+ V (t) ⊥ (V (t) ⊥ ) > = I, then we have\nH(t) −H(t−1)\n=α(t)I + (B(t))>B(t) − α(t−1)I− (B(t−1))>B(t−1)\n= 1\n2 (σ(t−1)m ) 2I− (σ(t−1)m )2V(t−1)(V(t−1))> + (µt + ηt)g(t)(g(t))>\n= 1\n2 (σ(t−1)m )\n2 [ V\n(t−1) ⊥ (V (t−1) ⊥ )\n> −V(t−1)(V(t−1))> ] + (µt + ηt)g (t)(g(t))>. (9)\nBy the proof of Theorem 2 in [4], we have\n2RT (w) ≤ α0‖w‖2 +RG +RD,\nwhere\nRG = T∑ t=1 (g(t))>(H(t))−1g(t),\nand\nRD = T∑ t=1 (w(t) −w)>[H(t) −H(t−1) − µtg(t)(g(t))>](w(t) −w).\nWe can bound RG as follow\nT∑ t=1 (g(t))>(H(t))−1g(t)\n= T∑ t=1 〈 (H(t))−1,g(t)(g(t))> 〉 =\nT∑ t=1\n1\nµt + ηt\n〈 (H(t))−1,H(t) −H(t−1) 1\n2 (σ(t−1)m ) 2[V (t−1) ⊥ (V (t−1) ⊥ )\n> −V(t−1)(V(t−1))> ]〉\n≤ 1 µ+ ηT T∑ t=1 〈 (H(t))−1,H(t) −H(t−1) + 1 2 (σ(t−1)m ) 2V(t−1)(V(t−1))> 〉\n= 1\nµ+ ηT T∑ t=1 [〈 (H(t))−1,H(t) −H(t−1) 〉 + 1 2 (σ(t−1)m ) 2tr ( V(t−1)(H(t))−1(V(t−1))> )] .\nThe above equalities come from the properties of trace operator and (9) and the inequality is due to ηt is increasing.\nThe term ∑T\nt=1〈(H(t))−1,H(t) −H(t−1)〉 can be bound as\nT∑ t=1 〈(H(t))−1,H(t) −H(t−1)〉\n≤ T∑ t=1 ln det(H(t)) det(H(t−1)) = ln det(H(T )) det(H(0))\n= ln\n∏d i=1 σi(H (T ))\nα0 = d∑ i=1 ln σi ( (B(T ))>B(T ) ) + α(T ) α0\n= m∑ i=1 ln σi ( (B(T ))>B(T ) ) + α(T ) α0 + (d−m) ln α (T ) α0\n≤m ln ∑m i=1[σi ( (B(T ))>B(T ) ) + α(T )]\nmα0 + (d−m) ln α\n(T )\nα0 =m ln ( tr ( (B(T ))>B(T ) ) + α(T )\nα0\n) + (d−m) ln α (T )\nα0 .\nThe first inequality is obtain by the concavity of the log determinant function, the second inequality comes from the Jensen’s inequality and the other steps is based on the procedure of the algorithm.\nThe other one 12 ∑T t=1(σ (t) m )2tr ( V(t)(H(t))−1(V(t))> ) can be bounded as\n1\n2 T∑ t=1 (σ(t)m ) 2tr ( V(t)(H(t))−1(V(t))> ) ≤1\n2 T∑ t=1 (σ (t) m )2 α(t) tr ( V(t)(V(t))> ) = m\n2 T∑ t=1 (σ (t) m )2 α(t) . (10)\nHence, we have\nRG ≤ 1\nµ+ ηT\n[ m ln ( tr ( (B(T ))>B(T ) ) + α(T )\nα0\n) + (d−m) ln α (T )\nα0 + m 2 T∑ t=1 (σ (t) m )2 α(t)\n] . (11)\nThen we bound the term RD by using the fact (9) and Assumption 1 and 2.\nRD = T∑ t=1 (w(t) −w)> [ ηtg (t)(g(t))> + 1 2 (σ(t−1)m ) 2I−V(t−1)(V(t−1))> ] (w(t) −w)\n≤ T∑ t=1 ηt(w (t) −w)>g(t)(g(t))>(w(t) −w) + 1 2 T∑ t=1 (σ(t−1)m ) 2(w(t) −w)>(w(t) −w)\n≤4(CL)2 T∑ t=1 ηt + 2C 2 T∑ t=1 (σ(t−1)m ) 2. (12)\nFinally, we obtain the result by combining (11) and (12)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>Online Newton step algorithms usually achieve good performance with less training samples than<lb>first order methods, but require higher space and time complexity in each iteration. In this paper,<lb>we develop a new sketching strategy called regularized frequent direction (RFD) to improve the<lb>performance of online Newton algorithms. Unlike the standard frequent direction (FD) which<lb>only maintains a sketching matrix, the RFD introduces a regularization term additionally. The<lb>regularization provides an adaptive stepsize for update, which makes the algorithm more stable.<lb>The RFD also reduces the approximation error of FD with almost the same cost and makes the<lb>online learning more robust to hyperparameters. Empirical studies demonstrate that our approach<lb>outperforms sate-of-the-art second order online learning algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}