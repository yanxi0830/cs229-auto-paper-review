{
  "name" : "1601.06180.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Latent Variable Interpretation in Sum-Product Networks",
    "authors" : [ "Robert Peharz", "Pedro Domingos" ],
    "emails" : [ "robert.peharz@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 1.\n06 18\n0v 2\n[ cs\n.A I]\n2 8\nO ct\n2 01\n6 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, ACCEPTED PRE-PRINT VERSION, OCTOBER 2016 1\nIndex Terms—Sum-Product Networks, Latent Variables, Mixture Models, Expectation-Maximization, MPE inference\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "SUM-PRODUCT NETWORKS are a promising type of prob-abilistic model, combining the domains of deep learning and graphical models [1], [2]. One of their main advantages is that many interesting inference scenarios are expressed as single forward and/or backward passes, i.e. these inference scenarios have a computational cost linear in the SPN’s representation size. SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9]. Since their proposition [1], one of the central themes in SPNs has been their interpretation as hierarchically structured latent variable (LV) models. This is essentially the same approach as the LV interpretation in mixture models. Consider for example a Gaussian mixture model with K components over a set of random variables (RVs) X:\nppXq “ K ÿ\nk“1\nwk N pX |µk,Σkq, (1)\nwhere N p¨ | ¨q is the Gaussian PDF, µk and Σk are the means and covariances of the kth component, and wk are the mixture weights with wk ě 0, ř\nwk “ 1. The GMM can be interpreted in two ways: i) It is a convex combination of PDFs and thus itself a PDF, or ii) it is a marginal distribution\n‚ R. Peharz is with the Institute of Physiology (iDN), Medical University of Graz and BioTechMed–Graz. E-mail: robert.peharz@gmail.com ‚ R. Gens and P. Domingos are with the Department of Computer Science and Engineering, University of Washington. E-mail: rcg@cs.washington.edu, pedrod@cs.washington.edu ‚ F. Pernkopf is with the Signal Processing and Speech Communication Lab, Graz University of Technology. E-mail: pernkopf@tugraz.at\nManuscript received November 12, 2015; revised June 20, 2016; accepted September 30, 2016.\nof a distribution ppX, Zq over X and a latent, marginalized variable Z , where ppX |Z “ kq “ N pX |µk,Σkq and ppZ “ kq “ wk . The second interpretation, the LV interpretation, yields a syntactically well-structured model. For example, following the LV interpretation, it is clear how to draw samples from ppXq by using ancestral sampling. This structure can also be of semantic nature, for instance when Z represents a clustering of X or when Z is a class variable. Furthermore, the LV interpretation allows the application of the EM algorithm – which is essentially maximum-likelihood learning under missing data [10], [11] – and enables advanced Bayesian techniques [12], [13].\nMixture models can be seen as a special case of SPNs with a single sum node, which corresponds to a single LV. More generally, SPNs can have arbitrarily many sum nodes, each corresponding to its own LV, leading to a hierarchically structured model. In [1], the LV interpretation in SPNs was justified by explicitly introducing the LVs in the SPN model, using the so-called indicator variables corresponding to the LVs’ states. However, as shown in this paper, this justification is actually too simplistic, since it is potentially in conflict with the completeness condition [1], leading to an incompletely specified model. As a remedy we propose the augmentation of an SPN, which additionally to the IVs also introduces the so-called twin sum nodes, in order to completely specify the LV model. We further investigate the independency structure of the LV model resulting from augmentation and find a parallel to the local independence assertions in Bayesian networks (BNs) [14], [15]. This allows us to define a BN representation of the augmented SPN. Using our BN interpretation and the differential approach [16], [17] in augmented SPNs, we give a sound derivation of the (soft) EM algorithm for SPNs.\nClosely related to the LV interpretation is the inference scenario of finding the most-probable-explanation (MPE),\ni.e. finding a probability maximizing assignment for all RVs. Using results form [18], [19], we first point out that this problem is generally NP-hard for SPNs. In [1] it was proposed that an MPE solution can be found efficiently when maximizing over both model RVs (i.e. non-latent RVs) and LVs. The proposed algorithm replaces sum nodes by max nodes and recovers the solution by using Viterbistyle backtracking. However, it was not shown that this algorithm delivers a correct MPE solution. In this paper, we show that this algorithm is indeed correct, when applied to selective SPNs [20]. In particular, since augmented SPNs are selective, this algorithm obtains an MPE solution in augmented SPNs. However, when applied to non-augmented SPNs, the algorithm still returns an MPE solution of the augmented SPN, but implicitly assumes that the weights for all twin sums are deterministic, i.e. they are all 0 except a single 1. This leads to a phenomenon in MPE inference which we call low-depth bias, i.e. more shallow parts of the SPN are preferred during backtracking.\nThe main contribution in this paper is to provide a sound theoretical foundation for the LV interpretation in SPNs and related concepts, i.e. the EM algorithm and MPE inference. Our theoretical findings are confirmed in experiments on synthetic data and 103 real-world datasets.\nThe paper is organized as follows: In the remainder of this section we introduce notation, review SPNs and discuss related work. In Section 2 we propose the augmentation of SPNs, show its soundness as hierarchical LV model and give an interpretation as BN. Furthermore, we discuss independency properties in augmented SPNs and the interpretation of sum-weights as conditional probabilities. The EM algorithm for SPNs is derived in Section 3. In Section 4 we discuss MPE inference for SPNs. Experiments are presented in Section 5 and Section 6 concludes the paper. Proofs for our theoretical findings are deferred to the Appendix."
    }, {
      "heading" : "1.1 Background and Notation",
      "text" : "RVs are denoted by upper-case letters W , X , Y and Z . The set of values of an RV X is denoted by valpXq, where corresponding lower-case letters denote elements of valpXq, e.g. x is an element of valpXq. Sets of RVs are denoted by boldface letters W, X, Y and Z. For RV set X “ tX1, . . . , XNu, we define valpXq “ ŚN\nn“1 valpXnq and use corresponding lower-case boldface letters for elements of valpXq, e.g. x is an element of valpXq. For a subset Y Ď X, xrYs denotes the projection of x onto Y.\nThe elements of valpXq can be interpreted as complete evidence, assigning each RV in X a fixed value. Partial evidence about X is represented as a subset X Ď valpXq, which is an element of the sigma-algebraAX induced by RV X . For all RVs we use AX “ tX P B | X Ď valpXqu, B being the Borel-sets over R. For discrete RVs, this choice yields the power-set AX “ 2\nvalpXq. For example, partial evidence X “ t1, 3, 5u for a discrete RV X with valpXq “ t1, . . . , 6u represents evidence that X takes one of the states 1, 3 or 5, and Y “ r´8, πs for a real-valued RV Y represents evidence that Y takes a value smaller than π. Formally speaking, partial evidence is used to express the domain of marginalization or maximization for a particular RV.\nFor sets of RVs X “ tX1, . . . , XNu, we use the product sets HX :“ t ŚN n“1 Xn | Xn P AXnu to represent partial\nevidence about X. Elements of HX are denoted using boldface notation, e.g. X . When Y Ď X and X P HX, we define X rYs :“ txrYs | x P X u. Furthermore, we use e to symbolize any combination of complete and partial evidence, i.e. for RVs X we have some complete evidence x1 for X1 Ď X and some partial evidence X 2 P HX2 for X2 “ XzX1.\nGiven a node N in some directed graph G, let chpNq and papNq be the set of children and parents of N, respectively. Furthermore, let descpNq be the set of descendants of N, recursively defined as the set containing N itself and any child of a descendant. Similarly, we define ancpNq as the ancestors of N, recursively defined as the set containing N itself and any parent of an ancestor. SPNs are defined as follows.\nDefinition 1 (Sum-Product Network). A Sum-Product network (SPN) S over a set of RVs X is a tuple pG,wq where"
    }, {
      "heading" : "G is a connected, rooted and acyclic directed graph, and w is a set of non-negative parameters. The graph G contains three",
      "text" : "types of nodes: distributions, sums and products. All leaves of G are distributions and all internal nodes are either sums or products. A distribution node (also called input distribution or simply distribution) DY : valpYq ÞÑ r0,8s is a distribution function over a subset of RVs Y Ď X, i.e. either a PMF (discrete RVs), a PDF (continuous RVs), or a mixed distribution function (discrete and continuous RVs mixed). A sum node S computes a weighted sum of its children, i.e. S “ ř\nCPchpSq wS,C C, where wS,C is a non-negative weight associated with edge S Ñ C, andw contains the weights for all outgoing sum-edges. A product node P computes the product over its children, i.e. P “ ś\nCPchpPq C.\nThe sets SpSq and PpSq contain all sum nodes and all product nodes in S, respectively.\nThe size |S| of the SPN is defined as the number of nodes and edges in G. For any node N in G, the scope of N is defined as\nscpNq “\n#\nY if N is a distribution DY Ť\nCPchpNq scpCq otherwise. (2)\nThe function computed by S is the function computed by its root and denoted as Spxq, where without loss of generality we assume that the scope of the root is X.\nWe use symbols D, S, P, N, C and F for nodes in SPNs, where D denotes a distribution, S denotes a sum, and P denotes a product. Symbols N, C and F denote generic nodes, where C and F indicate a child or parent relationship to another node, respectively. The distribution pS of an SPN S is defined as the normalized output of S, i.e. pSpxq9Spxq. For each node N, we define the sub-SPN SN rooted at N as the SPN defined by the graph induced by the descendants of N and the corresponding parameters.\nInference in unconstrained SPNs is generally intractable. However, efficient inference in SPNs is enabled by two structural constraints, completeness and decomposability [1]. An SPN is complete if for all sums S it holds that\n@C1,C2 P chpSq : scpC1q “ scpC2q. (3)\nAn SPN is decomposable if for all products P it holds that\n@C1,C2 P chpPq,C1 “ C2 : scpC1q X scpC2q “ H. (4)\nFurthermore, a sum node S is called selective [20] if for all choices of sum-weights w and all possible inputs x it holds that at most one child of S is non-zero. An SPN S is called selective if all its sum nodes are selective.\nAs shown in [17], [19], integrating Spxq over arbitrary sets X P HX, i.e. marginalization over X , reduces to the corresponding integrals at the input distributions and evaluating sums and products in the usual way. This property is known as validity of the SPNs [1], and key for efficient inference. In this paper we only consider complete and decomposable SPNs. Without loss of generality [17], [21], we assume locally normalized sum-weights, i.e. for each sum node S we have ř\nCPchpSq wS,C “ 1, and thus pS ” S, i.e. the SPN’s normalization constant is 1.\nFor RVs with finitely many states, we will use so-called indicator variables (IVs) as input distributions [1]. For a finitestate RV X and state x P valpXq, we introduce the IV λX“xpx\n1q :“ 1px “ x1q, assigning all probability mass to x. A complete and decomposable SPN represents the (extended) network polynomial of pS , which can be used in the differential approach to inference [1], [16], [17]. Assume any evidence e which is evaluated in the SPN. The derivatives of the SPN function with respect to the IVs (by interpreting the IVs as real-valued variables, see [16], [17] for details) yield\nBSpeq\nBλX“x “ SpX “ x, ezXq, (5)\nrepresenting the inference scenario of modified evidence, i.e. evidence e is modified such that X is set to x. The computationally attractive feature of the differential approach is that (5) can be evaluated for all X P X and all x P valpXq simultaneously using a single back-propagation pass in the SPN, after evidence has been evaluated. Similarly, for the second (and higher) derivatives, we get\nB2Speq\nBλX“xλY “y “\n#\nSpX “ x, Y “ y, eztX,Y uq if X “ Y\n0 otherwise.\n(6) Furthermore, the differential approach can be generalized to SPNs with arbitrary input distributions, i.e. SPNs over RVs with countably infinite or uncountably many states (cf. [17] for details)."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "SPNs are related to negation normal forms (NNFs), a potential deep network representation of propositional theories [22], [23], [24]. Like in SPNs, structural constraints in NNFs enable certain polynomial-time queries in the represented theory. In particular, the notions of smoothness, decomposability and determinism in NNFs translate to the notions of completeness, decomposability and selectivity in SPNs, respectively. The work on NNFs led to the concept of network polynomials as a multilinear representation of BNs over finitely many states [16], [25]. BNs were cast into an intermediate d-DNNF (deterministic decomposable NNF) representation in order to generate an arithmetic circuit (ACs), representing the BNs network polynomial. ACs, when restricted to sums and products, are equivalent to SPNs but have a slightly different syntax. In [26], ACs were learned by optimizing an objective trading off the log-likelihood on the\ntraining set and the inference cost of the AC, measured as the worst-case number of arithmetic operations required for inference (i.e. the number of edges in the AC). The learned models still represent BNs with context-specific independencies [27]. A similar approach learning Markov networks represented by ACs is followed in [28]. SPNs were the first time proposed in [1], where the represented distribution was not defined via a background graphical model any more, but directly as the normalized output of the network. In this work, SPNs were applied to image data, where a generic architecture reminiscent to convolutional neural networks was proposed. Structure learning algorithms not restricted to the image domain were proposed in [2], [3], [4], [29], [30], [31]. Discriminative learning of SPNs, optimizing conditional likelihood, was proposed in [6]. Furthermore, there is a growing body of literature on theoretical aspects of SPNs and their relationship to other types of probabilistic models. In [32] two families of functions were identified which are efficiently representable by deep, but not by shallow SPNs, where an SPN is considered as shallow if it has no more than three layers. In [17] it was shown that SPNs can w.l.o.g. be assumed to be locally normalized and that the notion of consistency does not allow exponentially more compact models than decomposability. These results were independently found in [21]. Furthermore, in [17], a sound derivation of inference mechanisms for generalized SPNs was given, i.e. SPNs over RVs with (uncountably) infinitely many states. In [21], a BN representation of SPNs was found, where LVs associated with sum nodes and the model RVs are organized in a two layer bipartite structure. The actual SPN structure is captured in structured conditional probability tables (CPTs) using algebraic decision diagrams. Recently, the notion of SPNswas generalized to sumproduct functions over arbitrary semirings [33]. This yields a general unifying framework for learning and inference, subsuming, among others, SPNs for probabilistic modeling, NNFs for logical propositions and function representations for integration and optimization."
    }, {
      "heading" : "2 LATENT VARIABLE INTERPRETATION",
      "text" : "As pointed out in [1], each sum node in an SPN can be interpreted as a marginalized LV, similar as in the GMM example in Section 1. For each sum node S, one postulates a discrete LV Z whose states correspond to the children of S. For each state, an IV and a product is introduced, such that the children are switched on/off by the corresponding IVs, as illustrated in Fig. 1.1 When all IVs in Fig. 1b are set to 1, S still computes the same value as in Fig. 1a. Since setting all IVs of Z to 1 corresponds to marginalizing Z , the sum S should be interpreted as a latent, marginalized RV.\nHowever, when we regard a larger structural context in Fig. 1b, we recognize that this justification is actually too simplistic. Explicitly introducing the IVs renders the ancestor S1 incomplete, when S is no descendant of N, and Z is thus not in the scope of N. Note that setting all IVs to 1 in an incomplete SPN generally does not correspond to\n1. In graphical representations of SPNs, IVs are depicted as nodes containing a small circle, general distributions as nodes containing a Gaussian-like PDF, and sum and products as nodes with ` and ˆ symbols. Empty nodes are of arbitrary type.\nmarginalization. Furthermore, note that also S1 corresponds to an LV, say Z 1. While we know the probability distribution of Z if Z 1 is in the state corresponding to P, namely the weights of S, we do not know this distribution when Z 1 is in the state corresponding to N. Intuitively, we recognize that the state of Z is irrelevant in this case, since it does not influence the resulting distribution over the model RVs X. Nevertheless, the probabilistic model is not completely specified, which is unsatisfying.\nA remedy for these problems is shown in Fig. 1c. We introduce the twin sum node S̄ whose children are the IVs corresponding to Z . The twin S̄ is connected as child of an additional product node, which is interconnected between S1 and N. Since this new product node has scope scpNq Y tZu, S1 is rendered complete now. Furthermore, if Z 1 takes the state corresponding to N (or actually the state corresponding to the new product node), we now have a specified conditional distribution for Z , namely the weights of the twin sum node. Clearly, given that all IVs of Z are set to 1, the network depicted in Fig. 1c still computes the same function as the network in Fig. 1a (or Fig. 1b), since S̄ constantly outputs 1, as long as we use normalized weights for it. Which weights should be used for the twin sum node S̄? Basically, we can assume arbitrary normalized weights, which will cause S̄ to constantly output 1, where, however, a natural choice would be to use uniform weights for S̄ (maximizing the entropy of the resulting LV model). Although the choice of weights is not crucial for evaluating evidence in the SPN, it plays a role in MPE inference, see Section 4. For now, let us formalize the explicit introduction of LVs, denoted as augmentation."
    }, {
      "heading" : "2.1 Augmentation of SPNs",
      "text" : "Let S be an SPN over X. For each S P SpSq we assume an arbitrary but fixed ordering of its children chpSq “ tC1\nS , . . . ,C KS S\nu, where KS “ |chpSq|. Let ZS be an RV on the same probability space as X, with valpZSq “ t1, . . . ,KSu, where state k corresponds to child Ck\nS . We call ZS the\nLV associated with S. For sets of sum nodes S we define ZS “ tZS | S P Su. To distinguish X from the LVs, we will refer to the former as model RVs. For node N, we define the sum ancestors/descendants as\nancSpNq :“ ancpNq X SpSq, (7)\ndescSpNq :“ descpNq X SpSq. (8)\n1: procedure AUGMENTSPN(S) 2: S 1 Ð S 3: @S P SpS 1q, @k P t1, . . . ,KSu :\nlet wS,k “ wS,Ck S , w̄S,k “ w̄S,Ck S\n4: for S P SpS 1q do 5: for k “ 1 . . .KS do 6: Introduce a new product node Pk\nS in SpS 1q\n7: Disconnect Ck S from S 8: Connect Ck S as child of Pk S 9: Connect Pk S as child of S with weight wS,k\n10: end for 11: end for 12: for S P SpS 1q do 13: for k P t1, . . . ,KSu do 14: Connect new IV λZS“k as child of P k S 15: end for 16: if ScpSq “ H then 17: Introduce a twin sum node S̄ in S 1 18: @k P t1, . . . ,KSu: connect λZS“k as child of S̄, and let wS̄,λZS“k\n“ w̄S,k 19: for Sc P ScpSq do 20: for k P tk | S R descpPk\nSc qu do\n21: Connect S̄ as child of Pk Sc 22: end for 23: end for 24: end if 25: end for 26: return S 1 27: end procedure\nFig. 2. Pseudo-code for augmentation of an SPN.\nFor each sum node S we define the conditioning sums as\nS cpSq :“ tSc P ancSpSqztSu | DC P chpS cq : S R descpCqu. (9) Furthermore, we assume a set of locally normalized twinweights w̄, containing a twin-weight w̄S,C for each weight wS,C in the SPN. We are now ready to define the augmentation of an SPN.\nDefinition 2 (Augmentation of SPN). Let S be an SPN over X, w̄ be a set of twin-weights and S 1 be the result of algorithm AUGMENTSPN, shown in Fig. 2. S 1 is called the augmented SPN of S, denoted as S 1 “: augpSq. Within the context of S 1, C k S is called the kth former child of S. The introduced product node Pk S is called link of S, Ck S and λZS“k, respectively. The sum node S̄, if introduced, is called the twin sum node of S. With respect to S 1, we denote S as the original SPN.\nIn steps 4–11 of AUGMENTSPN we introduce the links P k S which are interconnected between sum node S and its kth child. Each link Pk S has a single parent, namely S, and simply copies the former child Ck S . In steps 13–15, we introduce IVs corresponding to the associated LV ZS, as proposed in [1]. As we saw in Fig. 1 and the discussion above, this can render other sum nodes incomplete. These sums are clearly the conditioning sums ScpSq. Thus, when necessary, we introduce a twin sum node in steps 17–23, to treat this problem. The following proposition states the soundness of augmentation.\nProposition 1. Let S be an SPN over X, S 1 “ augpSq and Z :“ ZSpSq. Then S\n1 is a complete and decomposable SPN over XY Z with S 1pXq ” SpXq.\nProposition 1 states that the marginal distribution over X in the augmented SPN is the same distribution as represented by the original SPN, while being a completely specified probabilistic model over X and Z. Thus, augmentation provides a sound way to generalize the LV interpretation from mixture models to more general SPNs. An example of augmentation is shown in Fig. 3.\nNote that we understand the augmentation mainly as a theoretical tool to establish and work with the LV interpretation in SPNs. In most cases, it will be neither necessary nor advisable to explicitly construct the augmented SPN.\nAn interesting question is how the sizes of the original SPN and the augmented SPN relate to each other. A lower bound is |S 1| P Ωp|S|q, holding e.g. for SPNs with a single sum node. An asymptotic upper bound is |S 1| P Op|S|2q. To see this, note that the introduction of links, IVs and twin sums cause at most a linear increase of the SPN’s size. The number of edges introduced when connecting twins to the links of conditioning sums is bounded by |S|2, since the number of twins and links are both bounded by |S|. Therefore, we have |S 1| P Op|S|2q. This asymptotic upper bound is indeed achieved by certain types of SPNs: Consider e.g. a chain consisting of K sum nodes and K ` 1 distribution nodes. For k ă K the kth sum is the parent of the pk ` 1qth sum and the kth distribution, and the K th sum is the parent of the last two distributions. For the kth sum, all preceding sums are conditioning sums, yielding k ´ 1 introduced edges. In total this gives řK\nk“2pk ´ 1q “ K pK´1q 2 “ K 2´K 2\nedges, i.e. in this case |S 1| indeed grows quadratically in |S|."
    }, {
      "heading" : "2.2 Conditional Independencies in Augmented SPNs and Probabilistic Interpretation of Sum-Weights",
      "text" : "It is helpful to introduce the notion of configured SPNs, which takes a similar role as conditioning in the literature on DNNFs [22], [23], [24].\nDefinition 3 (Configured SPN). Let S be an SPN overX,Y Ď ZSpSq and y P valpYq. The configured SPN S\ny is obtained by deleting the IVs λY “y and their corresponding link for each Y P Y, y “ yrY s from augpSq, and further deleting all nodes which are rendered unreachable from the root.\nIntuitively, the configured SPN isolates the computational structure selected by y. All sum edges which ”survive” in the configured SPN are equipped with the same weights as in the augmented SPN. Therefore, a configured SPN is in general not locally normalized. We note the following properties of configured SPNs.\nProposition 2. Let S be an SPN over X, Y Ď ZSpSq and Z “ ZSpSqzY. Let y P valpYq and let S\n1 “ augpSq. It holds that\n1) Each node in Sy has the same scope as its corresponding node in S 1. 2) Sy is a complete and decomposable SPN overXYYYZ. 3) For any node N in Sy with scpNq X Y “ H, we have\nthat Sy N “ S 1 N .\n4) For y1 P valpYq it holds that\nSypX,Z,y1q “\n#\nS 1pX,Z,y1q if y1 “ y 0 otherwise (10)\nThe next theorem shows certain conditional independencies in the augmented SPN. For ease of discussion, we make the following definitions.\nDefinition 4. Let S be a sum node in an SPN and ZS its associated LV. All other RVs (model RVs and LVs) are divided into three sets:\n‚ Parents Zp, which are all LVs ”above” S, i.e. Zp “ ZancSpSqzZS. ‚ Children Yc, which are all model RVs and LVs ”below” S, i.e. Yc “ scpSq Y ZdescSpSqzZS. ‚ Non-descendants Yn, which are the remaining RVs, i.e. Yn “ pXY ZSpSqqzpZp YYc Y ZSq.\nWe will show that the parents, children and nondescendants play the likewise role as for independencies in\nBNs [14], [15], i.e. ZS is independent of Yn given Zp. We will further show that the sum-weights of S are the conditional distribution of ZS, conditioned on the event that ”Zp select a path to S”. One problem in the original LV interpretation [1] was, that no conditional distribution of ZS was specified for the complementary event. Here, we will show that the twin-weights are precisely this conditional distribution. This requires that the event “Zp select a path to the twin S̄” is indeed the complementary event to “Zp select a path to S”. This is shown in following lemma.\nLemma 1. Let S be an SPN over X, let S be a sum node in S and Zp be the parents of ZS. For any z P valpZpq, the configured SPN Sz contains either S or its twin S̄, but not both.\nWe are now ready to state the our theorem concerning conditional independencies in augmented SPNs.\nTheorem 1. Let S be an SPN over X and S 1 “ augpSq. Let S be an arbitrary sum in S and wk “ wS,Ck\nS , w̄k “ w̄S,Ck S , k “ 1, . . . ,KS. With respect to S, let Zp be the parents,Yc be the children and Yn be the non-descendants, respectively. Then there exists a two-partition of valpZpq, i.e.Z , Z̄ : ZY Z̄ “ valpZpq, Z X Z̄ “ H, such that\n@z P Z : S 1pZS “ k,Yn, zq “ wkS 1pYn, zq, and (11) @z P Z̄ : S 1pZS “ k,Yn, zq “ w̄kS 1pYn, zq. (12)\nFrom Theorem 1 it follows that the weights and twinweights of a sum node S can be interpreted as conditional probability tables (CPTs) of ZS, conditioned on Zp and that ZS is conditionally independent of Yn given Zp, i.e.\nS 1pZS “ k |Yn, zq “ S 1pZS “ k | zq “\n#\nwk if z P Z\nw̄k if z P Z̄ . (13)\nUsing this result, we can define a BN representing the augmented SPN as follows: For each sum node S, connect Zp as parents of ZS, and all RVs scpSq as children of ZS. By doing this for each LV, we obtain our BN representation of the augmented SPN, serving as a useful tool to understand SPNs in the context of probabilistic graphical models. An example of the BN interpretation is shown in Fig. 4.\nNote that the BN representation by Zhao et al. [21] can be recovered from the BN representation of augmented SPNs. They proposed a BN representation of SPNs using a bipartite structure, where an LV is a parent of a model\nRV if it is contained in the scope of the corresponding sum node. The model RVs and LVs are unconnected among each other, respectively. When we constrain the twin-weights to be equal to the sum-weights, we can see in (13) that ZS becomes independent of Zp. This special choice of twin weights effectively removes all edges between LVs, recovering the BN structure in [21]. In the next section, we use the augmented SPN and the BN interpretation to derive the EM algorithm for SPNs."
    }, {
      "heading" : "3 EM ALGORITHM",
      "text" : "The EM algorithm is a general scheme for maximum likelihood learning, when for some RVs complete evidence is missing [10], [11]. Thus, augmented SPNs are amenable for EM due to the LVs associated with sum nodes. Moreover, the twin-weights can be kept fixed, so that EM applied to augmented SPNs actually optimizes the weights of the original SPN. This approach was already pointed out in [1], where it was suggested that for evidence e and for any LV ZS, the marginal posteriors should be given as ppZS “ k | eq9wS,Ck S BSpeq BSpeq , which should be used for EM updates. These updates, however, cannot be the correct ones, as they actually leave the weights unchanged. Here, using augmented SPNs, we formally derive the standard EM updates for sum-weights and the input distributions, when they are chosen from an exponential family."
    }, {
      "heading" : "3.1 Updates for Weights",
      "text" : "Assume a dataset D “ tep1q, . . . , epLqu of L i.i.d. samples, where each eplq is any combination of complete and partial evidence for the model RVsX, cf. Section 1.1. Let Z “ ZSpSq be the set of all LVs and consider an arbitrary sum node S. Eq. (13) shows that the weights can be interpreted as conditional probabilities in our BN interpretation, where\nS 1pZS “ k |Zp “ zq “\n#\nwk if z P Z w̄k if z P Z̄ . (14)\nAs mentioned above, the twin-weights w̄k are kept fixed. Using the well-known EM-updates in BNs over discrete RVs [10], [15], the updates for sum-weight wk are given by summing over the expected statistics\nS 1pZS “ k,Zp P Z | e plqq, (15)\nfollowed by renormalization. We make the event Zp P Z explicit, by introducing a switching parent YS of ZS: When the twin sum of S exists, YS assumes the two states valpYSq “ tyS, yS̄u, where YS “ yS ô Zp P Z and YS “ yS̄ ô Zp P Z̄ . When the twin sum does not exist, YS just takes the single value valpYSq “ tySu. Clearly, when observed, YS renders ZS independent from Zp. The switching parent can be explicitly introduced in the augmented SPN, as depicted in Fig. 5. Here we simply introduce two new IVs λYS“yS and λYS“yS̄ , which switch on/off the output of S and S̄, respectively. It is easy to see that when these IV are constantly set to 1, i.e. when YS is marginalized, the augmented SPN performs exactly the same computations as before. It is furthermore easy to see that completeness and decomposability of the augmented SPN are maintained\nwhen the switching parent is introduced. Using the switching parent, the required expected statistics (15) translate to\nS 1pZS “ k, YS “ yS | e plqq. (16)\nTo compute (16), we use the differential approach, [16], [17], [19], cf. also Section 1.1. First note that\nS 1pZS “ k, YS “ yS, e plqq “\nB2S 1peplqq\nBλYS“ySBλZS“k . (17)\nThe first derivative is given as\nBS 1peplqq BλYS“yS “ BS 1peplqq BP Speplqq (18)\n“ BS 1peplqq\nBP\nKS ÿ\nk“1\nλZS“k wk C k Spe plqq, (19)\nwhere P is the common product parent of S and λYS“yS in the augmented SPN (see Fig. 5b). Differentiating (19) after λZS“k yields the second derivative\nB2S 1peplqq\nBλYS“ySBλZS“k “\nBS 1peplqq\nBP wk C\nk Spe plqq, (20)\ndelivering the required posteriors\nS 1pZS “ k, YS “ yS | e plqq “\n1\nS 1peplqq\nBS 1peplqq\nBP wk C\nk Spe plqq.\n(21) We do not want to construct the augmented SPN explicitly, so we express (21) in terms of the original SPN. Since all\nLVs are marginalized, it holds that S 1peplqq “ Speplqq and BS1peplqq\nBP “ BSpeplqq BS , yielding\nS 1pZS “ k, YS “ yS | e plqq “\n1\nSpeplqq\nBSpeplqq\nBS wk C\nk Spe plqq,\n(22) delivering the required statistics for updating the sumweights. We now turn to the updates of the input distributions."
    }, {
      "heading" : "3.2 Updates for Input Distributions",
      "text" : "For simplicity, we derive updates for univariate input distributions, i.e. for all distributions DY we have |scpDYq| “ 1. Similar updates can rather easily be derived also for multivariate input distributions. In [17], the so-called distribution selectors (DSs) were introduced to derive the differential approach for generalized SPNs. Similar as the switching parents for (twin) sum nodes, the DSs are RVs which render the respective model RVs independent from the remaining RVs. More formally, for each X P X, let DX be the set of all input distributions which have scope tXu. Assume an arbitrary but fixed ordering of DX and let rDXs be the index of DX in this ordering. Let the DS WX be a discrete RV with |DX | states. The so-called gated SPN S\ng is obtained by replacing each distribution by the product node\nDX Ñ DX ˆ λWX“rDX s. (23)\nThe introduced product is denoted as gate. As shown in [17], X is rendered independent from all other RVs in the SPN when conditioned on WX . Moreover, DX is the conditional distribution of X given WX “ rDX s. Therefore, each X and its DSWX can be incorporated as a two RV family in our BN interpretation. When each input distribution DX is chosen from an exponential family with natural parameters θDX , the M-step is given by the expected sufficient statistics\nθDX Ð\nř\nl S gpWX “ k | e plqq ş DXpx | e plqqθDX pxqdx\nř\nl S gpWX “ k | eplqq\n, (24)\nwhere k “ rDX s. When e plq contains complete evidence x1 for X , then the integral ş\nDXpx | e plqqθDX pxqdx reduces to\nθDX px 1q. When eplq contains partial evidence X , then\nż\nDXpx | e plqqθDX pxqdx “\nş\nX DXpxqθDX pxqdx ş\nX DXpxqdx\n. (25)\nDepending onX and the the type ofDX , evaluating (25) can be more or less demanding. A simple but practical case is when DX is Gaussian and X is some interval, permitting a closed form solution for integrating the Gaussian’s statistics θpxq “ px, x2q, using truncated Gaussians [34].\nTo obtain the posteriors SgpWX “ k | e plqq required in\n(24), we again use the differential approach. Note that\nSgpWX “ k, e plqq “\nBSgpeplqq BλWX“k “ BSgpeplqq BP DXpe plqq, (26)\nwhere k “ rDX s and P is the gate of DX , cf. (23). If we do not want to construct the gated SPN explicitly, we can use the identity BS gpeplqq BP “ BSpeplqq BDX\n. Thus the required posteriors are given as\nSgpWX “ k | e plqq “\n1\nSpeplqq\nBSpeplqq\nBDX DXpe\nplqq. (27)\nThe EM algorithm for SPNs, both for sum-weights and input distributions, is summarized in Fig. 6. In Section 5.1 we empirically verify our derivation of EM and show that standard EM successfully trains SPNs when a suitable structure is at hand.\nNote that recently Zhao and Poupart [35] derived a concave-convex procedure (CCCP) which yield the same sum-weight updates as the EM algorithm presented here and in [19]. This result is surprising, as EM and CCCP are rather different approaches in general."
    }, {
      "heading" : "4 MOST PROBABLE EXPLANATION",
      "text" : "In [1], [4], [7], SPNs were applied for reconstructing data using MPE inference. Given some distribution p over X and evidence e, MPE can be formalized as finding argmax\nxPe ppxq, where we assume that p actually has a maximum in e. MPE is a special case of MAP, defined as finding argmax\nyPerYs\nş\nerZs ppy, zqdz, for some two-partition of\nX, i.e. X “ Y Y Z,Y X Z “ H. Both MPE and MAP are generally NP-hard in BNs [36], [37], [38], and MAP is inherently harder than MPE [37], [38]. Using the result in [18], it follows that MAP inference is NP-hard also in SPNs. In particular, Theorem 5 in [18] shows that the decision version of MAP is NP-complete for a Naive Bayes model, when the class variable is marginalized. Naive Bayes is represented\nby the augmentation of an SPN with a single sum node, the LV representing the class variable. Therefore, MAP in SPNs is generally NP-hard. Since MAP in the augmented SPN representing the Naive Bayes model corresponds to MPE inference in the original SPN, i.e. a mixture model, it follows that also MPE inference is generally NP-hard in SPNs. A proof tailored to SPNs can be found in [19].\nHowever, when considering the the sub-class of selective SPNs (cf. Section 1.1 and [20]), an MPE solution can be obtained using a Viterbi-style backtracking algorithm in max-product networks.\nDefinition 5 (Max-Product Network). Let S be an SPN over X. We define the max-product network (MPN) Ŝ , by replacing each distribution node D by a maximizing distribution node\nD̂ : HscpDq ÞÑ r0,8s, D̂pYq :“ max yPY Dpyq, (28)\nand each sum node S by a max node\nŜ :“ max ĈPchpŜq w Ŝ,Ĉ Ĉ. (29)"
    }, {
      "heading" : "A product node P in S corresponds to a product node P̂ in Ŝ.",
      "text" : "Theorem 2. Let S be a selective SPN over X and let Ŝ the corresponding MPN. Let N be some node in S and N̂ its corresponding node in Ŝ . Then, for every X P HscpNq we have N̂pX q “ max xPX Npxq.\nTheorem 2 shows that the MPN maximizes the probability in its corresponding selective SPN. The proof (see appendix) also shows how to actually find a maximizing assignment. For a product, a maximizing assignment is given by combining the maximizing assignments of its children. For a sum, a maximizing assignment is given by the maximizing assignment of a single child, whose weighted maximum is maximal among all children. Here the children’s maxima are readily given by the upwards pass in the MPN. Thus, finding a maximizing assignment of any node in an selective SPN recursively reduces to finding maximizing assignments for the children of this node; this can be accomplished by a Viterbi-like backtracking procedure. This algorithm, denoted as MPESELECTIVE, is shown in Fig. 7. Here Q denotes a queue of nodes, where Q ð N and N ð Q denote the en-queue and de-queue operations, respectively. Note that Theorem 2 has already been derived for a special case, namely for arithmetic circuits representing network polynomials of BNs over discrete RVs [39].\nA direct corollary of Theorem 2 is that MPE inference is tractable in augmented SPNs, since augmented SPNs are selective SPNs over X and Z. This can easily be seen in AUGMENTSPN, as for any z and any sum S, exactly one IV of ZS is set to 1, causing that at most one child of S (or S̄) can be non-zero. Therefore, we can use MPESELECTIVE in augmented SPNs, in order to find an MPE solution over both model RVs and LVs. Note that an MPE solution for the augmented SPN does in general not correspond to an MPE solution for the original SPN, when discarding the states of the LVs. However, this procedure is a frequently used approximation for models where MPE is tractable for both model RVs and LVs, but not for model RVs alone.\nIn [1], MPESELECTIVE was applied to original SPNs, not to augmented SPNs, but also with the goal to recover an\nMPE solution over both model RVs and LVs. The states of the LVs were assigned during max-backtracking, as sumchildren and LV states are in one-to-one correspondence. The states of the LVs whose sums are not visited during backtracking, are not assigned – again, this causes some confusion, since some LVs appear to be undefined in some contexts, cf. the illustrations in Section 2. However, since this algorithm was used as approximation for MPE over model RVs by discarding the states of the LVs, this situation was not paid any further attention.\nNevertheless, as we show here, applying MPESELECTIVE to original (non-selective) SPNs effectively “simulates” MPESELECTIVE in the corresponding augmented SPN. Thereby, however, deterministic twin-weights are implicitly assumed, i.e. twin-weights which are 0, except a single 1. To see this, let us modify MPESELECTIVE, such that it can be applied to an original SPN, but returning an MPE solution for the corresponding augmented SPN. First note that in the augmented MPN, every twin node simply outputs the maximal twin-weight among all children whose states are contained in evidence e. For twin node S̄, let this maximal weight be denoted by ŵS̄. The effect of the twin nodes can now be simulated in the original SPN by replacing each weight wS,C in the original SPN by wS,C ˆ w̃S,C. Here w̃S,C is a correction factor and given as w̃S,C “ ś S̄ ŵS̄, where the product runs over all twins of those sums for which S is a conditioning sum. By using these corrected weights, each max node in the corresponding MPN gets the same input as in the MPN of the augmented SPN, i.e. the twin nodes are simulated. We can identify the maximizing states of those LVs whose sums are visited during backtracking, as in [1]. The states of the sums which are not visited are given by the child which correspond to the maximal twin-weight ŵS̄. Pseudo-code for this somewhat technical modification of MPESELECTIVE can be found in [19].\nWe see that the algorithm used in [1] is essentially equivalent to MPESELECTIVE in augmented SPNs when w̃S,C “ 1 for all sum nodes, which implies that the twin-weights are\ndeterministic. Therefore, although the LV model in [1] is not completely specified and it was not shown that the Viterbilike algorithm recovers an MPE solution, it nevertheless corresponds to MPE inference in the augmented SPN for special twin-weights, i.e. deterministic weights.\nHowever, using deterministic twin-weights is a rather unnatural choice, since this prefers one arbitrary state over the others in cases where this LV is actually “rendered irrelevant”. In this case, MPE inference also has a bias towards less structured sub-models, which we call lowdepth bias. This is illustrated in Fig. 8, which shows an SPN over three RVs X1, X2, X3. The augmented SPN has two twin sum nodes S̄2 and S̄3, corresponding to S2 and S3, respectively. When their twin-weights are deterministic, the selection of the state of ZS1 is biased towards the state corresponding to P1, which is a distribution assuming independence among X1, X2 and X3. This comes from the fact, that the values of P2 and P3 are dampened by the weights of S2 and S3, respectively, which are generally smaller than 1. Therefore, when using deterministic weights for twin sum nodes, we introduce a bias towards the selection of subSPNs that are less deep and less structured. Using uniform weights for twin sum nodes is somewhat “fairer”, since in this case P1 gets dampened by S̄2 and S̄3, P2 by S2 and S̄3, and P3 by S̄2 and S3. Uniform weights are to some extend the opposite choice to deterministic twin-weights: the former represent the strongest possible dampening via twin-weights and therefore actually penalize less structured distributions. Investigating these effects further is subject to future work."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "5.1 Experiments with EM Algorithm",
      "text" : "In [1], [40] SPNswere applied to image data, where a generic architecture reminiscent to convolutional neural networks was proposed. We refer to this architecture as PD architecture. Standard EM was not used in experiments for two reasons: First, explicitly constructing the proposed structure and to train it with standard EM is hardly possible with current hardware, since the number of nodes grows Opl3q, where l is the square-length of the modeled image domain in pixels [19]. Instead, a sparse hard EM algorithmwas used, which virtualizes the PD structure, i.e. sum and products are generated on the fly (see [40] for details). Second, using standard EM seemed unsuited to train large and dense SPNs, either because it is trapped in local optima or due to the gradient vanishing phenomenon.\nIn our experiments,2 we investigated three questions:\n1) Is our derivation of EM correct, both for complete and missing data? 2) Can the result of hard EM [1] be improved by standard EM? 3) Given a suited sparse structure, does EM yield a good solution for parameters?\nQuestion 1) is important since the original derivation contained an error. Questions 2) and 3) are concerned with the general applicability of EM for training SPN.\nWe used the same datasets and SPN structures as in [1], obtainable from [40]. The datasets comprise Caltech-101 (inclusive background class) [42] and the ORL face images [43], i.e. in total 103 datasets. The input distributions in these SPNs are single-dimensional Gaussians (4 for each pixel), where means were set to the averages of the 4-quantiles and variances were constantly 1. We ran EM (Fig. 6) for 30 iterations, with various settings:\n‚ Update any combination of the three different types of parameters, i.e. sum-weights, Gaussian means and Gaussian variances. Each set of parameters types is encoded by a string of letters W (weights), M (means) and V (variances). (7 combinations) ‚ Use original parameters for initialization, obtained from [40], or use 3 random initialization, where sumweights are drawn from a Dirichlet distribution with uniform α “ 1 hyper-parameter (i.e. uniform distribution on the standard simplex), Gaussian means are uniformly drawn from r´1, 1s and Gaussian variances from r0.01, 1s. Only parameters which are actually updated are initialized randomly; otherwise the original parameters [1] are used and kept fixed. (4 combinations) ‚ Use complete data or missing training data, randomly discarding 33% or 66% of the observations, independently for each sample. (3 combinations)\nThus, in total we ran EM 7 ˆ 4 ˆ 3 ˆ 103 “ 8652 times, yielding 259560 EM-iterations. To avoid pathological solutions we used a lower bound of 0.01 for the Gaussian variances. In no iteration we observed a decreasing likelihood\n2. Code available under [41].\non the training set,3 i.e. our derived EM algorithm showed monotonicity in our experiments. Moreover, as can be seen in Fig. 9a, the training log-likelihood actually increased over iterations. The curves for the missing data scenarios are similar. This gives affirmative evidence for question 1).\nFig. 9b shows the log-likelihood on the test set. Note that optimizing the parameter sets V and WV led to severe overfitting: while achieving extremely high likelihoods on the training set, they achieved extremely poor likelihoods on the test set. Also the parameter sets MV and WMV tend to overfit, although not as strong as V and WV.\nRegarding question 2), we closer inspected the test loglikelihood when the original parameters are used for initialization, i.e. when the parameters obtained by [40] are post-trained using EM. Table 1 summarizes the results. When parameter sets not including Gaussian variances are optimized (i.e. W, M, and WM), the test log-likelihood increased most of the time, i.e. for 83.5% (M) to up to 92.23% (WM) of the datasets. Furthermore, having oracle knowledge about the ideal number of iterations (i.e. column best), the average log-likelihood increased by 0.58% (M) to up to 1.39% (WM) relative to the original parameters. Most of this improvement happens in the first iteration, yielding 0.52% (M) up to 1.05% (WM) improvement. These results indicate that the parameters obtained by [40] slightly underfit the given datasets. Similar as in Fig. 9, we see that parameter sets including the Gaussian variances (V,\n3. Except for tiny occasional decreases (always ă 10´8) after EM had converged, which can be attributed to numerical artifacts.\nWV, MV, WMV) are prone to overfitting: more than 60% of the datasets decreased their test log-likelihood during EM. However, in the remaining 40% of the datasets, the test loglikelihood could be improved substantially by at least 14% on average.\nWe now turn to question 3). As pointed out above, a hard EM variant was used in [1], [40] which at the same time finds the effective SPN structure. Optimizing W using the 3 random initialization amounts to using the oracle structure obtained by [1], [40], discarding the learned parameters. For each dataset we selected the random initialization which yielded the highest likelihood on the training set in iteration 30. For this run, we compared the log-likelihoods with the log-likelihoods obtained by the original parameters. The results are summarized in Table 2. We see that on all data sets the log-likelihood on the training set is larger than for the original parameters. This is also the case for each individual random start (not just best one) – every random restart always yielded a higher training log-likelihood than the original parameters. Thus, by considering the actual optimization objective – the likelihood on the training set – EM successfully trains SPNs, given a suited oracle structure. Furthermore, as can be seen in Table 2, EM is also not more prone to overfitting than the algorithm in [1]: on 67.96% of the datasets, EM delivered a higher test log-likelihood than the original parameters, when using oracle knowledge about the ideal number of iterations (column best)."
    }, {
      "heading" : "5.2 Experiments with MPE Inference",
      "text" : "To illustrate correctness of MPESELECTIVE (Fig. 7) when applied to augmented SPNs, we generated SPNs using the PD architecture [1], arranging 4, 9 and 16 binary RVs in a 2ˆ2, 3ˆ3 and 4ˆ4 grid, respectively. As inputs we used two indicator variables for each RV representing their two states. The sum-weights were drawn from a Dirichlet distribution\nwith uniform α-parameters, where α P t0.5, 1, 2u. For all networks we drew 100 independent parameters sets. We ran MPESELECTIVE on the augmented SPN, once equipped with uniform twin-weights and once with deterministic twin-weights. For uniform twin-weights, we denote the result obtained by MPESELECTIVE as MPEUNI. For deterministic twin-weights, we denote the result as MPEDET. As described in Section 4, MPEDET corresponds essentially to the result when MPESELECTIVE is applied to the original SPN [1]. For each assignment, the log-likelihoods were evaluated in the augmented SPNwith deterministic weights, the augmented SPN with uniform weights and in the original SPN (discarding the states of the LVs). Additionally, we found ground truth MPE assignments in the two augmented SPNs and the original SPN using exhaustive enumeration. The results relative to the ground truth MPE solutions are shown in Tables 3, 4, and 5. As can be seen, MPEUNI always finds an MPE solution in the augmented SPN with uniform twin-weights and MPEDET always finds an MPE solution in augmented SPNs with deterministic twin-weights. This gives empirical evidence for the correctness of MPESELECTIVE for MPE inference in augmented SPNs.\nFurthermore, we wanted to investigate the quality of both algorithms when serving as approximation for MPE inference in the original SPNs. For the SPNs considered here, MPEDET delivered on average slightly better approximations than MPEUNI. However, these results should be interpreted with caution, due to the rather similar nature of the distributions considered here. Closer investigating approximate MPE for (original) SPNs is an interesting direction and will be subject to future research."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper we revisited the interpretation of SPNs as hierarchically structured LV model. We pointed out that the original approach to explicitly incorporate LVs does not produce a sound probabilistic model. As a remedy we proposed the augmentation of SPNs and proved its soundness as LV model. Within augmented SPNs, we investigated the independency structure represented as BN, and showed that the sum-weights can be interpreted as structured CPTs within this BN. Using augmented SPNs, we derived the EM algorithm for sum-weights and singledimensional input distributions from exponential families. While MPE-inference is generally NP-hard in SPNs, we showed that a Viterbi-style backtracking algorithm recovers an MPE solution in selective SPNs, and in particular in augmented SPNs. In experiments we give empirical evidence supporting our theoretical results. We furthermore showed that standard EM can successfully train generative SPNs, given a suitable network structure at hand."
    }, {
      "heading" : "APPENDIX A PROOFS",
      "text" : "A.1 Proof of Proposition 1\nIf S 1 is a complete and decomposable SPN over XYZ, then S 1pXq ” SpXq is immediate: Computing S 1pxq for any x P valpXq is done by marginalizing Z, i.e. setting all λZS“k “ 1. In this case, it is easy to see that none of the structural changes modifies the output of the SPN, i.e. the outputs of S and S 1 agree for each x, i.e. S 1pXq ” SpXq.\nIt remains to show that S 1 is complete and decomposable, and that the root’s scope is X Y Z. Steps 4–11 in AUGMENTSPN introduce the links, representing ”private copies” of the sum’s children, and clearly leave the SPN complete and decomposable. In steps 13–15 the LV ZS is introduced in the scope of S and thus in the scope of the root. Since this is done for all sum nodes, all Z are introduced in the root’s scope. Steps 13–15 cannot render products nondecomposable, since this would imply that S is reachable by two distinct children of this product – a contradiction to the fact that the SPN was decomposable before. However, as shown in Fig. 1, steps 13–15 can render ancestor sums incomplete. These are treated in steps 17–23. The twin sum S̄, if introduced, is clearly complete and has scope tZu. Furthermore, incompleteness of any conditioning sum Sc can only be caused by links not having ZS in their scope. The scope of these links is augmented by ZS in step 21.\nThese links clearly remain decomposable and moreover, Sc is rendered complete now.\nA.2 Proof of Proposition 2\nad 1.) When deleting the IVs and their links, the scopes of any (twin) sum remains the same, since it is complete and is left with one child. Thus also the scope of any ancestor remains the same. ad 2.) The graph of Sy is rooted and acyclic, since the root cannot be a link and deleting nodes and edges cannot introduce cycles. When an IV λY “y is deleted, also the link P y SY\nis deleted, so no internal nodes are left as leaves. The roots in Sy and S 1 are the same, and by point 1.,XYYYZ is the scope of the root. Sy is also complete and decomposable: Whenever an IV and its link are deleted, the corresponding sum node and twin sum node remain trivially complete, since they are left with a single child. Furthermore, completeness and decomposability of any ancestor of SY or S̄Y is left intact, since neither SY nor S̄Y changes its scope. ad 3.) According to point 1., the scope of N is the same in S 1 and Sy . Since scpNq XY “ H, the disconnected IVs and deleted links are no descendants of N, i.e. no descendants of N are disconnected during configuration. Since N is present in Sy , it must still be reachable from the root. Therefore also all descendants of N are reachable, i.e. Sy\nN “ S 1 N .\nad 4.) When the input is fixed to x, z,y, all IVs and links which are deleted from the configured SPN Sy evaluate to zero in the augmented SPN S 1. The outputs of all sums and twin sums are therefore the same in S 1 and Sy . Therefore, also the output of all other nodes remains the same. This includes the root and therefore Sypx, z,yq “ S 1px, z,yq, for any x, z.\nWhen y1 “ y, then there must be a Y P Y such that the IV λY “y1rY s has been deleted, i.e. λY “y1rY s R descpNq, where N is the root of Sy. Using Lemma 1 in [17], it follows that Sypx, z,y1q “ 0.\nA.3 Proof of Lemma 1\nSz must contain either S or S̄, since ZS is in the scope of the root by Proposition 2. To show that not both are in Sz, let Πk denote the set of paths of length k from the root to any node N with ZS P scpNq. For k ą 1, all paths in Πk can be constructed by extending each path inΠk´1 with each child of this path’s last node, if it has ZS in its scope. Let K be the smallest number such that there is a path in Πk containing S or S̄.\nWe show by induction, that |Πk| “ 1, k “ 1, . . . ,K . Note that Π1 contains a single path pNq, where N is the root, therefore the induction basis holds.\nFor the induction step, we show that given |Πk´1| “ 1, then also |Πk| “ 1. Let pN1, . . . ,Nk´1q be the single path in Πk´1. If Nk´1 is a product node, then it has a single child C with ZS P scpCq, due to decomposability. If Nk´1 is a sum node, then it must be in ancSpSqztSu, and therefore has a single child in the configured SPN. Therefore, there is a single way to extend the path and therefore |Πk| “ 1, k “ 1, . . . ,K . This single path does either lead to S or S̄. Since S R descpS̄q and S̄ R descpSq, Sz contains a single path to one of them, but not to both.\nA.4 Proof of Theorem 1\nBy Lemma 1, for each z P valpZpq the configured SPN S z contains either S or S̄, but not both. Let Z be the subset of valpZpq such that S is in S\nz and Z̄ be the subset of valpZpq such that S̄ is in Sz.\nFix ZS “ k and z P Z . We want to compute S 1pZS “ k,Yn, zq, i.e. we marginalizeYc. According to Proposition 2 (4.), this equals SzpZS “ k,Yn, zq. According to Proposition 2 (3.), the sub-SPN rooted at former child Ck\nS is the\nsame in S 1 and Sz. Since S 1 is locally normalized, this subSPN is also locally normalized in Sz. Since the scope of the former child Ck\nS is a sub-set of Yc, which is marginalized,\nand λZS“k “ 1, the link P k S outputs 1. Since λZS“k1 “ 0 for k1 “ k, the sum S outputs wk. Now consider the set of nodes in Sz which have ZS in their scope, not including λZS“k and P k S . Clearly, since S̄ is not in Sz, this set must be ancpSq. Let N1, . . . ,NL be a topologically ordered list of ancpSq, where S is N1 and NL is the root. Let Yn,l :“ scpNlq XYn and Zl :“ scpNlq XZp. We show by induction that for l “ 1, . . . , L, we have\nNlpZS “ k,Yn,l, zrZlsq “ wk NlpYn,l, zrZlsq. (30)\nSince Yn,1 “ H and Z1 “ H, and N1pZS “ kq “ wk, the induction basis holds. Assume that (30) holds for all N1, . . . ,Nl´1. If Nl is a sum, we have due to completeness\nNlpZS “ k,Yn,l, zrZlsq “ ÿ\nCPchpNlq\nwNl,Cwk CpYn,l, zrZlsq\n(31)\n“ wk NlpYn,l, zrZlsq, (32)\ni.e. the induction step holds for sums. When Nl is a product, due to decomposability, it must have a single child with ZS in its scope. Hence, this child must be a node Nm P ancpSq We have\nNlpZS “ k,Yn,l, zrZlsq (33)\n“ wk NmpYn,m, zrZmsq ź\nCPchpNlqzNm\nCpYn,l X scpCqq (34)\n“ wk NlpYn,l, zrZlsq, (35)\ni.e. the induction step holds for products. Therefore, by induction, (30) also holds for the root, and (11) follows.\nNow we show (12). If the twin sum S̄ does not exist, Z̄ is empty and (12) holds trivially. Otherwise, fix the input to ZS “ k and z P Z̄ . Clearly, S̄ outputs w̄k and (12) can be shown in similar way as (11).\nA.5 Proof of Theorem 2\nWe prove the theorem using an inductive argument. The\ntheorem clearly holds for any D̂ by definition. Consider a product P̂ and assume the theorem holds for all chpP̂q. Then the theorem also holds for P̂, since\nP̂pX q “ ź\nCPchpP̂q\nmax xPX Cpxq “ max xPX\nź\nCPchpP̂q\nCpxq “ max xPX Ppxq,\n(36)\nwhere the max and the product can be switched due to decomposability.\nNow consider a max node Ŝ and its corresponding sum node S. Let the support of an SPN-node N be the set supN :“\ntx |Npxq ą 0u. Since S is selective, its support is partitioned by the supports of its children, i.e. supS “ Ť CPchpSq supC, supC1 Ş supC2 “ H, for C 1 “ C2. Assuming that the theorem holds for all chpŜq, we have\nŜpX q “ max CPchpSq wS,C max xPX Cpxq (37)\n“ max CPchpSq wS,C max xPsup\nC XX\nCpxq (38)\n“ max CPchpSq max xPsupC XX wS,C Cpxq (39)\n“ max xPsupS XX Spxq “ max xPX Spxq. (40)\nIn (38) we have a slight abuse of notation, as we actually should use suprema over the sets supCXX and define the supremum over the empty set as 0. In (39) we used the fact that the support of the sum node is partitioned by the supports of its children and that for selective sums we have S “ wS,C C whenever we have single child with C ą 0.\nWe see that the induction step also holds for Ŝ. Therefore, the theorem holds for all nodes."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank the anonymous reviewers for their constructive comments. This work was supported by the Austrian Science Fund (FWF): P25244-N15 and Austrian Science Fund (FWF): P27803-N15. This research was partly funded byONR grant N00014-16-1-2697 andAFRL contract FA8750-13-2-0019."
    } ],
    "references" : [ {
      "title" : "Sum-product networks: A new deep architecture",
      "author" : [ "H. Poon", "P. Domingos" ],
      "venue" : "Proceedings of UAI, 2011, pp. 337–346.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning the structure of sum-product networks",
      "author" : [ "R. Gens", "R. Domingos" ],
      "venue" : "Proceedings of ICML, 2013, pp. 873–880.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning the architecture of sumproduct networks using clustering on variables",
      "author" : [ "A. Dennis", "D. Ventura" ],
      "venue" : "Proceedings of NIPS, 2012, pp. 2042–2050.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Greedy part-wise learning of sum-product networks",
      "author" : [ "R. Peharz", "B. Geiger", "F. Pernkopf" ],
      "venue" : "Proceedings of ECML/PKDD, vol. 8189. Springer Berlin, 2013, pp. 612–627.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sum-product networks for modeling activities with stochastic structure",
      "author" : [ "M. Amer", "S. Todorovic" ],
      "venue" : "Proceedings of CVPR, 2012, pp. 1314–1321.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Discriminative learning of sumproduct networks",
      "author" : [ "R. Gens", "P. Domingos" ],
      "venue" : "Proceedings of NIPS, 2012, pp. 3248–3256.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Modeling speech with sum-product networks: Application to bandwidth extension",
      "author" : [ "R. Peharz", "G. Kapeller", "P. Mowlaee", "F. Pernkopf" ],
      "venue" : "Proceedings of ICASSP, 2014, pp. 3699–3703.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Language modeling with sum-product networks",
      "author" : [ "W.C. Cheng", "S. Kok", "H.V. Pham", "H.L. Chieu", "K.M.A. Chai" ],
      "venue" : "Proceedings of Interspeech, 2014, pp. 2098–2102.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Representation learning for single-channel source separation and bandwidth extension",
      "author" : [ "M. Zöhrer", "R. Peharz", "F. Pernkopf" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 12, pp. 2398–2409, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A. Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society, Series B, vol. 39, no. 1, pp. 1–38, 1977.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Supervised learning from incomplete data via an EM approach",
      "author" : [ "Z. Ghahramani", "M. Jordan" ],
      "venue" : "Proceedings of NIPS, 1994, pp. 120–127.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Non-parametric Bayesian sum-product networks",
      "author" : [ "S.-W. Lee", "C. Watkins", "B. Zhang" ],
      "venue" : "ICML Workshop on Learning Tractable Probabilistic Models, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Structure inference in sum-product networks using infinite sum-product trees",
      "author" : [ "M. Trapp", "R. Peharz", "M. Skowron", "T. Madl", "F. Pernkopf", "R. Trappl" ],
      "venue" : "NIPS Workshop on Practical Bayesian Nonparametrics, 2016.  IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, ACCEPTED PRE-PRINT VERSION, OCTOBER 2016  14",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1988
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "A differential approach to inference in Bayesian networks",
      "author" : [ "A. Darwiche" ],
      "venue" : "Journal of the ACM, vol. 50, no. 3, pp. 280–305, 2003.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "On theoretical properties of sum-product networks",
      "author" : [ "R. Peharz", "S. Tschiatschek", "F. Pernkopf", "P. Domingos" ],
      "venue" : "Proceedings of AISTATS, 2015, pp. 744–752.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "New complexity results for MAP in Bayesian networks",
      "author" : [ "C. de Campos" ],
      "venue" : "Proceedings of IJCAI, 2011, pp. 2100–2106.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Foundations of sum-product networks for probabilistic modeling",
      "author" : [ "R. Peharz" ],
      "venue" : "Ph.D. dissertation, Graz University of Technology, 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning selective sumproduct networks",
      "author" : [ "R. Peharz", "R. Gens", "P. Domingos" ],
      "venue" : "ICML Workshop on Learning Tractable Probabilistic Models, 2014.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the relationship between sum-product networks and Bayesian networks",
      "author" : [ "H. Zhao", "M. Melibari", "P. Poupart" ],
      "venue" : "Proceedings of ICML, 2015, pp. 116–124.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Compiling knowledge into decomposable negation normal form",
      "author" : [ "A. Darwiche" ],
      "venue" : "Proceedings of IJCAI, 1999, pp. 284–289.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Decomposable negation normal form",
      "author" : [ "——" ],
      "venue" : "Journal of the ACM, vol. 48, no. 4, pp. 608–647, 2001.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A knowledge compilation map",
      "author" : [ "A. Darwiche", "P. Marquis" ],
      "venue" : "Journal of Artificial Intelligence Research, vol. 17, no. 1, pp. 229–264, 2002.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A logical approach to factoring belief networks",
      "author" : [ "A. Darwiche" ],
      "venue" : "Proceedings of KR, 2002, pp. 409–420.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning arithmetic circuits",
      "author" : [ "D. Lowd", "P. Domingos" ],
      "venue" : "Proceedings of UAI, 2008, pp. 383–392.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Contextspecific independence in Bayesian networks",
      "author" : [ "C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller" ],
      "venue" : "Proceedings of UAI, 1996, pp. 115–123.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Learning Markov networks with arithmetic circuits",
      "author" : [ "D. Lowd", "A. Rooshenas" ],
      "venue" : "Proceedings of AISTATS, 2013, pp. 406–414.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning sum-product networks with direct and indirect variable interactions",
      "author" : [ "A. Rooshenas", "D. Lowd" ],
      "venue" : "Proceedings of ICML, 2014, pp. 710–718.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning the structure of sum-product networks via an SVD-based algorithm",
      "author" : [ "T. Adel", "D. Balduzzi", "A. Ghodsi" ],
      "venue" : "Proceedings of UAI, 2015, pp. 32–41.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Simplifying, regularizing and strengthening sum-product network structure learning",
      "author" : [ "A. Vergari", "N. Di Mauro", "F. Esposito" ],
      "venue" : "Proceedings of ECML/PKDD, 2015, pp. 343–358.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Shallow vs. deep sum-product networks",
      "author" : [ "O. Delalleau", "Y. Bengio" ],
      "venue" : "Proceedings of NIPS, 2011, pp. 666–674.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The sum-product theorem: A foundation for learning tractable models",
      "author" : [ "A. Friesen", "P. Domingos" ],
      "venue" : "Proceedings of ICML, 2016, pp. 1909–1918.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A unified approach for learning the parameters of sum-product networks",
      "author" : [ "H. Zhao", "P. Poupart" ],
      "venue" : "http://arxiv.org/abs/1601.00318.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1601
    }, {
      "title" : "On the complexity of the MPA problem in probabilistic networks",
      "author" : [ "H. Bodlaender", "F. van den Eijkhof", "L. van der Gaag" ],
      "venue" : "Proceedings of ECAI, 2002, pp. 675–679.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Complexity results and approximation strategies for MAP explanations",
      "author" : [ "J.D. Park", "A. Darwiche" ],
      "venue" : "Journal of Artificial Intelligence Research, vol. 21, no. 1, pp. 101–133, 2004.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Most probable explanations in Bayesian networks: Complexity and tractability",
      "author" : [ "J. Kwisthout" ],
      "venue" : "International Journal of Approximate Reasoning, vol. 52, no. 9, pp. 1452–1469, 2011.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Modeling and Reasoning with Bayesian Networks",
      "author" : [ "A. Darwiche" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2014
    }, {
      "title" : "Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "R. Perona" ],
      "venue" : "Computer Vision and Image Understanding, vol. 106, no. 1, pp. 59–70, 2007.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "SUM-PRODUCT NETWORKS are a promising type of probabilistic model, combining the domains of deep learning and graphical models [1], [2].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "SUM-PRODUCT NETWORKS are a promising type of probabilistic model, combining the domains of deep learning and graphical models [1], [2].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9].",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : "SPNs have shown convincing performance in applications such as image completion [1], [3], [4], computer vision [5], classification [6] and speech and language modeling [7], [8], [9].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 0,
      "context" : "Since their proposition [1], one of the central themes in SPNs has been their interpretation as hierarchically structured latent variable (LV) models.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, the LV interpretation allows the application of the EM algorithm – which is essentially maximum-likelihood learning under missing data [10], [11] – and enables advanced Bayesian techniques [12], [13].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, the LV interpretation allows the application of the EM algorithm – which is essentially maximum-likelihood learning under missing data [10], [11] – and enables advanced Bayesian techniques [12], [13].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, the LV interpretation allows the application of the EM algorithm – which is essentially maximum-likelihood learning under missing data [10], [11] – and enables advanced Bayesian techniques [12], [13].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, the LV interpretation allows the application of the EM algorithm – which is essentially maximum-likelihood learning under missing data [10], [11] – and enables advanced Bayesian techniques [12], [13].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 0,
      "context" : "In [1], the LV interpretation in SPNs was justified by explicitly introducing the LVs in the SPN model, using the so-called indicator variables corresponding to the LVs’ states.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "However, as shown in this paper, this justification is actually too simplistic, since it is potentially in conflict with the completeness condition [1], leading to an incompletely specified model.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : "We further investigate the independency structure of the LV model resulting from augmentation and find a parallel to the local independence assertions in Bayesian networks (BNs) [14], [15].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "We further investigate the independency structure of the LV model resulting from augmentation and find a parallel to the local independence assertions in Bayesian networks (BNs) [14], [15].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 15,
      "context" : "Using our BN interpretation and the differential approach [16], [17] in augmented SPNs, we give a sound derivation of the (soft) EM algorithm for SPNs.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "Using our BN interpretation and the differential approach [16], [17] in augmented SPNs, we give a sound derivation of the (soft) EM algorithm for SPNs.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Using results form [18], [19], we first point out that this problem is generally NP-hard for SPNs.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "Using results form [18], [19], we first point out that this problem is generally NP-hard for SPNs.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "In [1] it was proposed that an MPE solution can be found efficiently when maximizing over both model RVs (i.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : "In this paper, we show that this algorithm is indeed correct, when applied to selective SPNs [20].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "However, efficient inference in SPNs is enabled by two structural constraints, completeness and decomposability [1].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "Furthermore, a sum node S is called selective [20] if for all choices of sum-weights w and all possible inputs x it holds that at most one child of S is non-zero.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "As shown in [17], [19], integrating Spxq over arbitrary sets X P HX, i.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "As shown in [17], [19], integrating Spxq over arbitrary sets X P HX, i.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "This property is known as validity of the SPNs [1], and key for efficient inference.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "Without loss of generality [17], [21], we assume locally normalized sum-weights, i.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : "Without loss of generality [17], [21], we assume locally normalized sum-weights, i.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "For RVs with finitely many states, we will use so-called indicator variables (IVs) as input distributions [1].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "A complete and decomposable SPN represents the (extended) network polynomial of pS , which can be used in the differential approach to inference [1], [16], [17].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "A complete and decomposable SPN represents the (extended) network polynomial of pS , which can be used in the differential approach to inference [1], [16], [17].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 16,
      "context" : "A complete and decomposable SPN represents the (extended) network polynomial of pS , which can be used in the differential approach to inference [1], [16], [17].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "The derivatives of the SPN function with respect to the IVs (by interpreting the IVs as real-valued variables, see [16], [17] for details) yield",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "The derivatives of the SPN function with respect to the IVs (by interpreting the IVs as real-valued variables, see [16], [17] for details) yield",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "[17] for details).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "SPNs are related to negation normal forms (NNFs), a potential deep network representation of propositional theories [22], [23], [24].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "SPNs are related to negation normal forms (NNFs), a potential deep network representation of propositional theories [22], [23], [24].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 23,
      "context" : "SPNs are related to negation normal forms (NNFs), a potential deep network representation of propositional theories [22], [23], [24].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "The work on NNFs led to the concept of network polynomials as a multilinear representation of BNs over finitely many states [16], [25].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 24,
      "context" : "The work on NNFs led to the concept of network polynomials as a multilinear representation of BNs over finitely many states [16], [25].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : "In [26], ACs were learned by optimizing an objective trading off the log-likelihood on the training set and the inference cost of the AC, measured as the worst-case number of arithmetic operations required for inference (i.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "The learned models still represent BNs with context-specific independencies [27].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 27,
      "context" : "A similar approach learning Markov networks represented by ACs is followed in [28].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "SPNs were the first time proposed in [1], where the represented distribution was not defined via a background graphical model any more, but directly as the normalized output of the network.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "Structure learning algorithms not restricted to the image domain were proposed in [2], [3], [4], [29], [30], [31].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Structure learning algorithms not restricted to the image domain were proposed in [2], [3], [4], [29], [30], [31].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "Structure learning algorithms not restricted to the image domain were proposed in [2], [3], [4], [29], [30], [31].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : "Structure learning algorithms not restricted to the image domain were proposed in [2], [3], [4], [29], [30], [31].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 29,
      "context" : "Structure learning algorithms not restricted to the image domain were proposed in [2], [3], [4], [29], [30], [31].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 30,
      "context" : "Structure learning algorithms not restricted to the image domain were proposed in [2], [3], [4], [29], [30], [31].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 5,
      "context" : "Discriminative learning of SPNs, optimizing conditional likelihood, was proposed in [6].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 31,
      "context" : "In [32] two families of functions were identified which are efficiently representable by deep, but not by shallow SPNs, where an SPN is considered as shallow if it has no more than three layers.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "In [17] it was shown that SPNs can w.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "These results were independently found in [21].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "Furthermore, in [17], a sound derivation of inference mechanisms for generalized SPNs was given, i.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 20,
      "context" : "In [21], a BN representation of SPNs was found, where LVs associated with sum nodes and the model RVs are organized in a two layer bipartite structure.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 32,
      "context" : "Recently, the notion of SPNswas generalized to sumproduct functions over arbitrary semirings [33].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "As pointed out in [1], each sum node in an SPN can be interpreted as a marginalized LV, similar as in the GMM example in Section 1.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "In steps 13–15, we introduce IVs corresponding to the associated LV ZS, as proposed in [1].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "It is helpful to introduce the notion of configured SPNs, which takes a similar role as conditioning in the literature on DNNFs [22], [23], [24].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "It is helpful to introduce the notion of configured SPNs, which takes a similar role as conditioning in the literature on DNNFs [22], [23], [24].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 23,
      "context" : "It is helpful to introduce the notion of configured SPNs, which takes a similar role as conditioning in the literature on DNNFs [22], [23], [24].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "BNs [14], [15], i.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 14,
      "context" : "BNs [14], [15], i.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "One problem in the original LV interpretation [1] was, that no conditional distribution of ZS was specified for the complementary event.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "[21] can be recovered from the BN representation of augmented SPNs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "This special choice of twin weights effectively removes all edges between LVs, recovering the BN structure in [21].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "The EM algorithm is a general scheme for maximum likelihood learning, when for some RVs complete evidence is missing [10], [11].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "The EM algorithm is a general scheme for maximum likelihood learning, when for some RVs complete evidence is missing [10], [11].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "This approach was already pointed out in [1], where it was suggested that for evidence e and for any LV ZS, the marginal posteriors should be given as ppZS “ k | eq9wS,Ck S BSpeq BSpeq , which should be used for EM updates.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "Using the well-known EM-updates in BNs over discrete RVs [10], [15], the updates for sum-weight wk are given by summing over the expected statistics",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "Using the well-known EM-updates in BNs over discrete RVs [10], [15], the updates for sum-weight wk are given by summing over the expected statistics",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "To compute (16), we use the differential approach, [16], [17], [19], cf.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 16,
      "context" : "To compute (16), we use the differential approach, [16], [17], [19], cf.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "To compute (16), we use the differential approach, [16], [17], [19], cf.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "In [17], the so-called distribution selectors (DSs) were introduced to derive the differential approach for generalized SPNs.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "As shown in [17], X is rendered independent from all other RVs in the SPN when conditioned on WX .",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 33,
      "context" : "Note that recently Zhao and Poupart [35] derived a concave-convex procedure (CCCP) which yield the same sum-weight updates as the EM algorithm presented here and in [19].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "Note that recently Zhao and Poupart [35] derived a concave-convex procedure (CCCP) which yield the same sum-weight updates as the EM algorithm presented here and in [19].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "In [1], [4], [7], SPNs were applied for reconstructing data using MPE inference.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "In [1], [4], [7], SPNs were applied for reconstructing data using MPE inference.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "In [1], [4], [7], SPNs were applied for reconstructing data using MPE inference.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 34,
      "context" : "Both MPE and MAP are generally NP-hard in BNs [36], [37], [38], and MAP is inherently harder than MPE [37], [38].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 35,
      "context" : "Both MPE and MAP are generally NP-hard in BNs [36], [37], [38], and MAP is inherently harder than MPE [37], [38].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 36,
      "context" : "Both MPE and MAP are generally NP-hard in BNs [36], [37], [38], and MAP is inherently harder than MPE [37], [38].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 35,
      "context" : "Both MPE and MAP are generally NP-hard in BNs [36], [37], [38], and MAP is inherently harder than MPE [37], [38].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 36,
      "context" : "Both MPE and MAP are generally NP-hard in BNs [36], [37], [38], and MAP is inherently harder than MPE [37], [38].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : "Using the result in [18], it follows that MAP inference is NP-hard also in SPNs.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "In particular, Theorem 5 in [18] shows that the decision version of MAP is NP-complete for a Naive Bayes model, when the class variable is marginalized.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "A proof tailored to SPNs can be found in [19].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "1 and [20]), an MPE solution can be obtained using a Viterbi-style backtracking algorithm in max-product networks.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 37,
      "context" : "Note that Theorem 2 has already been derived for a special case, namely for arithmetic circuits representing network polynomials of BNs over discrete RVs [39].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "In [1], MPESELECTIVE was applied to original SPNs, not to augmented SPNs, but also with the goal to recover an",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "We can identify the maximizing states of those LVs whose sums are visited during backtracking, as in [1].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "Pseudo-code for this somewhat technical modification of MPESELECTIVE can be found in [19].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "We see that the algorithm used in [1] is essentially equivalent to MPESELECTIVE in augmented SPNs when w̃S,C “ 1 for all sum nodes, which implies that the twin-weights are Fig.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Therefore, although the LV model in [1] is not completely specified and it was not shown that the Viterbilike algorithm recovers an MPE solution, it nevertheless corresponds to MPE inference in the augmented SPN for special twin-weights, i.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "In [1], [40] SPNswere applied to image data, where a generic architecture reminiscent to convolutional neural networks was proposed.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 18,
      "context" : "Standard EM was not used in experiments for two reasons: First, explicitly constructing the proposed structure and to train it with standard EM is hardly possible with current hardware, since the number of nodes grows Oplq, where l is the square-length of the modeled image domain in pixels [19].",
      "startOffset" : 291,
      "endOffset" : 295
    }, {
      "referenceID" : 0,
      "context" : "1) Is our derivation of EM correct, both for complete and missing data? 2) Can the result of hard EM [1] be improved by standard EM? 3) Given a suited sparse structure, does EM yield a good solution for parameters?",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "We used the same datasets and SPN structures as in [1], obtainable from [40].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 38,
      "context" : "The datasets comprise Caltech-101 (inclusive background class) [42] and the ORL face images [43], i.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "Only parameters which are actually updated are initialized randomly; otherwise the original parameters [1] are used and kept fixed.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "As pointed out above, a hard EM variant was used in [1], [40] which at the same time finds the effective SPN structure.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Optimizing W using the 3 random initialization amounts to using the oracle structure obtained by [1], [40], discarding the learned parameters.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, as can be seen in Table 2, EM is also not more prone to overfitting than the algorithm in [1]: on 67.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "7) when applied to augmented SPNs, we generated SPNs using the PD architecture [1], arranging 4, 9 and 16 binary RVs in a 2ˆ2, 3ˆ3 and 4ˆ4 grid, respectively.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "As described in Section 4, MPEDET corresponds essentially to the result when MPESELECTIVE is applied to the original SPN [1].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "Using Lemma 1 in [17], it follows that Spx, z,yq “ 0.",
      "startOffset" : 17,
      "endOffset" : 21
    } ],
    "year" : 2016,
    "abstractText" : "One of the central themes in Sum-Product networks (SPNs) is the interpretation of sum nodes as marginalized latent variables (LVs). This interpretation yields an increased syntactic or semantic structure, allows the application of the EM algorithm and to efficiently perform MPE inference. In literature, the LV interpretation was justified by explicitly introducing the indicator variables corresponding to the LVs’ states. However, as pointed out in this paper, this approach is in conflict with the completeness condition in SPNs and does not fully specify the probabilistic model. We propose a remedy for this problem by modifying the original approach for introducing the LVs, which we call SPN augmentation. We discuss conditional independencies in augmented SPNs, formally establish the probabilistic interpretation of the sum-weights and give an interpretation of augmented SPNs as Bayesian networks. Based on these results, we find a sound derivation of the EM algorithm for SPNs. Furthermore, the Viterbi-style algorithm for MPE proposed in literature was never proven to be correct. We show that this is indeed a correct algorithm, when applied to selective SPNs, and in particular when applied to augmented SPNs. Our theoretical results are confirmed in experiments on synthetic data and 103 real-world datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}