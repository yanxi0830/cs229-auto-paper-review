{
  "name" : "1508.06904.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Theory for Rapid Exact Signal Scanning with Deep Multi-Scale Convolutional Neural Networks",
    "authors" : [ "Markus Thom", "Franz Gritschneder" ],
    "emails" : [ "markus.thom@uni-ulm.de)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nInspired by findings on the structure of mammalian visual cortex [1], the concept of spatial weight sharing has been integrated into artificial neural networks to achieve an architecture nowadays broadly known as Convolutional Neural Network (CNN) [2], [3]. This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.\nDetection of objects in images can, for example, be realized by application of an object classifier at each image position that may contain an object of interest. If this is carried out for each feasible image position, it is possible to assign class membership estimations to all the pixels in an image resulting in a dense description of all objects in a scene [11], [12], [13]. While the computational complexity of a sophisticated classification system used in conjunction with a sliding window approach may seem excessive at first glance, the special structure of a CNN can be exploited so that intermediate computation results can be shared among adjacent image windows, resulting in a speed-up of several orders of magnitude compared to the naive approach. Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18]. However, all these methods have in common that it is not inherently clear that they actually produce the desired results, since instead of a satisfactory proof of correctness only toy examples are presented.\nM. Thom and F. Gritschneder are with driveU / Institute of Measurement, Control and Microtechnology, Ulm University, 89081 Ulm, Germany (e-mail: markus.thom@uni-ulm.de).\nThis paper improves upon previous work by establishing a comprehensive theory of subsignal compatible transformations: these form a family of functions that exactly fulfill the invariants required for a sound sliding window approach. We provide a characterization of such transformations, show that their compositions fulfill the same invariants and demonstrate how the developed theory connects with CNNs. Further, we analyze under which circumstances the information from a multi-scale image representation can be incorporated. Our results are proven rigorously and can be used directly for an efficient implementation on massively parallel processing systems.\nThe remainder of this paper is structured as follows. In Sect. II, we give an introduction to the CNN structure, fix the notation and introduce what we mean by subsignals. Section III establishes the basics of our theory on subsignal compatible transformations and shows how the major building blocks of CNNs fit into the theory. In the following Sect. IV we extend the theory to functions applied in a strided fashion, which is particularly important for pooling operators evaluated on non-overlapping blocks. Section V considers multiscale transformations, that is functions that depend on signals made available in different spatial resolutions. The paper is concluded with a discussion of our results in Sect. VI."
    }, {
      "heading" : "II. PREREQUISITES",
      "text" : "In this section, we start by formally introducing the building blocks of a CNN. Next, we fix the notation used throughout the paper. The section is concluded by the formal definition of the subsignal extraction operator and a few statements on its properties."
    }, {
      "heading" : "A. Convolutional Neural Networks",
      "text" : "Ordinary CNNs process input data by means of specialized layers [19]. Convolutional layers respect the weight sharing principle: they convolve their input with a filter bank which is learned during an adaptation process and add a trainable scalar bias to form the layer output. A mathematical definition is given in Sect. III-C.\nPooling layers strengthen a network’s invariance to small translations of the input data by evaluation of a fixed pooling kernel followed by a downsampling operation. Effectively, this is the same as the application of a function in a strided fashion, that is, merely certain equidistant subsignals are considered which are not required to start at exactly neighboring samples. This is discussed in greater detail in Sect. IV.\nBesides these layers, there are also fully-connected layers and non-linearity layers. The former are just a special case\nar X\niv :1\n50 8.\n06 90\n4v 1\n[ cs\n.L G\n] 2\n7 A\nug 2\n01 5\n2 of convolutional layers in that they carry out a convolution with unit kernel size. The latter send each sample of a larger signal through a scalar transfer function independently, which may for example be a hyperbolic tangent or a rectification nonlinearity [20].\nIt is further possible to improve performance through incorporation of context information using multi-scale analysis so that the network sees image regions in different spatial resolutions [13]. We will elaborately consider this approach in Sect. V."
    }, {
      "heading" : "B. Notation",
      "text" : "For the sake of simplicity, we restrict our analysis to vector-shaped signals. The generalization of our results to more complex signals like images with pixels arranged on a two-dimensional grid is straightforward by application of the theory to appropriate indices of the images.\nWe write N1 := N \\ { 0 } for the positive natural numbers and Z for the integers. The ceiling function that rounds up its argument to the next larger natural number is denoted by d·e. If M is a set and q ∈ N1, then Mq denotes the set of all q-tuples with entries from M . The elements of Mq are called signals, their q entries are called samples. If ξ = (ξ1, . . . , ξq) ∈ Mq and I ∈ {1, . . . , q}r is an index list with r entries, we use the formal sum ω := ∑r ν=1 ξIν · erν for the element ω ∈ Mr with ων = ξIν for all ν ∈ {1, . . . , r}. For example, when M = R and hence Mr is the r-dimensional Euclidean space, then the formal sum ω corresponds to the linear combination of canonical basis vectors erν weighted with selected coordinates of the signal ξ.\nFor ξ ∈Mq we write dimM (ξ) = q for the dimensionality of ξ. This does not need to correspond exactly with the concept of dimensionality in the sense of linear algebra. If for example M = Nc for categorical data with c ∈ N1 features, then Mq is not a vector space over M . The theory presented in this paper requires algebraic structures such as vector spaces or analytic structures such as the real numbers only for certain specialized statements. The bulk of our results hold for all signals from arbitrary sets.\nIf M is a set and c ∈ N1 is a natural number, we write ∪c(M) := ∪∞q=cMq for the set that contains all the tuples of length greater than or equal to c with entries from M . For example, if ξ ∈ ∪c(M) then there is a natural number q ≥ c so that ξ = (ξ1, . . . , ξq) with ξν ∈ M for all ν ∈ {1, . . . , q}. Note that ∪1(M) puts no restriction on tuple length except for positivity."
    }, {
      "heading" : "C. Division of a Signal into Subsignals",
      "text" : "A subsignal is a contiguous list of samples contained in a larger signal. Let us first formalize the concept of extracting subsignals with a fixed number of samples from a given signal:\nξ1\nξ2\nξ3\nξ4\nξ5\nξ6\nξ7\nξ8\nSubsignal4(ξ, 1)\nSubsignal4(ξ, 5)\nξ1\nξ2\nξ3\nξ4\nξ5\nξ6\nξ7\nξ8\nFig. 1. Illustration of the subsignal extraction operator applied to a signal ξ with eight samples for extraction of subsignals with four samples each.\nDefinition 1. Let M be a set and let d ∈ N1 denote a fixed subsignal dimensionality. Then\nSubsignald : ∞⋃ D=d ( MD × {1, . . . , D − d+ 1} ) →Md,\n(ξ, i) 7→ d∑ ν=1 ξi+ν−1 · edν ,\nis called the subsignal extraction operator.\nIt is straightforward to verify that Subsignald is welldefined and actually returns all possible D−d+ 1 contiguous subsignals of length d from a given signal with D samples (see Fig. 1). Note that for application of this operator it has always to be ensured that the requested subsignal index i is within bounds.\nIterated extraction of subsignals can be collapsed into one operator evaluation:\nLemma 2. Let M be a set and c, d ∈ N1, c ≤ d, be two subsignal dimensionalities. Then\nSubsignalc(Subsignald(ξ, i), j) = Subsignalc(ξ, i+ j − 1)\nfor all ξ ∈ ∪c(M), for all i ∈ {1, . . . ,dimM (ξ)− d+ 1} and for all j ∈ {1, . . . , d− c+ 1}.\nProof. The subsignal indices of the left-hand side are well within bounds. Since i + j − 1 ∈ {1, . . . ,dimM (ξ) − c + 1} this also holds for the right-hand side. We find that\nSubsignalc(Subsignald(ξ, i), j)\nD. 1 = c∑ λ=1 Subsignald(ξ, i)j+λ−1 · ecλ\nD. 1 = c∑ λ=1 ( d∑ ν=1 ξi+ν−1 · edν ) j+λ−1 · ecλ\n(♦) = c∑ λ=1 ξ(i+j−1)+λ−1 · ecλ D. 1 = Subsignalc(ξ, i+ j − 1),\nwhere in the (♦) step we have substituted ν = j + λ− 1.\n3"
    }, {
      "heading" : "III. SUBSIGNAL COMPATIBLE TRANSFORMATIONS",
      "text" : "This section introduces the concept of subsignal compatible transformations. These are functions that can be applied to an entire signal at once and then yield the same result as if they were applied to each subsignal independently. We show that functions applied in a sliding fashion can be grasped as subsignal compatible transformations, and that the composition of subsignal compatible transformations is again a subsignal compatible transformation.\nIn the end of this section, we consider Convolutional Neural Networks without pooling layers and demonstrate that these satisfy the requirements of subsignal compatible transformations. As a consequence, CNNs without pooling layers can be applied to the whole input signal at once without having to handle individual subsignals.\nWe begin with the major definition of this section:\nDefinition 3. Let M and N be sets, let c ∈ N1 be a positive natural number, and let T : ∪c (M)→ ∪1(N) be a function. We then call T a subsignal compatible transformation with dimensionality reduction constant c if and only if these two properties hold:\n(i) Dimensionality reduction property (DRP): dimN (T (ξ)) = dimM (ξ)− c+ 1 for all ξ ∈ ∪c(M).\n(ii) Exchange property (XP): For all subsignal dimensionalities d ∈ N1, d ≥ c, it holds that T (Subsignald(ξ, i)) = Subsignald−c+1(T (ξ), i) for all ξ ∈ ∪d(M) and all i ∈ {1, . . . ,dimM (ξ)−d+1}.\nThe first property guarantees that T reduces the dimensionality of its argument always by the same amount regardless of the input dimensionality. The second property states that if T was applied to a subsignal, then this is the same as applying T to the entire signal and afterwards extracting the appropriate samples from the resulting signal. Figure 2 gives an example for these concepts.\nWe note that the exchange property is well-defined: The dimensionality reduction property guarantees that the dimensionalities on both sides of the equation match. Further, the subsignal index i is within bounds on both sides. This is trivial for the left-hand side, and can be seen for the right-hand side since dimM (ξ)−d+1 = (dimM (ξ)−c+1)− (d−c+1)+1.\nWe immediately have an identity theorem for subsignal compatible transformations:\nTheorem 4. Let M,N be sets and T1, T2 : ∪c (M)→ ∪1(N) two subsignal compatible transformations with dimensionality reduction constant c ∈ N1. If T1(ρ) = T2(ρ) holds for all ρ ∈M c, then already T1 = T2.\nProof. Let ξ ∈ ∪c(M). For µ ∈ {1, . . . ,dimM (ξ)−c+1} we yield with the precondition (PC) and the exchange property where the subsignal dimensionality is set to c:\nT1(ξ)µ XP = T1 (Subsignalc(ξ, µ)) PC = T2 (Subsignalc(ξ, µ)) XP = T2(ξ)µ.\nHence all the samples of the transformed signals are equal, thus T1(ξ) = T2(ξ) for all ξ in the domain of T1 and T2."
    }, {
      "heading" : "A. Relationship between Functions Applied in a Sliding Fashion and Subsignal Compatible Transformations",
      "text" : "We now investigate functions which are applied to a signal in a sliding fashion. Let us first define what is meant hereby:\nDefinition 5. Let M and N be sets, let c ∈ N1 be a positive natural number and let f : M c → N be a function. Then\nSlidef : ∪c (M)→ ∪1(N),\nξ 7→ dimM (ξ)−c+1∑\ni=1\nf (Subsignalc(ξ, i)) · e dimM (ξ)−c+1 i ,\nis the operator that applies f in a sliding fashion to arbitrary signals.\nThe next result states that functions applied in a sliding fashion are essentially the same as subsignal compatible transformations, and that the exchange property could be weakened to hold only for the case where the dimensionality reduction constant equals the subsignal dimensionality:\nTheorem 6. Let M and N be sets, let c ∈ N1 and let T : ∪c (M) → ∪1(N) be a function. Then the following are equivalent:\n(a) T is a subsignal compatible transformation with dimensionality reduction constant c. (b) T fulfills the dimensionality reduction property, and for all ξ ∈ ∪c(M) and all i ∈ {1, . . . ,dimM (ξ) − c + 1} it holds that T (Subsignalc(ξ, i)) = T (ξ)i. (c) There exists a function f : M c → N with T = Slidef .\nProof. (a) ⇒ (b): Trivial, since the dimensionality reduction property is fulfilled by definition, and the claimed condition is only the special case of the exchange property where d = c.\n4 (b) ⇒ (c): Define f : M c → N , ξ 7→ T (ξ). For ξ ∈M c we have dimN (T (ξ)) = 1 due to the dimensionality reduction property, therefore f is well-defined. Now let ξ ∈ ∪c(M) and define D := dimM (ξ). It is clear that dimN (T (ξ)) = dimN (Slidef (ξ)) = D − c + 1. Let i ∈ {1, . . . , D − c + 1}, then the precondition (PC) implies\nSlidef (ξ)i D. 5 = f (Subsignalc(ξ, i))\n= T (Subsignalc(ξ, i)) PC = T (ξ)i,\nhence T = Slidef . (c) ⇒ (a): Let us suppose that T = Slidef for a function f : M c → N . Slidef inherently fulfills the dimensionality reduction property. Let d ∈ N1, d ≥ c, be an arbitrary subsignal dimensionality and let ξ ∈ ∪d(M) be a signal. Further, let i ∈ {1, . . . ,dimM (ξ)−d+1} be an arbitrary subsignal index. Remembering that dimM (Subsignald(ξ, i)) = d and using Lemma 2 we have\nSlidef (Subsignald(ξ, i))\nD. 5 = d−c+1∑ j=1 f (Subsignalc(Subsignald(ξ, i), j)) · ed−c+1j\nL. 2 = d−c+1∑ j=1 f (Subsignalc(ξ, i+ j − 1)) · ed−c+1j\nD. 5 = d−c+1∑ j=1 Slidef (ξ)i+j−1 · ed−c+1j\nD. 1 = Subsignald−c+1(Slidef (ξ), i),\nthus the exchange property is satisfied as well.\nThe set of subsignal compatible transformations is vast. There are, however, transformations that are not subsignal compatible:\nExample 7. As a counterexample, let c ∈ N1 be arbitrary and consider the function T : ∪c (M)→ ∪1(N),\nξ 7→ dimM (ξ)−c+1∑\nν=1\nξdimM (ξ)−ν+1 · e dimM (ξ)−c+1 ν ,\nwhich just reverses the final entries of its argument. The dimensionality reduction property is clearly satisfied by T . Let ξ ∈ ∪c(M) and i ∈ {1, . . . ,dimM (ξ)− c+ 1}, then we have T (Subsignalc(ξ, i)) = ξc+i−1 but T (ξ)i = ξdimM (ξ)−i+1. As these two expressions cannot be equal for all ξ ∈ ∪c(M) unless M is a singleton, T cannot be a subsignal compatible transformation by Theorem 6."
    }, {
      "heading" : "B. Composition of Subsignal Compatible Transformations",
      "text" : "The composition of subsignal compatible transformations is again a subsignal compatible transformation:\nTheorem 8. Let M , N and P be sets and let c1, c2 ∈ N1. Suppose T1 : ∪c1 (M) → ∪1(N) is a subsignal compatible transformation with dimensionality reduction constant c1, and T2 : ∪c2 (N)→ ∪1(P ) is a subsignal compatible transformation with dimensionality reduction constant c2.\nDefine c := c1 + c2 − 1 ∈ N1. Then T : ∪c (M)→ ∪1(P ), ξ 7→ T2(T1(ξ)), is a subsignal compatible transformation with dimensionality reduction constant c.\nProof. We first note that c ≥ 1 since c1 ≥ 1 and c2 ≥ 1, hence indeed c ∈ N1. Let ξ ∈ ∪c(M) be arbitrary for demonstrating that T is well-defined. As c ≥ c1 because of c2 ≥ 1, we yield ∪c(M) ⊆ ∪c1(M) and hence T1(ξ) is well-defined. Further, we yield dimN (T1(ξ)) = dimM (ξ)−c1 +1 ≥ c−c1 +1 = c2 using the dimensionality reduction property of T1, therefore T1(ξ) ∈ ∪c2(N). Thus T2(T1(ξ)) is well-defined, and so is T .\nFor all ξ ∈ ∪c(M), the dimensionality reduction property of T1 and T2 now implies\ndimP (T (ξ)) = dimP (T2(T1(ξ))) DRP = dimN (T1(ξ))− c2 + 1\nDRP = dimM (ξ)− c1 + 1− c2 + 1 = dimM (ξ)− c+ 1,\ntherefore T fulfills the dimensionality reduction property. Let d ∈ N1, d ≥ c, be arbitrary, and let ξ ∈ ∪d(M) and i ∈ {1, . . . ,dimM (ξ) − d + 1}. Since T1 and T2 satisfy the exchange property we find\nT (Subsignald(ξ, i))\n= T2(T1(Subsignald(ξ, i))) XP = T2(Subsignald−c1+1(T1(ξ, i))) XP = Subsignald−c1+1−c2+1(T2(T1(ξ)), i) = Subsignald−c+1(T (ξ), i),\nwhere d ≥ c1 and d − c1 + 1 ≥ c2 hold during the two respective applications of the exchange property. Therefore, T also fulfills the exchange property.\nThis result can be generalized immediately to compositions of more than two subsignal compatible transformations:\nCorollary 9. Let n ∈ N, n ≥ 2, and let M1, . . . ,Mn+1 be sets. For each λ ∈ {1, . . . , n} let Tλ : ∪cλ (Mλ)→ ∪1(Mλ+1) be a subsignal compatible transformation with dimensionality reduction constant cλ ∈ N1. Then the composed function T : ∪c (M1) → ∪1(Mn+1), ξ 7→ ( ◦λ=n1 Tλ ) (ξ), is a subsignal compatible transformation with dimensionality reduction constant c := ∑n µ=1 cµ − n+ 1 ∈ N1.\nProof. Define S1 := T1, and for each λ ∈ {2, . . . , n} let Sλ : ∪∑λ\nµ=1 cµ−λ+1 (M1) → ∪1(Mλ+1), ξ 7→ Tλ(Sλ−1(ξ)),\nbe a function. Since T = Sn, the claim follows when it is shown with induction for λ that Sλ is a subsignal compatible transformation with dimensionality reduction constant∑λ µ=1 cµ − λ + 1. While the situation λ = 1 is trivial, the induction step follows with Theorem 8."
    }, {
      "heading" : "C. CNNs without Pooling Layers",
      "text" : "The previous parts of this section were quite abstract. We now demonstrate how Convolutional Neural Networks without any pooling layers fit in the theory developed so far. Pooling layers require more preparations and are detailed in Sect. IV.\n5 The convolution operation is the substance of a CNN. Here, multi-channel input feature maps are convolved channelwise with previously learned filter banks, and the result is accumulated and a trainable bias added to yield the output feature map.\nWe start with introducing indexing rules for iterated structures to account for the multi-channel nature of the occurring signals. Let M be a set, a, b ∈ N1 positive natural numbers and ξ ∈ (Ma)b a multi-channel signal. It is then ξj ∈Ma for indices j ∈ {1, . . . , b}, and moreover (ξj)i ∈ M for indices j ∈ {1, . . . , b} and i ∈ {1, . . . , a}. This rule is extended in the natural way for sets written explicitly as products with more than two factors. Therefore, if ξ ∈ ((Ma)b)c for another number c ∈ N1, then for example (ξk)j ∈ Ma for indices k ∈ {1, . . . , c} and j ∈ {1, . . . , b}.\nThese rules will become more clear if we consider the multi-channel convolution operation ∗. Suppose the samples are members of a ring R, m ∈ N1 denotes the number of input channels, n ∈ N1 is the number of output channels, and c ∈ N1 equals the number of samples considered at any one time during convolution with the filter bank. Then input signals or feature maps with D ∈ N1 samples are of the form ξ ∈ (Rm)D, and filter banks can be represented by a tensor w ∈ ((Rn)m)c. We must have that D ≥ c, that is the filter kernel should be smaller than the input signal.\nThe output feature map (ξ ∗ w) ∈ (Rn)D−c+1 is then\n(ξ ∗ w)i := m∑ λ=1 c∑ µ=1 (wµ)λ · (ξc+i−µ)λ ∈ Rn\nfor indices i ∈ {1, . . . , D− c+1}. Note that (wµ)λ ∈ Rn and (ξc+i−µ)λ ∈ R, so that the result of their product is understood here as scalar product. The operation is well-defined since c + i − µ ∈ {1, . . . , D}, which follows immediately through substitution of the extreme values of i and µ.\nExample 10. The multi-channel convolution operation is a subsignal compatible transformation: Define M := Rm and N := Rn and consider\nfconv : M c → N ,\nξ 7→ m∑ λ=1 c∑ µ=1 (wµ)λ · (ξc−µ+1)λ.\nSince µ ∈ {1, . . . , c} it is c−µ+1 ∈ {1, . . . , c}, hence fconv is well-defined. For all ξ ∈MD and any i ∈ {1, . . . , D− c+ 1} follows\nSlidefconv(ξ)i D. 5 = fconv(Subsignalc(ξ, i))\nD. 1 = fconv\n( c∑\nν=1\nξi+ν−1 · ecν\n)\n= m∑ λ=1 c∑ µ=1 (wµ)λ · ( c∑ ν=1 ξi+ν−1 · ecν ) c−µ+1  λ\n(♦) = m∑ λ=1 c∑ µ=1 (wµ)λ · (ξi+c−µ+1−1)λ\n= (ξ ∗ w)i,\nwhere ν = c − µ + 1 was substituted in the (♦) step. The convolution operation as defined above is hence in fact the application of fconv in a sliding fashion. Therefore, Theorem 6 guarantees that ∗ is a subsignal compatible transformation with dimensionality reduction constant c.\nSince fully-connected layers are merely a special case of convolutional layers, these do not need any special treatment here. Addition of biases does not require any knowledge on the spatial structure of the convolution’s result and is therefore a subsignal compatible transformations with dimensionality reduction constant 1. Non-linearity layers are nothing but the application of a scalar-valued function to all the samples of an input signal. Hence these layers form also subsignal compatible transformations with dimensionality reduction constant 1 for all non-empty subsignals due to Theorem 6.\nFurthermore, compositions of these operations can also be understood as subsignal compatible transformation with Corollary 9. As a consequence, the exchange property facilitates application of a Convolutional Neural Networks without pooling layers to an entire signal at once instead of each subsignal independently without changing the actual output. The next section will extend this result to CNNs that also feature pooling layers."
    }, {
      "heading" : "IV. POOLING LAYERS AND FUNCTIONS APPLIED IN A STRIDED FASHION",
      "text" : "So far we have shown how convolutional layers and nonlinearity layers of a Convolutional Neural Network fit in the framework of subsignal compatible transformations. In this section, we analyze pooling layers which apply pooling kernels in a strided fashion. This is equivalent to functions applied in a sliding fashion followed by a downsampling operation. The theory developed herein can of course also be applied to other functions than pooling kernels, for example to strided multichannel convolutions using the results from Sect. III-C.\nWe will demonstrate how these functions can be turned into subsignal compatible transformations using a data structure recently introduced as fragmentation [15]. Here, we will greatly generalize the method proposed by [15] and rigorously prove the correctness of the approach. As a side effect of our results, we are able to accurately describe the dynamics of the entire execution chain, which also includes the possibility of tracking down the position of each processed subsignal in the fragmentation data structure.\nMoreover, we analyze under which circumstances the fragment dimensionalities are guaranteed to always be homogeneous. This is a desirable property as it facilitates the application of subsequent operations to signals which all have the same number of samples, rendering cumbersome handling of special cases obsolete and thus resulting in accelerated execution on massively parallel processors.\nWe first state more precisely what the application of a function in a strided fashion means (see Fig. 3 for orientation):\n6 ξ1\nξ2\nξ3\nξ4\nξ5\nξ6\ng\ng\nStrideg(ξ)1 = g(ξ1, ξ2, ξ3)\nStrideg(ξ)2 = g(ξ4, ξ5, ξ6)\nFig. 3. Illustration of the application of a function in a strided fashion to a signal with six samples. The function g maps three samples to one, it is here evaluated on non-overlapping subsignals extracted from the original signal ξ.\nDefinition 11. Let M and N be sets, let k ∈ N1 be a positive natural number and let g : Mk → N be a function. Then\nStrideg : ∪∞q=1 Mkq → ∪1(N), ξ 7→ dimM (ξ)/k∑\ni=1\ng (Subsignalk(ξ, k(i− 1) + 1)) · e dimM (ξ)/k i ,\nis the operator that applies g in a strided fashion to signals where the number of samples is a multiple of k.\nSince it is k(i − 1) + 1 ∈ {1, . . . ,dimM (ξ) − k + 1} for all i ∈ {1, . . . ,dimM (ξ)/k}, Strideg is well-defined. We further have dimM (ξ)/dimN (Strideg(ξ)) = k for all ξ in the domain of Strideg . Since the input dimensionality is here reduced through division with a natural number rather than a subtraction, the dimensionality reduction property cannot be fulfilled unless k = 1. The situation in which k = 1 is, however, not interesting since then Strideg = Slideg which was already handled in Sect. III.\nBefore continuing with the fragmentation data structure, let us discuss an example:\nExample 12. Suppose we want to process real-valued signals with m ∈ N1 channels, that is M = N = Rm, where each channel should be processed independently of the others. Average pooling is then realized by the pooling kernel gavg(ξ) := 1k ∑k ν=1 ξν , which determines the channelwise empirical mean value of the samples. Another example is max-pooling, where the maximum entry in each channel is sought. This can be computed with the pooling kernel gmax(ξ) := ∑m λ=1 ( maxkν=1(ξν)λ ) · emλ . Finally, assume nonunit stride convolutions should be carried out. This is equivalent to performing a conventional convolution followed by strided application of the pooling kernel gproj(ξ) := ξ1, which just projects a given signal onto its first sample."
    }, {
      "heading" : "A. Fragmentation",
      "text" : "The fragmentation operator performs a special reordering operation. For its precise analysis, we need to recap some elementary number theory. For all numbers a ∈ N and b ∈ N1, Euclidean division guarantees that there are unique numbers div(a, b) ∈ N and rem(a, b) ∈ {0, . . . , b− 1} so that\na = div(a, b) · b+ rem(a, b).\nHere is a small collection of results on these operators for further reference:\nProposition 13. It is div(a, 1) = a and rem(a, 1) = 0 for all a ∈ N. Moreover, div(a + bc, c) = div(a, c) + b and rem(a+ bc, c) = rem(a, c) for all a, b ∈ N and c ∈ N1.\nIf the fragmentation operator is applied to a signal, it puts certain samples into individual fragments, which can be grasped as signals themselves. If a collection of fragments is again fragmented, a larger collection of fragments results. The total number of samples is, however, left unchanged after these operations. For the sake of convenience, we will here use matrices as data structure for fragmented signals, where columns correspond to fragments and rows correspond to signal samples, see Fig. 4.\nLet us fix some notation. If M is a set and a, b ∈ N1, then Ma×b denotes the set of all matrices with a rows and b columns with entries from M . For ξ ∈ Ma×b we write rdimM (ξ) = a and cdimM (ξ) = b. Further, ξi, j is the entry in the i-th row and j-th column of ξ where i ∈ {1, . . . , a} and j ∈ {1, . . . , b}. The transpose of ξ is written as ξT .\nThe vectorization operator [21] stacks all the columns of a matrix on top of another:\nDefinition 14. Let M be a set and a, b ∈ N1. The vectorization operator veca×b : Ma×b → Mab is characterized by veca×b(ξ)j = ξrem(j−1, a)+1, div(j−1, a)+1 for all indices j ∈ {1, . . . , ab} and all matrices ξ ∈ Ma×b. The inverse vectorization operator vec−1a×b : M\nab → Ma×b is given by vec−1a×b(ξ)i, j = ξ(j−1)a+i for all indices i ∈ {1, . . . , a}, j ∈ {1, . . . , b} and all vectors ξ ∈Mab.\nIt is straightforward to verify that these two operators are well-defined and inversely related to one another. With their help we may now define the fragmentation operator:\nDefinition 15. Let M be a set and k ∈ N1. For arbitrary vector dimensionalities q ∈ N1 and numbers of input fragments s ∈ N1 we write\nFragk : M kq×s →Mq×ks, ξ 7→ ( vec−1ks×q ( vecs×kq ( ξT )))T ,\nfor the fragmentation operator.\nHere, k equals the corresponding parameter from the application of a function in a strided fashion. Fragk is clearly well-defined, and the number of output fragments is ks. An illustration of the operations performed during fragmentation is depicted in Fig. 4. We note that fragmentation is merely a certain reordering operation:\nLemma 16. Let M be a set, k, q, s ∈ N1 and ξ ∈Mkq×s. It is then rdimM (Fragk(ξ)) = 1 k ·rdimM (ξ), cdimM (Fragk(ξ)) = k · cdimM (ξ), and\nFragk(ξ)µ, ν = ξdiv((µ−1)ks+ν−1, s)+1, rem((µ−1)ks+ν−1, s)+1\nfor all indices µ ∈ {1, . . . , q} and ν ∈ {1, . . . , ks}.\nProof. The dimensionality statements are obvious by the definition of Fragk. To prove the identity, let µ ∈ {1, . . . , q}\n7 ξ1\nξ2\nξ3\nξ4\nξ5\nξ6\nξ7\nξ8\nξ9\nξ10\nξ11\nξ12\nξ1\nξ4\nξ7\nξ10\nξ2\nξ5\nξ8\nξ11\nξ3\nξ6\nξ9\nξ12\nξ1\nξ7\nξ2\nξ8\nξ3\nξ9\nξ4\nξ10\nξ5\nξ11\nξ6\nξ12\nFrag3\nFrag2\nDefrag6\nFig. 4. Fragmentation operator with size three is applied to a signal with 12 = 2 · 2 · 3 samples, followed by another fragmentation with size two. Defragmentation with size 6 = 3 · 2 ultimately yields the original signal.\nand ν ∈ {1, . . . , ks}. Using the definition of the vectorization operator we yield\nFragk(ξ)µ, ν = vec−1ks×q ( vecs×kq ( ξT )) ν, µ = vecs×kq ( ξT ) (µ−1)ks+ν\n= ( ξT ) rem((µ−1)ks+ν−1, s)+1, div((µ−1)ks+ν−1, s)+1 ,\nand the claim follows.\nNext consider this operator that undoes the ordering of the fragmentation operator:\nDefinition 17. Let M be a set, let k ∈ N1, and let q ∈ N1 denote a vector dimensionality and s ∈ N1 a number of output fragments. Then\nDefragk : M q×ks →Mkq×s, ξ 7→ ( vec−1s×kq ( vecks×q ( ξT )))T ,\nis called the defragmentation operator.\nWe note that Defragk is well-defined and the number of input fragments must equal ks. Fragmentation and defragmentation are inversely related, that is Defragk ◦Fragk = idMkq×s and Fragk ◦Defragk = idMq×ks, see also Fig. 4. These properties of the defragmentation operator will be used later:\nLemma 18. Let M be a set. Let k, q, s ∈ N1 be positive natural numbers and ξ ∈Mq×ks a fragmented signal. We have that rdimM (Defragk(ξ)) = k · rdimM (ξ), cdimM (Defragk(ξ)) = 1 k · cdimM (ξ), and\nDefragk(ξ)µ, ν\n= ξdiv((µ−1)s+ν−1, ks)+1, rem((µ−1)s+ν−1, ks)+1\nfor all indices µ ∈ {1, . . . , kq}, ν ∈ {1, . . . , s}.\nProof. Completely analogous to Lemma 16.\nAs already outlined in Fig. 4, compositions of the fragmentation operator are equivalent to a single fragmentation operator with an adjusted parameterization:\nRemark 19. Let M be a set and k1, k2, q, s ∈ N1. Then Fragk2(Fragk1(ξ)) = Fragk1k2(ξ) for all ξ ∈M k1k2q×s.\nProof. Let ξ ∈ Mk1k2q×s be a fragmented signal. We define A := Fragk1(ξ) ∈ M k2q×k1s, B := Fragk2(A) ∈ M q×k1k2s, and C := Fragk1k2(ξ) ∈ M q×k1k2s. Since B and C are of equal size, it is enough to show entry-wise equivalence. Let µ ∈ {1, . . . , q} and ν ∈ {1, . . . , k1k2s}. With Lemma 16 we have that Cµ, ν = ξµC , νC and Bµ, ν = AµB , νB = ξµA, νA using the indices\nµC := div((µ− 1)k1k2s+ ν − 1, s) + 1, νC := rem((µ− 1)k1k2s+ ν − 1, s) + 1, µB := div((µ− 1)k1k2s+ ν − 1, k1s) + 1, νB := rem((µ− 1)k1k2s+ ν − 1, k1s) + 1, µA := div((µB − 1)k1s+ νB − 1, s) + 1, νA := rem((µB − 1)k1s+ νB − 1, s) + 1.\nWe thus only have to show that µC = µA and νC = νA. It is\nµA = div( div((µ− 1)k1k2s+ ν − 1, k1s) · k1s + rem((µ− 1)k1k2s+ ν − 1, k1s), s) + 1,\nwhich equals µC since div(a, b) · b + rem(a, b) = a for all a, b. Completely analogous follows νA = νC .\nIt follows immediately that fragmentation is a commutative operation:\nRemark 20. If M denotes a set, k1, k2, q, s ∈ N1 are natural numbers and ξ ∈ Mk1k2q×s is a fragmented signal, then Fragk2(Fragk1(ξ)) = Fragk1(Fragk2(ξ)).\nProof. Obvious with Remark 19 as multiplication in N1 is commutative."
    }, {
      "heading" : "B. Relationship between Fragmentation, Functions Applied in a Strided Fashion and Subsignal Compatible Transformations",
      "text" : "We are almost ready for analyzing how functions applied in a strided fashion fit into the theory developed so far. The outcome of a subsignal compatible transformation applied to a fragmented signal is defined in the natural way:\nDefinition 21. Let M,N be sets and T : ∪c (M) → ∪1(N) a subsignal compatible transformation with dimensionality reduction constant c ∈ N1. Let ξ ∈ MD×s be a fragmented signal with D ∈ N1 samples in each of the s ∈ N1 fragments, where D ≥ c. For λ ∈ {1, . . . , s} we write ξ(λ) := ∑D ν=1 ξν, λ · eDν ∈ MD for the individual fragments. The output of T applied to ξ is then defined as\nT (ξ) := [T (ξ(1)) , . . . , T (ξ(s))] ∈ N (D−c+1)×s,\nthat is T is applied to all the fragments independently.\n8 Let us now formally introduce the concept of a processing chain, which captures and generalizes all the dynamics of a Convolutional Neural Network:\nDefinition 22. We call a collection of the following objects a processing chain: A fixed subsignal dimensionality B ∈ N1, a number of layers L ∈ N1, a sequence of sets M0, . . . ,ML, N1, . . . , NL, and for each j ∈ {1, . . . , L} subsignal compatible transformations Tj : ∪cj (Mj−1)→ ∪1(Nj) with dimensionality reduction constant cj ∈ N1 and functions gj : N kj j →Mj where kj ∈ N1. For j ∈ {0, . . . , L} we define EvalStridej : M B 0 → ∪1(Mj),\nρ 7→ { ρ, if j = 0, Stridegj (Tj(EvalStridej−1(ρ))), if j > 0,\nas the function that applies the processing chain in a strided fashion, and EvalSlidej : ∪B (M0)→ ∪1(Mj),\nξ 7→ { ξ, if j = 0, Fragkj (Slidegj (Tj(EvalSlidej−1(ξ)))), if j > 0,\nas the function that applies the processing chain in a sliding fashion. We note that these two chains of functions are not well-defined unless additional divisibility conditions are fulfilled, detailed below.\nThe number B here represents the extent of the region of interest that is fed into a Convolutional Neural Network. The functions Tj can be substituted with the types of layers discussed earlier, like convolutions or non-linearities. Pooling layers and other functions applied in a strided fashion can be plugged into a processing chain via the gj functions.\nThe EvalStride operator corresponds to the ordinary application of a CNN to an individual sample until an arbitrary layer. It is defined recursively, where in the recursion step the output of the previous layer is fed through a subsignal compatible transformation and then through a function applied in a strided fashion. The EvalSlide operator differs from this approach in that multiple overlapping samples are processed in one go. Here, the gj functions are applied in a sliding rather than a strided fashion, followed by a fragmentation operation.\nThis is much more efficient than the extraction of individual subsignals followed by ordinary CNN application since redundant computations are effectively prevented. An example of the concepts just introduced is given in Fig. 5. It remains, however, to be shown that this method actually produces the desired results, or in other words the correctness of the approach has to be proved.\nThe next result states under which circumstances the application of a processing chain is well-defined, and connects the two flavors of processing chain application with another:\nLemma 23. Suppose we are given a processing chain with the same notation as in Definition 22. Assume kj divides dimNj (Tj(EvalStridej−1(ρ))) for all j ∈ {1, . . . , L} and all ρ ∈ MB0 , that is the application of the processing chain in a strided fashion should be well-defined.\nLet k∗j := ∏j ν=1 kν for j ∈ {0, . . . , L} denote the stride products, which implies that k∗0 = 1. Let D ∈ N1, D ≥ B,\nbe a signal dimensionality so that the number of subsignals D−B+ 1 of length B is divisible by k∗L, and let ξ ∈MD0 be the considered signal. Then the application of the processing chain in a sliding fashion to ξ is well-defined, and we can yield additional statements:\nWrite uj := dimMj (EvalStridej(SubsignalB(ξ, i))) ∈ N1 for all j ∈ {0, . . . , L} as an abbreviation and note that this number actually does not depend on any subsignal index i. We further define U colj := cdimMj (EvalSlidej(ξ)) ∈ N1 and U rowj := rdimMj (EvalSlidej(ξ)) ∈ N1 for j ∈ {0, . . . , L} as abbreviations. Then for all i ∈ {1, . . . , D − B + 1} and all j ∈ {0, . . . , L} the following holds: (a) uj = 1k∗j ( B − ∑j µ=1 k ∗ µ−1(cµ − 1) ) .\n(b) U rowj = 1 k∗j\n( D − k∗j + 1− ∑j µ=1 k ∗ µ−1(cµ − 1) ) , and\nthe number of fragments is U colj = k ∗ j .\n(c) U rowj − uj + 1 = 1k∗j (D −B + 1), that is the number of distinct subsignals with uj samples in each fragment of the fragmented signals equals the original number of distinct subsignals divided by the corresponding number of fragments.\n(d) For all µ ∈ {1, . . . , uj} we have that\nEvalStridej(SubsignalB(ξ, i))µ\n= EvalSlidej(ξ)div(i−1, k∗j )+µ, rem(i−1, k∗j )+1,\nwhere the latter can also be understood as one sample of the Subsignaluj operator applied to a certain fragment of EvalSlidej(ξ).\nProof. (a) Let i ∈ {1, . . . , D−B+ 1} be arbitrary and define ρ := SubsignalB(ξ, i) ∈MB0 as an abbreviation. It is\nu0 = dimM0(EvalStride0(ρ)) = dimM0(ρ) = B,\nand the right-hand side of the claim trivially equals B for j = 0. Carrying out induction we yield for j − 1→ j:\nuj D. 22 = dimMj (Stridegj (Tj(EvalStridej−1(ρ))))\nD. 11 = 1kj dimNj (Tj(EvalStridej−1(ρ))) DRP = 1kj ( dimMj−1(EvalStridej−1(ρ))− cj + 1 ) IH = 1kj ( 1 k∗j−1 ( B − j−1∑ µ=1 k∗µ−1(cµ − 1) ) − (cj − 1) )\n= 1kjk∗j−1\n( B −\nj−1∑ µ=1 k∗µ−1(cµ − 1)− k∗j−1(cj − 1)\n) ,\nwhere IH denotes substitution of the induction hypothesis. Hence, the claimed expression follows since k∗j = kjk ∗ j−1. We note that uj is indeed a natural number because kj divides dimNj (Tj(EvalStridej−1(ρ))) by requirement.\n(b) Besides the statements on U rowj and U col j we here show that the application of the processing chain in a sliding fashion is well-defined using induction for j. Considering j = 0, EvalStride0(ξ) = ξ is trivially well-defined and by definition it is ξ ∈ MD0 = MD×10 . Therefore, U row0 = D and U col0 = 1 which equals the claimed expressions since k∗0 = 1.\n9 ξ1\nξ2\nξ3\nξ4\nξ5\nξ6\nξ7\nξ8\nξ1 ξ2\nξ2 ξ3\nξ3 ξ4\nξ4 ξ5\nξ5 ξ6\nξ6 ξ7\nξ7 ξ8\nmax\n{\nξ1 ξ2 , ξ2 ξ3\n}\nmax\n{\nξ2 ξ3 , ξ3 ξ4\n}\nmax\n{\nξ3 ξ4 , ξ4 ξ5\n}\nmax\n{\nξ4 ξ5 , ξ5 ξ6\n}\nmax\n{\nξ5 ξ6 , ξ6 ξ7\n}\nmax\n{\nξ6 ξ7 , ξ7 ξ8\n}\nmax\n{\nξ1 ξ2 , ξ2 ξ3\n}\nmax\n{\nξ3 ξ4 , ξ4 ξ5\n}\nmax\n{\nξ5 ξ6 , ξ6 ξ7\n}\nmax\n{\nξ2 ξ3 , ξ3 ξ4\n}\nmax\n{\nξ4 ξ5 , ξ5 ξ6\n}\nmax\n{\nξ6 ξ7 , ξ7 ξ8\n}\nξ1\nξ2\nξ3\nξ4\nξ5\nξ1 ξ2\nξ2 ξ3\nξ3 ξ4\nξ4 ξ5\nmax\n{\nξ1 ξ2 , ξ2 ξ3\n}\nmax\n{\nξ3 ξ4 , ξ4 ξ5\n}\nξ2\nξ3\nξ4\nξ5\nξ6\nξ2 ξ3\nξ3 ξ4\nξ4 ξ5\nξ5 ξ6\nmax\n{\nξ2 ξ3 , ξ3 ξ4\n}\nmax\n{\nξ4 ξ5 , ξ5 ξ6\n}\nQuot Slidemax Frag2\nSubsignal5(ξ, 1)\nQuot Stridemax\nSubsignal5(ξ, 2)\nQuot Stridemax\nFig. 5. Example for the two notions of application of a processing chain to an input signal: The upper part shows the dynamics of the EvalSlide approach which processes all possible subsignals in one go. The lower part shows the results of the EvalStride approach applied to two different subsignals. Here, Quot is a sub-signal compatible transformation that computes the quotient of two samples, and max is applied in a strided fashion to determine the maximum of two samples. The outcome of the EvalStride functions can be located in the results of the EvalSlide function, where the appropriate positions can be determined with Lemma 23. The graphics also emphasizes why the output of EvalStride should consist of only one sample in Theorem 25: If there is more than one sample, the defragmented version of the EvalSlide result will contain the result of the EvalStride function applied to subsignals in an interleaved fashion, which violates the exchange property of subsignal compatible transformations.\nFor j − 1 → j, we first demonstrate that kj divides χ := rdimMj (Slidegj (Tj(EvalSlidej−1(ξ)))) which implies well-definedness since the fragmentation operator can then indeed be applied. We find that\nχ D. 5 = rdimNj (Tj(EvalSlidej−1(ξ)))− kj + 1\nDRP = rdimMj−1(EvalSlidej−1(ξ))− cj + 1− kj + 1 = U rowj−1 − cj + 1− kj + 1\nIH = 1k∗j−1\n( D − k∗j−1 + 1− j−1∑ µ=1 k∗µ−1(cµ − 1) ) + 1k∗j−1 ( −k∗j−1(cj − 1)− k∗j + k∗j−1\n) = 1k∗j−1 ( D − k∗j + 1− j∑ µ=1 k∗µ−1(cµ − 1) ) .\nBy requirement on the signal length D there exists a number t ∈ N1 so that D −B + 1 = k∗Lt. Substitution yields\nχ = 1k∗j−1\n( B −\nj∑ µ=1 k∗µ−1(cµ − 1) + k∗Lt− k∗j ) = kjuj + kj · · · kL · t− kj .\nProposition 13 implies that kj divides χ since uj ∈ N1 as we have seen in (a), hence the processing chain can be applied until the j-th layer. With Lemma 16 one sees that U rowj = 1 kj χ which immediately yields the claimed expression. As only the fragmentation operator changes the number of columns in the entire processing chain, it follows that U colj = kjU col j−1 with Lemma 16, which shows the claimed identity. (c) Using (a) and (b) we obtain\nU rowj − uj + 1 = 1k∗j ( D − k∗j + 1−B ) + 1 = D−B+1k∗j ,\nwhich is a natural number as the number of subsignals was required to be divisible by k∗L, was implies divisibility by k ∗ j .\n(d) We prove this by induction for j. For j = 0, the left-hand side equals SubsignalB(ξ, i)µ = ξi+µ−1 using Definition 1 for all i ∈ {1, . . . , D−B + 1} and all µ ∈ {1, . . . , B}. Since k∗0 = 1 we find with Proposition 13 that the right-hand side is ξdiv(i−1, 1)+µ, rem(i−1, 1)+1 = ξi−1+µ, 1, hence both sides are equal.\nLet us now turn to j − 1 → j. Let µ ∈ {1, . . . , uj} be arbitrary, let i ∈ {1, . . . , D−B+1} be a fixed subsignal index and write τ := EvalStridej−1(SubsignalB(ξ, i)) ∈ M uj−1 j−1 as an abbreviation. Considering the left-hand side of the claim\n10\nwe obtain\nEvalStridej(SubsignalB(ξ, i))µ D. 22 = Stridegj (Tj(τ))µ\nD. 11 = gj  kj∑ ν=1 Tj(τ)kj(µ−1)+ν · e kj ν  T. 6 = gj\n kj∑ ν=1 Tj ( cj∑ λ=1 τkj(µ−1)+ν+λ−1 · e cj λ ) · ekjν  IH = gj\n kj∑ ν=1 Tj ( cj∑ λ=1 EvalSlidej−1(ξ)ψ, ω · e cj λ ) · ekjν  , where ψ := div(i− 1, k∗j−1) + kj(µ− 1) + ν + λ− 1 ∈ N1 and ω := rem(i− 1, k∗j−1) + 1 ∈ N1.\nLet π := EvalSlidej−1(ξ) ∈ M Urowj−1×U col j−1\nj−1 be an abbreviation for the analysis of the right-hand side of the claim. We have that\nEvalSlidej(ξ)div(i−1, k∗j )+µ, rem(i−1, k∗j )+1 D. 22 = Fragkj (Slidegj (Tj(π))div(i−1, k∗j )+µ, rem(i−1, k∗j )+1 L. 16 = Slidegj (Tj(π))div(φ, k∗j−1)+1, rem(φ, k∗j−1)+1,\nwhere the number of input fragments to Fragkj was k ∗ j−1 as shown in (b) and where we have defined φ := ( div(i− 1, k∗j ) + µ− 1 ) kjk ∗ j−1 + rem(i− 1, k∗j ) ∈ N.\nBy the definition of the operators from Euclidean division follows that φ = i−1+(µ− 1) kjk∗j−1. Using Proposition 13 one yields\ndiv(φ, k∗j−1) = div(i− 1, k∗j−1) + kj(µ− 1), and rem(φ, k∗j−1) + 1 = rem(i− 1, k∗j−1) + 1 = ω.\nTherefore\nEvalSlidej(ξ)div(i−1, k∗j )+µ, rem(i−1, k∗j )+1\nD. 5 = gj  kj∑ ν=1 Tj(π)div(i−1, k∗j−1)+kj(µ−1)+ν, ω · e kj ν  T. 6 = gj\n kj∑ ν=1 Tj ( cj∑ λ=1 πψ, ω · e cj λ ) · ekjν  , which equals the left-hand side of the claim as we have seen earlier and thus the proof is finished.\nTherefore, the result of the EvalStride operator applied to arbitrary subsignals of an input signal emerge in the result of the EvalSlide operator which processes the entire signal in one go. Lemma 23 requires the length of the input signal to satisfy certain divisibility constraints. For extension of its statements to signals of arbitrary length we need two more operators:\nDefinition 24. Let r ∈ N be a natural number, let M be a set and ζ ∈M be an arbitrary dummy element from M . Then\nStuffr : ∪1 (M)→ ∪r+1(M),\n(ξ1, . . . , ξq) 7→ q∑\nν=1\nξν · eq+rν + r∑\nν=1\nζ · eq+rq+ν ,\nis called the stuffing operator that appends r copies of ζ to its argument, and we further call\nTrimr : ∪r+1 (M)→ ∪1(M),\n(ξ1, . . . , ξq, ξq+1, . . . , ξq+r) 7→ q∑\nν=1\nξν · eqν ,\nthe trimming operator that removes the final r entries from its argument.\nThe concrete choice of the dummy element ζ does not matter in the following considerations since all output entries which are affected by its choice are trimmed away in the end. We are now in the position to state the main result of this section:\nTheorem 25. Suppose we are given a processing chain with the same notation as in Definition 22, where kj divides dimNj (Tj(EvalStridej−1(ρ))) for all j ∈ {1, . . . , L} and all ρ ∈ MB0 , and where dimML(EvalStrideL(ρ)) = 1 for all ρ ∈ MB0 , that is the output of the entire processing chain applied in a strided fashion consists of exactly one sample.\nLet k∗L := ∏L ν=1 kν denote the stride product of the L-th\nlayer, and let r̃ : N1 → {0, . . . , k∗L − 1},\nδ 7→ { 0, if k∗L divides δ −B + 1, k∗L − rem(δ −B + 1, k∗L), otherwise,\ndenote the number of dummy samples that have to be padded to an original signal with δ samples to satisfy divisibility requirements. Further define r : ∪1 (M0)→ {0, . . . , k∗L − 1}, ξ 7→ r̃(dimM0(ξ)), as an abbreviation that computes the required number of dummy samples in dependence on an original signal ξ.\nConsider the function\nT : ∪B (M0)→ ∪1(ML), ξ 7→ Trimr(ξ)(Defragk∗L(EvalSlideL(Stuffr(ξ)(ξ)))),\nwhich first pads the input signal with as many dummy entries such that each fragmentation operation during application of the processing chain in a sliding fashion comes out even, applies the processing chain in a sliding fashion, defragments the outcome and and eventually removes all superfluous entries that emerged from the initial stuffing.\nThen T is a subsignal compatible transformation with dimensionality reduction constant B. Furthermore, T (ρ) = EvalStrideL(ρ) for all ρ ∈MB0 and T = SlideEvalStrideL .\nProof. We note that r̃(δ) is well-defined if k∗L does not divide δ−B+ 1, since then rem(δ−B+ 1, k∗L) ∈ {1, . . . , k∗L− 1} and thus k∗L − rem(δ −B + 1, k∗L) ∈ {1, . . . , k∗L − 1}.\nLet ξ ∈ ∪B(M0) and write D̃ := dimM0(ξ). We have D := dimM0(Stuffr(ξ)(ξ)) = D̃+r(ξ). If k ∗ L divides D̃−B+1 it is\n11\nr(ξ) = 0, so rem(dimM0(Stuffr(ξ)(ξ))−B + 1, k∗L) = 0. If on the other hand k∗L does not divide D̃−B+1, then r(ξ) > 0 and furthermore\ndimM0(Stuffr(ξ)(ξ))−B + 1 = D̃ + k∗L − rem(D̃ −B + 1, k∗L)−B + 1.\nHence rem(dimM0(Stuffr(ξ)(ξ)) − B + 1, k∗L) = 0 due to the idempotence of the rem operator. Therefore, in every case is the number of subsignals of Stuffr(ξ)(ξ) with B samples divisible by k∗L, as required for application of Lemma 23.\nLemma 23 guarantees that π := EvalSlideL(Stuffr(ξ)(ξ)) is well-defined. With Lemma 23(b) follows cdimML(π) = k ∗ L. We demanded dimML(EvalStrideL(ρ)) = 1 for all ρ ∈MB0 , hence rdimML(π) =\n1 k∗L (D − B + 1) with Lemma 23(c). Therefore, Defragk∗L(π) is well-defined with exactly one output fragment, which has the same number of samples as we have subsignals of length B in the stuffed input signal: dimML(Defragk∗L(π)) = D −B + 1.\nWe have that dimML(Defragk∗L(π)) ≥ r(ξ) + 1 because D = D̃+ r(ξ) where D̃ ≥ B, thus Trimr(ξ)(Defragk∗L(π)) is well-defined. Since the trimming operator reduces dimensionality by r(ξ) we find\ndimM0(ξ)−dimML(T (ξ)) = D̃−(D−B+1−r(ξ)) = B−1.\nTherefore, T fulfills the dimensionality reduction property with dimensionality reduction constant B.\nWith Theorem 6 it is thus enough to demonstrate that\nT (SubsignalB(ξ, i)) = T (ξ)i ∈ML\nfor all i ∈ {1, . . . , D̃−B+1} for T to be subsignal compatible. We first show that T (ρ) = EvalStrideL(ρ) for all ρ ∈ MB0 , which we will then use to show the weakened exchange property.\nLet ρ ∈ MB0 , then SubsignalB(Stuffr(ρ)(ρ), 1) = ρ by the definition of the stuffing operator. We know that T (ρ) consists of a single sample as the dimensionality reduction constant of T is B, hence T (ρ) = T (ρ)1. Extraction of the very first sample of the result of the trimming operator is here equal to the extraction of the very first sample of the trimming operator’s argument. Therefore,\nT (ρ)\n= Defragk∗L(EvalSlideL(Stuffr(ρ)(ρ)))1, 1 L. 18 = EvalSlideL(Stuffr(ρ)(ρ))div(0, k∗L)+1, rem(0, k∗L)+1\nL. 23(d) = EvalStrideL(SubsignalB(Stuffr(ρ)(ρ), 1))1\n= EvalStrideL(ρ).\nLet us now return to the exchange property. As before let ξ ∈ M D̃0 , and let i ∈ {1, . . . , D̃ − B + 1} be an arbitrary\nsubsignal index. We can omit the trimming operator as before and yield\nT (ξ)i\n= Defragk∗L(EvalSlideL(Stuffr(ξ)(ξ)))i, 1 L. 18 = EvalSlideL(Stuffr(ξ)(ξ))div(i−1, k∗L)+1, rem(i−1, k∗L)+1\nL. 23(d) = EvalStrideL(SubsignalB(Stuffr(ξ)(ξ), i))1\nD. 24 = EvalStrideL(SubsignalB(ξ, i))\n= T (SubsignalB(ξ, i)) .\nHence T is a subsignal compatible transformation due to Theorem 6. Theorem 4 finally implies T = SlideEvalStrideL .\nWe can thus conclude that Convolutional Neural Networks can be turned into effectively computable subsignal compatible transformations using the EvalSlide operator. One could suspect that stuffing the input signal with dummy samples might have a negative effect on the computing time. However, the number of stuffed samples is always less than the stride product of the final layer and hence very small for reasonably sized CNNs.\nMoreover, stuffing guarantees that all fragments encountered during evaluation are homogeneous. This facilitates usage of simple parallelized implementations which can rely on fragmented signals where each fragment has the same number of samples, which is especially efficient on massively parallel processors."
    }, {
      "heading" : "V. MULTI-SCALE TRANSFORMATIONS",
      "text" : "The previous sections have shown how Convolutional Neural Networks can be efficiently evaluated on entire images through the theory of subsignal compatible transformations. We now consider functions that take multiple spatial resolutions of a single signal as input. Since here the context of local regions is incorporated as well, this approach has proven highly effective in classification tasks [13]. We here assume that the number of samples considered at any one time is fixed for all scale levels. This facilitates the design of scale-invariant representations, for example by using the same classifier for all scales of the input [13].\nA signal is downscaled by application of a lowpass filter to reduce aliasing artifacts, followed by a downsampling operator which returns a subset of equidistant samples. When a subsignal is extracted from a downscaled input signal, it should contain a downscaled copy of the corresponding subsignal from the original input signal. This requires boundary-handling of the input signal, since for example the very first subsignal cannot be extended to allow for a larger context by means of only the original samples. Let us first formalize the concepts of boundary handling and subsignal extraction subject to boundary handling:\nDefinition 26. Let M be a set, let d ∈ N1 denote a subsignal dimensionality and let R ∈ N be a boundary size. (a) A function ϑ : ∪1 (M) × Z → M is called boundary-\nhandling function if and only if ϑ(ξ, ν) = ξν for all ξ ∈ ∪1(M) and all ν ∈ {1, . . . ,dimM (ξ)}.\n12\n(b) We call the function\nPadϑR : ∪1 (M)→ ∪1+2R(M),\nξ 7→ dimM (ξ)+2R∑\nν=1\nϑ(ξ, ν −R) · edimM (ξ)+2Rν ,\nwhich extends signals at both ends with R samples subject to the boundary-handling function ϑ the padding operator.\n(c) The function\nSubsignalPadϑ(d,R) : ∞⋃ D=d ( MD × {1, . . . , D − d+ 1} ) →Md+2R,\n(ξ, i) 7→ d+2R∑ ν=1 ϑ(ξ, i+ ν −R− 1) · ed+2Rν ,\nthat extracts subsignals subject to the boundary-handling function ϑ and implicitly pads R samples at both ends is called the padded subsignal extraction operator.\nNext, we define how we extract downscaled subsignals using the concepts just introduced:\nDefinition 27. Let M be a set and let k ∈ N1 denote a downsampling step size. (a) We call\nDownk : ∪1 (M)→ ∪1(M),\nξ 7→ ddimM (ξ)/ke∑\nν=1\nξk(µ−1)+1 · eddimM (ξ)/keν ,\nthe downsampling operator, which extracts samples from equidistant locations.\n(b) The function\nMultiScaleIndexk : N1 → N1 i 7→ k · div(i− 1, k) + 1,\nis called the multi-scale subsignal index transformation. (c) Suppose H : Mh →M is a lowpass filter kernel of size\nh ∈ N1, where h ≥ k should hold to avoid aliasing artifacts. Further, let d ∈ N1 be a subsignal dimensionality, R ∈ N a boundary size and ϑ : ∪1 (M) × Z → M a boundary-handling function. Then\nMultiScaleSubsignal (ϑ,H) (d,R,k) :\n∞⋃ D=d ( MD × {1, . . . , D − d+ 1} ) → ∪1(M), (ξ, i) 7→ Downk(SlideH(SubsignalPadϑ(d,R)(ξ, i))),\nis called the multi-scale subsignal extraction operator.\nThere are a few requirements so that extraction of downscaled subsignals makes sense. Most importantly is the correct determination of the boundary size R in the definition of the MultiScaleSubsignal operator. It should be chosen so that the extracted subsignals from each scale level are always exactly centered around the corresponding subsignals from the original scale level. It is moreover beneficial if the entire input signal can be downscaled in one go, so that the output of the MultiScaleSubsignal operator equals simple extraction of subsignals from that downscaled signal.\n13\nHowever, if this approach is pursued there are subsignals in the original signal which do not possess a downscaled counterpart in this representation. The MultiScaleIndex function alleviates this problem through computation of an appropriate subsignal index which is guaranteed to possess a downscaled counterpart. Although this is merely an approximation, it is assured that the correct subsignal index in the downscaled signal is always less than one sample off. The next result formalizes these thoughts, an illustration of its statements is given in Fig. 6.\nLemma 28. Let M be a set, and let k ∈ N be a downsampling step size where we require that k ≥ 2. Moreover, let H : Mh → M be a lowpass filter kernel of size h ∈ N1, h ≥ k, and let B ∈ N1 be a subsignal dimensionality and suppose ϑ : ∪1(M)×Z→M is a boundary-handling function.\nDefine R̃ := (k−1)B+h−k ∈ N and R := ⌈ R̃/2 ⌉ ∈ N as the boundary size. Assume we are given a signal ξ ∈ ∪B(M) and let us write D := dimM (ξ) as an abbreviation. Ultimately, let π := Downk(SlideH(PadϑR(ξ))) ∈ ∪1(M) denote the downscaled signal. Then the following holds:\n(a) SubsignalB(ξ, i)µ = SubsignalPad ϑ (B,R)(ξ, i)µ+R for\nall µ ∈ {1, . . . , B} and all i ∈ {1, . . . , D −B + 1}, that is the padded subsignals are centered around the original subsignals.\n(b) dimM (π) = ⌈D−B+1+rem(R̃, 2)\nk\n⌉ + B − 1, hence there\nare at least ⌈ D−B+1\nk\n⌉ subsignals with B samples in π,\nand at most one additional subsignal. (c) Let i ∈ {1, . . . , D − B + 1} be a subsignal index and\nwrite j := MultiScaleIndexk(i) as the result of the index transformation. Then j ∈ {1, . . . , D − B + 1}, j ≤ i and i − j < k. The index adjustment hence decreases subsignal indices by at most k − 1 samples with respect to the original scale level.\n(d) It is\nSubsignalB(π, div(i− 1, k) + 1) = MultiScaleSubsignal\n(ϑ,H) (B,R,k)(ξ, MultiScaleIndexk(i))\nfor all i ∈ {1, . . . , D − B + 1}. In other words, the subsignals from π equal downscaled subsignals from the original signal ξ where the subsignal index was adjusted through MultiScaleIndexk.\nProof. (a) If µ ∈ {1, . . . , B} and i ∈ {1, . . . , D − B + 1}, then\nSubsignalPadϑ(B,R)(ξ, i)µ+R D. 26 = ϑ(ξ, i+ µ− 1) (♦)= ξi+µ−1 D. 1 = SubsignalB(ξ, i)µ,\nwhere in the (♦) step we have used that i+µ−1 ∈ {1, . . . , D}. Here, the boundary handling function evaluates to an original sample of the input signal. Hence all the samples in the middle of SubsignalPadϑ(B,R)(ξ, i) stem from the input signal ξ and are not subject to boundary conditions.\n(b) We first note that\n2R = R̃+ rem(R̃, 2) = (k − 1)B + h− k + rem(R̃, 2)\nby the definition of the ceiling function, which is marked with (♦) in the following. Therefore\ndimM (π) = dimM (Downk(SlideH(Pad ϑ R(ξ))))\nD. 27 = ⌈ 1 k · dimM (SlideH(Pad ϑ R(ξ))) ⌉ D. 5 = ⌈ 1 k ( dimM (Pad ϑ R(ξ))− h+ 1\n)⌉ D. 26 = ⌈ 1 k (dimM (ξ) + 2R− h+ 1)\n⌉ (♦) = ⌈ D−B+1+rem(R̃, 2) k +B − 1 ⌉\n= ⌈ D−B+1+rem(R̃, 2)\nk\n⌉ +B − 1,\nwhere we have used that B − 1 ∈ N in the final step so that this term could be moved outside of the ceiling function. Since rem(R̃, 2) ∈ { 0, 1 } there is at most one superfluous subsignal of length B in π, which is irrelevant in the following discussion.\n(c) Let i and j := k · div(i − 1, k) + 1 be given as in the claim. We clearly have j ∈ N. Since div(i − 1, k) ≥ 0 follows j ≥ 1. On the other hand, using Euclidean division we obtain k · div(i − 1, k) + 1 = i − rem(i − 1, k) ≤ i ≤ D −B + 1 because rem(i− 1, k) ≥ 0. Analogously follows i − j = rem(i − 1, k) ∈ {0, . . . , k − 1}, which proves the claimed inequalities.\n(d) Let i ∈ {1, . . . , D − B + 1} be an arbitrary subsignal index. We start by proving that the right-hand side of the claimed identity is indeed of dimensionality B. Let us define\nj := MultiScaleIndexk(i) and\nρ := MultiScaleSubsignal (ϑ,H) (B,R,k)(ξ, j)\nas abbreviations. Analogously to (b) where we deduced an expression for 2R, marked with (♦), one obtains\ndimM (ρ) D. 27 = ⌈ 1 k · dimM (SlideH(SubsignalPad ϑ (B,R)(ξ, j))) ⌉ D. 5 = ⌈ 1 k ( dimM (SubsignalPad ϑ (B,R)(ξ, j))− h+ 1\n)⌉ D. 26 = ⌈ 1 k (B + 2R− h+ 1)\n⌉ (♦) = B − 1 + ⌈ 1+rem(R̃, 2)\nk\n⌉ .\nAs 1 + rem(R̃, 2) ∈ { 1, 2 } and k ≥ 2 by requirement, we find 0 < 1k ( 1 + rem(R̃, 2) ) ≤ 1 and hence dimM (ρ) = B.\nNow let µ ∈ {1, . . . , B} for comparing both sides of the claim sample-wise. We have\nρµ D. 27 = SlideH(SubsignalPad ϑ (B,R)(ξ, j))k(µ−1)+1\nD. 5 = H ( h∑ ν=1 SubsignalPadϑ(B,R)(ξ, j)k(µ−1)+ν · ehν ) D. 26 = H\n( h∑ ν=1 ϑ(ξ, j + k(µ− 1) + ν −R− 1) · ehν ) .\n14\nThe corresponding sample of the left-hand side equals\nSubsignalB(π, div(i− 1, k) + 1)µ D. 1 = Downk(SlideH(Pad ϑ R(ξ)))div(i−1, k)+µ D. 27 = SlideH(Pad ϑ R(ξ))k·div(i−1, k)+k(µ−1)+1\nD. 5 = H ( h∑ ν=1 PadϑR(ξ)j+k(µ−1)+ν−1 · ehν ) D. 26 = H\n( h∑ ν=1 ϑ(ξ, j + k(µ− 1) + ν −R− 1) · ehν ) ,\nwhich is the same as ρµ, which proves the claimed identity.\nThe ultimate goal of this section is to analyze functions applied to different scale levels of a signal and propose an efficient evaluation scheme. We have already taken the first step by analyzing the connection between downscaled subsignal extraction and subsignal extraction from a downscaled signal in Lemma 28. The complement of downscaling in this course of action is to repeat samples as many times as samples were omitted during downsampling. This leads to the following definition:\nDefinition 29. Let M be a set and k ∈ N1. Then\nUpk : ∪1 (M)→ ∪k(M),\nξ 7→ dimM (ξ)∑ µ=1\n( ξµ ·\nk∑ λ=1 e k·dimM (ξ) k(µ−1)+λ\n) ,\nis called the upsampling operator with zero-order hold.\nWe can immediately provide a statement on which samples go where during upsampling:\nLemma 30. Let M be a set, q ∈ N1 and ξ ∈Mq . Then it is Upk(ξ)ν = ξdiv(ν−1, k)+1 for all ν ∈ {1, . . . , kq}.\nProof. With Definition 29 there exists µ ∈ {1, . . . , q} with Upk(ξ)ν = ξµ, where ν = k(µ− 1) + λ and λ ∈ {1, . . . , k}. One obtains\nk · (µ− 1) + (λ− 1) + 1 = (ν − 1) + 1 = k · div(ν − 1, k) + rem(ν − 1, k) + 1.\nHere, λ− 1 ∈ {0, . . . , k − 1}, hence uniqueness of Euclidean division implies µ−1 = div(ν−1, k), and the claim follows.\nWe are almost ready for the main result of this section, that states under which circumstances a function that accepts inputs in both the original scale and in a downscaled version can be evaluated efficiently.\nIndexing rules are here as follows. Suppose P and Q are sets and χ ∈ ∪1(P ×Q) is a signal with paired samples from P × Q. Then there exists a dimensionality d ∈ N1 so that χ ∈ (P × Q)d. Since (P × Q)d ∼= P d × Qd we can also write χ as pair of signals, say χ = (ψ, ω) where ψ ∈ P d and ω ∈ Qd. If ν ∈ {1, . . . , d} is an index, then we define χi := (ψi, ωi) ∈ P ×Q as an individual sample.\nTheorem 31. Let M,N be sets and let B ∈ N1 be a fixed subsignal dimensionality. Let f : MB × MB → N be a\nfunction that accepts signals in the original scale and in a downscaled version. Suppose f can be factorized into functions gOrg : MB → P , gDown : MB → Q and g : P×Q→ N , so that\nf(ρ, τ) = g (gOrg(ρ), gDown(τ)) for all ρ, τ ∈MB ,\nwhere P and Q are sets. As in Lemma 28, let k ∈ N, k ≥ 2, be a downsampling step size. Further, let h ∈ N1, h ≥ k, and H : Mh → M a lowpass filter kernel and ϑ : ∪1 (M) × Z → M a boundaryhandling function. Let R̃ := (k − 1)B + h − k ∈ N and let R := ⌈ R̃/2 ⌉ ∈ N denote the required boundary size. Considering a signal ξ ∈ ∪B(M), we write D := dimM (ξ), π := Downk(SlideH(Pad ϑ R(ξ))) ∈ ∪1(M), and moreover\nr := rem(R̃, 2)− rem(D −B + 1 + rem(R̃, 2), k)\n+ { 0, if k divides D −B + 1 + rem(R̃, 2), k, otherwise.\nThen r ∈ N and f (\nSubsignalB(ξ, i),\nMultiScaleSubsignal (ϑ,H) (B,R,k)(ξ, MultiScaleIndexk(i)) ) = Slideg ( SlidegOrg(ξ), Trimr(Upk(SlidegDown(π))) ) i\nfor all i ∈ {1, . . . , D − B + 1}, that is f applied to the subsignals of ξ and certain multi-scale subsignals of ξ equals the samples of g, gOrg and gDown applied in a sliding fashion to signals derived from ξ. After gDown has been applied to the downscaled signal π, the result has to be upsampled and the superfluous r trailing entries have to be trimmed away.\nProof. Through application of Lemma 28(b) we have that dimM (π) = ⌈D−B+1+rem(R̃, 2) k ⌉ +B−1. As dimM (π) ≥ B, SlidegDown(π) is well-defined and one obtains\ndimQ(SlidegDown(π)) D. 5 = dimM (π)−B + 1 = ⌈ D−B+1+rem(R̃, 2)\nk\n⌉ .\nTherefore one obtains\ndimQ(Upk(SlidegDown(π))) D. 29 = k ·\n⌈ D−B+1+rem(R̃, 2)\nk\n⌉ .\nIn the case of k dividing D −B + 1 + rem(R̃, 2) follows dimQ(Upk(SlidegDown(π))) = D − B + 1 + rem(R̃, 2) and by definition it is r = rem(R̃, 2) ∈ N, hence\ndimQ(Trimr(Upk(SlidegDown(π)))) = D −B + 1.\nIn the other case of k not dividing D−B+ 1 + rem(R̃, 2) follows⌈ D−B+1+rem(R̃, 2)\nk\n⌉ = div(D−B + 1 + rem(R̃, 2), k) + 1.\nFrom the definition of Euclidean division one yields\ndimQ(Upk(SlidegDown(π)))\n= D −B + 1 + rem(R̃, 2) − rem(D −B + 1 + rem(R̃, 2), k) + k.\n15\nWe further have by definition\nr = rem(R̃, 2)− rem(D −B + 1 + rem(R̃, 2), k) + k.\nAs rem(D−B+1+rem(R̃, 2), k) ∈ {0, . . . , k−1} follows r ≥ 1. Thus r ∈ N and Trimr(Upk(SlidegDown(π))) is welldefined. We have that dimQ(Trimr(Upk(SlidegDown(π)))) = D −B + 1 in this case as well.\nWe find that the number of samples in SlidegOrg(ξ) equals the number of samples in Trimr(Upk(SlidegDown(π))) in both cases. Since g works on individual samples we yield that\nSlideg ( SlidegOrg(ξ), Trimr(Upk(SlidegDown(π))) )\nconsists of D −B + 1 samples from N . Let i ∈ {1, . . . , D −B + 1} be arbitrary and define\nτ := MultiScaleSubsignal (ϑ,H) (B,R,k)(ξ, MultiScaleIndexk(i))\nas an abbreviation. From Definition 5 follows immediately that SlidegOrg(ξ)i = gOrg(SubsignalB(ξ, i)). Considering the second argument to Slideg , we have\nTrimr(Upk(SlidegDown(π)))i\n= Upk(SlidegDown(π))i L. 30 = SlidegDown(π)div(i−1, k)+1 D. 5 = gDown(SubsignalB(π, div(i− 1, k) + 1))\nL. 28(d) = gDown(τ),\nwhere we have used i ≤ D − B + 1 in the first step so we could eliminate the trimming operator.\nCombining these results and using the precondition (PC) that f can be factorized one finds\nSlideg ( SlidegOrg(ξ), Trimr(Upk(SlidegDown(π))) ) i\nD. 5 = g ( SlidegOrg(ξ)i, Trimr(Upk(SlidegDown(π)))i ) = g ( gOrg(SubsignalB(ξ, i)), gDown(τ)\n) PC = f ( SubsignalB(ξ, i), τ ) ,\nwhich equals the claimed expression.\nTheorem 31 directly provides an algorithm for efficient multi-scale analysis. The functional part operating on the original input signal should be applied in a sliding fashion. If this function can be cast as processing chain, which was discussed in Sect. IV, the there proposed theory can be used for efficient evaluation. The multi-scale subsignal index approximation proved in Lemma 28 facilitates application of the functional part operating on downscaled subsignals in a sliding fashion as well. Therefore, the theory from Sect. IV can be applied here also. We finally note that the generalization of the statements of Theorem 31 to functions that process several different downscaled signals is straightforward provided a proper factorization can be given."
    }, {
      "heading" : "VI. CONCLUSIONS",
      "text" : "This paper introduced and analyzed the concept of subsignal compatible transformations, functions that allow exchanging subsignal extraction with function evaluation without any\neffect on the outcome. In doing so, it was demonstrated rigorously how Convolutional Neural Networks can be applied efficiently to large signals in a sliding fashion while eliminating redundant computations and special case treatment. The final part demonstrated the versatility of the developed theory by considering multi-scale transformations in great detail. All results have been shown explicitly to be correct. The arguments given in the proofs can be used to verify whether a given implementation is correct, or they can serve as basis for algorithmic realizations. In either case is an extensive mathematical framework available that facilitates analysis of rapid exact signal scanning schemes."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work was supported by Daimler AG, Germany."
    } ],
    "references" : [ {
      "title" : "Receptive Fields, Binocular Interaction and Functional Architecture in the Cat’s Visual Cortex",
      "author" : [ "D.H. Hubel", "T.N. Wiesel" ],
      "venue" : "Journal of Physiology, vol. 160, no. 1, pp. 106–154, 1962.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position",
      "author" : [ "K. Fukushima" ],
      "venue" : "Biological Cybernetics, vol. 36, no. 4, pp. 193–202, 1980.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Handwritten Digit Recognition with a Back- Propagation Network",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "Advances in Neural Information Processing Systems, vol. 2, 1990, pp. 396–404.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Multi-Column Deep Neural Networks for Image Classification",
      "author" : [ "D.C. Cireşan", "U. Meier", "J. Schmidhuber" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3642–3649.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Regularization of Neural Networks using DropConnect",
      "author" : [ "L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus" ],
      "venue" : "Proceedings of the International Conference on Machine Learning, 2013, pp. 1058–1066.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multi-Column Deep Neural Network for Traffic Sign Classification",
      "author" : [ "D. Cireşan", "U. Meier", "J. Masci", "J. Schmidhuber" ],
      "venue" : "Neural Networks, vol. 32, pp. 333–338, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deeply- Supervised Nets",
      "author" : [ "C.-Y. Lee", "S. Xie", "P.W. Gallagher", "Z. Zhang", "Z. Tu" ],
      "venue" : "Proceedings of the International Conference on Artificial Intelligence and Statistics, 2015, pp. 562–570.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems, vol. 25, 2013, pp. 1097–1105.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Going Deeper with Convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "Tech. Rep. arXiv:1409.4842v1, 2014.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei- Fei" ],
      "venue" : "Tech. Rep. arXiv:1409.0575v3, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Toward Automatic Phenotyping of Developing Embryos From Videos",
      "author" : [ "F. Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P.E. Barbano" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 14, no. 9, pp. 1360–1371, 2005.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Deep Convolutional Networks for Scene Parsing",
      "author" : [ "D. Grangier", "L. Bottou", "R. Collobert" ],
      "venue" : "International Conference on Machine Learning, Workshop on Learning Feature Hierarchies, 2009.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning Hierarchical Features for Scene Labeling",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1915–1929, 2013.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1915
    }, {
      "title" : "An Original Approach for the Localization of Objects in Images",
      "author" : [ "R. Vaillant", "C. Monrocq", "Y. LeCun" ],
      "venue" : "Proceedings of the International Conference on Artificial Neural Networks, 1993, pp. 26–30.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks",
      "author" : [ "A. Giusti", "D.C. Cireşan", "J. Masci", "L.M. Gambardella", "J. Schmidhuber" ],
      "venue" : "IEEE International Conference on Image Processing, 2013, pp. 4034–4038.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification",
      "author" : [ "H. Li", "R. Zhao", "X. Wang" ],
      "venue" : "Tech. Rep. arXiv:1412.4526v2, 2014.  16",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "Proceedings of the International Conference on Learning Representations. arXiv:1312.6229v4, 2014.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids",
      "author" : [ "F. Iandola", "M. Moskewicz", "S. Karayev", "R. Girshick", "T. Darrell", "K. Keutzer" ],
      "venue" : "Tech. Rep. arXiv:1404.1869v1, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1869
    }, {
      "title" : "Gradient-Based Learning Applied to Document Recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Optimal Unsupervised Learning in Feedforward Neural Networks",
      "author" : [ "T.D. Sanger" ],
      "venue" : "Master’s thesis, Massachusetts Institute of Technology, 1989.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Some Theorems on Matrix Differentiation with Special Reference to Kronecker Matrix Products",
      "author" : [ "H. Neudecker" ],
      "venue" : "Journal of the American Statistical Association, vol. 64, no. 327, pp. 953–963, 1969.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1969
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Inspired by findings on the structure of mammalian visual cortex [1], the concept of spatial weight sharing has been integrated into artificial neural networks to achieve an architecture nowadays broadly known as Convolutional Neural Network (CNN) [2], [3].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "Inspired by findings on the structure of mammalian visual cortex [1], the concept of spatial weight sharing has been integrated into artificial neural networks to achieve an architecture nowadays broadly known as Convolutional Neural Network (CNN) [2], [3].",
      "startOffset" : 248,
      "endOffset" : 251
    }, {
      "referenceID" : 2,
      "context" : "Inspired by findings on the structure of mammalian visual cortex [1], the concept of spatial weight sharing has been integrated into artificial neural networks to achieve an architecture nowadays broadly known as Convolutional Neural Network (CNN) [2], [3].",
      "startOffset" : 253,
      "endOffset" : 256
    }, {
      "referenceID" : 3,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 4,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 256,
      "endOffset" : 259
    }, {
      "referenceID" : 6,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 261,
      "endOffset" : 264
    }, {
      "referenceID" : 7,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 313,
      "endOffset" : 316
    }, {
      "referenceID" : 8,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 318,
      "endOffset" : 321
    }, {
      "referenceID" : 9,
      "context" : "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.",
      "startOffset" : 323,
      "endOffset" : 327
    }, {
      "referenceID" : 10,
      "context" : "If this is carried out for each feasible image position, it is possible to assign class membership estimations to all the pixels in an image resulting in a dense description of all objects in a scene [11], [12], [13].",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 11,
      "context" : "If this is carried out for each feasible image position, it is possible to assign class membership estimations to all the pixels in an image resulting in a dense description of all objects in a scene [11], [12], [13].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 12,
      "context" : "If this is carried out for each feasible image position, it is possible to assign class membership estimations to all the pixels in an image resulting in a dense description of all objects in a scene [11], [12], [13].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 15,
      "context" : "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 16,
      "context" : "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 18,
      "context" : "Convolutional Neural Networks Ordinary CNNs process input data by means of specialized layers [19].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : "The latter send each sample of a larger signal through a scalar transfer function independently, which may for example be a hyperbolic tangent or a rectification nonlinearity [20].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 12,
      "context" : "It is further possible to improve performance through incorporation of context information using multi-scale analysis so that the network sees image regions in different spatial resolutions [13].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 14,
      "context" : "We will demonstrate how these functions can be turned into subsignal compatible transformations using a data structure recently introduced as fragmentation [15].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 14,
      "context" : "Here, we will greatly generalize the method proposed by [15] and rigorously prove the correctness of the approach.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "The vectorization operator [21] stacks all the columns of a matrix on top of another: Definition 14.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "Since here the context of local regions is incorporated as well, this approach has proven highly effective in classification tasks [13].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : "This facilitates the design of scale-invariant representations, for example by using the same classifier for all scales of the input [13].",
      "startOffset" : 133,
      "endOffset" : 137
    } ],
    "year" : 2017,
    "abstractText" : "We introduce and analyze a rigorous formulation of the dynamics of a signal processing scheme that aims at dense scanning of large input signals. Recently proposed methodologies lack a satisfactory discussion of whether they actually produce the correct results according to their definition, especially in the context of Convolutional Neural Networks. We improve on this through an exact characterization of the requirements for a sound sliding window approach. The tools developed in this paper are especially beneficial if Convolutional Neural Networks are employed, but can also be used as a more general framework to validate related approaches to signal scanning. The contributed theory helps to eliminate redundant computations and renders special case treatment unnecessary, resulting in a dramatic boost in efficiency particularly on massively parallel processors.",
    "creator" : "LaTeX with hyperref package"
  }
}