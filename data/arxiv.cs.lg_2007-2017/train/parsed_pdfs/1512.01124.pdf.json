{
  "name" : "1512.01124.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions",
    "authors" : [ "Peter Sunehag", "Richard Evans", "Gabriel Dulac-Arnold", "Yori Zwols", "Daniel Visentin", "Ben Coppin" ],
    "emails" : [ "sunehag@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n01 12\n4v 1\n[ cs"
    }, {
      "heading" : "1 Introduction",
      "text" : "Reinforcement Learning (RL) [SB98] is a paradigm for learning through trial-and-error while interacting with an unknown environment. The interaction happens in cycles during which the agent chooses an action and the environment returns an observation together with a real-valued reward. The agent’s goal is to maximize long-term accumulated reward. RL has had many successes, including autonomous helicopter control [NCD+04] and, recently, mastering a wide range of Atari games [MKS+15] (with a single agent) and a range of physics control tasks [LHP+15].\nAlthough these are impressive accomplishments, the Atari games only contain 18 actions and while the physics control tasks have continuous action spaces they are of limited dimensionality (below 10). Our work addresses combinatorial action spaces represented by feature vectors of dimensionality up to 2000, but with useful extra structure naturally present in the applications of interest. We consider the application of RL to problems such as recommendation systems [PKCK12] in which a whole slate (tuple) of actions is chosen at each time point. While these problems can be modeled with MDPs with combinatorial action spaces, the extra structure that is naturally present in the applications allows for tractable approximate value maximization of the slate.\nSlate Markov Decision Processes (Figure 1) We address RL problems that are such that at each time point the agent picks a fixed number of actions from a finite setA. We refer to a tuple of actions as a slate. The slates are ordered in our formalization, but as a special case one can have an environment that is invariant to this order. In our environments only one action from the slate is executed. For example, in the recommendation system case, the user’s choice when given the recommendations, is the execution of an action. We assume that we are given an underlying traditional RL problem, e.g. a Markov Decision Process (MDP) [Put94].\nIn an MDP, we observe a state st at each time point t and an action at is executed; the next state st+1 and reward rt (here non-negative) received are independent of what happened before time t. In other words, we assume that st summarizes all relevant information up to time t.\nThe key point of slate-MDPs is that, instead of taking one action, the agent chooses a whole slate of actions and then the environment chooses which one to execute in the underlying MDP; see Figure 1. The state information received tells the agent what\naction was executed, but not what would have happened for other actions. Slate-MDPs have important extra structure compared to the situations in which all the actions are executed and each full slate is its own discrete action in an enormous action space.\nWe investigate model-free agents that directly learn the slate-MDP value function for full slates. A simpler approach that is often deployed in large scale applications is to learn the values of individual actions and then combine the individually best actions. We present experiments that show serious shortcomings of this simple approach that completely ignores the combinatorial aspect of the tasks. When extra actions are added to a slate these might interfere with the execution of the highest value action. An agent that learns a slate value function is less harmed by this and can in principle learn beneficial slate patterns such as diversity without, unlike methods such as maximum marginal relevance [CG98] in information retrieval, being given a mathematical definition and a constant specifying the amount of diversity to introduce.\nThe main drawback of full slate agents is the number of evaluations of the value function needed for producing a slate based on them. Therefore, we also investigate the option of learning a parameterized policy (a neural network) using deterministic policy gradients [SLH+14, LHP+15, DAES16] to guide attention towards areas of the action space in which the value function is high. The neural network policy is combined with a nearest-neighbor lookup and an evaluation of the value function on this restricted set.\nRelated work As far as we are aware, [FP11] is the only work that picks multiple actions at a time for an MDP. They studied known MDPs, aiming to provide as many actions as possible while remaining nearly optimal in the worst case. Besides that they assume a known MDP, their work differs critically from our article in that they always execute an action from the slate and that they focus on the worst case choice. We work with action-execution probabilities and do not assume that any action from the slate will be executed. Further, we work with high-dimensional feature representations and aim for a scale at which achieving guaranteed near-optimal worst case behavior is not feasible. Other work on slate actions [KRS10, YG11, KWA+14] has focused on the bandit setting in which there are no state transitions. In these articles, rewards are received and summed for each action in the slate. In our slate-MDPs, the reward is only received for the executed action and we do not know what the reward would have been for the other actions in the slate. In the recommendation systems literature [PKCK12], the focus is on the immediate probability of having a recommendation accepted or being relevant, and not on expected value, which is successfully optimized for here. [VHW+09] used continuous control methods for discrete-action problems for which the actions are embedded in a feature space and the nearest discrete action is executed. This continuous control for discrete reinforcement learning approach is in a different way (as we present at greater length for RL with large action spaces in [DAES16]) utilized here as an attention mechanism."
    }, {
      "heading" : "2 Reinforcement Learning with Slate Actions",
      "text" : "As is common in reinforcement learning [SB98], we work with an agent-environment framework [RN10] in which an agent chooses actions a ∈ A executed in an environment and receives observations o ∈ O and real-valued rewards r ∈ R. A sequence\nof such interactions is generated cyclically at times t = 1, 2, 3, . . .. If the Markov property Pr(ot, rt | a1, o1, r1, . . . , at−1, ot−1, rt−1, at) = Pr(ot, rt | ot−1, at) is satisfied, then the environment is called a Markov Decision Process (MDP) [Put94] and the observations ot can be viewed as states st.\nAn MDP is defined by a tuple 〈S,A, T ,R〉 of a state space S, an action space A, a reward function R : S ×A× S → P(R) where P(X ) denotes the probability distributions over the set X , and a transition function T : S ×A → P(S). We will use R̄ to denote the expected value of R. A stationary policy is a function π : S → P(A) from states to probability distributions over A.\nThe agent is designed to accumulate as much reward as possible, which is often addressed by aiming for maximizing expected discounted reward for a discount factor γ. In this article we primarily work with episodic environments where the discounted rewards are summed to the end of the episode. Our agents learnQπ(s, a) = E{Rt|st = s, at = a, π} for a policy π, where Rt = ∑tend−t j=0 γ\njrt+j and tend is the end of the episode. Ideally, we want to find Q∗(s, a) = maxπ Qπ(s, a) because then acting according to π(s) = arg maxaQ\n∗(s, a) is optimal. One method for achieving this is Q-learning which is based on the update Qi+1(st, at) = (1− ηt)Qi(st, at) + ηt(rt + γmaxaQi(st+1, a)), where ηt ∈ (0, 1) is the learning rate. The update translates to a parameter update rule also for the case with a parameterized instead of a tabular value function. We employ the ε-greedy approach to action selection based on a value function, which means that with 1− ε probability we pick arg maxaQi(s, a) and with probability ε a random action. Our study focuses on deep architectures for the value function similar to those used by [MKS+15, LHP+15] and our approach incorporates the key techniques of target networks and experience replay employed there.\nSlate Markov Decision Processes (slate-MDPs) In this section we formally introduce slate-MDPs as well as some important special cases that enable more efficient inference.\nDefinition 1 (slate-MDP). LetM = 〈S,A, T ,R〉 be an MDP. Let ϕ : S × Al → A. Define T ′ : S ×Al × S → P(S) andR′ : S ×Al × S → P(R) by\nT ′(s,a, s′) = T (s, ϕ(a), s′),R′(s,a, s′) = R(s, ϕ(a), s′).\nThe tuple 〈S,Al, T ′,R′〉 is called a slate-MDP with underlying MDPM and actionexecution function ϕ. We assume that the previous executed action can be derived from the state through a function ψ : S → A.\nNote that any slate-MDP is itself an MDP with a special structure. In particular, the probability distribution of the next state s′ and the reward r′ conditional on the current state s and action a can be factored as:\nPr(s′, r′|s,a)︸ ︷︷ ︸ R′◦T ′ = ∑ a∈A Pr(s′, r′|s, a)︸ ︷︷ ︸ R◦T Pr(a|s,a)︸ ︷︷ ︸ ϕ .\nThe expected reward for a slate-MDP can be computed as R̄(s,a) = ∑\na∈A,s′∈S Pr(s′ | s, a) Pr(a |s,a)R̄(s, a, s′)\nIf we let Qexec(s, a, s′) = Pr(s′| s, a)(R̄(s, a, s′) + γV π(s′)), we have the following identity for the state-slate value function of the slate-MDP:\nQπ(s,a) = ∑\ns′∈S,a∈A Pr(a| s,a)Qexec(s, a, s′) (1)\nfor any slate policy π : S → Al. We do not require that the executed action ψ(st+1) is an element of at, but in the environment we create that will be the case for “good” slates a ∈ Al (Execution-IsBest in Definition 2). In the recommendation system setting, ψ(st+1) ∈ at means that a recommendation was selected by the user. We formally define what it means that having actions from the slate executed is the best outcome, by saying that the valueorder between policies coincides with that of a modified version of the environment in which ψ(st+1) /∈ at implies that the episode ends with zero reward. We call the latter property the fatal failure property.\nDefinition 2 (Value-order, Fatal Failure, Execution-Is-Best (EIB)). Let µ and ν be two environments with the same state space S and action space A. Value order: If, for any pair of policies π and π̃,\nV πµ (s) ≥ V π̃µ (s) ⇐⇒ V πν (s) ≥ V π̃ν (s) ∀s,\nthen we say that µ and ν have the same value-order. Further, suppose there is send ∈ S such that ν(send, r′ = 0| send,a) = 1. Fatal Failure: If ν(send, r′ = 0| s,a) = 1 whenever ψ(s′) /∈ a, then we say that ν has fatal failure.\nSuppose that ν(s′, r′ | s,a) = µ(s′, r′ | s,a) if ψ(s′) ∈ a, i.e., the environments coincide for executed slates. EIB: If ν has fatal failure and µ has the same value-order as ν, then we say that µ has Execution-Is-Best (EIB) property.\nTo be able to identify a value-maximizing slate in a large-scale setting, we need to avoid a combinatorial search. The first step is to note that if an environment has the fatal failure property, then ∑ a∈A can be replaced by ∑ a∈a in (1). In other words, only terms corresponding to actions in the slate are non-zero. While this condition does not hold in our environments, the EIB assumption is natural and implies that one can perform training for an environment ν modified as in Definition 2. Although the sum with fewer terms is easier to optimize, the problem is still combinatorial and does not scale. Therefore, monotonicity and submodularity are interesting to us since if f : ∪lj=0 Aj → R is monotonic and submodular, we can sequentially greedily choose a slate agreed and f(agreed) ≥ (1− 1/e) maxa f(a) [Fuj05]. Definition 3 (Monotonic and Submodular). We say that a function f : ∪lj=0X → R for X ⊂ Al is Monotonic if ∀a, a1, . . . , ai ∈ A it holds that f((a1, . . . , ai, a)) ≥ f((a1, . . . , ai)) and Submodular if (diminishing returns)\nf((a1, . . . , ai, a))− f((a1, . . . , ai)) ≤ f((a1, . . . , ai−1, a))− f((a1, . . . , ai−1))\nholds for all a, a1, . . . , ai ∈ A.\nTo guarantee monotonicity and submodularity we introduce a further assumption that we call sequential presentation since it is satisfied if the action-selection happens sequentially in the environment, e.g, if recommendations are presented one-by-one to a user or if the users are assumed to inspect them in such a manner. Although, our environments do not have sequential presentation, the sequentially greedy procedure works well. When we evaluate the choice of a first recommendation we look at it in the presence of the other recommendations provided by a default strategy. This brings our setting closer to sequential recommendations.\nDefinition 4 (Sequential Presentation). We say that a slate-MDP has sequential presentation if for all states s its action-execution probabilities satisfy\nPr(a|s, (a1, . . . , ai, a, ai+1, . . .)) = Pr(a|s, (a1, . . . , ai, a)) (2)\nand Pr(a| s, (a1, . . . , ai, a)) ≤ Pr(a| s, (a1, . . . , ai−1, a)).\nProposition 1. If a slate-MDP has sequential presentation and satisfies the fatal failure property then its state-slate value function Qπ is monotonic and submodular for all π.\nProof. Let ak = (a1, . . . , ak). For any vector a and any scalar a, let aa denote the vector constructed by concatenating a to a. Assume that a 6∈ ai. Also the rewards are nonnegative. Then, we have that Q(s,aia)−Q(s,ai) =∑ s′∈Sa′∈aia Pr(a′|s,aia)Qexec(s, a′, s′)− ∑ s′∈Sa′∈ai Pr(a′|s,ai)Qexec(s, a′, s′) =\n∑ s′∈Sa′∈ai [ Pr(a′|s,aia)−Pr(a′|s,ai) ] Qexec(s, a′, s′)+Pr(a|s,aia)Qexec(s, a, s′) =\nPr(a|s,aia)Qexec(s, a, s′).\nSequential presentation immediately implies that Pr(a|s,aia) ≤ Pr(a|s,ai−1a). This establishes thatQ(s,a) is indeed submodular in a. Monotonicity follows from (2).\nThe next section introduces agents based on the theory of this section. They learn the value of full slates and select a slate through a sequentially greedy procedure which under the sequential presentation assumption combined with EIB, is potentially performing slightly worse than combinatorial search. Further, motivated by EIB, training is performed on a modified environment for which fatal failure is satisfied.\nSlate agents We consider model-free agents that directly learn either the value of an individual action (Algorithm 1) or the value of a full slate (Algorithm 2). We perform the action selection for the latter in a way that only considers dependence on the actions in slots above. However, we still learn a value function which depends on a whole slate by using value function approximators that take both the features of the state and all the actions as arguments. We perform the maximization in a sequentially greedy manner and fill slots following the one being maximized with the same action that is being\nAlgorithm 1: Generic Simple (Top-K) Slate Agent Require: trainSteps, testSteps, update, ε ≥ 0, l ≥ 1\n1: t = 1, initialize θ for Qθ(s, a) 2: Receive initial state s and take random action a 3: Receive reward r and state s′. 4: repeat 5: Pick a′ ε-greedily (slate size 1) from Qθ(s′, ·) 6: Update θ using update(s, a, r, s′, a′) 7: s = s′, a = a′ 8: Perform action a in environment (slate size 1) 9: Receive new state s′ and reward r\n10: t = t+ 1 11: until t ≥ trainSteps 12: t = 1 13: Receive initial state s 14: repeat 15: Sort the available actions such that Qθ(s, ai) ≥ Qθ(s, ai+1) ∀i 16: Take slate-action a = (a1, .., al) 17: Receive reward r and state s′ 18: t = t+ 1, s = s′ 19: until t ≥ testSteps\nevaluated, while keeping previous ones fixed. Both Algorithm 1 and Algorithm 2 are stated in a generic manner while in our experiments we include useful techniques from [MKS+15, LHP+15] that stabilize and speed up learning, namely experience replay and target networks as in Algorithm 3. Algorithm 1 is presented in two phases; One with training using slate size 1 and one testing with slate size l. In our experiments we interleave test and training phases.\nDeterministic Policy Gradient (DPG) Learning of Slate Policies to Guide Attention To decrease the number of evaluations of the value function when choosing a slate, we attempt to learn an attention guiding policy that is trained to produce a slate that maximizes the learned value function as seen in Algorithm 3, which generalizes Algorithm 2 in which candidate actions are used as the nearest neighbors. The policy is optimized using gradient ascent on its parameters for Q ◦ π as a function of those and the state.\nThe main extra issue, besides the much higher dimensionality, compared to existing deterministic policy gradient work [SLH+14, LHP+15], is that instead of using the continuous action produced by the neural network, we must choose from a discrete subset. We resolve this by performing a k-nearest neighbor lookup among the available actions and either execute the nearest or evaluate Q for all the identified neighbors and pick the highest valued action. We introduce this approach in fuller detail and further developed for large action space in [DAES16]. The policy is still updated in the same way since we simply want it to produce vectors with as high Q-values as possible.\nAlgorithm 2: Generic Full Slate Require: Steps, update, ε ≥ 0, l ≥ 1\n1: t = 1, initialize weights θ for Qθ(s, ā) 2: Receive initial state s and take random slate-action ā 3: Receive reward r and state s′. 4: repeat 5: for i=1,l do 6: Set ai = arg maxa∈A(s′)Qθ(s\n′, a1, . . . , ai−1, a, a, . . .) 7: end for 8: a′ = (a1, . . . , al) 9: Update θ using update(s,a, r, s′,a′)\n10: s = s′, a = a′ 11: Perform slate-action a in environment 12: Receive new state s′ and reward r 13: t = t+ 1 14: until t ≥ Steps\nHowever, when we also learn Q we update based on Q(s, a) for the action actually taken. As in [SLH+14, LHP+15] the next action used for the TD-error (Q(s, a) − r− γQ′(s′, a′)) is the action π(s′) produced by the current target policy. To perform a nearest neighbor look-up for slates we focus on a slot at a time. We use the sequentially greedy maximization defined in Algorithm 2 but for each slot the choices are further restricted to only consider the result of that look-up."
    }, {
      "heading" : "3 Experimental comparison",
      "text" : "We perform an experimental comparison of a range of agents described in the previous section on a test environment of a generic template. The examples used in this study, have respectively 835, 1597, and 13138 states and actions represented by 100- dimensional vectors for 835 and 1597 and 200-dimensional for 13138.\nThe Test Environment: The test environments are such that A = S = {1, .., N} and N varies with the environment. An environment is defined by a transition weight matrix (from a real recommendation system) such that for each state i ∈ {1, . . . , N} and each action j ∈ {1, .., N} there is a real valued non-negative weightwi,j indicating how common it is that j follows i. For each i, only a limited number (larger than zero and at most 60) of wi,j are non-zero and the magnitude of a typical non-zero weight is 0.5. We refer to those j for which wi,j > 0 as the candidate actions for state i. Further, there is a weight wfail > 0.\nThe weight matrix represents a weighted directed graph, which is extracted as a subgraph from a very large full graph of the system, by choosing a seed node and performing a breadth first traversal to a limited depth and then pruning childless nodes in an iterative manner. There is also a reward rj ≥ 0 for each state j that is received upon transition to that state.\nAlgorithm 3: DPG+kNN 1: Randomly initialize Q(s, a|θQ) and policy π(s|θπ)\nwith weights θQ and θπ . 2: Initialize target network Q′ and π′ with weights θQ\n′ ← θQ, θπ′ ← θµ 3: Initialize replay buffer B 4: Receive initial observation state s1 5: for t = 1, T do 6: With probability 1− ε select action at as arg maxaQ(s, a|θQ) where a\nranges across the k nearest candidate actions of π(st|θµ), and with probability ε a random candidate action. For full slate agents, arg max is replaced by sequentially greedy maximization as in Algorithm 2.\n7: Perform at, receive reward rt and new state st+1 8: Store transition (st, at, rt, st+1) in B 9: Sample (s, a, r, s′) from B and choose a′ as at\nwas chosen above but with s′, Q′ and π′. 10: Set y = r + γQ′(s′, a′)|θQ′) 11: Update Q by gradient updates for the loss: L = (y −Q(s, a|θQ)2 12: Update the policy π using the sampled gradient:\n∇θπQ ◦ π|s ≈ ∇aQ(s, a|θQ)|π(s)∇θππ(s|θπ)|s\n13: Update the target networks:\nθQ ′ ← τθQ + (1− τ)θQ ′\nθπ ′ ← τθπ + (1− τ)θπ ′\n14: end for\nWhen an agent in state s produces a slate a, each action a ∈ a has a probability of being executed that is proportional (not counting duplicates) towa log2(i+1) (standard discount in information retrieval [JK02, CMS10]) where i is the position in the slate and the probability that no action is executed is proportional to wfail. If no action from the slate is executed, the environment transitions to a uniformly random next state and the agent receive the corresponding reward. After this transition, there is a probability (here 0.2) of the episode ending. If a ∈ a was executed the environment transitions to s′ = a, the reward is received and the episode ends with a fixed probability (here 0.1).\nThe Agents For all agents’ Q-functions, we use function approximators that are feedforward neural networks with two hidden layers, each with a 100 units. The policies are feed-forward neural networks with two hidden layers with 25 hidden units each. Fewer units suffice since we only need an approximate location of high values. We use learning rate η = 10−3 and target network update rate τ = 10−4. In line with theory\npresented in the previous section, training is performed on a modified version of the environment in which the episode ends with zero reward when an action not from the slate is executed. The update routine is a gradient step on a squared (L2) loss between Qθ(st, at) and rt + γQ′(st+1, a′), where Q′ is the target Q-network and a′ the action produced by the target policy network at state st+1. The target network parameters θ′ slowly track θ through the update θ′t+1 = (1 − τ)θ′t + τθ and similarly for the target policy network. Algorithm 3 details these procedures.\nEvaluation We evaluate our full slate agents and simple top-K agents on the three environments with slate sizes 1, 5 and 10. The full slate agents are evaluated in three variations with different number of actions in the slate. The cheapest version (in number of evaluations) immediately picks the action, for each slot, whose features are nearest (in L2 distance) to the vector produced by the policy. The most expensive agent considers all candidate actions and we also evaluate an agent that, for each slot, only considers the 10% nearest. We ran each experiment 6 times with different random seeds and plotted the average total reward per episode (averaged both over seeds and 1000 episodes at evaluation with ε = 0) in Figures 2-7 for which a moving average with window length 100 has also been employed. The error bars show one standard deviation. Figures 2, 3 and 4 compares different number of neighbors for N = 835, N = 1597 and 13138. Figures 5, 6 and 7 compare full and simple agents at different slate sizes for the same environments.\nResults We see the full slate agents performing much better overall than the simple top-K agents that we employ as a baseline. The baseline is relevant since agents used in recommendation systems are often of that form although they typically focus on recommendations being accepted [PKCK12]. Unlike the simple agents, full slate agents always perform well for larger slate sizes. The simple agents are unable to learn to avoid including actions with high weight but with lower value than the top pick. For slate size 1, the simple top-K agent coincides with the full slate agent which evaluates all candidate actions, hence these two agents are shown as one agent. Further, we can see that the curve for agents that only evaluate 10% of the candidate actions is almost identical to the one for agents that evaluate all. The nearest neighbor agent that simply picks the nearest neighbor is slightly worse and has larger variability than the other two. However, as we demonstrate in a further experiment that also highlight the ability to learn non-myopically, the nearest neighbor agent can outperform the other agents. The variability of the nearest neighbor agent aids exploration and the attention can help when Q is not estimated well everywhere.\nRisk-Seeking In the case of the N = 13138 environment in particular, it is possible to perform much better than we have already seen. In fact the performance seen in Figure 4 only reaches the performance of the optimal myopic policy. For this environment there are far better policies. We perform a simple modification to our agent (slate size 1, all neighbors) to make it more likely to discover multi-step paths to high reward outcomes. We transform the reward that the agent is training on by replacing r with rα, while still evaluating with the orginal reward. We see that for a wide range of exponents we eventually see far superior performance compared to α = 1. We refer to the agents with α > 1 as risk-seeking in line with prospect theory [KT79]."
    }, {
      "heading" : "4 Conclusions",
      "text" : "We introduced agents that successfully address sequential decision problems with highdimensional combinatorial slate-action spaces, found in important applications including recommendation systems. We focus on slate Markov Decision Processes introduced here, providing a formal framework for such applications. The new agents’ superiority over relevant baselines was demonstrated on a range of environments derived from real world data in a live recommendation system."
    } ],
    "references" : [ {
      "title" : "The use of MMR, diversity-based reranking for reordering documents and producing summaries",
      "author" : [ "Jaime Carbonell", "Jade Goldstein" ],
      "venue" : "In In SIGIR,",
      "citeRegEx" : "Carbonell and Goldstein.,? \\Q1998\\E",
      "shortCiteRegEx" : "Carbonell and Goldstein.",
      "year" : 1998
    }, {
      "title" : "Search engines: information retrieval in practice",
      "author" : [ "W. Bruce Croft", "Donald Metzler", "Trevor Strohman" ],
      "venue" : null,
      "citeRegEx" : "Croft et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Croft et al\\.",
      "year" : 2010
    }, {
      "title" : "Fast reinforcement learning in large discrete action spaces",
      "author" : [ "Gabriel Dulac-Arnold", "Richard Evans", "Peter Sunehag" ],
      "venue" : "In preparation,",
      "citeRegEx" : "Dulac.Arnold et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dulac.Arnold et al\\.",
      "year" : 2016
    }, {
      "title" : "Non-deterministic policies in markovian decision processes",
      "author" : [ "M.M. Fard", "J. Pineau" ],
      "venue" : "J. Artif. Intell. Res. (JAIR),",
      "citeRegEx" : "Fard and Pineau.,? \\Q2011\\E",
      "shortCiteRegEx" : "Fard and Pineau.",
      "year" : 2011
    }, {
      "title" : "Submodular Functions and Optimization: Second Edition",
      "author" : [ "S. Fujishige" ],
      "venue" : "Annals of Discrete Mathematics. Elsevier Science,",
      "citeRegEx" : "Fujishige.,? \\Q2005\\E",
      "shortCiteRegEx" : "Fujishige.",
      "year" : 2005
    }, {
      "title" : "Cumulated gain-based evaluation of ir techniques",
      "author" : [ "K. Järvelin", "J. Kekäläinen" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "Järvelin and Kekäläinen.,? \\Q2002\\E",
      "shortCiteRegEx" : "Järvelin and Kekäläinen.",
      "year" : 2002
    }, {
      "title" : "Non-Stochastic Bandit Slate Problems",
      "author" : [ "S. Kale", "L. Reyzin", "R. Schapire" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kale et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kale et al\\.",
      "year" : 2010
    }, {
      "title" : "Prospect theory: An analysis of decisions under risk",
      "author" : [ "Daniel Kahneman", "Amos Tversky" ],
      "venue" : null,
      "citeRegEx" : "Kahneman and Tversky.,? \\Q1979\\E",
      "shortCiteRegEx" : "Kahneman and Tversky.",
      "year" : 1979
    }, {
      "title" : "Matroid bandits: Fast combinatorial optimization with learning",
      "author" : [ "B. Kveton", "Z. Wen", "A. Ashkan", "H. Eydgahi", "B. Eriksson" ],
      "venue" : "CoRR, abs/1403.5045,",
      "citeRegEx" : "Kveton et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kveton et al\\.",
      "year" : 2014
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra" ],
      "venue" : "CoRR, abs/1509.02971,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2015
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Autonomous inverted helicopter flight via reinforcement learning",
      "author" : [ "A. Ng", "A. Coates", "M. Diel", "V. Ganapathi", "J. Schulte", "B. Tse", "E. Berger", "E. Liang" ],
      "venue" : "In Experimental Robotics IX,",
      "citeRegEx" : "Ng et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2004
    }, {
      "title" : "A literature review and classification of recommender systems research",
      "author" : [ "Deuk Hee Park", "Hyea Kyeong Kim", "Il Young Choi", "Jae Kyeong Kim" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "Park et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2012
    }, {
      "title" : "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "author" : [ "M. Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman.,? \\Q1994\\E",
      "shortCiteRegEx" : "Puterman.",
      "year" : 1994
    }, {
      "title" : "Artificial Intelligence: A Modern Approach",
      "author" : [ "S.J. Russell", "P. Norvig" ],
      "venue" : null,
      "citeRegEx" : "Russell and Norvig.,? \\Q2010\\E",
      "shortCiteRegEx" : "Russell and Norvig.",
      "year" : 2010
    }, {
      "title" : "Reinforcement Learning",
      "author" : [ "R. Sutton", "A. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Deterministic policy gradient algorithms",
      "author" : [ "David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin A. Riedmiller" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Silver et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2014
    }, {
      "title" : "Using continuous action spaces to solve discrete problems",
      "author" : [ "Hado Van Hasselt", "Marco Wiering" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "Hasselt and Wiering,? \\Q2009\\E",
      "shortCiteRegEx" : "Hasselt and Wiering",
      "year" : 2009
    }, {
      "title" : "Linear submodular bandits and their application to diversified retrieval",
      "author" : [ "Y. Yue", "C. Guestrin" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Yue and Guestrin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yue and Guestrin.",
      "year" : 2011
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Many real-world problems come with action spaces represented as feature vectors. Although high-dimensional control is a largely unsolved problem, there has recently been progress for modest dimensionalities. Here we report on a successful attempt at addressing problems of dimensionality as high as 2000, of a particular form. Motivated by important applications such as recommendation systems that do not fit the standard reinforcement learning frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A Slate-MDP is an MDP with a combinatorial action space consisting of slates (tuples) of primitive actions of which one is executed in an underlying MDP. The agent does not control the choice of this executed action and the action might not even be from the slate, e.g., for recommendation systems for which all recommendations can be ignored. We use deep Q-learning based on feature representations of both the state and action to learn the value of whole slates. Unlike existing methods, we optimize for both the combinatorial and sequential aspects of our tasks. The new agent’s superiority over agents that either ignore the combinatorial or sequential long-term value aspect is demonstrated on a range of environments with dynamics from a real-world recommendation system. Further, we use deep deterministic policy gradients to learn a policy that for each position of the slate, guides attention towards the part of the action space in which the value is the highest and we only evaluate actions in this area. The attention is used within a sequentially greedy procedure leveraging submodularity. Finally, we show how introducing risk-seeking can dramatically imporve the agents performance and ability to discover more far reaching strategies. 1 ar X iv :1 51 2. 01 12 4v 1 [ cs .A I] 3 D ec 2 01 5",
    "creator" : "LaTeX with hyperref package"
  }
}