{
  "name" : "1606.02206.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Minimax Approach to Supervised Learning",
    "authors" : [ "Farzan Farnia", "David Tse" ],
    "emails" : [ "farnia@stanford.edu", "dntse@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Supervised learning, the task of inferring a function that predicts a target Y from a feature vector X = (X1, . . . , Xd) by using n labeled training samples {(x1, y1), . . . , (xn, yn)}, has been a problem of central interest in machine learning. Given the underlying distribution P̃X,Y , the optimal prediction rules had long been studied and formulated in the statistics literature. However, the advent of highdimensional problems raised this important question that what would be an optimal prediction rule when we do not have enough samples to estimate the underlying distribution?\nTo understand the difficulty of learning in high-dimensional settings, consider a classification task for a genome-wide association studies (GWAS) problem where we seek to predict a binary label Y from an observation of 3, 000, 000 SNPs, each of which is a categorical variable Xi ∈ {0, 1, 2}. Hence, to estimate the underlying distribution we need O(33,000,000) samples, that is impossible.\nWith no possibility of estimating the underlying P ∗ in such problems, several methods have been proposed to deal with high-dimensional settings. A standard approach in statistical learning is the empirical risk minimization (ERM) [1]. ERM learns the prediction rule by minimizing an approximated loss under the empirical distribution of samples P̂ . However, to avoid overfitting ERM restricts the set of allowable decision rules to a class of functions with limited complexity, such as hypothesis classes of small VC dimension or spaces of norm-bounded linear functions.\nAs a complementary approach to ERM, one can learn the prediction rule through minimizing a decision rule’s worst-case loss over a larger set of distributions Γ(P̂ ) centered at the empirical distribution P̂ . In other words, instead of restricting the class of decision rules, we consider and evaluate all possible decision rules, but based on a more stringent criterion that they will have to perform well over all distributions in Γ(P̂ ). As seen in Figure 1, this minimax approach can be broken into three main steps: First, we compute the empirical distribution P̂ from the data; Second, we form a distribution set Γ(P̂ ) based on P̂ ; Finally, we learn a prediction rule ψ∗ that minimizes the worst-case expected loss over Γ(P̂ ). ∗Department of Electrical Engineering, Stanford University, Stanford, CA 94305.\nar X\niv :1\n60 6.\n02 20\n6v 1\n[ st\nat .M\nL ]\n7 J\nun 2\n01 6\nSome special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme. In this paper, we develop a general minimax approach for supervised learning problems with arbitrary loss function.\nTo formulate Step 3 in Figure 1, given a general loss function L and set of distribution Γ(P̂ ) we generalize the problem formulation discussed at [3] to\nargmin ψ∈Ψ max P∈Γ(P̂ )\nE [ L ( Y, ψ(X) ) ] . (1)\nHere, Ψ is the space of all decision rules. Notice the difference with the ERM setting where Ψ was restricted to smaller function classes while Γ(P̂ ) = {P̂}. If we have to predict Y with no access to X, (1) will reduce to the formulation studied at [5]. There, the authors propose to use the principle of maximum entropy [6], for a generalized definition of entropy, to find the optimal prediction minimizing the worst-case loss. By the principle of maximum entropy, we should select and predict based on a distribution in Γ(P̂ ) that maximizes the entropy function.\nHow can we use the principle of maximum entropy to solve (1) when we observe X as well? A natural idea is to apply the maximum entropy principle to the conditional PY |X=x instead of the marginal PY . This idea motivates a generalized version of the principle of maximum entropy, which we call the principle of maximum conditional entropy. In fact, this principle breaks Step 3 in Figure 1 into two smaller steps: First, we search for P ∗ the distribution maximizing the conditional entropy over Γ(P̂ ); Then, we find ψ∗ the optimal decision rule for P ∗.\nAlthough the principle of maximum conditional entropy characterizes the solution to (1), computing the maximizing distribution is hard, in general. In [7], the authors propose a conditional version of the principle of maximum entropy, for the specific case of Shannon entropy, and draw the principle’s connection to (1). They call it the principle of minimum mutual information, by which one should predict based on the distribution minimizing mutual information among X and Y . However, they develop their theory targeting a broad class of distribution sets, which results in a convex problem yet with an exponential number of variables in the dimension of the problem.\nIn this paper, we propose to fix the marginal PX across the distributions in Γ(P̂ ) to find the right structure for the distribution set. Note that for a prediction task the goal is to learn the conditional distribution PY |X. Thus, through convex duality we require to learn only the dual variables corresponding to the constraints Γ(P̂ ) enforces on PY |X. Therefore, if the empirical marginal P̂X provides sufficient knowledge of the underlying P̃X to learn those dual variables, we can learn a predictive model. Moreover, by imposing this specific structure on Γ(P̂ ), (1) reduces to an unconstrained convex problem with a number of variables linear in the number of constraints on PY |X in Γ(P̂ ).\nMore importantly, by applying the described idea for the generalized conditional entropy we provide a generalization of the duality derived in [8] between maximum conditional (Shannon) entropy and maximum likelihood for logistic regression. This generalization justifies all generalized linear models via a unified minimax framework. In particular, we show how under quadratic and logarithmic loss\nfunctions our framework leads to the linear regression and logistic regression models respectively. Through the same framework, we also derive two classification algorithms which we call the minimax SVM and the minimax Brier classifier. The minimax SVM, which is a relaxed version of the standard SVM, minimizes the worst-case 0-1 loss and by our numerical experiments outperforms the SVM. Note that ERM with the 0-1 loss is known to be NP-hard [9]. The minimax Brier classifier justifies making binary classification using the Huber penalty and extends this binary classification technique to a multi-class version. Finally, we discuss the framework application in robust feature selection."
    }, {
      "heading" : "2 Principle of Maximum Conditional Entropy",
      "text" : "In this section, we provide a conditional version of the key definitions and results developed in [5]. We propose the principle of maximum conditional entropy to break Step 3 into 3a and 3b in Figure 1. We also define and characterize Bayes decision rules under different loss functions to address Step 3b."
    }, {
      "heading" : "2.1 Decision Problems, Bayes Decision Rules, Conditional Entropy",
      "text" : "Consider a decision problem. Here the decision maker observes X ∈ X from which she predicts a random target variable Y ∈ Y using an action a ∈ A. Let PX,Y = (PX , PY |X) be the underlying distribution for the random pair (X,Y ). Given a loss function L : Y ×A → [0,∞], L(y, a) indicates the loss suffered by the decision maker by deciding action a when Y = y. The decision maker uses a decision rule ψ : X → A to select an action a = ψ(x) from A based on an observation x ∈ X . We will in general allow the decision rules to be random, i.e. ψ is random. The main purpose of extending to the space of randomized decision rules is to form a convex set of decision rules. Later in Theorem 2, this convexity is used to prove a saddle-point theorem.\nWe call a (randomized) decision rule ψBayes a Bayes decision rule if for all decision rules ψ and for all x ∈ X :\nE[L(Y, ψBayes(X))|X = x] ≤ E[L(Y, ψ(X))|X = x].\nIt should be noted that ψBayes depends only on PY |X , i.e. it remains a Bayes decision rule under a different PX . Although we are not generally guaranteed that a Bayes decision rule exists, we can define conditional entropy of Y given X = x as\nH(Y |X = x) := inf ψ E[L(Y, ψ(X))|X = x], (2)\nand the conditional entropy of Y given X as H(Y |X) := ∑ x PX(x)H(Y |X = x). (3)\nWe can also define an (unconditional) entropy [5]\nH(Y ) := inf a∈A\nE[L(Y, a)]. (4)\nNote that H(Y |X = x) and H(Y |X) are both concave in PY |X . Applying Jensen’s inequality, this concavity implies that\nH(Y |X) ≤ H(Y ),\nwhich motivates the following definition for the information that X carries about Y ,\nI(X;Y ) := H(Y )−H(Y |X), (5)\ni.e. the reduction of expected loss in predicting Y by observing X . In [10], the author has defined the same concept to which he calls a coherent dependence measure. It can be seen that I(X;Y ) = EPX [D(PY |X , PY ) ] where D is the divergence measure corresponding to the loss L, defined for any two probability distributions PY , QY with Bayes actions aP , aQ as [5]\nD(PY , QY ) := EP [L(Y, aQ)]− EP [L(Y, aP )] = EP [L(Y, aQ)]−HP (Y ). (6)"
    }, {
      "heading" : "2.2 Examples",
      "text" : ""
    }, {
      "heading" : "2.2.1 Logarithmic Loss",
      "text" : "For any y ∈ Y and distribution QY , define\nLlog(y,QY ) = − logQY (y). (7)\nIt can be seen that under the logarithmic loss Hlog(Y ), Hlog(Y |X), Ilog(X;Y ) are the well-known unconditional, conditional Shannon entropy and mutual information [11]. The divergence measure is the well-known KL-divergence. Also, the Bayes decision rule for every distribution PX,Y is given by\nψBayes(x) = PY |X(·|x). (8)"
    }, {
      "heading" : "2.2.2 0-1 loss function",
      "text" : "The 0-1 loss function is defined for any y, ŷ ∈ Y as L0-1(y, ŷ) = I(ŷ 6= y). Then, we can show\nH0-1(Y ) = 1−max y∈Y PY (y), H0-1(Y |X) = 1− ∑ x∈X max y∈Y PX,Y (x, y).\nUnder the 0-1 loss function, the Bayes decision rule for a distribution PX,Y is the well-known maximum a posteriori (MAP) rule, i.e.\nψBayes(x) = argmax y∈Y\nPY |X(y|x). (9)"
    }, {
      "heading" : "2.2.3 Quadratic loss function",
      "text" : "The quadratic loss function is defined as L2(y, ŷ) = (y − ŷ)2. It can be seen\nH2(Y ) = Var(Y ), H2(Y |X) = E [Var(Y |X)], I2(X;Y ) = Var (E[Y |X]) .\nAlso, the Bayes decision rule for any PX,Y is the well-known minimum mean-square error (MMSE) estimator that is\nψBayes(x) = E[Y |X = x]. (10)"
    }, {
      "heading" : "2.2.4 Brier loss function",
      "text" : "Unlike logarithmic loss and 0-1 loss functions, the quadratic loss function does not make perfect sense for a discrete variable Y. The Brier loss function [12] is an adjusted version of the quadratic loss function targeting a discrete Y , where for any distribution QY on Y and an outcome y ∈ Y ,\nLBR(y,QY ) = ‖δy − qY ‖22. (11)\nHere δy denotes a vector of size |Y|, 1 at index y and 0 elsewhere, and qY stands for the vector of probabilities for QY . Then,\nHBR(Y ) = 1− ‖pY ‖22, HBR(Y |X) = 1− E [ PY |X(Y |X) ] ,\nGiven the distribution PX,Y the Bayes decision rule is uniquely\nψBayes(x) = PY |X(·|x). (12)\nConnection to the maximal correlation: Consider the well-known Pearson correlation coefficient ρ(X,Y ) = COV(X,Y )σXσY that measures the linear dependence among random variables X and Y . To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-Rényi) maximal correlation has been proposed in the probability literature [13–15]. The HGR maximal correlation of two random variables X,Y is defined as\nρm(X;Y ) = sup f,g ρ (f(X), g(Y )) , (13)\nwhere the supremum is taken over all functions f, g with finite non-zero variance. In [15], it has been shown the maximal correlation satisfies several interesting properties. Here, we connect this measure to the information under the Brier loss IBR(X ; Y ).\nTheorem 1. Suppose Y ∈ Y = {0, . . . , t} takes each value i with probability pi. Consider Y ’s one-hot encoding as Yi = I(Y = i), then\nIBR(X ; Y ) = t∑ i=0 pi(1− pi) ρ2m(X;Yi). (14)\nProof. Refer to the Appendix to see the proof.\nCorollary 1. For a binary Y ∈ Y = {0, 1},\nIBR(X ; Y ) = 2 p0(1− p0) ρ2m(X;Y ). (15)"
    }, {
      "heading" : "2.3 Principle of Maximum Conditional Entropy & Robust Bayes decision rules",
      "text" : "Given a distribution set Γ, consider the following minimax problem to find a decision rule minimizing the worst-case expected loss over Γ\nargmin ψ∈Ψ max P∈Γ\nEP [L(Y, ψ(X))], (16)\nwhere Ψ is the space of all randomized mappings from X to A and EP denotes the expected value over distribution P . Note that by a randomized mapping ψ we mean a random selection of members of F , the space of deterministic functions from X to A, according to a certain distribution. We call any solution ψ∗ to the above problem a robust Bayes decision rule against Γ. When Γ is convex, the following theorem guarantees the existence of a saddle point for (16), under some mild conditions. Therefore, Theorem 2 motivates a generalization of the maximum entropy principle to find robust Bayes decision rules. Theorem 2. Suppose that Γ is convex and that under any P ∈ Γ there exists a Bayes decision rule. We also assume the continuity of Bayes decision rules for distributions in Γ (See the Appendix for the exact condition). Then, if P ∗ maximizes H(Y |X) over Γ, a Bayes decision rule for P ∗ will be a robust Bayes decision rule against Γ.\nProof. Refer to the Appendix for the proof.\nPrinciple of Maximum Conditional Entropy: Given a set of distributions Γ, select and predict Y based on a distribution in Γ that maximizes the conditional entropy of Y given X , i.e.\nargmax P∈Γ\nH(Y |X) (17)"
    }, {
      "heading" : "3 Prediction via Maximum Conditional Entropy Principle",
      "text" : "Consider a prediction task with target variable Y and feature vector X = (X1, . . . , Xd). Note that we do not require the variables to be discrete. As discussed earlier, the maximum conditional entropy principle reduces (16) to (17). Notice that (16) and (17) formulate Steps 3 and 3a in Figure 1, respectively. However, a general formulation of (17) in terms of the joint distribution PX,Y leads to an exponential computational complexity in the feature dimension d.\nThe key question is therefore under what structures of Γ(P̂ ) in Step 2 we can solve (17) efficiently. In this section, we propose a specific structure for Γ(P̂ ), under which we provide an efficient solution to Steps 3a and 3b in Figure 1. In fact, we show (17) reduces to the maximum likelihood problem over a generalized linear model, under this specific structure.\nTo describe this structure, consider a set of distributions Γ(Q) centered around a given distribution QX,Y , where for a given norm ‖ · ‖, mapping vector θ(Y )t×1,\nΓ(Q) = { PX,Y : PX = QX , (18) ∀ 1 ≤ i ≤ t : ‖EP [θi(Y )X]− EQ [θi(Y )X] ‖ ≤ i }.\nHere θ encodes Y with t-dimensional θ(Y ), and θi(Y ) denotes the ith entry of θ(Y ). The first constraint in the definition of Γ(Q) says that all distributions in Γ(Q) share the same marginal on X\nas Q; the second imposes constraints on the cross-moments between X and Y , allowing for some uncertainty in estimation. When applied to our supervised learning framework, we will choose Q to be the empirical distribution P̂ and select θ appropriately based on the loss function L. However, for now we will consider the problem of solving (17) over Γ = Γ(Q) for general Q and θ.\nTo that end, we apply the Fenchel’s duality technique, also used at [16–18] to address f-divergence minimization problems. However, we consider a different version of convex conjugate for −H , which is defined with respect to θ. Considering PY as the set of all probability distributions for the variable Y , we define Fθ : Rt → R as the convex conjugate of −H(Y ) with respect to the mapping θ,\nFθ(z) := max P∈PY\nH(Y ) + E[θ(Y )]T z. (19)\nTheorem 3. Define Γ(Q), Fθ as given by (18), (19). Then the following duality holds\nmax P∈Γ(Q) H(Y |X) = min A∈Rt×d\nEQ [ Fθ(AX)− θ(Y )TAX ] + t∑ i=1 i‖Ai‖∗, (20)\nwhere ‖Ai‖∗ denotes ‖ · ‖’s dual norm of the A’s ith row. Furthermore, for the optimal P ∗ and A∗\nEP∗ [θ(Y ) |X = x ] = ∇Fθ (A∗x). (21)\nProof. The proof has been relegated to the the Appendix.\nWhen applying Theorem 3 on a supervised learning problem with a specific loss function, θ will be chosen such that EP∗ [θ(Y ) |X = x ] provides sufficient information to compute the Bayes decision rule Ψ∗ for P ∗. This enables the direct computation of Ψ∗, i.e. step 3 of Figure 1 , without the need to explicitly compute P ∗ itself. Later in this section, we will discuss three examples to see how this technique applies to different loss functions.\nWe make the key observation that the problem in the RHS of (20), when i = 0 for all i’s, is equivalent to minimizing the negative log-likelihood for fitting a generalized linear model [19] given by\n• An exponential family distribution p(y|η) = h(y) exp ( ηTθ(y)− Fθ(η) ) with the log-partition\nfunction Fθ and the sufficient statistic θ(Y ), • A linear predictor , η(X) = AX, • A link function such that E[θ(Y )|X = x] = ∇Fθ(η(x)).\nTherefore, Theorem 3 reveals a duality between the maximum conditional entropy problem and the regularized maximum likelihood problem for the specified generalized linear model. This duality further provides a justification for generalized linear models, since given a generalized linear model\nwe can consider the convex conjugate of its log-partition function as the negative entropy in the maximum conditional entropy framework.\nTo interpret this duality geometrically, note that by solving the regularized maximum likelihood problem in the RHS of (20), we in fact minimize a regularized KL-divergence\nargmin PY |X: (QX,PY |X)∈SF EQX [DKL(QY |X ||PY |X ) ] + t∑ i=1 i‖Ai(PY |X)‖∗, (22)\nwhere SF = { ( QX, PY |X(y|x) = h(y) exp(θ(y)TAx− Fθ(Ax) ) ) |A ∈ Rt×s} is the set of all exponential family distributions for the described GLM. This can be viewed as projecting Q onto SF (See Figure 2).\nFurthermore, considering the definition of divergence D given in (6), it can be seen that maximizing H(Y |X) over Γ(Q) in the LHS of (20) is equivalent to the following divergence minimization problem\nargmin PY |X: (QX,PY |X)∈Γ(Q) EQX [D(PY |X,UY |X) ] (23)\nwhere UY |X denotes the uniform conditional distribution. This can be interpreted as projecting the joint distribution (QX,UY |X) onto Γ(Q) (See Figure 2). Notice the difference of divergence measures and the ordering of distributions between (23) and (22). Then, the duality shown in Theorem 3 implies the following corollary. Corollary 2. The solution to (22) would also minimize (23), i.e. (22) ⊆ (23).\nTo connect the proposed framework to the ERM setting, suppose Q = P̂n is the empirical distribution of n samples drawn i.i.d. from the underlying distribution P̃ . Then the problem in the RHS of (20) is equivalent to the ERM problem\nmin ψ∈Ψ(SF )\nEQ[Llog(Y, ψ(X))] +R(ψ), (24)\nwhere Ψ(SF ) denotes the set of the logarithmic-loss (Llog) Bayes decision rules corresponding to the distributions in SF . Also, R(ψ) = ∑t i=1 i‖Ai(ψ)‖∗ is the added regularizer. Then, an important question is how to bound the excess risk, that is the difference between the expected loss of the two decision rules ψ̂n and ψ̃ minimizing (24) for the empirical distribution Q = P̂n and the underlying distribution Q = P̃ , respectively. Replacing the original regularizer ∑t i=1 i‖Ai‖∗ with\nthe strongly-convex λ ∑t i=1 ‖Ai‖2∗, we show the following theorem to bound the excess risk.\nTheorem 4. Let the regularizer R(ψ) = λ ∑t i=1 ‖Ai(ψ)‖2∗. Take ‖ · ‖/‖ · ‖∗ to be the `p/`q pair for 1p + 1 q = 1, 1 < q. Assume that ‖X‖p ≤ B and ‖θ(Y )‖∞ ≤ L. Then, for any δ, with probability at least 1− δ\nEP̃ [L(Y, ψ̂n(X))]− EP̃ [L(Y, ψ̃(X))] = O ( tL2B2 log( 1δ )\n(q − 1)λn\n) . (25)\nProof. Due to the definition given in (19), ∇Fθ(z) = EP [θ(Y )] for some distribution P . Therefore, ‖∇Fθ(z)‖∞ ≤ L and Fθ(z) − θ(Y )T z is 2L-Lipschitz in each entry zi. Then, the theorem is an immediate consequence of Theorem 1 in [20].\nIn the remaining of this section, we apply the described framework to the loss functions discussed at Subsection 2.2."
    }, {
      "heading" : "3.1 Logarithmic Loss: Logistic Regression",
      "text" : "For classifying Y ∈ Y = {1, . . . , t + 1}, let θ(Y ) be the one-hot encoding of variable Y , i.e. θi(Y ) = I(Y = i) for 1 ≤ i ≤ t. Here, we exclude i = t+1 as I(Y = t+1) = 1− ∑t i=1 I(Y = i). Given this θ, for the logarithmic loss\nFθ(z) = log ( 1+ t∑ j=1 exp(zj) ) , ∀1 ≤ i ≤ t : ( ∇Fθ(z) ) i = exp (zi) / ( 1+ t∑ j=1 exp(zj) ) , (26)\nthat gives the multinomial logistic regression model [21]. Also, the RHS of (20) would be the regularized maximum likelihood problem for this specific GLM. This discussion is well-studied in the literature and straightforward using the duality result shown in [8]."
    }, {
      "heading" : "3.2 0-1 Loss: Minimax SVM",
      "text" : "Consider the same classification setting and θ described at the beginning of last subsection. We show in the Appendix that for the 0-1 loss we can calculate the gradient of Fθ using the following procedure. Given z ∈ Rt, let z̃ = (z, 0). Let σ be the permutation sorting z̃ in a descending order, i.e. i ≤ j : z̃σ(i) ≥ z̃σ(j). We find the smallest k where ∑k i=1[z̃σ(i) − z̃σ(k+1) ] > 1. If this does not hold for any k, let k = t+ 1. Then,\n∀ 1 ≤ i ≤ t : ( ∇Fθ(z) ) i = { 1/k if σ(i) ≤ k, 0 Otherwise.\n(27)\nKnowing that Fθ(0) = t/(t+ 1) that is the 0-1 entropy of a uniformly distributed Y , the characterization of Fθ is complete.\nWith∇Fθ characterized, for Step 3b in Figure 1 we should apply the MAP rule to the output of∇Fθ . We can also learn the linear predictor (Step 3a) through applying the gradient descent to solve the RHS of (20). The classifier minimizes the worst-case 0-1 loss over Γ(Q). In particular, if Y is binary, i.e. t+ 1 = 2\nFθ(z) = max{ 0 , z + 1\n2 , z }. (28)\nThen, if Y = {−1, 1}, the RHS problem of (20) would be\nmin α\nEQ [ max { 0 ,\n1− YαTX 2\n, −YαTX }] + ‖α‖∗. (29)\nSince max{0, 1−z2 ,−z} ≤ max{0, 1− z}, if we replace ‖α‖∗ with λ‖α‖ 2 2 in (29), we get a relaxation of the standard SVM formulated with the hinge loss [22]. We therefore call this classification algorithm the minimax SVM. Note that unlike the standard SVM, the minimax SVM can be naturally extended to a multi-class classification algorithm through (27)."
    }, {
      "heading" : "3.3 Brier Loss: Minimax Brier Classifier",
      "text" : "Consider the same classification setting and θ defined in the last two subsections. In the Appendix, we show we can characterize the gradient of Fθ for the Brier loss by repeating the same procedure as in the minimax SVM with two modifications. First, we change the level for finding the smallest k as∑k i=1[z̃σ(i) − z̃σ(k+1) ] > 2, and second we modify (27) as\n∀ 1 ≤ i ≤ t : ( ∇Fθ(z) ) i =  ( 2− k∑ j=1 z̃σ(j) ) /(2k) + z̃σ(i)/2 if σ(i) ≤ k,\n0 Otherwise.\n(30)\nAs discussed in Subsection 2.2.4, the robust Bayes decision rule is the conditional P ∗Y |X of the distribution maximizing the conditional Brier entropy. Therefore, to make prediction, we can apply the MAP rule to the probability vector∇Fθ returns. We call this classification algorithm the minimax Brier Classifier (mmBC). For the binary case when t = 1, it can be seen\nFθ(z)− z\n2 =  1 8 z2 if |z| ≤ 2, 1\n2 |z| − 1 2 Otherwise,\n(31)\nwhich is the Huber penalty function [23]. This binary classification problem is the same classification problem formulated with the modified Huber loss function at [24]. Through the developed minimax framework, we can naturally extend this binary classification technique to a multi-class classification algorithm.\nBased on Corollary 1, for a binary Y the conditional Brier entropy-maximizing distribution is the distribution minimizing maximal correlation between X and Y in Γ(P̃ ). Assuming some extra conditions, [4] solves the minimax problem of finding the maximal correlation-minimizing distribution, but for a larger class of distributions where only pairwise marginals are fixed. One can see that their proposed solution is based on replacing the Huber function in (31) with the following quadratic function\nFθ(z)− z\n2 =\n1 8 z2, (32)\nand the condition under which their solution solves the original problem is that for the A∗ minimizing the RHS of (20), |A∗x| ≤ 1 for every input x. In [4], the authors also show for a binary prediction problem over a convex set of distributions Γ, there exists a randomized prediction rule based on the maximal correlation-minimizing distribution, achieving a worst-case misclassification rate of at most twice the minimum worst-case misclassification rate. Here, we generalize their result to a multi-class version using the connection between the Brier information and the maximal correlation shown in Theorem 1. Theorem 5. Consider a prediction problem for Y ∈ Y = {1, . . . , t}. Let p∗ denote the conditional PY |X of the distribution maximizing HBR(Y |X) over a convex set of distributions Γ. Define the randomized decision rule ψBR,\n∀i, 1 ≤ i ≤ t : ψBR(x) = i, w.p. p∗\n2 i|x∑t j=1 p ∗2 j|x . (33)\nThen the worst-case misclassification rate of ψBR is bounded by twice the minimum worst-case misclassification rate over Γ, i.e.\nmax P∈Γ P (ψBR(X) 6= Y ) ≤ 2 min ψ∈Ψ max P∈Γ P (ψ(X) 6= Y ).\nProof. The proof has been relegated to the Appendix.\nHence, we can predict by applying this randomized decision rule to the probability vector that ∇Fθ returns. We call this classification algorithm the minimax Randomized Brier Classifier (mmRBC). The above theorem suggests the minimax Brier classification as a natural extension of the results proven in [4] for the binary case."
    }, {
      "heading" : "3.4 Quadratic Loss: Linear Regression",
      "text" : "For a regression problem on Y ∈ Y = R, let θ(Y ) = Y be the identity function, so t = 1. To derive Fθ for the quadratic loss, note that if we let PY in (19) include all possible distributions, the maximized entropy (variance for quadratic loss) and thus the Fθ value would be infinity. Therefore, in (19) we restrict PY to {PY : E[Y 2] ≤ ρ2} given a parameter ρ. We show in the Appendix that a slightly adjusted version of Theorem 3 remains valid after this change, and\nFθ(z)− ρ2 = { z2/4 if |z/2| ≤ ρ ρ(|z| − ρ) if |z/2| > ρ, (34)\nwhich is the Huber function [23]. To find the Bayes decision rule via (21), note that\ndFθ(z)\ndz =  −ρ if z/2 ≤ −ρ z/2 if − ρ < z/2 ≤ ρ ρ if ρ < z/2.\n(35)\nGiven the samples in a supervised learning problem if we choose the parameter ρ large enough, by solving the RHS of (20) when Fθ(z) is replaced with z2/4 and set ρ greater than maxi |A∗xi|, we can equivalently take Fθ(z) = z2/4 + ρ2 which by (21) gives the linear regression model. Then, the RHS of (20) would be equivalent to\n– Simple linear regression when = 0. – Lasso [25] when ‖ · ‖/‖ · ‖∗ is the `∞/`1 pair.\n– Ridge regression [21] when ‖ · ‖ is the `2-norm. – Group lasso [26] with the `1,p regularizer when we adjust Γ(Q)’s definition for disjoint subsets I1, . . . Ik of {1, . . . , d} as\nΓGL(Q) = { PX,Y : PX = QX , (36) ∀ 1 ≤ j ≤ k : ‖EP [ YXIj ] − EQ [ YXIj ] ‖q ≤ j }.\nHere, q is chosen such that 1/p + 1/q = 1. Also, XIj denotes the subvector including the Ij entries of X. See the Appendix for the proof of the group lasso case. Another type of minimax, but non-probabilistic, justification of the robustness of lasso and group lasso as regression algorithms can be found in [27, 28]."
    }, {
      "heading" : "4 Robust Feature Selection",
      "text" : "Using a minimax criterion over a set of distributions Γ, we solve the following problem to select the most informative subset of k features. Here, we evaluate a feature subset based on its minimum worst-case loss over Γ.\nargmin |S|≤k min ψ∈ΨS max P∈Γ\nEP [L(Y, ψ( XS )) ], (37)\nwhere XS denotes the feature vector X restricted to the indices in S. Theorem 2 reduces (37) to\nargmin |S|≤k max P∈Γ\nH(Y |XS ), (38)\nwhich under the assumption that H(Y ) is fixed across all distributions in Γ becomes equivalent to selecting a subset S maximizing the worst-case generalized information I(XS ;Y ) over Γ, i.e.\nargmax |S|≤k min P∈Γ I(XS ;Y ). (39)\nTo solve (38) when Γ = Γ(Q) (18), we apply the duality shown in Theorem 3 to obtain\nargmin A∈Rt×s: ‖A‖0,∞≤k\nEQ [ Fθ(AΦ(X))− θ(Y )TAΦ(X) ] + t∑ i=1 i‖Ai‖∗. (40)\nHere by constraining ‖A‖0,∞ = ‖ ( ‖A(1)‖∞, . . . , ‖A(s)‖∞ ) ‖0 where A(i) denotes the ith column of A, we impose the same sparsity pattern across the rows of A. Approximating the `0 with the convex `1 and taking ‖ · ‖∗ to be the `1-norm, we can approximate the solution by\nargmin A∈Rt×s\nEQ [ Fθ(AΦ(X))− θ(Y )TAΦ(X) ] + λ‖A‖1,∞. (41)\nIt is noteworthy that for the quadratic loss and identity θ, (41) is the same as the lasso [25]. Also, for the logarithmic loss and one-hot encoding θ, (41) is equivalent to the `1-regularized logistic regression. Hence, the `1-regularized logistic regression maximizes the worst-case mutual information over Γ(Q), which seems superior to the heuristic techniques for maximizing an approximation of the mutual information I(XS ;Y ) in the literature [29, 30]."
    }, {
      "heading" : "5 Numerical Experiments",
      "text" : "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete Rényi Classifiers (DRC) [4]. The results are summarized in Table 1 where the numbers indicate the percentage of error in the classification task.\nWe implemented the three mmSVM, mmBC, and mmRBC by applying gradient descent to solve the RHS of (20) with an added regularizer λ‖α‖22. We determined the value of λ by cross validation. To determine this coefficient, we used a randomly-selected 70% of the training set for training and\nthe rest 30% of the training set for testing. We tested the values in {2−10, . . . , 210}. Using the tuned lambda, we trained the algorithms over all the training set and then evaluated the error rate over the test set. We performed this procedure in 1000 Monte Carlo runs each training on 70% of the data points and testing on the rest 30% and averaged the results.\nAs seen in the table, the minimax Brier Classifier and the minimax SVM result in the best performance for five and three out of the six datasets, respectively. Observe that although our main theoretical guarantee is for the randomized Brier classifier, the non-randomized Brier classifier has outperformed the randomized Brier classifier in all the datasets. Also, except a single dataset the minimax SVM outperforms the SVM.\nTo compare these methods in high-dimensional problems, we ran an experiment over synthetic data with n = 200 samples and d = 10000 features. We generated features by i.i.d. Bernoulli with P (Xi = 1) = 0.75, and considered y = sign(γTx + z) where z ∼ N(0, 1). Using the same approach, we evaluated 20.6% error rate for SVM, 20.4% error rate for DRC, 20.0% for the mmSVM and 19.4% for the mmBC, which shows the mmSVM and mmBC can outperform SVM and DRC in high-dimensional settings as well."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Meisam Razaviyayn for helpful discussions on the topic and proofs."
    }, {
      "heading" : "6 Appendix",
      "text" : ""
    }, {
      "heading" : "6.1 Proof of Theorem 1",
      "text" : "In this proof, we use a known result for ρm(X;Z) for a BernoulliZ ∈ Z = {0, 1}with probabilities p0, p1 [32]. For simplicity, we use px,z and pz|x to denote PX,Z(x, z) and PZ|X(z|x), respectively. Then,\nρ2m(X;Z) = 1\np0p1 ∑ x [ p0p 2 x,1 + p1p 2 x,0 px ] − 1\n= 1\np0p1 ∑ x [ px ( p0p 2 1|x + p1p 2 0|x )] − 1\n= 1\np0p1 ∑ x [ px ( 1 2 (p21|x + p 2 0|x) + p0 − p1 2 (p21|x − p20|x) )] − 1\n= 1\n2p0p1\n( 1− p21 − p20 − 2 ∑ x pxp1|xp0|x ) = 1\np0p1\n( p1p0 − ∑ x pxp1|xp0|x ) .\nThen, we have t∑ i=0 pi(1− pi) ρ2m(X;Yi) = t∑ i=0 [ pi(1− pi)− ∑ x pxpi|x(1− pi|x) ]\n= t∑ i=0\n[ −p2i +\n∑ x pxp 2 i|x ] = E [ PY |X(Y |X)− PY (Y )\n] = IBR(X;Y )."
    }, {
      "heading" : "6.2 Proof of Theorem 2",
      "text" : "First, let us recall the assumptions of Theorem 2:\n• Γ is convex. • For any distribution P ∈ Γ, there exists a Bayes decision rule. • We assume continuity in Bayes decision rules, i.e., if a sequence of distributions (Qn)∞n=1 with the\ncorresponding Bayes decision rules (ψn)∞n=1 converges weakly to Q with a Bayes decision rule ψ, then under any P ∈ Γ, the expected loss of ψn converges to the expected loss of ψ.\n• P ∗ maximizes the conditional entropy H(Y |X).\nLet ψ∗ be a Bayes decision rule for P ∗. We need to show that ψ∗ is a robust Bayes decision rule against Γ. To show this, it suffices to show that (P ∗, ψ∗) is a saddle point of the mentioned minimax problem, i.e., EP∗ [L(Y, ψ∗(X))] ≤ EP∗ [L(Y, ψ(X))], (42) and EP∗ [L(Y, ψ∗(X))] ≥ EP [L(Y, ψ∗(X))]. (43) Clearly, inequality (42) holds due to the definition of the Bayes decision rule. To show (43), let us fix an arbitrary distribution P ∈ Γ. For any λ ∈ (0, 1], define Pλ = λP + (1− λ)P ∗. Notice that Pλ ∈ Γ since Γ is convex. Let ψλ be a Bayes decision rule for Pλ. Due to the linearity of the expected loss in the probability distribution, we have\nEP [L(Y, ψλ(X))]− EP∗ [L(Y, ψλ(X))] = EPλ [L(Y, ψλ(X))]− EP∗ [L(Y, ψλ(X))]\nλ\n≤ HPλ(Y |X)−HP ∗(Y |X) λ ≤ 0,\nfor any 0 < λ ≤ 1. Here the first inequality is due to the definition of the conditional entropy and the last inequality holds since P ∗ maximizes the conditional entropy over Γ. Applying the continuity assumption of the Bayes decision rules, we have\nEP [L(Y, ψ∗(X))]− EP∗ [L(Y, ψ∗(X))] = lim λ→0 EP [L(Y, ψλ(X))]− EP∗ [L(Y, ψλ(X))] ≤ 0, (44)\nwhich makes the proof complete."
    }, {
      "heading" : "6.3 Proof of Theorem 3",
      "text" : "Let us recall the definition of the set Γ(Q):\nΓ(Q) = { PX,Y : PX = QX , (45) ∀ 1 ≤ i ≤ t : ‖EP [θi(Y )X]− EQ [θi(Y )X] ‖ ≤ i }.\nDefining Ẽi , EQ [θi(Y )X] and Ci , {u : ‖u− Ẽi‖ ≤ i}, we have\nmax P∈Γ(Q) H(Y |X) = max P,w: ∀i: wi=EP [θi(Y )X] EQX [HP (Y |X = x)] + t∑ i=1 ICi(wi) (46)\nwhere IC is the indicator function for the set C defined as\nIC(x) = { 0 if x ∈ C, −∞ Otherwise.\n(47)\nFirst of all, the law of iterated expectations implies that EP [θi(Y )X] = EQX [ XE[θi(Y )|X = x] ] . Further-\nmore, problem (46) is convex and it is not hard to check that the Slater condition is satisfied. Hence strong duality holds and we can write the dual problem as\nmin A sup PY |X,w EQX\n[ HP (Y |X = x) +\nt∑ i=1 E[θi(Y )|X = x]AiX\n] +\nt∑ i=1 [ICi(wi)−Aiwi] , (48)\nwhere the rows of matrix A, denoted by Ai, are the Lagrange multipliers for the constraints of wi = EP [θi(Y )Φ(X)]. Notice that the above problem decomposes across PY |X=x’s and wi’s. Hence, the dual problem can be rewritten as\nmin A\n[ EQX [ sup\nPY |X=x HP (Y |X = x) + t∑ i=1 E[θi(Y )|X = x]AiX\n] +\nt∑ i=1 sup wi [ICi(wi)−Aiwi]\n] (49)\nFurthermore, according to the definition of Fθ, we have\nFθ(Ax) = sup PY |X=x\nH(Y |X = x) + E[θ(Y )|X = x]TAx. (50)\nMoreover, the definition of the dual norm ‖ · ‖∗ implies\nsup wi ICi(wi)−Aiwi = max u∈Ci Aiu = AiẼi + i‖Ai‖∗. (51)\nPlugging (50) and (51) in (49), the dual problem can be simplified to\nmin A EQX\n[ Fθ(AΦ(X))−\nt∑ i=1 AiẼi\n] +\nt∑ i=1 i‖Ai‖∗\n= min A\nEQ [ Fθ(AX)− θ(Y )TAX ] + t∑ i=1 i‖Ai‖∗, (52)\nwhich is equal to the primal problem (46) since the strong duality holds. Furthermore, applying Danskin’s theorem to (50) implies that\nEP∗ [θ(Y ) |X = x ] = ∇Fθ (A∗x). (53)"
    }, {
      "heading" : "6.4 Fθ derivation for the 0-1 Loss, minimax SVM",
      "text" : "Here, we derive ∇Fθ for the 0-1 loss function, where θ is the described one-hot encoding that is θi(Y ) = I(Y = i) for 1 ≤ i ≤ t. If P (Y = i) = pi for 1 ≤ i ≤ t+ 1, then\nH(Y ) + E[θ(Y )]T z = 1− max 1≤i≤t+1 pi + t∑ i=1 pizi. (54)\nHence, due to the Danskin’s theorem,\n∇Fθ(z) = argmax p∈Rt+1: p≥0,\n1Tp=1\nt∑ i=1 pizi − max 1≤i≤t+1 pi (55)\nTo solve the above problem we define z̃ = (z, 0) and rewrite the objective as\nt+1∑ i=1 piz̃i − max 1≤i≤t+1 pi. (56)\nThen, the optimal solution p∗ of (55) obeys the same order as the order of z̃. Without loss of generality suppose that z̃ is sorted in a descending order. Then, (55) is equivalent to\nargmax p∈Rt+1≥0 : 1 Tp=1,\n∀i≤j: pi≥pj\nt+1∑ i=1 piz̃i − p1 (57)\nNote that under the constraint 1Tp = 1, for any m ≤ t t+1∑ i=1 piz̃i−p1 = (z̃1− z̃m+1−1)p1 + m∑ i=2 [ (z̃i− z̃m+1)pi ] + z̃m+1− t+1∑ j=m+2 [ (z̃m+1− z̃j)pj ] . (58)\nFor the coefficients, we know z̃m+1 − z̃j is non-negative if j > m and non-positive if j < m. Let k be the smallest index for which ∑k i=1[z̃i − z̃m+1] > 1. If that does not hold for any k, let k = t+ 1. Then, according to (58), for any solution p∗ to (57)\n∀i > k : p∗i = 0. (59)\nThis is because if for some i ≥ k + 1, p∗i > 0 (let i be the largest index this happens), we construct a new feasible point p from p∗ by setting pi to be zero and for any j ≤ k let pj = p∗j +p∗i /k. Then, for the new feasible point p, we will get a larger objective, that is a contradiction to that p∗ maximizes the above objective. Now, we claim that p∗1 = p ∗ 2 = . . . = p ∗ k = 1/k. If this is not true, then there is an index i < k where p∗i − p∗i+1 > (i + 1) for some > 0. Now if we modify the solution as pj = p ∗ j − for any j ≤ i and pi+1 = p∗i+1 + i , we get a feasible point with the same order as p∗,\nbut since ∑i j=1[z̃j − z̃i+1] ≤ 1 the objective of (57) grows because of (58), that is a contradiction. Finally, this procedure characterizes the gradient as\n∀ 1 ≤ i ≤ t : ( ∇Fθ(z) ) i = { 1/k if i ≤ k, 0 Otherwise.\n(60)"
    }, {
      "heading" : "6.5 Fθ derivation for the Brier Loss, minimax Brier classifer",
      "text" : "Similar to the proof given for the 0-1 loss, we derive ∇Fθ for the Brier loss function with θ the described one-hot encoding. If P (Y = i) = pi for 1 ≤ i ≤ t+ 1, then\nH(Y ) + E[θ(Y )]T z = 1− t+1∑ i=1 [p2i ] + t∑ i=1 [pizi]. (61)\nHence, due to the Danskin’s theorem,\n∇Fθ(z) = argmax p∈Rt+1: p≥0,\n1Tp=1\nt∑ i=1 [pizi − p2i ]− p2t+1 (62)\nWe define z̃ = (z, 0) and rewrite the objective as\nt+1∑ i=1 [piz̃i − p2i ]. (63)\nThen, the optimal solution p∗ of (62) obeys the same order as the order of z̃. To characterize the solution to (62), we use the KKT conditions. It is not hard to check the Salter condition holds here; thus, the KKT conditions are necessary and sufficient [33]. According to KKT conditions, for any optimal solution p∗ there exists a dual solution λ∗ ≥ 0, β∗ where\n∀1 ≤ i ≤ t+ 1 : p∗i = 1\n2 (z̃i + λ\n∗ i + β ∗), λ∗i p ∗ i = 0. (64)\nSince p∗ has the same ordering as z̃, considering σ as the permutation sorting z̃ in a descending order, we find the smallest k such that ∑k i=1[z̃σ(i) − z̃σ(k+1) ] > 2 or let k = t+ 1 if the condition holds for no k ≤ t. We claim that the following feasible p satisfies (64) and hence provides a solution to (62).\n∀ 1 ≤ i ≤ t : pi =  ( 2− k∑ j=1 z̃σ(j) ) /2k + z̃σ(i)/2 if σ(i) ≤ k,\n0 Otherwise.\n(65)\nNote that due to the choice of k, p is a feasible point, i.e. p ≥ 0 and 1Tp = 1. Let β =( 2 − ∑k j=1 z̃σ(j) ) /k and λσ(i) = 0 for i ≤ k. Then, for i > k let λσ(i) = −z̃σ(i) − β =(\n−2+ ∑k j=1[z̃σ(j)− z̃σ(i)] ) /k ≥ 0, due to the choice of k. Therefore, p satisfies the KKT conditions\nand the procedure returns a solution to (62)."
    }, {
      "heading" : "6.6 Quadratic Loss: Linear Regression",
      "text" : ""
    }, {
      "heading" : "6.6.1 Fθ derivation",
      "text" : "Here, we find Fθ(z) = maxP∈PY H(Y ) + E[θ(Y )]T z for θ(Y ) = Y and PY = {PY : E[Y 2] ≤ ρ2}. Since for quadratic loss H(Y ) = Var(Y ) = E[Y 2]− E[Y ]2, the problem is equivalent to\nFθ(z) = max E[Y 2]≤ρ2\nE[Y 2]− E[Y ]2 + zE[Y ] (66)\nAs E[Y ]2 ≤ E[Y 2], it can be seen for the solution EP∗ [Y 2] = ρ2 and therefore we equivalently solve\nFθ(z) = max |E[Y ]|≤ρ ρ2 − E[Y ]2 + zE[Y ] = { ρ2 + z2/4 if |z/2| ≤ ρ ρ|z| if |z/2| > ρ. (67)"
    }, {
      "heading" : "6.6.2 Applying Theorem 3 while restricting PY",
      "text" : "For the quadratic loss, we first change PY = {PY : E[Y 2] ≤ ρ2} and then apply Theorem 3. Note that by modifying Fθ based on the new PY we also solve a modified version of the maximum conditional entropy problem\nmax P : PX,Y ∈Γ(Q) ∀x: PY |X=x∈PY\nH(Y |X) (68)\nIn the case PY = {PY : E[Y 2] ≤ ρ2} Theorem 3 remains valid given the above modification in the maximum conditional entropy problem. This is because the inequality constraint E[Y 2|X = x] ≤ ρ2 is linear in PY |X=x, and thus the problem remains convex and strong duality still holds. Also, when we move the constraints of wi = EP [θi(Y )X] to the objective function, we get a similar dual problem\nmin A\nsup PY |X,w:\n∀x: PY |X=x∈PY\nEQX [ HP (Y |X = x) + t∑ i=1 E[θi(Y )|X = x]AiX ] + t∑ i=1 [ICi(wi)−Aiwi]\n(69) Following the next steps of the proof of Theorem 3, the proof remains valid given the modification on Fθ and the maximum conditional entropy problem."
    }, {
      "heading" : "6.6.3 Derivation of group lasso",
      "text" : "To derive the group lasso, we slightly change the structure of Γ(Q). Given disjoint subsets I1, . . . , Ik, consider a set of distributions ΓGL(Q) with the following structure\nΓGL(Q) = { PX,Y : PX = QX , (70) ∀ 1 ≤ j ≤ k : ‖EP [ YXIj ] − EQ [ YXIj ] ‖ ≤ j }.\nNow we prove a modified version of Theorem 3,\nmax P∈ΓGL(Q) H(Y |X) = min α\nEQ [ Fθ(α TX)− YαTX ] + k∑ j=1 j‖αIj‖∗. (71)\nTo prove this identity, we can use the same proof provided for Theorem 3. We only need to redefine Ẽj = EQ [ YXIj ] and Cj = {u : ‖u− Ẽj‖ ≤ j} for 1 ≤ j ≤ k. Notice that here t = 1. Using the same technique in that proof, the dual problem is formulated as\nmin α sup PY |X,w\nEQX [ HP (Y |X = x) + E[Y |X = x]αTX ] + k∑ j=1 [ ICj (wIj )−αIjwIj ] . (72)\nSimilar to the proof of Theorem 3, we can decouple and simplify the above problem to show (71). Then, considering the problem for the quadratic loss and taking ‖ · ‖ as the `q-norm, we get the group lasso problem with the `1,p regularizer."
    }, {
      "heading" : "6.7 Proof of Theorem 5",
      "text" : "Since entropy measures the infimum expected loss given a distribution, it is sufficient to show that under any distribution P ∈ Γ the misclassification rate of ψBR is bounded by the maximum Brier entropy over Γ and the Brier entropy is generally bounded by twice the 0-1 entropy.\nTo show the first part, note that for any sequence (ai)ni=1,\n∀j : 2aj ≤ a2j∑ a2i\n+ ∑\na2i ,\n⇒ ∀j : 1− a2j∑ a2i ≤ 1− 2aj +\n∑ a2i ,\n⇒ ∀j : 1− a2j∑ a2i ≤ (1− aj)2 + ∑ i6=j a2i .\nTherefore, since the conditions of Theorem 2 hold, for any distribution P ∈ Γ\nP (ψBR(X) 6= Y ) = ∑ i,x px,i ( 1−\np∗ 2 i|x∑t j=1 p ∗2 j|x\n)\n≤ ∑ i,x px,i ( (1− p∗i|x) 2 + ∑ j 6=i p∗ 2 j|x ) = EP [ LBR(Y, P ∗ Y |X)\n] ≤ HBR(P ∗Y |X). (73)\nAlso, note that for any sequence (ai)ni=1,\n∀i : 2ai ≤ 1 + a2i ⇒ 2 max i ai ≤ 1 + ∑ i a2i ⇒ 1− ∑ i a2i ≤ 2(1−max i ai).\nTherefore, in general HBR(Y ) ≤ 2H0-1(Y ). (74)\nCombining (73) and (74), we have\nmax P∈Γ P (ψBR(X) 6= Y ) ≤ 2 max P∈Γ H0-1(Y |X) = 2 min ψ∈Ψ max P∈Γ P (ψ(X) 6= Y )."
    } ],
    "references" : [ {
      "title" : "A robust minimax approach to classification",
      "author" : [ "Gert RG Lanckriet", "Laurent El Ghaoui", "Chiranjib Bhattacharyya", "Michael I Jordan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Discrete chebyshev classifiers",
      "author" : [ "Elad Eban", "Elad Mezuman", "Amir Globerson" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Discrete rényi classifiers",
      "author" : [ "Meisam Razaviyayn", "Farzan Farnia", "David Tse" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Game theory, maximum entropy, minimum discrepancy and robust bayesian decision theory",
      "author" : [ "Peter D. Grünwald", "Philip Dawid" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Information theory and statistical mechanics",
      "author" : [ "Edwin T Jaynes" ],
      "venue" : "Physical review,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1957
    }, {
      "title" : "The minimum information principle for discriminative learning",
      "author" : [ "Amir Globerson", "Naftali Tishby" ],
      "venue" : "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "A maximum entropy approach to natural language processing",
      "author" : [ "Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1996
    }, {
      "title" : "Agnostic learning of monomials by halfspaces is hard",
      "author" : [ "Vitaly Feldman", "Venkatesan Guruswami", "Prasad Raghavendra", "Yi Wu" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Coherent measures of discrepancy, uncertainty and dependence, with applications to bayesian predictive experimental design",
      "author" : [ "Philip Dawid" ],
      "venue" : "Technical Report 139,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "Elements of information theory",
      "author" : [ "Thomas M Cover", "Joy A Thomas" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Verification of forecasts expressed in terms of probability",
      "author" : [ "Glenn W Brier" ],
      "venue" : "Monthly weather review,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1950
    }, {
      "title" : "A connection between correlation and contingency",
      "author" : [ "H.O. Hirschfeld" ],
      "venue" : "In Mathematical Proceedings of the Cambridge Philosophical Society,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1935
    }, {
      "title" : "Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung",
      "author" : [ "H. Gebelein" ],
      "venue" : "ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1941
    }, {
      "title" : "On measures of dependence",
      "author" : [ "A. Rényi" ],
      "venue" : "Acta mathematica hungarica,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1959
    }, {
      "title" : "Unifying divergence minimisation and statistical inference via convex duality",
      "author" : [ "Yasemin Altun", "Alexander Smola" ],
      "venue" : "In Learning Theory: Conference on Learning Theory COLT 2006,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Maximum entropy density estimation with generalized regularization and an application to species distribution modeling",
      "author" : [ "Miroslav Dudík", "Steven J Phillips", "Robert E Schapire" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Semi-supervised learning via generalized maximum entropy",
      "author" : [ "AN Erkan", "Y Altun", "Teh M Titterington" ],
      "venue" : "In Thirteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Generalized linear models, volume 37",
      "author" : [ "Peter McCullagh", "John A Nelder" ],
      "venue" : "CRC press,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1989
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "Karthik Sridharan", "Shai Shalev-Shwartz", "Nathan Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "The elements of statistical learning, volume 1",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2001
    }, {
      "title" : "Pattern recognition and machine learning",
      "author" : [ "Christopher M Bishop" ],
      "venue" : "springer,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    }, {
      "title" : "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
      "author" : [ "Tong Zhang" ],
      "venue" : "In Proceedings of the twenty-first international conference on Machine learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1996
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "Ming Yuan", "Yi Lin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Robust regression and lasso",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Shie Mannor" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "A unified robust regression model for lasso-like algorithms",
      "author" : [ "Wenzhuo Yang", "Huan Xu" ],
      "venue" : "In Proceedings of The International Conference on Machine Learning,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",
      "author" : [ "Hanchuan Peng", "Fuhui Long", "Chris Ding" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2005
    }, {
      "title" : "Normalized mutual information feature selection",
      "author" : [ "Pablo Estévez", "Michel Tesmer", "Claudio Perez", "Jacek M Zurada" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "Approximating discrete probability distributions with dependence trees",
      "author" : [ "CK Chow", "CN Liu" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1968
    }, {
      "title" : "On maximal correlation, hypercontractivity, and the data processing inequality studied by Erkip and Cover",
      "author" : [ "V. Anantharam", "A. Gohari", "S. Kamath", "C. Nair" ],
      "venue" : "arXiv preprint arXiv:1304.6133,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.",
      "startOffset" : 281,
      "endOffset" : 284
    }, {
      "referenceID" : 2,
      "context" : "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.",
      "startOffset" : 402,
      "endOffset" : 405
    }, {
      "referenceID" : 1,
      "context" : "To formulate Step 3 in Figure 1, given a general loss function L and set of distribution Γ(P̂ ) we generalize the problem formulation discussed at [3] to argmin ψ∈Ψ max P∈Γ(P̂ ) E [ L ( Y, ψ(X) ) ] .",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "If we have to predict Y with no access to X, (1) will reduce to the formulation studied at [5].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "There, the authors propose to use the principle of maximum entropy [6], for a generalized definition of entropy, to find the optimal prediction minimizing the worst-case loss.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "In [7], the authors propose a conditional version of the principle of maximum entropy, for the specific case of Shannon entropy, and draw the principle’s connection to (1).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "More importantly, by applying the described idea for the generalized conditional entropy we provide a generalization of the duality derived in [8] between maximum conditional (Shannon) entropy and maximum likelihood for logistic regression.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "Note that ERM with the 0-1 loss is known to be NP-hard [9].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "In this section, we provide a conditional version of the key definitions and results developed in [5].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "We can also define an (unconditional) entropy [5]",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "In [10], the author has defined the same concept to which he calls a coherent dependence measure.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "It can be seen that I(X;Y ) = EPX [D(PY |X , PY ) ] where D is the divergence measure corresponding to the loss L, defined for any two probability distributions PY , QY with Bayes actions aP , aQ as [5] D(PY , QY ) := EP [L(Y, aQ)]− EP [L(Y, aP )] = EP [L(Y, aQ)]−HP (Y ).",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "(7) It can be seen that under the logarithmic loss Hlog(Y ), Hlog(Y |X), Ilog(X;Y ) are the well-known unconditional, conditional Shannon entropy and mutual information [11].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 10,
      "context" : "The Brier loss function [12] is an adjusted version of the quadratic loss function targeting a discrete Y , where for any distribution QY on Y and an outcome y ∈ Y , LBR(y,QY ) = ‖δ − qY ‖2.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-Rényi) maximal correlation has been proposed in the probability literature [13–15].",
      "startOffset" : 178,
      "endOffset" : 185
    }, {
      "referenceID" : 12,
      "context" : "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-Rényi) maximal correlation has been proposed in the probability literature [13–15].",
      "startOffset" : 178,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-Rényi) maximal correlation has been proposed in the probability literature [13–15].",
      "startOffset" : 178,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "In [15], it has been shown the maximal correlation satisfies several interesting properties.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "To that end, we apply the Fenchel’s duality technique, also used at [16–18] to address f-divergence minimization problems.",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "To that end, we apply the Fenchel’s duality technique, also used at [16–18] to address f-divergence minimization problems.",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "To that end, we apply the Fenchel’s duality technique, also used at [16–18] to address f-divergence minimization problems.",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "We make the key observation that the problem in the RHS of (20), when i = 0 for all i’s, is equivalent to minimizing the negative log-likelihood for fitting a generalized linear model [19] given by • An exponential family distribution p(y|η) = h(y) exp ( ηθ(y)− Fθ(η) ) with the log-partition function Fθ and the sufficient statistic θ(Y ), • A linear predictor , η(X) = AX, • A link function such that E[θ(Y )|X = x] = ∇Fθ(η(x)).",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "Then, the theorem is an immediate consequence of Theorem 1 in [20].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "that gives the multinomial logistic regression model [21].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "This discussion is well-studied in the literature and straightforward using the duality result shown in [8].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "Since max{0, 1−z 2 ,−z} ≤ max{0, 1− z}, if we replace ‖α‖∗ with λ‖α‖ 2 2 in (29), we get a relaxation of the standard SVM formulated with the hinge loss [22].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 21,
      "context" : "This binary classification problem is the same classification problem formulated with the modified Huber loss function at [24].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Assuming some extra conditions, [4] solves the minimax problem of finding the maximal correlation-minimizing distribution, but for a larger class of distributions where only pairwise marginals are fixed.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "In [4], the authors also show for a binary prediction problem over a convex set of distributions Γ, there exists a randomized prediction rule based on the maximal correlation-minimizing distribution, achieving a worst-case misclassification rate of at most twice the minimum worst-case misclassification rate.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "The above theorem suggests the minimax Brier classification as a natural extension of the results proven in [4] for the binary case.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 22,
      "context" : "– Lasso [25] when ‖ · ‖/‖ · ‖∗ is the `∞/`1 pair.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 19,
      "context" : "– Ridge regression [21] when ‖ · ‖ is the `2-norm.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 23,
      "context" : "– Group lasso [26] with the `1,p regularizer when we adjust Γ(Q)’s definition for disjoint subsets I1, .",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 24,
      "context" : "Another type of minimax, but non-probabilistic, justification of the robustness of lasso and group lasso as regression algorithms can be found in [27, 28].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 25,
      "context" : "Another type of minimax, but non-probabilistic, justification of the robustness of lasso and group lasso as regression algorithms can be found in [27, 28].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "It is noteworthy that for the quadratic loss and identity θ, (41) is the same as the lasso [25].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "Hence, the `1-regularized logistic regression maximizes the worst-case mutual information over Γ(Q), which seems superior to the heuristic techniques for maximizing an approximation of the mutual information I(XS ;Y ) in the literature [29, 30].",
      "startOffset" : 236,
      "endOffset" : 244
    }, {
      "referenceID" : 27,
      "context" : "Hence, the `1-regularized logistic regression maximizes the worst-case mutual information over Γ(Q), which seems superior to the heuristic techniques for maximizing an approximation of the mutual information I(XS ;Y ) in the literature [29, 30].",
      "startOffset" : 236,
      "endOffset" : 244
    }, {
      "referenceID" : 1,
      "context" : "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete Rényi Classifiers (DRC) [4].",
      "startOffset" : 313,
      "endOffset" : 316
    }, {
      "referenceID" : 0,
      "context" : "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete Rényi Classifiers (DRC) [4].",
      "startOffset" : 354,
      "endOffset" : 357
    }, {
      "referenceID" : 28,
      "context" : "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete Rényi Classifiers (DRC) [4].",
      "startOffset" : 392,
      "endOffset" : 396
    }, {
      "referenceID" : 2,
      "context" : "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete Rényi Classifiers (DRC) [4].",
      "startOffset" : 435,
      "endOffset" : 438
    } ],
    "year" : 2017,
    "abstractText" : "Given a task of predicting Y from X , a loss function L, and a set of probability distributions Γ, what is the optimal decision rule minimizing the worst-case expected loss over Γ? In this paper, we address this question by introducing a generalization of the principle of maximum entropy. Applying this principle to sets of distributions with a proposed structure, we develop a general minimax approach for supervised learning problems, that reduces to the maximum likelihood problem over generalized linear models. Through this framework, we develop two classification algorithms called the minimax SVM and the minimax Brier classifier. The minimax SVM, which is a relaxed version of the standard SVM, minimizes the worst-case 0-1 loss over the structured set of distribution, and by our numerical experiments can outperform the SVM. We also explore the application of the developed framework in robust feature selection.",
    "creator" : "LaTeX with hyperref package"
  }
}