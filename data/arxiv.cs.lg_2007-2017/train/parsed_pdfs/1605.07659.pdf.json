{
  "name" : "1605.07659.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy",
    "authors" : [ "Aryan Mokhtari" ],
    "emails" : [ "aryanm@seas.upenn.edu", "aribeiro@seas.upenn.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A hallmark of empirical risk minimization (ERM) on large datasets is that evaluating descent directions requires a complete pass over the dataset. Since this is undesirable due to the large number of training samples, stochastic optimization algorithms with descent directions estimated from a subset of samples are the method of choice. First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster convergence. A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].\nWhen it comes to stochastic second order methods the first challenge is that while evaluation of Hessians is as costly as evaluation of gradients, the stochastic estimation of Hessians has proven more challenging. This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10]. Despite this incipient progress it is nonetheless fair to say that the striking success in developing stochastic first order methods is not matched by equal success in the development of stochastic second order methods. This is because even if the problem of estimating a Hessian is solved there are still four challenges left in the implementation of Newton-like methods in ERM:\n(i) Global convergence of Newton’s method requires implementation of a line search subroutine and line searches in ERM require a complete pass over the dataset.\n(ii) The quadratic convergence advantage of Newton’s method manifests close to the optimal solution but there is no point in solving ERM problems beyond their statistical accuracy.\n(iii) Newton’s method works for strongly convex functions but loss functions are not strongly convex for many ERM problems of practical importance.\n(iv) Newton’s method requires inversion of Hessians which is costly in large dimensional ERM.\nar X\niv :1\n60 5.\n07 65\n9v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\nBecause they can’t use line searches [cf. (i)], must work on problems that may be not strongly convex [cf. (iii)], and never operate very close to the optimal solution [cf (ii)], stochastic Newtonlike methods never experience quadratic convergence. They do improve convergence constants in ill-conditioned problems but they still converge at linear rates.\nIn this paper we attempt to overcome (i)-(iv) with the Ada Newton algorithm that combines the use of Newton iterations with adaptive sample sizes [6]. Say the total number of available samples is N , consider subsets of n ≤ N samples, and suppose the statistical accuracy of the ERM associated with n samples is Vn (Section 2). In Ada Newton we add a quadratic regularization term of order Vn to the empirical risk – so that the regularized risk also has statistical accuracy Vn – and assume that for a certain initial sample size m0, the problem has been solved to its statistical accuracy Vm0 . The sample size is then increased by a factor α > 1 to n = αm0. We proceed to perform a single Newton iteration with unit stepsize and prove that the result of this update solves this extended ERM problem to its statistical accuracy (Section 3). This permits a second increase of the sample size by a factor α and a second Newton iteration that is likewise guaranteed to solve the problem to its statistical accuracy. Overall, this permits minimizing the empirical risk in α/(α− 1) passes over the dataset and inverting logαN Hessians. Our theoretical and numerical analyses indicate that we can make α = 2. In that cases we can optimize to within statistical accuracy in about 2 passes over the dataset and after inversion of about 3.32 log10N Hessians."
    }, {
      "heading" : "2 Empirical risk minimization",
      "text" : "We want to solve ERM problems to their statistical accuracy. To state this problem formally consider an argument w ∈ Rp, a random variable Z with realizations z and a convex loss function f(w; z). We want to find an argument w∗ that minimizes the statistical average loss L(w) := EZ [f(w, Z)],\nw∗ := argmin w L(w) = argmin w EZ [f(w, Z)]. (1)\nThe loss in (1) can’t be evaluated because the distribution of Z is unknown. We have, however, access to a training set T = {z1, . . . , zN} containing N independent samples z1, . . . , zN that we can use to estimate L(w). We therefore consider a subset Sn ⊆ T and settle for minimization of the empirical risk Ln(w) := (1/n) ∑n k=1 f(w, zk),\nw†n := argmin w Ln(w) = argmin w\n1\nn n∑ k=1 f(w, zk), (2)\nwhere, without loss of generality, we have assumed Sn = {z1, . . . , zn} contains the first n elements of T . The difference between the empirical risk in (2) and the statistical loss in (1) is a fundamental quantities in statistical learning. We assume here that there exists a constant Vn, which depends on the number of samples n, that upper bounds their difference for all w with high probability (w.h.p),\nsup w |L(w)− Ln(w)| ≤ Vn, w.h.p. (3)\nThat the statement in (3) holds with w.h.p means that there exists a constant δ such that the inequality holds with probability at least 1 − δ. The constant Vn depends on δ but we keep that dependency implicit to simplify notation. For subsequent discussions, observe that bounds Vn of order Vn = O(1/ √ n) date back to the seminal work of Vapnik – see e.g., [26, Section 3.4]. Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]\nAn important consequence of (1) is that there is no point in solving (2) to an accuracy higher than Vn. Indeed, if we find a variable w for which Ln(wn)−Ln(w†) ≤ Vn finding a better approximation of w† is moot because (3) implies that this is not necessarily a better approximation of the minimizer w∗ of the statistical loss. We say the variable wn solves the ERM problem in (2) to within its statistical accuracy. In particular, this implies that adding a regularization of order Vn to (2) yields a problem that is essentially equivalent. We can then consider a quadratic regularizer of the form cVn/2‖w‖2 to define the regularized empirical risk Rn(w) := Ln(w) + (cVn/2)‖w‖2 and the corresponding optimal argument\nw∗n := argmin w Rn(w) = argmin w Ln(w) + cVn 2 ‖w‖2. (4)\nSince the regularization in (4) is of order Vn and (3) holds, the difference between Rn(w∗n) and L(w∗) is also of order Vn – this may be not as immediate as it seems; see [23]. Thus, we can say that a variable wn satisfying Rn(wn) − Rn(w∗n) ≤ Vn solves the ERM problem to within its statistical accuracy. We accomplish this in this paper with the Ada Newton algorithm."
    }, {
      "heading" : "3 Ada Newton",
      "text" : "To solve (4) suppose the problem has been solved to within its statistical accuracy for a set Sm ⊂ Sn withm = n/α samples. Therefore, we have found a variable wm for whichRm(wm)−Rm(w∗m) ≤ Vm. We want to update wm to obtain a variable wn that estimates w∗n with accuracy Vn. To do so compute the gradient of the risk Rn evaluated at wm\n∇Rn(wm) = 1\nn n∑ k=1 ∇f(wm, zk) + cVnwm, (5)\nas well as the Hessian Hn of Rn evaluated at wm\nHn := ∇2Rn(wm) = 1\nn n∑ k=1 ∇2f(wm, zk) + cVnI, (6)\nand update wm with the Newton step of the regularized risk Rn to compute\nwn = wm −H−1n ∇Rn(wm). (7)\nThe main contribution of this paper is to derive a condition that guarantees that wn solves Rn to within its statistical accuracy Vn.\nTheorem 1. Consider the variable wm as a Vm-optimal solution of the risk Rm, i.e., a solution such that Rm(wm) − Rm(w∗m) ≤ Vm. Let n = αm > m, consider the risk Rn associated with sample set Sn ⊃ Sm, and suppose assumptions 1 - 3 hold. If the sample size n is chosen such that(\n2(M + cVm)Vm cVn\n)1/2 +\n2(n−m) nc1/2 +\n( (2 + √ 2)c1/2 + c‖w∗‖ ) (Vm − Vn)\n(cVn)1/2 ≤ 1 4 (8)\nand\n144 ( Vm +\n2(n−m) n (Vn−m + Vm) + 2 (Vm − Vn) + c(Vm − Vn) 2 ‖w∗‖2\n)2 ≤ Vn (9)\nare satisfied, then the variable wn, which is the outcome of applying one Newton step on the variable wm as in (7), has sub-optimality error Vn with high probability, i.e.,\nRn(wn)−Rn(w∗n) ≤ Vn, w.h.p. (10)\nProof. See section 4.\nTheorem 1 states conditions under which we can iteratively increase the sample size while applying single Newton iterations without line search and staying within the statistical accuracy of the regularized empirical risk. The constants in (8) and (9) are not easy to parse but we can understand them qualitatively if we focus on large m. This results in a simpler condition that we state next.\nProposition 2. Consider a learning problem in which the statistical accuracy satisfies Vm ≤ αVn for n = αm and limn→∞ Vn = 0. If the regularization constant c is chosen so that(\n2αM\nc\n)1/2 +\n2α\n(α− 1)c1/2 <\n1 4 , (11)\nthen, there exists a sample size m̃ such that (8) and (9) are satisfied for all m > m̃ and n = αm. In particular, if α = 2 we can satisfy (8) and (9) with c > 64( √ M + 2)2.\nAlgorithm 1 Ada Newton 1: Parameters: Sample size increase constants α0 > 1 and 0 < β < 1. 2: Input: Initial sample size n = m0 and argument wn = wm0 with ‖∇Rn(wn)‖ < ( √ 2c)Vn\n3: while n ≤ N do {main loop} 4: Update argument and index: wm = wn and m = n. Reset factor α = α0 . 5: repeat {sample size backtracking loop} 6: Increase sample size: n = max{αm,N}. 7: Compute gradient [cf. (5)]: ∇Rn(wm) = (1/n) ∑n k=1∇f(wm, zk) + cVnwm\n8: Compute Hessian [cf. (6)]: Hn = (1/n) ∑n k=1∇2f(wm, zk) + cVnI\n9: Newton Update [cf. (7)]: wn = wm −H−1n ∇Rn(wm) 10: Compute gradient [cf. (5)]: ∇Rn(wn) = (1/n) ∑n k=1∇f(wn, zk) + cVnwn 11: Backtrack sample size increase α = βα. 12: until ‖∇Rn(wn)‖ > ( √ 2c)Vn\n13: end while\nProof. That the condition in (9) is satisfied for all m > m̃ follows simply because the left hand side is of order V 2m and the right hand side is of order Vn. To show that the condition in (8) is satisfied for sufficiently large m observe that the third summand in (8) is of order O((Vm − Vn)/V 1/2n ) and vanishes for large m. In the second summand of (8) we make n = αm to obtain the second summand in (11) and in the first summand replace the ratio Vm/Vn by its bound α to obtain the first summand of (11). To conclude the proof just observe that the inequality in (11) is strict.\nThe condition Vm ≤ αVn is satisfied if Vn = 1/n and is also satisfied if Vn = 1/ √ n because√\nα < α. This means that for most ERM problems we can progress geometrically over the sample size and arrive at a solution wN that solves the ERM problem RN to its statistical accuracy VN as long as (11) is satisfied .\nThe result in Theorem 1 motivates definition of the Ada Newton algorithm that we summarize in Algorithm 1. The core of the algorithm is in steps 6-9. Step 6 implements an increase in the sample size by a factor α and steps 7-9 implement the Newton iteration in (5)-(7). The required input to the algorithm is an initial sample size m0 and a variable wm0 that is known to solve the ERM problem with accuracy Vm0 . Observe that this initial iterate doesn’t have to be computed with Newton iterations. The initial problem to be solved contains a moderate number of samples m0, a mild condition number because it is regularized with constant cVm0 , and is to be solved to a moderate accuracy Vm0 – recall that Vm0 is of order Vm0 = O(1/m0) or order Vm0 = O(1/ √ m0) depending on regularity assumptions. Stochastic first order methods excel at solving problems with moderate number of samples m0 and moderate condition to moderate accuracy.\nWe remark that the conditions in Theorem 1 and Proposition 2 are conceptual but that the constants involved are unknown in practice. In particular, this means that the allowed values of the factor α that controls the growth of the sample size are unknown a priori. We solve this problem in Algorithm 1 by backtracking the increase in the sample size until we guarantee that wn minimizes the empirical risk Rn(wn) to within its statistical accuracy. This backtracking of the sample size is implemented in Step 11 and the optimality condition of wn is checked in Step 12. The condition in Step 12 is on the gradient norm that, because Rn is strongly convex, can be used to bound the suboptimality Rn(wn)−Rn(w∗n) as\nRn(wn)−Rn(w∗n) ≤ 1\n2cVn ‖∇Rn(wn)‖2. (12)\nObserve that checking this condition requires an extra gradient computation undertaken in Step 10. That computation can be reused in the computation of the gradient in Step 5 once we exit the backtracking loop. We emphasize that when the condition in (11) is satisfied, there exist a sufficiently large m for which the conditions in Theorem 1 are satisfied for n = αm. This means that the backtracking condition in Step 12 is satisfied after one iteration and that, eventually, Ada Newton progresses by increasing the sample size by a factor α. This means that Algorithm 1 can\nbe thought of as having a damped phase where the sample size increases by a factor smaller than ρ and a geometric phase where the sample size grows by a factor ρ in all subsequent iterations. The computational cost of this geometric phase is of not more than α/(α− 1) passes over the dataset and requires inverting not more than logαN Hessians. If c > 64( √ M + 2)2, we make α = 2 for optimizing to within statistical accuracy in about 2 passes over the dataset and after inversion of about 3.32 log10N Hessians."
    }, {
      "heading" : "4 Convergence Analysis",
      "text" : "In this section we study the proof of Theorem 1. In proving this result we first assume the following conditions are satisfied. Assumption 1. The loss functions f(w, z) are convex with respect to w for all values of z. Moreover, their gradients∇f(w, z) are Lipschitz continuous with constant M\n‖∇f(w, z)−∇f(w′, z)‖ ≤M‖w −w′‖, for all z. (13) Assumption 2. The loss functions f(w, z) are self-concordant with respect to w for all z. Assumption 3. The difference between the gradients of the empirical loss Ln and the statistical average loss L is bounded by V 1/2n for all w with high probability,\nsup w ‖∇L(w)−∇Ln(w)‖ ≤ V 1/2n , w.h.p. (14)\nThe conditions in Assumption 1 imply that the average loss L(w) and the empirical loss Ln(w) are convex and their gradients are Lipschitz continuous with constant M . Thus, the empirical risk Rn(w) is strongly convex with constant cVn and its gradients ∇Rn(w) are Lipschitz continuous with parameter M + cVn. Likewise, the condition in Assumption 2 implies that the average loss L(w), the empirical loss Ln(w), and the empirical risk Rn(w) are also self-concordant. The condition in Assumption 3 says that the gradients of the empirical risk converge to their statistical average at a rate of order V 1/2n . If the constant Vn in condition (3) is of order not faster than O(1/n) the condition in Assumption 3 holds if the gradients converge to their statistical average at a rate of order V 1/2n = O(1/ √ n). This is a conservative rate for the law of large numbers.\nThe main idea of the Ada Newton algorithm is introducing a policy for increasing the size of training set from m to n in a way that the current variable wm is in the Newton quadratic convergence phase for the next regularized empirical riskRn. In the following proposition, we characterize the required condition to guarantee staying in the local neighborhood of Newton’s method. Proposition 3. Consider the sets Sm and Sn as subsets of the training set T such that Sm ⊂ Sn ⊂ T . We assume that the number of samples in the sets Sm and Sn are m and n, respectively. Further, define wm as an Vm optimal solution of the riskRm, i.e., Rm(wm)−Rm(w∗m) ≤ Vm. In addition, define λn(w) := ( ∇Rn(w)T∇2Rn(w)−1∇Rn(w) )1/2 as the Newton decrement of variable w associated with the risk Rn. If Assumption 1-3 hold, then Newton’s method at point wm is in the quadratic convergence phase for the objective function Rn, i.e., λn(wm) < 1/4, if we have(\n2(M + cVm)Vm cVn\n)1/2 + (2(n−m)/n)V 1/2n + ( √ 2c+ 2 √ c+ c‖w∗‖)(Vm − Vn)\n(cVn)1/2 ≤ 1 4 w.h.p.\n(15)\nProof. See Section 7.1 in the Appendix.\nFrom the analysis of Newton’s method we know that if the Newton decrement λn(w) is smaller than 1/4, the variable w is in the local neighborhood of Newton’s method; see e.g., Chapter 9 of [5]. From the result in Proposition 3, we obtain a sufficient condition to guarantee that λn(wm) < 1/4 which implies that wm, which is a Vm optimal solution for the regularized empirical loss Rm, i.e., Rm(wm) − Rm(w∗m) ≤ Vm, is in the local neighborhood of the optimal argument of Rn that Newton’s method converges quadratically.\nUnfortunately, the quadratic convergence of Newton’s method for self-concordant functions is in terms of the Newton decrement λn(w) and it does not necessary guarantee quadratic convergence in terms of objective function error. To be more precise, we can show that λn(wn) ≤\nγλn(wm) 2; however, we can not conclude that the quadratic convergence of Newton’s method implies Rn(wn) − Rn(w∗n) ≤ γ′(Rn(wm) − Rn(w∗n))2. In the following proposition we try to characterize an upper bound for the error Rn(wn) − Rn(w∗n) in terms of the squared error (Rn(wm)−Rn(w∗n))2 using the quadratic convergence property of Newton decrement. Proposition 4. Consider wm as a variable that is in the local neighborhood of the optimal argument of the risk Rn where Newton’s method has a quadratic convergence rate, i.e., λn(wm) ≤ 1/4. Recall the definition of the variable wn in (7) as the updated variable using Newton step. If Assumption 1 and 2 hold, then the difference Rn(wn)−Rn(w∗n) is upper bounded by\nRn(wn)−Rn(w∗n) ≤ 144(Rn(wm)−Rn(w∗n))2. (16)\nProof. To prove the result in (16) first we need to find upper and lower bounds for the difference Rn(w)−Rn(w∗n) in terms of the Newton decrement parameter λn(w). To do so, we use the result in Theorem 4.1.11 of [17] which shows that\nλn(w)− ln (1 + λn(w)) ≤ Rn(w)−Rn(w∗n) ≤ −λn(w)− ln (1− λn(w)) . (17)\nNote that we assume that 0 < λn(w) < 1/4. Thus, we can use the Taylor’s expansion of ln(1 + a) for a = λn(w) to show that λn(w) − ln (1 + λn(w)) is bounded below by (1/2)λn(w)\n2 − (1/3)λn(w)3. Since 0 < λn(w) < 1/4 we can show that (1/6)λn(w)2 ≤ (1/2)λn(w)\n2 − (1/3)λn(w)3. Thus, the term λn(w) − ln (1 + λn(w)) is bounded below by (1/6)λ2. Likewise, we use Taylor’s expansion of ln(1 − a) for a = λn(w) to show that −λn(w) − ln (1− λn(w)) is bounded above by λn(w)2 for λn(w) < 1/4; see e.g., Chapter 9 of [5]. Considering these bounds and the inequalities in (17) we can write\n1 6 λn(w) 2 ≤ Rn(w)−Rn(w∗n) ≤ λn(w)2. (18)\nRecall that the variable wm satisfies the condition λn(wm) ≤ 1/4. Thus, according to the quadratic convergence rate of Newton’s method for self-concordant functions [5], we know that the Newton decrement has a quadratic convergence and we can write\nλn(wn) ≤ 2λn(wm)2. (19)\nWe use the result in (18) and (19) to show that the optimality error Rn(wn) − Rn(w∗n) has an upper bound which is proportional to (Rn(wm)−Rn(w∗n))2. In particular, we can writeRn(wn)− Rn(w ∗ n) ≤ λn(wn)2 based on the second inequality in (18). This observation in conjunction with the result in (19) implies that\nRn(wn)−Rn(w∗n) ≤ 4λn(wm)4. (20)\nThe first inequality in (18) implies that λn(wm)4 ≤ 36(Rn(wm) − Rn(w∗n))2. Thus, we can substitute λn(wm)4 in (20) by 36(Rn(wm)−Rn(w∗n))2 to obtain the result in (16).\nThe result in Proposition 4 provides an upper bound for the sub-optimality Rn(wn) − Rn(w∗n) in terms of the sub-optimality of variable wm for the riskRn, i.e., Rn(wm)−Rn(w∗n). Recall that we know that wm is in the statistical accuracy of Rm, i.e., Rm(wm)−Rm(w∗m) ≤ Vm, and we aim to show that the updated variable wn stays in the statistical accuracy ofRn, i.e.,Rn(wn)−Rn(w∗n) ≤ Vn. This can be done by showing that the upper bound for Rn(wn) − Rn(w∗n) in (16) is smaller than Vn. We proceed to derive an upper bound for the sub-optimality Rn(wm) − Rn(w∗n) in the following proposition. Proposition 5. Consider the sets Sm and Sn as subsets of the training set T such that Sm ⊂ Sn ⊂ T . We assume that the number of samples in the sets Sm and Sn are m and n, respectively. Further, define wm as an Vm optimal solution of the risk Rm, i.e., Rm(wm)−R∗m ≤ Vm. If Assumption 1-3 hold, then the empirical risk error Rn(wm)−Rn(w∗n) of the variable wm corresponding to the set Sn is bounded above by\nRn(wm)−Rn(w∗n) ≤ Vm+ 2(n−m)\nn (Vn−m + Vm)+2 (Vm − Vn)+ c(Vm − Vn) 2\n‖w∗‖2 w.h.p. (21)\nProof. See Section 7.2 in the Appendix.\nThe result in Proposition 5 characterizes the sub-optimality of the variable wm, which is an Vm sub-optimal solution for the risk Rm, with respect to the empirical risk Rn associated with the set Sn. The results in Proposition 3, 4, and 5 lead to the result in Theorem 1. To be more precise, from the result in Proposition 3 we obtain that the condition in (8) implies that wm is in the local neighborhood of the optimal argument of Rn and λn(wm) ≤ 1/4. Hence, the hypothesis of Proposition 4 is satisfied and we have Rn(wn) − Rn(w∗n) ≤ 144(Rn(wm) − Rn(w∗n))2. This result paired with the result in Proposition 5 shows that if the condition in (9) is satisfied we can conclude that Rn(wn)−Rn(w∗n) ≤ Vn which completes the proof of Theorem 1."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we study the performance of the proposed Ada Newton method and compare it with state-of-the-art in solving a large-scale classification problem. We use the protein homology dataset provided on KDD cup 2004 website. The dataset contains N = 145751 samples and the dimension of each sample is p = 74. We consider three algorithms to compare with the proposed Ada Newton method. One of them is the classic Newton’s method with backtracking line search. The second algorithm is Stochastic Gradient Descent (SGD) and the last one is the SAGA algorithm introduced in [7]. In our experiments, we use logistic loss and set the regularization parameters as c = 200 and Vn = 1/n.\nThe stepsize of SGD in our experiments is 2×10−2. Note that picking larger stepsize leads to faster but less accurate convergence and choosing smaller stepsize improves the accuracy convergence with the price of slower convergence rate. The stepsize for SAGA is hand-optimized and the best performance has been observed for α = 0.2 which is the one that we use in the experiments. For Newton’s method, the backtracking line search parameters are α = 0.4 and β = 0.5. In the implementation of Ada Newton we increase the size of the training set by factor 2 at each iteration, i.e., α = 2 and we observe that the condition ‖∇Rn(wn)‖ > ( √ 2c)Vn is always satisfied and there is no need for reducing the factor α. Moreover, the size of initial training set is m0 = 124. For the warmup step that we need to get into to the quadratic neighborhood of Newton’s method we use the gradient descent method. In particular, we run gradient descent with stepsize 10−3 for 100 iterations. Note that since the number of samples is very small at the beginning, m0 = 124, and the regularizer is very large, the condition number of problem is very small. Thus, gradient descent is able to converge to a good neighborhood of the optimal solution in a reasonable time. Notice that the computation of this warm up process is very low and is equal to 12400 gradient evaluations. This number of samples is less than 10% of the full training set. In other words, the cost is less than 10% of one pass over the dataset. Although, this cost is negligible, we consider it in comparison\nwith SGD, SAGA, and Newton’s method. We would like to mention that other algorithms such as Newton’s method and stochastic algorithms can also be used for the warm up process; however, the gradient descent method sounds the best option since the gradient evaluation is not costly and the problem is well-conditioned for a small training set .\nFigure 1 illustrates the convergence path of SGD, SAGA, Newton, and Ada Newton for the protein homology dataset. Note that the x axis is the total number of samples used divided by the size of the training set N = 145751 which we call number of passes over the dataset. As we observe, The best performance among the four algorithms belongs to Ada Newton. In particular, Ada Newton is able to achieve the accuracy of RN (w)− R∗n < 1/N by 2.4 passes over the dataset which is very close to theoretical result in Theorem 1 that guarantees accuracy of order O(1/N) after α/(α − 1) = 2 passes over the dataset. To achieve the same accuracy of 1/N Newton’s method requires 7.5 passes over the dataset, while SAGA needs 10 passes. The SGD algorithm can not achieve the statistical accuracy of order O(1/N) even after 25 passes over the dataset.\nAlthough, Ada Newton and Newton outperform SAGA and SGD, their computational complexity are different. We address this concern by comparing the algorithms in terms of runtime. Figure 2 demonstrates the convergence paths of the considered methods in terms of runtime. As we observe, Newton’s method requires more time to achieve the statistical accuracy of 1/N relative to SAGA. This observation justifies the belief that Newton’s method is not practical for large-scale optimization problems, since by enlarging p or making the initial solution worse the performance of Newton’s method will be even worse than the ones in Figure 1. Ada Newton resolves this issue by starting from small sample size which is computationally less costly. Ada Newton also requires Hessian inverse evaluations, but the number of inversions is proportional to logαN . Moreover, the performance of Ada Newton doesn’t depend on the initial point and the warm up process is not costly as we described before. We observe that Ada Newton outperforms SAGA significantly. In particular it achieves the statistical accuracy of 1/N in less than 25 seconds, while SAGA achieves the same accuracy in 62 seconds. Note that since the variable wN is in the quadratic neighborhood of Newton’s method for RN the convergence path of Ada Newton becomes quadratic eventually when the size of the training set becomes equal to the size of the full dataset. It follows that the advantage of Ada Newton with respect to SAGA is more significant if we look for a sub-optimality less than Vn. We have observed similar performances for other datasets such as A9A, COVTYPE, and SUSY."
    }, {
      "heading" : "6 Discussions",
      "text" : "As explained in Section 4, Theorem 1 holds because condition (8) makes wm part of the quadratic convergence region of Rn. From this fact, it follows that the Newton iteration makes the subopti-\nmality gapRn(wn)−Rn(w∗n) the square of the suboptimality gapRn(wm)−Rn(w∗n). This yields condition (9) and is the fact that makes Newton steps valuable in increasing the sample size. If we replace Newton iterations by any method with linear convergence rate, the orders of both sides on condition (9) are the same. This would make aggressive increase of the sample size unlikely.\nIn Section 1 we pointed out four reasons that challenge the development of stochastic Newton methods. It would not be entirely accurate to call Ada Newton a stochastic method because it doesn’t rely on stochastic descent directions. It is, nonetheless, a method for ERM that makes pithy use of the dataset. The challenges listed in Section 1 are overcome by Ada Newton because:\n(i) Ada Newton does not use line searches. Optimality improvement is guaranteed by increasing the sample size.\n(ii) The advantages of Newton’s method are exploited by increasing the sample size at a rate that keeps the solution for sample size m in the quadratic convergence region of the risk associated with sample size n = αm. This allows aggressive growth of the sample size.\n(iii) The ERM problem is not necessarily strongly convex. A regularization of order Vn is added to construct the empirical risk Rn\n(iv) Ada Newton inverts only logαN Hessians.\nIt is fair to point out that items (ii) and (iv) are true only to the extent that the damped phase in Algorithm 1 is not significant. Our numerical experiments indicate that this is true but the conclusion is not warranted by out theoretical bounds except when the dataset is very large. This suggests the bounds are loose and that further research is warranted to develop tighter bounds."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank Hadi Daneshmand, Aurelien Lucci, and Thomas Hofmann for useful discussions on the use of adaptive sample sizes for solving large-scale ERM problems and on the importance of using adaptive regularization coefficients."
    }, {
      "heading" : "7 Appendix",
      "text" : "In this section we study the proofs of Propositions 3 and 5. To do so, first we prove Lemmata 6 and 7 which are intermediate results that we use in proving the mentioned propositions.\nWe start the analysis by providing an upper bound for the difference between the loss functions Ln and Lm. The upper bound is studied in the following lemma which uses the condition in (3).\nLemma 6. Consider Ln and Lm as the empirical losses of the sets Sn and Sm, respectively, where they are chosen such that Sm ⊂ Sn. If we define n and m as the number of samples in the training sets Sn and Sm, respectively, then the absolute value of the difference between the empirical losses is bounded above by\n|Ln(w)− Lm(w)| ≤ n−m n (Vn−m + Vm) , w.h.p. (22)\nfor any w.\nProof. First we characterize the difference between the difference of the loss functions associated with the sets Sm and Sn. To do so, consider the difference\nLn(w)− Lm(w) = 1\nn ∑ i∈Sn fi(w)− 1 m ∑ i∈Sm fi(w). (23)\nNotice that the set Sm is a subset of the set Sn and we can write Sn = Sm ∪ Sn−m. Thus, we can rewrite the right hand side of (23) as\nLn(w)− Lm(w) = 1\nn ∑ i∈Sm fi(w) + ∑ i∈Sn−m fi(w) − 1 m ∑ i∈Sm fi(w)\n= 1\nn ∑ i∈Sn−m fi(w)− n−m mn ∑ i∈Sm fi(w). (24)\nFactoring (n−m)/n from the terms in the right hand side of (24) follows\nLn(w)− Lm(w) = n−m n  1 n−m ∑ i∈Sn−m fi(w)− 1 m ∑ i∈Sm fi(w)  (25) Now add and subtract the statistical loss L(w) to obtain\n|Ln(w)− Lm(w)| = n−m n ∣∣∣∣∣∣ 1n−m ∑\ni∈Sn−m\nfi(w)− L(w) + L(w)− 1\nm ∑ i∈Sm fi(w) ∣∣∣∣∣∣ ≤ n−m\nn (Vn−m + Vm) . (26)\nwhere the last inequality follows by using the triangle inequality and the upper bound in (3).\nThe result in Lemma 6 shows that the upper bound for the difference between the loss functions associated with the sets Sm and Sn where Sm ⊂ Sn is proportional to the difference between the size of these two sets n−m. This result will help us later to understand how much we can increase the size of the training set at each iteration. In other words, how large the difference n −m could be, while we have the statistical accuracy.\nIn the following lemma, we characterize an upper bound for the norm of the optimal argument w∗n of the empirical risk Rn(w) in terms of the norm of statistical average loss L(w) optimal argument w∗. Lemma 7. Consider Ln as the empirical loss of the set Sn and L as the statistical average loss. Moreover, recall w∗ as the optimal argument of the statistical average loss L, i.e., w∗ = argminw L(w). If Assumption 1 holds, then the norm of the optimal argument w ∗ n of the regularized empirical risk Rn(w) := Ln(w) + cVn‖w‖2 is bounded above by\n‖w∗n‖2 ≤ 4\nc + ‖w∗‖2, w.h.p. (27)\nProof. The optimality condition of w∗n for the the regularized empirical risk Rn(w) = Ln(w) + (cVn)/2‖w‖2 implies that\nLn(w ∗ n) + cVn 2 ‖w∗n‖2 ≤ Ln(w∗) + cVn 2 ‖w∗‖2. (28)\nBy regrouping the terms we obtain that the squared norm ‖w∗n‖2 is bonded above by\n‖w∗n‖2 ≤ 2\ncVn (Ln(w\n∗)− Ln(w∗n)) + ‖w∗‖2. (29)\nWe proceed to bound the difference Ln(w∗)−Ln(w∗n). By adding and subtracting the terms L(w∗) and L(w∗n) we obtain that\nLn(w ∗)− Ln(w∗n) = [ Ln(w ∗)− L(w∗) ] + [ L(w∗)− L(w∗n) ] + [ L(w∗n)− Ln(w∗n) ] . (30)\nNotice that the second bracket in (30) is non-positive since L(w∗) ≤ L(w∗n). Therefore, it is bounded by 0. According to (3), the first and third brackets in (30) are with high probability bounded above by Vn. Replacing these upper bounds by the brackets in (30) yields\nLn(w ∗)− Ln(w∗n) ≤ 2Vn. (31)\nSubstituting the upper bound in (31) into (29) follows the claim in (27)."
    }, {
      "heading" : "7.1 Proof of Proposition 3",
      "text" : "From the self-concordance analysis of Newton’s method we know that the variable wm is in the neighborhood that Newton’s method has a quadratic convergence rate if λn(wm) ≤ 1/4; see e.g., Chapter 9 of [5]. We proceed to come up with a condition for the quadratic convergence phase which guarantees that λn(wm) < 1/4 and wm is in the local neighborhood of the optimal argument of Rn. Recall that we have a wm which has sub-optimality Vm for Rm. We then proceed to enlarge the sample size to n and start from the observation that we can bound λn(wm) as\nλn(wm) = ‖∇Rn(wm)‖H−1n ≤ ‖∇Rm(wm)‖H−1n + ‖∇Rn(wm)−∇Rm(wm)‖H−1n , (32)\nwhere we have used the definition Hn = ∇2Rn(wm). Note that the weighted norm ‖a‖A for vector a and matrix A is equal to ‖a‖A = (aTAa)1/2. First, we bound the norm ‖∇Rn(wm)‖H−1n in (32). Notice that the Hessian ∇2Rn(wm) can be written as ∇2Ln(wm) + cVnI. Thus, the eigenvalues of the Hessian Hn = ∇2Rn(wm) are bounded below by cVn and consequently the eigenvalues of the Hessian inverse H−1n = ∇2Rn(wm)−1 are upper bounded by 1/(cVn). This bound implies that ‖H−1n ‖ ≤ 1/(cVn). Moreover, from Theorem 2.1.5 of [17], we know that the Lipschitz continuity of the gradients ∇Rm(w) with constant M + cVm implies that\n‖∇Rm(wm)‖2 ≤ 2(M + cVm)(Rm(wm)−Rm(w∗m)) ≤ 2(M + cVm)Vm, (33)\nwhere the last inequality holds comes from the condition that Rm(wm)−Rm(w∗m) ≤ Vm. Considering the upper bound for ‖∇Rm(wm)‖2 in (33) and the inequality ‖∇2Rn(wm)−1‖ ≤ 1/(cVn) we can write\n‖∇Rm(wm)‖H−1n = [ ∇Rm(wm)TH−1n ∇Rm(wm) ]1/2 ≤ ( 2(M + cVm)Vm\ncVn\n)1/2 . (34)\nNow we proceed to bound the second the term in (32). The definition of the risk function the gradient can be written as ∇Rn(w) = ∇Ln(w) + (cVn)w. Thus, we can derive an upper bound for the difference ‖∇Rn(wm)−∇Rm(wm)‖ as\n‖∇Rn(wm)−∇Rm(wm)‖ ≤ ‖∇Ln(wm)−∇Lm(wm)‖+ c(Vm − Vn)‖wm‖ ≤ ‖∇Ln(wm)−∇Lm(wm)‖+ c(Vm − Vn)‖wm −w∗m‖+ c(Vm − Vn)‖w∗m‖, (35)\nwhere in the second inequality we have used the triangle inequality and replaced ‖wm‖ by its upper bound ‖wm −w∗m‖ + ‖w∗m‖. By following the steps in (23)-(26) we can show that the difference ‖∇Ln(wm)−∇Lm(wm)‖ is bounded above by\n‖∇Ln(w)−∇Lm(w)‖ ≤ n−m n ‖∇Ln−m(w)−∇L(w)‖+ n−m n ‖∇Lm(w)−∇L(w)‖\n≤ 2(n−m) n V 1/2n , (36)\nwhere the second inequality uses the condition that ‖∇Lm(w)−∇L(w)‖ ≤ V 1/2m . Note that the strong convexity of the risk Rm with parameter cVm yields\n‖wm −w∗m‖2 ≤ 2\ncVm (Rm(wm)−Rm(w∗m)) ≤\n2 c . (37)\nThus, by considering the inequalities in (36) and (37) we can show that upper bound in (35) can be replaced by\n‖∇Rn(wm)−∇Rm(wm)‖ ≤ 2(n−m)\nn V 1/2n + (\n√ 2c+ c‖w∗m‖)(Vm − Vn). (38)\nSubstituting the upper bounds in (34) and (38) for the first and second summands in (32), respectively, follows the inequality\nλn(wm) ≤ ( 2(M + cVm)Vm\ncVn\n)1/2 + (2(n−m)/n)V 1/2n + ( √ 2c+ c‖w∗m‖)(Vm − Vn)\n(cVn)1/2 . (39)\nNote that the result in (27) shows that ‖w∗m‖2 ≤ (4/c) + ‖w∗‖2 with high probability. This observation follows that ‖w∗m‖ is bounded above by (2/ √ c) + ‖w∗‖. Replacing the norm ‖w∗m‖ in (39)\nby the upper bound (2/ √ c) + ‖w∗‖ follows\nλn(wm) ≤ ( 2(M + cVm)Vm\ncVn\n)1/2 + (2(n−m)/n)V 1/2n + ( √ 2c+ 2 √ c+ c‖w∗‖)(Vm − Vn)\n(cVn)1/2 .\n(40)\nAs we mentioned previously, the variable wm is in the neighborhood that Newton’s method has a quadratic convergence rate for the function Rn if the condition λn(wm) ≤ 1/4 holds. Hence, if the right hand side of (40) is bounded above by 1/4 we can conclude that wm is in the local neighborhood and the proof is complete."
    }, {
      "heading" : "7.2 Proof of Proposition 5",
      "text" : "Note that the difference Rn(wm)−Rn(w∗n) can be written as\nRn(wm)−Rn(w∗n) = Rn(wm)−Rm(wm) +Rm(wm)−Rm(w∗m) +Rm(w ∗ m)−Rm(w∗n) +Rm(w∗n)−Rn(w∗n). (41)\nWe proceed to bound the differences in (41). To do so, note that the differenceRn(wm)−Rm(wm) can be simplified as\nRn(wm)−Rm(wm) = Ln(wm)− Lm(wm) + c(Vn − Vm)\n2 ‖wm‖2\n≤ Ln(w)− Lm(w), (42)\nwhere the inequality follows from the fact that Vn < Vm and Vn − Vm is negative. It follows from the result in Lemma 6 that the right hand side of (42) is bounded by (n−m)/n (Vn−m + Vm). Therefore,\nRn(wm)−Rm(wm) ≤ n−m n (Vn−m + Vm) . (43)\nAccording to the fact that wm as an Vm optimal solution for the sub-optimalityRm(wm)−Rm(w∗m) we know that Rm(wm)−Rm(w∗m) ≤ Vm. (44) Based on the definition of w∗m which is the optimal solution of the risk Rm, the third difference in (41) which is Rm(w∗m)−Rm(w∗n) is always negative. I.e.,\nRm(w ∗ m)−Rm(w∗n) ≤ 0. (45)\nMoreover, we can use the triangle inequality to bound the difference Rm(w∗n)−Rn(w∗n) in (41) as\nRm(w ∗ n)−Rn(w∗n) = Lm(w∗n)− Ln(w∗n) + c(Vm − Vn) 2 ‖w∗n‖2\n≤ n−m n (Vn−m + Vm) + c(Vm − Vn) 2 ‖w∗n‖2. (46)\nReplacing the differences in (41) by the upper bounds in (43)-(46) follows\nRn(wm)−Rn(w∗n) ≤ Vm + 2(n−m)\nn (Vn−m + Vm) + c(Vm − Vn) 2 ‖w∗n‖2 w.h.p. (47)\nSubstitute ‖w∗n‖2 in (47) by the upper bound in (27) to obtain the result in (21)."
    } ],
    "references" : [ {
      "title" : "Convexity, classification, and risk bounds",
      "author" : [ "Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "Amir Beck", "Marc Teboulle" ],
      "venue" : "SIAM journal on imaging sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Sgd-qn: Careful quasi-newton stochastic gradient descent",
      "author" : [ "Antoine Bordes", "Léon Bottou", "Patrick Gallinari" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "Olivier Bousquet", "Léon Bottou" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Starting small–learning with adaptive sample sizes",
      "author" : [ "Hadi Daneshmand", "Aurelien Lucchi", "Thomas Hofmann" ],
      "venue" : "arXiv preprint arXiv:1603.02839,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Convergence rates of sub-sampled newton methods",
      "author" : [ "Murat A Erdogdu", "Andrea Montanari" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Competing with the empirical risk minimizer in a single pass",
      "author" : [ "Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford" ],
      "venue" : "arXiv preprint arXiv:1412.6606,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Stochastic block bfgs: Squeezing more curvature out of data",
      "author" : [ "Robert M Gower", "Donald Goldfarb", "Peter Richtárik" ],
      "venue" : "arXiv preprint arXiv:1603.09649,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "A globally convergent incremental newton method",
      "author" : [ "Mert Gürbüzbalaban", "Asuman Ozdaglar", "Pablo Parrilo" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Semi-stochastic gradient descent methods",
      "author" : [ "Jakub Konečnỳ", "Peter Richtárik" ],
      "venue" : "arXiv preprint arXiv:1312.1666,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Res: Regularized stochastic bfgs algorithm",
      "author" : [ "Aryan Mokhtari", "Alejandro Ribeiro" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Global convergence of online limited memory bfgs",
      "author" : [ "Aryan Mokhtari", "Alejandro Ribeiro" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "A linearly-convergent stochastic l-bfgs algorithm",
      "author" : [ "Philipp Moritz", "Robert Nishihara", "Michael I Jordan" ],
      "venue" : "arXiv preprint arXiv:1508.02087,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Introductory lectures on convex programming volume",
      "author" : [ "Yu Nesterov" ],
      "venue" : "i: Basic course",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1998
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Technical report,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "Boris T Polyak", "Anatoli B Juditsky" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1992
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Herbert Robbins", "Sutton Monro" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1951
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "Nicolas L Roux", "Mark Schmidt", "Francis R Bach" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "A stochastic quasi-newton method for online convex optimization",
      "author" : [ "Nicol N Schraudolph", "Jin Yu", "Simon Günter" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Learnability, stability and uniform convergence",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "The nature of statistical learning theory",
      "author" : [ "Vladimir Vapnik" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "Lin Xiao", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "Linear convergence with condition number independent access of full gradients",
      "author" : [ "Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster convergence.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster convergence.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 26,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 23,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 180,
      "endOffset" : 188
    }, {
      "referenceID" : 24,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 180,
      "endOffset" : 188
    }, {
      "referenceID" : 27,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 212,
      "endOffset" : 220
    }, {
      "referenceID" : 12,
      "context" : "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].",
      "startOffset" : 212,
      "endOffset" : 220
    }, {
      "referenceID" : 10,
      "context" : "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 13,
      "context" : "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 14,
      "context" : "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 5,
      "context" : "In this paper we attempt to overcome (i)-(iv) with the Ada Newton algorithm that combines the use of Newton iterations with adaptive sample sizes [6].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]",
      "startOffset" : 132,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]",
      "startOffset" : 132,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]",
      "startOffset" : 132,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "Since the regularization in (4) is of order Vn and (3) holds, the difference between Rn(w n) and L(w∗) is also of order Vn – this may be not as immediate as it seems; see [23].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : ", Chapter 9 of [5].",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 16,
      "context" : "11 of [17] which shows that",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : ", Chapter 9 of [5].",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "Thus, according to the quadratic convergence rate of Newton’s method for self-concordant functions [5], we know that the Newton decrement has a quadratic convergence and we can write",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "The second algorithm is Stochastic Gradient Descent (SGD) and the last one is the SAGA algorithm introduced in [7].",
      "startOffset" : 111,
      "endOffset" : 114
    } ],
    "year" : 2016,
    "abstractText" : "We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton’s method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton’s method and reach the statistical accuracy of each training set with only one iteration of Newton’s method. We show theoretically and empirically that Ada Newton can double the size of the training set in each iteration to achieve the statistical accuracy of the full training set with about two passes over the dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}