{
  "name" : "1402.0562.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Stochastic Optimization under Correlated Bandit Feedback",
    "authors" : [ "Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 2.\n05 62\nv3 [\nst at\n.M L\n] 1\n9 M"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of maximizing the sum of the rewards obtained by sequentially evaluating an unknown function, where the function itself may be stochastic. This is known as online stochastic optimization under bandit feedback or X -armed bandit, since each function evaluation can be viewed as pulling one of the arms in a general arm space X . Our objective is to minimize the cumulative regret relative to evaluating/executing at each time point the global maximum of the function. In particular, we focus on the case that the reward (function evaluation) of an arm may depend on prior history of evaluations and outcomes. This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007). X -armed bandit with correlated reward is relevant to many real world optimization applications, including internet auctions, adaptive routing, and online games. As one important example, we show that the problem of policy search in a Markov Decision Process (MDP), a popular approach to learning in unknown MDPs, can be framed as an instance of the setting we consider in this paper (Sect. 5). To the best of our knowledge, the algorithm introduced in this paper is the first to guarantee sub-linear regret in continuous state-action-policy space MDPs.\nOur approach builds on recent advances in X -armed bandits for iid settings (Bubeck et al., 2011a; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007). Under regularity assumptions on the mean-reward function (e.g. Lipschitzsmoothness), these methods provide formal guarantees in terms of bounds on the regret, which is proved to scale\n∗mohammad.azar@northwestern.edu †alessandro.lazaric@inria.fr ‡ebrun@cs.cmu.edu\nsub-linearly w.r.t. the number of steps n. To obtain this regret, these methods rely heavily on the iid assumption. To handle the correlated feedback, we introduce a new anytime X -armed bandit algorithm, called high confidence tree (HCT) (Sect. 3). Similar to the HOO algorithm of Bubeck et al. (2011a), HCT makes use of a covering binary tree for exploring the arm space. The tree is constructed incrementally in an optimistic fashion, exploring parts of the arm space guided by upper bounds on the potential best reward of the arms covered within a particular node.\nOur key insight is that to achieve good performance it is only necessary to expand the tree by refining an optimistic node when the estimate of the mean-reward of that node has become sufficiently accurate. This allows us to obtain an accurate estimate of the return of a particular arm even in the non-iid setting, under some mild ergodicity and mixing assumptions (Sect. 2). Despite handling a more general case of correlated feedback, our regret bounds matches (Sect. 4.1) that of HOO (Bubeck et al., 2011a) and zooming algorithm (Kleinberg et al., 2008), both of which only apply to iid setting, in terms of dependency on the number of steps n and the near-optimality dimension d (to be defined later). Furthermore, HCT also requires milder assumptions on the smoothness of the function, which is required to be Lipschitz only w.r.t. the maximum, whereas HOO assumes the mean-reward to be Lipschitz also between any pair of arms close to the maximum. An important part of our proof of this result (though we delay this and all proofs to the supplement, due to space considerations) is the development of concentration inequalities for non-iid episodic random variables. In addition to this main result, the structure of our HCT approach has a favorable sub-linear space complexity of O(nd/(d+2)(log n)2/(d+2)) and a linearithmic runtime complexity, making it suitable for scaling to big data scenarios. These results meet or improve the space and time complexity of prior work designed for iid data (Sect. 4.2), and we will demonstrate this benefit in simulations (Sect. 6). We also show how our approach can lead to finite-sample guarantees for policy search method, and provide preliminary simulation results which show the advantage of our method in the case of MDPs."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "The optimization problem. Let X be a measurable space of arms. We formalize the optimization problem as an interaction between the learner and the environment. At each time step t, the learner pulls an arm xt in X and the environment returns a reward rt ∈ [0, 1] and possibly a context yt ∈ Y , with Y a measurable space (e.g., the state space of a Markov decision process). Whenever needed, we relate rt to the arm pulled by using the notation rt(x). The context yt and the reward rt may depend on the history of all previous rewards, pulls, contexts and the current pull xt. For any time step t > 0, the space of histories Ht := ([0, 1] × X × Y)t is defined as the space of past rewards, arms,and observations (with H0 = ∅). An environment M corresponds to an infinite sequence of time-dependent probability measures M = (Q1, Q2, . . . ), such that each Qt : Ht−1 × X → M([0, 1] × Y) is a mapping from the history Ht−1 and the arm space X to the space of probability measures on rewards and contexts. Let Z = ([0, 1] × X × Y), at each step t we define the random variable zt = (rt, xt, yt) ∈ Z and we introduce the filtration Ft as a σ-algebra generated by (z1, z2, . . . , zt). At each step t, the arm xt is Ft−1-measurable since it is based on all the information available up to time t − 1. The pulling strategy of the learner can be expressed as an infinite sequence of measurable mappings (ψ1, ψ2, . . . ), where ψt : Ht−1 → M(X ) maps Ht−1 to the space of probability measures on arms. We refine this general setting with two assumptions on the reward-generating process.\nDefinition 1 (Time average reward). For any x ∈ X , S > 0 and 0 < s ≤ S, the time average reward is\nr̄s→S(x) := 1 S − s+ 1 S∑\ns′=s\nrs′(x). (1)\nWe now state our first assumption which guarantees that the mean of the process is well defined (ergodicity).\nAssumption 1 (Ergodicity). For any x ∈ X , any s > 0 and any sequence of prior pulls (x1, x2, . . . , xs−1), the process (zt)t>0 is such that the mean-reward function\nf(x) := lim S→∞E(r̄s→S(x)|Fs−1)\nexists.\nThis assumption implies that, regardless of the history of prior observations, if arm x is pulled infinitely many times from time s, then the time average reward converges in expectation to a fixed point which only depends on arm x and is independent from the past history. We also make the following mixing assumption (see e.g., Levin et al., 2006).\nAssumption 2 (Finite mixing time). There exists a constant Γ ≥ 0 (mixing time) such that for any x ∈ X , any S > 0, any 0 < s ≤ S and any sequence of prior pulls (x1, x2, . . . , xs−1), the process (zt)t>0 is such that we have that\n|E[ ∑\nS s′=s(rs′(x)− f(x)) ∣∣Fs−1]| ≤ Γ. (2)\nThis assumption implies that the stochastic reward process induced by pulling arm x can not substantially deviate from f(x) in expectation for more than Γ transient steps. Note that both assumptions trivially hold if each arm is an iid process: in this case f(x) is the mean of x and Γ = 0.\nGiven the mean-reward f , we assume that the maximizer x∗=argmaxx f(x) exists and we denote the corresponding maximum f(x∗) by f∗. We measure the performance of the learner over n steps by its regret Rn w.r.t. the f∗, defined as\nRn := nf ∗ −\nn∑\nt=1\nrt.\nThe goal of learner, at every 0 ≤ t ≤ n, is to choose a strategy ψt such that the regret Rn is as small as possible. Relationship to other models. Although the learner observes a context yt at each time t, this problem differs from the contextual bandit setting (see e.g., the extensions of the zooming algorithm to contextual bandits by Slivkins, 2009). In contextual bandits, the context y ∈ Y is provided before selecting an arm x, and the immediate reward rt is defined to be a function only of the selected arm and input context, rt(x, y). The contextual bandit objective is typically to minimize the regret against the optimal arm in the context provided at each step, yt, i.e. x∗t = argmax rt(x, yt). A key difference is that in our model the reward, and next context, may depend on the entire history of rewards, arms pulled, and contexts, instead of only the current context and arm, and we define f(x) only as the average reward obtained by pulling arm x. In this sense, our model is related to the reinforcement learning (RL) problem of trying to find a policy that maximizes the long run reward (see further discussion in Sect. 5). Among prior work in RL our setting is most similar to the general reinforcement learning model of Lattimore et al. (2013) which also considers an arbitrary temporal dependence between the rewards and observations. Our setting differs from that of Lattimore et al. (2013), since we consider the regret in undiscounted reward scenario, whereas Lattimore et al. (2013) focus on proving PAC-bounds in discounted reward case. Another difference is that in our model (unlike Lattimore et al., 2013) the space of observations and actions is not needed to be finite.\nThe cover tree. Similar to recent optimization methods (e.g., Bubeck et al., 2011a), our approach seeks to minimize regret by smartly building an estimate of f using an infinite binary covering tree T , in which each node covers a subset of X .1 We denote by (h, i) the node at depth h and index i among the nodes at the same depth (e.g., the root node which covers X is indexed by (0, 1)). By convention (h + 1, 2i − 1) and (h + 1, 2i) refer to the two children of the node (h, i). The area corresponding to each node (h, i) is denoted by Ph,i ⊂ X . These regions must be measurable and, at each depth, they partition X with no overlap:\n1The reader is referred to Bubeck et al. (2011a) for a more detailed description of the covering tree.\nP0,1 = X Ph,i = Ph+1,2i−1 ∪ Ph,2i ∀h ≥ 0 and 1 ≤ i ≤ 2h.\nFor each node (h, i), we define an arm xh,i ∈ Ph,i, which the algorithm pulls whenever the node (h, i) is selected. We now state a few additional geometrical assumptions.\nAssumption 3 (Dissimilarity). The space X is equipped with a dissimilarity function ℓ : X 2 → R such that ℓ(x, x′) ≥ 0 for all (x, x′) ∈ X 2 and ℓ(x, x) = 0.\nGiven a dissimilarity ℓ, the diameter of a subset A ⊆ X is defined as diam(A) := supx,y∈A ℓ(x, y), while an ℓ–open ball of radius ε > 0 and center x ∈ X is defined as B(x, ε) := {x′ ∈ X : ℓ(x, x′) ≤ ε}. Assumption 4 (local smoothness). We assume that there exist constants ν2, ν1 > 0 and 0 < ρ < 1 such that for all nodes (h, i):\n(a) diam(Ph,i) ≤ ν1ρh\n(b) ∃ xoh,i ∈ Ph,i s.t. Bh,i := B(xoh,i, ν2ρh) ⊂ Ph,i,\n(c) Bh,i ∩ Bh,j = ∅,\n(d) For all x ∈ X , f∗ − f(x) ≤ ℓ(x∗, x).\nLocal smoothness. These assumptions coincide with those in (Bubeck et al., 2011a), except for the local smoothness (Assumption 4.d), which is weaker than that of Bubeck et al. (2011a), where the function is assumed to be Lipschitz between any two arms x, x′ close to the maximum x∗ (i.e., |f(x) − f(x′)| ≤ ℓ(x, x′)), while here we only require the function to be Lipschitz w.r.t. the maximum.\nFinally, we characterize the complexity of the problem using the near-optimality dimension, which defines how large is the set of ǫ-optimal arms in X . For the sake of clarity, we consider a slightly simplified definition of near-optimality dimension w.r.t. Bubeck et al. (2011a).\nAssumption 5 (Near-optimality dimension). Let ǫ = 3ν1ρh and ǫ′ = ν2ρh < ǫ, for any subset of ǫ-optimal nodes Xǫ = {x ∈ X : f∗ − f(x) ≤ ǫ}, there exists a constant C such that N ( Xǫ, ℓ, ǫ′ ) ≤ C(ǫ′)−d, where d is the near-optimality dimension of function f and N (Xǫ, ℓ, ǫ′) is the ǫ′-cover number of the set Xǫ w.r.t. the dissimilarity measure ℓ.2"
    }, {
      "heading" : "3 The High Confidence Tree algorithm",
      "text" : "We now introduce the High Confidence Tree (HCT) algorithm for stochastic online optimization under bandit feedback. Throughout this discussion, a function evaluation is equivalent to the reward received from pulling an arm (since an arm corresponds to selecting an input to evaluate the function at). We first describe the general algorithm framework before discussing two particular variants: HCT-iid, designed for the case when rewards of a given arm are iid and HCT-Γ which handles the correlated feedback case, where the reward from pulling an arm may depend on all prior arms pulled and resulting outcomes. Alg. 1 shows the structure of the algorithm for HCT-iid and HCT-Γ, noting the minor modifications between the two.\n2Note that, in many cases, the near-optimality dimension d can be much smaller than D, the actual dimension of arm space X in the continuous case. In fact one can show that under some mild smoothness assumption the near optimality dimension of a function equals 0, regardless of the dimension of its input space X (see Munos, 2013; Valko et al., 2013, for a detailed discussion).\nAlgorithm 1 The HCT algorithm. Require: Parameters ν1 > 0, ρ ∈ (0, 1), c > 0, tree structure (Ph,i)h≥0,1≤i≤2i and confidence δ.\nInitialize t = 1, Tt = {(0, 1), (1, 1), (1, 2)}, H(t) = 1, U1,1(t) = U1,2(t) = +∞, loop\nif t = t+ then ⊲ Refresh phase for all (h, i) ∈ Tt do\nUh,i(t) ← µ̂h,i(t) + ν1ρh + √\nc2 log(1/δ̃(t+)) Th,i(t)\nend for; for all (h, i) ∈ Tt Backward from H(t) do\nif (h, i) ∈ leaf(Tt) then Bh,i(t) ← Uh,i(t) else Bh,i(t) ← min [ Uh,i(t), max\nj∈{2i−1,2i} Bh+1,j(t)\n]\nend if end for\nend if; {(ht, it), Pt} ← OptTraverse(Tt) if Algorithm HCT-iid then\nPull arm xh,i and observe rt t = t+ 1\nelse if Algorithm HCT-Γ then Tcur = Tht,it(t) while Tht,it(t) < 2Tcur AND t < t\n+ do Pull arm xh,i and observe rt (ht+1, it+1) = (ht, it) t = t+ 1\nend while end if Update counter Tht,it(t) and empirical average µ̂ht,it(t)\nUht,it(t) ← µ̂ht,it(t) + ν1ρh + √\nc2 log(1/δ̃(t+)) Tht,it (t)\nUpdateB(Tt, Pt, (ht, it)) τh(t) =\nc2 log(1/δ̃(t+)) ν21 ρ−2ht\nif Tht,it(t) ≥ τht(t) AND (ht, it) =leaf(T ) then It = {(ht + 1, 2it − 1), (ht + 1, 2it)} T ← T ∪ It Uht+1,2it−1(t) = Uht+1,2it(t) = +∞\nend if end loop\nThe general structure. The HCT algorithm relies on a binary covering tree T provided as input used to construct a hierarchical approximation of the mean-reward function f . At each node (h, i) of the tree, the algorithm keeps track of some statistics regarding the corresponding arm xh,i associated with the node (h, i). These include the empirical estimate µ̂h,i(t) of the mean-reward function corresponding for arm xh,i at time step t computed as\nµ̂h,i(t) := (1/Th,i(t)) ∑Th,i(t)\ns=1 rs(xh,i), (3)\nAlgorithm 2 The OptTraverse function. Require: Tree T\n(h, i) ← (0, 1), P ← (0, 1) T0,1 = τ0(t) = 1; while (h, i) /∈ Leaf(T ) AND Th,i(t) ≥ τh(t) do\nif Bh+1,2i−1 ≥ Bh+1,2i then (h, i) ← (h+ 1, 2i − 1) else (h, i) ← (h+ 1, 2i) end if P ← P ∪ {(h, i)}\nend while return (h, i) and P\nAlgorithm 3 The UpdateB function. Require: Tree T , the path Pt, selected node (ht, it)\nif (ht, it) ∈ Leaf(T ) then Bht,it(t) = Uht,it(t) else Bht,it(t) = min [ Uht,it(t), max\nj∈{2it−1,2it} Bht+1,j(t)\n]\nend if; for all (h, i) ∈ Pt − (ht, it) backward do\nBh,i(t) = min [ Uh,i(t), max\nj∈{2i−1,2i} Bh+1,j(t)\n]\nend for\nwhere Th,i(t) is the number of times node (h, i) has been selected in the past and rs(xh,i) denotes the s-th reward observed after pulling xh,i (while we previously used rt to denote the t-th sample of the overall process). As explained in Sect. 2, although a node is associated to a single arm xh,i, it also covers a full portion of the input space X , i.e., the subset Ph,i. Thus, similar to the HOO algorithm (Bubeck et al., 2011a), HCT also maintains two upper-bounds, Uh,i and Bh,i, which are meant to bound the mean-reward f(x) of all the arms x ∈ Ph,i. In particular, for any node (h, i), the upper-bound Uh,i is computed directly from the observed reward for pulling xh,i as\nUh,i(t) := µ̂h,i(t) + ν1ρ h + √ c2 log(1/δ̃(t+))/Th,i(t), (4)\nwhere t+ = 2⌊log(t)⌋+1 and δ̃(t) := min{c1δ/t, 1}. Intuitively speaking, the second term is related to the resolution of node (h, i) and the third term accounts for the uncertainty of µ̂h,i(t) in estimating the mean-reward f(xh,i). The B-values are designed to have a tighter upper bound on f(x) by taking the minimum between Uh,i for the current node, and the maximum upper bound of the node’s two child nodes, if present.3 More precisely,\nBh,i(t)=    Uh,i(t) (h, i)∈ leaf(Tt)\nmin[Uh,i(t), max j∈{2i−1,2i} Bh+1,j(t)] otherwise.\n(5)\nTo identify which arm to pull, the algorithm traverses the tree along a path Pt obtained by selecting nodes with maximum Bh,i until it reaches an optimistic node (ht, it), which is either a leaf or a node which is not pulled\n3Since the node’s children together contain the same input space as the node (i.e., Ph,i = Ph+1,2i−1 ∪ Ph,2i), the node’s maximum cannot be greater than the maximum of its children.\nenough w.r.t. to a given threshold τh(t), i.e., Th,i(t) ≤ τh(t) (see function OptTraverse in Alg. 2). Then the arm xht,it ∈ Pht,it corresponding to selected node (ht, it) is pulled. The key step of HCT is in deciding when to expand the tree. We expand a leaf node only if we have pulled its corresponding arm a sufficient number of times such that the uncertainty over the maximum value of the arms contained within that node is dominated by size of the subset of X it covers. Recall from Equation 4 that the upper bound Uh,i of a node (h, i) two additional terms added to the empirical average reward. The first ν1ρh is a constant that depends only on the node depth, and bounds the possible difference in the mean-reward function between the representative arm for this node and all other arms also contained in this node, i.e., the difference between f(xh,i) and f(x) for any other x ∈ Ph,i (as follows from Assumptions 3 and 4). The second term depends only on t and decreases with the number of pulls to this node. At some point, the second term will become smaller than the first term, meaning that the uncertainty over the possible rewards of nodes in Ph,i becomes dominated by the potential difference in rewards amongst arms that are contained within the same node. This means that the domain Ph,i is too large, and thus the resolution of the current approximation of f in that region needs to be increased. Therefore our approach chooses the point at which these two terms become of the same magnitude to expand a node, which occurs when the the number of pulls Tht,it(t) has exceeded a threshold\nτh(t) := c 2 log(1/δ̃(t+))ρ−2ht/ν21 . (6)\n(see Sect. A of the supplement for further discussion). It is at this point that expanding the node to two children can lead to a more accurate approximation of f(x), since ν1ρh+1 ≤ ν1ρh. Therefore if Tht,it(t) ≥ τh(t), the algorithm expands the leaf, creates both children leaves, and set their U -values to +∞. Furthermore, notice that this expansion only occurs for nodes which are likely to contain x∗. In fact, OptTraverse does select nodes with big B-value, which in turn receive more pulls and are thus expanded first. The selected arm xht,it is pulled either for a single time step (in HCT-iid) or for a full episode (in HCT-Γ), and then the statistics of all the nodes along the optimistic path Pt are updated backwards. The statistics of all the nodes outside the optimistic path remain unchanged.\nAs HCT is an anytime algorithm, we periodically need to recalculate the node upper bounds to guarantee their validity with enough probability (see supplementary material for a more precise discussion). To do so, at the beginning of each step t, the algorithm verifies whether the B and U values need to be refreshed or not. In fact, in the definition of U in Eq. 4, the uncertainty term depends on the confidence δ̃(t+), which changes at t = 1, 2, 4, 8, . . .. Refreshing the U and B values triggers a “resampling phase” of the internal nodes of the tree Tt along the optimistic path. In fact, the second condition in the OptTraverse function (Alg. 2) forces HCT to pull arms that belong to the current optimistic path Pt until the number of pulls Th,i(t) becomes greater than τh(t) again. Notice that the choice of the confidence term δ̃ is particularly critical. For instance, choosing a more natural δ̃(t) would tend to trigger the refresh (and the resampling) phase too often thus increasing the computational complexity of the algorithm and seriously affecting its theoretical properties in the correlated feedback scenario. 4 On the other hand, the choice of δ̃(t+) limits the need to refresh the U and B values to only O(log(n)) times over n rounds and guarantees that U and B are valid upper bounds with high probability.\nHCT-iid and HCT-Γ. The main difference between the two implementations of HCT is that, while HCT-iid pulls the selected arm for only one step before re-traversing the tree from the root to again find another optimistic node, HCT-Γ pulls the the representative arm of the optimistic node for an episode of Tcur steps, where Tcur is the number of pulls of arm xh,i at the beginning of episode. In other words, the algorithm doubles the number of pulls of each arm throughout the episode. Note that not all the episodes may actually finish after Tcur steps and double the number of pulls: The algorithm may interrupt the episode when the confidence bounds of B and U are not valid anymore (i.e., t ≥ t+) and perform a refresh phase. The reason for this change is that in order to accurately estimate the mean-reward given correlated bandit feedback,\n4If we refresh the upper-bound statistics at every time step the algorithm may select a different arm at every time step, whereas in correlated feedback scenario having a small number of switches is critical for the convergence of the algorithm.\nit is necessary to pull an arm for a series of pulls rather than a single pull. Due to our assumption on the mixing time (Assumption. 2), pulling an arm for a sufficiently long sequence will provide an accurate estimate of the potential mean reward even in the correlated setting, thus ensuring that the empirical average rewards µ̂h,i actually concentrates towards their mean value (see Lem. 7 in the supplementary material). It is this mechanism, coupled with only expanding the nodes after obtaining a good estimate of their mean reward, that allows us to handle correlated feedback setting. Although in this sense HCT-Γ is more general, we do however include the HCT-iid variant because whenever the rewards are iid it performs better than HCT-Γ. This is due to the fact that, unlike HCT-iid, HCT-Γ has to keep pulling an arm for a full episode even when there is evidence that another arm could be better. We also notice that there is a small difference in the constants c1 and c between HCT-iid and HCT-Γ: in the case of HCT-iid c1 := 8 √ ρ/(3ν1) and c := 2 √ 1/(1 − ρ), whereas HCT-Γ uses c1 := 9 √ ρ/(4ν1) and c := 3(3Γ + 1) √ 1/(1 − ρ)."
    }, {
      "heading" : "4 Theoretical Analysis",
      "text" : "In this section we analyze the regret and the complexity of HCT . All the proofs are reported in the supplement."
    }, {
      "heading" : "4.1 Regret Analysis",
      "text" : "We start by reporting a bound on the maximum depth of the trees generated by HCT .\nLemma 1. Given the threshold τh(t) in Eq. 6, the depth H(n) of the tree Tn is bounded as\nH(n) ≤ Hmax(n) = 1/(1 − ρ) log(nν21/(2(cρ)2)). (7)\nThis bound guarantees that HCT never expands trees beyond depth O(log n). This is ensured by the fact the HCT waits until the value of a node f(xh,i) is sufficiently well estimated before expanding it and this implies that the number of pulls exponentially grows with the depth of tree, thus preventing the depth to grow linearly as in HOO.\nWe report regret bounds in high probability, bounds in expectation can be obtained using standard techniques.\nTheorem 1 (Regret bound of HCT-iid). Pick a δ ∈ (0, 1). Assume that at each step t, the reward rt, conditioned on xt, is independent of all prior random events and the immediate mean reward f(x) = E(r|x) exists for every x ∈ X . Then under Assumptions 3–5 the regret of HCT-iid in n steps is, with probability (w.p.) 1− δ,5\nRn ≤ O (( log (n/δ) )1/(d+2) n(d+1)/(d+2) ) .\nRemark (the bound). We notice that the bound perfectly matches the bound for HOO up to constants (see Thm. 6 in (Bubeck et al., 2011a)). This represents a first sanity check w.r.t. the structure of HCT , since it shows that changing the structure of HOO and expanding nodes only when they are pulled enough, preserves the regret properties of the algorithm. Furthermore, this result holds under milder assumptions than HOO. In fact, Assumption 4-(d) only requires f to be Lipschitz w.r.t. to the maximum x∗. Other advantages of HCT-iid are discussed in the Sect. 4.2 and 6.\nAlthough the proof is mostly based on standard techniques and tools from bandit literature, HCT has a different structure from HOO (and similar algorithms) and moving from iid to correlated arms calls for the development of a significantly different proof technique. The main technical issue is to show that the empirical average µ̂h,i computed by averaging rewards obtained across different episodes actually converges to f(xh,i). In particular, we prove the following high-probability concentration inequality (see Lem. 7 in the supplement for further details).\n5Constants are provided in Sect. A of the supplement.\nLemma 2. Under Assumptions 1 and 2, for any fixed node (h, i) and step t, we have that, w.p. 1− δ,\n|µ̂h,i(t)− f(xh,i)| ≤ (3Γ + 1) √ 2 log(5/δ)\nTh,i(t) +\nΓ log(t)\nTh,i(t) .\nFurthermore Kh,i(t), the number of episodes in which (h, i) is selected, is bounded by log2(4Th,i(t)) + log2(t).\nThis technical lemma is at the basis of the derivation of the following regret bound for HCT-Γ.\nTheorem 2 (Regret bound of HCT-Γ). We assume that Assumptions 1–5 hold and that rewards are generated according to the general model defined in Section 2. Then the regret of HCT-iid after n steps is, w.p. 1− δ,\nRn ≤ O (( log (n/δ) )1/(d+2) n(d+1)/(d+2) ) .\nRemark (the bound). The most interesting aspect of this bound is that HCT-Γ achieves the same regret as HCT-iid when samples are non-iid. This represents a major step forward w.r.t. HOO since it shows that the very general case of correlated arms can be managed as well as the much simpler iid case. In the next section we also discuss how this result can be used in policy search for MDPs."
    }, {
      "heading" : "4.2 Complexity",
      "text" : "Time complexity. The run time complexity of both versions of HCT is O(n log(n)). This is due to the boundedness of the depth H(n) and by the structure of the refresh phase. By Lem. 1, we have that the maximum depth is O(log(n)). As a result, at each step t, the cost of traversing the tree to select a node is at most O(log n), which also coincides with the cost of updating the B and U values of the nodes in the optimistic path Pt. Thus, the total cost of selecting, pulling, and updating nodes is no larger than O(n log n). Notice that in case of HCT-Γ, once a node is selected is pulled for an entire episode, which further reduces the total selection cost. Another computational cost is represented by the refresh phase where all the nodes in the tree are actually updated. Since the refresh is performed only when t = t+, then the number of times all the nodes are refreshed is of order of O(log n) and the boundedness of the depth guarantees that the number of nodes to update cannot be larger than O(2log n), which still corresponds to a total cost of O(n log n). This implies that HCT achieves the same run time as T-HOO (Bubeck et al., 2011a). Though unlike T-HOO, our algorithm is fully anytime and it does not suffer from the extra regret incurred due to the truncation and the doubling trick.\nSpace complexity. The following theorem provides bound on space complexity of the HCT algorithm.\nTheorem 3. Under the same conditions of Thm. 2, let Nn denote the space complexity of HCT-Γ, then we have that\nE(Nn) = O(log(n)2/(d+2)nd/(d+2)).\nThe previous theorem guarantees that the space complexity of HCT scales sub-linearly w.r.t. n. An important observation is that the space complexity of HCT increases slower, by a factor of Õ(n1/(d+2)), than its regret. This implies that, for small values of d, HCT does not require to use a large memory space to achieve a good performance. An interesting special case is the class of problem with near-optimality dimension d = 0. For this class of problems the bound translates to a space complexity of O(log(n)), whereas the space complexity of alternative algorithms may be as large as n (see e.g., HOO). As it has been shown in (Valko et al., 2013) the case of d = 0 covers a rather large class of functions, since every function which satisfies some mild local smoothness assumption, around its\nglobal optima, has a near-optimality dimension equal to 0 (see Valko et al., 2013, for further discussions). The fact that HCT can achieve a near-optimal performance, using only a relatively small memory space, which makes it a suitable choice for big-data applications, where the algorithms with linear space complexity can not be used due to very large size of the dataset.\nSwitching frequency. Finally, we also remark another interesting feature of HCT-Γ. Since an arm is pulled for an entire episode before another arm could be selected, this drastically reduces the number of switches between arms. In many applications, notably in reinforcement learning (see next section), this can be a significant advantage since pulling an arm may correspond to the actual implementation of a complex solution (e.g., a position in a portfolio management problem) and continuously switch between different arms might not be feasible. More formally, since each node has a number of episodes bounded by O(log n) (Lem. 2), then the number of switches can be derived be the number of nodes in Thm. 3 multiplied by O(log n), which leads to O(log(n)(d+4)/(d+2)nd/(d+2))."
    }, {
      "heading" : "5 Application to Policy Search in MDPs",
      "text" : "As we discussed in Sect. 2, HCT is designed to handle the very general case of optimization in problems where there exists a strong correlation among the rewards, arm pulls, and contexts, at different time steps. An important subset of this general class is represented by the problem of policy search in infinite-horizon Markov decision processes. Notice that the extension to the case of partially observable MDPs is straightforward as long as the POMDP satisfies some ergodicity assumptions.\nA MDP M is defined as a tuple 〈S,A, P 〉 where S is the set of states, A is the set of actions, P : S × A → M(S × [0, 1]) is the transition kernel mapping each state-action pair to a distribution over states and rewards. A (stochastic) policy π : S → M(A) is a mapping from states to distribution over actions. Policy search algorithms (Scherrer & Geist, 2013; Azar et al., 2013; Kober & Peters, 2011) aim at finding the policy in a given policy set which maximizes the long-term performance. Formally, a policy search algorithm receives as input a set of policies G = {πθ; θ ∈ Θ}, each of them parameterized by a parameter vector θ in a given set Θ ⊂ ℜd. Any policy πθ ∈ G induces a state-reward transition kernel T : S ×Θ → M(S × [0, 1]). T relates to the state-reward-action transition kernel P and the policy kernel πθ as follows\nT (ds′, dr|s, θ) := ∫\nu∈A P (ds′, dr|s, u)πθ(du|s).\nFor any πθ ∈ G and initial state s0 ∈ S, the time-average reward over n steps is\nµπθ(s0, n) := 1\nn E [∑n t=1 rt ] ,\nwhere r1, r2, . . . , rn is the sequence of rewards observed by running πθ for n steps staring at s0. If the Markov reward process induced by πθ is ergodic, µπθ(s0, n) converges to a fixed point independent of the initial state s0. The average reward of πθ is thus defined as\nµ(θ) := lim n→∞\nµπθ(s0, n).\nThe goal of policy search is to find the best θ∗ = argmaxθ∈Θ µ(θ). Note that πθ∗ is optimal in the policy class G and it may not coincide with the optimal policy π∗ of the MDP, when π∗ is not covered by G. It is straightforward now to match the MDP scenario to the general setting in Sect. 2, notably mapping Θ to X and µ(θ) to f(x). More precisely the parameter space θ ∈ Θ corresponds to the space of arms X , since in the policy search we want to explore the parameter space Θ to learn the best parameter θ∗. Also the state space S in MDP\nsetting is the special from of context space of Sect. 2 where here the contexts evolve according to some controlled Markov process. Further the transition kernel T , which at each time step t determines the distribution on the current state and reward given the last state and θ is again a special case of of the more general (Qt)t which may depend on the entire history of prior observations. Likewise µ(θ), µ∗Θ and θ\n∗ translate into f(θ), f∗ and x∗, respectively, using the notation of Sect. 2. The Asm. 1 and 2 in Sect. 2 are also the general version of the standard ergodicity and mixing assumption in MDPs, in which the notion of filtration in assumptions of Sect. 2 is simply replaced by the the initial state s0 ∈ S . This allows us to directly apply HCT-Γ to the problem of policy search. The advantage of HCT-Γ algorithm w.r.t. prior work is that, to the best of our knowledge, it is the first policy search algorithm which provides finite sample guarantees in the form of regret bounds on the performance loss of policy search in MDPs (see Thm. 2), which guarantee that HCT-Γ suffers from a small sub-linear regret w.r.t. πθ∗ . Also it is not difficult to prove that the policy induced by HCT-Γ has a small simple regret, that is, the average reward of the policy chosen by HCT-Γ converges to µ(θ∗) with a polynomial rate.6 Another interesting feature of HCT-Γ is that can be readily used in large (continuous) state-action problems since it does not make any restrictive assumption on the size of state-action space.\nPrior regret bounds for continuous MDPs. A related work to HCT-Γ is the UCCRL algorithm by Ortner & Ryabko (2012), which extends the original UCRL algorithm (Jaksch et al., 2010) to continuous state spaces. Although a direct comparison between the two methods is not possible, it is interesting to notice that the assumptions used in UCCRL are stronger than for HCT-Γ, since they require both the dynamics and the reward function to be globally Lipschitz. Furthermore, UCCRL requires the action space to be finite, while HCT-Γ can deal with any continuous policy space. Finally, while HCT-Γ is guaranteed to minimize the regret against the best policy in the policy class G, UCCRL targets the performance of the actual optimal policy of the MDP at hand. Another relevant work is the OMDP algorithm of Abbasi et al. (2013) which deals with the problem of RL in continuous state-action MDPs with adversarial rewards. OMDP achieves a sub-linear regret under the assumption that the space of policies is finite, whereas in HCT the space of policy can be continuous."
    }, {
      "heading" : "6 Numerical Results",
      "text" : "While our primary contribution is the definition of HCT and its technical analysis, we also give some preliminary simulation results to demonstrate some of its properties.\nSetup. We focus on minimizing the regret across repeated noisy evaluations of the garland function f(x) = x(1 − x)(4 − √ | sin(60x)|) relative to repeatedly selecting its global optima.We select this function due to its several interesting properties: (1) it contains many local optima, (2) it is locally smooth around its global optima x∗ (it behaves as f∗ − c|x − x∗|α, for c = 2 and α = 1/2), (3) it is also possible to show that the near-optimality dimension d of f equals 0.\nWe evaluate the performances of each algorithm in terms of the per-step regret, R̃n = Rn/n. Each run is n = 105 steps and we average the performance on 10 runs. For all the algorithms compared in the following, parameters7 are optimized to maximize their performance.\nI.i.d. setting. For our first experiment\nwe compare HCT-iid to the truncated hierarchical optimistic optimization (T-HOO) algorithm Bubeck et al. (2011a). T-HOO is a state-of-the-art X -armed bandit algorithm, developed as a computationally-efficient alternative of HOO. In Fig. 1(a) we show the per-step regret, the runtime, and the space requirements of each approach. As predicted by the theoretical bounds, the per-step regret R̃n of both HCT-iid and truncated HOO decrease rapidly with number of steps. Though the big O theoretical bounds are identical for both approaches, empirically we observe in this\n6Refer to Bubeck et al. (2011a); Munos (2013) for how to transform bounds on accumulated regret to simple regret bounds. 7For both HCT and T-HOO we introduce a tuning parameter used to multiply the upper bounds, while for PoWER we optimize the\nwindow for computing the weighted average.\nexample that HCT-iid outperforms T-HOO by a large margin. Similarly, though the computational complexity of both approaches matches in the dependence on the number of time steps, empirically we observe that our approach outperforms T-HOO (Fig. 1(b)). Perhaps the most significant expected advantage of HCT-iid over T-HOO for iid settings is in the space requirements. HCT-iid has a space requirement for this domain that scales logarithmically with the time step n, as predicted by Thm. 3. since the near-optimality dimension d = 0). In contrast, a brief analysis of T-HOO suggests that its space requirements can grow polynomially, and indeed in this domain we observe a polynomial growth of memory usage for T-HOO. These patterns mean that HCT-iid can achieve a very small regret using a sparse decision tree with only few hundred nodes, whereas truncated HOO requires orders of magnitude more nodes than HCT-iid.\nCorrelated setting. We create a continuous-state-action MDP out of the previously described Garland function by introducing the state of the environment s. Upon taking continuous-valued action x, the state of the environment changes deterministically to st+1 = (1 − β)st + βx, where we set β = 0.2. The agent receives a stochastic reward for being in state s, which is (the Garland function) f(s) + ε, where as before ε is drawn randomly from [0, 1]. The initial state s0 is also drawn randomly from [0, 1]. A priori, the agent does not know the transition or reward function, making this a reinforcement learning problem. Though not a standard benchmark RL instance, this problem has multiple local optima and therefore is a interesting case for policy search, where Θ = X is the policy set (which coincides with the action set in this case).\nIn this setting, we compare HCT-Γ to a PoWER, a standard RL policy search algorithm Kober & Peters (2011) on the above MDP problem MDP constructed out of garland function.PoWER uses an Expectation Maximization approach to optimize the policy parameters and is therefore not guaranteed to find the global optima. We also compare our algorithm with T-HOO, though this algorithm is specifically designed for iid setting and one may expect that it may fail to converge to global optima under correlated bandit feedback. Fig. 2(a) shows per-step regret of the 3 approaches in the MDP. Only HCT-Γ succeeds in finding the globally optimal policy, as is evident because only in the case of HCT-Γ does the average regret tends to converge to zero (which is as predicted from Thm. 2). The PoWER method finds worse solutions than both stochastic optimization approaches for the same amount of computational time, likely due to using EM which is known to be susceptible to local optima. On the other hand, its primary advantage is that it has a very small memory requirement. Overall this suggests the benefit of our proposed approach to be used for online MDP policy search, since it quickly (as a function of samples and runtime) can find a global optima, and is, to our knowledge, one of the only policy search methods guaranteed to do so."
    }, {
      "heading" : "7 Discussion and Future Work",
      "text" : "In the current version of HCT we assume that the learner has access to the information regarding the smoothness of function f(x) and the mixing time Γ. In many problems those information are not available to the learner. In the future it would be interesting to build on prior work that handles unknown smoothness in iid settings and extend it to correlated feedback. For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm. 4, and do not require knowledge of the dissimilarity measure ℓ. On the other hand, Slivkins (2011) and Bull (2013) study the cumulative regret but consider a different definition of smoothness related to the zooming concept introduced by Kleinberg et al. (2008). Finally, we notice that to deal with unknown mixing time, one may rely on data-dependent tail’s inequalities, such as empirical Bernstein inequality (Tolstikhin & Seldin, 2013; Maurer & Pontil, 2009), replacing the mixing time with the empirical variance of the rewards.\nIn the future we also wish to explore using HCT to optimize other problems that can be modeled using correlated bandit feedback. For example, HCT may be used for policy search in partially observable MDPs (Vlassis & Toussaint, 2009; Baxter & Bartlett, 2000), as long as the POMDP is ergodic.\nTo conclude, in this paper we introduce a new X -armed bandit algorithm, called HCT , for optimization under bandit feedback and prove regret bounds and simulation results for it. Our approach improves on existing results to handle the important case of correlated bandit feedback. This allows HCT to be applied to a broader range of problems than prior X -armed bandit algorithms, such as we demonstrate by using it to perform policy search for continuous MDPs.\nAppendices"
    }, {
      "heading" : "A Proof of Thm. 1",
      "text" : "In this section we report the full proof of the regret bound of HCT-iid.\nWe begin by introducing some additional notation, required for the analysis of both algorithms. We denote the indicator function of an event E by IE . For all 1 ≤ h ≤ H(t) and t > 0, we denote by Ih(t) the set of all nodes created by the algorithm at depth h up to time t and by I+h (t) the subset of Ih(t) including only the internal nodes (i.e., nodes that are not leaves), which corresponds to nodes at depth h which have been expanded before time t. At each time step t, we denote by (ht, it) the node selected by the algorithm. For every (h, i) ∈ T , we define the set of time steps when (h, i) has been selected as Ch,i := {t = 1, . . . , n : (ht, it) = (h, i)}. We also define the set of times that a child of (h, i) has been selected as Cch,i := Ch+1,2i−1\n⋃Ch+1,2i. We need to introduce three important steps related to node (h, i):\n• t̄h,i := maxt∈Ch,i t is the last time (h, i) has been selected,\n• t̃h,i := maxt∈Cch,i t is the last time when any of the two children of (h, i) has been selected,\n• th,i := min{t : Th,i(t) > τh(t)} is the step when (h, i) is expanded.\nThe choice of τh. The threshold on the the number of pulls needed before expanding a node at depth h is determined\nso that, at each time t, the two confidence terms in the definition of U (Eq. 4) are roughly equivalent, that is\nν1ρ h = c\n√ log(1/δ̃(t+))\nτh(t) =⇒ τh(t) =\nc2 log(1/δ̃(t+))\nν21 ρ−2h.\nFurthermore, since t ≤ t+ ≤ 2t then\nc2\nν21 ρ−2h ≤ c\n2 log(1/δ̃(t))\nν21 ρ−2h ≤ τh(t) ≤\nc2 log(2/δ̃(t))\nν21 ρ−2h, (8)\nwhere we used the fact that 0 < δ̃(t) ≤ 1 for all t > 0. As described in Section 3, the idea is that the expansion of a node, which corresponds to an increase in the resolution of the approximation of f , should not be performed until the empirical estimate µ̂h,i of f(xh,i) is accurate enough. Notice that the number of pulls Th,i(t) for an expanded node (h, i) does not necessarily coincide with τh(t), since t might correspond to a time step when some leaves have not been pulled until τh(t) and other nodes have not been fully resampled after a refresh phase.\nWe begin our analysis by bounding the maximum depth of the trees constructed by HCT-iid.\nLemma 1 Given the number of samples τh(t) required for the expansion of nodes at depth h in Eq. 6, the depth H(n) of the tree Tn is bounded as\nH(n) ≤ Hmax(n) = 1 1− ρ log ( nν21 2(cρ)2 ) .\nProof. The deepest tree that can be developed by HCT-iid is a linear tree, where at each depth h only one node is expanded, that is , |I+h (n)| = 1 and |Ih(n)| = 2 for all h < H(n). Thus we have\nn =\nH(n)∑\nh=0\n∑\ni∈Ih(n) Th,i(n) ≥\nH(n)−1∑\nh=0\n∑\ni∈Ih(n) Th,i(n) ≥\nH(n)−1∑\nh=0\n∑\ni∈I+ h (n)\nTh,i(n) ≥ H(n)−1∑\nh=0\n∑\ni∈I+ h (n)\nTh,i(th,i)\n(1) ≥ H(n)−1∑\nh=0\n∑\ni∈I+ h (n)\nτh,i(th,i) ≥ H(n)−1∑\nh=1\nc2\nν21 ρ−2h ≥ (cρ)\n2\nν21 ρ−2H(n)\nH(n)−1∑\nh=1\nρ−2(h−H(n)+1),\nwhere inequality (1) follows from the fact that a node (h, i) is expanded at time th,i only when it is pulled enough, i.e., Th,i(th,i) ≥ τh(th,i). Since all the elements in the summation over h are positive, then we can lower-bound the sum by its last element (h = H(n)), which is 1, and obtain\nn ≥ 2(cρ) 2 ν21 H(n)ρ−2H(n) ≥ 2(cρ) 2 ν21 ρ−2H(n),\nwhere we used the fact that H(n) ≥ 1. By solving the previous expression we obtain\nρ−2H(n) ≤ n ν 2 1 2(cρ)2 =⇒ H(n) ≤ 1 2 log ( nν21 2(cρ)2 ) / log(1/ρ).\nFinally, the statement follows using log(1/ρ) ≥ 1− ρ.\nWe now introduce a high probability event under which the mean reward for all the expanded nodes is within a confidence interval of the empirical estimates at a fixed time t.\nLemma 3 (High-probability event). We define the set of all the possible nodes in trees of maximum depth Hmax(t) as\nLt = ⋃\nT :Depth(T )≤Hmax(t) Nodes(T ).\nWe introduce the event\nEt = { ∀(h, i) ∈ Lt,∀Th,i(t) = 1..t : ∣∣∣µ̂h,i(t)− f(xh,i) ∣∣∣ ≤ c\n√ log(1/δ̃(t))\nTh,i(t)\n} ,\nwhere xh,i ∈ Ph,i is the arm corresponding to node (h, i). If\nc = 2\n√ 1\n1− ρ and δ̃(t) = δ t 8\n√ ρ\n3ν1 ,\nthen for any fixed t, the event Et holds with probability at least 1− δ/t6.\nProof. We upper bound the probability of the complementary event as\nP[Ect ] ≤ ∑\n(h,i)∈Lt\nt∑\nTh,i(t)=1\nP [∣∣µ̂h,i(t)− µh,i ∣∣ ≥ c\n√ log(1/δ̃(t))\nTh,i(t)\n]\n≤ ∑\n(h,i)∈Lt\nt∑\nTh,i(t)=1\n2 exp ( − 2Th,i(t)c2 log(1/δ̃(t))\nTh.i(t)\n)\n= 2exp ( − 2c2 log(1/δ̃(t)) ) t|Lt|,\nwhere the first inequality is an application of a union bound and the second inequality follows from the ChernoffHoeffding inequality. We upper bound the number of nodes in Lt by the largest binary tree with a maximum depth Hmax(t), i.e., |Lt| ≤ 2Hmax(t)+1. Thus\nP[Ect ] ≤ 2(δ̃(t))2c 2 t2Hmax(t)+1.\nWe first derive a bound on the the term 2Hmax(t) as\n2Hmax(t) ≤ pow ( 2, log2 ( tν21\n2(cρ)2\n) 1 2 log2(e)(1−ρ) ) ≤ ( tν21\n2(cρ)2\n) 1 2(1−ρ)\n,\nwhere we used the upper bound Hmax(t) from Lemma 1 and log2(e) > 1. This leads to\nP[Ect ] ≤ 4t ( δ̃(t)\n)2c2 (\ntν21 2(cρ)2\n) 1 2(1−ρ)\n.\nThe choice of c and δ̃(t) as in the statement leads to\nP[Ect ] ≤ 4t ( 8 √ ρ/(3ν1)δ/t ) 8 1−ρ ( tν21(1− ρ)\n8ρ2\n) 1 2(1−ρ)\n= 4t ( δ/t ) 8 1−ρ ( ρ/(3ν1) ) 1 1−ρ t 1 2(1−ρ) ( ν1 √ 1− ρ√ 8ρ ) 1 1−ρ\n≤ 4δt1− 8 1−ρ + 1 2(1−ρ) (√ 1− ρ 3 √ 8 ) 1 1−ρ ≤ 4 3 √ 8 δt −2ρ−13 2(1−ρ) ≤ δt−13/2 ≤ δ/t6,\nwhich completes the proof.\nRecalling the definition the regret from Sect. s:preliminaries, we decompose the regret of HCT-iid in two terms depending on whether event Et holds or not (i.e., failing confidence intervals). Let the instantaneous regret be ∆t = f ∗ − rt, then we rewrite the regret as\nRn =\nn∑\nt=1\n∆t =\nn∑\nt=1\n∆tIEt + n∑\nt=1\n∆tIEct = R E n +R Ec n . (9)\nWe first study the regret in the case of failing confidence intervals.\nLemma 4 (Failing confidence intervals). Given the parameters c and δ̃(t) as in Lemma 3, the regret of HCT-iid when confidence intervals fail to hold is bounded as\nRE c n ≤ √ n,\nwith probability 1− δ 5n2 .\nProof. We first split the time horizon n in two phases: the first phase until √ n and the rest. Thus the regret becomes\nRE c\nn =\nn∑\nt=1\n∆tIEct =\n√ n∑\nt=1\n∆tIEct + n∑\nt= √ n+1\n∆tIEct .\nWe trivially bound the regret of first term by √ n. So in order to prove the result it suffices to show that event Ect\nnever happens after √ n, which implies that the remaining term is zero with high probability. By summing up the probabilities P[Ect ] from √ n+ 1 to n and applying union bound we deduce\nP\n[ n⋃\nt= √ n+1\nEct ] ≤ n∑\nt= √ n+1\nP[Ect ] ≤ n∑\n√ n+1\nδ t6 ≤ ∫ +∞ √ n δ t6 dt ≤ δ 5n5/2 ≤ δ 5n2 .\nIn words this result implies that w.p. ≥ 1 − δ/(5n2) we can not have a failing confidence interval after time √n. This combined with the trivial bound of √ n for the first √ n steps completes the proof.\nWe are now ready to prove the main theorem, which only requires to study the regret term under events {Et}. Theorem 1 (Regret bound of HCT-iid). Let δ ∈ (0, 1), δ̃(t) = 8 √ ρ/(3ν1)δ/t, and c = 2 √ 1/(1 − ρ). We assume that Assumptions 3–5 hold and that at each step t, the reward rt is independent of all prior random events and E(rt|xt) = f(xt). Then the regret of HCT-iid after n steps is\nRn ≤ 3 ( 22d+7ν 2(d+1) 1 Cν −d 2 ρ d (1− ρ)d+7 ) 1 d+2 ( log (2n δ 8 √ 3ν1 ρ )) 1d+2 n d+1 d+2 + 2 √ n log(4n/δ),\nwith probability 1− δ.\nProof. Step 1: Decomposition of the regret. We start by further decomposing the regret in two terms. We rewrite the instantaneous regret ∆t as\n∆t = f ∗ − rt = f∗ − f(xht,it) + f(xht,it)− rt = ∆ht,it + ∆̂t,\nwhich leads to a regret (see Eq. 9)\nREn = n∑\nt=1\n∆ht,itIEt + n∑\nt=1\n∆̂tIEt ≤ n∑\nt=1\n∆ht,itIEt + n∑\nt=1\n∆̂t = R̃ E n + R̂ E n. (10)\nWe start bounding the second term. We notice that the sequence {∆̂t}nt=1 is a bounded martingale difference sequence since E(∆̂t|Ft−1) = 0 and |∆̂t| ≤ 1. Therefore, an immediate application of the Azuma’s inequality leads to\nR̂En = n∑\nt=1\n∆̂t ≤ 2 √ n log(4n/δ), (11)\nwith probability 1− δ/(4n2). Step 2: Preliminary bound on the regret of selected nodes and their parents. We now proceed with the study of the first term R̃En, which refers to the regret of the selected nodes as measured by its mean-reward. We start by characterizing which nodes are actually selected by the algorithm under event Et. Let (ht, it) be the node chosen at time t and Pt be the path from the root to the selected node. Let (h′, i′) ∈ Pt and (h′′, i′′) be the node which immediately follows (h′, i′) in Pt (i.e., h′′ = h′ + 1). By definition of B and U values, we have that\nBh′,i′(t)=min [ Uh′,i′(t);max ( Bh′+1,2i′−1(t);Bh′+1,2i′(t) )] ≤max ( Bh′+1,2i′−1(t);Bh′+1,2i′(t) ) =Bh′′,i′′(t),\n(12)\nwhere the last equality follows from the fact that the OptTraverse function selects the node with the largest B value. By iterating the previous inequality for all the nodes in Pt until the selected node (ht, it) and its parent (h p t , i p t ), we obtain that\nBh′,i′(t) ≤ Bht,it(t) ≤ Uht,it(t), ∀(h′, i′) ∈ Pt Bh′,i′(t) ≤ Bhpt ,ipt (t) ≤ Uhpt ,ipt (t), ∀(h ′, i′) ∈ Pt − (ht, it)\nby definition of B-values. Thus for any node (h, i) ∈ Pt}, we have that Uht,it(t) ≥ Bh,i(t). Furthermore, since the root node (0, 1) which covers the whole arm space X is in Pt, thus there exists at least one node (h∗, i∗) in the set Pt which includes the maximizer x∗ (i.e., x∗ ∈ Ph∗,i∗) and has the the depth h∗ ≤ hpt < ht.8 Thus\nUht,it(t) ≥ Bh∗,i∗(t). Uhpt ,i p t (t) ≥ Bh∗,i∗(t)\n(13)\nNotice that in the set Pt we may have multiple nodes (h∗, i∗) which contain x∗ and that for all of them we have the following sequence of inequalities holds\nf∗ − f(xh∗,i∗) ≤ ℓ(x∗, xh∗,i∗) ≤ diam(Ph∗,i∗) ≤ ν1ρh ∗ , (14)\nwhere the second inequality holds since x∗ ∈ Ph∗,i∗ . Now we expand the inequality in Eq. 13 on both sides using the high-probability event Et. First we have\nUht,it(t) = µ̂ht,it(t) + ν1ρ ht + c\n√ log(1/δ̃(t+))\nTht,it(t) ≤ f(xht,it) + c\n√ log(1/δ̃(t))\nTht,it(t) + ν1ρ\nht + c\n√ log(1/δ̃(t+))\nTht,it(t)\n≤ f(xht,it) + ν1ρht + 2c √ log(1/δ̃(t+))\nTht,it(t) , (15)\nwhere the first inequality holds on E by definition of U and the second by the fact that t+ ≥ t (and log(1/δ̃(t)) ≤ log(1/δ̃(t+))). The same result also holds for (hpt , i p t ) at time t:\nUhpt ,i p t (t) ≤ f(xhpt ,ipt ) + ν1ρ\nhpt + 2c\n√ log(1/δ̃(t+))\nThpt ,i p t (t)\n. (16)\n8Note that we never pull the root node (0, 1), therefore ht > 0.\nWe now show that for any node (h∗, i∗) such that x∗ ∈ Ph∗,i∗ , then Uh∗,i∗(t) is a valid upper bound on f∗:\nUh∗,i∗(t) = µ̂h∗,i∗(t) + ν1ρ h + c\n√ log(1/δ̃(t+))\nTh∗,i∗(t)\n(1) ≥ µ̂h∗,i∗(t) + ν1ρh ∗ + c\n√ log(1/δ̃(t))\nTh∗,i∗(t)\n(2) ≥ f(xh∗,i∗) + ν1ρh ∗ (3) ≥ f∗,\nwhere (1) follows from the fact that t+ ≥ t, on (2) we rely on the fact that the event Et holds at time t and on (3) we use the regularity of the function w.r.t. the maximum f∗ from Eq. 14. If an optimal node (h∗, i∗) is a leaf, then Bh∗,i∗(t) = Uh∗,i∗(t) ≥ f∗. In the case that (h∗, i∗) is not a leaf, there always exists a leaf (h+, i+) such that x∗ ∈ Ph+,i+ for which (h∗, i∗) is its ancestor, since all the optimal nodes with h > h∗ are descendants of (h∗, i∗). Now by propagating the bound backward from (h+, i+) to (h∗, i∗) through Eq. 5 (see Eq. 12) we can show that Bh∗,i∗(t) is still a valid upper bound of the optimal value f∗. Thus for any optimal node (h∗, i∗) at time t under the event Et we have\nBh∗,i∗(t) ≥ f∗.\nCombining this with Eq. 15, Eq. 16 and Eq. 13 , we obtain that on event Et the selected node (ht, it) and its parent (hpt , i p t ) at any time t is such that\n∆ht,it = f ∗ − f(xht,it) ≤ ν1ρht + 2c\n√ log(1/δ̃(t+))\nTht,it(t) .\n∆hpt ,i p t = f∗ − f(xhpt ,ipt ) ≤ ν1ρ hpt + 2c\n√ log(1/δ̃(t+))\nThpt ,i p t (t)\n.\n(17)\nFurthermore, since HCT-iid only selects nodes with Th,i(t) < τh(t) the previous expression can be further simplified as\n∆ht,it ≤ 3c √ log(2/δ̃(t))\nTht,it(t) , (18)\nwhere we also used that t+ ≤ 2t for any t. Although this provides a preliminary bound on the instantaneous regret of the selected nodes, we need to further refine this bound.\nIn the case of parent (hpt , i p t ), since Thpt ,i p t (t) ≥ τhpt (t), we deduce\n∆hpt ,i p t ≤ ν1ρh\np t + 2c\n√ log(1/δ̃(t+))\nτhpt (t) = 3ν1ρ\nhpt , (19)\nThis implies that every selected node (ht, it) has a 3ν1ρht−1-optimal parent under the event Et. Step 3: Bound on the cumulative regret. We first decompose R̃En over different depths. Let 1 ≤ H ≤ H(n) a\nconstant to be chosen later, then we have\nR̃En = n∑\nt=1\n∆ht,itIEt ≤ H(n)∑\nh=0\n∑\ni∈Ih(n)\nn∑\nt=1\n∆h,iI(ht,it)=(h,i)IEt\n(1) ≤ H(n)∑\nh=0\n∑\ni∈Ih(n)\nn∑\nt=1\n3c\n√ log(2/δ̃(t))\nTh,i(t) I(ht,it)=(h,i)\n(2) ≤ H(n)∑\nh=0\n∑\ni∈Ih(n)\nTh,i(n)∑\ns=1\n3c\n√ log(2/δ̃(t̄h,i))\ns\n≤ H(n)∑\nh=0\n∑\ni∈Ih(n)\n∫ Th,i(n)\n1 3c\n√ log(2/δ̃(t̄h,i))\ns ds ≤\nH(n)∑\nh=0\n∑\ni∈Ih(n) 6c\n√ Th,i(n) log(2/δ̃(t̄h,i))\n= 6c\nH∑\nh=0\n∑\ni∈Ih(n)\n√ Th,i(n) log(2/δ̃(t̄h,i))\n︸ ︷︷ ︸ (a)\n+6c\nH(n)∑\nh=H+1\n∑\ni∈Ih(n)\n√ Th,i(n) log(2/δ̃(t̄h,i))\n︸ ︷︷ ︸ (b)\n(20)\nwhere in (1) we rely on the definition of event Et and Eq. 18 and in (2) we rely on the fact that at any time step t when the algorithm pulls the arm (h, i), Th,i is incremented by 1 and that by definition of t̄h,i we have that t ≤ t̄h,i . We now bound the two terms in the RHS of Eq. 20. We first simplify the first term as\n(a) =\nH(n)∑\nh=0\n∑\ni∈Ih(n)\n√ Th,i(n) log(2/δ̃(t̄h,i)) ≤ H∑\nh=0\n∑\ni∈Ih(n)\n√ τh(n) log(2/δ̃(n))\n= H∑\nh=0\n|Ih(n)| √ τh(n) log(2/δ̃(n)), (21)\nwhere the inequality follows from Th,i(n) ≤ τh(n) and t̄h,i ≤ n. We now need to provide a bound on the number of nodes at each depth h. We first notice that since T is a binary tree, the number of nodes at depth h is at most twice the number of nodes at depth h− 1 that have been expanded (i.e., the parent nodes), i.e., |Ih(n)| ≤ 2|I+h−1(n)|. We also recall the result of Eq. 19 which guarantees that (hpt , i p t ), the parent of the selected node (ht, it), is 3ν1ρ ht−1 optimal, that is, HCT never selects a node (ht, it) unless its parent is 3ν1ρht−1 optimal. From Asm. 5 we have that the number of 3ν1ρh-optimal nodes is bounded by the covering number N (3ν1/ν2ε, l, ε) with ε = ν1ρh. Thus we obtain the bound\n|Ih(n)| ≤ 2|I+h−1(n)| ≤ 2C(ν2ρ(h−1))−d, (22)\nwhere d is the near-optimality dimension of f around x∗. This bound combined with Eq. 21 implies that\n(a) ≤ H∑\nh=0\n2Cν−d2 ρ −(h−1)d √ τh(n) log(2/δ̃(n)) ≤ H∑\nh=0\n2Cν−d2 ρ −(h−1)d\n√ c2 log(1/δ̃(n+))\nν21 ρ−2h log(2/δ̃(n))\n≤ 2Cν−d2 c log(2/δ̃(n+))\nν1 ρd\nH∑\nh=0\nρ−h(d+1) ≤ 2Cν−d2 c log(2/δ̃(n+))\nν1 ρd\nρ−H(d+1)\n1− ρ . (23)\nWe now bound the second term of Eq. 20 as\n(b) (1) ≤ √√√√√ H(n)∑\nh=H+1\n∑\ni∈Ih(n) log(2/δ(t̄h,i))\n√√√√√ H(n)∑\nh=H+1\n∑\ni∈Ih(n) Th,i(n)\n(2) ≤ √√√√√ H(n)∑\nh=H+1\n∑\ni∈Ih(n) log(2/δ̃(t̄h,i))\n√ n (24)\nwhere in (1) we make use of Cauchy-Schwarz inequality and in (2) we simply bound the total number of samples by n. We now focus on the summation in the first square root. We recall that we denote by t̃h,i the last time when any of the two children of node (h, i) has been pulled. Then we have the following sequence of inequalities.\nn =\nH(n)∑\nh=0\n∑\ni∈Ih(n) Th,i(n) ≥\nH(n)−1∑\nh=0\n∑\ni∈I+ h (n)\nTh,i(n) ≥ H(n)−1∑\nh=0\n∑\ni∈I+ h (n)\nTh,i(t̃h,i) (1) ≥ H(n)−1∑\nh=0\n∑\ni∈I+ h (n)\nτh(t̃h,i)\n≥ H(n)−1∑\nh=H\n∑\ni∈I+ h (n)\nτh(t̃h,i) ≥ H(n)−1∑\nh=H\n∑\ni∈I+ h (n)\nρ−2hc2 log(1/δ̃(t̃+h,i))\nν21\n≥ c 2ρ−2H\nν21\nH(n)−1∑\nh=H\nρ2(H−h) ∑\ni∈I+ h (n)\nlog(1/δ̃(t̃+h,i)) (2) ≥ c 2ρ−2H\nν21\nH(n)−1∑\nh=H\n∑\ni∈I+ h (n)\nlog(1/δ̃(t̃+h,i))),\n(25)\nwhere in (1) we rely on the fact that, at each time step t, HCT-iid only selects a node when Th,i(t) ≥ τh,i(t) for its parent and in (2) we used that ρ2(H−h) ≥ 1 for all h ≥ H . We notice that, by definition of t̃h,i, for any internal node (h, i) t̃h,i = max(t̄h+1,2i−1, t̄h+1,2i). We also notice that for any t1, t2 > 0 we have that [max(t1, t2)]+ = max(t+1 , t + 2 ). This implies that\nn ≥ c 2ρ−2H\nν21\nH(n)−1∑\nh=H\n∑\ni∈I+ h (n)\nlog(1/δ̃([max(t̄h+1,2i−1, t̄h+1,2i)] +))\n(1) =\nc2ρ−2H\nν21\nH(n)−1∑\nh=H\n∑\ni∈I+ h (n)\nmax(log(1/δ̃(t̄+h+1,2i−1)), log(1/δ̃(t̄ + h+1,2i−1)))\n(2) ≥ c 2ρ−2H\nν21\nH(n)−1∑\nh=H\n∑\ni∈I+ h (n)\nlog(1/δ̃(t̄+h+1,2i−1)) + log(1/δ̃(t̄ + h+1,2i))\n2\n(3) =\nc2ρ−2H\n2ν21\nH(n)∑\nh′=H+1\n∑\ni∈I+ h′−1 (n)\nlog(1/δ̃(t̄+h′,2i−1)) + log(1/δ̃(t̄ + h′,2i))\n(4) =\nc2ρ−2H\n2ν21\nH(n)∑\nh′=H+1\n∑\ni′∈Ih′ (n) log(1/δ̃(t̄+h′,i′)),\n(26)\nwhere in (1) we rely on the fact that, for any t > 0, log(1/δ̃(t)) is an increasing function of t. Therefore we have that log(1/δ̃(max(t1, t2))) = max(log(1/δ̃(t1)), log(1/δ̃(t2))) for any t1, t2 > 0 . In (2) we rely on the fact that the maximum of some random variables is always larger than their average. We introduce a new variable h′ = h+1 to derive (3). For proving (4) we rely on the argument that, for any h > 0, I+h (n) covers all the internal nodes at layer h. This implies that the set of the children of I+h (n) covers Ih+1(n). This combined with fact that the inner sum in (3) is essentially taken on the set of the children of I+h′−1(n) proves (4). Inverting Eq. 26 we have\nH(n)∑\nh=H+1\n∑\ni∈Ih(n) log(1/δ̃(t̄+h,i)) ≤\n2ν21ρ 2Hn\nc2 . (27)\nBy plugging Eq. 27 into Eq. 24 we deduce\n(b) ≤ √√√√√ H(n)∑\nh=H+1\n∑ i∈Ih log(2/δ̃(t̄+h,i)) √ n ≤\n√√√√√ H(n)∑\nh=H+1\n∑ i∈Ih 2 log(1/δ̃(t̄+h,i)) √ n\n≤\n√ 4ν21ρ 2Hn\nc2 √ n = 2 c ν1ρ Hn.\nThis combined with Eq. 23 provides the following bound on R̃n:\nR̃En ≤ 12ν1 [ Cc2ν−d2 ρ d log(2/δ̃(n))\nν21(1− ρ) ρ−H(d+1) + ρHn\n] .\nWe then choose H to minimize the previous bound. Notably we equalize the two terms in the bound by choosing\nρH =\n( c2Cν−d2 ρ d\n(1− ρ)ν21 log(2/δ̃(n)) n\n) 1 d+2\n,\nwhich, once plugged into the previous regret bound, leads to\nR̃En ≤ 24ν1 c\n( c2Cν−d2 ρ d\n(1− ρ)ν21\n) 1 d+2 (\nlog(2/δ̃(n)) ) 1 d+2n d+1 d+2 .\nUsing the values of δ̃(t) and c defined in Lemma 3, the previous expression becomes\nR̃En ≤ 3 ( 22(d+3)ν 2(d+1) 1 Cν −d 2 ρ d (1− ρ)d/2+3 ) 1 d+2 ( log (2n δ 8 √ 3ν1 ρ )) 1d+2 n d+1 d+2 .\nThis combined with the regret bound of Eq. 11 and the result of Lem. 4 and a union bound on all n ∈ {1, 2, 3, . . . } proves the final result with a probability at least 1− δ."
    }, {
      "heading" : "B Correlated Bandit feedback",
      "text" : "We begin the analysis of HCT-Γ by proving some useful concentration inequalities for non-iid random variables under the mixing assumptions of Sect. 2.\nB.1 Concentration Inequality for non-iid Episodic Random Variables\nIn this section we extend the result in (Azar et al., 2013) and we derive a concentration inequality for averages of non-iid random variables grouped in episodes. In fact, given the structure of the HCT-Γ algorithm, the rewards observed from an arm x are not necessarily consecutive but they are obtained over multiple episodes. This result is of independent interest, thus we first report it in its general form and we later apply it to HCT-Γ.\nIn HCT-Γ, once an arm is selected, it is pulled for a number of consecutive steps and many steps may pass before it is selected again. As a result, the rewards observed from one arm are obtained through a series of episodes. Given a fixed horizon n, let Kn(x) be the total number of episodes when arm x has been selected, we denote by tk(x), with k = 1, . . . ,Kn(x), the step when k-th episode of arm x has started and by vk(x) the length of episode k. Finally,\nTn(x) = ∑Kn(x)\nk vk(x) is the total number of samples from arm x. The objective is to study the concentration of the empirical mean built using all the samples\nµ̂n(x) = 1\nTn(x)\nKn(x)∑\nk=1\ntk(x)+vk(x)∑\nt=tk(x)\nrt(x),\ntowards the mean-reward f(x) of the arm. In order to simplify the notation, in the following we drop the dependency from n and x and we use K , tk, and vk. We first introduce two quantities. For any t = 1, . . . , n and for any k = 1, . . . ,K , we define\nMkt (x) = E [ tk+vk∑\nt′=tk\nrt′ ∣∣Ft ] ,\nas the expectation of the sum of rewards within episode k, conditioned on the filtration Ft up to time t (see definition in Section 2),9 and the residual\nεkt (x) = M k t (x)−Mkt−1(x).\nWe prove the following.\nLemma 5. For any x ∈ X , k = 1, . . . ,K , and t = 1, . . . , n, εkt (x) is a bounded martingale sequence difference, i.e., εkt (x) ≤ 2Γ + 1 and E[εkt (x)|Ft−1] = 0.\nProof. Given the definition of Mkt (x) we have that\nεkt (x) = M k t (x)−Mkt−1(x) = E\n[ tk+vk∑\nt′=tk\nrt′ ∣∣Ft ] − E [ tk+vk∑\nt′=tk\nrt′ ∣∣Ft−1\n]\n=\nt∑\nt′=tk\nrt′ + E [ tk+vk∑\nt′=t+1\nrt′ ∣∣Ft ] − t−1∑\nt′=tk\nrt′ − E [ tk+vk∑\nt′=t\nrt′ ∣∣Ft−1\n]\n= rt + E [ tk+vk∑\nt′=t+1\nrt′ ∣∣Ft ] − E [ tk+vk∑\nt′=t\nrt′ ∣∣Ft−1\n]\n= rt − f(x) + E [ tk+vk∑\nt′=t+1\nrt′ ∣∣Ft ] − (tk + vk − t)f(x) + (tk + vk − t+ 1)f(x)− E [ tk+vk∑\nt′=t\nrt′ ∣∣Ft−1\n]\n≤ 1 + Γ + Γ.\nSince the previous inequality holds both ways, we obtain that |εkt (x)| ≤ 2Γ + 1. Furthermore, we have that\nE [ εkt (x)|Ft−1] = E [ Mkt (x)−Mkt−1(x)|Ft−1 ]\n= E [ rt + E [ tk+vk∑\nt′=t+1\nrt′ ∣∣Ft ]∣∣∣∣Ft−1 ] − E [ tk+vk∑\nt′=t\nrt′ ∣∣Ft−1 ] = 0.\nWe can now proceed to derive a high-probability concentration inequality for the average reward of each arm x.\n9Notice that the index t of the filtration can be before, within, or after the k-th episode.\nLemma 6. For any x ∈ X pulled K(x) episodes, each of length vk(x), for a total number of T (x) samples, we have that\n∣∣∣∣ 1\nT (x)\nK(x)∑\nk=1\ntk+vk∑\nt=tk\nrt − f(x) ∣∣∣∣ ≤ (2Γ + 1) √ 2 log(2/δ)\nT (x) +\nK(x)Γ\nT (x) , (28)\nwith probability 1− δ.\nProof. We first notice that for any episode k10\ntk+vk∑\nt=tk\nrt = M k tk+vk ,\nsince Mktk+vk = E [∑tk+vk t′=tk rt′ ∣∣Ftk+vk ] and the filtration completely determines all the rewards. We can further develop the previous expression using a telescopic expansion which allows us to rewrite the sum of the rewards as a sum of residuals εkt as\ntk+vk∑\nt=tk\nrt = M k tk+vk = Mktk+vk −M k tk+vk−1 +M k tk+vk−1 −M k tk+vk−2 +M k tk+vk−2 + · · · −M k tk +Mktk\n= εktk+vk + ε k tk+vk−1 + · · ·+ ε k tk+1 +Mktk =\ntk+vk∑\nt=tk+1\nεkt +M k tk .\nThus we can proceed by bounding\n∣∣∣∣ K(x)∑\nk=1\n( tk+vk∑\nt=tk\nrt − vkf(x) )∣∣∣∣ ≤ ∣∣∣∣ K(x)∑\nk=1\ntk+vk∑\nt=tk+1\nεkt ∣∣∣∣+ ∣∣∣∣ K(x)∑\nk=1\n( Mktk − vkf(x) )∣∣∣∣\n≤ ∣∣∣∣ K(x)∑\nk=1\ntk+vk∑\nt=tk+1\nεkt ∣∣∣∣+K(x)Γ.\nBy Lem. 5 εkt is a bounded martingale sequence difference, thus we can directly apply the Azuma’s inequality and obtain that\n∣∣∣∣ K(x)∑\nk=1\ntk+vk∑\nt=tk+1\nεkt ∣∣∣∣ ≤ (2Γ + 1) √ 2T (x) log(2/δ).\nGrouping all the terms together and dividing by T (x) leads to the statement.\nB.2 Proof of Thm. 2\nThe notation needed in this section is the same as in Section A. We only need to restate the notation about the episodes from previous section to HCT-Γ. We denote by Kh,i(n) the number of episodes for node (h, i) up to time n, by th,i(k) the step when episode k is started, and by vh,i(k) the number of steps of episode k.\nWe first notice that Lemma 1 holds unchanged also for HCT-Γ, thus bounding the maximum depth of an HCT tree\nto H(n) ≤ Hmax(n) = 11−ρ log ( nν21 2(cρ)2 ) . We begin the main analysis by applying the result of Lem. 6 to bound the estimation error of µ̂h,i(t) at each time step t.\n10We drop the dependency of M on x.\nLemma 7. Under Assumptions 1 and 2, for any fixed node (h, i) and step t, we have that\n|µ̂h,i(t)− f(xh,i)| ≤ (3Γ + 1) √ 2 log(5/δ)\nTh,i(t) +\nΓ log(t)\nTh,i(t) .\nwith probability 1− δ. Furthermore, the previous expression can be conveniently restated for any 0 < ε ≤ 1 as\nP(|µ̂h,i(t)− f(xh,i)| > ǫ) ≤ 5t1/3 exp ( − Th,i(t)ε 2\n2(3Γ + 1)2\n)\nProof. As a direct consequence of Lem. 6 we have w.p. 1− δ,\n|µ̂h,i(t)− f(xh,i)| ≤ (2Γ + 1) √ 2 log(2/δ)\nTh,i(t) +\nKh,i(t)Γ\nTh,i(t) ,\nwhere Kh,i(t) is the number of episodes in which we pull arm xh,i. At each episode in which xh,i is selected, its number of pulls Th,i is doubled w.r.t. the previous episode, except for those episodes where the current time s becomes larger than s+, which triggers the termination of the episode. However since s+ doubles whenever s becomes larger than s+, the total number of times when episodes are interrupted because of s ≥ s+ can be at maximum log2(t) withing a time horizon of t. This means that the total number of times an episode finishes without doubling Th,i(t) is bounded by log2(t). Thus we have\nTh,i(t) ≥ Kh,i(t)−log2(t)−1∑\nk=1\n2k−1 ≥ 2Kh,i(t)−log2(t)−2,\nwhere in the second inequality we simply keep the last term of the summation. Inverting the previous inequality we obtain that Kh,i(t) ≤ log2(4Th,i(t)) + log2(t), which bounds the number of episodes w.r.t. the number of pulls and the time horizon t. Combining this result with the high probability bound of Lem. 6, we obtain\n|µ̂h,i(t)− f(xh,i)| ≤ (2Γ + 1) √ 2 log(2/δ)\nTh,i(t) + Γ\nlog2(4Th,i(t))\nTh,i(t) + Γ\nlog(t) Th,i(t) ,\nwith probability 1 − δ. The statement of the Lemma is obtained by further simplifying the second term in the right hand side with the objective of achieving a more homogeneous expression. In particular, we have that\nlog2(4Th,i(t)) = 2 log2(2 √ Th,i(t)) = 2(log2( √ Th,i(t)) + 1) ≤ 2 √ Th,i(t),\nand\n|µ̂h,i(t)− f(xh,i)| ≤ (2Γ + 1) √ 2 log(2/δ)\nTh,i(t) +\n2Γ √\nTh,i(t)\nTh,i(t) +\nΓ log(t)\nTh,i(t)\n≤ (3Γ + 1) √ 2 log(5/δ)\nTh,i(t) +\nΓ log(t)\nTh,i(t) .\nTo prove the second statement we choose ε := (3Γ + 1) √\n2 log(5/δ) Th,i(t) + Γ log(t)Th,i(t) and we solve the previous expression\nw.r.t. δ:\nδ = 5exp [ −Th,i(t)(ε − Γ log(t)/Th,i(t)) 2\n2(3Γ + 1)2\n] .\nThe following sequence of inequalities then follows\nP(|µ̂h,i(t)− f(xh,i)| > ε) ≤ δ = 5exp [ −Th,i(t)(ε− Γ log(t)/Th,i(t)) 2\n2(3Γ + 1)2\n] ≤ 5 exp [ −Th,i(t)(ε\n2 − 2εΓ log(t)/Th,i(t)) 2(3Γ + 1)2\n]\n≤ 5 exp [ −Th,i(t)(ε\n2 − 2Γ log(t)/Th,i(t)) 2(3Γ + 1)2\n] = 5exp [ − Th,i(t)ε 2\n(3Γ + 1)2 +\n2Γ log(t)\n2(3Γ + 1)2\n]\n≤ 5 exp [ − Th,i(t)ε 2\n(3Γ + 1)2 +\n2Γ log(t)\n12Γ\n] = 5exp [ − Th,i(t)ε 2\n2(3Γ + 1)2 + log(t1/6)\n] ,\nwhich concludes the proof.\nThe result of Lem. 7 facilitates the adaption of the previous results of iid case to the case of correlated rewards, since this bound is similar to those of standard tail’s inequality such as Hoeffding and Azuma’s inequality. Based on this result we can extend the results of previous section to the case of dependent arms.\nWe now introduce the high probability event Et,n under which the mean reward for all the selected nodes in the interval [t, n] is within a confidence interval of the empirical estimates at every time step in the interval. The event Et,n is needed to concentrate the sum of obtained rewards around the sum of their corresponding arm means. Note that unlike the previous theorem where we could make use of a simple martingale argument to concentrate the rewards around their means, here the rewards are not unbiased samples of the arm means. Therefore, we need a more advanced technique than the Azuma’s inequality for concentration of measure.\nLemma 8 (High-probability event). We define the set of all the possible nodes in trees of maximum depth Hmax(t) as\nLt = ⋃\nT :Depth(T )≤Hmax(t) Nodes(T ).\nWe introduce the event\nΩt = { ∀(h, i) ∈ Lt,∀Th,i(t) = 1, . . . , t : ∣∣µ̂h,i(t)− f(xh,i) ∣∣ ≤ c\n√ log(1/δ̃(t))\nTh,i(t)\n} ,\nwhere xh,i ∈ Ph,i is the arm corresponding to node (h, i), and the event Et,n = ⋂n s=tΩs. If\nc = 6(3Γ + 1)\n√ 1\n1− ρ and δ̃(t) = δ t 9\n√ ρ\n4ν1 ,\nthen for any fixed t, the event Ωt holds with probability 1 − δ/t7 and the joint event Et,n holds with probability at least 1− δ/(6t6).\nProof. We upper bound the probability of complementary event of Ωt after t steps\nP[Ωct ] = ∑\n(h,i)∈Lt\nt∑\nTh,i(t)=1\nP [∣∣µ̂h,i(t)− f(xh,i) ∣∣ ≥ c\n√ log(1/δ̃(t))\nTh,i(t)\n]\n≤ ∑\n(h,i)∈Lt\nt∑\nTh,i(t)=1\n5t1/3 exp ( − Th,i(t)c2 log(1/δ̃(t))\n(3Γ + 1)2Th,i(t)\n)\n≤ 5 exp(−c2/(3Γ + 1)2 log(1/δ̃(t)))t4/3|Lt|,\nSimilar to the proof of Lem. 4, we have that |Lt| ≤ 2Hmax(t)+1. Thus\nP[Ωct ] ≤ 5(δ̃(t))(c/(3Γ+1)) 2 t4/32Hmax(t)+1.\nWe first derive a bound on the the term 2Hmax(t) as\n2Hmax(t) ≤ pow ( 2, log2 ( tν21\n2(cρ)2\n) 1 2 log2(e)(1−ρ) ) ≤ ( tν21\n2(cρ)2\n) 1 2(1−ρ)\n,\nwhere we used the definition of the upper bound Hmax(t). which leads to\nP[Ωct ] ≤ 10t4/3 ( δ̃(t)\n)(c/(3Γ+1))2 (\ntν21 2(cρ)2\n) 1 2(1−ρ)\n.\nThe choice of c and δ̃(t) as in the statement leads to P[Ωct ] ≤ δt7 (steps are similar to Lemma 3) . The bound on the joint event Et,n follows from a union bound as\nP [ Ect,n ] = P\n[ n⋃\ns=t\nΩcs\n] ≤ n∑\ns=t\nP(Ωcs) ≤ ∫ ∞\nt\nδ\ns7 ds =\nδ\n6t6 .\nRecalling the definition of regret from Sect. 2, we decompose the regret of HCT-iid in two terms depending on whether event Et holds or not (i.e., failing confidence intervals). Let the instantaneous regret be ∆t = f∗ − rt, then we rewrite the regret as\nRn =\nn∑\nt=1\n∆t =\nn∑\nt=1\n∆tIEt + n∑\nt=1\n∆tIEct = R E n +R Ec n . (29)\nWe first study the regret in the case of failing confidence intervals.\nLemma 9 (Failing confidence intervals). Given the parameters c and δ̃(t) as in Lemma 8, the regret of HCT-iid when confidence intervals fail to hold is bounded as\nRE c n ≤ √ n,\nwith probability 1− δ 30n2 .\nProof. The proof is the same as in Lemma 4 expect for the union bound which is applied to Et,n for t = √ n, . . . , n.\nWe are now ready to prove the main theorem, which only requires to study the regret term under events {Et,n}. Theorem 2 (Regret bound of HCT-Γ). Let δ ∈ (0, 1) and c := 6(3Γ + 1) √ 1/(1 − ρ). We assume that Assumptions 1–5 hold and that rewards are generated according to the general model defined in Section 2. Then the regret of HCT-Γ after n steps is\nRn ≤ 2(3 √ 2 + 4)\n( c2Cν−21 ν −d 2 ρ d\n1− ρ\n) 1 d+2 ( log (2n δ 9 √ 3ν1 ρ )) 1d+2 n d+1 d+2 + √ n,\nwith probability 1− δ.\nProof. The structure of the proof is exactly the same as in Thm. 1. Thus, here we report only the main differences in each step.\nStep 1: Decomposition of the regret. We first decompose the regret in two terms. We rewrite the instantaneous regret ∆t as\n∆t = f ∗ − rt = f∗ − f(xht,it) + f(xht,it)− rt = ∆ht,it + ∆̂t,\nwhich leads to a regret\nREn = n∑\nt=1\n∆ht,itIEt,n + n∑\nt=1\n∆̂tIEt,n = R̃ E n + R̂ E n. (30)\nUnlike in Thm. 1, the definition of R̂En still requires the event IEt,n and the sequence {∆̂t}nt=1 is no longer a bounded martingale difference sequence. In fact, E(∆̂t|Ft−1) 6= 0 since the expected value of rt does not coincide with the mean-reward value of the corresponding node f(xht,it). This prevents from directly using the Azuma inequality and extra care is needed to derive a bound. We have that\nR̂En = n∑\nt=1\n∆̂tIEt,n ≤ H(n)∑\nh=0\n∑\ni∈Ih(n)\nn∑\nt=1\n∆̂tIEt,nI(ht,it)=(h,i)\n=\nH(n)∑\nh=0\n∑\ni∈Ih(n)\nn∑\nt=1\n(f(xh,i)− rt)IEt,nI(ht,it)=(h,i) (1) ≤ H(n)∑\nh=0\n∑\ni∈Ih(n)\nn∑\nt=1\n(f(xh,i)− rt)IΩth,i,nI(ht,it)=(h,i)\n(2) =\nH(n)∑\nh=0\n∑\ni∈Ih(n) Th,i(t̄h,i)(f(xh,i)− µ̂h,i(t̄h,i))IΩt̄h,i\n(3) ≤ H(n)∑\nh=0\n∑\ni∈Ih(n) cTh,i(t̄h,i)\n√ log(2/δ̃(t̄h,i))\nTh,i(t̄h,i) ≤\nH(n)∑\nh=0\n∑\ni∈Ih(n) c\n√ Th,i(t̄h,i) log(2/δ̃(t̄h,i))\n≤ c H∑\nh=0\n∑\ni∈Ih(n)\n√ Th,i(n) log(2/δ̃(t̄h,i))\n︸ ︷︷ ︸ (a)\n+c\nH(n)∑\nh=H+1\n∑\ni∈Ih(n)\n√ Th,i(n) log(2/δ̃(t̄h,i))\n︸ ︷︷ ︸ (b)\n,\n(31)\nwhere (1) follows from the definition of Et,n = ⋂n\ns=tΩs, thus if Et,n holds at time t then Ωs also holds at s = t̄h,i ≥ t. Step (2) follows from the definition of µ̂h,i: First we notice that for the node (hn, in) we have that Thn,in(n)µ̂hn,in(n) = ∑n t=1 rtI(ht,it)=(hn,in) since we update the statistics at the end. for every other node we have that the last selection time t̄h,i and the end of last episode coincides together . Now since we update the statistics of the selected node at the end of every episode, thus, we have that Th,i(t̄h,i)µ̂h,i(t̄h,i) = ∑n t=1 rtI(ht,it)=(h,i) also for (h, i) 6= (hn, in). Step (3) follows from the definition of Ωs. The resulting bound matches the one in Eq. 20 up to constants and it can be bound similarly.\nR̂En ≤ 2ν1 [ Cc2ν−d2 ρ d log(2/δ̃(n))\nν21(1− ρ) ρ−H(d+1) + ρHn\n] .\nStep 2: Preliminary bound on the regret of selected nodes. The second step follows exactly the same steps as in the proof of Thm. 1 with the only difference that here we use the high-probability event Et,n. As a result the following inequalities hold for the node (ht, it) selected at time t and its parent (h p t , i p t )\n∆ht,it ≤ 3c √ log(2/δ̃(t))\nTht,it(t) .\n∆hpt ,i p t ≤ 3ν1ρht−1.\n(32)\nStep 3: Bound on the cumulative regret. Unlike in the proof of Thm. 1, the total regret R̃En should be analyzed with extra care since here we do not update the selected arm as well as the statistics Th,i(t) and µ̂h,i(t) for the the entire length of episode, whereas in Thm. 1 we update at every step. Thus the development of R̃En slightly differs from Eq. 20. Let 1 ≤ H ≤ H(n) a constant to be chosen later, then we have\nR̃En (1) =\nn∑\nt=1\n∆ht,itIEt,n = H(n)∑\nh=0\n∑\ni∈Ih(n)\nn∑\nt=1\n∆h,iI(ht,it)=(h,i)IEt,n = H(n)∑\nh=0\n∑\ni∈Ih(n)\nKh,i(n)∑\nk=1\nth,i(k)+vh,i(k)∑\nt=th,i(k)\n∆h,iIEt,n\n(2) ≤ H(n)∑\nh=0\n∑\ni∈Ih(n)\nKh,i(n)∑\nk=1\nth,i(k)+vh,i(k)∑\nt=th,i(k)\n 3c √ log(2/δ̃(t))\nTh,i(t)\n  (3)= H(n)∑\nh=0\n∑\ni∈Ih(n)\nKh,i(n)∑\nk=1\nvh,i(k)\n 3c √ log(2/δ̃(th,i(k)))\nTh,i(th,i(k))\n \n≤ H(n)∑\nh=0\n∑\ni∈Ih(n) 3c\n√ log(2/δ̃(t̄h,i)) Kh,i(n)∑\nk=1\nvh,i(k)√ Th,i(th,i(k))\n(4) ≤ 3( √ 2 + 1)c H(n)∑\nh=0\n∑\ni∈Ih(n)\n√ log(2/δ̃(t̄h,i))Th,i(th,i(Kh,i(n))) ≤ 3( √ 2 + 1)c H(n)∑\nh=0\n∑\ni∈Ih(n)\n√ log(2/δ̃(t̄h,i))Th,i(n)\n= 3( √ 2 + 1)c H∑\nh=0\n∑\ni∈Ih(n)\n√ Th,i(n) log(2/δ̃(t̄h,i))\n︸ ︷︷ ︸ (a)\n+3( √ 2 + 1)c H(n)∑\nh=H+1\n∑\ni∈Ih(n)\n√ Th,i(n) log(2/δ̃(t̄h,i))\n︸ ︷︷ ︸ (b)\n,\n(33)\nwhere the first sequence of equalities in (1) simply follows from the definition of episodes. In (2) we bound the instantaneous regret by Eq. 32. Step (3) follows from the fact that when (h, i) is selected, its statistics, including Th,i, are not changed until the end of the episode. Step (4) is an immediate application of Lemma 19 in Jaksch et al. (2010).\nConstants apart the terms (a) and (b) coincides with the terms defined in Eq. 20 and similar bounds can be derived.\nPutting the bounds on R̂En and R̃ E n together leads to\nREn ≤ 2(3 √ 2 + 4)ν1\n[ Cc2ν−d2 ρ d log(2/δ̃(n))\nν21(1− ρ) ρ−H(d+1) + ρHn\n] .\nIt is not difficult to prove that for a suitable choice H , we obtain the final bound of O(log(n)1/(d+2)n(d+1)/(d+2)) on Rn. This combined with the result of Lem. 8 and a union bound on all n ∈ {1, 2, 3, . . . } proves the final result.\nB.3 Proof of Thm. 3\nTheorem 3 Let δ ∈ (0, 1), δ̃(n) = 9 √ ρ/(4ν1)δ/n, and c = 3(3Γ+1) √\n1/(1 − ρ). We assume that Assumptions 1–5 hold and that rewards are generated according to the general model defined in Section 2. Then if δ = 1/n the space complexity of HCT-Γ is\nE(Nn) = O(log(n)2/(d+2)nd/(d+2)).\nProof. We assume that the space requirement for each node (i.e., storing variables such as µ̂h,i, Th.i) is a unit. Let Bt denote the event corresponding to the branching/expansion of the node (ht, it) selected at time t, then the space\ncomplexity is Nn = ∑n t=1 IBt . Similar to the regret analysis, we decompose Nn depending on events Et,n, that is\nNn = n∑\nt=1\nIBtIEt,n + n∑\nt=1\nIBtIEct,n = N E n +N E c n . (34)\nSince we are targeting the expected space complexity, we take the expectation of the previous expression and the second term can be easily bounded as\nE [ N Ecn ] = n∑\nt=1\nIBtP[Ect,n] ≤ n∑\nt=1\nP[Ect ] ≤ n∑\nt=1\nδ\n6t6 ≤ C, (35)\nwhere the last inequality follows from Lemma 8 and C is a constant independent from n. We now focus on the first term N En . We first rewrite it as the total number of nodes |Tn| generated by HCT over n steps. For any depth H > 0 we have\nN En = H(n)∑\nh=0\n|Ih(n)| = 1 + H∑\nh=1\n|Ih(n)|+ H(n)∑\nh=H+1 |Ih(n)| ≤ 1 +H|IH(n)|︸ ︷︷ ︸ (c) +\nH(n)∑\nh=H+1\n|Ih(n)|\n︸ ︷︷ ︸ (d)\n. (36)\nA bound on term (d) can be recovered through the following sequence of inequalities\nn =\nH(n)∑\nh=0\n∑\ni∈Ih(n) Th,i(n) ≥\nH(n)∑\nh=0\n∑\ni∈I+ h (n)\nTh,i(n) (1) ≥ H(n)∑\nh=0\n∑\ni∈I+ h (n)\nτh,i(th,i)\n(2) ≥ H(n)∑\nh=0\n∑\ni∈I+ h (n)\nc2 ν21 ρ−2h (3) ≥ 1 ν21\nH(n)−1∑\nh=H\n|I+h (n)|ρ−2h = 1 ν21 ρ−2H\nH(n)−1∑\nh=H\n|I+h (n)|ρ2(H−h)\n≥ 1 ν21\nρ−2H H(n)−1∑\nh=H\n|I+h (n)| (4) ≥ 1 2ν21\nρ−2H H(n)∑\nh=H+1\n|Ih(n)|,\n(37)\nwhere (1) follows from the fact that nodes in I+h (n) have been expanded at time th,i when their number of pulls Th,i(th,i) ≤ Th,i(n) exceeded the threshold τh,i(th,i). Step (2) follows from Eq. 8, while (3) from the definition of c > 1. Finally, step (4) follows from the fact that the number of nodes at depth h cannot be larger than twice the parent nodes at depth h− 1. By inverting the previous inequality, we obtain\n(d) ≤ 2ν21nρ2H .\nOn other hand, in order to bound (c), we need to use the same the high-probability events Et,n and similar passages as in Eq. 22, which leads to |Ih(n)| ≤ 2|I+h−1(n)| ≤ 2C(ν2ρ(h−1))−d. Plugging these results back in Eq. 36 leads to\nN En ≤ 1 + 2HC(ν2ρ(H−1))−d + 2ν21nρ2H ,\nwith high probability. Together with N Ecn we obtain\nE [ Nn ] ≤ 1 + 2HC(ν2ρ(H−1))−d + 2ν21nρ2H + C ≤ 1 + 2Hmax(n)C(ν2ρ(H−1))−d + 2ν21nρ2H + C,\nwhere Hmax(n) is the upper bound on the depth of the tree in Lemma 1. Optimizing H in the remaining terms leads to the statement."
    } ],
    "references" : [ {
      "title" : "Online learning in markov decision processes with adversarially chosen transition probability distributions",
      "author" : [ "Abbasi", "Yasin", "Bartlett", "Peter", "Kanade", "Varun", "Seldin", "Yevgeny", "Szepesvari", "Csaba" ],
      "venue" : "K.Q. (eds.), Advances in Neural Information Processing Systems",
      "citeRegEx" : "Abbasi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Abbasi et al\\.",
      "year" : 2013
    }, {
      "title" : "Improved rates for the stochastic continuum-armed bandit problem",
      "author" : [ "Auer", "Peter", "Ortner", "Ronald", "Szepesvári", "Csaba" ],
      "venue" : "In COLT, pp",
      "citeRegEx" : "Auer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2007
    }, {
      "title" : "Regret bounds for reinforcement learning with policy advice",
      "author" : [ "Azar", "Mohammad Gheshlaghi", "Lazaric", "Alessandro", "Brunskill", "Emma" ],
      "venue" : "In ECML/PKDD,",
      "citeRegEx" : "Azar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Azar et al\\.",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning in pomdp’s via direct gradient ascent",
      "author" : [ "Baxter", "Jonathan", "Bartlett", "Peter L" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Baxter et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Baxter et al\\.",
      "year" : 2000
    }, {
      "title" : "Lipschitz bandits without the lipschitz constant",
      "author" : [ "Bubeck", "Sébastien", "Stoltz", "Gilles", "Yu", "Jia Yuan" ],
      "venue" : "In ALT,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2011
    }, {
      "title" : "Adaptive-tree bandits",
      "author" : [ "Bull", "Adam" ],
      "venue" : "arXiv preprint arXiv:1302.2489,",
      "citeRegEx" : "Bull and Adam.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bull and Adam.",
      "year" : 2013
    }, {
      "title" : "Regret and convergence bounds for immediate-reward reinforcement learning with continuous action spaces",
      "author" : [ "Cope", "Eric" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Cope and Eric.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cope and Eric.",
      "year" : 2009
    }, {
      "title" : "High dimensional gaussian process bandits",
      "author" : [ "Djolonga", "Josip", "Krause", "Andreas", "Cevher", "Volkan" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Djolonga et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Djolonga et al\\.",
      "year" : 2013
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Jaksch et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "Kleinberg", "Robert", "Slivkins", "Aleksandrs", "Upfal", "Eli" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "Policy search for motor primitives in robotics",
      "author" : [ "Kober", "Jens", "Peters", "Jan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kober et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kober et al\\.",
      "year" : 2011
    }, {
      "title" : "The sample-complexity of general reinforcement learning",
      "author" : [ "Lattimore", "Tor", "Hutter", "Marcus", "Sunehag", "Peter" ],
      "venue" : "In Proceedings of Thirtieth International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Lattimore et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lattimore et al\\.",
      "year" : 2013
    }, {
      "title" : "Empirical bernstein bounds and sample variance penalization",
      "author" : [ "Maurer", "Andreas", "Pontil", "Massimiliano" ],
      "venue" : "arXiv preprint arXiv:0907.3740,",
      "citeRegEx" : "Maurer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Maurer et al\\.",
      "year" : 2009
    }, {
      "title" : "Optimistic optimization of a deterministic function without the knowledge of its smoothness",
      "author" : [ "Munos", "Rémi" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Munos and Rémi.,? \\Q2011\\E",
      "shortCiteRegEx" : "Munos and Rémi.",
      "year" : 2011
    }, {
      "title" : "From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning",
      "author" : [ "Munos", "Rémi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Munos and Rémi.,? \\Q2013\\E",
      "shortCiteRegEx" : "Munos and Rémi.",
      "year" : 2013
    }, {
      "title" : "Online regret bounds for undiscounted continuous reinforcement learning",
      "author" : [ "Ortner", "Ronald", "Ryabko", "Daniil" ],
      "venue" : "K.q. (eds.), Advances in Neural Information Processing Systems",
      "citeRegEx" : "Ortner et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ortner et al\\.",
      "year" : 2012
    }, {
      "title" : "Policy search: Any local optimum enjoys a global performance guarantee",
      "author" : [ "Scherrer", "Bruno", "Geist", "Matthieu" ],
      "venue" : "arXiv preprint arXiv:1306.1520,",
      "citeRegEx" : "Scherrer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Scherrer et al\\.",
      "year" : 2013
    }, {
      "title" : "Contextual bandits with similarity information",
      "author" : [ "Slivkins", "Aleksandrs" ],
      "venue" : "CoRR, abs/0907.3986,",
      "citeRegEx" : "Slivkins and Aleksandrs.,? \\Q2009\\E",
      "shortCiteRegEx" : "Slivkins and Aleksandrs.",
      "year" : 2009
    }, {
      "title" : "Multi-armed bandits on implicit metric spaces",
      "author" : [ "Slivkins", "Aleksandrs" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Slivkins and Aleksandrs.,? \\Q2011\\E",
      "shortCiteRegEx" : "Slivkins and Aleksandrs.",
      "year" : 2011
    }, {
      "title" : "Gaussian process bandits without regret: An experimental design approach",
      "author" : [ "Srinivas", "Niranjan", "Krause", "Andreas", "Kakade", "Sham M", "Seeger", "Matthias" ],
      "venue" : "CoRR, abs/0912.3995,",
      "citeRegEx" : "Srinivas et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Srinivas et al\\.",
      "year" : 2009
    }, {
      "title" : "PAC-bayes-empirical-bernstein inequality",
      "author" : [ "Tolstikhin", "Ilya O", "Seldin", "Yevgeny" ],
      "venue" : "In Advances in Neural Information Processing Systems, pp",
      "citeRegEx" : "Tolstikhin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tolstikhin et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic simultaneous optimistic optimization",
      "author" : [ "Valko", "Michal", "Carpentier", "Alexandra", "Munos", "Rémi" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Valko et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Valko et al\\.",
      "year" : 2013
    }, {
      "title" : "Model-free reinforcement learning as mixture learning",
      "author" : [ "Vlassis", "Nikos", "Toussaint", "Marc" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Vlassis et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Vlassis et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).",
      "startOffset" : 208,
      "endOffset" : 343
    }, {
      "referenceID" : 19,
      "context" : "This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).",
      "startOffset" : 208,
      "endOffset" : 343
    }, {
      "referenceID" : 9,
      "context" : "This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).",
      "startOffset" : 208,
      "endOffset" : 343
    }, {
      "referenceID" : 1,
      "context" : "This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).",
      "startOffset" : 208,
      "endOffset" : 343
    }, {
      "referenceID" : 9,
      "context" : "Our approach builds on recent advances in X -armed bandits for iid settings (Bubeck et al., 2011a; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).",
      "startOffset" : 76,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "Our approach builds on recent advances in X -armed bandits for iid settings (Bubeck et al., 2011a; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).",
      "startOffset" : 76,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : ", 2011a) and zooming algorithm (Kleinberg et al., 2008), both of which only apply to iid setting, in terms of dependency on the number of steps n and the near-optimality dimension d (to be defined later).",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "Similar to the HOO algorithm of Bubeck et al. (2011a), HCT makes use of a covering binary tree for exploring the arm space.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "Among prior work in RL our setting is most similar to the general reinforcement learning model of Lattimore et al. (2013) which also considers an arbitrary temporal dependence between the rewards and observations.",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "Among prior work in RL our setting is most similar to the general reinforcement learning model of Lattimore et al. (2013) which also considers an arbitrary temporal dependence between the rewards and observations. Our setting differs from that of Lattimore et al. (2013), since we consider the regret in undiscounted reward scenario, whereas Lattimore et al.",
      "startOffset" : 98,
      "endOffset" : 271
    }, {
      "referenceID" : 10,
      "context" : "Among prior work in RL our setting is most similar to the general reinforcement learning model of Lattimore et al. (2013) which also considers an arbitrary temporal dependence between the rewards and observations. Our setting differs from that of Lattimore et al. (2013), since we consider the regret in undiscounted reward scenario, whereas Lattimore et al. (2013) focus on proving PAC-bounds in discounted reward case.",
      "startOffset" : 98,
      "endOffset" : 366
    }, {
      "referenceID" : 4,
      "context" : ", Bubeck et al., 2011a), our approach seeks to minimize regret by smartly building an estimate of f using an infinite binary covering tree T , in which each node covers a subset of X .1 We denote by (h, i) the node at depth h and index i among the nodes at the same depth (e.g., the root node which covers X is indexed by (0, 1)). By convention (h + 1, 2i − 1) and (h + 1, 2i) refer to the two children of the node (h, i). The area corresponding to each node (h, i) is denoted by Ph,i ⊂ X . These regions must be measurable and, at each depth, they partition X with no overlap: The reader is referred to Bubeck et al. (2011a) for a more detailed description of the covering tree.",
      "startOffset" : 2,
      "endOffset" : 626
    }, {
      "referenceID" : 4,
      "context" : "These assumptions coincide with those in (Bubeck et al., 2011a), except for the local smoothness (Assumption 4.d), which is weaker than that of Bubeck et al. (2011a), where the function is assumed to be Lipschitz between any two arms x, x′ close to the maximum x∗ (i.",
      "startOffset" : 42,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "These assumptions coincide with those in (Bubeck et al., 2011a), except for the local smoothness (Assumption 4.d), which is weaker than that of Bubeck et al. (2011a), where the function is assumed to be Lipschitz between any two arms x, x′ close to the maximum x∗ (i.e., |f(x) − f(x′)| ≤ l(x, x′)), while here we only require the function to be Lipschitz w.r.t. the maximum. Finally, we characterize the complexity of the problem using the near-optimality dimension, which defines how large is the set of ǫ-optimal arms in X . For the sake of clarity, we consider a slightly simplified definition of near-optimality dimension w.r.t. Bubeck et al. (2011a). Assumption 5 (Near-optimality dimension).",
      "startOffset" : 42,
      "endOffset" : 655
    }, {
      "referenceID" : 21,
      "context" : "As it has been shown in (Valko et al., 2013) the case of d = 0 covers a rather large class of functions, since every function which satisfies some mild local smoothness assumption, around its",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "Policy search algorithms (Scherrer & Geist, 2013; Azar et al., 2013; Kober & Peters, 2011) aim at finding the policy in a given policy set which maximizes the long-term performance.",
      "startOffset" : 25,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "A related work to HCT-Γ is the UCCRL algorithm by Ortner & Ryabko (2012), which extends the original UCRL algorithm (Jaksch et al., 2010) to continuous state spaces.",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "Another relevant work is the OMDP algorithm of Abbasi et al. (2013) which deals with the problem of RL in continuous state-action MDPs with adversarial rewards.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "For our first experiment we compare HCT-iid to the truncated hierarchical optimistic optimization (T-HOO) algorithm Bubeck et al. (2011a). T-HOO is a state-of-the-art X -armed bandit algorithm, developed as a computationally-efficient alternative of HOO.",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "For our first experiment we compare HCT-iid to the truncated hierarchical optimistic optimization (T-HOO) algorithm Bubeck et al. (2011a). T-HOO is a state-of-the-art X -armed bandit algorithm, developed as a computationally-efficient alternative of HOO. In Fig. 1(a) we show the per-step regret, the runtime, and the space requirements of each approach. As predicted by the theoretical bounds, the per-step regret R̃n of both HCT-iid and truncated HOO decrease rapidly with number of steps. Though the big O theoretical bounds are identical for both approaches, empirically we observe in this Refer to Bubeck et al. (2011a); Munos (2013) for how to transform bounds on accumulated regret to simple regret bounds.",
      "startOffset" : 116,
      "endOffset" : 625
    }, {
      "referenceID" : 4,
      "context" : "For our first experiment we compare HCT-iid to the truncated hierarchical optimistic optimization (T-HOO) algorithm Bubeck et al. (2011a). T-HOO is a state-of-the-art X -armed bandit algorithm, developed as a computationally-efficient alternative of HOO. In Fig. 1(a) we show the per-step regret, the runtime, and the space requirements of each approach. As predicted by the theoretical bounds, the per-step regret R̃n of both HCT-iid and truncated HOO decrease rapidly with number of steps. Though the big O theoretical bounds are identical for both approaches, empirically we observe in this Refer to Bubeck et al. (2011a); Munos (2013) for how to transform bounds on accumulated regret to simple regret bounds.",
      "startOffset" : 116,
      "endOffset" : 639
    }, {
      "referenceID" : 4,
      "context" : "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm.",
      "startOffset" : 13,
      "endOffset" : 201
    }, {
      "referenceID" : 4,
      "context" : "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm.",
      "startOffset" : 13,
      "endOffset" : 218
    }, {
      "referenceID" : 4,
      "context" : "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm. 4, and do not require knowledge of the dissimilarity measure l. On the other hand, Slivkins (2011) and Bull (2013) study the cumulative regret but consider a different definition of smoothness related to the zooming concept introduced by Kleinberg et al.",
      "startOffset" : 13,
      "endOffset" : 439
    }, {
      "referenceID" : 4,
      "context" : "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm. 4, and do not require knowledge of the dissimilarity measure l. On the other hand, Slivkins (2011) and Bull (2013) study the cumulative regret but consider a different definition of smoothness related to the zooming concept introduced by Kleinberg et al.",
      "startOffset" : 13,
      "endOffset" : 455
    }, {
      "referenceID" : 4,
      "context" : "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm. 4, and do not require knowledge of the dissimilarity measure l. On the other hand, Slivkins (2011) and Bull (2013) study the cumulative regret but consider a different definition of smoothness related to the zooming concept introduced by Kleinberg et al. (2008). Finally, we notice that to deal with unknown mixing time, one may rely on data-dependent tail’s inequalities, such as empirical Bernstein inequality (Tolstikhin & Seldin, 2013; Maurer & Pontil, 2009), replacing the mixing time with the empirical variance of the rewards.",
      "startOffset" : 13,
      "endOffset" : 602
    }, {
      "referenceID" : 2,
      "context" : "1 Concentration Inequality for non-iid Episodic Random Variables In this section we extend the result in (Azar et al., 2013) and we derive a concentration inequality for averages of non-iid random variables grouped in episodes.",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "Step (4) is an immediate application of Lemma 19 in Jaksch et al. (2010). Constants apart the terms (a) and (b) coincides with the terms defined in Eq.",
      "startOffset" : 52,
      "endOffset" : 73
    } ],
    "year" : 2014,
    "abstractText" : "In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel any-time X -armed bandit algorithm, and derive regret bounds matching the performance of existing state-of-the-art in terms of dependency on number of steps and smoothness factor. The main advantage of HCT is that it handles the challenging case of correlated rewards, whereas existing methods require that the reward-generating process of each arm is an identically and independent distributed (iid) random process. HCT also improves on the state-of-the-art in terms of its memory requirement as well as requiring a weaker smoothness assumption on the mean-reward function in compare to the previous anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.",
    "creator" : "LaTeX with hyperref package"
  }
}