{
  "name" : "1405.2652.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Selecting Near-Optimal Approximate State Representations in Reinforcement Learning",
    "authors" : [ "Ronald Ortner", "Odalric-Ambrym Maillard", "Daniil Ryabko" ],
    "emails" : [ "rortner@unileoben.ac.at,", "odalric-ambrym.maillard@ens-cachan.org,", "daniil@ryabko.net" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n26 52\nv6 [\ncs .L\nG ]\n1 5\nSe p"
    }, {
      "heading" : "1 Introduction",
      "text" : "Inspired by [3], in [6] a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying Markov decision process (MDP). Instead, the learner has a set of models at her disposal that map histories (i.e., observations, chosen actions and collected rewards) to states. However, only some models give a correct MDP representation. The first regret bounds in this setting were derived in [6]. They recently have been improved in [7] and extended to infinite model sets in [8]. Here we extend and improve the results of [7] as follows. First, we do not assume anymore that the model set given to the learner contains a true model resulting in an MDP representation. Instead, models will only approximate an MDP. Second, we improve the bounds of [7] with respect to the dependence on the state space.\nFor discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6–8]. Here we only would like to mention the recent work [2] that considers a similar setting, however is mainly interested in the question whether the true model will be identified in the long run, a question we think is subordinate to that of minimizing the regret, which means fast learning of optimal behavior."
    }, {
      "heading" : "1.1 Setting",
      "text" : "For each time step t = 1, 2, . . ., let Ht := O × (A × R × O)t−1 be the set of histories up to time t, where O is the set of observations, A a finite set of\nactions, and R = [0, 1] the set of possible rewards. We consider the following reinforcement learning problem: The learner receives some initial observation h1 = o1 ∈ H1 = O. Then at any time step t > 0, the learner chooses an action at ∈ A based on the current history ht ∈ Ht, and receives an immediate reward rt and the next observation ot+1 from the unknown environment. Thus, ht+1 is the concatenation of ht with (at, rt, ot+1).\nState representation models.A state-representation model φ is a function from the set of histories H = ⋃t≥1 Ht to a finite set of states Sφ. A particular role will be played by state-representation models that induce a Markov decision process (MDP). An MDP is defined as a decision process in which at any discrete time t, given action at, the probability of immediate reward rt and next observation ot+1, given the past history ht, only depends on the current observation ot i.e., P (ot+1, rt|htat) = P (ot+1, rt|ot, at), and this probability is also independent of t. Observations in this process are called states of the environment. We say that a state-representation model φ is a Markov model of the environment, if the process (φ(ht), at, rt), t ∈ N is an MDP. Note that such an MDP representation needs not be unique. In particular, we assume that we obtain a Markov model when mapping each possible history to a unique state. Since these states are not visited more than once, this model is not very useful from the practical point of view, however. In general, an MDP is denoted as M(φ) = (Sφ,A, r, p), where r(s, a) is the mean reward and p(s′|s, a) the probability of a transition to state s′ ∈ Sφ when choosing action a ∈ A in state s ∈ Sφ.\nWe assume that there is an underlying true Markov model φ◦ that gives a finite and weakly communicating MDP, that is, for each pair of states s, s′ ∈ S◦ := Sφ◦ there is a k ∈ N and a sequence of actions a1, . . . , ak ∈ A such that the probability of reaching state s′ when starting in state s and taking actions a1, . . . , ak is positive. In such a weakly communicating MDP we can define the diameter D := D(φ◦) := D(M(φ◦)) to be the expected minimum time it takes to reach any state starting from any other state in the MDP M(φ◦), cf. [5]. In finite state MDPs, the Poisson equation relates the average reward ρπ of any policy π to the single step mean rewards and the transition probabilities. That is, for each policy π that maps states to actions, it holds that\nρπ + λπ(s) = r(s, π(s)) + ∑ s′∈S◦ p(s ′|s, π(s)) · λπ(s′), (1)\nwhere λπ is the so-called bias vector of π, which intuitively quantifies the difference in accumulated rewards when starting in different states. Accordingly, we are sometimes interested in the span of the bias vector λ of an optimal policy defined as span(λ) := maxs∈S◦ λ(s)−mins′∈S◦ λ(s′). In the following we assume that rewards are bounded in [0, 1], which implies that span(λ) is upper bounded by D, cf. [5, 1].\nProblem setting. Given a finite set of models Φ (not necessarily containing a Markov model), we want to construct a strategy that performs as well as the algorithm that knows the underlying true Markov model φ◦, including its rewards and transition probabilities. For that purpose we define for the Markov\nmodel φ◦ the regret of any strategy at time T , cf. [5, 1, 6], as\n∆(φ◦, T ) := Tρ∗(φ◦)−∑Tt=1 rt ,\nwhere rt are the rewards received when following the proposed strategy and ρ∗(φ◦) is the average optimal reward in φ◦, i.e. ρ∗(φ◦) := ρ∗(M(φ◦)) := ρ(M(φ◦), π∗φ◦) := limT→∞ 1 T E [∑T t=1 rt(π ∗ φ◦) ] where rt(π ∗ φ◦) are the rewards received when following the optimal policy π∗φ◦ on M(φ ◦). Note that for weakly communicating MDPs the average optimal reward does not depend on the initial state.\nWe consider the case when Φ is finite and the learner has no knowledge of the correct approximation errors of each model in Φ. Thus, while for each model φ ∈ Φ there is an associated ǫ = ǫ(φ) ≥ 0 which indicates the aggregation error (cf. Definition 1 below), this ǫ is unknown to the learner for each model.\nWe remark that we cannot expect to perform as well as the unknown underlying Markov model, if the model set only provides approximations. Thus, if the best approximation has error ǫ we have to be ready to accept respective error of order ǫD per step, cf. the lower bound provided by Theorem 2 below.\nOverview. We start with explicating our notion of approximation in Section 2, then introduce our algorithm in Section 3, present our regret bounds in Section 4, and conclude with the proofs in Section 5."
    }, {
      "heading" : "2 Preliminaries: MDP Approximations",
      "text" : "Approximations. Before we give the precise notion of approximation we are going to use, first note that in our setting the transition probabilities p(h′|h, a) for any two histories h, h′ ∈ H and an action a are well-defined. Then given an arbitrary model φ and a state s′ ∈ Sφ, we can define the aggregated transition probabilities pagg(s′|h, a) := ∑h′:φ(h′)=s′ p(h′|h, a). Note that the true transition probabilities under φ◦ are then given by p(s′|s, a) := pagg(s′|h, a) for s = φ◦(h) and s′ ∈ S◦.\nDefinition 1. A model φ is said to be an ǫ-approximation of the true model φ◦ if: (i) for all histories h, h′ with φ(h) = φ(h′) and all actions a\n∣∣r(φ◦(h), a)− r(φ◦(h′), a) ∣∣ < ǫ, and ∥∥p(·|φ◦(h), a)− p(·|φ◦(h′), a) ∥∥ 1 < ǫ2 , (2)\nand (ii) there is a surjective mapping α : S◦ → Sφ such that for all histories h and all actions a it holds that\n∑\nṡ′∈Sφ\n∣∣∣pagg(ṡ′|h, a)− ∑\ns′∈S◦:α(s′)=ṡ′ pagg(s′|h, a)\n∣∣∣ < ǫ2 . (3)\nIntuitively, condition (2) assures that the approximate model aggregates only histories that are mapped to similar states under the true model. Complementary, condition (3) guarantees that the state space under the approximate model\nresembles the true state space.1 Note that any model will be an ǫ-approximation of the underlying true model φ◦ for sufficiently large ǫ.\nA particular natural case are approximation models φ which also satisfy\n∀h, h′ ∈ H : φ◦(h) = φ◦(h′) =⇒ φ(h) = φ(h′).\nThat is, intuitively, states in S◦ are aggregated to meta-states in Sφ, and (3) holds trivially.\nWe may carry over our definition of ǫ-approximation to MDPs. This will turn out useful, since each approximate model can be interpreted as an MDP approximation, cf. Section 5.1 below.\nDefinition 2. An MDP M̄ = (S̄,A, r̄, p̄) is an ǫ-approximation of another MDP M = (S,A, r, p) if there is a surjective function α : S → S̄ such that for all s in S:\n∣∣r̄(α(s), a) − r(s, a) ∣∣ < ǫ, and ∑\nṡ′∈S̄\n∣∣∣p̄(ṡ′|α(s), a)− ∑\ns′:α(s′)=ṡ′\np(s′|s, a) ∣∣∣ < ǫ. (4)\nError Bounds for ǫ-Approximations. The following is an error bound on the error made by an ǫ-approximation. It generalizes bounds of [9] from ergodic to communicating MDPs. For a proof see Appendix A.\nTheorem 1. Let M be a communicating MDP and M̄ be an ǫ-approximation of M . Then ∣∣ρ∗(M)− ρ∗(M̄) ∣∣ ≤ ǫ (D(M) + 1).\nThe following is a matching lower bound on the error by aggregation. This is an improvement over the results in [9], which only showed that the error approaches 1 when the diameter goes to infinity.\nTheorem 2. For each ǫ > 0 and each 2 < D < 4ǫ there are MDPs M , M̄ such that M̄ is an ǫ-approximation of M , M has diameter D(M) = D, and\n|ρ∗(M)− ρ∗(M̄)| > 156ǫD(M).\nProof. Consider the MDP M shown in Figure 1 (left), where the (deterministic) reward in states s0, s ′ 0 is 0 and 1 in state s1. We assume that 0 < ε := ǫ 2 < δ := 2 D . Then the diameter D(M) is the expected transition time from s′0 to s0 and equal to 2δ = D. Aggregating states s0, s ′ 0 gives the MDP M̄ on the right hand side of Figure 1. Obviously, M̄ is an ǫ-approximation of M . It is straightforward to check that the stationary distribution µ (of the only policy) in M is (µ(s0), µ(s ′ 0), µ(s1)) = ( δ 3ε+4δ , ε+δ 3ε+4δ , 2ε+2δ 3ε+4δ ) , while the stationary distribution in M̄ is (12 , 1 2 ). Thus, the difference in average reward is\n|ρ∗(M)− ρ∗(M̄)| = 2ε+2δ3ε+4δ − 12 = ε2(3ε+4δ) > ε14δ = 156ǫD(M). ⊓⊔ 1 The allowed error in the conditions for the transition probabilities is chosen to be ǫ\n2\nso that the total error with respect to the transition probabilities is ǫ. This matches the respective condition for MDP approximations in Definition 2, cf. also Section 5.1.\nTheorems 1 and 2 compare the optimal policies of two different MDPs, however it is straightforward to see from the proofs that the same error bounds hold when comparing on some MDP M the optimal average reward ρ∗(M) to the average reward when applying the optimal policy of an ǫ-approximation M̄ of M . Thus, when we approximate an MDP M by an ǫ-approximation M̄ , the respective error of the optimal policy of M̄ on M can be of order ǫD(M) as well. Hence, we cannot expect to perform below this error if we only have an ǫ-approximation of the true model at our disposal."
    }, {
      "heading" : "3 Algorithm",
      "text" : "The OAMS algorithm (shown in detail as Algorithm 1) we propose for the setting introduced in Section 1 is a generalization of the OMS algorithm of [7]. Application of the original OMS algorithm to our setting would not work, since OMS compares the collected rewards of each model to the reward it would receive if the model were Markov. Models not giving sufficiently high reward are identified as nonMarkov and rejected. In our case, there may be no Markov model in the set of given models Φ. Thus, the main difference to OMS is that OAMS for each model estimates and takes into account the possible approximation error with respect to a closest Markov model.\nOAMS proceeds in episodes k = 1, 2, . . ., each consisting of several runs j = 1, 2, . . .. In each run j of some episode k, starting at time t = tkj , OAMS chooses a policy πkj applying the optimism in face of uncertainty principle twice.\nPlausible models. First, OAMS considers for each model φ ∈ Φ a set of plausible MDPs Mt,φ defined to contain all MDPs with state space Sφ and with rewards r+ and transition probabilities p+ satisfying\n∣∣r+(s, a)− r̂t(s, a) ∣∣ ≤ ǫ̃(φ) +\n√ log(48SφAt\n3/δ) 2Nt(s,a) , (5)\n∥∥p+(·|s, a)− p̂t(·|s, a) ∥∥ 1 ≤ ǫ̃(φ) +\n√ 2Sφ log(48SφAt\n3/δ) Nt(s,a) , (6)\nwhere ǫ̃(φ) is the estimate for the approximation error of model φ (cf. below), p̂t(·|s, a) and r̂t(s, a) are respectively the empirical state-transition probabilities\nand the mean reward at time t for taking action a in state s ∈ Sφ, Sφ := |Sφ| denotes the number of states under model φ, A := |A| is the number of actions, and Nt(s, a) is the number of times action a has been chosen in state s up to time t. (If a hasn’t been chosen in s so far, we set Nt(s, a) to 1.) The inequalities (5) and (6) are obviously inspired by Chernov bounds that would hold with high probability in case the respective model φ is Markov, cf. also Lemma 1 below.\nOptimistic MDP for each model φ. In line 4, the algorithm computes for each model φ a so-called optimistic MDP M+t (φ) ∈ Mt,φ and an associated optimal policy π+t,φ on M + t (φ) such that the average reward ρ(M + t (φ), π + t,φ) is maximized. This can be done by extended value iteration (EVI) [5]. Indeed, if r+t (s, a) and p + t (s\n′|s, a) denote the optimistic rewards and transition probabilities of M+t (φ), then EVI computes optimistic state values u + t,φ = (u + t,φ(s))s ∈ RSφ such that (cf. [5])\nρ̂+t (φ) := min s∈Sφ\n{ r+t (s, π + t,φ(s)) + ∑\ns′\np+t (s ′|s, π+t,φ(s))u+t,φ(s′)− u+t,φ(s)\n} (7)\nis an approximation of ρ∗(M+t (φ)), that is,\nρ̂+t (φ) ≥ ρ∗(M+t (φ)) − 2/ √ t. (8)\nOptimistic model selection. In line 5, OAMS chooses a model φkj ∈ Φ with corresponding MDP Mkj = M + t (φkj) and policy πkj := π + t,φkj that maximizes the average reward penalized by the term pen(φ, t) defined as\npen(φ, t) := 2−j/2 (( λ(u+t,φ) √ 2Sφ + 3√ 2 )√ SφA log ( 48SφAt3 δ ) (9)\n+λ(u+t,φ)\n√ 2 log(24t 2 δ ) ) + 2−jλ(u+t,φ) + ǫ̃(φ) ( λ(u+t,φ) + 3 ) ,\nwhere we define λ(u+t,φ) := maxs∈Sφ u + t,φ(s)−mins∈Sφ u+t,φ(s) to be the empirical value span of the optimistic MDP M+t (φ). Intuitively, the penalization term is an upper bound on the per-step regret of the model φ in the run to follow in case φ is chosen, cf. eq. (35) in the proof of the main theorem. Similar to the REGAL algorithm of [1] this shall prefer simpler models (i.e., models having smaller state space and smaller value span) to more complex ones.\nTermination of runs and episodes. The chosen policy πkj is then executed until either (i) run j reaches the maximal length of 2j steps, (ii) episode k terminates when the number of visits in some state has been doubled (line 12), or (iii) the executed policy πkj does not give sufficiently high rewards (line 9). That is, at any time t in run j of episode k it is checked whether the total reward in the current run is at least ℓkjρkj − lobkj(t), where ℓkj := t − tkj + 1 is the (current) length of run j in episode k, and lobkj(t) is defined as\nlobkj(t) := ( λ+kj √ 2Skj + 3√ 2 )∑\ns∈Skj\n∑\na∈A\n√ vkj(s, a) log ( 48SkjAt3kj δ )\n+ λ+kj √ 2ℓkj log ( 24t2kj δ ) + λ+kj + ǫ̃ ( φkj)ℓkj(λ + kj + 3 ) , (10)\nAlgorithm 1 Optimal Approximate Model Selection (OAMS)\ninput set of models Φ, confidence parameter δ ∈ (0, 1), precision parameter ǫ0 ∈ (0, 1) 1: Let t be the current time step, and set ǫ̃(φ) := ǫ0 for all φ ∈ Φ. 2: for episodes k = 1, 2, . . . do 3: for runs j = 1, 2, . . . do 4: ∀φ ∈ Φ, use EVI to compute an optimistic MDP M+t (φ) in Mt,φ (the set\nof plausible MDPs defined via the confidence intervals (6) and (5) for the estimates so far), a (near-)optimal policy π+t,φ on M + t (φ) with approximate average reward ρ̂+t (φ), and the empirical value span λ(u +\nt,φ). 5: Choose model φkj ∈ Φ such that\nφkj = argmax φ∈Φ\n{ ρ̂ + t (φ)− pen(φ, t) } . (11)\n6: Set tkj := t, ρkj := ρ̂ + t (φkj), πkj := π +\nt,φkj , and Skj := Sφkj .\n7: for 2j steps do 8: Choose action at := πkj(st), get reward rt, observe next state st+1 ∈ Skj . 9: if the total reward collected so far in the current run is less than\n(t− tkj + 1)ρkj − lobkj(t), (12)\nthen\n10: ǫ̃(φkj) := 2ǫ̃(φkj) 11: Terminate current episode. 12: else if ∑j j′=1\nvkj′(st, at) = Ntk1(st, at) then 13: Terminate current episode. 14: end if 15: end for 16: end for 17: end for\nwhere λ+kj := λ(u + tkj ,φkj\n), Skj := Sφkj , and vkj(s, a) are the (current) state-action counts of run j in episode k. That way, OAMS assumes each model to be Markov, as long as it performs well. We will see that lobkj(t) can be upper bounded by ℓkjpen(φkj , tkj), cf. eq. (35) below.\nGuessing the approximation error. The algorithm tries to guess for each model φ the correct approximation error ǫ(φ). In the beginning the guessed value ǫ̃(φ) for each model φ ∈ Φ is set to the precision parameter ǫ0, the best possible precision we aim for. Whenever the reward test fails for a particular model φ, it is likely that ǫ̃(φ) is too small and it is therefore doubled (line 10)."
    }, {
      "heading" : "4 Regret Bounds",
      "text" : "The following upper bound on the regret of OAMS is the main result of this paper.\nTheorem 3. There are c1, c2, c3 ∈ R such that in each learning problem where the learner is given access to a set of models Φ not necessarily containing the\ntrue model φ◦, the regret ∆(φ◦, T ) of OAMS (with parameters δ, ǫ0) with respect to the true model φ◦ after any T ≥ SA steps is upper bounded by\nc1 ·DSA(log( 1ǫ0 ) logT + log 2 T ) + c2 ·Dmax{ǫ0, ǫ(φ)}T\n+c3 · ( DSφ √ SA log3/2(Tδ ) + √ |Φ| log( 1ǫ0 ) logT )√ T\nwith probability at least 1− δ, where φ ∈ Φ is an ǫ(φ)-approximation of the true underlying Markov model φ◦, D := D(φ◦), and S := ∑ φ∈Φ Sφ.\nAs already mentioned, by Theorem 2 the second term in the regret bound is unavoidable when only considering models in Φ. Note that Theorem 3 holds for all models φ ∈ Φ. For the best possible bound there is a payoff between the size Sφ of the approximate model and its precision ǫ(φ).\nWhen the learner knows that Φ contains a Markov model φ◦, the original OMS algorithm of [7] can be employed. In case when the total number S = ∑ φ Sφ of states over all models is large, i.e., S > D2|Φ|S◦, we can improve on the state space dependence of the regret bound given in [7] as follows. The proof (found in Appendix H) is a simple modification of the analysis in [7] that exploits that by (11) the selected models cannot have arbitrarily large state space. Theorem 4. If Φ contains a Markov model φ◦, with probability at least 1 − δ the regret of OMS is bounded by Õ(D2S◦3/2A √ |Φ|T ).\nDiscussion. Unfortunately, while the bound in Theorem 3 is optimal with respect to the dependence on the horizon T , the improvement on the state space dependence that we could achieve in Theorem 4 for OMS is not as straightforward for OAMS and remains an open question just as the optimality of the bound with respect to the other appearing parameters. We note that this is still an open question even for learning in MDPs (without additionally selecting the state representation) as well, cf. [5].\nAnother direction for future work is the extension to the case when the underlying true MDP has continuous state space. In this setting, the models have the natural interpretation of being discretizations of the original state space. This could also give improvements over current regret bounds for continuous reinforcement learning as given in [10]. Of course, the most challenging goal remains to generate suitable state representation models algorithmically instead of assuming them to be given, cf. [3]. However, at the moment it is not even clear how to deal with the case when an infinite set of models is given."
    }, {
      "heading" : "5 Proof of Theorem 3",
      "text" : "The proof is divided into three parts and follows the lines of [7], now taking into account the necessary modifications to deal with the approximation error. First, in Section 5.1 we deal with the error of ǫ-approximations. Then in Section 5.2, we show that all state-representation models φ which are an ǫ(φ)-approximation of a Markov model pass the test in (12) on the rewards collected so far with high probability, provided that the estimate ǫ̃(φ) ≥ ǫ(φ). Finally, in Section 5.3 we use this result to derive the regret bound of Theorem 3."
    }, {
      "heading" : "5.1 Error Bounds for ǫ-Approximate Models",
      "text" : "We start with some observations about the empirical rewards and transition probabilities our algorithm calculates and employs for each model φ. While the estimated rewards r̂ and transition probabilities p̂ used by the algorithm do in general not correspond to some underlying true values, the expectation values of r̂ and p̂ are still well-defined, given the history h ∈ H so far. Indeed, consider some h ∈ H with φ(h) = ṡ ∈ Sφ, φ◦(h) = s ∈ S◦, and an action a, and assume that the estimates r̂(ṡ, a) and p̂(·|ṡ, a) are calculated from samples when action a was chosen after histories h1, h2, . . . , hn ∈ H that are mapped to the same state ṡ by φ. (In the following, we will denote the states of an approximation φ by variables with dot, such as ṡ, ṡ′, etc., and states in the state space S◦ of the true Markov model φ◦ without a dot, such as s, s′, etc.) Since rewards and transition probabilities are well-defined under φ◦, we have\nE[r̂(ṡ, a)] = 1n\nn∑\ni=1\nr(φ◦(hi), a), and E[p̂(ṡ ′|ṡ, a)] = 1n\nn∑\ni=1\n∑\nh′:φ(h′)=ṡ′\np(h′|hi, a). (13)\nSince φ maps the histories h, h1, . . . , hn to the same state ṡ ∈ Sφ, the rewards and transition probabilities in the states φ◦(h), φ◦(h1), . . . , φ\n◦(hn) of the true underlying MDP are ǫ-close, cf. (2). It follows that for s = φ◦(h) and ṡ = φ(h)\n∣∣∣E[r̂(ṡ, a)]− r(s, a) ∣∣∣ = ∣∣∣ 1n n∑\ni=1\n( r(φ◦(hi), a)− r(φ◦(h), a) )∣∣∣ < ǫ(φ). (14)\nFor the transition probabilities we have by (3) for i = 1, . . . , n ∑\nṡ′∈Sφ\n∣∣∣pagg(ṡ′|hi, a)− ∑\ns′∈S◦:α(s′)=ṡ′ pagg(s′|hi, a)\n∣∣∣ < ǫ(φ)2 . (15)\nFurther, all hi as well as h are mapped to ṡ by φ so that according to (2) and recalling that s = φ◦(h) we have for i = 1, . . . , n\n∑\nṡ′∈Sφ\n∣∣∣ ∑\ns′∈S◦:α(s′)=ṡ′ pagg(s′|hi, a)−\n∑\ns′∈S◦:α(s′)=ṡ′ p(s′|s, a)\n∣∣∣\n≤ ∑\ns′∈S◦\n∣∣pagg(s′|hi, a)− p(s′|s, a) ∣∣ < ǫ(φ)2 . (16)\nBy (15) and (16) for i = 1, . . . , n ∑\nṡ′∈Sφ\n∣∣∣pagg(ṡ′|hi, a)− ∑\ns′∈S◦:α(s′)=ṡ′ p(s′|s, a)\n∣∣∣ < ǫ(φ), (17)\nso that from (13) and (17) we can finally bound ∑\nṡ′∈Sφ\n∣∣∣E[p̂(ṡ′|ṡ, a)]− ∑\ns′∈S◦:α(s′)=ṡ′ p(s′|s, a)\n∣∣∣\n≤ 1n n∑\ni=1\n∑\nṡ′∈Sφ\n∣∣∣pagg(ṡ′|hi, a)− ∑\ns′∈S◦:α(s′)=ṡ′ p(s′|s, a)\n∣∣∣ < ǫ(φ). (18)\nThus, according to (14) and (18) the ǫ-approximate model φ gives rise to an MDP M̄ on Sφ with rewards r̄(ṡ, a) := E[r̂(ṡ, a)] and transition probabilities p̄(ṡ′|ṡ, a) := E[p̂(ṡ′|ṡ, a)] that is an ǫ-approximation of the true MDP M(φ◦). Note that M̄ actually depends on the history so far.\nThe following lemma gives some basic confidence intervals for the estimated rewards and transition probabilities. For a proof sketch see Appendix B.\nLemma 1. Let t be an arbitrary time step and φ ∈ Φ be the model employed at step t. Then the estimated rewards r̂ and transition probabilities p̂ satisfy for all ṡ, ṡ′ ∈ Sφ and all a ∈ A\nr̂(ṡ, a)− E[r̂(ṡ, a)] ≤ √ log(48SφAt 3/δ)\nNt(ṡ,a) ,\n∥∥∥p̂(·|ṡ, a)− E[p̂(·|ṡ, a)] ∥∥∥ 1 ≤\n√ 2Sφ log(48SφAt\n3/δ) Nt(ṡ,a) ,\neach with probability at least 1− δ24t2 .\nThe following is a consequence of Theorem 1, see Appendix C for a detailed proof.\nLemma 2. Let φ◦ be the underlying true Markov model leading to MDP M = (S◦,A, r, p), and φ be an ǫ-approximation of φ◦. Assume that the confidence intervals given in Lemma 1 hold at step t for all states ṡ, ṡ′ ∈ Sφ and all actions a. Then the optimistic average reward ρ̂+t (φ) defined in (7) satisfies\nρ̂+t (φ) ≥ ρ∗(M)− ǫ(D(M) + 1)− 2√t .\n5.2 Approximate Markov models pass the test in (12)\nAssume that the model φkj ∈ Φ employed in run j of episode k is an ǫkj := ǫ(φkj)-approximation of the true Markov model. We are going to show that φkj will pass the test (12) on the collected rewards with high probability at any step t, provided that ǫ̃kj := ǫ̃(φkj) ≥ ǫkj .\nLemma 3. For each step t in some run j of some episode k, given that tkj = t ′ the chosen model φkj passes the test in (12) at step t with probability at least 1− δ6t′2 whenever ǫ̃kj(φkj) ≥ ǫ(φkj).\nProof. In the following, ṡτ := φkj(hτ ) and sτ := φ ◦(hτ ) are the states at time step τ under model φkj and the true Markov model φ ◦, respectively.\nInitial decomposition. First note that at time t when the test is performed, we have ∑ ṡ∈Skj ∑ a∈A vkj(ṡ, a) = ℓkj = t− t′ + 1, so that\nℓkjρkj − t∑\nτ=t′\nrτ = ∑\nṡ∈Skj\n∑ a∈A vkj(ṡ, a) ( ρkj − r̂t′:t(ṡ, a) ) ,\nwhere r̂t′:t(ṡ, a) is the empirical average reward collected for choosing a in ṡ from time t′ to the current time t in run j of episode k.\nLet r+kj(ṡ, a) be the rewards and p + kj(·|ṡ, a) the transition probabilities of the\noptimistic model M+tkj (φkj). Noting that vkj(ṡ, a) = 0 when a 6= πkj(ṡ), we get\nℓkjρkj − t∑\nτ=t′\nrτ = ∑\nṡ,a\nvkj(ṡ, a) ( ρ̂+kj(φkj)− r+kj(ṡ, a) ) (19)\n+ ∑\nṡ,a\nvkj(ṡ, a) ( r+kj(ṡ, a)− r̂t′:t(ṡ, a) ) . (20)\nWe continue bounding the two terms (19) and (20) separately.\nBounding the reward term (20). Recall that r(s, a) is the mean reward for choosing a in s in the true Markov model φ◦. Then we have at each time step τ = t′, . . . , t\nr+kj(ṡτ , a)− r̂t′:t(ṡτ , a) = ( r+kj(ṡτ , a)− r̂t′(ṡτ , a) )\n+ ( r̂t′(ṡτ , a)− E[r̂t′(ṡτ , a)] ) + ( E[r̂t′(ṡτ , a)]− r(sτ , a) ) + ( r(sτ , a)− E[r̂t′ :t(ṡτ , a)] ) + ( E[r̂t′ :t(ṡτ , a)]− r̂t′:t(ṡτ , a) )\n≤ ǫ̃kj + 2 √\nlog(48SkjAt′3/δ) 2Nt′(ṡ,a)\n+ 2ǫkj + √\nlog(48SkjAt′3/δ) 2vkj(ṡ,a) , (21)\nwhere we bounded the first term in the decomposition by (5), the second term by Lemma 1, the third and fourth according to (14), and the fifth by an equivalent to Lemma 1 for the rewards collected so far in the current run. In summary, with probability at least 1− δ12t′2 we can bound (20) as ∑\nṡ,a\nvkj(ṡ, a) ( r+kj(ṡ, a)− r̂t′:t(ṡ, a) ) ≤ 3ǫ̃kjℓkj+ 3√2 ∑\nṡ,a\n√ vkj(ṡ, a) log (48SkjAt′3 δ ) , (22)\nwhere we used the assumption that ǫ̃kj ≥ ǫkj as well as vkj(ṡ, a) ≤ Nt′(ṡ, a). Bounding the bias term (19). First, notice that we can use (7) to bound ∑\nṡ,a\nvkj(ṡ, a) ( ρ̂+kj(φkj)−r+kj(ṡ, a) ) ≤ ∑\nṡ,a\nvkj(ṡ, a) (∑\nṡ′\np+kj(ṡ ′|ṡ, a)u+kj(ṡ′)−u+kj(ṡ)\n) ,\nwhere u+kj(ṡ) := u + tkj ,φkj (ṡ) are the state values given by EVI. Further, since the transition probabilities p+kj(·|ṡ, a) sum to 1, this is invariant under a translation of the vector u+kj . In particular, defining wkj(ṡ) := u + kj(ṡ)− 12 ( minṡ∈Skj u + kj(ṡ)+ maxṡ∈Skj u + kj(ṡ) ) , so that ‖wkj‖∞ = λ+kj/2, we can replace u+kj with wkj , and (19) can be bounded as ∑\nṡ,a\nvkj(ṡ, a) ( ρ̂+kj(φkj)− r+kj(ṡ, a) )\n≤ ∑\nṡ,a\nt∑\nτ=t′\n1 { (ṡτ , aτ ) = (ṡ, a)\n}( ∑\nṡ′∈Skj\np+kj(ṡ ′|ṡτ , a)wkj(ṡ′)− wkj(ṡτ )\n) . (23)\nNow we decompose for each time step τ = t′, . . . , t ∑\nṡ′∈Skj\np+kj(ṡ ′|ṡτ , a)wkj(ṡ′)− wkj(ṡτ ) =\n∑\nṡ′∈Skj\n( p+kj(ṡ ′|ṡτ , a)− p̂t′(ṡ′|ṡτ , a) ) wkj(ṡ ′) (24)\n+ ∑\nṡ′∈Skj\n( p̂t′(ṡ ′|ṡτ , a)− E[p̂t′(ṡ′|ṡτ , a)] ) wkj(ṡ ′) (25)\n+ ∑\nṡ′∈Skj\n( E[p̂t′(ṡ ′|ṡτ , a)]− ∑\ns′:α(s′)=ṡ′\np(s′|sτ , a) ) wkj(ṡ ′) (26)\n+ ∑\nṡ′∈Skj\n∑\ns′:α(s′)=ṡ′\np(s′|sτ , a)wkj(ṡ′)− wkj(ṡτ ) (27)\nand continue bounding each of these terms individually.\nBounding (24): Using ‖wkj‖∞ = λ+kj/2, (24) is bounded according to (6) as ∑\nṡ′∈Skj\n( p+kj(ṡ ′|ṡτ , a)− p̂t′(ṡ′|ṡτ , a) ) wkj(ṡ ′) ≤ ∥∥p+kj(·|ṡτ , a)− p̂t′(·|ṡτ , a) ∥∥ 1 ‖wkj‖∞\n≤ ǫ̃kjλ + kj 2 + λ+ kj 2\n√ 2Skj log(48SkjAt\n′3/δ) Nt′(s,a) . (28)\nBounding (25): Similarly, by Lemma 1 with probability at least 1 − δ24t′2 we can bound (25) at all time steps τ as\n∑\nṡ′∈Skj\n( p̂t′(ṡ ′|ṡτ , a)− E[p̂t′(ṡ′|ṡτ , a)] ) wkj(ṡ ′) ≤ λ + kj\n2\n√ 2Skj log(48SkjAt\n′3/δ) Nt′ (s,a) . (29)\nBounding (26): By (18) and using that ‖wkj‖∞ = λ+kj/2, we can bound (26) by ∑\nṡ′∈Skj\n( E[p̂t′(ṡ ′|ṡτ , a)]− ∑\ns′:α(s′)=ṡ′\np(s′|sτ , a) ) wkj(ṡ ′) < ǫkjλ + kj\n2 . (30)\nBounding (27): We set w′(s) := wkj(α(s)) for s ∈ S◦ and rewrite (27) as ∑\nṡ′∈Skj\n∑\ns′:α(s′)=ṡ′\np(s′|sτ , a)wkj(ṡ′)− wkj(ṡτ ) = ∑\ns′∈S◦ p(s′|sτ , a)w′(s′)− w′(sτ ).(31)\nSumming this term over all steps τ = t′, . . . , t, we can rewrite the sum as a martingale difference sequence, so that Azuma-Hoeffding’s inequality (e.g., Lemma 10 of [5]) yields that with probability at least 1− δ24t′3\nt∑\nτ=t′\n∑\ns′∈S◦ p(s′|sτ , a)w′(s′)− w′(sτ ) =\nt∑\nτ=t′\n(∑\ns′\np(s′|sτ , a)w′(s′)− w′(sτ+1) )\n+w′(st+1)− w′(st′) ≤ λ+kj √ 2ℓkj log( 24t′3 δ ) + λ + kj , (32)\nsince the sequence Xτ := ∑ s′ p(s ′|sτ , a)w′(s′)−w′(sτ+1) is a martingale difference sequence with |Xt| ≤ λ+kj . Wrap-up. Summing over the steps τ = t′, . . . , t, we get from (23), (27), (28), (29), (30), (31), and (32) that with probability at least 1− δ12t′2 ∑\nṡ,a\nvkj(ṡ, a) ( ρ̂+kj(φkj)− r+kj(ṡ, a) ) ≤ ǫ̃kjλ+kjℓkj\n+λ+kj\n∑\nṡ,a\n√ 2vkj(ṡ, a)Skj log ( 48SkjAt′3 δ ) + λ+kj √ 2ℓkj log ( 24t′2 δ ) + λ+kj , (33)\nusing that vkj(ṡ, a) ≤ Nt′(ṡ, a) and the assumption that ǫkj ≤ ǫ̃kj . Combining (20), (22), and (33) gives the claimed lemma. ⊓⊔\nSumming Lemma 3 over all episodes gives the following lemma, for a detailed proof see Appendix D.\nLemma 4. With probability at least 1 − δ, for all runs j of all episodes k the chosen model φkj passes all tests, provided that ǫ̃kj(φkj) ≥ ǫ(φkj)."
    }, {
      "heading" : "5.3 Preliminaries for the proof of Theorem 3",
      "text" : "We start with some auxiliary results for the proof of Theorem 3. Lemma 5 bounds the bias span of the optimistic policy, Lemma 6 deals with the estimated precision of φkj , and Lemma 7 provides a bound for the number of episodes. For proofs see Appendix E, F, and G.\nLemma 5. Assume that the confidence intervals given in Lemma 1 hold at some step t for all states ṡ ∈ Sφ and all actions a. Then for each φ, the set of plausible MDPs Mt,φ contains an MDP M̃ with diameter D(M̃) upper bounded by the true diameter D, provided that ǫ̃(φ) ≥ ǫ(φ). Consequently, the respective bias span λ(u+t,φ) is bounded by D as well.\nLemma 6. If all chosen models φkj pass all tests in run j of episode k whenever ǫ̃(φkj) ≥ ǫ(φkj), then ǫ̃(φ) ≤ max{ǫ0, 2ǫ(φ)} always holds for all models φ.\nLemma 7. Assume that all chosen models φkj pass all tests in run j of episode k whenever ǫ̃(φkj) ≥ ǫ(φkj). Then the number of episodes KT after any T ≥ SA steps is upper bounded as KT ≤ SA log2 ( 2T SA ) + ∑ φ:ǫ(φ)>ǫ0 log2 ( ǫ(φ) ǫ0 ) ."
    }, {
      "heading" : "5.4 Bounding the regret (Proof of Theorem 3)",
      "text" : "Now we can finally turn to showing the regret bound of Theorem 3. We will assume that all chosen models φkj pass all tests in run j of episode k whenever ǫ̃(φkj) ≥ ǫ(φkj). According to Lemma 4 this holds with probability at least 1−δ.\nLet φkj ∈ Φ be the model that has been chosen at time tkj , and consider the last but one step t of run j in episode k. The regret ∆kj of run j in episode k with respect to ρ∗ := ρ∗(φ◦) is bounded by\n∆kj := (ℓkj + 1)ρ ∗ − ∑t+1 τ=tkj rτ ≤ ℓkj ( ρ∗ − ρkj ) + ρ∗ + ℓkjρkj − ∑t τ=tkj rτ ,\nwhere as before ℓkj := t− tkj + 1 denotes the length of run j in episode k up to the considered step t. By assumption the test (12) on the collected rewards has been passed at step t, so that\n∆kj ≤ ℓkj ( ρ∗ − ρkj ) + ρ∗ + lobkj(t), (34)\nand we continue bounding the terms of lobkj(t).\nBounding the regret with the penalization term. Since we have vkj(ṡ, a) ≤ Ntk1(ṡ, a) for all ṡ ∈ Skj , a ∈ A and also ∑ ṡ,a vkj(ṡ, a) = ℓkj ≤ 2j , by Cauchy-Schwarz inequality ∑\nṡ,a\n√ vkj(ṡ, a) ≤ 2j/2 √ SkjA. Applying this to\nthe definition (10) of lobkj , we obtain from (34) and by the definition (9) of the penalty term that\n∆kj ≤ ℓkj ( ρ∗ − ρkj ) + ρ∗ + 2j/2 ( λ+kj √ 2Skj + 3√ 2 )√ SkjA log ( 48SkjAt3kj δ )\n+ λ+kj √ 2ℓkj log ( 24t2kj δ ) + λ+kj + ǫ̃ ( φkj)ℓkj(λ + kj + 3 )\n≤ ℓkj ( ρ∗ − ρkj ) + ρ∗ + 2jpen(φkj , tkj). (35)\nThe key step. Now, by definition of the algorithm and Lemma 2, for any approximate model φ we have\nρkj − pen(φkj , tkj) ≥ ρ̂+tkj (φ) − pen(φ, tkj) (36)\n≥ ρ∗ − (D + 1)ǫ(φ)− pen(φ, tkj)− 2t−1/2kj ,\nor equivalently ρ∗ − ρkj + pen(φkj , tkj) ≤ pen(φ, tkj) + (D + 1)ǫ(φ) + 2t−1/2kj . Multiplying this inequality with 2j and noting that ℓkj ≤ 2j then gives\nℓkj ( ρ∗ − ρkj ) + 2jpen(φkj , tkj) ≤ 2jpen(φ, tkj) + 2j(D + 1)ǫ(φ) + 2j+1t−1/2kj .\nCombining this with (35), we get by application of Lemma 5, i.e., λ(u+tkj ,φ) ≤ D, and the definition of the penalty term (9) that\n∆kj ≤ ρ∗ + 2j/2 (( D √ 2Sφ +\n3√ 2\n)√ SφA log ( 48SφAt3kj δ ) +D √ 2 log( 24t2kj δ ) )\n+D + 2j ǫ̃(φ) ( D + 3 ) + 2j(D + 1)ǫ(φ) + 2j+1t\n−1/2 kj .\nBy Lemma 6 and using that 2tkj ≥ 2j (so that 2j+1t−1/2kj ≤ 2 √ 2 · 2j/2) we get\n∆kj ≤ ρ∗ + 2j/2 (( D √ 2Sφ +\n3√ 2\n)√ SφA log ( 48SφAt3kj δ ) +D √ 2 log( 24t2kj δ ) )\n+D + 32 · 2 j max{ǫ0, 2ǫ(φ)} ( D + 73 ) + 2 √ 2 · 2j/2. (37)\nSumming over runs and episodes. Let Jk be the total number of runs in episode k, and let KT be the total number of episodes up to time T . Noting that tkj ≤ T and summing (37) over all runs and episodes gives\n∆(φ◦, T ) = KT∑\nk=1\nJk∑\nj=1\n∆kj ≤ ( ρ∗ +D )KT∑\nk=1\nJk + 3 2 max{ǫ0, 2ǫ(φ)} ( D + 73 ) KT∑\nk=1\nJk∑\nj=1\n2j\n+ (( D √ 2Sφ +\n3√ 2\n)√ SφA log ( 48SφAT 3 δ ) +D √ 2 log(24T 2 δ ) + 2 √ 2 )KT∑\nk=1\nJk∑\nj=1\n2j/2.\nAs shown in Section 5.2 of [7], ∑ k Jk ≤ KT log2(2T/KT ), ∑ k ∑ j 2\nj ≤ 2(T + KT ) and ∑ k ∑ j 2 j/2 ≤ √ 2KT log2(2T/KT )(T +KT ), and we may conclude the proof applying Lemma 7 and some minor simplifications. ⊓⊔\nAcknowledgments. This research was funded by the Austrian Science Fund (FWF): P 26219-N15, the European Community’s FP7 Program under grant agreements n◦ 270327 (CompLACS) and 306638 (SUPREL), the Technion, the Ministry of Higher Education and Research of France, Nord-Pas-de-Calais Regional Council, and FEDER (Contrat de Projets Etat Region CPER 2007-2013)."
    }, {
      "heading" : "1. Bartlett, P.L., Tewari, A.: REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In: UAI 2009, Proc. 25th Conf. on Uncertainty in Artificial Intelligence, pp. 25–42. AUAI Press (2009)",
      "text" : ""
    }, {
      "heading" : "2. Hallak, A., Castro, D.D., Mannor, S.: Model selection in Markovian processes. In: 19th ACM SIGKDD Int’l Conf. on Knowledge Discovery and Data Mining, KDD",
      "text" : "2013, pp. 374–382. ACM (2013) 3. Hutter, M.: Feature Reinforcement Learning: Part I: Unstructured MDPs. J. Artificial General Intelligence 1, 3–24 (2009) 4. Littman, M, Sutton, R., Singh S.: Predictive representations of state. In: Adv. Neural Inf. Process. Syst. 15, pp. 1555–1561 (2002) 5. Jaksch, T., Ortner, R., Auer, P.: Near-optimal regret bounds for reinforcement learning. J. Mach. Learn. Res. 11, 1563–1600 (2010) 6. Maillard, O.A., Munos, R., Ryabko, D.: Selecting the state-representation in reinforcement learning. In: Adv. Neural Inf. Process. Syst. 24, pp. 2627–2635 (2012) 7. Maillard, O.A., Nguyen, P., Ortner, R., Ryabko, D.: Optimal regret bounds for selecting the state representation in reinforcement learning. In: Proc. 30th Int’l Conf. on Machine Learning, ICML 2013, JMLR Proc., vol. 28, pp. 543–551 (2013) 8. Nguyen, P., Maillard, O.A., Ryabko, D., Ortner, R.: Competing with an infinite set of models in reinforcement learning. In: Proc. 16th Int’l Conf. on Artificial Intelligence and Statistics, AISTATS 2013, JMLR Proc., vol. 31, pp. 463–471 (2013) 9. Ortner, R.: Pseudometrics for state aggregation in average reward Markov decision processes. In: ALT 2007. LNCS (LNAI), vol. 4754, pp. 373–387. Springer (2007) 10. Ortner, R., Ryabko, D.: Online Regret Bounds for Undiscounted Continuous Reinforcement Learning, In: Adv. Neural Inf. Process. Syst. 25, pp. 1772–1780 (2012)"
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "We start with an error bound for approximation, assuming we compare two MDPs over the same state space.\nLemma 8. Consider a communicating MDP M = (S,A, r, p), and another MDP M̄ = (S,A, r̄, p̄) over the same state-action space which is an ǫapproximation of M (for α = id). Assume that an optimal policy π∗ of M is performed on M̄ for ℓ steps, and let v̄∗(s) be the number of times state s is visited state among these ℓ steps. Then\nℓρ∗(M)−∑s∈S v̄(s) · r̄(s, π∗(s)) < ℓǫ(D + 1) +D √ 2ℓ log(1/δ) +D\nwith probability at least 1− δ, where D = D(M) is the diameter of M .\nProof. We abbreviate r∗(s) := r(s, π∗(s)) and p∗(s′|s) := p(s′|s, π∗(s)), and use r̄∗(s) and p̄∗(s′|s) accordingly. Then\nℓρ∗(M)− ∑\ns∈S v̄∗(s) · r̄∗(s) =\n∑\ns\nv̄∗(s) ( ρ∗(M)− r̄∗(s) )\n= ∑\ns\nv̄∗(s) ( ρ∗(M)− r∗(s) ) + ∑\ns\nv̄∗(s) ( r∗(s)− r̄∗(s) ) . (38)\nNow, for the first term in (38) we can use the Poisson equation (for the optimal policy π∗ on M) to replace ρ∗(M)− r∗(s) = ∑s′ p∗(s′|s) ·λ∗(s′)−λ∗(s), writing λ∗ := λπ∗ for the bias of π\n∗ on M . Concerning the second term in (38) we can use (4) and the fact that ∑ s v̄ ∗(s) = ℓ. In summary, we get\nℓρ∗(M)− ∑\ns∈S v̄∗(s) · r̄∗(s) <\n∑\ns\nv̄∗(s) (∑\ns′\np∗(s′|s)λ∗(s′)− λ∗(s) ) + ℓǫ\n= ℓǫ+ ∑\ns\nv̄∗(s) (∑\ns′\np̄∗(s′|s)λ∗(s′)− λ∗(s) )\n+ ∑\ns\nv̄∗(s) (∑\ns′\np∗(s′|s)λ∗(s′)− ∑\ns′\np̄∗(s′|s)λ∗(s′) ) . (39)\nBy (4) and using that span(λ∗) ≤ D, the last term in (39) is bounded as ∑\ns\nv̄∗(s) ∑\ns′\n( p∗(s′|s, a)− p̄∗(s′|s, a) ) λ∗(s′) < ℓǫD. (40)\nOn the other hand, for the second term in (39), writing sτ for the state visited at time step τ we have\n∑\ns\nv̄∗(s) (∑\ns′\np̄∗(s′|s)λ∗(s′)− λ∗(s) ) = ℓ∑\nτ=1\n(∑\ns′\np̄∗(s′|sτ )λ∗(s′)− λ∗(sτ ) )\n= ℓ∑\nτ=1\n(∑\ns′\np̄∗(s′|sτ )λ∗(s′)− λ∗(sτ+1) ) + λ∗(sℓ+1)− λ∗(s1). (41)\nNow λ∗(sℓ+1)− λ∗(s1) ≤ span(λ∗) ≤ D, while the sequence\nXτ := ∑ s′ p̄ ∗(s′|sτ )λ∗(s′)− λ∗(sτ+1)\nis a martingale difference sequence with |Xt| ≤ D. Thus, an application of Azuma-Hoeffding’s inequality (e.g., Lemma 10 of [5]) to (41) yields that\n∑\ns\nv̄∗(s) (∑\ns′\np̄∗(s′|s)λ∗(s′)− λ∗(s) ) ≤ D √ 2ℓ log(1/δ) +D (42)\nwith probability higher than 1 − δ. Summarizing, (39), (40), and (42) give the claimed\nℓρ∗(M)− ∑\ns∈S\n∑ a∈A v̄(s)∗ · r̄∗(s) < ℓǫ(D + 1) +D √ 2ℓ log(1/δ) +D.\n⊓⊔ As a corollary to Lemma 8 we can also bound the approximation error in average reward, which we will need below to deal with the error of ǫ-approximate models.\nLemma 9. Let M be a communicating MDP with optimal policy π∗, and M̄ an ǫ-approximation of M over the same state space. Then\n∣∣ρ∗(M)− ρ∗(M̄) ∣∣ ≤ ∣∣ρ∗(M)− ρ(M̄, π∗) ∣∣ ≤ ǫ (D(M) + 1).\nProof. Divide the result of Lemma 8 by ℓ, choose δ = 1/ℓ, and let ℓ → ∞. Since the average reward of a policy is no random value, the result holds surely and not just with probability 1. ⊓⊔\nA.1 Proof of Theorem 1\nThe idea is to define a new MDP M ′ = (S,A, r′, p′) on S whose rewards r′ and transition probabilities p′ are ǫ-close to M and that has the same optimal average reward as M̄ . Thus, for each state s ∈ S and each action a we set r′(s, a) := r̄(α(s), a) and\np′(s′|s, a) := p(s ′|s, a) · p̄(α(s′)|α(s), a)∑ s′′:α(s′′)=α(s′) p(s ′′|s, a) .\nNote that p′(·|s, a) is indeed a probability distribution over S, that is, in particular it holds that\n∑ s′∈S p′(s′|s, a) = ∑ s′∈S p(s′|s, a)∑ s′′:α(s′′)=α(s′) p(s ′′|s, a) · p̄(α(s ′)|α(s), a)\n= ∑\nṡ′∈S̄\n∑\ns′:α(s′)=ṡ′\np(s′|s, a)∑ s′′:α(s′′)=ṡ′ p(s ′′|s, a) · p̄(ṡ ′|α(s), a)\n= ∑\nṡ′∈S̄\np̄(ṡ′|α(s), a) = 1.\nNow by definition, the rewards r′(s, a) = r̄(α(s), a) and aggregated transition probabilities\n∑\ns′:α(s′)=ṡ′\np′(s′|s, a) = ∑\ns′:α(s′)=ṡ′\np(s′|s, a)∑ s′′:α(s′′)=ṡ′ p(s ′′|s, a) · p̄(ṡ ′|α(s), a) = p̄(ṡ′|α(s), a)\nin M ′ have the same values for all states s that are mapped to the same metastate by α. It follows that ρ∗(M ′) = ρ∗(M̄).\nFurther by assumption, according to (4) we have\n|r(s, a) − r′(s, a)| = |r(s, a)− r̄(α(s), a)| < ǫ\nand\n∑\ns′∈S\n∣∣p(s′|s, a)− p′(s′|s, a) ∣∣ = ∑\ns′∈S p(s′|s, a)\n∣∣∣∣1− p̄(α(s′)|α(s), a)∑\ns′′:α(s′′)=α(s′) p(s ′′|s, a)\n∣∣∣∣\n= ∑\nṡ′∈S̄\n∑\ns′:α(s′)=ṡ′\np(s′|s, a) · ∣∣∣∣ ∑ s′′:α(s′′)=ṡ′ p(s ′′|s, a)− p̄(ṡ′|α(s), a) ∑\ns′′:α(s′′)=ṡ′ p(s ′′|s, a)\n∣∣∣∣\n= ∑\nṡ′∈S̄\n∣∣∣ ∑\ns′′:α(s′′)=ṡ′\np(s′′|s, a)− p̄(ṡ′|α(s), a) ∣∣∣ < ǫ.\nThus, M ′ is an ǫ-approximation of M that has the same optimal average reward as M̄ so that application of Lemma 9 to M and M ′ gives the claimed result. ⊓⊔"
    }, {
      "heading" : "B Proof of Lemma 1",
      "text" : "For any given number of observations n it holds that (cf. Appendix C.1 of [5]) for any θ > 0\nP { r̂(ṡ, a)− E[r̂(ṡ, a)] ≤ √ log(2/θ)\nn\n} < θ,\nP {∥∥∥p̂(·|ṡ, a)− E[p̂(·|ṡ, a)] ∥∥∥ 1 ≤ √ 2Sφ log(2/θ) n } < θ.\nChoosing suitable values for θ, a union bound over all states ṡ, all actions a and all possible values for Nt(ṡ, a) = 1, . . . , t shows the lemma. ⊓⊔"
    }, {
      "heading" : "C Proof of Lemma 2",
      "text" : "Let M̄ be the MDP on Sφ whose rewards and transition probabilities are given by the expectation values E[r̂(ṡ, a)] and E[p̂(ṡ′|ṡ, a)], respectively. We have already seen in (14) and (18) that M̄ is an ǫ-approximation of the true MDP M , so that by Theorem 1 ∣∣ρ∗(M)− ρ∗(M̄) ∣∣ ≤ ǫ(D(M) + 1). (43)\nIt remains to deal with the difference between ρ∗(M̄) and ρ̂+t (φ). By assumption, the confidence intervals of Lemma 1 hold for all state-action-pairs so that M̄ is contained in the set of plausible MDPs Mt,φ (defined via the empirical rewards and transition probabilities r̂(ṡ, a) and p̂(ṡ′|ṡ, a)). It follows together with (8) that\nρ̂+t (φ) ≥ ρ∗(M+t (φ)) − 2√t ≥ ρ ∗(M̄)− 2√ t , (44)\nwhich together with (43) proves the claimed inequality. ⊓⊔"
    }, {
      "heading" : "D Proof of Lemma 4",
      "text" : "By Lemma 3, at each step t of a run j in an episode k starting at step tkj = t ′ the test is passed with probability at least 1− δ6t′2 . Assuming that t′′ is the last step in that run and setting ℓ := t′′ − t′ + 1 to be the total number of steps in that run, the test is passed in all steps of the run with error probability bounded by (using that 2t′ ≥ 2j ≥ ℓ)\nℓ · δ 6t′2 ≤ ℓδ 2t′(t′ + ℓ) = δ 2t′ − δ 2(t′ + ℓ) =\n∫ t′′\nt′\nδ\n2τ2 dτ ≤\nt′′∑\nτ=t′\nδ\n2τ2 .\nSumming over all episodes and runs shows that the test is passed in all time steps with probability at least 1− ∑∞ τ=1 δ 2τ2 ≥ 1− δ. ⊓⊔"
    }, {
      "heading" : "E Proof of Lemma 5",
      "text" : "We define an MDP M̃ on state space Sφ as follows. First let β : Sφ → S be an arbitrary mapping that maps states in Sφ to some state in S such that α(β(ṡ)) = ṡ. Intuitively, β(ṡ) is an arbitrary reference state that is mapped to ṡ by α. Then for ṡ, ṡ′ in Sφ we set the transition probabilities of M̃ as\np̃(ṡ′|ṡ, a) := ∑\ns′:α(s′)=ṡ′\np(s′|β(ṡ), a). (45)\nThen by (18) and Lemma 1 we obtain\n∥∥p̃(·|ṡ, a)− p̂t(·|ṡ, a) ∥∥ 1 = ∑\nṡ′\n∣∣∣ ∑\ns′:α(s′)=ṡ′\np(s′|β(ṡ), a)− p̂t(·|ṡ, a) ∣∣∣\n≤ ∑\nṡ′\n(∣∣∣ ∑\ns′:α(s′)=ṡ′\np(s′|β(ṡ), a)− E[p̂t(ṡ′|ṡ, a)] ∣∣∣+ ∣∣∣E[p̂t(ṡ′|ṡ, a)]− p̂t(ṡ′|ṡ, a) ∣∣∣ )\n≤ ǫ(φ) + √ 2Sφ log(48SφAt 3/δ)\nNt(s,a) ,\nshowing that M̃ is contained in Mt,φ. To see that D(M̃) ≤ D, note that β maps all transitions in M̃ to transitions of the the same or lower probability in the\ntrue MDP. That is, for any ṡ, ṡ′ ∈ Sφ it holds that p̃(ṡ′|ṡ, a) ≥ p(β(ṡ′)|β(ṡ), a). Thus, each trajectory in M̃ can be mapped to a trajectory in the true MDP that cannot have higher probability, which proves the first claim of the lemma. The second claim follows immediately along the lines of Section 4.3.1 in [5]. ⊓⊔"
    }, {
      "heading" : "F Proof of Lemma 6",
      "text" : "By definition of the algorithm, ǫ̃(φ) for each model φ has initial value ǫ0 and is doubled whenever φ fails a test. Thus, by assumption if ǫ0 ≤ ǫ(φ), then as soon as ǫ(φ) ≤ ǫ̃(φ) < 2ǫ(φ) the value of ǫ̃(φ) will not change anymore, and consequently ǫ̃(φ) < 2ǫ(φ) always holds.\nOn the other hand, if ǫ0 > ǫ(φ) then also ǫ̃(φ) ≥ ǫ(φ) for the initial value ǫ̃(φ) = ǫ0 and again by assumption ǫ̃(φ) = ǫ0 remains unchanged, so that ǫ̃(φ) ≤ ǫ0 holds. ⊓⊔"
    }, {
      "heading" : "G Proof of Lemma 7",
      "text" : "First recall that an episode is terminated when either the number of visits in some state-action pair (s, a) has been doubled (line 12 of the algorithm) or when the test on the accumulated rewards has failed (line 9). By assumption, the test is passed provided that ǫ̃(φ) ≥ ǫ(φ). If ǫ(φ) ≤ ǫ0, then ǫ̃(φ) ≥ ǫ(φ) holds trivially. Otherwise, φ will fail the test only log2 ( ǫ(φ) ǫ0 ) times until ǫ̃(φ) ≥ ǫ(φ) (after which the test is passed w.h.p. and ǫ̃(φ) remains unchanged by Lemma 3). Therefore, the number of episodes terminated due to failure of the test is upper bounded by ∑\nφ:ǫ(φ)>ǫ0 log2 ( ǫ(φ) ǫ0 ) .\nFor the number of episodes terminated since the number of visits in some state-action pair (s, a) has been doubled, one can show that it is bounded by SA log2 ( 2T SA ) , cf. Appendix C.2 of [5] or Section 5.2 of [7], and the lemma follows.\n⊓⊔"
    }, {
      "heading" : "H Proof of Theorem 4",
      "text" : "As the proof of the regret bound for OMS given in [7] follows the same lines as the proof of Theorem 3 given here, we only sketch the key step leading to the improvement of the bound. Note that by (36) and since average rewards are by assumption in [0, 1], for the model φkj chosen in some run j of some episode k it holds that pen(φkj , tkj) ≤ pen(φ◦, tkj) − 1. Hence, by definition of the penalization term (9) and since 1 ≤ λ(u+t,φ◦) ≤ D, the chosen model φkj always satisfies\n( 2 √ 2Skj +\n3√ 2\n)√ SkjA log ( 48SkjAt3 δ ) + 2 √ 2 log( 24t2kj δ )\n≤ ( 2D √ 2S◦ + 3√\n2\n)√ S◦A log ( 48S◦At3\nδ\n) + 2D √ 2 log(\n24t2kj δ ) + 2 −j/2D.\nSome simplifications then show that Skj is Õ(D 2S◦), so that one can replace the\ntotal number of all states S = ∑\nφ Sφ in Theorem 3 (respectively in the regret\nbound of [7]) by the total number of states of models φ with Sφ = Õ(D 2S◦) and consequently by Õ(|Φ|D2S◦). ⊓⊔"
    } ],
    "references" : [ {
      "title" : "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs",
      "author" : [ "P.L. Bartlett", "A. Tewari" ],
      "venue" : "UAI 2009, Proc. 25th Conf. on Uncertainty in Artificial Intelligence, pp. 25–42. AUAI Press",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Model selection in Markovian processes",
      "author" : [ "A. Hallak", "D.D. Castro", "S. Mannor" ],
      "venue" : "19th ACM SIGKDD Int’l Conf. on Knowledge Discovery and Data Mining, KDD 2013, pp. 374–382. ACM",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Feature Reinforcement Learning: Part I: Unstructured MDPs",
      "author" : [ "M. Hutter" ],
      "venue" : "J. Artificial General Intelligence 1, 3–24",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Predictive representations of state",
      "author" : [ "M Littman", "R. Sutton", "Singh S." ],
      "venue" : "Adv. Neural Inf. Process. Syst. 15, pp. 1555–1561",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "T. Jaksch", "R. Ortner", "P. Auer" ],
      "venue" : "J. Mach. Learn. Res. 11, 1563–1600",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Selecting the state-representation in reinforcement learning",
      "author" : [ "O.A. Maillard", "R. Munos", "D. Ryabko" ],
      "venue" : "Adv. Neural Inf. Process. Syst. 24, pp. 2627–2635",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Optimal regret bounds for selecting the state representation in reinforcement learning",
      "author" : [ "O.A. Maillard", "P. Nguyen", "R. Ortner", "D. Ryabko" ],
      "venue" : "Proc. 30th Int’l Conf. on Machine Learning, ICML 2013, JMLR Proc., vol. 28, pp. 543–551",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Competing with an infinite set of models in reinforcement learning",
      "author" : [ "P. Nguyen", "O.A. Maillard", "D. Ryabko", "R. Ortner" ],
      "venue" : "Proc. 16th Int’l Conf. on Artificial Intelligence and Statistics, AISTATS 2013, JMLR Proc., vol. 31, pp. 463–471",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Pseudometrics for state aggregation in average reward Markov decision processes",
      "author" : [ "R. Ortner" ],
      "venue" : "ALT 2007. LNCS (LNAI), vol. 4754, pp. 373–387. Springer",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning, In: Adv",
      "author" : [ "R. Ortner", "D. Ryabko" ],
      "venue" : "Neural Inf. Process. Syst. 25, pp. 1772–1780",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "We consider a reinforcement learning setting introduced in [6] where the learner does not have explicit access to the states of the underlying Markov decision process (MDP).",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "Inspired by [3], in [6] a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying Markov decision process (MDP).",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 5,
      "context" : "Inspired by [3], in [6] a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying Markov decision process (MDP).",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "The first regret bounds in this setting were derived in [6].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "They recently have been improved in [7] and extended to infinite model sets in [8].",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "They recently have been improved in [7] and extended to infinite model sets in [8].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "Here we extend and improve the results of [7] as follows.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "Second, we improve the bounds of [7] with respect to the dependence on the state space.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6–8].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6–8].",
      "startOffset" : 159,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6–8].",
      "startOffset" : 159,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6–8].",
      "startOffset" : 159,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "Here we only would like to mention the recent work [2] that considers a similar setting, however is mainly interested in the question whether the true model will be identified in the long run, a question we think is subordinate to that of minimizing the regret, which means fast learning of optimal behavior.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "actions, and R = [0, 1] the set of possible rewards.",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "[5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "In the following we assume that rewards are bounded in [0, 1], which implies that span(λ) is upper bounded by D, cf.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "[5, 1].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "[5, 1].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "[5, 1, 6], as ∆(φ◦, T ) := Tρ∗(φ◦)−Tt=1 rt , where rt are the rewards received when following the proposed strategy and ρ∗(φ◦) is the average optimal reward in φ◦, i.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "[5, 1, 6], as ∆(φ◦, T ) := Tρ∗(φ◦)−Tt=1 rt , where rt are the rewards received when following the proposed strategy and ρ∗(φ◦) is the average optimal reward in φ◦, i.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "[5, 1, 6], as ∆(φ◦, T ) := Tρ∗(φ◦)−Tt=1 rt , where rt are the rewards received when following the proposed strategy and ρ∗(φ◦) is the average optimal reward in φ◦, i.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "It generalizes bounds of [9] from ergodic to communicating MDPs.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "This is an improvement over the results in [9], which only showed that the error approaches 1 when the diameter goes to infinity.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "The OAMS algorithm (shown in detail as Algorithm 1) we propose for the setting introduced in Section 1 is a generalization of the OMS algorithm of [7].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "This can be done by extended value iteration (EVI) [5].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "[5])",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Similar to the REGAL algorithm of [1] this shall prefer simpler models (i.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "When the learner knows that Φ contains a Markov model φ◦, the original OMS algorithm of [7] can be employed.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : ", S > D2|Φ|S◦, we can improve on the state space dependence of the regret bound given in [7] as follows.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "The proof (found in Appendix H) is a simple modification of the analysis in [7] that exploits that by (11) the selected models cannot have arbitrarily large state space.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "[5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "This could also give improvements over current regret bounds for continuous reinforcement learning as given in [10].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "[3].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "The proof is divided into three parts and follows the lines of [7], now taking into account the necessary modifications to deal with the approximation error.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : ", Lemma 10 of [5]) yields that with probability at least 1− δ 24t′3",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "2 of [7], ∑ k Jk ≤ KT log2(2T/KT ), ∑ k ∑ j 2 j ≤ 2(T + KT ) and ∑ k ∑ j 2 j/2 ≤ √ 2KT log2(2T/KT )(T +KT ), and we may conclude the proof applying Lemma 7 and some minor simplifications.",
      "startOffset" : 5,
      "endOffset" : 8
    } ],
    "year" : 2014,
    "abstractText" : "We consider a reinforcement learning setting introduced in [6] where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, she has access to several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting, and more importantly generalize to the case where the models given to the learner do not contain a true model resulting in an MDP representation but only approximations of it. We also give improved error bounds for state aggregation.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}