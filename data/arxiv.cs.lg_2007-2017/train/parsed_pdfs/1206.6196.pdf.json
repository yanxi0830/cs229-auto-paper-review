{
  "name" : "1206.6196.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Discrete Elastic Inner Vector Spaces with Application to Time Series and Sequence Mining",
    "authors" : [ "Pierre-Francois Marteau", "Gildas Ménier" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 6.\n61 96\nv1 [\ncs .L\nG ]\n2 7\nJu n\n20 12\nIndex Terms—Vector space, Discrete time series, Sequence mining, Non-uniform sampling, Elastic inner product, Time warping.\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "T IME series analysis in metric spaces has attractedmuch attention over numerous decades and in various domains such as biology, statistics, sociology, networking, signal processing, etc, essentially due to the ubiquitous nature of time series, whether they are symbolic or numeric. Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue. Unfortunately, this kind of elastic distance does not enable direct construction of definite kernels which are useful when addressing regression, classification or clustering of time series. A fortiori, they do not make it possible to directly construct inner products involving some time elasticity, which are basically able to cope with some stretching or some compression along specific dimension. Recently, [10] have shown that it is quite easy to propose inner product with time elasticity capability at least for some restricted time series spaces, basically spaces containing uniformly sampled time series, all of which have the same lengths (in such cases, time series can be embedded easily in Euclidean spaces).\n• P.-F. Marteau is with Université de Bretagne Sud, IRISA (UMR 6074), Campus de Tohannic, 56000 Vannes, France E-mail: pierre-francois.marteau AT univ-ubs.fr\n• N. Bonnel is with Université de Bretagne Sud, IRISA (UMR 6074), Campus de Tohannic, 56000 Vannes, France E-mail: nicolas.bonnel AT univ-ubs.fr\n• G. Ménier is with Université de Bretagne Sud, IRISA (UMR 6074), Campus de Tohannic, 56000 Vannes, France E-mail: gildas.menier AT univ-ubs.fr\nThe aim of this paper is to derive an extension from this preliminary work for the construction of time elastic inner products, to achieve the construction of an elastic inner product structure for a quasi-unrestricted set of sequential data or time series, i.e. sets for which the data is not necessarily uniformly sampled and may have any lengths. Section two of the paper gives the main notations used throughout this paper and presents a recursive construction for inner-like products. It then gives the conditions and the proof of existence of elastic inner products (and elastic vector spaces) defined on a quasi-unrestricted set of time series while explaining what we mean by quasi-unrestricted. The third section succinctly presents some preliminary applications, mainly to highlight some of the features of elastic inner product vector spaces such as orthogonality. The fourth section presents two effective experimentations, the first one relating to time series classification and the second one addressing symbolic sequences classification."
    }, {
      "heading" : "2 ELASTIC INNER PRODUCT VECTOR SPACES",
      "text" : "Starting from the recursive structure of the Dynamic Time Warping (DTW) equation in which the non linear Max operator is replaced by a Sum operator, we propose a quite general and parameterized recursive equation that we call elastic product. Our goal in this section is to derive the conditions for which such elastic product is an inner product that maintains a form of time elasticity."
    }, {
      "heading" : "2.1 Sequence and sequence element",
      "text" : "Definition 2.1. Given a finite sequence A we denote by A(i) the ith element (symbol or sample) of sequence A. We will consider that A(i) ∈ S × T where (S,⊕S ,⊗S) is a vector space that embeds the multidimensional\nspace variables (e.g. S ⊂ Rd, with d ∈ N+) and T ⊂ R embeds the timestamps variable, so that we can write A(i) = (a(i), ta(i)) where a(i) ∈ S and ta(i) ∈ T , with the condition that ta(i) > ta(j) whenever i > j (timestamps strictly increase in the sequence of samples). Aji with i ≤ j is the subsequence consisting of the ith through the jth element (inclusive) of A. So A j i = A(i)A(i+1)...A(j). Λ denotes the null element. By convention Aji with i > j is the null time series, e.g. Ω."
    }, {
      "heading" : "2.2 Sequence set",
      "text" : "Definition 2.2. The set of all finite discrete time series is thus embedded in a spacetime characterized by a single discrete temporal dimension, that encodes the timestamps, and any number of spatial dimensions that encode the value of the time series at a given timestamp. We note U = {Ap1|p ∈ N} the set of all finite discrete time series. Ap1 is a time series with discrete index varying between 1 and p. We note Ω the empty sequence (with null length) and by convention A01 = Ω so that Ω is a member of set U. |A| denotes the length of the sequence A. Let Up = {A ∈ U | |A| ≤ p} be the set of sequences whose length is shorter or equal to p. Finally let U∗ be the set of discrete time series defined on (S − {0S})× T , i.e. the set of time series that do not contain the null spatial value. We denote by 0S the null value in S."
    }, {
      "heading" : "2.3 Scalar multiplication on U∗",
      "text" : "Definition 2.3. For all A ∈ U∗ and all λ ∈ R, C = λ⊗A ∈ U∗ is such that for all i ∈ N such that 0 ≤ i ≤ |A|, C(i) = (λ.a(i), ta(i)) and thus |C| = |A|."
    }, {
      "heading" : "2.4 Addition on U∗",
      "text" : "Definition 2.4. For all (A,B) ∈ (U∗)2, the addition of A and B, noted C = A⊕B ∈ U∗, is defined in a constructive manner as follows: let i, j and k be in N.\nk = i = j = 1, As long as 1 ≤ i ≤ |A| and 1 ≤ j ≤ |B|, if ta(i) < tb(j), C(k) = (a(i), ta(i)) and i ← i + 1, k ← k + 1 else if ta(i) > tb(j), C(k) = (b(j), tb(j)) and j ← j + 1, k ← k + 1 else if a(i) + b(j) 6= 0, C(k) = (a(i) + b(j), ta(i)) and i ← i+ 1, j ← j + 1, k ← k + 1 else i ← i+ 1, j ← j + 1\nThree comments need to be made at this level to clarify the semantic of the operator ⊕: i) Note that the ⊕ addition of two time series of equal\nlengths and uniformly sampled coincides with the classical addition in vector spaces. Fig. 1 gives an\nexample of the addition of two time series that are not uniformly sampled and that have different lengths, except that zero values are discarded to ensure that the sum of two time series will remain a member of U∗. ii) Implicitly (in light of the last case described in Def. 2.4), any sequence element of the sort (0S , t), where 0S is the null value in S and t ∈ T must be assimilated to the null sequence element Λ. For instance, the addition of A = (1, 1)(1, 2)with B = (−1, 1)(1, 2) is C = A ⊕ B = (2, 2): the addition of the two first sequence elements is (0, 1) that is assimilated to Λ and as such suppressed in C.\niii) The ⊕ operator, when restricted to the set U∗ is reversible in that if C = A⊕B then A = C⊕((−1)⊗B) or B = C ⊕ ((−1) ⊗ A). This is not the case if we consider the entire set U.\n2.5 Elastic product (ep)\nDefinition 2.5. A function < ., . >: U∗ × U∗ → R is called an Elastic Product if, there exists a function f : S2 → R, a strictly positive function g : T 2 → R+ and three constants α, β and ξ in R such that, for any pair of sequences Ap1, B q 1 , the following recursive equation holds:\n< Ap1, B q 1 >ep=\n∑\n\n\n α· < Ap−11 , Bq1 >ep β· < Ap−11 , Bq−11 >ep +f(a(p), b(q)) · g(ta(p), tb(q)) α· < Ap1, Bq−11 >ep\n(1)\nThis recursive definition requires defining an initialization. To that end we set, ∀A ∈ U∗, < A,Ω >ep=< Ω, A >ep=< Ω,Ω >ep= ξ, where ξ is a real constant (typically we set ξ = 0), and Ω is the null sequence, with the convention that Ai\nj = Ω whenever i > j.\nThis paper addresses the most interesting question of the existence of elastic inner products on the set U∗, i.e. without any restriction on the lengths of the considered time series nor the way they are sampled. If the choice of functions f and g, although constrained, is potentially large, we show hereinafter that the choice for constants α, β and ξ is unique.\n2.6 Existence of elastic inner products(eip) defined on U∗\nTheorem 2.1. < ., . >ep is an inner product on (U ∗,⊕,⊗) iff:\ni) ξ = 0. ii) g : (T × T ) → R is symmetric and strictly positive, iii) f is an inner product on (S,⊕S ,⊗S), if we extend the\ndomain of f on S while setting f(0S, 0S) = 0. iv) α = 1 and β = −1,"
    }, {
      "heading" : "2.6.1 proof of theorem 2.1",
      "text" : "Proof of the direct implication Let us suppose first that < ., . >ep is an inner product defined on U∗. Then since < ., . >ep is positive-definite necessarily < Ω,Ω >ep= ξ = 0. Furthermore, for any A1 = (a, t1) and A2 = (a, t2) ∈ U∗ with a 6= 0S , < A1, A2 >ep= g(t1, t2).f(a, a) 6= 0. Since < .; . >ep is symmetric, we get that g(t1, t2) = g(t2, t1) for any (t1, t2) ∈ T 2 which establishes that g is symmetric. Since g is strictly positive by definition of < ., . >ep, i) and ii) are satisfied.\nFor any A = (a, ta) ∈ U∗, < A,A >ep= f(a, a)g(ta, ta) > 0. Since g is strictly positive, then we get that f(a, a) > 0. If we set f(0S, 0S) = 0, we establish that f is positive-definite on S.\nSince ξ = 0, for any A, B, and C ∈ U∗ such that A = (a, t), B(b, t) and C = (c, tc), we have: < A⊕B,C >ep= f(a⊕S b, c).g(t, tc). As < A⊕B,C >ep=< A,C >ep + < B,C >ep = f(a, c).g(t, tc) + f(b, c).g(t, tc) = (f(a, c) + f(b, c)).g(t, tc), As g is strictly positive, we get that f(a ⊕S b, c) = (f(a, c) + f(b, c)). Furthermore, < λ⊗A,C >ep= f(λ⊗S a, c).g(t, tc). As < λ⊗A,C >ep= λ. < A,C >ep= λ.f(a, c).g(t, tc) and g is strictly positive, we get that f(λ⊗S a, c) = λ.f(a, c).\nThis shows that f is linear, symmetric and positivedefinite. Hence it is an inner product on (S,⊕S ,⊗S) and iii) is satisfied.\nLet us show that necessarily α = 1 and β = −1. To that end, let us consider any Ap1, B q 1 and C r 1 in U\n∗, such that p > 1, q > 1, r > 1 and such that ta(p) < tb(q), i.e. if Xs1 = A p 1 ⊕Bq1 , then Xs−11 = Ap1 ⊕Bq−11 . Since by hypothesis < ., . >ep is an inner product (U∗,⊕,⊗), it is linear and thus we can write: < Ap1 ⊕Bq1 , Cr1 >ep=< Ap1, Cr1 >ep + < Bq1 , Cr1 >ep.\nDecomposing < Ap1 ⊕Bq1 , Cr1 >ep, we obtain: < Ap1 ⊕Bq1 , Cr1 >ep= α. < Ap1 ⊕Bq−11 , Cr1 >ep + β. < Ap1 ⊕ Bq−11 , Cr−11 >ep +f(b(q), c(r)).g(tb(q), tc(r)) + α. < Ap1 ⊕Bq1 , Cr−11 >ep As < ., . >ep is linear we get: < Ap1⊕Bq1 , Cr1 >ep= α. < Ap1, Cr1 >ep +α. < Bq−11 , Cr1 >ep + β. < Ap1, C r−1 1 >ep +β. < B q−1 1 , C r−1 1 >ep +f(b(q), c(r)).g(tb(q), tc(r))+ α. < Ap1, C r−1 1 >ep +α. < B q 1 , C r−1 1 >ep Hence, < Ap1⊕Bq1 , Cr1 >ep= α. < Ap1, Cr1 >ep +β. < Ap1, Cr−11 >ep + α. < Ap1, C r−1 1 >ep + < B q 1 , C r 1 >ep\nIf we decompose < Ap1, C r 1 >ep, we get:\n< Ap1⊕Bq1 , Cr1 >ep= (α2+β+α) < Ap1, Cr−11 >ep +α.β. < Ap−11 , C r−1 1 >ep +α.f(a(p), c(r)).g(ta(p), tc(r)) + α 2. < Ap−11 , C r 1 >ep + < B q 1 , C r 1 >ep\nThus we have to identify < Ap1, C r 1 >ep=\nα. < Ap1, C r−1 1 >ep +β. < A p−1 1 , C r−1 1 >ep +f(a(p), c(r)).g(ta(p), tc(r)) + α. < A p−1 1 , C r 1 >ep with (α2+β+α) < Ap1, C r−1 1 >ep +α.β. < A p−1 1 , C r−1 1 >ep +α.f(a(p), c(r)).g(ta(p), tc(r)) + α 2. < Ap−11 , C r 1 >ep.\nThe unique solution is α = 1 and β = −1. That is if < ., . >ep is an existing inner product, then necessarily α = 1 and β = −1, establishing iv).\nProof of the converse implication Let us suppose that i), ii), iii) and iv) are satisfied and show that < ., . >ep is an inner product on U ∗.\nFirst, by construction, since f and g are symmetric, so is < ., . >ep.\nIt is easy to show by induction that < ., . >ep is nondecreasing with the length of its arguments, namely, ∀Ap1 and Bq1 in U∗, < Ap1, B q 1 >ep − < Ap1, Bq−11 >ep≥ 0. Let n = p + q. The proposition is true at rank n = 0. It is also true if Ap1 = Ω, whatever B q 1 is, or B q 1 = Ω, whatever < A p 1 is. Suppose it is true at a rank n ≥ 0, and consider Ap1 6= Ω and Bq1 6= Ω such that p+ q = n. By decomposing < Ap1, B q 1 >ep we get: < Ap1, B q 1 >ep − < Ap1, Bq−11 >ep= − < Ap−11 , Bq−11 >ep +f(a(p), b(q)).g(ta(p), tb(q))+ < A p−1 1 , B q 1 >ep Since f(a(p), b(q)).g(ta(p), tb(q)) > 0 and the proposition is true by inductive hypothesis at rank n, we get that < Ap1, B q 1 >ep − < Ap1, Bq−11 >ep) > 0. By induction the proposition is proved.\nLet us show by induction on the length of the time series the positive definiteness of < ., . >ep. At rank 0 we have < Ω,Ω >ep= ξ = 0. At rank 1, let us consider any time series of length 1, A11. < A11, A 1 1 >ep= f(a(1), a(1)).g(ta(1), ta(1)) > 0 by hypothesis on f and g. Let us suppose that the proposition is true at rank n > 1 and let consider any time series of length n+ 1, An+11 . Then, since α = 1 and β = −1, we get < An+11 , A n+1 1 >ep= 2. < A n+1 1 , A n 1 >ep − < An1 , An1 >ep + f(a(n+ 1), a(n+ 1)).g(ta(n+1), ta(n+1)). Since < An+11 , A n 1 >ep − < An1 , An1 >ep≥ 0, and f(a(n + 1), a(n + 1)).g(ta(n+1), ta(n+1)) > 0, < An+11 , A n+1 1 >ep> 0, showing that the proposition is true at rank n + 1. By induction, the proposition is proved, which establishes the positive-definiteness of < ., . >ep since < A p 1, A p 1 >ep= 0 only if A p 1 = Ω.\nLet us consider any λ ∈ R, and any Ap1, Bq1 in U∗ and show by induction on n = p+ q that< λ⊗ Ap1, Bq1 >ep=\nλ. < Ap1, B q 1 >ep: The proposition is true at rank n = 0. Let us suppose that the proposition is true at rank n ≥ 0, i.e. for all r ≤ n, and consider any pair Ap1, Bq1 of time series such that p+ q = n+ 1. We have: < λ⊗Ap1, Bq1 >ep= α. < λ⊗Ap1, Bq−11 >ep +β. < λ⊗Ap−11 , Bq−11 >ep +f(λ⊗Sa(p), b(q)).g(ta(p), tb(q))+α. < λ⊗Ap−11 , Bq1 >ep Since f is linear on (S,⊕S ,⊗S), and since the proposition is true by hypothesis at rank n, we get that < λ ⊗ Ap1, Bq1 >ep= λ.α < Ap1, Bq−11 >ep +λ.β. < Ap−11 , B q−1 1 >ep +λ.f(a(p), b(q)).g(ta(p), tb(q)) + λ.α. < Ap−11 , B q 1 >ep= λ. < A p 1, B q 1 >ep. By induction, the proposition is true for any n, and we have proved this proposition. Furthermore, for any Ap1, B q 1 and C r 1 in U\n∗, let us show by induction on n = p+ q+ r that < Ap1 ⊕Bq1 , Cr1 >ep=< Ap1, C r 1 >ep + < B q 1 , C r 1 >ep. Let X s 1 be equal to A p 1 ⊕Bq1 . The proposition is obviously true at rank n = 0. Let us suppose that it is true up to rank n ≥ 0, and consider any Ap1, B q 1 and C r 1 such that p+ q + r = n+ 1.\nThree cases need then to be considered:\n1) if Xs−11 = A p−1 1 ⊕Bq−11 , then ta(p) = tb(q) = t and\n< Ap1 ⊕B q 1 , C r 1 >ep= α. < A p 1 ⊕B q 1 , C r−1 1 >ep +\nβ. < Ap−11 ⊕Bq−11 , Cr−11 >ep + f((a(p) + b(q)), c(r)).g(t, tc(r))+\nα. < Ap−11 ⊕Bq−11 , Cr1 >ep. Since f is linear on (S,⊕S ,⊗S), and the proposition true at rank n, we get the result.\n2) if Xs−11 = A p 1 ⊕Bq−11 , then ta(p) < tb(q) = t and\n< Ap1 ⊕ Bq1 , Cr1 >ep= α. < Ap1 ⊕ Bq1 , Cr−11 >ep +β. < Ap1 ⊕ Bq−11 , Cr−11 >ep +f(b(q), c(r)).g(t, tc(r)) + α. < Ap1 ⊕ Bq−11 , Cr1 >ep. Having α = 1 and β = −1 with the proposition supposed to be true at rank n we get the result. 3) if Xs−11 = A p−1 1 ⊕Bq−11 , we proceed similarly to case\n2).\nThus the proposition is true at rank n+ 1, and by induction the proposition is true for all n. This establishes the linearity of < ., . >ep. This ends the proof of the converse implication and theorem 2.1 is therefore established . The existence of functions f and g entering into the definition of < ., . >ep and satisfying the conditions allowing for the construction of an inner product on (U∗,⊕,⊗) is ensured by the following proposition: Proposition 2.2. The functions f : S2 → R defined as f(a, b) =< a, b >S where < ., . >S is an inner product on (S,⊕S ,⊗S) and g : T 2 → R defined as f(ta, tb) = e−ν·d(ta,tb), where d is a distance defined on T 2 and ν ∈ R+, satisfy the conditions required to construct an elastic inner product on (U∗,⊕,⊗). The proof of Prop.2.2 is obvious. This proposition establishes the existence of ep inner products, that we will denote eip (Time Elastic Inner Product). An eip as\nthus the following structure:\n< Ap1, B q 1 >eip=\n∑\n\n  \n  \n< Ap−11 , B q 1 >eip − < Ap−11 , Bq−11 >eip + g(ta(p), tb(q))· < (a(p), b(q) >eip(S) < Ap1, B q−1 1 >eip\n(2)\nWith the initialization: ∀A ∈ U∗, < A,Ω >eip=< Ω, A >eip=< Ω,Ω >eip= ξ, where ξ is a real constant (typically we set ξ = 0), and Ω is the null sequence, with the convention that Ai j = Ω whenever i > j.\nNote that < ., . >S can be chosen to be a eip as well, in the case where a second time elastic dimension is required. This leads naturally to recursive definitions for ep and eip.\nProposition 2.3. For any n ∈ N, and any discrete subset T = {t1, t2, · · · , tn} ⊂ R, let Un,R,T be the set of all time series defined on R× T whose lengths are n (the time series in Un,R,T are considered to be uniformly sampled). Then, the eip on Un,R constructed from the functions f and g defined in Prop. 2.2 tends towards the Euclidean inner product when ν → ∞ if S is an Euclidean space and < a, b >S is the Euclidean inner product defined on S.\nThe proof of Prop.2.3 is straightforward and is omitted. This proposition shows that eip generalizes the classical Euclidean inner product."
    }, {
      "heading" : "2.7 Algorithmic complexity",
      "text" : "The general complexity associated to the calculation of the eip of two time series of lengths p and q as specified by Eq.2 is O(p · q). Basically this is the same complexity as that required for the calculation of the complete dynamic programming solutions such as the Dynamic Time Warping (DTW) [20] [13] algorithm evaluated on these two time series. Nevertheless, as discussed in section 3.3, Prop. 3.2 allows for efficient implementations of retrieval process (in linear time complexity) once a straightforward indexing phase has been implemented. This result is demonstrated in practice the experimentation section (sec. 4)."
    }, {
      "heading" : "3 SOME PRELIMINARY APPLICATIONS",
      "text" : "We present in the following sections some applications to highlight the properties of Elastic Inner Product Vector Spaces (EIPV S).\n3.1 Distance in EIPV S\nThe following proposition provides U∗ with a norm and a distance, both induced by a eip.\nProposition 3.1. For all Ap1 ∈ U∗, and any < ., . >eip defined on (U∗,⊕,⊗) √\n< Ap1, A p 1 >eip is a norm on U ∗.\nFor all pair (Ap1, B q 1) ∈ (U∗)2, and any eip defined on (U∗,⊕,⊗), δeip(A p 1, B q 1) = √\n< Ap1 ⊕ (−1.⊗Bq1), Ap1 ⊕ (−1.⊗Bq1) >eip = √\n< Ap1, A p 1 >eip + < B q 1 , B q 1 >eip −2· < Ap1, Bq1 >eip\ndefines a distance metric on U∗.\nThe proof of Prop. 3.1 is straightforward and is omitted.\n3.2 Orthogonalization in EIPV S\nTo exemplify the effect of elasticity in EIPV S, we give below the result of the Gram-Schmidt orthogonalization algorithm for two families of independent univariate time series. The first family is composed of uniformly sampled time series having increasing lengths. The second family (a sine-cosine basis) is composed of uniformly sampled time series, all of which have the same length.\nThe tests which are described in the next sections were performed on a set U∗ of discrete time series whose elements are defined on (R − {0} × [0; 1])2 using the following eip:\n< Ap1, B q 1 >eip=\n∑\n\n  \n  \n< Ap1, B q−1 1 >eip − < Ap−11 , Bq−11 >eip + a(p)b(q) · e−ν.|ta(p)−tb(q)|2 < Ap−11 , B q 1 >eip\n(3)"
    }, {
      "heading" : "3.2.1 Orthogonalization of an independent family of time series with increasing lengths",
      "text" : "The family of time series we are considering is composed of 11 time series uniformly sampled, whose lengths are 11 samples:\n(1, 0) (ǫ, 0)(1, 1/10) (ǫ, 0)(ǫ, 0)(1, 1/10) · · · (ǫ, 0)(ǫ, 1/10)(ǫ, 2/10) · · · (1, 1)\n(4)\nSince, the zero value cannot be used for the space dimension, we replaced it by ǫ, which is the smallest non zero positive real for our test machine (i.e. 2−1074). The result of the Gram-Schmidt orthogonalization process using ν = .01 on this basis is given in Fig.2."
    }, {
      "heading" : "3.2.2 Orthogonalization of a sine-cosine basis",
      "text" : "An orthonormal family of discrete sine-cosine functions is not anymore orthogonal in a EIPV S. The result of the Gram-Schmidt orthogonalization process using ν = .01 when applied on a discrete sine-cosine basis is given in Fig.3, in which only the 8 first components are displayed. The lengths of the waves are 128 samples."
    }, {
      "heading" : "3.3 Indexing for fast retrieval in time series data bases",
      "text" : "Prop.2.3 shows how the elastic inner product generalizes the Euclidean inner product when time series are embedded into a finite dimensional vector space (one dimension per timestamps). We consider in this subsection such embeddings with the convention that if a time series has no value for a given timestamps a zero value is added on the dimension corresponding to the\nmissing timestamps. Each time series is thus represented by a finite dimensional vector, let say in Rn.\nProposition 3.2. Given a symmetric and strictly positive function g, let consider the so-called n× n symmetric elastic matrix in Rn 2 defined as:\nE =\n\n    \ng(t1, t1) g(t1, t2) g(t1, t3) · · · g(t1, tn) g(t2, t1) g(t2, t2) g(t2, t3) · · · g(t2, tn) g(t3, t1) g(t3, t2) g(t3, t3) · · · g(t3, tn)\n· · · · · · · · · · · · · · · g(tn, t1) g(tn, t2) g(tn, t3) · · · g(tn, tn)\n\n    \nand two time series An1 and B n 1 represented by two vectors\nin Rn. Then the elastic inner product defined recursively as:\n< An1 , B n 1 >eip=\n∑\n\n  \n  \n< An−11 , B n 1 >eip − < An−11 , Bn−11 >eip + a(n)b(n) · g(ta(n), tb(n)) < An1 , B n−1 1 >eip\n(5)\nidentifies to the following matrix products [An1 ] T E [Bn1 ]\nThis result is quite interesting in the scope of time series information retrieval especially when addressing very large databases. LetD = {B(i)n1}i=1···m be a time series database of size m, and consider the indexing phase that consists in constructing DE = {E [B(i)n1 ]}i=1···m. Then the retrieval of all the time series in D elastically similar to a given request An1 will require the computation of < An1 , B(i) n 1 >eip, for i ∈ {1, · · · ,m}, which reduces to evaluating:\n< An1 , B(i) n 1 >eip= [A n 1 ] T [EB(i)n1 ] (6)\nfor i ∈ {1, · · · ,m} which is done in linear complexity. Basically this construction breaks the quadratic complexity of the eip at retrieval stage and meets the same linear complexity as the classical Euclidean inner product. Moreover one can notice that the eip defined by Eq.5 has it continuous time equivalent that expresses as: < a, b >= ∫∫ a(t)b(τ)g(t, τ) dtdτ\n3.4 Kernel methods in EIPV S\nA wide range of literature exists on kernel theory, among which [3], [15] and [16] present some large syntheses of major results. We give hereinafter basic definitions and immediate results regarding kernel construction based on eip.\nDefinition 3.1. A kernel on a non empty set U refers to a complex (or real) valued symmetric function ϕ(x, y) : U × U → C (or R). Definition 3.2. Let U be a non empty set. A function ϕ : U × U → C is called a positive (resp. negative) definite kernel if and only if it is Hermitian (i.e. ϕ(x, y) = ϕ(y, x) where the overline stands for\nthe conjugate number) for all x and y in U and ∑n\ni,j=1 cic̄jϕ(xi, xj) ≥ 0 (resp. ∑n i,j=1 cic̄jϕ(xi, xj) ≤ 0), for all n in N, (x1, x2, ..., xn) ∈ Un and (c1, c2, ..., cn) ∈ Cn.\nDefinition 3.3. Let U be a non empty set. A function ϕ : U × U → C is called a conditionally positive (resp. conditionally negative) definite kernel if and only if it is Hermitian (i.e. ϕ(x, y) = ϕ(y, x) for all x and y in U ) and\n∑n i,j=1 cic̄jϕ(xi, xj) ≥ 0\n(resp. ∑n i,j=1 cic̄jϕ(xi, xj) ≤ 0), for all n ≥ 2 in N, (x1, x2, ..., xn) ∈ Un and (c1, c2, ..., cn) ∈ Cn with ∑n\ni=1 ci = 0.\nIn the last two above definitions, it is easy to show that it is sufficient to consider mutually different elements in U , i.e. collections of distinct elements x1, x2, ..., xn.\nDefinition 3.4. A positive (resp. negative) definite kernel defined on a finite set U is also called a positive (resp. negative) semidefinite matrix. Similarly, a positive (resp. negative) conditionally definite kernel defined on a finite set is also called a positive (resp. negative) conditionally semidefinite matrix.\n3.4.1 Definiteness of eip based kernel Proposition 3.3. A eip is a positive definite kernel.\nThe proof of Prop. 3.3 is straightforward and is omitted. The consequence is that numerous definite (positive or negative) kernels can be derived from a eip; among other immediate derivations it can be stated that:\n• (< ., . >eip) p is positive definite for all p ∈ N\n(polynomial kernel). • δeip defined by Prop.3.1 is negative definite. • e−ν·δeip is positive definite for all ν > 0. • e−ν·(δeip) p\nis positive definite for all ν > 0 and any 0 < p ≤ 2.\n• e<A,B>eip is positive definite.\nSome experimentations on Support vector Machine involving elastic kernels are reported in Sec.4.\n3.5 Elastic Cosine similarity in EIPV S, with application to symbolic (e.g. textual) information retrieval\nSimilarly to the definition of the cosine of two vectors in Euclidean space, we define the elastic cosine of two sequences by using any ep that satisfies the conditions of theorem 2.1.\nDefinition 3.5. Given two sequences, A and B, the elastic cosine similarity of these two sequences is given using a time elastic inner product < X, Y >e and the induced norm ‖X‖e = √ < X,X >e as similarity = eCOS(θ) = <A·B>e‖A‖e‖B‖e\nIn the case of textual or sequential data information retrieval, namely text matching or sequence matching, the\ntimestamps variable coincides with the index of words into the text or sequence, and the spatial dimensions encode the words or symbol into a given dictionary or alphabet. For instance, in text mining, each word can be represented using a vector whose dimension is the size of the set of concepts (or senses) that covers the conceptual domain associated to the dictionary, and each coordinate value, that is selected into [0; 1], encodes the degree of presence of the concept or senses into the considered word. In any case, the elastic cosine similarity measure takes value into [0; 1], 0 indicating the lowest possible similarity value between two sequential data and 1 the greatest possible similarity value between two sequential data. The elastic cosine similarity takes into account the order of occurrence of the words or symbols into the sequential data which could be an advantage compared to the Euclidean cosine measure that does not cope at all with the words or symbols ordering. Let us consider the following elastic inner product dedicated to text matching. In the following definition, Ap1 and B q 1 are sequences of words that represent textual content.\nDefinition 3.6.\n< Ap1, B q 1 >eiptm=\n∑\n\n  \n \n< Ap−11 , B q 1 >eiptm − < Ap−11 , Bq−11 >eiptm + e−ν.|ta(p)−tb(q)| 2 δ(a(p), b(q)) < Ap1, B q−1 1 >eiptm\n(7)\nwhere a(p) and b(q) are vectors whose coordinates identify words with weightings, δ(a, b) =< a, b > is the Euclidan inner product, and ν a time stiffness parameter.\nProposition 3.4. For ν = 0 and δ redefined as δ(a, b) = 1 if a = b, 0 otherwise, the elastic inner product defined in Eq.7 coincides with the euclidean inner product between two vectors whose coordinates correspond to term frequencies observed into the Ap1 and B q 1 text sequences. If, we change the definition of δ by the δ(a, b) = (IDF (a))2 if a = b, 0 otherwise, where IDF (a) is the inverse document frequency of term a into the considered collection, then for ν = 0, < Ap1, B q 1 >eiptm coincides with the euclidean inner product between two vectors whose coordinates correspond to the TFIDF (term frequency times the inverse document frequency) of terms occurring into the Ap1 and B q 1 text sequences.\nThe proof of proposition 3.4 is straightforward an is omitted.\nThus, the elastic cosine measure derived from the elastic inner product defined by Eq.7 generalizes somehow the cosine measure implemented in the vector space model [14] and commonly used in the text information retrieval community. To exemplify the behavior of the elastic cosine similarity on sequential data, we consider the four sequences depicted in Eq.8. The variations of the elastic cosine as\na function of ν evaluated on pairwise sequences are reported in Fig.4\nA = [abababab] B = [aaaabbbb] C = [bbbbaaaa] D = [bbaa]\n(8)\nIn Fig.4 the extreme left part of the curves, characterized with high ν values, corresponds, when applied on sequences having the same length and represented by vectors, to the cosine similarity constructed with the Euclidean inner product. The right part of the curves, characterized with very low ν values, corresponds to the cosine similarity evaluated using the tf(-idf) vector space model. The center part of the figure, characterized with medium ν values shows that elasticity allows to better discriminating between the sequences."
    }, {
      "heading" : "4 EXPERIMENTATIONS",
      "text" : ""
    }, {
      "heading" : "4.1 Time series classification",
      "text" : "We empirically evaluate the effectiveness of the distance induced by an elastic inner product comparatively to the Euclidean and the Dynamic Time Warping distances using some classification tasks on a set of time series coming from quite different application fields. The classification task we have considered consists of assigning one of the possible categories to an unknown time series for the 20 data sets available at the UCR repository [8]. As time is not explicitly given for these datasets, we used the index value of the samples as the timestamps for the whole experiment.\nFor each dataset, a training subset (TRAIN) is defined as well as an independent testing subset (TEST). We use the training sets to train two kinds of classifiers:\n• the first one is a first near neighbor (1-NN) classifier: first we select a training data set containing time series for which the correct category is known. To assign a category to an unknown time series selected from a testing data set (different from the train set), we select its nearest neighbor (in the sense of a distance or similarity measure) within the training data set, then, assign the associated category to its nearest neighbor. For that experiment, a leave one out procedure is performed on the training dataset to optimize the meta parameter ν of the considered elastic distance. • the second one is a SVM classifier [4], [19] configured with a Gaussian RBF kernel whose parameters are C > 0, a trade-off between regularization and constraint violation and σ that determines the width of the Gaussian function. To determine the C and σ hyper parameter values, we adopt a 5-folded cross-validation method on each training subset. According to this procedure, given a predefined training set TRAIN and a test set TEST, we adapt the meta parameters based on the training set TRAIN: we first divide TRAIN into 5 stratified subsets TRAIN1, TRAIN2, · · · , TRAIN5; then for each subset TRAINi we use it as a new test set, and regard (TRAIN − TRAINi) as a new training set; Based on the average error rate obtained on the five classification tasks, the optimal values of meta parameters are selected as the ones leading to the minimal average error rate. For the elastic kernel, the meta parameter ν is also optimized using this 5-folded cross-validation method performed on the training datasets.\nThe classification error rates are then estimated on the TEST datasets on the basis of the parameter values\noptimized on the TRAIN datasets. We have used the LIBSVM library [6] to implement the SVM classifiers. We tested the time elastic inner product < A,B >eip (Eq.3). Precisely, we used the timewarp distance induced by < A,B >eip, basically δeip(A,B) = (< A− B,A−B >eip)1/2 = (< A,A >eip + < B,B >eip −2. < A,B >eip)1/2.\nTo speed up the computation at exploitation stage, we have used the construction proposed in Sec.3.3 Prop.3.2 with the following elastic Matrix E, where ν > 0 :\nE = \n     \n1.0 e−ν|t1−t2| 2 e−ν|t1−t3| 2 · · · e−ν|t1−tn|2\ne−ν|t2−t1| 2\n1.0 e−ν|t2−t3| 2 · · · e−ν|t2−tn|2\ne−ν|t3−t1| 2 e−ν|t3−t2| 2 1.0 · · · e−ν|t3−tn|2 · · · · · · · · · · · · · · · e−ν|tn−t1| 2 e−ν|tn−t2| 2 e−ν|tn−t3| 2 · · · 1.0\n\n     \nThe databases DE = {EB(i)n1}i=1···m are thus constructed off-line from the TRAIN datasets, once the optimization procedure of the ν parameter has been completed. DE is then exploited on-line by the 1-NN and SVM classifiers."
    }, {
      "heading" : "4.1.1 Meta parameters",
      "text" : "δeip is characterized by the meta parameter ν (the stiffness parameter) that is optimized for each dataset on the train data by minimizing the classification error rate of a first near neighbor classifier. For this kernel, ν is selected in {100, 10, 1, .1, .01, ..., 1e− 5, 0}.\nTo explore the potential benefits of an elastic inner product against the Euclidean inner product, we also tested the Euclidean Distance δed which, as already stated, is the limit when ν → ∞ of δeip.\nThe kernels exploited by the SVM classifiers are the\nGaussian kernels Keip(A,B) = e −δeip(A,B) 2/(2·σ2) and Ked(A,B) = e −δed(A,B)\n2/(2·σ2). The meta parameter C is selected from the discrete set {2−8, 2−7, ..., 1, 2, ..., 210}, and σ2 from {2−5, 2−4, ..., 1, 2, ..., 210}. Table 1 gives for each data set and each tested kernels (Ked and Keip) the corresponding optimized values of the meta parameters.\nAccording to the classification results, this experiment shows that the distance induced by the elastic inner product δeip is significantly more effective for the considered tasks comparatively to the Euclidean distance. It exhibits, on average, the lowest error rates for most of the tested datasets and for both the 1-NN and SVM classifiers, as shown in Table 2 and Figures 5 and 6. The stiffness parameter ν in δeip seems to play a significant role in these classification tasks, and this for a quite large majority of data sets.\nOnly one dataset, yoga, is better classified by the 1-NN δed classifier on the test data, although the error rate on the train data is lower for the 1-NN δeip classifier. For the SVM classifiers, only two datasets, Face (all) and Coffee, are significantly better classified on the test data by SVM Ked classifiers. Nevertheless, for these two datasets, Keip reaches a better (or best) score on the train data. We are facing here the trade-off between learning and generalization capabilities. The meta parameter ν is selected such as to minimized the classification error on the train data. If this strategy is on average a winning strategy, some datasets show that it does not always lead to a good trade-off, this is the case for Face (all) and yoga datasets. However, δdtw based classifiers outperform δeip based classifiers on a majority of datasets. The average ranking of the classifiers shows clearly that δeip ranks in between δed and δdtw. δeip can be considered as a compromise between the linear Euclidean distance and the nonlinear δdtw. It maintains a low computational cost and nevertheless compensates some of the limitations of the Euclidean distance that it is very sensitive to distortions in the time axis. It should be noted that δeip can cope with sample substitution, deletion and insertion as well as δdtw. In addition, δeip can deal with sample permutations while δdtw cannot."
    }, {
      "heading" : "4.2 Sequence classification",
      "text" : "We report here a protein classification experiment carried out using the Protein Classification Benchmark Collection (PCBC) [18] [1]. This benchmark contains structural and functional annotations of proteins. The two datasets that we have exploited, SCOP95 and CATH95, are available at http://hydra.icgeb.trieste.it/benchmark. The entries of the SCOP95 dataset are characterized by sequences with variable lengths and relatively little sequence similarity (less than 95% sequence identity) between the protein families. The CATH95 dataset contains near-identical protein families of variable lengths in which the proteins have a high sequence similarity (more than 95% sequence identity). Basically, the considered classification tasks involve protein domain sequence and structure comparisons at various levels of the structural hierarchies. We have considered the following 14 PCB subsets:\n• PCB00001 SCOP95 Superfamily Family • PCB00002 SCOP95 Superfamily 5fold • PCB00003 SCOP95 Fold Superfamily • PCB00004 SCOP95 Fold 5fold • PCB00005 SCOP95 Class Fold • PCB00006 SCOP95 Class 5fold • PCB00007 CATH95 Homology Similarity • PCB00008 CATH95 Homology 5fold • PCB00009 CATH95 Topology Homology • PCB00010 CATH95 Topology 5fold • PCB00011 CATH95Architecture Topology\nTABLE 1 Dataset sizes and meta parameters used in conjunction with Ked, Keip and Kdtw kernels\nDATASET length|#class|#train|#test Ked : C, σ KDeip : ν,C, σ KDTW : C, σ Synthetic control 60|6|300|300 1.0;0.125 .1;.25;.0625 8.0;4.0 Gun-Point 150|2|50|150 256;.5 0.01;256;.5 16.0;0.0312 CBF 128|3|30|900 8;1.0 .001;4.0;.0312 1.0;1.0 Face (all) 131|14|560|1690 4;0.5 .1;8.0;.5 2.0;0.25 OSU Leaf 427|6|200|242 2;0.125 .1;4;0.125 4.0;0.062 Swedish Leaf 128|15|500|625 128;0.125 10;8;.0625 4.0;0.031 50 Words 270|50|450|455 32;0.5 0.01;32;0.5 4.0;0.062 Trace 275|4|100|100 8;0.0156 .001;256;.0625 4;0.25 Two Patterns 128 |4|1000|4000 4.0;0.25 .01;1;.0312 0.25,0.125 Wafer 152|2|1000|6174 4.0;0.5 0.01;32;.5 1.0;0.016 face (four) 350|4|24|88 8.0;2.0 .01;16;2 16;0.5 Ligthing2 637|2|60|61 2.0;0.125 .001;128;2 2.0;0.031 Ligthing7 319|7|70|73 32.0;256.0 .1;16;.5 4;0.25 ECG200 96|2|100|100 8.0;0.25 0, 1024, .0625 2;0.62 Adiac 176|37|390|391 1024.0;0.125 10;256.0;.0312 16;0.0039 Yoga 426|2|300|3000 64.0;0.125 1;32;.0625 4;0.008 Fish 463|7|175|175 64.0;1.0 .01;256;1 8;0.016 Coffee 286|2|28|28 128.0;4.0 .01;1024;4 8;0.062 OliveOil 570|4|30|30 2.0;0.125 .01;64;2 2;0.125 Beef 470|5|30|30 128.0;4.0 0;64;.5 16;0.016\nTABLE 2 Comparative study using the UCR datasets: classification error rates (in %) obtained using the first near neighbor (1-NN) classification rule and a SVM classifier for the Ked, Keip and Keip kernels. Two scores are given S1|S2: the first one, S1, is evaluated on the training data, while the second one, S2, is evaluated on the test data. For the each classification methods (1-NN and SVM) the rank of each classifier is given in parenthesis ((1): best classifier, (2): 2nd best classifier, (3): 3rd best classifier\nDATASET 1-NN δed 1-NN δeip 1-NN δdtw SVM Ked SVM Keip SVM Kdtw Synthetic control 9(3)|12(3) .67(1)|1(2) 1.0(2)|0.67(1) 3(3)|2.33(3) .33(2)|.67(1) 0(1)|1(2) Gun-Point 4.0(1)|8.67(1) 4.0(1)|8.67(1) 18.36(3)|9.3(3) 4.0(3)|6.0(3) 2.0(2)|2.67(2) 0(1)|1.33(1) CBF 16.67(3)|14.78(3) 3.33(2)|4.22(2) 0(1)|0.33(1) 3.33(2)|10.89(3) 3.33(1)|5(1) 3.33(1)|5.44(2) Face (all) 11.25(3)|28.64(3) 7.5(2)|26.33(2) 6.8(1)|19.23(1) 9.82(3)|16.45(3) 6.25(2)|24.91(2) .54(1)|16.98(1) OSU Leaf 37.0(2)|48.35(2) 37(2)|48.35(2) 33.17(1)|40.9(1) 35(3)|44.21(2) 34.5(2)|44.21(2) 20(1)|23.55(1) Swedish Leaf 26.6(3)|21.12(3) 24.4(1)|20.96(2) 24.65(2)|20.8(1) 15(2)|8.64(2) 15(2)|8.64(2) 7(1)|5.6(1) 50 Words 34.47(3)|36.32(3) 32.2(1)|32.73(2) 33.18(2)|31(1) 33.56(3)|30.99(3) 31.78(2)|29.67(2) 15.21(1)|17.58(1) Trace 18(3)|24(2) 16(2)|24(2) 0(1)|0(1) 9(3)|19(3) 1(2)|7(2) 0(1)|2(1) Two Patterns 8.5(3)|9.32(3) 4.3(2) |3.62(2) 0(1)|0(1) 8.6(3)|7.45(3) 5.5(2)|3.52(2) 0(1)|0(1) Wafer 0.7(2)|0.45(2) .5(1)|.42(1) 1.4(3)|2.01(3) .7(3)|.7(3) .2(2)|.68(2) 0(1)|0.39(1) face (four) 37.5(3)|21.59(3) 33.33(2)|19.31(2) 26.09(1)|17.05(1) 20.84(3)|19.31(3) 16.67(2)|13.63(2) 8.33(1)|5.68(1) Ligthing2 25.0(3)|24.59(3) 20(2)|16.39(2) 13.56(1)|13.1(1) 21.77(3)|31.14(3) 20(2)|26.22(2) 8.33(1)|19.67(1) Ligthing7 35.71(3)|42.47(3) 30.0(1)|32.87(2) 33.33(2)|27.4(1) 37.14(3)|36.98(3) 34.29(2)|35.61(2) 17.14(1)|16.43(1) ECG200 14.0(2)|12.0(2) 1.0(1)|2.0(1) 23.23(3)|23(3) 8.0(3)|9.0(2) 3.0(1)|7.0(1) 7(2)|13(3) Adiac 41.28(3)|38.87(1) 39.59(1)|38.87(1) 40.62(2)|39.64(3) 26.67(3)|24.04(1) 25.13(2)|24.04(1) 24.61(1)|25.32(3) Yoga 22.67(3)|16.9(2) 21.67(2)|22.26(3) 16.37(1)|16.4(1) 17.66(3)|14.43(2) 15.33(2)|14.4(2) 11(1)|11.2(1) Fish 24.0(1)|21.71(2) 24.0(1)|21.71(2) 26.44(3)|16.57(1) 14.86(3)|13.14(3) 13.14(2)|12.57(2) 6.86(1)|4.57(1) Coffee 21.43(2)|25.0(2) 21.43(2)|25.0(2) 14.81(1)|17.86(1) 0(1)|0(1) 0(1)|7.14(2) 10.71(3)|17.86(3) OliveOil 13.33(1)|13.33(1) 13.33(1)|13.33(1) 13.79(3)|13.33(1) 10.0(1)|10.0(1) 10.0(1)|10.0(1) 13.33(3)|16.67(3) Beef 46.67(1)|46.67(1) 46.67(1)|46.67(1) 55.17(3)|50(3) 37.67(2)|30(1) 37.67(2)|30(1) 32.14(1)|42.85(3) Average Rank (2.4)|(2.25) (1.45)|(1.75) (1.85)|(1.5) (2.65)|(2.4) (1.8)|(1.7) (1.25)|(1.6)\n• PCB00012 CATH95 Architecture 5fold • PCB00013 CATH95 Class Architecture • PCB00014 CATH95 Class 5fold\nWe evaluate the elastic cosine similarity based on the eip defined for symbolic sequences (Def.3.6, Eq.7) comparatively to five other similarity measures commonly used in Bioinformatics:\n• BLAST [2]: the Basic Local Alignment Search Tool is a very popular family of fast heuristic search methods used for finding similar regions between two or more nucleotides or amino acids.\n• SW [17]: The SmithWaterman algorithm is used for performing local sequence alignment, for determining similar regions between two nucleotide or protein sequences. Instead of looking at the sequence globally as NW does, the SmithWaterman algorithm compares subsequences of all possible lengths. • NW [11]: The Needleman Wunsch algorithm performs a maximal global alignment of two strings. It is commonly used in bioinformatics to align protein sequences or nucleotides. • LA kernel [12]: The Local Alignment kernel is used\nto detect local alignment between strings by convolving simple basic kernels. Its construction mimic the local alignment scoring schemes proposed in the\nSW algorithm. • PRIDE [5]: The PRIDE score is estimated as the\nPRobability of IDEntity between two protein 3D\nstructures. The calculation of similarity between two proteins, is based on the comparison of histograms of the pairwise distances between C − α residues whose distribution is highly characteristic of protein folds.\nThe average AUC (area under the ROC Curve) measure is evaluated for 1-NN classifiers exploiting respectively BLAST, SW, NW, LA, PRIDE and eCOS(ν) as alignment methods. One can notice that these datasets are quite well suited for global alignment since, as shown in table 3, the NW algorithm performs better than the SW algorithm. The eip structure that considers global alignment is thus well adapted to the task. We show on these experiments that for ν = .05 the eCOS classifier in average performs significantly better than BLAST heuristics [2] and LA [12], the local alignment kernel for string. Furthermore, it performs almost as well as the SW and NW algorithms. The PRIDE method [5] gets the best results, but it uses the tertiary structure of the proteins while all the other methods exploit the primary structure. Here again, a ranking based on eCOS similarity has a complexity that could be maintained linear at exploitation stage (i.e. when testing numerous sequences against massive datasets). These very positive results offer perspective in fast filtering of biological symbolic sequences."
    }, {
      "heading" : "4.3 Experimental complexity",
      "text" : "To evaluate in practice the computational cost of δeip, we compare it with two other distances, namely δed (the Euclidean distance) which has a linear complexity, and δdtw, the Dynamic Time Warping distance which has a quadratic complexity. In addition we evaluate the computational cost of the indexed version of δeip that we refer to as δi−eip. The experiment consists in producing random datasets of 100 time series of increasing lengths (10, 100, 1000 and 10000 samples) and computing the\n100x100 distance matrices. Figure 7 gives the elapsed time in second, according to a logarithmic scale, for the four distances as a function of the length of the time series. When compared to δdtw, δeip is evaluated very efficiently using the matrix computation given in Eq.6, although, without any off line indexing of the time series, δeip cannot compete with δed when the length of the time series increases. The δi−eip curve has clearly the same slope than the δed curve, showing that the offline indexed version of δeip is characterized with a linear complexity, that includes a nevertheless linear overhead when compared to δed, mainly due to the loading of the index."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "This paper has proposed what we call a family of elastic inner products able to cope with non-uniformly sampled time series of various lengths, as far as they do not contain the zero or null symbol value. These constructions allow one to embed any such time series in a single inner space, that some how generalizes the notion of Euclidean inner space. The recursive structure of the proposed construction offers the possibility to manage several elastic dimensions. Some applicative benefits can be expected in time series or sequence analysis when time elasticity is an issue, for instance in the field of numeric or symbolic sequence data mining. If the algorithmic complexity required to evaluate an elastic inner product is in general quadratic, its computation can be much more efficiently performed than dynamic programming algorithms. In addition we have shown that for some information retrieval applications for which embeddings of time series or symbolic sequences into a finite dimensional Euclidean space is possible, one can break this quadratic complexity down to a linear complexity at exploitation time, although a quadratic computational cost should still be paid once during a preprocessing step at indexing phase.\nThe preliminary experiments we have carried out on some time series and symbolic sequence classification tasks show that embedding time series into elastic inner product space may brought significant accuracy improvement when compared to Euclidean inner product space embeddings as they compensate, at least partially, the limitations of Euclidean distance which is very sensitive to distortions in the time axis. Although our experiments show that Dynamic Programming matching algorithms outperforms on a majority of dataset distances that are derived from an elastic inner product, on some datasets such distances lead to similar accuracies .\nThe experiment carried out on symbolic sequences alignment involves sequences of various lengths. It shows also some very interesting perspectives in the scope of fast filtering of massive data, since the accuracy obtained by a 1-NN symbolic elastic cosine classifier with a potentially linear complexity at exploitation time is somehow comparable to the one obtained using dynamic programming algorithms (NW and SW) whose complexity are quadratic when the alignment search space is not restricted.\nFinally, the general recursive structure of the elastic inner product opens perspectives in the processing of more complex data such as tree data mining for which considering several elastic dimensions may be relevant and efficient. The possibility to decompose complex structures onto sets of elastic basis vectors opens perspectives in various areas of application such as data compression, multi-dimensional scaling or matching pursuits."
    } ],
    "references" : [ {
      "title" : "Basic local alignment search tool",
      "author" : [ "S.F. Altschul", "W. Gish", "W. Miller", "E.W. Myers", "D.J. Lipman" ],
      "venue" : "Journal of molecular biology,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1990
    }, {
      "title" : "Harmonic Analysis on Semigroups: Theory of Positive Definite and Related Functions, volume 100 of Graduate Texts in Mathematics",
      "author" : [ "Christian Berg", "Jens Peter Reus Christensen", "Paul Ressel" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1984
    }, {
      "title" : "A training algorithm for optimal margin classifiers",
      "author" : [ "Bernhard E. Boser", "Isabelle Guyon", "Vladimir Vapnik" ],
      "venue" : "In COLT,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1992
    }, {
      "title" : "Protein fold similarity estimated by a probabilistic approach based on c(alpha)-c(alpha) distance comparison",
      "author" : [ "O. Carugo", "S. Pongor" ],
      "venue" : "Journal of Molecular Biology, 315(4):887–898",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "On the marriage of lp-norm and edit distance",
      "author" : [ "L. Chen", "R. Ng" ],
      "venue" : "Proceedings of the 30th International Conference on Very Large Data Bases, pages 792–801",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "L",
      "author" : [ "E.J. Keogh", "X. Xi" ],
      "venue" : "Wei, and C.A. Ratanamahatana. The ucr time series classification-clustering datasets",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Time warp edit distance with stiffness adjustment for time series matching",
      "author" : [ "P.F. Marteau" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 31(2):306–318",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Constructing positive elastic kernels with application to time series classification",
      "author" : [ "Pierre-François Marteau", "Sylvie Gibet" ],
      "venue" : "CoRR, abs/1005.5141,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "A general method applicable to the search for similarities in the amino acid sequence of two proteins",
      "author" : [ "Saul B. Needleman", "Christian D. Wunsch" ],
      "venue" : "Journal of Molecular Biology,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1970
    }, {
      "title" : "Protein homology detection using string alignment kernels",
      "author" : [ "H. Saigo", "J.P. Vert", "N. Ueda", "T. Akutsu" ],
      "venue" : "Bioinformatics, 20:1682– 1689",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A dynamic programming approach to continuous speech recognition",
      "author" : [ "H. Sakoe", "S. Chiba" ],
      "venue" : "Proceedings of the 7th International Congress of Acoustic, pages 65–68",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Introduction to Modern Information Retrieval",
      "author" : [ "Gerard Salton", "Michael McGill" ],
      "venue" : "McGraw-Hill Book Company,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1984
    }, {
      "title" : "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
      "author" : [ "Bernhard Scholkopf", "Alexander J. Smola" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2001
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "John Shawe-Taylor", "Nello Cristianini" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Identification of common molecular subsequences",
      "author" : [ "T. Smith", "Waterman M" ],
      "venue" : "Journal of Molecular Biology,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1981
    }, {
      "title" : "A protein classification benchmark collection for machine learning",
      "author" : [ "Paolo Sonego", "Mircea Pacurar", "Somdutta Dhir", "Attila Kertész- Farkas", "András Kocsor", "Zoltán Gáspári", "Jack A.M. Leunissen", "Sándor Pongor" ],
      "venue" : "Nucleic Acids Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Automatic recognition of 200 words",
      "author" : [ "V.M. Velichko", "N.G. Zagoruyko" ],
      "venue" : "International Journal of Man-Machine Studies, 2:223–234",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1970
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "Recently, [10] have shown that it is quite easy to propose inner product with time elasticity capability at least for some restricted time series spaces, basically spaces containing uniformly sampled time series, all of which have the same lengths (in such cases, time series can be embedded easily in Euclidean spaces).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 17,
      "context" : "Basically this is the same complexity as that required for the calculation of the complete dynamic programming solutions such as the Dynamic Time Warping (DTW) [20] [13] algorithm evaluated on these two time series.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 11,
      "context" : "Basically this is the same complexity as that required for the calculation of the complete dynamic programming solutions such as the Dynamic Time Warping (DTW) [20] [13] algorithm evaluated on these two time series.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "A wide range of literature exists on kernel theory, among which [3], [15] and [16] present some large syntheses of major results.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "A wide range of literature exists on kernel theory, among which [3], [15] and [16] present some large syntheses of major results.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "A wide range of literature exists on kernel theory, among which [3], [15] and [16] present some large syntheses of major results.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "7 generalizes somehow the cosine measure implemented in the vector space model [14] and commonly used in the text information retrieval community.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "The classification task we have considered consists of assigning one of the possible categories to an unknown time series for the 20 data sets available at the UCR repository [8].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "• the second one is a SVM classifier [4], [19] configured with a Gaussian RBF kernel whose parameters are C > 0, a trade-off between regularization and constraint violation and σ that determines the width of the Gaussian function.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "We have used the LIBSVM library [6] to implement the SVM classifiers.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "We report here a protein classification experiment carried out using the Protein Classification Benchmark Collection (PCBC) [18] [1].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "• BLAST [2]: the Basic Local Alignment Search Tool is a very popular family of fast heuristic search methods used for finding similar regions between two or more nucleotides or amino acids.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "• SW [17]: The SmithWaterman algorithm is used for performing local sequence alignment, for determining similar regions between two nucleotide or protein sequences.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 9,
      "context" : "• NW [11]: The Needleman Wunsch algorithm performs a maximal global alignment of two strings.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "• LA kernel [12]: The Local Alignment kernel is used",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "• PRIDE [5]: The PRIDE score is estimated as the PRobability of IDEntity between two protein 3D",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "05 the eCOS classifier in average performs significantly better than BLAST heuristics [2] and LA [12], the local alignment kernel for string.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "05 the eCOS classifier in average performs significantly better than BLAST heuristics [2] and LA [12], the local alignment kernel for string.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "The PRIDE method [5] gets the best results, but it uses the tertiary structure of the proteins while all the other methods exploit the primary structure.",
      "startOffset" : 17,
      "endOffset" : 20
    } ],
    "year" : 2012,
    "abstractText" : "This paper proposes a framework dedicated to the construction of what we call discrete elastic inner product allowing one to embed sets of non-uniformly sampled multivariate time series or sequences of varying lengths into inner product space structures. This framework is based on a recursive definition that covers the case of multiple embedded time elastic dimensions. We prove that such inner products exist in our general framework and show how a simple instance of this inner product class operates on some prospective applications, while generalizing the Euclidean inner product. Classification experimentations on time series and symbolic sequences datasets demonstrate the benefits that we can expect by embedding time series or sequences into elastic inner spaces rather than into classical Euclidean spaces. These experiments show good accuracy when compared to the euclidean distance or even dynamic programming algorithms while maintaining a linear algorithmic complexity at exploitation stage, although a quadratic indexing phase beforehand is required.",
    "creator" : "LaTeX with hyperref package"
  }
}