{
  "name" : "1310.7048.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scaling SVM and Least Absolute Deviations via Exact Data Reduction",
    "authors" : [ "Jie Wang", "Peter Wonka", "Jieping Ye" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The support vector machine is one of the most popular classification tools in machine learning. Many efforts have been devoted to develop efficient solvers for SVM [13, 18, 27, 16, 11]. However, the applications of SVM to large-scale problems still pose significant challenges. To address this issue, one promising approach is by “screening”. The key idea of screening is motivated by the well known feature of SVM, that is, the resulting classifier is determined only by the so called “support vectors”. If we first identify the non-support vectors via screening, and then remove them from the optimization, it may lead to substantial savings in the computational cost and memory. Another useful tool in machine learning and statistics is the least absolute deviations regression (LAD) [22, 30, 7, 24] or `1 method. When the protection against outliers is a major concern, LAD provides a useful and plausible alternative to the classical least squares or `2 method for linear regression. In this paper, we study both SVM and LAD under a unified framework.\nThe idea of screening has been successfully applied to a large class of `1-regularized problems [10, 31, 29, 28], including Lasso, `1-regularized logistic regression, elastic net, and more general convex problems. Those methods are able to discard a large portion of “inactive” features, which has 0 coefficients in the optimal solution, and the speedup can be several orders of magnitude.\nRecently, Ogawa et al. [20] proposed a “safe screening” rule to identify non-support vectors for SVM; in this paper, we refer to this method as SSNSV for convenience. Notice that, the former approaches for `1-regularized problems aim to discard inactive “features”, while SSNSV is to identify the non-support “vectors”. This essential difference makes SSNSV a nontrivial extension of the existing feature screening\nar X\niv :1\n31 0.\n70 48\nv1 [\ncs .L\nG ]\n2 5\nO ct\nmethods. Although there exist many methods for data reduction for SVM [1, 33, 5], they are not safe, in the sense that the resulting classification model may be different. To the best of our knowledge, SSNSV is the only existing safe screening method [20] to identify the non-support vectors for SVM. However, in order to run the screening, SSNSV needs to iteratively determine an appropriate parameter value and an associated feasible solution, which can be very time consuming.\nIn this paper, we develop novel efficient and effective screening rules, called “DVI”, for a class of supervised learning problems including SVM and LAD [4, 17]. The proposed method, DVI, shares the same advantage as SSNSV [20], that is, both rules are safe in the sense that the discarded vectors are guaranteed to be non-support vectors. The proposed DVI identifies the non-support vectors by estimating a lower bound of the inner product between each vector and the optimal solution, which is unknown. The more accurate the estimation is, the more non-support vectors can be detected. However, the estimation turns out to be highly non-trivial since the optimal solution is not available. To overcome this difficulty, we propose a novel framework to accurately estimate the optimal solution via the estimation of the “dual optimal solution”, as the primal and dual optimal solutions can be related by the KKT conditions [12]. Our main technical contribution is to estimate the dual optimal solution via the so called “variational inequalities” [12]. Our experiments on both synthetic and real data demonstrate that DVI can identify far more non-support vectors than SSNSV. Moreover, by using the same technique, that is, variational inequalities, we can strictly improve SSNSV in identifying the non-support vectors. Our results also show that DVI is very effective in discarding non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.\nThe rest of this paper is organized as follows. In Section 2, we study the SVM and LAD problems under a unified framework. We then introduce our DVI rules in detail for the general formulation in Sections 3 and 4. In Sections 5 and 6, we extend the DVI rules derived in Section 4 to SVM and LAD respectively. In Section 7, we evaluate our DVI rules for SVM and LAD using both synthetic and real data. We conclude this paper in Section 8.\nNotations: Throughout this paper, we use 〈x,y〉 = ∑ i xiyi to denote the inner product of vectors x and y, and ‖x‖2 = 〈x,x〉. For vector x, let [x]i be the ith component of x. If M is a matrix, mi is the ith column of M and [M]i,j is the (i, j)\nth entry of M. Given a scalar x, we denote max{x, 0} by [x]+. For the index set I := {1, . . . , l}, let J := {j1, . . . , jk} ⊆ I and J c := I \\ J . For a vector x or a matrix M, let [x]J = ([x]j1 , . . . , [x]jk)\nT and [M]J = (mj1 , . . . ,mjk). Moreover, let Γ0(<n) be the class of proper and lower semicontinuous convex functions from <n to (−∞,∞]. The conjugate of f ∈ Γ0(<n) is the function f∗ ∈ Γ0(<n) given by\nf∗ : <n → (−∞,∞] : θ 7→ sup x∈<n xT θ − f(x). (1)\nThe biconjugate of f ∈ Γ0(<n) is the function f∗∗ ∈ Γ0(<n) given by\nf∗∗ : <n → (−∞,∞] : x 7→ sup θ∈<n xT θ − f∗(θ). (2)"
    }, {
      "heading" : "2 Basics and Motivations",
      "text" : "In this section, we study the SVM and LAD problems under a unified framework. Then, we motivate the general screening rules via the KKT conditions. Consider the convex optimization problems of the following form:\nmin w∈<n\n1 2 ‖w‖2 + CΦ(w), (3)\nwhere Φ : <n → < is a convex function but not necessarily differentiable and C > 0 is a regularization parameter. Notice that, the function Φ is generally referred to as the empirical loss. More specifically, suppose we have a set of observations {xi, yi}li=1, where xi ∈ <n and yi ∈ < are the ith data instance and the corresponding response. We focus on the following function class:\nΦ(w) = l∑ i=1 ϕ ( wT (aixi) + biyi ) , (4)\nwhere ϕ : < → <+ is a nonconstant continuous sublinear function, and ai, bi are scalars. We provide the definition of sublinear function as follows.\nDefinition 1. [15] A function σ : <n → (−∞,∞] is said to be sublinear if it is convex, and positively homogeneous, i.e.,\nσ(tx) = tσ(x), ∀x ∈ <nand t > 0. (5)\nWe will see that SVM and LAD are both special cases of problem (3). A nice property of the function ϕ is that the biconjugate ϕ∗∗ is exactly ϕ itself, as stated in Lemma 2.\nLemma 2. For the function ϕ : < → <+ which is continuous and sublinear, we have ϕ ∈ Γ0(<), and thus ϕ∗∗ = ϕ.\nIt is straightforward to check the statement in Lemma 2 by verifying the requirements of the function class Γ0(<). For self-completeness, we provide a proof in the supplement. According to Lemma 2, problem (3) can be rewritten as\nmin w∈<n\n1 2 ‖w‖2 + C l∑ i=1 ϕ∗∗ ( wT (aixi) + biyi ) (6)\n= min w∈<n\n1 2 ‖w‖2 + C l∑ i=1 { sup θi∈< θi [ wT (aixi) + biyi ] − ϕ∗(θi) }\n= min w∈<n sup θi∈< i=1,...,l\n1 2 ‖w‖2 + C l∑ i=1 { θi [ wT (aixi) + biyi ] − ϕ∗(θi) }\n= sup θ∈<l −C l∑ i=1 ϕ∗(θi) + min w∈<n 1 2 ‖w‖2 + C〈Zw + ȳ, θ〉,\nwhere θ = (θ1, . . . , θl) T , Z = (aixi, . . . , alxl) T and ȳ = (b1y1, . . . , blyl) T . Let `(w) := 12‖w‖ 2 + C〈Zw + ȳ, θ〉. The reason we can exchange the order of min and sup in Eq. (6) is due to the strong duality of problem (3) [3].\nBy setting ∂`(w)∂w = 0, we have\nw∗ = −CZT θ, (7)\nand thus\nmin w\n`(w) = `(w∗) = −C 2\n2 ‖ZT θ‖2 + C〈ȳ, θ〉. (8)\nHence, Eq. (6) becomes\nsup θ∈<l −C l∑ i=1 ϕ∗(θi)− C2 2 ‖ZT θ‖2 + C〈ȳ, θ〉. (9)\nMoreover, because ϕ ∈ Γ0(<) is sublinear by Lemma 2, we know that ϕ∗ is the indicator function for a closed convex set. In fact, we have the following result:\nLemma 3. For the nonconstant continuous sublinear function ϕ : < → <+, there exists a nonempty closed interval Iϕ = [α, β] with α, β ∈ < and α < β such that\nϕ∗(t) := ι[α,β] = { 0, if t ∈ [α, β], ∞, otherwise.\n(10)\nLet I lϕ = [α, β] l. We can rewrite problem (9) as\nsup θ∈Ilϕ −C\n2\n2 ‖ZT θ‖2 + C〈ȳ, θ〉. (11)\nProblem (11) is in fact the dual problem of (3). Moreover, the “sup” in problem (11) can be replaced by “max” due to the strong duality [3] of problem (3). Since C > 0, problem (11) is equivalent to\nmin θ∈Ilϕ\nC 2 ‖ZT θ‖2 − 〈ȳ, θ〉. (12)\nLet w∗(C) and θ∗(C) be the optimal solutions of (3) and (11) respectively. Eq. (7) implies that\nw∗(C) = −CZT θ∗(C). (13)\nThe KKT conditions1 of problem (12) are\n[θ∗(C)]i ∈  β, if − 〈w∗(C), aixi〉 < biyi; [α, β], if − 〈w∗(C), aixi〉 = biyi; α, if − 〈w∗(C), aixi〉 > biyi; i = 1, . . . , l. (14)\nFor notational convenience, let\nR = {i : −〈w∗(C), aixi〉 > biyi}, E = {i : −〈w∗(C), aixi〉 = biyi}, L = {i : −〈w∗(C), aixi〉 < biyi}.\nWe call the vectors in the set E as “support vectors”. All the other vectors in R and L are called “nonsupport vectors”. The KKT conditions in (14) imply that, if some of the data instances are known to be members of R and L, then the corresponding components of θ∗(C) can be set accordingly and we only need the other components of θ∗(C). More precisely, we have the following result:\nLemma 4. Given index sets R̂ ⊆ R and L̂ ⊆ L, we have 1. [θ∗(C)]R̂ = α and [θ ∗(C)]L̂ = β.\n2. Let Ŝ = R̂ ⋃ L̂, |Ŝc| be the cardinality of the set Ŝc, Ĝ11 = [ZT ]TŜc [Z T ]Ŝc , Ĝ12 = [X T ]TŜc [X T ]Ŝ and\nŷ = yŜc − CĜ12[θ∗(C)]Ŝ . Then, [θ∗(C)]Ŝc can be computed by solving the following problem:\nmin θ̂∈<|Ŝc|\nC 2 θ̂T Ĝ11θ̂ − ŷT θ̂, s.t. θ̂ ∈ [α, β]|Ŝ c|. (15)\nClearly, if |Ŝ| is large compared to |I| = l, the computational cost for solving problem (15) can be much cheaper than solving the full problem (12). To determine the membership of the data instances, Eq. (13) and (14) imply that\nC〈ZT θ∗(C), aixi〉 > biyi ⇒ [θ∗(C)]i = α⇔ i ∈ R; (R1)\nC〈ZT θ∗(C), aixi〉 < biyi ⇒ [θ∗(C)]i = β ⇔ i ∈ L. (R2)\nHowever, (R1) and (R2) are generally not applicable since θ∗(C) is unknown. To overcome this difficulty, we can estimate a region Θ such that θ∗(C) ∈ Θ. As a result, we obtain the relaxed version of (R1) and (R2):\nmin θ∈Θ\nC〈ZT θ, aixi〉 > biyi ⇒ [θ∗(C)]i = α⇔ i ∈ R; (R1′)\n1Please refer to the supplement for details.\nmax θ∈Θ\nC〈ZT θ, aixi〉 < biyi ⇒ [θ∗(C)]i = β ⇔ i ∈ L. (R2′)\nNotice that, (R1′) and (R2′) serve as the foundation of the proposed DVI rules and the method in [20]. In the subsequent sections, we first estimate the region Θ which includes θ∗(C), and then derive the screening rules based on (R1′) and (R2′). Method to solve problem (15)\nIt is known that, problem (15) can be efficiently solved by the dual coordinate descent method [16]. More precisely, the optimization procedure starts from an initial point θ̂0 ∈ <|Ŝc| and generates a sequence of points {θ̂k}∞k=0. The process from θ̂k to θ̂k+1 is referred to as an outer iteration. In each outer iteration, we update the components of θ̂k one at a time and thus get a sequence of points θ̂k,i ∈ <|Ŝc|, i = 1, . . . , |Ŝc|. Suppose we are at the kth outer iteration. To get θ̂k,i from θ̂k,i−1, we need to solve the following optimization problem:\nmin t\nC 2 (θ̂k,i−1 + tei) T Ĝ11(θ̂ k,i−1 + tei)− ŷT (θ̂k,i−1 + tei) (16)\ns.t. [θ̂k,i−1]i + t ∈ [α, β], i = 1, . . . , l,\nwhere ei = (0, . . . , 1, . . . , 0) T . Clearly, problem (16) is equivalent to the following 1D optimization\nproblem:\nmin t\nC 2 [Ĝ11]i,it 2 + (CeTi Ĝ11θ̂ k,i−1 − [ŷ]i)t (17)\ns.t. [θ̂k,i−1]i + t ∈ [α, β],\nwhich admits a closed form solution t∗. Once t∗ is available, we can set θ̂k,i = θ̂k,i−1 + t∗ei. For more details, please refer to [16].\nIn Section 3, we first give an accurate estimation of the set Θ which includes θ∗(C) as in (R1′) and (R2′) via the variational inequalities. Then in Section 4, we present the novel DVI rules for problem (3) in detail."
    }, {
      "heading" : "3 Estimation of the Dual Optimal Solution",
      "text" : "For problem (12), suppose we are given two parameter values 0 < C0 < C and θ ∗(C0) is known. Then, Theorem 6 shows that θ∗(C) can be effectively bounded in terms of θ∗(C0). The main technique we use is the so called variational inequalities. For self-completeness, we cite the definition of variational inequalities as follows.\nTheorem 5. [12] Let A ⊆ <n be a convex set, and let h be a Gâteaux differentiable function on an open set containing A. If x∗ is a local minimizer of h on A, then\n〈∇h(x∗),x− x∗〉 ≥ 0, ∀x ∈ A. (18)\nVia the variational inequalities, the following theorem shows that θ∗(C) can estimated in terms of θ∗(C0).\nTheorem 6. For problem (12), let C > C0 > 0. Then\n‖ZT θ∗(C)− C0+C2C Z T θ∗(C0)‖ ≤ C−C02C ‖Z T θ∗(C0)‖.\nProof. Let g(θ) be the objective function of problem (12). The variational inequality implies that\n〈∇g(θ∗(C0)), θ − θ∗(C0)〉 ≥ 0, ∀θ ∈ [α, β]l; (19)\n〈∇g(θ∗(C)), θ − θ∗(C)〉 ≥ 0, ∀θ ∈ [α, β]l. (20)\nNotice that ∇g(θ) = CZZT θ − ȳ, and θ∗(C0) ∈ [α, β]l and θ∗(C) ∈ [α, β]l. Plugging ∇g(θ∗(C)) and ∇g(θ∗(C0)) into (19) and (20) leads to\n〈C0ZZT θ∗(C0)− ȳ, θ∗(C)− θ∗(C0)〉 ≥ 0; (21)\n〈CZZT θ∗(C)− ȳ, θ∗(C0)− θ∗(C)〉 ≥ 0. (22)\nWe can see that the inequality in (22) is equivalent to\n〈ȳ − CZZT θ∗(C), θ∗(C)− θ∗(C0)〉 ≥ 0. (23)\nThen the statement follows by adding the inequalities in (21) and (23) together."
    }, {
      "heading" : "4 The Proposed DVI Rules",
      "text" : "Given C > C0 > 0 and θ ∗(C0), we can estimate θ ∗(C) via Theorem 6. Combining (R1′), (R2′) and Theorem (6), we develop the basic screening rule for problem (3) as summarized in the following theorem:\nTheorem 7. (DVI) For problem (12), suppose we are given θ∗(C0). Then, for any C > C0, we have [θ∗(C)]i = α, i.e., i ∈ R, if the following holds\nC+C0 2 〈Z T θ∗(C0), aixi〉 − C−C02 ‖Z T θ∗(C0)‖‖aixi‖ > biyi.\nSimilarly, we have [θ∗(C)]i = β, i.e., i ∈ L, if\nC+C0 2 〈Z T θ∗(C0), aixi〉+ C−C02 ‖Z T θ∗(C0)‖‖aixi‖ < biyi.\nProof. We will prove the first half of the statement. The second half can be proved analogously. To show [θ∗(C)]i = α, i.e., i ∈ R, (R1) implies that we only need to show C〈ZT θ∗(C), aixi〉 > biyi. Thus, we can see that\nC〈ZT θ∗(C), aixi〉 =C 〈 ZT θ∗(C)− C0+C2C Z T θ∗(C0), aixi 〉 + C 〈 C0+C 2C Z T θ∗(C0), aixi 〉 ≥C0+C2 〈Z T θ∗(C0), aixi〉 − C ∥∥ZT θ∗(C)− C0+C2C XT θ∗(C0)∥∥ ‖aixi‖\n≥C0+C2 〈Z T θ∗(C0), aixi〉 − C−C02 ‖Z T θ∗(C0)‖‖aix‖ >biyi.\nNote that, the second inequality is due to Theorem 6, and the last line is due to the statement. This completes the proof.\nIn real applications, the optimal parameter value of C is unknown and we need to estimate it. Commonly used model selection strategies such as cross validation and stability selection need to solve the optimization problems over a grid of turning parameters 0 < C1 < C2 < . . . < CK to determine an appropriate value for C. This procedure is usually very time consuming, especially for large scale problems. To this end, we propose a sequential version of the proposed DVI below.\nCorollary 8. (DVI∗s) For problem (12), suppose we are given a sequence of parameters 0 < C1 < C2 < . . . < CK. Assume θ\n∗(Ck) is known for an arbitrary integer 1 ≤ k < K. Then, for Ck+1, we have [θ∗(Ck+1)]i = α, i.e., i ∈ R, if the following holds\nCk+1+Ck 2 〈Z T θ∗(Ck), aixi〉 − Ck+1−Ck2 ‖Z T θ∗(Ck)‖‖aixi‖ > biyi.\nSimilarly, we have [θ∗(Ck+1)]i = β, i.e., i ∈ L, if\nCk+1+Ck 2 〈Z T θ∗(Ck), aixi〉+ Ck+1−Ck2 ‖Z T θ∗(Ck)‖‖aixi‖ < biyi.\nThe main computational cost of DVI∗s is due to the evaluation of 〈ZT θ∗(Ck), aixi〉, ‖ZT θ∗(Ck)‖ and ‖aixi‖. Let G = ZZT . It is easy to see that\n〈ZT θ∗(Ck), aixi〉 = gTi θ∗(Ck), ‖ZT θ∗(Ck)‖2 = θ∗(Ck)TGθ∗(Ck), ‖x̄i‖2 = [G]i,i.\nwhere gi is the i th column of G. Since G is independent of Ck, it can be computed only once and thus the computational cost of DVI∗s reduces to O(l 2) to scan the entire data set. Indeed, by noting Eq. (13), we can reconstruct DVI rules without the explicit computation of G.\nCorollary 9. (DVIs) For problem (3), suppose we are given a sequence of parameters 0 < C1 < C2 < . . . < CK. Assume w\n∗(Ck) is known for an arbitrary integer 1 ≤ k < K. Then, for Ck+1, we have [θ∗(Ck+1)]i = α, i.e., i ∈ R, if the following holds\n− Ck+Ck+12Ck 〈w ∗(Ck), aixi〉 − Ck+1−Ck2Ck ‖w ∗(Ck)‖‖aixi‖ > biyi.\nSimilarly, we have [θ∗(Ck+1)]i = β, i.e., i ∈ L, if\n− Ck+Ck+12Ck 〈w ∗(Ck), aixi〉+ Ck+1−Ck2Ck ‖w ∗(Ck)‖‖aixi‖ < biyi."
    }, {
      "heading" : "5 Screening Rules for SVM",
      "text" : "In Section 5.1, we first present the sequential DVI rules for SVM based on the results in Section 4. Then, in Section 5.2, we show how to strictly improve SSNSV [20] by the same technique used in DVI."
    }, {
      "heading" : "5.1 DVI rules for SVM",
      "text" : "Given a set of observations {xi, yi}li=1, where xi and yi ∈ {1,−1} are the ith data instance and the corresponding class label, the SVM takes the form of:\nmin w\n1 2 ‖w‖2 + C l∑ i=1 [ 1−wT (yixi) ] + . (24)\nIt is easy to see that, if we set ϕ(t) = [t]+ and −ai = bi = yi, problem (3) becomes the SVM problem. To construct the DVI rules for SVM by Corollaries 8 and 9, we only need to find α and β. In fact, we have the following result:\nLemma 10. Let ϕ(t) = [t]+, then α = 0 and β = 1, i.e.,\nϕ∗(s) = ι[0,1]. (25)\nWe omit the proof of Lemma 10 since it is a direct application of Eq. (1). Then, we immediately have the following screening rules for the SVM problem. (For notational convenience, let x̄i = yixi and X = (x̄1, . . . , x̄l) T .)\nCorollary 11. (DVI∗s for SVM) For problem (24), suppose we are given a sequence of parameters 0 < C1 < C2 < . . . < CK. Assume θ\n∗(Ck) is known for an arbitrary integer 1 ≤ k < K. Then, for Ck+1, we have [θ∗(Ck+1)]i = 0, i.e., i ∈ R, if the following holds\nCk+1+Ck 2 〈X T θ∗(Ck), x̄i〉 − Ck+1−Ck2 ‖X T θ∗(Ck)‖‖x̄i‖ > 1.\nSimilarly, we have [θ∗(Ck+1)]i = 1, i.e., i ∈ L, if\nCk+1+Ck 2 〈X T θ∗(Ck), x̄i〉+ Ck+1−Ck2 ‖X T θ∗(Ck)‖‖x̄i‖ < 1.\nCorollary 12. (DVIs for SVM) For problem (24), suppose we are given a sequence of parameters 0 < C1 < C2 < . . . < CK. Assume w\n∗(Ck) is known for an arbitrary integer 1 ≤ k < K. Then, for Ck+1, we have [θ∗(Ck+1)]i = 0, i.e., i ∈ R, if the following holds\nCk+Ck+1 2Ck 〈w∗(Ck), x̄i〉 − Ck+1−Ck2Ck ‖w ∗(Ck)‖‖x̄i‖ > 1.\nSimilarly, we have [θ∗(Ck+1)]i = 1, i.e., i ∈ L, if Ck+Ck+1\n2Ck 〈w∗(Ck), x̄i〉+ Ck+1−Ck2Ck ‖w ∗(Ck)‖‖x̄i‖ < 1."
    }, {
      "heading" : "5.2 Improving the existing method",
      "text" : "In the rest of this section, we briefly describe how to strictly improve SSNSV [20] by using the same technique used in DVI rules (please refer to the supplement for more details). In view of Eq. (13), (R1′) and (R2′) can be rewritten as:\nmin w∈Ω 〈w, x̄i〉 > 1⇒ [θ∗(C)]i = 0⇔ i ∈ R, (R1′′) max w∈Ω 〈w, x̄i〉 < 1⇒ [θ∗(C)]i = 1⇔ i ∈ L, (R2′′)\nwhere Ω is a set which includes w∗(C) (notice that, we have already set −ai = bi = yi, α = 0 and β = 1). It is easy to see that, the smaller Ω is, the tighter the bounds are in (R1′′) and (R2′′). Thus, more data instances’ membership can be identified.\nEstimation of w∗ in SSNSV In [20], the authors consider the following equivalent formulation of SVM:\nmin w\n1 2 ‖w‖2, s.t. l∑ i=1 [1− yiwTxi]+ ≤ s (26)\nLet Fs = {w : ∑l i=1[1−yiwTxi]+ ≤ s}. Suppose we have two scalars sa > sb > 0, and Fsb 6= ∅, ŵ(sb) ∈ Fsb . Then for s ∈ [sb, sa], w∗(s) is inside the following region:\nΩ[sb,sa] :=\n{ w :\n〈w∗(sa),w −w∗(sa)〉 ≥ 0, ‖w‖2 ≤ ‖ŵ(sb)‖2\n} (27)\nEstimation of w∗ via VI By using the same technique as in DVI, we can conclude that w∗(s) is inside the region:\nΩ′[sb,sa] :=\n{ w :\n〈w∗(sa),w −w∗(sa)〉 ≥ 0, ‖w − 12ŵ(sb)‖ ≤ 1 2‖ŵ(sb)‖\n} (28)\nWe can see that Ω′[sb,sa] ⊂ Ω[sb,sa], and thus SSNSV can be strictly improved by the estimation in (28). The rule based on Ω′[sb, sa] is presented in Theorem 19 in the supplement, which is call the “enhanced” SSNSV (ESSNSV)."
    }, {
      "heading" : "6 Screening Rules for LAD",
      "text" : "In this section, we extend DVI rules in Section 4 to the least absolute deviations regression (LAD). Suppose we have a training set {xi, yi}li=1, where xi ∈ <n and yi ∈ <. The LAD problem takes the form of\nmin w\n1 2 ‖w‖2 + C l∑ i=1 |yi −wTxi|. (29)\nWe can see that, if we set ϕ(t) = |t| and −ai = bi = 1, problem (3) becomes the LAD problem. To construct the DVI rules for LAD based on Corollaries 8 and 9, we need to find α and β. Indeed, we have the following result:\nLemma 13. Let ϕ(t) = |t|, then α = −1 and β = 1, i.e.,\nϕ∗(s) = ι[−1,1]. (30)\nWe again omit the proof of Lemma 13 since it is a direct application of Eq. (1). Then, it is straightforward to derive the sequential DVI rules for the LAD problem.\nCorollary 14. (DVI∗s for LAD) For problem (29), suppose we are given a sequence of parameter values 0 < C1 < C2 < . . . < CK. Assume θ\n∗(Ck) is known for an arbitrary integer 1 ≤ k < K. Then, for Ck+1, we have [θ∗(Ck+1)]i = −1 or 1, i.e., i ∈ R or i ∈ L, if the following holds respectively\n1. Ck+1+Ck2 〈X T θ∗(Ck),xi〉 − Ck+1−Ck2 ‖X T θ∗(Ck)‖‖xi‖ > yi. 2. Ck+1+Ck2 〈X T θ∗(Ck),xi〉+ Ck+1−Ck2 ‖X T θ∗(Ck)‖‖xi‖ < yi.\nCorollary 15. (DVIs for LAD) For problem (29), suppose we are given a sequence of parameter values 0 < C1 < C2 < . . . < CK. Assume w\n∗(Ck) is known for an arbitrary integer 1 ≤ k < K. Then, for Ck+1, we have [θ∗(Ck+1)]i = −1 or 1, i.e., i ∈ R or i ∈ L, if the following holds respectively\n1. Ck+1+Ck2Ck 〈w ∗(Ck),xi〉 − Ck+1−Ck2Ck ‖w ∗(Ck)‖‖xi‖ > yi, 2. Ck+1+Ck2Ck 〈w ∗(Ck),xi〉+ Ck+1−Ck2Ck ‖w ∗(Ck)‖‖xi‖ < yi.\nTo the best of our knowledge, ours are the first screening rules for LAD."
    }, {
      "heading" : "7 Experiments",
      "text" : "We evaluate DVI rules on both synthetic and real data sets. To measure the performance of the screening rules, we compute the rejection rate, that is, the ratio between the number of data instances whose membership can be identified by the rules and the total number of data instances. We test the rules along a sequence of 100 parameters of C ∈ [10−2, 10] equally spaced in the logarithmic scale.\nIn Section 7.1, we compare the performance of DVI rules with SSNSV [20], which is the only existing method for identifying non-support vectors in SVM. Notice that, both of DVI rules and SSNSV are safe in the sense that no support vectors will be mistakenly discarded. We then evaluate DVI rules for LAD in Section 7.2."
    }, {
      "heading" : "7.1 DVI for SVM",
      "text" : "In this experiment, we first apply DVIs to three simple 2D synthetic data sets to illustrate the effectiveness of the proposed screening methods. Then we compare the performance of DVIs, SSNSV and ESSNSV on: (a) IJCNN1 data set [23]; (b) Wine Quality data set [8]; (c) Forest Covertype data set [14]. The original Forest Covertype data set includes 7 classes. We randomly pick two of the seven classes to construct the data set used in this paper.\nSynthetic Data Sets In this experiment, we show that DVIs are very effective in discarding nonsupport vectors even for largely overlapping classes. We evaluate DVIs rules on three synthetic data sets, i.e., Toy1, Toy2 and Toy3, plotted in the first row of Fig. 1. For each data set, we generate two classes. Each class has 1000 data points and is generated from N({µ, µ}T , 0.752I), where I ∈ <2×2 is the identity matrix. For the positive classes (the red dots), µ = 1.5, 0.75, 0.5, for Toy1, Toy2 and Toy 3, respectively; and µ = −1.5,−0.75,−0.5, for the negative classes (the blue dots). From the plots, we can observe that when |µ| decreases, the two classes increasingly overlap and thus the number of data instances belong to the set L increases.\nThe second row of Fig. 1 presents the stacked area charts of the rejection rates. For convenience, let R̃ and L̃ be the indices of data instances which are identified by DVIs as members of R and L, respectively. Then, the blue and red regions present the ratios of |R̃|/l and |L̃|/l (recall that, l is the number of data\ninstances, which is 2000 for this experiment). We can see that, for Toy1, the two classes are clearly apart from each other and thus most of the data instances belong to the set R. The first chart in the second row of Fig. 1 indicates that the proposed DVIs can identify almost all of the non-support vectors and thus the speedup is almost 60 times compared to the solver without screening (please refer to Table 1). When the two classes have a large overlap, e.g., Toy3, the number of data instances in L significantly increases. This will generally impose great challenge for the solver. But even for this challenging case, DVIs is still able to identify a large portion of the non-support vectors as indicated by the last charts in the second row of Fig. 1. Notice that, for Toy3, |L̃| is comparable to |R̃|. Table 1 shows that the speedup gained by DVIs is about 25 times for this challenging case. It is worthwhile to mention that the running time of “Solver+DVIs” in Table 1 includes the running time (the 5th column of Table 1) for solving SVM with the smallest parameter value.\nReal Data Sets In this experiment, we compare the performance of SSNSV, ESSNSV and DVIs in terms of the rejection ratio, that is, the ratio between the number of data instances identified as members\nof R or L by the screening rules and the number of total data instances. Fig. 2 shows the rejection ratios of the three screening rules on three real data sets. We can observe that DVIs rules identify far more nonsupport vectors than SSNSV and ESSNSV. For IJCNN1, about 80% of the data instances are identified as non-support vectors by DVIs. Therefore, as indicated by Table 2 the speedup gained by DVIs is about 5 times. For the Wine data set, more than 80% of the data instances are identified to belong to R or L by DVIs. As indicated in Table 2, the speedup is about 6 times gained by DVIs. For the Forest Covertype data set, almost all of data instances’ membership can be determined by DVIs. Table 2 shows that the speedup gained by DVIs is almost 80 times, which is much higher than that of SSNSV and ESSNSV. Moreover, Fig. 2 demonstrates that ESSNSV is more effective in identifying non-support vectors than SSNSV, which is consistent with our analysis."
    }, {
      "heading" : "7.2 DVI for LAD",
      "text" : "In this experiment, we evaluate the performance of DVIs for LAD on three real data sets: (a) Magic Gamma Telescope data set [2]; (b) Computer data set [25]; (c) Houses data set [21]. Fig. 3 shows the rejection ratio of DVIs rules for the three data sets. We can observe that the rejection ratio of DVIs on Magic Gamma Telescope data set is about 90%, leading to a 10 times speedup as indicated in Table 3. For the Computer and Houses data sets, we can see that the rejection rates are very close to 100%, i.e., almost all of the data instances’ membership can be determined by the DVIs rules. As expected, Table 3 shows that the resulting speedup are about 20 and 115 times, respectively. Notice that, the speedup for the Houses data set is more than two orders of magnitude. These results demonstrate the effectiveness of the proposed DVI rules."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we develop new screening rules for a class of supervised learning problems by studying their dual formulation with the variational inequalities. Our framework includes two well known models, i.e., SVM and LAD, as special cases. The proposed DVI rules are very effective in identifying non-support vectors for both SVM and LAD, and thus result in substantial savings in the computational cost and memory. Extensive experiments on both synthetic and real data sets demonstrate the effectiveness of the proposed DVI rules. We plan to extend the framework of DVI to other supervised learning problems, e.g., weighted SVM [32], RWLS (robust weighted least squres) [6], robust PCA [9], robust matrix factorization [19]."
    }, {
      "heading" : "A Proof of Lemma 2",
      "text" : "Before we prove Lemma 2, let us cite the following technical lemma.\nLemma 16. [15] The function f is equal to its biconjugate f∗∗ if and only if f ∈ Γ0(<n). We are now ready to derive a simple proof of Lemma 2 based on Lemma 16.\nProof. In order to show ϕ∗∗ = ϕ, it is enough to show ϕ ∈ Γ0(<) according to Lemma 16. Therefore we only to check the following three conditions:\n1). Properness: because ϕ : < → <+, i.e., there exists t ∈ < such that ϕ(t) is finite, ϕ is proper. 2). Lower semi-continuality: ϕ is lower semicontinuous because it is continuous. 3). Convexity: the convexity of ϕ is due to the its sublinearity, see Definition 1. Thus, we have ϕ ∈ Γ0(<), which completes the proof."
    }, {
      "heading" : "B Proof of Lemma 3",
      "text" : "To prove Lemma 3, we need to following results.\nLemma 17. [26] Let Z ⊆ <n be a convex and closed set. Let us define the support function of Z as\nσZ(s) := sup x∈Z\nsTx, (31)\nand the indicator function ιZ as\nιZ(x) = { 0, if x ∈ Z, ∞, otherwise.\n(32)\nThen\nσ∗Z = ιZ , amd ι ∗ Z = σZ . (33)\nTheorem 18. [15] Let σ ∈ Γ0(<n) be a sublinear function, then σ is the support function of the nonempty closed convex set\nSσ := {s ∈ <n : sTd ≤ σ(d), ∀d ∈ <n}. (34)\nWe are now ready to prove Lemma 3.\nProof. Due to Lemma 17 and Theorem 18, we can see that, there is a nonempty closed convex set Z ⊆ < such that\nϕ(t) = sup s∈Z\nst, ∀t ∈ <, (35)\nwhere\nZ := {s : st ≤ ϕ(t), ∀t ∈ <}. (36)\nLet t = 1 and −1 respectively, Eq. (36) implies that\nsup s∈Z s ≤ ϕ(1) and inf s∈Z s ≥ ϕ(−1). (37)\nTherefore, Z is a closed and bounded interval, i.e., Z = [α, β] with α, β ∈ <. Next, let us show that α 6= β. In fact, in view of the nonnegativity of ϕ and Eq. (36), it is easy to see that 0 ∈ Z. Therefore, if α = β, we must have Z = {0}. Thus, Lemma 17 implies that\nϕ = ι∗Z ≡ 0, (38)\nwhich contradicts the fact that ϕ is a nonconstant function. Hence, we can conclude that α < β, which completes the proof."
    }, {
      "heading" : "C Derivation of the KKT Condition in Eq. (14)",
      "text" : "The problem in (12) can be written as follows:\nmin θ\nC 2 ‖ZT θ‖2 − 〈ȳ, θ〉, (39)\ns.t. θi ∈ [α, β], i = 1, . . . , l.\nTherefore, we can see that the Lagrangian is\nL(θ, µ, ν) = C\n2 ‖ZT θ‖2 − 〈ȳ, θ〉+ l∑ i=1 µi(α− θi) + l∑ i=1 νi(θi − β), (40)\nwhere µ = (µ1, . . . , µl) T , ν = (ν1, . . . , νl) T , and µi ≥ 0, νi ≥ 0 for all i = 1, . . . , l. µ and ν are in fact the vector of Lagrangian multipliers.\nFor simplicity, let us denote θ∗(C) by θ∗. Then the KKT conditions [3] are\n∂L(θ, µ, ν)\n∂θ |θ∗ = 0⇒ CZZT θ∗ − ȳ − µ+ ν = 0, (41)\nµi(α− θ∗i ) = 0, νi(θ ∗ i − β) = 0, i = 1, . . . , l. (42)\nEq. (42) is known as the complementary slackness condition. The equation in (41) actually involves l equations. We can write down the ith equation as follows:\nC〈ZT θ∗, aixi〉 − µi + νi = biyi. (43)\nRecall that the ith column of Z is aixi. In view of Eq. (42) and Eq. (43), we can see that: 1. if θ∗i = α, then νi = 0 and Eq. (43) results in\nC〈ZT θ∗, aixi〉 ≥ biyi; (44)\n2. if θ∗i ∈ (α, β), then µi = νi = 0 and Eq. (43) results in\nC〈ZT θ∗, aixi〉 = biyi; (45)\n3. if θ∗i = β, then µi = 0 and Eq. (43) results in\nC〈ZT θ∗, aixi〉 ≤ biyi. (46)\nThen, in view of the inequalities in (44), (45) and (46), and Eq. (13), it is straightforward to derive the KKT condition in (14)."
    }, {
      "heading" : "D Proof of Lemma 4",
      "text" : "Proof. The first part of the statement is trivial by the definition of R̂ and L̂. Therefore, we only consider the second part of the statement.\nLet G = ZZT . By permuting the columns and rows of G, we have\nĜ = ( Ĝ11 Ĝ12 Ĝ21 Ĝ22 ) = ( [XT ]TŜc [X T ]Ŝc [X T ]TŜc [X T ]Ŝ\n[XT ]TŜ [X T ]Ŝc [X T ]TŜ [X T ]Ŝ\n) .\nAs a result, the objective function of problem (12) can be rewritten as\nC 2 [θ]TŜcĜ11[θ]Ŝc − ŷ T [θ]Ŝc +R([θ]Ŝ) (47)\nwhere\nŷ = yŜc − CĜ12[θ]Ŝ , , (48)\nR([θ]Ŝ) = C\n2 [θ]TŜ Ĝ22[θ]Ŝ − y T Ŝ [θ]Ŝ (49)\nDue to the assumption that [θ∗(C)]Ŝ is known, ŷ and R([θ]Ŝ) can be treated as constants, and thus problem (12) reduces to problem (15).\nE Improving SSNSV via VI\nIn this section, we describe how to strictly improve SSNSV by using the same technique used in DVI rules in a detailed manner.\nEstimation of w∗ via VI We show that Ω[sb,sa] in Eq. (27) can be strictly improved by the variational inequalities. Consider Fsa .\nBecause sa > sb, we can see that w ∗(sb) ∈ Fsa . Therefore, by Theorem 5, we have\n〈w∗(sa),w∗(s)−w∗(sa)〉 ≥ 0, (50)\nwhich is the first constraint in (27). Similarly, consider Fsb . Since ŵ(sb) ∈ Fsb , Theorem 5 implies that\n〈w∗(s), ŵ(sb)−w∗(s)〉 ≥ 0,\nwhich is equivalent to\n‖w∗(s)− 12ŵ(sb)‖ ≤ 1 2‖ŵ(sb)‖. (51)\nClearly, the radius determined by the inequality (51) is only a half of the radius determined by the second constraint in (27). In view of the inequalities in (50) and (51), we can see that w∗(s) can be bounded inside the following region:\nΩ′[sb,sa] :=\n{ w :\n〈w∗(sa),w −w∗(sa)〉 ≥ 0, ‖w − 12ŵ(sb)‖ ≤ 1 2‖ŵ(sb)‖ } It is easy to see that Ω′[sb,sa] ⊂ Ω[sb,sa]. As a result, the bounds in (R1 ′) and (R2′) with Ω′[sb,sa] are tighter than that of Ω[sb,sa]. Thus, SSNSV [20] can be strictly improved by the estimation in (28). In fact, we have the following theorem:\nTheorem 19. Suppose we are given two parameters sa > sb > 0, and let w ∗(sa) and ŵ(sb) be the optimal solution at s = sa and a feasible solution at s = sb, respectively. Moreover, let us define\nρ = −‖w∗(sa)‖2 + 12 〈w ∗(sa), ŵ(sb)〉\nv⊥ = v − v Tw∗(sa) ‖w∗(sa)‖2 w ∗(sa),∀v ∈ <n.\nThen, for all s ∈ [sb, sa],\n〈w∗(sa), x̄i〉 > 2‖x̄i‖‖ŵ(sb)‖ρ and `i > 1⇒ i ∈ R ⇔ αi = 0, (52)\nwhere\n`i = − 〈w ∗(sa),x̄i〉 ‖w∗(sa)‖2 ρ+ 1 2 〈ŵ(sb), x̄i〉 − ‖x̄ ⊥ i ‖ √ 1 4 ‖ŵ(sb)‖2 − ρ 2 ‖w∗(sa)‖2 . (53)\nSimilarly,\nui < 1⇒ i ∈ L ⇔ αi = c, (54)\nwhere\nui =  1 2 (〈ŵ(sb), x̄i〉+ ‖ŵ(sb)‖‖x̄i‖) , if 〈w∗(sa), x̄i〉 ≥ − 2‖x̄i‖‖ŵ(sb)‖ρ − 〈w ∗(sa),x̄i〉 ‖w∗(sa)‖2 ρ+ 1 2 〈ŵ(sb), x̄i〉 +‖x̄⊥i ‖ √ 1 4‖ŵ(sb)‖2 − ρ2 ‖w∗(sa)‖2 ,\notherwise.\n(55)\nFor convenience, we call the screening rule presented in Theorem 19 as the “enhanced” SSNSV (ESSNSV). To prove Theorem 19, we first establish the following technical lemma.\nLemma 20. Consider the problem as follows:\nmin w\nf(w) = vTw, s.t. uTw ≤ d, ‖w − o‖ ≤ r, (56)\nwhere r > 0. Let d′ = d− uTo and the optimal solution of problem (56) be f∗. Then we have\n1. If vTu + ‖v‖d ′\nr ≥ 0, then\nf∗ = vTo− r‖v‖.\n2. Otherwise,\nf∗ = vTo− ‖v⊥‖ √ r2 − (d ′)2\n‖u‖2 +\nvTud′\n‖u‖2 ,\nwhere v⊥ = v − v Tu ‖u‖2 u.\nNotice that, we assume problem (56) is feasible, i.e., |u T o−d| ‖u‖ ≤ r.\nProof. Let z = w − o, problem (56) can be rewritten as:\nmin z\nvT z + vTo, s.t. uT z ≤ d− uTo, ‖z‖ ≤ r. (57)\nProblem (57) reduces to\nmin z\nvT z, s.t. uT z ≤ d′, ‖z‖ ≤ r. (58)\nTo solve problem (58), we make use of the Lagrangian multiplier method. For notational convenience, let F := {z : uT z ≤ d′, ‖z‖ ≤ r}.\nmin z∈F vT z = min z max µ≥0, ν≥0\nvT z + ν(uT z− d′) + µ 2 (‖z‖2 − r2)\n= max µ≥0, ν≥0 min z\nvT z + ν(uT z− d′) + µ 2 (‖z‖2 − r2)\n= max µ≥0, ν≥0\n− 1 2µ ‖v + νu‖2 − νd′ − µr\n2\n2 . (59)\nNotice that, in Eq. (59), we make the assumption that µ > 0. However, we can not simply exclude this possibility. In fact, if µ = 0, we must have\nv + νu = 0, (60)\nsince otherwise the function value of vT z + ν(uT z− d′)\nin the second line of Eq. (59) can be made arbitrarily small. As a result, we will have\ng(µ, ν) = −∞,\nwhich contradicts the strong duality of problem (56) [3]. [Problem (56) is clearly lower bounded since the feasible set is compact.] Therefore, in view of Eq. (60), we can conclude that µ = 0 only if v point in the opposite direction of u.\nLet us first consider the general case, i.e., v does not point in the opposite direction of u. In view of\nEq. (59), let g(µ, ν) = − 12µ‖v + νu‖ 2 − νd′ − µr\n2\n2 . It is easy to see that\n∂g(µ, ν) ∂ν = 0⇔ ν = −v\nTu + µd′\n‖u‖2 . (61)\nSince ν has to be nonnegative, we have\nν = max { 0,−v Tu + µd′\n‖u‖2\n} . (62)\nCase 1. If −v Tu+µd′\n‖u‖2 ≤ 0, then ν = 0 and thus\n∂g(µ, ν) ∂µ = 0⇔ µ = ‖v‖ r . (63)\nThen g(µ, ν) = −r‖v‖ and the optimal value of problem (56) is given by\nvTo− r‖v‖. (64)\nCase 2. If −v Tu+µd′ ‖u‖2 > 0, then ν = − vTu+µd′ ‖u‖2 and\ng(µ, ν) = − 1 2µ ‖v⊥‖2 − µ 2\n( r2 − (d ′)2\n‖u‖2\n) + vTud′\n‖u‖2 , (65)\nThus,\n∂g(µ, ν) ∂µ = 0⇔ µ = ‖v ⊥‖√ r2 − (d ′)2\n‖u‖2\n(66)\nThen g(µ, ν) = −‖v⊥‖ √ r2 − (d ′)2\n‖u‖2 + vTud′ ‖u‖2 and the optimal value of problem (56) is given by\nvTo− ‖v⊥‖ √ r2 − (d ′)2\n‖u‖2 +\nvTud′\n‖u‖2 . (67)\nNow let us consider the case with v pointing in the opposite direction of u. We can see that there exists γ = − u Tv ‖u‖2 > 0 such that v = −γu. By plugging v = −γu in problem (56) and following an analogous argument as before, we can see that the statement in Lemma 20 is also applicable to this case. Therefore, the proof of the statement is completed.\nWe are now ready to prove Theorem 19.\nProof. To prove the statements in (52) and (53), we only need to set\nv := x̄i, u : −w∗(sa), d := −‖w∗(sa)‖2, o := 12ŵ, r := 1 2‖ŵ‖,\nd′ := ρ = −‖w∗(sa)‖2 + 1\n2 〈w∗(sa), ŵ〉,\nand then apply Lemma 20. Notice that, for case 1, the optimal value\nf∗ = 1\n2 (〈ŵ(sb), x̄i〉 − ‖ŵ(sb)‖‖x̄i‖) ≤ 0,\nand thus none of the non-support vectors can be identified [recall that, according to (R1′), f∗ has to be larger than 1 such that x̄i can be detected as a non-support vector]. As a result, we only need to consider case 2.\nThe statement in (54) and (55) follows with an analogous argument by noting that\nmax w∈Θ′\n[sb,sa]\n〈w, x̄i〉 = − min w∈Θ′\n[sb,sa]\n−〈w, x̄i〉."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "<lb>The support vector machine (SVM) is a widely used method for classification. Although many<lb>efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-<lb>scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting<lb>classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-<lb>support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result,<lb>the number of data instances to be entered into the optimization can be substantially reduced. Some<lb>appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded<lb>by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run<lb>the screening, whose computational cost is negligible compared to that of solving the SVM problem; (3)<lb>DVI is independent of the solvers and can be integrated with any existing efficient solvers. We also show<lb>that the DVI technique can be extended to detect non-support vectors in the least absolute deviations<lb>regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD. We<lb>have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly<lb>outperforms the existing state-of-the-art screening rules for SVM, and is very effective in discarding<lb>non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.",
    "creator" : "LaTeX with hyperref package"
  }
}