{
  "name" : "1611.03819.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates",
    "authors" : [ "Yuanzhi Li", "Yingyu Liang", "Andrej Risteski" ],
    "emails" : [ "risteski}@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n03 81\n9v 1\n[ cs\n.L G\n] 1\n1 N"
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we study the problem of non-negative matrix factorization (NMF), where given a matrix Y ∈ Rm×N , the goal to find a matrix A ∈ Rm×n and a non-negative matrix X ∈ Rn×N such that Y ≈ AX.1 A is often referred to as feature matrix and X referred as weights. NMF has been extensively used in extracting a parts representation of the data (e.g., [LS97, LS99, LS01]). Empirically it is observed that the non-negativity constraint on the coefficients forcing features to combine, but not cancel out, can lead to much more interpretable features and improved downstream performance of the learned features.\nDespite all the practical success, however, this problem is poorly understood theoretically, with only few provable guarantees known. Moreover, many of the theoretical algorithms are based on heavy tools from algebraic geometry (e.g., [AGKM12]) or tensors (e.g. [AKF+12]), which are still not as widely used in practice primarily because of computational feasibility issues or sensitivity to assumptions on A and X. Some others depend on specific structure of the feature matrix, such as separability [AGKM12] or similar properties [BGKP16].\nA natural family of algorithms for NMF alternate between decoding the weights and updating the features. More precisely, in the decoding step, the algorithm represents the data as a non-negative combination of the current set of features; in the updating step, it updates the features using the decoded representations. This meta-algorithm is popular in practice due to ease of implementation, computational efficiency, and empirical quality of the recovered features. However, even less theoretical analysis exists for such algorithms.\nThis paper proposes an algorithm in the above framework with provable recovery guarantees. To be specific, the data is assumed to come from a generative model y = A∗x∗ + ν. Here, A∗\n1In the usual formulation of the problem, A is also assumed to be non-negative, which we will not require in this paper.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nis the ground-truth feature matrix, x∗ are the non-negative ground-truth weights generated from an unknown distribution, and ν is the noise. Our algorithm can provably recover A∗ under mild conditions, even in the presence of large adversarial noise.\nOverview of main results. The existing theoretical results on NMF can be roughly split into two categories. In the first category, they make heavy structural assumptions on the feature matrix A∗ such as separability ([AGM12]) or allowing running time exponential in n ( [AGKM12]). In the second one, they impose strict distributional assumptions on x∗ ([AKF+12]), where the methods are usually based on the method of moments and tensor decompositions and have poor tolerance to noise, which is very important in practice.\nIn this paper, we present a very simple and natural alternating update algorithm that achieves the best of both worlds. First, we have minimal assumptions on the feature matrix A∗: the only essential condition is linear independence of the features. Second, it is robust to adversarial noise ν which in some parameter regimes can potentially be on the same order as the signal A∗x∗, and is robust to unbiased noise potentially even higher than the signal by a factor of O( √ n). The algorithm does not require knowing the distribution of x∗, and allows a fairly wide family of interesting distributions. We get this at a rather small cost of a mild “warm start”. Namely, we initialize each of the features to be “correlated” with the ground-truth features. This type of initialization is often used in practice as well, for example in LDA-c, the most popular software for topic modeling ([lda16]).\nA major feature of our algorithm is the significant robustness to noise. In the presence of adversarial noise on each entry of y up to level Cν , the noise level ‖ν‖1 can be in the same order as the signal A\n∗x∗. Still, our algorithm is able to output a matrix A such that the final ‖A∗ −A‖1 ≤ O(‖ν‖1) in the order of the noise in one data point. If the noise is unbiased (i.e., E[ν|x∗] = 0), the noise level ‖ν‖1 can be Ω( √ n) times larger than the signal A∗x∗, while we can still guarantee ‖A∗ −A‖1 ≤\nO (‖ν‖1 √ n) – so our algorithm is not only tolerant to noise, but also has very strong denoising effect. Note that even for the unbiased case the noise can potentially be correlated with the groundtruth in very complicated manner, and also, all our results are obtained only requiring the columns of A∗ are independent.\nTechnical contribution. The success of our algorithm crucially relies on exploiting the nonnegativity of x∗ by a ReLU thresholding step during the decoding procedure. Similar techniques have been considered in prior works on matrix factorization, however to the best of our knowledge, the analysis (e.g., [AGMM15]) requires that the decodings are correct in all the intermediate iterations, in the sense that the supports of x∗ are recovered with no error. Indeed, we cannot hope for a similar guarantee in our setting, since we consider adversarial noise that could potentially be the same order as the signal. Our major technical contribution is a way to deal with the erroneous decoding throughout all the intermediate iterations. We achieve this by a coupling between two potential functions that capture different aspects of the working matrix A. While analyzing iterative algorithms like alternating minimization or gradient descent in non-convex settings is a popular topic in recent years, the proof usually proceeds by showing that the updates are approximately performing gradient descent on an objective with some local or hidden convex structure. Our technique diverges from the common proof strategy, and we believe is interesting in its own right.\nOrganization. After reviewing related work, we define the problem in Section 3 and describe our main algorithm in Section 4. To emphasize the key ideas, we first present the results and the proof sketch for a simplified yet still interesting case in Section 5, and then present the results under much more general assumptions in Section 6. The complete proof is provided in the appendix."
    }, {
      "heading" : "2 Related work",
      "text" : "Non-negative matrix factorization relates to several different topics in machine learning. We provide a high level review, and discuss in more details in the appendix.\nNon-negative matrix factorization. The area of non-negative matrix factorization (NMF) has a rich empirical history, starting with the practical algorithm of [LS97].On the theoretical side, [AGKM12] provides a fixed-parameter tractable algorithm for NMF, which solves algebraic equations and thus has poor noise tolerance. [AGKM12] also studies NMF under separability assumptions about the features. [BGKP16] studies NMF under heavy noise, but also needs assumptions related to separability, such as the existence of dominant features. Also, their noise model is different from ours.\nTopic modeling. A closely related problem to NMF is topic modeling, a common generative model for textual data [BNJ03, Ble12]. Usually, ‖x∗‖1 = 1 while there also exist work that assume x∗i ∈ [0, 1] and are independent [ZX12]. A popular heuristic in practice for learning A∗ is variational inference, which can be interpreted as alternating minimization in KL divergence norm. On the theory front, there is a sequence of works by based on either spectral or combinatorial approaches, which need certain “non-overlapping” assumptions on the topics. For example, [AGH+13] assume the topic-word matrix contains “anchor words”: words which appear in a single topic. Most related is the work of [AR15] who analyze a version of the variational inference updates when documents are long. However, they require strong assumptions on both the warm start, and the amount of “non-overlapping” of the topics in the topic-word matrix.\nICA. Our generative model for x∗ will assume the coordinates are independent, therefore our problem can be viewed as a non-negative variant of ICA with high levels of noise. Results here typically are not robust to noise, with the exception of [AGMS12] that tolerates Gaussian noise. However, to best of our knowledge, no result in this setting is provably robust to adversarial noise.\nNon-convex optimization. The framework of having a “decoding” for the samples, along with performing an update for the model parameters has proven successful for dictionary learning as well. The original empirical work proposing such an algorithm (in fact, it suggested that the V1 layer processes visual signals in the same manner) was due to [OF97]. Even more, similar families of algorithms based on “decoding” and gradient-descent are believed to be neurally plausible as mechanisms for a variety of tasks like clustering, dimension-reduction, NMF, etc ([PC15a, PC14]). A theoretical analysis came latter for dictionary learning due to [AGMM15] under the assumption that the columns of A∗ are incoherent. The technique is not directly applicable to our case, as we don’t wish to have any assumptions on the matrix A∗. For instance, if A∗ is non-negative and columns with l1 norm 1, incoherence effectively means the the columns of A∗ have very small overlap."
    }, {
      "heading" : "3 Problem definition and assumptions",
      "text" : "Given a matrix Y ∈ Rm×N , the goal of non-negative matrix factorization (NMF) is to find a matrix A ∈ Rm×n and a non-negative matrix X ∈ Rn×N , so that Y ≈ AX. The columns of Y are called data points, those of A are features, and those of X are weights. We note that in the original NMF, A is also assumed to be non-negative, which is not required here. We also note that typically m ≫ n, i.e., the features are a few representative components in the data space. This is different from dictionary learning where overcompleteness is often assumed.\nThe problem in the worst case is NP-hard [AGKM12], so some assumptions are needed to design provable efficient algorithms. In this paper, we consider a generative model for the data point\ny = A∗x∗ + ν (1)\nwhere A∗ is the ground-truth feature matrix, x∗ is the ground-truth non-negative weight from some unknown distribution, and ν is the noise. Our focus is to recover A∗ given access to the data distribution, assuming some properties of A∗, x∗, and ν. To describe our assumptions, we let [M]i denote the i-th row of a matrix M, [M]j its i-th column, Mi,j its (i, j)-th entry. Denote its column norm, row norm, and symmetrized norm as ‖M‖1 = maxj ∑ i |Mi,j |, ‖M‖∞ = maxi ∑ j |Mi,j |, and ‖M‖s = max {‖M‖1, ‖M‖∞} , respectively. We assume the following hold for parameters C1, c2, C2, ℓ, Cν to be determined in our theorems.\n(A1) The columns of A∗ are linearly independent.\n(A2) For all i ∈ [n], x∗i ∈ [0, 1], E[x∗i ] ≤ C1n and c2n ≤ E[(x∗i )2] ≤ C2n , and x∗i ’s are independent. (A3) The initialization A(0) = A∗(Σ(0) + E(0)) + N(0), where Σ(0) is diagonal, E(0) is off-\ndiagonal, and\nΣ (0) (1− ℓ)I, ∥∥∥E(0) ∥∥∥ s ≤ ℓ.\nWe consider two noise models.\n(N1) Adversarial noise: only assume that maxi |νi| ≤ Cν almost surely.\n(N2) Unbiased noise: maxi |νi| ≤ Cν almost surely, and E[ν|x∗] = 0.\nRemarks. We make several remarks about each of the assumptions. (A1) is the assumption about A∗. It only requires the columns of A∗ to be linear independent, which is very mild and needed to ensure identifiability. Otherwise, for instance, if (A∗)3 = λ1(A∗)1 + λ2(A ∗)2, it is impossible to distinguish between the case when x∗3 = 1 and the case when x ∗ 2 = λ1 and x∗1 = λ2. In particular, we do not restrict the feature matrix to be non-negative, which is more general than the traditional NMF and is potentially useful for many applications. We also do not make incoherence or anchor word assumptions that are typical in related work.\n(A2) is the assumption on x∗. First, the coordinates are non-negative and bounded by 1; this is simply a matter of scaling. Second, the assumption on the moments requires that, roughly speaking, each feature should appear with reasonable probability. This is expected: if the occurrences of the features are extremely unbalanced, then it will be difficult to recover the rare ones. The third requirement on independence is motivated by that the features should be different so that their occurrences are not correlated. Here we do not stick to a specific distribution, since the moment conditions are more general, and highlight the essential properties our algorithm needs. Example distributions satisfying our assumptions will be discussed later.\nThe warm start required by (A3) means that each feature A(0)i has a large fraction of the groundtruth feature A∗i and a small fraction of the other features, plus some noise outside the span of the ground-truth features. We emphasize that N(0) is the component of A(0) outside the column space of A∗, and is not the difference between A(0) and A∗. This requirement is typically achieved in practice by setting the columns of A(0) to reasonable “pure” data points that contains one major feature and a small fraction of some other features (e.g. [lda16, AR15]); in this initialization, it is generally believed that N(0) = 0. But we state our theorems to allow some noise N(0) for robustness in the initialization.\nThe adversarial noise model (N1) is very general, only imposing an upper bound on the entry-wise noise level. Thus, ν can be correlated with x∗ in some complicated unknown way. (N2) additionally requires it to be zero mean, which is commonly assumed and will be exploited by our algorithm to tolerate larger noise."
    }, {
      "heading" : "4 Main algorithm",
      "text" : "Algorithm 1 Purification\nInput: initialization A(0), threshold α, step size η, scaling factor r, sample size N , iterations T 1: for t = 0, 1, 2, ..., T − 1 do 2: Draw examples y1, . . . , yN . 3: (Decode) Compute A†, the pseudo-inverse of A(t) with minimum ‖(A)†‖∞.\nSet x = φα(A†y) for each example y. // φα is ReLU activation; see (2) for the definition\n4: (Update) Update the feature matrix A (t+1) = (1− η)A(t) + rηÊ [ (y − y′)(x− x′)⊤ ]\nwhere Ê is over independent uniform y, y′ from {y1, . . . , yN}, and x, x′ are their decodings. Output: A = A(T )\nOur main algorithm is presented in Algorithm 1. It keeps a working feature matrix and operates in iterations. In each iteration, it first compute the weights for a batch of N examples (decoding), and then uses the computed weights to update the feature matrix (updating).\nThe decoding is simply multiplying the example by the pseudo-inverse of the current feature matrix and then passing it through the rectified linear unit (ReLU) φα with offset α. The pseudo-inverse with minimum infinity norm is used so as to maximize the robustness to noise (see the theorems). The ReLU function φα operates element-wise on the input vector v, and for an element vi, it is defined as\nφα(vi) = max {vi − α, 0} . (2)\nTo get an intuition why the decoding makes sense, suppose the current feature matrix is the groundtruth. Then A†y = A†A∗x∗ + A†ν = x∗ + A†ν. So we would like to use a small A† and use threshold to remove the noise term.\nIn the encoding step, the algorithm move the feature matrix along the direction E [ (y − y′)(x − x′)⊤ ] . To see intuitively why this is a good direction, note that when the decoding is perfect and there is no noise, E [ (y − y′)(x− x′)⊤ ] = A∗, and thus it is moving towards the ground-truth. Without those ideal conditions, we need to choose a proper step size, which is tuned by the parameters η and r."
    }, {
      "heading" : "5 Results for a simplified case",
      "text" : "We will state and demonstrate our results and proof intuition in a simplified setting first, with assumptions (A1), (A2’), (A3), and (N1), where\n(A2’) x∗i ’s are independent, and x ∗ i = 1 with probability s/n and 0 otherwise for a constant\ns > 0.\nFurthermore, we will assume N(0) = 0.\nNote this is a special case of our general assumptions, with C1 = c2 = C2 = s where s is the parameter in (A2’). It is still an interesting setting: to the best of our knowledge there is no existing guarantee of alternating type algorithms for it. Moreover, we will present the general result in Section 6 which will be easier to digest after we have presented this simplified setting.\nFor notational convenience, let (A∗)† denote the matrix satisfying (A∗)†A∗ = I. If there are multiple such matrices we let it denote the one with minimum ‖(A∗)†‖∞. Theorem 1 (Simplified case, adversarial noise). There exists an absolute constant G such that if Assumptions (A1),(A2’),(A3) and (N1) are satisfied with l = 1/10, Cν ≤ Gcmax{m,n‖(A∗)†‖\n∞ } for\nsome 0 ≤ c ≤ 1 and N(0) = 0, then there is a choice of parameters α, η, r such that for every 0 < ǫ, δ < 1 and N = poly(n,m, 1/ǫ, 1/δ) the following holds with probability at least 1− δ: After T = O ( ln 1ǫ ) iterations, Algorithm 1 outputs a solution A = A∗(Σ + E) +N where Σ (1 − ℓ)I is diagonal, ‖E‖1 ≤ ǫ+ c is off-diagonal, and ‖N‖1 ≤ c.\nRemarks. Consequently, when ‖A∗‖1 = 1, we can do normalization Âi = Ai/‖Ai‖1, and the normalized output Â satisfies ‖Â−A∗‖1 ≤ ǫ+ 2c. In particular, under mild conditions and with proper parameters, our algorithm recovers the groundtruth in a geometric rate. It can achieve arbitrary small recovery error in the noiseless setting, and achieve error up to the noise limit even with adversarial noise whose level is comparable to the signal.\nThe condition on ℓ means that a constant warm start is sufficient for our algorithm to converge, which is much better than previous work such as [AR15]: indeed, there ℓ depends on the dynamic range of the entries of A∗ which is problematic in practice.\nThe result implies that with large adversarial noise, the algorithm can still recover the features up to the noise limit. When m ≥ n‖ (A∗)† ‖∞, each data point has adversarial noise with ℓ1 norm as large as ‖ν‖1 = Cνm = Ω(c), which is in the same order as the signal ‖A∗x∗‖1 = O(1). Our algorithm still works in this regime. Furthermore, the final error ‖A−A∗‖1 is O(c), in the same order as the adversarial noise in one data point.\nNote the appearance of ‖ (A∗)† ‖∞ is not surprising. The case when the columns are the canonical unit vectors for instance, which corresponds to ‖ (A∗)† ‖∞ = 1, is expected to be easier than the case when the columns are nearly the same, which corresponds to large ‖ (A∗)† ‖∞. A similar theorem holds for the unbiased noise model. Theorem 2 (Simplified case, unbiased noise). If Assumptions (A1),(A2’),(A3) and (N2) are satisfied with Cν = Gc√n max{m,n‖(A∗)†‖\n∞ } , then the same guarantee as Theorem 1 holds.\nRemarks. With unbiased noise which is commonly assumed in many applications, the algorithm can tolerate noise level √ n larger than the adversarial case. When m ≥ n‖ (A∗)† ‖∞, each data\npoint has noise with ℓ1 norm as large as ‖ν‖1 = Cνm = Ω(c √ n), which can be Ω( √ n) times larger than the signal ‖A∗x∗‖1 = O(1). The algorithm can recover the ground-truth in this heavy noise regime. Furthermore, the final error ‖A−A∗‖1 is O (‖ν‖1/ √ n), which is only O(1/ √ n) fraction of the noise in one data point. This is a strong denoising effect and a bit counter-intuitive. It is possible since we exploit averaging of the noise for cancellation, as well as thresholding to remove noise spread out in the coordinates."
    }, {
      "heading" : "5.1 Analysis: intuition",
      "text" : "A natural approach typically employed to analyze algorithms for non-convex problems is to define a function on the intermediate solution A and the ground-truth A∗ measuring their distance and then show that the function decreases at each step. However, a single potential function will not be enough in our case, as we argue below, so we introduce a novel framework of maintaining two potential functions which capture different aspects of the intermediate solutions.\nLet us denote the intermediate solution and the update as (omitting the superscript (t))\nA = A∗(Σ+E) +N, Ê[(y − y′)(x − x′)⊤] = A∗(Σ̃+ Ẽ) + Ñ, (3) where Σ and Σ̃ are diagonal, E and Ẽ are off-diagonal, and N and Ñ are the terms outside the span of A∗ which is caused by the noise. To cleanly illustrate the intuition behind ReLU and the coupled potential functions, we focus on the noiseless case and assume that we have infinite samples. Since Ai = Σi,iA∗i + ∑ j 6=i Ej,iA ∗ j , if the ratio between ‖Ei‖1 = ∑ j 6=i |Ej,i| and Σi,i gets smaller, then the algorithm is making progress; if the ratio is large at the end, a normalization of Ai gives a good approximation of A∗i . So it suffices to show that Σi,i is always about a constant while ‖Ei‖1 decreases at each iteration. We will focus on E and consider the update rule in more detail to argue this. After some calculation, we have\nE ← (1− η)E+ rηẼ, Ẽ = E[(x∗ − (x′)∗) (x− x′)⊤], (4) where x, x′ are the decoding for x∗, (x′)∗ respectively:\nx = φα ( (Σ+E)−1x∗ ) , x′ = φα ( (Σ+E)−1(x′)∗ ) . (5)\nTo see why the ReLU function matters, consider the case when we do not use it.\nẼ = E(x∗ − (x′)∗) [ A † A ∗(x∗ − (x′)∗) ]⊤ = E [ (x∗ − (x′)∗)(x∗ − (x′)∗)⊤ ] [ (Σ+E)−1 ]⊤\n∝ [ (Σ+E)−1 ]⊤ ≈ Σ−1 −Σ−1EΣ−1. where we used Taylor expansion and the fact that E [ (x∗ − (x′)∗)(x∗ − (x′)∗)⊤ ] is a scaling of identity. Hence, if we think of Σ as approximately I and take an appropriate r, the update to the matrix E is approximately E ← E − ηE⊤. Since we do not have control over the signs of E throughout the iterations, the problematic case is when the entries of E⊤ and E roughly match in signs, which would lead to the entries of E increasing.\nNow we consider the decoding to see why the ReLU is helpful. Ignoring the higher order terms and regarding Σ = I, we have\nx = φα ( (Σ+E)−1x∗ ) ≈ φα ( Σ −1x∗ −Σ−1EΣ−1x∗ ) ≈ φα (x∗ −Ex∗) . (6)\nThe problematic term is Ex∗. These errors when summed up will be comparable or even larger than the signal, and the algorithm will fail. However, since the signal coordinates are non-negative and most coordinates with errors only have small values, the hope is that thresholding with ReLU can remove those errors while keeping a large fraction of the signal coordinates. This leads to large Σ̃i,i and small Ẽj,i’s, and then we can choose an r such that Ej,i’s keep decreasing while Σi,i’s stay in a certain range.\nTo quantify the intuition above, we need to divide E into its positive part E+ and its negative part E−:\n[E+]i,j = max {Ei,j , 0} , [E−]i,j = max {−Ei,j, 0} . (7)\nThe reason to do so is the following: when Ei,j is negative, by the Taylor expansion approximation,[ (Σ+E)−1x∗ ] i\nwill tend to be more positive and will not be thresholded to 0 by the ReLU most of the time. Therefore, Ej,i will become more positive at next iteration. On the other hand, when Ei,j is positive, [ (Σ+E)−1x∗\n] i\nwill tend to be more negative and zeroed out by the ReLU function. Therefore, Ej,i will not be more negative at next iteration. Informally, we will show for positive and negative parts of E:\npostive(t+1) ← (1−η)positive(t)+(η)negative(t), negative(t+1) ← (1−η)negative(t)+(εη)positive(t)\nfor a small ε ≪ 1. Due to the appearance of ε in the above updates, we can “couple” the two parts, namely show that a weighted average of them will decrease, which implies that ‖E‖s is small at the end. This leads to our coupled potential function.2"
    }, {
      "heading" : "5.2 Analysis: proof sketch",
      "text" : "We now provide a proof sketch for the simplified case presented above. The complete proof of the results for the general case (which is stated in the next section) is presented in the appendix. The lemmas here are direct corollaries of those in the appendix.\nOne iteration. We focus on one update and omit the superscript (t). Recall the definitions of E, Σ, N and Ẽ, Σ̃ and Ñ from (3). Our goal is to derive lower and upper bounds for Ẽ, Σ̃ and Ñ, assuming that Σi,i falls into some range around 1, while E and N are small. This will allow us to do induction on t.\nFirst, begin with the decoding. A simple calculation shows that the decoding for y = A∗x∗ + ν is\nx = φα (Zx ∗ + ξ) , where Z = (Σ+E)−1 , ξ = −A†NZx∗ +A†ν. (8)\nNow, we can present our key lemmas bounding Ẽ, Σ̃, and Ñ. Before doing this, we add that the particular value for r we will choose is r = ns (recalling s is the sparsity of x\n∗ according to Assumption (A2’)). We also set the threshold of the ReLU as ρ < α ≪ sn . Then, we get: Lemma 3 (Simplified bound on Ẽ, informal). (1) if Zi,j < 0, then ∣∣∣Ẽj,i ∣∣∣ ≤ o ( s n (|Zi,j |+ ρ) ) , (2) if Zi,j ≥ 0, then −O (( s n )2 Zi,j + ρZi,j ) ≤ Ẽj,i ≤ O ( ( sn + ρ)|Zi,j | ) .\nNote that Z ≈ Σ−1 − Σ−1EΣ−1, so Zi,j < 0 corresponds roughly to Ei,j > 0. In this case, keeping in mind that r = ns , the upper bound on |Ẽj,i| is small enough to ensure |Ej,i| decreases, as described in the intuition.\nOn the other hand, when Zi,j ≥ 0 (roughly Ei,j < 0), the upper bound on Ẽj,i is large enough that rẼj,i can be on the same order as Ei,j , corresponding to the intuition that negative Ei,j can contribute a large positive value to Ej,i. Fortunately, the lower bound on Ẽj,i is of much smaller absolute value, which allows us to show that a potential function that couples Case (1) and Case (2) in Lemma 3 actually decreases; see the induction below.\nLemma 4 (Simplified bound on Σ̃, informal). Σ̃i,i ≥ Ω((Σ−1i,i − α)/n). Lemma 5 (Simplified bound on Ñ, adversarial noise, informal). ∣∣∣Ñi,j ∣∣∣ ≤ O(Cν/n).\nInduction by iterations. We now show how to use the three lemmas to prove the theorem for the adversarial noise. The proof for the unbiased noise statement is similar. Let at := ∥∥∥E(t)+ ∥∥∥ s and bt := ∥∥∥E(t)− ∥∥∥ s , and choose η = ℓ/6. We begin with proving the following three claims by induction on t: at the beginning of iteration t,\n(1) (1− ℓ)I Σ(t) 2Note that since intuitively, Ei,j gets affected by Ej,i after an update, if we have a row which contains\nnegative entries, it is possible that ‖Ai − A∗i ‖1 increases. So we cannot simply use maxi ‖Ai − A ∗ i ‖1 as a potential function.\n(2) ∥∥E(t) ∥∥ s ≤ 1/8, and if t > 0, then at + βbt ≤ ( 1− 125η ) (at−1 + βbt−1) + ηh, for some\nβ ∈ (1, 8), and some small value h,\n(3) ∥∥N(t) ∥∥ s ≤ c/10.\nThe most interesting part is the second claim. At a high level, by Lemma 3, we can show that\nat+1 ≤ ( 1− 3\n25 η\n) at + 7ηbt + ηh, bt+1 ≤ ( 1− 24\n25 η\n) bt + 1\n100 ηat + ηh.\nNotice that the contribution of bt to at+1 is quite large (due to the larger upper bound in Case (2) in Lemma 3), but the other contributions are all small. This allows to choose a β ∈ (1, 8) so that at+1 + βbt+1 leads to the desired recurrence in the second claim. In other words, at+1 + βbt+1 is our potential function which decreases at each iteration up to the level h. The other claims can also be proved by the corresponding lemmas. Then the theorem follows from the induction claims."
    }, {
      "heading" : "6 More general results",
      "text" : "More general weight distributions. Our argument holds under more general assumptions on x∗.\nTheorem 6 (Adversarial noise). There exists an absolute constant G such that if Assumption (A0)(A3) and (N1) are satisfied with l = 1/10, C2 ≤ 2c2, C31 ≤ Gc22n, Cν ≤ { c22Gc C21m , c42Gc\nC51n‖(A∗)†‖∞\n}\nfor 0 ≤ c ≤ 1, and ∥∥N(0) ∥∥ ∞ ≤ c22Gc C31‖(A∗)†‖∞ , then there is a choice of parameters α, η, r such that for every 0 < ǫ, δ < 1 and N = poly(n,m, 1/ǫ, 1/δ), with probability at least 1− δ the following holds: After T = O ( ln 1ǫ ) iterations, Algorithm 1 outputs a solution A = A∗(Σ + E) +N where Σ (1 − ℓ)I is diagonal, ‖E‖1 ≤ ǫ+ c/2 is off-diagonal, and ‖N‖1 ≤ c/2.\nTheorem 7 (Unbiased noise). If Assumption (A0)-(A3) and (N2) are satisfied with Cν = c2G √ cn C1 max{m,n‖(A∗)†‖ ∞ } and the other parameters set as in Theorem 6, then the same guarantee holds.\nThe conditions on C1, c2, C2 intuitively mean that each feature needs to appear with reasonable probability. C2 ≤ 2c2 means that their proportions are reasonably balanced. This may be a mild restriction for some applications – however, we additionally propose a pre-processing step that can relax this in the following subsection.\nThe conditions allow a rather general family of distributions, so we point out an important special case to provide a more concrete sense of the parameters. For example, for the uniform independent distribution considered in the simplified case, we can actually allow s to be much larger than a constant; our algorithm just requires s ≤ Gn for a fixed constant G. So it works for uniform sparse distributions even when the sparsity is linear, which is an order of magnitude larger than what can be achieved in the dictionary learning regime. Furthermore, the distributions of x∗i can be very different, since we only require C31 = O(c 2 2n). Moreover, all these can be handled without specific structural assumptions on A∗.\nMore general proportions. A mild restriction in Theorem 6 and 7 is that C2 ≤ 2c2, that is, maxi∈[n] E[(x ∗ i )\n2] ≤ 2mini∈[n] E[(x∗i )2]. To relax this, we propose a pre-processing algorithm for balancing E[(x∗i ) 2]. The idea is quite natural: instead of solving Y ≈ A∗X, we could also solve Y ≈ [A∗D][(D)−1X] for a positive diagonal matrix D, where E[(x∗i )\n2]/D2i,i is with in a factor of 2 from each other. We show in the appendix that this can be done under assumptions as the above theorems, and additionally Σ (1 + ℓ)I and E(0) ≥ 0 entry-wise. After balancing, one can use Algorithm 1 on the new ground-truth matrix [A∗D] to get the final result."
    }, {
      "heading" : "7 Conclusion",
      "text" : "A simple and natural algorithm that alternates between decoding and updating is proposed for nonnegative matrix factorization and theoretical guarantees are provided. The algorithm provably recovers a feature matrix close to the ground-truth and is robust to noise. Our analysis provides insights on the effect of the ReLU units in the presence of the non-negativity constraints, and the resulting interesting dynamics of the convergence."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329."
    }, {
      "heading" : "A Preliminary",
      "text" : "Given a matrix Y ∈ Rm×N , the goal of non-negative matrix factorization (NMF) is to find a matrix A ∈ Rm×n and a non-negative matrix X ∈ Rn×N , so that Y ≈ AX. The columns of Y are called data points, those of A are features, and those of X are weights.\nThe notation [M]j denotes the j-th column of M, [M]i denotes the i-th row of M, and Mi,j denotes the element of M at the i-th row and j-th column. Furthermore, let M+ = denote the positive part of the matrix, and let M− denote the absolute value of the negative part of the matrix:\n[M+]i,j = { Mi,j if Mi,j ≥ 0, 0 if Mi,j < 0,\n[M−]i,j = { 0 if Mi,j ≥ 0, |Mi,j | if Mi,j < 0.\nFor analysis, the following norms of the matrices are needed. Definition (l1 norm of a matrix (induced column norm)). The (induced) l1 norm of a matrix E ∈ R n×n is\n‖E‖1 = max i∈[n]    n∑\nj=1\n|Ej,i|    .\nDefinition (l∞ norm of a matrix (induced row norm)). The (induced) l∞ norm of a matrix E ∈ R n×n is\n‖E‖∞ = max i∈[n]    n∑\nj=1\n|Ei,j |    .\nThese two norms are related, and they enjoy the sub-multipicity property of the induced norm. Property 8 (dual norm). For a matrix E ∈ Rn×n,\n‖E‖1 = ‖E⊤‖∞.\nNote that unlike l2 norm, it is possible that ‖E‖1 6= ‖E⊤‖1 or ‖E‖∞ 6= ‖E⊤‖∞. Property 9 (induced norm of a matrix). Let E1,E2 ∈ Rn×n be two matrices, then\n‖E1E2‖1 ≤ ‖E1‖1‖E2‖1, ‖E1E2‖∞ ≤ ‖E1‖∞‖E2‖∞.\nThe following two kinds of norms are also useful for the analysis. Definition (symmetrized norm of a matrix). The symmetrized norm of a matrix E ∈ Rn×n is\n‖E‖s = max(‖E‖1, ‖E‖∞).\nNote that ‖E‖s is a norm since it’s the maximum of two norms. Definition (max norm). The max norm of a matrix E ∈ Rm×n is\n‖E‖max = maxi,j |Ei,j | .\nFor the function φα used in our decoding algorithm, we frequently use the following properties in the analysis. Property 10 (ReLU). φα(z) = max (0, z − α) is non-decreasing. It is 1-Lipschitz, i.e.,\n|φα(z1)− φα(z2)| ≤ |z1 − z2| . (9) It satisfies\nφα(z) ≥ z − α, (10) φα(z) ≤ |z − α| . (11)\nFurthermore, if α > 0,\nφα(z) ≤ |z| . (12)"
    }, {
      "heading" : "B Proofs for main algorithm: Purification",
      "text" : "Since NMF is NP-hard in the worst case, some assumptions are needed to make it tractable. In this paper, we consider a generative model for the data point y = A∗x∗ + ν, where A∗ is the groundtruth feature matrix, x∗ is the ground-truth non-negative weight from some unknown distribution, and ν is the noise. Our focus is to recover A∗ given access to the data distribution, assuming the following hold for parameters C1, c2, C2, ℓ, Cν that will be determined in our theorems.\n(A1) The columns of A∗ are linearly independent.\n(A2) For all i ∈ [n], x∗i ∈ [0, 1], E[x∗i ] ≤ C1n and c2n ≤ E[(x∗i )2] ≤ C2n , and x∗i ’s are independent. (A3) The initialization A(0) = A∗(Σ(0) + E(0)) + N(0), where Σ(0) is diagonal, E(0) is off-\ndiagonal, and\nΣ (0) (1− ℓ)I, ∥∥∥E(0) ∥∥∥ s ≤ ℓ.\nWe consider two noise models.\n(N1) Adversarial noise: only assume that maxi |νi| ≤ Cν almost surely. (N2) Unbiased noise: maxi |νi| ≤ Cν almost surely, and E[ν|x∗] = 0.\nAlgorithm 1 Purification\nInput: initialization A(0), threshold α, step size η, scaling factor r, sample size N , iterations T 1: for t = 0, 1, 2, ..., T − 1 do 2: Draw examples y1, . . . , yN . 3: (Decode) Compute A†, the pseudo-inverse of A(t) with minimum ‖(A)†‖∞.\nSet x = φα(A†y) for each example y. // φα is ReLU activation; see (2) for the definition\n4: (Update) Update the feature matrix A (t+1) = (1− η)A(t) + rηÊ [ (y − y′)(x− x′)⊤ ]\nwhere Ê is over independent uniform y, y′ from {y1, . . . , yN}, and x, x′ are their decodings. Output: A = A(T )\nOur main algorithm is presented in Algorithm 1. It keeps a working feature matrix and operates in iterations. In each iteration, it first compute the weights for N examples (decoding), and then use the computed weights to update the feature matrix (updating).\nThe decoding is simply multiplying the example by the pseudo-inverse of the current feature matrix and then passing it through a one-sided threshold function φα. The pseudo-inverse with minimum infinity norm is used so as to maximize the robustness to noise (see the theorems). The one-sided threshold function operates element-wisely on the input vector v, and for an element vi, it is defined as\nφα(vi) = max {vi − α, 0} . This is just the rectified linear unit (ReLU) with offset α. To get some sense about the decoding, suppose the current feature matrix is the ground-truth. Then A†y = A†A∗x∗ +A†ν = x∗ +A†ν. So we would like to use a small A† and use threshold to remove the noise term.\nIn the encoding step, the algorithm move the feature matrix along the direction E [ (y − y′)(x − x′)⊤ ] . Suppose we have independent x∗i ’s, perfect decoding and no noise,\nthen E [ (y − y′)(x− x′)⊤ ] = A∗, and thus it is moving towards the ground-truth. Without those ideal conditions, we need to choose a proper step size, which is tune by the parameters η and r.\nAt the end, the algorithm simply outputs the scaled features with unit norm. The output enjoys the following guarantee in the adversarial noise model."
    }, {
      "heading" : "B.1 Analysis of one update step",
      "text" : "In this subsection, we focus on one update step, bounding the changes of Σ,E,N and some auxiliary variables, and then in the next subsection we put things together to prove the theorem. So through\nout this subsection we will focus on a particular iteration t and omit the superscript (t), while in the next subsection we will put back the superscript.\nFor analysis, denote A(t) as\nA = A∗(Σ+E) +N\nwhere Σ is a diagonal matrix, E is an off-diagonal matrix, and N is the component of A that lies outside the span of A∗ (e.g., the noise caused by the noise in the sample).\nRecall the following notations:\nZ = (Σ+E) −1 ,\nV = Z−Σ−1 = Σ−1 ∞∑\nk=1\n(−EΣ−1)k,\nξ = −A†NZx∗ +A†ν.\nConsider the update term Ê [ (y − y′)(x− x′)⊤ ] and denote it as\n∆ = Ê [ (y − y′)(x − x′)⊤ ] = A∗(Σ̃+ Ẽ) + Ñ\nwhere Σ̃ is a diagonal matrix, Ẽ is an off-diagonal matrix, and N is the component of ∆ that lies outside the span of A∗.\nSince we now use empirical average, we will have sampling noise. Denote it as\nNs = Ê[(y − y′)(x− x′)⊤]− E[(y − y′)(x − x′)⊤].\nThen by definition, for y = A∗x∗ + ν and y′ = A∗(x′)∗ + ν′, we have\nÊ[(y − y′)(x − x′)⊤] = E[(y − y′)(x − x′)⊤] +Ns = A∗ E [ (x∗ − (x′)∗)(x − x′)⊤\n] ︸ ︷︷ ︸\nΣ̃+Ẽ\n+E [ (ν − ν′)(x− x′)⊤ ] +Ns︸ ︷︷ ︸\nÑ\n.\nOur goal is then bounding Σ̃, Ẽ, Ñ in terms of Σ,E,N. Before doing so, we present a lemma for the decoding. Lemma 11 (Main: Decoding). Let m ≥ n be two positive integers. Let A ∈ Rm×n be a matrix such that A = A∗(Σ + E) +N where A∗ is full rank, Σ is a diagonal matrix such that Σ 12I and ‖E‖1 < 12 . Then for y = A∗x∗ + ν, the decoding is\nx = φα (Zx ∗ + ξ)\n= φα (( Σ −1 +V ) x∗ + ξ ) .\nProof of Lemma 11. Since A = A∗(Σ+E) +N, we have\nA ∗ = (A−N)(Σ+E)−1\ny = (A−N)(Σ+E)−1x∗ + ν. Plugging into the decoding we get the first statement.\nObserving that Σ + E = (I + EΣ−1)Σ and ‖EΣ−1‖1 ≤ ‖Σ−1‖1‖E‖1 ≤ 2‖E‖1 < 1, we have (Σ+E)−1 = ( Σ −1 +V ) , resulting in the second statement.\nLemma 12 (Main: Bound on Σ̃). Suppose |ξi| ≤ ρ < α for any example and every i ∈ [n], and suppose Σ 12I. Then for any i ∈ [n],\nΣ̃i,i ≥ E [ (x∗i ) 2 ] ( 2Σ−1i,i − 2 |Vi,i| ) − 2C1\nn\n( α+ 2ρ+\nC1 n Σ −1 i,i + 2C1 n ∥∥∥[V]i ∥∥∥ 1 ) ,\nΣ̃i,i ≤ E [ (x∗i ) 2 ] ( 2Σ−1i,i + 2 |Vi,i| ) +\n2C1 n\n( ρ+\nC1 n ∥∥∥[V]i ∥∥∥ 1 ) .\nProof of Lemma 12. According to the definition, we have\nΣ̃i,i = [ (A∗)†E[(y − y′)(x − x′)⊤] ] i,i\n= E [(x∗i − (x′i)∗)(xi − x′i)] = E [(x∗i − (x′i)∗)xi] + E [((x′i)∗ − x∗i )x′i] .\nSince (x∗i − (x′i)∗)xi and ((x′i)∗ − x∗i )x′i has the same distribution, and (x′)∗, x∗ are i.i.d. , we have Σ̃i,i = 2E [(x ∗ i − (x′i)∗)xi] = 2E[x∗i xi]− 2E[x∗i ]E[xi]. So it suffices to bound E[x∗i xi] and E[xi]. To do so, we first take a look at xi. By the decoding rule,\nxi = [ φα (( Σ −1 +V ) x∗ + ξ )] i .\nSince φα is 1-Lipschitz, denoting ∆ = |[Vx∗]i + ξi| we have[ φα ( Σ −1x∗ )]\ni −∆ ≤ xi ≤\n[ φα ( Σ −1x∗ )]\ni +∆. (13)\nFor [ φα ( Σ −1x∗ )]\ni , by the Property 10 of φα(z),\nΣ −1 i,i x ∗ i − α ≤ [ φα ( Σ −1x∗ )] i = φα ( Σ −1 i,i x ∗ i ) ≤ Σ−1i,i x∗i . (14)\nFor ∆ = |[Vx∗]i + ξi|,\nE[∆] ≤ E   ∣∣∣∣∣∣ ∑\nj\nVi,jx ∗ j ∣∣∣∣∣∣  + E [|ξi|]\n≤ E\n ∑\nj\n|Vi,j |x∗j\n + ρ\n= ∑\nj\n|Vi,j |E [ x∗j ] + ρ\n≤ C1 n ∥∥∥[V]i ∥∥∥ 1 + ρ (15)\nwhere the second step follows from the assumption |ξi| ≤ ρ, and the last step follows from Assumption (A2).\nBounding E[xi]. By (13),(14), and (15), we have\nE[xi] ≤ E[Σ−1i,i x∗] + E[∆] ≤ C1 n Σ −1 i,i + C1 n ∥∥∥[V]i ∥∥∥ 1 + ρ.\nBounding E[x∗i xi]. First, note that\nE[x∗i∆] ≤ E  x∗i ∣∣∣∣∣∣ ∑\nj\nVi,jx ∗ j ∣∣∣∣∣∣  + E [x∗i |ξi|]\n≤ E  x∗i ∑\nj\nx∗j |Vi,j |\n + ρC1\nn\n= ∑\nj\nE [ x∗i x ∗ j ] |Vi,j |+\nρC1 n\n= E [ (x∗i ) 2 ] |Vi,i|+ ∑ j:j 6=i E [ x∗i x ∗ j ] |Vi,j |+ ρC1 n ≤ E [ (x∗i ) 2 ] |Vi,i|+\nC21 n2\n∑\nj:j 6=i |Vi,j |+\nρC1 n\n≤ E [ (x∗i ) 2 ] |Vi,i|+\nC21 n2 ∥∥∥[V]i ∥∥∥ 1 ++ ρC1 n ,\nwhere the second and the fifth steps follow from Assumption (A2). Therefore,\nE[x∗i xi] ≥ E [ x∗i ( Σ −1 i,i x ∗ i − α−∆ )] (16)\n≥ Σ−1i,i E [ (x∗i ) 2 ] − (α+ ρ)C1\nn − E\n[ (x∗i ) 2 ] |Vi,i| −\nC21 n2 ∥∥∥[V]i ∥∥∥ 1 . (17)\nPutting together. For the first statement,\nΣ̃i,i = 2E[x ∗ i xi]− 2E[x∗i ]E[xi]\n≥ 2Σ−1i,i E [ (x∗i ) 2 ] − 2(α+ ρ)C1\nn − 2E\n[ (x∗i ) 2 ] |Vi,i| − 2\nC21 n2 ∥∥∥[V]i ∥∥∥ 1\n− 2C 2 1\nn2 Σ\n−1 i,i − 2 C21 n2 ∥∥∥[V]i ∥∥∥ 1 − 2ρC1 n\n≥ E [ (x∗i ) 2 ] ( 2Σ−1i,i − 2 |Vi,i| ) − 2C1\nn\n( α+ 2ρ+\nC1 n Σ −1 i,i + 2C1 n ∥∥∥[V]i ∥∥∥ 1 ) .\nThe second statement follows from\nΣ̃i,i ≤ 2E[x∗i xi] ≤ 2E[x∗i (Σ−1i,i x∗i +∆)]\nand the bound on E[x∗i∆].\nLemma 13 (Main: Bound on Ẽ). Suppose |ξi| ≤ ρ < α for any example and every i ∈ [n]. Then for all i, j ∈ [n] such that i 6= j, the following holds. (1) If Zi,j < 0, then ∣∣∣Ẽj,i ∣∣∣ ≤ 4C 2 1‖Zi‖1\nn2(α− ρ) (|Zi,j |+ ρ) .\n(2) If Zi,j ≥ 0, then\n− 8C1ρ n(α− ρ)\n( C1‖Zi‖1\nn + Zi,j\n) −2C 2 1\nn2 Zi,j ≤ Ẽj,i ≤\n8C1ρ\nn(α− ρ)\n( C1‖Zi‖1\nn + Zi,j\n) +2E[(x∗j ) 2]Zi,j .\nProof of Lemma 13. Since i 6= j, we know that\nẼj,i = E [ (x∗j − (x′j)∗)(xi − x′i) ]\n= E [ x∗j (xi − x′i) ] + E [ (x′j) ∗(x′i − xi) ] = 2E [ x∗j (xi − x′i) ]\nwhere the last equality follows from that x∗j (xi − x′i) and (x′j)∗(x′i − xi) has the same distribution. This quantity can be bounded by a coupling between xi and x′i. Define a new variable x̃ ∗ as\n[x̃∗]i = { x∗i , if i 6= j, (x′j) ∗, if i = j.\nBy Assumption (A2), conditional on x∗j , x̃ ∗ has the same distribution as (x′)∗. Therefore, consider the variable x̃ given by x̃ = φα(A†(A∗x̃∗ + ν′)), we then have\nE [ x∗j (xi − x′i) ] = E [ x∗j (xi − x̃i) ] .\nIn summary, we have Ẽj,i = 2E[x ∗ j (xi − x̃i)]\nwhere\nxi = [φα (Zx ∗ + ξ)]i , ξ = −A†NZx∗ +A†ν, x̃i = [ φα ( Zx∗ + ξ̃\n)] i , ξ̃ = −A†NZx̃∗ +A†ν′.\nIntroduce the notation w = Zi,ix ∗ i + ∑\nl 6=i,j Zi,lx\n∗ l .\nWe have\nxi = φα ( w + Zi,jx ∗ j + ξi ) ,\nx̃i = φα ( w + Zi,j(x ′ j) ∗ + ξ̃i ) .\n(1) Since Zi,j < 0, |ξi| ≤ ρ, and |ξ̃i| ≤ ρ, we know that when w < α− ρ, xi = x̃i = 0. Then E [ x∗j (xi − x̃i) ] = Pr[w ≥ α− ρ] E [ x∗j (xi − x̃i)|w ≥ α− ρ ] . (18)\nBy Property 10, φα (·) is 1-Lipschitz, so\n|xi − x̃i| ≤ |Zi,j | ∣∣x∗j − (x′j)∗ ∣∣+ ∣∣∣ξi − ξ̃i ∣∣∣ ,\nwhich implies that\n∣∣E [ x∗j (xi − x̃i)|w ≥ α− ρ ]∣∣ ≤ E [ x∗j |Zi,j | ∣∣x∗j − (x′j)∗ ∣∣+ x∗j ∣∣∣ξi − ξ̃i ∣∣∣ ∣∣∣∣w ≥ α− ρ ]\n≤ |Zi,j |max {∣∣x∗j − (x′j)∗ ∣∣}E [ x∗j |w ≥ α− ρ ] + 2ρE [ x∗j |w ≥ α− ρ ] ≤ |Zi,j |max {∣∣x∗j − (x′j)∗ ∣∣}E [ x∗j ] + 2ρE [ x∗j ] ≤ 2E [ x∗j ] (|Zi,j |+ ρ)\n≤ 2C1 n (|Zi,j |+ ρ) . (19)\nNow consider Pr[w ≥ α− ρ]. Since\nE |w| ≤ |Zi,i|E[x∗i ] + ∑\nl 6=i,j |Zi,l|E[x∗j ] ≤ C1 n ‖Zi‖1,\nwe have that\nPr[w ≥ α− ρ] ≤ E |w| α− ρ ≤ C1‖Zi‖1 n(α− ρ) (20)\nCombining (18)(19) and (20) together completes the proof for the case when Zi,j < 0.\n(2) Now consider the case when Zi,j ≥ 0. Again, we have xi = φα ( w + Zi,jx ∗ j + ξi ) ,\nx̃i = φα ( w + Zi,j(x ′ j) ∗ + ξ̃i ) .\nFor the analysis, introduce a variable\nũi = φα ( w + Zi,jx ∗ j + ξ̃i ) .\nIf (x′j) ∗ > x∗j , by Property 10 φα(·) is 1-Lipschitz, so\nx̃i ≤ ũi + Zi,j ( (x′j) ∗ − x∗j ) .\nIf (x′j) ∗ ≤ x∗j , by Property 10 φα(·) is non-decreasing, then\nx̃i ≤ ũi. In any case,\nx̃i ≤ ũi + Zi,j(x′j)∗.\nTherefore,\nE [ x∗j (xi − x̃i) ] ≥ E [ x∗j (xi − ũi) ] − E [ x∗jZi,j(x ′ j) ∗]\n≥ E [ x∗j (xi − ũi) ] − C 2 1\nn2 Zi,j .\nSo we only need to consider E [ x∗j (xi − ũi) ] . Let G denote the event that xi 6= 0 or ũi 6= 0. Then by conditioning on x∗j , we have\nE [ x∗j (xi − ũi) ] = E [ x∗jE [ xi − ũi ∣∣∣∣x ∗ j ]]\nand\nE [ xi − ũi ∣∣∣∣x ∗ j ] = Pr [ G ∣∣∣∣x ∗ j ] E [ xi − ũi ∣∣∣∣x ∗ j ,G ] .\nBy Property 10 φα(·) is 1-Lipschitz, so ∣∣∣∣E [ xi − ũi ∣∣∣∣x ∗ j ,G ]∣∣∣∣ ≤ E [ |ξi|+ ∣∣∣ξ̃i ∣∣∣ ∣∣∣∣x ∗ j ,G ] ≤ 2ρ.\nNow consider Pr [ G ∣∣∣∣x∗j ] . We have\nE [∣∣w + Zi,jx∗j ∣∣ ∣∣∣∣x ∗ j ] ≤ E [ |w| ∣∣∣∣x ∗ j ] + Zi,j\n≤ C1 n ∥∥Zi ∥∥ 1 + Zi,j ,\nwhere the first step follows from x∗j ≤ 1 and the second step follows from the conditional independence in Assumption (A2). Then by Markov’s inequality,\nPr [ xi 6= 0 ∣∣∣∣x ∗ j ] ≤ Pr [∣∣w + Zi,jx∗j ∣∣ ≥ α− ρ ∣∣∣∣x ∗ j ]\n≤ 1 α− ρ ( C1 n ∥∥Zi ∥∥ 1 + Zi,j ) .\nA similar argument leads to that\nPr [ ũi 6= 0 ∣∣∣∣x ∗ j ] ≤ 1\nα− ρ ( C1 n ∥∥Zi ∥∥ 1 + Zi,j )\nand thus\nPr [ G ∣∣∣∣x ∗ j ] ≤ 2\nα− ρ ( C1 n ∥∥Zi ∥∥ 1 + Zi,j ) .\nPutting things together,\n∣∣E [ x∗j (xi − ũi) ]∣∣ ≤ 4ρ α− ρ\n( C1‖Zi‖1\nn + Zi,j\n) E [ x∗j ]\n≤ 4C1ρ n(α− ρ)\n( C1‖Zi‖1\nn + Zi,j\n) .\nThis completes the proof for the lower bound.\nSimilarly, for the upper bound, introduce\nui = φα ( w + Zi,j(x ′ j) ∗ + ξi ) .\nThen in any case,\nxi ≤ ui + Zi,jx∗j\nand thus\nE [ x∗j (xi − x̃i) ] ≤ E [ x∗j (ui − x̃i) ] + E [ (x∗j ) 2 ] Zi,j .\nThe same argument as above shows that\n∣∣E [ x∗j (ui − x̃i) ]∣∣ ≤ 4C1ρ n(α− ρ)\n( C1‖Zi‖1\nn + Zi,j\n) .\nThis completes the whole proof.\nLemma 14 (Main: Bound on Ñ). Suppose ‖E‖s ≤ ℓ, Σ (1− ℓ)I, and |ξj | ≤ ρ < α. (1) If the noise is correlated (Assumption (N1)), then\n∣∣∣Ñi,j ∣∣∣ ≤ 4CνC1\n(1− 2ℓ)2n(α− ρ) + |[Ns]i,j |\n(2) If the noise is unbiased (Assumption (N2)) and ‖A†ν‖∞ ≤ ρ′ < α, then ∣∣∣Ñi,j ∣∣∣ ≤ 2C1Cνρ ′(1 + ‖A†N‖∞)\n(1− 2ℓ)n(α− ρ′) + ∣∣∣[Ns]i,j ∣∣∣ .\nProof of Lemma 14. (1) By the update rule,\nÑ = 2E[ν(x− x′)⊤] +Ns.\nUnder Assumption (N1), we have that for every i ∈ [n], j ∈ [n],\n|Ñi,j | = |2E[νi(xj − x′j)] + [Ns]i,j | ≤ 4CνE[xj ] + |[Ns]i,j | = 4CνE [φα ([Zx\n∗]j + ξj)] + |[Ns]i,j |. since |νi| is bounded by Cν . Now focus on the term E [φα ([Zx∗]j + ξj)]. We have\n|[Zx∗]j | ≤ ‖Z‖∞‖x∗‖∞ ≤ ‖Z‖∞ ≤ 1\n1− 2ℓ by the fact that ‖x∗‖∞ ≤ 1 in Assumption (A2), and the assumptions of the lemma on Σ and E. Then when [Zx∗]j + ξj ≥ α,\nφα ([Zx ∗]j + ξj) ≤ [Zx∗]j + ξj − α ≤\n1 1− 2ℓ + ρ− α ≤ 1 1− 2ℓ ,\nand thus\nE [φα ([Zx ∗]j + ξj)] ≤\n1\n1− 2ℓ Pr {[Zx ∗]j + ξj ≥ α}\n≤ 1 1− 2ℓ Pr {|[Zx ∗]j | ≥ α− ρ}\n≤ 1 1− 2ℓ E|[Zx∗]j | α− ρ\n≤ 1 1− 2ℓ ‖Z‖∞ α− ρ E [ x∗j ]\n≤ C1 (1 − 2ℓ)2n(α− ρ)\nwhere the last step uses the bound on E [ x∗j ] in Assumption (A2). Therefore,\n|Ñi,j | ≤ 4CνC1\n(1− 2ℓ)2n(α− ρ) + |[Ns]i,j |.\n(2) When the noise is unbiased, we have E[ν|x∗] = 0. Then E[νix′j ] = 0, and∣∣∣Ñi,j ∣∣∣ = ∣∣2E[νi(xj − x′j)] + [Ns]i,j ∣∣ ≤ 2 |E[νixj ]|+ |[Ns]i,j | . (21)\nConsider the first term for a fixed x∗, i.e., consider the conditional expectation E[νixj | x∗]. For notational simplicity, let Z̃ = (Z−A†NZ) and ξ̃ = A†ν. Then\nE[νixj | x∗] = E [νiφα ([Zx∗]j + ξj) | x∗] = E [ νiφα ( [Z̃x∗]j + ξ̃j ) | x∗ ] .\nWe consider the following two cases about [Z̃x∗]j .\n(a) If [Z̃x∗]j ≤ α− ρ′, then φα ( [Z̃x∗]j + ξ̃j ) = 0 always holds, which implies that\n|E[νixj | x∗]| = E [ νiφα ( [Z̃x∗]j + ξ̃j ) | x∗ ] = 0.\n(b) If [Z̃x∗]j > α− ρ′, then\nφα ( [Z̃x∗]j + ξ̃j ) ≤ φα ( [Z̃x∗]j + ρ ′ ) ≤ [Z̃x∗]j + ρ′ − α.\nOn the other side, by Property 10,\nφα ( [Z̃x∗]j + ξ̃j ) ≥ [Z̃x∗]j + ξ̃j − α ≥ [Z̃x∗]j − ρ′ − α.\nPutting together, we conclude that\nνi([Z̃x ∗]j − α)− |νiρ′| ≤ νiφα ( [Z̃x∗]j + ξ̃j ) ≤ νi([Z̃x∗]j − α) + |νiρ′|.\nNote that E[νi([Z̃x∗]j − α)|x∗] = 0, so\n|E[νixj | x∗]| = ∣∣∣E [ νiφα ( [Z̃x∗]j + ξ̃j ) | x∗ ]∣∣∣ ≤ E[|νiρ′||x∗] ≤ Cνρ′.\nPutting case (a) and case (b) together, we have\n|E[νixj | x∗]| ≤ Cνρ′ Pr { [Z̃x∗]j > α− ρ′ } ≤ Cνρ′ Pr {∣∣∣[Z̃x∗]j ∣∣∣ > α− ρ′ } .\nBy definition of Z̃ and the assumptions of the lemma on Σ and E, ∣∣∣[Z̃x∗]j ∣∣∣ ≤ (1 + ‖A†N‖∞) |[Zx∗]j | ≤ (1 + ‖A†N‖∞) |Z|∞ x∗j ≤ 1 + ‖A†N‖∞\n1− 2ℓ x ∗ j . (22)\nThen\nPr {∣∣∣[Z̃x∗]j ∣∣∣ > α− ρ′ } ≤ E\n∣∣∣[Z̃x∗]j ∣∣∣\nα− ρ′ ≤ C1(1 + ‖A†N‖∞) (1− 2ℓ)n(α− ρ′) .\nThe lemma then follows from (21) and (22).\nThere are three terms Z, V and ξ in the above lemmas that need to be bounded. Since Z = V+Σ−1, we only need to bound V and ξ in the following two lemmas, respectively. Lemma 15 (Bound on V). Suppose ‖E‖s < ℓe and Σ (1− ℓ)I. Then (1) ‖V+‖s ≤ 1− ℓe\n(1− ℓ)(1 − ℓe − ℓ) ‖E−‖s +\nℓ\n(1− ℓ)2(1− ℓe − ℓ) ‖E+‖s ,\n(2) ‖V−‖s ≤ 1− ℓe\n(1− ℓ)(1− ℓe − ℓ) ‖E+‖s +\nℓ\n(1− ℓ)2(1− ℓe − ℓ) ‖E−‖s ,\n(3) ‖V‖s ≤ ℓe(1− ℓe)\n(1 − ℓ)2(1− ℓe − ℓ) ,\n(4) |Vi,i| ≤ ℓℓe\n(1 − ℓ)2(1− ℓe − ℓ) , ∀i ∈ [n].\nProof of Lemma 15. Denote T = Σ−1 ∑∞\nk=2(−EΣ−1)k, so that V = −Σ−1EΣ−1 +T.\nThe following bound on ‖T‖1 will be useful.\n‖T‖1 ≤ ∥∥Σ−1 ∥∥ 1\n∞∑\nk=2\n∥∥(EΣ−1)k ∥∥ 1\n≤ ∥∥Σ−1 ∥∥ 1\n∞∑\nk=2\n∥∥EΣ−1 ∥∥k 1\n≤ ∥∥Σ−1 ∥∥ 1\n∥∥EΣ−1 ∥∥2 1\n1− ‖EΣ−1‖1 ≤ 1\n(1− ℓ)3 × ℓ× ‖E‖1\n1− ℓe1−ℓ ≤ ℓ\n(1− ℓ)2(1 − ℓe − ℓ) ‖E‖1. (23)\n(1) We need to show the bound for both ‖V+‖1 and ‖V+‖∞. By definition of V, for any i,\n‖V+‖1 = ∥∥∥ [ −Σ−1EΣ−1 +T ] + ∥∥∥ 1 .\nSince for any A and B,\n‖[A+B]+‖1 ≤ ‖[A]+‖1 + ‖[B]+‖1, and ‖[A]+‖1 ≤ ‖A‖1, we have\n‖[V+]i‖1 ≤ ∥∥∥ [ −Σ−1EΣ−1 ] + ∥∥∥ 1 + ‖T+‖1\n≤ 1 (1− ℓ)2 ‖E−‖1 + ‖T‖1. (24)\nBy (23),\n‖T‖1 ≤ ℓ\n(1− ℓ)2(1− ℓe − ℓ) ‖E‖1 ≤\nℓ\n(1 − ℓ)2(1− ℓe − ℓ) (‖E−‖1 + ‖E+‖1).\nCombined with (24), it implies\n‖[V+]i‖1 ≤ 1− ℓe\n(1− ℓ)2(1 − ℓe − ℓ) ‖E−‖1 +\nℓ\n(1− ℓ)2(1− ℓe − ℓ) ‖E+‖1.\nSimilarly, we have ∥∥∥[V+]i\n∥∥∥ 1 ≤ 1− ℓe (1 − ℓ)2(1− ℓe − ℓ) ‖E−‖∞ +\nℓ\n(1 − ℓ)2(1− ℓe − ℓ) ‖E+‖∞.\nPutting things together we have\n‖V+‖s ≤ 1− ℓe\n(1 − ℓ)(1− ℓe − ℓ) ‖E−‖s +\nℓ\n(1− ℓ)2(1− ℓe − ℓ) ‖E+‖s .\n(2) The argument for ‖V−‖s is similar to that for ‖V+‖s. (3) We need to show the bound for both ‖V‖1 and ‖V‖∞.\n‖V‖1 ≤ ∥∥−Σ−1EΣ−1 ∥∥ 1 + ‖T‖1\n≤ ℓe (1− ℓ)2 +\nℓ\n(1− ℓ)2(1− ℓe − ℓ) ‖E‖1\n≤ ℓe (1− ℓ)2 + ℓℓe (1− ℓ)2(1− ℓe − ℓ) = ℓe(1− ℓe)\n(1− ℓ)2(1− ℓe − ℓ)\nwhere the second step is by (23).\nSimilarly, ‖V‖∞ ≤ ℓe(1−ℓe) (1−ℓ)2(1−ℓe−ℓ) , so ‖V‖s ≤ ℓe(1−ℓe) (1−ℓ)2(1−ℓe−ℓ) .\n(4) Now consider Vi,i. By definition of T.\nVi,i = [ −Σ−1EΣ−1 ] i,i +Ti,i.\nNote that since Ei,i = 0, [ −Σ−1EΣ−1 ] i,i = 0. Then\n|Vi,i| = |Ti,i| ≤ ‖T‖1 ≤ ℓ\n(1− ℓ)2(1− ℓe − ℓ) ‖E‖1\n≤ ℓℓe (1− ℓ)2(1− ℓe − ℓ)\nwhere the third step is by (23). This completes the proof.\nLemma 16 (Bound on ξ). Suppose ‖E‖s < ℓ ≤ 1/8 and Σ (1− ℓ)I. Then for any i ∈ [n],\n|ξi| ≤ γ := 1 1− 2ℓ ∥∥A† ∥∥ ∞‖N‖∞ + Cν ∥∥A† ∥∥ ∞.\nIf furthermore, ‖N‖∞ ∥∥∥(A∗)† ∥∥∥ ∞ < 1/8, then\n∥∥A† ∥∥ ∞ ≤ 2 ∥∥∥(A∗)† ∥∥∥ ∞ ,\nγ ≤ 3 ∥∥(A∗)† ∥∥ ∞ (‖N‖∞ + Cν) .\nProof of Lemma 16. First, we have\n‖ξ‖∞ ≤ ∥∥A†NZx∗ ∥∥ ∞ + ∥∥A†ν ∥∥ ∞ ≤ ∥∥A† ∥∥ ∞‖N‖∞‖Z‖∞‖x ∗‖∞ + ∥∥A† ∥∥ ∞‖ν‖∞.\nNote that ‖x∗‖∞ ≤ 1 and ‖ν‖∞ ≤ Cν . Furthermore,\n‖Z‖∞ ≤ 1\n1− 2ℓ .\nThe first statement follows from combining these terms.\nNow consider the second statement. We apply Lemma 17. Since\nζ = ‖EΣ−1 + (A∗)†NΣ−1‖∞ ≤ ‖EΣ−1‖∞ + ‖(A∗)†NΣ−1‖∞ ≤ 1\n7 + ‖(A∗)†‖∞ × ‖N‖∞ × ‖Σ−1‖∞\n≤ 2 7 ,\nLemma 17 implies that\n‖A†‖∞ ≤ ‖Σ−1‖∞ 1− ζ ‖(A ∗)†‖∞ ≤ 2‖(A∗)†‖∞.\nThen γ is bounded by\nγ = 1 1− 2ℓ ∥∥A† ∥∥ ∞‖N‖∞ + Cν ∥∥A† ∥∥ ∞\n≤ 1 1− 2ℓ ×\n( 2‖(A∗)†‖∞ ) × ‖N‖∞ + Cν × ( 2‖(A∗)†‖∞ )\n≤ 3 ∥∥(A∗)† ∥∥ ∞ (‖N‖∞ + Cν) .\nThe following is the lemma about the norm of the pseudo-inverse, which is used in Lemma 16. Lemma 17 (Pseudo-inverse). Let A∗,N ∈ Rm×n be two matrices with m ≥ n. Let (A∗)† be one pseudo-inverse of A∗ such that (A∗)†A∗ = I. Let A = A∗(Σ + E) +N be another matrix, with Σ being diagonal and ζ := ‖EΣ−1 + (A∗)†NΣ−1‖∞. satisfies ζ < 1. Then there exists a pseudo-inverse A† of A such that A†A = I and\n‖A†‖∞ ≤ ‖Σ−1‖∞ 1− ζ ‖(A ∗)†‖∞.\nProof of Lemma 17. Consider the matrix\nA † = (Σ+E+ (A∗)†N)−1(A∗)†.\nThen by definition,\nA † A = (Σ+E+ (A∗)†N)−1(A∗)† (A∗(Σ+E) +N)\n= (Σ+E+ (A∗)†N)−1(Σ+E+ (A∗)†N)\n= I.\nWhat remains is to bound ‖A†‖∞. We have ‖A†‖∞ ≤ ‖(Σ+E+ (A∗)†N)−1‖∞‖(A∗)†‖∞.\nBy Taylor expansion rule, the first term on the right-hand side is\n(Σ+E+ (A∗)†N)−1 = (( I+EΣ−1 + (A∗)†NΣ−1 ) Σ )−1\n= Σ−1 ( I+EΣ−1 + (A∗)†NΣ−1 )−1\n=\n∞∑\ni=0\nΣ −1 (−EΣ−1 − (A∗)†NΣ−1 )i\nwhere we use the assumption that ‖EΣ−1 + (A∗)†NΣ−1‖∞ = ζ < 1. Therefore,\n‖(Σ+E+ (A∗)†N)−1‖∞ ≤ ‖Σ−1‖∞ ∞∑\ni=0\nζi = ‖Σ−1‖∞ 1− ζ ."
    }, {
      "heading" : "B.2 Putting things together",
      "text" : "We are now ready to prove our main theorems.\nTheorem 6 (Adversarial noise). There exists an absolute constant G such that if Assumption (A0)(A3) and (N1) are satisfied with l = 1/10, C2 ≤ 2c2, C31 ≤ Gc22n, Cν ≤ { c22Gc C21m , c42Gc\nC51n‖(A∗)†‖∞\n}\nfor 0 ≤ c ≤ 1, and ∥∥N(0) ∥∥ ∞ ≤ c22Gc C31‖(A∗)†‖∞ , then there is a choice of parameters α, η, r such that for every 0 < ǫ, δ < 1 and N = poly(n,m, 1/ǫ, 1/δ), with probability at least 1− δ the following holds: After T = O ( ln 1ǫ ) iterations, Algorithm 1 outputs a solution A = A∗(Σ + E) +N where Σ (1 − ℓ)I is diagonal, ‖E‖1 ≤ ǫ+ c/2 is off-diagonal, and ‖N‖1 ≤ c/2.\nProof of Theorem 6. We consider the following set of parameters\nα = c2\n80C1 , r =\nn c2 , η = ℓ 6 .\nFurthermore, set ρ = B1 c22c C31 for a sufficiently small absolute constant B1. Since C1 ≥ nE[x∗i ] ≥ nE[(x∗i ) 2] ≥ c2, this is small enough so that\nρ ≤ min { α\n2 ,\nc2α\n2048C1 ,\nc2α\n8000× 100C21 ,\ncc2α\n48000C21\n}\nwhich will be used in the proof. The proof also needs C21 ≤ B1c2n,C31 ≤ B2c22n for sufficiently small absolute constants B1 and B2. Since C1 > c2, we only need C31 ≤ Gc22n. Similarly, we need\nCν ≤ B1 min { c(α− ρ)c2\nmC1 , (α− ρ)c2 nC1‖(A∗)†‖∞ , (α− ρ)c2ρ nC1‖(A∗)†‖∞ ,\nρ\n‖(A∗)†‖∞\n}\nfor a sufficiently small absolute constant B1. This can be satisfied by setting G small enough in the theorem assumption.\nAfter setting the parameters needed, we now prove the theorem. We prove it by proving the following three claims by induction on t: at the beginning of iteration t,\n(1) (1− ℓ)I Σ(t), (2) ∥∥E(t) ∥∥ s ≤ 18 , and if t > 0\n∥∥∥E(t)+ ∥∥∥ s + β ∥∥∥E(t)− ∥∥∥ s ≤ ( 1− 1 25 η )(∥∥∥[E](t−1)+ ∥∥∥ s + β ∥∥∥[E](t−1)− ∥∥∥ s ) + c 10 ,\nfor β = √ 842+2800−84\n2 ∈ (1, 8),\n(3) ∥∥N(t) ∥∥ ∞ ≤ 1 8‖(A∗)†‖∞ , and ‖ξ(t)‖∞ ≤ ρ.\nClaim (1) and (2) are clearly true at t = 0 by the assumption on initialization. The first part of Claim (3) is true because of the assumption that ∥∥N(0) ∥∥ ∞ ≤ Gc 8µ3‖(A∗)†‖∞\nand that µ = C1/c2 ≥ 1. Then the second part follows from Lemma 16.\nNow we assume they are true up to t, and show them for t+ 1.\n(1) First consider the diagonal terms. Combining Lemma 12 and Lemma 15, we have\nΣ̃ (t) i,i ≥ E [ (x∗i ) 2 ] ( 2(Σ (t) i,i ) −1 − 2 ∣∣∣V(t)i,i ∣∣∣ ) − 2C1\nn\n( α+ 2ρ+\nC1 n (Σ (t) i,i ) −1 + 2C1 n ∥∥∥∥ [ V (t) ]i∥∥∥∥\n1\n) .\n≥ 2C2 n\n( 0− ℓ 2 (1− 2ℓ)(1− ℓ)2 ) − 2C1 n ( α+ α+ C1 n 1 1− ℓ + 2C1 n\nℓ\n(1 − ℓ)(1− 2ℓ)\n) .\n= 2C2 n\n( 0− ℓ 2 (1− 2ℓ)(1− ℓ)2 ) − 2C1 n ( 2α+ C1 n(1− ℓ)(1− 2ℓ) )\n> − c2 5n .\nThe first inequality uses ρ < α/2 and the last inequality is due to α ≤ c280C1 and C 2 1 ≤ c2n80 . Therefore, Σ\n(t+1) i,i = (1− η)Σ (t) i,i + ηrΣ̃ (t) i,i ≥ (1− η)Σ (t) i,i −\nη 5 .\nAssume for contradiction Σ(t+1)i,i < 1− ℓ. Then by the above inequality,\n1− ℓ > Σ(t+1)i,i ≥ (1 − η)Σ (t) i,i −\nη 5 .\nwhich implies Σ(t)i,i ≤ 1− ℓ+ 2η. In this case, by Lemma 12 and Lemma 15,\nΣ̃ (t) i,i ≥ E [ (x∗i ) 2 ] ( 2(Σ (t) i,i ) −1 − 2 ∣∣∣V(t)i,i ∣∣∣ ) − 2C1\nn\n( α+ 2ρ+\nC1 n (Σ (t) i,i ) −1 + 2C1 n ∥∥∥∥ [ V (t) ]i∥∥∥∥\n1\n) .\n≥ 2c2 n\n( 1\n1− ℓ+ 2η − ℓ2 (1 − 2ℓ)(1− ℓ)2 ) − 2C1 n ( 2α+ C1 n(1− ℓ)(1 − 2ℓ) )\n> c2 n .\nThen Σ\n(t+1) i,i = (1− η)Σ (t) i,i + ηrΣ̃ (t) i,i = (1− η)Σ (t) i,i + η > Σ (t) i,i ,\nwhich is a contradiction. Therefore, (1− ℓ)I Σ(t). (2) Now consider the off-diagonal terms. We shall split them into the positive part and the negative part. By the update rule, for any i ∈ [n],\n∥∥∥ [ E (t+1) + ] i ∥∥∥ 1 ≤ (1− η) ∥∥∥ [ E (t) + ] i ∥∥∥ 1 + ηr ∥∥∥ [ Ẽ (t) + ] i ∥∥∥ 1 .\nRecall the notations\nZ (t) = (Σ(t) +E(t))−1 = (Σ(t))−1 +V(t),\nV (t) = (Σ(t))−1\n∞∑\nk=1\n(−E(t)(Σ(t))−1)k\nBy Lemma 13, we have ∥∥∥ [ Ẽ (t) + ] i ∥∥∥ 1 ≤ ∑\nj 6=i\n4C21 n2(α− ρ) ∥∥∥∥ [ Z (t) ]i∥∥∥∥\n1\n(∣∣∣∣ [ Z (t) − ] i,j ∣∣∣∣+ ρ )\n︸ ︷︷ ︸ T1 + ∑\nj 6=i\n8C1ρ\nn(α− ρ) ( C1 n ∥∥∥∥ [ Z (t) ]i∥∥∥∥\n1\n+ ∣∣∣∣ [ Z (t) + ] i,j ∣∣∣∣ )\n︸ ︷︷ ︸ T2\n+ ∑\nj 6=i 2E[(x∗j ) 2]\n∣∣∣∣ [ Z (t) + ] i,j ∣∣∣∣ ︸ ︷︷ ︸\nT3\n.\nFirst, by Lemma 15, ∥∥∥∥ [ Z (t) ]i∥∥∥∥\n1\n≤ [( Σ (t) )−1]\ni,i\n+ ∥∥∥∥ [ V (t) ]i∥∥∥∥\n1\n≤ 1 1− 2ℓ\nNow consider Z(t)+ and Z (t) − . We have\n∑\nj:j 6=i\n∣∣∣∣ [ Z (t) − ] i,j ∣∣∣∣ ≤ ∥∥∥ [ V (t) − ] i ∥∥∥ 1 , ∑\nj:j 6=i\n∣∣∣∣ [ Z (t) + ] i,j ∣∣∣∣ ≤ ∥∥∥ [ V (t) + ] i ∥∥∥ 1 .\nTherefore,\nT 1 ≤ 8C 2 1 n2(α− ρ) ∥∥∥ [ V (t) − ] i ∥∥∥ 1 + 8C21ρ n(α− ρ) ,\nT 2 ≤ 16C 2 1ρ n(α− ρ) + 8C1ρ n(α− ρ) ∥∥∥ [ V (t) + ] i ∥∥∥ 1 ,\nT 3 ≤ 2C2 n\n∥∥∥ [ V (t) + ] i ∥∥∥ 1 .\nand thus we have ∥∥∥ [ Ẽ (t) + ] i ∥∥∥ 1 ≤ 8C 2 1 n2(α− ρ) ∥∥∥ [ V (t) − ] i ∥∥∥ 1 + ( 2C2 n + 8C1ρ n(α− ρ) )∥∥∥ [ V (t) + ] i ∥∥∥ 1 + 24C21ρ n(α− ρ) .\nSimilarly, for any i ∈ [n], ∥∥∥∥ [ Ẽ (t) +\n]i∥∥∥∥ 1 ≤ 8C 2 1 n2(α− ρ) ∥∥∥∥ [ V (t) − ]i∥∥∥∥ 1 + ( 2C2 n + 8C1ρ n(α− ρ) )∥∥∥∥ [ V (t) + ]i∥∥∥∥ 1 + 24C21ρ n(α− ρ) .\nPutting the two together, we have ∥∥∥Ẽ(t)+ ∥∥∥ s ≤ 8C 2 1 n2(α− ρ) ∥∥∥V(t)− ∥∥∥ s + ( 2C2 n + 8C1ρ n(α− ρ) )∥∥∥V(t)+ ∥∥∥ s + 24C21ρ n(α− ρ) . (25)\nBy Lemma 15 and ℓ ≤ 18 , we have: ∥∥∥V(t)+ ∥∥∥ s ≤ 32 21 ∥∥∥E(t)− ∥∥∥ s + 32 147 ∥∥∥E(t)+ ∥∥∥ s ,\n∥∥∥V(t)− ∥∥∥ s ≤ 32 21 ∥∥∥E(t)+ ∥∥∥ s + 32 147 ∥∥∥E(t)− ∥∥∥ s\nSo (25) becomes ∥∥∥Ẽ(t)+ ∥∥∥ s ≤ ( 64C2 147n + 256C1ρ 147n(α− ρ) + 256C21 21n2(α− ρ) )∥∥∥E(t)+ ∥∥∥ s\n(26)\n+ ( 64C2 21n + 256C1ρ 21n(α− ρ) + 256C21 147n2(α − ρ) )∥∥∥E(t)− ∥∥∥ s + 24C21ρ n(α− ρ) . (27)\nNow consider the negative part. The same argument as above leads to ∥∥∥Ẽ(t)− ∥∥∥ s ≤ ( 64C21 147n2 + 256C1ρ 147n(α− ρ) + 256C21 21n2(α− ρ) )∥∥∥E(t)+ ∥∥∥ s\n+ ( 64C21 21n2 + 256C1ρ 21n(α− ρ) + 256C21 147n2(α − ρ) )∥∥∥E(t)− ∥∥∥ s + 24C21ρ n(α− ρ) . (28)\nNote the difference between (27) and (28): C2n in the former is replaced by C21 n2 in the latter, which is much smaller. This is crucial for our proof, which will be clear below.\nFor simplicity, we introduce the following notations:\nat := ∥∥∥E(t)+ ∥∥∥ s , bt := ∥∥∥E(t)− ∥∥∥ s .\nThen by the update rule, we have\nat+1 ≤ (1− η)at + ηr ∥∥∥Ẽ(t)+ ∥∥∥ s , bt+1 ≤ (1− η)bt + ηr ∥∥∥Ẽ(t)− ∥∥∥ s .\nPlugging in (27)and since r = nc2 ≤ 2n C2 , we have\nat+1 ≤ (1− η)at + η 2n\nC2\n( 64C2 147n + 256C1ρ 147n(α− ρ) + 256C21 21n2(α− ρ) ) at\n+ η 2n\nC2\n( 64C2 21n + 256C1ρ 21n(α− ρ) + 256C21 147n2(α− ρ) ) bt + η 2n\nC2\n24C21ρ\nn(α− ρ)\nbt+1 ≤ (1− η)bt + η 2n\nC2\n( 64C21 147n2 + 256C1ρ 147n(α− ρ) + 256C21 21n2(α− ρ) ) at\n+ η 2n\nC2\n( 64C21 21n2 + 256C1ρ 21n(α− ρ) + 256C21 147n2(α − ρ) ) bt + η 2n\nC2\n24C21ρ\nn(α− ρ) .\nWhen 512C1ρC2(α−ρ) ≤ 1 2 and 512C21 C2n(α−ρ) ≤ 1 14 ,\nat+1 ≤ (1− η)at + 129\n147 ηat +\n129\n21 ηbt + η\n48C21ρ\nC2(α − ρ)\n≤ ( 1− 18\n147 η\n) at + 129\n21 ηbt + η\n48C21ρ\nC2(α− ρ)\nSimilarly, when 512C1ρC2(α−ρ) ≤ 1 2 and 512C21 C2n(α−ρ) ≤ 1 14 , and furthermore, 128C21 C2n ≤ 14 ,\nbt+1 ≤ (1− η)bt + 1\n100 ηat +\n1\n25 ηbt + η\n48C21ρ\nC2(α − ρ)\n≤ ( 1− 24\n25 η\n) bt + 1\n100 ηat + η\n48C21ρ\nC2(α − ρ)\nLet h = 48C 2 1ρ\nC2(α−ρ) , we then have:\nat+1 ≤ ( 1− 3\n25 η\n) at + 7ηbt + ηh,\nbt+1 ≤ ( 1− 24\n25 η\n) bt + 1\n100 ηat + ηh.\nNow set β = √ 842+2800−84\n2 , so that\nat+1 + βbt+1 ≤ ( 1− 3\n25 η\n) at + 7ηbt + ηh+ ( β − 24\n25 ηβ\n) bt + β\n100 ηat + ηβh\n= ( 1− 3\n25 η +\nβ\n100 η\n) (at + βbt) + η(1 + β)h\n≤ ( 1− 1\n25 η\n) (at + βbt) + 9ηh,\nwhere the last inequality follows from that β < 8.\nNote that the recurrence is true up to t+ 1. Using Lemma 29 to solve this recurrence, we obtain\nat + bt ≤ a0 + b0 + 250h ≤ 1 10 + 250h ≤ 1 8\nwhen 4000C 2 1ρ\nC2(α−ρ) ≤ 1 100 . Moreover, we know that\n∥∥∥E(t+1) ∥∥∥ s ≤ at+1 + βbt+1 ≤ ( 1− 1 25 η )t + 250h.\n(3) Finally, consider the noise term. Set the sample size N to be large enough, so that by Lemma 14, we have\n∣∣∣Ñ(t)i,j ∣∣∣ ≤ 4CνC1 (1− 2× ℓ)2n(α− ρ) + ∣∣∣[N(t)s ]i,j ∣∣∣\n≤ 8CνC1 n(α− ρ) .\nThen by the update rule, we have ∣∣∣N(t+1)i,j ∣∣∣ ≤ 8CνC1(α−ρ)c2 . Then ∥∥∥N(t+1)\n∥∥∥ ∞ ≤ nmax i,j ∣∣∣N(t+1)i,j ∣∣∣ ≤ 8nCνC1 (α− ρ)c2 ≤ 1\n8‖(A∗)†‖∞ where the last inequality is due to\nCν ≤ (α − ρ)c2\n64nC1‖(A∗)†‖∞ .\nOn the other hand, by Lemma 16, we have\n‖ξ(t+1)‖∞ ≤ 3‖(A∗)†‖∞(‖N(t+1)‖∞ + Cν)\n≤ 3‖(A∗)†‖∞ (\n8nCνC1 (α− ρ)c2 + Cν\n) ≤ ρ\nwhere the last inequality is due to\nCν ≤ (α− ρ)c2ρ\n48nC1‖(A∗)†‖∞ , and Cν ≤\nρ\n6‖(A∗)†‖∞ .\nWe also have (which will be useful in proving the final bound) ∥∥∥N(t+1)\n∥∥∥ 1 ≤ mmax i,j ∣∣∣N(t+1)i,j ∣∣∣ ≤ 8mCνC1 (α− ρ)c2 ≤ c 10\nwhere the last inequality is due to\nCν ≤ c(α− ρ)c2 80mC1 .\nNow, we shall prove the theorem statements. Recall that solving the recurrence about at and bt leads to ∥∥∥E(t+1)\n∥∥∥ s ≤ at+1 + βbt+1 ≤ ( 1− 1 25 η )t + 250h.\nSince the setting of ρ makes sure h = O(c), when t = O ( ln 1ǫ ) , we have the second statement∥∥∥Ê\n∥∥∥ s ≤ ǫ+ c2 . Note that\nA ∗ Σ̂ = A−A∗Ê− N̂\nand ∥∥∥ [ A ∗ Σ̂ ] i ∥∥∥ = Σ̂i,i, ‖A‖1 = 1, ∥∥∥A∗Ê ∥∥∥ 1 = ∥∥∥Ê ∥∥∥ 1 , so we have\nΣ̂i,i ≥ ‖A‖1 − ∥∥∥Ê ∥∥∥ 1 − ∥∥∥N̂ ∥∥∥ 1\n≥ 1− ǫ− c. Similarly,\nΣ̂i,i ≤ ‖A‖1 + ∥∥∥Ê ∥∥∥ 1 + ∥∥∥N̂ ∥∥∥ 1\n≤ 1 + ǫ+ c. Then the final statement of the theorem follows by replacing c with c/4. This completes the proof.\nTheorem 7 (Unbiased noise). If Assumption (A0)-(A3) and (N2) are satisfied with Cν = c2G √ cn C1 max{m,n‖(A∗)†‖ ∞ } and the other parameters set as in Theorem 6, then the same guarantee holds.\nProof. The proof is similar to that of Theorem 6, except using the second bound for unbiased noise in Lemma 14. We highlight the different part, that is, the induction on the noise term.\nIn the induction, by Lemma 14 we have when N is large enough, ∣∣∣Ñ(t)i,j ∣∣∣ ≤ 2C1Cνρ ′(1 + ‖A†N(t)‖∞) (1− 2ℓ)n(α− ρ′) + ∣∣∣∣ [ N (t) s ] i,j ∣∣∣∣ ≤ 3C1Cνρ ′(1 + ‖A†N(t)‖∞) n(α− ρ′) .\nBy Lemma 16 and the induction, we have ‖A†N(t)‖∞ ≤ 1/4. Furthermore, ρ′ ≤ Cν‖A†‖∞ ≤ 2Cν‖(A∗)†‖∞ and the parameter setting makes sure ρ′ ≤ α/2. Then\n∣∣∣Ñ(t)i,j ∣∣∣ ≤\n16C2νC1 ∥∥(A∗)† ∥∥ ∞\nnα .\nThen by the update rule, we have ∣∣∣N(t+1)i,j ∣∣∣ ≤ 32C2νC1 ∥∥(A∗)† ∥∥ ∞\nc2α\nand ∥∥∥N(t+1)\n∥∥∥ ∞ ≤ 32nC2νC1\n∥∥(A∗)† ∥∥ ∞\nc2α ≤ 1 8‖(A∗)†‖∞ (29)\nby the definition of α, and Cν ≤ 1256 c2C1 √ n n‖(A∗)†‖∞ . This completes the induction for the noise.\nAlso, in proving the final bounds, we have ∥∥∥N(t+1)\n∥∥∥ 1 ≤\n32mC2νC1 ∥∥(A∗)† ∥∥ ∞\nc2α ≤ c 10 (30)\nby the definition of α, and\nCν ≤ c\n320 c2 C1\n√ n\nmax {m,n‖(A∗)†‖∞} ≤\n√ c\n320 c2 C1 1√ m‖(A∗)†‖∞\nwhere the last inequality can be shown by consider the two cases when ∥∥(A∗)† ∥∥ ∞ ≤ m/n and∥∥(A∗)†\n∥∥ ∞ ≥ m/n. The rest of the proof is the same as in Theorem 6."
    }, {
      "heading" : "C Results for general proportions: Equilibration",
      "text" : "Algorithm 2 ColumnUpdate Input: A matrix A, a threshold value α, a step size η, ratios {rj : j ∈ [n]}, iteration number T , a\nsubset S ⊆ [n], sample size N 1: Set A(0) = A 2: for t = 0 → T − 1 do 3:\n∀i ∈ S, [A(t+1)]i = [ (1− η)A(t) + riηẼ [ (y − y′)(x− x′)⊤ ]] i\n(31)\nOutput: Â = A(T )\nAlgorithm 3 Rescale Input: A matrix A, a threshold value α, a step size η, ratios {rj : j ∈ [n]}, iteration number T , and\na set S ⊆ [n], ǫ ∈ (0, 1). 1: Let Ã = ColumnUpdate(A, α, η, {rj}j, T, S,N) 2: for i ∈ S do 3: Set [Â]i = 11−ǫ [Ã]i"
    }, {
      "heading" : "Output: Â",
      "text" : "Algorithm 4 Equilibration Input: A, α, η, T , and ǫ ∈ (0, 1), λ,N\n1: S ← ∅, D ← I 2: while |S| ≤ n do 3: mj ← Ê[x2j ] for j 6∈ S using N examples 4: while maxj 6∈S mj < λ do 5: A ← Rescale(A, α, η, {3/(5mj) : j ∈ [n]}, T, S, ǫ,N) 6: λ ← (1− ǫ)λ, Dj,j ← Dj,j/(1− ǫ) 7: mj ← (1− ǫ)2mj for j ∈ S, and mj ← Ê[x2j ] for j 6∈ S using N examples 8: S ← S ∪ {j : mj ≥ λ}\nOutput: A\nWhen the feature have various proportions (i.e., E[(x∗i ) 2] varies for different i), we propose Algorithm 4 for balancing them. The idea is quite simple: instead of solving Y ≈ A∗X, we could also solve Y ≈ [A∗D][(D)−1X] for a positive diagonal matrix D. Our goal is to find A = A∗D(Σ+ E) +N so that Σ is large, E,N are small, while E[(x∗i )\n2]/D2i,i is with in a factor of 2 from each other.\nThe algorithm works at stages and keeps a working set S of column index i such that E[(x∗i ) 2]/D2i,i is above a threshold λ. At each stage, it only updates the columns in S; at the end of the stage, it increases these columns by a small factor so that E[(x∗i )\n2]/D2i,i decreases. Then it decreases the threshold λ, and add more columns to the working set and repeat. In this way, E[(x∗i )\n2]/D2i,i(i ∈ S) are always balanced; in particular, they are balanced at the end when S = [n]. Formally,\nTheorem 18 (Main: Equilibration). If there exists an absolute constant G such that Assumption (A1)-(A3) and (N1) are satisfied with l = 1/50, C31 ≤ Gc22n, max { Cν , ‖N(0)‖∞ } ≤ Gc 4 2\nC51n‖(A∗)†‖∞ ,\nand additionally Σ(0) (1 − ℓ)I, and E ≥ 0 entry-wise, then there exist α, η, T, λ such that for sufficiently small ǫ > 0 and sufficiently large N = poly(n,m, 1/ǫ, 1/δ) the following hold with probability at least 1−δ: Algorithm 4 outputs a solutionA = A∗D(Σ+E)+Nwhere Σ (1−ℓ)I is diagonal, ‖E‖s ≤ γℓ is off-diagonal, ‖N‖∞ ≤ 2‖N(0)‖∞, and D is diagonal and satisfies\nmaxi∈[n] 1\nD2 i,i\nE[(x∗i ) 2]\nminj∈[n] 1\nD2 j,j\nE[(x∗j ) 2]\n≤ 2.\nIf Assumption (A1)-(A3) and (N2) are satisfied with the same parameters except max { Cν , ‖N(0)‖∞ } ≤ min {√ Gc42 C51n 1 ‖(A∗)†‖∞ , Gc22 C31‖(A∗)†‖∞ } , then the same guarantees hold.\nNow, we can view A∗D as the ground-truth feature matrix and D−1x∗ as the weights. Then applying Algorithm 1 with A can recover A∗D, and after normalization we get A∗.\nThe initialization condition of the theorem can be achieved by the popular practical heuristic that sets the columns of A(0) to reasonable almost pure data points. It is generally believed that it gives E\n(0) i,j ≥ 0 andN(0) = 0. We note that the parameters are not optimized; the algorithm can potentially\ntolerate much better initialization.\nIntuition. Before delving into the specifics of the algorithm, it will be useful to provide a highlevel outline of the proof. As described above, the algorithm makes use of the fact that samples from a ground truth matrix A∗ and distribution x∗ can equivalently be viewed as coming from the ground truth matrix A∗D and distribution D−1x∗, for some diagonal matrix D. Therefore, the goal is to find a D such that the features are balanced:\nmaxi∈[n] E[(x∗i ) 2]\nD2 i,i\nmini∈[n] E[(x∗ i )2]\nD2 i,i\n≤ κ.\nThe algorithm will implicitly calculate such a D gradually. Namely, at any point in time, the algorithm will have an active set S ⊆ [n] of features, which are balanced, i.e.\nmaxi∈[n] E[(x∗i ) 2]\nD2 i,i\nmini∈S E[(x∗ i )2]\nD2 i,i\n≤ κ. (32)\nIt is clear that when S = [n] the algorithm achieves the goal. Our algorithm begins with S = ∅ and gradually increase S until S = [n].\nThe mechanism for increasing S will be as follows. Given S, A is of the form\nA = A∗D(Σ+E) +N\nwith\nE =\n[ E1,1 E1,2\nE2,1 E2,2\n]\nwhere the columns of A are sorted such that the first |S| columns correspond to the features of S, and E1,1 ∈ R|S|×|S|, E2,1 ∈ R(n−|S|)×|S|, E1,2 ∈ R|S|×(n−|S|), E2,2 ∈ R(n−|S|)×(n−|S|). Then scaling up the columns of A indexed by S by a factor of 11−ǫ is equivalent to\n(1) scaling up the columns of D indexed by S by a factor of 11−ǫ and\n(2) scaling up the columns of E2,1 by a factor of 11−ǫ and (3) scaling down the columns of E1,2 by a factor of 1− ǫ.\nTherefore, to increase the set S, the algorithm will scale up the columns of A indexed by S, until some j 6∈ S satisfies\nmax i∈[n]\nE[(x∗i ) 2]\nD2i,i\n≤ κ E[(x∗j ) 2]\nD2j,j\n.\nThen it can add j into S while keeping the corresponding features balanced as in (32). Note that we do not need to explicitly maintain D, though it can be calculated along with the scaling. Further note that the values of E[(x∗i ) 2] are not known but they can be estimated using the current A.\nHowever, there is still one caveat: E should be kept small, so that at the end of the algorithm, we still have a good initialization A. For this reason, the algorithm additionally maintains that for a small constant 1 < γ < 2,\n‖E1,1‖s ≤ γℓ, ‖E1,2‖s ≤ ℓ, ‖E2,1‖s ≤ γℓ, ‖E2,2‖s ≤ ℓ. (33)\nSince scaling up A will scale up E2,1, we will need to first decrease ‖E2,1‖s before the scaling step. The key observation is that by applying our training algorithm only on the columns indexed by S, ‖E1,1‖s and ‖E2,1‖s will be decreased, while ‖E1,2‖s and ‖E2,2‖s unchanged. On a high level, using the fact that the matrix E1,2 has no negative entries (which we get by virtue of our initialization), and the fact that the contribution in the updates to the entry (E1,1)i,j mostly comes from (E1,1)j,i (i.e. the matrix E1,1 in the first order contribution “updates itself”), and the fact that the features in S are balanced, we can show that after sufficiently many updates, the symmetric norm of E1,1 and E2,1 drops by a reasonable amount: ‖E1,1‖s ≤ (γ− 1)ℓ and ‖E2,1‖s ≤ (1− ǫ)(γ− 1)ℓ. Now, we can do the scaling step without hurting the invariant 33.\nOrganization. The result of the section is as follows. We first prove in Section C.1 that applying our training algorithm only on the columns indexed by S will decrease ‖E1,1‖s and ‖E2,1‖s. Then in Section C.2 we analyze the scaling step, and show that the invariant (33) is maintained. In Section C.3, we show how to increase S while maintaining the invariant (32), where the main technical details are about how to estimate E[(x∗i ) 2]."
    }, {
      "heading" : "C.1 Equilibration: ColumnUpdate",
      "text" : "In this subsection, we focus on the update step, bounding the changes of Σ,E, and N.\nFirst recall some notations. Let A = A∗(Σ+E)+N where Σ is diagonal, E is off diagonal, and N is the component outside the span of A∗.3 Given the set S ⊆ [n] and a matrix M ∈ Rn×n, let M1,1 denote the submatrix indexed by S × S, and M2,1 denote the submatrix indexed by ([n] − S)× S, M1,2 denote the submatrix indexed by S × ([n] − S), and M2,2 denote the submatrix indexed by ([n]− S)× ([n]− S). 4 In the special case when S = [s] where s = |S|,\nM =\n[ M1,1 M1,2\nM2,1 M2,2\n] .\nAlso, let MS denote the submatrix formed by the columns indexed by S, and M−S the submatrix formed by the other columns. 5\nThe input A(0) of Algorithm 2 can be written as A(0) = A∗(Σ(0) + E(0)) + N(0) where Σ(0) is diagonal, and E(0) is off diagonal. Define E(0)1,1,E (0) 1,2,E (0) 2,1 and Ê2,2 as described above. Similarly, define Ê1,1, Ê1,2, Ê2,1 and Ê2,2 for the output Â = A∗(Σ̂+Ê)+N̂ of Algorithm 2. Finally, define N (0) S , N (0) −S , N̂S , and N̂−S as described above.\nThe main result of the subsection is Lemma 19.\n3Note that A∗ here can be any ground-truth matrix; in particular, later Lemma 19 will be applied where A∗\nin the lemma corresponds to A∗D in the intuition described above. 4These notations will be used for M = E, M = Ẽ, and related matrices. 5These notations will be used for M = N or M = Ñ, and related matrices.\nLemma 19 (Main: ColumnUpdate). Define\nRj = E[(x ∗ j ) 2], R = max j∈[n] Rj , r = max j∈S rj , (34)\nh1 = r 8C1(C1 + 1)ρ (1 − ℓ− βℓ)n(α− ρ) + 4C21 (1− ℓ− βℓ)n2(α− ρ)r (\n1 (1 − ℓ− βℓ) + 1 ) , (35)\nh2 = r Rβ2ℓ2 (1 − ℓ)2(1− ℓ− βℓ) + 12C1(C1 + 1) n2(α− ρ)(1 − ℓ− βℓ)\n( 1 1− ℓ− βℓ + nρ ) r, (36)\nh = h1 + h2, (37)\nUa = 8rCνC1 α− ρ , (38)\nUn = 10rC1C\n2 ν ∥∥(A∗)† ∥∥ ∞\n(1− 2ℓ)(α− 2Cν‖(A∗)†‖∞) . (39)\nSuppose ℓ ≤ 1/8, β is a constant with βℓ ≤ 1/2, γ ∈ (1, 2), ǫ ∈ (0, 1). The initialization satisfies (1 − ℓ)I Σ(0), ∥∥∥E(0)1,1 ∥∥∥ s ≤ γℓ, ∥∥∥E(0)2,1 ∥∥∥ s ≤ γℓ, ∥∥∥(E(0)1,2;E (0) 2,2) ∥∥∥ s ≤ ℓ, E(0)1,2 ≥ 0 and E (0) 2,2 ≥ 0 entrywise, and ‖N(0)−S‖∞ ≤ U and ‖N (0) S ‖∞ ≤ 2U ≤ 1/(16‖(A∗)†‖∞). Furthermore, the parameters satisfy that for any i ∈ S,\nη ( 1 + 2riRi\n1 (1− ℓ)2 βℓ2 1− βℓ− ℓ + ri 2C1 n\n( α+ 2ρ+\nC1 n + 2C1 n βℓ(1− βℓ) (1− ℓ)2(1− βℓ − ℓ) )) ≤ ℓ (40)\nriRi\n( 2− 2 1\n(1− ℓ)2 βℓ2 1− βℓ − ℓ\n) − ri ( 2C1 n ( α+ 2ρ+ C1 n 1 1− ℓ + 2C1 n βℓ(1− βℓ) (1 − ℓ)2(1− βℓ− ℓ) )) ≥ 1− ℓ\n(41)\nh1 ≤ ℓ, ( rR (1 − ℓ)2 + 1 ) (ǫ+ h1) + (ǫ+ h2) ≤ (γ − 1)ℓ (42)\nǫ+ h2 ≤ (1− ǫ)(γ − 1)ℓ (43)\nh1 + ℓ ≤ (β − 1)ℓ, h2 + ( rR (1 − ℓ)2 + 1 ) ℓ ≤ (β − 1)ℓ (44)\n3‖(A∗)†‖∞ (3U + Cν) ≤ ρ < α. (45) If we have adversarial noise (Assumption (N1)), assume\nǫ′ + Ua ≤ (1 − ǫ)U, and 3‖(A∗)†‖∞ (2U + Ua + Cν) ≤ ρ < α < 1. (46) If we have unbiased noise (Assumption (N2)), assume\nǫ′ + Un ≤ (1− ǫ)U. (47) Finally, let N = poly (n,m, 1/δ, 1/ǫ) sufficiently large.\nThen with probability at least 1 − δ, after 2 ln(ǫ/(γℓ))ln(1−η) + ln(ǫ′/U) ln(1−η) iterations, the output of Algorithm"
    }, {
      "heading" : "2 is Â = A∗(Σ̂+ Ê) + N̂ satisfying",
      "text" : "(1− ℓ)I Σ̂ uI, ‖Ê1,1‖s ≤ (γ − 1)ℓ, ‖Ê2,1‖s ≤ (1− ǫ)(γ − 1)ℓ, ‖(Ê1,2; Ê2,2)‖s ≤ ℓ, and Ê1,2 ≥ 0 and Ê2,2 ≥ 0 entry-wise. Furthermore, ‖N̂−S‖∞ ≤ U and ‖N̂S‖∞ ≤ (1− ǫ)U.\nProof of Lemma 19. It follows from Lemma 22 and the conditions (42) and (43).\nTo prove Lemma 22, we will first consider how E changes after one update step, and then derive the recurrence for all steps in Lemma 22."
    }, {
      "heading" : "C.1.1 One update step of E",
      "text" : "In this subsection, we focus on one update step, bounding the change of E. So through out this subsection we will focus on a particular iteration t and omit the superscript (t), while in the next subsection we will put back the superscript.\nFor analysis, denote A(t) as\nA = A∗(Σ+E) +N\nwhere Σ is a diagonal matrix, E is an off-diagonal matrix, and N is the component of A that lies outside the span of A∗ (e.g., the noise caused by the noise in the sample).\nRecall the following notations:\nZ = (Σ+E)−1 ,\nV = Z−Σ−1 = Σ−1 ∞∑\nk=1\n(−EΣ−1)k,\nξ = −A†NZx∗ +A†ν.\nConsider the update term Ê [ (y − y′)(x− x′)⊤ ] and denote it as\n∆ = Ê [ (y − y′)(x − x′)⊤ ] = A∗(Σ̃+ Ẽ) + Ñ\nwhere Σ̃ is a diagonal matrix, Ẽ is an off-diagonal matrix, and N is the component of ∆ that lies outside the span of A∗.\nSince we now use empirical average, we will have sampling noise. Denote it as\nNs = Ê[(y − y′)(x− x′)⊤]− E[(y − y′)(x − x′)⊤].\nThen by definition, for y = A∗x∗ + ν and y′ = A∗(x′)∗ + ν′, we have\nÊ[(y − y′)(x − x′)⊤] = E[(y − y′)(x − x′)⊤] +Ns = A∗ E [ (x∗ − (x′)∗)(x − x′)⊤\n] ︸ ︷︷ ︸\nΣ̃+Ẽ\n+E [ (ν − ν′)(x− x′)⊤ ] +Ns︸ ︷︷ ︸\nÑ\n.\nRecall the definition of E1,1, i.e., it is the submatrix of E indexed by S × S. Define Ẽ1,1 similarly, i.e., it is the submatrix of Ẽ indexed by S × S. Define Ẽ1,2, Ẽ2,1 and Ẽ2,2 accordingly. So in the special case when S = [s] where s = |S|,\nẼ =\n[ Ẽ1,1 Ẽ1,2\nẼ2,1 Ẽ2,2\n] .\nWe also use the notation M+ or M− to denote the positive or negative part of a matrix M.\nLemma 20 (Update Ẽ1,1). Let Ẽ1,1 be defined as above. If ‖ξ‖∞ ≤ ρ < α < 1 and Σ (1− ℓ)I, then (1). Negative entries:\n‖Ẽ−1,1‖s ≤ 4C21‖Z‖s(‖Z‖s + 1) n2(α− ρ) + 8C1(C1 + 1)ρ‖Z‖s n(α− ρ) .\n(2) Positive entries:\n‖Ẽ+1,1‖s ≤ 12C1(C1 + 1)‖Z‖s\nn2(α− ρ) (‖Z‖s + nρ) + 2maxj∈[n]{E[(x ∗ j )\n2]} (\n1\n(1− ℓ)2 ‖E − 1,1‖s + ‖E‖2s (1− ℓ)2(1− ℓ− ‖E‖s)\n) .\nProof of Lemma 20. (1) By Lemma 13, we have\n‖Ẽ−1,1‖s ≤ max { 4C21‖Z‖s n2(α− ρ) ‖Z‖s + 4C21‖Z‖s n2(α− ρ)nρ, 8C1ρ n(α− ρ) (C1 + 1)‖Z‖s + 2C21 n2 ‖Z‖s } .\nObserve that for α < 1,\n4C21‖Z‖s(‖Z‖s + 1) n2(α− ρ) ≥ max { 4C21‖Z‖2s n2(α− ρ) , 2C21 n2 ‖Z‖s } .\nMoreover,\n8C1ρ n(α− ρ) (C1 + 1)‖Z‖s ≥ 4C21‖Z‖s n2(α − ρ)nρ.\nTherefore,\n‖Ẽ−1,1‖s ≤ 4C21‖Z‖s n2(α− ρ) + 8C1(C1 + 1)ρ‖Z‖s n(α− ρ) .\n(2) By Lemma 13, when Zi,j < 0,\nẼj,i ≤ 4C21‖Zi‖1 n2(α− ρ) (|Zi,j |+ ρ) .\nWhen Zi,j ≥ 0,\nẼj,i ≤ 8C1ρ\nn(α− ρ)\n( C1‖Zi‖1\nn + Zi,j\n) + 2E[(x∗j ) 2]Zi,j\nConsider a fixed i. Let G = {j ∈ S,Zi,j ≥ 0} and let Gc = S −G. We know that\n‖[Ẽ+1,1]i‖1 = ∑\nj∈[n] [Ẽ+1,1]j,i\n≤ ∑\nj∈Gc\n4C21‖Zi‖1 n2(α− ρ) (|Zi,j |+ ρ)\n+ ∑\nj∈G\n( 8C1ρ\nn(α− ρ)\n( C1‖Zi‖1\nn + Zi,j\n) + 2E[(x∗j ) 2]Zi,j )\n≤ 4C 2 1‖Z‖s n2(α− ρ) (‖Z‖s + nρ) + 8C1(C1 + 1)ρ n(α− ρ) ‖Z‖s + ∑\nj∈G 2E[(x∗j ) 2]Zi,j\n≤ 4C 2 1‖Z‖2s n2(α− ρ) + 4C21‖Z‖s n2(α − ρ)nρ+ 8C1(C1 + 1)ρ n(α− ρ) ‖Z‖s + ∑\nj∈S 2E[(x∗j ) 2]Zi,j\n≤ 12C1(C1 + 1)‖Z‖s n2(α− ρ) (‖Z‖s + nρ) +\n∑ j∈G 2E[(x∗j ) 2]Zi,j .\nA similar bound holds for ‖[Ẽ+1,1]i‖1. By the definition of Z, we know that\nZ = (Σ+E)−1\n= Σ−1 ∞∑\nk=0\n(−EΣ−1)k\n= Σ−1 −Σ−1EΣ−1 +Σ−1 ∞∑\nk=2\n(−EΣ−1)k.\nTherefore, we know that for i 6= j,\nZi,j ≤ −[Σ−1EΣ−1]i,j + | ∞∑\nk=2\nΣ −1[(−EΣ−1)k]i,j |.\nThis implies that\n∑ j∈G Zi,j ≤ ∑ j∈G\n( −[Σ−1EΣ−1]i,j + ∞∑\nk=2\n∣∣Σ−1[(−EΣ−1)k]i,j ∣∣ )\n≤ 1 (1− ℓ)2 ‖E − 1,1‖s + 1 1− ℓ\n‖E‖2s (1−ℓ)2\n1− ‖E‖s1−ℓ\n≤ 1 (1− ℓ)2 ‖E − 1,1‖s + ‖E‖2s (1− ℓ)2(1− ℓ− ‖E‖s) .\nPutting together, we complete the proof.\nLemma 21 (Update Ẽ2,1). Let Ẽ2,1 be defined as above, and suppose ‖ξ‖∞ ≤ ρ < α < 1, Σ (1− ℓ)I and E1,2 ≥ 0, then we have\n‖Ẽ2,1‖s ≤ 12C1(C1 + 1)‖Z‖s\nn2(α − ρ) (‖Z‖s + nρ) + 2maxj∈[n]{E[(x ∗ j ) 2]} ( ‖E‖2s (1− ℓ)2(1 − ℓ− ‖E‖s) ) .\nProof of Lemma 21. The proof is almost the same as that of Lemma 20, combined with the fact that E1,2 ≥ 0 entry-wise."
    }, {
      "heading" : "C.1.2 Recurrence",
      "text" : "Recall that A = A∗(Σ+E) +N\nand recall that E1,1 is the submatrix indexed by S × S, and E1,2,E2,1,E2,2 are defined according. Recall that MS denote the submatrix of M formed by columns indexed by S, and let M−S denote the submatrix formed by the other columns.\nLemma 22 (Recurrence). Suppose the conditions in Lemma 19 hold. Then with probability at least 1− δ, after 2 ln(ǫ/(γℓ))ln(1−η) iterations,\n(1− ℓ)I Σ(t), ‖(E(t)1,1)−‖s ≤ ǫ+ h1,\n‖(E(t)1,1)+‖s ≤ rR\n(1− ℓ)2 (ǫ + h1) + h2 + ǫ,\n‖(E(t)2,1)‖s ≤ ǫ+ h2.\nAlso, after ln(ǫ′/U) ln(1−η) iterations, for both adversarial and unbiased noise,\n∥∥∥N(t)−S ∥∥∥ ∞ ≤ U, ∥∥∥N(t)S ∥∥∥ ∞ ≤ (1 − ǫ)U.\nProof of Lemma 22. We first prove the following claims by induction. (1) (1− ℓ)I Σ(t), (2)\n‖(E−1,1)(t)‖s ≤ γℓ\n‖(E+1,1)(t)‖s ≤ rR\n(1− ℓ)2 γℓ+ h2\n‖E(t)2,1‖s ≤ γℓ ‖E(t)1,2‖s ≤ ℓ ‖E(t)2,2‖s ≤ ℓ,\n(3) ‖E(t)‖s ≤ βℓ, (4) for adversarial noise, ∥∥∥N(t)S ∥∥∥ ∞ ≤ U +Ua, and ‖ξ(t)‖∞ ≤ ρ; or for unbiased noise, ∥∥∥N(t)S ∥∥∥ ∞\n≤ U + Uu.\nThe basis case for t = 0 is trivial by assumptions. Now assume they are true for iteration t and show that they are true for iteration t+ 1.\n(1) By the update of Σ, we have\nΣ (t+1) = (1− η)Σ(t) + ηrΣ̃(t).\nTo lower bound Σ(t+1)i,i , we will consider two cases, Σ (t) i,i ≥ 1 and Σ (t) i,i ≤ 1.\nFor Σ(t)i,i ≥ 1, by Lemma 12,\nΣ̃i,i ≥ E [ (x∗i ) 2 ] ( 2Σ−1i,i − 2 |Vi,i| ) − 2C1\nn\n( α+ 2ρ+\nC1 n Σ −1 i,i + 2C1 n ∥∥∥[V]i ∥∥∥ 1\n)\n≥ −2Ri |Vi,i| − ( 2C1 n ( α+ 2ρ+ C1 n Σ −1 i,i + 2C1 n ∥∥∥[V]i ∥∥∥ 1 )) .\nHence,\nΣ (t+1) i,i ≥ (1− η)Σ (t) i,i − η ( 2riRi ∣∣∣V(t)i,i ∣∣∣+ ri ( 2C1 n ( α+ 2ρ+ C1 n (Σ (t) i,i ) −1 + 2C1 n ∥∥∥∥ [ V (t) ]i∥∥∥∥\n1\n)))\n≥ 1− η ( 1 + 2riRi ∣∣∣V(t)i,i ∣∣∣+ ri\n2C1 n\n( α+ 2ρ+\nC1 n + 2C1 n ∥∥∥∥ [ V (t) ]i∥∥∥∥\n1\n))\n≥ 1− η ( 1 + 2riRi\n1 (1− ℓ)2 βℓ2 1− βℓ − ℓ + ri 2C1 n\n( α+ 2ρ+\nC1 n + 2C1 n βℓ(1− βℓ) (1− ℓ)2(1− βℓ − ℓ)\n)) .\nwhere we use the bound on V(t). By condition (40), the claim follows.\nFor Σ(t)i,i ≤ 1, again by Lemma 12,\nΣ̃ (t) i,i ≥ E [ (x∗i ) 2 ] ( 2− 2 ∣∣∣V(t)i,i ∣∣∣ ) − ( 2C1 n ( α+ 2ρ+ C1 n (Σ (t) i,i ) −1 + 2C1 n ∥∥∥∥ [ V (t) ]i∥∥∥∥\n1\n)) .\nHence,\nΣ (t+1) i,i = (1− η)Σ (t) i,i + ηrΣ̃ (t) i,i\n≥ (1− η)(1 − ℓ)\n+ η ( riRi ( 2− 2 ∣∣∣V(t)i,i ∣∣∣ ) − ri ( 2C1 n ( α+ 2ρ+ C1 n (Σ (t) i,i ) −1 + 2C1 n ∥∥∥∥ [ V (t) ]i∥∥∥∥\n1\n)))\n≥ (1− η)(1 − ℓ) + ηriRi ( 2− 2 1\n(1− ℓ)2 βℓ2 1− βℓ − ℓ\n)\n− ηri ( 2C1 n ( α+ 2ρ+ C1 n 1 1− ℓ + 2C1 n βℓ(1− βℓ) (1 − ℓ)2(1− βℓ− ℓ) )) .\nBy condition (41), the claim follows.\n(2) By Lemma 20,\n‖(Ẽ(t+1)1,1 )−‖s ≤ 8C1(C1 + 1)ρ‖Z(t)‖s n(α− ρ) + 4C21‖Z(t)‖s(‖Z(t)‖s + 1) n2(α− ρ) ,\n‖(Ẽ(t+1)1,1 )+‖s ≤ R (1 − ℓ)2 ‖(E − 1,1) (t)‖s + R‖E(t)‖2s (1− ℓ)2(1 − ℓ− ‖E(t)‖s)\n+ 12C1(C1 + 1)‖Z(t)‖s n2(α− ρ) (∥∥∥Z(t) ∥∥∥ s + nρ ) .\nBy the update rule, we have\n‖(E(t+1)1,1 )−‖s ≤ (1− η)‖(E (t) 1,1) −‖s\n+ rη 8C1(C1 + 1)ρ (1 − ℓ− βℓ)n(α− ρ) + 4C21 (1− ℓ− βℓ)n2(α− ρ)\n( 1 (1− ℓ− βℓ) + 1 ) rη,\n≤ (1− η)‖(E(t)1,1)−‖s + ηh1 (48)\n‖(E(t+1)1,1 )+‖s ≤ (1− η)‖(E (t) 1,1) +‖s + rη R\n(1 − ℓ)2 ‖(E (t) 1,1) −‖s\n+ rη Rβ2ℓ2\n(1− ℓ)2(1 − ℓ− βℓ)\n+ 12C1(C1 + 1)\nn2(α− ρ)(1 − ℓ− βℓ)\n( 1 1− ℓ− βℓ + nρ ) rη\n≤ (1− η)‖(E(t)1,1)+‖s + rη R\n(1 − ℓ)2 ‖(E (t) 1,1) −‖s + ηh2 (49)\nwhere we use ∥∥E(t) ∥∥ s ≤ βℓ and ‖Z(t)‖s ≤ 11−ℓ−βℓ .\nThe claim on ‖(E(t+1)1,1 )−‖s follows from (48) and the condition (42).\nFor ‖(E(t+1)1,1 )+‖s, by induction (49) becomes\n‖(E(t+1)1,1 )+‖s ≤ (1 − η)‖(E (t) 1,1) +‖s + rη R (1 − ℓ)2 γℓ+ ηh2 ≤ rR (1 − ℓ)2 γℓ+ h2.\nNow we consider ‖(E(t+1)2,1 )‖s. By Lemma 21,\n‖(E(t+1)2,1 )‖s ≤ (1− η)‖(E (t) 2,1)‖s\n+ rη Rβ2ℓ2\n(1− ℓ)2(1− ℓ− βℓ)\n+ 12C1(C1 + 1)\nn2(α− ρ)(1− ℓ − βℓ)\n( 1 1− ℓ− βℓ + nρ ) rη\n= (1− η)‖(E(t)2,1)‖s + ηh2 (50) ≤ γℓ\nwhere the last line follows by condition (43) and induction. Finally, clearly we have ∥∥∥E(t+1)1,2 ∥∥∥ s ≤ ℓ and ∥∥∥E(t+1)2,2 ∥∥∥ s ≤ ℓ, since they are not updated.\n(3) Note that (48) (49) hold for all iterations up to t+ 1. Then by Lemma 28, we have\n‖(E(t+1)1,1 )−‖s + ‖(E (t+1) 1,1 ) +‖s\n≤ max { ‖(E(0)1,1)−‖s + ‖(E (0) 1,1) +‖s, ‖(E(0)1,1)+‖s + h1, h2 + ( rR (1− ℓ)2 + 1 ) ‖(E(0)1,1)−‖s, h2 + ( rR (1 − ℓ)2 + 1 ) h1 } .\nSince h1 ≤ ℓ and h2 ≤ ℓ by (42)(43), and ‖(E(0)1,1)−‖s + ‖(E (0) 1,1) +‖s ≤ ℓ by assumption, we have\n‖(E(t+1)1,1 )−‖s + ‖(E (t+1) 1,1 ) +‖s ≤ max { ℓ+ h1, h2 + ( rR (1 − ℓ)2 + 1 ) ℓ } . (51)\nThen we have by condition (44),\n‖(E(t+1)1,1 )−‖s + ‖(E (t+1) 1,1 ) +‖s ≤ (β − 1)ℓ, ‖E(t+1)‖s ≤ βℓ.\n(4) Finally, we consider the noise. We first consider the adversarial noise. Set the sample size N to be large enough, so that by Lemma 14, we have\n∣∣∣Ñ(t)i,j ∣∣∣ ≤ 4CνC1 (1 − 2ℓ)2n(α− ρ) + ∣∣∣[Ñ(t)s ]i,j ∣∣∣ ≤ 8CνC1 n(α− ρ)\nand thus ∥∥∥N(t+1)\n∥∥∥ ∞ ≤ (1− η) ∥∥∥N(t) ∥∥∥ ∞ + η 8rCνC1 α− ρ . (52)\nThen for any t ≥ 0, ∥∥∥N(t)\n∥∥∥ ∞ ≤ ∥∥∥N(0) ∥∥∥ ∞ + 8rCνC1 α− ρ ≤ U + 8rCνC1 α− ρ ≤ 2U + Ua\nwhere the last inequality is by the definition of Ua. On the other hand, by Lemma 16, we have\n‖ξ(t)‖∞ ≤ 3‖(A∗)†‖∞(‖N(t)‖∞ + Cν)\n≤ 3‖(A∗)†‖∞ ( 2U +\n8rCνC1 α− ρ + Cν\n)\n≤ ρ where the last inequality is due to condition (46).\nWe now consider the unbiased noise, where the proof is similar. Set the sample size N to be large enough, so that by Lemma 14, we have\n∣∣∣Ñ(t)i,j ∣∣∣ ≤ 2C1Cνρ ′(1 + ‖A†N(t)‖∞) (1− 2ℓ)n(α− ρ′) + ∣∣∣[Ns]i,j ∣∣∣\n≤ 8C1C\n2 ν ∥∥(A∗)† ∥∥ ∞\n(1 − 2ℓ)n(α− 2Cν‖(A∗)†‖∞) + ∣∣∣[Ns]i,j ∣∣∣\n≤ 10C1C\n2 ν ∥∥(A∗)† ∥∥ ∞\n(1 − 2ℓ)n(α− 2Cν‖(A∗)†‖∞) ,\nand thus ∥∥∥N(t+1)S ∥∥∥ ∞ ≤ (1− η) ∥∥∥N(t)S ∥∥∥ ∞ + η 10rC1C 2 ν ∥∥(A∗)† ∥∥ ∞ (1− 2ℓ)(α− 2Cν‖(A∗)†‖∞) . (53)\nThen for any t ≥ 0, ∥∥∥N(t)S ∥∥∥ ∞ ≤ ∥∥∥NS(0) ∥∥∥ ∞ + 10rC1C 2 ν ∥∥(A∗)† ∥∥ ∞ (1− 2ℓ)(α− 2Cν‖(A∗)†‖∞) ≤ 2U +\n10rC1C 2 ν ∥∥(A∗)† ∥∥ ∞\n(1− 2ℓ)(α− 2Cν‖(A∗)†‖∞) ≤ 2U + Un\nwhere the last inequality is by the definition of Un. This completes the proof for the claims.\nNow, after proving the claims, we are ready to prove the last statement of the lemma. First, by (48) and Lemma 29, we have that after ln(ǫ/(γℓ))ln(1−η) iterations,\n‖(E(t)1,1)−‖s ≤ ǫ+ h1.\nNow (49) becomes\n‖(E(t+1)1,1 )+‖s ≤ (1− η)‖(E (t) 1,1) +‖s + rη R\n(1 − ℓ)2 (ǫ + h1) + ηh2 (54)\nAfter an additional ln(ǫ/(γℓ))ln(1−η) iterations, by Lemma 29,\n‖(E(t)1,1)+‖s ≤ rR\n(1− ℓ)2 (ǫ + h1) + h2 + ǫ\nSimilarly, Lemma 29 and (50), after ln(ǫ/(γℓ))ln(1−η) iterations,\n‖(E(t)2,1)‖s ≤ ǫ+ h2. ∥∥∥N(t)−S ∥∥∥ ∞ does not change since it is not updated. Now consider ∥∥∥N(t)S ∥∥∥ ∞ .\nFor the adversarial noise, by (52) and Lemma 29, after ln(ǫ′/U) ln(1−η) iterations,\n∥∥∥N(t)S ∥∥∥ ∞ ≤ ǫ′ + 8rCνC1 α− ρ ≤ (1− ǫ)U\nwhere the last inequality is due to condition (46).\nFor the unbiased noise, by (53) and Lemma 29, after ln(ǫ′/U) ln(1−η) iterations,\n∥∥∥N(t)S ∥∥∥ ∞ ≤ ǫ′ + 10rC1C 2 ν\n∥∥(A∗)† ∥∥ ∞\n(1− 2ℓ)(α− 2Cν‖(A∗)†‖∞) ≤ (1− ǫ)U\nwhere the last inequality is due to condition (47).\nThis completes the proof."
    }, {
      "heading" : "C.2 Equilibration: Rescale",
      "text" : "The input of of Algorithm 3 can be written as A(0) = A∗(Σ(0) + E(0)) +N(0). The output Â can be written as Â = (A∗D)(Σ̂ + Ê) + N̂ where Σ̂ is diagonal, and Ê is off diagonal, and D is a diagonal matrix with Di,i = 11−ǫ for i ∈ S and the rest being 1. Recall that for a matrix M, let M1,1 denote the submatrix of M indexed by S × S, and define M1,2,M2,1 and M2,2 accordingly. Also recall that MS denote the submatrix of M formed by columns indexed by S, and let M−S denote the submatrix formed by the other columns. Lemma 23 (Main: Rescale). Let A(0) = A∗(Σ(0)+E(0))+N(0) satisfies the condition in Lemma 19 and ǫ be defined as in Lemma 19. Then the output of Algorithm 3 is Â = (A∗D)(Σ̂ + Ê) + N̂ satisfying\n(1− ℓ)I Σ̂, ‖Ê1,1‖s ≤ (γ − 1)ℓ, ‖Ê2,1‖s ≤ (γ − 1)ℓ, ‖(Ê1,2, Ê2,2)‖s ≤ ℓ, ‖N̂S‖∞ ≤ U, ‖N̂−S‖∞ ≤ U. Moreover, Ê1,2 ≥ 0 and Ê2,2 ≥ 0 entry-wise.\nProof of Lemma 23. Note that Ã = A∗(Σ̃+ Ẽ) + Ñ for a diagonal matrix Σ̃, off-diagonal matrix Ẽ and error matrix Ñ. By lemma 19, we have Σ̃ (1− ℓ)I, error matrix ‖ÑS‖∞ ≤ (1− ǫ)U and\n‖Ẽ1,1‖s ≤ (γ − 1)ℓ, ‖Ẽ2,1‖s ≤ (1− ǫ)(γ − 1)ℓ, ‖(Ẽ1,2; Ẽ2,2)‖s ≤ ℓ and Ẽ1,2 ≥ 0 and Ẽ2,2 ≥ 0 entry-wise. Therefore, by the rescaling rule:\nÂ = ÃD = A∗(Σ̃+ Ẽ)D+ ÑD\n= A∗D(Σ̃+D−1ẼD) + ÑD.\nTherefore, Σ̂ = Σ̃ (1 − ℓ)I, ‖N̂S‖∞ ≤ 11−ǫ‖ÑS‖∞ ≤ U . ‖N̂−S‖∞ = ‖Ñ−S‖∞ ≤ U since it is not updated. For the Ê term, denote D1 = Diag ( 1 1−ǫ , . . . , 1 1−ǫ ) ∈ Rs×s. We know that\nÊ1,1 = D −1 1 Ẽ1,1D1 = Ẽ1,1\nÊ2,1 = Ẽ2,1D1 = 1\n1− ǫẼ2,1\nÊ1,2 = D −1 1 Ẽ1,2 = (1− ǫ)Ẽ1,2\nÊ2,2 = Ẽ2,2.\nThis leads to\n‖Ê1,1‖s ≤ (γ − 1)ℓ, ‖Ê2,1‖s ≤ (γ − 1)ℓ, ‖(Ê1,2, Ê2,2)‖s ≤ ℓ,\nwith Ê1,2, Ê2,2 ≥ 0. This completes the proof."
    }, {
      "heading" : "C.3 Equilibration: Main algorithm",
      "text" : "Lemma 24 (Main: Equilibration). Suppose the conditions in Lemma 23 each time Algorithm 3. Additionally, there exists constant 0 < b < 1, κ > 1 and u > 1 such that bκ > 1 such that the initial λ ≥ maxi∈[n] E[(x∗i )2]/b, and the initial Σ uI. Furthermore, for any λ ≥ mini∈[n] E[(x∗i )2]/κ,\n( 1 1− ℓ + h6 )2 bλ+ h25bκλ+ h3 ≤ ( 1− 1 100 ) λ, (55)\n( 1\nu − h6\n)2 (1− ǫ)bκλ− h25bκλ− h4 ≥ ( 1 + 1\n100\n) λ, 1\nu > h6 (56)\nh3 ≤ 1\n200 min i∈[n]\nE[(x∗i ) 2], (57)\nh4 ≤ 1\n200 min i∈[n]\nE[(x∗i ) 2], (58)\nwhere\nh3 = C21 n2 h5\n( h5 + 2\n1− ℓ\n) ,\nh4 = C21 n2 h5\n( h5 + 2\n1− ℓ\n) +\n2(α+ ρ)C1 n(1− ℓ) ,\nh5 = (γ + 1)ℓ(1− (γ + 1)ℓ) (1− ℓ)2(1− (γ + 2)ℓ) ,\nh6 = (γ + 1)ℓ2\n(1− ℓ)2(1− (γ + 2)ℓ) ."
    }, {
      "heading" : "Finally, set N = poly(1/mini∈[n] E[(x∗i )",
      "text" : "2], n, 1/δ) large enough.\nThen with probability at least 1 − δ, the following hold. During the execution of the algorithm, for any j ∈ S, ((\n1 u − h6\n)2 − κh25 − 1\n100\n) E[(x∗j ) 2]\n(Dj,j)2 ≤ mj ≤\n(( 1 1− ℓ + h6 )2 + κh25 + 1 100 ) E[(x∗j ) 2] (Dj,j)2 .\nFurthermore, the output of Algorithm 4 is A = A∗D(Σ + E) + N where Σ is diagonal and (1 − ℓ)I Σ, E is off diagonal and ‖E‖s ≤ γℓ, N satisfies ‖N‖∞ ≤ 2U , and\nmaxi∈[n] 1\nD2 i,i\nE[(x∗i ) 2]\nminj∈[n] 1\nD2 j,j\nE[(x∗j ) 2]\n≤ κ.\nProof of Lemma 24. We prove the lemma by induction. For notational convenience, let us introduce a counter (p) denoting the number of times the inner while cycle has been executed, and denote A as A(p). Recall that for a matrix M ∈ Rn×n and index set S ⊆ [n], let M1,1 denote the submatrix indexed by S × S, and M1,2,M2,1 and M2,2 are defined accordingly. Also, let MS denote the submatrix formed by the columns indexed by S, and M−S the submatrix formed by the other columns.\nOur inductive claims are as follows. At the beginning of each inner while cycle,\nA (p) = A∗D(p) ( Σ (p) +E(p) ) +N(p)\nwhere D(p) and Σ(p) are diagonal, E(p) are off diagonal satisfying\n(1) (1− ℓ)I Σ(p),\n(2) E(p)1,2 ≥ 0 and E (p) 2,2 ≥ 0 entry-wise and\n∥∥∥E(p)1,1 ∥∥∥ s ≤ γℓ, ∥∥∥E(p)2,1 ∥∥∥ s ≤ γℓ,\n∥∥∥(E(p)1,2;E (p) 2,2) ∥∥∥ s ≤ ℓ,\n(3) N(p)−S ≤ U and N (p) S ≤ 2U ,\n(4) We have\n(a) When E[(x∗j ) 2] < bλ(p), j /∈ S, then mj ≤ λ(p), (b) When E[(x∗j ) 2] ≥ (1− ǫ)bκλ(p), j /∈ S, then mj > λ(p),\nand consequently,\n(c) ∀i ∈ S, bλ(p) ≤ E[(x ∗ i ) 2]( D\n(p) i,i\n)2 ,\n(d) ∀i ∈ [n], E[(x ∗ i ) 2]( D\n(p) i,i\n)2 ≤ bκλ(p).\nThe claims are trivially true at initialization, so we proceed to the induction. Assume the claim is true at time p, we proceed to show it is true at time p+ 1.\nFirst, consider (1), (2) and (3). By Lemma 23, after applying the rescaling algorithm, (1−ℓ)I Σ(p) and\n‖E(p)1,1‖s ≤ (γ − 1)ℓ, ‖E (p) 2,1‖s ≤ (γ − 1)ℓ, ‖(E (p) 1,2,E (p) 2,2)‖s ≤ ℓ, ‖N (p) S ‖∞ ≤ U, ‖N (p) −S‖∞ ≤ U.\nMoreover, E(p)1,2 ≥ 0 and E (p) 2,2 ≥ 0 entry-wise. Observe that when moving from time p to p + 1, potentially the algorithm includes new elements in S. Then\n‖E(p+1)1,1 ‖s ≤ ‖E (p) 1,1‖s +max{‖E (p) 2,1‖s, ‖E (p) 1,2‖s} ≤ (γ − 1)ℓ+ ℓ = γℓ\nWhere the last inequality used the fact that γ < 2. Similarly,\n‖E(p+1)2,1 ‖s ≤ ‖E (p) 2,1‖s + ‖E (p) 2,2‖s ≤ (γ − 1)ℓ+ ℓ = γℓ.\nAlso, ‖(E(p+1)1,2 ,E (p+1) 2,2 )‖s ≤ ‖(E (p) 1,2,E (p) 2,2)‖s ≤ ℓ, and (E (p+1) 1,2 ,E (p+1) 2,2 ) ≥ 0 entry-wise. Furthermore, ‖N(p+1)−S ‖∞ ≤ ‖N (p) −S‖∞ ≤ U and\n‖N(p+1)S ‖∞ ≤ ‖N (p) S ‖∞ + ‖N (p) −S‖∞ ≤ 2U.\nHence, (1), (2) and (3) are also true at time (p+ 1).\nFinally, we proceed to (4). Since (a)(b) are true at time p, (c)(d) are true at time p+1. 6 Furthermore, when λ ≤ mini∈[n] E[(x∗i )2]/κ, it is guaranteed that all [n] ⊆ S, so we only need to prove that when λ ≥ mini∈[n] E[(x∗i )2]/κ, (a)(b) are also true at time p+ 1. To prove (a)(b) are true at time p + 1, we will use Lemma 25. Note that since A has been scaled, so A∗D should be regarded as the ground truth matrix A∗ in Lemma 25. We first make sure its assumption is satisfied. First, ‖N‖∞ ≤ 3U and ∥∥(A∗D)† ∥∥ ∞ ≤ ∥∥(A∗)† ∥∥ ∞. By Lemma 16 and condition (45), the assumption in Lemma 25 is satisfied.\nWe are now ready to prove (a). By Lemma 25,\nE[x2j ] ≤ ( Σ −1 j,j + |Vj,j | )2 E[(x∗j )2] D\n(p+1) j,j\n+ ‖[V]j‖22 max k∈[n]\nE[(x∗k) 2]\nD (p+1) k,k\n+ ‖[V]j‖1 ( ‖[V]j‖1 + 2Σ−1j,j ) C21 n2 .\n6Note that in (b), the factor (1− ǫ) is needed to ensure (d) is true at time p+ 1.\nBy Lemma 15, |Vj,j | ≤ h6, ‖[V]j‖22 ≤ ‖[V]j‖21 ≤ h25, so\nE[x2j ] ≤ ( 1 1− ℓ + h6 )2 E[(x∗j ) 2]\n(D (p+1) j,j )\n2 + h25 max k∈[n]\nE[(x∗k) 2] (D (p+1) k,k ) 2 + h3."
    }, {
      "heading" : "By (d), maxk∈[n]",
      "text" : "E[(x∗k) 2]\n(D (p+1) k,k\n)2 ≤ bκλ, so for any j /∈ S with E[(x∗j )2] =\nE[(x∗j ) 2]\n(D (p+1) j,j\n)2 < bλ, we have\nE[x2j ] ≤ ( 1 1− ℓ + h6 )2 bλ+ h25bκλ+ h3.\nBy using large enough sample, with high probability, the empirical estimation\nÊ[x2j ] ≤ E[x2j ] + 1\n100 λ ≤ λ\nwhere the last step is by condition (55).\nAs for (b), by Lemma 25 we have\nE[x2j ] ≥ ( Σ −1 j,j − |Vj,j | )2 E[(x∗j )2] (D\n(p+1) j,j )\n2 − ‖[V]j‖22 max k∈[n]\nE[(x∗k) 2] (D (p+1) k,k ) 2 − ( C21 n2 ‖[V]j‖1(‖[V]j‖1 + 2Σ−1j,j ) + 2(α+ ρ)C1 n Σ −1 j,j )\n≥ ( 1\nu − h6\n)2 E[(x∗j ) 2]\n(D (p+1) j,j )\n2 − h25 max k∈[n]\nE[(x∗k) 2] (D (p+1) k,k ) 2 − h4.\nThe last step uses that Σ−1j,j ≤ u, which is by the initial condition assumed and that it is not updated for j 6∈ S. Putting in the bound that E[(x ∗ k) 2]\n(D (p+1) k,k\n)2 ≤ bκλ, then for any j /∈ S with E[(x∗j )2] =\nE[(x∗j ) 2]\n(D (p+1) j,j\n)2 ≥ (1− ǫ)bκλ, we have\nE[x2j ] ≥ ( 1\nu − h6\n)2 (1 − ǫ)bκλ− h25bκλ− h4.\nAgain, use large enough sample to ensure that with high probability\nẼ[x2j ] ≥ E[x2j ]− 1\n100 λ ≥ λ\nwhere the last step follows from condition (56). This completes the proof of the induction.\nWe now prove the statements of the lemma. The statement about the output follows from the above claims. What is left is to prove that mj(j ∈ S) approximates E[(x∗j )2] well. Since mj for j ∈ S is updated along with Dj,j , we only need to check the right after adding j to S, the statement holds. Suppose the time point is p, we have\nE[x2j ] ≤ ( 1 1− ℓ + h6 )2 E[(x∗j ) 2]\n(D (p) j,j )\n2 + h25 max k∈[n]\nE[(x∗k) 2] (D (p) k,k) 2 + h3.\nSince j is in S, by the claims (c)(d) we have\nmax k∈[n]\nE[(x∗k) 2] (D (p) k,k) 2 ≤ bκλ(p) ≤ κ E[(x∗j ) 2] (D (p) j,j ) 2 .\nSince N is large enough so that\nE[x2j ] ≤ E[(x∗j )2] ( 1 + 1\n200\n) .\nCombined these with the condition (57), we have\nmj ≤ (( 1 1− ℓ + h6 )2 + κh25 + 1 100 ) E[(x∗j ) 2] (Dj,j)2 .\nThe upper bound on mj can be bounded similarly. This completes the proof of the lemma.\nThe following is the lemma used in the proof of Lemma 24.\nLemma 25 (Estimate of feature weight). Suppose |ξi| ≤ ρ < α for any example and every i ∈ [n], and suppose Σ 12I. Then\nE[x2i ] ≥ ( Σ −1 i,i − |Vi,i| )2 E[(x∗i )\n2]− ‖[V]i‖22max j∈[n] E[(x∗j ) 2]\n− ( C21 n2 ‖[V]i‖1(‖[V]i‖1 + 2Σ−1i,i ) + 2(α+ ρ)C1 n Σ −1 i,i )\nE[x2i ] ≤ ( Σ −1 i,i + |Vi,i| )2 E[(x∗i )\n2] + ‖[V]i‖22max j∈[n] E[(x∗j ) 2] + ‖[V]i‖1\n( ‖[V]i‖1 + 2Σ−1i,i ) C21 n2 .\nProof of Lemma 25. By the decoding rule,\nxi = [ φα(A †[A∗x∗ + ν]) ] i\n= [ φα (( Σ −1 +V ) x∗ + ξ )] i .\nLet [V]i = v and Σ−1i,i = σ, then we can rewrite above as\nxi = φα(σx ∗ i + 〈v, x∗〉+ ξi)\nwhich implies that\nσx∗i + 〈v, x∗〉 − ρ− α ≤ xi ≤ |σx∗i + 〈v, x∗〉| . (59)\nFirst, consider the lower bound.\nE[x2i ] ≥ E [(σx∗i + 〈v, x∗〉 − ρ− α)φα(σx∗i + 〈v, x∗〉+ ξi)]\nThe following simple lemma is useful.\nClaim 26. Let χ be a variable such that |χ| ≤ α, then for every w ∈ Rn, k ∈ [n],\nE[x∗kφα(〈w, x∗〉+ χ)] ≤ |wk|E[(x∗k)2] + C21 n2\n∑ j 6=k |wj | (60)\n≤ |wk|E[(x∗k)2] + C21 n2 ‖w‖1. (61)\nProof. The proof is a direct observation that when |χ| < α, φα(〈w, x〉 + χ) ≤ |〈w, x〉| ≤ 〈|w|, x〉\nwhere |w| is the entry wise absolute value.\nTherefore, we can obtain the following bounds.\n(1). By (17) in Lemma 12, we have\nE[x∗i φα(σx ∗ i + 〈v, x∗〉+ ξi)] ≥ Σ−1i,i E [ (x∗i ) 2 ] − (α + ρ)C1\nn − E\n[ (x∗i ) 2 ] |Vi,i| −\nC21 n2 ∥∥∥[V]i ∥∥∥ 1 ,\n(2). By (61) in the above claim,\nE[x∗jφα(σx ∗ i + 〈v, x∗〉+ ξi)] ≤ |vj |E[(x∗j )2] + C21 n2 (‖v‖1 + σ),\n(3). By (59), for j 6= i,\nE[φα(σx ∗ i + 〈v, x∗〉+ ξi)] ≤ E[|σx∗i + 〈v, x∗〉|] ≤ (σ + ‖v‖1)C1 n .\nPutting together, we can obtain\nE[x2i ] ≥ ( Σ −1 i,i − |Vi,i| )2 E[(x∗i )\n2]− ‖[V]i‖22 max j∈[n] E[(x∗j ) 2]\n− ( C21 n2 ‖[V]i‖1(‖[V]i‖1 + 2Σ−1i,i ) + 2(α+ ρ)C1 n Σ −1 i,i ) .\nSecond, we proceed to the upper bound. Similarly as the lower bound, by (59), we have\nE[x2i ] ≤ E    (|vi|+ σ)x∗i + ∑\nj 6=i |vj |x∗j\n φα(σx∗i + 〈v, x∗〉+ ξi)  \n= (|vi|+ σ)E[x∗i φα(σx∗i + 〈v, x∗〉+ ξi)] + ∑\nj 6=i |vj |E[x∗jφα(σx∗i + 〈v, x∗〉+ ξi)].\nFor the first summand, same as in (2), by (61) in the above claim we get\nE[x∗i φα(σx ∗ i + 〈v, x∗〉+ ξi)] ≤ (σ + |vi|)E[(x∗i )2] + C21 n2 ‖v‖1,\nE[x∗jφα(σx ∗ i + 〈v, x∗〉+ ξi)] ≤ |vj |E[(x∗j )2] + C21 n2 (‖v‖1 + σ).\nTherefore, we get\nE[x2i ] ≤ ( Σ −1 i,i + |Vi,i| )2 E[(x∗i ) 2] + ‖[V]i‖1(‖[V]i‖1 + 2Σ−1i,i ) C21 n2 + ‖[V]i‖22max j∈[n] E[(x∗j ) 2].\nwhich completes the proof."
    }, {
      "heading" : "C.4 Main theorem",
      "text" : "Theorem 18 (Main: Equilibration). If there exists an absolute constant G such that Assumption (A1)-(A3) and (N1) are satisfied with l = 1/50, C31 ≤ Gc22n, max { Cν , ‖N(0)‖∞ } ≤ Gc 4 2\nC51n‖(A∗)†‖∞ ,\nand additionally Σ(0) (1 − ℓ)I, and E ≥ 0 entry-wise, then there exist α, η, T, λ such that for sufficiently small ǫ > 0 and sufficiently large N = poly(n,m, 1/ǫ, 1/δ) the following hold with probability at least 1−δ: Algorithm 4 outputs a solutionA = A∗D(Σ+E)+Nwhere Σ (1−ℓ)I is diagonal, ‖E‖s ≤ γℓ is off-diagonal, ‖N‖∞ ≤ 2‖N(0)‖∞, and D is diagonal and satisfies\nmaxi∈[n] 1\nD2 i,i\nE[(x∗i ) 2]\nminj∈[n] 1\nD2 j,j\nE[(x∗j ) 2]\n≤ 2.\nIf Assumption (A1)-(A3) and (N2) are satisfied with the same parameters except max { Cν , ‖N(0)‖∞ } ≤ min {√ Gc42 C51n 1 ‖(A∗)†‖∞ , Gc22 C31‖(A∗)†‖∞ } , then the same guarantees hold.\nProof of Theorem 18. The theorem follows from Lemma 24 (taking union bound over all the iterations and setting a proper δ), if the conditions are satisfied. So in the following, we first specify the parameters and then verify the conditions in Lemma 19 and Lemma 24.\nRecall that ℓ = 1/50. Define u = 1 + ℓ, γ = 3/2, β = 4, κ = 2, b = 3/4, and let ǫ < 1/1000.\nConditions in Lemma 19. For (40), we need to compute riRi and the the third term. Note that by the induction in Lemma 24, the mj is an good approximation of E[(x∗j ) 2]/(Dj,j) 2. Furthermore, when Lemma 19 is applied in Lemma 24, it is applied on the ground-truth matrix (A∗)′ = A∗D and (x∗j ) ′ = x∗j/Dj,j , so mj is a good approximation of E[((x ∗ j ) ′)2]. Then\nriRi = 3E[((x∗i ) ′)2]\n5mi ≤ 3 5 ((\n1 u − h6 )2 − κh25 − 1100 ) .\nFor the third term, first note that C31 ≤ Gc22n, and thus C21 ≤ Gc2n by C1 > c2. Furthermore, ri = O(1/mi) = O(n/c2) for i ∈ S. Plugging in the parameters, we know that the third term is less than 1/1000 when G is sufficiently small. Then (40) can be verified by plugging the parameters. Similarly, for (41), we can compute riRi and let G small enough so that the second term is less than 1/1000, and then verify the condition.\nFor (42) (43) and (44), we need to bound h1 and h2, which in turn relies on r and rR. Since for i ∈ S, ri = O(n/c2), r = O(n/c2). Then similar to the argument as above, h1 < 2/10000 when G is sufficiently small. when Lemma 19 is applied in Lemma 24, it is applied on the ground-truth matrix (A∗)′ = A∗D and (x∗j ) ′ = x∗j/Dj,j . By the induction claims there, maxj∈[n] E[((x ∗ j )\n′)2] differ from minj∈S E[((x∗j )\n′)2] by a factor of at most κ, so rR ≤ 3κ5 . So the first term can be computed. The second term is less than 1/10000 when G is small enough. Then h2 can be computed. And the conditions can be verified.\nCondition (45) is true since max {Cν , ‖N‖∞} = O( c 2 2\nC31‖(A∗)†‖∞ ). Condition (46) is true by setting\nǫ′ < U/8 and by Ua < U/8 and U = ‖N‖∞ ≤ O( c 2 2\nC31‖(A∗)†‖∞ ). Similarly, condition (46) is true\nby setting ǫ′ < U/8 and by Un < U/8 and ‖N‖∞ is sufficiently small.\nConditions in Lemma 24. First, consider (57) and (58). As mentioned above, since C31 = O(c 2 2n) and C21 = O(c2n), then h3 and h4 can be made sufficiently small to satisfy the conditions. (55) and (56) can be verified by plugging (57) and (58) and the assumption that λ ≥ mini∈[n] E[(x∗i )2]/κ. This completes the proof."
    }, {
      "heading" : "D Auxiliary lemmas for solving recurrence",
      "text" : "The following lemmas are used when solving recurrence in our analysis.\nLemma 27 (Coupling update rule). Let {at}∞t=0, {bt}∞t=0 be sequences of non-negative numbers such that for fixed values h ≥ 0, η ∈ [0, 1], R > 4r > 0:\nat+1 ≤ (1− η)at + ηrbt + ηh bt+1 ≤ (1− η)bt + η\nR at + ηh\nThen the following two properties holds:\n1.\n∀t ≥ 0, at + bt ≤ a0 + b0 + Rr + 2R+ 1\nR− r h\n2. For all ǫ > 0, when t ≥ ln a0+b08ηǫ , we have:\nat ≤ R(r + 1) R− r h+ ǫ, bt ≤ R+ 1 R− r h+ ǫ\nProof of Lemma 27. Observe that the update rule is equivalent to ( at+1 − R(r + 1) R− r h ) ≤ (1 − η) ( at − R(r + 1) R− r h ) + ηr ( bt − R+ 1 R− r h )\n( bt+1 − R+ 1 R− r h ) ≤ (1 − η) ( bt − R + 1 R − rh ) + η R ( at − R(r + 1) R− r h )\nTherefore, define ct = at − R(r+1)R−r h and dt = bt − R+1R−rh, we can rewrite above as:\nct+1 ≤ (1− η)ct + ηrdt dt+1 ≤ (1− η)dt + η\nR ct\nSince we just need to upper bound ct, dt. without lose of generality, we can assume that\nct+1 = (1− η)ct + ηrdt dt+1 = (1− η)dt + η\nR ct\nWhich implies that ( ct+1 + √ R\nr dt+1\n) = ( 1− η + η √ r\nR\n)( ct + √ R\nr dt\n)\n( ct+1 − √ R\nr dt+1\n) = ( 1− η − η √ r\nR\n)( ct − √ R\nr dt\n)\nWhich can be simplified to ( ct + √ R\nr dt\n) = ( 1− η + η √ r\nR\n)t ( c0 + √ R\nr d0\n)\n( ct − √ R\nr dt\n) = ( 1− η − η √ r\nR\n)t ( c0 − √ R\nr d0\n)\nTherefore, we can solve\nct = 1\n2\n[( 1− η + η √ r\nR\n)t + ( 1− η − η √ r\nR\n)t] c0+ 1\n2\n√ R\nr\n[( 1− η + η √ r\nR\n)t − ( 1− η − η √ r\nR\n)t] d0\ndt = 1\n2\n√ r\nR\n[( 1− η + η √ r\nR\n)t − ( 1− η − η √ r\nR\n)t] c0+ 1\n2\n[( 1− η + η √ r\nR\n)t + ( 1− η − η √ r\nR\n)t] d0"
    }, {
      "heading" : "Observe that for every t ≥ 0, a ≥ b ≥ 0, at − bt ≤ (a− b)tat−1",
      "text" : "Which implies: ( 1− η + η √ r\nR\n)t − ( 1− η − η √ r\nR\n)t ≤ 2tη √ r\nR\n( 1− η + η √ r\nR\n)t−1\nTherefore, when c0, d0 ≥ 0,\nct ≤ ( 1− η + η √ r\nR\n)t c0 + tη ( 1− η + η √ r\nR\n)t−1 d0\nMoreover,\ndt ≤ r\nR η\n( 1− η + η √ r\nR\n)t c0 + ( 1− η + η √ r\nR\n)t d0\nTaking the optimal t, we obtain ct + dt ≤ c0 + d0, which implies that\nat + bt ≤ a0 + b0 + Rr + 2R+ 1\nR− r h\nOn the other hand, when t ≥ ln c0+d08ηǫ , ct, dt ≤ ǫ, which implies that\nat ≤ R(r + 1) R− r h+ ǫ, bt ≤ R+ 1 R− r h+ ǫ.\nLemma 28 (Simple coupling). Let {at}∞t=0, {bt}∞t=0 be sequences of non-negative numbers such that for fixed values h1, h2 ≥ 0, η ∈ [0, 1], r > 0:\nat+1 ≤ (1 − η)at + ηh1 bt+1 ≤ (1 − η)bt + ηsat + ηh2\nThen at ≤ ua := max {a0, h1} , bt ≤ max {b0, h2 + sua} .\nProof. We have (at+1 − h1) ≤ (1− η)(at − h1) (bt+1 − h2) ≤ (1− η)(bt − h2) + ηsat\nSolving the first one gives at ≤ ua := max {a0, h1} .\nThen (bt+1 − h2) ≤ (1 − η)(bt − h2) + ηsua\nleads to bt ≤ max {b0, h2 + sua} .\nLemma 29 (Simple recursion). Let {at}∞t=0 be a sequences of non-negative numbers such that for fixed values h ≥ 0, η ∈ [0, 1], at+1 ≤ (1− η)at + ηh. Then, at ≤ (1− η)ta0 + h, and thus for t ≥ ln(ǫ/a0)ln(1−η) , we have\nat ≤ ǫ+ h.\nProof. We will prove by induction that at ≤ (1 − η)ta0 + h, which implies the statement of the lemma. The base case is trivial, so we proceed to the induction:\nat+1 ≤ (1− η) ( (1− η)ta0 + h ) + ηh ≤ (1− η)t+1a0 + h\nas we need."
    }, {
      "heading" : "E Detailed discussion about related work",
      "text" : ""
    }, {
      "heading" : "E.1 Non-negative matrix factorization",
      "text" : "The area of non-negative matrix factorization (henceforth NMF) has a rich empirical history, starting with the work of [LS99]. In that paper, the authors propose two algorithms based on alternating minimization, one in KL divergence norm, and the other in Frobenius norm. They observe that these heuristics work quite well in practice, but no theoretical understanding of it is provided.\nOn the theoretical side, [AGKM12] provide a fixed-parameter tractable algorithm for NMF: namely when if the matrix A ∈ Rm×nand X ∈ Rn×N , they provide an algorithm that runs in time (mN)n. This is prohibitive unless n is extremely small. Furthermore, the algorithm is based on routines from algebraic geometry, so its tolerance to noise is fairly weak. More precisely, if there are matrices A∗, X\n∗, s.t. ‖Y −A∗X∗‖F ≤ ǫY\ntheir algorithm produces matrices A,X, s.t. ‖Y −A∗X∗‖F ≤ O(ǫ1/2n1/4)Y They further provide matching hardness results: namely they show there is no algorithm running in time (mN)o(n) unless there is a sub-exponential running time algorithm for 3-SAT. They also study the problem under separability assumptions about the feature matrix. [BGKP16] studies the problem under heavy noise setting, but also needs assumptions related to separability, such as the existence of dominant features. Also, their noise model is different from ours."
    }, {
      "heading" : "E.2 Topic modeling",
      "text" : "A closely related problem is topic modeling. Topic models are a generative model for text data, using the common bag-of-words assumption. In this case, the columns of the matrix A∗ (which have norm 1) can naturally be interpreted as topics, with the entries being the emmision probabilities of words in that topic. The vectors x∗ in this case also will have norm 1, and can be viewed as distributions over topics. In this way, y∗ = A∗x∗ can be viewed as the vector describing the emission probabilities of words in a given document: first a topic i is selected according to the distribution x∗, then a word is selected from topic i according to the distribution in column [A∗]i. There also exist work that assume x∗i ∈ [0, 1] and are independent (e.g., [ZX12]), which is closely related to our model. The distinction from NMF is that when documents are fairly short, the empirical frequencies of the words in the document might be very far from y∗. For this reason, typicall the algorithms with provable guarantees look at the empirical covariance matrix of the words, which will concentrate to the true one when the number of documents grows, even if the documents are very short. This, however, results in algorithms that scale quadratically in the vocabulary size, which often is prohibitive in practice. Also note that since x∗ is assumed to have norm 1 in topic modeling, it does not satisfy our assumption (A2). However, there also exist work on topic modeling [ZX12] that do not restrict x∗ is assumed to have norm 1 and can satisfying our assumption.\nThere is a rich body of empirical work on topic models, starting from the seminal work on LDA due to [BNJ03]. Typically in empirical papers the matrices A∗, as well as the vectors x∗ are learned using variational inference, which can be interpreted as a kind of alternating minimization in KL divergence norm, and in the limit of infinite-length documents converges to the [LS99] updates ([AR15]).\nFrom the theoretical side, there was a sequence of works by [AGM12],[AGH+13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14]. All of these works are based on either spectral or combinatorial (overlapping clustering) approaches, and need certain “non-overlapping”assumptions on the topics. For example, [AGM12] and [AGH+13] assume that the topic-word matrix contains “anchor words”. This means that each topic has a word which appears in that topic, and no other. [AHJK13] on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic. Finally, in the paper [AR15] a version of the standard variational inference updates is analyzed in the limit of infinite length documents. The algorithm there also involves a step of “decoding”, which recovers correctly the support of a given sample, and a “gradient descent” step, which updates A∗ in the direction of the gradient of a KL-divergence based objective function. However, [AR15] requires quite strong assumptions on both the warm start, and the amount of “non-overlapping” of the topics in the topic-word matrix.\nE.3 ICA\nIn the problem of independent component analysis (henceforth ICA, also known as blind-source separation), one is given samples y = A∗x∗ + η, where the distribution on the samples x∗ is independent for each coordinate, the 4-th moment of x∗i is strictly smaller than that of a Gaussian and A∗ has full rank. The classic papers [Com94] and [FJK96] solved this problem in the noiseless case, with an approach based on cumulants, and [AGMS12] solved it in another special case, when the noise η is Gaussian (albeit with an unknown covariance matrix).\nOur approach is significantly more robust to noise than these prior approaches, since it can handle both adversarial noise and zero mean noise. This is extremely important in practice, as often the nature of the noise may not be precisely known, let alone exactly Gaussian.\nE.4 Non-convex optimization via gradient descent\nThe framework of having a “decoding” for the samples, along with performing a gradient descentlike update for the model parameters has proven successful for dictionary learning as well, which is the problem of recovering the matrixA∗ from samples y = A∗x∗+η, where the matrixA∗ ∈ Rm×n is typically long (i.e. n ≫ m) and x∗ is sparse. (No non-negativity constraints are imposed on either\nA or x∗.) In this scenario, the columns of A∗ are thought of as a dictionary, and each sample y is generated as a (noisy) sparse combination of the columns of the dictionary.\nThe original empirical work which proposed an algorithm like this (in fact, it suggested that the V1 layer processes visual signals in the same manner) was due to [OF97]. In fact, it is suggest that similar families of algorithms based on “decoding” and gradient-descent are neurally plausible as mechanisms for a variety of tasks like clustering, dimension-reduction, NMF, etc. ([PC15a, PC15b, HPC14, PC14])\nA theoretical analysis of it came latter due to [AGMM15]. They showed that with a suitable warm start, the gradient calculated from the “decoding” of the samples is sufficiently correlated with the gradient calculated with the correct value x∗, therefore allowing them to show the algorithm converges to a matrix A close to the ground truth A∗. However, the assumption made in [AGMM15] is that the columns of A∗ are incoherent, which means that they have l2 norm bounded by 1, and inner products bounded by O( 1√\nm ). 7\nThe above techniques are not directly applicable to our case, as we don’t wish to have any assumptions on the matrix A∗. Additionally, the incoherence assumptions on the matrix A∗ used in [AGMM15], in the case when A∗ needs to be non-negative and has l1 column-wise norm would effectively imply that the columns of A∗ have very small overlap.\n7This is satisfied when the columns are random unit vectors, and intuitively says the columns of the dictionary are not too correlated."
    } ],
    "references" : [ {
      "title" : "A practical algorithm for topic modeling with provable guarantees",
      "author" : [ "S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu" ],
      "venue" : "ICML",
      "citeRegEx" : "AGH13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Computing a nonnegative matrix factorization–provably",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ravindran Kannan", "Ankur Moitra" ],
      "venue" : "STOC, pages 145–162. ACM,",
      "citeRegEx" : "AGKM12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning topic models – going beyond svd",
      "author" : [ "S. Arora", "R. Ge", "A. Moitra" ],
      "venue" : "FOCS",
      "citeRegEx" : "AGM12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Simple",
      "author" : [ "S. Arora", "R. Ge", "T. Ma", "A. Moitra" ],
      "venue" : "efficient, and neural algorithms for sparse coding. In COLT",
      "citeRegEx" : "AGMM15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "with implications for gaussian mixtures and autoencoders",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra", "Sushant Sachdeva. Provable ica with unknown gaussian noise" ],
      "venue" : "NIPS, pages 2375–2383,",
      "citeRegEx" : "AGMS12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning latent bayesian networks and topic models under expansion constraints",
      "author" : [ "A. Anandkumar", "D. Hsu", "A. Javanmard", "S. Kakade" ],
      "venue" : "ICML",
      "citeRegEx" : "AHJK13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation",
      "author" : [ "A. Anandkumar", "S. Kakade", "D. Foster", "Y. Liu", "D. Hsu" ],
      "venue" : "Technical report",
      "citeRegEx" : "AKF12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "In NIPS",
      "author" : [ "Pranjal Awasthi", "Andrej Risteski. On some provably correct cases of variational inference for topic models" ],
      "venue" : "pages 2089–2097,",
      "citeRegEx" : "AR15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A provable svd-based algorithm for learning topics in dominant admixture corpus",
      "author" : [ "T. Bansal", "C. Bhattacharyya", "R. Kannan" ],
      "venue" : "NIPS",
      "citeRegEx" : "BBK14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Nonnegative matrix factorization under heavy noise",
      "author" : [ "Chiranjib Bhattacharyya", "Navin Goyal", "Ravindran Kannan", "Jagdeep Pani" ],
      "venue" : "Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "BGKP16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Probabilistic topic models",
      "author" : [ "David M Blei" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Ble12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "JMLR",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation" ],
      "venue" : "3:993–1022,",
      "citeRegEx" : "BNJ03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "a new concept? Signal processing",
      "author" : [ "Pierre Comon. Independent component analysis" ],
      "venue" : "36(3):287–314,",
      "citeRegEx" : "Com94",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Topic discovery through data dependent and random projections",
      "author" : [ "W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama" ],
      "venue" : "arXiv preprint arXiv:1303.3664",
      "citeRegEx" : "DRIS13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient distributed topic modeling with provable guarantees",
      "author" : [ "W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama" ],
      "venue" : "AISTAT, pages 167–175",
      "citeRegEx" : "DRIS14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "page 359",
      "author" : [ "Alan Frieze", "Mark Jerrum", "Ravi Kannan. Learning linear transformations. In focs" ],
      "venue" : "IEEE,",
      "citeRegEx" : "FJK96",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization",
      "author" : [ "Tao Hu", "Cengiz Pehlevan", "Dmitri B Chklovskii" ],
      "venue" : "Asilomar Conference on Signals, Systems and Computers, pages 613–619. IEEE,",
      "citeRegEx" : "HPC14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "NIPS",
      "author" : [ "Daniel D Lee", "H Sebastian Seung. Unsupervised learning by convex", "conic coding" ],
      "venue" : "pages 515–521,",
      "citeRegEx" : "LS97",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Nature",
      "author" : [ "Daniel D Lee", "H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization" ],
      "venue" : "401(6755):788–791,",
      "citeRegEx" : "LS99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "In NIPS",
      "author" : [ "Daniel D Lee", "H Sebastian Seung. Algorithms for non-negative matrix factorization" ],
      "venue" : "pages 556–562,",
      "citeRegEx" : "LS01",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research",
      "author" : [ "Bruno A Olshausen", "David J Field" ],
      "venue" : "37(23):3311–3325,",
      "citeRegEx" : "OF97",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A hebbian/anti-hebbian network derived from online non-negative matrix factorization can cluster and discover sparse features",
      "author" : [ "Cengiz Pehlevan", "Dmitri B Chklovskii" ],
      "venue" : "Asilomar Conference on Signals, Systems and Computers, pages 769–775. IEEE,",
      "citeRegEx" : "PC14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "In NIPS",
      "author" : [ "Cengiz Pehlevan", "Dmitri Chklovskii. A normative theory of adaptive dimensionality reduction in neural networks" ],
      "venue" : "pages 2260–2268,",
      "citeRegEx" : "PC15a",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Optimization theory of hebbian/antihebbian networks for pca and whitening",
      "author" : [ "Cengiz Pehlevan", "Dmitri B Chklovskii" ],
      "venue" : "arXiv preprint arXiv:1511.09468,",
      "citeRegEx" : "PC15b",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sparse topical coding",
      "author" : [ "Jun Zhu", "Eric P Xing" ],
      "venue" : "arXiv preprint arXiv:1202.3778,",
      "citeRegEx" : "ZX12",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : ", [AGKM12]) or tensors (e.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 6,
      "context" : "[AKF12]), which are still not as widely used in practice primarily because of computational feasibility issues or sensitivity to assumptions on A and X.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "Some others depend on specific structure of the feature matrix, such as separability [AGKM12] or similar properties [BGKP16].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "Some others depend on specific structure of the feature matrix, such as separability [AGKM12] or similar properties [BGKP16].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "In the first category, they make heavy structural assumptions on the feature matrix A∗ such as separability ([AGM12]) or allowing running time exponential in n ( [AGKM12]).",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "In the first category, they make heavy structural assumptions on the feature matrix A∗ such as separability ([AGM12]) or allowing running time exponential in n ( [AGKM12]).",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "In the second one, they impose strict distributional assumptions on x∗ ([AKF12]), where the methods are usually based on the method of moments and tensor decompositions and have poor tolerance to noise, which is very important in practice.",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : ", [AGMM15]) requires that the decodings are correct in all the intermediate iterations, in the sense that the supports of x∗ are recovered with no error.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "The area of non-negative matrix factorization (NMF) has a rich empirical history, starting with the practical algorithm of [LS97].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "On the theoretical side, [AGKM12] provides a fixed-parameter tractable algorithm for NMF, which solves algebraic equations and thus has poor noise tolerance.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "[AGKM12] also studies NMF under separability assumptions about the features.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "[BGKP16] studies NMF under heavy noise, but also needs assumptions related to separability, such as the existence of dominant features.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "Usually, ‖x∗‖1 = 1 while there also exist work that assume xi ∈ [0, 1] and are independent [ZX12].",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "For example, [AGH13] assume the topic-word matrix contains “anchor words”: words which appear in a single topic.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "Most related is the work of [AR15] who analyze a version of the variational inference updates when documents are long.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "Results here typically are not robust to noise, with the exception of [AGMS12] that tolerates Gaussian noise.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 20,
      "context" : "The original empirical work proposing such an algorithm (in fact, it suggested that the V1 layer processes visual signals in the same manner) was due to [OF97].",
      "startOffset" : 153,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "A theoretical analysis came latter for dictionary learning due to [AGMM15] under the assumption that the columns of A∗ are incoherent.",
      "startOffset" : 66,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "The problem in the worst case is NP-hard [AGKM12], so some assumptions are needed to design provable efficient algorithms.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "The condition on l means that a constant warm start is sufficient for our algorithm to converge, which is much better than previous work such as [AR15]: indeed, there l depends on the dynamic range of the entries of A∗ which is problematic in practice.",
      "startOffset" : 145,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "References [AGH13] S.",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "[AGKM12] Sanjeev Arora, Rong Ge, Ravindran Kannan, and Ankur Moitra.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 2,
      "context" : "[AGM12] S.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "[AGMM15] S.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : "[AGMS12] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "[AHJK13] A.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "[AKF12] A.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "[AR15] Pranjal Awasthi and Andrej Risteski.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "[BBK14] T.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "[BGKP16] Chiranjib Bhattacharyya, Navin Goyal, Ravindran Kannan, and Jagdeep Pani.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "[Ble12] David M Blei.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "[BNJ03] David M Blei, Andrew Y Ng, and Michael I Jordan.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : "[Com94] Pierre Comon.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "[DRIS13] W.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 14,
      "context" : "[DRIS14] W.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 15,
      "context" : "[FJK96] Alan Frieze, Mark Jerrum, and Ravi Kannan.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "[HPC14] Tao Hu, Cengiz Pehlevan, and Dmitri B Chklovskii.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "[LS97] Daniel D Lee and H Sebastian Seung.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 18,
      "context" : "[LS99] Daniel D Lee and H Sebastian Seung.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : "[LS01] Daniel D Lee and H Sebastian Seung.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 20,
      "context" : "[OF97] Bruno A Olshausen and David J Field.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 21,
      "context" : "[PC14] Cengiz Pehlevan and Dmitri B Chklovskii.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 22,
      "context" : "[PC15a] Cengiz Pehlevan and Dmitri Chklovskii.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 23,
      "context" : "[PC15b] Cengiz Pehlevan and Dmitri B Chklovskii.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "[ZX12] Jun Zhu and Eric P Xing.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 18,
      "context" : "The area of non-negative matrix factorization (henceforth NMF) has a rich empirical history, starting with the work of [LS99].",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "On the theoretical side, [AGKM12] provide a fixed-parameter tractable algorithm for NMF: namely when if the matrix A ∈ Rm×nand X ∈ Rn×N , they provide an algorithm that runs in time (mN).",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "[BGKP16] studies the problem under heavy noise setting, but also needs assumptions related to separability, such as the existence of dominant features.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : ", [ZX12]), which is closely related to our model.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "However, there also exist work on topic modeling [ZX12] that do not restrict x∗ is assumed to have norm 1 and can satisfying our assumption.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "There is a rich body of empirical work on topic models, starting from the seminal work on LDA due to [BNJ03].",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "Typically in empirical papers the matrices A∗, as well as the vectors x∗ are learned using variational inference, which can be interpreted as a kind of alternating minimization in KL divergence norm, and in the limit of infinite-length documents converges to the [LS99] updates ([AR15]).",
      "startOffset" : 263,
      "endOffset" : 269
    }, {
      "referenceID" : 7,
      "context" : "Typically in empirical papers the matrices A∗, as well as the vectors x∗ are learned using variational inference, which can be interpreted as a kind of alternating minimization in KL divergence norm, and in the limit of infinite-length documents converges to the [LS99] updates ([AR15]).",
      "startOffset" : 279,
      "endOffset" : 285
    }, {
      "referenceID" : 2,
      "context" : "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "For example, [AGM12] and [AGH13] assume that the topic-word matrix contains “anchor words”.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "For example, [AGM12] and [AGH13] assume that the topic-word matrix contains “anchor words”.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "[AHJK13] on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "Finally, in the paper [AR15] a version of the standard variational inference updates is analyzed in the limit of infinite length documents.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "However, [AR15] requires quite strong assumptions on both the warm start, and the amount of “non-overlapping” of the topics in the topic-word matrix.",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "The classic papers [Com94] and [FJK96] solved this problem in the noiseless case, with an approach based on cumulants, and [AGMS12] solved it in another special case, when the noise η is Gaussian (albeit with an unknown covariance matrix).",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "The classic papers [Com94] and [FJK96] solved this problem in the noiseless case, with an approach based on cumulants, and [AGMS12] solved it in another special case, when the noise η is Gaussian (albeit with an unknown covariance matrix).",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "The classic papers [Com94] and [FJK96] solved this problem in the noiseless case, with an approach based on cumulants, and [AGMS12] solved it in another special case, when the noise η is Gaussian (albeit with an unknown covariance matrix).",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "The original empirical work which proposed an algorithm like this (in fact, it suggested that the V1 layer processes visual signals in the same manner) was due to [OF97].",
      "startOffset" : 163,
      "endOffset" : 169
    }, {
      "referenceID" : 3,
      "context" : "([PC15a, PC15b, HPC14, PC14]) A theoretical analysis of it came latter due to [AGMM15].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "However, the assumption made in [AGMM15] is that the columns of A∗ are incoherent, which means that they have l2 norm bounded by 1, and inner products bounded by O( 1 √ m ).",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "Additionally, the incoherence assumptions on the matrix A∗ used in [AGMM15], in the case when A∗ needs to be non-negative and has l1 column-wise norm would effectively imply that the columns of A∗ have very small overlap.",
      "startOffset" : 67,
      "endOffset" : 75
    } ],
    "year" : 2016,
    "abstractText" : "Non-negative matrix factorization is a popular tool for decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the nonnegativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest.",
    "creator" : "LaTeX with hyperref package"
  }
}