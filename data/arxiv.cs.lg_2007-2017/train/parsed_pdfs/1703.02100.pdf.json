{
  "name" : "1703.02100.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Guarantees for Greedy Maximization of Non-submodular Functions with Applications",
    "authors" : [ "Andrew An Bian", "Joachim M. Buhmann", "Andreas Krause", "Sebastian Tschiatschek" ],
    "emails" : [ "mann@inf.ethz.ch>,", "<krausea@ethz.ch>." ],
    "sections" : [ {
      "heading" : null,
      "text" : "−γα) for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian Aoptimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and real-world applications."
    }, {
      "heading" : "1. Introduction",
      "text" : "Many important problems, such as experimental design and sparse modeling, are naturally formulated as a subset selection problem, where a set function F (S) over a Kcardinality constraint is maximized, i.e.,\nmax S⊆V,|S|≤K F (S), (P)\n1Department of Computer Science, ETH Zurich, Zurich, Switzerland. Correspondence to: Joachim M. Buhmann <jbuhmann@inf.ethz.ch>, Andreas Krause <krausea@ethz.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nwhere V = {v1, . . . , vn} is the ground set. Specifically, in experimental design, the goal is to select a set of experiments to perform such that some statistical criterion is optimized. This problem arises naturally in domains where performing experiments is costly. In sparse modeling, the task is to identify sparse representations of signals, enabling interpretability and robustness in high-dimensional statistical problems—properties that are crucial in modern data analysis.\nFrequently, the standard GREEDY algorithm (Alg. 1) is used to (approximately) solve (P). For the case that F (S)\nAlgorithm 1: The GREEDY Algorithm Input: Ground set V , set function F : 2V→R+, budget K S0 ← ∅ for t = 1, . . . ,K do\nv∗ ← arg maxv∈V\\St−1 F (St−1 ∪ {v})− F (St−1) St ← St−1 ∪ {v∗}"
    }, {
      "heading" : "Output: SK",
      "text" : "is a monotone nondecreasing submodular set function1, the GREEDY algorithm enjoys the multiplicative approximation guarantee of (1 − 1/e) (Nemhauser et al., 1978; Vondrák, 2008; Krause & Golovin, 2014). This constant factor can be improved by refining the characterization of the objective using the curvature (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer et al., 2013), which informally quantifies how close a submodular function is to being modular (i.e., F (S) and −F (S) are submodular).\nHowever, for many applications, including experimental design and sparse Gaussian processes (Lawrence et al., 2003), F (S) is in general not submodular (Krause et al., 2008) and the above guarantee does not hold. In practice, however, the standard GREEDY algorithm often achieves very good performance on these applications, e.g., in subset selection with the R2 (squared multiple correlation) ob-\n1F (·) is monotone nondecreasing if ∀A ⊆ V, v ∈ V , F (A ∪ {v}) ≥ F (A). F (·) is submodular iff it satisfies the diminishing returns property F (A ∪ {v}) − F (A) ≥ F (B ∪ {v}) − F (B) for allA ⊆ B ⊆ V \\{v}. Assume wlog. that F (·) is normalized, i.e., F (∅) = 0.\nar X\niv :1\n70 3.\n02 10\n0v 3\n[ cs\n.D M\n] 1\n3 Ju\nn 20\n17\njective (Das & Kempe, 2011). To explain the good empirical performance, Das & Kempe (2011) proposed the submodularity ratio, a quantity characterizing how close a set function is to being submodular.\nAnother important class of non-submodular set functions comes as the auxiliary function when optimizing a continuous function f(x) s.t. combinatorial constraints, i.e., minx∈C,supp(x)∈I f(x), where supp(x) := {i | xi 6= 0} is the support set of x, C is a convex set, and I is the independent sets of the combinatorial structure. One of the most popular ways to solve this problem is to use the GREEDY algorithm to maximize the auxiliary function F (S) := maxx∈C,supp(x)⊆S −f(x). This setting covers various important applications, to name a few, feature selection (Guyon & Elisseeff, 2003), sparse approximation (Das & Kempe, 2008; Krause & Cevher, 2010), sparse recovery (Candes et al., 2006), sparse M-estimation (Jain et al., 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler et al., 2016). Recently, Elenberg et al. (2016) proved that if f(x) has L-restricted smoothness and m-restricted strong convexity, then the submodularity ratio of F (S) is lower bounded by m/L. This result significantly enlarges the domain where the GREEDY algorithm can be applied.\nIn this paper, we combine and generalize the ideas of curvature and submodularity ratio to derive improved constant factor approximation guarantees of the GREEDY algorithm. Our guarantees allow us to better characterize the empirical success of applying GREEDY on a significantly larger class of non-submodular functions. Furthermore, we bound these characteristics for important applications, rendering the usage of GREEDY a principled choice rather than a mere heuristic. Our main contributions are:\n- We prove the first tight constant-factor approximation guarantees for GREEDY on maximizing nonsubmodular nondecreasing set functions s.t. a cardinality constraint, characterized by a novel combination of the (generalized) notions of submodularity ratio γ and curvature α.\n- By theoretically bounding parameters (γ, α) for several important objectives, including Bayesian A-optimality in experimental design, the determinantal function of a square submatrix and maximization of LPs with combinatorial constraints, our theory implies the first guarantees for them.\n- Lastly, we experimentally validate our theory on several real-world applications. It is worth noting that for the Bayesian A-optimality objective, GREEDY generates comparable solutions as the classically used semidefinite programming (SDP) based method, but is usually two orders of magnitude faster.\nNotation. We use boldface letters, e.g., x, to represent vectors, and capital boldface letters, e.g., A, to denote matrices. xi is the ith entry of the vector x. We refer to V = {v1, ..., vn} as the ground set. We use f(·) to denote a continuous function, and F (·) to represent a set function. supp(x) := {i ∈ V | xi 6= 0} is the support set of the vector x, and [n] := {1, ..., n} for an integer n ≥ 1. We denote the marginal gain of a set Ω ⊆ V in context of a set S ⊆ V as ρΩ(S) := F (Ω ∪ S)− F (S). For v ∈ V , we use the shorthand ρv(S) for ρ{v}(S)."
    }, {
      "heading" : "2. Submodularity Ratio and Curvature",
      "text" : "In this section we provide the submodularity ratio and curvature for general, not necessarily submodular functions2, they are natural extensions of the classical ones. Let S0 = ∅, St = {j1, ..., jt}, t = 1, ...,K be the successive sets chosen by GREEDY. For brevity, let ρt := ρjt(S\nt−1) be the marginal gain of GREEDY in step t.\nDefinition 1 (Submodularity ratio (Das & Kempe, 2011)). The submodularity ratio of a non-negative set function F (·) is the largest scalar γ s.t.∑\nω∈Ω\\S ρω(S) ≥ γρΩ(S),∀ Ω, S ⊆ V.\nThe greedy submodularity ratio is the largest scalar γG s.t.∑ ω∈Ω\\St ρω(S t) ≥ γGρΩ(St),∀|Ω|=K, t = 0, . . . ,K − 1.\nIt is easy to see that γG ≥ γ. The submodularity ratio measures to what extent F (·) has submodular properties. We make the following observations:\nRemark 1. For a nondecreasing function F (·), it holds a) γ, γG ∈ [0, 1]; b) F (·) is submodular iff γ = 1. Definition 2 (Generalized curvature). The curvature of a non-negative function F (·) is the smallest scalar α s.t.\nρi(S \\ {i} ∪ Ω) ≥ (1− α)ρi(S \\ {i}), ∀ Ω, S ⊆ V, i ∈ S\\Ω.\nThe greedy curvature is the smallest scalar αG ≥ 0 s.t.\nρji(S i−1 ∪ Ω) ≥ (1− αG)ρji(Si−1),\n∀ Ω : |Ω| = K, i : ji ∈ SK−1\\Ω. 2Curvature is commonly defined for submodular functions. Sviridenko et al. (2013) presented a notion of curvature for monotone non-submodular functions. We show in Appendix C the details of these notions and the relations to ours. Additionally, we prove in Remark 3 of Appendix C.2 that our combination of curvature and submodularity ratio is more expressive than that of Sviridenko et al. (2013) in characterizing the maximization of problem (P) using standard GREEDY.\nWhen K = n or 1, SK−1\\Ω = ∅, it is natural to define αG = 0. It is easy to observe that αG ≤ α. Note that the classical total curvature is αtotal := 1−mini∈V ρi(V\\{i})ρi(∅) . Remark 2. For a nondecreasing function F (·), it holds: a) α, αG ∈ [0, 1]; b) F (·) is supermodular iff α = 0; c) If F (·) is submodular, then αG ≤ α = αtotal.\nSo for a submodular function, our notion of curvature is consistent with αtotal. Notably, αG usually characterizes the problem better than αtotal, as will be validated in Section 5."
    }, {
      "heading" : "3. Approximation Guarantee",
      "text" : "We present approximation guarantee of GREEDY in Theorem 1. Note that both versions of the submodularity ratio and curvature apply in the proof. For brevity, we use γ and α to refer to any of these versions in the sequel. In Section 3.3 we prove tightness of the approximation guarantees. All omitted proofs are given in Appendix B. Theorem 1. Let F (·) be a non-negative nondecreasing set function with submodularity ratio γ ∈ [0, 1] and curvature α ∈ [0, 1]. The GREEDY algorithm enjoys the following approximation guarantee for solving problem (P):\nF (SK) ≥ 1 α\n[ 1− ( K − αγ K )K] F (Ω∗)\n≥ 1 α (1− e−αγ)F (Ω∗), (1)\nwhere Ω∗ is the optimal solution of (P) and SK the output of the GREEDY algorithm.3"
    }, {
      "heading" : "3.1. Interpreting Theorem 1",
      "text" : "Before proving the theorem, we want to give the reader an intuition of the results and show how our results recover and extend several classical guarantees for the GREEDY algorithm. For the case α = 0 (i.e., F (·) is supermodular), the approximation guarantee is lim\nα→0 1 α (1 − e −αγ) = γ, which gives the first guarantee of greedily maximizing a nondecreasing supermodular function with bounded γ. When γ = 1, (i.e., F (·) is submodular), we recover the guarantee of α−1(1−e−α) (Conforti & Cornuéjols, 1984). For the case α = 1, we have a guarantee of (1 − e−γ) (Das & Kempe, 2011). For the case α = 1, γ = 1, we recover the classical guarantee of (1 − 1/e) (Nemhauser et al., 1978). We plot the constant-factor approximation guarantees for different values of γ and α in Fig. 1. One interesting phenomenon is that γ and α play different roles: Looking at γ = 0, the approximation factor is always 0, independent of the value α takes. In contrast, for α = 0, the\n3For the setting that GREEDY is allowed to pick more than K elements, e.g., pick K′ > K elements, our theory can be easily extended to show that F (SK ′ ) ≥ α−1(1− e−αγK ′/K)F (Ω∗).\napproximation guarantee is (1 − e−γ). This can be interpreted as the curvature boosting the guarantees."
    }, {
      "heading" : "3.2. Proof of Theorem 1",
      "text" : "The high-level proof framework is based on Conforti & Cornuéjols (1984) (where they derive the approximation guarantee for maximizing a nondecreasing submodular function with bounded curvature). However, adapting the proof to non-submodular functions requires several changes detailed in Section 6.\nProof overview. Let us denote all problem instances of maximizing a non-negative nondecreasing function F (·) s.t. K-cardinality constraint (max|S|≤K F (S)) to be PK,α,γ , where F (·) is parametrized by submodularity ratio γ and curvature α. Let PΩ∗,SK ∈ PK,α,γ denote those problem instances with optimal solution Ω∗ and greedy solution SK . We group all problem instances PK,α,γ according to the set Ω∗ ∩ SK := {l1 = jm1 , l2 = jm2 , . . . , ls = jms}, where jm1 , . . . , jms are consistent with the order of greedy selection. Let us denote the problem instances with Ω∗∩SK = {l1, . . . , ls} as the group PK,α,γ({l1, . . . , ls}).\nThe main idea of the proof is to investigate the worst-case approximation ratio of each group of the problem instances PK,α,γ({l1, . . . , ls}),∀{l1, . . . , ls} ⊆ SK . We do this by constructing LPs based on the properties of the problem instances. By studying the structures of these LPs, we will prove that the worst-case approximation ratio of all problem instances occurs when Ω∗ ∩SK = ∅. Thus the desired approximation guarantee corresponds to the worst-case approximation ratio of PK,α,γ(∅).\nThe proof. When γ = 0 or F (Ω∗) = 0, (1) holds naturally. In the following, let γ ∈ (0, 1] and F (Ω∗) > 0. First, we present Lemma 1, which will be used to construct the LPs.\nLemma 1. For any Ω ⊆ V with |Ω| = K and any t ∈ {0, . . . ,K − 1}, let wt := |St ∩ Ω|. It holds that\nα ∑\ni:ji∈St\\Ω\nρi + ∑\ni:ji∈St∩Ω\nρi + γ −1(K − wt)ρt+1 ≥ F (Ω).\nWe now specify the constructing of the LPs: For any problem instance PΩ∗,SK ∈ PK,α,γ({l1, . . . , ls}), we know that F (SK) = ∑K i=1 ρi (telescoping sum). Hence, the approximation ratio is F (S K) F (Ω∗) = ∑ i ρi F (Ω∗) , which we denote\nas R({l1, . . . , ls}) = ∑ i ρi F (Ω∗) . Define xi := ρi F (Ω∗) , i ∈ [K]. Since F is nondecreasing, xi ≥ 0. Plugging Ω = Ω∗ into Lemma 1, and considering t = 0, . . . ,K − 1, we have in total K constraints over the variables xi, which constitute the constraints of the LP. So the worst-case approximation ratio of the group PK,α,γ({l1, . . . , ls}) is:\nR({l1, . . . , ls}) = min ∑K\ni=1 xi, s.t. xi ≥ 0 and,\nrow (0) row (1)\n... row (l1 − 1) row (l2 − 1) row (q = lr) ... row (ls − 1) ... row (K − 1)  K γ α K γ ... ... . . . α α · · · K γ 0 α α · · · 1 K−1 γ α α · · · 1 1 K−r γ ... ... ... ... ... . . . α α · · · 1 1 α · · · K−s+1 γ ... ... ... ... ... ...\n. . . α α · · · 1 1 α · · · 1 · · · K−s\nγ\n ·  x1 x2 ... xl1 xl2 xq+1 ... xls\n... xK\n ≥  1 1 ... 1 1 1 ... 1 ... 1  (2)\nThe following Lemma presents the key structure of the constructed LPs, which will be used to deduce the relation between the LPs of different problem instance groups.\nLemma 2. Assume that the optimal solution of the constructed LP is x∗ ∈ RK+ and that s = |Ω∗ ∩ SK | ≥ 1. For all 1 ≤ r ≤ s it holds that x∗q ≤ x∗q+1, where q = lr.\nProof sketch of Lemma 2. Assume by virture of creating a contradiction that x∗q > x ∗ q+1. We can always create a new feasible solution y∗ ∈ RK+ by decreasing x∗q by some > 0, while increasing all the x∗q+1 to x ∗ K by some proper values, s.t. y∗ has smaller LP objective value. Specifically, we define y∗ as: for k = 1, . . . , q − 1, y∗k := x∗k; y∗q := x ∗ q − ; for k = q + 1, . . . ,K, y∗k := x∗k + k where\nks are defined recursively as: q+1 = γK−r , and\nq+1+u = q+u K − r − u+ 1− γ\nK − r − u , 1 ≤ u ≤ K − q − 1.\nClaim 1. a) The new solution y∗ ≥ 0; b) All of the constraints in (2) are still feasible for y∗.\nAfter that the change of the LP objective is,\n∆LP = − + q+1 + q+2 + . . .+ K .\nOne can prove that the LP objective decreases:\nClaim 2. For all K ≥ 1, 1 ≤ r ≤ q < K, it holds that ∆LP ≤ 0,∀γ ∈ (0, 1]. Equality is achieved when r = q and γ = 1. Therefore we reach the contradiction that x∗ is an optimal solution of the constructed LP.\nGiven Lemma 2, we prove in the following Lemma, which states that the worst-case approximation ratio of all problem instances occurs when Ω∗ ∩ SK = ∅. Lemma 3. For all {l1, . . . , ls} ⊆ SK , it holds that\nR({l1, . . . , ls}) ≥ R(∅) = 1α\n[ 1− ( K−αγ K )K] .\nSo the greedy solution has objective F (SK) ≥ 1 α [ 1− ( K−αγ K )K] F (Ω∗) ≥ 1α (1 − e −αγ)F (Ω∗).\n3.3. Tightness Result\nWe demonstrate that the approximation guarantee in Theorem 1 is tight, i.e., for every submodularity ratio γ and every curvature α, there exist set functions that achieve the bound exactly.\nAssume the ground set V contains the elements in S :=\n{j1, . . . , jK} and the elements in Ω := {ω1, . . . , ωK} (S ∩ Ω = ∅) and n− 2K dummy elements. The objective function we are going to construct will not depend on these dummy elements, i.e., the objective value of a set does not change if dummy elements are removed from or added to that set. Consequently, the dummy elements will not affect the submodularity ratio and the curvature. For the constants α ∈ [0, 1], γ ∈ (0, 1], we define the objective function as,\nF (T ) := f(|Ω ∩ T |)\nK\n( 1− αγ ∑ i:ji∈S∩T ξi ) + ∑ i:ji∈S∩T ξi, (3)\nwhere ξi := 1K ( K−γα K )i−1 , i ∈ [K]; f(x) = γ −1−1 K−1 x 2 + K−γ−1 K−1 x. Note that f(x) is convex nondecreasing over [0,K], and that f(0) = 0, f(1) = 1, f(K) = K/γ. It is clear that F (∅) = 0 and F (·) is monotone nondecreasing. The following lemma shows that it is generally nonsubmodular and non-supermodular.\nLemma 4. For the objective in (3): a) When α = 0, it is supermodular; b) When γ = 1, it is submodular; c) F (T ) has submodularity ratio γ and curvature α.\nConsidering the problem of max|T |≤K F (T ), we claim that the GREEDY algorithm may output S. This can be proved by induction. One can see that ρj1(∅) = ξ1 = ρω1(∅), so GREEDY can choose j1 in the first step. Assume in step t − 1 GREEDY has chosen St−1 = {j1, . . . , jt−1}, one can verify that the marginal gains coincide, i.e., ρjt(S t−1) = ξt = ρωt(S t−1). However, the optimal solution is actually Ω with function value as F (Ω) = 1γ . So the\napproximation ratio is F (S)F (Ω) = 1 α\n[ 1− ( K−αγ K )K] , which\nmatches our approximation guarantee in Theorem 1."
    }, {
      "heading" : "4. Applications",
      "text" : "We consider several important real-world applications and their corresponding objective functions. We show that the submodularity ratio and the curvature of these functions can be bounded and, hence, the approximation guarantees from our theoretical results are applicable. All the omitted proofs are provided in Appendix D."
    }, {
      "heading" : "4.1. Bayesian A-optimality in Experimental Design",
      "text" : "In Bayesian experimental design (Chaloner & Verdinelli, 1995), the goal is to select a set of experiments to perform s.t. some statistical criterion is optimized, e.g., the variance of certain parameter estimates is minimized. Krause et al. (2008) investigated several criteria for this purpose, amongst others the Bayesian A-optimality criterion. This criterion is used to maximally reduce the variance in the posterior distribution over the parameters. In general, the criterion is not submodular as shown in Krause et al. (2008, Section 8.4).\nFormally, assume there are n experimental stimuli {x1, . . . ,xn}, each xi ∈ Rd, which constitute the data matrix X ∈ Rd×n. Let us arrange a set S ⊆ V of stimuli as a matrix XS := [xv1 , . . . ,xvs ] ∈ Rd×|S|. Let θ ∈ Rd be the parameter vector in the linear model yS = X>S θ +w, wherew is the Gaussian noise with zero mean and variance σ2, i.e.,w ∼ N (0, σ2I), and yS is the vector of dependent variables. Suppose the prior takes the form of an isotropic Gaussian, i.e., θ ∼ N (0,Λ−1),Λ = β2I. Then,[ yS θ ] ∼ N (0,Σ),Σ = [ σ2I + X>SΛ −1XS X > SΛ −1 Λ−1XS Λ −1 ] .\nThis implies that Σθ|yS = (Λ + σ −2XSX > S ) −1. The Aoptimality objective is defined as,\nFA(S) := tr(Σθ)− tr(Σθ|yS ) (4) = tr(Λ−1)− tr((Λ + σ−2XSX>S )−1).\nThe following Proposition gives bounds on the submodularity ratio and curvature of (4).\nProposition 1. Assume normalized stimuli, i.e., ‖xi‖ = 1,∀i ∈ V . Let the spectral norm of X be ‖X‖.4 Then, a) The objective in (4) is monotone nondecreasing. b) Its submodularity ratio γ can be lower bounded by β 2\n‖X‖2(β2+σ−2‖X‖2) , and its curvature α can be upper\nbounded by 1− β 2\n‖X‖2(β2+σ−2‖X‖2) ."
    }, {
      "heading" : "4.2. The Determinantal Function",
      "text" : "The determinantal function of a square submatrix is widely used in many areas, e.g., in determinantal point processes (Kulesza & Taskar, 2012) and active set selection for sparse Gaussian processes. Monotone nondecreasing determinantal functions appear in the second problem. Assume Σ is the covariance matrix parameterized by a positive definite kernel. In the Informative Vector Machine (Lawrence et al., 2003), the information gain of a subset of points S ⊆ V is 1 2 logF (S), where\nF (S) := det(I + σ−2ΣS), (5)\nwhere σ is the noise variance in the Gaussian process model, ΣS is the square submatrix with both its rows and columns indexed by S. Although logF (S) is submodular, F (S) is in general not submodular. The approximation guarantee of GREEDY for maximizing logF (S) does not translate to a guarantee for maximizing F (S). The following Proposition characterizes (5).\nProposition 2. a) F (S) in (5) is supermodular, its curvature is 0; b) Let the eigenvalues of A := I + σ−2Σ be λ1 ≥ · · · ≥ λn > 1. The greedy submodularity ratio of F (S) can be lower bounded by K(λn−1)\n( ∏K j=1 λj)−1 ."
    }, {
      "heading" : "4.3. LPs with Combinatorial Constraints",
      "text" : "LPs with combinatorial constraints appear frequently in practice. Consider the following example: Suppose that V is the set of all products a company can produce. Given budget constraints on the raw materials needed, companies consider the LP maxx∈P〈d,x〉, where d is the vector of profits for the individual products and where P is a polytope representing the continuous constraints. The above LP can be used to assess the profit maximizing production plan. Usually the company needs to consider combinatorial constraints as well. For instance, the company has at most K production lines, thus they have to select a subset of K products to produce. Often this kind of problems can be formalized as maxx∈P,supp(x)∈I〈d,x〉, where I is the independent set of the combinatorial structure. Hence, a natural auxiliary set function is,\nF (S) := maxsupp(x)⊆S, x∈P〈d,x〉, ∀S ⊆ V. (6) 4By Weyl’s inequality, a naive upper bound is ‖X‖ ≤ √ n.\nLet P = {x ∈ Rn | 0 ≤ x ≤ ū,Ax ≤ b, ū ∈ Rn+,A ∈ Rm×n+ , b ∈ Rm+}. In general F (S) in (6) is non-submodular as illustrated by two examples in Appendix D.3. Upper bounding the curvature is equivalent to lower bounding F (S∪Ω)−F (S\\{i}∪Ω)F (S)−F (S\\{i}) , which can be 0 in the worst case. However, the submodularity ratio can be lower bounded by a non-zero scalar.\nProposition 3. a) F (S) in (6) is a normalized nondecreasing set function. b) With regular non-degenerancy assumptions (details in Appendix D.3.2), its submodularity ratio can be lower bounded by γ0 > 0."
    }, {
      "heading" : "4.4. More Applications",
      "text" : "Many real-world applications can benefit from the theory in this work, for instance: subset selection using the R2 objective, sparse modeling and the budget allocation problem with combinatorial constraints. Details on these applications are deferred to Appendix G."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "We empirically validated approximation guarantees characterized by the submodularity ratio and the curvature for several applications. Since it is too time consuming to calculate the full versions of α and γ using exhaustive search, we only calculated the greedy versions (αG, γG). All averaged results are from 20 repeated experiments. Source code is available at https://github.com/bianan/ non-submodular-max.5 More results are put in Appendix H."
    }, {
      "heading" : "5.1. Bayesian Experimental Design",
      "text" : "We considered the Bayesian A-optimality objective for both synthetic and real-world data. In all experiments, we normalized the data points to have unit `2-norm.\nReal-world results: We used the Boston Housing Data. 5All experiments were implemented using Matlab. We used the SDP solver provided by CVX (Version 2.1).\nThe dataset6 has 14 features (e.g., crime rate, property tax rates, etc.) and 516 samples. To be able to quickly calculate the parameters and optimal solution by exhaustive search, the first n = 14 samples were used. As a baseline, we used an SDP-based algorithm (abbreviated as SDP, details are available in Appendix E). Results are shown in Fig. 2 for varying values of K. In Fig. 2a we can observe that both GREEDY and SDP compute near-optimal solutions. From Fig. 2b we can see that the greedy submodularity ratio γG is close to 1, and that the greedy curvature αG is less than 1, while the classical curvature αtotal is always 1 (the worstcase value). This implies that the classical total curvature αtotal characterizes the considered maximization problems less accurate than the greedy curvature.\nSynthetic results: We generated random observations from a multivariate Gaussian distribution with different correlations. To be able to assess the ground truth, we used n = 12 samples with d = 6 features. Fig. 3 shows the results with correlation 0.2 (first column) and 0.6 (second column), respectively: The first row shows the average objective values over the optimal value with error bars, and the second row shows the parameters. One can observe that GREEDY always obtains near-optimal solutions and that these solutions are roughly comparable with those obtained by the SDP. The classical curvature αtotal is always close to 1, while αG take smaller values, and γG takes values close to 1, thus characterize the performance of GREEDY better.\nMedium-scale synthetic experiments: To compare the runtime of SDP and GREEDY, we considered mediumscale datasets (we cannot report results on larger datasets because of the huge computational demands of the SDP).\n6https://archive.ics.uci.edu/ml/datasets/ Housing\nFig. 4 shows the objective value achieved by GREEDY and SDP for different numbers of features d and numbers of samples n, as well as the correlations. We can observe that GREEDY computes solutions that are on par or superior to those of SDP. In Table 1 we summarize the runtime of GREEDY and SDP for different values of d and n, for correlation 0.5. Furthermore, we show the ratio of runtimes of the two algorithms. We can observe that GREEDY is usually two orders of magnitude faster than SDP."
    }, {
      "heading" : "5.2. LPs with Combinatorial Constraints",
      "text" : "We generated synthetic LPs as follows: Firstly, we generated the matrix A ∈ Rm×n+ , Aij ∈ [0, 1] by drawing all entries independently from a uniform distribution on\n[0, 1]. We set b = d = 1, and set ū as 1. The first row of Fig. 5 plots the optimal LP objective (calculated using exhaustive search) and the LP objective returned by GREEDY. The second row shows the curvature and submodularity ratio. The first column (Fig. 5a) presents the results for n = 6,m = 20, while the second column (Fig. 5b) presents that for n = 8,m = 30. Note the greedy submodularity ratio takes values between ∼ 0.15 and 1, and that the curvature is close to the worst-case value of 1. These observations are consistent with the theory in Section 4.3."
    }, {
      "heading" : "5.3. Determinantal Functions Maximization",
      "text" : "We experimented with synthetic and real-world data: For synthetic data, we generated random covariance matrices Σ ∈ Rn×n with uniformly distributed eigenvalues in [0, 1]. We set n = 10, σ = 2. In Fig. 6 (left) we plot the optimal determinantal objective value and the value achieved by GREEDY. Fig. 6 (right) traces the greedy submodularity ratio γG. Since the determinantal objective is supermodular, so the approximation guarantee equals to γG. We can see that γG can reasonably predict the performance of GREEDY.\nFor real-world data, we considered an active set selection task on the CIFAR-107 dataset. The first n = 12 images in the test set were used to calculate the covariance matrix with an squared exponential kernel (k(xi,xj) = exp(−‖xi − xj‖2/h2), h was set to be 1). The results in Fig. 7 shows similar results as with the synthetic data.\n7https://www.cs.toronto.edu/˜kriz/cifar. html"
    }, {
      "heading" : "6. Related Work",
      "text" : "In this section we briefly discuss related work on various notions of non-submodularity and the optimization of nonsubmodular functions (Further details in Appendix F).\nRelation to Conforti & Cornuéjols (1984) in deriving approximation guarantees. In proving Theorem 1 we use the similar proof framework (i.e., utilizing LP formulations to analyze the worst-case approximation ratios of different groups of problem instances) as that in Conforti & Cornuéjols (1984), where they derive guarantees for maximizing submodular functions. However, since we are proving guarantees for non-submodular functions, the specific techniques on how to manipulate these LPs are different. Specifically, 1) The building block to construct LPs (Lemma 1) is different; 2) The technique to prove the structure of the LPs (which corresponds to Lemma 2) is significantly different for a submodular function and a nonsubmodular function, and Lemma 2 is the key to investigate the worst-case approximation ratios of different groups of problem instances. 3) The specific way to prove Lemma 3 is also different since the constraints of the LPs are different for submodular and non-submodular functions.\nSubmodularity ratio and curvature. Curvature is typically defined for submodular functions. Sviridenko et al. (2013) present a notion of curvature for monotone nonsubmodular functions. Appendix C provides details of that notion and relates it to our definition. Yoshida (2016) prove an improved approximation ratio for knapsack-constrained maximization of submodular functions with bounded curvature. Submodularity ratio (Das & Kempe, 2011) is a quantity characterizing how close a function is to being submodular.\nApproximate submodularity. Krause et al. (2008) define approximately submodular functions with parameter ≥ 0 as those functions F that satisfy an approximate diminishing returns property, i.e., ∀A ⊆ B ⊆ V \\ v it holds that ρv(A) ≥ ρv(B) − . GREEDY yields a solution with objective F (SK) ≥ (1 − e−1)F (Ω∗) −K , for maximizing a monotone F s.t. a K-cardinality constraint. Du et al. (2008) study the greedy maximization of nonsubmodular potential functions with restricted submodularity and shifted submodularity. Restricted submodularity refers to functions which are submodular only over some collection of subsets of V , and shifted submodularity can be viewed as a special case of the approximate diminishing returns as defined above. Recently, Horel & Singer (2016) study -approximately submodular functions, which arised from their research on “noisy” submodular functions. A function F (·) is -approximately submodular if there exists a submodular function G s.t. (1 − )G(S) ≤ F (S) ≤ (1 + )G(S), ∀S ⊆ V .\nWeak submodularity. Borodin et al. (2014) study weakly submodular functions, i.e., montone, nomalized functions F (·) s.t. for any S, T , it holds |T |F (S) + |S|F (T ) ≥ |S ∩T |F (S ∪T ) + |S ∪T |F (S ∩T ). For a function F (·), we show in Remark 4 that the following two facts do not imply each other: i) F (·) is weakly submodular; ii) The submodularity ratio of F (·) is strictly larger than 0, and its curvature is strictly smaller than 1.\nOther notions of non-submodularity. Feige & Izsak (2013) introduce the supermodular degree as a complexity measure for set functions. They show that a greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree. Zhou & Spanos (2016) use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm (Buchbinder et al., 2014) for maximizing a non-monotone function.\nOptimization of non-submodular functions. The submodular-supermodular procedure has been proposed to minimize the difference of two submodular functions (Narasimhan & Bilmes, 2005; Iyer & Bilmes, 2012). Jegelka & Bilmes (2011) present the problem of minimizing “cooperative cuts”, which are non-submodular in general, and propose efficient algorithms for optimization. Kawahara et al. (2015) analyze unconstrained minimization of the sum of a submodular function and a treestructured supermodular function. Bai et al. (2016) investigate the minimization of the ratio of two submodular functions, which can be solved with bounded approximation factor."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We analyzed the guarantees for greedy maximization of non-submodular nondecreasing set functions. By combining the (generalized) curvature α and submodularity ratio γ for generic set functions, we prove the first tight approximation bounds in terms of these definitions for greedily maximizing nondecreasing set functions. These approximation bounds significantly enlarge the domain where GREEDY has guarantees. Furthermore, we theoretically bounded the parameters α and γ for several non-trivial applications, and validate our theory in various experiments."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "The authors would like to thank Adish Singla, Kfir Y. Levy and Aurelien Lucchi for valuable discussions. This research was partially supported by ERC StG 307036 and the Max Planck ETH Center for Learning Systems. This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing."
    }, {
      "heading" : "A. Organization of the Appendix",
      "text" : "Appendix B presents the proofs for our approximation guarantees and its tightness for the GREEDY algorithm.\nAppendix C provides details on existing notions of curvature and submodularity ratio, and relates it to the notions in this paper.\nAppendix D presents detailed proofs for bounding the submodularity ratio and curvature for various applications.\nAppendix E gives details on the classical SDP formulation of the Bayesian A-optimality objective.\nAppendix F provides proofs omitted in Section 6.\nAppendix G provides information on more applications, including sparse modeling with strongly convex loss functions, subset selection using the R2 objective and optimal budget allocation with combinatorial constraints.\nAppendix H provides experimental results on subset selection with the R2 objective and additional results on experimental design."
    }, {
      "heading" : "B. Proofs for Approximation Guarantee and Tightness Result (Section 2 and Section 3)",
      "text" : ""
    }, {
      "heading" : "B.1. Proof of Remarks in Section 2",
      "text" : ""
    }, {
      "heading" : "Proofs of Remark 1.",
      "text" : "a) Because F is nondecreasing, and γ, γG are defined as the largest scalars, γ, γG ≥ 0. At the same time, both γ and γG can be at most 1 because the conditions in Def. 1 also have to hold for the case that Ω\\S (Ω\\St, respectively) is a singleton.\nb) “⇒ ”: Let Ω \\ S = {ω1, . . . , ωk}, k ≥ 1. Submodularity implies ∑k i=1 ρωi(S) ≥ ρΩ(S). Hence, γ can take the largest value 1.\n“⇐ ”:\nγ = 1 implies that (setting Ω \\ S = {ωi, ωj}), for all ωi, ωj ∈ V \\ S, it holds that F ({ωi} ∪ S) + F ({ωj} ∪ S) ≥ F ({ωi, ωj} ∪ S) + F (S), which is an equivalent way to define submodularity (Bach, 2013, Proposition 2.3)."
    }, {
      "heading" : "Proof of Remark 2.",
      "text" : "a) “If F (·) is nondecreasing, then α, αG ∈ [0, 1]”;\nWhen Ω = ∅, α is at least 0. From the definition, αG ≥ 0. Since F is nondecreasing, ρi(S \\ {i} ∪ Ω) ≥ 0 (respectively, ρji(S i−1 ∪ Ω) ≥ 0), and we defined α, αG to be the smallest scalar, it must hold that α, αG ≤ 1.\nb) “For a nondecreasing function F (·), F (·) is supermodular iff α = 0 ”;\n“⇒ ”:\nIf F is supermodular, it always holds that ρi(S \\ {i} ∪ Ω) ≥ ρi(S \\ {i}), combined with the fact that α is at least 0, we know that α must be 0.\n“⇐ ”:\nOne can observe that α = 0 is equivalent to −F (·) satisfying the diminishing returns property, which is equivalent to F (·) being supermodular.\nc) “If F (·) is nondecreasing submodular, then αG ≤ α = αtotal.”\nSince it always holds that αG ≤ α, we only need to prove that α = αtotal. Wlog., assume ρi(S \\ {i}) > 0. Then,\n1− α = min Ω,S⊆V,i∈S\\Ω ρi(S \\ {i} ∪ Ω) ρi(S \\ {i})\n= min S⊆V,i∈S ρi(V \\ {i}) ρi(S \\ {i}) (diminishing returns, and taking Ω = V \\ {i})\n= min i∈V ρi(V \\ {i}) ρi(∅) (diminishing returns, and taking S = {i}) = 1− αtotal.\nSo it holds that αG ≤ α = αtotal."
    }, {
      "heading" : "B.2. Proof of Lemma 1",
      "text" : "Proof of Lemma 1. The proof needs the definitions of generalized curvature, submodularity ratio, and the selection rule of the GREEDY algorithm.\nFirstly, observe,\nF (Ω ∪ St) = F (Ω) + ∑\ni:ji∈St ρji(Ω ∪ Si−1)\n= F (Ω) + ∑\ni:ji∈St\\Ω\nρji(Ω ∪ Si−1) + ∑\ni:ji∈St∩Ω ρji(Ω ∪ Si−1)︸ ︷︷ ︸ = 0 because ji ∈ Ω\n= F (Ω) + ∑\ni:ji∈St\\Ω\nρji(Ω ∪ Si−1). (7)\nFrom the definition of the submodularity ratio,\nF (Ω ∪ St) ≤ F (St) + 1 γ ∑ ω∈Ω\\St ρω(S t). (8)\nFrom the definition of curvature (for the greedy curvature, since it holds for SK−1, it must also hold for St ⊆ SK−1), we have, ∑\ni:ji∈St\\Ω\nρji(Ω ∪ Si−1) ≥ (1− α) ∑\ni:ji∈St\\Ω\nρji(S i−1). (9)\nCombining (7) to (9), and remember that we use the shorthand ρt := ρjt(S t−1), it reads,\nF (Ω) = F (Ω ∪ St)− ∑\ni:ji∈St\\Ω\nρji(Ω ∪ Si−1)\n≤ α ∑\ni:ji∈St\\Ω\nρi + F (S t)− ∑ i:ji∈St\\Ω ρi + 1 γ ∑ ω∈Ω\\St ρω(S t)\n= α ∑\ni:ji∈St\\Ω\nρi + ∑\ni:ji∈St∩Ω\nρi + 1\nγ ∑ ω∈Ω\\St ρω(S t)\n≤ α ∑\ni:ji∈St\\Ω\nρi + ∑\ni:ji∈St∩Ω\nρi + γ −1(K − wt)ρt+1,\nwhere the last inequality is because of the selection rule of the GREEDY algorithm (ρω(St) ≤ ρt+1,∀ω)."
    }, {
      "heading" : "B.3. Proof of Claim 1",
      "text" : "Since the proof heavily relies on the structure of the constructed LPs, we restate it here: The worst-case approximation ratio of the group PK,α,γ({l1, ..., ls}) is\nR({l1, ..., ls}) = min ∑K\ni=1 xi, s.t. xi ≥ 0, i ∈ [K] and\nrow (0) row (1)\n... row (l1 − 1) row (l2 − 1) row (q = lr) ... row (ls − 1) ... row (K − 1)  K/γ α K/γ ... ... . . . α α · · · K/γ 0 α α · · · 1 (K − 1)/γ α α · · · 1 1 K−rγ ... ... ... ... ... . . . α α · · · 1 1 α · · · K−s+1γ ... ... ... ... ... ... . . .\nα α · · · 1 1 α · · · 1 · · · K−sγ\n ·  x1 x2 ... xl1 xl2 xq+1 ... xls\n... xK\n ≥  1 1 ... 1 1 1 ... 1 ... 1  (2)\nFor notational simplicity, w.l.o.g., assume that ji = i, i ∈ [K]. Let the row index in (2) start from 0."
    }, {
      "heading" : "Proof of Claim 1.",
      "text" : "Let = (K−r+1−γ)x∗q−(K−r)x ∗ q+1\nK−r+1 ≥ K−r K−r+1 (x ∗ q − x∗q+1) > 0.\na) It is easy to see that y∗ ≥ 0 since the only decreased entry is the qth entry, and one can easily see that y∗q = x∗q − ≥ 0.\nb) “All of the constraints in (2) are still feasible for y∗.”\n(i) For the rows 0 to (q − 2) in (2), there is no change, so they are still feasible.\n(ii) For the (q − 1)th and qth rows in (2), they are\nαx∗1 + · · ·+ α(or 1)x∗q−1 + K − r + 1\nγ x∗q ≥ 1 (10)\nαx∗1 + · · ·+ α(or 1)x∗q−1 + x∗q + K − r γ x∗q+1 ≥ 1 (11)\nFor (10), after plugging y∗ into its L.H.S., we get αy∗1 + · · · + α(or 1)y∗q−1 + K−r+1γ y ∗ q , subtract from which the L.H.S. of (11), we get[ αy∗1 + · · ·+ α(or 1)y∗q−1 +\nK − r + 1 γ y∗q\n] − [ αx∗1 + · · ·+ α(or 1)x∗q−1 + x∗q +\nK − r γ x∗q+1 ] = K − r + 1\nγ (x∗q − )− x∗q − K − r γ x∗q+1\n= 0,\nso αy∗1 + · · ·+ α(or 1)y∗q−1 + K−r+1γ y ∗ q ≥ 1 and y∗ is feasible for (10).\nAfter increasing x∗q+1 by q+1 = γ K−r , the q th row in (2) is feasible since the change in its L.H.S. is − + = 0.\n(iii) For the rows q to (K − 1) in (2), let us prove by induction.\nFor the base case, consider the (q + 1)th row in (2), it can be either,\nαx∗1 + · · ·+ α(or 1)x∗q−1 + x∗q + x∗q+1 + K − r − 1\nγ x∗q+2 ≥ 1\nor\nαx∗1 + · · ·+ α(or 1)x∗q−1 + x∗q + αx∗q+1 + K − r γ x∗q+2 ≥ 1\nIt can be easily verified that the (q + 1)th row in (2) is still feasible in both the above two situations. Let us use ∆q+u to denote the change of L.H.S. of the (q + u)th row after applying the changes.\nFor the inductive step, assume that the claim holds for u = u′, i.e., the (q + u′)th row in (2) is feasible or ∆q+u′ ≥ 0. The (q + u′)th row is,\n(...same as (q + u′ + 1)th row) + K − r − v\nγ x∗q+u′+1 ≥ 1\nwhere 0 ≤ v ≤ u′ is some integer dependent on the structure of (2), but not affect the final analysis. Then the (q+u′+1)th row can be either,\n(... same as (q + u′)th row) + x∗q+u′+1 + K − r − v − 1\nγ x∗q+u′+2 ≥ 1 (case 1)\nor\n(... same as (q + u′)th row) + αx∗q+u′+1 + K − r − v\nγ x∗q+u′+2 ≥ 1 (case 2)\nIn (case 1), the L.H.S. of (q + u′ + 1)th row minus the L.H.S. of (q + u′)th row is K−r−v−1γ x ∗ q+u′+2 − K−r−v−γ γ x ∗ q+u′+1, so\n∆q+u′+1 −∆q+u′ = K − r − v − 1\nγ q+u′+2 − K − r − v − γ γ q+u′+1\n=\n[ K − r − v − 1\nγ\nK − r − u′ − γ K − r − u′ − 1 − K − r − v − γ γ\n] q+u′+1\n= [ (K − r − v − 1)K − r − u\n′ − γ K − r − u′ − 1\n− (K − r − v − γ) ] q+u′+1\nγ ≥ [ (K − r − v − 1)K − r − v − γ\nK − r − v − 1 − (K − r − v − γ)\n] q+u′+1\nγ (since 0 ≤ v ≤ u′)\n= 0.\nso the (q + u′ + 1)th row is still feasible.\nIn (case 2), the L.H.S. of (q + u′ + 1)th row minus the L.H.S. of (q + u′)th row is K−r−vγ x ∗ q+u′+2 − (K−r−vγ − α)x∗q+u′+1, so\n∆q+u′+1 −∆q+u′ = K − r − v\nγ q+u′+2 − ( K − r − v γ − α) q+u′+1\n≥ K − r − v γ ( q+u′+2 − q+u′+1) (since α ≥ 0)\n≥ 0. (since q+u′+2 ≥ q+u′+1)\nso the (q + u′ + 1)th row is feasible. Thus we finish proving Claim 1."
    }, {
      "heading" : "B.4. Proof of Claim 2",
      "text" : "Proof of Claim 2. The change of the LP objective is\n∆LP = − + q+1 + q+2 + · · ·+ K\n= [ −1 + γ\nK − r +\nγ K − r · K − r − γ K − r − 1 + · · ·+ γ K − r · K − r − γ K − r − 1 · · · K − r −m+ 2− γ K − r −m+ 1\n] ,\nwhere inside the bracket there are m = K − q items except for the “− 1”. For notational simplicity, let the sum inside the bracket to be,\nhr(γ) := −1 + γ\nK − r +\nγ K − r · K − r − γ K − r − 1 + · · ·+ γ K − r · K − r − γ K − r − 1 · · · K − r −m+ 2− γ K − r −m+ 1 . (12)\nFirst of all, since K − r ≥ K − q = m, we have that\nhr(γ) ≤ hr=q(γ) = −1 + γ m + γ m · m− γ m− 1 + · · ·+ γ m · m− γ m− 1 · · · 3− γ 2 · 2− γ 1 . (13)\nLet us merge the items in (13) from left to right one by one,\nhr=q(γ) =− 1 + γ m + γ m · m− γ m− 1 + · · ·+ γ m · m− γ m− 1 · · · 3− γ 2 · 2− γ 1\n=− m− γ m + γ m · m− γ m− 1 + · · ·+ γ m · m− γ m− 1 · · · 3− γ 2 · 2− γ 1 =− m− γ m m− 1− γ m− 1 + · · ·+ γ m · m− γ m− 1 · · · 3− γ 2 · 2− γ 1\n· · ·\n=− (m− γ)(m− γ − 1) · · · (2− γ)(1− γ) m(m− 1) · · · 2 · 1\nsetting γ to be 1 ≤ 0\nThen hr(γ) ≤ 0,∀γ ∈ (0, 1]. And it is easy to see that the equality holds if r = q and γ = 1.\nSo we have that ∆LP = hr(γ) ≤ 0, where the equality is achieved at “boundary” situation (r = q and γ = 1)."
    }, {
      "heading" : "B.5. Proof of Lemma 3",
      "text" : ""
    }, {
      "heading" : "Proof of Lemma 3.",
      "text" : "For notational simplicity, wlog., assume that ji = i, i ∈ [K].\na) Firstly let us prove that R({l1, ..., ls}) ≥ R(∅).\nThe high-level idea is to change the structure of the constraint matrix in the LP associated with {l1, ..., ls}, such that in each change, the optimal LP objective value R never increases.\nTo better explain the proof, let us state the setup first of all. Let us call the elements inside the set Ω∗ ∩ SK = {l1 = jm1 , l2 = jm2 , ..., ls = jms} the “joint elements”, which means that they are joint elements in Ω∗ and SK . Similarly, the elements outside of Ω∗∩SK are called the “disjoint” elements. For the joint elements, two elements li, lj being “adjacent” means that li + 1 = lj . Mapping to the constraint matrix in (2), it means that the corresponding columns (column (li) and column (lj)) are adjacent with each other. So we also call the corresponding columns in the constraint matrix as “joint columns”.\nWe prove part a) of Lemma 3 by two steps: In the first step, we try to make all of the joint elements inside {l1, l2, ..., ls} to be adjacent with each other; In the second step, we get rid of the joint columns in the constraint matrix from left to right, one by one. Specifically,\nStep 1. Assume that some elements inside {l1, l2, ..., ls} are not adjacent, like the example in (2), where l2 and l3 are not adjacent. Suppose that lr and lr+1 are not adjacent, which means lr + 1 < lr+1. Denote p = lr for notational simplicity. Let us use A to represent the constraint matrix in the constructed LP associated with {l1, l2, ..., lr−1, lr, lr+1, ..., ls}, let A′ represent the constraint matrix associated with {l1, l2, ..., lr−1, lr + 1, lr+1, ..., ls}. Notice that lr + 1 is a disjoint element for A, but a joint element for A′. Furthermore A and A′ only differ by columns p and p + 1 = lr + 1. Assume that x∗ ∈ RK+ is the optimal solution of the constructed LP with A as its constraint matrix. From Lemma 2, it must hold that x∗p ≤ x∗p+1. Combining with the fact that Ax∗ ≥ 1, one can easily verify that A′x∗ ≥ 1, which implies that,\nR({l1, l2, ..., lr−1, lr, lr+1, ..., ls})\n≥ R({l1, l2, ..., lr−1, lr + 1, lr+1, ..., ls}). (14)\nThe change from {l1, l2, ..., lr−1, lr, lr+1, ..., ls} to {l1, l2, ..., lr−1, lr + 1, lr+1, ..., ls} is essentially to swap the roles of one originally disjoint element lr + 1 and the originally joint element lr. Repeatedly applying this operation for all 1 ≤ r ≤ s− 1 such that lr + 1 < lr+1, we can get that,\nR({l1, l2, ..., lr−1, lr, lr+1, ..., ls}) ≥ R({ls − s+ 1, ls − s+ 2, ..., ls − 1, ls}). (15)\nNow the s joint elements inside {ls − s+ 1, ls − s+ 2, ..., ls − 1, ls} are adjacent with each other.\nStep 2. Let B be the constraint matrix associated with {ls − s + 1, ls − s + 2, ..., ls − 1, ls}, and B′ be the constraint matrix associated with {ls − s + 2, ..., ls − 1, ls}. Note that B and B′ differ in the columns from ls − s + 1 to the end. Suppose the vector x∗ is the optimal solution of the constructed LP with B as the constraint matrix. According to Lemma 2 we know that x∗ls−s+1 ≤ x ∗ ls−s+2 ≤ · · · ≤ x ∗ ls ≤ x∗ls+1. So one can easily verify that it must hold that B\n′x∗ ≥ 1, which implies\nR({ls − s+ 1, ls − s+ 2, ..., ls − 1, ls}) ≥ R({ls − s+ 2, ..., ls − 1, ls}). (16)\nApply this process repeatedly s times, one can reach that R({ls − s+ 1, ls − s+ 2, ..., ls − 1, ls}) ≥ R(∅).\nCombining step 1 and step 2, we prove part a) of Lemma 3.\nb) Then let us prove that R(∅) = 1α\n[ 1− ( K−αγ K )K] .\nThe constructed LP associated with R(∅) is,\nR(∅) = min K∑ i=1 xi\nsubject to the constraints that, xi ≥ 0,∀i = 1, ...,K and  K γ α Kγ ... ... . . . α α · · · Kγ 0 α α · · · α Kγ α α · · · α α Kγ ... ... ... ... ... . . . α α · · · α α α · · · Kγ ... ... ... ... ... ... . . .\nα α · · · α α α · · · α · · · Kγ\n ·  x1 x2 ... xa xb xc ... xd ... xK  ≥  1 1 ... 1 1 1 ... 1 ... 1 \n(17)\nOne can observe that the vector y ∈ RK+ such that yi = γ K ( K−γα K )i−1 , i = 1, ...,K satisfies all the constraints and every row in (17) is tight, hence y is the optimal solution. So\nR(∅) = K∑ i=1 yi = 1 α\n[ 1− ( K − αγ K )K] ."
    }, {
      "heading" : "B.6. Proof for the Tightness Result",
      "text" : ""
    }, {
      "heading" : "Proof of Lemma 4.",
      "text" : "a) “When α = 0, F (·) is supermodular”;\nIt is easy to see that ξi = 1/K, i ∈ [K]. Since f(·) is convex, it can be easily verified that F (·) is supermodular.\nb) “When γ = 1, F (·) is submodular”;\nNow f(x) = x. Assume there are T1 ⊆ T2 ⊆ V, t ∈ V\\T2. Let T1 = S′1∪Ω′1, T2 = S′2∪Ω′2, where S′1, S′2 ⊆ S,Ω′1,Ω′2 ⊆ Ω. It holds that S′1 ⊆ S′2,Ω′1 ⊆ Ω′2. Now there are two cases:\n1) t = ji ∈ S. Then,\nρji(T1) = [ 1− αγ\nK f(|Ω′1|)\n] ξi, ρji(T2) = [ 1− αγ\nK f(|Ω′2|)\n] ξi\nBecause f(·) is nondecreasing, so it holds ρji(T1) ≥ ρji(T2).\n2) t = ωi ∈ Ω. It reads,\nρωi(T1) = 1\nK 1− αγ ∑ ji∈S′1 ξi  , ρωi(T2) = 1K 1− αγ ∑ ji∈S′2 ξi  Because S′1 ⊆ S′2, so ρωi(T1) ≥ ρωi(T2).\nThe above two situations prove the submodularity of F (T ) when γ = 1.\nc) “F (T ) has submodularity ratio γ and curvature α”.\nLet us assume T = A ∪B and T ′ = A′ ∪B′ are two disjoint sets (T ∩ T ′ = ∅), where A and A′ are subsets of S while B and B′ are subsets of Ω. It is easy to see that A ∩A′ = ∅, B ∩B′ = ∅.\nFirst of all, for the submodularity ratio, assume without loss of generality8 that ρT ′(T ) > 0, so the submodularity ratio is γ = minT,T ′ ∑ i∈T ′ ρi(T )\nρT ′ (T ) .\nOne can see that,\nρT ′(T ) = F (T ′ ∪ T )− F (T )\n= f(|B ∪B′|)− f(|B|)\nK (1− αγ ∑ ji∈A ξi) + [ 1− αγ K f(|B ∪B′|) ] ∑ ji∈A′ ξi\nand ∑ i∈T ′ ρi(T ) = ∑ ωi∈B′ ρωi(T ) + ∑ ji∈A′ ρji(T )\n= |B′|f(|B|+ 1)− f(|B|) K 1− αγ ∑ ji∈A ξi + [1− αγ K f(|B|) ] ∑ ji∈A′ ξi.\nBecause f(|B|) ≤ f(|B ∪B′|), so one has [ 1− αγK f(|B ∪B ′|) ]∑ ji∈A′ ξi ≤ [ 1− αγK f(|B|) ]∑ ji∈A′ ξi, equality holds when B′ = ∅ or A′ = ∅. Therefore,∑ i∈T ′ ρi(T )\nρT ′(T ) = |B′| f(|B|+1)−f(|B|)K\n( 1− αγ ∑ ji∈A ξi ) + [ 1− αγK f(|B|) ]∑ ji∈A′ ξi\nf(|B∪B′|)−f(|B|) K (1− αγ ∑ ji∈A ξi) + [ 1− αγK f(|B ∪B′|) ]∑ ji∈A′ ξi\n≥ |B′| f(|B|+1)−f(|B|)K\n( 1− αγ ∑ ji∈A ξi ) + [ 1− αγK f(|B|) ]∑ ji∈A′ ξi\nf(|B∪B′|)−f(|B|) K (1− αγ ∑ ji∈A ξi) + [ 1− αγK f(|B|) ]∑ ji∈A′ ξi\n8If ρT ′(T ) = 0, from monotonicity of F (·), it must hold ∑ i∈T ′ ρi(T ) = 0, this case is not of interest in Def. 1.\n≥ |B ′|(f(|B|+ 1)− f(|B|)) f(|B ∪B′|)− f(|B|) , (18)\nwhere (18) comes from the fact: f(·) is convex and nondecreasing in [0,K], thus |B′| f(|B|+1)−f(|B|)K ( 1− αγ ∑ ji∈A ξi ) ≤ f(|B∪B ′|)−f(|B|) K (1− αγ ∑ ji∈A ξi).\nNow to continue with (18), one can verify that by setting B = ∅, B′ = Ω, the minimum of (18) is achieved as γ, thus proving the submodularity ratio to be γ.\nThen for the curvature, for any t ∈ T = A ∪B, we want to lower bound ρt(T\\{t}∪T ′)\nρt(T\\{t}) . There are two cases:\n1) When t = ji ∈ A, we have\nρji(T \\ {ji} ∪ T ′) ρji(T \\ {ji}) =\n[ 1− αγK f(|B ∪B ′|) ] ξi[\n1− αγK f(|B|) ] ξi\n= 1− αγK f(|B ∪B ′|) 1− αγK f(|B|) . (19)\nSince f(·) is convex and nondecreasing in [0,K], it is easy to see that the minimum of (19) is achieved whenB = ∅, B′ = Ω as 1− α.\n2) When t = ωi ∈ B, we have,\nρωi(T \\ {ωi} ∪ T ′) ρωi(T \\ {ωi}) = f(|B∪B′|)−f(|B∪B′|−1) K\n[ 1− αγ ∑ i′∈A∪A′ ξi′ ] f(|B|)−f(|B|−1)\nK\n[ 1− αγ ∑ i∈A ξi ] ≥ 1− αγ ∑ i′∈A∪A′ ξi′\n1− αγ ∑ i∈A ξi\n(20)\n= 1− α+ α− αγ\n∑ i′∈A∪A′ ξi′\n1− αγ ∑ i∈A ξi\n(21)\nwhere (20) is because f(·) is convex and nondecreasing in [0,K]. Since α− αγ ∑ i′∈A∪A′ ξi′ ≥ 0 and −αγ ∑ i∈A ξi ≤ 0, continuing with (21) we have,\nρωi(T ′ \\ {ωi} ∪ T )\nρωi(T \\ {ωi}) ≥ 1− α.\nThe above two cases jointly prove that the objective in (3) has curvature α."
    }, {
      "heading" : "C. Existing Notions of Curvature and Submodularity Ratio",
      "text" : "In this section we firstly discuss existing notions of curvature and submodularity ratio, then secondly we present the relations to the notions in this paper."
    }, {
      "heading" : "C.1. Classical Notions of Curvature and Submodularity Ratio",
      "text" : "The curvature of submodular functions measures how close a submodular set function is to being modular, and has been used to prove improved theoretical results for constrained submodular minimization and learning of submodular functions (Iyer et al., 2013). Earlier, it has been used to tighten bounds for submodular maximization subject to a cardinality constraint (Conforti & Cornuéjols, 1984) or a matroid constraint (Vondrák, 2010).\nDefinition 3 (Curvature of submodular functions (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer et al., 2013)). The total curvature κF (which we term as αtotal in the main text) of a submodular function F and the curvature κF (S) w.r.t. a set S ⊆ V are defined as,\nκF := 1−min j∈V ρj(V \\ {j}) ρj(∅) and\nκF (S) := 1−min j∈S ρj(S \\ {j}) ρj(∅) ,\nrespectively. Assume without loss of generality that F ({j}) > 0,∀j ∈ V . One can observe that κF (S) ≤ κF . A modular function has curvature κF = 0, and a matroid rank function has maximal curvature κF = 1. Vondrák (2010) also defines the relaxed notion of curvature (which is called curvature with respect to the optimum) to be the smaller scalar κ̄F (S) s.t,\nρT (S) + ∑\nj∈S∪T ρj(S ∪ T \\ {j}) ≥ (1− κ̄F (S))ρT (∅),∀T ⊆ V. (22)\nIyer et al. (2013) propose two new notions of curvature, which are,\nκ̃F (S) := 1− min T⊆V ρT (S) + ∑ j∈S∪T ρj(S ∪ T \\ {j}) ρT (∅) ,\nκ̂F (S) := 1− ∑ j∈S ρj(S \\ {j})∑\nj∈S ρj(∅) .\nIyer et al. (2013) show that for submodular functions, it holds that κ̂F (S) ≤ κF (S) ≤ κ̃F (S) ≤ κF .\nSubmodularity ratio. Informally, the submodularity ratio quantifies how close a set function is to being submodular (Das & Kempe, 2011). Definition 4 (Original submodularity ratio from Das & Kempe (2011)). Let F (·) be a non-negative nondecreasing set function. The submodularity ratio of a set U w.r.t. an integer k is given by,\nγU,k := min L⊆U min L,S:L∩S=∅,|S|≤k\n∑ j∈L ρj(S)\nρL(S) ."
    }, {
      "heading" : "C.2. Curvature of Non-submodular Functions and Relation to Our Results",
      "text" : "Sviridenko et al. (2013) present a new notion of curvature for monotone set functions. We show how it is related to our notion of curvature in Def. 2. We also show that our approximation factors using the combination of curvature and submodularity ratio characterize the performance of GREEDY for solving problem (P) better.\nSpecifically, for a nondecreasing function F , Sviridenko et al. (2013, Section 8) define the curvature c as\n1− c = min j∈V min A,B∈V\\{j}\nρj(A) ρj(B) . (23)\n(Sviridenko et al., 2013, Theorem 8.1) show that for maximizing a nondecreasing function with bounded curvature c ∈ [0, 1] under a matroid constraint, GREEDY enjoys an approximation guarantee of (1 − c), and it is tight in terms of the definition of c in (23). The following remark discusses the relation to our definition of curvature. Remark 3. For a nondecreasing function F (·), it holds: a) c in (23) is always larger than the notion of curvature α in Def. 2, i.e., c ≥ α; b) For the GREEDY algorithm, there exists a class of functions for which the approximation guarantee characterized by c (which is 1− c) is strictly smaller than the approximation guarantee characterized by the combination of α and γ (which is α−1(1− e−αγ) according to Theorem 1)."
    }, {
      "heading" : "Proof of Remark 3.",
      "text" : "a) Note that the definition of curvature in Def. 2 is equivalent to the smallest scalar α such that,\n∀j ∈ V,∀B ⊆ A ∈ V \\ {j}, ρj(A) ≥ (1− α)ρj(B).\nNow it is easy to see that c ≥ α.\nb) Consider the class of functions in our tightness result in (3). From Lemma 4 we know that its curvature is α and submodularity ratio is γ. So its curvature c in (23) must be greater than or equal to α. Note that the approximation guarantee characterized by c is 1 − c ≤ 1 − α. Taking α = 1 in (3), the approximation guarantee of Sviridenko et al. (2013) is 0. While our approximation guarantee is γ, for any γ ∈ (0, 1], our approximation guarantee is strictly higher than 1− c."
    }, {
      "heading" : "C.3. Relation to Notions in This Work",
      "text" : "- There are two versions of submodularity ratio in this paper: γ and γG, γG cannot be recovered from Def. 4. Our theory can easily accommodate Def. 4: our approximation guarantee in Theorem 1 holds for Def. 4 as long as U contains Ω∗\nand k ≥ K. One benefit of the definition in this work (Def. 1) is that it better handles subtleties in Def. 4 where the denominator could be 0.\n- The curvature in this work is a natural extension of the classical ones for monotone nondecreasing submodular functions (Conforti & Cornuéjols, 1984).\n- Note that classical notions of curvature measure how close a submodular set function is to being modular. The notions of (generalized) curvature in Def. 2 measures how close a set function is to being supermodular.\n- Our combinations of (generalized) curvature and submodularity ratio gives tight approximation guarantees for GREEDY, and this combination is more expressive than the curvature by Sviridenko et al. (2013), as shown in Remark 3."
    }, {
      "heading" : "D. Proofs for Bounding Parameters of Applications",
      "text" : ""
    }, {
      "heading" : "D.1. Proving Proposition 1",
      "text" : ""
    }, {
      "heading" : "Proof of Proposition 1.",
      "text" : "Notice that in this subsection, the matrix XS = [xv1 , . . . ,xvs ] ∈ Rd×|S| is the submatrix consisting the columns of X indexed by the set S.\nOur proof considers the spectral parameters of the matrix XSX>S . For brevity, let us write B = Λ + σ −2XSX > S . B is a symmetric positive definite matrix, thus can be factorized as B = PDP−1.\nLet the eigenvalues of XSX>S be λ1(S) ≥ · · · ≥ λd(S) ≥ 0, where we use the notation that λi(S) := λi(XSX>S ),∀i ∈ [d]. Then the eigenvalues of B are β2+σ−2λi(S), i ∈ [d]. One can see that B−1 = PD−1P−1, and tr(B−1) = tr(D−1) =∑d i=1 1 β2+σ−2λi(S) .\nLet the singular values of XS be σ1(XS) ≥ · · · ≥ σq(XS), where q ≤ min{d, |S|}. For notational simplicity, when |S| < d, we still use the convention σi(XS) = 0, i = q + 1, ..., d to represent the zeros values. One has σ2i (XS) = λi(S), i = 1, ..., d. For notational simplicity, we use F (·) to represent FA(·) in the following.\nMonotonicity. It can be easily seen that F (∅) = 0. To prove that F (S) is monotone nondecreasing, one just needs to show that ∀ω ∈ V \\ S, it holds that F ({ω} ∪ S)− F (S) ≥ 0. One can see that,\nF ({ω} ∪ S)− F (S) = d∑ i=1\n1\nβ2 + σ−2σ2i (XS) − d∑ j=1\n1\nβ2 + σ−2σ2j (XS∪{ω})\n≥ 0 (Cauchy interlacing inequality of singular values).\nBounding parameters. Let us restate the assumption: The data points are normalized, i.e., ‖xi‖ = 1,∀i ∈ V . Given this assumption, it holds that the spectral norm of the data matrix ‖X‖ = σmax(X) ≤ √ n, because of Weyl’s inequality.\n–Bounding the submodularity ratio: We need to lower bound ∑ ω∈Ω\\S ρω(S)\nρΩ(S) =\n∑ ω∈Ω\\S F ({ω}∪S)−F (S)\nF (Ω∪S)−F (S) .\nFor the numerator, we have,\n∑ ω∈Ω\\S F ({ω} ∪ S)− F (S) = ∑ ω∈Ω\\S  d∑ i=1\n1\nβ2 + σ−2σ2i (XS) − d∑ j=1\n1\nβ2 + σ−2σ2j (XS∪{ω})  =\n∑ ω∈Ω\\S d∑ i=1 σ−2[σ2i (XS∪{ω})− σ2i (XS)] (β2 + σ−2σ2i (XS))(β 2 + σ−2σ2i (XS∪{ω}))\n≥ (β2 + σ−2σ2max(X))−2 ∑\nω∈Ω\\S d∑ i=1 σ−2[σ2i (XS∪{ω})− σ2i (XS)]\n= (β2 + σ−2‖X‖2)−2 ∑\nω∈Ω\\S d∑ i=1 σ−2[λi(S ∪ {ω})− λi(S)]\n= (β2 + σ−2‖X‖2)−2 ∑\nω∈Ω\\S\nσ−2[tr(XS∪{ω}X>S∪{ω})− tr(XSX > S )]\n= (β2 + σ−2‖X‖2)−2 ∑\nω∈Ω\\S\nσ−2[tr(XSX>S + xωx > ω )− tr(XSX>S )]\n= (β2 + σ−2‖X‖2)−2 ∑\nω∈Ω\\S\nσ−2tr(xωx>ω ) (linearity of the trace )\n= (β2 + σ−2‖X‖2)−2 ∑\nω∈Ω\\S\nσ−2‖xω‖2\n= σ−2(β2 + σ−2‖X‖2)−2|Ω \\ S| (normalization of the data points) (24)\nFor the denominator, one has, F (Ω ∪ S)− F (S) = d∑ i=1\n1\nβ2 + σ−2σ2i (XS) − d∑ j=1\n1\nβ2 + σ−2σ2j (XS∪Ω)\n≤ d∑\ni=d−|Ω\\S|+1\n1 β2 + σ−2σ2i (XS) − |Ω\\S|∑ j=1\n1\nβ2 + σ−2σ2j (XS∪Ω) (interlacing inequality of singular values)\n≤ |Ω \\ S|( 1 β2 − 1 β2 + σ−2‖X‖2 ) = |Ω \\ S| σ −2‖X‖2\nβ2(β2 + σ−2‖X‖2) . (25)\nCombining (24) and (25) yields,∑ ω∈Ω\\S F ({ω} ∪ S)− F (S)\nF (Ω ∪ S)− F (S) ≥|Ω \\ S|σ −2(β2 + σ−2‖X‖2)−2\n|Ω \\ S| σ −2‖X‖2\nβ2(β2+σ−2‖X‖2)\n= β2\n‖X‖2(β2 + σ−2‖X‖2) .\n–Bounding the curvature: We want to lower bound 1 − α, which corresponds to lower bounding F (S∪Ω)−F (S\\{i}∪Ω)F (S)−F (S\\{i}) . For the numerator, one has,\nF (S ∪ Ω)− F (S \\ {i} ∪ Ω) = d∑\ni′=1\n1\nβ2 + σ−2σ2i′(XS\\{i}∪Ω) − d∑ j=1\n1\nβ2 + σ−2σ2j (XS∪Ω)\n≥ σ−2(β2 + σ−2‖X‖2)−2 (similar derivation as in (24)) . (26)\nFor the denominator, one has (similar derivation as in (25)),\nF (S)− F (S \\ {i}) = d∑\ni′=1\n1\nβ2 + σ−2σ2i′(XS\\{i}) − d∑ j=1\n1\nβ2 + σ−2σ2j (XS)\n≤ 1 β2 + σ−2σ2d(XS\\{i}) − 1 β2 + σ−2σ21(XS) (Cauchy interlacing inequality)\n≤ σ −2‖X‖2\nβ2(β2 + σ−2‖X‖2) . (27)\nCombining (26) and (27) we get,\nF (S ∪ Ω)− F (S \\ {i} ∪ Ω) F (S)− F (S \\ {i}) ≥ β 2 ‖X‖2(β2 + σ−2‖X‖2) ."
    }, {
      "heading" : "D.2. Proofs for Determinantal Functions of Square Submatrix",
      "text" : ""
    }, {
      "heading" : "Proof of Proposition 2.",
      "text" : "Notice that in this subsection, the matrix ΣS is the square submatrix of Σ, with both its rows and columns indexed by S.\na) We want to prove that F (·) is supermodular. Assume that A ⊆ B ⊆ V and i ∈ V \\B, then\nρi(A) = det(I + σ−2ΣA∪{i})− det(I + σ−2ΣA) = ∑\nS⊆A∪{i} det((σ−2Σ)S)− ∑ S⊆A det((σ−2Σ)S) (Kulesza & Taskar, 2012, Theorem 2.1)\n= ∑ S⊆A det((σ−2Σ)S∪{i})\n≤ ∑ S⊆B det((σ−2Σ)S∪{i}) (Σ is positive semidefinite) = det(I + σ−2ΣB∪{i})− det(I + σ−2ΣB) = ρi(B),\nwhich proves that F (·) is supermodular. b) We want to lower bound ∑ ω∈Ω\\S ρω(S)\nρΩ(S) =\n∑ ω∈Ω\\S F ({ω}∪S)−F (S)\nF (Ω∪S)−F (S) .\nFor the numerator, one has,\n∑ ω∈Ω\\S F ({ω} ∪ S)− F (S) = ∑ ω∈Ω\\S |S∪{ω}|∏ i=1 λi(AS∪{ω})− |S|∏ j=1 λj(AS)\n= ∑\nω∈Ω\\S\nλ|S∪{ω}|(AS∪{ω}) |S|∏ i=1 λi(AS∪{ω})− |S|∏ j=1 λj(AS)\n≥ ∑\nω∈Ω\\S\nλ|S∪{ω}|(AS∪{ω}) |S|∏ i=1 λi(AS)− |S|∏ j=1 λj(AS) (Cauchy interlacing inequality)\n= ∑\nω∈Ω\\S (λ|S∪{ω}|(AS∪{ω})− 1) |S|∏ i=1 λi(AS). (28)\nFor the denonimator, it holds,\nF (Ω ∪ S)− F (S) = |Ω∪S|∏ i=|Ω\\S| λi(AΩ∪S) |Ω\\S|∏ j=1 λj(AΩ∪S)− |S|∏ i=1 λi(AS)\n≤ |Ω\\S|∏ j=1 λj(AS∪Ω)− 1  |S|∏ i=1 λi(AS) (Cauchy interlacing inequality). (29)\nCombining (28) and (29) gives,∑ ω∈Ω\\S F ({ω} ∪ S)− F (S) F (Ω ∪ S)− F (S) ≥ ∑ ω∈Ω\\S(λ|S∪{ω}|(AS∪{ω})− 1) ∏|S| i=1 λi(AS)(∏|Ω\\S| j=1 λj(AS∪Ω)− 1 )∏|S| i=1 λi(AS)\n= ∑ ω∈Ω\\S(λ|S∪{ω}|(AS∪{ω})− 1)(∏|Ω\\S|\nj=1 λj(AS∪Ω)− 1 )\n≥ K(λn − 1)∏K j=1 λj − 1 ,\nwhere the last inequality comes from that |Ω \\ S| ≤ K."
    }, {
      "heading" : "D.3. LP with Combinatorial Constraints",
      "text" : "D.3.1. TWO EXAMPLES WHERE F (S) IS NON-SUBMODULAR\n1), Considering the following LP:\nmax 4x1+ x2+ 4x3 s.t. 2x1+ x2 ≤ 2\nx2+ 2x3 ≤ 2 (30)\nx1, x2, x3 ≥ 0.\nFor this LP, one can easily see that F ({1, 2}) = 4, F ({2}) = 2, F ({1, 2, 3}) = 8, F ({2, 3}) = 4, thus F ({1, 2}) − F ({2}) < F ({1, 2, 3})− F ({2, 3}), which shows F is non-submodular.\n2), Considering the following LP:\nmax 10x1+ 12x2+ 12x3 s.t. x1+ 2x2+ 2x3 ≤ 20\n2x1+ x2+ 2x3 ≤ 20 2x2+ 2x2+ x3 ≤ 20\n(31)\nx1, x2, x3 ≥ 0.\nFor this LP, one can see that F ({1, 2}) = 120, F ({2}) = 120, F ({1, 2, 3}) = 136, F ({2, 3}) = 120, thus F ({1, 2}) − F ({2}) < F ({1, 2, 3})− F ({2, 3}). But this one has degenerate basic feasible solutions."
    }, {
      "heading" : "D.3.2. PROVING PROPOSITION 3",
      "text" : "To prove Proposition 3, we first need to present the setup. The LP corresponding to F (S) is,\nmax 〈dS ,xS〉 (LPS) s.t. ASxS ≤ b\n(32)\nxS ≥ 0.\nwhere the columns of AS ∈ Rm×|S|+ are the columns of A indexed by the set S. xS (respectively, dS) is the subvector of x (respectively, d) indexed by S. To apply the optimality condition of a LP in the standard form, let us change (LPS) to be the following standard LP by introducing the slack variable ξ ∈ Rm,\n−min 〈cS ,xS〉 (LP ∗S) s.t. ASxS + Imξ = b\n(33)\nxS ≥ 0, ξ ≥ 0.\nwhere cS := −dS . Let us denote Ā := [AS , Im] ∈ Rm×(|S|+m), x̄ := [x>S , ξ>]>.\nLet (x(S), ξ(S)) denote the optimal solution of (LP ∗S). The corresponding basis of of (LP ∗ S) is B (S), which is a subset of V ∪ {ξ1, · · · , ξm}, and |B(S)| = m.\nAccording to Bertsimas & Tsitsiklis (1997, Chapter 3.1), the optimality condition for (LP ∗S) is: Given a basic feasible solution (x, ξ) with the basis as B, the reduced cost is c̄j = cj − c>BĀ −1 B Ā·j . 1) If (x, ξ) is optimal and non-degenerate, then c̄j ≥ 0,∀j; 2) If c̄j ≥ 0,∀j, then (x, ξ) is optimal.\nProof of Proposition 3. First of all, let us detail the non-degenerancy assumption.\nNon-degenerancy assumption: The basic feasible solutions of the correpsonding LP in standard form (LP ∗S ) is nondegenerate ∀S ⊆ V .\na) It is easy to see that F (∅) = 0, and F (S) is nondecreasing. b) For the submodularity ratio, we want to lower bound ∑ ω∈Ω\\S ρω(S)\nρΩ(S) . There could be in total four situations: 1) ∑ ω∈Ω\\S ρω(S) = 0 but ρΩ(S) > 0. We will prove that this situation cannot happen, or in the other words,∑\nω∈Ω\\S F ({ω} ∪ S)− F (S) = 0 implies that F (Ω ∪ S)− F (S) = 0 as well.\nFirst of all, since F (S) is nondecreasing, so F ({ω} ∪ S) − F (S) = 0,∀ω. We know that (x(S), ξ(S)) is the optimal solution of (LP ∗S), and (x (S), ξ(S)) is a basic feasible solution of (LP ∗S∪{ω}), so (x (S), ξ(S)) is also the optimal solution of (LP ∗S∪{ω}). Since (LP ∗ S∪{ω}) is non-degenerate, according to the optimality condition, the reduced cost of xω: c̄ω must be greater than or equal zero.\nNow we know that c̄ω ≥ 0,∀ω ∈ Ω\\S, and (x(S), ξ(S)) is a basic feasible solution of (LP ∗S∪Ω) as well, again using the optimality condition, we know that (x(S), ξ(S)) is optimal for (LP ∗S∪{Ω}). So F (Ω ∪ S)− F (S) = 0.\n2) ∑ ω∈Ω\\S ρω(S) = 0 and ρΩ(S) = 0. The submodularity ratio is 1 in this situation.\n3) ∑ ω∈Ω\\S ρω(S) > 0 and ρΩ(S) = 0. This can be ignored since we want a lower bound.\n4) ∑ ω∈Ω\\S ρω(S) > 0 and ρΩ(S) > 0. This situation gives the lower bound:∑\nω∈Ω\\S ρω(S)\nρΩ(S) ≥\nmaxω∈Ω\\S ρω(S)\nF (V)\n≥ minS⊆V,ω∈V\\S,ρω(S)>0 ρω(S) F (V) =: γ0 > 0."
    }, {
      "heading" : "E. Details about SDP Formulation of Bayesian A-optimality Objective",
      "text" : "The SDP formulation used in this paper is consistent with that from Boyd & Vandenberghe (2004, Chapter 7.5) and Krause et al. (2008). To make this work self-contained, we present the details here.\nFirstly, maximizing the Bayesian A-optimality objective is equivalent to,\nmin S⊆V,|S|≤K\ntr((Λ + σ−2XSX>S ) −1) (34)\nBy introducing binary variables mj , j ∈ [n], (34) is equivalent to,\nmin tr((Λ + σ−2 n∑ j=1 mjxjx > j ) −1) (35)\ns.t. mj ∈ {0, 1}, j ∈ [n],m1 + · · ·+mn ≤ K\nA proper relaxation is (relaxing the variables λj = mj/K, j ∈ [n]),\nmin tr((Λ + σ−2 n∑ j=1 λjxjx > j ) −1) (36) s.t. λ ∈ Rn+,1>λ = 1.\nAccording to the Schur complement lemma, the relaxed formulation (36) is equivalent to the following SDP problem,\nmin u∈Rd\n1>u\ns.t. [ Λ + σ−2 ∑n j=1 λjxjx > j ek\ne>k uk\n] 0, k = 1, · · · , d (SDP)\nλ ∈ Rn+,1>λ = 1,\nwhere ek ∈ Rd is the kth standard basis vector. According to Krause et al. (2008), after solving the (SDP) problem we sort the entries of λ in descending order, and select the largest K coordinates as the indices of the K elements to be selected."
    }, {
      "heading" : "F. Proofs and Details in Related Work (Section 6)",
      "text" : "Remark 4. For a set function F (·): a) Its submodularity ratio γ is lower-bounded away from 0 and its curvature α is upper-bounded away from 1 does not imply that it is weakly submodular; b) F (·) is weakly submodular does not imply that its submodularity ratio γ is lower-bounded away from 0 and its curvature α is upper-bounded away from 1."
    }, {
      "heading" : "Proof of Remark 4.",
      "text" : "For argument a): Let F (S) := |S|4, S ⊆ V , which is a supermodular function, so the curvature is 0 (upper-bounded away from 1). The submodualrity ratio can be lower bounded by n−3. But it is not weakly submodular according to Proposition 3.11 in Borodin et al. (2014).\nFor argument b): Let us take a minimum cardinality function with k = 2, i.e., F (S) = B > 0 iff. |S| ≥ 2, otherwise F (S) = 0. According to Proposition 3.5 in Borodin et al. (2014), it is weakly submodular, but it is easy to see that its submodualrity ratio is 0.\nMore on submodularity index. It is defined as (equivalent to that in Zhou & Spanos (2016)):\nmin Ω,S⊆V min |Ω\\S|≤K ( ∑ ω∈Ω\\S ρω(S)− ρΩ(S) ) ."
    }, {
      "heading" : "G. More Applications",
      "text" : ""
    }, {
      "heading" : "G.1. Subset Selection Using the R2 Objective",
      "text" : "Subset selection aims to estimate a predictor variable Z using linear regression on a small subset from the set of observation variables V = {X1, ..., Xn}. Let C to be the covariance matrix among the observation variables {X1, ..., Xn}. We use b to denote the covariances between Z and the Xi, with entries bi = Cov(Z,Xi). Assuming there are m observations, let us arrange the data of all the observation variables to be a design matrix X ∈ Rm×n, with each column representing the observations of one variable. Given a budget parameter K, subset selection tries to find a set S ⊆ V of at most K elements, and a linear predictor Z ′ = ∑ i∈S αiXi = X·SαS , in order to maximize the squared multiple corrleation RZ,S = Var(Z)−E[(Z−Z′)2]\nVar(Z) , it measures the fraction of variance of Z explained by variables in S. Assume Z is normalized to have variance 1, and it is well-known that the optimal regression coefficients are αS = (CS)−1bS , so the R2 objective can be formulated as,\nF (S) := R2Z,S = b > S (CS) −1bS , S ⊆ V. (37)\nDas & Kempe (2011) show that the submodularity ratio of F in (37) can be lower bounded by λmin(C), which is the smallest eigenvalue of C. The theoretical results in this work suggests that the approximation guarantees for maximizing F in (37) can be further improved by analyzing the curvature parameters. The experimental results in Appendix H.2 demonstrates that it is promising to upper bound the curvature parameters of (37) (possibly with regular assumptions) ."
    }, {
      "heading" : "G.2. Sparse Modeling with Strongly Convex Loss Functions",
      "text" : "Sparse modeling aims to build a model with a small subset of at most K features, out of in total n features. Let f(x) : Rn 7→ R to be the loss function, the corresponding objective is,\nmin f(x) s.t. |supp(x)| ≤ K.\nAssume f(x) is m-strongly convex and has Lipschitz continuous gradient with parameter L, which is equilavent to say that g(x) := −f(x) is m-strongly concave and has L-Lipschitz continuous gradient. Then for all x,y ∈ dom(f) it holds,\nm 2 ‖y − x‖2 ≤ −g(y) + g(x) + 〈∇g(x),y − x〉 ≤ L 2 ‖y − x‖2. (38)\nIn solving this problem, the GREEDY algorithm maximizes the corresponding auxiliary set function,\nF (S) := max supp(x)⊆S\ng(x), S ⊆ [n] (39)\nElenberg et al. (2016) analyzed the approximation guarantees of GREEDY by bounding the submodularity ratio of F (S). Specifically, Lemma 5 (Paraphrasing Theorem 1 in Elenberg et al. (2016)). The submodularity ratio of F (S) in (39) is lower bounded by mL .\nBy further bounding the curvature parameters of the auxiliary set function in (39), one can get improved approximation guarantees according to our theoretical findings."
    }, {
      "heading" : "G.3. Optimal Budget Allocation with Combinatorial Constraints",
      "text" : "Optimal budget allocation (Soma et al., 2014) is a special case of the influence maximization problem, it aims to distribute the budget (e.g., space of an inline advertisement, or time for a TV advertisement) among the customers, and to maximize the expected influence on the potential customers. A concrete application is for the search marketing advertiser bidding task, in which vendors bid for the right to appear alongside the results of different search keywords. Let xis ∈ R+ to be the volume of advertising space allocated to the advertiser i to show his ad alongside query keyword s. Bian et al. (2017) present continuous DR-submodular objectives to model this problem with continuous assignments.\nThe search engine company (e.g., Google and Yahoo) needs to distribute the budget (ad space) to all vendors to maximize their influence on the customers, while respecting various continuous and combinatorial constraints. For the continuous constraints, for instance, each vendor has a specified budget limit for advertising, and the ad space associated with each search keyword can not be too large. These continuous constraints can be formulated as a convex set P . For combinatorial constraints, each vendor needs to obey the Internet regulations of sensitive search keywords in his country, so the search engine company can only choose a subset of “legal” keywords for a specific vendor. The combinatorial constraints can be arranged as a matroidM = (V, I). Hence the problem in general can be formulated as,\nmax x∈P and supp(x)∈I g(x),\nwhere g(x) is the total influence modeled by a DR-submodular function. For one of its possible forms, one can refer to Bian et al. (2017). The GREEDY algorithm solves this problem by maximizing the following auxiliary set function F (S) while respecting the combinatorial constraints,\nmax S∈I F (S), where F (S) := max supp(x)⊆S,x∈P g(x). (40)\nBy studying the submodularity ratio and curvature parameters of F (S) in (40), one could obtain theoretical guarantees of the GREEDY algorithm according to Theorem 1 in this work."
    }, {
      "heading" : "H. More Experimental Results",
      "text" : ""
    }, {
      "heading" : "H.1. Bayesian A-optimality Experiments",
      "text" : "We put the results on a randomly generated dataset, to illustrate what does the proved bounds looks like. In the synthetic experiments we generate random observations from a multivariate Gaussian distribution with correlation 0.5. Fig. 8 shows\nthe results (function value, parameters and approximation bounds) for one randomly generated data set with d = 6 features and n = 12 observations. Specifically, Fig. 8c traces the two approximation bounds from Theorem 1 (and Lemma 3): one\ncurve shows the constant-factor bound α−1(1 − e−αγ) and the other the K-dependent bound 1α\n[ 1− ( K−αγ K )K] . We\nobserve that both bounds give reasonable predictions of the performance of GREEDY."
    }, {
      "heading" : "H.2. Subset Selection Using the R2 Objective",
      "text" : "curvature and submodularity ratio take values in (0, 1), which can be used to give improved approximation bounds\nfor GREEDY."
    } ],
    "references" : [ {
      "title" : "Greedy column subset selection: New bounds and distributed algorithms",
      "author" : [ "Altschuler", "Jason", "Bhaskara", "Aditya", "Fu", "Gang", "Mirrokni", "Vahab", "Rostamizadeh", "Afshin", "Zadimoghaddam", "Morteza" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Altschuler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Altschuler et al\\.",
      "year" : 2016
    }, {
      "title" : "Algorithms for optimizing the ratio of submodular functions",
      "author" : [ "Bai", "Wenruo", "Iyer", "Rishabh", "Wei", "Kai", "Bilmes", "Jeff" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bai et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2016
    }, {
      "title" : "Introduction to Linear Optimization",
      "author" : [ "Bertsimas", "Dimitris", "Tsitsiklis", "John" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsimas et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Bertsimas et al\\.",
      "year" : 1997
    }, {
      "title" : "Guaranteed nonconvex optimization: Submodular maximization over continuous domains",
      "author" : [ "Bian", "Andrew An", "Mirzasoleiman", "Baharan", "Buhmann", "Joachim M", "Krause", "Andreas" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Bian et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Bian et al\\.",
      "year" : 2017
    }, {
      "title" : "Weakly submodular functions",
      "author" : [ "Borodin", "Allan", "Le", "Dai Tri Man", "Ye", "Yuli" ],
      "venue" : "arXiv preprint arXiv:1401.6697,",
      "citeRegEx" : "Borodin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Borodin et al\\.",
      "year" : 2014
    }, {
      "title" : "Submodular maximization with cardinality constraints",
      "author" : [ "Buchbinder", "Niv", "Feldman", "Moran", "Naor", "Joseph", "Schwartz", "Roy" ],
      "venue" : "In SODA, pp",
      "citeRegEx" : "Buchbinder et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Buchbinder et al\\.",
      "year" : 2014
    }, {
      "title" : "Stable signal recovery from incomplete and inaccurate measurements",
      "author" : [ "Candes", "Emmanuel J", "Romberg", "Justin K", "Tao", "Terence" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Candes et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Candes et al\\.",
      "year" : 2006
    }, {
      "title" : "Bayesian experimental design: A review",
      "author" : [ "Chaloner", "Kathryn", "Verdinelli", "Isabella" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Chaloner et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Chaloner et al\\.",
      "year" : 1995
    }, {
      "title" : "Submodular set functions, matroids and the greedy algorithm: tight worst-case bounds and some generalizations of the radoedmonds theorem",
      "author" : [ "Conforti", "Michele", "Cornuéjols", "Gérard" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "Conforti et al\\.,? \\Q1984\\E",
      "shortCiteRegEx" : "Conforti et al\\.",
      "year" : 1984
    }, {
      "title" : "Algorithms for subset selection in linear regression",
      "author" : [ "Das", "Abhimanyu", "Kempe", "David" ],
      "venue" : "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Das et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2008
    }, {
      "title" : "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection",
      "author" : [ "Das", "Abhimanyu", "Kempe", "David" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Das et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2011
    }, {
      "title" : "Analysis of greedy approximations with nonsubmodular potential functions",
      "author" : [ "Du", "Ding-Zhu", "Graham", "Ronald L", "Pardalos", "Panos M", "Wan", "Peng-Jun", "Wu", "Weili", "Zhao", "Wenbo" ],
      "venue" : "In SODA, pp",
      "citeRegEx" : "Du et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2008
    }, {
      "title" : "Restricted strong convexity implies weak submodularity",
      "author" : [ "Elenberg", "Ethan R", "Khanna", "Rajiv", "Dimakis", "Alexandros G", "Negahban", "Sahand" ],
      "venue" : "arXiv preprint arXiv:1612.00804,",
      "citeRegEx" : "Elenberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Elenberg et al\\.",
      "year" : 2016
    }, {
      "title" : "Welfare maximization and the supermodular degree",
      "author" : [ "Feige", "Uriel", "Izsak", "Rani" ],
      "venue" : "In Proceedings of the Fourth Conference on Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Feige et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Feige et al\\.",
      "year" : 2013
    }, {
      "title" : "An introduction to variable and feature selection",
      "author" : [ "Guyon", "Isabelle", "Elisseeff", "André" ],
      "venue" : "Journal of machine learning research,",
      "citeRegEx" : "Guyon et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Guyon et al\\.",
      "year" : 2003
    }, {
      "title" : "Maximization of approximately submodular functions",
      "author" : [ "Horel", "Thibaut", "Singer", "Yaron" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Horel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Horel et al\\.",
      "year" : 2016
    }, {
      "title" : "Algorithms for approximate minimization of the difference between submodular functions, with applications",
      "author" : [ "Iyer", "Rishabh", "Bilmes", "Jeff" ],
      "venue" : "In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Iyer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Iyer et al\\.",
      "year" : 2012
    }, {
      "title" : "Curvature and optimal algorithms for learning and minimizing submodular functions",
      "author" : [ "Iyer", "Rishabh K", "Jegelka", "Stefanie", "Bilmes", "Jeff A" ],
      "venue" : null,
      "citeRegEx" : "Iyer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Iyer et al\\.",
      "year" : 2013
    }, {
      "title" : "On iterative hard thresholding methods for high-dimensional m-estimation",
      "author" : [ "Jain", "Prateek", "Tewari", "Ambuj", "Kar", "Purushottam" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Jain et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2014
    }, {
      "title" : "Submodularity beyond submodular energies: coupling edges in graph cuts",
      "author" : [ "Jegelka", "Stefanie", "Bilmes", "Jeff" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Jegelka et al\\.,? \\Q1897\\E",
      "shortCiteRegEx" : "Jegelka et al\\.",
      "year" : 1897
    }, {
      "title" : "On approximate non-submodular minimization via treestructured supermodularity",
      "author" : [ "Kawahara", "Yoshinobu", "Iyer", "Rishabh K", "Bilmes", "Jeff A" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Kawahara et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kawahara et al\\.",
      "year" : 2015
    }, {
      "title" : "Submodular dictionary selection for sparse representation",
      "author" : [ "Krause", "Andreas", "Cevher", "Volkan" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Krause et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2010
    }, {
      "title" : "Submodular function maximization. In Tractability: Practical Approaches to Hard Problems",
      "author" : [ "Krause", "Andreas", "Golovin", "Daniel" ],
      "venue" : null,
      "citeRegEx" : "Krause et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2014
    }, {
      "title" : "Nearoptimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies",
      "author" : [ "Krause", "Andreas", "Singh", "Ajit", "Guestrin", "Carlos" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Krause et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2008
    }, {
      "title" : "Fast sparse gaussian process methods: The informative vector machine",
      "author" : [ "Lawrence", "Neil", "Seeger", "Matthias", "Herbrich", "Ralf" ],
      "venue" : "NIPS, pp",
      "citeRegEx" : "Lawrence et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lawrence et al\\.",
      "year" : 2003
    }, {
      "title" : "A submodularsupermodular procedure with applications to discriminative structure learning",
      "author" : [ "Narasimhan", "Mukund", "Bilmes", "Jeff" ],
      "venue" : "In Proceedings of the TwentyFirst Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2005
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functions–i",
      "author" : [ "Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nemhauser et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Nemhauser et al\\.",
      "year" : 1978
    }, {
      "title" : "Optimal budget allocation: Theoretical guarantee and efficient algorithm",
      "author" : [ "Soma", "Tasuku", "Kakimura", "Naonori", "Inaba", "Kazuhiro", "Kawarabayashi", "Ken-ichi" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Soma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Soma et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal approximation for submodular and supermodular optimization with bounded curvature",
      "author" : [ "Sviridenko", "Maxim", "Vondrák", "Jan", "Ward", "Justin" ],
      "venue" : "arXiv preprint arXiv:1311.4728,",
      "citeRegEx" : "Sviridenko et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sviridenko et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimal approximation for the submodular welfare problem in the value oracle model",
      "author" : [ "Vondrák", "Jan" ],
      "venue" : "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Vondrák and Jan.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vondrák and Jan.",
      "year" : 2008
    }, {
      "title" : "Submodularity and curvature: the optimal algorithm",
      "author" : [ "Vondrák", "Jan" ],
      "venue" : "RIMS Kokyuroku Bessatsu B,",
      "citeRegEx" : "Vondrák and Jan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vondrák and Jan.",
      "year" : 2010
    }, {
      "title" : "Maximizing a monotone submodular function with a bounded curvature under a knapsack constraint",
      "author" : [ "Yoshida", "Yuichi" ],
      "venue" : "arXiv preprint arXiv:1607.04527,",
      "citeRegEx" : "Yoshida and Yuichi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yoshida and Yuichi.",
      "year" : 2016
    }, {
      "title" : "Causal meets submodular: Subset selection with directed information",
      "author" : [ "Zhou", "Yuxun", "Spanos", "Costas J" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    }, {
      "title" : "functions, it holds that κ̂F (S) ≤ κF (S) ≤ κ̃F (S) ≤ κF",
      "author" : [ "Iyer" ],
      "venue" : null,
      "citeRegEx" : "Iyer,? \\Q2011\\E",
      "shortCiteRegEx" : "Iyer",
      "year" : 2011
    }, {
      "title" : "Curvature of Non-submodular Functions and Relation to Our Results Sviridenko et al. (2013) present a new notion of curvature for monotone set functions. We show how it is related to our notion of curvature in Def. 2. We also show that our approximation factors using the combination of curvature and submodularity ratio characterize the performance of GREEDY for solving problem (P) better",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "C.2.,? \\Q2013\\E",
      "shortCiteRegEx" : "C.2.",
      "year" : 2013
    }, {
      "title" : "Section 8) define the curvature c as 1− c = min",
      "author" : [ "Sviridenko" ],
      "venue" : null,
      "citeRegEx" : "F and Sviridenko,? \\Q2013\\E",
      "shortCiteRegEx" : "F and Sviridenko",
      "year" : 2013
    }, {
      "title" : "Details about SDP Formulation of Bayesian A-optimality Objective The SDP formulation used in this paper is consistent with that from Boyd",
      "author" : [ ],
      "venue" : "Chapter 7.5) and Krause et al",
      "citeRegEx" : "0.,? \\Q2004\\E",
      "shortCiteRegEx" : "0.",
      "year" : 2004
    }, {
      "title" : "2016) analyzed the approximation guarantees of GREEDY by bounding the submodularity ratio of F (S). Specifically, Lemma 5 (Paraphrasing Theorem 1 in Elenberg et al",
      "author" : [ "Elenberg" ],
      "venue" : null,
      "citeRegEx" : "Elenberg,? \\Q2016\\E",
      "shortCiteRegEx" : "Elenberg",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "is a monotone nondecreasing submodular set function1, the GREEDY algorithm enjoys the multiplicative approximation guarantee of (1 − 1/e) (Nemhauser et al., 1978; Vondrák, 2008; Krause & Golovin, 2014).",
      "startOffset" : 138,
      "endOffset" : 201
    }, {
      "referenceID" : 17,
      "context" : "This constant factor can be improved by refining the characterization of the objective using the curvature (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer et al., 2013), which informally quantifies how close a submodular function is to being modular (i.",
      "startOffset" : 107,
      "endOffset" : 170
    }, {
      "referenceID" : 24,
      "context" : "However, for many applications, including experimental design and sparse Gaussian processes (Lawrence et al., 2003), F (S) is in general not submodular (Krause et al.",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : ", 2003), F (S) is in general not submodular (Krause et al., 2008) and the above guarantee does not hold.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "This setting covers various important applications, to name a few, feature selection (Guyon & Elisseeff, 2003), sparse approximation (Das & Kempe, 2008; Krause & Cevher, 2010), sparse recovery (Candes et al., 2006), sparse M-estimation (Jain et al.",
      "startOffset" : 193,
      "endOffset" : 214
    }, {
      "referenceID" : 18,
      "context" : ", 2006), sparse M-estimation (Jain et al., 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : ", 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler et al., 2016).",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 32,
      "context" : "jective (Das & Kempe, 2011). To explain the good empirical performance, Das & Kempe (2011) proposed the submodularity ratio, a quantity characterizing how close a set function is to being submodular.",
      "startOffset" : 23,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : ", 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler et al., 2016). Recently, Elenberg et al. (2016) proved that if f(x) has L-restricted smoothness and m-restricted strong convexity, then the submodularity ratio of F (S) is lower bounded by m/L.",
      "startOffset" : 94,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "Sviridenko et al. (2013) presented a notion of curvature for monotone non-submodular functions.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 28,
      "context" : "Sviridenko et al. (2013) presented a notion of curvature for monotone non-submodular functions. We show in Appendix C the details of these notions and the relations to ours. Additionally, we prove in Remark 3 of Appendix C.2 that our combination of curvature and submodularity ratio is more expressive than that of Sviridenko et al. (2013) in characterizing the maximization of problem (P) using standard GREEDY.",
      "startOffset" : 0,
      "endOffset" : 340
    }, {
      "referenceID" : 26,
      "context" : "For the case α = 1, γ = 1, we recover the classical guarantee of (1 − 1/e) (Nemhauser et al., 1978).",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "Krause et al. (2008) investigated several criteria for this purpose, amongst others the Bayesian A-optimality criterion.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 24,
      "context" : "In the Informative Vector Machine (Lawrence et al., 2003), the information gain of a subset of points S ⊆ V is 1 2 logF (S), where",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Zhou & Spanos (2016) use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm (Buchbinder et al., 2014) for maximizing a non-monotone function.",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "Sviridenko et al. (2013) present a notion of curvature for monotone nonsubmodular functions.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "Sviridenko et al. (2013) present a notion of curvature for monotone nonsubmodular functions. Appendix C provides details of that notion and relates it to our definition. Yoshida (2016) prove an improved approximation ratio for knapsack-constrained maximization of submodular functions with bounded curvature.",
      "startOffset" : 0,
      "endOffset" : 185
    }, {
      "referenceID" : 16,
      "context" : "Krause et al. (2008) define approximately submodular functions with parameter ≥ 0 as those functions F that satisfy an approximate diminishing returns property, i.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "Du et al. (2008) study the greedy maximization of nonsubmodular potential functions with restricted submodularity and shifted submodularity.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "Du et al. (2008) study the greedy maximization of nonsubmodular potential functions with restricted submodularity and shifted submodularity. Restricted submodularity refers to functions which are submodular only over some collection of subsets of V , and shifted submodularity can be viewed as a special case of the approximate diminishing returns as defined above. Recently, Horel & Singer (2016) study -approximately submodular functions, which arised from their research on “noisy” submodular functions.",
      "startOffset" : 0,
      "endOffset" : 398
    }, {
      "referenceID" : 3,
      "context" : "Borodin et al. (2014) study weakly submodular functions, i.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Borodin et al. (2014) study weakly submodular functions, i.e., montone, nomalized functions F (·) s.t. for any S, T , it holds |T |F (S) + |S|F (T ) ≥ |S ∩T |F (S ∪T ) + |S ∪T |F (S ∩T ). For a function F (·), we show in Remark 4 that the following two facts do not imply each other: i) F (·) is weakly submodular; ii) The submodularity ratio of F (·) is strictly larger than 0, and its curvature is strictly smaller than 1. Other notions of non-submodularity. Feige & Izsak (2013) introduce the supermodular degree as a complexity measure for set functions.",
      "startOffset" : 0,
      "endOffset" : 482
    }, {
      "referenceID" : 3,
      "context" : "Borodin et al. (2014) study weakly submodular functions, i.e., montone, nomalized functions F (·) s.t. for any S, T , it holds |T |F (S) + |S|F (T ) ≥ |S ∩T |F (S ∪T ) + |S ∪T |F (S ∩T ). For a function F (·), we show in Remark 4 that the following two facts do not imply each other: i) F (·) is weakly submodular; ii) The submodularity ratio of F (·) is strictly larger than 0, and its curvature is strictly smaller than 1. Other notions of non-submodularity. Feige & Izsak (2013) introduce the supermodular degree as a complexity measure for set functions. They show that a greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree. Zhou & Spanos (2016) use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm (Buchbinder et al.",
      "startOffset" : 0,
      "endOffset" : 735
    }, {
      "referenceID" : 3,
      "context" : "Borodin et al. (2014) study weakly submodular functions, i.e., montone, nomalized functions F (·) s.t. for any S, T , it holds |T |F (S) + |S|F (T ) ≥ |S ∩T |F (S ∪T ) + |S ∪T |F (S ∩T ). For a function F (·), we show in Remark 4 that the following two facts do not imply each other: i) F (·) is weakly submodular; ii) The submodularity ratio of F (·) is strictly larger than 0, and its curvature is strictly smaller than 1. Other notions of non-submodularity. Feige & Izsak (2013) introduce the supermodular degree as a complexity measure for set functions. They show that a greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree. Zhou & Spanos (2016) use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm (Buchbinder et al., 2014) for maximizing a non-monotone function. Optimization of non-submodular functions. The submodular-supermodular procedure has been proposed to minimize the difference of two submodular functions (Narasimhan & Bilmes, 2005; Iyer & Bilmes, 2012). Jegelka & Bilmes (2011) present the problem of minimizing “cooperative cuts”, which are non-submodular in general, and propose efficient algorithms for optimization.",
      "startOffset" : 0,
      "endOffset" : 1118
    }, {
      "referenceID" : 3,
      "context" : "Borodin et al. (2014) study weakly submodular functions, i.e., montone, nomalized functions F (·) s.t. for any S, T , it holds |T |F (S) + |S|F (T ) ≥ |S ∩T |F (S ∪T ) + |S ∪T |F (S ∩T ). For a function F (·), we show in Remark 4 that the following two facts do not imply each other: i) F (·) is weakly submodular; ii) The submodularity ratio of F (·) is strictly larger than 0, and its curvature is strictly smaller than 1. Other notions of non-submodularity. Feige & Izsak (2013) introduce the supermodular degree as a complexity measure for set functions. They show that a greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree. Zhou & Spanos (2016) use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm (Buchbinder et al., 2014) for maximizing a non-monotone function. Optimization of non-submodular functions. The submodular-supermodular procedure has been proposed to minimize the difference of two submodular functions (Narasimhan & Bilmes, 2005; Iyer & Bilmes, 2012). Jegelka & Bilmes (2011) present the problem of minimizing “cooperative cuts”, which are non-submodular in general, and propose efficient algorithms for optimization. Kawahara et al. (2015) analyze unconstrained minimization of the sum of a submodular function and a treestructured supermodular function.",
      "startOffset" : 0,
      "endOffset" : 1283
    }, {
      "referenceID" : 1,
      "context" : "Bai et al. (2016) investigate the minimization of the ratio of two submodular functions, which can be solved with bounded approximation factor.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 17,
      "context" : "Classical Notions of Curvature and Submodularity Ratio The curvature of submodular functions measures how close a submodular set function is to being modular, and has been used to prove improved theoretical results for constrained submodular minimization and learning of submodular functions (Iyer et al., 2013).",
      "startOffset" : 292,
      "endOffset" : 311
    }, {
      "referenceID" : 17,
      "context" : "Definition 3 (Curvature of submodular functions (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer et al., 2013)).",
      "startOffset" : 48,
      "endOffset" : 111
    }, {
      "referenceID" : 36,
      "context" : "Assume without loss of generality that F ({j}) > 0,∀j ∈ V . One can observe that κF (S) ≤ κF . A modular function has curvature κF = 0, and a matroid rank function has maximal curvature κF = 1. Vondrák (2010) also defines the relaxed notion of curvature (which is called curvature with respect to the optimum) to be the smaller scalar κ̄F (S) s.",
      "startOffset" : 49,
      "endOffset" : 209
    }, {
      "referenceID" : 36,
      "context" : "Informally, the submodularity ratio quantifies how close a set function is to being submodular (Das & Kempe, 2011). Definition 4 (Original submodularity ratio from Das & Kempe (2011)).",
      "startOffset" : 110,
      "endOffset" : 183
    }, {
      "referenceID" : 28,
      "context" : "Curvature of Non-submodular Functions and Relation to Our Results Sviridenko et al. (2013) present a new notion of curvature for monotone set functions.",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 28,
      "context" : "Taking α = 1 in (3), the approximation guarantee of Sviridenko et al. (2013) is 0.",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 28,
      "context" : "- Our combinations of (generalized) curvature and submodularity ratio gives tight approximation guarantees for GREEDY, and this combination is more expressive than the curvature by Sviridenko et al. (2013), as shown in Remark 3.",
      "startOffset" : 181,
      "endOffset" : 206
    }, {
      "referenceID" : 21,
      "context" : "5) and Krause et al. (2008). To make this work self-contained, we present the details here.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : "According to Krause et al. (2008), after solving the (SDP) problem we sort the entries of λ in descending order, and select the largest K coordinates as the indices of the K elements to be selected.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "11 in Borodin et al. (2014). For argument b): Let us take a minimum cardinality function with k = 2, i.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "11 in Borodin et al. (2014). For argument b): Let us take a minimum cardinality function with k = 2, i.e., F (S) = B > 0 iff. |S| ≥ 2, otherwise F (S) = 0. According to Proposition 3.5 in Borodin et al. (2014), it is weakly submodular, but it is easy to see that its submodualrity ratio is 0.",
      "startOffset" : 6,
      "endOffset" : 210
    }, {
      "referenceID" : 27,
      "context" : "Optimal Budget Allocation with Combinatorial Constraints Optimal budget allocation (Soma et al., 2014) is a special case of the influence maximization problem, it aims to distribute the budget (e.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "Bian et al. (2017) present continuous DR-submodular objectives to model this problem with continuous assignments.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "For one of its possible forms, one can refer to Bian et al. (2017). The GREEDY algorithm solves this problem by maximizing the following auxiliary set function F (S) while respecting the combinatorial constraints,",
      "startOffset" : 48,
      "endOffset" : 67
    } ],
    "year" : 2017,
    "abstractText" : "We investigate the performance of the standard GREEDY algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of GREEDY for maximizing submodular functions, there are few guarantees for non-submodular ones. However, GREEDY enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by a combination of the (generalized) curvature α and the submodularity ratio γ. In particular, we prove that GREEDY enjoys a tight approximation guarantee of 1 α (1− e −γα) for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian Aoptimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and real-world applications.",
    "creator" : "LaTeX with hyperref package"
  }
}