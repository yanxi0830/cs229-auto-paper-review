{
  "name" : "1511.06049.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "What Objective Does Self-paced Learning Indeed Optimize?",
    "authors" : [ "Deyu Meng", "Qian Zhao" ],
    "emails" : [ "dymeng@mail.xjtu.edu.cn", "timmy.zhaoqian@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Since being raised, curriculum learning (CL) [1] and self-paced learning (SPL) [2] have been attracting increasing attention in machine learning, computer vision and multimedia analysis circles. The philosophy under this paradigm is to simulate a learning scheme as the learning principle of humans/animals, which generally starts by learning easier aspects of an learning task, and then gradually takes more complex examples into training [3]. Instead of heuristically designing a curriculum by ranking samples based on manually preset easiness measurements as CL presented [4, 5], the SPL research further attempted to formulate this ad-hoc idea as a concise SPL model through introducing a regularization term into the learning objective. Such amelioration guides a sound SPL regime to automatically optimize an appropriate curriculum by the model itself, which makes the model generalize well to diverse applications and avoid subjective “easiness” measure setting problem [6, 7, 8, 9].\nAlbeit rational in intuition and effective in experience, there is still few research to learn the insightful mechanism inside SPL. Specifically, even though it is easy to prove that the SPL regime is convergent by adopting an alternative search strategy (ASS) on the SPL model, it is still unknown where this SPL iteration converges to. There even does not exist a strict theoretical evidence to clarify why SPL is capable of performing robust especially in outlier/heavy noise cases. Such in-depth investigations, however, are extremely necessary for future developments of SPL, and will illuminate whether the SPL methodology would be dismissed as a ungrounded nine-day wonder, or is a rigorous and solid scientific research field worthy to be further explored.\nar X\niv :1\n51 1.\n06 04\n9v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\nThis study aims at providing some theoretical evidences to illuminate the insight under SPL. Our main results can be summarized as the following three-fold points:\nFirstly, we prove that the ASS algorithm commonly utilized to solve the SPL problem exactly accords with the widely known majorization minimization (MM) [10] algorithm implemented on a latent SPL objective function. In the recent decade, the research topic on MM has attracted much attention in machine learning and optimization, and many elegant theoretical results on it have been brought forward. Such a bridge is thus helpful for analyzing the property underlying the SPL solving strategy, like convergence and stability, by the aid of these MM knowledge.\nSecondly, we prove that the loss function contained in this latent SPL objective has a close relationship with non-convex regularized penalty (NCRP), an attractive research branch in statistics and machine learning. Specifically, we discover that multiple current SPL realizations exactly comply with some well known NCRP terms, e.g., the hard and linear SPL regimes are equivalent to the optimizations on losses with the capped-norm penalty [11, 12, 13] and minimax concave plus penalty (MCP) [14], respectively. Such equivalence on one hand facilitates a deeper comprehension of SPL by employing the off-the-shelf theoretical tools and statistical results in NCRP research, and on the other hand provides more choices for formulating NCPR terms and a new viewpoint to reexamine this advanced research direction.\nThirdly, by understanding the SPL optimization as a NCPR loss minimization problem, we provide an easy explanation on why SPL is able to perform robust in the presence of extreme outliers or heavy noises, and accordingly illustrate new insightful understandings on the intrinsic working mechanism for the previously utilized SPL regime. A rational termination criterion for age parameter control in SPL iteration can also be deduced accordingly.\nThis paper is organized as follows. Section 2 introduces related work on this research. Section 3 presents our main theoretical results, and clarify the relationships between ASS and MM algorithms as well as SPL and NCRP problems. Section 4 shows experimental results to verify these theoretical results. A concluding remark is finally made."
    }, {
      "heading" : "2 Related Work",
      "text" : "Curriculum Learning. Inspired by the intrinsic learning principle of humans/animals, Bengio et al. [1] formalized the fundamental definition of CL. The core idea is to incrementally involve samples into learning, where easy samples are introduced first and more complex ones are gradually included when the learner is ready for them. These gradually included samples from easy to complex correspond to the curriculums learned in different grown-up stages of humans/animals. This strategy, as supported by empirical evaluation, is helpful in alleviating the local optimum problem in nonconvex optimization [15, 16].\nSelf-paced Learning. Instead of using the aforementioned heuristic strategies, Kumar et al. [2] formulated the key principle of CL as a concise SPL model. Formally, given a training dataset D = {(xi, yi)}ni=1, in which xi denotes the ith observed sample, and yi represents its label, let L(yi, g(xi,w)) denote the loss function which calculates the cost between the ground truth label yi and the estimated one g(xi,w). Here w represents the model parameter inside the decision function g. The SPL model includes a weighted loss term on all samples and a general self-paced regularizer imposed on sample weights, expressed as:\nmin w,v∈[0,1]n E(w,v;λ) = n∑ i=1 (viL(yi, f(xi,w)) + f(vi, λ)) , (1)\nwhere λ is the age parameter for controlling the learning pace, and f(v, λ) represents the selfpaced regularizer (SP-regularizer), whose intrinsic conditions have been theoretically abstracted by [17, 18]. By jointly learn the model parameter w and the latent weight v = [v1, · · · , vn]T by ASS with gradually increasing age parameter, more samples can be automatically included into training from easy to complex in a purely self-paced way.\nMultiple variations of this SPL learning regime, like self-paced reranking [17], self-paced learning with diversity [19], self-paced curriculum learning [20] and self-paced multiple-instancelearning [21], have been proposed under the format (1). The effectiveness of this SPL paradigm, especially its robustness in highly corrupted data, has been validated in various machine learning\nand computer vision tasks, such as object detector adaptation [6], specific-class segmentation learning [7], visual category discovery [8], and long-term tracking [9]. Especially, the SPL paradigm has been integrated into the system developed by CMU Informedia team, and achieved the leading performance in challenging TRECVID MED/MER competition organized by NIST in 2014 [22].\nThere is few investigation, however, to theoretically explain the intrinsic effectiveness mechanism under SPL. In this paper, we attempt to take the first step against this issue.\nNon-convex Regularized Penalty. NCRP has been demonstrated to have attractive properties in sparse estimation both theoretically and practically, and attracted much attention in machine learning and statistics in recent years. Various NCRP realizations have also been proposed. Typical ones include capped-norm based penalty [11, 12, 13], minimax concave plus (MCP) penalty [14] and smoothly clipped absolute deviation (SCAD) penalty [23]. The mathematical forms of these NCRP terms in one dimension cases are listed as follows [24]:\nCapped− normpenalty : pCAPγ,λ (t) = γmin(|t|, λ), λ > 0;\nMCP : pMCPγ,λ (t) =\n{ γ(|t| − t 2\n2γλ ), if |t| < γλ γ2λ 2 , if |t| ≥ γλ ;\nSCAD : pSCADγ,λ (t) =  λ|t|, if |t| ≤ λ − t 2−2γλ|t|+λ2 2(γ−1) , if λ < |t| ≤ γλ (γ+1)λ2\n2 , if |t| ≥ γλ .\n(2)\nIn this work, we will prove the close relationship between these NCRP terms and the conventional SPL models.\nMajorization Minimization Algorithm. MM algorithms have wide applications in machine learning and statistical inference [25]. It aims to turn a relatively complicated optimization problem into a tractable one by alternatively iterating an Majorization step and an Minimization step. In particular, consider a minimization problem with objective F (w). Given an estimate wk at the kth iteration, a typical MM algorithm consists of the following two steps:\nMajorization Step: Substitute F (w) by a surrogate function Q(w|wk) such that\nF (w) ≤ Q(w|wk) with equality holding at w = wk.\nMinimization Step: Obtain the next parameter estimate wk+1 by solving the following minimization problem:\nwk+1 = arg min w Q(w|wk).\nIt is easy to see that when the the minimization ofQ(w|wk) is tractable, the MM algorithm can then be very easily implemented, even when the original objective F (w) might be difficult to optimize. Such a solving strategy has also been proven to own many good theoretical properties, like convergence and stability, under certain conditions. In our work, we will clarify that the ASS algorithm generally utilized to solve a SPL model is exactly an MM strategy on a latent SPL objective, and thus is hopeful to inherit some superiority possessed by this known optimization technique."
    }, {
      "heading" : "3 SPL Model and Algorithm Revisit",
      "text" : ""
    }, {
      "heading" : "3.1 Axiomic Definition of SP-regularizer",
      "text" : "By mathematically abstracting the insightful properties underlying a rational SPL regime, [17, 18] presented a formal definition for the SP-ragularizer f(v;λ) involved in the SPL model (1) as follows: Definition 1 (Self-paced regularizer). Suppose that v is a weight variable, ` is the loss, and λ is the age parameter. f(v, λ) is called self-paced regularizer, if\n1. f(v, λ) is convex with respect to v ∈ [0, 1];\n2. v∗(λ; `) is monotonically decreasing with respect to `, and it holds that lim`→0 v∗(λ; `) = 1, lim`→∞ v∗(λ; `) = 0;\n3. v∗(λ; `) is monotonically increasing with respect to λ, and it holds that limλ→∞ v∗(λ; `) ≤ 1, limλ→0 v∗(λ; `) = 0;\nwhere v∗(λ; `) = arg min\nv∈[0,1] v`+ f(v, λ). (3)\nThe three conditions in Definition 1 provide the axiomatic understanding for the SPL. Condition 2 indicates that the model inclines to select easy samples (with smaller losses) in favor of complex samples (with larger losses). Condition 3 states that when the model “age” (controlled by the age parameter λ) gets larger, it tends to incorporate more, probably complex, samples to train a “mature” model. The convexity in Condition 1 further ensures the soundness of this regularizer for optimization.\nUnder this axiomatic definition, multiple SP-regularizers have been constructed. The following lists several typical ones, together with their closed-form solutions v∗(λ; `) as defined in Definition 1:\nfH(v;λ) = −λv; v∗(λ; `) = {\n1, if ` < λ 0, if ` ≥ λ\nfL(v;λ) = λ( 12v 2 − v); v∗(λ; `) =\n{ −`/λ+ 1, if ` < λ\n0, if ` ≥ λ\nfM (v;λ, γ) = γ 2 v+γ/λ ; v ∗(λ, γ; `) =  1, if ` ≤ ( λγ λ+γ )2 0, if ` ≥ λ2 γ (\n1√ ` − 1λ\n) , otherwise.\n(4)\nThe above Eq. (4) represents the hard, linear and mixture SP-regularizers proposed in [2], [17], and [18], respectively. By iteratively updating v and w in the SPL regime (1) with gradually increasing age parameter λ, a rational solution to the problem is expected to be progressively approached."
    }, {
      "heading" : "3.2 Revisit ASS Algorithm for Solving SPL",
      "text" : "For convenience of notions, we briefly write L(yi, g(xi,w)) as `i(w)/`i and L(y, g(x,w)) as `(w)/` in following.\nGiven a SP-regularizer f(v, λ), we can get the integrative function of v∗(λ; `) calculated by Eq. (3) as:\nFλ(`) = ∫ ` 0 v∗(λ; l)dl. (5)\nThe following result can then be proved (the proof is given in the appendix).\nTheorem 1. For v∗(λ; `) conducted by an SP-regularizer and Fλ(`) calculated by (5), given a fixed w∗, it holds that:\nFλ(`(w)) ≤ Qλ(w|w∗) = Fλ(`(w∗)) + v∗(λ; `(w∗))(`(w)−`(w∗)).\nThe theorem verifies that Qλ(w|w∗) represents a tractable surrogate for Fλ(`(w)). Specifically, only considering the terms with respect to w, Qλ(w|w∗) simplifies Fλ, no matter how complicated its format is, as an easy weighted loss form v∗(λ; `(w∗))`(w). This constitutes the fundament of our new understanding on the ASS algorithm on SPL.\nBased on Theorem 1, denote\nQ (i) λ (w|w ∗ ) = Fλ(`i(w ∗)) + v∗(λ; `i(w ∗))(`i(w)−`i(w∗),\nand we can then easily get that: n∑ i=1 Fλ(`i(w)) ≤ n∑ i=1 Q (i) λ (w|w ∗ ). (6)\nThen we can prove the equivalence of the ASS strategy for solving the SPL problem (1) and the MM algorithm for solving ∑n i=1 Fλ(`i(w)) under surrogate function ∑n i=1Q (i) λ (w|w ∗ ) as follows:\nDenote wk as the model parameters in the kth iteration of the ASS implementation on solving SPL, and then its two alternative search steps in the next iteration can be precisely explained as a standard MM scheme:\nMajorization step: To obtain each Q(i)λ (w|w k ), we only need to calculate v∗(λ; `i(wk)) by solving the following problem under the corresponding SP-regularizer f(vi, λ):\nv∗(λ; `i(w k)) = min\nvi∈[0,1] vi`i(w\nk) + f(vi, λ).\nThis exactly complies with updating v in (1) under fixed w.\nMinimization step: we need to calculate:\nwk+1 = arg min w n∑ i=1 Fλ(`i(w k)) + v∗(λ; `i(w k))(`i(w)−`i(wk))\n= arg min w n∑ i=1 v∗(λ; `i(w k))`i(w),\nwhich is exactly equivalent to update w in (1) under fixed v.\nIt is then interesting to see that the commonly utilized ASS strategy in previous SPL regimes is exactly the well known MM algorithm on a minimization problem of the latent SPL objective∑n i=1 Fλ(`i(w)) with the latent SPL loss Fλ(`(w)). Various off-the-shelf theoretical results of MM can then be readily employed to explain the properties of such SPL solving strategy. For example, based on the MM theory, the lower-bounded latent SPL objective is monotonically decreasing during MM/ASS iteration, and the convergence of the SPL algorithm can then be guaranteed.\nThe above theory provides us a new viewpoint to explore the insight of SPL. We thus fairly expect to see what secrets hide under this latent SPL objective/loss."
    }, {
      "heading" : "3.3 Revisit SPL Model",
      "text" : "Now we try to discover more interesting insight from the latent SPL objective. To this aim, we first calculate the latent SPL losses under hard, linear and mixture SP-regularizers, as introduced in (4), by Eq. (5) as follows:\nFHλ (`) = { `, ` < λ, λ, ` ≥ λ;\nFLλ (`) = { `− `2/2λ, ` < λ, λ/2, ` ≥ λ;\nFMλ,γ(`) =  `, ` < 1(1/λ+1/γ)2 , γ(2 √ `− `/λ)− γ(1/λ+1/γ) , 1 (1/λ+1/γ)2 ≤ ` < λ 2,\nγ(λ− 11/λ+1/γ ), ` ≥ λ 2.\n(7)\nThe configurations of these Fλ(`)s under different age parameters are depicted in Fig. 1 for better observation.\nSome common patterns under these latent SPL losses can be easily seen from Fig. 1. E.g., there is an evident suppressing effect of Fλ(`) on large losses as compared with the original loss function `. When ` is larger than a certain threshold, Fλ(`) will become a constant thereafter. This provides a rational explanation on why the SPL regime can perform robust in the presence of extreme outliers or heavy noises: The samples with loss values larger than the age threshold will have no influence to the model training due to their 0 gradients. Corresponding to the original SPL model, these largeloss samples will be with 0 importance weights vi, and thus have no effect on the optimization of model parameters.\nNow, let’s reexamine the intrinsic mechanism inside SPL implementation based on such understanding. At start of SPL iteration, the age λ is small, the latent loss function Fλ(`) has a significant suppressing effect on large losses and only allows small amount of high-confidence samples (with small loss values) into training; then with gradually increasing λ, the supressing effect of Fλ(`) on outliers will gradually become weaker and more relatively less informative samples incline to\nbe involved into training. Through such robust guidance, more and more faithful knowledge under samples tend to be incrementally learned by such learning scheme. Such a gradually changing tendency of the latent SPL loss Fλ(`) can be easily understood by Fig. 1.\nAn interesting observation is that the latent SPL objective Fλ(`) has a close relationship to NCRP widely investigated in machine learning and statistics. E.g., the hard and linear SPL objectives FHλ (`) and F L λ (`) comply exactly with the forms of the capped norm penalty and MCP, as defined in (2), imposed on l by setting γ = 1, respectively. I.e.,\nFHλ (`) = p CAP 1,λ (`), F L λ (`) = p MCP 1,λ (`).\nFurthermore, the form of FMλ,γ(`) is almost similar to the SCAD term, both containing three phases of values, and the first and third of both are linear and constant, respectively. The only difference is in the second phase, where FMλ,γ(`) is of linear+sqrt+constant form while SCAD is of a linear+square+constant one. Actually, it is easy to deduce that any Fλ(`) led by a SP-ragularizer is non-convex, and has a very similar configuration with a general NCRP. Such a natural relationship on one hand provides a new viewpoint to see NCRP and facilitates more choices of NCRP formulations by virtue of Fλ(`) obtained under various SP-regularizers, and on the other hand inspires us to borrow mature statistical tools and theoretical results on NCRP to further understand SPL insight in our future investigation1."
    }, {
      "heading" : "3.4 Age Parameter Tuning",
      "text" : "In the starting stage of SPL iteration, the age value is small, and only small-loss samples can participate into the training process, which naturally conducts the problem of insufficient learning. Yet in the late SPL training stage, the age will become much larger, and then outliers or meaningless noisy samples tend to be involved into learning. Such false information incline to negatively influence the performance of the learned model. Therefore, it is necessary to terminate the SPL iteration at an appropriate intermediate age.\nThe latent SPL objective gradually optimized in the SPL process provides us helpful clues to this problem. Specifically, in SPL implementation, with gradually increasing age parameter, more and more samples are incremented into the learning process, which can be described as the following mapping function:\nγ(n) = λn,\nwhere λn represents the age value where n training samples begin to be involved in the SPL training (i.e., n samples are with nonzero importance weights vi). It is generally observed that in the early training stage, there are a lot of informative samples with small losses, and a small increase of the age parameter will lead to many of them involved into training. This implies that the discrepancy between adjacent λn (i.e., γ(n + 1) − γ(n)) will be small. While in the later stage, noisy samples\n1Albeit closely related, it should be noted that Fλ(`) and NCRP are different in that they are generally imposed on the loss term and the regularization term on model parameters, respectively.\nwith larger losses tend to get into the in-constant domain of Fλ(l). Each such noise involving tends to conduct a relatively larger increase of λn. We thus can rationally select a proper terminate age by easily setting a threshold for γ(n+ 1)− γ(n) or locating where the elbow place of γ(n) is. Such tendency of γ(n) curve can be easily observed in Fig. 2 obtained by our synthetic experiment. It is seen that the elbow location can be easily en-anchored. We thus prefer to utilize this easy strategy for tuning the output age for SPL."
    }, {
      "heading" : "3.5 Discussion on SPL Superiority",
      "text" : "A natural question is that why not directly optimize the latent SPL objective instead of the SPL model, and what is superiority of the latter? Actually, we can easily see that an intrinsic property of SPL is to decompose the minimization of the robust but difficult-to-solve non-convex loss Fλ(`(w)) into two much easier optimization problems with respect to sample importance weights v (solved by the closed-form solution to SP-regularizer) and model parameters w (solved by weighted loss problem). Such decomposition not only simplifies the solving of the problem, but more importantly, makes it easy to embed helpful prior knowledge on the sample importance (easiness) into the loss function of a SPL scheme. Here we list some of such useful sample importance knowledge which can be generally attained before learning:\n1. Spatial/temporal smoothness prior: Some spatially/temporally adjacent samples tend to have relatively similar sample importance;\n2. Partial order prior: We might possibly know some samples are more important (i.e., cleaner, easier, more high-confident) than some others.\n3. Diversity prior: Meaningful samples for the learning task should be scattered across the data range so that the learning can possibly include global-scale data knowledge.\nAttributed to the separation of the sample importance weight v from the original non-convex loss, such prior knowledge can be easily encoded into a SPL scheme. Specifically, Prior 1 can be formulated as a graph Laplacian term vTLv, where L is the Laplacian matrix on the data adjacent matrix; Prior 2 can be easily encoded as supplemental constraint vi > vj if the ith sample is known more important than jth one [20]; and Prior 3 can be realized by a −l2,1 norm or −l0.5,1 norm on v, as utilized in [19] and [21], respectively. Such easy loss-prior embedding capability inclines to guide a sounder learning manner for SPL, which, however, is hard to be integrated by conventional machine learning models with predefined loss functions."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we aim to verify the correctness of the proved theoretical results through synthetic and UCI experiments."
    }, {
      "heading" : "4.1 Synthetic Simulations",
      "text" : "We constructed two synthetic data sets for substantiation. The first is a classification dataset, with 600 points, generated by the following distribution:\n5∑ i=1 πipi(X,Y ), pi(X,Y ) = pi(X)pi(Y |X),\nwith\nπ1 = 0.32; p1(X) = N([−4,−4], 3); p1(1|X) = 0; p1(−1|X) = 1; π2 = 0.32; p2(X) = N([4, 4], 3); p2(1|X) = 1; p1(−1|X) = 0; π3 = 0.1; p3(X) = U(Ω1); p3(1|X) = 0.49; p3(−1|X) = 0.51;\nπ4 = 0.1; p4(X) = U(Ω2); p4(1|X) = 0.51; p4(−1|X) = 0.49; π5 = 0.16; p5(X) = U(Ω3); p1(1|X) = 0.5; p1(−1|X) = 0.5;\nwhere Ω1 = O([−7,−8], 4)∩{[x, y]|y < x},Ω2 = O([6, 8], 4)∩{[x, y]|y > x},Ω3 = {x, y|x, y ∈ [−180, 180]}, O(x0, r) represents a circle area with center x0 and radius r, N(µ, σ2) denotes the Gaussian distribution with mean µ and variance σ2 and U(Ω) represents the uniform distribution on Ω. It is easy to deduce that the optimal classification surface of the problem is y = −x, and p3 and p4 tend to generate noises while p5 inclines to yield outliers to the problem. 600 correctly classified data points were also yielded for performance testing.\nAnother dataset is the regression data, containing 1000 points and generated through the following distribution:\n3∑ i=1 πipi(X,Y ), pi(X,Y ) = p(X)pi(Y |X)\nwith p(X) = U(Ω1); Ω1 = {x|x ∈ [−1, 1]}; π1 = 0.3; p1(Y |X) = N(f(X), 1); π2 = 0.4; p2(Y |X) = Laplacian(f(X), 1); π3 = 0.3; p3(Y |X) = U(Ω3),Ω3 = {ε|ε ∈ [f(X)− 20, f(X) + 20]};\nwhere f(x) = 0.5x + 1 and Laplacian(µ, β) represents the Laplacian distribution with mean µ and scalar β. Note that the optimal regression curve of the problem is f(x), and p1 and p2 tend to generate noises while p3 inclines to yield outliers. 1000 data points were further generated along the this regression curve as the test dataset.\nFor classification experiments, we utilized the logistic regression (LR) [26] and support vector classification (SVC) [27], with log loss and hinge loss as objectives, respectively, as our baseline comparison methods. And for regression, we adopted the least square (LS) regression method [28], with LS loss for comparison. The hard, linear, mixture SPL regimes were respectively employed on these baseline methods to substantiate that if they can ameliorate performance. The age in SPL was tuned by drawing the γ(n) curve and locating the elbow position, as introduced in the last section. For classification and regression, the performance were assessed by the classification accuracy and LS-error on test data, respectively. The codes of LR and LS methods were written by ourselves, and those of SVC and weighted SVC involved in SPL were directly implemented by the “fitcsvm” function in Matlab2014.\nAll experimental results are listed in Table 1. For an easy observation of the working mechanism under SPL, Fig. 2 illustrates some related images on our regression experiment by utilizing the linear SPL regime on least square method.\nIt is easy to see from the figure that the latent SPL objectives under different ages are monotonically decreasing in iteration. This implies the correctness of our SPL theory, i.e., the SPL optimization\nproblem corresponds to the minimization problem on this latent SPL objective. From the table, the effectiveness of the proposed age tuning strategy can also be observed. Specifically, the performance of the original LR/SVC evidently have been hampered by noises/outliers in training data, while SPL regimes can perform stably robust in such cases under selected age values. This can be easily explained by Fig. 2: With the SPL iteration, more informative points can be gradually incremented into the learning process at the beginning, and after an appropriate age, the age threshold will become large and more interferential outliers with large losses tend to be involved into training, and the SPL performance thus tend to be degenerated. By properly terminating iteration at this age, the SPL can effectively avoid such degeneration unexpectation."
    }, {
      "heading" : "4.2 UCI Experiments",
      "text" : "We have further run experiments on various binary classification problems in UCI datasets2. In most cases, an SPL regime can more or less ameliorate the performance of a traditional classification method with fixed loss. In Table 2 we depict some typical results on three of them, which are Monk’s problem 1 (D1), Mammographic Mass (D2) and SPECT Heart (D3) datasets. LR and SVC were adopted as the baseline methods, and hard, linear and mixture SPL regimes were implemented to enhance their robustness.\nFrom the table, it is easy to see the better performance of the SPL regimes against the original LR and SVC in all experiments. Such performance improvement actually implies that all three data sets contain evident noises to a certain extent, and thus the noise suppressing capability of the latent SPL objective takes effect. Such capability, however, is not possessed by the baseline methods since their loss functions are pre-fixed while cannot flexibly adapt varying data distributions in practice as the proposed SPL paradigms."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have provided some new insightful understanding to the conventional SPL regime in this study. On one hand, we have shown that the ASS algorithm generally utilized for solving SPL exactly complies with the known MM algorithm on a latent SPL objective, and on the other hand we have verified that the loss function contained in this latent SPL objective precisely accords with the famous nonconvex regularized penalty (NCRP). The effectiveness, especially its robustness to outliers/heavy noises, of SPL, as substantiated by previous experiences, can then be naturally explained under such understanding. A rational age parameter tuning scheme can be easily conducted through this theory. In our future investigation, we will employ the theories on MM and NCRP to more deeply explore the theoretical/statistical properties underlying the SPL algorithm and model.\n2http://archive.ics.uci.edu/ml/\nAppendix\nProof of Theorem 1. To prove the theorem, we need to show that\nFλ(l) ≤ Fλ(l0) + v∗(λ; l0)(l − l0).\nThere are two cases should be dealt with.\nCase 1. v∗(λ; l) is continous with respect to l.\nFrom Eq. (3), we have that v∗(λ; l) = F ′λ(l). By Definition 1, v∗(λ; l) ≥ 0 when l ≥ 0, and thus F ′λ(l) is nondecreasing with respect to l on [0,∞). Besides, v∗(λ; l) is nonotonically decreasing with respect to l. Therefore, we can conclude that Fλ(l) is concave on [0,∞). Based on the property of concave function, we have\nFλ(l) ≤ Fλ(l0) + F ′λ(l0)(l − l0) = Fλ(l0) + v∗(λ; l0)(l − l0).\nCase 2. v∗(λ; l) is discontinuous with respect to l.\nWith out loss of generality, suppose there is only one discontinuous point l̃ ∈ [0,∞). When l, l0 ∈ [0, l̃) or l, l0 ∈ (l̃,∞), following the similar derivation as Case 1, we also have that\nFλ(l) ≤ Fλ(l0) + v∗(λ; l0)(l − l0)\nholds.\nNow suppose l ∈ [0, l̃) and l0 ∈ (l̃,∞). Pick l1 ∈ [0, l̃), and then we have that\nFλ(l) ≤ Fλ(l1) + v∗(λ; l1)(l − l1),\nand Fλ(l̃) ≤ Fλ(l0) + v∗(λ; l0)(l̃ − l0).\nDenote v∗(λ; l̃)− = liml→l̃− v ∗(λ; l), and let l1 → l̃−. Since Fλ(l) is continuous, we can have that\nFλ(l) ≤ Fλ(l̃) + v∗(λ; l̃)−(l − l̃).\nTherefore,\nFλ(l)− Fλ(l0) = Fλ(l)− Fλ(l̃) + Fλ(l̃)− Fλ(l0) ≤ v∗(λ; l̃)−(l − l̃) + v∗(λ; l0)(l̃ − l0) ≤ v∗(λ; l0)(l − l̃) + v∗(λ; l0)(l̃ − l0) = v∗(λ; l0)(l − l0),\nwhere the second inequality holds due to the fact that l ≤ l̃ and v∗(λ; l) ≥ 0 is decreasing with respect to l.\nSimilarly, if l ∈ (l̃,∞) and l0 ∈ [0, l̃), the result also hods.\nNow we consider the case l0 = l̃. Suppose l ∈ [0, l̃) (derivation is similar for l ∈ (l̃,∞)), and pick l1 ∈ [0, l̃). We have that Fλ(l) ≤ Fλ(l1) + v∗(λ; l1)(l − l1). Let l1 → l̃−. Since Fλ(l) is continuous, we can have that\nFλ(l) ≤ Fλ(l0) + v∗(λ; l̃)−(l − l0) ≤ Fλ(l0) + v∗(λ; l0)(l − l0),\nwhere the second inequality holds due to the fact that l ≤ l0 and v∗(λ; l) ≥ 0 is decreasing with respect to l.\nFrom the above discussion, we can conclude that\nFλ(l) ≤ Fλ(l0) + v∗(λ; l0)(l − l0).\nSubstitute l and l0 with l(w) and l(w∗), respectively, and then Theorem 1 follows."
    } ],
    "references" : [ {
      "title" : "Curriculum learning",
      "author" : [ "Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston" ],
      "venue" : "In ICML,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Self-paced learning for latent variable models",
      "author" : [ "M. Kumar", "B. Packer", "D. Koller" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "How do humans teach: on curriculum learning and teaching dimension",
      "author" : [ "F. Khan", "X. Zhu", "B. Mutlu" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Baby steps: How “less is more” in unsupervised dependency parsing",
      "author" : [ "V.I. Spitkovsky", "H. Alshawi", "D. Jurafsky" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Are all training examples equally valuable",
      "author" : [ "A. Lapedriza", "H. Pirsiavash", "Z. Bylinskii", "A. Torralba" ],
      "venue" : "In CoRR abs/1311.6510,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Shifting weights: Adapting object detectors from image to video",
      "author" : [ "K. Tang", "V. Ramanathan", "F. Li", "D. Koller" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Learning specific-class segmentation from diverse data",
      "author" : [ "M. Kumar", "H. Turki", "D. Preston", "D. Koller" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Learning the easy things first: Self-paced visual category discovery",
      "author" : [ "Y. Lee", "K. Grauman" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Self-paced learning for long-term tracking",
      "author" : [ "J. Supančič III", "D. Ramanan" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Parameter convergence for EM and MM algorithms",
      "author" : [ "F. Vaida" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Analysis of multi-stage convex relaxation for sparse regularization",
      "author" : [ "T. Zhang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "A general theory of concave regularization for high-dimensional sparse estimation problems",
      "author" : [ "C. Zhang", "T. Zhang" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems",
      "author" : [ "P. Gong", "C. Zhang", "Z. Lu", "J. Huang", "J. Ye" ],
      "venue" : "In ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Nearly unbiased variable selection under minimax concave penalty",
      "author" : [ "C. Zhang" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Supervised learning with minimal effort",
      "author" : [ "E. Ni", "C. Ling" ],
      "venue" : "In Advances in Knowledge Discovery and Data Mining,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Teaching classification boundaries to humans",
      "author" : [ "S. Basu", "J. Christensen" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Easy samples first: self-paced reranking for zeroexample multimedia search",
      "author" : [ "L. Jiang", "D. Meng", "T. Mitamura", "A. Hauptmann" ],
      "venue" : "In ACM MM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Self-paced learning for matrix factorization",
      "author" : [ "Q. Zhao", "D.Y. Meng", "L. Jiang", "Q. Xie", "Z.B. Xu", "A. Hauptman" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Self-paced learning with diversity",
      "author" : [ "L. Jiang", "D.Y. Meng", "S. Yu", "Z.Z. Lan", "S.G. Shan", "A. Hauptman" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Self-paced curriculum learning",
      "author" : [ "L. Jiang", "D.Y. Meng", "Q. Zhao", "S.G. Shan", "A. Hauptman" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Co-saliency detection via a self-paced multiple-instance learning framework",
      "author" : [ "D. Zhang", "D. Meng", "J. Han" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "CMU-Informedia@ TRECVID 2014 Multimedia Event Detection (MED)",
      "author" : [ "S. Yu", "L. Jiang", "Z. Mao", "X.J. Chang", "X.Z. Du", "C. Gan", "Z.Z. Lan", "Z.W. Xu", "X.C. Li", "Y. Cai", "A. Kumar", "Y. Miao", "L. Martin", "N. Wolfe", "S.C. Xu", "H. Li", "M. Lin", "Z.G. Ma", "Y. Yang", "D.Y. Meng", "S.G. Shan", "P.D. Sahin", "S. Burger", "F. Metze", "R. Singh", "B. Raj", "T. Mitamura", "R. Stern", "A. Hauptmann" ],
      "venue" : "In TRECVID Video Retrieval Evaluation Workshop,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Variable selection via nonconcave penalized likelihood and its oracle properties",
      "author" : [ "J. Fan", "R. Li" ],
      "venue" : "Journal of American Statistical Association,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2001
    }, {
      "title" : "On the global convergence of majorization minimization algorithms for nonconvex optimization problems",
      "author" : [ "Y. Kang", "Z. Zhang", "W. Li" ],
      "venue" : "In arXiv: 1504.07791v2,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Optimization transfer using surrogate objective functions",
      "author" : [ "K. Lange", "D. Hunter", "I. Yang" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2000
    }, {
      "title" : "The regression analysis of binary sequences (with discussion)",
      "author" : [ "DR Cox" ],
      "venue" : "Journal of the Royal Statistical Society: Series B,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1958
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "Wiley-Interscience, New York,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1998
    }, {
      "title" : "Gauss and the invention of least squares",
      "author" : [ "Stephen M. Stigler" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1981
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Since being raised, curriculum learning (CL) [1] and self-paced learning (SPL) [2] have been attracting increasing attention in machine learning, computer vision and multimedia analysis circles.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Since being raised, curriculum learning (CL) [1] and self-paced learning (SPL) [2] have been attracting increasing attention in machine learning, computer vision and multimedia analysis circles.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "The philosophy under this paradigm is to simulate a learning scheme as the learning principle of humans/animals, which generally starts by learning easier aspects of an learning task, and then gradually takes more complex examples into training [3].",
      "startOffset" : 245,
      "endOffset" : 248
    }, {
      "referenceID" : 3,
      "context" : "Instead of heuristically designing a curriculum by ranking samples based on manually preset easiness measurements as CL presented [4, 5], the SPL research further attempted to formulate this ad-hoc idea as a concise SPL model through introducing a regularization term into the learning objective.",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "Instead of heuristically designing a curriculum by ranking samples based on manually preset easiness measurements as CL presented [4, 5], the SPL research further attempted to formulate this ad-hoc idea as a concise SPL model through introducing a regularization term into the learning objective.",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Such amelioration guides a sound SPL regime to automatically optimize an appropriate curriculum by the model itself, which makes the model generalize well to diverse applications and avoid subjective “easiness” measure setting problem [6, 7, 8, 9].",
      "startOffset" : 235,
      "endOffset" : 247
    }, {
      "referenceID" : 6,
      "context" : "Such amelioration guides a sound SPL regime to automatically optimize an appropriate curriculum by the model itself, which makes the model generalize well to diverse applications and avoid subjective “easiness” measure setting problem [6, 7, 8, 9].",
      "startOffset" : 235,
      "endOffset" : 247
    }, {
      "referenceID" : 7,
      "context" : "Such amelioration guides a sound SPL regime to automatically optimize an appropriate curriculum by the model itself, which makes the model generalize well to diverse applications and avoid subjective “easiness” measure setting problem [6, 7, 8, 9].",
      "startOffset" : 235,
      "endOffset" : 247
    }, {
      "referenceID" : 8,
      "context" : "Such amelioration guides a sound SPL regime to automatically optimize an appropriate curriculum by the model itself, which makes the model generalize well to diverse applications and avoid subjective “easiness” measure setting problem [6, 7, 8, 9].",
      "startOffset" : 235,
      "endOffset" : 247
    }, {
      "referenceID" : 9,
      "context" : "Firstly, we prove that the ASS algorithm commonly utilized to solve the SPL problem exactly accords with the widely known majorization minimization (MM) [10] algorithm implemented on a latent SPL objective function.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : ", the hard and linear SPL regimes are equivalent to the optimizations on losses with the capped-norm penalty [11, 12, 13] and minimax concave plus penalty (MCP) [14], respectively.",
      "startOffset" : 109,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : ", the hard and linear SPL regimes are equivalent to the optimizations on losses with the capped-norm penalty [11, 12, 13] and minimax concave plus penalty (MCP) [14], respectively.",
      "startOffset" : 109,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : ", the hard and linear SPL regimes are equivalent to the optimizations on losses with the capped-norm penalty [11, 12, 13] and minimax concave plus penalty (MCP) [14], respectively.",
      "startOffset" : 109,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : ", the hard and linear SPL regimes are equivalent to the optimizations on losses with the capped-norm penalty [11, 12, 13] and minimax concave plus penalty (MCP) [14], respectively.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "[1] formalized the fundamental definition of CL.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "This strategy, as supported by empirical evaluation, is helpful in alleviating the local optimum problem in nonconvex optimization [15, 16].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "This strategy, as supported by empirical evaluation, is helpful in alleviating the local optimum problem in nonconvex optimization [15, 16].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "[2] formulated the key principle of CL as a concise SPL model.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "min w,v∈[0,1]n E(w,v;λ) = n ∑",
      "startOffset" : 8,
      "endOffset" : 13
    }, {
      "referenceID" : 16,
      "context" : "where λ is the age parameter for controlling the learning pace, and f(v, λ) represents the selfpaced regularizer (SP-regularizer), whose intrinsic conditions have been theoretically abstracted by [17, 18].",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 17,
      "context" : "where λ is the age parameter for controlling the learning pace, and f(v, λ) represents the selfpaced regularizer (SP-regularizer), whose intrinsic conditions have been theoretically abstracted by [17, 18].",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 16,
      "context" : "Multiple variations of this SPL learning regime, like self-paced reranking [17], self-paced learning with diversity [19], self-paced curriculum learning [20] and self-paced multiple-instancelearning [21], have been proposed under the format (1).",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "Multiple variations of this SPL learning regime, like self-paced reranking [17], self-paced learning with diversity [19], self-paced curriculum learning [20] and self-paced multiple-instancelearning [21], have been proposed under the format (1).",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "Multiple variations of this SPL learning regime, like self-paced reranking [17], self-paced learning with diversity [19], self-paced curriculum learning [20] and self-paced multiple-instancelearning [21], have been proposed under the format (1).",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 20,
      "context" : "Multiple variations of this SPL learning regime, like self-paced reranking [17], self-paced learning with diversity [19], self-paced curriculum learning [20] and self-paced multiple-instancelearning [21], have been proposed under the format (1).",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 5,
      "context" : "and computer vision tasks, such as object detector adaptation [6], specific-class segmentation learning [7], visual category discovery [8], and long-term tracking [9].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "and computer vision tasks, such as object detector adaptation [6], specific-class segmentation learning [7], visual category discovery [8], and long-term tracking [9].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "and computer vision tasks, such as object detector adaptation [6], specific-class segmentation learning [7], visual category discovery [8], and long-term tracking [9].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "and computer vision tasks, such as object detector adaptation [6], specific-class segmentation learning [7], visual category discovery [8], and long-term tracking [9].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 21,
      "context" : "Especially, the SPL paradigm has been integrated into the system developed by CMU Informedia team, and achieved the leading performance in challenging TRECVID MED/MER competition organized by NIST in 2014 [22].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 10,
      "context" : "Typical ones include capped-norm based penalty [11, 12, 13], minimax concave plus (MCP) penalty [14] and smoothly clipped absolute deviation (SCAD) penalty [23].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "Typical ones include capped-norm based penalty [11, 12, 13], minimax concave plus (MCP) penalty [14] and smoothly clipped absolute deviation (SCAD) penalty [23].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "Typical ones include capped-norm based penalty [11, 12, 13], minimax concave plus (MCP) penalty [14] and smoothly clipped absolute deviation (SCAD) penalty [23].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "Typical ones include capped-norm based penalty [11, 12, 13], minimax concave plus (MCP) penalty [14] and smoothly clipped absolute deviation (SCAD) penalty [23].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "Typical ones include capped-norm based penalty [11, 12, 13], minimax concave plus (MCP) penalty [14] and smoothly clipped absolute deviation (SCAD) penalty [23].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 23,
      "context" : "The mathematical forms of these NCRP terms in one dimension cases are listed as follows [24]: Capped− normpenalty : p γ,λ (t) = γmin(|t|, λ), λ > 0; MCP : p γ,λ (t) = { γ(|t| − t 2 2γλ ), if |t| < γλ γλ 2 , if |t| ≥ γλ ;",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "MM algorithms have wide applications in machine learning and statistical inference [25].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "By mathematically abstracting the insightful properties underlying a rational SPL regime, [17, 18] presented a formal definition for the SP-ragularizer f(v;λ) involved in the SPL model (1) as follows: Definition 1 (Self-paced regularizer).",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "By mathematically abstracting the insightful properties underlying a rational SPL regime, [17, 18] presented a formal definition for the SP-ragularizer f(v;λ) involved in the SPL model (1) as follows: Definition 1 (Self-paced regularizer).",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "f(v, λ) is convex with respect to v ∈ [0, 1]; 2.",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "where v∗(λ; `) = arg min v∈[0,1] v`+ f(v, λ).",
      "startOffset" : 27,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "(4) represents the hard, linear and mixture SP-regularizers proposed in [2], [17], and [18], respectively.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "(4) represents the hard, linear and mixture SP-regularizers proposed in [2], [17], and [18], respectively.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "(4) represents the hard, linear and mixture SP-regularizers proposed in [2], [17], and [18], respectively.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Denote w as the model parameters in the k iteration of the ASS implementation on solving SPL, and then its two alternative search steps in the next iteration can be precisely explained as a standard MM scheme: Majorization step: To obtain each Q λ (w|w k ), we only need to calculate v∗(λ; `i(w)) by solving the following problem under the corresponding SP-regularizer f(vi, λ): v∗(λ; `i(w )) = min vi∈[0,1] vi`i(w ) + f(vi, λ).",
      "startOffset" : 402,
      "endOffset" : 407
    }, {
      "referenceID" : 19,
      "context" : "Specifically, Prior 1 can be formulated as a graph Laplacian term vLv, where L is the Laplacian matrix on the data adjacent matrix; Prior 2 can be easily encoded as supplemental constraint vi > vj if the i sample is known more important than j one [20]; and Prior 3 can be realized by a −l2,1 norm or −l0.",
      "startOffset" : 248,
      "endOffset" : 252
    }, {
      "referenceID" : 18,
      "context" : "5,1 norm on v, as utilized in [19] and [21], respectively.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "5,1 norm on v, as utilized in [19] and [21], respectively.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "32; p2(X) = N([4, 4], 3); p2(1|X) = 1; p1(−1|X) = 0; π3 = 0.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "32; p2(X) = N([4, 4], 3); p2(1|X) = 1; p1(−1|X) = 0; π3 = 0.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "where Ω1 = O([−7,−8], 4)∩{[x, y]|y < x},Ω2 = O([6, 8], 4)∩{[x, y]|y > x},Ω3 = {x, y|x, y ∈ [−180, 180]}, O(x0, r) represents a circle area with center x0 and radius r, N(μ, σ) denotes the Gaussian distribution with mean μ and variance σ and U(Ω) represents the uniform distribution on Ω.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "where Ω1 = O([−7,−8], 4)∩{[x, y]|y < x},Ω2 = O([6, 8], 4)∩{[x, y]|y > x},Ω3 = {x, y|x, y ∈ [−180, 180]}, O(x0, r) represents a circle area with center x0 and radius r, N(μ, σ) denotes the Gaussian distribution with mean μ and variance σ and U(Ω) represents the uniform distribution on Ω.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "For classification experiments, we utilized the logistic regression (LR) [26] and support vector classification (SVC) [27], with log loss and hinge loss as objectives, respectively, as our baseline comparison methods.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : "For classification experiments, we utilized the logistic regression (LR) [26] and support vector classification (SVC) [27], with log loss and hinge loss as objectives, respectively, as our baseline comparison methods.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 27,
      "context" : "And for regression, we adopted the least square (LS) regression method [28], with LS loss for comparison.",
      "startOffset" : 71,
      "endOffset" : 75
    } ],
    "year" : 2015,
    "abstractText" : "Self-paced learning (SPL) has been attracting increasing attention in machine learning and computer vision. Albeit empirically substantiated to be effective, the investigation on its theoretical insight is still a blank. It is even unknown that what objective a general SPL regime converges to. To this issue, this study attempts to initially provide some new insights under this “heuristic” learning scheme. Specifically, we prove that the solving strategy on SPL exactly accords with a majorization minimization algorithm, a well known technique in optimization and machine learning, implemented on a latent objective. A more interesting finding is that, the loss function contained in this latent objective has a similar configuration with non-convex regularized penalty, an attractive topic in statistics and machine learning. In particular, we show that the previous hard and linear self-paced regularizers are equivalent to the capped norm and minimax concave plus penalties, respectively, both being widely investigated in statistics. Such connections between SPL and previous known researches enhance new insightful comprehension on SPL, like convergence and parameter setting rationality. The correctness of the proposed theory is substantiated by experimental results on synthetic and UCI data sets.",
    "creator" : "LaTeX with hyperref package"
  }
}