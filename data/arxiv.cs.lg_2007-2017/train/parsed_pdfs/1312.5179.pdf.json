{
  "name" : "1312.5179.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited",
    "authors" : [ "Matthias Hein", "Simon Setzer", "Leonardo Jost" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Graph-based learning is by now well established in machine learning and is the standard way to deal with data that encode pairwise relationships. Hypergraphs are a natural extension of graphs which allow to model also higher-order relations in data. It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.\nCurrent approaches in hypergraph-based learning can be divided into two categories. The first one uses tensor methods for clustering as the higher-order extension of matrix (spectral) methods for graphs [7, 8, 9]. While tensor methods are mathematically quite appealing, they are limited to socalled k-uniform hypergraphs, that is, each hyperedge contains exactly k vertices. Thus, they are not able to model mixed higher-order relationships. The second main approach can deal with arbitrary hypergraphs [10, 11]. The basic idea of this line of work is to approximate the hypergraph via a standard weighted graph. In a second step, one then uses methods developed for graph-based clustering and semi-supervised learning. The two main ways of approximating the hypergraph by a standard graph are the clique and the star expansion which were compared in [12]. One can summarize [12] by stating that no approximation fully encodes the hypergraph structure. Earlier, [13] have proven that an exact representation of the hypergraph via a graph retaining its cut properties is impossible.\nIn this paper, we overcome the limitations of both existing approaches. For both clustering and semisupervised learning the key element, either explicitly or implicitly, is the cut functional. Our aim is to directly work with the cut defined on the hypergraph. We discuss in detail the differences of the hypergraph cut and the cut induced by the clique and star expansion in Section 2.1. Then, in Section 2.2, we introduce the total variation on a hypergraph as the Lovasz extension of the hypergraph cut. Based on this, we propose a family of regularization functionals which interpolate between the total variation and a regularization functional enforcing smoother functions on the hypergraph corresponding to Laplacian-type regularization on graphs. They are the key for the semi-supervised learning method introduced in Section 3. In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut. In both learning problems, convex optimization problems have to be solved for which we derive scalable methods in Section 5. The main ingredients of these algorithms are proximal mappings for which we provide a novel algorithm and analyze its complexity. In the experimental section 6, we show that fully incorporating hypergraph structure is beneficial. All proofs are moved to the supplementary material.\nar X\niv :1\n31 2.\n51 79\nv1 [\nst at\n.M L\n] 1\n8 D\nec 2"
    }, {
      "heading" : "2 The Total Variation on Hypergraphs",
      "text" : "A large class of graph-based algorithms in semi-supervised learning and clustering is based either explicitly or implicitly on the cut. Thus, we discuss first in Section 2.1 the hypergraph cut and the corresponding approximations.In Section 2.2, we introduce in analogy to graphs, the total variation on hypergraphs as the Lovasz extension of the hypergraph cut."
    }, {
      "heading" : "2.1 Hypergraphs, Graphs and Cuts",
      "text" : "Hypergraphs allow modeling relations which are not only pairwise as in graphs but involve multiple vertices. In this paper, we consider weighted undirected hypergraphs H = (V,E,w) where V is the vertex set with |V | = n and E the set of hyperedges with |E| = m. Each hyperedge e ∈ E corresponds to a subset of vertices, i.e., to an element of 2V . The vector w ∈ Rm contains for each hyperedge e its non-negative weight we. In the following, we use the letter H also for the\nincidence matrix H ∈ R|V |×|E| which is for i ∈ V and e ∈ E, Hi,e = {\n1 if i ∈ e, 0 else. . The degree\nof a vertex i ∈ V is defined as di = ∑ e∈E weHi,e and the cardinality of an edge e can be written\nas |e| = ∑ j∈V Hj,e. We would like to emphasize that we do not impose the restriction that the hypergraph is k-uniform, i.e., that each hyperedge contains exactly k vertices.\nThe considered class of hypergraphs contains the set of undirected, weighted graphs which is equivalent to the set of 2-uniform hypergraphs. The motivation for the total variation on hypergraphs comes from the correspondence between the cut on a graph and the total variation functional. Thus, we recall the definition of the cut on weighted graphs G = (V,W ) with weight matrix W . Let C = V \\C denote the complement of C in V . Then, for a partition (C,C), the cut is defined as\ncutG(C,C) = ∑\ni,j : i∈C,j∈C wij .\nThis standard definition of the cut carries over naturally to a hypergraph H cutH(C,C) = ∑\ne∈E: e∩C 6=∅, e∩C 6=∅\nwe. (1)\nThus, the cut functional on a hypergraph is just the sum of the weights of the hyperedges which have vertices both in C and C. It is not biased towards a particular way the hyperedge is cut, that is, how many vertices of the hyperedge are in C resp. C. This emphasizes that the vertices in a hyperedge belong together and we penalize every cut of a hyperedge with the same value.\nIn order to handle hypergraphs with existing methods developed for graphs, the focus in previous works [11, 12] has been on transforming the hypergraph into a graph. In [11], they suggest using the clique expansion (CE), i.e., every hyperedge e ∈ H is replaced with a fully connected subgraph where every edge in this subgraph has weight we|e| . This leads to the cut functional cutCE ,\ncutCE(C,C) := ∑\ne∈E: e∩C 6=∅, e∩C 6=∅ we |e| |e ∩ C| |e ∩ C|. (2)\nNote that in contrast to the hypergraph cut (1), the value of cutCE depends on the way each hyperedge is cut since the term |e∩C| |e∩C|makes the weights dependent on the partition. In particular, the smallest weight is attained if only a single vertex is split off, whereas the largest weight is attained if the partition of the hyperedge is most balanced. In comparison to the hypergraph cut, this leads to a bias towards cuts that favor splitting off single vertices from a hyperedge which in our point of view is an undesired property for most applications. We illustrate this with an example in Figure 1, where the minimum hypergraph cut (cutH ) leads to a balanced partition, whereas the minimum clique expansion cut (cutCE) not only cuts an additional hyperedge but is also unbalanced. This is due to its bias towards splitting off single nodes of a hyperedge. Another argument against the clique expansion is computational complexity. For large hyperedges the clique expansion leads to (almost) fully connected graphs which makes computations slow and is prohibitive for large hypergraphs.\nWe omit the discussion of the star graph approximation of hypergraphs discussed in [12] as it is shown there that the star graph expansion is very similar to the clique expansion. Instead, we want to recall the result of Ihler et al. [13] which states that in general there exists no graph with the same vertex set V which has for every partition (C,C) the same cut value as the hypergraph cut.\nFinally, note that for weighted 3-uniform hypergraphs it is always possible to find a corresponding graph such that any cut of the graph is equal to the corresponding cut of the hypergraph.\nProposition 2.1. Suppose H = (V,E,w) is a weighted 3-uniform hypergraph. Then, W ∈ R|V |×|V | defined as W = 12Hdiag(w)H\nT defines the weight matrix of a graph G = (V,W ) where each cut of G has the same value as the corresponding hypergraph cut of H .\nProof. The cut value of a partition (C,C) of G is given as\ncutG(C,C) = 1\n2 ∑ e∈E |e ∩ C||e ∩ C|we.\nThe product |e ∩ C||e ∩ C| takes the values 2 if e is cut by C and zero otherwise. Because of the factor 12 , we thus get equivalence to the hypergraph cut."
    }, {
      "heading" : "2.2 The Total Variation on Hypergraphs",
      "text" : "In this section, we define the total variation on hypergraphs. The key technical element is the Lovasz extension which extends a set function, seen as a mapping on 2V , to a function on R|V |.\nDefinition 2.1. Let Ŝ : 2V → R be a set function with Ŝ(∅) = 0. Let f ∈ R|V |, let V be ordered such that f1 ≤ f2 ≤ . . . ≤ fn and define Ci = {j ∈ V | j > i}. Then, the Lovasz extension S : R|V | → R of Ŝ is given by\nS(f) = n∑ i=1 fi ( Ŝ(Ci−1)− Ŝ(Ci) ) = n−1∑ i=1 Ŝ(Ci)(fi+1 − fi) + f1Ŝ(V ).\nNote that for the characteristic function of a set C ⊂ V , we have S(1C) = Ŝ(C).\nIt is well-known that the Lovasz extension S is a convex function if and only if Ŝ is submodular [18]. For graphs G = (V,W ), the total variation on graphs is defined as the Lovasz extension of the graph cut [18] given as TVG : R|V | → R, TVG(f) = 12 ∑n i,j=1 wij |fi − fj |.\nProposition 2.2. The total variation TVH : R|V | → R on a hypergraph H = (V,E,w) defined as the Lovasz extension of the hypergraph cut, Ŝ(C) = cutH(C,C), is a convex function given by\nTVH(f) = ∑ e∈E we ( max i∈e fi −min j∈e fj ) = ∑ e∈E we max i,j∈e |fi − fj |.\nProof. Using Ci−1 = Ci ∪ {i} and Ci = Ci−1 ∪ {i} the Lovasz extension can be written as\nTVH(f) = n∑ i=1 fi ( cut(Ci−1, Ci−1)− cut(Ci, Ci) ) = n∑ i=1 fi ( cut({i}, Ci−1)− cut(Ci, {i}) ) =\nn∑ i=1 fi ( ∑ e∈E,i∈e\ne∩{1,...,i−1}6=∅\nwe − ∑\ne∈E,i∈e e∩{i+1,...,n}6=∅\nwe ) = ∑ e∈E we ( max i∈e fi −min j∈e fj ) .\nIt is easy to see that the Lovasz extension of the hypergraph cut is a convex function. Since the maximum of convex functions is convex, −mini∈e fi = maxi∈e fi and the hyperedge weights are non-negative, we have a non-negative combination of convex functions which is convex. Alternatively, one could use that the hypergraph cut is submodular and the Lovasz extension of every submodular set function is convex.\nNote that the total variation of a hypergraph cut reduces to the total variation on graphs if H is 2-uniform (standard graph). There is an interesting relation of the total variation on hypergraphs to sparsity inducing group norms. Namely, defining for each edge e ∈ E the difference operator De : R|V | → R|V |×|V | by (Def)ij = fi − fj if i, j ∈ e and 0 otherwise, TVH can be written as, TVH(f) = ∑ e∈E we ‖Def‖∞, which can be seen as inducing group sparse structure on the gradient level. The groups are the hyperedges and thus are typically overlapping. This could lead potentially to extensions of the elastic net on graphs to hypergraphs.\nIt is known that using the total variation on graphs as a regularization functional in semi-supervised learning (SSL) leads to very spiky solutions for small numbers of labeled points. Thus, one would like to have regularization functionals enforcing more smoothness of the solutions. For graphs this is achieved by using the family of regularization functionals ΩG,p : R|V | → R,\nΩG,p(f) = 1\n2 n∑ i,j=1 wij |fi − fj |p.\nFor p = 2 we get the regularization functional of the graph Laplacian which is the basis of a large class of methods on graphs. In analogy to graphs, we define a corresponding family on hypergraphs.\nDefinition 2.2. The regularization functionals ΩH,p : R|V | → R for a hypergraph H = (V,E,w) are defined for p ≥ 1 as\nΩH,p(f) = ∑ e∈E we ( max i∈e fi −min j∈e fj )p .\nLemma 2.1. The functionals ΩH,p : R|V | → R are convex.\nProof. The p-th power of positive, convex functions for p ≥ 1 is convex as( f(λx+ (1− λ)y) )p ≤ (λf(x) + (1− λ)f(y))p ≤ λf(x)p + (1− λ)f(y)p where the last inequality follows from the convexity of xp on R+. Thus, the p-th power of max\ni∈e fi−\nmin j∈e fj is convex.\nNote that ΩH,1(f) = TVH(f). IfH is a graph and p ≥ 1, ΩH,p reduces to the Laplacian regularization ΩG,p. Note that for characteristic functions of sets, f = 1C , it holds ΩH,p(1C) = cutH(C,C). Thus, the difference between the hypergraph cut and its approximations such as clique and star expansion carries over to ΩH,p and ΩGCE ,p, respectively."
    }, {
      "heading" : "3 Semi-supervised Learning",
      "text" : "With the regularization functionals derived in the last section, we can immediately write down a formulation for two-class semi-supervised learning on hypergraphs similar to the well-known approaches of [19, 20]. Given the label set L we construct the vector Y ∈ Rn with Yi = 0 if i /∈ L\nand Yi equal to the label in {−1, 1} if i ∈ L. We propose solving\nf∗ = arg min f∈R|V |\n1 2 ‖f − Y ‖22 + λΩH,p(f), (3)\nwhere λ > 0 is the regularization parameter. In Section 5, we discuss how this convex optimization problem can be solved efficiently for the case p = 1 and p = 2. Note, that other loss functions than the squared loss could be used. However, the regularizer aims at contracting the function and we use the label set {−1, 1} so that f∗ ∈ [−1, 1]|V |. Hence, on the interval [−1, 1] the squared loss behaves very similar to other margin-based loss functions. In general, we recommend using p = 2 as it corresponds to Laplacian-type regularization for graphs which is known to work well. For graphs p = 1 is known to produce spiky solutions for small numbers of labeled points. This is due to the effect that cutting “out” the labeled points leads to a much smaller cut than, e.g., producing a balanced partition. However, in the case where one has only a small number of hyperedges this effect is much smaller and we will see in the experiments that p = 1 also leads to reasonable solutions."
    }, {
      "heading" : "4 Balanced Hypergraph Cuts",
      "text" : "In Section 2.1, we discussed the difference between the hypergraph cut (1) and the graph cut of the clique expansion (2) of the hypergraph and gave a simple example in Figure 1 where these cuts yield quite different results. Clearly, this difference carries over to the famous normalized cut criterion introduced in [21, 22] for clustering of graphs with applications in image segmentation. For a hypergraph the ratio resp. normalized cut can be formulated as\nRCut(C,C) = cutH(C,C)\n|C||C| , NCut(C,C) =\ncutH(C,C)\nvol(C) vol(C) ,\nwhich incorporate different balancing criteria. Note, that in contrast to the normalized cut for graphs the normalized hypergraph cut allows no relaxation into a linear eigenproblem (spectral relaxation).\nThus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem. Although nonlinear eigenproblems are non-convex, one can compute nonlinear eigenvectors quite efficiently at the price of loosing global optimality. However, it has been shown that the potentially non-optimal solutions of the exact relaxation, outperform in practice the globally optimal solution of the loose relaxation, often by large margin. In this section, we extend their approach to hypergraphs and consider general balanced hypergraph cuts Bcut(C,C) of the form, Bcut(C,C) = cutH(C,C)\nŜ(C) , where Ŝ : 2V → R+\nis a non-negative, symmetric set function (that is Ŝ(C) = Ŝ(C)). For the normalized cut one has Ŝ(C) = vol(C) vol(C) whereas for the Cheeger cut one has Ŝ(C) = min{volC, volC}. Other examples of balancing functions can be found in [16]. Our following result shows that the balanced hypergraph cut also has an exact relaxation into a continuous nonlinear eigenproblem [14].\nTheorem 4.1. LetH = (V,E,w) be a finite, weighted hypergraph and S : R|V | → R be the Lovasz extension of the symmetric, non-negative set function Ŝ : 2V → R. Then, it holds that\nmin f∈R|V |\n∑ e∈E we ( max i∈e fi −min j∈e fj )\nS(f) = min C⊂V\ncutH(C,C)\nŜ(C) .\nFurther, let f ∈ R|V | and define Ct := {i ∈ V | fi > t}. Then,\nmin t∈R\ncutH(Ct, Ct)\nŜ(Ct) ≤\n∑ e∈E we ( max i∈e fi −min j∈e fj )\nS(f) .\nProof. By Prop. 2.2 the Lovasz extension of cutH(C,C) is given by ∑ e∈E we ( max i∈e fi −min j∈e fj ) .\nNoting that both cutH(C,C) and Ŝ(C) vanish on the full set V , the proof then follows from the recent result [17], which shows in this case the equivalence between the set problem and the continuous problem written in terms of the Lovasz extensions.\nThe last part of the theorem shows that “optimal thresholding” (turning f ∈ RV into a partition) among all level sets of any f ∈ R|V | can only lead to a better or equal balanced hypergraph cut.\nThe question remains how to minimize the ratio Q(f) = TVH(f)S(f) . As discussed in [16], every Lovasz extension S can be written as a difference of convex positively 1-homogeneous functions1 S = S1 − S2. Moreover, as shown in Prop. 2.2 the total variation TVH is convex. Thus, we have to minimize a non-negative ratio of a convex and a difference of convex (d.c.) function. We employ the RatioDCA algorithm [16] shown in Algorithm 1. The main part is the convex inner problem. In\nAlgorithm 1 RatioDCA – Minimization of a non-negative ratio of 1-homogeneous d.c. functions\n1: Objective: Q(f) = R1(f)−R2(f)S1(f)−S2(f) . Initialization: f 0 = random with ∥∥f0∥∥ = 1, λ0 = Q(f0) 2: repeat 3: s1(fk) ∈ ∂S1(fk), r2(fk) ∈ ∂R2(fk) 4: fk+1 = arg min\n‖u‖2≤1\n{ R1(u)− 〈 u, r2(f k) 〉 + λk ( S2(u)− 〈 u, s1(f k) 〉 )}\n5: λk+1 = (R1(fk+1)−R2(fk+1))/(S1(fk+1)− S2(fk+1)) 6: until |λ\nk+1−λk| λk <\n7: Output: eigenvalue λk+1 and eigenvector fk+1.\nour case R1 = TVH , R2 = 0, and thus the inner problem reads min‖u‖2≤1{TVH(u) + λ k ( S2(u)− 〈 u, s1(f k) 〉 ) }. (4)\nFor simplicity we restrict ourselves to submodular balancing functions, in which case S is convex and thus S2 = 0. For the general case, see [16]. Note that the balancing functions of ratio/normalized cut and Cheeger cut are submodular. It turns out that the inner problem is very similar to the semisupervised learning formulation (3). The efficient solution of both problems is discussed next."
    }, {
      "heading" : "5 Algorithms for the Total Variation on Hypergraphs",
      "text" : "The problem (3) we want to solve for semi-supervised learning and the inner problem (4) of RatioDCA have a common structure. They are the sum of convex functionals where one of them is the novel regularizer ΩH,p. We propose to solve these problems using a primal-dual algorithm, denoted PDHG in this paper, which was proposed in [23, 24]. Its main idea is to iteratively solve for each convex term in the objective function a so-called proximal problem. Solving the proximal problem w.r.t. a mapping g : Rn → R and a vector x̃ ∈ Rn means to compute the proximal map proxg defined by\nproxg(x̃) = arg min x∈Rn {1 2 ‖x− x̃‖22 + g(x)}.\nThe main idea here is that often these proximal problems can be solved efficiently leading to a fast convergence of the overall algorithm. In order to point out the common structure of PDHG for both (3) and the inner problems of Algorithm 1, we first consider a general optimization problem of the form minf∈Rn{G(f) + F (Kf)}, (5) where K ∈ Rm,n and G : Rn → R, F : Rm → R are lower-semicontinuous convex functions. Recall that the conjugate function of G∗ of G is defined as\nG∗(x) = sup f∈Rn {〈x, f〉 −G(f)}\nand similarly for F ∗. In terms of these conjugate functions, we can write the dual problem of (5) as\n−minα∈Rm{G∗(−KTα) + F ∗(α)}. (6)\nThe PDHG algorithm for (5) has the following general form. For convergence proofs we refer to [23, 24].\n1A function f : Rd → R is positively 1-homogeneous if ∀α > 0, f(αx) = αf(x).\nAlgorithm 2 PDHG\n1: Initialization: f (0) = f̄ (0) = 0, θ ∈ [0, 1], σ, τ > 0 with στ < 1/‖K‖22 2: repeat 3: α(k) = proxσF∗(α\n(k) + σKf̄ (k)) 4: f (k+1) = proxτG(f)(f\n(k) − τKT(α(k))) 5: f̄ (k+1) = f (k+1) + θ(f (k+1) − f (k)) 6: until relative duality gap < 7: Output: f (k+1).\nWe will now apply this general setting to the convex optimization problems arising in this paper. First, the following Table 1 shows how one can chooseG in (5) in order to solve (3) and (4), provides the solutions of the corresponding proximal problems, and gives the conjugate functions. However, note that smooth convex terms can also be directly exploited [25]. Note that we write the constraint in the inner problem of RatioDCA via the indicator function ι‖·‖2≤1 defined by ι‖·‖2≤1(x) = 0, if ‖x‖2 ≤ 1 and +∞ otherwise. Clearly, both proximal problems have an explicit solution.\nSecond, we discuss the choice of F and K to incorporate ΩH,p.\nPDHG algorithm for ΩH,1. Let me denote the number of vertices in hyperedge e ∈ E. The main idea is to write\nλΩH,1(f) = F (Kf) := ∑ e∈E (F(e,1)(Kef) + F(e,2)(Kef)), (7)\nwhere the rows of the matrices Ke ∈ Rme,n are the i-th standard unit vectors for i ∈ e and the functionals F(e,j) : Rme → R are defined as\nF(e,1)(α (e,1)) = λwe max(α (e,1)), F(e,2)(α (e,2)) = −λwe min(α(e,2)).\nThe primal problem has thus the form minf∈Rn{G(f) + ∑ e∈E (F(e,1)(Kef) + F(e,2)(Kef))}.\nIn contrast to the function G, we need in the PDHG algorithm the proximal maps for the conjugate functions of F(e,j). They are given by\nF ∗(e,1) = ιSλwe , F ∗ (e,2) = ι−Sλwe , where Sλwe = {x ∈ Rme : ∑me i=1 xi = λwe, xi ≥ 0} is the scaled simplex in Rme . By (6) the dual problem has the form\n−minα(e,1),α(e,2){G∗(− ∑ e∈E KTe(α (e,1) + α(e,2))) + ∑ e∈E (ιSeλwe (α (e,1)) + ι−Seλwe (α (e,2)))},\nwhere G∗ is given as in Table 1. The solutions of the proximal problems for F ∗(e,1) and F ∗ (e,1) are the orthogonal projections onto these simplexes written here as PSeλwe and P−Seλwe , respectively. These projections can be performed in linear time, cf., [26]. Using the proximal mappings we have presented so far, we obtain Algorithm 3. In line 1, ci =∑ e∈E Hi,e is the number of hyperedges the vertex i lies in. The bound on the product of the step sizes can be derived as follows\n‖K‖22 = ‖KTK‖2 = 2‖ ∑ e∈E KTeKe‖2 = 2 maxi=1,...,n{ci}.\nAlgorithm 3 PDHG for ΩH,1\n1: Initialization: f (0) = f̄ (0) = 0, θ ∈ [0, 1], σ, τ > 0 with στ < 1/(2 maxi=1,...,n{ci}) 2: repeat 3: α(e,1)\n(k+1) = PSeλwe (α (e,1)(k) + σKef̄ (k)), e ∈ E\n4: α(e,2) (k+1) = P−Seλwe (α (e,2)(k) + σKef̄ (k)), e ∈ E\n5: f (k+1) = proxτG(f (k) − τ ∑ e∈E K T e(α (e,1)(k+1) + α(e,2) (k+1) )) 6: f̄ (k+1) = f (k+1) + θ(f (k+1) − f (k)) 7: until relative duality gap < 8: Output: f (k+1).\nIt is important to point out here that the algorithm decouples the problem in the sense that in every iteration we solve subproblems which treat the functionals G,F(e,1), F(e,2) separately and thus can be solved in an efficient way.\nPDHG algorithm for ΩH,2. We define G and Ke as above. Moreover, we set\nFe(α e) = λwe (max(α e)−min(αe))2︸ ︷︷ ︸ =:he(αe) . (8)\nHence, the primal problem can be written as minf∈Rn{G(f) + ∑ e∈E Fe(Kef)}.\nIn order to formulate the dual problem, we need the conjugate of Fe. To this end, we first derive the conjugate function of he defined in (8), i.e.,\nh∗e(α e) = sup φ∈Rme {〈αe, φ〉 − (max(φ)−min(φ))2}.\nLemma 5.1. Let αe ∈ Rme and t+ = ∑ i:αei>0 αei and t− = ∑ i:αei<0 αei . It holds that\nh∗e(α e) =\n{ 1 4 t 2 + if 〈αe,1〉 = 0,\n+∞ otherwise.\nProof. Using the decomposition, φ = ψ + γ1, where 〈ψ,1〉 = 0 and γ ∈ R, we can write\n〈αe, φ〉 − (max(φ)−min(φ))2 = γ 〈αe,1〉+ 〈αe, ψ〉 − (max(ψ)−min(ψ))2. Thus for 〈αe,1〉 6= 0, we have h∗e(αe) = ∞. Now we consider the case where 〈αe,1〉 = 0. We write I− = {i : αei < 0} and I+ = {i : αei > 0} and define t+ = ∑ i∈I+ α e i and t− = ∑ i∈I− α e i . Note that 〈αe,1〉 = 0 implies t+ = −t−. Let us assume a = max(φ) and b = min(φ) are fixed. To maximize 〈αe, φ〉 − (max(φ)−min(φ))2 it is clearly best to choose φi = a for i ∈ I− and φi = b for i ∈ I+. Consequently,\n〈αe, φ〉 − (max(φ)−min(φ))2 = t+(b− a)− (b− a)2. (9) We maximize the gap ∆ = b− a for the objective m(∆) = t+∆−∆2 and obtain the maximizer as ∆ = t+2 . Thus we have h ∗ e(α e) = t2+ 4 if 〈α e,1〉 6= 0.\nWith t+ = ∑ i:αei>0 αei and t− = ∑ i:αei<0 αei we thus get\nF ∗e (α e) = λwe h\n∗ ( αe\nλwe\n) = { 1 4λwe t2+ if t+ = −t−,\n+∞ otherwise. (10)\nSo, we obtain the dual problem −minαe{G∗(− ∑ e∈E KTeα e) + ∑ e∈E 1 4λwe (te+) 2 + ∑ e∈E ι{0}(t e + + t e −)},\nwhere te+ = ∑ i:αei>0 αei and t e − = ∑ i:αei<0 αei . As we have seen in (10), the conjugate functions F ∗e are not indicator functions and we thus solve the corresponding proximal problems via proximal problems for Fe. More specifically, we exploit the fact that\nproxσF∗e (α̃ e) = α̃e − prox 1 σFe (α̃e), (11)\nsee [27, Lemma 2.10], and use the following novel result concerning the proximal problem on the right-hand side of (11). Proposition 5.1. For any σ > 0 and any α̃e ∈ Rme the proximal map\nprox 1 σFe (α̃e) = arg min αe∈Rme {1 2 ‖αe − α̃e‖22 + 1 σ λwe(max(α e)−min(αe))2}\ncan be computed with O(me logme) arithmetic operations.\nWe will now derive such an algorithm. To simplify the notation, we consider instead of 1σFe the function h : Rm → R defined by\nh(α) = (max(α)−min(α))2\nand show that proxµh(α), µ > 0, can be computed with O(m logm) arithmetic operations.\nLet us fix α ∈ Rm. For every pair r, s ∈ [min(α),max(α)] with r ≥ s, we define α(r,s) by\nα (r,s) i = { r if αi ≥ r αi if αi ∈ (r, s) s if αi ≤ s\n(12)\nClearly, if r = max(proxµh(α)) and s = min(proxµh(α)) then α (r,s) = proxµh(α). Hence, the above definition allows us to write the proximal problem in terms of the variables r, s since for\n(r, s) = arg min r̃,s̃ {1 2 ‖α(r̃,s̃) − α‖22︸ ︷︷ ︸\n=:E1(r̃,s̃)\n+µ(r̃ − s̃)2︸ ︷︷ ︸ =:E2(r̃,s̃) } (13)\nwe have proxµh(α) = α (r,s). Our goal is now to find a minimizer of (13). To this end, we first order α in an increasing order which can be done inO(m logm) arithmetic operations. W.l.o.g. we assume here that the components of α are pairwise different. Moreover, we introduce the following notation. For r, s ∈ [α1, αm] there exist unique p, q ∈ {1, . . . ,m} characterized by αm−p+1 = min{αi|αi ≥ r} and αq = max{αi|αi ≤ s}. Thus, the directional partial derivatives w.r.t. r and s are given by\n∂E1 ∂r− (r, s) = m∑ i=m−p+1 (αi − r), ∂E1 ∂s+ (r, s) = q∑ i=1 (s− αi). (14)\nThey tell us how much we increase E1 by decreasing r and increasing s, respectively. On the other hand both of these changes lead to a decrease in the energy E2. More precisely, it holds that\n∂E2 ∂r− (r, s) = ∂E2 ∂s+ (r, s) = 2µ(s− r). (15)\nThus, the main ideas behind our algorithm are as follows. Starting with r = max(α) and s = min(α), we decrease r and increase s keeping the two partial derivatives of (14) equal. We stop when the sum of the partial derivatives vanishes. So, the optimal r, s are characterized by the system\nm∑ i=m−p+1 (αi − r) = q∑ i=1 (s− αi), (16)\nm∑ i=m−p+1 (αi − r) + 2µ(s− r) = 0. (17)\nWe will now generate a sequence of pairs r(k), s(k) satisfying r(k) ≥ s(k) and (16) for each k. The corresponding indices needed to calculate the partial derivatives will be denoted by p(k), q(k). The main procedure is described in the next lemma.\nLemma 5.2. Assume r(k) ∈ (αm−p(k) , αm−p(k)+1] and s(k) ∈ [αq(k) , αq(k)+1) and property (16) holds for (r(k), s(k)). Then, we can either choose\nr(k+1) = r(k) − q (k)\np(k) (s(k+1) − s(k)) and s(k+1) = αq(k)+1 (18)\nor\nr(k+1) = αm−p(k) and s (k+1) = s(k) +\np(k) q(k) (r(k) − r(k+1)) (19)\nsuch that r(k+1) ∈ [αm−p(k) , αm−p(k)+1), s(k+1) ∈ (αq(k) , αq(k)+1] and (16) holds true for (r(k+1), s(k+1)).\nProof. Property (16) for (r(k+1), s(k+1)) means that\nm∑ i=m−p(k)+1 (αi − r(k+1)) = q(k)∑ i=1 (s(k+1) − αi). (20)\nSince by assumption (16) holds for (r(k), s(k)), equation (20) is equivalent to\np(k)(r(k+1) − r(k)) = q(k)(s(k) − s(k+1)).\nIf we set (r(k+1), s(k+1)) according to (18) but r(k+1) < αm−p(k) . Then we get\nr(k) − q (k)\np(k) (αq(k)+1 − s(k)) < αm−p(k)\n⇒ s(k) + p (k)\nq(k) (r(k) − αm−p(k)) < αq(k)+1,\ni.e., we can choose r(k+1), s(k+1) according to (19) and vice versa.\nAfter each computation of a new pair (r(k+1), s(k+1)) we check if the left-hand side of (17) is smaller than zero (note that initially the left-hand side of (17) is negative and it is increasing for every iteration). If this is not the case, we found the intervals where the optimal values r and s lie in. Restricted to this domain the functional E1 +E2 is a differentiable. Hence, we can compute r, s as follows. Lemma 5.3. Assume that the optimal r, s of (13) fulfill r ∈ [αm−p, αm−p+1] and s ∈ [αq, αq+1]. Then, it holds that\ns = ( q + 2µ− (2µ)\n2∑m i=m−p+1 αi + 2µ )−1( 2µ p+ 2µ m∑ i=m−p+1 αi + q∑ i=1 αi )\nr = 1\n2µ\n( (q + 2µ)s− q∑ i=1 αi ) .\nProof. When restricted to [αi, αi+1] × [αj , αj+1], the function (r, s) 7→ E1(r, s) + E2(r, s) is a quadratic function in (r, s). We can thus simply set the gradient to zero and solve the corresponding system of linear equations which yields the above result.\nIn conclusion, we obtain the following algorithm. Note that after the sorting, the algorithm takes in the order of m steps to compute the proximal map which proves Proposition 5.1.\nHence, the corresponding PDHG algorithm can be formulated as follows.\nWe solve the subproblems in line 3 via Algorithm 4. Note that the bound on the step sizes is now doubled, i.e., less restrictive since we have defined for each hyperedge one functional Fe and not two as for p = 1, i.e.,\n‖K‖22 = ‖KTK‖2 = ‖ ∑ e∈E KTeKe‖2 = maxi=1,...,n{ci}.\nAlgorithm 4 – Solution of the proximal problem proxµh(α)\n1: Sort α ∈ Rm in increasing order. 2: Initialization: r(0) = max(α), s(0) = min(α) 3: while ∂E1∂r− (r\n(k), s(k)) < 2µ(r(k) − s(k)) and q(k) + 1 ≤ m− p(k) do 4: Find (r(k+1), s(k+1)) according to Lemma 5.2. 5: end while 6: Compute r, s as described in Lemma 5.3. 7: Output: After restoring the original order, set\n(proxµh(α))i = { r if αi ≥ r, αi if αi ∈ (r, s), s if αi ≤ s, for i = 1, . . . ,m.\nAlgorithm 5 PDHG for ΩH,2\n1: Initialization: f (0) = f̄ (0) = 0, θ ∈ [0, 1], σ, τ > 0 with στ < 1/maxi=1,...,n{ci} 2: repeat 3: αe(k+1) = αe(k) + σKef̄ (k) − prox 1\nσFe (αe(k) + σKef̄ (k)), e ∈ E 4: f (k+1) = proxτG(f (k) − τ ∑ e∈E K T e(α e(k+1))) 5: f̄ (k+1) = f (k+1) + θ(f (k+1) − f (k)) 6: until relative duality gap < 7: Output: f (k+1)."
    }, {
      "heading" : "6 Experiments",
      "text" : "The method of Zhou et al [11] seems to be the standard algorithm for clustering and SSL on hypergraphs. We compare to them on a selection of UCI datasets summarized in Table 2. Zoo, Mushrooms and 20Newsgroups2 have been used also in [11] and contain only categorical features. As in [11], a hyperedge of weight one is created by all data points which have the same value of a categorical feature. For covertype we quantize the numerical features into 10 bins of equal size. Two datasets are created each with two classes (4,5 and 6,7) of the original dataset.\nSemi-supervised Learning (SSL). In [11], they suggest using a regularizer induced by the normalized Laplacian LCE arising from the clique expansion\nLCE = I−D − 12 CEHW ′HTD − 12 CE ,\nwhere DCE is a diagonal matrix with entries dEC(i) = ∑ e∈E Hi,e we |e| and W\n′ ∈ R|E|×|E| is a diagonal matrix with entries w′(e) = we/|e|. The SSL problem can then be formulated as\nλ > 0, arg minf∈R|V |{‖f − Y ‖ 2 2 + λ 〈f, LCEf〉}.\n2This is a modified version by Sam Roweis of the original 20 newsgroups dataset available at http: //www.cs.nyu.edu/˜roweis/data/20news_w100.mat.\nThe advantage of this formulation is that the solution can be found via a linear system. However, as Table 2 indicates the obvious downside is that LCE is a potentially very dense matrix and thus one needs in the worst case |V |2 memory and O(|V |3) computations. This is in contrast to our method which needs 2 ∑ e∈E |e| + |V | memory. For the largest example (covertype 6,7), where the clique expansion fails due to memory problems, our method takes 30-100s (depending on λ). We stop our method for all experiments when we achieve a relative duality gap of 10−6. In the experiments we do 10 trials for different numbers of labeled points. The reg. parameter λ is chosen for both methods from the set 10−k, where k = {0, 1, 2, 3, 4, 5, 6} via 5-fold cross validation. The resulting errors and standard deviations can be found in the following table(first row lists the no. of labeled points).\nOur SSL methods based on ΩH,p, p = 1, 2 outperform consistently the clique expansion technique of Zhou et al [11] on all datasets except 20newsgroups3. However, 20newsgroups is a very difficult dataset as only 10,267 out of the 16,242 data points are different which leads to a minimum possible error of 9.6%. A method based on pairwise interaction such as the clique expansion can better deal with such label noise as the large hyperedges for this dataset accumulate the label noise. On all other datasets we observe that incorporating hypergraph structure leads to much better results. As expected our squared TV functional (p = 2) outperforms slightly the total variation (p = 1) even though the difference is small. Thus, as ΩH,2 reduces to the standard regularization based on the graph Laplacian, which is known to work well, we recommend ΩH,2 for SSL on hypergraphs.\nZoo 20 25 30 35 40 45 50 Zhou et al. 35.1±17.2 30.3± 7.9 40.7±14.2 29.7± 8.8 32.9±16.8 27.6±10.8 25.3±14.4 ΩH,1 2.9± 3.0 1.4± 2.2 2.2± 2.1 0.7± 1.0 0.7± 1.5 0.9± 1.4 1.9± 3.0 ΩH,2 2.3± 1.9 1.5± 2.4 2.9± 2.3 0.9± 1.4 0.8± 1.7 1.2± 1.8 1.6± 2.9\nMushr. 20 40 60 80 100 120 160 200 Zhou et al. 15.5± 12.8 10.9±4.4 9.5± 2.7 10.3±2.0 9.0± 4.5 8.8± 1.4 8.8± 2.3 9.3± 1.0 ΩH,1 19.5±10.5 10.8±3.7 7.4± 3.8 5.6± 1.9 5.7± 2.2 5.4± 2.4 4.9± 3.8 5.6± 3.8 ΩH,2 18.4± 7.4 9.8± 4.5 9.9± 5.5 6.4± 2.7 6.3± 2.5 4.5± 1.8 4.4± 2.1 3.0± 0.6 covert45 20 40 60 80 100 120 160 200 Zhou et al. 18.9± 4.6 18.3±5.2 17.2±6.7 16.6±6.4 17.6±5.2 18.4±5.1 19.2±4.0 20.4±2.9 ΩH,1 21.4± 0.9 17.6±2.6 12.6±4.3 7.6± 3.5 6.2± 3.8 4.5± 3.6 2.6± 1.6 1.5± 1.3 ΩH,2 20.7± 2.0 16.1± 4.1 10.9± 4.9 5.9± 3.7 4.6± 3.4 3.3± 3.1 2.2± 1.8 1.0± 1.1 covert67 20 40 60 80 100 120 160 200 ΩH,1 40.6± 8.9 6.4±10.4 3.6± 3.2 3.3± 2.5 1.8± 0.8 1.3± 0.9 0.9± 0.4 1.2± 0.9 ΩH,2 25.2± 18.3 4.3± 9.6 2.1± 2.0 2.2± 1.4 1.4± 1.1 1.0± 0.8 0.7± 0.4 1.1± 0.8 20news 20 40 60 80 100 120 160 200 Zhou et al. 45.5± 7.5 34.4± 3.1 31.5± 1.4 29.8± 4.0 27.0± 1.3 27.3± 1.5 25.7± 1.4 25.0± 1.3 ΩH,1 65.7± 6.1 61.4±6.1 53.2±5.7 46.2±3.7 42.4±3.3 40.9±3.2 36.1±1.5 34.7±3.6 ΩH,2 55.0± 4.8 48.0±6.0 45.0±5.9 40.3±3.0 38.3±2.7 38.1±2.6 35.0±2.8 34.1±2.0\nTest error and standard deviation of the SSL methods over 10 runs for varying number of labeled points.\nClustering. We use the normalized hypergraph cut as clustering objective. For more than two clusters we recursively partition the hypergraph until the desired number of clusters is reached. For comparison we use the normalized spectral clustering approach based on the Laplacian LCE [11](clique expansion). The first part (first 6 columns) of the following table shows the clustering errors (majority vote on each cluster) of both methods as well as the normalized cuts achieved by these methods on the hypergraph and on the graph resulting from the clique expansion. Moreover, we show results (last 4 columns) which are obtained based on a kNN graph (unit weights) which is built based on the Hamming distance (note that we have categorical features) in order to check if the hypergraph modeling of the problem is actually useful compared to a standard similarity based graph construction. The number k is chosen as the smallest number for which the graph becomes connected and we compare results of normalized 1-spectral clustering [14] and the standard spectral clustering [22]. Note that the employed hypergraph construction has no free parameter.\nClustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.98 32.25 0.0011 0.0013 0.6991 0.7053 48.2 48.2 1e-4 1e-4 Zoo 16.83 15.84 0.6739 0.6784 5.1315 5.1703 5.94 5.94 1.636 1.636 20-newsgroup 47.77 33.20 0.0176 0.0303 2.3846 1.8492 66.38 66.38 0.1031 0.1034 covertype (4,5) 22.44 22.44 0.0018 0.0022 0.7400 0.6691 22.44 22.44 0.0152 0.02182 covertype (6,7) 8.16 - 8.18e-4 - 0.6882 - 45.85 45.85 0.0041 0.0041\n3Communications with the authors of [11] could not clarify the difference to their results on 20newsgroups\nFirst, we observe that our approach optimizing the normalized hypergraph cut yields better or similar results in terms of clustering errors compared to the clique expansion (except for 20-newsgroup for the same reason given in the previous paragraph). The improvement is significant in case of Mushrooms while for Zoo our clustering error is slightly higher. However, we always achieve smaller normalized hypergraph cuts. Moreover, our method sometimes has even smaller cuts on the graphs resulting from the clique expansion, although it does not directly optimize this objective in contrast to [11]. Again, we could not run the method of [11] on covertype (6,7) since the weight matrix is very dense. Second, the comparison to a standard graph-based approach where the similarity structure is obtained using the Hamming distance on the categorical features shows that using hypergraph structure is indeed useful. Nevertheless, we think that there is room for improvement regarding the construction of the hypergraph which is a topic for future research."
    }, {
      "heading" : "Acknowledgments",
      "text" : "M.H. would like to acknowledge support by the ERC Starting Grant NOLEPRO and L.J. acknowledges support by the DFG SPP-1324."
    } ],
    "references" : [ {
      "title" : "Video object segmentation by hypergraph cut",
      "author" : [ "Y. Huang", "Q. Liu", "D. Metaxas" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Higher order motion models and spectral clustering",
      "author" : [ "P. Ochs", "T. Brox" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Hypergraphs and cellular networks",
      "author" : [ "S. Klamt", "U.-U. Haus", "F. Theis" ],
      "venue" : "PLoS Computational Biology,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "A hypergraph-based learning algorithm for classifying gene expression and arraycgh data with prior knowledge",
      "author" : [ "Z. Tian", "T. Hwang", "R. Kuang" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Clustering categorical data: an approach based on dynamical systems",
      "author" : [ "D. Gibson", "J. Kleinberg", "P. Raghavan" ],
      "venue" : "VLDB Journal,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "Music recommendation by unified hypergraph: Combining social media information and music content",
      "author" : [ "J. Bu", "S. Tan", "C. Chen", "C. Wang", "H. Wu", "L. Zhang", "X. He" ],
      "venue" : "In Proc. of the Int. Conf. on Multimedia (MM),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Multi-way clustering using super-symmetric non-negative tensor factorization",
      "author" : [ "A. Shashua", "R. Zass", "T. Hazan" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Pellilo. A game-theoretic approach to hypergraph clustering",
      "author" : [ "M.S. Rota Bulo" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Efficient hypergraph clustering",
      "author" : [ "M. Leordeanu", "C. Sminchisescu" ],
      "venue" : "In AISTATS, pages 676–684,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Beyond pairwise clustering",
      "author" : [ "S. Agarwal", "J. Lim", "L. Zelnik-Manor", "P. Petrona", "D.J. Kriegman", "S. Belongie" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Learning with hypergraphs: Clustering, classification, and embedding",
      "author" : [ "D. Zhou", "J. Huang", "B. Schölkopf" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Higher order learning with graphs",
      "author" : [ "S. Agarwal", "K. Branson", "S. Belongie" ],
      "venue" : "In ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Modeling hypergraphs by graphs with the same mincut properties",
      "author" : [ "E. Ihler", "D. Wagner", "F. Wagner" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1993
    }, {
      "title" : "An inverse power method for nonlinear eigenproblems with applications in 1spectral clustering and sparse PCA",
      "author" : [ "M. Hein", "T. Bühler" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Total variation and Cheeger cuts",
      "author" : [ "A. Szlam", "X. Bresson" ],
      "venue" : "In ICML, pages 1039–1046,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Beyond spectral clustering - tight relaxations of balanced graph cuts",
      "author" : [ "M. Hein", "S. Setzer" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Constrained fractional set programs and their application in local clustering and community detection",
      "author" : [ "T. Bühler", "S. Rangapuram", "S. Setzer", "M. Hein" ],
      "venue" : "In ICML,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Learning with submodular functions: A convex optimization perspective",
      "author" : [ "F. Bach" ],
      "venue" : "CoRR, abs/1111.6453,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Semi-supervised learning on manifolds",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Schölkopf" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Trans. Patt. Anal. Mach. Intell.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2000
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science",
      "author" : [ "E. Esser", "X. Zhang", "T.F. Chan" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "A first-order primal-dual algorithm for convex problems with applications to imaging",
      "author" : [ "A. Chambolle", "T. Pock" ],
      "venue" : "J. of Math. Imaging and Vision,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "A primaldual splitting method for convex optimization involving lipschitzian, proximable and linear composite terms",
      "author" : [ "L. Condat" ],
      "venue" : "J. Optimization Theory and Applications,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "On Linear-Time algorithms for the continuous quadratic knapsack problem",
      "author" : [ "K. Kiwiel" ],
      "venue" : "J. Opt. Theory Appl.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "Signal recovery by proximal forward-backward splitting",
      "author" : [ "P.L. Combettes", "V.R. Wajs" ],
      "venue" : "Multiscale Modeling and Simulation,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "The first one uses tensor methods for clustering as the higher-order extension of matrix (spectral) methods for graphs [7, 8, 9].",
      "startOffset" : 119,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "The first one uses tensor methods for clustering as the higher-order extension of matrix (spectral) methods for graphs [7, 8, 9].",
      "startOffset" : 119,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "The first one uses tensor methods for clustering as the higher-order extension of matrix (spectral) methods for graphs [7, 8, 9].",
      "startOffset" : 119,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "The second main approach can deal with arbitrary hypergraphs [10, 11].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "The second main approach can deal with arbitrary hypergraphs [10, 11].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "The two main ways of approximating the hypergraph by a standard graph are the clique and the star expansion which were compared in [12].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "One can summarize [12] by stating that no approximation fully encodes the hypergraph structure.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Earlier, [13] have proven that an exact representation of the hypergraph via a graph retaining its cut properties is impossible.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut.",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut.",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut.",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut.",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "In order to handle hypergraphs with existing methods developed for graphs, the focus in previous works [11, 12] has been on transforming the hypergraph into a graph.",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "In order to handle hypergraphs with existing methods developed for graphs, the focus in previous works [11, 12] has been on transforming the hypergraph into a graph.",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "In [11], they suggest using the clique expansion (CE), i.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "We omit the discussion of the star graph approximation of hypergraphs discussed in [12] as it is shown there that the star graph expansion is very similar to the clique expansion.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "[13] which states that in general there exists no graph with the same vertex set V which has for every partition (C,C) the same cut value as the hypergraph cut.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "It is well-known that the Lovasz extension S is a convex function if and only if Ŝ is submodular [18].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "For graphs G = (V,W ), the total variation on graphs is defined as the Lovasz extension of the graph cut [18] given as TVG : R|V | → R, TVG(f) = 1 2 ∑n i,j=1 wij |fi − fj |.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "With the regularization functionals derived in the last section, we can immediately write down a formulation for two-class semi-supervised learning on hypergraphs similar to the well-known approaches of [19, 20].",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 19,
      "context" : "With the regularization functionals derived in the last section, we can immediately write down a formulation for two-class semi-supervised learning on hypergraphs similar to the well-known approaches of [19, 20].",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 20,
      "context" : "Clearly, this difference carries over to the famous normalized cut criterion introduced in [21, 22] for clustering of graphs with applications in image segmentation.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "Clearly, this difference carries over to the famous normalized cut criterion introduced in [21, 22] for clustering of graphs with applications in image segmentation.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.",
      "startOffset" : 42,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.",
      "startOffset" : 42,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.",
      "startOffset" : 42,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.",
      "startOffset" : 42,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "Other examples of balancing functions can be found in [16].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "Our following result shows that the balanced hypergraph cut also has an exact relaxation into a continuous nonlinear eigenproblem [14].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "Noting that both cutH(C,C) and Ŝ(C) vanish on the full set V , the proof then follows from the recent result [17], which shows in this case the equivalence between the set problem and the continuous problem written in terms of the Lovasz extensions.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "As discussed in [16], every Lovasz extension S can be written as a difference of convex positively 1-homogeneous functions1 S = S1 − S2.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "We employ the RatioDCA algorithm [16] shown in Algorithm 1.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "For the general case, see [16].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 22,
      "context" : "We propose to solve these problems using a primal-dual algorithm, denoted PDHG in this paper, which was proposed in [23, 24].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : "We propose to solve these problems using a primal-dual algorithm, denoted PDHG in this paper, which was proposed in [23, 24].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "For convergence proofs we refer to [23, 24].",
      "startOffset" : 35,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "For convergence proofs we refer to [23, 24].",
      "startOffset" : 35,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 2 PDHG 1: Initialization: f (0) = f̄ (0) = 0, θ ∈ [0, 1], σ, τ > 0 with στ < 1/‖K‖2 2: repeat 3: α = proxσF∗(α (k) + σKf̄ ) 4: f (k+1) = proxτG(f)(f (k) − τK(α)) 5: f̄ (k+1) = f (k+1) + θ(f (k+1) − f ) 6: until relative duality gap < 7: Output: f .",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "However, note that smooth convex terms can also be directly exploited [25].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : ", [26].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 3 PDHG for ΩH,1 1: Initialization: f (0) = f̄ (0) = 0, θ ∈ [0, 1], σ, τ > 0 with στ < 1/(2 maxi=1,.",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 5 PDHG for ΩH,2 1: Initialization: f (0) = f̄ (0) = 0, θ ∈ [0, 1], σ, τ > 0 with στ < 1/maxi=1,.",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "The method of Zhou et al [11] seems to be the standard algorithm for clustering and SSL on hypergraphs.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "Zoo, Mushrooms and 20Newsgroups2 have been used also in [11] and contain only categorical features.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : "As in [11], a hyperedge of weight one is created by all data points which have the same value of a categorical feature.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "In [11], they suggest using a regularizer induced by the normalized Laplacian LCE arising from the clique expansion LCE = I−D − 12 CEHW ′HTD − 1 2 CE , where DCE is a diagonal matrix with entries dEC(i) = ∑ e∈E Hi,e we |e| and W ′ ∈ R|E|×|E| is a diagonal matrix with entries w′(e) = we/|e|.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "Our SSL methods based on ΩH,p, p = 1, 2 outperform consistently the clique expansion technique of Zhou et al [11] on all datasets except 20newsgroups3.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "For comparison we use the normalized spectral clustering approach based on the Laplacian LCE [11](clique expansion).",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "The number k is chosen as the smallest number for which the graph becomes connected and we compare results of normalized 1-spectral clustering [14] and the standard spectral clustering [22].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "The number k is chosen as the smallest number for which the graph becomes connected and we compare results of normalized 1-spectral clustering [14] and the standard spectral clustering [22].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 10,
      "context" : "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 21,
      "context" : "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : "Communications with the authors of [11] could not clarify the difference to their results on 20newsgroups",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "Moreover, our method sometimes has even smaller cuts on the graphs resulting from the clique expansion, although it does not directly optimize this objective in contrast to [11].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "Again, we could not run the method of [11] on covertype (6,7) since the weight matrix is very dense.",
      "startOffset" : 38,
      "endOffset" : 42
    } ],
    "year" : 2013,
    "abstractText" : "Hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. In this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element is a family of regularization functionals based on the total variation on hypergraphs.",
    "creator" : "LaTeX with hyperref package"
  }
}