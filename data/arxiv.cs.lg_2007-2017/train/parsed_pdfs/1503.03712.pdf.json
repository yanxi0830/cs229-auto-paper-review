{
  "name" : "1503.03712.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Graduated Optimization for Stochastic Non-Convex Problems",
    "authors" : [ "Elad Hazan", "Kfir Y. Levy", "Shai Shalev-Swartz" ],
    "emails" : [ "ehazan@cs.princeton.edu.", "kfiryl@tx.technion.ac.il.", "shais@cs.huji.ac.il." ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a parameterized family of nonconvex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an ε-approximate solution within O(1/ε2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of “zero-order optimization”, and devise a a variant of our algorithm which converges at rate of O(d2/ε4)."
    }, {
      "heading" : "1 Introduction",
      "text" : "Non-convex optimization programs are ubiquitous in machine learning and computer vision. Of particular interest are non-convex optimization problem that arise in the training of deep neural networks Bengio (2009). Often, such problems admit a multimodal structure, and therefore, the use of convex optimization machinery may lead to a local optima.\nGraduated optimization (a.k.a. continuation), Blake and Zisserman (1987), is a methodology that attempts to overcome such numerous local optima. At first, a coarse-grained version of the problem is generated by a local smoothing operation. This coarse-grained version is easier to solve. Then, the method advances in stages by gradually refining the problem versions, using the solution of the previous stage as an initial point for the optimization in the next stage.\n∗Princeton University; ehazan@cs.princeton.edu. †Technion; kfiryl@tx.technion.ac.il. ‡The Hebrew University; shais@cs.huji.ac.il.\nar X\niv :1\n50 3.\n03 71\n2v 1\n[ cs\n.L G\n] 1\n2 M\nDespite its popularity, there are still many gaps concerning both theoretical and practical aspects of graduated optimization, and in particular we are not aware of a rigorous running time analysis to find a global optimum, or even conditions in which a global optimum is reached. Nor are we familiar with graudated optimization in the stochastic setting, in which only a noisy gradient or value oracle to the objective is given. Moreover, any practical application of graduated optimization requires an efficient construction of coarse-grained versions of the original function. For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) . However, in the general case, it is commonly suggested in the literature to convolve the original function with a gaussian kernel Wu (1996). Yet, this operation is prohibitively inefficient in high dimensions.\nIn this paper we take an algorithmic / analytic approach to graduated optimization and show the following.\n• We characterise a family of non-convex multimodal functions that allows convergence to a global optimum. This parametrized family we call σ-nice (see Definition 4.2 ).\n• We provide a stochastic algorithm inspired by graduated optimization, that performs only gradient updates and is ensured to find an ε-optimal solution of σ-nice functions within O(1/σ2ε2) iterations. The algorithm doesn’t require expensive convolutions and access the smoothed version of any function using random sampling. The algorithm only requires access to the objective function through a noisy gradient oracle.\n• We extend our method to the “zero-order optimization” model (a.k.a. “bandit feedback” model), in which the objective is only accessible through a noisy value oracle. We devise a variant of our algorithm that is guaranteed to find an ε-optimal solution within O(d2/σ2ε4) iterations.\nInterestingly, the next question is raised in Bengio (2009) which reviews recent developments in the field of deep learning: “Can optimization strategies based on continuation methods deliver significantly improved training of deep architectures?”\nAs an initial empirical study, we examine the task of training a NN (Neural Network) over the MNIST data set. Our experiments support the theoretical guarantees, demonstrating that graduated optimization according to the methodology proposed accelerates convergence in training the NN. Moreover, we show examples in which σ-nice functions capture nonconvex structure/phenomena that exists in natural data."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Among the machine vision community, the idea of graduated optimization was known since the 80’s. The term “Graduated Non-Convexity” (GNC) was coined by Blake and Zisserman (1987), who were the first to establish this idea explicitly. Similar attitudes in the machine vision literature appeared later in Yuille (1989); Yuille et al. (1990), and Terzopoulos (1988).\nConcepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990).\nOver the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation. These approaches, in the spirit of the continuation method, offer no guarantees on the quality of the obtained solution, and are tailored to specific applications.\nA comprehensive survey of the graduated optimization literature can be found in Mobahi and Fisher III (2015a).\nA recent work Mobahi and Fisher III (2015b) advances our theoretical understanding, by analyzing a continuation algorithm in the general setting. Yet, they offer no way to perform the smoothing efficiently, nor a way to optimize the smoothed versions; but rather assume that these are possible. Moreover, their guarantee is limited to a fixed precision that depends on the objective function and does not approach zero. In contrast, our approach can generate arbitrarily precise solutions."
    }, {
      "heading" : "2 Setting",
      "text" : "We discuss an optimization of a non-convex loss function f : K 7→ R, where K ⊆ Rd is a convex set. We assume that optimization lasts for T rounds; On each round t = 1, . . . , T , we may query a point xt ∈ K, and receive a feedback. After the last round, we choose x̄T ∈ K, and our performance measure is the excess loss, defined as:\nf(x̄T )−min x∈K f(x)\nIn Section 4.2 we characterize a family of non-convex multimodal functions we call σ-nice. Given such a σ-nice loss f , we are interested in algorithms that with a high probability ensure a ε-excess loss within poly(1/ε) rounds.\nWe consider two kinds of feedback:\n1. Noisy Gradient feedback: Upon querying xt we receive ∇f(xt) + ξt, where {ξτ}Tτ=1 are independent zero mean and bounded r.v.’s.\n2. Noisy Value feedback (Bandit feedback): Upon querying xt we receive f(xt)+ξt, where {ξτ}Tτ=1 are independent zero mean and bounded r.v.’s."
    }, {
      "heading" : "3 Preliminaries and Notation",
      "text" : "Notation: During this paper we use B,S to denote the unit Euclidean ball/sphere in Rd, and also Br(x),Sr(x) as the Euclidean r-ball/sphere in Rd centered at x. For a set A ⊂ Rd\n, u ∼ A denotes a random variable distributed uniformly over A."
    }, {
      "heading" : "3.1 Strong-Convexity",
      "text" : "Recall the definition of strongly-convex functions:\nDefinition 3.1. (Strong Convexity) We say that a function F : Rn → R is σ-strongly convex over the set K if for all x,y ∈ K it holds that,\nF (y) ≥ F (x) +∇F (x)>(y − x) + σ 2 ‖x− y‖2\nLet F be a σ-strongly convex over convex set K, and let x∗ be a point in K where F is minimized, then the following inequality is satisfied:\nσ 2 ‖x− x∗‖2 ≤ F (x)− F (x∗) (1)\nThis is immediate by the definition of strong convexity combined with ∇F (x∗)>(x− x∗) ≥ 0, ∀x ∈ K."
    }, {
      "heading" : "4 Smoothing and σ-Nice functions",
      "text" : "Constructing finer and finer approximations to the original objective function is at the heart of the continuation approach. In Section 4.1 we define the smoothed versions that we will employ. Next, in Section 4.1.1 we describe an efficient way to implicitly access the smoothed versions, which will enable us to perform optimization. Finally, in Section 4.2 we define a class of non-convex multimodal functions we denote as σ-nice. As we will see in Section 7, these functions are rich enough to capture non-convex structure that exists in natural data. Additionally, these functions lend themselves to an efficient optimization, and we can ensure a convergence to ε-solution within poly(1/ε) iterations, as described in Sections 5,6."
    }, {
      "heading" : "4.1 Smoothing",
      "text" : "Smoothing by local averaging is formally defined next.\nDefinition 4.1. Given an L-Lipschitz function f : Rd 7→ R define it’s δ-smooth version to be\nf̂δ(x) = Eu∼B[f(x + δu)].\nThe next lemma bounds the bias between f̂δ and f .\nLemma 4.1. Let f̂δ be the δ-smoothed version of f , then,\n∀x ∈ Rd : |f̂δ(x)− f(x)| ≤ δL\nProof of Lemma 4.1.\n|f̂δ(x)− f(x)| = |Eu∼B [f(x + δu)]− f(x)| ≤ Eu∼B [|f(x + δu)− f(x)|] ≤ Eu∼B [L‖δu‖] ≤ Lδ\nin the first inequality we used Jensen’s inequality, and in the last inequality we used ‖u‖ ≤ 1, since u ∈ B."
    }, {
      "heading" : "4.1.1 Implicit Smoothing using Sampling",
      "text" : "A direct way to optimize a smoothed version is by direct calculation of its gradients, nevertheless this calculation might be very costly in high dimensions. A much more efficient approach is to produce an unbiased estimate for the gradients of the smoothed version by sampling the function gradients/values. These estimates could then be used by a stochastic optimization algorithms such as SGD (Stochastic Gradient Descent). This sampling approach is outlined in Figures 1,2.\nThe following two Lemmas state that the resulting estimates are unbiased and bounded:\nLemma 4.2. Let x ∈ Rd, δ ≥ 0, and suppose that f is L-Lipschitz, then the output of SGOG (Figure 1) is bounded by L and is an unbiased estimate for ∇f̂δ(x).\nProof. SGOG outputs ∇f(x + δu) for some u ∈ B, so the first part is immediate by the Lipschitzness of f . Now, by definition, f̂δ(x) = Eu∼B[f(x + δu)], deriving both sides we get the second part of the Lemma.\nLemma 4.3. Let x ∈ K ⊆ Rd, δ ≥ 0, and suppose that maxx |f(x)| ≤ C, then the output of SGOV (Figure 2) is bounded by dC δ and is an unbiased estimate for ∇f̂δ(x).\nProof. SGOV outputs d δ f(x+δv)v for some v ∈ S, since f is C-Bounded over K the first part of the lemma is immediate. In order to prove the second part, we can use Stokes theorem to show that if v ∼ S, then:\n∀x ∈ Rd . Ev∼S[f(x + δv)v] = δ\nd ∇f̂δ(x) (2)\nA proof of Equation (2) is found in Flaxman et al. (2005).\nNote that the oracles depicted in Figures 1, 2 may require sampling function values outside K, (specifically in K + δB). We assume that this is possible, and that the bounds over the function gradients/values inside K, also apply in K + δB.\nExtensions to the noisy feedback settings: Note that for ease of notation, the oracles that appear in Figures 1, 2, assume we can access exact gradients/values of f . Given that we may only access noisy and bounded gradient/value estimates of f (Sec. 2), we could use these instead of the exact ones that appear in Figures 1,2, and still produce unbiased and bounded gradient estimates for the smoothed versions of f as shown in Lemmas 4.2,4.3.\nParticularly, in the case we may only access noisy gradients of f , then SGOG (Figure 1) will return ∇f(x + δu) + ξ instead of ∇f(x + δu), where ξ is a noise term. Since we assume zero bias and bounded noise this implies that ∇f(x + δu) + ξ is an unbiased estimate of ∇f̂δ(x), bounded by L+K where K is the bound on the noise and L is the Lipschitz constant of f . We can show the same for SGOV (Figure 2), given a noisy value feedback."
    }, {
      "heading" : "4.2 σ-Nice Functions",
      "text" : "Following is our main definition\nDefinition 4.2. A function f : K 7→ R is said to be σ-nice if the following two conditions hold:\n1. Centering property: For every δ > 0, and every x∗δ ∈ arg minx∈K f̂δ(x), there exists x∗δ/2 ∈ arg minx∈K f̂δ/2(x), such that:\n‖x∗δ − x∗δ/2‖ ≤ δ\n2\n2. Local strong convexity of the smoothed function: For every δ > 0 let rδ = 3δ, and denote x∗δ = arg minx∈K f̂δ(x), then over Brδ(x ∗ δ), the function f̂δ(x) is σ-strongly-\nconvex.\nHence, σ-nice is a combination of two properties. Both together imply that optimizing the smoothed version on a scale δ is a good start for optimizing a finer version on a scale of δ/2, which is sufficient for a scheme based on graduated optimization to work as we show next. In Section 7 we show that σ-nice functions arise naturally in data. An illustration of σ-nice function in 1-dimension appears in Figure 3."
    }, {
      "heading" : "5 Graduated Optimization with a Gradient Oracle",
      "text" : "In this section we assume that we can access a noisy gradient oracle for f . Thus, given x ∈ Rd, δ ≥ 0 we can use SGOG (Figure 1) to obtain an unbiased and bounded estimate for ∇f̂δ(x), as ensured by Lemma 4.2. Note that for ease of notation SGOG (Figure 1) is listed using an exact gradient oracle for f . As described at the end of Section 4.1.1, this could be replaced with a noisy gradient oracle for f , and Lemma 4.2, will still hold.\nFollowing is our main Theorem:\nTheorem 5.1. Let ε ∈ (0, 1) and p ∈ (0, 1/e), also let K be a convex set, and f be an LLipschitz σ-nice function. Suppose that we apply Algorithm 1, then after Õ(1/σ2ε2) rounds Algorithm 1 outputs a point x̄M+1 which is ε optimal with a probability greater than 1− p.\nAlgorithm 1 is divided into epochs, at epoch m it uses SGOG to obtain unbiased estimates for the gradients of f̂δm which are then employed by Suffix-SGD (Algorithm 2), to optimize this smoothed version. This optimization over f̂δm is performed until we are ensured to reach a point close enough to x∗m+1 := arg minx∈K f̂δm+1(x), i.e., the minimum of f̂δm+1 . Also note that at epoch m the optimization over f̂δm is initialized at x̄m which is the point reached at the previous epoch.\nSuffix-SGD (Algorithm 2), is a stochastic optimization algorithm for strongly convex functions. Its guarantees are presented in Section 5.1."
    }, {
      "heading" : "5.1 Analysis",
      "text" : "Let us first discuss Suffix-SGD (Algorithm 2). This algorithm performs projected gradient descent using the gradients received from GradOracle(·). The projection operator ΠK, is defined ∀y ∈ Rd as\nΠK(y) := arg min x∈K\n‖x− y‖ .\nAlgorithm 1 GradOptG Input: target error ε, maximal failure probability p, decision set K Choose x̄1 ∈ K uniformly at random. Set δ1 = diam(K)/2, p̃ = p/M , and M = log2 1α0ε where α0 = min{ 1 2Ldiam(K) , 2 √ 2√\nσdiam(K)} for m = 1 to M do\n// Perform SGD over f̂δm Set εm := σδ 2 m/32, and\nTF = 12480L2 σεm log (2 p̃ + 2 log 12480L2 σεm ) Set shrinked decision set,\nKm := K ∩B(x̄m, 1.5δm)\nSet gradient oracle for f̂δm ,\nGradOracle(·) = SGOG(·, δm)\nUpdate: x̄m+1 ← Suffix-SGD(TF ,Km, x̄m,GradOracle)\nδm+1 = δm/2 end for Return: x̄M+1\nAlgorithm 2 Suffix-SGD\nInput: total time TF , decision set K, initial point x1 ∈ K, gradient oracle GradOracle(·) for t = 1 to TF do\nSet ηt = 1/σt Query the gradient oracle at xt:\ngt ← GradOracle(xt)\nUpdate: xt+1 ← ΠK(xt − ηtgt) end for Return: x̄TF := 2 TF ( xTF /2+1 + . . .+ xTF ) Now consider a σ-strongly convex function F : K → R, and suppose that we have an oracle, GradOracle(·), that upon querying a point x ∈ K returns an unbiased and bounded\ngradient estimate, g, i.e., E[g] = ∇F (x), and ‖g‖ ≤ G. Note the following result from Rakhlin et al. (2011) regarding stochastic optimization of σ-strongly-convex functions, given such an oracle:\nTheorem 5.2. Let p ∈ (0, 1/e), and F be a σ-strongly convex function. Suppose that GradOracle(·) produces G-bounded, and unbiased estimates of ∇F . Then after no more than TF rounds, the final point x̄TF returned by Suffix-SGD (Algorithm 2 ) ensures that with a probability ≥ 1− p, we have:\nF (x̄TF )−min x∈K\nF (x) ≤ 6240 log\n( 2 log(TF )/p ) G2\nσTF\nCorollary 5.1. The latter means that for TF ≥ 12480G 2 σε log ( 2/p + 2 log(12480G2/σε) ) we will have an excess loss smaller than ε.\nNotice that at each epoch m of GradOptG, it initiates Suffix-SGD with a gradient oracle SGOG(·, δm). According to Lemma 4.2, SGOG(·, δm) produces an unbiased an L-bounded estimates of f̂δm , thus in the analysis of each epoch we can use Theorem 5.2 for f̂δm , taking G = L.\nFollowing is our key Lemma:\nLemma 5.1. Consider M , Km and x̄m+1 as defined in Algorithm 1. Also denote by x∗m the minimizer of f̂δm in K. Then the following holds for all 1 ≤ m ≤M w.p.≥ 1− p:\n1. The smoothed version f̂δmis σ-strongly convex over Km, and x∗m ∈ Km.\n2. Also, f̂δm(x̄m+1)− f̂δm(x∗m) ≤ σδ2m+1/8\nProof. We will prove by induction, let us prove it holds form = 1. Note that δ1 = diam(K)/2, therefore K1 = K, and also x∗1 ∈ K1. Also recall that σ-niceness of f implies that f̂δ1 is σstrongly convex in K, thus by Corollary 5.1, after less than TF = Õ( 12480L 2\nσ(σδ21/32) ) optimization\nsteps of Suffix-SGD with a probability greater than 1− p/M , we will have:\nf̂δ1(x̄2)− f̂δ1(x∗1) ≤ σδ21/32 = σδ22/8\nwhich establishes the case of m = 1. Now assume that lemma holds for m > 1. By this assumption, f̂δm(x̄m+1) − f̂δm(x∗m) ≤ σδ2m+1/8, f̂δm is σ-strongly convex in Km, and also x∗m ∈ Km. Hence, we can use Equation (1) to get:\n‖x̄m+1 − x∗m‖ ≤ √ 2\nσ\n√ f̂δm(x̄m+1)− f̂δm(x∗m) =\nδm+1 2\nCombining the latter with the centering property of σ-niceness yields:\n‖x̄m+1 − x∗m+1‖ ≤ ‖x̄m+1 − x∗m‖+ ‖x∗m − x∗m+1‖ ≤ 1.5δm+1\nand it follows that, x∗m+1 ∈ B(x̄m+1, 1.5δm+1) ⊂ B(x∗m+1, 3δm+1)\nRecalling that Km+1 := B(x̄m+1, 1.5δm+1), and the local strong convexity property of f (which is σ-nice), then the induction step for first part of the lemma holds. Now, by Corollary 5.1, after less than TF = Õ( 12480L 2\nσ(σδ2m+1/32) ) optimization steps of Suffix-SGD over f̂δm+1 , we\nwill have: f̂δm+1(x̄m+2)− f̂δm+1(x∗m+1) ≤ σδ2m+2/8\nwhich establishes the induction step for the second part of the lemma. An analysis of fail probability: since we have M epochs in total and at each epoch the fail probability is smaller than p/M , then the total fail probability of our algorithm is smaller than p.\nWe are now ready to prove Theorem 5.1:\nproof of Theorem 5.1. Algorithm 1 terminates after M = log2 1 α0ε epochs meaning, δM = diam(K)α0ε/2. According to Lemma 5.1 the following holds w.p.≥ 1− p ,\nf̂δM (x̄M+1)− f̂δM (x∗M) ≤ σδ2M+1/8\n=\n(√ σdiam(K)α0ε\n8 √ 2 )2 Due to Lemma 4.1, f̂δM is LδM biased from f , thus:\nf(x̄M+1)− f(x) ≤ Ldiam(K)α0ε+ (√\nσdiam(K)α0ε 8 √ 2 )2 ≤ ε\nwe used α0 = min{ 12Ldiam(K) , 2 √ 2√\nσdiam(K)}, and ε < 1. Let Ttotal, be the total number of queries made by by Algorithm 1, then we have:\nTtotal ≤ M∑ m=1 12480L2 σεm log Γ\n≤ M∑ m=1 12480L2 σ(σδ2m/32) log Γ\n≤ 4 · 10 5L2 log Γ\nσ2\nM∑ i=1 4i−1 δ21\n≤ 14 · 10 4L2 log Γ σ2 4M\nδ21\n≤ 14 · 10 4L2 log Γ σ2 max{16L2, σ/2} 1 ε2\nhere we used the notation:\nΓ := 2M\np + 2 log(12480L2/σεM)\n≤ 2M p + 2 log(4 · 105L2 max{16L2, σ 2 }/σ2ε2)"
    }, {
      "heading" : "6 Graduated Optimization with a Value Oracle",
      "text" : "In this section we assume that we can access a noisy value oracle for f . Thus, given x ∈ Rd, δ ≥ 0 we can use SGOV (Figure 2) as an oracle that outputs an unbiased and bounded estimates for ∇f̂δ(x), as ensured by Lemma 4.3. Note that for ease of notation SGOV (Figure 2) is listed using an exact value oracle for f . As described at the end of Section 4.1.1, this could be replaced with a noisy value oracle for f , and Lemma 4.3, will still hold.\nFollowing is our main Theorem:\nTheorem 6.1. Let ε > 0 and p ∈ (0, 1/e), also let K be a convex set, and f be an L-Lipschitz σ-nice function. Assume also that maxx |f(x)| ≤ C. Suppose that we apply Algorithm 3, then after after Õ(d2/σ2ε4) rounds Algorithm 3 outputs a point x̄M+1 which is ε optimal with a probability greater than 1− p.\nAn SGD algorithm for ε-approximating σ-nice functions is listed in Algorithm 1."
    }, {
      "heading" : "6.1 Analysis",
      "text" : "Notice that at each epoch m of GradOptV , it initiates Suffix-SGD with a gradient oracle SGOV (·, δm). According to Lemma 4.3, SGOV (·, δm) produces an unbiased and dC/δmbounded estimates for the gradients of f̂δm , thus in the analysis of each epoch we can use Corollary 5.1 for f̂δm , taking G = dC/δm.\nFollowing is our key Lemma:\nLemma 6.1. Consider M , Km and x̄m+1 as defined in Algorithm 3. Also denote by x∗m the minimizer of f̂δm in K. Then the following holds for all 1 ≤ m ≤M w.p.≥ 1− p:\n1. The smoothed version f̂δmis σ-strongly convex over Km, and x∗m ∈ Km.\n2. Also, f̂δm(x̄m+1)− f̂δm(x∗m) ≤ σδ2m+1/8\nThe proof of Lemma 6.1 is similar to the proof of Lemma 5.1 given in Section 5.1, we therefore omit the details.\nWe are now ready to prove Theorem 6.1:\nAlgorithm 3 GradOptV Input: target error ε, maximal failure probability p, decision set K Choose x̄1 ∈ K uniformly at random. Set δ1 = diam(K)/2, p̃ = p/M , and M = log2 1α0ε where α0 = min{ 1 2Ldiam(K) , 2 √ 2√\nσdiam(K)} for m = 1 to M do\n// Perform SGD over f̂δm Set εm := σδ 2 m/32, and\nTF = 12480\nσεm\nd2C2 δ2m log (2 p̃ + 2 log 12480d2C2 σεmδ2m ) Set shrinked decision set,\nKm := K ∩B(x̄m, 1.5δm)\nSet gradient oracle for f̂δm ,\nGradOracle(·) = SGOV (·, δm)\nUpdate: x̄m+1 ← Suffix-SGD(TF ,Km, x̄m,GradOracle)\nδm+1 = δm/2 end for Return: x̄M+1\nproof of Theorem 6.1. Let x̄M+1 be the output of Algorithm 3. Similarly to the proof of Theorem 5.1, we can show that:\nf(x̄M+1)− f(x) ≤ ε\nLet Ttotal, be the total number of queries made by by Algorithm 3, then we have:\nTtotal ≤ M∑ m=1 12480d2C2 σεmδ2m log Γ\n≤ M∑ m=1 12480d2C2 σ(σδ2m/32)δ 2 m log Γ\n≤ 4 · 10 5d2C2 log Γ\nσ2\nM∑ i=1 8i−1 δ41\n≤ 6 · 10 4d2C2 log Γ σ2 8M\nδ41\n≤ 6 · 10 4d2C2 log Γ σ2 max{256L4, σ2/4} 1 ε4\nhere we used the notation:\nΓ := 2M\np + 2 log(12480d2C2/σεMδ 2 M)\n≤ 2M p + 2 log(4 · 105d2C2 max{256L4, σ 2 4 }/σ2ε4)"
    }, {
      "heading" : "7 Experiments",
      "text" : "In the last two decades, performing complex learning tasks using Neural-Network (NN) architectures has become an active and promising line of research. Since learning NN architectures essentially requires to solve a hard non-convex program, we have decided to focus our empirical study on this type of tasks. As a test case, we train a NN with a single hidden layer of 30 units over the MNIST data set. We adopt the experimental setup of Dauphin et al. (2014) and train over a down-scaled version of the data, i.e., the original 28×28 images of MNIST were down-sampled to the size of 10 × 10. We use a ReLU activation function, and minimize the square loss."
    }, {
      "heading" : "7.1 Smoothing the NN",
      "text" : "First, we were interested in exploring the non-convex structure of the above NN learning task, and check whether our definition of σ-nice complies with this structure. We started by running MSGD (Minibatch Stochastic Gradient Descent) on the problem, while using a batch size of 100, and a step size rule of the form ηt = η0(1 + γt) −3/4, where η0 = 0.01, γ = 10 −4. This choice of step size rule was the most effective among a grid of rules that we examined. We have found out that MSGD frequently “stalls” in areas with a relatively high loss, here we relate to points at the end of such run as stall-points.\nIn order to learn about the non-convex nature of the problem, we examined the objective values along two directions around stall-points. The first direction was the gradient at the stall point, and the second direction was the line connecting the stall-point to x∗, where x∗ is the best weights configuration of the NN that we were able to find. A drawing depicting typical results appears on the left side of Figure 4. The stall-point appears in red, and x∗ in green; also the axis marked as X is the gradient direction, and one marked Y is the direction between stall-point and x∗. Note that the stall-point is inside a narrow “valley”, which prevents MSGD from “seeing” x∗, and so it seems that MSGD slowly progresses downstream. Interestingly, the objective around x∗ seems strongly-convex in the direction of the stall point.\nOn the middle of Figure 4, we present the δ = 3 smoothed version of the same objective that appears on the left side of Figure 4. We can see that the “valley” has not vanished, but the gradient of the smoothed version leads us slightly towards x∗ and out of the original “valley”. On the right side of Figure 4, we present the δ = 7 smoothed version of the objective. We can see that due to the coarse smoothing, the “valley” in which MSGD was stalled, has completely dissolved, and the gradient of this version leads us towards x∗."
    }, {
      "heading" : "7.2 Graduated Optimization of NN",
      "text" : "Here we present experiments that demonstrate the effectiveness of GradOptG (Algorithm 1) in training the NN mentioned above. First, we wanted to learn if smoothing can help us escape points where MSGD stalls. We used MSGD (δ = 0) to train the NN, and as before we found that its progress slows down, yielding relatively high error. We then took the point that MSGD reached after 5 · 104 iteration and initialized an optimization over the smoothed versions of the loss; this was done using smoothing values of {1, 3, 5, 7}. In Figure 5 we present the results of the above experiment.\nAs seen in Figure 5, small δ’s converge slower than large δ’s, but produce a much better solution. Furthermore, the initial optimization progresses in leaps, for large δ’s the leaps are sharper, and lower δ’s demonstrate smaller leaps. We believe that these leaps are associated with the advance of the optimization from one local “valley” to another.\nIn Figure 6 we compare our complete graduated optimization algorithm, namely GradOptG (Alg. 1) to MSGD. We started with an initial smoothing of δ = 7, which decayed according to GradOptG. Note that GradOptG progresses very fast and yields a much better solution than MSGD."
    }, {
      "heading" : "8 Discussion",
      "text" : "We have described a family of non-convex functions which admit efficient optimization via the graduated optimization methodology, and gave the first rigorous analysis of a first-order algorithm in the stochastic setting.\nWe view it as only a first glimpse of the potential of graduated optimization to provable non-convex optimization, and amongst the interesting questions that remain we find\n• Is σ-niceness necessary for convergence of first-order methods to a global optimum? Is there a more lenient property that better captures the power of graduated optimization?\n• Amongst the two properties of σ-niceness, can their parameters be relaxed in terms of the ratio of smoothing to strong-convexity, or to centering?\n• Can second-order/other methods give rise to better convergence rates / faster algorithms for stochastic or offline σ-nice non-convex optimization?"
    } ],
    "references" : [ {
      "title" : "Numerical continuation methods, volume 13",
      "author" : [ "E.L. Allgower", "K. Georg" ],
      "venue" : null,
      "citeRegEx" : "Allgower and Georg.,? \\Q1990\\E",
      "shortCiteRegEx" : "Allgower and Georg.",
      "year" : 1990
    }, {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundations and trends in Machine Learning,",
      "citeRegEx" : "Bengio.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2009
    }, {
      "title" : "A gnc algorithm for deblurring images with interacting discontinuities",
      "author" : [ "A. Boccuto", "M. Discepoli", "I. Gerace", "R. Pandolfi", "P. Pucci" ],
      "venue" : "Proc. VI SIMAI,",
      "citeRegEx" : "Boccuto et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Boccuto et al\\.",
      "year" : 2002
    }, {
      "title" : "Large displacement optical flow: descriptor matching in variational motion estimation",
      "author" : [ "T. Brox", "J. Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Brox and Malik.,? \\Q2011\\E",
      "shortCiteRegEx" : "Brox and Malik.",
      "year" : 2011
    }, {
      "title" : "Gradient descent optimization of smoothed information retrieval metrics",
      "author" : [ "O. Chapelle", "M. Wu" ],
      "venue" : "Information retrieval,",
      "citeRegEx" : "Chapelle and Wu.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chapelle and Wu.",
      "year" : 2010
    }, {
      "title" : "A continuation method for semi-supervised svms",
      "author" : [ "O. Chapelle", "M. Chi", "A. Zien" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2006
    }, {
      "title" : "Smoothing a program soundly and robustly",
      "author" : [ "S. Chaudhuri", "A. Solar-Lezama" ],
      "venue" : "In Computer Aided Verification,",
      "citeRegEx" : "Chaudhuri and Solar.Lezama.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chaudhuri and Solar.Lezama.",
      "year" : 2011
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "The difficulty of training deep architectures and the effect of unsupervised pre-training",
      "author" : [ "D. Erhan", "P.-A. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Erhan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2009
    }, {
      "title" : "Online convex optimization in the bandit setting: gradient descent without a gradient",
      "author" : [ "A. Flaxman", "A.T. Kalai", "H.B. McMahan" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "On the link between gaussian homotopy continuation and convex envelopes",
      "author" : [ "H. Mobahi", "J.W. Fisher III" ],
      "venue" : "In Energy Minimization Methods in Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Mobahi and III.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mobahi and III.",
      "year" : 2015
    }, {
      "title" : "A theoretical analysis of optimization by gaussian continuation",
      "author" : [ "H. Mobahi", "J.W. Fisher III" ],
      "venue" : null,
      "citeRegEx" : "Mobahi and III.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mobahi and III.",
      "year" : 2015
    }, {
      "title" : "Fast nonconvex nonsmooth minimization methods for image restoration and reconstruction",
      "author" : [ "M. Nikolova", "M.K. Ng", "C.-P. Tam" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "Nikolova et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nikolova et al\\.",
      "year" : 2010
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "A. Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : "arXiv preprint arXiv:1109.5647,",
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2011
    }, {
      "title" : "The computation of visible-surface representations",
      "author" : [ "D. Terzopoulos" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Terzopoulos.,? \\Q1988\\E",
      "shortCiteRegEx" : "Terzopoulos.",
      "year" : 1988
    }, {
      "title" : "The effective energy transformation scheme as a special continuation approach to global optimization with application to molecular conformation",
      "author" : [ "Z. Wu" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Wu.,? \\Q1996\\E",
      "shortCiteRegEx" : "Wu.",
      "year" : 1996
    }, {
      "title" : "Energy functions for early vision and analog networks",
      "author" : [ "A. Yuille" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Yuille.,? \\Q1989\\E",
      "shortCiteRegEx" : "Yuille.",
      "year" : 1989
    }, {
      "title" : "Stereo integration, mean field theory and psychophysics",
      "author" : [ "A.L. Yuille", "D. Geiger", "H. Bülthoff" ],
      "venue" : "In Computer Vision ECCV",
      "citeRegEx" : "Yuille et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Yuille et al\\.",
      "year" : 1990
    }, {
      "title" : "A path following algorithm for the graph matching problem",
      "author" : [ "M. Zaslavskiy", "F. Bach", "J.-P. Vert" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Zaslavskiy et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zaslavskiy et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Of particular interest are non-convex optimization problem that arise in the training of deep neural networks Bengio (2009). Often, such problems admit a multimodal structure, and therefore, the use of convex optimization machinery may lead to a local optima.",
      "startOffset" : 110,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "Of particular interest are non-convex optimization problem that arise in the training of deep neural networks Bengio (2009). Often, such problems admit a multimodal structure, and therefore, the use of convex optimization machinery may lead to a local optima. Graduated optimization (a.k.a. continuation), Blake and Zisserman (1987), is a methodology that attempts to overcome such numerous local optima.",
      "startOffset" : 110,
      "endOffset" : 333
    }, {
      "referenceID" : 5,
      "context" : "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) .",
      "startOffset" : 66,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) .",
      "startOffset" : 66,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) . However, in the general case, it is commonly suggested in the literature to convolve the original function with a gaussian kernel Wu (1996). Yet, this operation is prohibitively inefficient in high dimensions.",
      "startOffset" : 66,
      "endOffset" : 266
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, the next question is raised in Bengio (2009) which reviews recent developments in the field of deep learning: “Can optimization strategies based on continuation methods deliver significantly improved training of deep architectures?” As an initial empirical study, we examine the task of training a NN (Neural Network) over the MNIST data set.",
      "startOffset" : 46,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "Similar attitudes in the machine vision literature appeared later in Yuille (1989); Yuille et al.",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "Similar attitudes in the machine vision literature appeared later in Yuille (1989); Yuille et al. (1990), and Terzopoulos (1988).",
      "startOffset" : 69,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "(1990), and Terzopoulos (1988).",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990).",
      "startOffset" : 68,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al.",
      "startOffset" : 118,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al.",
      "startOffset" : 118,
      "endOffset" : 301
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011).",
      "startOffset" : 118,
      "endOffset" : 344
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al.",
      "startOffset" : 118,
      "endOffset" : 384
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al.",
      "startOffset" : 118,
      "endOffset" : 549
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010).",
      "startOffset" : 118,
      "endOffset" : 590
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al.",
      "startOffset" : 118,
      "endOffset" : 626
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al.",
      "startOffset" : 118,
      "endOffset" : 644
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al.",
      "startOffset" : 118,
      "endOffset" : 741
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation.",
      "startOffset" : 118,
      "endOffset" : 762
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation. These approaches, in the spirit of the continuation method, offer no guarantees on the quality of the obtained solution, and are tailored to specific applications. A comprehensive survey of the graduated optimization literature can be found in Mobahi and Fisher III (2015a). A recent work Mobahi and Fisher III (2015b) advances our theoretical understanding, by analyzing a continuation algorithm in the general setting.",
      "startOffset" : 118,
      "endOffset" : 1063
    }, {
      "referenceID" : 0,
      "context" : "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation. These approaches, in the spirit of the continuation method, offer no guarantees on the quality of the obtained solution, and are tailored to specific applications. A comprehensive survey of the graduated optimization literature can be found in Mobahi and Fisher III (2015a). A recent work Mobahi and Fisher III (2015b) advances our theoretical understanding, by analyzing a continuation algorithm in the general setting.",
      "startOffset" : 118,
      "endOffset" : 1108
    }, {
      "referenceID" : 9,
      "context" : "A proof of Equation (2) is found in Flaxman et al. (2005).",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "Note the following result from Rakhlin et al. (2011) regarding stochastic optimization of σ-strongly-convex functions, given such an oracle: Theorem 5.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "We adopt the experimental setup of Dauphin et al. (2014) and train over a down-scaled version of the data, i.",
      "startOffset" : 35,
      "endOffset" : 57
    } ],
    "year" : 2017,
    "abstractText" : "The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite its popularity, very little is known in terms of theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a parameterized family of nonconvex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an ε-approximate solution within O(1/ε2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of “zero-order optimization”, and devise a a variant of our algorithm which converges at rate of O(d2/ε4).",
    "creator" : "LaTeX with hyperref package"
  }
}