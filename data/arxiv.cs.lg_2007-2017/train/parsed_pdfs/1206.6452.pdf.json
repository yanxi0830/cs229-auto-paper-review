{
  "name" : "1206.6452.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Smoothness and Structure Learning by Proxy",
    "authors" : [ "Benjamin Yackley" ],
    "emails" : [ "benj@cs.unm.edu", "terran@cs.unm.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Probabilistic graphical models such as Bayesian networks (Koller & Friedman, 2009), which explain patterns in data through dependence relations among variables, are a useful tool because of the visibility and ease of interpretation of the models and because of the ability to estimate distributions given known values. However, generating these models from observed data runs into problems in a number of ways. If the data set has too many variables, the number of possible models grows exponentially, and if there are too many data\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\npoints, it becomes more time-intensive to analyze the relationships between them. Although this growth is only linear in the number of data points, modern data sets run into the gigabytes or larger. What is needed is a way to separate the size of the data set from the search process, and this is the purpose of the proxy. By training a function approximator on exact scores of a random sample of networks, we can then use that proxy in the search without ever needing to go back to the original data.\nWe prove here that the BDe scoring function is reasonably smooth over a properly chosen topology, and this fact motivates the use of a Gaussian process regressor as a proxy to the exact function. Once the proxy is built, the original data set need never be touched again, and the search itself can proceed extremely quickly. As our results show, even taking into account the additional time needed to score the training samples and generate the proxy from them, we are often able to generate better-scoring models than an exact-scoring search in a smaller amount of time."
    }, {
      "heading" : "2. Background",
      "text" : ""
    }, {
      "heading" : "2.1. Bayesian networks",
      "text" : "A Bayesian network (Heckerman et al., 1995) is a statistical model used to represent probabilistic relationships among a set of variables as a directed acyclic graph, where the distribution of a single variable is defined in terms of the values of that variable’s parents in the graph. Bayesian networks are commonly used to infer distributions over unobserved or query variables given known values for others (for instance, spam classification (Sebastiani & Ramoni, 2001) or disease diagnosis (Burge et al., 2009)). The process of learning a Bayesian network given a set of observed data is difficult, and is in fact NP-complete in the case of finding an exact optimum (Chickering, 1996; Chickering et al., 1995); most techniques are still limited in\npractice in the number of variables they can handle at once. One key component of many of these algorithms is a score function which, given a fixed data set, maps individual graphs onto real numbers; the optimal network is the one whose graph has the highest score. In other words, the function sc(G|D) maps the space of directed acyclic graphs Gn on n nodes, one for each variable x1 . . . xn, along with a data set D ∈ Nm×n with m i.i.d. observations of those variables, and the desired output of the search process is arg maxG sc(G|D)."
    }, {
      "heading" : "2.2. The BDe score",
      "text" : "There are many Bayesian network scoring functions one could use as a basis for a search, but the BDe score (Heckerman et al., 1995), has several desired properties. First, it is decomposable, meaning that it can be expressed as a function of independent components, one for each node in the graph. Second, it is a Bayesian formulation that allows us to enforce a prior belief over graph structures independent of the data itself. Finally, the structure of the BDe score is straightforward, requiring only counts of queries over the data (which can be made easier using an ADTree (Anderson & Moore, 1998), as described below) and the log-Gamma function, which itself is easily approximated numerically. The form of the BDe score is: sc(G|D) = n∏ i=1 ∏ j∈C(xi) Γ(λij) Γ(λij +Nij) ∏ k∈Vi Γ(λijk +Nijk) Γ(λijk)\n(1)\nHere, the variable i ranges over all of the n nodes of the graph, j ranges over all configurations of the parents of xi, and k over all possible values of xi, which I denote the set Vi. The set C(xi) which j ranges over is defined as a Cartesian product, C(xi) = ∏ Pa(xi)\nVi. Nijk is the count of all data instances where xi = k and the parents of xi are in state j, while Nij = ∑ kNijk. Similarly, λijk is a hyperparameter called a pseudocount, the set of which defines the effect of our prior on the score when the network parameters (the CPTs) are integrated out, and λij = ∑ k λijk."
    }, {
      "heading" : "2.3. Gaussian process regression",
      "text" : "In previous work (Yackley et al., 2008), we showed that a spline-based regression model could be used to estimate the BDe score of a network. However, this particular model turned out to be unsuitable for search; while the values it returned were very close to the exact ones, the gradients (i.e. the differences between the scores of graphs differing in one edge) were mostly wrong. Motivated by this failure, we tried a different\napproach. Gaussian process regression is both mathematically simpler than the previous model and gets the gradients mostly correct.\nThe form of Gaussian process regressor (Rasmussen, 2004) we use is known as simple kriging, and takes the form:\nŷ = K(g, ĝ)K(g, g)−1y (2)\nIn this equation, g ⊆ X = {G1, G2, . . . , Gns} is a set of ns training objects (graphs defining Bayesian networks, in our case) with all of the ys being their corresponding real-valued scores. K is a function that produces a kernel matrix such that [K(g, h)]ij = k(gi, hj), where the positive-definite kernel function k : X×X→ R maps pairs of objects to a value which can be seen as a generalized inner product; the more alike the two objects are, the higher this value will be. ĝ is the new graph (or set of graphs) we are trying to approximate a score for, and ŷ is that resulting score. Once the training data is scored, the matrix K(g, g)−1 need only be calculated once; from then on, finding an approximate score for any previously-unseen graph is just a matter of calculating K(g, ĝ) and performing the matrix multiplications.\nFinding a proper kernel function on a given set X is, in general, not a trivial task. However, because our objects are graphs which will always be on the same ordered collection of nodes, we can compare each of the ( n 2 ) possible edges directly between the two graphs. The form our kernel function takes is:\nk(G1, G2) = ∑ e weI[e ∈ G1 ∧ e ∈ G2] (3)\nThe sum runs over all possible edges of the graph, adding a weight we to the kernel’s value if that edge is present in both graphs. The weights are tuned using the marginal likelihood gradient 1; although this process involves repeatedly taking a matrix inverse until the values converge, the size of this matrix is only ns × ns, and thus, with a small number of training samples, this is relatively fast."
    }, {
      "heading" : "3. Motivation",
      "text" : "To do fast structure learning, we want to create a proxy to the exact score function, and this proxy must have two key traits - it must be quick to evaluate, and it must be a good approximation to the true function. Using a Gaussian process regressor gets us the first; once trained, its calculation is a simple matrix\n1See (Rasmussen, 2004), Equation 5.9\nproduct. To get the second, however, we need to know that the true function we are approximating is smooth enough for a Gaussian process to model. This requires, in turn, that we define some topology over the set of directed graphs over which we can say the function is smooth.\nThe topology we use here, which we call the metagraph (Yackley et al., 2008), is defined as the graph of some relation over a set of combinatorial objects. In this case, the objects are themselves directed graphs, and the relation between them is that of differing in exactly one edge. It has two desired properties that make it attractive as a topology over which to search. First, the edges correspond to the search operations we perform - addition and deletion of edges of the target graph. Second, the structure is highly symmetric, taking the form of a hypercube with dimension equal to the possible number of edges of the target. Note that, although they are not valid as Bayesian networks, the metagraph nevertheless includes graphs which contain loops. This is not a problem for an approximator; none of the training structures will contain loops, and a search will still be constrained to that part of the space with no loops.\nFurthermore, there is no danger of the approximator being asked to score a graph with cycles (even though the approximation would work mathematically, the answer it returned would be meaningless). Between any two acyclic graphs, a path must exist which never encounters a graph with a cycle; this is trivially proven by considering the process of removing every edge from the first target graph, resulting in a graph with no edges, and then adding back all edges in the second. In general, a shorter path will exist, but this serves as a proof that the region of the metagraph corresponding only to acyclic graphs is fully connected."
    }, {
      "heading" : "4. Analysis of smoothness of BDe score",
      "text" : ""
    }, {
      "heading" : "4.1. Notation",
      "text" : "Let the data set D ∈ Nm×n denote a data matrix of discrete values consisting of m i.i.d. observations of n variables. Denote a Bayesian network over these variables as having the graph G and parameters Θ, where G =< X,E > and X, the set of variables equals {x1, x2, . . . xn}. A score function sc(G|D) maps graphs onto real numbers given a fixed data set, with the convention that a higher score denotes a graph modeling a better explanation of the data. Each variable xi has a corresponding finite set of possible values Vi and a possibly-empty set of parents in the graph Pa(xi). The set of parent configurations Ci for node xi is given by\nthe Cartesian product Ci = ∏ xj∈Pa(xi) Vj . The notation Nijk denotes the count across the entire data set of the number of instances where xi = k and each variable in Pa(xi) takes on a value as given by configuration j ∈ Ci. Also, Nij = ∑ k∈Vi Nijk.\nThe hyperparameter λijk, needed for the BDe function below, indicates the strength of prior beliefs on the score of a network, needed for a proper Bayesian formulation. As with the Ns, λij = ∑ k∈Vi λijk."
    }, {
      "heading" : "4.2. Basic Definitions",
      "text" : "Consider the standard definition of the BDe score, as given in equation 1. In practice, we are more concerned with its logarithm:\nlog sc(G|D) = ∑ i ∑ j∈Ci ( log Γ(λij)− log Γ(λij +Nij)\n+ ∑ k∈Vi (log Γ(λijk +Nijk)− log Γ(λijk)) ) (4)\nWe assume here that the form of the prior is such that λijk is equal for all k given a fixed i and j, and that this value is inversely proportional to the cardinality of Ci. In other words, λij = ∑ k λijk = #(Vi)λijk. Assuming that all nodes are binary, then we simply have λij = 2λijk for all i and j, and all subscripted λs are proportional to some base λ. Note also that if all nodes are binary, then #(Vi) = 2 for all i, and #(Ci) = 2 #(Pa(xi)).\nIn order to prove smoothness, we wish to find upper and lower bounds on the magnitude of the change in score given the addition or deletion of an edge in the graph. Without loss of generality, assume that we add an edge. Call the graph before addition G, and the graph after addition G′, with scores sc and sc′ given the same data set D. Because the score takes the form of a sum over all nodes of the graph, the difference between sc and sc′ can be captured solely by a single term of the outermost sum, representing the node the new arc points to – call this node x∆. We can therefore drop the i subscripts in the formula itself, and represent the one differing term of the two sums using sc∆ and sc′∆ respectively. Because all other terms of the sum remain unchanged, sc − sc′ = sc∆ − sc′∆. Also, the range of the initial j variable, C∆, now splits into two sets, C0 and C1, where the subscript indicates the value of the newly added parent. For each element of C∆, there is a corresponding element both in C0 and C1, and #(C∆) = #(C0) = #(C1)."
    }, {
      "heading" : "4.3. Form of the bound on sc∆ − sc′∆",
      "text" : "From the above, we have:\nsc∆ = ∑ j∈C∆ ( log Γ(λj)− log Γ(λj +Nj)+\n∑ k∈V (log Γ(λjk +Njk)− log Γ(λjk))\n)\n= ∑ j∈C∆ ( log Γ(λj)− log Γ(λj +Nj) + log Γ(λj0 +Nj0)\n− log Γ(λj0) + log Γ(λj1 +Nj1)− log Γ(λj1) )\nSince λj0 = λj1 = λj/2 and Nj = Nj0 + Nj1, we can simplify this to: sc∆ = ∑ j∈C∆ (α− log Γ(λj +Nj0 +Nj1)+\nlog Γ( λj 2 +Nj0) + log Γ( λj 2 +Nj1)),\n(5)\nwhere α = log Γ(λij) − 2 log Γ(λij/2). The only difference between sc∆ and sc ′ ∆ is the set over which j ranges; if we abbreviate the preceding sum as sc∆ = ∑ j∈C f(j), then sc ′ ∆ = ∑ j∈C0∪C1 f(j) =∑\nj∈C0 f(j)+ ∑ j∈C1 f(j). Note, however, that the two addends each take the same form as the expression for sc∆, and that the sets C0 and C1 have the same size as C, with all elements in a one-to-one correspondence. With some relabeling of variables, we have:\nsc∆ − sc′∆ = ∑ j∈C f(j)− ∑ j0∈C0 f(j0)− ∑ j1∈C1 f(j1)\n= ∑ j∈C∆ (f(j)− f(j0)− f(j1))\nSome notation abuse takes place in the second equation; j0 and j1 are the corresponding configurations in C0 and C1 to j in C∆, with the value in the additional parent being 0 or 1 respectively. Because of this, Nj0k + Nj1k = Njk for any k, and likewise Nj0 + Nj1 = Nj . Also, we have λj0 = λj1 = λj/2. Corresponding to the above definition of α, let β = log Γ(λij/2) − 2 log Γ(λij/4). Even making these simplifications, the full expression for sc∆ − sc′∆ expands to a cumbersome form; to simplify it further, we introduce an auxiliary function denoted as γ."
    }, {
      "heading" : "4.4. The function γ(a, b)",
      "text" : "Let the function γ(a, b) be defined as follows2:\nγ(a, b) = log Γ(a+ b)− log Γ(a)− log Γ(b) (6) 2This function is related to the standard Beta function;\nγ(a, b) = − log B(a, b)\nUsing Stirling’s approximation for the log-gamma function (Abramowitz & Stegun, 1964) (lnx! = x lnx + x − Θ(x)), we obtain a result which will be important later:\nγ(a, a) = 2a log(2a)− 2a+ Θ(log 2a) −2(a log a− a+ Θ(log a))\n= (2 log 2)a+ Θ(log a) (7)\nNow, we can use γ to simplify the equation for sc∆ − sc′∆, given that Nj00 +Nj10 = Nj0 and Nj01 +Nj11 = Nj1. We also split out the term inside the sum and call it t, for reasons given below.\nt = γ( λj 4 +Nj00, λj 4 +Nj10)\n+ γ( λj 4 +Nj01, λj 4 +Nj11) − γ(λj 2 +Nj00 +Nj01, λj 2 +Nj10 +Nj11)\n(8)\nsc∆ − sc′∆ = 2#(Pa(xi))(α− 2β) + ∑ j∈C∆ t (9)"
    }, {
      "heading" : "4.5. Getting to the extrema",
      "text" : "We seek upper and lower bounds on sc∆ − sc′∆ given fixed λj (and therefore fixed α and β as well). We therefore differentiate the equation with respect to the four Ns and set the four derivatives all equal to zero. Because the sum over j ∈ Ci is irrelevant (a sum over any number of worst cases will produce a worst case, and likewise for best cases), we only need to calculate bounds for t, which we will accomplish by taking its derivative with respect to the four Npq variables to find its minimum and maximum.\nBecause t is defined in terms of γ, which is itself defined in terms of the log Γ function, the results will involve the ψ function3. For space reasons, we abbreviate expressions of the form ψ(\nλj 2 + Njab + Njcd) as\nψab,cd. ψab stands for ψ( λj 4 +Njab), and ψ alone stands for ψ(λj +Nj00 +Nj01 +Nj10 +Nj11). Using these abbreviations, the derivative of t with respect to some Nab is:\ndt\ndNjab = −ψab + ψa0,a1 + ψ0b,1b − ψ (10)\nTaking the four derivatives of t and setting them equal to zero, we obtain the system of equations:\nψ00 + ψ = ψ00,01 + ψ00,10\nψ01 + ψ = ψ00,01 + ψ01,11\nψ10 + ψ = ψ10,11 + ψ00,10\nψ11 + ψ = ψ10,11 + ψ01,11\n(11)\n3Defined the standard way as ψ(x) = d dx log Γ(x)\nBy subtracting pairs of equations, we obtain: ψ00 − ψ01 = ψ00,10 − ψ01,11 ψ00 − ψ10 = ψ00,01 − ψ10,11 ψ01 − ψ11 = ψ00,01 − ψ10,11 ψ10 − ψ11 = ψ00,10 − ψ01,11\n(12)\nOne solution is apparent from inspection. If we set Nj00 = Nj10 = Nj01 = Nj11, then all four equations reduce to 0 = 0. One of our extrema, therefore, occurs there, corresponding to the case where we add an edge to split apart data which is already uniformly distributed in both variables corresponding to the edge’s endpoints. In other words, this edge has no reason to exist in a Bayesian network, and should logically decrease the score by the most; this is a maximum.\nmax sc∆ − sc′∆ = 2#(Pa(xi))(α− 2β)\n+ ∑ j∈Ci ( 2γ( λj 4 +Nj00, λj 4 +Nj00)\n−γ(λj 2 + 2Nj00, λj 2 + 2Nj00)\n) (13)\nSince we are only concerned with the asymptotic behavior of this function, we can drop the constant terms as well as the summation (which is over a constant number of terms independent of the value of any of the Ns).\nmax sc∆ − sc′∆ = O (\n2γ( λj 4 +N, λj 4 +N)\n−γ(λj 2 + 2N, λj 2 + 2N)\n) (14)\nFrom equation 7, we obtain: max sc∆ − sc′∆ = O ((4 log 2)(λj/4 +N) −Θ(logN)− (2 log 2)(λj/2 + 2N) + Θ(logN))\n= O((log 2)(λj + 4N)− (log 2)(λj + 4N) + Θ(logN)) = O(logN)\nThis indicates that, in cases where adding an edge lowers the score, the worst it can lower it by is only logarithmic in the number of data points.\nThe other solutions to the system occur where Nj10 = Nj01 = 0 or Nj00 = Nj11 = 0, representing data which (in our binary-variable case) is perfectly aligned in such a way that both the marginal of the node and its new parent seem uniform, but adding the edge reveals their values to be in perfect correspondence with one another. The reasoning behind is is as follows.\nj = 0 j = 1 k = 0 0.5 0 k = 1 0 0.5\nt = γ( λj 4 + N 2 , λj 4 )+γ( λj 4 , λj 4 + N 2 )−γ( λj 2 + N 2 , λj 2 + N 2 )\nConsider our expression for t above. The minimum value occurs when the first two (positive) terms of the sum are minimized and the negative term is maximized. Because we know from section 4.4 that the γ function is maximized when the arguments are equal and minimized when they are farthest apart, we can force this to happen by setting Nj10 = Nj01 = 0 or Nj00 = Nj11 = 0 and the other two variables equal to one another. This case corresponds to having a marginal distribution over both variables which is uniform, but where the joint indicates a perfect correspondence between the two. This is exactly the sort of situation where an edge ought to be added.\nmin sc∆ − sc′∆ = O ( γ( λj 4 +N, λj 4 )\n+γ( λj 4 , λj 4 +N)− γ(λj 2 +N, λj 2 +N)\n)\nBecause γ(a, b) is maximized for a fixed a + b when a = b, we can say that γ(λj/4 +N,λj/4) < γ(λj/4 + N/2, λj/4 +N/2), and so\nminsc∆ − sc′∆ < O (\n2γ( λj 4 + N 2 , λj 4 + N 2 )\n−γ(λj 2 +N, λj 2 +N) ) =O((4 log 2)(λj/4 +N/2)\n− (2 log 2)(λj/2 +N) + Θ(logN)) =O((log 2)(λj + 2N)\n− (log 2)(λj + 2N) + Θ(logN)) =O(logN)\nBoth the minimum and maximum score jumps, then, are simply logarithmic in the number of data points, showing that, with respect to a topology derived from addition and deletion of edges, the BDe score is Lipschitz smooth with a constant of K = O(logN)."
    }, {
      "heading" : "4.6. Implications",
      "text" : "As one would expect, the worst case scenario is to add an edge that provides no information at all. If the joint distribution between xi and its new parent is uniform, the model gains nothing by putting the edge there, while the score (as it should) penalizes the addition. The best case, meanwhile, is for the new edge to link xi to a parent that perfectly matches its values (or at least a permutation of them) in all cases, while the marginals of the joint distribution are entirely uniform and uninformative. These fit our intuitions of how edges in a Bayesian network should be interpreted. Also, because the worst possible changes to the score are merely logarithmic in the size of the data set, the search landscape is sufficiently smooth that a Gaussian Process regressor is an appropriate choice to represent it.\nThe Gaussian process regressor is a good choice for another reason – the fact that it is based on a kernel function means that its complexity is not based on the size of the training set or the size of the graphs (or, for that matter, the size of the original data set), but the VC dimension of the kernel space (Schölkopf & Smola, 2001).\nNote also that, once the training set is scored, there is no longer a need to keep around the original data set – all of the information we need to search has been encapsulated into the proxy. This is a clear win in the case where the data set has a large number of instances; instead of needing to count up values for Nijk across perhaps millions of data points every time we take a search step, we can simply refer to the proxy."
    }, {
      "heading" : "4.7. Other score functions",
      "text" : "It is an open question, and one we hope to address in the future, whether the same kind of smoothness bound can be proven for other Bayesian network score functions. For example, the BIC score (Schwarz, 1978) is defined as follows, in terms of a log-likelihood score and a penalty term.\nscBIC(G|D) = n∑ i=1 ∑ j∈Ci ∑ k∈Vi Nijk log ( Nijk Nij ) − 1\n2 log(m)|B|\n(15)\n|B| = ∑n i=1(#Vi − 1)#Ci is the number of degrees of freedom across the parameter set Θ. In this form, adding an edge to a network will split the set of parent configurations, as before, by adding another term to the product which defines Ci. However, it will also alter the value of the penalty term |B|."
    }, {
      "heading" : "5. Proxy-Accelerated Search Results",
      "text" : "To compare the effects of the proxy to an exact-scoring search, we selected six data sets on which to build Bayesian networks. Three of these, Adult1, Adult2, and Adult3, came from the original paper that introduced the ADTree (Anderson & Moore, 1998), where they were used as examples of data sets an ADTree could be built on. The ADTree is a structure which provides a caching mechanism to accelerate the process of scoring; it trades off an initial tree-build time and the memory needed to store the structure to achieve much faster speed at the kind of Nijk counts necessary to compute a BDe score. The results for those three data sets show that, even with ADTree-based acceleration, we are able to find comparable scores in much less time using the proxy. The proxy-based search was performed 5 times with randomly selected training samples each time; the results shown here are the mean and standard deviation. The algorithm was a standard greedy search, chosen to be a reasonable baseline. It should be mentioned, though, that the benefit of using a proxy would extend, in theory, to any search algorithm that uses a scoring function.\nThe scores of the graphs as reported in the table are exact, not derived from the proxy. Although the values that the proxy returns are often very far off from exact, the gradients remain intact, and this is why we can count on the proxy to drive a search in the right direction. The values for ns reported in Table 5 are those for which our proxy performed best; experiments were conducted for a small range of different values for ns.\nThe other three data sets are taken from the UCI Data Repository (Frank & Asuncion, 2010); they are Census-Income, Tic2000, and Musk. All of these are too large for an ADTree to fit in memory, and so the scores were calculated using the Bayes Net Toolkit (Murphy, 2001) and its accompanying Structure Learning Package (Leray & Francois, 2004). The proxy-based searches on Census-Income and Tic2000 were performed five times, as above, while the Musk data set was large enough that it was only practical to perform a single search for each differing number of training samples. The algorithms were implemented in Matlab, on a Linux server running at 2.2 GHz with 32 gigabytes of RAM."
    }, {
      "heading" : "5.1. Discussion",
      "text" : "The effects of the proxy are clear; in all but one case, the networks found by the proxy-based search were either comparable to or significantly better than those found by the exact-scoring version, and always in a\nshorter time. At present, we don’t know what property of the Census-Income data set made it perform so poorly.\nIn every other case, however, the advantage of the smoothing induced by the proxy is clear, and this is most dramatic in the case of the Musk data set. With a relatively tiny number samples across the immense space of networks on 168 nodes, the proxy was nevertheless able to find a network with a greatly improved score. The reason for this — and the reason smoothness is so important — is shown in Figure 5.1. These lines are the search trajectories, with search step on the x axis and score on the y axis. The thick line is the trajectory taken by the exact-scoring search, while the thinner blue lines are the ones taken by five runs of the proxy with different sets of 50 training samples. The exact search stops partway through, having encountered a local maximum. However, the proxy will tend to smooth these local features out, letting the search process continue to greater heights. In fact, too many training samples can in fact hamper the proxy’s even-\n−13\n−12\n−11\n−10\ntual score. Figure 5.1 plots each of the proxy’s runs with a different value of ns as a bubble in a time-score graph, with the size of the oval being one standard deviation in either dimension. The lowest value, ns = 5, is at the bottom left, having taken a very short overall time but producing a relatively bad network. The bubbles continue up and to the right, with both time and score growing, until we reach the farthest-right point on the graph when ns = 60. From there, the time continues to increase, but the score worsens. We believe that this is due to the proxy starting to learn the space too well, capturing the finer features of the score landscape while losing sight of the bigger picture."
    }, {
      "heading" : "6. Future Work",
      "text" : "We are currently working on extending the proxy to other score-based search strategies, such as simulated\nannealing (Kirkpatrick et al., 1983), as well as to other combinatorial objects such as general 0-1 matrices and permutations. The success of these rests, it would seem, on finding a proper form for a kernel function on these objects, thus defining the topology of the space both traversed by the search method and used by the approximator.\nAnother direction we wish to extend this in is to implement the training phase on a massively parallel system, which would greatly reduce the time taken to train the proxy. This would also require the implementation of a way to combine the training results; a block-matrix inversion technique will be useful here, as well as adding the potential to add more training data in the middle of an ongoing search. This way, the space around an apparent local maximum could be examined in greater detail and refined."
    }, {
      "heading" : "7. Conclusion",
      "text" : "As data sets increase in size, it becomes more necessary to develop algorithms which can search for and identify models of them in reasonable amounts of time. However, the larger the data set gets, the more time this takes, and the larger the search space, the more chance there is of a search running into a local maximum instead of the desired global. A proxy function will alleviate both of these problems; in particular, we showed that the BDe score considered over a search space of single-edge additions and deletions is smooth enough to make a proxy-based search viable, and the results bear this out.\nThis process, building a proxy function from a set of random samples and then using it to drive a search, is readily applicable to any search algorithm that depends on calculating a series of scores, from a simple greedy search to more sophisticated ones such as Markov Chain Monte Carlo. These new accelerated forms of algorithms will allow researchers in fields as diverse as astronomy (Kent, 1994), biology (Roy et al., 2007), and linguistics (Davies, 2009) to better analyze data and create hypotheses given their often staggeringly large data sets. Through the use of the proxybased search accelerator, we will be able to find patterns in more complex data than had previously been feasible."
    }, {
      "heading" : "8. Acknowledgements",
      "text" : "The authors wish to thank Blake Anderson and Eduardo Corona for their ideas and support, as well as the Machine Learning Reading Group at the University of New Mexico. This research was supported by\nNational Science Foundation grant IIS-0705681 and Office of Naval Research grant N000141110139."
    } ],
    "references" : [ {
      "title" : "Handbook of mathematical functions with formulas, graphs, and mathematical tables",
      "author" : [ "M. Abramowitz", "I.A. Stegun" ],
      "venue" : "Dover publications,",
      "citeRegEx" : "Abramowitz and Stegun,? \\Q1964\\E",
      "shortCiteRegEx" : "Abramowitz and Stegun",
      "year" : 1964
    }, {
      "title" : "ADtrees for fast counting and for fast learning of association rules",
      "author" : [ "B. Anderson", "A. Moore" ],
      "venue" : "In Knowledge Discovery from Databases Conference,",
      "citeRegEx" : "Anderson and Moore,? \\Q1998\\E",
      "shortCiteRegEx" : "Anderson and Moore",
      "year" : 1998
    }, {
      "title" : "Discrete dynamic bayesian network analysis of fmri data",
      "author" : [ "J. Burge", "T. Lane", "H. Link", "S. Qiu", "V.P. Clark" ],
      "venue" : "Human Brain Mapping,",
      "citeRegEx" : "Burge et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Burge et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning bayesian networks is npcomplete",
      "author" : [ "D.M. Chickering" ],
      "venue" : "Learning from data: Artificial intelligence and statistics,",
      "citeRegEx" : "Chickering,? \\Q1996\\E",
      "shortCiteRegEx" : "Chickering",
      "year" : 1996
    }, {
      "title" : "Learning bayesian networks: Search methods and experimental results",
      "author" : [ "D.M. Chickering", "D. Geiger", "D. Heckerman" ],
      "venue" : "In Proceedings of Fifth Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Chickering et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Chickering et al\\.",
      "year" : 1995
    }, {
      "title" : "The 385+ million word corpus of contemporary american english (19902008+): Design, architecture, and linguistic insights",
      "author" : [ "M. Davies" ],
      "venue" : "International Journal of Corpus Linguistics,",
      "citeRegEx" : "Davies,? \\Q2009\\E",
      "shortCiteRegEx" : "Davies",
      "year" : 2009
    }, {
      "title" : "Learning bayesian networks: The combination of knowledge and statistical data",
      "author" : [ "D. Heckerman", "D. Geiger", "D.M. Chickering" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1995
    }, {
      "title" : "Optimization by simulated annealing",
      "author" : [ "S. Kirkpatrick", "D.G. Jr.", "M.P. Vecchi" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 1983
    }, {
      "title" : "Probabilistic graphical models: principles and techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedman,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "BNT structure learning package: Documentation and experiments",
      "author" : [ "P. Leray", "O. Francois" ],
      "venue" : "Laboratoire PSI, Tech. Rep,",
      "citeRegEx" : "Leray and Francois,? \\Q2004\\E",
      "shortCiteRegEx" : "Leray and Francois",
      "year" : 2004
    }, {
      "title" : "The bayes net toolbox for matlab",
      "author" : [ "K. Murphy" ],
      "venue" : "Computing science and statistics,",
      "citeRegEx" : "Murphy,? \\Q2001\\E",
      "shortCiteRegEx" : "Murphy",
      "year" : 2001
    }, {
      "title" : "Gaussian processes in machine learning",
      "author" : [ "C.E. Rasmussen" ],
      "venue" : "Advanced Lectures on Machine Learning,",
      "citeRegEx" : "Rasmussen,? \\Q2004\\E",
      "shortCiteRegEx" : "Rasmussen",
      "year" : 2004
    }, {
      "title" : "Integrative construction and analysis of condition-specific biological networks",
      "author" : [ "S. Roy", "T. Lane", "M. Warner-Washburne" ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "Roy et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf and Smola,? \\Q2001\\E",
      "shortCiteRegEx" : "Schölkopf and Smola",
      "year" : 2001
    }, {
      "title" : "Estimating the dimension of a model",
      "author" : [ "G. Schwarz" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Schwarz,? \\Q1978\\E",
      "shortCiteRegEx" : "Schwarz",
      "year" : 1978
    }, {
      "title" : "On the use of Bayesian networks to analyze survey data",
      "author" : [ "P. Sebastiani", "M. Ramoni" ],
      "venue" : "Research in Official Statistics,",
      "citeRegEx" : "Sebastiani and Ramoni,? \\Q2001\\E",
      "shortCiteRegEx" : "Sebastiani and Ramoni",
      "year" : 2001
    }, {
      "title" : "Bayesian network score approximation using a metagraph kernel",
      "author" : [ "B. Yackley", "E. Corona", "T. Lane" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Yackley et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Yackley et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "A Bayesian network (Heckerman et al., 1995) is a statistical model used to represent probabilistic relationships among a set of variables as a directed acyclic graph, where the distribution of a single variable is defined in terms of the values of that variable’s parents in the graph.",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "Bayesian networks are commonly used to infer distributions over unobserved or query variables given known values for others (for instance, spam classification (Sebastiani & Ramoni, 2001) or disease diagnosis (Burge et al., 2009)).",
      "startOffset" : 208,
      "endOffset" : 228
    }, {
      "referenceID" : 3,
      "context" : "The process of learning a Bayesian network given a set of observed data is difficult, and is in fact NP-complete in the case of finding an exact optimum (Chickering, 1996; Chickering et al., 1995); most techniques are still limited in",
      "startOffset" : 153,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "The process of learning a Bayesian network given a set of observed data is difficult, and is in fact NP-complete in the case of finding an exact optimum (Chickering, 1996; Chickering et al., 1995); most techniques are still limited in",
      "startOffset" : 153,
      "endOffset" : 196
    }, {
      "referenceID" : 6,
      "context" : "There are many Bayesian network scoring functions one could use as a basis for a search, but the BDe score (Heckerman et al., 1995), has several desired properties.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "In previous work (Yackley et al., 2008), we showed that a spline-based regression model could be used to estimate the BDe score of a network.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "The form of Gaussian process regressor (Rasmussen, 2004) we use is known as simple kriging, and takes the form:",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 11,
      "context" : "See (Rasmussen, 2004), Equation 5.",
      "startOffset" : 4,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "The topology we use here, which we call the metagraph (Yackley et al., 2008), is defined as the graph of some relation over a set of combinatorial objects.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "For example, the BIC score (Schwarz, 1978) is defined as follows, in terms of a log-likelihood score and a penalty term.",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "All of these are too large for an ADTree to fit in memory, and so the scores were calculated using the Bayes Net Toolkit (Murphy, 2001) and its accompanying Structure Learning Package (Leray & Francois, 2004).",
      "startOffset" : 121,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "annealing (Kirkpatrick et al., 1983), as well as to other combinatorial objects such as general 0-1 matrices and permutations.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "These new accelerated forms of algorithms will allow researchers in fields as diverse as astronomy (Kent, 1994), biology (Roy et al., 2007), and linguistics (Davies, 2009) to better analyze data and create hypotheses given their often staggeringly large data sets.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : ", 2007), and linguistics (Davies, 2009) to better analyze data and create hypotheses given their often staggeringly large data sets.",
      "startOffset" : 25,
      "endOffset" : 39
    } ],
    "year" : 2012,
    "abstractText" : "As data sets grow in size, the ability of learning methods to find structure in them is increasingly hampered by the time needed to search the large spaces of possibilities and generate a score for each that takes all of the observed data into account. For instance, Bayesian networks, the model chosen in this paper, have a super-exponentially large search space for a fixed number of variables. One possible method to alleviate this problem is to use a proxy, such as a Gaussian Process regressor, in place of the true scoring function, training it on a selection of sampled networks. We prove here that the use of such a proxy is well-founded, as we can bound the smoothness of a commonly-used scoring function for Bayesian network structure learning. We show here that, compared to an identical search strategy using the network’s exact scores, our proxy-based search is able to get equivalent or better scores on a number of data sets in a fraction of the time.",
    "creator" : "LaTeX with hyperref package"
  }
}