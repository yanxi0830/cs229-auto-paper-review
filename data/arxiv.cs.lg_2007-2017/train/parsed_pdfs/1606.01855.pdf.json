{
  "name" : "1606.01855.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations",
    "authors" : [ "Aaron Schein", "Mingyuan Zhou", "David M. Blei", "Hanna Wallach" ],
    "emails" : [ "ASCHEIN@CS.UMASS.EDU", "MINGYUAN.ZHOU@MCCOMBS.UTEXAS.EDU", "DAVID.BLEI@COLUMBIA.EDU", "WALLACH@MICROSOFT.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Like their inhabitants, countries interact with one another: they consult, negotiate, trade, threaten, and fight. These interactions are seldom uncoordinated. Rather, they are connected by a fabric of overlapping communities, such as security coalitions, treaties, trade cartels, and military alliances. For example, OPEC coordinates the petroleum export policies of its thirteen member countries, LAIA fosters trade among Latin American countries, and NATO guarantees collective defense against attacks by external parties.\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nA single country can belong to multiple communities, reflecting its different identities. For example, Venezuela— an oil-producing country and a Latin American country—is a member of both OPEC and LAIA. When Venezuela interacts with other countries, it sometimes does so as an OPEC member and sometimes does so as a LAIA member.\nCountries engage in both within-community and betweencommunity interactions. For example, when acting as an OPEC member, Venezuela consults with other OPEC countries, but trades with non-OPEC, oil-importing countries. Moreover, although Venezuela engages in betweencommunity interactions when trading as an OPEC member, it engages in within-community interactions when trading as a LAIA member. To understand or predict how countries interact, we must account for their community memberships and how those memberships influence their actions.\nIn this paper, we take a new approach to learning overlapping communities from interaction events of the form “country i took action a toward country j at time t.” A data set of such interaction events can be represented as either 1) a set of event tokens, 2) a tensor of event type counts, or 3) a series of weighted multinetworks. Models that use the token representation naturally yield efficient inference algorithms, models that use the tensor representation exhibit good predictive performance, and models that use the network representation learn latent structure that aligns with well-known concepts such as communities. Previous models of interaction event data have each used a subset of these representations. Our approach—Bayesian Poisson Tucker decomposition (BPTD)—takes advantage of all three. ar X\niv :1\n60 6.\n01 85\n5v 1\n[ st\nat .M\nL ]\nBPTD builds on the classic Tucker decomposition (Tucker, 1964) to factorize a tensor of event type counts into three factor matrices and a four-dimensional core tensor (section 2). The factor matrices embed countries into communities, action types into “topics,” and time steps into “regimes.” The core tensor interacts communities, topics, and regimes. The country–community factors enable BPTD to learn overlapping community memberships, while the core tensor enables it to learn directed community–community interaction networks specific to topics of action types and temporal regimes. Figure 1 illustrates this structure. BPTD leads to an efficient MCMC inference algorithm (section 4) and achieves better predictive performance than related models (section 6). Finally, BPTD discovers interpretable latent structure that agrees with our knowledge of international relations (section 7)."
    }, {
      "heading" : "2. Bayesian Poisson Tucker Decomposition",
      "text" : "We can represent a data set of interaction events as a set of N event tokens, where a single token en = (i\na−→j, t) indicates that sender country i ∈ [V ] took action a ∈ [A] toward receiver country j ∈ [V ] during time step t ∈ [T ]. Alternatively, we can aggregate these event tokens into a four-dimensional tensor Y , where element y(t)\ni a−→j is a count\nof the number of events of type (i a−→j, t). This tensor will be sparse because most event types never actually occur in practice. Finally, we can equivalently view this count tensor as a series of T weighted multinetwork snapshots, where the weight on edge i a−→j in the tth snapshot is y(t)\ni a−→j .\nBPTD models each element of count tensor Y as\ny (t)\ni a−→j ∼ Po ( C∑ c=1 θic C∑ d=1 θjd K∑ k=1 φak R∑ r=1 ψtr λ (r) c k−→d ) , (1)\nwhere θic, θjd, φak, ψtr, and λ (r)\nc k−→d\nare positive real num-\nbers. Factors θic and θjd capture the rates at which countries i and j participate in communities c and d, respectively; factor φak captures the strength of association between action a and topic k; and ψtr captures how well regime r explains the events in time step t. We can collectively view the V × C country–community factors as a latent factor matrix Θ, where the ith row represents country i’s community memberships. Similarly, we can view the A×K action–topic factors and the T×R time-step–regime factors as latent factor matrices Φ and Ψ, respectively. Factor λ(r)\nc k−→d\ncaptures the rate at which community c takes ac-\ntions associated with topic k toward community d during regime r. The C × C × K × R such factors form a core tensor Λ that interacts communities, topics, and regimes.\nThe country–community factors are gamma-distributed,\nθic ∼ Γ(αi, βi) , (2)\nwhere the shape and rate parameters αi and βi are specific to country i. We place an uninformative gamma prior over these shape and rate parameters: αi, βi ∼ Γ( 0, 0). This hierarchical prior enables BPTD to express heterogeneity in the countries’ rates of activity. For example, we expect that the US will engage in more interactions than Burundi.\nThe action–topic and time-step–regime factors are also gamma-distributed; however, we assume that these factors are drawn directly from an uninformative gamma prior,\nφak, ψtr ∼ Γ( 0, 0) . (3)\nBecause BPTD learns a single embedding of countries into communities, it preserves the traditional network-based notion of community membership. Any sender–receiver asymmetry is captured by the core tensor Λ, which we can\nview as a compression of count tensor Y . By allowing on-diagonal elements, which we denote by λ(r)\nc k and off-\ndiagonal elements to be non-zero, the core tensor can represent both within- and between-community interactions.\nThe elements of the core tensor are gamma-distributed,\nλ (r)\nc k ∼ Γ ( η c η ↔ c νkρr, δ ) (4)\nλ (r) c k−→d ∼ Γ(η↔c η↔d νkρr, δ) c 6= d. (5)\nEach community c ∈ [C] has two positive weights η c and η↔c that capture its rates of within- and betweencommunity interaction, respectively. Each topic k ∈ [K] has a positive weight νk, while each regime r ∈ [R] has a positive weight ρr. We place an uninformative prior over the within-community interaction rates and gamma shrinkage priors over the other weights: η c ∼ Γ( 0, 0), η↔c ∼ Γ(γ0 /C, ζ), νk ∼ Γ(γ0 /K, ζ), and ρr ∼ Γ(γ0 /R, ζ). These priors bias BPTD toward learning latent structure that is sparse. Finally, we assume that δ and ζ are drawn from an uninformative gamma prior: δ, ζ ∼ Γ( 0, 0).\nAs K → ∞, the topic weights and their corresponding action–topic factors constitute a drawGK = ∑∞ k=1 νk 1φk from a gamma process (Ferguson, 1973). Similarly, as R → ∞, the regime weights and their corresponding time-step–regime factors constitute a draw GR =∑∞ r=1 ρr 1ψr from another gamma process. As C → ∞, the within- and between-community interaction weights and their corresponding country–community factors constitute a draw GC = ∑∞ c=1 η ↔ c 1θc from a marked gamma process (Kingman, 1972). The mark associated with atom θc = (θ1c, . . . , θVc) is η c . We can view the elements of the core tensor and their corresponding factors as a draw G = ∑∞ c=1 ∑∞ d=1 ∑∞ k=1 ∑∞ r=1 λ (r)\nc k−→d\n1θc,θd,φk,ψr from a\ngamma process, provided that the expected sum of the core tensor elements is finite. This multirelational gamma process extends the relational gamma process of Zhou (2015).\nProposition 1: In the limit as C,K,R →∞, the expected sum of the core tensor elements is finite and equal to\nE  ∞∑ c=1 ∞∑ k=1 ∞∑ r=1 λ(r) c k + ∑ d6=c λ (r) c k−→d  = 1 δ ( γ30 ζ3 + γ40 ζ4 ) .\nWe prove this proposition in the supplementary material."
    }, {
      "heading" : "3. Connections to Previous Work",
      "text" : "Poisson CP decomposition: DuBois & Smyth (2010) developed a model that assigns each event token (ignoring time steps) to one of Q latent classes, where each class q ∈ [Q] is characterized by three categorical distributions—θ→q\nover senders, θ←q over receivers, and φq over actions—i.e.,\nP (en=(i a−→j, t) | zn=q) = θ→iq θ←jq φaq. (6)\nThis model is closely related to the Poisson-based model of Schein et al. (2015), which explicitly uses the canonical polyadic (CP) tensor decomposition (Harshman, 1970) to factorize count tensor Y into four latent factor matrices. These factor matrices jointly embed senders, receivers, action types, and time steps into a Q-dimensional space,\ny (t)\ni a−→j ∼ Po ( Q∑ q=1 θ→iq θ ← jq φaq ψtq ) , (7)\nwhere θ→iq , θ ← jq , φaq , and ψtq are positive real numbers.\nSchein et al.’s model generalizes Bayesian Poisson matrix factorization (Cemgil, 2009; Gopalan et al., 2014; 2015; Zhou & Carin, 2015) and non-Bayesian Poisson CP decomposition (Chi & Kolda, 2012; Welling & Weber, 2001).\nAlthough Schein et al.’s model is expressed in terms of a tensor of event type counts, the relationship between the multinomial and Poisson distributions (Kingman, 1972) means that we can also express it in terms of a set of event tokens. This yields an equation that is similar to equation 6,\nP (en=(i a−→j, t) | zn=q) ∝ θ→iq θ←jq φaq ψtq. (8)\nConversely, DuBois & Smyth’s model can be expressed as a CP tensor decomposition. This equivalence is analogous to the relationship between Poisson matrix factorization and latent Dirichlet allocation (Blei et al., 2003).\nWe can make Schein et al.’s model nonparametric by adding a per-class positive weight λq ∼ Γ(γ0Q , ζ), i.e.,\ny (t)\ni a−→j ∼ Po ( Q∑ q=1 θ→iq θ ← jq φaq ψtq λq ) . (9)\nAs Q → ∞ the per-class weights and their corresponding latent factors constitute a draw from a gamma process.\nAdding this per-class weight reveals that CP decomposition is a special case of Tucker decomposition where the cardinalities of the latent dimensions are equal and the offdiagonal elements of the core tensor are zero. DuBois & Smyth’s and Schein et al.’s models are therefore highly constrained special cases of BPTD that cannot capture dimension-specific structure, such as communities of countries or topics of action types. These models require each latent class to jointly summarize information about senders, receivers, action types, and time steps. This requirement conflates communities of countries and topics of action types, thus forcing each class to capture potentially redundant information. Moreover, by definition, CP decomposition models cannot express between-community interactions and cannot express sender–receiver asymmetry without learning completely separate latent factor matrices for\nsenders and receivers. These limitations make it hard to interpret these models as learning community memberships.\nInfinite relational models: The infinite relational model (IRM) of Kemp et al. (2006) also learns latent structure specific to each dimension of an M -dimensional tensor; however, unlike BPTD, the elements of this tensor are binary, indicating the presence or absence of the corresponding event type. The IRM therefore uses a Bernoulli likelihood. Schmidt & Mørup (2013) extended the IRM to model a tensor of event counts by replacing the Bernoulli likelihood with a Poisson likelihood (and gamma priors):\ny (t)\ni a−→j ∼ Po\n( λ (zt)\nzi za−→zj\n) , (10)\nwhere zi, zj ∈ [C] are the respective community assignments of countries i and j, za ∈ [K] is the topic assignment of action a, and zt ∈ [R] is the regime assignment of time step t. This model, which we refer to as the gamma–Poisson IRM (GPIRM), allocates M -dimensional event types to M -dimensional latent classes—e.g., it allocates all tokens of type (i a−→j, t) to class (zi za−→zj , zt).\nThe GPIRM is a special case of BPTD where the rows of the latent factor matrices are constrained to be “one-hot” binary vectors—i.e., θic = 1(zi = c), θjd = 1(zj = d), φak=1(za=k), and ψtr=1(zt=r). With this constraint, the Poisson rates in equations 1 and 10 are equal. Unlike BPTD, the GPIRM is a single-membership model. In addition, it cannot express heterogeneity in rates of activity of the countries, action types, and time steps. The latter limitation can be remedied by letting θizi , θjzj , φaza , and ψtzt be positive real numbers. We refer to this variant of the GPIRM as the degree-corrected GPIRM (DCGPIRM).\nStochastic block models: The IRM itself generalizes the stochastic block model (SBM) of Nowicki & Snijders (2001), which learns latent structure from binary networks. Although the SBM was originally specified using a Bernoulli likelihood, Karrer & Newman (2011) introduced an alternative specification that uses the Poisson likelihood:\nyi−→j ∼ Po ( C∑ c=1 θic C∑ d=1 θjd λc−→d ) , (11)\nwhere θic = 1(zi = c), θj = 1(zj = d), and λc−→d is a positive real number. Like the IRM and the GPIRM, the SBM is a single-membership model and cannot express heterogeneity in the countries’ rates of activity. Airoldi et al. (2008) addressed the former limitation by letting θic ∈ [0, 1] such that ∑C c=1 θic = 1. Meanwhile, Karrer & Newman (2011) addressed the latter limitation by allowing both θizi and θjzj to be positive real numbers, much like the DCGPIRM. Ball et al. (2011) simultaneously addressed both limitations by letting θic, θjd ≥ 0, but constrained λc−→d = λd−→c. Finally, Zhou (2015) extended\nBall et al.’s model to be nonparametric and introduced the Poisson–Bernoulli distribution to link binary data to the Poisson likelihood in a principled fashion. In this model, the elements of the core matrix and their corresponding factors constitute a draw from a relational gamma process.\nNon-Poisson Tucker decomposition: Researchers sometimes refer to the Poisson rate in equation 11 as being “bilinear” because it can equivalently be written as θj Λθ > i . Nickel et al. (2012) introduced RESCAL— a non-probabilistic bilinear model for binary data that achieves state-of-the-art performance at relation extraction. Nickel et al. (2015) then introduced several extensions for extracting relations of different types. Bilinear models, such as RESCAL and its extensions, are all special cases (albeit non-probabilistic ones) of Tucker decomposition.\nHoff (2015) recently developed a Gaussian-based Tucker decomposition model and multilinear tensor regression model (Hoff, 2014) for analyzing interaction event data.\nFinally, there are many other Tucker decomposition methods (Kolda & Bader, 2009). Although these include nonparametric (Xu et al., 2012) and nonnegative variants (Kim & Choi, 20007; Mørup et al., 2008; Cichocki et al., 2009), BPTD is the first such model to use a Poisson likelihood."
    }, {
      "heading" : "4. Posterior Inference",
      "text" : "Given an observed count tensor Y , inference in BPTD involves “inverting” the generative process to obtain the posterior distribution over the parameters conditioned on Y and hyperparameters 0 and γ0. The posterior distribution is analytically intractable; however, we can approximate it using a set of posterior samples. We draw these samples using Gibbs sampling, repeatedly resampling the value of each parameter from its conditional posterior given Y , 0, γ0, and the current values of the other parameters. We express each parameter’s conditional posterior in a closed form using gamma–Poisson conjugacy and the auxiliary variable techniques of Zhou & Carin (2012). We provide the conditional posteriors in the supplementary material.\nThe conditional posteriors depend on Y via a set of “latent sources” (Cemgil, 2009) or subcounts. Because of the Poisson additivity theorem (Kingman, 1972), each latent source y(tr)\nic ak−→jd\nis a Poisson-distributed random variable:\ny (tr)\nic ak−→jd\n∼ Po ( θic θjd φak ψtr λ (r)\nc k−→d\n) (12)\ny (t)\ni a−→j = C∑ c=1 D∑ d=1 K∑ k=1 R∑ r=1 y (tr) ic ak−→jd . (13)\nTogether, equations 12 and 13 are equivalent to equation 1. In practice, we can equivalently view each latent source in\nterms of the token representation described in section 2,\ny (tr)\nic ak−→jd\n= N∑ n=1 1(en=(i a−→j, t))1(zn=(c k−→d, r)), (14)\nwhere each token’s class assignment zn is an auxiliary latent variable. Using this representation, computing the latent sources (given the current values of the model parameters) simply involves allocating event tokens to classes, much like the inference algorithm for DuBois & Smyth’s model, and aggregating them using equation 14. The conditional posterior for each token’s class assignment is\nP (zn=(c k−→d, r) | en=(i a−→j, t),Y , 0, γ0, . . .)\n∝ θic θjd φak ψtr λ(r) c k−→d . (15)\nComputation is dominated by the normalizing constant\nZ (t)\ni a−→j = C∑ c=1 C∑ d=1 K∑ k=1 R∑ r=1 θic θjd φak ψtr λ (r) c k−→d . (16)\nComputing this normalizing constant naı̈vely involves O(C × C × K × R) operations; however, because each latent class (c k−→d, r) is composed of four separate dimensions, we can improve efficiency. We instead compute\nZ (t)\ni a−→j = C∑ c=1 θic C∑ d=1 θjd K∑ k=1 θak R∑ r=1 ψtr λ (r) c k−→d , (17)\nwhich involves O(C + C +K +R) operations.\nCompositional allocation using equations 15 and 17 improves computational efficiency significantly over naı̈ve non-compositional allocation using equations 15 and 16. In practice, we setC,K, andR to large values to approximate the nonparametric interpretation of BPTD. If, for example, C = 50, K = 10, and R = 5, computing the normalizing constant for equation 15 using equation 16 requires 2,753 times the number of operations implied by equation 17.\nProposition 2: For an M -dimensional core tensor with D1 × . . .×DM elements, computing the normalizing constant using non-compositional allocation requires 1 ≤ π < ∞ times the number of operations required to compute it using compositional allocation. When D1 = . . .=DM =1, π=1. As Dm, Dm′ →∞ for any m and m′ 6=m, π →∞.\nWe prove this proposition in the supplementary material.\nBPTD and other Poisson-based models yield allocation inference algorithms that take advantage of the inherent sparsity of the data and scale with the number of event tokens. In contrast, non-Poisson tensor decomposition models (including Hoff’s model) lead to algorithms that scale with the size of the count tensor. Allocation-based inference in BPTD is especially efficient because it compositionally allocates each M -dimensional event token to an\nM -dimensional latent class. Figure 2 illustrates this process. CP decomposition models, such as those of DuBois & Smyth (2010) and Schein et al. (2015), only permit noncompositional allocation. For example, while BPTD allocates each token en = (i\na−→j, t) to a four-dimensional latent class (c k−→d, r), Schein et al.’s model allocates en to a one-dimensional latent class q that cannot be decomposed. Therefore, whenQ=C×C×K×R, BPTD yields a faster allocation inference algorithm than Schein et al.’s model."
    }, {
      "heading" : "5. Country–Country Interaction Event Data",
      "text" : "Our data come from the Integrated Crisis Early Warning System (ICEWS) of Boschee et al. and the Global Database of Events, Language, and Tone (GDELT) of Leetaru & Schrodt (2013). ICEWS and GDELT both use the Conflict and Mediation Event Observations (CAMEO) hierarchy (Gerner et al.) for senders, receivers, and actions.\nThe top-level CAMEO coding for senders and receivers is their country affiliation, while lower levels in the hierarchy incorporate more specific attributes like their sectors (e.g., government or civilian) and their religious or ethnic affiliations. When studying international relations using CAMEO-coded event data, researchers usually consider only the senders’ and receivers’ countries. There are 249 countries represented in ICEWS, which include nonuniversally recognized states, such as Occupied Palestinian Territory, and former states, such as Former Yugoslav Republic of Macedonia; there are 233 countries in GDELT.\nThe top level for actions, which we use in our analyses, consists of twenty action classes, roughly ranked according to their overall sentiment. For example, the most negative is 20—Use Unconventional Mass Violence. CAMEO further divides these actions into the QuadClass scheme: Verbal Cooperation (actions 2–5), Material Cooperation (actions 6–7), Verbal Conflict (actions 8–16), and Material Conflict (16–20). The first action (1—Make Statement) is neutral."
    }, {
      "heading" : "6. Predictive Analysis",
      "text" : "Baseline models: We compared BPTD’s predictive performance to that of three baseline models, described in section 3: 1) GPIRM, 2) DCGPIRM, and 3) the Bayesian Poisson tensor factorization (BPTF) model of Schein et al. (2015). All three models use a Poisson likelihood and have the same two hyperparameters as BPTD—i.e., 0 and γ0. We set 0 to 0.1, as recommended by Gelman (2006), and we set γ0 so that (γ0 /C) 2 (γ0 /K) (γ0 /R) = 0.01. This parameterization encourages the elements of the core tensor Λ to be sparse. We implemented an MCMC inference algorithm for each model. We provide the full generative process for all three models in the supplementary material.\nGPIRM and DCGPIRM are both Tucker decomposition models and thus allocate events to four-dimensional latent classes. The cardinalities of these latent dimensions are the same as BPTD’s—i.e., C, K, and R. In contrast, BPTF is a CP decomposition model and thus allocates events to one-dimensional latent classes. We set the cardinality of this dimension so that the total number of latent factors in BPTF’s likelihood was equal to the total number of latent factors in BPTD’s likelihood—i.e., Q = d (V×C)+(A×K)+(T×R)+(C\n2×K×R) V+V+A+T+1 e. We chose not\nto let BPTF and BPTD use the same number of latent classes—i.e., to set Q = C2 × K × R. BPTF does not permit non-compositional allocation, so MCMC inference becomes very slow for even moderate values of C, K, and R. CP decomposition models also tend to overfit when Q is large (Zhao et al., 2015). Throughout our predictive experiments, we let C= 20, K= 6, and R= 3. These values were well-supported by the data, as we explain in section 7.\nExperimental setup: We constructed twelve different observed tensors—six from ICEWS and six from GDELT. Five of the six tensors for each source (ICEWS or GDELT) correspond to one-year time spans with monthly time steps, starting with 2004 and ending with 2008; the sixth corresponds to a five-year time span with monthly time steps, spanning 1995–2000. We divided each tensor Y into a training tensor Y train = Y (1), . . . ,Y (T−3) and a test tensor Y test = Y (T−2), . . . ,Y (T ). We further divided each test tensor into a held-out portion and an observed portion via a binary mask. We experimented with two different masks: one that treats the elements involving the most active fifteen countries as the held-out portion and the remaining elements as the observed portion, and one that does the opposite. The first mask enabled us to evaluate the models’ reconstructions of the densest (and arguably most interesting) portion of each test tensor, while the second mask enabled us to evaluate their reconstructions of its complement. Across the entire GDELT database, for example, the elements involving the most active fifteen countries—i.e., 6% of all 233 countries—account for 30%\nof the event tokens. Moreover, 40% of these elements are non-zero. These non-zero elements are highly dispersed, with a variance-to-mean ratio of 220. In contrast, only 0.7% of the elements involving the other countries are nonzero. These elements have a variance-to-mean ratio of 26.\nFor each combination of the four models, twelve tensors, and two masks, we ran 5,000 iterations of MCMC inference on the training tensor. We clamped the country–community factors, the action–topic factors, and the core tensor and then inferred the time-step–regime factors for the test tensor using its observed portion by running 1,000 iterations of MCMC inference. We saved every tenth sample after the first 500. We used each sample, along with the country– community factors, the action–topic factors, and the core tensor, to compute the Poisson rate for each element in the held-out portion of the test tensor. Finally, we averaged these rates across samples and used each element’s average rate to compute its probability. We combined the heldout elements’ probabilities by taking their geometric mean or, equivalently, by computing their inverse perplexity. We chose this combination strategy to ensure that the models were penalized heavily for making poor predictions on the non-zero elements and were not rewarded excessively for making good predictions on the zero elements. By clamping the country–community factors, the action–topic factors, and the core tensor after training, our experimental setup is analogous to that used to assess collaborative filtering models’ strong generalization ability (Marlin, 2004).\nResults: Figure 3 illustrates the results for each combination of the four models, twelve tensors, and two masks. The top row contains the results from the twelve experiments involving the first mask, where the elements involving the most active fifteen countries were treated as the held-out portion. BPTD outperformed the baselines significantly. BPTF—itself a state-of-the-art model—performed better than BPTD in only one experiment. In general, the Tucker decomposition allows BPTD to learn richer latent structure that generalizes better to held-out data. The bottom row contains the results from the experiments involving the second mask. The models’ performance was closer in these experiments, probably because of the large proportion of easy-to-predict zero elements. BPTD and BPTF performed indistinguishably in these experiments, and both models outperformed the GPIRM and the DCGPIRM. The single-membership nature of the GPIRM and the DCGPIRM prevents them from expressing high levels of heterogeneity in the countries’ rates of activity. When the heldout elements were highly dispersed, these models sometimes made extremely inaccurate predictions. In contrast, the mixed-membership nature of BPTD and BPTF allows them to better express heterogeneous rates of activity."
    }, {
      "heading" : "7. Exploratory Analysis",
      "text" : "We used a tensor of ICEWS events spanning 1995–2000, with monthly time steps, to explore the latent structure discovered by BPTD. We initially let C = 50, K = 8, and R= 3—i.e., C × C × K × R = 60, 000 latent classes— and used the shrinkage priors to adaptively learn the most appropriate numbers of communities, topics, and regimes. We found C = 20 communities and K = 6 topics with weights that were significantly greater than zero. We provide a plot of the community weights in the supplementary material. Although all three regimes had non-zero weights, one had a much larger weight than the other two. For comparison, Schein et al. (2015) used fifty latent classes to model the same data, while Hoff (2015) used C = 4, K=4, and R=4 to model a similar tensor from GDELT.\nTopics of action types: We show the inferred action–topic factors as a heatmap in the left subplot of figure 4. We ordered the topics by their weights ν1, . . . , νK , which are above the heatmap. The inferred topics correspond very closely to CAMEO’s QuadClass scheme. Moving from left to right, the topics place their mass on increasingly negative actions. Topics 1 and 2 place most of their mass on Verbal Cooperation actions; topic 3 places most of its mass on Material Cooperation actions and the neutral 1—Make Statement action; topic 4 places most of its mass on Verbal Conflict actions and the 1—Make Statement action; and topics 5 and 6 place their mass on Material Conflict actions.\nTopic-partitioned community–community networks: In the right subplot of figure 4, we visualize the inferred community structure for topic k=1 and the most active regime r. The bottom-left heatmap is the community–community interaction network Λ(r)k . The top-left heatmap depicts the rate at which each country i acts as a sender in each community c—i.e., θic ∑V j=1 ∑C d=1 θjd λ (r)\nc k−→d\n. Similarly, the\nbottom-right heatmap depicts the rate at which each country acts as a receiver in each community. The top-right heatmap depicts the number of times each country i took\nan action associated with topic k toward each country j during regime r—i.e., ∑C c=1 ∑C d=1 ∑A a=1 ∑T t=1 y (tr)\nic ak−→jd\n.\nWe grouped the countries by their strongest community memberships and ordered the communities by their withincommunity interaction weights η 1 , . . . , η\nC , from smallest to largest; the thin green lines separate the countries that are strongly associated with one community from the countries that are strongly associated with its adjacent communities.\nSome communities contain only one or two strongly associated countries. For example, community 1 contains only the US, community 6 contains only China, and community 7 contains only Russia and Belarus. These communities mostly engage in between-community interaction. Other larger communities, such as communities 9 and 15, mostly engage in within-community interaction. Most communities have a strong geographic interpretation. Moving upward from the bottom, there are communities that correspond to Eastern Europe, East Africa, South-Central Africa, Latin America, Australasia, Central Europe, Central Asia, etc. The community–community interaction network summarizes the patterns in the top-right heatmap. This topic is dominated by the 4–Consult action, so the network is symmetric; the more negative topics have asymmetric community–community interaction networks. We therefore hypothesize that cooperation is an inherently reciprocal type of interaction. We provide visualizations for the other five topics in the supplementary material."
    }, {
      "heading" : "8. Summary",
      "text" : "We presented Bayesian Poisson Tucker decomposition (BPTD) for learning the latent structure of international relations from country–country interaction events of the form “country i took action a toward country j at time t.” Unlike previous models, BPTD takes advantage of all three representations of an interaction event data set: 1) a set of event tokens, 2) a tensor of event type counts, and 3) a series of weighted multinetwork snapshots. BPTD uses a Poisson\nlikelihood, respecting the discrete nature of the data and its inherent sparsity. Moreover, BPTD yields a compositional allocation inference algorithm that is more efficient than non-compositional allocation algorithms. Because BPTD is a Tucker decomposition model, it shares parameters across latent classes. In contrast, CP decomposition models force each latent class to capture potentially redundant information. BPTD therefore “does more with less.” This efficiency is reflected in our predictive analysis: BPTD outperforms BPTF—a CP decomposition model—as well as two other baselines. BPTD learns interpretable latent structure that aligns with well-known concepts from the networks literature. Specifically, BPTD learns latent country– community memberships, including the number of communities, as well as directed community–community interaction networks that are specific to topics of action types and temporal regimes. This structure captures the complex-\nity of country–country interactions, while revealing patterns that agree with our knowledge of international relations. Finally, although we presented BPTD in the context of interaction events, BPTD is well suited to learning latent structure from other types of multidimensional count data."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Abigail Jacobs and Brandon Stewart for helpful discussions. This work was supported by NSF #SBE0965436, #IIS-1247664, #IIS-1320219; ONR #N0001411-1-0651; DARPA #FA8750-14-2-0009, #N66001-15-C4032; Adobe; the John Templeton Foundation; the Sloan Foundation; the UMass Amherst Center for Intelligent Information Retrieval. Any opinions, findings, conclusions, or recommendations expressed in this material are the authors’ and do not necessarily reflect those of the sponsors."
    } ],
    "references" : [ {
      "title" : "Mixed membership stochastic blockmodels",
      "author" : [ "E.M. Airoldi", "D.M. Blei", "S.E. Feinberg", "E.P. Xing" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Airoldi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Airoldi et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient and principled method for detecting communities in networks",
      "author" : [ "B. Ball", "B. Karrer", "M.E.J. Newman" ],
      "venue" : "Physical Review E,",
      "citeRegEx" : "Ball et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ball et al\\.",
      "year" : 2011
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Bayesian inference for nonnegative matrix factorisation models",
      "author" : [ "A.T. Cemgil" ],
      "venue" : "Computational Intelligence and Neuroscience,",
      "citeRegEx" : "Cemgil,? \\Q2009\\E",
      "shortCiteRegEx" : "Cemgil",
      "year" : 2009
    }, {
      "title" : "On tensors, sparsity, and nonnegative factorizations",
      "author" : [ "E.C. Chi", "T.G. Kolda" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Chi and Kolda,? \\Q2012\\E",
      "shortCiteRegEx" : "Chi and Kolda",
      "year" : 2012
    }, {
      "title" : "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-Way Data Analysis and Blind Source Separation",
      "author" : [ "A. Cichocki", "R. Zdunek", "A.H. Phan", "S. i Amari" ],
      "venue" : null,
      "citeRegEx" : "Cichocki et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cichocki et al\\.",
      "year" : 2009
    }, {
      "title" : "Modeling relational events via latent classes",
      "author" : [ "C. DuBois", "P. Smyth" ],
      "venue" : "In Proceedings of the Sixteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp",
      "citeRegEx" : "DuBois and Smyth,? \\Q2010\\E",
      "shortCiteRegEx" : "DuBois and Smyth",
      "year" : 2010
    }, {
      "title" : "A Bayesian analysis of some nonparametric problems",
      "author" : [ "T.S. Ferguson" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Ferguson,? \\Q1973\\E",
      "shortCiteRegEx" : "Ferguson",
      "year" : 1973
    }, {
      "title" : "Prior distributions for variance parameters in hierarchical models",
      "author" : [ "A. Gelman" ],
      "venue" : "Bayesian Analysis,",
      "citeRegEx" : "Gelman,? \\Q2006\\E",
      "shortCiteRegEx" : "Gelman",
      "year" : 2006
    }, {
      "title" : "Bayesian nonparametric Poisson factorization for recommendation systems",
      "author" : [ "P. Gopalan", "F.J.R. Ruiz", "R. Ranganath", "D.M. Blei" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Gopalan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gopalan et al\\.",
      "year" : 2014
    }, {
      "title" : "Scalable recommendation with Poisson factorization",
      "author" : [ "P. Gopalan", "J. Hofman", "D. Blei" ],
      "venue" : "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Gopalan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gopalan et al\\.",
      "year" : 2015
    }, {
      "title" : "Foundations of the PARAFAC procedure: Models and conditions for an “explanatory” multimodal factor analysis",
      "author" : [ "R. Harshman" ],
      "venue" : "UCLA Working Papers in Phonetics,",
      "citeRegEx" : "Harshman,? \\Q1970\\E",
      "shortCiteRegEx" : "Harshman",
      "year" : 1970
    }, {
      "title" : "Multilinear tensor regression for longitudinal relational data",
      "author" : [ "P. Hoff" ],
      "venue" : null,
      "citeRegEx" : "Hoff,? \\Q2014\\E",
      "shortCiteRegEx" : "Hoff",
      "year" : 2014
    }, {
      "title" : "Equivariant and scale-free Tucker decomposition models",
      "author" : [ "P. Hoff" ],
      "venue" : "Bayesian Analysis,",
      "citeRegEx" : "Hoff,? \\Q2015\\E",
      "shortCiteRegEx" : "Hoff",
      "year" : 2015
    }, {
      "title" : "Stochastic blockmodels and community structure in networks",
      "author" : [ "B. Karrer", "M.E.J. Newman" ],
      "venue" : "Physical Review E,",
      "citeRegEx" : "Karrer and Newman,? \\Q2011\\E",
      "shortCiteRegEx" : "Karrer and Newman",
      "year" : 2011
    }, {
      "title" : "Learning systems of concepts with an infinite relational model",
      "author" : [ "C. Kemp", "J.B. Tenenbaum", "T.L. Griffiths", "T. Yamada", "N. Ueda" ],
      "venue" : "In Proceedings of the Twenty-First National Conference on Artificial Intelligence,",
      "citeRegEx" : "Kemp et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kemp et al\\.",
      "year" : 2006
    }, {
      "title" : "Nonnegative Tucker decomposition",
      "author" : [ "Kim", "Y.-D", "S. Choi" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Kim et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2000
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T.G. Kolda", "B.W. Bader" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Kolda and Bader,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolda and Bader",
      "year" : 2009
    }, {
      "title" : "GDELT: Global data on events, location, and tone, 1979–2012",
      "author" : [ "K. Leetaru", "P. Schrodt" ],
      "venue" : "Working paper,",
      "citeRegEx" : "Leetaru and Schrodt,? \\Q2013\\E",
      "shortCiteRegEx" : "Leetaru and Schrodt",
      "year" : 2013
    }, {
      "title" : "Collaborative filtering: A machine learning perspective",
      "author" : [ "B. Marlin" ],
      "venue" : "Master’s thesis, University of Toronto,",
      "citeRegEx" : "Marlin,? \\Q2004\\E",
      "shortCiteRegEx" : "Marlin",
      "year" : 2004
    }, {
      "title" : "Algorithms for sparse nonnegative Tucker decompositions",
      "author" : [ "M. Mørup", "L.K. Hansen", "S.M. Arnfred" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Mørup et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mørup et al\\.",
      "year" : 2008
    }, {
      "title" : "Factorizing YAGO: Scalable machine learning for linked data",
      "author" : [ "M. Nickel", "V. Tresp", "Kriegel", "H.-P" ],
      "venue" : "In Proceedings of the Twenty-First International World Wide Web Conference,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2012
    }, {
      "title" : "A review of relational machine learning for knowledge graphs: From multi-relational link prediction to automated knowledge graph construction",
      "author" : [ "M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich" ],
      "venue" : null,
      "citeRegEx" : "Nickel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2015
    }, {
      "title" : "Estimation and prediction for stochastic blockstructures",
      "author" : [ "K. Nowicki", "T.A.B. Snijders" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Nowicki and Snijders,? \\Q2001\\E",
      "shortCiteRegEx" : "Nowicki and Snijders",
      "year" : 2001
    }, {
      "title" : "Nonparametric Bayesian modeling of complex networks: An introduction",
      "author" : [ "M.N. Schmidt", "M. Mørup" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Schmidt and Mørup,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt and Mørup",
      "year" : 2013
    }, {
      "title" : "The extension of factor analysis to threedimensional matrices",
      "author" : [ "L.R. Tucker" ],
      "venue" : "Contributions to Mathematical Psychology. Holt, Rinehart and Winston,",
      "citeRegEx" : "Tucker,? \\Q1964\\E",
      "shortCiteRegEx" : "Tucker",
      "year" : 1964
    }, {
      "title" : "Positive tensor factorization",
      "author" : [ "M. Welling", "M. Weber" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Welling and Weber,? \\Q2001\\E",
      "shortCiteRegEx" : "Welling and Weber",
      "year" : 2001
    }, {
      "title" : "Infinite Tucker decomposition: Nonparametric Bayesian models for multiway data analysis",
      "author" : [ "Z. Xu", "F. Yan", "Y. Qi" ],
      "venue" : "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,",
      "citeRegEx" : "Xu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "Bayesian CP factorization of incomplete tensors with automatic rank determination",
      "author" : [ "Q. Zhao", "L. Zhang", "A. Cichocki" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    }, {
      "title" : "Infinite edge partition models for overlapping community detection and link prediction",
      "author" : [ "M. Zhou" ],
      "venue" : "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Zhou,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou",
      "year" : 2015
    }, {
      "title" : "Augment-and-conquer negative binomial processes",
      "author" : [ "M. Zhou", "L. Carin" ],
      "venue" : "In Advances in Neural Information Processing Systems Twenty-Five,",
      "citeRegEx" : "Zhou and Carin,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhou and Carin",
      "year" : 2012
    }, {
      "title" : "Negative binomial process count and mixture modeling",
      "author" : [ "M. Zhou", "L. Carin" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Zhou and Carin,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou and Carin",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "BPTD builds on the classic Tucker decomposition (Tucker, 1964) to factorize a tensor of event type counts into three factor matrices and a four-dimensional core tensor (section 2).",
      "startOffset" : 48,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "As K → ∞, the topic weights and their corresponding action–topic factors constitute a drawGK = ∑∞ k=1 νk 1φk from a gamma process (Ferguson, 1973).",
      "startOffset" : 130,
      "endOffset" : 146
    }, {
      "referenceID" : 29,
      "context" : "This multirelational gamma process extends the relational gamma process of Zhou (2015). Proposition 1: In the limit as C,K,R →∞, the expected sum of the core tensor elements is finite and equal to E  ∞ ∑",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "(2015), which explicitly uses the canonical polyadic (CP) tensor decomposition (Harshman, 1970) to factorize count tensor Y into four latent factor matrices.",
      "startOffset" : 79,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "’s model generalizes Bayesian Poisson matrix factorization (Cemgil, 2009; Gopalan et al., 2014; 2015; Zhou & Carin, 2015) and non-Bayesian Poisson CP decomposition (Chi & Kolda, 2012; Welling & Weber, 2001).",
      "startOffset" : 59,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "’s model generalizes Bayesian Poisson matrix factorization (Cemgil, 2009; Gopalan et al., 2014; 2015; Zhou & Carin, 2015) and non-Bayesian Poisson CP decomposition (Chi & Kolda, 2012; Welling & Weber, 2001).",
      "startOffset" : 59,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "This equivalence is analogous to the relationship between Poisson matrix factorization and latent Dirichlet allocation (Blei et al., 2003).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "Infinite relational models: The infinite relational model (IRM) of Kemp et al. (2006) also learns latent structure specific to each dimension of an M -dimensional tensor; however, unlike BPTD, the elements of this tensor are binary, indicating the presence or absence of the corresponding event type.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "Infinite relational models: The infinite relational model (IRM) of Kemp et al. (2006) also learns latent structure specific to each dimension of an M -dimensional tensor; however, unlike BPTD, the elements of this tensor are binary, indicating the presence or absence of the corresponding event type. The IRM therefore uses a Bernoulli likelihood. Schmidt & Mørup (2013) extended the IRM to model a tensor of event counts by replacing the Bernoulli likelihood with a Poisson likelihood (and gamma priors):",
      "startOffset" : 67,
      "endOffset" : 371
    }, {
      "referenceID" : 12,
      "context" : "Hoff (2015) recently developed a Gaussian-based Tucker decomposition model and multilinear tensor regression model (Hoff, 2014) for analyzing interaction event data.",
      "startOffset" : 115,
      "endOffset" : 127
    }, {
      "referenceID" : 27,
      "context" : "Although these include nonparametric (Xu et al., 2012) and nonnegative variants (Kim & Choi, 20007; Mørup et al.",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : ", 2012) and nonnegative variants (Kim & Choi, 20007; Mørup et al., 2008; Cichocki et al., 2009), BPTD is the first such model to use a Poisson likelihood.",
      "startOffset" : 33,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : ", 2012) and nonnegative variants (Kim & Choi, 20007; Mørup et al., 2008; Cichocki et al., 2009), BPTD is the first such model to use a Poisson likelihood.",
      "startOffset" : 33,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "Airoldi et al. (2008) addressed the former limitation by letting θic ∈ [0, 1] such that ∑C c=1 θic = 1.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Airoldi et al. (2008) addressed the former limitation by letting θic ∈ [0, 1] such that ∑C c=1 θic = 1. Meanwhile, Karrer & Newman (2011) addressed the latter limitation by allowing both θizi and θjzj to be positive real numbers, much like the DCGPIRM.",
      "startOffset" : 0,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "Airoldi et al. (2008) addressed the former limitation by letting θic ∈ [0, 1] such that ∑C c=1 θic = 1. Meanwhile, Karrer & Newman (2011) addressed the latter limitation by allowing both θizi and θjzj to be positive real numbers, much like the DCGPIRM. Ball et al. (2011) simultaneously addressed both limitations by letting θic, θjd ≥ 0, but constrained λc− →d = λd− →c.",
      "startOffset" : 0,
      "endOffset" : 272
    }, {
      "referenceID" : 0,
      "context" : "Airoldi et al. (2008) addressed the former limitation by letting θic ∈ [0, 1] such that ∑C c=1 θic = 1. Meanwhile, Karrer & Newman (2011) addressed the latter limitation by allowing both θizi and θjzj to be positive real numbers, much like the DCGPIRM. Ball et al. (2011) simultaneously addressed both limitations by letting θic, θjd ≥ 0, but constrained λc− →d = λd− →c. Finally, Zhou (2015) extended Ball et al.",
      "startOffset" : 0,
      "endOffset" : 393
    }, {
      "referenceID" : 0,
      "context" : "Airoldi et al. (2008) addressed the former limitation by letting θic ∈ [0, 1] such that ∑C c=1 θic = 1. Meanwhile, Karrer & Newman (2011) addressed the latter limitation by allowing both θizi and θjzj to be positive real numbers, much like the DCGPIRM. Ball et al. (2011) simultaneously addressed both limitations by letting θic, θjd ≥ 0, but constrained λc− →d = λd− →c. Finally, Zhou (2015) extended Ball et al.’s model to be nonparametric and introduced the Poisson–Bernoulli distribution to link binary data to the Poisson likelihood in a principled fashion. In this model, the elements of the core matrix and their corresponding factors constitute a draw from a relational gamma process. Non-Poisson Tucker decomposition: Researchers sometimes refer to the Poisson rate in equation 11 as being “bilinear” because it can equivalently be written as θj Λθ > i . Nickel et al. (2012) introduced RESCAL— a non-probabilistic bilinear model for binary data that achieves state-of-the-art performance at relation extraction.",
      "startOffset" : 0,
      "endOffset" : 885
    }, {
      "referenceID" : 0,
      "context" : "Airoldi et al. (2008) addressed the former limitation by letting θic ∈ [0, 1] such that ∑C c=1 θic = 1. Meanwhile, Karrer & Newman (2011) addressed the latter limitation by allowing both θizi and θjzj to be positive real numbers, much like the DCGPIRM. Ball et al. (2011) simultaneously addressed both limitations by letting θic, θjd ≥ 0, but constrained λc− →d = λd− →c. Finally, Zhou (2015) extended Ball et al.’s model to be nonparametric and introduced the Poisson–Bernoulli distribution to link binary data to the Poisson likelihood in a principled fashion. In this model, the elements of the core matrix and their corresponding factors constitute a draw from a relational gamma process. Non-Poisson Tucker decomposition: Researchers sometimes refer to the Poisson rate in equation 11 as being “bilinear” because it can equivalently be written as θj Λθ > i . Nickel et al. (2012) introduced RESCAL— a non-probabilistic bilinear model for binary data that achieves state-of-the-art performance at relation extraction. Nickel et al. (2015) then introduced several extensions for extracting relations of different types.",
      "startOffset" : 0,
      "endOffset" : 1043
    }, {
      "referenceID" : 0,
      "context" : "Airoldi et al. (2008) addressed the former limitation by letting θic ∈ [0, 1] such that ∑C c=1 θic = 1. Meanwhile, Karrer & Newman (2011) addressed the latter limitation by allowing both θizi and θjzj to be positive real numbers, much like the DCGPIRM. Ball et al. (2011) simultaneously addressed both limitations by letting θic, θjd ≥ 0, but constrained λc− →d = λd− →c. Finally, Zhou (2015) extended Ball et al.’s model to be nonparametric and introduced the Poisson–Bernoulli distribution to link binary data to the Poisson likelihood in a principled fashion. In this model, the elements of the core matrix and their corresponding factors constitute a draw from a relational gamma process. Non-Poisson Tucker decomposition: Researchers sometimes refer to the Poisson rate in equation 11 as being “bilinear” because it can equivalently be written as θj Λθ > i . Nickel et al. (2012) introduced RESCAL— a non-probabilistic bilinear model for binary data that achieves state-of-the-art performance at relation extraction. Nickel et al. (2015) then introduced several extensions for extracting relations of different types. Bilinear models, such as RESCAL and its extensions, are all special cases (albeit non-probabilistic ones) of Tucker decomposition. Hoff (2015) recently developed a Gaussian-based Tucker decomposition model and multilinear tensor regression model (Hoff, 2014) for analyzing interaction event data.",
      "startOffset" : 0,
      "endOffset" : 1266
    }, {
      "referenceID" : 3,
      "context" : "The conditional posteriors depend on Y via a set of “latent sources” (Cemgil, 2009) or subcounts.",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 28,
      "context" : "We express each parameter’s conditional posterior in a closed form using gamma–Poisson conjugacy and the auxiliary variable techniques of Zhou & Carin (2012). We provide the conditional posteriors in the supplementary material.",
      "startOffset" : 138,
      "endOffset" : 158
    }, {
      "referenceID" : 28,
      "context" : "CP decomposition models also tend to overfit when Q is large (Zhao et al., 2015).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 19,
      "context" : "By clamping the country–community factors, the action–topic factors, and the core tensor after training, our experimental setup is analogous to that used to assess collaborative filtering models’ strong generalization ability (Marlin, 2004).",
      "startOffset" : 226,
      "endOffset" : 240
    }, {
      "referenceID" : 8,
      "context" : "1, as recommended by Gelman (2006), and we set γ0 so that (γ0 /C) 2 (γ0 /K) (γ0 /R) = 0.",
      "startOffset" : 21,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : "(2015) used fifty latent classes to model the same data, while Hoff (2015) used C = 4, K=4, and R=4 to model a similar tensor from GDELT.",
      "startOffset" : 63,
      "endOffset" : 75
    } ],
    "year" : 2016,
    "abstractText" : "We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country– country interaction event data. These data consist of interaction events of the form “country i took action a toward country j at time t.” BPTD discovers overlapping country– community memberships, including the number of latent communities. In addition, it discovers directed community–community interaction networks that are specific to “topics” of action types and temporal “regimes.” We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.",
    "creator" : "LaTeX with hyperref package"
  }
}