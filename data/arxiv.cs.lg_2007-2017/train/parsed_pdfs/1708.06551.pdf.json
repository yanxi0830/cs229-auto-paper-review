{
  "name" : "1708.06551.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reinforcement Learning in POMDPs with Memoryless Options and Option-Observation Initiation Sets",
    "authors" : [ "Denis Steckelmacher", "Diederik M. Roijers", "Anna Harutyunyan", "Peter Vrancx", "Ann Nowé" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Real-world applications of reinforcement learning (RL) face two main challenges: complex long-running tasks and partial observability. Hierarchical RL, of which options are the particular instance we focus on, addresses the first challenge by factoring a complex task into simpler sub-tasks (Barto and Mahadevan 2003; Roy et al. 2006; Tessler et al. 2016). Instead of learning which action to perform depending on an observation, the agent learns a top-level policy that repeatedly selects options, that in turn execute a sequence of actions before returning (Sutton et al. 1999). The second challenge, partial observability, is addressed by maintaining a belief of what the agent thinks the full state is (Cassandra et al. 1994), reasoning about possible future observations (Littman et al. 2001; Boots et al. 2011), storing information in an external memory for later reuse (Peshkin et al. 2001; Zaremba and Sutskever 2015; Graves et al. 2016), or using recurrent neural networks (RNNs) to allow information to flow between time-steps (Bakker 2001; Mnih et al. 2016).\nCombined solutions to the above two challenges have already been proposed, but are not always ideal. HQ-Learning decomposes a task into a list of fully-observable subtasks to be executed in sequence (Wiering and Schmidhuber 1997),\nwhich precludes cyclic tasks with an unbounded number of repetitions from being solved. Using recurrent neural networks in options and for the top-level policy (Sridharan et al. 2010) addresses both challenges, but brings in the design complexity of RNNs (Józefowicz et al. 2015; Angeline et al. 1994; Mikolov et al. 2014). RNNs also have limitations regarding long time horizons, as their memory decays over time (Hochreiter and Schmidhuber 1997), and provide no explanation on which past observations affect the current choice of action.\nIn her PhD thesis, Precup (2000, page 126) suggests that options may already be close to addressing partial observability, thus removing the need for more complicated solutions. In this paper, we prove this intuition correct by:\n1. Showing that options do not suffice in POMDPs; 2. Introducing Option-Observation Initiation Sets (OOIs),\nthat make the initiation sets of options conditional on the previously-executed option;\n3. Proving that OOIs make options at least as expressive as Finite State Controllers (Section 3.2), state-of-the-art in POMDPs.\nDesigning OOIs is as easy as describing a task in natural language (Section 3.1). In contrast to existing HRL algorithms for POMDPs (Wiering and Schmidhuber 1997; Theocharous 2002; Sridharan et al. 2010), OOIs handle repetitive tasks, do not restrict the action set available to subtasks, and keep the option policies and policy over options memoryless. Experimental results in Section 4 confirm that OOIs allow partially observable tasks to be solved optimally, while allowing learned (Section 4.4) or human-provided options to be used (Sections 4.3 and 4.5). Our experiments also show that OOIs are much more sample efficient than a recurrent neural network over options, and lead to a policy significantly outperforming an expert policy in Section 4.3."
    }, {
      "heading" : "1.1 Motivating Example",
      "text" : "OOIs are designed to solve complex partially observable tasks that can be decomposed into a set of fully-observable sub-tasks. For instance, a robot with first-person sensors may be able to avoid obstacles, open doors or manipulate objects even if its precise location in a building is not observed. We now introduce such an environment, on which our robotic experiments of Section 4.3 are based.\nar X\niv :1\n70 8.\n06 55\n1v 1\n[ cs\n.A I]\n2 2\nA ug\n2 01\n7\nA Khepera III1robot has to gather objects from a green and a blue terminal separated by a wall, and to bring them to a central root location. Objects have to be gathered one by one from a terminal until it becomes empty, which requires many journeys between the root and a terminal. When a terminal is emptied, the other one is automatically refilled. The robot therefore has to alternatively gather objects from both terminals, until the end of the episode. The root is colored in red and marked by a paper QR-code displaying 1. Each terminal has a screen displaying its color and a 1 QRcode when full, 2 when empty. Because the robot cannot read QR-codes from far away, the state of a terminal cannot be observed from the root. This makes the environment partially observable, and requires that the robot goes to a terminal, observes that it is empty, goes back to the root, and remembers to now go to the other terminal.\nThe robot is able to control the speed of its two wheels. A wireless camera mounted on top of the robot allows it to identify where the largest red, green or blue color blobs are in its field of view, and can read nearby QR-codes. Such low-level actions and observations, combined with a complicated task, motivate the use of hierarchical reinforcement learning. Fixed options allow the robot to move towards the largest red, green or blue blob in its field of view. The options terminate as soon as a QR-code is in front of the camera and close enough to be read. This allows the robot to move towards colored objects, then to make a decision based on the QR-code displayed on that object. The limited range of the QR-code reader and its inability to read more than one QR-code at once prevent the agent from observing the complete state of the environment. The robot has to learn a policy over options that solves the task.\nThe robot may have to gather a large number of objects, alternating between terminals several times. The repetitive nature of this task is incompatible with HQ-Learning (Wiering and Schmidhuber 1997). Options with standard initiation sets are not able to solve this task, as the top-level policy is memoryless (Sutton et al. 1999) and cannot remember from which terminal the robot arrives at the root, and whether that terminal was full or empty. Because the termi-\n1http://www.k-team.com/mobile-roboticsproducts/old-products/khepera-iii\nnals are far from the root, almost a hundred primitive actions have to be executed to complete any root/terminal journey. Without options, this represents a time horizon much larger than usually handled by recurrent neural networks (Bakker 2001) or finite history windows (Lin and Mitchell 1993).\nOOIs allow each option to be selected conditionally on the previously executed one (Section 3.1), which is much simpler than combining options and recurrent neural networks (Sridharan et al. 2010). The ability of OOIs to solve POMDPs builds on the time abstraction capabilities and expressiveness of options. Section 4.3 shows that OOIs allow the optimal policy for our robotic task to be represented and learned above expert level. Section 4.4 demonstrates on a simulated task that both the top-level and option policies can be learned by the agent. Finally, Section 4.5 shows that OOIs lead to substantial gains over standard initiation sets even if the option set is reduced or unsuited to the task."
    }, {
      "heading" : "2 Background",
      "text" : "This section formally introduces Markov Decision Processes (MDPs), Options, Partially Observable MDPs (POMDPs) and Finite State Controllers, before presenting our main contribution in Section 3."
    }, {
      "heading" : "2.1 Markov Decision Processes",
      "text" : "A discrete-time Markov Decision Process (MDP) 〈S,A,R, T, γ〉 with discrete actions is defined by a possibly-infinite set S of states, a finite set A of actions, a reward function R(st, at, st+1) ∈ R, that provides a scalar reward rt for each state transition, a transition function T (st, at, st+1) ∈ [0, 1], that outputs a probability distribution over new states st+1 given a (st, at) state-action pair, and 0 ≤ γ < 1 the discount factor, that defines how sensitive the agent should be to future rewards.\nA stochastic memoryless policy π(st, at) ∈ [0, 1] maps a state to a probability distribution over actions. The goal of the agent is to find a policy π∗ that maximizes the expected cumulative discounted reward Eπ∗ [ ∑ t γ\ntrt] obtainable by following that policy."
    }, {
      "heading" : "2.2 Options",
      "text" : "The options framework, defined in MDPs by Sutton et al., consists of a set of options O where each option ω ∈ O is a tuple 〈πω, Iω, βω〉, with πω(st, at) ∈ [0, 1] the memoryless option policy, βω(st) ∈ [0, 1] the termination function that\ngives the probability for the option ω to terminate in state st, and Iω ⊆ S the initiation set that defines in which states ω can be started (Sutton et al. 1999).\nThe memoryless policy over options µ(st, ωt) ∈ [0, 1] maps states to a distribution over options and allows to choose which option to start in a given state. When an option ω is started, it executes until termination (due to βω), at which point µ selects a new option based on the now current state."
    }, {
      "heading" : "2.3 Partially Observable MDPs",
      "text" : "Most real-world problems are not MDPs and present at least some degree of partial observability. A Partially Observable MDP (POMDP) 〈Ω, S,A,R, T,O, γ〉 is a MDP extended with two components: the possibly infinite set Ω of observations, and theO : S ∈ Ω function that produces observations x based on the hidden state s of the process. Two different states, requiring two different optimal actions, may produce the same observation. This makes POMDPs remarkably challenging for reinforcement learning algorithms, as memoryless policies, that select actions or options based only on the current observation, typically no longer suffice."
    }, {
      "heading" : "2.4 Finite State Controllers",
      "text" : "Finite State Controllers (FSCs) are state of the art in representing policies that work well in POMDPs. An FSC 〈N , ψ, η, η0〉 is defined by a finite set N of nodes, an action function ψ(nt, at) ∈ [0, 1] that maps nodes to a probability distribution over actions, a successor function η(nt−1, xt, nt) ∈ [0, 1] that maps nodes and observations to a probability distribution over next nodes, and an initial function η0(x1, n1) ∈ [0, 1] that maps initial observations to nodes (Meuleau et al. 1999).\nAt the first time-step, the agent observes x1 and activates a node n1 by sampling from η0(x1, ·). An action is performed by sampling from ψ(n1, ·). At each time-step t, a node nt is sampled from η(nt−1, xt, ·), then an action at is sampled from ψ(nt, ·). FSCs allow the agent to select actions according to the entire history of past observations (Meuleau et al. 1999), which has been shown to be one of the best approaches for POMDPs (Lin and Mitchell 1992). OOIs, our main contribution, make options at least as expressive and as relevant to POMDPs as FSCs, while being able to leverage the hierarchical structure of the problem."
    }, {
      "heading" : "3 Option-Observation Initiation Sets",
      "text" : "Our main contribution, Option-Observation Initiation Sets (OOIs), make the initiation sets of options conditional on the option that has just terminated. We prove that OOIs make options at least as expressive as FSCs (thus suited to POMDPs, see Section 3.2), even if the top-level and option policies are memoryless, while options without OOIs are strictly less expressive than FSCs (see Section 3.3). In Section 4, we show on one robotic and two simulated tasks that OOIs allow challenging POMDPs to be solved optimally."
    }, {
      "heading" : "3.1 Conditioning on Previous Option",
      "text" : "Descriptions of partially observable tasks in natural language often contain allusions at sub-tasks that must be se-\nquenced or cycled through, possibly with branches. This is easily mapped to a policy over options (learned by the agent) and sets of options that may or may not follow each other.\nA good memory-based policy for our motivating example, where the agent has to bring objects from two terminals to the root, can be described as “go to the green terminal, then go to the root, then go back to the green terminal if it was full, to the blue terminal otherwise”, and symmetrically so for the blue terminal. This sequence of sub-tasks, that contains a condition, is easily translated to a set of options. Two options, sharing a single policy, go from the green terminal to the root (using low-level motor actions). One is executed when the terminal is full, the other when it is empty. At the root, the option that goes back to the green terminal can only follow the option that goes to the root after having observed that the green terminal was full. This forces the agent to switch to the blue terminal when the green one is empty. In Section 4.3, we show that a slightly larger set of options avoids encoding parts of the solution in the option set, which allows the agent to discover a stochastic policy that outperforms our good policy.\nWe now formally define our main contribution, OptionObservation Initiation Sets (OOIs), that allow to describe which options may follow which ones. We define the initiation set Iω of option ω so that the set Ot of options available at time t depends on the observation xt and previouslyexecuted option ωt−1:\nIω ⊆ Ω× (O ∪ {∅}) Ot ≡ {ω ∈ O : (xt, ωt−1) ∈ Iω}\nwith ω0 = ∅, Ω the set of observations and O the set of options. Ot allows the agent to condition the option selected at time t on the one that has just terminated, even if the top-level policy does not observe ωt−1. The option policies and top-level policy remain memoryless. Not having to observe ωt−1 keeps the observation space of the top-level policy small, instead of extending it to Ω × O, without impairing the representational power of OOIs, as shown in the next sub-section."
    }, {
      "heading" : "3.2 OOIs Make Options as Expressive as FSCs",
      "text" : "Finite State Controllers are state-of-the-art in policies applicable to POMDPs (Meuleau et al. 1999). By proving that options with OOIs are as expressive as FSCs, we provide a lower bound on the expressiveness of OOIs and ensure that they are applicable to a wide range of POMDPs.\nTheorem 1. OOIs allow options to represent any policy that can be expressed using a Finite State Controller.\nProof. The reduction from any FSC to options requires one option 〈n′t−1, nt〉 per ordered pair of node in the FSC, and one option 〈∅, n1〉 per node in the FSC. Assuming that n0 = ∅ and η(∅, x1, ·) = η0(x1, ·), the options are defined by:\nβ〈n′t−1,nt〉(xt) = 1 (1)\nπ〈n′t−1,nt〉(xt, at) = ψ(nt, at) (2) µ(xt, 〈n′t−1, nt〉) = η(n′t−1, xt, nt) (3) I〈∅,n1〉 = Ω× {∅}\nI〈n′t−1,nt〉 = Ω× {〈n ′ t−2, nt−1〉 : n′t−1 = nt−1}\nEach option corresponds to an edge of the FSC. Equation 1 ensures that every option stops after having emitted a single action, as the FSC takes one transition every timestep. Equation 2 maps the current option to the action emitted by the destination node of its corresponding FSC edge. We show that µ and I〈n′t−1,nt〉 implement η(nt−1, xt, nt) by:\nµ(xt, 〈n′t−1, nt〉 | ωt−1 = 〈n′t−2, nt−1〉) = 0 〈n′t−2, nt−1〉 /∈ I〈n′t−1,nt〉 ⇔ n′t−1 6= nt−1\nη(nt−1, xt, nt) 〈n′t−2, nt−1〉 ∈ I〈n′t−1,nt〉 ⇔ n′t−1 = nt−1\nBecause η maps nodes to nodes and µ selects options representing pairs of nodes, µ is extremely sparse and returns a value different from zero, η(nt−1, xt, nt), only when 〈n′t−2, nt−1〉 and 〈n′t−1, nt〉 agree on nt−1.\nEquation 2 ignores the observation, which leads to a large amount of options to compensate. In practice, we expect to be able to express policies for real-world POMDPs with much less options than the number of states an FSC would require, as shown in our simulated (Section 4.4, 2 options) and robotic experiments (Section 4.3, 12 options). In addition to being sufficient, the next sub-section proves that OOIs are necessary for options to be as expressive as FSCs."
    }, {
      "heading" : "3.3 Original Options are not as Expressive as FSCs",
      "text" : "While options with regular initiation sets may be able to express some memory-based policies (Sutton et al. 1999, page 7), the tiny but valid Finite State Controller presented in Figure 3 cannot be mapped to a set of options and a policy over options (without OOIs). This proves that options without OOIs are strictly less expressive than FSCs. Theorem 2. Options without OOIs are not as expressive as Finite State Controllers.\nProof. Figure 3 shows a Finite State Controller that emits a sequence of alternating A’s and B’s, based on a constant uninformative observation x∅. This task requires memory because the observation does not provide any information about what was the last letter to be emitted, or which one must now be emitted. Options having memoryless policies, options executing for multiple time-steps are unable to represent the FSC exactly. A combination of options that execute for a single time-step cannot represent the FSC either, as the options framework is unable to represent memorybased policies with single-time-step options (Sutton et al. 1999)."
    }, {
      "heading" : "4 Experiments",
      "text" : "The experiments in this section illustrate how OOIs allow agents to perform optimally in environments where options without OOIs fail. Section 4.3 shows on our motivating example (Section 1.1) that OOIs allow the agent to find a policy outperforming an expert-provided one, which motivates the use of reinforcement learning in industrial POMDPs where the currently deployed policies may still be improved upon. Section 4.4 shows that the top-level and option policies required by a repetitive task can be learned, which is useful when no predefined options are available. In Section 4.5, we progressively reduce the amount of options available to the agent, and demonstrate how OOIs still allow good memory-based policies to emerge when a sub-optimal amount of options are used. Our experiments demonstrate how OOIs allow to reach optimal policies in real-world POMDPs with minimal engineering effort.\nAll our results are averaged across 20 runs, with standard deviation represented by the light regions in the figures."
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "All our agents learn their policy over options and option policies (if not fixed) using a single feed-forward neural network, with one hidden layer of 100 neurons, trained using Policy Gradient (Sutton et al. 2000) and the Adam optimizer (Kingma and Ba 2014). Our neural network π takes three inputs and produces one output. The inputs are observation features x = φ(xt), the one-hot encoded current option ω (ω = 0 when executing the top-level policy), and a mask, mask. The output is a distribution y over extended options:\nh1 = tanh(W1[x TωT ]T + b1),\nŷ = σ(W2h1 + b2) ◦mask,\ny = ŷ\n1T ŷ ,\nwith Wi and bi the trainable weights and biases of layer i, σ the sigmoid function, and ◦ the element-wise product of two vectors. The fraction ensures that a valid probability distribution is produced by the network2. The set of extended options Õ is the union of the set A of actions and the set O of options, multiplied by 2 as the agent is able to select an action while terminating (or not) the current option3:\n2A Softmax over allowed options did not provide results as good as this implementation\n3βωt(xt) = ∑ a π(xt, ωt, a+ terminate)\nÕ ≡ (O ∪A)× {end, continue}\nThe initiation sets of options are implemented using the mask input of the neural network, a vector of |Õ| integers. When ω = 0, the mask forces the probability of primitive actions to zero, preserves option ωi according to Iωi , and prevents the top-level policy from terminating. When ω 6= 0, the mask only allows primitive actions to be executed. For instance, if there are two options and three actions, mask = endcont ( 0 0 1 1 1 0 0 1 1 1 ) when executing any of the options. When executing the top-level policy, mask = end cont ( 0 0 0 0 0 a b 0 0 0 ), with a = 1 if and only if the option that has just finished is in the initiation set of the first option, and b = 1 according to the same rule but for the second option. The neural network π is trained using Policy Gradient, with the following loss:\nL(π) = − T∑ t=0 (Rt − V (xt, ωt)) log(π(xt, ωt, at))\nwith at ∼ π(xt, ωt, ·) the action executed at time t. The returnRt = ∑T τ=t γ\nτrτ , with rτ = R(sτ , aτ , sτ+1), is a simple discounted sum of future rewards, and ignores changes of current option. This gives the agent information about the complete outcome of an action or option, by directly evaluating its flattened policy. A baseline V (xt, ωt) is used to reduce the variance of the L estimate (Sutton et al. 2000). V (xt, ωt) predicts the expected cumulative reward obtainable from xt in option ωt using a separate neural network, trained on the monte-carlo return obtained from xt in ωt."
    }, {
      "heading" : "4.2 Comparison with LSTM over Options",
      "text" : "In order to provide a complete evaluation of OOIs, a variant of the π network of Section 4.1, where the hidden layer is replaced with a layer of 100 LSTM units (Hochreiter and Schmidhuber 1997; Sridharan et al. 2010), is also evaluated on every task. In every experiment, the agent based on the LSTM network manages to reach the optimal policy, but it requires hundreds of thousands of episodes to do so, an order of magnitude more than options with OOIs. We explain this result by the continuous nature of LSTM recurrent connections, and the fact that the LSTM agents are unable to exploit the high-level structure of the task, encoded by OOIs. Their policy spaces are therefore larger, and more difficult to search in. To keep our graphs readable, we do not show the full length of the learning curves of the LSTM agents."
    }, {
      "heading" : "4.3 Object Gathering",
      "text" : "The first experiment illustrates how OOIs can be applied to a complex real-world robotic partially observable task, leading to a stochastic top-level policy significantly outperforming an expert-provided policy. This result is particularly important as, combined with the simplicity of OOIs, it illustrates the benefits of using reinforcement learning for industrial tasks, even when existing policies already seem satisfactory.\nThe experiment takes place in the environment described in Section 1.1. A robot has to gather objects one by one\nfrom two terminals, green and blue, and bring them back to a root location. When the robot reaches one of the terminals, it either picks up an object and receives a reward of +2, or receives a reward of -2 when the terminal is empty. Whether a terminal is full or empty is observed by the agent only when it is at the terminal. At the beginning of the episode, each terminal contains 2 or 3 objects, this amount being selected randomly for each terminal. When the agent goes to an empty terminal, the other one is re-filled with 2 or 3 objects. The episode ends after 2 or 3 emptyings (combined across both terminals). As detailed in Section 1.1, the agent only has a first-person view of the world, cannot observe terminals from far away, and therefore has to remember information acquired at terminals in order to make good decisions at the root. When at the root, 10 feet away from any terminal, the agent has no way of knowing which terminal is empty, if any.\nThe agent has access to 12 memoryless options that go to red (ωR1..R4), green (ωG1..G4) or blue objects (ωB1..B4), and terminate when the agent is close enough to them to read a QR-code displayed on them. The initiation set of ωR1,R2 is ωG1..G4, of ωR3,R4 is ωB1..B4, and of ωGi,Bi is ωRi ∀i = 1..4. This description of the options and their OOIs is purposefully uninformative, and illustrates how little information the agent has about the task. The option set used in this experiment is also richer than the simple example of Section 3.1, to better illustrate the generality of OOIs.\nAgents with and without OOIs learn top-level policies over these options. We compare them to a fixed agent, using an expert top-level policy that interprets the options as follows: ωR1..R4 go to the root from a full/empty green/blue terminal (and are selected accordingly at the terminals depending on the QR-code displayed on them), while ωG1..G4,B1..B4 go to the green/blue terminal from the root when the previous terminal was full/empty and green/blue. At the root, the expert top-level policy outputs a uniform probability distribution over go to green after a full green, go to green after an empty blue, go to blue after a full blue and go to blue after an empty green. OOIs ensure that only a single of these options is selected, according to what the last\nterminal was and whether it was full or empty. The agent goes to a terminal until it is empty, then goes back to the root and starts emptying the other terminal, which leads to an average reward of 7.5.4\nWhen the top-level policy is not fixed, OOIs allow the task to be learned, as shown in Figure 4. Because experiments on a robot are slow, we developed a small simulator for this task, and used it to produce Figure 4 after having successfully asserted its accuracy using two 1000-episodes runs on the actual robot. The agent learns to properly select ωR1..R4 at the terminals, depending on the QR-code, and to output a proper distribution over options at the root. Interestingly, the agent converges to a stochastic policy (as permitted by Policy Gradient, see Section 4.1). Instead of always emptying a terminal before switching to the other one, the agent switches terminal early with a small probability, which reduces its chances of observing an empty terminal before having to switch anyway. This policy leads to an average reward of 7.825, significantly higher than the 7.5 average obtained by our expert policy (p < 10−10, computed from 2000 episodes). This demonstrates how reinforcement learning, options and OOIs can be applied to a complex partially observable task, fine-tuning probabilities more precisely than a human could do, to lead to a policy achieving better results than one produced by a human expert, even if the task seems easy to solve at first glance.\nBecause fixed option policies are not always available, we now show that OOIs allow them to be learned at the same time as the top-level policy."
    }, {
      "heading" : "4.4 Modified DuplicatedInput",
      "text" : "In some cases, a hierarchical reinforcement learning agent may not have been provided policies for several or all its options. In this case, OOIs allow the agent to learn its top-level policy, the option policies and their termination functions. In this experiment, the agent has to learn its top-level and option policies to copy characters from an input tape to an\n42.5×(−2+2.5×2), 2 or 3 emptyings of terminals that contain 2 or 3 objects. Average confirmed experimentally from 32 runs on the actual robot, p > 0.49.\noutput tape, removing duplicate B’s and D’s (mapping ABBCCEDD to ABCCED for instance; B’s and D’s always appear in pairs). The agent only observes a single input character at a time, and can write at most one character to the output tape per time-step.\nThe input tape is a sequence of N symbols x ∈ Ω, with Ω = {A,B,C,D,E} and N a random number between 20 and 30. The agent observes a single symbol xt ∈ Ω, read from the i-th position in the input sequence, and does not observe i. When t = 1, i = 0. The action set A contains 20 actions, each of them representing a symbol, whether it must be pushed onto the output tape, and whether i should be incremented or decremented. A reward of 1 is given for each correct symbol written to the output tape. The episode finishes with a reward of -0.5 when an error is made.\nThe agent has access to two options, ω1 and ω2. OOIs are designed so that ω2 cannot follow itself, with no such restriction on ω1. No reward shaping or hint about what each option should do is provided. The agent automatically discovers that ω1 must copy the current character to the output, and that ω2 must skip the character without copying it. It also learns the top-level policy, that selects ω2 (skip) when observing B or D and ω2 is allowed, ω1 otherwise (copy).\nFigure 5 shows that an agent with two options and OOIs learns an optimal policy for this task, while an agent with two options and only standard initiation sets (Iω = Ω ∀ω) fails to do so. The agent without OOIs only learns to copy characters and never skips any (having two options does not help it). This shows that OOIs are necessary for learning this task, and allow to learn a policy over options and option policies suited to our repetitive partially observable task.\nThe first two experiments use a set of options tailored to the task to solve. That set may not always be known, and the last experiment shows that OOIs provide large benefits over standard initiation sets and LSTM over options even when a sub-optimal amount of options are available."
    }, {
      "heading" : "4.5 TreeMaze",
      "text" : "Defining an extensive set of options and their OOIs may sometimes require more knowledge about the task and the environment than available. This experiment shows that a sub-optimal set of options, arising from a mis-specification of the environment or normal trial-and-error in design phase, does not prevent agents with OOIs from learning reasonably\ngood policies. TreeMaze is our generalization of the T-maze environment (Bakker 2001) to arbitrary heights. The agent starts at the root of the tree-like maze depicted in Figure 6, and has to reach the extremity of one of the 8 leaves. The leaf to be reached (the goal) is chosen uniformly randomly before each episode, and is indicated to the agent using 3 bits, observed one at a time during the first 3 time-steps. The agent receives no bit afterwards, and has to remember them in order to navigate to the goal. The agent observes its position in the current corridor (0 to 4) and the number of T junctions it has already crossed (0 to 3). A reward of -0.1 is given each time-step, +10 when reaching the goal. The episode finishes when the agent reaches any of the leaves. The optimal reward is 8.2.\nWe consider 14 options with predefined memoryless policies, several of them sharing the same policy, but encoding distinct states (among 14) of a 3-bit memory where some bits may be unknown. 6 partial-knowledge options ω0−−, ω1−−, ω00−, ..., ω11− go right then terminate. 8 full-knowledge options ω000, ω001, ..., ω111 go to their corresponding leaf. OOIs are defined so that any option may only be followed by itself, and every option that represents a memory state where a single 0 or - has been flipped to 1. Three agents have to learn their top-level policy, which requires them to discover how to use the available options to represent and update their implicit memory. The agents do not know the name or meaning of the options, and do not even know that an implicit memory is required for this task. The first agent has access to all 14 options. The second agent only has access to complete-knowledge options, and therefore cannot disambiguate unknown and 0 bits. The third agent is restricted to options ω000, ω010, ω100 and ω110 and therefore cannot reach odd-numbered goals. The options of the second and third agents terminate in the first two cells of the first corridor, to allow the top-level policy to observe the second and third bits.\nFigure 7 shows that OOIs allow the agent with 14 options to consistently learn the optimal policy for this task. When\nthe number of options is reduced, the quality of the resulting policies decreases, while still remaining above the agent without OOIs. Even the agent with 4 options, that cannot reach half the goals, performs better than the agent without OOIs. This experiment demonstrates that OOIs provide measurable benefits over standard initiation sets, even if the option set is largely reduced.\nCombined, our three experiments demonstrate that OOIs lead to optimal policies in challenging POMDPs, consistently outperform LSTM over options (and a carefullydesigned expert policy in Section 4.3), allow the option policies to be learned, and are robust to insufficient amounts of options."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "This paper proposes OOIs, an extension of the initiation sets of options so that they restrict which options are allowed to be executed after one terminates. This makes options as expressive as Finite State Controllers. Experimental results confirm that challenging partially observable tasks, simulated or on physical robots, one of them requiring exact information storage for hundreds of time-steps, can now be solved using options. On our robotic task, options with OOIs lead to a stochastic policy better than our expert one. The hierarchical nature of options and simplicity of OOIs enable us to explain the reasoning of the agent, potentially allowing it to be applied to new tasks.\nOptions with OOIs also perform surprisingly well compared to an LSTM network over options. While LSTM over options does not require the design of OOIs, efficient recurrent neural networks have several hyperparameters to be tuned, and their ability to learn without any a-priori knowledge comes at the cost of sample efficiency and explainability. OOIs therefore provide a compelling alternative, applicable to a wide range of problems.\nFinally, the compatibility between OOIs and a large variety of reinforcement learning algorithms leads to many future research opportunities. For instance, we have obtained very encouraging results in continuous action spaces, using CACLA (Van Hasselt and Wiering 2007) to implement parametric options, that take continuous arguments when executed, in continuous-action hierarchical POMDPs."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The first author is “Aspirant” with the Science Foundation of Flanders (FWO, Belgium), grant number 1129317N. The second author is “Postdoctoral Fellow” with the FWO, grant number 12J0617N.\nThanks to Finn Lattimore, who gave a computer to the first author, so that he could finish this paper while attending the UAI 2017 conference in Sydney, after his own computer unexpectedly fried."
    } ],
    "references" : [ {
      "title" : "Neural Networks",
      "author" : [ "Peter J. Angeline", "Gregory M. Saunders", "Jordan B. Pollack. An evolutionary algorithm that constructs recurrent neural networks. IEEE Trans" ],
      "venue" : "5(1):54–65,",
      "citeRegEx" : "Angeline et al. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Reinforcement learning with long short-term memory",
      "author" : [ "Bram Bakker" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bakker 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Discrete Event Dynamic Systems: Theory and Applications",
      "author" : [ "Andrew G Barto", "Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning" ],
      "venue" : "13(12):341–379,",
      "citeRegEx" : "Barto and Mahadevan 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The International Journal of Robotics Research",
      "author" : [ "Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon. Closing the learning-planning loop with predictive state representations" ],
      "venue" : "30(7):954–966,",
      "citeRegEx" : "Boots et al. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Acting optimaly in partially observable stochastic domains",
      "author" : [ "A Cassandra", "L Kaelbling", "M Littman" ],
      "venue" : "AAAI, (April)",
      "citeRegEx" : "Cassandra et al. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Hybrid computing using a neural network with dynamic external memory",
      "author" : [ "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis" ],
      "venue" : null,
      "citeRegEx" : "Cain et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cain et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural Computation",
      "author" : [ "Sepp Hochreiter", "Jurgen Jürgen Schmidhuber. Long short-term memory" ],
      "venue" : "9(8):1–32,",
      "citeRegEx" : "Hochreiter and Schmidhuber 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "In Proceedings of the 32nd International Conference on Machine Learning (ICML)",
      "author" : [ "Rafal Józefowicz", "Wojciech Zaremba", "Ilya Sutskever. An empirical exploration of recurrent network architectures" ],
      "venue" : "pages 2342–2350,",
      "citeRegEx" : "Józefowicz et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Carnegie-Mellon University",
      "author" : [ "Long-Ji Lin", "Tom M Mitchell. Memory approaches to reinforcement learning in nonMarkovian domains" ],
      "venue" : "Department of Computer Science,",
      "citeRegEx" : "Lin and Mitchell 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "From animals to animats",
      "author" : [ "Long-Ji Lin", "Tom M Mitchell. Reinforcement learning with hidden states" ],
      "venue" : "2:271–280,",
      "citeRegEx" : "Lin and Mitchell 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Predictive Representations of State",
      "author" : [ "Littman" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Littman,? \\Q2001\\E",
      "shortCiteRegEx" : "Littman",
      "year" : 2001
    }, {
      "title" : "Proceedings of the fifteenth conference on uncertainty in artificial intelligence",
      "author" : [ "Nicolas Meuleau", "Leonid Peshkin", "Kee-eung Kim", "Leslie Pack Kaelbling. Learning FiniteState Controllers for partially observable environments" ],
      "venue" : "pages 427–436,",
      "citeRegEx" : "Meuleau et al. 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "and Marc’Aurelio Ranzato",
      "author" : [ "Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michaël Mathieu" ],
      "venue" : "Learning longer memory in recurrent neural networks. CoRR, abs/1412.7753,",
      "citeRegEx" : "Mikolov et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Asynchronous Methods for Deep Reinforcement Learning",
      "author" : [ "Mnih" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Mnih,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih",
      "year" : 2016
    }, {
      "title" : "Sixteenth International Conference on Machine Learning",
      "author" : [ "Leonid Peshkin", "Nicolas Meuleau", "Leslie Kaelbling. Learning Policies with External Memory" ],
      "venue" : "page 8,",
      "citeRegEx" : "Peshkin et al. 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "PhD thesis",
      "author" : [ "Doina Precup. Temporal Abstraction in Reinforcement Learning" ],
      "venue" : "University of Massachusetts,",
      "citeRegEx" : "Precup 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Springer Tracts in Advanced Robotics",
      "author" : [ "Nicholas Roy", "Geoffrey Gordon", "Sebastian Thrun. Planning under uncertainty for reliable health care robotics" ],
      "venue" : "24:417–426,",
      "citeRegEx" : "Roy et al. 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Planning to see: A hierarchical approach to planning visual actions on a robot using POMDPs",
      "author" : [ "Mohan Sridharan", "Jeremy Wyatt", "Richard Dearden" ],
      "venue" : "Artificial Intelligence, 174(11):704–725,",
      "citeRegEx" : "Sridharan et al. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Richard Sutton", "Doina Precup", "Satinder Singh" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Sutton et al. 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard Sutton", "David McAllester", "Satinder Singh", "Yishay Mansour" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sutton et al. 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A Deep Hierarchical Approach to Lifelong Learning in Minecraft",
      "author" : [ "Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor" ],
      "venue" : "13th European Workshop on Reinforcement Learning,",
      "citeRegEx" : "Tessler et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "PhD thesis",
      "author" : [ "Georgios Theocharous. Hierarchical learning", "planning in partially observable Markov decision processes" ],
      "venue" : "Michigan State University,",
      "citeRegEx" : "Theocharous 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning",
      "author" : [ "Hado Van Hasselt", "Marco A. Wiering. Reinforcement learning in continuous action spaces" ],
      "venue" : "pages 272–279,",
      "citeRegEx" : "Van Hasselt and Wiering 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Adaptive Behavior",
      "author" : [ "Marco Wiering", "Jürgen Schmidhuber. HQ-Learning" ],
      "venue" : "6(2):219– 246,",
      "citeRegEx" : "Wiering and Schmidhuber 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Reinforcement Learning Neural Turing Machines",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever" ],
      "venue" : "Arxiv,",
      "citeRegEx" : "Zaremba and Sutskever 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Most real-world reinforcement learning problems have a hierarchical nature, and often exhibit some degree of partial observability. While hierarchy and partial observability are usually tackled separately, for instance by combining recurrent neural networks and options, we show that addressing both problems simultaneously is simpler and more efficient in many cases. More specifically, we make the initiation set of options conditional on the previously-executed option, and show that options with such Option-Observation Initiation Sets (OOIs) are at least as expressive as Finite State Controllers (FSCs), a state-of-the-art approach for learning in POMDPs. In contrast to other hierarchical methods in partially observable environments, OOIs are easy to design based on an intuitive description of the task, lead to explainable policies and keep the top-level and option policies memoryless. Our experiments show that OOIs allow agents to learn optimal policies in challenging POMDPs, outperforming an human-provided policy in our robotic experiment, while learning much faster than a recurrent neural network",
    "creator" : "LaTeX with hyperref package"
  }
}