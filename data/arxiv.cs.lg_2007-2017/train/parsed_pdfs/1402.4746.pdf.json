{
  "name" : "1402.4746.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Near-optimal-sample estimators for spherical Gaussian mixtures",
    "authors" : [ "Jayadev Acharya", "Ashkan Jafarpour" ],
    "emails" : [ "jacharya@ucsd.edu", "ashkan@ucsd.edu", "alon@ucsd.edu", "asuresh@ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 2.\n47 46\nv1 [\ncs .L\nG ]\n1 9\nFe b\nFor mixtures of any k d-dimensional spherical Gaussians, we derive an intuitive spectral-estimator that uses Ok(d log2 dǫ4 ) samples and runs in time Ok,ǫ(d3 log5 d), both significantly lower than previously known. The constant factor Ok is polynomial for sample complexity and is exponential for the time complexity, again much smaller than what was previously known. We also show that Ωk( dǫ2 ) samples are needed for any algorithm. Hence the sample complexity is near-optimal in the number of dimensions.\nWe also derive a simple estimator for k-component one-dimensional mixtures that uses O(k log kǫ ǫ2 ) samples and runs in time Õ ((k\nǫ )3k+1). Our other technical contributions include a faster algorithm for\nchoosing a density estimate from a set of distributions, that minimizes the ℓ1 distance to an unknown underlying distribution.\n∗jacharya@ucsd.edu †ashkan@ucsd.edu ‡alon@ucsd.edu §asuresh@ucsd.edu"
    }, {
      "heading" : "1 Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 Background",
      "text" : "Meaningful information often resides in high-dimensional spaces: voice signals are expressed in many frequency bands, credit ratings are influenced by multiple parameters, and document topics are manifested in the prevalence of numerous words. Some applications, such as topic modeling and genomic analysis consider data in over 1000 dimensions, [17, 44].\nTypically, information can be generated by different types of sources: voice is spoken by men or women, credit parameters correspond to wealthy or poor individuals, and documents address topics such as sports or politics. In such cases the overall data follow a mixture distribution [26, 36, 38].\nMixtures of high-dimensional distributions are therefore central to the understanding and processing of many natural phenomena. Methods for recovering the mixture components from the data have consequently been extensively studied by statisticians, engineers, and computer scientists.\nInitially, heuristic methods such as expectation-maximization (EM) were developed [27, 35]. Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23]. Many of these algorithms consider mixtures where the ℓ1 distance between the mixture components is 2 − od(1), namely approaches the maximum of 2 as d increases. They identify the distribution components in time and samples that grow polynomially in the dimension d. Recently, [22, 29] showed that any d-dimensional Gaussian mixture can be recovered in polynomial time. However, their algorithm uses > d100 time and samples.\nA different approach that avoids the large component-distance requirement and the high time and sample complexity, considers a slightly more relaxed notion of approximation, sometimes called PAC learning. PAC learning [24] does not approximate each mixture component, but instead derives a mixture distribution that is close to the original one. Specifically, given a distance bound ǫ > 0, error probability δ > 0, and samples from the underlying mixture f , where we use boldface letters for d-dimensional objects, PAC learning seeks a mixture estimate f̂ with at most k components such that D(f , f̂) ≤ ǫ with probability ≥ 1− δ, where D(⋅, ⋅) is some given distance measure, for example ℓ1 distance or KL divergence. This notion of estimation is also known as proper learning in the literature.\nAn important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means. Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42]."
    }, {
      "heading" : "1.2 Sample complexity",
      "text" : "Reducing the number of samples is of great practical significance. For example, in topic modeling every sample is a whole document, in credit analysis every sample is a person’s credit history, and in genetics, every sample is a human DNA. Hence samples can be very scarce and obtaining them can be very costly. By contrast, current CPUs run at several Giga Hertz, hence samples are typically much more scarce of a resource than time.\nNote that for one-dimensional statistical problems, the need for sample-efficient algorithms has been broadly recognized. The sample complexity of many problems is known quite accurately, often to within a constant factor. For example, for discrete distributions over {1, . . . ,s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using Θ(s/ log s) samples.\nLearning one-dimensional m-modal distributions over {1, . . . ,s} requires Θ(m log(s/m)/ǫ3) samples [14]. Similarly, one-dimensional mixtures of k structured distributions (log-concave, monotone hazard rate, and unimodal) over {1, . . . ,s} can be learned with O(k/ǫ4), O(k log(s/ǫ)/ǫ4), and O(k log(s)/ǫ4) samples, respectively, and these bounds are tight up to a factor of ǫ [31].\nCompared to one dimensional problems, in high dimensions there is a polynomial gap in the sample complexity. For example, for learning spherical Gaussian mixtures, the number of samples required by previous algorithms is O(d12) for k = 2 components, and increased exponentially with k [19]. In this paper we bridge this gap, by constructing near-linear sample complexity estimators."
    }, {
      "heading" : "1.3 Previous and new results",
      "text" : "Our main contribution is PAC learning d dimensional Gaussian mixtures with near-linear samples. We show few auxiliary results for one-dimensional Gaussians.\n1.3.1 d-dimensional Gaussian mixtures\nSeveral papers considered PAC learning of discrete- and Gaussian-product mixtures. [20] considered mixtures of two d-dimensional Bernoulli products where all probabilities are bounded away from 0. They showed that this class is PAC learnable in Õ(d2/ǫ4) time and samples, where the Õ notation hides logarithmic factors. [18] eliminated the probability constraints and generalized the results from binary to arbitrary discrete alphabets, and from 2 to k mixture components. They showed that mixtures of k discrete products are PAC learnable in Õ((d/ǫ)2k2(k+1)) time, and although they did not explicitly mention sample complexity, their algorithm uses Õ((d/ǫ)4(k+1)) samples. [19] generalized these results to Gaussian products, showing in particular that mixtures of k Gaussians, where the difference between the means normalized by the ratio of standard deviations is bounded by B, are PAC learnable in Õ((dB/ǫ)2k2(k+1)) time, and can be shown to use Õ((dB/ǫ)4(k+1)) samples. These algorithms consider the KL divergence between the distribution and its estimate, but it can be shown that the ℓ1 distance would result in similar complexities. It can also be shown that these algorithms or their simple modifications have similar time and sample complexities for spherical Gaussians as well.\nOur main contribution shows that mixtures of spherical-Gaussians are PAC learnable in ℓ1 distance with sample complexity that is nearly linear in the dimension. Specifically, Theorem 8 shows that mixtures of k spherical-Gaussian distributions can be learned in\nn = O(dk9 ǫ4 log2 d δ ) = Ok,ǫ(d log2 d)\nsamples and\nO(n2d logn + d2(k7 ǫ3 log d δ )k2) = Õk,ǫ(d3).\ntime. Observe that recent algorithms typically construct the covariance matrix [19,42], hence require ≥ nd2 time. In that sense, for small values of k, the time complexity we derive is comparable to the best such algorithms can hope for. Observe also that the exponential dependence on k is of the form d2(k7 ǫ3 log d δ )k2 , which is significantly lower than the dO(k 3) dependence in previous results.\nBy contrast, Theorem 2 shows that PAC learning k-component spherical Gaussian mixtures require Ω(dk/ǫ2) samples for any algorithm, hence our distribution learning algorithms are nearly sample optimal. In addition, their time complexity significantly improves on previously known ones."
    }, {
      "heading" : "1.3.2 One-dimensional Gaussian mixtures",
      "text" : "Independently and around the same time as this work [15] showed that mixtures of two one-dimensional Gaussians can be learnt with Õ(ǫ−2) samples and in time O(ǫ−7.01). We provide a natural estimator for learning mixtures of k one dimensional Gaussians using some basic properties of Gaussian distributions and show that mixture of any k-one dimensional Gaussians can be learnt with Õ(kǫ−2) samples and in time Õ ((k\nǫ )3k+1)."
    }, {
      "heading" : "1.4 The approach and technical contributions",
      "text" : "The popular SCHEFFE estimator takes a collection F of distributions and uses O(log ∣F∣) independent samples from an underlying distribution f to find a distribution in F whose distance from f is at most a constant factor larger than that of the distribution in F that is closet to f [16]. In Lemma 1, we lower the time complexity of the Scheffe algorithm from O(∣F∣2) time to Õ(∣F∣), helping us reduce the time complexity of our algorithms.\nOur goal is therefore to construct a small class of distributions that is ǫ-close to any possible underlying distribution. For simplicity, consider spherical Gaussians with the same variance and means bounded by B. Take the collection of all distributions derived by quantizing the means of all components in all coordinates to ǫm accuracy, and quantizing the weights to ǫw accuracy. It can be shown that to get distance ǫ from the underlying distribution, it suffices to take ǫm, ǫw ≤ 1/polyǫ(dk). There are at most ( Bǫm )dk ⋅ ( 1ǫw )k = 2Õǫ(dk) possible combinations of the k mean vectors and weights. Hence SCHEFFE implies an exponential-time algorithm with sample complexity Õ(dk).\nTo reduce the dependence on d, one can approximate the span of the k mean vectors. This reduces the problem from d to k dimensions, allowing us to consider a distribution collection of size 2O(k\n2), with SCHEFFE sample complexity of just O(k2). [18, 19] constructs the sample correlation matrix and uses k of its columns to approximate the span of mean vectors. This approach requires the k columns of the sample correlation matrix to be very close to the actual correlation matrix, and thus requires a lot more samples.\nWe derive a spectral algorithm that uses the top k eigenvectors of the sample covariance matrix to approximate the span of the k mean vectors. Since we use the entire covariance matrix instead of just k columns, a weaker concentration is sufficient and we gain on the sample complexity.\nUsing recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in Õ(d) samples. This result allows us to address most “reasonable” distributions, but still there are some “corner cases” that need to be analyzed separately. To address them, we modify some known clustering algorithms such as single-linkage, and spectral projections. While the basic algorithms were known before, our contribution here, which takes a fair bit of effort and space, is to show that judicious modifications of the algorithms and rigorous statistical analysis yield polynomial time algorithms with near optimal sample complexity.\nOur approach applies most directly to mixtures of spherical Gaussians. We provide a simple and practical recursive clustering and spectral algorithm that estimates all such distributions in Ok(d log2 d) samples.\nThe paper is organized as follows. In Section 2, we introduce notations, describe results on the Scheffe estimator, and state a lower bound. In Section 3, we present the algorithm for k-spherical Gaussians. In Section 4 we show a simple learning algorithm for one-dimensional Gaussian mixtures. To preserve readability, most of the technical details and proofs are given in the appendix."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Notation",
      "text" : "For arbitrary product distributions p1, . . . ,pk over a d dimensional space let pj,i be the distribution of pj over coordinate i, and let µj,i and σj,i be the mean and variance of pj,i respectively. Let f = (w1, . . . ,wk,p1, . . . ,pk) be the mixture of these distributions with mixing weights w1, . . . ,wk . We denote estimates of a quantity x by x̂. It can be empirical mean or a more complex estimate. ∣∣⋅∣∣ denotes the spectral norm of a matrix and∣∣⋅∣∣2 denotes the ℓ2 norm of a vector."
    }, {
      "heading" : "2.2 Selection from a pool of distributions",
      "text" : "Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20]. Our algorithm also obtains a set of distributions containing at least one that is close to the underlying in ℓ1 distance. The estimation problem now reduces to the following. Given a class F of distributions and samples from an unknown distribution f , find a distribution in F that is close to f . Let D(f ,F) def= minfi∈F D(f , fi).\nThe well-known Scheffe’s method [16] uses O(ǫ−2 log ∣F∣) samples from the underlying distribution f , and in time O(ǫ−2∣F∣2T log ∣F∣) outputs a distribution in F with ℓ1 distance of at most 9.1max(D(f ,F), ǫ) from f , where T is the time required to compute the probability of an x ∈ X by a distribution in F . A naive application of this algorithm requires time quadratic in the number of distributions in F . We propose a variant of this, that works in near linear time, albeit requiring slightly more samples. More precisely,\nLemma 1 (Appendix B). Let ǫ > 0. For some constant c, given c ǫ2 log( ∣F∣ δ ) independent samples from a distribution f , with probability ≥ 1 − δ, the output f̂ of MODIFIED SCHEFFE D(f̂ , f) ≤ 1000max(ǫ,D(f ,F)). Furthermore, the algorithm runs in time O( ∣F∣T log(∣F∣/δ)\nǫ2 ).\nWe therefore find a small class F with at least one distribution close to the underlying mixture. For our problem of estimating k component mixtures in d-dimensions, T = O(dk) and ∣F∣ = Õk,ǫ(d2). Note that we have not optimized the constant 1000 in the above lemma."
    }, {
      "heading" : "2.3 Lower bound",
      "text" : "Using Fano’s inequality, we show an information theoretic lower bound of Ω(dk/ǫ2) samples to learn kcomponent d-dimensional mixtures of spherical Gaussians for any algorithm. More precisely,\nTheorem 2 (Appendix C). Any algorithm that learns all k-component d-dimensional spherical Gaussian mixtures up to ℓ1 distance ǫ with probability ≥ 1/2 requires at least Ω(dkǫ2 ) samples."
    }, {
      "heading" : "3 Mixtures in d dimensions",
      "text" : "3.1 Description of LEARN k-SPHERE\nAlgorithm LEARN K-SPHERE learns mixtures of k spherical Gaussians using near-linear samples. For clarity, we assume that all components have the same variance σ2, i.e., pi = N(µi, σ2Id) for 1 ≤ i ≤ k. A modification of this algorithm works for components with different variances. The core ideas are same and we include it in the final version of the paper.\nThe easy part of the algorithm is estimating σ2. If X(1) and X(2) are two samples from the same component, then X(1)−X(2) is distributed N(0,2σ2Id). Hence for large d, ∣∣X(1) −X(2)∣∣22 concentrates around 2dσ2. By the pigeon-hole principle, given k+1 samples, two of them are from the same component. Therefore, the minimum pairwise distance between k + 1 samples is close to 2dσ2. This constitutes the first step of our algorithm.\nWe now concentrate on estimating the means. As stated in the introduction, given the span of the mean vectors µi, we can grid the k dimensional span to the required accuracy ǫg and use SCHEFFE, to obtain a polynomial time algorithm. One of the natural and well-used methods to estimate the span of mean vectors is using the correlation matrix [42]. Consider the correlation-type matrix,\nS = 1\nn n∑ i=1 X(i)X(i)t − σ2Id. In expectation, the fraction of terms from pi is wi. Furthermore for a sample X from a particular component j, E[XXt] = σ2Id +µjµj t. It follows that\nE[S] = k∑ j=1 wjµjµj t.\nTherefore, as n→∞, the matrix S converges to ∑kj=1wjµjµjt, and its top k eigenvectors span of means. While the above intuition is well understood, the number of samples necessary for convergence is not well studied. Ideally, irrespective of the values of the means, we wish Õ(d) samples to be sufficient for the convergence. However this is not true, as we demonstrate by a simple example. Example 3. Consider the special case, d = 1, k = 2, σ2 = 1, w1 = w2 = 1/2, and the difference of means∣µ1 − µ2∣ = L for a large L ≫ 1. Given this prior information, one can estimate the the average of the mixture, that yields µ1+µ2\n2 . Solving equations obtained by µ1 + µ2 and µ1 − µ2 = L, yields µ1 and µ2. The\nvariance of the mixture is 1 + L2 4 > L2 4 . With additional Chernoff type bounds, one can show that given n samples the error in estimating the average is\n∣µ1 + µ2 − µ̂1 − µ̂2∣ ≈ Θ( L√ n ) .\nTherefore to estimate the means to a small accuracy we need n ≥ L2, i.e., more the separation, more samples are necessary.\nA similar phenomenon happens in the convergence of the correlation matrices, where the variances of quantities of interest increases with separation. In other words, for the span to be accurate the number of samples necessary increases with the separation. To overcome this phenomenon, a natural idea is to cluster the Gaussians such that the means of components in the same cluster are close and then apply SCHEFFE on the span within each cluster.\nEven though spectral clustering algorithms are studied in [2,42], they assume that the weights are strictly bounded away from 0, which does not hold here. We use a simple recursive clustering algorithm that takes a cluster C with average µ(C). If there is a component in the cluster such that √wi ∣∣µi −µ(C)∣∣2 is Ω(log(n/δ)), then the algorithm divides the cluster into two nonempty clusters without any mis-clustering.\nFor technical reasons similar to the above example, we also use a coarse clustering algorithm that ensures that the mean separation is Õ(d1/4) within each cluster. The algorithm can be summarized as:\n1. Variance estimation: Use first k+1 samples and estimate the minimum distance among sample-pairs to estimate σ2.\n2. Coarse clustering: Using a single-linkage algorithm, group the samples such that within each cluster formed, the mean separation is smaller than Õ(d1/4).\n3. Recursive clustering: As long as there is a cluster that has samples from more than one component with means far apart, (described by a condition on the norm of its covariance matrix in the algorithm) estimate its largest eigenvector and project samples of this cluster onto this eigenvector and cluster them. This hierarchical method is continued until there are clusters that contain close-by-components. 4. Search in the span: The resulting clusters contain components that are close-by, i.e., ∣∣µi −µj ∣∣2 <O(k3/2σ̂2 log n δ ). We approximate the span of means by the top k − 1 eigenvectors and the mean\nvector, and perform an exhaustive search using MODIFIED SCHEFFE.\nWe now describe these steps stating the performance of each step.\nAlgorithm LEARN K-SPHERE Input: n samples x(1),x(2), . . . ,x(n) from f and ǫ.\n1. Sample variance: σ̂2 =mina≠b∶a,b∈[k+1] ∣∣x(a) − x(b)∣∣22 /2d. 2. Coarse single-linkage clustering: Start with each sample as a cluster,\n• While ∃ two clusters with squared-distance ≤ 2dσ̂2 + 23σ̂2√d log n2 δ , merge them.\n3. Recursive spectral-clustering: While there is a new cluster C with ∣C ∣ ≥ nǫ/5k and spectral norm of its sample covariance matrix ≥ 12k2σ̂2 logn3/δ,\n• Use nǫ/8k2 of the samples to find the largest eigenvector and discard these samples. • Project the remaining samples on the largest eigenvector.\n• Perform single-linkage in the projected space (as before) till the distance between clusters > 3σ̂ √ logn2k/δ creating new clusters.\n4. Exhaustive search: Let ǫg = ǫ/(16k3/2), L = 200√k4ǫ−1 log n2δ , and G ={−L, . . . ,−ǫg,0, ǫg ,2ǫg, . . . L}. Let W = {0, ǫ/(4k),2ǫ/(4k), . . . 1} and Σ def= {σ2 ∶ σ2 = σ̂2(1 + i/d)∀ − d < i ≤ d}.\n• For each cluster C find its top k − 1 eigenvectors u1,u2 . . .uk−1 and let Span(C) = {µ̂(C) +∑k−1i=1 giσ̂ui ∶ g1, g2 . . . gk−1 ∈ G}. • Let Span = {Span(C) ∶ ∣C ∣ ≥ nǫ/5k}. • For all w′i ∈ W , σ\n′2 ∈ Σ, µ̂i ∈ Span, add {(w′1, . . . ,w′k−1,1 −∑k−1i=1 w′i,N(µ̂1, σ′2), . . . ,N(µ̂k, σ′2)} in F .\n5. Run MODIFIED SCHEFFE on F and output the resulting distribution."
    }, {
      "heading" : "3.2 Sketch of correctness",
      "text" : "To simplify the bounds and expressions, we assume that d > 1000 and δ ≥ min(2n2e−d/10,1/3). For smaller values of δ, we run the algorithm with error 1/3 and repeat it O(log 1 δ ) times to choose a set of\ncandidate mixtures Fδ. By Chernoff-bound with error ≤ δ, Fδ contains a mixture ǫ-close to f . Finally, we run MODIFIED SCHEFFE on Fδ to obtain a mixture that is close to f . By the union bound and Lemma 1, the error is ≤ 2δ.\nVariance estimation: Let σ̂ be the variance estimate from step 1. In high dimensions, the difference between two random samples from a Gaussian concentrates. This is made precise in the next lemma which states σ̂ is a good estimate of the variance. Then the following is a simple application of Gaussian tail bounds.\nLemma 4 (Appendix D.1). Given n samples from the k-component mixture, with probability 1 − 2δ, ∣σ̂2 − σ2∣ ≤ 2.5σ2 √ log(n2/δ)\nd .\nCoarse single-linkage clustering: The second step is a single-linkage routine that clusters mixture components with far means. Single-linkage is a simple clustering scheme that starts out with each data point as a cluster, and at each step merges the two that are closest to form larger clusters. The algorithm stops when the distance between clusters is larger than a pre-specified threshold.\nSuppose the samples are generated by an one-dimensional mixture of k components that are far, then with high probability, when the algorithm generates k clusters and all the samples within a cluster are generated by a single component. More precisely, if ∀i, j ∈ [k], ∣µi − µj ∣ = Ω(σ logn), then all the n samples concentrate around their respective means and the separation between any two samples from different components would be larger than the largest separation between any two samples from the same component. Hence for a suitable value of threshold, single-linkage correctly identifies the clusters. For d-dimensional Gaussian mixtures a similar notion holds true, with minimum separation Ω(d1/4 log n δ ). More precisely, Lemma 5 (Appendix D.2). After Step 2 of LEARN K-SPHERE, with probability ≥ 1 − 2δ, all samples from each component will be in the same cluster and the maximum distance between two components within each cluster is ≤ 10kσ(d log n2 δ )1/4.\nRecursive spectral-clustering: The clusters formed at this step consists of components with mean separation O(d1/4 log n δ ). We now recursively zoom into the clusters formed and show that it is possible to cluster the components with much smaller mean separation. Note that since the matrix is symmetric, the largest magnitude of the eigenvalue is same as the spectral norm. We first find the largest eigenvector of\nS(C) def= 1∣C ∣( ∑ x∈C (x − µ̂(C))(x − µ̂(C))t) − σ̂2Id, which is the sample covariance matrix with its diagonal term reduced by σ̂2. If there are two components with means far apart, then using single-linkage we divide the cluster into two. The following lemma shows that this step performs accurate clustering of components with means well separated.\nLemma 6 (Appendix D.3). Let n ≥ c ⋅ dk4 ǫ log n 3 δ . After recursive clustering, with probability ≥ 1 − 4δ. the\nsamples are divided into clusters such that for each component i within any cluster C , √ wi ∣∣µi −µ(C)∣∣2 ≤\n25σ √ k3 log n 3\nδ . Furthermore, all the samples from one component remain in a single cluster.\nExhaustive search and Scheffe: After step 3, all clusters have a small weighted radius √ wi ∣∣µi −µ(C)∣∣2 ≤\n25σ √ k3 log n 3 δ , the the eigenvectors give an accurate estimate of the span of µi−µ(C) within each cluster. More precisely,\nLemma 7 (Appendix D.4). Let n ≥ c ⋅ dk9 ǫ4 log2 d δ\nfor some constant c. After step 3, with probability ≥ 1 − 7δ the following holds: if ∣C ∣ ≥ nǫ/5k, then the projection of [µi −µ(C)]/ ∣∣µi −µ(C)∣∣2 on the space orthogonal to the span of top k − 1 eigenvectors has magnitude ≤ ǫσ\n8 √ 2k √ wi∣∣µi−µ(C)∣∣2 .\nWe now have accurate estimates of the spans of the clusters and each cluster has components with close means. It is now possible to grid the set of possibilities in each cluster to obtain a set of distributions such that one of them is close to the underlying. There is a trade-off between a dense grid to obtain a good estimation and the computation time required. The final step takes the sparsest grid possible to ensure an error ≤ ǫ. This is quantized below.\nTheorem 8 (Appendix D.5). Let n ≥ c ⋅ dk9 ǫ4 log2 d δ for some constant c. Then Algorithm LEARN K-SPHERE\nwith probability ≥ 1 − 9δ, outputs a distribution f̂ such that D(f̂ , f) ≤ 1000ǫ. Furthermore, the algorithm runs in time O(n2d logn + d2(k7\nǫ3 log d δ )k2).\nNote that the run time is calculated based on the efficient implementation of single-linkage [37] and the exponential term is not optimized. We now study mixtures in one-dimension and provide an estimator using MODIFIED SCHEFFE."
    }, {
      "heading" : "4 Mixtures in one dimension",
      "text" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41]. We now provide a simple estimator for learning one dimensional mixtures using the MODIFIED SCHEFFE estimator proposed earlier. The d-dimension estimator uses spectral projections to find the span of means, whereas for one dimension case, we use a simple observation on properties of samples from Gaussians for estimation. Formally, given samples from f , a mixture of Gaussian distributions pi def = N(µi, σ2i ) with weights w1,w2, . . . wk, our goal is to find a mixture f̂ = (ŵ1, ŵ2, . . . ŵk, p̂1, p̂2, . . . p̂k)\nsuch that D(f, f̂) ≤ ǫ. Note that we make no assumption on the weights, means or the variances of the components.\nWe provide an algorithm that, using Õ(kǫ−2) samples and in time Õ(kǫ−3k−1), outputs an estimate that is at most ǫ from the underlying in ℓ1 distance with probability ≥ 1 − δ. Our algorithm is an immediate consequence of the following observation for samples from a Gaussian distribution. Lemma 9. Given n independent samples x1, . . . , xn from N(µ,σ2), there are two samples xj, xk such that∣xj − µ∣ ≤ σ 7 log 2/δ2n and ∣xj − xk − σ∣ ≤ 2σ 7 log 2/δ2n with probability ≥ 1 − δ. Proof The density of N(µ,σ2) is ≥ (7σ)−1 in the interval [µ−√2σ,µ+√2σ]. Therefore, the probability that a sample occurs in the interval µ−ǫσ,µ+ǫσ is ≥ 2ǫ/7. Hence, the probability that none of the n samples occurs in [µ − ǫσ,µ + ǫσ] is ≤ (1 − 2ǫ/7)n ≤ e−2nǫ/7. If ǫ ≥ 7 log 2/δ\n2n , then the probability that none of the\nsamples occur in the interval is ≤ δ/2. A similar argument shows that there is a sample within interval,[µ + σ − ǫσ,µ + σ + ǫσ], proving the lemma. The above observation can be translated into selecting a pool of candidate distributions such that one of the distributions is close to the underlying distribution.\nLemma 10. Given n ≥ 120k log 4k δ ǫ samples from a mixture f of k Gaussians. Let S = {N(xj , (xj − xk)2) ∶\n1 ≤ j, k ≤ n} be a set of Gaussians and W = {0, ǫ 2k , 2ǫ 2k . . . ,1} be the set of weights. Let F def= {ŵ1, ŵ2, . . . , ŵk−1,1 − k−1∑\ni=1 ŵi, p̂1, p̂2, . . . p̂k ∶ ŵi ∈W, p̂i ∈ S} be a set of n2k(2k ǫ )k−1 ≤ n3k−1 candidate mixture distributions. There exists a f̂ ∈ F such that D(f, f̂) ≤ ǫ. Proof. Let f = (w1,w2, . . . wk, p1, p2, . . . pk). For f̂ = (ŵ1, ŵ2, . . . , ŵk−1,1 − ∑k−1i=1 ŵi, p̂1, p̂2, . . . p̂k), by the triangle inequality,\nD(f, f̂) ≤ k−1∑ i=1 2∣ŵi −wi∣ + k∑ i=1 wiD(pi, p̂i). We show that there is a distribution in f̂ ∈ F such that the sum above is bounded by ǫ. Since we quantize the grids as multiples of ǫ/2k, we consider distributions in F such that each ∣ŵi −wi∣ ≤ ǫ/4k, and therefore∑i ∣ŵi −wi∣ ≤ ǫ2 .\nWe now show that for each pi there is a p̂i such that wiD(pi, p̂i) ≤ ǫ2k , thus proving that D(f, f̂) ≤ ǫ. If wi ≤ ǫ 4k\n, then wiD(pi, p̂i) ≤ ǫ2k . Otherwise, let w′i > ǫ4k be the fraction of samples from pi. By Lemma 9 and 14, with probability ≥ 1 − δ/2k,\nD(pi, p̂i)2 ≤ 2(µi − µ′i)2 σ2i + 16(σi − σ′i)2 σ2i\n≤ 25 log2 4k δ(nw′i)2 + 800 log2 4k\nδ(nw′i)2 ≤ 825 log2 4k\nδ(nw′i)2 . Therefore,\nwiD(pi, p̂i) ≤ 30wi log 4kδ nw′i .\nSince wi > ǫ/4k, with probability ≥ 1 − δ/2k, wi ≤ 2w′i. By the union bound with probability ≥ 1 − δ/k, wiD(pi, p̂i) ≤ 60 log 4kδn . Hence if n ≥ 120k log 4kδǫ , the above quantity is less than ǫ/2k. The total error probability is ≤ δ by the union bound.\nRunning MODIFIED SCHEFFE algorithm on the above set of candidates F yields a mixture that is close to the underlying one. By Lemma 1 and the above lemma we get\nCorollary 11. Let n ≥ c ⋅ k log kǫδ ǫ2 for some constant c. There is an algorithm that runs in time\nO⎛⎜⎝ ⎛⎝k log k ǫδ ǫ ⎞⎠ 3k−1 k2 log k ǫδ ǫ2 ⎞⎟⎠ , and returns a mixture f̂ such that D(f, f̂) ≤ 1000ǫ with error probability ≤ 2δ.\nProof. Use n′ def= 120k log 4k δ\nǫ samples to generate a set of at most n′3k−1 candidate distributions as stated in\nLemma 10. With probability ≥ 1 − δ, one of the candidate distributions is ǫ-close to the underlying one. Run MODIFIED SCHEFFE on this set of candidate distributions to obtain a 1000ǫ-close estimate of f with probability ≥ 1 − δ (Lemma 1). The run time is dominated by the run time of MODIFIED SCHEFFE which is O ( ∣F ∣T log ∣F∣δ ǫ2 ), where ∣F ∣ = n′3k−1 and T = k. The total error probability is ≤ 2δ by the union bound.\nRemark 12. The above bound matches the independent and contemporary result by [15] for k = 2. While the process of identifying the candidate means is same for both the papers, the process of identifying the variances and proof techniques are different."
    }, {
      "heading" : "5 Acknowledgements",
      "text" : "We thank Sanjoy Dasgupta, Todd Kemp, and Krishnamurthy Vishwanathan for helpful discussions."
    }, {
      "heading" : "A Useful tools",
      "text" : ""
    }, {
      "heading" : "A.1 Bounds on ℓ1 distance",
      "text" : "For two d dimensional product distributions p1 and p2, if we bound the ℓ1 distance on each coordinate by ǫ, then by triangle inequality D(p1,p2) ≤ dǫ. However this bound is often weak. One way to obtain a stronger bound is to relate ℓ1 distance to Bhattacharyya parameter, which is defined as follows: Bhattacharyya parameter B(p1, p2) between two distributions p1 and p2 is\nB(p1, p2) = ∫ x∈X\n√ p1(x)p2(x)dx.\nWe use the fact that for two product distributions p1 and p2, B(p1,p2) = ∏di=1B(p1,i, p2,i) to obtain stronger bounds on the ℓ1 distance. We first bound Bhattacharyya parameter for two one-dimensional Gaussian distributions. Lemma 13. The Bhattacharyya parameter for two one dimensional Gaussian distributions p1 = N(µ1, σ21) and p2 = N(µ2, σ22) is\nB(p1, p2) ≥ 1 − (µ1 − µ2)2) 4(σ21 + σ22) − (σ21 − σ22)2(σ21 + σ22)2 . Proof. For Gaussian distributions the Bhattacharyya parameter is (see [8]), B(p1, p2) = ye−x, where x = (µ1−µ2)2) 4(σ2\n1 +σ2 2 ) and y = √ 2σ1σ2 σ2 1 +σ2 2 . Observe that\ny =\n√ 2σ1σ2 σ21 + σ22 = ¿ÁÁÀ1 − (σ1 − σ2)2 σ21 + σ22 ≥ 1 − (σ1 − σ2)2 σ21 + σ22 ≥ 1 −\n(σ21 − σ22)2(σ21 + σ22)2 . Hence,\nB(p1, p2) = ye−x ≥ y(1 − x) ≥ (1 − x)(1 − (σ21 − σ22)2(σ21 + σ22)2) ≥ 1 − x − (σ21 − σ22)2(σ21 + σ22)2 .\nSubstituting the value of x results in the lemma.\nThe next lemma follows from the relationship between Bhattacharyya parameter and ℓ1 distance (see [34]), and the previous lemma.\nLemma 14. For any two Gaussian product distributions p1 and p2,\nD(p1,p2)2 ≤ 8( d∑ i=1 1 −B(p1,i, p2,i)) ≤ d∑ i=1 2 (µ1,i − µ2,i)2 σ21,i + σ22,i + 8 (σ21,i − σ22,i)2(σ21,i + σ22,i)2 ."
    }, {
      "heading" : "A.2 Concentration inequalities",
      "text" : "We use the following concentration inequalities for Gaussian, Chi-Square, and sum of Bernoulli random variables in the rest of the paper.\nLemma 15. For a Gaussian random variable X with mean µ and variance σ2,\nPr(∣X − µ∣ ≥ tσ) ≤ e−t2/2. Lemma 16 ( [25]). If Y1, Y2, . . . Yn be n i.i.d.Gaussian variables with mean 0 and variance σ2, then\nPr( n∑ i=1 Y 2i − nσ2 ≥ 2(√nt + t)σ2) ≤ e−t, and Pr( n∑ i=1 Y 2i − nσ2 ≤ −2√ntσ2) ≤ e−t. Furthermore for a fixed vector a,\nPr(∣ n∑ i=1 ai(Y 2i − 1)∣ ≤ 2(∣∣a∣∣2√t + ∣∣a∣∣∞ t)σ2) ≤ 2e−t. Lemma 17 (Chernoff bound). If X1,X2 . . .Xn are distributed according to Bernoulli p, then with probability 1 − δ,\n∣∑ni=1Xi n\n− p∣ ≤ √\n2p(1 − p) n log 2 δ + 2 3 log 2 δ n .\nWe now state a non-asymptotic concentration inequality for random matrices that helps us bound errors in spectral algorithms. Lemma 18 ( [43] Remark 5.51). Let y(1),y(2), . . . ,y(n) be generated according to N(0,Σ). For every ǫ ∈ (0,1) and t ≥ 1, if n ≥ c′d( t ǫ )2 for some constant c′, then with probability ≥ 1 − 2e−t2n,\n∣∣ n∑ i=1 1 n y(i)yt(i) −Σ∣∣ ≤ ǫ ∣∣Σ∣∣ ."
    }, {
      "heading" : "A.3 Matrix eigenvalues",
      "text" : "We now state few simple lemmas on the eigenvalues of perturbed matrices.\nLemma 19. Let λA1 ≥ λ A ≥ . . . λAd ≥ 0 and λ B 1 ≥ λ B ≥ . . . λBd ≥ 0 be the eigenvalues of two symmetric\nmatrices A and B respectively. If ∣∣A −B∣∣ ≤ ǫ, then ∀ i, ∣λAi − λBi ∣ ≤ ǫ. Proof. Let u1,u2, . . .ud be a set of eigenvectors of A that corresponds to λA1 , λ A 2 , . . . λ A d . Similarly let v1,v2, . . .vd be eigenvectors of B Consider the first eigenvalue of B,\nλB1 = ∣∣B∣∣ = ∣∣A + (B −A)∣∣ ≥ ∣∣A∣∣ − ∣∣B −A∣∣ ≥ λA1 − ǫ.\nNow consider an i > 1. If λBi < λ A i − ǫ, then by definition of eigenvalues\nmax v∶∀j≤i−1,v⋅vj=0 ∣∣Bv∣∣2 < λAi − ǫ. Now consider a unit vector ∑ij=1αjuj in the span of u1, . . .ui, that is orthogonal to v1, . . .vi−1. For this vector,\nRRRRRRRRRRR RRRRRRRRRRRB i ∑ j=1 αjuj RRRRRRRRRRR RRRRRRRRRRR2 ≥ RRRRRRRRRRR RRRRRRRRRRRA i ∑ j=1 αjuj RRRRRRRRRRR RRRRRRRRRRR2 − RRRRRRRRRRR RRRRRRRRRRR(A −B) i ∑ j=1 αjuj RRRRRRRRRRR RRRRRRRRRRR2 ≥ ¿ÁÁÁÀ i∑ j=1\nα2j(λAj )2 − ǫ ≥ λAi − ǫ, a contradiction. Hence, ∀i ≤ d, λBi ≥ λAi − ǫ. The proof in the other direction is similar and omitted. Lemma 20. Let A = ∑ki=1 η2i uiuti be a positive semidefinite symmetric matrix for k ≤ d. Let u1,u2, . . .uk span a k − 1 dimensional space. Let B = A + R, where ∣∣R∣∣ ≤ ǫ. Let v1,v2, . . .vk−1 be the top k − 1 eigenvectors of B. Then the projection of ui in space orthogonal to v1,v2, . . .vk−1 is ≤ 2 √ ǫ\nηi .\nProof. Let λBi be the i th largest eigenvalue of B. Observe that B + ǫId is a positive semidefinite matrix as\nfor any vector v, vt(A +R + ǫId)v ≥ 0. Furthermore ∣∣A +R + ǫId −A∣∣ ≤ 2ǫ. Since eigenvalues of B + ǫId is λB + ǫ, by Lemma 19, for all i ≤ d, ∣λAi − λBi − ǫ∣ ≤ 2ǫ. Therefore, ∣λBi ∣ for i ≥ k is ≤ 3ǫ.\nLet ui = ∑k−1j=1 αi,jvj + √\n1 −∑k−1j=1 α2i,ju′, for a vector u′ orthogonal to v1,v2, . . .vk−1. We compute u′tAu′ in two ways. Since A = B −R,\n∣u′t(B −R)u′∣ ≤ ∣u′tBu′∣ + ∣u′tRu′∣ ≤ ∣∣Bu′∣∣ 2 + ∣∣R∣∣ .\nSince u′ is orthogonal to first k eigenvectors, we have ∣∣Bu′∣∣2 ≤ 3ǫ and hence ∣u′(B −R)u′∣ ≤ 4ǫ. u′tAu′ ≥ η2i (1 − k−1∑\nj=1 α2i,j). We have shown that the above quantity is ≤ 4ǫ. Therefore (1 −∑k−1j=1 α2i,j)1/2 ≤ 2√ǫ/ηi."
    }, {
      "heading" : "B Selection from a set of candidate distributions",
      "text" : "Given samples from an unknown distribution f , the objective is to output a distribution from a known collection F of distributions with ℓ1 distance close to D(f,F). Scheffe estimate [16] outputs a distribution from F whose ℓ1 distance from f is at most 9.1max(D(f,F), ǫ) The algorithm requires O(ǫ−2 log ∣F ∣) samples and the runs in time O(∣F ∣2T (n + ∣X ∣)), where T is the time to compute the probability fj(x) of x, for any fj ∈ F . An approach to reduce the time complexity, albeit using exponential pre-processing, was proposed in [28]. We present the modified Scheffe algorithm with near linear time complexity and then prove Lemma 1.\nWe first present the algorithm SCHEFFE* with running time Õ(∣F ∣2Tn).\nAlgorithm SCHEFFE* Input: a set F of candidate distributions, ǫ ∶ upper bound on D(f,F), n independent samples x1, . . . , xn from f .\nFor each pair (p, q) in F do: 1. µf = 1 n ∑ni=1 I{p(xi) > q.(xi)}.\n2. Generate independent samples y1, . . . , yn and z1, . . . , zn from p and q respectively. 3. µp = 1 n ∑ni=1 I{p(yi) > q(yi)}, µq = 1n ∑ni=1 I{p(zi) > q(zi)}.\n4. If ∣µp − µf ∣ < ∣µq − µf ∣ declare p as winner, else q. Output the distribution with most wins, breaking ties arbitrarily.\nWe make the following modification to the algorithm where we reduce the size of potential distributions by half in every iteration.\nAlgorithm MODIFIED SCHEFFE Input: set F of candidate distributions, ǫ ∶ upper bound on minfi∈F D(f, fi), n independent samples x1, . . . , xn from f .\n1. Let G = F , C ← ∅ 2. Repeat until ∣G∣ > 1:\n(a) Randomly form ∣G∣/2 pairs of distributions in G and run SCHEFFE* on each pair using the n samples. (b) Replace G with the ∣G∣/2 winners. (c) Randomly select a set A of min{∣G∣, ∣F ∣1/3} elements from G. (d) Run SCHEFFE* on each pair in A and add the distributions with most wins to C.\n3. Run SCHEFFE* on C and output the winner\nRemark 21. For the ease of proof, we assume that δ ≥ 10 log ∣F ∣∣F ∣1/3 . If δ < 10 log ∣F ∣ ∣F ∣1/3 , we run the algorithm with\nerror probability 1/3 and repeat it O(log 1 δ ) times to choose a set of candidate mixtures Fδ . By Chernoffbound with error probability ≤ δ, Fδ contains a mixture close to f . Finally, we run SCHEFFE* on Fδ to obtain a mixture that is close to f .\nProof sketch of Lemma 1. For any set A and a distribution p, given n independent samples from p the empirical probability µn(A) has a distribution around p(A) with standard deviation ∼ 1√n . Together with an observation in Scheffe estimation in [16] one can show that if the number of samples n = O ( log ∣F∣δ ǫ2 ), then SCHEFFE* has a guarantee 10max(ǫ,D(f,F)) with probability ≥ 1 − δ.\nSince we run SCHEFFE* at most ∣F ∣(2 log ∣F ∣+ 1) times, choosing δ = δ/(4∣F ∣ log ∣F ∣+ 2∣F ∣) results in the sample complexity of\nO⎛⎝ log ∣F ∣2(4 log ∣F ∣+2) δ ǫ2 ⎞⎠ = O⎛⎝ log ∣F ∣ δ ǫ2 ⎞⎠ , and the total error probability of δ/2 for all runs of SCHEFFE* during the algorithm. The above value of n dictates our sample complexity. We now consider the following two cases:\n• If at some stage ≥ log(2/δ)∣F ∣1/3 fraction of elements in A have an ℓ1 distance ≤ 10ǫ from f , then at that stage with probability ≥ 1 − δ/2 an element with distance ≤ 10ǫ from f is added to A. Therefore a distribution with distance ≤ 100ǫ is selected to C.\n• If at no stage this happens, then consider the element that is closest to f , i.e., at ℓ1 distance at most ǫ.\nWith probability ≥ (1 − log(2/δ)∣F ∣1/3 )log ∣F ∣ it always competes with an element at a distance at least 10ǫ from f and it wins all these games with probability ≥ 1 − δ/2.\nTherefore with probability ≥ 1−δ/2 there is an element in C at ℓ1 distance at most 100ǫ. Running SCHEFFE* on this set yields a distribution at a distance ≤ 100 ⋅ 10ǫ = 1000ǫ. The error probability is ≤ δ by the union bound."
    }, {
      "heading" : "C Lower bound",
      "text" : "We first show a lower bound for a single Gaussian distribution and generalize it to mixtures."
    }, {
      "heading" : "C.1 Single Gaussian distribution",
      "text" : "The proof is an application of the following version of Fano’s inequality [9, 45]. It states that we cannot simultaneously estimate all distributions in a class using n samples if they satisfy certain conditions.\nLemma 22. (Fano’s Inequality) Let f1, . . . , fr+1 be a collection of distributions such that for any i ≠ j, D(fi, fj) ≥ α, and KL(fi, fj) ≤ β. Let f be an estimate of the underlying distribution using n i.i.d. samples from one of the fi’s. Then,\nsup i E[D(fi, f)] ≥ α 2 (1 − nβ + log 2 log r ).\nWe consider d−dimensional spherical Gaussians with identity covariance matrix, with means along any coordinate restricted to ± cǫ√\nd . The KL divergence between two spherical Gaussians with identity covariance\nmatrix is the squared distance between their means. Therefore, any two distributions we consider have KL distance at most\nβ = d\n∑ i=1 (2 cǫ√ d )2 = 4c2ǫ2,\nWe now consider a subset of these 2d distributions to obtain a lower bound on α. By the Gilbert-Varshamov bound, there exists a binary code with ≥ 2d/8 codewords of length d and minimum distance d/8. Consider one such code. Now for each codeword, map 1→ cǫ√\nd and 0→ − cǫ√ d to obtain a distribution in our class. We\nconsider this subset of ≥ 2d/8 distributions as our fi’s.\nConsider any two fi’s. Their means differ in at least d/8 coordinates. We show that the ℓ1 distance between them is ≥ cǫ/4. Without loss of generality, let the means differ in the first d/8 coordinates, and furthermore, one of the distributions has means cǫ/√d and the other has −cǫ/√d in the first d/8 coordinates. The sum of the first d/8 coordinates is N(cǫ√d/8, d/8) and N(−cǫ√d/8, d/8). The ℓ1 distance between these normal random variables is a lower bound on the ℓ1 distance of the original random variables. For small values of cǫ the distance between the two Gaussians is at least ≥ cǫ/4. This serves as our α.\nApplying the Fano’s Inequality, the ℓ1 error on the worst distribution is at least\ncǫ 8 (1 − n4c2ǫ2 + log 2 d/8 ), which for c = 16 and n < d\n214ǫ2 is at least ǫ. In other words, the smallest n to approximate all spherical\nnormal distributions to ℓ1 distance at most ǫ is > d\n214ǫ2 ."
    }, {
      "heading" : "C.2 Mixtures of k Gaussians",
      "text" : "We now provide a lower bound on the sample complexity of learning mixtures of k Gaussians in d dimensions. We extend the construction for learning a single spherical Gaussian to mixtures of k Gaussians and show a lower bound of Ω(kd/ǫ2) samples. We will again use Fano’s inequality over a class of 2kd/64 distributions as described next.\nTo prove the lower bound on the sample complexity of learning spherical Gaussians, we designed a class of 2d/8 distributions around the origin. Let P def= {P1, . . . , PT }, where T = 2d/8, be this class. Recall that each Pi is a spherical Gaussian with unit variance. For a distribution P over Rd and µ ∈ Rd, let P + µ be the distribution P shifted by µ.\nWe now choose µ1, . . . ,µk’s extremely well-separated. The class of distributions we consider will be a mixture of k components, where the jth component is a distribution from P shifted by µj . Since the µ’s will be well separated, we will use the results from last section over each component.\nFor i ∈ [T ], and j ∈ [k], Pij def= Pi +µj . Each (i1, . . . , ik) ∈ [T ]k corresponds to the mixture 1\nk (Pi11 + Pi22 + . . . + Pikk)\nof k spherical Gaussians. We consider this class of T k = 2kd/8 distributions. By the Gilbert-Varshamov bound, for any T ≥ 2, there is a T -ary codes of length k, with minimum distance ≥ k/8 and number of codewords ≥ 2k/8. This implies that among the T k = 2dk/8 distributions, there are 2kd/64 distributions such that any two tuples (i1, . . . , ik) and (i′1, . . . , i′k) corresponding to different distributions differ in at least k/8 locations.\nIf we choose the µ’s well separated, the components of any mixture distribution have very little overlap. For simplicity, we choose µj’s satisfying\nmin j1≠j2 ∣∣µj1 −µj2 ∣∣2 ≥ (2kdǫ ) 100 .\nThis implies that for j ≠ l, ∣∣Pij − Pi′l∣∣1 < (ǫ/2dk)10. Therefore, for two different mixture distributions, ∣∣1 k (Pi11 +Pi22 + . . . +Pikk) − 1k (Pi′11 + Pi′22 + . . . + Pi′kk)∣∣1\n(a) ≥ 1\nk ∑ j∈[k],ij ,i′j∈[T ] ∣Pijj − Pi′jj ∣ − k2(ǫ/2dk)10\n(b) ≥ 1\n8\ncǫ 4 − k2(ǫ/2dk)10.\nwhere (a) follows form the fact that two mixtures have overlap only in the corresponding components, (b) uses the fact that at least in k/8 components ij ≠ i′j , and then uses the lower bound from the previous section.\nTherefore, the ℓ1 distance between any two of the 2kd/64 distributions is ≥ c1ǫ/32 for c1 slightly smaller than c. We take this as α.\nNow, to upper bound the KL divergence, we simply use the convexity, namely for any distributions P1 . . . Pk and Q1 . . . Qk, let P̄ and Q̄ be the mean distributions. Then,\nD(P̄ ∣∣Q̄) ≤ 1 k k ∑ i=1 D(Pi∣∣Qi). By the construction and from the previous section, for any j, D(Pijj ∣∣Pi′jj) =D(Pi∣∣Pi′) ≤ 4c2ǫ2. Therefore, we can take β = 4c2ǫ2.\nTherefore by the Fano’s inequality, the ℓ1 error on the worst distribution is at least\nc1ǫ 64 (1 − n4c2ǫ2 + log 2 dk/64 ), which for c1 = 128, c = 128.1 and n < dk 88ǫ2 is at least ǫ."
    }, {
      "heading" : "D Proofs for k spherical Gaussians",
      "text" : "We first state a simple concentration result that helps us in other proofs.\nLemma 23. Given n samples from a set of Gaussian distributions, with probability ≥ 1 − 2δ, for every pair of samples X ∼ N(µ1, σ2Id) and Y ∼ N(µ2, σ2Id),\n∣∣X −Y∣∣22 ≤ 2dσ2 + 4σ2 √ d log n2 δ + ∣∣µ1 −µ2∣∣22 + 4σ ∣∣µ1 −µ2∣∣2 √ log n2 δ + 4σ2 log n2 δ . (1)\nand\n∣∣X −Y∣∣22 ≥ 2dσ2 − 4σ2 √ d log n2 δ + ∣∣µ1 −µ2∣∣22 − 4σ ∣∣µ1 −µ2∣∣2 √ log n2 δ . (2)\nProof. We prove the lower bound, the proof for the upper bound is similar and omitted. Since X and Y are Gaussians, X −Y is distributed as N(µ1 −µ2,2σ2). Rewriting ∣∣X −Y∣∣2 ∣∣X −Y∣∣22 = ∣∣X −Y − (µ1 −µ2)∣∣22 + ∣∣µ1 −µ2∣∣22 + 2(µ1 −µ2) ⋅ (X −Y − (µ1 −µ2)). Let Z =X −Y − (µ1 −µ2), then Z ∼ N(0,2σ2Id). Therefore by Lemma 16, with probability 1 − δ/n2,\n∣∣Z∣∣22 ≥ 2dσ2 − 4σ2 √ d log n2\nδ .\nFurthermore (µ1−µ2) ⋅Z is sum of Gaussians and hence a Gaussian distribution. It has mean 0 and variance 2σ2 ∣∣µ1 −µ2∣∣22. Therefore, by Lemma 15 with probability 1 − δ/n2,\n(µ1 −µ2) ⋅Z ≥ −2σ ∣∣µ1 −µ2∣∣2 √ log n2\nδ .\nBy the union bound with probability 1 − 2δ/n2, ∣∣X −Y∣∣22 ≥ 2dσ2 − 4σ2 √ d log n2 δ + ∣∣µ1 −µ2∣∣22 − 4σ ∣∣µ1 −µ2∣∣2 √ log n2 δ . There are (n 2 ) pairs and the lemma follows by the union bound."
    }, {
      "heading" : "D.1 Proof of Lemma 4",
      "text" : "We show that if Equations (1) and (2) are satisfied, then the lemma holds. The error probability is that of Lemma 23 and is ≤ 2δ. Since the minimum is over k + 1 indices, at least two samples are from the same component. Applying Equations (1) and (2) for these two samples\n2dσ̂2 ≤ 2dσ2 + 4σ2 √ d log n2\nδ + 4σ2 log n2 δ .\nSimilarly by Equations (1) and (2) for any two samples X(a),X(b) in [k + 1], ∣∣X(a) −X(b)∣∣22 ≥ 2dσ2 − 4σ2 √ d log n2 δ + ∣∣µi −µj ∣∣22 − 4σ ∣∣µi −µj ∣∣2 √ log n2 δ\n≥ 2dσ2 − 4σ2 √ d log n2\nδ − 4σ2 log n2 δ ,\nwhere the last inequality follows from the fact that α2−4αβ ≥ −4β2. The result follows from the assumption that d > 20 log n2/δ."
    }, {
      "heading" : "D.2 Proof of Lemma 5",
      "text" : "We show that if Equations (1) and (2) are satisfied, then the lemma holds. The error probability is that of Lemma 23 and is ≤ 2δ. Since Equations (1) and (2) are satisfied, by the proof of Lemma 4, ∣σ̂2 − σ2∣ ≤ 2.5σ2 √ log(n2/δ) d . If two samples X(a) and X(b) are from the same component, by Lemma 23,\n∣∣X(a) −X(b)∣∣22 ≤ 2dσ2 + 4σ2 √ d log n2 δ + 4σ2 log n2 δ ≤ 2dσ2 + 5σ2 √ d log n2 δ .\nBy Lemma 4, the above quantity is less than 2dσ̂2 + 23σ̂2√d log n2 δ\n. Hence all the samples from the same component are in a single cluster.\nSuppose there are two samples from different components in a cluster, then by Equations (1) and (2),\n2dσ̂2 + 23σ̂2 √ d log n2\nδ ≥ 2dσ2 − 4σ2\n√ d log n2\nδ + ∣∣µi −µj ∣∣22 − 4σ ∣∣µi −µj ∣∣2\n√ log n2\nδ .\nRelating σ̂2 and σ2 using Lemma 4,\n2dσ2 + 40σ2 √ d log n2\nδ ≥ 2dσ2 − 4σ2\n√ d log n2\nδ + ∣∣µi −µj ∣∣22 − 4σ ∣∣µi −µj ∣∣2\n√ log n2\nδ .\nHence ∣∣µi −µj ∣∣2 ≤ 10σ(d log n2δ )1/4. There are at most k components; therefore, any two components within the same cluster are at a distance ≤ 10kσ(d log n2 δ )1/4."
    }, {
      "heading" : "D.3 Proof of Lemma 6",
      "text" : "The proof is involved and we show it in steps. We first show few concentration bounds which we use later to argue that the samples are clusterable when the sample covariance matrix has a large eigenvalue. Let ŵi be the fraction of samples from component i. Let µ̂i be the empirical average of samples from pi. Let µ̂(C) be the empirical average of samples in cluster C . If C is the entire set of samples we use µ̂ instead of µ̂(C). We first show a concentration inequality that we use in rest of the calculations. Lemma 24. Given n samples from a k-component Gaussian mixture with probability ≥ 1 − 2δ, for every component i\n∣∣µ̂i −µi∣∣22 ≤ (d + 3 √ d log 2k\nδ ) σ2 nŵi\nand ∣ŵi −wi∣ ≤ ¿ÁÁÀ2wi log 2kδ\nn + 2 3\nlog 2k δ\nn . (3)\nProof. Since µ̂i −µi is distributed N(0, σ2Id/nŵi), by Lemma 16 with probability ≥ 1 − δ/k, ∣∣µ̂i −µi∣∣22 ≤ (d + 2 √ d log 2k\nδ + 2 log 2k δ ) σ2 nŵi\n≤ (d + 3 √ d log 2k\nδ ) σ2 nŵi .\nThe second inequality uses the fact that d ≥ 20 log n2/δ. For bounding the weights, observe that by Lemma 17 with probability ≥ 1 − δ/k,\n∣ŵi −wi∣ ≤ √ 2wi log 2k/δ n + 2 3 log 2k/δ n .\nBy the union bound the error probability is ≤ 2kδ/2k = δ. A simple application of triangle inequality yields the following lemma.\nLemma 25. Given n samples from a k-component Gaussian mixture if Equation (3) holds, then\n∣∣ k∑ i=1\nŵi(µ̂i −µi)(µ̂i −µi)t∣∣ ≤ (d + 3 √ d log 2k\nδ )kσ2 n .\nLemma 26. Given n samples from a k-component Gaussian mixture, if Equation (3) holds and the maximum distance between two components is ≤ 10kσ(d log n2 δ )1/4, then ∣∣µ̂ −µ)∣∣ 2 ≤ cσ\n√ dk log n 2\nδ n , for a constant c.\nProof. Observe that\nµ̂ −µ = k∑ i=1 ŵiµ̂i −wiµi = k ∑ i=1 ŵi(µ̂i −µi) + (ŵi −wi)µi = k∑ i=1 ŵi(µ̂i −µi) + (ŵi −wi)(µi −µ). (4) Hence by Equation (3) and the fact that the maximum distance between two components is ≤ 10kσ(d log n2 δ )1/4,\n∣∣µ̂ −µ∣∣ 2 ≤ k\n∑ i=1 ŵi\n¿ÁÁÀ(d + 3 √ d log 2k\nδ ) σ√ nŵi + (\n√ 2wi log 2k/δ\nn + 2 3 log 2k/δ n )10k(d log n2 δ )1/4σ.\nFor n ≥ d ≥max(k4,20 log n2/δ,1000), we get the above term is ≤ c√kd logn2/δ n σ, for some constant c.\nWe now make a simple observation on covariance matrices.\nLemma 27. Given n samples from a k-component mixture,\n∣∣ k∑ i=1 ŵi(µ̂i − µ̂)(µ̂i − µ̂)t − k∑ i=1 ŵi(µi −µ)(µi −µ)t∣∣ ≤ 2 ∣∣µ̂ −µ∣∣2\n2 + k∑\ni=1 2ŵi ∣∣µ̂i −µi∣∣22 + 2(√k ∣∣µ̂ −µ∣∣2 + k∑ i=1 √ ŵi ∣∣µ̂i −µi∣∣2)max j √ ŵj ∣∣µj −µ∣∣2 .\nProof. Observe that for any two vectors u and v, uut − vvt = u(ut − vt) + (u − v)vt = (u − v)(u − v)t + v(u − v)t + (u − v)vt. Hence by triangle inequality, ∣∣uut − vvt∣∣ ≤ ∣∣u − v∣∣22 + 2 ∣∣v∣∣2 ∣∣u − v∣∣2 . Applying the above observation to u = µ̂i − µ̂ and v = µi −µ, we get\nk\n∑ i=1 ŵi ∣∣(µ̂i − µ̂)(µ̂i − µ̂)t − (µi −µ)(µi −µ)t∣∣ ≤ k\n∑ i=1 (ŵi ∣∣µ̂i − µ̂ −µi −µ∣∣22 + 2√ŵi ∣∣µi −µ∣∣2√ŵi ∣∣µ̂i − µ̂ −µi −µ∣∣2) ≤ k\n∑ i=1 (2ŵi ∣∣µ̂i −µi∣∣22 + 2ŵi ∣∣µ̂ −µ∣∣22 + 2maxj √ŵj ∣∣µj −µ∣∣2 ( √ ŵi ∣∣µ̂i −µi∣∣2 +√ŵi ∣∣µ̂ −µ∣∣2))\n≤ 2 ∣∣µ̂ −µ∣∣2 2 + k∑\ni=1 2ŵi ∣∣µ̂i −µi∣∣22 + 2(√k ∣∣µ̂ −µ∣∣2 + k∑ i=1 √ ŵi ∣∣µ̂i −µi∣∣2)max j √ ŵj ∣∣µj −µ∣∣2 .\nThe lemma follows from triangle inequality.\nThe following lemma immediately follows from Lemmas 26 and 27.\nLemma 28. Given n samples from a k-component Gaussian mixture, if Equation (3) and the maximum distance between two components is ≤ 10kσ(d log n2 δ )1/4, then\n∣∣ k∑ i=1 ŵi(µ̂i − µ̂)(µ̂i − µ̂)t − k∑ i=1 ŵi(µi −µ)(µi −µ)t∣∣ ≤ cσ2dk2 log n 2 δ n + cσ ¿ÁÁÀdk2 log n2δ n max i √ ŵi ∣∣µi −µ∣∣2 ,\nfor a constant c. Lemma 29. For a set of samples X(1), . . .X(n) from a k-component mixture, n ∑ i=1 (X(i) − µ̂)(X(i) − µ̂)t n = k ∑ i=1 ŵi(µ̂i − µ̂)(µ̂i − µ̂)t − ŵi(µ̂i −µi)(µ̂i −µi)t + ∑ j∣X(j)∼pi (X(j) −µi)(X(j) −µi)t n .\nwhere ŵi and µ̂i are the empirical weights and averages of components i and µ̂ = 1 n ∑ni=1Xi.\nProof. The given expression can be rewritten as\n1\nn\nn\n∑ i=1 (X(i) − µ̂)(X(i) − µ̂)t = k∑ i=1 ŵi ∑ j∣X(j)∼pi 1 nŵi X(j) − µ̂)(X(j) − µ̂)t.\nFirst observe that for any set of points xi and their average x̂ and any value a,\n∑ i (xi − a)2 =∑ i (xi − x̂)2 + (x̂ − a)2. Hence for samples from a component i,\n∑ j∣X(j)∼pi\n1 nŵi (X(j) − µ̂)(X(j) − µ̂)t\n= ∑ j∣X(j)∼pi\n1 nŵi (µ̂i − µ̂)(µ̂i − µ̂)t + ∑\nj∣X(j)∼pi\n1 nŵi (X(j) − µ̂i)(X(j) − µ̂i)t\n= (µ̂i − µ̂)(µ̂i − µ̂)t + ∑ j∣X(j)∼pi 1 nŵi (X(j) − µ̂i)(X(j) − µ̂i)t = (µ̂i − µ̂)(µ̂i − µ̂)t + ∑ j∣X(j)∼pi 1 nŵi (X(j) −µi)(X(j) −µi)t − (µ̂i −µi)(µ̂i −µi)t.\nSumming over all components results in the lemma.\nWe now bound the error in estimating the eigenvalue of the covariance matrix.\nLemma 30. Given X(1), . . .X(n), n samples from a k-component Gaussian mixture, if Equations (1), (2), and (3) hold, then with probability ≥ 1 − 2δ,\n∣∣ 1 n n ∑ i=1 (X(i) − µ̂)(X(i) − µ̂)t − σ̂2Id − k∑ i=1 ŵi(µi −µ)(µi −µ)t∣∣ ≤ c(n) def= cσ2 ¿ÁÁÀd log n2δ n + cσ2 dk2 log n 2 δ n + cσ ¿ÁÁÀdk2 log n2δ n max i √ ŵi ∣∣µi −µ∣∣2 ,\n(5)\nfor a constant c.\nProof. Since Equations (1), (2), and (3) hold, conditions in Lemmas 26 and 28 are satisfied. By Lemma 28,\n∣∣ k∑ i=1 ŵi(µ̂i − µ̂)(µ̂i − µ̂)t − k∑ i=1 ŵi(µi −µ)(µi −µ)t∣∣ = O ⎛⎜⎜⎝σ 2 dk2 log n 2 δ n + σ ¿ÁÁÀdk2 log n2δ n max i √ ŵi ∣∣µi −µ∣∣2 ⎞⎟⎟⎠ . Hence it remains to show,\n∣∣ 1 n n ∑ i=1 (X(i) − µ̂)(X(i) − µ̂)t − k∑ i=1 ŵi(µ̂i − µ̂)(µ̂i − µ̂)t∣∣ = O ⎛⎜⎜⎝ ¿ÁÁÀkd log 5k2δ n σ2 ⎞⎟⎟⎠ . By Lemma 29, the covariance matrix can be rewritten as\nk\n∑ i=1 ŵi(µ̂i − µ̂)(µ̂i − µ̂)t − ŵi(µ̂i −µi)(µ̂i −µi)t + k∑ i=1 ∑ j∣X(j)∼pi 1 n (X(j) −µi)(X(j) −µi)t − σ̂2Id. (6)\nWe now bound the norms of second and third terms in the above equation. Consider the third term, ∑ki=1∑j∣X(j)∼pi 1n(X(j) − µi)(X(j) − µi)t. Conditioned on the fact that X(j) ∼ pi, X(j) − µi is distributed N(0, σ2Id), therefore by Lemma 18 and Lemma 4 ,with probability ≥ 1 − 2δ,\nRRRRRRRRRRRR RRRRRRRRRRRR k ∑ i=1 ∑ j∣X(j)∼pi 1 n (X(j) −µi)(X(j) −µi)t − σ̂2Id RRRRRRRRRRRR RRRRRRRRRRRR ≤ c ′ ¿ÁÁÀd log 2dδ n σ2 + 2.5σ2 ¿ÁÁÀ log n2δ d .\nThe second term in Equation (6) is bounded by Lemma 25. Hence together with the fact that d ≥ 20 log n2/δ we get that with probability ≥ 1 − 2δ, the second and third terms are bounded by O (σ2√dk\nn log n\n2 δ ) .\nLemma 31. Let u be the largest eigenvector of the sample covariance matrix and n ≥ c ⋅ dk2 log n2 δ\n. If maxi √ ŵi ∣∣µi −µ∣∣2 = ασ and Equation (5) holds, then there exists i such that ∣u ⋅ (µi − µ)∣ ≥ σ(α − 1 −\n1/α)/√k. Proof. Observe that ∣∣∑j wjvjvtj ∣∣ ≥ ∣∣∑j wjvjvtj vi∣∣vi∣∣ ∣∣2 ≥ wi ∣∣vi∣∣22. Therefore\n∣∣ k∑ i=1 ŵi(µi −µ)(µi −µ)t∣∣ ≥ RRRRRRRRRRR RRRRRRRRRRR k ∑ j=1 ŵj(µj −µ)(µj −µ)t(µi −µ)/ ∣∣µi −µ∣∣RRRRRRRRRRR RRRRRRRRRRR2 ≥ α 2σ2.\nHence by Lemma 30 and the triangle inequality, the largest eigenvalue of the sample-covariance matrix is ≥ α2σ2−c(n). Similarly by applying Lemma 30 again we get,∣∣∑ki=1 ŵi(µi −µ)(µi −µ)tu∣∣2 ≥ α2σ2−2c(n). By triangle inequality and Cauchy-Schwartz inequality,\n∣∣ k∑ i=1 ŵi(µi −µ)(µi −µ)tu∣∣ 2 ≤ k ∑ i=1 ∣∣ŵi(µi −µ)(µi −µ)tu∣∣2 ≤ k\n∑ i=1 ŵi ∣∣(µi −µ)∣∣2max j ∣(µj −µ) ⋅ u∣ ≤\n¿ÁÁÀ k∑ i=1 ŵi ∣∣(µi −µ)∣∣22max j\n∣(µj −µ) ⋅ u∣ ≤ √ kασmax\nj ∣(µj −µ) ⋅ u∣.\nHence √ kασmaxi ∣(µi − µ) ⋅ u∣ ≥ α2σ2 − 2c(n). The lemma follows by substituting the bound on n in\nc(n). We now make a simple observation on Gaussian mixtures.\nFact 32. The samples from a subset of components A of the Gaussian mixture are distributed according to a Gaussian mixture of components A with weights being w′i = wi/(∑j∈Awj).\nWe now prove Lemma 6.\nProof of Lemma 6. Observe that we run the recursive clustering at most n times. At every step, the underlying distribution within a cluster is a Gaussian mixture. Let Equations (1), (2) hold with probability 1− 2δ. Let Equations (3) (5) all hold with probability ≥ 1 − δ′, where δ′ = δ/2n at each of n steps. By the union bound the total error is ≤ 2δ + δ′ ⋅ 2n ≤ 3δ. Since Equations (1), (2) holds, the conditions of Lemmas 4 and 5 hold. Furthermore it can be shown that discarding at most nǫ/4k samples at each step does not affect the calculations.\nWe first show that if √ wi ∣∣µi −µ(C)∣∣2 ≥ 25√k3 log(n3/δ)σ, then the algorithm gets into the loop.\nLet w′i be the weight of the component within the cluster and n ′ ≥ nǫ/5k be the number of samples in\nthe cluster. Let α = 25 √ k3 log(n3/δ). By Fact 32, the components in cluster C have weight w′i ≥ wi.\nHence √ w′i ∣∣µi −µ(C)∣∣2 ≥ ασ. Since √w′i ∣∣µi −µ(C)∣∣2 ≥ ασ, and by Lemma 5 ∣∣µi −µ(C)∣∣ ≤\n10kσ(d log n2/δ)1/4, we have w′i ≥ α2/(100k2√d log n2/δ). Hence by lemma 24, w′i ≥ wi/2 and √ŵ′i ∣∣µi −µ(C)∣∣2 ≥ ασ/√2. Hence by Lemma 30 and triangle inequality the largest eigenvalue of S(C) is ≥ α2σ2/2 − c(n′) ≥ α2σ2/4 ≥ α2σ̂2/8 ≥ 12σ̂2k3 logn2/δ′ = 12σ̂2k3 logn3/δ. Therefore the algorithm gets into the loop.\nIf n′ ≥ nǫ/8k2 ≥ c⋅dk2 log n3 δ , then by Lemma 31, there exists a component i such that ∣u⋅(µi−µ(C))∣ ≥ σ(α/√2 − 1 −√2/α)/√k, where u is the top eigenvector of the first nǫ/4k2 samples.\nObserve that ∑i∈C wiu ⋅ (µi −µ(C)) = 0 and maxi ∣u ⋅ (µi −µ(C))∣ ≥ σ(α/√2 − 1 −√2/α)/√k. Let µi be sorted according to their values of u ⋅ (µi −µ(C)), then\nmax i ∣u ⋅ (µi −µi+1)∣ ≥ σα/ √ 2 − 1 −√2/α k3/2 ≥ 12σ √ log n3 δ ≥ 9σ̂ √ log n3 δ ,\nwhere the last inequality follows from Lemma 4 and the fact that d ≥ 20 log n2/δ. For a sample from component pi, similar to the proof of Lemma 5, by Lemma 15, with probability ≥ 1 − δ/n2k, ∣∣u ⋅ (X(i) −µi)∣∣ ≤ σ√2 log(n2k/δ)2 ≤ 2σ̂√log(n2k/δ), where the second inequality follows from Lemma 4. Since there are two components that are far apart by\n≥ 9σ̂ √ log n 2\nδ σ̂ and the maximum distance between a sample and its mean is ≤ 2σ̂\n√ log(n2k/δ) and the\nalgorithm divides into at-least two non-empty clusters such that no two samples from the same distribution are clustered into two clusters.\nFor the second part observe that by the above concentration on u, no two samples from the same component are clustered differently irrespective of the mean separation. Note that we are using the fact that each sample is clustered at most 2k times to get the bound on the error probability. The total error probability by the union bound is ≤ 4δ."
    }, {
      "heading" : "D.4 Proof of Lemma 7",
      "text" : "We show that if the conclusions in Lemmas 6 and 24 holds, then the lemma is satisfied. We also assume that the conclusions in Lemma 30 holds for all the clusters with error probability δ′ = δ/k. By the union bound the total error probability is ≤ 7δ.\nBy Lemma 6 all the components within each cluster satisfy √ wi ∣∣µi −µ(C)∣∣2 ≤ 25σ√k3 log(n3/δ). Let n ≥ c ⋅dk9ǫ−4 log2 d/δ. For notational convenience let S(C) = 1∣C∣ ∑∣C∣i=1(X(i)−µ(C))(X(i)−µ(C))t− σ̂2Id. Therefore by Lemma 30 for large enough c,\n∣∣S(C) − n∣C ∣ ∑i∈C ŵi(µi −µ(C))(µi −µ(C))t∣∣ ≤ ǫ2σ2 1000k2 n∣C ∣ .\nLet v1,v2, . . .vk−1 be the top eigenvectors of 1 ∣C∣ ∑i∈C wi(µi−µ(C))(µi−µ(C))t. Let ηi =√ŵ′i ∣∣µi −µ(C)∣∣2 =√\nŵi √\nn ∣C∣ ∣∣µi −µ(C)∣∣2. Let ∆i = µi−µ(C))∣∣(µi−µ(C))∣∣2 . Therefore,\n∑ i∈C n∣C ∣ ∑i∈C ŵi(µi −µ(C))(µi −µ(C))t = ∑i∈C η2i∆i∆ti. Hence by Lemma 20, the projection of ∆i on the space orthogonal to top k − 1 eigenvectors of S(C) is\n≤ ¿ÁÁÀ ǫ2σ2 1000k2 n∣C ∣ 1ηi ≤ ǫσ 16 √ ŵi ∣∣µi −µ(C)∣∣2 k ≤\nǫσ 8 √ 2 √ wi ∣∣µi −µ(C)∣∣2 k .\nThe last inequality follows from the bound on ŵi in Lemma 24."
    }, {
      "heading" : "D.5 Proof of Theorem 8",
      "text" : "We show that the theorem holds if the conclusions in Lemmas 7 and 26 holds with error probability δ′ = δ/k. Since in the proof of Lemma 7, the probability that Lemma 6 holds is included, Lemma 6 also holds with the same probability. Since there are at most k clusters, by the union bound the total error probability is ≤ 9δ.\nFor every component i, we show that there is a choice of mean vector and weight in the search step such that wiD(pi, p̂i) ≤ ǫ/2k and ∣wi − ŵi∣ ≤ ǫ/4k. That would imply that there is a f̂ during the search such that\nD(f , f̂) ≤∑ C ∑ i∈C wiD(pi, p̂i) + 2 k−1∑ i=1 ∣wi − ŵi∣ ≤ ǫ 2k + ǫ 2k = ǫ.\nSince the weights are gridded by ǫ/4k, there exists a ŵi such that ∣wi − ŵi∣ ≤ ǫ/4k. We now show that there exists a choice of mean vector such that wiD(pi, p̂i) ≤ ǫ/2k. Note that if a component has weight ≤ ǫ/4k, the above inequality follows immediately. Therefore we only look at those components with wi ≥ ǫ/4k, by Lemma 24, for such components ŵi ≥ ǫ/5k and therefore we only look at clusters such that ∣C ∣ ≥ nǫ/5k. By Lemmas 14 and for any i,\nD(pi, p̂i)2 ≤ 2 d∑ j=1 (µi,j − µ̂i,j)2 σ2 + 8d(σ2 − σ̂2)2 σ4 .\nNote that since we are discarding at most nǫ/8k2 random samples at each step. A total number of ≤ nǫ/8k random samples are discarded. It can be shown that this does not affect our calculations and we ignore it in\nthis proof. By Lemma 4, the first estimate of σ2 satisfies ∣σ̂2−σ2∣ ≤ 2.5σ2√logn2/δ. Hence while searching over values of σ̂2, there exist one such that ∣σ′2 − σ2∣ ≤ ǫσ2/√64dk2. Hence,\nD(pi, p̂i)2 ≤ 2 ∣∣µi − µ̂i∣∣22 σ2 + ǫ2 8k2 .\nTherefore if we show that there is a mean vector µ̂i during the search such that ∣∣µi − µ̂i∣∣2 ≤ ǫσ/√16k2ŵi, that would prove the Lemma. By triangle inequality, ∣∣µi − µ̂i∣∣2 ≤ ∣∣µ(C) − µ̂(C)∣∣2 + ∣∣µi −µ(C) − (µ̂i − µ̂(C))∣∣2 . By Lemma 26 for large enough n,\n∣∣µ(C) − µ̂(C)∣∣ 2 ≤ cσ ¿ÁÁÀdk log2 n2/δ∣C ∣ ≤ ǫσ8k√wi . The second inequality follows from the bound on n and the fact that ∣C ∣ ≥ nŵi. Since wi ≥ ǫ/4k, by Lemma 24, ŵi ≥ wi/2, we have ∣∣µi − µ̂i∣∣2 ≤ ∣∣µi −µ(C) − (µ̂i − µ̂(C))∣∣2 + ǫσ8k√wi . Let u1 . . .uk−1 are the top eigenvectors the sample covariance matrix of cluster C . We now prove that during the search, there is a vector of the form ∑k−1j=1 gjǫgσ̂uj such that ∣∣µi −µ(C) −∑k−1j=1 gjǫgσ̂uj ∣∣2 ≤ ǫσ8k√wi , during the search, thus proving the lemma. Let ηi = √ wi ∣∣µi −µ(C)∣∣2. By Lemma 7, there are set of coefficients αi such that µi −µ(C)∣∣µi −µ(C)∣∣2 = k−1 ∑ j=1 αjuj + √\n1 − ∣∣α∣∣2u′, where u′ is perpendicular to u1 . . .uk−1 and √ 1 − ∣∣α∣∣2 ≤ ǫσ/(8√2ηik). Hence, we have\nµi −µ(C) = k−1∑ j=1 ∣∣µi −µ(C)∣∣2 αjuj + ∣∣µi −µ(C)∣∣2√1 − ∣∣α∣∣22u′, Since wi ≥ ǫ/4k and by Lemma 6, ηi ≤ 25√k3σ log(n3/δ), and ∣∣µi −µ(C)∣∣2 ≤ 100√k4ǫ−1σ log(n3/δ). Therefore ∃gj such that ∣gj σ̂ −αj ∣ ≤ ǫgσ̂ on each eigenvector. Hence,\nwi ∣∣µi −µ(C) − k−1∑ i=1 gjǫgσ̂uj ∣∣2 2 ≤ wikǫ 2 gσ̂ 2 +wi ∣∣µi −µ(C)∣∣22 (1 − ∣∣α∣∣2) ≤ kǫ2gσ̂ 2 + η2i ǫ 2σ2\n128η2i k 2\n≤ ǫ2σ2 128k2 + ǫ2σ2 128k2 ≤ ǫ2σ2 64k2 .\nThe last inequality follows by Lemma 4 and the fact that ǫg ≤ ǫ/16k3/2, and hence the theorem. The run time can be easily computed by retracing the steps of the algorithm and using an efficient implementation of single-linkage."
    } ],
    "references" : [ {
      "title" : "Optimal probability estimation with applications to prediction and classification",
      "author" : [ "Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh" ],
      "venue" : "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "On spectral learning of mixtures of distributions",
      "author" : [ "Dimitris Achlioptas", "Frank McSherry" ],
      "venue" : "In Proceedings of the 18th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Strong converse for identification via quantum channels",
      "author" : [ "Rudolf Ahlswede", "Andreas Winter" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures",
      "author" : [ "Joseph Anderson", "Mikhail Belkin", "Navin Goyal", "Luis Rademacher", "James R. Voss" ],
      "venue" : "CoRR, abs/1311.2891,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Minimax theory for high-dimensional gaussian mixtures with sparse mean separation",
      "author" : [ "Martin Azizyan", "Aarti Singh", "Larry A. Wasserman" ],
      "venue" : "CoRR, abs/1306.2035,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "Mikhail Belkin", "Kaushik Sinha" ],
      "venue" : "In Proceedings of the 51st Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Learning mixtures of gaussians using the k-means algorithm",
      "author" : [ "Kamalika Chaudhuri", "Sanjoy Dasgupta", "Andrea Vattani" ],
      "venue" : "CoRR, abs/0912.0086,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Image segmentation by clustering",
      "author" : [ "G.B. Coleman", "Harry C. Andrews" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1979
    }, {
      "title" : "Elements of information theory (2",
      "author" : [ "Thomas M. Cover", "Joy A. Thomas" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Learning mixtures of gaussians",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "In Proceedings of the 40th Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "A two-round variant of EM for gaussian mixtures",
      "author" : [ "Sanjoy Dasgupta", "Leonard J. Schulman" ],
      "venue" : "In Proceedings of the 16th Annual Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2000
    }, {
      "title" : "A probabilistic analysis of EM for mixtures of separated, spherical gaussians",
      "author" : [ "Sanjoy Dasgupta", "Leonard J. Schulman" ],
      "venue" : "Journal on Machine Learning Research (JMLR),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Learning k-modal distributions via testing",
      "author" : [ "Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio" ],
      "venue" : "In SODA,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Learning poisson binomial distributions",
      "author" : [ "Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio" ],
      "venue" : "In Proceedings of the 44th Annual Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Faster and sample near-optimal algorithms for proper learning mixtures of gaussians",
      "author" : [ "Constantinos Daskalakis", "Gautam Kamath" ],
      "venue" : "CoRR, abs/1312.1054,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Combinatorial methods in density estimation",
      "author" : [ "Luc Devroye", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "Iterative clustering of high dimensional text data augmented by local search",
      "author" : [ "Inderjit S. Dhillon", "Yuqiang Guan", "Jacob Kogan" ],
      "venue" : "In Proceedings of the 2nd Industrial Conference on Data Mining (ICDM),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2002
    }, {
      "title" : "Learning mixtures of product distributions over discrete domains",
      "author" : [ "Jon Feldman", "Ryan O’Donnell", "Rocco A. Servedio" ],
      "venue" : "In Proceedings of the 46th Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "PAC learning axis-aligned mixtures of gaussians with no separation assumption",
      "author" : [ "Jon Feldman", "Rocco A. Servedio", "Ryan O’Donnell" ],
      "venue" : "In Proceedings of the 19th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Estimating a mixture of two product distributions",
      "author" : [ "Yoav Freund", "Yishay Mansour" ],
      "venue" : "In Proceedings of the 13th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
      "author" : [ "Daniel Hsu", "Sham M. Kakade" ],
      "venue" : "In Proceedings of the 4th Innovations in Theoretical Computer Science Conference (ITCS),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Efficiently learning mixtures of two gaussians",
      "author" : [ "Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "In Proceedings of the 42nd Annual Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "The spectral method for general mixture models",
      "author" : [ "Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "On the learnability of discrete distributions",
      "author" : [ "Michael J. Kearns", "Yishay Mansour", "Dana Ron", "Ronitt Rubinfeld", "Robert E. Schapire", "Linda Sellie" ],
      "venue" : "In Proceedings of the 26th Annual Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1994
    }, {
      "title" : "Adaptive estimation of a quadratic functional by model selection",
      "author" : [ "B. Laurent", "Pascal Massart" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2000
    }, {
      "title" : "Mixture Models: Theory, Geometry and Applications. NSF-CBMS Conference series in Probability and Statistics, Penn",
      "author" : [ "Bruce G. Lindsay" ],
      "venue" : "State University,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1995
    }, {
      "title" : "Asymptotic convergence rate of the em algorithm for gaussian mixtures",
      "author" : [ "Jinwen Ma", "Lei Xu", "Michael I. Jordan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2001
    }, {
      "title" : "Settling the polynomial learnability of mixtures of gaussians",
      "author" : [ "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "In Proceedings of the 51st Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "Efficient density estimation via piecewise polynomial approximation",
      "author" : [ "Siu on Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun" ],
      "venue" : "CoRR, abs/1305.3207,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    }, {
      "title" : "Learning mixtures of structured distributions over discrete domains",
      "author" : [ "Siu on Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun" ],
      "venue" : "In Proceedings of the 24th Annual Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "On modeling profiles instead of values",
      "author" : [ "Alon Orlitsky", "Narayana P. Santhanam", "Krishnamurthy Viswanathan", "Junan Zhang" ],
      "venue" : "In Proceedings of the 20th Annual Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2004
    }, {
      "title" : "Variational minimax estimation of discrete distributions under kl loss",
      "author" : [ "Liam Paninski" ],
      "venue" : "In Proceedings of the 18th Annual Conference on Neural Information Processing (NIPS),",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2004
    }, {
      "title" : "Mixture densities, maximum likelihood and the em algorithm",
      "author" : [ "Richard A. Redner", "Homer F. Walker" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1984
    }, {
      "title" : "Robust text-independent speaker identification using gaussian mixture speaker models",
      "author" : [ "Douglas A. Reynolds", "Richard C. Rose" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1995
    }, {
      "title" : "Slink: An optimally efficient algorithm for the single-link cluster method",
      "author" : [ "Robin Sibson" ],
      "venue" : "The Computer Journal,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1973
    }, {
      "title" : "Statistical analysis of finite mixture distributions, volume 7",
      "author" : [ "D Michael Titterington", "Adrian FM Smith", "Udi E Makov" ],
      "venue" : null,
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1985
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2012
    }, {
      "title" : "Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts",
      "author" : [ "G. Valiant", "P. Valiant" ],
      "venue" : "Proceedings of the 43rd Annual Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2011
    }, {
      "title" : "Estimating the unseen: A sublinear-sample canonical estimator of distributions",
      "author" : [ "Gregory Valiant", "Paul Valiant" ],
      "venue" : "Electronic Colloquium on Computational Complexity (ECCC),",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2010
    }, {
      "title" : "A spectral algorithm for learning mixtures of distributions",
      "author" : [ "Santosh Vempala", "Grant Wang" ],
      "venue" : "In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2002
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "Roman Vershynin" ],
      "venue" : "CoRR, abs/1011.3027,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2010
    }, {
      "title" : "Feature selection for high-dimensional genomic microarray data",
      "author" : [ "Eric P. Xing", "Michael I. Jordan", "Richard M. Karp" ],
      "venue" : "In Proceedings of the 18th Annual International Conference on Machine Learning (ICML),",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Some applications, such as topic modeling and genomic analysis consider data in over 1000 dimensions, [17, 44].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 41,
      "context" : "Some applications, such as topic modeling and genomic analysis consider data in over 1000 dimensions, [17, 44].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 25,
      "context" : "In such cases the overall data follow a mixture distribution [26, 36, 38].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 33,
      "context" : "In such cases the overall data follow a mixture distribution [26, 36, 38].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 35,
      "context" : "In such cases the overall data follow a mixture distribution [26, 36, 38].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : "Initially, heuristic methods such as expectation-maximization (EM) were developed [27, 35].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 32,
      "context" : "Initially, heuristic methods such as expectation-maximization (EM) were developed [27, 35].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 39,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 9,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 27,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 22,
      "context" : "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 21,
      "context" : "Recently, [22, 29] showed that any d-dimensional Gaussian mixture can be recovered in polynomial time.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 27,
      "context" : "Recently, [22, 29] showed that any d-dimensional Gaussian mixture can be recovered in polynomial time.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 23,
      "context" : "PAC learning [24] does not approximate each mixture component, but instead derives a mixture distribution that is close to the original one.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 39,
      "context" : "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].",
      "startOffset" : 175,
      "endOffset" : 190
    }, {
      "referenceID" : 10,
      "context" : "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].",
      "startOffset" : 175,
      "endOffset" : 190
    }, {
      "referenceID" : 11,
      "context" : "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].",
      "startOffset" : 175,
      "endOffset" : 190
    }, {
      "referenceID" : 39,
      "context" : "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].",
      "startOffset" : 175,
      "endOffset" : 190
    }, {
      "referenceID" : 30,
      "context" : ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using Θ(s/ log s) samples.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 37,
      "context" : ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using Θ(s/ log s) samples.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 38,
      "context" : ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using Θ(s/ log s) samples.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : ",s} requires Θ(m log(s/m)/ǫ3) samples [14].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 29,
      "context" : ",s} can be learned with O(k/ǫ4), O(k log(s/ǫ)/ǫ4), and O(k log(s)/ǫ4) samples, respectively, and these bounds are tight up to a factor of ǫ [31].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "For example, for learning spherical Gaussian mixtures, the number of samples required by previous algorithms is O(d12) for k = 2 components, and increased exponentially with k [19].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 19,
      "context" : "[20] considered mixtures of two d-dimensional Bernoulli products where all probabilities are bounded away from 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] eliminated the probability constraints and generalized the results from binary to arbitrary discrete alphabets, and from 2 to k mixture components.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] generalized these results to Gaussian products, showing in particular that mixtures of k Gaussians, where the difference between the means normalized by the ratio of standard deviations is bounded by B, are PAC learnable in Õ((dB/ǫ)2k(k+1)) time, and can be shown to use Õ((dB/ǫ)4(k+1)) samples.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Observe that recent algorithms typically construct the covariance matrix [19,42], hence require ≥ nd time.",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 39,
      "context" : "Observe that recent algorithms typically construct the covariance matrix [19,42], hence require ≥ nd time.",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "2 One-dimensional Gaussian mixtures Independently and around the same time as this work [15] showed that mixtures of two one-dimensional Gaussians can be learnt with Õ(ǫ−2) samples and in time O(ǫ−7.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "4 The approach and technical contributions The popular SCHEFFE estimator takes a collection F of distributions and uses O(log ∣F∣) independent samples from an underlying distribution f to find a distribution in F whose distance from f is at most a constant factor larger than that of the distribution in F that is closet to f [16].",
      "startOffset" : 326,
      "endOffset" : 330
    }, {
      "referenceID" : 17,
      "context" : "[18, 19] constructs the sample correlation matrix and uses k of its columns to approximate the span of mean vectors.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 18,
      "context" : "[18, 19] constructs the sample correlation matrix and uses k of its columns to approximate the span of mean vectors.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 2,
      "context" : "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in Õ(d) samples.",
      "startOffset" : 60,
      "endOffset" : 71
    }, {
      "referenceID" : 36,
      "context" : "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in Õ(d) samples.",
      "startOffset" : 60,
      "endOffset" : 71
    }, {
      "referenceID" : 40,
      "context" : "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in Õ(d) samples.",
      "startOffset" : 60,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].",
      "startOffset" : 241,
      "endOffset" : 251
    }, {
      "referenceID" : 17,
      "context" : "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].",
      "startOffset" : 241,
      "endOffset" : 251
    }, {
      "referenceID" : 19,
      "context" : "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].",
      "startOffset" : 241,
      "endOffset" : 251
    }, {
      "referenceID" : 15,
      "context" : "The well-known Scheffe’s method [16] uses O(ǫ−2 log ∣F∣) samples from the underlying distribution f , and in time O(ǫ−2∣F∣2T log ∣F∣) outputs a distribution in F with l1 distance of at most 9.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 39,
      "context" : "One of the natural and well-used methods to estimate the span of mean vectors is using the correlation matrix [42].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "Even though spectral clustering algorithms are studied in [2,42], they assume that the weights are strictly bounded away from 0, which does not hold here.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 39,
      "context" : "Even though spectral clustering algorithms are studied in [2,42], they assume that the weights are strictly bounded away from 0, which does not hold here.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 34,
      "context" : "Note that the run time is calculated based on the efficient implementation of single-linkage [37] and the exponential term is not optimized.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41].",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41].",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41].",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41].",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 28,
      "context" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41].",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41].",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 31,
      "context" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41].",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 38,
      "context" : "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13–15, 30, 31, 33, 41].",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "The above bound matches the independent and contemporary result by [15] for k = 2.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "[1] Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Dimitris Achlioptas and Frank McSherry.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Rudolf Ahlswede and Andreas Winter.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James R.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Martin Azizyan, Aarti Singh, and Larry A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Mikhail Belkin and Kaushik Sinha.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Kamalika Chaudhuri, Sanjoy Dasgupta, and Andrea Vattani.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] G.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Thomas M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Sanjoy Dasgupta.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Sanjoy Dasgupta and Leonard J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Sanjoy Dasgupta and Leonard J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Constantinos Daskalakis and Gautam Kamath.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Luc Devroye and Gábor Lugosi.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Inderjit S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Jon Feldman, Ryan O’Donnell, and Rocco A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Jon Feldman, Rocco A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Yoav Freund and Yishay Mansour.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Daniel Hsu and Sham M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Ravindran Kannan, Hadi Salmasian, and Santosh Vempala.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] Michael J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] Bruce G.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] Jinwen Ma, Lei Xu, and Michael I.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[29] Ankur Moitra and Gregory Valiant.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[30] Siu on Chan, Ilias Diakonikolas, Rocco A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[31] Siu on Chan, Ilias Diakonikolas, Rocco A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[32] Alon Orlitsky, Narayana P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[33] Liam Paninski.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[35] Richard A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[36] Douglas A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[37] Robin Sibson.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[38] D Michael Titterington, Adrian FM Smith, and Udi E Makov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[39] Joel A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[40] G.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[41] Gregory Valiant and Paul Valiant.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[42] Santosh Vempala and Grant Wang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "[43] Roman Vershynin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "[44] Eric P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "For Gaussian distributions the Bhattacharyya parameter is (see [8]), B(p1, p2) = ye, where x = (μ1−μ2)) 4(σ2 1 +σ 2 ) and y = √ 2σ1σ2 σ 1 +σ 2 .",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "Lemma 16 ( [25]).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 40,
      "context" : "Lemma 18 ( [43] Remark 5.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 15,
      "context" : "Scheffe estimate [16] outputs a distribution from F whose l1 distance from f is at most 9.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "Together with an observation in Scheffe estimation in [16] one can show that if the number of samples n = O ( log ∣F∣ δ ǫ ), then SCHEFFE* has a guarantee 10max(ǫ,D(f,F)) with probability ≥ 1 − δ.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "1 Single Gaussian distribution The proof is an application of the following version of Fano’s inequality [9, 45].",
      "startOffset" : 105,
      "endOffset" : 112
    } ],
    "year" : 2014,
    "abstractText" : "Statistical and machine-learning algorithms are frequently applied to high-dimensional data. In many of these applications data is scarce, and often much more costly than computation time. We provide the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. For mixtures of any k d-dimensional spherical Gaussians, we derive an intuitive spectral-estimator that uses Ok(d log d ǫ ) samples and runs in time Ok,ǫ(d3 log d), both significantly lower than previously known. The constant factor Ok is polynomial for sample complexity and is exponential for the time complexity, again much smaller than what was previously known. We also show that Ωk( d ǫ ) samples are needed for any algorithm. Hence the sample complexity is near-optimal in the number of dimensions. We also derive a simple estimator for k-component one-dimensional mixtures that uses O(k log kǫ ǫ ) samples and runs in time Õ ((k ǫ )). Our other technical contributions include a faster algorithm for choosing a density estimate from a set of distributions, that minimizes the l1 distance to an unknown underlying distribution. jacharya@ucsd.edu ashkan@ucsd.edu alon@ucsd.edu asuresh@ucsd.edu",
    "creator" : "LaTeX with hyperref package"
  }
}