{
  "name" : "1109.5647.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization",
    "authors" : [ "Ohad Shamir" ],
    "emails" : [ "ohadsh@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 9.\n56 47\nv1 [\ncs .L\nG ]\n2 6\nSe p"
    }, {
      "heading" : "1 Introduction",
      "text" : "Stochastic gradient descent (SGD) with averaging is one of the simplest and most popular first-order methods to solve convex learning problems. Given a convex loss function and a training set of T examples, SGD can be used to obtain a sequence of T predictors, whose average has a generalization error which converges (with T ) to the optimal one in the class of predictors we consider. The common framework to analyze such first-order algorithms is via stochastic optimization, where our goal is to optimize an unknown convex function F , given only unbiased estimates of F ’s subgradients (see Sec. 2 for a more precise definition).\nAn important special case is when F is strongly convex (intuitively, can be lower bounded by a quadratic function). Such functions arise, for instance, in Support Vector Machines and other regularized learning algorithms. For such problems, there is a well-known O(log(T )/T ) convergence guarantee for SGD with averaging. This rate is obtained using the analysis of the algorithm in the harder setting of online learning [3], with the O(log(T )/T ) rate obtained via an online-to-batch conversion (see [4] for more details).\nSurprisingly, a recent paper by Hazan and Kale [4] showed that in fact, an O(log(T )/T ) is not the best that one can achieve for strongly convex stochastic problems. In particular, an optimal O(1/T ) rate can be obtained using a different algorithm than SGD, which is more complex (although with comparable computational complexity)1. A very similar algorithm was also presented in [5].\nAn important gap left by these results is whether the true convergence rate of SGD might also be O(1/T ), and the known O(log(T )/T ) result is just an artifact of the analysis. Indeed, the whole motivation of [4] was that the standard online analysis is too loose to analyze the stochastic setting appropriately. Perhaps a similar looseness applies to the analysis of SGD as well? This question has immediate practical relevance: if the new algorithms enjoy a better rate\n1Roughly speaking, the algorithm divides the T iterations into exponentially increasing epochs, and runs stochastic gradient descent with averaging on each one. The resulting point of each epoch is used as the starting point of the next epoch. The algorithm returns the resulting point of the last epoch.\nthan SGD, it might indicate they will work better in practice, and that practitioners should abandon SGD in favor of the new algorithms.\nIn this paper, we study the convergence rate of SGD (with averaging) for stochastic strongly convex problems, with the following contributions:\n• First, we extend known results to show that if F is smooth as well as strongly convex, then SGD with averaging achieves the optimal O(1/T ) convergence rate.\n• We then show that for non-smooth F , there are cases where the convergence rate of SGD with averaging is Ω(log(T )/T ). In other words, the O(log(T )/T ) bound for general strongly convex problems is real, and not just an artifact of the currently-known analysis.\n• However, we show that one can recover the optimal O(1/T ) convergence rate, by a simple modification of SGD: Instead of simple averaging of all T points, we only average the last αT points, where α ∈ (0, 1) is arbitrary. Thus, to obtain an optimal rate, one does not need to use an algorithm significantly different than SGD, such as those discussed earlier.\nWhile the focus here is on getting the optimal rate in terms of T , we note that our upper bounds are also optimal in terms of other standard parameters, such as the strong convexity parameter and the variance of the gradient estimates. We make no claim as to the optimality of the (usually small) constants.\nFollowing the paradigm of [4], we analyze the algorithm directly in the stochastic setting, and avoid an online analysis with an online-to-batch conversion. This allows us to prove results which are often more general. For example, the standard analysis of SGD requires the step size of the algorithm at round t to equal 1/λt, where λ is the strong convexity parameter of F . In contrast, our analysis copes with any step size c/λt, as long as c is not too small.\nIn terms of related work, we note that the performance of SGD in a stochastic setting has been extensively researched in stochastic approximation theory (see for instance [6]). However, these results are usually obtained under smoothness assumptions, and are quite often asymptotic, so we do not get an explicit bound in terms of T which applies to our setting. We also note that a finite-sample analysis of SGD in the stochastic setting was recently presented in [2]. However, the focus there was different than ours. More importantly, the analysis was carried out under stronger smoothness assumptions than our analysis, and to the best of our understanding, does not apply to general, possibly non-smooth, strongly convex stochastic optimization problems. For example, smoothness assumptions do not cover the application of SGD to support vector machines (as in [7]), since it uses a non-smooth loss function.\nOur paper is structured as follows: we begin in Sec. 2 by introducing our setting and notation. In Sec. 3, we analyze the case where F is both strongly convex and smooth. We then turn to the general, possibly non-smooth case, and show in Sec. 4 how an Ω(log(T )/T ) rate may be inevitable for SGD with averaging. In Sec. 5, we demonstrate how a simple modification of the averaging step recovers the optimal O(1/T ) rate. We end with a short discussion in Sec. 6. A few technical lemmas are collected in Appendix A."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We use bold-face letters to denote vectors. Given some vector w, we use wi to denote its i-th coordinate. Similarly, given some indexed vector wt, we let wt,i denote its i-th coordinate. We let 1A denote the indicator function to some event A.\nWe consider the standard setting of convex stochastic optimization, using first-order methods. Our goal is to minimize a convex function F over some convex domain W (which is assumed to be a subset of some Hilbert space). However, we do not know F , and the only information available is through a stochastic gradient oracle, which given somew ∈ W , produces a vector ĝ, whose expectationE[ĝ] = g is a subgradient ofF at w. Using a bounded numberT of calls to this oracle, we wish to find a point wT such that F (wt) is as small as possible. In particular, we will assume that F attains a minimum at some w∗ ∈ W , and our analysis will provide bounds on E[F (wt)− F (w∗)], where the expectation is over the randomness of the algorithm. The application of this framework to learning is straightforward (see for instance [8]): given a hypothesis class W and a set of T i.i.d. examples, we wish to find a predictor w whose expected loss F (w) is close to optimal over W . Since the examples are chosen i.i.d., the subgradient of the loss function with respect to any individual example can be shown to be an unbiased estimate of a subgradient of F .\nWe will focus on a special case of the problem above, denoted as strongly convex stochastic optimization. This setting is characterized by F being a strongly convex function. Formally, we say that a function F is λ-strongly convex, if for all w,w′ ∈ W and any subgradient g of F at w,\nF (w′) ≥ F (w) + 〈g,w′ −w〉+ λ 2 ‖w′ −w‖2. (1)\nAnother class of functions we will consider are smooth functions. Formally, a function F is µ-smooth if for all w,w′ ∈ W and any subgradient g of F at w,\nF (w′) ≤ F (w) + 〈g,w′ −w〉+ µ 2 ‖w′ −w‖2. (2)\nThe algorithm we focus on is stochastic gradient descent (SGD). The SGD algorithm is parameterized by step sizes η1, . . . , ηT , and is defined as follows:\n1. Initialize w1 ∈ W arbitrarily (or randomly)\n2. For t = 1, . . . , T :\n• Query the stochastic gradient oracle at wt to get a random ĝt such that E[ĝt] = gt is a subgradient of F at wt. • Let wt+1 = ΠW(wt − ηtĝt), where ΠW is the projection operator on W .\nThis algorithm returns a sequence of points w1, . . . ,wT . To obtain a single point, the common procedure is to return their average\nw̄T = 1\nT (w1 + . . .+wT ).\nFor stochastic optimization of λ-strongly functions, the standard analysis (through online learning) focuses on the step size ηt being exactly 1/λt [3]. Our analysis will consider more general step-sizes c/λt, where c is a constant. We note that a step size of Θ(1/t) is the appropriate regime to consider, since for significantly smaller step sizes, SGD may not converge to the optimum, and for significantly larger step sizes, the algorithm may not obtain an O(1/T ) convergence rate.\nIn general, we will assume that regardless of how w1 is initialized, it holds that E[‖ĝt‖2] ≤ G2 for some fixed constant G. Note that this is a somewhat weaker assumption than [4], which required that ‖ĝt‖2 ≤ G2 with probability 1, since we focus only on bounds which hold in expectation. These types of assumptions are common in the literature, and we note that our assumption can be shown to hold if W is a bounded domain, or alternatively, if w1 is initialized not too far from w∗ and F satisfies certain technical conditions (see for instance the proof of Theorem 1 in [7])."
    }, {
      "heading" : "3 Smooth Functions",
      "text" : "We begin be considering the case where the expected function F (·) is both strongly convex and smooth. Our starting point is to show a O(1/T ) for the last predictor wT obtained by SGD. We note that this result is well-known in the stochastic approximation literature (see for instance [1] and references therein), and we include a proof for completeness.\nTheorem 1. Suppose F is λ-strongly convex and µ-smooth over a convex set W , and that E[‖ĝt‖2] ≤ G2. Furthermore, suppose that w∗ is in the interior of W . Then if we pick ηt = c/λt for some constant c > 1/2, it holds for any T that\nE[F (wT )− F (w∗)] ≤ 1\n2 max\n{\n4 , c\n2− 1/c\n}\nµG2 λ2T .\nThe theorem is an immediate corollary of the following lemma, and the observation that since F is also assumed to be µ-smooth and w∗ is in the interior of W , then F (wT )− F (w∗) ≤ µ2 ‖wT −w∗‖2.\nLemma 1. Suppose F is λ-strongly convex over a convex set W , and that E[‖ĝt‖2] ≤ G2. Then if we pick ηt = c/λt for some constant c > 1/2, it holds for any T that\nE [ ‖wT −w∗‖2 ] ≤ max { 4 , c\n2− 1/c\n}\nG2\nλ2T .\nProof. By the strong convexity of F and the fact that w∗ minimizes F in W , we have\n〈gt,wt −w∗〉 ≥ F (wt)− F (w∗) + λ\n2 ‖wt −w∗‖2,\nas well as\nF (wt)− F (w∗) ≥ λ\n2 E [‖wt −w∗‖]2 .\nAlso, by convexity of W , for any pointv and any w ∈ W we have ‖ΠW(v)−w‖ ≤ ‖v−w‖. Using these inequalities, we have the following:\nE [ ‖wt+1 −w∗‖2 ] = E[‖ΠW(wt − ηtĝt)−w∗‖2] ≤ E [ ‖wt − ηtĝt −w∗‖2 ]\n= E [ ‖wt −w∗‖2 ] − 2ηtE[〈ĝt,wt −w∗〉] + η2tE[‖ĝt‖2] = E [ ‖wt −w∗‖2 ] − 2ηtE[〈gt,wt −w∗〉] + η2tE[‖ĝt‖2]\n≤ E [ ‖wt −w∗‖2 ] − 2ηtE [ F (wt)− F (w∗) + λ\n2 ‖wt −w∗‖2\n]\n+ η2tG 2\n≤ E [ ‖wt −w∗‖2 ] − 2ηtE [ λ\n2 ‖wt −w∗‖2 +\nλ 2 ‖wt −w∗‖2\n]\n+ η2tG 2\n= (1− 2ηtλ)E [ ‖wt −w∗‖2 ] + η2tG 2.\nInvoking Lemma 3, using the fact that ηt = c/λt and E[‖w1 −w∗‖2] ≤ 4G 2\nλ2 (by Lemma 5), we get that\nE [ ‖wT −w∗‖2 ] ≤ G 2\nλ2T max\n{\n4, c2\n2c− 1\n}\n.\nWith this result in hand, we now turn to discuss the behavior of the average point w̄T = (w1 + . . .+wT )/T , and show that for smooth F , it also enjoys an optimal O(1/T ) convergence rate. Theorem 2. Suppose F is λ-strongly convex and µ-smooth over a convex set W , and that E[‖ĝt‖2] ≤ G2. Furthermore, suppose that w∗ is in the interior of W . Then if we pick ηt = c/λt for some constant c > 1/2, it holds for any T that\nE[F (w̄T )− F (w∗)] ≤ 2max { µG2\nλ2 , 4µG λ , µG λ\n√\n4c\n2− 1/c\n}\n1 T .\nProof. For any t, define w̄t = (w1 + . . .+wt)/t. Then we have\nE [ ‖w̄t+1 −w∗‖2 ] = E\n[\n‖ t t+ 1 w̄t + 1 t+ 1 wt+1 −w∗‖2\n]\n= E\n[\n‖ t t+ 1 (w̄t −w∗) + 1 t+ 1 (wt+1 −w∗)‖2\n]\n=\n(\nt\nt+ 1\n)2\nE [ ‖w̄t −w∗‖2 ] + 2t\n(t+ 1)2 E [〈w̄t −w∗,wt+1 −w∗〉] +\n1\n(t+ 1)2 E [ ‖wt+1 −w∗‖2 ]\n≤ ( t\nt+ 1\n)2\nE [ ‖w̄t −w∗‖2 ] + 2\nt+ 1 E [‖w̄t −w∗‖ ‖wt+1 −w∗‖] +\n1\n(t+ 1)2 E [ ‖wt+1 −w∗‖2 ]\nUsing the inequality E[|XY |] ≤ √ E[X2] √\nE[Y 2] for any random variables X,Y (which follows from CauchySchwartz), and the bound of Lemma 1, we get that E [ ‖w̄t+1 −w∗‖2 ] is at most\n(\nt\nt+ 1\n)2\nE [ ‖w̄t −w∗‖2 ] + 2max{2,\n√\nc/(2− 1/c)}G λ(t+ 1)3/2 √ E[‖w̄t −w∗‖2] + max{4, c/(2− 1/c)}G2 λ2(t+ 1)3 .\nUsing Lemma 4, and using the fact that E[‖w̄1 −w∗‖2] ≤ 4G2/λ2 by Lemma 5, we get that\nE [ ‖w̄T −w∗‖2 ] ≤ 1 T max\n{\n4G2 λ2 , 5max{2,\n√\nc/(2− 1/c)}G√ 2λ\n}\n.\nBy the assumed smoothness of F and the fact that w∗ is in the interior of W , we have F (w̄T )− F (w∗) ≤ µ2 ‖w̄T − w∗‖2. Combining it with the inequality above, and slightly upper bounding the constants for readability, the result follows."
    }, {
      "heading" : "4 Non-Smooth Functions",
      "text" : "We now turn to the discuss the more general case where the function F may not be smooth (i.e. there is no constant µ which satisfies Eq. (2) uniformly for all w,w′ ∈ W). In the context of learning, this may happen when we try to learn a predictor with respect to a non-smooth loss function, such as the hinge loss.\nAs discussed earlier, SGD with averaging is known to have a rate of at most O(log(T )/T ). In the previous section, we saw that for smooth F , the rate is actually O(1/T ). Moreover, [4] showed that for using a different algorithm than SGD, one can obtain a rate of O(1/T ) even in the non-smooth case. This might lead us to believe that an O(1/T ) rate for SGD is possible in the non-smooth case, and that the O(log(T )/T ) analysis is simply not tight.\nHowever, this intuition turns out to be wrong. Below, we show that there are strongly convex stochastic optimization problems in Euclidean space, in which the convergence rate of SGD is lower bounded by Ω(log(T )/T ). Thus, the logarithm in the bound is not merely a shortcoming in the standard online analysis of SGD, but is really a property of the algorithm.\nWe begin with a relatively simple example (Thm. 3), which shows the essence of the idea. The intuition of the construction is that the global optimum lies at a corner of W , so averaging the points returned by SGD does not help us in getting closer to it.\nTheorem 3. There exists a stochastic 1-strongly convex problem over the domain W = [0, 1]d, with a global minimum w∗ at 0 and E[‖ĝt‖2] ≤ d + 5, with the following property: If SGD is initialized at any point in W , and ran with ηt = c/t, then for any T ≥ T0 + 1, where T0 = max{2, c/2}, we have\nE[F (w̄T )− F (w∗)] ≥ c\n16T\nT−1 ∑\nt=T0\n1 t .\nWhen c is considered a constant, this lower bound is Ω(log(T )/T ).\nProof. Let F be the 1-strongly convex function\nF (w) = 1\n2 ‖w‖2 + w1,\nwhich has a global minimum at 0. Suppose the stochastic gradient oracle, given a point wt, returns the gradient estimate\nĝt = wt + (Zt, 0, . . . , 0),\nwhere Zt is uniformly distributed over [−1, 3]. It is easily verified that E[ĝt] is a subgradient of F (wt), and that E[‖ĝt‖2] ≤ d+ 5 which is a bounded quantity for fixed d.\nThe SGD iterate can be written separately for the first coordinate as\nwt+1,1 = Π[0,1] ((1 − ηt)wt,1 − ηtZt) (3)\nFix some t ≥ T0, and suppose first that Zt ≤ −1/2. Conditioned on this event, we have\nwt+1,1 ≥ Π[0,1] ( (1− ηt)wt,1 + 1\n2 ηt\n)\n≥ Π[0,1](ηt/2) ≥ ηt/2,\nsince t ≥ T0 implies ηt = c/t ≤ 2. On the other hand, if Zt > −1/2, we are still guaranteed that wt+1,1 ≥ 0 by the domain constraints. Using these results, we get\nE[wt+1,1] = Pr(Zt ≤ −1/2)E[wt+1,1 | Zt ≤ −1/2] + Pr(Zt > −1/2)E[wt+1,1 | Zt > −1/2] ≥ Pr(Zt ≤ −1/2)E[wt+1,1 | Zt ≤ −1/2]\n≥ Pr(Zt ≤ −1/2) 1\n2 ηt =\n1\n16 ηt. (4)\nTherefore,\nE[w̄T,1] ≥ 1\nT\nT ∑\nt=T0+1\nE[wt,1] ≥ 1\n16T\nT ∑\nt=T0+1\nηt−1 = 1\n16T\nT−1 ∑\nt=T0\nηt.\nThus, by definition of F ,\nE [F (w̄T )− F (w∗)] = E [F (w̄T )] ≥ E [F ((w̄T,1, 0, . . . , 0))] ≥ E [w̄T,1] ≥ 1\n16T\nT−1 ∑\nt=T0\nηt.\nSubstituting ηt = c/t gives the required result.\nWhile being relatively straightforward, this example is not fully satisfying, since it crucially relies on the fact that w∗ is on the border of W . In strongly convex problems, w∗ usually lies in the interior of W , so perhaps the Ω(log(T )/T ) lower bound does not hold in such cases. Our main result, presented below, shows that this is not the case, and that even if w∗ is well inside the interior of W , an Ω(log(T )/T ) rate for SGD with averaging can be unavoidable. The intuition is that we construct a non-smooth F , which forces wt to approach the optimum from just one direction, creating the same effect as in the previous example.\nTheorem 4. There exists a stochastic 1-strongly convex problem over W = [−1, 1]d, with a global minimum w∗ at 0 and E[‖ĝt‖2] ≤ d + 63, with the following property: If SGD is initialized at any point w1 with w1,1 ≥ 0, and ran with ηt = c/t, then for any T ≥ T0 + 2, where T0 = max{2, 6c+ 1}, we have\nE [F (w̄T )− F (w∗)] ≥ 3c\n16T\nT ∑\nt=T0+2\n(\n1\nt\n)\n− T0 T .\nWhen c is considered a constant, this lower bound is Ω(log(T )/T ).\nWe note that the requirement of w1,1 ≥ 0 is just for convenience - the analysis also carries through, with some additional second-order factors, if we let w1,1 < 0.\nProof. Let F be the 1-strongly convex function\nF (w) = 1\n2 ‖w‖2 +\n{\nw1 w1 ≥ 0 −7w1 w1 < 0 ,\nwhich has a minimum in W at 0. Suppose the stochastic gradient oracle, given a point wt, returns the gradient estimate\nĝt = wt +\n{\n(Zt, 0, . . . , 0) w1 ≥ 0 (−7, 0, . . . , 0) w1 < 0 ,\nwhere Zt is a random variable uniformly distributed over [−1, 3]. It is easily verified that E[ĝt] is a subgradient of F (wt), and that E[‖ĝt‖2] ≤ d+ 63 which is a bounded quantity for fixed d.\nThe SGD iterate for the first coordinate is\nwt+1,1 = Π[−1,1]\n(\n( 1− c t )\nwt,1 − { Zt c t wt,1 ≥ 0\n− 7ct wt,1 < 0\n)\n. (5)\nThe intuition of the proof is that wheneverwt,1 becomes negative, then the large gradient of F causes wt+1,1 to always be significantly larger than 0. This means that in some sense, wt,1 is “constrained” to be larger than 0, mimicking the actual constraint in the example of Thm. 3 and forcing the same kind of behavior, with a resulting Ω(log(T )/T ) rate.\nTo make this intuition rigorous, we begin with the following lemma, which shows that wt,1 can never be significantly smaller than 0, or “stay” below 0 for more than one iteration.\nLemma 2. For any t ≥ T0 = max{2, 6c+ 1}, it holds that\nwt,1 ≥ − c\nt− 1 ,\nand if wt,1 < 0, then\nwt+1,1 ≥ 5c\nt .\nProof. Suppose first that wt−1,1 ≥ 0. Then by Eq. (5) and the fact that Zt ≥ −1, we get wt,1 ≥ −c/(t−1). Moreover, in that case, if wt,1 < 0, then by Eq. (5) and the previous observation,\nwt+1,1 = Π[−1,1]\n(\n( 1− c t ) wt,1 + 7c t\n) ≥ Π[−1,1] ( − ( 1− c t ) c t− 1 + 7c t ) .\nSince t ≥ c, we have 1− c/t ∈ (0, 1), which implies that the above is lower bounded by\nΠ[−1,1]\n(\n− c t− 1 + 7c t\n)\n.\nMoreover, since t ≥ 6c, we have −c/(t− 1) + 7c/t ∈ [−1, 1], so the projection operator is unnecessary, and we get overall that\nwt+1,1 ≥ − c t− 1 + 7c t ≥ 5c t .\nThis result was shown to hold assuming that wt−1,1 ≥ 0. If wt−1,1 < 0, then repeating the argument above for t − 1 instead of t, we must have wt,1 ≥ 5c/(t − 1) ≥ −c/(t − 1), so the statement in the lemma holds also when wt−1,1 < 0.\nWe turn to the proof of Thm. 4 itself. By Lemma 2, if t ≥ T0, then wt,1 < 0 implies wt+1,1 ≥ 0, and moreover, wt,1 + wt+1,1 ≥ −c/(t− 1) + 5c/t ≥ 3c/t. Therefore, we have the following, where the sums below are only over t’s which are between T0 and T :\nE[wT0,1 + . . .+ wT,1] = 1\n2 E[2wT0,1 + . . .+ 2wT,1] ≥\n1 2 E\n\n\n∑\nt:wt,1<0\n(wt,1 + wt+1,1) + ∑\nt:wt,1>c/t\nwt,1\n\n\n≥ 1 2 E\n\n\n∑\nt:wt,1<0\n3c\nt +\n∑\nt:wt,1>c/t\nc\nt\n\n\n≥ 1 2 E\n\n\n∑\nt:wt,1 /∈[0,c/t]\nc\nt\n\n = ∑\nt\nPr(wt,1 /∈ [0, ηt]) c\n2t . (6)\nNow, we claim that the probabilities above can be lower bounded by a constant. To see this, consider each such probability for wt,1, conditioned on the event wt−1,1 ≥ 0. Using the fact that c/t < 1, we have\nPr(wt,1 ∈ [0, c/t] | wt−1,1 ≥ 0) = Pr ( Π[−1,1] ((1− c/t)wt−1,1 − (c/t)Zt) ∈ [0, c/t] | wt−1,1 ≥ 0 ) = Pr ((1− c/t)wt−1,1 − (c/t)Zt ∈ [0, c/t] | wt−1,1 ≥ 0) = Pr (Zt ∈ [(t/c− 1)wt−1,1 − 1 , (t/c− 1)wt−1,1] | wt−1,1 ≥ 0) .\nThis probability is at most 1/4, since it asks for Zt being constrained in an interval of size 1, whereas Zt is uniformly distributed over [−1, 3], which is an interval of length 4. As a result, we get\nPr(wt,1 /∈ [0, c/t] | wt−1,1 ≥ 0) = 1− Pr(wt,1 ∈ [0, c/t] | wt−1,1 ≥ 0) ≥ 3\n4 .\nFrom this, we can lower bound Eq. (6) as follows:\n∑\nt\nPr(wt,1 /∈ [0, ηt]) c\nt ≥\n∑\nt\nPr(wt,1 /∈ [0, ηt], wt−1,1 ≥ 0) c\n2t\n= ∑\nt\nPr(wt−1,1 ≥ 0)Pr(wt,1 /∈ [0, ηt] | wt−1,1 ≥ 0) c\n2t\n≥ 3 8\nT ∑\nt=T0\nc t Pr(wt−1,1 ≥ 0) = 3 8 E\n[\nT ∑\nt=T0\nc t 1wt−1,1≥0\n]\n= 3\n16 E\n[\nT ∑\nt=T0\nc t 1wt−1,1≥0 +\nT ∑\nt=T0\nc t 1wt−1,1≥0\n]\n≥ 3 16 E\n[\nT−1 ∑\nt=T0\nc t 1wt−1,1≥0 +\nT ∑\nt=T0+1\nc t 1wt−1,1≥0\n]\n= 3\n16 E\n[\nT−1 ∑\nt=T0\nc t 1wt−1,1≥0 +\nT−1 ∑\nt=T0\nc\nt+ 1 1wt,1≥0\n]\n≥ 3 16 E\n[\nT−1 ∑\nt=T0+1\nc\nt+ 1\n( 1wt−1,1≥0 + 1wt,1≥0 )\n]\n.\nNow, by Lemma 2, for any realization of wT0,1, . . . , wT,1, the indicators 1wt,1≥0 cannot equal 0 consecutively. Therefore, 1wt−1,1≥0 + 1wt,1≥0 must always be at least 1. Plugging it in the equation above, we get the lower bound (3c/16) ∑T\nt=T0+2 1 t . Summing up, we have shown that\nE[wT0,1 + . . .+ wT,1] ≥ 3c\n16\nT ∑\nt=T0+2\n1 t .\nBy the boundedness assumption on W , we have2 wt,1 ≥ −1, so\nE[w1,1 + . . .+ wT0−1,1] ≥ −T0.\nOverall, we get\nE[w̄T,1] = 1\nT E\n[\nT ∑\nt=1\nwt,1\n]\n≥ 3c 16T\nT ∑\nt=T0+2\n(\n1\nt\n)\n− T0 T .\nTherefore,\nE [F (w̄T )− F (w∗)] = E [F (w̄T )] ≥ E [F ((w̄T,1, 0, . . . , 0))] ≥ E [w̄T,1] ≥ 3c\n16T\nT ∑\nt=T0+2\n(\n1\nt\n)\n− T0 T .\nas required.\n2To analyze the case where W is unbounded, one can replace this by a coarse bound on how much wt,1 can change in the first T0 iterations, since the step sizes are bounded and T0 is essentially a constant anyway.\n5 Recovering an O(1/T ) Rate for SGD with α-Suffix Averaging In the previous section, we showed that SGD with averaging may have a rate of Ω(log(T )/T ) for non-smooth F . To get the optimal O(1/T ) rate for any F , we might turn to the algorithms of [4] and [5]. However, these algorithms constitute a significant departure from standard SGD. In this section, we show that it is actually possible to get an O(1/T ) rate using a simple modification of the SGD algorithm: given the sequence of points w1, . . . ,wT provided by SGD, instead of returning w̄T = (w1 + . . .+wT )/T , we return an average of just a suffix, namely\nw̄αT = w(1−α)T+1 + . . .+wT\nαT ,\nfor some constant α ∈ (0, 1) (assuming αT and (1− α)T are integers). We call this procedure α-suffix averaging. Theorem 5. Consider SGD with α-suffix averaging as described above, and with step sizes ηt = c/λt where c > 1/2 is a constant. Suppose F is λ-strongly convex, with an optimum w∗ in the interior of W , and that E[‖ĝt‖2] ≤ G for all t. Then for any T , it holds that\nE[F (w̄αT )− F (w∗)] ≤\n(\nc′ + ( c 2 + c ′ )\nlog (\n1 1−α\n))\nα\nG2 λT ,\nwhere c′ = max {\n2 c , 1 4−2/c\n}\n.\nNote that for any constant α ∈ (0, 1), the bound above is O(G2/λT ). This applies to any relevant step size c/λt, and matches the optimal guarantees in [4] up to constant factors. However, this is shown for standard SGD, as opposed to the more specialized algorithm of [4]. Finally, we note that it might be tempting to use Thm. 5 as a guide to choose the averaging window, by optimizing the bound for α (for instance, for c = 1, the optimum is achieved around α ≈ 0.65). However, we note that the optimal value of α is dependent on the constants in the bound, which may not be the tightest or most “correct” ones.\nProof. Using the derivation as in the proof of Lemma 1, we have\nE[‖wt+1 −w∗‖2] ≤ E[‖wt −w∗‖2]− 2ηtE[〈gt,wt −w∗〉] + η2tG2,\nExtracting the inner product and summing over t = (1− α)T + 1, . . . , T , we get T ∑\nt=(1−α)T+1\n〈E[gt],wt −w∗〉 ≤ T ∑\nt=(1−α)T+1\n(\nE[‖wt −w∗‖2] 2ηt − E[‖wt+1 −w ∗‖2] 2ηt + ηtG 2 2\n)\n. (7)\nBy convexity of F ,\nT ∑\nt=(1−α)T+1\n〈E[gt],wt −w∗〉 ≥ T ∑\nt=(1−α)T+1\nF (wt)− F (w∗) ≥ αT (F (w̄αT )− F (w∗)) .\nSubstituting this lower bound into Eq. (7) and slightly rearranging the right hand side, we get that F (w̄αT ) − F (w∗) can be upper bounded by\n1\n2αT\n\n\n1\nη(1−α)T+1 E[‖w(1−α)T+1 −w∗‖2] +\nT ∑\nt=(1−α)T+1\nE[‖wt −w∗‖2] ( 1 ηt − 1 ηt−1 ) +G2 T ∑\nt=(1−α)T+1\nηt\n\n .\nNow, we invoke Lemma 1, which tells us that with any strongly convexF , even non-smooth, we haveE[‖wt−w∗‖2] ≤ O(1/t). More specifically, we can upper bound the expression above by\n1\n2αT\n\n\nmax {\n4, c2−1/c\n}\nG2\nλ2\n\n\n1\n((1− α)T + 1)η(1−α)T+1 +\nT ∑\nt=(1−α)T+1\n1/ηt − 1/ηt−1 t\n +G2 T ∑\nt=(1−α)T+1\nηt\n\n .\nIn particular, since we take ηt = c/λt, we get\n1\n2αT\n\n\nmax {\n4, c2−1/c\n}\nG2\ncλ\n\n1 +\nT ∑\nt=(1−α)T+1\n1\nt\n + cG2\nλ\nT ∑\nt=(1−α)T+1\n1\nt\n\n .\nIt can be shown that ∑T t=(1−α)T+1 1 t ≤ log(1/(1 − α)). Plugging it in and slightly simplifying, we get the desired bound."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper, we analyzed the behavior of SGD with averaging for strongly convex stochastic optimization problems. We demonstrated that this simple and well-known algorithm performs optimally whenever the underlying function is also smooth, but can be suboptimal for non-smooth problems. However, a simple modification of the averaging step suffices to recover the optimal rate, and a more sophisticated algorithm is not necessary.\nThere are still several open issues remaining. For example, all our results are for bounds which hold in expectation, and it remains to be seen whether high-probability bounds with a similar rate can be obtained. Also, we are currently conducting experiments to complement our theoretical results, and study the practical implications of our findings."
    }, {
      "heading" : "A Technical Lemmas",
      "text" : "Lemma 3. Let a > 1, b ≥ 0 and x1 ∈ [0, D] be arbitrary constants. Let x2, x3, . . . be a non-negative sequence which satisfies\nxt+1 ≤ ( 1− a t ) xt + b t2 .\nThen for all t,\nxt ≤ max(D, b/(a− 1))\nt .\nProof. Let m = max(D, b/(a − 1)). The proof is by simple induction. The assertion clearly holds for t = 1. Now, suppose that xt ≤ mt . Then it suffices to show that\n( 1− a t ) m t + b t2 ≤ m t+ 1 .\nThis can be simplified to b(t+ 1) ≤ m((a− 1)t+ a).\nwhich clearly holds as m ≥ b/(a− 1).\nLemma 4. Let b ≥ 0,c ≥ 0 and x1 ∈ [0, D] be arbitrary constants. Let x2, x3, . . . be a non-negative sequence which satisfies\nxt+1 ≤ ( t\nt+ 1\n)2\nxt + b (t+ 1)3/2 √ xt +\nc\n(t+ 1)3 .\nthen for all t,\nxt ≤ max{D, b\n√ 2 + √\nc/2} t .\nProof. Let m = max{D, b √ 2 + √\nc/2}. As in the previous lemma, the proof is by induction. The assertion clearly holds for t = 1. Now, suppose that xt ≤ mt . Then it suffices to show that\n(\nt\nt+ 1\n)2 m\nt +\nb\n(t+ 1)3/2\n√\nm\nt +\nc (t+ 1)3 ≤ m t+ 1 .\nThis can be simplified to\n(t+ 1)m− b(t+ 1) √ t+ 1\nt\n√ m− c ≥ 0.\nBy solving the quadratic inequality, we get that this holds whenever\nm ≥ b 2\n√\nt+ 1\nt +\n1\n2\n√\nb2 t+ 1\nt +\n4c\nt+ 1 .\nTo ensure this holds for all t, it suffices to verify for the value of t maximizing the right hand side, namely t = 1:\nm ≥ b 2\n√ 2 + 1\n2\n√\n2b2 + 2c,\nwhich follows immediately from the definition of m.\nLemma 5. If E[‖ĝ1‖2] ≤ G2, then E[‖w1 −w∗‖2] ≤ 4G2\nλ2 .\nProof. Intuitively, the lemma holds because the strong convexity of F implies that the expected value of ‖ĝ1‖2 must strictly increase as we get farther from w∗. More precisely, strong convexity implies that for any w1,\n〈g1,w1 −w∗〉 ≥ λ\n2 ‖w1 −w∗‖2\nso by the Cauchy-Schwartz inequality,\n‖g1‖2 ≥ λ2\n4 ‖w1 −w∗‖2. (8)\nAlso, we have that whether g1 is random or not (depending on whether w1 is chosen arbitrarily or randomly),\nE[‖ĝ1‖2] = E[‖g1 + (ĝ1 − g1)‖2] = E[‖g1‖2] + E[‖ĝ1 − g1‖2] + 2E[〈ĝ1 − g1,g1〉] ≥ E[‖g1‖2].\nCombining this and Eq. (8), we get that for all t,\nE[‖w1 −w∗‖2] ≤ 4\nλ2 E[‖ĝt‖2] ≤\n4G2\nλ2 ."
    } ],
    "references" : [ {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "G. Lan A. Nemirovski", "A. Juditsky", "A. Shapiro" ],
      "venue" : "SIAM J. Optim.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
      "author" : [ "F. Bach", "E. Moulines" ],
      "venue" : "In NIPS (to appear),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Agarwal", "S. Kale" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Beyond the regret minimization barrier: An optimal algorithm for stochastic stronglyconvex optimization",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "In COLT,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Primal-dual subgradient methods for minimizing uniformly convex functions. Technical Report (August 2010), available at http://hal.archives-ouvertes.fr/docs/00/50/89/33/ PDF/Strong-hal.pdf",
      "author" : [ "A. Juditsky", "Y. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Stochastic Approximation and Recursive Algorithms and Applications",
      "author" : [ "H. Kushner", "G. Yin" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Pegasos: primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "This rate is obtained using the analysis of the algorithm in the harder setting of online learning [3], with the O(log(T )/T ) rate obtained via an online-to-batch conversion (see [4] for more details).",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "This rate is obtained using the analysis of the algorithm in the harder setting of online learning [3], with the O(log(T )/T ) rate obtained via an online-to-batch conversion (see [4] for more details).",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 3,
      "context" : "Surprisingly, a recent paper by Hazan and Kale [4] showed that in fact, an O(log(T )/T ) is not the best that one can achieve for strongly convex stochastic problems.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "A very similar algorithm was also presented in [5].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "Indeed, the whole motivation of [4] was that the standard online analysis is too loose to analyze the stochastic setting appropriately.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "Following the paradigm of [4], we analyze the algorithm directly in the stochastic setting, and avoid an online analysis with an online-to-batch conversion.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "In terms of related work, we note that the performance of SGD in a stochastic setting has been extensively researched in stochastic approximation theory (see for instance [6]).",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "We also note that a finite-sample analysis of SGD in the stochastic setting was recently presented in [2].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "For example, smoothness assumptions do not cover the application of SGD to support vector machines (as in [7]), since it uses a non-smooth loss function.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "For stochastic optimization of λ-strongly functions, the standard analysis (through online learning) focuses on the step size ηt being exactly 1/λt [3].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "Note that this is a somewhat weaker assumption than [4], which required that ‖ĝt‖ ≤ G with probability 1, since we focus only on bounds which hold in expectation.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "These types of assumptions are common in the literature, and we note that our assumption can be shown to hold if W is a bounded domain, or alternatively, if w1 is initialized not too far from w and F satisfies certain technical conditions (see for instance the proof of Theorem 1 in [7]).",
      "startOffset" : 283,
      "endOffset" : 286
    }, {
      "referenceID" : 0,
      "context" : "We note that this result is well-known in the stochastic approximation literature (see for instance [1] and references therein), and we include a proof for completeness.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "Moreover, [4] showed that for using a different algorithm than SGD, one can obtain a rate of O(1/T ) even in the non-smooth case.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "There exists a stochastic 1-strongly convex problem over the domain W = [0, 1], with a global minimum w at 0 and E[‖ĝt‖] ≤ d + 5, with the following property: If SGD is initialized at any point in W , and ran with ηt = c/t, then for any T ≥ T0 + 1, where T0 = max{2, c/2}, we have E[F (w̄T )− F (w)] ≥ c 16T T−1",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "The SGD iterate can be written separately for the first coordinate as wt+1,1 = Π[0,1] ((1 − ηt)wt,1 − ηtZt) (3) Fix some t ≥ T0, and suppose first that Zt ≤ −1/2.",
      "startOffset" : 80,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Conditioned on this event, we have wt+1,1 ≥ Π[0,1] (",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "≥ Π[0,1](ηt/2) ≥ ηt/2, since t ≥ T0 implies ηt = c/t ≤ 2.",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : "To get the optimal O(1/T ) rate for any F , we might turn to the algorithms of [4] and [5].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "To get the optimal O(1/T ) rate for any F , we might turn to the algorithms of [4] and [5].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "This applies to any relevant step size c/λt, and matches the optimal guarantees in [4] up to constant factors.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "However, this is shown for standard SGD, as opposed to the more specialized algorithm of [4].",
      "startOffset" : 89,
      "endOffset" : 92
    } ],
    "year" : 2017,
    "abstractText" : "Stochastic gradient descent (SGD) with averaging is a simple and popular method to solve stochastic optimization problems which arise in machine learning. For strongly convex problems, its convergence rate was known to be at most O(log(T )/T ). However, recent results showed that using a different algorithm, one can get an optimal O(1/T ) rate. This might lead one to believe that SGD is suboptimal, and maybe should even be replaced as a method of choice. In this paper, we investigate the convergence rate of SGD with averaging in a stochastic setting. We show that for smooth problems, the algorithm attains the optimal O(1/T ) rate. However, for non-smooth problems, the convergence rate might really be Ω(log(T )/T ), and this is not just an artifact of the analysis. On the flip side, we show that a simple modification of the averaging step suffices to recover the O(1/T ) step, and no significant change of the algorithm is necessary.",
    "creator" : "LaTeX with hyperref package"
  }
}