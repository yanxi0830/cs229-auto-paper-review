{
  "name" : "1505.02294.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Estimation with Norm Regularization",
    "authors" : [ "Arindam Banerjee", "Sheng Chen", "Farideh Fazayeli", "Vidyashankar Sivakumar" ],
    "emails" : [ "banerjee@cs.umn.edu}", "shengc@cs.umn.edu}", "farideh@cs.umn.edu}", "sivakuma@cs.umn.edu}" ],
    "sections" : [ {
      "heading" : null,
      "text" : "n , where c depends on the Gaussian width of the unit norm ball."
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the past decade, progress has been made in developing non-asymptotic bounds on the estimation error of structured parameters based on norm regularized regression. Such estimators are usually of the form [39, 29, 9]:\nθ̂λn = argmin θ∈Rp\nL(θ;Zn) + λnR(θ) , (1)\nwhere R(θ) is a suitable norm, L(·) is a suitable loss function, Zn = {(yi, Xi)}ni=1 where yi ∈ R, Xi ∈ Rp is the training set, and λn > 0 is a regularization parameter. The optimal parameter θ∗ is often assumed to be ‘structured,’ usually characterized or approximated as a small value according to some norm R(·). Recent work has viewed such characterizations in terms of atomic norms, which give the tightest convex relaxation of a structured set of atoms in which θ∗ belongs [14]. Since θ̂λn is an estimate of the optimal structure θ\n∗, the focus has been on bounding a suitable measure of the error vector ∆̂n = (θ̂λn − θ∗), e.g., the L2 norm ‖∆̂n‖2. To understand the state-of-the-art on non-asymptotic bounds on the estimation error for norm-regularized regression, four aspects of (1) need to be considered: (i) the norm R(θ), (ii) properties of the design matrix X = [X1 · · ·Xn]T ∈ Rn×p, (iii) the loss function L(·), and (iv) the noise model, typically in terms of ωi = yi − E[y|Xi]. Most of the literature has focused on a linear model: y = Xθ + ω, and a squared-loss function: L(θ;Zn) = 1n‖y −Xθ‖ 2 2 = 1 n ∑n i=1(yi − 〈θ,Xi〉)2. Early work on such estimators focussed on\nar X\niv :1\n50 5.\n02 29\n4v 3\n[ st\nat .M\nL ]\n3 0\nN ov\nthe L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33]. While much of the development has focussed on isotropic Gaussian design matrices, recent work has extended the analysis for L1 norm to correlated Gaussian designs [33] as well as anisotropic sub-Gaussian design matrices [34].\nBuilding on such development, [29] presents a unified framework for the case of decomposable norms and also considers generalized linear models (GLMs) for certain norms such as L1. Two key insights are offered in [29]: first, the error vector ∆̂n lies in a restricted set, a cone or a star, for suitably large λn, and second, the loss function needs to satisfy restricted strong convexity (RSC), a generalization of the RE condition, on the restricted error set for the analysis to work out.\nFor isotropic Gaussian design matrices, additional progress has been made. [14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon’s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere. [31] considers three related formulations for generalized Lasso problems, establish recovery guarantees based on Gordon’s inequality, and quantities related to the Gaussian width. Sharper analysis for recovery has been considered in [1], yielding a precise characterization of phase transition behavior using quantities related to the Gaussian width. [32] consider a linear programming estimator in a 1-bit compressed sensing setting and, interestingly, the concept of Gaussian width shows up in the analysis. In spite of the advances, with a few notable exceptions [40, 42], most existing results are restricted to isotropic Gaussian design matrices. Further, while a suitable scale for λn is known for special cases such as the L1, a general analysis applicable to any normR(·) has not been explored in the literature.\nIn this paper, we consider structured estimation problems with norm regularization of the form (1), and present a unified analysis which substantially generalizes existing results on all four pertinent aspects: the norm, the design matrix, the loss, and the noise model. The analysis we present applies to all norms, and the results can be divided into three groups: characterization of the error set and recovery guarantees, characterization of the regularization parameter λn, and characterization of the restricted eigenvalue conditions or restricted strong convexity. We provide a summary of the key results below.\nRestricted error set: We start with a characterization of the error set Er in which the error vector ∆̂n belongs. For a suitably large λn, we show that ∆̂n belongs to the restricted error set\nEr =\n{ ∆ ∈ Rp ∣∣∣∣ R(θ∗ + ∆) ≤ R(θ∗) + 1βR(∆) } , (2)\nwhere β > 1 is a constant. The restricted error set has interesting structure, and forms the basis of subsequent analysis for bounds on ‖∆̂n‖2. Regularized vs. constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(θ) under suitable constraints determined by the noise (y − Xθ) and/or the design matrix X [13, 6, 14, 15]. A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15], which generalizes the Dantzig selector [11] corresponding to the L1 norm, and is given by:\nθ̂γn = argmin θ∈Rp\nR(θ) s.t. R∗(XT (y −Xθ∗)) ≤ γn , (3)\nwhere R∗(·) denotes the dual norm of R(·). One can show [14, 15] that the restricted error set for such constrained estimators are of the form:\nEc = {∆ ∈ Rp | R(θ∗ + ∆) ≤ R(θ∗)} . (4)\nOne can readily see that Er is larger than Ec, i.e., Ec ⊆ Er, and Er approaches Ec as β increases. We establish a geometric relationship between the two sets, which will possibly help in transforming analysis done on regularized estimators as in (1) to corresponding constrained estimators as in (3) and vice versa. Let Bp2 be a L2 ball of radius 1 in Rp. Then, with Ar = Er ∩ B p 2 , Ac = Ec ∩ B p 2 , and Āc = cone(Ec) ∩ B p 2 , assuming ‖θ∗‖2 = 1, β = 2, we show that\nw(Ac) ≤ w(Ar) ≤ 3w(Āc) , (5)\nwhere w(A) = Eg[supa∈A〈a, g〉], with g ∼ N(0, Ip×p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37]. Note that Ar corresponds to the spherical cap of the error set Er, and Ac corresponds to the spherical cap of the error cone cone(Ec) at the unit ball. Interestingly, the above relationship between the widths of these spherical caps is geometric, and applies for any norm R(·). We establish a more general version of the above relationship. Let ρBp2 denote a L2 ball of any radius ρ in Rp. Then, with A(ρ)r = Er ∩ ρBp2 , A (ρ) c = Ec ∩ ρBp2 , and Ā (ρ) c = cone(Ec) ∩ ρBp2 , we show that\nw(A(ρ)c ) ≤ w(A(ρ)r ) ≤ ( 1 + 2 β − 1 ‖θ∗‖2 ρ ) w(Ā(ρ)c ) . (6)\nAs before, except for the scaling constants, the relationship between the restricted error sets is geometric, and does not change based on the choice of the norm R(·). For the special case of L1 norm, [6] considered a simultaneous analysis of the Lasso and the Dantzig selector, and characterized the structure of the error sets for regularized and constrained sets for the special case of L1 norm. Further, while the characterization in [6] was also geometric, it was not based on Gaussian widths. In contrast, our results apply to any norm, not just L1, and the geometric characterization is based on Gaussian widths. The utility of the Gaussian width based characterization becomes evident later when we establish sample complexity results for Gaussian and sub-Gaussian random matrices in terms of Gaussian widths of spherical caps.\nBounds on estimation error: We establish bounds on the estimation error ∆̂n under two assumptions, which are subsequently shown to hold with high probability for sub-Gaussian designs and noise models. The first assumption is that the regularization parameter λn is suitably large. In particular, for any β > 1, the regularization parameter λn needs to satisfy\nλn ≥ βR∗(∇L(θ∗;Zn)) , (7)\nwhere R∗(·) denotes the dual norm of R(·). The second assumption is that the design matrix X ∈ Rn×p satisfies the restricted strong convexity (RSC) condition [6, 29] in the error set Er, in particular, there exists a suitable constant κ > 0 so that\nδL(∆, θ∗) , L(θ∗ + ∆)− L(θ∗)− 〈∇L(θ∗),∆〉 ≥ κ‖∆‖22 ∀∆ ∈ Er . (8)\nWith such suitably large λn and L satisfying the RSC condition, we establish the following bound:\n‖∆̂n‖2 ≤ cψ(Er) λn κ , (9)\nwhere ψ(Er) = supu∈Er R(u) ‖u‖2 is a norm compatibility constant [29], and c > 0 is a constant. Note that the above bound is deterministic, but relies on assumptions on λn and κ. So, we focus on characterizations of λn and κ which hold with high probability for sub-Gaussian design matrices X and sub-Gaussian noise ω. Recent work in [36] has extended the analyses for sub-exponential distributions.\n1A gentle exposition to Gaussian width and some of its properties is given in Appendix A.\nBounds on the regularization parameter λn: From (7) above, for the analysis to work, one needs to have λn ≥ βR∗(∇L(θ∗;Zn)). There are a few challenges in getting a suitable bound for λn. First, the bound depends on θ∗, but θ∗ is unknown and is the quantity one is interested in estimating. Second, the bound depends on Zn, the samples, and is hence random. The goal will be to bound the expectation E[R∗(∇L(θ∗;Zn))] over all samples of size n, and obtain high-probability deviation bounds. Third, since the bound relies on the (dual) norm R∗(·) of a p-dimensional random vector, without proper care, the lower bound on λn may end up having a large scaling dependency, say √ p, on the ambient dimensionality. Since the error bound in (9) is directly proportional to λn, such dependencies will lead to weak bounds. In Section 3, we characterize the expectationE[R∗(∇L(θ∗;Zn))] in terms of the geometry of the unit normball of R, which leads to a sharp bound. Let ΩR = {u ∈ Rp|R(u) ≤ 1} denote the unit norm-ball. Then, for sub-Gaussian design matrices and squared loss, we show that\nE[R∗(∇L(θ∗;Zn))] ≤ c√ n w(ΩR) , (10)\nwhich scales as the Gaussian width of ΩR. Interestingly, for sub-Gaussian designs, one obtains the results in terms of the ‘sub-Gaussian width’ of the unit norm-ball, which can be upper bounded by a constant times the Gaussian width using generic chaining [37]. The result can be extended to the case of anisotropic subGaussian designs, where the constant c starts depending on the maximum eigenvalue (operator norm) of the corresponding covariance matrix. Further, one can get high-probability versions of these bounds using related advances in generic chaining [37, 38]. The results can also be extended to general convex losses, such as those from generalized linear models.\nThe above characterization allows one to choose λn ≥ c√nw(ΩR). For the special case of L1 regularization, ΩR is the unit L1 norm ball, and the corresponding Gaussian width w(ΩR) ≤ c1 √ log p, which explains the√\nlog p term one finds in existing bounds for Lasso [29, 9]. When working with other norms, one simply needs to get an upper bound on the corresponding w(ΩR).\nRestricted eigenvalue conditions: When the loss function under consideration is the squared loss, the RSC condition in (8) reduces to the restricted eigenvalue (RE) condition on the design matrix. Our analysis focuses on establishing the RE condition on Ār = cone(Er) ∩ Sp−1, the spherical cap obtained by intersecting the cone of the error set with the unit hypersphere, since it implies the RE condition on Er. For isotropic sub-Gaussian design matrices, a stronger two-sided restricted isometry property (RIP) holds, i.e., with high probability, for any A ⊆ Sp−1, we have\n1− cw(A)√ n ≤ inf u∈A 1 n ‖Xu‖2 ≤ sup\nu∈A\n1 n ‖Xu‖2 ≤ 1 + cw(A)√ n (11)\nwhere w(A) is the Gaussian width of A. Thus, for say n0 = 4c2w2(Ār), and for n > n0, an RE condition of the form\ninf u∈Ār\n1 n ‖Xu‖22 ≥ 1/2 , (12)\nis satisfied with high probability. Instead of the constant to be 1/2, one can have any constant less than 1, with suitable increase in n0. Thus, one does not need to treat the RE condition as an assumption for isotropic subGaussian designs—it always holds with high probability, with the phase transition happening at O(w2(Ār)) samples. The RIP results can be generalized to anisotropic sub-Gaussian designs, where additional constants depending on the restricted eigenvalues of the anisotropic covariance matrix Σ show up, but the form of the bound stays similar. Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].\nGeneralized linear models and restricted strong convexity: For convex loss functions, such as those coming from generalized linear models (GLMs), the sample complexity and associated phase transition behavior\nis determined by the Restricted Strong Convexity (RSC) condition [29]. By generalizing our argument for RE conditions corresponding to square loss, we show that the RSC conditions are going to be satisfied for convex losses for subGaussian designs at the same order of sample complexity as that for squared loss. In particular, for we show a high probability lower bound of the form\ninf u∈A δL(u, θ∗) ≥ c1 − c2 w(A)√ n , (13)\nwhere the constants c1, c2 > 0 depend on the tail probabilities of the design matrix distribution. Specializing the result to Ār = cone(Er) ∩ Sp−1, we note that the sample complexity still scales as O(w2(Ār)), similar to the case of squared loss. The result is thus a considerable generalization of earlier results on convex losses, such as GLMs, which had looked at specific norms and associated cones and/or did not express the results in terms of the Gaussian width of A [29].\nPutting everything together: With the above results in place, from (9), the main bound takes the form\n‖∆̂n‖2 ≤ c ψ(Er)[ c1 − c2w(Ār)√n ]\n+\nw(ΩR)√ n\n(14)\nwith high probability, wherew(ΩR) is the Gaussian width of the unit norm ball, w(Ār) is the Gaussian width of the spherical cap corresponding to the error set cone(Er), and the result is valid only when n > n0 = O(w2(Ār)) which corresponds to the sample complexity. For the special case of L1 norm, i.e., Lasso, the sample complexity n0 is of the order w2(Ār) = O(s log p). Further, w(ΩR) = √ log p and ψ(Er) = √ s.\nPlugging in these values, choosing β = 2, for n > c3s log p, the bound ‖∆̂n‖2 ≤ c √ s log p n holds with probability. For other norms, one can simply plug-in the widths to get the corresponding sample complexity and non-asymptotic error bounds.\nThe rest of the paper is organized as follows: Section 2 presents results on the restricted error set and deterministic error bounds under suitable bounds on the regularization parameter λn and RSC assumptions. Section 3 presents a characterization of λn in terms of the Gaussian width of the unit norm ball for Gaussian as well as sub-Gaussian designs and noise. Section 4 proves RE conditions and associated sample complexity results corresponding to squared loss functions. Results are presented for subGaussian designs, including anisotropic and correlated cases, and always in terms of the Gaussian width of the spherical cap corresponding to the error set. Section 6 presents RSC conditions corresponding to general convex losses arising from generalized linear models, and the results are again in terms of the Gaussian width of the spherical cap corresponding to the error set. We conclude in Section 7. All technical arguments and proofs are in the appendix, along with a gentle exposition to Gaussian widths and related results.\nA brief word on the notation used. We denote random matrices as X , and random vectors as Xi where i may be an index to a row or column of a random matrix. Vector norms are denoted as ‖ · ‖, e.g., ‖Xi‖2 for a (random) vector Xi, and norms of random variables are denoted as |||·|||, e.g., |||X|||2 = E[‖X‖2]."
    }, {
      "heading" : "2 Restricted Error Set and Recovery Guarantees",
      "text" : "In this section, we give a characterization of the restricted error setEr in which the error vector ∆̂n = (θ̂λn− θ∗) lies, establish clear relationships between the error sets for the regularized and constrained problems, and finally establish upper bounds on the estimation error. The error bound is deterministic, but has quantities which involve θ∗, X, ω, for which we develop high probability bounds in Sections 3, 4, and 6."
    }, {
      "heading" : "2.1 The Restricted Error Set and the Error Cone",
      "text" : "We start with a characterization of the restricted error set Er where ∆̂n will belong.\nLemma 1 For any β > 1, assuming\nλn ≥ βR∗(∇L(θ∗;Zn)) , (15)\nwhere R∗(·) is the dual norm of R(·). Then the error vector ∆̂n = θ̂λn − θ∗ belongs to the set\nEr = Er(θ ∗, β) = { ∆ ∈ Rp ∣∣∣∣ R(θ∗ + ∆) ≤ R(θ∗) + 1βR(∆) } . (16)\nThe restricted error set Er need not be convex for general norms. Interestingly, for β = 1, the inequality in (16) is just the triangle inequality, and is satisfied by all ∆. Note that β > 1 restricts the set of ∆ which satisfy the inequality, yielding the restricted error set. In particular, ∆ cannot go in the direction of θ∗, i.e., ∆ 6= αθ∗ for any α > 0. Further, note that the condition in (15) is similar to that in [29] for β = 2, but the above characterization holds for any norm, not just decomposable norms [29].\nWhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15]. A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15] given by:\nθ̂γn = argmin θ∈Rp\nR(θ) s.t. R∗(XT (y −Xθ∗)) ≤ γn , (17)\nwhere R∗(·) denotes the dual norm of R(·). One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form:\nEc = {∆ ∈ Rp | R(θ∗ + ∆) ≤ R(θ∗)} . (18)\nBy definition, it is easy to see that Ec is always convex, and that Ec ⊆ Er, as shown schematically in Figure 1.\nThe following results establishes a relationship between Er and Ec in terms of their Gaussian widths.\nTheorem 1 Let A(ρ)c = Ec ∩ ρBp2 , A (ρ) r = Er ∩ ρBp2 , and Ā (ρ) c = Cc ∩ ρBp2 , where ρB p 2 = {u|‖u‖2 ≤ ρ} is the L2 ball of any radius ρ > 0. Then, for any β > 1 we have\nw(A(ρ)c ) ≤ w(A(ρ)r ) ≤ ( 1 + 2 β − 1 ‖θ∗‖2 ρ ) w(Ā(ρ)c ) , (19)\nwhere w(A) denotes the Gaussian width of any set A given by: w(A) = Eg [ sup a∈A 〈a, g〉 ] , where g is an isotropic Gaussian random vector, i.e., g ∼ N(0, Ip×p).\nThus, the Gaussian width of the error sets of regularized and constrained problems are closely related. See Figure 1 for more details. In particular, for ‖θ∗‖2 = 1, with ρ = 1, β = 2, we have w(Ac) ≤ w(Ar) ≤ 3w(Āc) as introduced in Section 1. Related observations have been made for the special case of the L1 norm [6], although past work did not provide an explicit characterization in terms of Gaussian widths. The result also suggests that it is possible to move between the error analysis of the regularized and the constrained versions of the estimation problem."
    }, {
      "heading" : "2.2 Recovery Guarantees",
      "text" : "In order to establish recovery guarantees, we start by assuming that restricted strong convexity (RSC) is satisfied by the loss function in Er, the error set, so that for any ∆ ∈ Er, there exists a suitable constant κ so that\nδL(∆, θ∗) , L(θ∗ + ∆)− L(θ∗)− 〈∇L(θ∗),∆〉 ≥ κ‖∆‖22 . (20)\nIn Sections 4 and 6, we establish precise forms of the RSC condition for a wide variety of design matrices and loss functions. In order to establish recovery guarantees, we focus on the quantity\nF(∆) = L(θ∗ + ∆)− L(θ∗) + λn(R(θ∗ + ∆)−R(θ∗)) . (21)\nSince θ̂λn = θ ∗ + ∆̂n is the estimated parameter, i.e., θ̂λn is the minimum of the objective, we clearly have F(∆̂n) ≤ 0, which implies a bound on ‖∆̂n‖2. Unlike previous analysis, the bound can be established without making any additional assumptions on the norm R(θ). We start with the following result, which expresses the upper bound on ‖∆̂n‖2 in terms of the gradient of the objective at θ∗.\nLemma 2 Assume that the RSC condition is satisfied in Er by the loss L(·) with parameter κ. With ∆̂n = θ̂λn − θ∗, for any norm R(·), we have\n‖∆̂n‖2 ≤ 1 κ ‖∇L(θ∗) + λn∇R(θ∗)‖2 , (22)\nwhere∇R(·) is any sub-gradient of the norm R(·).\nFigure 3 illustrates the above results. Note that the right hand side is simply the L2 norm of the gradient of the objective evaluated at θ∗. For the special case when θ̂λn = θ\n∗, the gradient of the objective is zero, implying correctly that ‖∆̂n‖2 = 0. While the above result provides useful insights about the bound on ‖∆̂n‖2, the quantities on the right hand side depend on θ∗, which is unknown. We present another form of the result in terms of quantities such as λn, κ, and the norm compatibility constant ψ(Er) = supu∈Er R(u) ‖u‖2 , which are often easier to compute or bound.\nTheorem 2 Assume that the RSC condition is satisfied in Er by the loss L(·) with parameter κ. With ∆̂n = θ̂λn − θ∗, for any norm R(·), we have\n‖∆̂n‖2 ≤ ψ(Er) 1 + β\nβ λn κ . (23)\nThe above result is deterministic, but contains λn and κ. In Section 3, we give precise characterizations of λn, which needs to satisfy (15). In Sections 4 and 6, we characterize the RSC condition constant κ for different losses and a variety of design matrices."
    }, {
      "heading" : "2.3 A Special Case: Decomposable Norms",
      "text" : "In recent work, [29] considered regularized regression with the special case of decomposable norms, defined in terms of a pair of subspacesM ⊆ M̄ of Rp. The model is assumed to be in the subspaceM, and the\ndefinition considers the so-called perturbation subspace M̄⊥ which is the orthogonal complement of M̄. A norm R(·) is considered decomposable with respect to subspaces (M,M̄⊥) if R(θ+γ) = R(θ) +R(γ) for all θ ∈M and γ ∈ M̄⊥. We show that for decomposable norms, the error set Er in our analysis is included in the error cone defined in [29]. In the current context, let β = 2, θ∗ ∈M, then for any ∆ = ∆M̄⊥ + ∆M̄ ∈ Er, we have\nR(θ∗ + ∆) ≤ R(θ∗) + 1 2 R(∆) (24)\n⇒ R(θ∗ + ∆M̄⊥ + ∆M̄) ≤ R(θ∗) + 1\n2 R(∆M̄⊥ + ∆M̄) (25)\n⇒ R(θ∗ + ∆M̄⊥)−R(∆M̄) (a) ≤ R(θ∗) + 1 2 R(∆M̄⊥) + 1 2 R(∆M̄) (26) ⇒ R(θ∗) +R(∆M̄⊥)−R(∆M̄) (b)\n≤ R(θ∗) + 1 2 R(∆M̄⊥) + 1 2 R(∆M̄) (27)\n⇒ R(∆M̄⊥) ≤ 3R(∆M̄). (28)\nwhere inequality (a) follows from the triangle inequality and (b) follows from decomposability of the norm. The last inequality is precisely the error cone in [29] for θ∗ ∈ M. As a result, for any ∆ ∈ Er, for decomposable norms we have\nR(∆) = R(∆M̄⊥ + ∆M̄) ≤ R(∆M̄⊥) +R(∆M̄) ≤ 4R(∆M̄) (29)\nHence, the norm compatibility constant can be bounded as\nψ(Er) = sup ∆∈Er\nR(∆) ‖∆‖2 ≤ 4 sup ∆∈Er R(∆M̄) ‖∆‖2 ≤ 4 sup u∈M̄\\{0} R(u) ‖u‖2 = 4Ψ(M̄). (30)\nwhere Ψ(M̄) is the subspace compatibility in M̄, as used in [29]."
    }, {
      "heading" : "3 Bounds on the Regularization Parameter",
      "text" : "Recall that the parameter λn needs to satisfy the inequality\nλn ≥ βR∗(∇L(θ∗;Zn)) . (31)\nThe right hand side of the inequality has two issues: the expression depends on θ∗, the optimal parameter which is unknown, and the expression is a random variable, since it depends on Zn. In this section, we characterize the expectation E[R∗(∇L(θ∗;Zn))] in terms of the Gaussian width of the unit norm ball ΩR = {u : R(u) ≤ 1}, and further discuss its upper bounds. For ease of exposition, we present results for the case of squared loss, i.e., L(θ∗;Zn) = 12n‖y−Xθ\n∗‖2 with the linear model y = Xθ+ω, where ω is noise vector with i.i.d. entries. Under this setting,\n∇L(θ∗;Zn) = 1 n XT (y −Xθ∗) = 1 n XTω , (32)\nwhich eliminates the dependency on the unknown θ∗. Before presenting the results, we introduce a few notations. We let Λmax(·) denote the largest eigenvalue of a square matrix. We also recall the definition of the sub-Gaussian norm for a sub-Gaussian variable x, |||x|||ψ2 = supp≥1 1√ p(E[|x| p])1/p [41]. From this section onwards, the analysis will take into account the randomness of the design X and the noise ω. Here we give a brief description of our assumptions on X and ω as follows,\nIsotropic Sub-Gaussian Designs: the design matrix X ∈ Rn×p has independent sub-Gaussian rows where each row satisfies |||Xi|||ψ2 ≤ κ and E[XiX T i ] = Ip×p. Thus, the measure µ from which the rows Xi are sampled independently is an isotropic sub-Gaussian measure. Anisotropic Sub-Gaussian Designs: the design matrix X ∈ Rn×p has independent rows, and each row Xi is anisotropic sub-Gaussian with E[XTi Xi] = Σ. Further, we assume that corresponding isotropic random\nvector X̃i = XiΣ−1/2 satisfies ∣∣∣∣∣∣∣∣∣X̃i∣∣∣∣∣∣∣∣∣\nψ2 ≤ κ. A simple special case of such an anisotropic sub-Gaussian design is when Xi ∼ N(0,Σ), where X̃i = XiΣ−1/2 ∼ N(0, I) so that ∣∣∣∣∣∣∣∣∣X̃i∣∣∣∣∣∣∣∣∣\nψ2 = 1.\nSub-Gaussian Noise: the noise ω has i.i.d. centered unit-variance sub-Gaussian entries with |||ωi|||ψ2 ≤ K. For convenience, we only use the shorthand in bold font to specify the assumptions. In the following theorem, we characterize the expectation ofR∗(∇L(θ∗;Zn)) in terms of Gaussian width of the unit norm ballw(ΩR).\nTheorem 3 Let ΩR = {u : R(u) ≤ 1}, and L be the squared loss. For sub-Gaussian design X and noise ω, we have\nE [R∗(∇L(θ∗;Zn))] ≤ ηξ · κw(ΩR)√ n , (33)\nwhere the expectation is taken over both X and ω. The constant ξ is given by\nξ =\n{ 1 if X is isotropic√\nΛmax(Σ) if X is anisotropic .\nBounding the expectation of R∗(∇L(θ∗;Zn)) gives us a rough scale of the regularization parameter λn. In the next theorem, we present a high-probability upper bound for R∗(∇L(θ∗;Zn)).\nTheorem 4 Let designX and noise ω be sub-Gaussian, andL be squared loss. Define φ = supR(u)≤1 ‖u‖2, then for any τ > 0, with probability at least 1− c1 exp ( −min ( ( τc2ξκφ) 2, c0n )) , we have\nR∗ (∇L(θ∗;Zn)) ≤ √ 2K2 + 1\nn (cξκ · w(ΩR) + τ) , (34)\nwhere c, c0, c1 and c2 are all absolute constants, and ξ is the same as in Theorem 3.\nBounding the Gaussian width w(ΩR): In certain cases, one may be able to directly obtain a bound on the Gaussian width w(ΩR). Here, we provide a mechanism for bounding the Gaussian width w(ΩR) of the unit norm ball in terms of the Gaussian width of a suitable cone, obtained by shifting or translating the norm ball. In particular, the result involves taking any point on the boundary of the unit norm ball, considering that as the origin, and constructing a cone using the norm ball. Since such a construction can be done with any point on the boundary, the tightest bound is obtained by taking the infimum over all points on the boundary. The motivation behind getting an upper bound of the Gaussian width w(ΩR) of the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances have been made in recent years in upper bounding Gaussian widths of such cones [14, 1].\nLemma 3 Let ΩR = {u : R(u) ≤ 1} be the unit norm ball and ΘR = {u : R(u) = 1} be the boundary. For any θ̃ ∈ ΘR, ρ(θ̃) = supθ:R(θ)≤1 ‖θ − θ̃‖2 is the diameter of ΩR measured with respect to θ̃. Let G(θ̃) = cone(ΩR − θ̃) ∩ ρ(θ̃)Bp2 , i.e., the cone of (ΩR − θ̃) intersecting the ball of radius ρ(θ̃). Then\nw(ΩR) ≤ inf θ̃∈ΘR w(G(θ̃)) . (35)\nThe analysis and results for λn presented above can be extended to general convex losses arising in the context of GLMs for sub-Gussian designs and sub-Gaussian noise (see Section 6). ."
    }, {
      "heading" : "4 Least Squares Models: Restricted Eigenvalue Conditions",
      "text" : "The error bound analysis in Theorem 2 depends on the restricted strong convexity (RSC) assumption. In this section, we establish RSC conditions for sub-Gaussian design matrices when the loss function is the squared loss. For squared loss, i.e., L(θ;Zn) = 1n‖y − Xθ‖\n2, the RSC condition (20) becomes equivalent to the Restricted Eigenvalue (RE) condition [6, 29], since\nδL(∆, θ∗) = 1 n ‖y −X(θ∗ + ∆)‖2 − 1 n ‖y −Xθ∗‖2 + 1 n 〈XT (y −Xθ∗),∆〉\n= 1 n ‖X∆‖2 = 1 n n∑ i=1 〈Xi,∆〉2 . (36)\nso that the condition simplifies to\nδL(∆, θ∗) = 1 n n∑ i=1 〈Xi,∆〉2 ≥ κ‖∆‖22 , (37)\nfor all ∆ ∈ Er. We make two simplifications which lets us develop the RE results in terms of widths of spherical caps rather than over the error set Er. Let nEr be the sample complexity for the RE condition over the set Er, so that for n > nEr samples, with high probability\ninf ∆∈Er\n1 n ‖X∆‖22 ≥ κEr‖∆‖22 , (38)\nfor some κEr > 0. Let Cr = cone(Er) and let nCr be the sample complexity for the RE condition over the cone Cr, so that for n > nCr samples, with high probability\ninf ∆∈Cr\n1 n ‖X∆‖22 ≥ κCr‖∆‖22 , (39)\nfor some κCr > 0. Since Er ⊆ Cr, we have nEr ≤ nCr . Thus, it is sufficient to obtain (an upper bound to) the sample complexity nCr , since that will serve as an upper bound to nEr , the sample complexity over Er. Further, since Cr is a cone, the absolute magnitude ‖∆‖2 does not affect the sample complexity. As a result, it is sufficient to focus on a spherical cap A = Cr ∩Sp−1. In particular, if nA denotes the sample complexity for the RE condition over the spherical cap A, so that for n > nA samples, with high probability\ninf u∈A\n1 n ‖Xu‖22 ≥ κ̄A‖u‖22 , (40)\nfor some κ̄A > 0, then nA = nCr ≥ nEr . Noting that ‖u‖2 = 1 for u ∈ Cr ∩ Sp−1, we consider sample complexity results for RE conditions the form\ninf u∈A\n1 n ‖Xu‖22 = inf u∈A 1 n n∑ i=1 〈Xi, u〉2 ≥ κA(n, p) (41)\nwhere κA(n, p) > 0 with high probability for n > nA. In this section, we characterize sample complexity nA over any given spherical cap A, and establish RE conditions for isotropic and anisotropic sub-Gaussian design matrices X in terms of the Gaussian width w(A).\nAnalysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34]. The RE/RIP conditions for independent isotropic Gaussian designs have been widely studied for the case of L1 norm [10, 6]. The generalization to RE condition for correlated Gaussian designs for the special of L1 norm was studied in [33]. [14] consider the more general context of atomic norms, and RE condition analysis applies to any spherical cap A, with sample complexity results in terms of w(A), the Gaussian width of A. However, the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices. Progress has been made on establishing RE conditions for sub-Gaussian designs for error sets/caps corresponding to specific norms such as L1 [46]. In recent work, RE conditions were developed for anisotropic sub-Gaussian designs for the L1 norm [34]. Further, recent work have pointed out the differences between the RE and the RIP condition, which gives a two-sided bound on quadratic forms of random matrices [29]. In particular, while the RE condition is sufficient for structured estimation, the RIP results are stronger and may have higher sample complexity.\nIn the following, we establish the stronger RIP results for any spherical cap A and any sub-Gaussian design matrix, handling the isotropic and anisotropic cases separately. The special case of Gaussian design matrices are automatically covered by the sub-Gaussian results, and results such as Gordon’s inequality can be viewed as a special case. All results are in terms of w(A), the Gaussian width of A, even for sub-Gaussian designs. In fact, all existing RE results do implicitly have the width term, but in a form specific to the chosen norm [33, 34]. The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.\nThe proof technique we use is an application of generic chaining [37, 38]. The specific form we utilize was originally developed in [21, 28]. The main idea is to pose the RIP condition as a bound on the supremum of a suitable stochastic process, so that generic chaining can be invoked to obtain a bound. The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use\nof generic chaining which simplifies the analysis considerably. Further, the RIP results can be viewed as a generalization of the celebrated Johnson-Lindenstrauss (JL) lemma [17], and the interested reader can explore these connections in [21]. Isotropic Sub-Gaussian Designs: We first consider the case where the design matrix X ∈ Rn×p has independent sub-Gaussian rows where each row satisfies |||Xi|||ψ2 ≤ κ and E[XiX T i ] = Ip×p. Thus, the measure µ from which the rows Xi are sampled independently is an isotropic sub-Gaussian measure.\nTheorem 5 Let X be a design matrix with independent isotropic sub-Gaussian rows, i.e., |||Xi|||ψ2 ≤ κ and E[XiX T i ] = Ip×p. Then, for absolute constants η, c > 0, with probability at least (1 − 2 exp(−ηw2(A))), we have\nsup u∈A ∣∣∣∣ 1n ||Xu||2 − 1 ∣∣∣∣ = sup u∈A ∣∣∣∣∣ 1n n∑ i=1 〈Xi, u〉2 − 1 ∣∣∣∣∣ ≤ cw(A)√n , (42) or, equivalently,\n1− cw(A)√ n ≤ inf u∈A 1 n ||Xu||2 ≤ sup\nu∈A\n1 n ||Xu||2 ≤ 1 + cw(A)√ n . (43)\nAs a result, for n > c2w2(A), the RE condition: infu∈A ‖Xu‖2 ≥ 1 − cw(A)/ √ n > 0 is satisfied with high probability for any sub-Gaussian design matrix. More generally, choosing = cw(A)/ √ n, one can write the result in a traditional RIP form [10]. Anisotropic Sub-Gaussian Designs: We now consider the case where the design matrix X ∈ Rn×p has independent rows, and each row Xi is anisotropic sub-Gaussian with E[XTi Xi] = Σ. Further, we assume\nthat corresponding isotropic random vector X̃i = XiΣ−1/2 satisfies ∣∣∣∣∣∣∣∣∣X̃i∣∣∣∣∣∣∣∣∣\nψ2 ≤ κ. A simple special case\nof such an anisotropic sub-Gaussian design is when Xi ∼ N(0,Σ), where X̃i = XiΣ−1/2 ∼ N(0, I) so that ∣∣∣∣∣∣∣∣∣X̃i∣∣∣∣∣∣∣∣∣ ψ2 = 1. The result below characterizes RIP-style property of any such anisotropic sub-Gaussian designs.\nTheorem 6 Let X be a design matrix with independent anisotropic sub-Gaussian rows, i.e., E[XTi Xi] = Σ and ∣∣∣∣∣∣XiΣ−1/2∣∣∣∣∣∣ψ2 ≤ κ. Then, for absolute constants η, c > 0, with probability at least (1−2 exp(−ηw2(A))), we have\nsup u∈A ∣∣∣∣ 1n 1uTΣu ||Xu||2 − 1 ∣∣∣∣ = sup u∈A ∣∣∣∣∣ 1n 1uTΣu n∑ i=1 〈Xi, u〉2 − 1 ∣∣∣∣∣ ≤ cw(A)√n . (44) Further,\nλmin(Σ|A) (\n1− cw(A)√ n\n) ≤ inf\nu∈A\n1 n ||Xu||2 ≤ sup\nu∈A\n1 n ||Xu||2 ≤ λmax(Σ|A)\n( 1 + c\nw(A)√ n\n) ,\n(45) where\nλmin(Σ|A) = inf u∈A uTΣu , and λmax(Σ|A) = sup u∈A uTΣu (46)\nare the restricted minimum and maximum eigenvalues of Σ restricted to A ⊆ Sp−1.\nThus, for the anisotropic case, the RIP is with respect to the restricted minimum and maximum eigenvalues corresponding to A ⊆ Sp−1. For the special case when A = Sp−1, we have λmin(Σ|A) = λmin(Σ), the minimum eigenvalue, and λmax(Σ|A) = λmax(Σ), the maximum eigenvalue of Σ. Further, when Σ = I,\nwe get back the result in Theorem 11. Finally, it is instructive to compare the above result to existing characterizations of the RE condition for anisotropic Gaussian [33] and anisotropic sub-Gaussian [34] designs, focused on the L1 norm. The result in Theorem 12 is a more general RIP result, applies to any spherical cap A, and is in terms of w(A), the Gaussian width of the spherical cap A."
    }, {
      "heading" : "5 Examples and Applications",
      "text" : "In this section, we give examples of the analysis from previous sections for three norms: L1 norm, group sparse norm, andL2 norm. The summary of the results is given in Table 1. Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16]. L1 Norm: Assume that the statistical parameter θ∗ is s-sparse, and note that ‖θ∗‖1 ≤ √ s‖θ∗‖2. Since L1 norm is a decomposable norm following the result in (28), we have Ψ(Er) ≤ 4Ψ(M̄) = 4 √ s.\nApplying Lemma 3, let θ̄ be a 1-sparse vector, and ρ(θ̄) = 2, then w(ΩR) can be bounded by\nw(ΩR) ≤ inf θ̃∈ΘR\nw(G(θ̃)) = w(G(θ̄)) (a) = O (√ log p ) , (47)\nwhere (a) is obtained from the fact that Gaussian width ofG(θ̃) with θ̃ be a s-sparse vector is √\n2s log(ps ) + 5 4s\n[14]. See Figure 4 for more details. From Theorem 4 and (47), the bound on λn is\nλn ≤ c w(ΩR)√\nn = O\n(√ log p\nn\n) . (48)\nHence, the recovery error is bounded by\n‖∆̂n‖2 ≤ c3 Ψ(Er)λn\nκ = O\n(√ s log p\nn\n) , (49)\nwhich is similar to the results obtained in well known results [14, 29].\nGroup Sparse Norm: Suppose that the index set {1, 2, · · · , p} can be partitioned into a set of T disjoint groups, say G = {G1,G2, · · · ,GT }. Define (1, ν)-group norm for a given vector ν = (ν1, · · · , νT ) ∈ [1,∞]T as\n‖α‖G,ν = T∑ t=1 ‖αGt‖νt (50)\nAs shown in [29] Group norm is a decomposable norm. For a given subset SG ⊂ {1, . . . , T} with cardinality |SG |, define the subspace A(SG) = {α ∈ Rp |αGt = 0, ∀t /∈ SG }. Let νt ≥ 2, then we have\n‖∆‖G,ν = ∑ t∈SG ‖∆Gt‖νt ≤ ∑ t∈SG ‖∆Gt‖2 ≤ √ sG‖∆‖2. (51)\nHence, from (30) and (51) we have Ψ(Er) ≤ 4 √ sG . (52)\nApplying Lemma 3, define θ̄ with 1-active group, and ρ(θ̄) = 2, then w(ΩR) can be bounded by\nw(ΩR) ≤ inf θ̃∈ΘR\nw(G(θ̃)) = w(G(θ̄)) (a) = O (√ m+ log T ) , (53)\nwhere m = max t |Gt| and (a) is obtained from the fact that Gaussian width of G(θ̃) where θ̃ has k active group is √ 2k(m+ log(T − k)) + k [14]. From Theorem 4 and (53), the bound on λn is\nλn ≤ c w(ΩR)√\nn = O\n(√ m+ log T\nn\n) . (54)\nHence, the recovery error is bounded by\n‖∆̂n‖2 ≤ c3 Ψ(Er)λn\nκ = O\n(√ sG(m+ log T )\nn\n) , (55)\nwhich is similar to the results obtained in previous works [14, 29].\nL2 Norm: With L2 norm as the regularizer, the norm constant is obtained as\nΨ(Er) = sup ∆∈Er ‖∆‖2 ‖∆‖2 = 1. (56)\nApplying Lemma 3, set ρ(θ̃) = 1, then w(ΩR) can be bounded by\nw(ΩR) ≤ inf θ̃∈ΘR\nw(G(θ̃)) = O ( √ p) . (57)\nFrom Theorem 4 and (57), the bound on λn is\nλn ≤ c w(ΩR)√\nn = O\n(√ p\nn\n) . (58)\nHence, the recovery error is bounded by\n‖∆̂n‖2 ≤ c3 Ψ(Er)λn\nκ = O\n(√ p\nn\n) . (59)"
    }, {
      "heading" : "6 Generalized Linear Models: Restricted Strong Convexity",
      "text" : "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8]. Assume that the conditional distribution of the response yi conditioned on the covariates Xi is an exponential family distribution:\np(yi|Xi; θ∗) = p(yi|〈Xi, θ∗〉) = exp{yi〈Xi, θ∗〉 − ϕ(〈Xi, θ∗〉)} . (60)\nwhere\nϕ(〈Xi, θ∗〉) = log (∫\nyi\nexp{yi〈Xi, θ∗〉} dyi )\n(61)\nis the log-partition function [8, 4, 44].2 In GLMs, the conditional distribution of the response yi is characterized by an exponential family distribution p(yi|ηi) with natural parameter ηi = 〈Xi, θ∗〉 determined by the covariatesXi and the parameter θ∗. It is easy to verify that the gradient of the log-partition function w.r.t. the natural parameter ηi = 〈Xi, θ∗〉 gives the expectation of the response [4, 8], i.e.,\n∇ηiϕ(ηi) = ∇〈Xi,θ∗〉ϕ(〈Xi, θ ∗〉) = E[yi|〈Xi, θ∗〉] . (62)\nFor estimating θ∗, the loss function corresponding to GLMs typically consider the negative log likelihood of the conditional distribution:\nL(θ;Zn) = − 1 n log p(yi|Xi; θ∗) = 1 n n∑ i=1 (ϕ(〈Xi, θ〉)− yi〈Xi, θ〉) . (63)\nIn the current context, we assume θ∗ to be sparse/structured, and the structure can be suitably captured by a norm R(·). Then, the estimation of θ∗ with norm regularization takes the form:\nθ̂λn = argmin θ∈Rp L(θ;Zn) + λnR(θ) = argmin θ∈Rp\n1\nn n∑ i=1 (ϕ(〈Xi, θ〉)− yi〈Xi, θ〉) + λnR(θ) . (64)\nNoise in the context of GLMs is simply the deviation of a specific response yi from the conditional mean, i.e., ωi = E[yi|Xi] − yi. Popular examples of GLMs come from suitable choices of the conditional distribution, e.g., when p(yi|〈Xi, θ〉) is Gaussian so that ϕ(〈Xi, θ〉) = 〈Xi,θ〉 2\n2 , Bernoulli so that ϕ(〈Xi, θ〉) = log(1+exp(〈Xi, θ〉)), and Poisson where ϕ(〈Xi, θ〉) = exp(〈Xi, θ〉), respectively yielding least squares regression, logistic regression, and Poisson regression loss functions. Next, we provide the key results needed to characterize the regularization parameter λn and restricted strong convexity (RSC) in the context of GLMs. The non-asymptotic bound on the estimation error then follows from the general result in (23).\nBounds on the Regularization Parameter: Following the general analysis from Section 3, the regularization parameter needs to satisfy the condition: λn ≥ βR∗(∇θL(θ∗;Zn)) for any fixed β > 1. For GLMs,\n∇θL(θ∗;Zn) = − 1\nn n∑ i=1 yiXi+ 1 n n∑ i=1 Xi∇〈Xi,θ∗〉ϕ(〈Xi, θ ∗〉) = 1 n n∑ i=1 Xi(E[y|Xi]−yi) = 1 n XTω , (65)\nwhere we have used the fact, ∇〈Xi,θ∗〉ϕ(〈Xi, θ∗〉) = E[yi|〈Xi, θ∗〉] and ω = E[yi|〈Xi, θ∗〉] − yi. Thus, the form of ∇θL(θ∗;Zn) is the same as that in Section 3. Assuming the design matrix X and noise ω are sub-Gaussian, a characterization of λn follows from Theorems 3 and 4 in Section 3. In particular, E[∇θL(θ∗;Zn)] = O(w(ΩR)√n ), with corresponding high probability concentration results, and it suffices to have λn to be of this order.\n2Note that for GLMs over discrete responses yi, the integration needs to be suitably changed to summation [4, 8].\nRestricted Strong Convexity: By definition, the restricted strong convexity considers\nδL(u, θ∗) = L(θ∗ + u)− L(θ∗)− 〈∇L(θ∗), u〉 = 1 n n∑ i=1 ∇2ϕ(〈θ∗, Xi〉+ γi〈u,Xi〉)〈u,Xi〉2 ,\nwhere γi ∈ [0, 1], and where the last equality follows from a direct application of the mean value theorem [35]. Since the log-partition function ϕ is of Legendre type [3, 4, 8], the second derivative ∇2ϕ(·) is always positive. Since the RSC condition relies on a non-trivial lower bound for the above quantity, the analysis will consider suitable compact sets where∇2ϕ(·) is bounded away from zero by a constant. In particular, for a suitable constant T , we consider the sets {Xi||〈Xi, θ∗〉| < T} and {Xi||〈Xi, u〉| < T}. For Xi lying in these sets, the argument a = 〈Xi, θ∗〉+ γi〈u,Xi〉 of the second derivative satisfies |a| ≤ 2T , which is the compact set of interest. Within the compact set, ` = `ϕ(T ) = min|a|≤2T ∇2ϕ(a) is bounded away from zero. Outside the compact set, we will only assume ∇2ϕ(·) > 0. Based on the above construction, we have\nδL(u, θ∗) ≥ ` n n∑ i=1 〈Xi, u〉2 I[|〈Xi, θ∗〉| < T ] I[|〈Xi, u〉| < T ] . (66)\nThe quadratic form based lower bound allows us to establish RSC conditions for GLMs with isotropic subGaussian design matrices by building on results from Section 4 for RE conditions for squared loss. As a result, the sample complexity of the RSC condition is also expressed in terms of the Gaussian width of the spherical cap A derived from the error set. The analysis can be suitably generalized to anisotropic design matrices using techniques discussed in Section 4. As before, we consider u ∈ A ⊆ Sp−1 so that ‖u‖2 = 1. Assuming X has isotropic sub-Gaussian rows with |||Xi|||ϕ2 ≤ κ, 〈Xi, θ\n∗〉 and 〈Xi, u〉 are sub-Gaussian random variables with sub-Gaussian norm at most Cκ [42]. Denote by ε1 and ε2 the probability that 〈Xi, u〉 and 〈Xi, θ∗〉 exceeds some constant T , i.e., ε1(T ;u) = P{|〈Xi, u〉| > T} ≤ e · exp(−c2T 2/C2κ2) = ε̄1, and ε2(T ; θ∗) = P{|〈Xi, θ∗〉| > T} ≤ e · exp(−c2T 2/C2κ2) = ε̄2, where ε̄1 = ε̄1(T, κ) and ε̄2 = ε̄2(T ;κ) are uniform upper bounds on the individual tail probabilities. The result we present below is in terms of the above defined constants ` = `ϕ(T ), ε̄1 = ε̄1(T, κ) and ε̄2 = ε̄2(T, κ) for any suitably chosen T .\nTheorem 7 Let X ∈ Rn×p be a design matrix with independent isotropic sub-Gaussian rows such that |||Xi|||ϕ2 ≤ κ. Then, for any set A ⊆ S\np−1 for suitable constants η, c > 0, with probability at least 1− 2 exp ( −ηw2(A) ) , we have\ninf u∈A\n∂L(u, θ∗) ≥ `ρ2 (\n1− cκ21 w(A)√ n\n) , (67)\nwhere ρ2 = infu∈A ρ2u, with ρ 2 u = E[〈Xi, u〉2I[|〈Xi, θ∗〉| < T ]I[|〈Xi, u〉| < T ]], and κ1 = κ1−ε̄1−ε̄2 .\nThe form of the result is closely related to the corresponding result for the RE condition infu∈A ‖Xu‖2 considered in Section 4. Note that RSC analysis for GLMs was considered in [29] for specific norms, especially L1, whereas our analysis applies to any set A ⊆ Sp−1, hence to any norm, and the result is in terms of the Gaussian width w(A) of A. Further, following arguments in Section 4, the RE analysis for GLMs can be extended to anisotropic subGaussian design matrices."
    }, {
      "heading" : "7 Conclusions",
      "text" : "The paper presents a general set of results and tools for characterizing non-asymptotic estimation error in norm regularized regression problems. The analysis holds for any norm, and subsumes much of existing\nliterature focused on structured sparsity and related themes. The work can be viewed as a direct generalization of results in [29], which presented related results for decomposable norms. Our analysis illustrates the important role Gaussian widths, as a measure of size of suitable sets, play in such results. Further, the error sets for regularized and constrained versions of such problems are shown to be closely related [6].\nWhile the paper presents a unified geometric treatment of non-asymptotic structured estimation with regularized estimators, several technical questions need further investigation. The focus of the analysis has been on thin-tailed distributions, and the RE/RSC type analysis presented really gives two sided bounds, i.e., RIP, showing that thin-tailed distributions do satisfy the RIP condition. For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27]. Further, the sample complexity of the phase transitions in the RE/RSC conditions for anisotropic designs depend on the largest eigenvalue (operator norm) of the covariance matrix, making the estimator sample inefficient for highly correlated designs. Since real-world several problems, including spatial and temporal problems, do have correlated observations, it will be important to investigate estimators which perform well in such settings [18]. Finally, the focus of the work is on parametric estimation, and it will be interesting to explore generalizations of the analysis to non-parametric settings."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "A Background and Preliminaries",
      "text" : "We start with a review of some definitions and well-known results which will be used for our proofs."
    }, {
      "heading" : "A.1 Gaussian Width",
      "text" : "In several of our proofs, we use the concept of Gaussian width [20, 14], which is defined as follows.\nDefinition 1 (Gaussian width) For any set A ∈ Rp, the Gaussian width of the set A is defined as:\nw(A) = Eg [ sup u∈A 〈g, u〉 ] , (68)\nwhere the expectation is over g ∼ N(0, Ip×p), a vector of independent zero-mean unit-variance Gaussian random variable.\nThe Gaussian width w(A) provides a geometric characterization of the size of the set A. We consider three perspectives of the Gaussian width, and provide some properties which are used in our analysis. First, consider the Gaussian process {Zu} where the constituent Gaussian random variables Zu = 〈t, g〉 are indexed by u ∈ A, and g ∼ N(0, Ip×p). Then the Gaussian width w(A) can be viewed as the expectation of the supremum of the Gaussian process {Zt}. Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24]. Second, 〈u, g〉 can be viewed as a Gaussian random projection of each u ∈ A to one dimension, and the Gaussian width simply measures the expectation of largest value of such projections. Third, if A is the unit ball of any norm R(·), i.e., A = {x ∈ Rp | R(x) ≤ 1}, then w(A) = Eg[R∗(g)] by definition of the dual norm. Thus, the Gaussian width is the expected value of the dual norm of a standard Gaussian random vector. For instance, if A is unit ball of L1 norm, w(A) = E[‖g‖∞]. Below we list some simple and useful properties of the Gaussian width of A ⊆ Rp:\nProperty 1: w(A) ≤ w(B) for A ⊆ B.\nProperty 2: w(A) = w(conv(A)), where conv(·) denotes the convex hull of A.\nProperty 3: w(cA) = cw(A) for any positive scalar c, in which cA = {cx | x ∈ A}.\nProperty 4: w(ΓA) = w(A) for any orthogonal matrix Γ ∈ Rp×p.\nProperty 5: w(A+ b) = w(A) for any A ⊆ Rp and fixed b ∈ Rp. The last two properties illustrate the Gaussian width is rotation and translation invariant."
    }, {
      "heading" : "A.2 Sub-Gaussian and Sub-exponential Random Variables (Vectors)",
      "text" : "In the proof, we will also frequently use the properties of sub-Gaussian and sub-exponential random variables (vectors). In particular, we are interested in their definitions using moments.\nDefinition 2 Sub-Gaussian (sub-exponential) random variable: We say that a random variable x is subGaussian (sub-exponential) if the moments satisfies\n[E|x|p] 1 p ≤ K2 √ p ([E|x|p] 1 p ≤ K1p) (69)\nfor any p ≥ 1 with a constant K2 (K1). The minimum value of K2 (K1) is called sub-Gaussian (subexponential) norm of x, denoted by |||x|||ψ2 (|||x|||ψ1).\nDefinition 3 Sub-Gaussian (sub-exponential) random vector: We say that a random vector X in Rn is sub-Gaussian (sub-exponential) if the one-dimensional marginals 〈X,x〉 are sub-Gaussian (sub-exponential) random variables for all x ∈ Rn. The sub-Gaussian (sub-exponential) norm of X is defined as\n|||X|||ψ2 = sup x∈Sn−1 ‖〈X,x〉‖ψ2 (|||X|||ψ1 = sup x∈Sn−1 ‖〈X,x〉‖ψ1) (70)\nThe following definitions and lemmas are from [41].\nLemma 4 Consider a finite number of independent centered sub-Gaussian random variables Xi. Then∑ iXi is also a centered sub-Gaussian random variable. Moreover,∣∣∣∣∣ ∣∣∣∣∣ ∣∣∣∣∣∑ i Xi ∣∣∣∣∣ ∣∣∣∣∣ ∣∣∣∣∣ 2\nψ2\n≤ C ∑ i |||Xi|||2ψ2 (71)\nLemma 5 LetX1, . . . , Xn be independent centered sub-Gaussian random variables. ThenX = (X1, . . . , Xn) is a centered sub-Gaussian random vector in Rn, and\n|||X|||ψ2 ≤ C maxi≤n |||Xi|||ψ2 (72)\nwhere C is an absolute constant.\nLemma 6 Consider a sub-Gaussian random vector X with sub-Gaussian norm K = maxi |||Xi|||ψ2 , then, Z = 〈X, a〉 is a sub-Gaussian random variable with sub-Gaussian norm |||Z|||ψ2 ≤ CK‖a‖2.\nLemma 7 A random variable X is sub-Gaussian if and only if X2 is sub-exponential. Moreover, |||X|||2ψ2 ≤ ∣∣∣∣∣∣X2∣∣∣∣∣∣ ψ1 ≤ 2|||X|||2ψ2 (73)\nLemma 8 If X is sub-Gaussian (or sub-exponential), then so is X − EX . Moreover, the following holds,\n|||X − EX|||ψ2 ≤ 2|||X|||ψ2 , |||X − EX|||ψ1 ≤ 2|||X|||ψ1 (74)"
    }, {
      "heading" : "B Restricted Error Set and Recovery Guarantees",
      "text" : "Section 2 is about the restricted error set. Lemma 1 characterizes the restricted error set. Theorem 1 establishes the relation between the constrained and restricted error sets. In particular, we prove that the Gaussian width of the regularized and constrained error sets (cone) are of the same order. Starting with the assumption that the RSC condition is satisfied Lemma 2 and Theorem 2 derive results on the upper bound on the L2 norm of the error.\nWe collect the proofs of the different results in this section."
    }, {
      "heading" : "B.1 The Restricted Error Set",
      "text" : "Lemma 1 in Section 2 characterizes the set to which the error vector belongs. We give the proof of Lemma 1 below:\nLemma 1 For any β > 1, assuming\nλn ≥ βR∗(∇L(θ∗;Zn)) (75)\nwhere R∗(·) is the dual norm of R(·). Then the error vector ∆̂n = θ̂λn − θ∗ belongs to the set:\nEr = Er(θ ∗, β) = { ∆ ∈ Rp ∣∣∣∣ R(θ∗ + ∆) ≤ R(θ∗) + 1βR(∆) } . (76)\nProof: By the optimality of θ̂λn = θ ∗ + ∆̂n, we have\nL(θ∗ + ∆̂n) + λnR(θ∗ + ∆̂n)− {L(θ∗) + λnR(θ∗)} ≤ 0 . (77)\nNow, since L is convex,\nL(θ∗ + ∆)− L(θ∗) ≥ 〈∇L(θ∗),∆〉 ≥ −|〈∇L(θ∗),∆〉| . (78)\nFurther, by generalized Holder’s inequality, we have\n|〈∇L(θ∗),∆〉| ≤ R∗(∇L(θ∗))R(∆) ≤ λn β R(∆) , (79)\nwhere we have used λn ≥ βR∗(∇L(θ∗;Zn)). Hence, we have\nL(θ∗ + ∆̂n)− L(θ∗) ≥ − λn β R(∆̂n) . (80)\nAs a result,\nλn { R(θ∗ + ∆̂n)−R(θ∗)− 1\nβ R(∆̂n)\n} ≤ 0 . (81)\nNoting that λn > 0 and rearranging completes the proof."
    }, {
      "heading" : "B.2 Relation between the Constrained and Regularized Error Cones",
      "text" : "In this section we show that the sizes of the regularized and constrained error sets are of the same order. Recall from [14], that the error set for the constrained setting for atomic norms is a cone given by:\nCc = Cc(θ ∗) = cone(Ec) = cone {∆ ∈ Rp | R(θ∗ + ∆) ≤ R(θ∗)} . (82)\nThe error set Er is given by:\nEr = Er(θ ∗, β) = { ∆ ∈ Rp ∣∣∣∣ R(θ∗ + ∆) ≤ R(θ∗) + 1βR(∆) } .\nBelow we provide the proof of Theorem 1.\nTheorem 1 Let A(ρ)c = Ec ∩ ρBp2 , A (ρ) r = Er ∩ ρBp2 , and Ā (ρ) c = Cc ∩ ρBp2 , where ρB p 2 = {u|‖u‖2 ≤ ρ} is the L2 ball of any radius ρ > 0. Then, for any β > 1 we have\nw(A(ρ)c ) ≤ w(A(ρ)r ) ≤ ( 1 + 2 β − 1 ‖θ∗‖2 ρ ) w(Ā(ρ)c ) , (83)\nwhere w(A) denotes the Gaussian width of any set A given by: w(A) = Eg [ sup a∈A 〈a, g〉 ] , where g is an isotropic Gaussian random vector, i.e., g ∼ N(0, Ip×p). Proof: The first inequality simply follows from the fact that Ec ⊆ Er and Property 1 of Gaussian width. For the second part, from triangle inequality, we have\nR(∆) ≤ R(θ∗ + ∆) +R(θ∗) . (84)\nThen,\nEr(θ ∗, β) = { ∆ ∈ Rp ∣∣∣∣R(θ∗ + ∆) ≤ R(θ∗) + 1βR(∆) }\n⊆ { ∆ ∈ Rp ∣∣∣∣R(θ∗ + ∆) ≤ R(θ∗) + 1βR(θ∗ + ∆) + 1βR(θ∗) } = { ∆ ∈ Rp ∣∣∣∣(1− 1β ) R(θ∗ + ∆) ≤ ( 1 + 1 β ) R(θ∗)\n} = { ∆ ∈ Rp ∣∣∣∣R(θ∗ + ∆) ≤ β + 1β − 1R(θ∗) } = Ẽr(θ ∗, β) .\nLet C̃r(θ∗, β) denote the following set\nC̃r = C̃r(θ ∗, β) = cone { ∆− 2 β − 1 θ∗ ∣∣∆ ∈ Ēr}+ 2 β − 1 θ∗ . (85)\nIt follows naturally from the construction that Er ⊆ C̃r. Let Ã(ρ)r = C̃r(θ∗, β) ∩ ρBp2 . Since Er(θ∗, β) ⊆ C̃r(θ∗, β), we have w(A (ρ) r ) ≤ w(Ã(ρ)r ). We define two additional sets for our analysis:\nB̃(ρ)r = Ã (ρ) r −\n2\nβ − 1 θ∗ =\n{ ∆ ∈ Rp ∣∣∣∣∆ + 2β − 1θ∗ ∈ Ã(ρ)r } , (86)\nD̃(ρ)c = Cc(θ ∗, β) ∩\n( ρ+ 2\nβ − 1 ‖θ∗‖2\n) Bp2 = { ∆ ∈ Rp ∣∣∣∣∆ ∈ Cc, ‖∆‖2 ≤ (ρ+ 2β − 1 ) ‖θ∗‖2 } .\n(87)\nFollowing Property 3 of Gaussian width, we have\nw(D̃(ρ)c ) =\n( 1 + 2\nβ − 1 ‖θ∗‖2 ρ ) w(Ā(ρ)c ) . (88)\nFurther, using Property 5 of Gaussian width, we have\nw(Ã(ρ)r ) = w(B̃ (ρ) r ) . (89)\nFrom the construction it is clear that B̃(ρ)r ⊂ D̃(ρ)c . Hence we have\nw(D̃(ρ)c ) ≥ w(B̃(ρ)r ) (90)\nThen, we have\nw(Ã(ρ)r ) = w(B̃ (ρ) r ) ≤ w(D̃(ρ)c ) =\n( 1 + 2\nβ − 1 ‖θ∗‖2 ρ ) w(Ā(ρ)c )\nBy noting that w(A(ρ)r ) ≤ w(Ã(ρ)r ), we complete the proof."
    }, {
      "heading" : "B.3 Recovery Guarantees",
      "text" : "Lemma 2 and Theorem 2 in the paper are results which establish recovery guarantees. The result in Lemma 2 depends on θ∗, which is unknown. On the other hand Theorem 2 gives the result in terms of quantities like λn and the norm compatibility constant Ψ(Er) = supu∈Er R(u) ‖u‖2 which are easier to compute or bound. In this section we give proofs of Lemma 2 and Theorem 2.\nLemma 2 Assume that the RSC condition is satisfied in Er by the loss L(·) with parameter κ. With ∆̂n = θ̂λn − θ∗, for any norm R(·), we have\n‖∆̂n‖2 ≤ 1 κ ‖∇L(θ∗) + λn∇R(θ∗)‖2 , (91)\nwhere∇R(·) is any sub-gradient of the norm R(·). Proof: By the RSC property in Er, for any ∆ ∈ Er we have\nL(θ∗ + ∆)− L(θ∗) ≥ 〈∇L(θ∗),∆〉+ κ‖∆‖22 . (92)\nAlso, recall that any norm is convex, since by triangle inequality, for t ∈ [0, 1], we have\nR(tθ1 + (1− t)θ2) ≤ R(tθ1) +R((1− t)θ2) = tR(θ1) + (1− t)R(θ2) . (93)\nAs a result, for any sub-gradient∇R(θ) of R(θ), we have\nR(θ∗ + ∆)−R(θ∗) ≥ 〈∆,∇R(θ∗)〉 . (94)\nAdding (92) and (94), we get\nL(θ∗ + ∆)− L(θ∗) + λn(R(θ∗ + ∆)−R(θ∗)) ≥ 〈∇L(θ∗) + λn∇R(θ∗),∆〉+ κ‖∆‖22 (95)\nNow, by Cauchy-Schwartz inequality, we have\n|〈∇L(θ∗) + λn∇R(θ∗),∆〉| ≤ ‖∇L(θ∗) + λn∇R(θ∗)‖2‖∆‖2 ⇒ 〈∇L(θ∗) + λn∇R(θ∗),∆〉 ≥ −‖∇L(θ∗) + λn∇R(θ∗)‖2‖∆‖2 . (96)\nUsing (96) in (95), we have\nF(∆) = L(θ∗ + ∆)− L(θ∗) + λn(R(θ∗ + ∆)−R(θ∗)) ≥ −‖∇L(θ∗) + λn∇R(θ∗)‖2‖∆‖2 + κ‖∆‖22\n= κ‖∆‖2 { ‖∆‖2 −\n‖∇L(θ∗) + λn∇R(θ∗)‖2 κ\n} . (97)\nNow, since F(∆̂n) ≤ 0, from (97), we have\n‖∆̂n‖2 ≤ ‖∇L(θ∗) + λn∇R(θ∗)‖2\nκ , (98)\nwhich completes the proof.\nTheorem 2 Assume that the RSC condition is satisfied in Er by the loss L(·) with parameter κ. With ∆̂n = θ̂λn − θ∗, for any norm R(·), we have\n‖∆̂n‖2 ≤ 1 + β\nβ λn κ Ψ(Er) . (99)\nProof: By the RSC property in Er, we have for any ∆ ∈ Er\nL(θ∗ + ∆)− L(θ∗) ≥ 〈∇L(θ∗),∆〉+ κ‖∆‖22 . (100)\nBy definition of a dual norm, we have\n|〈∇L(θ∗),∆〉| ≤ R∗(∇L(θ∗))R(∆) . (101)\nFurther, by construction, R∗(∇L(θ∗)) ≤ λnβ , implying\n|〈∇L(θ∗),∆〉| ≤ λn β R(∆)\n⇒ 〈∇L(θ∗),∆〉 ≥ −λn β R(∆) . (102)\nFurther, from triangle inequality, we have\nR(θ∗ + ∆)−R(θ∗) ≥ −R(∆) (103)\nAdding (102) and (103), we have\nF(∆) = L(θ∗ + ∆)− L(θ∗) + λn(R(θ∗ + ∆)−R(θ∗)) ≥ − λn β R(∆) + κ‖∆‖22 − λnR(∆)\n= κ‖∆‖22 − λn 1 + β\nβ R(∆) . (104)\nBy definition of the norm compatibility constant ψEr , we have R(∆) ≤ ‖∆‖2ψEr implying −R(∆) ≥ −‖∆‖2ψEr . Plugging the inequality back into (104), we have\nF(∆) ≥ κ‖∆‖2 { ‖∆‖2 − 1 + β\nβ λn κ ψEr\n} . (105)\nSince F(∆̂n) ≤ 0, we have\n‖∆̂n‖2 ≤ 1 + β\nβ λn κ ψEr , (106)\nwhich completes the proof."
    }, {
      "heading" : "C Bounds on the Regularization Parameter",
      "text" : "In this section, we prove Theorem 3 and 4 in Section 3 of the paper. The regularization parameter should satisfy the condition λn ≥ βR∗(∇L(θ∗;Zn)). In Theorem 3 we establish the upper bound on the expectation E[R∗(∇L(θ∗;Zn))] in terms of the Gaussian width of the unit norm ball for least squares loss and Gaussian designs. In Theorem 4 we show that R∗(∇L(θ∗;Zn)) concentrates sharply around its expectation."
    }, {
      "heading" : "C.1 Proof of Theorem 3",
      "text" : "To prove Theorem 3, we first need the following theorem from generic chaining.\nTheorem 8 Let ΩR = {u : R(u) ≤ 1} be the unit norm ball of R(·). Assuming h is any centered subGaussian random vector with |||h|||ψ2 ≤ κ, then we have\nE [ sup\nR(u)≤1 〈h, u〉\n] ≤ η0κw (ΩR) ,\nwhere η0 is a universal constant.\nProof: The quantity E[supR(u)≤1〈h, u〉] can be considered the “sub-Gaussian width” of ΩR, the unit norm ball, since it has the exact same form as the Gaussian width, with h being a sub-Gaussian vector instead of a Gaussian vector. Next, we show that the sub-Gaussian width is always bounded by the Gaussian width times a factor proportional to κ.\nConsider the sub-Gaussian process Y = {Yu}, Yu = 〈u, h〉 indexed by u ∈ ΩR, the unit norm ball. Consider the Gaussian process X = {Xu}, Xu = 〈u, g〉, where g ∼ N(0, I), indexed by the same set, i.e., u ∈ ΩR, the unit norm ball. First, note that |Yu − Yv| = |〈h, u − v〉|, so that by the concentration of sub-Gaussian random variable [41, Equation 5.10], we have\nP (|Yu − Yv| ≥ ) ≤ e · exp ( − c 2\nκ2‖u− v‖2\n) , (107)\nwhere c > 0 is an absolute constant. As a result, a direct application of the generic chaining argument for upper bounds on such empirical processes [37, Theorem 2.1.5] gives\nE [ sup u,v |Yu − Yv| ] ≤ η1E [ sup u Xu ] = η1w(ΩR) , (108)\nwhere η1 is an absolute constant. Further, since {Yu} is a symmetric process, from [37, Lemma 1.2.8], we have\nE [ sup u,v |Yu − Yv| ] = 2E [ sup u Yu ] . (109)\nAs a result, with η0 = η1/2, we have\nE [ sup\nR(u)≤1 〈h, u〉\n] = E [ sup u Yu ] ≤ η0w(ΩR) . (110)\nThat completes the proof.\nNow we turn to the proof of Theorem 3. Theorem 3 Let ΩR = {u : R(u) ≤ 1}, and L be the squared loss. For sub-Gaussian design X and noise ω, we have\nE [R∗(∇L(θ∗;Zn))] ≤ ηξ · κw(ΩR)√ n , (111)\nwhere the expectation is taken over both X and ω. The constant ξ is given by\nξ =\n{ 1 if X is isotropic√\nΛmax(Σ) if X is anisotropic .\nProof: For least squares loss, we first note that E [R∗(∇L(θ∗;Zn))] = E [ R∗( 1\nn XTω)\n] = E [ sup\nR(u)≤1\n〈 1 n XTω, u\n〉]\n= E\n[ 1\nn ‖ω‖2 · E\n[ sup\nR(u)≤1\n〈 XT ω ‖ω‖2 , u 〉∣∣∣ω]]\n≤ E\n[ 1\nn ‖ω‖2 · sup v∈Sn−1 E\n[ sup\nR(u)≤1\n〈 XT v, u\n〉]]\n= 1\nn E [‖ω‖2] · sup v∈Sn−1 E\n[ sup\nR(u)≤1\n〈 XT v, u 〉] .\n(112)\nE[‖ω‖2] is the expected length of a centered sub-Gaussian random vector, which can be easily bounded using Jensen’s inequality,\nE[‖ω‖2] < √ E[‖ω‖22] = √ n . (113)\nThen we focus on E [ sup\nR(u)≤1 〈XT v, u〉\n] for any fixed v ∈ Sn−1. Let h = XT v be a random vector, and\nconsider the random variable 〈h, z〉 for any fixed z ∈ Sp−1. Note that\n〈h, z〉 = 〈v,Xz〉 .\nCase 1. IfX is independent isotropic, thenXz has i.i.d. centered sub-Gaussian entries with ψ2 norm at most κ. By Lemma 4, we know that 〈v,Xz〉 is sub-Gaussian with |||〈v,Xz〉|||ψ2 ≤ Cκ, where C is an absolute constant. Hence h is a sub-Gaussian random vector with |||h|||ψ2 ≤ Cκ. Using Theorem 8, we conclude that for any v ∈ Sn−1\nE [ sup\nR(u)≤1\n〈 XT v, u 〉] ≤ η0Cκ · w(ΩR) . (114)\nCase 2. If X is independent anisotropic, then Xz has i.i.d. centered sub-Gaussian entries with ψ2 norm at most κ √ Λmax(Σ), where Λmax(Σ) is the largest eigenvalue of Σ. By the same argument as Case 1, we have\nE [ sup\nR(u)≤1\n〈 XT v, u 〉] ≤ η0Cκ √ Λmax(Σ) · w(ΩR) . (115)\nLetting η = η0C and combining (114), (113) and (115), we complete the proof."
    }, {
      "heading" : "C.2 Proof of Theorem 4",
      "text" : "To prove Theorem 4, we also need the following result from generic chaining.\nTheorem 9 Let ΩR = {u : R(u) ≤ 1} be the unit norm ball of R(·). Assuming h is any centered subGaussian random vector with |||h|||ψ2 ≤ κ, then we have for any τ > 0,\nP ( sup\nR(u)≤1 〈h, u〉 ≥ ν0κw(ΩR) + τ\n) ≤ ν1 exp ( − ( τ\nν2κφ\n)2) , (116)\nwhere ν0, ν1 and ν2 are universal constants, and φ = supR(u)≤1 ‖u‖2.\nProof: Consider the sub-Gaussian process Y = {Yu}, Yu = 〈h, u〉 indexed by u ∈ ΩR. By the same argument in the proof of Theorem 8, we have\nP (|Yu − Yv| ≥ ) ≤ e · exp ( − c 2\nκ2‖u− v‖2\n) , (117)\nand\nE [ sup u,v |Yu − Yv| ] = 2E [ sup u Yu ] . (118)\nThen a direct application of [37, Theorem 2.1.5] and [38, Theorem 2.2.27] gives us (116).\nTheorem 4 Let designX and noise ω be sub-Gaussian, and L be squared loss. Define φ = supR(u)≤1 ‖u‖2, then for any τ > 0, with probability at least 1− c1 exp ( −min ( ( τc2ξκφ) 2, c0n )) , we have\nR∗ (∇L(θ∗;Zn)) ≤ √ 2K2 + 1\nn (cξκ · w(ΩR) + τ) , (119)\nwhere c, c0, c1 and c2 are all absolute constants, and ξ is the same as in Theorem 3. Proof: We only show the case for isotropic X , where ξ = 1. Note that\nP ( R∗ (∇L(θ∗;Zn)) ≥ √ 2K2 + 1\nn (cξκ · w(ΩR) + τ)\n)\n= P ( ‖ω‖2 ·R∗ ( XTω\n‖ω‖2\n) ≥ √ (2K2 + 1)n (cκ · w(ΩR) + τ) )\n≤ P ( ‖ω‖2 > √ (2K2 + 1)n ) + sup v∈Sn−1 P ( R∗ ( XT v ) ≥ cκ · w(ΩR) + τ ) ,\n(120)\nwhere the last inequality uses the union bound. We first prove the bound for ‖ω‖2. Since ω consists of i.i.d. centered unit-variance sub-Gaussian elements with |||ωi|||ψ2 < K, ω 2 i is sub-exponential with |||ωi|||ψ1 <\n2K2. By applying Bernstein’s inequality to ‖ω‖22 = ∑n i=1 ω 2 i , we obtain\nP (∣∣∣‖ω‖22 − E[‖ω‖22]∣∣∣ ≥ τ) ≤ 2 exp [−c0 min( τ24K4n, τ2K2 )] ,\nwhere c0 is an absolute constant. Setting τ = 2K2n and using (113), we get P ( ‖ω‖2 ≥ √ (2K2 + 1)n ) ≤ 2 exp(−c0n) (121)\nNext we bound R∗(XT v) for any v ∈ Sn−1. Given any fixed v ∈ Sn−1, we note that\nR∗(XT v) = sup R(u)≤1 〈XT v, u〉 ,\nand XT v is a sub-Gaussian random vector with ∣∣∣∣∣∣XT v∣∣∣∣∣∣\nψ2 ≤ Cκ as shown in the proof of Theorem 3.\nUsing Theorem 9, we have\nP ( R∗(XT v) ≥ ν0Cκ · w(ΩR) + τ ) ≤ ν1 exp ( − ( τ\nν2Cκφ\n)2) . (122)\nLetting c = ν0C, c1 = ν1 + 2 and c2 = ν2C, and combining (121) and (122), we complete the proof."
    }, {
      "heading" : "C.3 Proof of Lemma 3",
      "text" : "Lemma 3 Let ΩR = {u : R(u) ≤ 1} be the unit norm ball and ΘR = {u : R(u) = 1} be the boundary. For any θ̃ ∈ ΘR define ρ(θ̃) = supθ:R(θ)≤1 ‖θ − θ̃‖2 is the diameter of ΩR measured with respect to θ̃. If G(θ̃) = cone(ΩR − θ̃) ∩ ρ(θ̃)Bp2 , i.e., the cone of (ΩR − θ̃) intersecting the ball of radius ρ(θ̃). Then\nw(ΩR) ≤ inf θ̃∈ΘR w(G(θ̃)) (123)\nProof: For any θ̃ ∈ ΘR, consider the set FR(θ̃) = ΩR − θ̃ = {u : R(u + θ̃) ≤ 1}. Since Gaussian width is translation invariant, the Gaussian width of ΩR and FR are the same, i.e., w(ΩR) = w(FR(θ̃)). Since, ρ(θ̃) = supθ:R(θ)≤1 ‖θ− θ̃‖2 is the diameter of ΩR as well as FR(θ̃), a ball of radius ρ(θ̃) will include FR(θ̃), so that FR(θ̃) ⊆ ρ(θ̃)Bp2 . Further, by definition, FR(θ̃) ⊆ cone(FR(θ̃)) = cone(ΩR − θ̃). Let G(θ̃) = cone(ΩR − θ̃) ∩ ρ(θ̃)Bp2 . By construction, FR(θ̃) ⊆ G(θ̃). Then,\nw(ΩR) = w(FR(θ̃)) ≤ w(G(θ̃)) .\nNoting the analysis holds for any θ̃ ∈ ΘR, completes the proof."
    }, {
      "heading" : "D Restricted Eigenvalue Conditions: Sub-Gaussian Designs",
      "text" : "We focus on results in Section 4. In particular we consider RE conditions for sub-Gaussian design matrices for three different cases: (i) the design matrix has independent sub-Gaussian rows Xi with |||Xi|||ψ2 ≤ κ, (ii) the design matrix has independent rows with subGaussian elements xij so that |||xij |||ψ2 ≤ κ and the\ncolumns are correlated, and (iii) the columns are independent subGaussian but the rows are correlated, i.e., correlated samples. One can view (ii) as a special case of (i), but we highlight this special case because of its practical importance and past literature on RE conditions for anisotropic subGaussian designs [33, 34].\nOur results simply use a general treatment developed in [28], building on [21], based on Talagrand’s generic chaining [37, 38]. We specifically focus on results in [28] which provide uniform bounds on the supremum of certain empirical processes. RE results for the specific cases of interest in the current paper will then be established by suitable choices of these empirical processes. The results in [28], and more generally in generic chaining [37, 38], are based on certain γ-functionals which we briefly introduce below.\nConsider a metric space (T, d) and for a finite set A ⊂ T , let |A| denote its cardinality. An admissible sequence is an increasing sequence of subsets {An, n ≥ 0} of T , such that |A0| = 1 and for n ≥ 1, |An| = 22 n . Given α > 0, we define the γα-functional as\nγα(T, d) = inf sup t∈T ∞∑ n=0 Diam(An(t)) , (124)\nwhere An(t) is the unique element ofAn that contains t, Diam(An(t)) is the diameter of An according to d, and the infimum is over all admissible sequences of T . To get the desired RIP results in terms of Gaussian widths, we start with the following key result, originally [28, Theorem D].\nTheorem 10 (Mendelson, Pajor, Tomczak-Jaegermann [28]) There exist absolute constants c1, c2, c3 for which the following holds. Let (Ω, µ) be a probability space, set F be a subset of the unit sphere of L2(µ), i.e., F ⊆ SL2 = {f : |||f |||L2 = 1}, and assume that supf∈F |||f |||ψ2 ≤ κ. Then, for any θ > 0 and n ≥ 1 satisfying\nc1κγ2(F, |||·|||ψ2) ≤ θ √ n , (125)\nwith probability at least 1− exp(−c2θ2n/κ4),\nsup f∈F ∣∣∣∣∣ 1n n∑ i=1 f2(Xi)− E [ f2 ]∣∣∣∣∣ ≤ θ . (126)"
    }, {
      "heading" : "Further, if F is symmetric, then",
      "text" : "E [ sup f∈F ∣∣∣∣∣ 1n n∑ i=1 f2(Xi)− E [ f2 ]∣∣∣∣∣ ] ≤ c3 max { 2κ γ2(F, |||·|||ψ2)√ n , γ22(F, |||·|||ψ2) n } (127)\nWe use the above result and related arguments to establish RIP conditions for the cases of interest.\nD.1 Isotropic Sub-Gaussian Designs\nWe consider the case where the design matrix X ∈ Rn×p has independent subGaussian rows where each row satisfies |||Xi|||ψ2 ≤ κ and E[XiX T i ] = Ip×p. Thus, the measure µ from which the rows Xi are sampled independently is an isotropic sub-Gaussian measure.\nTheorem 11 Let X be a design matrix with independent isotropic subGaussian rows, i.e., |||Xi|||ψ2 ≤ κ and E[XiX T i ] = I. Then, for absolute constants η, c > 0, with probability at least (1 − 2 exp(−ηw2(A))), we have\nsup u∈A ∣∣∣∣ 1n ||Xu||2 − 1 ∣∣∣∣ = sup u∈A ∣∣∣∣∣ 1n n∑ i=1 〈Xi, u〉2 − 1 ∣∣∣∣∣ ≤ cw(A)√n , (128)\nor, equivalently,\n1− cw(A)√ n ≤ inf u∈A 1 n ||Xu||2 ≤ sup\nu∈A\n1 n ||Xu||2 ≤ 1 + cw(A)√ n . (129)\nProof: The result essentially follows from an application of Theorem 10. For convenience of notation, let X0 be i.i.d. as the rows Xi, i = 1, . . . , n, thus distributed following µ. To apply Theorem 10, we choose any A ⊆ Sp−1 consider the following class of functions: F = {〈·, u〉 : u ∈ A}. Then, f(X0) = 〈X0, u〉 and F is a subset of the unit sphere, i.e., F ⊆ SL2 , since |||f |||L2 = E[u\nTXT0 X0u] = ‖u‖2 = 1. Further, supf∈F |||f |||ψ2 = supu∈A |||〈X0, u〉|||ψ2 ≤ |||X0|||ψ2 ≤ κ/2. Next, we show that for the current setting, the γ2-functional can be upper bounded by w(A), the Gaussian width of A. Since µ is isotropic subGaussian with ψ2-norm bounded by κ, we have\nγ2(F ∩ SL2 , |||·|||ψ2) ≤ κγ2(F ∩ SL2 , |||·|||L2) ≤ κc4w(A) , (130)\nwhere the last inequality follows from generic chaining, in particular [37, Theorem 2.1.1], for an absolute constant c4 > 0.\nIn the context of Theorem 10, we choose\nθ = c1c4κ 2w(A)√\nn ≥ c1κ γ2(F ∩ SL2 , |||·|||ψ2)√ n ,\nso that the condition on θ is satisfied. With this choice of θ, we have\nθ2n/κ4 = c21c 2 4w 2(A) .\nThen, from Theorem 10, it follows that with probability at least 1− exp(−ηw2(A)), we have\nsup u∈A ∣∣∣∣∣ 1n n∑ i=1 〈Xi, u〉2 − 1 ∣∣∣∣∣ ≤ cw(A)√n , (131) where η = c2c21c 2 4 and c = c1c2κ 2 are absolute constants. As a result, we have\nsup u∈A\n( 1\nn n∑ i=1 〈Xi, u〉2 − 1\n) ≤ cw(A)√\nn , and sup\nu∈A\n( 1− 1\nn n∑ i=1\n〈Xi, u〉2 ) ≤ cw(A)√\nn ,\nyielding\n1− cw(A)√ n ≤ inf u∈A 1 n ||Xu||2 ≤ sup\nu∈A\n1 n ||Xu||2 ≤ 1 + cw(A)√ n . (132)\nThat completes the proof."
    }, {
      "heading" : "D.2 Anisotropic Sub-Gaussian Designs",
      "text" : "We now consider the case where the design matrix X ∈ Rn×p has independent rows, and each row Xi is anisotropic subGaussian with E[XTi Xi] = Σ. Further, we assume that corresponding isotropic random\nvector X̃i = XiΣ−1/2 satisfies ∣∣∣∣∣∣∣∣∣X̃i∣∣∣∣∣∣∣∣∣\nψ2 ≤ κ. A simple special case of such an anisotropic subGaussian design is when Xi ∼ N(0,Σ), where X̃i = XiΣ−1/2 ∼ N(0, I) so that ∣∣∣∣∣∣∣∣∣X̃i∣∣∣∣∣∣∣∣∣\nψ2 = 1. The result below\ncharacterizes RIP-style property of any such anisotropic subGaussian designs.\nTheorem 12 LetX be a design matrix with independent anisotropic subGaussian rows, i.e., E[XTi Xi] = Σ and ∣∣∣∣∣∣XiΣ−1/2∣∣∣∣∣∣ψ2 ≤ κ. Then, for absolute constants η, c > 0, with probability at least (1−2 exp(−ηw2(A))), we have\nsup u∈A ∣∣∣∣ 1n 1uTΣu ||Xu||2 − 1 ∣∣∣∣ = sup u∈A ∣∣∣∣∣ 1n 1uTΣu n∑ i=1 〈Xi, u〉2 − 1 ∣∣∣∣∣ ≤ cw(A)√n . (133) Further,\nλmin(Σ|A) (\n1− cw(A)√ n\n) ≤ inf\nu∈A\n1 n ||Xu||2 ≤ sup\nu∈A\n1 n ||Xu||2 ≤ λmax(Σ|A)\n( 1 + c\nw(A)√ n\n) ,\n(134) where\nλmin(Σ|A) = inf u∈A uTΣu , and λmax(Σ|A) = sup u∈A uTΣu (135)\nare the restricted minimum and maximum eigenvalues of Σ restricted to A ⊆ Sp−1.\nProof: The result also follows from an application of Theorem 10. For convenience of notation, let X0 be i.i.d. as the rows Xi, i = 1, . . . , n, thus distributed following µ. To apply Theorem 10, we choose any A ⊆ Sp−1 consider the following class of functions:\nF = {fu, u ∈ A : fu(·) = 1√ uTΣu 〈·, u〉} . (136)\nThen, fu(X0) = 1√ uT Σu 〈X0, u〉 and F is a subset of the unit sphere, i.e., F ⊆ SL2 , since for fu ∈ F\n|||fu|||2L2 = 1 uTΣu E[uTXT0 X0u] = 1 .\nNext, we focus on getting an upper bound on supfu∈F |||fu|||ψ2 = supu∈A ∣∣∣∣∣∣∣∣∣ 1√ uT Σu 〈X0, u〉 ∣∣∣∣∣∣∣∣∣ ψ2 . Let X̃0 = X0Σ −1/2 so that X̃0 is a isotropic vector with ∣∣∣∣∣∣∣∣∣X̃0∣∣∣∣∣∣∣∣∣ ψ2 ≤ κ. Noting that\n〈X0, u〉 = 〈X̃0,Σ1/2u〉 = √ uTΣu〈X̃0, Σ1/2u\n‖Σ1/2u‖2 〉 ,\nwe have\nsup u |||fu|||ψ2 = sup u∈A ∣∣∣∣∣∣∣∣∣∣∣∣ 1√ uTΣu 〈X0, u〉 ∣∣∣∣∣∣∣∣∣∣∣∣ ψ2 = sup u∈A ∣∣∣∣∣ ∣∣∣∣∣ ∣∣∣∣∣〈X̃0, Σ1/2u‖Σ1/2u‖2 〉 ∣∣∣∣∣ ∣∣∣∣∣ ∣∣∣∣∣ ψ2 ≤ ∣∣∣∣∣∣∣∣∣X̃0∣∣∣∣∣∣∣∣∣ ψ2 ≤ κ .\nAs a result, we have\nγ2(F ∩ SL2 , |||·|||ψ2) ≤ κ γ2(F ∩ SL2 , |||·|||L2) ≤ κc4w(A) , (137)\nwhere the last inequality follows from [37, Theorem 2.1.1], for an absolute constant c4 > 0.\nIn the context of Theorem 10, we choose\nθ = c1c4κ 2w(A)√\nn ≥ c1κ γ2(F ∩ SL2 , |||·|||ψ2)√ n ,\nso that the lower bound condition on θ is satisfied. With this choice of θ, we have\nθ2n/κ4 = c21c 2 4w 2(A) .\nThen, from Theorem 10, it follows that with probability at least 1− exp(−ηw2(A)), we have\nsup u∈A ∣∣∣∣∣ 1n 1uTΣu n∑ i=1 〈Xi, u〉2 − 1 ∣∣∣∣∣ ≤ cw(A)√n , (138) where η = c2c21c 2 4 and c = c1c2κ 2 are absolute constants. As a result, we have\nsup u∈A\n( 1\nn\n1\nuTΣu n∑ i=1 〈Xi, u〉2 − 1\n) ≤ cw(A)√\nn , (139)\nsup u∈A\n( 1− 1\nn\n1\nuTΣu n∑ i=1\n〈Xi, u〉2 ) ≤ cw(A)√\nn . (140)\nFrom (139), we have\n1\nλmax(Σ|A) sup u∈A\n1 n ‖Xu‖2 ≤ sup\nu∈A\n1\nn\n1 uTΣu ‖Xu‖2 ≤ 1 + cw(A)√ n ,\nso that\nsup u∈A\n1 n ‖Xu‖2 ≤ λmax(Σ|A) + cλmax(Σ|A) w(A)√ n . (141)\nSimilarly, from (140), we have\n1− cw(A)√ n ≤ inf u∈A 1 n 1 uTΣu ‖Xu‖2 ≤ 1 λmin(Σ|A) inf u∈A 1 n ‖Xu‖2 ,\nimplying\nλmin(Σ|A)− cλmin(Σ|A) w(A)√ n ≤ inf u∈A 1 n ‖Xu‖2 . (142)\nPutting (141) and (142) together completes the proof."
    }, {
      "heading" : "E Generalized Linear Models: Restricted Strong Convexity",
      "text" : "We establish bounds on the regularization parameter and RSC condition for GLMs as discussed in Section 6, along with a few specific examples."
    }, {
      "heading" : "E.1 Generalized Linear Models",
      "text" : "Loss functions for GLMs are derived as maximum likelihood estimators for the family of exponential distributions. The canonical density function of exponential family distributions is given by [4, 8, 44]:\nP (y|η) ∝ exp{ηy − ϕ(η)} , (143)\nwhere η is the natural parameter and has a one-to-one function mapping with the mean parameter µ = E[y] of the distribution, ϕ(η) is the log-partition function which ensures that P (y|η) remains a probability distribution. The gradient of the log-partition function is the response function, i.e., g(·) = ϕ′(·), which is monotonic by construction. The inverse of the response function is the so-called link function h(·) = g−1(·).\nThe mean of the distribution can be obtained from the gradient of the log-partition function at the natural parameter, i.e.,\nµ = ϕ ′ (η) = g(η) . (144)\nThe interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44]. Examples of distributions from the exponential family include the Gaussian, multinomial, exponential, Dirichlet, Poisson, Gamma, etc.\nGLMs are obtained from conditional exponential family distributions by assuming a suitable parametric form of the natural parameter η in terms of X and θ∗, in particular ηi = 〈Xi, θ∗〉. Then, the conditional distribution is given by\nP (yi|Xi, θ∗) ∝ exp{ηiyi − ϕ(ηi)} = exp{〈Xi, θ∗〉yi − ϕ(〈Xi, θ∗〉)} . (145)\nThe loss function for GLMs simply consider the negative log likelihood of such conditional exponential family forms. Assuming samples to be independent, we have\nL(θ;Zn) = − 1 n n∑ i=1 {ηiyi − ϕ(ηi)} = − 1 n n∑ i=1 {〈Xiyi, θ〉 − ϕ(〈Xi, θ〉)} . (146)\nUsing chain rule, the first derivative of the loss function evaluated at θ∗ is\n∇θL(θ∗;Zn) = − 1\nn n∑ i=1 yiXi + 1 n n∑ i=1 Xi ∂ϕ(〈θ∗, Xi〉) ∂ηi = 1 n n∑ i=1 Xi(E[yi|Xi]− yi) = 1 n XTω ,\nwhere each element of ω ∈ Rn is given as ωi = E(y|Xi) − yi. Next we look at some specific examples of exponential families and corresponding GLMs.\n1. Gaussian distribution: If the variance of the Gaussian distribution P (y|Xi) is assumed to be 1, then we have\nP (yi|Xi; θ∗) ∝ exp { yi〈Xi, θ∗〉 − 〈Xi, θ∗〉2\n2\n} .\nComparing it with the canonical form given earlier, the natural parameter is ηi = 〈Xi, θ∗〉, log-partition function is ϕ(〈Xi, θ∗〉) = 〈Xi,θ ∗〉2 2 and hence E[y|Xi] = ϕ ′ (〈Xi, θ∗〉) = 〈Xi, θ∗〉. The noise ωi = yi − E(yi|Xi) is Gaussian. Considering the negative log-likelihood, the GLM corresponding to the Gaussian distribution yields least squares regression [8].\n2. Bernoulli distribution: Assuming the conditional distribution of yi|Xi, θ∗ to have a Bernoulli distribution with conditional mean parameter pi, which is a suitable function of 〈Xi, θ∗〉, the likelihood of the observations is given by\nP (yi|pi) = pyii (1− pi) (1−yi) = exp(yi log pi + (1− yi) log(1− pi)) = exp ( yi log ( pi\n1− pi\n) + log(1− pi) ) Therefore, the natural parameter ηi = 〈Xi, θ∗〉 = log ( pi\n1−pi\n) giving pi =\nexp(〈Xi,θ∗〉) 1+exp(〈Xi,θ∗〉) . It can be\nverified from the fact that the probability density function P (yi|pi) adds to 1 and the log-partition function evaluates to ϕ(ηi) = log(1−pi) = log(1+exp(〈Xi, θ∗〉)). The noise in the model corresponds to random draws from a Bernoulli distribution, and each element of ω is ωi = pi − yi = exp(〈Xi,θ\n∗〉) 1+exp(〈Xi,θ∗〉) − yi, which\nis bounded and hence sub-Gaussian. Considering the negative log-likelihood, the GLM corresponding to the Bernoulli distribution yields logistic regression [8].\n3. Poisson distribution: Assuming the conditional distribution of yi|Xi, θ∗ to have a Poisson distribution with conditional mean parameter λi, which is a suitable function of 〈Xi, θ∗〉, the likelihood of the observations is given by\nP (yi|λi) = λyii exp(−λi)\nyi! ∝ exp{log(λyii exp(−λi))} = exp{yi log λi − λi} ,\nwhere the 1/yi! term constitutes the base measure for the distribution. Based on the form, the natural parameter ηi = 〈Xi, θ∗〉 = log λi giving λi = exp(ηi) = exp(〈Xi, θ∗〉). Also it can be verified that the log-partition function ϕ(ηi) = exp(ηi) = λi. Each element of ω is ωi = λi − yi = exp(〈Xi, θ∗〉) − yi. Considering the negative log-likelihood, the GLM corresponding to the Poisson distribution yields Poisson regression [8].\nAs discussed in Section 6, if the design matrix X and the noise is assumed to be sub-Gaussian, then the regularization parameter λn needs to be O(ΩR√n), following the analysis and results in Section 3. In the rest of this section, we focus on proving the GLMs with sub-Gaussian designs satisfy the RSC condition with sample complexity depending on the width of the spherical cap corresponding to the error set, as discussed in Section 2."
    }, {
      "heading" : "E.2 RSC condition for GLMs",
      "text" : "For any convex loss function to satisfy the RSC condition on any A ⊆ Sp−1, the following inequality\nδL(θ∗, u;Zn) = L(θ∗ + u;Zn)− L(θ∗;Zn)− 〈∇L(θ∗;Zn), u〉 ≥ κ‖u‖22 (147)\nneeds to hold ∀u ∈ Ar. For the general formulation of GLM discussed earlier, we have\nδL(θ∗, u;Zn) = −〈θ∗ + u, 1 n n∑ i=1 yiXi〉+ 1 n n∑ i=1 ϕ(〈θ∗ + u,Xi〉) + 〈θ∗, 1 n n∑ i=1 yiXi〉\n− 1 n n∑ i=1 ϕ(〈θ∗, Xi〉)− 〈− 1 n n∑ i=1 yiXi + 1 n n∑ i=1 Xiϕ ′ (〈θ∗, xi〉), u〉 .\nSimplifying the expression and applying mean value theorem twice we get the following\nδL(θ∗, u;Zn) = 1 n n∑ i=1 ϕ ′′ (〈θ∗, Xi〉+ γi〈u,Xi〉) 〈u,Xi〉2 , (148)\nfor suitable γi ∈ [0, 1]. The RSC condition for GLMs then needs to consider lower bounds for\nδL(θ∗, u;Zn) = 1 n n∑ i=1 ϕ ′′ (〈θ∗, Xi〉+ γi〈u,Xi〉)〈u,Xi〉2 (149)\nwhere γi ∈ [0, 1]. The second derivative of the log-partition function is always positive. Since the RSC condition relies on a non-trivial lower bound for the above quantity, the analysis will suitably consider a compact set where ` = `ϕ(T ) = min|a|≤2T ϕ ′′ (a) is bounded away from zero. The only assumption outside this compact set {a : |a| ≤ 2T} is that the second derivative is greater than 0. Further, we assume ‖θ∗‖2 ≤ c1 for some constant c1. With these assumptions\nδL(θ∗, u;Zn) ≥ ` n n∑ i=1 〈Xi, u〉2I[|〈Xi, θ∗〉| < T ]I[|〈Xi, u〉| < T ] . (150)\nWe give a characterization of the RSC condition for isotropic sub-Gaussian design matrices X ∈ Rn×p. We consider u ∈ A ⊆ Sp−1 so that ‖u‖2 = 1. Further, we assume ‖θ∗‖2 ≤ c1 for some constant c1. Assuming X has sub-Gaussian rows with |||Xi|||ϕ2 ≤ κ, 〈Xi, θ\n∗〉 and 〈Xi, u〉 are sub-Gaussian random variables with sub-Gaussian norm at most Cκ. Let ε1 and ε2 denote the tail probability that 〈Xi, u〉 and 〈Xi, θ∗〉 exceeds some constant T , i.e., ε1(T ;u) = P{|〈Xi, u〉| > T} ≤ e·exp(−c2T 2/C2κ2) = ε̄1, and ε2(T ; θ∗) = P{|〈Xi, θ∗〉| > T} ≤ e·exp(−c2T 2/C2κ2) = ε̄2, where ε̄1 = ε̄1(T, κ) and ε̄2 = ε̄2(T ;κ) are uniform upper bounds on the individual tail probabilities. The result we present below is in terms of the above defined constants ` = `ϕ(T ), ε̄1 = ε̄1(T, κ) and ε̄2 = ε̄2(T, κ) for any suitably chosen T . Theorem 7 Let X ∈ Rn×p be a design matrix with independent isotropic sub-Gaussian rows such that |||Xi|||ϕ2 ≤ κ. Then, for any set A ⊆ S\np−1 for suitable constants η, c > 0, with probability at least 1− 2 exp ( −ηw2(A) ) , we have\ninf u∈A\n∂L(θ∗;u,X) ≥ `ρ2 (\n1− cκ21 w(A)√ n\n) . (151)\nwhere ρ2 = infu∈A ρ2u, with ρ 2 u = E[〈Xi, u〉2I[|〈Xi, θ∗〉| < T ]I[|〈Xi, u〉| < T ]], and κ1 = κ(1−ε̄1−ε̄2)2 . Proof: For any fixed T , let Z̄i = Z̄ui = 〈Xi, u〉I(|〈Xi, u〉| ≤ T )I(|〈Xi, θ∗〉| ≤ T ). Then, the probability distribution over Z̄i can be written as:3\nP (Z̄i = z) = P (〈Xi, u〉 = z)I(|〈Xi, u〉| ≤ T )I(|〈Xi, θ∗〉| ≤ T )\nP (|〈Xi, u〉| ≤ T, |〈Xi, θ∗〉| ≤ T ) ≤ 1 1− ε̄1 − ε̄2 P (〈Xi, u〉 = z) . (152)\nAs a result, ∣∣∣∣∣∣Z̄i∣∣∣∣∣∣ψ2 ≤ κ1−ε̄1−ε̄2 = κ1. Thus, Z̄i = Z̄ui is a sub-Gaussian random variable for any u ∈ A. Let ρ2u = E[(Z̄i u )2] > 0. Let X0 be i.i.d. as the rows Xi, i = 1, . . . , n. Let A ⊆ Sp−1 and consider the following class of functions: F = { 1ρu 〈·, u〉I(|〈·, u〉| ≤ T )I(|〈·, θ ∗〉| ≤ T ) : u ∈ A}. Then for any f ∈ F , f(X0) = 1ρu 〈X0, u〉I(|〈X0, u〉| ≤ T )I(|〈X0, θ ∗〉| ≤ T ) and, by construction, F is a subset of the unit sphere, i.e., F ⊆ SL2 . Further, supf∈F |||f |||ψ2 ≤ κ1/2. Next, we show that for the current setting, the γ2-functional can be upper bounded by w(A), the Gaussian width of A. Since the process is sub-Gaussian with ϕ2-norm bounded by κ1, we have\nγ2(F ∩ SL2 , |||·|||ψ2) ≤ κ1γ2(F ∩ SL2 , |||·|||L2) ≤ κ1c4w(A) , (153)\nwhere the last inequality follows from generic chaining, in particular [37, Theorem 2.1.1], for an absolute constant c4 > 0.\nIn the context of Theorem 10, we choose\nθ = c1c4κ 2 1 w(A)√ n ≥ c1κ1 γ2(F ∩ SL2 , |||·|||ϕ2)√ n , (154)\nso that the condition on θ is satisfied. With this choice of θ, we have\nθ2n/κ41 = c 2 1c 2 4w 2(A) . (155)\nThen, from Theorem 10, it follows that with probability at least 1− exp(−ηw2(A)), we have\nsup u∈A ∣∣∣∣∣ 1ρun n∑ i=1 〈Xi, u〉2I(|〈X0, u〉| ≤ T )I(|〈X0, θ∗〉| ≤ T )− 1 ∣∣∣∣∣ ≤ cκ21w(A)√n (156) 3With abuse of notation, we treat the distribution over Z̄i as discrete for ease of notation. A similar argument applies for the true\ncontinuous distribution, but more notation is needed.\nwhere η = c2c21c 2 4 and c = c1c2 are absolute constants. Thus, with probability at least 1− exp(−ηw2(A)),\ninf u∈A\n1\nn n∑ i=1 〈Xi, u〉2I(|〈X0, u〉| ≤ T )I(|〈X0, θ∗〉| ≤ T ) ≥ inf u∈A ρ2u ( 1− cκ21 w(A)√ n ) . (157)\nDenoting ρ2 = infu∈A ρ2u, with probability at least 1− exp(−ηw2(A)), we have\ninf u∈A ∂L(θ∗;u,X) ≥ inf u∈A\n`\nn n∑ i=1 〈Xi, u〉2I[|〈Xi, θ∗〉| < T ]I[|〈Xi, u〉| < T ] ≥ `ρ2 ( 1− cκ21 w(A)√ n ) .\n(158) That completes the proof.\nAcknowledgements: We thank the reviewers of the conference version [2] for helpful comments and suggestions on related work. We thank Sergey Bobkov, Snigdhansu Chatterjee, and Pradeep Ravikumar for helpful discussions related to the paper. The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A."
    } ],
    "references" : [ {
      "title" : "Living on the edge: Phase transitions in convex programs with random data",
      "author" : [ "D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp" ],
      "venue" : "Inform. Inference,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Estimation with norm regularization",
      "author" : [ "A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Clustering with Bregman Divergences",
      "author" : [ "A Banerjee", "S Merugu", "I Dhillon", "J Ghosh" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Information and Exponential Families in Statistical Theory",
      "author" : [ "O Barndorff-Nielsen" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1978
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "P.J. Bickel", "Y. Ritov", "A.B. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Concentration Inequalities: A Nonasymptotic Theory of Independence",
      "author" : [ "S. Boucheron", "G. Lugosi", "P. Massart" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Fundamentals of Statistical Exponential Families",
      "author" : [ "L Brown" ],
      "venue" : "Institute of Mathematical Statistics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1986
    }, {
      "title" : "Statistics for High Dimensional Data: Methods, Theory and Applications",
      "author" : [ "P. Buhlmann", "S. van de Geer" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "The restricted isometry property and its implications for compressed sensing",
      "author" : [ "E. Candes" ],
      "venue" : "Comptes Rendus Mathematique,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "The Dantzig selector: statistical estimation when p is much larger than n",
      "author" : [ "E. Candes", "T Tao" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "E.J. Candes", "J. Romberg", "T. Tao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Decoding by linear programming",
      "author" : [ "E.J. Candes", "T. Tao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "The convex geometry of linear inverse problems",
      "author" : [ "V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Generalized dantzig selector: Application to the k-support norm",
      "author" : [ "S. Chatterjee", "S. Chen", "A. Banerjee" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Structured estimation with atomic norms: General bounds and applications",
      "author" : [ "S. Chen", "A. Banerjee" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "An elementary proof of a theorem of johnson and lindenstrauss",
      "author" : [ "Sanjoy Dasgupta", "Anupam Gupta" ],
      "venue" : "Random Struct. Algorithms,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Sparse estimation with strongly correlated variables using ordered weighted l1 regularization",
      "author" : [ "M.A.T. Figueiredo", "R.D. Nowak" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Some inequalities for gaussian processes and applications",
      "author" : [ "Y. Gordon" ],
      "venue" : "Israel Journal of Mathematics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1985
    }, {
      "title" : "On milmans inequality and random subspaces which escape through a mesh in rn",
      "author" : [ "Y. Gordon" ],
      "venue" : "In Geometric Aspects of Functional Analysis,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1988
    }, {
      "title" : "Empirical processes and random projections",
      "author" : [ "B. Klartag", "S. Mendelson" ],
      "venue" : "Journal of Functional Analysis,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2005
    }, {
      "title" : "Bounding the smallest singular value of a random matrix without concentration",
      "author" : [ "V. Koltchinskii", "S. Mendelson" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Sparse recovery under weak moment assumptions",
      "author" : [ "G. Lecué", "S. Mendelson" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Probability in Banach Spaces: Isoperimetry and Processes",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning",
      "author" : [ "A. Maurer", "M. Pontil", "B. Romera-Paredes" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Lasso-type recovery of sparse representations for high-dimensional data",
      "author" : [ "N. Meinshausen", "B. Yu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "Learning Without Concentration",
      "author" : [ "S. Mendelson" ],
      "venue" : "In Journal of the ACM,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Reconstruction and subGaussian operators in asymptotic geometric analysis",
      "author" : [ "S. Mendelson", "A. Pajor", "N. Tomczak-Jaegermann" ],
      "venue" : "Geometric and Functional Analysis,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    }, {
      "title" : "A unified framework for the analysis of regularized M -estimators",
      "author" : [ "S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "The lower tail of random quadratic forms, with applications to ordinary least squares and restricted eigenvalue properties",
      "author" : [ "R.I. Oliveira" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    }, {
      "title" : "The Squared-Error of Generalized Lasso: A Precise Analysis",
      "author" : [ "S. Oymak", "C. Thrampoulidis", "B. Hassibi" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach",
      "author" : [ "Y. Plan", "R. Vershynin" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "Restricted Eigenvalue Properties for Correlated Gaussian Designs",
      "author" : [ "G. Raskutti", "M.J. Wainwright", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2010
    }, {
      "title" : "Reconstruction from anisotropic random measurements",
      "author" : [ "Z. Rudelson", "S. Zhou" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2013
    }, {
      "title" : "Principles of Mathematical Analysis",
      "author" : [ "Walter Rudin" ],
      "venue" : "International Series in Pure & Applied Mathematics. McGraw-Hill,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1976
    }, {
      "title" : "Beyond sub-gaussian measurements: High-dimensional structured estimation with sub-exponential designs",
      "author" : [ "V. Sivakumar", "A. Banerjee", "P. Ravikumar" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2015
    }, {
      "title" : "The Generic Chaining",
      "author" : [ "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2005
    }, {
      "title" : "Upper and Lower Bounds for Stochastic Processes",
      "author" : [ "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2014
    }, {
      "title" : "Regression shrinkage and selection via the Lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1996
    }, {
      "title" : "Convex recovery of a structured signal from independent random linear measurements. In Sampling Theory, a Renaissance",
      "author" : [ "J.A. Tropp" ],
      "venue" : "(To Appear),",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2015
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "Compressed Sensing,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2012
    }, {
      "title" : "Estimation in high dimensions: A geometric perspective",
      "author" : [ "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2014
    }, {
      "title" : "Sharp thresholds for noisy and high-dimensional recovery of sparsity using `1constrained quadratic programming(Lasso)",
      "author" : [ "M.J. Wainwright" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2009
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M J Wainwright", "M I Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2008
    }, {
      "title" : "On model selection consistency of Lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2006
    }, {
      "title" : "Restricted eigenvalue conditions on subgaussian random matrices",
      "author" : [ "S. Zhou" ],
      "venue" : "Technical report, Department of Mathematics, ETH Zurich, December",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 38,
      "context" : "Such estimators are usually of the form [39, 29, 9]: θ̂λn = argmin θ∈Rp L(θ;Z) + λnR(θ) , (1)",
      "startOffset" : 40,
      "endOffset" : 51
    }, {
      "referenceID" : 28,
      "context" : "Such estimators are usually of the form [39, 29, 9]: θ̂λn = argmin θ∈Rp L(θ;Z) + λnR(θ) , (1)",
      "startOffset" : 40,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "Such estimators are usually of the form [39, 29, 9]: θ̂λn = argmin θ∈Rp L(θ;Z) + λnR(θ) , (1)",
      "startOffset" : 40,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "Recent work has viewed such characterizations in terms of atomic norms, which give the tightest convex relaxation of a structured set of atoms in which θ∗ belongs [14].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 44,
      "context" : "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 42,
      "context" : "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 25,
      "context" : "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].",
      "startOffset" : 185,
      "endOffset" : 196
    }, {
      "referenceID" : 28,
      "context" : "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].",
      "startOffset" : 185,
      "endOffset" : 196
    }, {
      "referenceID" : 32,
      "context" : "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].",
      "startOffset" : 185,
      "endOffset" : 196
    }, {
      "referenceID" : 32,
      "context" : "While much of the development has focussed on isotropic Gaussian design matrices, recent work has extended the analysis for L1 norm to correlated Gaussian designs [33] as well as anisotropic sub-Gaussian design matrices [34].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 33,
      "context" : "While much of the development has focussed on isotropic Gaussian design matrices, recent work has extended the analysis for L1 norm to correlated Gaussian designs [33] as well as anisotropic sub-Gaussian design matrices [34].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 28,
      "context" : "Building on such development, [29] presents a unified framework for the case of decomposable norms and also considers generalized linear models (GLMs) for certain norms such as L1.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 28,
      "context" : "Two key insights are offered in [29]: first, the error vector ∆̂n lies in a restricted set, a cone or a star, for suitably large λn, and second, the loss function needs to satisfy restricted strong convexity (RSC), a generalization of the RE condition, on the restricted error set for the analysis to work out.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon’s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon’s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon’s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 23,
      "context" : "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon’s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 30,
      "context" : "[31] considers three related formulations for generalized Lasso problems, establish recovery guarantees based on Gordon’s inequality, and quantities related to the Gaussian width.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Sharper analysis for recovery has been considered in [1], yielding a precise characterization of phase transition behavior using quantities related to the Gaussian width.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 31,
      "context" : "[32] consider a linear programming estimator in a 1-bit compressed sensing setting and, interestingly, the concept of Gaussian width shows up in the analysis.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "In spite of the advances, with a few notable exceptions [40, 42], most existing results are restricted to isotropic Gaussian design matrices.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 41,
      "context" : "In spite of the advances, with a few notable exceptions [40, 42], most existing results are restricted to isotropic Gaussian design matrices.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(θ) under suitable constraints determined by the noise (y − Xθ) and/or the design matrix X [13, 6, 14, 15].",
      "startOffset" : 249,
      "endOffset" : 264
    }, {
      "referenceID" : 5,
      "context" : "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(θ) under suitable constraints determined by the noise (y − Xθ) and/or the design matrix X [13, 6, 14, 15].",
      "startOffset" : 249,
      "endOffset" : 264
    }, {
      "referenceID" : 13,
      "context" : "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(θ) under suitable constraints determined by the noise (y − Xθ) and/or the design matrix X [13, 6, 14, 15].",
      "startOffset" : 249,
      "endOffset" : 264
    }, {
      "referenceID" : 14,
      "context" : "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(θ) under suitable constraints determined by the noise (y − Xθ) and/or the design matrix X [13, 6, 14, 15].",
      "startOffset" : 249,
      "endOffset" : 264
    }, {
      "referenceID" : 14,
      "context" : "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15], which generalizes the Dantzig selector [11] corresponding to the L1 norm, and is given by: θ̂γn = argmin θ∈Rp R(θ) s.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15], which generalizes the Dantzig selector [11] corresponding to the L1 norm, and is given by: θ̂γn = argmin θ∈Rp R(θ) s.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 13,
      "context" : "One can show [14, 15] that the restricted error set for such constrained estimators are of the form: Ec = {∆ ∈ R | R(θ∗ + ∆) ≤ R(θ∗)} .",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "One can show [14, 15] that the restricted error set for such constrained estimators are of the form: Ec = {∆ ∈ R | R(θ∗ + ∆) ≤ R(θ∗)} .",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 23,
      "context" : "Then, with Ar = Er ∩ B p 2 , Ac = Ec ∩ B p 2 , and Āc = cone(Ec) ∩ B p 2 , assuming ‖θ‖2 = 1, β = 2, we show that w(Ac) ≤ w(Ar) ≤ 3w(Āc) , (5) where w(A) = Eg[supa∈A〈a, g〉], with g ∼ N(0, Ip×p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].",
      "startOffset" : 271,
      "endOffset" : 286
    }, {
      "referenceID" : 4,
      "context" : "Then, with Ar = Er ∩ B p 2 , Ac = Ec ∩ B p 2 , and Āc = cone(Ec) ∩ B p 2 , assuming ‖θ‖2 = 1, β = 2, we show that w(Ac) ≤ w(Ar) ≤ 3w(Āc) , (5) where w(A) = Eg[supa∈A〈a, g〉], with g ∼ N(0, Ip×p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].",
      "startOffset" : 271,
      "endOffset" : 286
    }, {
      "referenceID" : 13,
      "context" : "Then, with Ar = Er ∩ B p 2 , Ac = Ec ∩ B p 2 , and Āc = cone(Ec) ∩ B p 2 , assuming ‖θ‖2 = 1, β = 2, we show that w(Ac) ≤ w(Ar) ≤ 3w(Āc) , (5) where w(A) = Eg[supa∈A〈a, g〉], with g ∼ N(0, Ip×p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].",
      "startOffset" : 271,
      "endOffset" : 286
    }, {
      "referenceID" : 36,
      "context" : "Then, with Ar = Er ∩ B p 2 , Ac = Ec ∩ B p 2 , and Āc = cone(Ec) ∩ B p 2 , assuming ‖θ‖2 = 1, β = 2, we show that w(Ac) ≤ w(Ar) ≤ 3w(Āc) , (5) where w(A) = Eg[supa∈A〈a, g〉], with g ∼ N(0, Ip×p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].",
      "startOffset" : 271,
      "endOffset" : 286
    }, {
      "referenceID" : 5,
      "context" : "For the special case of L1 norm, [6] considered a simultaneous analysis of the Lasso and the Dantzig selector, and characterized the structure of the error sets for regularized and constrained sets for the special case of L1 norm.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Further, while the characterization in [6] was also geometric, it was not based on Gaussian widths.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "The second assumption is that the design matrix X ∈ Rn×p satisfies the restricted strong convexity (RSC) condition [6, 29] in the error set Er, in particular, there exists a suitable constant κ > 0 so that δL(∆, θ∗) , L(θ∗ + ∆)− L(θ∗)− 〈∇L(θ∗),∆〉 ≥ κ‖∆‖2 ∀∆ ∈ Er .",
      "startOffset" : 115,
      "endOffset" : 122
    }, {
      "referenceID" : 28,
      "context" : "The second assumption is that the design matrix X ∈ Rn×p satisfies the restricted strong convexity (RSC) condition [6, 29] in the error set Er, in particular, there exists a suitable constant κ > 0 so that δL(∆, θ∗) , L(θ∗ + ∆)− L(θ∗)− 〈∇L(θ∗),∆〉 ≥ κ‖∆‖2 ∀∆ ∈ Er .",
      "startOffset" : 115,
      "endOffset" : 122
    }, {
      "referenceID" : 28,
      "context" : "where ψ(Er) = supu∈Er R(u) ‖u‖2 is a norm compatibility constant [29], and c > 0 is a constant.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 35,
      "context" : "Recent work in [36] has extended the analyses for sub-exponential distributions.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 36,
      "context" : "Interestingly, for sub-Gaussian designs, one obtains the results in terms of the ‘sub-Gaussian width’ of the unit norm-ball, which can be upper bounded by a constant times the Gaussian width using generic chaining [37].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 36,
      "context" : "Further, one can get high-probability versions of these bounds using related advances in generic chaining [37, 38].",
      "startOffset" : 106,
      "endOffset" : 114
    }, {
      "referenceID" : 37,
      "context" : "Further, one can get high-probability versions of these bounds using related advances in generic chaining [37, 38].",
      "startOffset" : 106,
      "endOffset" : 114
    }, {
      "referenceID" : 28,
      "context" : "For the special case of L1 regularization, ΩR is the unit L1 norm ball, and the corresponding Gaussian width w(ΩR) ≤ c1 √ log p, which explains the √ log p term one finds in existing bounds for Lasso [29, 9].",
      "startOffset" : 200,
      "endOffset" : 207
    }, {
      "referenceID" : 8,
      "context" : "For the special case of L1 regularization, ΩR is the unit L1 norm ball, and the corresponding Gaussian width w(ΩR) ≤ c1 √ log p, which explains the √ log p term one finds in existing bounds for Lasso [29, 9].",
      "startOffset" : 200,
      "endOffset" : 207
    }, {
      "referenceID" : 36,
      "context" : "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 37,
      "context" : "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 27,
      "context" : "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 28,
      "context" : "is determined by the Restricted Strong Convexity (RSC) condition [29].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "The result is thus a considerable generalization of earlier results on convex losses, such as GLMs, which had looked at specific norms and associated cones and/or did not express the results in terms of the Gaussian width of A [29].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 28,
      "context" : "Further, note that the condition in (15) is similar to that in [29] for β = 2, but the above characterization holds for any norm, not just decomposable norms [29].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "Further, note that the condition in (15) is similar to that in [29] for β = 2, but the above characterization holds for any norm, not just decomposable norms [29].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].",
      "startOffset" : 132,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].",
      "startOffset" : 132,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].",
      "startOffset" : 132,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].",
      "startOffset" : 132,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15] given by: θ̂γn = argmin θ∈Rp R(θ) s.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {∆ ∈ R | R(θ∗ + ∆) ≤ R(θ∗)} .",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {∆ ∈ R | R(θ∗ + ∆) ≤ R(θ∗)} .",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {∆ ∈ R | R(θ∗ + ∆) ≤ R(θ∗)} .",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {∆ ∈ R | R(θ∗ + ∆) ≤ R(θ∗)} .",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 39,
      "context" : "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {∆ ∈ R | R(θ∗ + ∆) ≤ R(θ∗)} .",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "Related observations have been made for the special case of the L1 norm [6], although past work did not provide an explicit characterization in terms of Gaussian widths.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : "In recent work, [29] considered regularized regression with the special case of decomposable norms, defined in terms of a pair of subspacesM ⊆ M̄ of Rp.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 28,
      "context" : "We show that for decomposable norms, the error set Er in our analysis is included in the error cone defined in [29].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 28,
      "context" : "The last inequality is precisely the error cone in [29] for θ∗ ∈ M.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 28,
      "context" : "where Ψ(M̄) is the subspace compatibility in M̄, as used in [29].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 40,
      "context" : "We also recall the definition of the sub-Gaussian norm for a sub-Gaussian variable x, |||x|||ψ2 = supp≥1 1 √ p(E[|x| p])1/p [41].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "The motivation behind getting an upper bound of the Gaussian width w(ΩR) of the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances have been made in recent years in upper bounding Gaussian widths of such cones [14, 1].",
      "startOffset" : 253,
      "endOffset" : 260
    }, {
      "referenceID" : 0,
      "context" : "The motivation behind getting an upper bound of the Gaussian width w(ΩR) of the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances have been made in recent years in upper bounding Gaussian widths of such cones [14, 1].",
      "startOffset" : 253,
      "endOffset" : 260
    }, {
      "referenceID" : 5,
      "context" : ", L(θ;Zn) = 1 n‖y − Xθ‖ 2, the RSC condition (20) becomes equivalent to the Restricted Eigenvalue (RE) condition [6, 29], since",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : ", L(θ;Zn) = 1 n‖y − Xθ‖ 2, the RSC condition (20) becomes equivalent to the Restricted Eigenvalue (RE) condition [6, 29], since",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].",
      "startOffset" : 147,
      "endOffset" : 158
    }, {
      "referenceID" : 32,
      "context" : "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].",
      "startOffset" : 147,
      "endOffset" : 158
    }, {
      "referenceID" : 33,
      "context" : "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].",
      "startOffset" : 147,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "The RE/RIP conditions for independent isotropic Gaussian designs have been widely studied for the case of L1 norm [10, 6].",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "The RE/RIP conditions for independent isotropic Gaussian designs have been widely studied for the case of L1 norm [10, 6].",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 32,
      "context" : "The generalization to RE condition for correlated Gaussian designs for the special of L1 norm was studied in [33].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "[14] consider the more general context of atomic norms, and RE condition analysis applies to any spherical cap A, with sample complexity results in terms of w(A), the Gaussian width of A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "However, the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.",
      "startOffset" : 52,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "However, the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.",
      "startOffset" : 52,
      "endOffset" : 64
    }, {
      "referenceID" : 23,
      "context" : "However, the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.",
      "startOffset" : 52,
      "endOffset" : 64
    }, {
      "referenceID" : 45,
      "context" : "Progress has been made on establishing RE conditions for sub-Gaussian designs for error sets/caps corresponding to specific norms such as L1 [46].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 33,
      "context" : "In recent work, RE conditions were developed for anisotropic sub-Gaussian designs for the L1 norm [34].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "Further, recent work have pointed out the differences between the RE and the RIP condition, which gives a two-sided bound on quadratic forms of random matrices [29].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 32,
      "context" : "In fact, all existing RE results do implicitly have the width term, but in a form specific to the chosen norm [33, 34].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 33,
      "context" : "In fact, all existing RE results do implicitly have the width term, but in a form specific to the chosen norm [33, 34].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon’s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 36,
      "context" : "The proof technique we use is an application of generic chaining [37, 38].",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 37,
      "context" : "The proof technique we use is an application of generic chaining [37, 38].",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 20,
      "context" : "The specific form we utilize was originally developed in [21, 28].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 27,
      "context" : "The specific form we utilize was originally developed in [21, 28].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 32,
      "context" : "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use",
      "startOffset" : 165,
      "endOffset" : 173
    }, {
      "referenceID" : 28,
      "context" : "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use",
      "startOffset" : 165,
      "endOffset" : 173
    }, {
      "referenceID" : 33,
      "context" : "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 16,
      "context" : "Further, the RIP results can be viewed as a generalization of the celebrated Johnson-Lindenstrauss (JL) lemma [17], and the interested reader can explore these connections in [21].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 20,
      "context" : "Further, the RIP results can be viewed as a generalization of the celebrated Johnson-Lindenstrauss (JL) lemma [17], and the interested reader can explore these connections in [21].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 9,
      "context" : "More generally, choosing = cw(A)/ √ n, one can write the result in a traditional RIP form [10].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 32,
      "context" : "Finally, it is instructive to compare the above result to existing characterizations of the RE condition for anisotropic Gaussian [33] and anisotropic sub-Gaussian [34] designs, focused on the L1 norm.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 33,
      "context" : "Finally, it is instructive to compare the above result to existing characterizations of the RE condition for anisotropic Gaussian [33] and anisotropic sub-Gaussian [34] designs, focused on the L1 norm.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 15,
      "context" : "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 13,
      "context" : "where (a) is obtained from the fact that Gaussian width ofG(θ̃) with θ̃ be a s-sparse vector is √ 2s log(ps ) + 5 4s [14].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "which is similar to the results obtained in well known results [14, 29].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 28,
      "context" : "which is similar to the results obtained in well known results [14, 29].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 28,
      "context" : "As shown in [29] Group norm is a decomposable norm.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "where m = max t |Gt| and (a) is obtained from the fact that Gaussian width of G(θ̃) where θ̃ has k active group is √ 2k(m+ log(T − k)) + k [14].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "which is similar to the results obtained in previous works [14, 29].",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "which is similar to the results obtained in previous works [14, 29].",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "is the log-partition function [8, 4, 44].",
      "startOffset" : 30,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "is the log-partition function [8, 4, 44].",
      "startOffset" : 30,
      "endOffset" : 40
    }, {
      "referenceID" : 43,
      "context" : "is the log-partition function [8, 4, 44].",
      "startOffset" : 30,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "the natural parameter ηi = 〈Xi, θ∗〉 gives the expectation of the response [4, 8], i.",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "the natural parameter ηi = 〈Xi, θ∗〉 gives the expectation of the response [4, 8], i.",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "Note that for GLMs over discrete responses yi, the integration needs to be suitably changed to summation [4, 8].",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "Note that for GLMs over discrete responses yi, the integration needs to be suitably changed to summation [4, 8].",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "i=1 ∇2φ(〈θ∗, Xi〉+ γi〈u,Xi〉)〈u,Xi〉 , where γi ∈ [0, 1], and where the last equality follows from a direct application of the mean value theorem [35].",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 34,
      "context" : "i=1 ∇2φ(〈θ∗, Xi〉+ γi〈u,Xi〉)〈u,Xi〉 , where γi ∈ [0, 1], and where the last equality follows from a direct application of the mean value theorem [35].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "Since the log-partition function φ is of Legendre type [3, 4, 8], the second derivative ∇2φ(·) is always positive.",
      "startOffset" : 55,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "Since the log-partition function φ is of Legendre type [3, 4, 8], the second derivative ∇2φ(·) is always positive.",
      "startOffset" : 55,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "Since the log-partition function φ is of Legendre type [3, 4, 8], the second derivative ∇2φ(·) is always positive.",
      "startOffset" : 55,
      "endOffset" : 64
    }, {
      "referenceID" : 41,
      "context" : "Assuming X has isotropic sub-Gaussian rows with |||Xi|||φ2 ≤ κ, 〈Xi, θ ∗〉 and 〈Xi, u〉 are sub-Gaussian random variables with sub-Gaussian norm at most Cκ [42].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 28,
      "context" : "Note that RSC analysis for GLMs was considered in [29] for specific norms, especially L1, whereas our analysis applies to any set A ⊆ Sp−1, hence to any norm, and the result is in terms of the Gaussian width w(A) of A.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 28,
      "context" : "The work can be viewed as a direct generalization of results in [29], which presented related results for decomposable norms.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "Further, the error sets for regularized and constrained versions of such problems are shown to be closely related [6].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 29,
      "context" : "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 26,
      "context" : "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 21,
      "context" : "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].",
      "startOffset" : 263,
      "endOffset" : 275
    }, {
      "referenceID" : 22,
      "context" : "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].",
      "startOffset" : 263,
      "endOffset" : 275
    }, {
      "referenceID" : 26,
      "context" : "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].",
      "startOffset" : 263,
      "endOffset" : 275
    }, {
      "referenceID" : 17,
      "context" : "Since real-world several problems, including spatial and temporal problems, do have correlated observations, it will be important to investigate estimators which perform well in such settings [18].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 19,
      "context" : "In several of our proofs, we use the concept of Gaussian width [20, 14], which is defined as follows.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "In several of our proofs, we use the concept of Gaussian width [20, 14], which is defined as follows.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 36,
      "context" : "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 37,
      "context" : "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 6,
      "context" : "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 23,
      "context" : "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 40,
      "context" : "The following definitions and lemmas are from [41].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "Recall from [14], that the error set for the constrained setting for atomic norms is a cone given by: Cc = Cc(θ ∗) = cone(Ec) = cone {∆ ∈ R | R(θ∗ + ∆) ≤ R(θ∗)} .",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "Also, recall that any norm is convex, since by triangle inequality, for t ∈ [0, 1], we have R(tθ1 + (1− t)θ2) ≤ R(tθ1) +R((1− t)θ2) = tR(θ1) + (1− t)R(θ2) .",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 32,
      "context" : "One can view (ii) as a special case of (i), but we highlight this special case because of its practical importance and past literature on RE conditions for anisotropic subGaussian designs [33, 34].",
      "startOffset" : 188,
      "endOffset" : 196
    }, {
      "referenceID" : 33,
      "context" : "One can view (ii) as a special case of (i), but we highlight this special case because of its practical importance and past literature on RE conditions for anisotropic subGaussian designs [33, 34].",
      "startOffset" : 188,
      "endOffset" : 196
    }, {
      "referenceID" : 27,
      "context" : "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand’s generic chaining [37, 38].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand’s generic chaining [37, 38].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 36,
      "context" : "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand’s generic chaining [37, 38].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 37,
      "context" : "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand’s generic chaining [37, 38].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "We specifically focus on results in [28] which provide uniform bounds on the supremum of certain empirical processes.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "The results in [28], and more generally in generic chaining [37, 38], are based on certain γ-functionals which we briefly introduce below.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 36,
      "context" : "The results in [28], and more generally in generic chaining [37, 38], are based on certain γ-functionals which we briefly introduce below.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 37,
      "context" : "The results in [28], and more generally in generic chaining [37, 38], are based on certain γ-functionals which we briefly introduce below.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 27,
      "context" : "Theorem 10 (Mendelson, Pajor, Tomczak-Jaegermann [28]) There exist absolute constants c1, c2, c3 for which the following holds.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "The canonical density function of exponential family distributions is given by [4, 8, 44]:",
      "startOffset" : 79,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "The canonical density function of exponential family distributions is given by [4, 8, 44]:",
      "startOffset" : 79,
      "endOffset" : 89
    }, {
      "referenceID" : 43,
      "context" : "The canonical density function of exponential family distributions is given by [4, 8, 44]:",
      "startOffset" : 79,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].",
      "startOffset" : 125,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].",
      "startOffset" : 125,
      "endOffset" : 135
    }, {
      "referenceID" : 43,
      "context" : "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].",
      "startOffset" : 125,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "Considering the negative log-likelihood, the GLM corresponding to the Gaussian distribution yields least squares regression [8].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "Considering the negative log-likelihood, the GLM corresponding to the Bernoulli distribution yields logistic regression [8].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "Considering the negative log-likelihood, the GLM corresponding to the Poisson distribution yields Poisson regression [8].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "for suitable γi ∈ [0, 1].",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "where γi ∈ [0, 1].",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : "Acknowledgements: We thank the reviewers of the conference version [2] for helpful comments and suggestions on related work.",
      "startOffset" : 67,
      "endOffset" : 70
    } ],
    "year" : 2015,
    "abstractText" : "Analysis of non-asymptotic estimation error and structured statistical recovery based on norm regularized regression, such as Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise model. This paper presents generalizations of such estimation error analysis on all four aspects compared to the existing literature. We characterize the restricted error set where the estimation error vector lies, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to any norm. Precise characterizations of the bound is presented for isotropic as well as anisotropic subGaussian design matrices, subGaussian noise models, and convex loss functions, including least squares and generalized linear models. Generic chaining and associated results play an important role in the analysis. A key result from the analysis is that the sample complexity of all such estimators depends on the Gaussian width of a spherical cap corresponding to the restricted error set. Further, once the number of samples n crosses the required sample complexity, the estimation error decreases as c √ n , where c depends on the Gaussian width of the unit norm ball.",
    "creator" : "LaTeX with hyperref package"
  }
}