{
  "name" : "0811.1790.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robust Regression and Lasso",
    "authors" : [ "Huan Xu", "Constantine Caramanis" ],
    "emails" : [ "(xuhuan@cim.mcgill.ca;", "shie.mannor@mcgill.ca)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :0\n81 1.\n17 90\nv1 [\ncs .I\nT ]\n1 1\nN ov\n2 00\n8 1\nSecondly, robustness can itself be used as an avenue to exploring different properties of the solution. In particular, it is shown that robustness of the solution explains why the solution is sparse. The analysis as well as the specific results obtained differ from standard sparsity results, providing different geometric intuition. Furthermore, it is shown that the robust optimization formulation is related to kernel density estimation, and based on this approach, a proof that Lasso is consistent is given using robustness directly. Finally, a theorem saying that sparsity and algorithmic stability contradict each other, and hence Lasso is not stable, is presented.\nIndex Terms\nStatistical Learning, Regression, Regularization, Kernel density estimator, Lasso, Robustness, Sparsity, Stability.\nI. INTRODUCTION\nIn this paper we consider linear regression problems with least-square error. The problem is to find a vector x so that the ℓ2 norm of the residual b − Ax is minimized, for a given matrix A ∈ Rn×m and vector b ∈ Rn. From a learning/regression perspective, each row of A can be regarded as a training sample, and the corresponding element of b as the target value of this observed sample. Each column of A corresponds to a feature, and the objective is to find a set of weights so that the weighted sum of the feature values approximates the target value.\nIt is well known that minimizing the least squared error can lead to sensitive solutions [1]– [4]. Many regularization methods have been proposed to decrease this sensitivity. Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms. These methods minimize a weighted sum of the residual norm and a certain regularization term, ‖x‖2 for Tikhonov regularization and ‖x‖1 for Lasso. In addition to providing regularity, Lasso is also known for the tendency to select sparse solutions. Recently this has attracted much attention for its ability to reconstruct sparse solutions when sampling occurs far below the Nyquist rate, and\nA preliminary version of this paper was presented at Twenty-Second Annual Conference on Neural Information Processing Systems.\nH. Xu and S. Mannor are with the Department of Electrical and Computer Engineering, McGill University, Montréal, H3A2A7, Canada email: (xuhuan@cim.mcgill.ca; shie.mannor@mcgill.ca)\nC. Caramanis is with the Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712 USA email: (cmcaram@ece.utexas.edu).\nNovember 11, 2008 DRAFT\n2 also for its ability to recover the sparsity pattern exactly with probability one, asymptotically as the number of observations increases (there is an extensive literature on this subject, and we refer the reader to [8]–[12] and references therein).\nThe first result of this paper is that the solution to Lasso has robustness properties: it is the solution to a robust optimization problem. In itself, this interpretation of Lasso as the solution to a robust least squares problem is a development in line with the results of [13]. There, the authors propose an alternative approach of reducing sensitivity of linear regression by considering a robust version of the regression problem, i.e., minimizing the worst-case residual for the observations under some unknown but bounded disturbance. Most of the research in this area considers either the case where the disturbance is row-wise uncoupled [14], or the case where the Frobenius norm of the disturbance matrix is bounded [13].\nNone of these robust optimization approaches produces a solution that has sparsity properties (in particular, the solution to Lasso does not solve any of these previously formulated robust optimization problems). In contrast, we investigate the robust regression problem where the uncertainty set is defined by feature-wise constraints. Such a noise model is of interest when values of features are obtained with some noisy pre-processing steps, and the magnitudes of such noises are known or bounded. Another situation of interest is where features are meaningfully coupled. We define coupled and uncoupled disturbances and uncertainty sets precisely in Section II-A below. Intuitively, a disturbance is feature-wise coupled if the variation or disturbance across features satisfy joint constraints, and uncoupled otherwise.\nConsidering the solution to Lasso as the solution of a robust least squares problem has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows more principled selection of the regularizer, and in particular, considering different uncertainty sets, we construct generalizations of Lasso that also yield convex optimization problems.\nSecondly, and perhaps most significantly, robustness is a strong property that can itself be used as an avenue to investigating different properties of the solution. We show that robustness of the solution can explain why the solution is sparse. The analysis as well as the specific results we obtain differ from standard sparsity results, providing different geometric intuition, and extending beyond the least-squares setting. Sparsity results obtained for Lasso ultimately depend on the fact that introducing additional features incurs larger ℓ1-penalty than the least squares error reduction. In contrast, we exploit the fact that a robust solution is, by definition, the optimal solution under a worst-case perturbation. Our results show that, essentially, a coefficient of the solution is nonzero if the corresponding feature is relevant under all allowable perturbations. In addition to sparsity, we also use robustness directly to prove consistency of Lasso.\nWe briefly list the main contributions as well as the organization of this paper. • In Section II, we formulate the robust regression problem with feature-wise independent\ndisturbances, and show that this formulation is equivalent to a least-square problem with a weighted ℓ1 norm regularization term. Hence, we provide an interpretation of Lasso from a robustness perspective. • We generalize the robust regression formulation to loss functions of arbitrary norm in Section III. We also consider uncertainty sets that require disturbances of different features to satisfy joint conditions. This can be used to mitigate the conservativeness of the robust solution and to obtain solutions with additional properties. • In Section IV, we present new sparsity results for the robust regression problem with feature-wise independent disturbances. This provides a new robustness-based explanation\nNovember 11, 2008 DRAFT\n3 to the sparsity of Lasso. Our approach gives new analysis and also geometric intuition, and furthermore allows one to obtain sparsity results for more general loss functions, beyond the squared loss. • Next, we relate Lasso to kernel density estimation in Section V. This allows us to re-prove consistency in a statistical learning setup, using the new robustness tools and formulation we introduce. Along with our results on sparsity, this illustrates the power of robustness in explaining and also exploring different properties of the solution. • Finally, we prove in Section VI a “no-free-lunch” theorem, stating that an algorithm that encourages sparsity cannot be stable.\nNotation. We use capital letters to represent matrices, and boldface letters to represent column vectors. Row vectors are represented as the transpose of column vectors. For a vector z, zi denotes its ith element. Throughout the paper, ai and r⊤j are used to denote the i\nth column and the jth row of the observation matrix A, respectively. We use aij to denote the ij element of A, hence it is the jth element of ri, and ith element of aj . For a convex function f(·), ∂f(z) represents any of its sub-gradients evaluated at z. A vector with length n and each element equals 1 is denoted as 1n.\nII. ROBUST REGRESSION WITH FEATURE-WISE DISTURBANCE\nIn this section, we show that our robust regression formulation recovers Lasso as a special case. We also derive probabilistic bounds that guide in the construction of the uncertainty set.\nThe regression formulation we consider differs from the standard Lasso formulation, as we minimize the norm of the error, rather than the squared norm. It is known that these two coincide up to a change of the regularization coefficient. Yet as we discuss above, our results lead to more flexible and potentially powerful robust formulations, and give new insight into known results."
    }, {
      "heading" : "A. Formulation",
      "text" : "Robust linear regression considers the case where the observed matrix is corrupted by some potentially malicious disturbance. The objective is to find the optimal solution in the worst case sense. This is usually formulated as the following min-max problem,\nRobust Linear Regression:\nmin x∈Rm\n{\nmax ∆A∈U\n‖b− (A+∆A)x‖2 } , (1)\nwhere U is called the uncertainty set, or the set of admissible disturbances of the matrix A. In this section, we consider the class of uncertainty sets that bound the norm of the disturbance to each feature, without placing any joint requirements across feature disturbances. That is, we consider the class of uncertainty sets:\nU , { (δ1, · · · , δm) ∣ ∣ ∣ ‖δi‖2 ≤ ci, i = 1, · · · , m } , (2)\nfor given ci ≥ 0. We call these uncertainty sets feature-wise uncoupled, in contrast to coupled uncertainty sets that require disturbances of different features to satisfy some joint constraints (we discuss these extensively below, and their significance). While the inner maximization problem of (1) is nonconvex, we show in the next theorem that uncoupled norm-bounded uncertainty sets lead to an easily solvable optimization problem.\nNovember 11, 2008 DRAFT\n4 Theorem 1: The robust regression problem (1) with uncertainty set of the form (2) is equivalent to the following ℓ1 regularized regression problem:\nmin x∈Rm\n{ ‖b− Ax‖2 + m ∑\ni=1\nci|xi| } . (3)\nProof: Fix x∗. We prove that max∆A∈U ‖b− (A+∆A)x∗‖2 = ‖b−Ax∗‖2 + ∑m i=1 ci|x∗i |. The left hand side can be written as\nmax ∆A∈U\n‖b− (A +∆A)x∗‖2\n= max (δ1,··· ,δm)|‖δi‖2≤ci\n∥ ∥ ∥ b − ( A+ (δ1, · · · , δm) ) x∗ ∥ ∥ ∥\n2\n= max (δ1,··· ,δm)|‖δi‖2≤ci\n‖b− Ax∗ − m ∑\ni=1\nx∗iδi‖2\n≤ max (δ1,··· ,δm)|‖δi‖2≤ci\n∥ ∥\n∥ b −Ax∗\n∥ ∥ ∥\n2 +\nm ∑\ni=1\n‖x∗i δi‖2\n≤‖b− Ax∗‖2 + m ∑\ni=1\n|x∗i |ci.\n(4)\nNow, let\nu ,\n{ b−Ax∗ ‖b−Ax∗‖2 if Ax ∗ 6= b, any vector with unit ℓ2 norm otherwise;\nand let δ ∗ i , −cisgn(x∗i )u.\nObserve that ‖δ∗i ‖2 ≤ ci, hence ∆A∗ , (δ∗1, · · · , δ∗m) ∈ U . Notice that max ∆A∈U\n‖b− (A+∆A)x∗‖2 ≥‖b − (A+∆A∗)x∗‖2 = ∥ ∥ ∥ b− ( A + (δ∗1, · · · , δ∗m) ) x∗ ∥ ∥ ∥\n2\n= ∥ ∥ ∥ (b− Ax∗)−\nm ∑\ni=1\n( − x∗i cisgn(x∗i )u )\n∥ ∥ ∥\n2\n= ∥ ∥ ∥ (b− Ax∗) + (\nm ∑\ni=1\nci|x∗i |)u ∥ ∥ ∥\n2\n=‖b −Ax∗‖2 + m ∑\ni=1\nci|x∗i |.\n(5)\nThe last equation holds from the definition of u. Combining Inequalities (4) and (5), establishes the equality max∆A∈U ‖b− (A+∆A)x∗‖2 = ‖b−Ax∗‖2 + ∑m\ni=1 ci|x∗i | for any x∗. Minimizing over x on both sides proves the theorem. Taking ci = c and normalizing ai for all i, Problem (3) recovers the well-known Lasso [6], [7].\nNovember 11, 2008 DRAFT\n5"
    }, {
      "heading" : "B. Uncertainty Set Construction",
      "text" : "The selection of an uncertainty set U in Robust Optimization is of fundamental importance. One way this can be done is as an approximation of so-called chance constraints, where a deterministic constraint is replaced by the requirement that a constraint is satisfied with at least some probability. These can be formulated when we know the distribution exactly, or when we have only partial information of the uncertainty, such as, e.g., first and second moments. This chance-constraint formulation is particularly important when the distribution has large support, rendering the naive robust optimization formulation overly pessimistic.\nFor confidence level η, the chance constraint formulation becomes:\nminimize: t\nSubject to: Pr(‖b − (A+∆A)x‖2 ≤ t) ≥ 1− η. Here, x and t are the decision variables.\nConstructing the uncertainty set for feature i can be done quickly via line search and bisection, as long as we can evaluate Pr(‖ai‖2 ≥ c). If we know the distribution exactly (i.e., if we have complete probabilistic information), this can be quickly done via sampling. Another setting of interest is when we have access only to some moments of the distribution of the uncertainty, e.g., the mean and variance. In this setting, the uncertainty sets are constructed via a bisection procedure which evaluates the worst-case probability over all distributions with given mean and variance. We do this using a tight bound on the probability of an event, given the first two moments.\nIn the scalar case, the Markov Inequality provides such a bound. The next theorem is a generalization of the Markov inequality to Rn, which bounds the probability where the disturbance on a given feature is more than ci, if only the first and second moment of the random variable are known. We postpone the proof to the appendix, and refer the reader to [15] for similar results using semi-definite optimization.\nTheorem 2: Consider a random vector v ∈ Rn, such that E(v) = a, and E(vv⊤) = Σ, Σ 0. Then we have\nPr{‖v‖2 ≥ ci} ≤\n\n     \n      \nminP,q,r,λ Trace(ΣP ) + 2q ⊤a + r\nsubject to:\n(\nP q q⊤ r\n)\n0 (\nI(m) 0 0⊤ −c2i\n)\nλ (\nP q q⊤ r − 1\n)\nλ ≥ 0.\n(6)\nThe optimization problem (6) is a semi-definite programming, which is known be solved in polynomial time. Furthermore, if we replace E(vv⊤) = Σ by an inequality E(vv⊤) ≤ Σ, the uniform bound still holds. Thus, even if our estimation to the variance is not precise, we are still able to bound the probability of having “large” disturbance.\nIII. GENERAL UNCERTAINTY SETS\nOne reason the robust optimization formulation is powerful, is that having provided the connection to Lasso, it then allows the opportunity to generalize to efficient “Lasso-like” regularization algorithms.\nNovember 11, 2008 DRAFT\n6 In this section, we make several generalizations of the robust formulation (1) and derive counterparts of Theorem 1. We generalize the robust formulation in two ways: (a) to the case of arbitrary norm; and (b) to the case of coupled uncertainty sets.\nWe first consider the case of an arbitrary norm ‖ · ‖a of Rn as a cost function rather than the squared loss. The proof of the next theorem is identical to that of Theorem 1, with only the ℓ2 norm changed to ‖ · ‖a. Theorem 3: The robust regression problem\nmin x∈Rm\n{\nmax ∆A∈Ua\n‖b − (A+∆A)x‖a } ; Ua , { (δ1, · · · , δm) ∣ ∣ ∣ ‖δi‖a ≤ ci, i = 1, · · · , m } ;\nis equivalent to the following regularized regression problem\nmin x∈Rm\n{ ‖b −Ax‖a + m ∑\ni=1\nci|xi| } .\nWe next remove the assumption that the disturbances are feature-wise uncoupled. Allowing coupled uncertainty sets is useful when we have some additional information about potential noise in the problem, and we want to limit the conservativeness of the worst-case formulation. Consider the following uncertainty set:\nU ′ , { (δ1, · · · , δm) ∣ ∣fj(‖δ1‖a, · · · , ‖δm‖a) ≤ 0; j = 1, · · · , k } ,\nwhere fj(·) are convex functions. Notice that, both k and fj can be arbitrary, hence this is a very general formulation, and provides us with significant flexibility in designing uncertainty sets and equivalently new regression algorithms (see for example Corollary 1 and 2). The following theorem converts this formulation to tractable optimization problems. The proof is postponed to the appendix.\nTheorem 4: Assume that the set\nZ , {z ∈ Rm|fj(z) ≤ 0, j = 1, · · · , k; z ≥ 0} has non-empty relative interior. Then the robust regression problem\nmin x∈Rm\n{\nmax ∆A∈U ′\n‖b− (A+∆A)x‖a }\nis equivalent to the following regularized regression problem\nmin λ∈Rk\n+ ,κ∈Rm + ,x∈Rm\n{ ‖b− Ax‖a + v(λ,κ,x) } ;\nwhere: v(λ,κ,x) , max c∈Rm\n[ (κ + |x|)⊤c − k ∑\nj=1\nλjfj(c) ]\n(7)\nRemark: Problem (7) is efficiently solvable. Denote zc(λ,κ,x) , [ (κ+|x|)⊤c− ∑k j=1 λjfj(c) ] .\nThis is a convex function of (λ,κ,x), and the sub-gradient of zc(·) can be computed easily for any c. The function v(λ,κ,x) is the maximum of a set of convex functions, zc(·) , hence is convex, and satisfies\n∂v(λ∗,κ∗,x∗) = ∂zc0(λ∗,κ∗,x∗),\nNovember 11, 2008 DRAFT\n7 where c0 maximizes [ (κ∗ + |x|∗)⊤c − ∑k j=1 λ ∗ jfj(c) ] . We can efficiently evaluate c0 due to convexity of fj(·), and hence we can efficiently evaluate the sub-gradient of v(·). The next two corollaries are a direct application of Theorem 4. Corollary 1: Suppose U ′ = { (δ1, · · · , δm) ∣ ∣ ∣ ∥ ∥‖δ1‖a, · · · , ‖δm‖a ∥ ∥\ns ≤ l;\n}\nfor a symmetric norm\n‖ · ‖s, then the resulting regularized regression problem is\nmin x∈Rm\n{ ‖b−Ax‖a + l‖x‖∗s } ; where ‖ · ‖∗s is the dual norm of ‖ · ‖s.\nThis corollary interprets arbitrary norm-based regularizers from a robust regression perspective. For example, it is straightforward to show that if we take both ‖ · ‖α and ‖ · ‖s as the Euclidean norm, then U ′ is the set of matrices with their Frobenious norms bounded, and Corollary 1 reduces to the robust formulation introduced by [13].\nCorollary 2: Suppose U ′ = { (δ1, · · · , δm) ∣ ∣ ∣ ∃c ≥ 0 : Tc ≤ s; ‖δj‖a ≤ cj; }\n, then the resulting regularized regression problem is\nMinimize: ‖b −Ax‖a + s⊤λ Subject to: x ≤ T⊤λ\n− x ≤ T⊤λ λ ≥ 0.\nUnlike previous results, this corollary considers general polytope uncertainty sets. Advantages of such sets include the linearity of the final formulation. Moreover, the modeling power is considerable, as many interesting disturbances can be modeled in this way.\nWe briefly mention some further examples meant to illustrate the power and flexibility of the robust formulation. We refer the interested reader to [16] for full details.\nAs the results above indicate, the robust formulation can model a broad class of uncertainties, and yield computationally tractable (i.e., convex) problems. In particular, one can use the polytope uncertainty discussed above, to show (see [16]) that by employing an uncertainty set first used in [17], we can model cardinality constrained noise, where some (unknown) subset of at most k features can be corrupted.\nAnother avenue one may take using robustness, and which is also possible to solve easily, is the case where the uncertainty set allows independent perturbation of the columns and the rows of the matrix A. The resulting formulation resembles the elastic-net formulation [18], where there is a combination of ℓ2 and ℓ1 regularization.\nIV. SPARSITY\nIn this section, we investigate the sparsity properties of robust regression (1), and equivalently Lasso. Lasso’s ability to recover sparse solutions has been extensively studied and discussed (cf [8]–[11]). There are generally two approaches. The first approach investigates the problem from a statistical perspective. That is, it assumes that the observations are generated by a (sparse) linear combination of the features, and investigates the asymptotic or probabilistic conditions required for Lasso to correctly recover the generative model. The second approach treats the problem from an optimization perspective, and studies under what conditions a pair (A, b) defines a problem with sparse solutions (e.g., [19]).\nNovember 11, 2008 DRAFT\n8 We follow the second approach and do not assume a generative model. Instead, we consider the conditions that lead to a feature receiving zero weight. Our first result paves the way for the remainder of this section. We show in Theorem 5 that, essentially, a feature receives no weight (namely, x∗i = 0) if there exists an allowable perturbation of that feature which makes it irrelevant. This result holds for general norm loss functions, but in the ℓ2 case, we obtain further geometric results. For instance, using Theorem 5, we show, among other results, that “nearly” orthogonal features get zero weight (Theorem 6). Using similar tools, we provide additional results in [16]. There, we show, among other results, that the sparsity pattern of any optimal solution must satisfy certain angular separation conditions between the residual and the relevant features, and that “nearly” linearly dependent features get zero weight.\nSubstantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]–[11], [20]–[23] and many others). In particular, similar results as in point (a), that rely on an incoherence property, have been established in, e.g., [19], and are used as standard tools in investigating sparsity of Lasso from the statistical perspective. However, a proof exploiting robustness and properties of the uncertainty is novel. Indeed, such a proof shows a fundamental connection between robustness and sparsity, and implies that robustifying w.r.t. a feature-wise independent uncertainty set might be a plausible way to achieve sparsity for other problems.\nTo state the main theorem of this section, from which the other results derive, we introduce some notation to facilitate the discussion. Given a feature-wise uncoupled uncertainty set, U , an index subset I ⊆ {1, . . . , n}, and any ∆A ∈ U , let ∆AI denote the element of U that equals ∆A on each feature indexed by i ∈ I , and is zero elsewhere. Then, we can write any element ∆A ∈ U as ∆AI +∆AIc (where Ic = {1, . . . , n} \\ I). Then we have the following theorem. We note that the result holds for any norm loss function, but we state and prove it for the ℓ2 norm, since the proof for other norms is identical.\nTheorem 5: The robust regression problem\nmin x∈Rm\n{\nmax ∆A∈U\n‖b− (A+∆A)x‖2 } ,\nhas a solution supported on an index set I if there exists some perturbation ∆ÃI c ∈ U of the features in Ic, such that the robust regression problem\nmin x∈Rm\n{\nmax ∆ÃI∈UI\n‖b− (A+∆ÃIc +∆ÃI)x‖2 } ,\nhas a solution supported on the set I . Thus, a robust regression has an optimal solution supported on a set I , if any perturbation of the features corresponding to the complement of I makes them irrelevant. Theorem 5 is a special case of the following theorem with cj = 0 for all j 6∈ I:\nTheorem 5’. Let x∗ be an optimal solution of the robust regression problem:\nmin x∈Rm\n{\nmax ∆A∈U\n‖b− (A+∆A)x‖2 } ,\nand let I ⊆ {1, · · · , m} be such that x∗j = 0 ∀ j 6∈ I . Let\nŨ , { (δ1, · · · , δm) ∣ ∣ ∣ ‖δi‖2 ≤ ci, i ∈ I; ‖δj‖2 ≤ cj + lj , j 6∈ I } .\nNovember 11, 2008 DRAFT\n9 Then, x∗ is an optimal solution of\nmin x∈Rm\n{\nmax ∆A∈Ũ\n‖b− (Ã+∆A)x‖2 } ,\nfor any Ã that satisfies ‖ãj − aj‖ ≤ lj for j 6∈ I , and ãi = ai for i ∈ I . Proof: Notice that\nmax ∆A∈Ũ\n∥ ∥\n∥ b− (A+∆A)x∗\n∥ ∥ ∥\n2\n= max ∆A∈U\n∥ ∥\n∥ b− (A+∆A)x∗\n∥ ∥ ∥\n2\n= max ∆A∈U\n∥ ∥\n∥ b− (Ã+∆A)x∗\n∥ ∥ ∥\n2 .\nThese equalities hold because for j 6∈ I , x∗j = 0, hence the jth column of both Ã and ∆A has no effect on the residual.\nFor an arbitrary x′, we have\nmax ∆A∈Ũ\n∥ ∥\n∥ b− (A+∆A)x′\n∥ ∥ ∥\n2\n≥ max ∆A∈U\n∥ ∥\n∥ b− (Ã+∆A)x′\n∥ ∥ ∥\n2 .\nThis is because, ‖aj − ãj‖ ≤ lj for j 6∈ I , and ai = ãi for i ∈ I . Hence, we have {\nA+∆A ∣ ∣∆A ∈ U } ⊆ { Ã+∆A ∣ ∣∆A ∈ Ũ } .\nFinally, notice that\nmax ∆A∈U\n∥ ∥\n∥ b − (A+∆A)x∗\n∥ ∥ ∥\n2 ≤ max ∆A∈U\n∥ ∥\n∥ b − (A +∆A)x′\n∥ ∥ ∥\n2 .\nTherefore we have\nmax ∆A∈Ũ\n∥ ∥\n∥ b − (Ã+∆A)x∗\n∥ ∥ ∥\n2 ≤ max ∆A∈Ũ\n∥ ∥\n∥ b − (Ã +∆A)x′\n∥ ∥ ∥\n2 .\nSince this holds for arbitrary x′, we establish the theorem. We can interpret the result of this theorem by considering a generative model1 b = ∑\ni∈I wiai+\nξ̃ where I ⊆ {1 · · · , m} and ξ̃ is a random variable, i.e., b is generated by features belonging to I . In this case, for a feature j 6∈ I , Lasso would assign zero weight as long as there exists a perturbed value of this feature, such that the optimal regression assigned it zero weight.\nWhen we consider ℓ2 loss, we can translate the condition of a feature being “irrelevant” into a geometric condition, namely, orthogonality. We now use the result of Theorem 5 to show that robust regression has a sparse solution as long as an incoherence-type property is satisfied. This result is more in line with the traditional sparsity results, but we note that the geometric reasoning is different, and ours is based on robustness. Indeed, we show that a feature receives zero weight, if it is “nearly” (i.e., within an allowable perturbation) orthogonal to the signal, and\n1While we are not assuming generative models to establish the results, it is still interesting to see how these results can help in a generative model setup.\nNovember 11, 2008 DRAFT\n10\nall relevant features. Theorem 6: Let ci = c for all i and consider ℓ2 loss. If there exists I ⊂ {1, · · · , m} such that for all v ∈ span ( {ai, i ∈ I} ⋃ {b} )\n, ‖v‖ = 1, we have v⊤aj ≤ c, ∀j 6∈ I , then any optimal solution x∗ satisfies x∗j = 0, ∀j 6∈ I .\nProof: For j 6∈ I , let a=j denote the projection of aj onto the span of {ai, i ∈ I} ⋃ {b}, and let a+j , aj − a=j . Thus, we have ‖a=j ‖ ≤ c. Let Â be such that\nâi =\n{\nai i ∈ I; a+i i 6∈ I.\nNow let Û , {(δ1, · · · , δm)|‖δi‖2 ≤ c, i ∈ I; ‖δj‖2 = 0, j 6∈ I}.\nConsider the robust regression problem minx̂ { max∆A∈Û ∥ ∥b− (Â+∆A)x̂ ∥ ∥\n2\n}\n, which is equiv-\nalent to minx̂ { ∥ ∥b − Âx̂ ∥ ∥\n2 +\n∑ i∈I c|x̂i| } . Note that the âj are orthogonal to the span of\n{âi, i ∈ I} ⋃ {b}. Hence for any given x̂, by changing x̂j to zero for all j 6∈ I , the minimizing objective does not increase.\nSince ‖â − âj‖ = ‖a=j ‖ ≤ c ∀j 6∈ I , (and recall that U = {(δ1, · · · , δm)|‖δi‖2 ≤ c, ∀i}) applying Theorem 5 concludes the proof.\nV. DENSITY ESTIMATION AND CONSISTENCY\nIn this section, we investigate the robust linear regression formulation from a statistical perspective and rederive using only robustness properties that Lasso is asymptotically consistent. The basic idea of the consistency proof is as follows. We show that the robust optimization formulation can be seen to be the maximum error w.r.t. a class of probability measures. This class includes a kernel density estimator, and using this, we show that Lasso is consistent."
    }, {
      "heading" : "A. Robust Optimization, Worst-case Expected Utility and Kernel Density Estimator",
      "text" : "In this subsection, we present some notions and intermediate results. In particular, we link a robust optimization formulation with a worst expected utility (w.r.t. a class of probability measures); we then briefly recall the definition of a kernel density estimator. Such results will be used in establishing the consistency of Lasso, as well as providing some additional insights on robust optimization. Proofs are postponed to the appendix.\nWe first establish a general result on the equivalence between a robust optimization formulation and a worst-case expected utility:\nProposition 1: Given a function g : Rm+1 → R and Borel sets Z1, · · · ,Zn ⊆ Rm+1, let\nPn , {µ ∈ P|∀S ⊆ {1, · · · , n} : µ( ⋃\ni∈S Zi) ≥ |S|/n}.\nThe following holds\n1\nn\nn ∑\ni=1\nsup (ri,bi)∈Zi h(ri, bi) = sup µ∈Pn\n∫\nRm+1\nh(r, b)dµ(r, b).\nThis leads to the following corollary for Lasso, which states that for a given x, the robust regression loss over the training data is equal to the worst-case expected generalization error.\nNovember 11, 2008 DRAFT\n11\nCorollary 3: Given b ∈ Rn, A ∈ Rn×m, the following equation holds for any x ∈ Rm,\n‖b− Ax‖2 + √ ncn‖x‖1 + √ ncn = sup\nµ∈P̂(n)\n√\nn\n∫\nRm+1\n(b′ − r′⊤x)2dµ(r′, b′). (8)\nHere2,\nP̂(n) , ⋃\n‖σ‖2≤ √ ncn;∀i:‖δi‖2≤ √ ncn\nPn(A,∆,b,σ);\nPn(A,∆,b,σ) , {µ ∈ P|Zi = [bi − σi, bi + σi]× m ∏\nj=1\n[aij − δij , aij + δij];\n∀S ⊆ {1, · · · , n} : µ( ⋃\ni∈S Zi) ≥ |S|/n}.\nRemark 1: We briefly explain Corollary 3 to avoid possible confusions. Equation (8) is a non-probabilistic equality. That is, it holds without any assumption (e.g., i.i.d. or generated by certain distributions) on b and A. And it does not involve any probabilistic operation such as taking expectation on the left-hand-side, instead, it is an equivalence relationship which hold for an arbitrary set of samples. Notice that, the right-hand-side also depends on the samples since P̂(n) is defined through A and b. Indeed, P̂(n) represents the union of classes of distributions Pn(A,∆,b,σ) such that the norm of each column of ∆ is bounded, where Pn(A,∆,b,σ) is the set of distributions corresponds to (see Proposition 1) disturbance in hyper-rectangle Borel sets Z1, · · · ,Zn centered at (bi, r⊤i ) with lengths (2σi, 2δi1, · · · , 2δim).\nWe will later show that P̂n consists a kernel density estimator. Hence we recall here its definition. The kernel density estimator for a density ĥ in Rd, originally proposed in [24], [25], is defined by\nhn(x) = (nc d n)\n−1 n ∑\ni=1\nK\n(\nx − x̂i cn\n)\n,\nwhere {cn} is a sequence of positive numbers, x̂i are i.i.d. samples generated according to f̂ , and K is a Borel measurable function (kernel) satisfying K ≥ 0, ∫\nK = 1. See [26], [27] and the reference therein for detailed discussions. Figure 1 illustrates a kernel density estimator using Gaussian kernel for a randomly generated sample-set. A celebrated property of a kernel density estimator is that it converges in L1 to ĥ when cn ↓ 0 and ncdn ↑ ∞ [26]."
    }, {
      "heading" : "B. Consistency of Lasso",
      "text" : "We restrict our discussion to the case where the magnitude of the allowable uncertainty for all features equals c, (i.e., the standard Lasso) and establish the statistical consistency of Lasso from a distributional robustness argument. Generalization to the non-uniform case is straightforward. Throughout, we use cn to represent c where there are n samples (we take cn to zero).\nRecall the standard generative model in statistical learning: let P be a probability measure with bounded support that generates i.i.d samples (bi, ri), and has a density f ∗(·). Denote the\n2Recall that aij is the jth element of ri\nNovember 11, 2008 DRAFT\n12\nset of the first n samples by Sn. Define\nx(cn,Sn) , argmin x\n{\n√ √ √ √ 1\nn\nn ∑\ni=1\n(bi − r⊤i x)2 + cn‖x‖1 }\n= argmin x\n{ √ n\nn\n√ √ √ √ n ∑\ni=1\n(bi − r⊤i x)2 + cn‖x‖1 } ;\nx(P) , argmin x\n{\n√\n∫\nb,r\n(b− r⊤x)2dP(b, r) } .\nIn words, x(cn,Sn) is the solution to Lasso with the tradeoff parameter set to cn √ n, and x(P) is the “true” optimal solution. We have the following consistency result. The theorem itself is a wellknown result. However, the proof technique is novel. This technique is of interest because the standard techniques to establish consistency in statistical learning including Vapnik-Chervonenkis (VC) dimension (e.g., [28]) and algorithmic stability (e.g., [29]) often work for a limited range of algorithms, e.g., the k-Nearest Neighbor is known to have infinite VC dimension, and we show in Section VI that Lasso is not stable. In contrast, a much wider range of algorithms have robustness interpretations, allowing a unified approach to prove their consistency.\nTheorem 7: Let {cn} be such that cn ↓ 0 and limn→∞ n(cn)m+1 = ∞. Suppose there exists a constant H such that ‖x(cn,Sn)‖2 ≤ H . Then,\nlim n→∞\n√\n∫\nb,r\n(b− r⊤x(cn,Sn))2dP(b, r) = √ ∫\nb,r\n(b− r⊤x(P))2dP(b, r),\nalmost surely. Proof: Step 1: We show that the right hand side of Equation (8) includes a kernel density estimator for the true (unknown) distribution. Consider the following kernel estimator given\nNovember 11, 2008 DRAFT\n13\nsamples Sn = (bi, ri)ni=1 and tradeoff parameter cn,\nfn(b, r) , (nc m+1 n )\n−1 n ∑\ni=1\nK\n(\nb− bi, r− ri cn\n)\n,\nwhere: K(x) , I[−1,+1]m+1(x)/2 m+1.\n(9)\nLet µ̂n denote the distribution given by the density function fn(b, r). Easy to check that µ̂n belongs to Pn(A, (cn1n, · · · , cn1n),b, cn1n) and hence belongs to P̂(n) by definition.\nStep 2: Using the L1 convergence property of the kernel density estimator, we prove the consistency of robust regression and equivalently Lasso.\nFirst notice that, ‖x(cn,Sn)‖2 ≤ H and P has a bounded support implies that there exists a universal constant C such that\nmax b,r\n(b− r⊤w(cn,Sn))2 ≤ C.\nBy Corollary 3 and µ̂n ∈ P̂(n) we have √\n∫\nb,r\n(b− r⊤x(cn,Sn))2dµ̂n(b, r)\n≤ sup µ∈P̂(n)\n√\n∫\nb,r\n(b− r⊤x(cn,Sn))2dµ(b, r)\n=\n√ n\nn\n√ √ √ √ n ∑\ni=1\n(bi − r⊤i x(cn,Sn))2 + cn‖x(cn,Sn)‖1 + cn\n≤ √ n\nn\n√ √ √ √ n ∑\ni=1\n(bi − r⊤i x(P))2 + cn‖x(P)‖1 + cn,\nthe last inequality holds by definition of x(cn,Sn). Taking the square of both sides, we have\n∫\nb,r\n(b− r⊤x(cn,Sn))2dµ̂n(b, r)\n≤1 n\nn ∑\ni=1\n(bi − r⊤i x(P))2 + c2n(1 + ‖x(P)‖1)2\n+ 2cn(1 + ‖x(P)‖1)\n√ √ √ √ 1\nn\nn ∑\ni=1\n(bi − r⊤i x(P))2.\nNotice that, the right-hand side converges to ∫ b,r (b − r⊤x(P))2dP(b, r) as n ↑ ∞ and cn ↓ 0\nNovember 11, 2008 DRAFT\n14\nalmost surely. Furthermore, we have ∫\nb,r\n(b− r⊤x(cn,Sn))2dP(b, r)\n≤ ∫\nb,r\n(b− r⊤x(cn,Sn))2dµ̂n(b, r)\n+ [\nmax b,r\n(b− r⊤x(cn,Sn))2 ]\n∫\nb,r\n|fn(b, r)− f ∗(b, r)|d(b, r)\n≤ ∫\nb,r\n(b− r⊤x(cn,Sn))2dµ̂n(b, r) + C ∫\nb,r\n|fn(b, r)− f ∗(b, r)|d(b, r),\nwhere the last inequality follows from the definition of C. Notice that ∫ b,r |fn(b, r)−f ∗(b, r)|d(b, r) goes to zero almost surely when cn ↓ 0 and ncm+1n ↑ ∞ since fn(·) is a kernel density estimation of f ∗(·) (see e.g. Theorem 3.1 of [26]). Hence the theorem follows.\nWe can remove the assumption that ‖x(cn,Sn)‖2 ≤ H , and as in Theorem 7, the proof technique rather than the result itself is of interest.\nTheorem 8: Let {cn} converge to zero sufficiently slowly. Then\nlim n→∞\n√\n∫\nb,r\n(b− r⊤x(cn,Sn))2dP(b, r) = √ ∫\nb,r\n(b− r⊤x(P))2dP(b, r),\nalmost surely. Proof: To prove the theorem, we need to consider a set of distributions belonging to P̂(n). Hence we establish the following lemma first. Lemma 1: Partition the support of P as V1, · · · , VT such the ℓ∞ radius of each set is less than cn. If a distribution µ satisfies\nµ(Vt) = ∣ ∣ ∣ { i|(bi, ri) ∈ Vt } ∣ ∣ ∣ /n; t = 1, · · · , T, (10)\nthen µ ∈ P̂(n). Proof: Let Zi = [bi − cn, bi + cn]× ∏m\nj=1[aij − cn, aij + cn]; recall that aij the jth element of ri. Notice Vt has ℓ∞ norm less than cn we have\n(bi, ri ∈ Vt) ⇒ Vt ⊆ Zi. Therefore, for any S ⊆ {1, · · · , n}, the following holds\nµ( ⋃\ni∈S Zi) ≥ µ(\n⋃\nVt|∃i ∈ S : bi, ri ∈ Vt)\n= ∑\nt|∃i∈S:bi,ri∈Vt\nµ(Vt) = ∑\nt|∃i∈S:bi,ri∈Vt\n# ( (bi, ri) ∈ Vt ) /n ≥ |S|/n.\nHence µ ∈ Pn(A,∆, b, cn) where each element of ∆ is cn, which leads to µ ∈ P̂(n). Now we proceed to prove the theorem. Partition the support of P into T subsets such that ℓ∞ radius of each one is smaller than cn. Denote P̃(n) as the set of probability measures satisfying Equation (10). Hence P̃(n) ⊆ P̂(n) by Lemma 1. Further notice that there exists a universal constant K such that ‖x(cn,Sn)‖2 ≤ K/cn due to the fact that the square loss of the solution\nNovember 11, 2008 DRAFT\n15\nx = 0 is bounded by a constant only depends on the support of P. Thus, there exists a constant C such that maxb,r(b− r⊤x(cn,Sn))2 ≤ C/c2n.\nFollow a similar argument as the proof of Theorem 7, we have\nsup µn∈P̃(n)\n∫\nb,r\n(b− r⊤x(cn,Sn))2dµn(b, r)\n≤1 n\nn ∑\ni=1\n(bi − r⊤i x(P))2 + c2n(1 + ‖x(P)‖1)2\n+ 2cn(1 + ‖x(P)‖1)\n√ √ √ √ 1\nn\nn ∑\ni=1\n(bi − r⊤i x(P))2,\n(11)\nand ∫\nb,r\n(b− r⊤x(cn,Sn))2dP(b, r)\n≤ inf µn∈P̃(n)\n{\n∫\nb,r\n(b− r⊤x(cn,Sn))2dµn(b, r)\n+ max b,r\n(b− r⊤x(cn,Sn))2 ∫\nb,r\n|fµn(b, r)− f(b, r)|d(b, r)\n≤ sup µn∈P̃(n)\n∫\nb,r\n(b− r⊤x(cn,Sn))2dµn(b, r)\n+ 2C/c2n inf µ′n∈P̃(n)\n{\n∫\nb,r\n|fµ′n(b, r)− f(b, r)|d(b, r) } ,\nhere fµ stands for the density function of a measure µ. Notice that P̃n is the set of distributions satisfying Equation (10), hence infµ′n∈P̃(n) ∫ b,r |fµ′n(b, r) − f(b, r)|d(b, r) is upper-bounded by ∑T t=1 |P(Vt) − #(bi, ri ∈ Vt)|/n, which goes to zero as n increases for any fixed cn (see for example Proposition A6.6 of [30]). Therefore,\n2C/c2n inf µ′n∈P̃(n)\n{\n∫\nb,r\n|fµ′n(b, r)− f(b, r)|d(b, r) } → 0,\nif cn ↓ 0 sufficiently slow. Combining this with Inequality (11) proves the theorem.\nVI. STABILITY\nKnowing that the robust regression problem (1) and in particular Lasso encourage sparsity, it is of interest to investigate another desirable characteristic of a learning algorithm, namely, stability. We show in this section that Lasso is not stable. This is a special case of a more general result we prove in [31], where we show that this is a common property for all algorithms that encourage sparsity. That is, if a learning algorithm achieves certain sparsity condition, then it cannot have a non-trivial stability bound.\nWe recall the definition of uniform stability [29] first. We let Z denote the space of points and labels (typically this will be a compact subset of Rn+1) so that S ∈ Zm denotes a collection of m labelled training points. We let L denote a learning algorithm, and for S ∈ Zm, we let LS\nNovember 11, 2008 DRAFT\n16\ndenote the output of the learning algorithm (i.e., the regression function it has learned from the training data). Then given a loss function l, and a labeled point s = (z, b) ∈ Z , we let l(LS, s) denote the loss of the algorithm that has been trained on the set S, on the data point s. Thus for squared loss, we would have l(LS, s) = ‖LS(z)− b‖2.\nDefinition 1: An algorithm L has uniform stability bound of βm with respect to the loss function l if the following holds\n∀S ∈ Zm, ∀i ∈ {1, · · · , m}, ‖l(LS, ·)− l(LS\\i, ·)‖∞ ≤ βm. Here LS\\i stands for the learned solution with the i\nth sample removed from S. At first glance, this definition may seem too stringent for any reasonable algorithm to exhibit good stability properties. However, as shown in [29], Tikhonov-regularized regression has stability that scales as 1/m. Stability that scales at least as fast as o( 1√\nm ) can be used to establish strong PAC\nbounds (see [29]). In this section we show that not only is the stability (in the sense defined above) of Lasso much worse than the stability of ℓ2-regularized regression, but in fact Lasso’s stability is, in the following sense, as bad as it gets. To this end, we define the notion of the trivial bound, which is the worst possible error a training algorithm can have for arbitrary training set and testing sample labelled by zero.\nDefinition 2: Given a subset from which we can draw m labelled points, Z ⊆ Rn×(m+1) and a subset for one unlabelled point, X ⊆ Rm, a trivial bound for a learning algorithm L w.r.t. Z and X is\nb(L,Z,X ) , max S∈Z,z∈X l ( LS, (z, 0) ) .\nAs above, l(·, ·) is a given loss function. Notice that the trivial bound does not diminish as the number of samples increases, since by repeatedly choosing the worst sample, the algorithm will yield the same solution.\nNow we show that the uniform stability bound of Lasso can be no better than its trivial bound with the number of features halved.\nTheorem 9: Given Ẑ ⊆ Rn×(2m+1) be the domain of sample set and X̂ ⊆ R2m be the domain of new observation, such that\n(b, A) ∈ Z =⇒ (b, A, A) ∈ X̂ , (z⊤) ∈ X =⇒ (z⊤, z⊤) ∈ X̂ .\nThen the uniform stability bound of Lasso is lower bounded by b(Lasso,Z,X ). Proof: Let (b∗, A∗) and (0, z∗⊤) be the sample set and the new observation such that they jointly achieve b(Lasso,Z,X ), and let x∗ be the optimal solution to Lasso w.r.t (b∗, A∗). Consider the following sample set\n( b∗ A∗ A∗\n0 0⊤ z∗⊤\n)\n.\nObserve that (x⊤, 0⊤)⊤ is an optimal solution of Lasso w.r.t to this sample set. Now remove the last sample from the sample set. Notice that (0⊤,x⊤)⊤ is an optimal solution for this new sample set. Using the last sample as a testing observation, the solution w.r.t the full sample set has zero cost, while the solution of the leave-one-out sample set has a cost b(Lasso,Z,X ). And hence we prove the theorem.\nNovember 11, 2008 DRAFT\n17\nVII. CONCLUSION\nIn this paper, we considered robust regression with a least-square-error loss. In contrast to previous work on robust regression, we considered the case where the perturbations of the observations are in the features. We show that this formulation is equivalent to a weighted ℓ1 norm regularized regression problem if no correlation of disturbances among different features is allowed, and hence provide an interpretation of the widely used Lasso algorithm from a robustness perspective. We also formulated tractable robust regression problems for disturbance coupled among different features and hence generalize Lasso to a wider class of regularization schemes.\nThe sparsity and consistency of Lasso are also investigated based on its robustness interpretation. In particular we present a “no-free-lunch” theorem saying that sparsity and algorithmic stability contradict each other. This result shows, although sparsity and algorithmic stability are both regarded as desirable properties of regression algorithms, it is not possible to achieve them simultaneously, and we have to tradeoff these two properties in designing a regression algorithm.\nThe main thrust of this work is to treat the widely used regularized regression scheme from a robust optimization perspective, and extend the result of [13] (i.e., Tikhonov regularization is equivalent to a robust formulation for Frobenius norm bounded disturbance set) to a broader range of disturbance set and hence regularization scheme. This provides us not only with new insight of why regularization schemes work, but also offer solid motivations for selecting regularization parameter for existing regularization scheme and facilitate designing new regularizing schemes.\nREFERENCES\n[1] L. Elden. Perturbation theory for the least-square problem with linear equality constraints. BIT, 24:472–476, 1985. [2] G. Golub and C. Van Loan. Matrix Computation. John Hopkins University Press, Baltimore, 1989. [3] D. Higham and N. Higham. Backward error and condition of structured linear systems. SIAM Journal on Matrix Analysis\nand Applications, 13:162–175, 1992. [4] R. Fierro and J. Bunch. Collinearity and total least squares. SIAM Journal on Matrix Analysis and Applications, 15:1167–\n1181, 1994. [5] A. Tikhonov and V. Arsenin. Solution for Ill-Posed Problems. Wiley, New York, 1977. [6] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B,\n58(1):267–288, 1996. [7] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2004. [8] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing,\n20(1):33–61, 1998. [9] A. Feuer and A. Nemirovski. On sparse representation in pairs of bases. IEEE Transactions on Information Theory,\n49(6):1579–1581, 2003. [10] E. Candès, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2):489–509, 2006. [11] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50(10):2231–2242, 2004. [12] M. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of spar-\nsity using ℓ1-constrained quadratic programming. Technical Report Available from: http://http://www.stat.berkeley.edu/tech-reports/709.pdf, Department of Statistics, UC Berkeley, 2006.\n[13] L. El Ghaoui and H. Lebret. Robust solutions to least-squares problems with uncertain data. SIAM Journal on Matrix Analysis and Applications, 18:1035–1064, 1997. [14] P. Shivaswamy, C. Bhattacharyya, and A. Smola. Second order cone programming approaches for handling missing and uncertain data. Journal of Machine Learning Research, 7:1283–1314, July 2006. [15] D. Bertsimas and I. Popescu. Optimal inequalities in probability theory: A convex optimization approach. SIAM Journal of Optimization, 15(3):780–800, 2004. [16] H. Xu, C. Caramanis, and S. Mannor. Robust regression and Lasso. Technical report, Gerad, Available from http://www.cim.mcgill.ca/∼xuhuan/LassoGerad.pdf, 2008.\nNovember 11, 2008 DRAFT\n18\n[17] D. Bertsimas and M. Sim. The price of robustness. Operations Research, 52(1):35–53, January 2004. [18] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal Of The Royal Statistical Society Series B, 67(2):301–320, 2005. [19] J. Tropp. Just relax: Convex programming methods for identifying sparse signals. IEEE Transactions on Information Theory, 51(3):1030–1051, 2006. [20] F. Girosi. An equivalence between sparse approximation and support vector machines. Neural Computation, 10(6):1445– 1480, 1998. [21] R. R. Coifman and M. V. Wickerhauser. Entropy-based algorithms for best-basis selection. IEEE Transactions on Information Theory, 38(2):713–718, 1992. [22] S. Mallat and Z. Zhang. Matching Pursuits with time-frequence dictionaries. IEEE Transactions on Signal Processing, 41(12):3397–3415, 1993. [23] D. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306, 2006. [24] M. Rosenblatt. Remarks on some nonparametric estimates of a density function. Annals of Mathematical Statistics, 27:832–837, 1956. [25] E. Parzen. On the estimation of a probability density function and the mode. Annals of Mathematical Statistics, 33:1065– 1076, 1962. [26] L. Devroye and L. Györfi. Nonparametric Density Estimation: the l1 View. John Wiley & Sons, 1985. [27] D. Scott. Multivariate Density Estimation: Theory, Practice and Visualization. John Wiley & Sons, 1992. [28] V. Vapnik and A. Chervonenkis. The necessary and sufficient conditions for consistency in the empirical risk minimization method. Pattern Recognition and Image Analysis, 1(3):260–284, 1991. [29] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499–526, 2002. [30] A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes. Springer-Verlag, New York, 2000. [31] H. Xu, C. Caramanis, and S. Mannor. Sparse algorithms are not stable: A no-free-lunch theorem. In Proceedings of\nForty-Sixth Allerton Conference on Communication, Control, and Computing, 2008.\nAPPENDIX A PROOF OF THEOREM 2\nTheorem 2. Consider a random vector v ∈ Rn, such that E(v) = a, and E(vv⊤) = Σ, Σ 0. Then we have\nPr{‖v‖2 ≥ ci} ≤\n\n      \n     \nminP,q,r,λ Trace(ΣP ) + 2q ⊤a + r\nsubject to:\n(\nP q q⊤ r\n)\n0 (\nI(m) 0 0⊤ −c2i\n)\nλ (\nP q q⊤ r − 1\n)\nλ ≥ 0.\n(12)\nProof: Consider a function f(·) parameterized by P,q, r defined as f(v) = v⊤Pv+2q⊤v+ r. Notice E ( f(v) )\n= Trace(ΣP ) + 2q⊤a+ r. Now we show that f(v) ≥ 1‖v‖≥ci for all P,q, r satisfying the constraints in (12).\nTo show f(v) ≥ 1‖v‖2≥ci , we need to establish (i) f(v) ≥ 0 for all v, and (ii) f(v) ≥ 1 when ‖v‖2 ≥ ci. Notice that\nf(v) =\n(\nv\n1 )⊤ ( P q q⊤ r )( v 1 ) ,\nhence (i) holds because (\nP q q⊤ r\n)\n0.\nTo establish condition (ii), it suffices to show v⊤v ≥ c2i implies v⊤Pv+2q⊤v+r ≥ 1, which is equivalent to show { v ∣ ∣v⊤Pv + 2q⊤v + r − 1 ≤ 0 } ⊆ { v ∣\n∣v⊤v ≤ c2i }\n. Noticing this is an ellipsoid-containment condition, by S-Procedure, we see that is equivalent to the condition that\nNovember 11, 2008 DRAFT\n19\nthere exists a λ ≥ 0 such that (\nI(m) 0 0⊤ −c2i\n)\nλ (\nP q q⊤ r − 1\n)\n.\nHence we have f(v) ≥ 1‖v‖2≥ci , taking expectation over both side that notice that the expectation of a indicator function is the probability, we establish the theorem.\nAPPENDIX B PROOF OF THEOREM 4\nTheorem 4. Assume that the set\nZ , {z ∈ Rm|fj(z) ≤ 0, j = 1, · · · , k; z ≥ 0} has non-empty relative interior. Then the robust regression problem\nmin x∈Rm\n{\nmax ∆A∈U ′\n‖b− (A+∆A)x‖a }\nis equivalent to the following regularized regression problem\nmin λ∈Rk\n+ ,κ∈Rm + ,x∈Rm\n{ ‖b− Ax‖a + v(λ,κ,x) } ;\nwhere: v(λ,κ,x) , max c∈Rm\n[ (κ + |x|)⊤c− k ∑\nj=1\nλjfj(c) ]\nProof: Fix a solution x∗. Notice that,\nU ′ = {(δ1, · · · , δm)|c ∈ Z; ‖δi‖a ≤ ci, i = 1, · · · , m}. Hence we have:\nmax ∆A∈U ′\n‖b − (A+∆A)x∗‖a\n=max c∈Z\n{\nmax ‖δi‖a≤ci, i=1,··· ,m\n‖b− ( A+ (δ1, · · · , δm) ) x∗‖a }\n=max c∈Z\n{ ‖b−Ax∗‖a + m ∑\ni=1\nci|x∗i | }\n=‖b −Ax∗‖a +max c∈Z\n{ |x∗|⊤c } .\n(13)\nThe second equation follows from Theorem 3. Now we need to evaluate maxc∈Z{|x∗|⊤c}, which equals to −minc∈Z{−|x∗|⊤c}. Hence we are minimizing a linear function over a set of convex constraints. Furthermore, by assumption the Slater’s condition holds. Hence the duality gap of minc∈Z{−|x∗|⊤c} is zero. A standard duality analysis shows that\nmax c∈Z\n{ |x∗|⊤c }\n= min λ∈Rk\n+ ,κ∈Rm +\nv(λ,κ,x∗). (14)\nNovember 11, 2008 DRAFT\n20\nWe establish the theorem by substituting Equation (14) back into Equation (13) and taking minimum over x on both sides.\nAPPENDIX C PROOF OF PROPOSITION 1\nProposition 1. Given a function g : Rm+1 → R and Borel sets Z1, · · · ,Zn ⊆ Rm+1, let\nPn , {µ ∈ P|∀S ⊆ {1, · · · , n} : µ( ⋃\ni∈S Zi) ≥ |S|/n}.\nThe following holds\n1\nn\nn ∑\ni=1\nsup (ri,bi)∈Zi h(ri, bi) = sup µ∈Pn\n∫\nRm+1\nh(r, b)dµ(r, b).\nProof: To prove Proposition 1, we first establish the following lemma. Lemma 2: Given a function f : Rm+1 → R, and a Borel set Z ⊆ Rm+1, the following holds:\nsup x′∈Z f(x′) = sup µ∈P|µ(Z)=1\n∫\nRm+1\nf(x)dµ(x).\nProof: Let x̂ be a ǫ−optimal solution to the left hand side, consider the probability measure µ′ that put mass 1 on x̂, which satisfy µ′(Z) = 1. Hence, we have\nsup x′∈Z f(x′)− ǫ ≤ sup µ∈P|µ(Z)=1\n∫\nRm+1\nf(x)dµ(x),\nsince ǫ can be arbitrarily small, this leads to\nsup x′∈Z f(x′) ≤ sup µ∈P|µ(Z)=1\n∫\nRm+1\nf(x)dµ(x). (15)\nNext construct function f̂ : Rm+1 → R as\nf̂(x) ,\n{\nf(x̂) x ∈ Z; f(x) otherwise.\nBy definition of x̂ we have f(x) ≤ f̂(x) + ǫ for all x ∈ Rm+1. Hence, for any probability measure µ such that µ(Z) = 1, the following holds\n∫\nRm+1\nf(x)dµ(x) ≤ ∫\nRm+1 f̂(x)dµ(x) + ǫ = f(x̂) + ǫ ≤ sup x′∈Z f(x′) + ǫ.\nThis leads to sup\nµ∈P|µ(Z)=1\n∫\nRm+1 f(x)dµ(x) ≤ sup x′∈Z f(x′) + ǫ.\nNotice ǫ can be arbitrarily small, we have\nsup µ∈P|µ(Z)=1\n∫\nRm+1 f(x)dµ(x) ≤ sup x′∈Z f(x′) (16)\nCombining (15) and (16), we prove the lemma.\nNovember 11, 2008 DRAFT\n21\nNow we proceed to prove the proposition. Let x̂i be an ǫ−optimal solution to supxi∈Zi f(xi). Observe that the empirical distribution for (x̂1, · · · , x̂n) belongs to Pn, since ǫ can be arbitrarily close to zero, we have\n1\nn\nn ∑\ni=1\nsup xi∈Zi f(xi) ≤ sup µ∈Pn\n∫\nRm+1\nf(x)dµ(x). (17)\nWithout loss of generality, assume\nf(x̂1) ≤ f(x̂2) ≤ · · · ≤ f(x̂n). (18) Now construct the following function\nf̂(x) ,\n{\nmini|x∈Zi f(x̂i) x ∈ ⋃n j=1Zj ; f(x) otherwise.\n(19)\nObserve that f(x) ≤ f̂(x) + ǫ for all x. Furthermore, given µ ∈ Pn, we have\n∫\nRm+1\nf(x)dµ(x)− ǫ\n=\n∫\nRm+1\nf̂(x)dµ(x)\n=\nn ∑\nk=1\nf(x̂k) [ µ(\nk ⋃\ni=1\nZi)− µ( k−1 ⋃\ni=1\nZi) ]\nDenote αk , [ µ( ⋃k i=1 Zi)− µ( ⋃k−1 i=1 Zi) ] , we have\nn ∑\nk=1\nαk = 1, t ∑\nk=1\nαk ≥ t/n.\nHence by Equation (18) we have n\n∑\nk=1\nαkf(x̂k) ≤ 1\nn\nn ∑\nk=1\nf(x̂k).\nThus we have for any µ ∈ Pn, ∫\nRm+1\nf(x)dµ(x)− ǫ ≤ 1 n\nn ∑\nk=1\nf(x̂k).\nTherefore,\nsup µ∈Pn\n∫\nRm+1 f(x)dµ(x)− ǫ ≤ sup xi∈Zi\n1\nn\nn ∑\nk=1\nf(xk).\nNotice ǫ can be arbitrarily close to 0, we proved the proposition by combining with (17).\nNovember 11, 2008 DRAFT\n22\nAPPENDIX D PROOF OF COROLLARY 3\nCorollary 3. Given b ∈ Rn, A ∈ Rn×m, the following equation holds for any x ∈ Rm,\n‖b− Ax‖2 + √ ncn‖x‖1 + √ ncn = sup\nµ∈P̂(n)\n√\nn\n∫\nRm+1\n(b′ − r′⊤x)2dµ(r′, b′). (20)\nHere,\nP̂(n) , ⋃\n‖σ‖2≤ √ ncn;∀i:‖δi‖2≤ √ ncn\nPn(A,∆,b,σ);\nPn(A,∆,b,σ) , {µ ∈ P|Zi = [bi − σi, bi + σi]× m ∏\nj=1\n[aij − δij , aij + δij];\n∀S ⊆ {1, · · · , n} : µ( ⋃\ni∈S Zi) ≥ |S|/n}.\nProof: The right-hand-side of Equation (20) equals\nsup ‖σ‖2≤ √ ncn;∀i:‖δi‖2≤ √ ncn\n{\nsup µ∈Pn(A,∆,b,σ)\n√\nn\n∫\nRm+1\n(b′ − r′⊤x)2dµ(r′, b′) } .\nNotice by the equivalence to robust formulation, the left-hand-side equals to\nmax ‖σ‖2≤ √ ncn;∀i:‖δi‖2≤ √ ncn\n∥ ∥ ∥ b+ σ − ( A+ [δ1, · · · , δm] ) x ∥ ∥ ∥\n2\n= sup ‖σ‖2≤ √ ncn; ∀i:‖δi‖2≤ √ ncn\n\n\n sup (b̂i,r̂i)∈[bi−σi,bi+σi]× Qm j=1[aij−δij ,aij+δij ]\n√ √ √ √ n ∑\ni=1\n(b̂i − r̂⊤i x)2  \n\n= sup ‖σ‖2≤ √ ncn; ∀i:‖δi‖2≤ √ ncn\n√ √ √ √ n ∑\ni=1 sup (b̂i,r̂i)∈[bi−σi,bi+σi]× Qm j=1[aij−δij ,aij+δij ]\n(b̂i − r̂⊤i x)2,\nfurthermore, applying Proposition 1 yields √\n√ √ √ n ∑\ni=1 sup (b̂i,r̂i)∈[bi−σi,bi+σi]× Qm j=1[aij−δij ,aij+δij ]\n(b̂i − r̂⊤i x)2\n=\n√\nsup µ∈Pn(A,∆,b,σ) n\n∫\nRm+1\n(b′ − r′⊤x)2dµ(r′, b′)\n= sup µ∈Pn(A,∆,b,σ)\n√\nn\n∫\nRm+1\n(b′ − r′⊤x)2dµ(r′, b′),\nwhich proves the corollary.\nNovember 11, 2008 DRAFT"
    } ],
    "references" : [ {
      "title" : "Perturbation theory for the least-square problem with linear equality constraints",
      "author" : [ "L. Elden" ],
      "venue" : "BIT, 24:472–476,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Matrix Computation",
      "author" : [ "G. Golub", "C. Van Loan" ],
      "venue" : "John Hopkins University Press, Baltimore,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Backward error and condition of structured linear systems",
      "author" : [ "D. Higham", "N. Higham" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications, 13:162–175,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Collinearity and total least squares",
      "author" : [ "R. Fierro", "J. Bunch" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications, 15:1167– 1181,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Solution for Ill-Posed Problems",
      "author" : [ "A. Tikhonov", "V. Arsenin" ],
      "venue" : "Wiley, New York,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B, 58(1):267–288,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani" ],
      "venue" : "Annals of Statistics, 32(2):407–499,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Atomic decomposition by basis pursuit",
      "author" : [ "S. Chen", "D. Donoho", "M. Saunders" ],
      "venue" : "SIAM Journal on Scientific Computing, 20(1):33–61,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "On sparse representation in pairs of bases",
      "author" : [ "A. Feuer", "A. Nemirovski" ],
      "venue" : "IEEE Transactions on Information Theory, 49(6):1579–1581,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "E. Candès", "J. Romberg", "T. Tao" ],
      "venue" : "IEEE Transactions on Information Theory, 52(2):489–509,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Greed is good: Algorithmic results for sparse approximation",
      "author" : [ "J. Tropp" ],
      "venue" : "IEEE Transactions on Information Theory, 50(10):2231–2242,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Sharp thresholds for noisy and high-dimensional recovery of sparsity using l1-constrained quadratic programming",
      "author" : [ "M. Wainwright" ],
      "venue" : " Technical Report Available from: http://http://www.stat.berkeley.edu/tech-reports/709.pdf, Department of Statistics, UC Berkeley,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust solutions to least-squares problems with uncertain data",
      "author" : [ "L. El Ghaoui", "H. Lebret" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications, 18:1035–1064,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Second order cone programming approaches for handling missing and uncertain data",
      "author" : [ "P. Shivaswamy", "C. Bhattacharyya", "A. Smola" ],
      "venue" : "Journal of Machine Learning Research, 7:1283–1314, July",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Optimal inequalities in probability theory: A convex optimization approach",
      "author" : [ "D. Bertsimas", "I. Popescu" ],
      "venue" : "SIAM Journal of Optimization, 15(3):780–800,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Robust regression and Lasso",
      "author" : [ "H. Xu", "C. Caramanis", "S. Mannor" ],
      "venue" : "Technical report, Gerad, Available from http://www.cim.mcgill.ca/∼xuhuan/LassoGerad.pdf,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The price of robustness",
      "author" : [ "D. Bertsimas", "M. Sim" ],
      "venue" : "Operations Research, 52(1):35–53, January",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "H. Zou", "T. Hastie" ],
      "venue" : "Journal Of The Royal Statistical Society Series B, 67(2):301–320,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Just relax: Convex programming methods for identifying sparse signals",
      "author" : [ "J. Tropp" ],
      "venue" : "IEEE Transactions on Information Theory, 51(3):1030–1051,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An equivalence between sparse approximation and support vector machines",
      "author" : [ "F. Girosi" ],
      "venue" : "Neural Computation, 10(6):1445– 1480,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Entropy-based algorithms for best-basis selection",
      "author" : [ "R.R. Coifman", "M.V. Wickerhauser" ],
      "venue" : "IEEE Transactions on Information Theory, 38(2):713–718,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Matching Pursuits with time-frequence dictionaries",
      "author" : [ "S. Mallat", "Z. Zhang" ],
      "venue" : "IEEE Transactions on Signal Processing, 41(12):3397–3415,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Compressed sensing",
      "author" : [ "D. Donoho" ],
      "venue" : "IEEE Transactions on Information Theory, 52(4):1289–1306,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Remarks on some nonparametric estimates of a density function",
      "author" : [ "M. Rosenblatt" ],
      "venue" : "Annals of Mathematical Statistics, 27:832–837,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1956
    }, {
      "title" : "On the estimation of a probability density function and the mode",
      "author" : [ "E. Parzen" ],
      "venue" : "Annals of Mathematical Statistics, 33:1065– 1076,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "Nonparametric Density Estimation: the l1 View",
      "author" : [ "L. Devroye", "L. Györfi" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Multivariate Density Estimation: Theory, Practice and Visualization",
      "author" : [ "D. Scott" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "The necessary and sufficient conditions for consistency in the empirical risk minimization method",
      "author" : [ "V. Vapnik", "A. Chervonenkis" ],
      "venue" : "Pattern Recognition and Image Analysis, 1(3):260–284,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Stability and generalization",
      "author" : [ "O. Bousquet", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research, 2:499–526,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Weak Convergence and Empirical Processes",
      "author" : [ "A. van der Vaart", "J. Wellner" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2000
    }, {
      "title" : "Sparse algorithms are not stable: A no-free-lunch theorem",
      "author" : [ "H. Xu", "C. Caramanis", "S. Mannor" ],
      "venue" : "Proceedings of Forty-Sixth Allerton Conference on Communication, Control, and Computing,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It is well known that minimizing the least squared error can lead to sensitive solutions [1]– [4].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "It is well known that minimizing the least squared error can lead to sensitive solutions [1]– [4].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "2 also for its ability to recover the sparsity pattern exactly with probability one, asymptotically as the number of observations increases (there is an extensive literature on this subject, and we refer the reader to [8]–[12] and references therein).",
      "startOffset" : 218,
      "endOffset" : 221
    }, {
      "referenceID" : 11,
      "context" : "2 also for its ability to recover the sparsity pattern exactly with probability one, asymptotically as the number of observations increases (there is an extensive literature on this subject, and we refer the reader to [8]–[12] and references therein).",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 12,
      "context" : "In itself, this interpretation of Lasso as the solution to a robust least squares problem is a development in line with the results of [13].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "Most of the research in this area considers either the case where the disturbance is row-wise uncoupled [14], or the case where the Frobenius norm of the disturbance matrix is bounded [13].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "Most of the research in this area considers either the case where the disturbance is row-wise uncoupled [14], or the case where the Frobenius norm of the disturbance matrix is bounded [13].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "Taking ci = c and normalizing ai for all i, Problem (3) recovers the well-known Lasso [6], [7].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "Taking ci = c and normalizing ai for all i, Problem (3) recovers the well-known Lasso [6], [7].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "We postpone the proof to the appendix, and refer the reader to [15] for similar results using semi-definite optimization.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "For example, it is straightforward to show that if we take both ‖ · ‖α and ‖ · ‖s as the Euclidean norm, then U ′ is the set of matrices with their Frobenious norms bounded, and Corollary 1 reduces to the robust formulation introduced by [13].",
      "startOffset" : 238,
      "endOffset" : 242
    }, {
      "referenceID" : 15,
      "context" : "We refer the interested reader to [16] for full details.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "In particular, one can use the polytope uncertainty discussed above, to show (see [16]) that by employing an uncertainty set first used in [17], we can model cardinality constrained noise, where some (unknown) subset of at most k features can be corrupted.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "In particular, one can use the polytope uncertainty discussed above, to show (see [16]) that by employing an uncertainty set first used in [17], we can model cardinality constrained noise, where some (unknown) subset of at most k features can be corrupted.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "The resulting formulation resembles the elastic-net formulation [18], where there is a combination of l and l regularization.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "Lasso’s ability to recover sparse solutions has been extensively studied and discussed (cf [8]–[11]).",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "Lasso’s ability to recover sparse solutions has been extensively studied and discussed (cf [8]–[11]).",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : ", [19]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "Using similar tools, we provide additional results in [16].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]–[11], [20]–[23] and many others).",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]–[11], [20]–[23] and many others).",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]–[11], [20]–[23] and many others).",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]–[11], [20]–[23] and many others).",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : ", [19], and are used as standard tools in investigating sparsity of Lasso from the statistical perspective.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 23,
      "context" : "The kernel density estimator for a density ĥ in R, originally proposed in [24], [25], is defined by hn(x) = (nc d n) −1 n ∑",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : "The kernel density estimator for a density ĥ in R, originally proposed in [24], [25], is defined by hn(x) = (nc d n) −1 n ∑",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 25,
      "context" : "See [26], [27] and the reference therein for detailed discussions.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 26,
      "context" : "See [26], [27] and the reference therein for detailed discussions.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "A celebrated property of a kernel density estimator is that it converges in L1 to ĥ when cn ↓ 0 and ncn ↑ ∞ [26].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 27,
      "context" : ", [28]) and algorithmic stability (e.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 28,
      "context" : ", [29]) often work for a limited range of algorithms, e.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 25,
      "context" : "1 of [26]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 29,
      "context" : "6 of [30]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 30,
      "context" : "This is a special case of a more general result we prove in [31], where we show that this is a common property for all algorithms that encourage sparsity.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : "We recall the definition of uniform stability [29] first.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "However, as shown in [29], Tikhonov-regularized regression has stability that scales as 1/m.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 28,
      "context" : "Stability that scales at least as fast as o( 1 √ m ) can be used to establish strong PAC bounds (see [29]).",
      "startOffset" : 101,
      "endOffset" : 105
    } ],
    "year" : 2008,
    "abstractText" : "Lasso, or l regularized least squares, has been explored extensively for its remarkable sparsity properties. It is shown in this paper that the solution to Lasso, in addition to its sparsity, has robustness properties: it is the solution to a robust optimization problem. This has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows a principled selection of the regularizer, and in particular, generalizations of Lasso that also yield convex optimization problems are obtained by considering different uncertainty sets. Secondly, robustness can itself be used as an avenue to exploring different properties of the solution. In particular, it is shown that robustness of the solution explains why the solution is sparse. The analysis as well as the specific results obtained differ from standard sparsity results, providing different geometric intuition. Furthermore, it is shown that the robust optimization formulation is related to kernel density estimation, and based on this approach, a proof that Lasso is consistent is given using robustness directly. Finally, a theorem saying that sparsity and algorithmic stability contradict each other, and hence Lasso is not stable, is presented.",
    "creator" : "LaTeX with hyperref package"
  }
}