{
  "name" : "1609.06390.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices",
    "authors" : [ "Kirthevasan Kandasamy", "Maruan Al-Shedivat", "Eric P. Xing" ],
    "emails" : [ "kandasamy@cs.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Hidden Markov models (HMMs) [1] are one of the most popular statistical models for analyzing time series data in various application domains such as speech recognition, medicine, and meteorology. In an HMM, a discrete hidden state undergoes Markovian transitions from one of m possible states to another at each time step. If the hidden state at time t is ht, we observe a random variable xt ∈ X drawn from an emission distribution, Oj = P(xt|ht = j). In its most basic form X is a discrete set and Oj are discrete distributions. When dealing with continuous observations, it is conventional to assume that the emissions Oj belong to a parametric class of distributions, such as Gaussian.\nRecently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2–4]. At a high level, these methods estimate higher order moments from the data and recover the parameters via a series of matrix operations such as singular value decompositions, matrix multiplications and pseudo-inverses of the moments. In the case of discrete HMMs [2], these moments correspond exactly to the joint probabilities of the observations in the sequence.\nAssuming parametric forms for the emission densities is often too restrictive since real world distributions can be arbitrary. Parametric models may introduce incongruous biases that cannot be reduced even with large datasets. To address this problem, we study nonparametric HMMs only assuming some mild smoothness conditions on the emission densities. We design a spectral algorithm for this setting. Our methods leverage some recent advances in continuous linear algebra [5, 6] which views two-dimensional functions as continuous analogues of matrices. Chebyshev polynomial approximations enable efficient computation of algebraic operations on these continuous objects [7, 8]. Using these ideas, we extend existing spectral methods for discrete HMMs to the continuous nonparametric setting. Our main contributions are:\n∗Joint lead authors.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 9.\n06 39\n0v 1\n[ st\nat .M\nL ]\n2 1\nSe p\n20 16\n1. We derive a spectral learning algorithm for HMMs with nonparametric emission densities. While the algorithm is similar to previous spectral methods for estimating models with a finite number of parameters, many of the ideas used to generalise it to the nonparametric setting are novel, and, to the best of our knowledge, have not been used before in the machine learning literature.\n2. We establish sample complexity bounds for our method. For this, we derive concentration results for nonparametric density estimation and novel perturbation theory results for the aforementioned continuous matrices. The perturbation results are new and might be of independent interest.\n3. We implement our algorithm by approximating the density estimates via Chebyshev polynomials which enables efficient computation of many of the continuous matrix operations. Our method outperforms natural competitors in this setting on synthetic and real data and is computationally more efficient than most of them. Our Matlab code is available at github.com/alshedivat/nphmm.\nWhile we focus on HMMs in this exposition, we believe that the ideas presented in this paper can be easily generalised to estimating other latent variable models and predictive state representations [9] with nonparametric observations using approaches developed by Anandkumar et al. [3].\nRelated Work: Parametric HMMs are usually estimated using maximum likelihood principle via EM techniques [10] such as the Baum-Welch procedure [11]. However, EM is a local search technique, and optimization of the likelihood may be difficult. Hence, recent work on spectral methods has gained appeal. Our work builds on Hsu et al. [2] who showed that discrete HMMs can be learned efficiently, under certain conditions. The key idea is that any HMM can be completely characterised in terms of quantities that depend entirely on the observations, called the observable representation, which can be estimated from data. Siddiqi et al. [4] show that the same algorithm works under slightly more general assumptions. Anandkumar et al. [3] proposed a spectral algorithm for estimating more general latent variable models with parametric observations via a moment matching technique.\nThat said, there has been little work on estimating latent variable models, including HMMs, when the observations are nonparametric. A commonly used heuristic is the nonparametric EM [12] which lacks theoretical underpinnings. This should not be surprising because EM is a maximum likelihood procedure and, for most nonparametric problems, the maximum likelihood estimate is degenerate [13]. In their work, Siddiqi et al. [4] proposed a heuristic based on kernel smoothing, with no theoretical justification, to modify the discrete algorithm for continuous observations. Further, their procedure cannot be used to recover the joint or conditional probabilities of a sequence which would be needed to compute probabilities of events and other inference tasks.\nSong et al. [14, 15] developed an RKHS-based procedure for estimating the Hilbert space embedding of an HMM. While they provide theoretical guarantees, their bounds are in terms of the RKHS distance of the true and estimated embeddings. This metric depends on the choice of the kernel and it is not clear how it translates to a suitable distance measure on the observation space such as an L1 or L2 distance. While their method can be used for prediction and pairwise testing, it cannot recover the joint and conditional densities. On the contrary, our model provides guarantees in terms of the more interpretable total variation distance and is able to recover the joint and conditional probabilities."
    }, {
      "heading" : "2 A Pint-sized Review of Continuous Linear Algebra",
      "text" : "We begin with a pint-sized review on continuous linear algebra which treats functions as continuous analogues of matrices. Appendix A contains a quart-sized review. Both sections are based on [5, 6]. While these objects can be viewed as operators on Hilbert spaces which have been studied extensively in the years, the above line of work simplified and specialised the ideas to functions.\nA matrix F ∈ Rm×n is anm×n array of numbers where F (i, j) denotes the entry in row i, column j. m or n could be (countably) infinite. A column qmatrix (quasi-matrix) Q ∈ R[a,b]×m is a collection of m functions defined on [a, b] where the row index is continuous and column index is discrete. WritingQ = [q1, . . . , qm] where qj : [a, b]→ R is the j th function,Q(y, j) = qj(y) denotes the value of the j th function at y ∈ [a, b]. Q> ∈ Rm×[a,b] denotes a row qmatrix with Q>(j, y) = Q(y, j). A cmatrix (continuous-matrix) C ∈ R[a,b]×[c,d] is a two dimensional function where both row and column indices are continuous and C(y, x) is the value of the function at (y, x) ∈ [a, b] × [c, d]. C> ∈ R[c,d]×[a,b] denotes its transpose with C>(x, y) = C(y, x). Qmatrices and cmatrices permit all matrix multiplications with suitably defined inner products. For example, if R ∈ R[c,d]×m and C ∈ R[a,b]×[c,d], then CR = T ∈ R[a,b]×m where T (y, j) = ∫ d c C(y, s)R(s, j)ds.\nA cmatrix has a singular value decomposition (SVD). If C ∈ R[a,b]×[c,d], it decomposes as an infinite sum, C(y, x) = ∑∞ j=1 σjuj(y)vj(x), that converges in L\n2. Here σ1 ≥ σ2 ≥ · · · ≥ 0 are the singular values of C. {uj}j≥1 and {vj}j≥1 are functions that form orthonormal bases for L2([a, b]) and L2([c, d]), respectively. We can write the SVD as C = UΣV > by writing the singular vectors as infinite qmatrices U = [u1, u2 . . . ], V = [v1, v2 . . . ], and Σ = diag(σ1, σ2 . . . ). If only m < ∞ first singular values are nonzero, we say that C is of rank m. The SVD of a qmatrix Q ∈ R[a,b]×m is, Q = UΣV > where U ∈ R[a,b]×m and V ∈ Rm×m have orthonormal columns and Σ = diag(σ1, . . . , σm) with σ1 ≥ · · · ≥ σm ≥ 0. The rank of a column qmatrix is the number of linearly independent columns (i.e. functions) and is equal to the number of nonzero singular values. Finally, as for the finite matrices, the pseudo inverse of the cmatrix C is C† = V Σ−1U> with Σ−1 = diag(1/σ1, 1/σ2, . . . ). The pseudo inverse of a qmatrix is defined similarly."
    }, {
      "heading" : "3 Nonparametric HMMs and the Observable Representation",
      "text" : "Notation: Throughout this manuscript, we will use P to denote probabilities of events while p will denote probability density functions (pdf). An HMM characterises a probability distribution over a sequence of hidden states {ht}t≥0 and observations {xt}t≥0. At a given time step, the HMM can be in one of m hidden states, i.e. ht ∈ [m] = {1, . . . ,m}, and the observation is in some bounded continuous domain X . Without loss of generality, we take2 X = [0, 1]. The nonparametric HMM will be completely characterised by the initial state distribution π ∈ Rm, the state transition matrix T ∈ Rm×m and the emission densities Oj : X → R, j ∈ [m]. πi = P(h1 = i) is the probability that the HMM would be in state i at the first time step. The element T (i, j) = P(ht+1 = i|ht = j) of T gives the probability that a hidden state transitions from state j to state i. The emission function, Oj : X → R+, describes the pdf of the observation conditioned on the hidden state j, i.e. Oj(s) = p(xt = s|ht = j). Note that we have Oj(x) > 0, ∀x and ∫ Oj(·) = 1 for all j ∈ [m]. In this exposition, we denote the emission densities by the qmatrix, O = [O1, . . . , Om] ∈ R[0,1]×m+ .\nIn addition, let Õ(x) = diag(O1(x), . . . , Om(x)), and A(x) = TÕ(x). Let x1:t = {x1, . . . , xt} be an ordered sequence and xt:1 = {xt, . . . , x1} denote its reverse. For brevity, we will overload notation for A for sequences and write A(xt:1) = A(xt)A(xt−1) . . . A(x1). It is well known [2, 16] that the joint probability density of the sequence x1:t can be computed via p(x1:t) = 1>mA(xt:1)π.\nKey structural assumption: Previous work on estimating HMMs with continuous observations typically assumed that the emissions, Oj , take a parametric form, e.g. Gaussian. Unlike them, we only make mild nonparametric smoothness assumptions on Oj . As we will see, to estimate the HMM well in this problem we will need to estimate entire pdfs well. For this reason, the nonparametric setting is significantly more difficult than its parametric counterpart as the latter requires estimating only a finite number of parameters. When compared to the previous literature, this is the crucial distinction and the main challenge in this work.\nObservable Representation: The observable representation is a description of an HMM in terms of quantities that depend on the observations [16]. This representation is useful for two reasons: (i) it depends only on the observations and can be directly estimated from the data; (ii) it can be used to compute joint and conditional probabilities of sequences even without the knowledge of T and O and therefore can be used for inference and prediction. First, we define the joint densities, P1, P21, P321:\nP1(t) = p(x1 = t), P21(s, t) = p(x2 = s, x1 = t), P321(r, s, t) = p(x3 = r, x2 = s, x1 = t),\nwhere xi, i = 1, 2, 3 denotes the observation at time i. Denote P3x1(r, t) = P321(r, x, t) for all x. We will find it useful to view both P21, P3x1 ∈ R[0,1]×[0,1] as cmatrices. We will also need an additional qmatrix U ∈ R[0,1]×m such that U>O ∈ Rm×m is invertible. Given one such U , the observable representation of an HMM is described by the parameters b1, b∞ ∈ Rm and B : [0, 1]→ Rm×m,\nb1 = U >P1, b∞ = (P > 21U) †P1, B(x) = (U >P3x1)(U >P21) † (1)\nAs before, for a sequence, xt:1 = {xt, . . . , x1}, we define B(xt:1) = B(xt)B(xt−1) . . . B(x1). The following lemma shows that the first m left singular vectors of P21 are a natural choice for U . Lemma 1. Let π > 0, T and O be of rank m and U be the qmatrix composed of the first m left singular vectors of P21. Then U>O is invertible.\n2 We discuss the case of higher dimensions in Section 7.\nTo compute the joint and conditional probabilities using the observable representation, we maintain an internal state, bt, which is updated as we see more observations. The internal state at time t is\nbt = B(xt−1:1)b1 b>∞B(xt−1:1)b1 . (2)\nThis definition of bt is consistent with b1. The following lemma establishes the relationship between the observable representation and the internal states to the HMM parameters and probabilities.\nLemma 2 (Properties of the Observable Representation). Let rank(T ) = rank(O) = m and U>O be invertible. Let p(x1:t) denote the joint density of a sequence x1:t and p(xt+1:t+t′ |x1:t) denote the conditional density of xt+1:t+t′ given x1:t in a sequence x1:t+t′ . Then the following are true.\n1. b1 = U>Oπ 2. b∞ = 1>m(U >O)−1 3. B(x) = (U>O)A(x)(U>O)−1 ∀x ∈ [0, 1].\n4. bt+1 = B(xt)bt/(b>∞B(xt)bt). 5. p(x1:t) = b>∞B(xt:1)b1. 6. p(xt+t′:t+1|x1:t) = b>∞B(xt+t′:t+1)bt.\nThe last two claims of the Lemma 2 show that we can use the observable representation for computing the joint and conditional densities. The proofs of Lemmas 1 and 2 are similar to the discrete case and mimic Lemmas 2, 3 & 4 of Hsu et al. [2]."
    }, {
      "heading" : "4 Spectral Learning of HMMs with Nonparametric Emissions",
      "text" : "The high level idea of our algorithm, NP-HMM-SPEC, is as follows. First we will obtain density estimates for P1, P21, P321 which will then be used to recover the observable representation b1, b∞, B by plugging in the expressions in (1). Lemma 2 then gives us a way to estimate the joint and conditional probability densities. For now, we will assume that we have N i.i.d sequences of triples {X(j)}Nj=1 where X(j) = (X (j) 1 , X (j) 2 , X (j) 3 ) are the observations at the first three time steps. We describe learning from longer sequences in Section 4.3."
    }, {
      "heading" : "4.1 Kernel Density Estimation",
      "text" : "The first step is the estimation of the joint probabilities which requires a nonparametric density estimate. While there are several techniques [17], we use kernel density estimation (KDE) since it is easy to analyse and works well in practice. The KDE for P1, P21, and P321 take the form:\nP̂1(t) = 1\nN N∑ j=1 1 h1 K ( t−X(j)1 h1 ) , P̂21(s, t) = 1 N N∑ j=1 1 h221 K ( s−X(j)2 h21 ) K ( t−X(j)1 h21 ) ,\nP̂321(r, s, t) = 1\nN N∑ j=1 1 h3321 K ( r −X(j)3 h321 ) K ( s−X(j)2 h321 ) K ( t−X(j)1 h321 ) . (3)\nHere K : [0, 1] → R is a symmetric function called a smoothing kernel and satisfies (at the very least) ∫ 1 0 K(s)ds = 1, ∫ 1 0 sK(s)ds = 0. The parameters h1, h21, h321 are the bandwidths, and are typically decreasing with N . In practice they are usually chosen via cross-validation."
    }, {
      "heading" : "4.2 The Spectral Algorithm",
      "text" : "Algorithm 1 NP-HMM-SPEC Input: Data {X(j) = (X(j)1 , X (j) 2 , X (j) 3 )}Nj=1, number of states m.\n• Obtain estimates P̂1, P̂21, P̂321 for P1, P21, P321 via kernel density estimation (3). • Compute the cmatrix SVD of P̂21. Let Û ∈ R[0,1]×m be the first m left singular vectors of P̂21. • Compute the parameters observable representation. Note that B̂ is a Rm×m valued function.\nb̂1 = Û >P̂1, b̂∞ = (P > 21Û) †P̂1, B̂(x) = (Û >P̂3x1)(Û >P̂21) †\nThe algorithm, given above in Algorithm 1, follows the roadmap set out at the beginning of this section. While the last two steps are similar to the discrete HMM algorithm of Hsu et al. [2], the SVD, pseudoinverses and multiplications are with q/c-matrices. Once we have the estimates b̂1, b̂∞, and B̂(x) the joint and predictive (conditional) densities can be estimated via (see Lemma 2):\np̂(x1:t) = b̂ > ∞B̂(xt:1)̂b1, p̂(xt+t′:t+1|x1:t) = b̂>∞B̂(xt+t′:t+1)̂bt. (4)\nHere b̂t is the estimated internal state obtained by plugging in b̂1, b̂∞, B̂ in (2). Theoretically, these estimates can be negative in which case they can be truncated to 0 without affecting the theoretical results in Section 5. However, in our experiments these estimates were never negative."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "C/Q-Matrix operations using Chebyshev polynomials: While our algorithm and analysis are conceptually well founded, the important practical challenge lies in the efficient computation of the many aforementioned operations on c/q-matrices. Fortunately, some very recent advances in the numerical analysis literature, specifically on computing with Chebyshev polynomials, have rendered the above algorithm practical [6, Ch.3-4]. Due to the space constraints, we provide only a summary. Chebyshev polynomials is a family of orthogonal polynomials on compact intervals, known to be an excellent approximator of one-dimensional functions [18, 19]. A recent line of work [5, 8] has extended the Chebyshev technology to two dimensional functions enabling the mentioned operations and factorisations such as QR, LU and SVD [6, Sections 4.6-4.8] of continuous matrices to be carried efficiently. The density estimates P̂1, P̂21, P̂321 are approximated by Chebyshev polynomials to within machine precision. Our implementation makes use of the Chebfun library [7] which provides an efficient implementation for the operations on continuous and quasi matrices.\nComputation time: Representing the KDE estimates P̂1, P̂21, P̂321 using Chebfun was roughly linear in N and is the brunt of the computational effort. The bandwidths for the three KDE estimates are chosen via cross validation which takes O(N2) effort. However, in practice the cost was dominated by the Chebyshev polynomial approximation. In our experiments we found that NPHMM-SPEC runs in linear time in practice and was more efficient than most alternatives. Training with longer sequences: When training with longer sequences we can use a sliding window of length 3 across the sequence to create the triples of observations needed for the algorithm. That is, given N samples each of length `(j), j = 1, . . . , N , we create an augmented dataset of triples { {(X(j)t , X (j) t+1, X (j) t+2)} `(j)−2 t=1 }Nj=1 and run NP-HMM-SPEC with the augmented data. As is with conventional EM procedures, this requires the additional assumption that the initial state is the stationary distribution of the transition matrix T ."
    }, {
      "heading" : "5 Analysis",
      "text" : "We now state our assumptions and main theoretical results. Following [2, 4, 14] we assume i.i.d sequences of triples are used for training. With longer sequences, the analysis should only be modified to account for the mixing of the latent state Markov chain, which is inessential for the main intuitions. We begin with the following regularity condition on the HMM. Assumption 3. π > 0 element-wise. T ∈ Rm×m and O ∈ R[0,1]×m are of rank m.\nThe rank condition on O means that emission pdfs are linearly independent. If either T or O are rank deficient, then the learner may confuse state outputs, which makes learning difficult3. Next, while we make no parametric assumptions on the emissions, some smoothness conditions are used to make density estimation tractable. We use the Hölder class, H1(β, L), which is standard in the nonparametrics literature. For β = 1, this assumption reduces to L-Lipschitz continuity. Assumption 4. All emission densities belong to the Hölder class,H1(β, L). That is, they satisfy,\nfor all α ≤ bβc, j ∈ [m], s, t ∈ [0, 1] ∣∣∣∣dαOj(s)dsα − dαOj(t)dtα ∣∣∣∣ ≤ L|s− t|β−|α|. Here bβc is the largest integer strictly less than β.\n3 Siddiqi et al. [4] show that the discrete spectral algorithm works under a slightly more general setting. Similar results hold for the nonparametric case too but will restrict ourselves to the full rank setting for simplicity.\nUnder the above assumptions we bound the total variation distance between the true and the estimated densities of a sequence, x1:t. Let κ(O) = σ1(O)/σm(O) denote the condition number of the observation qmatrix. The following theorem states our main result. Theorem 5. Pick any sufficiently small > 0 and a failure probability δ ∈ (0, 1). Let t ≥ 1. Assume that the HMM satisfies Assumptions 3 and 4 and the number of samples N satisfies,\nN\nlog(N) ≥ Cm1+\n3 2β\nκ(O)2+ 3 β\nσm(P21) 4+ 4β\n( t )2+ 3β log ( 1\nδ\n)1+ 32β .\nThen, with probability at least 1 − δ, the estimated joint density for a t-length sequence satisfies∫ |p(x1:t)− p̂(x1:t)|dx1:t ≤ . Here, C is a constant depending on β and L and p̂ is from (4).\nSynopsis: Observe that the sample complexity depends critically on the conditioning of O and P21. The closer they are to being singular, the more samples is needed to distinguish different states and learn the HMM. It is instructive to compare the results above with the discrete case result of Hsu et al. [2], whose sample complexity bound4 is N & m κ(O) 2\nσm(P21)4 t2 2 log 1 δ . Our bound is different in two\nregards. First, the exponents are worsened by additional ∼ 1β terms. This characterizes the difficulty of the problem in the nonparametric setting. While we do not have any lower bounds, given the current understanding of the difficulty of various nonparametric tasks [20–22], we think our bound might be unimprovable. As the smoothness of the densities increases β → ∞, we approach the parametric sample complexity. The second difference is the additional log(N) term on the left hand side. This is due to the fact that we want the KDE to concentrate around its expectation in L2 over [0, 1], instead of just point-wise. It is not clear to us whether the log can be avoided.\nTo prove Theorem 5, first we will derive some perturbation theory results for c/q-matrices; we will need them to bound the deviation of the singular values and vectors when we use P̂21 instead of P21. Some of these perturbation theory results for continuous linear algebra are new and might be of independent interest. Next, we establish a concentration result for the kernel density estimator."
    }, {
      "heading" : "5.1 Some Perturbation Theory Results for C/Q-matrices",
      "text" : "The first result is an analog of Weyl’s theorem which bounds the difference in the singular values in terms of the operator norm of the perturbation. Weyl’s theorem has been studied for general operators [23] and cmatrices [6]. We have given one version in Lemma 21 of Appendix B. In addition to this, we will also need to bound the difference in the singular vectors and the pseudo-inverses of the truth and the estimate. To our knowledge, these results are not yet known. To that end, we establish the following results. Here σk(A) denotes the kth singular value of a c/q-matrix A.\nLemma 6 (Simplified Wedin’s Sine Theorem for Cmatrices). Let A, Ã, E ∈ R[0,1]×[0,1] where Ã = A+ E and rank(A) = m. Let U, Ũ ∈ R[a,b]×m be the first m left singular vectors of A and Ã respectively. Then, for all x ∈ Rm, ‖Ũ>Ux‖2 ≥ ‖x‖2 √ 1− 2‖E‖2L2/σm(Ã)2.\nLemma 7 (Pseudo-inverse Theorem for Qmatrices). Let A, Ã, E ∈ R[a,b]×m and Ã = A+E. Then, σ1(A † − Ã†) ≤ 3 max{σ1(A†)2, σ1(A†)2}σ1(E)."
    }, {
      "heading" : "5.2 Concentration Bound for the Kernel Density Estimator",
      "text" : "Next, we bound the error for kernel density estimation. To obtain the best rates under Hölderian assumptions on O, the kernels used in KDE need to be of order β. A β order kernel satisfies,∫ 1\n0\nK(s)ds = 1,\n∫ 1 0 sαK(s)ds = 0, for all α ≤ bβc, ∫ 1 0 sβK(s)ds ≤ ∞. (5)\nSuch kernels can be constructed using Legendre polynomials [17]. Given N i.i.d samples from a d dimensional density f , where d ∈ {1, 2, 3} and f ∈ {P1, P21, P321}, for appropriate choices of the bandwidths h1, h21, h321, the KDE f̂ ∈ {P̂1, P̂21, P̂321} concentrates around f . Informally, we show\nP ( ‖f̂ − f‖L2 > ε ) . exp ( − log(N) d 2β+dN 2β 2β+d ε2 ) . (6)\n4 Hsu et al. [2] provide a more refined bound but we use this form to simplify the comparison.\nfor all sufficiently small ε and N/ logN & ε−2+ d β . Here .,& denote inequalities ignoring constants. See Appendix C for a formal statement. Note that when the observations are either discrete or parametric, it is possible to estimate the distribution using O(1/ε2) samples to achieve ε error in a suitable metric, say, using the maximum likelihood estimate. However, the nonparametric setting is inherently more difficult and therefore the rate of convergence is slower. This slow convergence is also observed in similar concentration bounds for the KDE [24, 25].\nA note on the Proofs: For Lemmas 6, 7 we follow the matrix proof in Stewart and Sun [26] and derive several intermediate results for c/q-matrices in the process. The main challenge is that several properties for matrices, e.g. the CS and Schur decompositions, are not known for c/q-matrices. In addition, dealing with various notions of convergences with these infinite objects can be finicky. The main challenge with the KDE concentration result is that we want an L2 bound – so usual techniques (such as McDiarmid’s [13, 17]) do not apply. We use a technical lemma from Giné and Guillou [25] which allows us to bound the L2 error in terms of the VC characteristics of the class of functions induced by an i.i.d sum of the kernel. The proof of theorem 5 just mimics the discrete case analysis of Hsu et al. [2]. While, some care is needed (e.g. ‖x‖L2 ≤ ‖x‖L1 does not hold for functional norms) the key ideas carry through once we apply Lemmas 21, 6, 7 and (6). A more refined bound on N that is tighter in polylog(N) terms is possible – see Corollary 25 and equation 13 in the appendix."
    }, {
      "heading" : "6 Experiments",
      "text" : "We compare NP-HMM-SPEC to the following. MG-HMM: An HMM trained using EM with the emissions modeled as a mixture of Gaussians. We tried 2, 4 and 8 mixtures and report the best result. NP-HMM-BIN: A naive baseline where we bin the space into n intervals and use the discrete spectral algorithm [2] with n states. We tried several values for n and report the best. NP-HMM-EM: The Nonparametric EM heuristic of [12]. NP-HMM-HSE: The Hilbert space embedding method of [14]. Synthetic Datasets: We first performed a series of experiments on synthetic data where the true distribution is known. The goal is to evaluate the estimated models against the true model. We generated triples from two HMMs with m = 4 and m = 8 states and nonparametric emissions. The details of the set up are given in Appendix E. Fig. 1 presents the results.\nFirst we compare the methods on estimating the one step ahead conditional density p(x6|x1:5). We report the L1 error between the true and estimated models. In Fig. 2 we visualise the estimated one step ahead conditional densities. NP-HMM-SPEC outperforms all methods on this metric. Next, we compare the methods on the prediction performance. That is, we sample sequences of length 6 and test how well a learned model can predict x6 conditioned on x1:5. When comparing on squared error, the best predictor is the mean of the distribution. For all methods we use the mean of p̂(x6|x1:5) except\nfor NP-HMM-HSE for which we used the mode since the mean cannot be computed. No method can do better than the true model (shown via the dotted line) in expectation. NP-HMM-SPEC achieves the performance of the true model with large datasets. Finally, we compare the training times of all methods. NP-HMM-SPEC is orders of magnitude faster than NP-HMM-HSE and NP-HMM-EM. Note that the error of MG-HMM– a parametric model – stops decreasing even with large data. This is due to the bias introduced by the parametric assumption. We do not train NP-HMM-EM for longer sequences because it is too slow. A limitation of the NP-HMM-HSE method is that it cannot recover conditional probabilities – so we exclude it from that experiment. We exclude NP-HMM-BIN from the time comparison because it was much faster than all other methods. We could not include the method of [4] in our comparisons since their code was not available and their method isn’t straightforward to implement. Further, their method cannot compute joint/predictive probabilities.\nReal Datasets: We compare all the above methods (except NP-HMM-EM which was too slow) on prediction error on 3 real datasets: internet traffic [27], laser generation [28] and sleep data [29]. Each model was trained using a training sequence and then the predictions were computed on a test sequence. The details on these datasets are in Appendix E. For all methods we used the mode of the conditional distribution p(xt+1|x1:t) as the prediction as it performed better. For NP-HMM-SPEC, NP-HMM-HSE,NP-HMM-BIN we follow the procedure outlined in Section 4.3 to create triples and train with the triples. In Table 1 we report the mean prediction error and the standard error. NP-HMMHSE and NP-HMM-SPEC perform better than the other two methods. However, NP-HMM-SPEC was faster to train (and has other attractive properties) when compared to NP-HMM-HSE."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We proposed and studied a method for estimating the observable representation of a Hidden Markov Model whose emission probabilities are smooth nonparametric densities. We derive a bound on the sample complexity for our method. While our algorithm is similar to existing methods for discrete models, many of the ideas that generalise it to the nonparametric setting are new. In comparison to other methods, the proposed approach has some desirable characteristics: we can recover the joint/conditional densities, our theoretical results are in terms of more interpretable metrics, the method outperforms baselines and is orders of magnitude faster to train.\nIn this exposition only focused on one dimensional observations. The multidimensional case is handled by extending the above ideas and technology to multivariate functions. Our algorithm and the analysis carry through to the d-dimensional setting, mutatis mutandis. The concern however, is more practical. While we have the technology to perform various c/q-matrix operations for d = 1 using Chebyshev polynomials, this is not yet the case for d > 1. Developing efficient procedures for these operations in the high dimensional settings is a challenge for the numerical analysis community and is beyond the scope of this paper. That said, some recent advances in this direction are promising [8, 30].\nWhile our method has focused on HMMs, the ideas in this paper apply for a much broader class of problems. Recent advances in spectral methods for estimating parametric predictive state representations [31], mixture models [3] and other latent variable models [32] can be generalised to the nonparamatric setting using our ideas. Going forward, we wish to focus on such models."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank Alex Townsend, Arthur Gretton, and Ahmed Hefny for the helpful discussions."
    }, {
      "heading" : "A A Quart-sized Review of Continuous Linear Algebra",
      "text" : "In this section we introduce continuous analogues of matrices and their factorisations. We only provide a brief quart-sized review for what is needed in this exposition. Chapters 3 and 4 of Townsend [6] contains a reservoir-sized review.\nA matrix F ∈ Rm×n is an m× n array of numbers where F (i, j) denotes the entry in row i, column j. We will also look at cases where either m or n is infinite. A column qmatrix (quasi-matrix) Q ∈ R[a,b]×m is a collection of m functions defined on [a, b] where the row index is continuous and column index is discrete. Writing Q = [q1, . . . , qm] where qj : [a, b] → R is the j th function, Q(y, j) = qj(y) denotes the value of the j th function at y ∈ [a, b]. Q> ∈ Rm×[a,b] denotes a row qmatrix withQ>(j, y) = Q(y, j). A cmatrix (continous-matrix)C ∈ R[a,b]×[c,d] is a two dimensional function where both the row and column indices are continuous and C(y, x) is value of the function at (y, x) ∈ [a, b]× [c, d]. C> ∈ R[c,d]×[a,b] denotes its transpose with C>(x, y) = C(y, x). Qmatrices and cmatrices permit all matrix multiplications with suitably defined inner products. Let F ∈ Rm×n, Q ∈ R[a,b]×m, P ∈ R[a,b]×n, R ∈ R[c,d]×m and C ∈ R[a,b]×[c,d]. It follows that F (:, j) ∈ Rm, Q(y, :) ∈ R1×m, Q(:, i) ∈ R[a,b], C(y, :) ∈ R1×[c,d] etc. Then the following hold:\n• QF = S ∈ R[a,b]×n where S(y, j) = Q(y, :)F (:, j) = ∑m k=1Q(y, k)F (i, k).\n• Q>P = H ∈ Rm×n where H(i, j) = Q(:, j)>P (:, j) = ∫ b a Q>(i, s)P (s, j)ds.\n• QR> = D ∈ R[a,b]×[c,d] where D(y, x) = Q(y, :)R(x, :)> = ∑m\n1 Q(y, k)R >(k, x). • CR = T ∈ R[a,b]×m where T (y, j) = C(y, :)R(:, j) = ∫ d c C(y, s)R(s, j)ds.\nHere, the integrals are with respect to the Lebesgue measure.\nA cmatrix has a singular value decomposition (SVD). If C ∈ R[a,b]×[c,d], an SVD of C is the sum C(y, x) = ∑∞ j=1 σjuj(y)vj(x), which converges in L\n2. Here σ1 ≥ σ2 ≥ . . . . are the singular values of C. {uj}j≥1 and {vj}j≥1 are the left and right singular vectors and form orthonormal bases for L2([a, b]) and L2([c, d]) respectively, i.e. ∫ b a uj(s)uk(s)ds = 1(j = k). It is known that the SVD of a cmatrix exists uniquely with σj → 0, and continuous singular vectors (Theorem 3.2, [6]). Further, if C is Lipshcitz continuous w.r.t both variables then the SVD is absolutely and uniformly convergent. Writing the singular vectors as infinite qmatrices U = [u1, u2 . . . ], V = [v1, v2 . . . ], and Σ = diag(σ1, σ2 . . . ) we can write the SVD as,\nC = UΣV > = ∞∑ j=1 σjU(:, j)V (:, j) >.\nIf only m <∞ singular values are nonzero then we say that C is of rank m. The SVD of a Qmatrix Q ∈ R[a,b]×m is, Q = UΣV > = ∑m j=1 σjU(:, j)V (:, j)\n>, where U ∈ R[a,b]×m and V ∈ Rm×m have orthonormal columns and Σ = diag(σ1, . . . , σm) with σ1 ≥ σ2 ≥ · · · ≥ σm ≥ 0. The SVD of a qmatrix also exists uniquely (Theorem 4.1, [6]). The rank of a column qmatrix is the number of linearly independent columns (i.e. functions) and is equal to the number of nonzero singular values.\nFinally, the pseudo inverse of the cmatrix C is C† = V Σ−1U> with Σ−1 = diag(1/σ1, 1/σ2, . . . ). The p-operator norm of a cmatrix, for 1 ≤ p ≤ ∞ is ‖C‖p = sup‖x‖p=1 ‖Cx‖p where x ∈ R [c,d],\nCx ∈ R[a,b], ‖x‖pp = ∫ d c (x(s))pds for p < ∞ and ‖x‖∞ = sups∈[c,d] x(s). The Frobenius\nnorm of a cmatrix is ‖C‖F = (∫ b\na ∫ d c C(y, x)2dxdy )1/2 . It can be shown that ‖C‖2 = σ1 and\n‖C‖2F = ∑ j σ 2 j where σ1 ≥ σ2 ≥ . . . are its singular values. Note that analogous relationships hold with finite matrices. The pseudo inverse and norms of a qmatrix are similarly defined and similar relationships hold with its singular values.\nNotation: In what follows we will use 1[a,b] to denote the function taking value 1 everywhere in [a, b] and 1m to denote m-vectors of 1’s. When we are dealing with Lp norms of a function we will explicitly use the subscript Lp to avoid confusion with the operator/Frobenius norms of qmatrices and cmatrices. For example, for a cmatrix ‖C‖2L2 = ∫ ∫ C(·, ·)2 = ‖C‖2F . As we have already done, throughout the paper we will overload notation for inner products, multiplications and pseudo-inverses depending on whether they hold for matrices, qmatrices or cmatrices. E.g. when\np, q ∈ Rm, p>q = ∑m 1 piqi and when p, q ∈ R[a,b], p>q = ∫ b a p(s)q(s)ds. P will be used to denote probabilities of events while p will denote probability density functions (pdf)."
    }, {
      "heading" : "B Some Perturbation Theory Results for Continuous Linear Algebra",
      "text" : "We recommend that readers unfamiliar with continuous linear algebra first read the review in Appendix A. Throughout this section L(·) maps a matrix (including q/cmatrices) to its eigenvalues. Similarly, σ(·) maps a matrix to its singular values. When we are dealing with infinite sequences and qmatrices “=\" refers to convergence in L2. When dealing with infinite sequences and cmatrices, “=\" refers to convergence in the operator norm. For all theorems, we follow the template of Stewart and Sun [26] for the matrix case and hence try to stick with their notation.\nBefore we proceed, we introduce the “cmatrix\" I[0,1] on [0, 1]. For any u ∈ R[0,1] this is the operator which satisfies I[0,1]u = u. That is, (I[0,1]u)(y) = ∫ 1 0 I[0,1](y, x)u(x)dx = u(y). Intuitively, it can be thought of as the Dirac delta function along the diagonal, δ(x − y). Let Q = [q1, q2, . . . , ] ∈ R[0,1]×∞ be a qmatrix containing an orthonormal basis for [0, 1] and Qk ∈ R[0,1]×k denote the first k columns of Q. We make note of the following observation.\nTheorem 8. QkQ>k → I[0,1] as k →∞. Here convergence is in the operator norm. Proof. We need to show that for all x ∈ R[0,1], ‖QkQ>k x− x‖2 → 0. Let x = Qα = ∑∞ k=1 αkqk\nbe the representation of x in the Q-basis. Here α = (α1, α2, . . . ) satisfies ∑ k α 2 k < ∞. We then\nhave ‖QkQ>k x− x‖22 = ∑∞ j=k+1 α 2 j → 0 by the properties of sequences in `2.\nWe now proceed to our main theorems. We begin with a series of intermediary results.\nTheorem 9. Let X ∈ R[0,1]×m. Define the linear operator T(X) = AX − XB where A ∈ R[0,1]×[0,1] and B ∈ Rm×m are a square cmatrix and matrix, respectively. Then, T is nonsingular if and only if L(A) ∩ L(B) = ∅.\nProof. Assume λ ∈ L(A) ∪ L(B). Then, let Ap = λp, q>B = λq> where p ∈ R[0,1] and q ∈ Rm. Then T(pq>) = 0 and T is singular. This proves one side of the theorem.\nNow, assume that L(A) ∩ L(B) = ∅. As the operator is linear, it is sufficient to show that AX −XB = C has a unique solution for any C ∈ R[0,1]×m. Let the Schur decomposition of B be Q = V >BV where V is orthogonal and Q is upper triangular. Writing Y = XV and D = CV it is sufficient to show that AY − Y Q = D has a unique solution. We write\nY = (y1, y2, . . . ym) ∈ R[0,1]×m and D = (d1, d2, . . . , dm) ∈ R[0,1]×m\nand use an inductive argument over the columns of Y .\nThe first column of Y is given by Ay1 − Q11y1 = (A − Q11I[0,1])y1 = d1. Since Q11 ∈ L(B) and L(A) ∩ L(B) is empty (A−Q11I[0,1]) is nonsingular. Therefore y1 is uniquely determined by inverting the cmatrix (see Appendix A). Assume y1, y2 . . . , yk−1 are uniquely determined. Then, the kth column is given by (A−QkkI[0,1])yk = dk+ ∑k−1 i=1 Qikyi. Again, (A−QkkI[0,1]) is nonsingular by assumption, and hence this uniquely determines yk.\nCorollary 10. Let T be as defined in Theorem 9. Then\nL(T) = L(A)− L(B) = {α− β : α ∈ L(A), β ∈ L(B)}.\nProof. If λ ∈ L(T) there exists X such that (A− λI[0,1])X −XB = 0. Therefore, by Theorem 9 there exists α ∈ L(A) and β ∈ L(B) such that λ = α− β. Therefore, L(T) ⊂ L(A)− L(B).\nConversely, consider any α ∈ L(A) and β ∈ L(B). Then there exists a ∈ R[0,1], b ∈ Rm such that Aa = αa and b>B = βb>. Writing X = ab> we have AX −XB = (α− β)ab>. Therefore, L(A)− L(B) ⊂ L(T).\nTheorem 11. Let T be as defined in Theorem 9. Then\ninf ‖X‖F=1\n‖T(X)‖F = minL(T) = min |L(A)− L(B)|. (7)\nProof. For any qmatrix P = (p1, p2, . . . , pm) ∈ R[0,1]×m let vec(P ) = [p>1 , p>2 , . . . , p>m]> ∈ R[0,m]×1 be the concatenation of all functions. Then vec(XB) = ~Bvec(X) where,\n~B =  B11I[0,1] B21I[0,1] · · · Bm1I[0,1] B12I[0,1] B22I[0,1] · · · Bm2I[0,1]\n... ...\n. . . ...\nB1mI[0,1] B2mI[0,1] · · · BmmI[0,1]  ∈ R[0,m]×[0,m]. Here I[0,1] have been translated and should be interpreted as being a dirac delta function on that block. Similarly, vec(AX) = ~Avec(X) where ~A = diag(A,A, . . . , A) ∈ R[0,m]×[0,m]. Therefore vec(T(X)) = ( ~A− ~B) ~X . Now noting that ‖X‖F = ‖vec(X)‖2 we have,\ninf ‖X‖F=1 ‖T(X)‖F = inf ‖vec(X)‖2=1 ‖vec(T(X))‖2 = min |L( ~A− ~B)|.\nThe theorem follows by noting that the eigenvalues of ( ~A− ~B) are the same as those of L(T).\nTheorem 12. Let X1, Y1 ∈ R[0,1]×` have orthonormal columns. Then, there exist Q ∈ R∞×[0,1] and U11, V11 ∈ R`×` such that the following holds,\nQX1U11 = [ I` 0 ] ∈ R∞×`, QY1V11 = [ Γ Σ 0 ] ∈ R∞×`.\nHere Γ = diag(γ1, . . . , γ`), Σ = diag(σ1, . . . , σ`) and they satisfy\n0 ≤ γ1 ≤ · · · ≤ γ`, σ1 ≥ · · · ≥ σ` ≥ 0, and γ2i + σ2i = 1, i = 1, . . . , `.\nProof. Let X2, Y2 ∈ R[0,1]×∞ be orthonormal bases for the complementary subspaces of R(X1),R(Y1), respectively. Denote X = [X1, X2], Y = [Y1, Y2] and\nW = X>Y = ( W11 W12 W21 W22 ) ∈ R∞×∞,\nwhereW11 = X>1 Y1 ∈ R`×` and the rest are defined accordingly. Now, using Theorem 5.1 from [26] there exist orthogonal matrices U = diag(U11, U22), V = diag(V11, V22) where U11, V11 ∈ R`×` and U22, V22 ∈ R∞×∞ such that the following holds,\nU>WV = ( Γ −Σ 0 Σ Γ 0 0 0 I∞ ) ∈ R∞×∞.\nHere Γ,Σ satisfy the conditions of the theorem. Now set X̂ = [X̂1, X̂2], Ŷ = [Ŷ1, Ŷ2] where X̂1 = X1U11, X̂2 = X2U11, Ŷ1 = Y1V11, Ŷ2 = Y2V11. Then, X̂>Y = U>WV . Setting Q = X̂> and setting U11, V11 as above yields,\nQX1U11 =\n( U>11X > 1\nU>22X > 2\n) X1U11 = [ I` 0 ] , QY1V11 = ( U>11X > 1\nU>22X > 2\n) Y1V11 = [ Γ Σ 0 ]\nwhere U>11X > 1 Y1U11 = Γ, U > 22X > 2 Y1U11 = [Σ >,0>]> from the decomposition of U>WV .\nRemark 13. Stewart and Sun [26] prove Theorem 5.1 for a finite unitary W . However, it is straightforward to verify that the same holds if W is a unitary operator on the `2 sequence space, i.e., Theorem 5.1 is valid for (countably) infinite matrices.\nDefinition 14 (Canonical Angles). Let X ,Y be ` dimensional subspaces of the same dimension for functions on [0, 1] and X1, Y1 ∈ R[0,1]×` be orthonormal functions spanning these subspaces. Then the canonical angles between X and Y are the diagonals of the matrix Θ[X ,Y] ∆= sin−1(Σ) where Σ is from Theorem 12. It follows that cos Θ[X ,Y] = Γ where sin and cos are in the usual trigonometric sense and satisfy cos2(x) + sin2(x) = 1.\nCorollary 15. Let X ,Y, X1, Y1 be as in Definition 14 and X2, Y2 be orthonormal functions for their complementary spaces. Then, the nonzero singular values of X>2 Y1 are the sines of the nonzero canonical angles between X ,Y . The singular values of X>1 Y1 are the cosines of the nonzero canonical angles.\nProof. From the proof of Theorem 12,\nX>2 Y1 = U22 ( Σ 0 ) U>11, X > 1 Y1 = U11ΓU > 11.\nSince U11, U22 are orthogonal, the above are the SVDs of X>2 Y1 and X > 1 Y1.\nTheorem 16. Let X ,Y be ` dimensional subspaces of functions on [0, 1] and X1, Y1 ∈ R[0,1]×l be an orthonormal bases. Let sin Θ[X ,Y] = diag(σ1, . . . , σ`). Denote PX = X1X>1 and PY = Y1Y >1 . Then, the singular values of PX (I[0,1] − PY) are σ1, σ2, . . . , σ`, 0, 0, . . . .\nProof. By Theorem 12, there exists Q ∈ R∞×[0,1], U11, V11 ∈ R`×`, such that\nQPX (I[0,1] − PY)Q> = QX1X>1 Q>Q(I[0,1] − Y1Y >1 )Q>\n= (QX1U1)(U > 1 X > 1 Q >)(I[0,1] −QY1V11(V >11Y >1 Q>)) = [ Σ 0 0 ] [Σ −Γ 0]\nHere we have used I[0,1] = Q>Q. The proof of this uses a technical argument involving the dual space of the class of operators described by cmatrices. (In the discrete matrix case this is similar to how the outer product of a complete orthonormal basis results in the identity UU> = I .) The last step follows from Theorem 12 and some algebra. Noting that [Σ −Γ 0] has orthonormal rows, it follows that the singular values of PX (I[0,1] − PY) are Σ.\nTheorem 17. Let A ∈ R[0,1]×[0,1] satisfy,\nA = [X1 X2] [ L1 0 0 L2 ] [ X>1 X>2 ] where X1 ∈ R[0,1]×` and [X1, X2] is unitary. Let Z ∈ R[0,1]×m and T = AZ − ZB where B ∈ Rm×m. Let δ = min |L(L2)− L(B)| > 0. Then,∥∥ sin Θ[R(X1),R(Z)]‖F ≤ ‖T‖F\nδ .\nProof. First note that X>2 T = L2X > 2 Z −X>2 ZB. The claim follows from Theorems 11 and 15.∥∥ sin Θ[R(X1),R(Z)]‖F = ‖X>2 Z‖F ≤ ‖X>2 T‖Fmin |L(L2)− L(B)| ≤ ‖T‖Fδ .\nTheorem 18 (Wedin’s Sine Theorem for cmatrices – Frobenius form). LetA, Ã, E ∈ R[0,1]×[0,1] with Ã = A+ E. Let A, Ã have the following conformal partitions,\nA = [U1 U2] [ Σ1 0 0 Σ2 ] [ V >1 V >2 ] , Ã = [ Ũ1 Ũ2 ] [Σ̃1 0 0 Σ̃2 ] [ Ṽ >1 Ṽ >2 ] .\nwhere U1, Ũ1 ∈ R[0,1]×m, V1, Ṽ1 ∈ R[0,1]×m and U2, Ũ2 ∈ R[0,1]×∞, V2, Ṽ2 ∈ R[0,1]×∞. Let R = AṼ1 − Ũ1Σ̃1 ∈ R[0,1]×m and S = A>Ũ1 − Ṽ1Σ̃1 ∈ R[0,1]×m. Assume there exists δ > 0 such that, min |σ(Σ̃1)− σ(Σ2)| ≥ δ and min |σ(Σ̃1)| ≥ δ. Let Φ1,Φ2 denote the canonical angles between (R(U1),R(Ũ1)) and (R(V1),R(Ṽ1)) respectively. Then,√\n‖ sin Φ1‖2F + ‖ sin Φ2‖2F ≤\n√ ‖R‖2F + ‖S‖2F\nδ .\nRemark 19. The two conditions on δ are needed because the theorem doesn’t require Σ1,Σ2, Σ̃1, Σ̃2 to be ordered. If they were ordered, then it reduces to δ = min |σ(Σ̃1)− σ(Σ2)| > 0.\nProof. First define Q ∈ R[0,2]×[0,2],\nQ = [ 0 A A> 0 ] .\nIt can be verified that if ui ∈ R[0,1], vi ∈ R[0,1] are a left/right singular vector pair with singular value σi, then (ui, vi) ∈ R[0,2] is an eigenvector with eigenvalue σi and (ui,−vi) ∈ R[0,2] is an eigenvector with eigenvalue −σi. Writing,\nX = 1√ 2 ( U1 U1 V1 −V1 ) , Y = 1√ 2 ( U2 U2 V2 −V2 ) ,\nwe have,\nQ = [X Y ] Σ1 0 0 00 −Σ1 0 00 0 Σ2 0 0 0 0 −Σ2 [X> Y > ] .\nWe similarly define Q̃, X̃, Ỹ for Ã. Now let T = QX̃−X̃diag(Σ̃1,−Σ̃1). We will apply Theorem 17 with L1 = diag(Σ1,−Σ1), L2 = diag(Σ2,−Σ2), Z = X̃ , B = diag(Σ̃1,−Σ̃1). Then, using the conditions on δ gives us, ∥∥ sin Θ[R(X),R(X̃)]‖F ≤ ‖T‖F\nδ .\nIt is straightforward to verify that ‖T‖2F = ‖R‖2F + ‖S‖2F. To conclude the proof, first note that XX>(I[0,2] − Y Y >) = [ (U1U > 1 )(I[0,1] − Ũ1Ũ>1 ) 0\n0 (V1V > 1 )(I[0,1] − Ṽ1Ṽ >1 ) ] Now, using Theorem 16 we have ‖ sin Θ[R(X),R(X̃)]‖2F = ‖ sin Φ21‖2F + ‖ sin Φ22‖2F.\nWe can now prove Lemma 6 which follows directly from Theorem 18.\nProof of Lemma 6. Let Ũ⊥ ∈ R[0,1]×m be an orthonormal basis for the complementary subspace of R(Ũ). Then, by Corollary 15, ‖Ũ>⊥U‖2F = ‖ sin Θ[R(Ũ),R(U)]‖2F, ‖Ṽ >⊥ V ‖2F = ‖ sin Θ[R(Ṽ ),R(V )]‖2F. For R,S as defined in Theorem 18, we have. ‖R‖2F, ‖S‖2F < ‖E‖2F. The lemma follows via the sin–cos relationships for canonical angles,\nminσ(Ũ>U)2 = 1−maxσ(Ũ>⊥U)2 ≥ 1− ‖Ũ>⊥U‖2F ≥ 1− 2‖E‖2F δ2 .\nwhere δ = σm(A).\nNext we prove the pseudo-inverse theorem. Recall that for A ∈ R[0,1]×m the SVD is A = UΣV > where U ∈ R[0,1]×m, Σ ∈ Rm×m and V ∈ Rm×m where U, V have orthonormal columns. Denote its pseudo-inverse by A† = V Σ−1U>.\nProof of Lemma 7. Let A = UΣV be the SVD of A and Ã = Ũ Σ̃Ṽ be the SVD of Ã. Let P̃ = Ũ Ũ>, R = V V >, R̃ = Ṽ Ṽ >, P⊥ = I[0,1] − UU>, R̃⊥ = I[0,1] − Ṽ Ṽ > and P = UU>. We then have,\nÃ† −A† = −Ã†P̃ERA† + (Ã>Ã)†R̃E>P⊥ + R̃⊥EP (AA>)†\n‖Ã† −A†‖2 ≤ ‖Ã†‖2‖E‖2‖A†‖2 + ‖(Ã>Ã)†‖2‖E‖2 + ‖E‖2‖(AA>)†‖2 = ( ‖Ã†‖2‖A†‖2 + ‖Ã†‖22 + ‖A†‖22 ) ‖E‖2 ≤ 3 max{‖Ã‖22, ‖A‖22}‖E‖2\nThe first step is obtained by substitutine for P̃ , E,R, R̃, P⊥, R̃⊥ and P , the second step uses the triangle inequality, and the third step uses Ã>Ã = UΣ2U>, AA> = V Σ2V >.\nRemark 20. P, P̃ , R, R̃ can be shown to be the projection operators toR(A),R(Ã),R(A>) and R(Ã>). Here, R(A) = {Ax;x ∈ Rm} ⊂ R[0,1] is the range of A. R(Ã) ⊂ R[0,1], R(A>) ⊂ Rm andR(Ã>) ⊂ Rm are defined similarly. P⊥, R̃⊥ are the complementary projectors of P, R̃.\nFinally, we state an analogue of Weyl’s theorem for cmatrices which bounds the difference in the singular values in terms of the operator norm of the perturbation. While Weyl’s theorem has been studied for general operators [23], we use the form below from Townsend [6] for cmatrices.\nLemma 21 (Weyl’s Theorem for Cmatrices, [6].). Let A,E ∈ R[a,b]×[c,d] and Ã = A+ E. Let the singular values of A be σ1 ≥ σ2, . . . and those of Ã be σ̃1 ≥ σ̃2, . . . . Then,\n|σi − σ̃i| ≤ ‖E‖2 ∀i ≥ 1."
    }, {
      "heading" : "C Concentration of Kernel Density Estimation",
      "text" : "We will first define the Hölder class in high dimensions. Definition 22. Let X ⊂ Rd be a compact space. For any r = (r1, . . . , rd), ri ∈ N, let |r| = ∑ i ri and Dr = ∂ |r|\n∂x r1 1 ...x rd d\n. The Hölder classHd(β, L) is the set of functions of L2(X ) satisfying\n|Drf(x)−Drf(y)| ≤ L‖x− y‖β−|r|, (8) for all r such that |r| ≤ bβc and for all x, y ∈ X .\nThe following result establishes concentration of kernel density estimators. At a high level, we follow the standard KDE analysis techniques to decompose the L2 error into bias and variance terms and bound them separately. A similar result for 2-dimensional densities was given by Liu et al. [24]. Unlike the previous work, here we deal with the general d-dimensional case as well as explicitly delineate the dependencies of the concentration bounds on the deviation, ε.\nLemma 23. Let f ∈ Hd(β, L) be a density on [0, 1]d and assume we have N i.i.d samples {Xi}Ni=1 ∼ f . Let f̂ be the kernel density estimate obtained using a kernel with order at least β and bandwidth h = ( logN N ) 1 2β+d . Then there exist constants κ1, κ2, κ3, κ4 > 0 such that for all ε < κ4 and number of samples satisfying NlogN > κ1\nε 2+ d β we have, P ( ‖f̂ − f‖L2 > ε ) ≤ κ2 exp ( −κ3N 2β 2β+d (logN) d 2β+d ε2 ) (9)\nProof. First note that P ( ‖f̂ − f‖L2 > ε ) ≤ P ( ‖f̂ − Ef̂‖L2 + ‖Ef̂ − f‖L2 > ε ) . (10)\nUsing the Hölderian conditions and assumptions on the kernel, standard techniques for analyzing the KDE [13, 17], give us a bound on the bias, ‖Ef̂ − f‖L2 ≤ κ5hβ , where κ5 = L ∫ K(u)uβdu. When the number of samples, N , satisfies\nN\nlogN > ( 2κ′5 ε )2+ dβ = κ5\nε2+ d β\n, where κ5 ∆ = (2κ′5) 2+ dβ (11)\nwe have ‖Ef̂ −f‖L2 ≤ ε/2, and hence (10) turns into P ( ‖f̂ −f‖L2 > ε ) ≤ P ( ‖f̂ −Ef̂‖L2 > ε/2 ) .\nThe main challenge in bounding the first term is that we want the difference to hold in L2. The standard techniques that bound the pointwise variance would not be sufficient here. To overcome the limitations, we use Corollary 2.2 from Giné and Guillou [25]. Using their notation we have,\nσ2 = sup t∈[0,1]d\nVX∼f [ 1\nhd K ( X − t h )] ≤ sup\nt∈[0,1]d\n1\nh2d ∫ K2 ( x− t h ) f(x)dx\n= sup t∈[0,1]d\n1\nhd\n∫ K2(u)f(t+ uh)du ≤ ‖f‖∞‖K‖L 2\nhd\nU = sup t∈[0,1]d ∥∥∥∥ 1hdK ( X − t h )∥∥∥∥ ∞ = ‖K‖L∞ hd .\nThen, there exist constants κ2, κ3, κ′4 such that for all ε ∈ ( κ′4 σ√ n √ log Uσ , σ2 U κ ′ 4 ) we have,\nP ( ‖f̂ − Ef̂‖L2 > ε\n2\n) ≤ κ2 exp ( −κ3Nhdε2 ) .\nSubstituting for h and then combining this with (10) gives us the probability inequality of the theorem. All that is left to do is to verify the that the conditions on ε hold. The upper bound condition requires ε ≤ κ\n′ 4‖f‖∞‖K‖L2 ‖K‖L∞ ∆ = κ4. After some algebra, the lower bound on ε reduces to NlogN > κ6\nε 2+ d β .\nCombining this with the condtion (11) and taking κ1 = max(κ6, κ5) gives the theorem.\nIn order to apply the above lemma, we need P1, P21, P321 to satisfy the Hölder condition. The following lemma shows that if all Ok’s are Hölderian, so are P1, P21, P321.\nLemma 24. Assume that the observation probabilities belong to the one dimensional Hölder class; ∀` ∈ [m], O` ∈ H1(β, L). Then for some constants L1, L2, L3, P1 ∈ H1(β, L1), P21 ∈ H2(β, L2), P321 ∈ H3(β, L3).\nProof. We prove the statement for P21. The other two follow via a similar argument. Let r = (r1, r2), ri ∈ N, |r| = r1 + r2 ≤ β, and let (s, t), (s′, t′) ∈ [0, 1]d. Note that we can write,\nP21(s, t) = ∑ k∈[m] ∑ `∈[m] p(x2 = s, x1 = t, h2 = k, h1 = `) = ∑ k∈[m] ∑ `∈[m] αklOk(s)O`(t),\nwhere ∑ k,` αk` = 1. Then,\n∂|r|P21(s, t)\n∂sr1∂tr2 − ∂\n|r|P21(s ′, t′)\n∂sr1∂tr2 = ∑ k,` αk` ( ∂Ok(s) ∂sr1 ∂O`(t) ∂tr2 − ∂Ok(s ′) ∂sr1 ∂O`(t ′) ∂tr2 )\n≤ ∑ k,` αk` (∣∣∣∣∂Ok(s)∂sr1 ∣∣∣∣ ∣∣∣∣∂O`(t)∂tr2 − ∂O`(t′)∂tr2 ∣∣∣∣ +∣∣∣∣∂O`(t′)∂tr2 ∣∣∣∣ ∣∣∣∣∂Ok(s)∂sr1 − ∂O`(s′)∂sr1\n∣∣∣∣) ≤ ∑ k,` αkl ( L′L|t− t′|β−r2 + L′L|s− s′|β−r1 ) (Hölder condition)\n≤ L′L ( |t− t′|β−|r| + |s− s′|β−|r| ) (domain of s, s′ and t, t′)\n≤ L2 √ (t− t′)2 + (s− s′)2 β−|r|\nHere, the third step uses the Hölder conditions on Ok and O` and the fact that the partial fractions are bounded in a bounded domain by a constant, which we denoted L′, due to the Hölder condition. Since r1 +r2 = |r| ≤ β and r1, r2 are positive integers, we have xβ−ri ≤ xβ−r, i = 1, 2 for any x ∈ [0, 1], which implies the fourth step. The last step uses Jensen’s inequality and sets L2 ≡ L′L.\nThe corollary belows follws as a direct consequence of Lemmas 23 and 24. We have absorbed the constants L1, L2, L3 into κ1, κ2, κ3, κ4.\nCorollary 25. Assume the HMM satisfies the conditions given in Section 3. Let 1, 21, 321 ∈ (0, κ4) and η ∈ (0, 1). If the number of samples N is large enough such that the following are true,\nN\nlogN >\nκ1 2+ 1β 1 , N logN > κ1 2+ 2β 21 , N logN > κ1 2+ 3β 321 ,\nN(logN) 1 2β >\n1\n2+ 1β 1\n( 1\nκ3 log ( 3κ2 η ))1+ 12β N(logN) 2 2β > 1\n2+ 2β 21\n( 1\nκ3 log ( 3κ2 η ))1+ 22β N(logN) 3 2β > 1\n2+ 3β 321\n( 1\nκ3 log ( 3κ2 η ))1+ 32β then with at least 1 − η probability the L2 errors between P1, P21, P321 and the KDE estimates P̂1, P̂21, P̂321 satisfy,\n‖P1 − P̂1‖L2 ≤ 1, ‖P21 − P̂21‖L2 ≤ 21, ‖P321 − P̂321‖L2 ≤ 321."
    }, {
      "heading" : "D Analysis of the Spectral Algorithm",
      "text" : "Our proof is a brute force generalization of the analysis in Hsu et al. [2]. Following their template, we use establish a few technical lemmas. We mainly focus on the cases where our analysis is different.\nThroughout this section 1, 21, 321 will refer to L2 errors. Using our notation for c/q-matrices the errors can be written as,\n1 = ‖P1 − P̂1‖L2 = ‖P1 − P̂1‖F , 21 = ‖P21 − P̂21‖L2 = ‖P21 − P̂21‖F , 321 = ‖P321 − P̂321‖L2 .\nWe begin with a series of Lemmas.\nLemma 26. Let 21 ≤ εσm(P21) where ε < 11+√2 . Denote ε0 = 221\n((1−ε)σm(P21))2 < 1. Then the following hold,\n1. σm(Û>P̂21) ≥ (1− ε)σm(P21).\n2. σm(Û>P21) ≥ √ 1− ε0σm(P21).\n3. σm(Û>P21) ≥ √ 1− ε0σm(P21).\nProof. The proof follows Hsu et al. [2] after an application of Weyl’s theorem (Lemma 21) and Wedin’s sine theorem (Lemma 6) for cmatrices.\nWe define an alternative observable representation for the true HMM given by, b̃∞, b̃1 ∈ Rm and B̃ : [0, 1]→ Rm×m.\nb̃1 = Û >P1 = (Û >O)π\nb̃∞ = (P > 21Û)P1 = (Û >O)−11m\nB̃(x) = (Û>P3x1)(Û >P21) † = (Û>O)A(x)(Û>O)−1.\nAs long as Û>O is invertible, the above parameters constitute a valid observable representation. This is guaranteed if Û is sufficiently close to U . We now define the following error terms,\nδ∞ = ‖(Û>O)>(̂b∞ − b̃∞)‖∞ = ‖(Û>O)>b̂∞ − 1m‖∞ δ1 = ‖(Û>O)−1(B̂(x)− B̃(x))(Û>O)‖1 = ‖(Û>O)−1B̂(x)(Û>O)−A(x)‖1\n∆(x) = ‖(Û>O)−1(B̂(x)− B̃(x))Û>O‖1 = ‖(Û>O)−1B̂(x)−A(x)‖1\n∆ = ∫ x∈[0,1] ∆(x)dx\nThe next lemma bounds the above quantities in terms of 1, 21, 321.\nLemma 27. Assume 21 < σm(P21)/3. Then, there exists constants c1, c2, c3, c4 such that, δ∞ ≤ c1 σ1(O) (\n21 σm(P21)2 + 1 σm(P21) ) δ1 ≤ c2\n1 σm(O)\n∆(x) ≤ c3 √ m κ(O)\n( 21\nσm(P21)2 ‖P3x1‖2 + ‖P3x1 − P̂3x1‖2 σm(P21)2\n)\n∆ ≤ c4 √ m κ(O)\n( 21\nσm(P21)2 + 321 σm(P21)2\n)\nProof. We will use .,& to denote inequalities ignoring constants. First we bound δ∞ ≤ ‖(Û>O)>(̂b∞ − b̃∞)‖2 ≤ σ1(O)‖b̂∞ − b̃∞‖2. Then we note,\n‖b̂∞ − b̃∞‖2 ≤ ‖(P̂>21Û)†P̂1 − (P21Û)†P1‖2 ≤ ‖(P̂>21Û)† − (P>21Û)†‖2‖P̂1‖2 + ‖(P>21Û)†‖2‖P̂21 − P1‖2 . 21\nmin{σm(P̂>21), σm(P>21Û)}2 +\n1\nσm(P>21Û)\n. 21\nσm(P21)2 + 1 σm(P21) ,\nwhere the third and fourth steps use Lemma 26 and Lemma 7 (the pseudoinverse theorem for qmatrices). This establishes the first result. The second result is straightforward from Lemma 26.\nδ1 ≤ √ m‖(Û>O)−1‖2‖b̂1 − b̃1‖2 ≤ √ m ‖b̂1 − b̃1‖2 σm(Û>O) . √ m ‖Û>(P̂1 − P1)‖2 σm(O) .\n√ m 1 σm(O) .\nFor the third result, we first note\n∆(x) ≤ √ m‖(Û>O)−1‖2‖B̂(x)− B̃(x)‖2‖Û>O‖2 ≤ √ m σ1(O)\nσm(Û>O) ‖B̂(x)− B̃(x)‖2\n. √ m κ(O)‖B̂(x)− B̃(x)‖2\nTo bound the last term we decompose it as follows.\n‖B̂(x)− B̃(x)‖2 = ‖(Û>P3x1)(Û>P21)† − (Û>P̂3x1)(Û>P̂21)†‖2 ≤ ‖(Û>P3x1)((Û>P21)† − (Û>P̂21)†)‖2 + ‖Û>(P3x1 − P̂3x1)(Û>P̂21)†‖2 ≤ ‖P3x1‖2‖‖(Û>P21)† − (Û>P̂21)†‖2 + ‖P3x1 − P̂3x1‖2‖(Û>P̂21)†‖2\n. ‖P3x1‖2 21 σm(P21)2 + ‖P3x1 − P̂3x1‖2 σm(P21) .\nThis proves the third claim. For the last claim, we make use of the proven statements. Observe,∫ ‖P3x1‖2dx ≤ (∫ ‖P3x1‖22dx )1/2 ≤ (∫ ∫ ∫ P321(s, x, t) 2dsdtdx )1/2 = ‖P321‖L2 ,\nwhere the first step uses inclusion of the Lp norms in [0, 1]. The second step uses ‖ · ‖2 ≤ ‖ · ‖F for cmatrices. A similar argument shows ∫ x ‖P3x1 − P̂3x1‖2 ≤ 321. Combining these results gives the fourth claim.\nFinally, we need the following Lemma. The proof almost exactly replicates the proof of Lemma 12 in Hsu et al. [2], as all operations can be done with just matrices.\nLemma 28. Assume 321 ≤ σm(P21)/3. Then ∀t ≥ 0,∫ |p(x1:t)− p̂(x1:t)|dx1:t ≤ δ∞ + (1 + δ∞) ( (1 + ∆)tδ1 + (1 + ∆) t − 1 ) , (12)\nwhere the integral is over [0, 1]t.\nWe are now ready to prove Theorem 5.\nProof of Theorem 5. If 1, 21, 321 satisfy the following for appropriate choices of c5, c6, c7,\n1 ≤ c5 min(σm(P21), κ(O)√ m ) , 21 ≤ c6 σm(P21)\n2\nκ(O) , 321 ≤ c7\nσm(P21)\nσ1(O)\n1\nt √ m , (13)\nwe then have δ1 ≤ /20, δ∞ ≤ /20 and ∆ ≤ 0.4 /t. Plugging these expressions into Lemma 28 gives ∫ |p(x1:t) − p̂(x1:t)|dx1:t ≤ . When we plug the expresssions for 1, 21, 321 in (13) into Corollary 25 we get the required sample complexity."
    }, {
      "heading" : "E Addendum to Experiments",
      "text" : "Details on Synthetic Experiments: Figure 3 shows the emission probabilities used in our synthetic experiments. For the transition matrices, we sampled the entries of the matrix from a U(0, 1) distribution and then renormalised the columns to sum to 1.\nIn our implementation, we use a Gaussian kernel for the KDE which is of order β = 2. While higher order kernels can be constructed using Legendre polynomials [17], the Gaussian kernel was more robust in practice. The bandwidth for the kernel was chosen via cross validation on density estimation.\nDetails on Real Datasets: Here, we first estimate the model parameters using the training sequence. Given a test sequence x1:n, we predict xt+1 conditioned on the previous x1:t for t = 1 : n.\n1. Internet Traffic. Training sequence length: 10, 000. Test sequence length: 10. 2. Laser Generation. Training sequence length: 10, 000. Test sequence length: 100. 3. Physiological data. Training sequence length: 15, 000. Test sequence length: 100."
    } ],
    "references" : [ {
      "title" : "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition",
      "author" : [ "Lawrence R. Rabiner" ],
      "venue" : "In Proceedings of the IEEE,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1989
    }, {
      "title" : "A Spectral Algorithm for Learning Hidden Markov Models",
      "author" : [ "Daniel J. Hsu", "Sham M. Kakade", "Tong Zhang" ],
      "venue" : "In COLT,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "A Method of Moments for Mixture Models and Hidden Markov Models",
      "author" : [ "Animashree Anandkumar", "Daniel Hsu", "Sham M Kakade" ],
      "venue" : "arXiv preprint arXiv:1203.0683,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Reduced-Rank Hidden Markov Models",
      "author" : [ "Sajid M. Siddiqi", "Byron Boots", "Geoffrey J. Gordon" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Continuous analogues of matrix factorizations",
      "author" : [ "Alex Townsend", "Lloyd N Trefethen" ],
      "venue" : "In Proc. R. Soc. A,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Computing with Functions in Two Dimensions",
      "author" : [ "Alex Townsend" ],
      "venue" : "PhD thesis, University of Oxford,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "An extension of chebfun to two dimensions",
      "author" : [ "Townsend", "Alex", "Trefethen", "Lloyd N" ],
      "venue" : "SIAM J. Scientific Computing,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Predictive representations of state",
      "author" : [ "Michael L Littman", "Richard S Sutton", "Satinder P Singh" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "SERIES B,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1977
    }, {
      "title" : "Hidden Markov models and the Baum-Welch algorithm",
      "author" : [ "Lloyd R Welch" ],
      "venue" : "IEEE Information Theory Society Newsletter,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "An EM-like algorithm for semi-and nonparametric estimation in multivariate mixtures",
      "author" : [ "Tatiana Benaglia", "Didier Chauveau", "David R Hunter" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Hilbert space embeddings of hidden markov models",
      "author" : [ "Le Song", "Byron Boots", "Sajid M Siddiqi", "Geoffrey J Gordon", "Alex Smola" ],
      "venue" : "In ICML,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Nonparametric Estimation of Multi-View Latent Variable Models",
      "author" : [ "Le Song", "Animashree Anandkumar", "Bo Dai", "Bo Xie" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Observable operator models for discrete stochastic time series",
      "author" : [ "Herbert Jaeger" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Introduction to Nonparametric Estimation",
      "author" : [ "Alexandre B. Tsybakov" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Chebyshev polynomials in numerical analysis",
      "author" : [ "L. Fox", "I.B. Parker" ],
      "venue" : "Oxford U.P. cop.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1968
    }, {
      "title" : "Approximation Theory and Approximation Practice",
      "author" : [ "Lloyd N. Trefethen" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Estimation of integral functionals of a density",
      "author" : [ "Lucien Birgé", "Pascal Massart" ],
      "venue" : "Ann. of Stat.,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1995
    }, {
      "title" : "Nonparametric Von Mises Estimators for Entropies, Divergences and Mutual Informations",
      "author" : [ "Kirthevasan Kandasamy", "Akshay Krishnamurthy", "Barnabás Póczos", "Larry Wasserman", "James Robins" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Weyl’s theorem for operator matrices",
      "author" : [ "Woo Young Lee" ],
      "venue" : "Integral Equations and Operator Theory,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1998
    }, {
      "title" : "Forest Density Estimation",
      "author" : [ "Han Liu", "Min Xu", "Haijie Gu", "Anupam Gupta", "John D. Lafferty", "Larry A. Wasserman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Rates of strong uniform consistency for multivariate kernel density estimators",
      "author" : [ "Evarist Giné", "Armelle Guillou" ],
      "venue" : "In Annales de l’IHP Probabilités et statistiques,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2002
    }, {
      "title" : "Matrix Perturbation Theory",
      "author" : [ "G.W. Stewart", "Ji-guang Sun" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1990
    }, {
      "title" : "Wide area traffic: the failure of Poisson modeling",
      "author" : [ "Vern Paxson", "Sally Floyd" ],
      "venue" : "IEEE/ACM Transactions on Networking,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1995
    }, {
      "title" : "Dimensions and entropies of chaotic intensity pulsations in a single-mode far-infrared NH 3 laser",
      "author" : [ "U Hübner", "NB Abraham", "CO Weiss" ],
      "venue" : "Physical Review A,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1989
    }, {
      "title" : "Chebfun to three dimensions",
      "author" : [ "B. Hashemi", "L.N. Trefethen" ],
      "venue" : "In preparation,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Predictive State Representations: A New Theory for Modeling Dynamical Systems",
      "author" : [ "Satinder Singh", "Michael R. James", "Matthew R. Rudary" ],
      "venue" : "In UAI,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Hidden Markov models (HMMs) [1] are one of the most popular statistical models for analyzing time series data in various application domains such as speech recognition, medicine, and meteorology.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2–4].",
      "startOffset" : 178,
      "endOffset" : 183
    }, {
      "referenceID" : 2,
      "context" : "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2–4].",
      "startOffset" : 178,
      "endOffset" : 183
    }, {
      "referenceID" : 3,
      "context" : "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2–4].",
      "startOffset" : 178,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "In the case of discrete HMMs [2], these moments correspond exactly to the joint probabilities of the observations in the sequence.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "Our methods leverage some recent advances in continuous linear algebra [5, 6] which views two-dimensional functions as continuous analogues of matrices.",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "Our methods leverage some recent advances in continuous linear algebra [5, 6] which views two-dimensional functions as continuous analogues of matrices.",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "Chebyshev polynomial approximations enable efficient computation of algebraic operations on these continuous objects [7, 8].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "While we focus on HMMs in this exposition, we believe that the ideas presented in this paper can be easily generalised to estimating other latent variable models and predictive state representations [9] with nonparametric observations using approaches developed by Anandkumar et al.",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 2,
      "context" : "[3].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "Related Work: Parametric HMMs are usually estimated using maximum likelihood principle via EM techniques [10] such as the Baum-Welch procedure [11].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "Related Work: Parametric HMMs are usually estimated using maximum likelihood principle via EM techniques [10] such as the Baum-Welch procedure [11].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "[2] who showed that discrete HMMs can be learned efficiently, under certain conditions.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] show that the same algorithm works under slightly more general assumptions.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] proposed a spectral algorithm for estimating more general latent variable models with parametric observations via a moment matching technique.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "A commonly used heuristic is the nonparametric EM [12] which lacks theoretical underpinnings.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "[4] proposed a heuristic based on kernel smoothing, with no theoretical justification, to modify the discrete algorithm for continuous observations.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[14, 15] developed an RKHS-based procedure for estimating the Hilbert space embedding of an HMM.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "[14, 15] developed an RKHS-based procedure for estimating the Hilbert space embedding of an HMM.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : "Both sections are based on [5, 6].",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Both sections are based on [5, 6].",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Without loss of generality, we take2 X = [0, 1].",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "It is well known [2, 16] that the joint probability density of the sequence x1:t can be computed via p(x1:t) = 1mA(xt:1)π.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "It is well known [2, 16] that the joint probability density of the sequence x1:t can be computed via p(x1:t) = 1mA(xt:1)π.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "Observable Representation: The observable representation is a description of an HMM in terms of quantities that depend on the observations [16].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "We will find it useful to view both P21, P3x1 ∈ R[0,1]×[0,1] as cmatrices.",
      "startOffset" : 49,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "We will find it useful to view both P21, P3x1 ∈ R[0,1]×[0,1] as cmatrices.",
      "startOffset" : 55,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "We will also need an additional qmatrix U ∈ R[0,1]×m such that U>O ∈ Rm×m is invertible.",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "Given one such U , the observable representation of an HMM is described by the parameters b1, b∞ ∈ R and B : [0, 1]→ Rm×m, b1 = U P1, b∞ = (P > 21U) P1, B(x) = (U P3x1)(U P21) † (1) As before, for a sequence, xt:1 = {xt, .",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "B(x) = (U>O)A(x)(U>O)−1 ∀x ∈ [0, 1].",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "[2].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "While there are several techniques [17], we use kernel density estimation (KDE) since it is easy to analyse and works well in practice.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "Here K : [0, 1] → R is a symmetric function called a smoothing kernel and satisfies (at the very least) ∫ 1 0 K(s)ds = 1, ∫ 1 0 sK(s)ds = 0.",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "Let Û ∈ R[0,1]×m be the first m left singular vectors of P̂21.",
      "startOffset" : 9,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2], the SVD, pseudoinverses and multiplications are with q/c-matrices.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "Chebyshev polynomials is a family of orthogonal polynomials on compact intervals, known to be an excellent approximator of one-dimensional functions [18, 19].",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 16,
      "context" : "Chebyshev polynomials is a family of orthogonal polynomials on compact intervals, known to be an excellent approximator of one-dimensional functions [18, 19].",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "A recent line of work [5, 8] has extended the Chebyshev technology to two dimensional functions enabling the mentioned operations and factorisations such as QR, LU and SVD [6, Sections 4.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "A recent line of work [5, 8] has extended the Chebyshev technology to two dimensional functions enabling the mentioned operations and factorisations such as QR, LU and SVD [6, Sections 4.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "Following [2, 4, 14] we assume i.",
      "startOffset" : 10,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Following [2, 4, 14] we assume i.",
      "startOffset" : 10,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "Following [2, 4, 14] we assume i.",
      "startOffset" : 10,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "T ∈ Rm×m and O ∈ R[0,1]×m are of rank m.",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "for all α ≤ bβc, j ∈ [m], s, t ∈ [0, 1] ∣∣∣∣dαOj(s) dsα − dOj(t) dtα ∣∣∣∣ ≤ L|s− t|β−|α|.",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "[4] show that the discrete spectral algorithm works under a slightly more general setting.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2], whose sample complexity bound4 is N & m κ(O) 2",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "While we do not have any lower bounds, given the current understanding of the difficulty of various nonparametric tasks [20–22], we think our bound might be unimprovable.",
      "startOffset" : 120,
      "endOffset" : 127
    }, {
      "referenceID" : 18,
      "context" : "While we do not have any lower bounds, given the current understanding of the difficulty of various nonparametric tasks [20–22], we think our bound might be unimprovable.",
      "startOffset" : 120,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "This is due to the fact that we want the KDE to concentrate around its expectation in L over [0, 1], instead of just point-wise.",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "Weyl’s theorem has been studied for general operators [23] and cmatrices [6].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Weyl’s theorem has been studied for general operators [23] and cmatrices [6].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Let A, Ã, E ∈ R[0,1]×[0,1] where Ã = A+ E and rank(A) = m.",
      "startOffset" : 15,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "Let A, Ã, E ∈ R[0,1]×[0,1] where Ã = A+ E and rank(A) = m.",
      "startOffset" : 21,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "Such kernels can be constructed using Legendre polynomials [17].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "[2] provide a more refined bound but we use this form to simplify the comparison.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 20,
      "context" : "This slow convergence is also observed in similar concentration bounds for the KDE [24, 25].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "This slow convergence is also observed in similar concentration bounds for the KDE [24, 25].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : "A note on the Proofs: For Lemmas 6, 7 we follow the matrix proof in Stewart and Sun [26] and derive several intermediate results for c/q-matrices in the process.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "The main challenge with the KDE concentration result is that we want an L bound – so usual techniques (such as McDiarmid’s [13, 17]) do not apply.",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "We use a technical lemma from Giné and Guillou [25] which allows us to bound the L error in terms of the VC characteristics of the class of functions induced by an i.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "[2].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "NP-HMM-BIN: A naive baseline where we bin the space into n intervals and use the discrete spectral algorithm [2] with n states.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "NP-HMM-EM: The Nonparametric EM heuristic of [12].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "NP-HMM-HSE: The Hilbert space embedding method of [14].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "We could not include the method of [4] in our comparisons since their code was not available and their method isn’t straightforward to implement.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "Real Datasets: We compare all the above methods (except NP-HMM-EM which was too slow) on prediction error on 3 real datasets: internet traffic [27], laser generation [28] and sleep data [29].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 24,
      "context" : "Real Datasets: We compare all the above methods (except NP-HMM-EM which was too slow) on prediction error on 3 real datasets: internet traffic [27], laser generation [28] and sleep data [29].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "That said, some recent advances in this direction are promising [8, 30].",
      "startOffset" : 64,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "That said, some recent advances in this direction are promising [8, 30].",
      "startOffset" : 64,
      "endOffset" : 71
    }, {
      "referenceID" : 26,
      "context" : "Recent advances in spectral methods for estimating parametric predictive state representations [31], mixture models [3] and other latent variable models [32] can be generalised to the nonparamatric setting using our ideas.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Recent advances in spectral methods for estimating parametric predictive state representations [31], mixture models [3] and other latent variable models [32] can be generalised to the nonparamatric setting using our ideas.",
      "startOffset" : 116,
      "endOffset" : 119
    } ],
    "year" : 2016,
    "abstractText" : "Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an m-state hidden Markov model (HMM) with only smoothness assumptions, such as Hölderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as continuous matrices. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient.",
    "creator" : "LaTeX with hyperref package"
  }
}