{
  "name" : "1608.05639.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "minh.haquang@iit.it" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n05 63\n9v 1\n[ cs\n.L G\n] 1\n9 A"
    }, {
      "heading" : "1. Introduction",
      "text" : "The current work is concerned with the construction of random feature maps for operatorvalued kernels and their applications in vector-valued learning. Much work has been done in machine learning recently on these kernels and their associated RKHS of vectorvalued functions, both theoretically and practically, see e.g. (Micchelli and Pontil, 2005; Carmeli et al., 2006; Reisert and Burkhardt, 2007; Caponnetto et al., 2008; Brouard et al., 2011; Dinuzzo et al., 2011; Kadri et al., 2011; Minh and Sindhwani, 2011; Zhang et al., 2012; Sindhwani et al., 2013). While rich in theory and potentially powerful in applications, one of the main challenges in applying operator-valued kernels is that they are computationally intensive on large datasets. In the scalar setting, one of the most powerful approaches for scaling up kernel methods is Random Fourier Features (Rahimi and Recht, 2007), which applies Bochner’s Theorem and the Inverse Fourier Transform to build random features that approximate a given shift-invariant kernel. The approach in (Rahimi and Recht, 2007) has been improved both in terms of computational speed (Le et al., 2013) and rates of convergence (Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015).\nc©201x Hà Quang Minh.\nOur contributions. The following are the contributions of this work.\n1. Firstly, we construct random feature maps for operator-valued shift-invariant kernels using the operator-valued version of Bochner’s Theorem. The key differences between the operator-valued and scalar settings are the following. The first key difference is that, in the scalar setting, a positive definite function k, with normalization, is the Fourier transform of a probability measure ρ, which is uniquely determined as the inverse Fourier transform of k. In the operator-valued setting, k is the Fourier transform of a unique finite positive operator-valued measure µ. However, the probability measure ρ, which is necessary for constructing the random feature maps, must be explicitly constructed, that is it is not automatically determined by k. In this work, we present a general formula for computing a probability measure ρ given a kernel k. The second key difference is that, in the operator-valued setting, the probability measure ρ is generally non-unique, being a factor of µ. As a consequence, we show that in general, there are (potentially infinitely) many random feature maps, which may be either unbounded or bounded. However, under appropriate assumptions, we show that there always exist bounded feature maps. This is true for many of the commonly encountered kernels, including separable kernels and curl-free and divergence-free kernels.\n2. Secondly, for the bounded feature maps, we show that the associated approximate kernel converges uniformly to the exact kernel in Hilbert-Schmidt norm on any compact subset in Euclidean space.\n3. Thirdly, when restricting to the scalar setting, our convergence holds for differentiable kernels, which is an improvement over the hypothesis of (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015; Brault et al., 2016), which all require the kernels to be twice-differentiable.\n4. Fourthly, we show how operator-valued feature maps and their approximations can be used directly in a general learning formulation in RKHS.\nRelated work. The work most closely related to our present work is (Brault et al., 2016). While the formal constructions of the Fourier feature maps in (Brault et al., 2016) and our work are similar, there are several crucial differences. The first and most important difference is that in (Brault et al., 2016) there is no general mechanism for computing a probability measure ρ, which is required for the construction of the Fourier feature maps. As such, the results presented in (Brault et al., 2016) are only for three specific kernels, namely separable kernels, curl-free and div-free kernels, not for a general kernel as in our setting. Moreover, for the curl-free and div-free kernels, (Brault et al., 2016) presented unbounded feature maps, whereas we show that, apart from unbounded feature maps, there are generally infinitely many bounded feature maps associated with these kernels. Secondly, more general than the matrix-valued kernel, i.e finite-dimensional, setting in (Brault et al., 2016), we work in the operator-valued kernel setting, with RKHS of functions with values in a Hilbert space. In this setting, the convergence in the Hilbert-Schmidt norm that we present is strictly stronger than the convergence in spectral norm given in (Brault et al., 2016). At the same time, our convergence requires weaker assumptions than\nthose in (Brault et al., 2016) and previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015).\nOrganization. We first briefly review random Fourier features and operator-valued kernels in Section 2. Feature maps for operator-valued kernels are described in Section 2.1. The core of the paper is Section 3, which describes the construction of random feature maps using operator-valued Bochner’s Theorem, the computation of the required probability measure, along with the uniform convergence of the corresponding approximate kernels. Section 4 employs feature maps and their approximations in a general vector-valued learning formulation, with the accompanying experiments in Section 5. All mathematical proofs are given in Appendix A."
    }, {
      "heading" : "2. Background",
      "text" : "Throughout the paper, we work with shift-invariant positive definite kernels K on Rn×Rn, so that K(x, t) = k(x− t)∀x, t ∈ Rn for some function k : Rn → R, which is then said to be a positive definite function on Rn.\nRandom Fourier features for scalar-valued kernels (Rahimi and Recht, 2007). Bochner’s Theorem in the scalar setting, see e.g. (Reed and Simon, 1975), states that a complex-valued, continuous function k on Rn is positive definite if and only if it is the Fourier transform of a finite, positive measure µ on Rn, that is\nk(x) = µ̂(x) =\n∫\nRn e−i〈ω,x〉dµ(ω). (1)\nFor our purposes, we consider exclusively the real-valued setting for k. Since µ is a finite positive measure, without loss of generality, we assume that µ is a probability measure, so that k(x) = Eµ[e −i〈ω,x〉]. The measure µ is uniquely determined via µ̂ = k. For the Gaussian function k(x) = e− ||x||2 σ2 , we have µ(ω) = (σ √ π)n\n(2π)n e −σ\n2||ω||2 4 ∼ N ( 0, 2σ2 ) . Consider now the\nkernel K(x, t) = k(x− t) = ∫ Rn\ne−i〈ω,x−t〉dµ(ω). Using the symmetry of K and the relation 1 2(e ix + e−ix) = cos(x), we obtain\nK(x, t) = 1\n2\n∫\nRn [ei〈ω,x−t〉 + e−i〈ω,x−t〉]dµ(ω) =\n∫\nRn cos(〈ω, x− t〉)dµ(ω). (2)\nLet {ωj}Dj=1 be points in Rn, independently sampled according to the measure µ. Then we have an empirical approximation K̂D of K and the associated feature map Φ̂D : R\nn → R2D, as follows\nK̂D(x, t) = 1\nD\nD∑\nj=1\ncos(〈ωj , x− t〉) = 1\nD\nD∑\nj=1\n[cos(〈ωj , x〉) cos(〈ωj , t〉) + sin(〈ωj , x〉) sin(〈ωj , t〉)\n= 〈Φ̂D(x), Φ̂D(t)〉, where Φ̂D(x) = (cos(〈ωj , x〉), sin(〈ωj , x〉))Dj=1 ∈ R2D. (3)\nThe current work generalizes the feature map Φ̂D above to the case K is an operator-valued kernel and the corresponding µ is a positive operator-valued measure.\nVector-valued RKHS. Let us now briefly recall operator-valued kernels and their corresponding RKHS of vector-valued functions, for more detail see e.g. (Carmeli et al.,\n2006; Micchelli and Pontil, 2005; Caponnetto et al., 2008; Minh and Sindhwani, 2011). Let X be a nonempty set, W a real, separable Hilbert space with inner product 〈·, ·〉W , L(W) the Banach space of bounded linear operators on W. Let WX denote the vector space of all functions f : X → W. A function K : X × X → L(W) is said to be an operator-valued positive definite kernel if for each pair (x, t) ∈ X ×X , K(x, t)∗ = K(t, x), and for every set of points {xi}Ni=1 in X and {wi}Ni=1 in W, N ∈ N, ∑N i,j=1〈wi,K(xi, xj)wj〉W ≥ 0. For x ∈ X and w ∈ W, form a function Kxw = K(., x)w ∈ WX by\n(Kxw)(t) = K(t, x)w ∀t ∈ X . (4)\nConsider the set H0 = span{Kxw|x ∈ X , w ∈ W} ⊂ WX . For f = ∑N\ni=1Kxiwi, g =∑N i=1Kziyi ∈ H0, we define the inner product 〈f, g〉HK = ∑N i,j=1〈wi,K(xi, zj)yj〉W , which makes H0 a pre-Hilbert space. Completing H0 by adding the limits of all Cauchy sequences gives the Hilbert space HK . This is the reproducing kernel Hilbert space (RKHS) of Wvalued functions on X . The reproducing property is\n〈f(x), y〉W = 〈f,Kxy〉HK for all f ∈ HK . (5)"
    }, {
      "heading" : "2.1 Operator-Valued Feature Maps for Operator-Valued Kernels",
      "text" : "Feature maps for operator-valued kernels were first considered in (Caponnetto et al., 2008). Let FK be a separable Hilbert space and L(W,FK) be the Banach space of all bounded linear operators mapping from W to FK . A feature map for K with corresponding feature space FK is a mapping\nΦK : X → L(W,FK), such that K(x, t) = ΦK(x)∗ΦK(t) ∀(x, t) ∈ X × X . (6)\nThe operator-valued map ΦK is generally nonlinear as a function on X . For each x ∈ X , ΦK(x) ∈ L(W,FK) and\n〈w,K(x, t)w〉W = 〈w,ΦK(x)∗ΦK(t)〉W = 〈ΦK(x)w,ΦK(t)w〉FK . (7)\nIn the following, for brevity, we also refer to the pair (ΦK ,FK) as a feature map for K. Existence of operator-valued feature maps and the canonical feature map. Let K be any operator-valued positive definite kernel on X × X , we now show that then there always exists at least one feature map, as follows. For each x ∈ X , consider the linear operator Kx : W → HK defined by Kxw(t) = K(t, x)w, x, t ∈ X , as above. Then\n||Kxw||2HK = 〈K(x, x)w,w〉W ≤ ||K(x, x)|| ||w|| 2 W , (8)\nwhich implies that Kx is a bounded operator, with\n||Kx : W → HK || ≤ √ ||K(x, x)||, (9)\nLet K∗x : HK → W be the adjoint operator for Kx. The reproducing property states that ∀w ∈ W,\n〈f(x), w〉W = 〈f,Kxw〉HK = 〈K∗xf,w〉W ⇒ K∗xf = f(x). (10)\nFor any u, v ∈ W, we have\n〈u,K(x, t)v〉W = 〈u,Ktv(x)〉W = 〈u,K∗xKtv〉W = 〈Kxu,Ktv〉HK ⇒ K(x, t) = K∗xKt, (11)\nfrom which it follows that\nΦK : X → L(W,HK), ΦK(x) = Kx ∈ L(W,HK) (12)\nis a feature map for K with feature space HK , which exists for any positive definite kernel K. Following the terminology in the scalar setting (Minh et al., 2006), we also call it the canonical feature map for K.\nRemark 1 In (Caponnetto et al., 2008), it is assumed that the kernel has the representation K(x, t) = ΦK(x)\n∗ΦK(t). However, as we have just shown, for any positive definite kernel K, there is always at least one such representation, given by the canonical feature map above.\nSimilar to the scalar setting (Minh et al., 2006), feature maps are generally non-unique, as we show below. However, they are all essentially equivalent, similar to the scalar case, as shown by the following.\nLemma 2 Let (ΦK ,FK) be any feature map for K. Then ∀f ∈ HK , there exists an h ∈ FK such that\nf(x) = K∗xf = ΦK(x) ∗h, ∀x ∈ X . (13)\nFurthermore, ||f ||HK = ||h||FK ."
    }, {
      "heading" : "3. Random Operator-Valued Feature Maps",
      "text" : "We now present the generalization of the random Fourier feature map from the scalar setting to the operator-valued setting. We begin by reviewing Bochner’s Theorem in the operatorvalued setting in Section 3.1, which immediately leads to the formal construction of the Fourier feature maps in Section 3.2. As we stated, in the operator-valued setting, we need to explicitly construct the required probability measure. This is done individually for some specific kernels in Section 3.3 and for a general kernel in Section 3.4."
    }, {
      "heading" : "3.1 Operator-Valued Bochner Theorem",
      "text" : "The operator-valued version of Bochner’s Theorem that we present here is from (Neeb, 1998), see also (Falb, 1969; Carmeli et al., 2010). Throughout this section, let H be a separable Hilbert space. Let L(H) denote the Banach space of bounded linear operators on H, Sym(H) ⊂ L(H) denote the subspace of bounded, self-adjoint operators on H, and Sym+(H) ⊂ Sym(H) denote the set of self-adjoint, bounded, positive operators on H. An operator A ∈ L(H) is said to be trace class, denoted by A ∈ Tr(H), if∑∞\nk=1〈ek, (A∗A)1/2ek〉 < ∞ for any orthonormal basis {ek}∞k=1 in H. If A ∈ Tr(H), then the trace of A is tr(A) = ∑∞ k=1〈ek, Aek〉, which is independent of the orthonormal basis.\nPositive operator-valued measures. Let (X ,Σ) be a measurable space, where X is a non-empty set and Σ is a σ-algebra of subsets of X . A Sym+(H)-valued measure µ is a\ncountably additive1 function µ : Σ → Sym+(H), with µ(∅) = 0, so that for any sequence of pairwise disjoint subsets {Aj}∞j=1 in Σ,\nµ(∪∞j=1Aj) = ∞∑\nj=1\nµ(Aj), which converges in the operator norm on L(H). (14)\nTo state Bochner’s Theorem for operator-valued measures, we need the notions of finite Sym+(H)-valued measure and ultraweak continuity. Let X = Rn (a locally compact space in general). A finite Sym+(H)-valued Radon measure is a Sym+(H)-valued measure such that for any operator A ∈ Sym+(H) ∩Tr(H), the scalar measure\nµA : Σ → R+, µA(B) = tr(Aµ(B)), B ∈ Σ, (15)\nis a finite positive Radon measure on Rn. A function k : Rn → L(H) is said to be ultraweakly continuous if for each operator A ∈ Tr(H), the following scalar function is continuous\nkA : R n → R, kA(x) = tr(Ak(x)). (16)\nThe following is then the generalization of Bochner’s Theorem to the vector-valued setting.\nTheorem 3 (Operator-valued Bochner Theorem (Neeb, 1998)) An ultraweakly continuous function k : Rn → L(H) is positive definite if and only if there exists a finite Sym+(H)-valued measure µ on Rn such that\nk(x) = µ̂(x) =\n∫\nRn exp(i〈ω, x〉)dµ(ω) =\n∫\nRn exp(−i〈ω, x〉)dµ(ω). (17)\nThe Radon measure µ is uniquely determined by µ̂ = K.\nGeneral case. The above version of Bochner’s Theorem holds in a much more general setting, where Rn is replaced by a locally compact abelian group G. For the general version, we refer to (Neeb, 1998).\nDetermining µ from k. In order to compute feature maps using Bochner’s Theorem, we need to compute µ from the given operator-valued function k. Suppose that the density function µ(ω) of µ with respect to the Lebesgue measure on Rn exists. Let {ej}∞j=1 be any orthonormal basis for H. For any vector a = ∑∞j=1 ajej ∈ H, we have\nµ(ω)a = ∞∑\nj=1\n〈ej , µ(ω)a〉ej = ∞∑\nj,l=1\nal〈ej , µ(ω)el〉ej .\nThus µ(ω) is completely determined by the infinite matrix of inner products (〈ej , µ(ω)el〉)∞j,l=1, which can be computed from k via the inverse Fourier transform F−1 as follows.\n1. Falb (Falb, 1969) used weakly countably additive vector measures, which are in fact countably additive (Diestel, 1984).\nProposition 4 Assume that 〈ej , k(x)el〉 ∈ L1(Rn) ∀j, l ∈ N. Then the density function µ(ω) of µ with respect to the Lebesgue measure on Rn exists and is given by\n〈ej, µ(ω)el〉 = F−1[〈ej , k(x)el〉]. (18)\nThe positive definite function k gives rise to the shift-invariant positive definite kernel\nK(x, t) = k(x− t) = ∫\nRn exp(−i〈ω, x− t〉)dµ(ω). (19)\nSimilar to the scalar case, using the property K(x, t) = K(t, x)∗ and the symmetry of µ, we obtain\nK(x, t) =\n∫\nRn cos(〈ω, x− t〉)dµ(ω). (20)\nIn order to generalize the random Fourier feature approach to the operator-valued kernel K(x, t) we need to construct a probability measure ρ on Rn such that K(x, t) is the expectation of an operator-valued random variable with respect to ρ. Equivalently, we need to factorize the density µ(ω) as\nµ(ω) = µ̃(ω)ρ(ω), (21)\nwhere µ̃(ω) is a finite Sym+(H)-valued function on Rn and ρ(ω) is the density function of the probability measure ρ.\nRemark 5 Throughout the rest of the paper, we assume that k satisfies the assumptions of Proposition 4. We then identify the measures µ and ρ by their density functions µ(ω) and ρ(ω), respectively, with respect to the Lebesgue measure.\nKey differences between the scalar and operator-valued settings. Before proceeding with the probability measure and feature map construction, we point out two key differences between the scalar and operator-valued settings.\n1. In the scalar setting, with normalization, µ is a probability measure uniquely determined via µ̂ = k. In the operator-valued setting, the operator-valued measure µ is also uniquely determined by k, as stated in Proposition 4. However, the probability measure ρ in Eq. (21) needs to be explicitly constructed, that is it is not automatically determined by k. We present a general formula for computing ρ in Section 3.4.\n2. The factorization stated in Eq. (21) is generally non-unique. As we show below, in general, there are many (in fact, potentially infinitely many) pairs (µ̃, ρ) such that Eq. (21) holds. Thus there are generally (infinitely) many operator-valued feature maps corresponding to the operator-valued version of Bochner’s Theorem. We illustrate this property via examples in Sections 3.2 and 3.4 below."
    }, {
      "heading" : "3.2 Formal Construction of Approximate Fourier Feature Maps",
      "text" : "Assuming for the moment that we have a pair (µ̃, ρ) satisfying the factorization in Eq. (21), then Eq. (20) takes the form\nK(x, t) =\n∫\nRn cos(〈ω, x− t〉)µ̃(ω)dρ(ω) = Eρ[cos(〈ω, x− t〉)µ̃(ω)]. (22)\nLet {ωj}Dj=1, D ∈ N, be D points in Rn randomly sampled independently from ρ. Then K(x, t) can be approximated by by the empirical sum\nK̂D(x, t) = k̂D(x− t) = 1\nD\nD∑\nl=1\ncos(〈ωl, x− t〉)µ̃(ωl)\n= 1\nD\nD∑\nl=1\n[cos(〈ωl, x〉) cos(〈ωl, t)]µ̃(ωl) + 1\nD\nD∑\nl=1\nsin(〈ωl, x〉) sin(〈ωl, t〉)]µ̃(ωl). (23)\nLet F be a separable Hilbert space and ψ : Rn → L(H,F) be such that\nµ̃(ω) = ψ(ω)∗ψ(ω), ψ(ω) : H → F (24)\nSuch a pair (ψ,F) always exists, with one example being F = H and ψ(ω) = √ µ̃(ω).\nRemark 6 As we demonstrate via the examples below, the decomposition µ̃(ω) = ψ(ω)∗ψ(ω) is also generally non-unique, which is another reason for the non-uniqueness of the approximate feature maps.\nOperator-valued Fourier feature map. The decompositions for K̂D in Eqs. (23) and (24) immediately give us the following approximate feature map\nΦ̂D(x) = 1√ D   cos(〈ω1, x〉)ψ(ω1) sin(〈ω1, x〉)ψ(ω1)\n· · · cos(〈ωD, x〉)ψ(ωD) sin(〈ωD, x〉)ψ(ωD)\n  : H → F2D. (25)\nwith\nKD(x, t) = [Φ̂D(x)] ∗[Φ̂D(t)]. (26)\nSpecial cases. For H = R, we have µ̃ = 1 (assuming normalization) and ρ = µ, and we thus recover the Fourier features in the scalar setting. For H = Rd, for some d ∈ N, we obtain the feature map in (Brault et al., 2016)."
    }, {
      "heading" : "3.3 Probability Measure and Feature Map Construction in Some Special Cases",
      "text" : "We first consider several examples of operator-valued kernels arising from scalar-valued kernels. For these examples, both the Sym+(H)-valued measure µ and the probability measure ρ can be derived from the corresponding probability measure for the scalar kernels.\nThese examples have also been considered by (Brault et al., 2016), however we treat them in greater depth here, particularly the curl-free and div-free kernels (see detail below). One important aspect that we note is that the approach for computing the probability measure ρ in this section is specific for each kernel and does not generalize to a general kernel. We return to these examples in the general setting of Section 3.4, where we present a general formula for computing ρ for a general kernel k.\nExample 1 (Separable kernels)\nConsider the simplest case, where the operator-valued positive definite function k has the form\nk(x) = g(x)A, (27)\nwhere A ∈ Sym+(H) and g : Rn → R is a scalar-valued positive definite function. Let ρ0 be the probability measure on Rn such that g(x) = Eρ0 [e −i〈ω,x〉]. It follows immediately that\nk(x) =\n∫\nRn e−i〈ω,x〉dµ(ω) where µ(ω) = Aρ0(ω). (28)\nThus we can set\nµ̃(ω) = A, ρ = ρ0. (29)\nFor the operator ψ(ω) in Eq. (24), we can set either\nψ(ω) = √ A, (30)\nor, if A is a symmetric positive definite matrix, we can also compute ψ via the Cholesky decomposition of A by setting\nψ(ω) = U, where A = UTU, (31)\nwith U being an upper triangular matrix. Thus in this case, with the probability measure ρ = ρ0, there are at least two choices for the feature map Φ̂D, each resulting from one choice of ψ(ω) as discussed above. In practice, a particular ψ should be chosen based on its computational complexity, which in turn depends on the structure of A itself.\nExample 2 (Curl-free and divergence-free kernels)\nConsider next the matrix-valued curl-free and divergence kernels in (Fuselier, 2006). In (Brault et al., 2016), the authors present what we call the unbounded feature maps below for these kernels, without, however, the analytical expression for the feature map of the div-free kernel. We now present the analytical expressions for the feature maps for both these kernels. More importantly, we show that, apart from the unbounded feature maps, there are generally infinitely many bounded feature maps associated with these kernels.\nLet φ be a scalar-valued twice-differentiable positive definite function on Rn. Let ∇ denote the n× 1 gradient operator and ∆ = ∇T∇ denote the Laplacian operator. Define\nkdiv = (−∆In +∇∇T )φ, kcurl = −∇∇Tφ. (32)\nThen kdiv and kcurl are n × n matrices, whose columns are divergence-free and curl-free functions, respectively. The functions kcurl and kdiv give rise to the corresponding positive definite kernels\nKcurl(x, t) = kcurl(x− t), and Kdiv(x, t) = kdiv(x− t).\nFor the Gaussian case φ(x) = exp(− ||x||2 σ2 ), the functions kcurl and kdiv are given by\nkcurl(x) = 2 σ2 exp(−||x|| 2 σ2 )[In − 2 σ2 xxT ]. (33)\nkdiv(x) = 2 σ2 exp(−||x|| 2 σ2 )[((n − 1)− 2 σ2 ||x||2)In + 2 σ2 xxT ]. (34)\nLemma 7 Let ρ0 be the probability measure on R n such that φ(x) = Eρ0 [e −i〈ω,x〉] = ρ̂0(x). Then, under the condition ∫ Rn ||ω||2dρ0(ω) < ∞, we have\nkcurl(x) =\n∫\nRn e−i〈ω,x〉ωωTρ0(ω)dω =\n∫\nRn e−i〈ω,x〉(µcurl)(ω)dω. (35)\nkdiv(x) =\n∫\nRn e−i〈ω,x〉[||ω||2In − ωωT ]ρ0(ω)dω =\n∫\nRn e−i〈ω,x〉(µdiv)(ω)dω, (36)\nwhere µcurl(ω) = ωω Tρ0(ω) and µdiv(ω) = [||ω||2In − ωωT ]ρ0(ω). The condition ∫ Rn\n||ω||2dρ0(ω) < ∞ in Lemma 7 guarantees that φ is twice-differentiable, which is the underlying assumption for curl-free and divergence-free kernels.\nUnbounded feature maps. Consider first the curl-free kernel. From the expression µcurl(ω) = ωω Tρ0(ω), we immediately see that for the factorization in Eq. (21), we can set\nµcurl(ω) = µ̃(ω)ρ(ω), with µ̃(ω) = ωω T , ρ = ρ0.\nFor the Gaussian case, ρ0(ω) = (σ\n√ π)n\n(2π)n e −σ\n2||ω||2\n4 ∼ N (0, 2 σ2 In). In Eq. (24), we can set\nψ(ω) = ωT , so that Φ̂D(x) is a matrix of size 2D × n. (37)\nWe can also set\nψ(ω) = √ ωωT = ωωT\n||ω|| , so that Φ̂D(x) is a matrix of size 2Dn× n. (38)\nClearly the choice for ψ(ω) in Eq. (37) is preferable computationally to that in Eq. (38). One thing that can be observed immediately is that both µcurl and ψ are unbounded functions of ω, which complicates the convergence analysis of the corresponding kernel approximation (see Section 3.5 for further discussion).\nBounded feature maps. The unbounded feature maps above correspond to one particular choice of the probability measure ρ, namely ρ = ρ0. However, this is not the only valid choice for ρ. We now exhibit another choice for ρ that results in a bounded feature\nmap, whose convergence behavior is much simpler to analyze. Consider the Gaussian case, with ρ0 as given above. Clearly, we can choose for another factorization of µcurl the factors\nµ̃(ω) = ωωT e− σ2||ω||2 8 2n/2, ρ(ω) = 1 2n/2 (σ √ π)n (2π)n e− σ2||ω||2 8 ∼ N (0, 4 σ2 In). (39)\nThen µ̃(ω) is a bounded function of ω, with the corresponding bounded map\nψ(ω) = ωT e− σ2||ω||2 16 2n/4. (40)\nFor the divergence-free kernel, we have √ ||ω||2In − ωωT = ||ω||In − ωω T\n||ω|| , giving the corresponding maps\nψ(ω) = (||ω||In − ωωT\n||ω|| ) (unbounded feature map), (41)\nψ(ω) = (||ω||In − ωωT\n||ω|| )e −σ\n2||ω||2\n16 2n/4 (bounded feature map). (42)\nSince there are infinitely many ways to split the Gaussian function e− σ2||ω||2\n4 into a product of two Gaussian functions, it follows that there are infinitely many bounded Fourier feature maps associated with both the curl-free and div-free kernels induced by the Gaussian kernel. We show below that, under appropriate conditions on k, bounded feature maps always exist."
    }, {
      "heading" : "3.4 First Main Result: Probability Measure Construction in the General Case",
      "text" : "For the separable and curl-free and div-free kernels, we obtain a probability measure ρ directly from the corresponding scalar-valued kernels. We now show how to construct ρ given a general k, under appropriate assumptions on k. Furthermore, we show that the corresponding feature map is bounded, in the sense that µ̃(ω) is a bounded function of ω (see the precise statement in Corollary 10).\nProposition 8 Let K : Rn×Rn → L(H) be an ultraweakly continuous shift-invariant positive definite kernel. Let µ be the unique finite Sym+(H)-valued measure satisfying Eq. (19). Then ∀a ∈ H, a 6= 0, the scalar-valued kernel defined by Ka(x, t) = 〈a,K(x, t)a〉 is positive definite. Furthermore, there exists a unique finite positive Borel measure µa on R\nn such that Ka is the Fourier transform of µa, that is\nKa(x, t) =\n∫\nRn exp(−i〈ω, x− t〉)dµa(ω). (43)\nThe measure µa is given by\nµa(ω) = 〈a, µ(ω)a〉, ω ∈ Rn. (44)\nLet {ej}∞j=1 be any orthonormal basis for H. By Proposition 8, ∀j ∈ N, the scalar-valued kernel\nKjj(x, t) = Kej (x, t) = 〈ej ,K(x, t)ej〉 (45)\nis positive definite and is the Fourier transform of the finite positive Borel measure\nµjj(ω) = 〈ej, µ(ω)ej〉, ω ∈ Rn. (46)\nThe measures µjj, j ∈ N, which depend on the choice of orthonormal basis {ej}j∈N, collectively give rise to the following measure, which is independent of {ej}j∈N.\nTheorem 9 (Probability Measure Construction) Assume that the positive definite function k in Bochner’s Theorem satisfies: (i) k(x) ∈ Tr(H) ∀x ∈ Rn, and (ii) ∫ Rn\n|tr[k(x)]|dx < ∞. Then its corresponding finite Sym+(H)-valued measure µ satisfies\nµ(ω) ∈ Tr(H) ∀ω ∈ Rn, tr[µ(ω)] ≤ 1 (2π)n\n∫\nRn |tr[k(x)]|dx. (47)\nThe following is a finite positive Borel measure on Rn\nµtr(ω) = tr(µ(ω)) =\n∞∑\nj=1\nµjj(ω) = 1\n(2π)n\n∫\nRn exp(i〈ω, x〉)tr[k(x)]dx. (48)\nThe normalized measure µtr(ω)tr[k(0)] is a probability measure on R n.\nSpecial case. For H = R, we obtain\nµtr(ω) = 1\n(2π)n\n∫\nRn exp(i〈ω, x〉)k(x)dx = µ(ω), (49)\nso that the scalar-setting is a special case of Theorem 9, as expected.\nCorollary 10 Under the hypothesis of Theorem 9, in Eq. (21) we can set\nρ(ω) = µtr(ω)\ntr[k(0)] , µ̃(ω) =\n{ tr[k(0)] µ(ω)µtr(ω) , µtr(ω) > 0\n0, µtr(ω) = 0. (50)\nThe function µ̃ in Eq. (50) satisfies µ̃(ω) ∈ Sym+(H) and has bounded trace, i.e.\n||µ̃(ω)||tr = tr[µ̃(ω)] ≤ tr[k(0)] ∀ω ∈ Rn. (51)\nLet us now illustrate Theorem 9 and 10 on the separable kernels and curl-free and divfree kernels. We note that for the separable kernels, we obtain the same probability measure ρ as in Section 3.3. However, for the curl-free and div-free kernels, we obtain a different probability measure compared to Section 3.3, which illustrates the non-uniqueness of ρ.\nExample 3 (Separable kernels)\nFor the separable kernels of the form k(x) = g(x)A, with A ∈ Sym+(H) ∩ Tr(H) and g ∈ L1(Rn), we have\nµtr(ω) = tr(A) 1\n(2π)n\n∫\nRn exp(i〈ω, x〉)g(x)dx = tr(A)ρ0(ω). (52)\nSince tr[k(0)] = tr(A), we recover ρ(ω) = ρ0(ω). Here µ̃(ω) = A and ||µ̃(ω)||tr = tr(A) < ∞.\nExample 4 (Curl-free and div-free kernels)\nFor the curl-free kernel, we have µ(ω) = ωωTρ0(ω) and thus\nµtr(ω) = ||ω||2ρ0(ω), (53)\nwhich is a finite measure by the assumption ∫ Rn\n||ω||2dρ0(ω) < ∞. For the Gaussian case, since tr[k(0)] = 2nσ2 , the corresponding probability measure is\nρ(ω) = σ2\n2n ||ω||2ρ0(ω). (54)\nSimilarly, for the div-free kernel, we have µ(ω) = [||ω||2In − ωωT ]ρ0(ω) and thus\nµtr(ω) = (n− 1)||ω||2ρ0(ω). (55)\nFor the Gaussian case, since tr[k(0)] = 2n(n−1)σ2 , the corresponding probability measure is\nρ(ω) = σ2\n2n ||ω||2ρ0(ω). (56)\nClearly, for both the curl-free and div-free kernels, the probability measure ρ is non-unique. We can, for example, obtain ρ by normalizing the measure\n(1 + ||ω||2)ρ0(ω) (57)\nand the corresponding µ̃(ω) still has bounded trace.\nExample 5 (Sum of kernels)\nConsider now the probability measure and feature maps corresponding to the sum of two kernels, which is readily generalizable to any finite sum of kernels. Let k1, k2 be two positive definite functions satisfying the assumptions of Theorem 9, which are the Fourier transforms of two Sym+(H)-valued measures µ1 and µ2, respectively. Then their sum k = k1 + k2 is clearly the Fourier transform of µ = µ1 + µ2. Then the probability measure ρ and the function µ̃ corresponding to k is given by\nρ(ω) = µtr(ω)\ntr[k(0)] =\nµ1,tr(ω) + µ2,tr(ω)\ntr[k1(0)] + tr[k2(0)] , (58)\nµ̃(ω) =\n{ (tr[k1(0)] + tr[k2(0)])\nµ1(ω)+µ2(ω) µ1,tr(ω)+µ2,tr(ω) , µ1,tr(ω) + µ2,tr(ω) > 0,\n0, µ1,tr(ω) + µ2,tr(ω) = 0. (59)\nWe the obtain the Fourier feature map for the kernel K(x, t) = k(x− t) using Eqs. (24) and (25).\nWe contrast this approach with the following concatenation approach. Let (ΦKj ,FKj ) be the feature maps associated with Kj(x, t) = kj(x − t), j = 1, 2. Let FK be the direct\nHilbert sum of FK1 and FK2 . Consider the map ΦK : X → L(H,FK), FK = FK1 ⊕ FK2 , defined by\nΦK(x) = ( ΦK1(x) ΦK2(x) ) , ΦK(x)w = ( ΦK1(x)w ΦK2(x)w ) , w ∈ H, (60)\nwhich essentially stacks to the two maps ΦK1 , ΦK2 on top of each other. Then clearly\nΦK(x) ∗ΦK(t) = ΦK1(x) ∗ΦK1(t) + ΦK2(x) ∗ΦK2(t) = K1(x, t) +K2(x, t) = K(x, t),\nso that (ΦK ,FK) is a feature map representation for K = K1 + K2. If dim(FKj ) < ∞, then we have dim(FK) = dim(FK1) + dim(FK2). Thus, from a practical viewpoint, this approach can be computationally expensive, since the dimension of the feature map for the sum kernel can be very large, especially if we have a sum of many kernels."
    }, {
      "heading" : "3.5 Second Main Result: Uniform Convergence Analysis",
      "text" : "Having computed the approximate version K̂D for K, we need to show that this approximation is consistent, that is K̂D approaches K in some sense, as D → ∞. Since K(x, t) = k(x− t) it suffices for us to consider the convergence of k̂D towards k.\nRecall the Hilbert space of Hilbert-Schmidt operators HS(H), that is of bounded operators on H satisfying\n||A||2HS = tr(A∗A) = ∞∑\nj=1\n||Aej ||2 < ∞,\nfor any orthonormal basis {ej}∞j=1 in H. Here || ||HS denotes the Hilbert-Schmidt norm, which is induced by the Hilbert-Schmidt inner product\n〈A,B〉HS = tr(A∗B) = ∞∑\nj=1\n〈Aej , Bej〉, A,B ∈ HS(H).\nIn the following, we assume that k(x) ∈ HS(H). Since we have shown that, under appropriate assumptions, bounded feature maps always exist, we focus exclusively on analyzing the convergence associated with them. Specifically, we show that for bounded feature maps, for any compact set Ω ⊂ Rn, we have\nsup x∈Ω\n||k̂D(x)− k(x)||HS → 0 as D → ∞, (61)\nwith high probability. This generalizes the convergence of supx∈Ω |k̂D(x) − k(x)| in the scalar setting. If dim(H) < ∞, then this is convergence in the Frobenius norm || ||F .\nTheorem 11 (Pointwise Convergence) Assume that ||µ̃(ω)||HS ≤ M almost surely and that σ2(µ̃(ω)) = Eρ[||µ̃(ω)||2HS] < ∞. Then for any fixed x ∈ Rn,\nP[||k̂D(x)− k(x)||HS ≥ ǫ] ≤ 2 exp ( − Dǫ 2M log [ 1 +\nMǫ\nσ2(µ̃(ω))\n]) ∀ǫ > 0. (62)\nAssumption 1. Our uniform convergence analysis requires the following condition\nm1 =\n∫\nRn ||ω|| ||µ(ω)||HSd(ω) =\n∫\nRn ||ω|| ||µ̃(ω)||HSdρ(ω) < ∞. (63)\nIn the scalar setting, we have µ̃(ω) = 1, and Assumption 1 becomes ∫ Rn\n||ω||dρ(ω) < ∞, so that k is differentiable. This is weaker than the assumptions in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015), which all require that∫ Rn ||ω||2dρ(ω) < ∞, that is k is twice-differentiable.\nTheorem 12 (Uniform Convergence) Let Ω ⊂ Rn be compact with diameter diam(Ω). Assume that ||µ̃(ω)||HS ≤ M almost surely and that σ2(µ̃(ω)) = Eρ[||µ̃(ω)||2HS] < ∞. Then for any ǫ > 0,\nP ( sup x∈Ω ||k̂D(x)− k(x)||HS ≥ ǫ ) ≤a(n) ( m1diam(Ω) ǫ ) n n+1\n× exp ( − Dǫ 4(n+ 1)M log [ 1 +\nMǫ\n2σ2(µ̃(ω))\n]) , (64)\nwhere a(n) = 2 3n+1 n+1 ( n 1 n+1 + n− n n+1 ) .\nExample 6 (Separable kernels)\nFor the separable kernel, Assumption 1 becomes ||A||HS < ∞, in which case we have uniform convergence with M = ||A||HS and σ2(µ̃) = ||A||2HS.\nExample 7 (Curl-free and div-free kernels)\nFor the curl-free kernel, we have ||µ(ω)||HS = ||ω||2ρ0(ω), thus Assumption 1 becomes ∫\nRn ||ω||3dρ0(ω) < ∞, (65)\nwhich, being stronger than the assumption ∫ Rn\n||ω||2dρ0(ω) < ∞ in Section 3.4, guarantees that a bounded feature map can be constructed, with\n||µ̃(ω)||HS ≤ ||µ̃(ω)||tr ≤ tr[k(0)] and σ2[µ̃(ω)] ≤ (tr[k(0)])2. (66)\nThe case of the div-free kernel is entirely similar. Comparison with the convergence analysis in (Brault et al., 2016). In (Brault et al., 2016), the authors carried out convergence analysis in the spectral norm for matrix-valued kernels. Our results are for the more general setting of operator-valued kernels, which induce RKHS of functions with values in a Hilbert space. In this setting, convergence in the HilbertSchmidt norm is strictly stronger than convergence in the spectral norm. Furthermore, as with previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015), the convergence in (Brault et al., 2016) also requires twice-differentiable kernels, whereas we require the weaker assumption of C1-differentiability."
    }, {
      "heading" : "4. Vector-Valued Learning with Operator-Valued Feature Maps",
      "text" : "Having discussed operator-valued feature maps and their random approximations, we now show how they can be applied in the context of learning in RKHS of vector-valued functions. Let W,Y be two Hilbert spaces, C : W → Y a bounded operator, K : Rn ×Rn → L(W) be a positive definite definite kernel with the corresponding RKHS HK of W-valued functions, and V be a convex loss function. Consider the following general learning problem from (Minh et al., 2016)\nfz,γ = argminf∈HK 1\nl\nl∑\ni=1\nV (yi, Cf(xi)) + γA||f ||2HK + γI〈f ,M f〉Wu+l . (67)\nHere z = (x,y) = {(xi, yi)}li=1∪{xi}u+li=l+1, u, l ∈ N, with u, l denoting unlabeled and labeled data points, respectively, f = (f(xj)) u+l j=1 ∈ Wu+l, M : Wu+l → Wu+l a positive operator, and γA > 0, γI > 0. In (Minh et al., 2016), it is shown that the optimization problem (67) represents a general learning formulation in RKHS that encompasses supervised and semi-supervised learning via manifold regularization, multi-view learning, and multi-class classification. For the case V is the least square and SVM loss, both binary and multiclass, the solution of (67) has been obtained in dual form, that is in terms of kernel matrices.\nWe now present the solution of problem (67) in terms of feature map representation, that is in primal form. Let (ΦK ,FK) be any feature map for K. On the set x, we define the following operator\nΦK(x) : Wu+l → FK , ΦK(x)w = u+l∑\nj=1\nΦK(xj)wj . (68)\nWe also view ΦK(x) as a (potentially infinite) matrix\nΦK(x) = [ΦK(x1), . . . ,ΦK(xu+l)] : Wu+l → FK , (69)\nwith the jth column being ΦK(xj). The following is the corresponding version of the Representer Theorem in (Minh et al., 2016) in feature map representation.\nTheorem 13 (Representer Theorem) The optimization problem (67) has a unique solution fz,γ(x) = ∑u+l i=1 K(x, xi)ai for ai ∈ W, i = 1, . . . , u + l. In terms of feature maps, fz,γ(x) = ΦK(x) ∗h, where\nh =\nu+l∑\ni=1\nΦK(xi)ai = ΦK(x)a ∈ FK , a = (aj)u+lj=1 ∈ Wu+l. (70)\nIn the case V is the least square loss, the optimization problem (67) has a closed-form solution, which is expressed explicitly in terms of the operator-valued feature map ΦK . In the following, let I(u+l)×l = [Il, 0l×u] T and Ju+ll = I(u+l)×lI T (u+l)×l, which is a (u+ l)×(u+ l) diagonal matrix, with the first l entries on the main diagonal equal to 1 and the rest being zero. The following is the corresponding version of Theorem 4 in (Minh et al., 2016) in feature map representation.\nTheorem 14 (Vector-Valued Least Square Algorithm) In the case V is the least square loss, that is V (y, f(x)) = ||y − Cf(x)||2Y , the solution of the optimization problem (67) is fz,γ(x) = ΦK(x) ∗h, with h ∈ FK given by\nh = ( ΦK(x)[(J u+l l ⊗ C∗C) + lγIM ]ΦK(x)∗ + lγAIFK )−1 ΦK(x)(I(u+l)×l ⊗ C∗)y. (71)\nComparison with the dual formulation. In Theorem 4 in (Minh et al., 2016), the solution of the least square problem above is equivalently given by fz,γ(x) = ∑u+l j=1K(x, xj)aj , where a = (aj) u+l j=1 ∈ Wu+l is given by\n(C∗CJW ,u+ll K[x] + lγIMK[x] + lγAIWu+l)a = C ∗y, (72)\nwhere C∗ = I(u+l)×l ⊗C∗ and K[x] is the (u+ l)× (u+ l) operator-valued matrix with the (i, j) entry being K(xi, xj).\nFor concreteness, consider the case W = Rd for some d ∈ N. Then Eq. (72) is a system of linear equations of size d(u+ l)× d(u+ l), which depends only on the dimension d of the output space and the number of data points (u+ l).\nIf the feature space FK is infinite-dimensional, then Eq. (71) is an infinite-dimensional system of linear equations.\nApproximate feature map vector-valued least square regression. Consider now the approximate finite-dimensional feature map Φ̂D(x) : R\nd → R2Dr, for some r, 1 ≤ r ≤ d. Here r depends on the decomposition µ̃ = ψ(ω)∗ψ(ω) in Eq. (24), with r = d corresponding to e.g. ψ(ω) = √ µ̃(ω). Then instead of the operator ΦK(x) : R\nd(u+l) → FK , we consider its approximation\nΦ̂D(x) = [Φ̂D(x1), . . . , Φ̂D(xu+l)] : R d(u+l) → R2Dr, (73)\nwhich is a matrix of size 2Dr × d(u + l). This gives rise to the following system of linear equations, which approximates Eq. (71)\nĥD = ( Φ̂D(x)[(J u+l l ⊗ C∗C) + lγIM ]Φ̂D(x)∗ + lγAI2Dk )−1 Φ̂D(x)(I(u+l)×l ⊗ C∗)y. (74)\nEq. (74) is a system of linear equations of size 2Dr × 2Dr, which is independent of the number of data points (u+ l). This system is more efficient to solve than Eq. (72) when\n2Dr < d(u+ l). (75)"
    }, {
      "heading" : "5. Numerical Experiments",
      "text" : "We report in this section several experiments to illustrate the numerical properties of the feature maps just constructed. Since the properties of the feature maps for separable kernels follow directly from those of the corresponding scalar kernels, we focus here on the curl-free and div-free kernels.\nApproximate kernel computation. We first checked the quality of the approximation of the kernel values using matrix-valued Fourier feature maps. Using the standard normal distribution, we generated a set of 100 points in R3, which are normalized to\nlie in the cube [−1, 1]3. On this set, we first computed the curl-free and div-free kernels induced by the Gaussian kernel, based on Eq. (33), with σ = 1. We computed the feature maps given by Eq. (25). For the curl-free kernel, in the unbounded map, ψ(ω) is given by Eq. (37), with ρ(ω) ∼ N (0, (2/σ2)I3), and in the bounded map, ψ(ω) is given by Eq. (40), with ρ(ω) ∼ N (0, (4/σ2)I3). Similarly, for the div-free kernel, the ψ(ω) maps, bounded and unbounded, are given by Eq. (41). We then computed the relative error ||K̂D(x, y)−K(x, y)||F /||K(x, y)||F , with F denoting the Frobenius norm, using D = 100, 500, 1000. The results are reported on Table 1.\nVector field reconstruction by approximate feature maps. Next, we tested the reconstruction of the following curl-free vector field in R2, F (x, y) = sin(4πx) sin2(2πy)i + sin2(2πx) sin(4πy)j on the rectangle [−1,−0.4765] × [−1,−0.4765], sampled on a regular grid consisting of 1600 points. The reconstruction is done using 5% of the points on the grid as training data. We first performed the reconstruction with exact kernel least square regression according to Eq. (72), using the curl-free kernel induced by the Gaussian kernel, based on Eq. (33), with σ = 0.2. With the same kernel, we then performed the approximate feature map least square regression according to Eq. (74 (W = Y = R2, C = I2, γI = 0, γA = 10−9), with D = 50, 100. The results are reported on Table 2.\nDiscussion of numerical results. As we can see from Tables 1 and 2, the matrixvalued Fourier feature maps can be used both for approximating the kernel values as well as directly in a learning algorithm, with increasing accuracy as the feature dimension increases. Furthermore, while all feature maps associated with a given kernel are essentially equivalent as in Lemma 2, we observe that numerically, on average, the bounded feature maps tend to outperform the unbounded maps."
    }, {
      "heading" : "6. Conclusion and Future Work",
      "text" : "We have presented a framework for constructing random operator-valued feature maps for operator-valued kernels, using the operator-valued version of Bochner’s Theorem. We have shown that, due to the non-uniqueness of the probability measure in this setting, in general many feature maps can be computed, which can be unbounded or bounded. Under certain conditions, which are satisfied for many common kernels such as curl-free and div-free kernels, bounded feature maps can always be computed. We then showed the uniform convergence, with the bounded maps, of the approximate kernel in the HilbertSchmidt norm, strengthening previous results in the scalar setting. Finally, we showed how a general vector-valued learning formulation can be expressed in terms of feature maps and demonstrated it experimentally. An extensive empirical evaluation of the proposed formulation is left to future work."
    }, {
      "heading" : "Appendix A. Proofs of Main Mathematical Results",
      "text" : "Proof of Lemma 2 For a function f ∈ HK of the form f = ∑N j=1Kxjaj, aj ∈ W, we have\nf(x) =\nN∑\nj=1\nK(x, xj)aj =\nN∑\nj=1\nΦK(x) ∗ΦK(xj)aj = ΦK(x) ∗\n  N∑\nj=1\nΦK(xj)aj\n  ,\n= ΦK(x) ∗h,\nwhere\nh = N∑\nj=1\nΦK(xj)aj ∈ FK .\nFor the norm, we have\n||f ||2HK = N∑\ni,j=1\n〈ai,K(xi, xj)aj〉W = N∑\ni,j=1\n〈ai,ΦK(xi)∗ΦK(xj)aj〉W\n= N∑\ni,j=1\n〈ΦK(xi)ai,ΦK(xj)aj〉FK = || N∑\ni=1\nΦK(xi)ai||2FK = ||h|| 2 FK .\nBy letting N → ∞ in the Hilbert space completion for HK , it follows that every f ∈ HK has the form\nf(x) = ΦK(x) ∗h, h ∈ FK ,\nand ||f ||HK = ||h||FK .\nThis completes the proof of the lemma.\nProof of Proposition 4 For each pair j, l ∈ N, we have by Bochner’s Theorem\n〈ej , k(x)el〉 = ∫\nRn exp(−i〈ω, x〉)〈ej , dµ(ω)el〉.\nThus the proposition follows immediately from the Fourier Inversion Theorem.\nProof of Proposition 8 Let ΦK : R n → FK be a feature map for K, then K(x, t) = ΦK(x) ∗ΦK(t). Then for any set of points {xj}Nj=1 and coefficients {bj}Nj=1, we have\nN∑\nj,l=1\nbjblKa(xj, xl) =\nN∑\nj,l=1\nbjbl〈a,ΦK(xj)∗ΦK(xl)a〉 = N∑\nj,l=1\nbjbl〈ΦK(xj)a,ΦK(xl)a〉FK\n=\nN∑\nj,l=1\n〈bjΦK(xj)a, blΦK(xl)a〉FK = ∥∥∥∥∥∥ N∑\nj=1\nbjΦK(xj)a ∥∥∥∥∥∥ 2\nFK\n≥ 0.\nThis shows that the scalar-valued kernel Ka is positive definite. Thus by the scalar-valued Bochner Theorem, there exists a unique finite positive Borel measure µa on R n such that\nKa(x, t) =\n∫\nRn exp(−i〈ω, x− t〉)dµa(ω).\nFrom the formulas K(x, t) = ∫ Rn\nexp(−i〈ω, x − t〉)dµ(ω) and Ka(x, t) = 〈a,K(x, t)a〉, we obtain µa(ω) = 〈a, µ(ω)a〉 as we claimed.\nProof of Theorem 9 By Proposition 4 and taking into account the fact that µ(ω) is a self-adjoint positive operator on H, we have\ntr[µ(ω)] = ∞∑\nj=1\n〈ej , µ(ω)ej〉 = ∣∣∣∣∣∣ ∞∑\nj=1\n〈ej , µ(ω)ej〉 ∣∣∣∣∣∣\n= 1\n(2π)n ∣∣∣∣∣∣ ∞∑\nj=1\n∫\nRn exp(i〈ω, x〉)〈ej , k(x)ej〉dx ∣∣∣∣∣∣\n= 1\n(2π)n ∣∣∣∣∣∣ ∫ Rn exp(i〈ω, x〉) ∞∑\nj=1\n〈ej , k(x)ej〉dx ∣∣∣∣∣∣\n= 1\n(2π)n\n∣∣∣∣ ∫\nRn exp(i〈ω, x〉tr[k(x)]dx\n∣∣∣∣ ≤ 1\n(2π)n\n∫\nRn |tr[k(x)]|dx < ∞.\nThis shows that µtr(ω) = tr[µ(ω)] = ∑∞\nj=1 µjj(ω) ∈ Tr(H). The positivity of µtr(ω) follows from the positivity of all the µjj’s. Furthermore, we have\n∫\nRn dµtr(ω) =\n∫\nRn d[\n∞∑\nj=1\nµjj(ω)] =\n∞∑\nj=1\n〈ej , k(0)ej〉 = tr[k(0)].\nWe note that we must have tr[k(0)] > 0, since tr[k(0)] = 0 ⇐⇒ k(0) = 0 ⇐⇒ K(x, x) = 0 ∀x ∈ Rn ⇐⇒ K(x, t) = 0 ∀(x, t) ∈ X × X . It follows that the normalized measure\nµtr(ω)\ntr[k(0)]\nis a probability measure on Rn. Since the trace operation is independent of the choice of orthonormal basis {ej}∞j=1, it follows that both the normalized and un-normalized measures are independent of the choice of {ej}∞j=1. This completes the proof.\nProof of Lemma 7 We make use of the following property of the Fourier transform (see e.g (Jones, 2001)). Assume that f and ||x||f are both integrable on Rn, then the Fourier transform f̂ is differentiable and\n∂f̂\n∂ωj (ω) = −îxjf(ω), 1 ≤ j ≤ n.\nAssume further that ||x||2f is integrable on Rn, then this rule can be applied twice to give\n∂2f̂\n∂ωj∂ωk (ω) =\n∂\n∂ωj\n( ∂f̂\n∂ωk\n) = − ∂\n∂ωj [îxkf ] = −x̂jxkf.\nFor the curl-free kernel, we have φ(x) = ρ̂0(x) and consequently, under the assumption that ∫ Rn ||ω||2ρ0(ω) < ∞, we have\n[kcurl(x)]jk = − ∂2φ ∂xj∂xk (x) = − ∂ 2ρ̂0 ∂xj∂xk (x) = ω̂jωkρ0(x), 1 ≤ j, k ≤ n.\nIn other words,\n[kcurl(x)]jk =\n∫\nRn e−i〈ω,x〉ωjωkρ0(ω)dω.\nIt thus follows that\nkcurl(x) =\n∫\nRn e−i〈ω,x〉ωωTρ0(ω)dω =\n∫\nRn e−i〈ω,x〉µ(ω)dω,\nwhere µ(ω) = ωωTρ0(ω). The proof for kdiv is entirely similar.\nTo prove Theorems 11 and 12 , we need the following concentration result for Hilbert space-valued random variables.\nLemma 15 ((Smale and Zhou, 2007)) Let H be a Hilbert space with norm || || and {ξj}Dj=1, D ∈ N, be independent random variables with values in H. Suppose that for each j, ||ξj || ≤ M < ∞ almost surely. Let σ2D = ∑D j=1 E(||ξj ||2). Then\nP   ∥∥∥∥∥∥ 1 D D∑\nj=1\n[ξj − E(ξj)] ∥∥∥∥∥∥ ≥ ǫ   ≤ 2 exp ( − Dǫ 2M log [ 1 + DMǫ σ2D ]) ∀ǫ > 0. (76)\nProof of Theorem 11 For each x ∈ Rn fixed, consider the random variable ξ(x, , .) : (Rn, ρ) → Sym(H) defined by\nξ(x, ω) = cos(〈ω, x〉)µ̃(ω).\nWe then have\nk̂D(x) = 1\nD\nD∑\nj=1\ncos(〈ωj , x〉)µ̃(ωj) = 1\nD\nD∑\nj=1\nξ(x, ωj),\nk(x) =\n∫\nRn cos(〈ω, x〉)µ̃(ω)dρ(ω) = Eρ[ξ(x, ω)].\nUnder the assumption that ||µ̃(ω)||HS ≤ M almost surely, we also have ||ξ(x, ω)||HS ≤ M almost surely. Its variance satisfies\nσ2(ξ(x, ω)) = Eρ||ξ(x, ω)||2HS ≤ E||µ̃(ω)||2HS = σ2(µ̃(ω)).\nIt follows from Lemma 15 that for each fixed x ∈ Rn, we have\nP[||k̂D(x)− k(x)||HS ≥ ǫ] = P   ∥∥∥∥∥∥ 1 D D∑\nj=1\nξ(x, ωj)− Eρ[ξ(x, ω)] ∥∥∥∥∥∥ HS ≥ ǫ  \n≤ 2 exp ( − Dǫ 2M log [ 1 +\nMǫ\nσ2(ξ(x, ω))\n])\n≤ 2 exp ( − Dǫ 2M log [ 1 +\nMǫ\nσ2(µ̃(ω))\n]) .\nThis completes the proof of the theorem.\nTo prove Theorem 12, we first prove the following preliminary results.\nLemma 16 Assume that ∫ Rn\n||ω|| ||µ̃(ω)||HSdρ(ω) < ∞. Then the function k : Rn → Sym(H), with the latter endowed with the Hilbert-Schmidt norm, is Lipschitz, with\n||k(x) − k(y)||HS ≤ ||x− y|| ∫\nRn ||ω|| ||µ̃(ω)||HSdρ(ω). (77)\nProof of Lemma 16 Using the fact that the cosine function is Lipschitz with constant 1, that is | cos(x)− cos(y)| ≤ |x− y| for all x, y ∈ R, we have\n||k(x) − k(y)||HS = ∥∥∥∥ ∫\nRn [cos(〈ω, x〉) − cos(〈ω, y〉)]µ̃(ω)dρ(ω) ∥∥∥∥ HS\n≤ ∫\nRn | cos(〈ω, x〉) − cos(〈ω, y〉)| ||µ̃(ω)||HSdρ(ω)\n≤ ∫\nRn |〈ω, x〉 − 〈ω, y〉| ||µ̃(ω)||HSdρ(ω)\n≤ ||x− y|| ∫\nRn ||ω|| ||µ̃(ω)||HSdρ(ω).\nThis completes the proof.\nLemma 17 The function k̂D(x) = 1 D ∑D j=1 cos(ωj, x〉)µ̃(ωj) : Rn → Sym(H), with the latter endowed with the Hilbert-Schmidt norm, is Lipschitz, with\n||k̂D(x)− k̂D(y)||HS ≤ ||x− y|| 1\nD\nD∑\nj=1\n||ωj || ||µ̃(ωj)||HS. (78)\nProof of Lemma 17 Similar to the proof of Lemma 16, we utilize the fact that | cos(x)− cos(y)| ≤ |x− y| for all x, y ∈ R to arrive at\n||k̂D(x)− k̂D(y)||HS = 1\nD ∥∥∥∥∥∥ D∑\nj=1\n[cos(〈ωj , x〉)− cos(〈ωj , y〉)]µ̃(ωj) ∥∥∥∥∥∥ HS\n≤ ||x− y|| 1 D\nD∑\nj=1\n||ωj|| ||µ̃(ωj)||HS.\nThis completes the proof.\nCorollary 18 Let f(x) = k̂D(x) − k(x) : Rn → Sym(H), with the latter endowed with the Hilbert-Schmidt norm, then f is Lipschitz, with\n||f(x)− f(y)|| ≤ ||x− y||\n  ∫\nRn ||ω|| ||µ̃(ω)||HSdρ(ω) +\n1\nD\nD∑\nj=1\n||ωj|| ||µ̃(ωj)||HS   . (79)\nThe Lipschitz constant Lf of f satisfies\nP(Lf ≥ ǫ) ≤ 2m1 ǫ , (80)\nwhere m1 = ∫ Rn ||ω|| ||µ̃(ω)||HSdρ(ω).\nProof of Corollary 18 By combing the results of Lemmas 16 and 17, we have\n||f(x)− f(y)||HS = ||(k̂D(x)− k(x)) − (k̂D(y)− k(y))||HS ≤ ||k̂D(x)− k̂D(y)||HS + ||k(x) − k(y)||HS\n≤ ||x− y||\n  ∫\nRn ||ω|| ||µ̃(ω)||HSdρ(ω) +\n1\nD\nD∑\nj=1\n||ωj || ||µ̃(ωj)||HS\n \nas we claimed. Thus f is Lipschitz, with the Lipschitz constant Lf satisfying\nLf ≤ ∫\nRn ||ω|| ||µ̃(ω)||HSdρ(ω) +\n1\nD\nD∑\nj=1\n||ωj || ||µ̃(ωj)||HS,\nwith expectation\nE(Lf ) ≤ 2 ∫\nRn ||ω|| ||µ̃(ω)||HSdρ(ω) = 2m1.\nBy Markov’s inequality, we have for any ǫ > 0,\nP(Lf ≥ ǫ) ≤ E(Lf ) ǫ ≤ 2m1 ǫ .\nThis completes the proof.\nProof of Theorem 12 For each r > 0 fixed, let N = N (Ω, r) be the covering number for Ω, that is the minimum number of balls Ωj, 1 ≤ j ≤ N , of radius r covering Ω. By Proposition 5 in (Cucker and Smale, 2002), the covering number N (Ω, r) is bounded above by the expression\nN = N (Ω, r) ≤ ( 2diam(Ω)\nr\n)n . (81)\nConsider the function f : Rn → Sym(H) defined by\nf(x) = k̂D(x)− k(x).\nOn the ball Ωj, we have\nP( sup x∈Ωj ||k̂D(x)− k(x)||HS ≥ ǫ) = P( sup x∈Ωj ||f(x)||HS ≥ ǫ).\nBy Corollary 18, f is a Lipschitz function with Lipschitz constant Lf > 0, with Sym(H) being endowed with the Hilbert-Schmidt norm. Let xj be the center of the jth ball Ωj . For each ǫ > 0, for any x ∈ Ωj, we have\n||f(xj)− f(x)||HS ≤ Lf ||xj − x|| ≤ rLf < ǫ\n2 when Lf <\nǫ\n2r .\nSince\n||f(x)||HS ≤ ||f(x)− f(xj)||HS + ||f(xj)||HS,\nwe have\nsup x∈Ωj\n||f(x)||HS < ǫ if Lf < ǫ\n2r and ||f(xj)||HS <\nǫ 2 .\nThus over the union of balls Ωj, 1 ≤ j ≤ N , we have\nsup x∈∪Nj=1Ωj\n||f(x)||HS < ǫ if Lf < ǫ\n2r and ||f(xj)||HS <\nǫ 2 , 1 ≤ j ≤ N .\nThus\nP   sup\nx∈∪Nj=1Ωj ||f(x)||HS < ǫ\n  = P ( Lf < ǫ\n2r and ||f(xj)||HS <\nǫ 2 , 1 ≤ j ≤ N\n) .\nWe now recall the following properties on an arbitrary probability space (Σ,P,F). For any events A,B, let A denote the complement of A in F , then we have A ∩B = A∪B, so that\nP(A ∩B) = P(A ∪B) ≤ P(A) + P(B), (82) P(A ∩B) = 1− P(A ∩B) = 1− P(A ∪B) ≥ 1− P(A)− P(B). (83)\nApplying property (83) with A = {Lf < ǫ2r} and B = {||f(xj)||HS < ǫ2 , 1 ≤ j ≤ N}, we obtain\nP   sup\nx∈∪Nj=1Ωj ||f(x)||HS < ǫ\n  ≥ 1− P ( Lf ≥ ǫ\n2r\n) − P ( {||f(xj)||HS < ǫ\n2 , 1 ≤ j ≤ N}\n) .\nApplying property (82) recursively to the set {||f(xj)||HS < ǫ2 , 1 ≤ j ≤ N}, we obtain\nP ( {||f(xj)||HS < ǫ\n2 , 1 ≤ j ≤ N}\n) ≤ N∑\nj=1\nP({||f(xj)||HS < ǫ\n2 }) =\nN∑\nj=1\nP({||f(xj)||HS ≥ ǫ\n2 }).\nCombining the last two expressions, we have\nP   sup\nx∈∪Nj=1Ωj ||f(x)||HS < ǫ\n  ≥ 1− P ( Lf ≥ ǫ\n2r\n) − N∑\nj=1\nP({||f(xj)||HS ≥ ǫ\n2 }).\nEquivalently,\nP   sup\nx∈∪N j=1 Ωj\n||f(x)||HS ≥ ǫ   ≤ P ( Lf ≥ ǫ\n2r\n) + N∑\nj=1\nP({||f(xj)||HS ≥ ǫ\n2 }).\nBy Corollary 18, we have\nP ( Lf ≥ ǫ\n2r\n) ≤ 4m1r\nǫ .\nBy Theorem 11, we have\nP ( ||f(xj)||HS ≥ ǫ\n2\n) ≤ 2 exp ( − Dǫ 4M log [ 1 +\nMǫ\n2σ2(µ̃(ω))\n]) .\nPutting everything together, we obtain\nP ( sup x∈Ω ||f(x)||HS ≥ ǫ ) ≤ 4m1r ǫ + 2N exp ( − Dǫ 4M log [ 1 +\nMǫ\n2σ2(µ̃(ω))\n])\n≤ 4m1r ǫ + 2exp ( − Dǫ 4M log [ 1 +\nMǫ\n2σ2(µ̃(ω))\n])( 2diam(Ω)\nr\n)n\n= ar + b\nrn ,\nwhere a = 4m1ǫ and b = 2exp ( − Dǫ4M log [ 1 + Mǫ2σ2(µ̃(ω)) ]) (2diam(Ω))n.\nLet us find the value r > 0 that minimizes the right hand side in the above expression.\nThe function g(r) = ar + brn for a, b > 0 achieves its minimum on (0,∞) at r = ( bn a ) 1 n+1 , with the minimum value given by\ngmin = a n n+1 b 1 n+1 ( n 1 n+1 + n− n n+1 ) .\nSubstituting the value for a and b, we obtain\nP ( sup x∈Ω ||f(x)||HS ≥ ǫ ) ≤ a(n) ( m1diam(Ω) ǫ ) n n+1 exp ( − Dǫ 4(n+ 1)M log [ 1 +\nMǫ\n2σ2(µ̃(ω))\n])\nwhere a(n) = 2 3n+1 n+1 ( n 1 n+1 + n− n n+1 ) . This completes the proof of the theorem.\nProof of Theorem 13 It is straightforward to show that the optimization problem (67) has a unique solution fz,γ , which has the form fz,γ(x) = ∑u+l i=1 K(x, xi)ai for some ai ∈ W. Under the feature map representation ΦK , we have\nfz,γ(x) =\nu+l∑\ni=1\nK(x, xi)ai =\nu+l∑\ni=1\nΦK(x) ∗ΦK(xi)ai = ΦK(x) ∗h,\nwhere\nh =\nu+l∑\ni=1\nΦK(xi)ai = ΦK(x)a,\nas we claimed.\nTo prove Theorem 14, we first consider the following operators. The sampling operator Sx : HK → W l is defined by Sx(f) = (f(xi))li=1, for any\ny = (yi) l i=1 ∈ W l,\n〈Sxf,y〉W l = l∑\ni=1\n〈f(xi), yi〉W = l∑\ni=1\n〈K∗xif, yi〉HK\n=\nl∑\ni=1\n〈f,Kxiyi〉HK = 〈f, l∑\ni=1\nKxiyi〉HK .\nThus the adjoint operator S∗ x : W l → HK is given by\nS∗ x y = S∗ x (y1, . . . , yl) =\nl∑\ni=1\nKxiyi, y ∈ W l, (84)\nand the operator S∗ x Sx : HK → HK is given by\nS∗ x Sxf =\nl∑\ni=1\nKxif(xi) =\nl∑\ni=1\nKxiK ∗ xif. (85)\nConsider the operator EC,x : HK → Y l, defined by\nEC,xf = (CK ∗ x1f, . . . , CK ∗ xl f), (86)\nwith CK∗xi : HK → Y and KxiC∗ : Y → HK . For b = (b1, . . . , bl) ∈ Y l, we have\n〈b, EC,xf〉Y l = l∑\ni=1\n〈bi, CK∗xif〉Y = l∑\ni=1\n〈KxiC∗bi, f〉HK . (87)\nThe adjoint operator E∗C,x : Y l → HK is thus\nE∗C,x : (b1, . . . , bl) → l∑\ni=1\nKxiC ∗bi. (88)\nThe operator E∗C,xEC,x : HK → HK is then\nE∗C,xEC,xf → l∑\ni=1\nKxiC ∗CK∗xif, (89)\nwith C∗C : W → W. Proof of Theorem 14 Since f(x) = K∗x, we have\nfz,γ = argminf∈HK 1\nl\nl∑\ni=1\n||yi − CK∗xif ||2Y + γA||f ||2HK + γI〈f ,M f〉Wu+l . (90)\nUsing the operator EC,x, this becomes\nfz,γ = argminf∈HK 1\nl ||EC,xf − y||2Y l + γA||f ||2HK + γI〈f ,M f〉Wu+l . (91)\nDifferentiating (91) and setting the derivative to zero gives\n(E∗C,xEC,x + lγAI + lγIS ∗ x,u+lMSx,u+l)fz,γ = E ∗ C,xy, (92)\nwhich is fz,γ = (E ∗ C,xEC,x + lγAIHK + lγIS ∗ x,u+lMSx,u+l) −1E∗C,xy. On the set x = (xi)u+li=1 , the operators Sx,u+l : HK → Wu+l and S∗x,u+l : Wu+l → HK are given by\nSx,u+lf = (K ∗ xif) u+l i=1 , f ∈ HK ,\nS∗ x,u+lb =\nu+l∑\ni=1\nKxibi, b ∈ Wu+l.\nBy definition of the operators Sx,u+l and S ∗ x,u+l, we have\nS∗ x,u+l(I(u+l)×l ⊗ C∗)y =\nl∑\ni=1\nKxi(C ∗yi).\nThus the operator E∗C,x : Y l → HK is\nE∗C,x = S ∗ x,u+l(I(u+l)×l ⊗ C∗). (93)\nThe operator E∗C,xEC,x : HK → HK is given by\nE∗C,xEC,x = S ∗ x,u+l(J u+l l ⊗ C∗C)Sx,u+l : HK → HK , (94)\nEquation (92) becomes [ S∗ x,u+l(J u+l l ⊗C∗C + lγIM)Sx,u+l + lγAIHK ] fz,γ = S ∗ x,u+l(I(u+l)×l ⊗ C∗)y, (95)\nwhich gives\nfz,γ = [ S∗ x,u+l(J u+l l ⊗ C∗C + lγIM)Sx,u+l + lγAIHK ]−1 S∗ x,u+l(I(u+l)×l ⊗ C∗)y. (96)\nFor any x ∈ X ,\n(S∗ x,u+l(I(u+l)×l ⊗ C∗)y)(x) =\nl∑\ni=1\nK(x, xi)(C ∗yi) ∈ W.\nUsing the feature map ΦK , we have for any x ∈ X ,\n(S∗ x,u+l(I(u+l)×l ⊗ C∗)y)(x) =\nl∑\ni=1\nΦK(x) ∗ΦK(xi)(C ∗yi) ∈ W,\nand for any w ∈ W,\n〈(S∗ x,u+l(I(u+l)×l ⊗ C∗)y)(x), w〉W =\nl∑\ni=1\n〈ΦK(x)∗ΦK(xi)(C∗yi), w〉W\n=\nl∑\ni=1\n〈ΦK(xi)(C∗yi),ΦK(x)w〉FK\n= 〈ΦK(x)(I(u+l)×l ⊗ C∗)y,ΦK(x)w〉FK .\nFor any f ∈ HK ,\nS∗ x,u+lMSx,u+lf =\nu+l∑\ni=1\nKxi(M f)i.\nFor any x ∈ X ,\n(S∗ x,u+lMSx,u+lf)(x) =\nu+l∑\ni=1\nK(x, xi)(M f)i =\nu+l∑\ni=1\nΦK(x) ∗ΦK(xi)(M f)i ∈ W,\nand for any w ∈ W,\n〈(S∗ x,u+lMSx,u+lf)(x), w〉W =\nu+l∑\ni=1\n〈ΦK(xi)(M f)i,ΦK(x)w〉FK .\nWe have\nu+l∑\ni=1\nΦK(xi)(M f)i =\nu+l∑\ni=1\nΦK(xi)(MΦK(x) ∗h)i = ΦK(x)MΦK(x) ∗h ∈ FK .\nIt follows that\n〈(S∗ x,u+lMSx,u+lf)(x), w〉W = 〈ΦK(x)MΦK(x)∗h,ΦK(x)w〉FK . (97)\nSimilarly, for any f ∈ HK ,\nS∗ x,u+l(J u+l l ⊗ C∗C)Sx,u+lf = S∗x,u+l(Ju+ll ⊗ C∗C)f =\nu+l∑\ni=1\nKxi((J u+l l ⊗ C∗C)f)i\n=\nu+l∑\ni=1\nKxi((J u+l l ⊗ C∗C)ΦK(x)∗h)i.\nFor any x ∈ X ,\n(S∗ x,u+l(J u+l l ⊗ C∗C)Sx,u+lf)(x) =\nu+l∑\ni=1\nK(x, xi)((J u+l l ⊗ C∗C)ΦK(x)∗h)i\n=\nu+l∑\ni=1\nΦK(x) ∗ΦK(xi)((J u+l l ⊗ C∗C)ΦK(x)∗h)i.\nFor any w ∈ W,\n〈(S∗ x,u+l(J u+l l ⊗ C∗C)Sx,u+lf)(x), w〉W = 〈\nu+l∑\ni=1\nΦK(xi)((J u+l l ⊗ C∗C)ΦK(x)∗h)i,ΦK(x)w〉W\n= 〈ΦK(x)(Ju+ll ⊗ C∗C)ΦK(x)∗h,ΦK(x)w〉FK .\nEquation (95) is then equivalent to\n〈ΦK(x)(Ju+ll ⊗ C∗C)ΦK(x)∗h,ΦK(x)w〉FK + lγI〈ΦK(x)MΦK(x)∗h,ΦK(x)w〉FK + lγA〈h,ΦK(x)w〉FK = 〈ΦK(x)(I(u+l)×l ⊗ C∗)y,ΦK(x)w〉FK .\nfor all x ∈ X , w ∈ W, which is\n〈ΦK(x)[(Ju+ll ⊗ C∗C) + lγIM ]ΦK(x)∗h,ΦK(x)w〉FK + lγA〈h,ΦK(x)w〉FK = 〈ΦK(x)(I(u+l)×l ⊗ C∗)y,ΦK(x)w〉FK .\nfor all x ∈ X , w ∈ W. This is satisfied if ( ΦK(x)[(J u+l l ⊗ C∗C) + lγIM ]ΦK(x)∗ + lγAIFK ) h = ΦK(x)(I(u+l)×l ⊗ C∗)y.\nThis completes the proof of the theorem."
    } ],
    "references" : [ {
      "title" : "Universal multi-task kernels",
      "author" : [ "A. Caponnetto", "C. Micchelli", "M. Pontil", "Y. Ying" ],
      "venue" : null,
      "citeRegEx" : "Caponnetto et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Caponnetto et al\\.",
      "year" : 2011
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Rahimi and Recht.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rahimi and Recht.",
      "year" : 2007
    }, {
      "title" : "On the error of random Fourier features",
      "author" : [ "D. Sutherland", "J. Schneider" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "Sutherland and Schneider.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sutherland and Schneider.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In the scalar setting, one of the most powerful approaches for scaling up kernel methods is Random Fourier Features (Rahimi and Recht, 2007), which applies Bochner’s Theorem and the Inverse Fourier Transform to build random features that approximate a given shift-invariant kernel.",
      "startOffset" : 116,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "The approach in (Rahimi and Recht, 2007) has been improved both in terms of computational speed (Le et al.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : ", 2013) and rates of convergence (Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015).",
      "startOffset" : 33,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "Thirdly, when restricting to the scalar setting, our convergence holds for differentiable kernels, which is an improvement over the hypothesis of (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015; Brault et al., 2016), which all require the kernels to be twice-differentiable.",
      "startOffset" : 146,
      "endOffset" : 254
    }, {
      "referenceID" : 2,
      "context" : "Thirdly, when restricting to the scalar setting, our convergence holds for differentiable kernels, which is an improvement over the hypothesis of (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015; Brault et al., 2016), which all require the kernels to be twice-differentiable.",
      "startOffset" : 146,
      "endOffset" : 254
    }, {
      "referenceID" : 1,
      "context" : ", 2016) and previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015).",
      "startOffset" : 51,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : ", 2016) and previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015).",
      "startOffset" : 51,
      "endOffset" : 138
    }, {
      "referenceID" : 1,
      "context" : "Random Fourier features for scalar-valued kernels (Rahimi and Recht, 2007).",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "This is weaker than the assumptions in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015), which all require that ∫ Rn ||ω||2dρ(ω) < ∞, that is k is twice-differentiable.",
      "startOffset" : 39,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "This is weaker than the assumptions in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015), which all require that ∫ Rn ||ω||2dρ(ω) < ∞, that is k is twice-differentiable.",
      "startOffset" : 39,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, as with previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015), the convergence in (Brault et al.",
      "startOffset" : 60,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, as with previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabó, 2015), the convergence in (Brault et al.",
      "startOffset" : 60,
      "endOffset" : 147
    } ],
    "year" : 2016,
    "abstractText" : "This paper presents a framework for computing random operator-valued feature maps for operator-valued positive definite kernels. This is a generalization of the random Fourier features for scalar-valued kernels to the operator-valued case. Our general setting is that of operator-valued kernels corresponding to RKHS of functions with values in a Hilbert space. We show that in general, for a given kernel, there are potentially infinitely many random feature maps, which can be bounded or unbounded. Most importantly, given a kernel, we present a general, closed form formula for computing a corresponding probability measure, which is required for the construction of the Fourier features, and which, unlike the scalar case, is not uniquely and automatically determined by the kernel. We also show that, under appropriate conditions, random bounded feature maps can always be computed. Furthermore, we show the uniform convergence, under the Hilbert-Schmidt norm, of the resulting approximate kernel to the exact kernel on any compact subset of Euclidean space. Our convergence requires differentiable kernels, an improvement over the twice-differentiability requirement in previous work in the scalar setting. We then show how operator-valued feature maps and their approximations can be employed in a general vector-valued learning framework. The mathematical formulation is illustrated by numerical examples on matrix-valued kernels.",
    "creator" : "LaTeX with hyperref package"
  }
}