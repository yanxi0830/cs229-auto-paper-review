{
  "name" : "1411.2919.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bounded Regret for Finite-Armed Structured Bandits",
    "authors" : [ "Tor Lattimore" ],
    "emails" : [ "tlattimo@ualberta.ca", "remi.munos@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n29 19\nv1 [\ncs .L\nG ]\n1 1\nN ov"
    }, {
      "heading" : "1 Introduction",
      "text" : "The multi-armed bandit problem is a reinforcement learning problem with K actions. At each timestep a learner must choose an action i after which it receives a reward distributed with mean µi. The goal is to maximise the cumulative reward. This is perhaps the simplest setting in which the wellknown exploration/exploitation dilemma becomes apparent, with a learner being forced to choose between exploring arms about which she has little information, and exploiting by choosing the arm that currently appears optimal. (a)\nµ\n−1 0 1\n−1\n0\n1 (b)\n−1 0 1\n(c)\nWe consider a general class of Karmed bandit problems where the expected return of each arm may be dependent on other arms. This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1]. Let Θ ∋ θ∗ be an arbitrary\nparameter space and define the expected return of arm i by µi(θ∗) ∈ R. The learner is permitted to know the functions µ1 · · ·µK , but not the true parameter θ∗. The unknown parameter θ∗ determines the mean reward for each arm. The performance of a learner is measured by the (expected) cumulative regret, which is the difference between the expected return of the optimal policy and the (expected) return of the learner’s policy. Rn := nmaxi∈1···K µi(θ∗) − ∑n t=1 µIt(θ\n∗) where It is the arm chosen at time-step t.\nA motivating example is as follows. Suppose a long-running company must decide each week whether or not to purchase some new form of advertising with unknown expected returns. The problem may be formulated using the new setting by letting K = 2 and Θ = [−∞,∞]. We assume the base-line performance without purchasing the advertising is known and so define µ1(θ) = 0 for all θ. The expected return of choosing to advertise is µ2(θ) = θ (see Figure (b) above).\nOur main contribution is a new algorithm based on UCB [6] for the structured bandit problem with strong problem-dependent guarantees on the regret. The key improvement over UCB is that the new algorithm enjoys finite regret in many cases while UCB suffers logarithmic regret unless all arms have the same return. For example, in (a) and (c) above we show that finite regret is possible for all\n1Current affiliation: Google DeepMind.\nθ∗, while in the advertising problem finite regret is attainable if θ∗ ≥ 0. The improved algorithm exploits the known structure and so avoids the famous negative results by Lai and Robbins [17]. One insight from this work is that knowing the return of the optimal arm and a bound on the minimum gap is not the only information that leads to the possibility of finite regret. In the examples given above neither quantity is known, but the assumed structure is nevertheless sufficient for finite regret.\nDespite the enormous literature on bandits, as far as we are aware this is the first time this setting has been considered with the aim of achieving finite regret. There has been substantial work on exploiting various kinds of structure to reduce an otherwise impossible problem to one where sublinear (or even logarithmic) regret is possible [19, 4, 10, and references therein], but the focus is usually on efficiently dealing with large action spaces rather than sub-logarithmic/finite regret. The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15]. Also relevant is the paper by Agrawal et. al. [1], which studied a similar setting, but where Θ was finite. Graves and Lai [12] extended the aforementioned contribution to continuous parameter spaces (and also to MDPs). Their work differs from ours in a number of ways. Most notably, their objective is to compute exactly the asymptotically optimal regret in the case where finite regret is not possible. In the case where finite regret is possible they prove only that the optimal regret is sub-logarithmic, and do not present any explicit bounds on the actual regret. Aside from this the results depend on the parameter space being a metric space and they assume that the optimal policy is locally constant about the true parameter."
    }, {
      "heading" : "2 Notation",
      "text" : "General. Most of our notation is common with [8]. The indicator function is denoted by 1{expr} and is 1 if expr is true and 0 otherwise. We use log for the natural logarithm. Logical and/or are denoted by ∧ and ∨ respectively. Define function ω(x) = min {y ∈ N : z ≥ x log z, ∀z ≥ y}, which satisfies logω(x) ∈ O(log x). In fact, limx→∞ log(ω(x))/ log(x) = 1.\nBandits. Let Θ be a set. A K-armed structured bandit is characterised by a set of functions µk : Θ → R where µk(θ) is the expected return of arm k ∈ A := {1, · · · ,K} given unknown parameter θ. We define the mean of the optimal arm by the function µ∗ : Θ → R with µ∗(θ) := maxi µi(θ). The true unknown parameter that determines the means is θ∗ ∈ Θ. The best arm is i∗ := argmaxi µi(θ\n∗). The arm chosen at time-step t is denoted by It while Xi,s is the sth reward obtained when sampling from arm i. We denote the number of times arm i has been chosen at time-step t by Ti(t). The empiric estimate of the mean of arm i based on the first s samples is µ̂i,s. We define the gap between the means of the best arm and arm i by ∆i := µ∗(θ∗) − µi(θ∗). The set of sub-optimal arms is A′ := {i ∈ A : ∆i > 0}. The minimum gap is ∆min := mini∈A′ ∆i while the maximum gap is ∆max := maxi∈A ∆i. The cumulative regret is defined\nRn := n ∑\nt=1\nµ∗(θ∗)− n ∑\nt=1\nµIt = n ∑\nt=1\n∆It\nNote quantities like ∆i and i∗ depend on θ∗, which is omitted from the notation. As is rather common we assume that the returns are sub-gaussian, which means that if X is the return sampled from some arm, then lnE exp(λ(X − EX)) ≤ λ2σ2/2. As usual we assume that σ2 is known and does not depend on the arm. If X1 · · ·Xn are sampled independently from some arm with mean µ and Sn = ∑n t=1 Xt, then the following maximal concentration inequality is well-known.\nP\n{\nmax 1≤t≤n |St − tµ| ≥ ε\n}\n≤ 2 exp\n(\n− ε2\n2nσ2\n)\n.\nA straight-forward corollary is that P {|µ̂i,n − µi| ≥ ε} ≤ 2 exp\n(\n− ε2n\n2σ2\n)\n.\nIt is an important point that Θ is completely arbitrary. The classic multi-armed bandit can be obtained by setting Θ = RK and µk(θ) = θk, which removes all dependencies between the arms. The setting where the optimal expected return is known to be zero and a bound on ∆i ≥ ε is known can be regained by choosing Θ = (−∞,−ε]K×{1, · · · ,K} and µk(θ1, · · · , θK , i) = θk1{k 6= i}. We do not demand that µk : Θ → R be continuous, or even that Θ be endowed with a topology."
    }, {
      "heading" : "3 Structured UCB",
      "text" : "We propose a new algorithm called UCB-S that is a straight-forward modification of UCB [6], but where the known structure of the problem is exploited. At each time-step it constructs a confidence interval about the mean of each arm. From this a subspace Θ̃t ⊆ Θ is constructed, which contains the true parameter θ with high probability. The algorithm takes the optimistic action over all θ ∈ Θ̃t.\nAlgorithm 1 UCB-S\n1: Input: functions µ1, · · · , µk : Θ → [0, 1] 2: for t ∈ 1, . . . ,∞ do\n3: Define confidence set Θ̃t ←\n{\nθ̃ : ∀i, ∣ ∣\n∣ µi(θ̃)− µ̂i,Ti(t−1)\n∣ ∣ ∣ <\n√\nασ2 log t\nTi(t− 1)\n}\n4: if Θ̃t = ∅ then 5: Choose arm arbitrarily 6: else 7: Optimistic arm is i ← argmaxi supθ̃∈Θ̃t µi(θ̃) 8: Choose arm i\nRemark 1. The choice of arm when Θ̃t = ∅ does not affect the regret bounds in this paper. In practice, it is possible to simply increase t without taking an action, but this complicates the analysis. In many cases the true parameter θ∗ is never identified in the sense that we do not expect that Θ̃t → {θ∗}. The computational complexity of UCB-S depends on the difficulty of computing Θ̃t and computing the optimistic arm within this set. This is efficient in simple cases, like when µk is piecewise linear, but may be intractable for complex functions."
    }, {
      "heading" : "4 Theorems",
      "text" : "We present two main theorems bounding the regret of the UCB-S algorithm. The first is for arbitrary θ∗, which leads to a logarithmic bound on the regret comparable to that obtained for UCB by [6]. The analysis is slightly different because UCB-S maintains upper and lower confidence bounds and selects its actions optimistically from the model class, rather than by maximising the upper confidence bound as UCB does.\nTheorem 2. If α > 2 and θ ∈ Θ, then the algorithm UCB-S suffers an expected regret of at most\nERn ≤ 2∆maxK(α− 1) α− 2 + ∑\ni∈A′\n8ασ2 logn\n∆i + ∑\ni\n∆i\nIf the samples from the optimal arm are sufficient to learn the optimal action, then finite regret is possible. In Section 6 we give something of a converse by showing that if knowing the mean of the optimal arm is insufficient to act optimally, then logarithmic regret is unavoidable.\nTheorem 3. Let α = 4 and assume there exists an ε > 0 such that\n(∀θ ∈ Θ) |µi∗(θ ∗)− µi∗(θ)| < ε =⇒ ∀i 6= i ∗, µi∗(θ) > µi(θ). (1)\nThen ERn ≤ ∑\ni∈A′\n(\n32σ2 logω∗\n∆i +∆i\n)\n+ 3∆maxK + ∆maxK\n3\nω∗ ,\nwith ω∗ := max\n{\nω\n(\n8σ2αK\nε2\n)\n, ω\n(\n8σ2αK\n∆2min\n)}\n.\nRemark 4. For small ε and large n the expected regret looks like ERn ∈ O\n(\nK ∑\ni=1\nlog ( 1 ε )\n∆i\n)\n(for small n the regret is, of course, even smaller).\nThe explanation of the bound is as follows. If at some time-step t it holds that all confidence intervals contain the truth and the width of the confidence interval about i∗ drops below ε, then by the condition in Equation (1) it holds that i∗ is the optimistic arm within Θ̃t. In this case UCB-S\nsuffers no regret at this time-step. Since the number of samples of each sub-optimal arm grows at most logarithmically by the proof of Theorem 2, the number of samples of the best arm must grow linearly. Therefore the number of time-steps before best arm has been pulled O(ε−2) times is also O(ε−2). After this point the algorithm suffers only a constant cumulative penalty for the possibility that the confidence intervals do not contain the truth, which is finite for suitably chosen values of α. Note that Agrawal et. al. [1] had essentially the same condition to achieve finite regret as (1), but specified to the case where Θ is finite.\nAn interesting question is raised by comparing the bound in Theorem 3 to those given by Bubeck et. al. [11] where if the expected return of the best arm is known and ε is a known bound on the minimum gap, then a regret bound of\nO\n(\n∑\ni∈A′\n(\nlog ( 2∆i ε )\n∆i\n(\n1 + log log 1\nε\n)\n))\n(2)\nis achieved. If ε is close to ∆i, then this bound is an improvement over the bound given by Theorem 3, although our theorem is more general. The improved UCB algorithm [7] enjoys a bound on the expected regret of O( ∑\ni∈A′ 1 ∆i logn∆2i ). If we follow the same reasoning as above we obtain a bound comparable to (2). Unfortunately though, the extension of the improved UCB algorithm to the structured setting is rather challenging with the main obstruction being the extreme growth of the phases used by improved UCB. Refining the phases leads to super-logarithmic regret, a problem we ultimately failed to resolve. Nevertheless we feel that there is some hope of obtaining a bound like (2) in this setting.\nBefore the proofs of Theorems 2 and 3 we give some example structured bandits and indicate the regions where the conditions for Theorem 3 are (not) met. Areas where Theorem 3 can be applied to obtain finite regret are unshaded while those with logarithmic regret are shaded.\n(a) The conditions for Theorem 3 are met for all θ 6= 0, but for θ = 0 the regret strictly vanishes for all policies, which means that the regret is bounded by ERn ∈ O(1{θ∗ 6= 0} 1|θ∗| log 1 |θ∗| ).\n(b) Action 2 is uninformative and not globally optimal so Theorem 3 does not apply for θ < 1/2 where this action is optimal. For θ > 0 the optimal action is 1, when the conditions are met and finite regret is again achieved.\nERn ∈ O\n(\n1{θ∗ < 0} logn\n|θ∗| + 1{θ∗ > 0}\nlog 1θ∗\nθ∗\n)\n.\n(c) The conditions for Theorem 3 are again met for all non-zero θ∗, which leads as in (a) to a regret of ERn ∈ O(1{θ∗ 6= 0} 1|θ∗| log 1 |θ∗|).\nExamples (d) and (e) illustrate the potential complexity of the regions in which finite regret is possible. Note especially that in (e) the regret for θ∗ = 12 is logarithmic in the horizon, but finite for θ ∗ arbitrarily close. Example (f) is a permutation bandit with 3 arms where it can be clearly seen that the conditions of Theorem 3 are satisfied."
    }, {
      "heading" : "5 Proof of Theorems 2 and 3",
      "text" : "We start by bounding the probability that some mean does not lie inside the confidence set.\nLemma 5. P {Ft = 1} ≤ 2Kt exp(−α log(t)) where\nFt = 1\n{\n∃i : |µ̂i,Ti(t−1) − µi| ≥\n√\n2ασ2 log t\nTi(t− 1)\n}\n.\nProof. We use the concentration guarantees:\nP {Ft = 1} (a) = P\n{\n∃i : ∣ ∣µi(θ ∗)− µ̂i,Ti(t−1) ∣ ∣ ≥\n√\n2ασ2 log t\nTi(t− 1)\n}\n(b) ≤ K ∑\ni=1\nP\n{\n∣ ∣µi(θ ∗)− µ̂i,Ti(t−1) ∣ ∣ ≥\n√\n2ασ2 log t\nTi(t− 1)\n}\n(c) ≤ K ∑\ni=1\nt ∑\ns=1\nP\n{\n|µi(θ ∗)− µ̂i,s| ≥\n√\n2ασ2 log t\ns\n}\n(d) ≤ K ∑\ni=1\nt ∑\ns=1\n2 exp(−α log t) (e) = 2Kt1−α\nwhere (a) follows from the definition of Ft. (b) by the union bound. (c) also follows from the union bound and is the standard trick to deal with the random variable Ti(t − 1). (d) follows from the concentration inequalities for sub-gaussian random variables. (e) is trivial.\nProof of Theorem 2. Let i be an arm with ∆i > 0 and suppose that It = i. Then either Ft is true or\nTi(t− 1) <\n⌈\n8σ2α logn\n∆2i\n⌉\n=: ui(n) (3)\nNote that if Ft does not hold then the true parameter lies within the confidence set, θ∗ ∈ Θ̃t. Suppose on the contrary that Ft and (3) are both false.\nmax θ̃∈Θ̃t\nµi∗(θ̃) (a) ≥ µ∗(θ∗) (b) = µi(θ ∗) + ∆i (c) > ∆i + µ̂i,Ti(t−1) −\n√\n2σ2α log t\nTi(t− 1)\n(d) ≥ µ̂i,Ti(t−1) +\n√\n2ασ2 log t\nTi(t− 1)\n(e) ≥ max θ̃∈Θ̃t µi(θ̃),\nwhere (a) follows since θ∗ ∈ Θ̃t. (b) is the definition of the gap. (c) since Ft is false. (d) is true because (3) is false. Therefore arm i is not taken. We now bound the expected number of times that arm i is played within the first n time-steps by\nETi(n) (a) = E\nn ∑\nt=1\n1{It = i} (b) ≤ ui(n) + E n ∑\nt=ui+1\n1{It = i ∧ (3) is false}\n(c) ≤ ui(n) + E n ∑\nt=ui+1\n1{Ft = 1 ∧ It = i}\nwhere (a) follows from the linearity of expectation and definition of Ti(n). (b) by Equation (3) and the definition of ui(n) and expectation. (c) is true by recalling that playing arm i at time-step t implies that either Ft or (3) must be true. Therefore\nERn ≤ ∑\ni∈A′\n∆i\n(\nui(n) + E\nn ∑\nt=ui+1\n1{Ft = 1 ∧ It = i}\n)\n≤ ∑\ni∈A′\n∆iui(n) + ∆maxE\nn ∑\nt=1\n1{Ft = 1}\n(4)\nBounding the second summation\nE\nn ∑\nt=1\n1{Ft = 1} (a) =\nn ∑\nt=1\nP {Ft = 1} (b) ≤ n ∑\nt=1\n2Kt1−α (c) ≤ 2K(α− 1)\nα− 2\nwhere (a) follows by exchanging the expectation and sum and because the expectation of an indicator function can be written as the probability of the event. (b) by Lemma 5 and (c) is trivial. Substituting into (4) leads to\nERn ≤ 2∆maxK(α− 1) α− 2 + ∑\ni∈A′\n8ασ2 logn\n∆i + ∑\ni\n∆i.\nBefore the proof of Theorem 3 we need a high-probability bound on the number of times arm i is pulled, which is proven along the lines of similar results by [5].\nLemma 6. Let i ∈ A′ be some sub-optimal arm. If z > ui(n), then P {Ti(n) > z} ≤ 2Kz2−α\nα− 2 .\nProof. As in the proof of Theorem 2, if t ≤ n and Ft is false and Ti(t − 1) > ui(n) ≥ ui(t), then arm i is not chosen. Therefore\nP {Ti(n) > z} ≤ n ∑\nt=z+1\nP {Ft = 1} (a) ≤ n ∑\nt=z+1\n2Kt1−α (b) ≤ 2K\n∫ n\nz\nt1−αdt (c) ≤ 2Kz2−α\nα− 2\nwhere (a) follows from Lemma 5 and (b) and (c) are trivial.\nLemma 7. Assume the conditions of Theorem 3 and additionally that Ti∗(t− 1) ≥ ⌈ 8ασ2 log t ε2 ⌉ and"
    }, {
      "heading" : "Ft is false. Then It = i∗.",
      "text" : "Proof. Since Ft is false, for θ̃ ∈ Θ̃t we have:\n|µi∗(θ̃)− µi∗(θ ∗)|\n(a) ≤ |µi∗(θ̃)− µ̂i∗,Ti(t−1)|+ |µ̂i∗,Ti(t−1) − µi∗(θ ∗)| (b) < 2\n√\n2σ2α log t\nTi∗(t− 1)\n(c) ≤ ε\nwhere (a) is the triangle inequality. (b) follows by the definition of the confidence interval and because Ft is false. (c) by the assumed lower bound on Ti∗(t− 1). Therefore by (1), for all θ̃ ∈ Θ̃t it holds that the best arm is i∗. Finally, since Ft is false, θ∗ ∈ Θ̃t, which means that Θ̃t 6= ∅. Therefore It = i∗ as required.\nProof of Theorem 3. Let ω∗ be some constant to be chosen later. Then the regret may be written as\nERn ≤ E ω∗ ∑\nt=1\nK ∑\ni=1\n∆i1{It = i}+∆maxE n ∑\nt=ω∗+1\n1{It 6= i ∗} . (5)\nThe first summation is bounded as in the proof of Theorem 2 by\nE\nω∗ ∑\nt=1\n∑\ni∈A\n∆i1{It = i} ≤ ∑\ni∈A′\n(\n∆i + 8ασ2 logω∗\n∆i\n)\n+\nω∗ ∑\nt=1\nP {Ft = 1} . (6)\nWe now bound the second sum in (5) and choose ω∗. By Lemma 6, if nK > ui(n), then\nP\n{ Ti(n) > n\nK\n} ≤ 2K\nα− 2\n(\nK\nn\n)α−2\n. (7)\nSuppose t ≥ ω∗ := max { ω ( 8σ2αK ε2 ) , ω ( 8σ2αK ∆2\nmin\n)}\n. Then tK > ui(t) for all i 6= i ∗ and tK ≥\n8σ2α log t ε2 . By the union bound\nP\n{\nTi∗(t) < 8σ2α log t\nε2\n}\n(a) ≤ P\n{\nTi∗(t) < t\nK\n}\n(b) ≤ P\n{\n∃i : Ti(t) > t\nK\n}\n(c) <\n2K2\nα− 2\n(\nK\nt\n)α−2\n(8)\nwhere (a) is true since tK ≥ 8σ2α log t ε2 . (b) since ∑K i=1 Ti(t) = t. (c) by the union bound and (7). Now if Ti(t) ≥ 8σ2α log t ε2 and Ft is false, then the chosen arm is i ∗. Therefore\nE\nn ∑\nt=ω∗+1\n1{It 6= i ∗} ≤\nn ∑\nt=ω∗+1\nP {Ft = 1}+ n ∑\nt=ω∗+1\nP\n{\nTi(t− 1) < 8σ2α log t\nε2\n}\n(a) ≤ n ∑\nt=ω∗+1\nP {Ft = 1}+ 2K2\nα− 2\nn ∑\nt=ω∗+1\n(\nK\nt\n)α−2\n(b) ≤ n ∑\nt=ω∗+1\nP {Ft = 1}+ 2K2\n(α− 2)(α− 3)\n(\nK\nω∗\n)α−3\n(9)\nwhere (a) follows from (8) and (b) by straight-forward calculus. Therefore by combining (5), (6) and (9) we obtain\nERn ≤ ∑\ni:∆i>0\n∆i\n⌈\n8σ2α logω∗\n∆2i\n⌉\n+ 2∆maxK\n2\n(α− 2)(α− 3)\n(\nK\nω∗\n)α−3\n+∆max\nn ∑\nt=1\nP {Ft = 1}\n≤ ∑\ni:∆i>0\n∆i\n⌈\n8σ2α logω∗\n∆2i\n⌉\n+ 2∆maxK\n2\n(α− 2)(α− 3)\n(\nK\nω∗\n)α−3\n+ 2∆maxK(α− 1)\nα− 2\nSetting α = 4 leads to ERn ≤ K ∑\ni=1\n(\n32σ2 logω∗\n∆i +∆i\n)\n+ 3∆maxK + ∆maxK\n3\nω∗ ."
    }, {
      "heading" : "6 Lower Bounds and Ambiguous Examples",
      "text" : "We prove lower bounds for two illustrative examples of structured bandits. Some previous work is also relevant. The famous paper by Lai and Robbins [17] shows that the bound of Theorem 2 cannot in general be greatly improved. Many of the techniques here are borrowed from Bubeck et. al. [11]. Given a fixed algorithm and varying θ we denote the regret and expectation by Rn(θ) and Eθ respectively. Returns are assumed to be sampled from a normal distribution with unit variance, so that σ2 = 1.\nTheorem 8. Given the structured bandit depicted in Figure 3.(a) or Figure 2.(c), then for all θ > 0 and all algorithms the regret satisfies max {E−θRn(−θ), EθRn(θ)} ≥ 18θ for sufficiently large n.\nProof. The proof uses the same technique as the proof of Theorem 5 in the paper by [11]. Fix an algorithm and let Pθ,t be the probability measure on the space of outcomes up to time-step t under the bandit determined by parameter θ.\nE−θRn(−θ) + EθRn(θ) (a) = 2θ\n(\nE−θ\nn ∑\nt=1\n1{It = 1}+ Eθ\nn ∑\nt=1\n1{It = 2}\n)\n(b) = 2θ\nn ∑\nt=1\n(P−θ,t {It = 1}+ Pθ,t {It = 2}) (c) ≥ θ n ∑\nt=1\nexp (−KL(P−θ,t,Pθ,t))\n(d) = θ\nn ∑\nt=1\nexp ( −4tθ2 )\n(e) ≥ 1\n8θ\nwhere (a) follows since 2|θ| is the gap between the expected returns of the two arms given parameter θ and by the definition of the regret. (b) by replacing the expectations with probabilities. (c) follows from Lemma 4 by [11] where KL(P−θ,t,Pθ,t) is the relative entropy between measures P−θ,t and Pθ,t. (d) is true by computing the relative entropy between two normals with unit variance and means separated by 2θ, which is 4θ2. (e) holds for sufficiently large n.\nTheorem 9. Let Θ, {µ1, µ2} be a structured bandit where returns are sampled from a normal distribution with unit variance. Assume there exists a pair θ1, θ2 ∈ Θ and constant ∆ > 0 such that µ1(θ1) = µ1(θ2) and µ1(θ1) ≥ µ2(θ1) + ∆ and µ2(θ2) ≥ µ1(θ2) + ∆. Then the following hold:\n(1) Eθ1Rn(θ1) ≥ 1+log 2n∆2 8∆ − 1 2Eθ2Rn(θ2)\n(2) Eθ2Rn(θ2) ≥ n∆ 2 exp (−4Eθ1Rn(θ1)∆)− Eθ1Rn(θ1)\nA natural example where the conditions are satisfied is depicted in Figure 3.(b) and by choosing θ1 = −1, θ2 = 1. We know from Theorem 3 that UCB-S enjoys finite regret of Eθ2Rn(θ2) ∈ O( 1 ∆ log 1 ∆ ) and logarithmic regret Eθ1Rn(θ1) ∈ O( 1 ∆ logn). Part 1 of Theorem 9 shows that if we demand finite regret Eθ2Rn(θ2) ∈ O(1), then the regret Eθ1Rn(θ1) is necessarily logarithmic. On the other hand, part 2 shows that if we demand Eθ1Rn(θ1) ∈ o(log(n)), then the regret Eθ2Rn(θ2) ∈ Ω(n). Therefore the trade-off made by UCB-S essentially cannot be improved.\nProof of Theorem 9. Again, we make use of the techniques of [11].\nEθ1Rn(θ1) + Eθ2Rn(θ2) (a) ≥ ∆(Eθ1T2(n) + Eθ2T1(n)) (b) ≥ ∆ n ∑\nt=1\n(Pθ1 {It = 2}+ Pθ2 {It = 1})\n(c) ≥ ∆\n2\nn ∑\nt=1\nexp (−KL(Pθ1,t,Pθ2,t)) (d) ≥ n∆\n2 exp (−KL(Pθ1,n,Pθ2,n))\n(e) ≥ n∆\n2 exp\n( −4∆2Eθ1T2(n) )\n(f) ≥ n∆\n2 exp (−4∆Eθ1Rn(θ1)) (10)\nwhere (a) follows from the definition of the regret and the bandits used. (b) by the definition of Tk(n). (c) by Lemma 4 of [11]. (d) since the relative entropy KL(Pθ1,t,Pθ2,t) is increasing with t. (e) By checking that KL(Pθ1,n,Pθ2,n) = 4∆ 2 Eθ1T2(n). (f) by substituting the definition of the regret. Now part 2 is completed by rearranging (10). For part 1 we also rearrange (10) to obtain\nEθ1Rn(θ1) ≥ n∆\n2 exp (−4∆Eθ1Rn(θ1))− Eθ2Rn(θ2)\nLetting x = Eθ1Rn(θ1) and using the constraint above we obtain:\nx ≥ x\n2 +\n1\n2\n(\nn∆\n2 exp (−4∆x)− Eθ2Rn(θ2)\n)\n.\nBut by simple calculus the function on the right hand side is minimised for x = 14∆ log(2n∆ 2), which leads to\nEθ1Rn(θ1) ≥ log(2n∆2)\n8∆ +\n1\n8∆ −\n1 2 Eθ2Rn(θ2).\nDiscussion of Figure 3.(c/d). In both examples there is an ambiguous region for which the lower bound (Theorem 9) does not show that logarithmic regret is unavoidable, but where Theorem 3 cannot be applied to show that UCB-S achieves finite regret. We managed to show that finite regret is possible in both cases by using a different algorithm. For (c) we could construct a carefully tuned algorithm for which the regret was at most O(1) if θ ≤ 0 and O(1θ log log 1 θ ) otherwise. This result contradicts a claim by Bubeck et. al. [11, Thm. 8]. Additional discussion of the ambiguous case in general, as well as this specific example, may be found in the supplementary material. One observation is that unbridled optimism is the cause of the failure of UCB-S in these cases. This is illustrated by Figure 3.(d) with θ ≤ 0. No matter how narrow the confidence interval about µ1, if the second action has not been taken sufficiently often, then there will still be some belief that θ > 0 is possible where the second action is optimistic, which leads to logarithmic regret. Adapting the algorithm to be slightly risk averse solves this problem."
    }, {
      "heading" : "7 Experiments",
      "text" : "We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to UCB [6, 8]. Rewards were sampled from normal distributions with unit variances. For UCB we chose α = 2, while we used the theoretically justified α = 4 for Algorithm 1. All code is available in the supplementary material. Each data-point is the average of 500 independent samples with the blue crosses and red squares indicating the regret of UCB-S and UCB respectively.\n−0.2 −0.1 0 0.1 0.2 0\n100\n200\nθ\nÊ θ R\nn (θ\n)\nK = 2, µ1(θ) = θ, µ2(θ) = −θ, n = 50 000 (see Figure 2.(a))\n0 5e4 1e5 0\n100\n200\nn\nÊ θ R\nn (θ\n)\nK = 2, µ1(θ) = θ, µ2(θ) = −θ, θ = 0.04 (see Figure 2.(a))\n−1 0 1 0\n200\n400\nθ\nÊ θ R\nn (θ\n)\nK = 2, µ1(θ) = 0, µ2(θ) = θ, n = 50 000 (see Figure 2.(b))\nThe results show that Algorithm 1 typically out-performs regular UCB. The exception is the top right experiment where UCB performs slightly better for θ < 0. This is not surprising, since in this case the structured version of UCB cannot exploit the additional structure and suffers due to worse constant factors. On the other hand, if θ > 0, then UCB endures logarithmic regret and performs significantly worse than its structured counterpart. The superiority of Algorithm 1 would be accentuated in the top left and bottom right experiments by increasing the horizon.\n−1 0 1 0\n50\n100\n150\nθ\nÊ θ R\nn (θ\n)\nK = 2, µ1(θ) = θ1{θ > 0}, µ2(θ) = −θ1{θ < 0}, n = 50 000 (see Figure 2.(c))"
    }, {
      "heading" : "8 Conclusion",
      "text" : "The limitation of the new approach is that the proof techniques and algorithm are most suited to the case where the number of actions is relatively small. Generalising the techniques to large action spaces is therefore an important open problem. There is still a small gap between the upper and lower bounds, and the lower bounds have only been proven for special examples. Proving a general problem-dependent lower bound is an interesting question, but probably extremely challenging given the flexibility of the setting. We are also curious to know if there exist problems for which the optimal regret is somewhere between finite and logarithmic. Another question is that of how to define Thompson sampling for structured bandits. Thompson sampling has recently attracted a great deal of attention [13, 2, 14, 3, 9], but so far we are unable even to define an algorithm resembling Thompson sampling for the general structured bandit problem. Not only because we have not endowed Θ with a topology, but also because choosing a reasonable prior seems rather problem-dependent. An advantage of our approach is that we do not rely on knowing the distribution of the rewards while with one notable exception [9] this is required for Thompson sampling.\nAcknowledgements. Tor Lattimore was supported by the Google Australia Fellowship for Machine Learning and the Alberta Innovates Technology Futures, NSERC. The majority of this work was completed while Rémi Munos was visiting Microsoft Research, New England. This research was partially supported by the European Community’s Seventh Framework Programme under grant agreements no. 270327 (project CompLACS)."
    }, {
      "heading" : "A Ambiguous Case",
      "text" : "We assume for convenience that K = 2 and µ1(θ) 6= µ2(θ) for all θ ∈ Θ. The second assumption is non-restrictive, since an algorithm cannot perform badly on the θ for which µ1(θ) = µ2(θ), so we can simply remove these points from the parameter space. Now Θ can be partitioned into three sets according to whether or not finite regret is expected by Theorem 3, or impossible by Theorem 9.\nΘeasy := { θ ∈ Θ : ∃ε > 0 such that ∣ ∣µi∗(θ)(θ ′)− µi∗(θ)(θ ′) ∣ ∣ < ε =⇒ i∗(θ′) = i∗(θ) } Θhard := { θ ∈ Θ : ∃θ′ ∈ Θ such that µi∗(θ)(θ) = µi∗(θ)(θ ′) and i∗(θ′) 6= i∗(θ) }\nΘamb := Θ−Θeasy −Θhard\nThe topic of this section is to study whether or not finite regret is possible on Θamb, and what sacrifices need to be made in order to achieve this. Some examples are given in Figure 4. Note that (a) was considered by Bubeck et. al. [11, Thm. 8] and will receive special attention here.\nThe main theorem is this section shows that finite regret is indeed possible for many θ ∈ Θamb without incurring significant additional regret for θ ∈ Θeasy and retaining logarithmic regret for θ ∈ Θhard. The following algorithm is similar to Algorithm 1, but favours actions which may be optimal for some plausible ambiguous θ. Theorems will be given subsequently, but proofs are omitted. Algorithm 2\n1: Input: functions µ1, · · · , µk : Θ → [0, 1], {βt} ∞ t=1 2: κ1 = 0 3: for t ∈ 1, . . . ,∞ do\n4: Define confidence set: Θ̃t ←\n{\nθ̃ : ∀i, ∣ ∣\n∣ µi(θ̃)− µ̂i,Ti(t−1)\n∣ ∣ ∣ <\n√\nασ2 log t\nTi(t− 1)\n}\n5: if κt = 0 ∧ ∃Θ̃t ∩Θamb 6= ∅ then 6: Choose θ ∈ Θ̃t ∩Θamb arbitrarily and set κt = i∗(θ)\n7: Choose It = argmax k sup θ∈Θ̃t µk(θ) + 1{k = κt}\n√\nβt log t\nTk(t− 1)\n8: κt+1 ← 1{It = κt}\nTheorem 10. Suppose K = 2, θ ∈ Θ and i∗ = 1 and βt = log log t and ∆ := µ1(θ) − µ2(θ). Then Algorithm 2 satisfies:\n1. lim supn→∞ EθRn(θ)/ logn < ∞.\n2. If θ ∈ Θeasy, then EθRn(θ) ∈ O( 1∆ (log 1 ∆)(log log 1 ∆ )).\n3. If θ is such that\nlim δ→0 sup θ′:|µ1(θ)−µ1(θ′)|<δ\nµ2(θ ′)− µ1(θ′) |µ1(θ)− µ1(θ′)| < ∞, (11)\nthen limn→∞ EθRn(θ) < ∞.\nRemark 11. The condition (11) not satisfied for θ ∈ Θhard, since in this case there exists some θ′ with µ1(θ) = µ1(θ′) but where µ2(θ′) − µ1(θ′) > 0. The condition may not be satisfied even for θ ∈ Θamb. See, for example, Figure 4.(c). The condition is satisfied for all other ambiguous θ for the problems shown in Figure 4.(a,b,d) where the risk µ2(θ′) − µ1(θ′) decreases linearly as µ1(θ) − µ1(θ′) converges to zero.\nThe following theorem shows that you cannot get finite regret for the ambiguous case where (11) is not satisfied without making sacrifices in the easy case. Theorem 12. Suppose θ ∈ Θamb with i∗(θ) = 1 andEθRn(θ) ∈ O(1). Then there exists a constant c > 0 such that for each θ′ with i∗(θ′) = 2 we have\nEθ′Rn(θ ′) ≥ c\nµ2(θ ′)− µ1(θ′)\n(µ1(θ) − µ1(θ′))2 .\nTherefore if the condition (3) in the statement of Theorem 10 is not satisfied for some ambiguous θ, then we can construct a sequence {θ′}∞i=1 such that limi→∞ µ1(θ ′ i) = µ1(θ) and where\nlim i→∞\n(µ2(θ ′ i)− µ1(θ ′))Eθ′ i Rn(θ ′ i) = ∞,\nwhich means that the regret must grow faster than the inverse of the gap. The situation becomes worse the faster the quantity below diverges to infinity.\nsup θ′:|µ1(θ)−µ1(θ′)|<δ\nµ2(θ ′)− µ1(θ)\n|µ1(θ)− µ1(θ′)| .\nIn summary, finite regret is often possible in the ambiguous case, but may lead to worse regret guarantees in the easy case. Ultimately we are not sure how to optimise these trade-offs and there are still many interesting unanswered questions.\nAnalysis of Figure 4.(a)\nWe now consider a case of special interest that was previously studied by Bubeck et. al. [11] and is depicted in Figure 4.(a). The structured bandit falls into the ambiguous case when θ ≤ 0, since no interval about µ1(θ) = 0 is sufficient to rule out the possibility that the second action is in fact optimal. Nevertheless, using a carefully crafted algorithm we show that the optimal regret is smaller than one might expect. The new algorithm operates in phases, choosing each action a certain number of times. If all evidence points to the first action being best, then this is taken until its optimality is proven to be implausible, while otherwise the second action is taken. The algorithm is heavily biased towards choosing the first action where estimation is more challenging, and where the cost of an error tends to be smaller. Algorithm 3\n1: α ← 5 2: for ℓ ∈ 2, . . . ,∞ do // Iterate over phases 3: n1,ℓ = 2 ℓ and n2,ℓ = ℓ2 4: Choose each arm k ∈ {1, 2} exactly nk,ℓ times and let µ̂k,ℓ,nk,ℓ be the average return 5: s ← 0 6: if µ̂1,ℓ,n1,ℓ ≥ − √ α n1,ℓ log logn1,ℓ and µ̂2,n2,ℓ < −1/2 then 7: while µ̂1,ℓ,n1,ℓ+s ≥ − √ α log log(n1,ℓ+s) n1,ℓ+s\ndo 8: Choose action 1 and s ← s+ 1 and µ̂1,ℓ,n1,ℓ+s is average return of arm 1 this phase\n9: else 10: while µ̂2,ℓ,n2,ℓ+s ≥ − 1 2 do 11: Choose action 2 and s ← s+ 1 and µ̂2,ℓ,n2,ℓ+s is average return of arm 2 this phase\nTheorem 13. Let Θ = [−1, 1] and µ1(0) = −θ1{θ > 0} and µ2(θ) = −1{θ ≤ 0}. Assume returns are normally distributed with unit variance. Then Algorithm 3 suffers regret bounded by\nEθRn(θ) ∈\n{ O ( 1 θ log log 1 θ )\nif θ > 0 O(1) otherwise.\nRemark 14. Theorem 13 contradicts a result by Bubeck et. al. [11, Thm. 8], which states that for any algorithm\nmax\n{\nE0Rn(0), sup θ>0 θ · EθRn(θ)\n}\n∈ Ω (logn) .\nBut by Theorem 13 there exists an algorithm for which\nmax\n{\nE0Rn(0), sup θ>0 θ · EθRn(θ)\n}\n∈ O\n(\nsup θ>0 min\n{\nθ2n, log log 1\nθ\n})\n= O(log log n).\nWe are currently unsure whether or not the dependence on log log 1θ can be dropped from the bound given in Theorem 13. Note that Theorem 3 cannot be applied when θ = 0, so Algorithm 1 suffers logarithmic regret in this case. Algorithm 3 is carefully tuned and exploits the asymmetry in the problem. It is possible that the result of Bubeck et. al. can be saved in spirit by using the symmetric structured bandit depicted in Figure 4.(b). This would still only give a worst-case bound and does not imply that finite problem-dependent regret is impossible.\nProof of Theorem 13. It is enough to consider only θ ∈ [0, 1], since the returns on the arms is constant for θ ∈ [−1, 0]. We let L be the number phases (times that the outer loop is executed) and Tℓ be the number of times the sub-optimal action is taken in the ℓth phase. Recall that µ̂k,ℓ,t denotes the empirical estimate of µ based on t samples taken in the ℓth phase.\nStep 1: Decomposing the regret\nThe regret is decomposed:\n(θ = 0) : E0Rn(0) = E0\nL ∑\nℓ=0\nTℓ =\n∞ ∑\nℓ=0\nP0 {L ≥ ℓ}E0[Tℓ|L ≥ ℓ]\n(θ > 0) : EθRn(θ) = θEθ\nL ∑\nℓ=0\nTℓ = θ\n∞ ∑\nℓ=0\nPθ {L ≥ ℓ}Eθ[Tℓ|L ≥ ℓ]\nStep 2: Bounding Eθ[Tℓ|L ≥ ℓ]\nWe need to consider the cases when θ = 0 and θ > 0 separately. If s ≥ 1, then\nP0 {Tℓ ≥ n2,ℓ + s|L ≥ ℓ} (a) ≤ P0\n{\nµ̂2,ℓ,n2,ℓ+s−1 ≥ − 1\n2\n}\n(b) = P0\n{\nµ̂2,ℓ,n2,ℓ+s−1 − µ2(0) ≥ 1\n2\n}\n(c) ≤ exp\n(\n− 1\n2 (n2,ℓ + s− 1)\n)\n(d) ≤ exp ( − s\n2\n)\n,\nwhere (a) follows since if the second action is chosen more than n2,ℓ times in the ℓth phase, then that phase ends when µ̂2,ℓ,t < − 12 , (b) by noting that µ2(0) = −1, (c) follows from the standard concentration inequality and the fact that unit variance is assumed, (d) since n2,ℓ ≥ 1. Therefore by Lemma 17 we have that E0[Tℓ|L ≥ ℓ] ≤ n2,ℓ + 2e1/2. Now assume θ > 0 and define\nω2(x) = min {z : y ≥ x log log y, ∀y ≥ z} ,\nwhich satisfies ω2(x) ∈ O(x log log x). If n1,ℓ + s− 1 ≥ ω2 ( 4α θ2 ) , then\nPθ {Tℓ ≥ n1,ℓ + s|L ≥ ℓ} (a) ≤ Pθ\n{\nµ̂1,ℓ,n1,ℓ+s−1 ≥ −\n√\nα\nnℓ,1 + s− 1 log log(n1,ℓ + s− 1)\n}\n(b) = Pθ\n{\nµ̂1,ℓ,n1,ℓ+s−1 − µ1(θ) ≥ θ −\n√\nα\nnℓ,1 + s− 1 log log(n1,ℓ + s− 1)\n}\n(c) ≤ Pθ { µ̂1,ℓ,n1,ℓ+s−1 − µ1(θ) ≥ θ/2 } (d) ≤ exp\n(\n− θ2\n8 (n1,ℓ + s− 1)\n)\n(e) ≤ exp\n(\n− θ2\n8 s\n)\n,\nwhere (a) follows since if the first arm (which is now sub-optimal) is chosen more than n1,ℓ times, then the phase ends if µ̂1,ℓ,t drops below the confidence interval. (b) since µ1(θ) = −θ. (c) since n1,ℓ + s− 1 ≥ ω2 ( 4α θ2 )\n. (d) by the usual concentration inequality and (e) since n1,ℓ ≥ 1. Another application of Lemma 17 yields\nEθ[Tℓ|L ≥ ℓ] ≤ max\n{\nn1,ℓ, ω2\n(\n4α\nθ2\n)}\n+ 8eθ\n2/8\nθ2 ,\nwhere the max appears because we demanded that n1,ℓ + s− 1 ≥ ω2 ( 4α θ2 )\nand since at the start of each phase the first action is taken at least n1,ℓ times before the phase can end.\nBounding the number of phases\nAgain we consider the cases when θ = 0 and θ ≥ 0 separately.\nP0 {L > ℓ} (a) ≤ P0\n{\nµ̂2,ℓ,n2,ℓ ≥ − 1\n2 ∨ ∃s : µ̂1,ℓ,n1,ℓ+s ≤ −\n√\nα\nn1,ℓ + s log log(n1,ℓ + s)\n}\n(b) ≤ P0\n{\nµ̂2,ℓ,n2,ℓ ≥ − 1\n2\n}\n+ P0\n{\n∃s : µ̂1,ℓ,n1,ℓ+s ≤ −\n√\nα\nn1,ℓ + s log log(n1,ℓ + s)\n}\n(c) ≤ exp ( − n2,ℓ 8 ) + P0\n{\n∃s : µ̂1,ℓ,n1,ℓ+s ≤ −\n√\nα\nn1,ℓ + s log log(n1,ℓ + s)\n}\n, (12)\nwhere (a) is true since the ℓth phase will not end if µ̂2,ℓ,n2,ℓ < −1/2 and if µ̂1,ℓ,t never drops below the confidence interval. (b) follows from the union bound and (c) by the concentration inequality. The second term is bounded using the maximal inequality and the peeling technique.\nP0\n{\n∃s : µ̂1,ℓ,n1,ℓ+s ≤ −\n√\nα\nn1,ℓ + s log log(n1,ℓ + s)\n}\n(a) ≤ ∞ ∑\nk=0\nP0\n{\n∃t : 2kn1,ℓ ≤ t ≤ 2 k+1n1,ℓ ∧ µ̂1,ℓ,t ≤ −\n√\nα t log log t\n}\n(b) ≤ ∞ ∑\nk=0\nP0\n{\n∃t ≤ 2k+1n1,ℓ : µ̂1,ℓ,t ≤ −\n√\nα\nn1,ℓ2k log log 2kn1,ℓ\n}\n(c) ≤ ∞ ∑\nk=0\nexp ( −α log log ( 2kn1,ℓ )) (d) = ∞ ∑\nk=0\n(\n1\nlog 2k + logn1,ℓ\n)α (e)\n≤ 2\nlog 2\n(\n1\nℓ log 2\n)α−1\nwhere (a) follows by the union bound, (b) by bounding t in the interval 2kn1,ℓ ≤ t ≤ 2k+1n1,ℓ. (c) follows from the maximal inequality. (d) is trivial while (e) follows by approximating the sum by an integral. By combining with (12) we obtain\nP0 {L > ℓ} ≤ exp ( − n2,ℓ 8 ) + 2 log 2\n(\n1\nℓ log 2\n)α−1\n= exp\n(\n− ℓ\n8\n)\n+ 2\nlog 2\n(\n1\nℓ log 2\n)α−1\n.\nMore straight-forwardly, if θ > 0, then\nPθ {L > ℓ} ≤ Pθ\n{\n∃s : µ̂2,n2,ℓ+s < − 1\n2\n}\n≤ 5 exp ( − n2,ℓ 16 ) ,\nwhere in the last inequality we used Lemma 16 and naive bounding.\nPutting it together\nWe now combine the results of the previous components to obtain the required bound on the regret. Recall that α = 5.\n(θ = 0) : E0Rn(0) = Eθ\n∞ ∑\nℓ=2\nTℓ =\n∞ ∑\nℓ=2\nP0 {L ≥ ℓ}E0[Tℓ|L ≥ ℓ]\n≤ ∞ ∑\nℓ=2\n(\nexp ( − n2,ℓ 8 ) + 2 log 2\n(\n1\nℓ log 2\n)α−1 )\n( n2,ℓ + 2e 1/2 )\n= ∞ ∑\nℓ=2\n(\nexp\n(\n− ℓ\n8\n)\n+ 2\nlog 2\n(\n1\nℓ log 2\n)α−1 )\n( ℓ2 + 2e1/2 ) ∈ O(1)\n(θ > 0) : EθRn(θ) = θEθ\n∞ ∑\nℓ=2\nTℓ = θ\n∞ ∑\nℓ=1\nP {L ≥ ℓ}E[Tℓ|L ≥ ℓ]\n≤ 5θ ∞ ∑\nℓ=2\nexp ( − n2,ℓ 16 )\n(\nmax\n{\nn1,ℓ, ω2\n(\n4α\nθ2\n)}\n+ 8eθ\n2/8\nθ2\n)\n∈ O ( θ · ω2 ( α\nθ2\n))\n= O\n(\n1 θ log log 1 θ\n)\n."
    }, {
      "heading" : "B Technical Lemmas",
      "text" : "Lemma 15. Define functions ω and ω2 by\nω(x) := min {z > 1 : y ≥ x log y, ∀y ≥ z}\nω2(x) := min {z > e : y ≥ x log log y, ∀y ≥ z} .\nThen ω(x) ∈ O (x log x) and ω2(x) ∈ O (x log log x).\nLemma 16. Let {Xi} ∞ i=1 be sampled from some sub-gaussian distributed arm with mean µ and unit sub-gaussian constant. Define µ̂t = 1t ∑t s=1 Xs. Then for s ≥ 6/∆ 2 we have\nP {∃t ≥ s : µ̂t − µ ≥ ∆} ≤ p+ 1\nlog 2 log\n1\n1− p\nwhere p = exp ( − s∆ 2\n4\n)\n.\nProof. We assume without loss of generality that µ = 0 and use a peeling argument combined with Azuma’s maximal inequality\nP {∃t > s : µ̂t ≥ ∆} (a) = P { ∃k ∈ N, 2ks ≤ t < 2k+1s : µ̂t ≥ ∆ }\n(b) = P { ∃k ∈ N, 2ks ≤ t < 2k+1s : tµ̂t ≥ t∆ }\n(c) ≤ P { ∃k ∈ N, 2ks ≤ t < 2k+1s : tµ̂t ≥ 2 ks∆ }\n(d) ≤ ∞ ∑\nk=0\nP { ∃2ks ≤ t < 2k+1s : tµ̂t ≥ 2 ks∆/2 }\n(e) ≤ ∞ ∑\nk=0\nP { ∃t < 2k+1s : tµ̂t ≥ 2 ks∆/2 }\n(f) ≤ ∞ ∑\nk=0\nexp\n(\n− 1\n2\n( 2ks∆ )2\n2k+1s\n)\n(g) =\n∞ ∑\nk=0\nexp\n(\n− 2ks∆2\n4\n)\n(h) =\n∞ ∑\nk=0\nexp\n(\n− s∆2\n4\n)2k (i)\n≤ p+ 1\nlog 2 log\n1\n1− p\nwhere (a) follows by splitting the sum over an exponential grid. (b) by comparing cumulative differences rather than the means. (c) since t > 2ks. (d) by the union bound over all k. (e) follows by increasing the range. (f) by Azuma’s maximal inequality. (g) and (h) are true by straight-forward arithmetic while (i) follows from Lemma 18.\nLemma 17. Suppose z is a positive random variable and for some α > 0 it holds for all natural numbers k that P {z ≥ k} ≤ exp(−kα). Then Ez ≤ e α\nα\nProof. Let δ ∈ (0, 1). Then\nP\n{\nz ≥ log 1\nδ\n}\n≤ P\n{\nz ≥\n⌊\nlog 1\nδ\n⌋}\n≤ exp\n(\n−α\n⌊\nlog 1\nδ\n⌋)\n≤ eα · exp\n(\n−α log\n(\n1\nδ\n))\n= eα · δα.\nTo complete the proof we use a standard identity to bound the expectation\nEz ≤\n∫ 1\n0\n1 δ P\n{\nz ≥ log 1\nδ\n} dδ ≤ eα ∫ 1\n0\nδα−1dδ = eα\nα .\nLemma 18. Let p ∈ (0, 7/10). Then ∞ ∑\nk=0\np2 k ≤ p+ 1\nlog 2 log\n1\n1− p .\nProof. Splitting the sum and comparing to an integral yields: ∞ ∑\nk=0\np2 k (a) = p+\n∞ ∑\nk=1\np2 k (b) ≤ p+\n∫ ∞\n0\np2 k dk (c) = p+\n1\nlog 2\n∫ ∞\n1\npu/udu\n(d) ≤ p+ 1\nlog 2\n∞ ∑\nu=1\npu/u (e) = p+\n1\nlog 2 log\n1\n1− p\nwhere (a) follows by splitting the sum. (b) by noting that p2 k\nis monotone decreasing and comparing to an integral. (c) by substituting u = 2k. (d) by reverting back to a sum. (e) follows from a standard formula."
    }, {
      "heading" : "C Table of Notation",
      "text" : "K number of arms Θ parameter space θ∗ unknown parameter θ∗ ∈ Θ It arm played at time-step t Ti(n) number times arm i has been played after time-step n Xi,s sth reward obtained when playing arm i ∆i gap between the means of the best arm and the ith arm ∆min minimum gap, ∆min := mini:∆i>0 ∆i ∆max maximum gap, ∆max := maxi ∆i A set of arms A := {1, 2, · · · ,K} A′ set of suboptimal arms A := {i : ∆i > 0} Rn regret at time-step n given unknown true parameter θ∗ Rn(θ) regret at time-step n given parameter θ µi(θ) mean of arm i given θ µ̂i,s empiric estimate of the mean of arm i after s plays µ∗(θ) maximum return at θ. µ∗(θ) := maxi µi(θ) i∗ optimal arm given θ∗ i∗(θ) optimal arm given θ ω(x) minimum value y such that z ≥ x log z for all z ≥ y ω2(x) minimum value y such that z ≥ x log log z for all z ≥ y Ft event that the true value of some mean is outside the confidence interval about the empiric estimate at time-step t α parameter controlling how exploring the algorithm UCB-S is σ2 known parameter controlling the tails of the distributions governing the return\nof the arms\nui(n) critical number of samples for arm i. ui(n) := ⌈ 8σ2α logn ∆2\ni\n⌉"
    } ],
    "references" : [ {
      "title" : "Asymptotically efficient adaptive allocation schemes for controlled markov chains: Finite parameter space",
      "author" : [ "Rajeev Agrawal", "Demosthenis Teneketzis", "Venkatachalam Anantharam" ],
      "venue" : "Automatic Control, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1989
    }, {
      "title" : "Analysis of Thompson sampling for the multi-armed bandit problem",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "Proceedings of the 25th Annual Conference on Learning Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Further optimal regret bounds for thompson sampling",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "Proceedings of the 16th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Bandits, query learning, and the haystack dimension",
      "author" : [ "Kareem Amin", "Michael Kearns", "Umar Syed" ],
      "venue" : "Journal of Machine Learning Research-Proceedings Track,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Variance estimates and exploration function in multi-armed bandit",
      "author" : [ "Jean-Yves Audibert", "Rémi Munos", "Csaba Szepesvári" ],
      "venue" : "Technical report, research report 07-31, Certis-Ecole des Ponts,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicoló Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem",
      "author" : [ "Peter Auer", "Ronald Ortner" ],
      "venue" : "Periodica Mathematica Hungarica,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi" ],
      "venue" : "Now Publishers Incorporated,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Prior-free and prior-dependent regret bounds for thompson sampling",
      "author" : [ "Sébastien Bubeck", "Che-Yu Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Online optimization in X-armed bandits",
      "author" : [ "Sébastien Bubeck", "Rémi Munos", "Gilles Stoltz", "Csaba Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Bounded regret in stochastic multiarmed bandits",
      "author" : [ "Sébastien Bubeck", "Vianney Perchet", "Philippe Rigollet" ],
      "venue" : "Proceedings of the 26th Annual Conference on Learning Theory,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Asymptotically efficient adaptive choice of control laws in controlled Markov chains",
      "author" : [ "Todd L Graves", "Tze Leung Lai" ],
      "venue" : "SIAM journal on control and optimization,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1997
    }, {
      "title" : "Thompson sampling: An asymptotically optimal finite-time analysis",
      "author" : [ "Emilie Kaufmann", "Nathaniel Korda", "Rémi Munos" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Thompson sampling for 1-dimensional exponential family bandits",
      "author" : [ "Nathaniel Korda", "Emilie Kaufmann", "Rémi Munos" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Asymptotically optimal allocation of treatments in sequential experiments",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Design of Experiments: Ranking and Selection,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1984
    }, {
      "title" : "Optimal sequential sampling from two populations",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1984
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Advances in applied mathematics,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1985
    }, {
      "title" : "A structured multiarmed bandit problem and the greedy policy",
      "author" : [ "Adam J Mersereau", "Paat Rusmevichientong", "John N Tsitsiklis" ],
      "venue" : "Automatic Control, IEEE Transactions on,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Our main contribution is a new algorithm based on UCB [6] for the structured bandit problem with strong problem-dependent guarantees on the regret.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "The improved algorithm exploits the known structure and so avoids the famous negative results by Lai and Robbins [17].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].",
      "startOffset" : 175,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].",
      "startOffset" : 175,
      "endOffset" : 182
    }, {
      "referenceID" : 15,
      "context" : "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 14,
      "context" : "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].",
      "startOffset" : 300,
      "endOffset" : 304
    }, {
      "referenceID" : 0,
      "context" : "[1], which studied a similar setting, but where Θ was finite.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "Graves and Lai [12] extended the aforementioned contribution to continuous parameter spaces (and also to MDPs).",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "Most of our notation is common with [8].",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "3 Structured UCB We propose a new algorithm called UCB-S that is a straight-forward modification of UCB [6], but where the known structure of the problem is exploited.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 1 UCB-S 1: Input: functions μ1, · · · , μk : Θ → [0, 1] 2: for t ∈ 1, .",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "The first is for arbitrary θ, which leads to a logarithmic bound on the regret comparable to that obtained for UCB by [6].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "[1] had essentially the same condition to achieve finite regret as (1), but specified to the case where Θ is finite.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "[11] where if the expected return of the best arm is known and ε is a known bound on the minimum gap, then a regret bound of O (",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "The improved UCB algorithm [7] enjoys a bound on the expected regret of O( ∑",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "Before the proof of Theorem 3 we need a high-probability bound on the number of times arm i is pulled, which is proven along the lines of similar results by [5].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "The famous paper by Lai and Robbins [17] shows that the bound of Theorem 2 cannot in general be greatly improved.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "[11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "The proof uses the same technique as the proof of Theorem 5 in the paper by [11].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "(c) follows from Lemma 4 by [11] where KL(P−θ,t,Pθ,t) is the relative entropy between measures P−θ,t and Pθ,t.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "Again, we make use of the techniques of [11].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "(c) by Lemma 4 of [11].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "7 Experiments We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to UCB [6, 8].",
      "startOffset" : 114,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "7 Experiments We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to UCB [6, 8].",
      "startOffset" : 114,
      "endOffset" : 120
    } ],
    "year" : 2014,
    "abstractText" : "We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problemdependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.",
    "creator" : "Creator"
  }
}