{
  "name" : "1307.1493.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dropout Training as Adaptive Regularization",
    "authors" : [ "Stefan Wager", "Sida Wang", "Percy Liang" ],
    "emails" : [ "swager@stanford.edu,", "pliang}@cs.stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dropout training was introduced by Hinton et al. [1] as a way to control overfitting by randomly omitting subsets of features at each iteration of a training procedure.1 Although dropout has proved to be a very successful technique, the reasons for its success are not yet well understood at a theoretical level.\nDropout training falls into the broader category of learning methods that artificially corrupt training data to stabilize predictions [2, 4, 5, 6, 7]. There is a well-known connection between artificial feature corruption and regularization [8, 9, 10]. For example, Bishop [9] showed that the effect of training with features that have been corrupted with additive Gaussian noise is equivalent to a form of L2-type regularization in the low noise limit. In this paper, we take a step towards understanding how dropout training works by analyzing it as a regularizer. We focus on generalized linear models (GLMs), a class of models for which feature dropout reduces to a form of adaptive model regularization.\nUsing this framework, we show that dropout training is first-order equivalent to L2-regularization after transforming the input by diag(Î)−1/2, where Î is an estimate of the Fisher information matrix. This transformation effectively makes the level curves of the objective more spherical, and so balances out the regularization applied to different features. In the case of logistic regression, dropout can be interpreted as a form of adaptive L2-regularization that favors rare but useful features.\nThe problem of learning with rare but useful features is discussed in the context of online learning by Duchi et al. [11], who show that their AdaGrad adaptive descent procedure achieves better regret bounds than regular stochastic gradient descent (SGD) in this setting. Here, we show that AdaGrad\nS.W. is supported by a B.C. and E.J. Eaves Stanford Graduate Fellowship. 1Hinton et al. introduced dropout training in the context of neural networks specifically, and also advocated omitting random hidden layers during training. In this paper, we follow [2, 3] and study feature dropout as a generic training method that can be applied to any learning algorithm.\nar X\niv :1\n30 7.\n14 93\nv2 [\nst at\n.M L\n] 1\nN ov\n2 01\nand dropout training have an intimate connection: Just as SGD progresses by repeatedly solving linearized L2-regularized problems, a close relative of AdaGrad advances by solving linearized dropout-regularized problems.\nOur formulation of dropout training as adaptive regularization also leads to a simple semi-supervised learning scheme, where we use unlabeled data to learn a better dropout regularizer. The approach is fully discriminative and does not require fitting a generative model. We apply this idea to several document classification problems, and find that it consistently improves the performance of dropout training. On the benchmark IMDB reviews dataset introduced by [12], dropout logistic regression with a regularizer tuned on unlabeled data outperforms previous state-of-the-art. In follow-up research [13], we extend the results from this paper to more complicated structured prediction, such as multi-class logistic regression and linear chain conditional random fields."
    }, {
      "heading" : "2 Artificial Feature Noising as Regularization",
      "text" : "We begin by discussing the general connections between feature noising and regularization in generalized linear models (GLMs). We will apply the machinery developed here to dropout training in Section 4.\nA GLM defines a conditional distribution over a response y ∈ Y given an input feature vector x ∈ Rd:\npβ(y | x) def = h(y) exp{y x · β −A(x · β)}, `x,y(β) def = − log pβ(y | x). (1)\nHere, h(y) is a quantity independent of x and β, A(·) is the log-partition function, and `x,y(β) is the loss function (i.e., the negative log likelihood); Table 1 contains a summary of notation. Common examples of GLMs include linear (Y = R), logistic (Y = {0, 1}), and Poisson (Y = {0, 1, 2, . . . }) regression.\nGiven n training examples (xi, yi), the standard maximum likelihood estimate β̂ ∈ Rd minimizes the empirical loss over the training examples:\nβ̂ def = arg min\nβ∈Rd n∑ i=1 `xi, yi(β). (2)\nWith artificial feature noising, we replace the observed feature vectors xi with noisy versions x̃i = ν(xi, ξi), where ν is our noising function and ξi is an independent random variable. We first create many noisy copies of the dataset, and then average out the auxiliary noise. In this paper, we will consider two types of noise:\n• Additive Gaussian noise: ν(xi, ξi) = xi + ξi, where ξi ∼ N (0, σ2Id×d). • Dropout noise: ν(xi, ξi) = xi ξi, where is the elementwise product of two vec-\ntors. Each component of ξi ∈ {0, (1 − δ)−1}d is an independent draw from a scaled Bernoulli(1− δ) random variable. In other words, dropout noise corresponds to setting x̃ij to 0 with probability δ and to xij/(1− δ) else.2\nIntegrating over the feature noise gives us a noised maximum likelihood parameter estimate:\nβ̂ = arg min β∈Rd n∑ i=1 Eξ [`x̃i, yi(β)] , where Eξ [Z] def = E [Z | {xi, yi}] (3)\nis the expectation taken with respect to the artificial feature noise ξ = (ξ1, . . . , ξn). Similar expressions have been studied by [9, 10].\nFor GLMs, the noised empirical loss takes on a simpler form: n∑ i=1 Eξ [`x̃i, yi(β)] = n∑ i=1 − (y xi · β − Eξ [A(x̃i · β)]) = n∑ i=1 `xi, yi(β) +R(β). (4)\n2Artificial noise of the form xi ξi is also called blankout noise. For GLMs, blankout noise is equivalent to dropout noise as defined by [1].\nThe first equality holds provided that Eξ[x̃i] = xi, and the second is true with the following definition:\nR(β) def = n∑ i=1 Eξ [A(x̃i · β)]−A(xi · β). (5)\nHere,R(β) acts as a regularizer that incorporates the effect of artificial feature noising. In GLMs, the log-partition function A must always be convex, and so R is always positive by Jensen’s inequality.\nThe key observation here is that the effect of artificial feature noising reduces to a penalty R(β) that does not depend on the labels {yi}. Because of this, artificial feature noising penalizes the complexity of a classifier in a way that does not depend on the accuracy of a classifier. Thus, for GLMs, artificial feature noising is a regularization scheme on the model itself that can be compared with other forms of regularization such as ridge (L2) or lasso (L1) penalization. In Section 6, we exploit the label-independence of the noising penalty and use unlabeled data to tune our estimate of R(β).\nThe fact that R does not depend on the labels has another useful consequence that relates to prediction. The natural prediction rule with artificially noised features is to select ŷ to minimize expected loss over the added noise: ŷ = argminy Eξ[`x̃, y(β̂)]. It is common practice, however, not to noise the inputs and just to output classification decisions based on the original feature vector [1, 3, 14]: ŷ = argminy `x, y(β̂). It is easy to verify that these expressions are in general not equivalent, but they are equivalent when the effect of feature noising reduces to a label-independent penalty on the likelihood. Thus, the common practice of predicting with clean features is formally justified for GLMs."
    }, {
      "heading" : "2.1 A Quadratic Approximation to the Noising Penalty",
      "text" : "Although the noising penalty R yields an explicit regularizer that does not depend on the labels {yi}, the form of R can be difficult to interpret. To gain more insight, we will work with a quadratic approximation of the type used by [9, 10]. By taking a second-order Taylor expansion of A around x · β, we get that Eξ [A(x̃ · β)] − A(x · β) ≈ 12A\n′′(x · β) Varξ [x̃ · β] . Here the first-order term Eξ [A′(x · β)(x̃− x)] vanishes because Eξ[x̃] = x. Applying this quadratic approximation to (5) yields the following quadratic noising regularizer, which will play a pivotal role in the rest of the paper:\nRq(β) def =\n1\n2 n∑ i=1 A′′(xi · β) Varξ [x̃i · β] . (6)\nThis regularizer penalizes two types of variance over the training examples: (i) A′′(xi · β), which corresponds to the variance of the response yi in the GLM, and (ii) Varξ[x̃i · β], the variance of the estimated GLM parameter due to noising.3\nAccuracy of approximation Figure 1a compares the noising penalties R and Rq for logistic regression in the case that x̃ · β is Gaussian;4 we vary the mean parameter p def= (1+ e−x·β)−1 and the noise level σ. We see that Rq is generally very accurate, although it tends to overestimate the true penalty for p ≈ 0.5 and tends to underestimate it for very confident predictions. We give a graphical explanation for this phenomenon in the Appendix (Figure A.1).\nThe quadratic approximation also appears to hold up on real datasets. In Figure 1b, we compare the evolution during training of both R and Rq on the 20 newsgroups alt.atheism vs\n3Although Rq is not convex, we were still able (using an L-BFGS algorithm) to train logistic regression with Rq as a surrogate for the dropout regularizer without running into any major issues with local optima.\n4This assumption holds a priori for additive Gaussian noise, and can be reasonable for dropout by the central limit theorem.\nsoc.religion.christian classification task described in [15]. We see that the quadratic approximation is accurate most of the way through the learning procedure, only deteriorating slightly as the model converges to highly confident predictions.\nIn practice, we have found that fitting logistic regression with the quadratic surrogate Rq gives similar results to actual dropout-regularized logistic regression. We use this technique for our experiments in Section 6."
    }, {
      "heading" : "3 Regularization based on Additive Noise",
      "text" : "Having established the general quadratic noising regularizer Rq, we now turn to studying the effects of Rq for various likelihoods (linear and logistic regression) and noising models (additive and dropout). In this section, we warm up with additive noise; in Section 4 we turn to our main target of interest, namely dropout noise.\nLinear regression Suppose x̃ = x + ε is generated by by adding noise with Var[ε] = σ2Id×d to the original feature vector x. Note that Varξ[x̃ · β] = σ2‖β‖22, and in the case of linear regression A(z) = 12z\n2, so A′′(z) = 1. Applying these facts to (6) yields a simplified form for the quadratic noising penalty:\nRq(β) = 1\n2 σ2n‖β‖22. (7)\nThus, we recover the well-known result that linear regression with additive feature noising is equivalent to ridge regression [2, 9]. Note that, with linear regression, the quadratic approximation Rq is exact and so the correspondence with L2-regularization is also exact.\nLogistic regression The situation gets more interesting when we move beyond linear regression. For logistic regression, A′′(xi · β) = pi(1− pi) where pi = (1 + exp(−xi · β))−1 is the predicted probability of yi = 1. The quadratic noising penalty is then\nRq(β) = 1\n2 σ2‖β‖22 n∑ i=1 pi(1− pi). (8)\nIn other words, the noising penalty now simultaneously encourages parsimonious modeling as before (by encouraging ‖β‖22 to be small) as well as confident predictions (by encouraging the pi’s to move away from 12 )."
    }, {
      "heading" : "4 Regularization based on Dropout Noise",
      "text" : "Recall that dropout training corresponds to applying dropout noise to training examples, where the noised features x̃i are obtained by setting x̃ij to 0 with some “dropout probability” δ and to xij/(1− δ) with probability (1− δ), independently for each coordinate j of the feature vector. We can check that:\nVarξ [x̃i · β] = 1\n2\nδ\n1− δ d∑ j=1 x2ijβ 2 j , (9)\nand so the quadratic dropout penalty is\nRq(β) = 1\n2\nδ\n1− δ n∑ i=1 A′′(xi · β) d∑ j=1 x2ijβ 2 j . (10)\nLetting X ∈ Rn×d be the design matrix with rows xi and V (β) ∈ Rn×n be a diagonal matrix with entries A′′(xi · β), we can re-write this penalty as\nRq(β) = 1\n2\nδ\n1− δ β> diag(X>V (β)X)β. (11)\nLet β∗ be the maximum likelihood estimate given infinite data. When computed at β∗, the matrix 1 nX >V (β∗)X = 1n ∑n i=1∇2`xi, yi(β∗) is an estimate of the Fisher information matrix I. Thus, dropout can be seen as an attempt to apply an L2 penalty after normalizing the feature vector by diag(I)−1/2. The Fisher information is linked to the shape of the level surfaces of `(β) around β∗. If I were a multiple of the identity matrix, then these level surfaces would be perfectly spherical around β∗. Dropout, by normalizing the problem by diag(I)−1/2, ensures that while the level surfaces of `(β) may not be spherical, the L2-penalty is applied in a basis where the features have been balanced out. We give a graphical illustration of this phenomenon in Figure A.2.\nLinear Regression For linear regression, V is the identity matrix, so the dropout objective is equivalent to a form of ridge regression where each column of the design matrix is normalized before applying the L2 penalty.5 This connection has been noted previously by [3].\nLogistic Regression The form of dropout penalties becomes much more intriguing once we move beyond the realm of linear regression. The case of logistic regression is particularly interesting. Here, we can write the quadratic dropout penalty from (10) as\nRq(β) = 1\n2\nδ\n1− δ n∑ i=1 d∑ j=1 pi(1− pi)x2ij β2j . (12)\nThus, just like additive noising, dropout generally gives an advantage to confident predictions and small β. However, unlike all the other methods considered so far, dropout may allow for some large pi(1− pi) and some large β2j , provided that the corresponding cross-term x2ij is small. Our analysis shows that dropout regularization should be better than L2-regularization for learning weights for features that are rare (i.e., often 0) but highly discriminative, because dropout effectively does not penalize βj over observations for which xij = 0. Thus, in order for a feature to earn a large β2j , it suffices for it to contribute to a confident prediction with small pi(1 − pi) each time that it is active.6 Dropout training has been empirically found to perform well on tasks such as document\n5Normalizing the columns of the design matrix before performing penalized regression is standard practice, and is implemented by default in software like glmnet for R [16].\n6To be precise, dropout does not reward all rare but discriminative features. Rather, dropout rewards those features that are rare and positively co-adapted with other features in a way that enables the model to make confident predictions whenever the feature of interest is active.\nclassification where rare but discriminative features are prevalent [3]. Our result suggests that this is no mere coincidence.\nWe summarize the relationship between L2-penalization, additive noising and dropout in Table 2. Additive noising introduces a product-form penalty depending on both β and A′′. However, the full potential of artificial feature noising only emerges with dropout, which allows the penalty terms due to β andA′′ to interact in a non-trivial way through the design matrixX (except for linear regression, in which all the noising schemes we consider collapse to ridge regression)."
    }, {
      "heading" : "4.1 A Simulation Example",
      "text" : "The above discussion suggests that dropout logistic regression should perform well with rare but useful features. To test this intuition empirically, we designed a simulation study where all the signal is grouped in 50 rare features, each of which is active only 4% of the time. We then added 1000 nuisance features that are always active to the design matrix, for a total of d = 1050 features. To make sure that our experiment was picking up the effect of dropout training specifically and not just normalization of X , we ensured that the columns of X were normalized in expectation.\nThe dropout penalty for logistic regression can be written as a matrix product\nRq(β) = 1\n2\nδ\n1− δ (· · · pi(1− pi) · · ·)  · · ·· · · x2ij · · · · · · · · ·β2j · · ·  . (13) We designed the simulation study in such a way that, at the optimal β, the dropout penalty should have structure\nSmall (confident prediction) Big (weak prediction) ( ) · · · · · · 0 · · ·  \nBig (useful feature)\nSmall (nuisance feature)\n . (14)\nA dropout penalty with such a structure should be small. Although there are some uncertain predictions with large pi(1 − pi) and some big weights β2j , these terms cannot interact because the corresponding terms x2ij are all 0 (these are examples without any of the rare discriminative features and thus have no signal). Meanwhile, L2 penalization has no natural way of penalizing some βj more and others less. Our simulation results, given in Table 3, confirm that dropout training outperforms L2-regularization here as expected. See Appendix A.1 for details."
    }, {
      "heading" : "5 Dropout Regularization in Online Learning",
      "text" : "There is a well-known connection between L2-regularization and stochastic gradient descent (SGD). In SGD, the weight vector β̂ is updated with β̂t+1 = β̂t − ηt gt, where gt = ∇`xt, yt(β̂t) is the gradient of the loss due to the t-th training example. We can also write this update as a linear L2-penalized problem\nβ̂t+1 = argminβ { `xt, yt(β̂t) + gt · (β − β̂t) + 1\n2ηt ‖β − β̂t‖22\n} , (15)\nwhere the first two terms form a linear approximation to the loss and the third term is an L2regularizer. Thus, SGD progresses by repeatedly solving linearized L2-regularized problems.\nAs discussed by Duchi et al. [11], a problem with classic SGD is that it can be slow at learning weights corresponding to rare but highly discriminative features. This problem can be alleviated by running a modified form of SGD with β̂t+1 = β̂t − η A−1t gt, where the transformation At is also learned online; this leads to the AdaGrad family of stochastic descent rules. Duchi et al. use At = diag(Gt) 1/2 where Gt = ∑t i=1 gig > i and show that this choice achieves desirable regret bounds in the presence of rare but useful features. At least superficially, AdaGrad and dropout seem to have similar goals: For logistic regression, they can both be understood as adaptive alternatives to methods based on L2-regularization that favor learning rare, useful features. As it turns out, they have a deeper connection.\nThe natural way to incorporate dropout regularization into SGD is to replace the penalty term ‖β − β̂‖22/2η in (15) with the dropout regularizer, giving us an update rule\nβ̂t+1 = argminβ { `xt, yt(β̂t) + gt · (β − β̂t) +Rq(β − β̂t; β̂t) } (16)\nwhere, Rq(·; β̂t) is the quadratic noising regularizer centered at β̂t:7\nRq(β − β̂t; β̂t) = 1\n2 (β − β̂t)> diag(Ht) (β − β̂t),where Ht = t∑ i=1 ∇2`xi, yi(β̂t). (17)\nThis implies that dropout descent is first-order equivalent to an adaptive SGD procedure with At = diag(Ht). To see the connection between AdaGrad and this dropout-based online procedure, recall that for GLMs both of the expressions\nEβ∗ [ ∇2`x, y(β∗) ] = Eβ∗ [ ∇`x, y(β∗)∇`x, y(β∗)> ] (18)\nare equal to the Fisher information I [17]. In other words, as β̂t converges to β∗,Gt andHt are both consistent estimates of the Fisher information. Thus, by using dropout instead of L2-regularization to solve linearized problems in online learning, we end up with an AdaGrad-like algorithm.\nOf course, the connection between AdaGrad and dropout is not perfect. In particular, AdaGrad allows for a more aggressive learning rate by using At = diag(Gt)−1/2 instead of diag(Gt)−1. But, at a high level, AdaGrad and dropout appear to both be aiming for the same goal: scaling the features by the Fisher information to make the level-curves of the objective more circular. In contrast, L2-regularization makes no attempt to sphere the level curves, and AROW [18]—another popular adaptive method for online learning—only attempts to normalize the effective feature matrix but does not consider the sensitivity of the loss to changes in the model weights. In the case of logistic regression, AROW also favors learning rare features, but unlike dropout and AdaGrad does not privilege confident predictions.\n7This expression is equivalent to (11) except that we used β̂t and not β − β̂t to compute Ht."
    }, {
      "heading" : "6 Semi-Supervised Dropout Training",
      "text" : "Recall that the regularizer R(β) in (5) is independent of the labels {yi}. As a result, we can use additional unlabeled training examples to estimate it more accurately. Suppose we have an unlabeled dataset {zi} of size m, and let α ∈ (0, 1] be a discount factor for the unlabeled data. Then we can define a semi-supervised penalty estimate\nR∗(β) def =\nn\nn+ αm\n( R(β) + αRUnlabeled(β) ) , (19)\nwhere R(β) is the original penalty estimate and RUnlabeled(β) = ∑ i Eξ[A(zi · β)] − A(zi · β) is computed using (5) over the unlabeled examples zi. We select the discount parameter by crossvalidation; empirically, α ∈ [0.1, 0.4] works well. For convenience, we optimize the quadratic surrogate Rq∗ instead of R∗. Another practical option would be to use the Gaussian approximation from [3] for estimating R∗(β).\nMost approaches to semi-supervised learning either rely on using a generative model [19, 20, 21, 22, 23] or various assumptions on the relationship between the predictor and the marginal distribution over inputs. Our semi-supervised approach is based on a different intuition: we’d like to set weights to make confident predictions on unlabeled data as well as the labeled data, an intuition shared by entropy regularization [24] and transductive SVMs [25].\nExperiments We apply this semi-supervised technique to text classification. Results on several datasets described in [15] are shown in Table 4a; Figure 2 illustrates how the use of unlabeled data improves the performance of our classifier on a single dataset. Overall, we see that using unlabeled data to learn a better regularizer R∗(β) consistently improves the performance of dropout training.\nTable 4b shows our results on the IMDB dataset of [12]. The dataset contains 50,000 unlabeled examples in addition to the labeled train and test sets of size 25,000 each. Whereas the train and test examples are either positive or negative, the unlabeled examples contain neutral reviews as well. We train a dropout-regularized logistic regression classifier on unigram/bigram features, and use the unlabeled data to tune our regularizer. Our method benefits from unlabeled data even in the presence of a large amount of labeled data, and achieves state-of-the-art accuracy on this dataset."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We analyzed dropout training as a form of adaptive regularization. This framework enabled us to uncover close connections between dropout training, adaptively balanced L2-regularization, and AdaGrad; and led to a simple yet effective method for semi-supervised training. There seem to be multiple opportunities for digging deeper into the connection between dropout training and adaptive regularization. In particular, it would be interesting to see whether the dropout regularizer takes on a tractable and/or interpretable form in neural networks, and whether similar semi-supervised schemes could be used to improve on the results presented in [1].\n8Our implementation of semi-supervised MNB. MNB with EM [20] failed to give an improvement."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Description of Simulation Study\nSection 4.1 gives the motivation for and a high-level description of our simulation study. Here, we give a detailed description of the study.\nGenerating features. Our simulation has 1050 features. The first 50 discriminative features form 5 groups of 10; the last 1000 features are nuisance terms. Each xi was independently generated as follows:\n1. Pick a group number g ∈ 1, ..., 25, and a sign sgn = ±1. 2. If g ≤ 5, draw the entries of xi with index between 10 (g − 1) + 1 and 10 (g − 1) + 10\nuniformly from sgn · Exp(C), where C is selected such that E[(xi)2j ] = 1 for all j. Set all the other discriminative features to 0. If g > 5, set all the discriminative features to 0.\n3. Draw the last 1000 entries of xi independently from N (0, 1).\nNotice that this procedure guarantees that the columns of X all have the same expected second moments.\nGenerating labels. Given an xi, we generate yi from the Bernoulli distribution with parameter σ(xi · β), where the first 50 coordinates of β are 0.057 and the remaining 1000 coordinates are 0. The value 0.057 was selected to make the average value of |xi · β| in the presence of signal be 2.\nTraining. For each simulation run, we generated a training set of size n = 75. For this purpose, we cycled over the group number g deterministically. The penalization parameters were set to roughly optimal values. For dropout, we used δ = 0.9 while from L2-penalization we used λ = 32."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "Dropout and other feature noising schemes control overfitting by artificially cor-<lb>rupting the training data. For generalized linear models, dropout performs a form<lb>of adaptive regularization. Using this viewpoint, we show that the dropout regular-<lb>izer is first-order equivalent to an L2 regularizer applied after scaling the features<lb>by an estimate of the inverse diagonal Fisher information matrix. We also establish<lb>a connection to AdaGrad, an online learning algorithm, and find that a close rel-<lb>ative of AdaGrad operates by repeatedly solving linear dropout-regularized prob-<lb>lems. By casting dropout as regularization, we develop a natural semi-supervised<lb>algorithm that uses unlabeled data to create a better adaptive regularizer. We ap-<lb>ply this idea to document classification tasks, and show that it consistently boosts<lb>the performance of dropout training, improving on state-of-the-art results on the<lb>IMDB reviews dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}