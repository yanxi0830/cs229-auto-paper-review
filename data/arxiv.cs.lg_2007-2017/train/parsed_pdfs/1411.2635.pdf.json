{
  "name" : "1411.2635.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A chain rule for the expected suprema of Gaussian processes",
    "authors" : [ "Andreas Maurer" ],
    "emails" : [ "am@andreas-maurer.eu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n26 35\nv1 [\ncs .L\nG ]\n1 0\nN ov"
    }, {
      "heading" : "1 Introduction",
      "text" : "Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.\nTo briefly describe the use of Gaussian averages (Rademacher averages will not concern us), let Y ⊆ Rn and let γ be a vector γ = (γ1, ..., γn) of independent standard normal variables. We define the (expected supremum of the) Gaussian average of Y as\nG (Y ) = E sup y∈Y\n〈γ,y〉 , (1)\nwhere 〈., .〉 denotes the inner product in Rn. Consider a loss class F of functions f : X → R, where X is some space of examples (such as input-output pairs), a sample x = (x1, ..., xn) ∈ Xn of observations and write F (x) for the subset of R\nn given by F (x) = {(f (x1) , ..., f (xn)) : f ∈ F}. Then we have the following result [1].\nTheorem 1. Let the members of F take values in [0, 1] and let X,X1, ..., Xn be iid random variables with values in X , X = (X1, ..., Xn). Then for δ > 0 with probability at least 1− δ we have for every f ∈ F that\nEf (X) ≤ 1 n ∑ f (Xi) +\n√ 2π\nn G (F (X)) +\n√\n9 ln 2/δ\n2n ,\nwhere the expectation in the definition (1) of G (F (X)) is conditional to the sample X.\n2 The utility of Gaussian averages is not limited to functions with values in [0, 1]. For real functions φ with Lipschitz constant L (φ) we have G ((φ ◦ F ) (x)) ≤ L (φ) G (F (x)) (see also Slepian’s Lemma, [6], [4]), where φ ◦ F is the class {x 7→ φ (f (x)) : f ∈ F}.\nThe inequality G ((φ ◦ F ) (x)) ≤ L (φ) G (F (x)), which in the above form holds also for Rademacher averages [10], is extremely useful and in part responsible for the success of these complexity measures. For Gaussian averages it holds in a more general sense: if φ : Rn → Rm has Lipschitz constant L (φ) with respect to the Euclidean distances, then G (φ (Y )) ≤ L (φ)G (Y ). This is a direct consequence of Slepian’s Lemma and can be applied to the analysis of clustering or learning to learn ([9] and [8]).\nBut what if we also want some freedom in the choice of φ after seeing the data? If the class of Lipschitz functions considered has small cardinality, a union bound can be used. If it is very large one can try to use covering numbers, but the matter soon becomes quite complicated and destroys the elegant simplicity of the method.\nThese considerations lead to a more general question: given a set Y ⊂ Rn and a class F of functions f : Rn→ Rm, how can we bound the Gaussian average G (F (Y )) = G ({f (y) : f ∈ F, y ∈ Y }) in terms of separate properties of Y and F , properties which should preferably very closely resemble Gaussian averages? If H is some class of functions mapping samples to Rn and Y = H (x), then the bound is on G (F (Y )) = G ((F ◦ H) (x)), so our question is relevant to the estimation of composite functions in general. Such estimates are necessary for multitask feature-learning, where H is a class of feature maps and F is vectorvalued, with components chosen independently for each task. Other potential applications are to the currently popular subject of deep learning, where we consider functional concatenations as in FM◦FM−1◦... ◦ F1.\nThe present paper gives a preliminary answer. To state it we introduce some notation. We will always take γ = (γ1, ...) to be a random vector whose components are independent standard normal variables, while ‖.‖ and 〈., .〉 denote norm and inner product in a Euclidean space, the dimension of which is determined by context, as is the dimension of the vector γ.\nDefinition 1. If Y ⊆ Rn we set\nD (Y ) = sup y,y′∈Y ‖y − y′‖ and G (Y ) = E sup y∈Y 〈γ,y〉 .\nIf F is a class of functions f : Y → Rm we set\nL (F, Y ) = sup y,y′∈Y, y 6=y′ sup f∈F ‖f (y) − f (y′)‖ ‖y − y′‖ and\nR (F, Y ) = sup y,y′∈Y, y 6=y′ E sup f∈F 〈γ, f (y) − f (y′)〉 ‖y − y′‖ .\n3 We also write F (Y ) = {f (y) : f ∈ F,y ∈ Y }. When there is no ambiguity we write L (F ) = L (F, Y ) and R (F ) = R (F, Y ).\nThen D (Y ) is the diameter of Y , and G (Y ) is the Gaussian average already introduced above. L (F ) is the smallest Lipschitz constant acceptable for all f ∈ F , and the more unusual quantity R (F ) can be viewed as a Gaussian average of Lipschitz quotients. In section 3.1 we give some properties of R (F ). Our main result is the following chain rule.\nTheorem 2. Let Y ⊂ Rn be finite, F a finite class of functions f : Y → Rm. Then there are universal constants C1 and C2 such that for any y0 ∈ Y\nG (F (Y )) ≤ C1L (F )G (Y ) + C2D (Y )R (F ) +G (F (y0)) . (2)\nWe make some general remarks on the implications of our result.\n1. The requirement of finiteness for Y and F is a simplification to avoid issues of measurability. The cardinality of these sets plays no role.\n2. The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand’s majorizing measure theorem and generic chaining [6][14][15][16]. This is a major shortcoming and the reason why our result is regarded as preliminary. Is there another proof of a similar result, avoiding majorizing measures and resulting in smaller constants? This question is the subject of current research.\n3. The first term on the right hand side of (2) describes the complexity inherited from the bottom layer Y (which we may think of as H (x)), and it depends on the top layer F only through the Lipschitz constant L (F ). The other two terms represent the complexity of the top layer, depending on the bottom layer only through the diameter D (Y ) of Y . If Y has unit diameter and the functions in F are contractions, then the two layers are completely decoupled in the bound. This decoupling is the most attractive property of our result.\n4. Apart from the large constants the inequality is tight in at least two situations: first, if Y = {y0} is a singleton, then only the last term remains, and we recover the Gaussian average of F (y0). This also shows that the last term cannot be eliminated. On the other hand if F consists of a single Lipschitz function φ, then we recover (up to a constant) the inequality G (φ (Y )) ≤ L (φ)G (Y ) above.\n5. The bound can be iterated to multiple layers by re-substitution of F (Y ) in place of Y . A corresponding formula is given in Section 3, where we also sketch applications to vector-valued function classes.\nThe next section gives a proof of Theorem 2, then we explain how our result can be applied to machine learning. The last section is devoted to the proof of a technical result encapsulating our use of majorizing measures.\n4"
    }, {
      "heading" : "2 Proving the chain rule",
      "text" : "To prove Theorem 2 we need the theory of majorizing measures and generic chaining. Our use of these techniques is summarized in the following theorem, which is also the origin of our large constants.\nTheorem 3. Let Xy be a random process indexed by a finite set Y ⊂ Rn. Suppose that there is a number K ≥ 1 such that for any distinct members y,y′ ∈ Y and any s > 0\nPr {Xy −Xy′ > s} ≤ K exp (\n−s2\n2 ‖y − y′‖2\n)\n(3)\nThen for any y0 ∈ Y\nE\n[\nsup y∈Y\nXy −Xy0 ] ≤ C′G (Y ) + C′′D (Y ) √ lnK,\nwhere C′ and C′′ are universal constants.\nThis is obtained from Talagrand’s majorizing measure theorem (Theorem 6 below) combined with generic chaining [16]. An early version of a similar result is Theorem 15 in [13], where the author remarks that his method of proof (which we also use) is very indirect, and that a more direct proof would be desirable. In Section 4 we do supply a proof, largely because the dependence on K, which can often be swept under the carpet, plays a crucial role in our arguments below.\nWe also need the following Gaussian concentration inequality (TsirelsonIbragimov-Sudakov inequality, Theorem 5.6 in [4]).\nTheorem 4. Let F : Rn → R be L-Lipschitz. Then for any s > 0\nPr {F (γ) > EF (γ) + s} ≤ e−s2/(2L2).\nTo conclude the preparation for the proof of Theorem 2 we give a simple lemma.\nLemma 1. Suppose a random variable X satisfies Pr {X −A > s} ≤ e−s2 , for any s > 0. Then\n∀s > 0 , Pr {X > s} ≤ eA2e−s2/2.\nProof. For s ≤ A the conclusion is trivial, so suppose that s > A. From s2 = (s−A+A)2 ≤ 2 (s−A)2 + 2A2 we get (s−A)2 ≥ ( s2/2 ) −A2, so\nPr {X > s} = Pr {X −A > s−A} ≤ e−(s−A)2 ≤ eA2e−s2/2.\n5 Proof (of Theorem 2). The result is trivial if F consists only of constants, so we can assume that L (F ) > 0. For y,y′ ∈ Y define a function F : Rm → R by\nF (z) = sup f∈F\n〈z, f (y) − f (y′)〉 .\nF is Lipschitz with Lipschitz constant bounded by supf∈F ‖f (y) − f (y′)‖ ≤ L (F ) ‖y − y′‖. Writing Zy,y′ = F (γ), it then follows from Gaussian concentration (Theorem 4) that\nPr {Zy,y′ > EZy,y′ + s} ≤ exp (\n−s2\n2L (F ) 2 ‖y − y′‖2\n)\n.\nSince by definition EZy,y′ ≤ R (F ) ‖y − y′‖, Lemma 1 gives\nPr {Zy,y′ > s} ≤ exp ( R (F ) 2\n2L (F ) 2\n)\nexp\n(\n−s2\n4L (F ) 2 ‖y − y′‖2\n)\n.\nNow define a process Xy, indexed by Y , as\nXy = 1√\n2L (F ) sup f∈F\n〈γ, f (y)〉 .\nSince Xy −Xy′ ≤ Zy,y′/ (√ 2L (F ) ) we have\nPr {Xy −Xy′ > s} ≤ Pr { Zy,y′ > √ 2L (F ) s }\n≤ exp ( R (F )2\n2L (F ) 2\n)\nexp\n(\n−s2\n2 ‖y − y′‖2\n)\nand by Theorem 3, with K = exp ( R (F ) 2 / ( 2L (F ) 2 )) ≥ 1,\nE sup y∈Y (Xy −Xy0) ≤ C′G (Y ) + C′′D (Y ) R (F )√ 2L (F ) .\nMultiplication by √ 2L (F ) then gives\nE sup y∈Y\n(\nsup f∈F 〈γ, f (y)〉 − sup f∈F\n〈γ, f (y0)〉 ) ≤ C1L (F )G (Y ) + C2D (Y )R (F )\nwith C1 = √ 2C′ and C2 = C ′′."
    }, {
      "heading" : "3 Applications",
      "text" : "We first give some elementary properties of the quantity R (F, Y ) which appears in Theorem 2. Then we apply Theorem 2 to a two layer kernel machine and give a bound for multi-task learning of low-dimensional representations.\n6"
    }, {
      "heading" : "3.1 Some properties of R (F )",
      "text" : "Recall the definition of R (F, Y ). If Y ⊆ Rnand F consists of functions f : Y → R m\nR (F, Y ) = sup y,y′∈Y, y 6=y′ E sup f∈F 〈γ, f (y)− f (y′)〉 ‖y − y′‖ .\nR (F ) is itself a supremum of Gaussian averages. For y,y′ ∈ Y let ∆F (y,y′) ⊆ R m be the set of quotients\n∆F (y,y′) =\n{\nf (y)− f (y′) ‖y − y′‖ : f ∈ F\n}\n.\nIt follows from the definition that R (F, Y ) = supy,y′∈Y, y 6=y′ G (∆F (y,y ′)). We record some simple properties. Recall that for a set S in a real vector space the convex hull Co (S) is defined as\nCo (S) =\n{\nn ∑\ni=1\nαizi : n ∈ N, zi ∈ S, αi ≥ 0, ∑\ni\nαi = 1\n}\n.\nTheorem 5. Let Y ⊆ Rn and let F and H be classes of functions f : Y → Rm. Then\n(i) If F ⊆ H then R (F, Y ) ≤ R (H, Y ). (ii) If Y ⊆ Y ′ then R (F, Y ) ≤ R (F, Y ′). (iii) If c ≥ 0 then R (cF, Y ) = cR (F, Y ) . (iv) R (F +H, Y ) ≤ R (F, Y ) +R (H, Y ). (v) R (F, Y ) = R (Co (F ) , Y ).\n(vi) If Z ⊆ RK and φ : Z → Rn has Lipschitz constant L (φ) and the members of F are defined on φ (Z), then R (F ◦ φ, Z) ≤ L (φ)R (F, φ (Z)).\n(vii) R (F ) ≤ L (F ) √ 2 ln |F |.\nRemarks:\n1. From (ii) we get R (F, Y ) ≤ R (F,Rn). In applications where Y = H (x) the quantity R (F,H (x)) is data-dependent, but R (F,Rn) is sometimes easier to bound.\n2. We see that the properties of R (F ) largely parallel the properties of the Gaussian averages themselves, except for the inequality G (φ (Y )) ≤ L (φ)G (Y ), for which there doesn’t seem to be an analogous property of R (F ). Instead we have a ’backwards’ version of it with (vi) above, with a rather trivial proof below.\n3. Of course (vii) is relevant only when ln |F | is reasonably small and serves the comparison of Theorem 2 to alternative bounds.\nProof. (i)-(iii) are obvious from the definition. (iv) follows from linearity of the inner product and the triangle inequality for the supremum. To see (v) first note that R (F ) ≤ R (Co (F )) follows from (i), while the reverse inequality follows\n7 from\nsup αi≥0, ∑ αi=1 sup f1,f2,...∈F\n〈\nγ, ∑\ni\nαifi (y)− ∑\ni\nαifi (y ′)\n〉\n= sup αi≥0, ∑ αi=1 sup f1,f2,...∈F\n∑\ni\nαi 〈γ, fi (y) − fi (y′)〉\n≤ sup αi≥0, ∑ αi=1\n∑\ni\nαi sup f∈F\n〈γ, f (y) − f (y′)〉\n= sup f∈F\n〈γ, f (y)− f (y′)〉 .\nFor (vi) we may chose y and y′ such that φ (y) 6= φ (y′), since otherwise both sides of the inequality to be proved are zero. But then\nE sup f∈F◦φ 〈γ, f (y) − f (y′)〉 ‖y − y′‖ = ‖φ (y) − φ (y′)‖ ‖y − y′‖ E supf∈F 〈γ, f (φ (y))− f (φ (y′))〉 ‖φ (y) − φ (y′)‖\n≤ L (φ)E sup f∈F 〈γ, f (φ (y))− f (φ (y′))〉 ‖φ (y) − φ (y′)‖ .\nTo see (vii) note that for every y and y′ and every f ∈ F it follows from Gaussian concentration (Theorem 4) that\nPr { 〈γ, f (y) − f (y′)〉 ‖y − y′‖ > s } ≤ e−s2/2L2 .\nThe conclusion then follows from standard estimates (e.g. [4], section 2.5)."
    }, {
      "heading" : "3.2 A double layer kernel machine",
      "text" : "We use the chain rule to bound the complexity of a double-layer kernel machine. The corresponding optimization problem is clearly non-convex and we are not aware of an efficient optimization method. The model is chosen to illustrate the application of Theorem 2. It is defined as follows.\nAssume the data to lie in Rm0 and fix two real numbers ∆1 and B1. On R m0 × Rm0 define a (Gaussian radial-basis-function) kernel κ by\nκ (z, z′) = exp\n(\n−‖z − z′‖2 2∆21\n)\n, z, z′ ∈ Rm0 ,\nand let φ : Rm0 → H be the associated feature map, where H is the associated RKHS with inner product 〈., .〉H and norm ‖.‖H (for kernel methods see . Now we let H be the class of vector valued functions h : Rm0 → Rm1 defined by\nH = {\nz ∈ Rm0 7→ (〈w1, φ (z)〉H , ..., 〈wm1 , φ (z)〉H) : ∑\nk\n‖wk‖2H ≤ B21\n}\n.\n8 This can also be written as H = {z ∈ Rm0 7→ Wφ (z) : ‖W‖HS ≤ B1}, where ‖W‖HS is the Hilbert-Schmidt norm of an operator W : H → Rm1 .\nFor the function class F , which we wish to compose with H, we proceed in a similar way, defining an analogous kernel of width ∆2 on R\nm1 × Rm1 , a corresponding feature map ψ : Rm1 → H and a class of real valued functions\nF = {z ∈ Rm1 7→ 〈v, ψ (z)〉H : ‖vl‖H ≤ B2} .\nWe now want high probability bounds on the estimation error for functional compositions f ◦ h, uniform over F ◦ H. To apply our result we should really restrict to finite subsets of F and H a requirement which we simply ignore. In machine learning we could of course always restrict all representations to some fixed, very high but finite precision.\nFix a sample x ∈ Rnm0 . Then Y = H (x) ⊂ Rnm1 . To use Theorem 2 we define a class F ′ of functions from Rnm1 to Rn by\nF ′ = {(y1, ..., yn) ∈ Rnm1 7→ (f (y1) , ..., f (yn)) ∈ Rn : f ∈ F} .\nSince the first feature map φ maps to the unit sphere of H we have\nD (H (x)) ≤ 2B1 √ n and\nG (H (x)) = E sup W\n∑\nik\nγik 〈wk, φ (xi)〉H ≤ B1 √ nm1.\nThe feature map corresponding to the Gaussian kernel∆2 has Lipschitz constant ∆−12 . For y,y ′ ∈ Rnm1 we obtain\nsup v\n(\n∑\ni\n(〈v, φ (yi)〉H − 〈v, φ (y′i)〉H) 2\n)1/2\n≤ B2 ( ∑\ni\n‖φ (yi)− φ (y′i)‖ 2 H\n)1/2\n≤ B2∆−12 ‖y − y′‖ ,\nso we have L (F ′,Rnm1) ≤ B2∆−12 . On the other hand\nE sup v\n∑\ni\nγi (〈v, φ (yi)〉H − 〈v, φ (y′i)〉H) ≤ B2E ∥ ∥ ∥ ∥\n∥\nn ∑\ni=1\nγi (φ (yi)− φ (y′i)) ∥ ∥ ∥ ∥\n∥\n≤ B2 ( ∑\ni\n‖φ (yi)− φ (y′i)‖ 2 H\n)1/2\n≤ B2∆−12 ‖y − y′‖ ,\nso we have R (F ′,Rnm1) ≤ B2∆−12 . Furthermore\nG (F ′ (h0 (x))) ≤ B2 √ n,\nsimilar to the bound for G (H (x)).\n9 For the composite network Theorem 2 gives us the bound\nG (F ′ (H (x))) ≤ C1B1B2∆−12 √ nm1 + 2C2B1B2 √ n∆−12 +B2 √ n.\nDividing by n and appealing to Theorem 1 one obtains the uniform bound: with probability at least 1− δ we have for every h ∈ H and every f ∈ F that\nEf (h (X)) ≤ 1 n ∑ f (h (Xi)) +\n+\n√\n2π\nn B2\n(\nB1∆ −1 2 (C1 √ m1 + 2C2) + 1 ) +\n√\n9 ln 2/δ\n2n .\nRemarks. 1. One might object that the result depends heavily on the intermediate dimension m1 so that only a very classical relationship between sample size and dimension is obtained. In this sense our result only works for intermediate representations of rather low dimension. The mapping stages ofH and F however include nonlinear maps to infinite dimensional spaces.\n2. Clearly the above choice of the Gaussian kernel is arbitrary. Any positive semidefinite kernel can be used for the first mapping stage, and the application of the chain rule requires only the Lipschitz property for the second kernel in the definition of F . The Gaussian kernel was only chosen for definiteness.\n3. Similarly the choice of the Hilbert-Schmidt norm as a regularizer for W in the first mapping stage is arbitrary, one could equally use another matrix norm. This would result in different bounds for G (H (x)) and D (H (x)), incurring a different dependency of our bound on m1."
    }, {
      "heading" : "3.3 Multitask learning",
      "text" : "As a second illustration we modify the above model to accommodate multitask learning [2][3]. Here one observes a T ×n sample x =(xti : 1 ≤ t ≤ T, 1 ≤ i ≤ n) ∈ XnT , where (xti : 1 ≤ i ≤ n) is the sample observed for the t-th task. We consider a two layer situation where the bottom-layer H consists of functions h : X → Rm, and the top layer function class is of the form\nFT = { x ∈ Rm1 7→ f (x) = (f1 (x) , ..., fT (x)) ∈ RT : ft ∈ F } ,\nwhere F is some class of functions mapping Rm1 to R. The functions (or representations) of the bottom layer H are optimized for the entire sample, in the top layer each function ft is optimized for the represented data corresponding to the t-th task. In an approach of empirical risk minimization one selects the composed function f̂ ◦ ĥ which minimizes the task-averaged empirical loss\nmin f∈Fn,h∈H\n1\nnT\nn ∑\ni=1\nT ∑\nt=1\nft (h (xit)) .\nWe wish to give a general explanation of the potential benefits of this method over the separate learning of functions from F ◦ H, as studied in the previous\n10\nsection. Clearly we must assume that the tasks are related in the sense that the above minimum is small, so any possible benefit can only be a benefit of improved estimation.\nFor the multitask model a result analogous to Theorem 1 is easily obtained (see e.g. [7]). Let X =(Xti) be a vector of independent random variables with values in X , where Xti is iid to Xtj for all ijt, and let Xt be iid to Xti. Then with probability at least 1− δ we have for every f ∈ Fn and every h ∈ H\n1\nT\n∑\nt\nEft (h (Xt)) ≤ 1\nnT\n∑\nti\nft (h (Xti)) +\n√ 2π\nnT G ( FT ◦ H (X) ) +\n√\n9 ln 2/δ\n2nT .\nHere the left hand side is interpreted as the task averaged risk and\nG ( FT ◦ H (x) )\n= E sup f∈FT ,h∈H\n∑\nti\nγtift (h (xti)) .\nFor a definite example we take H and F as in the previous section and observe that now there is an additional factor T on the sample size. This implies the modified bounds G (H (x)) ≤ B1 √ Tnm1 and D (H (x)) ≤ 2B1 √ Tn. Also for y,y′ ∈ RTnm1 with yti, y′ti ∈ Rm1\nsup f∈FT\n∑\nti\n(ft (yti)− ft (y′ti)) 2 ≤\n∑\nt\nsup f∈F\n∑\ni\n(ft (yti)− ft (y′ti)) 2\n≤ L2 (F,Rnm1 ) ∑\nt\n∑\ni\n‖yti − y′ti‖ 2 ,\nso L ( FT ,RTnm1 ) = L (F,Rnm1) . (4)\nTherefore L ( FT ,RTnm1 ) ≤ B2∆−12 . Similarly\nE sup f∈FT\n∑\nti\nγti (ft (yti)− ft (y′ti))\n= ∑\nt\nE sup f∈F\n∑\ni\nγti (ft (yti)− ft (y′ti))\n≤ √ T\n\n\n∑\nt\n(\nE sup f∈F\n∑\ni\nγti (ft (yti)− ft (y′ti)) )2\n\n\n1/2\n≤ √ T\n(\n∑\nt\nR2 (F,Rnm1) ∑\ni\n‖yti − y′ti‖ 2\n)1/2\n= √ TR (F,Rnm1) ‖y − y′‖ .\nWe conclude that R ( FT ,RnmT ) ≤ √ TR (F,Rnm) , (5)\nin the given case R ( FT ,RnmT ) ≤ √ TB2∆ −1 2 .\n11\nAlso\nG ( FT (h0 (x)) )\n= E sup f∈FT\n∑\nti\nγtift (h0 (xti))\n= ∑\nt\nE sup f∈F\n∑\ni\nγtif (h0 (xti))\n≤ TG (F (h0 (x))) , (6)\nso that here G ( FT (h0 (x)) ) ≤ B2T √ n. The chain rule then gives\nG (F ◦ H (x)) ≤ C1B1B2∆−12 √ Tnm1 + ( 2C2B1∆ −1 2 + 1 ) B2T √ n,\nwhere the first term represents the complexity of H and the second that of FT . Dividing by nT we obtain as the dominant term for the estimation error\nC1B1B2∆ −1 2\n√\nm1 nT +\n(\n2C2B1∆ −1 2 + 1\n)\nB2√ n .\nThis reproduces a general property of multitask learning [3]: in the limit T → ∞ the contribution of the common representation (including the intermediate dimension m1) to the estimation error vanishes. There remains only the cost of estimating the task specific functions in the top layer.\nWe have obtained this result for a very specific model. The relations (4), (5) and (6) for L ( FT ) , R ( FT ) and G ( FT (h0 (x)) )\nare nevertheless independent of the exact model, so the chain rule could be made the basis of a fairly general result about multitask feature learning."
    }, {
      "heading" : "3.4 Iteration of the bound",
      "text" : "We apply the chain rule to multi-layered or ”deep” learning machines, a subject which appears to be of some current interest. Here we have function classes F1, ..., FK , where Fk consists of functions f : R\nnk−1 → Rnk and we are interested in the generalization properties of the composite class\nFK ◦ ... ◦ F1 = {x ∈ Rn0 7→ fK (fK−1 (... (f1 (x)))) : fk ∈ Fk} .\nTo state our bound we are given some sample x in Rn0 and introduce the notation\nY0 = x\nYk = Fk (Yk−1) = Fk ◦ ... ◦ F1 (x) ⊆ Rnk , for k > 0 Gk = min\ny∈Yk−1(x) G (Fk (y)) .\nUnder the convention that the product over an empty index set is 1, induction shows that\nG (YK) ≤ K ∑\nk=1\n\nCK−k1\nK ∏\nj=k+1\nL (Fj)\n\n (C2D (Yk−1)R (Fk) +Gk) .\n12\nClearly the large constants are prohibitive for any useful quantitative prediction of generalization, but qualitative statements are possible. Observe for example that, apart from C1 and the Lipschitz constants, each layer only makes an additive contribution to the overall complexity. More specifically, for machine learning with a sample of size n, we can make the assumptions nk = nmk, where mk is the dimension of the k-th intermediate representations, and it is reasonable to postulate max {Gk, D (Yk)R (Fk)} ≤ Cnp, where C is some constant not depending on n and p is some exponent p < 1 (for multi-layered kernel machines with Lipschitz feature maps we would have p = 1/2 - see above). Then the above expression is of order np and Theorem 1 yields a uniform law of large numbers for the multi-layered class, with a uniform bound on the estimation error decreasing as np−1."
    }, {
      "heading" : "4 Proof of Theorem 3",
      "text" : "Talagrand has proved the following result ([14]).\nTheorem 6. There are universal constants r ≥ 2 and C such that for every finite Y ⊂ Rn there is an increasing sequence of partitions Ak of Y and a probability measure µ on Y , such that, whenever A ∈ Ak then D (A) ≤ 2r−k and\nsup y∈Y\n∞ ∑\nk>k0\nr−k\n√\nln 1\nµ (Ak (y)) ≤ C G (Y ) ,\nwhere Ak (y) denotes the unique member of Ak which contains y, and k0 is the largest integer k satisfying\n2r−k ≥ D (Y ) = sup y,y′∈Y ‖y − y′‖\nObserve that 2r−k0 ≥ D (Y ), so we can assume Ak0 = {Y }. As explained in [14], the above Theorem is equivalent to the existence of a measure µ on Y such that\nsup y∈Y\n∫ ∞\n0\n√\nln 1\nµ (B (y,ǫ)) dǫ ≤ C G (Y ) ,\nwhere C is some other universal constant and B (y,ǫ) is the ball of radius ǫ centered at y. The latter is perhaps the more usual formulation of the majorizing measure theorem.\nWe will use Talagrand’s theorem to prove Theorem 3, but before please note the inequality\nD (Y ) ≤ √ 2πG (Y ) , (7)\nwhich follows from\nsup y,y′∈Y\n‖y − y′‖ = √ π\n2 sup y,y′\nE |〈γ,y − y′〉|\n≤ √ π\n2 E sup y,y′ |〈γ,y − y′〉| =\n√\nπ 2 E sup y,y′ 〈γ,y − y′〉 .\n13\nIn the first equality we used the fact that ‖v‖ = √ π/2E |〈γ, v〉| for any vector v.\nProof (of Theorem 3.). Let µ and Ak be as determined for Y by Theorem 6. First we claim that for any δ ∈ (0, 1)\nPr\n{\n∃y ∈ Y : Xy −Xy0 > ∑\nk>k0\nr−k+1\n√\n8 ln\n(\n2k−k0K\nµ (A (y)) δ\n)\n}\n< δ. (8)\nFor every k > k0 and every A ∈ Ak let π (A) be some element chosen from A. We set π (Y ) = y0. We denote πk (y) = π (Ak (y)). This implies the chaining identity:\nXy −Xy0 = ∑\nk>k0\n( Xπk(y) −Xπk−1(y) ) .\nFor k > k0 and A ∈ Ak use Â to denote the unique member of Ak−1 such that A ⊆ Â. Since for A ∈ Ak both π (A) and π ( Â )\nare members of Â ∈ Ak−1 we must have ∥ ∥ ∥π (A)− π ( Â )∥ ∥ ∥ ≤ 2r−k+1. Also note πk−1 (y) = π ( Âk (y) ) = π ((Ak (πk (y))) ˆ). For k ≥ k0 we define a function ξk : Ak → R+ as follows:\nξk (A) = r −k+1\n√\n8 ln\n(\n2k−k0K\nµ (A) δ\n)\n.\nTo prove the claim we have to show that\nPr\n{\n∃y ∈ Y : Xy −Xy0 − ∑\nk>k0\nξk (Ak (y)) > 0\n}\n< δ.\nDenote the left hand side of this inequality with P . By the chaining identity\nP ≤ Pr { ∃y : ∑\nk>k0\n( Xπk(y) −Xπk−1(y) − ξk (Ak (y)) ) > 0\n}\n.\nIf the sum is positive, at least one of the terms has to be positive, so\nP ≤ Pr { ∃y, k > k0 : ( Xπk(y) −Xπk−1(y) − ξk (Ak (y)) ) > 0 } .\nThe event on the right hand side can also be written as\n{ ∃k > k0, ∃A ∈ Ak : Xπ(A) −Xπ(Â) > ξk (A) } ,\n14\nand a union bound gives\nP ≤ ∑\nk>k0\n∑\nA∈Ak\nPr { Xπ(A) −Xπ(Â) > ξk (A) }\n≤ ∑\nk>k0\n∑\nA∈Ak\nK exp\n\n \n−ξk (A)2\n2 ∥ ∥ ∥π (A)− π ( Â )∥ ∥ ∥ 2\n\n \n≤ ∑\nk>k0\n∑\nA∈Ak\nK exp\n(\n−ξk (A) 2\n2 (2r−k+1) 2\n)\n,\nwhere we used the bound (3) in the second and the bound on ∥ ∥ ∥π (A)− π ( Â )∥ ∥ ∥\nin the third inequality. Using the definition of ξk (A) the last expression is equal to\nδ ∑\nk>k0\n1\n2k−k0\n∑\nA∈Ak\nµ (A) = δ ∑\nk>k0\n1\n2k−k0 = δ,\nbecause µ is a probability measure. This establishes the claim. Now, using √ a+ b ≤ √a+ √ b for a, b ≥ 0, with probability at least 1− δ\nsup y\nXy −Xy0 ≤ r ∑\nk>k0\nr−k\n√\n8 ln\n(\n1\nµ (Ak (y))\n)\n+ r−k0+1 ∑\nk>0\nr−k+1\n√\n8 ln\n(\n2kK\nδ\n)\n≤ √ 8rC G (Y ) + √ 8r−k0+1 ∑\nk>0\n√ kr−k+1\n√\nln\n(\n2K\nδ\n)\n,\nwhere we used Talagrand’s theorem and the fact that K > 1. By the definition of k0 we have r −k0+1 ≤ r2D (Y ) /2, so this is bounded by\nC′′′G (Y ) + C′′′′D (Y )\n√\nln\n(\n2K\nδ\n)\n,\nwith C′′′ = √ 8rC and C′′′′ = √ 8 ( r2/2 ) ∑\nk>0\n√ kr−k+1. Converting the last\nbound into a tail bound and integrating we obtain\nE\n[\nsup y\nXy −Xy0 ] ≤ C′′′G (Y ) + C′′′′D (Y ) (√ ln 2K +\n√ π\n2\n)\n≤ C′′′G (Y ) + 3C′′′′D (Y ) √ ln 2K ≤ ( C′′′ + 3 √ 2π ln 2C′′′′ ) G (Y ) + 3C′′′′D (Y ) √ lnK,\nwhere we again usedK ≥ 1 in the second inequality and (7) in the last inequality. This gives the conclusion with C′ = C′′′ + 3 √ 2π ln 2C′′′′ and C′′ = 3C′′′′.\n15"
    } ],
    "references" : [ {
      "title" : "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research, 3: 463– 482,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Theoretical Models of Learning to Learn, in Learning to Learn, S.Thrun, L.Pratt Eds",
      "author" : [ "J. Baxter" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1998
    }, {
      "title" : "A Model of Inductive Bias Learning",
      "author" : [ "J. Baxter" ],
      "venue" : "Journal of Artificial Intelligence Research 12:149–198,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2000
    }, {
      "title" : "Concentration Inequalities",
      "author" : [ "S. Boucheron", "G. Lugosi", "P. Massart" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Rademacher processes and bounding the risk of function learning",
      "author" : [ "V.I. Koltchinskii", "D. Panchenko" ],
      "venue" : "In E. Gine, D. Mason, and J. Wellner, editors, High Dimensional Probability II, pages 443–459.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "M",
      "author" : [ "M. Ledoux" ],
      "venue" : "Talagrand. Probability in Banach Spaces, Springer,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Bounds for linear multi-task learning",
      "author" : [ "A. Maurer" ],
      "venue" : "Journal of Machine Learning Research, 7:117–139,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Transfer bounds for linear feature learning",
      "author" : [ "A. Maurer" ],
      "venue" : "Machine Learning, 75(3): 327–350,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "K-dimensional coding schemes in Hilbert spaces",
      "author" : [ "A. Maurer", "M. Pontil" ],
      "venue" : "IEEE Transactions on Information Theory, 56(11): 5839–5846,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Generalization error bounds for Bayesian mixture algorithms",
      "author" : [ "R. Meir", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research, 4: 839–860,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "l -norm and its application to learning theory",
      "author" : [ "S. Mendelson" ],
      "venue" : "Positivity, 5:177–191,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Regularity of Gaussian processes",
      "author" : [ "M. Talagrand" ],
      "venue" : "Acta Mathematica. 159: 99–149,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "A simple proof of the majorizing measure theorem",
      "author" : [ "M. Talagrand" ],
      "venue" : "Geometric and Functional Analysis. Vol 2, No.1: 118–125,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Majorizing measures without measures",
      "author" : [ "M. Talagrand" ],
      "venue" : "Ann. Probab. 29: 411–417,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The Generic Chaining",
      "author" : [ "M. Talagrand" ],
      "venue" : "Upper and Lower Bounds for Stochastic Processes. Springer, Berlin,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "Then we have the following result [1].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Let the members of F take values in [0, 1] and let X,X1, .",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "2 The utility of Gaussian averages is not limited to functions with values in [0, 1].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "For real functions φ with Lipschitz constant L (φ) we have G ((φ ◦ F ) (x)) ≤ L (φ) G (F (x)) (see also Slepian’s Lemma, [6], [4]), where φ ◦ F is the class {x 7→ φ (f (x)) : f ∈ F}.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "For real functions φ with Lipschitz constant L (φ) we have G ((φ ◦ F ) (x)) ≤ L (φ) G (F (x)) (see also Slepian’s Lemma, [6], [4]), where φ ◦ F is the class {x 7→ φ (f (x)) : f ∈ F}.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "The inequality G ((φ ◦ F ) (x)) ≤ L (φ) G (F (x)), which in the above form holds also for Rademacher averages [10], is extremely useful and in part responsible for the success of these complexity measures.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "This is a direct consequence of Slepian’s Lemma and can be applied to the analysis of clustering or learning to learn ([9] and [8]).",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "This is a direct consequence of Slepian’s Lemma and can be applied to the analysis of clustering or learning to learn ([9] and [8]).",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand’s majorizing measure theorem and generic chaining [6][14][15][16].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand’s majorizing measure theorem and generic chaining [6][14][15][16].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand’s majorizing measure theorem and generic chaining [6][14][15][16].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 15,
      "context" : "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand’s majorizing measure theorem and generic chaining [6][14][15][16].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 15,
      "context" : "This is obtained from Talagrand’s majorizing measure theorem (Theorem 6 below) combined with generic chaining [16].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "An early version of a similar result is Theorem 15 in [13], where the author remarks that his method of proof (which we also use) is very indirect, and that a more direct proof would be desirable.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "6 in [4]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : "[4], section 2.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "3 Multitask learning As a second illustration we modify the above model to accommodate multitask learning [2][3].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "3 Multitask learning As a second illustration we modify the above model to accommodate multitask learning [2][3].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "[7]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "This reproduces a general property of multitask learning [3]: in the limit T → ∞ the contribution of the common representation (including the intermediate dimension m1) to the estimation error vanishes.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "4 Proof of Theorem 3 Talagrand has proved the following result ([14]).",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "As explained in [14], the above Theorem is equivalent to the existence of a measure μ on Y such that sup y∈Y ∫ ∞ 0 √",
      "startOffset" : 16,
      "endOffset" : 20
    } ],
    "year" : 2014,
    "abstractText" : "The expected supremum of a Gaussian process indexed by the image of an index set under a function class is bounded in terms of separate properties of the index set and the function class. The bound is relevant to the estimation of nonlinear transformations or the analysis of learning algorithms whenever hypotheses are chosen from composite classes, as is the case for multi-layer models.",
    "creator" : "LaTeX with hyperref package"
  }
}