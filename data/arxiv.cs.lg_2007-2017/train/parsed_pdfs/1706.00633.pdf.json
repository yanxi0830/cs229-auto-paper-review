{
  "name" : "1706.00633.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test",
    "authors" : [ "Tianyu Pang", "Chao Du", "Jun Zhu" ],
    "emails" : [ "{pangty13@mails,", "du-c14@mails,", "dcszj@mail}.tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep learning (DL) has obtained substantial progress in many tasks, including image categorization, speech recognition, and natural language processing [3]. However, a high-accuracy DL model can be vulnerable in the adversarial setting [4, 26], where adversarial samples are carefully crafted to mislead the model to wrong predictions. Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22]. As DL is becoming ever more prevalent, it is vital to improve the robustness, especially in critical domains (e.g., self-driving cars, healthcare and finance).\nVarious defense strategies have been proposed to improve the robustness of DL. The adversarial training strategy [26] dynamically augments the training set with crafted adversarial samples. Though it promises an efficient defense on some specific and known attacking methods, it requires extra computational cost and is not a general strategy for unknown attacking methods. [5] considers adding a regularization term on the norm of gradients in the objective function, but it does not promise anything to gradient sign attacking methods that only require the sign of gradients. The defensive distillation method [23] trains a model with a high temperature and tests it with a low temperature. It can work well, but it is heuristic and needs to tune the training temperature. Furthermore, this defensive distillation method can be easily attacked by new attacking methods [1]. Overall, as adversarial samples always exist for a fixed parametric model (thus a fixed decision boundary), it is unlikely for such methods to solve the problem by preventing attackers from crafting adversarial samples, no matter how hard to find them.\nIn this paper, we attempt to improve the robustness of DL from a new angle. Instead of preventing adversarial samples, we try to distinguish adversarial samples from normal ones, so that we can filter out the adversarial ones for robust predictions. We make contributions by presenting a new method that consists of a novel training procedure and a thresholding test strategy. For training, we propose to minimize a new objective function, named as reverse cross-entropy (RCE), instead of minimizing\n∗Corresponding author.\nar X\niv :1\n70 6.\n00 63\n3v 1\n[ cs\n.L G\n] 2\nJ un\n2 01\nthe common cross-entropy (CE) loss [3]. By minimizing RCE, our training procedure encourages the DL classifiers to map the normal samples to the neighborhoods of low dimensional manifolds in the hidden space, and it also encourages the classifiers to return a high confidence on the true class while a uniform distribution on false classes for each data point. The minimization of RCE is simple to implement using stochastic gradient descent methods, and introduces little extra training cost, as compared to CE. Therefore, it can be easily applied to any deep neural networks.\nFor testing, we propose a thresholding strategy based on a new compositive metric of J-score, which simultaneously measures the degree of certainty about the given input being a normal sample and the probability of belonging to a selected class. By setting a proper threshold on the J-score, we can filter out adversarial samples for robust predictions, namely, we make the classifiers return meaningful predictions only when the J-score is higher than a given threshold and refuse to predict otherwise.\nWe apply our method to improve the state-of-the-art residual neural networks (Resnets) [6] on the widely used MNIST [13] and CIFAR-10 [11] datasets. The results show that we improve the robustness on adversarial samples while maintaining state-of-the-art accuracy on normal samples. Finally, we note that recent work [14] shows that a Bayesian neural network (BNN) can improve the robustness of DNN against adversarial samples. Our method is much simpler and computationally cheaper than BNNs which need to do expensive posterior inference. Moreover, our quantitative results demonstrate a better performance than BNNs on filtering adversarial samples."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Neural networks",
      "text" : "A deep neural network (DNN) classifier can be generally expressed as a mapping function F (X, θ) : Rd → RL, where X ∈ Rd is the input variable, θ denotes all the parameters and L is the number of classes. Here, we focus on the DNNs with softmax output layers. For notation clarity, we define the softmax function softmax(z) : RL → RL as softmax(z)i = exp(zi)/ ∑L i=1 exp(zi), i ∈ [L], where [L] := {1, · · · , L}. Let Zf be the output vector of the final hidden layer. This defines a mapping function: X → Zf to extract good representations. Then, the classifier can be expressed as F (X, θ) = softmax(WfZf + bf ), where Wf and bf are the weight matrix and bias vector of the softmax layer respectively. We denote Z(X, θ) := WfZf + bf as the logits. We can easily see that the softmax classifier has a piecewise linear decision boundary in the final hidden space Zf . Given an input x (i.e., an instance of X), the output of the classifier is a L-dimensional probability vector, with each element F (x, θ)i being the probability that x belongs to class i. The predicted label for x is denoted as t̂(x, θ) = arg maxi F (x, θ)i. The probability value F (x, θ)t̂(x,θ) is often used as the corresponding confidence score on this prediction [3], although it is insufficient as we shall see.\nLet D := {(xi, yi)}i∈[N ] be a training set with N input-label pairs, where yi ∈ RL is an one-hot vector indicating the true class of xi, i.e., yij = 1 if xi belongs to class j; 0 otherwise. One common training objective is to minimize the cross-entropy (CE) loss, which is defined as:\nLCE(x, y) = −y · logF (x, θ) = − logF (x, θ)t,\nfor a single pair (x, y), where t is the class of x. Then the CE training procedure intends to minimize the average CE loss to obtain the optimal parameters θ∗ = arg minθ 1 N ∑ i∈[N ] LCE(xi, yi), which can be efficiently done by stochastic gradient methods with back-propagation [10, 24]."
    }, {
      "heading" : "2.2 Adversarial sample crafting and robustness",
      "text" : "Though DNN has obtained substantial progress, adversarial samples can be easily identified to fool the network, even when its accuracy is high [19]. We consider the white-box attack, where attackers know everything about the target classifier F (X, θ) including the architecture, parameters, objective function and training tools. This setting is interesting because if we can effectively defend adversarial samples under the white-box attack, we can do at least as well in other adversarial settings where attackers have less knowledge on the target classifier. In the white-box setting, attackers can design different algorithms to craft adversarial samples based on their knowledge. In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23]. Specifically, let x be a normal input. FGSM crafts an adversarial sample x∗ by forcing the target model to classify x∗ into a class different from that of x. This is achieved by running the update: x∗ ← x+ ε · sgn(∇xJ(F, θ, x)), where J(F, θ, x) is the objective function to\ntrain the target classifier and ε controls the magnitude of perturbation on normal inputs. Moreover, to evaluate the ability of a classifier to resist adversarial perturbation, we define the robustness ∆(x; t̂) according to [17] as ∆(x; t̂) := min\nr ‖r‖2 subject to t̂(x+ r) 6= t̂(x), where r is the perturbation."
    }, {
      "heading" : "3 Algorithms",
      "text" : "In this section, we present a new algorithm to improve the robustness against adversarial samples. We first provide some new insights on why adversarial samples exist and how can we distinguish them. These insights guide us to a new algorithm."
    }, {
      "heading" : "3.1 Why adversarial samples exist",
      "text" : "Previous work [4, 22] hypothesizes that the existence of adversarial samples is because of certain defects in the training phase. One main support is from the universal approximator theorem [8] that DNNs are able to represent functions that resist adversarial samples. However, in practice any given DNN has an architecture of limited scale, which results in a limited representation capability, and the existence of adversarial samples is intrinsic, as explained below.\nFor a given DNN classifier, its decision boundary is fixed. Then, any pair of similar instances that are located on different sides of a decision boundary will be classified into different classes. Very often, such a pair of input points are not distinguishable by human observers; thus, it sounds counterintuitive and irrational to have a jump on the predicted labels. The previous defense attempts on adjusting the training phase, e.g., by adding regularizers in the objective function [5] or augmenting the training dataset [26], will only result in the change of the distribution of decision boundaries but the jump on the predicted labels nearby the decision boundary still exists. Therefore, a smart enough attacker can always find adversarial samples nearby the decision boundaries to successfully attack the target classifier no matter how the decision boundaries change, as has been demonstrated in [26].\nIn consideration of the intrinsic existence of adversarial samples, instead of designing defense strategies for the classifiers to prevent attackers from crafting adversarial samples, we design a defense strategy to help the classifiers to distinguish adversarial samples from normal ones so as to filter adversarial samples out for robust predictions."
    }, {
      "heading" : "3.2 The insufficiency of confidence and a new metric of non-ME",
      "text" : "We need some metrics to distinguish whether an input x is adversarial or not for a given classifier F (X, θ). A potential candidate is the confidence F (x, θ)t̂(x,θ) on the predicted label t̂(x, θ), which has in fact been widely used [3]. As F predicts incorrectly on adversarial samples, intuitively we should have a higher confidence on a normal sample than that on an adversarial sample, which consequently allows us to distinguish them by the confidence values. However, it has been shown that a well-trained DNN classifier usually not only misclassifies adversarial samples but also gives high confidence on its predictions [4, 19], which renders the confidence unreliable in the adversarial setting. This is because in the normal setting without attackers, the distribution of test data is similar with that of the training data, then a high confidence implies a high probability to be a correct prediction. In the adversarial setting, attackers can explore the points far from the data distribution [14], and the predictions in these domains mostly result from interpolation with high uncertainty. Therefore, a high confidence returned in these domains is unreliable and probably a false positive.\nTo avoid the unreliability of using confidence as a single metric in the adversarial setting, we construct another metric which is more pertinent and helpful to our goal. Namely, we define the metric of non-ME—the entropy of normalized non-maximal elements in F (x, θ), as:\nnon-ME(x, θ) = − ∑\ni6=t̂(x,θ)\nF̂ (x, θ)i log(F̂ (x, θ)i), (1)\nwhere F̂ (x, θ)i = F (x,θ)i∑\nj 6=t̂(x,θ) F (x,θ)j are the normalized non-maximal elements in F (x, θ).\nIt is easy to show that non-ME(x, θ) ≤ log(L − 1) and the equality holds if and only if all the non-maximal elements in F (x, θ) are equal, independent of the confidence score. Therefore, the non-ME metric conveys certain information in F (x, θ) that is ignored by the confidence. Note that for piecewise linear classifiers (e.g., DNNs with a softmax output layer), any input point that locates on the oppositely elongated decision boundaries has at least two equal non-maximal elements in F (x, θ). Therefore, any input point that has a high value of non-ME(x, θ) and (L−1) approximately\nequal non-maximal elements in F (x, θ) must locate nearby the junction manifolds of at least (L− 2) oppositely elongated decision boundaries.\nFor a DNN classifier F (x, θ) = softmax(Wfzf + bf ), if the learned representation transformation: x→ zf can map the normal inputs to the neighborhoods of oppositely elongated decision boundaries in the final hidden space, then using the two metrics of confidence and non-ME will improve the robustness in the final-layer hidden space and further result in an improvement in the input space. To illustrate this idea and show the effect of non-ME, Fig. 1 presents an example in the final hidden space, where zf ∈ R2 and L = 3. In this case, all the points that locate on any one oppositely elongated decision boundary have two equal non-maximal elements in F (x, θ), thus have highest values of non-ME(x, θ) = log 2. A good way to use non-ME to filter out adversarial samples is to enforce all the normal points have non-ME values higher than a threshold l, which indicates that the normal instances are located in the neighborhood of oppositely elongated decision boundaries in the final hidden space. Therefore, given any unidentified input (i.e., adversarial sample), if its non-ME value is less than l the classifier will immediately know that this input is not normal and filters it out.\nAs shown in Fig. 1, zf,0 is a normal instance located in the neighborhood of the oppositely elongated decision boundaries, where the neighborhood boundaries are the isolines of non-ME = l in the correspond decision domains. Let zf,0 be the original input, from which an attacker tries to craft adversarial samples. When confidence is the only metric, the nearest successful adversarial sample zf,1 locates on the nearest decision boundary w.r.t zf,0. In contrast, when both confidence and non-ME are used, zf,1 will be easily filtered out by the classifier because non-ME(zf,1, θ) is less than\nl, and the nearest successful adversarial sample zf,2 in this case locates on the nearest junction manifolds of neighborhood boundaries and decision boundaries w.r.t zf,0. It is easy to find that the minimal perturbation ‖zf,0 − zf,2‖2 is larger than ‖zf,0 − zf,1‖2 (equal on a zero measure set in the input space), which means that using two metrics can improve robustness ∆(zf,0; t̂), a.e. Besides, higher values of the non-ME threshold l can lead to larger minimal perturbation ‖zf,0 − zf,2‖2 and better robustness ∆(zf,0; t̂). It is easy to prove that these conclusions are tenable in more general cases of different values of L and different dimensions of the final hidden space."
    }, {
      "heading" : "3.3 Reverse cross-entropy",
      "text" : "Based on the above analysis, we now design a new training objective to improve the robustness of DNN classifiers. The key is to enforce a DNN classifier to map all the normal instances to the neighborhoods of oppositely elongated decision boundaries in the final hidden space. This can be achieved by letting the non-maximal elements of F (x, θ) be as equal as possible and have high non-ME values for all the normal inputs. Specifically, for a training data (x, y), where t is the index of the true label, we let yR denote its reverse label vector whose tth element is zero and other elements equal to 1L−1 . One obvious way to achieve the above goal is to add a cross-entropy term between yR and F (x, θ) in the traditional cross-entropy objective:\nLλCE(x, y) = LCE(x, y)− λyR · logF (x, θ) = − logF (x, θ)t − λ L− 1 ∑ i 6=t logF (x, θ)i, (2) where λ is a trade-off parameter. However, it is easy to show that minimizing LλCE equals to minimizing the cross-entropy between F (x, θ) and the N -dimensional probability vector Pλ:\nPλi =\n{ 1\nλ+1 i = t λ\n(L−1)(λ+1) i 6= t. (3)\nNote that there are y = P 0 and yR = P∞. Let θ∗λ = arg minθ LλCE , then the vector F (x, θ∗λ) will tend to equal to Pλ, rather than the ground-truth y. This makes the estimate of θ by minimizing LλCE be biased. In order to have an unbiased estimator of θ meeting the requirements that simultaneously make the output vector F (x, θ) tend to y and encourage uniformity among probabilities on untrue classes, we define another objective function based on what we call reverse cross-entropy (RCE) as\nLRCE(x, y) = −yR · logF (x, θ) = − 1 L− 1 ∑ i 6=t logF (x, θ)i. (4)\nMinimizing the RCE is equivalent to minimizing L∞CE . Note that by directly minimizing LRCE , i.e., θ∗R = arg minθ LRCE , one will get a ‘liar’ classifier F (X, θ∗R), which means that given an input x, the ‘liar’ classifier F (X, θ∗R) will not only tend to conceal the index of true class by assigning a low probability to it but also tend to output a uniform distribution on other classes."
    }, {
      "heading" : "3.4 The reverse cross-entropy training procedure",
      "text" : "It is easy to know the index that F (X, θ∗R) wants to conceal the most (i.e., assign the lowest probability) is actually the true label. This simple insight leads to our RCE training procedure which consists of two parts, as outlined below:\nReverse training: Given the training set D, we train the DNN F (X, θ) to be a ’liar’ classifier by minimizing the average RCE loss: θ∗R = arg minθ 1 N ∑N d=1 LRCE(xd, yd).\nReverse logits: We negate the final logits fed to the softmax layer to calculate the output predictions as FR(X, θ∗R) = softmax(−Z(X, θ∗R)). Then we will obtain the reverse ’liar’ network FR(X, θ∗R) that returns normal distribution on classes. Theorem 1 demonstrates two important properties of the RCE training procedure. First, it is consistent and unbiased in the sense that when the training error α→ 0, the output FR(x, θ∗R) converges to the one-hot label vector y . Second, the upper bounds of the difference between any two non-maximal elements in outputs decrease much faster in the RCE training than in the CE training w.r.t the training error α, where the upper bounds decrease as O(α2(L− 1)2) for RCE while O(α) for CE. These two properties make the RCE training procedure meet our requirements as described above. Theorem 1. (Proof in Appendix) Let x be a given input in the training dataset, y be the one-hot label vector, yR be the reverse label vector corresponding to y, L be the number of different classes and θ∗R is obtained from the reverse training. Under the L∞-norm, if there is a training error α 1L that ‖softmax(Z(x, θ∗R))− yR‖∞ ≤ α. Then we have bounds ‖softmax(−Z(x, θ∗R))− y‖∞ ≤ α(L− 1) 2 and ∀j, k 6= t\n|softmax(−Z(x, θ∗R))j − softmax(−Z(x, θ∗R))k| ≤ 2α2(L− 1)2, where t is the index of true label."
    }, {
      "heading" : "3.5 The thresholding test procedure",
      "text" : "Given a trained reverse ‘liar’ classifier FR(X, θ∗R), we present a thresholding test procedure for robust prediction. The test procedure consists of three key steps, as explained below.\nCalculate the predicted label and confidence: Given a testing input x, we first calculate the output vector FR(x, θ∗R) and then assign the predicted label of x to be t̂ = arg maxi FR(x, θ ∗ R)i. Thus, the corresponding confidence is Confidence = FR(x, θ∗R)t̂.\nCalculate the kernel non-ME: We let non-MER denote the non-ME value as defined in Eq. (1), where FR(x, θ∗R) substitutes F (x, θ). Since in practice non-ME changes much slower than confidence, instead of directly using non-MER as a metric, we use the kernel non-ME defined as\nnon-MEKer = exp(−(non-MER−µ)2/ησ2). (5) Here µ and σ are the kernel parameters, and η is the relaxation factor. As we will see in experiments, a good choice of µ and σ are log(L− 1) and the standard deviation of the non-ME values to log(L− 1) on the training set, and we simply set the relaxation factor η to be 1.\nThreshold-output strategy on the J-score: In the prediction release phase, we multiply the values of Confidence and non-MEKer to obtain a joint score (J-score) to measure a compositive certainty on a prediction. J-score has the same value range as the confidence (i.e., [0, 1]). A high J-score implies that the classifier is quite sure about both the label and the normality of the input x, while a low J-score implies uncertainty on either the label or the normality of the input x, or even uncertainty on both. Therefore, given a threshold T on the J-score, we decide to return a meaningful pair of label index and J-score if J-score ≥ T or return NOT SURE otherwise."
    }, {
      "heading" : "4 Experiments",
      "text" : "We now present both quantitative and qualitative results to demonstrate the effectiveness of our method on improving the robustness of state-of-the-art DNN classifiers."
    }, {
      "heading" : "4.1 Setup",
      "text" : "We use the two widely studied datasets—MNIST and CIFAR-10. MNIST is a collection of 70,000 images of handwritten digits in classes 0 to 9. It has a training set of 60,000 samples and a test set of 10,000 samples. CIFAR-10 consists of 60,000 color images in 10 classes with 6,000 images per class. There are 50,000 training images and 10,000 test images. Every image in both sets is standardized before feeding to classifiers to remove interval dependence among pixels, following the setup in [6].\nWe implement Resnet-32, Resnet-56 and Resnet110 [6] on both datasets. For each network, we use both the CE and RCE as the training objective, optimized by the same training tools as in [7]. The training steps for both methods are set to be 20k on MNIST and 80k on CIFAR10. Table 1 shows the test accuracy, where the Threshold-output strategy is disabled and all points receive their predicted labels. The results show that the classification performance of the networks trained via the RCE procedure is\nalmost as good as those trained by the traditional CE procedure. Due to limited space, we only show the results on Resnet-32 in the sequel, while deferring the results on Resnet-56 and Resnet-110 and other DNN architectures to Appendix, which essentially lead to the same conclusions."
    }, {
      "heading" : "4.2 Junction manifolds in the final hidden space",
      "text" : "In order to verify that the RCE training procedure tends to map all the normal inputs to the neighborhoods of oppositely elongated decision boundaries in the final hidden space, we apply the technique t-SNE [16] to visualize the distribution of the final hidden vector zf on the training set. Fig. 2 shows the 2-D visualization results. For clarity, we show the results on 1,000 training samples of MNIST. We defer the results on CIFAR-10 to Appendix, which are similar.\nFrom the results we note that the networks trained via the RCE procedure can successfully map the training samples to the neighborhoods of low-dimensional manifolds in the final hidden space. These results also partly verify the effectiveness of the non-ME being a metric."
    }, {
      "heading" : "4.3 The ability of recognizing adversarial samples",
      "text" : "To analyze the ability of a classifier to distinguish between adversarial samples and normal ones, we construct two mixed datasets—one for MNIST and one for CIFAR-10. Each mixed set takes the corresponding test set as the 10,000 normal samples, and then crafts 10,000 adversarial samples based on the normal ones by FGSM.\nWe feed all the 20,000 samples of a mixed set into the Resnet classifier that is learned on the corresponding training set, with Threshold-output strategy still disabled. Then we sort the samples in a descending order according to their values of a particular metric (i.e., confidence, non-ME or J-score). Fig. 3 shows the classification accuracy on the top-10,000 samples as well as the percentage of normal samples in the top-10,000 samples w.r.t the perturbation ε in FGSM. Here, we treat the Resnets trained by the CE procedure as the baselines, where the metric is the confidence.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nPerturbation\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\n1\nA c c u ra\nc y o\nf to\np -1\n0 ,0\n0 0 s\na m\np le\ns\nJ-score & RCE non-ME & RCE Confidence & RCE Confidence & CE\n(a) The accuracy of the top-10,000 samples (MNIST)\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nPerturbation\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nP e rc\ne n t o f n o rm\na l in\np u ts\nJ-score & RCE non-ME & RCE Confidence & RCE Confidence & CE\n(b) The percent of normal samples in the top-10,000 samples (MNIST)\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nPerturbation\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\nA c c u ra\nc y o\nf to\np -1\n0 ,0\n0 0 s\na m\np le\ns\nJ-score & RCE non-ME & RCE Confidence & RCE Confidence & CE\n(c) The accuracy of the top-10,000 samples (CIFAR-10)\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nPerturbation\n0.65\n0.7\n0.75\n0.8\n0.85\nP e rc\ne n t o f n o rm\na l in\np u ts\nJ-score & RCE non-ME & RCE Confidence & RCE Confidence & CE\n(d) The percent of normal samples in the top-10,000 samples (CIFAR-10)\nFigure 3: Recognizing adversarial samples in the mixed datasets. The model is Resnet-32, J in FGSM is the CE cost function.\nIt is worth noting that when we apply FGSM to craft adversarial samples in Fig. 3, the J in FGSM we used is the CE cost function. However, in FGSM the cost function J should be the objective function used to train the target classifier, and cross-entropy is not the objective function used in the RCE training procedure. Therefore we set J be the RCE cost function to evaluate the performance of our method under more specific attacks. Here we do not directly apply FGSM on the reverse ’liar’ classifier FR(X, θ∗R), instead we apply FGSM on the ’liar’ classifier F (X, θ ∗ R) to craft adversarial samples, because a successful attack on reverse classifiers can also successfully attack original classifiers, and vice versa. For comparison, we also apply FGSM with the same cost function on classifiers trained via the CE procedure, and similarly FGSM acts on the reverse classifier FR(X, θ∗). Results on are shown in Fig. 4.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nPerturbation\n0.96\n0.97\n0.98\n0.99\n1\nA c c u ra\nc y o\nf to\np -1\n0 ,0\n0 0 s\na m\np le\ns\nJ-score & RCE non-ME & RCE Confidence & RCE Confidence & CE\n(a) The accuracy of the top-10,000 samples (MNIST)\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nPerturbation\n0\n0.2\n0.4\n0.6\n0.8\n1\nP e rc\ne n t o f n o rm\na l in\np u ts\nJ-score & RCE non-ME & RCE Confidence & RCE Confidence & CE\n(b) The percent of normal samples in the top-10,000 samples (MNIST)\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nPerturbation\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nA c c u ra\nc y o\nf to\np -1\n0 ,0\n0 0 s\na m\np le\ns\nJ-score & RCE non-ME & RCE Confidence & RCE Confidence & CE\n(c) The accuracy of the top-10,000 samples (CIFAR-10)\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nPerturbation\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nP e rc\ne n t o f n o rm\na l in\np u ts\nJ-score & RCE non-ME & RCE Confidence & RCE Confidence & CE\n(d) The percent of normal samples in the top-10,000 samples (CIFAR-10)\nFigure 4: Recognizing adversarial samples in the mixed datasets. The model is Resnet-32, J in FGSM is the RCE cost function.\nWe can find that all three metrics combined with the RCE training procedure perform pretty well on both datasets. Here we simply set the relaxation factor η be 1, which leads to similar performance when using J-score and using confidence as the metric. The effect of different values of η is discussed later. Besides, we find that FGSM with CE cost function can better attack target classifiers than with RCE cost function, no matter which training procedure and metric the target classifier uses. On the MNIST dataset, non-ME works even better than confidence as the metric, which demonstrates that non-ME is a powerful and reasonable metric combined with the RCE training procedure. Note that in Fig. 4 the baselines have higher values on the percent of normal samples than using confidence or J-score with the RCE training procedure, the accuracy of the baselines is however much lower. The reason is that when the classifiers can still correctly classify most of the adversarial samples, a relatively low percent of normal samples is then rational and does not necessarily lead to a low accuracy, and an unsuccessful adversarial sample can actually be regarded as a normal sample."
    }, {
      "heading" : "4.4 The performance of the entire algorithm",
      "text" : "In order to evaluate the performance of our entire method, we activate the Threshold-output strategy in the prediction release phase and again feed the mixed datasets of 20,000 samples into the classifiers. For the purpose of comparison, we also impose the Threshold-output strategy in the prediction release phase on the baselines and other classifiers using different metrics and the RCE training procedure.\nThe results on MNIST and CIFAR-10 are shown in Fig. 5, where J in FGSM is set to be the CE cost function because of its better attacking ability. The ’Accuracy of output’ refers to the accuracy of returned predictions on the total 20,000 samples. The ’Refuse rate’ on normal or adversarial samples refers to the rate that a classifier refuses to predict and returns NOT SURE on the corresponding 10,000 samples.\nThe results show that our method is much more sensitive to adversarial samples by having high ’Refuse rate’s on them while still has low ’Refuse rate’s on normal\nsamples. Especially on the MNIST dataset, FGSM can not effectively attack the reverse ’liar’ net-\nworks trained by the RCE procedure. Compared to the MNIST dataset, the classification task on CIFAR-10 is much more difficult. However our method can still distinguish well between adversarial samples and normal samples by returning quite different ’Refuse rate’s. It is worth noting that using non-ME has high ’Refuse rate’s on adversarial samples even when the threshold T is small, but their values increase slowly as T increases. In contrast, using confidence has low ’Refuse rate’s on adversarial samples when threshold T is small, however, their values increase obviously as T increases. So as a combination of non-ME and confidence, using J-score benefits from non-ME when T is small and from confidence when T is large, which makes J-score perform best on different values of T . In addition to FGSM, we believe our method will also perform well on defending other attacking algorithms, because as demonstrated in our theoretical analysis we improve the robustness generally, not specially for certain attacking algorithms."
    }, {
      "heading" : "4.5 Different values of relaxation factor η",
      "text" : "In the above experiments, we simply set the relaxation factor η at 1. We now investigate its influence on J-score. In Fig. 6 we separately plot the mean values of J-score and confidence on the 10,000 adversarial samples crafted by FGSM based on the test sets of MNIST and CIFAR-10. Note that the value ranges of both J-score and confidence are [0, 1], and when η is large the\nnon-ME tends to be 1, which leads to identical values of J-score and confidence on all the inputs.\nFrom Fig. 6, we can verify two important points. First, the RCE training returns more reliable and reasonable confidence than the CE training, and J-score is indeed a more distinguishable metric than confidence in the adversarial setting. Second, there are certain medium values of η (e.g., η = 0.05 in both cases) work the best on distinguishability, in the sense of making the values of J-score decrease fastest as the perturbation increases. However, when η = 0.05 the values of J-score are undesirably low on normal samples in CIFAR-10, while our default choice of η = 1 has more balanced performance in both cases."
    }, {
      "heading" : "4.6 Comparison with Bayesian neural networks",
      "text" : "Bayesian neural networks (BNNs) [18] are a family of models that may defend adversarial samples. Recent work [14] shows the ability of BNNs on defending adversarial samples. Here we quantitatively compare our method with BNNs. We access a fully connected network architecture with 2\nhidden layers and 100 units in each layer, and apply the SGMCMC method [2] to train the BNN on MNIST. For comparison, we also apply the CE and the RCE procedure to train the network. We feed the 20,000 mixed samples based on MNIST to these trained networks, and impose the Thresholdoutput strategy on them. The results are shown in Fig. 7. We find that the trained BNN is not as vulnerable to FGSM as discriminative DNNs, partly because FGSM is designed for discriminative models. Our method performs even better than BNN on the overall accuracy and the refuse rate of adversarial samples. Moreover, our method is much simpler and computationally cheaper without the need to do expensive posterior inference as in BNN."
    }, {
      "heading" : "5 Conclusions and Discussions",
      "text" : "We present a promising method to improve the robustness by recognizing and filtering out adversarial samples, which performs well on different state-of-the-art DNN architectures, e.g., Resnets, full-connected networks and other DNN architectures (shown in Appendix). Besides, our method outperforms the BNN on improving the robustness in our experiments. Our method can be implemented using standard algorithms with little extra training cost. Because of the simplicity, our method is also easy to extend with other defense strategies such as adversarial training [26]."
    }, {
      "heading" : "A Proof and supplementary experiments",
      "text" : "A.1 Proof\nTheorem 1. Let x be a given input in the training dataset, y be the one-hot label vector, yR be the reverse label vector corresponding to y, L be the number of different classes and θ∗R is obtained from the reverse training. Under the L∞-norm, if there is a training error α 1L that\n‖softmax(Z(x, θ∗R))− yR‖∞ ≤ α.\nThen we have bounds\n‖softmax(−Z(x, θ∗R))− y‖∞ ≤ α(L− 1) 2\nand ∀j, k 6= t\n|softmax(−Z(x, θ∗R))j − softmax(−Z(x, θ∗R))k| ≤ 2α2(L− 1)2,\nwhere t is the index of true label.\nProof. For simplicity we omit the dependence of the logits Z on the input x and the parameters θ∗R, and explicitly express the logits as Z = (z1, z2, ..., zL). Let G = (g1, g2, ..., gL) = (ez1 , ez2 , ..., ezL) be the exponential logits, t be the label of x, then from the condition ‖softmax(Z)− yR‖∞ ≤ α we have\n{ gt∑ i gi ≤ α∣∣∣ gj∑\ni gi − 1L−1 ∣∣∣ ≤ α j 6= t. Let C = ∑ i gi, we can further write the condition as\n{ gt ≤ αC ( 1L−1 − α)C ≤ gj ≤ ( 1 L−1 + α)C j 6= t.\nThen we can have bounds (L ≥ 2)\nsoftmax(−Z)t = 1 gt\n1 gt + ∑ i6=t 1 gi\n= 1 1 + ∑ i 6=t gt gi\n≥ 1 1 + ∑ i 6=t\nαC ( 1L−1−α)C\n= 1\n1 + α(L−1) 2\n1−α(L−1)\n= 1− α(L− 1) 2\n1− α(L− 1) + α(L− 1)2\n≥ 1− α(L− 1)2\nand ∀j 6= t,\nsoftmax(−Z)j = 1 gj\n1 gt + ∑ i 6=t 1 gi\n=\ngt gj 1 + gtgj + ∑ i6=t,j gt gi\n≤ gt gj\n1 + gtgj\n= 1\n1 + gj gt\n≤ 1\n1 + ( 1L−1−α)C\nαC\n= α(L− 1).\nThen we have proven that ‖softmax(−Z)− y‖∞ ≤ α(L− 1)2. Furthermore, we have ∀j, k 6= t,\n|softmax(−Z)j − softmax(−Z)k| = ∣∣∣ 1gj − 1gk ∣∣∣ 1 gt + ∑ i 6=t 1 gi\n≤ 1 ( 1L−1−α)C − 1 ( 1L−1+α)C\n1 αC + ∑ i 6=t\n1 ( 1L−1+α)C\n=\nL−1 1−α(L−1) − L−1 1+α(L−1)\n1 α + (L−1)2 1+α(L−1)\n= 2α2(L− 1)2\n1 + α(L− 1)2(1− αL) ≤ 2α2(L− 1)2.\nA.2 Experiments on simple CNN\nIn order to test our method on other convolutional neural networks (CNNs) [3], we implement two neural networks as described in Table 2 and the training details are included in Table 3. The architectures are based on the most basic CNN operations, and we use training tools like Adam [10], dropout [25] and batch normalization [9] in our training procedures.\nWe separately apply the CE training procedure and the RCE training procedure on the MNIST and the CIFAR-10 datasets using the same architectures and training methods described above. The test accuracy is shown in Table 4, where the Threshold-output strategy is disabled and all points in the test set can receive their predicted labels. We use a weight decay of 0.001 and momentum of 0.9 to train the CIFAR-10 network. Besides, we standardize the data points in CIFAR-10 before feeding to classifiers.\nWe separately feed the mixed datasets of MNIST and CIFAR-10 into the corresponding trained networks. Results are shown in Fig. 8. We can find that our method can also work well on simple CNN architectures. Note that in the experiments on CIFAR-10, the gradual increase of the refuse rate on normal samples is reasonable because the test accuracy of the classifiers on CIFAR-10 are only around 85%, not higher than 98% as on MNIST."
    }, {
      "heading" : "B t-SNE on the CIFAR-10",
      "text" : "In Fig. 9 we show the results of t-SNE visualization on CIFAR-10."
    }, {
      "heading" : "C Full results of Resnets",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Towards evaluating the robustness of neural networks",
      "author" : [ "Nicholas Carlini", "David Wagner" ],
      "venue" : "arXiv preprint arXiv:1608.04644,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Bayesian sampling using stochastic gradient thermostats",
      "author" : [ "Nan Ding", "Youhan Fang", "Ryan Babbush", "Changyou Chen", "Robert D Skeel", "Hartmut Neven" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Deep Learning",
      "author" : [ "Ian Goodfellow", "Yoshua Bengio", "Aaron Courville" ],
      "venue" : "http://www.deeplearningbook.org",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Towards deep neural network architectures robust to adversarial examples",
      "author" : [ "Shixiang Gu", "Luca Rigazio" ],
      "venue" : "arXiv preprint arXiv:1412.5068,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "Kurt Hornik", "Maxwell Stinchcombe", "Halbert White" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1989
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : "Technical report, CiteSeerX,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Adversarial examples in the physical world",
      "author" : [ "Alexey Kurakin", "Ian Goodfellow", "Samy Bengio" ],
      "venue" : "arXiv preprint arXiv:1607.02533,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Dropout inference in bayesian neural networks with alphadivergences",
      "author" : [ "Yingzhen Li", "Yarin Gal" ],
      "venue" : "arXiv preprint arXiv:1703.02914,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2017
    }, {
      "title" : "Delving into transferable adversarial examples and black-box attacks",
      "author" : [ "Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Song" ],
      "venue" : "arXiv preprint arXiv:1611.02770,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Deepfool: a simple and accurate method to fool deep neural networks",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Bayesian learning for neural networks, volume 118",
      "author" : [ "Radford M Neal" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Jeff Clune" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow" ],
      "venue" : "arXiv preprint arXiv:1605.07277,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Practical black-box attacks against deep learning systems using adversarial examples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "arXiv preprint arXiv:1602.02697,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "In Security and Privacy (EuroS&P),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Distillation as a defense to adversarial perturbations against deep neural networks",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami" ],
      "venue" : "In Security and Privacy (SP),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "Cognitive modeling,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1988
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1929
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "1 Introduction Deep learning (DL) has obtained substantial progress in many tasks, including image categorization, speech recognition, and natural language processing [3].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : "However, a high-accuracy DL model can be vulnerable in the adversarial setting [4, 26], where adversarial samples are carefully crafted to mislead the model to wrong predictions.",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "[5] considers adding a regularization term on the norm of gradients in the objective function, but it does not promise anything to gradient sign attacking methods that only require the sign of gradients.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 22,
      "context" : "The defensive distillation method [23] trains a model with a high temperature and tests it with a low temperature.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, this defensive distillation method can be easily attacked by new attacking methods [1].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "the common cross-entropy (CE) loss [3].",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "We apply our method to improve the state-of-the-art residual neural networks (Resnets) [6] on the widely used MNIST [13] and CIFAR-10 [11] datasets.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "We apply our method to improve the state-of-the-art residual neural networks (Resnets) [6] on the widely used MNIST [13] and CIFAR-10 [11] datasets.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "We apply our method to improve the state-of-the-art residual neural networks (Resnets) [6] on the widely used MNIST [13] and CIFAR-10 [11] datasets.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Finally, we note that recent work [14] shows that a Bayesian neural network (BNN) can improve the robustness of DNN against adversarial samples.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "The probability value F (x, θ)t̂(x,θ) is often used as the corresponding confidence score on this prediction [3], although it is insufficient as we shall see.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "Then the CE training procedure intends to minimize the average CE loss to obtain the optimal parameters θ∗ = arg minθ 1 N ∑ i∈[N ] LCE(xi, yi), which can be efficiently done by stochastic gradient methods with back-propagation [10, 24].",
      "startOffset" : 227,
      "endOffset" : 235
    }, {
      "referenceID" : 23,
      "context" : "Then the CE training procedure intends to minimize the average CE loss to obtain the optimal parameters θ∗ = arg minθ 1 N ∑ i∈[N ] LCE(xi, yi), which can be efficiently done by stochastic gradient methods with back-propagation [10, 24].",
      "startOffset" : 227,
      "endOffset" : 235
    }, {
      "referenceID" : 18,
      "context" : "2 Adversarial sample crafting and robustness Though DNN has obtained substantial progress, adversarial samples can be easily identified to fool the network, even when its accuracy is high [19].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 3,
      "context" : "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 19,
      "context" : "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 22,
      "context" : "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 16,
      "context" : "Moreover, to evaluate the ability of a classifier to resist adversarial perturbation, we define the robustness ∆(x; t̂) according to [17] as ∆(x; t̂) := min r ‖r‖2 subject to t̂(x+ r) 6= t̂(x), where r is the perturbation.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "1 Why adversarial samples exist Previous work [4, 22] hypothesizes that the existence of adversarial samples is because of certain defects in the training phase.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "1 Why adversarial samples exist Previous work [4, 22] hypothesizes that the existence of adversarial samples is because of certain defects in the training phase.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "One main support is from the universal approximator theorem [8] that DNNs are able to represent functions that resist adversarial samples.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : ", by adding regularizers in the objective function [5] or augmenting the training dataset [26], will only result in the change of the distribution of decision boundaries but the jump on the predicted labels nearby the decision boundary still exists.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "A potential candidate is the confidence F (x, θ)t̂(x,θ) on the predicted label t̂(x, θ), which has in fact been widely used [3].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : "However, it has been shown that a well-trained DNN classifier usually not only misclassifies adversarial samples but also gives high confidence on its predictions [4, 19], which renders the confidence unreliable in the adversarial setting.",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 18,
      "context" : "However, it has been shown that a well-trained DNN classifier usually not only misclassifies adversarial samples but also gives high confidence on its predictions [4, 19], which renders the confidence unreliable in the adversarial setting.",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "In the adversarial setting, attackers can explore the points far from the data distribution [14], and the predictions in these domains mostly result from interpolation with high uncertainty.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : ", [0, 1]).",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "Every image in both sets is standardized before feeding to classifiers to remove interval dependence among pixels, following the setup in [6].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "94% We implement Resnet-32, Resnet-56 and Resnet110 [6] on both datasets.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "For each network, we use both the CE and RCE as the training objective, optimized by the same training tools as in [7].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "In order to verify that the RCE training procedure tends to map all the normal inputs to the neighborhoods of oppositely elongated decision boundaries in the final hidden space, we apply the technique t-SNE [16] to visualize the distribution of the final hidden vector zf on the training set.",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : "Note that the value ranges of both J-score and confidence are [0, 1], and when η is large the non-ME tends to be 1, which leads to identical values of J-score and confidence on all the inputs.",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Bayesian neural networks (BNNs) [18] are a family of models that may defend adversarial samples.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "Recent work [14] shows the ability of BNNs on defending adversarial samples.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 1,
      "context" : "We access a fully connected network architecture with 2 hidden layers and 100 units in each layer, and apply the SGMCMC method [2] to train the BNN on MNIST.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 0,
      "context" : "References [1] Nicholas Carlini and David Wagner.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert D Skeel, and Hartmut Neven.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Shixiang Gu and Luca Rigazio.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Kurt Hornik, Maxwell Stinchcombe, and Halbert White.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Sergey Ioffe and Christian Szegedy.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Diederik Kingma and Jimmy Ba.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Alex Krizhevsky and Geoffrey Hinton.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Alexey Kurakin, Ian Goodfellow, and Samy Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Yingzhen Li and Yarin Gal.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Laurens van der Maaten and Geoffrey Hinton.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Radford M Neal.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Anh Nguyen, Jason Yosinski, and Jeff Clune.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "2 Experiments on simple CNN In order to test our method on other convolutional neural networks (CNNs) [3], we implement two neural networks as described in Table 2 and the training details are included in Table 3.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "The architectures are based on the most basic CNN operations, and we use training tools like Adam [10], dropout [25] and batch normalization [9] in our training procedures.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "The architectures are based on the most basic CNN operations, and we use training tools like Adam [10], dropout [25] and batch normalization [9] in our training procedures.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "The architectures are based on the most basic CNN operations, and we use training tools like Adam [10], dropout [25] and batch normalization [9] in our training procedures.",
      "startOffset" : 141,
      "endOffset" : 144
    } ],
    "year" : 2017,
    "abstractText" : "Though the recent progress is substantial, deep learning methods can be vulnerable to the elaborately crafted adversarial samples. In this paper, we attempt to improve the robustness by presenting a new training procedure and a thresholding test strategy. In training, we propose to minimize the reverse cross-entropy, which encourages a deep network to learn latent representations that better distinguish adversarial samples from normal ones. In testing, we propose to use a thresholding strategy based on a new metric to filter out adversarial samples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization. We apply our method to various state-of-the-art networks (e.g., residual networks) and we achieve significant improvements on robust predictions in the adversarial setting.",
    "creator" : "LaTeX with hyperref package"
  }
}