{
  "name" : "1210.0508.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Inference algorithms for pattern-based CRFs on sequence data",
    "authors" : [ "Rustem Takhanov", "Vladimir Kolmogorov" ],
    "emails" : [ "takhanov@mail.ru", "vnk@ist.ac.at" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present efficient algorithms for the three standard inference tasks in a CRF, namely computing (i) the partition function, (ii) marginals, and (iii) computing the MAP. Their complexities are respectively O(nL), O(nL`max) and O(nLmin{|D|, log(`max+1)}) where L is the combined length of input patterns, `max is the maximum length of a pattern, and D is the input alphabet. This improves on the previous algorithms of [Ye et al. NIPS 2009] whose complexities are respectively O(nL|D|), O ( n|Γ|L2`2max ) and O(nL|D|), where |Γ| is the number of input patterns. In addition, we give an efficient algorithm for sampling, and revisit the case of MAP with non-positive weights.\nThis paper addresses the sequence labeling (or the sequence tagging) problem: given an observation z (which is usually a sequence of n values), infer labeling x = x1 . . . xn where each variable xi takes values in some finite domain D. Such problem appears in many domains such as text and speech analysis, signal analysis, and bioinformatics.\nOne of the most successful approaches for tackling the problem is the Hidden Markov Model (HMM). The kth order HMM is given by the probability distribution p(x|z) = 1Z exp{−E(x|z)} with the energy function\nE(x|z) = ∑ i∈[1,n] ψi(xi, zi) + ∑\n(i,j)∈Ek\nψij(xi:j) (1)\nwhere Ek = {(i, i+ k) | i ∈ [1, n− k]} and xi:j = xi . . . xj is the substring of x from i to j. A popular generalization is the Conditional Random Field model [3] that allows all terms to depend on the full observation z:\nE(x|z) = ∑ i∈[1,n] ψi(xi, z) + ∑\n(i,j)∈Ek\nψij(xi:j , z) (2)\nA preliminary version of this paper appeared in Proceedings of the 30th International Conference on Machine Learning (ICML), 2013 [8]. This work was partially supported by the European Research Council under the European Unions Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 616160.\nar X\niv :1\n21 0.\n05 08\nv5 [\ncs .L\nG ]\n2 0\nWe study a particular variant of this model called a pattern-based CRF. It is defined via E(x|z) = ∑ α∈Γ ∑ [i,j]⊆[1,n] j−i+1=|α| ψαij(z) · [xi:j = α] (3)\nwhere Γ is a fixed set of non-empty words, |α| is the length of word α and [·] is the Iverson bracket. If we take Γ = D ∪Dk then (3) becomes equivalent to (2); thus we do not loose generality (but gain more flexibility).\nIntuitively, pattern-based CRFs allow to model long-range interactions for selected subsequences of labels. This could be useful for a variety of applications: in part-of-speech tagging patterns could correspond to certain syntactic constructions or stable idioms; in protein secondary structure prediction - to sequences of dihedral angles corresponding to stable configuration such as α-helixes; in gene prediction - to sequences of nucleatydes with supposed functional roles such as “exon” or “intron”, specific codons, etc. Inference This paper focuses on inference algorithms for pattern-based CRFs. The three standard inference tasks are\n• computing the partition function Z = ∑\nx exp{−E(x|z)};\n• computing marginal probabilities p(xi:j = α|z) for all triplets (i, j, α) present in (3);\n• computing MAP, i.e. minimizing energy (3). The complexity of solving these tasks is discussed below. We denote L = ∑\nα∈Γ |α| to be total length of patterns and `max = maxα∈Γ |α| to be the maximum length of a pattern.\nA naive approach is to use standard message passing techniques for an HMM of order k=`max−1. However, they would take O(n|D|k+1) time which would become impractical for large k. More efficient algorithms with complexities O(nL|D|), O ( n|Γ|L2`2max ) and O(nL|D|) respectively were given by Ye et al. [10].1 Our first contribution is to improve this to O(nL), O(nL`max) and O(nL·min{|D|, log(`max+ 1)}) respectively (more accurate estimates are given in the next section).\nWe also give an algorithm for sampling from the distribution p(x|z). Its complexity is either (i) O(nL) per sample, or (ii) O(n) per sample with an O(nL|D|) preprocessing (assuming that we have an oracle that produces independent samples from the uniform distribution on [0, 1] in O(1) time).\nFinally, we consider the case when all costs ψαij(z) are non-positive. Komodakis and Paragios [2] gave an O(nL) technique for minimizing energy (3) in this case. We present a modification that has the same worst-case complexity but can beat the algorithm in [2] in the best case. Related work The works of [10] and [2] are probably the most related to our paper. The former applied pattern-based CRFs to the handwritten character recognition problem and to the problem of identification of named entities from texts. The latter considered a pattern-based CRF on a grid for a computer vision application; the MAP inference problem in [2] was converted to sequence labeling problems by decomposing the grid into thin “stripes”.\nQian et al. [5] considered a more general formulation in which a single pattern is characterized by a set of strings rather than a single string α. They proposed an exact inference algorithm and applied it to the OCR task and to the Chinese Organization Name Recognition task. However, their algorithm could take time exponential in the total lengths of input patterns; no subclasses of inputs were identified which could be solved in polynomial time.\nA different generalization (for non-sequence data) was proposed by Rother et al. [6]. Their inference procedure reduces the problem to the MAP estimation in a pairwise CRF with cycles, which\n1Some of the bounds stated in [10] are actually weaker. However, it is not difficult to show that their algorithms can be implemented in times stated above, using our Lemma 1.\nis then solved with approximate techniques such as BP, TRW or QPBO. This model was applied to the texture restoration problem.\nNguyen et al. [4] extended algorithms in [10] to the Semi-Markov model [7]. We conjecture that our algorithms can be extended to this case as well, and can yield a better complexity compared to [4].\nIn [8] we applied the pattern-based CRF model to the problem of the protein dihedral angles prediction."
    }, {
      "heading" : "1 Notation and preliminaries",
      "text" : "First, we introduce a few definitions.\n• A pattern is a pair α = ([i, j], x) where [i, j] is an interval in [1, n] and x = xi . . . xj is a sequence over alphabet D indexed by integers in [i, j] (j ≥ i− 1). The length of α is denoted as |α| = |x| = j − i+ 1.\n• Symbols “∗” denotes an arbitrary word or pattern (possibly the empty word ε or the empty pattern εs , ([s + 1, s], ε) at position s). The exact meaning will always be clear from the context. Similary, “+” denotes an arbitrary non-empty word or pattern.\n• The concatenation of patterns α = ([i, j], x) and β = ([j+1, k], y) is the pattern αβ , ([i, k], xy). Whenever we write αβ we assume that it is defined, i.e. α = ([·, j], ·) and β = ([j + 1, ·], ·) for some j.\n• For a pattern α = ([i, j], x) and interval [k, `] ⊆ [i, j], the subpattern of α at position [k, `] is the pattern αk:` , ([k, `], xk:`) where xk:` = xk . . . x`. If k = i then αk:` is called a prefix of α. If ` = j then αk:` is a suffix of α.\n• If β is a subpattern of α, i.e. β = αk:` for some [k, `], then we say that β is contained in α. This is equivalent to the condition α = ∗β∗.\n• Di:j = {([i, j], x) | x ∈ D[i,j]} is the set of patterns with interval [i, j]. We typically use letter x for patterns in D1:s and letters α, β, . . . for other patterns. Patterns x ∈ D1:s will be called partial labelings.\n• For a set of patterns Π and index s ∈ [0, n] we denote Πs to be the set of patterns in Π that end at position s: Πs = {([i, s], α) ∈ Π}.\n• For a pattern α let α− be the prefix of α of length |α| − 1; if α is empty then α− is undefined.\nWe will consider the following general problem. Let Π◦ be the set of patterns of words in Γ placed at all possible positions: Π◦ = {([i, j], α) | α ∈ Γ)}. Let (R,⊕,⊗) be a commutative semiring with elements O,1 ∈ R which are identities for ⊕ and ⊗ respectively. Define the cost of pattern x ∈ Di:j via\nf(x) = ⊗\nα∈Π◦,x=∗α∗ cα (4)\nwhere cα∈R are fixed constants. (Throughout the paper we adopt the convention that operations ⊕ and ⊗ over the empty set of arguments give respectively O and 1, and so e.g. f(εs) = 1.) Our goal is to compute\nZ = ⊕\nx∈D1:n f(x) (5)\nExample 1 If (R,⊕,⊗) = (R,+,×) then problem (5) is equivalent to computing the partition function for the energy (3), if we set c([i,j],α) = exp{−ψαij(z)}. Example 2 If (R,⊕,⊗) = (R,min,+) where R , R ∪ {+∞} then we get the problem of minimizing energy (3), if c([i,j],α) = ψ α ij(z).\nThe complexity of our algorithms will be stated in terms of the following quantities:\n• P = |{α | ∃α∗ ∈ Γ, α 6= ε}| is the number of distinct non-empty prefixes of words in Γ. Note that P ≤ L.\n• P ′ = |{α | ∃α+ ∈ Γ}| is the number of distinct proper prefixes of words in Γ. There holds P P ′ ∈ [1, |D|]. If Γ = D1 ∪D2 ∪ . . . ∪Dk then PP ′ = |D|. If Γ is a sparse random subset of the set above then P P ′ ≈ 1.\n• I(Γ) = {α | ∃α∗, ∗α ∈ Γ, α 6= ε} is the set of non-empty words which are both prefixes and suffixes of some words in Γ. Note that Γ⊆I(Γ) and |I(Γ)|≤P .\nWe will present 6 algorithms: Sec. 2: Θ(nP ) algorithm for the case when (R,⊕,⊗) is a ring, i.e. it has operation that satisfies (a b)⊕ b = a for all a, b ∈ R. This holds for the semiring in Example 1 (but not for Example 2). Sec. 3: Θ(nP ) algorithm for sampling. Alternatively, it can be implemented to produce independent samples in Θ(n) time per sample with a Θ(nP |D|) preprocessing. Sec. 4: O(n ∑ α∈I(Γ) |α|) algorithm for computing marginals for all patterns α ∈ Π◦. Sec. 5: Θ(nP ′|D|) algorithm for a general commutative semiring, which is equivalent to the algorithm in [10]. It will be used as a motivation for the next algorithm. Sec. 6: O(nP logP ) algorithm for a general commutative semiring; for the semiring in Example 2 the complexity can be reduced to O(nP log(`max + 1)). Sec. 7: O(nP ) algorithm for the case (R,⊕,⊗) = (R,min,+), cα ≤ 0 for all α ∈ Π◦.\nAll algorithms will have the following structure. Given the set of input patterns Π◦, we first construct another set of patterns Π; it will typically be either the set of prefixes or the set of proper prefixes of patterns in Π◦. This can be done in a preprocessing step since sets Πs will be isomorphmic (up to a shift) for indexes s that are sufficiently far from the boundary. (Recall that Πs is the set of patterns in Π that end at position s.) Then we recursively compute messages Ms(α) for α ∈ Πs which have the following interpretation: Ms(α) is the sum (“⊕”) of costs f(x) over a certain set of partial labelings of the form x = ∗α ∈ D1:s. In some of the algorithms we also compute messages Ws(α) which is the sum of f(x) over all partial labelings of the form x = ∗α ∈ D1:s. Graph G[Πs] The following construction will be used throughout the paper. Given a set of patterns Π and index s, we define G[Πs] = (Πs, E[Πs]) to be the Hasse diagram of the partial order on Πs, where α β iff α is a suffix of β (β = ∗α). In other words, G[Πs] is a directed acyclic graph with the following set of edges: (α, β) belongs to E[Πs] for α, β ∈ Πs if α ≺ β and there exists no “intermediate” pattern γ ∈ Πs with α ≺ γ ≺ β. It can be checked that graph G[Πs] is a directed forest. If εs ∈ Πs then G[Πs] is connected and therefore is a tree. In this case we treat εs as the root. An example is shown in Fig. 1.\nComputing partial costs Recall that f(α) for a pattern α is the cost of all patterns inside α (eq. (4)). We also define φ(α) to be the cost of only those patterns that are suffixes of α:\nφ(α) = ⊗\nβ∈Π◦,α=∗β cβ (6)\nQuantities φ(α) and f(α) will be heavily used by the algorithms below; let us show how to compute them efficiently.\nLemma 1. Let Π be a set of patterns with εs ∈ Π for all s ∈ [0, n]. Values φ(α) for all α ∈ Π can be computed using O(|Π|) multiplications (“⊗”). The same holds for values f(α) assuming that Π is prefix-closed, i.e. α− ∈ Π for all non-empty patterns α ∈ Π.\nProof. To compute φ(·) for patterns α ∈ Πs, we use the following procedure: (i) set φ(εs) := 1; (ii) traverse edges (α, β) ∈ E[Πs] of tree G[Πs] (from the root to the leaves) and set\nφ(β) :=\n{ φ(α)⊗ cβ if β ∈ Π◦\nφ(α) otherwise\nNow suppose that Π is prefix-closed. After computing φ(·), we go through indexes s ∈ [0, n] and set\nf(εs) :=1, f(α) :=f(α −)⊗ φ(α) ∀α ∈ Πs − {εs}\nSets of partial labelings Let Πs be a set of patterns that end at position s. Assume that εs ∈ Πs. For a pattern α ∈ Πs we define\nXs(α) = {x ∈ D1:s | x = ∗α} (7) Xs(α; Πs) = Xs(α)− ⋃ (α,β)∈E[Πs] Xs(β) (8)\nIt can be seen that sets Xs(α; Πs) are disjoint, and their union over α ∈ Πs is D1:s. Furthermore, there holds\nXs(α; Πs) = {x ∈ Xs(α) | x 6= ∗β ∀β = +α ∈ Πs} (9)\nWe will use eq. (9) as the definition of Xs(α; Πs) in the case when α /∈ Πs."
    }, {
      "heading" : "2 Computing partition function",
      "text" : "In this section we give an algorithm for computing quantity (5) assuming that (R,⊕,⊗) is a ring. This can be used, in particular, for computing the partition function. We will assume that D ⊆ Γ; we can always add D to Γ if needed2.\n2Note that we still claim complexity O(nP ) where P is the number of distinct non-empty prefixes of words in the original set Γ. Indeed, we can assume w.l.o.g. that each letter in D occurs in at least one word α∈Γ. (If not, then we can “merge” non-occuring letters to a single letter and add this letter to Γ; clearly, any instance over the original pair (D,Γ) can be equivalenly formulated as an instance over the new pair. The transformation increases P only by 1). The assumption implies that |D| ≤ P . Adding D to Γ increases P by at most P , and thus does not affect bound O(nP ).\nFirst, we select set Π as the set of prefixes of patterns in Π◦:\nΠ = {α | ∃α∗ ∈ Π◦} (10)\nWe will compute the following quantities for each s ∈ [0, n], α ∈ Πs:\nMs(α) = ⊕\nx∈Xs(α;Πs)\nf(x) , Ws(α) = ⊕\nx∈Xs(α)\nf(x) (11)\nIt is easy to see that for α ∈ Πs the following equalities relate Ms and Ws:\nMs(α) = Ws(α) ⊕\n(α,β)∈E[Πs]\nWs(β) (12a)\nWs(α) = Ms(α)⊕ ⊕\n(α,β)∈E[Πs]\nWs(β) (12b)\nThese relations motivate the following algorithm. Since |Πs| = P + 1 for indexes s that are sufficiently far from the boundary, its complexity is Θ(nP ) assuming that values φ(α) in eq. (13a) are computed using Lemma 1. Algorithm 1 Computing Z = ⊕\nx∈D1:n f(x) for a ring\n1: initialize messages: set W0(ε0) := O 2: for s = 1, . . . , n traverse nodes α ∈ Πs of tree G[Πs] starting from the leaves and set\nMs(α) := φ(α)⊗ Ws−1(α−) ⊕ (α,β)∈E[Πs] Ws−1(β −)  (13a) Ws(α) := Ms(α)⊕\n⊕ (α,β)∈E[Πs] Ws(β) (13b)\nException: if α = εs then set Ms(α) := O instead of (13a) 3: return Z := Wn(εn)\nTheorem 2. Algorithm 1 is correct, i.e. it returns the correct value of Z = ⊕\nx F (x)."
    }, {
      "heading" : "2.1 Proof of Theorem 2",
      "text" : "Eq. (13b) coincides with (12b); let us show that eq. (13a) holds for any α ∈ Πs − {εs}. (Note, for α = εs step 2 is correct: assumption D ⊆ Γ implies that Ds:s ⊆ Πs, and therefore Xs(εs; Πs) = ∅, Ms(εs) = O).\nFor a partial labeling x ∈ D1:s define the “reduced partial cost” as\nf−(x) = ⊗\nα∈Π◦,x=∗α+ cα (14)\nIt is easy to see from (11) that for any α ∈ Πs − {εs}\nWs−1(α −) = ∑ x∈Xs(α) f−(x) (15)\nConsider α ∈ Πs − {εs}. We will show that for any x ∈ Xs(α) there holds\nJx ∈ Xs(α; Πs)K⊗ f(x) = φ(α)⊗ f−(x) ⊕ (α,β)∈E[Πs]:x∈Xs(β) f−(x)  (16) where J·K = 1 if the argument is true, and O otherwise. This will be sufficient for establishing the theorem: summing these equations over x ∈ Xs(α) and using (11), (15) yields eq. (13a).\nTwo cases are possible: Case 1: x ∈ Xs(β) for some (α, β) ∈ E[Πs]. (Such β is unique since sets Xs(β) are disjoint.) Then both sides of (16) are O. Case 2: x ∈ Xs(α; Πs). Then eq. (16) is equivalent to f(x) = φ(α) ⊗ f−(x). This holds since there is no pattern γ ∈ Π◦s(x) with |γ| > |α| (otherwise we would have γ ∈ Πs and thus x /∈ Xs(α; Πs) by definition (9) - a contradiction)."
    }, {
      "heading" : "3 Sampling",
      "text" : "In this section consider the semiring (R,⊕,⊗) = (R,+,×) from Example 1. We assume that all costs cα are strictly positive. We present an algorithm for sampling labelings x ∈ D1:n according to the probability distribution p(x) = f(x)/Z.\nAs in the previous section, we assume that D ⊆ Γ, and define Π to be the set of prefixes of patterns in Π◦ (eq. (10)). For a node α ∈ Πs let Ts(α) be the set of nodes in the subtree of G[Πs] rooted at α, with α ∈ Ts(α) ⊆ Πs. For a pattern α ∈ Πs+1 − {εs+1} we define set\n∆s(α) = Ts(α −)− ⋃ (α,β)∈G[Πs+1] Ts(β −) (17)\nWe can now present the algorithm (see Algorithm 2).\nAlgorithm 2 Sample x ∼ p(x) = f(x)/Z 1: run Algorithm 1 to compute messages Ms(α) for all patterns α = ([·, s], ·) ∈ Π 2: sample αn∈Πn with probability p(αn)∝Mn(αn) 3: for s = n−1, . . . , 1 sample αs ∈ ∆s(αs+1) with probability p(αs) ∝Ms(αs) 4: return labeling x with xs:s = (αs)s:s for s ∈ [1, n]\nWe say that step s of the algorithm is valid if either (i) s = n, or (ii) s ∈ [1, n − 1], step s + 1 is valid, αs+1 6= εs+1 and Ms(α) > 0 for some α ∈ ∆s(αs+1). (This is a recursive definition.) Clearly, if step s is valid then line 3 of the algorithm is well-defined.\nTheorem 3. (a) With probability 1 all steps of the algorithm are valid. (b) The returned labeling x ∈ D1:n is distributed according to p(x) = f(x)/Z.\nComplexity Assume that we have an oracle that produces independent samples from the uniform distribution on [0, 1] in O(1) time.\nThe main subroutine performed by the algorithm is sampling from a given discrete distribution. Clearly, this can be done in Θ(N) time where N is the number of allowed values of the random variable. With a Θ(N) preprocessing, a sample can also be produced in O(1) time by the so-called “alias method” [9].\nThis leads to two possible complexities: (i) Θ(nP ) (without preprocessing); (ii) Θ(n) per sample (with preprocessing). Let us discuss the complexity of this preprocessing. Running Algorithm 1 takes\nΘ(nP ) time. After that, for each α ∈ Πs+1 we need to run the linear time procedure of [9] for distributions p(β) ∝ Ms(β), β ∈ ∆s(αs+1). The following theorem implies that this takes Θ(nP |D|) time.\nTheorem 4. There holds ∑\nα∈Πs−{εs} |∆s−1(α)| = |Πs−1| · |D|.\nProof. Consider pattern β ∈ Πs−1. For a letter a ∈ Ds:s let βa ∈ Πs be longest suffix α of βa with α ∈ Πs (at least one such suffix exists, namely a). It can be seen that the set {βa |a ∈ Ds:s} is exactly the set of patterns α ∈ Πs − {εs} for which ∆s−1(α) contains β (checking this fact is just definition chasing). Therefore, the sum in the theorem equals ∑ β∈Πs−1 |{β a | a ∈ Ds:s}| = |Πs−1| · |D|.\nTo summarize, we showed that with a Θ(nP |D|) preprocessing we can compute independent samples from p(x) in Θ(n) time per sample."
    }, {
      "heading" : "3.1 Proof of Theorem 3",
      "text" : "Suppose that step s ∈ [1, n] of the algorithm is valid; this means that patterns αt for t ∈ [s, n] are well-defined. For t ∈ [s, n] we then define the set of patterns At = ∆t(αt+1) ⊆ Πt (if t = n then we define At = Πt instead). We also define sets of labelings\nYt(α) = {yxt+1:n | y ∈ Xt(α; Πt)} ∀α ∈ At (18) Yt = Yt(αt) (19)\nwhere x is a labeling with xt:t = (αt)t:t for t ∈ [s, n]. Let Yn+1 = D1:n.\nLemma 5. Suppose that step s ∈ [1, n] is valid. (a) Ys+1 is a disjoint union of sets Ys(α) over α ∈ As. (b) For each y ∈ Ys+1 = ⋃ α∈As Ys(α) there holds f(y) = consts · f(y1:s), and consequently for any\nα ∈ As there holds ∑ y∈Ys(α) f(y) = consts · ∑ y∈Xs(α;Πs) f(y) = consts ·Ms(α)\nTheorem 3 will follow from this lemma. Indeed, the lemma shows that the algorithm implicitly computes a sequence of nested sets D1:n = Yn+1 ⊇ Yn ⊇ . . . ⊇ Y1 = {x}. At step s we divide set Ys+1 into disjoint subsets Ys(α), α ∈ As and select one of them, Ys = Ys(αs), with the probability proportional to Ms(αs) ∝ ∑ y∈Ys(αs) f(y).\nWe still need to show that if step s ∈ [2, n] is valid then step s− 1 is valid as well with probability 1. It follows from the precondition that αs sampled in line 3 satisfies Ms(αs) > 0 with probability 1; this implies that αs 6= εs. From the paragraph above we get that ∑ y∈Ys f(y) > 0 with probability\n1. We also have ∑ α∈As−1 Ms−1(α) ∝ ∑ α∈As−1 ∑ y∈Ys−1(α) f(y) = ∑\ny∈Ys f(y) > 0 implying that Ms−1(α) > 0 for some α ∈ As−1. This concludes the proof that step s− 1 is valid with probability 1.\nIt remains to prove Lemma 5.\nPart (a) First, we need to check that Xs(α−s+1; Π − s+1) is equal to the disjoint union of Xs(α; Πs) over α ∈ ∆s(αs+1) where Π−s+1 = {α− | α ∈ Πs+1}. Disjointness of Xs(α; Πs) for different α ∈ Πs is obvious. Since Π−s+1 ⊆ Πs, then for any α ∈ ∆s(αs+1), Xs(α; Πs) ⊆ Xs(α − s+1; Π − s+1) is straightforward from the definition of ∆s(αs+1). Thus, we only need to check the inclusion of Xs(α−s+1; Π − s+1) in the union. Elements of Π−s+1 ∪ { α−s+1 } can be seen as nodes in tree G[Πs]. Then any pattern x from Xs(α−s+1; Π − s+1) defines the longest suffix s(x) such that s(x) ∈ Πs. It is easy to see that s(x) ∈ Ts(α − s+1), and moreover, the descending path in G[Πs] from α − s+1 to s(x) does not contain elements from\nΠ−s+1 − { α−s+1 } , otherwise x, s(x) /∈ Xs(α−s+1; Π − s+1). It is easy to see that this is equivalent to s(x) ∈ ∆s(αs+1). Since x ∈ Xs(s(x); Πs), Xs(α−s+1; Π − s+1) is a subset of the union of Xs(α; Πs) over α ∈ ∆s(αs+1). Now according to definition of Ys+1 we can write:\nYs+1 = {yxs+2:n | y ∈ Xs+1(αs+1; Πs+1)} = {y(αs+1)s+1:s+1xs+2:n | y ∈ Xs(α−s+1; Π − s+1)}\n= ⋃\nα∈∆s(αs+1)\n{y(αs+1)s+1:s+1xs+2:n | y ∈ Xs(α; Πs)}\nIt only remains to check that in the last union the set corresponding to α ∈ ∆s(αs+1) is exactly equal to Ys(α). Part (b) Let p be the start position of αs+1, i.e. αs+1 = ([p, s+ 1], ·). Consider labeling y ∈ Ys+1, we then must have y = ∗αs+1∗. Let β = ([i, j], ·) be a pattern with y = ∗β∗, j > s. We will prove that i ≥ p; this will imply the claim.\nSuppose on the contrary that i < p. Denote γ = βi:s+1, then γ ∈ Πs+1 and γ = +αs+1. Therefore, y1:s+1 /∈ Xs+1(αs+1; Πs+1) (since y1:s+1 = ∗γ). However, this contradicts the assumption that y ∈ Ys+1 = Ys+1(αs+1)."
    }, {
      "heading" : "4 Computing marginals",
      "text" : "In this section we again consider the semiring (R,⊕,⊗) = (R,+,×) from Example 1 where all costs cα are strictly positive, and consider a probablity distribution p(x) = f(x)/Z over labelings x ∈ D1:n.\nFor a pattern α we define\nΩ(α) = {x ∈ D1:n | x = ∗α∗} (20) Z(α) = ∑ x∈Ω(α) f(x) (21)\nWe also define the set of patterns\nΠ = {α | ∃α∗, ∗α ∈ Π◦, α is non-empty} (22)\nNote that Π◦ ⊆ Π and |Πs| = |I(Γ)| for indexes s that are sufficiently far from the boundary. We will present an algorithm for computing Z(α) for all patterns α ∈ Π in time O(n ∑ α∈I(Γ) |α|). Marginal probabilities of a pattern-based CRF can then be computed as p(xi:j = α) = Z(α)/Z for a pattern α = ([i, j], ·).\nIn the previous section we used graph G[Πs] for a set of patterns Πs; here we will need an analogous but a slightly different construction for patterns in Π. For patterns α, β we write α v β if β = ∗α∗. If we have β = +α+ then we write α < β.\nNow consider α ∈ Π. We define Φ(α) to be the set of patterns β ∈ Π such that α < β and there is no other pattern γ ∈ Π with α < γ v β.\nOur algorithm is given below. In the first step it runs Algorithm 1 from left to right and from\nright to left; as a result, we get forward messages −→ W j(α) and backward messages ←− W i(α) for patterns α = ([i, j], ·) such that −→ W j(α) =\n∑ x=∗α x∈D1:j f(x) ←− W i(α) = ∑ y=α∗ y∈Di:n f(y) (23)\nAlgorithm 3 Computing values Z(α)\n1: run Algorithm 1 in both directions to get messages −→ W j(α), ←− W i(α). For each pattern α = ([i, j], ·) ∈\nΠ set\nW (α) :=\n−→ W j(α) ←− W i(α)\nf(α) (24a)\nW−(α) :=\n−→ W j−1(αi:j−1) ←− W i+1(αi+1:j)\nf(αi+1:j−1) (24b)\n2: for α∈Π (in the order of decreasing |α|) set\nZ(α) := W (α) + ∑\nβ∈Φ(α)\n[ Z(β)−W−(β) ] (25)\nTheorem 6. Algorithm 3 is correct.\nWe prove the theorem in section 4.1, but first let us discuss algorithm’s complexity. We claim that all values f(α) used by the algorithm can be computed in O(n(P + S)) time where P and S are respectively the number of distinct non-empty prefixes and suffixes of words in Γ. Indeed, we first compute these values for patterns in the set −→ Π , {α | ∃α∗ ∈ Π◦}; by Lemma 1, this takes O(nP ) time. This covers values f(α) used in eq. (24a). As for the value in eq. (24b) for pattern α = ([i, j], ·) ∈ Π, we can use the formula\nf(αi+1:j−1) = f(α)c̃α −→ φ (α) ←− φ (α)\nwhere c̃α = cα if α ∈ Π◦ and c̃α = 1 otherwise, and −→ φ (α) = ∏ β∈Π◦,α=∗β cβ , ←− φ (α) = ∏ β∈Π◦,α=β∗ cβ\nThe latter values can be computed in O(n(P + S)) time by applying Lemma 1 in the forward and backward directions. (In fact, they were already computed when running Algorithm 1.)\nWe showed that step 1 can be implemented in O(n(P + S)) time; let us analyze step 2. The following lemma implies that it performs O(n ∑ α∈I(Γ) |α|) arithmetic operations; since ∑ α∈I(Γ) |α| ≥∑\nα∈Γ |α| ≥ max {P, S}, we then get that the overall complexity is O(n ∑ α∈I(Γ) |α|).\nLemma 7. For each β ∈ Π there exist at most 2|β| patterns α ∈ Π such that β ∈ Φ(α).\nProof. Let Ψ be the set of such patterns α. Note, there holds β = +α+. We need to show that m , |Ψ| ≤ 2|β|. Let us order patterns α = ([i, j], ·) ∈ Ψ lexicographically (first by i, then by j): Ψ = {α1, . . . , αm} with αt = ([it, jt], ·), and denote σt = (it − k) + (jt − k) ∈ [2, 2(` − k − 1)] where [k, `] is the interval for β. We will prove by induction that σt ≥ t + 1 for t ∈ [1,m]; this will imply that m+ 1 ≤ σm ≤ 2(`− k − 1) = 2(|β| − 2), as desired.\nThe base case is trivial. Suppose that it holds for t − 1; let us prove it for t. If it = it−1 then jt > jt−1 by the definition of the order on Ψ, so the claim holds. Suppose that it > it−1. If jt < jt−1 then αt < αt−1 < β contradicting the condition β ∈ Φ(αt). Thus, jt ≥ jt−1, and so the claim of the induction step holds.\nRemark 1 An alternative method for computing marginals with complexity O ( n|Γ|L2`2max ) was given in [10]. They compute value Z(α) directly from messages −→ M j′(·) and ←− M i′(·) by summing over pairs of patterns (thus the square factor in the complexity). In contrast, we use a recursive rule that uses previously computed values of Z(·). We also use the existence of the “ ” operation. This allows us to achieve better complexity."
    }, {
      "heading" : "4.1 Proof of theorem 6",
      "text" : "Consider labeling x ∈ D1:n. We define Λ(x) = {α ∈ Π◦ | x = ∗α∗} to be the set of patterns contained in x. For an interval [i, j] ⊆ [1, n] we also define sets\nΛij(x) = {β ∈ Λ(x) | β = +xi:j+} (26a) Λ−ij(x) = {β ∈ Λ(x) | β = ∗xi:j∗} (26b)\nand corresponding costs\nfij(x) = ∏\nβ∈Λ(x)−Λij(x)\ncβ (27a)\nf−ij (x) = ∏\nβ∈Λ(x)−Λ−ij(x)\ncβ (27b)\nIt can be checked that quantities W (α), W−(α) defined via (23) and (24) satisfy\nW (α) = ∑\nx∈Ω(α)\nfij(x) W −(α) = ∑ x∈Ω(α) f−ij (x) (28)\nwhere [i, j] is the interval for α. Consider pattern α = ([i, j], ·) ∈ Π. We will show that for any x ∈ Ω(α) there holds\nf(x) = fij(x) + ∑\nβ=([k,`],·)∈Φ(α):x=∗β∗\n[ f(x)− f−k`(x) ] (29)\nThis will be sufficient for establishing algorithm’s correctness: summing these equations over x ∈ Ω(α) and using (21),(28) yields eq. (25).\nLemma 8. The sum in (29) contains at most one pattern β = ([k, `], ·) ∈ Φ(α) with x = ∗β∗.\nProof. Consider two such patterns β1 = ([k1, `1], ·) and β2 = ([k2, `2], ·). Define k = max{k1, k2}, `=min{`1, `2}, β=xk:`, then α<βvβt for t∈{1, 2}. Using the definition of set Π, it can be checked that β ∈ Π. The fact that βt ∈ Φ(α) then implies that βt = β for t ∈ {1, 2}, and so β1 = β2.\nWe now consider two possible cases. Case 1: There are no patterns β = ([k, `], ·) ∈ Φ(α) with x = ∗β∗. This implies that Λij(x) is empty, and therefore f(x) = fij(x). Eq. (29) thus holds. Case 2: There exists a (unique) pattern β = ([k, `], ·) ∈ Φ(α) with x = ∗β∗. Eq. (29) then becomes equivalent to the condition fij(x) = f − k`(x). We will prove this by showing that Λij(x) = Λ − k`(x).\nThe inclusion Λ−k`(x) ⊆ Λij(x) is obvious; let us show the other direction. Suppose that γ = ([p, q], ·) ∈ Λij(x). Define p̂ = max{k, p}, q̂ = min{q, `}, γ̂ = xp̂:q̂. It can be checked that γ̂ ∈ Π. We also have α < γ̂ v β. Therefore, condition β ∈ Φ(α) implies that γ̂ = β, and so p ≤ k, q ≥ `, and γ ∈ Λ−k`(x).\n5 General case: O(nP ′|D|) algorithm In this section and in the next one we consider the case of a general commutative semiring (R,⊕,⊗) (without assuming the existence of an inverse operation for ⊕). This can be used for computing MAP in CRFs containing positive costs cα. The algorithm closely resembles the method in [10]; it is based on the same idea and has the same complexity. Our primary goal of presenting this algorithm is to motivate the O(nP log(`max + 1)) algorithm for the MAP problem given in the next section.\nFirst, we select Π as the set of proper prefixes of patterns in Π◦:\nΠ = {α | ∃α+ ∈ Π◦} (30)\nFor each α ∈ Πs we will compute message\nMs(α) = ⊕\nx∈Xs(α;Πs)\nf(x) (31)\nIn order to go from step s−1 to s, we will use an extended set of patterns Π̂s:\nΠ̂s = {α | α− ∈ Πs−1} ∪ {εs} (32) = {αc | α ∈ Πs−1, c ∈ Ds:s} ∪ {εs}\nIt can be checked that Πs ⊆ Π̂s and Π◦s ⊆ Π̂s (33)\nIn step s we compute values Ms(α) in eq. (31) for all α ∈ Π̂s. Note, we now use the generalized definition of Xs(α; Πs) (eq. (9)) since we may have α /∈ Πs. After completing step s, messages Ms(α) for α ∈ Π̂s−Πs can be discarded.\nOur algorithm is given below. We have |Π̂s|=P ′|D|+ 1 for indexes s that are sufficiently far from the boundary, and thus the algorithm’s complexity is Θ(nP ′|D|) (if Lemma 1 is used for computing values φ(α)). Algorithm 4 Computing Z = ⊕\nx∈D1:n f(x)\n1: initialize messages: set M0(ε0) := O 2: for s = 1, . . . , n traverse nodes α ∈ Π̂s of tree G[Π̂s] starting from the leaves and set\nMs(α) := [ φ(α)⊗Ms−1(α−) ] ⊕ ⊕ (α,β)∈E[Π̂s],β /∈Πs Ms(β) (34)\nIf α = εs then use Ms−1(α −) = O 3: return Z := ⊕\nα∈ΠnMn(α)\nTheorem 9. Algorithm 4 is correct.\nRemark 2 As we already mentioned, Algorithm 4 resembles the algorithm in [10]. The latter computes the same set of messages as we do but using the following recursion: for a pattern α ∈ Πs − {εs} they set\nMs(α) := ⊕\nγ∈Ts−1(α−)− ⋃\n(α,β)∈E[Πs] Ts−1(β−)\nφ(γa)⊗Ms−1(γ) (35)\nwhere a = αs:s is the last letter of α and Ts−1(β) = {γ | γ = ∗β, γ ∈ Πs−1} for β ∈ Πs−1 is the set of patterns in the branch of G[Πs−1] rooted at β. It can be shown that updates (34) and (35) are equivalent: they need exactly the same number of additions (and the same number of multiplications, if φ(γa) in eq. (35) is replaced with φ(α) and moved before the sum)."
    }, {
      "heading" : "5.1 Proof of Theorem 9",
      "text" : "To prove the correctness, we need to show that eq. (34) holds for each α ∈ Π̂s.\nLemma 10. For any α ∈ Π̂s there holds\nφ(α)⊗Ms−1(α−) = ⊕\nx∈Xs(α;Π̂s)\nf(x) (36)\nProof. For α = εs the claim is trivial: we have D s:s ⊆ Π̂s (since εs−1 ∈ Πs−1), therefore Xs(α; Π̂s) = ∅ and the sum in (36) is O. We thus assume that α ∈ Π̂s−{εs}. Using definition (33), it can be checked that the mapping x 7→ x− is a bijection Xs(α; Π̂s) → Xs−1(α−; Πs−1). Consider x ∈ Xs(α; Π̂s). We claim that if x = ∗γ and γ ∈ Π◦ then |γ| ≤ |α|. Indeed, we have γ ∈ Π̂s (since Π◦s ⊆ Π̂s), and so if |γ| > |α| then x /∈ Xs(α; Π̂s) - a contradiction.\nUsing the claim, we conclude that φ(α)⊗ f(x−) = f(x). This implies the lemma.\nThe fact Πs ⊆ Π̂s implies the following characterization of Xs(α; Πs) for α ∈ Π̂s:\nXs(α; Πs) = { x ∈ Xs(α)\nx 6= ∗β for any β in the subtree of α in G[Π̂s] with β ∈ Πs, β 6= α } where the subtree of α in G[Π̂s] is defined as the set of descendants of α in G[Π̂s] (including α).\nNow it becomes clear that Xs(α; Π̂s) ⊆ Xs(α; Πs) and Xs(α; Πs)−Xs(α; Π̂s) equals the set of partial labelings x ∈ Xs(α) such that\n• x ends with β ∈ Π̂s −Πs, (α, β) ∈ E[Π̂s], and\n• x does not end with any pattern γ ∈ Πs from the subtree of β in G[Π̂s].\nIt is easy to check that the last set of partial labelings equals Xs(β; Πs), and such sets for different β’s are disjoint. We showed that Xs(α; Πs) is a disjoint union of sets Xs(α; Π̂s) and Xs(β; Πs) for (α, β) ∈ E[Π̂s], β /∈ Πs. This fact together with Lemma 10 implies eq. (34).\n6 General case: O(nP logP ) algorithm\nIn the previous section we presented an algorithm for a general commutative semiring with complexity O(nP ′|D|). In some applications the size of the input alphabet can be very large (e.g. hundreds or thousands), so the technique may be very costly. Below we present a more complicated version with complexity O(nP logP ). If (R,⊕,⊗) = (R,min,+) then this can be reduced to O(nP log(`max + 1)) using the algorithm for Range Minimum Queries by [1]. We assume that D ⊆ Γ.\nWe will use the same definitions of sets Πs and Π̂s as in the previous section, and the same intepretation of messages Ms(α) given by eq. (31). We need to solve the following problem: given messages Ms−1(α) for α ∈ Πs−1, compute messages Ms(α) for α ∈ Πs.\nRecall that in the previous section this was done by computing messages Ms(α) for patterns in the extended set Π̂s of size O(P\n′|D|). The idea of our modification is to compute these messages only for patterns in the set Σs where\nΣ◦s ⊆ Σs ⊆ Π̂s Σ◦s = Πs ∪Π◦s |Σs| ≤ 2|Σ◦s|\nNote that |Σ◦s| ≤ P +1. Patterns in Σs will be called special. To define them, we will use the following notation for a node α ∈ Π̂s:\n• Φ̂s(α) is the set of children of α in the tree G[Π̂s].\n• T̂s(α) is the set of nodes in the subtree of G[Π̂s] rooted at α. We have α ∈ T̂s(α) ⊆ Π̂s.\nWe now define set Σs as follows: pattern α ∈ Π̂s is special if either (i) α ∈ Σ◦s, or (ii) α has at least two children β1, β2 ∈ Φ̂s(α) such that subtree T̂s(βi) for i ∈ {1, 2} contains a pattern from Σ◦s, i.e. T̂s(βi) ∩ Σ◦s 6= ∅.\nThe set of remaining patterns Π̂s − Σs will be split into two sets As and Bs as follows:\n• As is the set of patterns α ∈ Π̂s − Σ◦s such that subtree T̂s(α) does not contain patterns from Σ◦s.\n• Bs is the set of patterns α ∈ Π̂s − Σ◦s such that α has exactly one child β in G[Π̂s] for which T̂s(β) ∩ Σ◦s 6=∅.\nClearly, Π̂s is a disjoint union of As, Bs and Σs. Consider a node α ∈ Bs. From the definition, α has exactly one link to a child in G[Π̂s] that belongs to Bs ∪ Σs. If this child does not belong to Σs, then it belongs to Bs and the same argument can be repeated for it. By following such links we eventually get to a node in Σs; the first such node will be denoted as α↓.\nWe will need two more definitions. For an index t and patterns α, β ending at position t with β = +α we denote\nWt(α) = ⊕\nx∈Xt(α)\nf(x) (37a)\nVt(α, β) = ⊕\nx∈Xt(α)−Xt(β)\nf(x) (37b)\nWe can now formulate the structure of the algorithm (see Algorithm 5).\nAlgorithm 5 Computing Z = ⊕\nx∈D1:n f(x)\n1: initialize messages: set M0(ε) := O 2: for each s = 1, . . . , n traverse nodes α ∈ Σs of tree G[Σs] starting from the leaves and set\nMs(α) := φ(α)⊗ [Ms−1(α−)⊕As(α)⊕Bs(α)]⊕ ⊕\n(α,β)∈E[Σs],β /∈Πs\nMs(β) (38)\nwhere\nAs(α) = ⊕\nβ∈Φ̂s(α)∩As\nWs−1(β −) (39a)\nBs(α) = ⊕\nβ∈Φ̂s(α)∩Bs\nVs−1(β −, β↓ −) (39b)\nIf α = εs then use Ms−1(α −) := O 3: return Z := ⊕\nα∈ΠnMn(α)\nTo fully specify the algorithm, we still need to describe how we compute quantities As(α) and Bs(α) defined by eq. (39a) and (39b). This is addressed by the theorem below.\nTheorem 11. (a) Algorithm 5 is correct. (b) There holds |Σs| ≤ 2|Σ◦s| − 1 ≤ 2P + 1. (c) Let h be the maximum depth of tree G[Πs−1]. (Note, h ≤ `max + 1.) With an O(P ′ log h) preprocessing, values Vs−1(α, β) for any α, β ∈ Πs−1 with β = +α can be computed in O(log h) time. (d) Values As(α) for all α ∈ Σs can be computed in O(P logP ) time, or in O(P ) time when (R,⊕,⊗)= (R,min,+).\nClearly, the theorem implies that the algorithm can be implemented in O(nP logP ) time, or in O(nP log(`max + 1)) time when (R,⊕,⊗)=(R,min,+). To see this, observe that the sum in (39b) is effectively over a subset of children of α in the tree G[Σs] (see Fig. 2), and this tree has size O(P ).\nBefore presenting the proof, let us make a few remarks. To give some insights into how we prove the theorem, let us make a few remarks.\nIt can be easily checked that graph G[Π̂s] has the following structure: the root εs has |D| children, and for each child c ∈ Ds:s ⊆ Π̂s the subtree of G[Π̂s] rooted at c is isomorphismic to the tree G[Πs−1]. The isomorphism T̂s(c)→ Πs−1 is given by the the mapping α 7→ α−.\nFor nodes α, β ∈ Πs−1 with β = +α we denote Ps−1(α, β) to be the unique path from α to β in G[Πs−1] (treated as a set of edges in E[Πs−1]). Analogously, for nodes α, β ∈ Π̂s with β = +α let P̂s(α, β) be the unique path from α to β in G[Π̂s]. It follows from the previous paragraph that if α 6= εs then path P̂s(α, β) is isomorphic to the path Ps−1(α−, β−)."
    }, {
      "heading" : "6.1 Proof of Theorem 11(a)",
      "text" : "The statement is equivalent to the correctness of (38). Let us first divide the sum over β in the last expression (38) into two parts: nodes β that belong to Φ̂(α) and those that do not:⊕\nβ∈Φ̂s(α)∩(Σs−Πs)\nMs(β)⊕ ⊕\n(α,β)∈E[Σs],β /∈Πs∪Φs(α)\nMs(β) (40)\nIt is easy to see that the second sum can be written as ∑\nβ∈Φ̂s(α)∩BsM s(β↓) (see Fig. 2), where\nM s(γ) = { Ms(γ) if γ /∈ Πs O otherwise\n(41)\nUsing the distributive law, we can rewrite (38) as Ms(α) := [ φ(α)⊗Ms−1(α−) ] ⊕\n⊕ β∈Φ̂s(α)∩As [φ(α)⊗Ws−1(β−)]\n⊕ ⊕\nβ∈Φ̂s(α)∩Bs\n[φ(α)⊗ Vs−1(β−, β↓−)]⊕M s(β↓)\n⊕ ⊕\nβ∈Φ̂s(α)∩(Σs−Πs)\nMs(β) (42)\nWe will use eq. (34) (for which the correctness is already proved) for the case when α ∈ Σs. We can rewrite it as follows (we use the fact that As ∩Πs = Bs ∩Πs = ∅):\nMs(α) := [ φ(α)⊗Ms−1(α−) ] ⊕ ⊕ β∈Φ̂s(α)∩As Ms(β)\n⊕ ⊕\nβ∈Φ̂s(α)∩Bs\nMs(β) ⊕ ⊕\nβ∈Φ̂s(α)∩(Σs−Πs)\nMs(β) (43)\nThe first and the last terms of the sum in (43) equal to that of the sum in (42). The lemma below implies that the same holds for the second and third terms, thus proving the correctness of eq. (42).\nLemma 12. (a) For any α ∈ As there holds\nMs(α) = φ(α)⊗Ws−1(α−) (44)\n(b) For any α ∈ Bs there holds\nMs(α) = [φ(α)⊗ Vs−1(α−, α↓−)]⊕M s (α↓) (45)\nProof. Part (a) Suppose that α ∈ As. This means that there are no patterns β ∈ Π◦s of the form β = +α (recall that Π◦s ⊆ Σ◦s). This in turn implies that Ms(α) = Ws(α). This also implies that for any x ∈ Xs(α) there holds f(x) = f(x−), and consequently Ws(α) = φ(α)⊗Ws−1(α−). Part (b) Suppose that α ∈ Bs. The definition of Bs implies that set T̂s(α)− T̂s(α↓) does not contain nodes in Πs or in Π ◦ s. Using this fact and the definition of sets Xs(·), Xs(·; ·) we get the following.\n(i) If α↓ ∈ Πs then Xs(α; Πs) = Xs(α)−Xs(α↓). (ii) If α↓ /∈ Πs then Xs(α; Πs) is a disjoint union of Xs(α)−Xs(α↓) and Xs(α↓; Πs). (iii) Partial labeling x ∈ Xs(α) − Xs(α↓) cannot end with a pattern β = +α ∈ Π◦s. (If such β exists then from the fact above we get β ∈ T̂s(α↓), i.e. β = ∗α↓ and x ∈ Xs(α↓) - a contradiction.) Therefore, for such x we have f(x) = φ(α) ⊗ f(x−). This implies that ∑ x∈Xs(α)−Xs(α↓) f(x) =\nφ(α)⊗ Vs−1(α−, α↓−). Recall that Ms(α) = ∑\nx∈Xs(α;Πs) f(x). Using this fact and properties (i)-(iii), we conclude that (45) holds in each of the two cases (α↓∈Πs and α↓ /∈Πs)."
    }, {
      "heading" : "6.2 Proof of Theorem 11(b)",
      "text" : "For a node α ∈ Π̂s we denote T ◦s (α) = T̂s(α) ∩ Σ◦s. Let us consider the process of a breadth-first search in the tree G[Π̂s] starting from the root. At each step we will keep a certain set of nodes of the tree (which we call active nodes), and the transition to the next step is made by choosing one of the active nodes and replacing it with its children. The process stops when the set of active nodes becomes equal to the set of the leaves of the tree. To each step of the process we correspond a partition of the set Σ◦s. The partition is defined by the following\nrule: if α1, . . . , αk are active nodes, then the partition is Σ ◦ s = k⋃ `=1 T ◦s (α`) ⋃ α∈Σ◦s ,α/∈T ◦s (α`),`=1,k {α}. Let us denote the partition at step t as Dt. Partitions of the set Πs is a poset with respect to the natural order defined as follows: {Si}i∈I ≤ {Pj}j∈J if for any i ∈ I there is j ∈ J such that Si ⊆ Pj . It is easy to see that D0 ≥ D1 ≥ D2 ≥ . . .. Moreover, if a chosen active node α at step t is a special one and does not belong to Σ◦s then Dt > Dt+1. Indeed, there are at least two children of α, denoted as α1 and α2, such that sets T ◦ s (α1) and T ◦ s (α2) are nonempty (by the definition of a special node). As step t+ 1 these sets are separate components of partition Dt+1, but at step t these sets still belong to the same component of Dt; thus, Dt 6= Dt+1.\nWe know that the length of a chain Dt1 > Dt2 > . . . cannot exceed |Σ◦s|. We conclude that the number of special patterns that do not belong to Σ◦s is bounded by |Σ◦s|−1; this implies Theorem 11(b)."
    }, {
      "heading" : "6.3 Proof of Theorem 11(c)",
      "text" : "For brevity denote t = s− 1, and define a set of pairs\nJ = {(α, β) | α, β ∈ Πt, β is a strict descendant of α in G[Πt]}\nNote that E[Πt] ⊆ J . In this section we describe a O(P ′ log h) preprocessing which will allow computing values Vt(α, β) for any (α, β) ∈ J in O(log h) time. The procedure will be based on the following observation; it follows trivially from the definition (37b).\nLemma 13. For any (α, β) ∈ J there holds\nVt(α, β) = ⊕\n(α′,β′)∈Pt(α,β)\nVt(α ′, β′) (46)\nDuring preprocessing we will compute values Vt(α, β) for pairs (α, β) in a certain set J̃ ⊆ J of size O(P ′ log h). In order to define J̃ , we need some notation. For a pattern α ∈ Πt let hα = |Pt(εt, α)| be the height of α in G[Πt], and for an integer d ∈ [0, hα] let α↑d be the node of tree G[Πt] obtained from α by taking d steps towards the root. We now define\nJ̃ = {(α↑d, α) | α ∈ Πt, d ∈ [0, hα], d = 2r for r ∈ Z≥0}\nThe preprocessing will consist of 3 steps. Step 1: compute values Wt(α) for all α ∈ Πt. We do it by traversing nodes α ∈ Πt of tree G[Πt] and setting\nWt(α) := Mt(α)⊕ ⊕\n(α,β)∈E[Πt]\nWt(β) (47)\nThis takes O(P ′) time. Step 2: go through α ∈ Πt and compute values Vt(α, β) for all β ∈ Φt(α) where Φt(α) is the set of children of α in G[Πt].\nA naive way is to use the formula Vt(α, β) = Mt(α)⊕ ⊕\nγ∈Φt(α)−{β}\nWt(γ)\nfor all β ∈ Γt(α); however, this would take O(k2) time where k = |Φt(α)|. Instead, we do the following. Let us order patterns in Φt(α) arbitrarily: Φt(α) = {β1, . . . , βk}. For i ∈ [1, k] denote\n−→ S i = i−1⊕ j=1 Wt(βj) ←− S i = k⊕ j=i+1 Wt(βj)\nWe compute these values in O(k) time by setting −→ S 1 := O, ←− S k := O and then using recursions\n−→ S i+1 := −→ S i ⊕Wt(βi)\n←− S i−1 := ←− S i ⊕Wt(βi)\nAfter that we set Vt(α, βi) := Mt(α)⊕ −→ S i ⊕ ←− S i\nFor a given α ∈ Πt the procedure takes O(|Φt(α)|) time, and thus for all α ∈ Πt it takes O(P ′) time. We now have values Vt(α, β) for all (α, β) ∈ E[Πt]. Step 3: compute values Vt(α, β) for all (α, β) ∈ J̃ using the recursion\nVt(α ↑2d, α) := Vt(α ↑2d, α↑d) + Vt(α ↑d, α)\nfor d = 20, 21, . . . , 2r, . . . and (α↑2d, α) ∈ J̃ . Evaluating queries for (α, β) ∈ J We showed how to compute values Vt(α, β) for (α, β) ∈ J̃ in time O(P ′ log h); let us now describe how to compute value Vt(α, β) for a given (α, β) ∈ J in time O(log h). Let us construct a sequence β0, β1, . . . , as follows: β0 = β, and for i ≥ 0 let βi+1 = β↑di where d is the maximum value such that d = 2r for r ∈ Z≥0 and β↑di is still a descendant of α. We stop when we get βk = α; clearly, this happens after k = O(log h) steps. We now set\nVt(α, β) := k−1⊕ i=0 Vt(βi+1, βi)"
    }, {
      "heading" : "6.4 Proof of Theorem 11(d)",
      "text" : "We will consider the general case of a commutative semiring and the case when (R,⊕,⊗)=(R,min,+); the latter will be called the MAP case.\nLet dα for α ∈ Πs−1 be the number of children of α in G[Πs−1], dmax = maxα∈Πs−1 dα, and d̃α for α ∈ Σs be the number of children of α in G[Σs]. We will present a O( ∑ α∈Πs−1 dα log dα) preprocessing technique that will allow computing value As(α) for α ∈ Σs − {εs} in time O((d̃α + 1) log dα−). The resulting complexity will be\nO( ∑\nα∈Πs−1 dα log dmax) + ∑ α∈Σs O((d̃α + 1) log dmax) = O(P log dmax)\nIn the MAP case (i.e. when (R,⊕,⊗) = (R,min,+)) we will present a faster solution. Namely, the preprocessing will take O( ∑ α∈Πs−1 dα) = O(P\n′) time, and computing value As(α) for α ∈ Σs − {εs} will take O(d̃α + 1) time, leading to the overall complexity O(P ). For that we will use the Range Minimum Query (RMQ) problem which is defined as follows: given N numbers z1, . . . , zN , compute\nmink∈I zk for a given interval I = [i, j] ⊆ [1, N ]. It is known [1] that with an O(N) preprocessing each query for can be answered in O(1) time per interval.\nAs in the previous section, we denote t = s− 1. We assume that for each α ∈ Πt we already have values Wt(α) for all α ∈ Πt (they were computed in the previous section). Preprocessing Consider α ∈ Πt, and let us fix an ordering of children of α: Φt(α) = {β1, . . . , βd} where d = dα. For an interval I ⊆ [1, d] we denote\nSI(α) = ⊕ i∈I Wt(βi) (48)\nThe goal of preprocessing is to build a data structure that we will allow an efficient computation of SI(α) for any given interval I.\nIn the MAP case we simply run the preprocessing of [1] for the sequence Wt(β1), . . . ,Wt(βd); this takes O(d) time. Value SI(α) for an interval I can then be computed in O(1) time.\nIn the general case we do the following. Define a set of intervals\nJd = {[i, j] ⊆ [1, d] | j − i = 2r − 1, r ∈ Z≥0}\nNote that |Jd| = O(d log d). We compute quantities SI(α) for all I ∈ Jd. This can be done in O(d log d) time by setting S[i,i](α) := Wt(βi) for i ∈ [1, d] and then using recursions\nS[i,i+2δ−1](α) = S[i,i+δ−1](α)⊕ S[i+δ,i+2δ−1](α)\nfor δ = 20, 21, . . .. Value SI(α) for an interval I can now be computed in O(log d) time. Indeed, we can represent I\nas a disjoint union of m = O(log d) intervals from Jd: I = ⋃m i=1 Ii with Ii ∈ Jd. We can then use the formula\nSI(α) = m⊕ i=1 SIi(α) (49)\nComputing As(α) for α ∈ Σs−{εs} Denote d = dα− . As discussed earlier, Φ̂s(α) (the set of children of α in G[Π̂s]) is isomorphic to Φs−1(α\n−). Let Γ̂s(α) = {β1, . . . , βd} be the ordering of patterns in Φ̂s(α) such that β − 1 , . . . , β − d is the ordering of patterns in Φs−1(α\n−) chosen in the preprocessing step. We need to compute\nAs(α) = ⊕ i∈J Ws−1(β − i ) , J , {i ∈ [1, d] | βi ∈ As}\nDenote J = [1, d] − J . We can represent J as a disjoint union of at most |J | + 1 intervals I1, . . . , Im where Ii ⊆ [1, p]. Clearly, |J | = d̃α (see Fig. 2), and so m ≤ d̃α + 1. We can write\nAs(α) = m⊕ i=1 SIi(α −) (50)\nAs discussed above, each value SIi(α −) can be computed in O(1) time in the MAP case and in O(log d) in the general case. Thus, computing As(α) takes respectively O(d̃α+1) and O((d̃α+1) log dα−) time, as desired."
    }, {
      "heading" : "7 MAP for non-positive costs",
      "text" : "In this section we assume that (R,⊕,⊗)=(R, min,+) and cα ≤ 0 for all α ∈ Π◦. [2] gave an algorithm that makes Θ(nP ) comparisons and Θ(nP ) additions. We will present a modification that makes only O(n|I(Γ)|) comparisons. The number of additions in general will still be O(nP ), but we will show that in certain scenarios it can be reduced using a Fast Fourier Transform (FFT).\nWe will assume that Γ contains at least one word α with |α| = 1 (it can always be added if needed). As usual, we first select a set of patterns Π with Π0 = {ε0}; this step will be described later. For a pattern α ∈ Π let α← be the longest proper prefix of α that is in Π (α = α←+, α← ∈ Π). If Π does not contains proper prefixes of α then α← is undefined.\nWe can now present the algorithm.\nAlgorithm 6 Computing Z = min x∈D1:n\nf(x) (if cα ≤ 0)\n1: initialize messages: set M0(ε0) := 0 2: for s = 1, . . . , n traverse nodes α ∈ Πs of forest G[Πs] starting from the leaves and set\nMs(α) := min{Mp(α←) + ψ(α), min (α,β)∈E[Πs] Ms(β)} (51)\nwhere p is the end position of α← (α← = ([·, p], ·)) and ψ(α) = f(α)− f(α←). If α← is undefined then ignore the first expression in (51).\n3: return Z := minα∈ΠnMn(α)\nSelecting Π It remains to specify how to choose set Π. For patterns α, β, γ we define\n〈α|β|γ〉 = {u = +β + | αβγ = ∗u∗} (52)\nTheorem 14. Suppose that Π0 = {ε0} and set Π contains set\nΠ̃ = { β ∃ labeling xαβγy ∈ D1:n s.t. (a) αβ, βγ ∈ Π◦; (b) 〈xα|β|γy〉 ∩Π◦ = ∅ } (53)\nThen Alg. 6 returns the correct value of Z= min x∈D1:n f(x).\nA proof of this theorem is given in Sec. 7.2. A simple valid option is to set Π = {α | ∃α∗, ∗α ∈ Π◦}. Computing set Π̃ is slightly more\ncomplicated, but can still be done in polynomial time for a given Π (we omit this procedure). In order to analyze set Π̃, let us define\nIδ = { β ∃ word xαβγy with |x| = |y| = δ s.t. (a) αβ, βγ ∈ Γ; (b) 〈xα|β|γy〉 ∩ Γ = ∅ } where set 〈·|·|·〉 for words is defined similarly to (52):\n〈α|β|γ〉 = {α̂βγ̂ | α = ∗α̂, γ = γ̂∗ and α̂, γ̂ 6= ε} (54)\nAs δ increases, set Iδ monotonically shrinks, and stops changing after δ ≥ `max = maxα∈Γ |α|. We denote this limit set as I∞, so that I∞ ⊆ I0 ⊆ I(Γ)∪{ε}. It can be seen that Π̃s = {([·, s], α) |α ∈ I∞} for all s ∈ [`max, n−`max+1]. Complexity Assume that we use Π = Π̃. The algorithm performs two types of operations: comparisons (to compute minima) and arithmetic operations (to compute the first expression in (51)). The\nnumber of comparisons does not exceed the total number of edges in graphs G[Πs] = (Πs, E[Πs]) for s ∈ [1, n], which is smaller than the number of nodes (since graphs are forests). Thus, comparisons take O(n|I∞|) time.\nThe time for arithmetic operations depends on how we compute quantities f(α). One possible approach is to use Lemma 1 for computing f(α) for all α ∈ Π̂ where Π̂ is the set of prefixes of patterns in Π◦ (note that Π ⊆ Π̂). We have |Π̂s| ≤ P + 1, and therefore the resulting overall complexity is O(nP ). Next, we describe an alternative approach based on a Fast Fourier Transform."
    }, {
      "heading" : "7.1 Computing f(α) using FFT",
      "text" : "For a word α and index s ≥ |α| let αs be the pattern ([s− |α|+ 1, s], α). It is easy to see that\nf(αs) = ∑ β∈Γ fs(α|β) where fs(α|β) = ∑ t:αs=∗βt∗ cβt\nLemma 15. For fixed words α, β quantities fs(α|β) for s ∈ [1, n] can be computed in O(n log n) time.\nProof. We assume that |α| ≥ |β|, otherwise the claim is trivial. Let p = |α| − |β| + 1, and define sequences a ∈ Rn−|α|+1, b ∈ Rn−|β|+1, λ ∈ {0, 1}p via\nai = fi+|α|(α|β) ∀i ∈ [1, n−|α|] bj = c([j,j+|β|−1],β) ∀j ∈ [1, n−|β|+1] λk = [αk:k+|β|−1 = β] ∀k ∈ [1, p]\nwhere [·] is the Iverson bracket. It can be checked that\nai = p∑ k=1 bi+kλk ∀i ∈ [1, n−|α| ]\nThus, a is the convolution of b and the reverse of λ. The convolution of such sequences can be computed in O(n log n) using a Fast Fourier Transform.\nIn practice this method can be useful for computing fs(α|β) when |α| |β|. A natural way to do this is first choose a subset S ⊆ Γ of words that are very often included as subwords in words from I∞ (usually, this means S = {β | β ∈ Γ, |β| ≤ δ} where δ is some threshold constant). Then computing fs(α|β) for all α ∈ I∞ and β ∈ S will take O (|I∞| · |S|n log n) time. For β ∈ Γ−S quantities fs(α|β) can be computed directly.\nAn example of using such an approach is the following theorem; it is proved by taking δ = 1 and S = D.\nTheorem 16. Suppose Γ = D ∪ A where A consists of words of a fixed length `. Then Algorithm 6 can be implemented in O (|I∞| · |D|n log n) time."
    }, {
      "heading" : "7.2 Proof of Theorem 14 (correctness)",
      "text" : "First, let us prove that for all patterns α = ([·, s], ·) ∈ Π there holds\nMs(α) ≥ min x=∗α∈D1:s f(x) (55)\nWe use induction on the order used in the algorithm. The base case α = ε0 is ensured by the initialization step. Consider pattern α ∈ Π − {ε0}. Suppose that α← is defined; let p be the end\nposition of α←. Consider partial labeling y = ∗α← ∈ D1:p, and let x be its unique extension by s− p letters (x = y+) such that x = ∗α. Due to non-positivity of costs cβ we have\nf(x) = f(y) + ψ(α) + ∑\nβ=([i,j],·)∈Π◦ x=∗β∗ i≤s−|α|, j>p\ncβ ≤ f(y) + ψ(α)\nApplying the induction hypothesis and the inequality above yields the desired claim:\nMs(α) = min{Mp(α←) + ψ(α), min (α,β)∈E[Πs] Ms(β)}\n≥ min{ min y=∗α← f(y) + ψ(α), min (α,β)∈E[Πs] min x=∗β f(x)} ≥ min{min x=∗α f(x), min x=∗α f(x)} = min x=∗α f(x)\nwhere in the equations above x always denotes a partial labeling in D1:s and y denotes a partial labeling in D1:p. If α← is undefined then we can write similar inequalities but omitting expressions contaiting α←.\nBy applying the claim to patterns α = ([·, n], ·) ∈ Π we conclude that Z ≥ minx∈D1:n f(x). The remainder of this section is devoted to the proof of the reverse inequality: Z ≤ minx∈D1:n f(x).\nLet us fix x∗ ∈ arg minx∈D1:n f(x). Let\nΛ = {α ∈ Π◦ | x∗ = ∗α∗}\nbe the set of patterns present in x∗, and\nΛ̂ = {α ∈ Λ | there is no α̂ ∈ Λ− {α}with α̂ = ∗α∗}\nbe the set of maximal patterns in Λ. We can assume w.l.o.g. that for each k ∈ [1, n] there exists α = ([i, j], ·) ∈ Λ̂ with k ∈ [i, j]. Indeed, if it is not the case for some k then we can modify x∗ by replacing the k-th letter of x∗ with some letter c ∈ Γ ∩D; this operation does not increase f(x∗).\nWe define a total order on patterns α = ([i, j], ·) ∈ Λ̂ as the lexicographical order with components (i, j) (the first component is more significant).\nLemma 17. (a) For each pattern β ∈ Λ̂ there holds β ∈ Π. (b) Consider two consecutive patterns α1 ≺ α2 in Λ̂ with α1 = ([i1, j1], ·), α2 = ([i2, j2], ·), and let β = x∗i2:j1 be the pattern at which they intersect. (By the assumption above, i2 ≤ j1 + 1). There holds β ∈ Π. (c) For the patterns in (b), condition Mj1(α1) ≤ f(x∗1:j1) implies Mj2(α2) ≤ f(x ∗ 1:j2 ).\nProof. Part (a) We can write labeling x∗ as x∗ = xαβγy where patterns α, β are empty. Let us show that this choice satisfies conditions in (53). Condition (a) holds since αβ = βγ = β ∈ Π◦. Suppose that (b) does not hold, then there exists pattern u = x∗k:` ∈ Π◦ with u = +β+. We have u ∈ Λ and thus β /∈ Λ̂ - a contradiction. Therefore, β ∈ Π. Part (b) The definitions of and Λ̂ imply that i1 < i2. (If i1 = i2 then we must have j1 < j2, but then α1 /∈ Λ̂ - a contradiction.) There also holds j1 < j2 (otherwise we would have α2 /∈ Λ̂ - a contradiction). This means that we can write labeling x∗ as x∗ = xαβγy where αβ = α1, βγ = α2 and the pattern intervals are as follows:\nx α β γ y\n[1, i1−1] [i1, i2−1] [i2, j1] [j1+1, j2] [j2+1, n]\nLet us show that this choice satisfies conditions in (53). Condition (a) holds since αβ = α1 ∈ Π◦ and βγ = α2 ∈ Π◦. Suppose that (b) does not hold, then there exists pattern u = x∗k:` ∈ Π◦ where k < i2 and ` > j1. This means that u ∈ Λ.\nFrom the definition of Λ̂, there exists pattern û = x∗ k̂:̂̀ ∈ Λ̂ with [k, `] ⊆ [k̂, ̂̀]. To summarize, we\nhave k̂ ≤ k < i2 and j1 < ` ≤ ̂̀. If k̂ ≤ i1 then [i1, j1] ⊂ [k̂, ̂̀] and thus α1 /∈ Λ̂ - a contradiction. Thus, there must hold k̂ > i1.\nSimilarly, we prove that ̂̀< j2. This implies that α1 ≺ û ≺ α2, and therefore patterns α1 and α2 are not consecutive in Λ̂ - a contradiction. Part (c) β is a proper prefix of α that belongs to Π. Therefore, α← is defined. We can define a sequence of patterns β0 = β, β1, . . . , βm = α2 with βk = ([i2, sk], ·), s0 = j1 < s1 < . . . < sm = j2 respectively such that βk ∈ Π and βk−1 = β←k for k ∈ [1,m]. Let us prove by induction on k that Msk(βk) ≤ f(x∗1:sk) for k ∈ [0,m].\nLet us first check the base of the induction. Since β, α1 ∈ Πj1 and α1 = ∗β, by the definition of graph G[Πj1 ] there is a (unique) path γ0, γ1, . . . , γr from γ0 = β to γr = α1 with (γl, γl+1) ∈ E[Πj1 ]. By eq. (51), Mj1(γl) ≤Mj1 (γl+1). Therefore, Mj1(β) ≤Mj1 (α1) ≤ f(x∗1:j1) where the last inequality holds by the assumption of part (c). This establishes the base case.\nNow suppose that the claim holds for k− 1 ∈ [0,m− 1]; let us prove it for k. Denote p = sk−1 and s = sk. Note, p is the index in step 2 of the algorithm during the processing of (s, βk). From eq. (51) and the induction hypothesis we get\nMs(βk) ≤Mp(βk−1) + ψ(βk) ≤ Fp(x∗1:p) + ψ(βk)\nWe will prove next that f(x∗1:p) + ψ(βk) = f(x ∗ 1:s), thus completing the induction step. It suffices to show that there is no pattern γ = x∗i:j with i < i2 and p < j ≤ s. Suppose on the contrary that such pattern exists. By the definition of Λ̂ there exists pattern γ̂ = ([̂i, ĵ], ·) ∈ Λ̂ with [i, j] ⊆ [̂i, ĵ]. We have î ≤ i < i2 and ĵ ≥ j > p ≥ s0 = j1. These facts imply that α1 ≺ γ̂ ≺ α2, contradicting the assumption that α1 and α2 are consecutive patterns in Λ̂.\nLemma 17 implies the main claim.\nCorollary 18. For each α = ([i, j], ·) ∈ Λ̂ there holds Mj(α) ≤ f(x∗1:j), and therefore Z ≤ f(x∗).\nProof. We use induction on the total order . The lowest pattern α ∈ Λ̂ starts at position 1; for such pattern the claim follows by inspecting Algorithm 6. The induction step follows from Lemma 17(c)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Herbert Edelsbrunner for helpful discussions."
    } ],
    "references" : [ {
      "title" : "Recursive star-tree parallel data structure",
      "author" : [ "Omer Berkman", "Uzi Vishkin" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1993
    }, {
      "title" : "Beyond pairwise energies: Efficient optimization for higher-order MRFs",
      "author" : [ "N. Komodakis", "N. Paragios" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J. Lafferty", "A. McCallum", "F Pereira" ],
      "venue" : "In ICML,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Semi-Markov conditional random field with high-order features",
      "author" : [ "Viet Cuong Nguyen", "Nan Ye", "Wee Sun Lee", "Hai Leong Chieu" ],
      "venue" : "In ICML 2011 Structured Sparsity: Learning and Inference Workshop,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Sparse higher order conditional random fields for improved sequence labeling",
      "author" : [ "Xian Qian", "Xiaoqian Jiang", "Qi Zhang", "Xuanjing Huang", "Lide Wu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Minimizing sparse higher order energy functions of discrete variables",
      "author" : [ "C. Rother", "P. Kohli", "W. Feng", "J. Jia" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Semi-Markov conditional random fields for information extraction",
      "author" : [ "S. Sarawagi", "W. Cohen" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "Inference algorithms for pattern-based CRFs on sequence data",
      "author" : [ "R. Takhanov", "V. Kolmogorov" ],
      "venue" : "In ICML,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "A linear algorithm for generating random numbers with a given distribution",
      "author" : [ "M.D. Vose" ],
      "venue" : "IEEE Transactions on software engineering,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1991
    }, {
      "title" : "Conditional random fields with high-order features for sequence labeling",
      "author" : [ "Nan Ye", "Wee Sun Lee", "Hai Leong Chieu", "Dan Wu" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "A popular generalization is the Conditional Random Field model [3] that allows all terms to depend on the full observation z: E(x|z) = ∑",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "A preliminary version of this paper appeared in Proceedings of the 30th International Conference on Machine Learning (ICML), 2013 [8].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Its complexity is either (i) O(nL) per sample, or (ii) O(n) per sample with an O(nL|D|) preprocessing (assuming that we have an oracle that produces independent samples from the uniform distribution on [0, 1] in O(1) time).",
      "startOffset" : 202,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "Komodakis and Paragios [2] gave an O(nL) technique for minimizing energy (3) in this case.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "We present a modification that has the same worst-case complexity but can beat the algorithm in [2] in the best case.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "Related work The works of [10] and [2] are probably the most related to our paper.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Related work The works of [10] and [2] are probably the most related to our paper.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "The latter considered a pattern-based CRF on a grid for a computer vision application; the MAP inference problem in [2] was converted to sequence labeling problems by decomposing the grid into thin “stripes”.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "[5] considered a more general formulation in which a single pattern is characterized by a set of strings rather than a single string α.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "Their inference procedure reduces the problem to the MAP estimation in a pairwise CRF with cycles, which Some of the bounds stated in [10] are actually weaker.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "[4] extended algorithms in [10] to the Semi-Markov model [7].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[4] extended algorithms in [10] to the Semi-Markov model [7].",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "[4] extended algorithms in [10] to the Semi-Markov model [7].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "We conjecture that our algorithms can be extended to this case as well, and can yield a better complexity compared to [4].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "In [8] we applied the pattern-based CRF model to the problem of the protein dihedral angles prediction.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "5: Θ(nP ′|D|) algorithm for a general commutative semiring, which is equivalent to the algorithm in [10].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Complexity Assume that we have an oracle that produces independent samples from the uniform distribution on [0, 1] in O(1) time.",
      "startOffset" : 108,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "With a Θ(N) preprocessing, a sample can also be produced in O(1) time by the so-called “alias method” [9].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "After that, for each α ∈ Πs+1 we need to run the linear time procedure of [9] for distributions p(β) ∝ Ms(β), β ∈ ∆s(αs+1).",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "Remark 1 An alternative method for computing marginals with complexity O ( n|Γ|L`max ) was given in [10].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "The algorithm closely resembles the method in [10]; it is based on the same idea and has the same complexity.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "Remark 2 As we already mentioned, Algorithm 4 resembles the algorithm in [10].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "If (R,⊕,⊗) = (R,min,+) then this can be reduced to O(nP log(`max + 1)) using the algorithm for Range Minimum Queries by [1].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "It is known [1] that with an O(N) preprocessing each query for can be answered in O(1) time per interval.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "In the MAP case we simply run the preprocessing of [1] for the sequence Wt(β1), .",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "[2] gave an algorithm that makes Θ(nP ) comparisons and Θ(nP ) additions.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2017,
    "abstractText" : "We consider Conditional Random Fields (CRFs) with pattern-based potentials defined on a chain. In this model the energy of a string (labeling) x1 . . . xn is the sum of terms over intervals [i, j] where each term is non-zero only if the substring xi . . . xj equals a prespecified pattern α. Such CRFs can be naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in a CRF, namely computing (i) the partition function, (ii) marginals, and (iii) computing the MAP. Their complexities are respectively O(nL), O(nL`max) and O(nLmin{|D|, log(`max+1)}) where L is the combined length of input patterns, `max is the maximum length of a pattern, and D is the input alphabet. This improves on the previous algorithms of [Ye et al. NIPS 2009] whose complexities are respectively O(nL|D|), O ( n|Γ|L`max ) and O(nL|D|), where |Γ| is the number of input patterns. In addition, we give an efficient algorithm for sampling, and revisit the case of MAP with non-positive weights. This paper addresses the sequence labeling (or the sequence tagging) problem: given an observation z (which is usually a sequence of n values), infer labeling x = x1 . . . xn where each variable xi takes values in some finite domain D. Such problem appears in many domains such as text and speech analysis, signal analysis, and bioinformatics. One of the most successful approaches for tackling the problem is the Hidden Markov Model (HMM). The kth order HMM is given by the probability distribution p(x|z) = 1 Z exp{−E(x|z)} with the energy function E(x|z) = ∑ i∈[1,n] ψi(xi, zi) + ∑ (i,j)∈Ek ψij(xi:j) (1) where Ek = {(i, i+ k) | i ∈ [1, n− k]} and xi:j = xi . . . xj is the substring of x from i to j. A popular generalization is the Conditional Random Field model [3] that allows all terms to depend on the full observation z: E(x|z) = ∑ i∈[1,n] ψi(xi, z) + ∑ (i,j)∈Ek ψij(xi:j , z) (2) A preliminary version of this paper appeared in Proceedings of the 30th International Conference on Machine Learning (ICML), 2013 [8]. This work was partially supported by the European Research Council under the European Unions Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 616160. 1 ar X iv :1 21 0. 05 08 v5 [ cs .L G ] 2 0 Ja n 20 17 We study a particular variant of this model called a pattern-based CRF. It is defined via",
    "creator" : "LaTeX with hyperref package"
  }
}