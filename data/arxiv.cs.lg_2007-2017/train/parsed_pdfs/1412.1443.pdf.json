{
  "name" : "1412.1443.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Structure learning of antiferromagnetic Ising models",
    "authors" : [ "Guy Bresler", "David Gamarnik", "Devavrat Shah" ],
    "emails" : [ "gbresler@mit.edu", "gamarnik@mit.edu", "devavrat@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 2.\n14 43\nv1 [\nst at\n.M L\n] 3\nlower bound suggests that the Õ(pd+2) runtime required by Bresler, Mossel, and Sly’s [2] exhaustive-search algorithm cannot be significantly improved without restricting the class of models.\nAside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari [3] showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time Õ(p2). We provide an algorithm whose performance interpolates between Õ(p2) and Õ(pd+2) depending on the strength of the repulsion."
    }, {
      "heading" : "1 Introduction",
      "text" : "Graphical models have had tremendous impact in a variety of application domains. For unstructured high-dimensional distributions, such as in social networks, biology, and finance, an important first step is to determine which graphical model to use. In this paper we focus on the problem of structure learning: Given access to n independent and identically distributed samples σ(1), . . . σ(n) from an undirected graphical model representing a discrete random vector σ = (σ1, . . . , σp), the goal is to find the graph G underlying the model. Two basic questions are 1) How many samples are required? and 2) What is the computational complexity?\nIn this paper we are mostly interested in the computational complexity of structure learning. We first consider the problem of learning a general discrete undirected graphical model of bounded degree."
    }, {
      "heading" : "1.1 Learning general graphical models",
      "text" : "Several algorithms based on exhaustively searching over possible node neighborhoods have appeared in the last decade [4, 2, 5]. Abbeel, Koller, and Ng [4] gave algorithms for learning general graphical models close to the true distribution in Kullback-Leibler distance. Bresler, Mossel, and Sly [2] presented algorithms guaranteed to learn the true underlying graph. The algorithms in both [4] and [2] perform a search over candidate neighborhoods, and for a graph of maximum degree d, the computational complexity for recovering a graph on p nodes scales as Õ(pd+2) (where the Õ notation hides logarithmic factors).\nWhile the algorithms in [2] are guaranteed to reconstruct general models under basic nondegeneracy conditions using an optimal number of samples n = O(d log p) (sample complexity lower bounds were proved by Santhanam and Wainwright [6] as well as [2]), the exponent d in the Õ(pd+2) run-time is impractically high even for constant but large graph degrees. This has motivated a great deal of work on structure learning for special classes of graphical models. But before giving up on general models, we ask the following question:\nQuestion 1: Is it possible to learn the structure of general graphical models on p nodes with maximum degree d using substantially less computation than pd?\nOur first result suggests that the answer to Question 1 is negative. We show an unconditional computational lower bound of p d 2 for the class of statistical algorithms introduced by Feldman et al. [1]. This class of algorithms was introduced in order to understand the apparent difficulty of the Planted Clique problem, and is based on Kearns’ statistical query model [7]. Kearns showed in his landmark paper that statistical query algorithms require exponential computation to learn parity functions subject to classification noise, and our hardness construction is related to this problem. Most known algorithmic approaches (including Markov chain Monte Carlo, semidefinite programming, and many others) can be implemented as statistical algorithms, so the lower bound is fairly convincing.\nWe give background and prove the following theorem in Section 4.\nTheorem 1.1. Statistical algorithms require at least Ω(p d 2 ) computation steps in order to learn the structure of a general graphical models of degree d.\nIf complexity pd is to be considered intractable, what shall we consider as tractable? Writing algorithm complexity in the form c(d)pf(d), for high-dimensional (large p) problems the exponent f(d) is of primary importance, and we will think of tractable algorithms as having an f(d) that is bounded by a constant independent of d. The factor c(d) is also important, and we will use it to compare algorithms with the same exponent f(d).\nIn light of Theorem 1.1, reducing computation below pΩ(d) requires restricting the class of models. One can either restrict the graph structure or the nature of the interactions between variables. The seminal paper of Chow and Liu [8] makes a model restriction of the first type, assuming that the graph is a tree; generalizations include to polytrees [9], hypertrees [10], and others. Among the many possible assumptions of the second type, the correlation decay property is distinguished: to the best of our knowledge all existing low-complexity algorithms require the correlation decay property [3]."
    }, {
      "heading" : "1.2 Correlation decay property",
      "text" : "Informally, a graphical model is said to have the correlation decay property (CDP) if any two variables σs and σt are asymptotically independent as the graph distance between s and t increases. Exponential decay of correlations holds when the distance from independence decreases exponentially fast in graph distance, and we will mean this stronger form when referring to correlation decay. Correlation decay is known to hold for a number of pairwise graphical models in the so-called high-temperature regime, including Ising, hard-core lattice gas, Potts (multinomial) model, and others (see, e.g., [11, 12, 13, 14, 15, 16]).\nBresler, Mossel, and Sly [2] observed that it is possible to efficiently learn models with (exponential) decay of correlations, under the additional assumption that neighboring variables have correlation bounded away from zero (as is true, e.g., for the ferromagnetic Ising model in the high temperature regime). The algorithm they proposed for this setting pruned the candidate set of neighbors for each node to roughly sizeO(d) by retaining only those variables with sufficiently high correlations, and then within this set performed the exhaustive search over neighborhoods mentioned before, resulting in a computational cost of dO(d)Õ(p2). The greedy algorithms of Netrapali et al. [17] and Ray et al. [18] also require the correlation decay property and perform a similar pruning step by retaining only nodes with high pairwise correlation; they then use a different method to select the true neighborhood.\nA number of papers consider the problem of reconstructing Ising models on graphs with few short cycles, beginning with Anandkumar et al. [19]. Their results apply to the case of Ising models on sparsely connected graphs such as the Erdös-Renyi random graph G(p, dp ). They additionally require the interaction parameters to be either generic or ferromagnetic. Ferromagnetic models have the benefit that neighbors always have a non-negligible correlation because the dependencies cannot cancel, but in either case the results still require the CDP to hold. Wu et al. [20] remove the assumption of generic parameters in [19], but again require the CDP.\nOther algorithms for structure learning are based on convex optimization, such as Ravikumar et al.’s [21] approach using regularized node-wise logistic regression. While this algorithm does not explicitly require the CDP, Bento and Montanari [3] found that the logistic regression algorithm of [21] provably fails to learn certain ferromagnetic Ising model on simple graphs not satisfying the CDP. Other convex optimization-based algorithms such as [22, 23, 24] require similar incoherence or restricted isometry-type conditions that are difficult to verify, but likely also require the CDP. Since all known algorithms for structure learning require the CDP, we ask the following question (paraphrasing Bento and Montanari):\nQuestion 2: Is low-complexity structure learning possible for models which do not exhibit the CDP, on general bounded degree graphs?\nOur second main result answers this question affirmatively by showing that a broad class of repelling models on general graphs can be learned using simple algorithms, even when the underlying model does not exhibit the CDP."
    }, {
      "heading" : "1.3 Repelling models",
      "text" : "The antiferromagnetic Ising model has a negative interaction parameter, whereby neighboring nodes prefer to be in opposite states. Other popular antiferromagnetic models include the Potts or coloring model, and the hard-core model.\nAntiferromagnetic models have the interesting property that correlations between neighbors can be zero due to cancellations. Thus algorithms based on pruning neighborhoods using pairwise correlations, such as the algorithm in [2] for models with correlation decay, does not work for anti-ferromagnetic models. To our knowledge there are no previous results that improve on the pd computational complexity for structure learning of antiferromagnetic models on general graphs of maximum degree d.\nOur first learning algorithm, described in Section 2, is for the hard-core model.\nTheorem 1.2 (Informal). It is possible to learn strongly repelling models, such as the hardcore model, with run-time Õ(p2).\nWe extend this result to weakly repelling models (equivalent to the antiferromagnetic Ising model parameterized in a nonstandard way, see Section 3). Here β is a repelling strength and h is an external field.\nTheorem 1.3 (Informal). Suppose β ≥ (d−α)(h+ ln 2) for a nonnegative integer α. Then it is possible to learn an antiferromagnetic Ising model with interaction β, with run-time Õ(p2+α).\nThe computational complexity of the algorithm interpolates between Õ(p2), achievable for strongly repelling models, and Õ(pd+2), achievable for general models using exhaustive search. The complexity depends on the repelling strength of the model, rather than structural assumptions on the graph as in [19, 20].\nWe remark that the strongly repelling models exhibit long-range correlations, yet the algorithmic task of graph structure learning is possible using a certain local procedure.\nThe focus of this paper is on structure learning, but the problem of parameter estimation is equally important. It turns out that the structure learning problem is strictly more challenging for the models we consider: once the graph is known, it is not difficult to estimate the parameters with low computational complexity (see, e.g., [4])."
    }, {
      "heading" : "2 Learning the graph of a hard-core model",
      "text" : "We warm up by considering the hard-core (independent set) model. The analysis in this section is straightforward, but serves as an example to highlight the fact that the CDP is not a necessary condition for structure learning.\nGiven a graphG = (V,E) on |V | = p nodes, denote by I(G) ⊆ {0, 1}p the set of independent set indicator vectors σ, for which at least one of σi or σj is zero for each edge {i, j} ∈ E(G). The hardcore model with fugacity λ > 0 assigns nonzero probability only to vectors in I(G), with\nP(σ) = λ|σ|\nZ , σ ∈ I(G) . (2.1)\nHere |σ| is the number of entries of σ equal to one and Z = ∑\nσ∈I(G) λ |σ| is the normalizing\nconstant called the partition function. If λ > 1 then more mass is assigned to larger independent sets. (We use indicator vectors to define the model in order to be consistent with the antiferromagnetic Ising model in the next section.)\nOur goal is to learn the graph G = (V,E) underlying the model (2.1) given access to independent samples σ(1), . . . , σ(n). The following simple algorithm reconstructs G efficiently.\nAlgorithm 1 simpleHC\nInput: n samples (σ(1), . . . , σ(n)) ∈ {0, 1}p. Output: edge set Ê. 1: Let S = ∅ 2: For each i, j, k: 3: If σ (k) i = σ (k) j = 1, then S ← S ∪ {i, j} 4: Output Ê = Sc\nThe idea behind the algorithm is very simple. If {i, j} belongs to the edge set E(G), then for every sample σ(k) either σ (k) i = 0 or σ (k) j = 0 (or both). Thus for every i, j and k such that σ (k) i = σ (k) j = 1 we can safely declare {i, j} not to be an edge. To show correctness of the algorithm it is therefore sufficient to argue that for every non-edge {i, j} there is a high likelihood that such an independent set σ(k) will be sampled.\nBefore doing this, we observe that simpleHC actually computes the maximum-likelihood estimate for the graph G. To see this, note that an edge e = {i, j} for which σ (k) i = σ (k) j = 1 for some k cannot be in Ĝ, since P(σ(k)|Ĝ+e) = 0 for any Ĝ. Thus the ML estimate contains a subset of those edges e which have not been ruled out by σ(1), . . . , σ(n). But adding any such edge e to the graph decreases the value of the partition function in (2.1) (the sum is over fewer independent sets), thereby increasing the likelihood of each of the samples.\nThe sample complexity and computational complexity of simpleHC is as follows:\nTheorem 2.1. Consider the hard-core model (2.1) on a graph G = (V,E) on |V | = p nodes and with maximum degree d. The sample complexity of simpleHC is\nn = O(22d max{1, λ2d} log p) , (2.2)\ni.e. with this many samples the algorithm simpleHC correctly reconstructs the graph with probability 1− o(1). The computational complexity is\nO(np2) = O((2λ)2d−2p2 log p) . (2.3)\nProof. Algorithm c correctly reconstructs the graph G if for every e = {i, j} not in E(G), at least one observed independent set vector σ(k) contains both i and j. Let Akij = {σ (k) i = 0 or σ (k) j = 0} be the event that at least one of i or j is missing from σ (k), and let Aij = ∩nk=1A k ij . We have by the union bound and independence of A k ij for different k,\nP(error) ≤ P\n( ⋃\n(i,j)∈Ec\nAij\n) ≤ ( p\n2\n) P(∩nk=1A k ij) = ( p\n2\n) P(A1ij) n ≤ ( p\n2\n) (1 − γ)n .\nThe last inequality is from Lemma 2.3, proved at the end of this section, with the quantity γ defined in the statement of the Lemma. To make P(error) approach zero at the rate 1/p it suffices to take\nn = 3γ−1 log p .\nThis proves the theorem.\nWe next show that the sample complexity bound in Theorem 2.1 is basically tight:\nTheorem 2.2 (Sample complexity lower bound). Consider the hard-core model (2.1). There is a family of graphs on p nodes with maximum degree d such that if the probability of successful reconstruction is above 1/2, then the number of samples must be\nn = Ω ( (1 + λ)d−2 log p ) .\nProof. We give a set of graphs G on p nodes with maximum degree at most d so that given samples generated from a graph selected uniformly at random from G, the (optimal) maximum a posteriori (MAP) rule requires the number of samples stated in the theorem. It is possible to prove the theorem using Fano’s inequality, but since we know the ML rule is equivalent to algorithm simpleHC, we can give a direct proof.\nWe define a set of graphs Gm as follows. Let G0 consist ofm stars of degree d−1, i.e. for each 1 ≤ v ≤ m add d − 1 nodes uv,1, . . . , uv,d−1 with edges {v, uv,i}. There are p = m · (d− 1) nodes in total. Now we let Gm be the set of ( m 2 ) graphs Gij obtained by adding the edge {i, j} between a pair of star centers i and j. The graph G is selected uniformly at random from Gm and samples are generated from the model (2.1).\nThe samples σ(1), . . . , σ(n) do not rule out edge e = {i, j} if there is no σ(k) with σ (k) i = σ (k) j = 1. Suppose that none of edges e1, e2, . . . , er have been ruled out. In this case the observations have the same likelihood under Get for each 1 ≤ t ≤ r, and it follows that the probability of error is at least 1− 1/(t− 1) since the prior on the models is uniform.\nFrom now onward we assume without loss of generality (by symmetry of the construction) that samples are generated from the model on Gab. Call σ (k) a witness for non-edge {i, j} 6= {a, b} if σ (k) i = σ (k) j = 1. We proceed by upper bounding the probability of observing a\nwitness for each of the ( m 2 ) − 1 missing edges. Each star center i is included in a particular random independent set σ(k) with probability at most\nλ\nλ+ ∑d−1\nr=0 ( d−1 r ) λr\n≤ 1\nλ−1(1 + λ)d−1 ≤\n1\n(1 + λ)d−2 := q ,\neven conditional on any assignment to other star centers. It follows that σ(k) is a witness for non-edge {i, j} with probability at most q2.\nTake an arbitrary cardinality m/3 matching M of non-edges (i.e. no two of the non-edges share an endpoint) with each edge also disjoint from a and b (recall that we are focusing on graph Gab). For each e ∈ M let Xe be the indicator variable for the event that in n samples, non-edge e has no witness. Note that the variablesXe are mutually independent. If we define Z = ∑ e∈M Xe, then we have EZ ≥ (m/3)(1− q 2)n, and moreover, EZ2 ≤ EZ + (EZ)2.\nBy the Paley-Zygmund inequality,\nP ( Z ≥ EZ\n10\n) ≥ 4(EZ)2\n5EZ2 ≥\n4\n5(1 + EZ/(EZ)2) .\nIf EZ ≥ 40, then P(Z ≥ 3) ≥ 2/3. If Z ≥ 4, then by the above discussion, the probability of error is at least 3/4, hence EZ ≥ 40 implies P(error) ≥ 23 · 3 4 = 1/2. Hence if the probability of successful reconstruction is above 1/2, then EZ < 40, which requires\nn ≥ (1 + o(1)) logm/3\n− log(1− q2) = Ω\n( (1 + λ)d−2 log p ) ,\nwhere we used the fact that − log(1− q2) = q2 + o(q4) and q−1 = (1 + λ)d−2.\nLemma 2.3. Suppose edge e = (i, j) /∈ G, and let I be an independent set chosen according to the Gibbs distribution (2.1). Then P({i, j} ⊆ I) ≥ (22d+1 max{1, λ2d})−1 , γ .\nProof. We can decompose the partition function as\nZ = ∑\nI\nλ|I| = ∑\nI∈S∅,∅\nλ|I| + ∑\nI∈S∅,j\nλ|I| + ∑\nI∈Si,∅\nλ|I| + ∑\nI∈Si,j\nλ|I|\n: = Z∅,∅ + Z∅,j + Zi,∅ + Zi,j , (2.4)\nwhere Sij = {I : i, j ∈ I}, Si,∅ = {I : i ∈ I, j /∈ I}, etc. Our goal is to lower bound Zi,j , since\nP({i, j} ⊆ I) =\n∑ I:{i,j}⊆I λ |I|\n∑ I λ |I| = Zi,j Z . (2.5)\nWe begin by observing that\n|Si,j | · 2 d ≥ |S∅,j| , (2.6)\nbecause for each independent set I with i ∈ I, there are at most 2d distinct independent sets I ′ with i /∈ I ′ with some subset of (at most d) neighbors of i included. One way of observing this is defining the map f : S∅,j → Si,j by I 7→ {i} ∪ I \\ N (i). The map f takes at most 2d sets I ′ ∈ S∅,j to each I ∈ Si,j , which implies (2.6).\nNow, each such set I ′ mapping to I has weight at most a factor max{1, λd−1} larger than I, so\n2dmax{1, λd−1}Zi,j ≥ Z∅,j . (2.7)\nSimilar reasoning gives\n2dmax{1, λd−1}Zi,j ≥ Zi,∅, and 2 2dmax{1, λ2d−2}Zi,j ≥ Z∅,∅ . (2.8)\nUsing these estimates, we obtain\nZ ≤ Zi,j ( 1 + 4 · 2d−1max{1, λd−1}+ 4 · 22d−2max{1, λ2d−2} ) ≤ Zi,j · 2 2d+1 max{1, λ2d} ,\nand plugging into (2.5) proves the lemma."
    }, {
      "heading" : "3 Learning anti-ferromagnetic Ising models",
      "text" : "In this section we consider the anti-ferromagnetic Ising model on a graph G = (V,E). We parametrize the model in such a way that each configuration has probability\nP(σ) = 1\nZ exp\n{ H(σ) } , σ ∈ {0, 1}p , (3.1)\nwhere H(σ) = −β ∑\n(i,j)∈E\nσiσj + ∑\ni∈V\nhiσi . (3.2)\nHere β > 0 and {hi}i∈V are real-valued parameters, and we assume that |hi| ≤ h for all i. Working with configurations in {0, 1}p rather than the more typical {−1,+1}p amounts to a reparametrization (which is without loss of generality as shown for example in Appendix 1 of [25]). Setting hi = h = ln λ for all i, we recover the hard-core model with fugacity λ in the limit β → ∞, so we think of (3.2) as a “soft” independent set model."
    }, {
      "heading" : "3.1 Strongly antiferromagnetic models",
      "text" : "We start by considering the situation in which the repelling strength β is sufficiently large that we can modify the approach used for the hard-core model.\nDefine the empirical conditional probability\nP̂(σa = 1|σb = 1) := P̂(σa = 1, σb = 1)\nP̂(σb = 1) ,\nwhere for any set S ⊂ V and xS ∈ {0, 1}|S|,\nP̂(σV = xV ) = 1\nn\nn∑\nk=1\n1 {σ (k)\nV =xV }\n.\nThe following lemma shows that we can obtain good estimates for P(σa = 1|σb = 1).\nLemma 3.1. Suppose that P(σb = 1) ≥ q for all b ∈ V . If the number of samples is n ≥ (2/q2ǫ2) log ( 8p2/ζ ) , then with probability at least 1− ζ we have for all a, b ∈ V\n|P(σa = 1|σb = 1)− P̂(σa = 1|σb = 1)| ≤ ǫ .\nThe proof is given in the Supplementary Material.\nThe structure estimation algorithm StrongRepelling, described next, determines\nwhether each edge {a, b} is present based on comparing P̂ to a threshold.\nAlgorithm 2 StrongRepelling\nInput: β, h, d, and n samples σ(1), . . . , σ(n) ∈ {0, 1}p. Output: edge set Ê. 1: Let δ = (1 + 2deh(d−1))−2 and Ê = ∅ 2: For each possible edge {a, b} ∈ ( V 2 ) : 3: If P̂ (σa = 1|σb = 1) ≤ (1 + eβ−h)−1 + δ, then add edge (a, b) to Ê"
    }, {
      "heading" : "4: Output Ê",
      "text" : "The performance of algorithm StrongRepelling is stated next in Proposition 3.2. The proof is similar to that of Theorem 2.1, replacing Lemma 2.3 by Lemma 3.3 below. Theorem 3.7, given in the next subsection, subsumes Proposition 3.2, so we prove only the stronger Theorem 3.7.\nProposition 3.2. Consider the antiferromagnetic Ising model (3.2) on a graph G = (V,E) on p nodes and with maximum degree d. If\nβ ≥ (d+ 2)(h+ ln 2) ,\nthen algorithm StrongRepelling has sample complexity\nn = O ( 22de2h(d+1) log p ) ,\ni.e. this many samples are sufficient to reconstruct the graph with probability 1− o(1). The computational complexity of StrongRepelling is\nO(np2) = O ( 22de2h(d+1)p2 log p ) .\nWhen the interaction parameter β ≥ (d + 2)(h+ ln 2) it is possible to identify edges using pairwise statistics. The next lemma shows the necessary separation.\nLemma 3.3. We have the following estimates:\n(i) If (a, b) /∈ E(G), then P(σa = 1|σb = 1) ≥ 1\n1+2deg(a)eh(deg(a)+1) .\n(ii) Conversely, if (a, b) ∈ E(G), then P(σa = 1|σb = 1) ≤ 1\n1+eβ−h .\n(iii) For any b ∈ V , P(σb = 1) ≥ 1\n1+2deg(b)eh(deg(b)+1) .\nProof. We start by defining restricted partition function summations: Let\nSab = {σ ∈ {0, 1} p : σa = σb = 1} ,\nSa∅ = {σ ∈ {0, 1} p : σa = 1, σb = 0} ,\nand analogously for S∅b and S∅∅. We then define Zab = ∑\nσ∈Sab exp(H(σ)) and again\nanalogously for Za∅, Z∅b, Z∅∅.\nWe first prove part (i) of the lemma, in which we assume that (a, b) /∈ E(G) and lower bound the probability\nP(σa = 1|σb = 1) = Zab\nZab + Z∅b .\nTo this end, consider the map f : S∅b → Sab defined by taking a configuration σ, setting σi = 0 for neighbors i ∈ N(a), and then setting σa = 1. Since the assumption (a, b) /∈ E(G) implies that σa = σb = 1 is a valid assignment to these variables, the definition of f implies in particular that (f(σ))b = 1 if σb = 1, so indeed f(σ) ∈ Sab for σ ∈ S∅b.\nNow, at most 2deg(a) sets are mapped by f to any one set (since the neighbors of a can be in any configuration), and for any σ ∈ S∅b, exp(H(f(σ)) ≥ exp(H(σ) − h(deg(a) + 1)). This shows that 2deg(a) exp[h(deg(a)+1)]Zab ≥ Z∅b , and proves part (i) of the lemma. The proof of part (iii) is omitted as it is almost identical to part (i).\nWe now turn to part (ii), assuming that (a, b) ∈ E(G). Consider the map g : Sab → S∅b taking σ ∈ Sab and setting σa = 0 (removing node a from the independent set). The map g is one-to-one, and H increases by β due to resolving the conflict on edge (a, b), but decreases by ha ≤ h due to omitting node a: exp(H(g(σ))) ≥ exp(H(σ) + β − h). This shows that Zab ≥ e−β+hZ∅b , and completes the proof."
    }, {
      "heading" : "3.2 Weakly antiferromagnetic models",
      "text" : "In this section we focus on learning weakly repelling models and show a trade-off between computational complexity and strength of the repulsion. Recall that for strongly repelling models (with β ≥ d(h + ln 2)) our algorithm has run-time O(p2 log p), the same as for the hard-core model (infinite repulsion).\nFor a subset of nodes U ⊆ V , let G \\ U denote the graph obtained from G by removing nodes in U (as well as any edges incident to nodes in U).\nWe can effectively remove nodes from the graph by conditioning: The family of models (3.2) has the property that conditioning on σi = 0 amounts to removing node i from the graph.\nFact 3.4 (Self-reducibility). Let G = (V,E), and consider the model (3.2). Then for any subset of nodes U ⊆ V , the probability law PG(σ ∈ · |σU = 0) is equal to PG\\U (σV \\U ∈ · ) with the same β and the natural restriction of (hi)i∈V to (hi)i∈V \\U .\nThe following corollary is immediate from Lemma 3.3.\nCorollary 3.5. We have the conditional probability estimates for deleting subsets of nodes:\n(i) If (a, b) /∈ E(G), then for any subset of nodes U ⊂ V \\ {a, b},\nPG\\U (σa = 1|σb = 1) ≥ 1\n1 + 2degG\\U (a)eh(degG\\U (a)+1) .\n(ii) Conversely, if (a, b) ∈ E(G), then for any subset of nodes U ⊆ V \\ {a, b}\nPG\\U (σa = 1|σb = 1) ≤ 1\n1 + eβ−h .\nThe final ingredient is to show that we can condition by restricting attention to a subset of the observed data, σ(1), . . . , σ(n), without throwing away too many samples.\nLemma 3.6. Let U ⊆ V be a subset of nodes and denote the subset of samples with variables σU equal to zero by AU = {σ(k) : σ (k) u = 0 for all u ∈ U}. Then with probability at least 1− exp(−n/8(1 + eh)2|U|) the number |AU | of such samples is at least n 2 · (1 + e h)−|U|.\nProof. We start by computing the probability that a particular sample σ(k) is in AU , or equivalently that σ (k) U = 0. Let W ⊆ V be any subset of nodes, and denote by xW an assignment to the corresponding variables. Due to the antiferromagnetic nature of the interaction, the distribution (3.2) satisfies the monotonicity property\nP(σa = 1|σW = xW ) ≤ P(σa = 1|σW = xW , σb = 0)\nfor any neighbor b ∈ N(a) \\W . This monotonicity together with Bayes’ rule gives\nP(σU = 0) =\n|U|∏\ni=1\nP(σui = 0|σu1 = · · · = σui−1 = 0) ≥\n|U|∏\ni=1\nP(σui = 0|σN(ui) = 0)\n=\n|U|∏\ni=1\n[1 + ehi ]−1 ≥ (1 + eh)−|U| .\nDenoting the last displayed quantity by q, we see that the number of samples obtained, |AU |, stochastically dominates a Binom(n, q) random variable. We apply Azuma’s inequality, which states that\nP(Bin(n, q)− nq ≤ −nt) ≤ exp(−nt2/2),\nwith t = q/2 and this proves the lemma.\nWe now present the algorithm. Effectively, it reduces node degree by removing nodes (which can be done by conditioning on value zero as discussed above), and then applies the strong repelling algorithm to the residual graph.\nAlgorithm 3 WeakRepelling\nInput: β, h, d, and n samples σ(1), . . . , σ(n) ∈ {0, 1}p. Output: edge set Ê. 1: Let δ = (4 + 4 · 2d−αeh(d−α+1))−1, Ê = ∅, and α = ⌈d− β/(h+ ln 2)⌉ 2: For each {a, b} ∈ ( V 2 ) : 3: For each U ⊆ V \\ {a, b} of size |U | ≤ α"
    }, {
      "heading" : "4: Compute P̂G\\U (σa = 1|σb = 1)",
      "text" : "5: If maxU :|U|≤α P̂G\\U (σa = 1|σb = 1) ≤ (1 + e β−h)−1 + δ, then add {a, b} to Ê\n6: Output Ê\nTheorem 3.7. Let α ≤ d be a nonnegative integer, and consider the antiferromagnetic Ising model 3.2 with\nβ ≥ (d+ 2− α)(h+ ln 2)\non a graph G. Algorithm WeakRepelling reconstructs the graph with probability 1− o(1) as p → ∞ using\nn = O ( (1 + eh)2α(d+ 2)24de4h(d+1) log p )\ni.i.d. samples, with run-time\nO ( np2+α ) = Õβ,h,d(p 2+α) .\nProof. We first argue that all of the empirical conditional probabilities P̂G\\U (σa = 1|σb = 1) computed in Step 4 of algorithm WeakRepelling are accurate up to tolerance δ when considering subsets U of cardinality up to α, i.e.,\n|P̂G\\U (σa = 1|σb = 1)− PG\\U (σa = 1|σb = 1)| ≤ δ . (3.3)\nThere are at most α ( p α ) ≤ αpα subsets |U | of size at most α. By Lemma 3.6, for each such U , with probability at least 1−exp(−n/8(1+eh)2|U|) ≥ 1−exp(−n/8(1+eh)2α) the number |AU | of samples with σU = 0 is at least n 2 · (1+ e\nh)−α. It follows from the union bound that with probability at least\n1− αpα exp(−n/8(1 + eh)2α)\nwe have |AU | ≥ n 2 · (1 + e h)−α for all U with |U | ≤ α. By the assumed n in the theorem statement, this holds with probability 1 − o(1). Denote the effective sample size by n′ = n 2 · (1 + e h)−α.\nWe now apply Lemma 3.1 with\nǫ = δ := 1\n4(1 + 2d−αeh(d−α+1)) .\nThis requires n′ ≥ (2/q2ǫ2) log ( 8p2/ζ ) , where q = ( 1 + 2deg(b)eh(deg(b)+1) )−1 and we can\ntake ζ = 1/p. The value of n given in the theorem statement suffices in order that (3.3) holds for all a, b ∈ V \\ U .\nWe first argue that E ⊆ Ê, that is, all true edges are added to Ê. Consider an arbitrary edge e = (a, b) ∈ E. By Corollary 3.5 and (3.3),\nmax U :|U|≤α\nP̂G\\U (σa = 1|σb = 1) ≤ (1 + e β−h)−1 + δ := A−1 + δ ,\nso in Line 5 of algorithm WeakRepelling the edge e is added to Ê.\nWe next show that Ê ⊆ E, so only true edges are included. Suppose e = (a, b) /∈ E. By choosing U ⊆ ∂a \\ {b}, Corollary 3.5 and (3.3) imply that\nP̂G\\U (σa = 1|σb = 1) ≥ ( 1 + 2d−αeh(d−α+1) )−1 − δ := B−1 − δ ,\nhence the same inequality applies to the maximum computed in Line 5 of the algorithm. Now, under the assumption β ≥ (d+ 2− α)(h+ ln 2), we have\nA− 1 = eβ−h ≥ 4 · e(d−α)heh2d−α = 4(B − 1) .\nHence\nB−1 −A−1 ≥ 1\nB −\n1\n4B − 3 =\n3B − 3\nB(4B − 3) >\n1\n2B = 2δ ,\nwhere the last inequality used the fact that B ≥ 2. This shows that e /∈ E is not added to Ê and completes the proof."
    }, {
      "heading" : "4 Statistical algorithms and proof of Theorem 1.1",
      "text" : "We start by describing the statistical algorithm framework introduced by [1]. In this section it is convenient to work with variables taking values in {−1,+1} rather than {0, 1}."
    }, {
      "heading" : "4.1 Background on statistical algorithms",
      "text" : "Let X = {−1,+1}p denote the space of configurations and let D be a set of distributions over X . Let F be a set of solutions (in our case, graphs) and Z : D → 2F be a map taking each distribution D ∈ D to a subset of solutions Z(D) ⊆ F that are defined to be valid solutions for D. In our setting, since each graphical model under our consideration will be identifiable, there is a single graph Z(D) corresponding to each distribution D. For n > 0, the distributional search problem Z over D and F using n samples is to find a valid solution f ∈ Z(D) given access to n random samples from an unknown D ∈ D.\nThe class of algorithms we are interested in are called unbiased statistical algorithms, defined by access to an unbiased oracle. Other related classes of algorithms are defined in [1], and similar lower bounds can be derived for those as well.\nDefinition 4.1 (Unbiased Oracle). Let D be the true distribution. The algorithm is given access to an oracle, which when given any function h : X → {0, 1}, takes an independent random sample x from D and returns h(x).\nThese algorithms access the sampled data only through the oracle: unbiased statistical algorithms outsource the computation. Because the data is accessed through the oracle, it is possible to prove unconditional lower bounds using information-theoretic methods. As noted in the introduction, many algorithmic approaches can be implemented as statistical algorithms.\nWe now define a key quantity called average correlation. The average correlation of a subset of distributions D′ ⊆ D relative to a distribution D is denoted ρ(D′, D),\nρ(D′, D) := 1\n|D′|2\n∑\nD1,D2∈D′\n∣∣∣∣ 〈 D1 D − 1, D2 D − 1 〉\nD\n∣∣∣∣ , (4.1)\nwhere 〈f, g〉D := Ex∼D[f(x)g(x)] and the ratio D1/D represents the ratio of probability mass functions, so (D1/D)(x) = D1(x)/D(x).\nWe quote the definition of statistical dimension with average correlation from [1], and then state a lower bound on the number of queries needed by any statistical algorithm.\nDefinition 4.2 (Statistical dimension). Fix γ > 0, η > 0, and search problem Z over set of solutions F and class of distributions D over X . We consider pairs (D,DD) consisting of a “reference distribution” D over X and a finite set of distributions DD ⊆ D with the following property: for any solution f ∈ F , the set Df = DD \\ Z−1(f) has size at least (1 − η) · |DD|. Let ℓ(D,DD) be the largest integer ℓ so that for any subset D′ ⊆ Df with |D′| ≥ |Df |/ℓ, the average correlation is |ρ(D′, D)| < γ (if there is no such ℓ one can take ℓ = 0). The statistical dimension with average correlation γ and solution set bound η is defined to be the largest ℓ(D,DD) for valid pairs (D,DD) as described, and is denoted by SDA(Z, γ, η).\nTheorem 4.3 ([1]). Let X be a domain and Z a search problem over a set of solutions F and a class of distributions D over X . For γ > 0 and η ∈ (0, 1), let ℓ = SDA(Z, γ, η). Any (possibly randomized) unbiased statistical algorithm that solves Z with probability δ requires at least m calls to the Unbiased Oracle for\nm = min\n{ ℓ(δ − η)\n2(1− η) , (δ − η)2 12γ\n} .\nIn particular, if η ≤ 1/6, then any algorithm with success probability at least 2/3 requires at least min{ℓ/4, 1/48γ} samples from the Unbiased Oracle.\nIn order to show that a graphical model on p nodes of maximum degree d requires computation pΩ(d) in this computational model, we therefore would like to show that SDA(Z, γ, η) = pΩ(d) with γ = p−Ω(d)."
    }, {
      "heading" : "4.2 Soft parities",
      "text" : "For any subset S ⊂ [p] of cardinality |S| = d, let χS(x) = ∏\ni∈S xi be the parity of variables in S. Define a probability distribution by assigning mass to x ∈ {−1,+1}p according to\npS(x) = 1\nZ exp(c · χS(x)) . (4.2)\nHere c is a constant, and the partition function is\nZ = ∑\nx\nexp(c · χS(x)) = 2 p−1(ec + e−c) . (4.3)\nOur family of distributions D is given by these soft parities over subsets S ⊂ [p], and |D| = ( p d ) . Lemma 4.4. Let U denote the uniform distribution on {−1,+1}p. For S 6= T , the correlation 〈pSU − 1, pT U − 1〉 is exactly equal to zero for any value of c. If S = T , the correlation 〈pSU − 1, pS U − 1〉 = 1− 4 (ec+e−c)2 ≤ 1.\nLemma 4.5. For any set D′ ⊆ D of size at least |D|/pd/2, the average correlation satisfies ρ(D′, U) ≤ ddp−d/2 .\nProof. By the preceding lemma, the only contributions to the sum (4.1) comes from choosing the same set S in the sum, of which there are a fraction 1/|D′|. Each such correlation is at most one by Lemma 4.4, so ρ ≤ 1/|D′| ≤ pd/2/|D| = pd/2/ ( p d ) ≤ dd/pd/2. Here we used the\nestimate ( n k ) ≥ (nk ) k.\nProof of Theorem 1.1. Let η = 1/6 and γ = ddp−d/2, and consider the set of distributions D given by soft parities as defined above. With reference distribution D = U , the uniform distribution, Lemma 4.5 implies that SDA(Z, γ, η) of the structure learning problem over distribution (4.2) is at least ℓ = pd/2/dd. The result follows from Theorem 4.3."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by NSF grants CMMI-1335155 and CNS-1161964, and by Army Research Office MURI Award W911NF-11-1-0036."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "In this paper we investigate the computational complexity of learning the<lb>graph structure underlying a discrete undirected graphical model from i.i.d.<lb>samples. Our first result is an unconditional computational lower bound of<lb>Ω(p) for learning general graphical models on p nodes of maximum degree<lb>d, for the class of so-called statistical algorithms recently introduced by<lb>Feldman et al. [1]. The construction is equivalent to the notoriously difficult<lb>learning parities with noise problem in computational learning theory. Our<lb>lower bound suggests that the Õ(p) runtime required by Bresler, Mossel,<lb>and Sly’s [2] exhaustive-search algorithm cannot be significantly improved<lb>without restricting the class of models.<lb>Aside from structural assumptions on the graph such as it being a tree,<lb>hypertree, tree-like, etc., many recent papers on structure learning assume<lb>that the model has the correlation decay property. Indeed, focusing on fer-<lb>romagnetic Ising models, Bento and Montanari [3] showed that all known<lb>low-complexity algorithms fail to learn simple graphs when the interaction<lb>strength exceeds a number related to the correlation decay threshold. Our<lb>second set of results gives a class of repelling (antiferromagnetic) models<lb>that have the opposite behavior: very strong interaction allows efficient<lb>learning in time Õ(p). We provide an algorithm whose performance in-<lb>terpolates between Õ(p) and Õ(p) depending on the strength of the<lb>repulsion.",
    "creator" : "LaTeX with hyperref package"
  }
}