{
  "name" : "1602.03552.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Privately from Multiparty Data",
    "authors" : [ "Jihun Hamm", "Paul Cao", "Mikhail Belkin" ],
    "emails" : [ "HAMMJ@CSE.OHIO-STATE.EDU", "YIC242@ENG.UCSD.EDU", "MBELKIN@CSE.OHIO-STATE.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Consider the problem of performing machine learning with data collected by multiple parties. In many settings, the parties may not wish to disclose the private information. For example, the parties can be medical institutions who aim to perform collaborative research using sensitive patient information they hold. For another example, the parties can be computer users who aim to collectively build a malware detector without sharing their usage data. A conventional approach to learning from multiparty data is to first collect data from all parties and then process them centrally. When privacy is a major concern, this approach\nis not always an appropriate solution since it is vulnerable to attacks during transmission, storage, and processing of data. Instead, we will consider a setting in which each party trains a local classifier from its private data without sending the data. The goal is to build a global classifier by combining local classifiers efficiently and privately. We expect the global classifier to be more accurate than individual local classifiers, as it has access to more information than individual classifiers.\nThis problem of aggregating classifiers was considered in (Pathak et al., 2010), where the authors proposed averaging of the parameters of local classifiers to get a global classifier. To prevent the leak of private information from the averaged parameters, the authors used a differentially private mechanism. Differential privacy measures maximal change in the probability of any outcome of a procedure when any item is added to or removed from a database. It provides a strict upper bound on the privacy loss against any adversary (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006). Parameter averaging is a simple and practical procedure that can be implemented by Secure Multiparty Computation (Yao, 1982). However, averaging is not applicable to classifiers with non-numerical parameters such as decision trees, nor to a collection of different classifier types. This raises the question if there are more flexible and perhaps better ways of aggregating local classifiers privately.\nIn this paper, we propose a method of building a global differentially private classifier from an ensemble of local classifier in two steps (see Fig. 1.) In the first step, locally-trained classifiers are collected by a trusted entity. A naive approach to use the collected classifiers is to release (the parameters of) the classifiers after sanitization by differentially private mechanisms, which is impractical (Sec. 3.2.) Instead, we use the classifier ensemble to generates (pseudo)labels for auxiliary unlabeled data, thus transferring the knowledge of the ensemble to the auxiliary data. In the second step, we use the labeled auxiliary data to find\nar X\niv :1\n60 2.\n03 55\n2v 1\n[ cs\n.L G\n] 1\n0 Fe\nb 20\nan empirical risk minimizer, and release a differentially private classifier using output perturbation (Chaudhuri et al., 2011).\nWhen generating labels for auxiliary data using an ensemble of classifiers, majority voting is the simplest choice. However, we show quantitatively that a global classifier trained from majority-voted labels is highly sensitive to individual votes of local classifiers. Consequently, the final classifier after differentially-private sanitization suffer a heavy loss in its performance. To address this, we propose a new risk insensitive to individual votes, where each sample is weighted by the confidence of the ensemble. We provide an interpretation of the weighted risk in terms of random hypothesis of an ensemble (Breiman, 1996a) in contrast to deterministic labeling rule of majority voting. One of our main results is in Theorem 4: we can achieve -differential privacy with a generalization error of O( −2M−2) and O(N−1) terms, relative to the expected risk of a non-private solution, where M is the number of parties and N is the number of samples in auxiliary data. This result is especially useful in a scenario where there are a large number of parties with weak local classifiers such as a group of connected smart devices with limited computing capability. We demonstrate the performance of our approach with several realistic tasks: activity recognition, network intrusion detection, and malicious URL detection. The results show that it is feasible to achieve both accuracy and privacy with a large number of parties.\nTo summarize, we propose a method of building a global differentially private classifier from locally-trained classifiers of multiple parties without access to their private data. The proposed method has the following advantages: 1) it\ncan use local classifiers of any (mixed) types and therefore is flexible; 2) its generalization error converges to that of a non-private solution with a fast rate of O( −2M−2) and O(N−1); 3) it also provides -differential privacy to all samples of a party and not just a single sample.\nIn Sec. 2, we formally describe privacy definitions. In Sec. 3, we discuss the first step of leveraging unlabeled data, and in Sec. 4, we present the second step of finding a global private classifier via empirical risk minimization in two different forms. In Sec. 5, we discuss related works. We evaluate the methods with real tasks in Sec. 6 and conclude the paper in Sec. 7. Appendix contains omitted proofs and several extensions of the algorithms."
    }, {
      "heading" : "2. Preliminary",
      "text" : ""
    }, {
      "heading" : "2.1. Differential privacy",
      "text" : "A randomized algorithm that takes dataD as input and outputs a function f is called -differentially private if\nP (f(D) ∈ S) P (f(D′) ∈ S) ≤ e (1)\nfor all measurable S ⊂ T of the output range and for all datasets D and D′ differing in a single item, denoted by D ∼ D′. That is, even if an adversary knows the whole dataset D except for a single item, she cannot infer much about the unknown item from the output f of the algorithm. When an algorithm outputs a real-valued vector f ∈ RD, its global L2 sensitivity (Dwork et al., 2006) can be defined as\nS(f) = max D∼D′\n‖f(D)− f(D′)‖ (2)\nwhere ‖·‖ is theL2 norm. An important result from (Dwork et al., 2006) is that a vector-valued output f with sensitivity S(f) can be made -differentially private by perturbing f with an additive noise vector η whose density is\nP (η) ∝ e− S(f) ‖η‖. (3)"
    }, {
      "heading" : "2.2. Output perturbation",
      "text" : "When a classifier which minimizes empirical risk is released in public, it leaks information about the training data. Such a classifier can be sanitized by perturbation with additive noise calibrated to the sensitivity of the classifier, known as output perturbation method (Chaudhuri et al., 2011). Specifically, the authors show the following. If ws is the minimizer of the regularized empirical risk\nRλS(w) = 1\nN ∑ (x,y)∈S l(h(x;w), y) + λ 2 ‖w‖2, (4)\nThen the perturbed outputwp = ws+η, p(η) ∝ e− Nλ 2 ‖η‖ is -differentially private for a single sample. Output perturbation was used to sanitize the averaged parameters in (Pathak et al., 2010). We also use output perturbation to sanitize global classifiers. One important difference of our setting to previous work is that we consider -differential privacy of all samples of a party, which is much stronger than -differential privacy of a single sample.\nThere are conditions on the loss function for this guarantee to hold. We will assume the following conditions for our global classifier1 similar to (Chaudhuri et al., 2011).\n• The loss-hypothesis has a form l(h(x;w), v)) = l(vwTφ(x)), where φ : X → Rd is a fixed map. We will consider only linear classifiers l(vwTx), where any nonlinear map φ is absorbed into the ddimensional features.\n• The surrogate loss l(·) is convex and continuously differentiable. • The derivative l′(·) is bounded: |l′(t)| ≤ 1, ∀t ∈ R, and c-Lipschitz: |l′(s)− l′(t)| ≤ c|s− t|, ∀s, t ∈ R.\n• The features are bounded: supx∈X ‖x‖ ≤ 1.\nThese conditions are satisfied by, e.g., logistic regression loss (c = 1/4) and approximate hinge loss."
    }, {
      "heading" : "3. Transferring knowledge of ensemble",
      "text" : ""
    }, {
      "heading" : "3.1. Local classifiers",
      "text" : "In this paper, we treat local classifiers as M black boxes h1(x), ..., hM (x). We assume that a local classifier hi(x)\n1Local classifiers are allowed to be of any type.\nis trained using its private i.i.d. training set S(i)\nS(i) = {(x(i)1 , y (i) 1 ), ... , (x (i) Ni , y (i) Ni )}, (5)\nwhere (x(i)j , y (i) j ) ∈ X ×{−1, 1} is a sample from a distribution P (x, y) common to all parties. We consider binary labels y ∈ {−1, 1} in the main paper, and present a multiclass extension in Appendix B.\nThis splitting of training data across parties is similar to the Bagging procedure (Breiman, 1996a) with some differences. In Bagging, the training set S(i) for party i is sampled with replacement from the whole dataset D, whereas in our setting, the training set is sampled without replacement from D, more similar to the Subagging (Politis et al., 1999) procedure."
    }, {
      "heading" : "3.2. Privacy issues of direct release",
      "text" : "In the first step of our method, local classifiers from multiple parties are first collected by a trusted entity. A naive approach to use the ensemble is to directly release the local classifier parameters to the parties after appropriate sanitization. However, this is problematic in efficiency and privacy. Releasing all M classifier parameters is an operation with a constant sensitivity, as opposed to releasing insensitive statistics such as an average whose sensitivity is O(M−1). Releasing the classifiers requires much stronger perturbation than necessary, incurring steep loss of performance of sanitized classifiers. Besides, efficient differentially private mechanisms are known only for certain types of classifiers so far (see (Ji et al., 2014) for a review.) Another approach is to use the ensemble as a service to make predictions for test data followed by appropriate sanitization. Suppose we use majority voting to provide a prediction for a test sample. A differentially private mechanism such as Report Noisy Max (Dwork & Roth, 2013) can be used to sanitize the votes for a single query. However, answering several queries requires perturbing all answers with noise linearly proportional to the number of queries, which is impractical in most realistic settings."
    }, {
      "heading" : "3.3. Leveraging auxiliary data",
      "text" : "To address the problems above, we propose to transfer the knowledge of the ensemble to a global classifier using auxiliary unlabeled data. More precisely, we use the ensemble to generate (pseudo)labels for the auxiliary data, which in turn are used to train a global classifier. Compared to directly releasing local classifiers, releasing a global classifier trained on auxiliary data is a much less sensitive operation with O(M−1) (Sec. 4.4) analogous to releasing an average statistic. The number of auxiliary samples does not affect privacy, and in fact the larger the data the closer the global classifier is to the original ensemble with O(N−1) bound (Sec. 4.4). Also, compared to using the ensemble to an-\nswer prediction queries, the sanitized global classifier can be used as many times as needed without its privacy being affected.\nWe argue that the availability of auxiliary unlabeled data is not an issue, since in many settings they are practically much easier to collect than labeled data. Furthermore, if the auxiliary data are obtained from public repositories, privacy of such data is not an immediate concern. We mainly focus on the privacy of local data, and discuss extensions for preserving the privacy of auxiliary data in Sec. 4.5."
    }, {
      "heading" : "4. Finding a global private classifier",
      "text" : "We present details of training a global private classifier. As the first attempt, we use majority voting of an ensemble to assign labels to auxiliary data, and find a global classifier from the usual ERM procedure. In the second attempt, we present a better approach where we use the ensemble to estimate the posterior P (y|x) of the auxiliary samples and solve a ‘soft-labeled’ weighted empirical minimization."
    }, {
      "heading" : "4.1. First attempt: ERM with majority voting",
      "text" : "As the first attempt, we use majority voting ofM local classifiers to generate labels of auxiliary data, and analyze its implications. Majority voting for binary classification is the following rule\nv(x) =\n{ 1, if ∑M i=1 I[hi(x) = 1] ≥ M 2\n−1, otherwise . (6)\nTies can be ignored by assuming an odd number M of parties. Regardless of local classifier types or how they are trained, we can consider the majority vote of the ensemble {h1, ..., hM} as a deterministic target concept to train a global classifier.\nThe majority-voted auxiliary data are\nS = {(x1, v(x1)), ..., (xN , v(xN ))}, (7)\nwhere xi ∈ X is an i.i.d. sample from the same distribution P (x) as the private data. We train a global classifier by minimizing the (regularized) empirical risk associated with a loss and a hypothesis class:\nRλS(w) = 1\nN ∑ (x,v)∈S l(h(x;w), v) + λ 2 ‖w‖2. (8)\nThe corresponding expected risks with and without regularization are\nRλ(w) = Ex[l(h(x;w), v(x))] + λ\n2 ‖w‖2, (9)\nand R(w) = Ex[l(h(x;w), v(x))]. (10)\nAlgorithm 1 DP Ensemble by Majority-voted ERM Input: h1, ..., hM (local classifiers), X (auxiliary unlabeled samples), , λ Output: wp Begin\nfor i = 1, ... , N do Generate majority voted labels v(xi) by (6) end for Find the minimizer ws of (8) with S = {(xi, v(xi))} Sample a random vector η from p(η) ∝ e−0.5λ ‖η‖ Output wp = ws + η\nAlgorithm 1 summarizes the procedure.\nApplying output perturbation to our multiparty setting gives us the following result.\nTheorem 1. The perturbed output wp = ws + η from Algorithm 1 with p(η) ∝ e−λ 2 ‖η‖ is -differentially private.\nThe proof of the theorem and others are in the Appendix A."
    }, {
      "heading" : "4.2. Performance issues of majority voting",
      "text" : "We briefly discuss the generalization error of majorityvoted ERM. In (Chaudhuri et al., 2011), it is shown that the expected risk of an output-perturbed ERM solution wp with respect to the risk of any reference hypothesis w0 is bounded by two terms – one due to noise and another due to the gap between expected and empirical regularized risks. This result is applicable to the majority-voted ERM with minor modifications. The sensitivity of majority-voted ERM from Theorem 1 is 2λ compared to 2 Nλ of a standard ERM, and corresponding the error bound is\nR(wp) ≤ R(w0) +O( −2) +O(N−1), (11)\nwith high probability, ignoring other variables. Unfortunately, the bound does not guarantee a successful learning due to the constant gap O( −2), which can be large for a small .\nWhat causes this is the worst-case scenario of multiparty voting. Suppose the votes of M − 1 local classifiers are exactly ties for all auxiliary samples {x1, ..., xN}. If we replace a local classifier hi(x) with the ‘opposite’ classifier h′i(x) = −hi(x), then the majority-voted labels {v1, ..., vN} become {−v1, ...,−vN}, and the resultant global classifier is entirely different. However unlikely this scenario may be in reality, differential privacy requires that we calibrate our noise to the worst case sensitivity."
    }, {
      "heading" : "4.3. Better yet: weighted ERM with soft labels",
      "text" : "The main problem with majority voting was its sensitivity to the decision of a single party. Let α(x) be the fraction of\npositive votes from M classifiers given a sample x:\nα(x) = 1\nM M∑ j=1 I[hj(x) = 1]. (12)\nIn terms of α, the original loss l(wTxv(x)) for majority voting can be written as\nl(ywTx) = I[α(x) ≥ 0.5] l(wTx) + I[α(x) < 0.5] l(−wTx), (13)\nwhich changes abruptly when the fraction α(x) crosses the boundary α = 0.5. We remedy the situation by introducing the new weighted loss:\nlα(·) = α(x)l(wTx) + (1− α(x))l(−wTx). (14)\nThe new loss has the following properties. When the M votes given a sample x are unanimously positive (or negative), then the weighted loss is lα(·) = l(wTx) (or l(−wTx)), same as the original loss. If the votes are almost evenly split between positive and negative, then the weighted loss is lα(·) ' 0.5 l(wTx)+0.5 l(−wTx) which is insensitive to the change of label by a single vote, unlike the original loss. Specifically, a single vote can change lα(·) only by a factor of 1/M (see Proof of Theorem 3.)\nWe provide a natural interpretation of α(x) and the weighted loss in the following. For the purpose of analysis, assume that the local classifiers h1(x), ..., hM (x) are from the same hypothesis class.2 Since the local training data are i.i.d. samples from P (x, y), the local classifiers {h1(x), ..., hM (x)} can be considered random hypotheses, as in (Breiman, 1996a). Let Q(j|x) be the probability of such a random hypothesis h(x) predicting label j given x:\nQ(j|x) = P (h(x) = j|x), (15)\nThen the fraction α(x) = 1M ∑M j=1 I[hj(x) = 1] is an unbiased estimate of Q(1|x). Furthermore, the weighted loss is directly related to the unweighted loss:\nLemma 2. For any w, the expectation of the weighted loss (14) is asymptotically the expectation of the unweighted loss:\nlim M→∞\nEx[l α(w)] = Ex,v[l(w Txv)]. (16)\nProof. The expected risk Ex,v[l(vwTx)] is\n= ExEv|x[l(vw Tx)]\n= Ex[Q(1|x)l(wTx) +Q(−1|x)l(−wTx)] = Ex[ lim\nM→∞ α(x)l(wTx) + (1− lim M→∞ α(x))l(−wTx)]\n2Our differential privacy guarantee holds whether they are from the same hypothesis class or not.\nAlgorithm 2 DP Ensemble by Weighted ERM Input: h1, ..., hM (local classifiers), X (auxiliary unlabeled samples), , λ Output: wp Begin\nfor i = 1, ... , N do Compute α(xi) by (12) end for Find the minimizer of ws of (19) with {(xi, α(xi))} Sample a random vector η from p(η) ∝ e−0.5Mλ ‖η‖ Output wp = ws + η\n(the law of large numbers)\n= lim M→∞\nEx[α(x)l(w Tx) + (1− α(x))l(−wTx)]\n(bounded α and l for ∀x ∈ X ) = lim\nM→∞ Ex[l\nα(w)]. (17)\nThis shows that minimizing the expected weighted loss is asymptotically the same as minimizing the standard expected loss, when the target v is a probabilistic concept from P (h(x) = v) of the random hypothesis, as opposed to a deterministic concept v(x) from majority voting.\nThe auxiliary dataset with ‘soft’ labels is now\nS = {(x1, α(x1)), ... , (xN , α(xN ))}. (18)\nwhere xi ∈ X is an i.i.d. sample from the same distribution P (x) as the private data, and 0 ≤ α ≤ 1. Note that we are not trying to learn a regression function X → [0, 1] but to learn a classifier X → {−1, 1} using α as a real-valued oracle on P (y = 1|x). Consequently, we find a global classifier by minimizing the regularized weighted empirical risk\nRλS(w) = 1\nN N∑ i=1 lα(h(xi;w), αi) + λ 2 ‖w‖2, (19)\nwhere αi = α(xi). The corresponding expected risks with and without regularization are\nRλ(w) = Ex[l α(h(x;w), α(x))] +\nλ 2 ‖w‖2, (20)\nand R(w) = Ex[l α(h(x;w), α(x))]. (21)\nWe again use output perturbation to make the classifier differentially private as summarized in Algorithm 2."
    }, {
      "heading" : "4.4. Privacy and performance",
      "text" : "Compared to Theorem 1 for majority-voted ERM with a noise of P (η) ∝ e−λ 2 ‖η‖, we have the following result: Theorem 3. The perturbed output wp = ws + η from Algorithm 2 with p(η) ∝ e−Mλ 2 ‖η‖ is -differentially private.\nThat is, we now require 1/M times smaller noise to achieve the same -differential privacy. This directly impacts the performance of the corresponding global classifier as follows.\nTheorem 4. Letw0 be any reference hypothesis. Then with probability of at least 1− δp − δs over the privacy mechanism (δp) and over the choice of samples (δs),\nR(wp) ≤ R(w0) + 4d2(c+ λ) log2(d/δp)\nλ2M2 2\n+ 16(32 + log(1/δs)) λN + λ 2 ‖w0‖2. (22)\nThe generalization error bound above has the O(M−2 −2) term compared to theO( −2) term for majority-voted ERM (11). This implies that by choosing a largeM , Algorithm 2 can find a solution whose expected risk is close to the minimum of a non-private solution for any fixed privacy level > 0.\nWe remind the user that the results should be interpreted with a caution. The bounds in (11) and (22) indicate the goodness of private ERM solutions relative to the best nonprivate solutions with deterministic and probability concepts which are not the same task. Also, they do not indicate the goodness of the ensemble approach itself relative to a centrally-trained classifier using all private data without privacy consideration. We leave this comparison to empirical evaluation in the experiment section."
    }, {
      "heading" : "4.5. Extensions",
      "text" : "We discuss extensions of Algorithms 1 and 2 to provide additional privacy for auxiliary data. More precisely, those algorithms can be made -differentially private for all private data of a single party and a single sample in the auxiliary data, by increasing the amount of perturbation as necessary. We outline the proof as follows. In the previous sections, a global classifier was trained on auxiliary data whose labels were generated either by majority voting or soft labeling. A change in the local classifier affects only the labels {vi} of the auxiliary data but not the features {xi}. Now assume in addition that the feature of one sample from the auxiliary data can also change arbitrarily, i.e., xj 6= x′j for some j and xi = x′i for all i ∈ {1, ..., N} \\ {j}. The sensitivity of the resultant risk minimizer can be computed similarly to the proofs of Theorems 1 and 3 in Appendix A. Briefly, the sensitivity is upper-bounded by the absolute sum of the\ndifference of gradients\n‖∇g(w)‖ ≤ 1 N N∑ i=1 ‖∇l(yiwTxi)−∇l(y′iwTx′i)‖. (23)\nFor majority voting, one term in the sum (23) is\n‖v(xj)xj l′(v(xj)wTxj)− v′(x′j)x′j l′(v′(x′j)wTx′j)‖ (24) which is at most 2 for any xj , x′j ∈ X , and therefore the sensitivity is the same whether xj = x′j or not. As a result, Algorithm 1 is already -differentially private for both labeled and auxiliary data without modification. Furthermore, the privacy guarantee remains the same if we allow xj 6= x′j for any number of samples. For soft labeling, one term in the sum (23) is\n‖αjxj l′(wTxj)− (1− αj)xj l′(−wTxj) −α′jx′j l′(wTx′j) + (1− α′j)x′j l′(−wTx′j)‖ (25)\nwhich is also at most 2 for any xj , x′j ∈ X and 2M when xj = x ′ j . When only a single auxiliary sample changes, i.e., xj 6= x′j for one j, the overall sensitivity increases by a factor of N+M−1N . By increasing the noise by this factor, Algorithm 2 is -differentially private for both labeled and auxiliary data. Note that this factor N+M−1N can be bounded close to 1 if we increase the number of auxiliary samples N relative to the number of parties M ."
    }, {
      "heading" : "5. Related work",
      "text" : "To preserve privacy in data publishing, several approaches such as k-anonymity (Sweeney, 2002) and secure multiparty computation (Yao, 1982) have been proposed (see (Fung et al., 2010) for a review.) Recently, differential privacy (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006) has addressed several weaknesses of k-anonymity (Ganta et al., 2008), and gained popularity as a quantifiable measure of privacy risk. The measure provides a bound on the privacy loss regardless of any additional information an adversary might have. Differential privacy has been used for a privacy-preserving data analysis platform (McSherry, 2009), and for sanitization of learned model parameters from a standard ERM (Chaudhuri et al., 2011). This paper adopts output perturbation techniques from the latter to sanitize non-standard ERM solutions from multiparty settings.\nPrivate learning from multiparty data has been studied previously. In particular, several differentially-private algorithms were proposed, including parameter averaging through secure multiparty computation (Pathak et al., 2010), and private exchange of gradient information to minimize empirical risks incrementally (Rajkumar & Agarwal, 2012; Hamm et al., 2015). Our paper is motivated by\n(Pathak et al., 2010) but uses a very different approach to aggregate local classifiers. In particular, we use an ensemble approach and average the classifier decisions (Breiman, 1996a) instead of parameters, which makes our approach applicable to arbitrary and mixed classifier types. Advantages of ensemble approaches in general have been analyzed previously, in terms of bias-variance decomposition (Breiman, 1996b), and in terms of the margin of training samples (Schapire et al., 1998).\nFurthermore, we are using unlabeled data to augment labeled data during training, which can be considered a semisupervised learning method (Chapelle et al., 2006). There are several related papers in this direction. Augmenting private data with non-private labeled data to lower the sensitivity of the output is straightforward, and was demonstrated in medical applications (Ji et al., 2014). Using nonprivate unlabeled data, which is more general than using labeled data, was demonstrated specifically to assist learning of random forests (Jagannathan et al., 2013). Our use of auxiliary data is not specific to classifier types. Furthermore, we present an extension to preserve the privacy of auxiliary data as well."
    }, {
      "heading" : "6. Experiments",
      "text" : "We use three real-world datasets to compare the performance of the following algorithms:\n• batch: classifier trained using all data ignoring privacy • soft: private ensemble using soft-labels (Algorithm 2) • avg: parameter averaging (Pathak et al., 2010) • vote: private ensemble using majority voting (Algo-\nrithm 1) • indiv: individually trained classifier using local data\nWe can expect batch to perform better than any private algorithm since it uses all private data for training ignoring privacy. In contrast, indiv uses only the local data for training and will perform significantly worse than batch, but it achieves a perfect privacy as long as the trained classifiers are kept local to the parties. We are interested in the range of where private algorithms (soft, avg, and vote) perform better than the baseline indiv.\nTo compare all algorithms fairly, we use only a single type of classier – binary or multiclass logistic regression. For Algorithms 1 and 2, both local and global classifiers are of this type as well. The only hyperparameter of the model is the regularization coefficient λ which we fixed to 10−4 after performing some preliminary experiments.About 10% of the original training data are used as auxiliary unlabeled data, and the rest 90% are randomly distributed to M parties as private data. We report the mean and s.d. over 10\ntrials for non-private algorithms and 100-trials for private algorithms."
    }, {
      "heading" : "6.1. Activity recognition using accelerometer",
      "text" : "Consider a scenario where wearable device users want to train a motion-based activity classifier without revealing her data to others. To test the algorithms, we use the UCI Human Activity Recognition Dataset (Anguita et al., 2012), which is a collection of motion sensor data on a smart device by multiple subjects performing 6 activities (walking, walking upstairs, walking downstairs, sitting, standing, laying). Various time and frequency domain variables are extracted from the signal, and we apply PCA to get d = 50 dimensional features. The training and testing samples are 7K and 3K, respectively.\nWe simulate a case with M = 1K users (i.e., parties). Each user can use only 6 samples to train a local classifier. The remaining 1K samples are used as auxiliary unlabeled data. Figure 2 shows the test accuracy of using different algorithms with varying privacy levels. For nonprivate algorithms, the top solid line (batch) shows the accuracy of a batch-trained classifier at around 0.90, and the bottom dashed line (indiv) shows the averaged accuracy of local classifiers at around 0.47. At 1/ = 0, the private algorithms achieve test accuracy of 0.79 (vote), 0.76 (soft) and 0.67 (avg), and as the the privacy level 1/ increases, the performance drops for all private algorithms. As expected from the bound (11), vote becomes useless even at 1/ = 0.1, while soft and avg are better than indiv until 1/ = 1. We fixed M to 1000 in this experiment due to the limited number of samples, the tendency in the graph is similar to other experiments with larger M ’s."
    }, {
      "heading" : "6.2. Network intrusion detection",
      "text" : "Consider a scenario where multiple gateways or routers collect suspicious network activities independently, and aim to collaboratively build an accurate network intrusion detector without revealing local traffic data. For this task we use the KDD-99 dataset, which consists of examples of ‘bad’ connections, called intrusions or attacks, and ‘good’ normal connections. Features of this dataset consists of continuous values and categorical attributes. To apply logistic regression, we change categorical attributes to onehot vectors to get d = 123 dimensional features. The training and testing samples are 493K and 311K, respectively.\nWe simulate cases withM = 5K/10K/20K parties. Each party can only use 22 samples to train a local classifier. The remaining 43K samples are used as auxiliary unlabeled data. Figure 3 shows the test accuracy of using different algorithms with varying privacy levels. For each of M = 5K/10K/20K the tendency of algorithms is similar to Figure 2 – for a small 1/ , private algorithms perform roughly in between batch and indiv, and as 1/ increases private algorithms start to perform worse than indiv. When M is large (e.g., M = 20K), private algorithms soft and avg hold their accuracy better than when M is small (e.g., M = 5K.) In particular, soft performs nearly as well as the\nnon-private batch until around 1/ = 10 compared to avg ."
    }, {
      "heading" : "6.3. Malicious URL prediction",
      "text" : "In addition to network intrusion detection, multiple parties such as computer users can collaborate on detecting malicious URLs without revealing the visited URLs. The Malicious URL Dataset (Ma et al., 2009) is a collection of examples of malicious URLs from a large Web mail provider. The task is to predict whether a URL is malicious or not by various lexical and host-based features of the URL. We apply PCA to get d = 50 dimensional feature vectors. We choose days 0 to 9 for training, and days 10 to 19 for testing, which amount to 200K samples for training and 200K samples for testing.\nWe simulate cases withM = 5K/10K/20K parties. Each party can use only 9 samples to train a local classifier. The remaining 16K samples are used as auxiliary unlabeled data. Figure 4 shows the test accuracy of using different algorithms with varying privacy levels. The gap between batch and other algorithms is larger compared to the previous experiment (Figure 3), most likely due to the smaller number (=9) of samples per party. However, the overall tendency is very similar to previous experiments."
    }, {
      "heading" : "7. Conclusion",
      "text" : "In this paper, we propose a method of building global differentially private classifiers from local classifiers using two new ideas: 1) leveraging unlabeled auxiliary data to transfer the knowledge of the ensemble, and 2) solving a weighted ERM using class probability estimates from the ensemble. In general, privacy comes with a loss of classification performance. We present a solution to minimize the performance gap between private and non-private ensembles demonstrated with real world tasks."
    }, {
      "heading" : "A. Proofs",
      "text" : "A.1. Proof of Theorem 1\nTheorem 1: The perturbed output wp = ws + η from Algorithm 1 with p(η) ∝ e−λ 2 ‖η‖ is -differentially private.\nProof. We will compute the sensitivity of the minimizer ws of the regularized empirical risk with majority-voted labels (8). Suppose D = (S(1), ..., S(M)) is the ordered set of private training data (5) for M parties, and D′ = ((S′)(1), ..., S(M)) is a neighboring set which differs from D only at party 1’s data, without loss of generality. The local classifiers after training with D and D′ are\nH = (h1, ..., hM ) and H ′ = (h′1, ..., hM ), respectively, which are again different only for classifier 1. The majority votes v(x) and v′(x) from D and D′ generates two auxiliary training sets S = {(xi, v(xi))} and S′ = {(xi, v′(xi)} which have the same features but possibly different labels.\nLet RλS(w) and R λ S′(w) be the regularized empirical risks for training sets S and S′, and let ws and ws′ be the minimizers of the respective risks. From Corollaries 7 and 8 (Chaudhuri et al., 2011), the L2 difference of ws and ws′ is bounded by\n‖ws − ws′‖ ≤ 1\nλ max w ‖∇g(w)‖, (26)\nwhere g(w) is the risk difference RλS(w)−RλS′(w), which, in our case, satisfies\n‖∇g(w)‖ ≤ 1 N N∑ i=1 ‖v(xi)xil′(v(xi)wTxi)\n−v′(xi)xil′(v′(xi)wTxi)‖.\n≤ 1 N N∑ i=1 ‖xi‖ ×\n|l′(wTxi) + l′(−wTxi)|. (27)\nRecall that ‖x‖ ≤ 1 and |l′(·)| ≤ 1 by assumption. In the worst case, v(xi) 6= v′(xi) for all i = 1, ..., N , and therefore the RHS of (27) is bounded by 2. Consequently, the L2 sensitivity of the minimizer ws is\nmax S,S′ ‖ws − ws′‖ ≤\n2 λ . (28)\n-differential privacy follows from the sensitivity result (3).\nA.2. Proof of Theorem 3\nTheorem 3: The perturbed output wp = ws + η from Algorithm 2 with p(η) ∝ e−Mλ 2 ‖η‖ is -differentially private.\nProof. The proof parallels the proof of Theorem 1. We again assume D = (S(1), ..., S(M)) is the ordered set of private training data (5) for M parties, and D′ = ((S′)(1), ..., S(M)) is a neighboring set which differs from D only at party 1’s data, without loss of generality. Let S = {(xi, αi)} and S′ = {(xi, α′i)} be the two resulting datasets which have the same the features but possibly different α’s. We first compute the sensitivity of the minimizer of the weighted regularized empirical risk (19). Let RλS(w) and R λ S′(w) be the regularized empirical risks for training sets S and S′, and let ws and ws′ be the minimizers of the respective risks. Also let g(w) be the difference\nRλS(w)−RλS′(w) of two risks\ng(w) = 1\nN N∑ i=1 [αil(w Txi) + (1− αi)l(−wTxi) −α′il(wTxi)− (1− α′i)l(−wTxi)]. (29)\nThe gradient of g(w) is bounded by\n‖∇g(w)‖ ≤ 1 N N∑ i=1 [|αi − α′i|‖xi‖|l′(wTxi)|\n+|αi − α′i|‖xi‖|l′(−wTxi)|] (30)\n≤ 1 N N∑ i=1 2|αi − α′i|. (31)\nIn the worst case, αi 6= α′i for all i = 1, ..., N . Since αi is the fraction of positive votes, |αi − α′i| ≤ 1/M holds for all i = 1, ..., N . Therefore the L2 sensitivity of the minimizer ws is at most 2λM and the -differential privacy follows.\nA.3. Lemma 5\nWe use the following lemma.\nLemma 5 (Lemma 17 of (Chaudhuri et al., 2011)). If X ∼ Γ(k, θ), where k is an integer, then with probability of at least 1− δ,\nX ≤ kθ log(k/δ).\nA.4. Lemma 6\nLemma 6. If ws is the minimizer of (19) and wp is the - differentially private version from Algorithm 2, then with probability of at least 1− δp over the privacy mechanism,\nRλS(wp) ≤ RλS(ws) + 2d2(c+ λ) log2(d/δ)\nλ2M2 2 (32)\nProof. A differentiable function f : Rd → R is called βsmooth, if ∃β > 0 such that ‖∇f(v)−∇f(u)‖ ≤ β‖v−u‖ for all u, v. From the Mean Value Theorem, such a function satisfies\nf(v) ≤ f(u) +∇T f(u)(v − u) + β 2 ‖v − u‖2, ∀u, v.\nSince |l′(·)| is c-Lipschitz, RλS(w) is (c+ λ)-smooth:\n‖∇RλS(v)−∇RλS(u)‖ ≤ 1 N ∑ i ∥∥αixil′(vTxi)− (1− αi)xil′(−vTxi) −αixil′(uTxi) + (1− αi)xil′(−uTxi)\n∥∥ +λ‖v − u‖\n≤ 1 N ∑ i [ αic‖(v − u)Txi‖+\n(1− αi)c‖(u− v)Txi‖ ]\n+ λ‖v − u‖ ≤ (c+ λ)‖u− v‖. (33)\nBy setting v = wp and u = ws and using the (c + λ)smoothness of RλS(w), we have\nRλS(wp) ≤ RλS(ws) +∇TRλS(ws)(wp − ws)\n+ (c+ λ)\n2 ‖wp − w∗s‖2\n= RλS(ws) + (c+ λ)\n2 ‖wp − ws‖2. (34)\nSince\nP ( ‖wp − w∗s‖ ≤ 2d log(d/δ)\nλM\n) ≥ 1− δp (35)\nfrom Lemma 5 with k = d and θ = 2λM , we have the desired result.\nA.5. Proof of Theorem 4\nTheorem 4: Let w0 be any reference hypothesis. Then with probability of at least 1− δp − δs over the privacy mechanism (δp) and over the choice of samples (δs),\nR(wp) ≤ R(w0) + 4d2(c+ λ) log2(d/δp)\nλ2M2 2\n+ 16(32 + log(1/δs)) λN + λ 2 ‖w0‖2. (36)\nProof. Let ws and w∗ be the minimizers of the regularized empirical risk (19) and the regularized expected risk (20), respectively. The risk at wp relative to a reference classifier w0 can be written as\nR(wp)−R(w0) = Rλ(wp)−Rλ(w∗) +Rλ(w∗)−Rλ(w0)\n+ λ\n2 ‖w0‖2 −\nλ 2 ‖wp‖2\n≤ Rλ(wp)−Rλ(w∗) + λ\n2 ‖w0‖2.\n(37)\nThe inequality above follows from Rλ(w∗) ≤ Rλ(w0) by definition. Note that since ‖x‖ ≤ 1 and |l′| ≤ 1 by assumption, the weighted loss α(x)l(wTx) + (1 − α(x))l(wTx) is 1-Lipschitz in w. From Theorem 1 of (Sridharan et al., 2009) with a = 1, we can also bound Rλ(wp) − Rλ(w∗) as\nRλ(wp)−Rλ(w∗) ≤ 2(RλS(wp)−RλS(w∗s))\n+ 16(32 + log(1/δs))\nλN (38)\nwith probability of 1 − δs over the choice of samples. By combining this inequality with Lemma 6 using the union bound, we have\nRλ(wp)−Rλ(w∗) ≤ 4d2(c+ λ) log2(d/δp)\nλ2M2 2\n+ 16(32 + log(1/δs))\nλN . (39)\nThe theorem follows from (37)."
    }, {
      "heading" : "B. Differentially private multiclass logistic regression",
      "text" : "We extend our methods to multiclass classification problems and provide a sketch of -differential privacy proofs for multiclass logistic regression loss.\nB.1. Standard ERM\nSuppose y ∈ 1, ...,K, and let w = [w1; ... ;wK ] be a stacked (d K)× 1 vector. The multiclass logistic loss (i.e. softmax) is\nl(h(x), y) = −wTy x+ log( ∑ l ew T l x), (40)\nand the regularized empirical risk is\nRλS(w) = − 1\nN ∑ i [wTyixi − log( ∑ l ew T l xi)] + λ 2 ‖w‖2.\n(41) Note that RλS(w) is λ-strongly convex in w.\nThe sensitivity of ws which minimizes (41) can be computed as follows. Suppose S and S′ are two different datasets which are not necessarily neighbors: S = {(xi, yi))} and S′ = {(x′i, y′i)}. Let g(w) be the difference RλS(w) − RλS′(w) of the two risks. Then the partial gradient w.r.t. wk is\n∇wkRλS(w) = − 1\nN ∑ i xi∆k(xi, yi, w) + λwk, (42)\nwhere\n∆k(xi, yi, w) = I[yi = k]− ew T k xi∑\nl e wTl xi\n= I[yi = k]−Pk(xi).\n(43) Since I[yi = k] can be non-zero (i.e. 1) for only one k, and∑ k Pk(xi) = 1 with 0 ≤ Pk(xi) ≤ 1, we have∑ k ∆2k = ∑ k (Ik − Pk)2 ≤ ∑ k (I2k + P 2 k ) ≤ 2, (44)\nLet ∆(xi, yi, w) = [∆1(xi, yi, w), ...,∆K(xi, yi, w)] be a K×1 vector (which depends on xi, yi, w.) The gradient of the risk difference g(w) is then\n∇g(w) = − 1 N ∑ i ∆(xi, yi, w)⊗ xi−∆(x′i, y′i, w)⊗ x′i, (45) where ⊗ is a Kronecker product of two vectors. Note that\n‖∆⊗x‖2 = ∑ k ‖∆kx‖2 ≤ ‖x‖2 ∑ k ∆2k ≤ 2‖x‖2. (46)\nWithout loss of generality, we assume that only (x1, y1) and (x′1, y ′ 1) are possibly different and (xi, yi) = (x ′ i, y ′ i) for all i = 2, ..., N . In this case we have\n‖∇g(w)‖ ≤ 1 N ‖∆(x1, y1, w)⊗ x1‖\n+ 1\nN ‖∆(x′1, y′1, w)⊗ x′1‖\n≤ √ 2\nN (‖x1‖+ ‖x′1‖) ≤\n2 √ 2\nN , (47)\nand the therefore the L2 sensitive of the minimizer of a multiclass logistic regression is\n2 √ 2 Nλ (48)\nfrom Corollaries 7 and 8 (Chaudhuri et al., 2011). Note that the sensitivity does not depend on the number of classesK.\nB.2. Majority-voted ERM\nLet S = {(xi, vi)} and S′ = {(xi, v′i)} be two datasets with the same features but with possibly different labels for all i = 1, ..., N . Then the partial gradient of the risk difference g(w) is\n∇wkg(w) = − 1\nN ∑ i xi[I[vi = k]− I[v′i = k]]\n= − 1 N ∑ i xiak(vi, v ′ i), (49)\nwhere ak(vi, v′i) is\nak(vi, v ′ i) = I[vi = k]− I[v′i = k] ∈ {−1, 0, 1}. (50)\nLet a = [a1, ..., aK ] be a K × 1 vector (which depends on vi, v′i.) Note that at most two elements of a can be nonzero (i.e. ±1.) The gradient can be rewritten using the Kronecker product ⊗ as\n∇g(w) = − 1 N ∑ i a(vi, v ′ i)⊗ xi, (51)\nand its norm is bounded by\n‖∇g(w)‖ ≤ 1 N ∑ i √ 2‖xi‖ ≤ √ 2. (52)\nTherefore the L2 sensitivity of the minimizer of majoritylabeled multiclass logistic regression is\n√ 2\nλ . (53)\nB.3. Weighted ERM\nA natural multiclass extension of the weighted loss (14) is lα(w) = ∑ k αk(x)l(wTk x), (54)\nwhere αk(x) is the unbiased estimate of the probability P (v = k|x). The corresponding weighted regularized empirical risk is\nRλS(w) = 1\nN ∑ i ∑ k αk(xi)l(w T k x) + λ 2 ‖w‖2\n= 1\nN ∑ i ∑ k αk(xi)[log( ∑ l ew T l xi)− wTk xi]\n+ λ\n2 ‖w‖2\n= − 1 N ∑ i [ ∑ k αk(xi)w T k xi − log( ∑ l ew T l xi)]\n+ λ\n2 ‖w‖2, (55)\nand its partial gradient is\n∇wkRλS(w) = − 1\nN ∑ i xi\n[ αk(xi)− ew T k xi∑\nl e wTl xi\n] + λwk.\n(56)\nLet S = {(xi, αi)} and S′ = {(xi, α′i)} be two datasets with the same features but with possibly different labels for all i = 1, ..., N . Then the partial gradient of the risk difference g(w) is\n∇wkg(w) = − 1\nN ∑ i xi[α k(xi)− (α′)k(xi)]\n= − 1 N ∑ i xibk(α k i , (α ′)ki ), (57)\nwhere bk(αki , (α ′)ki ) = α k(xi) − (α′)k(xi). Let b = [b1, ..., bK ] be aK×1 vector (which depends αi, α′i.) Note that at most two elements of b can be nonzero (i.e.,±1/M .) The gradient can then be rewritten as\n∇g(w) = − 1 N ∑ i b(αi, α ′ i)⊗ xi, (58)\nand its norm is bounded by\n‖∇g(w)‖ ≤ 1 N ∑ i √ 2 M ‖xi‖ ≤ √ 2 M . (59)\nTherefore the L2 sensitivity of the minimizer of the weighted multiclass logistic regression is\n√ 2 Mλ . (60)\nB.4. Parameter averaging\nFor the purposes of comparison, we also derive the sensitivity of parameter averaging (Pathak et al., 2010) for multiclass logistic regression. Let the two neighboring datasets be W = (w1, w2, ..., wM ) and W ′ = (w′1, w ′ 2, ..., w ′ M ), which are collections of parameters from M parties. The corresponding averages for the two sets are w̄ = 1M ∑ i wi\nand w̄′ = 1M ∑ i w ′ i. Without loss of generality, we assume the parameters w1 and w′1 differ only for party 1 and wi = w ′ i for others i = 2, ...,M . Since ‖w̄ − w̄′‖ =\n1 M ‖w1−w ′ 1‖, the L2 sensitivity is 1/M times the sensitivity of the minimizer of the minimizer of a single classifier, when all training samples of party 1 are allowed to change. Therefore the L2 sensitivity of the average parameters for multiclass logistic regression is 2 √ 2\nMλ ."
    } ],
    "references" : [ {
      "title" : "Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine",
      "author" : [ "Anguita", "Davide", "Ghio", "Alessandro", "Oneto", "Luca", "Parra", "Xavier", "Reyes-Ortiz", "Jorge L" ],
      "venue" : "In International Workshop of Ambient Assisted Living (IWAAL",
      "citeRegEx" : "Anguita et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anguita et al\\.",
      "year" : 2012
    }, {
      "title" : "Bias, variance, and arcing classifiers",
      "author" : [ "Breiman", "Leo" ],
      "venue" : "Technical report, Statistics Department,",
      "citeRegEx" : "Breiman and Leo.,? \\Q1996\\E",
      "shortCiteRegEx" : "Breiman and Leo.",
      "year" : 1996
    }, {
      "title" : "Semi-Supervised Learning",
      "author" : [ "O. Chapelle", "B. Schölkopf", "Zien", "A. (eds" ],
      "venue" : "URL http: //www.kyb.tuebingen.mpg.de/ssl-book",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2006
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "Chaudhuri", "Kamalika", "Monteleoni", "Claire", "Sarwate", "Anand D" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2011
    }, {
      "title" : "Differential privacy. In Automata, languages and programming",
      "author" : [ "Dwork", "Cynthia" ],
      "venue" : null,
      "citeRegEx" : "Dwork and Cynthia.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork and Cynthia.",
      "year" : 2006
    }, {
      "title" : "Privacy-preserving datamining on vertically partitioned databases",
      "author" : [ "Dwork", "Cynthia", "Nissim", "Kobbi" ],
      "venue" : "In Advances in Cryptology–CRYPTO",
      "citeRegEx" : "Dwork et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2004
    }, {
      "title" : "The algorithmic foundations of differential privacy",
      "author" : [ "Dwork", "Cynthia", "Roth", "Aaron" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2013
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Dwork", "Cynthia", "McSherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam" ],
      "venue" : "In Theory of cryptography,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Privacypreserving data publishing: A survey of recent developments",
      "author" : [ "Fung", "Benjamin", "Wang", "Ke", "Chen", "Rui", "Yu", "Philip S" ],
      "venue" : "ACM Comp. Surveys (CSUR),",
      "citeRegEx" : "Fung et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Fung et al\\.",
      "year" : 2010
    }, {
      "title" : "Composition attacks and auxiliary information in data privacy",
      "author" : [ "Ganta", "Srivatsava Ranjit", "Kasiviswanathan", "Shiva Prasad", "Smith", "Adam" ],
      "venue" : "In Proc. ACM SIGKDD,",
      "citeRegEx" : "Ganta et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ganta et al\\.",
      "year" : 2008
    }, {
      "title" : "Crowd-ML: A privacy-preserving learning framework for a crowd of smart devices",
      "author" : [ "Hamm", "Jihun", "Champion", "Adam", "Chen", "Guoxing", "Belkin", "Mikhail", "Xuan", "Dong" ],
      "venue" : "In Proceedings of the 35th IEEE International Conference on Distributed Computing Systems (ICDCS). IEEE,",
      "citeRegEx" : "Hamm et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hamm et al\\.",
      "year" : 2015
    }, {
      "title" : "A semi-supervised learning approach to differential privacy",
      "author" : [ "Jagannathan", "Geetha", "Monteleoni", "Claire", "Pillaipakkamnatt", "Krishnan" ],
      "venue" : "In Data Mining Workshops (ICDMW),",
      "citeRegEx" : "Jagannathan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jagannathan et al\\.",
      "year" : 2013
    }, {
      "title" : "Differentially private distributed logistic regression using private and public data",
      "author" : [ "Ji", "Zhanglong", "Jiang", "Xiaoqian", "Wang", "Shuang", "Xiong", "Li", "Ohno-Machado", "Lucila" ],
      "venue" : "BMC medical genomics,",
      "citeRegEx" : "Ji et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2014
    }, {
      "title" : "Identifying suspicious urls: an application of largescale online learning",
      "author" : [ "Ma", "Justin", "Saul", "Lawrence K", "Savage", "Stefan", "Voelker", "Geoffrey M" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Ma et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2009
    }, {
      "title" : "Privacy integrated queries: an extensible platform for privacy-preserving data analysis",
      "author" : [ "McSherry", "Frank D" ],
      "venue" : "In Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,",
      "citeRegEx" : "McSherry and D.,? \\Q2009\\E",
      "shortCiteRegEx" : "McSherry and D.",
      "year" : 2009
    }, {
      "title" : "Multiparty differential privacy via aggregation of locally trained classifiers",
      "author" : [ "Pathak", "Manas", "Rane", "Shantanu", "Raj", "Bhiksha" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Pathak et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pathak et al\\.",
      "year" : 2010
    }, {
      "title" : "A differentially private stochastic gradient descent algorithm for multiparty classification",
      "author" : [ "Rajkumar", "Arun", "Agarwal", "Shivani" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Rajkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rajkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Boosting the margin: A new explanation for the effectiveness of voting methods",
      "author" : [ "Schapire", "Robert E", "Freund", "Yoav", "Bartlett", "Peter", "Lee", "Wee Sun" ],
      "venue" : "Annals of statistics,",
      "citeRegEx" : "Schapire et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Schapire et al\\.",
      "year" : 1998
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "Sridharan", "Karthik", "Shalev-Shwartz", "Shai", "Srebro", "Nathan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sridharan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sridharan et al\\.",
      "year" : 2009
    }, {
      "title" : "k-anonymity: A model for protecting privacy",
      "author" : [ "Sweeney", "Latanya" ],
      "venue" : "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,",
      "citeRegEx" : "Sweeney and Latanya.,? \\Q2002\\E",
      "shortCiteRegEx" : "Sweeney and Latanya.",
      "year" : 2002
    }, {
      "title" : "Protocols for secure computations",
      "author" : [ "Yao", "Andrew C" ],
      "venue" : "IEEE Symp. Found. Comp. Sci.,",
      "citeRegEx" : "Yao and C.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yao and C.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "This problem of aggregating classifiers was considered in (Pathak et al., 2010), where the authors proposed averaging of the parameters of local classifiers to get a global classifier.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "It provides a strict upper bound on the privacy loss against any adversary (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006).",
      "startOffset" : 75,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "an empirical risk minimizer, and release a differentially private classifier using output perturbation (Chaudhuri et al., 2011).",
      "startOffset" : 103,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "When an algorithm outputs a real-valued vector f ∈ R, its global L2 sensitivity (Dwork et al., 2006) can be defined as S(f) = max D∼D′ ‖f(D)− f(D′)‖ (2)",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "An important result from (Dwork et al., 2006) is that a vector-valued output f with sensitivity S(f) can be made -differentially private by perturbing f with an additive noise vector η whose density is",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Such a classifier can be sanitized by perturbation with additive noise calibrated to the sensitivity of the classifier, known as output perturbation method (Chaudhuri et al., 2011).",
      "startOffset" : 156,
      "endOffset" : 180
    }, {
      "referenceID" : 15,
      "context" : "Output perturbation was used to sanitize the averaged parameters in (Pathak et al., 2010).",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "We will assume the following conditions for our global classifier1 similar to (Chaudhuri et al., 2011).",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "Besides, efficient differentially private mechanisms are known only for certain types of classifiers so far (see (Ji et al., 2014) for a review.",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "In (Chaudhuri et al., 2011), it is shown that the expected risk of an output-perturbed ERM solution wp with respect to the risk of any reference hypothesis w0 is bounded by two terms – one due to noise and another due to the gap between expected and empirical regularized risks.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "To preserve privacy in data publishing, several approaches such as k-anonymity (Sweeney, 2002) and secure multiparty computation (Yao, 1982) have been proposed (see (Fung et al., 2010) for a review.",
      "startOffset" : 165,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : ") Recently, differential privacy (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006) has addressed several weaknesses of k-anonymity (Ganta et al.",
      "startOffset" : 33,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : ", 2006; Dwork, 2006) has addressed several weaknesses of k-anonymity (Ganta et al., 2008), and gained popularity as a quantifiable measure of privacy risk.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "Differential privacy has been used for a privacy-preserving data analysis platform (McSherry, 2009), and for sanitization of learned model parameters from a standard ERM (Chaudhuri et al., 2011).",
      "startOffset" : 170,
      "endOffset" : 194
    }, {
      "referenceID" : 15,
      "context" : "In particular, several differentially-private algorithms were proposed, including parameter averaging through secure multiparty computation (Pathak et al., 2010), and private exchange of gradient information to minimize empirical risks incrementally (Rajkumar & Agarwal, 2012; Hamm et al.",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : ", 2010), and private exchange of gradient information to minimize empirical risks incrementally (Rajkumar & Agarwal, 2012; Hamm et al., 2015).",
      "startOffset" : 96,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "(Pathak et al., 2010) but uses a very different approach to aggregate local classifiers.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : "Advantages of ensemble approaches in general have been analyzed previously, in terms of bias-variance decomposition (Breiman, 1996b), and in terms of the margin of training samples (Schapire et al., 1998).",
      "startOffset" : 181,
      "endOffset" : 204
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, we are using unlabeled data to augment labeled data during training, which can be considered a semisupervised learning method (Chapelle et al., 2006).",
      "startOffset" : 139,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "Augmenting private data with non-private labeled data to lower the sensitivity of the output is straightforward, and was demonstrated in medical applications (Ji et al., 2014).",
      "startOffset" : 158,
      "endOffset" : 175
    }, {
      "referenceID" : 11,
      "context" : "Using nonprivate unlabeled data, which is more general than using labeled data, was demonstrated specifically to assist learning of random forests (Jagannathan et al., 2013).",
      "startOffset" : 147,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "• batch: classifier trained using all data ignoring privacy • soft: private ensemble using soft-labels (Algorithm 2) • avg: parameter averaging (Pathak et al., 2010) • vote: private ensemble using majority voting (Algorithm 1) • indiv: individually trained classifier using local data",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "To test the algorithms, we use the UCI Human Activity Recognition Dataset (Anguita et al., 2012), which is a collection of motion sensor data on a smart device by multiple subjects performing 6 activities (walking, walking upstairs, walking downstairs, sitting, standing, laying).",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "The Malicious URL Dataset (Ma et al., 2009) is a collection of examples of malicious URLs from a large Web mail provider.",
      "startOffset" : 26,
      "endOffset" : 43
    } ],
    "year" : 2016,
    "abstractText" : "Learning a classifier from private data collected by multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party’s private data? We propose to transfer the ‘knowledge’ of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global -differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by O( −2M−2) where M is the number of parties. This allows strong privacy without performance loss whenM is large, such as in crowdsensing applications. We demonstrate the performance of our method with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection.",
    "creator" : "LaTeX with hyperref package"
  }
}