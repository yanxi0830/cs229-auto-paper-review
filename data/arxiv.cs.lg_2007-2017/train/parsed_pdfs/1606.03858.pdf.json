{
  "name" : "1606.03858.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sorting out typicality with the inverse moment matrix SOS polynomial",
    "authors" : [ "Jean-Bernard Lasserre" ],
    "emails" : [ "lasserre@laas.fr", "edouard.pauwels@irit.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Capturing and summarizing the global shape of a cloud of points is at the heart of many data processing applications such as novelty detection, outlier detection as well as related unsupervised learning tasks such as clustering and density estimation. One of the main difficulties is to account for potentially complicated shapes in multidimensional spaces, or equivalently to account for non standard dependence relations between variables. Such relations become critical in applications, for example in fraud detection where a fraudulent action may be the dishonest combination of several actions, each of them being reasonable when considered on their own.\nAccounting for complicated shapes is also related to computational geometry and nonlinear algebra applications, for example integral computation [9] and reconstruction of sets from moments data [4, 5, 10]. Some of these problems have connections and potential applications in machine learning. The work presented in this paper brings together ideas from both disciplines, leading to a method which allows to encode in a simple manner the global shape and spatial concentration of points within a cloud.\nWe start with a surprising (and apparently unnoticed) empirical observation. Given a collection of points, one may build up a distinguished sum-of-squares (SOS) polynomial whose coefficients (or Gram matrix) is the inverse of the empirical moment matrix (see Section 3). Its degree depends on how many moments are considered, a choice left to the user. Remarkably its sublevel sets capture much of the global shape of the cloud as illustrated in Figure 3. This phenomenon is not incidental as illustrated in many additional examples in Appendix A. To the best of our knowledge, this observation has remained unnoticed and the purpose of this paper is to report this empirical finding to the machine learning community and provide first elements toward a mathematical understanding as well as potential machine learning applications.\nar X\niv :1\n60 6.\n03 85\n8v 2\n[ cs\n.L G\n] 1\n4 Ju\nn 20\nThe proposed method is based on the computation of the coefficients of a very specific polynomial which depends solely on the empirical moments associated with the data points. From a practical perspective, this can be done via a single pass through the data, or even in an online fashion via a sequence of efficient Woodbury updates. Furthermore the computational cost of evaluating the polynomial does not depend on the number of data points which is a crucial difference with existing nonparametric methods such as nearest neighbors or kernel based methods [1]. On the other hand, this computation requires the inversion of a matrix whose size depends on the dimension of the problem (see Section 3). Therefore, the proposed framework is suited for moderate dimensions and potentially very large number of observations.\nIn Section 4 we first describe an affine invariance result which suggests that the distinguished SOS polynomial captures very intrinsic properties of clouds of points. In a second step, we provide a mathematical interpretation that supports our empirical findings based on connections with orthogonal polynomials [3]. We propose a generalization of a well known extremality result for orthogonal univariate polynomials on the real line (or the complex plane) [13, Theorem 3.1.2]. As a consequence, the distinguished SOS polynomial of interest in this paper is understood as the unique optimal solution of a convex optimization problem: minimizing an average value over a structured set of positive polynomials. In addition, we revisit [13, Theorem 3.5.6] about the Christoffel function. The mathematics behind provide a simple and intuitive explanation for the phenomenon that we empirically observed.\nFinally, in Section 5 we perform numerical experiments on KDD cup network intrusion dataset [11]. Evaluation of the distinguished SOS polynomial provides a score that we use as a measure of outlyingness to detect network intrusions (assuming that they correspond to outlier observations). We refer the reader to [1] for a discussion of available methods for this task. For the sake of a fair comparison we have reproduced the experiments performed in [14] for the same dataset. We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14]."
    }, {
      "heading" : "2 Multivariate polynomials and moments",
      "text" : ""
    }, {
      "heading" : "2.1 Notations",
      "text" : "We fix the ambient dimension to be p throughout the text. For example, we will manipulate vectors in Rp as well as p-variate polynomials with real coefficients. We denote by X a set of p variables X1, . . . , Xp which we will use in mathematical expressions defining polynomials. We identify\nmonomials from the canonical basis of p-variate polynomials with their exponents in Np: we associate to α = (αi)i=1...p ∈ Np the monomial Xα := Xα11 X α2 2 . . . X αp p which degree is deg(α) :=∑p\ni=1 αi. We use the expressions <gl and ≤gl to denote the graded lexicographic order, a well ordering over p-variate monomials. This amounts to, first, use the canonical order on the degree and, second, break ties in monomials with the same degree using the lexicographic order with X1 = a,X2 = b . . . For example, the monomials in two variables X1, X2, of degree less or equal to 3 listed in this order are given by: 1, X1, X2, X21 , X1X2, X 2 2 , X 3 1 , X 2 1X2, X1X 2 2 , X 3 2 .\nWe denote by Npd, the set {α ∈ Np; deg(α) ≤ d} ordered by ≤gl. R[X] denotes the set of p-variate polynomials: linear combinations of monomials with real coefficients. The degree of a polynomial is the highest of the degrees of its monomials with nonzero coefficients1. We use the same notation, deg(·), to denote the degree of a polynomial or of an element of Np. For d ∈ N, Rd[X] denotes the set of p-variate polynomials of degree less or equal to d. We set s(d) = ( p+d d ) , the number of monomials of degree less or equal to d. We will denote by vd(X) the vector of monomials of degree less or equal to d sorted by ≤gl. We let vd(X) := (Xα)α∈Npd ∈ Rd[X]\ns(d). With this notation, we can write a polynomial P ∈ Rd[X] as follows P (X) = 〈p,vd(X)〉 for some real vector of coefficients p = (pα)α∈Npd ∈ R\ns(d) ordered using ≤gl. Given x = (xi)i=1...p ∈ Rp, P (x) denotes the evaluation of P with the assignments X1 = x1, X2 = x2, . . . Xp = xp. Given a Borel probability measure µ and α ∈ Np, yα(µ) denotes the moment α of µ: yα(µ) = ∫ Rp x\nαdµ(x). Throughout the paper, we will only consider measures of which all moments are finite."
    }, {
      "heading" : "2.2 Moment matrix",
      "text" : "Given a Borel probability measure µ on Rp, the moment matrix of µ, Md(µ), is a matrix indexed by monomials of degree at most d ordered by ≤gl. For α, β ∈ Npd, the corresponding entry in Md(µ) is defined by Md(µ)α,β := yα+β(µ), the moment α+ β of µ. When p = 2, letting yα = yα(µ) for α ∈ N24, we have\nM2(µ) :\n1 X1 X2 X 2 1 X1X2 X 2 2\n1 1 y10 y01 y20 y11 y02 X1 y10 y20 y11 y30 y21 y12 X2 y01 y11 y02 y21 y12 y03 X21 y20 y30 y21 y40 y31 y22 X1X2 y11 y21 y12 y31 y22 y13 X22 y02 y12 y03 y22 y13 y04 .\nMd(µ) is positive semidefinite for all d ∈ N. Indeed, for any p ∈ Rs(d), let P ∈ Rd[X] be the polynomial with vector of coefficients p, we have pTMd(µ)p = ∫ Rp P\n2(x)dµ(x) ≥ 0. Furthermore, we have the identity Md(µ) = ∫ Rp vd(x)vd(x) T dµ(x) where the integral is understood elementwise."
    }, {
      "heading" : "2.3 Sum of squares (SOS)",
      "text" : "We denote by Σ[X] ⊂ R[X] (resp. Σd[X] ⊂ Rd[X]), the set of polynomials (resp. polynomials of degree at most d) which can be written as a sum of squares of polynomials. Let P ∈ R2m[X] for some m ∈ N, then P belongs to Σ2m[X] if there exists a finite J ⊂ N and a family of polynomials Pj ∈ Rm[X], j ∈ J , such that P = ∑ j∈J P 2 j . It is obvious that sum of squares polynomials are always nonnegative. A further interesting property is that this class of polynomials is connected with positive semidefiniteness. Indeed, P belongs to Σ2m[X] if and only if\n∃Q ∈ Rs(m)×s(m), Q 0, P (x) = vd(x)TQvd(x), ∀x ∈ Rp. (1)\nAs a consequence, every positive semidefinite matrix Q ∈ Rs(m)×s(m) defines a polynomial in Σ2m[X] by using the representation in (1).\n1For the null polynomial, we use the convention that its degree is 0 and it is ≤gl smaller than all other monomials."
    }, {
      "heading" : "3 Empirical observations on the inverse moment matrix SOS polynomial",
      "text" : "The inverse moment-matrix SOS polynomial is associated to a measure µ which satisfies the following.\nAssumption 1 µ is a Borel probability measure on Rp with all its moments finite and Md(µ) is positive definite for a given d ∈ N.\nDefinition 1 Let µ, d satisfy Assumption 1. We call the SOS polynomial Qµ,d ∈ Σ2d[X] defined by the application:\nx 7→ Qµ,d(x) := vd(x)TMd(µ)−1vd(x), x ∈ Rp, (2)\nthe inverse moment-matrix SOS polynomial of degree 2d associated to µ.\nActually, connection to orthogonal polynomials will show that the inverse function x 7→ Qµ,d(x)−1 is called the Christoffel function in the literature [13, 3] (see also Section 4).\nIn the remainder of this section, we focus on the situation when µ corresponds to an empirical measure over n points in Rp which are fixed. So let x1, . . . ,xn ∈ Rp be a fixed set of points and let µ := 1n ∑n i=1 δxi where δx corresponds to the Dirac measure at x. In such a case the polynomial Qµ,d in (2) is determined only by the empirical moments up to degree 2d of our collection of points. Note that we also require that Md(µ) 0. In other words, the points x1, . . . ,xn do not belong to an algebraic set defined by a polynomial of degree less or equal to d. We first describe empirical properties of inverse moment matrix SOS polynomial in this context of empirical measures. A mathematical intuition and further properties behind these observations are developped in Section 4."
    }, {
      "heading" : "3.1 Sublevel sets",
      "text" : "The starting point of our investigations is the following phenomenon which to the best of our knowledge has remained unnoticed in the literature. For the sake of clarity and simplicity we provide an illustration in the plane. Consider the following experiment in R2 for a fixed d ∈ N: represent on the same graphic, the cloud of points {xi}i=1...n and the sublevel sets of SOS polynomial Qµ,d in R2 (equivalently, the superlevel sets of the Christoffel function). This is illustrated in the left panel of Figure 3. The collection of points consists of 500 simulations of two different Gaussians and the value of d is 4. The striking feature of this plot is that the level sets capture the global shape of the cloud of points quite accurately. In particular, the level set {x : Qµ,d(x) ≤ ( p+d d ) } captures most of the points. We could reproduce very similar observations on different shapes with various number of points in R2 and degree d (see Appendix A)."
    }, {
      "heading" : "3.2 Measuring outlyingness",
      "text" : "An additional remark in a similar line is that Qµ,d tends to take higher values on points which are isolated from other points. Indeed in the left panel of Figure 3, the value of the polynomial tends to be smaller on the boundary of the cloud. This extends to situations where the collection of points correspond to shape with a high density of points with a few additional outliers. We reproduce a similar experiment on the right panel of Figure 3. In this example, 1000 points are sampled close to a ring shape and 40 additional points are sampled uniformly on a larger square. We do not represent the sublevel sets of Qµ,d here. Instead, the color and shape of the points are taken proportionally to the value of Qµ,d, with d = 8.\nFirst, the results confirm the observation of the previous paragraph, points that fall close to the ring shape tend to be smaller and points on the boundary of the ring shape are larger. Second, there is a clear increase in the size of the points that are relatively far away from the ring shape. This highlight the fact that Qµ,d tends to take higher value in less populated areas of the space."
    }, {
      "heading" : "3.3 Relation to maximum likelihood estimation",
      "text" : "If we fix d = 1, we recover the maximum likelihood estimation for the Gaussian, up to a constant additive factor. To see this, set µ = 1n ∑n i=1 xi and S = 1 n ∑n i=1 xix T i . With this notation, we have\nthe following block representation of the moment matrix,\nMd(µ) =\n( 1 µT\nµ S\n) Md(µ) −1 = ( 1 + µTV −1µ −µTV −1 −V −1µ V −1 ) ,\nwhere V = S − µµT is the empirical covariance matrix and the expression for the inverse is given by Schur complement. In this case, we have Qµ,1(x) = 1 + (x− µ)TV −1(x− µ) for all x ∈ Rp. We recognize the quadratic form that appears in the density function of the multivariate Gaussian with parameters estimated by maximum likelihood. This suggests a connection between the inverse SOS moment polynomial and maximum likelihood estimation. Unfortunately, this connection is difficult to generalize for higher values of d and we do not pursue the idea of interpreting the empirical observations of this section through the prism of maximum likelihood estimation and leave it for further research. Instead, we propose an alternative view in Section 4."
    }, {
      "heading" : "3.4 Computational aspects",
      "text" : "Recall that s(d) = ( p+d d ) is the number of p-variate monomials of degree up to d. The computation of Qµ,d requires O(ns(d)2) operations for the computation of the moment matrix and O(s(d)3) operations for the matrix inversion. The evaluation of Qµ,d requires O(s(d)2) operations.\nEstimating the coefficients of Qµ,d has a computational cost that depends only linearly in the number of points n. The cost of evaluating Qµ,d is constant with respect to the number of points n. This is an important contrast with kernel based or distance based methods (such as nearest neighbors and one class SVM) for density estimation or outlier detection since they usually require at least O(n2) operations for the evaluation of the model [1]. Moreover, this is well suited for online settings where inverse moment matrix computation can be done using Woodbury updates.\nThe dependence in the dimension p is of the order of pd for a fixed d. Similarly, the dependence in d is of the order of dp for a fixed dimension p and the joint dependence is exponential. This suggests that the computation and evaluation of Qµ,d will mostly make sense for moderate dimensions and degree d."
    }, {
      "heading" : "4 Invariances and interpretation through orthogonal polynomials",
      "text" : "The purpose of this section is to provide a mathematical rationale that explains the empirical observations made in Section 3. All the proofs are postponed to Appendix B. We fix a Borel probability measure µ on Rp which satisfies Assumption 1. Note that Md(µ) is always positive definite if µ is not supported on the zero set of a polynomial of degree at most d. Under Assumption 1, Md(µ) induces an inner product on Rs(d) and by extension on Rd[X] (see Section 2). This inner product is denoted by 〈·, ·〉µ and satisfies for any polynomials P,Q ∈ Rd[X] with coefficients p,q ∈ Rs(d),\n〈P,Q〉µ := 〈p,Md(µ)q〉Rs(d) = ∫ Rp P (x)Q(x)dµ(x).\nWe will also use the canonical inner product over Rd[X] which we write 〈P,Q〉Rd[X] := 〈p,q〉Rs(d) for any polynomials P,Q ∈ Rd[X] with coefficients p,q ∈ Rs(d). We will omit the subscripts for this canonical inner product and use 〈·, ·〉 for both products."
    }, {
      "heading" : "4.1 Affine invariance",
      "text" : "It is worth noticing that the mapping x 7→ Qµ,d(x) does not depend on the particular choice of vd(X) as a basis of Rd[X], any other basis would lead to the same mapping. This leads to the result that Qµ,d captures affine invariant properties of µ.\nLemma 1 Let µ satisfy Assumption 1 and A ∈ Rp×p, b ∈ Rp define an invertible affine mapping on Rp,A : x→ Ax+b. Then, the push foward measure, defined by µ̃(S) = µ(A−1(S)) for all Borel sets S ⊂ Rp, satisfies Assumption 1 (with the same d as µ) and for all x ∈ Rp, Qµ,d(x) = Qµ̃,d(Ax+ b).\nLemma 1 is probably better understood when µ = 1/n ∑n i=1 δxi as in Section 3. In this case, we\nhave µ̃ = 1/n ∑n i=1 δAxi+b and Lemma 1 asserts that the level sets of Qµ̃,d are simply the images of those of Qµ,d under the affine transformation x 7→ Ax + b. This is illustrated in Appendix D."
    }, {
      "heading" : "4.2 Connection with orthogonal polynomials",
      "text" : "We define a classical [13, 3] family of orthonormal polynomials, {Pα}α∈Npd ordered according to ≤gl which satisfies for all α ∈ Npd\n〈Pα, Xβ〉 = 0 if α <gl β, 〈Pα, Pα〉µ = 1, 〈Pα, Xβ〉µ = 0 if β <gl α, 〈Pα, Xα〉µ > 0. (3)\nIt follows from (3) that 〈Pα, Pβ〉µ = 0 if α 6= β. Existence and uniqueness of such a family is guaranteed by the Gram-Schmidt orthonormalization process following the ≤gl order on the monomials, and by the positivity of the moment matrix, see for instance [3, Theorem 3.1.11].\nLet Dd(µ) be the lower triangular matrix which rows are the coefficients of the polynomials Pα defined in (3) ordered by ≤gl. It can be shown that Dd(µ) = Ld(µ)−T , where Ld(µ) is the Cholesky factorization of Md(µ). Furthermore, there is a direct relation with the inverse moment matrix as Md(µ) −1 = Dd(µ) TDd(µ) [7, Proof of Theorem 3.1]. This has the following consequence.\nLemma 2 Let µ satisfy Assumption 1, then Qµ,d = ∑ α∈Npd\nP 2α, where the family {Pα}α∈Npd is defined by (3) and ∫ Rp Qµ,d(x)dµ(x) = s(d).\nThat is, Qµ,d is a very specific and distinguished SOS polynomial, the sum of squares of the orthonormal basis elements {Pα}α∈Npd of Rd(X) (w.r.t. µ). Furthermore, the average value of Qµ,d with respect to µ is s(d) which corresponds to the red level set in left panel of Figure 3."
    }, {
      "heading" : "4.3 A variational formulation for the inverse moment matrix SOS polynomial",
      "text" : "In this section, we show that the family of polynomials {Pα}α∈Npd defined in (3) is the unique solution (up to a multiplicative constant) of a convex optimization problem over polynomials. This fact combined with Lemma 2 provides a mathematical rationale for the empirical observations outlined in Section 3. Consider the following optimization problem.\nmin Qα,θα,α∈Npd\n1\n2 ∫ Rp ∑ α∈Npd Qα(x) 2dµ(x) (4)\ns.t. qαα ≥ exp(θα), qαβ = 0, α, β ∈ Npd, α <gl β, ∑ α∈Npd θα = 0,\nwhere Qα(x) = ∑ β∈Npd qαβx β , α ∈ Npd. We first comment on problem (4). Let P = ∑ α∈Npd\nQ2α be the SOS polynomial appearing in the objective function of (4). The constraints of problem (4) restrict P to be in a certain set Sd ⊂ Σd[X]. With this notation, problem (4) is reformulated as minP∈Sd ∫ Pdµ. Therefore problem (4) balances two antagonist targets, on one hand the minimization of the average value of the SOS polynomial P with respect to µ, on the other hand the avoidance of the trivial polynomial, enforced by the constraint that P ∈ Sd. The constraints on P are simple and natural, they ensure that P is a sum of squares of polynomials {Qα}α∈Npd , where the leading term of Qα (according to the ordering ≤gl) is qααxα with qαα > 0 (and hence does not vanish). Inversely, using Cholesky factorization, for any SOS polynomial Q of degree 2d which coefficient matrix (see equation (1)) is positive definite, there exists a > 0 such that aQ ∈ Sd. This suggests that Sd is a quite general class of nonvanishing SOS polynomials. The following result, which gives a relation between Qµ,d and solutions of (4), uses a generalization of [13, Theorem 3.1.2] to several orthogonal polynomials of several variables.\nTheorem 1 : Under Assumption 1, problem (4) is a convex optimization problem with a unique optimal solution (Q∗α, θ ∗ α), which satisfies Q ∗ α = √ λPα, α ∈ Npd, for some λ > 0. In particular,\nthe distinguished SOS polynomial Qµ,d = ∑ α∈Npd P 2α = 1 λ ∑ α∈Npd (Q∗α) 2, is (part of) the unique optimal solution of (4).\nTheorem 1 states that up to the scaling factor λ, the distinguished SOS polynomial Qµ,d is the unique optimal solution of problem (4). A detailed proof is provided in the Appendix B and we only sketch the main ideas here. First, it is remarkable that for each fixed α ∈ Npd (and again up to a scaling factor) the polynomial Pα is the unique optimal solution of the problem:\nminQ { ∫ Q2dµ : Q ∈ Rd[X], Q(x) = xα + ∑ β<glα qβ x β }\n. This fact is well-known in the univariate case [13, Theorem 3.1.2] and does not seem to have been exploited in the literature, at least for purposes similar to ours. So intuitively, P 2α should be as close to 0 as possible on the support of µ. Problem (4) has similar properties and the constraint on the vector of weights θ enforces that, at an optimal solution, the contribution ∫ (Q∗α)\n2 dµ to the overall sum in the criterion is the same for all α. Using Lemma 2 yields (up to a multiplicative constant) the polynomial Qµ,d. Other constraints on θ would yield different weighted sum of the squares P 2α. This will be a subject of further investigations.\nTo sum up, Theorem 1 provides a rationale for our observations. Indeed when solving (4), intuitively, Qµ,d should be as close to 0 as possible on average while remaining in a large class of nonvanishing SOS polynomials."
    }, {
      "heading" : "4.4 Christoffel function and outlier detection",
      "text" : "The following result from [3, Theorem 3.5.6] draws a direct connection between Qµ,d and the Chritoffel function (the right hand side of (5)).\nTheorem 2 ([3]) Let Assumption 1 hold and let x̄ ∈ Rp be fixed, arbitrary. Then\nQµ,d(x̄) −1 = min\nP∈Rd[X] {∫ Rp P (x)2 dµ(x) : P (x̄) = 1 } . (5)\nTheorem 2 provides a mathematical rationale for the use of Qµ,d for outlier or novelty detection purposes. Indeed, from Lemma 2 and equation (3), we have Qµ,d ≥ 1 on Rp. Furthermore, the solution of the minimization problem in (5) satisfies P (x̄)2 = 1 and µ ({ x ∈ Rp : P (x)2 ≤ 1 }) ≥ 1 − Qµ,d(x̄)−1 (by Markov’s inequality). Hence, for high values of Qµ,d(x̄), the sublevel set{ x ∈ Rp : P (x)2 ≤ 1 } contains most of the mass of µ while P (x̄)2 = 1. Again the result of Theorem 2 does not seem to have been interpreted for purposes similar to ours."
    }, {
      "heading" : "5 Experiments on network intrusion datasets",
      "text" : "In addition to having its own mathematical interest, Theorem 1 can be exploited for various purposes. For instance, the sub-level sets of Qµ,d, and in particular {x ∈ Rp : Qµ,d(x) ≤ ( p+d d ) }, can be used to encode a cloud of points in a simple and compact form. However in this section we focus on another potential application in anomaly detection.\nEmpirical findings described in Section 3 suggest that the polynomial Qµ,d can be used to detect outliers in a collection of real vectors by taking µ to be the corresponding empirical measure. This is backed up by the results presented in Section 4. In this section we illustrate these properties on a real world example. We choose the KDD cup 99 network intrusion dataset (available at [11]) which consists of network connection data with labels describing whether they correspond to normal traffic or network intrusions. We follow [15] and [14] and construct five datasets consisting of labeled vectors in R3, the label indicating normal traffic or network attack. The content of these datasets is summarized in the following table.\nDataset http smtp ftp-data ftp others Number of examples 567498 95156 30464 4091 5858 Proportions of attacks 0.004 0.0003 0.023 0.077 0.016\nThe details on how these datasets are constructed are available in [15, 14] and are reproduced in Appendix C. The main idea is to give to each datapoint an outlyingness score solely based on its position in R3 and then compare outliers predicted by the score with the label indicating network intrusion. The underlying assumption is that network intrusion corresponds to infrequent abnormal behaviors and could thus be considered as outliers.\nWe reproduce the exact same experiment that was described in [14, Section 5.4] using the value of the inverse moment matrix SOS polynomial from Definition 1 as an outlyingness score (with d = 3). The authors of [14] have compared different types of methods for outlier detection in the same experimental setting: methods based on robust estimation and Mahalanobis distance [6, 8], mixture model based methods [12] and recurrent neural network based methods. These results are gathered\nin [14, Figure 7]. In the left panel of Figure 2 we represent the same performance measure for our approach. We first compute the value of the inverse moment SOS polynomial for each datapoint and use it as an outlyingness score. We then display the proportion of correctly identified outliers, with score above a given threshold, as a function of the proportion of examples with score above the threshold (for different values of the threshold). The main comments are as follows.\n• The inverse moment matrix SOS polynomial does detect network intrusions with varying performances on the five datasets.\n• Except for the “ftp-data dataset”, the global shape of these curves are very similar to results reported in [14, Figure 7] indicating that the proposed approach is comparable to other dedicated methods for intrusion detection in these four datasets.\nIn a second experiment, we investigate the effect of changing the value of d in Qµ,d on the performances in terms of outlier detection. We focus on the “others” dataset because it is the most heterogeneous in term of data and outliers. We adopt a slightly different measure of performance and use precision recall curves (see for example [2]) to measure performances in identifying network intrusions (the higher the curve, the better). We call the area under such curves the AUPR. The right panel of Figure 2 represents these results. First, the case d = 1, which corresponds to vanilla Mahalanobis distance as outlined in Section 3.3, gives poor performances. Second, the global performances rapidly increase with d and then decrease and stabilize.\nThis suggests that d can be used as a tuning parameter which controls the “complexity” of Qµ,d. Indeed, 2d is the degree of the polynomial Qµ,d and it is expected that more complex models will potentially identify more diverse classes of examples of points as outliers. In our case, this means identifying regular traffic as outliers while it actually does not correspond to intrusions."
    }, {
      "heading" : "6 Conclusion and future work",
      "text" : "We presented empirical findings with a mathematical intuition regarding the sublevel sets of the inverse moment matrix SOS polynomial. This opens many potential subjects of investigations.\n• Similarities with maximum likelihood.\n• Statistics in the context of empirical processes.\n• Relation between a density and its inverse moment matrix SOS polynomial. Assymptotics when the degree increases.\n• Connections with computational geometry and non Gaussian integrals.\n• Computationally tractable extensions in higher dimensions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partly supported by project ERC-ADG TAMING 666981,ERC-Advanced Grant of the European Research Council and grant number FA9550-15-1-0500 from the Air Force Office of Scientific Research, Air Force Material Command."
    }, {
      "heading" : "A Additional examples",
      "text" : ""
    }, {
      "heading" : "B Proofs",
      "text" : "We use the same notation as in the main text. We recall that Assumption 1.\nAssumption 1 µ is a Borel probability measure on Rp with all its moments finite and Md(µ) is positive definite for a given d ∈ N.\nLemma 2 and Theorem 2 are taken from the literature and we provide a proof for completeness.\nB.1 Proof of Lemma 1\nFirst we show that the mapping x 7→ Qµ,d(x) does not depend on the choice of a specific basis of Rd[X]. Then we will deduce the affine invariance property.\nLemma 3 Let wd(X) be an arbitrary basis of Rd[X] and let Rµ,d ∈ Rd[X] be derived in the same way as Qµ,d (see Definition 1), with wd in place of vd. Then Qµ,d(x) = Rµ,d(x) for all x ∈ Rp.\nProof : Since wd is a basis of Rd[X], there exists an invertible matrix C ∈ Rs(d)×s(d) such that wd(X) = Cvd(X). We reproduce the computation of Definition 1 with this new basis. We write Nd(µ) the moment matrix computed with the polynomial basis wd. We have\nNd(µ) = ∫ Rp wd(x)wd(x) T dµ(x)\n= ∫ Rp Cvd(x)vd(x) TCT dµ(x)\n= C ∫ Rp vd(x)vd(x) T dµ(x)CT = CMd(µ)C T ,\nwhich leads to Nd(µ)−1 = C−TMd(µ)−1C−1. Using Definition 1, for all x ∈ Rp, we have\nRµ,d(x) = wd(x) TNd(µ) −1wd(x)\n= vd(x) TCTC−TMd(µ) −1C−1Cvd(x)\n= vd(x) TMd(µ) −1vd(x)\n= Qµ,d(x),\nwhich concludes the proof.\nLemma 1 Let µ satisfy Assumption 1 and A ∈ Rp×p, b ∈ Rp define an invertible affine mapping on Rp,A : x→ Ax+b. Then, the push foward measure, defined by µ̃(S) = µ(A−1(S)) for all Borel sets S ⊂ Rp, satisfies Assumption 1 (with the same d as µ) and for all x ∈ Rp, Qµ,d(x) = Qµ̃,d(Ax+ b).\nProof : Let us first computeMd(µ̃). For the push forward measure µ̃, it holds that for any µ integrable function f : Rp → R, ∫\nRp f(x)dµ̃(x) = ∫ Rp f(Ax + b)dµ(x).\nBy considering polynomial f , we have that µ̃ has all its moments finite and satisfies Assumption 1 with the same d as µ. Furthermore, we have\nMd(µ̃) = ∫ Rp vd(x)vd(x) T dµ̃(x) = ∫ Rp vd(Ax + b)vd(Ax + b) T dµ(x). (7)\nWe can deduce the following identity for all x ∈ Rp,\nQµ̃,d(Ax + b) = vd(Ax + b) TMd(µ̃) −1 vd(Ax + b). (8)\nIt remains to notice that mappings defined by wd(x) = vd(Ax + b) for all x ∈ Rp form a basis of the polynomials of degree up to d on Rp (by invertibility of the affine mapping). Combining (7) and (8), we see that x 7→ vd(Ax + b) simply corresponds to the use of a different basis of Rd[X]. The result follows by applying Lemma 3 and the proof is complete.\nB.2 Proof of Lemma 2\nRecall that the orthogonal polynomials satisfy for all α ∈ Npd\n〈Pα, Xβ〉 = 0 if α <gl β, 〈Pα, Pα〉µ = 1, 〈Pα, Xβ〉µ = 0 if β <gl α, 〈Pα, Xα〉µ > 0. (3)\nLemma 2 Let µ satisfy Assumption 1, then Qµ,d = ∑ α∈Npd\nP 2α, where the family {Pα}α∈Npd is defined by (3) and ∫ Rp Qµ,d(x)dµ(x) = s(d).\nProof : Let Dd(µ) be the lower triangular matrix which rows are the coefficients of the polynomials Pα defined in (3) ordered by ≤gl. From properties in (3), Dd(µ) is lower triangular with positive coefficients on its diagonal and therefore invertible. We have Dd(µ)Md(µ)Dd(µ)T = I , the identity. It follows that Md(µ) = Dd(µ)−1Dd(µ)−T and Md(µ)−1 = Dd(µ)TDd(µ). Plugging this in definition 1 and using equation (1) leads to the desired identity. The average value result follows because we manipulate an orthonormal basis of s(d) polymials, each of which has a square average value (with respect to µ) equal to 1.\nB.3 Proof of Theorem 1\nWe recall the the optimization problem.\nmin Qα,θα,α∈Npd\n1\n2 ∫ Rp ∑ α∈Npd Qα(x) 2dµ(x) (4)\ns.t. qαα ≥ exp(θα), α ∈ Npd, qαβ = 0, α <gl β, α, β ∈ Npd,∑ α∈Npd θα = 0.\nwhere Qα(x) = ∑ β qαβx β , α ∈ Npd. The statement of Theorem 1 goes as follows.\nTheorem 1 : Problem (4) is a convex optimization problem with a unique optimal solution (Q∗α, θ∗α), which satisfies Q∗α = √ λPα, α ∈ Npd, for some λ > 0. In particular, the distinguished SOS polynomial\nQµ,d = ∑ α∈Npd P 2α = 1 λ ∑ α∈Npd (Q∗α) 2,\nis (part of) the unique optimal solution of (4).\nProof :\nGeneral remarks. Observe that (4) is a convex optimization problem as we have∫ Rp ∑ α∈Npd Qα(x) 2dµ(x) = ∑ α∈Npd\nqTαMd(µ)qα, which is strictly convex in {qα}α∈Npd . The proof is based on KKT optimality conditions for Problem (4). We first prove that any optimal solution should be of the form Q∗α = √ λPα, α ∈ Npd, for some λ > 0. Then we show that there exists a solution of the KKT system which has this form and finally that this solution is unique. The conclusion of Theorem 1 will then follow from Lemma 1. We begin with some notations that we will use throughout the proof.\nNotation. Let {eα}α∈Npd denote the canonical basis of R s(d) indexed by α ∈ Npd according to ≤gl order. The orthonormal polynomials {Pα}α∈Npd (with respect to µ) are uniquely defined. For each α ∈ Npd, we write pα = (pαβ)β∈Npd ∈ R\ns(d) the coefficients of the polynomial Pα. By construction of Pα, for every α, β ∈ Npd, α <gl β, pαβ = 0 and pαα > 0.\nOptimality conditions Problem (4) is strictly feasible, we can choose any θ such that ∑ α θα = 0 and for every α ∈ Npd, set Qα := κPα for some sufficiently large κ > 0. Therefore the KKT optimality conditions are necessary and sufficient for global optimality. We introduce Lagrange multipliers for problem (4): λα ≥ 0 for each inequality constraint, λαβ ∈ R for each linear equality constraint on polynomials with α <gl β and λ ∈ R for the last linear equality constraint on {θα}α∈Npd . The KKT optimality conditions for problem (4) can be written as follows\nλα ≥ 0, eTαq∗α ≥ exp(θ∗α), α ∈ N p d, (9)\neTβq ∗ α = 0, α, β ∈ N p d, α <gl β, (10)∑\nα∈Npd\nθ∗α = 0, (11)\nMd(µ)q ∗ α = λαeα + ∑ α<glβ λαβ eβ , α ∈ Npd, (12)\nλα exp(θ ∗ α) = λαe T αq ∗ α = λ, α ∈ N p d, (13)\nfor optimal variables θ∗α, polynomials Q ∗ α with coefficients q ∗ α ∈ Rs(d), for each α ∈ N p d. We next show that the part (Q∗α)α∈Npd of an optimal solution is necessarily a family of orthogonal polynomials.\nAny optimal solution has the form Q∗α = √ λPα, α ∈ Npd, for some λ > 0. Since KKT conditions are necessary and sufficient for optimality, we only focus on them. For each α 6= 0 and β <gl α, multiplying (12) by eβ , we obtain〈\nXβ , Q∗α 〉 µ = ∫ xβQ∗α(x) dµ(x) = e T β Md(µ)q ∗ α = λα e T β eα + ∑ α<glγ λαγ e T β eγ = 0. (14)\nSimilarly, multiplying (12) by q∗α yields for all α ∈ N p d,\n〈Q∗α, Q∗α〉µ = ∫ Q∗α(x) 2 dµ(x) = (q∗α) T Md(µ)q ∗ α = λα (q ∗ α) Teα = λ, (15)\nwhere we have used (13) for the last identity. In particular, with α = 0, Q∗0(x) = q ∗ 00 (≥ exp(θ∗0)) for all x and so\nλ = ∫ Q∗0(x) 2 dµ(x) = (q∗00) 2 ∫ dµ ≥ exp(2θ∗0),\nwhich shows that λ > 0. Next, combining (14), (15) and the condition (10), we immediately deduce\n〈 Q∗β , Q ∗ α 〉 µ = ∫ Q∗β(x)Q ∗ α(x) dµ(x) = { λ if α = β 0 otherwise.\n(16)\nFinally, for every α ∈ Npd, multiplying (12) by eα yields\n〈Xα, Q∗α〉µ = ∫ xαQ∗α(x) dµ(x) = e T αMd(µ)q ∗ α = λα > 0, α ∈ N p d. (17)\nThe last inequality follows from (13). Indeed, suppose λα = 0 for some α ∈ Npd, this would yield λ = 0. Since we have shown that λ > 0, it must also hold that λα > 0 for all α. Combining relations (10), (14), (16) and (17), we have shown that the {Q∗α}α∈Npd form a family of orthogonal polynomials with respect to µ. In addition, by the uniqueness of the orthonormal basis {Pα}α∈Npd , it follows from (16) that Q∗α = √ λPα for every α ∈ Npd.\nThere exists a solution of this form. Recall that, for each α ∈ Npd, pα = (pαβ)β∈Npd ∈ R s(d) is the vector of coefficients of the polynomial Pα which satisfies by construction pαα > 0 and pαβ = 0\nfor all β ∈ Npd, α <gl β. We use the following assignment for the primal and dual variables.\nλ =  ∏ α∈Npd pαα  −2 s(d) > 0 (18)\nλα = √ λeTαMd(µ)pα =\n√ λ pαα > 0, α ∈ Npd\nλαβ = √ λeTβMd(µ)pα, α, β ∈ N p d, α <gl β\nq∗α = √ λpα, θ ∗ α = log( √ λpαα), α ∈ Npd.\nUsing orthonormality of the polynomials {Pα}α∈Npd , it can be check that the assignment (18) satisfies KKT optimality conditions (9), (10), (11), (12) and (13). We have therefore constructed an optimal solution of (4) with the desired form.\nThe optimal solution is unique. From what precedes any optimal solution of (4) is necessarily such that Q∗α = √ λPα, for every α ∈ Nn, for some λ > 0. In addition the optimal value of (4) is s(d)λ. Suppose that there exists two different optimal solutions (Qα, θα)α∈Npd and (Q ′ α, θ ′ α)α∈Npd with associated dual variables (λ, λα, λαβ)α,β∈Npd and (λ ′, λ′α, λ ′ αβ)α,β∈Npd . Then necessarily λ = λ\n′, Qα = Q ′ α = √ λPα and λα, λ′α > 0 for all α ∈ N p d. But then from (13), √ λpαα = exp(θα) = exp(θ′α) and so θ ′ α = θα for every α ∈ N p d. Therefore the solution is unique and this concludes the proof of Theorem 1.\nB.4 Proof of Theorem 2\nTheorem 2 Let Assumption 1 hold and let x̄ ∈ Rp be fixed, arbitrary. Then\nQµ,d(x̄) −1 = min\nP∈Rd[X]\n{∫ P (x)2 dµ : P (x̄) = 1 } . (7)\nProof :\nFix an arbitrary P ∈ Rd[X] and x̄ ∈ Rp. Assume that P (x̄) = 1. Letting for all α ∈ Npd, aα = 〈P, Pα〉µ, by orthonormality, we have\nP = ∑ α∈Npd aαPα, (19)\n〈P, P 〉µ = ∑ α∈Npd a2α.\nThe assumption that P (x̄) = 1 can be used in conjonction with Cauchy-Schwartz inequality to obtain\n1 = P (x̄) (20) = ∑ α∈Npd aαPα(x̄)\n≤ ∑ α∈Npd a2α ∑ α∈Npd Pα(x̄) 2  = 〈P, P 〉µQµ,d(x̄),\nwhere the last equality comes from the definition of 〈·, ·〉µ and Lemma 2. There is equality in equation 20 if and only if aα = Pα(x̄)/Qµ,d(x̄) which always leads to P (x̄) = 1. This shows that the infimum is attained and concludes the proof."
    }, {
      "heading" : "C Details about the preparation of the datasets",
      "text" : "We reproduce the exact same manipulations as in the references. We downloaded the kddcup.data from the following repository\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/\nThis file contains 4898431 instances of network connections described by 42 features including the type of connection (attack or normal). We filter the records by keeping only those for which the variable logged in is positive. We kept the labels (type of connection) together with the four most important features: service, duration, src_bytes, dst_bytes. We applied to the three last variables (numerical) the function log(·+0.1)/10. We build four datasets with the four most frequent instances of service and group all the remaining records in the dataset others to get our five datasets.\nD Illustration of affine invariance\nThe following Figure illustrate the affine invariance property described in Lemma 1."
    } ],
    "references" : [ {
      "title" : "Anomaly detection: A survey. ACM computing surveys",
      "author" : [ "V. Chandola", "A. Banerjee", "V. Kumar" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "The relationship between Precision-Recall and ROC curves",
      "author" : [ "J. Davis", "M. Goadrich" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning (pp. 233-240)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Orthogonal polynomials of several variables",
      "author" : [ "C.F. Dunkl", "Y. Xu" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "A stable numerical method for inverting shape from moments",
      "author" : [ "G.H Golub", "P. Milanfar", "J. Varah" ],
      "venue" : "SIAM Journal on Scientific Computating",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1999
    }, {
      "title" : "A modification of a method for the detection of outliers in multivariate samples",
      "author" : [ "A.S. Hadi" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1994
    }, {
      "title" : "Measures with zeros in the inverse of their moment matrix",
      "author" : [ "J.W. Helton", "M.J.B. Lasserre" ],
      "venue" : "Putinar",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Level Sets and NonGaussian Integrals of Positively Homogeneous Functions",
      "author" : [ "J.B. Lasserre" ],
      "venue" : "International Game Theory Review,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "UCI Machine Learning Repository, http://archive.ics.uci.edu/ml University of California, Irvine, School of Information and Computer Sciences",
      "author" : [ "M. Lichman" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Unsupervised learning using MML",
      "author" : [ "J.J. Oliver", "R.A.Baxter", "C.S. Wallace" ],
      "venue" : "Proceedings of the International Conference on Machine Learning (pp. 364-372)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "Orthogonal polynomials",
      "author" : [ "G. Szegö" ],
      "venue" : "In Colloquium publications,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1974
    }, {
      "title" : "A Comparative Study of RNN for Outlier Detection in Data Mining",
      "author" : [ "G. Williams", "R. Baxter", "H. He", "S. Hawkins", "L. Gu" ],
      "venue" : "IEEE International Conference on Data Mining (p",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Accounting for complicated shapes is also related to computational geometry and nonlinear algebra applications, for example integral computation [9] and reconstruction of sets from moments data [4, 5, 10].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "Accounting for complicated shapes is also related to computational geometry and nonlinear algebra applications, for example integral computation [9] and reconstruction of sets from moments data [4, 5, 10].",
      "startOffset" : 194,
      "endOffset" : 204
    }, {
      "referenceID" : 0,
      "context" : "Furthermore the computational cost of evaluating the polynomial does not depend on the number of data points which is a crucial difference with existing nonparametric methods such as nearest neighbors or kernel based methods [1].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 2,
      "context" : "In a second step, we provide a mathematical interpretation that supports our empirical findings based on connections with orthogonal polynomials [3].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : "Finally, in Section 5 we perform numerical experiments on KDD cup network intrusion dataset [11].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "We refer the reader to [1] for a discussion of available methods for this task.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "For the sake of a fair comparison we have reproduced the experiments performed in [14] for the same dataset.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14].",
      "startOffset" : 242,
      "endOffset" : 248
    }, {
      "referenceID" : 8,
      "context" : "We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14].",
      "startOffset" : 265,
      "endOffset" : 269
    }, {
      "referenceID" : 10,
      "context" : "We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14].",
      "startOffset" : 300,
      "endOffset" : 304
    }, {
      "referenceID" : 9,
      "context" : "Actually, connection to orthogonal polynomials will show that the inverse function x 7→ Qμ,d(x) is called the Christoffel function in the literature [13, 3] (see also Section 4).",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "Actually, connection to orthogonal polynomials will show that the inverse function x 7→ Qμ,d(x) is called the Christoffel function in the literature [13, 3] (see also Section 4).",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "This is an important contrast with kernel based or distance based methods (such as nearest neighbors and one class SVM) for density estimation or outlier detection since they usually require at least O(n) operations for the evaluation of the model [1].",
      "startOffset" : 248,
      "endOffset" : 251
    }, {
      "referenceID" : 9,
      "context" : "2 Connection with orthogonal polynomials We define a classical [13, 3] family of orthonormal polynomials, {Pα}α∈Npd ordered according to ≤gl which satisfies for all α ∈ Npd 〈Pα, X〉 = 0 if α <gl β, 〈Pα, Pα〉μ = 1, 〈Pα, X〉μ = 0 if β <gl α, 〈Pα, X〉μ > 0.",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "2 Connection with orthogonal polynomials We define a classical [13, 3] family of orthonormal polynomials, {Pα}α∈Npd ordered according to ≤gl which satisfies for all α ∈ Npd 〈Pα, X〉 = 0 if α <gl β, 〈Pα, Pα〉μ = 1, 〈Pα, X〉μ = 0 if β <gl α, 〈Pα, X〉μ > 0.",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "Theorem 2 ([3]) Let Assumption 1 hold and let x̄ ∈ R be fixed, arbitrary.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 7,
      "context" : "We choose the KDD cup 99 network intrusion dataset (available at [11]) which consists of network connection data with labels describing whether they correspond to normal traffic or network intrusions.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "We follow [15] and [14] and construct five datasets consisting of labeled vectors in R, the label indicating normal traffic or network attack.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "016 The details on how these datasets are constructed are available in [15, 14] and are reproduced in Appendix C.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "The authors of [14] have compared different types of methods for outlier detection in the same experimental setting: methods based on robust estimation and Mahalanobis distance [6, 8], mixture model based methods [12] and recurrent neural network based methods.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "The authors of [14] have compared different types of methods for outlier detection in the same experimental setting: methods based on robust estimation and Mahalanobis distance [6, 8], mixture model based methods [12] and recurrent neural network based methods.",
      "startOffset" : 177,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "The authors of [14] have compared different types of methods for outlier detection in the same experimental setting: methods based on robust estimation and Mahalanobis distance [6, 8], mixture model based methods [12] and recurrent neural network based methods.",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 10,
      "context" : "13) Figure 2: Left: reproduction of the results described in [14] with the inverse moment SOS polynomial value as an outlyingness score (d = 3).",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "We adopt a slightly different measure of performance and use precision recall curves (see for example [2]) to measure performances in identifying network intrusions (the higher the curve, the better).",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "References [1] V.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] J.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] C.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] G.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7] J.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[9] J.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[11] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[12] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[13] G.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[14] G.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "We study a surprising phenomenon related to the representation of a cloud of data points using polynomials. We start with the previously unnoticed empirical observation that, given a collection (a cloud) of data points, the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately. This distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner from the inverse of the empirical moment matrix. In fact, this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function. This allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon. Among diverse potential applications, we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature.",
    "creator" : "LaTeX with hyperref package"
  }
}