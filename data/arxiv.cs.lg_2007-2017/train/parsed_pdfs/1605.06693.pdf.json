{
  "name" : "1605.06693.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Document Indexing Using Pivot Tree",
    "authors" : [ "Gaurav Singh", "Benjamin Piwowarski" ],
    "emails" : [ "gaurav.singh.15@ucl.ac.uk", "benjamin.piwowarski@lip6.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction and Related Work",
      "text" : "There are two main areas of research in information retrieval 1.) Search in metric spaces 2.) Search in non-metric spaces. A metric space basically refers to a similarity measure which follows all the metric properties like reflexivity, symmetry, non-negativity and triangle-inequality. All other properties can be achieved by trivial transformations, but triangle inequality is considered most important out of all others, since it is difficult to achieve it using trivial transformations and it can be effectively used in pruning elements. On the other hand, a non-metric space basically refers to a similarity measure that doesn’t follow triangle inequality. In such spaces, metric access methods using triangle inequality can not be applied directly. The retrieval approaches in non-metric spaces can be broadly categorized as embedding and classification.\nA metric space is described by a similarity measure that follows the properties of reflexivity, nonnegativity, symmetry and most importantly triangle inequality. A number of methods have been developed in the past to search metric spaces[4, 11, 14], most metric spaces can be search efficiently using the triangle inequality. [12, 8] discuss the use of range queries and kNN in metric space. A lot of research has been done in the field of information retrieval from nonmetric spaces, but a lot of similarity measures do not follow triangle inequality. In such non metric spaces, there are broadly two approaches followed, embedding and classification\nEmbedding basically refers to the conversion of nonmetric space into metric spaces. There are certain embedding methods which perform exact conversion[5]\nar X\niv :1\n60 5.\n06 69\n3v 1\n[ cs\n.I R\n] 2\n1 M\nay 2\nto metric space whereas others do approximate conversion to metric space[2]. A number of approximate embedding methods have been developed such as [2] that converts data objects into vector space. They introduce a query sensitive distance function together with the embedding method, in order to give different importance to different embedding dimensions for each query object. TriGen is another method developed to convert nonmetric spaces into metric spaces by using metric preserving and similarity invariant modifiers. But the author himself acknowledges, not all nonmetric measures are suitable to be converted by these methods. In the case of exact embedding methods, the most prominent is LCE[5], which tried to divide objects into groups and then adds a small local constant to all pairwise distances within a group to make them follow triangle inequality. This method although exact, becomes completely unscalable for large datasets, since it requires the computation of all possible triplets of objects within a cluster, which can be a huge computational cost. A number of other approximate embedding techniques like Fastmap[6], Metric Map[13], and Sparse Map[7] exist, but the only exact method with no false dismissals are LCE and CSE[10]\n[3] presented a non-metric clustering method based on distances to the socalled fiduciary templates (some selected random objects from the set). The distances to these fiduciary templates form a vector, which is used to decide in which cluster a new object belongs. [1] proposed a k-median clustering algorithm for nonmetric functions (specifically, the Kullback-Leibler divergence) that computes a(1 + )− approximation of the k-median problem.\nRecently [9] published maximum inner product based appraoch for querying documents. Their approach is based on creating tigher bounds as the query object traverses down the tree because of reduced number of documents at each new level and therefore a reduced radius. In the proposed method, we project query object on a set of orthogonal pivots as we descend down the pivot tree. We use the previous pivots to construct an orthogonal pivot to all other pivots in the descend path of the query object. We avoid any euclidean addition/subtraction operations that are expensive in high dimensional spaces. The proposed method is based on maximimizing the projection for group of documents on a set of orthogonal projectors."
    }, {
      "heading" : "2 Proposed Method",
      "text" : "We observe experimentally that the following relation holds for any given query q ∈ Rv, where v is the vocabulary size, projector S ∈ Rv×v, document d ∈ Rv and orthogonal projector S⊥ ∈ Rv×v\nqT d ≤ ‖Sq‖‖Sd‖+ ‖S⊥q‖‖S⊥d‖ (1) ≤ 1 + 2‖Sq‖‖Sd‖ − ‖Sq‖ − ‖Sd‖ (2)\nWe can bound the distance between a given document d and query q using the above inequality. We use the above inequality to bound ‖qT d‖ for all documents\ncontained in the subtree rooted at node Np, we select a radom pivot pn+1 from all such documents."
    }, {
      "heading" : "2.1 Updating the Projector",
      "text" : "We construct basis Bn for the subspace spanned by the vectors (pivots) p1, ...., pn in the descend path to the node Np from the root of the tree.\nBn = PnAn with Pn = ( p1 . . . pn ) Let pn+1 be the new vector to be added to the subspace then, we have the\nnew basis Bn+1:\nBn+1 = ( Bn x ) such that:\nx = y\n‖y‖ with y =\n( Id−BnB†n ) pn+1\nWe can get a projection vector(y) orthogonal to Bn using the relation:\n‖y‖2 = ‖pn+1‖2 − ∥∥BnB†npn+1∥∥2 = ‖pn+1‖2 − ∥∥B†npn+1∥∥2\nThen, denoting α = ‖y‖−1,\nBn+1 = ( PnAn α ( Id−BnB†n ) pn+1 ) (3)\n= ( Pn pn+1 )An −αAnA†nP †npn+1 0 α  (4)"
    }, {
      "heading" : "2.2 Updating the Similarity",
      "text" : "We compute the value of ∥∥∥B†n+1D∥∥∥ from ∥∥B†nD∥∥ for all the documents(D) contained in the subtree rooted at node Np. Each node of pivot tree contains\nmax( ∥∥∥B†n+1D∥∥∥2) and min(∥∥∥B†n+1D∥∥∥2) ∀D ∈ Dp where Dp is the set of all documents contained in the subtree rooted at node Np.\n∥∥D†Bn+1∥∥2 = ∥∥∥∥∥∥(D†Pn D†pn+1 ) An −αAnA†nP †npn+1 0 α ∥∥∥∥∥∥ 2\n(5)\n= ∥∥(D†PnAn αD†pn+1 − αD†PnAnA†nP †npn+1 )∥∥2 (6)\n= ∥∥D†Bn∥∥2 + ∥∥αD†pn+1 − αD†PnAnA†nP †npn+1∥∥ (7)"
    }, {
      "heading" : "2.3 Algorithm",
      "text" : "In this section we describe the algorithm we use to construct the pivot tree. We then describe an algorithm to search the pivot tree using a given query.\n1. Algorithm SelectPivot(Data S)\nSelectPivot(Data S)\nPick some random pivots P ∈ S Choose a random pivot p ∈ P s.t. argmaxp( ∑ ‖pT pSi‖2) ∀Si ∈ S return (p)\n2. Algorithm MakeSplit(Data S, Pivot p)"
    }, {
      "heading" : "MakeSplit(Data S, Pivot p)",
      "text" : "A ← {s ∈ S: ‖DT pn+1‖2 > c} B ← S/A return (A,B)\n3. Algorithm UpdateProjections(Data Dl, Pivot p, A)"
    }, {
      "heading" : "UpdateProjections((Data D, Pivot p, A)",
      "text" : "Di.Projections ← update(Di,p,A) ∀Di ∈ D; # Using eqn. 5\n4. Algorithm BuildTree(Data S)"
    }, {
      "heading" : "BuildTree(Data S)",
      "text" : "Input ← S Output ← Tree T T.S ← S T.min ← min(S.Projections) T.max ← max(S.Projections) if (|S| ≤ No)\nreturn T\nT.p ← SelectPivot(Data S) Dl,Dr ← MakeSplit(T.S, T.p) # Using eqn. 4 T.A ← UpdateA(pivot p) # Using eqn. 4 T.P ← UpdateP(pivot p) # Using eqn 7. Dl.Projections ← UpdateProjections(Data Dl, Pivot p, T.A) T.left ← BuildTree(Data Dl) T.right ← BuildTree(Data Dr) return T\n5. Algorithm SearchTree(Query S, Tree T)"
    }, {
      "heading" : "SearchTree(Query S, Tree T)",
      "text" : "Input ← Query S, Tree T Output ← Document Set D Bl ← ComputeBound(Tree T.left, Query q) # using eqn 2 Br ← ComputeBound(Tree T.right, Query q) #using eqn 2\n#getLast: Returns the element with least similarity with query if (Bl ≥ getLast(queue)) searchL=True if (Br ≥ getLast(queue)) searchR=True\nif(searchL and searchR)\nif (Bl > Br) queue ← SearchTree(Query S, Tree T.left) else\nqueue ← SearchTree(Query S, Tree T.right) else if (searchL and !searchR)\nqueue ← SearchTree(Query S, Tree T.left) else if (!searchL and searchR)\nqueue ← SearchTree(Query S, Tree T.right) else\nreturn (queue)"
    }, {
      "heading" : "3 Experimentation and Results",
      "text" : "We present in this section experimental results for the proposed method based on the MTA (Maximized Trace Approach) against state of the art method MIP(Maximum Inner Product) appraoch. The precision versus prunes is drawn for both appraoches by reducing the bound artificially, reduction in bound leads to more prunes, but reduced precision. We can see in Figure 1 that MTA outperforms MIP[9] in terms of both ranking (as measured by spearman distance) and precision for different values of prunes."
    } ],
    "references" : [ {
      "title" : "Clustering for metric and nonmetric distance measures",
      "author" : [ "Marcel R Ackermann", "Johannes Blömer", "Christian Sohler" ],
      "venue" : "ACM Transactions on Algorithms (TALG),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Non-metric biometric clustering",
      "author" : [ "Glenn Becker", "Mark Potts" ],
      "venue" : "In Biometrics Symposium,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Searching in metric spaces",
      "author" : [ "Edgar Chávez", "Gonzalo Navarro", "Ricardo Baeza-Yates", "José Luis Marroqúın" ],
      "venue" : "ACM Comput. Surv.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Efficient similarity search in nonmetric spaces with local constant embedding",
      "author" : [ "Lei Chen", "Xiang Lian" ],
      "venue" : "IEEE Trans. on Knowl. and Data Eng.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Fastmap: A fast algorithm for indexing, datamining and visualization of traditional and multimedia datasets",
      "author" : [ "Christos Faloutsos", "King-Ip Lin" ],
      "venue" : "SIGMOD Rec.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1995
    }, {
      "title" : "Cluster-preserving embedding of proteins",
      "author" : [ "Hristescu Gabriela", "Farach Martin" ],
      "venue" : "Technical report,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1999
    }, {
      "title" : "Properties of embedding methods for similarity searching in metric spaces",
      "author" : [ "G.R. Hjaltason", "H. Samet" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Maximum inner-product search using cone trees",
      "author" : [ "Parikshit Ram", "Alexander G. Gray" ],
      "venue" : "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Going metric: Denoising pairwise data",
      "author" : [ "Volker Roth", "Julian Laub", "Joachim M Buhmann", "Klaus-Robert Müller" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "Foundations of Multidimensional and Metric Data Structures (The Morgan Kaufmann Series in Computer Graphics and Geometric Modeling)",
      "author" : [ "Hanan Samet" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Optimal multi-step k-nearest neighbor search",
      "author" : [ "Thomas Seidl", "Hans-Peter Kriegel" ],
      "venue" : "In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1998
    }, {
      "title" : "An index structure for data mining and clustering",
      "author" : [ "Xiong Wang", "Jason T.L. Wang", "King ip Lin", "Dennis Shasha", "Bruce A. Shapiro", "Kaizhong Zhang" ],
      "venue" : "Knowledge and Information Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2000
    }, {
      "title" : "Similarity Search: The Metric Space Approach",
      "author" : [ "Pavel Zezula", "Giuseppe Amato", "Vlastislav Dohnal", "Michal Batko" ],
      "venue" : "Springer Publishing Company, Incorporated,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "A number of methods have been developed in the past to search metric spaces[4, 11, 14], most metric spaces can be search efficiently using the triangle inequality.",
      "startOffset" : 75,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "A number of methods have been developed in the past to search metric spaces[4, 11, 14], most metric spaces can be search efficiently using the triangle inequality.",
      "startOffset" : 75,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "A number of methods have been developed in the past to search metric spaces[4, 11, 14], most metric spaces can be search efficiently using the triangle inequality.",
      "startOffset" : 75,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "[12, 8] discuss the use of range queries and kNN in metric space.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "[12, 8] discuss the use of range queries and kNN in metric space.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "There are certain embedding methods which perform exact conversion[5] ar X iv :1 60 5.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "In the case of exact embedding methods, the most prominent is LCE[5], which tried to divide objects into groups and then adds a small local constant to all pairwise distances within a group to make them follow triangle inequality.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "A number of other approximate embedding techniques like Fastmap[6], Metric Map[13], and Sparse Map[7] exist, but the only exact method with no false dismissals are LCE and CSE[10] [3] presented a non-metric clustering method based on distances to the socalled fiduciary templates (some selected random objects from the set).",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "A number of other approximate embedding techniques like Fastmap[6], Metric Map[13], and Sparse Map[7] exist, but the only exact method with no false dismissals are LCE and CSE[10] [3] presented a non-metric clustering method based on distances to the socalled fiduciary templates (some selected random objects from the set).",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "A number of other approximate embedding techniques like Fastmap[6], Metric Map[13], and Sparse Map[7] exist, but the only exact method with no false dismissals are LCE and CSE[10] [3] presented a non-metric clustering method based on distances to the socalled fiduciary templates (some selected random objects from the set).",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "A number of other approximate embedding techniques like Fastmap[6], Metric Map[13], and Sparse Map[7] exist, but the only exact method with no false dismissals are LCE and CSE[10] [3] presented a non-metric clustering method based on distances to the socalled fiduciary templates (some selected random objects from the set).",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "A number of other approximate embedding techniques like Fastmap[6], Metric Map[13], and Sparse Map[7] exist, but the only exact method with no false dismissals are LCE and CSE[10] [3] presented a non-metric clustering method based on distances to the socalled fiduciary templates (some selected random objects from the set).",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "[1] proposed a k-median clustering algorithm for nonmetric functions (specifically, the Kullback-Leibler divergence) that computes a(1 + )− approximation of the k-median problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "Recently [9] published maximum inner product based appraoch for querying documents.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "We can see in Figure 1 that MTA outperforms MIP[9] in terms of both ranking (as measured by spearman distance) and precision for different values of prunes.",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2016,
    "abstractText" : "We present a novel method for efficiently searching top-k neighbors for documents represented in high dimensional space of terms based on the cosine similarity. Mostly, documents are stored as bagof-words tf-idf representation. One of the most used ways of computing similarity between a pair of documents is cosine similarity between the vector representations, but cosine similarity is not a metric distance measure as it doesn’t follow triangle inequality, therefore most metric searching methods can not be applied directly. We propose an efficient method for indexing documents using a pivot tree that leads to efficient retrieval. We also study the relation between precision and efficiency for the proposed method and compare it with a state of the art in the area of document searching based on inner product.",
    "creator" : "TeX"
  }
}