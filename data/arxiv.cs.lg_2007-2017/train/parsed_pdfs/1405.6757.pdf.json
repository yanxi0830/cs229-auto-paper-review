{
  "name" : "1405.6757.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces",
    "authors" : [ "Sridhar Mahadevan", "Bo Liu", "Philip Thomas", "Will Dabney", "Steve Giguere", "Ji Liu" ],
    "emails" : [ "mahadeva@cs.umass.edu,", "boliu@cs.umass.edu,", "imgemp@cs.umass.edu,", "PThomasCS@gmail.com", "sgiguere9@gmail.com", "amarack@gmail.com", "jliu@cs.rochester.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Reinforcement learning is a simple, and yet, comprehensive theory of learning that simultaneously models the adaptive behavior of artificial agents, such as robots and autonomous software programs, as well as attempts to explain the emergent behavior of biological systems. It also gives rise to computational ideas that provide a powerful tool to solve problems involving sequential prediction and decision making. Temporal difference learning is the most widely used method to solve reinforcement learning problems, with a rich history dating back more than three decades. For these and many other reasons, devel-\n1 This article is currently not under review for the journal Foundations and Trends in ML, but will be submitted for formal peer review at some point in the future, once the draft reaches a stable “equilibrium” state.\nar X\niv :1\noping a complete theory of reinforcement learning, one that is both rigorous and useful has been an ongoing research investigation for several decades. In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified “safely” guarantees, and remains in a stable region of the parameter space (iii) how to design “off-policy” temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators. The most important idea that emerges is the use of primal dual spaces connected through the use of a Legendre transform. This allows temporal difference updates to occur in dual spaces, allowing a variety of important technical advantages. The Legendre transform, as we show, elegantly generalizes past algorithms for solving reinforcement learning problems, such as natural gradient methods, which we show relate closely to the previously unconnected framework of mirror descent methods. Equally importantly, proximal operator theory enables the systematic development of operator splitting methods that show how to safely and reliably decompose complex products of gradients that occur in recent variants of gradient-based temporal difference learning. This key technical innovation makes it possible to finally design “true” stochastic gradient methods for reinforcement learning. Finally, Legendre transforms enable a variety of other benefits, including modeling sparsity and domain geometry. Our work builds extensively on recent work on the convergence of saddle-point algorithms, and on the theory of monotone operators in Hilbert spaces, both in optimization and for variational inequalities. The latter framework, the subject of another ongoing investigation by our group, holds the promise of an even more elegant framework for reinforcement learning. Its explication is currently the topic of a further monograph that will appear in due course.\nDedicated to Andrew Barto and Richard Sutton for inspiring a generation of researchers to the study of reinforcement learning.\nAlgorithm 1 TD (1984)\n(1) δt = rt + γφ ′ t T θt − φTt θt (2) θt+1 = θt + βtδt\nAlgorithm 2 GTD2-MP (2014)\n(1) wt+ 1 2 = wt + βt(δt − φTt wt)φt, θt+ 1\n2 = proxαth\n( θt + αt(φt − γφ′t)(φTt wt) ) (2) δt+ 1\n2 = rt + γφ\n′ t T θt+ 1 2 − φTt θt+ 1 2\n(3) wt+1 = wt + βt(δt+ 1\n2 − φTt wt+ 1 2 )φt ,\nθt+1 = proxαth ( θt + αt(φt − γφ′t)(φTt wt+ 1 2 ) )\nContents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Elements of the Overall Framework 2 1.2 Illustrating the Solution 8 1.3 Safe Reinforcement Learning 9 1.4 True Stochastic Gradient Reinforcement Learning 11 1.5 Sparse Reinforcement Learning using Mirror Descent 12 1.6 Summary 13"
    }, {
      "heading" : "2 Background 15",
      "text" : "2.1 Reinforcement Learning 15 2.2 Stochastic Composite Optimization 18 2.3 Subdifferentials and Monotone Operators 25 2.4 Convex-concave Saddle-Point First Order Algorithms 26 2.5 Abstraction through Proximal Operators 27 2.6 Decomposition through Operator Splitting 28 2.7 Natural Gradient Methods 31 2.8 Summary 32\ni\nii Contents"
    }, {
      "heading" : "3 Sparse Temporal Difference Learning in Primal",
      "text" : "Dual Spaces 33\n3.1 Problem Formulation 34 3.2 Mirror Descent RL 35 3.3 Convergence Analysis 39 3.4 Experimental Results: Discrete MDPs 41 3.5 Experimental Results: Continuous MDPs 44 3.6 Comparison of Link Functions 46 3.7 Summary 47"
    }, {
      "heading" : "4 Regularized Off-Policy Temporal Difference",
      "text" : "Learning 48\n4.1 Introduction 49 4.2 Problem Formulation 50 4.3 Algorithm Design 54 4.4 Theoretical Analysis 57 4.5 Empirical Results 57 4.6 Summary 61"
    }, {
      "heading" : "5 Safe Reinforcement Learning using Projected",
      "text" : "Natural Actor Critic 62\n5.1 Introduction 62 5.2 Related Work 64 5.3 Equivalence of Natural Gradient Descent and Mirror\nDescent 65\n5.4 Projected Natural Gradients 66 5.5 Compatibility of Projection 67 5.6 Natural Actor-Critic Algorithms 69 5.7 Projected Natural Actor-Critics 70 5.8 Case Study: Functional Electrical Stimulation 72 5.9 Case Study: uBot Balancing 73 5.10 Summary 75\n6 True Stochastic Gradient Temporal Difference\nContents iii\nLearning Algorithms 77\n6.1 Introduction 78 6.2 Background 79 6.3 Problem Formulation 80 6.4 Algorithm Design 82 6.5 Accelerated Gradient Temporal Difference Learning\nAlgorithms 84\n6.6 Theoretical Analysis 84 6.7 Experimental Study 87 6.8 Summary 88"
    }, {
      "heading" : "7 Variational Inequalities: The Emerging Frontier of",
      "text" : "Machine Learning 90\n7.1 Variational Inequalities 91 7.2 Algorithms for Variational Inequalities 97"
    }, {
      "heading" : "8 Appendix: Technical Proofs 102",
      "text" : "8.1 Convergence Analysis of Saddle Point Temporal\nDifference Learning 102\n8.2 Convergence Analysis of True Gradient Temporal\nDifference Learning 103\n1 Introduction\nIn this chapter, we lay out the elements of our novel framework for reinforcement learning [1], based on doing temporal difference learning not in the primal space, but in a dual space defined by a so-called mirror map. We show how this technical device holds the fundamental key to solving a whole host of unresolved issues in reinforcement learning, from designing stable and reliable off-policy algorithms, to making algorithms achieve safety guarantees, and finally to making them scalable in high dimensions. This new vision of reinforcement learning developed by us over the past few years yields mathematically rigorous solutions to longstanding important questions in the field, which have remained unresolved for almost three decades. We introduce the main concepts in this chapter, from proximal operators to the mirror descent and the extragradient method and its non-Euclidean generalization, the mirrorprox method. We introduce a powerful decomposition strategy based on operator splitting, exploiting deep properties of monotone operators in Hilbert spaces. This technical device, as we show later, is fundamental in designing “true” stochastic gradient methods for reinforcement learning, as it helps to decompose the complex product of terms that occur in recent work on gradient temporal difference learning. We provide\n1\n2 Introduction\nexamples of the benefits of our framework, showing each of the four key pieces of our solution: the improved performance of our new off-policy temporal difference methods over previous gradient TD methods, like TDC and GTD2 [2]; how we are able to generalize natural gradient actor critic methods using mirror maps, and achieve safety guarantees to control learning in complex robots; and finally, elements of our saddle point reformulation of temporal difference learning. The goal of this chapter is to lay out the sweeping power of our primal dual framework for reinforcement learning. The details of our approach, including technical proofs, algorithms, and experimental validations are relegated to future chapters."
    }, {
      "heading" : "1.1 Elements of the Overall Framework",
      "text" : ""
    }, {
      "heading" : "1.1.1 Primal Dual Mirror Maps",
      "text" : "In this section, we provide a succinct explanation of the overall framework, leaving many technical details to future chapters. Central to the proposed framework is the notion of mirror maps, which facilitates doing temporal learning updates not just in the usual primal space, but also in a dual space. More precisely, Φ : D → R for some domain D is a mirror map if it is strongly convex, differentiable, and the gradient of Φ has the range Rn (i.e., takes on all possible vector values). Instead of doing gradient updates in the primal space, we do gradient updates in the dual space, which correspond to:\n∇Φ(y) = ∇Φ(x)− α∇f(x) The step size or learning rate α is a tunable parameter. To get back to the primal space, we use the conjugate mapping ∇Φ∗, which can be shown to also correspond to the inverse mapping (∇Φ)−1, where the conjugate of a function f(x) is defined as\nf∗(y) = sup x (〈x, y〉 − f(x)) .\nHere 〈x, y〉 = xT y, the standard inner product on Rn. When f(x) is differentiable and smooth, the conjugate function f∗(y) achieves the maximum value at x∗ = ∇f(x). This is a special instance of the “Leg-\n1.1. Elements of the Overall Framework 3\nendre” transform [3]. To achieve “safety” guarantees in reinforcement learning, such as ensuring a robot learning a task never moves into dangerous values of the parameter space, we need to ensure that when domain constraints are not violated. We use Bregman divergences [4] to ensure that safety constraints are adhered to, where the projection is defined as:\nΠΦX (y) = argminX∩DDΦ(x, y).\nA distance generating function Φ(x) is defined as a strongly convex function which is differentiable. Given such a function Φ, the Bregman divergence associated with it is defined as:\nDΦ(x, y) = Φ(x)− Φ(y)− 〈∇Φ(y), x− y〉\nIntuitively, the Bregman divergence measures the difference between the value of a strongly convex function Φ(x) and the estimate derived from the first-order Taylor series expansion at Φ(y). Many widely used distance measures turn out to be special cases of Bregman divergences, such as Euclidean distance (where Φ(x) = 12‖x‖2 ) and Kullback Liebler divergence (where Φ(x) = ∑ i xi log2 xi, the negative entropy function). In general, Bregman divergences are non-symmetric, but projections onto a convex set with respect to a Bregman divergence is well-defined."
    }, {
      "heading" : "1.1.2 Mirror Descent, Extragradient, and Mirror Prox Methods",
      "text" : "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8]. Figure 1.1 illustrates the mirror descent method, and Figure 1.2 illustrates the extragradient method.\nThe extragradient method was developed to solve variational inequalities (VIs), a beautiful generalization of optimization. Variational inequalities, in the infinite-dimensional setting, were originally proposed by Hartman and Stampacchia [10] in the mid-1960s in the\n4 Introduction\ncontext of solving partial differential equations in mechanics. Finitedimensional VIs rose in popularity in the 1980s partly as a result of work by Dafermos [11], who showed that the traffic network equilibrium problem could be formulated as a finite-dimensional VI. This advance inspired much follow-on research, showing that a variety of equilibrium problems in economics, game theory, sequential decision-making etc. could also be formulated as finite-dimensional VIs – the books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs. While we leave the full explication of the VI approach to reinforcement learning to a subsequent monograph, we discuss in the last chapter a few intriguing aspects of this framework that is now the subject of another investigation by our group. A VI(F,K) is specified by a vector field F and a feasible set K. Solving a VI means finding an element x∗ within the feasible set K where the vector field F (x∗) is pointed inwards and makes an acute angle with all vectors x − x∗. Equivalently, −F (x∗) belongs in the normal cone of the convex feasible set K at the point x∗. Any optimization problem reduces to a VI, but the converse is only true for vector fields F whose Jacobians are symmetric. A more detailed discussion of VIs is beyond the scope of this paper, but a longer\n1.1. Elements of the Overall Framework 5\nsummary is given in Chapter 7.\nIn Figure 1.2, the concept of extragradient is illustrated. A simple way to understand the figure is to imagine the vector field F here is defined as the gradient ∇f(x) of some function being minimized. In that case, the mapping −F (xk) points as usual in the direction of the negative gradient. However, the clever feature of extragradient is that it moves not in the direction of the negative gradient at xk, but rather in the direction of the negative gradient at the point yk, which is the projection of the original gradient step onto the feasible set K. We will see later how this property of extragradient makes its appearance in accelerating gradient temporal difference learning algorithms, such as TDC [2].\nThe mirror-prox method generalizes the extragradient method to non-Euclidean geometries, analogous to the way mirror descent generalizes the regular gradient method. The mirror-prox algorithm (MP) [7] is a first-order approach that is able to solve saddle-point problems at a convergence rate of O(1/t). The MP method plays a key role in our framework as our approach extensively uses the saddle point reformulation of reinforcement learning developed by us [14]. Figure 1.3 illustrates the mirror-prox method.\n6 Introduction"
    }, {
      "heading" : "1.1.3 Proximal Operators",
      "text" : "We now review the concept of proximal mappings, and then describe its relation to the mirror descent framework. The proximal mapping associated with a convex function h is defined as:\nproxh(x) = argminu∈X\n( h(u) + 1\n2 ‖u− x‖2 ) If h(x) = 0, then proxh(x) = x, the identity function. If h(x) = IC(x), the indicator function for a convex set C, then proxIC (x) = ΠC(x), the projector onto set C. For learning sparse representations, the case when h(w) = λ‖w‖1 (the L1 norm of w) is particularly important. In this case:\nproxh(w)i =  wi − λ, if wi > λ 0, if |wi| ≤ λ wi + λ, otherwise\nAn interesting observation follows from noting that the projected subgradient method can be written equivalently using the proximal map-\n1.1. Elements of the Overall Framework 7\nping as:\nwk+1 = argminw∈X ( 〈w, ∂f(wk)〉+ 1\n2αk ‖w − wk‖2 ) where X is a closed convex set. An intuitive way to understand this equation is to view the first term as requiring the next iterate wk+1 to move in the direction of the (sub) gradient of f at wk, whereas the second term requires that the next iterate wk+1 not move too far away from the current iterate wk.\nWith this introduction, we can now introduce the main concept of mirror descent, which was originally proposed by Nemirovksi and Yudin [5]. We follow the treatment in [6] in presenting the mirror descent algorithm as a nonlinear proximal method based on a distance generator function that is a Bregman divergence [4]. The general mirror descent procedure can thus be defined as:\nwk+1 = argminw∈X ( 〈w, ∂f(wk)〉+ 1\nαk Dψ(w,wk) ) The solution to this optimization problem can be stated succinctly as the following generalized gradient descent algorithm, which forms the core procedure in mirror descent:\nwk+1 = ∇ψ∗ (∇ψ(wk)− αk∂f(wk))\nAn intuitive way to understand the mirror descent procedure specified in Equation 1.1.3 is to view the gradient update in two stages: in the first step, the gradient is computed in the dual space using a set of auxiliary weights θ, and subsequently the updated auxilary weights are mapped back into the primal space w. Mirror descent is a powerful first-order optimization method that is in some cases “optimal” in that it leads to low regret. One of the earliest and most successful applications of mirror descent is Positron Emission Tomography (PET) imaging, which involves minimizing a convex function over the unit simplex X. It is shown in [15] that the mirror descent procedure specified in Equation 1.1.3 with the Bregman divergence defined by the p-norm function [16] can outperform regular projected subgradient method by a factor nlogn where n is the dimensionality of the space. For\n8 Introduction\nhigh-dimensional spaces, this ratio can be quite large. We will discuss below specific choices of Bregman divergences in the target application of this framework to reinforcement learning."
    }, {
      "heading" : "1.1.4 Operator Splitting Strategies",
      "text" : "In our framework, a key insight used to derive a true stochastic gradient method for reinforcement learning is based on the powerful concept of operator splitting [17, 18]. Figure 1.4 illustrates this concept for the convex feasibility problem, where we are given a collection of convex sets, and have to find a point in their intersection. This problem originally motivated the development of Bregman divergences [4]. The convex feasibility problem is an example of many real-world problems, such as 3D voxel reconstruction in brain imaging [15], a high-dimensional problem that mirror descent was originally developed for. To find an element in the common intersection of two sets A and B in Figure 1.4, a standard method called alternating projections works as follows. Given an initial point x0, the first step projects it to one of the two convex sets, say A, giving the point ΠA(x0). Since A is convex, this is a uniquely defined point. The next step is to project the new point on the second set B, giving the next point ΠB(ΠA(x0)). The process continues, ultimately leading to the desired point common to the two sets. Operator splitting studies a generalized version of this problem, where the projection problem is replaced by the proximal operator problem, as described above. Many different operator splitting strategies have been developed, such as Douglas Rachford splitting [18], which is a generalization of widely used distributed optimization methods like Alternating Direction Method of Multipliers [19]. We will see later that using a sophisticated type of operator splitting strategy, we can address the problem of off-policy temporal difference learning."
    }, {
      "heading" : "1.2 Illustrating the Solution",
      "text" : "Now that we have described the broad elements of our framework, we give a few select examples of the tangible solutions that emerge to the problem of designing safe, reliable, and stable reinforcement learning algorithms. We pick three cases: how to design a “safe” reinforcement\n1.3. Safe Reinforcement Learning 9\nlearning method; how to design a “true” stochastic gradient reinforcement learning method; and finally, how to design a “robust” reinforcement learning method that does not overfit its training experience."
    }, {
      "heading" : "1.3 Safe Reinforcement Learning",
      "text" : "Figure 1.5 shows a complex high-degree of freedom humanoid robot. Teaching robots complex skills is a challenging problem, particularly since reinforcement learning not only may take a long time, but also because it may cause such robots to operate in dangerous regions of the parameter space. Our proposed framework solves this problem by establishing a key technical result, stated below, between mirror descent and the well-known, but previously unrelated, class of algorithms called natural gradient [22]. We develop the projected natural actor critic (PNAC) algorithm, a policy gradient method that exploits this equivalence to yield a safe method for training complex robots using reinforcement learning. We explain the significance of the below result connecting mirror descent and natural gradient methods later in this paper when we describe a novel class of methods called projected natural actor critic (PNAC).\n10 Introduction\nTheorem 1.3.1. The natural gradient descent update at step k with metric tensor Gk , G(xk):\nxk+1 = xk − αkG−1k ∇f(xk),\nis equivalent to the mirror descent update at step k, with ψk(x) = (1/2)xᵀGkx.\n1.4. True Stochastic Gradient Reinforcement Learning 11"
    }, {
      "heading" : "1.4 True Stochastic Gradient Reinforcement Learning",
      "text" : "First-order temporal difference learning is a widely used class of techniques in reinforcement learning. Although many more sophisticated methods have been developed over the past three decades, such as least-squares based temporal difference approaches, including LSTD [23], LSPE [24] and LSPI [25], first-order temporal difference learning algorithms may scale more gracefully to high dimensional problems. Unfortunately, the initial class of TD methods was known to converge only when samples are drawn “on-policy”. This motivated the development of the gradient TD (GTD) family of methods [26]. A crucial step in the development of our framework was the development of a novel saddle-point framework for sparse regularized GTD [14]. However, there have been several unresolved questions regarding the current off-policy TD algorithms. (1) The first is the convergence rate of these algorithms. Although these algorithms are motivated from the gradient of an objective function such as mean-squared projected Bellman error (MSPBE) and NEU [26], they are not true stochastic gradient methods with respect to these objective functions, as pointed out in [27], which make the convergence rate and error bound analysis difficult, although asymptotic analysis has been carried out using the ODE approach. (2) The second concern is regarding acceleration. It is believed that TDC performs the best so far of the GTD family of algorithms. One may intuitively ask if there are any gradient TD algorithms that can outperform TDC. (3) The third concern is regarding compactness of the feasible set θ. The GTD family of algorithms all assume that the feasible set θ is unbounded, and if the feasible set θ is compact, there is no theoretical analysis and convergence guarantee. (4) The fourth question is on regularization: although the saddle point framework proposed in [14] provides an online regularization framework for the GTD family of algorithms, termed as RO-TD, it is based on the inverse problem formulation and is thus not quite explicit. One further question is whether there is a more straightforward algorithm, e.g, the regularization is directly based on the MSPBE and NEU objective functions.\nBiased sampling is a well-known problem in reinforcement learning.\n12 Introduction\nBiased sampling is caused by the stochasticity of the policy wherein there are multiple possible successor states from the current state where the agent is. If it is a deterministic policy, then there will be no biased sampling problem. Biased sampling is often caused by the product of the TD errors, or the product of TD error and the gradient of TD error w.r.t the model parameter θ. There are two ways to avoid the biased sampling problem, which can be categorized into double sampling methods and two-time-scale stochastic approximation methods.\nIn this paper, we propose a novel approach to TD algorithm design in reinforcement learning, based on introducing the proximal splitting framework [28]. We show that the GTD family of algorithms are true stochastic gradient descent (SGD) methods, thus making their convergence rate analysis available. New accelerated off-policy algorithms are proposed and their comparative study with RO-TD is carried out to show the effectiveness of the proposed algorithms. We also show that primal-dual splitting is a unified first-order optimization framework to solve the biased sampling problem. Figure 1.6 compares the performance of our newly designed off-policy methods compared to previous methods, like TDC and GTD2 on the classic 5-state Baird counterexample. Note the significant improvement of TDC-MP over TDC: the latter converges much more slowly, and has much higher variance. This result is validated not only by experiments, but also by a detailed theoretical analysis of sample convergence, which goes beyond the previous asymptotic convergence analysis of off-policy methods."
    }, {
      "heading" : "1.5 Sparse Reinforcement Learning using Mirror Descent",
      "text" : "How can we design reinforcement learning algorithms that are robust to overfitting? In this paper we explore a new framework for (on-policy convergent) TD learning algorithms based on mirror descent and related algorithms. Mirror descent can be viewed as an enhanced gradient method, particularly suited to minimization of convex functions in high-dimensional spaces. Unlike traditional temporal difference learning methods, mirror descent temporal difference learning undertakes updates of weights in both the dual space and primal space, which are linked together using a Legendre transform. Mirror descent can be\n1.6. Summary 13\nviewed as a proximal algorithm where the distance-generating function used is a Bregman divergence. We will present a new class of proximalgradient based temporal-difference (TD) methods based on different Bregman divergences, which are more powerful than regular TD learning. Examples of Bregman divergences that are studied include p-norm functions, and Mahalanobis distance based on the covariance of sample gradients. A new family of sparse mirror-descent reinforcement learning methods are proposed, which are able to find sparse fixed-point of an l1-regularized Bellman equation at significantly less computational cost than previous methods based on second-order matrix methods. Figure 1.7 illustrates a sample result, showing how the mirror descent variant of temporal difference learning results in faster convergence, and much lower variance (not shown) on the classic mountain car task [1]."
    }, {
      "heading" : "1.6 Summary",
      "text" : "We provided a brief overview of our proposed primal-dual framework for reinforcement learning. The fundamentally new idea underlying the\n14 Introduction\napproach is the systematic use of mirror maps to carry out temporal difference updates, not in the original primal space, but rather in a dual space. This technical device, as we will show in subsequent chapters, provides for a number of significant advantages. By choosing the mirror map carefully, we can generalize popular methods like natural gradient based actor-critic methods, and provide safety guarantees. We can design more robust temporal difference learning methods that are less prone to overfitting the experience of an agent. Finally, we can exploit proximal mappings to design a rich variety of true stochastic gradient methods. These advantages, when combined, provide a compelling case for the fundamental correctness of our approach. However, much remains to be done in more fully validating the proposed framework on large complex real-world applications, as well as doing a deeper theoretical analysis of our proposed approach. These extensions will be the subject of ongoing research by us in the years ahead.\n2 Background\nIn this chapter we introduce relevant background material that form the two cornerstones of this paper: reinforcement learning and firstorder stochastic composite optimization. The Markov decision process (MDP) model, value function approximation and some basics of reinforcement learning are also introduced. For stochastic composite optimization, we first introduce the problem formulation, and then introduce some tools such as proximal gradient method, mirror descent, etc."
    }, {
      "heading" : "2.1 Reinforcement Learning",
      "text" : ""
    }, {
      "heading" : "2.1.1 MDP",
      "text" : "The learning environment for decision-making is generally modeled by the well-known Markov Decision Process[29] M = (S,A, P,R, γ), which is derived from a Markov chain.\nDefinition 2.1.1. (Markov Chain): A Markov Chain is a stochastic process defined as M = (S, P ). At each time step t = 1, 2, 3, · · · , the agent is in a state st ∈ S, and the state transition probability is given\n15\n16 Background\nby the state transition kernel P : S × S → R satisfying ||P ||∞ = 1, where P (st|st−1) is the state-transition probability from state st−1 at time step t− 1 to the state st at time step st.\nA Markov decision process (MDPs) is comprised of a set of states S, a set of (possibly state-dependent) actions A (As), a dynamical system model comprised of the transition probabilities P ass′ specifying the probability of transition to state s′ from state s under action a, and a reward model R.\nDefinition 2.1.2. (Markov Decision Process)[29]: A Markov Decision Process is a tuple (S,A, P,R, γ) where S is a finite set of states, A is a finite set of actions, P : S × A × S → [0, 1] is the transition kernel, where P (s, a, s′) is the probability of transmission from state s to state s′ given action a, and reward r : S × A → R+ is a reward function, 0 ≤ γ < 1 is a discount factor."
    }, {
      "heading" : "2.1.2 Basics of Reinforcement Learning",
      "text" : "A policy π : S → A is a deterministic (stochastic) mapping from states to actions.\nDefinition 2.1.3. (Policy): A deterministic stationary policy π : S → A assigns an action to each state of the Markov decision process. A stochastic policy π : S ×A→ [0, 1].\nValue functions are used to compare and evaluate the performance of policies.\nDefinition 2.1.4. (Value Function): A value function w.r.t a policy π termed as V π : S → R assigns each state the expected sum of discounted rewards\nV π = E [ t∑ i=1 γi−1ri ]\n2.1. Reinforcement Learning 17\nThe goal of reinforcement learning is to find a (near-optimal) policy that maximizes the value function. V π is a fixed-point of the Bellman equation\nV π(st) = E [r(st, π(st)) + γV π(st+1)]\nEquation (2.1.2) can be written in a concise form by introducing the Bellman operator T π w.r.t a policy π and denoting the reward vector as Rπ ∈ Rn where Rπi = E[r(si, π(si))].\nV π = T π(V π) = Rπ + γP πV π\nAny optimal policy π∗ defines the unique optimal value function V ∗\nthat satisfies the nonlinear system of equations:\nV ∗ (s) = max\na ∑ s′ P ass′ ( Rass′ + γV ∗(s′) )"
    }, {
      "heading" : "2.1.3 Value Function Approximation",
      "text" : "The most popular and widely used RL method is temporal difference (TD) learning [30]. TD learning is a stochastic approximation approach to solving Equation (2.1.2). The state-action value Q∗(s, a) represents a convenient reformulation of the value function, defined as the longterm value of performing a first, and then acting optimally according to V ∗:\nQ∗(s, a) = E ( rt+1 + γmax\na′ Q∗(st+1, a\n′)|st = s, at = a )\nwhere rt+1 is the actual reward received at the next time step, and st+1 is the state resulting from executing action a in state st. The (optimal) action value formulation is convenient because it can be approximately solved by a temporal-difference (TD) learning technique called Q-learning [31]. The simplest TD method, called TD(0), estimates the value function associated with the fixed policy using a normal stochastic gradient iteration, where δt is called temporal difference error:\nVt+1(st) = Vt(st) + αtδt δt = rt + γVt(st+1)− Vt(st)\n18 Background\nTD(0) converges to the optimal value function V π for policy π as long as the samples are “on-policy”, namely following the stochastic Markov chain associated with the policy; and the learning rate αt is decayed according to the Robbins-Monro conditions in stochastic ap-\nproximation theory: ∑ t αt = ∞, ∑ t α 2 t < ∞ [32]. When the set of states S is large, it is often necessary to approximate the value function V using a set of handcrafted basis functions (e.g., polynomials, radial basis functions, wavelets etc.) or automatically generated basis functions [33]. In linear value function approximation, the value function is assumed to lie in the linear spanning space of the basis function matrix Φ of dimension |S|×d, where it is assumed that d |S|. Hence,\nV π ≈ Vθ = Φθ\nThe equivalent TD(0) algorithm for linear function approximated value functions is given as:\nθt+1 = θt + αtδtφ(st) δt = rt + γφ(st+1) T θt − φ(st)T θt"
    }, {
      "heading" : "2.2 Stochastic Composite Optimization",
      "text" : ""
    }, {
      "heading" : "2.2.1 Stochastic Composite Optimization Formulation",
      "text" : "Stochastic optimization explores the use of first-order gradient methods for solving convex optimization problems. We first give some definitions before moving on to introduce stochastic composite optimization.\nDefinition 2.2.1. (Lipschitz-continuous Gradient): The gradient of a closed convex function f(x) is L-Lipschitz continuous if ∃L, ||∇f(x)− ∇f(y)|| ≤ L||x− y||, ∀x, y ∈ X.\nDefinition 2.2.2. (Strong Convexity): A convex function is µ−strongly convex if ∃µ, µ2 ||x − y||2 ≤ f(y) − f(x) − 〈∇f(x), y − x〉 , ∀x, y ∈ X.\n2.2. Stochastic Composite Optimization 19\nRemark: If f(x) is both with L-Lipschitz continuous gradient and µ-strongly convex, then we have ∀x, y ∈ X,\nµ 2 ||x− y||2 ≤ f(y)− f(x)− 〈∇f(x), y − x〉 ≤ L 2 ||x− y||2\nDefinition 2.2.3. (Stochastic Subgradient) : The stochastic subgradient for closed convex function f(x) at x is defined as g(x, ξt) satisfying E[g(x, ξt)] = ∇f(x) ∈ ∂f(x). Further, we assume that the variance is bounded ∃σ > 0 such that\n∀x ∈ X,E[||g(x, ξt)−∇f(x)||2∗] ≤ σ2\nHere we define the problem of Stochastic Composite Optimization (SCO)[34]:\nDefinition 2.2.4. (Stochastic Composite Optimization): A stochastic composite optimization problem F(L,M, µ, σ) : Ψ(x) on a closed convex set X is defined as\nmin x∈X\nΨ(x) def = f(x) + h(x)\nf(x) is a convex function with L-Lipschitz continuous gradient and h(x) is a convex Lipschitz continuous function such that\n|h(x)− h(y)| ≤M ||x− y||,∀x, y ∈ X\ng(x, ξt) is the stochastic subgradient of Ψ(x) defined above with variance bound σ. Such Ψ(x) is termed as a F(L,M, µ, σ) problem."
    }, {
      "heading" : "2.2.2 Proximal Gradient Method and Mirror Descent",
      "text" : "Before we move on to introduce mirror descent, we first introduce some definitions and notations.\n20 Background\nDefinition 2.2.5. (Distance-generating Function)[35]: A distancegenerating function ψ(x) is defined as a continuously differentiable µstrongly convex function. ψ∗ is the Legendre transform of ψ, which is defined as ψ∗(y) = sup x∈X (〈x, y〉 − ψ(x)).\nDefinition 2.2.6. (Bregman Divergence)[35]: Given distancegenerating function ψ, the Bregman divergence induced by ψ is defined as:\nDψ(x, y) = ψ(x)− ψ(y)− 〈∇ψ(y), x− y〉\nLegendre transform and Bregman divergence have the following\nproperties\n• ∇ψ∗ = (∇ψ)−1 • Dψ(u, v) = Dψ∗(∇ψ(u),∇ψ(v)) • ∇Dψ(u, v) = ∇ψ(u)−∇ψ(v)\nAn interesting choice of the link function ψ(·) is the (q − 1)- strongly convex function ψ(θ) = 12‖θ‖2q , and ψ∗(θ̃) = 12 ||θ̃||2p. Here,\n‖θ‖q = (∑ j |θj |q ) 1 q , and p and q are conjugate numbers such that 1 p + 1 q = 1 [36]. θ and θ̃ are conjugate variables in primal space and dual space, respectively .\n∇ψ θ→θ̃ (θ)j = sign(θj)|θj |q−1 ||θ||q−2q\n∇ψ θ̃→θ ∗(θ̃)j = sign(θ̃j)|θ̃j |p−1 ||θ̃||p−2p\nAlso it is worth noting that when p = q = 2, the Legendre transform is the identity mapping.\nWe now introduce the concept of proximal mapping, and then describe the mirror descent framework. The proximal mapping associated with\n2.2. Stochastic Composite Optimization 21\na convex function h(x) is defined as:\nproxh(x) = arg min u∈X\n(h(u) + 1\n2 ‖u− x‖2)\nIn the case of h(x) = ρ‖x‖1(ρ > 0), which is particularly important for sparse feature selection, the proximal operator turns out to be the softthresholding operator Sρ(·), which is an entry-wise shrinkage operator that moves a point towards zero, i.e.,\nproxh(x)i = Sρ(x)i = sign(xi) max(|xi − ρ|, 0)\nwhere i is the index, and ρ is a threshold. With this background, we now introduce the proximal gradient method. At each iteration, the optimization sub-problem of Equation (2.2.4) can be rewritten as\nxt+1 = arg min u∈X\n(h(u) + 〈∇ft, u〉+ 1\n2αt ‖u− xt‖2)\nIf computing proxh is not expensive, then computation of Equation (2.2.4) is of the following formulation, which is called the proximal gradient method\nxt+1 = proxαth (xt − αt∇f(xt))\nwhere αt > 0 is stepsize, constant or determined by line search. The mirror descent [35] algorithm is a generalization of classic gradient descent, which has led to developments of new more powerful machine learning methods for classification and regression. Mirror descent can be viewed as an enhanced gradient method, particularly suited to minimization of convex functions in high-dimensional spaces. Unlike traditional gradient methods, mirror descent undertakes gradient updates of weights in the dual space, which is linked together with the primal space using a Legendre transform. Mirror descent can be viewed as a proximal algorithm where the distance-generating function used is a Bregman divergence w.r.t the distance-generating function ψ, and thus the optimization problem is\nproxh(x) = arg min u∈X (h(u) +Dψ(u, x))\n22 Background\nThe solution to this optimization problem of Equation (2.2.2) forms the core procedure of mirror descent as a generalization of Equation (2.2.2)\nxt+1 = arg min u∈X\n(h(u) + 〈∇ft, u〉+ 1\nαt Dψ(u, xt))\nwhich is a nonlinear extension of Equation(2.2.2)\nxt+1 = ∇ψ∗ (proxαth (∇ψ(xt)− αt∇f(xt)))\nMirror descent has become the cornerstone of many online l1 regularization approaches such as in [37], [38] and [39]."
    }, {
      "heading" : "2.2.3 Dual Averaging",
      "text" : "Regularized dual averaging (RDA) [38] is a variant of Dual averaging (DA) with “simple” regularizers, such as l1 regularization. DA method is strongly related to cutting-plane methods. Cutting-plane methods formulate a polyhedral lower bound model of the objective function where each gradient from past iterations contributes a supporting hyperplane w.r.t its corresponding previous iteration, which is often expensive to compute. The DA method approximates this lower bound model with an approximate (possibly not supporting) lower bound hyperplane with the averaging of all the past gradients [40].\nWe now explain RDA from the proximal gradient perspective. Thus far, the proximal gradient methods we have described in Equation (2.2.2) adjust the weights to lie in the direction of the current gradient ∇ft. Regularized dual averaging methods (RDA) uses a (weighted) averaging of gradients, which explain their name. Compared with Equation (2.2.2), the main difference is the average (sub)gradient ∇f̄t is used, where ∇f̄t = 1t t∑ i=1 ∇fi. The equivalent space-efficient recursive representation is\n∇f̄t = t− 1 t ∇f̄t−1 + 1 t ∇ft\nThe generalized mirror-descent proximal gradient formulation of RDA iteratively solves the following optimization problem at each step:\n2.2. Stochastic Composite Optimization 23\nxt+1 = arg min x∈X\n{〈 x,∇f̄t 〉 + h(x) + 1\nαt Dψ(x) } Note that different from Equation (2.2.2), besides the averaging gradient ∇f̄t is used instead of ∇ft, a global origin-centered stabilizer Dψ(x) is used. RDA with local stabilizer can be seen in [41]. There are several advantages of RDA over other competing methods in regression and classification problems. The first is the sparsity of solution when the penalty term is h(x) = ρ||x||1. Compared with other firstorder l1 regularization algorithms of the mirror-descent type, including truncated gradient method [42] and SMIDAS [43], RDA tends to produce sparser solutions in that the RDA method is more aggressive on sparsity than many other competing approaches. Moreover, many optimization problems can be formulated as composite optimization, e.g., a smooth objective component in conjunction with a global nonsmooth regularization function. It is worth noting that problems with non-smooth regularization functions often lead to solutions that lie on a low-dimensional supporting data manifold, and regularized dual averaging is capable of identifying this manifold, and thus bringing the potential benefit of accelerating convergence rate by searching on the low-dimensional manifold after it is identified, as suggested in [44]. Moreover, the finite iteration behavior of RDA is much better than SGD in practice."
    }, {
      "heading" : "2.2.4 Extragradient",
      "text" : "The extragradient method was first proposed by Korpelevich[8] as a relaxation of ordinary gradient descent to solve variational inequality (VI) problems. Conventional ordinary gradient descent can be used to solve VI problems only if some strict restrictions such as strong monotonicity of the operator or compactness of the feasible set are satisfied. The extragradient method was proposed to solve VIs to relax the aforementioned strict restrictions. The essence of extragradient methods is that instead of moving along the steepest gradient descent direction w.r.t the initial point in each iteration, two steps, i.e., a extrapolation step and a gradient descent step, are taken. In the extrapolation step,\n24 Background\na step is made along the steepest gradient descent direction of the initial point, resulting in an intermediate point which is used to compute the gradient. Then the gradient descent step is made from the initial point in the direction of the gradient w.r.t the intermediate point. The extragradient take steps as follows\nxt+ 1 2 = ΠX (xt − αt∇f(xt)) xt+1 = ΠX ( xt − αt∇f(xt+ 1 2 ) )\nΠX(x) = argminy∈X‖x−y‖2 is the projection onto the convex setX, and αt is a stepsize. Convergence of the iterations of Equation (2.2.4) is guaranteed under the constraints 0 < αt < 1√ 2L [7], where L is the Lipschitz constant for ∇f(x)."
    }, {
      "heading" : "2.2.5 Accelerated Gradient",
      "text" : "Nesterov’s seminal work on accelerated gradient (AC) enables deterministic smooth convex optimization to reach its optimal convergence rate O( L N2 ). The AC method consists of three major steps: an interpolation step, a proximal gradient step and a weighted averaging step. During each iteration,\nyt = αtxt−1 + (1− αt)zt−1 xt = arg min\nx\n{ 〈x,∇f(yt)〉+ h(x) + 1\nβt Dψ(x, xt−1) } zt = αtxt + (1− αt)zt−1\nIt is worth noting that in the proximal gradient step, the stabilizer makes xt start from xt−1, and go along the gradient descent direction of ∇f(yt), which is quite similar to extragradient. The essence of Nesterov’s accelerated gradient method is to carefully select the prox-center for proximal gradient step, and the selection of two stepsize sequences {αt, βt} where αt is for interpolation and averaging, βt is for proximal gradient. Later work and variants of Nesterov’s method utilizing the strong convexity of the loss function with Bregman divergence are summarized in [45]. Recently, the extension of accelerated gradient method\n2.3. Subdifferentials and Monotone Operators 25\nfrom deterministic smooth convex optimization to stochastic composite optimization, termed as AC-SA, is studied in [34]."
    }, {
      "heading" : "2.3 Subdifferentials and Monotone Operators",
      "text" : "We introduce the important concept of a subdifferential.\nDefinition 2.1. The subdifferential of a convex function f is defined as the set-valued mapping ∂f :\n∂f(x) = {v ∈ Rn : f(z) ≥ f(x) + vT (z − x),∀z ∈ dom(f)\nA simple example of a subdifferential is the normal cone, which is the subdifferential of the indicator function IK of a convex set K (defined as 0 within the set and +∞ outside). More formally, the normal cone NK(x ∗) at the vector x∗ of a convex set K is defined as NK(x ∗) = {y ∈ Rn|yT (x − x∗) ≤ 0,∀x ∈ K}. Each vector v ∈ ∂f(x) is referred to as the subgradient of f at x.\nAn important property of closed proper convex functions is that their subdifferentials induce a relation on Rn called a maximal monotone operator [17, 46].\nDefinition 2.2. A relation F on Rn is monotone if\n(u− v)T (x− y) ≥ 0 for all (x, u), (y, v) ∈ F F is maximal monotone is there is no monotone operator that properly contains it.\nThe subdifferential ∂f of a convex function f is a canonical example of a maximal monotone operator. A very general way to formulate optimization problems is monotone inclusion:\nDefinition 2.3. Given a monotone operator F , the monotone inclusion problem is to find a vector x such that 0 ∈ F (x). For example, given a (subdifferentiable) convex function f , finding a vector x∗ that minimizes f is equivalent to solving the monotone inclusion problem 0 ∈ ∂f(x∗).\n26 Background"
    }, {
      "heading" : "2.4 Convex-concave Saddle-Point First Order Algorithms",
      "text" : "A key novel contribution of our paper is a convex-concave saddle-point formulation for reinforcement learning. A convex-concave saddle-point problem is formulated as follows. Let x ∈ X, y ∈ Y , where X,Y are both nonempty closed convex sets, and f(x) : X → R be a convex function. If there exists a function ϕ(·, ·) such that f(x) can be represented as f(x) := supy∈Y ϕ(x, y), then the pair (ϕ, Y ) is referred as the saddle-point representation of f . The optimization problem of minimizing f over X is converted into an equivalent convex-concave saddle-point problem SadV al = infx∈Xsupy∈Y ϕ(x, y) of ϕ on X×Y . If f is non-smooth yet convex and well structured, which is not suitable for many existing optimization approaches requiring smoothness, its saddle-point representation ϕ is often smooth and convex. The convexconcave saddle-point problems are, therefore, usually better suited for first-order methods [47]. A comprehensive overview on extending convex minimization to convex-concave saddle-point problems with unified variational inequalities is presented in [48]. As an example, consider f(x) = ||Ax− b||m which admits a bilinear minimax representation\nf(x) := ‖Ax− b‖m = max‖y‖n<1 (〈y,Ax− b〉)\nwhere m,n are conjugate numbers. Using the approach in [49], Equation (2.4) can be solved as\nxt+1 = xt − αt 〈yt, A〉 , yt+1 = Π‖yt‖n≤1(yt + αt(Axt − b))\nwhere Π‖yt‖n≤1 is the projection operator of yt onto the unit-ln ball ‖y‖n ≤ 1,which is defined as\nΠ‖y‖n≤1y = min(1, 1/‖y‖n)y, n = 2, ( Π‖y‖n≤1y ) i = min(1, 1 |yi| )yi, n =∞\nand Π‖y‖∞≤1y is an entrywise operator.\n2.5. Abstraction through Proximal Operators 27"
    }, {
      "heading" : "2.5 Abstraction through Proximal Operators",
      "text" : "A general procedure for solving the monotone inclusion problem, the proximal point algorithm [50], uses the following identities:\n0 ∈ ∂f(x)↔ 0 ∈ α∂f(x)↔ x ∈ (I + α∂(x))↔ x = (I + α∂f)−1(x)\nHere, α > 0 is any real number. The proximal point algorithm is based on the last fixed point identity, and consists of the following iteration:\nxk+1 ← (I + αk∂f)−1(xk)\nInterestingly, the proximal point method involves the computation of the so-called resolvent of a relation, defined as follows:\nDefinition 2.4. The resolvent of a relation F is given as the relation RF = (I + λF ) −1, where λ > 0.\nIn the case where the relation R = ∂f of some convex function f , the resolvent can be shown to be the proximal mapping [51], a crucially important abstraction of the concept of projection, a cornerstone of constrained optimization.\nDefinition 2.5. The proximal mapping of a vector v with respect to a convex function f is defined as the minimization problem:\nproxf (v) = argminx∈K(f(x) + ‖v − x‖22)\nIn the case where f(x) = IK(x), the indicator function for a convex set K, the proximal mapping reduces to the projection ΠK . While the proximal point algorithm is general, it is not very effective for problems in high-dimensional machine learning that involve minimizing a sum of two or more functions, one or more of which may not be differentiable. A key extension of the proximal point algorithm is through a general decomposition principle called operator splitting, reviewed below.\n28 Background"
    }, {
      "heading" : "2.6 Decomposition through Operator Splitting",
      "text" : "Operator splitting [17, 18] is a generic approach to decomposing complex optimization and variational inequality problems into simpler ones that involve computing the resolvents of individual relations, rather than sums or other compositions of relations. For example, given a monotone inclusion problem of the form:\n0 ∈ A(x) +B(x) for two relations A and B, how can we find the solution x∗ without computing the resolvent (I +λ(A+B))−1, which may be complicated, but rather only compute the resolvents of A and B individually? There are several classes of operator splitting schemes. We will primarily focus on the Douglas Rachford algorithm [18] specified in Figure 2.1, because it leads to a widely used distributed optimization method called Alternating Direction Method of Multipliers (ADMM) [19]. The Douglas Rachford method is based on the “damped iteration” given by:\nzk+1 = 1\n2 (I + CACB)(zk)\nwhere CA = 2RA + I and CB = 2RB + I are the “reflection” or Cayley operators associated with the relations A and B. Note that the Cayley operator is defined in terms of the resolvent, so this achieves the necessary decomposition. When A = ∂f and B = ∂g, two convex functions, the Douglas Rachford algorithm becomes the well-known Alternating Direction Method of Multipliers (ADMM) method, as described in Figure 2.1, where the resolvent of A and B turn into proximal minimization steps. The ADMM algorithm has been extensively studied in optimization; a detailed review is available in the tutorial paper by Boyd and colleagues [19], covering both its theoretical properties, operator splitting origins, and applications to high-dimensional data mining. ADMMs have also recently been studied for spectroscopic data, in particular hyperspectral unmixing [52]."
    }, {
      "heading" : "2.6.1 Forward Backwards Splitting",
      "text" : "In this section we will give a brief overview of proximal splitting algorithms [28]. The two key ingredients of proximal splitting are proximal\n2.6. Decomposition through Operator Splitting 29\noperators and operator splitting. Proximal methods [53, 54], which are widely used in machine learning, signal processing, and stochastic optimization, provide a general framework for large-scale optimization. The proximal mapping associated with a convex function h is defined as:\nproxh(x) = arg minu (h(u) +\n1 2 ‖u− x‖2)\nOperator splitting is widely used to reduce the computational complexity of many optimization problems, resulting in algorithms such as sequential non-iterative approach (SNIA), Strang splitting, and sequential iterative approach (SIA). Proximal splitting is a technique that combines proximal operators and operator splitting, and deals with problems where the proximal operator is difficult to compute at first, yet is easier to compute after decomposition. The very basic scenario\n30 Background\nis Forward-Backward Splitting (FOBOS) [55]\nmin θ (Ψ(θ) = f(θ) + h(θ))\nwhere f(x) is a convex, continuously differentiable function with LLipschitz-continuous bounded gradients, i.e. ∀x, y, ||∇f(x)−∇f(y)|| ≤ L||x−y||, and h(θ) is a convex (possibly not smooth) function. FOBOS solves this problem via the following proximal gradient method\nθt+1 = proxαth(θt − αt∇f(θt))\nAn extension of FOBOS is when the objective function is separable, i.e.,\nmin θ m∑ i=1 fi(θ)\nwhere computing prox m∑ i=1 fi (·) is difficult, yet for each i, proxfi(·) is easy to compute. To solve this problem, Douglas-Rachford splitting [28] and Alternating Direction of Multiple Multipliers (ADMM) can be used. Recently, ADMM has been used proposed for sparse RL [56]."
    }, {
      "heading" : "2.6.2 Nonlinear Primal Problem Formulation",
      "text" : "In this paper we will investigate a scenario of proximal splitting that is different from the problem formulation in Section (2.6.1), namely the nonlinear primal form\nmin θ (Ψ(θ) = F (K(θ)) + h(θ))\nwhere F (·) is a lower-semicontinuous (l.s.c) nonlinear convex function, K is a linear operator, the induced norm is ||K||. In the following, we will denote F (K(θ)) as F ◦K(θ). The proximal operator of this problem is\nθt+1 = arg min θ {Ψ(θ) + 1 2αt ||θ − θt||22} = proxαt (F◦K+h)(θt)\n2.7. Natural Gradient Methods 31\nIn many cases, although proxαtF and proxαtK are easy to compute, proxαtF◦K is often difficult to compute. For the NEU case, we have\nK(θ) = E[φtδt] = ΦTΞ(TVθ − Vθ) = ΦTΞ(R+ γΦ ′ θ − Φθ), F (·) = 12 || · ||22\nIt is straightforward to verify that proxαtF , proxαtK are easy to compute, but proxαtF◦K is not easy to compute since it involves the biased sampling problem as indicated in Equation (6.3). To solve this problem, we transform the problems formulation to facilitate operator splitting, i.e., which only uses proxαtF , proxαtK ,proxαth and avoids computing proxαtF◦K directly. We will use the primal-dual splitting framework to this end."
    }, {
      "heading" : "2.6.3 Primal-Dual Splitting",
      "text" : "The corresponding primal-dual formulation [57, 28, 58] of Section (2.6.2) is\nmin θ∈X max y∈Y\n(L(θ, y) = 〈K(θ), y〉 − F ∗(y) + h(θ))\nwhere F ∗(·) is the Legendre transform of the convex nonlinear function F (·), which is defined as F ∗(y) = supx∈X(〈x, y〉−F (x)). The proximal splitting update per iteration is written as\nyt+1 = arg min y∈Y 〈−Kt(θt), y〉+ F ∗(y) + 12αt ||y − yt|| 2 θt+1 = arg min θ∈X 〈Kt(θ), yt〉+ h(θ) + 12αt ||θ − θt|| 2\nThus we have the general update rule as\nyt+1 = yt + αtKt(θt)− αt∇F ∗t (y) , θt+1 = proxαth(θt − αt∇Kt(θt)yt)\nHowever, in stochastic learning setting, we do not have knowledge of the exact Kt(θt), ∇F ∗t (y) and ∇Kt(θt)yt, whereas a stochastic oracle SO is able to provide unbiased estimation of them."
    }, {
      "heading" : "2.7 Natural Gradient Methods",
      "text" : "Consider the problem of minimizing a differentiable function f : Rn → R. The standard gradient descent approach is to select an initial x0 ∈\n32 Background\nRn, compute the direction of steepest descent, −∇f(x0), and then move some amount in that direction (scaled by a stepsize parameter, α0). This process is then repeated indefinitely: xk+1 = xk−αk∇f(xk), where {αk} is a stepsize schedule and k ∈ {1, . . .}. Gradient descent has been criticized for its low asymptotic rate of convergence. Natural gradients are a quasi-Newton approach to improving the convergence rate of gradient descent.\nWhen computing the direction of steepest descent, gradient descent assumes that the vector xk resides in Euclidean space. However, in several settings it is more appropriate to assume that xk resides in a Riemannian space with metric tensor G(xk), which is an n × n positive definite matrix that may vary with xk [22]. In this case, the direction of steepest descent is called the natural gradient and is given by −G(xk)−1∇f(xk) [59]. In certain cases, (which include our policy search application), following the natural gradient is asymptotically Fisher-efficient [22]."
    }, {
      "heading" : "2.8 Summary",
      "text" : "We provided a brief overview of some background material in reinforcement learning and optimization in this chapter. The subsequent chapters contain further elaboration of this material as it is required. The overall goal of our work is to bring reinforcement learning into the main fabric of modern stochastic optimization theory. As we show in subsequent chapters, accomplishing this goal gives us access to many advanced algorithms and analytical tools. It is worth noting that we make little use of classical stochastic approximation theory, which has traditionally been used to analyze reinforcement learning methods (as discussed in detail in books such as [32]). Classical stochastic approximation theory provides only asymptotic convergence bounds, for the most part. We are interested, however, in getting tighter sample complexity bounds, which stochastic optimization provides.\n3 Sparse Temporal Difference Learning in Primal Dual Spaces\nIn this chapter we explore a new framework for (on-policy convergent) TD learning algorithm based on mirror descent and related algorithms.1 Mirror descent can be viewed as an enhanced gradient method, particularly suited to minimization of convex functions in high-dimensional spaces. Unlike traditional gradient methods, mirror descent undertakes gradient updates of weights in both the dual space and primal space, which are linked together using a Legendre transform. Mirror descent can be viewed as a proximal algorithm where the distance-generating function used is a Bregman divergence. A new class of proximal-gradient based temporal-difference (TD) methods are presented based on different Bregman divergences, which are more powerful than regular TD learning. Examples of Bregman divergences that are studied include p-norm functions, and Mahalanobis distance based on the covariance of sample gradients. A new family of sparse mirrordescent reinforcement learning methods are proposed, which are able to find sparse fixed-point of an l1-regularized Bellman equation at significantly less computational cost than previous methods based on second-\n1 This chapter is based on the paper “Sparse Q-learning with Mirror Descent” published in UAI 2012.\n33\n34 Sparse Temporal Difference Learning in Primal Dual Spaces\norder matrix methods."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "The problem formulation in this chapter is based on the Lasso-TD objective defined as follows, which is used in LARS-TD and LCP-TD. We first define l1-regularized Projection, and then give the definition of Lasso-TD objective function.\nDefinition 3.1.1. [60] (l1-regularized Projection): Πl1 is the l1regularized projection defined as:\nΠl1y = Φθ, θ = arg minw ‖y − Φw‖2 + ρ‖w‖1\nwhich is a non-expansive mapping w.r.t weighted l2 norm, as proven in [60].\nLemma 3.1.1. [60]: Πρ is a non-expansive mapping such that\n∀x, y ∈ Rd, ||Πρx−Πρy||2 ≤ ||x− y||2 − ||x− y − (Πρx−Πρy)||2\nDefinition 3.1.2. [60] (Lasso-TD) Lasso-TD is a fixed-point equation w.r.t l1 regularization with parameter ρ, which is defined as\nθ = f(θ) = argminu∈Rd ( ||TΦθ − Φu||2 + ρ||u||1 ) = argminu∈Rd ( ||Rπ + γP πΦθ − Φu||2 + ρ||u||1\n) The properties of Lasso-TD is discussed in detail in [60]. Note that the above l1 regularized fixed-point is not a convex optimization problem but a fixed-point problem. Several prevailing sparse RL methods use Lasso-TD as the objective function, such as SparseTD[61], LARS-TD[62] and LCP-TD[63]. The advantage of LARS-TD comes from LARS in that it computes a homotopy path of solutions with different regularization parameters, and thus offers a rich solution family.\n3.2. Mirror Descent RL 35\nThe major drawback comes from LARS, too. To maintain the LARS criteria wherein each active variable has the same correlation with the residual, variables may be added and dropped several times, which is computationally expensive. In fact, the computational complexity per iteration is O(Ndk2) where k is the cardinality of the active feature set. Secondly, LARS-TD requires the A matrix to be a P -matrix(a square matrix which does not necessarily to be symmetric, but all the principal minors are positive), which poses extra limitation on applications. The author of LARS-TD claims that this seems never to be a problem in practice, and given on-policy sampling condition or given large enough ridge regression term, P -matrix condition can be guaranteed. LCPTD [12] formulates LASSO-TD as a linear complementarity problem (LCP), which can be solved by a variety of available LCP solvers.\nWe then derive the major step by formulating the problem as a\nforward-backward splitting problem (FOBOS) as in [55],\nθt+ 1 2 = θt − αtgt θt+1 = arg min\nθ\n{ 1 2 ||θ − θt+ 12 || 2 2 + αth(θ) } This is equivalent to the formulation of proximal gradient method\nθt+1 = arg min θ\n{ 〈gt, θ〉+ h(θ) + 1\n2αt ||θ − θt||22 } Likewise, we could formulate the sparse TD algorithm as\nθt+ 1 2 = θt − αt2 ∇MSE(θ) θt+1 = arg min\nθ\n{ 1 2 ||θt − θt+ 12 || 2 2 + αth(θ) } And this can be formulated as\nθt+1 = arg min θ\n{〈 1\n2 ∇MSE(θ), θ\n〉 + h(θ) + 1\n2αt ||θ − θt||22 }"
    }, {
      "heading" : "3.2 Mirror Descent RL",
      "text" : "Algorithm 1 describes the proposed mirror-descent TD(λ) method.2 Unlike regular TD, the weights are updated using the TD error in the\n2 All the algorithms described extend to the action-value case where φ(s) is replaced by φ(s, a).\n36 Sparse Temporal Difference Learning in Primal Dual Spaces\nAlgorithm 5 Adaptive Mirror Descent TD(λ) Let π be some fixed policy for an MDP M, and s0 be the initial state. Let Φ be some fixed or automatically generated basis.\n1: repeat 2: Do action π(st) and observe next state st+1 and reward rt. 3: Update the eligibility trace et ← et + λγφ(st) 4: Update the dual weights θt for a linear function approximator:\nθt+1 = ∇ψt(wt) + αt(rt + γφ(st+1)Twt − φ(st)Twt)et\nwhere ψ is a distance generating function.\n5: Set wt+1 = ∇ψ∗t (θt+1) where ψ∗ is the Legendre transform of ψ. 6: Set t← t+ 1. 7: until done.\nReturn V̂ π ≈ Φwt as the value function associated with policy π for MDP M .\ndual space by mapping the primal weights w using a gradient of a strongly convex function ψ. Subsequently, the updated dual weights are converted back into the primal space using the gradient of the Legendre transform of ψ, namely ∇ψ∗. Algorithm 1 specifies the mirror descent TD(λ) algorithm wherein each weight wi is associated with an eligibility trace e(i). For λ = 0, this is just the features of the current state φ(st), but for nonzero λ, this corresponds to a decayed set of features proportional to the recency of state visitations. Note that the distance generating function ψt is a function of time."
    }, {
      "heading" : "3.2.1 Choice of Bregman Divergence",
      "text" : "We now discuss various choices for the distance generating function in Algorithm 1. In the simplest case, suppose ψ(w) = 12‖w‖22, the Euclidean length of w. In this case, it is easy to see that mirror descent TD(λ) corresponds to regular TD(λ), since the gradients ∇ψ and ∇ψ∗ correspond to the identity function. A much more interesting choice of ψ is ψ(w) = 12‖w‖2q , and its conjugate Legendre transform\n3.2. Mirror Descent RL 37\nψ∗(w) = 12‖w‖2p. Here, ‖w‖q = (∑ j |wj |q ) 1 q , and p and q are conjugate numbers such that 1p + 1 q = 1. This ψ(w) leads to the p-norm link function θ = f(w) where f : Rd → Rd [16]:\nfj(w) = sign(wj)|wj |q−1 ‖w‖q−2q , f−1j (θ) = sign(θj)|θj |p−1 ‖θ‖p−2p\nThe p-norm function has been extensively studied in the literature on online learning [16], and it is well-known that for large p, the corresponding classification or regression method behaves like a multiplicative method (e.g., the p-norm regression method for large p behaves like an exponentiated gradient method (EG) [64, 65]).\nAnother distance generating function is the negative entropy function ψ(w) = ∑\niwi logwi, which leads to the entropic mirror descent\nalgorithm [6]. Interestingly, this special case has been previously explored [66] as the exponentiated-gradient TD method, although the connection to mirror descent and Bregman divergences were not made in this previous study, and EG does not generate sparse solutions [37]. We discuss EG methods vs. p-norm methods in Section 3.6."
    }, {
      "heading" : "3.2.2 Sparse Learning with Mirror Descent TD",
      "text" : "Algorithm 2 describes a modification to obtain sparse value functions resulting in a sparse mirror-descent TD(λ) algorithm. The main difference is that the dual weights θ are truncated according to Equation 1.1.3 to satisfy the l1 penalty on the weights. Here, β is a sparsity parameter. An analogous approach was suggested in [37] for l1 penalized classification and regression."
    }, {
      "heading" : "3.2.3 Composite Mirror Descent TD",
      "text" : "Another possible mirror-descent TD algorithm uses as the distancegenerating function a Mahalanobis distance derived from the subgradients generated during actual trials. We base our derivation on the composite mirror-descent approach proposed in [67] for classification and regression. The composite mirror-descent solves the following op-\n38 Sparse Temporal Difference Learning in Primal Dual Spaces\nAlgorithm 6 Sparse Mirror Descent TD(λ)\n1: repeat 2: Do action π(st) and observe next state st+1 and reward rt. 3: Update the eligibility trace et ← et + λγφ(st) 4: Update the dual weights θt:\nθ̃t+1 = ∇ψt(wt) + αt ( rt + γφ(st+1) Twt − φ(st)Twt ) et\n(e.g., ψ(w) = 12‖w‖2q is the p-norm link function). 5: Truncate weights:\n∀j, θt+1j = sign(θ̃t+1j ) max(0, |θ̃t+1j | − αtβ)\n6: wt+1 = ∇ψ∗t (θt+1) (e.g., ψ∗(θ) = 12‖θ‖2p and p and q are dual norms such that 1p + 1 q = 1). 7: Set t← t+ 1. 8: until done.\nReturn V̂ π ≈ Φwt as the l1 penalized sparse value function associated with policy π for MDP M .\ntimization problem at each step:\nwt+1 = argminx∈X (αt〈x, ∂ft〉+ αtµ(x) +Dψt(x,wt))\nHere, µ serves as a fixed regularization function, such as the l1 penalty, and ψt is the time-dependent distance generating function as in mirror descent. We now describe a different Bregman divergence to be used as the distance generating function in this method. Given a positive definite matrix A, the Mahalanobis norm of a vector x is defined as\n‖x‖A = √ 〈x,Ax〉. Let gt = ∂f(st) be the subgradient of the function\nbeing minimized at time t, and Gt = ∑ t gtg T t be the covariance matrix of outer products of the subgradients. It is computationally more\nefficient to use the diagonal matrix Ht = √ diag(Gt) instead of the full covariance matrix, which can be expensive to estimate. Algorithm 3 describes the adaptive subgradient mirror descent TD method.\n3.3. Convergence Analysis 39\nAlgorithm 7 Composite Mirror Descent TD(λ)\n1: repeat 2: Do action π(st) and observe next state st+1 and reward rt. 3: Set TD error δt = rt + γφ(st+1) Twt − φ(st)Twt 4: Update the eligibility trace et ← et + λγφ(st) 5: Compute TD update ξt = δtet. 6: Update feature covariance\nGt = Gt−1 + φ(st)φ(st) T\n7: Compute Mahalanobis matrix Ht = √ diag(Gt). 8: Update the weights w:\nwt+1,i = sign(wt,i − αtξt,i Ht,ii )(|wt,i − αtξt,i Ht,ii | − αtβ Ht,ii )\n9: Set t← t+ 1. 10: until done.\nReturn V̂ π ≈ Φwt as the l1 penalized sparse value function associated with policy π for MDP M ."
    }, {
      "heading" : "3.3 Convergence Analysis",
      "text" : "Definition 2 [60]: Πl1 is the l1-regularized projection defined as: Πl1y = Φα such that α = arg minw‖y − Φw‖2 +β‖w‖1, which is a nonexpansive mapping w.r.t weighted l2 norm induced by the on-policy sample distribution setting, as proven in [60]. Let the approximation error f(y, β) = ‖y −Πl1y‖2. Definition 3 (Empirical l1-regularized projection): Π̂l1 is the empirical l1-regularized projection with a specific l1 regularization solver, and satisfies the non-expansive mapping property. It can be shown using a direct derivation that Π̂l1ΠT is a γ-contraction mapping. Any unbiased l1 solver which generates intermediate sparse solution before convergence, e.g., SMIDAS solver after t-th iteration, comprises an empirical l1-regularized projection.\n40 Sparse Temporal Difference Learning in Primal Dual Spaces\nTheorem 1 The approximation error ||V − V̂ || of Algorithm 2 is bounded by (ignoring dependence on π for simplicity):\n||V − V̂ || ≤ 11−γ×( ‖V −ΠV ‖+ f(ΠV, β) + (M − 1)P (0) + ‖w∗‖21 MαtN ) where V̂ is the approximated value function after N -th iteration, i.e., V̂ = ΦwN , M = 2\n2−4αt(p−1)e , αt is the stepsize, P (0) =\n1 N N∑ i=1 ‖ΠV (si)‖22, si is the state of i-th sample, e = d p 2 , d is the number of features, and finally, w∗ is l1-regularized projection of ΠV such that Φw∗ = Πl1ΠV . Proof: In the on-policy setting, the solution given by Algorithm 2 is the fixed point of V̂ = Π̂l1ΠT V̂ and the error decomposition is illustrated in Figure 3.1. The error can be bounded by the triangle inequality\n||V − V̂ || = ||V −ΠTV ||+ ||ΠTV − Π̂l1ΠTV ||+ ||Π̂l1ΠTV − V̂ ||\nSince Π̂l1ΠT is a γ-contraction mapping, and V̂ = Π̂l1ΠT V̂ , we have\n||Π̂l1ΠTV − V̂ || = ||Π̂l1ΠTV − Π̂l1ΠT V̂ || ≤ γ||V − V̂ ||\nSo we have\n(1− γ)||V − V̂ || ≤ ||V −ΠTV ||+ ||ΠTV − Π̂l1ΠTV ||\n‖V −ΠTV ‖ depends on the expressiveness of the basis Φ, where if V lies in span(Φ), this error term is zero. ||ΠTV − Πl1Π̂TV || is further\n3.4. Experimental Results: Discrete MDPs 41\nbounded by the triangle inequality\n||ΠTV − Π̂l1ΠTV || ≤ ||ΠTV −Πl1ΠTV ||+ ||Πl1ΠTV − Π̂l1ΠTV ||\nwhere ‖ΠTV −Πl1ΠTV ‖ is controlled by the sparsity parameter β, i.e., f(ΠTV, β) = ||ΠTV −Πl1ΠTV ||, where ε = ||Π̂l1ΠTV −Πl1ΠTV || is the approximation error depending on the quality of the l1 solver employed. In Algorithm 2, the l1 solver is related to the SMIDAS l1 regularized mirror-descent method for regression and classification [37]. Note that for a squared loss function L(〈w, xi〉 , yi) = || 〈w, xi〉 − yi||22, we have |L′|2 ≤ 4L. Employing the result of Theorem 3 in [37], after the N -th iteration, the l1 approximation error is bounded by\nε ≤ (M − 1)P (0) + ||w∗||21 M\nαtN ,M =\n2\n2− 4αt(p− 1)e By rearranging the terms and applying V = TV , Equation (3.3) can be deduced."
    }, {
      "heading" : "3.4 Experimental Results: Discrete MDPs",
      "text" : "Figure 3.2 shows that mirror-descent TD converges more quickly with far smaller Bellman errors than LARS-TD [68] on a discrete “tworoom” MDP [69]. The basis matrix Φ was automatically generated as 50 proto-value functions by diagonalizing the graph Laplacian of the discrete state space connectivity graph[69]. The figure also shows that Algorithm 2 (sparse mirror-descent TD) scales more gracefully than LARS-TD. Note LARS-TD is unstable for γ = 0.9. It should be noted that the computation cost of LARS-TD is O(Ndm3), whereas that for Algorithm 2 is O(Nd), where N is the number of samples, d is the number of basis functions, and m is the number of active basis functions. If p is linear or sublinear w.r.t d, Algorithm 2 has a significant advantage over LARS-TD.\nFigure 3.3 shows the result of another experiment conducted to test the noise immunity of Algorithm 2 using a discrete 10× 10 grid world domain with the goal set at the upper left hand corner. For this problem, 50 proto-value basis functions were automatically generated, and 450 random Gaussian mean 0 noise features were added. The sparse\n42 Sparse Temporal Difference Learning in Primal Dual Spaces\nmirror descent TD algorithm was able to generate a very good approximation to the optimal value function despite the large number of irrelevant noisy features, and took a fraction of the time required by LARS-TD.\nFigure 3.4 compares the performance of mirror-descent Q-learning with a fixed p-norm link function vs. a decaying p-norm link function for a 10×10 discrete grid world domain with the goal state in the upper left-hand corner. Initially, p = O(log d) where d is the number of features, and subsequently p is decayed to a minimum of p = 2. Varying p-norm interpolates between additive and multiplicative updates. Dif-\n3.4. Experimental Results: Discrete MDPs 43\nferent values of p yield an interpolation between the truncated gradient method [42] and SMIDAS [43].\nFigure 3.5 illustrates the performance of Algorithm 3 on the two-\nroom discrete grid world navigation task.\n44 Sparse Temporal Difference Learning in Primal Dual Spaces"
    }, {
      "heading" : "3.5 Experimental Results: Continuous MDPs",
      "text" : "Figure 3.6 compares the performance of Q-learning vs. mirror-descent Q-learning for the mountain car task, which converges more quickly to a better solution with much lower variance. Figure 3.7 shows that mirror-descent Q-learning with learned diffusion wavelet bases converges quickly on the 4-dimensional Acrobot task. We found in our experiments that LARS-TD did not converge within 20 episodes (its curve, not shown in Figure 3.6, would be flat on the vertical axis at 1000 steps). Finally, we tested the mirror-descent approach on a more\ncomplex 8-dimensional continuous MDP. The triple-link inverted pendulum [71] is a highly nonlinear time-variant under-actuated system,\n3.5. Experimental Results: Continuous MDPs 45\nwhich is a standard benchmark testbed in the control community. We base our simulation using the system parameters described in [71], except that the action space is discretized because the algorithms described here are restricted to policies with discrete actions. There are three actions, namely {0, 5Newton,−5Newton}. The state space is 8- dimensional, consisting of the angles made to the horizontal of the three links in the arm as well as their angular velocities, the position and velocity of the cart used to balance the pendulum. The goal is to learn a policy that can balance the system with the minimum number of episodes. A run is successful if it balances the inverted pendulum for the specified number of steps within 300 episodes, resulting in a reward of 0. Otherwise, this run is considered as a failure and yields a negative reward −1. The first action is chosen randomly to push the pendulum away from initial state. Two experiments were conducted on the triple-link pendulum domain with 20 runs for each experiment. As\nTable 1 shows, Mirror Descent Q-learning is able to learn the policy with fewer episodes and usually with reduced variance compared with regular Q-learning.\nThe experiment settings are Experiment 1: Zero initial state and the system receives a reward 1 if it is able to balance 10,000 steps. Experiment 2: Zero initial state and the system receives a reward 1 if it is able to balance 100,000 steps. Table 1 shows the comparison result between regular Q-learning and Mirror Descent Q-learning.\n46 Sparse Temporal Difference Learning in Primal Dual Spaces"
    }, {
      "heading" : "3.6 Comparison of Link Functions",
      "text" : "The two most widely used link functions in mirror descent are the p-norm link function [6] and the relative entropy function for exponentiated gradient (EG) [64]. Both of these link functions offer a multiplicative update rule compared with regular additive gradient methods. The differences between these two are discussed here. Firstly, the loss function for EG is the relative entropy whereas that of the p-norm link function is the square l2-norm function. Second and more importantly, EG does not produce sparse solutions since it must maintain the weights away from zero, or else its potential (the relative entropy) becomes unbounded at the boundary.\nAnother advantage of p-norm link functions over EG is that the p-norm link function offers a flexible interpolation between additive and multiplicative gradient updates. It has been shown that when the features are dense and the optimal coefficients θ∗ are sparse, EG converges faster than the regular additive gradient methods [64]. However, according to our experience, a significant drawback of EG is the overflow of the coefficients due to the exponential operator. To prevent overflow, the most commonly used technique is rescaling: the weights are re-normalized to sum to a constant. However, it seems that this approach does not always work. It has been pointed out [66] that in the EG-Sarsa algorithm, rescaling can fail, and replacing eligible traces instead of regular additive eligible traces is used to prevent overflow. EG-Sarsa usually poses restrictions on the basis as well. Thanks to the flexible interpolation capability between multiplicative and additive gradient updates, the p-norm link function is more robust and applicable to various basis functions, such as polynomial, radial basis function (RBF), Fourier basis [70], proto-value functions (PVFs), etc.\n3.7. Summary 47"
    }, {
      "heading" : "3.7 Summary",
      "text" : "We proposed a novel framework for reinforcement learning using mirror-descent online convex optimization. Mirror Descent Q-learning demonstrates the following advantage over regular Q learning: faster convergence rate and reduced variance due to larger stepsizes with theoretical convergence guarantees [72]. Compared with existing sparse reinforcement learning algorithms such as LARS-TD, Algorithm 2 has lower sample complexity and lower computation cost, advantages accrued from the first-order mirror descent framework combined with proximal mapping [37]. There are many promising future research topics along this direction. We are currently exploring a mirror-descent fast-gradient RL method, which is both convergent off-policy and quicker than fast gradient TD methods such as GTD and TDC [2]. To scale to large MDPs, we are investigating hierarchical mirror-descent RL methods, in particular extending SMDP Q-learning. We are also undertaking a more detailed theoretical analysis of the mirror-descent RL framework, building on existing analysis of mirror-descent methods [67, 37]. Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].\n4 Regularized Off-Policy Temporal Difference Learning\nIn the last chapter we proposed an on-policy convergent sparse TD learning algorithm. Although TD converges when samples are drawn “on-policy” by sampling from the Markov chain underlying a policy in a Markov decision process (MDP), it can be shown to be divergent when samples are drawn “off-policy”.\nIn this chapter, the off-policy TD learning problem is formulated from the stochastic optimization perspective. 1 A novel objective function is proposed based on the linear equation formulation of the TDC algorithm. The optimization problem underlying off-policy TD methods, such as TDC, is reformulated as a convex-concave saddle-point stochastic approximation problem, which is both convex and incrementally solvable. A detailed theoretical and experimental study of the RO-TD algorithm is presented.\n1 This chapter is based on the paper ”Regularized Off-Policy TD-Learning” published in NIPS 2012.\n48\n4.1. Introduction 49"
    }, {
      "heading" : "4.1 Introduction",
      "text" : ""
    }, {
      "heading" : "4.1.1 Off-Policy Reinforcement Learning",
      "text" : "Off-policy learning refers to learning about one way of behaving, called the target policy, from sample sets that are generated by another policy of choosing actions, which is called the behavior policy, or exploratory policy. As pointed out in [75], the target policy is often a deterministic policy that approximates the optimal policy, and the behavior policy is often stochastic, exploring all possible actions in each state as part of finding the optimal policy. Learning the target policy from the samples generated by the behavior policy allows a greater variety of exploration strategies to be used. It also enables learning from training data generated by unrelated controllers, including manual human control, and from previously collected data. Another reason for interest in off-policy learning is that it enables learning about multiple target policies (e.g., optimal policies for multiple sub-goals) from a single exploratory policy generated by a single behavior policy, which triggered an interesting research area termed as “parallel reinforcement learning”. Besides, offpolicy methods are of wider applications since they are able to learn while executing an exploratory policy, learn from demonstrations, and learn multiple tasks in parallel [76]. Sutton et al. [26] introduced convergent off-policy temporal difference learning algorithms, such as TDC, whose computation time scales linearly with the number of samples and the number of features. Recently, a linear off-policy actor-critic algorithm based on the same framework was proposed in [76]."
    }, {
      "heading" : "4.1.2 Convex-concave Saddle-point First-order Algorithms",
      "text" : "The key novel contribution of this chapter is a convex-concave saddlepoint formulation for regularized off-policy TD learning. A convexconcave saddle-point problem is formulated as follows. Let x ∈ X, y ∈ Y , where X,Y are both nonempty bounded closed convex sets, and f(x) : X → R be a convex function. If there exists a function ϕ(·, ·) such that f(x) can be represented as f(x) := supy∈Y ϕ(x, y), then the pair (ϕ, Y ) is referred as the saddle-point representation of f . The optimization problem of minimizing f over X is converted into an equivalent\n50 Regularized Off-Policy Temporal Difference Learning\nconvex-concave saddle-point problem SadV al = infx∈Xsupy∈Y ϕ(x, y) of ϕ on X × Y . If f is non-smooth yet convex and well structured, which is not suitable for many existing optimization approaches requiring smoothness, its saddle-point representation ϕ is often smooth and convex. Thus, convex-concave saddle-point problems are, therefore, usually better suited for first-order methods [47]. A comprehensive overview on extending convex minimization to convex-concave saddlepoint problems with unified variational inequalities is presented in [48]. As an example, consider f(x) = ||Ax − b||m which admits a bilinear minimax representation\nf(x) := ‖Ax− b‖m = max‖y‖n≤1 yT (Ax− b)\nwhere m,n are conjugate numbers. Using the approach in [49], Equation (4.1.2) can be solved as\nxt+1 = xt − αtAT yt, yt+1 = Πn(yt + αt(Axt − b))\nwhere Πn is the projection operator of y onto the unit ln-ball ‖y‖n ≤ 1,which is defined as\nΠn(y) = min(1, 1/‖y‖n)y, n = 2, 3, · · · ,Π∞(yi) = min(1, 1/|yi|)yi\nand Π∞ is an entrywise operator."
    }, {
      "heading" : "4.2 Problem Formulation",
      "text" : ""
    }, {
      "heading" : "4.2.1 Objective Function Formulation",
      "text" : "Now let’s review the concept of MSPBE. MSPBE is defined as\nMSPBE(θ) = ‖Φθ −ΠT (Φθ)‖2Ξ = (ΦTΞ(TΦθ − Φθ))T (ΦTΞΦ)−1ΦTΞ(TΦθ − Φθ) = E[δt(θ)φt]TE[φtφTt ]−1E[δt(θ)φt]\nTo avoid computing the inverse matrix (ΦTΞΦ)−1 and to avoid the double sampling problem [1] in (4.2.1), an auxiliary variable w is defined\nw = E[φtφTt ]−1E[δt(θ)φt] = (ΦTΞΦ)−1ΦTΞ(TΦθ − Φθ)\n4.2. Problem Formulation 51\nThus we can have the following linear inverse problem\nE[δt(θ)φt] = E[φtφTt ]w = (ΦTΞΦ)w = ΦTΞ(TΦθ − Φθ)\nBy taking gradient w.r.t θ for optimum condition ∇MSPBE(θ) = 0 and utilizing Equation (4.2.1), we have\nE[δt(θ)φt] = γE[φ′tφTt ]w\nRearranging the two equality of Equation (4.2.1,4.2.1), we have the\nfollowing linear system equation[ ηΦTΞΦ ηΦTΞ(Φ− γΦ′) γΦ ′T ΞΦ ΦTΞ(Φ− γΦ′) ] [ w θ ] = [ ηΦTΞR ΦTΞR ] The stochastic gradient version of the above equation is as follows, where\nA = E[At], b = E[bt], x = [w; θ]\nAt =\n[ ηφtφt\nT ηφt(φt − γφ′t)T γφ′tφt T φt(φt − γφ′t)T\n] , bt = [ ηrtφt rtφt ] Following [26], the TDC algorithm solution follows from the linear\nequation Ax = b, where a single iteration gradient update would be\nxt+1 = xt − αt(Atxt − bt) where xt = [wt; θt]. The two time-scale gradient descent learning method TDC [26] is\nθt+1 = θt + αtδtφt − αtγφt′(φTt wt), wt+1 = wt + βt(δt − φTt wt)φt\nwhere −αtγφt′(φTt wt) is the term for correction of gradient descent direction, and βt = ηαt, η > 1.\nThere are some issues regarding the objective function, which arise from the online convex optimization and reinforcement learning perspectives, respectively. The first concern is that the objective function should be convex and stochastically solvable. Note that A,At are neither PSD nor symmetric, and it is not straightforward to formulate a\n52 Regularized Off-Policy Temporal Difference Learning\nconvex objective function based on them. The second concern is that since we do not have knowledge of A, the objective function should be separable so that it is stochastically solvable based on At, bt. The other concern regards the sampling condition in temporal difference learning: double-sampling. As pointed out in [1], double-sampling is a necessary condition to obtain an unbiased estimator if the objective function is the Bellman residual or its derivatives (such as projected Bellman residual), wherein the product of Bellman error or projected Bellman error metrics are involved. To overcome this sampling condition constraint, the product of TD errors should be avoided in the computation of gradients. Consequently, based on the linear equation\n4.2. Problem Formulation 53\nformulation in (4.2.1) and the requirement on the objective function discussed above, we propose the regularized loss function as\nL(x) = ‖Ax− b‖m + h(x)\nHere we also enumerate some intuitive objective functions and give a brief analysis on the reasons why they are not suitable for regularized off-policy first-order TD learning. One intuitive idea is to add a sparsity penalty on MSPBE, i.e., L(θ) = MSPBE(θ)+ρ‖θ‖1. Because of the l1 penalty term, the solution to ∇L = 0 does not have an analytical form and is thus difficult to compute. The second intuition is to use the online least squares formulation of the linear equation Ax = b. However, since A is not symmetric and positive semi-definite (PSD), A 1 2 does not exist and thus Ax = b cannot be reformulated as minx∈X ||A 1 2x−A− 12 b||22. Another possible idea is to attempt to find an objective function whose gradient is exactly Atxt − bt and thus the regularized gradient is proxαth(xt)(Atxt − bt). However, since At is not symmetric, this gradient does not explicitly correspond to any kind of optimization problem, not to mention a convex one2."
    }, {
      "heading" : "4.2.2 Squared Loss Formulation",
      "text" : "It is also worth noting that there exists another formulation of the loss function different from Equation (4.2.1) with the following convexconcave formulation as in [77, 47],\nmin x\n1 2 ‖Ax− b‖22 + ρ‖x‖1 = max‖AT y‖∞≤1 (bT y − ρ 2 yT y)\n= min x max ‖u‖∞≤1,y\n( xTu+ yT (Ax− b)− ρ\n2 yT y\n)\nHere we give the detailed deduction of formulation in Equation (4.1). First, using the dual norm representation, the standard LASSO problem formulation is reformulated as\n2 Note that the A matrix in GTD2’s linear equation representation is symmetric, yet is not PSD, so it cannot be formulated as a convex problem.\n54 Regularized Off-Policy Temporal Difference Learning\nf(x) = 1\n2 ‖Ax− b‖22 + ρ‖x‖1 = max\ny,‖AT y‖∞≤1\n[ 〈b/ρ, y〉 − 1\n2 yT y ] Then3\n〈b, y〉 − 12yT y = 〈b, y〉 − 12yT y + 〈 x,AT y 〉 − 〈y,Ax〉\n= 〈y, b−Ax〉 − 12yT y + 〈 x,AT y 〉 which can be solved iteratively without the proximal gradient step\nas follows, which serves as a counterpart of Equation (4.3),\nxt+1 = xt − αtρ(ut +AtT yt) , yt+1 = yt + αt ρ (Atxt − bt − ρyt)\nut+ 1 2 = ut + αt ρ xt , ut+1 = Π∞(ut+ 1 2 )"
    }, {
      "heading" : "4.3 Algorithm Design",
      "text" : ""
    }, {
      "heading" : "4.3.1 RO-TD Algorithm Design",
      "text" : "In this section, the problem of (4.2.1) is formulated as a convex-concave saddle-point problem, and the RO-TD algorithm is proposed. Analogous to (4.1.2), the regularized loss function can be formulated as\n‖Ax− b‖m + h(x) = max‖y‖n≤1 yT (Ax− b) + h(x)\nSimilar to (2.4), Equation (4.3.1) can be solved via an iteration procedure as follows, where xt = [wt; θt].\nxt+ 1 2 = xt − αtATt yt , yt+ 1 2 = yt + αt(Atxt − bt) xt+1 = proxαth(xt+ 1\n2 ) , yt+1 = Πn(yt+ 1 2 )\n3 Let w = −y, then we will have the same formulation as in Nemirovski’s tutorial in COLT2012.\nΦ(x,w) = 〈w,Ax− b〉 − 1\n2 wTw −\n〈 x,ATw 〉\n4.3. Algorithm Design 55\nThe averaging step, which plays a crucial role in stochastic optimization convergence, generates the approximate saddle-points [47, 78]\nx̄t = (∑t\ni=0 αi )−1∑t i=0 αixi, ȳt = (∑t i=0 αi )−1∑t i=0 αiyi\nDue to the computation of At in (4.3) at each iteration, the computation cost appears to be O(Nd2), where N, d are defined in Figure 4.1. However, the computation cost is actually O(Nd) with a linear algebraic trick by computing not At but y T t At, Atxt − bt. Denoting yt = [y1,t; y2,t], where y1,t; y2,t are column vectors of equal length, we have\nyTt At = [ ηφTt (y T 1,tφt) + γφ T t (y T 2,tφ ′ t) (φt − γφ′t)T (ηyT1,t + yT2,t)φt ] Atxt − bt can be computed according to Equation (4.2.1) as follows:\nAtxt − bt = [ −η(δt − φTt wt)φt; γ(φTt wt)φt′ − δtφt ] Both (4.3.1) and (4.3.1) are of linear computational complexity. Now we are ready to present the RO-TD algorithm:\nThere are some design details of the algorithm to be elaborated. First, the regularization term h(x) can be any kind of convex regularization, such as ridge regression or sparsity penalty ρ||x||1. In case of h(x) = ρ||x||1, proxαth(·) = Sαtρ(·). In real applications the sparsification requirement on θ and auxiliary variable w may be different, i.e., h(x) = ρ1‖θ‖1 + ρ2‖w‖1, ρ1 6= ρ2, one can simply replace the uniform soft thresholding Sαtρ by two separate soft thresholding operations Sαtρ1 , Sαtρ2 and thus the third equation in (4.3) is replaced by the following,\nxt+ 1 2\n= [ wt+ 1\n2 ; θt+ 1 2\n] , θt+1 = Sαtρ1(θt+ 1\n2 ), wt+1 = Sαtρ2(wt+ 1 2 )\nAnother concern is the choice of conjugate numbers (m,n). For ease of computing Πn, we use (2, 2)(l2 fit), (+∞, 1)(uniform fit) or (1,+∞). m = n = 2 is used in the experiments below.\n56 Regularized Off-Policy Temporal Difference Learning\nAlgorithm 8 RO-TD Let π be some fixed policy of an MDP M , and let the sample set S = {si, ri, si′}Ni=1. Let Φ be some fixed basis.\n(1) REPEAT (2) Compute φt, φt ′ and TD error δt = (rt + γφ ′T t θt)− φTt θt (3) Compute yT t At, Atxt − bt in Equation (4.3.1) and (4.3.1). (4) Compute xt+1, yt+1 as in Equation (4.3) (5) Set t← t+ 1; (6) UNTIL t = N ; (7) Compute x̄N , ȳN as in Equation (4.3.1) with t = N ."
    }, {
      "heading" : "4.3.2 RO-GQ(λ) Design",
      "text" : "GQ(λ)[79] is a generalization of the TDC algorithm with eligibility traces and off-policy learning of temporally abstract predictions, where the gradient update changes from Equation (4.2.1) to\nθt+1 = θt +αt[δtet− γ(1−λ)wtT etφ̄t+1], wt+1 = wt + βt(δtet−wTt φtφt)\nThe central element is to extend the MSPBE function to the case where it incorporates eligibility traces. The objective function and corresponding linear equation component At, bt can be written as follows:\nL(θ) = ||Φθ −ΠT πλΦθ||2Ξ\nAt =\n[ ηφtφt T ηet(φt − γφ̄t+1)T\nγ(1− λ)φ̄t+1eTt et(φt − γφ̄t+1) T\n] , bt = [ ηrtet rtet ] Similar to Equation (4.3.1) and (4.3.1), the computation of yT\nt At, Atxt−\nbt is\nyT t At = [ ηφTt (y T 1,tφt) + γ(1− λ)eTt (yT2,tφ̄t+1) (φt − γφ̄t+1)T (ηyT1,t + yT2,t)et ] Atxt − bt = [ −η(δtet − φTt wtφt); γ(1− λ)(eTt wt)φ̄t+1 − δtet\n] where eligibility traces et, and φ̄t, T πλ are defined in [79]. Algorithm 9, RO-GQ(λ), extends the RO-TD algorithm to include eligibility traces.\n4.4. Theoretical Analysis 57\nAlgorithm 9 RO-GQ(λ) Let π be some fixed policy of an MDP M . Let Φ be some fixed basis. Starting from s0.\n(1) REPEAT (2) Compute φt, φ̄t+1 and TD error δt = (rt + γφ̄ T t+1θt)− φTt θt (3) Compute yT t At, Atxt − bt in Equation (4.4). (4) Compute xt+1, yt+1 as in Equation (4.3) (5) Choose action at, and get st+1 (6) Set t← t+ 1; (7) UNTIL st is an absorbing state; (8) Compute x̄t, ȳt as in Equation (4.3.1)"
    }, {
      "heading" : "4.4 Theoretical Analysis",
      "text" : "The theoretical analysis of RO-TD algorithm can be seen in the Appendix."
    }, {
      "heading" : "4.5 Empirical Results",
      "text" : "We now demonstrate the effectiveness of the RO-TD algorithm against other algorithms across a number of benchmark domains. LARS-TD [62], which is a popular second-order sparse reinforcement learning algorithm, is used as the baseline algorithm for feature selection and TDC is used as the off-policy convergent RL baseline algorithm, respectively."
    }, {
      "heading" : "4.5.1 MSPBE Minimization and Off-Policy Convergence",
      "text" : "This experiment aims to show the minimization of MSPBE and offpolicy convergence of the RO-TD algorithm. The 7 state star MDP is a well known counterexample where TD diverges monotonically and TDC converges. It consists of 7 states and the reward w.r.t any transition is zero. Because of this, the star MDP is unsuitable for LSTDbased algorithms, including LARS-TD since ΦTR = 0 always holds. The random-walk problem is a standard Markov chain with 5 states and two absorbing state at two ends. Three sets of different bases Φ\n58 Regularized Off-Policy Temporal Difference Learning\n4.5. Empirical Results 59\nare used in [26], which are tabular features, inverted features and dependent features respectively. An identical experiment setting to [26] is used for these two domains. The regularization term h(x) is set to 0 to make a fair comparison with TD and TDC. α = 0.01, η = 10 for TD, TDC and RO-TD. The comparison with TD, TDC and RO-TD is shown in the left sub-figure of Figure 4.2, where TDC and RO-TD have almost identical MSPBE over iterations. The middle sub-figure shows the value of yT t\n(Axt − b) and ‖Axt − b‖2, wherein ‖Axt − b‖2 is always greater than the value of yT\nt (Axt − b). Note that for this prob-\nlem, the Slater condition is satisfied so there is no duality gap between the two curves. As the result shows, TDC and RO-TD perform equally well, which illustrates the off-policy convergence of the RO-TD algorithm. The result of random-walk chain is averaged over 50 runs. The rightmost sub-figure of Figure 4.2 shows that RO-TD is able to reduce MSPBE over successive iterations w.r.t three different basis functions."
    }, {
      "heading" : "4.5.2 Feature Selection",
      "text" : "In this section, we use the mountain car example with a variety of bases to show the feature selection capability of RO-TD. The Mountain car is an optimal control problem with a continuous two-dimensional state space. The steep discontinuity in the value function makes learning difficult for bases with global support. To make a fair comparison, we use the same basis function setting as in [62], where two dimensional grids of 2, 4, 8, 16, 32 RBFs are used so that there are totally 1365 basis functions. For LARS-TD, 500 samples are used. For RO-TD and TDC, 3000 samples are used by executing 15 episodes with 200 steps for each episode, stepsize αt = 0.001, and ρ1 = 0.01, ρ2 = 0.2. We use the result of LARS-TD and l2 LSTD reported in [62]. As the result shows in Table 4.1, RO-TD is able to perform feature selection successfully, whereas TDC and TD failed. It is worth noting that comparing the performance of RO-TD and LARS-TD is not the major focus here, since LARS-TD is not convergent off-policy and RO-TD’s performance can be further optimized using the mirror-descent approach with the Mirror-Prox algorithm [47] which incorporates mirror descent with an extragradient [8], as discussed below.\n60 Regularized Off-Policy Temporal Difference Learning"
    }, {
      "heading" : "4.5.3 High-dimensional Under-actuated Systems",
      "text" : "The triple-link inverted pendulum [71] is a highly nonlinear underactuated system with 8-dimensional state space and discrete action space. The state space consists of the angles and angular velocity of each arm as well as the position and velocity of the car. The discrete action space is {0, 5Newton,−5Newton}. The goal is to learn a policy that can balance the arms for Nx steps within some minimum number of learning episodes. The allowed maximum number of episodes is 300. The pendulum initiates from zero equilibrium state and the first action is randomly chosen to push the pendulum away from initial state. We test the performance of RO-GQ(λ), GQ(λ) and LARS-TD. Two experiments are conducted with Nx = 10, 000 and 100, 000, respectively. Fourier basis [80] with order 2 is used, resulting in 6561 basis functions. Table 4.2 shows the results of this experiment, where RO-GQ(λ) performs better than other approaches, especially in Experiment 2, which is a harder task. LARS-TD failed in this domain, which is mainly not due to LARS-TD itself but the quality of samples collected via random walk.\nTo sum up, RO-GQ(λ) tends to outperform GQ(λ) in all aspects, and is able to outperform LARS-TD based policy iteration in high di-\n4.6. Summary 61\nmensional domains, as well as in selected smaller MDPs where LARSTD diverges (e.g., the star MDP). It is worth noting that the computation cost of LARS-TD is O(Ndk2), where that for RO-TD is O(Nd). If k is linear or sublinear w.r.t d, RO-TD has a significant advantage over LARS-TD. However, compared with LARS-TD, RO-TD requires fine tuning the parameters of αt, ρ1, ρ2 and is usually not as sample efficient as LARS-TD. We also find that tuning the sparsity parameter ρ2 generates an interpolation between GQ(λ) and Q-learning, where a large ρ2 helps eliminate the correction term of TDC update and make the update direction more similar to the TD update."
    }, {
      "heading" : "4.6 Summary",
      "text" : "In this chapter we present a novel unified framework for designing regularized off-policy convergent RL algorithms combining a convexconcave saddle-point problem formulation for RL with stochastic firstorder methods. A detailed experimental analysis reveals that the proposed RO-TD algorithm is both off-policy convergent and robust to noisy features.\n5 Safe Reinforcement Learning using Projected Natural Actor Critic\nNatural actor-critics form a popular class of policy search algorithms for finding locally optimal policies for Markov decision processes. In this paper we address a drawback of natural actor-critics that limits their real-world applicability—their lack of safety guarantees. We present a principled algorithm for performing natural gradient descent over a constrained domain 1. In the context of reinforcement learning, this allows for natural actor-critic algorithms that are guaranteed to remain within a known safe region of policy space. While deriving our class of constrained natural actor-critic algorithms, which we call Projected Natural Actor-Critics (PNACs), we also elucidate the relationship between natural gradient descent and mirror descent."
    }, {
      "heading" : "5.1 Introduction",
      "text" : "Natural actor-critics form a class of policy search algorithms for finding locally optimal policies for Markov decision processes (MDPs) by approximating and ascending the natural gradient [59] of an objective\n1 This paper is a revised version of the paper “Projected Natural Actor-Critic” that was published in NIPS 2013.\n62\n5.1. Introduction 63\nfunction. Despite the numerous successes of, and the continually growing interest in, natural actor-critic algorithms, they have not achieved widespread use for real-world applications. A lack of safety guarantees is a common reason for avoiding the use of natural actor-critic algorithms, particularly for biomedical applications. Since natural actor-critics are unconstrained optimization algorithms, there are no guarantees that they will avoid regions of policy space that are known to be dangerous.\nFor example, proportional-integral-derivative controllers (PID controllers) are the most widely used control algorithms in industry, and have been studied in depth [81]. Techniques exist for determining the set of stable gains (policy parameters) when a model of the system is available [82]. Policy search can be used to find the optimal gains within this set (for some definition of optimality). A desirable property of a policy search algorithm in this context would be a guarantee that it will remain within the predicted region of stable gains during its search.\nConsider a second example: functional electrical stimulation (FES) control of a human arm. By selectively stimulating muscles using subcutaneous probes, researchers have made significant strides toward returning motor control to people suffering from paralysis induced by spinal cord injury [83]. There has been a recent push to develop controllers that specify how much and when to stimulate each muscle in a human arm to move it from its current position to a desired position [84]. This closed-loop control problem is particularly challenging because each person’s arm has different dynamics due to differences in, for example, length, mass, strength, clothing, and amounts of muscle atrophy, spasticity, and fatigue. Moreover, these differences are challenging to model. Hence, a proportional-derivative (PD) controller, tuned to a simulation of an ideal human arm, required manual tuning to obtain desirable performance on a human subject with biceps spasticity [85].\nResearchers have shown that policy search algorithms are a viable approach to creating controllers that can automatically adapt to an individual’s arm by training on a few hundred two-second reaching movements [86]. However, safety concerns have been raised in regard to both this specific application and other biomedical applications of policy search algorithms. Specifically, the existing state-of-the-art gradient-\n64 Safe Reinforcement Learning using Projected Natural Actor Critic\nbased algorithms, including the current natural actor-critic algorithms, are unconstrained and could potentially select dangerous policies. For example, it is known that certain muscle stimulations could cause the dislocation of a subject’s arm. Although we lack an accurate model of each individual’s arm, we can generate conservative safety constraints on the space of policies. Once again, a desirable property of a policy search algorithm would be a guarantee that it will remain within a specified region of policy space (known-safe policies).\nIn this paper we present a class of natural actor-critic algorithms that perform constrained optimization—given a known safe region of policy space, they search for a locally optimal policy while always remaining within the specified region. We call our class of algorithms Projected Natural Actor-Critics (PNACs) since, whenever they generate a new policy, they project the policy back to the set of safe policies. The interesting question is how the projection can be done in a principled manner. We show that natural gradient descent (ascent), which is an unconstrained optimization algorithm, is a special case of mirror descent (ascent), which is a constrained optimization algorithm. In order to create a projected natural gradient algorithm, we add constraints in the mirror descent algorithm that is equivalent to natural gradient descent. We apply this projected natural gradient algorithm to policy search to create the PNAC algorithms, which we validate empirically."
    }, {
      "heading" : "5.2 Related Work",
      "text" : "Researchers have addressed safety concerns like these before [87]. Bendrahim and Franklin [88] showed how a walking biped robot can switch to a stabilizing controller whenever the robot leaves a stable region of state space. Similar state-avoidant approaches to safety have been proposed by several others [89, 90, 91]. These approaches do not account for situations where, over an unavoidable region of state space, the actions themselves are dangerous. Kuindersma et al. [92] developed a method for performing risk-sensitive policy search, which models the variance of the objective function for each policy and permits runtime adjustments of risk sensitivity. However, their approach does not guarantee that an unsafe region of state space or policy space will be avoided.\n5.3. Equivalence of Natural Gradient Descent and Mirror Descent 65\nBhatnagar et al. [93] presented projected natural actor-critic algorithms for the average reward setting. As in our projected natural actor-critic algorithms, they proposed computing the update to the policy parameters and then projecting back to the set of allowed policy parameters. However, they did not specify how the projection could be done in a principled manner. We show in Section 5.5 that the Euclidean projection can be arbitrarily bad, and argue that the projection that we propose is particularly compatible with natural actor-critics (natural gradient descent).\nDuchi et al. [94] presented mirror descent using the Mahalanobis norm for the proximal function, which is very similar to the proximal function that we show to cause mirror descent to be equivalent to natural gradient descent. However, their proximal function is not identical to ours and they did not discuss any possible relationship between mirror descent and natural gradient descent."
    }, {
      "heading" : "5.3 Equivalence of Natural Gradient Descent and Mirror Descent",
      "text" : "We begin by showing an important relationship between natural gradient methods and mirror descent.\nTheorem 5.3.1. The natural gradient descent update at step k with metric tensor Gk , G(xk):\nxk+1 = xk − αkG−1k ∇f(xk), (5.1)\nis equivalent to the mirror descent update at step k, with ψk(x) = (1/2)xᵀGkx.\nProof. First, notice that ∇ψk(x) = Gkx. Next, we derive a closed-form for ψ∗k:\nψ∗k(y) = max x∈Rn\n{ xᵀy − 1\n2 xᵀGkx\n} . (5.2)\nSince the function being maximized on the right hand side is strictly concave, the x that maximizes it is its critical point. Solving for this\n66 Safe Reinforcement Learning using Projected Natural Actor Critic\ncritical point, we get x = G−1k y. Substituting this into (5.2), we find that ψ∗k(y) = ( 1/2)yᵀG−1k y. Hence, ∇ψ∗k(y) = G−1k y. Using the definitions of ∇ψk(x) and ∇ψ∗k(y), we find that the mirror descent update is\nxk+1 =G −1 k (Gkxk − αk∇f(xk)) = xk − αkG−1k ∇f(xk),\nwhich is identical to (5.1). Although researchers often use ψk that are norms like the p-norm and Mahalanobis norm, notice that the ψk that results in natural gradient descent is not a norm. Also, since Gk depends on k, ψk is an adaptive proximal function [94]."
    }, {
      "heading" : "5.4 Projected Natural Gradients",
      "text" : "When x is constrained to some set, X, ψk in mirror descent is augmented with the indicator function IX , where IX(x) = 0 if x ∈ X, and +∞ otherwise. The ψk that was shown to generate an update equivalent to the natural gradient descent update, with the added constraint that x ∈ X, is ψk(x) = (1/2)xᵀGkx + IX(x). Hereafter, any references to ψk refer to this augmented version.\nFor this proximal function, the subdifferential of ψk(x) is ∇ψk(x) = Gk(x) + N̂X(x) = (Gk + N̂X)(x), where N̂X(x) , ∂IX(x) and, in the middle term, Gk and N̂X are relations and + denotes Minkowski addition.2 N̂X(x) is the normal cone of X at x if x ∈ X and ∅ otherwise [95].\n∇ψ∗k(y) = (Gk + N̂X)−1(y). (5.3)\nLet ΠGkX (y), be the set of x ∈ X that are closest to y, where the length of a vector, z, is (1/2)zᵀGkz. More formally,\nΠGkX (y) , arg minx∈X 1 2 (y − x)ᵀGk(y − x). (5.4)\n2 Later, we abuse notation and switch freely between treating Gk as a matrix and a relation. When it is a matrix, Gkx denotes matrix-vector multiplication that produces a vector. When it is a relation, Gk(x) produces the singleton {Gkx}.\n5.5. Compatibility of Projection 67\nLemma 5.4.1. ΠGkX (y) = (Gk + N̂X) −1(Gky).\nProof. We write (5.4) without the explicit constraint that x ∈ X by appending the indicator function:\nΠGkX (y) = arg minx∈Rn hy(x),\nwhere hy(x) = (1/2)(y−x)ᵀGk(y−x)+IX(x). Since hy is strictly convex over X and +∞ elsewhere, its critical point is its global minimizer. The critical point satisfies\n0 ∈ ∇hy(x) = −Gk(y) +Gk(x) + N̂X(x).\nThe globally minimizing x therefore satisfies Gky ∈ Gk(x) + N̂X(x) = (Gk + N̂X)(x). Solving for x, we find that x = (Gk + N̂X)\n−1(Gky). Combining Lemma 5.4.1 with (5.3), we find that ∇ψ∗(y) = ΠGkX (G −1 k y). Hence, mirror descent with the proximal function that produces natural gradient descent, augmented to include the constraint that x ∈ X, is:\nxk+1 =Π Gk X ( G−1k ( (Gk + N̂X)(xk)− αk∇f(xk) )) =ΠGkX ( (I +G−1k N̂X)(xk)− αkG−1k ∇f(xk) ) ,\nwhere I denotes the identity relation. Since xk ∈ X, we know that 0 ∈ N̂X(xk), and hence the update can be written as\nxk+1 = Π Gk X ( xk − αkG−1k ∇f(xk) ) , (5.5)\nwhich we call projected natural gradient (PNG)."
    }, {
      "heading" : "5.5 Compatibility of Projection",
      "text" : "The standard projected subgradient (PSG) descent method follows the negative gradient (as opposed to the negative natural gradient) and projects back to X using the Euclidean norm. If f and X are convex and the stepsize is decayed appropriately, it is guaranteed to converge to a global minimum, x∗ ∈ X. Any such x∗ is a fixed point. This means\n68 Safe Reinforcement Learning using Projected Natural Actor Critic\nthat a small step in the negative direction of any subdifferential of f at x∗ will project back to x∗.\nOur choice of projection, ΠGkX , results in PNG having the same fixed points (see Lemma 5.5.1). This means that, when the algorithm is at x∗ and a small step is taken down the natural gradient to x′, ΠGkX will project x′ back to x∗. We therefore say that ΠGkX is compatible with the natural gradient. For comparison, the Euclidean projection of x′ will not necessarily return x′ to x∗.\nLemma 5.5.1. The sets of fixed points for PSG and PNG are equivalent.\nProof. A necessary and sufficient condition for x to be a fixed point of PSG is that −∇f(x) ∈ N̂X(x) [96]. A necessary and sufficient condition for x to be a fixed point of PNG is\nx =ΠGkX ( x− αkG−1k ∇f(x) ) = (Gk + N̂X) −1 ( Gk ( x− αkG−1k ∇f(x) ) ) =(Gk + N̂X)\n−1 (Gkx− αk∇f(x)) ⇔Gkx− αk∇f(x) ∈ Gk(x) + N̂X(x) ⇔−∇f(x) ∈ N̂X(x).\nTo emphasize the importance of using a compatible projection, consider the following simple example. Minimize the function f(x) = xᵀAx+ bᵀx, where A = diag(1, 0.01) and b = [−0.2,−0.1]ᵀ, subject to the constraints ‖x‖1 ≤ 1 and x ≥ 0. We implemented three algorithms, and ran each for 1000 iterations using a fixed stepsize:\n(1) PSG - projected subgradient descent using the Euclidean\nprojection. (2) PNG - projected natural gradient descent using ΠGkX . (3) PNG-Euclid - projected natural gradient descent using the\nEuclidean projection.\nThe results are shown in Figure 1. Notice that PNG and PSG converge to the optimal solution, x∗. From this point, they both step in different directions, but project back to x∗. However, PNG-Euclid converges to\n5.6. Natural Actor-Critic Algorithms 69\nFig. 5.1: The thick diagonal line shows one constraint and dotted lines show projections. Solid arrows show the directions of the natural gradient and gradient at the optimal solution, x∗. The dashed blue arrows show PNG-Euclid’s projections, and emphasize the the projections cause PNGEuclid to move away from the optimal solution.\na suboptimal solution (outside the domain of the figure). If X were a line segment between the point that PNG-Euclid and PNG converge to, then PNG-Euclid would converge to the pessimal solution within X, while PSG and PNG would converge to the optimal solution within X. Also, notice that the natural gradient corrects for the curvature of the function and heads directly towards the global unconstrained minimum. Since the natural methods in this example use metric tensor G = A, which is the Hessian of f , they are essentially an incremental form of Newton’s method. In practice, the Hessian is usually not known, and an estimate thereof is used."
    }, {
      "heading" : "5.6 Natural Actor-Critic Algorithms",
      "text" : "An MDP is a tuple M = (S,A,P,R, d0, γ), where S is a set of states, A is a set of actions, P(s′|s, a) gives the probability density of the system entering state s′ when action a is taken in state s, R(s, a) is the expected reward, r, when action a is taken in state s, d0 is the initial state distribution, and γ ∈ [0, 1) is a reward discount parameter. A parameterized policy, π, is a conditional probability density function— π(a|s, θ) is the probability density of action a in state s given a vector of policy parameters, θ ∈ Rn.\n70 Safe Reinforcement Learning using Projected Natural Actor Critic\nLet J(θ) = E [∑∞\nt=0 γ trt|θ\n] be the discounted-reward objective or the\naverage reward objective function with J(θ) = limn→∞ 1 nE [ ∑n t=0 rt|θ]. Given an MDP, M , and a parameterized policy, π, the goal is to find policy parameters that maximize one of these objectives. When the action set is continuous, the search for globally optimal policy parameters becomes intractable, so policy search algorithms typically search for locally optimal policy parameters.\nNatural actor-critics, first proposed by Kakade [97], are algorithms that estimate and ascend the natural gradient of J(θ), using the average Fisher information matrix as the metric tensor:\nGk = G(θk) = Es∼dπ ,a∼π\n[( ∂\n∂θk log π(a|s, θk)\n)( ∂\n∂θk log π(a|s, θk)\n)ᵀ] ,\nwhere dπ is a policy and objective function-dependent distribution over the state set [98].\nThere are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(λ) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(λ) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93]. All of them form an estimate, typically denoted wk, of the natural gradient of J(θk). That is, wk ≈ G(θk)−1∇J(θk). They then perform the policy parameter update, θk+1 = θk + αkwk."
    }, {
      "heading" : "5.7 Projected Natural Actor-Critics",
      "text" : "If we are given a closed convex set, Θ ⊆ Rn, of admissible policy parameters (e.g., the stable region of gains for a PID controller), we may wish to ensure that the policy parameters remain within Θ. The natural actor-critic algorithms described in the previous section do not provide such a guarantee. However, their policy parameter update equations, which are natural gradient ascent updates, can easily be modified to the projected natural gradient ascent update in (5.5) by projecting the parameters back onto Θ using Π G(θk) Θ :\nθk+1 = Π G(θk) Θ (θk + αkwk) .\n5.7. Projected Natural Actor-Critics 71\nMany of the existing natural policy gradient algorithms, including NAC-LSTD, eNAC, NAC-Sarsa, and INAC, follow biased estimates of the natural policy gradient [103]. For our experiments, we must use an unbiased algorithm since the projection that we propose is compatible with the natural gradient, but not necessarily biased estimates thereof.\nNAC-Sarsa and INAC are equivalent biased discounted-reward natural actor-critic algorithms with per-time-step time complexity linear in the number of features. The former was derived by replacing the LSTD-Q(λ) component of NAC-LSTD with Sarsa(λ), while the latter is the discounted-reward version of NGAC. Both are similar to NTD, which is a biased average-reward algorithm. The unbiased discounted - reward form of NAC-Sarsa was recently derived [103]. References to NAC-Sarsa hereafter refer to this unbiased variant. In our case studies we use the projected natural actor-critic using Sarsa(λ) (PNAC-Sarsa), the projected version of the unbiased NAC-Sarsa algorithm.\nNotice that the projection, Π G(θk) Θ , as defined in (5.4), is not merely the Euclidean projection back onto Θ. For example, if Θ is the set of θ that satisfy Aθ ≤ b, for some fixed matrix A and vector b, then the projection, Π\nG(θk) Θ , of y onto Θ is a quadratic program,\nminimize f(θ) =− yᵀG(θk)θ + 1\n2 θᵀG(θk)θ, s.t. Aθ ≤ b.\nIn order to perform this projection, we require an estimate of the average Fisher information matrix, G(θk). If the natural actor-critic algorithm does not already include this (like NAC-LSTD and NACSarsa do not), then an estimate can be generated by selecting G0 = βI, where β is a positive scalar and I is the identity matrix, and then updating the estimate with Gt+1 = (1− µt)Gt + µt ( ∂\n∂θk log π(at|st, θk)\n)( ∂\n∂θk log π(at|st, θk)\n)ᵀ ,\nwhere {µt} is a stepsize schedule [93]. Notice that we use t and k subscripts since many time steps of the MDP may pass between updates to the policy parameters.\n72 Safe Reinforcement Learning using Projected Natural Actor Critic"
    }, {
      "heading" : "5.8 Case Study: Functional Electrical Stimulation",
      "text" : "In this case study, we searched for proportional-derivative (PD) gains to control a simulated human arm undergoing FES. We used the Dynamic Arm Simulator 1 (DAS1) [104], a detailed biomechanical simulation of a human arm undergoing functional electrical stimulation. In a previous study, a controller created using DAS1 performed well on an actual human subject undergoing FES, although it required some additional tuning in order to cope with biceps spasticity [85]. This suggests that it is a reasonably accurate model of an ideal arm.\nThe DAS1 model, depicted in Figure 2a, has state st =\n(φ1, φ2, φ̇1, φ̇2, φ target 1 , φ target 2 ), where φ target 1 and φ target 2 are the desired joint angles, and the desired joint angle velocities are zero. The goal is to, during a two-second episode, move the arm from its random initial state to a randomly chosen stationary target. The arm is controlled by providing a stimulation in the interval [0, 1] to each of six muscles. The reward function used was similar to that of Jagodnik and van den Bogert [85], which punishes joint angle error and high muscle stimulation. We searched for locally optimal PD gains using PNAC-Sarsa where the policy was a PD controller with Gaussian noise added for exploration.\nAlthough DAS1 does not model shoulder dislocation, we added safety constraints by limiting the l1-norm of certain pairs of gains. The constraints were selected to limit the forces applied to the humerus. These constraints can be expressed in the form Aθ ≤ b, where A is a matrix, b is a vector, and θ are the PD gains (policy parameters). We compared the performance of three algorithms:\n(1) NAC: NAC-Sarsa with no constraints on θ. (2) PNAC: PNAC-Sarsa using the compatible projection,\nΠ G(θk) Θ .\n(3) PNAC-E: PNAC-Sarsa using the Euclidean projection.\nSince we are not promoting the use of one natural actor-critic over another, we did not focus on finely tuning the natural actor-critic nor comparing the learning speeds of different natural actor-critics. Rather, we show the importance of the proper projection by allowing PNACSarsa to run for a million episodes (far longer than required for con-\n5.9. Case Study: uBot Balancing 73\nvergence), after which we plot the mean sum of rewards during the last quarter million episodes. Each algorithm was run ten times, and the results averaged and plotted in Figure 2b. Notice that PNAC performs worse than the unconstrained NAC. This happens because NAC leaves the safe region of policy space during its search, and converges to a dangerous policy—one that reaches the goal quickly and with low total muscle force, but which can cause large, short, spikes in muscle forces surrounding the shoulder, which violates our safety constraints. We suspect that PNAC converges to a near-optimal policy within the region of policy space that we have designated as safe. PNAC-E converges to a policy that is worse than that found by PNAC because it uses an incompatible projection."
    }, {
      "heading" : "5.9 Case Study: uBot Balancing",
      "text" : "In the previous case study, the optimal policy lay outside the designated safe region of policy space (this is common when a single failure is so costly that adding a penalty to the reward function for failure is impractical, since a single failure is unacceptable). We present a second case study in which the optimal policy lies within the designated safe region of policy space, but where an unconstrained search algorithm may enter the unsafe region during its search of policy space (at which point large negative rewards return it to the safe region).\nThe uBot-5, shown in Figure 5.2, is an 11-DoF mobile manipulator developed at the University of Massachusetts Amherst [20, 21]. During experiments, it often uses its arms to interact with the world. Here, we consider the problem faced by the controller tasked with keeping the robot balanced during such experiments. To allow for results that are easy to visualize in 2D, we use a PD controller that observes only the current body angle, its time derivative, and the target angle (always vertical). This results in the PD controller having only two gains (tunable policy parameters). We use a crude simulation of the uBot-5 with random upper-body movements, and search for the PD gains that minimize a weighted combination of the energy used and the mean angle error (distance from vertical).\nWe constructed a set of conservative estimates of the region of stable\n74 Safe Reinforcement Learning using Projected Natural Actor Critic\n(Figure 2a) DAS1, the two-joint, sixmuscle biomechanical model used. Antagonistic muscle pairs are as follows, listed as (flexor, extensor): monoarticular shoulder muscles (a: anterior deltoid, b: posterior deltoid); monoarticular elbow muscles (c: brachialis, d: triceps brachii (short head)); biarticular muscles (e: biceps brachii, f: triceps brachii (long head)).\nNAC PNAC PNAC‐E\n‐15\nrn Re tu r\n‐16 M ea n \n1\nM\n‐ 7\n(Figure 2b) Mean return during the last 250,000 episodes of training using thee algorithms. Standard deviation error bars from the 10 trials are provided. The NAC bar is red to emphasize that the final policy found by NAC resides in the dangerous region of policy space.\ngains, with which the uBot-5 should never fall, and used PNAC-Sarsa and NAC-Sarsa to search for the optimal gains. Each training episode lasted 20 seconds, but was terminated early (with a large penalty) if the uBot-5 fell over. Figure 5.2 (middle) shows performance over 100 training episodes. Using NAC-Sarsa, the PD weights often left the conservative estimate of the safe region, which resulted in the uBot-5\n5.10. Summary 75\nfalling over. Figure 5.2 (right) shows one trial where the uBot-5 fell over four times (circled in red). The resulting large punishments cause NAC-Sarsa to quickly return to the safe region of policy space. Using PNAC-Sarsa, the simulated uBot-5 never fell. Both algorithms converge to gains that reside within the safe region of policy space. We selected this example because it shows how, even if the optimal solution resides within the safe region of policy space (unlike the in the previous case study), unconstrained RL algorithms may traverse unsafe regions of policy space during their search."
    }, {
      "heading" : "5.10 Summary",
      "text" : "We presented a class of algorithms, which we call projected natural actor-critics (PNACs). PNACs are the simple modification of existing natural actor-critic algorithms to include a projection of newly computed policy parameters back onto an allowed set of policy parameters (e.g., those of policies that are known to be safe). We argued that a principled projection is the one that results from viewing natural gradient descent, which is an unconstrained algorithm, as a special case of\n76 Safe Reinforcement Learning using Projected Natural Actor Critic\nmirror descent, which is a constrained algorithm.\nWe show that the resulting projection is compatible with the natural gradient and gave a simple empirical example that shows why a compatible projection is important. This example also shows how an incompatible projection can result in natural gradient descent converging to a pessimal solution in situations where a compatible projection results in convergence to an optimal solution. We then applied a PNAC algorithm to a realistic constrained control problem with six-dimensional continuous states and actions. Our results support our claim that the use of an incompatible projection can result in convergence to inferior policies. Finally, we applied PNAC to a simulated robot and showed its substantial benefits over unconstrained natural actor-critic algorithms.\n6 True Stochastic Gradient Temporal Difference Learning Algorithms\nWe now turn to the solution of a longstanding puzzle: how to design a “true” gradient method for reinforcement learning? We address longstanding questions in reinforcement learning: (1) Are there any firstorder reinforcement learning algorithms that can be viewed as “true” stochastic gradient methods? If there are, what are their objective functions and what are their convergence rates? (2) What is the general framework for avoiding biased sampling (instead of double-sampling, which is a stringent sampling requirement) in reinforcement learning? To this end, we introduce a novel primal-dual splitting framework for reinforcement learning, which shows that the GTD family of algorithms are true stochastic algorithms with respect to the primal-dual formulation of the objective functions such as NEU and MSPBE, which facilitates their convergence rate analysis and regularization. We also propose operator splitting as a unified framework to avoid bias sampling in reinforcement learning. We present an illustrative empirical study on simple canonical problems validating the effectiveness of the proposed algorithms compared with previous approaches.\n77\n78 True Stochastic Gradient Temporal Difference Learning Algorithms"
    }, {
      "heading" : "6.1 Introduction",
      "text" : "First-order temporal difference (TD) learning is a widely used class of techniques in reinforcement learning. Although least-squares based temporal difference approaches, such as LSTD [23], LSPE [24] and LSPI [25] perform well with moderate size problems, first-order temporal difference learning algorithms scale more gracefully to high dimensional problems. The initial class of TD methods was known to converge only when samples are drawn “on-policy”. This motivated the development of the gradient TD (GTD) family of methods [26]. A novel saddle-point framework for sparse regularized GTD was proposed recently [14]. However, there have been several questions regarding the current off-policy TD algorithms. (1) The first is the convergence rate of these algorithms. Although these algorithms are motivated from the gradient of an objective function such as MSPBE and NEU, they are not true stochastic gradient methods with respect to these objective functions, as pointed out in [27], which make the convergence rate and error bound analysis difficult, although asymptotic analysis has been carried out using the ODE approach. (2) The second concern is regarding acceleration. It is believed that TDC performs the best so far of the GTD family of algorithms. One may intuitively ask if there are any gradient TD algorithms that can outperform TDC. (3) The third concern is regarding compactness of the feasible set θ. The GTD family of algorithms all assume that the feasible set θ is unbounded, and if the feasible set θ is compact, there is no theoretical analysis and convergence guarantee. (4) The fourth question is on regularization: although the saddle point framework proposed in [14] provides an online regularization framework for the GTD family of algorithms, termed as RO-TD, it is based on the inverse problem formulation and is thus not quite explicit. One further question is whether there is a more straightforward algorithm, e.g, the regularization is directly based on the MSPBE and NEU objective functions.\nBiased sampling is a well-known problem in reinforcement learning. Biased sampling is caused by the stochasticity of the policy wherein there are multiple possible successor states from the current state where the agent is. If it is a deterministic policy, then there will be no biased\n6.2. Background 79\nsampling problem. Biased sampling is often caused by the product of the TD errors, or the product of TD error and the gradient of TD error w.r.t the model parameter θ. There are two ways to avoid the biased sampling problem, which can be categorized into double sampling methods and two-time-scale stochastic approximation methods.\nIn this paper, we propose a novel approach to TD algorithm design in reinforcement learning, based on introducing the proximal splitting framework [28]. We show that the GTD family of algorithms are true stochastic gradient descent (SGD) methods, thus making their convergence rate analysis available. New accelerated off-policy algorithms are proposed and their comparative study with RO-TD is carried out to show the effectiveness of the proposed algorithms. We also show that primal-dual splitting is a unified first-order optimization framework to solve the biased sampling problem.\nHere is a roadmap to the rest of the chapter. Section 2 reviews reinforcement learning and the basics of proximal splitting formulations and algorithms. Section 3 introduces a novel problem formulation which we investigate in this paper. Section 4 proposes a series of new algorithms, demonstrates the connection with the GTD algorithm family, and also presents accelerated algorithms. Section 5 presents theoretical analysis of the algorithms. Finally, empirical results are presented in Section 6 which validate the effectiveness of the proposed algorithmic framework. Abbreviated technical proofs of the main theoretical results are provided in a supplementary appendix."
    }, {
      "heading" : "6.2 Background",
      "text" : ""
    }, {
      "heading" : "6.2.1 Markov Decision Process and Reinforcement Learning",
      "text" : "In linear value function approximation, a value function is assumed to lie in the linear span of a basis function matrix Φ of dimension |S| × d, where d is the number of linear independent features. Hence, V ≈ Vθ = Φθ. For the t-th sample, φt (the t-th row of Φ), φ′t (the t-th row of Φ′) are the feature vectors corresponding to st, s ′ t, respectively. θt is the weight vector for t-th sample in first-order TD learning methods, and δt = (rt+γφ ′T t θt)−φTt θt is the temporal difference error. TD learning uses the following update rule θt+1 = θt + αtδtφt, where\n80 True Stochastic Gradient Temporal Difference Learning Algorithms\nαt is the stepsize. However, TD is only guaranteed to converge in the on-policy setting, although in many off-policy situations, it still has satisfactory performance [105]. To this end, Sutton et al. proposed a family of off-policy convergent algorithms including GTD, GTD2 and TD with gradient correction (TDC). GTD is a two-time-scale stochastic approximation approach which aims to minimize the norm of the expected TD update (NEU), which is defined as\nNEU(θ) = E[δt(θ)φt]TE[δt(θ)φt].\nTDC [26] aims to minimize the mean-square projected Bellman error (MSPBE) with a similar two-time-scale technique, which is defined as MSPBE(θ) =\n‖Φθ −ΠT (Φθ)‖2Ξ = (ΦTΞ(TΦθ − Φθ))T (ΦTΞΦ)−1ΦTΞ(TΦθ − Φθ),\nwhere Ξ is a diagonal matrix whose entries ξ(s) are given by a positive probability distribution over states."
    }, {
      "heading" : "6.3 Problem Formulation",
      "text" : "Biased sampling is a well-known problem in reinforcement learning. Biased sampling is caused by E[φ′Tt φ ′ t] or E[φ ′ tφ ′T t ], where φ ′ t is the feature vector for state s ′ t in sample (st, at, rt, s ′ t). Due to the stochastic nature of the policy, there may be many s′t w.r.t the same st, thus E[φ ′T t φ ′ t] or E[φ′tφ ′T t ] cannot be consistently estimated via a single sample. This problem hinders the objective functions to be solved via stochastic gradient descent (SGD) algorithms. As pointed out in [27], although many algorithms are motivated by well-defined convex objective functions such as MSPBE and NEU, due to the biased sampling problem, the unbiased stochastic gradient is impossible to obtain, and thus the algorithms are not true SGD methods w.r.t. these objective functions. The biased sampling is often caused by the product of the TD errors, or the product of TD error and the derivative of TD error w.r.t. the parameter θ. There are two ways to avoid the biased sampling problem, which can be categorized into double sampling methods and stochastic approximation methods. Double sampling, which samples both s′ and\n6.3. Problem Formulation 81\ns′′ and thus requires computing φ′ and φ′′, is possible in batch reinforcement learning, but is usually impractical in online reinforcement learning. The other approach is stochastic approximation, which introduces a new variable to estimate the part containing φ ′ t, thus avoiding the product of φ ′ t and φ ′′ t . Consider, for example, the NEU objective function in Section (6.2.1). Taking the gradient w.r.t. θ, we have\n−1 2 NEU(θ) = E[(φt − γφ′t)φTt ]E[δt(θ)φt]\nIf the gradient can be written as a single expectation value, then it is straightforward to use a stochastic gradient method, however, here we have a product of two expectations, and due to the correlation between (φt− γφ′t)φTt and δt(θ)φt, the sampled product is not an unbiased estimate of the gradient. In other words, E[(φt−γφ′t)φTt ] and E[δt(θ)φt] can be directly sampled, yet E[(φt − γφ′t)φTt ]E[δt(θ)φt] can not be directly sampled. To tackle this, the GTD algorithm uses the two-time-scale stochastic approximation method by introducing an auxiliary variable wt, and thus the method is not a true stochastic gradient method w.r.t. NEU(θ) any more. This auxiliary variable technique is also used in [56].\nThe other problem for first-order reinforcement learning algorithms is that it is difficult to define the objective functions, which is also caused by the biased sampling problem. As pointed out in [27], although the GTD family of algorithms are derived from the gradient w.r.t. the objective functions such as MSPBE and NEU, because of the biasedsampling problem, these algorithms cannot be formulated directly as SGD methods w.r.t. these objective functions.\nIn sum, due to biased sampling, the RL objective functions cannot be solved via a stochastic gradient method, and it is also difficult to find objective functions of existing first-order reinforcement learning algorithms. Thus, there remains a large gap between first-order reinforcement learning algorithms and stochastic optimization, which we now show how to bridge.\n82 True Stochastic Gradient Temporal Difference Learning Algorithms"
    }, {
      "heading" : "6.4 Algorithm Design",
      "text" : "In what follows, we build on the operator splitting methods introduced in Section 2.6.3, which should be reviewed before reading the section below."
    }, {
      "heading" : "6.4.1 NEU Objective Function",
      "text" : "The primal-dual formulation of the NEU defined in Section (6.2.1) is as follows:\nmin θ∈X\n( 1\n2 NEU(θ) + h(θ)\n) = min\nθ∈X max y\n( 〈ΦTΞ(R+ γΦ′θ − Φθ), y〉 − 1\n2 ||y||22 + h(θ) ) We have K(θ) = ΦTΞ(R + γΦ\n′ θ − Φθ) , and F (·) = 12 || · ||22 , thus the\nLegendre transform is F ∗(·) = F (·) = 12 || · ||22. Thus the update rule is\nyt+1 = yt + αt(δtφt − yt), θt+1 = proxαth ( θt + αt(φt − γφ′t)(yTt φt) ) Note that if h(θ) = 0 andX = Rd, then we will have the GTD algorithm proposed in [106]."
    }, {
      "heading" : "6.4.2 MSPBE Objective Function",
      "text" : "Based on the definition of MSPBE in Section (6.2.1), we can reformulate MSPBE as\nMSPBE(θ) = ||ΦTΞ(TVθ − Vθ)||2(ΦTΞΦ)−1\nThe gradient of MSPBE is correspondingly computed as\n−1 2 MSPBE(θ) = E[(φt − γφ ′ t)φ T t ]E[φtφTt ] −1E[δt(θ)φt]\nAs opposed to computing the NEU gradient, computing Equation (6.4.2) involves computing the inverse matrix E[φtφTt ] −1, which imposes extra difficulty. To this end, we propose another primal-dual splitting formulation with weighted Euclidean norm as follows,\nmin x∈X\n1 2 ||x||2M−1 = minx∈X maxw 〈x,w〉 − 1 2 ||w||2M\n6.4. Algorithm Design 83\nwhere M = ΦTΞΦ, and the dual variable is denoted as wt to differentiate it from yt used for the NEU objective function. Then we have\nmin θ∈X\n1 2 MSPBE(θ)+h(θ) = min θ∈X max w 〈ΦTΞ(R+ γΦ′θ − Φθ), w〉−1 2 ||w||2M+h(θ)\nNote that the nonlinear convex F (·) = 12 ||·||2M−1 , and thus the Legendre transform is F ∗(·) = 12 || · ||2M . We can see that by using the primal-dual splitting formulation, computing the inverse matrix M−1 is avoided. Thus the update rule is as follows:\nwt+1 = wt + αt(δt − φTt wt)φt, θt+1 = proxαth ( θt + αt(φt − γφ′t)(wTt φt) ) Note that if h(θ) = 0 and X = Rd, then we will have the GTD2 algorithm proposed in [26]. It is also worth noting that the TDC algorithm seems not to have an explicit proximal splitting representation, since it incorporates wt(θ) = E[φtφTt ] −1E[δt(θ)φt] into the update of θt, a quasistationary condition which is commonly used in two-time-scale stochastic approximation approaches. An intuitive answer to the advantage of TDC over GTD2 is that the TDC update of θt can be considered as incorporating the prior knowledge into the update rule: for a stationary θt, if the optimal wt(θt) (termed as w ∗ t (θt)) has a closed-form solution or is easy to compute, then incorporating this w∗t (θt) into the update rule tends to accelerate the algorithm’s convergence performance. For the GTD2 update in Equation (6.4.2), note that there is a sum of two terms where wt appears: which are (φt−γφ′t)(wTt φt) = φt(wTt φt)−γφ′t(wTt φt). Replacing wt in the first term with w ∗ t (θ) = E[φtφTt ]\n−1E[δt(θ)φt], we have the update rule as follows\nwt+1 = wt + αt(δt − φTt wt)φt , θt+1 = proxαth ( θt + αt(φt − γφ′t)(φTt wt) ) Note that if h(θ) = 0 and X = Rd, then we will have TDC algorithm proposed in [26]. Note that this technique does not have the same convergence guarantee as the original objective function. For example, if we use a similar trick on the GTD update with the optimal yt(θt) (termed as y∗t (θt)) where y ∗ t (θ) = E[δt(θ)φt], then we can have\nθt+1 = proxαth ( θt + αtδt(φt − γφ ′ t) )\n84 True Stochastic Gradient Temporal Difference Learning Algorithms\nwhich is the update rule of residual gradient [107], and is proven not to converge to NEU any more.1"
    }, {
      "heading" : "6.5 Accelerated Gradient Temporal Difference Learning Algorithms",
      "text" : "In this section we will discuss the acceleration of GTD2 and TDC. The acceleration of GTD is not discussed due to space consideration, which is similar to GTD2. A comprehensive overview of the convergence rate of different approaches to stochastic saddle-point problems is given in [108]. In this section we present accelerated algorithms based on the Stochastic Mirror-Prox (SMP) Algorithm [47, 109]. Algorithm 11, termed as GTD2-MP, is accelerated GTD2 with extragradient. Algorithm 12, termed as TDC-MP, is accelerated TDC with extragradient.\nAlgorithm 10 Algorithm Template Let π be some fixed policy of an MDP M , Φ be some fixed basis.\n1: repeat 2: Compute φt, φt ′ and TD error δt = rt + γφt\n′T θt − φTt θt 3: Compute θt+1, wt+1 according to each algorithm update rule 4: until t = N ;\n5: Compute primal average θ̄N = 1 N N∑ i=1 θi, w̄N = 1 N N∑ i=1 wi"
    }, {
      "heading" : "6.6 Theoretical Analysis",
      "text" : "In this section, we discuss the convergence rate and error bound of GTD, GTD2 and GTD2-MP."
    }, {
      "heading" : "6.6.1 Convergence Rate",
      "text" : "Proposition 1 The convergence rates of the GTD/GTD2 algorithms with primal average are O(LF∗+LK+σ√ N ), where LK = ||ΦTΞ(Φ −\n1 It converges to mean-square TD error (MSTDE), as proven in [75].\n6.6. Theoretical Analysis 85\nAlgorithm 11 GTD2-MP\n(1) wt+ 1 2 = wt + βt(δt − φTt wt)φt, θt+ 1\n2 = proxαth\n( θt + αt(φt − γφt′)(φTt wt) ) (2) δt+ 1\n2 = rt + γφt ′T θt+ 1 2 − φTt θt+ 1 2\n(3) wt+1 = wt + βt(δt+ 1\n2 − φTt wt+ 1 2 )φt ,\nθt+1 = proxαth ( θt + αt(φt − γφt′)(φTt wt+ 1 2 ) )\nAlgorithm 12 TDC-MP\n(1) wt+ 1 2 = wt + βt(δt − φTt wt)φt, θt+ 1\n2 = proxαth\n( θt + αtδtφt − αtγφt′(φTt wt) ) (2) δt+ 1\n2 = rt + γφt ′T θt+ 1 2 − φTt θt+ 1 2\n(3) wt+1 = wt + βt(δt+ 1\n2 − φTt wt+ 1 2 )φt ,\nθt+1 = proxαth ( θt + αtδt+ 1 2 φt − αtγφt′(φTt wt+ 1 2 ) )\nγΦ ′T )||2, for GTD, LF ∗ = 1 and for GTD2, LF ∗ = ||ΦTΞΦ||2, σ is defined in the Appendix due to space limitations.\nNow we consider the convergence rate of GTD2-MP. Proposition 2 The convergence rate of the GTD2-MP algorithm\nis O(LF∗+LKN + σ√ N ).\nSee supplementary materials for an abbreviated proof. Remark: The above propositions imply that when the noise level is low, the GTD2-MP algorithm is able to converge at the rate of O( 1N ), whereas the convergence rate of GTD2 is O( 1√ N ). However, when the noise level is high, both algorithms’ convergence rates reduce to O( σ√ N )."
    }, {
      "heading" : "6.6.2 Value Approximation Error Bound",
      "text" : "Proposition 3: For GTD/GTD2, the prediction error of ||V − Vθ|| is bounded by ||V − Vθ||∞ ≤ LΞφ 1−γ · O ( LF∗+LK+σ√\nN\n) ; For GTD2-MP, it\n86 True Stochastic Gradient Temporal Difference Learning Algorithms\nis bounded by ||V − Vθ||∞ ≤ LΞφ 1−γ · O\n( LF∗+LK\nN + σ√ N ) , where LΞφ =\nmaxs||(ΦTΞΦ)−1φ(s)||1. Proof : see Appendix."
    }, {
      "heading" : "6.6.3 Related Work",
      "text" : "Here we will discuss previous related work. To the best of our knowledge, the closest related work is the RO-TD algorithm, which first introduced the convex-concave saddle-point framework to regularize the TDC family of algorithms. 2 The major difference is that RO-TD is motivated by the linear inverse problem formulation of TDC algorithm and uses its dual norm representation as the objective function, which does not explore the auxiliary variable wt. In contrast, by introducing the operator splitting framework, we demonstrate that the GTD family of algorithms can be nicely explained as a “true” SGD approach, where the auxiliary variable wt has a nice explanation.\nAnother interesting question is whether ADMM is suitable for the operator splitting algorithm here. Let’s take NEU for example. The ADMM formulation is as follows, where we assume K(θ) = Kθ for simplicity, and other scenarios can be derived similarly,\nmin θ,z (F (z) + h(θ)) s.t.z = Kθ\nThe update rule is as follows, where αt is the stepsize\nθt+1 = arg min θ\n( h(θ) + 〈yt,Kθ − zt〉+ 12 ||Kθ − zt||2 ) zt+1 = arg min\nz\n( F (z) + 〈yt,Kθt+1 − z〉+ 12 ||Kθt+1 − z||2 ) yt+1 = yt + αt(Kθt+1 − zt+1)\nAt first glance the operator of F (·) and Kθ seem to be split, however, if we compute the closed-form update rule of θt, we can see that the update of θt includes (K TK)−1, which involves both biased-sampling and computing the inverse matrix, thus regular ADMM does not seem to be practical for this first-order reinforcement learning setting. However, using the pre-conditioning technique introduced in [110], ADMM\n2 Although only regularized TDC was proposed in [14], the algorithm can be easily extended to regularized GTD and GTD2.\n6.7. Experimental Study 87\ncan be reduced to the primal-dual splitting method as pointed out in [58]."
    }, {
      "heading" : "6.7 Experimental Study",
      "text" : ""
    }, {
      "heading" : "6.7.1 Off-Policy Convergence: Baird Example",
      "text" : "The Baird example is a well-known example where TD diverges and TDC converges. The stepsizes are set to be constants where βt = µαt as shown in Figure 6.1. From Figure 6.1, we can see that GTD2-MP and TDC-MP have a significant advantage over the GTD2 and TDC algorithms wherein both the MSPBE and the variance are substantially reduced."
    }, {
      "heading" : "6.7.2 Regularization Solution Path: Two-State Example",
      "text" : "Now we consider the two-state MDP in [111]. The transition matrix and reward vector are [0, 1; 0, 1] and R = [0,−1]T , γ = 0.9, and a one-feature basis Φ = [1, 2]T . The objective function are θ = arg min θ ( 1 2L(θ) + ρ||θ||1 ) , where L(θ) is NEU(θ) and MSPBE(θ). The objective functions are termed as l1-NEU and l1-MSPBE for short. In Figure 6.2, both l1-NEU and l1-MSPBE have well-defined solution\n88 True Stochastic Gradient Temporal Difference Learning Algorithms\npaths w.r.t ρ, whereas Lasso-TD may have multiple solutions if the P -matrix condition is not satisfied [62].\n6.7.3 On-Policy Performance: 400-State Random MDP\nIn this experiment we compare the on-policy performance of the four algorithms. We use the random generated MDP with 400 states and 10 actions in [112]. Each state is represented by a feature vector with 201 features, where 200 features are generated by sampling from a uniform distribution the 201-th feature is a constant. The stepsizes are set to be constants where βt = µαt as shown in Figure 6.3. The parameters of each algorithm are chosen via comparative studies similar to [112]. The result is shown in Figure 6.3. The results for each algorithm are averaged on 100 runs, and the parameters of each algorithm are chosen via experiments. TDC shows high variance and chattering effect of MSPBE curve on this domain. Compared with GTD2, GTD2-M1P is able to reduce the MSPBE significantly. Compared with TDC, TDC-MP not only reduces the MSPBE, but also the variance and the ”chattering” effect."
    }, {
      "heading" : "6.8 Summary",
      "text" : "This chapter shows that the GTD/GTD2 algorithms are true stochastic gradient methods w.r.t. the primal-dual formulation of their corresponding objective functions, which enables their convergence rate analysis and regularization. Second, it proposes operator splitting as a\n6.8. Summary 89\nbroad framework to solve the biased-sampling problem in reinforcement learning. Based on the unified primal-dual splitting framework, it also proposes accelerated algorithms with both rigorous theoretical analysis and illustrates their improved performance w.r.t. previous methods. Future research is ongoing to explore other operator splitting techniques beyond primal-dual splitting as well as incorporating random projections [113], and investigating kernelized algorithms [114, 115]. Finally, exploring the convergence rate of the TDC algorithm is also important and interesting.\n7 Variational Inequalities: The Emerging Frontier of Machine Learning\nThis paper describes a new framework for reinforcement learning based on primal dual spaces connected by a Legendre transform. The ensuing theory yields surprising and beautiful solutions to several important questions that have remained unresolved: (i) how to design reliable, convergent, and stable reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified “safety” guarantees, and remains in a stable region of the parameter space (iv) how to design “off-policy” TD-learning algorithms in a reliable and stable manner, and finally, (iii) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we gave detailed answers to all these questions using the powerful framework of proximal operators. The single most important idea that emerges is the use of primal dual spaces connected through the use of a Legendre transform. This allows temporal-difference updates to occur in dual spaces, allowing a variety of important technical advantages. The Legendre transform, as we show, elegantly generalizes past algorithms for solving reinforcement learning problems, such as natural gradient methods, which we show relate closely to the previously unconnected framework of mirror descent methods. Equally importantly, proximal\n90\n7.1. Variational Inequalities 91\noperator theory enables the systematic development of operator splitting methods that show how to safely and reliably decompose complex products of gradients that occur in recent variants of gradient-based temporal-difference learning. This key technical contribution makes it possible to finally show to design “true” stochastic gradient methods for reinforcement learning. Finally, Legendre transforms enable a variety of other benefits, including modeling sparsity and domain geometry. Our work builds extensively on recent work on the convergence of saddlepoint algorithms, and on the theory of monotone operators in Hilbert spaces, both in optimization and for variational inequalities. The latter represents possibly the most exciting future research direction, and we give a more detailed description of this ongoing research thrust."
    }, {
      "heading" : "7.1 Variational Inequalities",
      "text" : "Our discussion above has repeatedly revolved around the fringes of variational inequality theory. Methods like extragradient [8] and the mirror-prox algorithm were originally proposed to solve variational inequalities and related saddle point problems. We are currently engaged in redeveloping the proposed ideas more fully within the fabric of variational inequality (VI). Accordingly, we briefly describe the framework of VIs, and give the reader a brief tour of this fascinating extension of the basic underlying framework of optimization. We lack the space to do a thorough review. That is the topic of another monograph to be published at a later date, and several papers on this topic are already under way.\nAt the dawn of a new millennium, the Internet dominates our economic, intellectual and social lives. The concept of equilibrium plays a key role in understanding not only the Internet, but also other networked systems, such as human migration [116], evolutionary dynamics and the spread of infectious diseases [117], and social networks [118]. Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas. We are currently exploring two powerful mathematical tools for the study of equilibria – variational inequalities (VIs) and projected dynamical systems (PDS) [12, 122] – in developing a new machine learning framework for solving\n92 Variational Inequalities: The Emerging Frontier of Machine Learning\nequilibrium problems in a rich and diverse range of practical applications. As Figure 7.1 illustrates, finite-dimensional VIs provide a mathematical framework that unifies many disparate equilibrium problems of significant importance, including (convex) optimization, equilibrium problems in economics, game theory and networks, linear and nonlinear complementarity problems, and solutions of systems of nonlinear equations.\nVariational inequalities (VIs), in the infinite-dimensional setting, were originally proposed by Hartman and Stampacchia [10] in the mid1960s in the context of solving partial differential equations in mechanics. Finite-dimensional VIs rose in popularity in the 1980s partly as a result of work by Dafermos [11]. who showed that the traffic network equilibrium problem could be formulated as a finite-dimensional VI. This advance inspired much follow-on research, showing that a variety of equilibrium problems in economics, game theory, sequential decisionmaking etc. could also be formulated as finite-dimensional VIs – the\n7.1. Variational Inequalities 93\nbooks by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs. Projected dynamical systems (PDS) [122] are a class of ordinary differential equations (ODEs) with a discontinuous right-hand side. Associated with every finite-dimensional VI is a PDS, whose stationary points are the solutions of the VI. While VIs provide a static analysis of equilibria, PDS enable a microscopic examination of the dynamic processes that lead to or away from stable equilibria. . There has been longstanding interest in AI in the development of gradient-based learning algorithms for finding Nash equilibria in multiplayer games, e.g. [123, 119, 124]. A gradient method for finding Nash equilibria can be formalized by a set of ordinary differential equations, whose phase space portrait solution reveals the dynamical process of convergence to an equilibrium point, or lack thereof. A key complication in this type of analysis is that the classical dynamical systems approach does not allow incorporating constraints on values of variables, which are omnipresent in equilibria problems, not only in games, but also in many other applications in economics, network flow, traffic modeling etc. In contrast, the right-hand side of a PDS is a discontinuous projection operator that allows enabling constraints to be modeled.\nOne of the original algorithms for solving finite-dimensional VIs is the extragradient method proposed by Korpelevich [125]. It has been applied to structured prediction models in machine learning by Taskar et al. [126]. Bruckner et al. [127] use a modified extragradient method for solving the spam filtering problem modeled as a prediction game. We are developing a new family of extragradient-like methods based on well-known numerical methods for solving ordinary differential equations, specifically the Runge Kutta method [128]. In optimization, the extragradient algorithm was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called “mirrror-prox” algorithm [129, 130]. We have extended the mirror-prox method by combining it with Runge-Kutta methods for solving high-dimensional VI problems over the simplex and other spaces. We show the enhanced performance of Runge-Kutta extragradient methods on a range of benchmark variational inequalities drawn\n94 Variational Inequalities: The Emerging Frontier of Machine Learning\nfrom standard problems in the optimization literature."
    }, {
      "heading" : "7.1.1 Definition",
      "text" : "The formal definition of a VI as follows:1\nDefinition 7.1. The finite-dimensional variational inequality problem VI(F,K) involves finding a vector x∗ ∈ K ⊂ Rn such that\n〈F (x∗), x− x∗〉 ≥ 0, ∀x ∈ K where F : K → Rn is a given continuous function and K is a given\nclosed convex set, and 〈., .〉 is the standard inner product in Rn.\nFigure 7.2 provides a geometric interpretation of a variational inequality. 2 The following general result characterizes when solutions to VIs exist:\n1 Variational problems can be defined more abstractly in Hilbert spaces. We confine our\ndiscussion to n-dimensional Euclidean spaces. 2 In Figure 7.2, the normal cone C(x∗) at the vector x∗ of a convex set K is defined as C(x∗) = {y ∈ Rn|〈y, x− x∗〉 ≤ 0,∀x ∈ K}.\n7.1. Variational Inequalities 95\nTheorem 7.1. Suppose K is compact, and that F : K → Rn is continuous. Then, there exists a solution to VI(F,K).\nAs Figure 7.2 shows, x∗ is a solution to V I(F,K) if and only if the angle between the vectors F (x∗) and x − x∗, for any vector x ∈ K, is less than or equal to 900. To build up some intuition, the reduction of a few well-known problems to a VI is now provided.\nTheorem 7.2. Let x∗ be a solution to the optimization problem of minimizing a continuously differentiable function f(x), subject to x ∈ K, where K is a closed and convex set. Then, x∗ is a solution to V I(∇f,K), such that 〈∇f(x∗), x− x∗〉 ≥ 0, ∀x ∈ K.\nProof: Define φ(t) = f(x∗ + t(x− x∗)). Since φ(t) is minimized at t = 0, it follows that 0 ≤ φ′(0) = 〈∇f(x∗), x − x∗〉 ≥ 0, ∀x ∈ K, that is x∗ solves the VI.\nTheorem 7.3. If f(x) is a convex function, and x∗ is the solution of V I(∇f,K), then x∗ minimizes f .\nProof: Since f is convex, it follows that any tangent lies below the function, that is f(x) ≥ f(x∗) + 〈∇f(x∗), x− x∗〉, ∀x ∈ K. But, since x∗ solves the VI, it follows that f(x∗) is a lower bound on the value of f(x) everywhere, or that x∗ minimizes f .\nA rich class of problems called complementarity problems (CPs) also can be reduced to solving a VI. When the feasible set K is a cone, meaning that if x ∈ K, then αx ∈ K,α ≥ 0, then the VI becomes a CP.\nDefinition 7.2. Given a cone K ⊂ Rn, and a mapping F : K → Rn, the complementarity problem CP(F,K) is to find an x ∈ K such that F (x) ∈ K∗, the dual cone to K, and 〈x, F (x)〉 ≥ 0. 3\n3 Given a cone K, the dual cone K∗ is defined as K∗ = {y ∈ Rn|〈y, x〉 ≥ 0, ∀x ∈ K}.\n96 Variational Inequalities: The Emerging Frontier of Machine Learning\nA number of special cases of CPs are important. The nonlinear complementarity problem (NCP) is to find x∗ ∈ Rn+ (the non-negative orthant) such that F (x∗) ≥ 0 and 〈F (x∗), x∗〉 = 0. The solution to an NCP and the corresponding V I(F,Rn+) are the same, showing that NCPs reduce to VIs. In an NCP, whenever the mapping function F is affine, that is F (x) = Mx + b, where M is an n × n matrix, then the corresponding NCP is called a linear complementarity problem (LCP) [131]. Recent work on learning sparse models using L1 regularization has exploited the fact that the standard LASSO objective [132] of L1 penalized regression can be reduced to solving an LCP [133]. This reduction to LCP has been used in recent work on sparse value function approximation as well in a method called LCP-TD [134]. A final crucial property of VIs is that they can be formulated as finding fixed points.\nTheorem 7.4. The vector x∗ is the solution of VI(F,K) if and only if, for any γ > 0, x∗ is also a fixed point of the map x∗ = ΠK(x ∗−γF (x∗)), where ΠK is the projector onto convex set K.\nIn terms of the geometric picture of a VI illustrated in Figure 7.2. this property means that the solution of a VI occurs at a vector x∗ where the vector field F (x∗) induced by F on K is normal to the boundary of K and directed inwards, so that the projection of x∗ − γF (x∗) is the vector x∗ itself. This property forms the basis for the projection class of methods that solve for the fixed point."
    }, {
      "heading" : "7.1.2 Equilibrium Problems in Game Theory",
      "text" : "The VI framework provides a mathematically elegant approach to model equilibrium problems in game theory [119, 120]. A Nash game consists of m players, where player i chooses a strategy xi belonging to a closed convex set Xi ⊂ Rn. After executing the joint action, each player is penalized (or rewarded) by the amount Fi(x1, . . . , xm), where Fi : Rni → R is a continuously differentiable function. A set of strategies x∗ = (x∗1, . . . , x ∗ m) ∈ ∏M i=1Xi is said to be in equilibrium if no player can reduce the incurred penalty (or increase the incurred reward) by unilaterally deviating from the chosen strategy. If each Fi is convex\n7.2. Algorithms for Variational Inequalities 97\non the set Xi, then the set of strategies x ∗ is in equilibrium if and only if 〈(xi − x∗i ),∇iFi(x∗i )〉 ≥ 0. In other words, x∗ needs to be a solution of the VI 〈(x − x∗), f(x∗)〉 ≥ 0, where f(x) = (∇F1(x), . . . ,∇Fm(x)). Nash games are closely related to saddle point problems [129, 130, 135]. where we are given a function F : X × Y → R, and the objective is to find a solution (x∗, y∗) ∈ X × Y such that\nF (x∗, y) ≤ F (x∗, y∗) ≤ F (x, y∗), ∀x ∈ X, ∀y ∈ Y\nHere, F is convex in x for each fixed y, and concave in y for each fixed x. Many equilibria problems in economics can be modeled using VIs [12]."
    }, {
      "heading" : "7.2 Algorithms for Variational Inequalities",
      "text" : "We briefly describe two algorithms for solving variational inequalities below: the projection method and the extragradient method. We conclude with a brief discussion of how these relate to reinforcement learning."
    }, {
      "heading" : "7.2.1 Projection-Based Algorithms for VIs",
      "text" : "The basic projection-based method (Algorithm 1) for solving VIs is based on Theorem 7.4 introduced earlier.\nAlgorithm 13 The Basic Projection Algorithm for solving VIs. INPUT: Given VI(F,K), and a symmetric positive definite matrix D.\n1: Set k = 0 and xk ∈ K. 2: repeat 3: Set xk+1 ← ΠK,D(xk −D−1F (xk)). 4: Set k ← k + 1. 5: until xk = ΠK,D(xk −D−1F (xk)). 6: Return xk\nHere, ΠK,D is the projector onto convex set K with respect to the natural norm induced by D, where ‖x‖2D = 〈x,Dx〉. It can be shown that the basic projection algorithm solves any V I(F,K) for which the\n98 Variational Inequalities: The Emerging Frontier of Machine Learning\nmapping F is strongly monotone 4 and Lipschitz.5A simple strategy is to set D = αI, where α > L 2\n2µ , and L is the Lipschitz smooth-\nness constant, and µ is the strong monotonicity constant. The basic projection-based algorithm has two critical limitations: it requires that the mapping F be strongly monotone. If, for example, F is the gradient map of a continuously differentiable function, strong monotonicity implies the function must be strongly convex. Second, setting the parameter α requires knowing the Lipschitz smoothness L and the strong monotonicity parameter µ. The extragradient method of Korpolevich [125] addresses some of these concerns, and is defined as Algorithm 2 below.\nAlgorithm 14 The Extragradient Algorithm for solving VIs. INPUT: Given VI(F,K), and a scalar α.\n1: Set k = 0 and xk ∈ K. 2: repeat 3: Set yk ← ΠK(xk − αF (xk)). 4: Set xk+1 ← ΠK(xk − αF (yk)). 5: Set k ← k + 1. 6: until xk = ΠK(xk − αF (xk)). 7: Return xk\nFigure 7.3 shows a simple example where Algorithm 1 fails to converge, but Algorithm 2 does. If the initial point x0 is chosen to be on the boundary of X, using Algorithm 1, it stays on it and fails to converge to the solution of this VI (which is at the origin). If x0 is chosen to be in the interior of K, Algorithm 1 will move towards the boundary. In contrast, using Algorithm 2, the solution can be found for any starting point. The extragradient algoriithm derives its name from the property that it requires an “extra gradient” step (step 4 in Algorithm 2), unlike the basic projection algorithm given earlier as Algorithm 1. The principal advantage of the extragradient method is that it can be shown to converge under a considerably weaker condition on the mapping F ,\n4 A mapping F is strongly monotone if 〈F (x)−F (y), x− y〉 ≥ µ‖x− y‖22, µ > 0, ∀x, y ∈ K. 5 A mapping F is Lipschitz if ‖F (x)− F (y)‖2 ≤ L‖x− y‖2,∀x, y ∈ K.\n7.2. Algorithms for Variational Inequalities 99\nwhich now has to be merely monotonic: 〈F (x)− F (y), x− y〉 ≥ 0. The earlier Lipschitz condition is still necessary for convergence.\nThe extragradient algorithm has been the topic of much attention in optimization since it was proposed, e.g., see [137, 138, 139, 140, 141, 142]. Khobotov [138] proved that the extragradient method converges under the weaker requirement of pseudo-monotone mappings, 6 when the learning rate is automatically adjusted based on a local measure of the Lipschitz constant. Iusem [137] proposed a variant whereby the current iterate was projected onto a hyperplane separating the current iterate from the final solution, and subsequently projected from the hyperplane onto the feasible set. Solodov and Svaiter [142] proposed another hyperplane method, whereby the current iterate is projected onto the intersection of the hyperplane and the feasible set. Finally, the extragradient method was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called “mirrror-prox” algorithm [129].\n6 A mapping F is pseudo-monotone if 〈F (y), x− y〉 ≥ 0⇒ 〈F (x), x− y〉 ≥ 0, ∀x, y ∈ K.\n100 Variational Inequalities: The Emerging Frontier of Machine Learning"
    }, {
      "heading" : "7.2.2 Variational Inequaities and Reinforcement Learning",
      "text" : "Variational inequalities also provide a useful framework for reinforcement learning [32, 1]. In this case, it can be shown that the mapping F for the VI defined by reinforcement learning is affine and represents a (linear) complementarity problem. For this case, a number of special properties can be exploited in designing a faster more scalable class of algorithms. Recall from Theorem 7.4 that each VI(F,K) corresponds to solving a particular fixed point problem x∗ = ΠK(x ∗− γF (x∗)), which led to the projection algorithm (Algorithm 1). Generalizing this, consider solving for the fixed point of a projected equation x∗ = ΠŜT (x ∗) [143, 144] for a functional mapping T : Rn → Rn, where ΠŜ is the projector onto a low-dimensional convex subspace Ŝ w.r.t. some positive definite matrix Ξ, so that\nŜ = {Φr|r ∈ R̂}, R̂ = {r|Φr ∈ Ŝ} ⇒ Φr∗ = ΠŜT (Φr∗)\nHere. Φ is an n × s matrix where s n, and the goal is to make the computation depend on s, not n. Note that x∗ = ΠŜT (x ∗) if and only if 〈(x∗ − T (x∗),Ξ(x− x∗)〉 ≥ 0. ∀x ∈ Ŝ Following [143], note that this is a variational inequality of the form 〈F (x∗), (x−x∗)〉 ≥ 0 if we identify F (x) = Ξ(x−T (x)), and in the lowerdimensional space, 〈F (Φr∗),Φ(r− r∗)〉, ∀r ∈ R̂. Hence, the projection algorithm takes on the form:\nxk+1 = ΠŜ(xk − γD−1〈Φ, F (Φxk))〉\nIt is shown in [143] that if T is a contraction mapping, then F (x) = Ξ(x − T (x)) is strongly monotone. Hence, the above projection algorithm will converge to the solution x∗ of the VI for any given starting point x0 ∈ Ŝ. Now, the only problem is how to ensure the computation depends only on the size of the projected lower-dimensional space (i.e., s, not n). To achieve this, let us assume that the mapping T (x) = Ax+b is affine, and that the constraint region R̂ is polyhedral. In this case, we can use the following identities:\n〈Φ, F (Φ, x)〉 = ΦTΞF (Φx) = Cr − d, C = ΦTΞ(I −A)Φ, d = ΦTΞb\n7.2. Algorithms for Variational Inequalities 101\nand the projection algorithm for this affine case can be written as:\nxk+1 = ΠŜ(xk − γD−1(Cr − d))\nOne way to solve this iteratively is to compute a progressively more accurate approximation Ck → C and dk → d by sampling from the rows and columns of the matrix A and vector b, as follows:\nCk = 1\nk + 1 k∑ t=0 φ(it)(φ(it)− aitjt pitjt φ(jt)) T , dk = 1 k + 1 k∑ t=0 φ(it)bit\nwhere the row sampling generates the indices (i0, i1, . . .) and the column sampling generates the transitions ((i0, j0), (i1, j1), . . . , ) in such a way that the relative frequency of row index i matches the diagonal element ξi of the positive definite matrix Ξ. Given Ck and dk, the solution can be found by x∗ ≈ C−1k dk, or by using an incremental method. Computation now only depends on the dimension s of the lower-dimensional space, not on the original high-dimensional space. Gordon [144] proposes an alternative approach separating the projection of the current iterate on the low-dimensional subspace spanned by Φ from its projection onto the feasible set. Both of these approaches [143, 144] have been only studied with the simple projection method (Algorithm 1), and can be generalized to a more powerful class of VI methods that we are currently developing."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We like to acknowledge the useful feedback of past and present members of the Autonomous Learning Laboratory at the University of Massachusetts, Amherst. Principal funding for this research was provided by the National Science Foundation under the grant NSF IIS-1216467. Past work by the first author has been funded by NSF grants IIS0534999 and IIS-0803288.\n8"
    }, {
      "heading" : "Appendix: Technical Proofs",
      "text" : ""
    }, {
      "heading" : "8.1 Convergence Analysis of Saddle Point Temporal Difference Learning",
      "text" : "Proof of Proposition 1\nWe give a descriptive proof here. We first present the monotone operator corresponding to the bilinear saddle-point problem and then extend it to stochastic approximation case with certain restrictive assumptions, and use the result in [47].\nThe monotone operator Φ(x, y) with saddle-point problem\nSadV al = infx∈Xsupy∈Y φ(x, y) is a point-to-set operator\nΦ(x, y) = {∂xφ(x, y)} × {−∂yφ(x, y)}\nWhere ∂xφ(x, y) is the subgradient of φ(x, ·) over x and ∂yφ(x, y) is the subgradient of φ(·, y) over y. For the bilinear problem in Equation (4.1.2), the corresponding Φ(x, y) is\nΦ(x, y) = (AT y, b−Ax)\nNow we verify that the problem (4.2.1) can be reduced to a standard bilinear minimax problem. To prove this we only need to prove X in\n102\n8.2. Convergence Analysis of True Gradient Temporal Difference Learning 103\nour RL problem is indeed a closed compact convex set. This is easy to verify as we can simply define X = {x|‖x‖2 ≤ R} where R is large enough. In fact, the sparse regularization h(x) = ρ||x||1 helps xt stay within this l2 ball. Now we extend to the stochastic approximation case wherein the objective function f(x) is given by the stochastic oracle, and in our case, it is Ax− b, where A, b are defined in Equation (4.2.1), with Assumption 3 and further assuming that the noise εt for t-th sample is i.i.d noise, then with the result in [109], we can prove that the RO-TD algorithm converges to the global minimizer of\nx∗ = arg min x∈X ‖Ax− b‖m + ρ‖x‖1\nThen we prove the error level of approximate saddle-point x̄t, ȳt defined in (4.3.1) is αtL 2. With the subgradient boundedness assumption and using the result in Proposition 1 in [78], this can be proved."
    }, {
      "heading" : "8.2 Convergence Analysis of True Gradient Temporal Difference Learning",
      "text" : "We first present the assumptions for the MDP and basis functions, which are similar to [26, 14].\nAssumption 1 (MDP): The underlying Markov Reward Process (MRP) M = (S, P,R, γ) is finite and mixing, with stationary distribution π. The training sequence (st, at, s ′ t) is an i.i.d sequence.\nAssumption 2 (Basis Function): The inverses E[φtφTt ]−1 and [φt(φt − γφt ′T )]−1 exist. This implies that Φ is a full column rank matrix. Also, assume the features (φt, φ ′ t) have uniformly bounded second moments, and ‖φt‖∞ < +∞, ‖φ′t‖∞ < +∞. Next we present the assumptions for the stochastic saddle point problem formulation, which are similar to [108, 109].\nAssumption 3 (Compactness): Assume for the primal-dual loss\nfunction,\nmin θ∈X max y∈Y\n(L(θ, y) = 〈K(θ), y〉 − F ∗(y) + h(θ)) ,\nthe sets X,Y are closed compact sets.\nAssumption 4 (F ∗(·)): We assume that F ∗(·) is a smooth convex function with Lipschitz continuous gradient, i.e., ∃LF ∗ such that ∀x, y ∈\n104 Appendix: Technical Proofs\nX\nF ∗(y)− F ∗(x)− 〈∇F ∗(x), y − x〉 ≤ LF ∗ 2 ||y − x||2\nAssumption 5 (K(θ)): K(θ) is a linear mapping which can be extended to Lipschitz continuous vector-valued mapping defined on a closed convex cone CK . Assuming K(θ) to be CK-convex, i.e., ∀θ, θ′ ∈ X,λ ∈ [0, 1],\nK(λθ + (1− λ)θ′)≤CKλK(θ) + (1− λ)K(θ′),\nwhere a≤CK b means that b− a ∈ CK . Assumption 6 (Stochastic Gradient): In the stochastic saddle point problem, we assume that there exists a stochastic oracle SO that is able to provide unbiased estimation with bounded variance such that\nE[F∗(yt)] = ∇F ∗(yt) E[||F∗(yt)−∇F ∗(yt)||2] ≤ σ2F ∗ E[Kθ(θt)] = K(θt) E[||Kθ(θt)−K(θt)||2] ≤ σ2K,θ\nE[Ky(θt)T yt] = ∇K(θt)T yt E[||Ky(θt)T yt −∇K(θt)T yt||2] ≤ σ2K,y\nwhere σF ∗ , σK,θ and σK,y are non-negative constants. We further\ndefine\nσ = √ σ2F ∗ + σ 2 K,θ + σK,y"
    }, {
      "heading" : "8.2.1 Convergence Rate",
      "text" : "Here we discuss the convergence rate of the proposed algorithms. First let us review the nonlinear primal form\nmin θ∈X (Ψ(θ) = F (K(θ)) + h(θ))\nThe corresponding primal-dual formulation [57, 28, 58] of Equation (8.2.1) is Equation (2.6.3). Thus we have the general update rule as\nyt+1 = yt + αtKθ(θt)− αtF∗(yt), θt+1 = proxαth(θt − αtKy(θt)T yt)\n8.2. Convergence Analysis of True Gradient Temporal Difference Learning 105\nLemma 1 (Optimal Convergence Rate): The optimal conver-\ngence rate of 2.6.3 is O(LF∗ N2 + LKN + σ√ N ).\nProof: Equation (8.2.1) can be easily converted to the following\nprimal-dual formulation\nmin y∈Y max θ∈X\n(〈−K(θ), y〉+ F ∗(y)− h(θ))\nUsing the bounds proved in [145, 109, 108], the optimal convergence rate of stochastic saddle-point problem is O(LF∗ N2 + LKN + σ√ N ).\nThe GTD/GTD2 algorithms can be considered as using Polyak’s algorithm without the primal average step. Hence, by adding the primal average step, GTD/GTD2 algorithms will become standard Polyak’s algorithms [146], and thus the convergence rates are O(LF∗+LK+σ√ N ) according to [49]. So we have the following propositions.\nProposition 1 The convergence rates of GTD/GTD2 algorithms\nwith primal average are O(LF∗+LK+σ√ N ), where LK = ||ΦTΞ(Φ − γΦ ′T )||2, for GTD, LF ∗ = 1 and for GTD2, LF ∗ = ||M ||2. Now we consider the acceleration using the SMP algorithm, which incorporates the extragradient term. According to [109] which extends the SMP algorithm to solving saddle-point problems and variational inequality problems, the convergence rate is accelerated to O(LF∗+LKN + σ√ N ). Consequently,\nProposition 2 The convergence rate of the GTD2-MP algorithm\nis O(LF∗+LKN + σ√ N )."
    }, {
      "heading" : "8.2.2 Value Approximation Error Bound",
      "text" : "One key question is how to give the error bound of ||V −Vθ|| given that of ||K(θ)||. Here we use the result in [111], which is similar to the one in [147].\nLemma 2 [111]: For any Vθ = Φθ, the following component-wise\nequality holds\nV − Vθ = (I − γΠΞP )−1 (( V −ΠΞV ) + Φ(ΦTΞΦ)−1K(θ) ) Proof :\n106 Appendix: Technical Proofs\nUse the equality V = TV and Vθ = Π ΞVθ, where the first equality is the Bellman equation, and the second is that Vθ lies within the spanning space of Φ.\nwe have\nV −ΠΞV = V −ΠΞTV + (Vθ −ΠΞTVθ)− (Vθ −ΠΞTVθ) = (I − γΠΞP )(V − Vθ) + ΠΞ(Vθ − TVθ)\nAfter rearranging the equation, we have Equation (8.2.2) and find that\nΠΞ(Vθ − TVθ) = −Φ(ΦTΞΦ)−1K(θ) Proposition 3: For GTD/GTD2, the prediction error of ||V − Vθ||\nis bounded by ||V − Vθ||∞ ≤ LΞφ 1−γ · O\n( LF∗+LK+σ√\nN\n) ; For GTD2-MP,\nit is bounded by ||V − Vθ||∞ ≤ LΞφ 1−γ · O\n( LF∗+LK\nN + σ√ N ) , where LΞφ =\nmaxs||(ΦTΞΦ)−1φ(s)||1. Proof:\nFrom Lemma 2, we have ||V − Vθ||∞ ≤ ||(I − γΠΞP )−1||∞ · ( ||V −ΠΞV ||∞ + LΞφ ||K(θ)||∞ ) Using the results in Proposition 1 and Proposition 2, we have for GTD and GTD2,\n||V−Vθ||∞ ≤ ||(I − γΠΞP )−1||∞· ( ||V −ΠΞV ||∞ + LΞφ ·O ( LF ∗ + LK + σ√\nN(1− γ) )) For GTD2-MP,\n||V−Vθ||∞ ≤ ||(I − γΠΞP )−1||∞· ( ||V −ΠΞV ||∞ + LΞφ ·O(\nLF ∗ + LK N + σ√ N ) ) If we further assume a rich expressive hypothesis space H, i.e., ΠΞP = P,ΠΞR = R, ||V − ΠΞV ||∞ = 0, ||(I − γΠΞP )−1||∞ = 11−γ , then for GTD and GTD2, we have\n||V − Vθ||∞ ≤ LΞφ 1− γ ·O ( LF ∗ + LK + σ√\nN ) For GTD2-MP, we have\n||V − Vθ||∞ ≤ LΞφ 1− γ ·O ( LF ∗ + LK N + σ√ N )"
    } ],
    "references" : [ {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "Richard S. Sutton", "Hamid Reza Maei", "Doina Precup", "Shalabh Bhatnagar", "David Silver", "Csaba Szepesvri", "Eric Wiewiora" ],
      "venue" : "Proceedings of the 26th International Conference on Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming",
      "author" : [ "L. Bregman" ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics, 7:200–217,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Problem Complexity and Method Efficiency in Optimization",
      "author" : [ "A. Nemirovksi", "D. Yudin" ],
      "venue" : "John Wiley Press,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "Operations Research Letters, Jan",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems",
      "author" : [ "A. Nemirovski" ],
      "venue" : "SIAM Journal on Optimization, 15(1):229–251,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Korpelevich. The extragradient method for finding saddle points and other problems",
      "author" : [ "M. G" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1976
    }, {
      "title" : "On some nonlinear elliptic differential functional equations",
      "author" : [ "P. Hartman", "G. Stampacchia" ],
      "venue" : "Acta Mathematica, 115:271–310,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "Traffic equilibria and variational inequalities",
      "author" : [ "S. Dafermos" ],
      "venue" : "Transportation Science, 14:42–54,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Network Economics: A Variational Inequality Approach",
      "author" : [ "A. Nagurney" ],
      "venue" : "Kluwer Academic Press,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Finite-Dimensional Variational Inequalities and Complimentarity Problems",
      "author" : [ "F. Facchinei", "Pang J" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2003
    }, {
      "title" : "Regularized off-policy TDlearning",
      "author" : [ "B. Liu", "S. Mahadevan", "J. Liu" ],
      "venue" : "Advances in Neural Information Processing Systems 25, pages 845–853,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The ordered subsets mirror descent optimization method with applications to tomography",
      "author" : [ "A. Ben-Tal", "T. Margalit", "A. Nemirovski" ],
      "venue" : "SIAM Journal of Optimization, Jan",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The robustness of the p-norm algorithms",
      "author" : [ "Claudio Gentile" ],
      "venue" : "ISSN 0885-6125. doi: 10. 1023/A:1026319107706. URL http://dl.acm.org/citation. cfm?id=948445.948447",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Proximal splitting methods in signal processing",
      "author" : [ "P. Combetes", "J.C. Pesquel" ],
      "venue" : "Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the numerical solution of heat conduction problems in two and three space variables",
      "author" : [ "J. Douglas", "H. Rachford" ],
      "venue" : "Transactions of the American Mathematical Society, 82:421–439,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1956
    }, {
      "title" : "Convergence Analysis of True Gradient Temporal Difference Learning 109 tributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein. Dis" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "Whole-Body Strategies for Mobility and Manipulation",
      "author" : [ "P. Deegan" ],
      "venue" : "PhD thesis, University of Massachusetts Amherst,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Dexterous mobility with the uBot-5 mobile manipulator",
      "author" : [ "S.R. Kuindersma", "E. Hannigan", "D. Ruiken", "R.A. Grupen" ],
      "venue" : "Proceedings of the 14th International Conference on Advanced Robotics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Why natural gradient",
      "author" : [ "S. Amari", "S. Douglas" ],
      "venue" : "In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning",
      "author" : [ "S. Bradtke", "A. Barto" ],
      "venue" : "Machine Learning, 22:33–57,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Least-squares policy evaluation algorithms with linear function approximation",
      "author" : [ "A. Nedic", "D. Bertsekas" ],
      "venue" : "Discrete Event Systems Journal, 13,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "M. Lagoudakis", "R. Parr" ],
      "venue" : "Journal of Machine Learning Research, 4:1107–1149,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvári", "E. Wiewiora" ],
      "venue" : "International Conference on Machine Learning, pages 993–1000,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Algorithms for reinforcement learning",
      "author" : [ "C. Szepesvári" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning, 4(1): 1–103,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Proximal splitting methods in signal processing",
      "author" : [ "P. L Combettes", "J. C Pesquet" ],
      "venue" : "Fixed-point algorithms for inverse problems in science and engineering, pages 185–212.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Markov Decision Processes",
      "author" : [ "M.L. Puterman" ],
      "venue" : "Wiley Interscience, New York, USA,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning, 3:9–44,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Learning from Delayed Rewards",
      "author" : [ "C. Watkins" ],
      "venue" : "PhD thesis, King’s  110 Appendix: Technical Proofs College, Cambridge, England,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Neuro-Dynamic Programming",
      "author" : [ "D. Bertsekas", "J. Tsitsiklis" ],
      "venue" : "Athena Scientific, Belmont, Massachusetts,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Learning representation and control in Markov Decision Processes: new frontiers",
      "author" : [ "S. Mahadevan" ],
      "venue" : "Foundations and Trends in Machine Learning, 1(4):403–565,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An optimal method for stochastic composite optimization",
      "author" : [ "G. Lan" ],
      "venue" : "Mathematical Programming, 133(1-2):365–397,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "Operations Research Letters, 31:167–175,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The robustness of the p-norm algorithms",
      "author" : [ "C. Gentile" ],
      "venue" : "Machine Learning, 53(3):265–299,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Stochastic methods for l1 regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "Journal of Machine Learning Research, pages 1865–1892, June",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Dual averaging methods for regularized stochastic learning and online optimization",
      "author" : [ "L. Xiao" ],
      "venue" : "The Journal of Machine Learning Research, 11:2543–2596,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Composite objective mirror descent",
      "author" : [ "J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari" ],
      "venue" : "COLT, pages 14–26,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A voted regularized dual averaging method for large-scale discriminative training in natural language processing",
      "author" : [ "J. Gao", "T. Xu", "L. Xiao", "X. He" ],
      "venue" : "Technical report, Microsoft Research,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization",
      "author" : [ "H.B. McMahan" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, pages 525–533,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sparse online learning via truncated gradient",
      "author" : [ "J. Langford", "L. Li", "T. Zhang" ],
      "venue" : "The Journal of Machine Learning Research, 10:777–801,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Stochastic methods for l1 regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, pages 929–936,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Manifold identification in dual averaging 8.2. Convergence Analysis of True Gradient Temporal Difference Learning 111 for regularized stochastic online learning",
      "author" : [ "S. Lee", "S.J. Wright" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2012
    }, {
      "title" : "On accelerated proximal gradient methods for convexconcave optimization",
      "author" : [ "P. Tseng" ],
      "venue" : "submitted to SIAM Journal on Optimization,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Convex Analysis",
      "author" : [ "R Rockafellar" ],
      "venue" : "Princeton University Press,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Optimization for Machine Learning, chapter First-Order Methods for Nonsmooth Convex Large- Scale Optimization",
      "author" : [ "A. Juditsky", "A. Nemirovski" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Non-Euclidean restricted memory level method for large-scale convex optimization",
      "author" : [ "A. Ben-Tal", "A. Nemirovski" ],
      "venue" : "Mathematical Programming, 102(3):407–456,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization, 19:1574–1609,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Monotone operators and the proximal point algorithm",
      "author" : [ "R. Rockafellar" ],
      "venue" : "SIAM Journal of Optimization, 14(5):877–898,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Functions convexes duales et points proximaux dans un espace hilbertien",
      "author" : [ "J. Moreau" ],
      "venue" : "Reports of the Paris Academy of Sciences, Series A, 255:2897–2899,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "Monotone operators and the proximal point algorithm",
      "author" : [ "R. T Rockafellar" ],
      "venue" : "SIAM Journal on Control and Optimization, 14(5): 877–898,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Proximal algorithms",
      "author" : [ "N. Parikh", "S. Boyd" ],
      "venue" : "Foundations and Trends in optimization, 1(3):123–231,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient learning using forward-backward splitting",
      "author" : [ "J. Duchi", "Y. Singer" ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sparse Reinforcement Learning via Convex Optimization",
      "author" : [ "Z. Qin", "W. Li" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Convex analysis and mono-  112 Appendix: Technical Proofs tone operator theory in Hilbert spaces",
      "author" : [ "H. H Bauschke", "P. L Combettes" ],
      "venue" : "Springer,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A first-order primal-dual algorithm for convex problems with applications to imaging",
      "author" : [ "A. Chambolle", "T. Pock" ],
      "venue" : "Journal of Mathematical Imaging and Vision, 40(1):120–145,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "S. Amari" ],
      "venue" : "Neural Computation, 10:251–276,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Finite- Sample Analysis of Lasso-TD",
      "author" : [ "M. Ghavamzadeh", "A. Lazaric", "R. Munos", "M. Hoffman" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Representation policy iteration",
      "author" : [ "S. Mahadevan" ],
      "venue" : "Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence (UAI-05), pages 372–37. AUAI Press,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Regularization and feature selection in least-squares temporal difference learning",
      "author" : [ "J.Z. Kolter", "A.Y. Ng" ],
      "venue" : "Proceedings of the 26th annual international conference on machine learning, pages 521–528,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Linear complementarity for regularized policy evaluation and improvement",
      "author" : [ "J. Johns", "C. Painter-Wakefield", "R. Parr" ],
      "venue" : "Proceedings of the International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exponentiated gradient versus gradient descent for linear predictors",
      "author" : [ "J. Kivinen", "M.K. Warmuth" ],
      "venue" : "Information and Computation, 132,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm",
      "author" : [ "N. Littlestone" ],
      "venue" : "Machine Learning, pages 285–318,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Exponentiated gradient methods for reinforcement learning",
      "author" : [ "D. Precup", "R.S. Sutton" ],
      "venue" : "ICML, pages 272–277,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Regularization and feature selection in least-squares temporal difference learning",
      "author" : [ "J. Zico Kolter", "Andrew Y. Ng" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 2009
    }, {
      "title" : "Proto-Value Functions: A Laplacian framework for learning representation and control in Markov Decision Processes",
      "author" : [ "S. Mahadevan", "M. Maggioni" ],
      "venue" : "Journal of Machine Learning Research, 8: 2169–2231,",
      "citeRegEx" : "69",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Value function approximation in reinforcement learning using the fourier basis",
      "author" : [ "G. Konidaris", "S. Osentoski", "PS Thomas" ],
      "venue" : "Computer Science Department Faculty Publication Series, page 101,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Online learning control by association and reinforcement",
      "author" : [ "J. Si", "Y. Wang" ],
      "venue" : "IEEE Transactions on Neural Networks, 12:264– 276,",
      "citeRegEx" : "71",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A Nemirovski", "A Juditsky", "G Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization, 14(4):1574–1609,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the worst-case analysis of temporal-difference learning algorithms",
      "author" : [ "Robert Schapire", "Manfred K. Warmuth" ],
      "venue" : "In Machine Learning,",
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 1994
    }, {
      "title" : "Stochastic Approximation: A Dynamical Systems Viewpoint",
      "author" : [ "V. Borkar" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "74",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Gradient temporal-difference learning algorithms",
      "author" : [ "H.R. Maei" ],
      "venue" : "PhD thesis, University of Alberta,",
      "citeRegEx" : "75",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Linear off-policy actorcritic",
      "author" : [ "T. Degris", "M. White", "R.S. Sutton" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "76",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Y. Nesterov" ],
      "venue" : "www.optimization-online.org,",
      "citeRegEx" : "77",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Subgradient methods for saddle-point problems",
      "author" : [ "A. Nedic", "A. Ozdaglar" ],
      "venue" : "Journal of optimization theory and applications, 142 (1):205–228,",
      "citeRegEx" : "78",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "GQ (λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
      "author" : [ "H.R. Maei", "R.S. Sutton" ],
      "venue" : "Proceedings of the Third Conference on Artificial General Intelligence, pages 91–96,",
      "citeRegEx" : "79",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Value function approximation in reinforcement learning using the fourier basis",
      "author" : [ "G. Konidaris", "S. Osentoski", "P.S. Thomas" ],
      "venue" : "Proceedings of the Twenty-Fifth Conference on Artificial In-  114 Appendix: Technical Proofs telligence,",
      "citeRegEx" : "80",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "PID Controllers: Theory, Design, and Tuning",
      "author" : [ "K.J. Åström", "T. Hägglund" ],
      "venue" : "ISA: The Instrumentation, Systems, and Automation Society,",
      "citeRegEx" : "81",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Fast calculation of stabilizing PID controllers",
      "author" : [ "M.T. Söylemez", "N. Munro", "H. Baki" ],
      "venue" : "Automatica, 39(1):121–126,",
      "citeRegEx" : "82",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A real-time 3-D musculoskeletal model for dynamic simulation of arm movements",
      "author" : [ "E.K. Chadwick", "D. Blana", "A.J. van den Bogert", "R.F. Kirsch" ],
      "venue" : "In IEEE Transactions on Biomedical Engineering,",
      "citeRegEx" : "84",
      "shortCiteRegEx" : "84",
      "year" : 2009
    }, {
      "title" : "A proportional derivative FES controller for planar arm movement",
      "author" : [ "K. Jagodnik", "A. van den Bogert" ],
      "venue" : "In 12th Annual Conference International FES Society,",
      "citeRegEx" : "85",
      "shortCiteRegEx" : "85",
      "year" : 2007
    }, {
      "title" : "Application of the actor-critic architecture to functional electrical stimulation control of a human arm",
      "author" : [ "P.S. Thomas", "M.S. Branicky", "A.J. van den Bogert", "K.M. Jagodnik" ],
      "venue" : "In Proceedings of the Twenty-First Innovative Applications of Artificial Intelligence,",
      "citeRegEx" : "86",
      "shortCiteRegEx" : "86",
      "year" : 2009
    }, {
      "title" : "Lyapunov design for safe reinforcement learning",
      "author" : [ "T.J. Perkins", "A.G. Barto" ],
      "venue" : "Journal of Machine Learning Research, 3: 803–832,",
      "citeRegEx" : "87",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Biped dynamic walking using reinforcement learning",
      "author" : [ "H. Bendrahim", "J.A. Franklin" ],
      "venue" : "Robotics and Autonomous Systems, 22: 283–302,",
      "citeRegEx" : "88",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Control of markov chains with safety bounds",
      "author" : [ "A. Arapostathis", "R. Kumar", "S.P. Hsu" ],
      "venue" : "IEEE Transactions on Automation Science and Engineering, volume 2, pages 333–343, October",
      "citeRegEx" : "89",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Control design for Markov chains under safety constraints: A convex approach",
      "author" : [ "E. Arvelo", "N.C. Martins" ],
      "venue" : "CoRR, abs/1209.2883,",
      "citeRegEx" : "90",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Risk-sensitive reinforcement learning applied to control under constraints",
      "author" : [ "P. Geibel", "F. Wysotzki" ],
      "venue" : "Journal of Artificial Intelligence Research 24, pages 81–108,",
      "citeRegEx" : "91",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Variational bayesian 8.2. Convergence Analysis of True Gradient Temporal Difference Learning 115 optimization for runtime risk-sensitive control",
      "author" : [ "S. Kuindersma", "R. Grupen", "A.G. Barto" ],
      "venue" : "In Robotics: Science and Systems VIII,",
      "citeRegEx" : "92",
      "shortCiteRegEx" : "92",
      "year" : 2012
    }, {
      "title" : "Natural actor-critic algorithms",
      "author" : [ "S. Bhatnagar", "R.S. Sutton", "M. Ghavamzadeh", "M. Lee" ],
      "venue" : "Automatica, 45(11):2471–2482,",
      "citeRegEx" : "93",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Technical Report UCB/EECS-2010-24, Electrical Engineering and Computer Sciences, University of California at Berkeley, March",
      "citeRegEx" : "94",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convex Analysis",
      "author" : [ "R. Tyrell Rockafellar" ],
      "venue" : "Princeton University Press, Princeton, New Jersey,",
      "citeRegEx" : "95",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Numerical Optimization",
      "author" : [ "J. Nocedal", "S. Wright" ],
      "venue" : "Springer, second edition,",
      "citeRegEx" : "96",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A natural policy gradient",
      "author" : [ "S. Kakade" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 14, pages 1531–1538,",
      "citeRegEx" : "97",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour" ],
      "venue" : "Advances in Neural Information Processing Systems 12, pages 1057–1063,",
      "citeRegEx" : "98",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Utilizing the natural gradient in temporal difference reinforcement learning with eligibility traces",
      "author" : [ "T. Morimura", "E. Uchibe", "K. Doya" ],
      "venue" : "International Symposium on Information Geometry and its Application,",
      "citeRegEx" : "99",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Natural actor-critic",
      "author" : [ "J. Peters", "S. Schaal" ],
      "venue" : "Neurocomputing, 71:1180–1190,",
      "citeRegEx" : "100",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Motor primitive discovery",
      "author" : [ "P.S. Thomas", "A.G. Barto" ],
      "venue" : "Procedings of the IEEE Conference on Development and Learning and EPigenetic Robotics,",
      "citeRegEx" : "101",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Model-free reinforcement learning with continuous action in practice",
      "author" : [ "T. Degris", "P.M. Pilarski", "R.S. Sutton" ],
      "venue" : "Proceedings of the 2012 American Control Conference,",
      "citeRegEx" : "102",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bias in natural actor-critic algorithms",
      "author" : [ "P.S. Thomas" ],
      "venue" : "Technical Report UM-CS-2012-018, Department of Computer Science, University of Massachusetts at Amherst,",
      "citeRegEx" : "103",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Combined feedforward and feedback control of a redundant, nonlinear, dynamic musculoskeletal system",
      "author" : [ "D. Blana", "R.F. Kirsch", "E.K. Chadwick" ],
      "venue" : "Medical and Biological Engineering and 116 Appendix: Technical Proofs Computing, 47:533–542,",
      "citeRegEx" : "104",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The Fixed Points of Off-Policy TD",
      "author" : [ "J. Zico Kolter" ],
      "venue" : "Advances in Neural Information Processing Systems 24, pages 2169–2177,",
      "citeRegEx" : "105",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A convergent o(n) algorithm for off-policy temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "C. Szepesvari", "H.R. Maei" ],
      "venue" : "Neural Information Processing Systems, pages 1609–1616,",
      "citeRegEx" : "106",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L.C. Baird" ],
      "venue" : "International Conference on Machine Learning, pages 30–37,",
      "citeRegEx" : "107",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Optimal primal-dual methods for a class of saddle point problems",
      "author" : [ "Y. Chen", "G. Lan", "Y. Ouyang" ],
      "venue" : "arXiv preprint arXiv:1309.5548,",
      "citeRegEx" : "108",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Solving variational inequalities with stochastic mirror-prox algorithm",
      "author" : [ "A. Juditsky", "A.S. Nemirovskii", "C. Tauvel" ],
      "venue" : "Arxiv preprint arXiv:0809.0815,",
      "citeRegEx" : "109",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science",
      "author" : [ "E. Esser", "X. Zhang", "T. F Chan" ],
      "venue" : "SIAM Journal on Imaging Sciences, 3(4): 1015–1046,",
      "citeRegEx" : "110",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A Dantzig Selector Approach to Temporal Difference Learning",
      "author" : [ "M. Geist", "B. Scherrer", "A. Lazaric", "M. Ghavamzadeh" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "111",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Policy evaluation with temporal differences: A survey and comparison",
      "author" : [ "C. Dann", "G. Neumann", "J. Peters" ],
      "venue" : "Journal of Machine Learning Research, 15:809–883,",
      "citeRegEx" : "112",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "LSTD with Random Projections",
      "author" : [ "M. Ghavamzadeh", "A. Lazaric", "O.A. Maillard", "R. Munos" ],
      "venue" : "Proceedings of the International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "113",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Practical kernel-based reinforcement learning",
      "author" : [ "A. MS Barreto", "D. Precup", "J. Pineau" ],
      "venue" : null,
      "citeRegEx" : "114",
      "shortCiteRegEx" : "114",
      "year" : 2013
    }, {
      "title" : "Kernelized value function approximation for reinforcement learning",
      "author" : [ "G. Taylor", "R. Parr" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, pages 1017–1024. 8.2. Convergence Analysis of True Gradient Temporal Difference Learning 117 ACM,",
      "citeRegEx" : "115",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Migration equilibrium and variational inequalities",
      "author" : [ "A. Nagurney" ],
      "venue" : "Economics Letters, 31:109–112,",
      "citeRegEx" : "116",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Evolutionary Dynamics: Exploring the Equations of Life",
      "author" : [ "M. Novak" ],
      "venue" : "Harvard Belknap Press,",
      "citeRegEx" : "117",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Networks, Crowds, and Markets: Reasoning About a Highly Connected World",
      "author" : [ "D. Easley", "J. Kleinberg" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "118",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The Theory of Learning in Games",
      "author" : [ "D. Fudenberg", "D. Levine" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "119",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Algorithmic Game Theory",
      "author" : [ "N. Nisan", "T. Roughgarden", "E. Tardos", "V. Vazirani" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "120",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Economics",
      "author" : [ "P. Samuelson", "W. Nordhaus" ],
      "venue" : "McGraw Hill Press,",
      "citeRegEx" : "121",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Projected Dynamical Systems and Variational Inequalities with Applications",
      "author" : [ "A. Nagurney", "D. Zhang" ],
      "venue" : "Kluwer Academic Press,",
      "citeRegEx" : "122",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Multiagent learning with a variable learning rate",
      "author" : [ "M. Bowling", "M. Veloso" ],
      "venue" : "Artificial Intelligence, 136:215–250,",
      "citeRegEx" : "123",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Nash convergence of gradient dynamics in general-sum games",
      "author" : [ "S. Singh", "M. Kearns", "Y. Mansour" ],
      "venue" : "Proceedings of the Uncertainty in AI conference,",
      "citeRegEx" : "124",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The extragradient method for finding saddle points and other problems",
      "author" : [ "G. Korpelevich" ],
      "venue" : "Matekon, 13:35–49,",
      "citeRegEx" : "125",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Structured prediction, dual extragradient and bregman projections",
      "author" : [ "B. Taskar", "S. Lacoste-Julien", "Michael Jordan" ],
      "venue" : "Matekon, 7:627–1653,",
      "citeRegEx" : "126",
      "shortCiteRegEx" : "126",
      "year" : 2008
    }, {
      "title" : "Static prediction games for adversarial learning problems",
      "author" : [ "M. Bruckner", "C. Kanzow", "T. Scheffer" ],
      "venue" : "Journal of Machine Learning Research, 13:2617–2654,",
      "citeRegEx" : "127",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Numerical Recipes in C",
      "author" : [ "W. Press", "S. Tuekolsky", "W. Vettering", "B. Flannery" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "128",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "First order methods for nonsmooth convex large-scale optimization, i: General purpose methods",
      "author" : [ "A. Juditsky", "A. Nemirovski" ],
      "venue" : "Optimization in Machine Learning. MIT Press,",
      "citeRegEx" : "129",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "First order methods for non- 118 Appendix: Technical Proofs smooth convex large-scale optimization, ii: Utilizing problem structure",
      "author" : [ "A. Juditsky", "A. Nemirovski" ],
      "venue" : "Optimization in Machine Learning. MIT Press,",
      "citeRegEx" : "130",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Linear Complementarity, Linear and Nonlinear Programming",
      "author" : [ "K. Murty" ],
      "venue" : "Heldermann Verlag,",
      "citeRegEx" : "131",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B, Jan",
      "citeRegEx" : "132",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Fast active-set-type algorithms for L1regularized linear regression",
      "author" : [ "J. Kim", "H. Park" ],
      "venue" : "Proceedings of the Conference on AI and Statistics, pages 397–404,",
      "citeRegEx" : "133",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Linear complementarity for regularized policy evaluation and improvement",
      "author" : [ "J. Johns", "C. Painter-Wakefield", "R. Parr" ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems, 23,",
      "citeRegEx" : "134",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Regularized off-policy TDlearning",
      "author" : [ "B. Liu", "S. Mahadevan", "J. Liu" ],
      "venue" : "Proceedings of the International Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "135",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Parallel and Distributed Computation: Numerical Methods",
      "author" : [ "D. Bertsekas", "J. Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "136",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A variant of Korpelevich’s method for variational inequalities with a new search strategy",
      "author" : [ "A. Iusem", "B. Svaiter" ],
      "venue" : "Optimization, 42:309–321,",
      "citeRegEx" : "137",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Modification of the extragradient method for solving variational inequalities of certain optimization problems",
      "author" : [ "E. Khobotov" ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics, 27:120–127,",
      "citeRegEx" : "138",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Application of Khobotov’s algorithm to variational inequalities and network equilibrium problems",
      "author" : [ "P. Marcotte" ],
      "venue" : "INFORM, 29,",
      "citeRegEx" : "139",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "A new hybrid extragradient method for generalized mixed equilibrium problems, fixed point problems, and variational inequality problems",
      "author" : [ "J. Peng", "J. Yao" ],
      "venue" : "Taiwanese Journal of Mathematics, 12:1401–1432,",
      "citeRegEx" : "140",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Dual extrapolation and its application to solving variational inequalities and related problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical Programming Series B., 109:319–344,",
      "citeRegEx" : "141",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A new projection method for variational inequality problems",
      "author" : [ "M. Solodov", "B. Svaiter" ],
      "venue" : "SIAM Journal of Control and Optimization, 37(3):756–776,",
      "citeRegEx" : "142",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Projected equations, variational inequalities, and temporal difference methods",
      "author" : [ "D. Bertsekas" ],
      "venue" : "Technical Report LIDS-P-2808, MIT, March",
      "citeRegEx" : "143",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Galerkin methods for complementarity problems and variational inequalities",
      "author" : [ "G. Gordon" ],
      "venue" : "Arxiv, June",
      "citeRegEx" : "144",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course, volume",
      "author" : [ "Y. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "145",
      "shortCiteRegEx" : "145",
      "year" : 2004
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "B. T Polyak", "A. B Juditsky" ],
      "venue" : "SIAM Journal on Control and Optimization, 30(4):838–855,",
      "citeRegEx" : "146",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "New error bounds for approximations from projected linear equations",
      "author" : [ "H. Yu", "D.P. Bertsekas" ],
      "venue" : "Technical Report C-2008-43, Dept. Computer Science, Univ. of Helsinki,",
      "citeRegEx" : "147",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In this chapter, we lay out the elements of our novel framework for reinforcement learning [1], based on doing temporal difference learning not in the primal space, but in a dual space defined by a so-called mirror map.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "examples of the benefits of our framework, showing each of the four key pieces of our solution: the improved performance of our new off-policy temporal difference methods over previous gradient TD methods, like TDC and GTD2 [2]; how we are able to generalize natural gradient actor critic methods using mirror maps, and achieve safety guarantees to control learning in complex robots; and finally, elements of our saddle point reformulation of temporal difference learning.",
      "startOffset" : 224,
      "endOffset" : 227
    }, {
      "referenceID" : 2,
      "context" : "endre” transform [3].",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "We use Bregman divergences [4] to ensure that safety constraints are adhered to, where the projection is defined as:",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8].",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8].",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8].",
      "startOffset" : 246,
      "endOffset" : 249
    }, {
      "referenceID" : 8,
      "context" : "Variational inequalities, in the infinite-dimensional setting, were originally proposed by Hartman and Stampacchia [10] in the mid-1960s in the",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "Finitedimensional VIs rose in popularity in the 1980s partly as a result of work by Dafermos [11], who showed that the traffic network equilibrium problem could be formulated as a finite-dimensional VI.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "could also be formulated as finite-dimensional VIs – the books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "could also be formulated as finite-dimensional VIs – the books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "We will see later how this property of extragradient makes its appearance in accelerating gradient temporal difference learning algorithms, such as TDC [2].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "The mirror-prox algorithm (MP) [7] is a first-order approach that is able to solve saddle-point problems at a convergence rate of O(1/t).",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "The MP method plays a key role in our framework as our approach extensively uses the saddle point reformulation of reinforcement learning developed by us [14].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "With this introduction, we can now introduce the main concept of mirror descent, which was originally proposed by Nemirovksi and Yudin [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "We follow the treatment in [6] in presenting the mirror descent algorithm as a nonlinear proximal method based on a distance generator function that is a Bregman divergence [4].",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "We follow the treatment in [6] in presenting the mirror descent algorithm as a nonlinear proximal method based on a distance generator function that is a Bregman divergence [4].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : "It is shown in [15] that the mirror descent procedure specified in Equation 1.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "3 with the Bregman divergence defined by the p-norm function [16] can outperform regular projected subgradient method by a factor n logn where n is the dimensionality of the space.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "In our framework, a key insight used to derive a true stochastic gradient method for reinforcement learning is based on the powerful concept of operator splitting [17, 18].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "In our framework, a key insight used to derive a true stochastic gradient method for reinforcement learning is based on the powerful concept of operator splitting [17, 18].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "This problem originally motivated the development of Bregman divergences [4].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "The convex feasibility problem is an example of many real-world problems, such as 3D voxel reconstruction in brain imaging [15], a high-dimensional problem that mirror descent was originally developed for.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "Many different operator splitting strategies have been developed, such as Douglas Rachford splitting [18], which is a generalization of widely used distributed optimization methods like Alternating Direction Method of Multipliers [19].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "Many different operator splitting strategies have been developed, such as Douglas Rachford splitting [18], which is a generalization of widely used distributed optimization methods like Alternating Direction Method of Multipliers [19].",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 20,
      "context" : "Our proposed framework solves this problem by establishing a key technical result, stated below, between mirror descent and the well-known, but previously unrelated, class of algorithms called natural gradient [22].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 18,
      "context" : "5: The uBot-5 is a 11 degree of freedom mobile manipulator developed at the Laboratory of Perceptual Robotics (LPR) at the University of Massachusetts, Amherst [20, 21].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 19,
      "context" : "5: The uBot-5 is a 11 degree of freedom mobile manipulator developed at the Laboratory of Perceptual Robotics (LPR) at the University of Massachusetts, Amherst [20, 21].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : "How can we design a “safe” reinforcement learning algorithm which is guaranteed to ensure that policy learning will not violate pre-defined constraints such that such robots will operate in dangerous regions of the control parameter space? Our framework provides a key solution, based on showing an equivalence between mirror descent and a previously well-studied but unrelated algorithm called natural gradient [22].",
      "startOffset" : 412,
      "endOffset" : 416
    }, {
      "referenceID" : 21,
      "context" : "Although many more sophisticated methods have been developed over the past three decades, such as least-squares based temporal difference approaches, including LSTD [23], LSPE [24] and LSPI [25], first-order temporal difference learning algorithms may scale more gracefully to high dimensional problems.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 22,
      "context" : "Although many more sophisticated methods have been developed over the past three decades, such as least-squares based temporal difference approaches, including LSTD [23], LSPE [24] and LSPI [25], first-order temporal difference learning algorithms may scale more gracefully to high dimensional problems.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 23,
      "context" : "Although many more sophisticated methods have been developed over the past three decades, such as least-squares based temporal difference approaches, including LSTD [23], LSPE [24] and LSPI [25], first-order temporal difference learning algorithms may scale more gracefully to high dimensional problems.",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 24,
      "context" : "This motivated the development of the gradient TD (GTD) family of methods [26].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "A crucial step in the development of our framework was the development of a novel saddle-point framework for sparse regularized GTD [14].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 24,
      "context" : "Although these algorithms are motivated from the gradient of an objective function such as mean-squared projected Bellman error (MSPBE) and NEU [26], they are not true stochastic gradient methods with respect to these objective functions, as pointed out in [27], which make the convergence rate and error bound analysis difficult, although asymptotic analysis has been carried out using the ODE approach.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 25,
      "context" : "Although these algorithms are motivated from the gradient of an objective function such as mean-squared projected Bellman error (MSPBE) and NEU [26], they are not true stochastic gradient methods with respect to these objective functions, as pointed out in [27], which make the convergence rate and error bound analysis difficult, although asymptotic analysis has been carried out using the ODE approach.",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 12,
      "context" : "(4) The fourth question is on regularization: although the saddle point framework proposed in [14] provides an online regularization framework for the GTD family of algorithms, termed as RO-TD, it is based on the inverse problem formulation and is thus not quite explicit.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "In this paper, we propose a novel approach to TD algorithm design in reinforcement learning, based on introducing the proximal splitting framework [28].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "7 illustrates a sample result, showing how the mirror descent variant of temporal difference learning results in faster convergence, and much lower variance (not shown) on the classic mountain car task [1].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 27,
      "context" : "The learning environment for decision-making is generally modeled by the well-known Markov Decision Process[29] M = (S,A, P,R, γ), which is derived from a Markov chain.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 27,
      "context" : "(Markov Decision Process)[29]: A Markov Decision Process is a tuple (S,A, P,R, γ) where S is a finite set of states, A is a finite set of actions, P : S × A × S → [0, 1] is the transition kernel, where P (s, a, s′) is the probability of transmission from state s to state s′ given action a, and reward r : S × A → R+ is a reward function, 0 ≤ γ < 1 is a discount factor.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "(Markov Decision Process)[29]: A Markov Decision Process is a tuple (S,A, P,R, γ) where S is a finite set of states, A is a finite set of actions, P : S × A × S → [0, 1] is the transition kernel, where P (s, a, s′) is the probability of transmission from state s to state s′ given action a, and reward r : S × A → R+ is a reward function, 0 ≤ γ < 1 is a discount factor.",
      "startOffset" : 163,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "A stochastic policy π : S ×A→ [0, 1].",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 28,
      "context" : "The most popular and widely used RL method is temporal difference (TD) learning [30].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 29,
      "context" : "The (optimal) action value formulation is convenient because it can be approximately solved by a temporal-difference (TD) learning technique called Q-learning [31].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 30,
      "context" : "TD(0) converges to the optimal value function V π for policy π as long as the samples are “on-policy”, namely following the stochastic Markov chain associated with the policy; and the learning rate αt is decayed according to the Robbins-Monro conditions in stochastic approximation theory: ∑ t αt = ∞, ∑ t α 2 t < ∞ [32].",
      "startOffset" : 316,
      "endOffset" : 320
    }, {
      "referenceID" : 31,
      "context" : ") or automatically generated basis functions [33].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 32,
      "context" : "Here we define the problem of Stochastic Composite Optimization (SCO)[34]:",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 33,
      "context" : "(Distance-generating Function)[35]: A distancegenerating function ψ(x) is defined as a continuously differentiable μstrongly convex function.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 33,
      "context" : "(Bregman Divergence)[35]: Given distancegenerating function ψ, the Bregman divergence induced by ψ is defined as:",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 34,
      "context" : "1 p + 1 q = 1 [36].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 33,
      "context" : "The mirror descent [35] algorithm is a generalization of classic gradient descent, which has led to developments of new more powerful machine learning methods for classification and regression.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 35,
      "context" : "Mirror descent has become the cornerstone of many online l1 regularization approaches such as in [37], [38] and [39].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 36,
      "context" : "Mirror descent has become the cornerstone of many online l1 regularization approaches such as in [37], [38] and [39].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 37,
      "context" : "Mirror descent has become the cornerstone of many online l1 regularization approaches such as in [37], [38] and [39].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 36,
      "context" : "Regularized dual averaging (RDA) [38] is a variant of Dual averaging (DA) with “simple” regularizers, such as l1 regularization.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 38,
      "context" : "The DA method approximates this lower bound model with an approximate (possibly not supporting) lower bound hyperplane with the averaging of all the past gradients [40].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 39,
      "context" : "RDA with local stabilizer can be seen in [41].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 40,
      "context" : "Compared with other firstorder l1 regularization algorithms of the mirror-descent type, including truncated gradient method [42] and SMIDAS [43], RDA tends to produce sparser solutions in that the RDA method is more aggressive on sparsity than many other competing approaches.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 41,
      "context" : "Compared with other firstorder l1 regularization algorithms of the mirror-descent type, including truncated gradient method [42] and SMIDAS [43], RDA tends to produce sparser solutions in that the RDA method is more aggressive on sparsity than many other competing approaches.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 42,
      "context" : "It is worth noting that problems with non-smooth regularization functions often lead to solutions that lie on a low-dimensional supporting data manifold, and regularized dual averaging is capable of identifying this manifold, and thus bringing the potential benefit of accelerating convergence rate by searching on the low-dimensional manifold after it is identified, as suggested in [44].",
      "startOffset" : 384,
      "endOffset" : 388
    }, {
      "referenceID" : 7,
      "context" : "The extragradient method was first proposed by Korpelevich[8] as a relaxation of ordinary gradient descent to solve variational inequality (VI) problems.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "4) is guaranteed under the constraints 0 < αt < 1 √ 2L [7], where L is the Lipschitz constant for ∇f(x).",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 43,
      "context" : "Later work and variants of Nesterov’s method utilizing the strong convexity of the loss function with Bregman divergence are summarized in [45].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 32,
      "context" : "from deterministic smooth convex optimization to stochastic composite optimization, termed as AC-SA, is studied in [34].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "An important property of closed proper convex functions is that their subdifferentials induce a relation on Rn called a maximal monotone operator [17, 46].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 44,
      "context" : "An important property of closed proper convex functions is that their subdifferentials induce a relation on Rn called a maximal monotone operator [17, 46].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 45,
      "context" : "The convexconcave saddle-point problems are, therefore, usually better suited for first-order methods [47].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 46,
      "context" : "A comprehensive overview on extending convex minimization to convex-concave saddle-point problems with unified variational inequalities is presented in [48].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 47,
      "context" : "Using the approach in [49], Equation (2.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 48,
      "context" : "A general procedure for solving the monotone inclusion problem, the proximal point algorithm [50], uses the following identities:",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 49,
      "context" : "In the case where the relation R = ∂f of some convex function f , the resolvent can be shown to be the proximal mapping [51], a crucially important abstraction of the concept of projection, a cornerstone of constrained optimization.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "Operator splitting [17, 18] is a generic approach to decomposing complex optimization and variational inequality problems into simpler ones that involve computing the resolvents of individual relations, rather than sums or other compositions of relations.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "Operator splitting [17, 18] is a generic approach to decomposing complex optimization and variational inequality problems into simpler ones that involve computing the resolvents of individual relations, rather than sums or other compositions of relations.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "We will primarily focus on the Douglas Rachford algorithm [18] specified in Figure 2.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "1, because it leads to a widely used distributed optimization method called Alternating Direction Method of Multipliers (ADMM) [19].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 17,
      "context" : "The ADMM algorithm has been extensively studied in optimization; a detailed review is available in the tutorial paper by Boyd and colleagues [19], covering both its theoretical properties, operator splitting origins, and applications to high-dimensional data mining.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 26,
      "context" : "In this section we will give a brief overview of proximal splitting algorithms [28].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 50,
      "context" : "Proximal methods [53, 54], which are widely used in machine learning, signal processing, and stochastic optimization, provide a general framework for large-scale optimization.",
      "startOffset" : 17,
      "endOffset" : 25
    }, {
      "referenceID" : 51,
      "context" : "Proximal methods [53, 54], which are widely used in machine learning, signal processing, and stochastic optimization, provide a general framework for large-scale optimization.",
      "startOffset" : 17,
      "endOffset" : 25
    }, {
      "referenceID" : 52,
      "context" : "is Forward-Backward Splitting (FOBOS) [55]",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 26,
      "context" : "To solve this problem, Douglas-Rachford splitting [28] and Alternating Direction of Multiple Multipliers (ADMM) can be used.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 53,
      "context" : "Recently, ADMM has been used proposed for sparse RL [56].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 54,
      "context" : "The corresponding primal-dual formulation [57, 28, 58] of Section (2.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "The corresponding primal-dual formulation [57, 28, 58] of Section (2.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 55,
      "context" : "The corresponding primal-dual formulation [57, 28, 58] of Section (2.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : "However, in several settings it is more appropriate to assume that xk resides in a Riemannian space with metric tensor G(xk), which is an n × n positive definite matrix that may vary with xk [22].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 56,
      "context" : "In this case, the direction of steepest descent is called the natural gradient and is given by −G(xk)∇f(xk) [59].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 20,
      "context" : "In certain cases, (which include our policy search application), following the natural gradient is asymptotically Fisher-efficient [22].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 30,
      "context" : "It is worth noting that we make little use of classical stochastic approximation theory, which has traditionally been used to analyze reinforcement learning methods (as discussed in detail in books such as [32]).",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 57,
      "context" : "[60] (l1-regularized Projection): Πl1 is the l1regularized projection defined as:",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 57,
      "context" : "t weighted l2 norm, as proven in [60].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 57,
      "context" : "[60]: Πρ is a non-expansive mapping such that ∀x, y ∈ R, ||Πρx−Πρy|| ≤ ||x− y|| − ||x− y − (Πρx−Πρy)||",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 57,
      "context" : "[60] (Lasso-TD) Lasso-TD is a fixed-point equation w.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 57,
      "context" : "The properties of Lasso-TD is discussed in detail in [60].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 58,
      "context" : "Several prevailing sparse RL methods use Lasso-TD as the objective function, such as SparseTD[61], LARS-TD[62] and LCP-TD[63].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 59,
      "context" : "Several prevailing sparse RL methods use Lasso-TD as the objective function, such as SparseTD[61], LARS-TD[62] and LCP-TD[63].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 60,
      "context" : "Several prevailing sparse RL methods use Lasso-TD as the objective function, such as SparseTD[61], LARS-TD[62] and LCP-TD[63].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "LCPTD [12] formulates LASSO-TD as a linear complementarity problem (LCP), which can be solved by a variety of available LCP solvers.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 52,
      "context" : "We then derive the major step by formulating the problem as a forward-backward splitting problem (FOBOS) as in [55],",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "This ψ(w) leads to the p-norm link function θ = f(w) where f : Rd → Rd [16]:",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "The p-norm function has been extensively studied in the literature on online learning [16], and it is well-known that for large p, the corresponding classification or regression method behaves like a multiplicative method (e.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 61,
      "context" : ", the p-norm regression method for large p behaves like an exponentiated gradient method (EG) [64, 65]).",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 62,
      "context" : ", the p-norm regression method for large p behaves like an exponentiated gradient method (EG) [64, 65]).",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "Another distance generating function is the negative entropy function ψ(w) = ∑ iwi logwi, which leads to the entropic mirror descent algorithm [6].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 63,
      "context" : "Interestingly, this special case has been previously explored [66] as the exponentiated-gradient TD method, although the connection to mirror descent and Bregman divergences were not made in this previous study, and EG does not generate sparse solutions [37].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 35,
      "context" : "Interestingly, this special case has been previously explored [66] as the exponentiated-gradient TD method, although the connection to mirror descent and Bregman divergences were not made in this previous study, and EG does not generate sparse solutions [37].",
      "startOffset" : 254,
      "endOffset" : 258
    }, {
      "referenceID" : 35,
      "context" : "An analogous approach was suggested in [37] for l1 penalized classification and regression.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 64,
      "context" : "We base our derivation on the composite mirror-descent approach proposed in [67] for classification and regression.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 57,
      "context" : "Definition 2 [60]: Πl1 is the l1-regularized projection defined as: Πl1y = Φα such that α = arg minw‖y − Φw‖ +β‖w‖1, which is a nonexpansive mapping w.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 57,
      "context" : "t weighted l2 norm induced by the on-policy sample distribution setting, as proven in [60].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 35,
      "context" : "In Algorithm 2, the l1 solver is related to the SMIDAS l1 regularized mirror-descent method for regression and classification [37].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 35,
      "context" : "Employing the result of Theorem 3 in [37], after the N -th iteration, the l1 approximation error is bounded by",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 65,
      "context" : "2 shows that mirror-descent TD converges more quickly with far smaller Bellman errors than LARS-TD [68] on a discrete “tworoom” MDP [69].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 66,
      "context" : "2 shows that mirror-descent TD converges more quickly with far smaller Bellman errors than LARS-TD [68] on a discrete “tworoom” MDP [69].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 66,
      "context" : "The basis matrix Φ was automatically generated as 50 proto-value functions by diagonalizing the graph Laplacian of the discrete state space connectivity graph[69].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 40,
      "context" : "ferent values of p yield an interpolation between the truncated gradient method [42] and SMIDAS [43].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : "ferent values of p yield an interpolation between the truncated gradient method [42] and SMIDAS [43].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 67,
      "context" : "6: Top: Q-learning; Bottom: mirror-descent Q-learning with pnorm link function, both with 25 fixed Fourier bases [70] for the mountain car task.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 68,
      "context" : "The triple-link inverted pendulum [71] is a highly nonlinear time-variant under-actuated system,",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 68,
      "context" : "We base our simulation using the system parameters described in [71], except that the action space is discretized because the algorithms described here are restricted to policies with discrete actions.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "The two most widely used link functions in mirror descent are the p-norm link function [6] and the relative entropy function for exponentiated gradient (EG) [64].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 61,
      "context" : "The two most widely used link functions in mirror descent are the p-norm link function [6] and the relative entropy function for exponentiated gradient (EG) [64].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 61,
      "context" : "It has been shown that when the features are dense and the optimal coefficients θ∗ are sparse, EG converges faster than the regular additive gradient methods [64].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 63,
      "context" : "It has been pointed out [66] that in the EG-Sarsa algorithm, rescaling can fail, and replacing eligible traces instead of regular additive eligible traces is used to prevent overflow.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 67,
      "context" : "Thanks to the flexible interpolation capability between multiplicative and additive gradient updates, the p-norm link function is more robust and applicable to various basis functions, such as polynomial, radial basis function (RBF), Fourier basis [70], proto-value functions (PVFs), etc.",
      "startOffset" : 248,
      "endOffset" : 252
    }, {
      "referenceID" : 69,
      "context" : "Mirror Descent Q-learning demonstrates the following advantage over regular Q learning: faster convergence rate and reduced variance due to larger stepsizes with theoretical convergence guarantees [72].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 35,
      "context" : "Compared with existing sparse reinforcement learning algorithms such as LARS-TD, Algorithm 2 has lower sample complexity and lower computation cost, advantages accrued from the first-order mirror descent framework combined with proximal mapping [37].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 1,
      "context" : "We are currently exploring a mirror-descent fast-gradient RL method, which is both convergent off-policy and quicker than fast gradient TD methods such as GTD and TDC [2].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 64,
      "context" : "We are also undertaking a more detailed theoretical analysis of the mirror-descent RL framework, building on existing analysis of mirror-descent methods [67, 37].",
      "startOffset" : 153,
      "endOffset" : 161
    }, {
      "referenceID" : 35,
      "context" : "We are also undertaking a more detailed theoretical analysis of the mirror-descent RL framework, building on existing analysis of mirror-descent methods [67, 37].",
      "startOffset" : 153,
      "endOffset" : 161
    }, {
      "referenceID" : 70,
      "context" : "Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 69,
      "context" : "Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 30,
      "context" : "Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].",
      "startOffset" : 222,
      "endOffset" : 230
    }, {
      "referenceID" : 71,
      "context" : "Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].",
      "startOffset" : 222,
      "endOffset" : 230
    }, {
      "referenceID" : 72,
      "context" : "As pointed out in [75], the target policy is often a deterministic policy that approximates the optimal policy, and the behavior policy is often stochastic, exploring all possible actions in each state as part of finding the optimal policy.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 73,
      "context" : "Besides, offpolicy methods are of wider applications since they are able to learn while executing an exploratory policy, learn from demonstrations, and learn multiple tasks in parallel [76].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 24,
      "context" : "[26] introduced convergent off-policy temporal difference learning algorithms, such as TDC, whose computation time scales linearly with the number of samples and the number of features.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 73,
      "context" : "Recently, a linear off-policy actor-critic algorithm based on the same framework was proposed in [76].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 45,
      "context" : "Thus, convex-concave saddle-point problems are, therefore, usually better suited for first-order methods [47].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 46,
      "context" : "A comprehensive overview on extending convex minimization to convex-concave saddlepoint problems with unified variational inequalities is presented in [48].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 47,
      "context" : "Using the approach in [49], Equation (4.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "MSPBE(θ) = ‖Φθ −ΠT (Φθ)‖2Ξ = (ΦTΞ(TΦθ − Φθ))T (ΦTΞΦ)−1ΦTΞ(TΦθ − Φθ) = E[δt(θ)φt]E[φtφt ]E[δt(θ)φt] To avoid computing the inverse matrix (ΦTΞΦ)−1 and to avoid the double sampling problem [1] in (4.",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 24,
      "context" : "Following [26], the TDC algorithm solution follows from the linear equation Ax = b, where a single iteration gradient update would be",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "The two time-scale gradient descent learning method TDC [26] is",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "As pointed out in [1], double-sampling is a necessary condition to obtain an unbiased estimator if the objective function is the Bellman residual or its derivatives (such as projected Bellman residual), wherein the product of Bellman error or projected Bellman error metrics are involved.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 74,
      "context" : "1) with the following convexconcave formulation as in [77, 47],",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 45,
      "context" : "1) with the following convexconcave formulation as in [77, 47],",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 45,
      "context" : "The averaging step, which plays a crucial role in stochastic optimization convergence, generates the approximate saddle-points [47, 78]",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 75,
      "context" : "The averaging step, which plays a crucial role in stochastic optimization convergence, generates the approximate saddle-points [47, 78]",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 76,
      "context" : "GQ(λ)[79] is a generalization of the TDC algorithm with eligibility traces and off-policy learning of temporally abstract predictions, where the gradient update changes from Equation (4.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 76,
      "context" : "where eligibility traces et, and φ̄t, T πλ are defined in [79].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 59,
      "context" : "LARS-TD [62], which is a popular second-order sparse reinforcement learning algorithm, is used as the baseline algorithm for feature selection and TDC is used as the off-policy convergent RL baseline algorithm, respectively.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 24,
      "context" : "are used in [26], which are tabular features, inverted features and dependent features respectively.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 24,
      "context" : "An identical experiment setting to [26] is used for these two domains.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 59,
      "context" : "To make a fair comparison, we use the same basis function setting as in [62], where two dimensional grids of 2, 4, 8, 16, 32 RBFs are used so that there are totally 1365 basis functions.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 59,
      "context" : "We use the result of LARS-TD and l2 LSTD reported in [62].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 45,
      "context" : "It is worth noting that comparing the performance of RO-TD and LARS-TD is not the major focus here, since LARS-TD is not convergent off-policy and RO-TD’s performance can be further optimized using the mirror-descent approach with the Mirror-Prox algorithm [47] which incorporates mirror descent with an extragradient [8], as discussed below.",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 7,
      "context" : "It is worth noting that comparing the performance of RO-TD and LARS-TD is not the major focus here, since LARS-TD is not convergent off-policy and RO-TD’s performance can be further optimized using the mirror-descent approach with the Mirror-Prox algorithm [47] which incorporates mirror descent with an extragradient [8], as discussed below.",
      "startOffset" : 318,
      "endOffset" : 321
    }, {
      "referenceID" : 68,
      "context" : "The triple-link inverted pendulum [71] is a highly nonlinear underactuated system with 8-dimensional state space and discrete action space.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 77,
      "context" : "Fourier basis [80] with order 2 is used, resulting in 6561 basis functions.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 56,
      "context" : "Natural actor-critics form a class of policy search algorithms for finding locally optimal policies for Markov decision processes (MDPs) by approximating and ascending the natural gradient [59] of an objective",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 78,
      "context" : "For example, proportional-integral-derivative controllers (PID controllers) are the most widely used control algorithms in industry, and have been studied in depth [81].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 79,
      "context" : "Techniques exist for determining the set of stable gains (policy parameters) when a model of the system is available [82].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 80,
      "context" : "There has been a recent push to develop controllers that specify how much and when to stimulate each muscle in a human arm to move it from its current position to a desired position [84].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 81,
      "context" : "Hence, a proportional-derivative (PD) controller, tuned to a simulation of an ideal human arm, required manual tuning to obtain desirable performance on a human subject with biceps spasticity [85].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 82,
      "context" : "Researchers have shown that policy search algorithms are a viable approach to creating controllers that can automatically adapt to an individual’s arm by training on a few hundred two-second reaching movements [86].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 83,
      "context" : "Researchers have addressed safety concerns like these before [87].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 84,
      "context" : "Bendrahim and Franklin [88] showed how a walking biped robot can switch to a stabilizing controller whenever the robot leaves a stable region of state space.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 85,
      "context" : "Similar state-avoidant approaches to safety have been proposed by several others [89, 90, 91].",
      "startOffset" : 81,
      "endOffset" : 93
    }, {
      "referenceID" : 86,
      "context" : "Similar state-avoidant approaches to safety have been proposed by several others [89, 90, 91].",
      "startOffset" : 81,
      "endOffset" : 93
    }, {
      "referenceID" : 87,
      "context" : "Similar state-avoidant approaches to safety have been proposed by several others [89, 90, 91].",
      "startOffset" : 81,
      "endOffset" : 93
    }, {
      "referenceID" : 88,
      "context" : "[92] developed a method for performing risk-sensitive policy search, which models the variance of the objective function for each policy and permits runtime adjustments of risk sensitivity.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 89,
      "context" : "[93] presented projected natural actor-critic algorithms for the average reward setting.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 90,
      "context" : "[94] presented mirror descent using the Mahalanobis norm for the proximal function, which is very similar to the proximal function that we show to cause mirror descent to be equivalent to natural gradient descent.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 90,
      "context" : "Also, since Gk depends on k, ψk is an adaptive proximal function [94].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 91,
      "context" : "2 N̂X(x) is the normal cone of X at x if x ∈ X and ∅ otherwise [95].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 92,
      "context" : "A necessary and sufficient condition for x to be a fixed point of PSG is that −∇f(x) ∈ N̂X(x) [96].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 93,
      "context" : "Natural actor-critics, first proposed by Kakade [97], are algorithms that estimate and ascend the natural gradient of J(θ), using the average Fisher information matrix as the metric tensor:",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 94,
      "context" : "where dπ is a policy and objective function-dependent distribution over the state set [98].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 95,
      "context" : "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(λ) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(λ) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 96,
      "context" : "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(λ) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(λ) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].",
      "startOffset" : 177,
      "endOffset" : 182
    }, {
      "referenceID" : 96,
      "context" : "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(λ) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(λ) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].",
      "startOffset" : 221,
      "endOffset" : 226
    }, {
      "referenceID" : 97,
      "context" : "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(λ) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(λ) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].",
      "startOffset" : 276,
      "endOffset" : 281
    }, {
      "referenceID" : 98,
      "context" : "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(λ) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(λ) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].",
      "startOffset" : 323,
      "endOffset" : 328
    }, {
      "referenceID" : 89,
      "context" : "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(λ) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(λ) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].",
      "startOffset" : 397,
      "endOffset" : 401
    }, {
      "referenceID" : 99,
      "context" : "Many of the existing natural policy gradient algorithms, including NAC-LSTD, eNAC, NAC-Sarsa, and INAC, follow biased estimates of the natural policy gradient [103].",
      "startOffset" : 159,
      "endOffset" : 164
    }, {
      "referenceID" : 99,
      "context" : "The unbiased discounted reward form of NAC-Sarsa was recently derived [103].",
      "startOffset" : 70,
      "endOffset" : 75
    }, {
      "referenceID" : 89,
      "context" : "where {μt} is a stepsize schedule [93].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 100,
      "context" : "We used the Dynamic Arm Simulator 1 (DAS1) [104], a detailed biomechanical simulation of a human arm undergoing functional electrical stimulation.",
      "startOffset" : 43,
      "endOffset" : 48
    }, {
      "referenceID" : 81,
      "context" : "In a previous study, a controller created using DAS1 performed well on an actual human subject undergoing FES, although it required some additional tuning in order to cope with biceps spasticity [85].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 0,
      "context" : "The arm is controlled by providing a stimulation in the interval [0, 1] to each of six muscles.",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 81,
      "context" : "The reward function used was similar to that of Jagodnik and van den Bogert [85], which punishes joint angle error and high muscle stimulation.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "2, is an 11-DoF mobile manipulator developed at the University of Massachusetts Amherst [20, 21].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "2, is an 11-DoF mobile manipulator developed at the University of Massachusetts Amherst [20, 21].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "Although least-squares based temporal difference approaches, such as LSTD [23], LSPE [24] and LSPI [25] perform well with moderate size problems, first-order temporal difference learning algorithms scale more gracefully to high dimensional problems.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : "Although least-squares based temporal difference approaches, such as LSTD [23], LSPE [24] and LSPI [25] perform well with moderate size problems, first-order temporal difference learning algorithms scale more gracefully to high dimensional problems.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 23,
      "context" : "Although least-squares based temporal difference approaches, such as LSTD [23], LSPE [24] and LSPI [25] perform well with moderate size problems, first-order temporal difference learning algorithms scale more gracefully to high dimensional problems.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "This motivated the development of the gradient TD (GTD) family of methods [26].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "A novel saddle-point framework for sparse regularized GTD was proposed recently [14].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 25,
      "context" : "Although these algorithms are motivated from the gradient of an objective function such as MSPBE and NEU, they are not true stochastic gradient methods with respect to these objective functions, as pointed out in [27], which make the convergence rate and error bound analysis difficult, although asymptotic analysis has been carried out using the ODE approach.",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 12,
      "context" : "(4) The fourth question is on regularization: although the saddle point framework proposed in [14] provides an online regularization framework for the GTD family of algorithms, termed as RO-TD, it is based on the inverse problem formulation and is thus not quite explicit.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "In this paper, we propose a novel approach to TD algorithm design in reinforcement learning, based on introducing the proximal splitting framework [28].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 101,
      "context" : "However, TD is only guaranteed to converge in the on-policy setting, although in many off-policy situations, it still has satisfactory performance [105].",
      "startOffset" : 147,
      "endOffset" : 152
    }, {
      "referenceID" : 24,
      "context" : "TDC [26] aims to minimize the mean-square projected Bellman error (MSPBE) with a similar two-time-scale technique, which is defined as MSPBE(θ) =",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 25,
      "context" : "As pointed out in [27], although many algorithms are motivated by well-defined convex objective functions such as MSPBE and NEU, due to the biased sampling problem, the unbiased stochastic gradient is impossible to obtain, and thus the algorithms are not true SGD methods w.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 53,
      "context" : "This auxiliary variable technique is also used in [56].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 25,
      "context" : "As pointed out in [27], although the GTD family of algorithms are derived from the gradient w.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 102,
      "context" : "Note that if h(θ) = 0 andX = Rd, then we will have the GTD algorithm proposed in [106].",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "Note that if h(θ) = 0 and X = Rd, then we will have the GTD2 algorithm proposed in [26].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "Note that if h(θ) = 0 and X = Rd, then we will have TDC algorithm proposed in [26].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 103,
      "context" : "which is the update rule of residual gradient [107], and is proven not to converge to NEU any more.",
      "startOffset" : 46,
      "endOffset" : 51
    }, {
      "referenceID" : 104,
      "context" : "A comprehensive overview of the convergence rate of different approaches to stochastic saddle-point problems is given in [108].",
      "startOffset" : 121,
      "endOffset" : 126
    }, {
      "referenceID" : 45,
      "context" : "In this section we present accelerated algorithms based on the Stochastic Mirror-Prox (SMP) Algorithm [47, 109].",
      "startOffset" : 102,
      "endOffset" : 111
    }, {
      "referenceID" : 105,
      "context" : "In this section we present accelerated algorithms based on the Stochastic Mirror-Prox (SMP) Algorithm [47, 109].",
      "startOffset" : 102,
      "endOffset" : 111
    }, {
      "referenceID" : 72,
      "context" : "1 It converges to mean-square TD error (MSTDE), as proven in [75].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 106,
      "context" : "However, using the pre-conditioning technique introduced in [110], ADMM",
      "startOffset" : 60,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "2 Although only regularized TDC was proposed in [14], the algorithm can be easily extended to regularized GTD and GTD2.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 55,
      "context" : "can be reduced to the primal-dual splitting method as pointed out in [58].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 107,
      "context" : "Now we consider the two-state MDP in [111].",
      "startOffset" : 37,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "9, and a one-feature basis Φ = [1, 2]T .",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "9, and a one-feature basis Φ = [1, 2]T .",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 59,
      "context" : "t ρ, whereas Lasso-TD may have multiple solutions if the P -matrix condition is not satisfied [62].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 108,
      "context" : "We use the random generated MDP with 400 states and 10 actions in [112].",
      "startOffset" : 66,
      "endOffset" : 71
    }, {
      "referenceID" : 108,
      "context" : "The parameters of each algorithm are chosen via comparative studies similar to [112].",
      "startOffset" : 79,
      "endOffset" : 84
    }, {
      "referenceID" : 109,
      "context" : "Future research is ongoing to explore other operator splitting techniques beyond primal-dual splitting as well as incorporating random projections [113], and investigating kernelized algorithms [114, 115].",
      "startOffset" : 147,
      "endOffset" : 152
    }, {
      "referenceID" : 110,
      "context" : "Future research is ongoing to explore other operator splitting techniques beyond primal-dual splitting as well as incorporating random projections [113], and investigating kernelized algorithms [114, 115].",
      "startOffset" : 194,
      "endOffset" : 204
    }, {
      "referenceID" : 111,
      "context" : "Future research is ongoing to explore other operator splitting techniques beyond primal-dual splitting as well as incorporating random projections [113], and investigating kernelized algorithms [114, 115].",
      "startOffset" : 194,
      "endOffset" : 204
    }, {
      "referenceID" : 7,
      "context" : "Methods like extragradient [8] and the mirror-prox algorithm were originally proposed to solve variational inequalities and related saddle point problems.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 112,
      "context" : "The concept of equilibrium plays a key role in understanding not only the Internet, but also other networked systems, such as human migration [116], evolutionary dynamics and the spread of infectious diseases [117], and social networks [118].",
      "startOffset" : 142,
      "endOffset" : 147
    }, {
      "referenceID" : 113,
      "context" : "The concept of equilibrium plays a key role in understanding not only the Internet, but also other networked systems, such as human migration [116], evolutionary dynamics and the spread of infectious diseases [117], and social networks [118].",
      "startOffset" : 209,
      "endOffset" : 214
    }, {
      "referenceID" : 114,
      "context" : "The concept of equilibrium plays a key role in understanding not only the Internet, but also other networked systems, such as human migration [116], evolutionary dynamics and the spread of infectious diseases [117], and social networks [118].",
      "startOffset" : 236,
      "endOffset" : 241
    }, {
      "referenceID" : 115,
      "context" : "Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas.",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 116,
      "context" : "Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas.",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 117,
      "context" : "Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas.",
      "startOffset" : 72,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "We are currently exploring two powerful mathematical tools for the study of equilibria – variational inequalities (VIs) and projected dynamical systems (PDS) [12, 122] – in developing a new machine learning framework for solving",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 118,
      "context" : "We are currently exploring two powerful mathematical tools for the study of equilibria – variational inequalities (VIs) and projected dynamical systems (PDS) [12, 122] – in developing a new machine learning framework for solving",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "Variational inequalities (VIs), in the infinite-dimensional setting, were originally proposed by Hartman and Stampacchia [10] in the mid1960s in the context of solving partial differential equations in mechanics.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "Finite-dimensional VIs rose in popularity in the 1980s partly as a result of work by Dafermos [11].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 118,
      "context" : "Projected dynamical systems (PDS) [122] are a class of ordinary differential equations (ODEs) with a discontinuous right-hand side.",
      "startOffset" : 34,
      "endOffset" : 39
    }, {
      "referenceID" : 119,
      "context" : "[123, 119, 124].",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 115,
      "context" : "[123, 119, 124].",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 120,
      "context" : "[123, 119, 124].",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 121,
      "context" : "One of the original algorithms for solving finite-dimensional VIs is the extragradient method proposed by Korpelevich [125].",
      "startOffset" : 118,
      "endOffset" : 123
    }, {
      "referenceID" : 122,
      "context" : "[126].",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 123,
      "context" : "[127] use a modified extragradient method for solving the spam filtering problem modeled as a prediction game.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 124,
      "context" : "We are developing a new family of extragradient-like methods based on well-known numerical methods for solving ordinary differential equations, specifically the Runge Kutta method [128].",
      "startOffset" : 180,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "In optimization, the extragradient algorithm was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called “mirrror-prox” algorithm [129, 130].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 125,
      "context" : "In optimization, the extragradient algorithm was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called “mirrror-prox” algorithm [129, 130].",
      "startOffset" : 191,
      "endOffset" : 201
    }, {
      "referenceID" : 126,
      "context" : "In optimization, the extragradient algorithm was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called “mirrror-prox” algorithm [129, 130].",
      "startOffset" : 191,
      "endOffset" : 201
    }, {
      "referenceID" : 127,
      "context" : "In an NCP, whenever the mapping function F is affine, that is F (x) = Mx + b, where M is an n × n matrix, then the corresponding NCP is called a linear complementarity problem (LCP) [131].",
      "startOffset" : 182,
      "endOffset" : 187
    }, {
      "referenceID" : 128,
      "context" : "Recent work on learning sparse models using L1 regularization has exploited the fact that the standard LASSO objective [132] of L1 penalized regression can be reduced to solving an LCP [133].",
      "startOffset" : 119,
      "endOffset" : 124
    }, {
      "referenceID" : 129,
      "context" : "Recent work on learning sparse models using L1 regularization has exploited the fact that the standard LASSO objective [132] of L1 penalized regression can be reduced to solving an LCP [133].",
      "startOffset" : 185,
      "endOffset" : 190
    }, {
      "referenceID" : 130,
      "context" : "This reduction to LCP has been used in recent work on sparse value function approximation as well in a method called LCP-TD [134].",
      "startOffset" : 124,
      "endOffset" : 129
    }, {
      "referenceID" : 115,
      "context" : "The VI framework provides a mathematically elegant approach to model equilibrium problems in game theory [119, 120].",
      "startOffset" : 105,
      "endOffset" : 115
    }, {
      "referenceID" : 116,
      "context" : "The VI framework provides a mathematically elegant approach to model equilibrium problems in game theory [119, 120].",
      "startOffset" : 105,
      "endOffset" : 115
    }, {
      "referenceID" : 125,
      "context" : "Nash games are closely related to saddle point problems [129, 130, 135].",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 126,
      "context" : "Nash games are closely related to saddle point problems [129, 130, 135].",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 131,
      "context" : "Nash games are closely related to saddle point problems [129, 130, 135].",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : "Many equilibria problems in economics can be modeled using VIs [12].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 121,
      "context" : "The extragradient method of Korpolevich [125] addresses some of these concerns, and is defined as Algorithm 2 below.",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 132,
      "context" : "3: Left: This figure illustrates a VI where the basic projection algorithm (Algorithm 1) fails, but the extragradient algorithm (Algorithm 2) succeeds [136].",
      "startOffset" : 151,
      "endOffset" : 156
    }, {
      "referenceID" : 133,
      "context" : ", see [137, 138, 139, 140, 141, 142].",
      "startOffset" : 6,
      "endOffset" : 36
    }, {
      "referenceID" : 134,
      "context" : ", see [137, 138, 139, 140, 141, 142].",
      "startOffset" : 6,
      "endOffset" : 36
    }, {
      "referenceID" : 135,
      "context" : ", see [137, 138, 139, 140, 141, 142].",
      "startOffset" : 6,
      "endOffset" : 36
    }, {
      "referenceID" : 136,
      "context" : ", see [137, 138, 139, 140, 141, 142].",
      "startOffset" : 6,
      "endOffset" : 36
    }, {
      "referenceID" : 137,
      "context" : ", see [137, 138, 139, 140, 141, 142].",
      "startOffset" : 6,
      "endOffset" : 36
    }, {
      "referenceID" : 138,
      "context" : ", see [137, 138, 139, 140, 141, 142].",
      "startOffset" : 6,
      "endOffset" : 36
    }, {
      "referenceID" : 134,
      "context" : "Khobotov [138] proved that the extragradient method converges under the weaker requirement of pseudo-monotone mappings, 6 when the learning rate is automatically adjusted based on a local measure of the Lipschitz constant.",
      "startOffset" : 9,
      "endOffset" : 14
    }, {
      "referenceID" : 133,
      "context" : "Iusem [137] proposed a variant whereby the current iterate was projected onto a hyperplane separating the current iterate from the final solution, and subsequently projected from the hyperplane onto the feasible set.",
      "startOffset" : 6,
      "endOffset" : 11
    }, {
      "referenceID" : 138,
      "context" : "Solodov and Svaiter [142] proposed another hyperplane method, whereby the current iterate is projected onto the intersection of the hyperplane and the feasible set.",
      "startOffset" : 20,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "Finally, the extragradient method was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called “mirrror-prox” algorithm [129].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 125,
      "context" : "Finally, the extragradient method was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called “mirrror-prox” algorithm [129].",
      "startOffset" : 180,
      "endOffset" : 185
    }, {
      "referenceID" : 30,
      "context" : "Variational inequalities also provide a useful framework for reinforcement learning [32, 1].",
      "startOffset" : 84,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Variational inequalities also provide a useful framework for reinforcement learning [32, 1].",
      "startOffset" : 84,
      "endOffset" : 91
    }, {
      "referenceID" : 139,
      "context" : "Generalizing this, consider solving for the fixed point of a projected equation x∗ = ΠŜT (x ∗) [143, 144] for a functional mapping T : Rn → Rn, where ΠŜ is the projector onto a low-dimensional convex subspace Ŝ w.",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 140,
      "context" : "Generalizing this, consider solving for the fixed point of a projected equation x∗ = ΠŜT (x ∗) [143, 144] for a functional mapping T : Rn → Rn, where ΠŜ is the projector onto a low-dimensional convex subspace Ŝ w.",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 139,
      "context" : "Following [143], note that this is a variational inequality of the form 〈F (x∗), (x−x∗)〉 ≥ 0 if we identify F (x) = Ξ(x−T (x)), and in the lowerdimensional space, 〈F (Φr∗),Φ(r− r∗)〉, ∀r ∈ R̂.",
      "startOffset" : 10,
      "endOffset" : 15
    }, {
      "referenceID" : 139,
      "context" : "It is shown in [143] that if T is a contraction mapping, then F (x) = Ξ(x − T (x)) is strongly monotone.",
      "startOffset" : 15,
      "endOffset" : 20
    }, {
      "referenceID" : 140,
      "context" : "Gordon [144] proposes an alternative approach separating the projection of the current iterate on the low-dimensional subspace spanned by Φ from its projection onto the feasible set.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 139,
      "context" : "Both of these approaches [143, 144] have been only studied with the simple projection method (Algorithm 1), and can be generalized to a more powerful class of VI methods that we are currently developing.",
      "startOffset" : 25,
      "endOffset" : 35
    }, {
      "referenceID" : 140,
      "context" : "Both of these approaches [143, 144] have been only studied with the simple projection method (Algorithm 1), and can be generalized to a more powerful class of VI methods that we are currently developing.",
      "startOffset" : 25,
      "endOffset" : 35
    }, {
      "referenceID" : 45,
      "context" : "We first present the monotone operator corresponding to the bilinear saddle-point problem and then extend it to stochastic approximation case with certain restrictive assumptions, and use the result in [47].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 105,
      "context" : "d noise, then with the result in [109], we can prove that the RO-TD algorithm converges to the global minimizer of",
      "startOffset" : 33,
      "endOffset" : 38
    }, {
      "referenceID" : 75,
      "context" : "With the subgradient boundedness assumption and using the result in Proposition 1 in [78], this can be proved.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 24,
      "context" : "We first present the assumptions for the MDP and basis functions, which are similar to [26, 14].",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "We first present the assumptions for the MDP and basis functions, which are similar to [26, 14].",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 104,
      "context" : "Next we present the assumptions for the stochastic saddle point problem formulation, which are similar to [108, 109].",
      "startOffset" : 106,
      "endOffset" : 116
    }, {
      "referenceID" : 105,
      "context" : "Next we present the assumptions for the stochastic saddle point problem formulation, which are similar to [108, 109].",
      "startOffset" : 106,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : ", ∀θ, θ′ ∈ X,λ ∈ [0, 1], K(λθ + (1− λ)θ)≤CKλK(θ) + (1− λ)K(θ′),",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 54,
      "context" : "The corresponding primal-dual formulation [57, 28, 58] of Equation (8.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "The corresponding primal-dual formulation [57, 28, 58] of Equation (8.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 55,
      "context" : "The corresponding primal-dual formulation [57, 28, 58] of Equation (8.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 141,
      "context" : "Using the bounds proved in [145, 109, 108], the optimal convergence rate of stochastic saddle-point problem is O(F N2 + LK N + σ √ N ).",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 105,
      "context" : "Using the bounds proved in [145, 109, 108], the optimal convergence rate of stochastic saddle-point problem is O(F N2 + LK N + σ √ N ).",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 104,
      "context" : "Using the bounds proved in [145, 109, 108], the optimal convergence rate of stochastic saddle-point problem is O(F N2 + LK N + σ √ N ).",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 142,
      "context" : "Hence, by adding the primal average step, GTD/GTD2 algorithms will become standard Polyak’s algorithms [146], and thus the convergence rates are O(FK √ N ) ac-",
      "startOffset" : 103,
      "endOffset" : 108
    }, {
      "referenceID" : 47,
      "context" : "cording to [49].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 105,
      "context" : "According to [109] which extends the SMP algorithm to solving saddle-point problems and variational inequality problems, the convergence rate is accelerated to O(FK N + σ √ N ).",
      "startOffset" : 13,
      "endOffset" : 18
    }, {
      "referenceID" : 107,
      "context" : "Here we use the result in [111], which is similar to the one in [147].",
      "startOffset" : 26,
      "endOffset" : 31
    }, {
      "referenceID" : 143,
      "context" : "Here we use the result in [111], which is similar to the one in [147].",
      "startOffset" : 64,
      "endOffset" : 69
    }, {
      "referenceID" : 107,
      "context" : "Lemma 2 [111]: For any Vθ = Φθ, the following component-wise equality holds V − Vθ = (I − γΠP )−1 (( V −ΠV ) + Φ(ΦTΞΦ)−1K(θ) )",
      "startOffset" : 8,
      "endOffset" : 13
    } ],
    "year" : 2014,
    "abstractText" : "Reinforcement learning is a simple, and yet, comprehensive theory of learning that simultaneously models the adaptive behavior of artificial agents, such as robots and autonomous software programs, as well as attempts to explain the emergent behavior of biological systems. It also gives rise to computational ideas that provide a powerful tool to solve problems involving sequential prediction and decision making. Temporal difference learning is the most widely used method to solve reinforcement learning problems, with a rich history dating back more than three decades. For these and many other reasons, devel1 This article is currently not under review for the journal Foundations and Trends in ML, but will be submitted for formal peer review at some point in the future, once the draft reaches a stable “equilibrium” state. ar X iv :1 40 5. 67 57 v1 [ cs .L G ] 2 6 M ay 2 01 4 oping a complete theory of reinforcement learning, one that is both rigorous and useful has been an ongoing research investigation for several decades. In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified “safely” guarantees, and remains in a stable region of the parameter space (iii) how to design “off-policy” temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators. The most important idea that emerges is the use of primal dual spaces connected through the use of a Legendre transform. This allows temporal difference updates to occur in dual spaces, allowing a variety of important technical advantages. The Legendre transform, as we show, elegantly generalizes past algorithms for solving reinforcement learning problems, such as natural gradient methods, which we show relate closely to the previously unconnected framework of mirror descent methods. Equally importantly, proximal operator theory enables the systematic development of operator splitting methods that show how to safely and reliably decompose complex products of gradients that occur in recent variants of gradient-based temporal difference learning. This key technical innovation makes it possible to finally design “true” stochastic gradient methods for reinforcement learning. Finally, Legendre transforms enable a variety of other benefits, including modeling sparsity and domain geometry. Our work builds extensively on recent work on the convergence of saddle-point algorithms, and on the theory of monotone operators in Hilbert spaces, both in optimization and for variational inequalities. The latter framework, the subject of another ongoing investigation by our group, holds the promise of an even more elegant framework for reinforcement learning. Its explication is currently the topic of a further monograph that will appear in due course. Dedicated to Andrew Barto and Richard Sutton for inspiring a generation of researchers to the study of reinforcement learning. Algorithm 1 TD (1984) (1) δt = rt + γφ ′ t T θt − φt θt (2) θt+1 = θt + βtδt Algorithm 2 GTD2-MP (2014) (1) wt+ 1 2 = wt + βt(δt − φt wt)φt, θt+ 1 2 = proxαth ( θt + αt(φt − γφt)(φt wt) ) (2) δt+ 1 2 = rt + γφ ′ t T θt+ 1 2 − φt θt+ 1 2 (3) wt+1 = wt + βt(δt+ 1 2 − φt wt+ 1 2 )φt , θt+1 = proxαth ( θt + αt(φt − γφt)(φt wt+ 1 2 ) )",
    "creator" : "LaTeX with hyperref package"
  }
}