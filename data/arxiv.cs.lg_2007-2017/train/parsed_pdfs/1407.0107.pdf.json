{
  "name" : "1407.0107.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Randomized Block Coordinate Descent for Online and Stochastic Optimization",
    "authors" : [ "Huahua Wang" ],
    "emails" : [ "huwang@cs.umn.edu", "banerjee@cs.umn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 7.\n01 07\nv1 [\ncs .L\nG ]\n1 J\nul 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, considerable efforts in machine learning have been devoted to solving the following composite objective minimization problem:\nmin x\nf(x) + g(x) = 1\nI\nI ∑\ni=1\nfi(x) + J ∑\nj=1\ngj(xj) , (1)\nwhere x ∈ Rn×1 and xj is a block coordinate of x. f(x) is the average of some smooth functions, and g(x) is a simple function which may be non-smooth. In particular, g(x) is block separable and blocks are non-overlapping. A variety of machine learning and statistics problems can be cast into the problem (1). In regularized risk minimization problems [10], f is the average of losses of a large number of samples and g is a simple regularizer on high dimensional features to induce structural sparsity [1]. While f is separable among samples, g is separable among features. For example, in lasso [32], fi is a square loss or logistic loss function and g(x) = λ‖x‖1 where λ is the tuning parameter. In group lasso [37], gj(xj) = λ1‖xj‖2, which enforces group sparsity among variables. To induce both group sparsity and sparsity, sparse group lasso [9] uses composite regularizers gj(xj) = λ1‖xj‖2 + λ2‖xj‖1.\nDue to the simplicity, gradient descent (GD) type methods have been widely used to solve problem (1). If gj is nonsmooth but simple enough for proximal mapping [], it is better to just use the gradient of fi but keep gj untouched in GD. This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8]. Without loss of generality, we\nsimply use GD to represent GD and its variants in the rest of this paper. Let m be the number of samples and n be dimension of features. m samples are divided into I blocks (mini-batch), and n features are divided into J non-overlapping blocks. If both m and n are large, solving (1) using batch methods like gradient descent (GD) type methods is computationally expensive. To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.\nInstead of computing gradients of all samples in GD at each iteration, OGD/SGD only computes the gradient of one block samples, and thus the cost-per-iteration is just one I-th of GD. For large scale problems, it has been shown that OGD/SGD is faster than GD [31, 29, 30]. OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35]. OGD and SGD use a decreasing step size and converge at a slower rate than GD. In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38]. Stochastic average gradient (SVG) [27] is the first SGD algorithm achieving the linear convergence rate for stronly convex functions, catching up with the convergence speed of GD [21]. However, SVG needs to store all gradients, which becomes an issue for large scale datasets. It is also difficult to understand the intuition behind the proof of SVG. To address the issue of storage and better explain the faster convergence, [13] proposed an explicit variance reduction scheme into SGD. The two scheme SGD is refered as stochastic variance reduction gradient (SVRG). SVRG computes the full gradient periodically and progressively mitigates the variance of stochastic gradient by removing the difference between the full gradient and stochastic gradient. For smooth and strongly convex functions, SVRG converges at a geometric rate in expectation. Compared to SVG, SVRG is free from the storage of full gradients and has a much simpler proof. The similar idea was also proposed independently by [19]. The results of SVRG is then improved in [15]. In [36], SVRG is generalized to solve composite minimization problem by incorporate the variance reduction technique into proximal gradient method.\nOn the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers. RBCD randomly chooses a block coordinate to update at each iteration. The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17]. RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17]. Compared to GD, the cost-per-iteration of RBCD is much cheaper. Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18]. Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].\nWhile OGD/SGD is well suitable for problems with a large number of samples, RBCD is suitable for high dimension problems with non-overlapping composite regularizers. For large scale high dimensional problems, particular with non-overlapping composite regularizers, it is more economic to combine the two type of methods together. In this paper, we propose a new method named online randomized overlapping block coordinate descent (ORBCD) which combines the well-known OGD/SGD and RBCD together for the first time. ORBCD first randomly picks up one block samples and one block coordinates, then performs the block coordinate gradient descent on the randomly chosen samples at each iteration. Essentially, ORBCD performs RBCD in the online and stochastic setting. If fi is a linear function, the cost-per-iteration of ORBCD is O(1) and thus is far smaller than O(n) in OGD/SGD and O(m) in RBCD. We show that the iteration complexity for ORBCD has the same order as OGD/SGD. In the stochastic setting, ORBCD is still suffered from the variance of stochastic gradient. To accelerate the convergence speed of ORBCD, we adopt the varaince reduction technique [13] to alleviate the effect of randomness. As expected, the\nlinear convergence rate for ORBCD with variance reduction (ORBCDVD) is established for strongly convex functions for stochastic optimization. Moreover, ORBCDVD does not necessarily require to compute the full gradient at once which is necessary in SVRG and prox-SVRG. Instead, µ̃ can be partially computed at each iteration and then stored for the next retrieval, which may be useful when the computation of full gradient is expensive or data is partially available at the moment [23].\nThe rest of the paper is organized as follows. In Section 2, we review the SGD and RBCD. ORBCD and ORBCD with variance reduction are proposed in Section 3. The convergence results are given in Section 4. The paper is concluded in Section 5."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we briefly review the two types of low cost-per-iteration gradient descent (GD) methods, i.e., OGD/SGD and RBCD. Applying GD on (1), we have the following iterate:\nxt+1 = argmin x 〈∇f(xt),x〉+ g(x) + ηt\n2 ‖x− xt‖22 . (2)\nIn some cases, e.g. g(x) is ℓ1 norm, (2) can have a closed-form solution."
    }, {
      "heading" : "2.1 Online and Stochastic Gradient Descent",
      "text" : "In (2), it requires to compute the full gradient of m samples at each iteration, which could be computationally expensive if m is too large. Instead, OGD/SGD simply computes the gradient of one block samples.\nIn the online setting, at time t+ 1, OGD first presents a solution xt+1 by solving\nxt+1 = argmin x 〈∇ft(xt),x〉+ g(x) + ηt 2 ‖x− xt‖22 . (3)\nwhere ft is given and assumed to be convex. Then a function ft+1 is revealed which incurs the loss ft(xt). The performance of OGD is measured by the regret bound, which is the discrepancy between the cumulative loss over T rounds and the best decision in hindsight,\nR(T ) = T ∑\nt=1\n[ft(x t) + g(xt)]− [ft(x∗) + g(x∗)] , (4)\nwhere x∗ is the best result in hindsight. The regret bound of OGD is O( √ T ) when using decreasing step size ηt = O( 1√t). For strongly convex functions, the regret bound of OGD is O(log T ) when using the step size ηt = O(1t ). Since ft can be any convex function, OGD considers the worst case and thus the mentioned regret bounds are optimal.\nIn the stochastic setting, SGD first randomly picks up it-th block samples and then computes the gradient of the selected samples as follows:\nxt+1 = argmin x 〈∇fit(xt),x〉+ g(x) + ηt 2 ‖x− xt‖22 . (5)\nxt depends on the observed realization of the random variable ξ = {i1, · · · , it−1} or generally {x1, · · · ,xt−1}. Due to the effect of variance of stochastic gradient, SGD has to choose decreasing step size, i.e., ηt = O( 1√t), leading to slow convergence speed. For general convex functions, SGD converges at a rate of O( 1√ t ). For\nstrongly convex functions, SGD converges at a rate of O(1 t ). In contrast, GD converges linearly if functions are strongly convex. To accelerate the SGD by reducing the variance of stochastic gradient, stochastic variance reduced gradient (SVRG) was proposed by [13]. [36] extends SVRG to composite functions (1), called prox-SVRG. SVRGs have two stages, i.e., outer stage and inner stage. The outer stage maintains an estimate x̃ of the optimal point x∗ and computes the full gradient of x̃\nµ̃ = 1\nn\nn ∑\ni=1\n∇fi(x̃) = ∇f(x̃) . (6)\nAfter the inner stage is completed, the outer stage updates x̃. At the inner stage, SVRG first randomly picks it-th sample, then modifies stochastis gradient by subtracting the difference between the full gradient and stochastic gradient at x̃,\nvt = ∇fit(xt)−∇fit(x̃) + µ̃ . (7)\nIt can be shown that the expectation of vt given xt−1 is the full gradient at xt, i.e., Evt = ∇f(xt). Although vt is also a stochastic gradient, the variance of stochastic gradient progressively decreases. Replacing ∇fit(xt) by vt in SGD step (5),\nxt+1 = argmin x 〈vt,x〉+ g(x) +\nη 2 ‖x− xt‖22 . (8)\nBy reduding the variance of stochastic gradient, xt can converge to x∗ at the same rate as GD, which has been proved in [13, 36]. For strongly convex functions, prox-SVRG [36] can converge linearly in expectation if η > 4L and m satisfy the following condition:\nρ = η2 γ(η − 4L)m + 4L(m+ 1) (η − 4L)m < 1 . (9)\nwhere L is the constant of Lipschitz continuous gradient. Note the step size is 1/η here."
    }, {
      "heading" : "2.2 Randomized Block Coordinate Descent",
      "text" : "Assume xj(1 ≤ j ≤ J) are non-overlapping blocks. At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem:\nxt+1jt = argminxjt 〈∇jtf(x t),xjt〉+ gjt(xjt) + ηt 2 ‖xjt − xtjt‖22 . (10)\nTherefore, xt+1 = (xt+1jt ,x t k 6=jt). x t depends on the observed realization of the random variable\nξ = {j1, · · · , jt−1} . (11)\nSetting the step size ηt = Ljt where Ljt is the Lipshitz constant of jt-th coordinate of the gradient ∇f(xt), the iteration complexity of RBCD is O(1\nt ). For strongly convex function, RBCD has a linear convergence\nrate. Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17]."
    }, {
      "heading" : "3 Online Randomized Block Coordinate Descent",
      "text" : "In this section, our goal is to combine OGD/SGD and RBCD together to solve problem (1). We call the algorithm online randomized block coordinate descent (ORBCD), which computes one block coordinate of the gradient of one block of samples at each iteration. ORBCD essentially performs RBCD in online and stochastic setting.\nLet {x1, · · · ,xJ},xj ∈ Rnj×1 be J non-overlapping blocks of x. Let Uj ∈ Rn×nj be nj columns of an n× n permutation matrix U, corresponding to j block coordinates in x. For any partition of x and U,\nx =\nJ ∑\nj=1\nUjxj ,xj = U T j x . (12)\nThe j-th coordinate of gradient of f can be denoted as\n∇jf(x) = UTj ∇f(x) . (13)\nThroughout the paper, we assume that the minimum of problem (1) is attained. In addition, ORBCD needs the following assumption :\nAssumption 1 ft or f has block-wise Lipschitz continuous gradient with constant Lj , e.g.,\n‖∇jft(x+ Ujhj)−∇jft(x)‖2 ≤ Lj‖hj‖2 , (14)\nAssumption 2 1. ‖∇ft(xt)‖2 ≤ Rf , or ‖∇f(xt)‖2 ≤ Rf ; 2. xt is assumed in a bounded set X , i.e., sup\nx,y∈X ‖x− y‖2 = D.\nWhile the Assumption 1 is used in RBCD, the Assumption 2 is used in OGD/SGD. We may assume the sum of two functions is strongly convex.\nAssumption 3 ft(x) + g(x) or f(x) + g(x) is γ-strongly convex, e.g., we have\nft(x) + g(x) ≥ ft(y) + g(y) + 〈∇ft(y) + g′(y),x − xt〉+ γ\n2 ‖x− y‖22 . (15)\nwhere γ > 0 and g′(y) denotes the subgradient of g at y."
    }, {
      "heading" : "3.1 ORBCD for Online Learning",
      "text" : "In online setting, ORBCD considers the worst case and runs at rounds. At time t, given any function ft which may be agnostic, ORBCD randomly chooses jt-th block coordinate and presents the solution by solving the following problem:\nxt+1jt = argminxjt 〈∇jtft(x t),xjt〉+ gjt(xjt) + ηt 2 ‖xjt − xtjt‖22\n= Proxgjt (xjt − 1\nηt ∇jtft(xt)) , (16)\nwhere Prox denotes the proximal mapping. If ft is a linear function, e.g., ft = ltxt, then ∇jtft(xt) = ljt , so solving (16) is J times cheaper than OGD. Thus, xt+1 = (xt+1jt ,x t k 6=jt), or\nxt+1 = xt + Ujt(x t+1 jt − xtjt) . (17)\nAlgorithm 1 Online Randomized Block Coordinate Descent for Online Learning\n1: Initialization: x1 = 0 2: for t = 1 to T do 3: randomly pick up jt block coordinates 4: xt+1jt = argminxjt∈Xj 〈∇jtft(x\nt),xjt〉+ gjt(xjt) + ηt2 ‖xjt − xtjt‖22 5: xt+1 = xt + Ujt(x t+1 jt\n− xtjt) 6: receives the function ft+1(x) + g(x) and incurs the loss ft+1(xt+1) + g(xt+1) 7: end for\nThen, ORBCD receives a loss function ft+1(x) which incurs the loss ft+1(xt+1). The algorithm is summarized in Algorithm 1.\nxt is independent of jt but depends on the sequence of observed realization of the random variable\nξ = {j1, · · · , jt−1}. (18)\nLet x∗ be the best solution in hindsight. The regret bound of ORBCD is defined as\nR(T ) =\nT ∑\nt=1\n{ Eξ[ft(x t) + g(xt)]− [ft(x∗) + g(x∗)] } . (19)\nBy setting ηt = √ t+ L where L = maxj Lj , the regret bound of ORBCD is O( √ T ). For strongly convex functions, the regret bound of ORBCD is O(log T ) by setting ηt = γt J + L."
    }, {
      "heading" : "3.2 ORBCD for Stochastic Optimization",
      "text" : "In the stochastic setting, ORBCD first randomly picks up it-th block sample and then randomly chooses jt-th block coordinate. The algorithm has the following iterate:\nxt+1jt = argminxjt 〈∇jtfit(x t),xjt〉+ gjt(xjt) + ηt 2 ‖xjt − xtjt‖22\n= Proxgjt (xjt −∇jtfit(x t)) . (20)\nFor high dimensional problem with non-overlapping composite regularizers, solving (20) is computationally cheaper than solving (5) in SGD. The algorithm of ORBCD in both settings is summarized in Algorithm 2.\nxt+1 depends on (it, jt), but jt and it are independent. xt is independent of (it, jt) but depends on the observed realization of the random variables\nξ = {(i1, j1), · · · , (it−1, jt−1)} . (21)\nThe online-stochastic conversion rule [7, 8, 35] still holds here. The iteration complexity of ORBCD can be obtained by dividing the regret bounds in the online setting by T . Setting ηt = √ t+L where L = maxj Lj , the iteration complexity of ORBCD is\nEξ[f(x̄ t) + g(x̄t)]− [f(x) + g(x)] ≤ O( 1√\nT ) . (22)\nAlgorithm 2 Online Randomized Block Coordinate Descent for Stochastic Optimization\n1: Initialization: x1 = 0 2: for t = 1 to T do 3: randomly pick up it block samples and jt block coordinates 4: xt+1jt = argminxjt∈Xj 〈∇jtfit(x\nt),xjt〉+ gjt(xjt) + ηt2 ‖xjt − xtjt‖22 5: xt+1 = xt + Ujt(x t+1 jt\n− xtjt) 6: end for\nAlgorithm 3 Online Randomized Block Coordinate Descent with Variance Reduction 1: Initialization: x1 = 0 2: for t = 1 to T do 3: x0 = x̃ = x\nt. 4: for k = 0 to m− 1 do 5: randomly pick up ik block samples 6: randomly pick up jk block coordinates 7: v\nik jk = ∇jkfik(xk)−∇jkfik(x̃) + µ̃jk where µ̃jk = ∇jkf(x̃) 8: xkjk = argminxjk 〈vikjk ,xjk〉+ gjk(xjk) + ηk 2 ‖xjk − xkjk‖ 2 2 9: xk+1 = xk + Ujk(x k+1 jj\n− xkjk) 10: end for 11: xt+1 = 1\nm ∑m k=1 x k\n12: end for\nFor strongly convex functions, setting ηt = γt J + L,\nEξ[f(x̄ t) + g(x̄t)]− [f(x) + g(x)] ≤ O( log T\nT ) . (23)\nThe iteration complexity of ORBCD match that of SGD. Simiarlar as SGD, the convergence speed of ORBCD is also slowed down by the variance of stochastic gradient."
    }, {
      "heading" : "3.3 ORBCD with variance reduction",
      "text" : "In the stochastic setting, we apply the variance reduction technique [36, 13] to accelerate the rate of convergence of ORBCD, abbreviated as ORBCDVD. As SVRG and prox-SVRG, ORBCDVD consists of two stages. At time t+1, the outer stage maintains an estimate x̃ = xt of the optimal x∗ and updates x̃ every m iterations. The inner stage takes m iterations which is indexed by k = 0, · · · ,m− 1. At the k-th iteration, ORBCDVD randomly picks ik-th sample and jk-th coordinate and compute\nv ik jk = ∇jkfik(xk)−∇jkfik(x̃) + µ̃jk , (24)\nwhere\nµ̃jk = 1\nn\nn ∑\ni=1\n∇jkfi(x̃) = ∇jkf(x̃) . (25)\nvitjt depends on (it, jt), and it and jt are independent. Conditioned on x k, taking expectation over ik, jk gives\nEv ik jk = EikEjk [∇jkfik(xk)−∇jkfik(x̃) + µ̃jk ]\n= 1\nJ Eik [∇fik(xk)−∇fik(x̃) + µ̃]\n= 1\nJ ∇f(xk) . (26)\nAlthough vikjk is stochastic gradient, the variance E‖v ik jk − ∇jkf(xk)‖22 decreases progressively and is bounded by\nE‖vikjk −∇jkfik(x k)‖22 ≤\n4L\nJ [h(xk)− h(x∗) + h(x̃)− h(x∗)] , (27)\nwhich is much smaller than E‖∇fit(xt) − ∇f(xt)‖22. Using the variance reduced gradient vikjk , ORBCD then performs RBCD as follows:\nxk+1jk = argminxjk 〈vikjk ,xjk〉+ gjk(xjk) +\nη 2 ‖xjk − xkjk‖ 2 2 . (28)\nAfter m iterations, the outer stage updates xt+1 = 1 m ∑m k=1 x k. The algorithm is summarized in Algorithm 3. At the outer stage, ORBCDVD does not necessarily require to compute the full gradient at once. The computation of full gradient may require substantial computational eorts, let alone data may be only partially available at the moment [23]. In ORBCD, µ̃ can be partially computed at each iteration and then stored for the next retrieval.\nLet h(x) = f(x) + g(x). Assume η > 3L and m satisfy the following condition:\nρ = 2L η − 3L + (η − L)J (η − 3L)m − 1 m + η(η − L)J (η − 3L)mγ < 1 . (29)\nThen h(x) converges linearly in expectation, i.e.,\nEξh(x t)− h(x∗) ≤ O(ρt) . (30)\nWithout loss of generality, assume L/γ ≥ 1. Setting η = αL in (29) yields\nρ = 2(m− 1) (α− 3)m + (α − 1)J (α− 3)m − 1 m + α(α− 1)LJ (α− 3)mγ < 2 α− 3 + (α+ 1)(α − 1)JL (α− 3)mγ . (31)\nIf η = 11L,m = 100JL/γ, then ρ ≤ 0.4. In particular, if picking up all coordinates (J = 1), m = 100L/γ. For prox-SVRG [36], setting η = 10L and m = 100L/γ, ρ ≈ 5/6 in (9). If also setting η = 11L and m = 100L/γ, ρ ≈ 0.75 in (9), which is almost two times larger than our result ρ = 0.4."
    }, {
      "heading" : "4 The Rate of Convergnce",
      "text" : "The following lemma is a key building block of the proof of the convergence of ORBCD in both online and stochastic setting.\nLemma 1 Let the Assumption 1 and 2 hold. Let xt be the sequences generated by ORBCD. jt is sampled randomly and uniformly from {1, · · · , J}. We have\n〈∇jtft(xt) + g′jt(xtjt),xtjt − xjt〉 ≤ ηt 2 (‖x − xt‖22 − ‖x− xt+1‖22) + R2f 2(ηt − L) + g(xt)− g(xt+1) .\n(32)\nwhere L = maxk Lk.\nProof: The optimality condition is\n〈∇jtft(xt) + ηt(xt+1jt − x t jt ) + g′jt(x t+1 jt ),xt+1jt − xjt〉 ≤ 0 . (33)\nRearranging the terms yields\n〈∇jtft(xt) + g′jt(x t+1 jt ),xt+1jt − xjt〉 ≤ −ηt〈x t+1 jt − xtjt ,x t+1 jt − xjt〉\n≤ ηt 2 (‖xjt − xtjt‖22 − ‖xjt − x t+1 jt ‖22 − ‖xt+1jt − x t jt‖22) = ηt 2 (‖x− xt‖22 − ‖x− xt+1‖22 − ‖xt+1jt − x t jt‖22) , (34)\nwhere the last equality uses xt+1 = (xt+1jt ,x t k 6=jt). By the smoothness of ft, we have\nft(x t+1) ≤ ft(xt) + 〈∇jft(xt),xt+1j − xtj〉+ Lj 2 ‖xt+1j − xtj‖22 . (35)\nSince xt+1 − xt = Ujt(xt+1jt − xtjt), we have\nft(x t+1) + g(xt+1)− [ft(xt) + g(xt)]\n≤ 〈∇jtft(xt),xt+1jt − x t jt 〉+ Ljt\n2 ‖xt+1jt − x t jt ‖22 + gjt(xt+1jt )− gjt(xjt) + gjt(x t jt )− gjt(xjt)\n≤ 〈∇jtft(xt) + g′jt(x t+1 jt ),xt+1jt − xjt〉+ Ljt 2 ‖xt+1jt − x t jt ‖22 − 〈∇jtft(xt) + g′jt(x t jt ),xtjt − xjt〉 ≤ ηt 2 (‖x− xt‖22 − ‖x− xt+1‖22) + Ljt − ηt 2 ‖xt+1jt − x t jt‖22 − 〈∇jtft(xt) + g′jt(xtjt),xtjt − xjt〉 . (36)\nRearranging the terms yields\n〈∇jtft(xt) + g′jt(xt),xtjt − xjt〉 ≤ ηt 2 (‖x− xt‖22 − ‖x− xt+1‖22) + Ljt − ηt 2 ‖xt+1jt − x t jt ‖22\n+ ft(x t) + g(xt)− [ft(xt+1) + g(xt+1)] . (37)\nThe convexity of ft gives\nft(x t)− ft(xt+1) ≤ 〈∇ft(xt),xt − xt+1〉 = 〈∇jtft(xt),xtjt − x t+1 jt 〉 ≤ 1 2α ‖∇jtft(xt)‖22 + α 2 ‖xtjt − x t+1 jt\n‖22 . (38)\nwhere the equality uses xt+1 = (xt+1jt ,x t k 6=jt). Plugging into (37), we have\n〈∇jtft(xt) + g′jt(xtjt),xtjt − xjt〉\n≤ ηt 2 (‖x− xt‖22 − ‖x− xt+1‖22) + Ljt − ηt 2 ‖xt+1jt − x t jt ‖22 + 〈∇jtft(xt),xtjt − x t+1 jt 〉+ g(xt)− g(xt+1) ≤ ηt 2 (‖x− xt‖22 − ‖x− xt+1‖22) + Ljt − ηt 2 ‖xt+1jt − x t jt‖22 + α 2 ‖xtjt − xt+1jt ‖ 2 2 + 1 2α ‖∇jtft(xt)‖22 .\n(39)\nLet L = maxk Lk. Setting α = ηt − L where ηt > L completes the proof. This lemma is also a key building block in the proof of iteration complexity of GD, OGD/SGD and RBCD. In GD, by setting ηt = L, the iteration complexity of GD can be established. In RBCD, by simply setting ηt = Ljt , the iteration complexity of RBCD can be established."
    }, {
      "heading" : "4.1 Online Optimization",
      "text" : "Note xt depends on the sequence of observed realization of the random variable ξ = {j1, · · · , jt−1}. The following theorem establishes the regret bound of ORBCD.\nTheorem 1 Let ηt = √ t+L in the ORBCD and the Assumption 1 and 2 hold. jt is sampled randomly and uniformly from {1, · · · , J}. The regret bound R(T ) of ORBCD is\nR(T ) ≤ J( √ T + L\n2 D2 +\n√ TR2 + g(x1)− g(x∗)) . (40)\nProof: In (32), conditioned on xt, take expectation over jt, we have\n1 J 〈∇ft(xt) + g′(xt),xt − x〉 ≤ ηt 2 (‖x− xt‖22 − E‖x− xt+1‖22) +\nR2\n2(ηt − L) + g(xt)− Eg(xt+1) .\n(41)\nUsing the convexity, we have\nft(x t) + g(xt)− [ft(x) + g(x)] ≤ 〈∇ft(xt) + g′(xt),xt − x〉 . (42)\nTogether with (41), we have\nft(x t) + g(xt)− [ft(x) + g(x)] ≤ J\n{\nηt 2 (‖x− xt‖22 − E‖x− xt+1‖22) +\nR2\n2(ηt − L) + g(xt)− Eg(xt+1)\n}\n.\n(43)\nTaking expectation over ξ on both sides, we have\nEξ\n[\nft(x t) + g(xt)− [ft(x) + g(x)]\n] ≤ J {ηt 2 (Eξ‖x− xt‖22 − Eξ‖x− xt+1‖22)\n+ R2\n2(ηt − L) + Eξg(x\nt)− Eξg(xt+1) } . (44)\nSumming over t and setting ηt = √ t+ L, we obtain the regret bound\nR(T ) =\nT ∑\nt=1\n{\nEξ[ft(x t) + g(xt)]− [ft(x) + g(x)]\n}\n≤ J {\n−ηT 2 Eξ‖x− xT+1‖22 +\nT ∑\nt=1\n(ηt − ηt−1)Eξ‖x− xt‖22 + T ∑\nt=1\nR2\n2(ηt − L) + g(x1)− Eξg(xT+1)\n}\n≤ J {\nηT 2 D2 +\nT ∑\nt=1\nR2\n2(ηt − L) + g(x1)− g(x∗)\n}\n≤ J {√ T + L\n2 D2 +\nT ∑\nt=1\nR2\n2 √ t + g(x1)− g(x∗)\n}\n≤ J( √ T + L\n2 D2 +\n√ TR2 + g(x1)− g(x∗)) , (45)\nwhich completes the proof.\nIf one of the functions is strongly convex, ORBCD can achieve a log(T ) regret bound, which is established in the following theorem.\nTheorem 2 Let the Assumption 1-3 hold and ηt = γt J +L in ORBCD. jt is sampled randomly and uniformly from {1, · · · , J}. The regret bound R(T ) of ORBCD is\nR(T ) ≤ J2R2 log(T ) + J(g(x1)− g(x∗)) . (46)\nProof: Using the strong convexity of ft + g in (15), we have\nft(x t) + g(xt)− [ft(x) + g(x)] ≤ 〈∇ft(xt) + g′(xt),xt − x〉 −\nγ 2 ‖x− xt‖22 . (47)\nTogether with (41), we have\nft(x t) + g(xt)− [ft(x) + g(x)] ≤ Jηt − γ 2 ‖x− xt‖22 − Jηt 2 E‖x− xt+1‖22)\n+ JR2\n2(ηt − L) + J [g(xt)− Eg(xt+1)] . (48)\nTaking expectation over ξ on both sides, we have\nEξ\n[\nft(x t) + g(xt)− [ft(x) + g(x)]\n] ≤ Jηt − γ 2 Eξ‖x− xt‖22 − Jηt 2 Eξ[‖x− xt+1‖22])\n+ JR2\n2(ηt − L) + J [Eξg(x\nt)− Eξg(xt+1)] . (49)\nSumming over t and setting ηt = γt J + L, we obtain the regret bound\nR(T ) =\nT ∑\nt=1\n{\nEξ[ft(x t) + g(xt)]− [ft(x) + g(x)]\n}\n≤ −JηT 2\nEξ‖x− xT+1‖22 + T ∑\nt=1\nJηt − γ − Jηt−1 2 Eξ‖x− xt‖22 + T ∑\nt=1\nJR2\n2(ηt − L) + J(g(x1)− Eξg(xT+1))\n≤ T ∑\nt=1\nJ2R2\n2γt + J(g(x1)− g(x∗))\n≤ J2R2 log(T ) + J(g(x1)− g(x∗)) , (50)\nwhich completes the proof.\nIn general, ORBCD can achieve the same order of regret bound as OGD and other first-order online optimization methods, although the constant could be J times larger."
    }, {
      "heading" : "4.2 Stochastic Optimization",
      "text" : "In the stochastic setting, ORBCD first randomly chooses the it-th block sample and the jt-th block coordinate. jt and it are independent. xt depends on the observed realization of the random variables ξ = {(i1, j1), · · · , (it−1, jt−1)}. The following theorem establishes the iteration complexity of ORBCD for general convex functions. Theorem 3 Let ηt = √ t + L and x̄T = 1\nT ∑T t=1 x t in the ORBCD. it, jt are sampled randomly and uniformly from {1, · · · , I} and {1, · · · , J} respectively. The iteration complexity of ORBCD is\nEξ[f(x̄ t) + g(x̄t)]− [f(x) + g(x)] ≤ J(\n√ T+L 2 D 2 + √ TR2 + g(x1)− g(x∗)) T . (51)\nProof: In the stochastic setting, let ft be fit in (32), we have\n〈∇jtfit(xt) + g′jt(xt),xtjt − xjt〉 ≤ ηt 2 (‖x− xt‖22 − ‖x− xt+1‖22) +\nR2\n2(ηt − L) + g(xt)− g(xt+1) .\n(52)\nNote it, jt are independent of xt. Conditioned on xt, taking expectation over it and jt, the RHS is\nE〈∇jtfit(xt) + g′jt(xt),xtjt − xjt〉 = Eit [Ejt [〈∇jtfit(xt) + g′jt(xt),xtjt − xjt〉]]\n= 1\nJ Eit [〈∇fit(xt),xt − x〉+ 〈g′(xt),xt − x〉]\n= 1\nJ 〈∇f(xt) + g′(xt),xt − x〉 . (53)\nPlugging back into (52), we have\n1 J 〈∇f(xt) + g′(xt),xt − x〉 ≤ ηt 2 (‖x− xt‖22 − E‖x− xt+1‖22) +\nR2\n2(ηt − L) + g(xt)− Eg(xt+1) . (54)\nUsing the convexity of f + g, we have\nf(xt) + g(xt)− [f(x) + g(x)] ≤ 〈∇f(xt) + g′(xt),xt − x〉 . (55)\nTogether with (54), we have\nf(xt) + g(xt)− [f(x) + g(x)] ≤ J { ηt 2 (‖x− xt‖22 − E‖x− xt+1‖22) +\nR2\n2(ηt − L) + g(xt)− Eg(xt+1)\n}\n.\n(56)\nTaking expectation over ξ on both sides, we have\nEξ\n[ f(xt) + g(xt) ] − [f(x) + g(x)] ≤ J {ηt 2 (Eξ‖x− xt‖22 − Eξ[‖x− xt+1‖22])\n+ R2\n2(ηt − L) + Eξg(x\nt)− Eξg(xt+1) } . (57)\nSumming over t and setting ηt = √ t+ L, following similar derivation in (45), we have\nT ∑\nt=1\n{\nEξ[f(x t) + g(xt)]− [f(x) + g(x)]\n} ≤ J( √ T + L\n2 D2 +\n√ TR2 + g(x1)− g(x∗)) . (58)\nDividing both sides by T , using the Jensen’s inequality and denoting x̄T = 1 T ∑T t=1 x t complete the proof.\nFor strongly convex functions, we have the following results.\nTheorem 4 For strongly convex function, setting ηt = γt J + L in the ORBCD. it, jt are sampled randomly and uniformly from {1, · · · , I} and {1, · · · , J} respectively. Let x̄T = 1 T ∑T t=1 x\nt. The iteration complexity of ORBCD is\nEξ[f(x̄ T ) + g(x̄T )]− [f(x) + g(x)] ≤ J 2R2 log(T ) + J(g(x1)− g(x∗)) T . (59)\nProof: If f + g is strongly convex, we have\nf(xt) + g(xt)− [f(x) + g(x)] ≤ 〈∇f(xt) + g′(xt),xt − x〉 − γ 2 ‖x− xt‖22 . (60)\nPlugging back into (54), following similar derivation in Theorem 2 and Theorem 3 complete the proof."
    }, {
      "heading" : "4.3 ORBCD with Variance Reduction",
      "text" : "The following results mostly follow [36, 13].\nLemma 2 Define h(x) = f(x) + g(x). Let x∗ be an optimal solution and L = maxi Li, we have\n1\nI\nI ∑\ni=1\n‖∇fi(x)−∇fi(x∗)‖22 ≤ 2L[h(x) − h(x∗)] . (61)\nLemma 3 Let vikjk and x k+1 jk be generated by (24)-(28). Conditioned on xk, we have\nE‖vikjk −∇jkf(x k)‖22 ≤\n4L\nJ [h(xk)− h(x∗) + h(x̃)− h(x∗)] . (62)\nProof: Conditioned on xk, we have\nEik [∇fik(xk)−∇fik(x̃) + µ̃] = 1\nI\nI ∑\ni=1\n[∇fi(xk)−∇fi(x̃) + µ̃] = ∇f(xk) . (63)\nNote xk is independent of ik, jk. Conditioned on xk, using (24) gives\nE‖vikjk −∇jkf(x k)‖22 = Eik [Ejk‖v ik jk −∇jkf(xk)‖22] = Eik [Ejk‖∇jkfik(xk)−∇jkfik(x̃) + µ̃jk −∇jkf(xk)‖22]\n= 1\nJ Eik‖∇fik(xk)−∇fik(x̃) + µ̃−∇f(xk)‖22\n≤ 1 J Eik‖∇fik(xk)−∇fik(x̃)‖22 ≤ 2 J Eik‖∇fik(xk)−∇fik(x∗)‖22 + 2 J Eik‖∇fik(x̃)−∇fik(x∗)‖22\n= 2\nIJ\nI ∑\ni=1\n‖∇fi(xk)−∇fi(x∗)‖22 + 2\nIJ\nI ∑\ni=1\n‖∇fi(x̃)−∇fi(x∗)‖22\n≤ 4L J [h(xk)− h(x∗) + h(x̃)− h(x∗)] . (64)\nThe first inequality uses the fact E‖ζ − Eζ‖22 ≤ E‖ζ‖22 given a random variable ζ , the second inequality uses ‖a+ b‖22 ≤ 2‖a‖22 + 2‖b‖22, and the last inequality uses Lemma 2.\nNow, we are ready to establish the linear convergence rate of ORBCD with variance reduction for strongly convex functions.\nTheorem 5 Let h(x) = f(x)+ g(x) and xt be generated by ORBCD with variance reduction (25)-(28). jk is sampled randomly and uniformly from {1, · · · , J}. Assume η > 3L and m satisfy the following condition:\nρ = 2L η − 3L + (η − L)J (η − 3L)m − 1 m + η(η − L)J (η − 3L)mγ < 1 . (65)\nThen h(x) converges linearly in expectation, i.e.,\nEξh(x t+1)− h(x∗) ≤ ρ[Eξh(xt)− h(x∗)] . (66)\nProof: The optimality condition of (28) is\n〈vikjk + η(x k+1 jk − xkjk) + g ′ jk (xk+1jk ),x k+1 jk − xjk〉 ≤ 0 . (67)\nRearranging the terms yields\n〈vikjk + g ′ jk (xk+1jk ),x k+1 jk − xjk〉 ≤ −η〈xk+1jk − x k jk ,xk+1jk − xjk〉\n≤ η 2 (‖xjk − xkjk‖ 2 2 − ‖xjk − xk+1jk ‖ 2 2 − ‖xk+1jk − x k jk ‖22) = η\n2 (‖x− xk‖22 − ‖x− xk+1‖22 − ‖xk+1jk − x k jk ‖22) , (68)\nwhere the last equality uses xk+1 = (xk+1jk ,x t k 6=jk). Using the convecxity of gj and the fact that g(x k) − g(xk+1) = gjk(x k)− gjk(xk+1), we have\n〈vikjk ,x k jk − xjk〉+ gjk(xk)− gjk(x) ≤ 〈v ik jk ,xkjk − x k+1 jk 〉+ g(xk)− g(xk+1)\n+ η\n2 (‖x− xk‖22 − ‖x− xk+1‖22 − ‖xk+1jk − x k jk ‖22) . (69)\nSince f is smooth, we have\n〈∇jkf(xk),xkjk − x k+1 jk 〉 ≤ f(xk)− f(xk+1) + L 2 ‖xkjk − x k+1 jk ‖22 . (70)\nLetting x = x∗ and using the smoothness of f , we have\n〈vikjk ,x k jk − xjk〉+ gjk(xk)− gjk(x∗) ≤ 〈v ik jk −∇jkf(xk),xkjk − x k+1 jk 〉+ h(xk)− h(xk+1)\n+ η\n2 (‖x∗ − xk‖22 − ‖x∗ − xk+1‖22 − ‖xk+1jk − x k jk ‖22) +\nL 2 ‖xkjk − x k+1 jk ‖22\n≤ 1 2(η − L)‖v ik jk −∇jkf(xk)‖22 + h(xk)− h(xk+1) + η 2 (‖x∗ − xk‖22 − ‖x∗ − xk+1‖22) , (71)\nTaking expectation over ik, jk on both sides and using Lemma 3, we have\nE[〈vikjk ,x k jk − x∗jk〉+ gjk(x k)− gjk(x∗)]\n≤ 2L J(η − L) [h(x k)− h(x∗) + h(x̃)− h(x∗)] + h(xk)− Eh(xk+1) + η 2 (‖x∗ − xk‖22 − E‖x∗ − xk+1‖22) .\n(72)\nThe left hand side can be rewritten as\nE[〈vikjk ,x k jk − x∗jk〉+ gjk(x k)− gjk(x∗)] =\n1 J [Eik〈vik ,xk − x∗〉+ g(xk)− g(x∗)]\n= 1 J [〈∇f(xk),xk − x∗〉+ g(xk)− g(x∗)] ≥ 1 J [h(xk)− h(x∗)] . (73)\nPlugging into (72) gives\n1 J [h(xk)− h(x∗)] ≤ 2L J(η − L) [h(x k)− h(x∗) + h(x̃)− h(x∗)]\n+ h(xk)− Eh(xk+1) + η 2 (‖x∗ − xk‖22 − E‖x∗ − xk+1‖22) . (74)\nRearranging the terms yields\nη − 3L J(η − L) [h(x k)− h(x∗)] ≤ 2L J(η − L) [h(x̃)− h(x ∗)]\n+ h(xk)− Eh(xk+1) + η 2 (‖x− xk‖22 − E‖x− xk+1‖22) . (75)\nAt time t + 1, we have x0 = x̃ = xt. By running the algorithm m steps, xt+1 = 1m ∑m\nk=1 xk. Summing over k = 0, · · · ,m− 1 and taking expectation with respect to the history of random variable ξ, we have\nη − 3L J(η − L)\nm−1 ∑\nk=0\n[Eξh(xk)− h(x∗)]\n≤ 2Lm J(η − L) [Eξh(x̃)− h(x ∗)] + Eξh(x0)− Eξh(xm) + η 2 (Eξ‖x∗ − x0‖22 − Eξ‖x∗ − xm‖22) . (76)\nRearranging the terms gives\nη − 3L J(η − L)\nm−1 ∑\nk=1\n[Eξh(x k)− h(x∗)] + Eξh(xm)− h(x∗)\n≤ 2Lm J(η − L) [Eξh(x̃)− h(x ∗)] + (1− η − 3L J(η − L) )[Eξh(x0)− h(x ∗)] + η 2 (Eξ‖x∗ − x0‖22 − Eξ‖x∗ − xm‖22) .\n(77)\nSince 0 < η−3L J(η−L) < 1 for any η > 3L and Eh(xm)− h(x∗) ≥ 0, we have\nη − 3L J(η − L)\nm ∑\nk=1\n[Eξh(x k)− h(x∗)]\n≤ 2Lm J(η − L) [Eξh(x̃)− h(x ∗)] + (1− η − 3L J(η − L))[Eξh(x0)− h(x ∗)] + η 2 (‖x − x0‖22 − Eξ‖x− xm‖22) .\n(78)\nApplying the Jesen’s inequality to the left hand side and using h(xt+1) ≤ 1 m ∑m k=1 h(xk) where x t+1 = 1 m ∑m k=1 xk, we have\nη − 3L J(η − L)m[Eξh(x t+1)− h(x∗)] ≤ [ 2Lm J(η − L) + 1− η − 3L J(η − L) ][Eξh(x t)− h(x∗)] + η 2 Eξ‖x∗ − xt‖22 ,\n(79)\nwhere ther right hand side uses xt = x0 = x̃. Assuming h(x) is γ-strongly convex, we have\n‖x∗ − xt‖22 ≤ 2 γ [h(xt)− h(x∗)] . (80)\nPlugging into (79) yields\nη − 3L J(η − L)m[Eξh(x t+1)− h(x∗)] ≤ [ 2Lm J(η − L) + 1− η − 3L J(η − L) + η γ ][Eξh(x t)− h(x∗)] . (81)\nDividing both sides by η−3L J(η−L)m, we have\nEξh(x t+1)− h(x∗) ≤ ρ[Eξh(xt)− h(x∗)] , (82)\nwhere\nρ = 2L η − 3L + (η − L)J (η − 3L)m − 1 m + η(η − L)J (η − 3L)mγ < 1 , (83)\nwhich completes the proof."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We proposed online randomized block coordinate descent (ORBCD) which combines online/stochastic gradient descent and randomized block coordinate descent. ORBCD is well suitable for large scale high dimensional problems with non-overlapping composite regularizers. We established the rate of convergence for ORBCD, which has the same order as OGD/SGD. For stochastic optimization with strongly convex functions, ORBCD can converge at a geometric rate in expectation by reducing the variance of stochastic gradient."
    } ],
    "references" : [ {
      "title" : "Convex Optimization with Sparsity-Inducing Norms",
      "author" : [ "F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Science,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Parallel coordinate descent for l1-regularized loss minimization",
      "author" : [ "J. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Coordinate descent method for large-scale l2-loss linear support vector machines",
      "author" : [ "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Proximal splitting methods in signal processsing. Fixed-Point Algorithms for Inverse Problems in Science and Engineering",
      "author" : [ "P. Combettes", "J. Pesquet" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Composite objective mirror descent",
      "author" : [ "J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "J. Duchi", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "A note on the group lasso and sparse group lasson",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "A dual coordinate descent method for large-scale linear svm",
      "author" : [ "C.-J. Hsieh", "K.-W. Chang", "S. Keerthi C.-J. Lin", "S. Sundararajan" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Juditsky", "A. Nemirovski", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Richtarik. Semi-stochastic gradient descent methods",
      "author" : [ "P.J. Konecny" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Coordinate descent optimization for l1 minimization with application to compressed sensing; a greedy algorithm",
      "author" : [ "Y. Li", "S. Osher" ],
      "venue" : "Inverse Problems and Imaging,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "On the complexity analysis of randomized block-coordinate descent methods",
      "author" : [ "Z. Lu", "L. Xiao" ],
      "venue" : "ArXiv,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "On the convergence of the coordinate descent method for convex differentiable minimization",
      "author" : [ "Z.-Q. Luo", "P. Tseng" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "Mixed optimization for smooth functions",
      "author" : [ "L. Zhang M. Mahdavi", "R. Jin" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "N. Le Roux M. Schmidt", "F. Bach" ],
      "venue" : "Technical Report HAL 00860051,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Basic Course. Kluwer,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2004
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Technical Report 76, Center for Operation Research and Economics (CORE), Catholic University of Louvain (UCL),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization methods",
      "author" : [ "Y. Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "P. Richtarik", "M. Takac" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Parallel coordinate descent methods for big data optimization",
      "author" : [ "P. Richtarik", "M. Takac" ],
      "venue" : "ArXiv,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1951
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "N. Le Roux", "M. Schmidt", "F. Bach" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "On the non-asymptotic convergence of cyclic coordinate descent methods",
      "author" : [ "A. Saha", "A. Tewari" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Stochastic methods for l1 regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "Mini-batch primal and dual methods for svms",
      "author" : [ "M. Takac", "A. Bijral", "P. Richtarik", "N. Srebro" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society B,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1996
    }, {
      "title" : "Convergence of a block coordinate descent method for nondifferentiable minimization",
      "author" : [ "P. Tseng" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2001
    }, {
      "title" : "On accelerated proximal gradient methods for convex-concave optimization",
      "author" : [ "P. Tseng" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2008
    }, {
      "title" : "Dual averaging methods for regularized stochastic learning and online optimization",
      "author" : [ "L. Xiao" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "L. Xiao", "T. Zhang" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Journal of the Royal Statistical Society B,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2007
    }, {
      "title" : "Linear convergence with condition number independent access of full gradients",
      "author" : [ "L. Zhang", "M. Mahdavi", "R. Jin" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2013
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "In regularized risk minimization problems [10], f is the average of losses of a large number of samples and g is a simple regularizer on high dimensional features to induce structural sparsity [1].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "In regularized risk minimization problems [10], f is the average of losses of a large number of samples and g is a simple regularizer on high dimensional features to induce structural sparsity [1].",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 30,
      "context" : "For example, in lasso [32], fi is a square loss or logistic loss function and g(x) = λ‖x‖1 where λ is the tuning parameter.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 35,
      "context" : "In group lasso [37], gj(xj) = λ1‖xj‖2, which enforces group sparsity among variables.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "To induce both group sparsity and sparsity, sparse group lasso [9] uses composite regularizers gj(xj) = λ1‖xj‖2 + λ2‖xj‖1.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 25,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 13,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 37,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 33,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 22,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 208,
      "endOffset" : 223
    }, {
      "referenceID" : 2,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 208,
      "endOffset" : 223
    }, {
      "referenceID" : 24,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 208,
      "endOffset" : 223
    }, {
      "referenceID" : 23,
      "context" : "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.",
      "startOffset" : 208,
      "endOffset" : 223
    }, {
      "referenceID" : 29,
      "context" : "For large scale problems, it has been shown that OGD/SGD is faster than GD [31, 29, 30].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 28,
      "context" : "For large scale problems, it has been shown that OGD/SGD is faster than GD [31, 29, 30].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 32,
      "context" : "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 33,
      "context" : "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].",
      "startOffset" : 219,
      "endOffset" : 243
    }, {
      "referenceID" : 19,
      "context" : "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].",
      "startOffset" : 219,
      "endOffset" : 243
    }, {
      "referenceID" : 34,
      "context" : "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].",
      "startOffset" : 219,
      "endOffset" : 243
    }, {
      "referenceID" : 12,
      "context" : "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].",
      "startOffset" : 219,
      "endOffset" : 243
    }, {
      "referenceID" : 18,
      "context" : "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].",
      "startOffset" : 219,
      "endOffset" : 243
    }, {
      "referenceID" : 36,
      "context" : "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].",
      "startOffset" : 219,
      "endOffset" : 243
    }, {
      "referenceID" : 26,
      "context" : "Stochastic average gradient (SVG) [27] is the first SGD algorithm achieving the linear convergence rate for stronly convex functions, catching up with the convergence speed of GD [21].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 20,
      "context" : "Stochastic average gradient (SVG) [27] is the first SGD algorithm achieving the linear convergence rate for stronly convex functions, catching up with the convergence speed of GD [21].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 12,
      "context" : "To address the issue of storage and better explain the faster convergence, [13] proposed an explicit variance reduction scheme into SGD.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "The similar idea was also proposed independently by [19].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "The results of SVRG is then improved in [15].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 34,
      "context" : "In [36], SVRG is generalized to solve composite minimization problem by incorporate the variance reduction technique into proximal gradient method.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 23,
      "context" : "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 28,
      "context" : "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 27,
      "context" : "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].",
      "startOffset" : 97,
      "endOffset" : 109
    }, {
      "referenceID" : 31,
      "context" : "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].",
      "startOffset" : 97,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].",
      "startOffset" : 97,
      "endOffset" : 109
    }, {
      "referenceID" : 31,
      "context" : "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 27,
      "context" : "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "To accelerate the convergence speed of ORBCD, we adopt the varaince reduction technique [13] to alleviate the effect of randomness.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 22,
      "context" : "Instead, μ̃ can be partially computed at each iteration and then stored for the next retrieval, which may be useful when the computation of full gradient is expensive or data is partially available at the moment [23].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "To accelerate the SGD by reducing the variance of stochastic gradient, stochastic variance reduced gradient (SVRG) was proposed by [13].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 34,
      "context" : "[36] extends SVRG to composite functions (1), called prox-SVRG.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "(8) By reduding the variance of stochastic gradient, x can converge to x∗ at the same rate as GD, which has been proved in [13, 36].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 34,
      "context" : "(8) By reduding the variance of stochastic gradient, x can converge to x∗ at the same rate as GD, which has been proved in [13, 36].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 34,
      "context" : "For strongly convex functions, prox-SVRG [36] can converge linearly in expectation if η > 4L and m satisfy the following condition:",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt 〈∇jtf(x ),xjt〉+ gjt(xjt) + ηt 2 ‖xjt − xtjt‖22 .",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt 〈∇jtf(x ),xjt〉+ gjt(xjt) + ηt 2 ‖xjt − xtjt‖22 .",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt 〈∇jtf(x ),xjt〉+ gjt(xjt) + ηt 2 ‖xjt − xtjt‖22 .",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].",
      "startOffset" : 90,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].",
      "startOffset" : 90,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].",
      "startOffset" : 90,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 33,
      "context" : "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 34,
      "context" : "3 ORBCD with variance reduction In the stochastic setting, we apply the variance reduction technique [36, 13] to accelerate the rate of convergence of ORBCD, abbreviated as ORBCDVD.",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : "3 ORBCD with variance reduction In the stochastic setting, we apply the variance reduction technique [36, 13] to accelerate the rate of convergence of ORBCD, abbreviated as ORBCDVD.",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "The computation of full gradient may require substantial computational eorts, let alone data may be only partially available at the moment [23].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 34,
      "context" : "For prox-SVRG [36], setting η = 10L and m = 100L/γ, ρ ≈ 5/6 in (9).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 34,
      "context" : "3 ORBCD with Variance Reduction The following results mostly follow [36, 13].",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "3 ORBCD with Variance Reduction The following results mostly follow [36, 13].",
      "startOffset" : 68,
      "endOffset" : 76
    } ],
    "year" : 2017,
    "abstractText" : "Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent ( OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, for the first time, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}