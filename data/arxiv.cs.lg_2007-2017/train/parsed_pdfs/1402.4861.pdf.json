{
  "name" : "1402.4861.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A QUASI-NEWTON METHOD FOR LARGE SCALE SUPPORT VECTOR MACHINES",
    "authors" : [ "Aryan Mokhtari", "Alejandro Ribeiro" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Given a training set with points whose class is known the goal of a support vector machine (SVM) is to find a hyperplane that best separates the training set. If future samples are statistically identical to the training set this hyperplane provides the best classification accuracy. Computation of the separating hyperplane entails solution of a convex optimization problem that can be implemented without much difficulty in problems of moderate size [1]. Large scale problems in which the dimension of the points to be classified is large require a commensurably large training set. In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1–4].\nHowever practical, stochastic gradient descent methods need a large number of iterations to converge. This translates into the need of very large training sets, or, since the size of the training set is in general limited by data collection, in the computation of hyperplanes that are not as good classifiers as they could be given the available data. In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set. In particular, we adapt a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) method [9] for the solution of SVM classification problems (Section 2). The proposed method is shown to converge almost surely over realizations of the training set to the optimal classifier (Theorem 1) at a rate that is linear in expectation (Theorem 2). Numerical results show that the method exhibits a convergence time that degrades smoothly with the dimensionality of the feature vectors. (Section 4)."
    }, {
      "heading" : "2. STOCHASTIC QUASI-NEWTON METHOD",
      "text" : "Consider a training set S = {(xi, yi)}Ni=1 containing N pairs of the form (xi, yi), where xi ∈ Rn is a feature vector and yi ∈ {−1, 1} the corresponding vector’s class. We want to find a hyperplane supported by a vector w ∈ Rn which separates the training set so that wTxi > 0 for all points with yi = 1 and wTxi < 0 for all points with yi = −1. Since this vector may not exist if the data is not perfectly separable we introduce the loss function l((x, y);w) measuring the distance between the point xi and the hyperplane supported by w and proceed to select the hyperplane supporting vector as the one with minimum aggregate loss\nw∗ := argmin λ 2 ‖w‖2 + 1 N N∑ i=1 l((xi, yi);w), (1)\nwhere we also added the regularization term λ‖w‖2/2 for some constant λ > 0. The vector w∗ in (1) balances the minimization of the sum of\nSupported by NSF CAREER CCF-0952867 and ONR N00014-12-1-0997.\ndistances to the separating hyperplane, as measured by the loss function l((x, y);w), with the minimization of the L2 norm ‖w‖2 to enforce desirable properties in w∗ [13]. Common selections for the loss function are the squared hinge loss l((x, y);w) = max(0, 1− y(wTx))2 and the log loss l((x, y);v) = log(1 + exp(−y(wTx))), e.g. [1].\nTo model (1) as a stochastic optimization problem let θi := (xi, yi) be a given training point and consider a uniform probability distribution on the training set S = {(xi, yi)}Ni=1 = {θi}Ni=1. Upon defining the function f(w,θ) := λ‖w‖2/2 + l((xi, yi);w) we can rewrite (1) as\nw∗ := argmin w Eθ[f(w,θ)] := argmin w F (w). (2)\nIn (2), we (re-)interpret the sum in (1) as an expectation over the uniform discrete distribution on the set S. We refer to f(w,θ) as the instantaneous functions and to F (w) := Eθ[f(w,θ)] as the average function.\nSince the loss functions l((xi, yi);w) are convex, the functions f(w,θ) := λ‖w‖2/2 + l((xi, yi);w) are strongly convex. Thus, the average objective F (w) in (2) is also strongly convex and the optimal separating hyperplane w∗ can be found by stochastic gradient descent algorithms. However, the number of iterations required to run these algorithms, which translates to the number of training features (xi, yi) that need to be acquired, becomes prohibitive for large dimensional problems. To reduce the number of iterations required for convergence we develop a regularized stochastic version of the BFGS method.\nTo be precise let t ≥ 0 be an iteration index and assume that at time t we are given a sample of L realizations of the random variables θ. Group these samples in the vector θ̃t := [θt1; ...;θtL] and let wt denote the current hyperplane normal vector iterate. We then define the stochastic gradient of F (w) associated with samples θ̃t at point wt as\nŝ(wt, θ̃t) = 1\nL L∑ l=1 ∇f(wt,θtl). (3)\nFurther introduce a step size sequence t, a positive definite curvature approximation matrix B̂t, and a regularization constant Γ > 0. The regularized stochastic BFGS algorithm is then defined by the iteration\nwt+1 = wt − t ( B̂−1t + ΓI ) ŝ(wt, θ̃t). (4)\nThe update in (4) proceeds along the negative stochastic gradient direction −ŝ(wt, θ̃t) premultiplied by the positive definite matrix B̂−1t + ΓI and modulated by the step size t.\nFor the algorithm in (4) to have better convergence properties than gradient descent we need the matrix B̂t to approximate the Hessian of the objective function H(wt) := ∇2F (wt) so that (4) approximates an stochastic version of Newton’s method – the role of ΓI is to provide a guarantee of minimum progress as we discuss in the convergence analysis in Section 3. To define such approximation we use a stochastic version of the secant condition used in deterministic BFGS. Start by defining the variable and stochastic gradient variations at time t as\nvt := wt+1 −wt, r̂t := ŝ(wt+1, θ̃t)− ŝ(wt, θ̃t), (5)\nrespectively, and select the matrix B̂t+1 to be used in the next time step so that it satisfies the secant condition B̂t+1vt = r̂t. The rationale for this selection is that the Hessian H(wt) satisfies this condition for wt+1 tending to wt. Notice however that the secant condition B̂t+1vt = r̂t\nar X\niv :1\n40 2.\n48 61\nv1 [\ncs .L\nG ]\n2 0\nFe b\n20 14\nis not enough to completely specify B̂t+1. To resolve this indeterminacy, matrices B̂t+1 in BFGS are also required to be as close as possible to B̂t in terms of minimizing the Gaussian differential entropy,\nB̂t+1 = argmin Z\ntr [ B̂−1t Z ] − log det [ B̂−1t Z ] − n,\ns. t. Zvt = r̂t, Z 0. (6)\nThe constraint Z 0 restricts the feasible space to positive semidefinite matrices whereas the constraint Zvt = r̂t requires Z to satisfy the secant condition. The objective tr(B̂−1t Z)− log det(B̂−1t Z)−n is the differential entropy between Gaussian variables with covariances B̂t and Z.\nObserve that B̂t+1 stays positive definite as long as the matrix B̂t 0 is positive definite, e.g. [10]. However, it is possible for the smallest eigenvalue of B̂t to become arbitrarily close to zero which means that the largest eigenvalue of B̂−1t becomes very large. To avoid this problem we introduce a regularization of (6) that requires the smallest eigenvalue of B̂t+1 to be larger than a positive constant δ,\nB̂t+1 = argmin Z\ntr [ B̂−1t (Z− δI) ] − log det [ B̂−1t (Z− δI) ] − n,\ns. t. Zvt = r̂t, Z 0. (7)\nSince the logarithm determinant log det[B̂−1t (Z − δI)] diverges as the smallest eigenvalue of Z approaches δ, the smallest eigenvalue of the Hessian approximation matrices B̂t+1 computed as solutions of (7) exceeds the lower bound δ. Thus, the largest eigenvalue of B̂−1t+1 is bounded above by 1/δ. The following lemma shows that solutions of (7) can be computed by a simple algebraic formula (see [14] for proofs of results in this paper).\nLemma 1 Consider the semidefinite program in (7) where the matrix B̂t 0 is positive definite and define the corrected gradient variation\nr̃t := r̂t − δvt, (8)\nIf r̃Tt vt = (r̂t− δvt)Tvt > 0, the solution B̂t+1 of (7) can be written as\nB̂t+1 = B̂t + r̃tr̃\nT t\nvTt r̃t − B̂tvtv\nT t B̂t\nvTt B̂tvt + δI. (9)\nWhen δ = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15]. Therefore, the differences between BFGS and regularized BFGS are the replacement of the gradient variation r̂t by the corrected variation r̃t := r̂t− δvt and the addition of the regularization term δI. Notice that the expression in (9) is the solution to (7) only when the inner product r̃Tt vt = (r̂t − δvt)Tvt > 0."
    }, {
      "heading" : "2.1. Regularized stochastic BFGS support vector machines",
      "text" : "To solve the SVM problem in (1) using regularized stochastic BFGS we need the stochastic gradient in (3). For that, select a sample of L feature vectors x̃ = [x1; ...;xL] and corresponding classes ỹ = [y1; ...; yL] from the training set and compute the stochastic gradient as [cf. (3)]\nŝ(w, (x̃, ỹ)) = λw + 1\nL L∑ i=1 ∇w l((xi, yi);w). (10)\nStart at time t with current iterate wt and recall that B̂t stands for the Hessian approximation computed by stochastic BFGS in the previous iteration. Proceed to collect feature vectors x̃t = [xt1; ...;xtL] and their corresponding class vectors ỹt = [yt1; ...; ytL] and for each pair (x̃t, ỹt) determine the stochastic gradients ŝ(wt, (x̃t, ỹt)) as per (10). Descend then along the direction (B̂−1t + ΓI) ŝ(wt, (x̃t, ỹt)) as per (4). This leads to the next iterate wt+1, but to complete the iteration we still need to compute the updated Hessian approximation B̂t+1. To do so compute the stochastic gradient ŝ(wt+1, (x̃t, ỹt)) associated with the same set of\nAlgorithm 1 Regularized stochastic BFGS support vector machines Require: Variable w0. Hessian approximation B̂0 δI. 1: for t = 0, 1, 2, . . . do 2: Collect L training points x̃t = [xt1, . . . ,xtL] and ỹt = [yt1, . . . , ytL] 3: Compute stochastic gradient ŝ(wt, (x̃t, ỹt)) [cf. (10)].\nŝ(wt, (x̃t, ỹt)) = λwt + 1\nL L∑ i=1 ∇wl((xti, yti);wt).\n4: Descend along direction (B̂−1t + ΓI) ŝ(wt, (x̃t, ỹt)) [cf. (4)]\nwt+1 = wt − t (B̂−1t + ΓI) ŝ(wt, (x̃t, ỹt)). 5: Compute ŝ(wt+1, (x̃t, ỹt)) [cf. (10)]\nŝ(wt+1, (x̃t, ỹt)) = λwt+1 + 1\nL L∑ i=1 ∇w l((xti, yti);wt+1).\n6: Variable and modified stochastic gradient variations [cf. (5) and (8)]\nvt = wt+1 −wt, r̃t = ŝ(wt+1, (x̃t, ỹt))− ŝ(wt, (x̃t, ỹt))− δvt\n7: Update Hessian approximation matrix [cf. (9)]\nB̂t+1 = B̂t + r̃tr̃Tt vTt r̃t − B̂tvtvTt B̂t vTt B̂tvt + δI.\n8: end for\nrandom data points samples (x̃t, ỹt) used to compute the stochastic gradient ŝ(wt, (x̃t, ỹt)). The stochastic gradient variation r̂t, the variable variation vt, and the modified stochastic gradient variation r̃t at time t are now computed using (5) and (8). The Hessian approximation B̂t+1 for the next iteration is defined as the matrix that satisfies the stochastic secant condition B̂t+1vt = r̂t and is closest to B̂t in the sense of (7). As per Lemma 1 we can compute B̂t+1 using (9).\nThe solution of (1) using regularized stochastic BFGS is summarized in Algorithm 1. The two core steps in each iteration are the descent in Step 4 and the update of the Hessian approximation B̂t in Step 8. Step 2 comprises the observation of L pairs of data points and feature vectors that are required to compute the stochastic gradients in steps 3 and 5. The stochastic gradient ŝ(wt, (x̃t, ỹt)) in Step 3 is used in the descent iteration in Step 4. The stochastic gradient of Step 3 along with the stochastic gradient ŝ(wt+1, (x̃t, ỹt)) of Step 5 are used to compute the variations in steps 6 and 7 that permit carrying out the update of the Hessian approximation B̂t in Step 8. Iterations are initialized with arbitrary vector w0 and matrix B̂0 having all eigenvalues larger than δ.\n3. CONVERGENCE ANALYSIS\nOur goal here is to show that as time progresses the sequence of classifiers wt approaches the optimal classifier w∗. In proving this result we make the following assumptions.\nAssumption 1 For any set of samples θ̃ = [θ1, . . . ,θL] the instantaneous functions f̂(w, θ̃) := (1/L) ∑L l=1 f(w,θl) are twice differentiable and their Hessians Ĥ(w, θ̃) = ∇2wf̂(w, θ̃) have lower and upper bounded eigenvalues,\nm̃I Ĥ(w, θ̃) M̃I. (11)\nAssumption 2 There exists a constant S2 such that for all variables w the second moment of the norm of the stochastic gradient satisfies\nEθ [ ‖ŝ(wt, θ̃t)‖2 ] ≤ S2, (12)\nAssumption 3 The regularization constant δ is smaller than the smallest Hessian eigenvalue m̃, i.e., δ < m̃.\nRecall that according to Lemma 1 the update in (9) is a solution to (7) as long as the inner product (r̂t − δvt)Tvt = r̃Tt vt > 0 is positive. Our first result is to show that selecting δ < m̃ as required by Assumption 3 guarantees that this inequality is satisfied for all times t.\nLemma 2 Consider the modified stochastic gradient variation r̃t defined in (8) and the variable variation vt defined in (5). If assumptions 1 and 3 are true, then, for all times t it holds\nr̃Tt vt = (r̂t − δvt)Tvt ≥ (m̃− δ)‖vt‖2 > 0. (13)\nThe result in Lemma 2 guarantees that the regularized stochastic BFGS algorithm as defined by recursive application of (4), (5), (8), and (9) results in matrices B̂t that solve (7). In particular, this implies that B̂t is positive definite with smallest eigenvalue not smaller than δ, i.e., B̂t δI. This implies that all the eigenvalues of B̂−1t are between 0 and 1/δ and that, as a consequence, the matrix B̂−1t + ΓI is such that\nΓI B̂−1t + ΓI (Γ + 1\nδ )I. (14)\nHaving matrices B̂−1t + ΓI that are strictly positive definite with eigenvalues uniformly upper bounded by Γ + (1/δ) leads to the conclusion that if ŝ(wt, θ̃t) is a descent direction, the same holds true of (B̂−1t + ΓI) ŝ(wt, θ̃t). The stochastic gradient ŝ(wt, θ̃t) is not a descent direction in general, but we know that this is true for its conditional expectation E[ŝ(wt, θ̃t)\n∣∣wt] = ∇wF (wt). Therefore, we conclude that (B̂−1t + ΓI)ŝ(wt, θ̃t) is an average descent direction because E[(B̂ −1 t +\nΓI) ŝ(wt, θ̃t) ∣∣wt] = (B̂−1t + ΓI)∇wF (wt). Having a displacement wt+1 − wt that is a descent direction on average implies convergence towards optimal arguments as we claim in the following theorem.\nTheorem 1 Consider the regularized stochastic BFGS algorithm as defined by (4), (5), (8), and (9). If assumptions 1-3 hold true and the sequence of stepsizes satisfies is nonsummable but square summable, i.e., if ∑∞ t=0 t = ∞, and ∑∞ t=0 2 t < ∞, the limit infimum of the squared Euclidean distance to optimality ‖wt −w∗‖2 satisfies\nlim inf t→∞\n‖wt −w∗‖2 = 0 a.s. (15)\nover realizations of the random samples {θ̃t}∞t=1.\nTheorem 1 establishes convergence of the stochastic regularized BFGS algorithm summarized in Algorithm 1. In the proof of this result the lower bound in the eigenvalues of B̂t enforced by the regularization in (9) plays a fundamental role. Roughly speaking, the lower bound in\nthe eigenvalues of B̂t results in an upper bound on the eigenvalues of B̂−1t which limits the effect of random variations on the stochastic gradient ŝ(wt, θ̃t). If this regularization is not implemented, i.e., if we keep δ = 0, we may observe catastrophic amplification of random variations of the stochastic gradient. This effect is indeed observed in the numerical experiments in Section 4. The addition of the identity matrix bias ΓI in (4) is also instrumental in the proof of Theorem 1. This bias limits the effects of randomness in the curvature estimate B̂t. If random variations in the curvature estimate B̂t result in a matrix B̂−1t with small eigenvalues the term ΓI dominates and (4) reduces to stochastic gradient descent. This ensures continued progress towards the optimal argument w∗.\nThe convergence claim in Theorem 1is complemented by a expected convergence rate result which we state in the following theorem.\nTheorem 2 Consider the regularized stochastic BFGS algorithm as defined by (4)-(9) and let the sequence of stepsizes be given by t = 0τ/(τ + t) with the parameter 0 sufficiently small and the parameter τ sufficiently large so as to satisfy the inequality\n2 0τΓ > 1 . (16)\nIf assumptions 1 and 2 hold true the difference between the expected objective value E [F (wt)] at time t and the optimal objective F (w∗) satisfies\nE [F (wt)]− F (w∗) ≤ ξ\nτ + t , (17)\nwhere the constant ξ satisfies\nξ = max\n{ 20 τ 2K\n2 0τΓ− 1 , (1 + τ)(F (w0)− F (w∗))\n} . (18)\nTheorem 2 shows the convergence rate of regularized stochastic BFGS is at least linear in terms of the expectation of the objective function. This rate is typical of stochastic optimization algorithms and, in that sense, no better than stochastic gradient descent. While the convergence rate doesn’t change, improvements in convergence time are marked as we illustrate with the numerical experiments of the following section.\n4. NUMERICAL ANALYSIS\nWe test Algorithm 1 when using the squared hinge loss l((x, y);w) = max(0, 1−y(xTw))2 in (1). The training set S = {(xi, yi)}Ni=1 contains N = 104 feature vectors half of which belong to the class yi = −1 with the other half belonging to the class yi = 1. For the class yi = −1 each of the n components of each of the feature vectors xi ∈ Rn is chosen uniformly at random from the interval [−0.8, 0.2]. Likewise, each of the n components of each of the feature vectors xi ∈ Rn is chosen uniformly\nat random from the interval [−0.2, 0.8] for the class yi = 1. The overlap in the range of the feature vectors is such that the classification accuracy expected from a clairvoyant classifier that knows the statistic model of the data set is less than 100%. Exact values can be computed from the Irwin-Hall distribution [16]. For n = 4 this amounts to 98%.\nWe set the parameter λ in (1) to λ = 10−3. Since the Hessian eigenvalues of f(w,θ) := λ‖w‖2/2 + l((xi, yi);w) are, at least, equal to λ this implies that the eigenvalue lower bound m̃ is such that m̃ ≥ λ = 10−3. Thus, we set the BFGS regularization parameter to δ = λ = 10−3. Further set the minimum progress parameter in (3) to Γ = 10−4 and the sample size for computation of stochastic gradients to L = 5. Stepsizes are of the form t = 0τ/(τ + t) with 0 = 3 × 10−2 and τ = 102. We compare the behavior of stochastic gradient descent and stochastic BFGS for a small dimensional problem with n = 4 and a large problem with n = 40. For stochastic gradient descent the sample size in (3) is L = 1 and we use the same stepsize sequence used for stochastic BFGS.\nAn illustration of the relative performances of stochastic gradient descent and BFGS for n= 4 is presented in Fig. 1. The value of the objective function F (wt) is represented with respect to the number of feature vectors processed, which is given by the product Lt between the iteration index and the sample size used to compute stochastic gradients. This is done because the sample sizes in stochastic BFGS (L = 5) and stochastic gradient descent (L = 1) are different. The curvature correction of stochastic BFGS results in significant reductions in convergence time. E.g., Stochastic BFGS achieves an objective value of F (wt) = 6.5× 10−2 upon processing of Lt = 315 feature vectors. To achieve the same objective value F (wt) = 6.5×10−2 stochastic gradient descent processes 1.74×103 feature vectors. Conversely, after processing Lt = 2.5× 103 feature vectors the objective values achieved by stochastic BFGS and gradient descent are F (wt) = 4.14× 10−2 and F (wt) = 6.31× 10−2, respectively.\nThe performance difference between the two methods is larger for feature vectors of larger dimension n. The plot of the value of the objective function F (wt) with respect to the number of feature vectors processed Lt is shown in Fig. 2 for n = 40. The convergence time of stochastic BFGS increases but is still acceptable. For stochastic gradient descent the algorithm becomes unworkable. After processing 3.5×103 stochastic BFGS reduces the objective value to F (wt) = 5.55×10−4 while stochastic gradient descent has barely made progress at F (wt) = 1.80× 10−2.\nDifferences in convergence times translate into differences in classification accuracy when we process all N vectors in the training set. This is shown for dimension n = 4 and training set sizeN = 2.5×103 in Fig. 3. To build Fig. 3 we process N = 2.5× 103 feature vectors with stochastic BFGS and stochastic gradient descent with the same parameters used in Fig. 1. We then use these vectors to classify 104 observations in the test set and record the percentage of samples that are correctly classified. The\nprocess is repeated 103 times to estimate the probability distribution of the correct classification percentage represented by the histograms shown. The dominance of stochastic BFGS with respect to stochastic gradient descent is almost uniform. The vector wt computed by stochastic gradient descent classifies correctly at most 65% of the of the feature vectors in the test set. The vector wt computed by stochastic BFGS exceeds this accuracy with probability 0.98. Perhaps more relevant, the classifier computed by stochastic BFGS achieves a mean classification accuracy of 82.2% which is not far from the clairvoyant classification accuracy of 98%. Although performance is markedly better in general, stochastic BFGS fails to compute a working classifier with probability 0.02.\nWe also investigate the difference between regularized and nonregularized versions of stochastic BFGS for feature vectors of dimension n = 10. Observe that non-regularized stochastic BFGS corresponds to making δ = 0 and Γ = 0 in Algorithm 1. To illustrate the advantage of the regularization induced by the proximity requirement in (7), as opposed to the non regularized proximity requirement in (6), we keep a constant stepsize t = 10−1. The corresponding evolutions of the objective function values F (wt) with respect to the number of feature vectors processed Lt are shown in Fig. 4 along with the values associated with stochastic gradient descent. As we reach convergence the likelihood of having small eigenvalues appearing in B̂t becomes significant. In regularized stochastic BFGS this results in recurrent jumps away from the optimal classifier w∗. However, the regularization term limits the size of the jumps and further permits the algorithm to consistently recover a reasonable curvature estimate. In Fig. 4 we process 104 feature vectors and observe many occurrences of small eigenvalues. However, the algorithm always recovers and heads back to a good approximation of w∗. In the absence of regularization small eigenvalues in B̂t result in larger jumps away from w∗. This not only sets back the algorithm by a much larger amount than in the regularized case but also results in a catastrophic deterioration of the curvature approximation matrix B̂t. In Fig. 4 we observe recovery after the first two occurrences of small eigenvalues but eventually there is a catastrophic deviation after which non-regularized stochastic BFSG behaves not better than stochastic gradient descent.\n5. CONCLUSIONS\nWe considered the problem of determining the separating hyperplane of a support vector machine using stochastic optimization. In order to handle large scale problems with reasonable convergence times we adapted a regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method [9]. We derived theoretical convergence guarantees that are customary of stochastic optimization and illustrated improvements in convergence time through numerical analysis."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” In Proceedings of COMPSTAT’2010, pp. 177–186, Physica-Verlag HD, 2010.\n[2] S. Shalev-Shwartz, Y. Singer, and N. Srebro, “Pegasos: Primal estimated sub-gradient solver for svm,” In Proceedings of the 24th international conference on Machine learning, pp. 807–814, ACM, 2007.\n[3] T. Zhang, “Solving large scale linear prediction problems using stochastic gradient descent algorithms,” In Proceedings of the twenty-first international conference on Machine learning, p. 919926, ACM, 2004.\n[4] N. LeRoux, M. Schmidt, and F. Bach, “A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets,” arXiv preprint arXiv, 1202.6258, 2012.\n[5] C. G. Broyden, J. E. D. Jr., Wang, and J. J. More, “On the local and superlinear convergence of quasi-newton methods,” IMA J. Appl. Math, vol. 12, no. 3, pp. 223–245, June 1973.\n[6] A. Bordes, L. Bottou, and P. Gallinari, “Sgd-qn: Careful quasinewton stochastic gradient descent,” The Journal of Machine Learning Research, vol. 10, pp. 1737–1754, 2009.\n[7] R. H. Byrd, J. Nocedal, and Y. Yuan, “Global convergence of a class of quasi-newton methods on convex problems,” SIAM J. Numer. Anal., vol. 24, no. 5, pp. 1171–1190, October 1987.\n[8] A. Mokhtari and A. Ribeiro, “A dual stochastic dfp algorithm for optimal resource allocation in wireless systems,” in Proc. IEEE 14th Workshop on Signal Process. Advances in Wireless Commun. (SPAWC). pp. 21-25, Darmstadt Germany, June 16-19 2013.\n[9] ——, “Regularized stochastic bfgs algorithm,” in Proc. IEEE Global Conf. on Signal and Inform. Process. pp. 1109-1112, Austin Texas, Dec. 3-5 2013.\n[10] J. Nocedal and S. J. Wright, Numerical optimization, 2nd ed. New York, NY: Springer-Verlag, 1999.\n[11] M. J. D. Powell, Some global convergence properties of a variable metric algorithm for minimization without exact line search, 2nd ed. London, UK: Academic Press, 1971.\n[12] N. N. Schraudolph, J. Yu, and S. Gnter, “A stochastic quasi-newton method for online convex optimization,” In Proc. 11th Intl. Conf. on Artificial Intelligence and Statistics (AIstats), p. 433 440, Soc. for Artificial Intelligence and Statistics, 2007.\n[13] V. Vapnik, The nature of statistical learning theory, 2nd ed. springer, 1999.\n[14] A. Mokhtari and A. Ribeiro, “Res: Regularized stochastic bfgs algorithm,” arXiv preprint arXiv, 1401.7625, 2014.\n[15] J. J. E. Dennis and J. J. More, “A characterization of super linear convergence and its application to quasi-newton methods,” Mathematics of computation, vol. 28, no. 126, pp. 549–560, 1974.\n[16] N. L. Johnson, S. Kotz, and N. Balakrishnan, Continuous Univariate Distributions, vol. 2, 2nd ed. Wiley-Interscience, 1995."
    } ],
    "references" : [ {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "Proceedings of COMPSTAT’2010, pp. 177–186, Physica-Verlag HD, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro" ],
      "venue" : "Proceedings of the 24th international conference on Machine learning, pp. 807–814, ACM, 2007.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
      "author" : [ "T. Zhang" ],
      "venue" : "Proceedings of the twenty-first international conference on Machine learning, p. 919926, ACM, 2004.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets",
      "author" : [ "N. LeRoux", "M. Schmidt", "F. Bach" ],
      "venue" : "arXiv preprint arXiv, 1202.6258, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the local and superlinear convergence of quasi-newton methods",
      "author" : [ "C.G. Broyden", "J.E.D. Jr.", "Wang", "J.J. More" ],
      "venue" : "IMA J. Appl. Math, vol. 12, no. 3, pp. 223–245, June 1973.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Sgd-qn: Careful quasinewton stochastic gradient descent",
      "author" : [ "A. Bordes", "L. Bottou", "P. Gallinari" ],
      "venue" : "The Journal of Machine Learning Research, vol. 10, pp. 1737–1754, 2009.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Global convergence of a class of quasi-newton methods on convex problems",
      "author" : [ "R.H. Byrd", "J. Nocedal", "Y. Yuan" ],
      "venue" : "SIAM J. Numer. Anal., vol. 24, no. 5, pp. 1171–1190, October 1987.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "A dual stochastic dfp algorithm for optimal resource allocation in wireless systems",
      "author" : [ "A. Mokhtari", "A. Ribeiro" ],
      "venue" : "Proc. IEEE 14th Workshop on Signal Process. Advances in Wireless Commun. (SPAWC). pp. 21-25, Darmstadt Germany, June 16-19 2013.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Regularized stochastic bfgs algorithm",
      "author" : [ "——" ],
      "venue" : "Proc. IEEE Global Conf. on Signal and Inform. Process. pp. 1109-1112, Austin Texas, Dec. 3-5 2013.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Numerical optimization, 2nd ed",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Some global convergence properties of a variable metric algorithm for minimization without exact line search, 2nd ed",
      "author" : [ "M.J.D. Powell" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1971
    }, {
      "title" : "A stochastic quasi-newton method for online convex optimization",
      "author" : [ "N.N. Schraudolph", "J. Yu", "S. Gnter" ],
      "venue" : "Proc. 11th Intl. Conf. on Artificial Intelligence and Statistics (AIstats), p. 433 440, Soc. for Artificial Intelligence and Statistics, 2007.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The nature of statistical learning theory, 2nd ed",
      "author" : [ "V. Vapnik" ],
      "venue" : "springer,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Res: Regularized stochastic bfgs algorithm",
      "author" : [ "A. Mokhtari", "A. Ribeiro" ],
      "venue" : "arXiv preprint arXiv, 1401.7625, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A characterization of super linear convergence and its application to quasi-newton methods",
      "author" : [ "J.J.E. Dennis", "J.J. More" ],
      "venue" : "Mathematics of computation, vol. 28, no. 126, pp. 549–560, 1974.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1974
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Computation of the separating hyperplane entails solution of a convex optimization problem that can be implemented without much difficulty in problems of moderate size [1].",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1–4].",
      "startOffset" : 272,
      "endOffset" : 277
    }, {
      "referenceID" : 1,
      "context" : "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1–4].",
      "startOffset" : 272,
      "endOffset" : 277
    }, {
      "referenceID" : 2,
      "context" : "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1–4].",
      "startOffset" : 272,
      "endOffset" : 277
    }, {
      "referenceID" : 3,
      "context" : "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1–4].",
      "startOffset" : 272,
      "endOffset" : 277
    }, {
      "referenceID" : 4,
      "context" : "In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "In this paper we resort to quasi-Newton methods [5–12] to make better use of the provided training set.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "In particular, we adapt a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) method [9] for the solution of SVM classification problems (Section 2).",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "distances to the separating hyperplane, as measured by the loss function l((x, y);w), with the minimization of the L2 norm ‖w‖2 to enforce desirable properties in w∗ [13].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "The following lemma shows that solutions of (7) can be computed by a simple algebraic formula (see [14] for proofs of results in this paper).",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "When δ = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].",
      "startOffset" : 74,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "When δ = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].",
      "startOffset" : 74,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "When δ = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].",
      "startOffset" : 74,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "When δ = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].",
      "startOffset" : 74,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "In order to handle large scale problems with reasonable convergence times we adapted a regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method [9].",
      "startOffset" : 192,
      "endOffset" : 195
    } ],
    "year" : 2014,
    "abstractText" : "This paper adapts a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the solution of support vector machine classification problems. The proposed method is shown to converge almost surely to the optimal classifier at a rate that is linear in expectation. Numerical results show that the proposed method exhibits a convergence rate that degrades smoothly with the dimensionality of the feature vectors.",
    "creator" : "LaTeX with hyperref package"
  }
}