{
  "name" : "1410.4210.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets",
    "authors" : [ "Jie Wang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Sparse-Group Lasso (SGL) [7, 23] is a powerful regression technique in identifying important groups and features simultaneously. To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties. In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc. Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28]. However, when the feature dimension is extremely high, the complexity of the SGL regularizers imposes great computational challenges. Therefore, there is an increasingly urgent need for nontraditional techniques to address the challenges posed by the massive volume of the data sources.\nRecently, El Ghaoui et al. [6] proposed a promising feature reduction method, called SAFE screening, to screen out the so-called inactive features, which have zero coefficients in the solution, from the optimization. Thus, the size of the data matrix needed for the training phase can be significantly reduced, which may lead to substantial improvement in the efficiency of solving sparse models. Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc. It is worthwhile to mention that the discarded features by exact feature screening methods such as\nar X\niv :1\n41 0.\n42 10\nv1 [\ncs .L\nG ]\n1 5\nO ct\n2 01\nSAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution. However, heuristic feature screening methods like Strong Rule [26] may mistakenly discard features which have nonzero coefficients in the solution. More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30]. As a promising data reduction tool, exact feature/sample screening would be of great practical importance because they can effectively reduce the data size without sacrificing the optimality [16].\nHowever, all of the existing feature/sample screening methods are only applicable for the sparse models with one sparsity-inducing regularizer. In this paper, we propose an exact two-layer feature screening method, called TLFre, for the SGL problem. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to have zero coefficients in the solution. To the best of our knowledge, TLFre is the first screening method which is capable of dealing with multiple sparsity-inducing regularizers.\nWe note that most of the existing exact feature screening methods involve an estimation of the dual optimal solution. The difficulty in developing screening methods for sparse models with multiple sparsity-inducing regularizers like SGL is that the dual feasible set is the sum of simple convex sets. Thus, to determine the feasibility of a given point, we need to know if it is decomposable with respect to the summands, which is itself a nontrivial problem (see Section 2). One of our major contributions is that we derive an elegant decomposition method of any dual feasible solutions of SGL via the framework of Fenchel’s duality (see Section 3). Based on the Fenchel’s dual problem of SGL, we motivate TLFre by an in-depth exploration of its geometric properties and the optimality conditions in Section 4. We derive the set of the regularization parameter values corresponding to zero solutions. To develop TLFre, we need to estimate the upper bounds involving the dual optimal solution. To this end, we first give an accurate estimation of the dual optimal solution via the normal cones. Then, we formulate the estimation of the upper bounds via nonconvex optimization problems. We show that these nonconvex problems admit closed form solutions.\nThe rest of this paper is organized as follows. In Section 2, we briefly review some basics of the SGL problem. We then derive the Fenchel’s dual of SGL with nice geometric properties under the elegant framework of Fenchel’s Duality in Section 3. In Section 4, we develop the TLFre screening rule for SGL. To demonstrate the flexibility of the proposed framework, we extend TLFre to the nonnegative Lasso problem in Section 5. Experiments in Section 6 on both synthetic and real data sets demonstrate that the speedup gained by the proposed screening rules in solving SGL and nonnegative Lasso can be orders of magnitude.\nNotation: Let ‖ · ‖1, ‖ · ‖ and ‖ · ‖∞ be the `1, `2 and `∞ norms, respectively. Denote by Bn1 , Bn, and Bn∞ the unit `1, `2, and `∞ norm balls in Rn (we omit the superscript if it is clear from the context). For a set C, let int C be its interior. If C is closed and convex, we define the projection operator as PC(w) := argminu∈C‖w − u‖. We denote by IC(·) the indicator function of C, which is 0 on C and ∞ elsewhere. Let Γ0(Rn) be the class of proper closed convex functions on Rn. For f ∈ Γ0(Rn), let ∂f be its subdifferential. The domain of f is the set dom f := {w : f(w) < ∞}. For w ∈ Rn, let [w]i be its ith component. For γ ∈ R, let sgn(γ) = sign(γ) if γ 6= 0, and sgn(0) = 0. We define\nSGN(w) = { s ∈ Rn : [s]i ∈ { sign([w]i), if [w]i 6= 0; [−1, 1], if [w]i = 0. }\nWe denote by γ+ = max(γ, 0). Then, the shrinkage operator Sγ(w) : Rn → Rn with γ ≥ 0 is\n[Sγ(w)]i = (|[w]i| − γ)+sgn([w]i), i = 1, . . . , n. (1)"
    }, {
      "heading" : "2 Basics and Motivation",
      "text" : "In this section, we briefly review some basics of SGL. Let y ∈ RN be the response vector and X ∈ RN×p be the matrix of features. With the group information available, the SGL problem [7] is\nmin β∈Rp\n1\n2 ∥∥∥∥y−∑Gg=1 Xgβg ∥∥∥∥2 + λ1∑Gg=1√ng‖βg‖+ λ2‖β‖1, (2)\nwhere ng is the number of features in the g th group, Xg ∈ RN×ng denotes the predictors in that group with the corresponding coefficient vector βg, and λ1, λ2 are positive regularization parameters. Without loss of generality, let λ1 = αλ and λ2 = λ with α > 0. Then, problem (2) becomes:\nmin β∈Rp\n1\n2 ∥∥∥∥y−∑Gg=1 Xgβg ∥∥∥∥2 + λ(α∑Gg=1√ng‖βg‖+ ‖β‖1 ) . (3)\nBy the Lagrangian multipliers method [4] (see the supplement), the dual problem of SGL is\nsup θ\n{ 1 2‖y‖ 2 − 12 ∥∥y λ − θ ∥∥2 : XTg θ ∈ Dαg := α√ngB + B∞, g = 1, . . . , G} . (4) It is well-known that the dual feasible set of Lasso is the intersection of closed half spaces (thus a polytope); for group Lasso, the dual feasible set is the intersection of ellipsoids. Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].\nWhen we incorporate multiple sparse-inducing regularizers to the sparse models, problem (4) indicates that the dual feasible set can be much more complicated. Although (4) provides a geometric description of the dual feasible set of SGL, it is not suitable for further analysis. Notice that, even the feasibility of a given point θ is not easy to determine, since it is nontrivial to tell if XTg θ can be decomposed into b1 + b2 with b1 ∈ α √ ngB and b2 ∈ B∞. Therefore, to develop screening methods for SGL, it is desirable to gain deeper understanding of the sum of simple convex sets. In the next section, we analyze the dual feasible set of SGL in depth via the Fenchel’s Duality Theorem. We show that for each XTg θ ∈ Dαg , Fenchel’s duality naturally leads to an explicit decomposition XTg θ = b1 + b2, with one belonging to α √ ngB and the other one belonging to B∞. This lays the foundation of the proposed screening method for SGL."
    }, {
      "heading" : "3 The Fenchel’s Dual Problem of SGL",
      "text" : "In Section 3.1, we derive the Fenchel’s dual of SGL via Fenchel’s Duality Theorem. We then motivate TLFre and sketch our approach in Section 3.2. In Section 3.3, we discuss the geometric properties of the Fenchel’s dual of SGL and derive the set of (λ, α) leading to zero solutions."
    }, {
      "heading" : "3.1 The Fenchel’s Dual of SGL via Fenchel’s Duality Theorem",
      "text" : "To derive the Fenchel’s dual problem of SGL, we need the Fenchel’s Duality Theorem as stated in Theorem 1. The conjugate of f ∈ Γ0(Rn) is the function f∗ ∈ Γ0(Rn) defined by\nf∗(z) = supw 〈w, z〉 − f(w). (5)\nTheorem 1. [Fenchel’s Duality Theorem] Let f ∈ Γ0(RN ), Ω ∈ Γ0(Rp), and T (β) = y − Xβ be an affine mapping from Rp to RN . Let p∗, d∗ ∈ [−∞,∞] be primal and dual values defined, respectively, by the Fenchel problems:\np∗ = infβ∈Rp f(y −Xβ) + λΩ(β); d∗ = supθ∈RN −f∗(λθ)− λΩ∗(XT θ) + λ〈y, θ〉.\nOne has p∗ ≥ d∗. If, furthermore, f and Ω satisfy the condition 0 ∈ int (dom f − y + Xdom Ω), then the equality holds, i.e., p∗ = d∗, and the supreme is attained in the dual problem if finite.\nWe omit the proof of Theorem 1 since it is a slight modification of Theorem 3.3.5 in [3]. Let f(w) = 12‖w‖ 2, and λΩ(β) be the second term in (3). Then, SGL can be written as\nminβ f(y −Xβ) + λΩ(β). (6)\nTo derive the Fenchel’s dual problem of SGL, Theorem 1 implies that we need to find f∗ and Ω∗. It is well-known that f∗(z) = 12‖z‖\n2. Therefore, we only need to find Ω∗, where the concept infimal convolution is needed:\nDefinition 2. [2] Let h, g ∈ Γ0(Rn). The infimal convolution of h and g is defined by\n(h g)(ξ) = infη h(η) + g(ξ − η), (7)\nand it is exact at a point ξ if there exists a η∗(ξ) such that\n(h g)(ξ) = h(η∗(ξ)) + g(ξ − η∗(ξ)). (8)\nh g is exact if it is exact at every point of its domain, in which case it is denoted by h g.\nWith the infimal convolution, we derive the conjugate function of Ω in Lemma 3. Lemma 3. Let Ωα1 (β) = α ∑G g=1 √ ng‖βg‖, Ω2(β) = ‖β‖1 and Ω(β) = Ωα1 (β) + Ω2(β). Moreover, let Cαg = α √ ngB ⊂ Rng , g = 1, . . . , G. Then, the following hold:\n(i) (Ωα1 ) ∗(ξ) = ∑G g=1 ICαg (ξg) , (Ω2) ∗(ξ) = ∑G g=1 IB∞ (ξg),\n(ii) Ω∗(ξ) = ((Ωα1 ) ∗ (Ω2)∗) (ξ) = ∑G g=1 IB ( ξg−PB∞ (ξg) α √ ng ) ,\nwhere ξg ∈ Rng is the sub-vector of ξ corresponding to the gth group.\nTo prove Lemma 3, we first cite the following technical result.\nTheorem 4. [10] Let f1, · · · , fk ∈ Γ0(Rn). Suppose there is a point in ∩ki=1dom fi at which f1, · · · , fk−1 is continuous. Then, for all p ∈ Rn:\n(f1 + · · ·+ fk)∗(p) = min p1+···+pk=p [f∗1 (p1) + · · ·+ f∗k (pk)].\nWe now give the proof of Lemma 3.\nProof. The first part can be derived directly by the definition as follows:\n(Ωα1 ) ∗(ξ) = sup β 〈β, ξ〉 − Ω1(β) = G∑ g=1 α √ ng ( sup βg 〈 βg, ξg α √ ng 〉 − ‖βg‖ )\n= G∑ g=1 α √ ngIB ( ξg α √ ng ) = G∑ g=1 IB ( ξg α √ ng ) = G∑ g=1 ICαg (ξg).\n(Ω2) ∗(ξ) = sup\nβ 〈β, ξ〉 − Ω2(β) = IB∞ (ξ) = G∑ g=1 IB∞ (ξg) .\nTo show the second part, Theorem 4 indicates that we only need to show (Ωα1 ) ∗ (Ω2)∗(ξ) is\nexact (note that Ωα1 and Ω2 are continuous everywhere). Let us now compute (Ω α 1 ) ∗ (Ω2)∗.\n((Ω1) ∗ (Ω2) ∗) (ξ) = inf η (Ω1) ∗(ξ − η) + (Ω2)∗(η) (9)\n= G∑ g=1 inf ηg IB ( ξg − ηg α √ ng ) + IB∞ (ηg)\n= G∑ g=1 inf ‖ηg‖∞≤1 IB ( ξg − ηg α √ ng ) To solve the optimization problem in (9), i.e.,\nµ∗g = infηg\n{ IB ( ξg − ηg α √ ng ) : ‖ηg‖∞ ≤ 1 } , (10)\nwe can consider the following problem\nν∗g = infηg\n{ 1\nα √ ng ‖ξg − ηg‖ : ‖ηg‖∞ ≤ 1\n} . (11)\nWe can see that the optimal solution of problem (11) must also be an optimal solution of problem (10). Let η∗g(ξg) be the optimal solution of (11). We can see that η ∗ g(ξg) is indeed the projection of ξg on B∞, which admits a closed form solution:\n[η∗g(ξg)]i = [PB∞(ξg)]i =  1, if [ξg]i > 1,\n[ξg]i, if |[ξg]i| ≤ 1, −1, if [ξg]i < −1.\nThus, problem (10) can be solved as\nµ∗g = IB\n( ξg −PB∞(ξg)\nα √ ng\n) .\nHence, the infimal convolution in Eq. (9) is exact and Theorem 4 leads to\nΩ∗(ξ) = ((Ωα1 ) ∗ (Ω2) ∗) (ξ) = G∑ g=1 IB ( ξg −PB∞(ξg) α √ ng ) , (12)\nwhich completes the proof.\nNote that PB∞(ξg) admits a closed form solution, i.e., [PB∞(ξg)]i = sgn ([ξg]i) min (|[ξg]i| , 1). Combining Theorem 1 and Lemma 3, the Fenchel’s dual of SGL can be derived as follows.\nTheorem 5. For the SGL problem in (3), the following hold:\n(i) The Fenchel’s dual of SGL is given by:\ninf θ\n{ 1 2‖ y λ − θ‖ 2 − 12‖y‖ 2 : ∥∥XTg θ −PB∞(XTg θ)∥∥ ≤ α√ng, g = 1, . . . , G} . (13)\n(ii) Let β∗(λ, α) and θ∗(λ, α) be the optimal solutions of problems (3) and (13), respectively. Then,\nλθ∗(λ, α) =y −Xβ∗(λ, α), (14) XTg θ ∗(λ, α) ∈α√ng∂‖β∗g (λ, α)‖+ ∂‖β∗g (λ, α)‖1, g = 1, . . . , G. (15)\nTo show Theorem 5, we need the Fenchel-Young inequality as follows:\nLemma 6. [Fenchel-Young inequality] [3] Any point z ∈ Rn and w in the domain of a function h : Rn → (−∞,∞] satisfy the inequality\nh(w) + h∗(z) ≥ 〈w, z〉.\nEquality holds if and only if z ∈ ∂h(w).\nWe now give the proof of Theorem 5.\nProof. We first show the first part. Combining Theorem 1 and Lemma 3, the Fenchel’s dual of SGL can be written as:\nsup θ −λ\n2\n2 ‖θ‖2 − ∑G g=1 λIB\n( XTg θ −PB∞(XTg θ)\nα √ ng\n) + λ〈y, θ〉,\nwhich is equivalent to problem (13). To show the second half, we have the following inequalities by Fenchel-Young inequality:\nf(y −Xβ) + f∗(λθ) ≥ 〈y −Xβ, λθ〉, (16) λΩ(β) + λΩ∗(XT θ) ≥ λ〈β,XT θ〉. (17)\nWe sum the inequalities in (16) and (17) together and get\nf(y −Xβ) + λΩ(β) ≥ −f∗(λθ)− λΩ∗(XT θ) + λ〈y, θ〉. (18)\nClearly, the left and right hand sides of inequality (18) are the objective functions of the pair of Fenchel’s problems. Because dom f = RN and dom Ω = Rp, we have\n0 ∈ int (dom f − y + Xdom Ω).\nThus, the equality in (18) holds at β∗(λ, α) and θ∗(λ, α), i.e.,\nf(y −Xβ∗(λ, α)) + λΩ(β∗(λ, α)) = −f∗(λθ∗(λ, α))− λΩ∗(XT θ∗(λ, α)) + λ〈y, θ∗(λ, α)〉.\nTherefore, the equality holds in both (16) and (17) at β∗(λ, α) and θ∗(λ, α). By applying Lemma 6 again, we have\nλθ∗(λ, α) ∈ ∂f(y −Xβ∗(λ, α)) = y −Xβ∗(λ, α), XT θ∗(λ, α) ∈ ∂Ω(β∗(λ, α)) = ∂Ωα1 (β∗(λ, α)) + ∂Ω2(β∗(λ, α)),\nwhich completes the proof.\nEq. (14) and Eq. (15) are the so-called KKT conditions [4] and can also be obtained by the Lagrangian multiplier method (see A.1 in the supplement).\nRemark 1. We note that the shrinkage operator can also be expressed by\nSγ(w) = w −PγB∞(w), γ ≥ 0. (19)\nTherefore, problem (13) can be written more compactly as\ninf θ\n{ 1 2‖ y λ − θ‖ 2 − 12‖y‖ 2 : ∥∥S1(XTg θ)∥∥ ≤ α√ng, g = 1, . . . , G} . (20)\nThe equivalence between the dual formulations For the SGL problem, its Lagrangian dual in (4) and Fenchel’s dual in (13) are indeed equivalent to each other. We bridge them together by the following lemma.\nLemma 7. [2] Let C1 and C2 be nonempty subsets of Rn. Then IC1 IC2 = IC1+C2.\nIn view of Lemmas 3 and 7, and recall that Dαg = Cαg + B∞, we have\nΩ∗(ξ) = ((Ωα1 ) ∗ (Ω2)\n∗) (ξ) = ∑G\ng=1\n( ICαg IB∞ ) (ξg) = ∑G g=1 IDαg (ξg). (21)\nCombining Eq. (21) and Theorem 1, we obtain the dual formulation of SGL in (4). Therefore, the dual formulations of SGL in (4) and (13) are the same.\nRemark 2. An appealing advantage of the Fenchel’s dual in (13) is that we have a natural decomposition of all points ξg ∈ Dαg : ξg = PB∞(ξg) + S1(ξg)) with PB∞(ξg) ∈ B∞ and S1(ξg) ∈ Cαg . As a result, this leads to a convenient way to determine the feasibility of any dual variable θ by checking if S1(XTg θ) ∈ Cαg , g = 1, . . . , G."
    }, {
      "heading" : "3.2 Motivation of the Two-Layer Screening Rules",
      "text" : "We motive the two-layer screening rules via the KKT condition in Eq. (15). As implied by the name, there are two layers in our method. The first layer aims to identify the inactive groups, and the second layer is designed to detect the inactive features for the remaining groups.\nby Eq. (15), we have the following cases by noting ∂‖w‖1 = SGN(w) and\n∂‖w‖ =\n{{ w ‖w‖ } , if w 6= 0,\n{u : ‖u‖ ≤ 1}, if w = 0.\nCase 1. If β∗g (λ, α) 6= 0, we have\n[XTg θ ∗(λ, α)]i ∈\n{ α √ ng\n[β∗g (λ,α)]i ‖β∗g (λ,α)‖ + sign([β∗g (λ, α)]i), if [β ∗ g (λ, α)]i 6= 0,\n[−1, 1], if [β∗g (λ, α)]i = 0. (22)\nIn view of Eq. (22), we can see that\n(a): S1(XTg θ∗(λ, α)) = α √ ng β∗g (λ1,λ2)\n‖β∗g (λ1,λ2)‖ and ‖S1(XTg θ∗(λ, α))‖ = α\n√ ng, (23)\n(b): If ∣∣[XTg θ∗(λ, α]i∣∣ ≤ 1 then [β∗g (λ, α)]i = 0. (24)\nCase 2. If β∗g (λ, α) = 0, we have\n[XTg θ ∗(λ, α)]i ∈ α √ ng[ug]i + [−1, 1], ‖ug‖ ≤ 1. (25)\nThe first layer (group-level) of TLFre From (23) in Case 1, we have∥∥S1(XTg θ∗(λ, α))∥∥ < α√ng ⇒ β∗g (λ, α) = 0. (R1) Clearly, (R1) can be used to identify the inactive groups and thus a group-level screening rule.\nThe second layer (feature-level) of TLFre Let xgi be the i th column of Xg. We have\n[XTg θ ∗(λ, α)]i = x T giθ ∗(λ, α). In view of (24) and (25), we can see that∣∣xTgiθ∗(λ, α)∣∣ ≤ 1⇒ [β∗g (λ, α)]i = 0. (R2)\nDifferent from (R1), (R2) detects the inactive features and thus it is a feature-level screening rule. However, we cannot directly apply (R1) and (R2) to identify the inactive groups/features because both need to know θ∗(λ, α). Inspired by the SAFE rules [6], we can first estimate a region Θ containing θ∗(λ, α). Let XTg Θ = {XTg θ : θ ∈ Θ}. Then, (R1) and (R2) can be relaxed as follows:\nsupξg { ‖S1(ξg)‖ : ξg ∈ Ξg ⊇ XTg Θ } < α √ ng ⇒ β∗g (λ, α) = 0, (R1∗)\nsupθ {∣∣xTgiθ∣∣ : θ ∈ Θ} ≤ 1⇒ [β∗g (λ, α)]i = 0. (R2∗)\nInspired by (R1∗) and (R2∗), we develop TLFre via the following three steps:\nStep 1. Given λ and α, we estimate a region Θ that contains θ∗(λ, α).\nStep 2. We solve for the supreme values in (R1∗) and (R2∗).\nStep 3. By plugging in the supreme values from Step 2, (R1∗) and (R2∗) result in the desired two-layer screening rules for SGL."
    }, {
      "heading" : "3.3 The Set of Parameter Values Leading to Zero Solutions",
      "text" : "In this section, we explore the geometric properties of the Fenchel’s dual of SGL in depth—based on which we can derive the set of parameter values such that the primal optimal solutions are 0. We consider the SGL problem in (3) and (2) in Section 3.3.1 and 3.3.2, respectively."
    }, {
      "heading" : "3.3.1 The Set of Parameter Values Leading to Zero Solutions of Problem (3)",
      "text" : "Consider the SGL problem in (3). For notational convenience, let\nFαg = {θ : ‖S1(XTg θ)‖ ≤ α √ ng}, g = 1, . . . , G.\nWe denote the feasible set of the Fenchel’s dual of SGL by\nFα = ∩g=1,...,GFαg .\nIn view of problem (13) [or (20)], we can see that θ∗(λ, α) is the projection of y/λ on Fα, i.e.,\nθ∗(λ, α) = PFα(y/λ). (26)\nThus, if y/λ ∈ Fα, we have θ∗(λ, α) = y/λ. Moreover, by (R1), we can see that β∗(λ, α) = 0 if y/λ is an interior point of Fα. Indeed, we have the following stronger result.\nTheorem 8. For the SGL problem, let λαmax = maxg {ρg : ∥∥S1(XTg y/ρg)∥∥ = α√ng}. Then, the following statements are equivalent:\n(i) y λ ∈ Fα, (ii) θ∗(λ, α) = y λ , (iii) β∗(λ, α) = 0, (iv) λ ≥ λαmax. Proof. The equivalence between (i) and (ii) can be see from the fact that θ∗(λ, α) = PFα(y/λ). Next, we show (ii)⇔(iii). Let us first show (ii)⇒(iii). We assume that θ∗(λ, α) = y/λ. By the KKT condition in (14), we have Xβ∗(λ, α) = 0. We claim that β∗(λ, α) = 0. To see this, let β′ 6= 0 with Xβ′ = 0 be another optimal solution of SGL. We denote by h the objective function of SGL in (3). Then, we have\nh(0) = 1 2 ‖y‖2 < h(β′) = 1 2 ‖y‖2 + λ1 ∑ g √ ng‖β′g‖+ λ2‖β′‖1,\nwhich contradicts with the assumption β′ 6= 0 is also an optimal solution. This contradiction indicates that β∗(λ, α) must be 0. The converse direction, i.e., (ii)⇐(iii), can be derived directly from the KKT condition in Eq. (14).\nFinally, we show the equivalence (i)⇔(iv). Indeed, in view of the dual problem in (20), we can see that y/λ ∈ Fα if and only if\n‖S1(XTg y/λ)‖ ≤ α √ ng, g = 1, . . . , G. (27)\nWe note that ‖S1(XTg y/λ)‖ is monotonically decreasing with respect to λ. Thus, the inequality in (27) is equivalent to (iv), which completes the proof.\nWe note that ρg in the definition of λ α max admits a closed form solution. For notational convenience, let |w| be the vector by taking absolute value of w component-wisely and [w](k) be the vector consisting of the first k components of w.\nLemma 9. We sort 0 6= |XTg y| ∈ Rng in descending order and denote it by z.\n(i) If there exists [z]k such that ‖S1(XTg y/[z]k)‖ = α √ ng, then ρg = [z]k.\n(ii) Otherwise, let τi = ‖S1(XTg y/[z]i)‖, i = 1, . . . , ng, and τng+1 =∞. There exists a k such that α √ ng ∈ (τk, τk+1), and ρg ∈ ([z]k+1, [z]k) is the root of\n(k − α2ng)ρ2 − 2ρ‖[z](k)‖1 + ‖[z](k)‖2 = 0.\nWe omit the proof of Lemma 9 because it is a direct consequence by noting that\n‖S1(XTg y/λ)‖2 = α2ng\nis piecewise quadratic."
    }, {
      "heading" : "3.3.2 The Set of Parameter Values Leading to Zero Solutions of Problem (2)",
      "text" : "Theorem 8 implies that the optimal solution β∗(λ, α) is 0 as long as y/λ ∈ Fα. This geometric property also leads to an explicit characterization of the set of (λ1, λ2) such that the corresponding solution of problem (2) is 0. We denote by β̄∗(λ1, λ2) the optimal solution of problem (2).\nCorollary 10. For the SGL problem in (2), let λmax1 (λ2) = maxg 1√ ng ‖Sλ2(XTg y)‖. Then,\n(i) β̄∗(λ1, λ2) = 0⇔ λ1 ≥ λmax1 (λ2).\n(ii) If λ1 ≥ λmax1 := maxg 1√ng ‖X T g y‖ or λ2 ≥ λmax2 := ‖XTy‖∞, then β̄∗(λ1, λ2) = 0.\nBefore we prove Corollary 10, we first derive the Fenchel’s dual of (2). By letting f(w) = 12‖w‖ 2 and Ω(β) = λ1 ∑G g=1 √ ng‖βg‖+ λ2‖β‖1, the SGL problem in (2) can be written as:\nmin β\nf(y −Xβ) + Ω(β).\nThen, by Fenchel’s Duality Theorem, the Fenchel’s dual problem of (2) is\ninf θ\n{ 1\n2 ‖y − θ‖2 − 1 2 ‖y‖2 : ∥∥Sλ2(XTg θ)∥∥ ≤ λ1√ng, g = 1, . . . , G} . (28) Let β̄∗(λ1, λ2) and θ̄\n∗(λ1, λ2) be the optimal solutions of problem (2) and (28). The optimality conditions can be written as\nθ̄∗(λ1, λ2) =y −Xβ̄∗(λ1, λ2), (29) XTg θ̄ ∗(λ1, λ2) ∈ λ1 √ ng∂‖β̄∗g (λ1, λ2)‖+ λ2∂‖β̄∗g (λ1, λ2)‖1, g = 1, . . . , G. (30)\nWe denote by F(λ1, λ2) the feasible set of problem (28). It is easy to see that\nθ̄∗(λ1, λ2) = PF(λ1,λ2)(y).\nWe now present the proof of Corollary 10.\nProof. For notational convenience, let\n(i). y ∈ F(λ1, λ2),\n(ii). θ̄∗(λ1, λ2) = y,\n(iii). β̄∗(λ1, λ2) = 0,\n(iv). λ1 ≥ λmax1 (λ2) = maxg 1√ng ‖Sλ2(X T g y)‖.\nThe first half of the statement is (iii)⇔(iv). Indeed, by a similar argument as in the proof of Theorem 8, we can see that the above statements are all equivalent to each other.\nWe now show the second half. We first show that\nλ1 ≥ λmax1 ⇒ β̄∗(λ1, λ2) = 0. (31)\nBy the first half, we only need to show\nλ1 ≥ λmax1 ⇒ y ∈ F(λ1, λ2).\nIndeed, the definition of λ1 implies that\n‖XTg y‖ ≤ λ1 √ ng, g = 1, . . . , G.\nWe note that for any λ2 ≥ 0, we have\n‖Sλ2(XTg y)‖ ≤ ‖XTg y‖.\nTherefore, we can see that\n‖Sλ2(XTg y)‖ ≤ ‖XTg y‖ ≤ λ1 √ ng, g = 1, . . . , G⇒ y ∈ F(λ1, λ2).\nThe proof of (31) is complete. Similarly, to show that λ2 ≥ λmax2 ⇒ β̄∗(λ1, λ2), we only need to show\nλ2 ≥ λmax2 ⇒ y ∈ F(λ1, λ2).\nBy the definition of λ2, we can see that\n‖XTg y‖∞ ≤ λ2, g = 1, . . . , G⇒ ‖Sλ2(XTg y)‖ = 0 ≤ λ1 √ ng, g = 1, . . . , G.\nThus, we have y ∈ F(λ1, λ2), which completes the proof."
    }, {
      "heading" : "4 The Two-Layer Screening Rules for SGL",
      "text" : "We follow the three steps in Section 3.2 to develop TLFre. In Section 4.1, we give an accurate estimation of θ∗(λ, α) via normal cones [20]. Then, we compute the supreme values in (R1∗) and (R2∗) by solving nonconvex problems in Section 4.2. We present the TLFre rules in Section 4.3."
    }, {
      "heading" : "4.1 Estimation of the Dual Optimal Solution",
      "text" : "Because of the geometric property of the dual problem in (13), i.e., θ∗(λ, α) = PFα(y/λ), we have a very useful characterization of the dual optimal solution via the so-called normal cones [20].\nProposition 11. [20, 2] For a closed convex set C ∈ Rn and a point w ∈ C, the normal cone to C at w is defined by\nNC(w) = {v : 〈v,w′ −w〉 ≤ 0, ∀w′ ∈ C}. (32)\nThen, the following hold:\n(i) NC(w) = {v : PC(w + v) = w}.\n(ii) PC(w + v) = w, ∀v ∈ NC(w).\n(iii) Let w /∈ C. Then, w = PC(w)⇔ w −w ∈ NC(w).\n(iv) Let w /∈ C and w = PC(w). Then, PC(w + t(w −w)) = w for all t ≥ 0.\nBy Theorem 8, θ∗(λ̄, α) is known if λ̄ = λαmax. Thus, we can estimate θ ∗(λ, α) in terms of θ∗(λ̄, α). Due to the same reason, we only consider the cases with λ < λαmax for θ ∗(λ, α) to be estimated.\nRemark 3. In many applications, the parameter values that perform the best are usually unknown. To determine appropriate parameter values, commonly used approaches such as cross validation and stability selection involve solving SGL many times over a grip of parameter values. Thus, given {α(i)}Ii=1 and λ(1) ≥ · · · ≥ λ(J ), we can fix the value of α each time and solve SGL by varying the value of λ. We repeat the process until we solve SGL for all of the parameter values.\nTheorem 12. For the SGL problem in (3), suppose that θ∗(λ̄, α) is known with λ̄ ≤ λαmax. Let ρg, g = 1, . . . , G, be defined by Theorem 8. For any λ ∈ (0, λ̄), we define\nnα(λ̄) =  y λ̄ − θ∗(λ̄, α), if λ̄ < λαmax, X∗S1 ( XT∗\ny λαmax\n) , if λ̄ = λαmax, where X∗ = argmaxXg ρg,\nvα(λ, λ̄) = y λ − θ∗(λ̄, α),\nvα(λ, λ̄) ⊥ = vα(λ, λ̄)− 〈vα(λ, λ̄),nα(λ̄)〉 ‖nα(λ̄)‖2 nα(λ̄).\nThen, the following hold:\n(i) nα(λ̄) ∈ NFα(θ∗(λ̄, α)),\n(ii) ‖θ∗(λ, α)− (θ∗(λ̄, α) + 12v ⊥ α (λ, λ̄))‖ ≤ 12‖v ⊥ α (λ, λ̄)‖.\nProof. (i) Suppose that λ̄ < λαmax. Theorem 8 implies that y/λ̄ /∈ Fα and thus\ny/λ̄−PFα ( y/λ̄ ) = y/λ̄− θ∗(λ̄, α) 6= 0.\nBy the third part of Proposition 11, we can see that\ny/λ̄− θ∗(λ̄, α) ∈ NFα(θ∗(λ̄, α)). (33)\nThus, the statement holds for all λ̄ < λαmax. Suppose that λ̄ = λαmax. By Theorem 8, we have\nθ∗(λ̄, α) = y/λ̄ ∈ Fα.\nIn view of the definition of X∗, we have∥∥∥S1 (XT∗ yλαmax)∥∥∥ = α√n∗, where n∗ is the number of feature contained in X∗. Moreover, it is easy to see that\n‖S1(XT∗ θ)‖ ≤ α √ n∗, ∀θ ∈ Fα.\nTherefore, to prove the statement, we need to show that〈 X∗S1 ( XT∗\ny λαmax ) , θ − yλαmax 〉 ≤ 0, ∀θ ∈ Fα. (34)\nRecall Remark 1, we have the following identity [see Eq. (19)] S1 ( XT∗\ny λαmax\n) = XT∗\ny λαmax\n−PB∞ ( XT∗\ny λαmax\n) .\nThus, we have〈 X∗S1 ( XT∗\ny λαmax ) , θ − yλαmax 〉 (35)\n= 〈 S1 ( XT∗\ny λαmax\n) ,XT∗ ( θ − yλαmax ) + PB∞ ( XT∗ y λαmax ) −PB∞ ( XT∗ y λαmax )〉 = 〈 S1 ( XT∗\ny λαmax\n) ,XT∗ θ −PB∞ ( XT∗\ny λαmax )〉 − ∥∥∥S1 (XT∗ yλαmax)∥∥∥2\n= 〈 S1 ( XT∗\ny λαmax\n) ,XT∗ θ −PB∞ ( XT∗\ny λαmax\n)〉 − α2n∗.\nConsider the first term on the right hand side of Eq. (35), we have〈 S1 ( XT∗\ny λαmax\n) ,XT∗ θ −PB∞ ( XT∗\ny λαmax\n)〉 (36)\n= 〈 S1 ( XT∗\ny λαmax\n) ,XT∗ θ −PB∞(XT∗ θ) + PB∞(XT∗ θ)−PB∞ ( XT∗\ny λαmax )〉 = 〈 S1 ( XT∗\ny λαmax\n) ,S1(XT∗ θ) 〉 + 〈 S1 ( XT∗\ny λαmax\n) ,PB∞(X T ∗ θ)−PB∞ ( XT∗\ny λαmax\n)〉 .\nLet P = {i : [XT∗ y λαmax ]i > 1} and N = {i : [XT∗ y λαmax ]i < −1}. We note that the second term on the right hand side of Eq. (36) can be written as〈\nS1 ( XT∗\ny λαmax\n) ,PB∞(X T ∗ θ)−PB∞ ( XT∗\ny λαmax\n)〉 (37)\n= ∑ i∈P ( [XT∗ y λαmax ]i − 1 ) ( [PB∞(X T ∗ θ)]i − 1 ) + ∑ j∈N ( [XT∗ y λαmax ]j + 1 ) ( [PB∞(X T ∗ θ)]j + 1 ) .\nBecause ‖PB∞(XT∗ θ)‖∞ ≤ 1, we can see that Eq. (37) is non-positive. Therefore, by Eq. (36), we have〈\nS1 ( XT∗\ny λαmax\n) ,XT∗ θ −PB∞ ( XT∗\ny λαmax\n)〉 ≤ 〈 S1 ( XT∗\ny λαmax\n) ,S1(XT∗ θ) 〉 (38)\n≤ ∥∥∥S1 (XT∗ yλαmax)∥∥∥∥∥S1(XT∗ θ)∥∥ ≤α2n∗.\nCombining Eq. (35) and the inequality in (38), we can see that the inequality in (34) holds. Thus, the statement holds for λ̄ = λαmax. This completes the proof.\n(ii) We now show the second half. It is easy to see that the statement is equivalent to\n‖θ∗(λ, α)− θ∗(λ̄, α)‖2 ≤ 〈θ∗(λ, α)− θ∗(λ̄, α), v⊥α (λ, λ̄)〉. (39)\nThus, we will show that the inequality in (39) holds.\nBecause of the first half, we have\n〈nα(λ̄), θ − θ∗(λ̄, α)〉 ≤ 0, ∀ θ ∈ Fα. (40)\nBy letting θ = θ∗(λ, α), the inequality in (40) leads to\n〈nα(λ̄), θ∗(λ, α)− θ∗(λ̄, α)〉 ≤ 0. (41)\nIn view of the first half and by letting θ = 0, the inequality in (40) leads to\n〈nα(λ̄), 0− θ∗(λ̄, α)〉 ≤ 0⇒ { 〈nα(λ̄), y〉 ≥ 0, if λ̄ = λαmax, ‖y‖/λ̄ ≥ ‖θ∗(λ̄, α)‖, if λ̄ < λαmax.\n(42)\nMoreover, the first half also leads to yλ − θ ∗(λ, α) ∈ NFα(θ∗(λ, α)). Thus, we have\n〈yλ − θ ∗(λ, α), θ − θ∗(λ, α)〉 ≤ 0, ∀ θ ∈ Fα. (43)\nBy letting θ = θ∗(λ̄, α), the inequality in (43) results in\n〈yλ − θ ∗(λ, α), θ∗(λ̄, α)− θ∗(λ, α)〉 ≤ 0, ∀ θ ∈ Fα. (44)\nWe can see that the inequality in (44) is equivalent to\n‖θ∗(λ, α)− θ∗(λ̄, α)‖2 ≤〈θ∗(λ, α)− θ∗(λ̄, α), vα(λ, λ̄)〉. (45)\nOn the other hand, the right hand side of (39) can be rewritten as\n〈θ∗(λ, α)− θ∗(λ̄, α), v⊥α (λ, λ̄)〉 (46) =〈θ∗(λ, α)− θ∗(λ̄, α), vα(λ, λ̄)〉 − 〈θ∗(λ, α)− θ∗(λ̄, α), vα(λ, λ̄)− v⊥α (λ, λ̄)〉\n=〈θ∗(λ, α)− θ∗(λ̄, α), vα(λ, λ̄)〉 − 〈 θ∗(λ, α)− θ∗(λ̄, α), 〈vα(λ,λ̄),nα(λ̄)〉\n‖nα(λ̄)‖2 nα(λ̄)\n〉 .\nIn view of (41), (45) and (46), we can see that (39) holds if 〈vα(λ, λ̄),nα(λ̄)〉 ≥ 0. Indeed,\n〈vα(λ, λ̄),nα(λ̄)〉 = 〈 y/λ− θ∗(λ̄, α),nα(λ̄) 〉 (47)\n= ( 1/λ− 1/λ̄ ) 〈y,nα(λ̄)〉+ 〈y/λ̄− θ∗(λ̄, α),nα(λ̄)〉\nConsider the first term on the right hand side of Eq. (47). By the first half of (42), we have\n〈y,nα(λ̄)〉 ≥ 0, if λ̄ = λαmax. (48)\nSuppose that λ̄ < λαmax. By the second half of (42), we can see that\n〈y,nα(λ̄)〉 = 〈y,y/λ̄− θ∗(λ̄, α)〉 ≥ 1/λ̄‖y‖2 − ‖y‖‖θ∗(λ̄, α)‖ ≥ 0. (49)\nConsider the second term on the right hand side of Eq. (47). It is easy to see that\n〈y/λ̄− θ∗(λ̄, α),nα(λ̄)〉 =\n{ 0, if λ̄ = λαmax,\n‖nα(λ̄)‖2, if λ̄ < λαmax. (50)\nCombining (48), (49) and Eq. (50), we have 〈vα(λ, λ̄),nα(λ̄)〉 ≥ 0, which completes the proof.\nFor notational convenience, we denote\noα(λ, λ̄) = θ ∗(λ̄, α) + 12v ⊥ α (λ, λ̄). (51)\nTheorem 12 shows that θ∗(λ, α) lies inside the ball of radius 12‖v ⊥ α (λ, λ̄)‖ centered at oα(λ, λ̄)."
    }, {
      "heading" : "4.2 Solving for the Supreme Values via Nonconvex Optimization",
      "text" : "We solve the optimization problems in (R1∗) and (R2∗). To simplify notations, let\nΘ = {θ : ‖θ − oα(λ, λ̄)‖ ≤ 12‖v ⊥ α (λ, λ̄)‖}, (52) Ξg = { ξg : ‖ξg −XTg oα(λ, λ̄)‖ ≤ 12‖v ⊥ α (λ, λ̄)‖‖Xg‖2 } , g = 1, . . . , G. (53)\nTheorem 12 indicates that θ∗(λ, α) ∈ Θ. Moreover, we can see that XTg Θ ⊆ Ξg, g = 1, . . . , G. To develop the TLFre rule by (R1∗) and (R2∗), we need to solve the following optimization problems:\ns∗g(λ, λ̄;α) = supξg {‖S1(ξg)‖ : ξg ∈ Ξg}, g = 1, . . . , G, (54) t∗gi(λ, λ̄;α) = supθ {|x T giθ| : θ ∈ Θ}, i = 1, . . . , ng, g = 1, . . . , G. (55)"
    }, {
      "heading" : "4.2.1 The Solution of Problem (54)",
      "text" : "We consider the following equivalent problem of (54):\n1 2 ( s∗g(λ, λ̄;α) )2 = supξg { 1 2‖S1(ξg)‖ 2 : ξg ∈ Ξg } . (56)\nWe can see that the objective function of problem (56) is continuously differentiable and the feasible set is a ball. Thus, problem (56) is nonconvex because we need to maximize a convex function subject to a convex set. We first derive the necessary optimality conditions in Lemma 13 and then deduce the closed form solutions of problems (54) and (56) in Theorem 15.\nLemma 13. Let Ξ∗g be the set of optimal solutions of (56) and ξ ∗ g ∈ Ξ∗g. Then, the following hold: (i) Suppose that ξ∗g is an interior point of Ξg. Then, Ξg is a subset of B∞. (ii) Suppose that ξ∗g is a boundary point of Ξg. Then, there exists µ ∗ ≥ 0 such that\nS1(ξ∗g) = µ∗ ( ξ∗g −XTg oα(λ, λ̄) ) . (57)\n(iii) Suppose that there exists ξ0g ∈ Ξg and ξ0g /∈ B∞. Then, we have (iiia) ξ∗g /∈ B∞ and ξ∗g is a boundary point of Ξg, i.e.,\n‖ξ∗g −XTg oα(λ, λ̄)‖ = 12‖v ⊥ α (λ, λ̄)‖‖Xg‖2.\n(iiib) The optimality condition in Eq. (57) holds with µ∗ > 0.\nTo show Lemma 13, we need the following proposition.\nProposition 14. [9] Suppose that h ∈ Γ0 and C is a nonempty closed convex set. If w∗ ∈ C is a local maximum of h on C, then ∂h(w∗) ⊆ NC(w∗).\nWe now present the proof of Lemma 13.\nProof. To simplify notations, let\nc = XTg oα(λ, λ̄) and r = 1 2 ‖v⊥α (λ, λ̄)‖‖Xg‖2. (58)\nBy Eq. (1), we have\nh(w) := 1\n2 ‖S1(w)‖2 =\n1\n2 ∑ i (|[w]i| − 1)2+. (59)\nIt is easy to see that h(·) is continuously differentiable. Indeed, we have\n∇h(w) = S1(w). (60)\nThen, problem (56) can be written as\n1 2 (s∗g(λ, λ̄;α)) 2 = sup ξg\n{ h(ξg) = 1\n2 ∑ i ([ξg]i − 1)2+ : ξg ∈ Ξg\n} , (61)\nwhere Ξg = {ξg : ‖ξg − c‖ ≤ r}. Then, Proposition 14 results in\nS1(ξ∗g) = ∇h(ξ∗g) = ∂h(ξ∗g) ⊆ NΞg(ξ∗g). (62)\n(i) Suppose that ξ∗g is an interior point of Ξg. Then, we have NΞg(ξ ∗ g) = 0. By Eq. (62), we\ncan see that\n0 = S1(ξ∗g)⇒ 0 = 1 2 ‖S1(ξ∗g)‖2 = 1 2 (s∗g(λ, λ̄;α)) 2 = sup ξg\n{ 1\n2 ‖S1(ξg)‖2 : ξg ∈ Ξg\n} .\nTherefore, we have\n‖S1(ξg)‖ = 0, ∀ ξg ∈ Ξg. (63)\nBecause S1(ξg) = ξg −PB∞(ξg) (see Remark 1), Eq. (63) implies that\nξg = PB∞(ξg), ∀ ξg ∈ Ξg ⇒ ξg ∈ B∞, ∀ ξg ∈ Ξg.\nThis completes the proof.\n(ii) Suppose that ξ∗g is a boundary point of Ξg. We can see that\nNΞg(ξ ∗ g) = {µ(ξ∗g − c), µ ≥ 0}. (64)\nThen, Eq. (57) follows by combining Eq. (64) and the optimality condition in (62).\n(iii) Suppose that there exists ξ0g ∈ Ξg and ξ0g /∈ B∞.\n(iiia) The definition of ξ0g leads to\n0 < ‖S1(ξ0g)‖ ≤ ‖S1(ξ∗g)‖ ⇒ ξ∗g /∈ B∞.\nMoreover, we can see that ξ∗g is a boundary point of Ξg. Because if ξ ∗ g is an interior point of Ξg, the first part implies that Ξg ⊂ B∞. This contradicts with the existence of ξ0g . Thus, ξ ∗ g must be a boundary point of Ξg, i.e. ‖ξ∗g − c‖ = r.\n(iiib) Because ξ∗g is a boundary point of Ξg, the second part implies that Eq. (57) holds. Moreover, from (iiia), we know that ξ∗g /∈ B∞. Therefore, both sides of Eq. (57) are nonzero and thus µ∗ > 0. This completes the proof.\nBased on the necessary optimality conditions in Lemma 13, we derive the closed form solutions of (54) and (56) in the following Theorem. The notations are the same as the ones in the proof of Lemma 13 [see Eq. (58) and Eq. (59)].\nTheorem 15. For problems (54) and (56), let c = XTg oα(λ, λ̄), r = 1 2‖v ⊥ α (λ, λ̄)‖‖Xg‖2 and Ξ∗g be the set of the optimal solutions.\n(i) Suppose that c /∈ B∞, i.e., ‖c‖∞ > 1. Let u = rS1(c)/‖S1(c)‖. Then,\ns∗g(λ, λ̄;α) = ‖S1(c)‖+ r and Ξ∗g = {c + u}. (65)\n(ii) Suppose that c is a boundary point of B∞, i.e., ‖c‖∞ = 1. Then,\ns∗g(λ, λ̄;α) = r and Ξ ∗ g = {c + u : u ∈ NB∞(c), ‖u‖ = r} . (66)\n(iii) Suppose that c ∈ intB∞, i.e., ‖c‖∞ < 1. Let i∗ ∈ I∗ = {i : |[c]i| = ‖c‖∞}. Then,\ns∗g(λ, λ̄;α) = (‖c‖∞ + r − 1)+ , (67)\nΞ∗g =  Ξg, if Ξg ⊂ B∞, {c + r · sgn([c]i∗)ei∗ : i∗ ∈ I∗} , if Ξg 6⊂ B∞ and c 6= 0, {r · ei∗ ,−r · ei∗ : i∗ ∈ I∗} , if Ξg 6⊂ B∞ and c = 0,\nwhere ei is the i th standard basis vector.\nProof. (i) Suppose that c /∈ B∞. By the third part of Lemma 13, we have\nξ∗g /∈ B∞, ‖ξ∗g − c‖ = r, (68) ξ∗g −PB∞(ξ∗g) = S1(ξ∗g) = µ∗(ξ∗g − c), µ∗ > 0. (69)\nBy Eq. (69), we can see that µ∗ 6= 1 because otherwise we would have c = PB∞(ξ∗g) ∈ B∞. Moreover, we can only consider the cases with µ∗ > 1 because ‖S1(ξ∗g)‖ = µ∗r and we aim to maximize ‖S1(ξ∗g)‖. Therefore, if we can find a solution with µ∗ > 1, there is no need to consider the cases with µ∗ ∈ (0, 1). Suppose that µ∗ > 1. Then, Eq. (69) leads to\nc =PB∞(ξ ∗ g) +\n( 1− 1\nµ∗\n)( ξ∗g −PB∞(ξ∗g) ) , (70)\nξ∗g =PB∞(ξ ∗ g) +\nµ∗ µ∗ − 1 ( c−PB∞(ξ∗g) ) . (71)\nIn view of part (iv) of Proposition 11 and Eq. (70), we have\nPB∞(c) = PB∞(ξ ∗ g). (72)\nTherefore, Eq. (71) can be rewritten as\nS1(ξ∗g) = ξ∗g −PB∞(ξ∗g) = µ∗\nµ∗ − 1 (c−PB∞(c)) =\nµ∗\nµ∗ − 1 S1(c). (73)\nCombining Eq. (69) and Eq. (73), we have\nµ∗\nµ∗ − 1 ‖S1(c)‖ = µ∗‖ξ∗g − c‖ = µ∗r ⇒ µ∗ = 1 + ‖S1(c)‖ r > 1. (74)\nThe statement holds by plugging Eq. (74) and Eq. (72) into Eq. (71) and Eq. (73). Moreover, the above discussion implies that Ξ∗g only contains one element as shown in Eq. (65).\n(ii) Suppose that c is a boundary point of B∞. Then, we can find a point ξ0g ∈ Ξg and ξ0g /∈ B∞. By the third part of Lemma 13, we also have Eq. (68) and Eq. (69) hold. We claim that µ∗ ∈ (0, 1]. The argument is as follows. Suppose that µ∗ > 1. By the same argument as in the proof of the first part, we can see that Eq. (73) holds. Because S1(ξ∗g) 6= 0 by Eq. (68), we have S1(c) 6= 0. This implies that c /∈ B∞. Thus, we have a contradiction, which implies that µ∗ ∈ (0, 1].\nLet us consider the cases with µ∗ = 1. Because ‖S1(ξ∗g)‖ = µ∗r [see Eq. (69)] and we want to maximize ‖S1(ξ∗g)‖, there is no need to consider the cases with µ∗ ∈ (0, 1) if we can find solutions of problem (54) with µ∗ = 1. Therefore, Eq. (69) leads to\nPB∞(ξ ∗ g) = c.\nBy part (iii) of Proposition 11, we can see that\nPB∞(ξ ∗ g) = c⇔ ξ∗g − c ∈ NB∞(c). (75)\nCombining Eq. (75) and Eq. (68), the statement holds immediately, which confirms that µ∗ = 1.\n(iii) Suppose that c is an interior point of B∞.\n(a) We first consider the cases with Ξg ⊂ B∞. Then, we can see that\nS1(ξ) = 0, ∀ξ ∈ Ξg ⇒ Ξ∗g = Ξg.\nIn other words, an arbitrary point of Ξg is an optimal solution of problem (54). Thus, we have\nc + r · sgn(ei∗)ei∗ ∈ Ξ∗g, s∗g(λ, λ̄;α) = 0.\nOn the other hand, we can see that\nc− rei ∈ Ξg ⊂ B∞, c + rei ∈ Ξg ⊂ B∞, i = 1, . . . , ng ⇒ ‖c‖∞ + r ≤ 1.\nTherefore, we have\n(‖c‖∞ + r − 1)+ = 0,\nand thus\ns∗g(λ, λ̄;α) = (‖c‖∞ + r − 1)+.\n(b) Suppose that Ξg 6⊂ B∞, i.e., there exists ξ0 ∈ Ξg such that ξ0 /∈ B∞. By the third part of Lemma 13, we have Eq. (68) and Eq. (69) hold. Moreover, in view of the proof of the first and second part, we can see that µ∗ ∈ (0, 1). Therefore, Eq. (69) leads to\n(1− µ∗)ξ∗g + µ∗c = PB∞(ξ∗g). (76)\nBy rearranging the terms of Eq. (76), we have\nPB∞(ξ ∗ g)− c = (1− µ∗)(ξ∗g − c). (77)\nBecause µ∗ ∈ (0, 1), Eq. (76) implies that PB∞(ξ∗g) lies on the line segment connecting ξ∗g and c. Thus, we have\n‖ξ∗g −PB∞(ξ∗g)‖+ ‖PB∞(ξ∗g)− c‖ = ‖ξ∗g − c‖ = r. (78)\nTherefore, to maximize ‖S1(ξ∗g)‖ = ‖ξ∗g−PB∞(ξ∗g)‖, we need to minimize ‖PB∞(ξ∗g)− c‖. Because ξ∗g /∈ B∞, we can see that PB∞(ξ∗g) is a boundary point of B∞. Therefore, we need to solve the following minimization problem:\nmin φg {‖φg − c‖ : ‖φg‖∞ = 1}. (79)\nSuppose that c = 0. We can see that the set of optimal solutions of problem (79) is\nΦ∗g = {ei} ng i=1 ∪ {−ei} ng i=1.\nFor each φ∗g ∈ Φ∗g, we set it as PB∞(ξ∗g). In view of Eq. (77) and Eq. (68), the statement follows immediately. Suppose that c 6= 0. Recall that I∗ = {i∗ : |[c]i∗ | = ‖c‖∞}. It is easy to see that\nΦ∗g = { φi∗ : [φi∗ ]k = { sgn([c]i∗), if k = i ∗,\n[c]k, otherwise, i∗ ∈ I∗\n} .\nWe can see that\nφi∗ − c = (1− |[c]∞|)sgn([c]i∗)ei∗ , i∗ ∈ I∗.\nFor each φi∗ , we set it to PB∞(ξ ∗ g). Then, we can see that the statement holds by Eq. (77) and Eq. (68). This completes the proof."
    }, {
      "heading" : "4.2.2 The Solution of Problem (55)",
      "text" : "Problem (55) can be solved directly via the Cauchy-Schwarz inequality.\nTheorem 16. For problem (55), we have t∗gi(λ, λ̄;α) = |x T gioα(λ, λ̄)|+ 1 2‖v ⊥ α (λ, λ̄)‖‖xgi‖. Proof. To simplify notations, let o = oα(λ, λ̄), r = 1 2‖v ⊥ α (λ, λ̄)‖ and t∗g = t∗g(λ, λ̄;α). Therefore, the set Θ in Eq. (52) can be written as\nΘ = {o + v : ‖v‖ ≤ r}.\nThen, problem (55) becomes\nt∗gi = sup v {|xTgi(o + v)| : ‖v‖ ≤ r}.\nWe can see that\n|xTgi(o + v)| ≤ |x T gio|+ |x T giv| ≤ |x T gio|+ ‖xgi‖‖v‖ ≤ |x T gio|+ ‖xgi‖r.\nThus, we have\nt∗gi ≤ |x T gio|+ ‖xgi‖r.\nConsider v∗ = rxgi/‖xgi‖. It is easy to see that o + v∗ ∈ Θ and\n|xTgi(o + v)| = |x T gio|+ ‖xgi‖r.\nTherefore, we have\nt∗gi = |x T gio|+ ‖xgi‖r,\nwhich completes the proof."
    }, {
      "heading" : "4.3 The Proposed Two-Layer Screening Rules",
      "text" : "To develop the two-layer screening rules for SGL, we only need to plug the supreme values s∗g(λ2, λ̄2;λ1) and t∗gi(λ2, λ̄2;λ1) in (R1 ∗) and (R2∗). We present the TLFre rule as follows.\nTheorem 17. For the SGL problem in (3), suppose that we are given α and a sequence of parameter values λαmax = λ\n(0) > λ(1) > . . . > λ(J ). Moreover, assume that β∗(λ(j), α) is known for an integer 0 ≤ j < J . Let θ∗(λ(j), α), v⊥α (λ(j+1), λ(j)) and s∗g(λ(j+1), λ(j);α) be given by Eq. (14), Theorems 12 and 15, respectively. Then, for g = 1, . . . , G, the following holds\ns∗g(λ (j+1), λ(j);α) < α √ ng ⇒ β∗g (λ(j+1), α) = 0. (L1)\nFor the ĝth group that does not pass the rule in (L1), we have [β∗ĝ (λ(j+1), α)]i = 0 if∣∣∣∣∣xTĝi ( y −Xβ∗(λ(j), α) λ(j) + 1 2 v⊥α (λ (j+1), λ(j)) )∣∣∣∣∣+ 12‖v⊥α (λ(j+1), λ(j))‖‖xĝi‖ ≤ 1. (L2) (L1) and (L2) are the first layer and second layer screening rules of TLFre, respectively."
    }, {
      "heading" : "5 Extension to Nonnegative Lasso",
      "text" : "The framework of TLFre is applicable to a large class of sparse models with multiple regularizers. As an example, we extend TLFre to nonnegative Lasso:\nmin β∈Rp\n{ 1\n2 ‖y−Xβ‖2 + λ‖β‖1 : β ∈ Rp+\n} , (80)\nwhere λ > 0 is the regularization parameter and Rp+ is the nonnegative orthant of Rp. In Section 5.1, we transform the constraint β ∈ Rp+ to a regularizer and derive the Fenchel’s dual of the nonnegative Lasso problem. We then motivate the screening method—called DPC since the key step is to decompose a convex set via Fenchel’s Duality Theorem—via the KKT conditions in Section 5.2. In Section 5.3, we analyze the geometric properties of the dual problem and derive the set of parameter values leading to zero solutions. We then develop the screening method for nonnegative Lasso in Section 5.4."
    }, {
      "heading" : "5.1 The Fenchel’s Dual of Nonnegative Lasso",
      "text" : "Let IRp+ be the indicator function of R p +. By noting that IRp+ = λIR p + for any λ > 0, we can rewrite the nonnegative Lasso problem in (80) as\nmin β∈Rp\n1 2 ‖y−Xβ‖2 + λ‖β‖1 + λIRp+(β). (81)\nIn other words, we incorporate the constraint β ∈ Rp+ to the objective function as an additional regularizer. As a result, the nonnegative lasso problem in (81) has two regularizers. Thus, similar to SGL, we can derive the Fenchel’s dual of nonnegative Lasso via Theorem 1.\nWe now proceed by following a similar procedure as the one in Section 3.1. We note that the nonnegative Lasso problem in (81) can also be formulated as the one in (6) with f(·) = 12‖ · ‖ 2 and\nΩ(β) = ‖β‖1 + IRp+(β). To derive the Fenchel’s dual of nonnegative Lasso, we need to find f ∗ and Ω∗ by Theorem 1. Since we have already seen that f∗(·) = 12‖ · ‖ 2 in Section 3.1, we only need to find Ω∗(·). The following result is indeed a counterpart of Lemma 3.\nLemma 18. Let Ω2(β) = ‖β‖1, Ω3 = IRp+(β), and Ω(β) = Ω2(β) + Ω3(β). Then,\n(i) (Ω2) ∗(ξ) = IB∞(ξ) and (Ω3) ∗(ξ) = IRp−(ξ), where R p − is the nonpositive orthant of Rp.\n(ii) Ω∗(ξ) = ((Ω2) ∗ (Ω3)∗)(ξ) = IRp−(ξ − 1), where R p 3 1 = (1, 1, . . . , 1)T .\nWe omit the proof of Lemma 18 since it is very similar to that of Lemma 3.\nRemark 4. Consider the second part of Lemma 18. Let C1 = {ξ : ξ ≤ 1}, where “≤” is defined component-wisely. We can see that\nIRp−(ξ − 1) = IC1(ξ)."
    }, {
      "heading" : "On the other hand, Lemma 7 implies that",
      "text" : "Ω∗(ξ) = ((Ω2) ∗ (Ω3) ∗)(ξ) = IB∞+Rp−(ξ).\nThus, we have B∞ + Rp− = C1. The second part of Lemma 18 decomposes each ξ ∈ B∞ + R p − into two components: 1 and ξ − 1 that belong to B∞ and Rp−, respectively.\nBy Theorem 1 and Lemma 18, we can derive the Fenchel’s dual of nonnegative Lasso in the following theorem (which is indeed the counterpart of Theorem 5).\nTheorem 19. For the nonnegative Lasso problem, the following hold:\n(i) The Fenchel’s dual of nonnegative Lasso is given by:\ninf θ\n{ 1\n2 ∥∥∥y λ − θ ∥∥∥2 − 1 2 ‖y‖2 : 〈xi, θ〉 ≤ 1, i = 1, . . . , p } . (82)\n(ii) Let β∗(λ) and θ∗(λ) be the optimal solutions of problems (81) and (82), respectively.Then,\nλθ∗(λ) = y −Xβ∗(λ), (83) XT θ∗(λ) ∈ ∂‖β∗(λ)‖1 + ∂IRp+(β ∗(λ)). (84)\nWe omit the proof of Theorem 19 since it is very similar to that of Theorem 5."
    }, {
      "heading" : "5.2 Motivation of the Screening Method via KKT Conditions",
      "text" : "The key to develop the DPC rule for nonnegative lasso is the KKT condition in (84). We can see that ∂‖w‖1 = SGN(w) and\n∂IRp+(w) =\n{ ξ ∈ Rp : [ξ]i = { 0, if [w]i > 0,\nρ, ρ ≤ 0, if [w]i = 0,\n} .\nTherefore, the KKT condition in (84) implies that\n〈xi, θ∗(λ)〉 ∈\n{ 1, if [β∗(λ)]i > 0,\n%, % ≤ 1, if [β∗(λ)]i = 0. (85)\nBy Eq. (85), we have the following rule:\n〈xi, θ∗(λ)〉 < 1⇒ [β∗(λ)]i = 0. (R3)\nBecause θ∗(λ) is unknown, we can apply (R3) to identify the inactive features—which have 0 coefficients in β∗(λ). Similar to TLFre, we can first find a region Θ that contains θ∗(λ). Then, we can relax (R3) as follows:\nsup θ∈Θ 〈xi, θ〉 < 1⇒ [β∗(λ)]i = 0. (R3∗)\nInspired by (R3∗), we develop DPC via the following three steps:\nStep 1. Given λ, we estimate a region Θ that contains θ∗(λ).\nStep 2. We solve the optimization problem ωi = supθ∈Θ 〈xi, θ〉.\nStep 3. By plugging in ωi computed from Step 2, (R3 ∗) leads to the desired screening method\nDPC for nonnegative Lasso."
    }, {
      "heading" : "5.3 Geometric Properties of the Fenchel’s Dual of Nonnegative Lasso",
      "text" : "In view of the Fenchel’s dual of nonnegative Lasso in (82), we can see that the optimal solution is indeed the projection of y/λ onto the feasible set F = {θ : 〈xi, θ〉 ≤ 1, i = 1, . . . , p}, i.e.,\nθ∗(λ) = PF (y λ ) . (86)\nTherefore, if y/λ ∈ F , Eq. (86) implies that θ∗(λ) = y/λ. If further y/λ is an interior point of F , R3∗ implies that β∗(λ) = 0. The next theorem gives the set of parameter values leading to 0 solutions of nonnegative Lasso.\nTheorem 20. For the nonnegative Lasso problem (81), Let λmax = maxi〈xi,y〉. Then, the following statements are equivalent:\n(i) y λ ∈ F , (ii) θ∗(λ) = y λ , (iii) β∗(λ) = 0, (iv) λ ≥ λmax.\nWe omit the proof of Theorem 20 since it is very similar to that of Theorem 8."
    }, {
      "heading" : "5.4 The Proposed Screening Rule for Nonnegative Lasso",
      "text" : "We follow the three steps in Section 5.2 to develop the screening rule for nonnegative Lasso. We first estimate a region that contains θ∗(λ). Because θ∗(λ) admits a closed form solution with λ ≥ λmax by Theorem 20, we focus on the cases with λ < λmax.\nTheorem 21. For the nonnegative Lasso problem, suppose that θ∗(λ̄) is known with λ̄ ≤ λmax. For any λ ∈ (0, λ̄), we define\nn(λ̄) =  y λ̄ − θ∗(λ̄), if λ̄ < λαmax,\nx∗, if λ̄ = λmax, where x∗ = argmaxxi 〈xi,y〉,\nv(λ, λ̄) = y\nλ − θ∗(λ̄),\nv(λ, λ̄)⊥ = v(λ, λ̄)− 〈v(λ, λ̄),n(λ̄)〉 ‖n(λ̄)‖2 n(λ̄).\nThen, the following hold:\n(i) n(λ̄) ∈ NF (θ∗(λ̄)),\n(ii) ∥∥∥∥θ∗(λ)− (θ∗(λ̄) + 12v⊥(λ, λ̄) )∥∥∥∥ ≤ 12‖v⊥(λ, λ̄)‖.\nProof. We only show that n(λmax) ∈ NF (θ∗(λmax)) since the proof of the other statement is very similar to that of Theorem 12.\nBy Proposition 11 and Theorem 20, it suffices to show that\n〈x∗, θ − y/λmax〉 ≤ 0, ∀ θ ∈ F . (87)\nBecause θ ∈ F , we have 〈x∗, θ〉 ≤ 1. The definition of x∗ implies that 〈x∗,y/λmax〉 = 1. Thus, the inequality in (87) holds, which completes the proof.\nTheorem 21 implies that θ∗(λ) is in a ball—denoted by B(λ, λ̄)—of radius 12‖v ⊥(λ, λ̄)‖ centered\nat θ∗(λ̄) + 12v ⊥(λ, λ̄). Simple calculations lead to\nωi = sup θ∈B(λ,λ̄)\n〈xi, θ〉 = 〈 xi, θ ∗(λ̄) + 1\n2 v⊥(λ, λ̄)\n〉 + 1\n2 ‖v⊥(λ, λ̄)‖‖xi‖. (88)\nBy plugging ωi into (R3 ∗), we have the DPC screening rule for nonnegative Lasso as follows.\nTheorem 22. For the nonnegative Lasso problem, suppose that we are given a sequence of parameter values λmax = λ (0) > λ(1) > . . . > λ(J ). Then, [β∗(λ(j+1))]i = 0 if β ∗(λ(j)) is known and the following holds:〈 xi,\ny −Xβ∗(λ(j)) λ(j) + 1 2 v⊥(λ(j+1), λ(j))\n〉 + 1\n2 ‖v⊥(λ(j+1), λ(j))‖‖xi‖ < 1. (89)"
    }, {
      "heading" : "6 Experiments",
      "text" : "We evaluate TLFre for SGL and DPC for nonnegative Lasso in Sections 6.1 and 6.2, respectively, on both synthetic and real data sets. To the best of knowledge, the TLFre and DPC are the first screening methods for SGL and nonnegative Lasso, respectively."
    }, {
      "heading" : "6.1 TLFre for SGL",
      "text" : "We perform experiments to evaluate TLFre on synthetic and real data sets in Sections 6.1.1 and 6.1.2, respectively. To measure the performance of TLFre, we compute the rejection ratios of (L1) and (L2), respectively. Specifically, let m be the number of features that have 0 coefficients in the solution, G be the index set of groups that are discarded by (L1) and p be the number of inactive features that are detected by (L2). The rejection ratios of (L1) and (L2) are defined by r1 = ∑ g∈G ng m and r2 = |p| m , respectively. Moreover, we report the speedup gained by TLFre, i.e., the ratio of the running time of solver without screening to the running time of solver with TLFre. The solver used in this paper is from SLEP [12].\nTo determine appropriate values of α and λ by cross validation or stability selection, we can run TLFre with as many parameter values as we need. Given a data set, for illustrative purposes only, we select seven values of α from {tan(ψ) : ψ = 5◦, 15◦, 30◦, 45◦, 60◦, 75◦, 85◦}. Then, for each value of α, we run TLFre along a sequence of 100 values of λ equally spaced on the logarithmic scale of λ/λαmax from 1 to 0.01. Thus, 700 pairs of parameter values of (λ, α) are sampled in total."
    }, {
      "heading" : "6.1.1 Simulation Studies",
      "text" : "We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36]. The true model is y = Xβ∗ + 0.01 , ∼ N(0, 1). We generate two data sets with 250 × 10000 entries: Synthetic 1 and Synthetic 2. We randomly break the 10000 features into 1000 groups. For Synthetic 1, the entries of the data matrix X are i.i.d. standard Gaussian with pairwise correlation zero, i.e., corr(xi,xi) = 0. For Synthetic 2, the entries of the data matrix X are drawn from i.i.d. standard Gaussian with pairwise correlation 0.5|i−j|, i.e., corr(xi,xj) = 0.5\n|i−j|. To construct β∗, we first randomly select γ1 percent of groups. Then, for each selected group, we randomly select γ2 percent of features. The selected components of β\n∗ are populated from a standard Gaussian and the remaining ones are set to 0. We set γ1 = γ2 = 10 for Synthetic 1 and γ1 = γ2 = 20 for Synthetic 2.\nThe figures in the upper left corner of Fig. 1 and Fig. 2 show the plots of λmax1 (λ2) (see Corollary 10) and the sampled parameter values of λ and α (recall that λ1 = αλ and λ2 = λ). For the other figures, the blue and red regions represent the rejection ratios of (L1) and (L2), respectively. We\ncan see that TLFre is very effective in discarding inactive groups/features; that is, more than 90% of inactive features can be detected. Moreover, we can observe that the first layer screening (L1) becomes more effective with a larger α. Intuitively, this is because the group Lasso penalty plays a more important role in enforcing the sparsity with a larger value of α (recall that λ1 = αλ). The top and middle parts of Table 1 indicate that the speedup gained by TLFre is very significant (up to 30 times) and TLFre is very efficient. Compared to the running time of the solver without screening, the running time of TLFre is negligible. The running time of TLFre includes that of computing ‖Xg‖2, g = 1, . . . , G, which can be efficiently computed by the power method [8]. Indeed, this can be shared for TLFre with different parameter values."
    }, {
      "heading" : "6.1.2 Experiments on Real Data Set",
      "text" : "We perform experiments on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data set (http://adni.loni.usc.edu/). The data matrix consists of 747 samples with 426040 single nucleotide polymorphisms (SNPs), which are divided into 94765 groups. The response vectors are the grey matter volume (GMV) and white matter volume (WMV), respectively.\nThe figures in the upper left corner of Fig. 3 and Fig. 4 show the plots of λmax1 (λ2) (see Corollary 10) and the sampled parameter values of α and λ. The other figures present the rejection ratios of (L1) and (L2) by blue and red regions, respectively. We can see that almost all of the inactive groups/features are discarded by TLFre. The rejection ratios of r1 + r2 are very close to 1 in all cases. Table 2 shows that TLFre leads to a very significant speedup (about 80 times). In other words, the solver without screening needs about eight and a half hours to solve the 100 SGL problems for each value of α. However, combined with TLFre, the solver needs only six to eight minutes. Moreover, we can observe that the computational cost of TLFre is negligible compared to that of the solver without screening. This demonstrates the efficiency of TLFre."
    }, {
      "heading" : "6.2 DPC for Nonnegative Lasso",
      "text" : "In this experiment, we evaluate the performance of DPC on two synthetic data sets and six real data sets. We integrate DPC with the solver [12] to solve the nonnegative Lasso problem along a sequence of 100 parameter values of λ equally spaced on the logarithmic scale of λ/λmax from 1.0 to 0.01. The two synthetic data sets are the same as the ones we used in Section 6.1.1. To construct β∗, we first randomly select 10 percent of features. The corresponding components of β∗\nare populated from a standard Gaussian and the remaining ones are set to 0. We list the six real data sets and the corresponding experimental settings as follows.\na) Breast Cancer data set [32, 21]: this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 × 7129). The response vector y ∈ {1,−1}44 contains the binary label of each sample.\nb) Leukemia data set [1]: this data set contains 11225 gene expression values of 52 samples (X ∈ R52×11225). The response vector y contains the binary label of each sample.\nc) Prostate Cancer data set [19]: this data set contains 15154 measurements of 132 patients (X ∈ R132×15154). By protein mass spectrometry, the features are indexed by time-of-flight values, which are related to the mass over charge ratios of the constituent proteins in the blood. The response vector y contains the binary label of each sample.\nd) PIE face image data set [22, 5]: this data set contains 11554 gray face images (each has 32× 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions. In each trial, we first randomly pick an image as the response y ∈ R1024, and then use the remaining images to form the data matrix X ∈ R1024×11553. We run 100 trials and report the average performance of DPC.\ne) MNIST handwritten digit data set [11]: this data set contains grey images of scanned handwritten digits (each has 28 × 28 pixels). The training and test sets contain 60, 000 and 10, 000 images, respectively. We first randomly select 5000 images for each digit from the training set and get a data matrix X ∈ R784×50000. Then, in each trial, we randomly select an image from the testing set as the response y ∈ R784. We run 100 trials and report the average performance of the screening rules.\nf) Street View House Number (SVHN) data set [15]: this data set contains color images of street view house numbers (each has 32 × 32 pixels), including 73257 images for training and 26032 for testing. In each trial, we first randomly select an image as the response y ∈ R3072, and then use the remaining ones to form the data matrix X ∈ R3072×99288. We run 20 trials and report the average performance.\nWe present the rejection ratios—the ratio of the number of inactive features identified by DPC to the actual number of inactive features—in Fig. 5. We also report the running time of the solver with and without DPC, the time for running DPC, and the corresponding speedup in Table 3.\nFig. 5 shows that DPC is very effective in identifying the inactive features even for small parameter values: the rejection ratios are very close to 100% for the entire sequence of parameter values on the eight data sets. Table 3 shows that DPC leads to a very significant speedup on all the data sets. Take MNIST as an example. The solver without DPC takes 50 minutes to solve the 100 nonnegative Lasso problems. However, combined with DPC, the solver only needs 10 seconds. The speedup gained by DPC on the MNIST data set is thus more than 300 times. Similarly, on the SVHN data set, the running time for solving the 100 nonnegative Lasso problems by the solver without DPC is close to seven hours. However, combined with DPC, the solver takes less than two minutes to solve all the 100 nonnegative Lasso problems, leading to a speedup about 230 times. Moreover, we can also observe that the computational cost of DPC is very low—which is negligible compared to that of the solver without DPC."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose a novel feature reduction method for SGL via decomposition of convex sets. We also derive the set of parameter values that lead to zero solutions of SGL. To the best of our knowledge, TLFre is the first method which is applicable to sparse models with multiple\nsparsity-inducing regularizers. More importantly, the proposed approach provides novel framework for developing screening methods for complex sparse models with multiple sparsity-inducing regularizers, e.g., `1 SVM that performs both sample and feature selection, fused Lasso and tree Lasso with more than two regularizers. To demonstrate the flexibility of the proposed framework, we develop the DPC screening rule for the nonnegative Lasso problem. Experiments on both synthetic and real data sets demonstrate the effectiveness and efficiency of TLFre and DPC. We plan to generalize the idea of TLFre to `1 SVM, fused Lasso and tree Lasso, which are expected to consist of multiple layers of screening."
    }, {
      "heading" : "A Sparse-Group Lasso",
      "text" : "A.1 The Lagrangian Dual Problem of SGL\nWe derive the dual problem of SGL in (4) via the Lagrangian multiplier method. By introducing an auxiliary variable\nz = y − G∑ g=1 Xgβg, (90)\nthe SGL problem in (3) becomes:\nmin β 12‖z‖2 + αλ G∑ g=1 √ ng‖βg‖+ λ‖β‖1 : z = y − G∑ g=1 Xgβg  . Let λθ be the Lagrangian multiplier, the Lagrangian function is\nL(β, z; θ) = 1\n2 ‖z‖2 + αλ G∑ g=1 √ ng‖βg‖+ λ‖β‖1 + 〈λθ,y − G∑ g=1 Xgβg − z〉 (91)\n=αλ G∑ g=1 √ ng‖βg‖+ λ‖β‖1 − λ〈θ, G∑ g=1 Xgβg〉+ 1 2 ‖z‖2 − λ〈θ, z〉+ λ〈θ,y〉. (92)\nLet\nf1(β) = G∑ g=1 fg1 (βg) = G∑ g=1 ( αλ √ ng‖βg‖+ λ‖βg‖1 − λ〈θ,Xgβg〉 ) , f2(z) = 1\n2 ‖z‖2 − λ〈θ, z〉.\nTo derive the dual problem, we need to minimize the Lagrangian function with respect to β and z. In other words, we need to minimize f1 and f2, respectively. We first consider\nmin βg\nfg1 (βg) = αλ √ ng‖βg‖+ λ‖β‖1 − λ〈θ,Xgβg〉.\nBy the Fermat’s rule, we have\n0 ∈ ∂fg1 (βg) = αλ √ ng∂‖βg‖+ λ∂‖βg‖1 − λXTg θ, (93)\nwhich leads to\nXTg θ = α √ ngζ1 + ζ2, ζ1 ∈ ∂‖βg‖, ζ2 ∈ ∂‖βg‖1. (94)\nBy noting that\n〈ζ1, βg〉 = ‖βg‖, 〈ζ2, βg〉 = ‖βg‖1,\nwe have\n〈XTg θ, βg〉 = α √ ng∂‖βg‖+ ∂‖βg‖1.\nThus, we can see that\n0 = min βg\nfg1 (βg). (95)\nMoreover, because ζ1 ∈ ∂‖βg‖, ζ2 ∈ ∂‖βg‖1, Eq. (94) implies that\nXTg θ ∈ α √ ngB + B∞. (96)\nTo minimize f2, the Fermat’s rule results in\nz = λθ, (97)\nand thus\n−λ 2\n2 ‖θ‖2 = min z f2(z). (98)\nIn view of Eq. (91), Eq. (95), Eq. (98) and Eq. (96), the dual problem of SGL can be written as\nsup θ\n{ 1\n2 ‖y‖2 − 1 2 ∥∥∥θ − y λ ∥∥∥2 : XTg θ ∈ α√ngB + B∞, g = 1, . . . , G} , which is equivalent to (4).\nRecall that β∗(λ, α) and θ∗(λ, α) are the primal and dual optimal solutions of SGL, respectively. By Eq. (90), Eq. (93) and Eq. (97), we can see that the KKT conditions are\nλθ∗(λ, α) =y −Xβ∗(λ, α), XTg θ ∗(λ, α) ∈α√ng∂‖β∗g (λ, α)‖+ ∂‖β∗g (λ, α)‖1, g = 1, . . . , G."
    } ],
    "references" : [ {
      "title" : "MLL translocations specify a distinct gene expression profile that distinguishes a unique leukemia",
      "author" : [ "S. Armstrong", "J. Staunton", "L. Silverman", "R. Pieters", "M. den Boer", "M. Minden", "S. Sallan", "E. Lander", "T. Golub", "S. Korsmeyer" ],
      "venue" : "Nature Genetics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
      "author" : [ "H.H. Bauschke", "P.L. Combettes" ],
      "venue" : "Springer,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Convex Analysis and Nonlinear Optimization, Second Edition",
      "author" : [ "J. Borwein", "A. Lewis" ],
      "venue" : "Canadian Mathematical Society,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Efficient kernel discriminant analysis via spectral regression",
      "author" : [ "D. Cai", "X. He", "J. Han" ],
      "venue" : "ICDM,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Safe feature elimination in sparse supervised learning",
      "author" : [ "L. El Ghaoui", "V. Viallon", "T. Rabbani" ],
      "venue" : "Pacific Journal of Optimization, 8:667–698,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P. Martinsson", "J. Tropp" ],
      "venue" : "SIAM Review, 53:217–288,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "From convex optimization to nonconvex optimization",
      "author" : [ "J.-B. Hiriart-Urruty" ],
      "venue" : "necessary and sufficient conditions for global optimality. In Nonsmooth optimization and related topics. Springer,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "A note on the Legendre-Fenchel transform of convex composite functions",
      "author" : [ "J.-B. Hiriart-Urruty" ],
      "venue" : "Nonsmooth Mechanics and Analysis. Springer,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "SLEP: Sparse Learning with Efficient Projections",
      "author" : [ "J. Liu", "S. Ji", "J. Ye" ],
      "venue" : "Arizona State University,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Moreau-Yosida regularization for grouped tree structure learning",
      "author" : [ "J. Liu", "J. Ye" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Safe screening with variational inequalities and its application to lasso",
      "author" : [ "J. Liu", "Z. Zhao", "J. Wang", "J. Ye" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Reading digits in nature images with unsupervised feature learning",
      "author" : [ "Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A. Ng" ],
      "venue" : "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Safe sample screening for Support Vector Machine",
      "author" : [ "K. Ogawa", "Y. Suzuki", "S. Suzumura", "I. Takeuchi" ],
      "venue" : "arXiv:1401.6740,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Safe screening of non-support vectors in pathwise SVM computation",
      "author" : [ "K. Ogawa", "Y. Suzuki", "I. Takeuchi" ],
      "venue" : "ICML,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Regularized multivariate regression for indentifying master predictors with application to integrative genomics study of breast cancer",
      "author" : [ "J. Peng", "J. Zhu", "A. Bergamaschi", "W. Han", "D. Noh", "J. Pollack", "P. Wang" ],
      "venue" : "The Annals of Appliced Statistics, 4:53–77,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Serum proteomic patterns for detection of prostate cancer",
      "author" : [ "E. Petricoin", "D. Ornstein", "C. Paweletz", "A. Ardekani", "P. Hackett", "B. Hitt", "A. Velassco", "C. Trucco", "L. Wiegand", "K. Wood", "C. Simone", "P. Levine", "W. Linehan", "M. Emmert-Buck", "S. Steinberg", "E. Kohn", "L. Liotta" ],
      "venue" : "Journal of National Cancer Institute, 94:1576–1578,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Nonlinear Optimization",
      "author" : [ "A. Ruszczyński" ],
      "venue" : "Princeton University Press,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A simple and efficient algorithm for gene selection using sparse logistic regression",
      "author" : [ "S. Shevade", "S. Keerthi" ],
      "venue" : "Bioinformatics, 19:2246–2253,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The CMU pose, illumination, and expression database",
      "author" : [ "T. Sim", "B. Baker", "M. Bsat" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 25:1615–1618,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A Sparse-Group Lasso",
      "author" : [ "N. Simon", "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "C-HiLasso: a collaborative hierarchical sparse modeling framework",
      "author" : [ "P. Sprechmann", "I. Ramı́rez", "G. Sapiro", "Y. Eldar" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Regression shringkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society Series B, 58:267–288,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Strong rules for discarding predictors in lasso-type problems",
      "author" : [ "R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society Series B, 74:245–266,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Machine learning methods in the cocomputation biology of cancer",
      "author" : [ "M. Vidyasagar" ],
      "venue" : "Proceedings of the Royal Society A,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sparse group lasso and high dimensional multinomial classification",
      "author" : [ "M. Vincent", "N. Hansen" ],
      "venue" : "Computational Statistics and Data Analysis, 71:771–786,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Scaling svm and least absolute deviations via exact data reduction",
      "author" : [ "J. Wang", "P. Wonka", "J. Ye" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Lasso screening rules via dual polytope projection",
      "author" : [ "J. Wang", "J. Zhou", "P. Wonka", "J. Ye" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Predicting the clinical status of human breast cancer by using gene expression profiles",
      "author" : [ "M. West", "C. Blanchette", "H. Dressman", "E. Huang", "S. Ishida", "R. Spang", "H. Zuzan", "J. Olson", "J. Marks", "J. Nevins" ],
      "venue" : "Proceedings of the National Academy of Sciences, 98:11462–11467,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Fast lasso screening tests based on correlations",
      "author" : [ "Z.J. Xiang", "P.J. Ramadge" ],
      "venue" : "IEEE ICASSP,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Linguistic structured sparsity in text categorization",
      "author" : [ "D. Yogatama", "N. Smith" ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Journal of the Royal Statistical Society Series B, 68:49–67,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "H. Zou", "T. Hastie" ],
      "venue" : "Journal of the Royal Statistical Society Series B, 67:301–320,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Sparse-Group Lasso (SGL) [7, 23] is a powerful regression technique in identifying important groups and features simultaneously.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 32,
      "context" : "To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 25,
      "context" : "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 31,
      "context" : "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 21,
      "context" : "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "[6] proposed a promising feature reduction method, called SAFE screening, to screen out the so-called inactive features, which have zero coefficients in the solution, from the optimization.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 28,
      "context" : "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.",
      "startOffset" : 128,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.",
      "startOffset" : 128,
      "endOffset" : 144
    }, {
      "referenceID" : 24,
      "context" : "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.",
      "startOffset" : 128,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.",
      "startOffset" : 128,
      "endOffset" : 144
    }, {
      "referenceID" : 28,
      "context" : "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 24,
      "context" : "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 5,
      "context" : "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 30,
      "context" : "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 28,
      "context" : "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "However, heuristic feature screening methods like Strong Rule [26] may mistakenly discard features which have nonzero coefficients in the solution.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 27,
      "context" : "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 27,
      "context" : "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 14,
      "context" : "As a promising data reduction tool, exact feature/sample screening would be of great practical importance because they can effectively reduce the data size without sacrificing the optimality [16].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "By the Lagrangian multipliers method [4] (see the supplement), the dual problem of SGL is",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].",
      "startOffset" : 201,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].",
      "startOffset" : 201,
      "endOffset" : 216
    }, {
      "referenceID" : 28,
      "context" : "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].",
      "startOffset" : 201,
      "endOffset" : 216
    }, {
      "referenceID" : 5,
      "context" : "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].",
      "startOffset" : 201,
      "endOffset" : 216
    }, {
      "referenceID" : 2,
      "context" : "5 in [3].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "[2] Let h, g ∈ Γ0(R).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[10] Let f1, · · · , fk ∈ Γ0(R).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[Fenchel-Young inequality] [3] Any point z ∈ Rn and w in the domain of a function h : Rn → (−∞,∞] satisfy the inequality h(w) + h∗(z) ≥ 〈w, z〉.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "(15) are the so-called KKT conditions [4] and can also be obtained by the Lagrangian multiplier method (see A.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "[2] Let C1 and C2 be nonempty subsets of Rn.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "Inspired by the SAFE rules [6], we can first estimate a region Θ containing θ∗(λ, α).",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "1, we give an accurate estimation of θ∗(λ, α) via normal cones [20].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : ", θ∗(λ, α) = PFα(y/λ), we have a very useful characterization of the dual optimal solution via the so-called normal cones [20].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "[20, 2] For a closed convex set C ∈ Rn and a point w ∈ C, the normal cone to C at w is defined by NC(w) = {v : 〈v,w′ −w〉 ≤ 0, ∀w′ ∈ C}.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "[20, 2] For a closed convex set C ∈ Rn and a point w ∈ C, the normal cone to C at w is defined by NC(w) = {v : 〈v,w′ −w〉 ≤ 0, ∀w′ ∈ C}.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "[9] Suppose that h ∈ Γ0 and C is a nonempty closed convex set.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "The solver used in this paper is from SLEP [12].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 24,
      "context" : "1 Simulation Studies We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36].",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 33,
      "context" : "1 Simulation Studies We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36].",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "01 by (a): the solver [12] without screening; (b): the solver combined with TLFre.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : ", G, which can be efficiently computed by the power method [8].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "We integrate DPC with the solver [12] to solve the nonnegative Lasso problem along a sequence of 100 parameter values of λ equally spaced on the logarithmic scale of λ/λmax from 1.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "01 by (a): the solver [12] without screening; (b): the solver combined with TLFre.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : "a) Breast Cancer data set [32, 21]: this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 × 7129).",
      "startOffset" : 26,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "a) Breast Cancer data set [32, 21]: this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 × 7129).",
      "startOffset" : 26,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "b) Leukemia data set [1]: this data set contains 11225 gene expression values of 52 samples (X ∈ R52×11225).",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "c) Prostate Cancer data set [19]: this data set contains 15154 measurements of 132 patients (X ∈ R132×15154).",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "d) PIE face image data set [22, 5]: this data set contains 11554 gray face images (each has 32× 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "d) PIE face image data set [22, 5]: this data set contains 11554 gray face images (each has 32× 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "e) MNIST handwritten digit data set [11]: this data set contains grey images of scanned handwritten digits (each has 28 × 28 pixels).",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "f) Street View House Number (SVHN) data set [15]: this data set contains color images of street view house numbers (each has 32 × 32 pixels), including 73257 images for training and 26032 for testing.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "01 by (a): the solver [12] without screening; (b): the solver combined with DPC.",
      "startOffset" : 22,
      "endOffset" : 26
    } ],
    "year" : 2014,
    "abstractText" : "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the `1 and `2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method—called DPC (decomposition of convex set)—for the nonnegative Lasso problem. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.",
    "creator" : "LaTeX with hyperref package"
  }
}