{
  "name" : "1703.04274.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning with Local Permutations and Delayed Feedback",
    "authors" : [ "Ohad Shamir", "Liran Szlak" ],
    "emails" : [ "ohad.shamir@weizmann.ac.il", "liran.szlak@weizmann.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "regret can be improved to O( √ T (1 + √ τ2/M)), using a Mirror-Descent based algorithm which can be applied for both Euclidean and non-Euclidean geometries. We also prove a lower bound, showing that for M < τ/3, it is impossible to improve the standard O( √ τT ) regret bound by more than constant factors. Finally, we provide some experiments validating the performance of our algorithm."
    }, {
      "heading" : "1 Introduction",
      "text" : "Online learning is traditionally posed as a repeated game where the learner has to provide predictions on an arbitrary sequence of loss functions, possibly even generated adversarially. Although it is often possible to devise algorithms with non-trivial regret guarantees, these have to cope with arbitrary loss sequences, which makes them conservative and in some cases inferior to algorithms not tailored to cope with worstcase behavior. Indeed, an emerging line of work considers how better online learning can be obtained on “easy” data, which satisfies some additional assumptions. Some examples include losses which are sampled i.i.d. from some distribution, change slowly in time, have a consistently best-performing predictor across time, have some predictable structure, mix adversarial and stochastic losses, etc. (e.g. Sani et al. [2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).\nIn this paper, we take a related but different direction: Rather than explicitly excluding highly adversarial loss sequences, we consider how slightly perturbing them can mitigate their worst-case behavior, and lead to improved performance. Conceptually, this resembles smoothed analysis Spielman and Teng [2004], in which one considers the worst-case performance of some algorithm, after performing some perturbation to their input. The idea is that if the worst-case instances are isolated and brittle, then a perturbation will lead to easier instances, and better reflect the attainable performance in practice.\nar X\niv :1\n70 3.\n04 27\n4v 1\n[ cs\n.L G\n] 1\n3 M\nar 2\nSpecifically, we propose a setting, in which the learner is allowed to slightly reorder the sequence of losses generated by an adversary: Assuming the adversary chooses losses h1, . . . , hT , and before any losses are revealed, the learner may choose a permutation σ on {1, . . . , T}, satisfying maxt |t − σ(t)| ≤ M for some parameter M , and then play a standard online learning game on losses hσ(1), . . . , hσ(T ). We denote this as the Online Learning with Local Permutations (OLLP) setting. Here, M controls the amount of power given to the learner: M = 0 means that no reordering is performed, and the setting is equivalent to standard adversarial online learning. At the other extreme, M = T means that the learner can reorder the losses arbitrarily. For example, the learner may choose to order the losses uniformly at random, making it a quasistochastic setting (the only difference compared to i.i.d. losses is that they are sampled without-replacement rather than with-replacement).\nWe argue that allowing the learner some flexibility in the order of responses is a natural assumption. For example, when the learner needs to provide rapid predictions on a high-frequency stream of examples, it is often immaterial if the predictions are not provided in the exact same order at which the examples arrived. Indeed, by buffering examples for a few rounds before being answered, one can simulate the local permutations discussed earlier.\nWe believe that this setting can be useful in various online learning problems, where it is natural to change a bit the order of the loss functions. In this paper, we focus on one well-known problem, namely online learning with delayed feedback. In this case, rather than being provided with the loss function immediately after prediction is made, the learner only receives the loss function after a certain number τ ≥ 1 of rounds. This naturally models situations where the feedback comes much more slowly than the required frequency of predictions: To give a concrete example, consider a web advertisement problem, where an algorithm picks an ad to display, and then receives a feedback from the user in the form of a click. It is likely that the algorithm will be required to choose ads for new users while still waiting for the feedback from the previous user.\nFor convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( √ τT ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al. [2009], Joulani et al. [2013], Quanrud and Khashabi [2015]. On the other hand, in a stochastic setting where the losses are sampled i.i.d. from some distribution, Agarwal and Duchi [2011] show that the attainable regret is much better, on the order of O( √ T + τ). This gap between the worst-case adversarial setting, and the milder i.i.d. setting, hints that this problem is a good fit for our OLLP framework. Thus, in this paper, we focus on online learning with feedback delayed up to τ rounds, in the OLLP framework where the learner is allowed to locally permute the loss functions (up to a distance of M ). First, we devise an algorithm, denoted as Delayed Permuted Mirror Descent, and prove that it achieves an expected regret bound of order O( √ T (τ2/M + 1)) assuming M ≥ τ . As M increases compared to τ , this regret bound interpolates between the standard adversarial √ τT regret, and a milder √ T regret, typical of i.i.d. losses. As its name implies, the algorithm is based on the well-known online mirror descent (OMD) algorithm (see Hazan et al. [2016], Shalev-Shwartz et al. [2012]), and works in the same generality, involving both Euclidean and non-Euclidean geometries. The algorithm is based on dividing the entire sequence of functions into blocks of size M and performing a random permutation within each block. Then, two copies of OMD are ran on different parts of each block, with appropriate parameter settings. A careful analysis, mixing adversarial and stochastic elements, leads to the regret bound.\nIn addition, we provide a lower bound complementing our upper bound analysis, showing that when M is significantly smaller than τ (specifically, τ/3), then even with local permutations, it is impossible to obtain a worse-case regret better than Ω( √ τT ), matching (up to constants) the attainable regret in the\nstandard adversarial setting where no permutations are allowed. Finally, we provide some experiments validating the performance of our algorithm.\nThe rest of the paper is organized as follows: in section 2 we formally define the Online Learning with Local Permutation setting, section 3 describes the Delayed Permuted Mirror Descent algorithm and outlines its regret analysis, section 4 discusses a lower bound for the delayed setting with limited permutation power, section 5 shows experiments, and finally section 6 provides concluding remarks, discussion, and open questions. Appendix A contains most of the proofs."
    }, {
      "heading" : "2 Setting and Notation",
      "text" : "Convex Online Learning. Convex online learning is posed as a repeated game between a learner and an adversary (assumed to be oblivious in this paper). First, the adversary chooses T convex losses h1, . . . , hT which are functions from a convex set W to R. At each iteration t ∈ {1, 2, . . . , T}, the learner makes a prediction wt, and suffers a loss of ht (wt). To simplify the presentation, we use the same notation ∇ht(w) to denote either a gradient of ht at w (if the loss is differentiable) or a subgradient at w otherwise, and refer to it in both cases as a gradient. We assume that both w ∈ W and the gradients of any function ht in any point w ∈ W are bounded w.r.t. some norm: Given a norm ‖ · ‖ with a dual norm ‖ · ‖∗, we assume that the diameter of the spaceW is bounded by B2 and that ∀w ∈ W,∀h ∈ {h1, h2, ..., hT } : ‖∇h (w) ‖∗ ≤ G. The purpose of the learner is to minimize her (expected) regret, i.e.\nR(T ) = E [ T∑ t=1 ht (wt)− T∑ t=1 ht (w ∗) ] where w∗ = argmin w∈W T∑ t=1 ht (w)\nwhere the expectation is with respect to the possible randomness of the algorithm. Learning with Local Permutations. In this paper, we introduce and study a variant of this standard setting, which gives the learner a bit more power, by allowing her to slightly modify the order in which the losses are processed, thus potentially avoiding highly adversarial but brittle loss constructions. We denote this setting as the Online Learning with Local Permutations (OLLP) setting. Formally, letting M be a permutation window parameter, the learner is allowed (at the beginning of the game, and before any losses are revealed) to permute h1, . . . , hT to hσ−1(1), . . . , hσ−1(T ), where σ is a permutation from the set Perm := {σ : ∀t, |σ (t)− t| ≤M}. After this permutation is performed, the learner is presented with the permuted sequence as in the standard online learning setting, with the same regret as before. To simplify notation, we let ft = hσ−1(t), so the learner is presented with the loss sequence f1, . . . , fT , and the regret\nis the same as the standard regret, i.e. R(T ) = E [∑T t=1 ft(wt)− ∑T t=1 ft(w ∗) ] . Note that if M = 0 then we are in the fully adversarial setting (no permutation is allowed). At the other extreme, if M = T and σ is chosen uniformly at random, then we are in a stochastic setting, with a uniform distribution over the set of functions chosen by the adversary (note that this is close but differs a bit from a setting of i.i.d. losses). In between, as M varies, we get an interpolation between these two settings.\nLearning with Delayed Feedback. The OLLP setting can be useful in many applications, and can potentially lead to improved regret bounds for various tasks, compared to the standard adversarial online learning. In this paper, we focus on studying its applicability to the task of learning from delayed feedback.\nWhereas in standard online learning, the learner gets to observe the loss ft immediately at the end of iteration t, here we assume that at round t, she only gets to observe ft−τ for some delay parameter τ < T (and if t < τ , no feedback is received). For simplicity, we focus on the case where τ is fixed, independent of t, although our results can be easily generalized (as discussed in subsection 3.3). We emphasize that this\nis distinct from another delayed feedback scenario sometimes studied in the literature (Agarwal and Duchi [2011], Langford et al. [2009]), where rather than receiving ft−τ the learner only receives a (sub)gradient of ft−τ at wt−τ . This is a more difficult setting, which is relevant for instance when the delay is due to the time it takes to compute the gradient."
    }, {
      "heading" : "3 Algorithm and Analysis",
      "text" : "Our algorithmic approach builds on the well-established online mirror descent framework. Thus, we begin with a short reminder of the Online Mirror Descent algorithm (see e.g. Hazan et al. [2016] for more details). Readers who are familiar with the algorithm are invited to skip to Subsection 3.1.\nThe online mirror descent algorithm is a generalization of online gradient descent, which can handle non-Euclidean geometries. The general idea is the following: we start with some point wt ∈ W , where W is our primal space. We then map this point to the dual space using a (striclty convex and continuously differentiable) mirror map ψ, i.e. ∇ψ (wt) ∈ W∗, then perform the gradient update in the dual space, and finally map the resulting new point back to our primal spaceW again, i.e. we want to find a point wt+1 ∈ W s.t. ∇ψ (wt+1) = ∇ψ (wt) − η · gt where gt denotes the gradient. Denoting by wt+ 1\n2 the point satisfying\n∇ψ(wt+ 1 2 ) = ∇ψ (wt) − η · gt, it can be shown that wt+ 1 2 = (∇ψ∗) (∇ψ (wt)− η · gt), where ψ∗ is the dual function of ψ. This point, wt+ 1 2 , might lie outside our hypothesis classW , and thus we might need to project it back to our spaceW . We use the Bregman divergence associated to ψ to do this:\nwt+1 = argmin w∈W 4ψ(w,wt+ 1 2 ),\nwhere the Bregman divergence ∆ψ is defined as\n4ψ (x, y) = ψ (x)− ψ (y)− 〈∇ψ(y), x− y〉.\nSpecific choices of the mirror map ψ leads to specific instantiations of the algorithms for various geometries. Perhaps the simplest example is ψ (x) = 12‖x‖ 2 2, with associated Bregman divergence 4ψ (x, y) = 1 2 · ‖x − y‖\n2. This leads us to the standard and well-known online gradient descent algorithm, where wt+1 is the Euclidean projection on the setW of wt − η · gt.\nAnother example is the negative entropy mirror map ψ (x) = ∑n\ni=1 xi · log (xi), which is 1-strongly convex with respect to the 1-norm on the simplexW = { x ∈ Rn+ : ∑n i=1 xi = 1 } . In that case, the resulting\nalgorithm is the well-known multiplicative updates algorithm, wherewt+1,i = wt,i·exp(−ηgt,i)/ ∑n\nj=1wt,i· exp(−ηgt,j). Instead of the 1-norm on the simplex, one can also consider arbitrary p-norms, and take ψ(x) = 12 · ‖x‖ 2 q , where q is the dual norm (satisfying 1/p+ 1/q = 1)."
    }, {
      "heading" : "3.1 The Delayed Permuted Mirror Descent Algorithm",
      "text" : "Before describing the algorithm, we note that we will focus here on the case where the permutation window parameter M is larger than the delay parameter τ . If M < τ , then our regret bound is generally no better than the O( √ τT ) obtainable by a standard algorithm without any permutations, and for M < τ/3, this is actually tight as shown in Section 4. We now turn to present our algorithm, denoted as The Delayed Permuted Mirror Descent algorithm (see algorithm 1 below as well as figure 1 for a graphical illustration). First, the algorithm splits the time horizon T into M consecutive blocks, and performs a uniformly random permutation on the loss functions within each block. Then, it runs two online mirror descent algorithms in parallel, and uses the delayed gradients in\norder to update two separate predictors – wf and ws, where wf is used for prediction in the first τ rounds of each block, and ws is used for prediction in the remaining M − τ rounds (here, f stands for “first” and s stands for “second”). The algorithm maintaining ws crucially relies on the fact that the gradient of any two functions in a block (at some point w) is equal, in expectation over the random permutation within each block. This allows us to avoid most of the cost incurred by delays within each block, since the expected gradient of a delayed function and the current function are equal. A complicating factor is that at the first τ rounds of each block, no losses from the current block has been revealed so far. To tackle this, we use another algorithm (maintaining wf ), specifically to deal with the losses at the beginning of each block. This algorithm does not benefit from the random permutation, and its regret scales the same as standard adversarial online learning with delayed feedback. However, as the block size M increases, the proportion of losses handled by wf decreases, and hence its influence on the overall regret diminishes.\nThe above refers to how the blocks are divided for purposes of prediction. For purposes of updating the predictor of each algorithm, we need to use the blocks a bit differently. Specifically, we let T1 and T2 be two sets of indices. T1 includes all indices from the first τ time points of every block, and is used to update wf . T2 includes the first M − τ indices of every block, and is used to update ws (see figure 1). Perhaps surprisingly, note that T1 and T2 are not disjoint, and their union does not cover all of {1, . . . , T}. The reason is that due to the random permutation in each block, the second algorithm only needs to update on some of the loss functions in each block, in order to obtain an expected regret bound on all the losses it predicts on.\nAlgorithm 1 Delayed Permuted Mirror Descent Input: M , ηf , ηs Init: wf1 = 0, w s 1 = 0, jf = js = 1\nDivide T to consecutive blocks of sizeM , and permute the losses uniformly at random within each block. Let f1, . . . , fT denote the resulting permuted losses. for t = 1..., T do\nif t ∈ first τ rounds of the block then Predict using wfjf Receive a loss function from τ places back: ft−M = fT1(jf−τ). If none exists (in the first τ iterations), take the 0 function. Compute: ∇fT1(jf−τ) ( wfjf−τ\n) Update: wf\njf+ 1 2\n= (∇ψ∗) ( ∇ψ ( wfjf ) − ηf∇fT1(jf−τ) ( wfjf−τ )) Project: wjf+1 = argmin\nw∈W 4ψ\n( w,wf\njf+ 1 2 ) jf = jf + 1\nelse Predict using wsjs Receive a loss function from τ places back: ft−τ = fT2(js) Compute: ∇ft−τ ( wsjs ) = ∇fT2(js) ( wsjs\n) Update: ws\njs+ 1 2\n= (∇ψ∗) ( ∇ψ ( wsjs ) − ηs · ∇fT2(js) ( wsjs )) Project: wjs+1 = argmin\nw∈W 4ψ\n( w,ws\njs+ 1 2 ) js = js + 1\nend if end for"
    }, {
      "heading" : "3.2 Analysis",
      "text" : "The regret analysis of the Delayed Permuted Mirror Descent algorithm is based on a separate analysis of each of the two mirror descent sub-algorithms, where in the first sub-algorithm the delay parameter τ enters multiplicatively, but doesn’t play a significant role in the regret of the second sub-algorithm (which utilizes the stochastic nature of the permutations). Combining the regret bound of the two sub-algorithms, and using the fact that the portion of losses predicted by the second algorithm increases with M , leads to an overall regret bound improving in M .\nIn the proof, to analyze the effect of delay, we need a bound on the distance between any two consequent predictors wt, wt+1 generated by the sub-algorithm. This depends on the mirror map and Bregman divergence used for the update, and we currently do not have a bound holding in full generality. Instead, we let Ψ(ηf ,G) be some upper bound on ‖wt+1 − wt‖, where the update is using step-size ηf and gradients of norm ≤ G. Using Ψ(ηf ,G) we prove a general bound for all mirror maps. In Lemmas 3 and 4 in Appendix A.1, we show that for two common mirror maps (corresponding to online gradient descent and multiplicative weights), Ψ(ηf ,G) ≤ c · ηfG for some numerical constant c, leading to a regret bound of O( √ T (τ2/M + 1)). Also, we prove theorem 1 for 1-strongly convex mirror maps, although it can be generalized to any λ-strongly convex mirror map by scaling.\nTheorem 1. Given a norm ‖ ·‖, suppose that we run the Delayed Permuted Mirror Descent algorithm using a mirror map ψ which is 1-strongly convex w.r.t. ‖ · ‖, over a domainW with diameter B2 w.r.t the bregman divergence of ψ: ∀w, v ∈ W : 4ψ(w, v) ≤ B2, and such that the (sub)-gradient g of each loss function on any w ∈ W satisfies ‖g‖∗ ≤ G (where ‖ · ‖∗ is the dual norm of ‖ · ‖). Then the expected regret, given a delay parameter τ and step sizes ηf , ηs satisfies:\nE [ T∑ t=1 ft (wt)− ft (w∗) ] ≤ B 2 ηf + ηf · Tτ M · G 2 2 + Tτ2 M ·G ·Ψ(ηf ,G) + B2 ηs + ηs · T · (M − τ) M · G 2 2\nFurthermore, if Ψ(ηf ,G) ≤ c · ηfG for some constant c, and ηf = B· √ M G· √ T ·τ ·( 12+c·τ) , ηs = B· √ 2M G· √ T ·(M−τ) , the regret is bounded by\nc\n√ Tτ\nM ·BG\n√ 1\n2 + c · τ +\n√ 2T (M − τ)\nM ·BG = O\n( √ T · (√ τ2\nM + 1\n))\nWhen M = O(τ), this bound is O( √ τT ). similar to the standard adversarial learning case. However, as M increases, the regret gradually improves to O( √ T + τ), which is the regret attainable in a purely stochastic setting with i.i.d. losses. The full proof can be found in appendix A.1.1, and we sketch below the main ideas.\nFirst, using the definition of regret, we show that it is enough to upper-bound the regret of each of the two sub-algorithms separately. Then, by a standard convexity argument, we reduce this to bounding sums of terms of the form E[〈wft − w∗f ,∇ft(w f t )〉] for the first sub-algorithm, and E [〈wst − w∗s ,∇ft (wst )〉] for the second sub-algorithm (where w∗f and w ∗ s are the best fixed points in hindsight for the losses predicted on by the first and second sub-algorithms, respectively, and where for simplicity we assume the losses are differentiable). In contrast, we can use the standard analysis of mirror descent, using delayed gradients, to get a bound for the somewhat different terms E[〈wft − w∗f ,∇ft−τ (w f t−τ )〉] for the first sub-algorithm, and E [〈wst − w∗s ,∇ft−τ (wst )〉] for the second sub-algorithm. Thus, it remains to bridge between these terms. Starting with the second sub-algorithm, we note that since we performed a random permutation within each block, the expected value of all loss functions within a block (in expectation over the block, and evaluated at a fixed point) is equal. Moreover, at any time point, the predictor ws maintained by the second sub-algorithm does not depend on the delayed nor the current loss function. Therefore, conditioned on wst , and in expectation over the random permutation in the block, we have that\nE[∇ft(wst )] = E[ft−τ (wst )]\nfrom which it can be shown that\nE [〈wst − w∗s ,∇ft(wst )〉] = E [〈wst − w∗s ,∇ft−τ (wst )〉]\nThus, up to a negligible factor having to do with the first few rounds of the game, the second sub-algorithm’s expected regret does not suffer from the delayed feedback.\nFor the first sub-algorithm, we perform an analysis which does not rely on the random permutation. Specifically, we first show that since we care just about the sum of the losses, it is sufficient to bound the difference between E[〈wft − w∗f ,∇ft(w f t )〉] and E[〈w f t+τ − w∗f ,∇ft(w f t )〉]. Using Cauchy-Shwartz, this difference can be upper bounded by ‖wft −w f t+τ‖ · ‖∇ft(w f t )‖, which in turn is at most c · τ · ηf ·G2 using\nour assumptions on the gradients of the losses and the distance between consecutive predictors produced by the first sub-algorithm.\nOverall, we get two regret bounds, one for each sub-algorithm. The regret of the first sub-algorithm scales with τ , similar to the no-permutation setting, but the sub-algorithm handles only a small fraction of the iterations (the first τ in every block of size M ). In the rest of the iterations, where we use the second sub-algorithm, we get a bound that resembles more the stochastic case, without such dependence on τ . Combining the two, the result stated in Theorem 1 follows."
    }, {
      "heading" : "3.3 Handling Variable Delay Size",
      "text" : "So far, we discussed a setting where the feedback arrives with a fixed delay of size τ . However, in many situations the feedback might arrive with a variable delay size τt at any iteration t, which may raise a few issues. First, feedback might arrive in an asynchronous fashion, causing us to update our predictor using gradients from time points further in past after already using more recent gradients. This complicates the analysis of the algorithm. A second, algorithmic problem, is that we could also possibly receive multiple feedbacks simultaneously, or no feedback at all, in certain iterations, since the delay is of variable size. One simple solution is to use buffering and reduce the problem to a constant delay setting. Specifically, we assume that all delays are bounded by some maximal delay size τ . We would like to use one gradient to update our predictor at every iteration (this is mainly for ease of analysis, practically one could update the predictor with multiple loss functions in a single iteration). In order to achieve this, we can use a buffer to store loss functions that were received but have not been used to update the predictors yet. We defineGradf and Grads, two buffers that will contain gradients from time points in T1 or T2, correspondingly. Each buffer is of size τ . If we denote by Ft the set of function that have arrived in time t, we can simply store loss functions that have arrived asynchronously in the buffers defined above, sort them in ascending order, and take the delayed loss function from exactly τ iterations back in the update step. This loss function must be in the appropriate buffer since the maximal delay size is τ . From this moment on, the algorithm can proceed as usual and its analysis still applies."
    }, {
      "heading" : "4 Lower Bound",
      "text" : "In this section, we give a lower bound in the setting where M < τ3 with all feedback having delay of exactly τ . We will show that for this case, the regret bound cannot be improved by more than a constant factor over the bound of the adversarial online learning problem with a fixed delay of size τ , namely Ω (√ τT )\nfor a sequence of length T . We hypothesize that this regret bound also cannot be significantly improved for any M = O(τ) (and not just τ/3). However, proving this remains an open problem.\nTheorem 2. For every (possible randomized) algorithmA with a permutation window of sizeM ≤ τ3 , there exists a choice of linear, 1-Lipschitz functions over [−1, 1] ⊂ R, such that the expected regret of A after T rounds (with respect to the algorithm’s randomness), is\nE [ T∑ t=1 ft (wt)− T∑ t=1 ft (w ∗) ] = Ω (√ τT ) where w∗ = argmin w∈W T∑ t=1 ft (w)\nFor completeness, we we also provide in appendix A.2 a proof that when M = 0 (i.e. no permutations allowed), then the worst-case regret is no better than Ω( √ τT ). This is of course a special case of Theorem 2,\nbut applies to the standard adversarial online setting (without any local permutations), and the proof is simpler. The proof sketch for the setting where no permutation is allowed was already provided in Langford et al. [2009], and our contribution is in providing a full formal proof.\nThe proof in the case where M = 0 is based on linear losses of the form ft = αt · wt over [−1,+1], where αt ∈ {−1,+1}. Without permutations, it is possible to prove a Ω( √ τT ) lower bound by dividing the T iterations into blocks of size τ , where the α values of all losses at each block is the same and randomly chosen to equal either +1 or−1. Since the learner does not obtain any information about this value until the block is over, this reduces to adversarial online learning over T/τ rounds, where the regret at each round scales linearly with τ , and overall regret at least Ω(τ √ T/τ) = Ω( √ τT ).\nIn the proof of theorem 2, we show that by using a similar construction, even with permutations, having a permutation window less than τ/3 still means that the α values would still be unknown until all loss functions of the block are processed, leading to the same lower bound up to constants.\nThe formal proof appears in the appendix, but can be sketched as follows: first, we divide the T iterations into blocks of size τ/3. Loss functions within each block are identical, of the form ft = αt · wt, and the value of α per block is chosen uniformly at random from {−1,+1}, as before. Since here, the permutation window M is smaller than τ/3, then even after permutation, the time difference between the first and last time we encounter an α that originated from a single block is less than τ . This means that by the time we get any information on the α in a given block, the algorithm already had to process all the losses in the block, which leads to the same difficulty as the no-permutation setting. Specifically, since the predictors chosen by the algorithm when handling the losses of the block do not depend on the α value in that block, and that α is chosen randomly, we get that the expected loss of the algorithm at any time point t equals 0. Thus, the cumulative loss across the entire loss sequence is also 0. In contrast, for w∗, the optimal predictor in hindsight over the entire sequence, we can prove an expected accumulated loss of −Ω( √ τT ) after T iterations, using Khintchine inequality and the fact that the α’s were randomly chosen per block. This leads us to a lower bound of expected regret of order √ τT , for any algorithm with a local permutation window of size M < τ/3."
    }, {
      "heading" : "5 Experiments",
      "text" : "We consider the adversarial setting described in section 4, where an adversary chooses a sequence of functions such that every τ functions are identical, creating blocks of size τ of identical loss functions, of the form ft(wt) = αt · wt where αt is chosen randomly in {−1,+1} for each block. In all experiments we use T = 105 rounds, a delay parameter of τ = 200, set our step sizes according to the theoretical analysis, and report the mean regret value over 1000 repetitions of the experiments.\nIn our first experiment, we considered the behavior of our Delayed Permuted Mirror Descent algorithm, for window sizes M > τ , ranging from τ + 1 to T . In this experiment, we chose the α values randomly, while ensuring a gap of 200 between the number of blocks with +1 values and the number of blocks with−1 values (this ensures that the optimal w∗ is a sufficiently strong competitor, since otherwise the setting is too “easy” and the algorithm can attain negative regret in some situations). The results are shown in Figures 2 and 3, where the first figure presents the accumulated regret of our algorithm over time, whereas the second figure presents the overall regret after T rounds, as a function of the window size M .\nWhen applying our algorithm in this setting with different values of M > τ , ranging from M = τ + 1 and up to M = T , we get a regret that scales from the order of the adversarial bound to the order of the stochastic bound depending on the window size, as expected by our analysis. For all window sizes greater than 5 · τ , we get a regret that is in the order of the stochastic bound - this is not surprising, since after\nthe permutation we get a sequence of functions that is very close to an i.i.d. sequence, in which case any algorithm can be shown to achieve O( √ T ) regret in expectation. Note that this performance is better than that predicted by our theoretical analysis, which implies an O( √ T ) behavior only when M ≥ Ω(τ2). It is an open and interesting question whether it means that our analysis can be improved, or whether there is a harder construction leading to a tighter lower bound.\nIn our second experiment, we demonstrate the brittleness of the lower bound construction for standard online learning with delayed feedback, focusing on theM < τ regime. Specifically, we create loss functions with blocks as before (where following the lower bound construction, the α values in each block of size τ = 200 is chosen uniformly at random). Then, we perform a random permutation over consecutive windows of size M (ranging from M = 0 up to M = 910τ in intervals of 1 10τ ). Finally, we run standard Online\nGradient Descent with delayed gradients (and fixed step size 1/ √ T ), on the permuted losses. The results are presented in Figure 4. For window sizesM < τ2 we see that the regret is close to the adversarial bound, whereas as we increase the window size the regret decreases towards the stochastic bound. This experiment evidently shows that this hardness construction is indeed brittle, and easily breaks in the face of local permutations, even for window sizes M < τ ."
    }, {
      "heading" : "6 Discussion",
      "text" : "We presented the OLLP setting, where a learner can locally permute the sequence of examples from which she learns. This setting can potentially allow for improved learning in many problems, where the worst-case regret is based on highly adversarial yet brittle constructions. In this paper, we focused on the problem of learning from delayed feedback in the OLLP setting, and showed how it is possible to improve the regret\nby allowing local permutations. Also, we proved a lower bound in the situation where the permutation window is significantly smaller than the feedback delay, and showed that in this case, permutations cannot allow for a better regret bound than the standard adversarial setting. We also provided some experiments, demonstrating the power of the setting as well as the feasibility of the proposed algorithm. An interesting open question is what minimal permutation size allows non-trivial regret improvement, and whether our upper bound in Theorem 1 is tight. As suggested by our empirical experiments, it is possible that even small local permutations are enough to break highly adversarial sequences and improve performance in otherwise worst-case scenarios. Another interesting direction is to extend our results to a partial feedback (i.e. bandit) setting. Finally, it would be interesting to study other cases where local permutations allow us to interpolate between fully adversarial and more benign online learning scenarios."
    }, {
      "heading" : "Acknowledgements",
      "text" : "OS is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel Science Foundation grant 425/13."
    }, {
      "heading" : "A Proofs",
      "text" : "A.1 Analysis Of The Delayed Permuted Mirror Descent Algorithm\nWe will use throughout the proofs the well known Pythagorean Theorem for Bregman divergences, and the ’projection’ lemma that considers the projection step in the algorithm.\nLemma 1. Pythagorean Theorem for Bregman divergences Let v be the projection ofw onto a convex setW w.r.t Bregman divergence4ψ: v = argminu∈W4ψ (u,w), then: 4ψ (u,w) ≥ 4ψ (u, v) +4ψ (v, w)\nLemma 2. Projection Lemma LetW be a closed convex set and let v be the projection of w ontoW , namely, v = argmin\nx∈W ‖x− w‖2. Then, for every u ∈ W , ‖w − u‖2 − ‖v − u‖2 ≥ 0\nThe following lemma gives a bound on the distance between two consequent predictions when using the Euclidean mirror map:\nLemma 3. Let g ∈ Rn s.t. ‖g‖2 < G,W a convex set, and η > 0 be fixed. Let w ∈ W and w2 = w− η · g. Then, for w′ = argmin\nu∈W ‖w2 − u‖22, we have that ‖w − w′‖ ≤ η ·G\nProof. From the projection lemma: ‖w2 − w‖22 ≥ ‖w′ − w‖22 and so: ‖w2 − w‖2 ≥ ‖w′ − w‖2. From definition: ‖w2 − w‖2 = ‖η · g‖2 ≤ η ·G. and so we get: ‖w′ − w‖2 ≤ ‖w2 − w‖2 ≤ η ·G\nWe prove a modification of Lemma 2 given in Menache et al. [2014] in order to bound the distance between two consequent predictions when using the negative entropy mirror map:\nLemma 4. Let g ∈ Rn s.t. ‖g‖1 ≤ G for some G > 0 and let η > 0 be fixed, with η < 1√2·G . For any distribution vector w in the n− simplex, if we define w′ to be the new distribution vector\n∀i ∈ {1, ..., n} , w′i = wi · exp (−η · gi)∑n j=1wj · exp (−η · gj)\nThen ‖w − w′‖1 ≤ 3ηG\nProof. Since ‖g‖∞ < G and η < 1√2·G we get that ∀i : |η · gi| < 1. We have that:\n‖w − w′‖1 = n∑ i=1 |wi − w′i| = n∑ i=1 ∣∣∣∣∣wi · ( 1− exp (−η · gi)∑n j=1wj · exp (−η · gj) )∣∣∣∣∣ Since ‖w‖1 = 1, we can apply Holder’s inequality, and upper bound the above by\nmax i ∣∣∣∣∣1− exp (−η · gi)∑nj=1wj · exp (−η · gj) ∣∣∣∣∣\nUsing the inequality 1− x ≤ exp (−x) ≤ 11+x for all |x| ≤ 1, we know that\n1− η · gi ≤ exp (−η · gi) ≤ 1\n1 + η · gi\nand since −ηG ≤ η · gi ≤ ηG we have that\n1− η · gi ≤ exp (−η · gi) ≤ 1 1 + η · gi ≤ 1 1− ηG\nand so we get:\n1− 1 1+ηgi 1 + ηG ≤ 1− exp (−η · gi)∑n j=1wj · exp (−η · gj) ≤ 1− 1− η · gi1\n1−ηG\nUsing again the fact that −ηG ≤ η · gi ≤ ηG, we have\n1− 1\n1−ηG 1 + ηG ≤ 1− exp (−η · gi)∑n j=1wj · exp (−η · gj) ≤ 1− 1− ηG1 1−ηG\n=⇒ −η 2G2 1− η2G2 = 1− 1 1− η2G2 ≤ 1− exp (−η · gi)∑n j=1wj · exp (−η · gj) ≤ 1− (1− ηG)2 = 2ηG+ η2G2\nNow, since ηG < 1, we get that:\n−η2G2 1− η2G2 ≤ 1− exp (−η · gi)∑n j=1wj · exp (−η · gj) ≤ 2ηG+ ηG = 3ηG\nand so we can conclude that\nmax i ∣∣∣∣∣1− exp (−η · gi)∑nj=1wj · exp (−η · gj) ∣∣∣∣∣ ≤ maxi (∣∣∣∣ −η2G21− η2G2 ∣∣∣∣ , |3ηG|) ≤ maxi ( ηG 1− η2G2 , 3ηG )\nSince η < 1√ 2G , we get (η ·G)2 < 12 . Thus we get:\nmax i ∣∣∣∣∣1− exp (−η · gi)∑nj=1wj · exp (−η · gj) ∣∣∣∣∣ ≤ maxi (2ηG, 3ηG) ≤ 3ηG\nwhich gives us our desired bound.\nWith the above two lemmas in hand, we bound the distance between consequent predictors by cηG, where c is a different constant in each mirror map: c = 1 for the euclidean case, and c = 3 for the negative entropy mirror map. Note that both mapping are 1-strongly convex with respect to their respective norms. For other mappings with a different strong convexity constant, one would need to scale the step sizes according to the strong convexity parameter in order to get the bound.\nA.1.1 Proof of Theorem 1\nWe provide an upper bound on the regret of the algorithm, by competing against the best fixed action in each one of the sets of iterations- the first τ iterations and the last M − τ iterations in each block. This is\nan upper bound on competing against the best fixed predictor in hindsight for the entire sequence. Formally, we bound:\nR(T ) = E [ T∑ t=1 ft (wt)− T∑ t=1 ft (w ∗) ]\n≤ E  TM−1∑ i=0  M ·i+τ∑ t=M ·i+1 ft (wt)− ft ( w∗f ) + M ·(i+1)∑ t=M ·i+τ+1 ft (wt)− ft (w∗s)  where\nw∗f = argmin w∈W\nT M −1∑\ni=0 M ·i+τ∑ t=M ·i+1 ft (w) and w∗s = argmin w∈W T M −1∑ i=0 M ·(i+1)∑ t=M ·i+τ+1 ft (w)\nwhere expectation is taken over the randomness of the algorithm. The diameter of the domainW is bounded by B2, and so4ψ ( w∗f , w f 0 ) ≤ B2 and4ψ (w∗s , ws0) ≤ B2. We start with a general derivation that will apply both for ws and for wf simultaneously. For the following derivation we use the notation wj , wj+1 omitting the f, s superscript, for denoting subsequent updates of the predictor vector, whether it is ws or wf .\nDenote by gj the gradient used to update wj , i.e., ∇ψ ( wj+ 1\n2\n) = ∇ψ (wj) − η · gj , and wj+1 =\nargmin w∈W\n4ψ ( w,wj+ 1\n2\n) .\nLooking at the update step in the algorithm, we have that gj = 1η · ( ∇ψ (wj)−∇ψ ( wj+ 1\n2\n)) and thus:\n〈wj − w∗, gj〉 = 1 η · 〈 wj − w∗, ( ∇ψ (wj)−∇ψ ( wj+ 1 2 ))〉 = 1 η · ( 4ψ (w∗, wj) +4ψ ( wj , wj+ 1 2 ) −4ψ ( w∗, wj+ 1 2\n)) We now use the Pythagorean Theorem to get:\n≤ 1 η · ( 4ψ (w∗, wj) +4ψ ( wj , wj+ 1 2 ) −4ψ (w∗, wj+1)−4ψ ( wj+1, wj+ 1 2 )) When we sum terms for all updates of the predictor, wf or ws respectively, the terms 4ψ (w∗, wj) − 4ψ (w∗, wj+1) will result in a telescopic sum, canceling all terms expect the first and last. Thus we now concentrate on bounding the term: 4ψ ( wj , wj+ 1\n2\n) −4ψ ( wj+1, wj+ 1\n2\n) .\n4ψ ( wj , wj+ 1\n2\n) −4ψ ( wj+1, wj+ 1\n2\n) = ψ (wj)− ψ (wj+1)− 〈 wj − wj+1,∇ψ ( wj+ 1\n2 )〉 ≤\nψ 1-strong convex\n〈 wj − wj+1,∇ψ (wj)−∇ψ ( wj+ 1\n2\n)〉 − 1\n2 · ‖wj − wj+1‖2\n= 〈wj − wj+1, η · gj〉 − 1\n2 · ‖wj − wj+1‖2\n≤ η ·G · ‖wj − wj+1‖ − 1\n2 · ‖wj − wj+1‖2\n≤ (η ·G) 2\n2\nwhere the last inequality stems from the fact that ( ‖wj − wj+1‖ · √ 1√ 2 − η·G√ 2 )2 ≥ 0 We now continue with the analysis referring to wf and ws separately. Summing over j = τ + 1 to( T M + 1 ) · τ for wf (these are the TM τ iterations in which the first sub-algorithm is in use), and from j = 1 to TM · (M − τ) for w s (these are the TM (M − τ) iterations in which the second sub-algorithm is in use) we get: For wf : ( TM +1)·τ∑ j=τ+1 〈 wfj − w ∗ f , gj 〉\n= ( TM +1)·τ∑ j=τ+1 〈 wfj − w ∗ f ,∇fT1(j−τ) ( wfj−τ )〉\n= ( TM +1)·τ∑ j=τ+1 1 η · 〈 wfj − w ∗ f , ( ∇ψ ( wfj ) −∇ψ ( ws j+ 1 2 ))〉\n= ( TM +1)·τ∑ j=τ+1 1 η · ( 4ψ ( w∗f , w f j ) +4ψ ( wfj , w f j+ 1 2 ) −4ψ ( w∗f , w f j+ 1 2 )) ≤ ( TM +1)·τ∑ j=τ+1 1 η · ( 4ψ ( w∗f , w f j ) +4ψ ( wfj , w f j+ 1 2 ) −4ψ ( w∗f , w f j+1 ) −4ψ ( wfj+1, w f j+ 1 2 )) ≤ 1 η · ( TM +1)·τ∑ j=τ+1 4ψ ( w∗f , w f j ) −4ψ ( w∗f , w f j+1 ) + 1 η · ( TM +1)·τ∑ j=τ+1 4ψ ( wfj , w f j+ 1 2 ) −4ψ ( wfj+1, w f j+ 1 2 )\n= 1\nη · 4ψ\n( w∗f , w f τ+1 ) −4ψ ( w∗f , w f\n( TM +1)·τ\n) + 1 η · ( TM +1)·τ∑ j=τ+1 4ψ ( wfj , w f j+ 1 2 ) −4ψ ( wfj+1, w f j+ 1 2 )\n≤ 1 ηf · 4ψ\n( w∗f , w f τ+1 ) + 1\nηf · T M · τ ·\n(ηf ·G)2\n2\n≤ 1 ηf ·B2 + T M · τ ·\nηf ·G2\n2\nFor ws: T M ·(M−τ)∑ j=1 〈 wsj − w∗s , gj 〉\n= T M ·(M−τ)∑ j=1 〈 wsj − w∗s ,∇fT2(j) ( wsj )〉\n= T M ·(M−τ)∑ j=1 1 η · 〈 wsj − w∗s , ( ∇ψ ( wsj ) −∇ψ ( ws j+ 1 2 ))〉\n= T M ·(M−τ)∑ j=1 1 η · ( 4ψ ( w∗s , w s j ) +4ψ ( wsj , w s j+ 1 2 ) −4ψ ( w∗s , w s j+ 1 2 ))\n≤ T M ·(M−τ)∑ j=1 1 η · ( 4ψ ( w∗s , w s j ) +4ψ ( wsj , w s j+ 1 2 ) −4ψ ( w∗s , w s j+1 ) −4ψ ( wsj+1, w s j+ 1 2 ))\n≤ 1 η · T M ·(M−τ)∑ j=1 4ψ ( w∗s , w s j ) −4ψ ( w∗s , w s j+1 ) + 1 η · 4ψ ( wsj , w s j+ 1 2 ) −4ψ ( wsj+1, w s j+ 1 2 )\n= 1\nη · 4ψ (w∗s , ws1)−4ψ\n( w∗s , w\ns ( TM +1)·τ\n) + 1\nη · T M ·(M−τ)∑ j=1 4ψ ( wsj , w s j+ 1 2 ) −4ψ ( wsj+1, w s j+ 1 2 ) ≤ 1 ηs · 4ψ (w∗s , ws1) + 1 ηs · T M · (M − τ) · (ηs ·G) 2 2\n≤ 1 ηs ·B2 + T M · (M − τ) · ηs ·G 2 2\nWe are after bounding the regret, which in itself is upper bounded by the sum of the regret accumulated by each sub-algorithm, considering iterations in the first τ and lastM−τ per block separately, as mentioned above. Using the convexity of ft for all t, we bound these terms:\nE  TM−1∑ i=0 M ·i+τ∑ t=M ·i+1 ft (wt)− ft ( w∗f ) + T M −1∑ i=0 M ·(i+1)∑ t=M ·i+τ+1 ft (wt)− ft (w∗s)  ≤ E\n TM−1∑ i=0 M ·i+τ∑ t=M ·i+1 〈 wt − w∗f ,∇ft (wt) 〉 + T M −1∑ i=0 M ·(i+1)∑ t=M ·i+τ+1 〈wt − w∗s ,∇ft (wt)〉  = E\n TM ·τ∑ j=1 〈 wfj − w ∗ f ,∇fT1(j) ( wfj )〉 + T M ·(M−τ)∑ j=1 〈 wsj − w∗s ,∇fT2(j)+τ ( wsj )〉\nIn the last equality of the above derivation, we simply replace notations, writing the gradient ∇ft (wt) in notation of T1 and T2. T1 contains all time points in the first τ iterations of each block, and T2 contains all time points in the first M − τ iterations of each block.\nNote that what we have bounded so far is ∑( T M +1)·τ\nj=τ+1 〈w f j−w∗f ,∇fT1(j−τ)(w f j−τ )〉 forwf and\n∑ T M ·(M−τ)\nj=1\n〈 wsj − w∗s ,∇fT2(j) ( wsj )〉 for ws, which are not the terms we need to bound in order to get a regret bound since they use the delayed gradient, and so we need to take a few more steps in order to be able to bound the regret.\nWe begin with wf :\nT M −1∑\ni=0 M ·i+τ∑ t=M ·i+1 〈 wt − w∗f ,∇ft (wt) 〉 = T M ·τ∑ j=1 〈 wfj − w ∗ f ,∇fT1(j) ( wfj )〉\n=\nT M ·τ∑\nj=1\n〈 wfj+τ − w ∗ f ,∇fT1(j) ( wfj )〉 + 〈 wfj − w f j+τ ,∇fT1(j) ( wfj )〉\n= ( TM +1)·τ∑ j=τ+1 〈 wfj − w ∗ f ,∇fT1(j−τ) ( wfj−τ )〉 + 〈 wfj−τ − w f j ,∇fT1(j−τ) ( wfj−τ )〉\n≤ 1 ηf ·B2 + T M · τ ·\nηf ·G2\n2 + ( TM +1)·τ∑ j=τ+1 〈 wfj−τ − w f j ,∇fT1(j−τ) ( wfj−τ )〉\n≤ 1 ηf ·B2 + T M · τ ·\nηf ·G2\n2 + ( TM +1)·τ∑ j=τ+1 ‖wfj−τ − w f j ‖ · ‖∇fT1(j−τ) ( wfj−τ ) ‖\n≤ 1 ηf ·B2 + T M · τ ·\nηf ·G2\n2 + ( TM +1)·τ∑ j=τ+1 τ∑ i=1 ‖wfj−i − w f j−i+1‖ ·G\nThe last term in the above derivation, is the sum of differences between consecutive predictors. This difference, is determined by the mirror map in use, the step size ηf , and the bound over the norm of the gradient used in the update stage of the algorithm, G. This is because every consecutive predictor is received by taking a gradient step from the previous predictor, in the dual space, with a step size ηf , and projecting back to the primal space by use of the bregman divergence with the specific mirror map in use. We denote the bound on this difference by Ψ(ηf ,G), i.e., ∀j, j + 1 : ‖w f j − w f j+1‖ ≤ Ψ(ηf ,G). Continuing our derivation, we have:\n≤ 1 ηf ·B2 + T M · τ ·\nηf ·G2\n2 + ( TM +1)·τ∑ j=τ+1 τ∑ i=1 Ψ(ηf ,G) ·G\n≤ 1 ηf ·B2 + T M · τ ·\nηf ·G2\n2 + T M · τ2 ·Ψ(ηf ,G) ·G\nSince this upper bound does not depend on the permutation,and holds for every sequence, it holds also in expectation, i.e.\nE  TM−1∑ i=0 M ·i+τ∑ t=M ·i+1 ft (wt)− ft ( w∗f ) ≤ 1 ηf ·B2 + T M · τ · ηf ·G2 2 + T M · τ2 ·Ψ(ηf ,G) ·G\nWe now turn to ws\nT M −1∑\ni=0 M ·(i+1)∑ t=M ·i+τ+1 ft (wt)− ft (w∗s)\n≤\nT M −1∑\ni=0 M ·(i+1)∑ t=M ·i+τ+1 〈wt − w∗s ,∇ft (wt)〉\n= T M ·(M−τ)∑ j=1 〈 wsj − w∗s ,∇fT2(j)+τ ( wsj )〉\n= T M ·(M−τ)∑ j=1 〈 wsj − w∗s ,∇fT2(j) ( wsj )〉 + T M ·(M−τ)∑ j=1 〈 wsj − w∗s ,∇fT2(j)+τ ( wsj ) −∇fT2(j) ( wsj )〉\n≤ 1 ηs ·B2 + T M · (M − τ) · ηs ·G 2 2 + T M ·(M−τ)∑ j=1 〈 wsj − w∗s ,∇fT2(j)+τ ( wsj ) −∇fT2(j) ( wsj )〉\nWe now look at the expression 〈 wsj − w∗s ,∇fT2(j)+τ ( wsj ) −∇fT2(j) ( wsj )〉 for any j.\nWe first notice that for any j, wsj only depends on gradients of time points: T2 (1) , T2 (2) , ..., T2 (j − 1). We also notice that given the functions received at these time points, i.e, given fT2(1), fT2(2), ..., fT2(j−1), wsj is no longer a random variable. We have that for all j, T2 (j) and T2 (j) + τ are both time points that are part of the same M -sized block. Suppose we have observed n functions of the block to which T2 (j) and T2 (j) + τ belong. All of these n functions are further in the past than both T2 (j) and T2 (j)+τ , because of the delay of size τ . We haveM−n functions in the block that have not been observed yet, and since we performed a random permutation within each block, all remaining functions in the block have the same expected value. Formally, given wsj , the\nexpected value of the current and delayed gradient are the same, since we have: E[∇fT2(j)+τ ( wsj ) |wsj ] =\n1 M−n · ∑M−n i=1 ∇fT2(j)+i ( wsj ) = E[∇fT2(j) ( wsj ) |wsj ]. As mentioned above, this stems from the random permutation we performed within the block - all M − n remaining functions (that were not observed yet in this block) have an equal (uniform) probability of being in each location, and thus the expected value of the gradients is equal. From the law of total expectation we have that\nE[∇fT2(j)+τ ( wsj ) ] = E[E[∇fT2(j)+τ ( wsj ) |wsj ]] = E[E[∇fT2(j) ( wsj ) |wsj ]] = E[∇fT2(j) ( wsj ) ]\nans thus E[∇fT2(j)+τ ( wsj ) −∇fT2(j) ( wsj ) ] = 0.\nWe get that E[ 〈 wsj − w∗s ,∇fT2(j)+τ ( wsj ) −∇fT2(j) ( wsj )〉 ] = 0 So we have that the upper bound on the expected regret of the time point in which we predict with ws\nis:\nE  TM−1∑ i=0 M ·(i+1)∑ t=M ·i+τ+1 ft (wt)− ft (w∗s)  ≤ 1 ηs ·B2 + T M · (M − τ) · ηs ·G 2 2\nSumming up the regret of the two sub-algorithms, we get:\nE [ T∑ t=1 ft (wt)− ft (w∗) ] ≤ E  TM−1∑ i=0 M ·i+τ∑ t=M ·i+1 ft (wt)− ft ( w∗f ) + T M −1∑ i=0 M ·(i+1)∑ t=M ·i+τ+1 ft (wt)− ft (w∗s)  ≤ B 2\nηf + ηf ·\nTτ M · G\n2 2 + Tτ2 M ·G ·Ψ(ηf ,G) + B2 ηs + ηs · T · (M − τ) M · G 2 2\nwhich gives us the bound. For Ψ(ηf ,G) ≤ c · ηf ·G where c is some constant, choosing the step sizes, ηf , ηs optimally:\nηf = B · √ M G · √ T · τ · ( 1 2 + c · τ ) , ηs = B · √2M G · √ T · (M − τ)\nwe get the bound:\nE [ T∑ t=1 ft (wt)− ft (w∗) ]\n= √ T · τ M ·B ·G · √ 1 2 + c · τ + √ T · τ M ·B ·G · 1√\n1 2 + cτ\n+ √ T · τ M ·B ·G · cτ√\n1 2 + cτ\n+\n√ 2 · T · (M − τ)\nM ·B ·G ≤ c · √ T · τ M ·B ·G · √ 1 2 + c · τ + √ 2 · T · (M − τ) M ·B ·G\n= O (√ T · τ2 M + √ T · (M − τ) M ) = O ( √ T · (√ τ2 M + 1 ))\nA.2 Lower Bound For Algorithms With No Permutation Power\nTheorem 3. For every (possible randomized) algorithm A, there exists a choice of linear, 1-Lipschitz functions over [−1, 1] ⊂ R, with τ a fixed size delay of feedback, such that the expected regret of A after T rounds (with respect to the algorithm’s randomness), is\nE [RA (T )] = E [ T∑ t=1 ft (wt)− T∑ t=1 ft (w ∗) ] = Ω (√ τT ) , where w∗ = argmin w∈W T∑ t=1 ft (w)\nProof. First, we note that in order to show that for every algorithm, there exists a choice of loss functions by an oblivious adversary, such that the expected regret of the algorithm is bounded from below, it is enough to show that there exists a distribution over loss function sequences such that for any algorithm, the expected regret is bounded from below, where now expectation is taken over both the randomness of the algorithm and the randomness of the adversary. This is because if there exists such a distribution over loss function sequences, then for any algorithm, there exists some sequence of loss functions that can lead to a regret at least as high. To put it formally, if we mark E\nalg the expectation over the randomness of the algorithm, and\nE f1,...,fT the expectation over the randomness of the adversary, then:\n∃ a (randomized) adversary s.t. ∀ algorithm A, E f1,...,fT E alg\n[RA (T )] > Ω (√ τT ) →\n∀ algorithm A, ∃f1, ..., fT s.t. E alg\n[RA (T )] > Ω (√ τT )\nThus, we prove the first statement above, that immediately gives us the second statement which gives the lower bound.\nWe consider the setting whereW = [−1, 1], and ∀t ∈ [1, T ] : ft (wt) = αt · wt where αt ∈ {1,−1}. We divide the T rounds to blocks of size τ . αt is chosen in the following way: if αt is the first α in the block, it is randomly picked, i.e, Pr (α = ±1) = 12 . Following this random selection, the next τ − 1 α’s of the block will be identical to the first α in it, so that we now have a block of τ consecutive functions in which α is identical. We wish to lower bound the expected regret of any algorithm in this setting.\nConsider a sequence of predictions by the algorithm w1, w2, ..., wT . Denote by αi,j the j’th α in the i’th block, and similarly for wi,j , fi,j . We denote the entire sequence of α’s by ᾱ(1→T ), and the sequence of α’s until time point j in block i by ᾱ(1→i,j). Notice that wi,j is a function of the α’s that arrive up until time point i · τ + j − τ − 1. We denote these α’s as ᾱ(1→i,j−τ−1).\nThen the expected sum of losses is:\nE [ T∑ t=1 ft (wt) ] = E  Tτ∑ i=1 τ∑ j=1 fi,j (wi,j)  =\nT τ∑ i=1 τ∑ j=1 E [fi,j (wi,j)]\n= T τ∑ i=1 τ∑ j=1 Eᾱ(1→T ) [αi,j · wi,j ]\n= T τ∑ i=1 τ∑ j=1 Eᾱ(1→i,j−τ−1) [ Eᾱ(i,j−τ→T ) [ αi,j · wi,j |ᾱ(1→i,j−τ−1) ]]\n= T τ∑ i=1 τ∑ j=1 Eᾱ(1→i,j−τ−1) [ wi,j · Eᾱ(i,j−τ→T ) [ αi,j |ᾱ(1→i,j−τ−1) ]]\n= T τ∑ i=1 τ∑ j=1 Eᾱ(1→i,j−τ−1) [ wi,j · Eᾱ(i,1→i,j) [ αi,j |ᾱ(1→i,j−τ−1) ]]\n= T τ∑ i=1 τ∑ j=1 Eᾱ(1→i,j−τ−1) [ wi,j · Eαi,1 [αi,1] ]\n= T τ∑ i=1 τ∑ j=1 Eᾱ(1→i,j−τ−1) [ wi,j · ( 1 2 · 1 + 1 2 · (−1) )] = 0\nThe last equality is true because every first α in any block has probability 12 to be either +1 or −1. We now continue to the expected sum of losses for the optimal choice ofw∗ = argminw∈W (∑T t=1 ft (w) ) .\nNote that in this setting, w∗ ∈ {+1,−1} and is with opposite sign to the majority of α’s in the sequence.\nE [ T∑ t=1 ft (w ∗) ] = E  Tτ∑ i=1 τ∑ j=1 fi,j (w ∗)  = E  Tτ∑ i=1 τ∑ j=1 αi,j · w∗ \n= E  Tτ∑ i=1 τ · αi,1 · w∗  = τ · E  Tτ∑ i=1 αi,1 · w∗ \n= −τ · E | Tτ∑ i=1 αi,1|  Using Khintchine inequality we have that:\n−τ · E | Tτ∑ i=1 αi,1 · 1|  ≤ −τ · C · √√√√√  Tτ∑ i=1 12  = −τ · C ·√T τ = −Ω (√ τ · T )\nwhere C is some constant. Thus we get that for a sequence of length T the expected regret is:\nE [ T∑ t=1 ft (wt) ] − E [ T∑ t=1 ft (w ∗) ] = Ω (√ τ · T )\nA.3 Proof of Theorem 2\nProof. First, we note that to show that for every algorithm, there exists a choice of loss functions by an oblivious adversary, such that the expected regret of the algorithm is bounded from below, it is enough to show that there exists a distribution over loss function sequences such that for any algorithm, the expected regret is bounded from below, where now expectation is taken over both the randomness of the algorithm and the randomness of the adversary. This is because if there exists such a distribution over loss function sequences, then for any algorithm, there exists some sequence of loss functions that can lead to a regret at least as high. To put it formally, if we mark E\nalg the expectation over the randomness of the algorithm, and\nE f1,...,fT the expectation over the randomness of the adversary, then:\n∃ a (randomized) adversary s.t. ∀ algorithm A, E f1,...,fT E alg\n[RA (T )] > Ω (√ τT ) →\n∀ algorithm A, ∃f1, ..., fT s.t. E alg\n[RA (T )] > Ω (√ τT )\nThus, we prove the first statement above, that immediately gives us the second statement which is indeed our lower bound.\nWe consider the setting whereW = [−1, 1], and ∀t ∈ [1, T ] : ft (wt) = αt · wt where αt ∈ {1,−1}. We start by constructing our sequence of α’s. We divide the T iterations to blocks of size τ3 . In each block,\nall α’s are identical, and are chosen to be +1 or −1 w.p. 12 . This choice gives us blocks of τ 3 consecutive functions in which α is identical within each block. Let M be a permutation window of size smaller than τ3 . We notice first that since M < τ3 and the sequence of α’s is organized in blocks of size τ 3 , then even after permutation, the time difference between the first and last time we encounter an α is ≤ τ , which means we will not get the feedback from the first time we encountered this α before encountering the next one, and we will not be able to use it for correctly predicting α’s of this (original) block that arrive later. This is the main idea that stands in the basis of this lower bound.\nFormally, consider a sequence of w1, w2, ..., wT chosen by the algorithm. Denote by αi,j the j’th α in the i’th block, and similarly for wi,j , fi,j . We denote the entire sequence of α’s by ᾱ(1→T ), and the sequence of α’s until time point j in block i by ᾱ(1→i,j). For simplicity we will denote βt as the α that was presented at time t, after permutation, i.e. βt := ασ−1(()t). Notice that wi,j is a function of the β’s that arrive up until time point i · ( τ 3 ) + j − τ − 1. We denote these β’s as β̄(1→i,j−τ−1). I.e wi,j = g ( β̄(1→i,j−τ−1) ) where g is some function. Going back to our main idea of the construction, we can put it in this new terminology- since the delay is τ and the permutation window is M < τ3 , for any i, j, the first time we encountered ασ−1(i,j) is less than τ iterations ago, and thus, βi,j is independent of β̄(1→i,j−τ−1), while wi,j is a function of it: wi,j = g ( β̄(1→i,j−τ−1) ) .\nWith this in hand, we look at the sum of losses of the predictions of the algorithm, w1, w2, ..., wT :\nE [ T∑ t=1 ft (wt) ] = E T/τ3∑ i=1 τ 3∑ j=1 fi,j (wi,j)  =\nT/τ3∑ i=1 τ 3∑ j=1 E [fi,j (wi,j)]\n= T/τ3∑ i=1 τ 3∑ j=1 Eβ̄(1→T ) [βi,j · wi,j ]\n= T/τ3∑ i=1 τ 3∑ j=1 Eβ̄(1→i,j−τ−1) [ Eβ̄(i,j−τ→T ) [ βi,j · wi,j |β̄(1→i,j−τ−1) ]]\n= T/τ3∑ i=1 τ 3∑ j=1 Eβ̄(1→i,j−τ−1) [ wi,j · Eβ̄(i,j−τ→T ) [ βi,j |β̄(1→i,j−τ−1) ]]\n= T/τ3∑ i=1 τ 3∑ j=1 Eβ̄(1→i,j−τ−1) [ wi,j · Eβ̄(i,j−τ→T ) [ ασ−1(i,j) ]]\n= T/τ3∑ i=1 τ 3∑ j=1 Eβ̄(1→i,j−τ−1) [ wi,j · ( 1 2 · 1 + 1 2 · (−1) )] = 0\nwhere the last equality stems from the fact that βi,j = ασ−1(i,j) is equal to the expected value of the first time we encountered the α that corresponds to ασ−1(i,j), i.e, the first α that came from the same block of ασ−1(i,j). This expectation is 0 since we choose α = 1 or α = −1 with probability 12 for each block.\nWe now continue to the expected sum of losses for the optimal choice ofw∗ = argminw∈W (∑T t=1 ft (w) ) .\nNote that after permutation, the expected sum of losses of the optimalw remains the same since it is best predictor over the entire sequence, and so for simplicity we look at the sequence of α’s as it is chosen initially. Also, in this setting, w∗ ∈ {+1,−1} and is with opposite sign to the majority of α’s in the sequence.\nE [ T∑ t=1 ft (w ∗) ] = E T/τ3∑ i=1 τ 3∑ j=1 fi,j (w ∗)  = E T/τ3∑ i=1 τ 3∑ j=1 αi,j · w∗ \n= E T/τ3∑ i=1 τ 3 · αi,1 · w∗  = τ 3 · E T/τ3∑ i=1 αi,1 · w∗ \n= −τ 3 · E | T/τ3∑ i=1 αi,1|  Using Khintchine inequality we have that:\n−τ 3 · E | T/τ3∑ i=1 αi,1 · 1|  ≤ −τ 3 · C · √√√√√ T/τ3∑ i=1 12  = −τ 3 · C · √ T τ 3\n= −Ω (√ τ 3 · T ) = −Ω (√ τ · T )\nwhere C is some constant. Thus we get that overall expected regret for any algorithm with permutation power M < τ3 is:\nE [ T∑ t=1 ft (wt) ] − E [ T∑ t=1 ft (w ∗) ] = Ω (√ τ · T ) as in the adversarial case."
    } ],
    "references" : [ {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "Alekh Agarwal", "John C Duchi" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Agarwal and Duchi.,? \\Q2011\\E",
      "shortCiteRegEx" : "Agarwal and Duchi.",
      "year" : 2011
    }, {
      "title" : "The best of both worlds: Stochastic and adversarial bandits",
      "author" : [ "Sébastien Bubeck", "Aleksandrs Slivkins" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Bubeck and Slivkins.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Slivkins.",
      "year" : 2012
    }, {
      "title" : "Online optimization with gradual variations",
      "author" : [ "Chao-Kai Chiang", "Tianbao Yang", "Chia-Jung Lee", "Mehrdad Mahdavi", "Chi-Jen Lu", "Rong Jin", "Shenghuo Zhu" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Chiang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chiang et al\\.",
      "year" : 2012
    }, {
      "title" : "Extracting certainty from uncertainty: Regret bounded by variation in costs",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2010
    }, {
      "title" : "Better algorithms for benign bandits",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2011
    }, {
      "title" : "Multi-armed bandits: Competing with optimal sequences",
      "author" : [ "Zohar S Karnin", "Oren Anava" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Karnin and Anava.,? \\Q2016\\E",
      "shortCiteRegEx" : "Karnin and Anava.",
      "year" : 2016
    }, {
      "title" : "Slow learners are fast",
      "author" : [ "John Langford", "Alexander Smola", "Martin Zinkevich" ],
      "venue" : "arXiv preprint arXiv:0911.0491,",
      "citeRegEx" : "Langford et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2009
    }, {
      "title" : "On-demand, spot, or both: Dynamic resource allocation for executing batch jobs in the cloud",
      "author" : [ "Ishai Menache", "Ohad Shamir", "Navendu Jain" ],
      "venue" : "In 11th International Conference on Autonomic Computing (ICAC",
      "citeRegEx" : "Menache et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Menache et al\\.",
      "year" : 2014
    }, {
      "title" : "On-line learning with delayed label feedback",
      "author" : [ "Chris Mesterharm" ],
      "venue" : "In International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Mesterharm.,? \\Q2005\\E",
      "shortCiteRegEx" : "Mesterharm.",
      "year" : 2005
    }, {
      "title" : "Online learning with adversarial delays",
      "author" : [ "Kent Quanrud", "Daniel Khashabi" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Quanrud and Khashabi.,? \\Q2015\\E",
      "shortCiteRegEx" : "Quanrud and Khashabi.",
      "year" : 2015
    }, {
      "title" : "Online learning with predictable sequences",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In COLT, pages 993–1019,",
      "citeRegEx" : "Rakhlin and Sridharan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rakhlin and Sridharan.",
      "year" : 2013
    }, {
      "title" : "Exploiting easy data in online optimization",
      "author" : [ "Amir Sani", "Gergely Neu", "Alessandro Lazaric" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sani et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sani et al\\.",
      "year" : 2014
    }, {
      "title" : "One practical algorithm for both stochastic and adversarial bandits",
      "author" : [ "Yevgeny Seldin", "Aleksandrs Slivkins" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Seldin and Slivkins.,? \\Q2014\\E",
      "shortCiteRegEx" : "Seldin and Slivkins.",
      "year" : 2014
    }, {
      "title" : "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time",
      "author" : [ "Daniel A Spielman", "Shang-Hua Teng" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Spielman and Teng.,? \\Q2004\\E",
      "shortCiteRegEx" : "Spielman and Teng.",
      "year" : 2004
    }, {
      "title" : "Adaptivity and optimism: An improved exponentiated gradient algorithm",
      "author" : [ "Jacob Steinhardt", "Percy Liang" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Steinhardt and Liang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Steinhardt and Liang.",
      "year" : 2014
    }, {
      "title" : "On delayed prediction of individual sequences",
      "author" : [ "Marcelo J Weinberger", "Erik Ordentlich" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Weinberger and Ordentlich.,? \\Q2002\\E",
      "shortCiteRegEx" : "Weinberger and Ordentlich.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Sani et al. [2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.",
      "startOffset" : 33,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.",
      "startOffset" : 33,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).",
      "startOffset" : 33,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).",
      "startOffset" : 33,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).",
      "startOffset" : 33,
      "endOffset" : 185
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).",
      "startOffset" : 33,
      "endOffset" : 215
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).",
      "startOffset" : 33,
      "endOffset" : 243
    }, {
      "referenceID" : 1,
      "context" : "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]). In this paper, we take a related but different direction: Rather than explicitly excluding highly adversarial loss sequences, we consider how slightly perturbing them can mitigate their worst-case behavior, and lead to improved performance. Conceptually, this resembles smoothed analysis Spielman and Teng [2004], in which one considers the worst-case performance of some algorithm, after performing some perturbation to their input.",
      "startOffset" : 33,
      "endOffset" : 558
    }, {
      "referenceID" : 11,
      "context" : "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( √ τT ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al.",
      "startOffset" : 207,
      "endOffset" : 240
    }, {
      "referenceID" : 6,
      "context" : "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( √ τT ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al.",
      "startOffset" : 241,
      "endOffset" : 259
    }, {
      "referenceID" : 5,
      "context" : "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( √ τT ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al. [2009], Joulani et al.",
      "startOffset" : 260,
      "endOffset" : 283
    }, {
      "referenceID" : 5,
      "context" : "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( √ τT ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al. [2009], Joulani et al. [2013], Quanrud and Khashabi [2015].",
      "startOffset" : 260,
      "endOffset" : 306
    }, {
      "referenceID" : 5,
      "context" : "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( √ τT ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al. [2009], Joulani et al. [2013], Quanrud and Khashabi [2015]. On the other hand, in a stochastic setting where the losses are sampled i.",
      "startOffset" : 260,
      "endOffset" : 335
    }, {
      "referenceID" : 0,
      "context" : "from some distribution, Agarwal and Duchi [2011] show that the attainable regret is much better, on the order of O( √ T + τ).",
      "startOffset" : 24,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "from some distribution, Agarwal and Duchi [2011] show that the attainable regret is much better, on the order of O( √ T + τ). This gap between the worst-case adversarial setting, and the milder i.i.d. setting, hints that this problem is a good fit for our OLLP framework. Thus, in this paper, we focus on online learning with feedback delayed up to τ rounds, in the OLLP framework where the learner is allowed to locally permute the loss functions (up to a distance of M ). First, we devise an algorithm, denoted as Delayed Permuted Mirror Descent, and prove that it achieves an expected regret bound of order O( √ T (τ2/M + 1)) assuming M ≥ τ . As M increases compared to τ , this regret bound interpolates between the standard adversarial √ τT regret, and a milder √ T regret, typical of i.i.d. losses. As its name implies, the algorithm is based on the well-known online mirror descent (OMD) algorithm (see Hazan et al. [2016], Shalev-Shwartz et al.",
      "startOffset" : 24,
      "endOffset" : 930
    }, {
      "referenceID" : 0,
      "context" : "from some distribution, Agarwal and Duchi [2011] show that the attainable regret is much better, on the order of O( √ T + τ). This gap between the worst-case adversarial setting, and the milder i.i.d. setting, hints that this problem is a good fit for our OLLP framework. Thus, in this paper, we focus on online learning with feedback delayed up to τ rounds, in the OLLP framework where the learner is allowed to locally permute the loss functions (up to a distance of M ). First, we devise an algorithm, denoted as Delayed Permuted Mirror Descent, and prove that it achieves an expected regret bound of order O( √ T (τ2/M + 1)) assuming M ≥ τ . As M increases compared to τ , this regret bound interpolates between the standard adversarial √ τT regret, and a milder √ T regret, typical of i.i.d. losses. As its name implies, the algorithm is based on the well-known online mirror descent (OMD) algorithm (see Hazan et al. [2016], Shalev-Shwartz et al. [2012]), and works in the same generality, involving both Euclidean and non-Euclidean geometries.",
      "startOffset" : 24,
      "endOffset" : 960
    }, {
      "referenceID" : 0,
      "context" : "is distinct from another delayed feedback scenario sometimes studied in the literature (Agarwal and Duchi [2011], Langford et al.",
      "startOffset" : 88,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "is distinct from another delayed feedback scenario sometimes studied in the literature (Agarwal and Duchi [2011], Langford et al. [2009]), where rather than receiving ft−τ the learner only receives a (sub)gradient of ft−τ at wt−τ .",
      "startOffset" : 88,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "The proof sketch for the setting where no permutation is allowed was already provided in Langford et al. [2009], and our contribution is in providing a full formal proof.",
      "startOffset" : 89,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "and so we get: ‖w′ − w‖2 ≤ ‖w2 − w‖2 ≤ η ·G We prove a modification of Lemma 2 given in Menache et al. [2014] in order to bound the distance between two consequent predictions when using the negative entropy mirror map: Lemma 4.",
      "startOffset" : 88,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "We propose an Online Learning with Local Permutations (OLLP) setting, in which the learner is allowed to slightly permute the order of the loss functions generated by an adversary. On one hand, this models natural situations where the exact order of the learner’s responses is not crucial, and on the other hand, might allow better learning and regret performance, by mitigating highly adversarial loss sequences. Also, with random permutations, this can be seen as a setting interpolating between adversarial and stochastic losses. In this paper, we consider the applicability of this setting to convex online learning with delayed feedback, in which the feedback on the prediction made in round t arrives with some delay τ . With such delayed feedback, the best possible regret bound is well-known to be O( √ τT ). We prove that by being able to permute losses by a distance of at most M (for M ≥ τ ), the regret can be improved to O( √ T (1 + √ τ2/M)), using a Mirror-Descent based algorithm which can be applied for both Euclidean and non-Euclidean geometries. We also prove a lower bound, showing that for M < τ/3, it is impossible to improve the standard O( √ τT ) regret bound by more than constant factors. Finally, we provide some experiments validating the performance of our algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}