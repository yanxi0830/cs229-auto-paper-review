{
  "name" : "1206.6384.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization",
    "authors" : [ "Haim Avron", "Satyen Kale", "Shiva Prasad Kasiviswanathan" ],
    "emails" : [ "haimav@us.ibm.com", "sckale@us.ibm.com", "spkasivi@us.ibm.com", "vsindhw@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "We consider the following convex optimization problem over matrices:\nmin X∈Rm×n\nf(X) + λ‖X‖∗, (1)\nwhere f(X) is any convex function (not necessarily differentiable), λ > 0 is a regularization parameter, and ‖X‖∗ denotes the nuclear (trace) norm of a matrix X, which is the sum, or equivalently the l1 norm, of the singular values of X. We assume without loss of generality that m ≥ n. This setup generalizes, from vectors to matrices, the widely successful idea of using l1-regularization as a convex proxy for imposing sparsity constraints. Sparsity in the spectrum of a matrix\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ncorresponds to low-rankness, a key modeling idea that is naturally justified in a variety of application contexts, e.g., recommender systems, topic models, and multi-task learning.\nTwo broad lines of research have emerged around the analysis and implementation of matrix estimation algorithms based on nuclear norm regularization. One concerns the theoretical characterization of the conditions under which an unknown low-rank matrix can be exactly recovered by solving a problem of the form (1), or variations thereof, given a set of partially observed entries with respect to which the function f provides a measure of prediction quality. A complementary line of work, which this paper contributes to, concerns the development of algorithmic frameworks to efficiently solve (1) for large-scale problems. Given the definition of the nuclear norm, the Singular Value Decomposition (SVD) tends to unsurprisingly play a critical computational role in the design of nuclear norm solvers, e.g., the Singular Value Thresholding (SVT) (Cai et al., 2010), Soft-Impute (Mazumder et al., 2010), accelerated Proximal Gradient approach (Ji & Ye, 2009) and related efforts, all involve applying a soft-thresholding operator on the singular values of an iterate, which requires repeated calls to an SVD solver. In particular, if the iterates need to pass through a region where the spectrum is dense, these techniques can potentially become prohibitively expensive. An alternative approach is proposed by Jaggi & Sulovský (2010) who map (1) to the problem of optimizing a convex function over the set of positive semi-definite matrices with unit trace, for which the Sparse-SDP solver of Hazan (2008) is invoked. This approach is appealing since each iteration involves the computation of only the largest singular value, and associated left and right singular vectors, of the gradient of f at the current iterate which involves relatively cheap sparse-matrix operations followed by quick rank-one updates. On the other hand, this ap-\nproach produces an -accurate solution with rank possibly as large as Θ( 1 ) making it challenging to hold the factorization of the solution in memory and apply it in practice for generating predictions. Another class of methods, e.g. (Recht & Ré, 2011), works with a lowrank parameterization directly; in general, this leads to a non-convex formulation whose solutions may be highly sensitive to initialization.\nThe contributions of this paper are as follows:\n◦ A new stochastic subgradient descent approach to solving (1). By utilizing a novel subgradient probing technique that generates low-rank steps, combined with enforcing low-rankness in the iterates, our method is able to do cheap iterates and produce a low-rank solution. Furthermore, instead of using sparse SVD as the main computational kernel, our method uses highly scalable dense matrix operation (QR factorization of a tall-and-skinny matrix) as its basic kernel, so it is better poised to take advantage of the computational power of modern platforms. ◦ While the theoretical worst-case running time of our basic algorithm is O ( mn2 −2 ) , an efficient variant\nthat enforces low-rankness shows, empirically, linear complexity in m, n and r, where the rank of the solution is at most r. ◦ We apply our method to matrix completion and show that it compares favorably to state-of-the-art techniques for solving (1) (Jaggi & Sulovský, 2010; Mazumder et al., 2010; Shalev-Shwartz et al., 2011). In addition, our approach is more general than these methods in that it applies, without any major modifications, to the broader class of subdifferentiable elementwise loss functions."
    }, {
      "heading" : "1.1. Preliminaries",
      "text" : "We use i : j to denote the set {i, . . . , j}, and [n] = 1 : n. Vectors are always column vectors and are denoted by boldface letters. We use 0m×n to denote an m × n matrix of all zeros. Given R ⊆ [m] and C ⊆ [n], we denote by XR,C the submatrix of X consisting of rows in R and columns in C. For a matrix X ∈ Rm×n with m ≥ n, σ(X) = (σ1(X), . . . , σn(X)) is the vector of singular values of X, with entries in non-increasing order. The nuclear (trace) norm ‖X‖∗ of a matrix X is the sum of the singular values of X. For a matrix X, let rank(X) denote its rank. When the referenced matrix X is clear, we just use rank to denote rank(X).\nGiven the SVD of a matrix A ∈ Rm×n with m ≥ n, viz. A = UΣV >, the reduced SVD factorization consists of discarding the last m−n columns of U and bottom m−n rows of Σ. By removing singular triplets associated with zero singular values we get the compact\nSVD. Truncating a matrix to rank t (or truncated SVD), denoted by TSVD(A, t) consists of removing the n − t smallest singular values from the reduced SVD. It is well known that TSVD(A, t) is the best rank t approximation to A (spectral and Frobenius norm).\nA QR factorization of a matrix A ∈ Rm×n is decomposition of A into a product A = QR of an unitary matrix Q ∈ Rm×m and an upper triangular matrix R ∈ Rm×n. If m ≥ n, then the reduced QR factorization consists of discarding the last m − n columns of Q and bottom m−n rows of R. The QR factorization can be computed efficiently and in a stable manner using O(mn2) operations.\nWe assume the following properties of the function f , which are quite basic and easily satisfied in most applications:\n1. A subgradient of f at any point X, ∇f(X) ∈ ∂Xf(X), is efficiently computable. 2. We know an upper bound ∆ on ‖Xopt‖F. 3. We know an upper bound G on ‖∇f(X)‖F for all X such that ‖X‖F ≤ ∆."
    }, {
      "heading" : "2. Stochastic Subgradient Descent",
      "text" : "We now describe our stochastic subgradient descent (SSGD) algorithm for solving nuclear norm regularized problems of type (1). Let F (X) = f(X) + λ‖X‖∗. Instead of solving (1), we solve\nmin X∈K\nf(X) + λ‖X‖∗, (2)\nwhere K = {X ∈ Rm×n : ‖X‖F ≤ ∆}. Note that Xopt ∈ K, so this is problem is equivalent to (1). The reason we use this set K is to make sure our iterates are bounded. This is ensured by projecting on K if we ever step outside of K. The projection onto K is defined as follows:\nDefinition 1 (Projection Operator for K). Define ΠK(P ) = argminQ∈K ‖P −Q‖F = min{1, ∆‖P‖F }P.\nLet X = UΣV > be a compact SVD of X. Let Urank, Vrank be U, V truncated to the first rank(X) columns. It is well known that UrankV > rank is a subgradient of ‖X‖∗ (Watson, 1992), so we find that a subgradient of F (X) = f(X) + λ‖X‖∗ is\nG(F (X)) def= ∇f(X) + λ · UrankV >rank ∈ ∂XF (X) .\nThis gives the following upper bound on ‖G(F (X))‖F for any X ∈ K:\n‖G(F (X))‖F ≤ ‖∇f(X)‖F + λ‖UrankV >rank‖F ≤ G+ λ √ rank .\nThe crucial ingredient for a SSGD algorithm is an unbiased estimator for a subgradient. Our technique for computing such an estimator is to probe G(F (X)) by multiplying it by a random matrix.\nDefinition 2 (Probing Matrix). A random n×k matrix Y is a probing matrix if E[Y Y >] = In×n where In×n is the n × n identity matrix and the expectation is over the choice of Y .\nHere, k n is a parameter that is adjustable in our algorithm. The following lemma gives several families of distributions that generate probing matrices efficiently. (All missing proofs appear in the full version of the paper (Avron et al., 2012).) Lemma 2.1. Let Y = Z/ √ k where Z is a random matrix drawn any one of the following distributions:\n1. Independent entries taking values +1 and −1 with equal probability 1/2.\n2. Independent and identically distributed standard normal entries.\n3. Each column of Z is drawn uniformly at random and independent of each other from { √ ne1, . . . , √ nen} (scaled identity vectors)."
    }, {
      "heading" : "Then Y is a probing matrix.",
      "text" : "By linearity of expectation, for any matrix A we have E[AY Y >] = AE[Y Y >] = A. Thus, in our algorithms we use G(F (X))Y Y > as an unbiased estimator of G(F (X)). Now, using an unbiased estimator instead of an exact subgradient (that is, using SSGD instead of subgradient descent) is beneficial only if there is some computational advantage in doing so. Our probing technique has two potential advantages over the use of an exact subgradient. First, for any matrix A the matrix AY Y > has rank at most k. So our estimator is, in fact, a low-rank unbiased estimator. We will utilize this low-rankness later on. Second, often it is more efficient to compute the product G(F (X))Y than to actually compute G(F (X)). For example, if Y is composed of scaled identity vectors (case 3 in Lemma 2.1), then G(F (X))Y is a matrix composed of k columns of G(F (X)), so we need to compute only a small portion of G(F (X)). Henceforth, we use the scaled identity vectors as the default probing matrix."
    }, {
      "heading" : "2.1. Basic SSGD Algorithm",
      "text" : "Using the above ingredients, we now describe a basic SSGD algorithm for solving (2). We use g(t) = G(F (X(t)))Y Y > as an unbiased estimator of G(F (X(t))). Algorithm Basic-SSGD describes the complete procedure. Basic-SSGD is not efficient in terms of running time and memory requirements. Subsequent subsections give more efficient variants.\nAlgorithm 1 Basic-SSGD\nInput: f , λ, T , step sizes η(1), . . . , η(T−1), and k\nInitialize X(0) = 0m×n for t = 0 to T − 1 do\nGenerate an n× k probing matrix Y g(t) ←− G(F (X(t)))Y Y > X(t+1) ←− ΠK(X(t) − η(t)g(t))\nend for Return X(`) = argminX(t),0≤t≤TF (X (t))\nWe now analyze Algorithm Basic-SSGD.\nLemma 2.2. Let Y ∈ Rn×k. Then for any matrix A ∈ Rm×n,\nE[‖AY Y >‖2F] = trace(E[Y Y >Y Y >]A>A).\nIn particular, if Y is a probing matrix from Definition 2, then E[‖AY Y >‖2F] = nk ‖A‖ 2 F.\nSubstituting A = G(F (X(t))) in Lemma 2.2 gives\nE[‖g(t)‖2F] ≤ n\nk ‖G(F (X(t)))‖2F\n≤ n k\n( G+ λ √ rank(X(t))) )2 .\nFinally, using this in a standard bound on the convergence rate of the SSGD algorithm , we get the following theorem about the convergence rate of the BasicSSGD algorithm.\nTheorem 2.3. Suppose r is an upper bound on rank(X(t)) for all t. Then if we set η(t) = β √ k∆√\nn(G+λ √ r) √ T , the solution X(l) of Algorithm Basic-\nSSGD satisfies\nE[X(l)]− F (Xopt) ≤ 4 √ n (G+ λ √ r)∆√\nkT max\n{ β, 1\nβ\n} .\nThus Basic-SSGD converges to within · ‖Xopt‖F of the optimal goal in O(n/k 2) iterations. If we fix then convergence is in O(n/k) iterations."
    }, {
      "heading" : "2.2. Decreasing the Iteration Cost of",
      "text" : "Basic-SSGD for Low Rank Iterates\nIn each iteration of Basic-SSGD, we have to compute g(t). The straightforward way of computing g(t) requires computing the SVD of X(t), which has O(mn2) time complexity and is computationally prohibitive for large matrices. Fortunately, when the rank of X(t) is small we can do much better. Let r(t) denote the rank\nof X(t). Assume that we have a compact SVD decomposition of X(t): X(t) = U (t)Σ(t)(V (t))>. Since X(0) = 0m×n, we certainly have such a decomposition for X(0). We now show how to generate the SVD of X(t+1), without explicitly forming X(t+1). To do so, we first define a class of loss functions.\nDefinition 3 (Sum Loss Function). A function f : Rm×n → R is a sum loss function if for X ∈ Rm×n, f(X) = ∑ ij fij(Xij) where the fijs are convex functions. We say that f is efficiently computable if fij(x) and a subgradient f ′ij(x) of fij at x can be computed in O(1) time for every i, j, and x.\nNote that the above definition captures many commonly used functions, including squared error, absolute error, hinge loss function (with or without smoothing), and can also apply to arbitrary domainspecific subdifferentiable elementwise loss functions.\nLemma 2.4. Suppose that f is an efficiently computable sum loss function. Suppose that the probing matrix Y is composed of scaled identity vectors. Given the SVD of X(t), there is an algorithm to compute the SVD of X(t+1) = ΠK(X\n(t) − η(t)g(t)) (without explicitly computing X(t+1) itself) in O(m(r(t) + k)2) time, where r(t) is the rank of X(t).\nProof. We first write a formula for g(t):\ng(t) = G(X(t))Y Y >\n= [∇f(U (t)Σ(t)(V (t))>)Y + U (t)(V (t))>Y ]︸ ︷︷ ︸ S(t) Y >.\nBy first computing (V (t))>Y and then multiplying by U (t) we can compute U (t)(V (t))>Y in O(mkr(t)) time.\nSince Y is composed of scaled identity vectors, ∇f(X(t))Y is composed of scaled columns of ∇f(X(t)). The expansion f(X) = ∑ ij fij(Xij) implies that (∇f(X))ij = f ′ij(Xij), so to find ∇f(X(t))Y we simply have to compute the values ∇f(X(t)) at the corresponding columns. The entries of a single column of X(t) can be computed from the decomposition X(t) = U (t)Σ(t)(V (t)))> in O(mr(t)) time, so k columns can be computed in O(mkr(t)). Therefore, S(t) can be computed in O(mkr(t)) time.\nSince g(t) = S(t)Y > we have X(t+1) = ΠK(X (t) − η(t)S(t)Y >). Let us now define\nÛ (t+1) def = [U (t)Σ(t) S(t)] and V̂ (t+1) def = [V (t) − η(t)Y ]\nand we get that X(t+1) = ΠK(Û (t+1)(V̂ (t+1))>). This is already a low-rank representation of the matrix. We now show how to turn the low-rank representation\nÛ (t+1)(V̂ (t+1))> to a compact SVD. First, we compute a reduced QR decomposition of Û (t+1) = QURU and V̂ (t+1) = QVRV . We then compute RUR > V and find an SVD decomposition RUR > V = M Σ̄\n(t+1)N>. Then we compute Ū (t+1) = QUM and V̄\n(t+1) = QVN and we have\nX(t+1) = ΠK(Ū (t+1)Σ̄(t+1)(V̄ (t+1))>) .\nThe decomposition of the inner matrix is, in fact, a reduced SVD decomposition since QU , M , QV and N are orthonormal, and Σ̄(t+1) is diagonal with positive values. Since it is a SVD we can efficiently do the K projection and remove zero (or numerically zero) singular values to obtain the compact SVD decomposition X(t+1) = U (t+1)Σ(t+1)(V (t+1)) of the next iterate.\nIt is easy to verify that the dominant operation, in terms of running time, is the computation of the QR factorization of Û (t+1). Since this is a factorization of an m × (r(t) + k) matrix, the overall cost is O(m(r(t) + k)2), which is much better than O(mn2) if r(t) + k n. For a pseudo-code description, see full version of the paper (Avron et al., 2012).\nUnfortunately, in Basic-SSGD it is quite probable that rank of X(t+1) is r(t) +k, and therefore, after n/k iterations the iterates could become full rank, and from there on updates will take O(mn2) time per iteration (same iteration cost as the trivial implementation of Basic-SSGD). We will see how to avoid this situation in the next subsection.\nScalability. The running time of each iteration is dominated by computing QR factorizations of talland-skinny dense matrices. We could have used SVD computations instead of QR computations. Since QR and SVD have equivalent asymptotic running this would not have changed the time complexity of the algorithm. However, we chose to use QR factorizations instead since it is a simpler operation that exhibits better running time in practice. The use of QR factorization also makes the update operation highly scalable on modern parallel architectures. By using a highly tuned package like ScaLAPACK (Choi et al., 1992) good speedups should be attainable with little effort. Recent research on QR factorization has shown how to implement it efficiently on communication-bound massive parallel machines (Demmel et al., 2008), MapReduce clusters (Constantine & Gleich, 2011), and GPUs (Anderson et al., 2011). It is worth noting that our algorithm avoids the computation of singular values on large sparse matrices, which require more sophisticated communication-avoiding methods (Hoemmen, 2010), which do not scale as well as dense linear algebra operations."
    }, {
      "heading" : "2.3. Enforcing Low-Rank Iterates",
      "text" : "The update algorithm of Lemma 2.4 could be used in Algorithm Basic-SSGD to go from the SVD of X(t) to SVD of X(t+1) = ΠK(X (t) − η(t)g(t)). The discussion in the previous section shows that if the iterates (X(t)s) in Algorithm Basic-SSGD are low rank then the iterations are fast. This suggests the idea of explicitly truncating the least singular values of the iterates to ensure that the iterates always remain low rank. In this section, we formalize this idea.\nLet Mr = Mm×nr denote the set of m-by-n matrices of rank at most r. Suppose we assume that rank(Xopt) ≤ r, i.e., Xopt ∈ Mr, for some parameter r. Then minimizing F (X) over K ∩ Mr yields the same optimum solution. We look at the problem of solving (2) with the additional constraint that X ∈ Mr. However, the set Mr is non-convex. Furthermore, rank constraints, like X ∈ Mr, typically result in NP-hard problems (see (Natarajan, 1995)).\nOur strategy is to again use a projected subgradient method. That is, in each iteration we start with a regular stochastic subgradient step. This step takes us out of Mr. The following step is to project back to Mr. It is well known that for any matrix X the best rank r approximation to X (measured in terms of Frobenius norm) can be computed by truncating the SVD to the top r singular values. Since our algorithm keeps a SVD representation of the iterates X(t), we can compute the projection onMr efficiently. Despite not having a theoretical guarantee because of the non convexity ofMr, our experiments, which we report in Section 4, suggest that O(n/k) iterations are still sufficient for this explicit rank-constrained nuclear norm regularized problem. Note that the projection operator on K does not change the rank of the matrix since it is a simple scaling. For a pseudo-code description, see full version of the paper (Avron et al., 2012).\nRole of r. Since nuclear norm regularization is typically used as a proxy for rank constraints, one might wonder why we impose an explicit rank constraint. Primarily, we use the rank constraint only for computational efficiency reasons though we consistently observed statistical benefits from additionally keeping the nuclear norm regularizer (i.e., λ > 0). As long as r ≥ rank(Xopt) the solution to the problem does not change, and the rank constraint is passive. So we need only an upper bound on the rank of the optimal solution. We can even select r = n, which will remove the rank constraint completely. However, the running time of the algorithm does depend on r, so it is best to set r as close as possible to rank(Xopt). Since we use nuclear norm regularization we expect the rank of\nAlgorithm 2 SSGD-Matrix-Completion\nInput: Z ∈ Rm×n and parameters r, s, δ, and ν\n[U (0),Σ(0), V (0)] = TSVD(Z, r) α← 1‖Z‖2F , β ← δ·f(X(0))\n(‖Z|2F·‖X(0)‖∗) ∆← αβ−1‖Z‖F, η ← ν‖Z‖2F for t = 0 to s ⌈ n r ⌉ do\nCreate C = c1, . . . , cr, ∀i ci drawn i.i.d. from [n] Y ← [ec1 , . . . , ecr ] P (t) ← βU (t)(V (t)C,1:r)>\nS(t) ← √\nn k (2α(U (t)Σ(t)(V (t) C,1:r) >−Z1:m,C)+P (t)) Û (t+1) ← [U (t)Σ(t) S(t)] V̂ (t+1) ← [V (t) − η(t)Y ] Factorize: Û (t+1) = QURU , V̂\n(t+1) = QVRV T ← RUR>V SVD computation: T = M Σ̄(t+1)N> Ū (t+1) ← QUM , V̄ (t+1) ← QVN U (t+1) ← Ū (t+1)1:m,1:r, V (t+1) ← V̄ (t+1) 1:n,1:r Σ(t+1) ← Σ̄(t+1)1:r,1:r if ‖Σ(t+1)‖F > ∆ then\nΣ(t+1) ← Σ(t+1)∆/‖Σ(t+1)‖F end if\nend for\nXopt to be small.\nOutput. The output of our algorithm matrix in compact SVD form. This is a much more succinct representation that requires only O(mr) memory words, instead of O(mn). A specific entry in the matrix can be computed in O(r) time, and the entire matrix can be computed in O(mnr) time.\nChoice of k. Since we are explicitly enforcing the rank of the iterates to be smaller than r we have r(t) ≤ r. This implies that each iteration is computed in O(m(r + k)2) time. There is a relation between k and the number of iterations: as k grows our gradients improve so we expect to converge in less iterations. In all our experiments we set k = r to avoid an additional tunable parameter"
    }, {
      "heading" : "3. Application: Matrix Completion",
      "text" : "In the low rank matrix completion problem, we are given a set of indices Ω ∈ [m]× [n] and associated values (Zij)(i,j)∈Ω and we are required to complete the matrix with the lowest possible rank. Minimizing the rank is hard, so a popular approach for solving the matrix completion is as follows. Let PΩ be the projection onto the index set Ω. That is, (PΩ(X))ij = Xij if (i, j) ∈ Ω and 0 otherwise. Let Z be the matrix\ncontaining the known values at their correct position and 0 in all other positions. The problem to be solved is then\nmin X∈Rm×n\nα‖PΩ(X)− Z‖2F + β‖X‖∗ . (3)\nFor reasons that will be apparent later we chose to write the problem with two parameters (α and β) instead of a single parameter λ. Other variants are possible, like using `1-loss instead of `2-loss, or any subdifferentiable loss function for that matter, but we will focus on (3).\nWe now show how are approach can be used to solve (3). Set f(X) = α‖PΩ(X) − Z‖2F. We have ∇f(X) = 2α(PΩ(X) − Z). Note that f(X) is an efficiently computable sum loss function.\nWe also need to bound the Frobenius norm of Xopt in order to define the convex set KF. We observe that\n‖Xopt‖F ≤ ‖Xopt‖∗ ≤ β−1F (Xopt) ≤ β−1F (0m×n) = αβ−1‖Z‖2F.\nFinally, we need to set the step sizes η(t). In our experiments, we found that using a fixed step size η gave the best results. To make the step size scale free we set η = ν‖Z‖2F where ν is a parameter."
    }, {
      "heading" : "Heuristics for warm-starting the algorithm and",
      "text" : "setting the parameters. Our algorithm can start from any matrix with rank up to r as long as we have a compact SVD of that matrix. We warm-start our algorithm by a rank r truncated SVD of Z (i.e., X(0) = TSVD(Z, r)). The initial SVD is also useful for setting α and β in a scale free manner. The nuclear norm serves as a regularizer, so we expect β‖Xopt‖∗ to be some magnitude smaller than αf(Xopt). We do not know the values of ‖Xopt‖∗ and f(Xopt), so instead we use the values of ‖X(0)‖∗ and f(X(0)). First, we set α = 1/‖Z‖2F, which will make the value of αf(X) range between 0 and 1. We then find β such that β‖X(0)‖∗ = δ · αf(X(0)), where δ is a new parameter. That is we set β = δ · f(X(0))/(‖Z‖2F · ‖X(0)‖∗).\nThe complete pseudo-code listed in Algorithm SSGDMatrix-Completion. Overall, Algorithm SSGDMatrix-Completion has four parameters: (i) bound on the rank of the solution (r), (ii) number of superiterations (s; we call every ⌈ n r ⌉ iterations a superiteration), (iii) normalized regularization parameter (δ), and (iv) normalized step size (ν)."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "We ran our matrix completion algorithm (Algorithm SSGD-Matrix-Completion) on two stan-\ndard collaborative filtering datasets. The first dataset (MovieLens 10M, partition-rb) has about 107 ratings of 69878 users on 10677 movies. The second dataset (Netflix) has about 108 ratings of 480189 users on 17770 movies. The ratings are on an integer scale from 1 to 5. We partitioned each dataset into training and test sets as done by Jaggi & Sulovský (2010). We preprocessed the datasets as follows. For every row and column of training matrix Z we computed the mean. Let µi denote the mean rating of row i, and µ̂j denote the mean rating of column j. We subtract from each training and test rating, Xij , the value (µi+ µ̂j)/2. In the graphs we refer to our algorithm as “SSGD”.\nJSH and Soft-Impute We compared our results to two other matrix completion algorithms which are also based on solving nuclear norm regularized problems. The first algorithm (which we refer to as “JSH”), suggested by the works of Hazan (2008) and Jaggi & Sulovský (2010), is based on an extension of the FrankWolfe (Frank & Wolfe, 1956) algorithm for optimizing a function over the bounded positive semidefinite cone (see also (Clarkson, 2008)). We used a simple MATLAB implementation of the Algorithm 2 from (Jaggi & Sulovský, 2010). The function ApproxEV was implemented by calling MATLAB’s svds function with default parameters (fixed tolerance). The regularization parameter (t) was set according to the best values reported (Jaggi & Sulovský, 2010), i.e., t = 48333 for MovieLens and t = 99592 for Netflix. The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as “Soft-Impute”. Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)). We used the path-following strategy suggested by the code, i.e., we set a path of values for the regularization parameter (λ) where the results on a value are used as a warmstart for the next value. The results where examined (RMSE measured) at the different points along the regularization path, and the running time at any point λi is the sum of time needed to run the algorithm at λi and the running time of λi−1 (because of the warm start). The path used for MovieLens was (λ0/2, λ0/4, λ0/8, λ0/10), and the path used for Netflix was (λ0/250, λ0/300), where λ0 is the spectral norm of the input matrix Z. Note that both Jaggi & Sulovský (2010) and Mazumder et al. (2010) apply additional heuristics and/or post-processing to their basic algorithm. Additionally, Mazumder et al. (2010) measure only time spent on SVD computations.\nThese differences in the experimental setup, together with our effort to bring all algorithms under exactly the same experimental protocol, might explain\nthe discrepancy between the results we show here and the results reported in (Jaggi & Sulovský, 2010) and (Mazumder et al., 2010). Nonetheless, for completeness, we also report their published RMSEs. When all algorithms are compared in the same setting, and even relative to best reported RMSEs, we find that our SSGD approach compares favorably to the Frank-Wolfe and singular value thresholding based approaches for solving matrix completion problems.\nExperimental Setup. We used a 64-bit version of MATLAB 7.8. The experiments were done on a two quad-core Intel E5410 computer running at 2.33 GHz, with 32GB DDR2 800 MHz RAM, running Linux 2.6. None of the codes explicitly uses the eight cores, although some operations (like dense QR factorization) are automatically parallelized by MATLAB. The measured running times are wall-clock times and were measured using the ftime Linux system call."
    }, {
      "heading" : "Setting the parameters and sensitivity to",
      "text" : "them. In Figure 1, we examine SSGD-MatrixCompletion’s sensitivity to the value of the parameters r, δ, and ν. The best choice of rank (r) is 11. However, increasing the rank from 11 to 15 only results in a 0.75% increase in RMSE, and decreasing the rank to 7 only causes a 0.38% increase in RMSE. The best choice of δ is around 0.01. Overestimating δ affects the RMSE more adversely than underestimating. For example, setting δ to 0.1 results in a 5% increase in the RMSE, whereas setting δ to 0.001 only leads to a 0.48% increase. The best choice of step size (ν) is around 0.009, and the RMSE increases quite smoothly as we go away from this value. We set δ = 0.015, and ν = 0.005 based on preliminary observations on MovieLens 10M without attempting any exhaustive tuning. For Netflix, we simply used the same parameters without any additional experimentation.\nResults on the MovieLens 10M Dataset. In Figure 2(a), we plot the RMSE on the test set as a function of time. SSGD-Matrix-Completion decreases the error much faster than the other two algorithms, and maintains a better error throughout. SSGDMatrix-Completion achieved a RMSE of 0.8721 after 1 hour. After running for 180 super-iterations, which took 11.47 hours, the RMSE of SSGD-MatrixCompletion was 0.8555. Compared to this, the JSH obtained a RMSE 0.8640 after 11.66 hours and SoftImpute obtained a RMSE of 0.8605 after 12.19 hours. Jaggi & Sulovský (2010) report 0.8573 as the best RMSE obtained using their implementation.\nIn Figure 2(b), we plot the RMSE as a function of rank of the iterates. Every iteration of JSH involves addition of a rank-one matrix, so the rank of iterates\nsoon becomes large. For both JSH and Soft-Impute, we needed to go to a much larger rank to obtain a RMSE comparable to that of a rank-11 solution obtained by SSGD-Matrix-Completion. We stress that at a comparable RMSE a low rank solution is more useful than an high rank solution since it can be held in memory and can be queried much faster to produce a prediction.\nResults on the Netflix Dataset. Here too, SSGDMatrix-Completion outperforms JSH and SoftImpute. After running for 25 super-iterations, which took 23.97 hours, SSGD-Matrix-Completion obtained a RMSE of 0.9516. After 24.08 hours, JSH obtained a RMSE of 0.9583 while Soft-Impute achieved its best RMSE of 0.9603 after 8.55 hours (after 24 hours Soft-Impute’s RMSE was worse than this number). After 180 super-iterations, SSGDMatrix-Completion obtained a RMSE of 0.9411. Jaggi & Sulovský (2010) report 0.9478 as the best RMSE obtained using their optimized implementation. Mazumder et al. (2010) report 0.9497 as the best RMSE obtained in their experiments.\nComparison with GECO. The GECO algorithm proposed in Shalev-Shwartz et al. (2011) is not a solver for nuclear norm regularized problems, but uses a greedy method, with optimality guarantees, to optimize f under explicit low-rank constraints. We used the implementation provided to us by the authors with recommended parameters. On MovieLens 10M, GECO returned the best RMSE of 0.8771 at a rank of 17. Our approach yields a better RMSE at a lower rank though GECO’s results reaffirm the effectiveness of maintaining explicit low-rank constraints. On the other hand, the running time of the GECO implementation was found to be significantly worse than other methods and we considered it impractical to run it on the Netflix dataset.\nComparison to Non-Convex Methods. Methods that work directly with a low-rank parameterization are significantly different in flavor from convex methods since the local minima they find may or may not be optimal for a given choice of rank parameter. In theory, they may be sensitive to initialization and require restarts as reported by Recht & Ré (2011). In practice, on the two datasets that we tested after our draft was written, we found them to be quite robust and efficient, e.g., on MovieLens 10M they get to an RMSE of 0.857 in 45 mins and on Netflix they attain RMSE of 0.9399 in a couple of hours. This is not surprising since these methods, and their variations, dominated the Netflix contest. We advocate that efforts to improve the performance of convex nuclear norm meth-\nods, with an eye towards making them efficient and practical, should include such comparisons."
    }, {
      "heading" : "Anderson, Michael, Ballard, Grey, Demmel, James, and",
      "text" : "Keutzer, Kurt. Communication-Avoiding QR Decomposition for GPUs. In IPDPS, 2011."
    }, {
      "heading" : "Avron, Haim, Kale, Satyen, Kasiviswanathan, Shiva, and",
      "text" : "Sindhwani, Vikas. Efficient and practical stochastic subgradient descent for nuclear norm regularization (full version). Technical report, IBM T. J. Watson Research Center, Yorktown Heights, NY, 2012."
    }, {
      "heading" : "Cai, J.F., Candes, E.J., and Zhen, Z. A Singular Value",
      "text" : "Thresholding Algorithm for Matrix Completion. SIAM J. Optimization, 20(4), 2010."
    }, {
      "heading" : "Choi, J., Dongarra, J.J., Pozo, R., and Walker, D.W.",
      "text" : "ScaLAPACK: a Scalable Linear Algebra Library for Distributed Memory Concurrent Computers. In Frontiers of Massively Parallel Computation, 1992.\nClarkson, Kenneth L. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. In SODA, pp. 922–931, 2008."
    }, {
      "heading" : "Constantine, Paul G. and Gleich, David F. Tall and",
      "text" : "skinny QR factorizations in MapReduce architectures. In MapReduce, pp. 43–50, 2011."
    }, {
      "heading" : "Demmel, James, Grigori, Laura, Hoemmen, Mark,",
      "text" : "and Langou, Julien. Communication-Otimal Parallel and Sequential QR and LU Factorizations. Arxiv, abs/0808.2664, 2008."
    }, {
      "heading" : "Frank, Marguerite and Wolfe, Philip. An Algorithm",
      "text" : "for Quadratic Programming. Naval Research Logistics Quarterly, 3(1-2):95–110, 1956.\nHazan, Elad. Sparse Approximate Solutions to Semidefinite Programs. In LATIN, 2008.\nHoemmen, Mark. Communication-Avoiding Krylov Subspace Methods. PhD thesis, University of California, Berkeley, 2010."
    }, {
      "heading" : "Jaggi, Martin and Sulovský, Marek. A Simple Algorithm",
      "text" : "for Nuclear Norm Regularized Problems. In ICML, pp. 471–478, 2010."
    }, {
      "heading" : "Ji, S. and Ye, J. An Accelerated Gradient Method for Trace",
      "text" : "Norm Minimization. In ICML, 2009."
    }, {
      "heading" : "Mazumder, Rahul, Hastie, Trevor, and Tibshirani, Robert.",
      "text" : "Spectral Regularization Algorithms for Learning Large Incomplete Matrices. J. Mach. Learn. Res., 99:2287– 2322, 2010."
    }, {
      "heading" : "Natarajan, B K. Sparse Approximate Solutions to Linear",
      "text" : "Systems. SICOMP, 24(2):227–234, 1995."
    }, {
      "heading" : "Recht, Benjamin and Ré, Christopher. Parallel Stochastic Gradient Algorithms for Large-Scale Matrix Completion. Optimization Online, 2011.",
      "text" : ""
    }, {
      "heading" : "Shalev-Shwartz, Shai, Gonen, Alon, and Shamir, Ohad.",
      "text" : "Large-Scale Convex Minimization with a Low-Rank Constraint. In ICML, pp. 329–336, 2011."
    }, {
      "heading" : "Watson, G.A. Characterization of the subdifferential of",
      "text" : "some matrix norms. Linear Algebra and its Applications, 170(0):33 – 45, 1992."
    } ],
    "references" : [ {
      "title" : "Communication-Avoiding QR Decomposition for GPUs",
      "author" : [ "Anderson", "Michael", "Ballard", "Grey", "Demmel", "James", "Keutzer", "Kurt" ],
      "venue" : "In IPDPS,",
      "citeRegEx" : "Anderson et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient and practical stochastic subgradient descent for nuclear norm regularization (full version)",
      "author" : [ "Avron", "Haim", "Kale", "Satyen", "Kasiviswanathan", "Shiva", "Sindhwani", "Vikas" ],
      "venue" : "Technical report, IBM T. J. Watson Research",
      "citeRegEx" : "Avron et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Avron et al\\.",
      "year" : 2012
    }, {
      "title" : "A Singular Value Thresholding Algorithm for Matrix Completion",
      "author" : [ "J.F. Cai", "E.J. Candes", "Z. Zhen" ],
      "venue" : "SIAM J. Optimization,",
      "citeRegEx" : "Cai et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2010
    }, {
      "title" : "ScaLAPACK: a Scalable Linear Algebra Library for Distributed Memory Concurrent Computers",
      "author" : [ "J. Choi", "J.J. Dongarra", "R. Pozo", "D.W. Walker" ],
      "venue" : "In Frontiers of Massively Parallel Computation,",
      "citeRegEx" : "Choi et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 1992
    }, {
      "title" : "Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm",
      "author" : [ "Clarkson", "Kenneth L" ],
      "venue" : "In SODA, pp",
      "citeRegEx" : "Clarkson and L.,? \\Q2008\\E",
      "shortCiteRegEx" : "Clarkson and L.",
      "year" : 2008
    }, {
      "title" : "Tall and skinny QR factorizations in MapReduce architectures",
      "author" : [ "Constantine", "Paul G", "Gleich", "David F" ],
      "venue" : "In MapReduce,",
      "citeRegEx" : "Constantine et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Constantine et al\\.",
      "year" : 2011
    }, {
      "title" : "Communication-Otimal Parallel and Sequential QR and LU Factorizations",
      "author" : [ "Demmel", "James", "Grigori", "Laura", "Hoemmen", "Mark", "Langou", "Julien" ],
      "venue" : "Arxiv, abs/0808.2664,",
      "citeRegEx" : "Demmel et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Demmel et al\\.",
      "year" : 2008
    }, {
      "title" : "An Algorithm for Quadratic Programming",
      "author" : [ "Frank", "Marguerite", "Wolfe", "Philip" ],
      "venue" : "Naval Research Logistics Quarterly,",
      "citeRegEx" : "Frank et al\\.,? \\Q1956\\E",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 1956
    }, {
      "title" : "Sparse Approximate Solutions to Semidefinite Programs",
      "author" : [ "Hazan", "Elad" ],
      "venue" : "In LATIN,",
      "citeRegEx" : "Hazan and Elad.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hazan and Elad.",
      "year" : 2008
    }, {
      "title" : "Communication-Avoiding Krylov Subspace Methods",
      "author" : [ "Hoemmen", "Mark" ],
      "venue" : "PhD thesis, University of California, Berkeley,",
      "citeRegEx" : "Hoemmen and Mark.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hoemmen and Mark.",
      "year" : 2010
    }, {
      "title" : "A Simple Algorithm for Nuclear Norm Regularized Problems",
      "author" : [ "Jaggi", "Martin", "Sulovský", "Marek" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Jaggi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaggi et al\\.",
      "year" : 2010
    }, {
      "title" : "An Accelerated Gradient Method for Trace Norm Minimization",
      "author" : [ "S. Ji", "J. Ye" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ji and Ye,? \\Q2009\\E",
      "shortCiteRegEx" : "Ji and Ye",
      "year" : 2009
    }, {
      "title" : "Spectral Regularization Algorithms for Learning Large Incomplete Matrices",
      "author" : [ "Mazumder", "Rahul", "Hastie", "Trevor", "Tibshirani", "Robert" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Mazumder et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mazumder et al\\.",
      "year" : 2010
    }, {
      "title" : "Sparse Approximate Solutions to Linear Systems",
      "author" : [ "Natarajan", "B K" ],
      "venue" : "SICOMP,",
      "citeRegEx" : "Natarajan and K.,? \\Q1995\\E",
      "shortCiteRegEx" : "Natarajan and K.",
      "year" : 1995
    }, {
      "title" : "Parallel Stochastic Gradient Algorithms for Large-Scale Matrix Completion",
      "author" : [ "Recht", "Benjamin", "Ré", "Christopher" ],
      "venue" : "Optimization Online,",
      "citeRegEx" : "Recht et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2011
    }, {
      "title" : "Large-Scale Convex Minimization with a Low-Rank Constraint",
      "author" : [ "Shalev-Shwartz", "Shai", "Gonen", "Alon", "Shamir", "Ohad" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Characterization of the subdifferential of some matrix norms",
      "author" : [ "G.A. Watson" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "Watson,? \\Q1992\\E",
      "shortCiteRegEx" : "Watson",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : ", the Singular Value Thresholding (SVT) (Cai et al., 2010), Soft-Impute (Mazumder et al.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : ", 2010), Soft-Impute (Mazumder et al., 2010), accelerated Proximal Gradient approach (Ji & Ye, 2009) and related efforts, all involve applying a soft-thresholding operator on the singular values of an iterate, which requires repeated calls to an SVD solver.",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : ", the Singular Value Thresholding (SVT) (Cai et al., 2010), Soft-Impute (Mazumder et al., 2010), accelerated Proximal Gradient approach (Ji & Ye, 2009) and related efforts, all involve applying a soft-thresholding operator on the singular values of an iterate, which requires repeated calls to an SVD solver. In particular, if the iterates need to pass through a region where the spectrum is dense, these techniques can potentially become prohibitively expensive. An alternative approach is proposed by Jaggi & Sulovský (2010) who map (1) to the problem of optimizing a convex function over the set of positive semi-definite matrices with unit trace, for which the Sparse-SDP solver of Hazan (2008) is invoked.",
      "startOffset" : 41,
      "endOffset" : 527
    }, {
      "referenceID" : 2,
      "context" : ", the Singular Value Thresholding (SVT) (Cai et al., 2010), Soft-Impute (Mazumder et al., 2010), accelerated Proximal Gradient approach (Ji & Ye, 2009) and related efforts, all involve applying a soft-thresholding operator on the singular values of an iterate, which requires repeated calls to an SVD solver. In particular, if the iterates need to pass through a region where the spectrum is dense, these techniques can potentially become prohibitively expensive. An alternative approach is proposed by Jaggi & Sulovský (2010) who map (1) to the problem of optimizing a convex function over the set of positive semi-definite matrices with unit trace, for which the Sparse-SDP solver of Hazan (2008) is invoked.",
      "startOffset" : 41,
      "endOffset" : 699
    }, {
      "referenceID" : 12,
      "context" : "◦ We apply our method to matrix completion and show that it compares favorably to state-of-the-art techniques for solving (1) (Jaggi & Sulovský, 2010; Mazumder et al., 2010; Shalev-Shwartz et al., 2011).",
      "startOffset" : 126,
      "endOffset" : 202
    }, {
      "referenceID" : 15,
      "context" : "◦ We apply our method to matrix completion and show that it compares favorably to state-of-the-art techniques for solving (1) (Jaggi & Sulovský, 2010; Mazumder et al., 2010; Shalev-Shwartz et al., 2011).",
      "startOffset" : 126,
      "endOffset" : 202
    }, {
      "referenceID" : 16,
      "context" : "It is well known that UrankV > rank is a subgradient of ‖X‖∗ (Watson, 1992), so we find that a subgradient of F (X) = f(X) + λ‖X‖∗ is",
      "startOffset" : 61,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "(All missing proofs appear in the full version of the paper (Avron et al., 2012).",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "For a pseudo-code description, see full version of the paper (Avron et al., 2012).",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "By using a highly tuned package like ScaLAPACK (Choi et al., 1992) good speedups should be attainable with little effort.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "Recent research on QR factorization has shown how to implement it efficiently on communication-bound massive parallel machines (Demmel et al., 2008), MapReduce clusters (Constantine & Gleich, 2011), and GPUs (Anderson et al.",
      "startOffset" : 127,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : ", 2008), MapReduce clusters (Constantine & Gleich, 2011), and GPUs (Anderson et al., 2011).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "For a pseudo-code description, see full version of the paper (Avron et al., 2012).",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)).",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as “Soft-Impute”.",
      "startOffset" : 94,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as “Soft-Impute”. Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)). We used the path-following strategy suggested by the code, i.e., we set a path of values for the regularization parameter (λ) where the results on a value are used as a warmstart for the next value. The results where examined (RMSE measured) at the different points along the regularization path, and the running time at any point λi is the sum of time needed to run the algorithm at λi and the running time of λi−1 (because of the warm start). The path used for MovieLens was (λ0/2, λ0/4, λ0/8, λ0/10), and the path used for Netflix was (λ0/250, λ0/300), where λ0 is the spectral norm of the input matrix Z. Note that both Jaggi & Sulovský (2010) and Mazumder et al.",
      "startOffset" : 94,
      "endOffset" : 896
    }, {
      "referenceID" : 12,
      "context" : "The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as “Soft-Impute”. Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)). We used the path-following strategy suggested by the code, i.e., we set a path of values for the regularization parameter (λ) where the results on a value are used as a warmstart for the next value. The results where examined (RMSE measured) at the different points along the regularization path, and the running time at any point λi is the sum of time needed to run the algorithm at λi and the running time of λi−1 (because of the warm start). The path used for MovieLens was (λ0/2, λ0/4, λ0/8, λ0/10), and the path used for Netflix was (λ0/250, λ0/300), where λ0 is the spectral norm of the input matrix Z. Note that both Jaggi & Sulovský (2010) and Mazumder et al. (2010) apply additional heuristics and/or post-processing to their basic algorithm.",
      "startOffset" : 94,
      "endOffset" : 923
    }, {
      "referenceID" : 12,
      "context" : "The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as “Soft-Impute”. Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)). We used the path-following strategy suggested by the code, i.e., we set a path of values for the regularization parameter (λ) where the results on a value are used as a warmstart for the next value. The results where examined (RMSE measured) at the different points along the regularization path, and the running time at any point λi is the sum of time needed to run the algorithm at λi and the running time of λi−1 (because of the warm start). The path used for MovieLens was (λ0/2, λ0/4, λ0/8, λ0/10), and the path used for Netflix was (λ0/250, λ0/300), where λ0 is the spectral norm of the input matrix Z. Note that both Jaggi & Sulovský (2010) and Mazumder et al. (2010) apply additional heuristics and/or post-processing to their basic algorithm. Additionally, Mazumder et al. (2010) measure only time spent on SVD computations.",
      "startOffset" : 94,
      "endOffset" : 1037
    }, {
      "referenceID" : 12,
      "context" : "the discrepancy between the results we show here and the results reported in (Jaggi & Sulovský, 2010) and (Mazumder et al., 2010).",
      "startOffset" : 106,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "Mazumder et al. (2010) report 0.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "The GECO algorithm proposed in Shalev-Shwartz et al. (2011) is not a solver for nuclear norm regularized problems, but uses a greedy method, with optimality guarantees, to optimize f under explicit low-rank constraints.",
      "startOffset" : 31,
      "endOffset" : 60
    } ],
    "year" : 2012,
    "abstractText" : "We describe novel subgradient methods for a broad class of matrix optimization problems involving nuclear norm regularization. Unlike existing approaches, our method executes very cheap iterations by combining low-rank stochastic subgradients with efficient incremental SVD updates, made possible by highly optimized and parallelizable dense linear algebra operations on small matrices. Our practical algorithms always maintain a low-rank factorization of iterates that can be conveniently held in memory and efficiently multiplied to generate predictions in matrix completion settings. Empirical comparisons confirm that our approach is highly competitive with several recently proposed state-of-the-art solvers for such problems.",
    "creator" : "LaTeX with hyperref package"
  }
}