{
  "name" : "1511.08551.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees",
    "authors" : [ "Xinyang Yi" ],
    "emails" : [ "yixy@utexas.edu", "constantine@utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n08 55\n1v 2\n[ cs\n.L G\n] 5\nD ec\nLatent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art high-dimensional prescriptions (e.g., à la Wainwright (2014)) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples."
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we give general conditions and an analytical framework for the convergence of the EM method for high-dimensional parameter estimation in latent variable models. We specialize these conditions to several problems of interest, including high-dimensional sparse and low-rank mixed regression, sparse gaussian mixture models, and regression with missing covariates. As we explain below, the key problem in the high-dimensional setting is the M -step. A natural idea is to modify this step via appropriate regularization, yet choosing the appropriate sequence of regularizers is a critical problem. As we know from the theory of regularized M-estimators (e.g., Wainwright (2014)) the regularizer should be chosen proportional to the target estimation error. For EM, however, the target estimation error changes at each step.\nThe main contribution of our work is technical: we show how to perform this iterative regularization. We show that the regularization sequence must be chosen so that it converges to a quantity controlled by the ultimate estimation error. In existing work, the estimation error is given\nby the relationship between the population and empirical M -step operators, but the M -operator is not well defined in the high-dimensional setting. Thus a key step, related both to our algorithm and its convergence analysis, is obtaining a different characterization of statistical error for the high-dimensional setting."
    }, {
      "heading" : "Background and Related Work",
      "text" : "EM (e.g., Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently. Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems – low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow “close” to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step.\nIn contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of β∗), as well as the statistical error. Finally, we note that for finite mixture regression, Städler et al.Städler et al. (2010) consider an ℓ1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality. However, this work doesn’t establish the theoretical properties of local optima arising from regularized EM. Our work addresses this issue from a local convergence perspective by using a novel choice of regularization. Notation: Let u = (u1, u2, . . . , up) ⊤ ∈ Rp be a vector and M = [Mi,j ] ∈ Rp1×p2 be a matrix. The ℓq norm of u is defined as ‖u‖p = ( ∑p\ni=1 |ui|q)1/q. We use ‖M‖∗ to denote the nuclear norm of M and ‖M‖2 to denote its spectral norm. We use ⊙ to denote the Hadamard product between two vectors, i.e., u ⊙ v = (u1v1, u2v2, . . . , upvp)⊤. A p-by-p identity matrix is denoted as Ip. We use capital letter (e.g., X) to denote random variable, vector and matrix. For a sub-Gaussian (subexponential) random variable X, we use ‖X‖ψ2 (‖X‖ψ1) to denote its Orlicz norm (see Vershynin (2010) for detailed definitions). For two functions f(n) and g(n), we use f(n) . g(n) to represent f(n) ≤ Cg(n) for some absolute constant C > 0. In parallel, we use f(n) & g(n) to represent f(n) ≥ C ′g(n) for some absolute constant C ′ > 0. For any differentiable function f : Rp → R, we use ∇f to denote its gradient.\nThe rest of our paper is organized as follows. We present our regularized EM algorithm, including the precise sequence of regularization, and discuss its applications to several example models in Section 2. The specific examples to which we show our results apply, are sparse gaussian mixture models, sparse or low-rank mixed regression, and regression with missing covariates. In section 3, we establish our analytical framework and show the main theory, i.e., computational and statistical guarantees of the regularized EM algorithm. Then, by applying our main theory, we establish several near optimal statistical rate of those aforementioned models in section 4. Section 5 demonstrates our results through numerical examples. We outline the proof of our main result in section 6. The detailed proofs of other results and multiple technical lemmas are deferred to the appendix."
    }, {
      "heading" : "2 Regularized EM Algorithm",
      "text" : "In this section, we first present a general regularized EM algorithm in which a convex regularizer is used to enforce certain type of structure. Then we turn to revisit three well known latent variable models and show how the proposed algorithm can be applied to high dimensional parameter estimation in these models."
    }, {
      "heading" : "2.1 Algorithm",
      "text" : "Before introducing our approach, we first review the classic EM algorithm. Let Y ,Z be random variables taking values in Y,Z. Suppose they have join distribution\nfβ(y, z)\ndepending on model parameter β ⊆ Ω where Ω is some parameter space in Rp. In latent variable models, it is common to assume we can only obtain samples from Y while Z, called latent variable, can not be observed. Consider the marginal distribution of Y as\nyβ(y) :=\n∫\nZ fβ(y, z)dz.\nGiven n i.i.d. observations y1,y2, . . . ,yn of Y , our goal is to estimate the model parameter β. We consider the maximum likelihood estimation: compute β̂ ∈ Ω that maximizes the log likelihood function, namely,\nβ̂ = argmax β∈Ω\nh(β;yn1 ), (2.1)\nwhere\nh(β;yn1 ) := 1\nn\nn∑\ni=1\nlog yβ(yi).\nIn many settings, the objective function in (2.1) is highly nonconvex, thereby it’s computationally inefficient to solve it directly. Instead, we turn to a lower bound of h(β;yn1 ) which is more friendly\nto evaluate and optimize. Let κβ(z|y) denote the conditional distribution of Z given Y = y. For any β′ ∈ Ω, we have\nh(β′;yn1 ) = 1\nn\nn∑\ni=1\nlog yβ′(yi) = 1\nn\nn∑\ni=1\nlog\n∫\nZ fβ′(yi, z)dz\n= 1\nn\nn∑\ni=1\nlog\n∫\nZ κβ(z|yi)\nfβ′(yi, z) κβ(z|yi) dz\n(a) ≥ 1 n\nn∑\ni=1\n∫\nZ κβ(z|yi) log\nfβ′(yi, z) κβ(z|yi) dz\n= 1\nn\nn∑\ni=1\n∫\nZ κβ(z|yi) log fβ′(yi, z)dz −\n∫\nZ κβ(z|yi) log κβ(z|yi)dz, (2.2)\nwhere (a) follows from Jensen’s inequality. The key idea of EM algorithm is to perform iterative maximization of the obtained lower bound (2.2). We denote the first term in (2.2) as function Qn(·|·), i.e.,\nQn(β ′|β) := 1\nn\nn∑\ni=1\n∫\nZ κβ(z|yi) log fβ′(yi, z)dz. (2.3)\nOne iteration of EM algorithm, mapping β(t) to β(t+1), consists of the following two steps:\n• E-step: Compute function Qn(β|β(t)) given β(t).\n• M-step: β(t+1) ← argmaxβ∈Ω Qn(β|β(t)).\nIt’s convenient to introduce mapping Mn : Ω → Ω to denote the above algorithm\nMn(β) := argmax β′∈Ω Qn(β ′|β). (2.4)\nWhen n → ∞, we define the population level Q(·|·) function as\nQ(β′|β) := ∫\nY yβ∗(y)\n∫\nZ κβ(z|y) log fβ′(y, z)dzdy. (2.5)\nSimilar to (2.4), we define the population level mapping M : Ω → Ω as\nM(β) = argmax β′∈Ω\nQ(β′|β). (2.6)\nGenerally, the classic EM procedure is not applicable to high dimensional regime where n ≪ p: First, with insufficient number of samples, Mn(β) is usually far way from M(β). In this case, even if the initial parameter is β∗, Mn(β∗) is not a meaningful estimation of β∗. As an example, in Gaussian mixture models, the minimum estimation error ‖Mn(β∗)−M(β∗)‖ can be much larger than signal strength ‖β∗‖. Second, in some models, Mn(β) is not even well defined. For instance, in mixture linear regression, solving (2.4) involves inverting sample covariance matrix that is not full rank when n < p. (See Section 2.2.2 for detailed discussion.)\nWe now turn to our regularized EM algorithm that is designed to overcome the aforementioned high dimensionality challenges. In particular, we propose to replace the M-step with regularized\nAlgorithm 1 High Dimensional Regularized EM Algorithm Input Samples {yi}ni=1, regularizer R, number of iterations T , initial parameter β(0), initial regularization parameter λ (0) n , estimated statistical error ∆, contractive factor κ < 1.\n1: For t = 1, 2, . . . , T do\n2: Regularization parameter update:\nλ(t)n ← κλ(t−1)n +∆. (2.7)\n3: E-step: Compute function Qn(·|β(t−1)) according to (2.3). 4: Regularized M-step:\nβ(t) ← argmax β∈Ω Qn(β|β(t−1))− λ(t)n · R(β).\n5: End For\nOutput β(T ).\nmaximization step. In detail, for some convex regularizer R : Ω → R+ and user specified regularization regularization parameter λn, our regularized M-step is defined as:\nMrn(β) := argmax β′∈Ω Qn(β ′|β)− λnR(β′). (2.8)\nWe present the details of our algorithm in Algorithm 1. The role of R is to enforce the solution to have a certain structure of the model parameter β∗.\nThe choice of regularization parameter λ (t) n plays an important role in controlling statistical and optimization error. As stated in (2.7), the update of λ (t) n involves a linear combination of old parameter λ (t−1) n and the quantity ∆. Then λ (t) n takes the form\nλ(t)n = κ tλ(0)n + 1− κt 1− κ ∆.\nAs shown in Figure 1, λ (t) n first decays geometrically from λ (0) n and then it gradually approaches 1 1−κ ·∆. Quantity ∆ characterizes the target statistical error which depends on number of samples n, data dimension p and some factor associated with concrete models. Usually, we have ∆ = O( √ log p/n), which vanishes when n, p increase with a fixed ratio. To provide some intuitions of such choice, we first note that from theory of high dimensional regularized M-estimator Wainwright (2014), suitable λn should be proportional to the target estimation error. Analogous to our setting, we let λ (t) n be proportional to ‖Mrn(β(t)) − β∗‖2 which is the estimation error in step t. Consider the following triangle inequality\n‖Mrn(β(t))− β∗‖2 ≤ ‖Mrn(β(t))−Mrn(β∗)‖2 + ‖Mrn(β∗)− β∗‖2.\nNote that the second term ‖Mrn(β∗) − β∗‖2 corresponds to quantity ∆ since they both have the sense of final estimation error. The first term ‖Mrn(β(t))−Mrn(β∗)‖2, resulting from optimization error ‖β(t) − β∗‖2, then corresponds to κλ(t−1)n in (2.7). By setting t = 1, we observe that λ(0)n is\n1−κ ∆ represented by the red and blue lines respectively.\nproportional to the initialization error. Consequently, we have λtn ≥ κλ(t−1) +∆. Inspired by the low-dimensional analysis of EM in Balakrishnan et al. (2014), we expect the optimization error to decay geometrically, so we choose κ ∈ (0, 1). Beyond the intuition, we provide the rigorous analysis and detailed parameter update in Section 3."
    }, {
      "heading" : "2.2 Example Models",
      "text" : "Now we introduce three well known latent variable models. For each model, we review the specific formulations of standard EM algorithm, discuss the extensions in high dimensional setting, and provide the implementations of high dimensional regularized EM iterations."
    }, {
      "heading" : "2.2.1 Gaussian Mixture Model",
      "text" : "We consider the balanced isotropic Gaussian mixture model (GMM) with two components where the distribution of random variables (Y,Z) ∈ Rp × {−1, 1} is determined by\nPr (Y = y|Z = z) = φ(y; z · β∗, σ2Ip)\nand Pr(Z = 1) = Pr(Z = −1) = 1/2. Here we use φ(·|µ,Σ) to denote probability density function of N (µ,Σ). In this example, Z is latent variable that indicates the cluster id of each sample. In this example, given n i.i.d. samples {yi}ni=1, function Qn(·|·) defined in (2.3) corresponds to\nQGMMn (β ′|β) = − 1\n2n\nn∑\ni=1\n[ w(yi;β)‖yi − β′‖22 + (1−w(yi;β))‖yi + β′‖22 ] , (2.9)\nwhere\nw(y;β) := exp (−‖y−β‖ 2 2 2σ2 )\nexp (−‖y−β‖ 2 2 2σ2 ) + exp (−‖y+β‖ 2 2 2σ2 ) . (2.10)\nThen we have that the standard EM update (2.4) corresponds to\nMn(β) = 2\nn\nn∑\ni=1\nw(yi;β)yi − 1\nn\nn∑\ni=1\nyi. (2.11)\nIn high dimensional regime, we assume β∗ is sparse. Formally, let B0(s; p) := {u ∈ Rp : | supp(u)| ≤ s}, we have β∗ ∈ B0(s; p). Naturally, we choose regularizer R to be ℓ1 norm in order to recover the sparse structure. Consequently, our regularized EM iteration (2.8) corresponds to\nMrn(β) = arg max β′∈Rp QGMMn (β ′|β)− λn‖β′‖1."
    }, {
      "heading" : "2.2.2 Mixed Linear Regression",
      "text" : "Mixed linear regression (MLR), as considered in some recent work (Chaganty and Liang, 2013; Yi et al., 2013; Chen et al., 2014b), is the problem of recovering two or more linear vectors from mixed linear measurements. In the case of mixed linear regression with two symmetric and balanced components, response-covariate pair (Y,X) ∈ R× Rp is linked through\nY = 〈X, Z · β∗〉+W,\nwhere W is noise term and Z is latent variable that has Rademacher distribution over {−1, 1}. We assume X ∼ N (0, Ip), W ∼ N (0, σ2). In this setting, with n i.i.d. samples {yi,xi}ni=1 of pair (Y,X), function Qn(·|·) then corresponds to\nQMLRn (β ′|β) = − 1\n2n\nn∑\ni=1\n[ w(yi,xi;β)(yi − 〈xi,β′〉)2 + (1− w(yi,xi;β))(yi + 〈xi,β′〉)2 ] , (2.12)\nwhere w(y,x;β) is defined as\nw(y,x;β) := exp (− (y−〈x,β〉)2 2σ2 )\nexp (− (y−〈x,β〉)2 2σ2 ) + exp (− (y+〈x,β〉)2 2σ2\n) .\nThe standard EM iteration (2.4) corresponds to\nMn(β) = ( n∑\ni=1\nxix ⊤ i\n)−1( n∑\ni=1\n(2w(yi,xi;β)− 1)yixi ) . (2.13)\nNote that (2.13) involves inverting sample covariance matrix. Therefore, in high dimensional setting Mn(β) is not well defined since sample covariance matrix has rank much smaller than the ambient dimension. As discussed earlier, characterizing statistical error in terms of Mn(β)−M(β) is not well suited to this case.\nNext we consider two kinds of structure about β∗ in order to deal with high dimensionality. First we assume β∗ is an s-sparse vector, i.e., β∗ ∈ B0(s; p). Then by using ℓ1 regularizer, (2.8) corresponds to\nMrn(β) = arg max β′∈Rp QMLRn (β ′|β)− λn‖β′‖1.\nSecond we consider that the model parameter is a matrix Γ∗ ∈ Rp1×p2 with rank(Γ∗) = θ ≪ min(p1, p2). We further assume X ∈ Rp1×p2 is an i.i.d. Gaussian matrix, i.e., entries of X are independent random variables with distribution N (0, 1). Note that in low dimensional case n ≫ p1 × p2, there is no essential difference between assuming parameter is vector and matrix since we can always treat X and Γ∗ as (p1 × p2)-dimensional vectors. In high dimensional regime, low rank structure leads to different regularization. We choose R to be nuclear norm to serve such structure. Consequently, given n samples with form {yi,Xi}ni=1, (2.8) then corresponds to\nMrn(Γ) = arg max Γ′∈Rp1×p2 − 1 2n\nn∑\ni=1\n[ w(yi,Xi;Γ)(yi − 〈Xi,Γ′〉)2+\n(1− w(yi,Xi;Γ))(yi + 〈Xi,Γ′〉)2 ] − λn‖Γ′‖∗. (2.14)\nThe standard low rank matrix recovery with a single component, including other sensing matrix designs beyond Gaussian matrix, has been studied extensively (e.g.,Candès and Recht (2009); Recht et al. (2010); Candès and Plan (2011); Negahban et al. (2011); Jain et al. (2013); Chen et al. (2013); Cai and Zhang (2015)). To the best of our knowledge, theoretical study of the mixed low rank matrix recover has not been considered in existing literature."
    }, {
      "heading" : "2.2.3 Missing Covariate Regression",
      "text" : "As our last example, we consider the missing covariate regression (MCR) problem. To be the same as standard linear regression, {yi,xi}ni=1 are samples of (Y,X) linked through Y = 〈X,β∗〉 +W . However, we assume each entry of xi is missing independently with probability ǫ ∈ (0, 1). Therefore, the observed covariate x̃i takes the form\nx̃i,j = { xi,j with probability 1− ǫ ∗ otherwise .\nTo ease notation, we introduce vector zi ∈ {0, 1}p to indicate the positions of missing entries, i.e., zi,j = 1 if xi,j is missing. In this example, the E step involves computing the distribution of missing entries given current parameter guess β. Under Gaussian design X ∼ N (0, Ip),W ∼ N (0, σ2), given observed covariate entries (1− zi)⊙ xi and yi, the conditional mean vector of x̃i has form\nµβ(yi, zi,xi) := E[x̃i ∣∣β, yi, (1− zi)⊙ xi] = (1− zi)⊙ xi + yi − 〈β, (1 − zi)⊙ xi〉 σ2 + ‖zi ⊙ β‖22 zi ⊙ β, (2.15)\nand the conditional correlation matrix of x̃i has form\nΣβ(yi, zi,xi) := E [ x̃ix̃ ⊤ i ∣∣β, yi, (1 − zi)⊙ xi ]\n= µβµ ⊤ β + diag(zi)−\n( 1\nσ2 + ‖zi ⊙ β‖22\n) (zi ⊙ β)(zi ⊙ β)⊤. (2.16)\nConsequently, Qn(·|·) corresponds to\nQMCRn (β ′|β) = 1\nn\nn∑\ni=1\n〈yiµβ(yi, zi,xi),β′〉 − 1 2 β⊤Σβ(yi, zi,xi)β. (2.17)\nThe standard EM update corresponds to\nMn(β) = [ n∑\ni=1\nΣβ(yi, zi,xi)\n]−1 n∑\ni=1\nyiµβ(yi, zi,xi).\nNote that Σβ(yi, zi,xi) has rank at most O(ǫp) with high probability. When ǫ = O(1/p), the empirical covariance matrix is non-invertible when n ≪ p. We now assume β∗ ∈ B0(s; p). By leveraging ℓ1 regularization, one step update in Algorithm 1 corresponds to\nMrn(β) = argmax β′∈Rp QMCRn (β ′|β)− λn‖β′‖1."
    }, {
      "heading" : "3 General Computational and Statistical Guarantees",
      "text" : "We now turn to the theoretical analysis of high dimensional regularized EM algorithm. In Section 3.1, we set up a general analytical framework for regularized EM where the key ingredients are decomposable regularizer and several technical conditions about population based Q(·|·) and sample based Qn(·|·). In Section 3.2, we first introduce a resampling version of Algorithm 1 and provide our main result (Theorem 3.3) that characterizes both computational and statistical performance of the proposed variant of regularized EM algorithm."
    }, {
      "heading" : "3.1 Framework",
      "text" : ""
    }, {
      "heading" : "3.1.1 Decomposable Regularizers",
      "text" : "Decomposable regularizer, as considered in a body of work (e.g., Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al. (2014a)), has been shown to be useful, both empirically and theoretically, for high dimensional structural estimation. It also plays an important role in our analytical framework. We begin with the assumption that R : Rp → R+ is a norm, thereby we have R(u+v) ≤ R(u) +R(v), ∀ u,v ∈ Rp. Consider a pair of subspaces (S,S) in Rp such that S ⊆ S. We denote the subspace orthogonal to S with respect to inner product 〈 ·, · 〉 as S⊥, namely\nS⊥ := { u ∈ Ω : 〈 u,v 〉 , ∀ v ∈ S}.\nDefinition 3.1. (Decomposability) Regularizer R : Rp → R+ is decomposable with respect to (S,S) if\nR(u+ v) = R(u) +R(v), for any u ∈ S,v ∈ S⊥.\nUsually the structure of model parameter β∗ can be characterized by specifying a subspace S such that β∗ ∈ S. The common use of regularizer is thus to penalize the compositions of solution that live outside S. As R is a norm, for v ∈ S⊥, we always have R(β∗ + v) ≤ R(β∗) + R(v). Consequently, decomposable regularizers actually make such penalty as much as possible by achieving the upper bound. We are interested in bounding the estimation error in some norm ‖ · ‖. The following quantity is critical in connecting R to ‖ · ‖.\nDefinition 3.2. (Subspace Compatibility Constant) For any subspace S ⊆ Rp, a given regularizer R and some norm ‖ · ‖, the subspace compatibility constant of S with respect to R, ‖ · ‖ is given by\nΨ(S) := sup u∈S\\{0} R(u) ‖u‖ .\nStandardly, the dual norm of R is defined as R∗(v) := supR(u)≤1 〈 u,v 〉 . To simplify notation,\nwe let ‖u‖R := R(u) and ‖u‖R∗ := R∗(u)."
    }, {
      "heading" : "3.1.2 Conditions on Q(·|·)",
      "text" : "Next, we review three technical conditions, originally proposed by Balakrishnan et al. (2014), about population level Q(·|·) function. Recall that Ω ⊆ Rp is the basin of attraction. It is well known that performance of EM algorithm is sensitive to initialization. Analyzing Algorithm 1 with any initial point is not desirable in this paper. Our theory is developed with focus on a r-neighbor region round β∗ that is defined as B(r;β∗) := { u ∈ Ω, ‖u− β∗‖ ≤ r } .\nWe first assume that Q(·|β∗) is self consistent as stated below.\nCondition 1. (Self Consistency) Function Q(·|β∗) is self consistent, namely\nβ∗ = argmax β∈Ω Q(β|β∗).\nIt is usually assumed that β∗ maximizes the population log likelihood function. Under this condition, Condition 1 is always satisfied by following the classical theory of EM algorithmMcLachlan and Krishnan (2007).\nBasically, we require Q(·|β) is differentiable over Ω for any β ∈ Ω. We assume the function Q(·|·) satisfies a certain strongly concavity condition and is smooth over Ω.\nCondition 2. (Strong Concavity and Smoothness (γ, µ, r)) Q(·|β∗) is γ-strongly concave over Ω, i.e.,\nQ(β2|β∗)−Q(β1|β∗)− 〈 ∇Q(β1|β∗),β2 − β1 〉 ≤ −γ\n2 ‖β2 − β1‖2, ∀ β1,β2 ∈ Ω. (3.1)\nFor any β ∈ B(r;β∗), Q(·|β) is µ-smooth over Ω, i.e.,\nQ(β2|β)−Q(β1|β)− 〈 ∇Q(β1|β),β2 − β1 〉 ≥ −µ\n2 ‖β2 − β1‖2, ∀ β1,β2 ∈ Ω. (3.2)\nCondition 2 states that Q(·|β∗) is upper bounded by a quadratic function as shown in (3.1). Meanwhile, (3.2) implies that the function is lower bounded by another quadratic function. It’s worth to note we require such lower bound holds for any function Q(·|β) with β ∈ B(r;β∗) while the upper bound condition is imposed on single function Q(·|β∗). Similar strong concavity and smoothness conditions are widely used in convex optimization and play important roles in showing geometric convergence of gradient descent. Here, such condition will help us achieve geometric decay of optimization error in EM algorithm.\nThe next condition is key in guaranteeing the curvature of Q(·|β) is similar to that of Q(·|β∗) when β is close to β∗.\nCondition 3. (Gradient Stability (τ, r)) For any β ∈ B(r;β∗), we have ∥∥∇Q(M(β)|β) −∇Q(M(β)|β∗) ∥∥ ≤ τ‖β − β∗‖.\nThe above condition only requires the gradient is stable at one point M(β). This is sufficient for our analysis. In fact, for many concrete examples, one can verify a stronger version of condition 3, i.e., for any β′ ∈ B(r;β∗) we have ∥∥∇Q(β′|β)−∇Q(β′|β∗) ∥∥ ≤ τ‖β − β∗‖."
    }, {
      "heading" : "3.1.3 Conditions on Qn(·|·)",
      "text" : "Recall that Qn(·|·) is computed from finite number of samples according to (2.3). We now turn to the two conditions about Qn(·|·). Our first condition, parallel to Condition 2 about function Q(·|·), imposes curvature constraint on Qn(·|·) under finite number of samples. In order to guarantee the estimation error ‖β(t)−β∗‖ in step t of EM algorithm is well controlled, we expect that Qn(·|β(t−1)) is strongly concave at β∗. However, in the setting where n ≪ p, there might exist directions along which Qn(·|β(t−1)) is flat, as we observed in mixed linear regression and missing covariate regression. In contrast with Condition 2, we suppose Qn(·|·) is strongly concave over a particular set C(S,S;R) that is defined in terms of subspace pair (S,S) and regularizer R. In detail, it takes form\nC(S,S;R) := { u ∈ Rp :\n∥∥ΠS⊥(u) ∥∥ R ≤ 2 · ∥∥ΠS(u) ∥∥ R + 2 ·Ψ(S) ·\n∥∥u ∥∥ } , (3.3)\nwhere the projection operator ΠS : Rp → Rp is defined as\nΠS(u) := argmin v∈S\n‖v − u‖. (3.4)\nWith the geometric definition in hand, we provide the restricted strong concavity (RSC) condition as follows.\nCondition 4. (RSC (γn,S,S , r, δ)) For any fixed β ∈ B(r;β∗), with probability at least 1− δ, we have that for all β′ − β∗ ∈ Ω⋂ C(S,S;R),\nQn(β ′|β)−Qn(β∗|β)− 〈 ∇Qn(β∗|β),β′ − β∗ 〉 ≤ −γn\n2 ‖β′ − β∗‖2.\nThe above condition states that Qn(·|β) is strongly concave in direction β′ − β∗ that belongs to C(S,S ;R). It’s instructive to compare Condition 4 with a related condition proposed by Negahban et al. (2009) for analyzing high dimensional M-estimator. In detail, they assume the loss function is strongly convex over cone {u ∈ Rp : ‖ΠS⊥(u)‖R . ‖ΠS(u)‖R}. Therefore our restrictive set (3.3) is similar to the cone but has additional term 2Ψ(S)‖u‖. The main purpose of term 2Ψ(S)‖u‖ is to allow regularization parameter λn jointly control optimization and statistical error. Note that while Condition 4 is stronger than RSC condition in M-estimator since we expand the set, we usually only require the property ‖ΠS⊥(u)‖R . Ψ(S)‖u‖ for showing strong convexity/concavity. Both the set (3.3) and the cone in M-estimator imply such property naturally.\nNext, we establish the second condition that characterizes the achievable statistical error.\nCondition 5. (Statistical Error (∆n, r, δ)) For any fixed β ∈ B(r;β∗), with probability at least 1− δ, we have ∥∥∇Qn(β∗|β)−∇Q(β∗|β) ∥∥ R∗ ≤ ∆n. (3.5)\nTo provide some intuitions why the quantity ∥∥∇Qn(β∗|β) − ∇Q(β∗|β) ∥∥ R∗ is useful in representing the statistical error, we first note that limn→∞∆n = 0, which suggests that we obtain zero statistical error with infinite number of samples. In the case of finite samples, it’s reasonable to believe that ∆n decreases while we increase n. The decreasing rate is indeed the statistical convergence rate we aims to figure out. We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of ‖Mn(β)−M(β)‖2 and ‖Mn(β)−M(β)‖∞ respectively. As mentioned earlier, in high dimensional setting, Mn(β) is not well defined in some models such as mixed linear regression. For mixed linear regression, Wang et al. (2014) resolves this issue by invoking a high dimensional inverse covariance matrix estimation algorithm proposed by Cai et al. (2011). Our formulation (3.5) avoids resolving such ad hoc problems arising from specific models."
    }, {
      "heading" : "3.2 Main Results",
      "text" : "In this section, we provide the theoretical guarantees for regularized EM algorithm. Instead of analyzing Algorithm 1 directly, we introduce a resampling version of Algorithm 1 that is well suited to Conditions 4-5. The key idea is to split the whole dataset into T pieces and use a fresh piece of data in each iteration of regularized EM. We present the details in Algorithm 2.\nAlgorithm 2 High Dimensional Regularized EM Algorithm with Resampling Input Samples {yi}ni=1, number of iterations T , m = n/T , initial regularization parameter λ (0) m ,\nregularizer R, initial parameter β(0), estimated statistical error ∆, contractive factor κ < 1. 1: Evenly split {yi}ni=1 into T disjoint subsets D1,D2, . . . ,DT . 2: For t = 1, 2, . . . , T do\n3: Regularization parameter update:\nλ(t)m ← κλ(t−1)m +∆. (3.6)\n4: E-step: Compute function Q (t) m (·|β(t−1)) from sample set Dt according to (2.3). 5: Regularized M-step:\nβ(t) ← argmax β∈Ω Q(t)m (β|β(t−1))− λ(t)m · R(β).\n6: End For\nOutput β(T ).\nFor norm ‖ · ‖ under our consideration, we let α := supu∈Rp\\{0} ‖u‖∗/‖u‖, where ‖ · ‖∗ is the dual norm of ‖ · ‖. For Algorithm 1, we prove the following result.\nTheorem 3.3. We assume the model parameter β∗ ∈ S and regularizer R is decomposable with respect to (S,S) where S ⊆ S ⊆ Rp. For some r > 0, suppose B(r;β∗) ⊆ Ω. Suppose function Q(·|·), defined in (2.5), is self consistent and satisfies Conditions 2-3 with parameters (γ, µ, r) and (τ, r). Given n samples and T iterations, let m := n/T . Suppose Qm(·|·), computed from any m i.i.d. samples according to (2.3), satisfies conditions 4-5 with parameters (γm,S,S, r, 0.5δ/T ) and (∆m, r, 0.5δ/T ). Let\nκ∗ := 5 αµτ\nγγm .\nWe assume 0 < τ < γ and 0 < κ∗ ≤ 3/4. Moreover, we define ∆ := rγm/[60Ψ(S)] and assume ∆m is sufficiently small such that\n∆m ≤ ∆. (3.7)\nConsider the procedures in Algorithm 2 with initialization β(0) ∈ B(r;β∗) and let the regularization parameters be\nλ(t)m = κ t γm 5Ψ(S) ‖β(0) − β∗‖+ 1− κ t 1− κ ∆, t = 1, 2, . . . , T (3.8)\nfor any ∆ ∈ [3∆m, 3∆], κ ∈ [κ∗, 3/4]. Then with probability at least 1 − δ, we have that for any t ∈ [T ],\n‖β(t) − β∗‖ ≤ κt‖β(0) − β∗‖+ 5 γm 1− κt 1− κ Ψ(S)∆. (3.9)\nProof. See Section 6 for detailed proof.\nThe above result suggests that with suitable regularization parameters, the estimation error is bounded by two terms. The first term, decaying geometrically with number of iterations t, results from iterative optimization of function Qm thus is referred to as optimization error. The second term called statistical error characterizes the ultimate estimation error of Algorithm 2. With sufficiently large T such that the second term dominates the first term and suitable choice of ∆ such that ∆ = O(∆n/T ), we have the ultimate estimation error as\n‖β(T ) − β∗‖ . 1 (1− κ)γn/T Ψ(S)∆n/T . (3.10)\nSince the optimization error decays exponentially with T and ∆n usually decays polynomially with 1/n, it’s sufficient to set T = O(log n) which thus bring us estimation error O ( Ψ(S)∆n/ logn ) . We have a log n factor loss by using the resampling technique. Removing the logarithmic factor requires direct analysis of Algorithm 1 where the main ingredient is to assume Conditions 4-5 hold uniformly for all β ∈ B(r;β∗) with high probability. Although extending Theorem 3.3 to cover Algorithm 1 with the new ingredient is straightforward, it’s challenging to validate the new conditions in example models.\nWe place the constraint ∆m . rγm/Ψ(S) in (3.7) so that β(t) is guaranteed to be contained in B(r;β∗) for all t ∈ [T ]. Note that this constraint is quite mild in the sense that if ∆m = Ω(rγm/Ψ(S)), β(0) is a decent estimator with estimation error O(Ψ(S)∆m/γm) that already matches our expectation.\nEquality (3.8) corresponds to update rule of regularization parameters (3.6) in Algorithm 2.\nRecall that with (λ (0) m ,∆, κ), we choose the update rule to be\nλtm ← κ · λt−1m +∆.\nFollowing Theorem 3.3, (∆, κ, λ (0) m ) should satisfy the following conditions\nκ∗ ≤ κ ≤ 3/4, 3∆m ≤ ∆ ≤ 3∆, λ(0)m = γm 5Ψ(S)‖β (0) − β∗‖. (3.11)\nAs we observe, quantity κ∗ is the minimum contractive parameter that is allowed to set. Parameter ∆ characterizes the obtainable statistical error and should be set proportional to ∆m. Initial regularization parameter λ (0) m characterizes the initial estimation error ‖β(0) − β∗‖. Note that accurate estimation of ‖β(0) − β∗‖ is not required. In fact, one can set λ(0)m = γm5Ψ(S)ε with any ε ∈ [ ‖β(0) − β∗‖2, r ] . Then with proof similar to that of Theorem 3.3, we can show that\n‖β(t) − β∗‖ ≤ κtε+ 5 γm 1− κt 1− κ Ψ(S)∆, for all t ∈ [T ].\nConsequently, overestimating initial error will potentially increase the total number of iterations but has no essential impact on the ultimate estimation error."
    }, {
      "heading" : "4 Applications to Example Models",
      "text" : "In this section, we apply our high dimensional regularized algorithm and the analytical framework introduced in Section 3 to the aforementioned three example models: Gaussian mixture model, mixed linear regression and missing covariate regression. For each model, based on the high dimensional regularized EM iterations introduced in Section 2.2, we provide the corresponding initialization condition, regularization update, computational convergence guarantee and statistical rate."
    }, {
      "heading" : "4.1 Gaussian Mixture Model",
      "text" : "We now use our analytical framework to analyze the Gaussian Mixture Model (GMM) with sparse model parameter β∗. Recall that we consider the isotropic, balanced Gaussian Mixture Model with two components where sample yi is generated from either N (β∗, σ2Ip) or N (β∗, σ2Ip). The following quantity called SNR is critical in characterizing the difficulty of estimating β∗.\nSNR := ‖β∗‖2/σ. (4.1)\nWe focus on the high SNR regime where we assume SNR ≥ ρ for some constant ρ. Note that the work in Ma and Xu (2005) provides empirical and theoretical evidences that in low SNR regime, where the overlap density of two Gaussian cluster is small, standard EM algorithm suffers from sublinear convergence asymptotically. Therefore the high SNR condition is necessary for showing exponential/linear convergence of EM algorithm and our high dimensional variant. Note that we\nare interested in quantizing estimation error using ℓ2 norm. We thus set the norm ‖ · ‖ in our framework to be ‖ · ‖2 in this section. Recall that we set regularizer R to be ℓ1 norm. For any subset S ⊆ {1, . . . , p}, ℓ1 norm is decomposable with respect to (S,S). For any β∗ ∈ B0(s; p), by letting S = supp(β∗),S = supp(β∗), we have Ψ(S) = √s and C(S,S ;R) corresponds to {‖uS⊥‖1 ≤ 2‖uS‖1 + 2 √ s‖u‖2}.\nAccording to the QGMMn (·|·) introduced in (2.9), by taking expectation of it, we have\nQGMM(β′|β) = −1 2 E [ w(Y ;β)‖Y − β′‖22 + (1− w(Y ;β))‖Y + β′‖22 ] . (4.2)\nWe now check Conditions 1-3 hold for QGMM(·|·). We begin with proving the following result. Lemma 4.1. (Self consistency of GMM) Consider Gaussian mixture model with QGMM (·|·) given in (4.2). For model parameter β∗ we have\nβ∗ = arg max β∈Rp QGMM (β|β∗).\nProof. See Appendix A.1 for detailed proof.\nThe above result suggests thatQGMM (·|·) satisfies Condition 1. It is easy to see∇2QGMM (β′|β) = −Ip, which implies that QGMM (·|·) satisfy Condition 2 with parameters (γ, µ, r) = (1, 1, r) for any r > 0. Next we present a result showing that QGMM (·|·) satisfies Condition 3 with arbitrarily small stability factor τ when SNR is sufficiently large.\nLemma 4.2. (Gradient stability of GMM) Consider the Gaussian Mixture Model with QGMM (·|·) given in (4.2). Suppose SNR defined in (4.1) is lower bounded by ρ, i.e., SNR ≥ ρ. Function QGMM (·|·) satisfies Condition 3 with parameters (τ, ‖β∗‖2/4), where τ ≤ exp(−Cρ2) for some absolute constant C.\nProof. See the proof of Lemma 3 in Balakrishnan et al. (2014).\nNow we turn to the conditions about QGMMn (·|·). Lemma 4.3. (RSC of GMM) Consider Gaussian mixture model with any β∗ ∈ B0(s; p) and QGMMn (·|·) given in (2.9). For any r > 0, we have QGMMn (·|·) satisfies Condition 4 with parameters (γn,S,S , r, δ), where γn = 1, δ = 0, (S,S) = (supp(β∗), supp(β∗)). Proof. See Appendix A.2 for detailed proof.\nThis above result indicates that the restricted strong concavity condition holds deterministically in this example. The next lemma validates the statistical error condition and provides the corresponding parameters.\nLemma 4.4. (Statistical error of GMM) Consider Gaussian mixture model with QGMMn (·|·) and QGMM (·|·) given in (2.9) and (4.2) respectively. For any r > 0, δ ∈ (0, 1) and some absolute constant C, Condition 5 holds with parameters (∆n, r, δ) where\n∆n = C(‖β∗‖∞ + σ) √ log p+ log(2e/δ)\nn .\nProof. See Appendix A.3 for detailed proof.\nNow we give the guarantees of Algorithm 2 for Gaussian mixture model.\nCorollary 4.5. (Sparse Recovery in GMM) Consider the Gaussian mixture model with any fixed β∗ ∈ B0(s; p) and implementations of Algorithm 2 with regularizer ℓ1 norm. Suppose β(0) ∈ B(‖β∗‖2/4;β∗) and SNR ≥ ρ with sufficiently large ρ. Let initial regularization parameter λ(0)n/T be\nλ (0) n/T =\n1\n5 √ s ‖β(0) − β∗‖2\nand quantity ∆ be\n∆ = C(‖β∗‖∞ + σ) √ log p\nn T\nfor sufficiently large constant C. Moreover, let the number of samples n be sufficiently large such that\nn/T ≥ [80C(‖β∗‖∞ + σ)/‖β∗‖2]2 s log p. (4.3)\nThen by setting λ (t) n/T = κ tλ (0) n/T + 1−κt 1−κ ∆ for any κ ∈ [1/2, 3/4], with probability at least 1 − T/p, we have that\n‖β(t) − β∗‖2 ≤ κt‖β(0) − β∗‖2 + 5C(‖β∗‖∞ + σ)\n1− κ\n√ s log p\nn T , for all t ∈ [T ]. (4.4)\nProof. This result follows from Theorem 3.3. First, recall that the minimum contractive factor κ∗ is κ∗ = 5 αµτγγn/T . For ℓ2 norm, we have α = 1. Following the fact that (γ, µ) = (1, 1) and Lemma 4.2- 4.3, we have κ∗ ≤ 20 exp(−Cρ2) for some constant C. We further have κ∗ ≤ 12 when ρ is sufficiently large. Second, based on Lemma 4.4, we set δ = 1/p and let ∆ be ∆ = C(‖β∗‖∞ + σ) √ T log p/n with sufficiently large C such that ∆ ≥ 3∆n/T . Thirdly, by the assumption in (4.3), we have that ∆ ≤ 3∆ where ∆ = ‖β∗‖2/(240 √ s) in this example. Finally, we choose λ (0) n/T = ‖β(0) −β∗‖/(5 √ s) by following (3.11). Packing up these ingredients and following Theorem 3.3, we have that by choosing any κ ∈ [1/2, 3/4], ‖β(t) − β∗‖2 ≤ κt‖β(0) − β∗‖2 + 5 √ s∆/(1− κ), which thus completes the proof.\nNote that ‖β(0) − β∗‖ . ‖β∗‖2 ≤ √ s‖β∗‖∞. Let us set T = C log( nlog p) for sufficiently large C such that the first term in (4.4) is dominated by the second term. Then Corollary 4.5 suggests the final estimation is\n‖β(T ) − β∗‖2 . C(‖β∗‖∞ + δ) √ s log p\nn log\n( n\nlog p\n) .\nNote that the minimax rate for estimating s-sparse vector in a single Gaussian cluster is √ s log p/n, thereby the established rate is optimal on (n, p, s) up to a logarithmic factor."
    }, {
      "heading" : "4.2 Mixed Linear Regression",
      "text" : "We now turn to Mixed Linear Regression (MLR) model. In particular, we will consider two sets of model parameters: β∗ ∈ B0(s; p) and Γ∗ ∈ Rp1×p2 with rank(Γ∗) = r. For the two settings, the population level analysis is identical under i.i.d. Gaussian covariate design. Without loss of generality, we begin with treating the model parameter as a vector β∗ ∈ Rp and validate Conditions 1-3 for QMLR(·|·) in this example. Given function QMLRn (·|·) in (2.12), by taking expectation of it, we have\nQMLR(β′|β) = −1 2 E [ w(Y,X;β)(Y − 〈X,β′〉)2 + (1− w(Y,X;β))(Y + 〈X,β′〉)2 ] (4.5)\nFor now, we set the norm ‖ · ‖ in our framework to ‖ · ‖2. We begin by checking the self consistency condition.\nLemma 4.6. (Self consistency of MLR) Consider mixed linear regression with model parameter β∗ ∈ Rp and QMLR(·|·) given in (4.5). We have\nβ∗ = arg max β∈Rp QMLR(β|β∗).\nProof. See Appendix B.1 for detailed proof.\nIt is easy to check ∇2QMLR(β′|β) = −Ip. Therefore, QMLR(·|·) satisfies Condition 2 with parameters (γ, µ, r) = (1, 1, r) for any r > 0. Similar to Gaussian mixture model, we introduce the following SNR quantity to characterize the hardness of the problem.\nSNR := ‖β∗‖/σ.\nThe work in Chen et al. (2014b) shows that there exists an unavoidable phase transition of statistical rate from high SNR to low SNR. In detail, in low-dimensional setting, the obtainable statistical error is Ω( √ p/n) that matches the standard linear regression when SNR ≥ ρ for some constant ρ. Meanwhile, the unavoidable rate becomes Ω((p/n)1/4) when SNR ≪ ρ. We conjecture such transition phenomenon still exists in high dimensional setting. For now we focus on the high SNR regime and show our algorithm achieves statistical rate that matches the standard sparse linear regression and low rank matrix recovery (up to logarithmic factor) in the end.\nThe following result validates Condition 3 holds with arbitrarily small stability factor τ when\nSNR is sufficiently large and the radius r of ball B(r;β∗) is sufficiently small.\nLemma 4.7. (Gradient Stability of MLR) Consider mixed linear regression model with function QMLR(·|·) given in (4.5). For any ω ∈ [0, 1/4], let r = ω‖β∗‖2. Suppose SNR ≥ ρ for some constant ρ. Then for any β ∈ B(r;β∗), we have\n‖∇QMLR(M(β)|β) −∇QMLR(M(β)|β∗)‖2 ≤ τ‖β − β∗‖2\nwith\nτ = 17\nρ + 7.3ω.\nProof. See Appendix B.2 for detailed proof.\nIn Balakrishnan et al. (2014), it is proved that when r = 132‖β∗‖2, there exists τ ∈ [0, 1/2] such that QMLR(·|·) satisfies Condition 3 with parameter τ when ρ is sufficiently large. Note that Lemma 4.7 recovers this result. Moreover, Lemma 4.7 provides an explicit function to characterize the relationship between τ and ρ, ω.\nNext we turn to validate the two technical conditions of QMLRn (·|·) and establish the computational and statistical guarantees of estimating mixed linear parameters in high dimensional regime. We consider two different structures of linear parameters: (1) model parameter β∗ is a sparse vector; (2) model parameter Γ∗ is a low rank matrix. Note that we assume X is a fully random Gaussian vector/matrix, thereby the population level conditions about QMLR(·|·) hold in both settings.\nSparse Recovery. We assume model parameter β∗ is s-sparse, i.e., β∗ ∈ B0(s; p). Recall that, in order to serve sparse structure, we choose R to be ℓ1 norm. Setting S = S = supp(β∗), set C(S,S ;R) corresponds to {u : ‖uS⊥‖1 ≤ 2‖uS‖1 + 2 √ s‖u‖2}. Restricted concavity of QMLR(·|·) is validated in the following result.\nLemma 4.8. (RSC of MLR with sparsity) Consider mixed linear regression with any model parameter β∗ ∈ B0(s; p) and function QMLRn (·|·) defined in (2.12). There exit absolute constants {Ci}3i=0 such that, if n ≥ C0s log p, then for any r > 0, QMLRn (·|·) satisfies Condition 4 with parameters (γn,S,S , r, δ), where\nγn = 1 3 , (S,S) = (supp(β∗), supp(β∗)), δ = C1 exp(−C2n).\nProof. See Appendix B.3 for detailed proof.\nLemma 4.8 states that using n = O(s log p) samples makes QMLRn (·|·) be strongly concave over C with high probability.\nLemma 4.9. (Statistical error of MLR with sparsity) Consider mixed linear regression model with any β∗ ∈ B0(s; p) and functions QMLRn (·|·), QMLR(·|·) defined in (2.12) and (4.5) respectively. There exist constants C and C1 such that, for any r > 0 and δ ∈ (0, 1), if n ≥ C1(log p+ log(6/δ)), then\n‖∇QMLRn (β∗|β)−∇QMLR(β∗|β)‖∞ ≤ C(‖β∗‖2 + δ) √ log p+ log(6/δ)\nn for all β ∈ B(r;β∗)\nwith probability at least 1− δ.\nProof. See Appendix B.4 for detailed proof.\nLemma 4.9 implies Condition 5 hold with parameters ∆n = O ( (‖β∗‖2 + δ) √ log p n ) , any r > 0\nand δ = 1/p. Putting all the ingredients together leads to the following guarantee about sparse recovery in mixed linear regression using regularized EM algorithm.\nCorollary 4.10. (Sparse recovery in MLR) Consider the mixed linear regression model with any fixed model parameter β∗ ∈ B0(s; p) and the implementation of Algorithm 2 using ℓ1 regularization. Suppose SNR ≥ ρ for sufficiently large ρ. Let quantity ∆ be\n∆ = C(‖β∗‖2 + δ) √ log p\nn T\nand number of samples n satisfy\nn/T ≥ C ′ [(‖β∗‖2 + δ)/‖β∗‖2]2 s log p\nfor some sufficiently large constants C and C ′. Given any fixed β(0) ∈ B(‖β∗‖2/240,β∗), let initial regularization parameter λ\n(0) n/T be\nλ (0) n/T =\n1\n15 √ s ‖β(0) − β∗‖2.\nThen by setting λ (t) n/T = κ tλ (0) n/T + 1−κt 1−κ ∆ for any κ ∈ [1/2, 3/4], we have that, with probability at least 1− T/p,\n‖β(t) − β∗‖2 ≤ κt‖β(0) − β∗‖2 + 15C(‖β∗‖2 + δ)\n1− κ\n√ s log p\nn T , for all t ∈ [T ].\nProof. The result follows from Theorem 3.3. First, we note that the minimum contractive factor κ∗ = 5 αµτγγn/T = 15τ in this example since α = 1, µ = γ = 1 and γn/T = 1/3 w.h.p when n & s log p (see Lemma 4.8). Following Lemma 4.7, κ∗ ≤ 1/2 when w ≤ 1/240 and ρ is sufficiently large. Second, by choosing n/T & s log p, we have ∆n/T . (‖β∗‖2 + δ) √ T log p n w.h.p., as proved in Lemma 4.9. Lastly, we have ∆ ≤ 3∆ by assuming n/T & [(‖β∗‖2 + δ)/‖β∗‖2]2 s log p. Putting these ingredients together and plugging the established parameters into (3.9) complete the proof.\nCorollary 4.10 provides that the final estimation error is\n‖β(T ) − β∗‖2 . κT ‖β(0) − β∗‖2 + (‖β∗‖2 + δ) √ T s log p\nn .\nNote that the second term dominates when T is chosen to satisfy T & log (n/(Ts log p)). Performing T = C log(n/(s log p)) iterations gives us\n‖β(T ) − β∗‖2 . (‖β∗‖2 + δ) √ s log p\nn log\n( n\ns log p\n) .\nThe dependence on (s, p, n) is thus O ( (s log p/n)1/2−c ) for any c > 0. Note that the standard sparse regression has optimal statistical error √ s log p/n, thereby the obtained rate for mixed linear regression is optimal up to logarithmic factor. A caveat here is that the estimation error is proportional to signal strength ‖β∗‖2, i.e., s log p/n determines the relative error instead of absolute error as usually observed in high dimensional estimation. This phenomena, also appearing\nin low dimensional analysis Balakrishnan et al. (2014), arises from the fundamental limits of EM algorithm. It’s worth to note that Chen et al. (2014b) establish near-optimal low dimensional estimation error that does not depend on ‖β∗‖2 based on a convex optimization approach. It’s interesting to explore how to remove ‖β∗‖2 in high dimensional setting.\nLow Rank Recovery. In the sequel, we assume model parameter Γ∗ ∈ Rp1×p2 is a low rank matrix that has rank(Γ∗) = θ ≪ min{p1, p2}. We focus on measuring the estimation error in Frobenius norm thus set ‖ · ‖ in our framework to be ‖ · ‖F . Note that by treating Γ∗ as a vector, Frobenius norm is equivalent to ℓ2 norm, thereby we still have Lemma 4.6-4.7 in this setting. Moreover, SNR is similarly defined as\nSNR := ‖Γ∗‖F /σ.\nIn order to serve the low rank structure, we choose R to be nuclear norm ‖ · ‖∗. For any matrix M, we let row(M) denote the subspace spanned by the rows of M and col(M) denote the subspace spanned by the columns of M. Moreover, for subspace represented by the columns of matrix U, we denote the subspace orthogonal to U as U⊥. For Γ∗ with singular value decomposition U∗ΣV∗⊤, we thus let\nS = { M ∈ Rp1×p2 : col(M) ⊆ U∗, row(M) ⊆ V∗ } (4.6)\nand\nS⊥ = { M ∈ Rp1×p2 : col(M) ⊆ U∗⊥, row(M) ⊆ V∗⊥ } . (4.7)\nSo S contains all matrices with rows (and columns) living in the row (and column) space of Γ∗. Subspace S⊥ contains all matrices with rows (and columns) orthogonal to the row (and column) space of Γ∗. Nuclear norm is decomposable with respect to (S,S). We have Ψ(S) = supM∈S\\{0} ‖M‖∗/‖M‖F ≤ √ 2θ since matrix in S has rank at most 2θ. Similar to Lemma 4.8 and 4.9 for sparse structure, we have the following two results for low rank structure.\nLemma 4.11. (RSC of MLR with low rank structure) Consider mixed linear regression with model parameter Γ∗ ∈ Rp1×p2 that has rank(Γ∗) = θ. There exists constants {Ci}2i=0 such that, if n ≥ C0θmax{p1, p2}, then for any θ ∈ (0,min{p1, p2}), QMLRn (·|·) satisfies Condition 4 with parameters (γn,S,S , r, δ), where (S,S) are given in (4.6) and (4.7),\nγn = 1\n20 , δ = C1 exp(−C2n).\nProof. See Appendix B.5 for detailed proof.\nLemma 4.12. (Statistical error of MLR with low rank structure) Consider the mixed linear regression with any Γ∗ ∈ Rp1×p2 . There exists constants C and C1 such that, for any fixed Γ ∈ Rp1×p2 and δ ∈ (0, 1), if n ≥ C1(p1 + p2 + log(6/δ)), then\n‖∇QMLR(Γ∗|Γ)−∇QMLRn (Γ∗|Γ)‖2 ≤ C(‖Σ∗‖F + σ) √ p1 + p2 + log(6/δ)\nn\nwith probability at least 1− δ.\nProof. See the Appendix B.6 for detailed proof.\nSetting δ = 6exp(−(p1 + p2)) in Lemma 4.12 suggests that Condition 5 holds with parameters (∆n, r, δ) where ∆n . (‖Γ∗‖F + δ) √ (p1 + p2)/n, δ = exp(−(p1 + p2)) and r can be any positive number. Putting these pieces together leads to the following guarantee about low rank recovery.\nCorollary 4.13. (Low rank recovery in MLR) Consider mixed linear regression with any model parameter Γ∗ ∈ Rp1×p2 that has rank at most θ and the implementation of Algorithm 2 with nuclear norm regularization. Suppose SNR ≥ ρ for sufficiently large ρ. Let quantity ∆ be\n∆ = C(‖Γ∗‖F + σ) √\np1 + p2 n T\nand the number of samples n satisfy\nn/T ≥ C ′ [(‖Γ∗‖F + σ)/‖Γ∗‖F ]2 θ(p1 + p2)\nfor some sufficiently large constants C and C ′. Given any fixed Γ(0) ∈ B(‖Γ∗‖F /1600,Γ∗), let initial regularization parameter λ\n(0) n/T be\nλ (0) n/T =\n1\n100 √ 2θ ‖Γ(0) − Γ∗‖F .\nThen by setting λ (t) n/T = κ tλ (0) n/T + 1−κt 1−κ ∆ for any κ ∈ [1/2, 3/4], we have that, with probability at least 1− T exp(−p1 − p2),\n‖Γ(t) − Γ∗‖F ≤ κt‖Γ(0) − Γ∗‖F + 100C ′(‖Γ∗‖F + σ)\n1− κ\n√ 2θ(p1 + p2)\nn T , for all t ∈ [T ].\nProof. This result is parallel to Corollary 4.10 for sparse recovery thus can be proved similarly. We omit the details.\nCorollary 4.13 indicates that the final estimation error can be characterized by\n‖Γ(T ) − Γ∗‖F . κT ‖Γ(0) − Γ∗‖F + (‖Γ∗‖F + σ) √ θ(p1 + p2)\nn T .\nNote that the initialization error is proportional to ‖Γ∗‖F . Choosing T = O(log(n/[θ(p1 + p2)])), the first term representing optimization error is then dominated by the second term. We thus have\n‖Γ(T ) − Γ∗‖F . (‖Γ∗‖F + σ) √ θ(p1 + p2)\nn log\n( n\nθ(p1 + p2)\n) .\nThe established statistical rate matches (up to the logarithmic factor) the (single) low rank matrix estimation rate proved in Candès and Plan (2011); Negahban et al. (2011), which is known to be minimax optimal. It’s worth to note that our rate is proportional to the signal strength ‖Γ∗‖F . Therefore, the normalized sample complexity n/[θ(p1 + p2)] controls the relative error instead of absolute error in standard low rank matrix estimation."
    }, {
      "heading" : "4.3 Missing Covariate Regression",
      "text" : "We now consider the sparse recovery guarantee of Algorithm 2 for missing covariate regression. We begin by validating conditions about function QMCR(·|·), which has form\nQMCR(β′|β) = 〈 E [Y µβ(Y,Z,X)] ,β ′〉− 1 2 〈 E [Σβ(Y,Z,X)] ,ββ ⊤ 〉 . (4.8)\nFirst, M(·) is self consistent as stated below.\nLemma 4.14. (Self-consistency of MCR) Consider missing covariate regression with parameter β∗ ∈ Rp and QMCR(·|·) given in (4.8). We have\nβ∗ = arg max β∈Rp QMCR(β|β∗).\nProof. See Appendix C.1 for detailed proof.\nFor our analysis, we define ρ := ‖β∗‖2/σ to be the signal to noise ratio and ω := r/‖β∗‖2 to be the relative contractivity radius. Let\nζ := (1 + ω)ρ.\nRecall that ǫ is the missing probability of every entry. The next result characterizes the smoothness and concavity of QMCR(·|·).\nLemma 4.15. (Smoothness and concavity of MCR) Consider missing covariate regression with parameter β∗ ∈ Rp and QMCR(·|·) given in (4.8). For any ω > 0, we have that QMCR(·|·) satisfies Condition 2 with parameters (γ, µ, ω‖β∗‖2), where\nγ = 1, µ = 1 + 2ζ2 √ ǫ+ (1 + ζ2)ζ2ǫ.\nProof. See Appendix C.2 for detailed proof.\nWe revisit the following result about the gradient stability from Balakrishnan et al. (2014).\nLemma 4.16. (Gradient stability of MCR) Consider the missing covariate regression with β∗ ∈ Rp and QMCR(·|·) given in (4.8). For any ω > 0, ρ > 0, QMCR(·|·) satisfies Condition 3 with parameter (τ, ω‖β∗‖2) where\nτ = ζ2 + 2ǫ(1 + ζ2)2\n1 + ζ2 .\nProof. See the proof of Corollary 6 in Balakrishnan et al. (2014).\nUnlike the previous two models, we require an upper bound on the signal noise ratio. This\nunusual constraint is in fact unavoidable, as pointed out in Loh and Wainwright (2012).\nWe now turn to validate the conditions about finite sample function QMCRn (·|·). In particular, we have the following two guarantees.\nLemma 4.17. (RSC of MCR) Consider missing covariate regression with any fixed parameter β∗ ∈ B0(s; p) and QMCRn (·|·) given in (2.17). There exist constants {Ci}3i=0 such that if ǫ ≤ C0 min{1, ζ−4} and n ≥ C1(1 + ζ)8s log p, then we have QMCRn (·|·) satisfies Condition 4 with parameters (γn,S,S , ω‖β∗‖2, δ), where\nγn = 1 9 , (S,S) = (supp(β∗), supp(β∗)), δ = C2 exp(−C3n(1 + ζ)−8).\nProof. See Appendix C.3 for detailed proof.\nLemma 4.18. (Statistical error of MCR) Consider missing covariate regression with any fixed parameter β∗ ∈ B0(s; p) and QMCRn (·|·) given in (2.17). There exist constants C0, C1 such that if n ≥ C0[log p + log(24/δ)], then for any δ ∈ (0, 1) and any fixed β ∈ B(ω‖β∗‖2,β∗), we have that for\n‖∇QMCRn (β∗|β)−QMCR(β∗|β)‖∞ ≤ C1(1 + ζ)5σ √ log p+ log(24/δ)\nn\nwith probability at least 1− δ.\nProof. See Appendix C.4 for detailed proof.\nBy setting δ = 1/p in Lemma 4.18 immediately implies that QMCRn satisfies Condition 5 with\nparameters ∆n = O ( (1 + ζ)5σ √ log p/n ) , r = ω‖β∗‖2 and δ = 1/p.\nEnsembling all pieces leads to the following guarantee about resampling version of regularized\nEM on missing covariate regression.\nCorollary 4.19. (Sparse Recovery in MCR) Consider the missing covariate regression with any fixed model parameter β∗ ∈ B0(s; p) and the implementation of Algorithm 2 with ℓ1 regularization. Let quantity ∆ be\n∆ = Cσ\n√ log p\nn T\nand number of samples n satisfies\nn/T ≥ C ′max{σ2(ωρ)−1, 1}s log p\nfor sufficiently large constants C,C ′. Suppose (1 + ω)ρ ≤ C0 < 1 and ǫ ≤ C1 for sufficiently small constants C0, C1. Given any fixed β\n(0) ∈ B(ω‖β∗‖2,β∗), let initial regularization parameter λ(0)n/T be\nλ (0) n/T =\n1\n45 √ s ‖β(0) − β∗‖2.\nBy choosing λ (t) n/T = κ tλ (0) n/T + 1−κt 1−κ ∆ for any κ ∈ [1/2, 3/4] in Algorithm 2 leads to\n‖β(t) − β∗‖2 ≤ κt‖β(0) − β∗‖2 + 45Cσ\n1− κ\n√ s log p\nn T , for all t ∈ [T ],\nwith probability at least 1− T/p.\nProof. Following Theorem 3.3, we have κ∗ = 5 αµτγγn/T . For ℓ2 norm, α = 1. Based on Lemma 4.17, we have γn = 1/9. Following Lemma 4.15 and 4.16, we have γ = 1 and can always find sufficiently small constants C0, C1 such that µ ≤ 10/9 and τ ≤ 1/100. We thus obtain κ∗ ≤ 1/2. From Lemma 4.18, one can check ∆ > 3∆n/T under suitable C. We choose n/T & σ\n2(ωρ)−1s log p to make sure ∆ ≤ 3∆. With these conditions in hand, direct applying Theorem 3.3 completes the proof.\nBy choosing T = O(log(n/[s log p])) (for simplicity, we let ω = O(1)) in Corollary 4.19, the final\nestimation can be controlled by\n‖β(T ) − β∗‖2 . σ √ s log p\nn log\n( n\ns log p\n) ,\nwhich is optimal up to logarithmic factor. As stated, Corollary 4.19 is applicable whenever (1 + ω)ρ ≤ C0 and ǫ ≤ C1 for some constants C0. In particular, we have C0 < 1 that implies σ > ‖β∗‖2. Note that while low SNR is favorable in analysis, for fixed signal strength, lower SNR still leads to higher estimation error as standard (sparse) linear regression. For models with ‖β∗‖2 ≥ σ, we can always add stochastic noise manually to the response yi such that (1 + ω)ρ ≤ C0 holds. This preprocessing trick combined with regularized EM algorithm thus leads to sparse recovery with error Õ(max{σ, ‖β∗‖2} √ s log p/n) for the whole range of SNR."
    }, {
      "heading" : "5 Simulations",
      "text" : "In this section, we provide the simulation results to back up our theory. Note that even our theory built on resampling technique, it’s statistically efficient to use partial dataset in practice. Consequently, we test the performance of regularized EM algorithm without sample splitting (Algorithm 1). We apply Algorithm 1 to the four latent variable models introduced in Section 2.2: Gaussian mixture model (GMM), mixed linear regression with sparse vector (MLR-Sparse), mixed linear regression with low rank matrix (MLR-LowRank) and missing covariate regression (MCR). We conduct two sets of experiments."
    }, {
      "heading" : "5.1 Convergence Rate",
      "text" : "We first evaluate the convergence of Algorithm 1 with good initialization β(0) ( particularly, we use Γ(0) to denote a matrix initial parameter for model MLR-LowRank), that is, ‖β(0)−β∗‖2 = ω‖β∗‖2 for some constant ω. For models with s-sparse parameters (GMM, MLR-Sparse and MCR), we choose the problem size to be n = 500, p = 800, s = 5. For MLR-LowRank, we choose n = 600, p1 = p2 = p = 30, rank θ = 3. In addition, we set SNR = 5, ω = 0.5 for GMM, MLR-Sparse and MLR-LowRank; we set SNR = 0.5, ω = 0.5 and missing probability ǫ = 20% for MCR. The initialization error we set, represented by ω, for some models is larger than that provided by our theory. It’s worth to note that we didn’t put much effort to optimize the constant about initialization error in theory. The empirical results indicate that the practical convergence region can be much bigger than the theoretical region we proved in many settings. For a given error ω‖β∗‖2, the initial parameter β(0) is picked from sphere {u : ‖u − β∗‖2 = ω‖β∗‖2} uniformly\nat random. We ran Algorithm 1 on each model for T = 7 iterations. We set contractive factor κ = 0.7. The choice of λ (0) n follows Theorem 3.3. Parameter ∆ for each model is given in Table 1. For every single independent trial, we report the estimation error ‖β(t) − β∗‖2 in each iteration and the optimization error ‖β(t) −β(T )‖2, which is the difference between β(t) and the final output β(T ). We plot the log of errors over iteration t in Figure 2. We observe that for each of the plotted 10 independent trials, estimation error converges to certain value that is much smaller than the initialization error. Moreover, the optimization error has an approximately linear convergence as predicted by our theory."
    }, {
      "heading" : "5.2 Statistical Rate",
      "text" : "In the second set of experiments, we evaluate the relationship between final estimation error ‖β(T )− β∗‖2 and problem dimensions (n, p, s) or (n, p, θ) for the aforementioned latent variable models. The choices of algorithmic parameters, i.e., κ, ∆ and λ (0) n , and the initial parameter follow the first set of experiments in Section 5.1. Moreover, we set T = 7 and let output β̂ = β(T ). In Figure 3, we plot ‖β̂ − β∗‖2 over normalized sample complexity, i.e., n/(s log p) for s-sparse parameter and n/(θp) for rank θ p-by-p parameter. In particular, we fix s = 5 and θ = 3 for related models. We observe that the same normalized sample complexity leads to almost identical estimation error in practice, which thus supports the corresponding statistical rate established in Section 4."
    }, {
      "heading" : "6 Proof of Main Result",
      "text" : "In this section, we provide the proof of Theorem 3.3 that characterizes the computational and statistical performance of regularized EM algorithm with resampling. We first present a result which shows population EM operator M : Ω → Ω is contractive when τ < γ.\nLemma 6.1. Suppose Q(·|·) satisfies all the corresponding conditions stated in Theorem 3.3. Mapping M is contractive over B(r;β∗), namely\n‖M(β)− β∗‖ ≤ τ γ ‖β − β∗‖, ∀ β ∈ B(r;β∗).\nProof. A similar result is proved in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al. (2014) shows Lemma 6.1 with ℓ2 norm. Extending ℓ2 norm to arbitrary norm is trivial, so we omit the details.\nNow we are ready to prove Theorem 3.3.\nProof of Theorem 3.3. We first consider one iteration of Algorithm 1 and shows the relationship between ‖β(t) − β∗‖ and ‖β(t−1) − β∗‖. Recall that\nβ(t) = argmax β′∈Ω Qm(β ′|β(t−1))− λ(t)m · R(β′).\nwhere m = n/T is the number of samples in each step. We assume β(t−1) ∈ B(r;β∗). To simplify the notation, we drop the superscripts of β(t−1), λ(t)m and denote β(t) as β+. From the optimality of β+, we have\nQm(β +|β)− λm · R(β+) ≥ Qm(β∗|β)− λm · R(β∗). (6.1)\nEquivalently,\nλm · R(β+)− λm · R(β∗) ≤ Qm(β+|β)−Qm(β∗|β). (6.2)\nUsing the fact that Qm(·|β) is concave function, the right hand side of the above inequality can be bounded as\nQm(β +|β)−Qm(β∗|β) ≤ 〈 ∇Qm(β∗|β),β+ − β 〉 ≤ ∣∣〈∇Qm(β∗|β),β+ − β 〉∣∣ ︸ ︷︷ ︸\nA\n. (6.3)\nA key ingredient of our proof is to bound the term A. Let Θ := β+ − β∗, we have ∣∣〈∇Qm(β∗|β),β+ − β 〉∣∣ = ∣∣〈∇Qm(β∗|β)−∇Q(β∗|β) +∇Q(β∗|β),Θ 〉∣∣\n≤ ∣∣〈∇Qm(β∗|β)−∇Q(β∗|β),Θ 〉∣∣+ ∣∣〈∇Q(β∗|β),Θ 〉∣∣ (a)\n≤ ∥∥Qm(β∗|β)−∇Q(β∗|β)‖R∗ · R(Θ) + ∥∥∇Q(β∗|β) ∥∥ ∗ × ‖Θ‖ (b)\n≤ ∆mR(Θ) + α ∥∥∇Q(β∗|β) ∥∥× ‖Θ‖ (c)\n≤ ∆mR(Θ) + α ∥∥∇Q(β∗|β)−∇Q(M(β)|β) ∥∥ × ‖Θ‖ (d)\n≤ ∆mR(Θ) + αµ ∥∥M(β)− β∗ ∥∥× ‖Θ‖ (e) ≤ ∆mR(Θ) + αµτ\nγ\n∥∥β − β∗ ∥∥× ‖Θ‖ (6.4)\nwhere (a) follows from Cauchy-Schwarz inequality, (b) follows from the statistical error condition 5 and the definition of α, (c) follows from the fact that M(β) maximizes Q(·|β), (d) follows from the smoothness condition 2, (e) follows from Lemma 6.1. For inequality (c), note that we assume that B(r;β∗) ⊆ Ω. From Lemma 6.1, we know that if β ∈ B(r;β∗), under condition τ < γ, we must have M(β) ∈ B(rτ/γ;β∗) ⊆ B(r;β∗). Therefore M(β) lies in the interior of Ω thus the optimality condition corresponds to ∇Q(M(β)|β) = 0.\nPlugging (6.4) back into (6.3), we obtain\nQm(β +|β)−Qm(β∗|β) ≤ ∆mR(Θ) +\nαµτ\nγ\n∥∥β − β∗ ∥∥× ‖Θ‖.\nUsing the above result and (6.2), we have\nλmR(β∗ +Θ)− λmR(β∗) ≤ ∆mR(Θ) + αµτ\nγ\n∥∥β − β∗ ∥∥× ‖Θ‖. (6.5)\nTo ease notation, we use uS to denote the projection operator ΠS(u) defined in (3.4). From the decomposability of R, we have\nR(β∗ +Θ)−R(β∗) ≥ R(β∗ +ΘS⊥)−R(ΘS)−R(β ∗)\n= R(ΘS⊥)−R(ΘS⊥),\nwhere the inequality is from triangle inequality and the equality is from decomposability of R. Plugging the above result back into (6.5) yields that\nλm · ( R(ΘS⊥)−R(ΘS) ) ≤ ∆mR(Θ) + αµτ\nγ\n∥∥β − β∗ ∥∥× ‖Θ‖.\nBy assuming that λm satisfies the following condition\nλm ≥ 3∆m + αµτ γΨ(S)‖β − β ∗‖, (6.6)\nwe have that\nR(ΘS⊥)−R(ΘS) ≤ ∆m λm\nR(Θ) + αµτ ∥∥β − β∗ ∥∥ γλm ‖Θ‖ ≤ 1 3 R(Θ) + Ψ(S)‖Θ‖.\nPlugging R(Θ) ≤ R(ΘS) +R(ΘS⊥) into the above inequality, we obtain\n2R(ΘS⊥) ≤ 4R(ΘS) + 3Ψ(S) · ‖Θ‖. (6.7)\nTherefore, we have shown that Θ lies in the quasi cone C(S,S ;R) defined in (3.3). Recall that Condition 4 states that for any fixed β ∈ B(r;β∗), Qm(·|β) is strongly concave over set Ω\n⋂({β∗}+ C(S,S ;R) ) . Using this condition yields that\nQm(β ∗ +Θ|β)−Qm(β∗|β) ≤ 〈 ∇Qm(β∗|β),Θ 〉 − γm\n2 ‖Θ‖2\n≤ ∆mR(Θ) + αµτ\nγ\n∥∥β − β∗ ∥∥× ‖Θ‖ − γm\n2 ‖Θ‖2, (6.8)\nwhere the second inequality follows from (6.4).\nNow we turn back to optimality condition (6.2), following which we have\nQm(β ∗ +Θ|β)−Qm(β∗|β) ≥ λm · R(β∗ +Θ)− λm · R(β∗) ≥ −λmR(ΘS). (6.9)\nPutting (6.8) and (6.9) together gives us\nγm 2 ‖Θ‖2 ≤ λmR(ΘS) + ∆mR(Θ) + αµτ γ ‖β − β∗‖ × ‖Θ‖.\nUsing R(Θ) ≤ R(ΘS⊥) +R(ΘS) ≤ (9/2)Ψ(S)‖Θ‖, we further have\nγm 2 ‖Θ‖2 ≤ λmΨ(S)‖Θ‖ + 9 2 ∆mΨ(S)‖Θ‖+ αµτ γ ‖β − β∗‖ × ‖Θ‖.\nCanceling term ‖Θ‖ on both sides of the above inequality yields that\n‖Θ‖ ≤ 2Ψ(S)λm γm + Ψ(S) γm\n( 9∆m + 2 αµτ γΨ(S)‖β − β ∗‖ ) ≤ 5Ψ(S)λm γm . (6.10)\nThe last inequality follows from our assumption (6.6). Putting (6.6) and (6.10) together, we reach the conclusion that if β(t−1) ∈ B(r;β∗) and\nλ(t)m ≥ 3∆m + αµτ γΨ(S) ‖β(t−1) − β∗‖, (6.11)\nthen we have\n‖β(t) − β∗‖ ≤ 5Ψ(S)λ (t) m\nγm . (6.12)\nWe let κ∗ := 5αµτγγm and assume κ ∗ < 3/4. Then for any κ ∈ [κ∗, 3/4], ∆ ≥ 3∆m suppose we set\nλ(t)m = 1− κt 1− κ ∆+ κ t γm 5Ψ(S)‖β (0) − β∗‖ (6.13)\nfor all t ∈ [T ]. When t = 1, we have β(0) ∈ B(r;β∗) and one can check inequality (6.11) holds by setting t = 1 in (6.13), thereby applying (6.12) yields that\n‖β(1) − β∗‖ ≤ 5Ψ(S)λ (1) m\nγm = 5Ψ(S) γm 1− κ 1− κ∆+ κ‖β (0) − β∗‖.\nNow we prove Theorem 3.3 by induction. Assume that for some t ≥ 1,\n‖β(t) − β∗‖ ≤ 5Ψ(S) γm 1− κt 1− κ ∆+ κ t‖β(0) − β∗‖. (6.14)\nUnder condition ∆ ≤ 3∆, κ ≤ 3/4, we have\n‖β(t) − β∗‖ ≤ 15Ψ(S) γm 1− (3/4)t 1− 3/4 ∆ + (3/4) t‖β(0) − β∗‖ ≤ 15Ψ(S) γm 1− (3/4)t 1− 3/4 ∆ + (3/4) t · r\n= (1− (3/4)t) · r + (3/4)t · r = r,\nwhere the first equality is from our definition of ∆. Consequently, we have β(t) ∈ B(r;β∗). Now we check that by our choice of λ (t+1) m , inequality (6.11) holds. Note that\n3∆m + αµτ γΨ(S)‖β (t) − β∗‖ ≤ ∆+ 5αµτ γγm 1− κt 1− κ ∆+ αµτ γΨ(S)κ t‖β(0) − β∗‖\n≤ ∆+ κ1− κ t 1− κ ∆+ κ t+1 γm 5Ψ(S) ‖β(0) − β∗‖ = 1− κ t+1 1− κ ∆+ κ t+1 γm 5Ψ(S) ‖β(0) − β∗‖ = λ(t+1)m ,\nwhere the first inequality is from (6.14) and the second inequality is from the fact κ ≥ κ∗ = 5αµτγγm . Therefore (6.11) holds for t+ 1. Then applying (6.12) with t+ 1 implies that\n‖β(t+1) − β∗‖ ≤ 5Ψ(S) γm 1− κt+1 1− κ ∆+ κ t+1‖β(0) − β∗‖.\nPutting pieces together we prove that (6.14) holds for all t ∈ [T ] when Conditions 4 and 5 hold in every step. Applying probabilistic union bound, we reach the conclusion."
    }, {
      "heading" : "A Proofs about Gaussian Mixture Model",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Lemma 4.1",
      "text" : "In this example, we have\nM(β∗) = 2E [w(Y ;β∗)Y ] = 2E [\n1\n1 + exp(− 2 σ2 〈Z · β∗ +W,β∗〉) (Z · β\n∗ +W ) ] ,\nwhere W ∼ N (0, σ2) and Z has Rademacher distribution over {−1, 1}. Due to the rotation invariance of Gaussianity, without loss of generality, we assume β∗ = Ae1. It’s easy to check supp(M(β∗)) = {1}. Moreover, the first coordinate of M(β∗) takes form\n(M(β∗))1 = 2E [\n1\n1 + exp(− 2 σ2 (AZ +W1))\n(AZ +W1)\n] = A,\nwhere the last equality follows by the substitution X = W1, Z = Z, γ = 0, a = A in Lemma D.7. Therefore, M(β∗) = β∗."
    }, {
      "heading" : "A.2 Proof of Lemma 4.3",
      "text" : "Although Condition 4 is a stochastic condition, for Gaussian mixture model, particularly it is satisfied deterministically. Note that\nQGMMn (β ′|β) = − 1\n2n\nn∑\ni=1\n[ w(yi;β)‖yi − β′‖22 + (1−w(yi;β))‖yi + β′‖22 ] .\nWe have that for any β′,β ∈ Rp, ∇2QGMMn (β′|β) = −Ip, which implies that QGMMn (β′|β) is strongly concave with parameter 1. Consequently, Condition 4 holds with γn = 1."
    }, {
      "heading" : "A.3 Proof of Lemma 4.4",
      "text" : "Note that R∗ is ‖ · ‖∞ in this example. Following the specific formulations of QGMMn (·|·) and QGMM (·|·) in (2.9) and (4.2), we have\n∇QGMMn (β∗|β)−∇QGMM(β∗|β) = − 1\nn\nn∑\ni=1\nyi + 2\nn\nn∑\ni=1\nw(yi;β)yi − 2E [w(Y ;β)Y ] .\nTherefore,\n∥∥∇QGMMn (β∗|β)−∇QGMM (β∗|β) ∥∥ ∞ ≤ ∥∥∥∥∥ 1 n n∑\ni=1\nyi ∥∥∥∥∥ ∞︸ ︷︷ ︸\n(a)\n+ ∥∥∥∥∥ 2 n n∑\ni=1 w(yi;β)yi − 2E [w(Y ;β)Y ] ∥∥∥∥∥ ∞︸ ︷︷ ︸\n(b)\nNext we bound the two terms (a) and (b) respectively.\nTerm (a). Let ζ := 1n ∑n i=1 yi. Let yi = (yi,1, . . . , yi,p) ⊤ for all i ∈ [n]. Consider the j-th coordinate ζj of ζ, we have\nζj = 1\nn\nn∑\ni=1\nyi,j.\nNote that {yi,j}ni=1 are independent copies of random variable Yj that is\nYj = Z · β∗j + V, (A.1)\nwhere Z is Rademacher random variable taking values in {−1, 1} and V has distribution N (0, σ2). Since Z · β∗j and V are both sub-Gaussian random variables with norm ‖Z · β∗j ‖ψ2 ≤ |β∗j | and ‖V ‖ψ2 . δ. Following the rotation invariance sub-Gaussian random variables (e.g., Lemma 5.9 in Vershynin (2010)), we have that\n‖Yj‖ψ2 . √ ‖Z · β∗j ‖2ψ2 + ‖V ‖2ψ2 . √ ‖β∗‖2∞ + σ2.\nFollowing the standard sub-Gaussian concentration argument in Lemma D.1, there exists some constant C such that for any j ∈ [p] and all t ≥ 0,\nPr (∣∣ζj ∣∣ ≥ t ) ≤ e · exp ( − Cnt 2 ‖β∗‖2∞ + σ2 ) .\nThen by applying union bound, we have\nPr ( sup j∈[p] ∣∣ζj ∣∣ ≥ t ) ≤ pe · exp ( − Cnt 2 ‖β∗‖2∞ + σ2 ) .\nSetting the right hand side to be δ, we have that, with probability at least 1− δ/2, ∥∥∥∥∥ 1 n n∑\ni=1\nyi ∥∥∥∥∥ ∞ . (‖β∗‖∞ + δ) √ log p+ log(2e/δ) n . (A.2)\nTerm (b). Now let ζ := 2n ∑n\ni=1w(yi;β)yi − 2E [w(Y ;β)Y ]. We also consider the j-th coordinate ζj of ζ, which takes form\nζj = 2\nn\nn∑\ni=1\n{ w(yi;β)yi,j − E(w(Y ;β)Yj) } .\nNote that w(yi;β)yi,j − E(w(Y ;β)Yj), i = 1, . . . , n are independent copies of random variable w(Y ;β)Yj − E(w(Y ;β)Yj) where Yj is given in (A.1). We have shown that Yj is sub-Gaussian random variable. Note that w(Y ;β) is random variable taking values in [0, 1]. We thus always have\nPr (|w(Y ;β)Yj | ≥ t) ≤ Pr(|Yj| > t) ≤ exp(1− Ct2/‖Yj‖2ψ2).\nUsing the equivalent properties of sub-Gaussian (see Lemma 5.5 in Vershynin (2010)) , we conclude that w(Y ;β)Yj is sub-Gaussian random variable with norm ‖w(Y ;β)Yj‖ψ2 ≤ ‖Yj‖ψ2 .\n√ ‖β∗‖2∞ + σ2. Following Lemma D.3, we have ‖w(Y ;β)Yj − E [w(Y ;β)Yj ] ‖ψ2 ≤ 2‖w(Y ;β)Yj‖ψ2 . Using concentration result in Lemma D.1 yields that for any j ∈ [p] and some constant C,\nPr (|ζj | ≥ t) = Pr {∣∣∣∣ 2\nn\nn∑\ni=1\nw(yi;β)yi,j − E(w(Y ;β)Y ) ∣∣∣∣ > t } ≤ e · exp ( − Cnt 2 ‖β∗‖2∞ + σ2 ) .\nApplying union bound over p coordinates, we have\nPr ( sup j∈[p] |ζj| > t ) ≤ pe · exp ( − Cnt 2 ‖β∗‖2∞ + σ2 ) ,\nwhich implies that, with probability at least 1− δ/2, ∥∥∥∥∥ 2 n n∑\ni=1 w(yi;β)yi − 2E [w(Y ;β)Y ] ∥∥∥∥∥ ∞ . (‖β∗‖∞ + σ) √ log p+ log(2e/δ) n . (A.3)\nPutting (A.2) and (A.3) together completes the proof."
    }, {
      "heading" : "B Proofs about Mixed Linear Regression",
      "text" : ""
    }, {
      "heading" : "B.1 Proof of Lemma 4.6",
      "text" : "In this example, we have\nM(β∗) = 2E [w(Y,X;β∗)Y X] = 2E [\n1\n1 + exp(−2(〈X,Z·β∗〉+W )〈X,β∗〉σ2 ) (Z · β∗ +W )X\n] ,\nwhere X ∼ N (0, Ip),W ∼ N (0, σ2), Z has Rademacher distribution. Due to the rotation invariance of Gaussianity, without loss of generality, we can assume β∗ = Ae1. It’s easy to check supp(M(β∗)) = {1}. Moreover,\n(M(β∗))1 = 2E [\n1\n1 + exp(− 2 σ2 (AZX1 +W )AX1)\n(AZX21 +X1W ) ] = E(AX21 ) = A,\nwhere the second inequality follows by the substitution X = W,Z = Z, γ = 0, a = AX1 in Lemma D.7. We thus have M(β∗) = β∗."
    }, {
      "heading" : "B.2 Proof of Lemma 4.7",
      "text" : "Recall that we hope to find τ such that for any β ∈ B(r;β∗)\n‖∇QMLR(M(β)|β) −∇QMLR(M(β)|β∗)‖2 ≤ τ‖β − β∗‖2.\nIn this example, we have\nM(β) = 2E [w(Y,X;β)Y X] ,\nand ∇QMLR(β′|β) = 2E [w(Y,X;β)Y X]− β′. Therefore,\n∇QMLR(M(β)|β) −∇QMLR(M(β)|β∗) = 2E [w(Y,X;β)Y X]− 2E [w(Y,X;β∗)Y X] = 2E [w(Y,X;β)Y X]− β∗,\nwhere the last equality is from the self consistent property of QMLR(·|·). Due to the rotation invariance of Gaussianity, without loss of generality, we assume β∗ = Ae1,β = (1+ǫ1)Ae1+ǫ2Ae2, where A = ‖β∗‖2, ‖β − β∗‖2 = A √ ǫ21 + ǫ 2 2. Let random vector T be\nT := w(Y,X;β)Y X − 1 2 β∗.\nNote that for any β ∈ Rp,\nw(Y,X;β) = exp(− (Y−〈X,β〉)22σ2 )\nexp(− (Y−〈X,β〉)22σ2 ) + exp(− (Y+〈X,β〉)2 2σ2 ) =\n1\n1 + exp(−2Y 〈X,β〉 σ2\n) ,\nthereby\nT = 1 1 + exp(−2Y 〈X,β〉σ2 ) Y X − 1 2 β∗\n= 1\n1 + exp(−2(ZAX1+W )(A(1+ǫ1)X1+ǫ2X2) σ2\n) (ZAX1 +W )X −\n1 2 Ae1,\nwhere Z is Rademacher random variable taking values in {−1, 1}, W is stochastic noise with distribution N (0, σ2), X1 and X2 are the first two coordinates of X. It’s easy to note that E [Ti] = 0 for i = 3, . . . , p. We focus on characterizing the first two coordinates T1, T2 of T .\nCoordinate T1. First, we compute the expectation of T1. Particularly we let γ = ǫ1 + ǫ2X2/X1. Then we have\n∣∣E [T1] ∣∣ = ∣∣∣∣∣E [\nX1(W + ZAX1)\n1 + exp(−2AX1(1+γ) σ2 (W + ZAX1)) − 1 2 AX21\n]∣∣∣∣∣\n≤ E [ |X1| · ∣∣∣∣∣ (W + ZAX1)\n1 + exp(−2AX1(1+γ) σ2 (W + ZAX1)) − 1 2 AX1\n∣∣∣∣∣ ]\n= EX1,X2 { |X1| · EW,Z [∣∣∣∣∣ (W + ZAX1)\n1 + exp(−2AX1(1+γ)σ2 (W + ZAX1)) − 1 2 AX1\n∣∣∣∣∣ ]}\n≤ EX1,X2 [ |X1| ·min { 1\n2 A · |X1γ| · exp( γ2(AX1) 2 − (AX1)2 2σ2 ), σ√ 2π\n+A|X1| }] , (B.1)\nwhere the last inequality follows from Lemma D.7 by replacing the parameters (X,Z, a, γ) in the statement with (W,Z,AX1, γ). Let event E be E := {γ2 ≤ 0.9}. Computing the expectation in\n(B.1) conditioning on E and Ec yields that ∣∣E [T1] ∣∣ ≤E [ 1\n2 |γ|AX21 exp( γ2(AX1) 2 − (AX1)2 2σ2 ) ∣∣∣∣ E ] · Pr(E)\n+ E [ σ|X1|√\n2π +AX21\n∣∣∣∣ Ec ] · Pr(Ec). (B.2)\nWe bound the two terms on the right hand side of the above inequality respectively. For the first term we have\nE\n[ 1\n2 |γ|AX21 exp( γ2(AX1) 2 − (AX1)2 2σ2 ) ∣∣∣∣ E ] · Pr(E) ≤ E [ 1 2 |γ|AX21 exp( −(AX1)2 20σ2 ) ∣∣∣∣ E ] · Pr(E)\n≤ E [ 1\n2 |γ|AX21 exp( −(AX1)2 20σ2 )\n] ≤ E [ 1\n2 A ( |ǫ1| ·X21 + |ǫ2X1X2| ) exp(− 1 20 ρ2X21 )\n]\n= 1\n2 A |ǫ1| (1 + 0.1ρ2)3/2 + 1 π A |ǫ2| 1 + 0.1ρ2 ≤ 1 2 A\n1\n1 + 0.1ρ2 (|ǫ1|+ |ǫ2|), (B.3)\nwhere the third inequality is from ‖β∗‖2/σ ≥ ρ. For the second term in (B.2), first note that √\nǫ21 + ǫ 2 2 ≤ ‖β − β∗‖2 ‖β∗‖2 ≤ ω ≤ 1/4,\nthereby |γ| ≤ |ǫ1|+ |ǫ2| · |X2/X1| ≤ 1/4 + |ǫ2| · |X2/X1|. We define event E ′ := {X22/X21 ≥ (2.1ǫ22)−1}. Note that Ec = {γ2 ≥ 0.9}, we thus have Ec ⊆ E ′, i.e., the occurrence of Ec must lead to the occurrence of E ′. For the second term in (B.2), we have\nE [ σ|X1|√\n2π +AX21\n∣∣∣∣ Ec ] · Pr(Ec) ≤ E [ σ|X1|√\n2π +AX21\n∣∣∣∣ E ′ ] · Pr(E ′)\n≤ E [ σ|X1|√ 2π + √ 2.1ǫ22A|X1X2| ∣∣∣∣ E ′ ] · Pr(E ′) (B.4)\n= σ\nπ\n[ 1− √ 1\n1 + 2.1ǫ22\n] + √ 2.1ǫ22A 2\nπ 2.1ǫ22 1 + 2.1ǫ22\n≤ √ 2.1σ\nπ |ǫ2|+\n2 √ 2.1 3\nπ A|ǫ2|3, (B.5)\nwhere the equality is from Lemma D.6 by setting C in the statement to be √\n2.1ǫ22.\nPutting (B.3) and (B.4) together, we have\n|E [T1] | ≤ 1\n2 A\n1\n1 + 0.1ρ2 (|ǫ1|+ |ǫ2|) +\n√ 2.1σ\nπ |ǫ2|+\n2 √ 2.1 3\nπ A|ǫ2|3. (B.6)\nCoordinate T2. Now we turn to the second coordinate T2. Using E [X1X2] = 0, we have\n∣∣E [T2] ∣∣ = ∣∣∣∣∣E [\nX2(W + ZAX1) 1 + exp(−2AX1(1+γ)σ2 (W + ZAX1)) − 1 2 AX1X2\n]∣∣∣∣∣\n≤ E [ |X2| · ∣∣∣∣∣ (W + ZAX1)\n1 + exp(−2AX1(1+γ) σ2 (W + ZAX1)) − 1 2 AX1\n∣∣∣∣∣ ] .\nSimilar to (B.1), using Lemma D.7 leads to\n∣∣E [T2] ∣∣ ≤ E [ |X2| ·min { 1\n2 A · |X1γ| · exp( γ2(AX1) 2 − (AX1)2 2σ2 ), σ√ 2π\n+A|X1| }]\n≤ E [ 1\n2 A|γ| · |X1X2| exp( γ2(AX1) 2 − (AX1)2 2σ2 ) ∣∣∣∣ E ] · Pr(E)\n+ E [ σ|X2|√\n2π +A|X1X2|\n∣∣∣∣ Ec ] · Pr(Ec).\nWe bound the two terms in the right hand side of the above inequality respectively. For the first term, we have\nE\n[ 1\n2 A|γ| · |X1X2| exp( γ2(AX1) 2 − (AX1)2 2σ2 ) ∣∣∣∣ E ] · Pr(E)\n≤ E [ 1\n2 A|γ| · |X1X2| exp( −0.1(AX1)2 2σ2\n) ∣∣ E ] · Pr(E) ≤ E [ 1\n2 A|γ| · |X1X2| exp( −0.1(AX1)2 2σ2 )\n]\n≤ E [ 1\n2 A ( |ǫ1X1X2|+ |ǫ2|X22 ) exp(− 1 20 ρ2X21 )\n] = 1\nπ A |ǫ1| 1 + 0.1ρ2 + 1 2 A |ǫ2|√ 1 + 0.1ρ2\n(B.7)\nFor the second term, recall that event E ′ is defined as {X22/X21 ≥ (2.1ǫ22)−1}, we have\nE [ σ|X2|√\n2π +A|X1X2|\n∣∣∣∣ Ec ] · Pr(Ec) ≤ E [ σ|X2|√\n2π +A|X1X2|\n∣∣∣∣ E ′ ] · Pr(E ′)\n= σ\nπ\n√ 2.1ǫ2√\n1 + 2.1ǫ22 +\n2A\nπ 2.1ǫ22 1 + 2.1ǫ22\n≤ √ 2.1σ\nπ |ǫ2|+\n4.2A\nπ ǫ22. (B.8)\nwhere the equality follows from Lemma D.6 by setting C in the statement to be √\n2.1ǫ22. Putting\n(B.7) and (B.8) together, we have\n|E [T2] | ≤ 1\nπ A |ǫ1| 1 + 0.1ρ2 + 1 2 A |ǫ2|√ 1 + 0.1ρ2 +\n√ 2.1σ\nπ |ǫ2|+\n4.2A\nπ ǫ22. (B.9)\nNow based on (B.6) and (B.9), we conclude that\nE [‖T‖2] = E [√ T 21 + T 2 2 ] ≤ E [|T1|+ |T2|]\n≤ A 1√ 1 + 0.1ρ2\n(|ǫ1|+ |ǫ2|) + √ 2.1σ\nπ |ǫ2|+\n2 √ 2.1 3\nπ A|ǫ2|3 +\n√ 2.1σ\nπ |ǫ2|+\n4.2A\nπ ǫ22\n≤ A (\n1√ 1 + 0.1ρ2\n(|ǫ1|+ |ǫ2|) + |ǫ2|/ρ+ 1.83ω|ǫ2| )\n≤ A(|ǫ1|+ |ǫ2|) · ( 4.2\nρ + 1.83ω\n) ≤ 2A √ ǫ21 + ǫ 2 2 · ( 4.2\nρ + 1.83ω\n)\n= 2\n( 4.2\nρ + 1.83ω\n) ‖β − β∗‖2.\nNote that ∇QMLR(M(β)|β)−∇QMLR(M(β)|β∗) = 2T , thereby we conclude that for any ω ≤ 1/4, QMLR(·|·) satisfies gradient stability condition over B(ω‖β∗‖2;β∗) with parameter\nτ = 17\nρ + 7.3ω."
    }, {
      "heading" : "B.3 Proof of Lemma 4.8",
      "text" : "Recall that\nQMLRn (β ′|β) = − 1\n2n\nn∑\ni=1\n[ w(yi,xi;β)(yi − 〈xi,β′〉)2 + (1− w(yi,xi;β))(yi + 〈xi,β′〉)2 ] .\nFor any β,β′ ∈ Rp, we have\nQMLRn (β ′|β)−QMLRn (β∗|β)− 〈∇QMLRn (β∗|β),β′ − β∗〉 = −\n1 2 (β′ −β∗)⊤\n( 1\nn\nn∑\ni=1\nxix ⊤ i ) (β′ −β∗).\n(B.10)\nNote that we want to find γn such that the right hand side of (B.10) is less than −γn2 ‖β′−β‖22 for any β′−β∗ ∈ C(S,S;R). In this example, we have C(S,S;R) = {u ∈ Rp : ‖uS⊥‖1 ≤ 2‖uS‖1 + 2 √ s‖u‖2}. It’s sufficient to prove that the sample covariance matrix has restricted eigenvalues over set C(S,S;R). The following statement is follows by the substitution Σ = Ip and X = X in Lemma D.5: there exist constants {Ci}2i=0 such that\n1 n\nn∑\ni=1\n〈xi,u〉2 ≥ 1\n2 ‖u‖22 − C0\nlog p\nn ‖u‖21, for all u ∈ Rp, (B.11)\nwith probability at least 1− C1 exp(−C2n). For any u ∈ C(S,S;R), we have\n‖u‖1 = ‖uS‖1 + ‖uS⊥‖1 ≤ 3‖uS‖1 + 2 √ s‖u‖2 ≤ 5 √ s‖u‖2.\nApplying (B.11) yields that\n1 n\nn∑\ni=1\n〈xi,u〉2 ≥ 1\n2 ‖u‖22 − 25C0\ns log p\nn ‖u‖22, for all u ∈ C(S,S;R).\nConsequently, when n ≥ C3s log p for sufficiently large C3, 1n ∑n\ni=1〈xi,u〉2 ≥ 1/3‖u‖22, which implies γn = 1/3."
    }, {
      "heading" : "B.4 Proof of Lemma 4.9",
      "text" : "According to the formulations of QMLRn (·|·) and QMLR(·|·) in (2.12) and (4.5), we have\n∇QMLRn (β∗|β)−∇QMLR(β∗|β)\n= β∗ − ( 1\nn\nn∑\ni=1\nxix ⊤ i ) β∗ + 2\nn\nn∑\ni=1\nw(yi,xi;β)yixi − 2E [w(Y,X;β)Y X]− 1\nn\nn∑\ni=1\nyixi. (B.12)\nSo\n‖∇QMLRn (β∗|β)−∇QMLR(β∗|β)‖∞\n≤ ∥∥∥∥∥ 1 n n∑\ni=1\nyixi ∥∥∥∥∥ ∞︸ ︷︷ ︸\n(a)\n+ ∥∥∥∥∥β ∗ − ( 1 n n∑\ni=1\nxix ⊤ i ) β∗ ∥∥∥∥∥ ∞︸ ︷︷ ︸\n(b)\n+ ∥∥∥∥∥ 2 n n∑\ni=1 w(yi,xi;β)yixi − 2E [w(Y,X;β)Y X] ∥∥∥∥∥ ∞︸ ︷︷ ︸\n(c)\n.\nNext we bound the above three terms (a), (b) and (c) respectively. Term (a). We let vector ζ := 1n ∑n i=1 yixi. Consider jth coordinate of ζ. For any j ∈ [p], we have\nζj = 1\nn\nn∑\ni=1\nyixi,j,\nwhere xi,j is the jth coordinate of xi. Note that {yixij}ni=1 are independent copies of random variables (〈X,Z ·β∗〉+W )Xj where X ∼ N (0, Ip), W ∼ N (0, σ2) and Z has Rademacher distribution. 〈X,Z ·β∗〉+W is sub-Gaussian random variable that has norm ‖〈X,Z ·β∗〉+W‖ψ2 . √ ‖β∗‖22 + σ2. Also Xj is sub-Gaussian random variable that has norm ‖Xj‖ψ2 . 1. Then based on Lemma D.4, (〈X,Z · β∗〉+W )Xj is sub-exponential with norm ‖(〈X,Z · β∗〉+W )Xj‖ψ1 . √ ‖β∗‖22 + σ2. Following standard concentration result of sub-exponential random variables (e.g., Lemma D.2), there exists some constant C such that the following inequality\nPr (|ζj| ≥ t) ≤ 2 exp ( −C t 2n ‖β∗‖22 + σ2 )\nholds for sufficiently small t > 0. Therefore,\nPr ( sup j∈[p] |ζj | > t ) ≤ 2p exp ( −C t 2n ‖β∗‖22 + σ2 ) .\nSetting the right hand side to be δ/3, we have that, when n is sufficiently large (i.e., n ≥ C(log p+ log(6/δ)) for some constant C), with probability at least 1− δ/3.\n∥∥∥∥∥ 1 n n∑\ni=1\nyixi ∥∥∥∥∥ ∞ . (‖β∗‖2 + σ) √ log p+ log(6/δ) n . (B.13)\nTerm (b). Now we let ζ = β∗ − 1nxixiβ∗. For any j ∈ [p],\nζj = 1\nn\nn∑\ni=1\nβ∗j − xi,j〈xi,β∗〉.\nNote that {β∗j − xi,j〈xi,β∗〉}ni=1 are independent copies of random variable β∗j −Xj〈X,β∗〉. Using similar analysis in bounding term (a), we claim that β∗j − Xj〈X,β∗〉 is centered sub-exponential random variable with norm ‖β∗j − Xj〈X,β∗〉‖ψ1 . ‖β∗‖2. Therefore, for sufficiently small t and some constant C,\nPr (|ζj | ≥ t) ≤ 2 exp ( −C t 2n\n‖β∗‖22\n) .\nUsing union bound implies that\nPr ( sup j∈[p] |ζj| ≥ t ) ≤ 2p · exp ( −C t 2n ‖β∗‖22 ) .\nSetting the right hand side to be δ/3, we have that, when n is sufficiently large, ∥∥∥∥∥β ∗ − ( 1 n n∑\ni=1\nxix ⊤ i ) β∗ ∥∥∥∥∥ ∞ . ‖β∗‖2 √ log p+ log(6/δ) n (B.14)\nholds with probability at least 1− δ/3. Term (c). The analysis of this term is similar to the previous two terms. We let\nζ := 1\nn\nn∑\ni=1\nw(yi,xi;β)yixi − E [w(Y,X;β)Y X] .\nFor any j ∈ [p],\nζj = 1\nn\nn∑\ni=1\nw(yi,xi;β)yixi,j − E [w(Y,X;β)Y X] .\nNote that {w(yi,xi;β)yixi,j}ni=1 are independent copies of random variable w(Y,X;β)Y Xj . We know that Y is sub-Gaussian with norm ‖Y ‖ψ2 . √ ‖β∗‖22 + σ2. Since w(Y,X;β) is bounded, w(Y,X;β)Y is also sub-Gaussian. Consequently, w(Y,X;β)Y Xj is sub-exponential. By standard concentration result, for some constant C and sufficiently small t,\nPr(|ζj| ≥ t) ≤ 2 exp ( −C nt 2 ‖β∗‖22 + σ2 ) .\nTherefore,\nPr(sup j∈[p]\n|ζj| ≥ t) ≤ 2 exp ( −C nt 2 ‖β∗‖22 + σ2 ) .\nSetting the right hand side to be δ/3, we have that, when n is sufficiently large, ∥∥∥∥∥ 2 n n∑\ni=1 w(yi,xi;β)yixi − 2E [w(Y,X;β)Y X] ∥∥∥∥∥ ∞ . (‖β∗‖2 + δ) √ log p+ log(6/δ) n (B.15)\nwith probability at least 1− δ/3. Putting (B.13), (B.14) and (B.15) together completes the proof."
    }, {
      "heading" : "B.5 Proof of Lemma 4.11",
      "text" : "Similar to (B.10), we have that for any Γ′,Γ ∈ Rp1×p2 ,\nQMLRn (Γ ′|Γ)−QMLRn (Γ∗|Γ)− 〈∇QMLRn (Γ∗|Γ),Γ′ − Γ∗〉 = −\n1\n2n\nn∑\ni=1\n〈Xi,Γ′ − Γ∗〉2. (B.16)\nNote that Γ′ − Γ∗ ∈ C(S,S ; ‖ · ‖∗). Let Θ := Γ′ − Γ∗, we thus have ‖ΘS⊥‖∗ ≤ 2 · ‖ΘS‖∗ + 2 · √ 2θ‖Θ‖F .\nWe make use of the following result.\nLemma B.1. Let {Xi}ni=1 be n independent samples of random matrix X ∈ Rp1×p2 where the entries are i.i.d. Gaussian random variable with distribution N (0, 1). There exits constants C1, C2 such that\n1√ n\n√√√√ n∑\ni=1\n〈Xi,Θ〉2 ≥ 1\n4 ‖Θ‖F − 12 (√ p1 n + √ p2 n ) ‖Θ‖∗, for all Θ ∈ Rp1×p2 ,\nwith probability at least 1− C1 exp(−C2n).\nProof. See Proposition 1 in Negahban et al. (2011) for detailed proof.\nThen for our Θ, using the above result yields that\n1√ n\n√√√√ n∑\ni=1\n〈Xi,Θ〉2 ≥ 1\n4 ‖Θ‖F − 12 (√ p1 n + √ p2 n )( ‖ΘS‖∗ + ‖ΘS⊥‖∗ )\n≥ 1 4 ‖Θ‖F − 12 (√ p1 n + √ p2 n )( 3‖ΘS‖∗ + 2 √ 2r‖Θ‖F ) ≥ [ 1\n4 − 60\n√ 2θ (√ p1 n + √ p2 n )] ‖Θ‖F .\nSo when n ≥ Cθmax{p1, p2} for sufficient large C, we have 1√n √∑n i=1〈Xi,Θ〉2 ≥ ‖Θ‖F / √ 20. Plugging this result back into (B.16) gives us γn = 1/20 thus completes the proof."
    }, {
      "heading" : "B.6 Proof of Lemma 4.12",
      "text" : "Parallel to (B.12), we have\n∇QMLRn (Γ∗|Γ)−∇QMLR(Γ∗|Γ)\n= Γ∗ − 1 n\nn∑\ni=1\n〈Xi,Γ∗〉Γ∗ + 2\nn\nn∑\ni=1\nw(yi,Xi;Γ)yiXi − 2E [w(Y,X;Γ)Y X]− 1\nn\nn∑\ni=1\nyiXi.\nThe dual norm of nuclear norm is spectral norm. So we are interested in bounding the following term for fixed Γ:\n∥∥∇QMLRn (Γ∗|Γ)−∇QMLR(Γ∗|Γ) ∥∥ 2\n≤ ∥∥∥∥∥ 1 n n∑\ni=1\nyiXi ∥∥∥∥∥ 2︸ ︷︷ ︸\nU1\n+ ∥∥∥∥∥Γ ∗ − 1 n n∑\ni=1 〈Xi,Γ∗〉Xi ∥∥∥∥∥ 2︸ ︷︷ ︸\nU2\n+ ∥∥∥∥∥ 2 n n∑\ni=1 w(yi,Xi;Γ)yiXi − 2E [w(Y,X;Γ)Y X] ∥∥∥∥∥ 2︸ ︷︷ ︸\nU3\n.\nNext we bound the three terms U1, U2 and U3 respectively. Term U1. We first note that\nU1 = sup u ∈ Sp1−1 v ∈ Sp2−1\n1 n\nn∑\ni=1\nyi〈uv⊤,Xi〉.\nIn particular, we let\nZ(a, b) = sup u ∈ aSp1−1 v ∈ bSp2−1\n1 n\nn∑\ni=1\nyi〈uv⊤,Xi〉.\nWe thus have Z(a, b) = abZ(1, 1). We construct 1/4-covering sets of Sp1−1 and Sp2−1, which we denote as N1 and N2 respectively. Therefore, for any u ∈ Sp−1,v ∈ Sp2−1, we can always find u′ ∈ N1,v′ ∈ N2 such that ‖u − u′‖2 ≤ 1/4, ‖v − v′‖2 ≤ 1/4. Moreover, we have the following decomposition uv⊤ = u′v′⊤ + (u− u′)v′⊤ + u′(v − v′)⊤ + (u− u′)(v − v′)⊤. Therefore, we have\nZ(1, 1) ≤ max u∈N1,v∈N2\n1 n\nn∑\ni=1\nyi〈uv⊤,Xi〉+ Z(1/4, 1) + Z(1/4, 1) + Z(1/4, 1/4),\nwhich implies that\nZ(1, 1) ≤ 16 7 max u∈N1,v∈N2 1 n\nn∑\ni=1\nyi〈uv⊤,Xi〉.\nFor any fixed u and v, {yi〈uv⊤,Xi〉}ni=1 are n independent copies of random variable Y 〈uv⊤,X〉 where Y is sub-Gaussian with norm ‖Y ‖ψ2 . √ ‖Γ∗‖2F + σ2, 〈uv⊤,X〉 is zero mean Gaussian with variance 1. Following Lemma D.4, Y 〈uv⊤,X〉 is sub-exponential with norm ‖Y 〈uv⊤,X〉‖ψ1 .√ ‖Γ∗‖2F + σ2. Using concentration result in Lemma D.2, we have\nPr (∣∣∣∣∣ 1 n n∑\ni=1\nyi〈uv⊤,Xi〉 ∣∣∣∣∣ ≥ t ) ≤ 2 exp ( − Ct 2n ‖Γ∗‖2F + σ2 )\nfor sufficiently small t > 0. Note that |N1| ≤ 9p1 , |N2| ≤ 9p2 . By applying union bounds over N1 and N2, we have\nPr ( max\nu∈N1,v∈N2\n1 n\nn∑\ni=1\nyi〈uv⊤,Xi〉 ≥ t ) ≤ 2 · 9(p1+p2) exp ( − Ct 2n ‖Γ∗‖2F + σ2 ) .\nBy setting the right hand side to be δ/3, we have that if n ≥ C(p1 + p2 + log(6/δ)) for sufficiently large C, then\nU1 . (‖Γ∗‖F + σ) √ p1 + p2 + log(6/δ)\nn (B.17)\nwith probability at least 1− δ/3. Term U2. Parallel to the analysis of term U1, we have\nU2 = sup u ∈ Sp1−1 v ∈ Sp2−1\n〈uv⊤,Γ∗〉 − 1 n\nn∑\ni=1\n〈Xi,Γ∗〉 · 〈uv⊤,Xi〉.\nWe construct 1/4-nets N1,N2 of Sp1−1 and Sp2−1 respectively. Then\nU2 ≤ 16\n7 max u∈N1,v∈N2 〈uv⊤,Γ∗〉 − 1 n\nn∑\ni=1\n〈Xi,Γ∗〉 · 〈uv⊤,Xi〉.\nFor any fixed u,v, note that {〈Xi,Γ∗〉·〈uv⊤,Xi〉}ni=1 are n independent samples of random variable 〈X,Γ∗〉 · 〈uv⊤,X〉 where 〈X,Γ∗〉 ∼ N (0, ‖Γ∗‖2F ) and 〈uv⊤,X〉 ∼ N (0, 1). So 〈X,Γ∗〉 · 〈uv⊤,X〉 is sub-exponential with norm O(‖Γ∗‖F ). Using the centering argument (Lemma D.3) and concentration result (Lemma D.2), we have\nPr (∣∣∣∣∣〈uv ⊤,Γ∗〉 − 1 n n∑\ni=1\n〈Xi,Γ∗〉 · 〈uv⊤,Xi〉 ∣∣∣∣∣ ≥ t ) ≤ 2 · exp ( −C t 2n\n‖Γ∗‖2F\n)\nfor sufficiently small t. Using the union bound over sets N1,N2, we conclude that when n ≥ C(p1 + p2 + log(6/δ)) for sufficiently large C, we have\nU2 . ‖Γ∗‖F √ p1 + p2 + log(6/δ)\nn (B.18)\nwith probability at least 1− δ/3. Term U3. We first have\nU3 = sup u∈Sp1−1 v∈Sp2−1\n2 n\nn∑\ni=1\nw · yi〈uv⊤,Xi〉 − 2E [ w · Y 〈uv⊤,X〉 ] .\nSimilar to the analysis of the first two terms, by constructing N1,N2, we have\nU3 ≤ 16\n7 max u∈N1,v∈N2\n2 n\nn∑\ni=1\nw · yi〈uv⊤,Xi〉 − 2E [ w · Y 〈uv⊤,X〉 ] .\nNote that {wyi〈uv⊤,Xi〉}ni=1 are n independent samples of random variable wY 〈uv⊤,X〉 where 〈uv⊤,X〉 ∼ N (0, 1) and wY is sub-Gaussian with norm ‖wY ‖ψ2 . √ ‖Γ∗‖2F + σ2 since |w| ≤ 1. We thus have wY 〈uv⊤,X〉 is sub-exponential with norm ‖wY 〈uv⊤,X〉‖ψ1 . √\n‖Γ∗‖2F + σ2. Then following the similar steps in analyzing the first two terms, we reach the conclusion that\nU3 . (‖Γ∗‖F + σ) √ p1 + p2 + log(6/δ)\nn (B.19)\nwith probability at least 1− δ/3 when n & p1 + p2 + log(6/δ). Putting (B.17), (B.18) and (B.19) together completes the proof."
    }, {
      "heading" : "C Proofs about Missing Covariate Regression",
      "text" : "In this section, we provide the proofs for missing covariate regression model. We begin with a result that states several properties of the conditional correlation matrix, which play important roles in proving curvature conditions. Recall that, given samples (yi, zi,xi) and β, Σβ(yi, zi,xi) is given in (2.16). We let Z ∈ Rp be random vector with i.i.d. binary entries such that Pr(Z1 = 1) = ǫ. Define the population level correlation covariance matrix as\nΣβ := E [Σβ(Y,Z,X)] .\nLemma C.1. For Σβ, we have the following decomposition\nΣβ = ǫIp +Σ1 −Σ2,\nwhere\nΣ1 = E { [(1− Z)⊙X + νZ ⊙ β] · [(1− Z)⊙X + νZ ⊙ β]⊤ } ,\nΣ2 = E\n[ 1\nσ2 + ‖Z ⊙ β‖22 (Z ⊙ β)(Z ⊙ β)⊤\n] , ν =\nY − 〈β, (1− Z)⊙X〉 σ2 + ‖Z ⊙ β‖22 .\nLet ζ := (1 + ω)ρ, we have\nλmin(Σ1) ≥ 1− ǫ− 2ζ2 √ ǫ, (C.1) λmax(Σ2) ≤ ζ2ǫ, (C.2) λmax(Σβ) ≤ 1 + 2ζ2 √ ǫ+ (1 + ζ2)ζ2ǫ. (C.3)\nIn particular, let β = β∗, we have Σβ∗ = Ip.\nProof. The decomposition follows by taking expectation of (2.16). For Σ1, expanding the bracket leads to Σ1 = (1−ǫ)Ip+E { ν[(1− Z)⊙X](Z ⊙ β)⊤ + ν(Z ⊙ β)[(1 − Z)⊙X]⊤ }\n︸ ︷︷ ︸ M\n+E [ ν2(Z ⊙ β)(Z ⊙ β)⊤ ]\n︸ ︷︷ ︸ N\n.\nFor term M, consider its spectral norm. Since it’s symmetric, we have\n‖M‖2 = sup u∈Sp−1 2 |E [ν〈Z ⊙ β,u〉 · 〈(1− Z)⊙X,u〉]|\n= 2 sup u∈Sp−1\n∣∣∣∣E [\n1\nσ2 + ‖Z ⊙ β‖2 〈(1− Z)⊙ (β∗ − β),u〉 · 〈Z ⊙ β,u〉\n]∣∣∣∣\n≤ 2 1 σ2 E [‖(1− Z)⊙ (β∗ − β)‖2‖Z ⊙ β‖2] ≤ 2 1 σ2\n√ E [ ‖(1− Z)⊙ (β∗ − β)‖22 · ‖Z ⊙ β‖22 ]\n≤ 2 1 σ2\n√ ǫ(1− ǫ)‖β − β∗‖2‖β‖2 ≤ 2ρ2ω(1 + ω) √ ǫ(1− ǫ) ≤ 2ζ2√ǫ.\nwhere the second equality follows by taking expectation of X and Gaussian noise W , the last inequality follows from the definitions of ω, ρ given in Section 4.3. Note that N 0. Then the lower bound of λmin(Σ1) follows by using λmin(Σ1) ≥ 1− ǫ− ‖M‖2. For Σ2, we have\nΣ2 = E\n[ 1\nσ2 + ‖Z ⊙ β‖22 (Z ⊙ β)(Z ⊙ β)⊤\n] 1\nσ2\n( (ǫ− ǫ2)diag(β ⊙ β) + ǫ2ββ⊤ ) .\nTherefore, λmax(Σ2) ≤ ζ2ǫ. Note that\nN 1 σ4 E\n[ (Y − 〈β, (1 − Z)⊙X〉)2(Z ⊙ β)(Z ⊙ β)⊤ ]\n= 1\nσ4 E\n[ (σ2 + ‖β∗ − (1− Z)⊙ β‖22)(Z ⊙ β)(Z ⊙ β)⊤ ]\n1 σ4\n(σ2 + ‖β∗‖22 + ‖β − β∗‖22) ( (ǫ− ǫ2)diag(β ⊙ β) + ǫ2ββ⊤ ) .\nWe thus have λmax(N) ≤ 1σ4 (σ2 + ‖β∗‖22 + ‖β − β∗‖22)ǫ‖β‖22 ≤ (1 + ζ2)ζ2ǫ. The corresponding bound for λmax(Σβ) then follows from λmax(Σβ) ≤ 1 + λmax(M) + λmax(N).\nWhen β = β∗, we have\nEX,W (ν 2) =\nEX,W\n[ (〈X,β∗〉+W − 〈X, (1 − Z)⊙ β∗〉)2 ]\n(σ2 + ‖Z ⊙ β∗‖22)2 =\n1\nσ2 + ‖Z ⊙ β∗‖22 and\nEX,W (ν(1− Z)⊙X) = E [(〈X,β∗〉+W − 〈X, (1 − Z)⊙ β∗〉)(1− Z)⊙X]\nσ2 + ‖Z ⊙ β∗‖22 =\n(1− Z)⊙ Z ⊙ β∗ σ2 + ‖Z ⊙ β∗‖22 = 0.\nTherefore, M = 0 and N = Σ2. We thus have Σβ∗ = ǫIp + (1− ǫ)Ip = Ip."
    }, {
      "heading" : "C.1 Proof of Lemma 4.14",
      "text" : "In this example M(β∗) = (E [Σβ∗(Y,Z,X)])−1 E [Y µβ∗(Y,Z,X)] . Following Lemma C.1, we have Σβ∗(Y,Z,X) = Ip. Meanwhile, we have\nE [Y µβ∗(Y,Z,X)] = E\n[ (〈β∗,X〉 +W ) ( (1− Z)⊙X + 〈Z ⊙ β\n∗,X〉 +W σ2 + ‖Z ⊙ β∗‖22\nZ ⊙ β∗ )]\n= E [(1− Z)⊙ β∗ + Z ⊙ β∗] = β∗.\nThus M(β∗) = β∗."
    }, {
      "heading" : "C.2 Proof of Lemma 4.15",
      "text" : "Following Lemma C.1, we have Σβ∗ = Ip. Therefore, Q MCR(·|β∗) is 1-strongly concave. For any β ∈ B(w‖β∗‖;β∗), following (C.3), we have that QMCR(·|β) is µ-smooth with µ = 1 + 2ζ2√ǫ + (1 + ζ2)ζ2ǫ."
    }, {
      "heading" : "C.3 Proof of Lemma 4.17",
      "text" : "In order to show QMCRn (·|β) is γn-strongly concave over C(S,S ;R), since QMCRn (·|β) is quadratic, it’s then equivalent to show\n1 n\nn∑\ni=1\nu⊤Σβ(yi, zi,xi)u ≥ γn‖u‖22\nfor all u ∈ C(S,S,R). Expanding Σβ gives us\n1 n\nn∑\ni=1\nu⊤Σβ(yi, zi,xi)u ≥ 1\nn\nn∑\ni=1\n〈µβ(yi, zi,xi),u〉2\n︸ ︷︷ ︸ L1\n− 1 n\nn∑\ni=1\n( 1\nσ2 + ‖zi ⊙ β‖22\n) 〈zi ⊙ β,u〉2\n︸ ︷︷ ︸ L2\n.\nWe choose to bound each term using restricted eigenvalue argument in Lemma D.5. To ease notation, we let ν := yi−〈(1−zi)⊙β,xi〉 σ2+‖zi⊙β‖22 . Term L1. Note that µβ(yi, zi,xi) are samples of µβ(Y,Z,X) which is zero mean sub-Gaussian random vector with covariance matrix Σ1 given in Lemma C.1. Moreover, we have λmin(Σ1) ≥ 1 − ǫ − 2ζ2√ǫ. By restricting ǫ ≤ 1/4 and assuming ǫ ≤ Cζ−4 for sufficiently small C, we have λmin(Σ1) ≥ 12 . Moreover\n‖µβ(Y,Z,X)‖ψ2 . ‖(1 − Z)⊙X‖ψ2 + ‖νZ ⊙ β‖ψ2 . 1 + ‖νZ ⊙ β‖ψ2 .\nNote that ‖νZ⊙β‖ψ2 = supu∈Sp−1 ‖ν〈Z⊙β,u〉‖ψ2 ≤ ‖β‖2 · ∥∥|ν| ∥∥ ψ2 ≤ σ−2‖β‖2 · ∥∥|W+〈X,β∗−(1−\nZ)⊙ β〉| ∥∥ ψ2\n. (1 + ω)ρ+ (1 + ω)2ρ2. As ζ := (1 + ω)ρ. We thus have ‖µβ(Y,Z,X)‖ψ2 . (1 + ζ)2. Using Lemma D.5 with the substitution Σ = Σ1 and X = µβ(Y,Z,X), we claim that there exist constants Ci such that\nL1 ≥ 1\n4 ‖u‖22 − C0(1 + ζ)8\nlog p\nn ‖u‖21 for all u ∈ Rp. (C.4)\nwith probability at least 1− C1 exp(−C2n(1 + ζ)−8). Term L2. We now turn to term L2. We introduce n i.i.d. samples {pi}ni=1 of Rademacher random variable P with Pr(P = 1) = Pr(P = −1) = 1/2. Equivalently, we have\nL2 = 1\nn\nn∑\ni=1\n1\nσ2 + ‖zi ⊙ β‖22 〈pizi ⊙ β,u〉2.\nNote that √\n(σ2 + ‖Z ⊙ β‖22)−1PZ ⊙ β is zero mean sub-Gaussian random vector with covariance matrix Σ2 given in Lemma C.1. Moreover, we have λmax(Σ2) ≤ ζ2ǫ ≤ 1/12, where the last inequality follows by letting ǫ ≤ Cζ−2 for sufficiently small C. Also note that\n∥∥∥∥ √ (σ2 + ‖Z ⊙ β‖22)−1PZ ⊙ β ∥∥∥∥ ψ2 . σ−1‖Z ⊙ β‖ψ2 . ζ.\nUsing Lemma D.5 with substitution Σ = Σ2 and X = √\n(σ2 + ‖Z ⊙ β‖22)−1PZ⊙β, we claim there exists constants C ′i such that\nL2 ≤ 1 8 ‖u‖22 + C ′0max{ζ4, 1} log p n ‖u‖21, for all u ∈ Rp. (C.5)\nwith probability at least 1− C ′1 exp(−C ′2nmin{ζ−4, 1}). Now we put (C.4) and (C.5) together. So we obtain\n1 n\nn∑\ni=1\nu⊤Σβ(yi, zi,xi)u ≥ 1 8 ‖u‖22 − (C0 + C ′0)(1 + ζ)8 log p n ‖u‖21.\nFor any u ∈ C(S,S;R), we have ‖u‖1 ≤ 5 √ s‖u‖2. Consequently, when n ≥ C(1 + ζ)8s log p for sufficiently large C, we have that, with high probability, QMCRn (·|β) is γn-strongly concave over C with γn = 1/9."
    }, {
      "heading" : "C.4 Proof of Lemma 4.18",
      "text" : "In this example,\n‖∇QMCRn (β∗|β)−∇QMCR(β∗|β)‖R∗\n≤ ∥∥∥∥∥ 1 n n∑\ni=1 yiµβ(yi, zi,xi)− E [Y µβ(Y,Z,X)] ∥∥∥∥∥ ∞︸ ︷︷ ︸\nU1\n+ ∥∥∥∥∥ 1 n n∑\ni=1\nΣβ(yi, zi,xi)β ∗ − E [Σβ(Y,Z,X)]β∗ ∥∥∥∥∥ ∞︸ ︷︷ ︸\nU2\n.\nTo ease notation, we let ν := yi−〈(1−zi)⊙β,xi〉 σ2+‖zi⊙β‖22 . Next we bound the term U1 and U2 respectively. Term U1. Consider one coordinate of vector V := Y µβ(Y,Z,X). For any j ∈ [p], we have\nVj = Y [(1− Zj)Xj + νZjβj ].\nSo Vj is sub-exponential random variable since Y and (1−Zj)Xj + νZjβj are both sub-Gaussians. Moreover, we have ‖Y ‖ψ2 . σ+‖β∗‖2 and ‖(1−Zj)Xj+νZjβj‖ψ2 . ‖(1−Zj)Xj‖ψ2+‖νZjβj‖ψ2 . 1 + σ−2(σ + √ 1 + ω2‖β∗‖2)‖β‖2. The last inequality follows from the fact that ν is sub-Gaussian\nwith ‖ν‖ψ2 . σ−2(σ+ √ 1 + ω2‖β∗‖2). We have ‖Vi‖ψ1 . ‖Y ‖ψ2 ·‖(1−Zj)Xj+νZjβj‖ψ2 . (1+ζ)3σ, where ζ := (1+ω)ρ. By concentration result of sub-exponentials (Lemma D.2) and applying union bound, we have that there exists constant C such that for t . (1 + ζ)3σ,\nPr(U1 ≥ t) ≤ pe · exp(− Cnt2\n(1 + ζ)6σ2 ).\nSetting the right hand side to be δ/2 implies that for n & log p+ log(2e/δ),\nU1 . (1 + ζ) 3σ\n√ log p+ log(2e/δ)\nn (C.6)\nwith probability at least 1− δ/2. Term U2. Term U2 can be further decomposed into several terms as follows\nU2 ≤ ‖a1‖∞ + ‖a2‖∞ + ‖a3‖∞ + ‖a4‖∞ + σ−2‖a5‖∞ + ‖a6‖∞,\nwhere\na1 = 1\nn\nn∑\ni=1\n〈 (1− zi)⊙ xi,β∗ 〉 (1− zi)⊙ xi − E [〈 (1− Z)⊙X,β∗ 〉 (1− Z)⊙X ] ,\na2 = 1\nn\nn∑\ni=1\n〈 νzi ⊙ β,β∗ 〉 (1− zi)⊙ xi − E [〈 νZ ⊙ β,β∗ 〉 (1− Z)⊙X ] ,\na3 = 1\nn\nn∑\ni=1\n〈 (1− zi)⊙ xi,β∗ 〉 νzi ⊙ β − E [〈 (1− Z)⊙X,β∗ 〉 νZ ⊙ β ] ,\na4 = 1\nn\nn∑\ni=1\nν2〈zi ⊙ β,β∗〉zi ⊙ β − E [ ν2〈Z ⊙ β,β∗〉Z ⊙ β ] ,\na5 = 1\nn\nn∑\ni=1\n〈 zi ⊙ β,β∗ 〉 zi ⊙ β − E [〈 Z ⊙ β,β∗ 〉 Z ⊙ β ] , a6 = 1\nn\nn∑\ni=1\ndiag(zi)β ∗ − ǫβ∗.\nThe key idea to bound the infinite norm of each term ai is the same: showing that each coordinate is finite summation of independent sub-Gaussian (or sub-exponential) random variables and applying concentration result and probabilistic union bound. For each term ai, i = 1, 2, . . . , 6, we have that for any j ∈ [p],\n‖ 〈 (1− Z)⊙X,β∗ 〉 (1− Zj)⊙Xj‖ψ1 . ‖β∗‖2, ‖ 〈 νZ ⊙ β,β∗ 〉 (1− Zj)⊙Xj‖ψ1 . σ(1 + ζ)ζ2, ‖ 〈 (1− Z)⊙X,β∗ 〉 νZjβj‖ψ1 . σ(1 + ζ)ζ2, ‖ν2〈Z ⊙ β,β∗〉Zjβj‖ψ1 . σ(1 + ζ2)ζ3, σ−2‖ 〈 Z ⊙ β,β∗ 〉 Zj ⊙ βj‖ψ2 . σζ3, ‖ǫβ∗j ‖ψ2 . ǫ‖β∗‖∞\nrespectively. For simplicity, we treat coordinates of every ai as finite sum of sub-exponentials with ψ1 norm O(σ(1 + ζ) 5). Consequently, by concentration result in Lemma D.2, there exists constant C such that\nPr(U2 ≥ t) ≤ 12p · exp ( − Cnt 2\nσ2(1 + ζ)10\n)\nfor t . σ(1 + ζ)5. By setting the right hand side to be δ/2 in the above inequality, we have that when n & log p+ log(24/δ),\nU2 . σ(1 + ζ) 5\n√ log p+ log(24/δ)\nn . (C.7)\nwith probability at least 1− δ/2. Finally, putting (C.6) and (C.7) together completes the proof."
    }, {
      "heading" : "D Supporting Lemmas",
      "text" : "Lemma D.1. Suppose X1,X2, . . . ,Xn are n i.i.d. centered sub-Gaussian random variables with Orlicz norm ‖X1‖ψ2 ≤ K. Then for every t ≥ 0, we have\nPr (∣∣∣∣ 1\nn\nn∑\ni=1\nXi ∣∣∣∣ ≥ t ) ≤ e · exp ( −Cnt 2\nK2\n) ,\nwhere C is an absolute constant.\nProof. See the proof of Proposition 5.10 in Vershynin (2010).\nLemma D.2. Suppose X1,X2, . . . ,Xn are n i.i.d. centered sub-exponential random variables with Orlicz norm ‖X1‖ψ1 ≤ K. Then for every t > 0, we have\nPr (∣∣∣∣ 1\nn\nn∑\ni=1\nXi ∣∣∣∣ ≥ t ) ≤ 2 · exp ( −Cmin { t2 K2 , t K } n ) ,\nwhere C is an absolute constant.\nProof. See the proof of Corollary 5.7 in Vershynin (2010).\nLemma D.3. Let X be sub-Gaussian random variable and Y be sub-exponential random variable. Then X − E[X] is also sub-Gaussian; Y − E[Y ] is also sub-exponential. Moreover, we have\n‖X − E[X]‖ψ2 ≤ 2‖X‖ψ2 , ‖Y − E[Y ]‖ψ1 ≤ 2‖Y ‖ψ1 .\nProof. See Remark 5.18 in Vershynin (2010).\nLemma D.4. Let X,Y be two sub-Gaussian random variables. Then Z = X ·Y is sub-exponential random variable. Moreover, there exits constant C such that\n‖Z‖ψ1 ≤ C‖X‖ψ2 · ‖Y ‖ψ2 .\nProof. It follows from the basic properties. We omit the details.\nLemma D.5. Let matrix X be an n-by-p random matrix with i.i.d. rows drawn from X, which is zero mean sub-Gaussian random vector with ‖X‖ψ2 ≤ K and covariance matrix Σ. We let λ1 := λmin(Σ), λp := λmax(Σ). (1) There exist constants Ci such that\n1 n ‖Xu‖22 ≥ λ1 2 ‖u‖22 − C0λ1 max\n{ K4\nλ21 , 1\n} log p\nn ‖u‖21, for all u ∈ Rp,\nwith probability at least 1− C1 exp ( −C2nmin { λ2 1 K4 , 1 }) . (2) In Parallel, there exist constants C ′i such that\n1 n ‖Xu‖22 ≤ 3λp 2\n‖u‖22 + C ′0λpmax { K4\nλ2p , 1\n} log p\nn ‖u‖21, for all u ∈ Rp,\nwith probability at least 1− C ′1 exp ( −C ′2nmin { λ2p K4 , 1 }) .\nProof. It follows by putting Lemma 12 and Lemma 15 in Loh and Wainwright (2011) together.\nLemma D.6. Let X1 and X2 be independent random variables with distribution N (0, 1). For any positive constant C > 0, let event E := {C · |X2| ≥ |X1|}. Then we have (a)\nE [ |X1| ∣∣ E ] · Pr(E) =\n√ 2\nπ\n[ 1− √ 1\nC2 + 1\n] .\n(b)\nE [ |X2| ∣∣ E ] · Pr(E) =\n√ 2\nπ C√ 1 + C2 .\n(c)\nE [ |X1X2| ∣∣ E ] · Pr(E) = 2C 2\nπ(1 + C2) .\nProof. (a)\nE [ |X1| ∣∣ E ] · Pr(E) = 4 ·\n∫ ∞\n0\n∫ uC\n0\n1 2π exp(−1 2 v2) exp(−u\n2\n2 )vdvdu =\n√ 2\nπ\n[ 1− √ 1\nC2 + 1\n] .\n(b)\nE [ |X2| ∣∣ E ] · Pr(E) = 4 ·\n∫ ∞\n0\n∫ ∞\nv/C\n1 2π exp(−1 2 v2) exp(−u\n2\n2 )ududv =\n√ 2\nπ C√ 1 + C2 .\n(c)\nE [ |X1X2| ∣∣ E ] · Pr(E) = 4 ·\n∫ ∞\n0\n∫ ∞\nv/C\n1 2π exp(−u\n2\n2 ) exp(−v\n2\n2 )uvdudv\n= 2\nπ\n∫ ∞\n0 exp(−C\n2 + 1\n2 v2)vdv =\n2C2\nπ(1 + C2) .\nLemma D.7. Let X ∼ N (0, σ2) and Z be Rademacher random variable taking values in {−1, 1}. Moreover, X and Z are independent. Function f(x, z; a, γ) is defined as\nf(x, z; a, γ) = x+ az\n1 + exp(−2(1+γ) σ2\na(x+ az)) .\nThen for any a ∈ R, γ ∈ R, we have ∣∣∣E [f(X,Z; a, γ)] − a\n2\n∣∣∣ ≤ min { 1\n2 |aγ| exp(γ 2a2 − a2 2σ2 ), σ√ 2π\n+ |a| } .\nIn the special case γ = 0, we have E [f(X,Z; a, γ)] = a/2.\nProof. First note that\nE [f(X,Z; a, γ)] = 1\n2 E\n[ X + a\n1 + exp(−2(1+γ) σ2\na(X + a)) + X − a 1 + exp(−2(1+γ)\nσ2 a(X − a))\n]\n= 1\n2 E\n[ X + a\n1 + exp(−2(1+γ) σ2\na(X + a)) + −X − a 1 + exp(−2(1+γ)\nσ2 a(−X − a))\n] ,\nwhere the first equality is from taking expectation of Z, the second equality is from the fact that the distribution of X is symmetric around 0. Let X ′ = X + a, then we have\nE [f(X,Z; a, γ)] = 1\n2 E\n[ X ′\n1 + exp(−2(1+γ) σ2\naX ′) +\n−X ′\n1 + exp(2(1+γ) σ2 aX ′)\n]\n= 1\n2 E\n[ X ′ − 2 exp(− 2(1+γ) σ2 aX ′)X ′\n1 + exp(−2(1+γ) σ2 aX ′)\n] .\nUsing E [X ′] = a, we have E [f(X,Z; a, γ)] − a/2 = E [ − exp(− 2(1+γ) σ2 aX ′)X ′\n1 + exp(−2(1+γ)σ2 aX ′)\n]\n=\n∫ ∞\n−∞\nexp(− (x−a)2 2σ2\n)√ 2πσ\n− exp(−2(1+γ) σ2 ax)x 1 + exp(−2(1+γ)σ2 ax) dx =\n∫ ∞\n−∞\nexp(−x2+a2 2σ2\n)x√ 2πσ\n− exp(−γax σ2 )\nexp(a(1+γ)xσ2 ) + exp( −a(1+γ)x σ2 ) dx\n=\n∫ ∞\n0\nexp(−x2+a2 2σ2\n)x√ 2πσ\nexp(γax σ2 )− exp(−γax σ2 )\nexp(a(1+γ)xσ2 ) + exp( −a(1+γ)x σ2 ) dx (D.1)\nWhen aγ ≥ 0, we have E [f(X,Z; a, γ)] − a/2 ≥ 0. Under this setting, (D.1) yields that\nE [f(X,Z; a, γ)] − a/2 ≤ ∫ ∞\n0\nexp(−x2+a2 2σ2 )x\n2 √ 2πσ\n[ exp( γax\nσ2 )− exp(−γax σ2 ) ] dx\n= 1\n2 exp( γ2a2 − a2 2σ2 )\n∫ ∞\n0\n1√ 2πσ\n[ exp ( −(x− γa) 2\n2σ2\n) − exp ( −(x+ γa) 2\n2σ2\n)] xdx\n= 1\n2 exp( γ2a2 − a2 2σ2 )\n∫ ∞\n−∞ 1√ 2πσ exp\n( −(x− γa) 2\n2σ2\n) xdx = 1\n2 exp( γ2a2 − a2 2σ2 )γa,\nwhere the first inequality follows from the fact that x+1/x ≥ 2 for any x > 0, the second equality is from\n− ∫ ∞\n0 exp\n( −(x+ γa) 2\n2σ2\n) xdx = ∫ 0\n−∞ exp\n( −(x− γa) 2\n2σ2\n) xdx.\nWhen aγ ≤ 0, using similar proof, we have 12 exp( γ2a2−a2 2σ2 )γa ≤ E [f(X,Z; a, γ)] − a/2 ≤ 0. Combining the two cases, we prove that\n∣∣E [f(X,Z; a, γ)] − a/2 ∣∣ ≤ 1\n2 |aγ| exp(γ 2a2 − a2 2σ2 ). (D.2)\nIn the special case when γ = 0, we thus have E(f(X,Z; a, γ)) = a/2.\nNote that when aγ ≥ 0, (D.1) also implies that\nE [f(X,Z; a, γ)]− a/2 ≤ ∫ ∞\n0\nexp(−x2+a2 2σ2\n)x√ 2πσ\nexp(γax σ2 )\nexp(a(1+γ)xσ2 ) dx =\n∫ ∞\n0\nexp(− (x+a)2 2σ2\n)x√ 2πσ dx\n=\n∫ ∞\n0\nexp(− (x+a)2 2σ2\n)(x+ a)√ 2πσ\ndx− ∫ ∞\n0\nexp(− (x+a)2 2σ2\n)a√ 2πσ dx ≤ σ√ 2π + |a|.\nSimilarly, when aγ ≤ 0, we have\nE [f(X,Z; a, γ)] − a/2 ≥ ∫ ∞\n0\nexp(−x2+a2 2σ2\n)x√ 2πσ\n− exp(−γax σ2 )\nexp(−a(1+γ)x σ2\n) dx = −\n∫ ∞\n0\nexp(− (x−a)2 2σ2\n)x√ 2πσ dx\n= − ∫ ∞\n0\nexp(− (x−a)2 2σ2\n)(x− a)√ 2πσ\ndx− ∫ ∞\n0\nexp(− (x−a)2 2σ2\n)a√ 2πσ dx ≥ − σ√ 2π − |a|.\nTherefore, we have that ∣∣E [f(X,Z; a, γ)] − a/2 ∣∣ ≤ σ√\n2π + |a|. (D.3)\nPutting (D.2) and (D.3) together completes the proof."
    } ],
    "references" : [ {
      "title" : "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
      "author" : [ "Sivaraman Balakrishnan", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "arXiv preprint arXiv:1408.2156,",
      "citeRegEx" : "Balakrishnan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Balakrishnan et al\\.",
      "year" : 2014
    }, {
      "title" : "Rop: Matrix recovery via rank-one projections",
      "author" : [ "T Tony Cai", "Anru Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Cai and Zhang.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cai and Zhang.",
      "year" : 2015
    }, {
      "title" : "A constrained 1 minimization approach to sparse precision matrix estimation",
      "author" : [ "Tony Cai", "Weidong Liu", "Xi Luo" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Cai et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2011
    }, {
      "title" : "The Dantzig selector: statistical estimation when p is much larger than n",
      "author" : [ "Emmanuel Candes", "Terence Tao" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Candes and Tao.,? \\Q2007\\E",
      "shortCiteRegEx" : "Candes and Tao.",
      "year" : 2007
    }, {
      "title" : "Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements",
      "author" : [ "Emmanuel J Candès", "Yaniv Plan" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candès and Plan.,? \\Q2011\\E",
      "shortCiteRegEx" : "Candès and Plan.",
      "year" : 2011
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "Spectral experts for estimating mixtures of linear regressions",
      "author" : [ "Arun Tejasvi Chaganty", "Percy Liang" ],
      "venue" : "arXiv preprint arXiv:1306.3729,",
      "citeRegEx" : "Chaganty and Liang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chaganty and Liang.",
      "year" : 2013
    }, {
      "title" : "Improved graph clustering",
      "author" : [ "Yudong Chen", "Sujay Sanghavi", "Huan Xu" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "A convex formulation for mixed regression with two components: Minimax optimal rates",
      "author" : [ "Yudong Chen", "Xinyang Yi", "Constantine Caramanis" ],
      "venue" : "In Conf. on Learning Theory,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Exact and stable covariance estimation from quadratic sampling via convex programming",
      "author" : [ "Yuxin Chen", "Yuejie Chi", "Andrea Goldsmith" ],
      "venue" : "arXiv preprint arXiv:1310.0807,",
      "citeRegEx" : "Chen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society",
      "author" : [ "Arthur P Dempster", "Nan M Laird", "Donald B Rubin" ],
      "venue" : "Series B (methodological),",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Jain et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2013
    }, {
      "title" : "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity",
      "author" : [ "Po-Ling Loh", "Martin J Wainwright" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Loh and Wainwright.,? \\Q2011\\E",
      "shortCiteRegEx" : "Loh and Wainwright.",
      "year" : 2011
    }, {
      "title" : "Corrupted and missing predictors: Minimax bounds for high-dimensional linear regression",
      "author" : [ "Po-Ling Loh", "Martin J Wainwright" ],
      "venue" : "In Information Theory Proceedings (ISIT),",
      "citeRegEx" : "Loh and Wainwright.,? \\Q2012\\E",
      "shortCiteRegEx" : "Loh and Wainwright.",
      "year" : 2012
    }, {
      "title" : "Asymptotic convergence properties of the em algorithm with respect to the overlap in the mixture",
      "author" : [ "Jinwen Ma", "Lei Xu" ],
      "venue" : null,
      "citeRegEx" : "Ma and Xu.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ma and Xu.",
      "year" : 2005
    }, {
      "title" : "The EM algorithm and extensions, volume 382",
      "author" : [ "Geoffrey McLachlan", "Thriyambakam Krishnan" ],
      "venue" : null,
      "citeRegEx" : "McLachlan and Krishnan.,? \\Q2007\\E",
      "shortCiteRegEx" : "McLachlan and Krishnan.",
      "year" : 2007
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Sahand Negahban", "Bin Yu", "Martin J Wainwright", "Pradeep K Ravikumar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Negahban et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Negahban et al\\.",
      "year" : 2009
    }, {
      "title" : "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
      "author" : [ "Sahand Negahban", "Martin J Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Negahban and Wainwright,? \\Q2011\\E",
      "shortCiteRegEx" : "Negahban and Wainwright",
      "year" : 2011
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "Recht et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2010
    }, {
      "title" : "L1-penalization for mixture regression models",
      "author" : [ "Nicolas Städler", "Peter Bühlmann", "Sara Van De Geer" ],
      "venue" : null,
      "citeRegEx" : "Städler et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Städler et al\\.",
      "year" : 2010
    }, {
      "title" : "An analysis of the em algorithm and entropy-like proximal point methods",
      "author" : [ "Paul Tseng" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Tseng.,? \\Q2004\\E",
      "shortCiteRegEx" : "Tseng.",
      "year" : 2004
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "Roman Vershynin" ],
      "venue" : "arXiv preprint arXiv:1011.3027,",
      "citeRegEx" : "Vershynin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vershynin.",
      "year" : 2010
    }, {
      "title" : "Structured regularizers for high-dimensional problems: Statistical and computational issues",
      "author" : [ "Martin J Wainwright" ],
      "venue" : "Annual Review of Statistics and Its Application,",
      "citeRegEx" : "Wainwright.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wainwright.",
      "year" : 2014
    }, {
      "title" : "High dimensional expectationmaximization algorithm: Statistical optimization and asymptotic normality",
      "author" : [ "Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu" ],
      "venue" : "arXiv preprint arXiv:1412.8729,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "On the convergence properties of the em algorithm",
      "author" : [ "C.F.Jeff Wu" ],
      "venue" : "The Annals of statistics,",
      "citeRegEx" : "Wu.,? \\Q1983\\E",
      "shortCiteRegEx" : "Wu.",
      "year" : 1983
    }, {
      "title" : "Alternating minimization for mixed linear regression",
      "author" : [ "Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi" ],
      "venue" : "arXiv preprint arXiv:1310.3745,",
      "citeRegEx" : "Yi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art high-dimensional prescriptions (e.g., à la Wainwright (2014)) is not guaranteed to provide this balance.",
      "startOffset" : 18,
      "endOffset" : 674
    }, {
      "referenceID" : 22,
      "context" : ", Wainwright (2014)) the regularizer should be chosen proportional to the target estimation error.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement.",
      "startOffset" : 2,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.",
      "startOffset" : 2,
      "endOffset" : 335
    }, {
      "referenceID" : 9,
      "context" : ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.",
      "startOffset" : 2,
      "endOffset" : 349
    }, {
      "referenceID" : 9,
      "context" : ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.",
      "startOffset" : 2,
      "endOffset" : 380
    }, {
      "referenceID" : 0,
      "context" : "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.",
      "startOffset" : 20,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems – low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step.",
      "startOffset" : 20,
      "endOffset" : 519
    }, {
      "referenceID" : 0,
      "context" : "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems – low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow “close” to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step.",
      "startOffset" : 20,
      "endOffset" : 960
    }, {
      "referenceID" : 0,
      "context" : "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems – low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow “close” to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization.",
      "startOffset" : 20,
      "endOffset" : 1208
    }, {
      "referenceID" : 0,
      "context" : "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems – low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow “close” to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of β∗), as well as the statistical error. Finally, we note that for finite mixture regression, Städler et al.Städler et al. (2010) consider an l1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality.",
      "startOffset" : 20,
      "endOffset" : 1579
    }, {
      "referenceID" : 0,
      "context" : "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems – low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow “close” to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of β∗), as well as the statistical error. Finally, we note that for finite mixture regression, Städler et al.Städler et al. (2010) consider an l1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality. However, this work doesn’t establish the theoretical properties of local optima arising from regularized EM. Our work addresses this issue from a local convergence perspective by using a novel choice of regularization. Notation: Let u = (u1, u2, . . . , up) ⊤ ∈ Rp be a vector and M = [Mi,j ] ∈ Rp1×p2 be a matrix. The lq norm of u is defined as ‖u‖p = ( ∑p i=1 |ui|). We use ‖M‖∗ to denote the nuclear norm of M and ‖M‖2 to denote its spectral norm. We use ⊙ to denote the Hadamard product between two vectors, i.e., u ⊙ v = (u1v1, u2v2, . . . , upvp)⊤. A p-by-p identity matrix is denoted as Ip. We use capital letter (e.g., X) to denote random variable, vector and matrix. For a sub-Gaussian (subexponential) random variable X, we use ‖X‖ψ2 (‖X‖ψ1) to denote its Orlicz norm (see Vershynin (2010) for detailed definitions).",
      "startOffset" : 20,
      "endOffset" : 2490
    }, {
      "referenceID" : 22,
      "context" : "To provide some intuitions of such choice, we first note that from theory of high dimensional regularized M-estimator Wainwright (2014), suitable λn should be proportional to the target estimation error.",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "Inspired by the low-dimensional analysis of EM in Balakrishnan et al. (2014), we expect the optimization error to decay geometrically, so we choose κ ∈ (0, 1).",
      "startOffset" : 50,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "Mixed linear regression (MLR), as considered in some recent work (Chaganty and Liang, 2013; Yi et al., 2013; Chen et al., 2014b), is the problem of recovering two or more linear vectors from mixed linear measurements.",
      "startOffset" : 65,
      "endOffset" : 128
    }, {
      "referenceID" : 25,
      "context" : "Mixed linear regression (MLR), as considered in some recent work (Chaganty and Liang, 2013; Yi et al., 2013; Chen et al., 2014b), is the problem of recovering two or more linear vectors from mixed linear measurements.",
      "startOffset" : 65,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : ",Candès and Recht (2009); Recht et al.",
      "startOffset" : 1,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : ",Candès and Recht (2009); Recht et al. (2010); Candès and Plan (2011); Negahban et al.",
      "startOffset" : 1,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "(2010); Candès and Plan (2011); Negahban et al.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "(2010); Candès and Plan (2011); Negahban et al. (2011); Jain et al.",
      "startOffset" : 8,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "(2010); Candès and Plan (2011); Negahban et al. (2011); Jain et al. (2013); Chen et al.",
      "startOffset" : 8,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "(2010); Candès and Plan (2011); Negahban et al. (2011); Jain et al. (2013); Chen et al. (2013); Cai and Zhang (2015)).",
      "startOffset" : 8,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "(2013); Cai and Zhang (2015)).",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : ", Candes and Tao (2007); Negahban et al.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al.",
      "startOffset" : 2,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al.",
      "startOffset" : 2,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al. (2014a)), has been shown to be useful, both empirically and theoretically, for high dimensional structural estimation.",
      "startOffset" : 2,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "2 Conditions on Q(·|·) Next, we review three technical conditions, originally proposed by Balakrishnan et al. (2014), about population level Q(·|·) function.",
      "startOffset" : 90,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "2 Conditions on Q(·|·) Next, we review three technical conditions, originally proposed by Balakrishnan et al. (2014), about population level Q(·|·) function. Recall that Ω ⊆ Rp is the basin of attraction. It is well known that performance of EM algorithm is sensitive to initialization. Analyzing Algorithm 1 with any initial point is not desirable in this paper. Our theory is developed with focus on a r-neighbor region round β∗ that is defined as B(r;β∗) := { u ∈ Ω, ‖u− β∗‖ ≤ r } . We first assume that Q(·|β∗) is self consistent as stated below. Condition 1. (Self Consistency) Function Q(·|β∗) is self consistent, namely β∗ = argmax β∈Ω Q(β|β∗). It is usually assumed that β∗ maximizes the population log likelihood function. Under this condition, Condition 1 is always satisfied by following the classical theory of EM algorithmMcLachlan and Krishnan (2007). Basically, we require Q(·|β) is differentiable over Ω for any β ∈ Ω.",
      "startOffset" : 90,
      "endOffset" : 865
    }, {
      "referenceID" : 16,
      "context" : "It’s instructive to compare Condition 4 with a related condition proposed by Negahban et al. (2009) for analyzing high dimensional M-estimator.",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "We note that, in Balakrishnan et al. (2014) and Wang et al.",
      "startOffset" : 17,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of ‖Mn(β)−M(β)‖2 and ‖Mn(β)−M(β)‖∞ respectively.",
      "startOffset" : 17,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of ‖Mn(β)−M(β)‖2 and ‖Mn(β)−M(β)‖∞ respectively. As mentioned earlier, in high dimensional setting, Mn(β) is not well defined in some models such as mixed linear regression. For mixed linear regression, Wang et al. (2014) resolves this issue by invoking a high dimensional inverse covariance matrix estimation algorithm proposed by Cai et al.",
      "startOffset" : 17,
      "endOffset" : 337
    }, {
      "referenceID" : 0,
      "context" : "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of ‖Mn(β)−M(β)‖2 and ‖Mn(β)−M(β)‖∞ respectively. As mentioned earlier, in high dimensional setting, Mn(β) is not well defined in some models such as mixed linear regression. For mixed linear regression, Wang et al. (2014) resolves this issue by invoking a high dimensional inverse covariance matrix estimation algorithm proposed by Cai et al. (2011). Our formulation (3.",
      "startOffset" : 17,
      "endOffset" : 465
    }, {
      "referenceID" : 14,
      "context" : "Note that the work in Ma and Xu (2005) provides empirical and theoretical evidences that in low SNR regime, where the overlap density of two Gaussian cluster is small, standard EM algorithm suffers from sublinear convergence asymptotically.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "See the proof of Lemma 3 in Balakrishnan et al. (2014). Now we turn to the conditions about QGMM n (·|·).",
      "startOffset" : 28,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "The work in Chen et al. (2014b) shows that there exists an unavoidable phase transition of statistical rate from high SNR to low SNR.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "In Balakrishnan et al. (2014), it is proved that when r = 1 32‖β‖2, there exists τ ∈ [0, 1/2] such that QMLR(·|·) satisfies Condition 3 with parameter τ when ρ is sufficiently large.",
      "startOffset" : 3,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "in low dimensional analysis Balakrishnan et al. (2014), arises from the fundamental limits of EM algorithm.",
      "startOffset" : 28,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "in low dimensional analysis Balakrishnan et al. (2014), arises from the fundamental limits of EM algorithm. It’s worth to note that Chen et al. (2014b) establish near-optimal low dimensional estimation error that does not depend on ‖β∗‖2 based on a convex optimization approach.",
      "startOffset" : 28,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "The established statistical rate matches (up to the logarithmic factor) the (single) low rank matrix estimation rate proved in Candès and Plan (2011); Negahban et al.",
      "startOffset" : 127,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "The established statistical rate matches (up to the logarithmic factor) the (single) low rank matrix estimation rate proved in Candès and Plan (2011); Negahban et al. (2011), which is known to be minimax optimal.",
      "startOffset" : 127,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "We revisit the following result about the gradient stability from Balakrishnan et al. (2014). Lemma 4.",
      "startOffset" : 66,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "See the proof of Corollary 6 in Balakrishnan et al. (2014).",
      "startOffset" : 32,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "This unusual constraint is in fact unavoidable, as pointed out in Loh and Wainwright (2012). We now turn to validate the conditions about finite sample function QMCR n (·|·).",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "A similar result is proved in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al.",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "A similar result is proved in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al. (2014) shows Lemma 6.",
      "startOffset" : 30,
      "endOffset" : 115
    } ],
    "year" : 2015,
    "abstractText" : "Latent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art high-dimensional prescriptions (e.g., à la Wainwright (2014)) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.",
    "creator" : "LaTeX with hyperref package"
  }
}