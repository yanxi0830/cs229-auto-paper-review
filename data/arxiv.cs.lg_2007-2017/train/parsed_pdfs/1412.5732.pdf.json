{
  "name" : "1412.5732.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MORES: Online Incremental Multiple-Output Regression for Data Streams",
    "authors" : [ "Changsheng Li", "Weishan Dong", "Qingshan Liu", "Xin Zhang" ],
    "emails" : [ "zxin}@cn.ibm.com", "qsliu@nuist.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Online multiple-output regression, dynamic relationship learning, forgetting factor, lossless compression\nI. INTRODUCTION\nData streams arise in many scenarios, such as online transactions in the financial market, Internet traffic and so on [1]. Unlike traditional datasets in batch mode, a data stream should be viewed as a potentially infinite process collecting data with varying update rates, as well as continuously evolving over time. In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc., have been extensively studied over the last decade, little attention is paid to multiple-output regression. However, multiple-output regression also has a great variety of potential applications on data streams, including weather forecast, air quality prediction, etc.\nIn batch data processing, the purpose of multiple-output regression is to learn a mapping Φ from an input space Rd to an output space Rm on the whole training dataset. Based on the learned Φ, we can simultaneously predict multiple output variables y ∈ Rm to a given new input vector x ∈ Rd by: y = Φ(x). According to the property of\nC. Li, W. Dong, and X. Zhang are with IBM Research-China, Beijing 100093. E-mail: {lcsheng, dongweis, zxin}@cn.ibm.com\nQ. Liu is with the B-DAT Laboratory at the school of Information and Control, Nanjing University of Information Science and Technology, Nanjing 210014, China. E-mail: qsliu@nuist.edu.cn\nthe learned mapping Φ, multiple-output regression techniques can be divided into linear and nonlinear ones. Since linear methods usually have low complexities and reliable prediction performance, they have attracted more attention in the past. Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24]. However, in streaming environments, the data is not stored in a batch mode, but it arrives sequentially and continuously. If using these batch methods to re-train the models for streaming data, the computational complexity and memory complexity will increase sharply. Moreover, when the size of the arrived data becomes large, it is also impractical to load all the historical data into memory for model training. Therefore, it is necessary to develop an online regression algorithm for simultaneously predicting multiple outputs.\nSo far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27]. The representative method is online passive-aggressive (PA) algorithm [26]. PA is a margin based online learning algorithm, and it has an analytical solution to update model parameters as the new data sample(s) arrives. Since there are often correlations among outputs, mining the correlation relationships can improve the prediction accuracy of the model [19]. However, PA only treats each of multiple outputs as an independent task, and thus can not capture any correlations among outputs. Recently, McWilliams and Montana take advantage of partial least squares (PLS) to build a recursive regression model for online predicting multiple outputs, called iS-PLS [28]. iSPLS aims at finding a low-dimensional subspace to make the correlation between inputs and outputs maximized. iS-PLS focuses on the correlation between inputs and outputs, while it does not consider the correlations among outputs.\nIn this paper, we propose a novel Multiple-Output REgression method for Streaming data, named as MORES. MORES works in an incremental fashion. It aims at dynamically learning the structures of both the regression coefficients and the residual errors to continuously update the model. Specifically, when a new training sample arrives, we transform the update of the regression coefficients into an optimization problem. In the objective function, we highlight the following three aspects:\nFirst, we take advantage of the matrix Mahalanobis norm to measure the divergence between updated regression coefficient matrix and current regression coefficient matrix, which can learn the structure of the regression coefficients to facilitate model’s update.\nSecond, due to limited expressive power of the regression coefficient matrix, especially in the early stage of online\nar X\niv :1\n41 2.\n57 32\nv1 [\ncs .L\nG ]\n1 8\nD ec\n2 01\n4\n2 update, there are often correlations between the residual errors. MORES thus utilizes the Mahalanobis distance instead of the Euclidean distance to measure the prediction error between the true values and the predicted values, which can learn the structure of the residual errors to improve the prediction accuracy.\nThird, we define three statistical variables to store necessary information of both the historical data and the current data for exactly measuring the prediction error, such that MORES can avoid loading all the data into memory and visiting data multiple times. This also effectively prevents the updated regression coefficient matrix gradually deviating from the true coefficient matrix due to noise and outliers in the data streams. Meanwhile, we introduce a forgetting factor to set different weights on samples for adapting to data streams’ evolvement.\nExtensive experiments are conducted on three real-world datasets, and the experimental results demonstrate the effectiveness and efficiency of MORES."
    }, {
      "heading" : "II. ONLINE MULTIPLE-OUTPUT REGRESSION FOR STREAMING DATA",
      "text" : "Following the general setting of online learning [26], we assume that the learner first observes an instance xt ∈ Rd on the t-th round, and it simultaneously predicts multiple outputs ŷt ∈ Rm based on the current model Pt−1 ∈ Rm×d. After that, the learner receives the true responses yt ∈ Rm for this instance. Finally, the learner updates the current model Pt−1 based on the new data point (xt,yt). In this paper, our goal is to online update Pt−1, such that the updated Pt can predict the outputs for the incoming instance xt+1 as accurately as possible. The prediction can be expressed in the following form:\nyt+1 = Ptxt+1 + t+1, (1)\nwhere Pt = [pt,1, . . . ,pt,m]T denotes the learned regression coefficient matrix on the t-th round, and pt,i is the regression coefficient vector of the i-th output. t+1 = [ t+1,1, . . . , t+1,m]\nT is a vector consisting of m residual errors."
    }, {
      "heading" : "A. Objective Function",
      "text" : "In order to obtain Pt on the t-th round, we first propose a simple formulation as:\nPt = arg min P∈Rm×d ‖P−Pt−1‖2F s.t. ‖yt −Pxt‖22 ≤ ξ,\n(2)\nwhere ξ is a positive parameter that controls the sensitivity to the prediction error. ‖ ·‖F denotes the matrix Frobenius norm, and ‖ · ‖2 denotes the l2 norm of a vector.\nThe core idea of objective function (2) is as follows: On one hand, it intends to minimize the distance between Pt and Pt−1 to make Pt close to Pt−1 as much as possible, which can retain the information learned on previous rounds. On the other hand, it requires Pt to meet the condition: The total prediction error on the current data point (xt,yt) is less than or equal to ξ.\nFollowing [26], the optimization problem defined by (2) can be easily solved by the Lagrange multiplier method.\nAlthough (2) has some merits for online regression prediction of streaming data, it still has the following limitations:\n(i) Because of ‖P − Pt−1‖2F = tr((P − Pt−1)(P − Pt−1) T ) = ∑m i=1((pi − pt−1,i)T (pi − pt−1,i)), we can see that the objective function in (2) treats the update of regression coefficients as m independent tasks. However, in streaming environments, outputs are often dependent, i.e., there are some positive or negative correlations among outputs, so updating regression coefficient vectors for all the outputs should not be regarded as completely independent tasks.\n(ii) In order to acquire the updater Pt, (2) imposes a constraint on P, i.e., ‖yt − Pxt‖22 ≤ ξ. This constraint just refers to the current data point (xt,yt) but ignores the historical data points St−1 = {(xi,yi)}t−1i=1 . This may lead to the updated coefficient matrix gradually deviating from the true coefficient matrix because of noise and outliers in many practical streaming data applications.\n(iii) The constraint of (2) takes advantage of the l2 norm to measure the total prediction error. As we know, the l2 norm of a vector assumes that all the variables in the vector are independent. However, due to limited expressive power of Pt, especially when the round t is small, there are often correlations between the residual errors. Therefore, the l2 norm is often the suboptimal choice for measuring the total prediction error.\nIn light of above three limitations, we propose to minimize the following objective function, in order to update the model on round t:\nmin J(P,Ω,Γ) =‖P−Pt−1‖2Ω + α`(P,Γ;St) + β∆φ(Ω,Ωt−1)− η log |Γ| (3)\ns.t. Ω 0, Γ 0,\nwhere α, β, and η are three trade-off parameters. ‖ · ‖Ω denotes the matrix Mahalanobis norm. `(P,Γ;St) is the total prediction error on the data points St = {(xi,yi)}ti=1. ∆φ(Ω,Ωt−1) denotes the Bregman divergence [29] that measures the distance between the matrix Ω and the matrix Ωt−1. Ω 0 and Γ 0 means that they are positive semi-definite.\nIn the objective function (3), the first term aims to learn the structure Ω of the regression coefficient matrix, and leverage Ω to measure the divergence between the updated matrix P and the current matrix Pt−1 on round t. The second term intends to mine the underlying structure Γ of the residual errors, and take advantage of Γ to measure the total prediction error on the current data and the historical data. Here we define three statistical variables to store necessary information of data for lowering memory complexity, and introduce a forgetting factor to adapt to the evolving data streams (See (5), (6) for details). The third term is a regularization term, which aims at keeping Ω updated in a conservative strategy to reduce the influence of noise. The last term is used to control the complexity of Γ. Next, we will respectively explain the first three terms in detail. The first term: In order to capture the structure of the regression coefficient matrix, we introduce Mahalanobis norm\n3 of the matrix (P−Pt−1) to measure the divergence between P and Pt−1. The Mahalanobis norm is expressed as:\n‖P−Pt−1‖Ω = √ tr((P−Pt−1)TΩ(P−Pt−1))\n= √∑d i=1 (P(i)−Pt−1(i))TΩ(P(i)−Pt−1(i)), (4)\nwhere P(i) denotes the i-th column of P. When Ω is set to the identity matrix, the Mahalanobis norm of the matrix is reduced to the Frobenius norm. In (4), the term (P(i) − Pt−1(i))TΩ(P(i) − Pt−1(i)) is actually the Mahalanobis distance between P(i) and Pt−1(i), where Ω encodes the correlations between the variables of the i-th column of the regression coefficient matrix on round t [30]. Therefore, ‖P−Pt−1‖2Ω can be viewed as a summation of d Mahalanobis distances, of which each measures the distance between the corresponding column vectors of P and Pt−1. The second term: The loss function `(P,Γ;S) measures the prediction error on S , which is defined as:\n`(P,Γ;S) = ∑t\ni=1 µt−i(yi −Pxi)TΓ(yi −Pxi), (5)\nwhere 0 ≤ µ ≤ 1 is a forgetting factor1. When µ = 0, the prediction loss is only measured on the current sample without any historical samples. When µ = 1, all the samples have equal weights to contribute to the prediction loss. When 0 < µ < 1, all the samples have different contributions to the prediction loss. As a matter of fact, the function of µ is similar to a new form of time window on samples. The farther the historical sample is from the current sample in the time domain, the lower its importance is, which will fit in the evolving characteristic of data streams well. The matrix Γ embeds the correlation relationships among the residual errors on round t. The term (yi − Pxi)TΓ(yi − Pxi) measures the Mahalnobis distance between the true value yi and the predicted value Pxi, which can remove the influence of the residual errors’ correlations on distance calculation [31].\nIn streaming environments, it is impractical to load all the historical data into memory or scan a sample multiple times. An effective way to handle this issue is to define some statistical variables to store necessary information of the samples. In this paper, we introduce three statistical variables to realize lossless compression of the data. To do this, we will make use of the following property and lemma.\nProperty 1: Given a set of arbitrary sequence vectors, x1,. . .,xt, and a constant µ, if Ct = ∑t i=1µ t−ixix T i , then Ct=µCt−1+xtx T t , where t is the timestamp.\nLemma 1: The loss function (5) can be expressed as\n` = tr(ΓCt,YY) + tr(P TΓPCt,XX)− 2tr(ΓPCt,XY).\n(6)\nwhere Ct,YY, Ct,XY, and Ct,XX are three variables, which are respectively defined as:\nCt,YY = µCt−1,YY + yty T t . (7) Ct,XY = µCt−1,XY + xty T t . (8) Ct,XX = µCt−1,XX + xtx T t . (9)\n1When µ = 0, and t− i = 0, we define µt−i = 1 to ensure consistency.\nProof: Based on (5), we have `(P,Γ;S) = ∑t\ni=1 µt−i(yi −Pxi)TΓ(yi −Pxi) = ∑t\ni=1 µt−i(tr(yTi Γyi) + tr(x T i P TΓPxi)) − 2 ∑t\ni=1 µt−itr(yTi ΓPxi) = ∑t\ni=1 µt−i(tr(Γyiy T i ) + tr(P TΓPxix T i )) − 2 ∑t\ni=1 µt−itr(ΓPxiy T i ) =tr(Γ ∑t\ni=1 µt−iyiy T i ) + tr(P\nTΓP ∑t\ni=1 µt−ixix T i ) − 2tr(ΓP ∑t\ni=1 µt−ixiy T i ). (10)\nDefining the matrix variables Ct,YY,Ct,XY, and Ct,XX as:\nCt,YY = ∑t\ni=1 µt−iyiy T i (11) Ct,XY = ∑t\ni=1 µt−ixiy T i (12) Ct,XX = ∑t\ni=1 µt−ixix T i (13)\nSubstituting Ct,YY,Ct,XY, and Ct,XX into (10), the loss function (5) becomes (6). Based on the Property 1, (11), (12), and (13) can be expressed by (7), (8), and (9), respectively.\nWhen a new data point (xt+1,yt+1) arrives on round t+1, we first update the statistical variables Ct+1,YY, Ct+1,XY, Ct+1,XX based on (7), (8), and (9) respectively, whose memory complexity is a constant: O(m2 + md + d2). After that, the prediction loss ` can be measured by (6), so it is no longer necessary to load all the training samples into memory or visit a training sample multiple times.\nIn summary, the loss function (5) has the following merits: First, it can dynamically learn the structure of the residual errors as the samples continuously arrive, and leverage the structure information to effectively measure the true distance between the predicted value and the ground truth. Second, the loss is measured based on all the seen samples not just the current sample, which can cut down on the influence of noise on model’s update. Third, on each round, instead of loading all the samples into memory, the loss can be measured just relying on three defined statistical variables without information loss, as expressed in (6). Furthermore, by introducing the factor µ to weight the samples, MORES can fit in streaming data’s evolvement well. The third term: In order to restrain φ fluctuating drastically on each round, we hope that the divergence ∆φ(Ω,Ωt−1) is as small as possible. ∆φ(Ω,Ωt−1) is defined as:\n∆φ(Ω,Ωt−1) = φ(Ω)− φ(Ωt−1)− tr(g(Ωt−1)(Ω− Ωt−1)),\nwhere φ is a real-valued strictly convex differentiable function on the parameter domain Rm×m. g(Ωt−1) = ∇Ωφ(Ω)|Ωt−1 . In this paper, we employ two matrix divergence metrics, quantum relative entropy and LogDet divergence, because of their good properties stated in [32].\n(i) When φ(Ω) = tr(ΩlogΩ − Ω), the quantum relative entropy is defined as:\n∆φ(Ω,Ωt−1) = tr(ΩlogΩ− ΩlogΩt−1 − Ω + Ωt−1),\n4 where logΩ = V(logΛ)VT , and (logΛ)i,i = log(Λi,i). Ω is a strictly positive definite matrix, and Ω = VΛVT .\n(ii) When φ(Ω) = −logdet(Ω), LogDet is defined as:\n∆φ(Ω,Ωt−1) = log det(Ωt−1)\ndet(Ω) + tr(Ω−1t−1Ω)−m,\nwhere det(·) denotes the determinant of a matrix."
    }, {
      "heading" : "B. Optimization Procedure",
      "text" : "The objective function (3) is not convex with respect to all variables, but it is convex with each variable when others are fixed. We adopt an alternating optimization strategy to solve (3), which can find local minima.\nOptimizing P, given Ω and Γ: When Ω and Γ are fixed, (3) is then unconstrained and convex. P can be obtained by minimizing the following objective function:\nmin J1(P) = ‖P−Pt−1‖2Ω + α`(P,Γ;St) (14)\nThe necessary condition for the optimality is:\n∂J1(P)\n∂P = 0\n⇒ ΩP + αΓPCt,XX = ΩPt−1 + αΓCTt,XY ⇒ vec(ΩP + αΓPCt,XX) = vec(ΩPt−1 + αΓCTt,XY)\n(15)\nwhere vec(·) is an operator that reshapes a matrix of size m×n into a vector of size mn× 1.\nNoticing that vec(ABC) = (CT ⊗ A)vec(B) and vec(A + B)=vec(A)+vec(B), (15) can become:\n(I⊗ Ω + αCt,XX ⊗ Γ)vec(P) = vec(ΩPt−1 + αΓCTt,XY) ⇒ vec(P)=(I⊗ Ω+αCt,XX ⊗ Γ)†vec(ΩPt−1+αΓCTt,XY)\nwhere ⊗ denotes Kronecker product. (I⊗ Ω + αCt,XX ⊗ Γ)† is the Moore-Penrose pseudoinverse of the matrix (I⊗ Ω + αCt,XX ⊗ Γ). We know that when a matrix is invertible, its pseudoinverse is equal to its inverse. After obtaining vec(P), P can be easily found by the operator unvec(·) that reshapes a vector into a matrix.\nOptimizing Ω, given P and Γ: When P and Γ are fixed, solving Ω becomes a convex optimization problem as:\nmin Ω 0\nJ2(Ω) = ‖P−Pt−1‖2Ω + β∆φ(Ω,Ωt−1). (16)\n(i) When ∆φ(·) is quantum relative entropy, (16) becomes\nmin Ω 0\nJ2(Ω) = tr((P−Pt−1)TΩ(P−Pt−1))\n+βtr(ΩlogΩ− ΩlogΩt−1 − Ω + Ωt−1). (17)\nTaking the derivative of J2(Ω) with respect to Ω, and setting it to zero, we can obtain a closed form of Ω:\nΩ = exp(logΩt−1 − 1\nβ (P−Pt−1)(P−Pt−1)T ),\nwhere exp(·) is the matrix exponential. According to [32], it is easily inferred that Ω is positive definite.\nAlgorithm 1 Multiple-Output Regression for Streaming Data(MORES) Input: Data streams {(x1,y1), (x2,y2), · · · } that arrive one sample each time; Parameters: α, β, η, and the forgetting factor µ; Initialize P0 = 0d×m, C0,XX = 0d×d, C0,XY = 0d×m, C0,YY = 0m×m, and Ω0 = Γ0 = 1m × Im×m; Method 1. for t = 1, 2, · · · 2. Ct,YY = µCt−1,YY + ytyTt ; 3. Ct,XY = µCt−1,XY + xtyTt ; 4. Ct,XX = µCt−1,XX + xtxTt ; 5. if t < Tmin 6. Find the optimal P∗ by solving (14); 7. Pt ← P∗; Ωt ← Ω0; Γt ← Γ0; 8. else 9. Find the local optimal P∗, Ω∗, and Γ∗ by solving (14), (17) or (18), and (19), respectively; 10. Pt ← P∗; Ωt ← Ω∗; Γt ← Γ∗; 11. end 12. end end Method Output: Regression coefficient matrix Pt ∈ Rm×d.\n(ii) When using LogDet to represent ∆φ(·), (16) becomes\nmin Ω 0\nJ3(Ω) = tr((P−Pt−1)TΩ(P−Pt−1))\n+β(log det(Ωt−1)\ndet(Ω) + tr(Ω−1t−1Ω)−m). (18)\nSimilarly, solving ∂J3(Ω)∂(Ω) = 0, we have\nΩ = (Ω−1t−1 + 1 β (P−Pt−1)(P−Pt−1)T )−1.\nFor simplicity, we use the matrix M to represent (P − Pt−1)(P − Pt−1)T . To guarantee Ω to be positive semidefinite, the matrix M should be positive semi-definite. We adopt the following strategy to make M positive semi-definite: first, we calculate its spectral decomposition: M = UΛUT . Then M is updated by thresholding the corresponding eigenvalues as: M = Umax(Λ, 0)UT .\nOptimizing Γ, given P and Ω: When P and Ω are fixed, Γ can be obtained by solving the following convex optimization problem:\nmin Γ 0\nJ4(Γ) = α`(Γ;P,S)− η log |Γ|. (19)\nThe necessary condition for the optimality is ∂J4(Γ)∂Γ = 0. Therefore, we obtain the following closed form solution:\nΓ= η\nα (Ct,YY−CTt,XYPT −PCt,XY + PCt,XXPT )−1.\nAfter obtaining the solution Γ, we employ the same strategy to make Γ positive semi-definite as in (18).\nFinally, we summarize the procedure of MORES in Algorithm 1. During the preliminary stage of online update, the regression coefficient matrix P are not well formed, and poor initial estimations of P may result in poor estimations of Ω and Γ. To avoid this case, we do not update Ω or Γ until the round t is greater than or equal to a given threshold Tmin.\n5\nTABLE I MAES OF DIFFERENT METHODS ON THE BARRETT WAM DATASET. THE LAST COLUMN IS THE AVERAGE MAE. BEST RESULTS ARE HIGHLIGHTED IN BOLD FONTS.\nMethod 1st DOF 2nd DOF 3rd DOF 4th DOF 5th DOF 6th DOF 7th DOF Average PA-I 1.196 0.753 0.350 0.382 0.101 0.089 0.045 0.417 PA-II 1.179 0.772 0.332 0.391 0.095 0.084 0.043 0.414 iS-PLS 1.026 6.510 0.398 2.162 0.087 0.089 0.036 1.473 SOMOR 1.176 0.756 0.339 0.384 0.098 0.086 0.044 0.412\nMORES-LD 0.547 0.375 0.176 0.184 0.047 0.047 0.025 0.200 MORES-QE 0.526 0.391 0.182 0.180 0.044 0.049 0.026 0.200"
    }, {
      "heading" : "C. Time Complexity Analysis",
      "text" : "In Algorithm 1, the most time-consuming part of MORES is to update Pt, Ωt, and Γt, and the time cost of other parts can be ignored. Here we focus on analyzing the complexity of the case where t is greater than or equal to Tmin. For updating Pt, the complexity is O(m2d2 +d3m3) = O(d3m3). Updating Γ needs O(m3 + d2m). In order to update Ωt, we utilize two kinds of divergence metric(quantum relative entropy and LogDet divergence) in this paper. When using quantum relative entropy as the divergence metric, updating Ωt involves the spectral decomposition, whose complexity is the same as the eigen-decomposition, typically, O(m3) in practice. Therefore, updating Ωt needs O(m3 + dm2). When using LogDet, it also costs O(m3 + dm2) to update Ωt. Therefore, the total time complexity of MORES is of order O(d3m3 + m3 + d2m + m3 + dm2) = O(d3m3), which is dominated by the update of Pt."
    }, {
      "heading" : "III. EXPERIMENTS",
      "text" : "To evaluate the performance of MORES, we perform the experiments on three real-world datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34]."
    }, {
      "heading" : "A. Experimental Setups",
      "text" : "We compare our method with iS-PLS [19] that is the most relevant work to ours. We also compare with two variants of PA algorithm in [26] called PA-I and PA-II, which are two classical online learning approaches for single regression tasks. In the experiment, we use PA to train a regression model for each output. We name our simple formulation of online multiple-output regression proposed at the beginning of Sect. 2 as SOMOR for short. In addition, our proposed MORES has two variants: one using quantum relative entropy to measure the divergence of matrices, which is named as MORES-QE. The other using LogDet is called MORES-LD.\nThere are some parameters to be set in advance. In order to lower the cost of parameter tuning, we tune the parameters α, β, and η in our algorithms by a grid-search strategy from {10−3, 10−2, . . . , 102, 103} on the Concrete Slump dataset [35], and choose the optimal parameters to directly apply to the above three datasets. The threshold Tmin is set to 50, and the forgetting factor µ is set to 0.8 throughout the experiments, unless otherwise stated. To fairly conduct experimental comparisons, the parameters in the other methods are also searched from {10−3, 10−2, . . . , 102, 103}.\nIn the experimental study, we focus on the accuracy of online regression prediction to evaluate our models’ quality. The popular metric, Mean Absolute Error (MAE), is used to measure the prediction quality. MAE is defined as: MAE = 1t ∑t i=1 |yi − ŷi|, where ŷi denotes the estimated values of the i-th instance, and yi is the true response values."
    }, {
      "heading" : "B. Robot Inverse Dynamics",
      "text" : "We first study the problem of online learning the inverse dynamics of a 7 degrees of freedom of robotic arms on the Barrett WAM dataset. This dataset consists of a total of 16,200 samples, where each sample is represented by 21 input features, corresponding to seven joint positions, seven joint velocities and seven joint accelerations. Seven joint torques for the seven degrees of freedom (DOF) are used as the outputs.\nWe summarize the results of different methods in Table I. For each output, MORES-QE and MORES-LD attain better prediction performances than all the other methods. Meanwhile, they both achieve 51.5% relative error deduction in terms of the average MAE over SOMOR. The performance of MORES-QE is comparable to that of MORES-LD. It indicates that the quantum relative entropy divergence metric has similar effect on the prediction performance with the LogDet divergence metric. In addition, iS-PLS has a poor performance on the second and the fourth outputs. The reason may be that iS-PLS tries to find a low-dimensional subspace, where the covariance between the inputs and the outputs is maximized, while the found subspace does not preserve the structures of both the second and the fourth outputs well.\nIn our method, there are three main components: dynamically learning the structures of both the regression coefficient matrix and the residual error vector, and introducing a forgetting factor to measure the prediction error in an incremental fashion. We verify the effectiveness of the three components\nin terms of the average MAE of all the outputs on this dataset. The experimental setting is as follows: When both Ω and Γ are set to the identity matrix I in (3), the model is updated Without Relationship Learning on each round. We name it WRL for short. When Ω is set to I and Γ is updated on each round, it indicates that we only Dynamically learn the Relationships of the Residual errors in the process of model’s update. We call it DRR. When Ω is updated on each round and Γ is set to I, we only Dynamically learn the Relationships of the regression Coefficient matrix. Because we utilize quantum relative entropy and LogDet in updating Ω, we call them DRC-QE and DRC-LD, respectively. The results are shown in Fig. 1. Taking Fig. 1(a) as an example, both DRR and DRC-LD are better than WRL. It shows that dynamically learning the structures of the regression coefficient matrix and the residual errors are both beneficial to online regression. MORES-LD achieves the best performance. This indicates that the combination of the two components is effective for online multiple-output regression.\nIn order to verify the effectiveness of the forgetting factor µ, we conduct the experiments with different values of µ. Table II lists the results. Based on previous analysis in Sect. 2, we know no historical data is utilized to update the model on each\nround if µ = 0. When µ = 1, all the samples have the same weight for calculating the prediction loss. As can be seen in Table II, when 0.6 ≤ µ ≤ 0.9 , the performance is improved compared to that of µ = 0. This shows that taking advantage of the historical samples is good for online regression. Moreover, we observe that when µ = 0.7 and µ = 0.9, MORES-LD and MORES-QE respectively achieve the best performances. It implies that the data in this dataset is indeed evolving. By introducing µ to set higher weights on the newer training samples, the model can adapt to the data stream’s evolvement, and the prediction accuracy can be improved."
    }, {
      "heading" : "C. Stock Price Prediction",
      "text" : "Following previous studies in [36] and [20], we also apply our algorithms to the stock data of companies for price prediction. We choose the daily stock price data of five companies including IBM, Yahoo, Microsoft, Apple, and Oracle in the period from 2010 to 2013. The learned model can predict the stock prices in the future by using the stock prices in the past as inputs. We use the autoregressive 1, aka AR(1) model yt+1 = Ptyt+ t, where yt+1 represents the real stock prices of the five companies at time t+1, and Pt denotes the learned regression coefficient matrix at time t.\nThe experimental results are reported in Table III. MORESLD and MORES-QE achieve better performances compared to the other methods. Taking MORES-LD and PA-II as an example, MORES-LD gains 33.6%, 13.1%, 22.3%, 16.6%, and 17.0% relative accuracy improvement over PA-II for IBM, Yahoo, Microsoft, Apple, and Oracle, respectively. Meanwhile, MORES-LD obtains 20.6% relative improvement in terms\n7\nof the average MAE over PA-II. These results show that dynamically learning the structures of both the regression coefficient matrix and the residual error vector, as well as utilizing the historical data in an appropriate way, is good for online multiple-output regression.\nWe also investigate the performances of different methods as a function of sequence length (t). At the end of each online round, we calculate the MAE for each output attained so far. Fig. 2 shows the results. The performances of MORES-LD and MORES-QE are superior to those of the other methods, especially when the sequence length t is larger. In addition, the MAE curves of Fig. 2 (a), (b), and (d) rise after falling as t increases. This is because the stock price is severely evolving at the inflection point, such that the current model can not predict the future price well. Although the data is evolving, our algorithms are still better than the other methods under this circumstance. From Fig. 2 (a), we can see MORES-LD and MORES-QE can quickly adjust the model to fit in the data’s evolvement."
    }, {
      "heading" : "D. Weather Forecast",
      "text" : "We also evaluate our algorithms on the weather dataset for weather forecast [34]. This dataset consists of wind speed, wind direction, barometric pressure, water depth, maximum gust, maximum wave height, air temperature, water temperature and average wave height, which is collected every five minutes by a sensor network located on the south coast of England. One and a half years’ data containing 143,034 samples are used in the experiments. The first five variables are used as the predictors, and the rest are the response variables.\nThe experimental results are reported in Table IV. MORESLD and MORES-QE obtain better prediction performances than the other methods for all the response variables. Meanwhile, the results of our algorithms are superior to those of the other methods in terms of the average MAE.\nWe further test all the methods with different model update frequencies. The experimental setting is as follows: The model is updated when accumulatively receiving N (=1, . . . , 10) training data points, while the test is still performed on all the data points. As reported in Table V, the prediction accuracies of all the methods are gradually reduced as N increases. Moreover, for various values of N , MORES-LD and MORESQE perform better than the other methods because of dynamic learning of the output structures and the utilization of the historical data. In addition, we can see that the performance of MORES-LD with N = 10 is comparable to that of PA-II with N = 1."
    }, {
      "heading" : "E. Sensitivity Analysis",
      "text" : "We also study the sensitivity of parameters α, β, and η in our algorithm on the larger dataset, the weather dataset. As shown in Fig. 3, with the fixed η, our method is not sensitive to α and β with wide ranges. As for parameter η, when α and β are fixed, the performance is gradually improved as η increases. When η > 0.1, the performance is gradually degraded with η increasing. When η is set to 0.1, the performance is the best."
    }, {
      "heading" : "F. Efficiency",
      "text" : "We test the update speeds of our algorithms on the above three datasets. The experiments are conducted on a desktop with Inter(R) Core(TM) i7-3770 CPU, and MORES are implemented using MATLAB R2012b 64bit edition without parallel operation. The update speeds of our algorithms are reported in Table VI. On the weather dataset, MORES-QE achieves 3566 updates per second. And on the Inverse Dynamics dataset having the largest dimensions, our algorithms perform more than 1000 updates per second. If we apply some parallel implementations or use more efficient programming language, the update speed of MORES can be further improved."
    }, {
      "heading" : "IV. RELATED WORK",
      "text" : "In this section, we review the related works from two aspects: online single-output regression and batch multipleoutput regression.\n8 0.11 10 100 1000 0.001 0.01 0.1 1 10 0 0.1 0.2 0.3 0.4 M A E\n(a) MORES-LD (η = 0.1)\n0.11 10 100\n1000 0.001 0.01 0.1\n1 10\n0 0.1 0.2 0.3 0.4 M A E\n(b) MORES-QE (η = 0.1)\n0.010.1 1 10\n100 0.001 0.01 0.1\n1 10\n0 0.1 0.2 0.3 0.4 M A E\n(c) MORES-LD (β = 10)\n0.010.1 1 10\n100 0.001 0.01 0.1\n1 10\n0 0.1 0.2 0.3 0.4 M A E\n(d) MORES-QE (β = 10)\n0.010.1 1 10\n100 0.1\n1 10 100 1000\n0 0.1 0.2 0.3 0.4 M A E\n(e) MORES-LD (α = 1)\n0.010.1 1 10\n100 0.1\n1 10 100 1000\n0 0.1 0.2 0.3 0.4 M A E\n(f) MORES-QE (α = 1)\nFig. 3. MORES-LD and MORES-QE with different α, β, and η on the weather dataset.\nOnline single-output regression: [25] presented an online version of support vector regression algorithm, called (AOSVR). AOSVR classified all training samples into three distinct auxiliary sets according to the KKT conditions that define the optimal solution. After that, the rules for recursively updating the optimal solution were devised based on this classification. [26] proposed a margin based online regression algorithm, called passive-aggressive (PA). PA incrementally updated the model by formalizing the trade-off between the amount of progress made on each round and the amount of information retained from previous rounds. [27] proposed an incremental support vector regression algorithm, which evolved a pool of online SVR experts and learned to trade by dynamically weighting the experts’ opinions.\nBatch multiple-output regression: Many batch multipleout-put regression algorithms have been proposed, which tried to mine the structure among outputs. Rothman et al. [19] presented MRCE, which jointly learned the output structure in the form of the noise covariance matrix and the regression coefficients for predicting each output. Sohn and Kim [20] designed an algorithm to simultaneously estimate the regression coefficient vector for each output along with the covariance structure of the outputs with a shared sparsity assumption on\nthe regression coefficient vectors. Rai et al. [21] proposed an approach that leveraged the covariance structure of the regression coefficient matrix and the conditional covariance structure of the outputs for learning the model parameters. [22] proposed a tree-guided group lasso, or tree lasso, that directly combined statistical strength across multiple related outputs. They estimated the structured sparsity under multioutput regression by employing a novel penalty function constructed from the tree. Since these methods are trained in the batch mode, they are not suitable for online multiple-output prediction."
    }, {
      "heading" : "V. CONCLUSIONS",
      "text" : "In this paper, we proposed a novel online multiple-output regression method for streaming data. The proposed method can simultaneously and dynamically learn the structures of both the regression coefficients and the residual errors, and leverage the learned structure information to continuously update the model. Meanwhile, we accumulated the prediction error on all the seen samples in an incremental way without information loss, and introduced a forgetting factor to weight the samples so as to fit in data streams’ evolvement. The experiments were conducted on three real-world datasets, and the experimental results demonstrated the effectiveness and efficiency of the proposed method."
    } ],
    "references" : [ {
      "title" : "Data Mining: Concepts and Techniques",
      "author" : [ "J. Han", "M. Kamber" ],
      "venue" : "Second Edition: Diane Cerra,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Mining high-speed data streams",
      "author" : [ "P. Domingos", "G. Hulten" ],
      "venue" : "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2000, pp. 71–80.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A framework for on-demand classification of evolving data streams",
      "author" : [ "C.C. Aggarwal", "J. Han", "J. Wang", "P.S. Yu" ],
      "venue" : "IEEE Trans. on Knowledge and Data Engineering, vol. 18, no. 5, pp. 577–589, 2006.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learn ++.NC: Combining ensemble of classifiers with dynamically weighted consult-and-vote for efficient incremental learning of new classes",
      "author" : [ "M.D. Muhlbaier", "A. Topalis", "R. Polikar" ],
      "venue" : "IEEE Trans. on Neural Networks, vol. 20, no. 1, pp. 152–168, 2009.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A framework for clustering evolving data streams",
      "author" : [ "C.C. Aggarwal", "J. Han", "J. Wang", "P.S. Yu" ],
      "venue" : "International Conference on Very Large Data Bases (VLDB), 2003, pp. 81–92.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Density-based clustering for real-time stream data",
      "author" : [ "Y. Chen", "L. Tu" ],
      "venue" : "ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 2007, pp. 133–142.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Svstream: A support vector-based algorithm for clustering data streams",
      "author" : [ "C.-D. Wang", "J.-H. Lai", "D. Huang", "W.-S. Zheng" ],
      "venue" : "IEEE Trans. on Knowledge and Data Engineering, vol. 25, no. 6, pp. 1410–1424, 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Active learning from data streams",
      "author" : [ "X. Zhu", "P. Zhang", "X. Lin", "Y. Shi" ],
      "venue" : "IEEE International Conference on Data Mining (ICDM), 2007, pp. 757–762.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Unbiased online active learning in data streams",
      "author" : [ "W. Chu", "M. Zinkevich", "L. Li", "A. Thomas", "B. Tseng" ],
      "venue" : "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2011, pp. 195–203.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Active learning with drifting streaming data.",
      "author" : [ "I. Zliobaite", "A. Bifet", "B. Pfahringer", "G. Holmes" ],
      "venue" : "IEEE Trans. on Neural Networks and Learning Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Online feature selection with streaming features",
      "author" : [ "X. Wu", "K. Yu", "W. Ding", "H. Wang", "X. Zhu" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 35, no. 5, pp. 1178–1192, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Online feature selection and its applications",
      "author" : [ "J. Wang", "P. Zhao", "S. Hoi", "R. Jin" ],
      "venue" : "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 3, pp. 698–710, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Online learning of multiple tasks and their relationships",
      "author" : [ "A. Saha", "P. Rai", "H. Daumé III", "S. Venkatasubramanian" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 643–651.  9",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Online learning of multiple tasks with a shared loss.",
      "author" : [ "O. Dekel", "P.M. Long", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "A martingale framework for detecting changes in data streams by testing exchangeability",
      "author" : [ "S.-S. Ho", "H. Wechsler" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 32, no. 12, pp. 2113– 2127, 2010.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Change-point detection with feature selection in high-dimensional time-series data",
      "author" : [ "M. Yamada", "A. Kimura", "F. Naya", "H. Sawada" ],
      "venue" : "International joint conference on Artificial Intelligence (AAAI). AAAI Press, 2013, pp. 1827–1833.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A multivariate regression approach to association analysis of a quantitative trait network",
      "author" : [ "S. Kim", "K.-A. Sohn", "E.P. Xing" ],
      "venue" : "Bioinformatics, vol. 25, no. 12, pp. i204–i212, 2009.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Svm multiregression for nonlinear channel estimation in multiple-input multiple-output systems",
      "author" : [ "M. Sánchez-Fernández", "M. de Prado-Cumplido", "J. Arenas-Garcı́a", "F. Pérez-Cruz" ],
      "venue" : "IEEE Trans. on Signal Processing, vol. 52, no. 8, pp. 2298–2307, 2004.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Sparse multivariate regression with covariance estimation",
      "author" : [ "A.J. Rothman", "E. Levina", "J. Zhu" ],
      "venue" : "Journal of Computational and Graphical Statistics, vol. 19, no. 4, pp. 947–962, 2010.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Joint estimation of structured sparsity and output structure in multiple-output regression via inverse-covariance regularization",
      "author" : [ "K.-A. Sohn", "S. Kim" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics (AISTATS), 2012, pp. 1081–1089.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eqtl mapping",
      "author" : [ "S. Kim", "E.P. Xing" ],
      "venue" : "The Annals of Applied Statistics, vol. 6, no. 3, pp. 1095–1117, 2012.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Kernelized bayesian matrix factorization",
      "author" : [ "M. Gonen", "S. Kaski" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 36, no. 10, pp. 2047–2060, 2014.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multivariate regression with calibration",
      "author" : [ "H. Liu", "L. Wang", "T. Zhao" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2014, pp. 127–135.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Accurate on-line support vector regression",
      "author" : [ "J. Ma", "J. Theiler", "S. Perkins" ],
      "venue" : "Neural Computation, vol. 15, no. 11, pp. 2683–2703, 2003.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Online passive-aggressive algorithms",
      "author" : [ "K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research, vol. 7, pp. 551–585, 2006.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning to trade with incremental support vector regression experts",
      "author" : [ "G. Montana", "F. Parrella" ],
      "venue" : "Hybrid Artificial Intelligence Systems. Springer, 2008, pp. 591–598.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Sparse partial least squares regression for on-line variable selection with multivariate data streams",
      "author" : [ "B. McWilliams", "G. Montana" ],
      "venue" : "Statistical Analysis and Data Mining, vol. 3, no. 3, pp. 170–193, 2010.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exponentiated gradient versus gradient descent for linear predictors",
      "author" : [ "J. Kivinen", "M.K. Warmuth" ],
      "venue" : "Information and Computation, vol. 132, no. 1, pp. 1–63, 1997.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A convex formulation for learning task relationships in multi-task learning",
      "author" : [ "Y. Zhang", "D.-Y. Yeung" ],
      "venue" : "The Conference on Uncertainty in Artificial Intelligence (UAI), 2010, pp. 733–742.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K.Q. Weinberger", "J. Blitzer", "L.K. Saul" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2005, pp. 1473–1480.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Matrix exponentiated gradient updates for on-line learning and bregman projection.",
      "author" : [ "K. Tsuda", "G. Rätsch", "M.K. Warmuth", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2005
    }, {
      "title" : "Model learning with local gaussian process regression",
      "author" : [ "D. Nguyen-Tuong", "M. Seeger", "J. Peters" ],
      "venue" : "no. 15, pp. 2015–2034, 2009.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sparse convolved gaussian processes for multi-output regression.",
      "author" : [ "M.A. Alvarez", "N.D. Lawrence" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2008
    }, {
      "title" : "Modeling slump flow of concrete using second-order regressions and artificial neural networks",
      "author" : [ "I. Yeh" ],
      "venue" : "Cement and Concrete Composites, vol. 29, no. 6, pp. 474–480, 2007.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Robust sparse estimation of multiresponse regression and inverse covariance matrix via the l2 distance",
      "author" : [ "A.C. Lozano", "H. Jiang", "X. Deng" ],
      "venue" : "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2013, pp. 293–301.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Data streams arise in many scenarios, such as online transactions in the financial market, Internet traffic and so on [1].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 13,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 14,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 15,
      "context" : "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 16,
      "context" : "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 22,
      "context" : "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 23,
      "context" : "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 25,
      "context" : "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 24,
      "context" : "The representative method is online passive-aggressive (PA) algorithm [26].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "Since there are often correlations among outputs, mining the correlation relationships can improve the prediction accuracy of the model [19].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : "Recently, McWilliams and Montana take advantage of partial least squares (PLS) to build a recursive regression model for online predicting multiple outputs, called iS-PLS [28].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 24,
      "context" : "Following the general setting of online learning [26], we assume that the learner first observes an instance xt ∈ R on the t-th round, and it simultaneously predicts multiple outputs ŷt ∈ R based on the current model Pt−1 ∈ Rm×d.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "Following [26], the optimization problem defined by (2) can be easily solved by the Lagrange multiplier method.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 27,
      "context" : "∆φ(Ω,Ωt−1) denotes the Bregman divergence [29] that measures the distance between the matrix Ω and the matrix Ωt−1.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 28,
      "context" : "In (4), the term (P(i) − Pt−1(i))Ω(P(i) − Pt−1(i)) is actually the Mahalanobis distance between P(i) and Pt−1(i), where Ω encodes the correlations between the variables of the i-th column of the regression coefficient matrix on round t [30].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 29,
      "context" : "The term (yi − Pxi)Γ(yi − Pxi) measures the Mahalnobis distance between the true value yi and the predicted value Pxi, which can remove the influence of the residual errors’ correlations on distance calculation [31].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 30,
      "context" : "In this paper, we employ two matrix divergence metrics, quantum relative entropy and LogDet divergence, because of their good properties stated in [32].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 30,
      "context" : "According to [32], it is easily inferred that Ω is positive definite.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 31,
      "context" : "To evaluate the performance of MORES, we perform the experiments on three real-world datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 32,
      "context" : "To evaluate the performance of MORES, we perform the experiments on three real-world datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 18,
      "context" : "We compare our method with iS-PLS [19] that is the most relevant work to ours.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : "We also compare with two variants of PA algorithm in [26] called PA-I and PA-II, which are two classical online learning approaches for single regression tasks.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 33,
      "context" : ", 10, 10} on the Concrete Slump dataset [35], and choose the optimal parameters to directly apply to the above three datasets.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 34,
      "context" : "Following previous studies in [36] and [20], we also apply our algorithms to the stock data of companies for price prediction.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "Following previous studies in [36] and [20], we also apply our algorithms to the stock data of companies for price prediction.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 32,
      "context" : "We also evaluate our algorithms on the weather dataset for weather forecast [34].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "Online single-output regression: [25] presented an online version of support vector regression algorithm, called (AOSVR).",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "[26] proposed a margin based online regression algorithm, called passive-aggressive (PA).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[27] proposed an incremental support vector regression algorithm, which evolved a pool of online SVR experts and learned to trade by dynamically weighting the experts’ opinions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] presented MRCE, which jointly learned the output structure in the form of the noise covariance matrix and the regression",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "Sohn and Kim [20] designed an algorithm to simultaneously estimate the regression coefficient vector for each output along with the covariance structure of the outputs with a shared sparsity assumption on the regression coefficient vectors.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : "[22] proposed a tree-guided group lasso, or tree lasso, that directly combined statistical strength across multiple related outputs.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "Online multiple-output regression is an important machine learning technique for modeling, predicting, and compressing multi-dimensional correlated data streams. In this paper, we propose a novel online multiple-output regression method, called MORES, for streaming data. MORES can dynamically learn the structure of the regression coefficients to facilitate the model’s continuous refinement. We observe that limited expressive ability of the regression model, especially in the preliminary stage of online update, often leads to the variables in the residual errors being dependent. In light of this point, MORES intends to dynamically learn and leverage the structure of the residual errors to improve the prediction accuracy. Moreover, we define three statistical variables to exactly represent all the seen samples for incrementally calculating prediction loss in each online update round, which can avoid loading all the training data into memory for updating model, and also effectively prevent drastic fluctuation of the model in the presence of noise. Furthermore, we introduce a forgetting factor to set different weights on samples so as to track the data streams’ evolving characteristics quickly from the latest samples. Experiments on three real-world datasets validate the effectiveness and efficiency of the proposed method.",
    "creator" : "LaTeX with hyperref package"
  }
}