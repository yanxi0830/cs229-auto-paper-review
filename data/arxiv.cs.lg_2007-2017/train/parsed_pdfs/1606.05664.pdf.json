{
  "name" : "1606.05664.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Talat Nazir" ],
    "emails" : [ "xiaomin.qi@mdh.se,", "sergei.silvestrov@mdh.se,", "talat.nazir@mdh.se" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n05 66\n4v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\n01 6\nLinear Classification of data with Support Vector Machines and Generalized Support\nVector Machines Xiaomin Qi, Sergei Silvestrov and Talat Nazir\nDivision of Applied Mathematics, School of Education,\nCulture and Communication, Mälardalen University, 72123 Väster̊as, Sweden. E-mail: xiaomin.qi@mdh.se, sergei.silvestrov@mdh.se, talat.nazir@mdh.se\n——————————————————————————————– Abstract: In this paper, we study the support vector machine and introduced the notion of generalized support vector machine for classification of data. We show that the problem of generalized support vector machine is equivalent to the problem of generalized variational inequality and establish various results for the existence of solutions. Moreover, we provide various examples to support our results. ——————————————— Keywords and Phrases: support vector machine, generalized support vector machine, control function. 2010 Mathematics Subject Classification: 62H30. ———————————————\n1 Support Vector Machine\nOver the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression. It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22]. Recently, Wang et al. [15] presented SVM based fault classifier design for a water level control system. They also studied the SVM classifier based fault diagnosis for a water level process [16].\nFor the standard support vector classification (SVC), the basic idea is to find the optimal separating hyperplane between the positive and negative examples. The optimal hyperplane may be obtained by maximizing the margin between two parallel hyperplanes, which involves the minimization of a quadratic programming problem.\nSupport Vector Machines is based on the concept of decision planes that define decision boundaries. A decision plane is one that separates between a set of objects having different class memberships.\nSupport Vector Machines can be thought of as a method for constructing a special kind of rule, called a linear classifier, in a way that produces classifiers with theoretical guarantees of good predictive performance (the quality of classification on unseen data).\nIn this paper, we study the problems of support vector machine and define generalized support vector machine. We also show the sufficient conditions for the existence of solutions for problems of generalized support vector machine. We also support our results with various examples.\nThought this paper, by N, R, Rn and R+n we denote the set of all natural numbers, the set of all real numbers, the set of all n-tuples real numbers, the set of all n-tuples of nonnegative real numbers, respectively.\nAlso, we consider ‖·‖ and < ·, · > as Euclidean norm and usual inner product on Rn, respectively.\nFurthermore, for two vectors x,y ∈ Rn, we say that x ≤ y if and only if xi ≤ yi for all i ∈ {1, 2, ..., n}, where xi and yi are the components of x and y, respectively.\nLinear Classifiers\nBinary classification is frequently performed by using a function f : Rn → R in the following way: the input x = (x1, ..., xn) is assigned to the positive class if, f (x) ≥ 0 and otherwise to the negative class. We consider the case where f (x) is a linear function of x, so that it can be written as\nf (x) = 〈w · x〉+ b\n=\nn ∑\ni=1\nwixi + b,\nwhere w ∈ Rn, b ∈ R are the parameters that control the function and the decision rule is given by sgn (f (x)) . The learning methodology implies that these parameters must be learned from the data.\nDefinition 1.1. We define the functional margin of an example (xi, yi) with respect to a hyperplane (w, b) to be the quantity\nγi = yi (〈w · xi〉+ b) ,\nwhere yi ∈ {−1, 1}. Note that γi > 0 implies correct classification of (xi, yi) . If we replace functional margin by geometric margin we obtain the equivalent quantity for the normalized linear function (\n1 ‖w‖w, 1 ‖w‖b\n)\n, which therefore\nmeasures the Euclidean distances of the points from the decision boundary in the input space. Actually geometric margin can be written as\nγ̃ = 1\n‖w‖γ.\nTo find the hyperplane which has maximal geometric margin for a training set S means to find maximal γ̃. For convenience, we let γ = 1, the objective function can be written as\nmax 1\n‖w‖ .\nOf course, there have some constraints for the optimization problem. According to the definition of margin, we have yi (〈w · xi〉+ b) ≥ 1, i = 1, ..., l. We rewrite the equivalent formation of the objective function with the constraints as\nmin 1\n2 ‖w‖2 such that yi (〈w · xi〉+ b) ≥ 1, i = 1, ..., l.\nWe denote this problem by SVM.\n2 Generalized Support Vector Machines\nWe replace w, b by W,B respectively, the control function F : Rn → Rn defined as\nF (x) = W.x +B, (2.1)\nwhere W ∈ Rn×n, B ∈ Rn are the parameters of control function. Define\nγ̃∗k = yk (Wxk +B) > 1 for k = 1, 2, ..., l, (2.2)\nwhere yk ∈ {(−1,−1, ...,−1) , (1, 1, ..., 1)} is n dimensional vector.\nDefinition 2.1. We define a map G : Rn → Rn+ by\nG (wi) = (‖wi‖ , ‖wi‖ , ..., ‖wi‖) for i = 1, 2, ..., n, (2.3)\nwhere wi be the row of Wn×n for i = 1, 2, ..., n. The problem is find wi ∈ Rn that satisfy\nmin wi∈W G (wi) such that η > 0, (2.4)\nwhere η = yk (Wxk +B)− 1.\nWe call this problem as the Generalized Support Vector Machine (GSVM). The GSVM is equivalent to\nfind wi ∈ W : 〈G′ (wi) ,v−wi〉 ≥ 0 for all v ∈ Rn with η > 0,\nor more specifically\nfind wi ∈ W : 〈ηG′ (wi) ,v−wi〉 ≥ 0 for all v ∈ Rn. (2.5)\nHence the problem of GSVM becomes to the problem of generalized variational inequality.\nExample 2.2. Let us take the group of points positive class (1, 0) , (0, 1) and negative class (−1, 0), (0,−1) . First we use SVM to solve this problem to find the hyperplane < w,x > +b = 0 that separate this two kinds of points. Obviously, we know that the hyperplane is H which is shown in the Figure.\nFor two positive points, we have\n(w1, w2)\n[\n1 0\n]\n+ b = 1\n(w1, w2)\n[\n0 1\n]\n+ b = 1\nwhich implies\nw1 + b = 1\nw2 + b = 1.\nFor two negative points, we have\n(w1, w2)\n[\n−1 0\n]\n+ b = −1\n(w1, w2)\n[\n0 −1\n]\n+ b = −1\nimplies that\n−w1 + b = −1 −w2 + b = −1.\nFrom the equations, we get w = (1, 1) and b = 0. The result is ‖w‖ = √ 2.\nNow we apply GSVM for this data. For two positive points, we have\n[\nw11 w12 w21 w22\n] [\n1 0\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand [\nw11 w12 w21 w22\n] [\n0 1\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nwhich gives\n[\nw11 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand\n[\nw12 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n. (2.6)\nFor two negative points, we have\n[\nw11 w12 w21 w22\n] [\n−1 0\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\nand [\nw11 w12 w21 w22\n] [\n0 −1\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\n,\nwhich provides\n[\n−w11 −w21\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\nand\n[\n−w12 −w22\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\n. (2.7)\nFrom (2.6) and (2.7), we get\nW =\n[\n1 1 1 1\n]\nB =\n[\nb1 b2\n]\n=\n[\n0 0\n]\n.\nThus minG (wi) = min {G (w1) , G (w2)} = ( √ 2, √ 2).\nHence we get w = (1, 1) that minimize G (wi) for i = 1, 2.\nConclusion: The above example shows that we get same result by applying any method SVM and GSVM.\nIn the next example, we consider the two distinct group of data, first solve both data for separate cases and then solve it for combine case for both methods SVM and GSVM. Example 2.3. Let us consider the three categories of data: Situation 1, suppose that, we have data (1, 0), (0, 1) as positive class and data (−1/2, 0), (0,−1/2) as negative class.\nUsing SVM to solve this problem, we have\n(w1, w2)\n[\n1 0\n]\n+ b = 1 and\n(w1, w2)\n[\n0 1\n]\n+ b = 1,\nwhich implies w1 + b = 1 and w2 + b = 1. (2.8)\nFor two negative points, we have\n(w1, w2)\n[\n−1/2 0\n]\n+ b = −1, and\n(w1, w2)\n[\n0 −1/2\n]\n+ b = −1,\nwhich gives\n− w1 2 + b = −1 and − w2 2 + b = −1. (2.9)\nFrom (2.8) and (2.9), we get w = (4 3 , 4 3 ) with b = −1 3 , where ‖w‖ = √ 32 3 .\nFor situation 2, we consider the data (1 2 , 0) and (0, 1 2 ) as positive class, data (−2, 0) and (0,−2) as negative class.\nUsing SVM to solve this problem, we have\n(w1, w2)\n[\n1/2 0\n]\n+ b = 1 and\n(w1, w2)\n[\n0 1/2\n]\n+ b = 1,\nwhich implies 1\n2 w1 + b = 1 and\n1 2 w2 + b = 1. (2.10)\nFrom the negative points, we have\n(w1, w2)\n[\n−2 0\n]\n+ b = −1 and\n(w1, w2)\n[\n0 −2\n]\n+ b = −1,\nimplies that − 2w1 + b = −1 and − 2w2 + b = −1. (2.11)\nFrom (2.10) and (2.11), we get w = (4 5 , 4 5 ) and b = 3 5 with ‖w‖ = √ 32 5 .\nIn the next situation 3, we combine of this two groups of data. Now, we have data (1/2, 0), (0, 1/2), (1, 0) , (0, 1) as positive class and (−1/2, 0), (0,−1/2), (−2, 0), (0,−2) as negative class.\nUsing SVM to solve this problem, we have\n(w1, w2)\n[\n1/2 0\n]\n+ b = 1 and\n(w1, w2)\n[\n0 1/2\n]\n+ b = 1,\nwhich implies w1/2 + b = 1 and w2/2 + b = 1. (2.12)\nFor two negative points, we have\n(w1, w2)\n[\n−1/2 0\n]\n+ b = −1 and\n(w1, w2)\n[\n0 −1/2\n]\n+ b = −1,\nimplies that\n− 1 2 w1 + b = −1 and − 1 2 w2 + b = −1. (2.13)\nFrom (2.12) and (2.13), we obtain w = (2, 2) and b = 0, where ‖w‖ = 2 √ 2.\nNow we solve the same problem for all three situations by using GSVM. For two positive points of situation 1, we have\n[\nw11 w12 w21 w22\n] [\n1 0\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand [\nw11 w12 w21 w22\n] [\n0 1\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n,\nwhich implies\n[\nw11 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand\n[\nw12 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n. (2.14)\nAgain, for the negative points, we have\n[\nw11 w12 w21 w22\n] [\n−1/2 0\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\nand [\nw11 w12 w21 w22\n] [\n0 −1/2\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\n,\nwhich gives\n[\n−1 2 w11 −1 2 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\nand\n[\n−1 2 w12 −1 2 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\n. (2.15)\nFrom (2.14) and (2.15), we get\nW =\n[\n4 3 4 3 4 3 4 3\n]\nand B =\n[\n−1 3 −1 3\n]\n.\nThus we get\nmin wi∈W\nG (wi) = ( 4 √ 2 3 , 4 √ 2 3 ).\nHence we get w = (4 3 , 4 3 ) that minimize G (wi) for i = 1, 2. Now, for positive points of situation 2, we have\n[\nw11 w12 w21 w22\n] [\n1/2 0\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand [\nw11 w12 w21 w22\n] [\n0 1/2\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n,\nwhich gives [\n1 2 w11 1 2 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand\n[\n1 2 w12 1 2 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n.\nFor two negative points for this case, we have [\nw11 w12 w21 w22\n] [\n−2 0\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\nand [\nw11 w12 w21 w22\n] [\n0 −2\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\n,\nwhich gives [\n−2w11 −2w21\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\nand\n[\n−2w12 −2w22\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\n.\nThus, we obtain that\nW =\n[\n4 5 4 5 4 5 4 5\n]\nand B =\n[\n3 5 3 5\n]\n.\nThus we get\nmin i∈{1,2}\nG (wi) = ( 4 √ 2 5 , 4 √ 2 5 ).\nHence we get w = (4 5 , 4 5 ) that minimize G (wi) for i = 1, 2.\nFor the positive points of the combination of situation 3, we have [\nw11 w12 w21 w22\n] [\n1/2 0\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand [\nw11 w12 w21 w22\n] [\n0 1/2\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n,\nwhich gives [\n1 2 w11 1 2 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand\n[\n1 2 w12 1 2 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n.\nFor two negative points for this case, we have [\nw11 w12 w21 w22\n] [\n−1 2\n0\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\nand [\nw11 w12 w21 w22\n] [\n0 −1\n2\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\n,\nwhich gives [\n−1 2 w11 −1 2 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\nand\n[\n−1 2 w12 −1 2 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n−1 −1\n]\n.\nFrom this, we obtain that\nW =\n[\n2 2 2 2\n]\nand B =\n[\n0 0\n]\n.\nThus we get min\ni∈{1,2} G (wi) = (2\n√ 2, 2 √ 2).\nHence we get w = (2, 2) that minimize G (wi) for i = 1, 2.\nProposition 2.4. Let G : Rn → Rn+ be a differentiable operator. An element w∗ ∈ Rn minimize G if and only if G′ (w∗) = 0, that is, w∗ ∈ Rn solves GSVM if and only if G′ (w∗) = 0. Proof. Let G′ (w∗) = 0, then for all v ∈ Rn,\n< ηG′ (w∗) ,v −w∗ > = < 0,v −w∗ > = 0.\nConsequently, the inequality\n< ηG′ (w∗) ,v −w∗ > = < 0,v −w∗ > ≥ 0\nholds for all v ∈ Rn. Hence w∗ ∈ Rn solves problem of GSVM. Conversely, assume that w∗ ∈ Rn satisfies\n< ηG′ (w∗) ,v −w∗ > ≥ 0 ∀ v ∈ Rn.\nTake v = w∗ −G′ (w∗) in the above inequality implies that\n< ηG′ (w∗) ,−G′ (w∗) > ≥ 0,\nwhich further implies −η||G′(w∗)||2 ≥ 0.\nSince η > 0, so we get G′(w∗) = 0.\nDefinition 2.5. Let K be a closed and convex subset of Rn. Then, for every point x ∈ Rn, there exists a unique nearest point in K, denoted by PK (x), such that ‖x− PK (x)‖ ≤ ‖x− y‖ for all y ∈ K and also note that PK (x) = x if x ∈ K. PK is called the metric projection of Rn onto K. It is well known that PK : R n → K is characterized by the properties:\n(i) PK (x) = z for x ∈ Rn if and only if < z,y − z > ≥ < x,y − z > for all y ∈ Rn;\n(ii) For every x,y ∈ Rn, ‖PK (x)− PK (y)‖2 ≤ < x−y, PK (x)−PK (y) >;\n(iii) ‖PK (x)− PK (y)‖ ≤ ‖x− y‖, for every x,y ∈ Rn, that is, PK is nonexpansive map.\nProposition 2.6. Let G : Rn → Rn+ be a differentiable operator. An element w∗ ∈ Rn minimize mapping G defined in (2.3) if and only if w∗ is the fixed point of map\nPRn + (I − ρG′) : Rn → Rn+ for any ρ > 0.\nthat is,\nw∗ = PRn + (I − ρG′) (w∗)\n= PRn + (w∗ − ρG′ (w∗)) ,\nwhere PRn + is a projection map from Rn to Rn+. Proof. Suppose w∗ ∈ Rn+ is solution of GSVM then for η > 0, we have\n< ηG′ (w∗) ,w −w∗ > ≥ 0 for all w ∈ Rn.\nAdding < w∗,w −w∗ > on both sides, we get\n< w∗,w−w∗ > + < ηG′ (w∗) ,w−w∗ > ≥ < w∗,w−w∗ > for all w ∈ Rn,\nwhich further implies that\n< w∗,w −w∗ > ≥ < w∗ − ηG′ (w∗) ,w−w∗ > for all w ∈ Rn,\nwhich is possible only if w∗ = PRn + (w∗ − ρG′ (w∗)) , that is, w∗ is the fixed point of G′. Conversely, let w∗ = PRn + (w∗ − ρG′ (w∗)) , then we have\n< w∗,w −w∗ > ≥ < w∗ − ηG′ (w∗) ,w−w∗ > for all w ∈ Rn,\nwhich implies\n< w∗,w −w∗ > − < w∗ − ηG′ (w∗) ,w−w∗ > ≥ 0 for all w ∈ Rn,\nand so < ηG′ (w∗) ,w−w∗ > ≥ 0 for all w ∈ Rn.\nThus w∗ ∈ Rn+ is the solution of GSVM.\nDefinition 2.7. A map G : Rn → Rn is said to be (I) L-Lipschitz if for every L > 0,\n‖G (x)−G (y)‖ ≤ L ‖x− y‖ for all x,y ∈ Rn.\n(II) monotone if\n< G (x)−G (y) ,x− y > ≥ 0 for all x,y ∈ Rn.\n(III) strictly monotone if\n< G (x)−G (y) ,x− y > > 0 for all x,y ∈ Rn with x 6= y.\n(IV) α-strongly monotone if\n< G (x)−G (y) ,x− y > ≥ α ‖x− y‖2 for all x,y ∈ Rn.\nNote that, every α-strongly monotone map G : Rn → Rn is strictly monotone and every strictly monotone map is monotone.\nExample 2.8. Let G : Rn → Rn be a mapping defined as\nG (wi) = αwi + β,\nwhere α is any non negative scalar and β is any real number. Then G is Lipschitz continuous with Lipschitz constant L = α.\nAlso, for any x,y ∈ Rn,\n< G (x)−G (y) ,x− y > = α ‖x− y‖2\nwhich show that G is α-strongly monotone.\nTheorem 2.9. Let K ⊆ Rn be closed and convex and G′ : Rn → K is strictly monotone. If there exists a w∗ ∈ K which is the solution of GSVM, then w∗ is unique in K. Proof. Suppose that w∗1,w ∗ 2 ∈ K with w∗1 6= w∗2 be the two solutions of GSVM, then we have\n< ηG′ (w∗1) ,w−w∗1 > ≥ 0 for all w ∈ Rn (2.16)\nand < ηG′ (w∗2) ,w −w∗2 > ≥ 0 for all w ∈ Rn, (2.17) where η > 0. Putting w = w∗2 in (2.16) and w = w ∗ 1 in (2.17), we get\n< ηG′ (w∗1) ,w ∗ 2 −w∗1 > ≥ 0 (2.18)\nand < ηG′ (w∗2) ,w ∗ 1 −w∗2 > ≥ 0. (2.19)\nEq. (2.18) can be further write as\n< −ηG′ (w∗1) ,w∗1 −w∗2 > ≥ 0. (2.20)\nAdding (2.19) and (2.20) implies that\n< ηG′ (w∗2)− ηG′ (w∗1) ,w∗1 −w∗2 > ≥ 0\nwhich implies\nη < G′ (w∗1)−G′ (w∗2) ,w∗1 −w∗2 > ≤ 0\nor < G′ (w∗1)−G′ (w∗2) ,w∗1 −w∗2 > ≤ 0. (2.21) Since G′ is strictly monotone, so we must have\n< G′ (w∗1)−G′ (w∗2) ,w∗1 −w∗2 > > 0,\nwhich contradicts (2.21). Thus w∗1 = w ∗ 2. Theorem 2.10. LetK ⊆ Rn be closed and convex. If the map G′ : Rn → K is L-Lipchitz and α-strongly monotone then there exists a unique w∗ ∈ K which is the solution of GSVM . Proof. Uniqueness: Suppose that w∗1,w ∗ 2 ∈ K be the two solutions of GSVM, then for η > 0, we have < ηG′ (w∗1) ,w−w∗1 > ≥ 0 for all w ∈ Rn (2.22)\nand < ηG′ (w∗2) ,w −w∗2 > ≥ 0 for all w ∈ Rn. (2.23) Putting w = w∗2 in (2.22) and w = w ∗ 1 in (2.23), we get\n< ηG′ (w∗1) ,w ∗ 2 −w∗1 > ≥ 0 (2.24)\nand < ηG′ (w∗2) ,w ∗ 1 −w∗2 > ≥ 0. (2.25)\nEq. (2.24) can be further write as\n< −ηG′ (w∗1) ,w∗1 −w∗2 > ≥ 0. (2.26)\nAdding (2.25) and (2.26) implies that\n< ηG′ (w∗2)− ηG′ (w∗1) ,w∗1 −w∗2 > ≥ 0\nwhich implies\nη < G′ (w∗1)−G′ (w∗2) ,w∗1 −w∗2 > ≤ 0. (2.27)\nSince G′ is α-strongly monotone, so we have\nαη ‖w∗1 −w∗2‖2 ≤ η < G′ (w∗1)−G′ (w∗2) ,w∗1 −w∗2 > ≤ 0,\nwhich implies that αη ‖w∗1 −w∗2‖2 ≤ 0. Since αη > 0, so we must have ‖w∗1 −w∗2‖ = 0 and hence w∗1 = w∗2. Existence:\nAs we know that if w∗ ∈ Rn+ is solution of GSVM then for η > 0, we have\n< ηG′ (w∗) ,w −w∗ > ≥ 0 for all w ∈ Rn\nif and only if w∗ = PRn + (w∗ − ρG′(w∗)) ≡ F (w∗) (say). Now for any w∗1,w ∗ 2 ∈ Rn+, we have\n‖F (w∗1)− F (w∗2)‖2 = ∥ ∥\n∥ PRn + (w∗1 − ρG′(w∗1))− PRn+(w∗2 − ρG′(w∗2))\n∥ ∥ ∥ 2\n≤ ‖(w∗1 − ρG′(w∗1))− (w∗2 − ρG′(w∗2))‖2 (as PRn+ is nonexpansive) = ‖(w∗1 −w∗2)− ρ[G′(w∗1)−G′(w∗2)]‖2 = < (w∗1−w∗2)− ρ[G′(w∗1)−G′(w∗2)], (w∗1−w∗2)− ρ[G′(w∗1)−G′(w∗2)] > = ‖w∗1−w∗2‖2 − 2ρ < w∗1−w∗2, G′(w∗1)−G′(w∗2) > +ρ2 ‖G′(w∗1)−G′(w∗2)‖2 .\nNow since G′ is L-Lipchitz and α-strongly monotone, so we get\n‖F (w∗1)− F (w∗2)‖2 ≤ ‖w∗1 −w∗2‖2 − 2αρ ‖w∗1 −w∗2‖2\n+ρ2L2 ‖w∗1 −w∗2‖2\n= (1 + ρ2L2 − 2ρα) ‖w∗1 −w∗2‖2 ,\nthat is, ‖F (w∗1)− F (w∗2)‖ ≤ θ ‖w∗1 −w∗2‖ , (2.30) where θ = √\n1 + ρ2L2 − 2ρα. Since ρ > 0, so that when ρ ∈ (0, 2α L2 ), then we\nget θ ∈ [0, 1). Now, by using Principle of Banach contraction, we obtain the fixed point of map F, that is, there exists a unique w∗ ∈ Rn+ such that\nF (w∗) = PRn + (w∗ − ρG′(w∗))\n= w∗.\nHence w∗ ∈ Rn+ is the solution of GSVM.\nExample 2.11. Let us take the group of data of positive class (α1, α2, ..., αn−1, 0) , (α1, α2, ..., αn−2, 0, αn), ..., (0, α2, α3, ..., αn) and negative class (kα1, kα2, ..., kαn−1, 0) , (kα1, kα2, ..., kαn−2, 0, kαn), ..., (0, kα2, kα3, ..., kαn) for n ≥ 2, where each αi 6= 0 for i ∈ N and k 6= 1.\nA map G : Rn → Rn+ be given as\nG (wi) = (‖wi‖ , ‖wi‖ , ..., ‖wi‖) for i = 1, 2, ..., n,\nwhere wi are the row of Wn×n for i = 1, 2, ..., n. Then we have\nG′ (wi) = 1\n‖wi‖ wi for i = 1, 2, ..., n.\nNow from the given data, we get\nW = 2\n(n− 1) (1− k)\n\n    \n1 α1 1 α2 · · · 1 αn 1 α1 1 α2 · · · 1 αn\n· ·\n· ·\n· ·\n· ·\n1 α1 1 α2 · · · 1 αn\n\n    \nand so we have\nG (wi) = 2\n(n− 1) (1− k)\n√\n1\nα21 +\n1\nα22 + ...+\n1\nα2n (1, 1, ..., 1) for i = 1, 2, ..., n\nand\nG′ (wi) = 1 √\n1 α2 1 + 1 α2 2 + ...+ 1 α2n\n( 1 α1 , 1 α2 , ..., 1 αn ) for i = 1, 2, ..., n.\nNote that, for any w1,w2 ∈ W,\n‖G′ (w1)−G′ (w2)‖ = 0 = L ‖w1 −w2‖\nis satisfied where L is any nonnegative real number. Also\n< G′ (w1)−G′ (w2) ,w1 −w2 > ≥ 0\nis satisfied which show that G′ is monotone operator. Moreover, w = 2 (n−1)(1−k)( 1 α1 , 1 α2 , ..., 1 αn ) is the solution of GSVMwith ‖w‖ = 2 (n−1)(1−k) √ 1 α2 1 + 1 α2 2 + ...+ 1 α2 n .\nExample 2.12. Let us take the group of data of positive class (α1, α2, ..., αm, 0, 0..., 0) , (0, α2, α3, ..., αm+1, 0, 0, ..., 0), ..., (α1, α2, ..., αm−1, 0, 0, ..., 0, αn) and negative\nclass (κα1, κα2, ..., καm, 0, 0..., 0) , (0, κα2, κα3, ..., καm+1, 0, 0, ..., 0), ..., (κα1, κα2, ..., καm−1, 0, 0, ..., for n > m ≥ 1, where each αi 6= 0 for i ∈ N and κ 6= 1. A map G : Rn → Rn+ be given as\nG (wi) = (‖wi‖ , ‖wi‖ , ..., ‖wi‖) for i = 1, 2, ..., n,\nwhere wi are the row of Wn×n for i = 1, 2, ..., n. Then we have\nG′ (wi) = 1\n‖wi‖ wi for i = 1, 2, ..., n.\nNow from the given data, we get\nW = 2\nm (1− k)\n\n    \n1 α1 1 α2 · · · 1 αn 1 α1 1 α2 · · · 1 αn\n· ·\n· ·\n· ·\n· ·\n1 α1 1 α2 · · · 1 αn\n\n    \nand so we have\nG (wi) = 2\n(n− 1) (1− k)\n√\n1\nα21 +\n1\nα22 + ...+\n1\nα2n (1, 1, ..., 1) for i = 1, 2, ..., n\nand\nG′ (wi) = 1 √\n1 α2 1 + 1 α2 2 + ...+ 1 α2 n\n( 1 α1 , 1 α2 , ..., 1 αn ) for i = 1, 2, ..., n.\nIt is easy to verify that G′ is monotone and Lipchitz continuous operator. The vector w = 2\nm(1−k)( 1 α1 , 1 α2 , ..., 1 αn ) is the solution of GSVM with\n‖w‖ = 2 m(1−k)\n√\n1 α2 1 + 1 α2 2 + ...+ 1 α2 n .\nExample 2.13. Consider (α1, 0, 0) , (0, α2, 0) , (0, 0, α3) , (β1, 0, 0), (0, β2, 0), (0, 0, β3) as data of positive class and (kα1, 0, 0) , (0, kα2, 0) , (0, 0, kα3) , (kβ1, 0, 0), (0, kβ2, 0), (0, 0, kβ3) as negative class of data, where αi, βi and k are positive real numbers with each αi ≤ βi for i = 1, 2, 3 and k 6= 1. The map G : Rn → Rn+ is given as\nG (wi) = (‖wi‖ , ‖wi‖ , ..., ‖wi‖) for i = 1, 2, 3,\nwhere wi are the row of W3×3 for i = 1, 2, ..., n. Then we have\nG′ (wi) = 1\n‖wi‖ wi for i = 1, 2, 3.\nNow from the given data, we get\nW = 2\n(1− k)\n\n\n1 α1 1 α2 1 α3 1 α1 1 α2 1 α3 1 α1 1 α2 1 α3  \nand so we have\nG (wi) = 2 (1− k)( √ 1 α21 + 1 α22 + 1 α23 , √ 1 α21 + 1 α22 + 1 α23 , √ 1 α21 + 1 α22 + 1 α23 )\nand\nG′ (wi) = 1 √\n1 α2 1 + 1 α2 2 + 1 α2 3\n( 1 α1 , 1 α2 , 1 α3 ).\nNote that, for any w1,w2 ∈ W,\n‖G′ (w1)−G′ (w2)‖ = 0 = L ‖w1 −w2‖\nis satisfied for L > 0. Also\n< G′ (w1)−G′ (w2) ,w1 −w2 > ≥ 0\nis satisfied which show that G′ is monotone operator. Moreover, w = 2\n(1−k)( 1 α1 , 1 α2 , 1 α3 ) is the solution of GSVM with ‖w‖ = 2 (1−k)\n√\n1 α2 1 + 1 α2 2 + 1 α2 3 .\nExample 2.14. Let us take the group of data of positive class (1, 0, 0) , (1, 1, 0), (0, 1, 1) and negative class (−1 2 , 0, 0), (−1 2 ,−1 2 , 0), (0,−1 2 ,−1 2 ). Now from the given data, we have\nW =\n\n\n4 3 0 4 3 4 3 0 4 3 4 3 0 4 3\n\n\nwith\nG (wi) = 4 3 ( √ 2, √ 2, √ 2) for i = 1, 2, 3\nand\nG′ (wi) = 1√ 2 (1, 0, 1) for i = 1, 2, 3.\nIt is easy to verify that G′ is monotone operator and Lipchitz continuous. Moreover, w = ( 4\n3 , 0,\n4 3 ) is the solution of GSVM with ‖w‖ = 4 3\n√ 2.\nConclusion. Recently many results appeared in the literature giving the problems related to the support vector machine and it applications. In this paper, initiate the study of generalized support vector machine and present linear classification of data by using support vector machine and generalized support vector machine. We also provide sufficient conditions under which the solution of generalized support vector machine exist. Various examples are also present to show the validity of these results.\n\\end{conclusion}\nReferences\n[1] Adankon, M.M., and Cheriet, M.: Model selection for the LS-SVM. Application to handwriting recognition. Pattern Recognition 42 (12), 3264-3270 (2009)\n[2] Cortes, C., Vapnik, V.N.: Support-vector networks. Machine Learning 20 (3), 273-297 (1995)\n[3] Cristianini, N., and Shawe-Taylor, J.: An Introduction to support vector machines and other kernelbased learning methods. Cambridge University Press, 2000.\n[4] Guyon, I., Weston, J., Barnhill, S., and Vapnik, V.: Gene selection for cancer classification using support vector machines. Machine Learning 46 (1-3), 389-422 (2002)\n[5] Joachims, T.: Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning. Springer, 1998.\n[6] Khan, N., Ksantini, R., Ahmad I., and Boufama, B.: A novel SVM+NDA model for classification with an application to face recognition. Pattern Recognition 45 (1), 66-79 (2012)\n[7] Li, Kwok, S., J.T., Zhu, H., Wang, Y.: Texture classification using the support vector machines. Pattern Recognition 36 (12), 2883-2893 (2003).\n[8] Liu, R., Wang, Y., Baba, T., Masumoto, D., and Nagata, S.: SVMbased active feedback in image retrieval using clustering and unlabeled data. Pattern Recognition 41 (8), 2645-2655 (2008)\n[9] Michel, P., and Kaliouby, R.E.: Real time facial expresion recognition in video using support vector machines. In Proceedings of ICMI’03 258- 264, (2003).\n[10] Noble, W.S.: Support Vector Machine Applications in Computational Biology, MIT Press, 2004.\n[11] Shao, Y.H., Chen, W.J., and Deng, N.Y.: Nonparallel hyperplane support vector machine for binary classification problems. Information Sciences 263 (0), 22-35 (2014)\n[12] Shao, Y., and Lunetta, R.S.: Comparison of support vector machine, neural network, and CART algorithms for the land-cover classification using limited training data points. ISPRS Journal of Photogrammetry and Remote Sensing 70 (0), 78-87 (2012)\n[13] Vapnik, V.N.: The nature of statistical learning theory. Springer, New York, 1996.\n[14] Vapnik, V.N.: Statistical Learning Theory, John Wiley and Sons, New York, 1998.\n[15] Wang, D., Qi, X., Wen, S., and Deng., M.: SVM based fault classifier design for a water level control system. Proceedings of 2013 International Conference on Advanced Mechatronic Systems, 2013/9/25-2013/9/27, 2013/9/25, EI (2013)\n[16] Wang, D., Qi, X., Wen, S., Dan, Y., Ouyang, L., and Deng, M.: Robust nonlinear control and SVM classifier based fault diagnosis for a water level process. ICIC Express Letters 5 (1), (2014)\n[17] Wu, Y.C., Lee, Y.-S., and Yang, J.-C.: Robust and efficient multiclass SVM models for phrase pattern recognition. Pattern Recognition 41 (9), 2874-2889 (2008)\n[18] Weston, J., and Watkins, C.: Multi-class support vector machines. Technical Report CSD-TR- 98-04, Department of Computer Science, Royal Holloway, University of London 1998.\n[19] Wang, X.Y., Wang, T., and Bu, J.: Color image segmentation using pixel wise support vector machine classification. Pattern Recognition 44 (4), 777-787 (2011).\n[20] Xue, Z., Ming, D., Song, W., Wan, B., and Jin, S.: Infrared gait recognition based on wavelet transform and support vector machine. Pattern Recognition 43 (8), 2904-2910 (2010)\n[21] Zhao, Z., Liu, J., and Cox, J.: Safe and efficient screening for sparse support vector machine. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 14, pages 542–551, New York, NY, USA, (2014)\n[22] Zuo, R., and E.J.M.: Carranza, Support vector machine: A tool for mapping mineral prospectivity. Computers Geosciences 37 (12),1967- 1975 (2011)"
    } ],
    "references" : [ {
      "title" : "Model selection for the LS-SVM. Application to handwriting recognition",
      "author" : [ "M.M. Adankon", "M. Cheriet" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "An Introduction to support vector machines and other kernelbased learning methods",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2000
    }, {
      "title" : "Gene selection for cancer classification using support vector machines",
      "author" : [ "I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "Text categorization with support vector machines: Learning with many relevant features",
      "author" : [ "T. Joachims" ],
      "venue" : "In Proceedings of the European Conference on Machine Learning. Springer,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "A novel SVM+NDA model for classification with an application to face recognition",
      "author" : [ "N. Khan", "R. Ksantini", "Ahmad I", "B. Boufama" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Texture classification using the support vector machines",
      "author" : [ "Li", "S. Kwok", "J.T", "H. Zhu", "Y. Wang" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "SVMbased active feedback in image retrieval using clustering and unlabeled data",
      "author" : [ "R. Liu", "Y. Wang", "T. Baba", "D. Masumoto", "S. Nagata" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Real time facial expresion recognition in video using support vector machines",
      "author" : [ "P. Michel", "R.E. Kaliouby" ],
      "venue" : "In Proceedings of ICMI’03",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "Support Vector Machine Applications in Computational Biology",
      "author" : [ "W.S. Noble" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Nonparallel hyperplane support vector machine for binary classification problems",
      "author" : [ "Y.H. Shao", "W.J. Chen", "N.Y. Deng" ],
      "venue" : "Information Sciences",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Comparison of support vector machine, neural network, and CART algorithms for the land-cover classification using limited training data points",
      "author" : [ "Y. Shao", "R.S. Lunetta" ],
      "venue" : "ISPRS Journal of Photogrammetry and Remote Sensing",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "The nature of statistical learning theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1996
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "SVM based fault classifier design for a water level control system",
      "author" : [ "D. Wang", "X. Qi", "S. Wen", "M. Deng" ],
      "venue" : "Proceedings of 2013 International Conference on Advanced Mechatronic Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Robust nonlinear control and SVM classifier based fault diagnosis for a water level process",
      "author" : [ "D. Wang", "X. Qi", "S. Wen", "Y. Dan", "L. Ouyang", "M. Deng" ],
      "venue" : "ICIC Express Letters",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Robust and efficient multiclass SVM models for phrase pattern recognition",
      "author" : [ "Y.C. Wu", "Lee", "Y.-S", "Yang", "J.-C" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Multi-class support vector machines",
      "author" : [ "J. Weston", "C. Watkins" ],
      "venue" : "Technical Report CSD-TR- 98-04,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Color image segmentation using pixel wise support vector machine classification",
      "author" : [ "X.Y. Wang", "T. Wang", "J. Bu" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Infrared gait recognition based on wavelet transform and support vector machine",
      "author" : [ "Z. Xue", "D. Ming", "W. Song", "B. Wan", "S. Jin" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Safe and efficient screening for sparse support vector machine",
      "author" : [ "Z. Zhao", "J. Liu", "J. Cox" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Support vector machine: A tool for mapping mineral prospectivity",
      "author" : [ "R. Zuo", "E.J.M.: Carranza" ],
      "venue" : "Computers Geosciences",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1967
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "——————————————— 1 Support Vector Machine Over the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "——————————————— 1 Support Vector Machine Over the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "——————————————— 1 Support Vector Machine Over the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "——————————————— 1 Support Vector Machine Over the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 5,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 6,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 8,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 9,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 10,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 15,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 18,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 19,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 20,
      "context" : "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 13,
      "context" : "[15] presented SVM based fault classifier design for a water level control system.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "They also studied the SVM classifier based fault diagnosis for a water level process [16].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "+ b = 1 (w1, w2) [ 0 1 ]",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "and [ w11 w12 w21 w22 ] [ 0 1 ]",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "7), we get W = [ 1 1 1 1 ]",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "7), we get W = [ 1 1 1 1 ]",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "7), we get W = [ 1 1 1 1 ]",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "7), we get W = [ 1 1 1 1 ]",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "15), we get W = [ 4 3 4 3 4 3 4 3 ]",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "15), we get W = [ 4 3 4 3 4 3 4 3 ]",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "15), we get W = [ 4 3 4 3 4 3 4 3 ]",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "15), we get W = [ 4 3 4 3 4 3 4 3 ]",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "15), we get W = [ 4 3 4 3 4 3 4 3 ]",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "15), we get W = [ 4 3 4 3 4 3 4 3 ]",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "15), we get W = [ 4 3 4 3 4 3 4 3 ]",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "15), we get W = [ 4 3 4 3 4 3 4 3 ]",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "and B = [ 3 5 3 5 ]",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "and B = [ 3 5 3 5 ]",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "and B = [ 3 5 3 5 ]",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "and B = [ 3 5 3 5 ]",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "= [ 1 1 ]",
      "startOffset" : 2,
      "endOffset" : 9
    } ],
    "year" : 2016,
    "abstractText" : "——————————————————————————————– Abstract: In this paper, we study the support vector machine and introduced the notion of generalized support vector machine for classification of data. We show that the problem of generalized support vector machine is equivalent to the problem of generalized variational inequality and establish various results for the existence of solutions. Moreover, we provide various examples to support our results. ———————————————",
    "creator" : "LaTeX with hyperref package"
  }
}