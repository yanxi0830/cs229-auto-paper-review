{
  "name" : "1602.01910.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Multiplier Methods to Optimize Non-exhaustive, Overlapping Clustering",
    "authors" : [ "Yangyang Hou", "Joyce Jiyoung Whang", "David F. Gleich", "Inderjit S. Dhillon" ],
    "emails" : [ "dgleich}@purdue.edu", "jjwhang@skku.edu", "inderjit@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Traditional clustering algorithms, such as k-means, produce a disjoint, exhaustive clustering, i.e., the clusters are pairwise disjoint, and every data point is assigned to some cluster. However, in real-world datasets, the clusters may overlap with each other, and there are often outliers that should not belong to any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive, Overlapping K-Means) objective as a generalization of the k-means clustering objective that allows us to simultaneously identify overlapping clusters as well as outliers [24]. Hence, it produces a non-exhaustive clustering. Curiously, both operations appear to be necessary because the outliers induce non-obvious effects when the clusters are allowed to overlap. It has been shown that the NEO-K-Means objective is effective in\n∗Department of Computer Science, Purdue University. Email: {hou13, dgleich}@purdue.edu †Department of Computer Engineering, Sungkyunkwan University. Email: jjwhang@skku.edu ‡Department of Computer Science, The University of Texas at Austin. Email: inderjit@cs.utexas.edu\nfinding ground-truth clusters in data clustering problems. Furthermore, by considering a weighted and kernelized version of the NEO-K-Means objective, we can also tackle the problem of finding overlapping communities in social and information networks.\nThere are currently two practical methods to optimize the non-convex NEO-K-Means objective for large problems: the iterative NEO-K-Means algorithm [24] that generalizes Lloyd’s algorithm [14] and an augmented Lagrangian algorithm to optimize a non-convex, low-rank semidefinite programming (SDP) relaxation of the NEO-K-Means objective [11]. The iterative algorithm is fast, but it tends to get stuck into regions where the more sophisticated optimization methods can make further progress. The augmented Lagrangian method for the non-convex objective, when started from the output of the iterative algorithm, is able to make further progress on optimizing the objective function. In addition, the augmented Lagrangian method tends to achieve better F1 performance on identifying groundtruth clusters and produce better overlapping communities in real-world networks than the simple iterative algorithm [11]. In this paper, our goal is to improve upon the augmented Lagrangian method to optimize the low-rank SDP for the NEO-K-Means objective more quickly.\nThe optimization problem that results from the low-rank strategy on the convex SDP is a non-convex, quadratically constrained, bound-constrained problem. We consider two multiplier methods for this problem. The first method adds a proximal regularizer to the augmented Lagrangian method. This general strategy is called either the proximal augmented Lagrangian method (e.g., [12]) or the proximal method of multipliers [21]. The second method is an alternating direction method of multipliers (ADMM) strategy for our objective function. Both strategies, when specialized on the NEO-K-Means problem, have the potential to accelerate our solution process.\nThere is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13]. However, we were unable to identify any existing convergence guarantees for these methods that mapped to our specific instantiations with bound-\nar X\niv :1\n60 2.\n01 91\n0v 1\n[ cs\n.L G\n] 5\nF eb\n2 01\n6\nconstrained subproblems. Towards that end, we specialize a general convergence result about the proximal augmented Lagrangian or proximal method of multipliers due to Pennanen [19] to our algorithm. The resulting theorem is a general convergence result about the proximal augmented Lagrangian method for nonconvex problems with bound-constrained subproblems (Theorem 5.1). The proof involves adapting a few details from Pennanen to our case.\nWe evaluate the resulting methods on real-world problems where the existing augmented Lagrangian takes over an hour of computation time. The proximal augmented Lagrangian strategy tends to run about 3−6 times faster, and the ADMM strategy tends to run about 4− 13 times faster bringing the runtimes of these methods down into range of 5 to 10 minutes. The iterative method, in contrast, runs in seconds – so there is still a considerable gap between the approaches. That said, the optimization based approaches have runtimes that are reasonable for a pipeline-style analysis and cases where the data collection itself is highly timeconsuming as would be common in many datasets from the biological and physical sciences.\nIn summary: • We propose two algorithms to optimize the non-\nconvex problem for non-exhaustive, overlapping clustering: a proximal augmented Lagrangian method and an ADMM method. • We specialize a general convergence result about\nthe proximal method of multipliers for non-convex problems to the bound-constrained proximal augmented Lagrangian method to have a sound convergence theory. • We show that these new methods reduce the run-\ntime for problems where the classical augmented Lagrangian method takes over an hour to the range of 5 to 10 minutes with no change in quality. The rest of the paper is organized as follows. In Section 2, we review the NEO-K-Means objective and its low-rank SDP formulation, and in Section 3, we formally describe the classical augmented Lagrangian method. In Section 4, we present our two multiplier methods: the proximal augmented Lagrangian method, and an ADMM for the NEO-K-Means low-rank SDP. For the proximal augmented Lagrangian method, we present the convergence analysis in Section 5. In Section 6, we discuss simplified ADMM variants. Finally, we present experimental results in Section 7, and discuss future work in Section 8."
    }, {
      "heading" : "2 The NEO-K-Means Objective",
      "text" : "The goal of non-exhaustive, overlapping clustering is to find a set of cohesive clusters such that clusters are\nallowed to overlap with each other and outliers are not assigned to any cluster. That is, given a set of data points X = {x1,x2, ...,xn}, we find a set of clusters C1, C2, ..., Ck such that C1∪C2∪...∪Ck ⊆ X and Ci∩Cj 6= ∅ for some i 6= j.\nTo find such clusters, we proposed the NEO-KMeans objective function in [24]. The NEO-K-Means objective is an intuitive variation of the classical kmeans where two parameters α and β are introduced to control the amount of overlap and non-exhaustiveness, respectively. We also found that optimizing a weighted and kernelized NEO-K-Means objective is equivalent to optimizing normalized cuts for overlapping community detection [24].\nLet us define an assignment matrix U = [uij ]n×k such that uij = 1 if a data point xi belongs to Cj ; and uij = 0 otherwise. Let I{exp} denote the indicator function such that I{exp} = 1 if exp is true; 0 otherwise. Given a positive weight for each data point wi, and a nonlinear mapping φ, the weighted kernel NEO-KMeans objective function is defined as follows:\n(2.1)\nminimize U\n∑k c=1 ∑n i=1 uicwi‖φ(xi)−mc‖2\nwhere mc = ∑n\ni=1 uicwiφ(xi)∑n i=1 uicwi\nsubject to trace(UTU) = (1 + α)n,∑n i=1 I{(U1)i = 0} ≤ βn.\nThis objective function implies that (1 + α)n assignments are made while minimizing the sum of the squared distances between a data point and its cluster center. Also notice that at most βn data points are allowed to have no membership in any cluster. If α = 0 and β = 0, then this objective is equivalent to the classical weighted kernel k-means objective. Some guidelines about how to select α and β have been described in [24]. To optimize the objective function (2.1), a simple iterative algorithm has also been proposed in [24]. However, the simple iterative algorithm tends to get stuck at a local optimum that can be far away from the global optimum, like the standard k-means algorithm [14].\nThe following optimization problem is a non-convex relaxation of the NEO-K-Means problem that was developed in our previous work [11]. We call it the lowrank SDP based on its derivation as a low-rank heuristic for solving large-scale SDPs. We introduce a bit of notation to state the problem. Let K be a standard kernel matrix (Kij = φ(xi)\nTφ(xj)), let W denote a diagonal weight matrix such that Wii = wi indicates the weight of data point i, and let d denote a vector of length n where di = wiKii. In terms of the solution variables, let f be a length n vector where fi is a real-valued count of the number of clusters data point i is assigned to, and let g be a length n vector where gi is close to 0 if i\nshould not be assigned to any cluster and gi is close to 1 if i should be assigned to a cluster. The solution matrix Y represents a relaxed, normalized assignment matrix where Yij indicates that data point i should be in cluster j with columns normalized by the cluster size. The low-rank SDP optimization problem for (2.1) is then\n(2.2)\nminimize Y ,f ,g,s,r\nfTd− trace(Y TKY )\nsubject to k = trace(Y TW−1Y ) (a)\n0 = Y Y Te−W f (b) 0 = eT f − (1 + α)n (c) 0 = f − g − s (d) 0 = eTg − (1− β)n− r (e) Yi,j ≥ 0, s ≥ 0, r ≥ 0 0 ≤ f ≤ ke, 0 ≤ g ≤ 1\nwhere s and r are slack variables to convert the inequality constraints into equality constraints. The objective function is derived following a standard kernelized conversion. Constraint (a) gives the normalization condition on the variable Y to normalize for cluster-size; constraint (b) requires that the number of assignments listed in f corresponds to the number in the solution matrix Y ; constraint (c) bounds the total number of assignments as (1 + α)n; constraint (d) is equivalent to f ≥ g; and constraint (e) enforces the number of assigned data points to be at least (1−β)n; the remaining bound constraints enforce simple non-negativity and upper-bounds on the number of cluster assignments. We will discuss how to solve the low-rank SDP problem in the next section."
    }, {
      "heading" : "3 The Augmented Lagrangian Method for the NEO-K-Means Low-Rank SDP",
      "text" : "To solve the low-rank SDP problem (2.2), the classical augmented Lagrangian method (ALM) has been used in [11]. The augmented Lagrangian technique is an iterative process where each iteration is done by minimizing an augmented Lagrangian problem that includes a current estimate of the Lagrange multipliers for the constraints as well as a quadratic penalty term that enforces the feasibility of the solution. We introduce it here because we will draw heavily on the notation for our subsequent results.\nLet λ = [λ1;λ2;λ3] denote the Lagrange multipliers for the three scalar constraints (a), (c), (e). For the vector constraints (b) and (d), let µ and γ denote the corresponding Lagrange multipliers, respectively. Let σ be a positive penalty parameter. Then, the augmented Lagrangian for (2.2) is:\nLA(Y, f ,g, s, r;λ,µ,γ, σ) =\nfTd− trace(Y TKY )︸ ︷︷ ︸ the objective − λ1(trace(Y TW−1Y )− k)\n+ σ\n2 (trace(Y TW−1Y )− k)2\n− µT (Y Y Te−W f)\n+ σ\n2 (Y Y Te−W f)T (Y Y Te−W f)\n− λ2(eT f − (1 + α)n)\n+ σ\n2 (eT f − (1 + α)n)2\n− γT (f − g − s)\n+ σ\n2 (f − g − s)T (f − g − s)\n− λ3(eTg − (1− β)n− r)\n+ σ\n2 (eTg − (1− β)n− r)2\nAt each iteration of the augmented Lagrangian framework, the following subproblem is solved:\n(3.3)\nminimize LA(Y , f ,g, s, r;λ,µ,γ, σ) subject to Yi,j ≥ 0, s ≥ 0, r ≥ 0,\n0 ≤ f ≤ ke, 0 ≤ g ≤ 1.\nTo minimize the subproblem with respect to the variables Y , f , g, s, and r, we can use a limited-memory BFGS with bound constraints algorithm [6]. In [11], it has been shown that this technique produces reasonable solutions for the NEO-K-Means objective. In particular, when the clustering performance is evaluated on real-world datasets, this technique has been shown to be effective in finding the ground-truth clusters. Furthermore, by optimizing the weighted kernel NEO-KMeans, this technique is also able to find cohesive overlapping communities in real-world networks. The empirical success of the augmented Lagrangian framework motivates us to investigate developing faster solvers for the NEO-K-Means low-rank SDP problem, which will be discussed in the next section."
    }, {
      "heading" : "4 Fast Multiplier Methods for the NEO-K-Means Low-Rank SDP",
      "text" : "There is a resurgence of interest in proximal point methods and alternating methods for convex and nearly convex objectives in machine learning due to their fast convergence rate. Here we propose two variants of the classical augmented Lagrangian approach on problem (2.2) that can utilize some of these techniques for improved speed."
    }, {
      "heading" : "4.1 Proximal Augmented Lagrangian (PALM).",
      "text" : "The proximal augmented Lagrangian method differs\nfrom the classical augmented Lagrangian method only in an additional proximal regularization term for primal updates. This can be considered as a type of simultaneous primal-dual proximal-point step that helps to regularize the subproblems solved at each step. This idea leads to the following iterates:\nxk+1 = argmin x∈C\nLA(x;λk,µk,γk, σ) + 1\n2τ ‖x− xk‖2\nwhere x represents [y; f ; g; s; r] for simplicity with y = Y (:) vectorized by column. Then we update the multipliers λ,µ,γ as in the classical augmented Lagrangian. We may also need to update the penalty parameter σ and the proximal parameter τ respectively.\nWe use a limited-memory BFGS with bound constraints to solve the new subproblem with respect to the variable x. If we let τ = σ, this special case is called proximal method of multipliers, first introduced in [21]. The proximal method of multipliers has better theoretical convergence guarantees for convex optimization problems (compared with the augmented Lagrangian) [21]. In this non-convex setting, we believe it is likely to help to improve conditioning of the Hessian’s in the subproblems and thus reduce the solution time for each subproblem. And this is indeed what we find.\n4.2 Alternating Direction Method of Multipliers (ADMM). There are four sets of variables in problem (2.2) (Y , f , g and slack variables). We can use this structure to break the augmented Lagrangian function into smaller subproblems for each set of variables. Some of these subproblems are then easier to solve. For example, updating variable f alone is a simple convex problem, thus it is very efficient to have a globally optimal solution. The alternating direction method of multipliers approach of updating block variables Y , f , g, s and r respectively, utilizes this property, which leads to the following iterates:\nY k+1 = argmin Y LA(Y , fk,gk, sk, rk;\nλk,µk,γk, σ)\nfk+1 = argmin f LA(Y k+1, f ,gk, sk, rk;\nλk,µk,γk, σ)\ngk+1 = argmin g LA(Y k+1, fk+1,g, sk, rk;\nλk,µk,γk, σ)\nsk+1 = argmin s LA(Y k+1, fk+1,gk+1, s, rk;\nλk,µk,γk, σ)\nrk+1 = argmin r LA(Y k+1, fk+1,gk+1, sk+1, r;\nλk,µk,γk, σ)\nthen the multipliers λ, µ, γ and the penalty parameter σ are updated accordingly.\nWe expect that this strategy will aid convergence because it decouples the update of Y from the update of f . In the problem with all variables, the interaction of these terms has the strongest non-convex interaction. We now detail how we solve each of the subproblems.\nUpdate Y . We use a limited-memory BFGS with bound constraints to solve the subproblem with respect to the variables Y since it is non-convex.\nUpdate f and g. The update for f and g respectively both have the following general form:\n(4.4) minimize x f(x) = xTa +\nσ 2 xTDx + σ 2 (eTx)2\nsubject to 0 ≤ x ≤ be\nwhere e is the vector of all 1s and D is a positive diagonal matrix. To solve this, we use ideas similar to [18, S6.2.5]. Let τ = eTx, thus 0 ≤ τ ≤ bn. We solve this problem by finding roots of the following function F (τ):\nF (τ) = τ − eTP [− 1σD −1(a + στe); 0, b]\nwhere the function P [x; 0, b] projects the point x onto the rectangular box [0, b]. (To find these roots, bisection suffices because F (0) ≤ 0 and F (bn) ≥ 0.) This is a globally optimal solution by the following lemma.\nLemma 4.1. x∗ = P [− 1σD −1(a + στ∗e); 0, b], where τ∗ is the root of F (τ), satisfies the first order KKT conditions: x∗ − P [x∗ − ∇f(x∗); 0, b] = 0 (this form is given in equation 17.51 of [17]).\nProof. We have three cases: x∗i = 0; x ∗ i = b; and 0 < x∗i < b for any i. For x∗i = 0, which means ai + στ ≥ 0, we have\nx∗i − P [x∗i − (ai + σdix∗i + στ); 0, b] = − P [−ai − στ ; 0, b] = 0.\nFor x∗i = b, which means −(ai + στ)/(σdi) ≥ b, we have\nx∗i − P [x∗i − (ai + σdix∗i + στ); 0, b] = b− P [b− (ai + σdib+ στ); 0, b] = b− b = 0.\nFor 0 < x∗i < b, which means x ∗ i = −(ai+στ)/(σdi),\nwe have\nx∗i − P [x∗i − (ai + σdix∗i + στ); 0, b] = x∗i − P [x∗i ; 0, b] = x∗i − x∗i = 0.\nUpdate s and r. These updates just require solving one variable quadratic optimization with simple bound constraints; the result is a simple update procedure."
    }, {
      "heading" : "5 Convergence Analysis of the Proximal Augmented Lagrangian",
      "text" : "We use both the proximal augmented Lagrangian and the ADMM strategy on the problem without any convexity. For these cases, local convergence is the best we can achieve. We now establish a general convergence result for the proximal augmented Lagrangian with bound constraints. We observed empirical convergence of the ADMM method, but currently lack any theoretical guarantees.\nFrom Pennanen [19], we know that the proximal method of multipliers is locally convergent for a general class of problems with sufficient assumptions. We will show that our proximal method of multipliers algorithm applied to (2.2) can be handled by their approach and we extend their analysis to our case. Because we are imitating the analysis from Pennanen for a specific new problem, we decided to explicitly mimic the original language to highlight the changes in the derivation. Thus, there is a high degree of textual overlap between the following results and [19].\nFirst, we state some notation and one useful fact. The indication function δC of a set C in Hilbert Space H has value 0 for x ∈ C and +∞ otherwise. The subdifferential of δC is the normal cone operator of C: NC(x) = {v ∈ H|〈v,y − x〉 ≤ 0,∀y ∈ C}.\nProposition 5.1. Let x̄ be a solution to problem of minimizing f(x) on C and suppose f is differentiable at x̄, then\n∇f(x̄) +NC(x̄) 3 0.\nProof. We need to show that ∇f(x̄) + NC(x̄) 3 0 is equivalent to ∇f(x̄)T (y − x̄) ≥ 0 for all y ∈ C, which is clear from the definition of the normal cone.\nTo simplify the convergence behavior analysis of the proximal method of multipliers on (2.2), we generalize the optimization problem in the following form:\n(5.5)\nminimize x f(x)\nsubject to c(x) = 0,\nl ≤ x ≤ u\nwhere f(x) and c(x) are continuous and differentiable. Let C be the closed convex sets corresponding to simple bound constrains l ≤ x ≤ u.\nThe Lagrangian and augmented Lagrangian function are defined respectively as follows:\nL(x,λ) = f(x) + λT c(x)\nLA(x,λ, σ) = f(x) + λT c(x) + σ\n2 ‖c(x)‖2.\nAlgorithm 1 Proximal Method of Multipliers\n1: Input: Choose x0, λ0, set k = 0. 2: Repeat 3: xk+1 := argmin\nx∈C LA(x,λk, σk)\n4: + 12σk ‖x− xk‖ 2 (P k) 5: λk+1 := λk + σkc(xk+1) 6: k := k + 1 7: Until converged\nThe multipliers λ can be added or subtracted. We choose adding the multipliers here in order to be consistent with the analysis in [19].\nA point (x̄, λ̄) is said to satisfy the strong secondorder sufficient condition [20] for problem (5.5) if there is a ρ ∈ R such that\n(5.6) 〈ω,∇2xxL(x̄, λ̄)ω〉+ ρ ∑ i 〈∇ci(x̄),ω〉2 > 0\n∀ω ∈ TC(x̄)/{0}\nwhere TC(x) is the tangent cone of C at point x. We describe the proximal method of multipliers for the general form of problem (5.5) in Algorithm 1.\nTheorem 5.1. Let (x̄, λ̄) be a KKT pair for problem (5.5) satisfying the strongly second order sufficient condition and assume the gradients ∇c(x̄) are linearly independent. If the {σk} are large enough with σk → σ̄ ≤ ∞ and if ‖(x0,λ0)− (x̄, λ̄)‖ is small enough, then there exists a sequence {(xk,λk)} conforming to Algorithm 1 along with open neighborhoods Ck such that for each k, xk+1 is the unique solution in Ck to (P k). Then also, the sequence {(xk,λk)} converges linearly and Fejér monotonically to x̄, λ̄ with rate r(σ̄) < 1 that is decreasing in σ̄ and r(σ̄)→ 0 as σ̄ →∞.\nProof. (Note that the theorem and proof are revisions and specializations of Theorem 19 from [19].) By Robinson (1980, Theorem 4.1) [20], the strongly secondorder sufficient condition and the linear independence condition imply that the KKT system for (5.5) is strongly regular at (x̄, λ̄).\nWhen we solve the subproblem (P k) with explicit bound constraints, from Proposition 5.1, we actually solve\n∇f(x)+ 1 σk (x−xk)+NC(x)+∇c(x)∗(λk+σkc(x)) 3 0.\nThus, Algorithm 1 is equivalent to Algorithm 3 in [19] (their general algorithm), and by Theorem 17 of [19], we have the local convergence result stated in Theorem 5.1.\nIt remains to show that for large enough σk, the unique stationary point is in fact a minimizer of (P k).\nWe apply the second-order sufficient condition in 13.26 from [22] and the analogous derivation in the proof of Theorem 19 of [19]. Then a sufficient condition for xk+1 to be a local minimizer of (P k) is that\n〈ω,∇2xxL(xk+1,λk+1)ω〉+ 1\nσk ‖ω‖2+ σk ∑ i 〈∇ci(xk+1),ω〉2 > 0,∀ω ∈ TC(xk+1)/{0}.\nThis condition holds by the continuity of∇2xxL and∇ci, and by (5.6), provided σk is large enough.\nA main assumption for the analysis above is that we can solve the subproblem (P k) exactly. This was adjusted in [13], which showed local convergence for approximate solutions of (P k)."
    }, {
      "heading" : "6 Simplified Alternating Direction Method of Multipliers",
      "text" : "One downside to both of the proposed methods is that they involve using the L-BFGS-B method to solve the bound-constrained non-convex objectives in the substeps. This is a complex routine with a high runtime itself. In this section, we are interested in seeing if there are simplified ADMM variants that might futher improve runtime by avoiding this non-convex solver. This corresponds to, for example, inexact ADMM (allowing inexact primal alternating minimization solutions, e.g., one proximal gradient step per block).\nIn the ADMM method from Section 4.2, we know that updating the block variables f , g, s and r is simple and convex, so we can get globally optimal solutions. The only hard part is to update Y , which is non-convex. However, there are few results about convergence for ADMM in the non-convex case as well as the case with multiple blocks, i.e., more than two blocks of variables (e.g., Y , f , g, s and r) that would apply to this problem. For instance, in [7], it has been shown that an ADMM method does not converge for a multi-block case even for a convex problem.\nIn fact, in our preliminary investigations, many common variations on the ADMM methods did not yield any performance increase or resulted in slower performance or did not converge at all. For example, we tried to avoid the L-BFGS-B in the update for Y by simply using a step of projected gradient descent instead. We found the resulting Simplified ADMM (SADMM) method converges much slower than our ADMM method with the non-convex solver (more details are in Section 7.1). The same experiment with multiple steps of projected gradient descent only performed worse.\nTherefore, common accelerated variants of ADMM proposed for convex problems with two-block case do not necessarily improve the performance of ADMM in our problem. We believe that the NEO-K-means lowrank SDP problem will be a useful test case for future research in this area."
    }, {
      "heading" : "7 Experimental Results",
      "text" : "In this section, we demonstrate the efficiency of our proposed methods on real-world problems. Our primary goal is to compare our two new methods, PALM and ADMM with the classical augmented Lagrangian method (denoted by ALM) in terms of their ability to optimize (2.2). All these three algorithms are implemented in MATLAB and use the L-BFGS-B routine [6] written in Fortran to solve the bound-constrained nonlinear subproblems."
    }, {
      "heading" : "7.1 Convergence Analysis on the Karate Club",
      "text" : "Network. We first illustrate the convergence behavior of each of the methods on an overlapping community detection task in a graph. We use the Zachary’s karate club network [25] which is a small social network among 34 members of a karate club.\nIn Figure 1, (a) shows the infinity norm of the constraints vector and (b) shows the NEO-K-Means low-rank SDP objective function values defined in (2.2) as time progresses respectively. We set the infeasibility tolerance to be less than 10−3. Both of our methods, PALM and ADMM, achieve faster convergence than ALM in terms of both the feasibility of the solution and the objective function value mainly because the subproblems for L-BFGS-B are faster to solve. To demonstrate that the common variants of ADMM do not accelerate the convergence in our problem, we also compare with the simplified alternating direction method of multipliers (Section 6, denoted by SADMM). Note that for SADMM, we do not need to use L-BFGSB to solve the subproblems, instead, we use one single gradient-descent step to have the solution inexactly. It is clear to see that SADMM is much slower than ADMM, and even slower than ALM."
    }, {
      "heading" : "7.2 Data Clustering on Real-world Datasets.",
      "text" : "Next, we compare the three methods (ALM, PALM and ADMM) on larger datasets. We use three different datasets from [1]. The SCENE dataset [4] contains 2,407 scenes represented as feature vectors; the YEAST dataset [9] consists of 2,417 genes where the features are based on micro-array expression data and phylogenetic profiles; the MUSIC dataset [23] contains a set of 593 different songs. There are known ground-truth clusters on these datasets (we set k as the number of ground-\ntruth clusters; k=6 for MUSIC and SCENE, and k=14 for YEAST). The goal of this comparison is to demonstrate that PALM and ADMM have performance equivalent to the ALM method, while running substantially faster. We will also compare against the iterative NEOK-Means algorithm as a reference.\nWe initialize ALM, PALM, and ADMM using the iterative NEO-K-Means algorithm as also used in [11]. The parameters α and β in the NEO-K-Means are automatically estimated by the strategies proposed in [24]. This initialization renders the method sensitive to the local region selected by the iterative method, but this is usually a high-quality region. We use the procedure from [11] to round the real-valued solutions to discrete assignments. Briefly, this uses the solution vectors g and f to determine which points to assign and roughly how many clusters each data point resides in. Assignments are then greedily made based on values of the solution matrix Y . We run all the methods 25 times on the three datasets, and summarize the results in Figure 2. The results from these experiments illustrate the following points: • (Top-row – objective values) The PALM, ADMM,\nand ALM methods are all indistinguishable as far as their ability to optimize the objective of the NEO-K-Means low-rank SDP problem (2.2). • (Second-row – runtimes) Both the PALM and\nADMM methods are significantly faster than ALM on the larger two datasets, SCENE and YEAST. In particular, ADMM is more than 13 times faster on the SCENE dataset. Since the MUSIC dataset is relatively small, the speedup is also relatively small, but the two new methods, PALM and ADMM are consistently faster than ALM. Note that we do not expect any of the optimization-based methods will be faster than the iterative NEO-K-Means method\nsince it is a completely different type of algorithm (In particular, it optimizes the discretized objective). Thus, we conclude that the new optimization procedures (PALM and ADMM) are considerably faster than the ALM method while achieving similar objective function values.\nThe next investigation studies the discrete assignments produced by the methods. Here, we see that (third row of Figure 2) there are essentially no differences among any of the optimization methods (ALM, PALM, ADMM) in terms of their objective values after rounding to the discrete solution and evaluating the NEO-K-Means objective. The optimization methods outperform the iterative method on the YEAST dataset by a considerable margin.\nFinally, to see the clustering performance, we compute the F1 score which measures the matching between the algorithmic solutions and the ground-truth clusters in the last row of Figure 2. Higher F1 scores indicate better alignment with the ground-truth clusters. We also compare the results with other stateof-the-art overlapping clustering methods, MOC [3], ESA [15], and OKM [8]. On the YEAST dataset, MOC returns 13 empty clusters and one large cluster which contains all the data points. So, we do not report F1 score of MOC on this dataset. We first observe that the NEO-K-Means based methods (denoted by NEO-*) are able to achieve higher F1 scores than other methods. When we compare the performance among the three NEO-K-Means optimization methods (NEOALM, NEO-PALM, and NEO-ADMM), there is largely no difference among these methods except for the MUSIC dataset. On the MUSIC problem, the ADMM method has a slightly lower F1 score than PALM or ALM. This is because the objective values obtained by\nADMM on this dataset seem to be minutely higher than the other two optimization strategies and this manifests as a noticeable change in the F1 score. In this case, however, the scale of the variation is low and essentially, the results from all the NEO-K-Means based methods are equivalent. On the SCENE dataset, the iterative algorithm (NEO-iterative) can sometimes outperform the optimization methods in terms of F1 although we note that the median performance of the optimization is much better and there is essentially no difference between NEO-PALM, NEO-ADMM, and NEO-ALM. On the YEAST dataset, the reduced objective function value corresponds with an improvement in the F1 scores for notably better results than NEO-iterative."
    }, {
      "heading" : "8 Discussion",
      "text" : "Overall, the result from the previous section indicate that both the PALM and ADMM methods are faster than ALM with essentially no change in quality. Thus, we can easily recommend them instead of ALM for optimizing these low-rank objectives. There is still a substantial gap between the performance of the simple iterative algorithm and the optimization procedures we propose here. However, the optimization procedures avoid the worst-case behavior of the iterative method and result in more robust and reliable results as illustrated on the YEAST dataset and in other experiments from [11].\nIn terms of future opportunities, we are attempting to identify a convergence guarantee for the ADMM method in this non-convex case. This would put the fastest method we have for the optimization on firm theoretical ground. In terms of the clustering problem, we are exploring the integrality properties of the SDP relaxation itself [11]. Our goal here is to show a result akin to that proved in [2] about integrality in relaxations of the k-means objective. Finally, another goal we are pursuing involves understanding when our method can recover the partitions from an overlapping block-model with outliers. This should hopefully show that the optimization approaches have a wider recovery region than the simple iterative methods and provide a theoretical basis for empirically observed improvement.\nAcknowledgments This research was supported by NSF CAREER award CCF-1149756 to DG, and by NSF grants CCF-1117055 and CCF-1320746 to ID."
    } ],
    "references" : [ {
      "title" : "Relax",
      "author" : [ "P. Awasthi", "A.S. Bandeira", "M. Charikar", "R. Krishnaswamy", "S. Villar", "R. Ward" ],
      "venue" : "no need to round: integrality of clustering formulations. In ITCS, pages 191–200",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Model-based overlapping clustering",
      "author" : [ "A. Banerjee", "C. Krumpelman", "J. Ghosh", "S. Basu", "R.J. Mooney" ],
      "venue" : "KDD, pages 532–537",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning multi-label scene classification",
      "author" : [ "M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown" ],
      "venue" : "Pattern Recognition, 37(9):1757 – 1771",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Found. Trends Mach. Learn.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "A limited memory algorithm for bound constrained optimization",
      "author" : [ "R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu" ],
      "venue" : "SIAM J. Sci. Comput., 16(5):1190–1208",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "The direct extension of admm for multi-block convex minimization problems is not necessarily convergent",
      "author" : [ "C. Chen", "B. He", "Y. Ye", "X. Yuan" ],
      "venue" : "Math. Prog., pages 1–23",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An extended version of the k-means method for overlapping clustering",
      "author" : [ "G. Cleuziou" ],
      "venue" : "ICPR, pages 1–4",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A kernel method for multilabelled classification",
      "author" : [ "A. Elisseeff", "J. Weston" ],
      "venue" : "NIPS, pages 681–687",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A primal-dual regularized interior-point method for convex quadratic programs",
      "author" : [ "M. Friedlander", "D. Orban" ],
      "venue" : "Math. Prog. Comput., 4(1):71–107",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Nonexhaustive",
      "author" : [ "Y. Hou", "J.J. Whang", "D.F. Gleich", "I.S. Dhillon" ],
      "venue" : "overlapping clustering via low-rank semidefinite programming. In KDD, pages 427–436",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Carlos",
      "author" : [ "J. Humes" ],
      "venue" : "P. Silva, and B. Svaiter. Some inexact hybrid proximal augmented Lagrangian algorithms. Numerical Algorithms, 35(2-4):175–184",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Inexact variants of the proximal point algorithm without monotonicity",
      "author" : [ "A.N. Iusem", "T. Pennanen", "B.F. Svaiter" ],
      "venue" : "SIAM J. Optimiz., 13(4):1080–1097",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Least squares quantization in PCM",
      "author" : [ "S.P. Lloyd" ],
      "venue" : "IEEE Trans. Inf. Theory, 28(2):129–137",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Icdm workshops",
      "author" : [ "H. Lu", "Y. Hong", "W.N. Street", "F. Wang", "H. Tong" ],
      "venue" : "Overlapping clustering with sparseness constraints, pages 486–494",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the convergence of alternating direction Lagrangian methods for nonconvex structured optimization problems",
      "author" : [ "S. Magnússon", "P.C. Weeraddana", "M.G. Rabbat", "C. Fischione" ],
      "venue" : "IEEE Trans. Control Netw. Syst.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Numerical Optimization",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : "Springer, 2nd edition",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Proximal algorithms",
      "author" : [ "N. Parikh", "S. Boyd" ],
      "venue" : "Found. Trends Opt., 1(3):127–239",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Local convergence of the proximal point algorithm and multiplier methods without monotonicity",
      "author" : [ "T. Pennanen" ],
      "venue" : "Math. Oper. Res., 27(1):170–191",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Strongly regular generalized equations",
      "author" : [ "S.M. Robinson" ],
      "venue" : "Math. Oper. Res., 5(1):43–62",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Augmented Lagrangians and applications of the proximal point algorithm in convex programming",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : "Math. Oper. Res., 1(2):97–116",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Variational analysis",
      "author" : [ "R.T. Rockafellar", "R.J.-B. Wets" ],
      "venue" : "Springer",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multi-label classification of music into emotions",
      "author" : [ "K. Trohidis", "G. Tsoumakas", "G. Kalliris", "I.P. Vlahavas" ],
      "venue" : "ISMIR, pages 325–330",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Nonexhaustive",
      "author" : [ "J.J. Whang", "I.S. Dhillon", "D.F. Gleich" ],
      "venue" : "overlapping k-means. In SDM, pages 936–944",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An information flow model for conflict and fission in small groups",
      "author" : [ "W.W. Zachary" ],
      "venue" : "J. Anthropol. Res., 33(4):452–473",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1977
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "We recently proposed the NEO-K-Means (Non-Exhaustive, Overlapping K-Means) objective as a generalization of the k-means clustering objective that allows us to simultaneously identify overlapping clusters as well as outliers [24].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 22,
      "context" : "There are currently two practical methods to optimize the non-convex NEO-K-Means objective for large problems: the iterative NEO-K-Means algorithm [24] that generalizes Lloyd’s algorithm [14] and an augmented Lagrangian algorithm to optimize a non-convex, low-rank semidefinite programming (SDP) relaxation of the NEO-K-Means objective [11].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "There are currently two practical methods to optimize the non-convex NEO-K-Means objective for large problems: the iterative NEO-K-Means algorithm [24] that generalizes Lloyd’s algorithm [14] and an augmented Lagrangian algorithm to optimize a non-convex, low-rank semidefinite programming (SDP) relaxation of the NEO-K-Means objective [11].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 9,
      "context" : "There are currently two practical methods to optimize the non-convex NEO-K-Means objective for large problems: the iterative NEO-K-Means algorithm [24] that generalizes Lloyd’s algorithm [14] and an augmented Lagrangian algorithm to optimize a non-convex, low-rank semidefinite programming (SDP) relaxation of the NEO-K-Means objective [11].",
      "startOffset" : 336,
      "endOffset" : 340
    }, {
      "referenceID" : 9,
      "context" : "In addition, the augmented Lagrangian method tends to achieve better F1 performance on identifying groundtruth clusters and produce better overlapping communities in real-world networks than the simple iterative algorithm [11].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 10,
      "context" : ", [12]) or the proximal method of multipliers [21].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : ", [12]) or the proximal method of multipliers [21].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].",
      "startOffset" : 159,
      "endOffset" : 171
    }, {
      "referenceID" : 17,
      "context" : "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].",
      "startOffset" : 159,
      "endOffset" : 171
    }, {
      "referenceID" : 11,
      "context" : "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].",
      "startOffset" : 159,
      "endOffset" : 171
    }, {
      "referenceID" : 17,
      "context" : "Towards that end, we specialize a general convergence result about the proximal augmented Lagrangian or proximal method of multipliers due to Pennanen [19] to our algorithm.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 22,
      "context" : "To find such clusters, we proposed the NEO-KMeans objective function in [24].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "We also found that optimizing a weighted and kernelized NEO-K-Means objective is equivalent to optimizing normalized cuts for overlapping community detection [24].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "Some guidelines about how to select α and β have been described in [24].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "1), a simple iterative algorithm has also been proposed in [24].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "However, the simple iterative algorithm tends to get stuck at a local optimum that can be far away from the global optimum, like the standard k-means algorithm [14].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "The following optimization problem is a non-convex relaxation of the NEO-K-Means problem that was developed in our previous work [11].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "2), the classical augmented Lagrangian method (ALM) has been used in [11].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "To minimize the subproblem with respect to the variables Y , f , g, s, and r, we can use a limited-memory BFGS with bound constraints algorithm [6].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "In [11], it has been shown that this technique produces reasonable solutions for the NEO-K-Means objective.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "If we let τ = σ, this special case is called proximal method of multipliers, first introduced in [21].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "The proximal method of multipliers has better theoretical convergence guarantees for convex optimization problems (compared with the augmented Lagrangian) [21].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "51 of [17]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "From Pennanen [19], we know that the proximal method of multipliers is locally convergent for a general class of problems with sufficient assumptions.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 17,
      "context" : "Thus, there is a high degree of textual overlap between the following results and [19].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "We choose adding the multipliers here in order to be consistent with the analysis in [19].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "A point (x̄, λ̄) is said to satisfy the strong secondorder sufficient condition [20] for problem (5.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "(Note that the theorem and proof are revisions and specializations of Theorem 19 from [19].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "1) [20], the strongly secondorder sufficient condition and the linear independence condition imply that the KKT system for (5.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "Thus, Algorithm 1 is equivalent to Algorithm 3 in [19] (their general algorithm), and by Theorem 17 of [19], we have the local convergence result stated in Theorem 5.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "Thus, Algorithm 1 is equivalent to Algorithm 3 in [19] (their general algorithm), and by Theorem 17 of [19], we have the local convergence result stated in Theorem 5.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "26 from [22] and the analogous derivation in the proof of Theorem 19 of [19].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "26 from [22] and the analogous derivation in the proof of Theorem 19 of [19].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "This was adjusted in [13], which showed local convergence for approximate solutions of (P ).",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "For instance, in [7], it has been shown that an ADMM method does not converge for a multi-block case even for a convex problem.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "All these three algorithms are implemented in MATLAB and use the L-BFGS-B routine [6] written in Fortran to solve the bound-constrained nonlinear subproblems.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : "We use the Zachary’s karate club network [25] which is a small social network among 34 members of a karate club.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "The SCENE dataset [4] contains 2,407 scenes represented as feature vectors; the YEAST dataset [9] consists of 2,417 genes where the features are based on micro-array expression data and phylogenetic profiles; the MUSIC dataset [23] contains a set of 593 different songs.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "The SCENE dataset [4] contains 2,407 scenes represented as feature vectors; the YEAST dataset [9] consists of 2,417 genes where the features are based on micro-array expression data and phylogenetic profiles; the MUSIC dataset [23] contains a set of 593 different songs.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "The SCENE dataset [4] contains 2,407 scenes represented as feature vectors; the YEAST dataset [9] consists of 2,417 genes where the features are based on micro-array expression data and phylogenetic profiles; the MUSIC dataset [23] contains a set of 593 different songs.",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 9,
      "context" : "We initialize ALM, PALM, and ADMM using the iterative NEO-K-Means algorithm as also used in [11].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : "The parameters α and β in the NEO-K-Means are automatically estimated by the strategies proposed in [24].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "We use the procedure from [11] to round the real-valued solutions to discrete assignments.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "We also compare the results with other stateof-the-art overlapping clustering methods, MOC [3], ESA [15], and OKM [8].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "We also compare the results with other stateof-the-art overlapping clustering methods, MOC [3], ESA [15], and OKM [8].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "We also compare the results with other stateof-the-art overlapping clustering methods, MOC [3], ESA [15], and OKM [8].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "However, the optimization procedures avoid the worst-case behavior of the iterative method and result in more robust and reliable results as illustrated on the YEAST dataset and in other experiments from [11].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "In terms of the clustering problem, we are exploring the integrality properties of the SDP relaxation itself [11].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "Our goal here is to show a result akin to that proved in [2] about integrality in relaxations of the k-means objective.",
      "startOffset" : 57,
      "endOffset" : 60
    } ],
    "year" : 2016,
    "abstractText" : "Clustering is one of the most fundamental and important tasks in data mining. Traditional clustering algorithms, such as K-means, assign every data point to exactly one cluster. However, in real-world datasets, the clusters may overlap with each other. Furthermore, often, there are outliers that should not belong to any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive, Overlapping K-Means) objective as a way to address both issues in an integrated fashion. Optimizing this discrete objective is NPhard, and even though there is a convex relaxation of the objective, straightforward convex optimization approaches are too expensive for large datasets. A practical alternative is to use a low-rank factorization of the solution matrix in the convex formulation. The resulting optimization problem is non-convex, and we can locally optimize the objective function using an augmented Lagrangian method. In this paper, we consider two fast multiplier methods to accelerate the convergence of an augmented Lagrangian scheme: a proximal method of multipliers and an alternating direction method of multipliers (ADMM). For the proximal augmented Lagrangian or proximal method of multipliers, we show a convergence result for the non-convex case with bound-constrained subproblems. These methods are up to 13 times faster—with no change in quality—compared with a standard augmented Lagrangian method on problems with over 10,000 variables and bring runtimes down from over an hour to around 5 minutes.",
    "creator" : "LaTeX with hyperref package"
  }
}