{
  "name" : "1002.3086.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "peortega@dcc.uchile.cl", "dab54@cam.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 2.\n30 86\nv1 [\ncs .A\nI] 1\n6 Fe\nb 20\n10\nKeywords: Adaptive behavior, Intervention calculus, Bayesian control, Kullback-Leibler-divergence"
    }, {
      "heading" : "1. Introduction",
      "text" : "When the behavior of a plant under any control signal is fully known, then the designer can choose a controller that produces the desired dynamics. Instances of this problem include hitting a target with a cannon under known weather conditions, solving a maze having its map and controlling a robotic arm in a manufacturing plant. However, when the behavior of the plant is unknown, then the designer faces the problem of adaptive control. For example, shooting the cannon lacking the appropriate measurement equipment, finding the way out of an unknown maze and designing an autonomous robot for Martian exploration. Adaptive control turns out to be far more difficult than its nonadaptive counterpart. Even when the plant dynamics is known to belong to a particular class for which optimal controllers are available, constructing the corresponding optimal adaptive controller is in general\nCopyright 2010 by the authors.\nintractable even for simple toy problems (Duff, 2002). Thus, virtually all of the effort of the research community is centered around the development of tractable approximations.\nRecently, new formulations of the adaptive control problem that are based on the minimization of a relative entropy criterion have attracted the interest of the control and reinforcement learning community. For example, it has been shown that a large class of optimal control problems can be solved very efficiently if the problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov, 2006; 2009; Kappen et al., 2009). A similar approach minimizes the deviation of the causal input/outputrelationship of a Bayesian mixture of controllers from the true controller, obtaining an explicit solution called the Bayesian control rule (Ortega & Braun, 2010). This control rule is particularly interesting because it leads to stochastic controllers that infer the optimal controller on-line by combining the plant-specific controllers, implicitly using the uncertainty of the dynamics to trade-off exploration versus exploitation.\nAlthough the Bayesian control rule constitutes a promising approach to adaptive control, there are currently no proofs that guarantee its convergence to the desired policy. The aim of this paper is to develop a set of sufficient conditions of convergence and then to provide a proof. The analysis is limited to the simple case of controllers having a finite amount of modes of operation. Special care has been taken to illustrate the motivation behind the concepts."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "The exposition is restricted to the case of discrete time with discrete stochastic observations and control signals. Let O and A be two finite sets of symbols, where the former is the set of inputs (observations) and the second the set of outputs (actions). Actions and observations at time t are denoted as at ∈ A and ot ∈ O re-\nspectively, and the shorthand a≤t := a1, a2, . . . , at and the like are used to simplify the notation of strings. Symbols are underlined to glue them together as in ao≤t = a1, o1, a2, o2, . . . , at, ot. It is assumed that the interaction between the controller and the plant proceeds in cycles t = 1, 2, . . . where in cycle t the controller issues action at and the plant responds with an observation ot.\nA controller is defined as a probability distribution P over the input/output (I/O) stream, and it is fully characterized by the conditional probabilities\nP (at|ao<t) and P (ot|ao<tat)\nrepresenting the probabilities of emitting action at and collecting observation ot given the respective I/O history. Similarly, a plant is defined as a probability distribution Q characterized by the conditional probabilities Q(ot|ao<tat) representing the probabilities of emitting observation ot given the I/O history.\nIf the plant is known, i.e. if the conditional probabilities Q(ot|ao<tat) are known, then the designer can build a suitable controller by equating the observation streams as P (ot|ao<tat) = Q(ot|ao<tat) and by defining action probabilities P (at|ao<t) such that the resulting distribution P maximizes a desired utility criterion. In this case P is said to be tailored to Q. In many situations the conditional probabilities P (at|ao<t) will be deterministic, but there are cases (e.g. in repeated games) where the designer might prefer stochastic policies instead.\nIf the plant is unknown then one faces an adaptive control problem. Assume we know that the plant Qm is going to be drawn randomly from a set Q := {Qm}m∈M of possible plants indexed by M. Assume further we have available a set of controllers P := {Pm}m∈M, where each Pm is tailored to Qm. How can we now construct a controller P such that its behavior is as close as possible to the tailored controller Pm under any realization of Qm ∈ Q?"
    }, {
      "heading" : "3. Bayesian Control Rule",
      "text" : "A näıve approach would be to minimize the relative entropy of the controller P with respect to the true controller Pm, averaged over all possible values of m. However, this is syntactically incorrect. The important observation made in Ortega & Braun (2010) is that we do not want to minimize the deviation of P from Pm, but the deviation of the causal I/O dependencies in P from the causal I/O dependencies in Pm.\nIntuitively speaking, one does not want to predict actions and observations, but to predict the observations (effect) given actions (causes). More specifically, they propose to minimize a set of (causal) divergences C defined by\nC := lim sup t→∞\n∑\nm\nP (m)\nt ∑\nτ=1\nCτ (1)\nwhere\nCτ := ∑\no<τ\nPm(âo<τ )Cτ (âo<τ )\nCτ (h) := ∑\naτ\n∑\noτ\nPm(aoτ |h) log Pm(aoτ |h) P (aoτ |h) ,\nand where P (m) is the prior probability of m ∈ M, âτ denotes an intervened (not observed) action at time τ , and â1, â2, â3, . . . is an arbitrary sequence of intervened actions that gives rise to a particular instantiation of C.\nIn Ortega & Braun (2010), it is shown that the controller P that minimizes C in Equation (1) for any sequence of intervened actions is given by the conditional probabilities\nP (at|âo<τ ) := ∑\nm\nPm(at|ao<τ )P (m|âo<τ )\nP (ot|âo<τ ) := ∑\nm\nPm(ot|ao<τaτ )P (m|âo<τ ) (2)\nwhere\nP (m|âo≤t) := Pm(ot|ao<tat)P (m|âo<t) ∑\nm′ Pm′(ot|ao<tat)P (m′|ao<t) . (3)\nEquations (2) and (3) constitute the Bayesian control rule. This result is obtained by using properties of interventions using causal calculus (Pearl, 2000). It is worth to point out that the resulting controller is fully defined in terms of its constituent controllers in P . It is customary to use the notation\nP (at|m, ao<t) := Pm(at|ao<t) P (ot|m, ao<tat) := Pm(ot|ao<tat),\nthat is, treating the different controllers as “hypotheses” of a Bayesian model. In the context of the Bayesian control rule, these “I/O hypotheses” are called operation modes. Note that the resulting control law is in general stochastic."
    }, {
      "heading" : "4. Policy Diagrams",
      "text" : "A policy diagram is a useful informal tool to analyze the effect of control policies on plants. Figure 1, illustrates an example. One can imagine a plant as a\ncollection of states connected by transitions labeled by I/O symbols. For instance, Figure 1 highlights a state s where taking action a ∈ A and collecting observation o ∈ O leads to state s′. In a policy diagram, one abstracts away from the underlying details of the plant’s dynamics, representing sets of states and transitions as enclosed areas similar to a Venn diagram. Choosing a particular policy in a plant amounts to partially controlling the transitions taken in the state space, thereby choosing a subset of the plant’s dynamics. Accordingly, a policy is represented by a subset in state space (enclosed by a directed curve) as illustrated in Figure 1.\nPolicy diagrams are especially useful to analyze the effect of policies on different hypotheses about the plant’s dynamics. A controller that is endowed with a set of operation modes M can be seen as having hypotheses about the plant’s underlying dynamics, given by the observation models P (ot|m, ao<tat), and associated policies, given by the action models P (at|m, ao<t), for all m ∈ M. For the sake of simplifying the interpretation of policy diagrams, we will assume1 the existence of a state space S and a function T : (A × O) → S mapping I/O histories into states. With this assumption, policies and hypotheses can be seen as conditional probabilities\nP (at|m, s) := P (at|m, ao<t) and P (ot|m, s, at) := P (ot|m, ao<tat)\nrespectively, defining transition probabilities\nP (s′|m, s) = ∑\nS′\nP (aot|m, s)\nfor a Markov chain in the state space, where s = T (ao<t) and S ′ contains the transitions aot such that T (ao≤t) = s ′."
    }, {
      "heading" : "5. Divergence Processes",
      "text" : "One of the obvious questions to ask oneself with respect to the Bayesian control rule is whether it con-\n1Note however that no such assumptions are made to obtain the results of this paper.\nverges to the right control law or not. That is, whether P (at|âot) → P (at|m∗, ao<t) as t → ∞ when m∗ is the true operation mode, i.e. the operation mode such that P (at|m∗, ao<t) = Q(at|ao<t). As will be obvious from the discussion in the rest of this paper, this is in general not true.\nAs it is easily seen from Equation 2, showing convergence amounts to show that the posterior distribution P (m|âo<t) concentrates its probability mass on a subset of operation modesM∗ having essentially the same output stream as m∗,\n∑\nm∈M\nP (at|m, ao<t)P (m|âo<t)\n≈ ∑\nm∈M∗\nP (at|m∗, ao<t)P (m|âo<t)\n≈ P (at|m∗, ao<t).\nHence, understanding the asymptotic behavior of the posterior probabilities\nP (m|âo≤t)\nis the main goal of this paper. In particular, one wants to understand under what conditions these quantities converge to zero. The posterior can be rewritten as\nP (m|âo≤t) = P (âo≤t|m)P (m) ∑\nm′∈M P (âo≤t|m′)P (m′)\n= P (m)\n∏t τ=1 P (oτ |m, ao<τaτ )\n∑ m′∈M P (m ′) ∏t τ=1 P (oτ |m′, ao<τaτ ) .\nIf all the summands but the one with index m∗ are dropped from the denominator, one obtains the bound\nP (m|âo≤t) ≤ ln P (m)\nP (m∗)\nt ∏\nτ=1\nP (oτ |ao<τaτ |m) P (oτ |ao<τaτ |m∗) ,\nwhich is valid for all m∗ ∈ M. From this inequality, it is seen that it is convenient to analyze the behavior of the stochastic process\ndt(m ∗‖m) :=\nt ∑\nτ=1\nln P (oτ |m∗, ao<τaτ ) P (oτ |m, ao<τaτ )\nwhich is the divergence process of m from the reference m∗. Indeed, if dt(m ∗‖m) → ∞ as t → ∞, then\nlim t→∞\nP (m)\nP (m∗)\nt ∏\nτ=1\nP (oτ |ao<τaτ |m) P (oτ |ao<τaτ |m∗)\n= lim t→∞\nP (m)\nP (m∗) · e−dt(m∗‖m) = 0,\nand thus clearly P (m|âo≤t) → 0. Figure 2 illustrates simultaneous realizations of the divergence processes of a controller. Intuitively speaking, these processes provide lower bounds on accumulators of surprise value measured in information units.\nA divergence process is a random walk, i.e. whose value at time t depends on the whole history up to time t− 1. What makes them cumbersome to characterize is the fact that their statistical properties depend on the particular policy that is applied; hence, a given divergence process can have different growth rates depending on the policy (Figure 3). Indeed, the behavior of a divergence process might depend critically on the distribution over actions that is used. For example, it can happen that a divergence process stays stable under one policy, but diverges under another. In the context of the Bayesian control rule this problem is further aggravated, because in each time step, the policy to apply is determined stochastically. More specifically, if m∗ is the true operation mode, then dt(m\n∗‖m) is a random variable that depends on the realization ao≤t which is drawn from\nt ∏\nτ=1\nP (aτ |mτ , ao≤τ )P (oτ |m∗, ao≤τaτ ),\nwhere the m1,m2, . . . ,mt are drawn themselves from P (m1), P (m2|âo1), . . . , P (mt|âo<t). To deal with the heterogeneous nature of divergence processes, one can introduce a temporal decomposition that demultiplexes the original process into many sub-processes belonging to unique policies. Let Nt := {1, 2, . . . , t} be the set of time steps up to time t. Let T ⊂ Nt, and let m,m′ ∈ M. Define a sub-divergence of dt(m‖m) as a random variable\ng(m′; T ) := ∑\nτ∈T\nln P (oτ |m∗, ao<τaτ ) P (oτ |m, ao<τaτ )\ndrawn from\nPmm′({aoτ}τ∈T |{aoτ}τ∈T ∁) := ( ∏\nτ∈T\nP (aτ |m, ao<τ ) )( ∏\nτ∈T\nP (oτ |m′, ao<τaτ ) ) ,\nwhere T ∁ := Nt \\ T and where {aoτ}τ∈T ∁ are given conditions that are kept constant. In this definition, m′ plays the role of the policy that is used to sample the actions in the time steps T . Clearly, any realization of the divergence process dt(m\n∗‖m) can be decomposed into a sum of sub-divergences, i.e.\ndt(m ∗‖m) =\n∑\nm′\ng(m′; Tm′), (4)\nwhere {Tm}m∈M forms a partition of Nt. Figure 4 shows an example decomposition.\nThe averages of sub-divergences will play an important rôle in the analysis. Define the average over all realizations of g(m′; T ) as\nG(m′, T ) := ∑\n(ao τ )τ∈T\nPmm′({aoτ}τ∈T |{aoτ}τ∈T ∁)g(m′; T ).\nNotice that for any τ ∈ Nt,\nG(m′; {τ}) = ∑\nao τ\nP (aτ |m′, ao<τ )P (oτ |m∗, ao<τaτ )\n· ln P (oτ |m ∗, ao<τaτ )\nP (oτ |m, ao<τaτ ) ≥ 0,\nbecause of Gibbs’ inequality. In particular,\nG(m∗; {τ}) = 0.\nClearly, this holds as well for any T ⊂ Nt: ∀m′ G(m′; T ) ≥ 0,\nG(m∗; T ) = 0. (5)"
    }, {
      "heading" : "6. Boundedness",
      "text" : "In general, a divergence process is very complex: virtually all the classes of distributions that are of interest in control go well beyond i.i.d. and stationary processes. This increased complexity can jeopardize the analytic tractability of the divergence process, i.e. such that no predictions about its asymptotic behavior can be made anymore. More specifically, if the growth rates of the divergence processes vary too much from realization to realization, then the posterior distribution over operation modes can vary qualitatively between realizations. Hence, one needs to impose a stability requirement akin to ergodicity to limit the class of possible divergence-processes to a class that is analytically tractable. In the light of this insight, the following property is introduced.\nA divergence process dt(m ∗‖m) is said to be bounded in M iff for any δ > 0, there is a C ≥ 0, such that for all m′ ∈ M, all t and all T ⊂ Nt\n∣ ∣ ∣ g(m′; T )−G(m′; T ) ∣ ∣ ∣ ≤ C\nwith probability ≥ 1− δ. Figure 5 illustrates this property. Boundedness is the key property that is going to be used to construct the results of this paper. The first important result is that the posterior probability of the true operation mode is bounded from below.\nTheorem 1. Let the set of operation modes of a controller be such that for all m ∈ M the divergence process dt(m\n∗‖m) is bounded. Then, for any δ > 0, there is a λ > 0, such that for all t ∈ N,\nP (m∗|âo≤t) ≥ λ\n|M| with probability ≥ 1− δ.\nProof. As has been pointed out in (4), a particular realization of the divergence process dt(m\n∗‖m) can be decomposed as\ndt(m ∗‖m) =\n∑\nm′\ngm(m ′; Tm′),\nwhere the gm(m ′; Tm′) are sub-divergences of dt(m ∗‖m) and the Tm′ form a partition of Nt. However, since dt(m ∗‖m) is bounded in M, one has for all δ′ > 0, there is a C(m) ≥ 0, such that for all m′ ∈ M, all t ∈ Nt and all T ⊂ Nt, the inequality\n∣ ∣ ∣ gm(m ′; Tm′)−Gm(m′; Tm′) ∣ ∣ ∣ ≤ C(m)\nholds with probability ≥ 1− δ′. However, due to (5),\nGm(m ′; Tm′) ≥ 0\nfor all m′ ∈ M. Thus,\ngm(m ′; Tm′) ≥ −C(m).\nIf all the previous inequalities hold simultaneously then the divergence process can be bounded as well. That is, the inequality\ndt(m ∗‖m) ≥ −MC(m) (6)\nholds with probability ≥ (1 − δ′)M where M := |M|. Choose\nβ(m) := max{0, ln P (m)P (m∗)}.\nSince 0 ≥ ln P (m)P (m∗)−β(m), it can be added to the right hand side of (6). Using the definition of dt(m\n∗‖m), taking the exponential and rearranging the terms one obtains\nP (m∗) t ∏\nτ=1\nP (oτ |m∗, ao<τaτ )\n≥ e−α(m)P (m) t ∏\nτ=1\nP (oτ |m∗, ao<τaτ )\nwhere α(m) := MC(m) + β(m) ≥ 0. Identifying the posterior probabilities of m∗ and m by dividing both sides by the normalizing constant yields the inequality\nP (m∗|âo≤t) ≥ e−α(m)P (m|âo≤t).\nThis inequality holds simultaneously for all m ∈ M with probability ≥ (1 − δ′)M2 and in particular for λ := minm{e−α(m)}, that is,\nP (m∗|âo≤t) ≥ λP (m|âo≤t).\nBut since this is valid for any m ∈ M, and because maxm{P (m|âo≤t)} ≥ 1M , one gets\nP (m∗|âo≤t) ≥ λ\nM ,\nwith probability ≥ 1− δ for arbitrary δ > 0 related to δ′ through the equation δ′ := 1− M2 √ 1− δ."
    }, {
      "heading" : "7. Core",
      "text" : "If one wants to identify the operation modes whose posterior probabilities vanish, then it is not enough to characterize them as those whose hypothesis does not match the true hypothesis. Figure 6 illustrates this problem. Here, three hypotheses along with their associated policies are shown. H1 and H2 share the prediction made for region A but differ in region B. Hypothesis H3 differs everywhere from the others. Assume H1 is true. As long as we apply policy P2, hypothesis H3 will make wrong predictions and thus its divergence process will diverge as expected. However, no evidence against H2 will be accumulated. It is only when we apply policy P1 for long enough time that the controller will eventually enter region B and hence accumulate counter-evidence for H2.\nBut what does “long enough” mean? If P1 is executed only for a short period, then the controller risks not visiting the disambiguating region. But unfortunately, neither the right policy nor the right length of the period to run it are known beforehand. Hence, the controller needs a clever time-allocating strategy to test\nall policies for all finite time intervals. This motivates following definition.\nThe core of an operation mode m∗, denoted as [m∗], is the subset of M containing operation modes behaving like m∗ under its policy. More formally, an operation mode m /∈ [m∗] (i.e. is not in the core) iff for any C ≥ 0, δ, ξ > 0, there is a t0 ∈ N, such that for all t ≥ t0, G(m∗; T ) ≥ C with probability ≥ 1 − δ, where G(m∗; T ) is a subdivergence of dt(m\n∗‖m), and Pr{τ ∈ T } ≥ ξ for all τ ∈ Nt. In other words, if the controller was to apply m∗’s policy in each time step with probability at least ξ, and under this strategy the expected sub-divergence G(m∗; T ) of dt(m∗‖m) grows unboundedly, then m is not in the core of m∗. Note that demanding a strictly positive probability of execution in each time step guarantees that controller will run m∗ for all possible finite time-intervals. As the following theorem shows, the posterior probabilities of the operation modes that are not in the core vanish almost surely.\nTheorem 2. Let the set of operation modes of a controller be such that for all m ∈ M the divergence process dt(m\n∗‖m) is bounded. Then, if m /∈ [m∗], then P (m|âo≤t) → 0 as t → ∞ almost surely.\nProof. The divergence process dt(m ∗‖m) can be decomposed into a sum of sub-divergences (see Equation 4)\ndt(m ∗‖m) =\n∑\nm′\ng(m′; Tm′). (7)\nFurthermore, for every m′ ∈ M, one has that for all δ > 0, there is a C ≥ 0, such that for all t ∈ N and for all T ⊂ Nt\n∣ ∣ ∣ g(m′; T )−G(m′; T ) ∣ ∣ ∣ ≤ C(m)\nwith probability ≥ 1− δ′. Applying this bound to the summands in (7) yields the lower bound\n∑\nm′\ng(m′; Tm′) ≥ ∑\nm′\n( G(m′; Tm′)− C(m) )\nwhich holds with probability ≥ (1−δ′)M , where M := |M|. Due to Inequality 5, one has that for allm′ 6= m∗, G(m′; Tm′) ≥ 0. Hence,\n∑\nm′\n( G(m′; Tm′)− C(m) ) ≥ G(m∗; Tm∗)−MC\nwhere C := maxm{C(m)}. The members of the set Tm∗ are determined stochastically; more specifically,\nthe ith member is included into Tm∗ with probability P (m∗|âo≤i). But since m /∈ [m∗], one has that G(m∗; Tm∗) → ∞ as t → ∞ with probability ≥ 1− δ′ for arbitrarily chosen δ′ > 0. This implies that\nlim t→∞\ndt(m ∗‖m) ≥ lim t→∞ G(m∗; Tm∗)−MC ր ∞\nwith probability ≥ 1− δ, where δ > 0 is arbitrary and related to δ′ as δ = 1− (1− δ′)M+1. Using this result in the upper bound for posterior probabilities yields the final result\n0 ≤ lim t→∞ P (m|âo≤t) ≤ limt→∞ P (m) P (m∗) e−dt(m ∗‖m) = 0."
    }, {
      "heading" : "8. Consistency",
      "text" : "Even if an operation mode m is in the core of m∗, i.e. given that m is essentially indistinguishable from m∗ under m∗’s control, it can still happen that m∗ and m have different policies. Figure 7 shows an example of this. The hypotheses H1 and H2 share region A but differ in region B. In addition, both operation modes have their policies P1 and P2 respectively confined to region A. Note that both operation modes are in the core of each other. However, their policies are different. This means that it is unclear whether multiplexing the policies in time will ever disambiguate the two hypotheses. This is undesirable, as it could impede the convergence to the right control law.\nThus, it is clear that one needs to impose further restrictions on the mapping of hypotheses into policies. With respect to Figure 7, one can make the following observations:\n1. Both operation modes have policies that select subsets of region A. Therefore, the dynamics in A are preferred over the dynamics in B.\n2. Knowing that the dynamics in A are preferred over the dynamics in B allows to drop region B from the analysis when choosing a policy.\n3. Since both hypotheses agree in regionA, they have to choose the same policy in order to be consistent\nin their selection criterion.\nThis motivates the following definition. An operation mode m is said to be consistent with m∗ iff m ∈ [m∗] implies that for all ε < 0, there is a t0, such that for all t ≥ t0 and all ao<tat,\n∣ ∣ ∣ P (at|m∗, ao≤t)− P (at|m∗, ao≤t) ∣ ∣ ∣ < ε.\nIn other words, if m is in the core of m∗, then m’s policy has to converge to m∗’s policy. Intuitively, this property parallels the well-known sure-thing principle of expected utility theory (Savage, 1954). The following theorem shows that consistency is a sufficient condition for convergence to the right control law.\nTheorem 3. Let the set of operation modes of a controller be such that: for all m ∈ M the divergence process dt(m\n∗‖m) is bounded; and for all m,m′ ∈ M, m is consistent with m′. Then,\nP (at|âo≤t) → P (at|m∗, ao≤t) almost surely as t → ∞.\nProof. We will use the abbreviations pm(t) := P (at|m, âo≤t) and wm(t) := P (m|ao≤t). Decompose P (at|âo≤t) as\nP (at|âo≤t) = ∑\nm/∈[m∗]\npm(t)wm(t) + ∑\nm∈[m∗]\npm(t)wm(t).\n(8)\nThe first sum on the right-hand side is lower-bounded by zero and upper-bounded by\n∑\nm/∈[m∗]\npm(t)wm(t) ≤ ∑\nm/∈[m∗]\nwm(t)\nbecause pm(t) ≤ 1. Due to Theorem 2, wm(t) → 0 as t → ∞ almost surely. Given ε′ > 0 and δ′ > 0, let t0(m) be the time such that for all t ≥ t0(m), wm(t) < ε\n′. Choosing t0 := maxm{t0(m)}, the previous inequality holds for all m and t ≥ t0 simultaneously with probability ≥ (1− δ′)M . Hence,\n∑\nm/∈[m∗]\npm(t)wm(t) ≤ ∑\nm/∈[m∗]\nwm(t) < Mε ′. (9)\nTo bound the second sum in (8) one proceeds as follows. For every member m ∈ [m∗], one has that pm(t) → pm∗(t) as t → ∞. Hence, following a similar construction as above, one can choose t′0 such that for all t ≥ t′0 and m ∈ [m∗], the inequalities\n∣ ∣ ∣ pm(t)− pm∗(t) ∣ ∣ ∣ < ε′\nhold simultaneously for the precision ε′ > 0. Applying this to the first sum yields the bounds\n∑\nm∈[m∗]\n( pm∗(t)− ε′ ) wm(t)\n≤ ∑\nm∈[m∗]\npm(t)wm(t)\n≤ ∑\nm∈[m∗]\n( pm∗(t) + ε ′ ) wm(t).\nHere ( pm∗(t) ± ε′ )\nare multiplicative constants that can be placed in front of the sum. Note that\n1 ≥ ∑\nm∈[m∗]\nwm(t) = 1− ∑\nm/∈[m∗]\nwm(t) > 1− ε.\nDiligently using of the above inequalities allows simplifying the lower and upper bounds respectively:\n( pm∗(t)− ε′ )\n∑\nm∈[m∗]\nwm(t) > pm∗(t)(1 − ε′)− ε′\n≥ pm∗(t)− 2ε′, (\npm∗(t) + ε ′ )\n∑\nm∈[m∗]\nwm(t) ≤ pm∗(t) + ε′\n< pm∗(t) + 2ε ′.\n(10)\nCombining the inequalities (9) and (10) in (8) yields the final result:\n∣ ∣ ∣ P (at|âo≤t)− pm∗(t) ∣ ∣ ∣ < 3ε′ = ε,\nwhich holds with probability≥ 1−δ for arbitrary δ > 0 related to δ′ as δ′ = 1 − M √ 1− δ and arbitrary precision ε."
    }, {
      "heading" : "9. Summary and Conclusions",
      "text" : "The Bayesian control rule constitutes a promising approach to adaptive control based on the minimization of the relative entropy of the causal I/O distribution of a mixture controller from the true controller. In this work, a proof of convergence of the Bayesian control rule to the true controller is provided.\nAnalyzing the asymptotic behavior of a controllerplant dynamics could be perceived as a difficult problem that involves the consideration of domain-specific assumptions. Here it is shown that this is not the case: the asymptotic analysis can be recast as the study of concurrent divergence processes that determine the evolution of the posterior probabilities over operation modes, thus abstracting away from the details of the classes of I/O distributions. In particular,\nif the set of operation modes is finite, then two extra assumptions are sufficient to prove convergence. The first one, boundedness, imposes the stability of divergence processes under the partial influence of the policies contained within the set of operation modes. This condition can be regarded as an ergodicity assumption. The second one, consistency, requires that if a hypothesis makes the same predictions as another hypothesis within its most relevant subset of dynamics, then both hypotheses share the same policy. This relevance is formalized as the core of an operation mode.\nThe concepts and proof strategies developed in this work are appealing due to their intuitive interpretation and formal simplicity. Most importantly, they strengthen the intuition about potential pitfalls that arise in the context of controller design. The approach presented in this work can also be considered as a guide for possible extensions to infinite sets of operation modes. For example, one can think of partitioning a continuous space of operation modes into “essentially different” regions where representative operation modes subsume their neighborhoods (Grünwald, 2007).\nFinally, convergence proofs play a crucial rôle in the mathematical justification of any new theory of control. Hopefully, this proof will contribute to establish relative entropy control theories as solid alternative formulations to the problem of adaptive control."
    } ],
    "references" : [ {
      "title" : "Optimal learning: computational procedures for bayes-adaptive markov decision processes",
      "author" : [ "M.O. Duff" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Duff,? \\Q2002\\E",
      "shortCiteRegEx" : "Duff",
      "year" : 2002
    }, {
      "title" : "The Minimum Description Length Principle",
      "author" : [ "P. Grünwald" ],
      "venue" : null,
      "citeRegEx" : "Grünwald,? \\Q2007\\E",
      "shortCiteRegEx" : "Grünwald",
      "year" : 2007
    }, {
      "title" : "Optimal control as a graphical model inference problem",
      "author" : [ "B. Kappen", "V. Gomez", "M. Opper" ],
      "venue" : null,
      "citeRegEx" : "Kappen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kappen et al\\.",
      "year" : 2009
    }, {
      "title" : "A bayesian rule for adaptive control based on causal interventions",
      "author" : [ "P.A. Ortega", "D.A. Braun" ],
      "venue" : "In Proceedings of the third conference on general artificial intelligence,",
      "citeRegEx" : "Ortega and Braun,? \\Q2010\\E",
      "shortCiteRegEx" : "Ortega and Braun",
      "year" : 2010
    }, {
      "title" : "Causality: Models, Reasoning, and Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q2000\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 2000
    }, {
      "title" : "The Foundations of Statistics",
      "author" : [ "L.J. Savage" ],
      "venue" : null,
      "citeRegEx" : "Savage,? \\Q1954\\E",
      "shortCiteRegEx" : "Savage",
      "year" : 1954
    }, {
      "title" : "Linearly solvable markov decision problems",
      "author" : [ "E. Todorov" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Todorov,? \\Q2006\\E",
      "shortCiteRegEx" : "Todorov",
      "year" : 2006
    }, {
      "title" : "Efficient computation of optimal actions",
      "author" : [ "E. Todorov" ],
      "venue" : "Proceedings of the National Academy of Sciences U.S.A.,",
      "citeRegEx" : "Todorov,? \\Q2009\\E",
      "shortCiteRegEx" : "Todorov",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "intractable even for simple toy problems (Duff, 2002).",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "For example, it has been shown that a large class of optimal control problems can be solved very efficiently if the problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov, 2006; 2009; Kappen et al., 2009).",
      "startOffset" : 255,
      "endOffset" : 297
    }, {
      "referenceID" : 2,
      "context" : "For example, it has been shown that a large class of optimal control problems can be solved very efficiently if the problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov, 2006; 2009; Kappen et al., 2009).",
      "startOffset" : 255,
      "endOffset" : 297
    }, {
      "referenceID" : 4,
      "context" : "This result is obtained by using properties of interventions using causal calculus (Pearl, 2000).",
      "startOffset" : 83,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "Intuitively, this property parallels the well-known sure-thing principle of expected utility theory (Savage, 1954).",
      "startOffset" : 100,
      "endOffset" : 114
    } ],
    "year" : 2010,
    "abstractText" : "Recently, new approaches to adaptive control have sought to reformulate the problem as a minimization of a relative entropy criterion to obtain tractable solutions. In particular, it has been shown that minimizing the expected deviation from the causal input-output dependencies of the true plant leads to a new promising stochastic control rule called the Bayesian control rule. This work proves the convergence of the Bayesian control rule under two sufficient assumptions: boundedness, which is an ergodicity condition; and consistency, which is an instantiation of the surething principle.",
    "creator" : "dvips(k) 5.98 Copyright 2009 Radical Eye Software"
  }
}