{
  "name" : "1306.2035.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation∗",
    "authors" : [ "Martin Azizyan", "Aarti Singh", "Larry Wasserman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n20 35\nv1 [\nst at\n.M L\n] 9\nJ un\n2 01\n1 Introduction Gaussian mixture models provide a simple framework for several machine learning problems including clustering, density estimation and classification. Mixtures are especially appealing in high dimensional problems. Perhaps the most common use of Gaussian mixtures is for clustering. Of course, the statistical (and computational) behavior of these methods can degrade in high dimensions. Inspired by the success of variable selection methods in regression, several authors have considered variable selection for clustering. However, there appears to be no theoretical results justifying the advantage of variable selection in high dimensional setting.\nTo see why some sort of variable selection might be useful, consider clustering n subjects using a vector of d genes for each subject. Typically d is much larger than n which suggests that statistical clustering methods will perform poorly. However, it may be the case that there are only a small number of relevant genes in which case we might expect better behavior by focusing on this small set of relevant genes.\nThe purpose of this paper is to provide precise bounds on clustering error with mixtures of Gaussians. We consider both the general case where all features are relevant, and the special case where only a subset of features are relevant. Mathematically, we model an irrelevant feature by requiring the mean of that feature to be the same across clusters, so that the feature does not serve to differentiate the groups. Throughout this paper, we use the probability of misclustering an observation, relative to the optimal clustering if we had known the true distribution, as our loss function. This is akin to using excess risk in classification.\nThis paper makes the following contributions:\n• We provide information theoretic bounds on the sample complexity of learning a mixture of two isotropic Gaussians in the small mean separation setting that precisely captures the dimension dependence, and matches known sample complexity requirements for some existing algorithms. This also debunks the myth that there is a gap between statistical and computational complexity of learning mixture of two isotropic Gaussians for small mean separation. Our bounds require non-standard arguments since our loss function does not satisfy the triangle inequality.\n∗This work is supported in part by NSF grant IIS-1116458 and NSF CAREER award IIS-1252412.\n• We consider the high-dimensional setting where only a subset of relevant dimensions determine the mean separation between mixture components and demonstrate that learning is substantially easier as the sample complexity only depends on the sparse set of relevant dimensions. This provides some theoretical basis for feature selection approaches to clustering.\n• We show that a simple computationally feasible procedure nearly achieves the information theoretic sample complexity even in high-dimensional sparse mean separation settings.\nRelated Work. There is a long and continuing history of research on mixtures of Gaussians. A complete review is not feasible but we mention some highlights of the work most related to ours.\nPerhaps the most popular method for estimating a mixture distribution is maximum likelihood. Unfortunately, maximizing the likelihood is NP-Hard. This has led to a stream of work on alternative methods for estimating mixtures. These new algorithms use pairwise distances, spectral methods or the method of moments.\nPairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be √ d while the latter two improve it to d1/4. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. The assumption that the components are spherical was removed in Brubaker and Vempala (2008). Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = Ω(d4 log d) samples. Other spectral methods include Kannan et al. (2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm.\nKalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting µ > 1, Chaudhuri et al. (2009) show that n = Ω̃(d/µ2) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and µ. When the mean separation is small µ < 1, they show that n = Ω̃(d/µ4) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition.\nMost of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation.\nWe should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier.\nFinally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al. (2012) and Guo et al. (2010). The applied bioinformatics literature also contains a huge number of heuristic methods for this problem. None of these papers provide minimax bounds for the clustering error or provide theoretical evidence of the benefit of using variable selection in unsupervised problems such as clustering.\n2 Problem Setup\nIn this paper, we consider the simple setting of learning a mixture of two isotropic Gaussians with equal mixing weights, given n data points X1, . . . , Xn ∈ Rd drawn i.i.d. from a d-dimensional mixture density function\npθ(x) = 1\n2 f(x;µ1, σ\n2I) + 1\n2 f(x;µ2, σ\n2I),\nwhere f(·;µ,Σ) is the density of N (µ,Σ), σ > 0 is a fixed constant, and θ := (µ1, µ2) ∈ Θ. We consider two classes Θ of parameters:\nΘλ = {(µ1, µ2) : ‖µ1 − µ2‖ ≥ λ} Θλ,s = {(µ1, µ2) : ‖µ1 − µ2‖ ≥ λ, ‖µ1 − µ2‖0 ≤ s} ⊆ Θλ.\nThe first class defines mixtures where the components have a mean separation of at least λ > 0. The second class defines mixtures with mean separation λ > 0 along a sparse set of s ∈ {1, . . . , d} dimensions. Also, let Pθ denote the probability measure corresponding to pθ. Throughout the paper, we will use φ and Φ to denote the standard normal density and distribution functions.\nFor a mixture with parameter θ, the Bayes optimal classification, that is, assignment of a point x ∈ Rd to the correct mixture component, is given by the function\nFθ(x) = argmax i∈{1,2}\nf(x;µi, σ 2I).\nGiven any other candidate assignment function F : Rd → {1, 2}, we define the loss incurred by F as\nLθ(F ) = min π\nPθ({x : Fθ(x) 6= π(F (x))})\nwhere the minimum is over all permutations π : {1, 2} → {1, 2}. This is the probability of misclustering relative to an oracle that uses the true distribution to do optimal clustering.\nWe denote by F̂n any assignment function learned from the data X1, . . . , Xn, also referred to as estimator. The goal of this paper is to quantify how the minimax expected loss (worst case expected loss for the best estimator)\nRn ≡ inf F̂n sup θ∈Θ EθLθ(F̂n)\nscales with number of samples n, the dimension of the feature space d, the number of relevant dimensions s, and the signal-to-noise ratio defined as the ratio of mean separation to standard deviation λ/σ. We will also demonstrate a specific estimator that achieves the minimax scaling.\nFor the purposes of this paper, we say that feature j is irrelevant if µ1(j) = µ2(j). Otherwise we say that feature j is relevant.\n3 Minimax Bounds\n3.1 Small mean separation setting without sparsity\nWe begin without assuming any sparsity, that is, all features are relevant. In this case, comparing the projections of the data to the projection of the sample mean onto the first principal component suffices to achieve both minimax optimal sample complexity and clustering loss.\nTheorem 1 (Upper bound). Define\nF̂n(x) = { 1 if xT v1(Σ̂n) ≥ µ̂Tnv1(Σ̂n) 2 otherwise.\nwhere µ̂n = n−1 ∑n i=1 Xi is the sample mean, Σ̂n = n −1 ∑n\ni=1(Xi − µ̂n)(Xi − µ̂n)T is the sample covariance and v1(Σ̂n) denotes the eigenvector corresponding to the largest eigenvalue of Σ̂n. If n ≥ max(68, 4d), then\nsup θ∈Θλ\nEθLθ(F̂ ) ≤ 600max ( 4σ2\nλ2 , 1\n)√ d log(nd)\nn .\nFurthermore, if λσ ≥ 2max(80, 14 √ 5d), then\nsup θ∈Θλ EθLθ(F̂ ) ≤ 17 exp ( − n 32 ) + 9 exp ( − λ 2 80σ2 ) .\nTheorem 2 (Lower bound). Assume that d ≥ 9 and λσ ≤ 0.2. Then\ninf F̂n sup θ∈Θλ\nEθLθ(F̂n) ≥ 1\n500 min\n{√ log 2\n3\nσ2 λ2 √ d− 1 n , 1 4 } .\nWe believe that some of the constants (including lower bound on d and exact upper bound on λ/σ) can be tightened, but the results demonstrate matching scaling behavior of clustering error with d, n and λ/σ. Thus, we see (ignoring constants and log terms) that\nRn ≈ σ2\nλ2\n√ d\nn , or equivalently n ≈ d λ4/σ4 for a constant target value of Rn.\nThe result is quite intuitive: the dependence on dimension d is as expected. Also we see that the rate depends in a precise way on the signal-to-noise ratio λ/σ. In particular, the results imply that we need d ≤ n.\nIn modern high-dimensional datasets, we often have d > n i.e. large number of features and not enough samples. However, inference is usually tractable since not all features are relevant to the learning task at hand. This sparsity of relevant feature set has been successfully exploited in supervised learning problems such as regression and classification. We show next that the same is true for clustering under the Gaussian mixture model.\n3.2 Sparse and small mean separation setting\nNow we consider the case where there are s < d relevant features. Let S denote the set of relevant features. We begin by constructing an estimator Ŝn of S as follows. Define\nτ̂n = 1 + α\n1− α mini∈{1,...,d} Σ̂n(i, i),\nwhere\nα =\n√ 6 log(nd)\nn +\n2 log(nd)\nn .\nNow let Ŝn = {i ∈ {1, . . . , d} : Σ̂n(i, i) > τ̂n}.\nNow we use the same method as before, but using only the features in Ŝn identified as relevant.\nTheorem 3 (Upper bound). Define\nF̂n(x) =\n{ 1 if xT\nŜn v1(Σ̂Ŝn) ≥ µ̂ T Ŝn v1(Σ̂Ŝn)\n2 otherwise\nwhere Ŝn is the estimated set of relevant dimensions, xŜn are the coordinates of x restricted to the dimensions in Ŝn, and µ̂Ŝn and Σ̂Ŝn are the sample mean and covariance of the data restricted to the dimensions in Ŝn. If n ≥ max(68, 4s), d ≥ 2 and α ≤ 14 , then\nsup θ∈Θλ,s\nEθLθ(F̂ ) ≤ 603max ( 16σ2\nλ2 , 1\n)√ s log(ns)\nn + 220\nσ √ s\nλ\n( log(nd)\nn\n) 1 4\n.\nNext we find the lower bound.\nTheorem 4 (Lower bound). Assume that λσ ≤ 0.2, d ≥ 17, and that 5 ≤ s ≤ d−14 + 1. Then\ninf F̂n sup θ∈Θλ,s\nEθLθ(F̂n) ≥ 1\n600 min\n{√ 8\n45\nσ2 λ2 √ s− 1 n log ( d− 1 s− 1 ) , 1 2 } .\nWe remark again that the constants in our bounds can be tightened, but the results suggest that\nσ\nλ\n( s2 log d\nn\n)1/4 ≻ Rn ≻ σ2\nλ2\n√ s log d\nn ,\nor n = Ω\n( s2 log d\nλ4/σ4\n) for a constant target value of Rn.\nIn this case, we have a gap between the upper and lower bounds for the clustering loss. Also, the sample complexity can possibly be improved to scale as s (instead of s2) using a different method. However, notice that the dimension only enters logarithmically. If the number of relevant dimensions is small then we can expect good rates. This provides some justification for feature selection. We conjecture that the lower bound is tight and that the gap could be closed by using a sparse principal component method as in Vu and Lei (2012) to find the relevant features. However, that method is combinatorial and so far there is no known computationally efficient method for implementing it with similar guarantees.\nWe note that the upper bound is achieved by a two-stage method that first finds the relevant dimensions and then estimates the clusters. This is in contrast to the methods described in the introduction which do clustering and variable selection simultaneously. This raises an interesting question: is it always possible to achieve the minimax rate with a two-stage procedure or are there cases where a simultaneous method outperforms a two-stage procedure? Indeed, it is possible that in the case of general covariance matrices (non-spherical) two-stage methods might fail. We hope to address this question in future work.\n4 Proofs of the Lower Bounds\nThe lower bounds for estimation problems rely on a standard reduction from expected error to hypothesis testing that assumes the loss function is a semi-distance, which the clustering loss isn’t. However, a local triangle inequality-type bound can be shown (Proposition 2). This weaker condition can then be used to lower-bound the expected loss, as stated in Proposition 1 (which follows easily from Fano’s inequality).\nThe proof techniques of the sparse and non-sparse lower bounds are almost identical. The main difference is that in the non-sparse case, we use the Varshamov–Gilbert bound (Lemma 1) to construct a set of sufficiently dissimilar hypotheses, whereas in the sparse case we use an analogous result for sparse hypercubes (Lemma 2). See the appendix for complete proofs of all results.\nLemma 1 (Varshamov–Gilbert bound). Let Ω = {0, 1}m for m ≥ 8. There exists a subset {ω0, ..., ωM} ⊆ Ω such that ω0 = (0, ..., 0), ρ(ωi, ωj) ≥ m8 for all 0 ≤ i < j ≤ M , and M ≥ 2m/8, where ρ denotes the Hamming distance between two vectors (Tsybakov (2009)).\nLemma 2. Let Ω = {ω ∈ {0, 1}m : ‖ω‖0 = s} for integers m > s ≥ 1 such that s ≤ m/4. There exist ω0, ..., ωM ∈ Ω such that ρ(ωi, ωj) > s/2 for all 0 ≤ i < j ≤ M , and log(M + 1) ≥ s5 log ( m s ) (Massart (2007), Lemma 4.10).\nProposition 1. Let θ0, ..., θM ∈ Θλ (or Θλ,s), M ≥ 2, 0 < α < 1/8, and γ > 0. If for all 1 ≤ i ≤ M , KL(Pθi , Pθ0) ≤ α logMn , and if Lθi(F̂ ) < γ implies Lθj(F̂ ) ≥ γ for all 0 ≤ i 6= j ≤ M and clusterings F̂ , then inf F̂n maxi∈[0..M ] EθiLθi(F̂n) ≥ 0.07γ. Proposition 2. For any θ, θ′ ∈ Θλ, and any clustering F̂ , let τ = Lθ(F̂ ) + √ KL(Pθ, Pθ′)/2. If Lθ(Fθ′) + τ ≤ 1/2, then Lθ(Fθ′)− τ ≤ Lθ′(F̂ ) ≤ Lθ(Fθ′) + τ. We will also need the following two results. Let θ = (µ0 − µ/2, µ0 + µ/2) and θ′ = (µ0 − µ′/2, µ0 + µ′/2) for µ0, µ, µ ′ ∈ Rd such that ‖µ‖ = ‖µ′‖, and let cosβ = |µ Tµ′| ‖µ‖2 . Proposition 3. Let g(x) = φ(x)(φ(x) − xΦ(−x)). Then 2g (\n‖µ‖ 2σ ) sinβ cosβ ≤ Lθ(Fθ′) ≤ tan βπ .\nProposition 4. Let ξ = ‖µ‖2σ . Then KL(Pθ, Pθ′) ≤ ξ4(1− cosβ).\nProof of Theorem 2. Let ξ = λ2σ , and define ǫ = min {√ log 2 3 σ2 λ 1√ n , λ 4 √ d−1 } . Define λ20 = λ\n2 − (d − 1)ǫ2. Let Ω = {0, 1}d−1. For ω = (ω(1), ..., ω(d − 1)) ∈ Ω, let µω = λ0ed + ∑d−1 i=1 (2ω(i) − 1)ǫei (where {ei}di=1 is the\nstandard basis for Rd). Let θω = ( −µω2 , µω 2 ) ∈ Θλ.\nBy Proposition 4, KL(Pθω , Pθν ) ≤ ξ4(1 − cosβω,ν) where cosβω,ν = 1 − 2ρ(ω,ν)ǫ 2 λ2 and ρ is the Hamming\ndistance, so KL(Pθω , Pθν ) ≤ ξ4 2(d−1)ǫ 2 λ2 . By Proposition 3, since cosβω,ν ≥ 12 ,\nLθω(Fθν ) ≤ 1\nπ tanβω,ν ≤\n4\nπ √ d− 1ǫ λ , and\nLθω(Fθν ) ≥ 2g(ξ) sinβω,ν cosβω,ν ≥ √ 2g(ξ)\n√ ρ(ω, ν)ǫ\nλ\nwhere g(x) = φ(x)(φ(x) − xΦ(−x)). By Lemma 1, there exist ω0, ..., ωM ∈ Ω such that M ≥ 2(d−1)/8 and ρ(ωi, ωj) ≥ d−18 for all 0 ≤ i < j ≤ M . For simplicity of notation, let θi = θωi for all i ∈ [0..M ]. Then, for i 6= j ∈ [0..M ],\nKL(Pθi , Pθj ) ≤ ξ4 2(d− 1)ǫ2\nλ2 , Lθi(Fθj ) ≤\n4\nπ √ d− 1ǫ λ and Lθi(Fθj ) ≥ 1 2 g(ξ) √ d− 1ǫ λ .\nDefine γ = 14 (g(ξ)− 2ξ2) √ d−1ǫ λ . Then for any i 6= j ∈ [0..M ], and any F̂ such that Lθi(F̂ ) < γ,\nLθi(Fθj ) + Lθi(F̂ ) +\n√ KL(Pθi , Pθj )\n2 <\n( 4\nπ +\n1 4 (g(ξ)− 2ξ2) + ξ2 ) √ d− 1ǫ λ ≤ 1 2\nbecause, for ξ ≤ 0.1, by definition of ǫ, ( 4\nπ +\n1 4 (g(ξ)− 2ξ2) + ξ2 ) √ d− 1ǫ λ ≤ 2 √ d− 1ǫ λ ≤ 1 2 .\nSo, by Proposition 2, Lθj (F̂ ) ≥ γ. Also, KL(Pθi , Pθ0) ≤ (d − 1)ξ4 2ǫ 2 λ2 ≤ logM 9n for all 1 ≤ i ≤ M , because, by definition of ǫ, ξ4 2ǫ 2\nλ2 ≤ log 2 72n . So by Proposition 1 and the fact that ξ ≤ 0.1,\ninf F̂n max i∈[0..M ]\nEθiLθi(F̂n) ≥ 0.07γ ≥ 1\n500 min\n{√ log 2\n3\nσ2 λ2 √ d− 1 n , 1 4 }\nand to complete the proof we use supθ∈Θλ EθLθ(F̂n) ≥ maxi∈[0..M ] EθiLθi(F̂n) for any F̂n. Proof of Theorem 4. For simplicity, we state this construction for Θλ,s+1, assuming 4 ≤ s ≤ d−14 . Let ξ = λ2σ ,\nand define ǫ = min {√ 8 45 σ2 λ √ 1 n log ( d−1 s ) , 12 λ√ s } . Define λ20 = λ 2 − sǫ2. Let Ω = {ω ∈ {0, 1}d−1 : ‖ω‖0 = s}. For ω = (ω(1), ..., ω(d − 1)) ∈ Ω, let µω = λ0ed + ∑d−1\ni=1 ω(i)ǫei (where {ei}di=1 is the standard basis for Rd). Let θω = ( −µω2 , µω 2 ) ∈ Θλ,s. By Lemma 2, there exist ω0, ..., ωM ∈ Ω such that log(M + 1) ≥ s5 log ( d−1 s ) and ρ(ωi, ωj) ≥ s2 for all 0 ≤ i < j ≤ M . The remainder of the proof is analogous to that of Theorem 2 with γ = 14 (g(ξ)− √ 2ξ2) √ sǫ λ .\n5 Proofs of the Upper Bounds\nPropositions 5 and 6 below bound the error in estimating the mean and principal direction, and can be obtained using standard concentration bounds and a variant of the Davis–Kahan theorem. Proposition 7 relates these errors to the clustering loss. For the sparse case, Propositions 8 and 9 bound the added error induced by the support estimation procedure. See appendix for proof details.\nProposition 5. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ . For any δ > 0, we have ‖µ0 − µ̂n‖ ≥ σ √ 2max(d,8 log 1 δ ) n + ‖µ‖ √ 2 log 1 δ n with probability at least 1− 3δ.\nProposition 6. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ with d > 1 and n ≥ 4d. Define cosβ = |v1(σ2I + µµT )T v1(Σ̂n)|. For any 0 < δ < d−1√e , if max ( σ2 ‖µ‖2 , σ ‖µ‖ )√ max(d,8 log 1 δ )\nn ≤ 1160 , then with probability at least 1− 12δ − 2 exp ( − n20 ) ,\nsinβ ≤ 14max ( σ2\n‖µ‖2 , σ ‖µ‖\n)√ d\n√ 10\nn log\nd δ max\n( 1, 10\nn log\nd\nδ\n) .\nProposition 7. Let θ = (µ0 − µ, µ0 + µ), and for some x0, v ∈ Rd with ‖v‖ = 1, let F̂ (x) = 1 if xT v ≥ xT0 v, and 2 otherwise. Define cosβ = |vTµ|/‖µ‖. If |(x0 − µ0)T v| ≤ σǫ1 + ‖µ‖ǫ2 for some ǫ1 ≥ 0 and 0 ≤ ǫ2 ≤ 14 , and if sinβ ≤ 1√\n5 , then\nLθ(F̂ ) ≤ exp { −1 2 max ( 0, ‖µ‖ 2σ − 2ǫ1 )2}[ 2ǫ1 + ǫ2 ‖µ‖ σ + 2 sinβ ( 2 sinβ ‖µ‖ σ + 1 )] .\nProof. Let r = ∣∣∣ (x0−µ0)\nT v cos β\n∣∣∣. Since the clustering loss is invariant to rotation and translation,\nLθ(F̂ ) ≤ 1\n2\n∫ ∞\n−∞\n1 σ φ (x σ )[ Φ (‖µ‖+ |x| tanβ + r σ ) − Φ (‖µ‖ − |x| tanβ − r σ )] dx\n≤ ∫ ∞\n−∞ φ(x)\n[ Φ (‖µ‖ σ ) − Φ (‖µ‖ − r σ − |x| tanβ )] dx.\nSince tanβ ≤ 12 and ǫ2 ≤ 14 , we have r ≤ 2σǫ1+2‖µ‖ǫ2, andΦ ( ‖µ‖ σ ) −Φ ( ‖µ‖−r σ ) ≤ 2 ( ǫ1 + ǫ2 ‖µ‖ σ ) φ ( max ( 0, ‖µ‖2σ − 2ǫ1 )) .\nDefining A = ∣∣∣ ‖µ‖−rσ ∣∣∣,\n∫ ∞\n−∞ φ(x)\n[ Φ (‖µ‖ − r σ ) − Φ (‖µ‖ − r σ − |x| tan β )] dx ≤ 2 ∫ ∞\n0\n∫ A\nA−x tan β φ(x)φ(y)dydx\n= 2\n∫ ∞\n−A sin β\n∫ A cos β+(u+A sin β) tan β\nA cos β\nφ(u)φ(v)dudv ≤ 2φ (A) tanβ (A sinβ + 1)\n≤ 2φ ( max ( 0,\n‖µ‖ 2σ\n− 2ǫ1 )) tanβ (( 2 ‖µ‖ σ + 2ǫ1 ) sinβ + 1 )\nwhere we used u = x cosβ− y sinβ and v = x sinβ+ y cosβ in the second step. The bound now follows easily.\nProof of Theorem 1. Using Propositions 5 and 6 with δ = 1√ n , Proposition 7, and the fact that (C+x) exp(−max(0, x− 4)2/8) ≤ (C + 6) exp(−max(0, x− 4)2/10) for all C, x > 0,\nEθLθ(F̂ ) ≤ 600max ( 4σ2\nλ2 , 1\n)√ d log(nd)\nn\n(it is easy to verify that the bounds are decreasing with ‖µ‖, so we use ‖µ‖ = λ2 to bound the supremum). In the d = 1 case Proposition 6 need not be applied, since the principal directions agree trivially. The bound for λσ ≥ 2max(80, 14 √ 5d) can be shown similarly, using δ = exp ( − n32 ) .\nProposition 8. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ . For any 0 < δ < 1√e such that√ 6 log 1\nδ n ≤ 12 , with probability at least 1− 6dδ, for all i ∈ [d],\n|Σ̂n(i, i)− (σ2 + µ(i)2)| ≤ σ2 √\n6 log 1δ n + 2σ|µ(i)|\n√ 2 log 1δ\nn + (σ + |µ(i)|)2 2 log\n1 δ\nn .\nProposition 9. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ . Define\nS(θ) = {i ∈ [d] : µ(i) 6= 0} and S̃(θ) = {i ∈ [d] : |µ(i)| ≥ 4σ√α}.\nAssume that n ≥ 1, d ≥ 2, and α ≤ 14 . Then S̃(θ) ⊆ Ŝn ⊆ S(θ) with probability at least 1− 6n .\nProof. By Proposition 8, with probability at least 1− 6n ,\n|Σ̂n(i, i)− (σ2 + µ(i)2)| ≤ σ2 √ 6 log(nd)\nn + 2σ|µ(i)|\n√ 2 log(nd)\nn + (σ + |µ(i)|)2 2 log(nd) n\nfor all i ∈ [d]. Assume the above event holds. If S(θ) = [d], then of course Ŝn ⊆ S(θ). Otherwise, for i /∈ S(θ), we have (1 − α)σ2 ≤ Σ̂n(i, i) ≤ (1 + α)σ2, so it is clear that Ŝn ⊆ S(θ). The remainder of the proof is trivial if S̃(θ) = ∅ or S(θ) = ∅. Assume otherwise. For any i ∈ S(θ),\nΣ̂n(i, i) ≥ (1− α)σ2 + ( 1− 2 log(nd)\nn\n) µ(i)2 − 2ασ|µ(i)|.\nBy definition, |µ(i)| ≥ 4σ√α for all i ∈ S̃(θ), so (1+α) 2\n1−α σ 2 ≤ Σ̂n(i, i) and i ∈ Ŝn (we ignore strict equality above as\na measure 0 event), i.e. S̃(θ) ⊆ Ŝn, which concludes the proof.\nProof of Theorem 3. Define S(θ) = {i ∈ [d] : µ(i) 6= 0} and S̃(θ) = {i ∈ [d] : |µ(i)| ≥ 4σ√α}. Assume S̃(θ) ⊆ Ŝn ⊆ S(θ) (by Proposition 9, this holds with probability at least 1 − 6n ). If S̃(θ) = ∅, then we simply have EθLθ(F̂n) ≤ 12 .\nAssume S̃(θ) 6= ∅. Let cos β̂ = |v1(Σ̂Ŝn) T v1(Σ)|, cos β̃ = |v1(ΣŜn) T v1(Σ)|, and cosβ = |v1(Σ̂Ŝn) T v1(ΣŜn)| where Σ = σ2I + µµT , and for simplicity we define Σ̂Ŝn and ΣŜn to be the same as Σ̂n and Σ in Ŝn, respectively, and 0 elsewhere. Then sin β̂ ≤ sin β̃ + sinβ, and\nsin β̃ = ‖µ− µŜ(θ)‖ ‖µ‖ ≤ ‖µ− µS̃(θ)‖ ‖µ‖ ≤ 4σ\n√ α √ |S(θ)| − |S̃(θ)| ‖µ‖ ≤ 8 σ √ sα λ .\nUsing the same argument as the proof of Theorem 1, as long as the above bound is smaller than 1 2 √ 5 ,\nEθLθ(F̂ ) ≤ 600max (\nσ2 ( λ 2 − 4σ √ sα\n)2 , 1 )√ s log(ns)\nn + 104\nσ √ sα\nλ +\n3 n .\nUsing the fact Lθ(F̂ ) ≤ 12 always, and that α ≤ 14 implies log(nd) n ≤ 1, the bound follows.\n6 Conclusion\nWe have provided minimax lower and upper bounds for estimating high dimensional mixtures. The bounds show explicitly how the statistical difficulty of the problem depends on dimension d, sample size n, separation λ and sparsity level s.\nFor clarity, we have focused on the special case where there are two spherical components and the mixture weights are equal. In future work, we plan to extend the results to general mixtures of k Gaussians.\nOne of our motivations for this work is the recent interest in variable selection methods to facilitate clustering in high dimensional problems. Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al. (2012) and Guo et al. (2010) provide promising numerical evidence that variable selection does improve high dimensional clustering. Our results provide some theoretical basis for this idea.\nHowever, there is a gap between the results in this paper and the methodology papers mentioned above. Indeed, as of now, there is no rigorous proof that the methods in those papers outperform a two stage approach where the first stage screens for relevant features and the second stage applies standard clustering methods on the features extracted from the first stage. We conjecture that there are conditions under which simultaneous feature selection and clustering outperforms the two stage approach. Settling this questions will require the aforementioned extension of our results to the general mixture case.\nReferences\nDimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In Learning Theory, pages 458–469. Springer, 2005.\nSanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the thirty-third annual ACM symposium on Theory of computing, pages 247–257. ACM, 2001.\nMikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 103–112. IEEE, 2010.\nS Charles Brubaker and Santosh S Vempala. Isotropic pca and affine-invariant clustering. In Building Bridges, pages 241–281. Springer, 2008.\nKamalika Chaudhuri, Sanjoy Dasgupta, and Andrea Vattani. Learning mixtures of gaussians using the k-means algorithm. arXiv preprint arXiv:0912.0086, 2009.\nSanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science, 1999. 40th Annual Symposium on, pages 634–644. IEEE, 1999.\nG.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins Studies in the Mathematical Sciences. Johns Hopkins University Press, 3rd edition, 1996. ISBN 9780801854149.\nJian Guo, Elizaveta Levina, George Michailidis, and Ji Zhu. Pairwise variable selection for high-dimensional modelbased clustering. Biometrics, 66(3):793–804, 2010.\nDaniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer Science, pages 11–20. ACM, 2013.\nAdam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. Communications of the ACM, 55 (2):113–120, 2012.\nRavindran Kannan, Hadi Salmasian, and Santosh Vempala. The spectral method for general mixture models. In Learning Theory, pages 444–457. Springer, 2005.\nPascal Massart. Concentration inequalities and model selection. 2007.\nWei Pan and Xiaotong Shen. Penalized model-based clustering with application to variable selection. The Journal of Machine Learning Research, 8:1145–1164, 2007.\nAdrian E Raftery and Nema Dean. Variable selection for model-based clustering. Journal of the American Statistical Association, 101(473):168–178, 2006.\nLeonard J. Schulman and Sanjoy Dasgupta. A two-round variant of em for gaussian mixtures. In Proc. 16th UAI (Conference on Uncertainty in Artificial Intelligence), pages 152–159, 2000.\nWei Sun, Junhui Wang, and Yixin Fang. Regularized k-means clustering of high-dimensional data and its asymptotic consistency. Electronic Journal of Statistics, 6:148–167, 2012.\nA.B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. Springer, 2009.\nSantosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841–860, 2004.\nVincent Q Vu and Jing Lei. Minimax sparse principal subspace estimation in high dimensions. arXiv preprint arXiv:1211.0373, 2012.\nDaniela M Witten and Robert Tibshirani. A framework for feature selection in clustering. Journal of the American Statistical Association, 105(490), 2010.\n7 Notation\nFor θ = (µ1, µ2) ∈ R2×d, define\npθ(x) = 1\n2 f(x;µ1, σ\n2I) + 1\n2 f(x;µ2, σ\n2I),\nwhere f(·;µ,Σ) is the density of N (µ,Σ), σ > 0 is a fixed constant. Let Pθ denote the probability measure corresponding to pθ. We consider two classes Θ of parameters:\nΘλ = {(µ1, µ2) : ‖µ1 − µ2‖ ≥ λ}\nΘλ,s = {(µ1, µ2) : ‖µ1 − µ2‖ ≥ λ, ‖µ1 − µ2‖0 ≤ s} ⊆ Θλ. Throughout this document, φ and Φ denote the standard normal density and distribution functions. For a mixture with parameter θ, the Bayes optimal classification, that is, assignment of a point x ∈ Rd to the\ncorrect mixture component, is given by the function\nFθ(x) = argmax i∈{1,2}\nf(x;µi, σ 2I).\nGiven any other candidate assignment function F : Rd → {1, 2}, we define the loss incurred by F as\nLθ(F ) = min π\nPθ({x : Fθ(x) 6= π(F (x))})\nwhere the minimum is over all permutations π : {1, 2} → {1, 2}. For X1, . . . , Xn\ni.i.d.∼ Pθ , let µ̂n and Σ̂n be the mean and covariance of the corresponding empirical distribution. Also, for a matrix B, vi(B) and λi(B) are the i’th eigenvector and eigenvalue of B (assuming B is symmetric),\narranged so that λi(B) ≥ λi+1(B), and ‖B‖2 is the spectral norm.\n8 Upper bounds\n8.1 Standard concentration bounds\n8.1.1 Concentration bounds for estimating the mean\nProposition 10. Let X ∼ χ2d. Then for any ǫ > 0,\nP(X > (1 + ǫ)d) ≤ exp { −d 2 (ǫ− log(1 + ǫ)) } .\nIf ǫ < 1, then\nP(X < (1− ǫ)d) ≤ exp { d\n2 (ǫ+ log(1 − ǫ))\n} .\nProof. Since EetX = (1− 2t)−d2 for 0 < t < 12 ,\nP(X > (1 + ǫ)d) = P(etX > et(1+ǫ)d)\n≤ e−t(1+ǫ)d(1− 2t)− d2\n= exp [ −t(1 + ǫ)d+ d\n2 log\n1\n1− 2t\n] .\nTo minimize the right hand side, we differentiate the exponent with respect to t to obtain the equation\n−(1 + ǫ)d+ d 1− 2t = 0\nwhich can be satisfied by setting t = 12 ( 1− 11+ǫ ) < 12 (it is easy to verify that this is a global minimum). Using this value for t, the first bound follows. Also, for t > 0 and ǫ < 1,\nP(X < (1− ǫ)d) = P(e−tX > e−t(1−ǫ)d) ≤ et(1−ǫ)d(1 + 2t)−d2\n= exp [ t(1− ǫ)d− d\n2 log(1 + 2t)\n]\nand setting t = 12\n( 1 1−ǫ − 1 ) ,\nP(X < (1 − ǫ)d) ≤ exp [ d\n2 (ǫ+ log(1 − ǫ))\n] .\nProposition 11. Let Z1, ..., Zn i.i.d.∼ N (0, Id). Then for any ǫ > 0,\nP (∥∥∥∥∥ 1 n n∑\ni=1\nZi ∥∥∥∥∥ ≥ √ (1 + ǫ)d n ) ≤ exp { −d 2 (ǫ− log(1 + ǫ)) } .\nProof. Using Proposition 10,\nP (∥∥∥∥∥ 1 n n∑\ni=1\nZi ∥∥∥∥∥ ≥ √ (1 + ǫ)d n ) = P   ∥∥∥∥∥ 1√ n n∑\ni=1\nZi ∥∥∥∥∥ 2 ≥ (1 + ǫ)d  \n= P (X ≥ (1 + ǫ)d) ≤ exp { −d 2 (ǫ − log(1 + ǫ)) }\nwhere X ∼ χ2d.\n8.1.2 Concentration bounds for estimating principal direction\nProposition 12. Let Z1, ..., Zn i.i.d.∼ N (0, Id) and δ > 0. If n ≥ d then with probability at least 1− 3δ,\n‖Σ̂n − Id‖2 ≤3  1 + √ 2 log 1δ\nd\n  √ d\nn max\n 1,  1 + √ 2 log 1δ\nd\n  √ d\nn\n \n+  1 + √ 8 log 1δ\nd max\n( 1,\n8 log 1δ d\n)  d\nn\nwhere Σ̂n is the empirical covariance of Zi. Proof. Let Zn = 1n ∑n i=1 Zi. Then\n‖Σ̂n − Id‖2 = ∥∥∥∥∥ 1 n n∑\ni=1\nZiZ T i − Id − ZnZ\nT n ∥∥∥∥∥ 2\n≤ ∥∥∥∥∥ 1 n n∑\ni=1\nZiZ T i − Id ∥∥∥∥∥ 2 + ∥∥Zn ∥∥2 .\nIt is well known that for any ǫ1 > 0,\nP (∥∥∥∥∥ 1 n n∑\ni=1\nZiZ T i − Id ∥∥∥∥∥ 2 ≥ 3 (1 + ǫ1) √ d n max ( 1, (1 + ǫ1) √ d n )) ≤ 2 exp { −dǫ 2 1 2 } .\nUsing this along with Proposition 11, we have for any ǫ2 > 0,\nP ( ‖Σ̂n − Id‖2 ≥ 3 (1 + ǫ1) √ d\nn max\n( 1, (1 + ǫ1) √ d\nn\n) + (1 + ǫ2)d\nn\n)\n≤ 2 exp { −dǫ 2 1\n2\n} + exp { −d 2 (ǫ2 − log(1 + ǫ2)) } .\nSetting ǫ1 = √ 2 log 1 δ\nd ,\nP  ‖Σ̂n − Id‖2 ≥ 3  1 + √ 2 log 1δ\nd\n  √ d\nn max\n 1,  1 + √ 2 log 1δ\nd\n  √ d\nn\n + (1 + ǫ2)d\nn\n \n≤ 2δ + exp { −d 2 (ǫ2 − log(1 + ǫ2)) } ≤ 2δ + exp { −d 8 ǫ2min(1, ǫ2) }\nand, setting ǫ2 =\n√ 8 log 1\nδ d max ( 1, 8 log 1 δ d ) , with probability at least 1− 3δ,\n‖Σ̂n − Id‖2 ≤3  1 + √ 2 log 1δ\nd\n  √ d\nn max\n 1,  1 + √ 2 log 1δ\nd\n  √ d\nn\n \n+  1 + √ 8 log 1δ\nd max\n( 1,\n8 log 1δ d\n)  d\nn .\nProposition 13. Let X1, Y1, ..., Xn, Yn i.i.d.∼ N (0, 1). Then for any ǫ > 0,\nP (∣∣∣∣∣ 1 n n∑\ni=1\nXiYi ∣∣∣∣∣ > ǫ 2 ) ≤ 2 exp { −nǫmin(1, ǫ) 10 } .\nProof. Let Z = XY where X,Y i.i.d.∼ N (0, 1). Then for any t such that |t| < 1,\nEetZ = 1√\n1− t2 .\nSo for 0 < t < 1,\nP\n( 1\nn\nn∑\ni=1\nXiYi > ǫ\n) = P ( exp { n∑\ni=1\ntXiYi\n} > exp(nǫt) )\n≤ E ( exp { n∑\ni=1\ntXiYi\n}) exp(−nǫt)\n= (E exp(tXiYi)) n exp(−nǫt) = (1 − t2)−n2 exp(−nǫt) = exp\n{ −n 2 ( 2ǫt+ log(1 − t2) )} .\nThe bound is minimized by t = 12ǫ (√ 1 + 4ǫ2 − 1 ) < 1, so\nP\n( 1\nn\nn∑\ni=1\nXiYi > ǫ\n) ≤ exp\n{ −n 2 h(2ǫ) }\nwhere\nh(u) = (√ 1 + u2 − 1 ) + log ( 1− 1\nu2\n(√ 1 + u2 − 1 )2) .\nSince h(u) ≥ u5 min(1, u),\nP\n( 1\nn\nn∑\ni=1\nXiYi > ǫ\n) ≤ exp { −n 2 2ǫ 5 min (1, 2ǫ) }\nand the proof is complete by noting that the distribution of XiYi is symmetric.\n8.2 Davis–Kahan\nLemma 3. Let A,E ∈ Rd×d be symmetric matrices, and u ∈ Rd−1 such that\nui = vi+1(A) TEv1(A).\nIf λ1(A)− λ2(A) > 0 and ‖E‖2 ≤\nλ1(A)− λ2(A) 5\nthen √ 1− (v1(A)T v1(A+ E))2 ≤\n4‖u‖ λ1(A)− λ2(A)\n(Corollary 8.1.11 of Golub and Van Loan (1996)).\n8.3 Bounding error in estimating the mean\nProposition 14. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ . For any δ > 0,\nP  ‖µ0 − µ̂n‖ ≥ σ √ 2max(d, 8 log 1δ )\nn + ‖µ‖\n√ 2 log 1δ\nn\n  ≤ 3δ.\nProof. Let Z1, ..., Zn i.i.d.∼ N (0, I) and Y1, ..., Yn i.i.d. such that P(Yi = −1) = P(Yi = 1) = 12 . Then for any ǫ1, ǫ2 > 0,\nP ( ‖µ0 − µ̂n‖ ≥ σ √ (1 + ǫ1)d\nn + ‖µ‖ǫ2\n)\n= P (∥∥∥∥∥µ0 − 1 n d∑\ni=1\n(σZi + µ0 + µYi) ∥∥∥∥∥ ≥ σ √ (1 + ǫ1)d n + ‖µ‖ǫ2 )\n= P (∥∥∥∥∥σ 1 n d∑\ni=1\nZi + µ 1\nn\nd∑\ni=1\nYi ∥∥∥∥∥ ≥ σ √ (1 + ǫ1)d n + ‖µ‖ǫ2 )\n≤ P ( σ ∥∥∥∥∥ 1 n d∑\ni=1\nZi ∥∥∥∥∥+ ‖µ‖ ∣∣∣∣∣ 1 n d∑\ni=1\nYi ∣∣∣∣∣ ≥ σ √ (1 + ǫ1)d n + ‖µ‖ǫ2 )\n≤ P (∥∥∥∥∥ 1 n d∑\ni=1\nZi ∥∥∥∥∥ ≥ √ (1 + ǫ1)d n ) + P (∣∣∣∣∣ 1 n d∑\ni=1\nYi ∣∣∣∣∣ ≥ ǫ2 )\n≤ exp { −d 2 (ǫ1 − log(1 + ǫ1)) } + 2 exp { −nǫ 2 2 2 }\nwhere the last step is using Hoeffding’s inequality and Proposition 11. Setting ǫ2 = √ 2 log 1 δ\nn ,\nP  ‖µ0 − µ̂n‖ ≥ σ √ (1 + ǫ1)d\nn + ‖µ‖\n√ 2 log 1δ\nn\n \n≤ exp { −d 2 (ǫ1 − log(1 + ǫ1)) } + 2δ.\nSince ǫ1 − log(1 + ǫ1) ≥ ǫ14 min(1, ǫ1),\nexp { −d 2 (ǫ1 − log(1 + ǫ1)) } ≤ exp { −d 8 ǫ1 min(1, ǫ1) } .\nSetting\nǫ1 =\n√ 8 log 1δ\nd max\n( 1,\n8 log 1δ d\n) ,\nwe have\nP  ‖µ0 − µ̂n‖ ≥ σ √√√√√ d n  1 + √ 8 log 1δ d max ( 1, 8 log 1δ d ) + ‖µ‖ √ 2 log 1δ n   ≤ 3δ\nand the bound follows.\n8.4 Bounding error in estimating principal direction\nProposition 15. Let θ = (µ0−µ, µ0+µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ . If n ≥ d then for any δ, δ1 > 0, with probability at least 1− 5δ − 2δ1,\n‖Σ̂n − (σ2Id + µµT )‖2\n≤ 3σ2  1 + √ 2 log 1δ\nd\n  √ d\nn max\n 1,  1 + √ 2 log 1δ\nd\n  √ d\nn\n \n+ σ2  1 + √ 8 log 1δ\nd max\n( 1,\n8 log 1δ d\n)  d\nn\n+ 4σ‖µ‖ √√√√√  1 + √ 8 log 1δ\nd max\n( 1,\n8 log 1δ d\n)  d\nn + 2‖µ‖2 log 1δ1 n\nwhere Σ̂n is the empirical covariance of Xi.\nProof. We can express Xi as Xi = σZi + µYi + µ0 where Z1, ..., Zn i.i.d.∼ N (0, Id) and Y1, ..., Yn i.i.d. such that P(Yi = −1) = P(Yi = 1) = 12 . Then\nΣ̂n − (σ2Id + µµT ) = σ2(Σ̂Zn − Id)− µµTY 2\n+ σ\n( 1\nn\nn∑\ni=1\nYiZi − Y Z ) µT\n+ σµ\n( 1\nn\nn∑\ni=1\nYiZi − Y Z )T\nwhere Σ̂Zn is the empirical covariance of Zi and Y and Z are the empirical means of Yi and Zi. So\n‖Σ̂n − (σ2Id + µµT )‖2 ≤ σ2‖Σ̂Zn − Id‖2 + ‖µ‖2Y 2\n+ 2σ‖µ‖ (∥∥∥∥∥ 1 n n∑\ni=1\nYiZi ∥∥∥∥∥+ |Y |‖Z‖ ) .\nBy Hoeffding’s inequality,\nP ( ‖µ‖2Y 2 ≥\n2‖µ‖2 log 1δ1 n\n) ≤ 2δ1.\nSince |Y | ≤ 1 and since YiZi has the same distribution as Zi, by Proposition 11, for any ǫ > 0,\nP ( 2σ‖µ‖ (∥∥∥∥∥ 1 n n∑\ni=1\nYiZi ∥∥∥∥∥+ |Y |‖Z‖ ) ≥ 4σ‖µ‖ √ (1 + ǫ)d n )\n≤ 2 exp { −d 2 (ǫ− log(1 + ǫ)) } ≤ 2 exp { −d 8 ǫmin(1, ǫ) } .\nSetting\nǫ =\n√ 8 log 1δ\nd max\n( 1,\n8 log 1δ d\n)\nwe have\nP  2σ‖µ‖ (∥∥∥∥∥ 1 n n∑\ni=1\nYiZi ∥∥∥∥∥+ |Y |‖Z‖ ) ≥ 4σ‖µ‖\n√√√√√  1 + √ 8 log 1δ\nd max\n( 1,\n8 log 1δ d\n)  d\nn\n \n≤ 2δ.\nFinally, by Proposition 12, with probability at least 1− 3δ,\nσ2‖Σ̂Zn − Id‖2 ≤3σ2  1 + √ 2 log 1δ\nd\n  √ d\nn max\n 1,  1 + √ 2 log 1δ\nd\n  √ d\nn\n \n+ σ2  1 + √ 8 log 1δ\nd max\n( 1,\n8 log 1δ d\n)  d\nn\nand we complete the proof by combining the three bounds.\nProposition 16. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ . If n ≥ d > 1 then for any 0 < δ ≤ 1√\ne and i ∈ [2..d], with probability at least 1− 7δ,\n∣∣∣vi(σ2I + µµT )T (Σ̂n − (σ2I + µµT ))v1(σ2I + µµT ) ∣∣∣\n≤ σ2 1 2\n√ 10 log 1δ\nn max\n( 1,\n10 log 1δ n\n) + σ‖µ‖ √ 2 log 1δ\nn + (σ2 + σ‖µ‖)2 log\n1 δ\nn .\nProof. Let Z1,W1, ..., Zn,Wn i.i.d.∼ N (0, 1) and Y1, ..., Yn i.i.d. such that P(Yi = −1) = P(Yi = 1) = 12 . It is easy to see that the quantity of interest is equal in distribution to ∣∣∣∣∣∣ 1 n n∑\nj=1\n(σZi − σZ)(σWi − σW + ‖µ‖Yi − ‖µ‖Y ) ∣∣∣∣∣∣\nwhere Z,W, Y are the respective empirical means. Moreover, ∣∣∣∣∣∣ 1 n n∑\nj=1\n(σZi − σZ)(σWi − σW + ‖µ‖Yi − ‖µ‖Y ) ∣∣∣∣∣∣\n≤ σ2 ∣∣∣∣∣∣ 1 n n∑\nj=1\nZiWi ∣∣∣∣∣∣ + σ2 ∣∣Z ∣∣ ∣∣W ∣∣+ σ‖µ‖ ∣∣∣∣∣∣ 1 n n∑\nj=1\nZiYi ∣∣∣∣∣∣ + σ‖µ‖ ∣∣Z ∣∣ ∣∣Y ∣∣ .\nFrom Proposition 13, we have\nP   ∣∣∣∣∣ 1 n n∑\ni=1\nZiWi ∣∣∣∣∣ > 1 2 √ 10 log 1δ n max ( 1, 10 log 1δ n )  ≤ 2δ;\nusing Hoeffding’s inequality,\nP  |Y | ≥ √ 2 log 1δ\nn\n  ≤ 2δ;\nand using the Gaussian tail bound, for δ ≤ 1√ e ,\nP  |Z| ≥ √ 2 log 1δ\nn\n  ≤ δ\nand the final result follows easily.\nProposition 17. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ with d > 1 and n ≥ 4d. For any 0 < δ < d−1√\ne , if\nmax\n( σ2\n‖µ‖2 , σ ‖µ‖\n)√ max(d, 8 log 1δ )\nn ≤ 1 160\nthen with probability at least 1− 12δ − 2 exp ( − n20 ) ,\n√ 1− (v1(σ2I + µµT )T v1(Σ̂n))2 ≤ 14max ( σ2\n‖µ‖2 , σ ‖µ‖\n)√ d √√√√10 log d δ\nn max\n( 1,\n10 log dδ n\n) .\nProof. By Proposition 15 (with δ1 = exp ( − n20 ) ), Proposition 16 (with δ2 = δd−1 ), and Lemma 3, with probability at\nleast 1− 12δ − 2 exp ( − n20 ) ,\n√ 1− (v1(σ2I + µµT )T v1(Σ̂n))2\n≤ 4 √ d− 1\n‖µ‖2\n σ2 1\n2\n√√√√10 log d−1 δ\nn max\n( 1,\n10 log d−1δ n\n) + σ‖µ‖ √ 2 log d−1δ\nn + (σ2 + σ‖µ‖)2 log\nd−1 δ\nn\n \nand the result follows after some simplifications.\n8.5 General result relating error in estimating mean and principal direction to clustering loss\nProposition 18. Let θ = (µ0 − µ, µ0 + µ) and let\nF̂ (x) = { 1 if xT v ≥ xT0 v 2 otherwise\nfor some x0, v ∈ Rd, with ‖v‖ = 1. Define cosβ = |vTµ|/‖µ‖. If |(x0 −µ0)T v| ≤ σǫ1 + ‖µ‖ǫ2 for some ǫ1 ≥ 0 and 0 ≤ ǫ2 ≤ 14 , and if sinβ ≤ 1√5 , then\nLθ(F̂ ) ≤ exp { −1 2 max ( 0, ‖µ‖ 2σ − 2ǫ1 )2}[ 2ǫ1 + ǫ2 ‖µ‖ σ + 2 sinβ ( 2 sinβ ‖µ‖ σ + 1 )] .\nProof.\nLθ(F̂ ) =min π\nPθ({x : Fθ(x) 6= π(F̂ (x))})\n=min { Pθ[{x : ((x − µ0)Tµ)((x − x0)T v) ≥ 0}], Pθ[{x : ((x − µ0)Tµ)((x− x0)T v) ≤ 0}] } .\nWLOG assume vTµ ≥ 0 (otherwise we can simply replace v with −v, which does not affect the bound). Then\nLθ(F̂ ) = Pθ[{x : ((x − µ0)Tµ)((x − x0)T v) ≤ 0}] = Pθ[{x : ((x − µ0)Tµ)((x − µ0)T v − (x0 − µ0)T v) ≤ 0}]\n= Pθ\n[{ x : ( (x− µ0)T µ\n‖µ‖\n) ((x− µ0)T v − (x0 − µ0)T v) ≤ 0 }] .\nDefine µ̆ = µ\n‖µ‖ ,\nx̆ = (x− µ0)T µ̆, and\ny̆ = (x− µ0)T v − µ̆µ̆T v ‖v − µ̆µ̆T v‖ ≡ (x− µ0) T v − µ̆µ̆T v sinβ\nso that\nLθ(F̂ ) = Pθ [{ x : x̆ ( y̆ sinβ + x̆ cosβ − (x0 − µ0)T v ) ≤ 0 }]\n= Pθ [{x : min(0, B(y̆)) ≤ x̆ ≤ max(0, B(y̆))}]\nwhere\nB(y̆) = (x0 − µ0)T v\ncosβ − y̆ tanβ.\nSince x̆ and y̆ are projections of x − µ0 onto orthogonal unit vectors, and since x̆ is exactly the component of x− µ0 that lies in the direction of µ, we can integrate out all other directions and obtain\nLθ(F̂ ) =\n∞∫\n−∞\nφσ(y̆)\nmax(0,B(y̆))∫\nmin(0,B(y̆))\n( 1\n2 φσ(x̆+ ‖µ‖) +\n1 2 φσ(x̆− ‖µ‖)\n) dx̆dy̆\nwhere φσ is the density of N (0, σ2). But,\nmax(0,B(y̆))∫\nmin(0,B(y̆))\n( 1\n2 φσ(x̆+ ‖µ‖) +\n1 2 φσ(x̆− ‖µ‖)\n) dx̆\n= 1\n2\nmax(0,B(y̆))∫\nmin(0,B(y̆))\nφσ(x̆+ ‖µ‖)dx̆+ 1\n2\nmax(0,B(y̆))∫\nmin(0,B(y̆))\nφσ(x̆ − ‖µ‖)dx̆\n= 1\n2\n( Φ ( max(0, B(y̆)) + ‖µ‖\nσ\n) − Φ ( min(0, B(y̆)) + ‖µ‖\nσ\n))\n+ 1\n2\n( −Φ (−max(0, B(y̆)) + ‖µ‖ σ ) +Φ (−min(0, B(y̆)) + ‖µ‖ σ ))\n= 1\n2\n( Φ (‖µ‖+ |B(y̆)| σ ) − Φ (‖µ‖ − |B(y̆)| σ )) .\nSince the above quantity is increasing in |B(y̆)|, and since |B(y̆)| ≤ |y̆| tanβ + r where\nr = ∣∣∣∣ (x0 − µ0)T v\ncosβ\n∣∣∣∣ ,\nwe have that, replacing y̆ by x,\nLθ(F̂ ) ≤ 1\n2\n∞∫\n−∞\n1 σ φ (x σ )[ Φ (‖µ‖+ |x| tan β + r σ ) − Φ (‖µ‖ − |x| tanβ − r σ )] dx\n≤ ∞∫\n−∞\n1 σ φ (x σ )[ Φ (‖µ‖ σ ) − Φ (‖µ‖ − |x| tanβ − r σ )] dx\n=\n∞∫\n−∞\nφ(x) [ Φ (‖µ‖ − r σ ) − Φ (‖µ‖ − r σ − |x| tan β )] dx\n+ [ Φ (‖µ‖ σ ) − Φ (‖µ‖ − r σ )] .\nSince tanβ ≤ 12 , we have that r ≤ 2|(x0 − µ0)T v| ≤ 2σǫ1 + 2‖µ‖ǫ2 and\nΦ (‖µ‖ σ ) − Φ (‖µ‖ − r σ ) ≤ r σ φ ( max ( 0, ‖µ‖ − r σ ))\n≤ ( 2ǫ1 + 2ǫ2\n‖µ‖ σ\n) φ ( max ( 0, (1− 2ǫ2)\n‖µ‖ σ\n− 2ǫ1 )) ,\nand since ǫ2 ≤ 14 ,\nΦ (‖µ‖ σ ) − Φ (‖µ‖ − r σ ) ≤ 2 ( ǫ1 + ǫ2 ‖µ‖ σ ) φ ( max ( 0, ‖µ‖ 2σ − 2ǫ1 )) .\nDefining A = ∣∣∣‖µ‖−rσ ∣∣∣, ∞∫\n−∞\nφ(x) [ Φ (‖µ‖ − r σ ) − Φ (‖µ‖ − r σ − |x| tanβ )] dx\n≤ 2 ∞∫\n0\nA∫\nA−x tan β\nφ(x)φ(y)dydx = 2\n∞∫\n−A sin β\nA cosβ+(x+A sin β) tan β∫\nA cosβ\nφ(x)φ(y)dydx\n≤ 2φ(A cos β) tanβ ∞∫\n−A sin β\n(x+A sinβ)φ(x)dx\n= 2φ(A cos β) tanβ (A sinβΦ(A sin β) + φ(A sinβ)) ≤ 2φ (A) tanβ (A sinβ + 1) ≤ 2φ ( max ( 0,\n‖µ‖ − r σ\n)) tanβ ((‖µ‖+ r σ ) sinβ + 1 )\nand ∞∫\n−∞\nφ(x) [ Φ (‖µ‖ − r σ ) − Φ (‖µ‖ − r σ − |x| tanβ )] dx\n≤ 2φ ( max ( 0,\n‖µ‖ 2σ\n− 2ǫ1 )) tanβ (( 2 ‖µ‖ σ + 2ǫ1 ) sinβ + 1 ) .\nSo we have that\nLθ(F̂ ) ≤ 2 ( ǫ1 + ǫ2\n‖µ‖ σ\n) φ ( max ( 0,\n‖µ‖ 2σ\n− 2ǫ1 ))\n+ 2φ ( max ( 0,\n‖µ‖ 2σ\n− 2ǫ1 )) tanβ (( 2 ‖µ‖ σ + 2ǫ1 ) sinβ + 1 )\n≤ φ ( max ( 0,\n‖µ‖ 2σ\n− 2ǫ1 )) ×\n× [ 2ǫ1 + 2ǫ2\n‖µ‖ σ + 4 sinβ tanβ ‖µ‖ σ + 4ǫ1 sinβ tanβ + 2 tanβ\n]\n≤ exp { −1 2 max ( 0, ‖µ‖ 2σ − 2ǫ1 )2}[ 2ǫ1 + ǫ2 ‖µ‖ σ + tanβ ( 2 sinβ ‖µ‖ σ + 1 )] .\n8.6 Non-sparse upper bound\nTheorem 5. For any θ ∈ Θλ and X1, ..., Xn i.i.d.∼ Pθ, let\nF̂ (x) = { 1 if xT v1(Σ̂n) ≥ µ̂Tnv1(Σ̂n) 2 otherwise,\nand let n ≥ max(68, 4d), d ≥ 1. Then\nsup θ∈Θλ\nELθ(F̂ ) ≤ 600max ( 4σ2\nλ2 , 1\n)√ d log(nd)\nn .\nFurthermore, if λσ ≥ 2max(80, 14 √ 5d), then\nsup θ∈Θλ ELθ(F̂ ) ≤ 17 exp ( − n 32 ) + 9 exp ( − λ 2 80σ2 ) .\nProof. Using Propositions 14 and 17 with δ = 1√ n , Proposition 18, and the fact that (C + x) exp(−max(0, x − 4)2/8) ≤ (C + 6) exp(−max(0, x− 4)2/10) for all C, x > 0,\nELθ(F̂ ) ≤ 600max ( 4σ2\nλ2 , 1\n)√ d log(nd)\nn\n(it is easy to verify that the bounds are decreasing with ‖µ‖, so we use ‖µ‖ = λ2 to bound the supremum). Note that the d = 1 case must be handled separately, but results in a bound that agrees with the above.\nAlso, when λσ ≥ 2max(80, 14 √ 5d), using δ = exp ( − n32 ) ,\nELθ(F̂ ) ≤17 exp ( − n 32 ) + 9 exp ( − λ 2 80σ2 ) .\n8.7 Estimating the support in the sparse case\nProposition 19. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ. For any 0 < δ < 1√e such\nthat √ 6 log 1 δ\nn ≤ 12 , with probability at least 1− 6dδ,\n|Σ̂n(i, i)− (σ2 + µ(i)2)| ≤ σ2 √\n6 log 1δ n + 2σ|µ(i)|\n√ 2 log 1δ\nn + (σ + |µ(i)|)2 2 log\n1 δ\nn\nfor all i ∈ [d].\nProof. Consider any i ∈ [d]. Let Z1, ..., Zn i.i.d.∼ N (0, 1) and Y1, ..., Yn i.i.d. such that P(Yj = −1) = P(Yj = 1) = 12 . Then Σ̂n(i, i) is equal in distribution to\n1\nn\nn∑\nj=1\n(σZj + µ(i)Yj − σZ − µ(i)Y )2\nwhere Z and Y are the respective empirical means, and\n1\nn\nn∑\nj=1\n(σZj + µ(i)Yj − σZ − µ(i)Y )2 = 1\nn\nn∑\nj=1\n(σZj + µ(i)Yj) 2 − (σZ + µ(i)Y )2\n= σ2 1\nn\nn∑\nj=1\nZ2j + µ(i) 2 + 2σµ(i)\n1\nn\nn∑\nj=1\nZjYj\n− σ2Z2 − µ(i)2Y 2 − 2σµ(i)ZY\nSo, by Hoeffding’s inequality, a Gaussian tail bound, and Proposition 10, we have that for any 0 < δ < 1√ e , with probability at least 1− 6δ,\n|Σ̂n(i, i)− (σ2 + µ(i)2)| ≤ σ2 √\n6 log 1δ n + 2σ|µ(i)|\n√ 2 log 1δ\nn + (σ + |µ(i)|)2 2 log\n1 δ\nn\nwhere we have used the fact that for ǫ ∈ (0, 0.5],\nmax {−ǫ+ log(1 + ǫ), ǫ + log(1− ǫ)} ≤ − ǫ 2\n3\nand the result follows easily.\nProposition 20. Let θ = (µ0 − µ, µ0 + µ) for some µ0, µ ∈ Rd and X1, ..., Xn i.i.d.∼ Pθ . Define\nS(θ) = {i ∈ [d] : µ(i) 6= 0},\nα =\n√ 6 log(nd)\nn +\n2 log(nd)\nn ,\nS̃(θ) = {i ∈ [d] : |µ(i)| ≥ 4σ√α},\nτ̂n = 1 + α\n1− α mini∈[d] Σ̂n(i, i),\nand Ŝn = {i ∈ [d] : Σ̂n(i, i) > τ̂n}.\nAssume that n ≥ 1, d ≥ 2, and α ≤ 14 . Then S̃(θ) ⊆ Ŝn ⊆ S(θ) with probability at least 1− 6n . Proof. By Proposition 19, with probability at least 1− 6n ,\n|Σ̂n(i, i)− (σ2 + µ(i)2)| ≤ σ2 √ 6 log(nd)\nn + 2σ|µ(i)|\n√ 2 log(nd)\nn + (σ + |µ(i)|)2 2 log(nd) n\nfor all i ∈ [d]. Assume the above event holds. If S(θ) = [d], then of course Ŝn ⊆ S(θ). Otherwise, for i /∈ S(θ),\n(1− α)σ2 ≤ Σ̂n(i, i) ≤ (1 + α)σ2\nso it is clear that Ŝn ⊆ S(θ). The remainder of the proof is trivial if S̃(θ) = ∅ or S(θ) = ∅. Assume otherwise. For any i ∈ S(θ),\nΣ̂n(i, i) ≥ (1− α)σ2 + µ(i)2 − 2σ|µ(i)| √ 2 log(nd)\nn − 2σ|µ(i)|2 log(nd) n − µ(i)2 2 log(nd) n\n≥ (1− α)σ2 + ( 1− 2 log(nd)\nn\n) µ(i)2 − 2ασ|µ(i)|.\nBy definition, |µ(i)| ≥ 4σ√α for all i ∈ S̃(θ), so\n(1 + α)2\n1− α σ 2 ≤ (1− α)σ2 +\n( 1− 2 log(nd)\nn\n) µ(i)2 − 2ασ|µ(i)| ≤ Σ̂n(i, i)\nand i ∈ Ŝn (we ignore strict equality above as a measure 0 event), i.e. S̃(θ) ⊆ Ŝn, which concludes the proof.\n8.8 Sparse upper bound\nTheorem 6. For any θ = (µ0 − µ, µ0 + µ) ∈ Θλ,s and X1, ..., Xn i.i.d.∼ Pθ with n ≥ max(68, 4s) and s ≥ 1, define\nα =\n√ 6 log(nd)\nn +\n2 log(nd)\nn ,\nτ̂n = 1 + α\n1− α mini∈[d] Σ̂n(i, i),\nand Ŝn = {i ∈ [d] : Σ̂n(i, i) > τ̂n}. Assume that d ≥ 2, and α ≤ 14 . Let\nF̂n(x) =\n{ 1 if xT\nŜn v1(Σ̂Ŝn) ≥ µ̂ T Ŝn v1(Σ̂Ŝn)\n2 otherwise\nwhere µ̂Ŝn and Σ̂Ŝn are the empirical mean and covariance of Xi for the dimensions in Ŝn, and 0 elsewhere. Then\nsup θ∈Θλ,s\nELθ(F̂ ) ≤ 603max ( 16σ2\nλ2 , 1\n)√ s log(ns)\nn + 220\nσ √ s\nλ\n( log(nd)\nn\n) 1 4\n.\nProof. Define S(θ) = {i ∈ [d] : µ(i) 6= 0}\nand S̃(θ) = {i ∈ [d] : |µ(i)| ≥ 4σ√α},\nAssume S̃(θ) ⊆ Ŝn ⊆ S(θ) (by Proposition 20, this holds with probability at least 1 − 6n ). If S̃(θ) = ∅, then we simply have ELθ(F̂n) ≤ 12 .\nAssume S̃(θ) 6= ∅. Let cos β̂ = |v1(Σ̂Ŝn)\nT v1(Σ)|, cos β̃ = |v1(ΣŜn)\nT v1(Σ)|, and\ncosβ = |v1(Σ̂Ŝn) T v1(ΣŜn)|\nwhere Σ = σ2I + µµT , and ΣŜn is the same as Σ in Ŝn, and 0 elsewhere. Then\nsin β̂ ≤ sin β̃ + sinβ. Also\nsin β̃ = ‖µ− µŜ(θ)‖\n‖µ‖\n≤ ‖µ− µS̃(θ)‖\n‖µ‖\n≤ 4σ\n√ α √ |S(θ)| − |S̃(θ)| ‖µ‖\n≤ 8σ √ sα\nλ .\nUsing the same argument as the proof of Theorem 5, we have that as long as the above bound is smaller than 1 2 √ 5 ,\nELθ(F̂ ) ≤ 600max (\nσ2 ( λ 2 − 4σ √ sα\n)2 , 1 )√ s log(ns)\nn + 104\nσ √ sα\nλ +\n3\nn\n≤ 603max ( 16 σ2\nλ2 , 1\n)√ s log(ns)\nn + 104\nσ √ sα\nλ .\nHowever, when 8σ √ sα\nλ > 1 2 √ 5 , the above bound is bigger than 12 , which is a trivial upper bound on the clustering\nerror, hence the bound can be stated without further conditions. Finally, since α ≤ 14 , we must have log(nd) n ≤ 1, so α ≤ ( √ 6 + 2) √ log(nd)\nn , which completes the proof.\n9 Lower bounds\n9.1 Standard tools\nLemma 4. Let P0, P1, ..., PM be probability measures satisfying\n1\nM\nM∑\ni=1\nKL(Pi, P0) ≤ α logM\nwhere 0 < α < 1/8 and M ≥ 2. Then inf ψ max i∈[0..M ] Pi(ψ 6= i) ≥ 0.07\n(Tsybakov (2009)).\nLemma 5. (Varshamov–Gilbert bound) Let Ω = {0, 1}m for m ≥ 8. Then there exists a subset {ω0, ..., ωM} ⊆ Ω such that ω0 = (0, ..., 0),\nρ(ωi, ωj) ≥ m\n8 , ∀ 0 ≤ i < j ≤ M,\nand M ≥ 2m/8,\nwhere ρ denotes the Hamming distance between two vectors (Tsybakov (2009)).\nLemma 6. Let Ω = {ω ∈ {0, 1}m : ‖ω‖0 = s} for integers m > s ≥ 1. For any α, β ∈ (0, 1) such that s ≤ αβm, there exists ω0, ..., ωM ∈ Ω such that for all 0 ≤ i < j ≤ M ,\nρ(ωi, ωj) > 2(1− α)s\nand log(M + 1) ≥ cs log (m s )\nwhere c =\nα\n− log(αβ) (− logβ + β − 1).\nIn particular, setting α = 3/4 and β = 1/3, we have that ρ(ωi, ωj) > s/2, log(M + 1) ≥ s5 log ( m s ) , as long as s ≤ m/4 (Massart (2007), Lemma 4.10).\n9.2 A reduction to hypothesis testing without a general triangle inequality\nProposition 21. Let θ0, ..., θM ∈ Θλ (or Θλ,s), M ≥ 2, 0 < α < 1/8, and γ > 0. If\nmax i∈[M ]\nKL(Pθi , Pθ0) ≤ α logM\nn\nand for all 0 ≤ i 6= j ≤ M and clusterings F̂ ,\nLθi(F̂ ) < γ implies Lθj(F̂ ) ≥ γ,\nthen\ninf F̂n max i∈[0..M ]\nEθiLθi(F̂n) ≥ 0.07γ.\nProof. Using Markov’s inequality,\ninf F̂n max i∈[0..M ] EθiLθi(F̂n) ≥ γ inf F̂n max i∈[0..M ] Pnθi\n( Lθi(F̂n) ≥ γ ) .\nDefineψ∗(F̂n) = argmin i∈[0..M ] Lθi(F̂n). By assumption,Lθi(F̂n) < γ impliesLθj(F̂n) ≥ γ for any j 6= i, so Lθi(F̂n) < γ only when ψ∗(F̂n) = i. Hence,\nPnθi ( ψ∗(F̂n) = i ) ≥ Pnθi ( Lθi(F̂n) < γ )\nand\ninf F̂n max i∈[0..M ]\nPnθi ( Lθi(F̂n) ≥ γ ) ≥ max\ni∈[0..M ] Pnθi\n( ψ∗(F̂n) 6= i )\n≥ inf ψ̂n max i∈[0..M ] Pnθi\n( ψ̂n 6= i )\n≥ 0.07\nwhere the last step is by Lemma 4.\n9.3 Properties of the clustering error\nProposition 22. For any θ, θ′ ∈ Θλ, and any clustering F̂ , if\nLθ(Fθ′) + Lθ(F̂ ) +\n√ KL(Pθ, Pθ′)\n2 ≤ 1 2 ,\nthen\nLθ(Fθ′)− Lθ(F̂ )− √ KL(Pθ, Pθ′)\n2 ≤ Lθ′(F̂ ) ≤ Lθ(Fθ′) + Lθ(F̂ ) +\n√ KL(Pθ, Pθ′)\n2 .\nProof. WLOG assume Fθ , Fθ′ , and F̂ are such that, using simplified notation,\nLθ(Fθ′) = Pθ(Fθ 6= Fθ′)\nand Lθ(F̂ ) = Pθ(Fθ 6= F̂ ).\nThen\nPθ(Fθ′ 6= F̂ ) = Pθ ( (Fθ = Fθ′) ∩ (Fθ 6= F̂ ) ∪ (Fθ 6= Fθ′) ∩ (Fθ = F̂ ) )\n= Pθ ( (Fθ = Fθ′) ∩ (Fθ 6= F̂ ) ) + Pθ ( (Fθ 6= Fθ′) ∩ (Fθ = F̂ ) ) .\nSince\n0 ≤ Pθ ( (Fθ = Fθ′) ∩ (Fθ 6= F̂ ) ) ≤ Pθ ( Fθ 6= F̂ ) = Lθ(F̂ ),\nPθ ( (Fθ 6= Fθ′) ∩ (Fθ = F̂ ) ) ≤ Pθ (Fθ 6= Fθ′) = Lθ(Fθ′),\nand\nLθ(Fθ′)− Lθ(F̂ ) = Pθ (Fθ 6= Fθ′)− Pθ(Fθ 6= F̂ ) ≤ Pθ ( (Fθ 6= Fθ′) ∩ (Fθ = F̂ ) ) ,\nwe have that\nLθ(Fθ′)− Lθ(F̂ ) ≤ Pθ(Fθ′ 6= F̂ ) ≤ Lθ(Fθ′) + Lθ(F̂ )\nand\nLθ(Fθ′)− Lθ(F̂ )− TV(Pθ, Pθ′) ≤ Pθ′(Fθ′ 6= F̂ ) ≤ Lθ(Fθ′) + Lθ(F̂ ) + TV(Pθ, Pθ′).\nIt is easy to see that if Lθ(Fθ′) + Lθ(F̂ ) + TV(Pθ, Pθ′) ≤ 12 , then the above bound implies\nLθ(Fθ′)− Lθ(F̂ )− TV(Pθ, Pθ′) ≤ Lθ′(F̂ ) ≤ Lθ(Fθ′) + Lθ(F̂ ) + TV(Pθ, Pθ′).\nThe final step is to use the fact that TV(Pθ, Pθ′) ≤ √ KL(Pθ,Pθ′ ) 2 .\nProposition 23. For some µ0, µ, µ′ ∈ Rd such that ‖µ‖ = ‖µ′‖, let\nθ = ( µ0 − µ\n2 , µ0 +\nµ\n2\n)\nand\nθ′ = ( µ0 − µ′\n2 , µ0 +\nµ′\n2\n) .\nThen\n2g (‖µ‖ 2σ ) sinβ cosβ ≤ Lθ(Fθ′) ≤ 1 π tanβ\nwhere cosβ = |µ Tµ′|\n‖µ‖2 and g(x) = φ(x)(φ(x) − xΦ(−x)). Proof. It is easy to see that\nLθ(Fθ′) = 1\n2\n∫\nR\n1 σ φ (x σ )( Φ (‖µ‖ 2σ + |x| tanβ σ ) − Φ (‖µ‖ 2σ − |x| tanβ σ )) dx.\nDefine ξ = ‖µ‖2σ . With a change of variables, we have\nLθ(Fθ′) = 1\n2\n∫\nR\nφ(x) (Φ (ξ + |x| tanβ) − Φ (ξ − |x| tanβ)) dx\n=\n∞∫\n0\nφ(x)(Φ(ξ + x tanβ)− Φ(ξ − x tanβ))dx.\nFor any a ≤ b, Φ(b)− Φ(a) ≤ b−a√ 2π , so\nLθ(Fθ′) =\n∞∫\n0\nφ(x)(Φ(ξ + x tanβ)− Φ(ξ − x tanβ))dx\n≤ ∞∫\n0\nφ(x)(Φ(x tan β)− Φ(−x tanβ))dx\n≤ tanβ √ 2\nπ\n∞∫\n0\nxφ(x)dx\n= 1\nπ tanβ.\nAlso,\nLθ(Fθ′) =\n∞∫\n0\nφ(x)(Φ(ξ + x tanβ)− Φ(ξ − x tanβ))dx\n≥ 2 tanβ ∞∫\n0\nxφ(x)φ(ξ + x tanβ)dx\n= 2 tanβ 1√ 2π\n∞∫\n0\nx 1√ 2π exp\n{ −x 2 + (ξ + x tanβ)2\n2\n} dx\n= 2 tanβ 1√ 2π exp\n{ −ξ 2\n2\n( 1− tan 2 β\n1 + tan2 β\n)} ∞∫\n0\nx 1√ 2π exp    − ( x+ ξ tan β1+tan2 β )2 2 ( 1√\n1+tan2 β\n)2    dx\n≥ 2 tanβ 1√ 2π exp\n{ −ξ 2\n2\n} ∞∫\n0\nx 1√ 2π exp\n{ − (x+ ξ sinβ cosβ) 2\n2 cos2 β\n} dx\n= 2 tanβφ(ξ) [ cos2 β√\n2π exp\n{ −ξ 2 sin2 β\n2\n} − ξ sinβ cos2 βΦ(−ξ sinβ) ]\n= 2 sinβ cosβφ(ξ) [φ(ξ sinβ) − ξ sinβΦ(−ξ sinβ)] ≥ 2 sinβ cosβφ(ξ) [φ(ξ) − ξΦ(−ξ)] .\n9.4 A KL divergence bound of the necessary order\nProposition 24. For some µ0, µ, µ′ ∈ Rd such that ‖µ‖ = ‖µ′‖, let\nθ = ( µ0 − µ\n2 , µ0 +\nµ\n2\n)\nand\nθ′ = ( µ0 − µ′\n2 , µ0 +\nµ′\n2\n) .\nThen\nKL(Pθ , Pθ′) ≤ ξ4(1− cosβ)\nwhere ξ = ‖µ‖2σ and cosβ = |µTµ′| ‖µ‖‖µ′‖ .\nProof. Since the KL divergence is invariant to affine transformations, it is easy to see that\nKL(Pθ , Pθ′) =\n∫\nR\n∫\nR\np1(x, y) log p1(x, y)\np2(x, y) dxdy\nwhere\np1(x, y) = 1\n2 φ(x + ξx)φ(y + ξy) +\n1 2 φ(x− ξx)φ(y − ξy),\np2(x, y) = 1\n2 φ(x + ξx)φ(y − ξy) +\n1 2 φ(x− ξx)φ(y + ξy),\nξx = ξ cos β\n2 , ξy = ξ sin\nβ 2 .\nSince\np1(x, y) p2(x, y) = φ(x + ξx)φ(y + ξy) + φ(x − ξx)φ(y − ξy) φ(x + ξx)φ(y − ξy) + φ(x − ξx)φ(y + ξy)\n= exp(−xξx − yξy) + exp(xξx + yξy) exp(−xξx + yξy) + exp(xξx − yξy)\nwe have\nlog p1(x, y)\np2(x, y) = log\ncosh(xξx + yξy) cosh(xξx − yξy) .\nFurthermore, ∫\nR\n∫\nR\n1 2 φ(x + ξx)φ(y + ξy) log cosh(xξx + yξy) cosh(xξx − yξy) dxdy\n=\n∫\nR\n∫\nR\n1 2 φ(−x+ ξx)φ(−y + ξy) log cosh(−xξx − yξy) cosh(−xξx + yξy) dxdy\n=\n∫\nR\n∫\nR\n1 2 φ(x − ξx)φ(y − ξy) log cosh(xξx + yξy) cosh(xξx − yξy) dxdy\nso\nKL(Pθ, Pθ′) =\n∫\nR\n∫\nR\nφ(x − ξx)φ(y − ξy) log cosh(xξx + yξy)\ncosh(xξx − yξy) dxdy\n=\n∫\nR\n∫\nR\nφ(x)φ(y) log cosh(xξx + ξ\n2 x + yξy + ξ 2 y)\ncosh(xξx + ξ2x − yξy − ξ2y) dxdy.\nBut for any x\n− ∫\nR\nφ(x)φ(y) log cosh(xξx + ξ 2 x − yξy − ξ2y)dy\n= − ∫\nR\nφ(x)φ(−y) log cosh(xξx + ξ2x + yξy − ξ2y)dy\n= − ∫\nR\nφ(x)φ(y) log cosh(xξx + ξ 2 x + yξy − ξ2y)dy,\nthus,\nKL(Pθ, Pθ′) =\n∫\nR\n∫\nR\nφ(x)φ(y) log cosh(xξx + ξ\n2 x + yξy + ξ 2 y)\ncosh(xξx + ξ2x + yξy − ξ2y) dxdy\n=\n∫\nR\nφ(z) log cosh(z\n√ ξ2x + ξ 2 y + ξ 2 x + ξ 2 y)\ncosh(z √ ξ2x + ξ 2 y + ξ 2 x − ξ2y) dz\n=\n∫\nR\nφ(z) log cosh(ξz + ξ2x + ξ 2 y)\ncosh(ξz + ξ2x − ξ2y) dz\nsince ξ2x + ξ 2 y = ξ 2. By the mean value theorem and the fact that tanh is monotonically increasing,\nlog cosh(ξz + ξ2x + ξ 2 y)\ncosh(ξz + ξ2x − ξ2y) ≤ 2ξ2y tanh(ξz + ξ2x + ξ2y)\n= 2ξ2y tanh(ξz + ξ 2)\nfor all z. Since tanh is an odd function,\nKL(Pθ , Pθ′) ≤ 2ξ2y ∫\nR\nφ(z) tanh(ξz + ξ2)dz\n= 2ξ2y\n∫\nR\nφ(z)(tanh(ξz + ξ2)− tanh(ξz))dz.\nUsing the mean value theorem again,\ntanh(ξz + ξ2)− tanh(ξz) ≤ ξ2 max x∈[ξz,ξz+ξ2] (1− tanh2(x))\n≤ ξ2\nfor all z, so\nKL(Pθ, Pθ′) ≤ 2ξ2ξ2y = 2ξ4 sin2 β\n2\n= ξ4(1− cosβ).\n9.5 Non-sparse lower bound\nTheorem 7. Assume that d ≥ 9 and λσ ≤ 0.2. Then\ninf F̂n sup θ∈Θλ\nEθLθ(F̂n) ≥ 1\n500 min\n{√ log 2\n3\nσ2 λ2 √ d− 1 n , 1 4 } .\nProof. Let ξ = λ2σ , and define\nǫ = min\n{√ log 2\n3\nσ2\nλ 1√ n ,\nλ\n4 √ d− 1\n} .\nDefine λ20 = λ 2 − (d− 1)ǫ2. Let Ω = {0, 1}d−1. For ω = (ω(1), ..., ω(d− 1)) ∈ Ω, let µω = λ0ed + ∑d−1 i=1 (2ω(i)−\n1)ǫei (where {ei}di=1 is the standard basis for Rd). Let θω = ( −µω2 , µω 2 ) ∈ Θλ.\nBy Proposition 24,\nKL(Pθω , Pθν ) ≤ ξ4(1 − cosβω,ν)\nwhere\ncosβω,ν = |µTωµν | λ2 = 1− 2ρ(ω, ν)ǫ 2 λ2\nand ρ is the Hamming distance, so\nKL(Pθω , Pθν ) ≤ ξ4 2ρ(ω, ν)ǫ2\nλ2\n≤ ξ4 2(d− 1)ǫ 2\nλ2 .\nBy Proposition 23, since cosβω,ν ≥ 12 ,\nLθω(Fθν ) ≤ 1\nπ tanβω,ν\n≤ 2 π sinβω,ν\n≤ 4 π √ d− 1ǫ λ\nand\nLθω(Fθν ) ≥ 2g(ξ) sinβω,ν cosβω,ν ≥ g(ξ) sinβω,ν\n≥ √ 2g(ξ)\n√ ρ(ω, ν)ǫ\nλ\nwhere g(x) = φ(x)(φ(x) − xΦ(−x)). By Lemma 5, there exist ω0, ..., ωM ∈ Ω such that M ≥ 2(d−1)/8 and\nρ(ωi, ωj) ≥ d− 1 8 , ∀ 0 ≤ i < j ≤ M.\nFor simplicity of notation, let θi = θωi for all i ∈ [0..M ]. Then, for i 6= j ∈ [0..M ],\nKL(Pθi , Pθj ) ≤ ξ4 2(d− 1)ǫ2\nλ2 ,\nand\nLθi(Fθj ) ≤ 4\nπ √ d− 1ǫ λ\nand\nLθi(Fθj ) ≥ 1\n2 g(ξ) √ d− 1ǫ λ .\nDefine\nγ = 1\n4 (g(ξ)− 2ξ2) √ d− 1ǫ λ .\nThen for any i 6= j ∈ [0..M ], and any F̂ such that Lθi(F̂ ) < γ,\nLθi(Fθj ) + Lθi(F̂ ) +\n√ KL(Pθi , Pθj )\n2 <\n( 4\nπ +\n1 4 (g(ξ)− 2ξ2) + ξ2 ) √ d− 1ǫ λ ≤ 1 2\nbecause, for ξ ≤ 0.1, by definition of ǫ, ( 4\nπ +\n1 4 (g(ξ)− 2ξ2) + ξ2 ) √ d− 1ǫ λ ≤ 2 √ d− 1ǫ λ ≤ 1 2 .\nSo, by Proposition 22,\nLθj (F̂ ) ≥ Lθi(Fθj )− Lθi(F̂ )− √ KL(Pθi , Pθj )\n2 ≥ γ.\nAlso,\nmax i∈[M ]\nKL(Pθi , Pθ0) ≤ (d− 1)ξ4 2ǫ2\nλ2\n≤ logM 9n\nbecause, by definition of ǫ,\nξ4 2ǫ2 λ2 ≤ log 2 72n .\nSo by Proposition 21 and the fact that ξ ≤ 0.1,\ninf F̂n max i∈[0..M ]\nEθiLθi(F̂n) ≥ 0.07γ\n= 0.07 1\n4 (g(ξ)− 2ξ2) √ d− 1ǫ λ\n≥ 1 500 min\n{√ log 2\n3\nσ2 λ2 √ d− 1 n , 1 4 }\nand to complete the proof we use the fact that\ninf F̂n sup θ∈Θλ EθLθ(F̂n) ≥ inf F̂n max i∈[0..M ] EθiLθi(F̂n).\n9.6 Sparse lower bound\nTheorem 8. Assume that λσ ≤ 0.2, d ≥ 17, and\n5 ≤ s ≤ d− 1 4 + 1.\nThen\ninf F̂n sup θ∈Θλ,s\nEθLθ(F̂n) ≥ 1\n600 min\n{√ 8\n45\nσ2 λ2 √ s− 1 n log ( d− 1 s− 1 ) , 1 2 } .\nProof. For simplicity, we state this proof for Θλ,s+1, assuming 4 ≤ s ≤ d−14 . Let ξ = λ2σ , and define\nǫ = min\n{√ 8\n45\nσ2\nλ\n√ 1\nn log ( d− 1 s ) , 1 2 λ√ s } .\nDefine λ20 = λ 2 − sǫ2. Let Ω = {ω ∈ {0, 1}d−1 : ‖ω‖0 = s}. For ω = (ω(1), ..., ω(d − 1)) ∈ Ω, let µω =\nλ0ed + ∑d−1 i=1 ω(i)ǫei (where {ei}di=1 is the standard basis for Rd). Let θω = ( −µω2 , µω 2 ) ∈ Θλ,s.\nBy Proposition 24,\nKL(Pθω , Pθν ) ≤ ξ4(1 − cosβω,ν)\nwhere\ncosβω,ν = |µTωµν | λ2 = 1− ρ(ω, ν)ǫ 2 2λ2\nand ρ is the Hamming distance, so\nKL(Pθω , Pθν ) ≤ ξ4 ρ(ω, ν)ǫ2\n2λ2\n≤ ξ4 sǫ 2\nλ2 .\nBy Proposition 23, since cosβω,ν ≥ 12 ,\nLθω(Fθν ) ≤ 1\nπ tanβω,ν\n≤ 2 π sinβω,ν ≤ 2 √ 2\nπ\n√ sǫ\nλ\nand\nLθω(Fθν ) ≥ 2g(ξ) sinβω,ν cosβω,ν ≥ g(ξ) sinβω,ν\n≥ g(ξ)√ 2\n√ ρ(ω, ν)ǫ\nλ\nwhere g(x) = φ(x)(φ(x)− xΦ(−x)). By Lemma 6, there exist ω0, ..., ωM ∈ Ω such that log(M +1) ≥ s5 log ( d−1 s ) and ρ(ωi, ωj) ≥ s\n2 , ∀ 0 ≤ i < j ≤ M.\nFor simplicity of notation, let θi = θωi for all i ∈ [0..M ]. Then, for i 6= j ∈ [0..M ],\nKL(Pθi , Pθj ) ≤ ξ4 sǫ2\nλ2 ,\nand\nLθi(Fθj ) ≤ 2 √ 2\nπ\n√ sǫ\nλ\nand\nLθi(Fθj ) ≥ g(ξ)\n2\n√ sǫ\nλ .\nDefine\nγ = 1\n4 (g(ξ)−\n√ 2ξ2)\n√ sǫ\nλ .\nThen for any i 6= j ∈ [0..M ], and any F̂ such that Lθi(F̂ ) < γ,\nLθi(Fθj ) + Lθi(F̂ ) +\n√ KL(Pθi , Pθj )\n2 <\n( 2 √ 2\nπ +\n1 4 (g(ξ)−\n√ 2ξ2) + ξ2√ 2\n) √ sǫ\nλ ≤ 1 2\nbecause, for ξ ≤ 0.1, by definition of ǫ, ( 2 √ 2\nπ +\n1 4 (g(ξ)−\n√ 2ξ2) + ξ2√ 2\n) √ sǫ\nλ ≤\n√ sǫ\nλ ≤ 1 2 .\nSo, by Proposition 22,\nLθj (F̂ ) ≥ Lθi(Fθj )− Lθi(F̂ )− √ KL(Pθi , Pθj )\n2 ≥ γ.\nAlso,\nmax i∈[M ]\nKL(Pθi , Pθ0) ≤ ξ4 sǫ2\nλ2\n≤ 1 18n log ( d− 1 s ) s 5\n≤ 1 9n log (( d− 1 s ) s 5 − 1 )\n≤ logM 9n\nbecause, by definition of ǫ,\nξ4 sǫ2 λ2 ≤ s 90n log ( d− 1 s ) .\nSo by Proposition 21 and the fact that ξ ≤ 0.1,\ninf F̂n max i∈[0..M ]\nEθiLθi(F̂n) ≥ 0.07γ\n≥ 0.070.1 4\n√ sǫ\nλ\n≥ 1 600 min\n{√ 8\n45\nσ2 λ2\n√ s\nn log ( d− 1 s ) , 1 2\n}\nand to complete the proof we use the fact that\ninf F̂n sup θ∈Θλ,s EθLθ(F̂n) ≥ inf F̂n max i∈[0..M ] EθiLθi(F̂n)."
    } ],
    "references" : [ {
      "title" : "On spectral learning of mixtures of distributions",
      "author" : [ "Dimitris Achlioptas", "Frank McSherry" ],
      "venue" : "In Learning Theory,",
      "citeRegEx" : "Achlioptas and McSherry.,? \\Q2005\\E",
      "shortCiteRegEx" : "Achlioptas and McSherry.",
      "year" : 2005
    }, {
      "title" : "Learning mixtures of arbitrary gaussians",
      "author" : [ "Sanjeev Arora", "Ravi Kannan" ],
      "venue" : "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Arora and Kannan.,? \\Q2001\\E",
      "shortCiteRegEx" : "Arora and Kannan.",
      "year" : 2001
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "Mikhail Belkin", "Kaushik Sinha" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Belkin and Sinha.,? \\Q2010\\E",
      "shortCiteRegEx" : "Belkin and Sinha.",
      "year" : 2010
    }, {
      "title" : "Isotropic pca and affine-invariant clustering",
      "author" : [ "S Charles Brubaker", "Santosh S Vempala" ],
      "venue" : "In Building Bridges,",
      "citeRegEx" : "Brubaker and Vempala.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brubaker and Vempala.",
      "year" : 2008
    }, {
      "title" : "Learning mixtures of gaussians using the k-means algorithm",
      "author" : [ "Kamalika Chaudhuri", "Sanjoy Dasgupta", "Andrea Vattani" ],
      "venue" : "arXiv preprint arXiv:0912.0086,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning mixtures of gaussians",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Dasgupta.,? \\Q1999\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 1999
    }, {
      "title" : "Matrix Computations. Johns Hopkins Studies in the Mathematical Sciences",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : null,
      "citeRegEx" : "Golub and Loan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Golub and Loan.",
      "year" : 1996
    }, {
      "title" : "Pairwise variable selection for high-dimensional modelbased clustering",
      "author" : [ "Jian Guo", "Elizaveta Levina", "George Michailidis", "Ji Zhu" ],
      "venue" : null,
      "citeRegEx" : "Guo et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
      "author" : [ "Daniel Hsu", "Sham M Kakade" ],
      "venue" : "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Hsu and Kakade.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hsu and Kakade.",
      "year" : 2013
    }, {
      "title" : "Disentangling gaussians",
      "author" : [ "Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Kalai et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2012
    }, {
      "title" : "The spectral method for general mixture models",
      "author" : [ "Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala" ],
      "venue" : "In Learning Theory,",
      "citeRegEx" : "Kannan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2005
    }, {
      "title" : "Concentration inequalities and model selection",
      "author" : [ "Pascal Massart" ],
      "venue" : null,
      "citeRegEx" : "Massart.,? \\Q2007\\E",
      "shortCiteRegEx" : "Massart.",
      "year" : 2007
    }, {
      "title" : "Penalized model-based clustering with application to variable selection",
      "author" : [ "Wei Pan", "Xiaotong Shen" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Pan and Shen.,? \\Q2007\\E",
      "shortCiteRegEx" : "Pan and Shen.",
      "year" : 2007
    }, {
      "title" : "Variable selection for model-based clustering",
      "author" : [ "Adrian E Raftery", "Nema Dean" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Raftery and Dean.,? \\Q2006\\E",
      "shortCiteRegEx" : "Raftery and Dean.",
      "year" : 2006
    }, {
      "title" : "A two-round variant of em for gaussian mixtures",
      "author" : [ "Leonard J. Schulman", "Sanjoy Dasgupta" ],
      "venue" : "In Proc. 16th UAI (Conference on Uncertainty in Artificial Intelligence),",
      "citeRegEx" : "Schulman and Dasgupta.,? \\Q2000\\E",
      "shortCiteRegEx" : "Schulman and Dasgupta.",
      "year" : 2000
    }, {
      "title" : "Regularized k-means clustering of high-dimensional data and its asymptotic consistency",
      "author" : [ "Wei Sun", "Junhui Wang", "Yixin Fang" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "Sun et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2012
    }, {
      "title" : "Introduction to Nonparametric Estimation",
      "author" : [ "A.B. Tsybakov" ],
      "venue" : null,
      "citeRegEx" : "Tsybakov.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tsybakov.",
      "year" : 2009
    }, {
      "title" : "A spectral algorithm for learning mixture models",
      "author" : [ "Santosh Vempala", "Grant Wang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Vempala and Wang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Vempala and Wang.",
      "year" : 2004
    }, {
      "title" : "Minimax sparse principal subspace estimation in high dimensions",
      "author" : [ "Vincent Q Vu", "Jing Lei" ],
      "venue" : "arXiv preprint arXiv:1211.0373,",
      "citeRegEx" : "Vu and Lei.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vu and Lei.",
      "year" : 2012
    }, {
      "title" : "A framework for feature selection in clustering",
      "author" : [ "Daniela M Witten", "Robert Tibshirani" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Witten and Tibshirani.,? \\Q2010\\E",
      "shortCiteRegEx" : "Witten and Tibshirani.",
      "year" : 2010
    }, {
      "title" : "2, where ρ denotes the Hamming distance between two vectors (Tsybakov",
      "author" : [ ],
      "venue" : "Let Ω = {ω ∈ {0,",
      "citeRegEx" : "≥,? \\Q2009\\E",
      "shortCiteRegEx" : "≥",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001).",
      "startOffset" : 34,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001).",
      "startOffset" : 34,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension.",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be √ d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension.",
      "startOffset" : 84,
      "endOffset" : 312
    }, {
      "referenceID" : 0,
      "context" : "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be √ d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. The assumption that the components are spherical was removed in Brubaker and Vempala (2008). Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = Ω(d log d) samples.",
      "startOffset" : 84,
      "endOffset" : 551
    }, {
      "referenceID" : 0,
      "context" : "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be √ d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. The assumption that the components are spherical was removed in Brubaker and Vempala (2008). Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = Ω(d log d) samples. Other spectral methods include Kannan et al. (2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013).",
      "startOffset" : 84,
      "endOffset" : 744
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013).",
      "startOffset" : 8,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm.",
      "startOffset" : 8,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components.",
      "startOffset" : 8,
      "endOffset" : 203
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al.",
      "startOffset" : 8,
      "endOffset" : 376
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians.",
      "startOffset" : 8,
      "endOffset" : 401
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting μ > 1, Chaudhuri et al. (2009) show that n = Ω̃(d/μ) samples are needed.",
      "startOffset" : 8,
      "endOffset" : 547
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting μ > 1, Chaudhuri et al. (2009) show that n = Ω̃(d/μ) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and μ. When the mean separation is small μ < 1, they show that n = Ω̃(d/μ) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously.",
      "startOffset" : 8,
      "endOffset" : 2197
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting μ > 1, Chaudhuri et al. (2009) show that n = Ω̃(d/μ) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and μ. When the mean separation is small μ < 1, they show that n = Ω̃(d/μ) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering.",
      "startOffset" : 8,
      "endOffset" : 2311
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting μ > 1, Chaudhuri et al. (2009) show that n = Ω̃(d/μ) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and μ. When the mean separation is small μ < 1, they show that n = Ω̃(d/μ) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al.",
      "startOffset" : 8,
      "endOffset" : 2410
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting μ > 1, Chaudhuri et al. (2009) show that n = Ω̃(d/μ) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and μ. When the mean separation is small μ < 1, they show that n = Ω̃(d/μ) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al. (2012) and Guo et al.",
      "startOffset" : 8,
      "endOffset" : 2429
    }, {
      "referenceID" : 0,
      "context" : "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting μ > 1, Chaudhuri et al. (2009) show that n = Ω̃(d/μ) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and μ. When the mean separation is small μ < 1, they show that n = Ω̃(d/μ) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al. (2012) and Guo et al. (2010). The applied bioinformatics literature also contains a huge number of heuristic methods for this problem.",
      "startOffset" : 8,
      "endOffset" : 2451
    }, {
      "referenceID" : 18,
      "context" : "We conjecture that the lower bound is tight and that the gap could be closed by using a sparse principal component method as in Vu and Lei (2012) to find the relevant features.",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : ", 0), ρ(ωi, ωj) ≥ m8 for all 0 ≤ i < j ≤ M , and M ≥ 2, where ρ denotes the Hamming distance between two vectors (Tsybakov (2009)).",
      "startOffset" : 114,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : ", ωM ∈ Ω such that ρ(ωi, ωj) > s/2 for all 0 ≤ i < j ≤ M , and log(M + 1) ≥ s 5 log ( m s ) (Massart (2007), Lemma 4.",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.",
      "startOffset" : 25,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.",
      "startOffset" : 25,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al. (2012) and Guo et al.",
      "startOffset" : 25,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "(2012) and Guo et al. (2010) provide promising numerical evidence that variable selection does improve high dimensional clustering.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "(Tsybakov (2009)).",
      "startOffset" : 1,
      "endOffset" : 17
    }, {
      "referenceID" : 16,
      "context" : "and M ≥ 2, where ρ denotes the Hamming distance between two vectors (Tsybakov (2009)).",
      "startOffset" : 69,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "In particular, setting α = 3/4 and β = 1/3, we have that ρ(ωi, ωj) > s/2, log(M + 1) ≥ s 5 log ( m s ) , as long as s ≤ m/4 (Massart (2007), Lemma 4.",
      "startOffset" : 125,
      "endOffset" : 140
    } ],
    "year" : 2013,
    "abstractText" : "While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.",
    "creator" : "LaTeX with hyperref package"
  }
}