{
  "name" : "1202.6258.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets",
    "authors" : [ "Nicolas Le Roux", "Mark Schmidt", "Francis Bach" ],
    "emails" : [ "nicolas@le-roux.name", "mark.schmidt@inria.fr", "francis.bach@ens.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A plethora of the problems arising in machine learning involve computing an approximate minimizer of the sum of a loss function over a large number of training examples, where there is a large amount of redundancy between examples. The most wildly successful class of algorithms for taking advantage of this type of problem structure are stochastic gradient (SG) methods Robbins and Monro [1951], Bottou and LeCun [2003]. Although the theory behind SG methods allows them to be applied more generally, in the context of machine learning SG methods are typically used to solve the problem of optimizing a sample average over a finite training set, i.e.,\nminimize x∈Rp\ng(x) := 1\nn n∑ i=1 fi(x). (1)\nIn this work, we focus on such finite training data problems where each fi is smooth and the average function g is strongly-convex.\nAs an example, in the case of `2-regularized logistic regression we have fi(x) := λ 2 ‖x‖ 2 + log(1 + exp(−biaTi x)), where ai ∈ Rp and bi ∈ {−1, 1} are the training examples associated with a binary classification problem and λ is a regularization parameter. More generally, any `2-regularized empirical risk minimization problem of the form\nminimize x∈Rp\nλ 2 ‖x‖2 + 1 n n∑ i=1 li(x), (2)\nar X\niv :1\n20 2.\n62 58\nv4 [\nm at\nh. O\nC ]\nfalls in the framework of (1) provided that the loss functions li are convex and smooth. An extensive list of convex loss functions used in machine learning is given by Teo et al. [2007], and we can even include non-smooth loss functions (or regularizers) by using smooth approximations.\nThe standard full gradient (FG) method, which dates back to Cauchy [1847], uses iterations of the form\nxk+1 = xk − αkg′(xk) = xk − αk n n∑ i=1 f ′i(x k). (3)\nUsing x∗ to denote the unique minimizer of g, the FG method with a constant step size achieves a linear convergence rate:\ng(xk)− g(x∗) = O(ρk),\nfor some ρ < 1 which depends on the condition number of g [Nesterov, 2004, Theorem 2.1.15]. Linear convergence is also known as geometric or exponential convergence, because the cost is cut by a fixed fraction on each iteration. Despite the fast convergence rate of the FG method, it can be unappealing when n is large because its iteration cost scales linearly in n. SG methods, on the other hand, have an iteration cost which is independent of n, making them suited for that setting. The basic SG method for optimizing (1) uses iterations of the form\nxk+1 = xk − αkf ′ik(x k), (4)\nwhere αk is a step-size and a training example ik is selected uniformly among the set {1, . . . , n}. The randomly chosen gradient f ′ik(x\nk) yields an unbiased estimate of the true gradient g′(xk), and one can show under standard assumptions that, for a suitably chosen decreasing step-size sequence {αk}, the SG iterations achieve the sublinear convergence rate\nE[g(xk)]− g(x∗) = O(1/k),\nwhere the expectation is taken with respect to the selection of the ik variables. Under certain assumptions this convergence rate is optimal for strongly-convex optimization in a model of computation where the algorithm only accesses the function through unbiased measurements of its objective and gradient (see Nemirovski and Yudin [1983], Nemirovski et al. [2009], Agarwal et al. [2012]). Thus, we cannot hope to obtain a better convergence rate if the algorithm only relies on unbiased gradient measurements. Nevertheless, by using the stronger assumption that the functions are sampled from a finite dataset, in this paper we show that we can achieve an exponential converengence rate while preserving the iteration cost of SG methods.\nThe primay contribution of this work is the analysis of a new algorithm that we call the stochastic average gradient (SAG) method, a randomized variant of the incremental aggregated gradient (IAG) method Blatt et al. [2007], which combines the low iteration cost of SG methods with a linear convergence rate as in FG methods. The SAG method uses iterations of the form\nxk+1 = xk − αk n n∑ i=1 yki , (5)\nwhere at each iteration a random training example ik is selected and we set\nyki =\n{ f ′i(x k) if i = ik,\nyk−1i otherwise.\nThat is, like the FG method, the step incorporates a gradient with respect to each training example. But, like the SG method, each iteration only computes the gradient with respect to a single training example and the cost of the iterations is independent of n. Despite the low cost of the SAG\niterations, in this paper we show that the SAG iterations have a linear convergence rate, like the FG method. That is, by having access to ik and by keeping a memory of the most recent gradient value computed for each training example i, this iteration achieves a faster convergence rate than is possible for standard SG methods. Further, in terms of effective passes through the data, we also show that for certain problems the convergence rate of SAG is faster than is possible for standard FG methods.\nIn a machine learning context where g(x) is a training cost associated with a predictor parameterized by x, we are often ultimately interested in the testing cost, the expected loss on unseen data points. Note that a linear convergence rate for the training cost does not translate into a similar rate for the testing cost, and an appealing propertly of SG methods is that they achieve the optimal O(1/k) rate for the testing cost as long as every datapoint is seen only once. However, as is common in machine learning, we assume that we are only given a finite training data set and thus that datapoints are revisited multiple times. In this context, the analysis of SG methods only applies to the training cost and, although our analysis also focuses on the training cost, in our experiments the SAG method typically reached the optimal testing cost faster than both FG and SG methods.\nThe next section reviews closely-related algorithms from the literature, including previous attempts to combine the appealing aspects of FG and SG methods. However, despite 60 years of extensive research on SG methods, most of the applications focusing on finite datasets, we are not aware of any other SG method that achieves a linear convergence rate while preserving the iteration cost of standard SG methods. Section 3 states the (standard) assumptions underlying our analysis and gives the main technical results; we first give a slow linear convergence rate that applies for any problem, and then give a very fast linear convergence rate that applies when n is sufficiently large. Section 4 discusses practical implementation issues, including how to reduce the storage cost from O(np) to O(n) when each fi only depends on a linear combination of x. Section 5 presents a numerical comparison of an implementation based on SAG to SG and FG methods, indicating that the method may be very useful for problems where we can only afford to do a few passes through a data set."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is a large variety of approaches available to accelerate the convergence of SG methods, and a full review of this immense literature would be outside the scope of this work. Below, we comment on the relationships between the new method and several of the most closely-related ideas.\nMomentum: SG methods that incorporate a momentum term use iterations of the form\nxk+1 = xk − αkf ′ik(x k) + βk(x k − xk−1),\nsee Tseng [1998]. It is common to set all βk = β for some constant β, and in this case we can rewrite the SG with momentum method as\nxk+1 = xk − ∑k j=1 αjβ k−jf ′ij (x j).\nWe can re-write the SAG updates (5) in a similar form as\nxk+1 = xk − ∑k j=1 αkS(j, i1:k)f ′ ij (xj), (6)\nwhere the selection function S(j, i1:k) is equal to 1/n if j corresponds to the last iteration where j = ik and is set to 0 otherwise. Thus, momentum uses a geometric weighting of previous gradients while the SAG iterations select and average the most recent evaluation of each previous gradient.\nWhile momentum can lead to improved practical performance, it still requires the use of a decreasing sequence of step sizes and is not known to lead to a faster convergence rate.\nGradient Averaging: Closely related to momentum is using the sample average of all previous gradients, xk+1 = xk − αkk ∑k j=1 f ′ ij (xj),\nwhich is similar to the SAG iteration in the form (5) but where all previous gradients are used. This approach is used in the dual averaging method Nesterov [2009], and while this averaging procedure leads to convergence for a constant step size and can improve the constants in the convergence rate Xiao [2010], it does not improve on the O(1/k) rate.\nIterate Averaging: Rather than averaging the gradients, some authors use the basic SG iteration but take an average over xk values. With a suitable choice of step-sizes, this gives the same asymptotic efficiency as Newton-like second-order SG methods and also leads to increased robustness of the convergence rate to the exact sequence of step sizes Polyak and Juditsky [1992]. Baher’s method [Kushner and Yin, 2003, §1.3.4] combines gradient averaging with online iterate averaging, and also displays appealing asymptotic properties. The epoch SG method uses averaging to obtain the O(1/k) rate even for non-smooth objectives Hazan and Kale [2011]. However, the convergence rates of these averaging methods remain sublinear.\nStochastic versions of FG methods: Various options are available to accelerate the convergence of the FG method for smooth functions, such as the accelerated full gradient (AFG) method Nesterov [1983], as well as classical techniques based on quadratic approximations such as non-linear conjugate gradient, quasi-Newton, and Hessian-free Newton methods. Several authors have analyzed stochastic variants of these algorithms Schraudolph [1999], Sunehag et al. [2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al. [2009]. Alternately, if we split the convergence rate into a deterministic and stochastic part, these methods can improve the dependency on the deterministic part Ghadimi and Lan [2010], Xiao [2010]. However, as with all other methods we have discussed thus far in this section, we are not aware of any existing method of this flavor that improves on the O(1/k) rate.\nConstant step size: If the SG iterations are used with a constant step size (rather than a decreasing sequence), then the convergence rate of the method can be split into two parts [Nedic and Bertsekas, 2000, Proposition 2.4], where the first part depends on k and converges linearly to 0 and the second part is independent of k but does not converge to 0. Thus, with a constant step size the SG iterations have a linear convergence rate up to some tolerance, and in general after this point the iterations do not make further progress. Indeed, convergence of the basic SG method with a constant step size has only been shown under extremely strong assumptions about the relationship between the functions fi Solodov [1998]. This contrasts with the method we present in this work which converges to the optimal solution using a constant step size and does so with a linear rate (without additional assumptions).\nAccelerated methods: Accelerated SG methods, which despite their name are not related to the aforementioned AFG method, take advantage of the fast convergence rate of SG methods with a constant step size. In particular, accelerated SG methods use a constant step size by default, and only decrease the step size on iterations where the inner-product between successive gradient estimates is negative Kesten [1958], Delyon and Juditsky [1993]. This leads to convergence of the method and allows it to potentially achieve periods of linear convergence where the step size stays constant. However, the overall convergence rate of the method remains sublinear.\nHybrid Methods: Some authors have proposed variants of the SG method for problems of the form (1) that seek to gradually transform the iterates into the FG method in order to achieve a linear convergence rate. Bertsekas proposes to go through the data cyclically with a specialized weighting that allows the method to achieve a linear convergence rate for strongly-convex quadratic\nfunctions Bertsekas [1997]. However, the weighting is numerically unstable and the linear convergence rate treats full passes through the data as iterations. A related strategy is to group the fi functions into ‘batches’ of increasing size and perform SG iterations on the batches Friedlander and Schmidt [2012]. In both cases, the iterations that achieve the linear rate have a cost that is not independent of n, as opposed to SAG.\nIncremental Aggregated Gradient: Finally, Blatt et al. presents the most closely-related algorithm, the IAG method Blatt et al. [2007]. This method is identical to the SAG iteration (5), but uses a cyclic choice of ik rather than sampling the ik values. This distinction has several important consequences. In particular, Blatt et al. are only able to show that the convergence rate is linear for strongly-convex quadratic functions (without deriving an explicit rate), and their analysis treats full passes through the data as iterations. Using a non-trivial extension of their analysis and a proof technique involving bounding the gradients and iterates simultaneously by a Lyapunov potential function, in this work we give an explicit linear convergence rate for general strongly-convex functions using the SAG iterations that only examine a single training example. Further, as our analysis and experiments show, when the number of training examples is sufficiently large, the SAG iterations achieve a linear convergence rate under a much larger set of step sizes than the IAG method. This leads to more robustness to the selection of the step size and also, if suitably chosen, leads to a faster convergence rate and improved practical performance. We also emphasize that in our experiments IAG and the basic FG method perform similarly, while SAG performs much better, showing that the simple change (random selection vs. cycling) can dramatically improve optimization performance."
    }, {
      "heading" : "3 Convergence Analysis",
      "text" : "In our analysis we assume that each function fi in (1) is differentiable and that each gradient f ′ i is Lipschitz-continuous with constant L, meaning that for all x and y in Rp we have\n‖f ′i(x)− f ′i(y)‖ ≤ L‖x− y‖.\nThis is a fairly weak assumption on the fi functions, and in cases where the fi are twice-differentiable it is equivalent to saying that the eigenvalues of the Hessians of each fi are bounded above by L. In addition, we also assume that the average function g = 1n ∑n i=1 fi is strongly-convex with constant µ > 0, meaning that the function x 7→ g(x) − µ2 ‖x‖ 2 is convex. This is a stronger assumption and is not satisfied by all machine learning models. However, note that in machine learning we are typically free to choose the regularizer, and we can always add an `2-regularization term as in Eq. (2) to transform any convex problem into a strongly-convex problem (in this case we have µ ≥ λ). Note that strong-convexity implies that the problem is solvable, meaning that there exists some unique x∗ that achieves the optimal function value. Our convergence results assume that we initialize y0i to a zero vector for all i, and our results depend on the variance of the gradient norms at the optimum x∗, denoted by σ2 = 1n ∑ i ‖f ′i(x∗)‖2. Finally, all our convergence results consider expectations with respect to the internal randomization of the algorithm, and not with respect to the data (which are assumed to be deterministic and fixed).\nWe first consider the convergence rate of the method when using a constant step size of αk = 1\n2nL , which is similar to the step size needed for convergence of the IAG method in practice.\nProposition 1 With a constant step size of αk = 1 2nL , the SAG iterations satisfy for k ≥ 1:\nE [ ‖xk − x∗‖2 ] 6 (\n1− µ 8Ln\n)k[ 3‖x0 − x∗‖2 + 9σ2\n4L2\n] .\nThe proof is given in the Appendix. Note that the SAG iterations also trivially obtain the O(1/k) rate achieved by SG methods, since(\n1− µ 8Ln\n)k 6 exp ( − kµ\n8Ln\n) 6 8Ln\nkµ = O(n/k),\nalbeit with a constant which is proportional to n. Despite this constant, they are advantageous over SG methods in later iterations because they obtain an exponential convergence rate as in FG methods. We also note that an exponential convergence rate is obtained for any constant step size smaller than 12nL .\nIn terms of passes through the data, the rate in Proposition 1 is similar to that achieved by the basic FG method. However, our next result shows that, if the number of training examples is slightly larger than L/µ (which will often be the case, as discussed in Section 6), then the SAG iterations can use a larger step size and obtain a better convergence rate that is independent of µ and L (see proof in the Appendix).\nProposition 2 If n > 8Lµ , with a step size of αk = 1 2nµ the SAG iterations satisfy for k > n:\nE [ g(xk)− g(x∗) ] 6 C ( 1− 1\n8n\n)k ,\nwith C =\n[ 16L\n3n ‖x0 − x∗‖2 + 4σ\n2\n3nµ\n( 8 log ( 1 + µn\n4L\n) + 1 )] .\nWe state this result for k > n because we assume that the first n iterations of the algorithm use an SG method and that we initialize the subsequent SAG iterations with the average of the iterates, which leads to an O((log n)/k) rate. In contrast, using the SAG iterations from the beginning gives the same rate but with a constant proportional to n. Note that this bound is obtained when initializing all yi to zero after the SG phase.\n1 However, in our experiments we do not use the SG initialization but rather use a minor variant of SAG (discussed in the next section), which appears more difficult to analyze but which gives better performance.\nIt is interesting to compare this convergence rate with the known convergence rates of first-order methods [Nesterov, 2004, see §2]. For example, if we take n = 100000, L = 100, and µ = 0.01 then the basic FG method has a rate of ((L− µ)/(L+ µ))2 = 0.9996 and the ‘optimal’ AFG method has a faster rate of (1 − √ µ/L) = 0.9900. In contrast, running n iterations of SAG has a much faster rate of (1− 1/8n)n = 0.8825 using the same number of evaluations of f ′i . Further, the lower-bound for a black-box first-order method is (( √ L−√µ)/( √ L+ √ µ))2 = 0.9608, indicating that SAG can be substantially faster than any FG method that does not use the structure of the problem.2 In the Appendix, we compare Propositions 1 and 2 to the rates of primal and dual FG and coordinate-wise methods for the special case of `2-regularized leasts squares.\nEven though n appears in the convergence rate, if we perform n iterations of SAG (i.e., one effective pass through the data), the error is multiplied by (1 − 1/8n)n ≤ exp(−1/8), which is independent of n. Thus, each pass through the data reduces the excess cost by a constant multiplicative factor that is independent of the problem, as long as n > 8L/µ. Further, while the step size in Proposition 2 depends on µ and n, we can obtain the same convergence rate by using a step size as large as αk = 1 16L . This is because the proposition is true for all values of µ satisfying µ L > 8 n , so we can choose the smallest possible value of µ = 8Ln . We have observed in practice that the IAG method\n1While it may appear suboptimal to not use the gradients computed during the n iterations of stochastic gradient descent, using them only improves the bound by a constant.\n2Note that L in the SAG rates is based on the f ′i functions, while in the FG methods it is based on g ′ which can\nbe much smaller.\nwith a step size of αk = 1\n2nµ may diverge, even under these assumptions. Thus, for certain problems the SAG iterations can tolerate a much larger step size, which leads to increased robustness to the selection of the step size. Further, as our analysis and experiments indicate, the ability to use a large step size leads to improved performance of the SAG iterations.\nWhile we have stated Proposition 1 in terms of the iterates and Proposition 2 in terms of the function values, the rates obtained on iterates and function values are equivalent because, by the Lipschitz and strong-convexity assumptions, we have µ2 ‖x k − x∗‖2 6 g(xk)− g(x∗) 6 L2 ‖x k − x∗‖2."
    }, {
      "heading" : "4 Implementation Details",
      "text" : "In this section we describe modifications that substantially reduce the SAG iteration’s memory requirements, as well as modifications that lead to better practical performance.\nStructured gradients: For many problems the storage cost of O(np) for the yki vectors is prohibitive, but we can often use structure in the f ′i to reduce this cost. For example, many loss functions fi take the form fi(a T i x) for a vector ai. Since ai is constant, for these problems we only need to store the scalar f ′ik(u k i ) for u k i = a T ik xk rather than the full gradient aTi f ′ i(u k i ), reducing the storage cost to O(n). Further, because of the simple form of the SAG updates, if ai is sparse we can use ‘lazy updates’ in order to reduce the iteration cost from O(p) down to the sparsity level of ai.\nMini-batches: To employ vectorization and parallelism, practical SG implementations often group training examples into ‘mini-batches’ and perform SG iterations on the mini-batches. We can also use mini-batches within the SAG iterations, and for problems with dense gradients this decreases the storage requirements of the algorithm since we only need a yki for each mini-batch. Thus, for example, using mini-batches of size 100 leads to a 100-fold reduction in the storage cost.\nStep-size re-weighting: On early iterations of the SAG algorithm, when most yki are set to the uninformative zero vector, rather than dividing αk in (5) by n we found it was more effective to divide by m, the number of unique ik values that we have sampled so far (which converges to n). This modification appears more difficult to analyze, but with this modification we found that the SAG algorithm outperformed the SG/SAG hybrid algorithm analyzed in Proposition 2.\nExact regularization: For regularized objectives like (2) we can use the exact gradient of the regularizer, rather than approximating it. For example, our experiments on `2-regularized optimization problems used the recursion\nd← d− yi, yi ← l′i(xk), d← d+ yi, x← ( 1− αλ ) x− α\nm d . (7)\nThis can be implemented efficiently for sparse data sets by using the representation x = κz, where κ is a scalar and z is a vector, since the update based on the regularizer simply updates κ.\nLarge step sizes: Proposition 1 requires αk 6 1/2Ln while under an additional assumption Proposition 2 allows αk 6 1/16L. In practice we observed better performance using step sizes of αk = 1/L and αk = 2/(L + nµ). These step sizes seem to work even when the additional assumption of Proposition 2 is not satisfied, and we conjecture that the convergence rates under these step sizes are much faster than the rate obtained in Proposition 1 for the general case.\nLine search: Since L is generally not known, we experimented with a basic line-search, where we start with an initial estimate L0, and we double this estimate whenever we do not satisfy the instantiated Lipschitz inequality\nfik(x k − (1/Lk)f ′ik(x k)) 6 fik(x k)− 1 2Lk ‖f ′ik(x k)‖2.\nTo avoid instability caused by comparing very small numbers, we only do this test when ‖f ′ik(x k)‖2 > 10−8. To allow the algorithm to potentially achieve a faster rate due to a higher degree of local smoothness, we multiply Lk by 2 (−1/n) after each iteration."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "Our experiments compared an extensive variety of competitive FG and SG methods. Our first experiments focus on the following methods, which we chose because they have no dataset-dependent tuning parameters:\n– Steepest: The full gradient method described by iteration (3), with a line-search that uses cubic Hermite polynomial interpolation to find a step size satisfying the strong Wolfe conditions, and where the parameters of the line-search were tuned for the problems at hand.\n– AFG: Nesterov’s accelerated full gradient method Nesterov [1983], where iterations of (3) with a fixed step size are interleaved with an extrapolation step, and we use an adaptive line-search based on Liu et al. [2009].\n– L-BFGS: A publicly-available limited-memory quasi-Newton method that has been tuned for log-linear models.3 This method is by far the most complicated method we considered.\n– Pegasos: The state-of-the-art SG method described by iteration (4) with a step size of αk = 1/µk and a projection step onto a norm-ball known to contain the optimal solution ShalevShwartz et al. [2007].\n– RDA: The regularized dual averaging method Xiao [2010], another recent state-of-the-art SG method.\n– ESG: The epoch SG method Hazan and Kale [2011], which runs SG with a constant step size and averaging in a series of epochs, and is optimal for non-smooth stochastic strongly-convex optimization.\n– NOSG: The nearly-optimal SG method Ghadimi and Lan [2010], which combines ideas from SG and AFG methods to obtain a nearly-optimal dependency on a variety of problem-dependent constants.\n– SAG: The proposed stochastic average gradient method described by iteration (5) using the modifications discussed in the previous section. We used a step-size of αk = 2/(Lk + nλ) where Lk is either set constant to the global Lipschitz constant (SAG-C) or set by adaptively estimating the constant with respect to the logistic loss function using the line-search described in the previous section (SAG-LS). The SAG-LS method was initialized with L0 = 1 .\nThe theoretical convergence rates suggest the following strategies for deciding on whether to use an FG or an SG method:\n1. If we can only afford one pass through the data, then an SG method should be used.\n2. If we can afford to do many passes through the data (say, several hundred), then an FG method should be used.\n3http://www.di.ens.fr/~mschmidt/Software/minFunc.html\nWe expect that the SAG iterations will be most useful between these two extremes, where we can afford to do more than one pass through the data but cannot afford to do enough passes to warrant using FG algorithms like L-BFGS. To test whether this is indeed the case on real data sets, we performed experiments on a set of freely available benchmark binary classification data sets. The quantum (p = 50000, p = 78) and protein (n = 145751, p = 74) data set was obtained from the KDD Cup 2004 website,4 the sido data set was obtained from the Causality Workbench website,5 while the rcv1 (n = 20242, p = 47236) and covertype (n = 581012, p = 54) data sets were obtained from the LIBSVM data website.6 Although our method can be applied to any differentiable function, on these data sets we focus on the `2-regularized logistic regression problem, with λ = 1/n. We split each dataset in two, training on one half and testing on the other half. We added a (regularized) bias term to all data sets, and for dense features we standardized so that they would have a mean of zero and a variance of one. We measure the training and testing costs as a function of the number of effective passes through the data, measured as the number of f ′i evaluations divided by n. These results are thus independent of the practical implementation of the algorithms. We plot the training and testing costs of the different methods for 30 effective passes through the data in Figure 1.\nIn our second series of experiments, we sought to test whether SG methods (or the IAG method) with a very carefully chosen step size would be competitive with the SAG iterations. In particular, we compared the following variety of basic FG and SG methods.\n1. FG: The full gradient method described by iteration (3).\n2. AFG: The accelerated full gradient method Nesterov [1983], where iterations of (3) are interleaved with an extrapolation step.\n3. peg: The pegasos algorithm of Shalev-Shwartz et al. [2007], but where we multiply the step size by a constant.\n4. SG: The stochastic gradient method described by iteration (4), where we use a constant stepsize.\n5. ASG: The stochastic gradient method described by iteration (4), where we use a constant step size and average the iterates.7\n6. IAG: The incremental aggregated gradient method of Blatt et al. [2007] described by iteration (5) but with a cyclic choice of ik.\n7. SAG: The proposed stochastic average gradient method described by iteration (5).\nFor all of the above methods, we chose the step size that gave the best performance among powers of 10. On the full data sets, we compare these methods to each other and to the L-BFGS and the SAG-LS algorithms from the previous experiment in Figure 2, which also shows the selected step sizes.\nWe can observe several trends across these experiments:\n– FG vs. SG: Although the performance of SG methods can be catastrophic if the step size is not chosen carefully (e.g., the quantum and covertype data), with a carefully-chosen step-size\n4http://osmot.cs.cornell.edu/kddcup 5 http://www.causality.inf.ethz.ch/home.php 6 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets 7We have also compared to a variety of other SG methods, such as SG with momentum, SG with gradient averaging, accelerated SG, and using SG but delaying averaging until after the first effective pass. However, none of these SG methods performed better than the ASG method above so we omit them to keep the plots simple.\nthe SG methods always do substantially better than FG methods on the first few passes through the data. In contrast, the adaptive FG methods in the first experiment are not sensitive to the step size and because of its steady progress the best FG method (L-BFGS) always eventually passes the SG methods.\n– (FG and SG) vs. SAG: The SAG iterations seem to achieve the best of both worlds. They start out substantially better than FG methods, but continue to make steady (linear) progress which leads to better performance than SG methods. The significant speed-up observed for SAG in reaching low training costs often also seems to translate into reaching the optimal testing cost more quickly than the other methods. We also note that the proposed line-search seems to perform as well or better than choosing the optimal fixed step-size in hind sight.\n– IAG vs. SAG: The second experiment shows that the IAG method performs similarly to the regular FG method, and they also show the surprising result that the randomized SAG method outperforms the closely-related deterministic IAG method by a very large margin. This is due to the larger step sizes used by the SAG iterations, which would cause the IAG iterations to diverge."
    }, {
      "heading" : "6 Discussion",
      "text" : "Optimal regularization strength: One might wonder if the additional hypothesis in Proposition 2 is satisfied in practice. In a learning context, where each function fi is the loss associated to a single data point, L is equal to the largest value of the loss second derivative ξ (1 for the square loss, 1/4 for the logistic loss) times R2, where R is a the uniform bound on the norm of each data point. Thus, the constraint µL > 8 n is satisfied when λ > 8ξR2\nn . In low-dimensional settings, the optimal regularization parameter is of the form C/n Liang et al. [2009] where C is a scalar constant, and may thus violate the constraint. However, the improvement with respect to regularization parameters of\nthe form λ = C/ √ n is known to be asymptotically negligible, and in any case in such low-dimensional settings, regular stochastic or batch gradient descent may be efficient enough in practice. In the more interesting high-dimensional settings where the dimension p of our covariates is not small compared to the sample size n, then all theoretical analyses we are aware of advocate settings of λ which satisfy this constraint. For example, Sridharan et al. [2008] considers parameters of the form λ = C√\nn in the parametric setting, while Eberts and Steinwart [2011] considers λ = C nβ with β < 1\nin a non-parametric setting.\nTraining cost vs. testing cost: The theoretical contribution of this work is limited to the convergence rate of the training cost. Though there are several settings where this is the metric of interest (e.g., variational inference in graphical models), in many cases one will be interested in the convergence speed of the testing cost. Since the O(1/k) convergence rate of the testing cost, achieved by SG methods with decreasing step sizes (and a single pass through the data), is provably optimal when the algorithm only accesses the function through unbiased measurements of the objective and its gradient, it is unlikely that one can obtain a linear convergence rate for the testing cost with the SAG iterations. However, as shown in our experiments, the testing cost of the SAG iterates often reaches its minimum quicker than existing SG methods, and we could expect to improve the constant in the O(1/k) convergence rate, as is the case with online second-order methods Bottou and Bousquet [2007].\nStep-size selection and termination criteria: The three major disadvantages of SG methods are: (i) the slow convergence rate, (ii) deciding when to terminate the algorithm, and (iii) choosing the step size while running the algorithm. This paper showed that the SAG iterations achieve a much faster convergence rate, but the SAG iterations may also be advantageous in terms of tuning step sizes and designing termination criteria. In particular, the SAG iterations suggest a natural termination criterion; since the average of the yki variables converges to g\n′(xk) as ‖xk − xk−1‖ converges to zero, we can use (1/n)‖ ∑ i y k i ‖ as an approximation of the optimality of xk. Further, while SG methods require specifying a sequence of step sizes and mispecifying this sequence can have a disastrous effect on the convergence rate [Nemirovski et al., 2009, §2.1], our theory shows that the SAG iterations iterations achieve a linear convergence rate for any sufficiently small constant step size and our experiments indicate that a simple line-search gives strong performance."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Nicolas Le Roux, Mark Schmidt, and Francis Bach are supported by the European Research Council (SIERRA-ERC-239993). Mark Schmidt is also supported by a postdoctoral fellowship from the Natural Sciences and Engineering Research Council of Canada (NSERC)."
    }, {
      "heading" : "Appendix",
      "text" : "In this Appendix, we first give the proofs of the two propositions. Subsequently, we compare the convergence rates of primal and dual FG and coordinate-wise methods to the rates of SAG for `2-regularized least squares in terms of effective passes through the data."
    }, {
      "heading" : "A Proofs of the propositions",
      "text" : "We present here the proofs of Propositions 1 and 2."
    }, {
      "heading" : "A.1 Problem set-up and notations",
      "text" : "We use g = 1n ∑n i=1 fi to denote a µ−strongly convex function, where the functions fi, i = 1, . . . , n are convex functions from Rp to R with L-Lipschitz continuous gradients. Let us denote by x∗ the unique minimizer of g.\nFor k > 1, the stochastic average gradient algorithm performs the recursion\nxk = xk−1 − α n n∑ i=1 yki ,\nwhere an ik is selected in {1, . . . , n} uniformly at random and we set\nyki =\n{ f ′i(x k−1) if i = ik,\nyk−1i otherwise.\nDenoting zki a random variable which takes the value 1 − 1n with probability 1 n and − 1 n otherwise (thus with zero expectation), this is equivalent to\nyki =\n( 1− 1\nn\n) yk−1i + 1\nn f ′i(x\nk−1) + zki [ f ′i(x k−1)− yk−1i ]\nxk = xk−1 − α n n∑ i=1 [( 1− 1 n ) yk−1i + 1 n f ′i(x k−1) + zki [ f ′i(x k−1)− yk−1i ]]\n= xk−1 − α n\n[( 1− 1\nn\n) e>yk−1 + g′(xk−1) + (zk)> [ f ′(xk−1)− yk−1 ]] ,\nwith\ne =  I... I  ∈ Rnp×p, f ′(x) =  f ′ 1(x) ...\nf ′n(x)\n ∈ Rnp, zk =  z\nk 1 I ... zknI  ∈ Rnp×p. Using this definition of zk, we have E[(zk)(zk)>] = 1nI − 1 n2 ee\n>. Note that, for a given k, the variables zk1 , . . . , z k n are not independent.\nWe also use the notation\nθk =  yk1 ... ykn xk  ∈ R(n+1)p, θ∗ =  f ′1(x ∗) ... f ′n(x ∗) x∗  ∈ R(n+1)p .\nFinally, if M is a tp× tp matrix and m is a tp× p matrix, then:\n• diag(M) is the tp × p matrix being the concatenation of the t (p × p)-blocks on the diagonal of M ;\n• Diag(m) is the tp × tp block-diagonal matrix whose (p × p)-blocks on the diagonal are equal to the (p× p)-blocks of m."
    }, {
      "heading" : "A.2 Outline of the proofs",
      "text" : "Each Proposition will be proved in multiple steps.\n1. We shall find a Lyapunov function Q from R(n+1)p to R such that the sequence EQ(θk) decreases at a linear rate.\n2. We shall prove that Q(θk) dominates ‖xk−x∗‖2 (in the case of Proposition 2) or g(xk)−g(x∗) (in the case of Proposition 2) by a constant for all k.\n3. In the case of Proposition 2, we show how using one pass of stochastic gradient as the initialization provides the desired result.\nThroughout the proofs, Fk will denote the σ-field of information up to (and including time k), i.e., Fk is the σ-field generated by z1, . . . , zk.\nA.3 Convergence results for stochastic gradient descent\nThe constant in both our bounds depends on the initialization chosen. While this does not affect the linear convergence of the algorithm, the bound we obtain for the first few passes through the data is the O(1/k) rate one would get using stochastic gradient descent, but with a constant proportional to n. This problem can be alleviated for the second bound by running stochastic gradient descent for a few iterations before running the SAG algorithm. In this section, we provide bounds for the stochastic gradient descent algorithm which will prove useful for the SAG algorithm.\nThe assumptions made in this section about the functions fi and the function g are the same as the ones used for SAG. To get initial values for x0 and y0, we will do one pass of standard stochastic gradient.\nWe denote by σ2 = 1n ∑n i=1 ‖f ′i(x∗)‖2 the variance of the gradients at the optimum. We will use the following recursion: x̃k = x̃k−1 − γkf ′ik ( x̃k−1 ) .\nDenoting δk = E‖x̃k − x∗‖2, we have (following Bach and Moulines [2011]) δk 6 δk−1 − 2γk(1− γkL)E [ g′(x̃k−1)>(x̃k−1 − x∗) ] + 2γ2kσ 2 .\nIndeed, we have\n‖x̃k − x∗‖2 = ‖x̃k−1 − x∗‖2 − 2γkf ′ik(x̃ k−1)>(x̃k−1 − x∗) + γ2k‖f ′ik(x̃ k−1)‖2\n6 ‖x̃k−1 − x∗‖2 − 2γkf ′ik(x̃ k−1)>(x̃k−1 − x∗) + 2γ2k‖f ′ik(x ∗)‖2 + 2γ2k‖f ′ik(x̃ k−1)− f ′ik(x ∗)‖2 6 ‖x̃k−1 − x∗‖2 − 2γkf ′ik(x̃ k−1)>(x̃k−1 − x∗) + 2γ2k‖f ′ik(x ∗)‖2\n+ 2Lγ2k(f ′ ik (x̃k−1)− f ′ik(x ∗))>(x̃k−1 − x∗) .\nBy taking expectations, we get E [ ‖x̃k − x∗‖2|Fk−1 ] 6 ‖x̃k−1 − x∗‖2 − 2γkg′(x̃k−1)>(x̃k−1 − x∗) + 2γ2kσ2 + 2Lγ2kg′(x̃k−1)>(x̃k−1 − x∗)\nE [ ‖x̃k − x∗‖2 ] 6 E [ ‖x̃k−1 − x∗‖2 ] − 2γk(1− γkL)E [ g′(x̃k−1)>(x̃k−1 − x∗) ] + 2γ2kσ 2\nThus, if we take\nγk = 1\n2L+ µ2 k ,\nwe have γk 6 2γk(1− γkL) and\nδk 6 δk−1 − γkE [ g′(x̃k−1)>(xk−1 − x∗) ] + 2γ2kσ 2\n6 δk−1 − γk [ E [ g(xk−1)− g(x∗) ] + µ\n2 δk−1\n] + 2γ2kσ 2 using the strong convexity of g\nEg(xk−1)− g(x∗) 6 − 1 γk δk +\n( 1\nγk − µ 2\n) δk−1 + 2γkσ 2\n6 − ( 2L+ µ 2 k ) δk + ( 2L+ µ 2 (k − 1) ) δk−1 + 2γkσ 2 .\nAveraging from i = 0 to k − 1 and using the convexity of g, we have\n1\nk k−1∑ i=0 Eg(xk−1)− g(x∗) 6 2L k δ0 + 2σ2 k k∑ i=1 γi\nEg\n( 1\nk k−1∑ i=0 xi\n) − g(x∗) 6 2L\nk δ0 +\n2σ2\nk k∑ i=1 γi\n6 2L k ‖x0 − x∗‖2 + 2σ\n2\nk k∑ i=1\n1\n2L+ µ2 i\n6 2L k L‖x0 − x∗‖2 + 2σ\n2\nk ∫ k 0\n1\n2L+ µ2 t dt\n6 2L k ‖x0 − x∗‖2 + 4σ\n2\nkµ log\n( 1 + µk\n4L\n) .\nA.4 Important lemma In both proofs, our Lyapunov function contains a quadratic termR(θk) = (θk − θ∗)> (\nA b b> c\n) (θk − θ∗)\nfor some values of A, b and c. The lemma below computes the value of R(θk) in terms of elements of θk−1.\nLemma 1 If P = ( A b b> c ) , for A ∈ Rnp×np, b ∈ Rnp×p and c ∈ Rp×p, then\nE [ (θk − θ∗)> (\nA b b> c\n) (θk − θ∗) ∣∣∣∣Fk−1] = (yk−1 − f ′(x∗))> [( 1− 2\nn\n) S + 1\nn Diag(diag(S))\n] (yk−1 − f ′(x∗))\n+ 1\nn (f ′(xk−1)− f ′(x∗))>Diag(diag(S))(f ′(xk−1)− f ′(x∗))\n+ 2\nn (yk−1 − f ′(x∗))> [S −Diag(diag(S))] (f ′(xk−1)− f ′(x∗))\n+ 2 ( 1− 1\nn\n) (yk−1 − f ′(x∗))> [ b− α n ec ] (xk−1 − x∗)\n+ 2\nn (f ′(xk−1)− f ′(x∗))>\n[ b− α n ec ] (xk−1 − x∗)\n+ (xk−1 − x∗)>c(xk−1 − x∗) ,\nwith\nS = A− α n be> − α n eb> +\nα2 n2 ece> .\nNote that for square n × n matrix, diag(M) denotes a vector of size n composed of the diagonal of M , while for a vector m of dimension n, Diag(m) is the n × n diagonal matrix with m on its diagonal. Thus Diag(diag(M)) is a diagonal matrix with the diagonal elements of M on its diagonal, and diag(Diag(m)) = m.\nProof Throughout the proof, we will use the equality g′(x) = e>f ′(x)/n. Moreover, all conditional expectations of linear functions of zk will be equal to zero.\nWe have E [ (θk − θ∗)> (\nA b b> c\n) (θk − θ∗) ∣∣∣∣Fk−1] = E [ (yk − f ′(x∗))>A(yk − f ′(x∗)) + 2(yk − f ′(x∗))>b(xk − x∗) + (xk − x∗)>c(xk − x∗)|Fk−1 ] .\n(8)\nThe first term (within the expectation) on the right-hand side of Eq. (8) is equal to (yk − f ′(x∗))>A(yk − f ′(x∗)) = (\n1− 1 n\n)2 (yk−1 − f ′(x∗))>A(yk−1 − f ′(x∗))\n+ 1\nn2 (f ′(xk−1)− f ′(x∗))>A(f ′(xk−1)− f ′(x∗))\n+ [Diag(zk)(f ′(xk−1)− yk−1)]>A[Diag(zk)(f ′(xk−1)− yk−1)]\n+ 2\nn\n( 1− 1\nn\n) (yk−1 − f ′(x∗))>A(f ′(xk−1)− f ′(x∗)) .\nThe only random term (given Fk−1) is the third one whose expectation is equal to E [ [Diag(zk)(f ′(xk−1)− yk−1)]>A[Diag(zk)(f ′(xk−1)− yk−1)]|Fk−1 ] = 1\nn (f ′(xk−1)− yk−1)>\n[ Diag(diag(A))− 1\nn A\n] (f ′(xk−1)− yk−1) .\nThe second term (within the expectation) on the right-hand side of Eq. (8) is equal to (yk − f ′(x∗))>b(xk − x∗) = (\n1− 1 n\n) (yk−1 − f ′(x∗))>b(xk−1 − x∗)\n+ 1\nn (f ′(xk−1)− f ′(x∗))>b(xk−1 − x∗)\n− α n\n( 1− 1\nn\n)2 (yk−1 − f ′(x∗))>be>(yk−1 − f ′(x∗))\n− α n 1 n\n( 1− 1\nn\n) (f ′(xk−1)− f ′(x∗))>be>(yk−1 − f ′(x∗))\n− α n 1 n\n( 1− 1\nn\n) (yk−1 − f ′(x∗))>be>(f ′(xk−1)− f ′(x∗))\n− α n 1 n2 (f ′(xk−1)− f ′(x∗))>be>(f ′(xk−1)− f ′(x∗)) − α n [Diag(zk)(f ′(xk−1)− yk−1)]>b(zk)> [ (f ′(xk−1)− yk−1) ] The only random term (given Fk−1) is the last one whose expectation is equal to\nE [ [Diag(zk)(f ′(xk−1)− yk−1)]>b(zk)> [ (f ′(xk−1)− yk−1) ] |Fk−1 ] = 1\nn (f ′(xk−1)− yk−1)>\n( Diag(diag(be>)− 1 n be> ) (f ′(xk−1)− yk−1) .\nThe last term on the right-hand side of Eq. (8) is equal to\n(xk − x∗)>c(xk − x∗) = (xk−1 − x∗)>c(xk−1 − x∗)\n+ α2\nn2\n( 1− 1\nn\n)2 (yk−1 − f ′(x∗))>ece>(yk−1 − f ′(x∗))\n+ α2 n2 1 n2 (f ′(xk−1)− f ′(x∗))>ece>(f ′(xk−1)− f ′(x∗)) − 2α n ( 1− 1 n ) (xk−1 − x∗)>ce>(yk−1 − f ′(x∗))\n− 2α n 1 n (xk−1 − x∗)>ce>(f ′(xk−1)− f ′(x∗)) + 2α2\nn2 1 n\n( 1− 1\nn\n) (yk−1 − f ′(x∗))>ece>(f ′(xk−1)− f ′(x∗))\n+ α2 n2 [ (zk)>(f ′(xk−1)− yk−1) ]> c [ (zk)>(f ′(xk−1)− yk−1) ] .\nThe only random term (given Fk−1) is the last one whose expectation is equal to\nE [[ (zk)>(f ′(xk−1)− yk−1) ]> c [ (zk)>(f ′(xk−1)− yk−1) ] |Fk−1 ] = 1\nn (f ′(xk−1)− yk−1)>\n[ Diag(diag(ece>))− 1\nn ece>\n] (f ′(xk−1)− yk−1) .\nSumming all these terms together, we get the following result: E [ (θk − θ∗)> (\nA b b> c\n) (θk − θ∗) ∣∣∣∣Fk−1] = ( 1− 1\nn\n)2 (yk−1 − f ′(x∗))>S(yk−1 − f ′(x∗))\n+ 1\nn2 (f ′(xk−1)− f ′(x∗))>S(f ′(xk−1)− f ′(x∗))\n+ 1\nn (f ′(xk−1)− yk−1)>\n[ Diag(diag(S))− 1\nn S\n] (f ′(xk−1)− yk−1)\n+ 2\nn\n( 1− 1\nn\n) (yk−1 − f ′(x∗))>S(f ′(xk−1)− f ′(x∗))\n+ 2 ( 1− 1\nn\n) (yk−1 − f ′(x∗))> [ b− α n ec ] (xk−1 − x∗)\n+ 2\nn (f ′(xk−1)− f ′(x∗))>\n[ b− α n ec ] (xk−1 − x∗)\n+ (xk−1 − x∗)>c(xk−1 − x∗)\nwith S = A− αn be > − αneb > + α 2 n2 ece > = A− bc−1b> + (b− αnec)c −1(b− αnec) >. Rewriting f ′(xk−1)− yk−1 = (f ′(xk−1)− f ′(x∗))− (yk−1 − f ′(x∗)), we have\nf ′(xk−1)− yk−1)> [ Diag(diag(S))− 1\nn S\n] (f ′(xk−1)− yk−1)\n= (f ′(xk−1)− f ′(x∗))> [ Diag(diag(S))− 1\nn S\n] (f ′(xk−1)− f ′(x∗))\n+ (yk−1 − f ′(x∗))> [ Diag(diag(S))− 1\nn S\n] (yk−1 − f ′(x∗))\n− 2(yk−1 − f ′(x∗))> [ Diag(diag(S))− 1\nn S\n] (f ′(xk−1)− f ′(x∗)).\nHence, the sum may be rewritten as E [ (θk − θ∗)> (\nA b b> c\n) (θk − θ∗) ∣∣∣∣Fk−1] = (yk−1 − f ′(x∗))> [( 1− 2\nn\n) S + 1\nn Diag(diag(S))\n] (yk−1 − f ′(x∗))\n+ 1\nn (f ′(xk−1)− f ′(x∗))>Diag(diag(S))(f ′(xk−1)− f ′(x∗))\n+ 2\nn (yk−1 − f ′(x∗))> [S −Diag(diag(S))] (f ′(xk−1)− f ′(x∗))\n+ 2 ( 1− 1\nn\n) (yk−1 − f ′(x∗))> [ b− α n ec ] (xk−1 − x∗)\n+ 2\nn (f ′(xk−1)− f ′(x∗))>\n[ b− α n ec ] (xk−1 − x∗)\n+ (xk−1 − x∗)>c(xk−1 − x∗)\nThis concludes the proof.\nA.5 Analysis for α = 1 2nL\nWe now prove Proposition 1, providing a bound for the convergence rate of the SAG algorithm in the case of a small step size, α = 12nL ."
    }, {
      "heading" : "Proof",
      "text" : "Step 1 - Linear convergence of the Lyapunov function\nIn this case, our Lyapunov function is quadratic, i.e., Q(θk) = (θk − θ∗)> (\nA b b> c\n) (θk − θ∗) .\nWe consider\nA = 3nα2I + α2\nn (\n1 n − 2)ee>\nb = −α(1− 1 n )e c = I\nS = 3nα2I\nb− α n ec = −αe .\nThe goal will be to prove that E[Q(θk)|Fk−1] − (1 − δ)Q(θk−1) is negative for some δ > 0. This will be achieved by bounding all the terms by a term depending on g′(xk−1)>(xk−1 − x∗) whose positivity is guaranteed by the convexity of g.\nWe have, with our definition of A, b and c:\nS −Diag(diag(S)) = 3nα2I − 3nα2I = 0 e>(f ′(xk−1)− f ′(x∗)) = n[g′(xk−1)− g′(x∗)] = ng′(xk−1) .\nThis leads to (using the lemma of the previous section): E[Q(θk)|Fk−1] = E [ (θk − θ∗)> ( A b b> c ) (θk − θ∗) ∣∣∣∣Fk−1] = ( 1− 1\nn\n) 3nα2(yk−1 − f ′(x∗))>(yk−1 − f ′(x∗))\n+ (xk−1 − x∗)>(xk−1 − x∗)− 2α n (xk−1 − x∗)>e>(f ′(xk−1)− f ′(x∗)) + 3α2(f ′(xk−1)− f ′(x∗))>(f ′(xk−1)− f ′(x∗))\n− 2α (\n1− 1 n\n) (yk−1 − f ′(x∗))>e(xk−1 − x∗)\n= ( 1− 1\nn\n) 3nα2(yk−1 − f ′(x∗))>(yk−1 − f ′(x∗))\n+ (xk−1 − x∗)>(xk−1 − x∗)− 2α(xk−1 − x∗)>g′(xk−1) + 3α2(f ′(xk−1)− f ′(x∗))>(f ′(xk−1)− f ′(x∗))\n− 2α (\n1− 1 n\n) (yk−1 − f ′(x∗))>e(xk−1 − x∗)\n6 ( 1− 1\nn\n) 3nα2(yk−1 − f ′(x∗))>(yk−1 − f ′(x∗))\n+ (xk−1 − x∗)>(xk−1 − x∗)− 2α(xk−1 − x∗)>g′(xk−1) + 3α2nL(xk−1 − x∗)>g′(xk−1)\n− 2α (\n1− 1 n\n) (yk−1 − f ′(x∗))>e(xk−1 − x∗) .\nThe third line is obtained using the Lipschitz property of the gradient, that is\n(f ′(xk−1)− f ′(x∗))>(f ′(xk−1)− f ′(x∗)) = n∑ i=1 ‖f ′i(xk−1)− f ′i(x∗)‖2\n6 n∑ i=1 L(f ′i(x k−1)− f ′i(x∗))>(xk−1 − x∗) = nL(g′(xk−1)− g′(x∗))>(xk−1 − x∗) ,\nwhere the inequality in the second line stems from [Nesterov, 2004, Theorem 2.1.5].\nWe have (1− δ)Q(θk−1) = (1− δ)(θk−1 − θ∗)> (\nA b b> c\n) (θk−1 − θ∗)\n= (1− δ)(yk−1 − f ′(x∗))> [ 3nα2I + α2\nn\n( 1 n − 2 ) ee> ] (yk−1 − f ′(x∗))\n+ (1− δ)(xk−1 − x∗)>(xk−1 − x∗) − 2α(1− δ) (\n1− 1 n\n) (yk−1 − f ′(x∗))>e(xk−1 − x∗) .\nThe difference is then:\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1) 6 (yk−1 − f ′(x∗))> [ 3nα2 ( δ − 1\nn\n) I + (1− δ)α 2\nn\n( 2− 1\nn\n) ee> ] (yk−1 − f ′(x∗))\n+ δ(xk−1 − x∗)>(xk−1 − x∗) − (2α− 3α2nL)(xk−1 − x∗)>g′(xk−1)\n− 2αδ (\n1− 1 n\n) (yk−1 − f ′(x∗))>e(xk−1 − x∗).\nNote that for any symmetric negative definite matrix M and for any vectors s and t we have\n(s+ 1\n2 M−1t)>M(s+\n1 2 M−1t) 6 0,\nand thus that\ns>Ms+ s>t 6 −1 4 t>M−1t .\nUsing this fact with\nM = [ 3nα2 ( δ − 1\nn\n) I + (1− δ)α 2\nn\n( 2− 1\nn\n) ee> ] = [ 3nα2 ( δ − 1\nn\n)( I − ee >\nn\n) + α2 ( 3nδ − 1− 2δ + δ − 1\nn\n) ee>\nn ] s = yk−1 − f ′(x∗)\nt = −2αδ (\n1− 1 n\n) e(xk−1 − x∗) ,\nwe have (yk−1 − f ′(x∗))> [ 3nα2 ( δ − 1\nn\n) I + (1− δ)α 2\nn\n( 2− 1\nn\n) ee> ] (yk−1 − f ′(x∗))\n− 2αδ (\n1− 1 n\n) (yk−1 − f ′(x∗))>e(xk−1 − x∗)\n6 −α2δ2 (\n1− 1 n\n)2 (xk−1 − x∗)>e> [ 3nα2 ( δ − 1\nn\n)( I − ee >\nn ) +α2 ( 3nδ − 1− 2δ + δ − 1\nn\n) ee>\nn\n]−1 e(xk−1 − x∗)\n= − α2δ2\n( 1− 1n )2 n\nα2 [ 3nδ − 1− 2δ + δ−1n ]‖xk−1 − x∗‖2 = − δ2 ( 1− 1n )2 n\n3nδ − 1− 2δ + δ−1n ‖xk−1 − x∗‖2 .\nA sufficient condition for M to be negative definite is to have δ 6 13n .\nThe bound then becomes\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1) 6 −(2α− 3α2nL)(xk−1 − x∗)>g′(xk−1)\n+ ( δ −\nδ2 ( 1− 1n )2[ 3nδ − 1− 2δ + δ−1n ]n) ‖xk−1 − x∗‖2 .\nWe now use the strong convexity of g to get the inequality\n‖xk−1 − x∗‖2 6 1 µ (xk−1 − x∗)>g′(xk−1) .\nThis yields the final bound\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1) 6 − ( 2α− 3α2nL+\nδ2 ( 1− 1n )2[ 3nδ − 1− 2δ + δ−1n ] n µ − δ µ ) (xk−1 − x∗)>g′(xk−1).\nSince we know that (xk−1 − x∗)>g′(xk−1) is positive, due to the convexity of g, we need to prove\nthat ( 2α− 3α2nL+\nδ2 ( 1− 1n )2[ 3nδ − 1− 2δ + δ−1n ] n µ − δ µ ) is positive.\nUsing δ = µ8nL and α = 1 2nL gives\n2α− 3α2nL+ δ2 ( 1− 1n )2[ 3nδ − 1− 2δ + δ−1n ] n µ − δ µ = 1 nL − 3 4nL − 1 8nL −\nδ2 ( 1− 1n )2 n µ\n1− 3nδ + 2δ + 1−δn\n> 1\n8nL − δ2 nµ 1− 3nδ\n= 1\n8nL −\nµ 64nL2\n1− 3µ8L\n> 1\n8nL −\nµ 64nL2 1− 38\n= 1 8nL − µ 40nL2 = 1\n8nL − 1\n40nL > 0 .\nHence, E[Q(θk)|Fk−1]− (1− δ)Q(θk−1) 6 0 .\nWe can then take a full expectation on both sides to obtain:\nEQ(θk)− (1− δ)EQ(θk−1) 6 0 .\nSince Q is a non-negative function (we show below that it dominates a non-negative function), this results proves the linear convergence of the sequence EQ(θk) with rate 1− δ. We have\nEQ(θk) 6 (\n1− µ 8nL\n)k Q(θ0) .\nStep 2 - Domination of ‖xk − x∗‖2 by Q(θk) We now need to prove that Q(θk) dominates ‖xk−x∗‖2. If P − (\n0 0 0 13I\n) is positive definite, then\nQ(θk) > 13‖x k − x∗‖2.\nWe shall use the Schur complement condition for positive definiteness. Since A is positive definite, the other condition to verify is 23I − b >A−1b 0.\n2 3 I − α2\n( 1− 1\nn\n)2 e> [( 3nα2 + α2\nn − 2α2\n) ee>\nn\n]−1 e = 2\n3 I −\nn ( 1− 1n )2 3n+ 1n − 2 ee> n\n2 3 I − n 3n− 2 ee> n\n0 for n > 2 ,\nand so P dominates ( 0 0 0 13I ) .\nThis yields\nE‖xk − x∗‖2 6 3EQ(θk) 6 3 (\n1− µ 8nL\n)k Q(θ0) .\nWe have Q(θ0) = 3nα2 ∑ i ‖y0i − f ′i(x∗)‖2 + (1− 2n)α n2 ∥∥∥∥∥∑ i y0i ∥∥∥∥∥ 2 − 2α ( 1− 1 n ) (x0 − x∗)> (∑ i y0i ) + ‖x0 − x∗‖2\n= 3\n4nL2 ∑ i ‖y0i − f ′i(x∗)‖2 + (1− 2n) 2n3L ∥∥∥∥∥∑ i y0i ∥∥∥∥∥ 2 − n− 1 n2L (x0 − x∗)> (∑ i y0i ) + ‖x0 − x∗‖2 .\nInitializing all the y0i to 0, we get\nQ(θ0) = 3σ2\n4L2 + ‖x0 − x∗‖2 ,\nand\nE‖xk − x∗‖2 6 (\n1− µ 8nL )k ( 9σ2 4L2 + 3‖x0 − x∗‖2 ) .\nA.6 Analysis for α = 1 2nµ\nStep 1 - Linear convergence of the Lyapunov function\nWe now prove Proposition 2, providing a bound for the convergence rate of the SAG algorithm in the case of a small step size, α = 12nµ .\nWe shall use the following Lyapunov function:\nQ(θk) = 2g ( xk + α\nn e>yk\n) − 2g(x∗) + (θk − θ∗)> ( A b b> c ) (θk − θ∗) ,\nwith\nA = ηα\nn I +\nα n (1− 2ν)ee>\nb = −νe c = 0 .\nThis yields\nS = ηα\nn I +\nα n ee>\nDiag(diag(S)) = (1 + η)α\nn I\nS −Diag(diag(S)) = α n (ee> − I)( 1− 2\nn\n) S + 1\nn Diag(diag(S)) =\n( 1− 2\nn )[ηα n I + α n ee> ] + 1 n (1 + η)α n I = ( 1− 2 n ) α n ee> + ( η − η − 1 n ) α n I .\nWe have\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1) = 2g(xk−1)− 2g(x∗)− 2(1− δ)g ( xk−1 + α\nn e>yk−1\n) + 2(1− δ)g(x∗)\n+ (yk−1 − f ′(x∗))> [(\n1− 2 n\n) α\nn ee> +\n( η − η − 1\nn\n) α\nn I − (1− δ)ηα n I\n−(1− δ)α n\n(1− 2ν)ee> ] (yk−1 − f ′(x∗))\n− 2ν n (xk−1 − x∗)>e>(f ′(xk−1)− f ′(x∗)) + (1 + η)α\nn2 (f ′(xk−1)− f ′(x∗))>(f ′(xk−1)− f ′(x∗))\n+ 2α\nn2 (yk−1 − f ′(x∗))>\n[ ee> − I ] (f ′(xk−1)− f ′(x∗))\n+ 2\n( 1 n − δ ) ν(yk−1 − f ′(x∗))>e(xk−1 − x∗).\nOur goal will now be to express all the quantities in terms of (xk−1−x∗)>g′(xk−1) whose positivity is guaranteed by the convexity of g.\nUsing the convexity of g, we have −2(1− δ)g ( xk−1 + α\nn e>yk−1\n) 6 −2(1− δ) [ g(xk−1) + α\nn g′(xk−1)e>yk−1\n] .\nUsing the Lipschitz property of the gradients of fi, we have\n(f ′(xk−1)− f ′(x∗))>(f ′(xk−1)− f ′(x∗)) = n∑ i=1 ‖f ′i(xk−1)− f ′i(x∗)‖2\n6 n∑ i=1 L(f ′i(x k−1)− f ′i(x∗))>(xk−1 − x∗) = nL(g′(xk−1)− g′(x∗))>(xk−1 − x∗) .\nUsing e>[f ′(xk−1)− f ′(x∗)] = ng′(xk−1), we have\n−2ν n (xk−1 − x∗)>e>(f ′(xk−1)− f ′(x∗)) = −2ν(xk−1 − x∗)>g′(xk−1)\n2α n2 (yk−1 − f ′(x∗))>ee>(f ′(xk−1)− f ′(x∗)) = 2α n (yk−1 − f ′(x∗))>eg′(xk−1) .\nReassembling all the terms together, we get\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1)\n6 2δ[g(xk−1)− g(x∗)] + 2δα n g′(xk−1)e>yk−1\n+ (yk−1 − f ′(x∗))> [(\n1− 2 n\n) α\nn ee> +\n( η − η − 1\nn\n) α\nn I − (1− δ)ηα n I−\n(1− δ)α n\n(1− 2ν)ee> ] (yk−1 − f ′(x∗))\n− (\n2ν − (1 + η)αL n\n) (xk−1 − x∗)>g′(xk−1)\n− 2α n2\n(yk−1 − f ′(x∗))> ( f ′(xk−1)− f ′(x∗))\n+ 2\n( 1 n − δ ) ν(yk−1 − f ′(x∗))>e(xk−1 − x∗).\nUsing the convexity of g gives\n2δ[g(xk−1)− g(x∗)] 6 2δ[xk−1 − x∗]>g′(xk−1) ,\nand, consequently,\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1)\n6 2δ[(xk−1)− (x∗)]>g′(xk−1) + 2δα n g′(xk−1)e>yk−1\n+ (yk−1 − f ′(x∗))> [(\n1− 2 n\n) α\nn ee> +\n( η − η − 1\nn\n) α\nn I\n−(1− δ)ηα n I − (1− δ)α n (1− 2ν)ee>\n] (yk−1 − f ′(x∗))\n− (\n2ν − (1 + η)αL n\n) (xk−1 − x∗)>g′(xk−1)\n− 2α n2\n(yk−1 − f ′(x∗))> ( f ′(xk−1)− f ′(x∗))\n+ 2\n( 1 n − δ ) ν(yk−1 − f ′(x∗))>e(xk−1 − x∗) .\nIf we regroup all the terms in [(xk−1)−(x∗)]>g′(xk−1) together, and all the terms in (yk−1−f ′(x∗))> together, we get\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1)\n6 α\nn (yk−1 − f ′(x∗))>\n[( δη − η − 1\nn\n) I + ( δ − 2\nn + 2ν(1− δ)\n) ee> ] (yk−1 − f ′(x∗))\n− (\n2ν − 2δ − (1 + η)αL n\n) (xk−1 − x∗)>g′(xk−1)\n+ 2(yk−1 − f ′(x∗))> [ − α n2 (f ′(xk−1)− f ′(x∗)) + ( 1 n − δ)νe(xk−1 − x∗) + δα n eg′(xk−1) ] .\nLet us rewrite this as\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1) 6 (yk−1 − f ′(x∗))> ( τy,II + τy,e ee>\nn\n) (yk−1 − f ′(x∗))\n+ τx,g(x k−1 − x∗)>g′(xk−1) + (yk−1 − f ′(x∗))> [ τy,f (f ′(xk−1)− f ′(x∗)) + τy,xe(xk−1 − x∗) + τy,geg′(xk−1) ]\nwith\nτy,I = α\nn\n( δη − η − 1\nn ) τy,e = α ( δ − 2\nn + 2ν(1− δ) ) τx,g = −(2ν − 2δ − (1 + η)αL\nn )\nτy,f = − 2α\nn2\nτy,x = 2\n( 1 n − δ ) ν\nτy,g = 2δα\nn .\nAssuming that τy,I and τy,e are negative, we have by completing the square that (yk−1 − f ′(x∗))> ( τy,II + τy,e ee>\nn\n) (yk−1 − f ′(x∗))\n+ (yk−1 − f ′(x∗))> ( τy,f (f ′(xk−1)− f ′(x∗)) + τy,xe(xk−1 − x∗) + τy,geg′(xk−1) )\n6 −1 4\n( τy,f (f ′(xk−1)− f ′(x∗)) + τy,xe(xk−1 − x∗) + τy,geg′(xk−1) )>( 1\nτy,I\n( I − ee >\nn\n) +\n1\nτy,I + τy,e\nee>\nn ) ( τy,f (f ′(xk−1)− f ′(x∗)) + τy,xe(xk−1 − x∗) + τy,geg′(xk−1) )\n= −1 4 τ2y,f τy,I ‖f ′(xk−1)− f ′(x∗)‖2 − 1 4 τ2y,fn‖g′(xk−1)‖2\n( 1\nτy,I + τy,e − 1 τy,I ) − 1\n4\nτ2y,xn\nτy,I + τy,e ‖xk−1 − x∗‖2 − 1 4\nτ2y,gn\nτy,I + τy,e ‖g′(xk−1)‖2\n− 1 2 τy,fτy,xn τy,I + τy,e (xk−1 − x∗)>g′(xk−1)− 1 2 τy,fτy,gn τy,I + τy,e ‖g′(xk−1)‖2 − 1 2 τy,gτy,xn τy,I + τy,e (xk−1 − x∗)>g′(xk−1) ,\nwhere we used the fact that (f ′(xk−1) − f ′(x∗))>e = g′(xk−1). After reorganization of the terms, we obtain\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1) 6 [ τx,g −\nnτy,x 2(τy,I + τy,e) (τy,f + τy,g)\n] (xk−1 − x∗)>g′(xk−1)\n−\n[ 1\n4 τ2y,fn\n( 1\nτy,I + τy,e − 1 τy,I\n) + 1\n4\nτ2y,gn\nτy,I + τy,e +\n1\n2\nτy,fτy,gn\nτy,I + τy,e\n] ‖g′(xk−1)‖2\n− 1 4 τ2y,f τy,I ‖f ′(xk−1)− f ′(x∗)‖2 − 1 4 τ2y,xn τy,I + τy,e ‖xk−1 − x∗‖2 .\nWe now use the strong convexity of the function to get the following inequalities:\n‖f ′(xk−1)− f ′(x∗)‖2 6 Ln(xk−1 − x∗)>g′(xk−1)\n‖xk−1 − x∗‖2 6 1 µ (xk−1 − x∗)>g′(xk−1) .\nFinally, we have\nE[Q(θk)|Fk−1]− (1− δ)Q(θk−1)\n6 [ τx,g −\nnτy,x 2(τy,I + τy,e) (τy,f + τy,g)− Ln 4 τ2y,f τy,I − 1 4µ τ2y,xn τy,I + τy,e\n] (xk−1 − x∗)>g′(xk−1)\n−\n[ 1\n4 τ2y,fn\n( 1\nτy,I + τy,e − 1 τy,I\n) + 1\n4\nτ2y,gn\nτy,I + τy,e +\n1\n2\nτy,fτy,gn\nτy,I + τy,e\n] ‖g′(xk−1)‖2 .\nIf we choose δ = δ̃n with δ̃ 6 1 2 , ν = 1 2n , η = 2 and α = 1 2nµ , we get\nτy,I = 1\n2n2µ\n( 2δ̃\nn − 1 n\n) = −1− 2δ̃\n2n3µ 6 0\nτy,e = 1\n2nµ\n( δ̃\nn − 2 n + 1 n\n( 1− δ̃\nn\n)) = − 1\n2n2µ\n( 1− δ̃ + δ̃\nn\n) 6 0\nτx,g = −\n( 1\nn − 2δ̃ n − 3L 2n2µ\n) = 3L\n2n2µ − 1− 2δ̃ n\nτy,f = − 1\nn3µ\nτy,x = 1− δ̃ n2\nτy,g = δ̃\nn3µ .\nThus,\nτx,g − nτy,x\n2(τy,I + τy,e) (τy,f + τy,g)−\nLn\n4 τ2y,f τy,I − 1 4µ τ2y,xn τy,I + τy,e\n6 3L 2n2µ − 1− 2δ̃ n −\n1−δ̃ 2n 2δ̃−1 n3µ\nτy,I + τy,e + Ln 4\n1 n6µ2 1−2δ̃ 2n3µ − 1 4µ (1−δ̃)2 n3 τy,I + τy,e\n= L\nn2µ\n[ 3\n2 +\n1\n2(1− 2δ̃)\n] − 1− 2δ̃\nn − 1 µn3(τy,I + τy,e)\n[ (1− δ̃)2\n4 + (1− δ̃)(2δ̃ − 1) 2n\n]\n6 L\nn2µ 2− 3δ̃ 1− 2δ̃ − 1− 2δ̃ n + 1 µn3 (\n1−2δ̃ 2n3µ + 1 2n2µ ( 1− δ̃ + δ̃n )) (1− δ̃)2 4\n= L\nn2µ 2− 3δ̃ 1− 2δ̃ − 1− 2δ̃ n + (1− δ̃)2 2− 4δ̃ + 2n− 2nδ̃ + 2δ̃\n= L\nn2µ 2− 3δ̃ 1− 2δ̃ − 1− 2δ̃ n + 1− δ̃ 2(1 + n)\n6 L\nn2µ 1− 3δ̃ 1− 2δ̃ − 1− 2δ̃ n + 1− δ̃ 2n\n= L\nn2µ 2− 3δ̃ 1− 2δ̃ − 1− 3δ̃ 2n .\nThis quantity is negative for δ̃ 6 13 and µ L > 4−6δ̃ n(1−2δ̃)(1−3δ̃) . If we choose δ̃ = 18 , then it is sufficient to have nµL > 8. To finish the proof, we need to prove the positivity of the factor of ‖g′(xk−1)‖2.\n1 4 τ2y,fn\n( 1\nτy,I + τy,e − 1 τy,I\n) + 1\n4\nτ2y,gn\nτy,I + τy,e +\n1\n2\nτy,fτy,gn τy,I + τy,e = n 4\n1\nτy,I + τy,e (τy,f + τy,g) 2 − n 4 τ2y,f τy,I\n> n\n4\n(τy,f + τy,g) 2\nτy,I − n 4 τ2y,f τy,I\n= n\n4τy,I τy,g(2τy,f + τy,g)\n> 0 .\nThen, following the same argument as in the previous section, we have\nEQ(θk) 6 (\n1− 1 8n\n)k Q(θ0)\n= ( 1− 1\n8n\n)k [ 2(g(x0)− g(x∗)) + σ 2\nnµ\n] ,\nwith σ2 = 1n ∑ i ‖f ′i(x∗)‖2 the variance of the gradients at the optimum.\nStep 2 - Domination of g(xk)− g(x∗) by Q(θk)\nWe now need to prove that Q(θk) dominates g(xk)− g(x∗).\nQ(θk) = 2g ( xk + α\nn e>yk\n) − 2g(x∗) + (θk − θ∗)> ( A b b> c ) (θk − θ∗)\n= 2g ( xk + α\nn e>yk\n) − 2g(x∗) + 1\nn2µ ∑ i ∥∥yki − f ′i(x∗)∥∥2 + n− 12n3µ ‖e>y‖2 − 1n (xk − x∗)>(e>yk) > 2g(xk) + 2α\nn g′(xk)>(e>yk)− 2g(x∗)\n+ 1\nn2µ ∑ i ∥∥∥∥ 1ne>yk + yki − 1ne>yk − f ′i(x∗) ∥∥∥∥2 + n− 12n3µ ‖e>y‖2 − 1n (xk − x∗)>(e>yk)\nusing the convexity of g and the fact that ∑ i f ′i(x ∗) = 0\n= 2g(xk)− 2g(x∗) + ( 2α\nn g′(xk)− 1 n (xk − x∗)\n)> (e>yk)\n+ 1 n3µ ‖e>yk‖2 + 1 n2µ ∑ i ∥∥∥∥yki − 1ne>yk − f ′i(x∗) ∥∥∥∥2 + n− 12n3µ ‖e>y‖2\n> 2g(xk)− 2g(x∗) + ( 2α\nn g′(xk)− 1 n (xk − x∗)\n)> (e>yk) + n+ 1\n2n3µ ‖e>y‖2\nby dropping some terms.\nThe quantity on the right-hand side is minimized for e>y = n 3µ\nn+1 ( 1 n (x k − x∗)− 2αn g ′(xk) ) . Hence,\nwe have\nQ(θk) > 2g(xk)− 2g(x∗)− n 3µ\n2(n+ 1) ∥∥∥∥ 1n (xk − x∗)− 2αn g′(xk) ∥∥∥∥2\n= 2g(xk)− 2g(x∗)− n 3µ\n2(n+ 1)\n( 1\nn2 ‖xk − x∗‖2 + 4α\n2\nn2 ‖g′(xk)‖2 − 4α n2 (xk − x∗)>g′(xk) ) > 2g(xk)− 2g(x∗)− n 3µ\n2(n+ 1)\n( 1\nn2 ‖xk − x∗‖2 + 4α\n2\nn2 ‖g′(xk)‖2 ) using the convexity of g\n> 2g(xk)− 2g(x∗)− nµ 2(n+ 1)\n( 1 + L2\nµ2n2\n) ‖xk − x∗‖2\nusing the Lipschitz continuity of g′\n> 2g(xk)− 2g(x∗)− nµ 2(n+ 1) 65 64 ‖xk − x∗‖2 since µ L > 8 n\n> 2g(xk)− 2g(x∗)− n (n+ 1) 65 64 (g(xk)− g(x∗))\n> 63\n64 (g(xk)− g(x∗))\n> 6\n7 (g(xk)− g(x∗)) .\nWe thus get\nE [ g(xk)− g(x∗) ] 6 2EQ(θk)\n= ( 1− 1\n8n\n)k [ 7\n3 (g(x0)− g(x∗)) + 7σ\n2\n6nµ\n] .\nStep 3 - Initialization of x0 using stochastic gradient descent\nDuring the first few iterations, we obtain the O(1/k) rate obtained using stochastic gradient descent, but with a constant which is proportional to n. To circumvent this problem, we will first do n iterations of stochastic gradient descent to initialize x0, which will be renamed xn to truly reflect the number of iterations done.\nUsing the bound from section A.3, we have\nEg\n( 1\nn n−1∑ i=0 x̃i\n) − g(x∗) 6 2L\nn ‖x0 − x∗‖2 + 4σ\n2 nµ log ( 1 + µn 4L ) .\nAnd so, using xn = 1n ∑n−1 i=0 x̃ i, we have for k > n\nE [ g(xk)− g(x∗) ] 6 ( 1− 1\n8n\n)k−n [ 14L\n3n ‖x0 − x∗‖2 + 28σ\n2 3nµ log ( 1 + µn 4L ) + 7σ2 6nµ ] .\nSince ( 1− 1\n8n\n)−n 6 8\n7 ,\nwe get\nE [ g(xk)− g(x∗) ] 6 ( 1− 1\n8n\n)k [ 16L\n3n ‖x0 − x∗‖2 + 32σ\n2 3nµ log ( 1 + µn 4L ) + 4σ2 3nµ ] ."
    }, {
      "heading" : "B Comparison of convergence rates",
      "text" : "We consider the `2-regularized least squares problem\nminimize x∈Rp\ng(x) := λ 2 ‖x‖2 + 1 2n n∑ i=1 (aTi x− bi)2,\nwhere to apply SG methods and SAG we can use\nfi(x) := λ 2 ‖x‖2 + 1 2 (aTi x− bi)2.\nIf we use b to denote a vector containing the values bi and A to denote a matrix withs rows ai, we can re-write this problem as\nminimize x∈Rp\nλ 2 ‖x‖2 + 1 2n ‖Ax− b‖2.\nThe Fenchel dual of this problem is\nminimize y∈Rn\nd(y) := n 2 ‖y‖2 + 1 2λ y>AA>y + y>b.\nWe can obtain the primal variables from the dual variables by the formula x = (−1/λ)A>y. Convergence rates of different primal and dual algorithms are often expressed in terms of the following Lipschitz constants:\nLg = λ+Mσ/n (Lipschitz constant of g ′) Lig = λ+Mi (Lipschitz constant for all f ′ i) Ljg = λ+Mj/n (Lipschitz constant of all g ′ j) Ld = n+Mσ/λ (Lipschitz constant of d ′) Lid = n+Mi/λ (Lipschitz constant of all d ′ i)\nHere, we use Mσ to denote the maximum eigenvalue of A >A, Mi to denote the maximum squared row-norm maxi{‖ai‖2}, and Mj to denote the maximum squared column-norm maxj{ ∑n i=1(ai) 2 j}. We use g′j to refer to element of j of g ′, and similarly for d′i. The convergence rates will also depend on the primal and dual strong-convexity constants:\nµg = λ+mσ/n (Strong-convexity constant of g) µd = n+m ′ σ/λ (Strong-convexity constant of d)\nHere, mσ is the minimum eigenvalue of A >A, and m′σ is the minimum eigenvalue of AA >."
    }, {
      "heading" : "B.1 Full Gradient Methods",
      "text" : "Using a similar argument to [Nesterov, 2004, Theorem 2.1.15], if we use the basic FG method with a step size of 1/Lg, then (f(x\nk)− f(x∗)) converges to zero with rate( 1− µg\nLg\n)2 = ( 1− λ+mσ/n\nλ+Mσ/n\n)2 = ( 1− nλ+mσ\nnλ+Mσ\n)2 ≤ exp ( −2nλ+mσ\nnλ+Mσ\n) ,\nwhile a larger step-size of 2/(Lg + µg) gives a faster rate of( 1− µg + µg\nLg + µg\n)2 = ( 1− nλ+mσ\nnλ+ (Mσ +mσ)/2\n)2 ≤ exp ( −2 nλ+mσ\nnλ+ (Mσ +mσ)/2\n) ,\nwhere the speed improvement is determined by the size of mσ.\nIf we use the basic FG method on the dual problem with a step size of 1/Ld, then (d(x k) − d(x∗)) converges to zero with rate( 1− µd\nLd\n)2 = ( 1− n+m ′ σ/λ\nn+Mσ/λ\n)2 = ( 1− nλ+m ′ σ\nnλ+Mσ\n)2 ≤ exp ( −2nλ+m ′ σ\nnλ+Mσ\n) ,\nand with a step-size of 2/(Ld + µd) the rate is( 1− µd + µd\nLd + µd\n)2 = ( 1− nλ+m ′ σ\nnλ+ (Mσ +m′σ)/2\n)2 ≤ exp ( −2 nλ+m ′ σ\nnλ+ (Mσ +m′σ)/2\n) .\nThus, whether we can solve the primal or dual method faster depends on mσ and m ′ σ. In the over-determined case where A has independent columns, a primal method should be preferred. In the under-determined case where A has independent rows, we can solve the dual more efficiently. However, we note that a convergence rate on the dual objective does not necessarily yield the same rate in the primal objective. If A is invertible (so that mσ = m ′ σ) or it has neither independent columns nor independent rows (so that mσ = m ′ σ = 0), then there is no difference between the primal and dual rates.\nThe AFG method achieves a faster rate. Applied to the primal with a step-size of 1/Lg it has a rate of [Nesterov, 2004, Theorem 2.2.2](\n1− √ µg Lg ) = ( 1− √ λ+mσ/n λ+Mσ/n ) = ( 1− √ nλ+mσ nλ+Mσ ) ≤ exp ( − √ nλ+mσ nλ+Mσ ) ,\nand applied to the dual with a step-size of 1/Ld it has a rate of( 1− √ µd Ld ) = ( 1− √ n+m′σλ n+Mσ/λ ) = ( 1− √ nλ+m′σ nλ+Mσ ) ≤ exp ( − √ nλ+m′σ nλ+Mσ ) ."
    }, {
      "heading" : "B.2 Coordinate-Descent Methods",
      "text" : "The cost of applying one iteration of an FG method is O(np). For this same cost we could apply p iterations of a coordinate descent method to the primal, assuming that selecting the coordinate to update has a cost of O(1). If we select coordinates uniformly at random, then the convergence rate for p iterations of coordinate descent with a step-size of 1/Ljg is [Nesterov, 2010, Theorem 2](\n1− µg pLjg\n)p = ( 1− λ+mσ/n\np(λ+Mj/n)\n)p = ( 1− nλ+mσ\np(nλ+Mj)\n)p ≤ exp ( −nλ+mσ nλ+Mj ) .\nHere, we see that applying a coordinate-descent method can be much more efficient than an FG method if Mj << Mσ. This can happen, for example, when the number of variables p is much larger than the number of examples n. Further, it is possible for coordinate descent to be faster than the AFG method if the difference between Mσ and Mj is sufficiently large.\nFor the O(np) cost of one iteration of the FG method, we could alternately perform n iterations of coordinate descent on the dual problem. With a step size of 1/Lid this would obtain a rate on the dual objective of(\n1− µd nLid\n)n = ( 1− n+m ′ σ/λ\nn(n+Mi/λ)\n)n = ( 1− nλ+m ′ σ\nn(nλ+Mi)\n)n ≤ exp ( −nλ+m ′ σ\nnλ+Mi\n) ,\nwhich will be faster than the dual FG method if Mi << Mσ. This can happen, for example, when the number of examples n is much larger than the number of variables p. The difference between the primal and dual coordinate methods depends on Mi compared to Mj and mσ compared to m ′ σ."
    }, {
      "heading" : "B.3 Stochastic Average Gradient",
      "text" : "For the O(np) cost of one iteration of the FG method, we can perform n iterations of SAG. With a step size of 1/2nLg, performing n iterations of the SAG algorithm has a rate of(\n1− µg 8nLig\n)n = ( 1− λ+mσ/n\n8n(λ+Mi)\n)n = ( 1− nλ+mσ\n8n(nλ+ nMi)\n)n ≤ exp ( −1\n8 nλ+mσ nλ+ nMi\n) ,\nThis is most similar to the rate obtained with the dual coordinate descent method, but is likely to be slower because of the n term scaling Mi. However, the difference will be decreased for overdetermined problems when mσ >> m ′ σ.\nUnder the condition n > 8Lig/µg = 8(λ+Mi)/(λ+mσ/n), with a step size of 1/2nµg performing n iterations of the SAG algorithm has a rate of(\n1− 1 8n\n)n = ( 1− nλ\n8n(nλ)\n)n ≤ exp ( −1\n8\n) .\nNote that depending on the constants this may or may not not be faster than coordinate descent methods. However, if we consider the typical case where mσ = m ′ σ = 0 with Mi = O(p) and Mj = O(n), then if we have n = 8(λ+Mi)/λ we obtain( 1− 1\n8n\n)n = ( 1− λ\n64(λ+Mi)\n)n = ( 1− nλ\n64n(λ+Mi)\n)n ≤ exp ( − 1\n64\nnλ\nλ+Mi\n) .\nDespite the constant of 64 (which is likely to be highly sub-optimal), from these rates we see that SAG outperforms coordinate descent methods when n is sufficiently large."
    } ],
    "references" : [ {
      "title" : "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization",
      "author" : [ "A. Agarwal", "P.L. Bartlett", "P. Ravikumar", "M.J. Wainwright" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2012
    }, {
      "title" : "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
      "author" : [ "F. Bach", "E. Moulines" ],
      "venue" : null,
      "citeRegEx" : "Bach and Moulines.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bach and Moulines.",
      "year" : 2011
    }, {
      "title" : "A new class of incremental gradient methods for least squares problems",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Bertsekas.,? \\Q1997\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 1997
    }, {
      "title" : "A convergent incremental gradient method with a constant step size",
      "author" : [ "D. Blatt", "A.O. Hero", "H. Gauchman" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Blatt et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blatt et al\\.",
      "year" : 2007
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "L. Bottou", "O. Bousquet" ],
      "venue" : null,
      "citeRegEx" : "Bottou and Bousquet.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bottou and Bousquet.",
      "year" : 2007
    }, {
      "title" : "Accelerated stochastic approximation",
      "author" : [ "B. Delyon", "A. Juditsky" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Delyon and Juditsky.,? \\Q1993\\E",
      "shortCiteRegEx" : "Delyon and Juditsky.",
      "year" : 1993
    }, {
      "title" : "Optimal learning rates for least squares SVMs using Gaussian kernels",
      "author" : [ "M. Eberts", "I. Steinwart" ],
      "venue" : null,
      "citeRegEx" : "Eberts and Steinwart.,? \\Q2011\\E",
      "shortCiteRegEx" : "Eberts and Steinwart.",
      "year" : 2011
    }, {
      "title" : "Hybrid deterministic-stochastic methods for data fitting",
      "author" : [ "M.P. Friedlander", "M. Schmidt" ],
      "venue" : "SIAM Journal of Scientific Computing,",
      "citeRegEx" : "Friedlander and Schmidt.,? \\Q2012\\E",
      "shortCiteRegEx" : "Friedlander and Schmidt.",
      "year" : 2012
    }, {
      "title" : "Optimal stochastic‘ approximation algorithms for strongly convex stochastic composite optimization",
      "author" : [ "S. Ghadimi", "G. Lan" ],
      "venue" : "Optimization Online,",
      "citeRegEx" : "Ghadimi and Lan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ghadimi and Lan.",
      "year" : 2010
    }, {
      "title" : "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : null,
      "citeRegEx" : "Hazan and Kale.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2011
    }, {
      "title" : "Accelerated stochastic approximation",
      "author" : [ "H. Kesten" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "Kesten.,? \\Q1958\\E",
      "shortCiteRegEx" : "Kesten.",
      "year" : 1958
    }, {
      "title" : "Stochastic approximation and recursive algorithms and applications",
      "author" : [ "H.J. Kushner", "G. Yin" ],
      "venue" : "Springer-Verlag, Second edition,",
      "citeRegEx" : "Kushner and Yin.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kushner and Yin.",
      "year" : 2003
    }, {
      "title" : "Asymptotically optimal regularization in smooth parametric models",
      "author" : [ "P. Liang", "F. Bach", "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Liang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2009
    }, {
      "title" : "Large-scale sparse logistic regression",
      "author" : [ "J. Liu", "J. Chen", "J. Ye" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep learning via Hessian-free optimization",
      "author" : [ "J. Martens" ],
      "venue" : null,
      "citeRegEx" : "Martens.,? \\Q2010\\E",
      "shortCiteRegEx" : "Martens.",
      "year" : 2010
    }, {
      "title" : "Convergence rate of incremental subgradient algorithms. In Stochastic Optimization: Algorithms and Applications, pages 263–304",
      "author" : [ "A. Nedic", "D. Bertsekas" ],
      "venue" : "Kluwer Academic,",
      "citeRegEx" : "Nedic and Bertsekas.,? \\Q2000\\E",
      "shortCiteRegEx" : "Nedic and Bertsekas.",
      "year" : 2000
    }, {
      "title" : "Problem complexity and method efficiency in optimization",
      "author" : [ "A. Nemirovski", "D.B. Yudin" ],
      "venue" : null,
      "citeRegEx" : "Nemirovski and Yudin.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nemirovski and Yudin.",
      "year" : 1983
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "A method for unconstrained convex minimization problem with the rate of convergence O(1/k)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Doklady AN SSSR,",
      "citeRegEx" : "Nesterov.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 1983
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course",
      "author" : [ "Y. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "Nesterov.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2004
    }, {
      "title" : "Primal-dual subgradient methods for convex problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Nesterov.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2009
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "CORE Discussion Paper,",
      "citeRegEx" : "Nesterov.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2010
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "B.T. Polyak", "A.B. Juditsky" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Polyak and Juditsky.,? \\Q1992\\E",
      "shortCiteRegEx" : "Polyak and Juditsky.",
      "year" : 1992
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "Robbins and Monro.,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins and Monro.",
      "year" : 1951
    }, {
      "title" : "Local gain adaptation in stochastic gradient descent",
      "author" : [ "N. Schraudolph" ],
      "venue" : "ICANN,",
      "citeRegEx" : "Schraudolph.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schraudolph.",
      "year" : 1999
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2007
    }, {
      "title" : "Incremental gradient algorithms with stepsizes bounded away from zero",
      "author" : [ "M. Solodov" ],
      "venue" : "Computational Optimization and Applications,",
      "citeRegEx" : "Solodov.,? \\Q1998\\E",
      "shortCiteRegEx" : "Solodov.",
      "year" : 1998
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "K. Sridharan", "S. Shalev-Shwartz", "N. Srebro" ],
      "venue" : null,
      "citeRegEx" : "Sridharan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sridharan et al\\.",
      "year" : 2008
    }, {
      "title" : "Variable metric stochastic approximation theory",
      "author" : [ "P. Sunehag", "J. Trumpf", "S. Vishwanathan", "N. Schraudolph" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Sunehag et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sunehag et al\\.",
      "year" : 2009
    }, {
      "title" : "A scalable modular convex solver for regularized risk",
      "author" : [ "C.H. Teo", "Q. Le", "A.J. Smola", "S.V.N. Vishwanathan" ],
      "venue" : null,
      "citeRegEx" : "Teo et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Teo et al\\.",
      "year" : 2007
    }, {
      "title" : "An incremental gradient(-projection) method with momentum term and adaptive stepsize rule",
      "author" : [ "P. Tseng" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Tseng.,? \\Q1998\\E",
      "shortCiteRegEx" : "Tseng.",
      "year" : 1998
    }, {
      "title" : "Dual averaging methods for regularized stochastic learning and online optimization",
      "author" : [ "L. Xiao" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Xiao.,? \\Q2010\\E",
      "shortCiteRegEx" : "Xiao.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "The most wildly successful class of algorithms for taking advantage of this type of problem structure are stochastic gradient (SG) methods Robbins and Monro [1951], Bottou and LeCun [2003].",
      "startOffset" : 139,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "The most wildly successful class of algorithms for taking advantage of this type of problem structure are stochastic gradient (SG) methods Robbins and Monro [1951], Bottou and LeCun [2003]. Although the theory behind SG methods allows them to be applied more generally, in the context of machine learning SG methods are typically used to solve the problem of optimizing a sample average over a finite training set, i.",
      "startOffset" : 139,
      "endOffset" : 189
    }, {
      "referenceID" : 29,
      "context" : "An extensive list of convex loss functions used in machine learning is given by Teo et al. [2007], and we can even include non-smooth loss functions (or regularizers) by using smooth approximations.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 15,
      "context" : "Under certain assumptions this convergence rate is optimal for strongly-convex optimization in a model of computation where the algorithm only accesses the function through unbiased measurements of its objective and gradient (see Nemirovski and Yudin [1983], Nemirovski et al.",
      "startOffset" : 230,
      "endOffset" : 258
    }, {
      "referenceID" : 15,
      "context" : "Under certain assumptions this convergence rate is optimal for strongly-convex optimization in a model of computation where the algorithm only accesses the function through unbiased measurements of its objective and gradient (see Nemirovski and Yudin [1983], Nemirovski et al. [2009], Agarwal et al.",
      "startOffset" : 230,
      "endOffset" : 284
    }, {
      "referenceID" : 0,
      "context" : "[2009], Agarwal et al. [2012]).",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "The primay contribution of this work is the analysis of a new algorithm that we call the stochastic average gradient (SAG) method, a randomized variant of the incremental aggregated gradient (IAG) method Blatt et al. [2007], which combines the low iteration cost of SG methods with a linear convergence rate as in FG methods.",
      "startOffset" : 204,
      "endOffset" : 224
    }, {
      "referenceID" : 30,
      "context" : "x = x − αkf ′ ik(x ) + βk(x k − xk−1), see Tseng [1998]. It is common to set all βk = β for some constant β, and in this case we can rewrite the SG with momentum method as x = x − ∑k j=1 αjβ k−jf ′ ij (x ).",
      "startOffset" : 43,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "This approach is used in the dual averaging method Nesterov [2009], and while this averaging procedure leads to convergence for a constant step size and can improve the constants in the convergence rate Xiao [2010], it does not improve on the O(1/k) rate.",
      "startOffset" : 51,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "This approach is used in the dual averaging method Nesterov [2009], and while this averaging procedure leads to convergence for a constant step size and can improve the constants in the convergence rate Xiao [2010], it does not improve on the O(1/k) rate.",
      "startOffset" : 51,
      "endOffset" : 215
    }, {
      "referenceID" : 20,
      "context" : "With a suitable choice of step-sizes, this gives the same asymptotic efficiency as Newton-like second-order SG methods and also leads to increased robustness of the convergence rate to the exact sequence of step sizes Polyak and Juditsky [1992]. Baher’s method [Kushner and Yin, 2003, §1.",
      "startOffset" : 218,
      "endOffset" : 245
    }, {
      "referenceID" : 9,
      "context" : "The epoch SG method uses averaging to obtain the O(1/k) rate even for non-smooth objectives Hazan and Kale [2011]. However, the convergence rates of these averaging methods remain sublinear.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : "Stochastic versions of FG methods: Various options are available to accelerate the convergence of the FG method for smooth functions, such as the accelerated full gradient (AFG) method Nesterov [1983], as well as classical techniques based on quadratic approximations such as non-linear conjugate gradient, quasi-Newton, and Hessian-free Newton methods.",
      "startOffset" : 185,
      "endOffset" : 201
    }, {
      "referenceID" : 16,
      "context" : "Stochastic versions of FG methods: Various options are available to accelerate the convergence of the FG method for smooth functions, such as the accelerated full gradient (AFG) method Nesterov [1983], as well as classical techniques based on quadratic approximations such as non-linear conjugate gradient, quasi-Newton, and Hessian-free Newton methods. Several authors have analyzed stochastic variants of these algorithms Schraudolph [1999], Sunehag et al.",
      "startOffset" : 185,
      "endOffset" : 443
    }, {
      "referenceID" : 16,
      "context" : "Stochastic versions of FG methods: Various options are available to accelerate the convergence of the FG method for smooth functions, such as the accelerated full gradient (AFG) method Nesterov [1983], as well as classical techniques based on quadratic approximations such as non-linear conjugate gradient, quasi-Newton, and Hessian-free Newton methods. Several authors have analyzed stochastic variants of these algorithms Schraudolph [1999], Sunehag et al. [2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010].",
      "startOffset" : 185,
      "endOffset" : 466
    }, {
      "referenceID" : 8,
      "context" : "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010].",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010].",
      "startOffset" : 8,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al.",
      "startOffset" : 8,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al. [2009]. Alternately, if we split the convergence rate into a deterministic and stochastic part, these methods can improve the dependency on the deterministic part Ghadimi and Lan [2010], Xiao [2010].",
      "startOffset" : 8,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al. [2009]. Alternately, if we split the convergence rate into a deterministic and stochastic part, these methods can improve the dependency on the deterministic part Ghadimi and Lan [2010], Xiao [2010].",
      "startOffset" : 8,
      "endOffset" : 337
    }, {
      "referenceID" : 8,
      "context" : "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al. [2009]. Alternately, if we split the convergence rate into a deterministic and stochastic part, these methods can improve the dependency on the deterministic part Ghadimi and Lan [2010], Xiao [2010]. However, as with all other methods we have discussed thus far in this section, we are not aware of any existing method of this flavor that improves on the O(1/k) rate.",
      "startOffset" : 8,
      "endOffset" : 350
    }, {
      "referenceID" : 2,
      "context" : "Constant step size: If the SG iterations are used with a constant step size (rather than a decreasing sequence), then the convergence rate of the method can be split into two parts [Nedic and Bertsekas, 2000, Proposition 2.4], where the first part depends on k and converges linearly to 0 and the second part is independent of k but does not converge to 0. Thus, with a constant step size the SG iterations have a linear convergence rate up to some tolerance, and in general after this point the iterations do not make further progress. Indeed, convergence of the basic SG method with a constant step size has only been shown under extremely strong assumptions about the relationship between the functions fi Solodov [1998]. This contrasts with the method we present in this work which converges to the optimal solution using a constant step size and does so with a linear rate (without additional assumptions).",
      "startOffset" : 192,
      "endOffset" : 724
    }, {
      "referenceID" : 9,
      "context" : "In particular, accelerated SG methods use a constant step size by default, and only decrease the step size on iterations where the inner-product between successive gradient estimates is negative Kesten [1958], Delyon and Juditsky [1993].",
      "startOffset" : 195,
      "endOffset" : 209
    }, {
      "referenceID" : 5,
      "context" : "In particular, accelerated SG methods use a constant step size by default, and only decrease the step size on iterations where the inner-product between successive gradient estimates is negative Kesten [1958], Delyon and Juditsky [1993]. This leads to convergence of the method and allows it to potentially achieve periods of linear convergence where the step size stays constant.",
      "startOffset" : 210,
      "endOffset" : 237
    }, {
      "referenceID" : 2,
      "context" : "functions Bertsekas [1997]. However, the weighting is numerically unstable and the linear convergence rate treats full passes through the data as iterations.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "functions Bertsekas [1997]. However, the weighting is numerically unstable and the linear convergence rate treats full passes through the data as iterations. A related strategy is to group the fi functions into ‘batches’ of increasing size and perform SG iterations on the batches Friedlander and Schmidt [2012]. In both cases, the iterations that achieve the linear rate have a cost that is not independent of n, as opposed to SAG.",
      "startOffset" : 10,
      "endOffset" : 312
    }, {
      "referenceID" : 3,
      "context" : "Incremental Aggregated Gradient: Finally, Blatt et al. presents the most closely-related algorithm, the IAG method Blatt et al. [2007]. This method is identical to the SAG iteration (5), but uses a cyclic choice of ik rather than sampling the ik values.",
      "startOffset" : 42,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "– AFG: Nesterov’s accelerated full gradient method Nesterov [1983], where iterations of (3) with a fixed step size are interleaved with an extrapolation step, and we use an adaptive line-search based on Liu et al.",
      "startOffset" : 7,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "– AFG: Nesterov’s accelerated full gradient method Nesterov [1983], where iterations of (3) with a fixed step size are interleaved with an extrapolation step, and we use an adaptive line-search based on Liu et al. [2009].",
      "startOffset" : 203,
      "endOffset" : 221
    }, {
      "referenceID" : 31,
      "context" : "– RDA: The regularized dual averaging method Xiao [2010], another recent state-of-the-art SG method.",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "– ESG: The epoch SG method Hazan and Kale [2011], which runs SG with a constant step size and averaging in a series of epochs, and is optimal for non-smooth stochastic strongly-convex optimization.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "– NOSG: The nearly-optimal SG method Ghadimi and Lan [2010], which combines ideas from SG and AFG methods to obtain a nearly-optimal dependency on a variety of problem-dependent constants.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : "AFG: The accelerated full gradient method Nesterov [1983], where iterations of (3) are interleaved with an extrapolation step.",
      "startOffset" : 42,
      "endOffset" : 58
    }, {
      "referenceID" : 25,
      "context" : "peg: The pegasos algorithm of Shalev-Shwartz et al. [2007], but where we multiply the step size by a constant.",
      "startOffset" : 30,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "IAG: The incremental aggregated gradient method of Blatt et al. [2007] described by iteration (5) but with a cyclic choice of ik.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "In low-dimensional settings, the optimal regularization parameter is of the form C/n Liang et al. [2009] where C is a scalar constant, and may thus violate the constraint.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "For example, Sridharan et al. [2008] considers parameters of the form λ = C √ n in the parametric setting, while Eberts and Steinwart [2011] considers λ = C nβ with β < 1 in a non-parametric setting.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "[2008] considers parameters of the form λ = C √ n in the parametric setting, while Eberts and Steinwart [2011] considers λ = C nβ with β < 1 in a non-parametric setting.",
      "startOffset" : 83,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "However, as shown in our experiments, the testing cost of the SAG iterates often reaches its minimum quicker than existing SG methods, and we could expect to improve the constant in the O(1/k) convergence rate, as is the case with online second-order methods Bottou and Bousquet [2007].",
      "startOffset" : 259,
      "endOffset" : 286
    }, {
      "referenceID" : 1,
      "context" : "Denoting δk = E‖x̃ − x∗‖2, we have (following Bach and Moulines [2011]) δk 6 δk−1 − 2γk(1− γkL)E [ g′(x̃k−1)>(x̃k−1 − x∗) ] + 2γ kσ 2 .",
      "startOffset" : 46,
      "endOffset" : 71
    } ],
    "year" : 2013,
    "abstractText" : "We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.",
    "creator" : "LaTeX with hyperref package"
  }
}