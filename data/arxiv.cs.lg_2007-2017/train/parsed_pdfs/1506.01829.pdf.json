{
  "name" : "1506.01829.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semidefinite and Spectral Relaxations for Multi-Label Classification",
    "authors" : [ "Rémi Lajugie" ],
    "emails" : [ "remi.lajugie@ens.fr", "piotr.bojanowski@inria.fr", "sylvain.arlot@ens.fr", "francis.bach@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n01 82\n9v 1\n[ cs\nIn this paper, we address the problem of multi-label classification. We consider linear classifiers and propose to learn a prior over the space of labels to directly leverage the performance of such methods. This prior takes the form of a quadratic function of the labels and permits to encode both attractive and repulsive relations between labels. We cast this problem as a structured prediction one aiming at optimizing either the accuracies of the predictors or the F1-score. This leads to an optimization problem closely related to the max-cut problem, which naturally leads to semidefinite and spectral relaxations. We show on standard datasets how such a general prior can improve the performances of multi-label techniques."
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-label classification aims at predicting a set of labels for each data instance [26, 28]. This setting is ubiquitous in real-world applications and for example can take the form of video or text tagging, where the goal is to assign instances to categories [14]. For video, [27] proposes to consider the problem of labeling scenes, on which several objects appear.\nOne of the main difficulties of this problem lies in the fact that the space of potential labelings Y is exponentially bigger than the set of labels V . Doing an exhaustive search over the space of labelings is thus not possible. Moreover, contrary to the standard binary classification setting, the set V has a specific structure and one has to take it into account, especially when the number of labels is large. Indeed, imagine that we are given one classifier fv for each v ∈ V , we would probably observe that some fv predict labels that are not actually present; for instance, in image tagging, if it is very likely to see a zebra and a lion on the same image, it is rather not probable to see a reindeer with a lion. A prior over labels could have, for instance, penalized the prediction of a reindeer together with the lion. Incorporating structure into the label set can be done a priori by assuming labels are organized in a certain hierarchy [21]; [13] incorporates a prior knowledge when training the classifiers, permitting to learn correlated classifiers. However this prior does not affect the way predictions are done.\nOur goal is to learn such a prior over labels directly from data, at the same time that classifiers are learnt. This idea has already been tackled by [20] who restricted their study to the specific case of\nincorporating positive affinities between labels. We go beyond this approach and propose a model permitting to take into account affinities and incompatibilities between labels.\nRelated work.\nA large part of the recent literature considers a moderately large set of labels V (order of hundreds) and a huge space of labelings Y . In this setting it is possible to learn specific classifiers for each label separately. One way to train such classifiers is the well-known one-versus-rest technique (a.k.a. binary relevance technique [26]).\nWithin this setting, some approaches use the structured prediction framework [25, 23] as we do. This corresponds to considering the task of prediction as being a task over the huge output space Y . [18] has proposed to plug a model within a structured SVM, and considers the prior knowledge between labels as fixed a priori, whereas we aim at learning it. They defined a proper loss and the corresponding loss-augmented decoding. The loss they used is called the “max loss” and is slightly related to the Hamming loss. This approach leads to an efficient loss-augmented decoding, and avoids an exhaustive search over the power set Y . Other approaches [5] considered the direct optimization of the F1-score within a structured SVM. Another part of the recent literature dealing with multi-label classification [2] considers the case where the space of labels V itself is huge. In these papers, the goal is to use the fact that only few labels are present in an instance. This allows to reduce the dimension of the prediction space and performing the labeling over a lower dimensional space. The priors we propose here could be combined with these approaches.\nContributions. Our contribution is four-fold: (1) we propose a model with priors for multi-label classification allowing attractive as well as repulsive weights, (2) we cast the learning of this model into the framework of structured prediction using either Hamming of F1 losses and propose an approach for solving exactly the loss-augmented decoding using the F1 loss, (3) we propose semidefinite and spectral relaxations to efficiently solve the resulting structured prediction problem, (4) we show on real datasets how the learning of such a general prior can improve the multi-label prediction over the models where no prior is learnt or when only attractive weights are allowed."
    }, {
      "heading" : "2 Structured Prediction for Multi-Label Classification",
      "text" : "In this section, we review several ways to perform the multi-label classification task when a prior over the labels is fixed. Decoding consists in assigning potentially several labels to a data point belonging to some feature space. We then discuss how to learn the parameters of the predictive function. For the rest of the paper we denote our feature space by X ⊂ Rd."
    }, {
      "heading" : "2.1 The multi-label classification problem problem",
      "text" : "Let us consider the set of possible labels V of cardinal V . We define the set of labelings, as the set of binary vectors Y = {−1, 1}V . The set Y is the one on which we perform our structured prediction. Let us assume that for each possible label v, we are given a linear classifier parameterized by wv ∈ R\nd. We denote by W ∈ Rd×V , the vertical concatenation of all the vectors wv . In the multi-label setting, the decoding problem is:\nŷ(x;W ) ∈ argmax y∈{−1,1}V D(x, y;W ) := y⊤W⊤x. (1)\nThis is usually referred to as the binary relevance method for multi-label learning [26].\nThe aforementioned approach does not take into account any dependency between the different labels. A way to do so is to penalize the discriminative function by some penalty F depending on the subset of predicted labels. In our case, we propose to consider:\nŷ ∈ argmax y∈{−1,1}V D(x, y;W,F ) := y⊤W⊤x− F (y). (2)\nHowever, not all functions F are admissible, so that (2) remains tractable since |Y| = 2V . A class of penalizations that are well-suited for our problem is the class of submodular functions [1, 20]. When F is submodular, the decoding becomes the maximization of a supermodular function\n(maximization of a modular minus a submodular function). This is known to be tractable (solvable in polynomial time in V ). [20] has proposed to use a graph-cut based penalty. This corresponds to F (y) = y⊤Ay − y⊤b where b ∈ RV and A ∈ RV×V is proportional to the Laplacian matrix of a graph. Intuitively, this corresponds to considering that labels are organized in a graph G with non-negative weights, encoding attractive affinities between the labels; the linear part of the prior b corresponds to a prior over the frequencies of the classes.\nFor general weights, meaning that the matrix A not only encodes affinities but also costs, the decoding task becomes as hard as solving a max-cut problem. In Sec. 5.1 we review common convex relaxations permitting to obtain a good approximate solution in polynomial time. Using a matrix A with arbitrary entries, our decoding model becomes:\nŷ(x;W,A, b) ∈ argmax y∈{−1,1}V D(x, y;W,A, b) = argmax y∈{−1,1}V y⊤W⊤x+ y⊤b− y⊤Ay. (3)"
    }, {
      "heading" : "2.2 Learning the parameters W , b and A",
      "text" : "In the previous section we have assumed that we are given V linear classifiers wv ∈ Rd, a linear prior b ∈ RV and a matrix A ∈ RV ×V . Thus, the discussed decoding problem can be seen as being parameterized by W , b and A.\nSuppose that we are given N examples (xi, yi) ∈ X ×Y, i = 1, . . . , N , and consider a loss function between two labelings ℓ : Y × Y → R+. Ideally, given this loss, we would like to minimize the following regularized empirical loss:\nmin W,A,b\n1\nN\nN∑\ni=1\nℓ(ŷ(xi;W,A), yi) + λΩ(W,A), (4)\nwhere Ω is a convex regularizer (typically a squared ℓ2-norm) over the parameter space. This is a hard combinatorial problem that thus needs to be relaxed. Following [25, 23], we define the structural hinge loss H as:\nH(xi, yi,W,A, b) = max y∈{−1,1}V\n{ℓ(y, yi) +D(xi, y;W,A, b)−D(xi, yi,W,A)} . (5)\nWe estimate parameters W ∗, b∗ and A∗ by solving the following problem:\nmin W,A,b\n1\nN\nN∑\ni=1\nH(xi, yi;W,A, b) + λΩ(W,A, b). (6)"
    }, {
      "heading" : "3 Performance Measures and Losses for Multi-Label Tasks",
      "text" : "In order to set up the aforementioned problem, we need to define a proper loss function ℓ.\nNormalized Hamming loss. The simplest loss is based on accuracy, and is defined as:\na(y, yi) = V + y⊤yi\n2V ∈ [0, 1] . (7)\nThe loss associated to accuracy is the so-called Hamming loss [12, 28]. It is defined as a linear function of the binary label vector y by:\nℓ(y, y′) = ∥∥∥∥ 1\n2 √ V (1− y)− 1 2 √ V (1− yi)\n∥∥∥∥ 2\n2\n(8)\n= 1\n2V\n( V − yi⊤y ) = 1− a(y, yi) ∈ [0, 1] , (9)\nwhere 1 is the V -dimensional vector with ones. This loss corresponds to the symmetric difference between two sets A∆B = (A ∪B) \\ (A ∩B). Note also that, if we consider that not all the errors are equivalent, one can use a weighted Hamming loss instead.\nF1 loss. A common choice in the multi-label learning literature is the Fβ − score loss [26, 20]. This loss is a function of precision and recall and has some important advantages over the Hamming\nloss. In the common situations where each instance has only few labels among all the ones that are possible, the Fβ loss penalizes a lot the solution (−1, . . . ,−1)⊤ while the Hamming does not. Precision and recall with respect to a training labeling yi ∈ Y are defined respectively as:\np(y, yi) = (1 + yi)\n⊤(1 + y)\n(1 + y)⊤(1 + y) , r(y, yi) =\n(1 + yi) ⊤(1 + y) (1 + yi)⊤(1 + yi) .\nThen the general Fβ score is defined as, for every β > 0,\nFβ(y, yi) = (1 + β2) p(y, yi) r(y, yi)\nβ2 p(y, yi) + r(y, yi) ∈ [0, 1] . (10)\nThe most widely used is the F1 score (which turns out to be the harmonic mean of precision and recall), and the associated loss is then ℓ(y, yi) = 1− F1(y, yi). More precisely:\nℓ(y, yi) = V − y⊤yi\n2V + y⊤i 1+ y ⊤1\n∈ [0, 1] . (11)\nPlease note the non linear dependency of this loss in y."
    }, {
      "heading" : "4 Loss-Augmented Decoding",
      "text" : "We propose to derive a structured-SVM-like optimization objective [25]. As mentioned earlier, we want to learn the parameters of our predictive function using annotated data. Following the definition of H , we can write the complete optimization problem (6) as:\nmin W,A,b\n1\nN\nN∑\ni=1\n[ max\ny∈{−1,1}V\n{ ℓ(yi, y) + y ⊤W⊤xi + y ⊤b− y⊤Ay } − y⊤i W⊤xi − y⊤i b+ y⊤i Ayi\n]\n+ λW 2 ‖W‖22 + λA 2 ‖A‖22. (12)\nUsing the Hamming loss. If we use the Hamming loss for ℓ, then ℓ(yi, y) = 12V ( V − y⊤yi ) . Our optimization problem can be re-written as follows:\nmin W,A,b\n1\nN\nN∑\ni=1\n[ max\ny∈{−1,1}V\n{ y⊤ ( W⊤xi + b− 1\n2V yi\n) − y⊤Ay } − y⊤i W⊤xi − y⊤i b+ y⊤i Ayi ]\n+ λW 2 ‖W‖22 + λA 2 ‖A‖22. (13)\nNote that the objective function of the optimization is jointly convex but not smooth.\nUsing the F1 loss. If in turn we decide to use the F1 loss, the proposed problem is harder because of the vector y in the denominator. To cope with this issue, we can split the set Y into (V +1) subsets. We define the set Yk as the set of labelings such that k entries are positive:\n∀k ∈ {0, . . . V }, Yk = { y ∈ {−1, 1}V , y⊤1 = 2k − V } .\nAs is often done when optimizing the F1 score, which is a contingency-table based loss [15], we can divide the initial problem into V + 1 subproblems by replacing y⊤1 by 2k − V as follows:\nmax k∈{0,...,V }\n[ V\nV + yi1+ 2k + max y∈Yk\n{ y⊤ ( yi\nV + yi1+ 2k +W⊤xi + b\n) − y⊤Ay }] . (14)\nThe problems of Eq. (13)–(14) above assume that we are able to solve quadratic optimization problems for y ∈ Y . [20] proposes a greedy approximate algorithm for solving this type of problems in the specific case where off diagonal entries of the prior A are negative.\nIn the following section, we propose relaxations of these problems leading to a tractable lossaugmented decoding with no restriction over the matrix A."
    }, {
      "heading" : "5 Optimization in y",
      "text" : "So far, we have written three problems that we are not able to solve efficiently. The first one was the general decoding of Eq. (3). The other ones were the subproblems of Eq. (13) and Eq. (14). All of these are quadratic boolean optimization problems and are closely linked to the max-cut problem (see, e.g., [3, Sec. 5.1.5]). These can be written in the canonical form as follows:\nmax u∈{−1,1}V\nL(u)=0\nu⊤b− u⊤Au, (15)\nwhere A ∈ RV ×V , b ∈ RV and L is an affine function. Note the presence of the additional constraint L(u) = 0. This additional equation is only needed for the problem mentioned in Eq. (14). For the two other problems, one can simply ignore it. Eq. (15) allows us to tackle three problems in a unified framework. In the next section we discuss two relaxations to this problem. First we describe the standard SDP relaxation. We then present how to cast this optimization problem as a spectral problem."
    }, {
      "heading" : "5.1 Classical semidefinite relaxation for max-cut",
      "text" : "The family of problems presented in Eq. (15) is known as the two-way partitioning problems. They are a generalization of max-cut, with potentially negative entries in A. Also, they contain an extra linear term (see Sec. 5.1.5 of [3]) and potential constraints over the domain.\nThere exists a classical semidefinite relaxation. Following [3, 6], we use a similar relaxation to the one used by [11] to approximate the max-cut problem. We introduce a new variable U = uu⊤ ∈ RV×V . Using this notation we can re-write the term u⊤Au as Tr (AU). Then using a set of constraints that is equivalent to U = uu⊤ the problem (15) can be re-written as:\nmax u∈{−1,1}V\nU∈RV ×V\nu⊤b− Tr(AU) such that\n   Diag(U) = 1, Rank(U) = 1, U uu⊤, L(u) = 0.\n(16)\nFollowing [3], the convex relaxation of this problem is obtained by removing the rank constraint. We define L as the affine function L(u) = u⊤α − β where α ∈ RV and β ∈ R. We use the Schur complement trick (see, e.g., [3]) and define the matrix M as:\nM = ( U u u⊤ 1 ) .\nUsing eV , the vector with all coordinates equal to zero except the last one, our relaxation of (15) can be re-written as:\nmax M∈RV ×V Tr\n[ M ( −A 12b 1 2b 0 )] such that\n  \nDiag(M) = 1, M 0, α⊤MeV = β.\n(17)\nProblem (17) can be solved using any standard convex optimization solver at least for small V (< 100). When V is large, one can use specific techniques relying explicitly on the fact the solution is expected to be low-rank (see, e.g., [16] and references therein).\nRounding scheme\nAt test time, we follow [3] to round the relaxed solution, i.e., get back to some admissible solution of (15). We notice that at the optimum (u, U) of Eq. (17), U u⊤u implies that U − uu⊤ is a covariance matrix. Therefore, we simply sample several v ∼ N ( u, U − uu⊤ ) from a normal distribution, round the solution by taking the signs and choose the best one in terms of the objective function. This procedure leads to good feasible points in our experiments."
    }, {
      "heading" : "5.2 Spectral relaxation",
      "text" : "The generic problem in Eq. (15) can be rewritten by replacing the integrality constraint u ∈ {−1, 1}V with a quadratic equality u⊤u = V . Please note this makes the problem non-convex. Using the same expression for L(u) as in the previous section leads to the following optimization problem:\nmax u∈RV\nu⊤b− u⊤Au such that { u⊤u = V\nu⊤α = β. (18)\nWe deal with the linear constraint by dualizing it, yielding the following problem:\nmin µ∈R\n[ µβ + max\nu∈RV\nu⊤u=V\nu⊤ (b− µα)− u⊤Au ] . (19)\nThis can be solved by performing a binary search over µ.\nThe inner loop problem is classical in optimization, in particular in trust-region methods [9, 22]. It reduces—using the Lagrange multiplier technique—to solving a quadratic eigenvalue problem [24]. Solving the inner loop problem of Eq. (19) (with nonzero b) is equivalent to finding the minimal eigenvalue of the quadratic eigenvalue problem:(\nλ2I − 2λA+A2 − 14V (b − µα)(b − µα)⊤ ) u = 0, (20)\nwhere I denotes the V × V identity matrix. The problem above is solved efficiently by performing the SVD of the matrix S:\nS =\n( A −I\n− 14V (b− µα)(b − µα)⊤ A\n) . (21)\nOnce this has been solved, we get the desired solution by taking u = 12 (A− λI)−1(b− µα), where λ is the smallest non-zero eigenvalue of S.\nNote that when optimizing the Hamming loss, we get rid of the constraint L(u) = 0. In that case we can set µ = 0 and solve the inner loop problem only once."
    }, {
      "heading" : "5.3 Cheaper (but still efficient) solution for the spectral relaxation",
      "text" : "In this section we present an other way to deal with the spectral relaxation, inspired by [10]. The proposed method is more efficient computationnally than the one of the previous section since it does not involve solving the binary search problem over the Lagrange multiplier µ.\nWe start from the problem of Eq. (18). By the change of variables v = ( u 1 ) and B = ( −A b/2 b/2 0 )\nand by introducingD = ( I 0 0 0 ) (I is the V dimensional identity matrix) we can write the problem as:\nmax v∈RV +1\nv⊤Bv such that\n   v⊤Dv = V v⊤ ( α 0\n0 1\n) = ( β\n1\n) .\n(22)\nFollowing [10], let us simply introduce the QR factorization of the matrix ( α 0 0 1 ) = QR, where Q ∈ RV +1×V+1 is an orthogonal matrix and R ∈ RV+1×2. Let us now introduce U = ( U1 U2 ) = QTV . U1 ∈ R2 and U2 ∈ RV−1\nEq. (22) can be rewritten as:\nmax U∈RV +1\nU⊤QTBQU such that\n   U⊤DU = V\nU⊤1 R =\n( β\n1\n) .\n(23)\nNote that the last constraint permit to fix the variable U⊤1 = R −1 ( β 1 ) . With a slight abuse of nota-\ntion, R−1 corresponds to the inverse of the rotation part. Let us define QTBQ =\n( ∆ Γ/2\nΓ⊤/2 C\n) ,\nwith ∆ ∈ R2×2 , Γ ∈ RT×2 and C ∈ R(V −1)×(V−1). Using the previous notations, we get: U⊤Q⊤BQU = UT2 CU2 + U ⊤ 1 ΓU2 + U ⊤ 1 ∆U1.\nLet us also introduce S = V −UT1 U1 Since U1 is not entirely determined this problem is equivalent to:\nmax U2∈RV −1\nU⊤2 CU2 +R T ( β 1 ) ΓU2 such that\n{ U⊤2 DU2 = S . (24)\nNote that we slightly abuse of notations with D being restricted to its last components."
    }, {
      "heading" : "5.4 Links with graph-cuts",
      "text" : "The min-cut problem can be written as an optimization problem through the following equation:\nmin z∈{0,1}V\nV∑\nj=1\nj−1∑\ni=1\nCi,j |zi − zj |+ c⊤z, (25)\nwhere C ∈ RV×V+ and c ∈ RV . By making the change of variables z = y+12 and carrying on some calculations, we get the following equivalent program:\nmin y∈{−1,1}V\n2y⊤c− y⊤Cy. (26)\nTherefore, when the matrix A has negative off-diagonal entries, the problem formulated in Eq. (15) can be solved using min-cut / max-flow. We can use standard min-cut / max-flow toolboxes by providing the matrix C = −A and c = 12b. When optimizing the F1 loss, note that we can use the same dualization for the constraint L(u) = 0. We proceed exactly as with the spectral relaxation except that the inner loop is solved with min-cut / max-flow."
    }, {
      "heading" : "5.5 Solving the F1 loss augmented decoding for negative A",
      "text" : "In this section, we show how we can solve the constrained problem by relating it to the well-studied total variation denoising problem [4, 1]. Note that, contrary to [20], in this section, we deal with the cardinality constraint exactly and we do not use any approximation algorithm in this specific case. We just use total variation minimization algorithm to perform the constrained minimization.\nHere we consider that the constraint of Eq. (15) is simply a cardinality constraint, namely that it is of the form u⊤c = α for a certain α ∈ {1 . . . V } and c is the V dimensional vector composed of ones.\nNow, we dualize this equality constraint by introducing the associated Lagrange multiplier. This yields the following problem:\nmax µ∈R min u∈{−1,1}\nu⊤Au − u⊤b+ µ(α− u⊤c) (27)\nEquivalentally, by considering the variable z ∈ {0, 1}V we get the following problem:\nmax µ∈R µ(α− V ) min z∈{0,1}V 4z⊤Az − z⊤(4Ac+ 2b) + µz⊤c. (28)\nThe problem of Eq.(28) is a separable submodular optimization problem [1]. Thus solving it can be done by considering the associated proximal problem. More precisely, if we introduce the Choquet\nintegral of the cut J(u) (often referred to the “co-area formula” [4] for the specific case of cut functions or Lovasz extension for submodular functions), the generic proximal problem associated to any cut problem is:\nmin u∈RV\n1 2 ‖u− g‖22 + J(u). (29)\nwhere g in our case is exactly 4Ac+ 2b.\nThis problem is the well known total variation denoising problem. There exists several efficient algorithms to deal with it, especially the ones relying on parametric max-flow techniques. Once problem(29) has been solved and that we recovered its (unique if α is positive) solution u∗, we get all the candidates for being a solution of (27) by considering the different 1u≥−µ. Then, we just have to compute the associated objective values and select the optimal one."
    }, {
      "heading" : "6 Optimization in W and A",
      "text" : "We optimize our cost function in Eq. (12) with stochastic subgradient descent. When we relax the inner optimization problem in y ∈ Y we implicitly modify the cost function. Therefore we have to be careful when computing the subgradients.\nIn this section we provide the derivations in one specific case. The details for the other cases can be found in the supplementary material. When using the Hamming loss and the SDP relaxation, our cost function becomes, with U = {(U, u), U ∈ RV ×V , u ∈ RV , U u⊤u,Diag(U) = 1V }:\nmin W,A,b\n1\nN\nN∑\ni=1\n[ max u,U∈U { u⊤ ( W⊤xi + b− 1 2V yi ) − Tr(AU) } − y⊤i W⊤xi − y⊤i b+ y⊤i Ayi ]\n+ λW 2 ‖W‖22 + λA 2 ‖A‖22. (30)\nTo obtain the subgradients, we first solve the relaxed loss-augmented inference. Using the obtained u and U , we compute the subgradients in W and A as follows:\n∂W g(W,A) = λWW + 1 N ∑N i=1 xi(u− yi)⊤, (31)\n∂bg(W,A) = 1 N ∑N i=1(u− yi), (32)\n∂Ag(W,A) = λAA+ 1 N ∑N i=1 −U + yiy⊤i . (33)"
    }, {
      "heading" : "7 Experimental Evaluation",
      "text" : "We now validate the proposed approach on standard benchmarks. We compare our implementation to [20] and to the one-versus-rest model (OvR). The code corresponding to the described method will be made publicly available. In this experimental section we first describe the used datasets and discuss the baselines to which we compare.\nDatasets. We validate our approach on four datasets. Following [20], we picked our datasets from the mulan1 repository. We picked the yeast [7], enron, medical [19] and bibtex [17] datasets. The datasets are of various sizes and natures: yeast only has 14 labels while bibtex has 159. All of them also present different challenges (different structures, label concurrence patterns, etc.).\nThese datasets are given with a train / test split. We further split the training set to generate a validation set. We select all relevant parameters by plain validation on this set. We report all performances on the actual test set as given in the dataset. Caracteristics of these datasets are given in Table 1.\nOne-versus-rest results. In Table 2 we report the performance of a one-versus-rest model for all the datasets. For every label, we train a linear classifier using a standard SVM toolbox [8]. We select the hyper-parameters by validation on a held-out part of the training set. We compare three criteria for choosing the optimal set of regularization parameters. We can either select a common\n1http://mulan.sourceforge.net/datasets.html\nregularization parameter for all classes (“Single λ” column), chosen with the Hamming loss (which decouples over classes), or one per class (“Multiple λ” column). When choosing a common λ for all classes, one can choose it according to the F1 or Hamming loss on the validation set.\nTable 2 shows that it is sometimes important to use the relevant loss as a criterion to select hyperparameter. In our experiments, this becomes more and more important as the size of the label set increases and thus as discussed in Sec. 3 the Hamming loss behaves more and more differently from the F1 loss.\nOne would also expect that picking one parameter per label would lead to better performance. But the benefits from selecting a specific parameter per class is offset by the fact that one cannot use the F1 loss in this case. In all our remaining simulations, we use a single λ for all classes.\nOur model and comparison to [20]. We run our algorithm—with ℓ equal to the Hamming loss—on all four datasets and compare to the available implementation of [20]. For all methods we select all hyper-parameters based on the performance in terms of F1 loss on the validation set. Because of the challenging number of labels for bibtex, we were able to run neither the code from [20], nor the SDP, in reasonable time.\nTable 3 compares the one-versus-rest approach, the approach described in [20] and variants of our method. We compare the two relaxations we proposed while optimizing the Hamming loss. Please recall that the min-cut (MC) solution implies that A 6 0 (non-positive entries).\nWhen A 6 0, we can measure the tightness of the proposed relaxations. We see that the various relaxations, SDP then spectral, do not degrade performances over the exact approach MC (which cannot be run for general A).\nWe also notice that using a negative matrix A is a strong limitation. The performance observed when A is unconstrained or non-negative is better. This motivates our formulation and shows that repulsive weights between labels are relevant.\nThe Hamming loss and the F1 loss. In this experiment we do not make use of the quadratic prior, so A = 0. Table 7 gives the F1 loss we obtain by optimizing either the F1 loss or the Hamming loss. We compare the implementation of the F1 score minimization in [20] (carried out using a greedy technique). In that table, “OurF1” is our own implementation of the support vector technique forF1loss [15] using the optimization described in Section 4. This is an exact optimization technique. We also report the results obtained by training SVMs, using the one-versus-rest scheme. It appears that, on these standard datasets (V ≈ 10− 50), optimizing the F1 loss does not yield better performances than optimizing the Hamming loss."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We have proposed a framework to learn a prior for improving the performances of multi-label classification tasks. This prior takes the form of a quadratic function over the space of labels and incorporates both affinities and negative affinities. Existing work [20] only takes into account positive affinities between labels. We provide semidefinite and spectral relaxations of the learning problem, yielding to an efficient optimization scheme. In particular the spectral relaxation permits to deal computationally with datasets rather large (V > 150) whereas existing algorithms cannot (since the loss-augmented decoding problems have to solved many times).\nIt would be interesting to see how it is possible to leverage the range of applicability of the semidefinite relaxations which is, for now, limited to multi-label problems for which V is of the order of hundreds. To that extent, we could use techniques from matrix optimization theory, taking into account for the fact that the solution we aim at finding has low rank [16]."
    } ],
    "references" : [ {
      "title" : "Learning with submodular functions: A convex optimization perspective",
      "author" : [ "F. Bach" ],
      "venue" : "Foundations and Trends in ML",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient multi-label classification with many labels",
      "author" : [ "W. Bi", "J. Kwok" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convex optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge Univ.Press",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "On total variation minimization and surface evolution using parametric maximum flows",
      "author" : [ "A. Chambolle", "J. Darbon" ],
      "venue" : "International journal of computer vision, 84(3):288–307",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Optimizing the F-measure in multi-label classification: Plug-in rule approach versus structured loss minimization",
      "author" : [ "K. Dembczynski", "A. Jachnik", "W. Kotlowski", "W. Waegeman", "E. Huellermeier" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Relaxations and randomized methods for nonconvex QCQPs",
      "author" : [ "A. d’Aspremont", "S. Boyd" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "A kernel method for multi-labelled classification",
      "author" : [ "A. Elisseeff", "J. Weston" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "R. Fan", "K. Chang", "C. Hsieh", "X. Wang", "C. Lin" ],
      "venue" : "JMLR",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On the stationary values of a second-degree polynomial on the unit sphere",
      "author" : [ "G. Forsythe", "G. Golub" ],
      "venue" : "SIAM Journal on Applied Mathematics",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "A constrained eigenvalue problem",
      "author" : [ "Walter Gander", "Gene H Golub", "Urs von Matt" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1991
    }, {
      "title" : "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming",
      "author" : [ "M.. Goemans", "D. Williamson" ],
      "venue" : "JACM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1995
    }, {
      "title" : "Error detecting and error correcting codes",
      "author" : [ "R. Hamming" ],
      "venue" : "Bell system technical journal",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1950
    }, {
      "title" : "Large scale max-margin multi-label classification with priors",
      "author" : [ "B. Hariharan", "L. Zelnik-Manor", "S.V.N. Vishwanathan", "M. Varma" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Text categorization with support vector machines: Learning with many relevant features machine learning",
      "author" : [ "T. Joachims" ],
      "venue" : "Proc. ECML-98",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A support vector method for multivariate performance measures",
      "author" : [ "T. Joachims" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Low-rank optimization on the cone of positive semidefinite matrices",
      "author" : [ "M. Journée", "F. Bach", "P. Absil", "R. Sepulchre" ],
      "venue" : "SIAM Journal on Optimization, 20(5):2327–2351",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multilabel text classification for automated tag suggestion",
      "author" : [ "I. Katakis", "G. Tsoumakas", "I. Vlahavas" ],
      "venue" : "Proc. ECML",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Maximum margin multi-label structured prediction",
      "author" : [ "C. Lampert" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A shared task involving multi-label classification of clinical free text",
      "author" : [ "J.P. Pestian", "C. Brew", "P. Matykiewicz", "D.J. Hovermale", "N. Johnson", "K.B. Cohen", "W. Duch" ],
      "venue" : "Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing. Association for Computational Linguistics",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Submodular multi-label learning",
      "author" : [ "J. Petterson", "T. Caetano" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Kernel-based learning of hierarchical multilabel classification models",
      "author" : [ "J. Rousu", "C. Saunders", "S. Szedmak", "J. Shawe-Taylor" ],
      "venue" : "JMLR",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A note on a theorem of Forsythe and Golub",
      "author" : [ "E. Spjøtvoll" ],
      "venue" : "SIAM Journal on Applied Mathematics",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Max-margin markov networks",
      "author" : [ "B. Taskar", "C. Guestrin", "D. Koller" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The quadratic eigenvalue problem",
      "author" : [ "F. Tisseur", "K. Meerbergen" ],
      "venue" : "SIAM review",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Thomas Joachims",
      "author" : [ "I. Tsochantaridis" ],
      "venue" : "T., Y. Altun, and Y. Singer. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Multi-label classification: An overview",
      "author" : [ "G. Tsoumakas", "I. Katakis" ],
      "venue" : "International Journal of Data Warehousing and Mining (IJDWM)",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Sun database: Large-scale scene recognition from abbey to zoo",
      "author" : [ "J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba" ],
      "venue" : "CVPR. IEEE",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A review on multi-label learning algorithms",
      "author" : [ "M. Zhang", "Z. Zhou" ],
      "venue" : "TKDE",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Multi-label classification aims at predicting a set of labels for each data instance [26, 28].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 27,
      "context" : "Multi-label classification aims at predicting a set of labels for each data instance [26, 28].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "This setting is ubiquitous in real-world applications and for example can take the form of video or text tagging, where the goal is to assign instances to categories [14].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 26,
      "context" : "For video, [27] proposes to consider the problem of labeling scenes, on which several objects appear.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 20,
      "context" : "Incorporating structure into the label set can be done a priori by assuming labels are organized in a certain hierarchy [21]; [13] incorporates a prior knowledge when training the classifiers, permitting to learn correlated classifiers.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "Incorporating structure into the label set can be done a priori by assuming labels are organized in a certain hierarchy [21]; [13] incorporates a prior knowledge when training the classifiers, permitting to learn correlated classifiers.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 19,
      "context" : "This idea has already been tackled by [20] who restricted their study to the specific case of",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 25,
      "context" : "binary relevance technique [26]).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 24,
      "context" : "Within this setting, some approaches use the structured prediction framework [25, 23] as we do.",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "Within this setting, some approaches use the structured prediction framework [25, 23] as we do.",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "[18] has proposed to plug a model within a structured SVM, and considers the prior knowledge between labels as fixed a priori, whereas we aim at learning it.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Other approaches [5] considered the direct optimization of the F1-score within a structured SVM.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Another part of the recent literature dealing with multi-label classification [2] considers the case where the space of labels V itself is huge.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : "This is usually referred to as the binary relevance method for multi-label learning [26].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "A class of penalizations that are well-suited for our problem is the class of submodular functions [1, 20].",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "A class of penalizations that are well-suited for our problem is the class of submodular functions [1, 20].",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "[20] has proposed to use a graph-cut based penalty.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "Following [25, 23], we define the structural hinge loss H as: H(xi, yi,W,A, b) = max y∈{−1,1} {l(y, yi) +D(xi, y;W,A, b)−D(xi, yi,W,A)} .",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 22,
      "context" : "Following [25, 23], we define the structural hinge loss H as: H(xi, yi,W,A, b) = max y∈{−1,1} {l(y, yi) +D(xi, y;W,A, b)−D(xi, yi,W,A)} .",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "a(y, yi) = V + yyi 2V ∈ [0, 1] .",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "The loss associated to accuracy is the so-called Hamming loss [12, 28].",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : "The loss associated to accuracy is the so-called Hamming loss [12, 28].",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "= 1 2V ( V − yiy ) = 1− a(y, yi) ∈ [0, 1] , (9)",
      "startOffset" : 35,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : "A common choice in the multi-label learning literature is the Fβ − score loss [26, 20].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "A common choice in the multi-label learning literature is the Fβ − score loss [26, 20].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "Fβ(y, yi) = (1 + β) p(y, yi) r(y, yi) β2 p(y, yi) + r(y, yi) ∈ [0, 1] .",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "l(y, yi) = V − yyi 2V + y⊤ i 1+ y ⊤1 ∈ [0, 1] .",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "We propose to derive a structured-SVM-like optimization objective [25].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "As is often done when optimizing the F1 score, which is a contingency-table based loss [15], we can divide the initial problem into V + 1 subproblems by replacing y1 by 2k − V as follows:",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "[20] proposes a greedy approximate algorithm for solving this type of problems in the specific case where off diagonal entries of the prior A are negative.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "5 of [3]) and potential constraints over the domain.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 2,
      "context" : "Following [3, 6], we use a similar relaxation to the one used by [11] to approximate the max-cut problem.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "Following [3, 6], we use a similar relaxation to the one used by [11] to approximate the max-cut problem.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "Following [3, 6], we use a similar relaxation to the one used by [11] to approximate the max-cut problem.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "Following [3], the convex relaxation of this problem is obtained by removing the rank constraint.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : ", [3]) and define the matrix M as:",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 15,
      "context" : ", [16] and references therein).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "At test time, we follow [3] to round the relaxed solution, i.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "The inner loop problem is classical in optimization, in particular in trust-region methods [9, 22].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "The inner loop problem is classical in optimization, in particular in trust-region methods [9, 22].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 23,
      "context" : "It reduces—using the Lagrange multiplier technique—to solving a quadratic eigenvalue problem [24].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "In this section we present an other way to deal with the spectral relaxation, inspired by [10].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "Following [10], let us simply introduce the QR factorization of the matrix ( α 0 0 1 ) = QR, where",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "In this section, we show how we can solve the constrained problem by relating it to the well-studied total variation denoising problem [4, 1].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "In this section, we show how we can solve the constrained problem by relating it to the well-studied total variation denoising problem [4, 1].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "Note that, contrary to [20], in this section, we deal with the cardinality constraint exactly and we do not use any approximation algorithm in this specific case.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "(28) is a separable submodular optimization problem [1].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "integral of the cut J(u) (often referred to the “co-area formula” [4] for the specific case of cut functions or Lovasz extension for submodular functions), the generic proximal problem associated to any cut problem is:",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "We compare our implementation to [20] and to the one-versus-rest model (OvR).",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "Following [20], we picked our datasets from the mulan1 repository.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 6,
      "context" : "We picked the yeast [7], enron, medical [19] and bibtex [17] datasets.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "We picked the yeast [7], enron, medical [19] and bibtex [17] datasets.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "We picked the yeast [7], enron, medical [19] and bibtex [17] datasets.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "For every label, we train a linear classifier using a standard SVM toolbox [8].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "Our model and comparison to [20].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "We run our algorithm—with l equal to the Hamming loss—on all four datasets and compare to the available implementation of [20].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "Because of the challenging number of labels for bibtex, we were able to run neither the code from [20], nor the SDP, in reasonable time.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "SDP Spectral OvR [20] MC A 6 0 A > 0 Any A A 6 0 A > 0 Any A yeast 0.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "Table 3: Comparison between [20] and different variants of our method.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "Table 3 compares the one-versus-rest approach, the approach described in [20] and variants of our method.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "OvR [20] Our Hamming Our F1 yeast 0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 19,
      "context" : "We compare the implementation of the F1 score minimization in [20] (carried out using a greedy technique).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "In that table, “OurF1” is our own implementation of the support vector technique forF1loss [15] using the optimization described in Section 4.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "Existing work [20] only takes into account positive affinities between labels.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "To that extent, we could use techniques from matrix optimization theory, taking into account for the fact that the solution we aim at finding has low rank [16].",
      "startOffset" : 155,
      "endOffset" : 159
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we address the problem of multi-label classification. We consider linear classifiers and propose to learn a prior over the space of labels to directly leverage the performance of such methods. This prior takes the form of a quadratic function of the labels and permits to encode both attractive and repulsive relations between labels. We cast this problem as a structured prediction one aiming at optimizing either the accuracies of the predictors or the F1-score. This leads to an optimization problem closely related to the max-cut problem, which naturally leads to semidefinite and spectral relaxations. We show on standard datasets how such a general prior can improve the performances of multi-label techniques.",
    "creator" : "LaTeX with hyperref package"
  }
}