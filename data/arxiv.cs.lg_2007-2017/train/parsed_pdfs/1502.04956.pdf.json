{
  "name" : "1502.04956.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Linearization of Belief Propagation on Pairwise Markov Random Fields",
    "authors" : [ "Wolfgang Gatterbauer" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction Belief Propagation (BP) is an iterative message-passing algorithm for performing inference in graphical models (GMs), such as Markov Random Fields (MRFs). BP calculates the marginal distribution for each unobserved node, conditional on any observed nodes (Pearl 1988). It achieves this by propagating the information from a few observed nodes throughout the network by iteratively passing information between neighboring nodes. It is known that when the graphical model has a tree structure, then BP converges to the true marginals (according to exact probabilistic inference) after a finite number of iterations. In loopy graphs, convergence to the correct marginals is not guaranteed; in fact, it is not guaranteed at all, and using BP can lead to welldocumented convergence problems (Sen et al. 2008). While there is a lot of research on convergence of BP (Elidan, McGraw, and Koller 2006; Ihler, Fisher III, and Willsky 2005; Mooij and Kappen 2007), exact criteria for convergence are not known (Murphy 2012), and most existing bounds for BP\nThis paper is a significantly extended version of a paper with the same title presented at the 31st AAAI Conference on Artificial Intelligence (AAAI-17). The present paper contains all proofs and details on the experimental results. Possible future updates will be made available on CORR at http://arxiv.org/abs/1502.04956.\non general pairwise MRFs give only sufficient convergence criteria, or are for restricted cases, such as when the underlying distributions are Gaussians (Malioutov, Johnson, and Willsky 2006; Su and Wu 2015; Weiss and Freeman 2001).\nSemi-supervised node classification. BP is also a versatile formalism for semi-supervised learning; i.e., assigning classes to unlabeled nodes while maximizing the number of correctly labeled nodes (Koller and Friedman 2009, ch. 4). The goal is to predict the most probable class for each node in a network independently, which corresponds to the Maximum Marginal (MM) assignment (Domke 2013; Weiss 2000). Let P be a probability distribution over a set of random variables X∪Y. MM-inference (or “MM decoding”) searches for the most probable assignment yi for each unlabeled node Yi independently, given evidence X = x:\nMM(y|x) = {arg max yi P(Yi=yi|X=x)|Yi ∈ Y}\nNotice that this problem is simpler than finding the actual marginal distribution. It is also different from finding the Maximum A-Posteriori (MAP) assignment (the “most probable configuration”), which is the mode or the most probable joint classification of all non-evidence variables:1\nMAP(y|x) = arg max y P(Y=y|X=x)\nConvergent message-passing algorithms. There has been much research on finding variations to the update equations of BP that guarantee convergence. These algorithms are often similar in structure to the non-convergent algorithms, yet it can be proven that the value of the variational problem (or its dual) improves at each iteration (Hazan and Shashua 2008; Heskes 2006; Meltzer, Globerson, and Weiss 2009). Another body of recent papers have suggested to solve the convergence problems of MM-inference by linearizing the update equations. Krzakala et al. study a form of linearization for unsupervised classification called “spectral redemption” in the stochastic block model. That model\n1See (Murphy 2012, ch. 5.2.1) for a detailed discussion on why MAP has some undesirable properties and is not necessarily a “representative” assignment. While in theory it is arguably preferable to compute marginal probabilities, in practice researchers often use MAP inference due to the availability of efficient discrete optimization algorithms (Korč, Kolmogorov, and Lampert 2012).\nar X\niv :1\n50 2.\n04 95\n6v 2\n[ cs\n.A I]\n2 7\nD ec\n2 01\n6\nis unsupervised and has no obvious way to include supervision in its setup (i.e., it is not clear how to leverage labeled nodes). Donoho, Maleki, and Montanari propose “approximate message-passing” (AMP) as an iterative thresholding algorithm for compressed sensing that is largely inspired by BP. Koutra et al. linearize BP for the case of two classes and proposed “Fast Belief Propagation” (FaBP) as a method to propagate existing knowledge of homophily or heterophily to unlabeled data. This framework allows one to specify a homophily factor h (h > 0 for homophily or h < 0 for heterophily) and to then use this algorithm with exact convergence criteria for binary classification. Gatterbauer et al. derive a multivariate (“polytomous”) generalization of FaBP from binary to multiple labels called “Linearized Belief Propagation” (LinBP). Both aforementioned papers show considerable speed-ups for the application of node classification and relational learning by transforming the update equations of BP into an efficient matrix formulation. However, those papers solve only special cases: FaBP is restricted to two classes per node (de facto, one single score). LinBP can handle multiple classes, but is restricted to one single node type, one single edge type, and a potential that is symmetric and doubly stochastic (see Fig. 1).2\nContributions. This paper derives a linearization of BP for arbitrary pairwise MRFs, which transforms the parameters of an MRF into an equation system that replaces multiplication with addition. In contrast to standard BP, the derived update equations (i) come with exact convergence guarantees, (ii) allow a closed-form solution, (iii) keep the derived beliefs normalized at each step, and (iv) can thus be put into an efficient linear algebra framework. We also show empirically that this approach – in addition to its compelling computational advantages – performs comparably to Loopy BP for a large part of the parameter space. In contrast to prior work on linearizing BP, we remove any restriction on the potentials and solve the most general case for pairwise MRFs (see Fig. 1). Since it is known that any higher-order MRF can be converted to a pairwise MRF (Wainwright and Jordan 2008, Appendix E.3), the approach can be also be used for higher-order potentials. Our formalism can thus model arbitrary heterogeneous networks; i.e., such that have directed edges or have different types of nodes.3 This generalization is not obvious and required us to solve several new algebraic problems: (i) Non-symmetric potentials modulate messages differently across both directions of an edge; each direction then requires different centering points (this is particularly pronounced for non-quadratic potentials; i.e., when nodes adjacent to an edge have different numbers of classes). (ii) Multiplying belief vectors with non-stochastic\n2A potential is “doubly stochastic” if all rows and columns sum up to 1. As potentials can be scaled without changing the semantics of BP, this definition also extends to any potential where the rows and columns sum to the same value.\n3Notice that an underlying directed network is still modeled as an undirected Graphical Model (GM). For example, while the “friendship” relation on Facebook is undirected, the “follower” relation on Twitter is directed and has different implications on the two nodes adjacent to a directed “links to”-edge. Yet, the resulting GM is still undirected, but now has asymmetric potentials.\nBP FaBP LinBP this work # node types arbitrary 1 1 arbitrary # node classes arbitrary 2 const k arbitrary # edge types arbitrary 1 1 arbitrary edge symmetry arbitrary required required arbitrary edge potential arbitrary doubly stoch. doubly stoch. arbitrary closed form no yes yes yes\nHere, we write Zs for a normalizer that makes the elements of ys sum up to 1. Thus, the posterior belief ys(j) is computed by multiplying the prior belief xs(j) with the incoming messagesmus(j) from all neighbors u ∈ N(s), and then normalizing so that the beliefs in all ks classes sum to 1. In parallel, each node sends messages to each of its neighbors:\nmst(i)← 1\nZst\n∑\nj\nψst(j, i) xs(j) ∏\nu∈N(s)\\t mus(j) (2)\nHere,ψst(j, i) is a proportional “coupling weight” (or “compatibility,” “affinity,” “modulation”) that indicates the relative influence of class j of node s on class i of node t. Thus, the message mst(i) is computed by multiplying together all incoming messages at node s – except the one sent by the recipient t – and then passing through the ψst edge potential. Notice that we use Zst in Eq. (2) as a normalizer that makes the elements of mst sum up to kt at each iteration. As pointed out by Murphy, Weiss, and Jordan; Pearl, normalizing the messages has no effect on the final beliefs; however, this intermediate normalization of messages will become crucial in our derivations. BP then repeatedly computes the above update equations for each node until the values (hopefully) converge. At iteration r of the algorithm, ys(j) represents the posterior belief of j conditioned on the evidence that is r steps away in the network.\n3 Linearizing BP over any pairwise MRF This section gives a closed form description for the final beliefs after convergence of BP in arbitrary pairwise MRFs under a certain limit consideration of all parameters. This is a strict and non-trivial generalization of recent works (Fig. 1). The difficulty of our generalization lies in technical details: non-symmetric potentials require different centering points for messages across different directions of an edge; non-stochastic potentials require different normalizers for different iterations (and for different potentials in the networks) which does not easily lead to a simple matrix formulation; and the full heterogenous case (e.g., different number of classes k for different nodes) requires a considerably more general derivation and final formulation.\nOur approach is conceptually simple: we center all matrix entries around well-chosen default values and then focus only on the deviations from these defaults using Maclaurin series at several steps in our derivation. The resulting equations replace multiplication with addition and can thus be put into the framework of matrix-vector multiplication, which can leverage existing highly-optimized code. It also allows us to give exact convergence criteria for the resulting update equations and a closed form solution (that would require the inversion of a large matrix). The approach is similar in spirit to the idea of writing any MRF (with strictly positive density) as log-linear model. However, by starting from the update equations for loopy BP, we solve the intractability problem by ignoring all dependencies between messages that have traveled over a path of length 2 or more. Definition 1 (Centering). We call a vector x or matrix X “centered around cwith standard deviation v” if the average entry µ(x) = c and standard deviation σ(x) = v.\nDefinition 2 (Residual vector/matrix). If a vector x is centered around c, then the “residual vector” x̂ around c is defined as x̂ = [x1− c, x2− c, . . .]ᵀ. Accordingly, we denote a matrix X̂ as a “residual matrix” if each entry is the residual after centering around c.\nFor example, the vector x = [1.1, 1.2, 0.7]ᵀ is centered around c = 1, and the residuals from 1 form the residual vector x̂ = [0.1, 0.2,−0.3]ᵀ; i.e., x = 13 + x̂, where 13 is the 3-dimensional vector with all entries equal to 1. By definition of a normalized vector, beliefs for any node s are centered around 1ks , and the residuals for prior beliefs have non-zero elements (i.e., x̂s 6= 0ks ) only for nodes with local evidence (nodes “with explicit beliefs”). Further notice that the entries in a residual vector or matrix always sum up to 0 (i.e., ∑ i x̂(i) = 0). This is done by construction and will become important in the derivations of our results. The main idea of our derivation relies then on the following observation: if we start with messages and potentials with rows and columns centered around 1 with small enough standard deviations, then the normalizer of the update equation Eq. (2) is independent of the beliefs and remains constant as Zst = k−1t . Importantly, the resulting equations do not require further normalization. The derivation further makes use of certain linearizing approximations that result in a well-behaved linear equation system. We show that the MM solutions implied by this equation system are identical to those from the original BP update equations in case of nearly uniform priors and potentials. For strong priors and potentials (e.g., [ 1 100100 1 ]), the resulting solutions are not identical anymore, yet serve as reasonable approximations in a wide range of problem parameters (see Section 4). WLOG, we start with potentials that are centered around 1 and then re-center the potentials before using them:4\nDefinition 3 (Row-recentered residual matrix). Let ψ ∈ R`×k be centered around 1 and ψ̂ be the residual matrix around 1. Furthermore, let r̂(j) := ∑ i ψ̂(j, i) be the sum of the residuals of row j. Then the “row-recentered residual matrix” ψ̂\n′ has entries ψ̂′(j, i) := 1k ( ψ̂(j, i)− r̂(j)k ) .\nBefore we can state our main result, we need some additional notation. WLOG, let [n] be the set of all nodes. For each node s ∈ [n], let ks be the number of its possible classes. Let ks := 1ks1ks , i.e., the ks-dimensional uniform stochastic column vector. Furthermore, let ktot :=∑ s∈[n] ks be the sum of classes across nodes. To write all our resulting equations as one large equation system, we stack the individual explicit (x̂) and implicit (ŷ) residual belief vectors together with the ks-vectors one underneath the other to form three ktot-dimensional stacked column vectors. We also combine all row-recentered residual matrices into one large but sparse [ktot × ktot]-square block matrix\n4Without changing the joint probability distribution, every potential in a MRF can be scaled so that the average entry is 1. For\nexample, given ψ = [ 4 6 56 8 7 ], we scale by 1 6\nto get ψ = [ 2 3 1 5 6\n1 4 3 7 6\n] ,\nwhich has the identical semantics but is now centered around 1.\n(notice that all entries for non-existing edges remain empty):\nŷ :=   ŷ1 ... ŷn  , x̂ :=   x̂1 ... x̂n  , k :=   k1 ... kn  , ψ̂ ′ :=   ψ̂ ′ 11 . . . ψ̂ ′ 1n ... . . . ... ψ̂ ′ n1 . . . ψ̂ ′ nn  \nWe can now state our main theorem:\nTheorem 4 (Linearizing Belief Propagation). Let ŷ, x̂, k̂, and ψ̂ ′ be the above defined residual vectors and matrix. Let be a bound on the standard deviation of all non-zero entries of ψ̂ ′ and x̂, σ(ψ̂ ′ ) < and σ(x̂) < . Let yBPv be the final belief assignment for any node v after convergence of BP. Then, for lim →0+ , arg maxi yBPv (i) = arg maxi ŷ Lin v (i), where ŷv results from solving the following system of ktot linear equations in ŷ:\nŷ = x̂︸︷︷︸ 1st\n+ ψ̂ ′ᵀ k︸ ︷︷ ︸\n2nd\n+ ψ̂ ′ᵀ ŷ︸ ︷︷ ︸\n3rd − ψ̂′ᵀ2ŷ︸ ︷︷ ︸ 4th\n(3)\nIn other words, the MM node labeling from BP can be approximated by solving a linear equation system if each of the potentials and each of the beliefs are reasonably tightly centered around their average values. Notice that the 2nd term ψ̂ ′ᵀ k is a “bias” vector that depends only on the structure of the network and the potentials, but not the beliefs. We thus sometimes prefer to write ĉ′∗ := ψ̂ ′ᵀ k to emphasize that it remains constant during the iterations. This term vanishes if all potentials are doubly stochastic. Also notice that the 4th term is what was called the “echo cancellation” in (Gatterbauer et al. 2015).5 Simple algebraic manipulations then lead a closed-form solution by solving Eq. (3) for ŷ:\nŷ = ( Iktot − ψ̂ ′ᵀ + ψ̂ ′ᵀ2)−1( x̂ + ĉ′∗ ) (4)"
    }, {
      "heading" : "Iterative updates and convergence",
      "text" : "The complexity of inverting a matrix is cubic in the number of variables, which makes direct application of Eq. (4) difficult. Instead, we use Eq. (3), which gives an implicit definition of the final beliefs, iteratively. Starting with an arbitrary initialization of ŷ (e.g., all values zero), we repeatedly compute the right hand side of the equations and update the values of ŷ until the process converges:6\n5Notice that the BP update equations send a message across an edge that excludes information received across the same edge from the other direction: “u ∈ N(s)\\ t” in Eq. (2). In a probabilistic scenario on tree-based graphs, this echo cancellation is required for correctness. In loopy graphs (without well-justified semantics), this term still compensates for the message a node t would otherwise send to itself via a neighbor s, i.e., via the path t→ s→ t.\n6Interestingly, our linearized update equations, Eq. (5), are reminiscent of the update equations for the mean beliefs in Gaussian MRFs (Malioutov, Johnson, and Willsky 2006; Su and Wu 2015; Weiss and Freeman 2001). Notice however, that whereas the update equations are exact in the case of continuous Gaussian MRFs, our equations are approximations for the general discrete case.\nProposition 5 (Update equations). The positive fix points for Eq. (3) can be calculated iteratively with the following update equations starting from ŷ(0) = 0:\nŷ(r+1) ← ( x̂ + ĉ′∗ ) + ( ψ̂ ′ᵀ − ψ̂′ᵀ2 ) ŷ(r) (5)\nThese particular update equations allow us to give a sufficient and necessary criterium for convergence via the spectral radius ρ of a matrix.7\nCorollary 6 (Convergence). The update Eq. (5) converges if and only if ρ ( ψ̂ ′ − ψ̂′2 ) < 1.\nThus, the updates converge towards the closed-form solution, and the final beliefs of each node can be computed via efficient matrix operations with optimized packages, while the implicit form gives us guarantees for the convergence of this process.8 In order to apply our approach to problem settings with spectral radius bigger than one (and thus direct application of Eq. (5) would not work), we propose to modify the model by weakening the potentials. In other words, we multiply ψ̂ ′ with a factor that guarantees convergence. We call the multiplicative factor which exactly separates convergence from divergence, the “convergence boundary” ∗. Choosing any with s := ∗ and s < 1 guarantees convergence. We call any choice of s the “convergence parameter.”\nDefinition 7 (Convergence boundary ∗). For any ψ̂ ′ , the convergence boundary ∗ > 0 is defined implicitly by ρ ( ∗ψ̂ ′ − 2∗ψ̂ ′2) = 1."
    }, {
      "heading" : "Computational complexity",
      "text" : "Naively materializing ψ̂ ′\nwould lead to a space requirement of O(n2k2max) where n is the number of nodes and kmax the max number of classes per node. However, by using a sparse matrix implementation, both the space requirement and the computational complexity of each iteration are only proportional to the number of edges: O(mk2max). The time complexity is identical to the one of message-passing with division, which avoids redundant calculations and is faster than standard BP on graphs with high node degrees (Koller and Friedman 2009). However, the ability to use existing highly-optimized packages for efficient matrix-vector multiplication will considerably speed-up the actual calculations.\n4 Experiments Questions. Our experiments will answer the following 3 questions: (1) What is the effect of the convergence parameter s on accuracy and number of required iterations until convergence? (2) How accurate is our approximation under\n7The “spectral radius” ρ(·) of a matrix is the supremum among the absolute values of its eigenvalues.\n8The intuition behind these equivalences can be illustrated by comparing to the geometric series S = 1 + x + x2 + . . . and its closed form S = (1 − x)−1. Whereas for |x| < 1, the series converges to its closed-form, for |x| > 1, it diverges, and the closed-form is meaningless.\nvarying conditions: (i) the density of the network, (ii) the strength on the interaction, and (iii) the fraction of labeled nodes? (3) How fast is the linearized approximation as compared to standard Loopy BP?\nExperimental protocol. We define “accuracy” as the fraction of unlabeled nodes that receive correct labels. In order to evaluate the accuracy of a method, we need to use graphs with known label ground truth (GT). As we are interested in the accuracy as a function of various parameters, we need graphs with controlled GT. We thus decided to compare BP against its linearization on synthetic graphs with known GT, which allows us to measure the accuracy as result of systematic parameter changes. The well-studied stochastic block-model (Airoldi et al. 2008) leads to networks with degree distributions that are not similar to those found in most empirical network data. Our synthetic graph generator is thus a variant thereof with two important differences: (1) we actively control the degree distributions in the resulting graph; and (2) we “plant” exact graph properties (instead of fixing a property only in expectation). In other words, our generator preserves desired degree distribution and compatibilities between classes. The online appendix (Gatterbauer 2015) contains all details. We focus on the scenario of a network with one non-symmetric potential along each edge. The generator creates a graph using a tuple of parameters (n,m,α,ψ,dist), where n is the number of nodes, m is the number of edges, α is the node label distribution with α(i) being the fraction of nodes of class i, ψ is the edge potential, and dist is a chosen degree distribution (e.g., uniform or power law with chosen coefficient).\nParameter choices. Throughout our experiments, we use k = 3 classes and the potential ψ = [ 1 h 1 1 1 h h 1 1 ] , parameterized by a value h representing the ratio between min and max entries. Dividing by (2 + h) centers it around 1. Thus parameter h models the strength of the potential, and we expect higher values of h to make our approximation less suitable. Notice that this matrix is not symmetric and shows very different modulation behavior across both directions of an edge. We create graphs with n nodes and assign the same fraction of nodes to one of the 3 classes: α = [ 13 , 1 3 , 1 3 ]. We also vary the parameters m and d = mn as the average inand outdegree in the graph, and we assume a power law distribution with coefficient 0.5. We then keep a fraction f of node labels and measure accuracy on the remainder.\nComputational setup. All methods are implemented in Python and use the optimized SciPy library (Jones et al. 2001) to handle sparse matrix operations. The experiments are run on a 2.5 Ghz Intel Core i5 with 16G of main memory and a 1TB SSD hard drive. To allow comparability across implementations, we limit evaluation to one processor. For timing BP, we use message-passing with division which is faster than standard BP on graphs with high node degree (Koller and Friedman 2009). To calculate the approximate spectral radius of a matrix, we use a method from the PyAMG library (Bell, Olson, and Schroder 2011) that implements a technique described in (Bai et al. 2000). Our code, including the data generator, is inspired by Scikit-learn (Pedregosa et al. 2011) and is available on Github to encour-\nage reproducible research (SSLH 2015).\nQuestion 1. What is the effect of scaling parameter s on accuracy and number of iterations for convergence?\nResult 1. Our scaling parameter s gives an exact criterion for our approach to converge. In contrast, BP often does not converge and requires a lot of fine-tuning; e.g., damping or even scaling of the potential. The accuracy of the linearization is highest for s close or slightly above 1 and by not iterating until convergence.\nFigure 2a shows the number of required iterations to reach convergence and confirms our theoretical results from Corollary 6. In case the convergence condition does not hold, we scale the centered potential by a value , resulting from = s · ∗ with s < 1. This action weakens the potentials, but preserves the relative affinities (we also use the same approach to help BP find a fixed point if it does not converge within 200 iterations). Figure 2b shows what happens to accuracy if we run the iterative updates a fixed number of times as a function of s. Notice that even considerably scaling a potential does not entirely change the model and still gives reasonable approximations. The figure fixes a number of iterations, but then varies again via s. Also interestingly, almost all of the performance gains from the linearized update equations come from running just a few iterations, and convergence for optimal labeling is not necessary; instead, by choosing s ≈ 1 (at the exact boundary of convergence) or even s > 1 and iterating only a few times, we can maximize the expected accuracy. For the remaining accuracy experiments, we use s = 0.5 and run our algorithm to convergence.\nQuestion 2. How accurate is our approximation, and under which conditions is it reasonable?\nResult 2. The linearization gives comparable labeling accuracy as LBP for graphs with weak potentials. The performance deteriorates the most in dense networks with strong potentials.\nWe found that h, d and f have important influence on the labeling accuracy of BP and its linearization (whereas n, dist and α influence only to a lesser extent). Figures 2c and 2d show accuracy as a function of the fraction f of labeled nodes. Notice that we chose the best BP was able to perform (over several choices of and damping factors to make it converge) whereas for LinBP we consistently chose s = 0.5 as proposed in (Gatterbauer et al. 2015). Figures 2e to 2g show labeling quality as a function the strength h of the potential. For strong potentials (h > 3), BP gives better accuracy if it converges. In practice, BP often did not converge within 200 iterations even for weak potentials (bordered data points required dampening; red crosses required additional entry-wise scaling of the potential with our convergence boundary ∗). In our experiments, BP often did not converge despite using damping, surprisingly often when h is not big. It is known that if the potentials are close to indifference then loopy BP usually converges. In this case, our formalism is equivalent to loopy BP (this follows from our linearization). Thus, whenever loopy BP did not converge,\nwe simply exponentiated the entries of the potential with a varying factor until BP converged. Thus for high h, BP can perform better than the linearization, but only after a lot of fine-tuning of parameters. In contrast, for our formulation we know exactly the boundary of convergence.\nOverall, the linearization gives comparable results to the original BP for small potentials, and BP performance is better than the linearization only either for strong potentials with h ≥ 3 and dampening (see a few yellow dots without borders as exceptions) or after fine-tuning BP after using our own convergence boundary and scaling the potentials, or after a lot of manual fine-tuning.\nQuestion 3. How fast is the linearized approximation as compared to BP?\nResult 3. The linearization is around 100 times faster than BP per iteration and often needs 10 times fewer iterations until convergence. In practice, this can lead to a speed-up of 1000 times.\nA key advantage of the linearization is that it has predictable convergence and comes with considerable speedups. Figure 2h shows that our approach scales linearly in the number of edges and is 50 times faster than regular loopy BP per iteration; an iteration on a graph with 3 million nodes and 30 million edges takes less than 2 sec. Calculating the exact convergence boundary via a spectral radius calcula-\ntion can take more time (approx. 1000 sec for the same graph). Notice that any dampening strategy for BP results in increased number of iterations and needs to overcome the additional slow-down of further iterations. Also recall that on each circled point in Figs. 2e to 2g, BP did not converge within 200 iterations and required dampening; each red cross required additional scaling of the potentials with our calculated ∗ in order to make BP converge.\n5 Conclusions We have derived a linearization of BP for arbitrary pairwise MRFs for the purpose of node labeling with MM-inference. The approach transforms the parameters of an MRF into a linear equation system that can be solved with simple iterative updates. These updates come with exact convergence guarantees, allow a closed-form solution, keep the derived beliefs normalized at each step, and can thus be put into an efficient linear algebra framework that does not require normalization at each step. Experiments on carefully controlled synthetic data with known ground truth show that our approach performs comparably with Loopy BP for weak potentials and comes with a predictable behavior, compelling computational advantages, and an easy implementation with only few lines of code. An unexplored application of the linearization may be speeding-up convergence of regular BP by starting from good approximations of its fixed points.\nAcknowledgements. This work was supported in part by NSF grant IIS-1553547. I would like to thank Christos Faloutsos for very convincingly persuading me of the power of linear algebra and continued support. I am also grateful to Stephan Günneman, Vladimir Kolmogorov, and Christoph Lampert for a number of insightful comments.\nReferences Airoldi, E. M.; Blei, D. M.; Fienberg, S. E.; and Xing, E. P. 2008. Mixed membership stochastic blockmodels. Journal of Machine Learning Research 9:1981–2014. Bai, Z.; Demmel, J.; Dongarra, J.; Ruhe, A.; and van der Vorst, H. 2000. Templates for the solution of algebraic eigenvalue problems. SIAM. Bell, W. N.; Olson, L. N.; and Schroder, J. B. 2011. PyAMG: Algebraic multigrid solvers in Python v2.0. Domke, J. 2013. Learning graphical model parameters with approximate marginal inference. IEEE Trans. Pattern Anal. Mach. Intell. 35(10):2454–2467. Donoho, D. L.; Maleki, A.; and Montanari, A. 2009. Message-passing algorithms for compressed sensing. PNAS 106(45):18914–18919. Elidan, G.; McGraw, I.; and Koller, D. 2006. Residual belief propagation: Informed scheduling for asynchronous message passing. In UAI, 165–173. Gatterbauer, W.; Günnemann, S.; Koutra, D.; and Faloutsos, C. 2015. Linearized and single-pass belief propagation. PVLDB 8(5):581–592. Gatterbauer, W. 2015. The linearization of belief propagation on pairwise markov random fields. CoRR abs/1502.04956. (http://arxiv.org/abs/1502.04956). Hazan, T., and Shashua, A. 2008. Convergent messagepassing algorithms for inference over general graphs with convex free energies. In UAI, 264–273. Heskes, T. 2006. Convexity arguments for efficient minimization of the Bethe and Kikuchi free energies. J. Artif. Intell. Res. (JAIR) 26:153–190. Ihler, A. T.; Fisher III, J. W.; and Willsky, A. S. 2005. Loopy belief propagation: Convergence and effects of message errors. Journal of Machine Learning Research 6:905–936. Jones, E.; Oliphant, T.; Peterson, P.; et al. 2001. SciPy: Open source scientific tools for Python. Koller, D., and Friedman, N. 2009. Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning. MIT Press. Korč, F.; Kolmogorov, V.; and Lampert, C. 2012. Approximating marginals using discrete energy minimization. In ICML Workshop on Inferning: Interactions between Inference and Learning. Koutra, D.; Ke, T.-Y.; Kang, U.; Chau, D. H.; Pao, H.-K. K.; and Faloutsos, C. 2011. Unifying guilt-by-association approaches: Theorems and fast algorithms. In ECML/PKDD (2), 245–260.\nKrzakala, F.; Moore, C.; Mossel, E.; Neeman, J.; Sly, A.; Zdeborová, L.; and Zhang, P. 2013. Spectral redemption in clustering sparse networks. PNAS 110(52):20935–20940. Malioutov, D. M.; Johnson, J. K.; and Willsky, A. S. 2006. Walk-sums and belief propagation in Gaussian graphical models. Journal of Machine Learning Research 7:2031– 2064. Meltzer, T.; Globerson, A.; and Weiss, Y. 2009. Convergent message passing algorithms – a unifying view. In UAI, 393– 401. Mooij, J. M., and Kappen, H. J. 2007. Sufficient conditions for convergence of the sum-product algorithm. IEEE Transactions on Information Theory 53(12):4422–4437. Murphy, K. P.; Weiss, Y.; and Jordan, M. I. 1999. Loopy belief propagation for approximate inference: An empirical study. In UAI, 467–475. Murphy, K. P. 2012. Machine learning: a probabilistic perspective. Adaptive computation and machine learning series. MIT Press. Pearl, J. 1988. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V.; Vanderplas, J.; Passos, A.; Cournapeau, D.; Brucher, M.; Perrot, M.; and Duchesnay, E. 2011. Scikitlearn: Machine learning in Python. Journal of Machine Learning Research 12:2825–2830. Sen, P.; Namata, G.; Bilgic, M.; Getoor, L.; Gallagher, B.; and Eliassi-Rad, T. 2008. Collective classification in network data. AI Magazine 29(3):93–106. SSLH. 2015. A Python package for Semi-Supervised Learning with Heterophily. http://github.com/sslh/sslh/. Su, Q., and Wu, Y. 2015. On convergence conditions of Gaussian belief propagation. IEEE Transactions on Signal Processing 63(5):1144–1155. Wainwright, M. J., and Jordan, M. I. 2008. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning 1(1-2):1–305. Weiss, Y., and Freeman, W. T. 2001. Correctness of belief propagation in Gaussian graphical models of arbitrary topology. Neural Computation 13(10):2173–2200. Weiss, Y. 2000. Correctness of local probability propagation in graphical models with loops. Neural Computation 12(1):1–41.\nGiven a matrix X, we write X(i, j) for one scalar entry, X(i, :) for the i-th row vector, and X(:, j) for the j-th column vector. We also write ∑ j as short form for ∑ j∈[k] whenever k is clear from the context.\nB Derivation of the linearization of BP over any pairwise MRFs This section contains the derivation of Theorem 4. We will center the elements of all message and belief vectors around their “natural default values,” i.e., the elements of mst around 1, and the elements of xs, and ys around 1 ks\n(Lemma 10 will provide some intuition why our chosen center points are the natural choice to simplify all later derivations). We are interested in the residual values defined by m̂(i) := m(i)− 1, x̂s(j) := xs(j)− 1ks , and ŷs(j) := ys(j)− 1 ks\n. WLOG, we start from a potential ψ ∈ R`×k that is centered around 1 (Recall that we can scale any potential with a positive real number without changing the semantics of the MRF). We then appropriately recenter a potential differently across both directions of an edge as to make it singly stochastic for either direction and most of the residual terms for the belief update equations cancel each other out, leading to simplified equations. Definition 3 provided the definition for the residual matrix in one direction, row-recentering. Adding to that definition, the row-recentered stochastic matrixψ′ is centered around 1k and has entries ψ′(j, i) := ψ̂′(j, i) + 1k . Both matrices are indicated with a single apostrophe ′.\nAnalogously, let ĉ(i) = ∑ i ψ̂(j, i) be the residual sum of column i. Then, a column-recentered residual matrix ψ̂ ′′ has\nentries ψ̂′′(j, i) := 1` (ψ̂(j, i) − ĉ(i) ` ) and the column-recentered stochastic matrix ψ ′′ has entries ψ′′(j, i) := 1` + ψ̂ ′′(j, i). Notice that both matrices are indicated with a double apostrophe ′′. The resulting recentered residual potentials are coupling matrices that make explicit the relative attraction and repulsion of neighboring nodes. For example, the sign of ψ̂′(j, i) tells\nψ̂ ′ and ψ̂ ′′ (and stochastic matrices ψ′ and ψ′′).\nus if the class j attracts or repels class i in a neighbor, and the magnitude of ψ̂′(j, i) indicates the extent. Subsequently, this centering allows us to rewrite belief propagation in terms of the residuals.\nNotice that column-recentering and row-recentering are connected via the transpose. However, message modulation across one direction of an edge is is not simply the transpose of the modulation across the other direction:\nCorollary 8 (Row-recentering vs. column-recentering). (ψ̂ ′′ )ᵀ = (ψ̂ ᵀ )′. In particular,\nψ̂ ′′ st = (ψ̂ ′ ts) ᵀ\nWe also write the `-dimensional vector r := ψ1k for the row sums, the k-dimensional vector c := ψ ᵀ 1` for the column\nsums, and s := ∑ j r(j) = 1 ᵀ ` r = 1 ᵀ ` ψ1k for the sum of all entries in a matrix. We illustrate recentering next with a detailed example.\nExample 9 (Recentering). Figure 4 shows the 3× 2 matrix ψ that is centered around 1 (i.e., each entry is close to 1 and the average is exactly 1) together with the row sums r(j) and the column sums c(i). ψ̂ is then the residual matrix. Notice that the recentered residual matrices ψ̂ ′ and ψ̂ ′′ have zero row sums r̂(j)′ or column sums ĉ(i)′′, respectively. As consequence, the row-recentered matrix ψ′ and column-recentered matrix ψ′′ are row-stochastic or column-stochastic, respectively.\nWe will further make use of the linearizing Maclaurin series approximations shown in Fig. 5 to derive a well-behaved linear equation system."
    }, {
      "heading" : "Recentering",
      "text" : "The following lemma provides the mathematical justification for our particular choice of recentering:\nLemma 10 (Recentering). Consider the update equation\ny← 1 Z ψᵀx (6)\nwith x being a `-dimensional stochastic vector, ψ ∈ R`×k being centered around 1, and Z a normalizer that makes the elements of the resulting k-dimensional vector y sum up to k. Then, the update equation can be approximated with the row-recentered stochastic matrix ψ′ by\ny← kψ′ᵀx (7)\nProof Lemma 10. Our proof will express both equations (Eq. (6) with ψ and Eq. (7) with ψ′) in terms of the residual matrix ψ̂ ′ , and show that they lead to the same equation. From Definition 3 and the definitions at the beginning of Appendix B, we know that ψ(j, i) = 1 + ψ̂(j, i) and ψ̂(j, i) = k ψ̂′(j, i) + r̂(j)k . Therefore, ψ(j, i) = 1 + k ψ̂ ′(j, i) + r̂(j)k . Similarly, ψ′(j, i) = 1k + ψ̂ ′(j, i).\nIn the following, we are going to use matrix notation that allows us to express the above identities very compactly as: ψᵀ = 1k1 ᵀ ` + 1 k1kr̂ ᵀ + k ψ̂ ′ᵀ , ψ′ᵀ = 1k1k1 ᵀ ` + ψ̂ ′ᵀ , and x = 1`1` + x̂. 9\n(i) Equation (6): We calculate y in two steps that treat the normalization separately: first z = ψᵀx, and then y = 1Z z.\nz = ψᵀx\n= ( 1k1 ᵀ ` + 1\nk 1kr̂\nᵀ + kψ̂ ′ᵀ) · (1 ` 1` + x̂ )\n= 1k + 1\nk` 1k ŝ︸︷︷︸\n=0\n+ k ` ψ̂ ′ᵀ 1`︸ ︷︷ ︸\n=ĉ′ +1k 1 ᵀ ` x̂︸︷︷︸ =0 + 1 k 1kr̂ ᵀx̂ + kψ̂ ′ᵀ x̂\n= 1k + 1\nk 1kr̂\nᵀx̂ + k ` ĉ′ + k ψ̂ ′ᵀ x̂\nWe next calculate the value of the normalizer. Recall that the normalizer makes the entries of the vector z sum up to k.\nZ = 1 k 1ᵀkz = 1 k\n( k + r̂ᵀx̂ + k\n` 1ᵀk ĉ ′\n︸︷︷︸ =0\n+k 1ᵀkψ̂ ′ᵀ x̂︸ ︷︷ ︸\n=0\n)\n= 1 + r̂ᵀx̂ k\nWe see that the normalizer is not a constant but also depends on ψ and x. However, notice that if each row of ψ is centered around 1 (not just the matrix as a whole), then r̂(j) = 0 for all rows, and thus Z = 1. In the following, we approximate 1/(1 + ) ≈ (1− ) and (1 + 1)(1 + 2) ≈ 1 + 1 − 2.\ny = ( 1k + 1\nk 1kr̂\nᵀx̂ + k ` ĉ′ + kψ̂\n′ᵀ x̂ )( 1 + r̂ᵀx̂ k )−1\n≈ ( 1k + 1\nk 1kr̂\nᵀx̂ + k ` ĉ′ + kψ̂\n′ᵀ x̂ )( 1− r̂ ᵀx̂ k )\n≈ 1k + 1\nk 1kr̂\nᵀx̂ + k ` ĉ′ + kψ̂ ′ᵀ x̂− 1 k 1kr̂ ᵀx̂\n= 1k + k ` ĉ′ + kψ̂ ′ᵀ x̂\nNotice that the above equation is exact if r̂(j) = 0 for all rows. (ii) Equation (7): Here we get the same result much faster:\ny = kψ′ᵀx\n= ( 1k1 ᵀ ` + kψ̂ ′ᵀ) · (1 ` 1` + x̂ ) = 1k + k ψ̂ ′ᵀ 1`︸ ︷︷ ︸\nĉ′\n1 ` + 1k 1 ᵀ ` x̂︸︷︷︸ 0 +kψ̂ ′ᵀ x̂\n= 1k + k ` ĉ′ + kψ̂ ′ᵀ x̂\n9A quick illustration: 1 k 1kr̂ ᵀ = 1 1 [1, 1]ᵀ[−0.06, 0, 0.06] = 1 2 [−0.06 0 0.06 −0.06 0 0.06 ] for ψ in Fig. 4.\nIt follows that Eq. (7) is an approximation of Eq. (6), in general, and both equations are equivalent if each row in ψ is centered around 1.\nNotice that, since y(j) = 1 + ŷ(j), we can express the update equation in terms of residuals as\nŷ = k ` ĉ′ + kψ̂ ′ᵀ x̂\nFurther notice that if each column in the original potential is centered around 1, then the term ĉ′ disappears. Overall, Lemma 10 implies that by recentering the coupling matrix, we can replace the normalizer with a constant, which considerably simplifies our later derivations. The proof also showed that the approximation becomes exact if each row in ψ is centered around 1.\nExample 11 (Recentering (Example 9 continued)). Consider again matrix ψ ∈ R3×2 in Fig. 4: The matrix is centered around 1 as the sum of its entries is s = 6. However, row 1 is not centered around 1 as its row sum r(1) = 1.94 instead of 2. Next assume x = [0.1, 0.1, 0.8]ᵀ. Then y = [0.99021, 1.00979]ᵀ for Eq. (6), but y = [0.99, 1.01]ᵀ with Eq. (7). Thus, the residuals are ±0.00979 and ±0.01, respectively, and the relative difference ≈ 2%."
    }, {
      "heading" : "Centered BP",
      "text" : "By using the previous lemma and focusing on the residuals only, we can next transform the belief update equations from multiplication into addition:\nLemma 12 (Centered BP). By appropriately centering the coupling matrix, beliefs and messages, the equations for belief propagation can be approximated by:\nŷs(j)← x̂s(j) + 1\nks\n∑\nu∈N(s) m̂us(j) (8)\nm̂st(i)← kt ks ĉst(i)\n′ + kt ∑\nj\nψ̂′st(j, i) ( ŷs(j)− 1\nks m̂ts(j)\n) (9)\nProof Lemma 12. (i) Equation (8): Substituting the expansions into the belief updates Eq. (1) leads to\n1\nks + ŷs(j)←\n1 Zs · ( 1 ks + x̂s(j) ) · ∏\nu∈N(s)\n( 1 + m̂us(j) )\nln ( 1 + ksŷs(i) ) ← − lnZs + ln ( 1 + ksx̂s(j) ) + ∑ u∈N(s) ln ( 1 + m̂us(j) )\nWe then use the approximation ln(1 + ) ≈ for small :\nkŷs(j)← − lnZs + kx̂s(j) + ∑\nu∈N(s) m̂us(j) (10)\nSumming both sides over j gives us:\nks ∑\nj\nŷs(j)\n︸ ︷︷ ︸ =0\n← −ks lnZs + ks ∑\nj\nx̂s(j)\n︸ ︷︷ ︸ =0\n+ ∑\nj\n∑\nu∈N(s) m̂us(j) ︸ ︷︷ ︸ =0\nHence, we see that lnZs needs to be 0, and therefore our normalizer is actually a normalization constant and for all nodes Zs = 1. Plugging Zs = 1 back into Eq. (10) leads to Eq. (8):\nŷs(j)← x̂s(j) + 1\nks\n∑\nu∈N(s) m̂us(j)\n(ii) Equation (9): Using Lemma 10, we can write Eq. (2) as follows (recall that kt and ψ′st take care of the normalization):\nmst(i)← kt ∑\nj\nψ′st(j, i)xs(j) ∏\nu∈N(s)\\t mus(j)\nBy further using Eq. (8), we get:\n← kt ∑\nj\nψ′st(j, i) xs(j)\n∏ u∈N(s)mus(j)\nmts(j)\n← kt ∑\nj\nψ′st(j, i) ys(j)\nmts(j)\nThen, using the centering together with the approximation 1 k+ 1 1+ 2 ≈ 1k + 1 − 1k 2, we get:\n1 + m̂st(i)← kt ∑\nj\n( 1 kt + ψ̂′st(j, i) ) 1 ks + ŷs(j) 1 + m̂ts(j)\n← kt ∑\nj\n( 1 kt + ψ̂′st(j, i) )( 1 ks + ŷs(j)− 1 ks m̂ts(j) )\n← kt ( 1 kt + 1 ks ∑\nj\nψ̂′st(j, i)\n︸ ︷︷ ︸ =ĉ′st(i)\n+ 1\nkt\n∑\nj\nŷs(j)\n︸ ︷︷ ︸ =0\n+ ∑\nj\nψ̂′st(j, i) ŷs(j)\n− 1 kskt\n∑\nj\nm̂ts(j)\n︸ ︷︷ ︸ 0\n− 1 ks\n∑\nj\nψ̂′st(j, i) m̂ts(j) )\nm̂st(i)← kt ks ĉ′i + kt\n∑\nj\nψ̂′st(j, i) ŷs(j)− kt ks\n∑\nj\nψ̂′st(j, i) m̂ts(j)\nEquations (8) and (9) can be written in matrix notation as:\nŷs ← ( x̂s + 1 ks · ∑\nu∈N(s) m̂us\n)\nm̂st ← kt ks ĉ′st + ktψ̂ ′ᵀ st\n( ŷs − 1\nks m̂ts\n)\n(11)\n(12)\nAn alternative way to write the message updates is\nm̂st ← kt ks ĉ′st + ktψ̂ ′ᵀ st\n( x̂s + 1 ks · ∑\nu∈N(s)\\t m̂us\n) (13)\nIt is instructive to compare the above derived linearized update equations against the matrix formulations of the original BP update equations Eq. (1) and Eq. (2): by using the symbol for the Hadamard product10, those can be written compactly in matrix notation, as:\nys ← 1\nZs\n( xs\n⊙\nu∈N(s) mus\n) (14)\nmst ← 1 Zst ψᵀst ( xs\n⊙\nu∈N(s)\\t mus\n) (15)\n10The Hadamard product is defined by: Z=X Y ⇔ Z(i, j)=X(i, j) · Y (i, j).\nNotice that the potential ψst is represented by a ks × kt-dimensional “compatibility matrix” and that the transpose ψᵀst = ψts (see Fig. 6). This follows from the definition of a potential in a pairwise MRF and the resulting derivation of belief propagation (Yedidia, Freeman, and Weiss 2003). Also notice that we could reduce the amount of necessary calculation by first multiplying all incoming messages at a node, and then dividing through the message that a node sends to itself via a neighbor (we call this compensation “echo cancellation”). This approach is also called “message-passing with division” (Koller and Friedman 2009) (or “belief-update message passing”) and can be made precise by defining a component-wise division operator by: Z=X Y ⇔ Z(i, j)=X(i, j)/Y (i, j) where 0/0 = 0. Equation (15) can then be written more concisely as:\nmst ← 1 Zst ψᵀst ( ys mts ) (16)\nWe invite the reader to carefully compare Eqs. (11) and (12) with the original BP update equations Eqs. (11) and (13). Notice that the first term ktks ĉ ′ st in Eq. (12) vanishes in the case of doubly stochastic potentials (or more generally, potentials with equal column and row sums). For non-doubly stochastic potentials, this term captures the prior probabilities of node classes resulting from non-equivalent row or column sums."
    }, {
      "heading" : "Steady state messages",
      "text" : "From Lemma 12, we can derive a closed-form equation for the message in steady-state of belief propagation.\nLemma 13 (Steady state messages). After convergence of belief propagation, message propagation can be approximated in terms of the steady centered beliefs as:\nm̂st = kt ks ĉ′st + ktψ̂ ′ᵀ st ( ŷs − ψ̂ ′′ stŷt ) (17)\nProof Lemma 13. To increase the readability of this proof, we ignore the subscripts in ψst, cst, rst, and write instead ψ, c, r, respectively. We start by writing Eq. (9) for the messages in each of the two directions across the same edge s− t:\nm̂st(i)← kt ks ĉ(i)′ + kt\nks∑\nj=1\nψ̂′(j, i) ( ŷs(j)− 1\nks m̂ts(j)\n)\nm̂ts(j)← ks kt r̂(j)′′ + ks\nkt∑\ng=1\nψ̂′′(g, j) ( ŷt(g)− 1\nkt m̂st(g)\n)\nWe then simply combine both equations into one:\nm̂st(i)← kt ks ĉ(i)′ + kt\nks∑\nj=1\nψ̂′(j, i) ŷs(j)− kt ks\nks∑\nj=1\nψ̂′(j, i) ·\n(ks kt r̂(j)′′+ ks kt∑\ng=1\nψ̂′′(g, j) ŷt(g)− ks kt\nkt∑\ng=1\nψ̂′′(g, j) m̂st(g) )\nNow, if the equations converge, then m̂st(g) on both the left and right side of the equation need to be equivalent. We can, therefore, replace the update symbol with equality and group related terms together:\nm̂st(i)− ktks kskt\nks∑\nj=1\nψ̂′(j, i) kt∑\ng=1\nψ̂′′(g, j) m̂st(g) =\nkt ks ĉ(i)′ − ktks kskt\nks∑\nj=1\nψ̂′(j, i) r̂(j)′′ + kt\nks∑\nj=1\nψ̂′(j, i) ŷs(j)− kt ks\nks\nks∑\nj=1\nψ̂′(j, i) kt∑\ng=1\nψ̂′′(g, j) ŷt(g)\nWith Ikt as the kt-dimensional identity matrix, this can then be written in matrix notation as:\n(Ikt − ψ̂ ′ᵀ ψ̂ ′′ )m̂st = kt ks ĉ′ − ψ̂′ᵀr̂′′ + ktψ̂ ′ᵀ ŷs − ktψ̂ ′ᵀ ψ̂ ′′ ŷt\nRecall that ĉ′ = ψ̂ ′ᵀ 1ks and r̂ ′′ = ψ̂ ′′ 1kt .\n(Ikt − ψ̂ ′ᵀ ψ̂ ′′ )m̂st = ( kt ks ψ̂ ′ᵀ 1ks − ψ̂ ′ᵀ ψ̂ ′′ 1kt ) + ktψ̂ ′ᵀ ŷs − ktψ̂ ′ᵀ ψ̂ ′′ ŷt\n= ψ̂ ′ᵀ( kt\nks 1ks + ktŷs\n) − ψ̂′ᵀψ̂′′ ( 1kt + ktŷt )\nIf all entries of ψ̂ are appropriately small (so that the spectral radius ρ(ψ̂ ′ᵀ ψ̂ ′′ ) < 1), then the inverse of (Ikt − ψ̂ ′ᵀ ψ̂ ′′ ) exists. Thus, by further substituting ψ̂ ′ᵀ 4 := (Ikt − ψ̂ ′ᵀ ψ̂ ′′ )−1ψ̂ ′ᵀ , we can write:\nm̂st = ψ̂ ′ᵀ 4 ( kt ks 1ks + ktŷs ) − ψ̂′ᵀ4ψ̂ ′′( 1kt + ktŷt ) (18)\n= ψ̂ ′ᵀ 4 ( kt ks 1ks − ψ̂ ′′ 1kt ) + ktψ̂ ′ᵀ 4ŷs − ktψ̂ ′ᵀ 4ψ̂ ′′ ŷt\nBy further substituting ĥ ′ := ktks ψ̂ ′ᵀ 4 1ks − ψ̂ ′ᵀ 4ψ̂ ′′ 1kt , we get the following equation for the message updates Eq. (12) after convergence:\nm̂st = ĥ ′ + ktψ̂ ′ᵀ 4 ( ŷs − ψ̂ ′′ ŷt )\n(19)\nNext notice that ψ′ᵀ4 ≈ ψ′ᵀ, since Ikt |ψ̂ ′ᵀ ψ̂ ′′|, and therefore (Ikt − ψ̂ ′ᵀ ψ̂ ′′ )−1 ≈ Ikt . From this, we can now also approximate ĥ ′ ≈ ktks ( ψ̂ ′ᵀ 1ks ) − ψ̂′ᵀ ( ψ̂ ′′ 1kt ) = ktks ĉ ′ − ψ̂′ᵀr̂′′. Further ignoring the second term, we get ĥ′ ≈ ktks ĉ\n′. Plugging back into Eq. (19) finally gives us Eq. (17).\nAlso notice that we can alternatively write Eq. (18) as function of the uncentered beliefs, which results in a very intuitive equation:\nm̂st = ktψ̂ ′ᵀ 4 ( 1 ks 1ks + ŷs ) − ktψ̂ ′ᵀ 4ψ̂ ′′( 1 kt 1kt + ŷt )\n= ktψ̂ ′ᵀ 4 ( ys ) − ktψ̂ ′ᵀ 4ψ̂ ′′( yt ) = ktψ̂ ′ᵀ 4 ( ys − ψ̂ ′′ yt )\nTheorem 4: The actual linearization Finally, by using matrix notation, we can transform and write Eq. (17) for all nodes and edges together as one large equation system and get Theorem 4.\nProof Theorem 4. For steady-state, we can write Eq. (8) in vector form as:\nŷs = x̂s + 1\nks\n∑\nu∈N(s) m̂us\nBy permuting subscripts, we can also write Eq. (17) as\nm̂us = ks ku ĉ′us + ksψ̂ ᵀ us(ŷu − ψ̂ ′′ usŷs)\nCombining the last two equations, we get\nŷs = x̂s\n︸︷︷︸ 1st\n+ ∑\nu∈N(s)\nĉ′us ku\n︸ ︷︷ ︸ 2nd\n+ ∑\nu∈N(s) ψ̂ ′ᵀ usŷu\n︸ ︷︷ ︸ 3rd\n− ∑\nu∈N(s) ψ̂ ′ᵀ usψ̂ ′′ usŷs\n︸ ︷︷ ︸ 4th\n(20)\nBy using our combined new vectors and matrices ŷ, x̂, k, and ψ̂ (and analogously for the column-recentered residual matrix ψ̂ ′′ ), we can write Eq. (20) in matrix form as:\nŷ = X̂ + ψ̂ ′ᵀ k + ψ̂ ′ᵀ ŷ − ψ̂′ᵀψ̂′′ŷ\nFrom Corollary 8, we know ψ̂ ′ᵀ ij = ψ̂ ′′ ji. Therefore, from our construction we also have ψ̂ ′ᵀ = ψ̂ ′′ . We thus get\nŷ = x̂ + ψ̂ ′ᵀ k + ψ̂ ′ᵀ ŷ − ψ̂′ᵀ2ŷ (21)\nEquation (21) is now a straight-forward linear equation system that can be solved for ŷ. To finish the proof, first notice that each of our approximations become exact for lim →0+ . Second, notice that higher order deltas vanish and our equation simplify to ŷ = x̂ + ψ̂ ′ᵀ k + ψ̂ ′ᵀ ŷ. While the individual beliefs go to zero during this limit consideration, their relative sizes stay the same, and thus the Maximum Marginal for each node stays the same.\nProposition 5: Update equations and convergence Proof Proposition 5. From the Jacobi method for solving linear systems (Saad 2003) – also known as the Neumann series expansion of the matrix inverse – we know that the solution for y = (I−M)−1x can be calculated (under certain conditions) via the iterative update equation\ny(r+1) ← x + My(r)\nThese updates are known to converge for any choice of initial values for y(0), as long as M has a spectral radius ρ(M) < 1. The same convergence guarantees carry over to Eq. (5). We thus know that the update equation Eq. (5) converges if and only if the spectral radius of the matrix ψ̂ ′ᵀ − ψ̂′ᵀ2 is smaller than 1.\nC Special formulations In this section, we derive alternative formulations of Theorem 4 for increasingly specialized cases: Graphs with node and edge types (Appendix C), graphs with equal number of classes for all nodes (Appendix C), graphs with one single directed edge potential (Appendix C), and the special case described in prior work of one single symmetric, doubly stochastic potential (Appendix C).\nNode types and edge types with repeated potentials In many realistic scenarios, the number of edges is usually larger than the number of different edge types (or edge potentials). For example, assume a set Q of different node types.11 We then have a |Q|-partite network and each node with type q ∈ Q can be one of k(q) classes. Further assume that the couplings along an edge only depend on the types at both ends of the edge. Then there are maximal |Q|2 different row-recentered potentials irrespective of the size of the network, whereas the most general formulation of Theorem 4 would redundantly store in ψ̂ ′ one full potential for each edge (Recall that we have one rowrecentered potential for every edge direction, thus two for every edge type). In the following, we transform the update equations so that every different row-recentered edge potential appears only once in the equations. Notice that the ensuing formulation allows for more than one potential between any pair of node types.\nA complication in deriving a compact matrix formulation is that different types of nodes may have different numbers of classes. We address this issue by creating separate matrices that contain the beliefs of nodes with the same number of classes. Concretely, let N = {1, 2, . . . , n} be the set of all nodes, q(s) be the type of node s, k(q) be the number of classes for type q, andK = {k(1), k(2), . . . , k(|Q|)} be the set of numbers of classes across all nodes.12 LetNk ⊆ N denote the set of nodes with k ∈ K classes so that all nodes are partitioned into groups Nk1 , Nk2 , . . . , Nk|K| . Let nk = |Nk| denote the number of nodes with k classes. We assume a numbering of nodes such that Nk1 = {1, 2, . . . , nk1}, Nk2 = {nk1 + 1, nk1 + 2, . . . , nk1 + nk2}, and so on. Given this convention, each node s has a unique order o(s) within the set Nk(q(s)). For each k ∈ K, we create two nk × k matrices Ŷk and X̂k that contain the posterior and prior residual beliefs of all nodes with k classes.\nFor each potential ψ ∈ R`×k, we create two centered residual potentials ψ̂′ ∈ R`×k and ψ̂′′ᵀ = (ψ̂ᵀ)′ ∈ Rk×` that correspond to the two modulations across the two directions of an edge. For notational convenience, we treat them as two distinct potentials and ignore their common ancestry. For example,ψ12 ∈ R3×2 leads to ψ̂ ′ 12 ∈ R3×2 and ψ̂ ′ 21 = ψ̂ ′′ᵀ 12 ∈ R2×3. For each newly created row-recentered residual potential ψ̂ ′ ∈ R`×k, we create two new matrices: (i) the adjacency matrix W ψ̂ ′ ∈ Rn`×nk with W ψ̂ ′(o(s), o(t)) = 1 if node s with ` classes is connected to node t with k classes via an edge potential ψ̂ ′ ; and (ii) the diagonal in-degree matrix Din\nψ̂ ′ ∈ Rnk×nk with Din ψ̂ ′(o(t), o(t)) = d if there are d different nodes s that are\nconnected to t via an edge potential ψ̂ ′ (notice that ψ̂ ′\nmodulates along the direction s → t, therefore “in-degree” at node t with k classes).\n11Notice our use of vocabulary: the “type” of a node in a network is observed and known a priori (e.g., whether the node represents a user or a product), whereas the “class” of a node is the label that we are trying to learn (e.g., whether the user is male or female).\n12In a slight abuse of set notation, we allow here e.g. {1, 1, 2} to stand for a set {1, 2}.\nProposition 14 (Edge types). Let P̂ ′ bet the set of all row-recentered potentials, P̂ ′`×k ⊆ P ′ be the subset with dimensions ` × k, and Ŷk, X̂k, Wψ̂′ , D in ψ̂ ′ be the above defined partitioned matrices for all k ∈ K. For each ψ̂′ ∈ P̂ ′ let ψ̂′′ be the corresponding column-recentered potential. The update equation Eq. (5) can then be written as follows:\n∀k ∈ K : Ŷk ← X̂k + Ĉ ′ k∗ +\n∑\nψ̂ ′∈P̂′`×k,`∈K\n( Wᵀ ψ̂ ′Ŷ`ψ̂ ′ −Din ψ̂ ′Ŷkψ̂ ′′ᵀ ψ̂ ′)\n(22)\nwith\nĈ ′ k∗ :=\n∑\nψ̂ ′∈P̂′`×k,`∈K\n1 ` Wᵀ ψ̂ ′1n`1 ᵀ ` ψ̂ ′\nProof Proposition 14. We are going to derive Eq. (22) from Eq. (20). For convenience, we repeat here both equations:\nŶk = X̂k + Ĉ ′ k∗+\n∑\nψ̂ ′∈P̂′`×k,`∈K\n( Wᵀ ψ̂ ′Ŷ`ψ̂ ′−Din ψ̂ ′Ŷkψ̂ ′′ᵀ ψ̂ ′)\nŷs = x̂s\n︸︷︷︸ 1st\n+ ∑\nu∈N(s)\nĉ′us ku\n︸ ︷︷ ︸ 2nd\n+ ∑\nu∈N(s) ψ̂ ′ᵀ usŷu\n︸ ︷︷ ︸ 3rd\n− ∑\nu∈N(s) ψ̂ ′ᵀ usψ̂ ′′ usŷs\n︸ ︷︷ ︸ 4th\nWe need to show that any vector ŷᵀs for a node s with k classes is equivalent to the o(s)-th row of Ŷk, for which we are going to write Ŷk(o(s), :) from now on (recall that o(s) is the order of node s within Nk). We show the equivalence for each the 4 terms separately:\n1st term: x̂ᵀs = X̂k(o(s), :) by construction. 2nd term: For the following, recall that ĉ′us = ψ̂ ′ᵀ us1ku :\n(∑\nu∈N(s)\n1 ku ĉ′us )ᵀ = ∑\nu∈N(s)\n1\nku ĉ′ᵀus\n= ∑\nu∈N(s)\n1\nku 1ᵀkuψ̂ ′ us\n= ∑\nu∈N(s)\n1\nku W ᵀ ψ̂ ′ us (o(s), o(u))1ᵀkuψ̂ ′ us\n= ∑\nψ̂ ′∈P̂′`×k,`∈K\n1 ` Wᵀ ψ̂ ′(o(s), :)1n`1 ᵀ ` ψ̂ ′\n= Ĉ ′ k∗(o(s), :)\n3rd term: (∑\nu∈N(s) ψ̂ ′ᵀ usŷu\n)ᵀ = ∑\nu∈N(s) ŷᵀuψ̂ ′ us\n= ∑\nu∈N(s) Ŷku(o(u), :)ψ̂\n′ us\n= ∑\nu∈N(s) W ᵀ ψ̂ ′ us (o(s), o(u))Ŷku(o(u), :)ψ̂ ′ us\n= ∑\nψ̂ ′∈P̂′`×k,`∈K\nWᵀ ψ̂ ′(o(s), :)Ŷ`ψ̂\n′\n= ( ∑\nψ̂ ′∈P̂′`×k,`∈K\nWᵀ ψ̂ ′Ŷ`ψ̂\n′) (o(s), :)\n4th term: (∑\nu∈N(s) ψ̂ ′ᵀ usψ̂ ′′ usŷs\n)ᵀ = ∑\nu∈N(s) ŷᵀs ψ̂ ′′ᵀ usψ̂ ′ us\n= ∑\nu∈N(s) Ŷk(o(s), :)ψ̂\n′′ᵀ usψ̂ ′ us\n= ∑\nψ̂ ′∈P̂′`×k,`∈K\nDin ψ̂ ′(o(s), o(s)) Ŷk(o(s), :)ψ̂\n′′ᵀ ψ̂ ′\n= ( ∑\nψ̂ ′∈P̂′`×k,`∈K\nDin ψ̂ ′Ŷkψ̂\n′′ᵀ ψ̂ ′) (o(s), :)\nNodes with same number of classes k Proposition 14 simplifies considerably when all nodes have the same number of classes k:\nCorollary 15 (Same k). Let k be the number of classes for each node in the graph, P̂ ′ the set of row-recentered residual edge potentials (all with k× k dimensions), Ŷ and X̂ the n× k dimensional final and explicit belief matrices, and W\nψ̂ ′ and Din ψ̂ ′\nthe adjacency and in-degree matrices for each potential ψ̂ ∈ P̂ ′. The update equations can then be simplified to:\nŶ ← X̂ + Ĉ′∗ + ∑\nψ̂ ′∈P̂′\n( Wᵀ ψ̂ ′Ŷψ̂ ′ −Din ψ̂ ′Ŷψ̂ ′′ᵀ ψ̂ ′)\n(23)\nwith\nĈ ′ ∗ := 1\nk\n∑\nψ̂ ′∈P̂′\nWᵀ ψ̂ ′1n1 ᵀ kψ̂ ′\nAlso the convergence criterium and the closed-form solution allow very concise formulations. For this step we need to introduce two new notations: Let xj denote the j-th column of matrix X (i.e., X = {xij} = [x1 . . .xn]) and let X and Y be matrices of order m × n and p × q, respectively. First, the vectorization of a matrix X stacks its columns one underneath the other to form a single column vector:\nvec (X) =   x1 ... xn  \nSecond, the Kronecker product (⊗) of X and Y results in a mp× nq block matrix:\nX⊗Y =   x11Y x12Y . . . x1nY ... ... . . . ...\nxm1Y xm2Y . . . xmnY\n \nWith these notations, Corollary 6 now becomes\nProposition 16 (Convergence with same k). Update Eq. (23) converges if and only if the spectral radius ρ(M) < 1 for\nM := ∑\nψ̂ ′∈P̂′\n( Wᵀ ψ̂ ′ ⊗ ψ̂ ′ᵀ−Din ψ̂ ′ ⊗ (ψ̂′ᵀψ̂′′) )\nFurthermore, let ŷ := vec ( Ŷ ᵀ) , x̂ := vec ( X̂ ᵀ) , and ĉ′∗ := vec ( Ĉ ′ᵀ ∗ ) . The closed-form solution of Eq. (23) is given by:\nŷ = ( Ink −M )−1( x̂ + ĉ′∗ ) (24)\nNotice that Eq. (24) is a special case of Eq. (4) that factors out repeated edge potentials. This concise factorization with the Kronecker product is only possible if all nodes have the same number of classes k.\nProof Proposition 16. If all nodes have the same number of classes k then all final and explicit beliefs form single n×k matrices\nŶ and X̂. Furthermore, all potentials have k × k dimensions. Hence, Eq. (23) can be written as a single matrix equation:\nŶ = X̂ + Ĉ∗ + ∑\nψ̂ ′∈P′\n( Wᵀ ψ̂ ′Ŷψ̂ ′ −D ψ̂ ′Ŷψ̂ ′′ᵀ ψ̂ ′)\nŶ ᵀ = X̂ ᵀ + Ĉ ᵀ ∗ +\n∑\nψ̂ ′∈P′\n( ψ̂ ′ᵀ Ŷ ᵀ W ψ̂ ′ − ψ̂′ᵀψ̂′′ŶᵀD ψ̂ ′ )\nwith Ĉ ᵀ ∗ := 1 k ∑ ψ̂ ′∈P′ ψ̂ ′ᵀ W ψ̂ ′ . We used the transpose in order for the later vectorization vec to create vectors where the\ndifferent beliefs of a node are adjacent (otherwise vec(Ŷ) results in a vector where all beliefs in the same class from different nodes are adjacent). We next use Roth’s column lemma (Henderson and Searle 1981; Roth 1934) that states that\nvec (XYZ) = (Zᵀ⊗X) vec (Y) to rewrite this equation as\nŷ = x̂ + ĉ′∗ + ( ∑\nψ̂ ′∈P′\n( Wᵀ ψ̂ ′ ⊗ ψ̂ ′ᵀ −D ψ̂ ′ ⊗ (ψ̂′ᵀψ̂′′) )) ŷ\nfor ŷ = vec ( Ŷ ᵀ) , x̂ = vec ( X̂ ᵀ) , and ĉ′∗ = vec ( Ĉ ′ᵀ ∗ ) . Using the substitution\nM := ∑\nψ̂ ′∈P′\n( Wᵀ ψ̂ ′ ⊗ ψ̂ ′ᵀ−D ψ̂ ′ ⊗ (ψ̂′ᵀψ̂′′) )\nand reforming the equation leads to the closed-form solution:\nŷ = ( Ink −M )−1( x̂ + ĉ′∗ )\nwhich is defined if the spectral radius ρ of M is smaller than 1.\nOne single directed edge type Equation (23) simplifies further when we have just one single edge potential. In other words, we have a directed network and assume only one single type of edge whose meaning changes across the two directions (e.g., who follows some type of person on Twitter has a different meaning from who is followed by same type).\nCorollary 17 (One potential). Let k be the number of classes for each node in the graph, ψ the k × k-dimensional potential across an edge in the direction from source to target, Ŷ and X̂ the n × k dimensional final and explicit belief matrices, W the directed adjacency matrix, and Din (Dout) the in-degree (out-degree) matrices. Then the update equations simplify to:\nŶ ← X̂ + Ĉ′∗ + WᵀŶψ̂ ′ + WŶψ̂ ′′ᵀ −DinŶψ̂′′ᵀψ̂′ −DoutŶψ̂′ψ̂′′ᵀ (25) with\nĈ ′ ∗ := 1\nk\n( Wᵀ1n1 ᵀ kψ̂ ′ + W1n1 ᵀ kψ̂ ′′ᵀ)\nCorollary 18 (Convergence). Update Eq. (25) converges if and only if ρ(M) < 1 for\nM = Wᵀ ⊗ ψ̂′ᵀ + W ⊗ ψ̂′′ −Din ⊗ (ψ̂′ᵀψ̂′′)−Dout ⊗ (ψ̂′′ψ̂′ᵀ)\nOne symmetric, doubly stochastic potential Recent work (Gatterbauer et al. 2015) derived a linearization of BP for the special case of one single symmetric, doubly stochastic edge potential that is used throughout the network (Recall that for such a potential all residual row and column sums are 0, and that by multiplying it by the number of classes it will be centered around 1). We can recover this special case from Corollary 15 and Proposition 16 with a slightly updated notation.\nProposition 19 (One symmetric, doubly stochastic potential). If the MRF contains only one single edge type with a symmetric doubly stochastic potential ψ, then the update equations simplify to:\nŶ ← X̂ + WŶψ̂′ −DŶψ̂′2\nAt the same time, the closed form solution simplifies to:\nŷ = ( Ink −W ⊗ ψ̂ ′ + D⊗ ψ̂′2 )−1 x̂ (26)\nProof Proposition 19. First, notice that for any symmetric potential ψ ∈ Rk×k, ψ̂′ = ψ̂′′ = ψ̂/k, and hence Wᵀ ψ̂ ′Ŷψ̂ ′ + Wᵀ ψ̂ ′′Ŷψ̂ ′′ = (Wᵀ ψ̂ ′ +Wψ̂′)Ŷψ̂ ′ . Thus, its adjacency matrix becomes symmetric. Since we only have one potential, we also have only one adjacency matrix W. Furthermore, ψ̂ ′ᵀ = ψ̂ ′ and hence, ψ̂ ′′ᵀ ψ̂ ′ = ψ̂ ′2\n. Second, the constant term Ĉ ′ ∗ disappears for doubly stochastic potentials. This follows from the proof of Lemma 12 and the\nfact that in any doubly stochastic matrix ψ ∈ Rk×k, each column is centered around 1k . This allows us to simplify Eq. (23) to\nŶ = X̂ + WŶψ̂ ′ −DŶψ̂′2 (27)\nSimilarly, applying above assumptions to our closed-form solution Eq. (24) leads to:\nŷ = ( Ink −W ⊗ ψ̂ ′ + D⊗ ψ̂′2 )−1 x̂\nNotice that Eq. (27) and Eq. (26) are exactly the ones given by (Gatterbauer et al. 2015), except for slightly different notation. In particular, the authors choose to center the potentialψ around 1/k, which is possible in the case that all nodes have the same number of classes k (and thus all potentials are quadratic with k × k dimensions).13 We also chose here to formulate Eq. (26) as ŷ = vec(Ŷ ᵀ ) instead of vec(Ŷ) to keep the beliefs of same nodes adjacent in the resulting stacked vectors. Vectorizing instead the transpose, we get the exact original formulation:\nvec(Ŷ) = ( Ink − ψ̂ ′ ⊗W+ ψ̂′2 ⊗D )−1 vec(X̂) (28)\nNotice that in a slight abuse of notation, we used ψ̂ ′\nin Theorem 4 for the sparse ktot × ktot-square matrix, whereas we use it here for the single k × k recentered residual potential.\nD Illustrating examples Example 20 (Linearization). Consider the network Fig. 7a consisting of nodes N = {1, 2, 3}. Node 1 has three classes, whereas nodes 2 and 3 have two classes. We have two edges, e.g., the edge between nodes 1 and 2 with a 3× 2 potentialψ12. Fig. 7b illustrates Eq. (3). Notice that ĉ′∗ = ψ̂ ′ᵀ k with k = [ 13 , 1 3 , 1 3 , 1 2 , 1 2 , 1 2 , 1 2 ] ᵀ and ktot = 3 + 2 + 2. Further notice that the matrix ψ̂ ′ᵀ2\nis block-diagonal (entries represent the echo modulation that a node receives through all its neighbors). In the following, we write 〈·〉2 for the projection of a stacked vector on the entries for node 2, e.g., 〈ŷ〉2 = ŷ2:\n〈x̂〉2 = x̂2 〈ĉ′∗〉2 = 13ψ̂ ′ᵀ 1213 + 1 2ψ̂ ′ᵀ 3212\n〈ψ̂′ᵀŷ〉2 = ψ̂ ′ᵀ 12ŷ1 + ψ̂ ′ᵀ 32ŷ3\n〈ψ̂′ᵀ2ŷ〉2 = ( ψ̂ ′ᵀ 12ψ̂ ′ᵀ 21 + ψ̂ ′ᵀ 32ψ̂ ′ᵀ 23 ) ︸ ︷︷ ︸\nψ̂2∗\nŷ2\nThen, the single update equation could also be written as several simultaneous update equations:\nŷ1 ← x̂1 + 〈ĉ′∗〉1 + ψ̂ ′ᵀ 21ŷ2 − ψ̂1∗ŷ1 ŷ2 ← x̂2 + 〈ĉ′∗〉2 + ψ̂ ′ᵀ 12ŷ1 + ψ̂ ′ᵀ 32ŷ3 − ψ̂2∗ŷ2 ŷ3 ← x̂3 + 〈ĉ′∗〉3 + ψ̂ ′ᵀ 23ŷ1 − ψ̂3∗ŷ3\nExample 21 (Repeating potentials). We use Example 20 to also illustrate Proposition 14. Let ysj be the belief of node s in class j. We create two belief matrices ∀k ∈ K = {2, 3}: Ŷ2 = [ y21 y22y31 y32 ], and Ŷ3 = [ y11 y12 y13 ]. Thus, for example, the\n13“Row-recentering” and “column-recentering” as proposed in the present paper are more general forms of the centerings proposed in earlier work, which is necessary in order to deal with the general case of a non-quadratic and non-doubly stochastic potentials.\n1 2\n=\n3 b2 b3b1\n= 23 12\n(a)\n= + + −\n321\n1\n2 3  ̂03⇤\n ̂ 0 2⇤\n ̂ 0 1⇤\n ̂ 0| 12\n ̂ 0| 21\n ̂ 0| 23\n ̂ 0| 32\nb̂1\nb̂2\nb̂3\nŷ = x̂ + ĉ′∗ + (\nψ̂ ′ᵀ − ψ̂′ᵀ2\n) ŷ\n(b)\nE Weighted edges in MRFs The notion of “weights” on edges in MRFs is not defined and it is not immediately clear what an appropriate semantics would be. Here we give an a natural interpretation of edge weights in MRFs and derive a modification of the linearization to handle such weighted edges. We derive this interpretation starting from one single axiom:\nAxiom 22 (Edge weights). An edge with weight w ∈ N behaves identically as w parallel edges with the same potential but weight 1.\nFrom the original BP update equations Eq. (1) and Eq. (2), we see that two parallel edges carry the same messages, and that these two messages need to be multiplied to calculate the resulting messages and beliefs. It follows that these parallel edges behave identically to having one single edge with a new potential ψst ψst, i.e., the result of element-wise multiplying the entries of the original potential. More generally, an edge with a potential ψ and weight w is the same as an unweighted edge with a new potential ψw with entries entries ψw(j, i) = ψ(j, i)w.\nTo see how weights affect our linearized formulation in terms of residuals, recall that ψ(j, i) = 1 + ψ̂(j, i). Therefore, ψ(j, i)w = ( 1 + ψ̂(j, i) )w = 1 + wψ̂(j, i) + O(ψ̂(j, i)2). Under the assumption of small deviations from the center, we thus\nget: ψ̂w(j, i) = wψ̂(j, i). Hence, weights on edges simply multiply the residual potentials in our linearized formulation. In other words, weights on an edges simply scale the coupling strengths between two neighbors.\nIt follows that Proposition 14 can be generalized to weighted networks by using weighted adjacency matrices W ψ̂ ′ with\nelements A ψ̂ ′(o(s), o(t)) = w > 0 if the edge s → t with potential ψ̂′ and weight w exists, and W ψ̂ ′(o(s), o(t)) = 0\notherwise. In addition, each entry Din ψ̂ ′(o(t), o(t)) in the block-diagonal matrix Din ψ̂ ′ is now the sum of the squared weights of\nedges to neighbors that are connected to t via edge potential ψ̂, instead of just the number of neighbors (recall that the echo cancellation goes back and forth, and notice again that the potentials work along the direction s → t). After this modification, Proposition 14 can immediately be used for weighted graphs as well.\nExample 23 (Edge weights). We give here a small detailed example that shows the effects of weights for a potential whose entries are not really close to each other (i.e., the average entry is 1, however entries can diverge considerably from 1). We start with the potentialψ = [ 4 6 56 8 7 ]. By dividing all entries by 6, we get an equivalent potential that is centered around 1; and from this we get the residual and the row-recentered residual matrices:\nψ = 16 [ 4 6 5 6 8 7 ] , ψ̂ = 1 6 [−2 0 −1 0 2 1 ] , ψ̂ ′ = 118 [ 1 −1 0 1 −1 0 ]\nThe squared potential centered around 1 is then: ψ2 = 3\n113\n[ 42 62 52\n62 82 72\n] . And the residual and row-recentered residual matri-\nces: ψ̂2 ≈ [ 0.575 0.044 0.336 0.044 −0.699 −0.300 ] , ψ̂ ′ 2 ≈ [ 0.085 −0.091 0.006 0.121 −0.127 0.006 ]\nWe can now compare the potential we get by multiplying the residual by 2, or by squaring the original potential and then recentering:\n2ψ̂ ≈ [ 0.111 −0.111 0 0.111 −0.111 0 ] , ψ̂ ′ 2 ≈ [ 0.085 −0.091 0.006 0.121 −0.127 0.006 ]\nWe see that the overall direction is correct, but there are considerable differences (e.g., ≈ 30% relative difference for the first matrix entry: 0.111 vs. 0.085). We next bring each entry in the potential closer to the center. Concretely, we reduce the deviation by one order of magnitude:\nψ = 16 [ 5.8 6.0 5.9 6.0 6.2 6.1 ] , ψ̂ = 1 60 [−2 0 −1 0 2 1 ] , ψ̂ ′ = 1180 [ 1 −1 0 1 −1 0 ]\nNow both versions are very close (e.g., ≈ 2% relative difference for the first matrix entry: 0.0111 vs. 0.0109):\n2ψ̂ ≈ [ 0.0111 −0.0111 0 0.0111 −0.0111 0 ] , ψ̂ ′ 2 ≈ [ 0.0109 −0.0110 0.00005 0.0113 −0.0113 0.00005 ]\nF Illustrating examples Example 24 (Convergence). We illustrate the different convergence behaviors of BP and our formalism in a graph with several different potentials. The scenario is a variant of an example given by (Heskes 2002) of a 4-clique graph with weights on the edges (see Fig. 8a). The weights are used to entry-wise exponentiate the entries of the potential ψ = [ 4 11 4 ] before normalization: ψ(w)(i, j) = ( ψ(i, j) )w . For example, a weight 2 leads toψ(2) = [ 16 11 16 ], and a weight −3 leads toψ(−3) =[\n1 64 1 1 164\n] . The graph has 2 nodes A and B with prior beliefs: xA = xC = [0.8, 0.2]ᵀ.\nFigure 8b shows the beliefs for nodes A and C for various iterations of BP. The dashed lines show the maximum marginal (MM) distribution as determined by complete iteration. We see that BP has a somewhat erratic cyclic behavior and does not converge. Figure 8c shows a variant that uses damping (Koller and Friedman 2009, ch. 11.1), a method that is often used to make BP converge when it otherwise would not. Damping with 0.1 (meaning an updated message is a linear combination of 90% the prior message and 10% of newly calculated values) is able to dampen the the behavior, but convergence happens only after 1,000s of iterations, and even then the maximum marginal for node A is wrong (0.48 leads to choosing class 2, vs. 0.51 leads to choosing class 1). Furthermore, after replacing ψ in our example with [ 8 11 8 ], BP will not converge anymore for any damping factor and the fixed points of BP are unstable.\nA B\nC\nD\n(a) Network (b) Parameter space\nG Existing graph generators and hardness of node labeling There is a large body of work that proposes various realistic synthetic generative graph models. However, almost all of this work assumes unlabeled graphs. While one could use these existing graph generators to have realistic graphs, one cannot easily take a graph and then label the nodes according to some desired compatibility matrix. In fact, this problem is NP-hard.\nProposition 26 (Labeling with potentials). Given a graph G(V,E). Finding labels ` : v ∈ V → [k] so that the labels follow a given stochastic affinity matrixψ (where ψ(i, j) determines the average fraction of nodes of class j connected to a node of class i) is NP-hard.\nProof Proposition 26. Membership in NP follows from the fact that we can easily verify a solution by calculating the average neighbor-to-neighbor relations in a labeled graph. We use a simple reduction from the problem of Graph 3-colorability. Graph 3-colorability is the question of whether there exists a labeling function k : v → {1, 2, 3} such that k(u) 6= k(v) whenever (u, v) ∈ E for a given graph G = (V,E) and is well known to be NP-hard (Stockmeyer 1973). Assume now that we have a method that allows us to label any graph G(V,E) following the heterophily matrix ψ = 12 [ 0 1 1 1 0 1 1 1 0 ] , i.e., neighboring nodes never have the same label. It follows immediately that such a solution would also be solution to graph 3-colorability.\nWe thus need graph generators which generate both the graph topology (i.e., W) and the node labels (i.e., X) in the same process. We know of only two papers that have proposed graph generators that generate labeled data in the process: the early work by (Sen and Getoor 2007) and the very recent work by (Lee et al. 2015). Neither graph generator is available. In addition,\nneither of the papers gives a way to know the “ground truth” actual potential matrix that was used to label data (e.g., (Lee et al. 2015) suggests this as future work).\nWe therefore had to implement our own synthetic graph generator with the additional design decision that any potential matrix can be “planted” as exact graph property. This allows us to separate the concern between (1) how does our method work on graphs with certain properties, (2) what is the variation in properties of a given generative model. By planting exact properties (instead of expected properties) we can focus on question (1) only. The random graph generator is described in detail in (Gatterbauer 2016).\nReferences Gatterbauer, W. 2016. Semi-supervised learning with heterophily. (http://arxiv.org/abs/1412.3100CoRR abs/1412.3100). Henderson, H. V., and Searle, S. R. 1981. The vec-permutation matrix, the vec operator and Kronecker products: a review. Linear and Multilinear Algebra 9(4):271–288. Heskes, T. 2002. Stable fixed points of loopy belief propagation are local minima of the bethe free energy. In NIPS, 343–350. Lee, J.; Zaheer, M.; Günnemann, S.; and Smola, A. J. 2015. Preferential attachment in graphs with affinities. In AISTATS, 571–580. Roth, W. E. 1934. On direct product matrices. Bull. Amer. Math. Soc. 40:461–468. Saad, Y. 2003. Iterative methods for sparse linear systems. SIAM, 2nd ed edition. Sen, P., and Getoor, L. 2007. Link-based classification. Technical report, University of Maryland Technical Report CS-TR4858. Stockmeyer, L. 1973. Planar 3-colorability is polynomial complete. SIGACT News 5(3):19–25. Yedidia, J. S.; Freeman, W. T.; and Weiss, Y. 2003. Understanding belief propagation and its generalizations. In Exploring artificial intelligence in the new millennium. Morgan Kaufmann Publishers. 239–269."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Belief Propagation (BP) is a widely used approximation for exact probabilistic inference in graphical models, such as Markov Random Fields (MRFs). In graphs with cycles, however, no exact convergence guarantees for BP are known, in general. For the case when all edges in the MRF carry the same symmetric, doubly stochastic potential, recent works have proposed to approximate BP by linearizing the update equations around default values, which was shown to work well for the problem of node classification. The present paper generalizes all prior work and derives an approach that approximates loopy BP on any pairwise MRF with the problem of solving a linear equation system. This approach combines exact convergence guarantees and a fast matrix implementation with the ability to model heterogenous networks. Experiments on synthetic graphs with planted edge potentials show that the linearization has comparable labeling accuracy as BP for graphs with weak potentials, while speeding-up inference by orders of magnitude.",
    "creator" : "LaTeX with hyperref package"
  }
}