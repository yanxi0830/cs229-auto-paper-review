{
  "name" : "1106.1887.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning with Latent Factors in Time Series",
    "authors" : [ "Ali Jalali", "Sujay Sanghavi" ],
    "emails" : [ "alij@mail.utexas.edu", "sanghavi@mail.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 6.\n18 87\nv1 [\ncs .L\nG ]\n9 J\nun 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Linear stochastic dynamical systems are classic processes that are widely used to model time series data in a huge number of fields: financial data [8], biological networks of species [17] or genes [1], chemical reactions [11, 12], control systems with noise [27], etc. An important task in several of these domains is learning the model from data [25]; doing so is often the first step in both data interpretation, and making predictions of future values or the effect of perturbations. Often one is interested in learning the dependency structure [15]; i.e. identifying, for each variable, which set of other variables it directly interacts with. For stock market data, for example, this can reveal which other stocks most directly affect a given stock.\nWe consider model structure learning in a particularly challenging yet widely prevalent setting: where (the time series of) some state variables are observed, and others are unobserved/latent. We are interested in learning the dependency structure between the observed variables. However, the presence of latent time series, if not properly accounted for by the model learning procedure, will result in the appearance of spurious interactions between observed variables – two observed variables that interact with the same unobserved variable may now be reported to be interacting. This happens, for example, if one uses the classic maximum-likelihood estimator [9], and persists even if we have observations over a long time horizon.\nSuppose, for illustration, that we are interested in learning the dependency structure between the prices of a set of stocks via a linear stochastic model. Clearly, stock prices depend not only on each other, but are also jointly influenced by several variables that may not be part of our model, for example, currency markets, commodity prices etc.; these are latent time series. Their presence means that a naive structure learning algorithm (say max-likelihood) that takes as input only the stock prices, will report several spurious interactions; say, e.g. between all stocks that fluctuate with the price of oil.\nClearly there are several issues with regards to fundamental identifiability, and sample and computational complexity, that need to be defined and resolved. We do so below in the specific context of our model setting. We provide both theoretical characterization and guarantees of the problem, as well as numerical illustrations for both synthetic data and some real data extracted from stock market.\nThe rest of the paper is organized as follows: We define the problem in Section 2. Section 3 reveals our main result followed by a high-level outline of the proof in Section 4. The simulation results are included in Section 5."
    }, {
      "heading" : "2 Problem Setting and Main Idea",
      "text" : "This paper considers the problem of structure learning in linear stochastic dynamical systems, in a setting where only a subset of the time series are observed, and others are unobserved/latent. In particular, we consider a system with state vectors x(t) ∈ Rp and u(t) ∈ Rr, for t ∈ R+ and dynamics described by\nd\ndt\n[ x(t) u(t) ] = [ A∗ B∗ C∗ D∗ ]\n︸ ︷︷ ︸ A∗\n[ x(t) u(t) ] + d dt w(t), (1)\nwhere, w(t) ∈ Rp+r is an independent standard Brownian motion vector and A∗, B∗, C∗, D∗ are system parameters. We observe the process x(t) for some time horizon 0 ≤ t ≤ T , but not the process u(·). We are interested in learning the matrix A∗, which captures the interactions between the observed variables.\nWe will also be interested in a similar objective for an analogous discrete time system with parameter 0 < η < 2\nσmax(A∗) : [\nx(n+ 1) u(n+ 1)\n] − [ x(n) u(n) ] = η [ A∗ B∗ C∗ D∗ ] [ x(n) u(n) ] +w(n) n ∈ N0, (2)\nwhere, w(n) is a zero-mean Gaussian noise vector with covariance matrix ηI(p+r)×(p+r). Here η can be thought of as the sampling step; in particular notice that as η → 0, we recover model (1) from model (2). The upper bound on η ensures the stability of the discrete time system as required by our theorem. Intuitively, σmax(A∗) corresponds to the fastest convergence rate in the system and the upper bound on η corresponds to the Nyquist minimum sampling rate required for the reconstruction of the signal. As done in [2], our proofs will initially focus on the discrete case (2), and derive results for (1) afterwards.\n(A1) Stable Overall System: We only consider stable systems. In fact, we impose an assumption slightly stronger than the stability on the overall system. For the continuous system (1), we require D := −λmax(A ∗+A∗T 2 ) > 0. With slightly abuse of notation, for the discrete system (2), we require D := 1−Σ 2 max\nη > 0, where, Σmax := σmax(I + ηA∗). As a consequence of this assumption, by Lyapunov theory, the continuous system (1) has a unique stationary measure which is a zero-mean Gaussian distribution with positive definite (otherwise, it is not unique) covariance matrix Q∗ ∈ R(p+r)×(p+r) given by the solution of A∗Q∗+Q∗A∗T +I = 0. Similarly, for the discrete time system (2), we have A∗Q∗+Q∗A∗T + ηA∗Q∗A∗T + I = 0. This matrix Q∗ has the form Q∗ = [Q∗ R∗T ; R∗ P ∗], where, Q∗ and P ∗ are the steady-state covariance matrix of the observed and latent variables, respectively and R∗ is the steady-state cross-covariance between observed and latent variables. By stability, we have Cmin := Λmin(Q∗) > 0 and Dmax := Λmax(Q∗) < ∞. Identifiability: Clearly, the above objective of identifying A∗ is in general impossible without some additional assumptions on the model; in particular, several different choices of the overall model (including different choices of A∗) can result in the same effective model for the x(·) process. x(·) would then be statistically identical under both models, and correct identification would not be possible even over an infinite time horizon. Additionally, it would in general be impossible to achieve identification if the number of latent variables is comparable to or exceeds the number of observed variables. Thus, to make the problem\nwell-defined, we need to restrict (via appropriate assumptions) the set of models we are interested in."
    }, {
      "heading" : "2.1 Main Idea",
      "text" : "Consider the discrete-time system (2) in steady state and suppose, for a moment, that we ignored the fact that there may be latent time series; in this case, we would be back in the classical setting, for which the (population version of) the likelihood is\nL(A) = 1 2η2\nE [ ‖x(i+ 1)− x(i)− ηAx(i)‖22 ] .\nLemma 1. For x(·) generated by (2), the the optimum Â := maxA L(A) is given by\nÂ = A∗ +B∗R∗(Q∗)−1.\nThus, the optimal Â is a sum of the original A∗ (which we want to recover) and the matrix B∗R∗(Q∗)−1 that captures the spurious interactions obtained due to the latent time series. Notice that the matrix B∗R∗(Q∗)−1 has the rank at most equal to number r of latent time series. We will assume that the number of latent time series is smaller than the number of observed ones – i.e. r < p – and hence B∗R∗(Q∗)−1 is a low-rank matrix."
    }, {
      "heading" : "2.2 Identifiability",
      "text" : "Besides identifying the effect of the latent time series, the above realization also allows us to quantify the assumptions we would need to impose to ensure identifiability. In particular, the true model should be such that A∗ should be uniquely identifiable from B∗R∗(Q∗)−1. We choose to study models that have a local-global structure where (a) each of the observed time series xi(t) interacts with only a few other observed series, while (b) each of the latent series interacts with a (relatively) large number of observed series. In the stock market example, for instance, this would model the case where the latent series corresponds to macro-economic factors, like currencies or the price of oil, that affect a lot of stock prices.\nIn particular, let s be the maximum number of non-zero entries in each row, and d be the maximum number of non-zero entries in any column, of A∗. Parameters s and d control the number of other observed variables any given observed variable interacts with. Note that this means A∗ is a sparse matrix. Let L∗ := B∗R∗(Q∗)−1 and assume it has SVD L∗ = U∗Σ∗V ∗T , and recall that its rank is r. Then, following [7], L∗ is said to be (µ1, µ2)incoherent if µ1, µ2 > 0 are smallest real numbers satisfying\nmax i\n‖U∗T ei‖ ≤ √ µ2r\np , max i ‖V ∗Tei‖ ≤\n√ µ1r\np , ‖U∗V ∗T ‖∞ ≤\n√ r √ µ1µ2\np2 ,\nwhere, ei’s are standard basis vectors with proper length. Here ‖ · ‖ represents the Euclidean norm of the vector. Smaller values of µ mean the row and column spaces make larger angles with the standard bases, and hence the resulting matrix is more dense.\n(A2) Identifiability: We require that the s, d of the sparse matrix A∗ and the µ1, µ2 of the low-rank L∗, which has rank r, satisfy α := (√ µ1s+ √ µ2d )√ r p < 1."
    }, {
      "heading" : "2.3 Algorithm",
      "text" : "Recall that our task is to recover the matrix A∗ given observations of the x(·) process. We saw that the max-likelihood estimate (in the population case) was the sum of A∗ and a lowrank matrix; we subsequently assumed that A∗ is sparse. In light of this development, it is natural to use the max-likelihood as the loss function for the sum of a sparse and low-rank matrix, and separate appropriate regularizers for each of the components. Thus, for the continuous-time system observed up to time T , we propose solving\n(Â, L̂) = argmin A,L\n1\n2T\n∫ T\nt=0\n‖(A+ L)x(t)‖22 dt− 1\nT\n∫ T\nt=0\nx(t)T (A+ L)T dx(t) + λA‖A‖1 + λL‖L‖∗, (3)\nand for the discrete-time system given n samples, we propose solving\n(Â, L̂) = argmin A,L\n1\n2η2n\nn−1∑\ni=0\n‖x(i+ 1)− x(i)− η(A+ L)x(i)‖22 + λA‖A‖1 + λL‖L‖∗. (4)\nHere ‖ · ‖1 is the ℓ1 norm (a convex surrogate for sparsity), and ‖ · ‖∗ is the nuclear norm (i.e. sum of singluar values, a convex surrogate for low-rank). The optimum Â of (4) or (3) is our estimate of A∗, and our main result provides conditions under which we recover the support of A∗, as well as a bound on the error in values ‖Â − A∗‖∞ (maximum absolute value). We provide a bound on the error ‖L̂ − L∗‖2 (spectral norm) for the low-rank part. Notice that the discrete objective function goes to the continuous one as η → 0."
    }, {
      "heading" : "2.4 High-dimensional setting",
      "text" : "Note that when A∗ is a sparse matrix, the actual degrees of freedom between the observed variables is smaller than that evinced by the ambient dimension p. Indeed, we will be interested in recovering A∗ with a number of samples n that is potentially much smaller than p (for small s and d). In the special case when we are in steady state and L = 0 (i.e. λL large) the recovery of each row of A∗ is akin to a LASSO [24] problem (of sparse vector recovery from noisy linear measurements) with Q∗ being the covariance of the design matrix. We thus require Q∗ to satisfy incoherence conditions that are akin to those in LASSO (see e.g. [26] for the necessity of such conditions).\n(A3) Incoherence: To control the effect of the irrelevant (not latent) variables on the set of relevant variables, we require θ := 1 − maxk ‖Q∗Sc\nk Sk ( Q∗SkSk )−1 ‖∞,1 > 0, where, Sk is the support of the kth row of A∗ and Sck is the complement of that. The norm ‖ · ‖∞,1 is the maximum of the ℓ1-norm of the rows."
    }, {
      "heading" : "2.5 Related Work",
      "text" : "A comprehensive body of work is done on sparse and low-rank decomposition of matrices both for noiseless [6, 4, 7] and noisy [28, 3] cases, as well as on graphical model learning [22, 14, 20], all via convex optimization. Recently, Chandrasekaran et. al. [5] studied the problem of learning latent variables in graphical models via sparse and low-rank decomposition. In most of these results the emphasis is on the recovery of the low-rank component, where as in our work, we try to recover the sparse component. Moreover, in the graphical model learning works, the samples are assumed to be independent and certainly, we can not make that assumption.\nLearning graphical model for time series has been recently studied in [2]. However, they do not consider latent time series and hence just used LASSO to get a sparse graphical model. Our method can handle latent time series if they exist. Notice that if there is no latent time series, taking λL large enough will effectively result into solving just LASSO."
    }, {
      "heading" : "3 Main Results",
      "text" : "In this section, we present our main result for both Continuous and Discrete time systems. We start by imposing some assumptions on the regularizers and the sample complexity.\n(A4) Regularizers: We need to impose some assumptions on the regularizers to be able to guarantee our result. Let m = max ( 80√ D ‖B∗‖∞,1, √ ‖x(0)‖22 + ‖u(0)‖22 + ( √ η + 1)2 ) , be the constant capturing the effect of initial condition and latent variables through matrix B∗. We impose the following assumptions on the regularizers:\n• (A4-1) λA ≥ 16m(4−θ)θ√D\n√ log ( 4((s+2r)p+r2)\nδ\n)\nnη .\n• (A4-2) λLλA√p ≥ 1 (1−α)(4−θ)\n(( 3α(4−θ)√s\n4 + (8−θ)s θ\n)( θ √ p\n9s √ s + 1 ) + 12 ) .\n(A5) Sample Complexity: As we mentioned before, η plays the role of the step size when we sample the continuous time system, hence, we present our sample complexity in terms of ηn ≈ T . Our result is stated in terms of the probability of failure δ. We require two sets of constraints on the sample complexity T = ηn as follows:\n• (A5-1) Covariance matrix concentration: To get a consistent estimate of the covariance matrix, we require\nT = nη ≥ 3×10 6s3\nD2θ2C2min log\n( 4((s+ 2r)p+ r2)\nδ\n) .\nNote that our guarantee is for the worst possible sample complexity since L∗ has infinite (order p2) support size and we provide ℓ∞ error bound to get the sign support recovery. Clearly, one can provide much stronger guarantee if n can scale with p (as opposed to log(p) in our case); but here, our intension is to provide guarantees in low sample complexity regime.\n• (A5-2) Minimum Energy: To recover the exact sign support of A∗, we require\nT = nη ≥ 256m 2ν2(4− θ)2\nD2θ2A2min log\n( 4((s+ 2r)p+ r2)\nδ\n) ,\nwhere, ν = αθ 2Dmax + (8−θ)√s Cmin(4−θ)\nand Amin := min | (A∗)(k)j | . The reason behind this scaling is the way we show the exact sign support recovery. First, we show that ‖Â− A∗‖∞ ≤ νλA (See equation (11)). Second, we take T = ηn so large that λA is small enough to satisfy νλA < Amin and hence, Sign(Â) = Sign(A∗).\nWe identify the distance between the spaces L̂ and L∗ come from with a parameter\ndefined as ρ := min (\nα 4 , θαλA 5θαλA+16Dmax‖L∗‖2\n) . The following (unified) theorem states our\nmain result for both discrete and continuous time systems.\nTheorem 1. If assumptions (A1)-(A5) are satisfied, then with probability 1−δ, our algorithm outputs a pair (Â, L̂) satisfying\n• (a) Sign Support Recovery: Sign(Supp(Â)) = Sign(Supp(A∗)).\n• (b) Error Bounds: ‖Â−A∗‖∞ ≤ νλA and ‖L̂− L∗‖2 ≤ ρ1−5ρ‖L∗‖2.\nRemark 1: Our result has a significant difference with other results on sparse and low-rank decomposition. In a typical sparse and low-rank decomposition, the number of observations has the same order as number of variables, i.e., n = Θ(p), mainly because the typical goal is to recover the low-rank component (either the rank [5] or values [4]) accurately. In contrast, we are interested primarily on the sign support recovery of the sparse component. Our result shows that we can significantly reduce the number of observation to n = Θ(log(p)) for sparse component recovery.\nRemark 2: Notice that ν2 = Θ(s) and hence our result have the same sample complexity (up to a constant) as [2], even though we recover the sign support in the presence of latent variables. The difference (in constants) between our result and [2] is summarized in parameter m. As sample complexity grows, the parameter ρ goes to zero and hence we get a better estimation of L∗. Moreover, if the identifiability parameter α is small, we get a better estimation of L̂ (see expression of ρ and Assumption (A4-2)). If α is close to 1, then we can get a really bad estimate of L̂. Finally, choosing η to be small enough (increasing the sampling frequency), so that the overall system has small singular values, is enough to guarantee the sign support recovery.\nRemark 3: As a technical remark, we would like to mention the three key ingredients of the proof enabling us to get this low sample complexity. The first ingredient comes from our new set of optimality conditions inspired by [4]. This optimality conditions enable us to certify an approximation of L∗ while certifying the exact sign support of A∗. The second ingredient\ncomes from the bounds on the Schur complement of the perturbation of positive semidefinite matrices [23]. This result enables us to get a bound on the Schur complement of a perturbation of a positive semi-definite matrix of size p with only log(p) samples. The third and last key ingredient is taking advantage of the structure of L∗. Since L∗ = B∗R∗(Q∗)−1 and Q∗ is a symmetric positive semi-definite matrix, the row-space of L∗ is a subset of the row-space of Q∗. This property enables us to bound the projection of the matrices of the form FQ∗ (for some matrix F ) into the null space of L∗ with low sample complexity."
    }, {
      "heading" : "4 Proof Outline",
      "text" : "In this section, we first introduce some notations and definitions and then, provide a three step proof technique to prove the main theorem for the discrete time system. The proof of the continuous time system is done via a coupling argument in the appendix.\nSparse Matrix Notation: For any matrix A ∈ Rp×p, define Supp(A) = {(j, k) : A(k)j 6= 0}, and let Ω = {A ∈ Rp×p : Supp(A) ⊆ Supp(A∗)} be the subspace of matrices whose their support is the subset of the matrix A∗. The orthogonal projection to the subspace Ω can be defined such that (PΩ(M))(k)j is equal to M (k) j if (j, k) ∈ Supp(A∗) and zero otherwise. Denote the orthogonal complement space of Ω with Ωc. The orthogonal projection to this space can be defined as PΩc(M) = M − PΩ(M). Low-Rank Matrix Notation: For any matrix L ∈ Rp×p with singular value decomposition UΣV T , where, U, V ∈ Rp×r and Σ ∈ Rr×r, let Span(L) = {UXT + Y V T : X,Y ∈ Rp×r}. Let T (L) = {N ∈ Rp×p : Span(N) ⊆ Span(L)} denote the subspace of matrices whose column and row spans are included in those of L. The orthogonal projection to the subspace T (L) can be characterized by PT (N) = UUTN+NV V T −UUTNV V T . Denote the orthogonal complement space of T with T c with orthogonal projection PT c(·). We define a metric to measure the closeness of two tangent spaces T1 and T2 as follows\nρ (T1, T2) = max N∈Rp×p ‖PT1(N) − PT2(N)‖2 ‖N‖2 .\nAn immediate consequence of this definition is that ‖PT2(U1V T1 ) − U2V T2 ‖2 ≤ ρ (T1, T2), because otherwise, let N = vvT to get the contradiction, where, v is the eigenvector corresponding to the eigenvalue, exceeding ρ (T1, T2). Finally, let T = T (L∗) for the shorter notation.\nProof Technique: We outline the proof in three steps as follows:\nSTEP 1: Constructing a candidate primal optimal solution (Ã, L̃) with the desired sparsity pattern using the restricted support optimization problem, called oracle problem:\n(Ã, L̃)= arg min L:ρ(T (L),T )≤ρ A:PΩc (A)=0\n1\n2η2n\nn−1∑\ni=0\n‖x(i+ 1)−x(i)−η(A+ L)x(i)‖22+λA‖A‖1+λL‖L‖∗. (5)\nThis oracle is similar to the one used in [5]. It ensures that the right sparsity pattern is chosen for Ã and the tangent spaces L̃ and L∗ come from are close with parameter ρ. Note that this is a proof technique, not a method to construct the solution.\nSTEP 2: Writing down a set of sufficient (stationary) optimality conditions for (Ã, L̃) to be the unique solution of the (unrestricted) optimization problem (4):\nLemma 2. If Ω∩ T = {0}, then (Ã, L̃), the solution to the oracle problem (5), is the unique solution of the problem (4) if there exists a matrix Z̃ ∈ Rp×p such that (C1) PΩ(Z̃) = λASign ( Ã ) . (C2)\n∥∥∥PΩc(Z̃) ∥∥∥ ∞ < λA.\n(C3) ∥∥∥PT (Z̃)− λLU∗V ∗T ∥∥∥ 2 ≤ 4ρλL. (C4) ∥∥∥PT c(Z̃) ∥∥∥ 2 < (1− α)λL.\n(C5) − 1 ηn\nn∑\ni=1\n( x(i+ 1)− x(i)− η(Ã+ L̃)x(i) ) x(i)T + Z̃ = 0 =: Jn + Z̃.\nSTEP 3: Constructing a dual variable Z̃ that satisfies the sufficient optimality conditions stated in Lemma 2. For matrices M ∈ Ω and N ∈ T , let\nHM = M − PT (M) + PΩPT (M)−PT PΩPT (M) + . . . GN = N − PΩ(N) + PT PΩ(N)− PΩPT PΩ(N) + . . . .\nIt has been shown in [7] that if α < 1 then both infinite sums converge. Suppose we have the SVD decomposition L̃ = ŨΣ̃Ṽ T . Let\nZ̃ = HλASign(Ã) + GPT (λLŨ Ṽ T ) +∆,\nwhere, ∆ is a matrix such that (C5) is satisfied. As a result of this construction, we have PΩ(Z̃ − ∆) = λASign(Ã) and since PT (Z̃ − ∆) = PT (λLŨ Ṽ T ), by Lemma 3, we have PT̃ (Z̃ −∆) = λLŨ Ṽ T . Notice that PΩ(Jn) = PΩ(Z̃ −∆) and PT̃ (Jn) = PT̃ (Z̃ −∆) (and hence by Lemma 3, PT (Jn) = PT (Z̃ − ∆)); thus, PΩ(∆) = PT (∆) = 0. Now, we can establish PΩ(Z̃) = λASign(Ã) and PT (Z̃) = PT (λLŨ Ṽ T ) and consequently the conditions (C1) and (C3) in Lemma 2 are satisfied. It suffices to show that (C2) and (C4) are satisfied with high probability. This has been shown in Lemma 6."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In this section, we numerically illustrate the power of our theoretical result for both synthetic and real data collected from the stock market."
    }, {
      "heading" : "5.1 Synthetic Data",
      "text" : "We generate a sparse matrix A∗ with values taken from a standard normal distribution and generate B∗, C∗ and D∗ with i.i.d. entries drawn from a standard normal distribution scaled down by a factor r (to make sure that Amin is large enough statistically). To make the matrix A∗ negative definite (hence, stable), using Geršgorin disk theorem [10], we put a large-enough negative value on the diagonal. We simulate the discrete time system for different values of η = Cη 2σmax(A∗) for the stability parameter Cη ∈ (0, 1) and solve (4) using accelerated proximal gradient method [18]. Notice that the value of η by itself is not a good criterion to judge the performance of these types of algorithms because the rate of changes in the dynamic system depend on A∗. That is why, unlike [2], we use Cη for performance analysis. Thus, we plot our result with respect to the control parameter\nΘ = ηn\n2 σmax(A∗)\ns3 log ((s+ 2r)p+ r2) .\nWe pick the values of λA and λL by dividing the training data into chunks each having consecutive samples of the time series and do the cross validation over those chunks. Note\nthat this is different from the standard cross validation technique due to the dependency of samples.\nFigure 1(a) shows that as long as η < 2σmax(A∗) , the number of samples n remains almost constant. This can be seen (but not pointed out) in Figure 2 of [2] as well. This phenomenon matches our intuition that the amount of information we get from each sample remains the same as long as the Nyquist sampling criterion is met. Figure 1(b) indicates that the phase transition point of the control parameter is not sensitive to the value of r, which means the control parameter is the right scaling of the sample complexity."
    }, {
      "heading" : "5.2 Stock Market Data",
      "text" : "We take the end-of-the-day closing stock prices for 50 different companies in the period of May 17, 2010 - May 13, 2011 (255 business days). These companies (among them, Amazon, eBay, Pepsi, etc) are consumer goods companies traded either at NASDAQ or NYSE in USD. The data is collected from Google Finance website. Our goal is to observe the stock prices for a period of time and predict it for the entire days of the next month with small error.\nWe apply our algorithm to this data and try to learn the model using the data for n (consecutive) days and then compute the mean squared error in the prediction of the following month (25 business days). We randomly pick an starting day n0 between day 1 and day 255 − 25 − n. Then we learn the model using the data from the day n0 to the day n0 + n (total of n days). Then, we test our data on the consecutive 25 days. Finally, we average the error over 10 different starting points n0 for each value of n. We pick the regularizers by the semi-cross validation process explained in the previous section. The ratio n25 shows the ratio of training sample size to the testing sample size.\nFigure 2(b) shows the prediction error for both our method and pure LASSO [2] method as the train/test ratio increases. It can be seen that our method not only have better prediction, but also is more robust. Our algorithm requires only 3 months of the past data to give a robust estimation of the next month; in contrast with almost 6 months requirement of LASSO. However, the error of our algorithm is much smaller (by a factor of 6) than LASSO even in the steady state. Figure 2(a) shows the sparsity level for our model and the LASSO model. The number of latent variables our model finds varies from 8 − 12 for different train/test ratios. As Figure 2(a) illustrates, our estimated Â is order of magnitude sparser than the one estimated by LASSO."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "Proof. Ignoring the term ‖x(n+1)−x(n)‖22 which is independent of A, minimization of L(A) with this infinite sample size is equivalent to\nmin A E\n[ x(n)TATAx(n)− 2\nη (x(n + 1)− x(n))TAx(n)\n]\n= min A E\n[ trace ( Ax(n)x(n)TAT ) − 2 (A∗x(n) +B∗u(t))T Ax(n) ]\n= min A\ntrace ( AQ∗AT ) − 2trace ( A∗Q∗AT ) − 2trace ( B∗R∗AT )\n= min A\ntrace (( A− 2 ( A∗ +B∗R∗(Q∗)−1 ) ) Q∗AT ) .\nHere we ignored the term w(n) due to the fact that it is zero mean and independent of x(n) and u(n). This implies that the asympotatic optimizer of L(·) satisfies Â = A∗ + B∗R∗(Q∗)−1."
    }, {
      "heading" : "B Proof of Lemma 2",
      "text" : "General Notation: For a matrix X ∈ Ra×b, we use X(1), . . . , X(a) to denote rows, X1, . . . , Xb to denote columns and X (1) 1 , . . . , X (a) b to denote entries. Also, for the sets of indecies S1 ⊆ {1, · · ·, a} and S2 ⊆ {1, · · ·, b}, the matrix XS1S2 ∈ R|S1|×|S2| represents the sub-matrix of X consisting of the rows and columns corresponding to index sets S1 and S2.\nProof. Suppose Ã = Â +DA and L̃ = L̂ +DL for some matrices DA and DL. Let ZA = λASign(Ã) − F0 with PΩ(F0) = 0 and 〈F0, DA〉 = λA‖PΩc(DA)‖1. Let T̃ = T (L̃) with SVD decomposition L̃ = ŨΣ̃Ṽ T . Now, let ZL = λLU∗V ∗T + W1 − W0 with PT (W0) = 0 and ‖W0‖2 ≤ (1 − α)λL and 〈W0, DL〉 = (1 − α)λL‖PT c(DL)‖∗ and with PT c(W1) = 0 and ‖W1‖2 ≤ 4ρλL such that λLU∗V ∗T + W1 − W0 = λLŨ Ṽ T + W2 for some matrix W2 with PT̃ c(W2) = 0 and ‖W2‖2 < λL. By zero duality gap between dual norms, matrices F0 and W0 exist and the existence of matrices W1 and W2 is guaranteed by Lemma 3. Hence, ZA and ZL are subgradients of λA‖Ã‖1 and λL‖L̃‖∗. By convexity of the program, we have\nL(Â+ L̂) + λA‖Â‖1 + λL‖L̂‖∗ ≥ L(Ã + L̃) + λA‖Ã‖1 + λL‖L̃‖∗ + 〈 Z̃,DA +DL 〉 − 〈ZA, DA〉 − 〈ZL, DL〉 .\nIt suffices to show that 〈 Z̃,DA +DL 〉 − 〈ZA, DA〉 − 〈ZL, DL〉 ≥ 0 to conclude by the\noptimality of (Â, L̂) that the result holds. Notice that (C5) ensures that PΩ(Z̃) = λASign(Ã) and PT̃ (Z̃) = λLŨ Ṽ T . By Lemma 3, we conclude that PT (Z̃) = λLU∗V ∗T +W1. Thus, for some γ < 1, we have 〈 Z̃,DA +DL 〉 − 〈ZA, DA〉 − 〈ZL, DL〉\n= λA‖PΩc(DA)‖1 + (1− α)λL‖PT c(DL)‖∗ + 〈 Z̃,DA +DL 〉 − 〈 λASign(Ã),PΩ(DA) 〉 − 〈 λLU ∗V ∗T +W1,PT (DL) 〉 = λA‖PΩc(DA)‖1 + (1− α)λL‖PT c(DL)‖∗ + 〈 PΩc(Z̃),PΩc(DA) 〉 + 〈 PT c(Z̃),PT c(DL) 〉\n+ 〈 PT (Z̃)− λLU∗V ∗T −W1,PT (DL) 〉 + 〈 PΩ(Z̃)− λASign(Ã),PΩ(DA) 〉\n≥ (1− γ)λA‖PΩc(DA)‖1 + (1− γ)(1− α)λL‖PT c(DL)‖∗ ≥ 0.\nThis concludes the proof of the lemma.\nLemma 3. For any two matrices with SVD L∗ = U∗Σ∗V ∗T and L̃ = Ũ Σ̃Ṽ T and corresponding tanget spaces T and T̃ , if ρ(T , T̃ ) = ρ ≤ α4 , then, for any matrix W0 with PT (W0) = 0 and ‖W0‖2 ≤ (1− α)λL, there exist matrices W1 and W2 such that\nZ̃L := λLU ∗V ∗T +W1 −W0 = λLŨ Ṽ T +W2,\nwhere, PT̃ (W2) = 0 with ‖W2‖2 < λL and PT c(W1) = 0 with ‖W1‖2 ≤ 4ρλL.\nProof. Let W2 = −PT c(λLŨ Ṽ T )−W0 and W1 = PT (λLŨ Ṽ T )−λLU∗V ∗T . For this choice, the equality constraints hold. First, notice that ‖PT (M)‖2 ≤ 2 ‖M‖2 and hence,\n∥∥∥Z̃L ∥∥∥ 2 = ∥∥∥λLŨ Ṽ T − PT c(λLŨ Ṽ T )−W0 ∥∥∥ 2\n≤ ∥∥∥PT (λLŨ Ṽ T ) ∥∥∥ 2 + ‖W0‖2 ≤ 2 ∥∥∥λLŨ Ṽ T\n∥∥∥ 2 + ‖W0‖2 ≤ (3− α)λL.\nUsing this, we can bound both W1 and W2. For W2, we have\n‖W2‖2 ≤ ∥∥∥PT c(λLŨ Ṽ T ) ∥∥∥ 2 + ‖W0‖2\n= ∥∥∥PT c(λLŨ Ṽ T − λLU∗V ∗T −W1) ∥∥∥ 2 + ‖W0‖2 ≤ ∥∥∥λLŨ Ṽ T − λLU∗V ∗T −W1\n∥∥∥ 2 + ‖W0‖2\n= ∥∥∥PT̃ (Z̃L)− PT (Z̃L) ∥∥∥ 2 + ‖W0‖2 ≤ ρ ∥∥∥Z̃L\n∥∥∥ 2 + ‖W0‖2\n≤ ((3 − α)ρ+ (1− α)) λL < λL.\nNote that PT̃ (Z̃L) = λLŨ Ṽ T and hence, we can establish\n‖W1‖2 = ∥∥∥PT (λLŨ Ṽ T )− λLU∗V ∗T ∥∥∥ 2\n= ∥∥∥PT (Z̃L)− PT̃ (Z̃L) ∥∥∥ 2 + ∥∥∥λLŨ Ṽ T − λLU∗V ∗T ∥∥∥ 2 ≤ ρ ∥∥∥Z̃L\n∥∥∥ 2 + ρλL ≤ ((3− α)ρ+ ρ)λL.\nThis concludes the proof of the lemma."
    }, {
      "heading" : "C Auxiliary Optimality Lemmas",
      "text" : "Lemma 4 (Convex Optimality). If Â is a solution of (4) then there exists a matrix Ẑ ∈ Rp×p, called dual variable, such that Ẑ ∈ λA∂‖Â‖1 and Ẑ ∈ λL∂‖L̂‖∗ and\n− 1 ηn\nn∑\ni=1\n( x(i + 1)− x(i)− η(Â+ L̂)x(i) ) x(i)T + Ẑ = 0. (6)\nProof. The proof follows from the standard first order optimality argument.\nLemma 5. If α < 1 then Ω ∩ T = {0}.\nProof. The proof follows the same technique used in [7]. Trivially, {0} ∈ Ω ∩ T . Assume that there exists a non-zero matrix M ∈ Ω ∩ T . By idempotency of orthogonal projections, we have M = PΩ(M) = PT (PΩ(M)) and hence\n‖PT (PΩ(M))‖∞ = ∥∥U∗U∗TPΩ(M) + PΩ(M)V ∗V ∗ − U∗U∗TPΩ(M)V ∗V ∗T ∥∥ ∞\n≤ ∥∥U∗U∗TPΩ(M) ∥∥ ∞ + ∥∥(I − U∗U∗T )PΩ(M)V ∗V ∗T ∥∥ ∞ ≤ max i ∥∥U∗U∗Tei ∥∥max j ‖ejPΩ(M)‖\n+ ∥∥I − U∗U∗T ∥∥ 2 max j ‖PΩ(M)ej‖max i ∥∥V ∗V ∗Tei ∥∥\n≤ max i\n∥∥U∗U∗Tei ∥∥√d1 ‖PΩ(M)‖∞ + √ d2 ‖PΩ(M)‖∞ maxi ∥∥V ∗V ∗T ei ∥∥\n≤ α‖PΩ(M)‖∞ = α‖PT (PΩ(M)) ‖∞.\nHence, ‖M‖∞ = 0 or equivalently, M = 0. This is a contradiction.\nLemma 6. For constructed dual variable, conditions (C2) and (C4) are satisfied.\nProof. Let Q(n) = 1n ∑n i=1 x(i)x(i) T and R(n) = 1n ∑n i=1 u(i)x(i)\nT . Substituting x(i + 1) − x(i) = ηA∗x(i) + ηB∗u(i) + w(i) and L∗ = B∗R∗(Q∗)−1 in (C5), we equivalently get\n(Ã−A∗)Q(n)+(L̃− L∗)Q(n)−B∗ ( R(n) −R∗(Q∗)−1Q(n) )\n︸ ︷︷ ︸ Y (n)\n− 1 nη\nn∑\ni=1\nw(i)x(i)T\n︸ ︷︷ ︸ W (n)\n+Z̃=0. (7)\nWe can rewrite this equation as\nPΩc(L̃− L∗)Q(n) + (Ã−A∗ + PΩ(L̃− L∗))Q(n)− Y (n) −W (n) + Z̃ = 0. (8)\nLet us only focus on the kth row of the system of equation (7). We can break down (7) on the kth row into two sets of linear equations as follows:\n(Ã−A∗ + L̃− L∗)(k)Sk Q (n) SkSk = −(L̃− L ∗) (k) Sc k Q (n) Sc k Sk + Y (n) Sk +W (n) Sk − Z̃Sk (Ã−A∗ + L̃− L∗)(k)Sk Q (n) SkSck = −(L̃− L∗)(k)Sc k Q (n) Sc k Sc k + Y (n) Sc k +W (n) Sc k − Z̃Sc k .\n(9)\nBy Lemma 7, we have ∥∥∥∥(L̃− L∗) (k) Sc k Q (n) Sc k Sk ( Q (n) SkSk )−1∥∥∥∥ ∞ ≤ ( 1− θ 2 )∥∥∥PΩc(L̃ − L∗) ∥∥∥ ∞ ≤ ∥∥∥L̃− L∗ ∥∥∥ ∞ . Since Z̃ satisfies (C3), we have ∥∥∥PT (Ũ Ṽ T )− U∗V ∗T\n∥∥∥ 2 ≤ 4ρ. By the properties of the\noracle problem (closeness of spaces T and T̃ ), we have ∥∥∥L̃− L∗\n∥∥∥ 2 ≤ ∥∥∥PT̃ (L̃− L∗)− PT (L̃ − L∗) ∥∥∥ 2 + ∥∥PT̃ (L∗)− PT (L∗) ∥∥ 2 + ∥∥∥PT (L̃)− L∗ ∥∥∥ 2\n≤ ρ ∥∥∥L̃− L∗ ∥∥∥ 2 + ρ ‖L∗‖2 + ∥∥∥PT (Ũ Ṽ T )− U∗V ∗T ∥∥∥ 2 ∥∥∥L̃− L∗ ∥∥∥ 2 ≤ ρ ∥∥∥L̃− L∗\n∥∥∥ 2 + ρ ‖L∗‖2 + 4ρ ∥∥∥L̃− L∗ ∥∥∥ 2 .\nHence, ∥∥∥L̃− L∗ ∥∥∥ ∞ ≤ ∥∥∥L̃− L∗ ∥∥∥ 2 ≤ ρ 1− 5ρ ‖L ∗‖2 . (10)\nThus, from the first equation in (9) and Lemma 8, we get\n∥∥∥Ã−A∗ ∥∥∥ ∞ ≤ 2ρ 1− 5ρ ‖L ∗‖2 + √ s Cmin (∥∥∥Y (n) ∥∥∥ ∞ + ∥∥∥W (n) ∥∥∥ ∞ + λA )\n≤ 2ρ 1− 5ρ ‖L ∗‖2 + (8− θ)λA\n√ s\nCmin(4− θ)\n≤   αθ Dmax ( 1 + DmaxCmin ) + (8− θ) √ s Cmin(4− θ)  λA.\n(11)\nThe last inequality follows from Lemmas 9 and 10. Substituting (Ã− A∗ + L̃− L∗)(k)Sk from the first equation in the second in (9), we get\nZ̃Sc k = −(L̃− L∗)(k)Sc k Q (n) Sc k Sc k + Y (n) Sc k +W (n) Sc k\n− ( −(L̃− L∗)(k)Sc\nk Q (n) Sc k Sk + Y (n) Sk +W (n) Sk − Z̃Sk\n)( Q\n(n) SkSk\n)−1 Q\n(n) SkSck .\nTaking maximum absolute value from both sides, using Lemmas 7,9 and 10, we get\n∥∥∥PΩc(Z̃) ∥∥∥ ∞ ≤ max k ∥∥∥∥(L̃− L ∗) (k) Sc k ( Q (n) Sc k Sc k −Q(n)Sc k Sk ( Q (n) SkSk )−1 Q (n) SkSck )∥∥∥∥ ∞ + ∥∥∥Y (n) ∥∥∥ ∞ + ∥∥∥W (n) ∥∥∥ ∞\n+max k ∥∥∥∥Q (n) Sc k Sk ( Q (n) SkSk )−1∥∥∥∥ ∞,1 (∥∥∥Y (n) ∥∥∥ ∞ + ∥∥∥W (n) ∥∥∥ ∞ + λA )\n≤ max k\n∥∥∥∥(L̃− L ∗) (k) Sc k ( Q (n) Sc k Sc k −Q(n)Sc k Sk ( Q (n) SkSk )−1 Q (n) SkSck )∥∥∥∥ ∞ + θλA 4(4− θ) + θλA 4(4− θ)\n+ ( 1− θ\n2\n)( θλA\n4(4− θ) + θλA 4(4− θ) + λA )\n≤ 2ρ 1− 5ρ\n( 1 +\nDmax Cmin\n) Dmax ‖L∗‖2 + ( 1− θ\n4\n) λA ≤ ( 1− (1− α)θ\n4\n) λA.\nThe one to the last inequality follows from perturbation theory for the Schur complement of semi-definite matrices (See Lemma 13). The last inequality holds for our choice of ρ. Hence, condition (C2) is satisfied.\nTo show (C3) also holds, notice that from (8), we have ∥∥∥PT (Z̃)\n∥∥∥ 2 ≤ ∥∥∥PT c ( (Ã+ L̃−A∗ − L∗)Q(n) )∥∥∥ 2 + ∥∥∥Y (n) ∥∥∥ 2 + ∥∥∥W (n) ∥∥∥ 2\n≤ ∥∥∥PT c ( (Ã+ L̃−A∗ − L∗)Q(n) )∥∥∥ 2 + θλA √ p 2(4− θ) .\nThe last inequality follows from Lemmas 9 and 10 and the fact that Q(n) on the support is invertible for the given sample complexity due to Lemma 8.\nNext, notice that L∗ = B∗R∗(Q∗)−1 and hence the row-space of L∗ is the column/row space of Q∗ and consequently, for any matrix F ∈ T , we have PT c(FQ∗) = 0. Thus, we have ∥∥∥PT c ( (Ã+ L̃−A∗ − L∗)Q(n)\n)∥∥∥ 2\n= ∥∥∥PT c ( (Ã+ L̃−A∗ − L∗) ( Q(n) −Q∗ ))∥∥∥ 2 + ∥∥∥PT c ( Ã+ L̃−A∗ − L∗ ) Q∗ ∥∥∥ 2 ≤ ∥∥∥(Ã+ L̃−A∗ − L∗) ( Q(n) −Q∗\n)∥∥∥ 2 + ∥∥∥Ã+ L̃−A∗ − L∗ ∥∥∥ 2 ‖Q∗‖2 √ p\n≤ (√ s ∥∥∥Ã−A∗ ∥∥∥ ∞ + ∥∥∥L̃− L∗ ∥∥∥ 2 )(√ p ∥∥∥Q(n) −Q∗ ∥∥∥ ∞ +Dmax )√ p.\nFinally, from (11), (10) and Lemma 12, we get\n∥∥∥PT c(Z̃) ∥∥∥ 2 ≤ ( θCmin√p 9s √ s +Dmax )  3αθ √ s\n2Dmax ( 1 + DmaxCmin\n) + (8− θ)sCmin(4− θ)\n λA √ p+ θλA √ p\n2(4− θ)\n≤ θ(1− α)λL.\nHence, condition (C4) is also satisfied. This concludes the proof of the lemma."
    }, {
      "heading" : "D Concentration Results",
      "text" : "In this section we prove the concentration results used throughout the paper. Before, we state the results, we want to introduce some useful inequalities used to get the results. By the dynamics of the system, we have\n[ x(i) u(i) ] = (I + ηA∗)i [ x(0) u(0) ] + i−1∑\nl=0\n(I + ηA∗)i−l−1 w(l).\nLemma 7. For any S ⊆ {1, 2, . . . , p} with |S| ≤ s and sample complexity nη ≥ 3×106 s3 D2 θ2 C2min log ( 4((s+2r)p+r2) δ ) with high probability, we have ∥∥∥∥Q (n) ScS ( Q(n)SS\n)−1∥∥∥∥ ∞,1 ≤ 1− θ 2 ,\nprovided that ∥∥∥Q∗ScS (Q∗SS) −1 ∥∥∥ ∞,1 ≤ 1− θ.\nProof. Using Lemma 8, it can be shown (see Lemma A.1 in [2] for example) that ∥∥∥∥Q (n) ScS ( Q(n)SS )−1∥∥∥∥ ∞,1 ≤ ∥∥∥Q∗ScS (Q∗SS)−1 ∥∥∥ ∞,1\n+ 3|S|\n√ |S|\nCmin ∥∥∥Q(n) −Q∗ ∥∥∥ ∞ + 2|S|2\n√ |S|\nC2min\n∥∥∥Q(n) −Q∗ ∥∥∥ 2\n∞ .\nThe result follows from Lemma 11. This concludes the proof of the lemma.\nLemma 8. For any S ⊆ {1, 2, . . . , p} with |S| ≤ s and sample complexity nη ≥ 3×106 s3 D2 θ2 C2min log ( 4((s+2r)p+r2) δ ) with high probability, we have\nΛmin ( Q(n)SS ) ≥ Cmin\n2 .\nProof. By the Courant-Fischer variational representation [13], we have\nΛmin ( Q(n)SS ) ≥ Λmin (Q∗SS)− Λmax ( Q∗SS −Q(n)SS )\n≥ Cmin − √ s ∥∥∥Q∗ −Q(n) ∥∥∥ ∞ .\nThe last inequality follows from Lemma 11. This concludes the proof of the lemma.\nLemma 9. For λA ≥ 16(4−θ) √ ‖x(0)‖22+‖u(0)‖22+( √ η+1)2\nθ √ D\n√ log ( 4((s+2r)p+r2)\nδ\n)\nnη with high probability, we have ∥∥∥W(n)\n∥∥∥ ∞ ≤ θλA 4(4− θ) .\nProof. Let X(i) = [x(i) u(i)]T . According to the dynamics of the system, we have\nW(n) = 1 ηn\nn−1∑\ni=0\nw(i)X(0)T ( (I + ηA∗)i )T ︸ ︷︷ ︸\nE1(i)\n+ 1\nηn\nn−1∑\ni=1\nw(i) i−1∑\nl=0\nw(l)T ( (I + ηA∗)i−l−1 )T\n︸ ︷︷ ︸ E2(i)\n.\nWe bound these two terms separately. Notice that w(i) is distributed N (0, ηI) independent of x(0) and w(j)’s. Given x(0), we have\nw(i)jE1(i) (k) ∼ N ( 0, η ( E1(i) (k) )2) .\nBy stability assumption, we have ( E1(i) (k) )2 ≤ Σ2imax(‖x(0)‖22 + ‖u(0)‖22) and hence,\nVAR\n( 1\nηn\nn−1∑\ni=0\nw(i)jE1(i) (k) ) ≤ 1\nη2n2\nn−1∑\ni=0\nVAR ( w(i)jE1(i) (k) )\n≤ ‖x(0)‖ 2 2 + ‖u(0)‖22\nηn(1− Σ2max) .\nConsequently, by standard concentration of Gaussian random variables and union bound, we get\nP [∥∥∥∥∥ 1 ηn n−1∑\ni=0\nw(i)E1(i) ∥∥∥∥∥ ∞ ≥ ǫ ] ≤ p∑ j=1 p∑ k=1 P [∣∣∣∣∣ 1 ηn n−1∑ i=0 w(i)jE1(i) (k) ∣∣∣∣∣ ≥ ǫ ]\n≤ 2 exp ( − ǫ\n2(1− Σ2max) 2 (‖x(0)‖22 + ‖u(0)‖22) ηn+ log((s+ 2r)p+ r2)\n) .\nWith similar analysis, we get\nVAR\n( 1\nηn\nn−1∑\ni=0\nw(i)jE2(i) (k) ) ≤ 1\nη2n2\nn−1∑\ni=0\nVAR ( w(i)jE2(i) (k) )\n≤ (√ η + 1 )2\nηn(1− Σ2max) .\nThe last inequality follows from the concentration of χ2 random variables [16], in particular,\nP\n[ 1\nηn\nn−2∑\nl=0\n‖w(l)jE2(l)(k)‖22 ≥ ( 1 + √ η )2\n1− Σ2max\n] ≤ exp ( −1 2 ηn+ log((s+ 2r)p+ r2) ) .\nFinally, we get\nP [∥∥∥∥∥ 1 ηn n−1∑\ni=0\nw(i)E2(i) ∥∥∥∥∥ ∞ ≥ ǫ ] ≤ p∑ j=1 p∑ k=1 P [∣∣∣∣∣ 1 ηn n−1∑ i=0 w(i)jE2(i) (k) ∣∣∣∣∣ ≥ ǫ ]\n≤ 2 exp ( − ǫ\n2(1− Σ2max) 2 (√ η + 1 )2 ηn+ log((s+ 2r)p+ r2)\n) .\nThe result follows for ǫ = θλA8(4−θ) . This concludes the proof of the lemma.\nLemma 10. For λA ≥ 640(4−θ)‖B∗‖∞,1\n( Dmax Cmin +1 )\nθD\n√ log ( 4((s+2r)p+r2)\nδ\n)\nnη with high probability, we have ∥∥∥Y (n)\n∥∥∥ ∞ ≤ θλA 4(4− θ) .\nProof. We can establish\nY (n) = B∗ ( R(n) −R∗ )\n︸ ︷︷ ︸ +B∗R∗(Q∗)−1\n( Q∗ −Q(n) )\n︸ ︷︷ ︸ .\nWe bound these two terms separately. For the first term, we have ∥∥∥B∗ ( R∗ −R(n)\n)∥∥∥ ∞ ≤ ‖B∗‖∞,1 ∥∥∥Q∗ −Q(n) ∥∥∥ ∞ .\nFor the second term, we have ∥∥∥B∗R∗(Q∗)−1 ( Q∗ −Q(n)\n)∥∥∥ ∞ ≤ ∥∥B∗R∗(Q∗)−1 ∥∥ ∞,1 ∥∥∥Q∗ −Q(n) ∥∥∥ ∞\n≤ ‖B∗‖∞,1 σmax ( R∗(Q∗)−1 ) ∥∥∥Q∗ −Q(n) ∥∥∥ ∞ ≤ ‖B∗‖∞,1 Dmax Cmin ∥∥∥Q∗ −Q(n) ∥∥∥ ∞ .\nThe result follows from Lemma 12. This concludes the proof of the lemma.\nLemma 11. For sample complexity nη ≥ 3×106 s3 D2 θ2 C2min\nlog (\n4((s+2r)p+r2) δ\n) with high probability,\nwe have ∥∥∥Q∗ −Q(n) ∥∥∥ ∞ ≤ θ Cmin 9 s √ s .\nProof. Let X(i) = [x(i) u(i)]T . Let µ(i) = E [X(i)] (clearly, µ(∞) = 0). We have\nQ(n) −Q∗ = 1 n\nn−1∑\ni=0\nµ(i)µ(i)T\n︸ ︷︷ ︸ +\n1\nn\nn−1∑\ni=0\nE [ (X(i)− µ(i)) (X(i)− µ(i))T ] −Q∗\n︸ ︷︷ ︸ E1\n+ 1\nn\nn−1∑\ni=0\nE [ (X(i)− µ(i)) (X(i)− µ(i))T ] − 1\nn\nn−1∑\ni=0\n(X(i)− µ(i)) (X(i)− µ(i))T\n︸ ︷︷ ︸ E2\n.\nWe bound these three terms, separately. For the first term, we have ∥∥∥∥∥ 1 n n−1∑\ni=0\nµ(i)µ(i)T ∥∥∥∥∥ ∞ ≤ 1 n n−1∑ i=0 Σ2imax ( ‖x(0)‖22 + ‖u(0)‖ 2 2 )\n≤ ‖x(0)‖ 2 2 + ‖u(0)‖ 2 2\nn(1− Σ2max) .\nFor the second term, notice that by independency assumption on w, we have\n1\nn\nn−1∑\ni=0\nE [ (X(i)− µ(i)) (X(i)− µ(i))T ] = η\nn\nn−1∑\ni=0\ni−1∑\nl=0\n(I + ηA∗)2l\n= η\nn\nn−1∑\ni=0\n( I − (I + ηA∗)2i )( I − (I + ηA∗)2 )−1\n= η ( n− 1 n I − (I + ηA∗)2 + 1 n (I + ηA∗)2n )( I − (I + ηA∗)2 )−2 .\nOn the other hand, we have Q∗ = E [ lim i→∞ (X(i)− µ(i)) (X(i)− µ(i))T ] = lim i→∞ E [ (X(i)− µ(i)) (X(i)− µ(i))T ]\n= lim i→∞ η\ni−1∑\nl=0\n(I + ηA∗)2l\n= lim i→∞\nη ( I − (I + ηA∗)2i )( I − (I + ηA∗)2 )−1\n= η ( I − (I + ηA∗)2 )−1 .\nIn the above inequalities, we interchanged limit and expectation as a result of Gaussianity assumption and the stability of the system. Finally we get\n‖E1‖∞ ≤ η(1− Σ2nmax) n(1− Σ2max)2 ≤ η n(1− Σ2max)2 .\nTo bound the third term, notice that\n1\nn\nn−1∑\ni=0\n(X(i)− µ(i)) (X(i)− µ(i))T = n−1∑\nj=0\n(I+ηA∗)j   n− j n 1 n− j n−j−1∑\ni=0\nw(i)w(i)T\n︸ ︷︷ ︸ Vj\n  ( (I + ηA∗)j )T .\nBy Lemma 1 in [21], we have\nP [ ‖Vj − ηI‖∞ > n n− j ǫ ] ≤ 4 exp ( − ǫ 2n 3200η(n− j)n+ log ( (s+ 2r)p+ r2 )) .\nConsequently, we get\nP [ n− j n Σ2(n−j−1)max ‖Vj − ηI‖∞ > Σ2(n−j−1)max ǫ ] ≤ 4 exp ( − ǫ 2n 3200η(n− j)n+ log ( (s+ 2r)p+ r2 )) .\nThus, we conclude\nP [ ‖E2‖∞ >\n1\n1− Σ2max ǫ\n] ≤ 4 exp ( − ǫ 2\n3200η n+ log\n( (s+ 2r)p+ r2 )) .\nWe want this probability to be less than δ. Putting all thre parts together, we get\n∥∥∥Q∗ −Q(n) ∥∥∥ ∞ ≤ 1 1− Σ2max\n( η(1 − Σ2max)−1 + ‖x(0)‖22 + ‖u(0)‖ 2 2\nn + ǫ\n) . (12)\nFor nη ≥ 18 s √ s\nD θ Cmin\n( D−1 + ‖x(0)‖22 + ‖u(0)‖ 2 2 ) and ǫ = ηDθCmin 18 s √ s , the result follows, provide\nthat the probabilities go to zero, i.e.,\nnη ≥ 3× 10 6 s3\nD2 θ2 C2min log\n( 4((s+ 2r)p+ r2)\nδ\n) .\nFor large enough values of p, this lower bound dominates the earlier lower bound of nη, hence, we ignore that one. This concludes the proof of the lemma.\nLemma 12. For λA ≥ 640(4−θ)‖B∗‖∞,1\n( Dmax Cmin +1 )\nθD\n√ log ( 4((s+2r)p+r2)\nδ\n)\nnη , with high probability, we have ∥∥∥Q∗ −Q(n)\n∥∥∥ ∞\n≤ θλA 4(4− θ) ‖B∗‖∞,1 ( Dmax Cmin + 1 ) .\nProof. According to (12), the result follows if ǫ = θλA D 8(4−θ)‖B∗‖∞,1 ( Dmax Cmin +1 ) assuming p is large enough.\nLemma 13. For sample complexity nη ≥ 3×10 6(Dmax+2Cmin) D2(Dmax+Cmin) log ( 4((s+2r)p+r2) δ ) with high probability, we have ∥∥∥∥Q (n) Sc k Sc k −Q(n)Sc k Sk ( Q (n) SkSk )−1 Q (n) SkSck\n∥∥∥∥ 2︸ ︷︷ ︸\nS(n)\n≤ 2(1 + DmaxCmin )Dmax.\nProof. Since Q∗ and Q(n) are positive semi-definite matrices and ∥∥∥S(n)\n∥∥∥ 2 ≤ ∥∥∥S(n) − S∗ ∥∥∥ 2 + ‖S∗‖2\nThe result directly follows from Theorem in [23] for ǫ := ‖Q(n) − Q∗‖∞ = Dmax+Cmin4(Dmax+2Cmin) considering the fact that ‖S∗‖2 ≤ Dmax ( 1 + DmaxCmin ) ."
    }, {
      "heading" : "E Proof of the Continuous Time Theorem",
      "text" : "Proof. Denote X(t) = [x(t)u(t)]T and let\nQ̂ = 1 T\n∫ T\nt=0\nX(t)X(t)Tdt Ŵ = 1 T\n∫ T\nt=0\ndw(t)X(t)T .\nHaving the result for the discrete time system, it suffices (see proof of Theorem 1.1 in [2] for more details) to show that for a given continuous time system, there exists a discrete time system with Q(n) and W(n) such that almost surely,\nQ(n) −→ Q̂ W(n) −→ Ŵ , as n → ∞ for a fixed T = nη (and hence, η → 0). Let Q∗ be the matrix satisfying the continuous time Lyapunov stability equation A∗Q∗ + Q∗A∗T + I = 0 and Q∗(η) be the matrix satisfying the discrete time Lyapunov stability equation A∗Q∗(η) +Q∗(η)A∗T + ηA∗Q∗(η)A∗T + I = 0. It is easy to see that Q∗(η) → Q∗ as η → 0 by the uniqueness of the stationary distribution. Moreover, by Lemma 11, we know that Q(n) → Q∗(η) as n → ∞. Now, let the initial state of the discrete time system be\nX(i = 0) = (Q∗(η))1/2 (Q∗)−1/2 X(t = 0), and the noise w(i) = w(t = iη) − w(t = (i − 1)η). It can be easily checked that w(i) ∼ N (0, ηI) if the continuous time w(t) is a Brownian motion. Thus, x(i) and x(t) are coupled and the almost sure convergence, follows from the convergence of random walks to Brownian motions [19]. This concludes the proof of the theorem for continuous time systems."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "This paper considers the problem of learning, from samples, the depen-<lb>dency structure of a system of linear stochastic differential equations,<lb>when some of the variables are latent. In particular, we observe the time<lb>evolution of some variables, and never observe other variables; from this,<lb>we would like to find the dependency structure between the observed vari-<lb>ables – separating out the spurious interactions caused by the (marginal-<lb>izing out of the) latent variables’ time series. We develop a new method,<lb>based on convex optimization, to do so in the case when the number of<lb>latent variables is smaller than the number of observed ones. For the case<lb>when the dependency structure between the observed variables is sparse,<lb>we theoretically establish a high-dimensional scaling result for structure re-<lb>covery. We verify our theoretical result with both synthetic and real data<lb>(from the stock market).",
    "creator" : "LaTeX with hyperref package"
  }
}