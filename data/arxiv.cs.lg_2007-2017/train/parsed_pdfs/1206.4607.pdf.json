{
  "name" : "1206.4607.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Distributed Tree Kernels",
    "authors" : [ "Fabio Massimo Zanzotto", "Lorenzo Dell’Arciprete" ],
    "emails" : [ "FABIO.MASSIMO.ZANZOTTO@UNIROMA2.IT", "LORENZO.DELLARCIPRETE@GMAIL.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Trees are fundamental data structures used to represent very different objects such as proteins, HTML documents, or interpretations of natural language utterances. Thus, many areas – for example, biology (Vert, 2002; Hashimoto et al., 2008), computer security (Düssel et al., 2008), and natural language processing (Collins & Duffy, 2001; Gildea & Jurafsky, 2002; Pradhan et al., 2005; MacCartney et al., 2006) – fostered extensive research in methods for learning classifiers that leverage on these data structures.\nTree kernels (TK), firstly introduced in (Collins & Duffy, 2001) as specific convolution kernels (Haussler, 1999), are widely used to fully exploit tree structured data when learning classifiers. Different tree kernels modeling different feature spaces have been proposed (see (Shin et al., 2011) for a survey), but a primary research focus is the reduction of their execution time. Kernel machines compute many times TK functions during learning and classification. The original tree kernel algorithm (Collins & Duffy, 2001), that relies on dynamic programming techniques, has a quadratic time and space complexity with respect to the size of input trees. Execution time and space occupation are still affordable for parse trees of natural language sen-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ntences that hardly go beyond the hundreds of nodes (Rieck et al., 2010). But these tree kernels hardly scale to large training and application sets.\nAs worst-case complexity of TKs is hard to improve, the biggest effort has been devoted in controlling the average execution time of TK algorithms. Three directions have been mainly explored. The first direction is the exploitation of some specific characteristics of trees. For example, it is possible to demonstrate that the execution time of the original algorithm becomes linear in average for parse trees of natural language sentences (Moschitti, 2006). Yet, the tree kernel has still to be computed over the full underlying feature space and the space occupation is still quadratic. The second explored direction is the reduction of the underlying feature space of tree fragments to control the execution time by approximating the kernel function. The feature selection is done in the learning phase. Then, for the classification, either the selection is directly encoded in the kernel computation by selecting subtrees headed by specific node labels (Rieck et al., 2010) or the smaller selected space is made explicit (Pighin & Moschitti, 2010). In these cases, the beneficial effect is only during the classification and learning is overloaded with feature selection. The third direction exploits dynamic programming on the whole training and application sets of instances (Shin et al., 2011). Kernel functions are reformulated to be computed using partial kernel computations done for other pairs of trees. As any dynamic programming technique, this approach is transferring time complexity in space complexity.\nIn this paper, we propose the distributed tree kernels (DTK) (introduced in (Zanzotto & Dell’Arciprete, 2011)) as a novel method to reduce time and space complexity of tree kernels. The idea is to embed feature spaces of tree fragments in low-dimensional spaces, where the computation is approximated but its worst-case complexity is linear with respect to the dimension of the space. As a direct embedding is impractical, we propose a recursive algorithm with linear complexity to compute reduced vectors for trees in the low-dimensional space. We formally show that the dot product among reduced vectors approximates the original\ntree kernel when a vector composition function with specific ideal properties is used. We then propose two approximations of the ideal vector composition function and we study their properties. Finally, we empirically investigate the execution time of DTKs and how well these new kernels approximate original tree kernels. We show that DTKs are faster, correlate with tree kernels, and obtain a statistically similar performance in two natural language processing tasks.\nThe rest of the paper is organized as follows. Section 2 introduces the notation, the basic idea, and the expected properties for DTKs. Section 3 introduces the DTKs and proves their properties. Section 4 compares the complexity of DTKs with other tree kernels. Section 5 empirically investigates these new kernel algorithms. Finally, section 6 draws some conclusions."
    }, {
      "heading" : "2. Challenges for Distributed Tree Kernels",
      "text" : ""
    }, {
      "heading" : "2.1. Notation and Basic Idea",
      "text" : "Tree kernels (TK) (Collins & Duffy, 2001) have been proposed as efficient methods to implicitly compute dot products in feature spaces Rm of tree fragments. A direct computation in these high-dimensional spaces is impractical. Given two trees, T1 and T2 in T, tree kernels TK(T1, T2) perform weighted counts of the common subtrees τ . By construction, these counts are the dot products of the vectors representing the trees, ~T1 and ~T2 in Rm, i.e.:\nTK(T1, T2) = ~T1 · ~T2 (1)\nVectors ~T encode trees T as forests of active tree fragments F(T ). Each dimension ~τi of Rm corresponds to a tree fragment τi. The trivial weighting scheme assigns ωi = 1 to dimension ~τi if tree fragment τi is a subtree of the original tree T and ωi = 0 otherwise. Different weighting schemes are possible and used. Function I, that maps trees in T to vectors in Rm, is:\n~T = I(T ) = m∑ i=1 ωiI(τi) = m∑ i=1 ωi~τi (2)\nwhere I maps tree fragments into related vectors of the standard orthogonal basis of Rm, i.e., ~τi = I(τi).\nTo reduce computational complexity of tree kernels, we want to explore the possibility of embedding vectors ~T ∈ Rm into smaller vectors ;\nT ∈ Rd, with d m, to allow for an approximated but faster and explicit computation of these kernel functions. The direct embedding f : Rm → Rd is, in principle, possible with techniques like singular value decomposition or random indexing (Sahlgren, 2005), but it is again impractical due to the huge dimension of Rm.\nThen, our basic idea is to look for a function F̂ : T → Rd that directly maps trees T into small vectors ;\nT . We call these latter distributed trees (DT) in line with Distributed Representations (Hinton et al., 1986). The computation of similarity over distributed trees is the distributed tree kernel (DTK):\nDTK(T1, T2) , ; T1 · ; T2 = F̂ (T1) · F̂ (T2) (3)\nAs the two distributed trees are in the low dimensional space Rd, the dot product computation, having constant complexity, is extremely efficient. Computation of function F̂ is more expensive than the actual DTK, but it is done once for each tree and outside of the learning algorithms. We also propose a recursive algorithm with linear complexity to perform this computation."
    }, {
      "heading" : "2.2. Distributed Trees, Distributed Tree Fragments, and Expected Properties",
      "text" : "Distributed tree kernels are faster than tree kernels. We here examine the properties required of F̂ so that DTKs are also approximated computations of TKs, i.e.:\nDTK(T1, T2) ≈ TK(T1, T2) (4)\nTo derive these properties and describe function F̂ , we show the relations between the traditional function I : T→ Rm that maps trees into forests of tree fragments, in the tree fragments feature space, I : T → Rm that maps tree fragments into the standard orthogonal basis of Rm, the linear embedding function f : Rm → Rd that maps ~T into a smaller vector ;\nT = f(~T ), and our newly defined function F̂ .\nEquation 2 presents vectors ~T with respect to the standard orthonormal basis E = {~e1 . . . ~em} = {~τ1 . . . ~τm} of Rm. Then, according to this reading, we can rewrite the distributed tree ;\nT ∈ Rd as: ; T = f(~T ) = f( ∑ i ωi~τi) = ∑ i ωif(~τi) = ∑ i ωi ; τ i\nwhere each ; τ i represents tree fragment τi in the new space. The linear function f works as a sort of approximated basis transformation, mapping vectors ~τ of the standard basis E into approximated vectors ; τ that should represent them. As ; τi represents a single tree fragment τi, we call it a distributed tree fragment (DTF). The set of vectors Ẽ = {;τ 1 . . . ; τ m} should be the approximated orthonormal basis of Rm embedded in Rd . Then, these two properties should hold:\nProperty 1 (Nearly Unit Vectors) A distributed tree fragment ; τ representing a tree fragment τ is a nearly unit vector: 1− < ||;τ || < 1 +\nProperty 2 (Nearly Orthogonal Vectors) Given two different tree fragments τ1 and τ2, their distributed vectors are nearly orthogonal: if τ1 6= τ2, then | ; τ1 · ; τ2| <\nAs vectors ; τ ∈ Ẽ represent the basic tree fragments τ , the idea is that ; τ can be obtained directly from tree fragment\nτ by means of a function f̂(τ) = f(I(τ)) that composes f and I . Using this function to obtain distributed tree fragments ; τ , distributed trees ;\nT can be obtained as follows: ; T = F̂ (T ) = ∑\nτi∈F(T )\nωif̂(τi) (5)\nThis latter equation is presented with respect to the active tree fragments forest F(T ) of T , neglecting vectors where ωi = 0. It is easy to show that, if properties 1 and 2 hold for function f̂ , distributed tree kernels approximate tree kernels (see Equation 4)."
    }, {
      "heading" : "3. Computing Distributed Tree Fragments and Distributed Trees",
      "text" : "Johnson-Lindenstrauss Lemma (JLL) (Johnson & Lindenstrauss, 1984) guarantees that the embedding function f : Rm → Rd exists. It also points out the relation between the desired approximation of Property 2 (Nearly Orthogonal Vectors) and the required dimension d of the target space, for a certain value of dimension m. This relation affects how well DTKs approximate TKs (Equation 4).\nKnowing that f exists, we are presented with the following issues:\n• building a function f̂ that directly computes the distributed tree fragment ; τ i from tree fragment τi (Sec.\n3.2);\n• showing that distributed trees ;\nT = F̂ (T ) can be computed efficiently (Sec. 3.1).\nOnce the above issues are solved, we need to empirically show that Equation (4) is satisfied and that computing DTKs is more efficient than computing TKs. These latter points are discussed in the experimental section."
    }, {
      "heading" : "3.1. Computing Distributed Tree Fragments from Trees",
      "text" : "This section introduces function f̂ for distributed tree fragments and shows that, using an ideal vector composition function , the proposed function f̂(τi) satisfies properties 1 and 2."
    }, {
      "heading" : "3.1.1. REPRESENTING TREES AS VECTORS",
      "text" : "The basic blocks needed to represent trees are their nodes. We then start from a set N ⊂ Rd of nearly orthonormal\nvectors representing nodes. Each node n is mapped to a vector ; n ∈ N . To ensure that these basic vectors are statistically nearly orthonormal, their elements ( ; n)i are randomly drawn from a normal distribution N(0, 1) and they are normalized so that ||;n || = 1 (cf. the demonstration of Johnson-Lindenstrauss Lemma in (Dasgupta & Gupta, 1999)). Actual node vectors depend on the node labels, so that ; n1 = ; n2 if L(n1) = L(n2), where L(·) is the node label.\nTree structure can be univocally represented in a ‘flat’ format using a parenthetical notation. For example, the tree in Fig. 1 is represented by the sequence (A (B W1)(C (D W2)(E W3))). This notation corresponds to a depth-first visit of the tree, augmented with parentheses so that the tree structure is determined as well.\nReplacing the nodes with their corresponding vectors and introducing a vector composition function : Rd × Rd → Rd, the above formulation can be seen as a mathematical expression that defines a representative vector for a whole tree. The example tree would then be represented by vector ; τ = ( ; A ( ; B ; W1) ( ; C ( ; D ; W2) ( ; E ; W3))).\nThen, we formally define function f̂(τ), as follows:\nDefinition 1 Let τ be a tree andN the set of nearly orthogonal vectors for node labels. We recursively define f̂(τ) as:\n• f̂(n) = ;n if n is a terminal node, where ;n ∈ N\n• f̂(τ) = (;n f̂(τc1 . . . τck)) if n is the root of τ and τci are its children subtrees\n• f̂(τ1 . . . τk) = (f̂(τ1) f̂(τ2 . . . τk)) if τ1 . . . τk is a sequence of trees"
    }, {
      "heading" : "3.1.2. THE IDEAL VECTOR COMPOSITION FUNCTION",
      "text" : "We here introduce the ideal properties of the vector composition function , such that function f̂(τi) has the two desired properties.\nThe definition of the ideal composition function follows:\nDefinition 2 The ideal composition function is : Rd × Rd → Rd such that, given ;a , ; b , ; c , ; d ∈ N , a scalar s\nand a vector ;\nt obtained composing an arbitrary number of vectors inN by applying , the following properties hold:\n2.1 Non-commutativity with a very high degree k1\n2.2 Non-associativity: ; a (\n; b ;c ) 6= (;a ;\nb ) ;c 2.3 Bilinearity:\nI) ( ; a +\n; b ) ;c = ;a ;c + ; b ;c\nII) ; c (;a +\n; b ) = ; c ;a + ;c ; b\nIII) (s ; a )\n; b = ; a (s ; b ) = s( ; a ; b )\nApproximation Properties\n2.4 ||;a ; b || = ||;a || · || ; b ||\n2.5 |;a · ; t | < if ; t 6= ;a\n2.6 |;a ; b ·;c ; d | < if |;a ·;c | < or | ; b · ; d | <\nThe ideal function cannot exist. Property 2.5 can be only statistically valid and never formally as it opens to an infinite set of nearly orthogonal vectors. But, this function can be approximated (see Sec. 5.1)."
    }, {
      "heading" : "3.1.3. PROPERTIES OF DISTRIBUTED TREE FRAGMENTS",
      "text" : "Having defined the ideal basic composition function , we can now focus on the two properties needed to have DTFs as a nearly orthonormal basis of Rm embedded in Rd, i.e., Property 1 and Property 2.\nFor property 1 (Nearly Unit Vectors), we need the following lemma:\nLemma 1 Given tree τ , vector f̂(τ) has norm equal to 1.\nThis lemma can be easily proven using property 2.4 and knowing that vectors in N are versors.\nFor property 2 (Nearly Orthogonal Vectors), we first need to observe that, due to properties 2.1 and 2.2, a tree τ generates a unique sequence of application of function in f̂(τ) representing its structure. We can now address the following lemma:\nLemma 2 Given two different trees τa and τb, the corresponding DTFs are nearly orthogonal: |f̂(τa) · f̂(τb)| < .\nProof The proof is done by induction on the structure of τa and τb.\nBasic step 1We assume the degree of commutativity k as the lowest number such that is non-commutative, i.e., ;a ; b 6= ;\nb ;a , and for any j < k, ; a c1 . . . cj ; b 6= ; b c1 . . . cj ; a\nLet τa be the single node a. Two cases are possible: τb is the single node b 6= a. Then, by the properties of vectors in N , |f̂(τa) · f̂(τb)| = | ; a · ;\nb | < ; Otherwise, by Property 2.5, |f̂(τa) · f̂(τb)| = | ; a · f̂(τb)| < .\nInduction step\nCase 1 Let τa be a tree with root production a→ a1 . . . ak and τb be a tree with root production b → b1 . . . bh. The expected property becomes |f̂(τa) · f̂(τb)| = |( ; a f̂(τa1 . . . τak)) · ( ;\nb f̂(τb1 . . . τbh))| < . We have two cases: If a 6= b, |;a · ;\nb | < . Then, |f̂(τa) · f̂(τb)| < by Property 2.6. Else if a = b, then τa1 . . . τak 6= τb1 . . . τbh as τa 6= τb. Then, as |f̂(τa1 . . . τak) · f̂(τb1 . . . τbh)| < is true by inductive hypothesis, |f̂(τa) · f̂(τb)| < by Property 2.6.\nCase 2 Let τa be a tree with root production a→ a1 . . . ak and τb = τb1 . . . τbh be a sequence of trees. The expected property becomes |f̂(τa) · f̂(τb)| = |( ; a f̂(τa1 . . . τak)) · (f̂(τb1) f̂(τb2 . . . τbh))| < . Since | ; a · f̂(τb1)| < is true by inductive hypothesis, |f̂(τa) · f̂(τb)| < by Property 2.6.\nCase 3 Let τa = τa1 . . . τak and τb = τb1 . . . τbh be two sequences of trees. The expected property becomes |f̂(τa) · f̂(τb)| = |(f̂(τa1) f̂(τa2 . . . τak)) · (f̂(τb1) f̂(τb2 . . . τbh))| < . We have two cases: If τa1 6= τb1 , |f̂(τa) · f̂(τb)| < by inductive hypothesis. Then, |f̂(τa) · f̂(τb)| < by Property 2.6. Else, if τa1 = τb1 , then τa2 . . . τak 6= τb2 . . . τbh as τa 6= τb. Then, as |f̂(τa2 . . . τak) · f̂(τb2 . . . τbh)| < is true by inductive hypothesis, |f̂(τa) · f̂(τb)| < by Property 2.6."
    }, {
      "heading" : "3.2. Recursive Algorithm for Distributed Trees",
      "text" : "This section discusses how to efficiently compute DTs. We focus on the space of tree fragments implicitly defined in (Collins & Duffy, 2001). This feature space refers to subtrees as any subgraph which includes more than one node, with the restriction that entire (not partial) rule productions must be included. We want to show that the related distributed trees can be recursively computed using a dynamic programming algorithm without enumerating the subtrees. We first define the recursive function and then we show that it exactly computes DTs."
    }, {
      "heading" : "3.2.1. RECURSIVE FUNCTION",
      "text" : "The structural recursive formulation for the computation of distributed trees ; T is the following:\n; T = ∑\nn∈N(T )\ns(n) (6)\nwhere N(T ) is the node set of tree T and s(n) represents the sum of distributed vectors for the subtrees of T rooted in node n. Function s(n) is recursively defined as follows:\n• s(n) = ~0 if n is a terminal node. • s(n) = ;n (;c1 + √ λs(c1)) . . . ( ; cm + √ λs(cm))\nif n is a node with children c1 . . . cm.\nAs for the classic TK, the decay factor λ decreases the weight of large tree fragments in the final kernel value. With dynamic programming, the time complexity of this function is linear O(|N(T )|) and the space complexity is d (where d is the size of the vectors in Rd)."
    }, {
      "heading" : "3.2.2. THE RECURSIVE FUNCTION COMPUTES DISTRIBUTED TREES",
      "text" : "The overall theorem we need is the following.\nTheorem 3 Given the ideal vector composition function , the equivalence between equation (5) and equation (6) holds, i.e.:\n; T = ∑\nn∈N(T )\ns(n) = ∑\nτi∈F(T )\nωif̂(τi)\nAccording to (Collins & Duffy, 2001), the contribution of tree fragment τ to the TK is λ|τ |−1, where |τ | is the number of nodes in τ . Thus, we consider ωi = √ λ|τi|−1. We demonstrate Theorem 3 by showing that s(n) computes the weighted sum of vectors for the subtrees rooted in n (see Theorem 5).\nDefinition 3 Let n be a node of tree T . We define R(n) = {τ |τ is a subtree of T rooted in n}\nWe need to introduce a simple lemma, whose proof is trivial.\nLemma 4 Let τ be a tree with root node n. Let c1, . . . , cm be the children of n. Then R(n) is the set of all trees τ ′ = (n, τ1, ..., τm) such that τi ∈ R(ci) ∪ {ci}.\nNow we can show that function s(n) computes exactly the weighted sum of the distributed tree fragments for all the subtrees rooted in n.\nTheorem 5 Let n be a node of tree T . Then s(n) =∑ τ∈R(n) √ λ|τ |−1f̂(τ).\nProof The theorem is proved by structural induction.\nBasis. Let n be a terminal node. Then we have R (n) = ∅. Thus, by its definition, s(n) = ~0 = ∑ τ∈R(n) √ λ|τ |−1f̂(τ).\nStep. Let n be a node with children c1, . . . , cm. The inductive hypothesis is then s(ci) = ∑ τ∈R(ci) √ λ|τ |−1f̂(τ). Applying the inductive hypothesis, the definition of s(n) and the property 2.3, we have\ns(n) = ; n ( ; c1 + √ λs(c1) ) . . . ( ; cm + √ λs(cm) ) = ; n\n;c1 + ∑ τ1∈R(c1) √ λ|τ1|f̂(τ1)  . . . ;cm + ∑ τm∈R(cm) √ λ|τm|f̂(τm)\n = ; n\n∑ τ1∈T1 √ λ|τ1|f̂(τ1) . . . ∑ τm∈Tm √ λ|τm|f̂(τm)\n= ∑\n(n,τ1,...,τm)∈{n}×T1×...×Tm\n√ λ|τ1|+...+|τm| ; n f̂(τ1)\n. . . f̂(τm)\nwhere Ti is the setR(ci)∪{ci}. Thus, by means of Lemma 4 and the definition of f̂ , we can conclude that s(n) =∑ τ∈R(n) √ λ|τ |−1f̂(τ)."
    }, {
      "heading" : "4. Comparative Analysis of Computational Complexity",
      "text" : "DTKs have an attractive constant computational complexity. We here compare their complexity with respect to the traditional tree kernels (TK) (Collins & Duffy, 2001), the fast tree kernels (FTK) (Moschitti, 2006), the fast tree kernels plus feature selection (FTK+FS) (Pighin & Moschitti, 2010), and the approximate tree kernels (ATK) (Rieck et al., 2010). We discussed basic features of these kernels in the introduction.\nTable 4 reports time and space complexity of the kernels in learning and in classification. DTK is clearly competitive with respect to other methods, since both complexities are constant, according to the size d of the reduced feature space. In these two phases, kernels are applied many times by the learning algorithms. Then, a constant complexity is extremely important. Clearly, there is a trade-off between the chosen d and the average size of trees n. A comparison among execution times is done applying these algorithms to actual trees (see Section 5.3)."
    }, {
      "heading" : "5. Empirical Analysis and Experimental Evaluation",
      "text" : "In this section we propose two approximations of the ideal composition function , we investigate on their appropri-\nateness with respect to the ideal properties, we evaluate whether these concrete basic composition functions yield to effective DTKs, and, finally, we evaluate the computation efficiency by comparing average computational execution times of TKs and DTKs. For the following experiments, we focus on a reduced space Rd with d = 8192."
    }, {
      "heading" : "5.1. Approximating Ideal Basic Composition Function",
      "text" : ""
    }, {
      "heading" : "5.1.1. CONCRETE COMPOSITION FUNCTIONS",
      "text" : "We consider two possible approximations for the ideal composition function : the shuffled γ-product and shuffled circular convolution . These functions are defined as follows:\n; a\n; b = γ · p1( ; a )⊗ p2( ; b )\n; a\n; b = p1( ; a ) p2( ; b )\nwhere: ⊗ is the element-wise product between vectors and is the circular convolution (as for distributed representations in (Plate, 1995)) between vectors; p1 and p2 are two different permutations of the vector elements; and γ is a normalization scalar parameter, computed as the average norm of the element-wise product of two vectors."
    }, {
      "heading" : "5.1.2. EMPIRICAL EVALUATIONS OF PROPERTIES",
      "text" : "Properties 2.1, 2.2, and 2.3 hold by construction. The two permutation functions, p1 and p2, guarantee Prop. 2.1, for a high degree k, and Prop. 2.2. Property 2.3 is inherited from element-wise product ⊗ and circular convolution .\nProperties 2.4, 2.5 and 2.6 can only be approximated. Thus, we performed tests to evaluate the appropriateness of the two considered functions.\nProperty 2.4 approximately holds for since approximate norm preservation already holds for circular convolution, whereas uses factor γ to preserve norm. We empirically evaluated this property. Figure 2(a) shows the average norm for the composition of an increasing number of basic vectors (i.e. vectors with unitary norm) with the two basic composition functions. Function behaves much better\nthan .\nProperties 2.5 and 2.6 were tested by measuring similarities between some combinations of vectors. The first experiment compared a single vector ; a to a combination ;\nt of several other vectors, as in property 2.5. Both functions resulted in average similarities below 1%, independently of the number of vectors in ;\nt , satisfying property 2.5. To test property 2.6 we compared two compositions of vectors ; a ; t and ; b ;\nt , where all the vectors are in common except for the first one. The average similarity fluctuates around 0, with performing better than ; this is mostly notable observing that the variance grows with the number of vectors in ;\nt as shown in Fig. 2(b). A similar test was performed, with all the vectors in common except for the last one, yielding to similar results.\nIn light of these results, seems to be a better choice than , although it should be noted that, for vectors of dimension d, is computed in O(d) time, while takes O(d log d) time."
    }, {
      "heading" : "5.2. Evaluating Distributed Tree Kernels: Direct and Task-based Comparison",
      "text" : "In this section, we evaluate whether DTKs with the two concrete composition functions, DTK and DTK , approximate the original TK (as in Equation 4).We perform two sets of experiments: (1) a direct comparison where we directly investigate the correlation between DTK and TK values; and, (2) a task based comparison where we compare the performance of DTK against that of TK on two natural language processing tasks, i.e., question classification (QC) and textual entailment recognition (RTE)."
    }, {
      "heading" : "5.2.1. EXPERIMENTAL SET-UP",
      "text" : "For the experiments, we used standard datasets for the two NLP tasks of QC and RTE.\nFor QC, we used a standard question classification training and test set2, where the test set are the 500 TREC 2001 test questions. To measure the task performance, we used a question multi-classifier by combining n binary SVMs according to the ONE-vs-ALL scheme, where the final output class is the one associated with the most probable prediction.\nFor RTE we considered the corpora ranging from the first challenge to the fifth (Dagan et al., 2006), except for the fourth, which has no training set. These sets are referred to as RTE1-5. The dev/test distribution for RTE13, and RTE5 is respectively 567/800, 800/800, 800/800, and 600/600 T-H pairs. We used these sets for the traditional task of pair-based entailment recognition, where a pair of text-hypothesis p = (t, h) is assigned a positive or negative entailment class. For our comparative analysis, we use the syntax-based approach described in (Moschitti & Zanzotto, 2007) with two kernel function schemes: (1) PKS(p1, p2) = KS(t1, t2) + KS(h1, h2); and, (2) PKS+Lex(p1, p2) = Lex(t1, h1)Lex(t2, h2) + KS(t1, t2) +KS(h1, h2). Lex is a standard similarity feature between the text and the hypothesis andKS is realized with TK, DTK , and DTK . In the plots, the different PKS kernels are referred to as TK, DTK , and DTK whereas the different PKS+Lex kernels are referred to as TK + Lex, DTK + Lex, and DTK + Lex."
    }, {
      "heading" : "5.2.2. CORRELATION BETWEEN TK AND DTK",
      "text" : "As a first measure of the ability of DTK to emulate the classic TK, we considered the Spearman’s correlation of their values computed on the parse trees for the sentences contained in QC and RTE corpora. Table 2 reports results and shows that DTK does not approximate adequately TK for λ = 1. This highlights the difficulty of DTKs to correctly handle pairs of large active forests, i.e., trees with many subtrees with weights around 1. The correlation improves dramatically when parameter λ is reduced. We can conclude that DTKs efficiently approximate TK for the\n2The QC set is available at http://l2r.cs.uiuc.edu/ ˜cogcomp/Data/QA/QC/\nλ ≤ 0.6. These values are relevant for the applications as we will also see in the next section.\n5.2.3. TASK-BASED COMPARISON\nWe performed both QC and RTE experiments for different values of parameter λ. Results are shown in Fig. 3 and 4 for QC and RTE tasks respectively.\nFor QC, DTK leads to worse performances with respect to TK, but the gap is narrower for small values of λ ≤ 0.4 (withDTK better thanDTK ). These λ values produce better performance for the task. For RTE, for λ ≤ 0.4, DTK and DTK is similar to TK. Differences are not statistically significant except for for λ = 0.4 where DTK behaves better than TK (with p < 0.1). Statistical significance is computed using the two-sample Student t-test. DTK + Lex and DTK + Lex are statisti-\ncally similar to TK + Lex for any value of λ. DTKs are a good approximation of TKs for λ ≤ 0.4, that are the values where TKs have the best performances in the tasks.\n5.3. Average computation time"
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper we proposed the distributed tree kernels (DTKs) as an approach to reduce computational complexity of tree kernels. Having an ideal function for vector composition, we have formally shown that highdimensional spaces of tree fragments can be embedded in low-dimensional spaces where tree kernels can be directly computed with dot products. We have empirically shown that we can approximate the ideal function for vector composition. The resulting DTKs correlate with original tree kernels, obtain similar results in two natural language processing tasks, and, finally, are faster."
    } ],
    "references" : [ {
      "title" : "Convolution kernels for natural language",
      "author" : [ "Collins", "Michael", "Duffy", "Nigel" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Collins et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2001
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Dagan", "Ido", "Glickman", "Oren", "Magnini", "Bernardo" ],
      "venue" : "In Quionero-Candela et al.(ed.),",
      "citeRegEx" : "Dagan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2005
    }, {
      "title" : "Incorporation of application layer protocol syntax into anomaly detection",
      "author" : [ "Düssel", "Patrick", "Gehl", "Christian", "Laskov", "Pavel", "Rieck", "Konrad" ],
      "venue" : "In ICISS,",
      "citeRegEx" : "Düssel et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Düssel et al\\.",
      "year" : 2008
    }, {
      "title" : "Automatic Labeling of Semantic Roles",
      "author" : [ "Gildea", "Daniel", "Jurafsky" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Gildea et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gildea et al\\.",
      "year" : 2002
    }, {
      "title" : "Mining significant tree patterns in carbohydrate sugar",
      "author" : [ "Hashimoto", "Kosuke", "Takigawa", "Ichigaku", "Shiga", "Motoki", "Kanehisa", "Minoru", "Mamitsuka", "Hiroshi" ],
      "venue" : "chains. Bioinformatics,",
      "citeRegEx" : "Hashimoto et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2008
    }, {
      "title" : "Convolution kernels on discrete structures",
      "author" : [ "Haussler", "David" ],
      "venue" : "Tech. Rept., Univ. of California at Santa Cruz,",
      "citeRegEx" : "Haussler and David.,? \\Q1999\\E",
      "shortCiteRegEx" : "Haussler and David.",
      "year" : 1999
    }, {
      "title" : "Extensions of lipschitz mappings into a hilbert space",
      "author" : [ "W. Johnson", "J. Lindenstrauss" ],
      "venue" : "Contemp. Math.,",
      "citeRegEx" : "Johnson and Lindenstrauss,? \\Q1984\\E",
      "shortCiteRegEx" : "Johnson and Lindenstrauss",
      "year" : 1984
    }, {
      "title" : "Learning to recognize features of valid textual entailments",
      "author" : [ "MacCartney", "Bill", "Grenager", "Trond", "de Marneffe", "MarieCatherine", "Cer", "Daniel", "Manning", "Christopher D" ],
      "venue" : "In NAACL,",
      "citeRegEx" : "MacCartney et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "MacCartney et al\\.",
      "year" : 2006
    }, {
      "title" : "Making tree kernels practical for natural language learning",
      "author" : [ "Moschitti", "Alessandro" ],
      "venue" : "In EACL, Trento, Italy,",
      "citeRegEx" : "Moschitti and Alessandro.,? \\Q2006\\E",
      "shortCiteRegEx" : "Moschitti and Alessandro.",
      "year" : 2006
    }, {
      "title" : "Fast and effective kernels for relational learning from texts",
      "author" : [ "Moschitti", "Alessandro", "Zanzotto", "Fabio Massimo" ],
      "venue" : null,
      "citeRegEx" : "Moschitti et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Moschitti et al\\.",
      "year" : 2007
    }, {
      "title" : "On reverse feature engineering of syntactic tree kernels",
      "author" : [ "Pighin", "Daniele", "Moschitti", "Alessandro" ],
      "venue" : "In CoNLL, Uppsala,",
      "citeRegEx" : "Pighin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pighin et al\\.",
      "year" : 2010
    }, {
      "title" : "Holographic reduced representations",
      "author" : [ "T.A. Plate" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Plate,? \\Q1995\\E",
      "shortCiteRegEx" : "Plate",
      "year" : 1995
    }, {
      "title" : "Semantic role labeling using different syntactic views",
      "author" : [ "Pradhan", "Sameer", "Ward", "Wayne", "Hacioglu", "Kadri", "Martin", "James H", "Jurafsky", "Daniel" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Pradhan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2005
    }, {
      "title" : "Approximate tree kernels",
      "author" : [ "Rieck", "Konrad", "Krueger", "Tammo", "Brefeld", "Ulf", "Müller", "KlausRobert" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Rieck et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rieck et al\\.",
      "year" : 2010
    }, {
      "title" : "An introduction to random indexing",
      "author" : [ "Sahlgren", "Magnus" ],
      "venue" : "In Workshop of Methods and Applications of Semantic Indexing at TKE, Copenhagen,",
      "citeRegEx" : "Sahlgren and Magnus.,? \\Q2005\\E",
      "shortCiteRegEx" : "Sahlgren and Magnus.",
      "year" : 2005
    }, {
      "title" : "Mapping kernels for trees",
      "author" : [ "Shin", "Kilho", "Cuturi", "Marco", "Kuboyama", "Tetsuji" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Shin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2011
    }, {
      "title" : "A tree kernel to analyse phylogenetic profiles",
      "author" : [ "Vert", "Jean-Philippe" ],
      "venue" : "Bioinformatics, 18(suppl 1):S276–S284,",
      "citeRegEx" : "Vert and Jean.Philippe.,? \\Q2002\\E",
      "shortCiteRegEx" : "Vert and Jean.Philippe.",
      "year" : 2002
    }, {
      "title" : "Distributed structures and distributional meaning",
      "author" : [ "Zanzotto", "Fabio Massimo", "Dell’Arciprete", "Lorenzo" ],
      "venue" : "In Proceedings of the Workshop on Distributional Semantics and Compositionality,",
      "citeRegEx" : "Zanzotto et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zanzotto et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Thus, many areas – for example, biology (Vert, 2002; Hashimoto et al., 2008), computer security (Düssel et al.",
      "startOffset" : 40,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : ", 2008), computer security (Düssel et al., 2008), and natural language processing (Collins & Duffy, 2001; Gildea & Jurafsky, 2002; Pradhan et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : ", 2008), and natural language processing (Collins & Duffy, 2001; Gildea & Jurafsky, 2002; Pradhan et al., 2005; MacCartney et al., 2006) – fostered extensive research in methods for learning classifiers that leverage on these data structures.",
      "startOffset" : 41,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : ", 2008), and natural language processing (Collins & Duffy, 2001; Gildea & Jurafsky, 2002; Pradhan et al., 2005; MacCartney et al., 2006) – fostered extensive research in methods for learning classifiers that leverage on these data structures.",
      "startOffset" : 41,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "Different tree kernels modeling different feature spaces have been proposed (see (Shin et al., 2011) for a survey), but a primary research focus is the reduction of their execution time.",
      "startOffset" : 81,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "tences that hardly go beyond the hundreds of nodes (Rieck et al., 2010).",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "Then, for the classification, either the selection is directly encoded in the kernel computation by selecting subtrees headed by specific node labels (Rieck et al., 2010) or the smaller selected space is made explicit (Pighin & Moschitti, 2010).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 15,
      "context" : "The third direction exploits dynamic programming on the whole training and application sets of instances (Shin et al., 2011).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "We here compare their complexity with respect to the traditional tree kernels (TK) (Collins & Duffy, 2001), the fast tree kernels (FTK) (Moschitti, 2006), the fast tree kernels plus feature selection (FTK+FS) (Pighin & Moschitti, 2010), and the approximate tree kernels (ATK) (Rieck et al., 2010).",
      "startOffset" : 276,
      "endOffset" : 296
    }, {
      "referenceID" : 11,
      "context" : "where: ⊗ is the element-wise product between vectors and is the circular convolution (as for distributed representations in (Plate, 1995)) between vectors; p1 and p2 are two different permutations of the vector elements; and γ is a normalization scalar parameter, computed as the average norm of the element-wise product of two vectors.",
      "startOffset" : 124,
      "endOffset" : 137
    } ],
    "year" : 2012,
    "abstractText" : "In this paper, we propose the distributed tree kernels (DTK) as a novel method to reduce time and space complexity of tree kernels. Using a linear complexity algorithm to compute vectors for trees, we embed feature spaces of tree fragments in low-dimensional spaces where the kernel computation is directly done with dot product. We show that DTKs are faster, correlate with tree kernels, and obtain a statistically similar performance in two natural language processing tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}