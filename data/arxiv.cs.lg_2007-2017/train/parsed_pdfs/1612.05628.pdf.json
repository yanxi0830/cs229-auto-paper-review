{
  "name" : "1612.05628.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A New Softmax Operator for Reinforcement Learning",
    "authors" : [ "Kavosh Asadi", "Michael L. Littman" ],
    "emails" : [ "KAVOSH@BROWN.EDU", "MLITTMAN@BROWN.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "There is a fundamental tension in decision making between choosing the action that has highest expected reward estimate and avoiding “starving” the other actions. The issue arises in the context of the exploration–exploitation dilemma (Thrun, 1992), non-stationary decision problems (Sutton, 1990), and when interpreting observed decisions (Baker et al., 2007).\nIn the reinforcement learning setting, a typical approach to addressing this tension is the use of softmax operators for value function optimization and softmax policies for action selection. Examples of this commonly used approach include on-policy value function based methods such as\nSARSA (Rummery & Niranjan, 1994) or expected SARSA (Sutton & Barto, 1998; Van Seijen et al., 2009), and policy search methods such as REINFORCE (Williams, 1992).\nAn ideal softmax operator is a parameterized set of operators that:\n1. has parameter settings that allow it to approximate maximization arbitrarily accurately (allowing for reward seeking behavior);\n2. is a non-expansion for all parameter settings (guaranteeing convergence to a fixed point);\n3. is differentiable (making it possible to improve via gradient descent); and\n4. puts non-zero weight on non-maximizing actions (to avoid starving non-maximizing actions).\nLet X = x1, . . . , xn be a vector of values. We define the following operators:\nmax(X) = max i∈{1,...,n} xi ,\nmean(X) = 1\nn\nn∑\ni=1\nxi ,\neps (X) = mean(X) + (1− ) max(X) ,\nboltzβ(X) = ∑n i=1 xi e βxi\n∑n i=1 e βxi .\nThe first operator, max(X), is known to be a non-expansion (Littman & Szepesvári, 1996). However, it is non-differentiable (Property 3), and ignores non-maximizing selections (Property 4).\nThe next operator, mean(X), computes the average of its inputs. It is differentiable and, like any operator that takes a fixed convex combination of its inputs, is a non-expansion. However, it does not allow for maximization (Property 1).\nar X\niv :1\n61 2.\n05 62\n8v 1\n[ cs\n.A I]\n1 6\nD ec\n2 01\nThe third operator eps (X), commonly referred to as epsilon greedy (Sutton & Barto, 1998), interpolates between max and mean. Though the operator is a non-expansion, it is non-differentiable (Property 3).\nThe Boltzmann operator boltzβ(X), in contrast, is differentiable. It also approximates max as β → ∞, and mean as β → 0. However, it is not a non-expansion, as will be shown in a later section (Property 2).\nIn the following section we provide a simple example illustrating why the non-expansion property is important, especially in the context of planning and on-policy learning. We then present an alternative softmax operator that is similar to the Boltzmann operator yet is a non-expansion. We then prove several critical properties of this new operator and conclude with other possible applications."
    }, {
      "heading" : "2. Boltzmann Operator Is Prone to Misbehavior",
      "text" : "We first show that boltzβ can lead to problematic behavior in learning. To this end, we ran SARSA with Boltzmann softmax policy (Algorithm 1) on the MDP in Figure 1. The edges are labeled with a transition probability (unsigned) and a reward (signed). Note that in this MDP rewards are functions of states and actions, and not the next states. Also, state s2 is a terminal state, so we only consider two action valus, namely Q̂(s1, a) and Q̂(s2, b). Recall that Boltzmann softmax policy assigns the following probability to each action:\nπ(a|s) = e βQ̂(s,a)\n∑ a e βQ̂(s,a) .\nIn Figure 2, we plot state–action value estimates at the end of each episode of a single run (smoothed by averaging over ten consecutive points). We set α = .1 and β = 16.53. The values and policy never converge.\nAlgorithm 1 SARSA with Boltzmann softmax policy Input: initial Q̂(s, a) ∀s ∈ S ∀a ∈ A, α, and β for each episode do\nInitialize s a ∼ Boltzmann softmax with parameter β repeat\nTake action a, observe r, s′ a ′ ∼ Boltzmann softmax with parameter β Q̂(s, a)← Q̂(s, a) +α [ r+ γQ̂(s′, a′)− Q̂(s, a) ]\ns← s′ , a← a′ until s is terminal\nend for\nSARSA is known to converge in the tabular setting using -greedy exploration (Littman & Szepesvári, 1996), under decreasing exploration (Singh et al., 2000), and to a region in the function approximation setting (Gordon, 2001). There are also variants of the SARSA update rule that converge more generally (Perkins & Precup, 2002; Baird & Moore, 1999; Van Seijen et al., 2009). However, this example is the first, to our knowledge, to show that SARSA fails to converge in the tabular setting with Boltzmann softmax. The next section provides background for our analysis of this example."
    }, {
      "heading" : "3. Background",
      "text" : "A Markov decision process (Puterman, 1994), or MDP, is specified by the tuple 〈S,A,R,P, γ〉, where S is the set of states and A is the set of actions. The functions R and P denote the reward and transition dynamics of the MDP. More precisely, the expected immediate reward following an action a ∈ A in a state s ∈ S before moving to a next state s′ ∈ S is specified by:\nR(s, a, s′) = E[Rt+1|St = s,At = a, St+1 = s′] ,\nand the probability of this transition is defined by:\nP(s, a, s′) = Pr(St+1 = s′ ∣∣∣St = s,At = a) .\nFinally, γ ∈ [0, 1), the discount rate, determines the relative importance of immediate reward as opposed to the rewards received in the future.\nA typical approach to finding a good policy is to define and estimate how good it is to be in a particular state—the state value function. The value of a particular state s given a policy π is formally defined to be:\nvπ(s) = E π\n[ Rt+1 + γRt+2 + γ 2Rt+3 + ... ∣∣St = s ] .\nIt can also be useful to define the state–action value function, formally defined as:\nqπ(s, a) = E π\n[ Rt+1+γRt+2+γ 2Rt+3+ ... ∣∣St = s,At = a ] ,\nwhich is the expected sum of future discounted rewards upon taking an action a in a state s and committing to policy π thereafter.\nWe define the optimal value of a state–action pair\nq?(s, a) = max π qπ(s, a).\nIt is possible to define q?(s, a) recursively and as a function of the optimal value of the other state action pairs:\nq?(s, a) = ∑\ns′∈S R(s, a, s′) + γ P(s, a, s′) max a′ q?(s′, a′).\nBellman equations, such as the above equation, are at the core of many reinforcement-learning algorithms.\nValue iteration is an example of a fundamental planning algorithm for MDPs. It computes the value of the best policy in an iterative fashion using the update rule:\nQ̂(s, a)← ∑\ns′∈S R(s, a, s′) + γP(s, a, s′) max a′ Q̂(s′, a′).\nRegardless of its initial value, Q̂ will then converge to q∗. Q̂ can then be used for decision making.\n(Littman & Szepesvári, 1996) generalized this approach by replacing the max operator by any arbitrary operator ⊗ , resulting in the generalized value iteration (GVI) algorithm with the following update rule:\nQ̂(s, a)← ∑\ns′∈S R(s, a, s′) + γP(s, a, s′)\n⊗\na′\nQ̂(s′, a′).\nAlgorithm 2 GVI algorithm Input: initial Q̂(s, a) ∀s ∈ S ∀a ∈ A and δ ∈ R++ repeat\ndiff← 0 for each s ∈ S do\nfor each a ∈ A do Qcopy ← Q̂(s, a) Q̂(s, a)←∑s′∈S R(s, a, s′)\n+ γP(s, a, s′)⊗ Q̂(s′, .) diff← max { diff, |Qcopy − Q̂(s, a)| }\nend for end for\nuntil diff < δ\nCrucially, convergence of GVI to a unique fixed point follows if operator ⊗ is a non-expansion:\n∣∣∣ ⊗\na\nQ̂(s, a)− ⊗\na\nQ̂′(s, a) ∣∣∣ ≤ max\na\n∣∣∣Q̂(s, a)− Q̂′(s, a) ∣∣∣,\nfor any Q̂, Q̂′ and s. As mentioned earlier, the max operator is shown to be a non-expansion, as illustrated by Figure 3. Also, mean, and eps operators are non-expansions. Therefore, each of these operators causes GVI to converge to the corresponding unique fixed point. However, the Boltzmann softmax operator, boltzβ , is not a non-expansion (Littman, 1996), and so, its fixed point may not be unique.\nNote that we can relate GVI to SARSA by noticing that SARSA update can be thought of as a stochastic implementation of GVI update. For example, under a Boltzmann softmax policy we have:\nE π\n[ r + γQ̂(s′, a′)− Q̂(s, a) ∣∣∣s, a ]\n= ∑\ns′∈S R(s, a, s′) + γP(s, a, s′)\n∑\na′∈A π(a′|s′)Q̂(s′, a′) ︸ ︷︷ ︸ boltzβ ( Q̂(s′,·) ) −Q̂(s, a)."
    }, {
      "heading" : "4. Boltzmann Has Multiple Fixed Points",
      "text" : "Although it has been known for a long time that the Boltzmann operator is not a non-expansion (Littman, 1996), we are not aware of a published example of an MDP for which two distinct fixed points exist. The MDP presented in Figure 1 is the first example of such an MDP. As shown in Figure 4, GVI under boltzβ has two fixed points for β ∈ [16.53, 16.78]. (For other values of β, we found a single fixed point.) We also show a vector field visualizing GVI updates under boltzβ=16.53. The behavior of SARSA in Figure 2 is, therefore, caused by the algorithm stochastically bouncing back and forth between the two different fixed points."
    }, {
      "heading" : "5. Mellowmax and its Properties",
      "text" : "We advocate an alternative softmax operator defined as follows:\nmmω(X) = log( 1n\n∑n i=1 e ωxi)\nω ,\nwhich can be viewed as a particular instantiation of the quasi-arithmetic mean (Beliakov et al., 2016).\nWe show that mmω(X), which we refer to as mellowmax, has all four of the desired properties and compares quite favorably to boltzβ in practice."
    }, {
      "heading" : "5.1. Mellowmax is a Non-Expansion",
      "text" : "We prove that mmω is a non-expansion (Property 2), and therefore, GVI under mmω is guaranteed to converge to a unique fixed point.\nLet X = x1, . . . , xn and Y = y1, . . . , yn be two vectors of values. Let ∆i = xi − yi for i ∈ {1, . . . , n} be the difference of the ith components of the two vectors. Also, let i∗ be the index with the maximum component-wise difference, i∗ = argmaxi ∆i. For simplicity, we assume that i∗ is unique. Also, without loss of generality, we assume that xi∗ − yi∗ ≥ 0. It follows that:\n∣∣mmω(X)−mmω(Y) ∣∣\n= ∣∣ log( 1\nn\nn∑\ni=1\neωxi)/ω − log( 1 n\nn∑\ni=1\neωyi)/ω ∣∣\n= ∣∣ log 1 n\n∑n i=1 e ωxi\n1 n ∑n i=1 e\nωyi /ω ∣∣\n= ∣∣ log\n∑n i=1 e\nω ( yi+∆i ) ∑n i=1 e ωyi /ω ∣∣\n≤ ∣∣ log\n∑n i=1 e\nω ( yi+∆i∗ ) ∑n i=1 e ωyi /ω ∣∣\n= ∣∣ log e\nω∆i∗ ∑n i=1 e ωyi\n∑n i=1 e\nωyi /ω ∣∣\n= ∣∣ log(eω∆i∗ )/ω ∣∣ = ∣∣∆i∗ ∣∣ = max i ∣∣xi − yi ∣∣ ,\nallowing us to conclude that mellowmax is a non-expansion.\nExperiments confirm that under mellowmax convergence is consistent and rapid."
    }, {
      "heading" : "5.2. Maximization",
      "text" : "Mellowmax includes parameter settings that allow for maximization (Property 1) as well as for minimization. In\nparticular, as ω goes to infinity, mmω acts like max.\nLet m = max(X) and let W = |{xi = m|i ∈ {1, . . . , n}}|. Note that W ≥ 1 is the number of maximum values (“winners”) in X. Then:\nlim ω→∞ mmω(X) = lim ω→∞\nlog( 1n ∑n i=1 e ωxi)\nω\n= lim ω→∞\nlog( 1ne ωm ∑n i=1 e ω(xi−m))\nω\n= lim ω→∞\nlog( 1ne ωmW )\nω\n= lim ω→∞ log(eωm)− log(n) + log(W ) ω\n= m+ lim ω→∞ − log(n) + log(W ) ω = m = max(X) .\nThat is, the operator acts more and more like pure maximization as the value of ω is increased. Conversely, as ω goes to −∞, the operator approaches the minimum.\nlim ω→−∞ mmω(X) = lim ω→∞ mm−ω(X)\n= lim ω→∞ −mmω(−X) = −max(−X) = min(X) ."
    }, {
      "heading" : "5.3. Derivatives",
      "text" : "We can take the derivative of mellowmax with respect to each one of the arguments xi and for any non-zero ω:\n∂mmω(X) ∂xi = eωxi∑n i=1 e ωxi ≥ 0 .\nNote that the operator is non-decreasing in each component of X.\nMoreover, we can take the derivative of mellowmax with respect to ω. We define nω(X) = log( 1n ∑n i=1 e\nωxi) and dω(X) = ω. Then:\n∂nω(X) ∂ω =\n∑n i=1 xie ωxi\n∑n i=1 e ωxi and ∂dω(X) ∂ω = 1 ,\nand so:\n∂mmω(X) ∂ω\n= ∂nω(X) ∂ω dω(X)− nω(X) ∂dω(X) ∂ω\ndω(X)2 ,\nensuring differentiablity of the operator (Property 3)."
    }, {
      "heading" : "5.4. Averaging",
      "text" : "Because of the division by ω in the definition of mmω , the parameter ω cannot be set to zero. However, we can examine the behavior of mmω as ω approaches zero and show that the operator computes an average in the limit.\nSince both the numerator and denominator go to zero as ω goes to zero, we will use L’Hôpital’s rule and the derivative given in the previous section to derive the value in the limit:\nlim ω→0 mmω(X) = lim ω→0\nlog( 1n ∑n i=1 e ωxi)\nω\nL’Hôpital = lim\nω→0\n1 n ∑n i=1 xie ωxi\n1 n ∑n i=1 e ωxi\n= 1\nn\nn∑\ni=1\nxi = mean(X) .\nThat is, as ω gets closer to zero, mmω(X) approaches the mean of the values in X."
    }, {
      "heading" : "6. Maximum Entropy Mellowmax Policy",
      "text" : "As described, mmω computes a value for a list of numbers somewhere between its minimum and maximum. However, it is often useful to actually provide a probability distribution over the actions such that (1) a non-zero probability mass is assigned to each action, and (2) the resulting expected value equals the computed value. Such a probability distribution can then be used for action selection in algorithms such as SARSA.\nIn this section, we address the problem of identifying such a probability distribution as a maximum entropy problem—over all distributions that satisfy the properties above, pick the one that maximizes information entropy (Cover & Thomas, 2006; Peters et al., 2010). We formally define the maximum entropy mellowmax policy of a state s as:\nπME(s) = argmin π\n∑ a∈A π(a|s) log ( π(a|s) )\nsubject to { ∑ a∈A π(a|s)Q̂(s, a) = mmω(Q̂(s, .))\nπ(a|s) ≥ 0∑ a∈A π(a|s) = 1 .\nNote that this optimization problem is convex and can be solved reliably using any numerical convex optimization library.\nOne way of finding the solution, which leads to an interesting policy form, is to use the method of Lagrange multipliers. Here, the Lagrangian is:\nL(π, λ1, λ2) = ∑\na∈A π(a|s) log\n( π(a|s) )\n−λ1 (∑\na∈A π(a|s)− 1\n)\n−λ2 (∑\na∈A π(a|s)Q̂(s, a)−mmω\n( Q̂(s, .) )) .\nTaking the derivative of the Lagrangian with respect to each π(a|s) and setting them to zero, we obtain:\n∂L ∂π(a|s) = log ( π(a|s) ) +1−λ1−λ2Q̂(s, a) = 0 ∀ a ∈ A .\nThese |A| equations, together with the two linear constraints, form |A| + 2 equations to constrain the |A| + 2 variables, π(a|s) ∀a ∈ A and the two Lagrangian multipliers λ1 and λ2.\nSolving this system of equations, the probability of taking an action in the maximum entropy mellowmax policy has the form:\nπME(a|s) = eβQ̂(s,a)∑ a∈A e βQ̂(s,a) ∀a ∈ A ,\nwhere β is a value of y for which:\n∑ a∈A e y\n(( Q̂(s,a)−mmω ( Q̂(s,.) ))( Q̂(s, a)−mmω ( Q̂(s, .) )) .\nis zero. The value of β can be found easily using any root-finding algorithm. In particular, in our experiments we used Brent’s method (Brent, 2013) available in python’s numpy library.\nIt is simple to show that a unique root always exists. As ω gets higher, the term corresponding to the best action dominates, and so, the function is positive. Conversely, as beta goes to−∞, the term corresponding to the action with lowest utility dominates, and so the function is negative. Finally, by taking the derivative it is clear that the function is monotonically increasing, allowing us to conclude that there exists only a single answer.\nThis policy has the same form as Boltzmann softmax, but with a parameter β whose value depends indirectly on ω. This mathematical form arose not from the structure of mmω , but from maximizing the entropy. One way to view the use of the mellowmax operator, then, is as a\nform of Boltzmann softmax policy with a parameter chosen adaptively to ensure that the non-expansion property holds.\nFinally, note that the SARSA update under the maximum entropy mellowmax policy could be thought of as a stochastic implementation of the GVI update under mmω operator:\nE π\n[ r + γQ̂(s′, a′)− Q̂(s, a) ∣∣∣s, a ] = ∑\ns′∈S R(s, a, s′) +\nγP(s, a, s′) ∑\na∈A π(a′|s′)Q̂(s′, a′) ︸ ︷︷ ︸ mmω ( Q̂(s′,.) ) −Q̂(s, a)\ndue to the first constraint of the above optimization problem. As such, SARSA with the maximum entropy mellowmax policy is guaranteed to converge."
    }, {
      "heading" : "7. Experiments On MDPs",
      "text" : "Next, we repeat the experiment from Figure 2 using SARSA with the maximum entropy mellowmax policy and with ω = 16.53. The results, presented in Figure 6, show rapid convergence to the unique fixed point. Analogously to Figure 5, we provide a vector field for GVI updates under mmω=16.53. As shown above, using mmω ensures that GVI updates move estimates steadily to the fixed point. As a result, GVI under mmω can terminate significantly faster\nthan GVI under boltzβ , as illustrated in Figure 8.\nWe now present two experiments on standard reinforcement learning domains. The first experiment compares softmax policies when used in SARSA with a tabular representation. The second experiment is a policy gradient experiment where a deep neural network is used to directly represent the policy."
    }, {
      "heading" : "7.1. Multi-passenger Taxi Domain",
      "text" : "We evaluated SARSA with various policies on the multi-passenger taxi domain introduced by (Dearden et al., 1998). (See Figure 9.)\nOne challenging aspect of this domain is that it admits many locally optimal policies. Exploration needs to be set carefully to avoid either over-exploring or under-exploring the state space. Note also that Boltzmann softmax performs remarkably well on this domain, outperforming sophisticated bayesian reinforcement learning algorithms. (Dearden et al., 1998)\nAs shown in Figure 10, SARSA with the epsilon-greedy policy performs poorly. In fact, in our experiment, the algorithm rarely was able to deliver all the passengers. However, SARSA with Boltzman softmax and SARSA with the maximum entropy mellowmax policy achieved significantly higher average reward. Maximum entropy mellowmax policy is no worse than Boltzmann softmax here."
    }, {
      "heading" : "7.2. Lunar Lander Domain",
      "text" : "In this section we evaluate maximum entropy mellowmax policy in the context of the policy gradient algorithms. Specifically, we represent policy by a deep neural network (we discuss the details of the network below). Usually the activation function of the last layer of the network is a softmax function, and typically Boltzmann softmax boltzβ is used. Alternatively, we use maximum entropy mellowmax policy, presented in section 6, by treating the input of the activation function as the Q̂ values.\nWe used the lunar lander domain, from OpenAI Gym (Brockman et al., 2016), as our benchmark. A screenshot of the domain is presented in Figure 11, and the code of the domain is publicly available. This domain has a continuous state space with 8 dimensions, namely x-y coordinates, x-y velocities, angle and angular velocities, and leg-touchdown sensors. There are 4 discrete actions to control 3 engines. The reward is +100 for a safe landing in the designated area, and -100 for a crash. There is a small shaping reward for approaching the landing area. Using the engines will also result in a negative reward. Episode finishes when the\nspacecraft crashes or lands. Solving the domain is defined as maintaining mean episode return higher than 200 in 100 consecutive episodes.\nThe policy is represented by a neural network with a hidden layer comprised of 16 units with RELU activation function, followed by a second layer with 16 units and softmax activation functions. We used REINFORCE to train the network. A batch episode size of 10 is used, as we had stability issues with a batch size of 1. We used Adam algorithm (Kingma & Ba, 2014) with α = 0.005 and set the other parameters as suggested by the paper. We used Keras (Chollet, 2015) and Theano (Team et al., 2016) to implement the neural network.\nFor each softmax policy, we present in Figure 12 ,the learning curves for different values of their free parameter. We further plot average return over all 40000 episodes. Mellowmax is indeed less sensitive to the choice of its free parameter and outperforms Boltzmann."
    }, {
      "heading" : "8. Related Work",
      "text" : "Softmax operators play an important role in sequential decision-making algorithms.\nIn model-free reinforcement-learning, they can help strike a balance between exploration (mean) and exploitation (max). Decision rules based on epsilon-greedy and Boltzmann softmax, while very simple, often perform surprisingly well in practice, even outperforming more advanced exploration techniques (Kuleshov & Precup, 2014).\nWhen learning “on policy”, exploration steps can (Rummery & Niranjan, 1994) and perhaps should (John, 1994) become part of the value-estimation process itself. On-policy algorithms like SARSA can be made to converge to optimal behavior in the limit when the exploration rate and the update operator is gradually moved toward max (Singh et al., 2000). Our use of softmax operators in learning updates reflects this point of view.\nAnalyses of the behavior of human subjects in choice experiments very frequently use softmax. Sometimes referred to in the literature as logit choice (Stahl & Wilson, 1994), it forms an important part of the most accurate predictor of human decisions in normal-form games (Wright & Leyton-Brown, 2010), quantal level-k reasoning (QLk). Softmax-based fixed points play a crucial role in this work. As such, mellowmax could potentially make a good replacement resulting in better behaved solutions.\nAlgorithms for inverse reinforcement learning (IRL), the problem of inferring reward functions from observed behavior (Ng & Russell, 2000), frequently use a Boltzmann operator to avoid assigning zero probability to non-optimal actions and hence assessing an observed sequence as impossible. Such methods include Bayesian IRL (Ramachandran & Amir, 2007), natural gradient IRL (Neu & Szepesvári, 2007), and maximum likelihood IRL (Babes et al., 2011). Given the recursive nature of value defined in these problems, mellowmax could be a more stable and efficient choice."
    }, {
      "heading" : "9. Conclusion and Future Work",
      "text" : "We proposed the mellowmax operator as an alternative for the Boltzmann operator. We showed that mellowmax has several desirable properties and that it works favorably in practice. Arguably, mellowmax could be used in place of Boltzmann throughout reinforcement-learning research.\nImportant future work is to expand the scope of investigation to the function approximation setting in which the state space or the action space is large and\nabstraction techniques are used. We expect mellowmax operator and its non-expansion property to behave more consistently than the Boltzmann operator when estimates of state–action values can be arbitrarily inaccurate.\nAnother direction is to analyze the fixed point of planning, reinforcement-learning, and game-playing algorithms when using softmax and mellowmax operators. In particular, an interesting analysis could be one that bounds the suboptimality of fixed points found by value iteration under each operator.\nFinally, due to the convexity (Boyd & Vandenberghe, 2004) of mellowmax, it is compelling to use this operator in a gradient ascent algorithm in the context of sequential decision making. Inverse reinforcement-learning algorithms is a natural candidate given the popularity of softmax in these settings."
    }, {
      "heading" : "10. Acknowledgments",
      "text" : "The authors gratefully acknowledge the assistance of George Konidaris."
    } ],
    "references" : [ {
      "title" : "Apprenticeship learning about multiple intentions",
      "author" : [ "Babes", "Monica", "Marivate", "Vukosi N", "Littman", "Michael L", "Subramanian", "Kaushik" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Babes et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Babes et al\\.",
      "year" : 2011
    }, {
      "title" : "Gradient descent for general reinforcement learning",
      "author" : [ "Baird", "Leemon", "Moore", "Andrew W" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Baird et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Baird et al\\.",
      "year" : 1999
    }, {
      "title" : "Goal inference as inverse planning",
      "author" : [ "Baker", "Chris L", "Tenenbaum", "Joshua B", "Saxe", "Rebecca R" ],
      "venue" : "In Proceedings of the 29th Annual Meeting of the Cognitive Science Society,",
      "citeRegEx" : "Baker et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 2007
    }, {
      "title" : "A Practical Guide to Averaging",
      "author" : [ "Beliakov", "Gleb", "Sola", "Humberto Bustince", "Sánchez", "Tomasa Calvo" ],
      "venue" : null,
      "citeRegEx" : "Beliakov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Beliakov et al\\.",
      "year" : 2016
    }, {
      "title" : "Algorithms for minimization without derivatives",
      "author" : [ "Brent", "Richard P" ],
      "venue" : "Courier Corporation,",
      "citeRegEx" : "Brent and P.,? \\Q2013\\E",
      "shortCiteRegEx" : "Brent and P.",
      "year" : 2013
    }, {
      "title" : "Elements of Information Theory",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : null,
      "citeRegEx" : "Cover and Thomas,? \\Q2006\\E",
      "shortCiteRegEx" : "Cover and Thomas",
      "year" : 2006
    }, {
      "title" : "Bayesian Q-learning",
      "author" : [ "Dearden", "Richard", "Friedman", "Nir", "Russell", "Stuart" ],
      "venue" : "In Fifteenth National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Dearden et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Dearden et al\\.",
      "year" : 1998
    }, {
      "title" : "Reinforcement learning with function approximation converges to a region, 2001. Unpublished",
      "author" : [ "Gordon", "Geoffrey J" ],
      "venue" : null,
      "citeRegEx" : "Gordon and J.,? \\Q2001\\E",
      "shortCiteRegEx" : "Gordon and J.",
      "year" : 2001
    }, {
      "title" : "When the best move isn’t optimal: Q-learning with exploration",
      "author" : [ "John", "George H" ],
      "venue" : "In Proceedings of the Twelfth National Conference on Artificial Intelligence,",
      "citeRegEx" : "John and H.,? \\Q1994\\E",
      "shortCiteRegEx" : "John and H.",
      "year" : 1994
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Algorithms for multi-armed bandit problems",
      "author" : [ "Kuleshov", "Volodymyr", "Precup", "Doina" ],
      "venue" : "arXiv preprint arXiv:1402.6028,",
      "citeRegEx" : "Kuleshov et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kuleshov et al\\.",
      "year" : 2014
    }, {
      "title" : "A generalized reinforcement-learning model: Convergence and applications",
      "author" : [ "Littman", "Michael L", "Szepesvári", "Csaba" ],
      "venue" : "Proceedings of the Thirteenth International Conference on Machine Learning,",
      "citeRegEx" : "Littman et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Littman et al\\.",
      "year" : 1996
    }, {
      "title" : "Algorithms for Sequential Decision Making",
      "author" : [ "Littman", "Michael Lederman" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Littman and Lederman.,? \\Q1996\\E",
      "shortCiteRegEx" : "Littman and Lederman.",
      "year" : 1996
    }, {
      "title" : "Apprenticeship learning using inverse reinforcement learning and gradient methods",
      "author" : [ "Neu", "Gergely", "Szepesvári", "Csaba" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Neu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2007
    }, {
      "title" : "Algorithms for inverse reinforcement learning",
      "author" : [ "Ng", "Andrew Y", "Russell", "Stuart" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Ng et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2000
    }, {
      "title" : "A convergent form of approximate policy iteration",
      "author" : [ "Perkins", "Theodore J", "Precup", "Doina" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Perkins et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Perkins et al\\.",
      "year" : 2002
    }, {
      "title" : "Relative entropy policy search",
      "author" : [ "Peters", "Jan", "Mülling", "Katharina", "Altun", "Yasemin" ],
      "venue" : "In AAAI. Atlanta,",
      "citeRegEx" : "Peters et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2010
    }, {
      "title" : "Markov Decision Processes—Discrete Stochastic Dynamic Programming",
      "author" : [ "Ramachandran", "Deepak", "Amir", "Eyal" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Ramachandran et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Ramachandran et al\\.",
      "year" : 1994
    }, {
      "title" : "On-line Q-learning using connectionist systems",
      "author" : [ "G.A. Rummery", "M. Niranjan" ],
      "venue" : "Technical Report CUED/F-INFENG/TR 166,",
      "citeRegEx" : "Rummery and Niranjan,? \\Q1994\\E",
      "shortCiteRegEx" : "Rummery and Niranjan",
      "year" : 1994
    }, {
      "title" : "Convergence results for single-step on-policy reinforcement-learning algorithms",
      "author" : [ "Singh", "Satinder", "Jaakkola", "Tommi", "Littman", "Michael L", "Szepesvári", "Csaba" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Singh et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2000
    }, {
      "title" : "Experimental evidence on players’ models of other players",
      "author" : [ "Stahl", "Dale O", "Wilson", "Paul W" ],
      "venue" : "Journal of Economic Behavior and Organization,",
      "citeRegEx" : "Stahl et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Stahl et al\\.",
      "year" : 1994
    }, {
      "title" : "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
      "author" : [ "Sutton", "Richard S" ],
      "venue" : "In Proceedings of the Seventh International Conference on Machine Learning,",
      "citeRegEx" : "Sutton and S.,? \\Q1990\\E",
      "shortCiteRegEx" : "Sutton and S.",
      "year" : 1990
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "The role of exploration in learning control",
      "author" : [ "Thrun", "Sebastian B" ],
      "venue" : null,
      "citeRegEx" : "Thrun and B.,? \\Q1992\\E",
      "shortCiteRegEx" : "Thrun and B.",
      "year" : 1992
    }, {
      "title" : "A theoretical and empirical analysis of expected sarsa",
      "author" : [ "Van Seijen", "Harm", "Van Hasselt", "Hado", "Whiteson", "Shimon", "Wiering", "Marco" ],
      "venue" : "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,",
      "citeRegEx" : "Seijen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Seijen et al\\.",
      "year" : 2009
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Williams", "Ronald J" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Williams and J.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams and J.",
      "year" : 1992
    }, {
      "title" : "Beyond equilibrium: Predicting human behavior in normal-form games",
      "author" : [ "Wright", "James R", "Leyton-Brown", "Kevin" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Wright et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wright et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "The issue arises in the context of the exploration–exploitation dilemma (Thrun, 1992), non-stationary decision problems (Sutton, 1990), and when interpreting observed decisions (Baker et al., 2007).",
      "startOffset" : 177,
      "endOffset" : 197
    }, {
      "referenceID" : 19,
      "context" : "SARSA is known to converge in the tabular setting using -greedy exploration (Littman & Szepesvári, 1996), under decreasing exploration (Singh et al., 2000), and to a region in the function approximation setting (Gordon, 2001).",
      "startOffset" : 135,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "which can be viewed as a particular instantiation of the quasi-arithmetic mean (Beliakov et al., 2016).",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "In this section, we address the problem of identifying such a probability distribution as a maximum entropy problem—over all distributions that satisfy the properties above, pick the one that maximizes information entropy (Cover & Thomas, 2006; Peters et al., 2010).",
      "startOffset" : 222,
      "endOffset" : 265
    }, {
      "referenceID" : 6,
      "context" : "Multi-passenger Taxi Domain We evaluated SARSA with various policies on the multi-passenger taxi domain introduced by (Dearden et al., 1998).",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "(Dearden et al., 1998) As shown in Figure 10, SARSA with the epsilon-greedy policy performs poorly.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "On-policy algorithms like SARSA can be made to converge to optimal behavior in the limit when the exploration rate and the update operator is gradually moved toward max (Singh et al., 2000).",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "Such methods include Bayesian IRL (Ramachandran & Amir, 2007), natural gradient IRL (Neu & Szepesvári, 2007), and maximum likelihood IRL (Babes et al., 2011).",
      "startOffset" : 137,
      "endOffset" : 157
    } ],
    "year" : 2017,
    "abstractText" : "A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study an alternative softmax operator that, among other properties, is both a non-expansion (ensuring convergent behavior in learning and planning) and differentiable (making it possible to improve decisions via gradient descent methods). We provide proofs of these properties and present empirical comparisons between various softmax operators.",
    "creator" : "LaTeX with hyperref package"
  }
}