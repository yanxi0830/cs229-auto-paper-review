{
  "name" : "1303.3664.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Topic Discovery through Data Dependent and Random Projections",
    "authors" : [ "Weicong Ding", "Mohammad H. Rohban" ],
    "emails" : [ "dingwc@bu.edu", "mhrohban@bu.edu", "pi@bu.edu", "srv@bu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 3.\n36 64\nv2 [\nst at\n.M L\n] 1\n8 M\nar 2"
    }, {
      "heading" : "1. Introduction",
      "text" : "We consider a corpus of M documents composed of words chosen from a vocabulary of W distinct words indexed by w = 1, . . . ,W . We adopt the classic “bags of words” modeling paradigm widely-used in probabilistic topic modeling (Blei, 2012). Each document is modeled as being generated by N independent and identically distributed (iid) drawings of words from an unknown W × 1 document word-distribution vector. Each document word-distribution vector is itself modeled as an unknown probabilistic mixture of\nK < min(M,W ) unknown W × 1 latent topic worddistribution vectors that are shared among the M documents in the corpus. Documents are generated independently. For future reference, we adopt the following notation. We denote by β the unknown W ×K topicmatrix whose columns are the K latent topic worddistribution vectors. θ denotes the K × M weightmatrix whose M columns are the mixing weights over K topics for the M documents. These columns are assumed to be iid samples from a prior distribution. Each column of the W × M matrix A = βθ corresponds to a document word-distribution vector. X denotes the observed W ×M word-by-document matrix realization. TheM columns ofX are the empirical word-frequency vectors of the M documents. Our goal is to estimate the latent topic word-distribution vectors (β) from the empirical word-frequency vectors of all documents (X).\nA fundamental challenge here is that word-bydocument distributions (A) are unknown and only a realization is available through sampled word frequencies in each document. Another challenge is that even when these distributions are exactly known, the decomposition into the product of topic-matrix, β, and topic-document distributions, θ, which is known as Nonnegative Matrix Factorization (NMF), has been shown to be an NP-hard problem in general. In this paper, we develop computationally efficient algorithms with provable guarantees for estimating β for topic matrices satisfying the separability condition (Donoho & Stodden, 2004; Arora et al., 2012b).\nDefinition 1. (Separability) A topic matrix β ∈ RW×K is separable if for each topic k, there is some word i such that βi,k > 0 and βi,l = 0, ∀l 6= k.\nThe condition suggests the existence of novel words that are unique to each topic. Our algorithm has three main steps. In the first step, we identify novel words by means of data dependent or random projections. A key insight here is that when each word is associated with\na vector consisting of its occurrences across all documents, the novel words correspond to extreme points of the convex hull of these vectors. A highlight of our approach is the identification of novel words based on data-dependent and random projections. Our idea is that whenever a convex object is projected along a random direction, the maximum and minimum values in the projected direction correspond to extreme points of the convex object. While our method identifies novel words with negligible false and miss detections, evidently multiple novel words associated with the same topic can be an issue. To account for this issue, we apply a distance based clustering algorithm to cluster novel words belonging to the same topic. Our final step involves linear regression to estimate topic word frequencies using novel words.\nWe show that our scheme has a similar sample complexity to that of state-of-art such as (Arora et al., 2012a). On the other hand, the computational complexity of our scheme can scale as small as O( √ MW + MN) for a corpora containing M documents, with an average of N words per document from a vocabulary containing W words. We then present a set of experiments on synthetic and real-world datasets. The results demonstrates qualitative and quantitative superiority of our scheme in comparison to other stateof-art schemes."
    }, {
      "heading" : "2. Related Work",
      "text" : "The literature on topic modeling and discovery is extensive. One direction of work is based on solving a nonnegative matrix factorization (NMF) problem. To address the scenario where only the realization X is known and not A, several papers (Lee & Seung, 1999; Donoho & Stodden, 2004; Cichocki et al., 2009; Recht et al., 2012) attempt to minimize a regularized cost function. Nevertheless, this joint optimization is non-convex and suboptimal strategies have been used in this context. Unfortunately, when N ≪ W which is often the case, many words do not appear in X and such methods often fail in these cases.\nLatent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012) is a statistical approach to topic modeling. In this approach, the columns of θ are modeled as iid random drawings from some prior distributions such as Dirichlet. The goal is to compute MAP (maximum aposteriori probability) estimates for the topic matrix. This setup is inherently non-convex and MAP estimates are computed using variational Bayes approximations of the posterior distribution, Gibbs sampling or expectation propagation.\nA number of methods with provable guarantees have also been proposed. (Anandkumar et al., 2012) describe a novel method of moments approach. While their algorithm does not impose structural assumption on topic matrix β, they require Dirichlet priors for θ matrix. One issue is that such priors do not permit certain classes of correlated topics (Blei & Lafferty, 2007; Li & McCallum, 2007). Also their algorithm is not agnostic since it uses parameters of the Dirichlet prior. Furthermore, the algorithm suggested involves finding empirical moments and singular decompositions which can be cumbersome for large matrices.\nOur work is closely related to recent work of (Arora et al., 2012b) and (Arora et al., 2012a) with some important differences. In their work, they describe methods with provable guarantees when the topic matrix satisfies the separability condition. Their algorithm discovers novel words from empirical word co-occurrence patterns and then in the second step the topic matrix is estimated. Their key insight is that when each word, j, is associated with aW dimensional vector1 the novel words correspond to extreme points of the convex hull of these vectors. (Arora et al., 2012a) presents combinatorial algorithms to recover novel words with computational complexity scaling as O(MN2 +W 2 +WK/ǫ2), where ǫ is the element wise tolerable error of the topic matrix β. An important computational remark is that ǫ often scales with W , i.e. probability values in β get small when W is increased, hence one needs smaller ǫ to safely estimate β when W is too large. The other issue with their method is that empirical estimates of joint probabilities in the word-word co-occurrence matrix can be unreliable, especially when M is not large enough. Finally, their novel word detection algorithm requires linear independence of the extreme points of the convex hull. This can be a serious problem in some datasets where word co-occurrences lie on a low dimensional manifold.\nMajor Differences: Our work also assumes separability and existence of novel words. We associate each word with a M -dimensional vector consisting of the word’s frequency of occurrence in the M -documents rather than word co-occurrences as in (Arora et al., 2012b;a). We also show that extreme points of the convex hull of these cross-document frequency patterns are associated with novel words. While these differences appear technical, it has important consequences. In several experiments our approach appears to significantly outperform (Arora et al., 2012a) and mir-\n1kth component is probability of occurrence of word j and word k in the same document in the entire corpus\nror performance of more conventional methods such as LDA (Griffiths & Steyvers, 2004). Furthermore, our approach can deal with degenerate cases found in some image datasets where the data vectors can lie on a lower dimensional manifold than the number of topics. At a conceptual level our approach appears to hinge on distinct cross-document support patterns of novel words belonging to different topics. This is typically robust to sampling fluctuations when support patterns are distinct in comparison to word cooccurrences statistics of the corpora. Our approach also differs algorithmically. We develop novel algorithms based on data-dependent and random projections to find extreme points efficiently with computational complexity scaling as O(MN + √ MW ) for the random scheme.\nOrganization: We illustrate the motivating Topic Geometry in Section 3. We then present our threestep algorithm in Section 4 with intuitions and computational complexity. Statistical correctness of each step of proposed approach are summarized in Section 5. We address practical issues in Section 6."
    }, {
      "heading" : "3. Topic Geometry",
      "text" : "Recall that X and A respectively denote the W ×M empirical and actual document word distribution matrices, and A = βθ, where β is the latent topic word distribution matrix and θ is the underlying weight matrix. Let Ã, θ̃ and X̃ denote the A, θ and X matrices after ℓ1 row normalization. We set β̃ = diag(A1)−1β diag(θ1), so that Ã = β̃θ̃. Let Xi and Ai respectively denote the i − th row of X and A representing the cross-document patterns of word i. We assume that β is separable (Def. 1). Let Ck be the set of novel words of topic k and let C0 be the set of non-novel words.\nThe geometric intuition underlying our approach is formulated in the following proposition :\nProposition 1. Let β be separable. Then for all novel words i ∈ Cj, Ãi = θ̃j and for all non-novel words i ∈ C0, Ãi is a convex combination of θ̃j’s, for j = 1, . . . ,K.\nProof: Note that for all i,\nK∑\nk=1\nβ̃ik = 1\nand for all i ∈ Cj , β̃ij = 1. Moreover, we have\nÃi =\nK∑\nk=1\nβ̃ikθ̃k\nHence Ãi = θ̃j for i ∈ Cj . In addition, Ãi = K∑\nk=1\nβ̃ikθ̃k\nfor i ∈ C0. Fig. 1 illustrates this geometry. Without loss of generality, we could assume that novel word vectors θ̃i are not in the convex hull of the other rows of θ̃. Hence, The problem of identifying novel words reduces to finding extreme points of all Ãi’s.\nFurthermore, retrieving topic matrix β is straightforward given all K distinct novel words :\nProposition 2. If the matrix A and K distinct novel words {i1, . . . , iK} are given, then β can be calculated using W linear regressions.\nProof: By Proposition 1, we have θ̃ = (A⊤i1 , . . . ,A ⊤ iK )\n⊤. Next Ãi = β̃iθ̃. So β̃i can be computed by solving a linear system of equations. Specifically, if we let β′ = diag(A1)β̃ = β diag(θ1)−1, β can be obtained by column normalizing β′.\nProposition 1 and 2 validate the approach to estimate β via identifying novel words given access to A. However, only X, a realization of A, is available in the real problem which is not close to A in typical settings of interest (N ≪ W ). However, even when the number of samples per document (N) is limited, if we collect enough documents (M → ∞), the proposed algorithm could still asymptotically estimate β with arbitrary precision, as we will discuss in the following sections."
    }, {
      "heading" : "4. Proposed Algorithm",
      "text" : "The geometric intuition mentioned in Propositions 1 and 2 motivates the following three-step approach for topic discovery :\n(1) Novel Word Detection: Given the empirical word-by-document matrix X, extract the set of all novel words I. We present variants of projection-based algorithms in Sec. 4.1.\n(2) Novel Word Clustering: Given a set of novel words I with |I| ≥ K , cluster them into K groups corresponding to K topics. Pick a representative for each group. We adopt a distance based clustering algorithm. (Sec. 4.2).\n(3) Topic Estimation: Estimate topic matrix as suggested in Proposition 2 by constrained linear regression. (Section 4.3)."
    }, {
      "heading" : "4.1. Novel Word Detection",
      "text" : "Fig. 1 illustrates the key insight to identify novel words as extreme points of some convex body. When we project every point of a convex body onto some direction d, the maximum and minimum correspond to extreme points of the convex object. Our proposed approaches, data dependent and random projection, both exploit this fact. They only differ in the choice of projected directions."
    }, {
      "heading" : "A. Data Dependent Projections (DDP)",
      "text" : "To simplify our analysis, we randomly split each document into two subsets, and obtain two statistically independent document collections X and X′, both distributed as A, and then row normalize as X̃ and X̃′. For some threshold, d, to be specified later, and for each word i, we consider the set, Ji, of all other words that are sufficiently different from word i in the following sense:\nJi = {j | M(X̃i − X̃j)(X̃′i − X̃′j)⊤ ≥ d/2} (1)\nWe then declare word i as a novel word if all words j ∈ Ji are uniformly uncorrelated to word i with some margin, γ/2 to be specified later.\nM〈X̃i, X̃′i〉 ≥ M〈X̃i, X̃′j〉+ γ/2, ∀j ∈ Ji (2)\nThe correctness of DDP Algorithm is established by the following Proposition and will be further discussed in section 5. The proof is given in the Supplementary section.\nProposition 3. Suppose conditions P1 and P2 (will be defined in section 5) on prior distribution of θ hold. Then, there exists two positive constants d and γ such that if i is a novel word, for all j ∈ Ji, M〈X̃i, X̃′i〉 − M〈X̃i, X̃′j〉 ≥ γ/2 with high probability (converging to one as M → ∞). In addition, if i is a non-novel word, there exists some j ∈ Ji such that M〈X̃i, X̃′i〉 − M〈X̃i, X̃′j〉 ≤ γ/2 with high probability.\nAlgorithm 1 Novel Word Detection - DDP\n1: Input X̃, X̃′, d, γ,K 2: Output: The indices of the novel words I 3: C ← M X̃′X̃⊤ 4: I ← ∅ 5: for all 1 ≤ i ≤ W do 6: Ji ← All indices j 6= i : Ci,i − 2Ci,j + Cj,j ≥ d2 7: if ∀j ∈ Ji : Ci,i − Ci,j ≥ γ/2 then 8: I ← I ∪ {i} 9: end if\n10: end for\nThe algorithm is elaborated in Algorithm 1. The running time of the algorithm is summarized in the following proposition. Detailed justification is provided in the Supplementary section.\nProposition 4. The running time of Algorithm 1 is O(MN2 +W 2).\nProof Sketch. Note that X is sparse since N ≪ W . Hence by exploiting the sparsity C = MXX′⊤ can be computed in O(MN2 +W ) time. For each word i, finding Ji and calculating Ci,i−Ci,j ≥ γ/2 costO(W 2) time in the worst case."
    }, {
      "heading" : "B. Random Projections (RP)",
      "text" : "DDP usesW different directions to find all the extreme points. Here we use random directions instead. This significantly reduces the time complexity by decreasing the number of required projections.\nThe Random Projection Algorithm (RP) uses roughly P = O(K) random directions drawn uniformly iid over the unit sphere. For each direction d, we project all X̃i’s onto it and choose the maximum and minimum. Note that X̃id will converge to Ãid conditioned on d\nAlgorithm 2 Novel Word Detection - RP\n1: Input X̃, P 2: Output : The indices of the novel words I 3: I ← ∅ 4: for all 1 ≤ j ≤ P do 5: Generate d ∼ Uniform(unit-sphere in RM ) 6: imax = argmax X̃id, imin = argmax X̃id 7: I ← I ∪ {imax, imin} 8: end for\nand θ as M increases. Moreover, only for the extreme points i, Ãid can be the maximum or minimum projection value. This provides intuition of consistency for RP. Since the directions are independent, we expect to find all the novel words using P = O(K) number of random projections."
    }, {
      "heading" : "C. Random Projections with Binning",
      "text" : "Another alternative to RP is a Binning algorithm which is computationally more efficient. Here the corpus is split into √ M equal sized bins. For each bin j a random direction d(j) is chosen and the word with the maximum projection along d(j) is chosen as a winner. Then, we find the number of wins for each word i. We then divide these winning frequencies by √ M as an estimate for pi , Pr(∀j 6= i : Ãid ≥ Ãjd). pi can be shown to be zero for all non-novel words. For nondegenerate prior over θ, these probabilities converge to strictly positive values for novel words. Hence, estimating pi’s helps in identifying novel words. We then choose the indices of O(K) largest pi values as novel words. The Binning algorithm is outlined in Algorithm 3.\nAlgorithm 3 Novel Word Detection - Binning\n1: Input : X̃, X̃′, d, K 2: Output : The indices of the novel words I 3: Split documents in X into √ M equal sized groups\nof documents X(1), . . . ,X( √ M) and normalize each one separately to obtain X̃(1), . . . , X̃( √ M) as well.\n4: for all 1 ≤ j ≤ √ M do 5: d(j) ← a sample from U(S √ M−1) 6: l ← argmax 1≤i≤W X̃ (j) i d (j) 7: p̂ (j) l ← p̂ (j) l + 1 8: end for 9: for all 1 ≤ i ≤ W do\n10: p̂i ← 1√M ∑√M j=1 p̂ (j) i 11: end for 12: k ← 0, I ← ∅ and i ← 1 13: repeat 14: j ← the index of the ith largest value of\n(p̂1, . . . , p̂W )\n15: if I = ∅ or ∀l ∈ I : M(X̃j−X̃l)(X̃′j−X̃′l) ≥ d/2 then 16: I ← I ∪ {j} 17: k ← k + 1 18: end if 19: i ← i+ 1 20: until k = K\nIn contrast with DDP, the RP algorithm is completely agnostic and parameter-free. This means that it requires no parameters like d and γ to find the novel words. Moreover, it significantly reduces the computational complexity :\nProposition 5. The running times of the RP and Binning algorithms are O(MNK+WK) and O(MN+\n√ MW ), respectively.\nProof. We will sketch the proof and provide a more detailed justification in the Supplementary section. Note that the number of operations needed to find the projections is O(MN+W ) in Binning and O(MNK+W ) in RP. In addition, finding the the maximum takes O(WK) for RP and O( √ MW ) for Binning. In sum,\nit takes O(MNK+WK) for RP and O(MN+ √ MW ) for Binning to find all the novel words."
    }, {
      "heading" : "4.2. Novel Word Clustering",
      "text" : "Since there may be multiple novel words for a single topic, our DDP or RP algorithm can extract multiple novel words for each topic. This necessitates clustering to group the copies. We can show that our clustering scheme is consistent if we assume that R = 1M E(θθ\n⊤) is positive definite: Proposition 6. Let Ci,j , MX̃iX̃ ′⊤ j , and Di,j , Ci,i − 2Ci,j + Cj,j. If R is positive definite, then Di,j converges to zero in probability whenever i and j are novel words of the same topic as M → ∞. Moreover, if i and j are novel words of different types, it converges in probability to some strictly positive value greater than some constant d .\nThe proof is presented in the Supplementary section. As the Proposition 6 suggests, we construct a bi-\nAlgorithm 4 Novel Word Clustering\n1: Input : I, X̃, X̃′, d, K 2: Output : J which is a set of K novel words of\ndistinct topics 3: C ← M X̃′X̃⊤ 4: B ← a |I| × |I| zero matrix 5: for all i, j ∈ I, i 6= j do 6: if Ci,i − 2Ci,j + Cj,j ≤ d/2 then 7: Bi,j ← 1 8: end if 9: end for\n10: J ← ∅ 11: for all 1 ≤ j ≤ K do 12: c ← one of the indices of the jth connected component vertices in B 13: J ← J ∪ {c} 14: end for\nnary graph with its vertices correspond to the novel words. An edge between word i and j is established if Di,j ≤ d/2. Then, the clustering reduces to finding K connected components. The procedure is described in Algorithm 4.\nIn Algorithm 4, we simply choose any word of a cluster as the representative for each topic. This is simply\nfor theoretical analysis. However, we could set the representative to be the average of data points in each cluster, which is more noise resilient."
    }, {
      "heading" : "4.3. Topic Matrix Estimation",
      "text" : "Given K novel words of different topics (J ), we could directly estimate (β) as in Proposition 2. This is described in Algorithm 5. We note that this part of the algorithm is similar to some other topic modeling approaches, which exploit separability. Consistency of this step is also validated in (Arora et al., 2012b). In fact, one may use the convergence of extremum estimators (Amemiya, 1985) to show the consistency of this step.\nAlgorithm 5 Topic Matrix Estimation\n1: Input: J = {j1, . . . , jK}, X, X′ 2: Output: β̂, which is the estimation of β matrix 3: Y = (X̃⊤j1 , . . . , X̃ ⊤ jK )⊤,Y′ = (X̃′⊤j1 , . . . , X̃ ′⊤ jK )⊤ 4: for all 1 ≤ i ≤ W do 5: β̂i ← ( 1MXi1) argmin\nbj≥0, ∑ K j=1 bj=1\nM(X̃i − bY)(X̃′i −\nbY′)⊤\n6: end for 7: column normalize β̂"
    }, {
      "heading" : "5. Statistical Complexity Analysis",
      "text" : "In this section, we describe the sample complexity bound for each step of our algorithm. Specifically, we provide guarantees for DDP algorithm under some mild assumptions on the distribution over θ. The analysis of the random projection algorithm is much more involved and requires elaborate arguments. We will omit it in this paper.\nWe require following technical assumptions on the correlation matrix R and the mean vector a of θ :\n(P1) R is positive definite with its minimum eigenvalue being lower bounded by λ∧ > 0. In addition, ∀i, ai ≥ a∧ > 0. (P2) There exists a positive value ζ such that for i 6= j, Ri,i/(aiai)−Ri,j/(aiaj) ≥ ζ. The second condition captures the following intuition : if two novel words are from different topics, they must appear in a substantial number of distinct documents. Note that for two novel words i and j of different topics, MÃi(Ãi − Ãj)⊤ p−→ Ri,i/(aiai) − Ri,j/(aiaj). Hence, this requirement means that M(Ãi − Ãj) should be fairly distant from the origin, which implies that the number of documents these two words co-\noccur in, with similar probabilities, should be small. This is a reasonable assumption, since otherwise we would rather group two related topics into one. In fact, we show in the Supplementary section (Section A.5) that both conditions hold for the Dirichlet distribution, which is a traditional choice for the prior distribution in topic modeling. Moreover, we have tested the validity of these assumptions numerically for the logistic normal distribution (with non-degenerate covariance matrices), which is used in Correlated Topic Modeling (CTM) (Blei & Lafferty, 2007)."
    }, {
      "heading" : "5.1. Novel Word Detection Consistency",
      "text" : "In this section, we provide analysis only for the DDP Algorithm. The sample complexity analysis of the randomized projection algorithms is however more involved and is the subject of the ongoing research. Suppose P1 and P2 hold. Denote β∧ and λ∧ to be positive lower bounds on non-zero elements of β and minimum eigenvalue of R, respectively. We have:\nTheorem 1. For parameter choices d = λ∧β2∧ and γ = ζa∧β∧ the DDP algorithm is consistent as M → ∞. Specifically, true novel and non-novel words are asymptotically declared as novel and non-novel, respectively. Furthermore, for\nM ≥ C1\n( logW + log ( 1 δ1 ))\nβ2∧η8 min(λ 2 ∧β 2 ∧, ζ2a 2 ∧)\nwhere C1 is a constant, Algorithm 1 finds all novel words without any outlier with probability at least 1− δ1, where η = min\n1≤i≤W βia.\nProof Sketch. The detailed justification is provided in the Supplementary section. The main idea of the proof is a sequence of statements :\n• Given P1, for a novel word i, Ji defined in the Algorithm 1 is a subset of J∗i asymptotically with high probability, where J∗i = {j : supp(βj) 6= supp(βi)}. Moreover Ji is a superset of J∗i with high probability for a non-novel word with J∗i = {j : | supp(βj)| = 1}.\n• Given P2, for a novel word i, Ci,i−Ci,j converges to a strictly positive value greater than γ for j ∈ J∗i , and if i is non-novel, ∃j ∈ J∗i such that Ci,i − Ci,j converges to a non-positive value.\nThese statements imply Proposition 3, which proves the consistency of the DDP Algorithm.\nThe term η−8 seems to be the dominating factor in the sample complexity bound. Basically,\nη = min 1≤i≤W\n1 M E(Xi1) represents the minimum propor-\ntion of documents that a word would appear in. This is not surprising as the rate of convergence of Ci,j = M〈X̃i, X̃′j〉 is dependent on the values of 1M E(Xi1) and 1M E(Xj1). As these values are decreased, Ci,j converges to a larger value and the convergence get slower. In another view, given that the number of words per document N is bounded, in order to have Ci,j converge, a large number of documents is needed to observe all the words sufficiently. It is remarkable that a similar term p−6 would also arise in the sample complexity bound of (Arora et al., 2012b), where p is the minimum non-zero element of diagonal part of β. It may be noted that although it seems that the sample complexity bound scales logarithmically with W , η and p would be decreased typically as W increases."
    }, {
      "heading" : "5.2. Novel Word Clustering Consistency",
      "text" : "We similarly prove the consistency and sample complexity of the novel word clustering algorithm :\nTheorem 2. For d = λ∧β2∧, given all true novel words as the input, the clustering algorithm, Algorithm 4 (ClusterNovelWords) asymptotically (as M → ∞ recovers K novel word indices of different types, namely, the support of the corresponding β rows are different for any two retrieved indices. Furthermore, if\nM ≥ C2\n( logW + log ( 1 δ2 ))\nη8λ2∧β 4 ∧\nthen Algorithm 4 clusters all novel words correctly with probability at least 1− δ2.\nProof Sketch. More detailed analysis is provided in the Supplementary section. We can show that Ci,i−2Ci,j +Cj,j converges to a strictly positive value d if i and j are novel words of different topics. Moreover, it converges to zero if they are novel words of the same topic. Hence all novel words of the same topic are connected in the graph with high probability asymptotically. Moreover, there would not be an edge between the novel words of different topics with high probability. Therefore, the connected components of the graph corresponds to the true clusters asymptotically. The detailed discussion of the convergence rate is provided in the Supplementary section.\nIt is noticeable that the sample complexity of the clustering is similar to that of the novel word detection. This means that the hardness of novel word detection and distance based clustering using the proposed algorithms are almost the same."
    }, {
      "heading" : "5.3. Topic Estimation Consistency",
      "text" : "Finally, we show that the topic estimation by regression is also consistent.\nTheorem 3. Suppose that Algorithm 5 outputs β̂ given the indices of K distinct novel words. Then, β̂ p−→ β. Specifically, if\nM ≥ C3W 4(log(W ) + log(K) + log(1/δ3))\nλ2∧η8ǫ4a 8 ∧\nthen for all i and j, β̂i,j will be ǫ close to βi,j with probability at least 1 − δ3, with ǫ < 1, C3 being a constant, a∧ = mini ai and η = min\n1≤i≤W βia.\nProof Sketch. We will provide a detailed analysis in the Supplementary section. To prove the consistency of the regression algorithm, we will use a consistency result for the extremum estimators : If we assume QM (β) to be a stochastic objective function which is minimized at β̂ under the constraint β ∈ Θ (for a compact Θ), and QM (β) converges uniformly to Q̄(β), which in turn is minimized uniquely in β∗, then β̂ p−→ β∗ (Amemiya, 1985). In our setting, we may take QM to be the objective function in Algorithm 5. Then, QM (b) p−→ Q̄(b) = bDRDb⊤−2bDR β ⊤ i\nβia + βi βia R β⊤i βia ,\nwhere D = diag(a)−1. Note that if R is positive definite, Q̄ is uniquely minimized at b∗ = βi\nβia D−1, which\nsatisfies the conditions of the optimization. Moreover, QM converges to Q uniformly as a result of Lipschitz continuity of QM . Therefore, according to Slutsky’s theorem, ( 1MXi1)b ∗ = β̂i converges to βiD−1, and hence the column normalization of β̂ converges to β. We will provide a more detailed analysis of this part in the Supplementary section.\nIn sum, consider the approach outlined at the beginning of section 4 based on data-dependent projections method, and assume that β̂ is the output. Then,\nTheorem 4. The output of the topic modeling algorithm β̂ converges in probability to β element-wise. To be precise, if\nM ≥ max { C′2W\n4 log WKδ λ2∧η8ǫ4a 8 ∧ , C′1 log W δ β2∧η8 min(λ 2 ∧β 2 ∧, ζ2a 2 ∧)\n}\nthen with probability at least 1 − 3δ, for all i and k, β̂i,k will be ǫ close to βi,k, with ǫ < 1, C ′ 1 and C ′ 2 being two constants.\nThe proof is a combination of Theorems 1, 2 and 3."
    }, {
      "heading" : "6. Experimental Results",
      "text" : ""
    }, {
      "heading" : "6.1. Practical Considerations",
      "text" : "DDP algorithm requires two parameters γ and d. In practice, we can apply DDP without knowing them adaptively and agnostically. Note that d is for the construction of Ji. We can otherwise construct Ji by finding r < W words that are maximally distant from i in the sense of Eq. 1. To bypass γ, we can rank the values of minj∈Ji M〈X̃i, X̃′i〉 −M〈X̃i, X̃′j〉 across all i and declare the topmost s values as the novel words.\nThe clustering algorithm also requires parameter d. Note that d is just for thresholding a 0 − 1 weighted graph. In practice, we could avoid hard thresholding by using exp(−(Ci,i − 2Ci,j +Cj,j)) as weights for the graph and apply spectral clustering. To point out, typically the size of I in Algorithm 4 is of the same order as K. Hence the spectral clustering is on a relative small graph which typically adds O(K3) computational complexity.\nImplementation Details: We choose the parameters of the DDP and RP in the following way. For DDP in all datasets except the Donoho image corpus, we use the agnostic algorithm discussed in section 6.1 with r = W/2. Moreover, we take s = 10 × K. For the image dataset, we used d = 1 and γ = 3. For RP, we set the number of projections P ≈ 50 × K in all datasets to obtain the results."
    }, {
      "heading" : "6.2. Synthetic Dataset",
      "text" : "In this section, we validate our algorithm on synthetic examples. We generate a W ×K separable topic matrix β with W1/K > 1 novel words per topic as follows: first, iid 1×K row-vectors corresponding to nonnovel words are generated uniformly on the probability simplex. Then, W1 iid Uniform[0, 1] values are generated for the nonzero entries in the rows of novel words. The resulting matrix is then column-normalized to get one realization of β. Let ρ := W1/W . Next, M iid K × 1 column-vectors are generated for the θ matrix according to a Dirichlet prior c\nK∏ i=1 θαi−1i . Following\n(Griffiths & Steyvers, 2004), we set αi = 0.1 for all i. Finally, we obtain X by generating N iid words for each document.\nFor different settings of W , ρ, K, M and N , we calculate the ℓ1 distance of the estimated topic matrix to the ground truth after finding the best matching between two sets of topics. For each setting we average the error over 50 random samples. For RP & DDP we set parameters as discussed in the implementation details.\nWe compare the DDP and RP against the Gibbs sampling approach (Griffiths & Steyvers, 2004) (Gibbs), a state-of-art NMF-based algorithm (Tan & Févotte, in press) (NMF) and the most recent practical provable algorithm in (Arora et al., 2012a) (RecL2). The NMF algorithm is chosen because it compensates for the type of noise in our topic model. Fig. 2 depicts the estimation error as a function of the number of documents M (Upper) and the number of words/document N (bottom). RP and DDP have similar performance and are uniformly better than comparable techniques. Gibbs performs relatively poor in the first setting and NMF in the second. RecL2 perform worse in all the settings. Note that M is relatively small (≤ 1, 000) compared to W = 500. DDP/RP outperform other methods with fairly small sample size. Meanwhile, as is also observed in (Arora et al., 2012a), RecL2 has a poor performance with small M .\n6.3. Swimmer Image Dataset\nIn this section we apply our algorithm to the synthetic swimmer image dataset introduced in (Donoho & Stodden, 2004). There are M = 256 binary images, each with W = 32 × 32 = 1024 pixels. Each image represents a swimmer composed of four limbs, each of which can be in one of 4 distinct positions, and a torso. We interpret pixel positions (i, j) as words. Each image is interpreted as a document composed of pixel positions with non-zero values. Since each position of a limb features some unique pixels in the image, the topic matrix β satisfies the separability assumption with K = 16 “ground truth” topics that\ncorrespond to 16 single limb positions.\nFollowing the setting of (Tan & Févotte, in press), we set body pixel values to 10 and background pixel values to 1. We then take each “clean” image, suitably normalized, as an underlying distribution across pixels and generate a “noisy” document of N = 200 iid “words” according to the topic model. Examples are shown in Fig. 3. We then apply RP and DDP algorithms to the “noisy” dataset and compare against Gibbs (Griffiths & Steyvers, 2004), NMF (Tan & Févotte, in press), and RecL2 (Arora et al., 2012a). Results are shown in Figs. 4 and 5. We set the parameters as discussed in the implementation details.\nThis dataset is a good validation test for different algorithms since the ground truth topics are known and unique. As we see in Fig. 4, both Gibbs and NMF produce topics that do not correspond to any pure left/right arm/leg positions. Indeed, many of them are composed of multiple limbs. Nevertheless, as shown in Fig. 5, no such errors are realized in RP and DDP and our topic-estimates are closer to the ground truth images. In the meantime, RecL2 algorithm failed to work even with the clean data. Although it also extracts extreme points of a convex body, the algorithm additionally requires these points to be linearly independent. It is possible that extreme points of a convex body are linearly dependent (for example, a 2-D square on a 3-D simplex). This is exactly the case in the swimmer dataset. As we see in the last row in Fig. 5, RecL2 produces only a few topics close to ground truth. Its extracted topics for the noisy im-\nages are shown in Fig. 4. Results of RecL2 on noisy images are no close to ground truth as shown in Fig. 4."
    }, {
      "heading" : "6.4. Real World Text Corpora",
      "text" : "In this section, we apply our algorithm on two different real world text corpora from (Frank & Asuncion,\n2010). The smaller corpus is NIPS proceedings dataset with M = 1, 700 documents, a vocabulary of W = 14, 036 words and an average of N ≈ 900 words in each document. Another is a large corpus New York (NY) Times articles dataset, with M = 300, 000, W = 102, 660, and N ≈ 300. The vocabulary is obtained by deleting a standard “stop” word list used in computational linguistics, including numbers, individual characters, and some common English words such as “the”.\nIn order to compare with the practical algorithm in (Arora et al., 2012a), we followed the same pruning in their experiment setting to shrink the vocabulary size to W = 2, 500 for NIPS and W = 15, 000 for NY Times. Following typical settings in (Blei, 2012) and (Arora et al., 2012a), we set K = 40 for NIPS and K = 100 for NY Times. We set our parameters as discussed in implementation details.\nWe compare DDP and RP algorithms against RecL2 (Arora et al., 2012a) and a practically widely successful algorithm (Griffiths & Steyvers, 2004)(Gibbs). Table 1 and 22 depicts typical topics extracted by the different methods. For each topic, we show its most frequent words, listed in descending order of the estimated probabilities. Two topics extracted by different algorithms are grouped if they are close in ℓ1 distance.\nDifferent algorithms extract some fraction of similar topics which are easy to recognize. Table 1 indicates most of the topics extracted by RP and DDP are similar and are comparable with that of Gibbs. We observe that the recognizable themes formed with DDP or RP topics are more abundant than that by RecL2. For example, topic on “chip design” as shown in the first panel in Table 1 is not extracted by RecL2, and topics in Table 2 on “weather” and “emotions” are missing in RecL2. Meanwhile, RecL2 method produces some obscure topics. For example, in the last panel of Table 1, RecL2 contains more than one theme, and in the last panel of Table 2 RecL2 produce some unfathomable combination of words. More details about the topics extracted are given in the Supplementary section."
    }, {
      "heading" : "7. Conclusion and Discussion",
      "text" : "We summarize our proposed approaches (DDP, Binning and RP) while comparing with other existing methods in terms of assumptions, computational complexity and sample complexity (see Table 3). Among the list of the algorithms, DDP and RecL2 are the best and competitive methods. While the DDP algorithm has a polynomial sample complexity, its running time\n2the zzz prefix annotates the named entity.\nis better than that of RecL2, which depends on 1/ǫ2. Although ǫ seems to be independent of W , by increasing W the elements of β would be decreased and the precision (ǫ) which is needed to recover β would be decreased. This results in a larger time complexity in RecL2. In contrast, time complexity of DDP does not scale with ǫ. On the other hand, the sample complexity of both DDP and RecL2, while polynomially scaling, depend on too many different terms. This makes the comparison of these sample complexities difficult.\nHowever, terms corresponding to similar concepts appeared in the two bounds. For example, it can be seen that pa∧ ≈ η, because the novel words are possibly the most rare words. Moreover, λ∧ and γ which are the ℓ2 and ℓ1 condition numbers of R are closely related. Finally, a = a∨a∧ , with a∨ and a∧ being the maximum and minimum values in a."
    }, {
      "heading" : "Supplementary Materials",
      "text" : ""
    }, {
      "heading" : "A. Proofs",
      "text" : "Given β is separable, we can reorder the rows of β such\nthat β = [ D β′ ] , where D is diagonal. We will assume the same structure for β throughout the section."
    }, {
      "heading" : "A.1. Proof of Proposition 3",
      "text" : "Proposition 3 is a direct result of Theorem 1. Please refer to section A.7 for more details."
    }, {
      "heading" : "A.2. Proof of Proposition 4",
      "text" : "Recall that Proposition 4 summarizes the computational complexity of the DDP Algorithm 1. Here we provide more details.\nProposition 4 (in Section 4.1). The running time of Data dependent projection Algorithm DDP 1 is O(MN2 +W 2). Proof : We can show that, because of the sparsity of X, C = MXX′⊤ can be computed in O(MN2 + W ) time. First, note that C is a scaled word-word cooccurrence matrix, which can be calculated by adding up the co-occurrence matrices of each document. This running time can be achieved, if all W words in the vocabulary are first indexed by a hash table (which takes O(W )). Then, since each document consists of at most N words, O(N2) time is needed to compute the co-occurrence matrix of each document. Finally, the summation of these matrices to obtain C would cost O(MN2), which results in total O(MN2 + W ) time complexity. Moreover, for each word i, we have to find Ji and test whether Ci,i − Ci,j ≥ γ/2 for all j ∈ Ji. Clearly, the cost to do this is O(W 2) in the worst case."
    }, {
      "heading" : "A.3. Proof of Proposition 5",
      "text" : "Recall that Proposition 5 summarizes the computational complexity of RP ( Algorithm 2) and Binning (and see Section B in appendix for more details). Here we provide a more detailed proof.\nProposition 5 (in Section 4.1) Running time of RP (Algorithm 2) and Binning algorithm (in Appendix Section B) are O(MNK+WK) and O(MN+ √ MW ), respectively.\nProof : Note that number of operations needed to find the projections is O(MN + W ) in Binning and O(MNK + W ) in RP. This can be achieved by first indexing the words by a hash table and then finding\nthe projection of each document along the corresponding component of the random directions. Clearly, that takes O(N) time for each document. In addition, finding the word with the maximum projection value (in RP) and the winner in each bin (in Binning) will take O(W ). This counts to be O(WK) for all projections in RP and O( √ MW ) for all of the bins in Binning. Adding running time of these two parts, the computational complexity of the RP and Binning algorithms will be O(MNK + WK) and O(MN + √ MW ), respectively."
    }, {
      "heading" : "A.4. Proof of Proposition 6",
      "text" : "Proposition 6 (in Section 4.2) is a direct result of Theorem 2. Please read section A.8 for the detailed proof.\nA.5. Validation of Assumptions in Section 5 for Dirichelet Distribution\nIn this section, we prove the validity of the assumptions P1 and P2 which were made in Section 5. For x ∈ RK with ∑Ki=1 xi = 1, xi ≥ 0, x ∼ Dir(α1, . . . , αK) has pdf P(x) = c ∏K i=1 x αi−1 i . Let α∧ = min 1≤i≤K αi and α0 = ∑K i=1 αi.\nProposition A.1 For a Dirichlet prior Dir(α1, . . . , αK):\n1. The correlation matrix R is positive definite with minimum eigenvalue λ∧ ≥ α∧α0(α0+1) ,\n2. ∀1 ≤ i 6= j ≤ K, Ri,iaiai − Ri,j aiaj = α0αi(α0+1) > 0.\nProof. The covariance matrix of Dir(α1, . . . , αK), denoted as Σ, can be written as\nΣi,j =\n{ −αiαj α2\n0 (α0+1) if i 6= j αi(α0−αi) α2\n0 (α0+1)\notherwise (3)\nCompactly we have Σ = 1 α2\n0 (α0+1)\n( −αα⊤ + α0 diag(α) )\nwith α = (α1, . . . , αK). The mean vector µ = 1 α0\nα. Hence we obtain\nR = 1\nα20(α0 + 1)\n( −αα⊤ + α0 diag(α) ) + 1\nα20 αα⊤\n= 1\nα0(α0 + 1)\n( αα⊤ + diag(α) )\nNote that αi > 0 for all i, αα ⊤ and diag(α) are positive definite. Hence R is strictly positive definite, with eigenvalues λi =\nαi α0(α0+1) . Therefore λ∧ ≥ α∧α0(α0+1) . The second property follows by directly plug in equation (3)."
    }, {
      "heading" : "A.6. Convergence Property of the co-occurrence Matrix",
      "text" : "In this section, we prove a set of Lemmas as ingredients to prove the main Theorems 1, 2 and 3 in Section 5. These Lemmas in sequence show :\n• Convergence of C = MX̃X̃′⊤; (Lemma 1) • Convergence of Ci,i − 2Ci,j + Cj,j to a strictly positive value if i, j are not novel words of the same topic; (Lemma 2) • Convergence of Ji to J∗i such that if i is novel, Ci,i − Ci,j converges to a strictly positive value for j ∈ J∗i , and if i is non-novel, ∃j ∈ J∗i such that Ci,i − Ci,j converges to a non-positive value (Lemmas 3 and 4).\nRecall that in Algorithm 1, C = MX̃X̃′⊤. Let’s fur-\nther define Ei,j = βi βia R β⊤j βja . η = min 1≤i≤W βia. Let R and a be the correlation matrix and mean vector of prior distribution of θ.\nBefore we dig into the proofs, we provide two limit analysis results of Slutsky’s theorem :\nProposition 7. For random variables Xn and Yn and real numbers x, y ≥ 0, if Pr(|Xn − x| ≥ ǫ) ≤ gn(ǫ) and Pr(|Yn − y| ≥ ǫ) ≤ hn(ǫ), then\nPr(|Xn/Yn−x/y| ≥ ǫ) ≤ gn (yǫ 4 ) +hn\n( ǫy2\n4x\n) +hn (y 2 )\nAnd if 0 ≤ x, y ≤ 1\nPr(|XnYn − xy| ≥ ǫ) ≤ gn ( ǫ 2 ) + hn ( ǫ 2 )\n+ gn\n( ǫ\n2y\n) + hn ( ǫ 2x )\nLemma 1. Let Ci,j , MX̃iX̃ ′ j . Then Ci,j p−→ Ei,j = βi βia R β⊤j βja . Specifically,\nPr (|Ci,j − Ei,j | ≥ ǫ) ≤ 8 exp(−Mǫ2η8/32)\nProof. By the definition of Ci,j , we have :\nCi,j = 1 MXiX ′⊤ j\n( 1MXi1)( 1 MX ′ j1)\np−→ E( 1MXiX ′⊤ j )\nE( 1MXi1)E( 1 MX ′ j1)\n(4)\nas M → ∞, where 1 = (1, 1, . . . , 1)⊤ and the convergence follows because of convergence of numerator and denominator and then applying the Slutsky’s theorem. The convergence of numerator and denominator are results of strong law of large numbers due to the fact that entries in Xi and X ′ i are independent.\nTo be precise, we have:\nE( 1MXiX ′⊤ j )\nE( 1MXi1)E( 1 MX ′ j1)\n= Eθ EX|θ( 1 MXiX ′⊤ j )\nEθ EX|θ( 1 MXi1)Eθ EX|θ( 1 MX ′ j1)\n= Eθ( 1MAiA ⊤ j )\nEθ( 1MAi1)Eθ( 1 MAj1)\n= Eθ( 1M βiθθ ⊤βj)\nEθ( 1M βiθ1)Eθ( 1 Mβjθ1)\n= βiRβ\n⊤ j\n(βia)(βja)\n=Ei,j\nTo show the convergence rate explicitly, we use proposition 7. For simplicity, define Ci,j =\nFi,j GiHj . Note that\nentries in Xi and X ′ i are independent and bounded, by Hoeffding’s inequality, we obtain:\nPr(|Fi,j − E(Fi,j)| ≥ ǫ) ≤ 2 exp(−2Mǫ2) Pr(|Gi − E(Gi)| ≥ ǫ) ≤ 2 exp(−2Mǫ2) Pr(|Hj − E(Hj)| ≥ ǫ) ≤ 2 exp(−2Mǫ2)\nHence,\nPr(|GiHj − E(Gi)E(Hj)| ≥ ǫ) ≤ 8 exp(−Mǫ2/2)\nand\nPr (∣∣∣∣ Fi,j GiHj − E(Fi,j) E(Gi)E(Hj) ∣∣∣∣ ≥ ǫ ) ≤\n2 exp(−Mǫ2(βjaβia)2/8)+8 exp(−Mǫ2(βjaβia)4/32) + 8 exp(−M(βjaβia)2/8)\n(5)\nLet η = min 1≤i≤W\nβia ≤ 1. We obtain\nPr (∣∣∣∣ Fi,j GiHj − E(Fi,j) E(Gi)E(Hj) ∣∣∣∣ ≥ ǫ )\n≤ 18 exp(−Mǫ2η8/32)\nCorollary 1. Ci,i−2Ci,j+Cj,j converges as M → ∞. The convergence rate is c1 exp(−Mc2ǫ2η8) for ǫ error, with c1 and c2 being constants in terms of M . Corollary 2. Ci,i − Ci,j converges as M → ∞. The convergence rate is d1 exp(−Md2ǫ2η8) for ǫ error, with d1 and d2 being constants in terms of M .\nRecall that we define Ck, k = 1, . . . ,K to be the novel words of topic k, and C0 to be the set of non-novel words. supp(βi) denotes the column indices of nonzero entries of a row vector βi of β matrix. Lemma 2. If i, j ∈ Ck, (i, j are novel words of the same topic), then Ci,i − 2Ci,j + Cj,j p−→ 0. Otherwise, ∀k, if i ∈ Ck, j /∈ Ck, then Ci,i− 2Ci,j +Cj,j p−→ f(i,j) ≥ d > 0 where d = λ∧β2∧. Especially, if i ∈ C0 and j /∈ C0, then Ci,i − 2Ci,j + Cj,j p−→ f(i,j) ≥ d > 0\nProof. It was shown in lemma 1 that Ci,j p−→\nβi βia R β⊤j βja , where R is the correlation matrix and a = (a1, . . . , aK) ⊤ is the mean of the prior. Hence Ci,i − 2Ci,j + Cj,j p−→ ( βi\nβia − βj βja\n) R ( βi\nβia − βj βja\n)\n≥ λ∧ ∥∥∥∥ βi\nβia − βj βja\n∥∥∥∥ 2\nNote that we’ve assumedR to be positive definite with its minimum eigenvalue lower bounded by a positive value, λ∧ > 0.\nIf i, j ∈ Ck for some k, then βiβia − βj βja = 0 and hence Ci,i − 2Ci,j + Cj,j p−→ 0 . Otherwise, if supp(βi) 6= supp(βj), then∥∥∥ βiβia − βj βja ∥∥∥ 2\n≥ β2∧, (note that βia ≤ 1) which proves the first part of the lemma.\nFor the second part, note that if i ∈ C0 and j /∈ C0, the support of βi and βj is necessarily different. Hence, the previous analysis directly leads to the conclusion.\nRecall that in Algorithm 1, Ji = {j : j 6= i, Ci,i − 2Ci,j + Cj,j ≥ d/2}. we have : Lemma 3. Ji converges in probability in the following senses:\n1. For a novel word i ∈ Ck, define J∗i = Ckc . Then for all novel words i, lim\nM→∞ Pr(Ji ⊆ J∗i ) = 1.\n2. For a nonnovel word i ∈ C0, define J∗i = C0c. Then for all non-novel words i, lim\nM→∞ Pr(Ji ⊇ J∗i ) = 1.\nProof. Let d , λ∧β2∧. According to the lemma 2, whenever supp(βj) 6= supp(βi), Di,j , Ci,i − 2Ci,j + Cj,j\np−→ f(i,j) ≥ d for the novel word i. In another word, for a novel word i ∈ Ck and j /∈ Ck, Di,j will be concentrated around a value greater than or equal to\nd. Hence, the probability that Di,j be less than d/2 will vanish. In addition, by union bound we have\nPr(Ji * J ∗ i ) ≤ Pr(Ji 6= J∗i )\n= Pr(∃j ∈ J∗i : j /∈ Ji) ≤ ∑\nj∈J∗ i\nPr(j /∈ Ji)\n≤ ∑\nj /∈Ck\nPr(Di,j ≤ d/2)\nSince ∑\nj /∈Ck Pr(Di,j ≤ d/2) is a finite sum of vanishing terms given i ∈ Ck, Pr(Ji * J∗i ) also vanish asM → ∞ and hence we prove the first part.\nFor the second part, note that for a non-novel word i ∈ C0,Di,j converges to a value no less than d provided that j /∈ C0 (according to the lemma 2). Hence\nPr(Ji + J ∗ i ) ≤ Pr(Ji 6= J∗i )\n= Pr(∃j ∈ J∗i : j /∈ Ji) ≤ ∑\nj∈J∗ i\nPr(j /∈ Ji)\n≤ ∑\nj /∈C0\nPr(Di,j ≤ d/2)\nSimilarly ∑\nj /∈C0 Pr(Di,j ≤ d/2) vanishes for a nonnovel word i ∈ C0 as M → ∞, Pr(Ji + J∗i ) will also vanish and hence concludes the second part.\nAs a result of Lemma 1, 2 and 3, the convergence rate of events in Lemma 3 is :\nCorollary 3. For a novel word i ∈ Ck we have Pr(Ji * J∗i ) ≤ Wc1 exp(−Mc3d2η8). And for a non-novel word i ∈ C0, Pr(Ji + J∗i ) ≤ Kc1 exp(−Mc4d2η8), where c1, c3, and c4 are constants and d = λ∧β2∧.\nLemma 4. If ∀i 6= j, Ri,iaiai − Ri,j aiaj ≥ ζ, we have the following results on the convergence of Ci,i − Ci,j :\n1. If i is a novel word, ∀j ∈ Ji ⊆ J∗i : Ci,i−Ci,j p−→\ng(i,j) ≥ γ > 0, where J∗i is defined in lemma 3, γ , ζa∧β∧ and a∧ is the minimum component of a. 2. If i is a non-novel word, ∃j ∈ J∗i such that Ci,i − Ci,j p−→ g(i,j) ≤ 0.\nProof. Let’s reorder the words so that i ∈ Ci. Using the equation (4), Ci,i p−→ Ri,iaiai and Ci,j p−→∑Kk=1 bk Ri,k aiak with bk , βj,kak∑ K l=1 βj,lal . Not that bk’s are non-negative and sum up to one.\nBy the assumption, Ri,i aiai − Ri,jaiaj ≥ ζ for j 6= i. Note that ∀j ∈ Ji ⊆ J∗i , there exists some index k 6= i such that bk 6= 0. Then\nCi,i − Ci,j p−→ Ri,i aiai\n− K∑\nk=1\nbk Ri,k aiak\n=\nK∑\nk=1\nbk ( Ri,i aiai − Ri,k aiak )\n≥ ζ ∑\nk 6=i bk\nSince βja ≤ 1, we have ∑\nk 6=i bk ≥ β∧a∧βja ≥ β∧a∧, and the first part of the lemma is concluded.\nTo prove the second part, note that for i ∈ C0 and j /∈ C0,\nCi,j p−→\nK∑\nk=1\nbk Rj,k ajak\nwith bk = βi,k βia . Now define :\nj∗i , argmax j∈J∗\ni\nK∑\nk=1\nbk Rj,k ajak\n(6)\nWe obtain,\nCi,i p−→\nK∑\nl=1\nbl\nK∑\nk=1\nbk Rl,k alak\n≤ K∑\nk=1\nbk Rj∗ i ,k\naj∗ i ak\nAs a result, Ci,i − Ci,j∗ i p−→ ∑Kl=1 bl ∑K k=1 bk Rl,k alak\n− ∑K\nk=1 bk Rj∗ i ,k\naj∗ i ak\n≤ 0 and the proof is complete."
    }, {
      "heading" : "A.7. Proof of Theorem 1",
      "text" : "Now we can prove the Theorem 1 in Section 5. To summarize the notations, let β∧ be a strictly positive lower bound on non-zero elements of β, λ∧ be the minimum eigenvalue of R, and a∧ be the minimum component of mean vector a. Further we define η = min\n1≤i≤W βia\nand ζ , min 1≤i6=j≤K Ri,i aiai − Ri,jaiaj > 0.\nTheorem 1 (in Section 5.1)\nFor parameter choices d = λ∧β2∧ and γ = ζa∧β∧ the DDP algorithm is consistent as M → ∞. Specifically, true novel and non-novel words are asymptotically declared as novel and non-novel, respectively. Furthermore, for\nM ≥ C1\n( logW + log ( 1 δ1 ))\nβ2∧η8 min(λ 2 ∧β 2 ∧, ζ2a 2 ∧)\nwhere C1 is a constant, Algorithm 1 finds all novel words without any outlier with probability at least 1− δ1, where η = min\n1≤i≤W βia.\nProof of Theorem 1. Suppose that i is a novel word. The probability that i is not detected by the DDP Algorithm can be written as\nPr(Ji * J ∗ i or (Ji ⊆ J∗i and ∃j ∈ Ji : Ci,i − Ci,j ≤ γ/2))\n≤ Pr(Ji * J∗i ) + Pr((Ji ⊆ J∗i and ∃j ∈ Ji : Ci,i − Ci,j ≤ γ/2)) ≤ Pr(Ji * J∗i ) + Pr(∃j ∈ J∗i : Ci,i − Ci,j ≤ γ/2) ≤ Pr(Ji * J∗i ) + ∑\nj∈J∗ i\nPr(Ci,i − Ci,j ≤ γ/2)\nThe first and second term in the right hand side converge to zero according to Lemma 3 and 4, respectively. Hence, this probability of failure in detecting i as a novel word converges to zero.\nOn the other hand, the probability of claiming a nonnovel word as a novel word by the Algorithm DDP can be written as :\nPr(Ji + J ∗ i or (Ji ⊇ J∗i and ∀j ∈ Ji : Ci,i − Ci,j ≥ γ/2))\n≤ Pr(Ji + J∗i ) + Pr((Ji ⊇ J∗i and ∀j ∈ Ji : Ci,i − Ci,j ≥ γ/2)) ≤ Pr(Ji + J∗i ) + Pr(∀j ∈ J∗i : Ci,i − Ci,j ≥ γ/2) ≤ Pr(Ji + J∗i ) + Pr(Ci,i − Ci,j∗i ≥ γ/2)\nwhere j∗i was defined in equation (6). We have shown in Lemma 3 and 4 that both of the probabilities in the right hand side converge to zero. This concludes the consistency of the algorithm.\nCombining the convergence rates given in the Corollaries 1, 2 and 3, the probability that the DDP Algorithm fails in finding all novel words without any outlier will be bounded by We1 exp(−Me2min(d2, γ2)η8), where e1 and e2 are constants and d and γ are defined in the Theorem."
    }, {
      "heading" : "A.8. Proof of Theorem 2",
      "text" : "Theorem 2 (in Section 5.2) For d = λ∧β2∧, given all true novel words as the input, the clustering algorithm, Algorithm 4 (ClusterNovelWords) asymptotically (as M → ∞ recovers K novel word indices of different types, namely, the support of the corresponding β rows are different for any two retrieved indices."
    }, {
      "heading" : "Furthermore, if",
      "text" : "M ≥ C2\n( logW + log ( 1 δ2 ))\nη8λ2∧β 4 ∧\nthen Algorithm 4 clusters all novel words correctly with probability at least 1− δ2.\nProof of Theorem 2. The statement follows using (|I|\n2\n)\nnumber of union bounds on the probability that Ci,i− 2Ci,j + Cj,j is outside an interval of the length d/2 centered around the value it converges to. The convergence rate of the related random variables are given in Lemma 1. Hence the probability that the clustering algorithm fails in clustering all the novel words truly is bounded by e1W\n2 exp(−Me2η8d2), where e1 and e2 are constants and d is defined in the theorem."
    }, {
      "heading" : "A.9. Proof of Theorem 3",
      "text" : "Theorem 3 (in Section 5.3) Suppose that Algorithm"
    }, {
      "heading" : "5 outputs β̂ given the indices of K distinct novel words. Then, β̂",
      "text" : "p−→ β. Specifically, if\nM ≥ C3W 4(log(W ) + log(K) + log(1/δ3))\nλ2∧η8ǫ4a 8 ∧\nthen for all i and j, β̂i,j will be ǫ close to βi,j with probability at least 1 − δ3, with ǫ < 1, C3 being a constant, a∧ = mini ai and η = min\n1≤i≤W βia.\nProof. We reorder the rows so that Y and Y′ be the first K rows of X and X′, respectively. For the optimization objective function in Algorithm 5, if i < K, b = ei achieves the minimum, where all components of ei are zero, except its i\nth component, which is one. Now fix i, we denote the objective function as QM (b) = M(X̃i − bY)(X̃′i − bY′)⊤, and denote the optimal solution as b∗M . By the previous lemmas, QM (b) p−→ Q̄(b) = bDRDb⊤−2bDR β ⊤ i\nβia + βi βia R β⊤i βia ,\nwhere D = diag(a)−1. Note that if R is positive definite, Q̄ is uniquely minimized at b∗ = βi\nβia D−1.\nFollowing the notation in Lemma 1 and its proof,\nPr (|Ci,j − Ei,j | ≥ ǫ) ≤ 8 exp(−Mǫ2η8/32)\nwhere Ci,j = MX̃iX̃ ⊤ j , Ei,j = βi βia R β⊤j βja , and η =\nmin 1≤i≤W βia. Note that b ∈ B = {b : 0 ≤ bk ≤ 1, ∑ bk = 1}. Therefore, ∀s, r ∈ {1, . . . ,K, i} :\n|Cs,r − Es,r| ≤ ǫ implies that\n∀b ∈ B :|QM (b)− Q̄(b)| ≤ |Ci,i − Ei,i|\n+\nK∑\nk=1\nbk|Ck,i − Ek,i|+ K∑\nk=1\nbk|Ci,k − Ei,k|\n+\nK∑\nr=1\nK∑\ns=1\nbrbs|Cr,s − Er,s|\n≤ 4ǫ\nHence\nPr ( ∃b ∈ B : |QM (b)− Q̄(b)| ≥ 4ǫ )\n≤ Pr (∃i, j ∈ {1, . . . ,K, i} : |Ci,j − Ei,j | ≥ ǫ) (7)\nUsing (K+1)2 union bounds for the right hand side of the equation 7, we obtain the following equation with c1 and c2 being two constants:\nPr ( ∃b ∈ B : |QM (b)− Q̄(b)| ≥ ǫ )\n≤ c1(K + 1)2 exp(−c2Mǫ2η8) (8)\nNow we show that b∗M converge to b ∗. Note that b∗ is the unique minimizer of the strictly convex function Q̄(b). The strict convexity of Q̄ is followed by the fact that R is assumed to be positive definite. Therefore, we have, ∀ǫ0 > 0, ∃δ > 0 such that ‖b − b∗‖ ≥ ǫ0 ⇒ Q̄(b)− Q̄(b∗) ≥ δ. Hence,\nPr(‖b∗M − b∗‖ ≥ ǫ0) ≤Pr(Q̄(b∗M )− Q̄(b∗) ≥ δ) ≤Pr(Q̄(b∗M )−QM (b∗M ) +QM (b∗M )−QM (b∗)+ QM (b\n∗)− Q̄(b∗) ≥ δ) (i)\n≤ Pr(Q̄(b∗M )−QM (b∗M ) +QM (b∗)− Q̄(b∗) ≥ δ) (ii)\n≤ Pr(2 sup b∈B |QM (b)− Q̄(b)| ≥ δ)\n≤Pr(∃b ∈ B : |QM (b)− Q̄(b)| ≥ δ/2) (iii)\n≤ c1(K + 1)2 exp ( −c2\n4 δ2η8M\n)\nwhere (i) follows because QM (b ∗ M ) −QM (b∗) ≤ 0 by definition, (ii) holds considering the fact that b,b∗ ∈ B and (iii) follows as a result of equation 8. For the ǫ0 and δ relationship, let y = b− b∗,\nQ̄(b)− Q̄(b∗) = y(DRD)y⊤ ≥ ‖y‖2λ∗\nwhere λ∗ > 0 is the minimum eigenvalue of DRD. Note that λ∗ ≥ ( min\n1≤j≤K a−1j ) 2λ∧, where λ∧ > 0 is a\nlower bound on the minimum eigenvalues of R. But 0 < aj ≤ 1, hence λ∗ ≥ λ∧. Hence we could set δ = λ∧ǫ20. In sum, we could obtain\nPr(‖b∗M − b∗‖ ≥ ǫ0) ≤ c1(K + 1)2 exp(−c′2Mǫ40λ2∧η8)\nfor the constants c1 and c′2. Or simply b ∗ M p−→ b∗. Note that before column normalization, we let β̂i = ( 1MXi1)(b ∗ M ). The convergence of the first term (to βia), as we have already verified in Lemma 1, and using Slutsky’s theorem, we get β̂i p−→ βiD−1. Hence after column normalization, which involves convergence of W random variables, by Slutsky’s theorem again we can prove that β̂i p−→ βi for any 1 ≤ i ≤ W . This concludes our proof and directly implies the convergence in the Mean-Square sense.\nTo show the exact convergence rate, we apply the Proposition 7. For β̂i before column normalization, note that 1MXi1 converges to βia with error probability 2 exp ( −2ǫ2M ) , we obtain\nPr(|β̂i,j−βi,jaj | ≥ ǫ) ≤ e1(K+1)2 exp(−e2λ2∧η8Mǫ4) + e3 exp ( −2e4ǫ2M )\nfor constants e1, . . . , e4. On the other hand, the column normalization factors can be obtained by 1⊤β̂. Denote normalization factor of the jth column by Pj =∑W\ni=1 β̂i,j and hence Pr(|Pj − aj | ≥ ǫ) ≤ e1W (K + 1)2 exp(−e2λ2∧η8Mǫ4/W 4) + e3W exp ( −e4ǫ2M/W 2 ) . Now using the Proposition 7 again we obtain that after column normalization,\nPr (∣∣∣∣∣ β̂i,j∑W\nk=1 β̂k,j − βi,j\n∣∣∣∣∣ ≥ ǫ )\n≤ f1(K + 1)2 exp(−f2λ2∧η8Mǫ4a4∧) + f3 exp ( −2f4ǫ2Ma2∧ )\n+ f5W (K + 1) 2 exp(−f6λ2∧η8Mǫ4a8∧/W 4) + f7W exp ( −f8ǫ2Ma4∧/W 2 )\nfor constants f1, . . . , f8 and a∧ being the minimum value of ai’s. Assuming ǫ < 1, we can simplify the previous expression to obtain\nPr (∣∣∣∣∣ β̂i,j∑W\nk=1 β̂k,j − βi,j\n∣∣∣∣∣ ≥ ǫ )\n≤ b1W (K + 1)2 exp(−b2λ2∧η8Mǫ4a8∧/W 4)\nfor constants b1 and b2. Finally, to get the error probability of the whole matrix, we can use WK union\nbounds. Hence we have :\nPr ( ∃i, j : ∣∣∣∣∣ β̂i,j∑W\nk=1 β̂k,j − βi,j\n∣∣∣∣∣ ≥ ǫ )\n≤ b1W 2K(K + 1)2 exp(−b2λ2∧η8Mǫ4a8∧/W 4)\nTherefore, the sample complexity of ǫ-close estimation of βi,j by the Algorithm 5 with probability at least 1− δ3 will be given by:\nM ≥ C ′W 4(log(W ) + log(K) + log(1/δ3))\nλ2∧η8ǫ4a 8 ∧"
    }, {
      "heading" : "B. Experiment results",
      "text" : "B.1. Sample Topics extracted on NIPS dataset\nTables 4, 5, 6, and 7 show the most frequent words in topics extracted by various algorithms on NIPS dataset. The words are listed in the descending order. There are M = 1, 700 documents. Average words per document is N ≈ 900. Vocabulary size is W = 2, 500. It is difficult and confusing to group four sets of topics. We simply show topics extracted by each algorithm individually.\nB.2. Sample Topics extracted on New York Times dataset\nTables 8 to 11 show the most frequent words in topics extracts by algorithms on NY Times dataset. There are M = 300, 000 documents. Average words per document is N ≈ 300. Vocabulary size is W = 15, 000."
    } ],
    "references" : [ {
      "title" : "Advanced econometrics",
      "author" : [ "T. Amemiya" ],
      "venue" : null,
      "citeRegEx" : "Amemiya,? \\Q1985\\E",
      "shortCiteRegEx" : "Amemiya",
      "year" : 1985
    }, {
      "title" : "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation",
      "author" : [ "A. Anandkumar", "D. Foster", "D. Hsu", "S. Kakade", "Y. Liu" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "A Practical Algorithm for Topic Modeling with Provable Guarantees",
      "author" : [ "S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "Zhu", "Michael" ],
      "venue" : null,
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning topic models – going beyond",
      "author" : [ "S. Arora", "R. Ge", "A. Moitra" ],
      "venue" : "SVD. arXiv:1204.1956v2 [cs.LG],",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "A correlated topic model of science. annals of applied statistics",
      "author" : [ "D. Blei", "J. Lafferty" ],
      "venue" : "Annals of Applied Statistics,",
      "citeRegEx" : "Blei and Lafferty,? \\Q2007\\E",
      "shortCiteRegEx" : "Blei and Lafferty",
      "year" : 2007
    }, {
      "title" : "Probabilistic topic models",
      "author" : [ "D.M. Blei" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Blei,? \\Q2012\\E",
      "shortCiteRegEx" : "Blei",
      "year" : 2012
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation",
      "author" : [ "A. Cichocki", "R. Zdunek", "A.H. Phan", "S. Amari" ],
      "venue" : null,
      "citeRegEx" : "Cichocki et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cichocki et al\\.",
      "year" : 2009
    }, {
      "title" : "When does non-negative matrix factorization give a correct decomposition into parts",
      "author" : [ "D. Donoho", "V. Stodden" ],
      "venue" : "In Advances in Neural Information Processing Systems 16,",
      "citeRegEx" : "Donoho and Stodden,? \\Q2004\\E",
      "shortCiteRegEx" : "Donoho and Stodden",
      "year" : 2004
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T. Griffiths", "M. Steyvers" ],
      "venue" : "In Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Griffiths and Steyvers,? \\Q2004\\E",
      "shortCiteRegEx" : "Griffiths and Steyvers",
      "year" : 2004
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization",
      "author" : [ "D.D. Lee", "H.S. Seung" ],
      "venue" : "Nature, 401(6755):788–791,",
      "citeRegEx" : "Lee and Seung,? \\Q1999\\E",
      "shortCiteRegEx" : "Lee and Seung",
      "year" : 1999
    }, {
      "title" : "Pachinko allocation: Dagstructured mixture models of topic correlations",
      "author" : [ "W. Li", "A. McCallum" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Li and McCallum,? \\Q2007\\E",
      "shortCiteRegEx" : "Li and McCallum",
      "year" : 2007
    }, {
      "title" : "Factoring nonnegative matrices with linear programs",
      "author" : [ "B. Recht", "C. Re", "J. Tropp", "V. Bittorf" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Recht et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "We adopt the classic “bags of words” modeling paradigm widely-used in probabilistic topic modeling (Blei, 2012).",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "To address the scenario where only the realization X is known and not A, several papers (Lee & Seung, 1999; Donoho & Stodden, 2004; Cichocki et al., 2009; Recht et al., 2012) attempt to minimize a regularized cost function.",
      "startOffset" : 88,
      "endOffset" : 174
    }, {
      "referenceID" : 12,
      "context" : "To address the scenario where only the realization X is known and not A, several papers (Lee & Seung, 1999; Donoho & Stodden, 2004; Cichocki et al., 2009; Recht et al., 2012) attempt to minimize a regularized cost function.",
      "startOffset" : 88,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012) is a statistical approach to topic modeling.",
      "startOffset" : 34,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012) is a statistical approach to topic modeling.",
      "startOffset" : 34,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "(Anandkumar et al., 2012) describe a novel method of moments approach.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "In fact, one may use the convergence of extremum estimators (Amemiya, 1985) to show the consistency of this step.",
      "startOffset" : 60,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "β̂ p −→ β∗ (Amemiya, 1985).",
      "startOffset" : 11,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "Following typical settings in (Blei, 2012) and (Arora et al.",
      "startOffset" : 30,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : ", 2012a); ECA from (Anandkumar et al., 2012); Gibbs from (Griffiths & Steyvers, 2004); NMF from (Lee & Seung, 1999).",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "R) max { C1aK 3 log(W ) ǫγ6p6 , C2a K log(W ) ǫ3γ4p4 } Separable β; Robust Simplicial Property of R Pr(Error) → 0; Requires Novel words to be linearly independent; ECA (Anandkumar et al., 2012) O(W 3 +MN) N/A : For the provided basic algorithm, the probability of error is at most 1/4 but does not converge to zero LDA model; The concentration parameter of the Dirichlet distribution α0 is known Requires solving SVD for large matrix, which makes it impractical; Pr(Error) 9 0 for the basic algorithm Gibbs (Griffiths & Steyvers, 2004) N/A N/A LDA model No convergence guarantee NMF (Tan & Févotte, in press) N/A N/A General model Non-convex optimization; No convergence guarantee",
      "startOffset" : 168,
      "endOffset" : 193
    } ],
    "year" : 2013,
    "abstractText" : "We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms based on data-dependent and random projections of word-frequency patterns to identify novel words and associated topics. We will also discuss the statistical guarantees of the data-dependent projections method based on two mild assumptions on the prior density of topic document matrix. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme.",
    "creator" : "LaTeX with hyperref package"
  }
}