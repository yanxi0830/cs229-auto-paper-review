{
  "name" : "1610.01986.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Active exploration in parameterized reinforcement learning",
    "authors" : [ "Mehdi Khamassi", "Costas Tzafestas" ],
    "emails" : [ "mehdi.khamassi@upmc.fr", "ktzaf@cs.ntua.gr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Reinforcement Learning, Exploration/Exploitation, Multi-Armed Bandits, Meta-Learning, Active Exploration, Parameterized/Structured Reinforcement Learning."
    }, {
      "heading" : "1. Introduction",
      "text" : "Important progresses have been made in recent years in reinforcement learning (RL) with continuous action spaces, permitting successful real-world applications such as Robotics applications (Kober and Peters, 2011; Stulp and Sigaud, 2013). Nevertheless, a recent review on reinforcement learning applied to Robotics (Kober et al., 2013) highlighted, among other points, that (i) a variety of algorithms have been developed, each being appropriate to specific tasks: model-based versus model-free, function approximation versus policy search, continuous versus discrete action spaces; (ii) important human knowledge is injected concerning the search in the parameter space, either by reducing it through learning from demonstration, or by pre-adjusting parameters such as the exploration rate based on the prior determination of the total number of episodes in the experiment. In particular, the balance between exploration and exploitation is often pre-determined with human prior knowledge and does not extend well to tasks with non-stationary reward functions.\nTo address the first issue, the recent proposal of RL algorithms in structured Parameterized Action Space Markov Decision Processes (PAMDP) (Masson and Konidaris, 2016; Hausknecht and Stone, 2016) seems to open a promising line of research. It combines a\nc©2016 Mehdi Khamassi and Costas Tzafestas.\nar X\niv :1\n61 0.\n01 98\n6v 1\n[ cs\n.L G\n] 6\nO ct\n2 01\nset of discrete actions Ad = {a1, a2, ..., ak}, each action a ∈ Ad featuring ma continuous parameters {θa1 , ..., θama} ∈ R ma . Actions are thus represented by tuples (a, θa1 , ..., θ a ma) and the overall action space is defined as A = ∪a∈Ad(a, θa1 , ..., θama). This framework has been successfully applied to simulations of a Robocup 2D soccer task where agents have to learn to timely select between discrete actions such as running, turning or kicking the ball, and should learn at the same time with which speed to run, which angle to turn or which strength to kick. To ensure algorithm convergence, Masson and Konidaris (2016) alternate between learning phases: (i) given a fixed policy for parameter selection, they use Q-Learning to optimize the policy discrete action selection; (ii) Next, they fix the policy for discrete action selection and use a policy search method to optimize the parameter selection. In contrast, Hausknecht and Stone (2016) learn both in parallel by employing a parameterized actor that learns both discrete actions and parameters, and a parameterized critic that learns only the action-value function. Instead of relying on an external policy search procedure, they are thus able to directly query the critic for gradients.\nNevertheless, the exploration-exploitation trade-off is fixed in these methods, thus falling into the second issue raised by Kober et al. (2013)’s review. Exploration in continuous action spaces being different from discrete spaces, Hausknecht and Stone (2016) adapt -greedy exploration to parameterized action space by picking a random discrete action a ∈ Ad with probability and sampling the action’s parameters θai from a uniform random distribution. is arbitrarily annealed from 1.0 to 0.1 over the first 10,000 updates, thus requiring human prior knowledge about the duration of the task to appropriately tune exploration.\nHere we use the Gaussian exploration for continuous action parameters proposed by van Hasselt and Wiering (2007), which in the original formulation uses a fixed Gaussian width σ. We then apply a noiseless version of the meta-learning algorithm of Schweighofer and Doya (2003), which tracks online variations of the agent’s performance measured by short-term and long-term reward running averages. At each timestep, we use the difference between the two averages to simultaneously tune the inverse temperature βt used for selecting between discrete actions aj , and the width σt of the Gaussian distribution from which each continuous action parameter θai is sampled around its current value. We test our algorithm on a simple simulated human-robot interaction, where the algorithm tries to maximise reward computed as the virtual engagement of the human in the task, this engagement representing the attention that the human pays to the robot. We show that the proposed algorithm outperforms both continuous parameterized RL without active exploration and with active exploration based on uncertainty variations measured by a Kalman-RL algorithm (Geist and Pietquin, 2010)."
    }, {
      "heading" : "2. Active exploration algorithm",
      "text" : "The algorithm is summarized in Algorithm 1. It first employs Q-Learning (Watkins and Dayan, 1992) to learn the value of discrete action at ∈ Ad selected at timestep t in state st:\nQt+1(st, at)← Qt(st, at) + αQ[rt + γmax a (Qt(st+1, a))−Qt(st, at)] (1)\nwhere αQ is a learning rate and γ is a discount factor. The probability of executing discrete action aj at timestep t is given by a Boltzmann softmax equation:\nP (aj |st, βt) = exp (βtQt(st, aj))∑ a exp (βtQt(st, a))\n(2)\nwhere βt is a dynamic inverse temperature meta-parameter which will be tuned through meta-learning (see below).\nIn parallel, continuous θ̃ aj i,t parameters with which action aj is executed at timestep t are selected from a Gaussian exploration function centered on the current values θ aj i,t(st) in state st of the parameters of this action (van Hasselt and Wiering, 2007):\nP (θ̃ aj i,t|st, aj , σt) = 1√ 2πσt\nexp ( −(θ̃aji,t − θ aj i,t(st)) 2/(2σ2t ) )\n(3)\nwhere the width σt of the Gaussian is a meta-parameter which will be tuned through metalearning (see below) and action parameters θai,t(st) are learned with a continuous actor-critic algorithm (van Hasselt and Wiering, 2007). A reward prediction error is computed from the critic: δt = rt + γVt(st+1)− Vt(st) and is used to update the parameter vectors ωCt and ωAt of the neural network function approximations in the critic and the actor:\nωCi,t+1 = ω C i,t + αCδt\nδVt(st)\nδωCi,t and ωAi,t+1 = ω A i,t + αAδt(θ̃ a i,t − θai,t(st))\nδθai,t(st)\nδωAi,t (4)\nwhere αC and αA are learning rates. In contrast to the original version where ω A t updates are performed only when δt > 0 (van Hasselt and Wiering, 2007), here we update them all the time and proportionally to δt as in (Caluwaerts et al., 2012).\nFinally, in order to perform active exploration on βt and σt, we implement a noiseless version of the meta-learning algorithm proposed by (Schweighofer and Doya, 2003). We compute short- and long-term reward running averages:\n∆r̄(t) = (r(t)− r̄(t))/τ1 and ∆r̄(t) = (r̄(t)− r̄(t))/τ2 (5)\nwhere τ1 and τ2 are two time constants. We then update βt and σt with:\nβt = F (µ(r̄(t)− r̄(t))) and σt = G(µ(r̄(t)− r̄(t))) (6)\nwhere µ is a learning rate, F (x) > 0 is affine, and 0 < G(x) < 20 is a sigmoid. We also compared this meta-learning algorithm with the Kalman Q-Learning proposed by (Geist and Pietquin, 2010). However, in contrast to the original formulation which proposes a purely exploratory agent by replacing Q-values in Equation 2 by the actionspecific diagonal terms of the covariance matrix – these terms representing the current variance/uncertainty about an action’s Q-value –, here we multiply these terms by a weight η and add them as exploration bonuses bat to Q-values in Equation 2. We also use the covariance terms to tune action-specific σat with function G(x).\nAlgorithm 1 Active exploration with meta-learning\n1: Initialize ωAi,0, ω C i,0, Qi,0, β0 and σ0 2: for t = 0, 1, 2, ... do 3: Select discrete action at with softmax(st, βt) (Eq. 2) 4: Select action parameters θ̃ai,t with GaussianExploration(st, at, θ a i,t, σt) (Eq. 3) 5: Observe new state and reward {st+1, rt+1} ← Transition(st, at, θ̃ai,t) 6: Update Qt+1(st, at) in the discrete Q-Learning (Eq. 1) 7: Update function approx. ωCi,t+1 and ω A i,t+1 in continuous actor-critic (Eq. 4) 8: if meta-learning then 9: Update reward running averages r̄(t) and r̄(t) (Eq. 5)"
    }, {
      "heading" : "3. Experiments",
      "text" : "We test the algorithm described in Section 2 in a simple simulated human-robot interaction involving a single state, 6 discrete actions, and continuous action parameters between - 100 and 100. The task is similar to a non-stationary stochastic multi-armed bandit task except that rather than associating a fixed probability of reward to each discrete action, an action will yield reward only when its continuous parameters are chosen within a Gaussian distribution around the current optimal action parameter µ? with variance σ? (which are unknown to the robot). Every n timesteps, µ? changes so that the task is non-stationary and requires constant re-exploration and learning by the robot.\nThe reward is given by a dynamical system which is based on the virtual engagement e(t) of the human in the task. This engagement is supposed to represent the attention that the human pays to the robot. It starts at 5, increases up to a maximum emax = 10 when the robot performs the appropriate actions with the appropriate parameters, and decreases down to a minimum emin = 0 otherwise:\ne(t+ 1) =  e(t) + η1(emax − e(t))H(θat ), if a(t) = a? & H(θat ) ≥ 0 e(t)− η2(emin − e(t))H(θat ), if a(t) = a? & H(θat ) < 0 e(t) + η2(emin − e(t)), otherwise\nwhere η1 = 0.1 is the increasing rate, η2 = 0.05 is the decreasing rate, and H(x) is the reengagement function given by H(x) = 2 ( exp ( − (x−µ ?)2\n2σ?2\n) − 0.5 ) where a?, µ? and σ? are\nrespectively the optimal action, action parameter and variance around a?.\nThe reward function is then computed as r(t+ 1) = (1− λ)e(t+ 1) + λ∆e(t+ 1) where λ = 0.7 is a weight. This reward function ensures that the algorithm gets rewarded in cases where the engagement e(t+ 1) is low but nevertheless has just been increased by the action tuple (a(t), θa(t)) performed by the robot.\nWe first simulated the algorithm without active exploration (thus with a fixed σ = 20) in a task where the optimal action tuple (a?, µ?) is (a6,−20) during 200 timesteps (σ? = 10 in all the experiments presented here), then switches to (a2,−20) until timestep 600. Figure 1A shows that the algorithm first learns the appropriate action tuple (a6,−20), then takes some time to learn the second tuple, making the engagement drop between timesteps 200 and 400 and eventually finds the second optimal tuple. Nevertheless, σ = 20 makes the robot select action parameters θ̃at with a large variance (illustrated by the clouds of blue dots around the learned action parameters θ2t and θ 6 t plotted as black curves). As a consequence, the engagement is not optimized and always remains below 7.5. In contrast, the same algorithm with a smaller fixed variance σ = 10 can make the engagement reach the optimum of 10 when the optimal action tuple is learned (Figure 1B before timestep 400), but results in too little exploration which prevents the robot from finding a new action parameter which is too far away from the previously learned one (after timestep 400, the new optimal action tuple is (a6, 20)). These two examples illustrate the need to actively vary the variance σt as a function of changes in the robot’s performance.\nWe next tested active exploration with the Kalman Q-Learning algorithm in a task alternating between optimal tuples (a2,−20) and (a6, 20) every 400 timesteps (Figure 1C). The diagonal terms of the covariance matrix COV in the Kalman filter nearly monotonically\ndecrease, resulting in a large variance σt when action a6 is executed until about timestep 600, and progressively decreasing the variance until the end of the experiment. Nevertheless, the algorithm quickly finds the appropriate action parameters and rapidly shifts between actions a2 and a6 after each change in the task condition. In the long run, the model progressively averages the statistics of the two conditions and learns to perform both actions with 50/50 probabilities (bottom part of Figure 1C) which decreases the simulated engagement (top).\nWe then tested active exploration with the meta-learning algorithm in a slightly more difficult task where the optimal action tuple alternate between (a2,−50) and (a6, 50) every 1000 timesteps (Figure 1D). Transient drops in the engagement result in transient decreases in the exploration parameter βt as well as transient increases in the variance σt. This enables the algorithm to go through quick transient but wide exploration phases and to rapidly reconverge to exploitation, thus maximizing the simulated engagement.\nFinally, we performed 10 simulations of each model on the difficult version of the task and plotted the average and standard deviation of the simulated engagement (Figure 2). The blue curve shows the performance of the algorithm without active exploration (i.e. fixed σ = 19 obtained through parameter optimization), which adapts to each new condition but never exceeds a plateau of about 6. The green curve shows the active exploration with Kalman, which adapts faster at the beginning but progressively decreases its maximal engagement. The red curve shows the active exploration with meta-learning which initially takes more time to adapt but then only performs short transient explorations and reaches the optimum engagement of 10."
    }, {
      "heading" : "4. Conclusion",
      "text" : "In this work, we have shown that a meta-learning algorithm based on online variations of reward running averages can be used to adaptively tune two exploration parameters simultaneously used to select between both discrete actions and continuous action parameters in a parameterized action space. While we had previously successfully used the Kalman Q-Learning proposed by Geist and Pietquin (2010) to coordinate model-based and modelfree reinforcement learning in a stationary task (Viejo et al., 2015), it was not appropriate for the current non-stationary task. The proposed active exploration scheme could be a promising solution for Robotics applications of parameterized reinforcement learning."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Kenji Doya, Benôıt Girard, Olivier Pietquin, Bilal Piot, Inaki Rano, Olivier Sigaud and Guillaume Viejo for useful discussions. This research work has been partially supported by the EU-funded Project BabyRobot (H2020-ICT-24-2015, grant agreement no. 687831) (MK, CT), by the Agence Nationale de la Recherche (ANR-11-IDEX0004-02 Sorbonne-Universités SU-15-R-PERSU-14 Robot Parallearning Project) (MK), and by Labex SMART (ANR-11- LABX-65 Online Budgeted Learning Project) (MK)."
    } ],
    "references" : [ {
      "title" : "Robot skill learning: From reinforcement learning to evolution",
      "author" : [ "F. Stulp", "O. Sigaud" ],
      "venue" : null,
      "citeRegEx" : "Stulp and Sigaud.,? \\Q2003\\E",
      "shortCiteRegEx" : "Stulp and Sigaud.",
      "year" : 2003
    }, {
      "title" : "Modeling choice and reaction time during",
      "author" : [ "G. Viejo", "M. Khamassi", "A. Brovelli", "B. Girard" ],
      "venue" : null,
      "citeRegEx" : "Viejo et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Viejo et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Online model-free reinforcement learning (RL) methods with continuous actions are playing a prominent role when dealing with real-world applications such as Robotics. However, when confronted to non-stationary environments, these methods crucially rely on an exploration-exploitation trade-off which is rarely dynamically and automatically adjusted to changes in the environment. Here we propose an active exploration algorithm for RL in structured (parameterized) continuous action space. This framework deals with a set of discrete actions, each of which is parameterized with continuous variables. Discrete exploration is controlled through a Boltzmann softmax function with an inverse temperature β parameter. In parallel, a Gaussian exploration is applied to the continuous action parameters. We apply a meta-learning algorithm based on the comparison between variations of short-term and long-term reward running averages to simultaneously tune β and the width of the Gaussian distribution from which continuous action parameters are drawn. When applied to a simple virtual human-robot interaction task, we show that this algorithm outperforms continuous parameterized RL both without active exploration and with active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}