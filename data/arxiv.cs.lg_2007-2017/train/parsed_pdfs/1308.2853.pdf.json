{
  "name" : "1308.2853.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity",
    "authors" : [ "Animashree Anandkumar", "Daniel Hsu", "Majid Janzamin" ],
    "emails" : [ "dahsu@microsoft.com,", "skakade@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 8.\n28 53\nv1 [\ncs .L\nG ]\n1 3\nA ug\nKeywords: Overcomplete representations, topic models, generic identifiability, tensor decomposition."
    }, {
      "heading" : "1 Introduction",
      "text" : "The performance of many machine learning methods is hugely dependent on the choice of data representations or features. Overcomplete representations, where the number of features can be greater than the dimensionality of the input data, have been extensively employed, and are arguably critical in a number of applications such as speech and computer vision [1]. Overcomplete\n∗A. Anandkumar and M. Janzamin are with the Center for Pervasive Communications and Computing, Electrical Engineering and Computer Science Dept., University of California, Irvine, USA 92697. Email: a.anandkumar@uci.edu,mjanzami@uci.edu. Daniel Hsu and Sham Kakade are with Microsoft Research New England, 1 Memorial Drive, Cambridge, MA 02142. Email: dahsu@microsoft.com, skakade@microsoft.com\nrepresentations are known to be more robust to noise, and can provide greater flexibility in modeling [2]. Unsupervised estimation of overcomplete representations has been hugely popular due to the availability of large-scale unlabeled samples in many applications.\nA probabilistic framework for incorporating features posits latent or hidden variables that can provide a good explanation to the observed data. Overcomplete probabilistic models can incorporate a much larger number of latent variables compared to the observed dimensionality. In this paper, we characterize the conditions under which overcomplete latent variable models can be identified from their observed moments.\nFor any parametric statistical model, identifiability is a fundamental question of whether the model parameters can be uniquely recovered given the observed statistics. Identifiability is crucial in a number of applications where the latent variables are the quantities of interest, e.g. inferring diseases (latent variables) through symptoms (observations), inferring communities (latent variables) via the interactions among the actors in a social network (observations), and so on. Moreover, identifiability can be relevant even in predictive settings, where feature learning is employed for some higher level task such as classification. For instance, non-identifiability can lead to the presence of non-isolated local optima for optimization-based learning methods, and this can affect their convergence properties, e.g. see [3].\nIn this paper, we characterize identifiability for a popular class of latent variable models, known as the admixture or topic models [4, 5]. These are hierarchical mixture models, which incorporate the presence of multiple latent states (i.e. topics) in each document consisting of a tuple of observed variables (i.e. words). Previous works have established that the model parameters can be estimated efficiently using low order observed moments (second and third order) under some non-degeneracy assumptions, e.g. [6–8]. However, these non-degeneracy conditions imply that the model is undercomplete, i.e., the latent dimensionality (number of topics) cannot exceed the observed dimensionality (word vocabulary size). In this paper, we remove this restriction and consider overcomplete topic models, where the number of topics can far exceed the word vocabulary size.\nIt is perhaps not surprising that general topic models are not identifiable in the overcomplete regime. To this end, we introduce an additional constraint on the model, referred to as topic persistence. Intuitively, this captures the “locality” effect among the observed words, and is not present in the usual “bag-of-words” or exchangeable topic model. Such local dependencies among observations abound in applications such as text, images and speech, and can lead to a more faithful representation. In addition, we establish that the presence of topic persistence is central towards obtaining model identifiability in the overcomplete regime, and we provide an in-depth analysis of this phenomenon in this paper."
    }, {
      "heading" : "1.1 Summary of results",
      "text" : "In this paper, we provide conditions for generic 1 model identifiability of overcomplete topic models given observable moments of a certain order (i.e., having a certain number of words in each doc-\n1A model is generically identifiable, if all the parameters in the parameter space are identifiable, almost surely. Refer to Definition 1 for more discussion.\nument). We introduce the notion of topic persistence, and analyze its effect on identifiability. We establish identifiability in the presence of a novel combinatorial object, referred to as perfect n-gram matching, in the bipartite graph from topics to words. Finally, we prove that random structured topic models satisfy these criteria, and are thus identifiable in the overcomplete regime.\nPersistent Topic Model: We first introduce the n-persistent topic model, where the parameter n determines the persistence level of a common topic in a sequence of n successive words. For instance, in Figure 1, the sequence of successive words x1, . . . , xn share a common topic y1, and similarly, the words xn+1, . . . , x2n share topic y2, and so on. The n-persistent model reduces to the popular “bag-of-words” model, when n = 1, and to the single topic model (i.e. only one topic in each document) when n → ∞. Intuitively, topic persistence aids identifiability since we have multiple views of the common hidden topic generating a sequence of successive words. We establish that the bag-of-words model (with n = 1) is too non-informative about the topics in the overcomplete regime, and is therefore, not identifiable. On the other hand, n-persistent overcomplete topic models with n ≥ 2 can become identifiable, and we establish a set of transparent conditions for identifiability.\nDeterministic Conditions for Identifiability: Our sufficient conditions for identifiability are in the form of expansion conditions from the latent topic space to the observed word space. In the overcomplete regime, there are more topics than words in the vocabulary, and thus it is impossible to have expansion on the bipartite graph from topics to words, i.e., the graph encoding the sparsity pattern of the topic-word matrix. Instead, we impose an expansion constraint from topics to “higher order” words, which allows us to incorporate overcomplete models. We establish that this condition translates to the presence of a novel combinatorial object, referred to as the perfect n-gram matching, on the topic-word bipartite graph. Intuitively, the perfect n-gram matching condition implies “diversity” among the higher-order word supports for different topics which leads to identifiability. In addition, we present trade-offs among the following quantities: number of topics, size of the word vocabulary, the topic persistence level, the order of the observed moments at hand, the minimum and maximum degrees of any topic in the topic-word bipartite graph, and the Kruskal rank [9] of the topic-word matrix, under which identifiability holds. To the best of our knowledge, this is the first work to provide conditions for characterizing identifiability of overcomplete topic models with structured sparsity.\nIdentifiability of Random Structured Topic Models: We explicitly characterize the regime of identifiability for the random setting, where each topic i is randomly supported on a set of di words, i.e. the bipartite graph is a random graph. For this random model with q topics, pdimensional word vocabulary, and topic persistence level n, when q = O(pn) and Θ(log p) ≤ di ≤ Θ(p1/n), for all topics i, the topic-word matrix is identifiable from 2nth order observed moments with high probability. Intuitively, the upper bound on the degrees di is needed to limit the overlap of word supports among different topics in the overcomplete regime: as the number of topics q increases (i.e., n increases in the above degree bound), the degree needs to be correspondingly smaller to ensure identifiability, and we make this dependence explicit. Intuitively, as the extent of overcompleteness increases, we need sparser connections from topics to words to ensure sufficient diversity in the word supports among different topics. The lower bound on the degrees is required so that there are enough edges in the topic-word bipartite graph so that various topics can be distinguished from one another. Furthermore, we establish that the size condition q = O(pn) for identifiability is tight.\nImplications on Uniqueness of Overcomplete Tucker and CP Tensor Decompositions: We establish that identifiability of an overcomplete topic model is equivalent to uniqueness of decomposition of the observed moment tensor (of a certain order). Our identifiability results for persistent topic models imply uniqueness of a structured class of tensor decompositions, which is contained in the class of Tucker decompositions, but is more general than the candecomp/parafac (CP) decomposition [10]. This sub-class of Tucker decompositions involves structured sparsity and symmetry constraints on the core tensor, and sparsity constraints on the inverse factors of the Tucker decomposition. The structural constraints on the Tucker tensor decomposition are related to the topic model as follows: the sparsity and symmetry constraints on the core tensor are related to the persistence property of the topic model, and the sparsity constraints on the inverse factors are equivalent to the sparsity constraints on the topic-word matrix. For n-persistent topic model with n = 1 (bag-of-words model), the tensor decomposition is a general Tucker decomposition, where the core tensor is fully dense, while for n → ∞ (single-topic model), the tensor decomposition reduces to a CP decomposition, i.e. the core tensor is a diagonal tensor. For a finite persistence level n, in between these two extremes, the core tensor satisfies certain sparsity and symmetry constraints, which becomes crucial towards establishing identifiability in the overcomplete regime."
    }, {
      "heading" : "1.2 Overview of Techniques",
      "text" : "We now provide a short overview of the techniques employed in this paper.\nRecap of Identifiability Conditions in Under-complete Setting (Expansion Conditions on Topic-Word Matrix): Our approach is based on the recent results of [7], where conditions for identifiability of topic models are derived, given pairwise observed moments (specifically, cooccurrence of word-pairs in documents). Consider a topic model with q topics and observed word vocabulary of size p. Let A ∈ Rp×q denote the topic-word matrix. Expansion conditions are imposed in [7] on the topic-word bipartite graph which imply that (generically) the sparsest vectors in the column span of A, denoted by Col(A), are the columns of A themselves. Thus the topic-word matrix\nA is identifiable from pairwise moments under expansion constraints. However, these expansion conditions constrain the model to be under-complete, i.e., the number of topics q ≤ p, the size of the word vocabulary. Therefore, the techniques derived in [7] are not directly applicable here since we consider overcomplete models.\nIdentifiability in Overcomplete Setting and Why Topic-Persistence Helps: Pairwise moments are thus not sufficient for identifiability of overcomplete models, and the question is whether higher order moments can yield identifiability. We can view the higher order moments as pairwise moments of another equivalent topic model, which enables us to apply the techniques of [7]. The key question is whether we have expansion in the equivalent topic model, which implies identifiability. For a general topic model (without any topic persistence constraints), it can be shown that for identifiability, we require expansion of the nth-order Kronecker product of the original topic-word matrix A, denoted by A⊗n ∈ Rp\nn×qn , when given access to (2n)th-order moments, for any integer n ≥ 1. In the overcomplete regime where q > p, A⊗n cannot expand, and therefore, overcomplete models are not identifiable in general. On the other hand, we show that imposing the constraint of topic persistence can lead to identifiability. For a n-persistent topic model, given (2n)th-order moments, we establish that identifiability occurs when the nth-order Khatri-Rao product of A, denoted by A⊙n ∈ Rp\nn×q, expands. Note that the Khatri-Rao product A⊙n is a sub-matrix of the Kronecker product A⊗n, and the Khatri-Rao product A⊙n can expand as long as q ≤ pn. Thus, the property of topic persistence is central towards achieving identifiability in the overcomplete regime.\nFirst-Order Approach for Identifiability of Overcomplete Models (Expansion of ngram Topic-Word Matrix): We refer to A⊙n ∈ Rp\nn×q as the n-gram topic-word matrix, and intuitively, it relates topics to n-tuple words. Imposing the expansion conditions derived in [7] on A⊙n implies that (generically) the sparsest vectors in Col(A⊙n), are the columns of A⊙n themselves. Thus, the topic-word matrix A is identifiable from (2n)th-order moments for a n-persistent topic model. We refer to this as the “first-order” approach since we directly impose the expansion conditions of [7] on A⊙n, without exploiting the additional structure present in A⊙n.\nWhy the First-Order Approach is not Enough: Note that A⊙n ∈ Rp n×q matrix relates topics to n-tuples of words. Thus, the entries of A⊙n are highly correlated, even if the original topic-word matrix A is assumed to be randomly generated. It is non-trivial to derive conditions on A, so that A⊙n expands. Moreover, we establish that A⊙n fails to expand on “small” sets, as required in [7], when the degrees are sufficiently different 2. Thus, the first-order approach is highly restrictive in the overcomplete setting.\n2For A⊙n to expand on a set of size s ≥ 2, it is necessary that s · ( dmin+n−1\nn\n) ≥ s + ( dmax+n−1\nn\n) , where dmin and\ndmax are the minimum and maximum degrees, and n is the extent of overcompleteness: q = Θ(p n). When the model is highly overcomplete (large n) and we require small set expansion (small s), the degrees need to be nearly the same. Thus, it is desirable to impose expansion only on large sets, since it allows for more degree diversity.\nIncorporating Rank Criterion: Note that A⊙n is highly structured: the columns of A⊙n matrix possess a tensor 3 rank of 1, when n > 1. This can be incorporated in our identifiability criteria as follows: we provide conditions under which the sparsest vectors in Col(A⊙n), which also possess a tensor rank of 1, are the columns of A⊙n themselves. This implies identifiability of a npersistent topic model, when given access to (2n)th-order moments. Note that when a small number of columns of A⊙n are combined, the resulting vector cannot possess a tensor rank of 1, and thus, we can rule out that such sparse combinations of columns using the rank criterion. The maximum such number is at least the Kruskal rank 4 of A. Thus, sparse combinations of columns of A (up to the Kruskal rank) can be ruled out using the rank criterion, and we require expansion on A⊙n only on large sets of topics (of size larger than the Kruskal rank). This agrees with the intuition that when the topic-word matrix A has a larger Kruskal rank, it should be easier to identify A, since the Kruskal rank is related to the mutual incoherence 5 among the columns of A, see [11].\nNotion of Perfect n-gram Matching and Final Identifiability Conditions: Thus, we establish identifiability of overcomplete topic models subject to expansion conditions A⊙n on sets of size larger than the Kruskal rank of the topic-word matrix A. However, it is desirable to impose transparent and interpretable conditions directly on A for identifiability. We introduce the notion of perfect n-gram matching on the topic-word bipartite graph, which ensures that each topic can be uniquely matched to a n-tuple word. This combined with a lower bound on the Kruskal rank provides the final set of deterministic conditions for identifiability of the overcomplete topic model. Intuitively, we require that the columns of A be sparse, while still maintaining a large enough Kruskal rank; in other words, the topics have to be sparse and have sufficiently diverse word supports. Thus, we establish identifiability under a set of transparent conditions on the topic-word matrix A, consisting of perfect n-gram matching condition and a lower bound on the Kruskal rank of A.\nAnalysis under Random-Structured Topic-Word Matrices: Finally, we establish that the derived deterministic conditions are satisfied when the topic-word bipartite graph is randomly generated, as long as the degrees satisfy certain lower and upper bounds. Intuitively, a lower bound on the degrees of the topics is required to have degree concentration on various subsets so that expansion can occur, while the upper bound is required so that the Kruskal rank of the topic-word matrix is large enough compared to the sparsity level. Here, the main technical result is establishing the presence of a perfect n-gram matching in a random bipartite graph with a wide range of degrees. We present a greedy and a recursive mechanism for constructing such a n-gram matching for overcomplete models, which can be relevant even in other settings. For instance, our results imply the presence of a perfect matching when the edges of a bipartite graph are correlated in a structured manner, as given by the Khatri-Rao product.\n3When any column of A⊙n ∈ Rp n×q (of length pn) is reshaped as a nth-order tensor T ∈ Rp×p×···×p, the tensor T is rank 1. 4The Kruskal rank is the maximum number k such that every k-subset of columns of A are linearly independent. Note that the Kruskal rank is equal to the rank of A, when A has full column rank. But this cannot happen in the overcomplete setting.\n5It is easy to show that krank ≥ (maxi6=j |a ⊤ i aj |) −1, where ai, aj are any pair of columns of A. Thus, higher incoherence leads to a larger kruskal rank."
    }, {
      "heading" : "1.3 Related works",
      "text" : "We now summarize some recent related works in the area of identifiability and learning of latent variable models.\nIdentifiability, learning and applications of overcomplete latent representations: Many recent works employ unsupervised estimation of overcomplete features for higher level tasks such classification, e.g. [1,12–14], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision. However, theoretical understanding regarding learnability or identifiability of overcomplete representations is far more limited.\nOvercomplete latent representations have been analyzed in the context of the independent components analysis (ICA), where the sources are assumed to be independent, and the mixing matrix is unknown. In the overcomplete or under-determined regime of the ICA, there are more sources than sensors. Identifiability and learning of the overcomplete ICA reduces to the problem of finding an overcomplete candecomp/parafac (CP) tensor decomposition. The classical result by Kruskal provides conditions for uniqueness of a CP decomposition [9, 15], with recent extensions to the notion of robust identifiability [16]. These results provide conditions for strict identifiability of the model, and here, the dimensionality of the latent space is required to be of the same order as the observed space dimensionality. In contrast, a number of recent works analyze generic identifiability of overcomplete CP decomposition, which is weaker than strict identifiability, e.g. [17–23]. These works assume that the factors (i.e. the components) of the CP decomposition are generically drawn and provide conditions for uniqueness. They allow for the latent dimensionality to be much larger (polynomially larger) than the observed dimensionality. These results on the uniqueness of CP decompositions also lead to identifiability of other latent variable models, such as latent tree models, e.g. [24, 25], and the single-topic model, or more generally latent Dirichlet allocation (LDA). Recently Goyal et. al. [26] proposed an alternative framework for overcomplete ICA models based on the eigen-decomposition of the reweighted covariance matrix (or higher order moments), where the weights are the Fourier coefficients. However, their approach requires independence of sources (i.e. latent topics in our context), which is not imposed here.\nIn contrast to the above works dealing with the CP tensor decomposition, we require uniqueness for a more general class of tensor decompositions, in order to establish identifiability of topic models with arbitrarily correlated topics. We establish that our class of tensor decomposition is contained in the class of Tucker decompositions which is more general than CP decomposition. Moreover, we explicitly characterize the effect of the sparsity pattern of the factors (i.e., the topic-word matrix) on model identifiability, while all the previous works based on generic identifiability assume fully dense factors (since sparse factors are not generic). For a general overview of tensor decompositions, see [10,27].\nIdentifiability and learning of undercomplete/over-determined latent representations: Much of the theoretical results on identifiability and learning of the latent variable models are limited to non-singular models, which implies that the latent space dimensionality is at most the observed dimensionality. We outline some of the recent works below.\nThe works of Anandkumar et. al. [6,28,29] provide an efficient moment-based approach for learning topic models, under constraints on the distribution of the topic proportions, e.g. the single topic model, and more generally latent Dirichlet allocation (LDA). In addition, the approach can handle a variety of latent variable models such as Gaussian mixtures, hidden Markov models (HMM) and community models [30]. The high-level idea is to reduce the problem of learning of the latent variable model to finding a CP decomposition of the (suitably adjusted) observed moment tensor. Various approaches can then be employed to find the CP decomposition. In [6], a tensor power method approach is analyzed and is shown to be an efficient guaranteed recovery method in the nondegenerate (i.e. undercomplete) setting. Previously, simultaneous diagonalization techniques have been employed for solving the CP decomposition, e.g. [28, 31, 32]. However, these techniques fail when the model is overcomplete, as considered here. We note that some recent techniques, e.g. [20], can be employed instead, albeit at a cost of higher computational complexity for overcomplete CP tensor decomposition. However, it is not clear how the sparsity constraints affect the guarantees of such methods. Moreover, these approaches cannot handle general topic models, where the distribution of the topic proportions is not limited to these classes (i.e. either single topic or Dirichlet distribution), and we require tensor decompositions which are more general than the CP decomposition.\nThere are many other works which consider learning mixture models when multiple views are available. See [28] for a detailed description of these works. Recently, Rabani et. al. [33] consider learning discrete mixtures given a large number of “views”, and they refer to the number of views as the sampling aperture. They establish improved recovery results (in terms of ℓ1 bounds) when sufficient number of views are available (2k − 1 views for a k-component mixture). However, their results are limited to discrete mixtures or single-topic models, while our setting can handle more general topic models. Moreover, our approach is different since we incorporate sparsity constraints in the topic-word distribution. Another series of recent works by Arora et. al. [8, 34] employ approaches based on non-negative matrix factorization (NMF) to recover the topic-word matrix. These works allow models with arbitrarily correlated topics, as considered here. They establish guaranteed learning when every topic has an anchor word, i.e. the word is uniquely generated from that topic, and does not occur under any other topic. Note that the anchor-word assumption cannot be satisfied in the overcomplete setting.\nOur work is closely related to the work of Anandkumar et. al. [7] which considers identifiability and learning of topic models under expansion conditions on the topic-word matrix. The work of Spielman et. al [35] considers the problem of dictionary learning, which is closely related to the setting of [7], but in addition assumes that the coefficient matrix is random. However, these works [7,35] can handle only the under-complete setting, where the number of topics is less than the dimensionality of the word vocabulary (or the number of dictionary atoms is less than the number of observations in [35]). We extend these results to the overcomplete setting by proposing novel higher order expansion conditions on the topic-word matrix, and also incorporate additional rank constraints present in higher order moments.\nDictionary learning/sparse coding: Overcomplete representations have been very popular in the context of dictionary learning or sparse coding. Here, the task is to jointly learn a dictionary as well as a sparse selection of the dictionary atoms to fit the observed data. There have been Bayesian as well as frequentist approaches for dictionary learning [2,36,37]. However, the heuristics\nemployed in these works [2, 36, 37] have no performance guarantees. The work of Spielman et. al [35] considers learning (undercomplete) dictionaries and provide guaranteed learning under the assumption that the coefficient matrix is random (distributed as Bernoulli-Gaussian variables). Recent works [38, 39] provide generalization bounds for predictive sparse coding, where the goal of the learned representation is to obtain good performance on some predictive task. This differs from our framework since we do not consider predictive tasks here, but the task of recovering the underlying latent representation. Hillar and Sommer [40] consider the problem of identifiability of sparse coding and establish that when the dictionary succeeds in reconstructing a certain set of sparse vectors, then there exists a unique sparse coding, up to permutation and scaling. However, our setting here is different, since we do not assume that a sparse set of topics occur in each document."
    }, {
      "heading" : "2 Model",
      "text" : "Notation: The set {1, 2, . . . , n} is denoted by [n] := {1, 2, . . . , n}. Given a set X = {1, . . . , p}, set X(n) denotes all ordered n-tuples generated from X. The cardinality of a set S is denoted by |S|. For any vector u (or matrix U), the support is denoted by Supp(u), and the ℓ0 norm is denoted by ‖u‖0, which corresponds to the number of non-zero entries of u, i.e., ‖u‖0 := |Supp(u)|. For a vector u ∈ Rq, Diag(u) ∈ Rq×q is the diagonal matrix with vector u on its diagonal. The column space of a matrix A is denoted by Col(A). Vector ei ∈ R\nq is the i-th basis vector, with the i-th entry equal to 1 and all the others equal to zero. For A ∈ Rp×q and B ∈ Rm×n, the Kronecker product A⊗B ∈ Rpm×qn is defined as [41]\nA⊗B =   a11B a12B · · · a1qB a21B a22B · · · a2qB ... ... . . .\n... ap1B ap2B · · · apqB\n  ,\nand for A = [a1|a2| · · · |ar] ∈ R p×r and B = [b1|b2| · · · |br] ∈ R m×r, the Khatri-Rao product A⊙B ∈ R pm×r is defined as\nA⊙B = [a1 ⊗ b1|a2 ⊗ b2| · · · |ar ⊗ br] ."
    }, {
      "heading" : "2.1 Persistent topic model",
      "text" : "In this section, the n-persistent topic model is introduced and this imposes an additional constraint, known as topic persistence on the popular admixture model [4,5,42]. The n-persistent topic model reduces to the bag-of-words admixture model when n = 1.\nAn admixture model specifies a q-dimensional vector of topic proportions h ∈ ∆q−1 := {u ∈ Rq : ui ≥ 0, ∑q i=1 ui = 1} which generates the observed variables xl ∈ R\np through vectors a1, . . . , aq ∈ R p. This collection of vectors ai, i ∈ [q], is referred to as the population structure or the topic-word matrix [42]. For instance, ai is the conditional distribution of words given topic i. The latent variable h is a q dimensional random vector h := [h1, . . . , hq] ⊤ known as proportion vector. A prior\ndistribution P (h) over the probability simplex ∆q−1 characterizes the prior joint distribution over the latent variables hi, i ∈ [q]. In the topic modeling, this is the prior distribution over the q topics.\nThe n-persistent topic model has a three-level multi-view hierarchy in Figure 1. 2rn number of words (views) are shown in the model for some integer r ≥ 1. In this model, a common hidden topic is persistent for a sequence of n words {x(j−1)n+1, . . . , x(j−1)n+n}, j ∈ [2r]. Note that the random observed variables (words) are exchangeable within groups of size n, where n is the persistence level, but are not globally exchangeable.\nWe now describe a linear representation of the n-persistent topic model, on lines of [6], but with extensions to incorporate persistence. Each random variable yj, j ∈ [2r], is a discrete valued random variable taking one of the q possibilities {1, . . . , q}, i.e., yj ∈ [q] for j ∈ [2r]. In the n-persistent model, a single common topic is chosen for a sequence of n words {x(j−1)n+1, . . . , x(j−1)n+n}, j ∈ [2r], i.e., the topic is persistent for n successive views. For notational purposes, we equivalently assume that variables yj, j ∈ [2r], are encoded by the basis vectors ei, i ∈ [q]. Thus, the variable yj, j ∈ [2r], is\nyj = ei ∈ R q ⇐⇒ the topic of j-th group of words is i.\nGiven proportion vector h, topics yj, j ∈ [2r], are independently drawn according to the conditional expectation\nE [ yj|h ] = h, j ∈ [2r],\nor equivalently Pr [ yj = ei|h ] = hi, j ∈ [2r], i ∈ [q].\nFinally, at the bottom layer, each observed variable xl for l ∈ [2rn], is a discrete-valued pdimensional random variable, where p is the size of word vocabulary. Again, we assume that variables xl, are encoded by the basis vectors ek, k ∈ [p], such as\nxl = ek ∈ R p ⇐⇒ the l-th word in the document is k.\nGiven the corresponding topic yj, j ∈ [2r], words xl, l ∈ [2rn], are independently drawn according to the conditional expectation\nE [ x(j−1)n+k|yj = ei ] = ai, i ∈ [q], j ∈ [2r], k ∈ [n], (1)\nwhere vectors ai ∈ R p, i ∈ [q], are the conditional probability distribution vectors. The matrix A = [a1|a2| · · · |aq] ∈ R p×q collecting these vectors is the population structure or topic-word matrix.\nThe (2rn)-th order moment of observed variables xl, l ∈ [2rn], for some integer r ≥ 1, is defined as (in the matrix form) 6\nM2rn(x) := E [ (x1 ⊗ x2 ⊗ · · · ⊗ xrn)(xrn+1 ⊗ xrn+2 ⊗ · · · ⊗ x2rn) ⊤ ] ∈ Rp rn×prn . (2)\nFor the n-persistent topic model with 2rn number of observations (words) xl, l ∈ [2rn], the corresponding moment is denoted by M (n) 2rn(x). Note that to estimate the (2rn) th moment, we require\n6Vector x is the vector generated by concatenating all vectors xl, l ∈ [2rn].\na minimum of 2rn words in each document. We can select the first 2rn words in each document, and average over the different documents to obtain a consistent estimate of the moment. In this paper, we consider the problem of identifiability when exact moments are available. The moment characterization of the n-persistent topic model is provided in Lemma 1 in Section 4.1. Given M (n) 2rn(x), what are the sufficient conditions under which the population structure A is identifiable? This is answered in Section 3. Remark 1. Note that our results are valid for the more general linear model xl = Ayj (more precisely, x(j−1)n+k = Ayj, j ∈ [2r], k ∈ [n]), i.e., each column of matrix A does not need to be a valid probability distribution. Furthermore, the observed random variables xl, can be continuous while the hidden ones yj are assumed to be discrete."
    }, {
      "heading" : "3 Sufficient Conditions for Generic Identifiability",
      "text" : "In this section, the identifiability result for the n-persistent topic model with access to (2n)-th order observed moment is provided. First, sufficient deterministic conditions on the population structure A are provided for identifiability in Theorem 1. Next, the deterministic analysis is specialized to a random structured model in Theorem 2.\nWe now make the notion of identifiability precise. As defined in literature, (strict) identifiability means that the population structure A can be uniquely recovered up to permutation and scaling for all A ∈ Rp×q. Instead, we consider a more relaxed notion of identifiability, known as generic identifiability. Definition 1 (Generic identifiability). We refer to a matrix A ∈ Rp×q as generic, with a fixed sparsity pattern when the nonzero entries of A are drawn from a distribution which is absolutely continuous with respect to Lebesgue measure 7. For a given sparsity pattern, the class of population structure matrices is said to be generically identifiable [25], if all the non-identifiable matrices form a set of Lebesgue measure zero.\nThe (2r)-th order moment of hidden variables h ∈ Rq, denoted by M2r(h) ∈ R qr×qr , is defined as\nM2r(h) := E\n[( r times︷ ︸︸ ︷ h⊗ · · · ⊗ h )( r times︷ ︸︸ ︷ h⊗ · · · ⊗ h )⊤] ∈ Rq r×qr . (3)\nWe now provide a set of sufficient conditions for generic identifiability of structured topic models given (2rn)-th order observed moment. We first start with a natural assumption on the hidden variables. Condition 1 (Non-degeneracy). The (2r)-th order moment of hidden variables h ∈ Rq, defined in equation (3), is full rank (non-degeneracy of hidden nodes).\nNote that there is no hope of distinguishing distinct hidden nodes without this non-degeneracy assumption. We do not impose any other assumption on hidden variables and can incorporate arbitrarily correlated topics.\n7As an equivalent definition, if the non-zero entries of an arbitrary sparse matrix are independently perturbed with noise drawn from a continuous distribution to generate A, then A is called generic.\nFurthermore, we can only hope to identify the population structureA up to scaling and permutation. Therefore, we can identify A up to a canonical form defined as: Definition 2 (Canonical form). Population structure A is said to be in canonical form if all of its columns have unit norm."
    }, {
      "heading" : "3.1 Deterministic conditions for generic identifiability",
      "text" : "In this section, we consider a fixed sparsity pattern on the population structure A and establish generic identifiability when non-zero entries of A are drawn from some continuous distribution. Before providing the main result, a generalized notion of (perfect) matching for bipartite graphs is defined. We subsequently impose these conditions on the bipartite graph from topics to words which encodes the sparsity pattern of population structure A.\nGeneralized matching for bipartite graphs\nA bipartite graph with two disjoint vertex sets Y and X and an edge set E between them is denoted by G(Y,X;E). Given the bi-adjacency matrix A, the notation G(Y,X;A) is also used to denote a bipartite graph. Here, the rows and columns of matrix A ∈ R|X|×|Y | are respectively indexed by X and Y vertex sets. For any subset S ⊆ Y , the set of neighbors of vertices in S with respect to A is defined as NA(S) := {i ∈ X : Aij 6= 0 for some j ∈ S}, or equivalently, NE(S) := {i ∈ X : (j, i) ∈ E for some j ∈ S} with respect to edge set E.\nHere, we define a generalized notion of matching for a bipartite graph and refer to it as n-gram matching. Definition 3 ((Perfect) n-gram matching). A n-gram matching M for a bipartite graph G(Y,X;E) is a subset of edges M ⊆ E which satisfies the following conditions. First, for any j ∈ Y , we have |NM (j)| ≤ n. Second, for any j1, j2 ∈ Y, j1 6= j2, we have min{|NM (j1)|, |NM (j2)|} > |NM (j1) ∩NM (j2)|. A perfect n-gram matching or Y -saturating n-gram matching for the bipartite graph G(Y,X;E) is a n-gram matching M in which each vertex in Y is the end-point of exactly n edges in M .\nIn words, in a n-gram matching M , each vertex j ∈ Y is at most the end-point of n edges in M and for any pair of vertices in Y (j1, j2 ∈ Y, j1 6= j2), there exists at least one non-common neighbor in set X for each of them (j1 and j2).\nAs an example, a bipartite graph G(Y,X;E) with |X| = 4 and |Y | = 6 is shown in Figure 2 for which the edge set E itself is a perfect 2-gram matching.\nRemark 2 (Relationship to other matchings). The relationship of n-gram matching to other types of matchings is discussed below.\n• Regular matching: For special case n = 1, the (perfect) n-gram matching reduces to the usual (perfect) matching for bipartite graphs.\n• b-matching: A b-matching for a bipartite graph G(Y,X;E) (with equal vertex sizes |X| = |Y |) is a subset of edges Mb ⊆ E, where each vertex is connected to b edges. Comparing with the proposed perfect n-gram matching, b-matching does not enforce that the set of neighbors be different, and furthermore, it requires that X = Y , which is not possible under the overcomplete setting. Remark 3 (Necessary size bound). Consider a bipartite graph G(Y,X;E) with |Y | = q and |X| = p which has a perfect n-gram matching. Note that there are (p n ) n-combinations on X side and each combination can at most have one neighbor (a node in Y which is connected to all nodes in the combination) through the matching, and therefore we necessarily have q ≤ (p n ) .\nFinally, note that the existence of perfect n-gram matching results the existence of perfect (n+1)gram matching 8, but the reverse is not true. For example, the bipartite graph G(Y,X;E) with |X| = 4 and |Y | = (4 2 ) = 6 in Figure 2, has a perfect 2-gram matching, but not a perfect (1-gram) matching (since 6 > 4).\nIdentifiability conditions based on existence of perfect n-gram matching in topic-word graph\nNow, we are ready to propose the identifiability conditions and result. Condition 2 (Perfect n-gram matching on A). The bipartite graph G(Vh, Vo;A) between hidden and observed variables, has a perfect n-gram matching.\nThe above condition implies that the sparsity pattern of matrix A is appropriately scattered in the mapping from hidden to observed variables to be identifiable. Intuitively, it means that every hidden node can be distinguished from another hidden node by its unique set of neighbors under the corresponding n-gram matching. Furthermore, condition 2 is the key to be able to propose identifiability in the overcomplete regime. As stated in the size bound in Remark 3, for n ≥ 2, the number of hidden variables can be more than the number of observed variables and we can still have perfect n-gram matching. Definition 4 (Kruskal rank, [15]). The Kruskal rank or the krank of matrix A is defined as the maximum number k such that every subset of k columns of A is linearly independent.\nNote that krank is different from the general notion of matrix rank and it is a lower bound for the matrix rank, i.e., Rank(A) ≥ krank(A). Condition 3 (Krank condition on A). The Kruskal rank of matrix A satisfies the bound krank(A) ≥ dmax(A) n, where dmax(A) is the maximum node degree of any column of A.\nIn the overcomplete regime, it is not possible for A to be full column rank and krank(A) < |Vh| = q. However, note that a large enough krank ensures that appropriate sized subsets of columns of A are linearly independent. For instance, when krank(A) > 1, any two columns cannot be collinear\n8Note that the degree of each node (on matching side Y ) in the original bipartite graph should be at least n+ 1.\nand the above condition rules out the collinear case for identifiability. In the above condition, we see that a larger krank can incorporate denser connections between topics and words.\nThe main identifiability result under a fixed graph structure is stated in the following theorem for n ≥ 2, where n is the topic persistence level. The identifiability result relies on having access to the (2rn)-th order moment of observed variables xl, l ∈ [2rn], defined in equation (2) as\nM2rn(x) := E [ (x1 ⊗ x2 ⊗ · · · ⊗ xrn)(xrn+1 ⊗ xrn+2 ⊗ · · · ⊗ x2rn) ⊤ ] ∈ Rp rn×prn ,\nfor some integer r ≥ 1. Theorem 1 (Generic identifiability under deterministic topic-word graph structure). Let M (n) 2rn(x) in equation (2) be the (2rn)-th order observed moment of the n-persistent topic model for some integer r ≥ 1. If the model satisfies conditions 1, 2 and 3, then, for any n ≥ 2, all the columns of population structure A are generically identifiable from M (n) 2rn(x). Furthermore, the (2r)-th order moment of the hidden variables, denoted by M2r(h), is also generically identifiable.\nThe theorem is proved in Appendix A. It is seen that the population structure A is identifiable, given any observed moment of order at least 2n. Increasing the order of observed moment results in identifying higher order moments of the hidden variables. The above theorem does not cover the case when the persistence level n = 1. This is the usual bag-of-words admixture model. Identifiability of this model has been studied earlier [7] and we recall it below. Remark 4 (Bag-of-words admixture model, [7]). Given (2r)-th order observed moments with r ≥ 1, the structure of the popular bag-of-words admixture model and the (2r)-th order moment of hidden variables are identifiable, when A is full column rank and the following expansion condition holds [7]\n|NA(S)| ≥ |S|+ dmax(A), ∀S ⊆ Vh, |S| ≥ 2. (4)\nOur result for n ≥ 2 in Theorem 1, provides identifiability in the overcomplete regime with weaker matching condition 2 and krank condition 3. The matching condition 2 is weaker than the above expansion condition which is based on the perfect matching and hence, does not allow overcomplete models. Furthermore, the above result for the bag-of-words admixture model requires full column rank of A which is more stringent than our krank condition 3. Remark 5 (Kruskal rank and degree diversity). Condition 3 requires that the Kruskal rank of the topic-word matrix be large enough compared to the maximum degree of the topics. Intuitively, a larger Kruskal rank ensures enough diversity in the word supports among different topics under a higher level of sparsity. This Kruskal rank condition also allows for more degree diversity among the topics, when the topic persistence level n > 1. On the other hand, for the bag-of-words model (n = 1), using (4) implies that 2dmin > dmax, where dmin, dmax are the minimum and maximum degrees of the topics. Thus, we provide identifiability results with more degree diversity when higher order moments are employed. Remark 6 (Recovery using ℓ1 optimization). It turns out that our conditions for identifiability imply that the columns of the n-gram matrix A⊙n, defined in Definition 6, are the sparsest vectors in\nCol ( M\n(n) 2n (x)\n) , having a tensor rank of one. See Appendix A. This implies recovery of the columns\nof A through exhaustive search, which is not efficient. Efficient ℓ1-based recovery algorithms have been analyzed in [7,43] for the undercomplete case (n = 1). They can be employed here for recovery from higher order moments as well. Exploiting additional structure present in A⊙n, for n > 1, such as rank-1 test devices proposed in [20] are interesting avenues for future investigation."
    }, {
      "heading" : "3.2 Analysis under random topic-word graph structures",
      "text" : "In this section, we specialize the identifiability result to the random case. This result is based on more transparent conditions on the size and the degree of the random bipartite graph G(Vh, Vo;A). We consider the random model where in the bipartite graph G(Vh, Vo;A), each node i ∈ Vh is randomly connected to di different nodes in set Vo. Note that this is a heterogeneous degree model. Condition 4 (Size condition). The random bipartite graph G(Vh, Vo;A) with |Vh| = q, |Vo| = p, and A ∈ Rp×q, satisfies the size condition q ≤ ( c pn )n for some constant 0 < c < 1.\nThis size condition is required to establish that the random bipartite graph has a perfect n-gram matching (and hence satisfies deterministic condition 2). It is shown in Section 5.2.1 that the necessary size constraint q = O(pn) stated in Remark 3, is achieved in the random case. Thus, the above constraint allows for the overcomplete regime, where q ≫ p for n ≥ 2, and is tight. Condition 5 (Degree condition). In the random bipartite graph G(Vh, Vo;A) with |Vh| = q, |Vo| = p, and A ∈ Rp×q, the degree di of nodes i ∈ Vh satisfies the following lower and upper bounds (di ∈ [dmin, dmax]):\n• Lower bound: dmin ≥ max{1+β log p, α log p} for some constants β > n−1 log 1/c , α > max { 2n2 ( β log 1c+\n1 ) , 2βn } .\n• Upper bound: dmax ≤ (cp) 1 n .\nIntuitively, the lower bound on the degree is required to show that the corresponding bipartite graph G(Vh, Vo;A) has sufficient number of random edges to ensure that it has perfect n-gram matching with high probability. The upper bound on the degree is mainly required to satisfy the krank condition 3, where dmax(A) n ≤ krank(A).\nIt is important to see that, for n ≥ 2, the above condition on degree covers a range of models from sparse to intermediate regimes and it is reasonable in a number of applications that each topic does not generate a very large number of words. Definition 5 (whp). A sequence of events Ep occurs with high probability (whp) if Pr(Ep) = 1−O(p−ǫ) for some ǫ > 0.\nThe main random identifiability result is stated in the following theorem for n ≥ 2, while n = 1 case is addressed in Remark 8. The identifiability result relies on having access to the (2rn)-th order moment of observed variables xl, l ∈ [2rn], defined in equation (2) as\nM2rn(x) := E [ (x1 ⊗ x2 ⊗ · · · ⊗ xrn)(xrn+1 ⊗ xrn+2 ⊗ · · · ⊗ x2rn) ⊤ ] ∈ Rp rn×prn ,\nfor some integer r ≥ 1.\nProbability rate constants: The probability rate of success in the following random identifiability result is specified by constants β′ > 0 and γ = γ1 + γ2 > 0 as\nβ′ = −β log c− n+ 1, (5)\nγ1 = e n−1 ( c nn−1 + e2 1− δ1 nβ ′+1 ) , (6)\nγ2 = cn−1e2\nnn(1− δ2) , (7)\nwhere δ1 and δ2 are some constants satisfying e 2 ( p n )−β log 1/c < δ1 < 1 and cn−1e2 nn p −β′ < δ2 < 1. Theorem 2 (Random identifiability). Let M\n(n) 2rn(x) in equation (2) be the (2rn)-th order observed\nmoment of the n-persistent topic model for some integer r ≥ 1. If the model with random population structure A satisfies conditions 1, 4 and 5, then whp (with probability at least 1−γp−β ′ for constants β′ > 0 and γ > 0, specified in (5)-(7)), for any n ≥ 2, all the columns of population structure A are identifiable from M (n) 2rn(x). Furthermore, the (2r)-th order moment of hidden variables, denoted by M2r(h), is also identifiable, whp.\nThe theorem is proved in Appendix B. Similar to the deterministic analysis, it is seen that the population structure A is identifiable given any observed moment with order at least 2n. Increasing the order of observed moment results in identifying higher order moments of the hidden variables.\nRemark 7 (Trade-off between topic-word size ratio and degree). When the number of hidden variables increases, i.e. c increases, but the order n is kept fixed, the bounds on degree in condition 5 also needs to grow. Intuitively, a larger degree is needed to provide more flexibility in choosing the subsets of neighbors for hidden nodes to ensure the existence of a perfect n-gram matching in the bipartite graph, which in turn ensures identifiability. Note that as c grows, the parameter β, which is the lower bound on d also grows, and the probability rate (i.e., the term −β log c) remains constant. Hence, the probability rate does not change as c increases, since the increase in the degree d compensates the additional “difficulty” arising due to a larger number of hidden variables.\nThe above identifiability theorem only covers for n ≥ 2 and the n = 1 case is addressed in the following remark. Remark 8 (Bag-of-words admixture model). The identifiability result for the random bag-of-words admixture model is comparable to the result in [43], which considers exact recovery of sparsely-used dictionaries. They assume that Y = DX is given for some unknown arbitrary dictionary D ∈ Rq×q and unknown random sparse coefficient matrix X ∈ Rq×p. They establish that if D ∈ Rq×q is full rank and the random sparse coefficient matrix X ∈ Rq×p follows the Bernoulli-subgaussian model with size constraint p > Cq log q and degree constraint O(log q) < E[d] < O(q log q), then the model is identifiable, whp. Comparing the size and degree constraints, our identifiability result for n ≥ 2 requires more stringent upper bound on the degree (d = O(p1/n)), while more relaxed condition on the size (q = O(pn)) which allows to identifiability in the overcomplete regime. Remark 9 (The size condition is tight). The size bound q = O(pn) in the above theorem achieves the necessary condition that q ≤ ( p n ) = O(pn) (see Remark 3), and is therefore tight. The sufficiency is argued in Theorem 3, where we show that the matching condition 2 holds under the above size and degree conditions 4 and 5."
    }, {
      "heading" : "4 Identifiability via Uniqueness of Tensor Decompositions",
      "text" : "In this section, we characterize the moments of the n-persistent topic model in terms of the model parameters, i.e. the topic-word matrix A and the moment of hidden variables. We relate identifia-\nbility of the topic model to uniqueness of a certain class of tensor decompositions, which in turn, enables us to prove Theorems 1 and 2. We then discuss the special cases of the persistent topic model, viz., the single topic model (infinite-persistent topic model) and the bag-of-words admixture model (1-persistent topic model)."
    }, {
      "heading" : "4.1 Moment characterization of the persistent topic model",
      "text" : "The moment characterization requires the following definition of a n-gram matrix. Definition 6 (n-gram Matrix). Given a matrix A ∈ Rp×q, its n-gram matrix A⊙n ∈ Rp\nn×q is defined as the matrix whose (i, j)-th entry is given by, for i := (i1, i2, . . . , in) ∈ [p] n and j ∈ [q],\nA⊙n(i, j) := Ai1,jAi2,j · · ·Ain,j, or A ⊙n :=\nn times︷ ︸︸ ︷ A⊙ · · · ⊙A .\nThat is, A⊙n is the column-wise nth order Kronecker product of n copies of A, and is known as the Khatri-Rao product [41].\nIn the following lemma, which is proved in Appendix A.2, we characterize the observed moments of a persistent topic model. Throughout this section, the order of the observed moment is fixed to 2m. Lemma 1 (n-persistent topic model moment characterization). The (2m)-th order moment of observed variables, defined in equation (2), for the n-persistent topic model is characterized as 9:\n• if m = rn, for some integer r ≥ 1, then\nM (n) 2m (x) =\n( r times︷ ︸︸ ︷ A⊙n ⊗ · · · ⊗A⊙n ) M2r(h) ( r times︷ ︸︸ ︷ A⊙n ⊗ · · · ⊗A⊙n )⊤ , (8)\nwhere M2r(h) ∈ R qr×qr is the (2r)-th order moment of hidden variables h ∈ Rq, defined in equation (3).\n• If n ≥ 2m, then\nM (n) 2m (x) =\n( A⊙m ) M1(h) ( A⊙m )⊤ , (9)\nwhere M1(h) := Diag(E[h]) ∈ R q×q is the first order moment of hidden variables h ∈ Rq, stacked in a diagonal matrix.\nThus, we see that the observed moments can be expressed in terms of the hidden moments M(h) and the Kronecker products of the n-gram matrices. In the special case, when the persistence level is large enough compared to the order of the moment (n ≥ 2m), the moment form reduces to a Khatri-Rao product form in (9). Moreover, in (9), we have a diagonal matrix M1(h) instead of a general (dense) matrix M2r(h) in (8), when n < 2m = 2rn. Thus, we have a more succinct representation of the moments in (9) when the persistence level of the topics is large enough.\nIn the following, we contrast the special cases when the persistence level n is n → ∞ (single topic model) and n = 1 (bag of words admixture model), as shown in Fig.3a and Fig.3b. In order to\n9The other cases not covered in Lemma 1 are deferred to Appendix A.2. See Remark 12.\nhave a fair comparison, the number of observed variables is fixed to 2m and the persistence level is varied.\nSingle topic model (n → ∞): The condition in (9) (n ≥ 2m) is always satisfied for the singletopic model, since n → ∞ in this case, and we have\nM (∞) 2m (x) =\n( A⊙m ) M1(h) ( A⊙m )⊤ . (10)\nNote that M1(h) is a diagonal matrix.\nBag-of-words admixture model (n = 1): From Lemma 1, the (2m)-th order moment of observed variables xl, l ∈ [2m], for the bag-of-words admixture model (1-persistent topic model), shown in Figure 3b, is given by\nM (1) 2m(x) =\n( m times︷ ︸︸ ︷ A⊗ · · · ⊗A ) M2m(h) ( m times︷ ︸︸ ︷ A⊗ · · · ⊗A )⊤ , (11)\nwhere M2m(h) ∈ R qm×qm is the (2m)-th order moment of hidden variables h ∈ Rq, defined in (3). Note that M2m(h) is a full matrix in general.\nContrasting single topic (n → ∞) and bag of words models (n = 1): Comparing equations (10) and (11), it is seen that the moments under the single topic model in (10) are more “structured” compared to the bag of words model in (11). In (11), we have Kronecker products of the topicword matrix A, while (10) involves Khatri-Rao products of A. This forms a crucial criterion in determining of whether overcomplete models are identifiable, as discussed below.\nWhy persistence helps in identifiability of overcomplete models? For simplicity, let the order of the moment 2m = 4. The equations (10) and (11) reduce to\nM (∞) 4 (x) = (A⊙A)Diag\n( E [ h] ) (A⊙A)⊤, (12)\nM (1) 4 (x) = (A⊗A)E\n[ (h⊗ h)(h⊗ h)⊤ ] (A⊗A)⊤. (13)\nNote that for the single topic model in (12), the Khatri-Rao product matrix A ⊙ A ∈ Rp 2×q has the same as the number of columns (i.e. the latent dimensionality) of the original matrix A, while the number of rows (i.e. the observed dimensionality) is increased. Thus, the Khatri-Rao product\n“expands” the effect of hidden variables to higher order observed variables, which is the key towards identifying overcomplete models. In other words, the original overcomplete representation becomes determined due to the ‘expansion effect’ of the Khatri-Rao product structure of the higher order observed moments.\nOn the other hand, in the bag-of-words admixture model in (13), this interesting ‘expansion property’ does not occur, and we have the Kronecker product A⊗A ∈ Rp\n2×q2 , in place of the Khatri-Rao products. The Kronecker product operation increases both the number of the columns (i.e. latent dimensionality) and the number of rows (i.e. observed dimensionality), which implies that higher order moments do not help in identifying overcomplete models.\nAn example is provided in Figure 4 which helps to see how the matrices A⊙A and A⊗A behave differently in terms of mapping topics to word tuples.\nNote that for the n-persistent model, for n = 2, the 4th order moment reduces to\nM (2) 4 (x) = (A⊙A)E\n[ hh⊤](A⊙A)⊤. (14)\nContrasting the above equation with (12) and (13), we find that the 2-persistent model retains the desirable property of possessing Khatri-Rao products, while being more general than the form for single topic model in (12). This key property enables us to establish identifiability of topic models with finite persistence levels."
    }, {
      "heading" : "4.2 Tensor algebra of the model",
      "text" : "In Section 4.1, we provided a representation of the moment forms in the matrix form. We now provide the equivalent tensor representation of the moments. The tensor representation is more compact and transparent, and allows us to compare the topic models under different levels of persistence. We compare the derived tensor form with the well-known Tucker and CP decompositions. We first introduce some tensor notations and definitions."
    }, {
      "heading" : "4.2.1 Tensor notations and definitions",
      "text" : "A real-valued order-n tensor A ∈ ⊗n\ni=1R pi := Rp1×···×pn is a n dimensional array A(1 : p1, . . . , 1 :\npn), where the i-th mode is indexed from 1 to pi. In this paper, we restrict ourselves to the case that p1 = · · · = pn = p, and simply write A ∈ ⊗n R p. A fiber of a tensor A is a vector obtained by\nfixing all indices of A except one, e.g., for A ∈ ⊗4 R 3, the vector f = A(2, 1 : 3, 3, 1) is a fiber. For a vector u ∈ Rp, Diagn (u) ∈ ⊗n R p is the n-th order diagonal tensor with vector u on its\ndiagonal. The tensor A ∈ ⊗n R p, is stacked as a vector a ∈ Rp n by the vec(·) operator, defined as\na = vec(A) ⇔ a ( (i1 − 1)p n−1 + (i2 − 1)p n−2 + · · · + (in−1 − 1)p+ in) ) = A(i1, i2, . . . , in).\nThe inverse of a = vec(A) operation is denoted by A = ten(a). For vectors ai ∈ R pi , i ∈ [n], the tensor outer product operator “◦” is defined as [41]\nA = a1 ◦ a2 ◦ · · · ◦ an ∈ n⊗\ni=1\nR pi ⇔ A(i1, i2, . . . , in) := a1(i1)a2(i2) · · · an(in). (15)\nThe above generated tensor is a rank-1 tensor. The tensor rank is the minimal number of rank-1 tensors into which a tensor can be decomposed. This type of rank is called CP (Candecomp/Parafac) tensor rank in the literature [41]. According to above definitions, for any set of vectors ai ∈ R\npi, i ∈ [n], we have the following pair of equalities:\nvec(a1 ◦ a2 ◦ · · · ◦ an) = a1 ⊗ a2 ⊗ · · · ⊗ an,\nten(a1 ⊗ a2 ⊗ · · · ⊗ an) = a1 ◦ a2 ◦ · · · ◦ an.\nFor any vector a ∈ Rp, the power notations are also defined as\na⊗n := n times︷ ︸︸ ︷ a⊗ a⊗ · · · ⊗ a ∈ Rp n ,\na◦n := n times︷ ︸︸ ︷ a ◦ a ◦ · · · ◦ a ∈ n⊗ R p.\nThe second power is usually called the n-th order tensor power of vector a. Finally, the Tucker and CP (Candecomp/Parafac) representations are defined as follows [10,41]. Definition 7 (Tucker representation). Given a core tensor S ∈ ⊗n i=1R\nri and inverse factors Ui ∈ R pi×ri , i ∈ [n], the Tucker representation of the n-th order tensor A ∈ ⊗n i=1 R pi is\nA =\nr1∑\ni1=1\nr2∑\ni2=1\n· · · rn∑\nin=1\nS(i1, i2, . . . , in)U1(:, i1) ◦ U2(:, i2) ◦ · · · ◦ Un(:, in) =: [[S;U1, U2, . . . , Un]], (16)\nwhere Uj(:, ij) denotes the ij-th column of matrix Uj . The tensor S is referred to as the core tensor. Definition 8 (CP representation). Given λ ∈ Rr, Ui ∈ Rpi×r, i ∈ [n], the CP representation of the n-th order tensor A ∈ ⊗n i=1R pi is\nA = r∑\ni=1\nλiU1(:, i) ◦ U2(:, i) ◦ · · · ◦ Un(:, i) =: [[Diagn (λ);U1, U2, . . . , Un]], (17)\nwhere Uj(:, i) denotes the i-th column of matrix Uj.\nNote that the CP representation is a special case of the Tucker representation when the core tensor S is square and diagonal."
    }, {
      "heading" : "4.2.2 Tensor representation of moments under topic model",
      "text" : "We now provide a tensor representation of the moments.\nFor the n-persistent topic model, the 2m-th observed moment is denoted by T (n) 2m (x), which is the tensor form of the moment matrix M (n) 2m (x), characterized in Lemma 1. It is given by\nT2m(x)(i1,i2,...,i2m) := E[x1(i1)x2(i2) · · · x2m(i2m)], i1, i2, . . . , i2m ∈ [p], (18)\nwhere T2m(x) ∈ ⊗2m R p.\nThis tensor is characterized in the following lemma, and is proved in Appendix A.2. Lemma 2 (n-persistent topic model moment characterization in tensor form). The (2m)-th order moment of words, defined in equation (18), for the n-persistent topic model is characterized as 10:\n• if m = rn for some integer r ≥ 1, then\nT (n) 2m (x) =\nq∑\ni1=1\nq∑\ni2=1\n· · ·\nq∑\ni2r=1\nE[hi1hi2 · · · hi2r ]a ◦n i1 ◦ a ◦n i2 ◦ · · · ◦ a ◦n i2r (19)\n= [[ Sr; 2m times︷ ︸︸ ︷ A,A, . . . , A ]] ,\n10The other cases not covered in Lemma 2 are deferred to Appendix A.2. See Remark 12.\nwhere Sr ∈ ⊗2rn R q is the core tensor in the above Tucker representation with the sparsity pattern as\nSr ( i ) =\n{ M2r(h)((in,i2n,...,irn),(i(r+1)n,i(r+2)n,...,i2rn)\n) , i1= i2= · · ·= in, in+1= in+2= · · ·= i2n, . . . 0 , o.w.,\nwhere i := (i1, i2, . . . , i2rn).\n• If n ≥ 2m, then\nT (n) 2m (x) =\n∑\ni∈[q]\nE[hi]a ◦2m i = [[ Diag2m(E[h]); 2m times︷ ︸︸ ︷ A,A, . . . , A ]] . (20)\nThe tensor representation in (19) is a specific type of tensor decomposition which is a special case of the Tucker representation (since Sr is not fully dense), but more general than the CP representation. The tensor representation in (20) has a CP form.\nComparison with single topic model and bag-of-words admixture model\nWe now provide the tensor form for the special cases single topic model and bag-of-words admixture model. In order to have a fair comparison, the number of observed variables is fixed to 2m and the persistence level is varied.\nCP representation of the single topic model: The (2m)-th order moment of the words for the single topic model (infinite-persistent topic model) is provided in equation (20) as\nT (∞) 2m (x) =\n∑\ni∈[q]\nE[hi]a ◦2m i = [[ Diag2m(E[h]); 2m times︷ ︸︸ ︷ A,A, . . . , A ]] . (21)\nThis representation is the symmetric CP representation 11 of T (∞) 2m (x).\nTucker representation of the bag-of-words admixture model: From Lemma 2, the tensor form of the (2m)-th order moment of observed variables xl, l ∈ [2m], for the bag-of-words admixture model (1-persistent topic model) is given by\nT (1) 2m(x) =\nq∑\ni1=1\nq∑\ni2=1\n· · ·\nq∑\ni2m=1\nE[hi1hi2 · · · hi2m ]ai1 ◦ ai2 ◦ · · · ◦ ai2m\n= [[ E [ h◦(2m) ] ; 2m times︷ ︸︸ ︷ A,A, . . . , A ]] . (22)\nThis representation is the Tucker representation (decomposition) of T (1) 2m(x) where the core tensor\nS = E [ h◦(2m) ] is the tensor form of the (2m)-th order hidden moment M2m(h), defined in equation (3), and the inverse factors correspond to the population structure A.\n11In Appendix C, we provide a more detailed comparison between our approach and some of the previous identifiability results for the (overcomplete) CP decomposition.\nComparing the tensor forms for the n-persistent topic model (19), single topic model (21), and bag of words admixture model (22), we find that all of them involve Tucker decompositions, where the inverse factors correspond to the topic-word matrix A, and the only difference is in the sparsity level of the core tensor S. For the bag of words model, with n = 1, the core tensor is fully dense in general, while for the single topic model, with n → ∞, the core tensor is diagonal which reduces to the CP decomposition. For a general topic model with persistence level n, the core tensor is in between these two extremes and has structured sparsity. This sparsity property of the core tensor is crucial towards establishing identifiability in the overcomplete regime. The bag-of-words model is not identifiable in the overcomplete regime since the core tensor is fully dense in this case, while an overcomplete n-persistent topic model can be identified under certain constraints provided in Section 3, since the core tensor has structured sparsity and symmetry."
    }, {
      "heading" : "5 Proof Techniques and Auxiliary Results",
      "text" : "The main identifiability results are given in Theorems 1 and 2 for deterministic and random cases of topic-word graph structures. In this section, we provide a proof sketch of these results, and then, we propose auxiliary results on the existence of perfect n-gram matching for random bipartite graphs and a lower bound on the Kruskal rank of random matrices."
    }, {
      "heading" : "5.1 Proof sketch",
      "text" : "Summary of relationships among different conditions: To summarize, there exists a hierarchy among the proposed conditions as follows. See Figure 5. First, in the random analysis, the size and the degree conditions 4 and 5 are sufficient for satisfying the perfect n-gram matching and the krank conditions 2 and 3, shown by Theorems 3 and 4. Then, these conditions 2 and 3 ensure that the rank and the expansion conditions 6 and 7 hold, shown by Lemma 5. And finally, these conditions 6 and 7 together with non-degeneracy condition 1 conclude the primary identifiability result in Theorem 5. Note that the genericity of A is also required for these results to hold.\nPrimary deterministic analysis in Theorem 5: The deterministic analysis is primarily based on conditions on the n-gram matrix A⊙n; but since these conditions are opaque (mainly expansion condition on A⊙n, provided in condition 7), this analysis is related to conditions on matrix A itself. See Theorem 5 in Appendix A.1 for the identifiability result based on A⊙n. We briefly discuss it below for the case when 2n number of words are available under the n-persistent topic model. From equation (8), the (2n)-th order moment of the observed variables under the n-persistent topic\nmodel can be written as\nM (n) 2n (x) =\n( A⊙n ) E [ hh⊤ ]( A⊙n )⊤ . (23)\nThe question is whether we can recover A, given the M (n) 2n (x). Obviously, the matrix A is not identifiable without any further conditions. First, non-degeneracy and rank conditions (conditions 1 and 6) are required. Assuming these two conditions, we have from (23) that\nCol ( M\n(n) 2n (x)\n) = Col ( A⊙n ) .\nTherefore, the problem of recovering A from M (n) 2n (x) reduces to finding A\n⊙n in Col ( A⊙n ) .\nThen, we show that under the following expansion condition on A⊙n and the genericity property, matrix A is identifiable from Col ( A⊙n ) . The expansion condition (refer to condition 7 for a more detailed statement), imposes the following property on the bipartite graph G ( Vh, V (n) o ;A⊙n ) 12,\n∣∣∣NA⊙nRest.(S) ∣∣∣ ≥ |S|+ dmax ( A⊙n ) , ∀S ⊆ Vh, |S| > krank(A), (24)\nwhere dmax ( A⊙n ) is the maximum node degree in set Vh, and the restricted version of n-gram matrix, denoted by A⊙nRest., is obtained by removing its redundant (identical) rows (see Definition 9). The identifiability claim is proved by showing that the columns of A⊙n are the sparsest and rank-1 vectors (in the tensor form) in Col ( A⊙n ) under the expansion condition in (24) and genericity conditions. Note that since we only require expansion on sets larger than Kruskal rank, the expansion condition (24) is a more relaxed condition compared to expansion condition proposed in [7, 43] for identifiability in the undercomplete regime. For a more detailed comparison, refer to Remark 11 in Appendix A.1.\nDeterministic analysis in Theorem 1: Expansion and rank conditions in Theorem 5 are imposed on the n-gram matrix A⊙n. According to the generalized matching notions, defined in Section 3.1, sufficient combinatorial conditions on matrix A (conditions 2 and 3) are introduced which ensure that the expansion and rank conditions on A⊙n are satisfied. The following lemma is employed to establish these results, where we state an interesting property which relates the existence of a perfect matching in A⊙n to the existence of a perfect n-gram matching in A. Lemma 3. If G(Y,X;A) has a perfect n-gram matching, then G(Y,X(n);A⊙n) has a perfect matching. In the other direction, if G(Y,X(n);A⊙n) has a perfect matching M⊙n, then G(Y,X;A) has a perfect n-gram matching under the following condition on M⊙n. All the matching edges (j, (i1, . . . , in)) ∈ M\n⊙n should satisfy i1 6= i2 6= · · · 6= in for all j ∈ Y . In words, the matching edges should be connected to nodes in X(n), which are indexed by tuples of distinct indices.\nSee Appendix A.4 for a proof. Using this lemma, condition 2 implies that G(Y,X(n);A⊙n) has a perfect matching. Then, it is straightforward to argue that the expansion and rank conditions on A⊙n are satisfied, which is shown in Lemma 5 in Appendix A.3. This leads to the generic identifiability result stated in Theorem 1.\n12V (n) o denotes all ordered n-tuples generated from set Vo := {1, . . . , p} which indexes the rows of A ⊙n."
    }, {
      "heading" : "5.2 Analysis of Random Structures",
      "text" : "The identifiability result for a random structured matrix A is provided in Theorem 2. Sufficient size and degree conditions 4 and 5 on the random matrix A are proposed such that the deterministic combinatorial conditions 2 and 3 on A are satisfied. The details of these auxiliary results are provided in the following two subsequent sections. In Section 5.2.1, it is proved in Theorem 3 that a random bipartite graph satisfying reasonable size and degree constraints, has a perfect n-gram matching (condition 2), whp. Then, a lower bound on the Kruskal rank of a random matrix A under size and degree constraints is provided in Theorem 4 in Section 5.2.2, which implies the krank condition 3. Intuitions on why such size and degree conditions are required, are mentioned in Section 3.2 where these conditions are proposed.\n5.2.1 Existence of perfect n-gram matching for random bipartite graphs\nWe show in the following theorem that a random bipartite graph satisfying reasonable size and degree constraints, proposed earlier in conditions 4 and 5, has a perfect n-gram matching whp. Theorem 3 (Existence of perfect n-gram matching for random bipartite graphs). Consider a random bipartite graph G(Y,X;E) with |Y | = q nodes on the left side and |X| = p nodes on the right side, and each node i ∈ Y is randomly connected to di different nodes in X. Let dmin := mini∈Y di. Assume that it satisfies the size condition q ≤ ( c pn )n (condition 4) for some constant 0 < c < 1 and the degree condition dmin ≥ max{1 + β log p, α log p} for some constants β > n−1\nlog 1/c , α >\nmax { 2n2 ( β log 1c+1 ) , 2βn } (lower bound in condition 5). Then, there exists a perfect (Y -saturating) n-gram matching in the random bipartite graph G(Y,X;E), with probability at least 1− γ1p −β′ for constants β′ > 0 and γ1 > 0, specified in (5) and (6).\nNote that the sufficient size bound q = O(pn) in the above theorem is also necessary (see Remark 3), and is therefore tight. Remark 10 (Insufficiency of the union bound argument). It is easier to exploit the union bound arguments to propose random bipartite graphs which have a perfect n-gram matching whp. It is proved in Appendix B.1 that if d ≥ n and the size constraint |Y | = O(|X| n 2 −δ) for some δ > 0 is satisfied, then whp, the random bipartite graph has a perfect n-gram matching. Comparing this result with ours in Theorem 3, our approach has a better size scaling while the union bound approach has a better degree scaling. The size scaling limitation in the union bound argument makes it unattractive. In order to identify the population structure A in the overcomplete regime where |Y | = O(|X|n), we need access to at least (4n)-th order moment under the union bound argument, while only the (2n)-th order moment is required under our argument."
    }, {
      "heading" : "5.2.2 Lower bound on the Kruskal rank of random matrices",
      "text" : "In the following theorem, a lower bound on the Kruskal rank of a random matrix A under dimension and degree constraints is provided, which is proved in Appendix B.1.\nTheorem 4 (Lower bound on the Kruskal rank of random matrices). Consider a random matrix A ∈ Rp×q, where for any i ∈ [q], there are di number of random non-zero entries in column i. Let\ndmin := mini∈[q] di. Assume that it satisfies the size condition q ≤ ( c pn )n (condition 4) for some constant 0 < c < 1 and the degree condition dmin ≥ 1 + β log p for some constant β > n−1\nlog 1/c (lower\nbound in condition 5) and in addition A is generic. Then, krank(A) ≥ cp, with probability at least 1− γ2p −β′ for constants β′ > 0 and γ2 > 0, specified in (5) and (7)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors acknowledge useful discussions with Sina Jafarpour, Adel Javanmard, Alex Dimakis, Moses Charikar, Sanjeev Arora, Ankur Moitra and Kamalika Chaudhuri. A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, NSF Award CCF-1219234, ARO Award W911NF-12-1-0404, and ARO YIP Award W911NF-13-1-0084. M. Janzamin is supported by NSF Award CCF-1219234, ARO Award W911NF-12-1-0404 and ARO YIP Award W911NF-13-1-0084.\nAppendix"
    }, {
      "heading" : "A Proof of Deterministic Identifiability Result (Theorem 1)",
      "text" : "First, we show the identifiability result under an alternative set of conditions on the n-gram matrix, A⊙n, and then, we show that the conditions of Theorem 1 are sufficient for these conditions to hold.\nA.1 Deterministic analysis based on A⊙n\nIn this section, the deterministic identifiability result based on conditions on the n-gram matrix, A⊙n, is provided.\nIn the n-gram matrix, A⊙n ∈ Rp n×q, redundant rows exist. If some row of A⊙n is indexed by n-tuple (i1, . . . , in) ∈ [p] n, then another row indexed by any permutation of the tuple (i1, . . . , in)\nhas the same entries. Therefore, the number of distinct rows of A⊙n is at most (p+n−1\nn\n) . In the\nfollowing definition, we define a non-redundant version of n-gram matrix which is restricted to the (potentially) distinct rows. Definition 9 (Restricted n-gram matrix). For any matrix A ∈ Rp×q, restricted n-gram matrix A⊙nRest. ∈ R s×q, s = (p+n−1 n ) , is defined as the restricted version of n-gram matrix A⊙n ∈ Rp\nn×q, where the redundant rows of A⊙n are removed, as explained above. Condition 6 (Rank condition). The n-gram matrix A⊙n is full column rank. Condition 7 (Graph expansion). Let G(Vh, V (n) o ;A⊙n) denote the bipartite graph with vertex sets Vh corresponding to the hidden variables (indexing the columns of A ⊙n) and V (n) o corresponding to the n-th order observed variables (indexing the rows of A⊙n) and edge matrix A⊙n ∈ R|V (n) o |×|Vh|. The bipartite graph G(Vh, V (n) o ;A⊙n) satisfies the following expansion property on the restricted\nversion specified by A⊙nRest.,\n∣∣∣NA⊙nRest.(S) ∣∣∣ ≥ |S|+ dmax ( A⊙n ) , ∀S ⊆ Vh, |S| > krank(A), (25)\nwhere dmax\n( A⊙n ) is the maximum node degree in set Vh.\nRemark 11. The expansion condition for the bag-of-words admixture model is provided in (4), introduced in [7]. The proposed expansion condition in (25) is inherited from (4), with two major modifications. First, the condition is appropriately generalized for our model which involves a graph with edges specified by the n-gram matrix, A⊙n, as stated in (23). Second, the expansion property (4), proposed in [7], needs to be satisfied for all subsets S with size |S| ≥ 2, which is a stricter condition than the one proposed here in (25), since we can have krank(A) ≫ 2.\nThe deterministic identifiability result based on the conditions on A⊙n, is stated in the following theorem for n ≥ 2, while n = 1 case is addressed in Remarks 4 and 11. The identifiability result relies on access to the (2n)-th order moment of observed variables xl, l ∈ [2n], defined in equation (2) as\nM2n(x) := E [ (x1 ⊗ x2 ⊗ · · · ⊗ xn)(xn+1 ⊗ xn+2 ⊗ · · · ⊗ x2n) ⊤ ] ∈ Rp n×pn .\nTheorem 5 (Generic identifiability under deterministic conditions on A⊙n). Let M (n) 2n (x) (defined in equation (2)) be the (2n)-th order moment of the n-persistent topic model described in Section 2. If the model satisfies conditions 1, 6 and 7, then, for any n ≥ 2, all the columns of population structure A are generically identifiable from M (n) 2n (x). Proof: Define B := A⊙n ∈ Rp n×q. Then, the moment characterized in equation (23) can be written as M (n) 2n (x) = BE [ hh⊤ ] B⊤. Since both matrices E [ hh⊤ ] and B have full column rank\n(from conditions 1 and 6), the rank of BE [ hh⊤ ] B⊤ is q where q = O(pn), and furthermore Col(BE [ hh⊤ ] B⊤) = Col(B). Let U := {u1, . . . , uq} ∈ R pn be any basis of Col(BE [ hh⊤ ] B⊤) satisfying the following two properties:\n1) ui’s have the smallest ℓ0 norms.\n2) ui’s have q smallest (tensor) ranks in the n-th order tensor form, i.e., Ui := ten(ui), i ∈ [q], have q smallest ranks.\nLet the columns of matrix B be bi for i ∈ [q]. Since all the bi’s (which belong to Col(BE [ hh⊤ ] B⊤)) are rank-1 in the n-th order tensor form (since ten(bi) = a ◦n i ) and the number of non-zero entries in each of bi’s is at most dmax(B) = dmax(A) n, we conclude that\nmax i Rank(ten(ui)) = 1 and max i ‖ui‖0 ≤ dmax(B). (26)\nThe above bounds are concluded from the fact that bi ∈ Col(BE [ hh⊤ ] B⊤), i ∈ [q], and therefore the ℓ0 norm and the rank properties of bi’s are upper bounds for the corresponding properties of basis vectors ui’s (according to the proposed conditions for ui’s). Now, exploiting these observations and also the genericity of A and the expansion condition 7, we show that the basis vectors ui’s are scaled columns of B. Since ui for i ∈ [q], is a vector in the column space of B, it can be represented as ui = Bvi for some vector vi ∈ R q. Equivalently, for\nany i ∈ [q], ui = ∑q j=1 vi(j)bj where bj = a ⊗n j is the j-th column of matrix B and vi(j) is a scalar which is the j-th entry of vector vi. Then, the tensor form of ui can be written as\nten(ui) =\nq∑\nj=1\nvi(j) ten(bj) =\nq∑\nj=1\nvi(j) ten(a ⊗n j ) =\nq∑\nj=1\nvi(j)a ◦n j = [[Diagn (vi);\nn times︷ ︸︸ ︷ A, . . . , A]], (27)\nwhere the last equality is based on the notation defined in Definition 8. We define ṽi := [vi(j)]j:vi(j)6=0 as the vector which contains only the non-zero entries of vi, i.e., ṽi is the restriction of vector vi to its support. Therefore, ṽi ∈ R\nr, where r := ‖vi‖0. Furthermore, the matrix Ãi := {aj : vi(j) 6= 0} ∈ Rp×r is defined as the restriction of A to its columns corresponding to the support of vi. Let (ãi)j denote the j-th column of Ãi. According to these definitions, equation (27) reduces to\nten(ui) = [[Diagn (ṽi); n times︷ ︸︸ ︷ Ãi, . . . , Ãi]] = r∑\nj=1\nṽi(j)[(ãi)j ] ◦n, (28)\nwhich is derived by removing columns of A corresponding to the zero entries in vi. Next, we rule out that ‖vi‖0 ≥ 2 under two cases (2 ≤ ‖vi‖0 ≤ krank(A) and krank(A) < ‖vi‖0 ≤ q), to conclude that ui’s vectors are scaled columns of B.\nCase 1: 2 ≤ ‖vi‖0 ≤ krank(A). Here, the number of columns of Ãi ∈ R p×‖vi‖0 is less than or equal to krank(A) and therefore it is full column rank. Since, all the components of CP representation in equation (28) are full column rank 13, for any 14 n ≥ 2, we have Rank(ten(ui)) = r = ‖vi‖0 > 1, which contradicts the fact that maxiRank(ten(ui)) = 1 in (26).\nCase 2: krank(A) < ‖vi‖0 ≤ q. Here, we first restrict the n-gram matrix B to distinct rows, denoted by BRest., as defined in Definition 9. Let u ′ i = BRest.vi. Since u ′ i is the restricted version of ui, we have\n‖ui‖0 ≥ ‖u ′ i‖0 = ‖BRest.vi‖0\n> ∣∣NBRest.(Supp(vi)) ∣∣− |Supp(vi)| ≥ dmax(B),\nwhere the second inequality is from Lemma 4, and the third inequality follows from the graph expansion property (condition 7). This result contradicts the fact that maxi‖ui‖0 ≤ dmax(B) in (26).\nFrom above contradictions, ‖vi‖0 = 1 and hence, columns of B := A ⊙n are the scaled versions of ui’s. ✷\n13Note that for n ≥ 3, this full rank condition can be relaxed by Kruskal’s condition for uniqueness of CP decomposition [15] and its generalization to higher order tensors [44]. Precisely, instead of saying Rank ( Ãi ) = krank ( Ãi ) = r, it is only required to have krank ( Ãi ) ≥ (2r+n− 1)/n to argue the result of case 1. This only improves the constants involved in the final result. 14Note that for n = 1, since the (tensor) rank of any vector is 1, this analysis does not work.\nThe following lemma is useful in the proof of Theorem 5. The result proposed in this lemma is similar to the parameter genericity condition in [7], but generalized for the n-gram matrix, A⊙n. The lemma is proved on lines of the proof of Remark 2.2 in [7]. Lemma 4. If A ∈ Rp×q is generic, then the n-gram matrix A⊙n ∈ Rp\nn×q satisfies the following property with Lebesgue measure one. For any vector v ∈ Rq with ‖v‖0 ≥ 2, we have\n∥∥A⊙nRest.v ∥∥ 0 > ∣∣∣NA⊙nRest.(Supp(v)) ∣∣∣ − |Supp(v)|,\nwhere for a set S ⊆ [q], NA⊙n(S) := {i ∈ [p] n : A⊙n(i, j) 6= 0 for some j ∈ S}.\nHere, we prove the result for the case of n = 2. The proof can be easily generalized to larger n.\nLet A := M + Z be generic, where M is an arbitrary matrix, perturbed by random continuous perturbations Z. Consider the 2-gram matrix B := A⊙A ∈ Rp 2×q . It is shown that the restricted version of B, denoted by B̃ := BRest. ∈ R p(p+1) 2 ×q, satisfies the above genericity condition. We first establish some definitions. Definition 10. We call a vector fully dense if all of its entries are non-zero. Definition 11. We say a matrix has the Null Space Property (NSP) if its null space does not contain any fully dense vector. Claim 1. Fix any S ⊆ [q] with |S| ≥ 2, and set R := N\nM (2 -gram) Rest.\n(S). Let C̃ be a |S|× |S| submatrix\nof B̃R,S. Then Pr(C̃ has the NSP) = 1.\nProof of Claim 1: First, note that B̃ can be expanded as\nB̃ := (A⊙A)Rest. = (M ⊙M)Rest. + (M ⊙ Z + Z ⊙M)Rest. + (Z ⊙ Z)Rest.︸ ︷︷ ︸ :=U .\nLet s = |S| and let C̃ = [c̃1|c̃2| · · · |c̃s] ⊤, where c̃⊤i is the i-th row of C̃. Also, let C := [c1|c2| · · · |cs] ⊤ and W := [w1|w2| · · · |ws] ⊤ be the corresponding |S| × |S| submatrices of M (2 -gram) Rest. and U , respectively. For each i ∈ [s], denote by Ni the null space of the matrix C̃i = [c̃1|c̃2| · · · |c̃i] ⊤. Finally let N0 = R s. Then, N0 ⊇ N1 ⊇ · · · ⊇ Ns. We need to show that, with probability one, Ns does not contain any fully dense vector.\nIf one of Ni, i ∈ [s], does not contain any full dense vector, the result is proved. Suppose that Ni contains some fully dense vector v. Since C is a submatrix of M (2 -gram) R,S , every row c ⊤ i+1 of C contains at least one non-zero entry. Therefore,\nv⊤c̃i+1 = ∑\nj∈[s]\nv(j)c̃i+1(j)\n= ∑\nj∈[s]:ci+1(j)6=0\nv(j)(ci+1(j) + wi+1(j)),\nwhere {wi+1(j) : j ∈ [s] s.t. ci+1(j) 6= 0} are independent random variables, and moreover, they are independent of c̃1, . . . , c̃i and thus of v. By assumption on the distribution of the wi+1(j),\nPr [ v ∈ Ni+1 ∣∣∣∣c̃1, c̃2, . . . , c̃i ] = Pr [ ∑\nj∈[s]:ci+1(j)6=0\nv(j)(ci+1(j) + wi+1(j)) = 0 ∣∣∣∣c̃1, c̃2, . . . , c̃i ] = 0. (29)\nConsequently,\nPr [ dim(Ni+1) < dim(Ni) ∣∣∣∣c̃1, c̃2, . . . , c̃i ] = 1 (30)\nfor all i = 0, . . . , s− 1. As a result, with probability one, dim(Ns) = 0. ✷\nNow, we are ready to prove Lemma 4.\nProof of Lemma 4: It follows from Claim 1 that, with probability one, the following event holds: for every S ⊆ [q], |S| ≥ 2, and every |S| × |S| submatrix C̃ of B̃R,S where R := NM (2 -gram)Rest. (S), then C̃ has the NSP.\nNow fix v ∈ Rq with ‖v‖0 ≥ 2. Let S := Supp(v) and H := B̃R,S . Furthermore, let u ∈ (R \\ {0}) |S| be the restriction of vector v to S; observe that u is fully dense. It is clear that ‖B̃v‖0 = ‖Hu‖0, so we need to show that\n‖Hu‖0 > |R| − |S|. (31)\nFor the sake of contradiction, suppose that Hu has at most |R| − |S| non-zero entries. Since Hu ∈ R|R|, there is a subset of |S| entries on which Hu is zero. This corresponds to a |S| × |S| submatrix of H := B̃R,S which contains u in its null space. It means that this submatrix does not have the NSP, which is a contradiction. Therefore we conclude that Hu must have more than |R| − |S| non-zero entries, which finishes the proof. ✷\nA.2 Proof of moment characterization lemmata\nRemark 12. In Lemmata 1 and 2, a specific case of order and persistence (m = rn) was considered. Here, we provide the moment form for a more general case. Assume that m = rn + s for some integers r ≥ 1, 1 ≤ s ≤ n2 , then\nM (n) 2m (x) =\n( r times︷ ︸︸ ︷ A⊙n ⊗ · · · ⊗A⊙n⊗A(s -gram) )\nM̃2r(h)\n( A((n−s) -gram) ⊗ r−1 times︷ ︸︸ ︷ A⊙n ⊗ · · · ⊗A⊙n⊗A(2s -gram) )⊤ ,\nwhere M̃2r(h) ∈ Rq r+1×qr+1 is the hidden moment as\nM̃2r(h)((i1,...,ir+1),(j1,...,jr+1) ) :=\n{ E[hi1 · · · hirh 2 ir+1 hj2 · · · hjr+1 ] if ir+1 = j1,\n0 o.w .\nThe tensor form is also characterized as\nT (n) 2m (x) = [[ S̃r; 2m times︷ ︸︸ ︷ A,A, . . . , A ]] ,\nwhere S̃r ∈ ⊗2m R q is the core tensor in the above Tucker representation with the sparsity pattern as follows. Let i := (i1, i2, . . . , i2m). If\ni1 = i2 = · · · = in, in+1 = in+2 = · · · = i2n, · · · , i(2r−1)n+1 = i(2r−1)n+2 = · · · = i2rn,\ni2(m−s)+1 = i2(m−s)+2 = · · · = i2m,\nwe have S̃r ( i ) = M̃2r(h)((in,i2n,...,irn,im),(i(r+1)n,i(r+2)n,...,i2rn,i2m) ). Otherwise, S̃r ( i ) = 0.\nProof of Lemma 1: In order to simplify the notation, similar to tensor powers for vectors, the tensor power for a matrix U ∈ Rp×q is defined as\nU⊗r := r times︷ ︸︸ ︷ U ⊗ U ⊗ · · · ⊗ U ∈ Rp r×qr . (32)\nFirst, consider the case m = rn for some integer r ≥ 1. One advantage of encoding yj, j ∈ [2r], by basis vectors appears in characterizing the conditional moments. The first order conditional moment of words xl, l ∈ [2m], in the n-persistent topic model can be written as\nE [ x(j−1)n+k|yj ] = Ayj, j ∈ [2r], k ∈ [n],\nwhere A = [a1|a2| · · · |aq] ∈ R p×q. Next, the m-th order conditional moment of different views xl, l ∈ [m], in the n-persistent topic model can be written as\nE[x1 ⊗ x2 ⊗ · · · ⊗ xm|y1 = ei1 , y2 = ei2 , . . . , yr = eir ] = a ⊗n i1 ⊗ a⊗ni2 ⊗ · · · ⊗ a ⊗n ir ,\nwhich is derived from the conditional independence relationships among the observations xl, l ∈ [m], given topics yj, j ∈ [r]. Similar to the first order moments, since vectors yj, j ∈ [r], are encoded by the basis vectors ei ∈ R\nq, the above moment can be written as the following matrix multiplication\nE[x1 ⊗ x2 ⊗ · · · ⊗ xm|y1, y2, . . . , yr] = ( A⊙n )⊗r (y1 ⊗ y2 ⊗ · · · ⊗ yr) , (33)\nwhere the (·)⊗r notation is defined in equation (32). Now for the (2m)-th order moment, we have\nM (n) 2m (x) := E [ (x1 ⊗ x2 ⊗ · · · ⊗ xm)(xm+1 ⊗ xm+2 ⊗ · · · ⊗ x2m) ⊤ ]\n= E(y1,y2,...,y2r)\n[ E [ (x1 ⊗ · · · ⊗ xm)(xm+1 ⊗ · · · ⊗ x2m) ⊤|y1, y2, . . . , y2r ]]\n(a) = E(y1,y2,...,y2r) [ E [ (x1 ⊗ · · · ⊗ xm)|y1, . . . , y2r ] E [ (xm+1 ⊗ · · · ⊗ x2m) ⊤|y1, . . . , y2r ]]\n(b) = E(y1,y2,...,y2r) [ E [ (x1 ⊗ · · · ⊗ xm)|y1, . . . , yr ] E [ (xm+1 ⊗ · · · ⊗ x2m) ⊤|yr+1, . . . , y2r ]]\n(c) = E(y1,y2,...,y2r)\n[([ A⊙n ]⊗r) (y1 ⊗ · · · ⊗ yr) (yr+1 ⊗ · · · ⊗ y2r) ⊤ ([ A⊙n ]⊗r)⊤ ]\n= ([ A⊙n ]⊗r) E [ (y1 ⊗ · · · ⊗ yr) (yr+1 ⊗ · · · ⊗ y2r) ⊤ ]([ A⊙n ]⊗r)⊤\n(d) = ([ A⊙n ]⊗r) M2r(y) ([ A⊙n ]⊗r)⊤ , (34)\nwhere (a) results from the independence of (x1, . . . , xm) and (xm+1, . . . , x2m) given (y1, y2, . . . , y2r) and (b) is concluded from the independence of (x1, . . . , xm) and (yr+1, . . . , y2r) given (y1, . . . , yr) and the independence of (xm+1, . . . , x2m) and (y1, . . . , yr) given (yr+1, . . . , y2r). Equation (33) is used in (c) and finally, the (2r)-th order moment of (y1, . . . , y2r) is defined as M2r(y) :=\nE [ (y1 ⊗ · · · ⊗ yr) (yr+1 ⊗ · · · ⊗ y2r) ⊤ ] in (d).\nFor M2r(y), we have by the law of total expectation\nM2r(y) := E [ (y1 ⊗ · · · ⊗ yr) (yr+1 ⊗ · · · ⊗ y2r) ⊤]\n= Eh [ E [ (y1 ⊗ · · · ⊗ yr) (yr+1 ⊗ · · · ⊗ y2r) ⊤ |h ]]\n= Eh\n[( r times︷ ︸︸ ︷ h⊗ · · · ⊗ h )( r times︷ ︸︸ ︷ h⊗ · · · ⊗ h )⊤]\n= M2r(h),\nwhere the third equality is concluded from the conditional independence of variables yj, j ∈ [2r], given h and the model assumption that E [ yj|h ] = h, j ∈ [2r]. Substituting this in equation (34), finishes the proof for the n-persistent topic model. Similarly, the moment of single topic model (infinite persistence) can be also derived. ✷\nProof of Lemma 2: Defining Λ := M2r(h) ∈ R qr×qr and B := [ A⊙n ]⊗r ∈ Rp rn×qr , the (2rn)-th order moment M (n) 2rn(x) ∈ R\nprn×prn of the n-persistent topic model proposed in equation (8) can be written as\nM (n) 2rn(x) = BΛB ⊤.\nLet b(i1,...,ir) ∈ R prn denote the corresponding column of B indexed by r-tuple (i1, . . . , ir), ik ∈ [q], k ∈ [r]. Then, the above matrix equation can be expanded as\nM (n) 2rn(x) =\n∑\ni1,...,ir∈[q] j1,...,jr∈[q]\nΛ ( (i1, . . . , ir), (j1, . . . , jr) ) b(i1,...,ir)b ⊤ (j1,...,jr)\n= ∑\ni1,...,ir∈[q] j1,...,jr∈[q]\nΛ ( (i1, . . . , ir), (j1, . . . , jr) ) [a⊗ni1 ⊗ · · · ⊗ a ⊗n ir ][a⊗nj1 ⊗ · · · ⊗ a ⊗n jr ]⊤,\nwhere relation b(i1,...,ir) = a ⊗n i1 ⊗· · ·⊗a⊗nir , i1, . . . , ir ∈ [q], is used in the last equality. Let m (n) 2rn(x) ∈ R p2rn denote the vectorized form of (2rn)-th order moment M\n(n) 2rn(x) ∈ R prn×prn . Therefore, we have\nm (n) 2rn(x) := vec\n( M\n(n) 2rn(x)\n)\n= ∑\ni1,...,ir∈[q] j1,...,jr∈[q]\nΛ ( (i1, . . . , ir), (j1, . . . , jr) ) a⊗ni1 ⊗ · · · ⊗ a ⊗n ir ⊗ a⊗nj1 ⊗ · · · ⊗ a ⊗n jr .\nThen, we have the following equivalent tensor form for the original model proposed in equation (8)\nT (n) 2rn(x) := ten\n( m\n(n) 2rn(x)\n)\n= ∑\ni1,...,ir∈[q] j1,...,jr∈[q]\nΛ ( (i1, . . . , ir), (j1, . . . , jr) ) a◦ni1 ◦ · · · ◦ a ◦n ir ◦ a ◦n j1 ◦ · · · ◦ a ◦n jr .\n✷\nA.3 Sufficient matching properties for satisfying rank and graph expansion conditions\nIn the following lemma, it is shown that under a perfect n-gram matching and additional genericity and krank conditions, the rank and graph expansion conditions 6 and 7 on A⊙n, are satisfied. Lemma 5. Assume that the bipartite graph G(Vh, Vo;A) has a perfect n-gram matching (condition 2 is satisfied). Then, the following results hold for the n-gram matrix A⊙n:\n1) If A is generic, A⊙n is full column rank (condition 6) with Lebesgue measure one (almost surely).\n2) If krank condition 3 holds, A⊙n satisfies the proposed expansion property in condition 7.\nProof: Let M denote the perfect n-gram matching of the bipartite graph G(Vh, Vo;A). From Lemma 3, there exists a perfect matching M⊙n for the bipartite graph G(Vh, V (n) o ;A⊙n). Denote the corresponding bi-adjacency matrix to the edge set M as AM . Similarly, BM denotes the corresponding bi-adjacency matrix to the edge set M⊙n. Note that Supp(AM ) ⊆ Supp(A) and Supp(BM ) ⊆ Supp(A ⊙n).\nSince BM is a perfect matching, it consists of q := |Vh| rows, each of which has only one non-zero entry, and furthermore, the non-zero entries are in q different columns. Therefore, these rows form q linearly independent vectors. Since the row rank and column rank of a matrix are equal, and the number of columns of BM is q, the column rank of BM is q or in other words, BM is full column rank. Since A is generic, from Lemma 6 (with a slight modification in the analysis 15), A⊙n is also full column rank with Lebesgue measure one (almost surely). This completes the proof of part 1.\nNext, the second part is proved. From krank definition, we have\n|NA(S ′)| ≥ |S′| for S′ ⊆ Vh, |S ′| ≤ krank(A),\n15Lemma 6 result is about the column rank of A itself, but here it is about the column rank of A⊙n for which the same analysis works. Note that the support of BM (which is full column rank here) is within the support of A ⊙n and therefore Lemma 6 can still be applied.\nwhich is concluded from the fact that the corresponding submatrix of A specified by S′ should be full column rank. From this inequality, we have\n|NA(S ′)| ≥ krank(A) for S′ ⊆ Vh, |S ′| = krank(A). (35)\nThen, we have\n|NA(S)| ≥ |NA(S ′)| for S′ ⊂ S ⊆ Vh, |S| > krank(A), |S ′| = krank(A),\n≥ krank(A) ≥ dmax(A) n, (36)\nwhere (35) is used in the second inequality and the last inequality is from krank condition 3.\nIn the restricted n-gram matrix A⊙nRest., the number of neighbors for a set S ⊆ Vh, |S| > krank(A), can be bounded as\n∣∣∣NA⊙nRest.(S) ∣∣∣ ≥ |NA(S)| + |S|\n≥ dmax(A) n + |S| for |S| > krank(A),\nwhere the first inequality is due to the fact that the set NA⊙nRest. consists of rows indexed by the following two subsets: n-tuples (i, i, . . . , i) where all the indices are equal and n-tuples (i1, . . . , in) with distinct indices, i.e., i1 6= i2 . . . 6= in. The former subset is exactly NA(S) while the size of the latter subset is at least |S| due to the existence of a perfect n-gram matching in A. The bound (36) is used in the second inequality. Since dmax ( A⊙n ) = dmax(A)\nn, the proof of part 2 is also completed.\n✷\nRemark 13. The second result of above lemma is similar to the necessity argument of (Hall’s) Theorem 6 for the existence of perfect matching in a bipartite graph, but generalized to the case of perfect n-gram matching and with additional krank condition.\nA.4 (Auxiliary) lemma\nProof of Lemma 3: We show that ifG(Y,X;A) has a perfect n-gram matching, thenG(Y,X(n);A⊙n) has a perfect matching. The reverse can be also immediately shown by reversing the discussion and exploiting the additional condition stated in the lemma. Let E⊙n denote the edge set of the bipartite graph G(Y,X(n);A⊙n). Assume G(Y,X;A) has a perfect n-gram matching M ⊆ E. For any j ∈ Y , let NM (j) denote the set of neighbors of vertex j according to edge set M . Since M is a perfect n-gram matching, |NM (j)| = n for all j ∈ Y . It can be immediately concluded from Definition 3 that sets NM (j) are all distinct, i.e., NM (j1) 6= NM (j2) for any j1, j2 ∈ Y, j1 6= j2. For any j ∈ Y , let N ′ M (j) denote an arbitrary ordered n-tuple generated from the elements of set NM (j). From the definition of n-gram matrix, we have A ⊙n(N ′M (j), j) 6= 0 for all j ∈ Y . Hence, (j,N ′M (j)) ∈ E ⊙n for all j ∈ Y which together with the fact that all N ′M (j)’s tuples are distinct, it results that M⊙n := {(j,N ′M (j))|j ∈ Y } ⊆ E ⊙n is a perfect matching for G(Y,X(n);A⊙n). ✷\nLemma 6. Consider matrix C ∈ Rm×r which is generic. Let C̃ ∈ Rm×r be such that Supp(C̃) ⊆ Supp(C) and the non-zero entries of C̃ are the same as the corresponding non-zero entries of C. If C̃ is full column rank, then C is also full column rank, almost surely.\nProof: Since C̃ is full column rank, there exists a r × r submatrix of C̃, denoted by C̃S, with non-zero determinant, i.e., det(C̃S) 6= 0. Let CS denote the corresponding submatrix of C indexed by the same rows and columns as C̃S . The determinant of CS is a polynomial in the entries of CS . Since C̃S can be derived from CS by keeping the corresponding non-zero entries, det(CS) can be decomposed into two terms as\ndet(CS) = det(C̃S) + f(CS),\nwhere the first term corresponds to the monomials for which all the variables (entries of CS) are also in C̃S and the second term corresponds to the monomials for which at least one variable is not in C̃S . The first term is non-zero as stated earlier. Since C is generic, the polynomial f(CS) is non-trivial and therefore its roots have Lebesgue measure zero. It implies that det(CS) 6= 0 with Lebesgue measure one (almost surely), and hence, it is full (column) rank. Thus, C is also full column rank, almost surely. ✷\nFinally, Theorem 1 is proved by combining the results of Theorem 5 and Lemma 5.\nProof of Theorem 1: Since conditions 2 and 3 hold and A is generic, Lemma 5 can be applied which results that rank condition 6 is satisfied almost surely and expansion condition 7 also holds. Therefore, all the required conditions for Theorem 5 are satisfied almost surely and this completes the proof. ✷"
    }, {
      "heading" : "B Proof of Random Identifiability Result (Theorem 2)",
      "text" : "We provide detailed proof of the steps stated in the proof sketch of random result in Section 5.2.\nB.1 Proof of existence of perfect n-gram matching and Kruskal results\nProof of Theorem 3: Vertex sets X and Y are partitioned, described as follows (see Figure 6). Define J := c pn . Partition set X uniformly at random into n sets of (almost) equal size\n16, denoted by X ′l , l ∈ [n]. Define sets Xl := ∪ l i=1X ′ i, l ∈ [n]. Furthermore, partition set Y uniformly at random,\nhierarchically as follows. First, partition into J sets, each with size at most ( c pn )n−1 , and denote them by Yi, i ∈ [J ]. Next, partition each of these new smaller sets Yi further into J sets, each with size at most ( c pn )n−2 . Do it iteratively up to n − 1 steps, where at the end, set Y is partitioned into sets with size at most c pn . The first two steps are shown in Figure 6.\nProof by induction: The existence of perfect n-gram matching from set Y to set X is proved by an induction argument. Consider one of intermediate sets in the hierarchical partitioning of\n16By almost, we mean the maximum difference in the size of partitions is 1 which is always possible.\nY with size O(pl) and its further partitioning into J := c pn sets, each with size O(p l−1), for any l ∈ {2, . . . , n}. In the induction step, it is shown that if there exists a perfect (l−1)-gram matching from each of these subsets of Y with size O(pl−1) to Xl−1, then there exists a perfect l-gram matching from the original set with size O(pl) to set Xl. Specifically, in the last induction step, it is shown that if there exists a perfect (n− 1)-gram matching from each set Yl, l ∈ [J ], to set Xn−1, then there exists a perfect n-gram matching from Y to Xn = X.\nBase case: The base case of induction argument holds as follows. By applying Lemma 8 and Lemma 7, there exists a perfect matching from each partition in Y with size at most c pn = O(p) to set X1, whp.\nInduction step: Consider J different bipartite graphs Gi(Yi,Xn−1;Ei), i ∈ [J ], by considering sets Yi andXn−1 and the corresponding subset of edges Ei ⊂ E incident to them. See Figure 7a. The induction step is to show that if each of the corresponding J bipartite graphs Gi(Yi,Xn−1;Ei), i ∈ [J ], has a perfect (n− 1)-gram matching, then whp, the original bipartite graph G(Y,X;E) has a perfect n-gram matching.\nLet us denote the corresponding perfect (n − 1)-gram matching of Gi(Yi,Xn−1;Ei) by Mi. Furthermore, the set of all subsets of Xn−1 with cardinality n − 1 are denoted by Pn−1(Xn−1), i.e., Pn−1(Xn−1) includes the sets with (n − 1) elements in the power set\n17 of Xn−1. For each set S ∈ Pn−1(Xn−1), take the set of all nodes in Y which are connected to all members of S according to the union of matchings ∪Ji=1Mi. Call this set as the parents of S, denoted by Pa(S). According to the definition of perfect (n − 1)-gram matching, there is at most one node in each set Yi which is connected to all members of S through the matching Mi and therefore, |Pa(S)| ≤ J = c p n . In\n17The power set of any set S is the set of all subsets of S.\naddition, note that sets Pa(S) impose a partitioning on set Y , i.e., each node j ∈ Y is exactly included in one set Pa(S) for some S ∈ Pn−1(Xn−1). This is because of the perfect (n − 1)-gram matchings considered for sets Yi, i ∈ [J ]. Now, a perfect n-gram matching for the original bipartite graph is constructed as follows. For any S ∈ Pn−1(Xn−1), consider the set of parents Pa(S). Create the bipartite graph GS(Pa(S),X ′ n;ES), where ES ⊂ E is the subset of edges incident to partitions Pa(S) ⊂ Y and X ′ n ⊂ X. Denote by dS the minimum degree of nodes in set Pa(S) in the bipartite graph GS(Pa(S),X ′ n;ES). Applying Lemma 8, we have\nPr[dS ≥ 1 + β log(p/n)] ≥ 1− J exp\n( − 2\nn2 (dmin − βn log(p/n))\n2\ndmin\n) (37)\n≥ 1− c\nn p−β log 1/c = 1−O(p−β log 1/c),\nwhere β log 1/c > n− 1, and the last inequality is concluded from the degree bound dmin ≥ α log p. Furthermore, we have |Pa(S)| ≤ c pn = c|X ′ n|. Now, we can apply Lemma 7 concluding that there exists a perfect matching from Pa(S) to X ′n within the bipartite graph GS(Pa(S),X ′ n;ES), with probability at least 1−O(p−β log 1/c). Refer to Figure 7b for a schematic picture. The edges of this perfect matching are combined with the corresponding edges of the existing perfect (n − 1)-gram matchings Mi, i ∈ [J ], to provide n incident edges to each node i ∈ Pa(S). It is easy to see that this provides a perfect n-gram matching from Pa(S) to X. We perform the same steps for all sets S ∈ Pn−1(Xn−1) to obtain a perfect n-gram matching from any Pa(S), S ∈ Pn−1(Xn−1), to X. Finally, according to this construction, the union of all of these matchings is a perfect n-gram matching from ∪S∈Pn−1(Xn−1) Pa(S) = Y to X. This finishes the proof of induction step. Note that here we analyzed the last induction step where the existence of perfect n-gram matching is concluded from the existence of corresponding perfect (n− 1)-gram matchings. The earlier induction steps, where the existence of perfect l-gram matching is concluded from the existence of corresponding perfect (l − 1)-gram matchings for any l ∈ {2, . . . , n}, can be\nsimilarly proven.\nProbability rate: We now provide the probability rate of the above events. Let N (hp) l , l ∈ [n], denote the total number of times that perfect matching result of Lemma 7 is used in step l in order to ensure that there exists a perfect l-gram matching from corresponding partitions of Y to set Xl, whp. Let N (hp) = ∑\nl∈[n]N (hp) l . As earlier, let Pl−1 ( Xl−1 ) denote the set of all subsets of Xl−1\nwith cardinality l − 1. We have\n∣∣Pl−1 ( Xl−1 )∣∣ = (∣∣Xl−1 ∣∣ l − 1 ) = ( l−1 n p l − 1 ) , l ∈ {2, . . . , n}.\nAccording to the construction method of l-gram matching from (l − 1)-gram matchings, proposed in the induction step, ∣∣Pl−1 ( Xl−1 )∣∣ is the number of times Lemma 7 is used in order to ensure that there exists a perfect l-gram matching for each partition on the Y side. Since at most Jn−l number of such l-gram matchings are proposed in step l, the number N (hp) l can be bounded as\nN (hp) l ≤ J n−l ∣∣Pl−1 ( Xl−1 )∣∣ = Jn−l ( l−1 n p\nl − 1\n) , l ∈ {2, . . . , n}. (38)\nSince in the first step, N (hp) 1 = J n−1 number of perfect matchings needs to exist in the above discussion, we have\nN (hp) = Jn−1 + n∑\nl=2\nN (hp) l\n≤ Jn−1 + n∑\nl=2\nJn−l ( l−1 n p\nl − 1\n)\n≤ ( c p\nn\n)n−1 + n∑\nl=2\n( c p\nn\n)n−l( e p\nn\n)l−1\n≤ n ( e p\nn\n)n−1 = O(pn−1),\nwhere inequality (38) is used in the first inequality and J := c pn and inequality (n k ) ≤ ( enk )k are exploited in the second inequality. Since the result of Lemma 7 holds with probability at least 1−O(p−β log 1/c) and it is assumed that β log 1/c > n−1, by applying union bound, we have the existence of perfect n-gram matching with probability at least 1−O(p−β\n′ ), for β′ = β log 1c − (n− 1) > 0.\nFurthermore, note that the degree concentration bound in (37) is also used O(pn−1) times. Since the bound in (37) holds with probability at least 1−O(p−β log 1/c) and it is assumed that β log 1/c > n−1, this also reduces to the same probability rate. The coefficient of the above polynomial probability rate is also explicitly computed, saying that the perfect n-gram matching exists with probability at least 1− γ1p −β′ , with\nγ1 = e n−1 ( c nn−1 + e2 1− δ1 nβ ′+1 ) ,\nwhere δ1 is a constant satisfying e 2 ( p n )−β log 1/c < δ1 < 1. ✷\nProof of Theorem 4: Let G(Y,X;A) denote the corresponding bipartite graph to matrix A where node sets Y = [q] and X = [p] index the columns and rows of A respectively. Therefore, |Y | = q and |X| = p. Fix some S ⊆ Y such that |S| ≤ p. Then\nPr(|N(S)| ≤ |S|) ≤ ∑\nT⊆X: |T |=|S|\nPr(N(S) ⊆ T )\n= ∑\nT⊆X: |T |=|S|\n∏\ni∈S\n( |S|\ndi )/( p di )\n≤ ∑\nT⊆X: |T |=|S|\n∏\ni∈S\n( |S|\np\n)di\n≤ ∑\nT⊆X: |T |=|S|\n∏\ni∈S\n( |S|\np\n)dmin\n=\n( p\n|S|\n)( |S|\np\n)dmin|S| , (39)\nwhere the bound (|S| di )/( p di ) ≤ ( |S| p )di is used in the second inequality, and the last inequality is concluded from the fact that |S|p ≤ 1. Let E denote the event that for any subset S ⊆ Y with |S| ≤ r, we have |N(S)| ≥ |S|, i.e.,\nE := “∀S ⊆ Y ∧ 1 ≤ |S| ≤ r : |N(S)| ≥ |S|”.\nThen, by the union bound and inequality (39), we have\nPr(Ec) = Pr(∃S ⊆ Y s. t. 1 ≤ |S| ≤ r ∧ |N(S)| < |S|) ≤ r∑\ns=1\n( q\ns\n)( p\ns\n)( s\np\n)dmins\n≤ r∑\ns=1\n( e q\ns\n)s( e p\ns )s(s p )dmins\n≤ r∑\ns=1\n( e2qrdmin−2\npdmin−1\n)s ,\nwhere the bound ( n k ) ≤ ( enk )k is used in the second inequality. For r = cp , the above inequality reduces to\nPr(Ec) ≤ r∑\ns=1\n( e2cdmin−2 q\np\n)s\n≤ r∑\ns=1\n( e2c′cdmin−1pn−1 )s\n≤ r∑\ns=1\n( e2c′cβ log ppn−1 )s\n=\nr∑\ns=1\n( e2c′pn−1−β log 1/c )s\n≤ e2c′\npβ′ − e2c′ = O(p−β\n′ ), for β′ = β log 1\nc − (n− 1) > 0,\nwhere the size condition assumed in the theorem is used in the second inequality with c′ := 1c ( c n )n , and the degree condition is exploited in the third inequality. The last inequality is concluded from the geometric series sum formula for large enough p. Then, Lemma 9 can be applied concluding that krank(A) ≥ r = cp, with probability at least 1− γ2p −β′ for constants β′ = β log 1c − (n− 1) > 0 and γ2 > 0 as\nγ2 = cn−1e2\nnn(1− δ2) ,\nwhere δ2 is a constant satisfying c ′e2p−β ′ < δ2 < 1. ✷\nProof of Remark 10: Consider a random bipartite graph G(Y,X;E) where for each node i ∈ X:\n1. Neighbors N(i) ⊆ X are picked uniformly at random among all size d subsets of X.\n2. Matching M(i) ⊆ N(i) is picked uniformly at random among all size n subsets of N(i).\nNote that as long as n ≤ d, the distribution of M(i) is uniform over all size n subsets of X. Fix some pair i, i′ ∈ Y . Then\nPr(M(i) = M(i′)) =\n( |X|\nn\n)−1 .\nBy the union bound,\nPr ( ∃i, i′ ∈ Y, i 6= i′ s. t.M(i) = M(i′) ) ≤\n( |Y |\n2\n)( |X|\nn\n)−1 ,\nwhich is Θ(|Y |2/|X|n) when n is constant. Therefore, if d ≥ n and the size constraint |Y | = O(|X|s) for some s < n2 is satisfied, then whp, there is no pair of nodes in set Y with the same random n-gram matching. This concludes that the random bipartite graph has a perfect n-gram matching whp, under these size and degree conditions.\n✷\nB.2 (Auxiliary) lemmata\nLemma 7 (Existence of perfect matching for random bipartite graphs). Consider a random bipartite graph G(W,Z;E) with |W | = w nodes on the left side and |Z| = z on the right side, and each node i ∈ W is randomly connected to di different nodes in set Z. Let dw := mini∈W di. Assume\nthat it satisfies the size condition w ≤ cz for some constant 0 < c < 1 and the degree condition dw ≥ 1 + β log z for some constant β > 0. Then, there exists a perfect matching in the random bipartite graph G(W,Z;E) with probability at least 1−O(z−β log 1/c) where β log 1c > 0.\nProof: From Hall’s theorem (Theorem 6), the existence of perfect matching for a bipartite graph is equivalent to occurrence of the following event\nẼ := “∀S ⊆ W : |N(S)| ≥ |S|”.\nSimilar to the analysis in the proof of Theorem 4, it is concluded from union bound\nPr ( Ẽc ) = Pr(∃S ⊆ W s. t. |N(S)| < |S|) ≤ w∑\ns=1\n( w\ns\n)( z\ns\n)( s\nz\n)dws\n≤ w∑\ns=1\n( e w\ns\n)s( e z\ns )s(s z )dws\n≤ w∑\ns=1\n( e2wdw−1\nzdw−1\n)s\n≤ w∑\ns=1\n( e2cdw−1 )s ,\nwhere the bound (n k ) ≤ ( enk )k is used in the second inequality. From the assumed lower bound on the degree dw and the fact that 0 < c < 1, we have\nPr ( Ẽc ) ≤ w∑\ns=1\n( e2cβ log z )s = w∑\ns=1\n( e2zβ log c )s ≤\ne2\nzβ log 1 c − e2\n≤ e2\n1− δ1 z−β log 1/c,\nwhere the second inequality is concluded from the geometric series sum formula for large enough z, and δ1 is a constant satisfying e\n2z−β log 1/c < δ1 < 1. ✷ Lemma 8 (Degree concentration bound). Consider a random bipartite graph G(Y,X;E) with |Y | = q and |X| = p, where each node i ∈ Y is randomly connected to di different nodes in set X. Let Y ′ ⊂ Y be any subset 18 of nodes in Y with size |Y ′| = q′ and X ′ ⊂ X be a random (uniformly chosen) subset of nodes in X with size |X ′| = p′. Create the new bipartite graph G(Y ′,X ′;E′) where edge set E′ ⊂ E is the subset of edges in E incident to Y ′ and X ′. Denote the degree of each node i ∈ Y ′ within this new bipartite graph by d′i. Let dmin := mini∈Y di and d ′ min := mini∈Y ′ d ′ i. Then, if dmin > r p p′ for a non-negative integer r, we have\nPr[d′min ≥ r + 1] ≥ 1− q ′ exp\n( −2(p′/p)2 (dmin − (p/p ′)r)2\ndmin\n) .\nProof: For any i ∈ Y ′, we have\nPr[d′i ≤ r] = r∑\nj=0\n( p′\nj\n)( p− p′\ndi − j )/( p di ) ,\n18Note that Y ′ need not to be uniformly chosen and the result is valid for any subset of nodes Y ′ ⊂ Y .\nwhere the inner term of summation is a hypergeometric distribution with parameters p (population size), p′ (number of success states in the population), di (number of draws) and j is the hypergeometric random variable denoting number of successes. The following tail bound for the hypergeometric distribution is provided [45,46]\nPr[d′i ≤ r] ≤ exp(−2t 2 i di),\nfor ti > 0 given by r = (p′ p − ti ) di. Note that assumption dmin > p p′ r in the lemma is equivalent to having ti > 0, i ∈ Y . Considering the minimum degree, for any i ∈ Y ′, we have\nPr[d′i ≤ r] ≤ exp(−2t 2dmin),\nfor t > 0 given by r = (p′ p −t ) dmin. Substituting t from this equation gives the following bound\nPr[d′i ≤ r] ≤ exp\n( −2(p′/p)2 (dmin − (p/p ′)r)2\ndmin\n) . (40)\nFinally, applying the union bound, we can prove the result as follows\nPr[d′min ≥ r + 1] =Pr[∩ q′ i=1{d ′ i ≥ r + 1}]\n≥1−\nq′∑\ni=1\nPr[d′i ≤ r]\n≥1−\nq′∑\ni=1\nexp ( −2(p′/p)2 (dmin − (p/p ′)r)2\ndmin\n)\n=1− q′ exp ( −2(p′/p)2 (dmin − (p/p ′)r)2\ndmin\n) ,\nwhere the union bound is applied in the first inequality and the second inequality is concluded from (40). ✷\nA lower bound on the Kruskal rank of matrix A based on a sufficient relaxed expansion property on A is provided in the following lemma. Lemma 9. If A is generic and the bipartite graph G(Y,X;A) satisfies the relaxed 19 expansion property |N(S)| ≥ |S| for any subset S ⊆ Y with |S| ≤ r, then krank(A) ≥ r, almost surely.\nBefore proposing the proof, we state the marriage or Hall’s theorem which gives an equivalent condition for having a perfect matching in a bipartite graph. Theorem 6 (Hall’s theorem, [47]). A bipartite graph G(Y,X;E) has Y -saturating matching if and only if for every subset S ⊆ Y , the size of the neighbors of S is at least as large as S, i.e., |N(S)| ≥ |S|.\nProof of Lemma 9: Denote the submatrix AN(S),S by ÃS , i.e., ÃS := AN(S),S . Exploiting marriage or Hall’s theorem, it is concluded that the bipartite graph G(S,N(S); ÃS ) has a perfect matching MS for any subset S ⊆ Y such that |S| ≤ r. Denote by ÃMS the corresponding matrix to this perfect matching edge set MS , i.e., ÃMS keeps the non-zero entries of ÃS on edge set MS and\n19There is no dmax term in contrast to the expansion property proposed in condition 7.\neverywhere else, it is zero. Note that the support of ÃMS is within the support of ÃS . According to the definition of perfect matching, the matrix ÃMS is full column rank. From Lemma 6, it is concluded that ÃS is also full column rank almost surely. This is true for any ÃS with S ⊆ Y and |S| ≤ r, which directly results that krank(A) ≥ r, almost surely. ✷\nFinally, Theorem 2 is proved by exploiting the random results on the existence of perfect n-gram matching and Kruskal rank, provided in Theorems 3 and 4.\nProof of Theorem 2: We claim that if random conditions 4 and 5 are satisfied, then deterministic conditions 2 and 3 hold whp. Then Theorem 1 can be applied and the proof is done. From size and degree conditions, Theorem 3 can be applied, which implies that the perfect n-gram matching condition 2 is satisfied with probability at least 1− γ1p\n−β′ for β′ = β log 1c − (n− 1) > 0. The conditions required for Theorem 4 also hold and by applying this theorem we have the bound krank(A) ≥ cp, with probability at least1−γ2p\n−β′ . Combining this inequality with the upper bound on degree d in condition 5, we conclude that krank condition 3 is also satisfied whp. Hence, all the conditions required for Theorem 1 are satisfied with probability at least 1− γp−β ′ , where\nγ = γ1 + γ2 = e n−1 ( c nn−1 + e2 1− δ1 nβ ′+1 ) + cn−1e2 nn(1− δ2) ,\nand this completes the proof. ✷"
    }, {
      "heading" : "C Relationship to CP Decomposition Uniqueness Results",
      "text" : "In this section, we provide a more detailed comparison with some uniqueness results of overcomplete CP decomposition. Here, the following CP decomposition for the third order tensor T ∈ Rp×s×q is considered,\nT =\nr∑\ni=1\nai ◦ bi ◦ ci, (41)\nwhere A = [a1| . . . |ar] ∈ R p×r, B = [b1| . . . |br] ∈ R s×r and C = [c1| . . . |cr] ∈ R q×r. The most important and general uniqueness result of CP, called Kruskal’s condition, is provided in [15], where it is guaranteed that the above CP decomposition is unique if\nkrank(A) + krank(B) + krank(C) ≥ 2r + 2.\nSince then, several works have analyzed the uniqueness of CP decomposition. One set of works assume that one of the components, say C, is full column rank [17, 18]. it is shown in [18], for generic (fully dense) components A,B and C, if r ≤ q and r(r − 1) ≤ p(p− 1)s(s − 1)/2, then the CP decomposition in (41) is generically unique. Now, we demonstrate how this CP uniqueness result can be adapted to our setting. First, consider the matrix M ∈ Rps×q which is obtained by stacking the entries of T as\nM(i−1)s+j,k = Tijk.\nThen, we have\nM = (A⊙B)C⊤. (42)\nOn the other hand, for the 2-persistent topic model with 4 words (n = 2,m = 2), the moment can be written as\nM (2) 4 (x) = (A⊙A)E\n[ hh⊤ ] (A⊙A)⊤,\nfor A ∈ Rp×q. The following matrix has the same column span of M (2) 4 (x),\nM ′ = (A⊙A)C ′⊤,\nfor some full rank matrix C ′ ∈ Rq×q. Our random identifiability result in Theorem 2 provides the uniqueness of A and C ′, given M ′, under the size condition q ≤ ( cp2 )2 and the additional degree condition 5. Note that as discussed in the previous section, this identifiability argument is the same as the unique decomposition of the corresponding tensor. Thus, in equation (42), by setting A = B and a full rank square matrix C, we obtain the 2-persistent topic model, under consideration in this paper. Thus, the identifiability results of [18] are applicable to our setting, if we assume generic (i.e. fully dense) matrix A. However, we incorporate a sparse matrix A, and therefore, require different techniques to provide identifiability results. We note that the size bound specified in [18] is comparable to the size bound derived in this paper (for random structured matrices), but we have additional degree considerations for identifiability. Analyzing the regime where the uniqueness conditions of [18] are satisfied under sparsity constraints is an interesting question for future investigation."
    } ],
    "references" : [ {
      "title" : "Unsupervised feature learning and deep learning: A review and new perspectives",
      "author" : [ "Yoshua Bengio", "Aaron Courville", "Pascal Vincent" ],
      "venue" : "arXiv preprint arXiv:1206.5538,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Learning overcomplete representations",
      "author" : [ "Michael S. Lewicki", "Terrence J. Sejnowski", "Howard Hughes" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1998
    }, {
      "title" : "Local convergence of the alternating least squares algorithm for canonical tensor approximation",
      "author" : [ "André Uschmajew" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Latent Dirichlet Allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Inference of population structure using multilocus genotype",
      "author" : [ "J.K. Pritchard", "M. Stephens", "P. Donnelly" ],
      "venue" : "data. Genetics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "Tensor Methods for Learning Latent Variable Models",
      "author" : [ "A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky" ],
      "venue" : "Under Review. J. of Machine Learning. Available at arXiv:1210.7559,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Learning Linear Bayesian Networks with Latent Variables",
      "author" : [ "A. Anandkumar", "D. Hsu", "A. Javanmard", "S.M. Kakade" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "A practical algorithm for topic modeling with provable guarantees",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Yoni Halpern", "David M. Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu" ],
      "venue" : "ArXiv 1212.4777,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "More factors than subjects, tests and treatments: an indeterminacy theorem for canonical decomposition and individual differences",
      "author" : [ "J.B. Kruskal" ],
      "venue" : "scaling. Psychometrika,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1976
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "Tamara Kolda", "Brett Bader" ],
      "venue" : "SIREV,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Tensor completion and low-n-rank tensor recovery via convex optimization",
      "author" : [ "Silvia Gandy", "Benjamin Recht", "Isao Yamada" ],
      "venue" : "Inverse Problems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "Adam Coates", "Honglak Lee", "Andrew Y. Ng" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning",
      "author" : [ "Quoc V. Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Y. Ng" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Deep Learning for Signal and Information Processing",
      "author" : [ "Li Deng", "Dong Yu" ],
      "venue" : "NOW Publishers,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics",
      "author" : [ "J.B. Kruskal" ],
      "venue" : "Linear algebra and its applications,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1977
    }, {
      "title" : "Uniqueness of Tensor Decompositions with Applications to Polynomial Identifiability",
      "author" : [ "A. Bhaskara", "M. Charikar", "A. Vijayaraghavan" ],
      "venue" : "ArXiv 1304.8087,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Kruskal’s permutation lemma and the identification of candecomp/parafac and bilinear models with constant modulus constraints",
      "author" : [ "Tao Jiang", "Nicholas D Sidiropoulos" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "A Link between the Canonical Decomposition in Multilinear Algebra and Simultaneous Matrix Diagonalization",
      "author" : [ "Lieven De Lathauwer" ],
      "venue" : "SIAM J. Matrix Analysis and Applications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Sufficient conditions for uniqueness in candecomp/parafac and indscal with random component",
      "author" : [ "Alwin Stegeman", "Jos M.F. Ten Berge", "Lieven De Lathauwer" ],
      "venue" : "matrices. Psychometrika,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Fourth-order cumulant-based blind identification of underdetermined mixtures",
      "author" : [ "L. De Lathauwer", "J. Castaing", "J.-F Cardoso" ],
      "venue" : "IEEE Tran. on Signal Processing,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "On generic identifiability of 3-tensors of small rank",
      "author" : [ "Luca Chiantini", "Giorgio Ottaviani" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Refined methods for the identifiability of tensors",
      "author" : [ "Cristiano Bocci", "Luca Chiantini", "Giorgio Ottaviani" ],
      "venue" : "arXiv preprint arXiv:1303.6915,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "One example of general unidentifiable tensors",
      "author" : [ "Luca Chiantini", "Massimiliano Mella", "Giorgio Ottaviani" ],
      "venue" : "arXiv preprint arXiv:1303.6914,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Identifiability of parameters in latent structure models with many observed variables",
      "author" : [ "E.S. Allman", "C. Matias", "J.A. Rhodes" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "A semialgebraic description of the general markov model on phylogenetic trees",
      "author" : [ "Elizabeth S. Allman", "John A. Rhodes", "Amelia Taylor" ],
      "venue" : "Arxiv preprint arXiv:1212.1200,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Tensors: Geometry and applications, volume 128",
      "author" : [ "Joseph M Landsberg" ],
      "venue" : "American Mathematical Soc.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "A Method of Moments for Mixture Models and Hidden Markov Models",
      "author" : [ "A. Anandkumar", "D. Hsu", "S.M. Kakade" ],
      "venue" : "In Proc. of Conf. on Learning Theory,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "A Spectral Algorithm for Latent Dirichlet Allocation",
      "author" : [ "A. Anandkumar", "D.P. Foster", "D. Hsu", "S.M. Kakade", "Y.K. Liu" ],
      "venue" : "In Proc. of Neural Information Processing (NIPS),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "A Tensor Spectral Approach to Learning Mixed Membership Community Models",
      "author" : [ "A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    }, {
      "title" : "Learning nonsingular phylogenies and hidden markov models",
      "author" : [ "E. Mossel", "S. Roch" ],
      "venue" : "The Annals of Applied Probability,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2006
    }, {
      "title" : "Full reconstruction of markov models on evolutionary trees: identifiability and consistency",
      "author" : [ "J.T. Chang" ],
      "venue" : "Mathematical Biosciences,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1996
    }, {
      "title" : "Learning mixtures of arbitrary distributions over large discrete domains",
      "author" : [ "Yuval Rabani", "Leonard Schulman", "Chaitanya Swamy" ],
      "venue" : "arXiv preprint arXiv:1212.1527,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    }, {
      "title" : "Learning topic models—going beyond svd",
      "author" : [ "Saneev Arora", "Rong Ge", "Ankur Moitra" ],
      "venue" : "In Symposium on Theory of Computing,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2012
    }, {
      "title" : "Exact recovery of sparsely-used dictionaries",
      "author" : [ "Daniel A Spielman", "HuanWang", "JohnWright" ],
      "venue" : "In Proc. of Conf. on Learning Theory,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2012
    }, {
      "title" : "Dictionary learning algorithms for sparse representation",
      "author" : [ "Kenneth Kreutz-Delgado", "Joseph F. Murray", "Bhaskar D. Rao", "Kjersti Engan", "Te-Won Lee", "Terrence J. Sejnowski" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2003
    }, {
      "title" : "An affine scaling methodology for best basis selection",
      "author" : [ "B. Rao", "K. Kreutz-Delgado" ],
      "venue" : "IEEE Tran. Signal Processing,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1999
    }, {
      "title" : "Sparsity-based generalization bounds for predictive sparse coding",
      "author" : [ "Nishant A. Mehta", "Alexander G. Gray" ],
      "venue" : "In Proc. of the Intl. Conf. on Machine Learning (ICML), Atlanta,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2013
    }, {
      "title" : "Sparse coding for multitask and transfer learning",
      "author" : [ "Andreas Maurer", "Massimiliano Pontil", "Bernardino Romera-Paredes" ],
      "venue" : "ArxXiv preprint,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2012
    }, {
      "title" : "Ramsey theory reveals the conditions when sparse coding on subsampled data is unique",
      "author" : [ "Christopher J Hillar", "Friedrich T Sommer" ],
      "venue" : "arXiv preprint arXiv:1106.3616,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2011
    }, {
      "title" : "Matrix Computations",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : null,
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2012
    }, {
      "title" : "Posterior contraction of the population polytope in finite admixture models",
      "author" : [ "XuanLong Nguyen" ],
      "venue" : "arXiv preprint arXiv:1206.0068,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2012
    }, {
      "title" : "Exact recovery of sparsely-used dictionaries",
      "author" : [ "Daniel A. Spielman", "Huan Wang", "John Wright" ],
      "venue" : "ArxXiv preprint,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2012
    }, {
      "title" : "On the uniqueness of multilinear decomposition of N-way arrays",
      "author" : [ "Nicholas D. Sidiropoulos", "Rasmus Bro" ],
      "venue" : "Journal of Chemometrics,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2000
    }, {
      "title" : "The tail of the hypergeometric distribution",
      "author" : [ "V. Chvátal" ],
      "venue" : "Discrete Mathematics,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1979
    }, {
      "title" : "On representatives of subsets",
      "author" : [ "Philip Hall" ],
      "venue" : "J. London Math. Soc.,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1935
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Overcomplete representations, where the number of features can be greater than the dimensionality of the input data, have been extensively employed, and are arguably critical in a number of applications such as speech and computer vision [1].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 1,
      "context" : "representations are known to be more robust to noise, and can provide greater flexibility in modeling [2].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "see [3].",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "In this paper, we characterize identifiability for a popular class of latent variable models, known as the admixture or topic models [4, 5].",
      "startOffset" : 133,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "In this paper, we characterize identifiability for a popular class of latent variable models, known as the admixture or topic models [4, 5].",
      "startOffset" : 133,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "[6–8].",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 6,
      "context" : "[6–8].",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 7,
      "context" : "[6–8].",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 8,
      "context" : "In addition, we present trade-offs among the following quantities: number of topics, size of the word vocabulary, the topic persistence level, the order of the observed moments at hand, the minimum and maximum degrees of any topic in the topic-word bipartite graph, and the Kruskal rank [9] of the topic-word matrix, under which identifiability holds.",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 9,
      "context" : "Our identifiability results for persistent topic models imply uniqueness of a structured class of tensor decompositions, which is contained in the class of Tucker decompositions, but is more general than the candecomp/parafac (CP) decomposition [10].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 6,
      "context" : "Recap of Identifiability Conditions in Under-complete Setting (Expansion Conditions on Topic-Word Matrix): Our approach is based on the recent results of [7], where conditions for identifiability of topic models are derived, given pairwise observed moments (specifically, cooccurrence of word-pairs in documents).",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Expansion conditions are imposed in [7] on the topic-word bipartite graph which imply that (generically) the sparsest vectors in the column span of A, denoted by Col(A), are the columns of A themselves.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "Therefore, the techniques derived in [7] are not directly applicable here since we consider overcomplete models.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "We can view the higher order moments as pairwise moments of another equivalent topic model, which enables us to apply the techniques of [7].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Imposing the expansion conditions derived in [7] on A⊙n implies that (generically) the sparsest vectors in Col(A⊙n), are the columns of A⊙n themselves.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "We refer to this as the “first-order” approach since we directly impose the expansion conditions of [7] on A⊙n, without exploiting the additional structure present in A⊙n.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "Moreover, we establish that A⊙n fails to expand on “small” sets, as required in [7], when the degrees are sufficiently different 2.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "This agrees with the intuition that when the topic-word matrix A has a larger Kruskal rank, it should be easier to identify A, since the Kruskal rank is related to the mutual incoherence 5 among the columns of A, see [11].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 0,
      "context" : "[1,12–14], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "[1,12–14], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 12,
      "context" : "[1,12–14], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : "[1,12–14], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "The classical result by Kruskal provides conditions for uniqueness of a CP decomposition [9, 15], with recent extensions to the notion of robust identifiability [16].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "The classical result by Kruskal provides conditions for uniqueness of a CP decomposition [9, 15], with recent extensions to the notion of robust identifiability [16].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "The classical result by Kruskal provides conditions for uniqueness of a CP decomposition [9, 15], with recent extensions to the notion of robust identifiability [16].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 16,
      "context" : "[17–23].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "[17–23].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "[17–23].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "[17–23].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "[17–23].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "[17–23].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "[17–23].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 23,
      "context" : "[24, 25], and the single-topic model, or more generally latent Dirichlet allocation (LDA).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "[24, 25], and the single-topic model, or more generally latent Dirichlet allocation (LDA).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "For a general overview of tensor decompositions, see [10,27].",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "For a general overview of tensor decompositions, see [10,27].",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "[6,28,29] provide an efficient moment-based approach for learning topic models, under constraints on the distribution of the topic proportions, e.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 26,
      "context" : "[6,28,29] provide an efficient moment-based approach for learning topic models, under constraints on the distribution of the topic proportions, e.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "[6,28,29] provide an efficient moment-based approach for learning topic models, under constraints on the distribution of the topic proportions, e.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 28,
      "context" : "In addition, the approach can handle a variety of latent variable models such as Gaussian mixtures, hidden Markov models (HMM) and community models [30].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "In [6], a tensor power method approach is analyzed and is shown to be an efficient guaranteed recovery method in the nondegenerate (i.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 26,
      "context" : "[28, 31, 32].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 29,
      "context" : "[28, 31, 32].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 30,
      "context" : "[28, 31, 32].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 19,
      "context" : "[20], can be employed instead, albeit at a cost of higher computational complexity for overcomplete CP tensor decomposition.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "See [28] for a detailed description of these works.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 31,
      "context" : "[33] consider learning discrete mixtures given a large number of “views”, and they refer to the number of views as the sampling aperture.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[8, 34] employ approaches based on non-negative matrix factorization (NMF) to recover the topic-word matrix.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 32,
      "context" : "[8, 34] employ approaches based on non-negative matrix factorization (NMF) to recover the topic-word matrix.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "[7] which considers identifiability and learning of topic models under expansion conditions on the topic-word matrix.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 33,
      "context" : "al [35] considers the problem of dictionary learning, which is closely related to the setting of [7], but in addition assumes that the coefficient matrix is random.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "al [35] considers the problem of dictionary learning, which is closely related to the setting of [7], but in addition assumes that the coefficient matrix is random.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "However, these works [7,35] can handle only the under-complete setting, where the number of topics is less than the dimensionality of the word vocabulary (or the number of dictionary atoms is less than the number of observations in [35]).",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 33,
      "context" : "However, these works [7,35] can handle only the under-complete setting, where the number of topics is less than the dimensionality of the word vocabulary (or the number of dictionary atoms is less than the number of observations in [35]).",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 33,
      "context" : "However, these works [7,35] can handle only the under-complete setting, where the number of topics is less than the dimensionality of the word vocabulary (or the number of dictionary atoms is less than the number of observations in [35]).",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 1,
      "context" : "There have been Bayesian as well as frequentist approaches for dictionary learning [2,36,37].",
      "startOffset" : 83,
      "endOffset" : 92
    }, {
      "referenceID" : 34,
      "context" : "There have been Bayesian as well as frequentist approaches for dictionary learning [2,36,37].",
      "startOffset" : 83,
      "endOffset" : 92
    }, {
      "referenceID" : 35,
      "context" : "There have been Bayesian as well as frequentist approaches for dictionary learning [2,36,37].",
      "startOffset" : 83,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "employed in these works [2, 36, 37] have no performance guarantees.",
      "startOffset" : 24,
      "endOffset" : 35
    }, {
      "referenceID" : 34,
      "context" : "employed in these works [2, 36, 37] have no performance guarantees.",
      "startOffset" : 24,
      "endOffset" : 35
    }, {
      "referenceID" : 35,
      "context" : "employed in these works [2, 36, 37] have no performance guarantees.",
      "startOffset" : 24,
      "endOffset" : 35
    }, {
      "referenceID" : 33,
      "context" : "al [35] considers learning (undercomplete) dictionaries and provide guaranteed learning under the assumption that the coefficient matrix is random (distributed as Bernoulli-Gaussian variables).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 36,
      "context" : "Recent works [38, 39] provide generalization bounds for predictive sparse coding, where the goal of the learned representation is to obtain good performance on some predictive task.",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 37,
      "context" : "Recent works [38, 39] provide generalization bounds for predictive sparse coding, where the goal of the learned representation is to obtain good performance on some predictive task.",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 38,
      "context" : "Hillar and Sommer [40] consider the problem of identifiability of sparse coding and establish that when the dictionary succeeds in reconstructing a certain set of sparse vectors, then there exists a unique sparse coding, up to permutation and scaling.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 39,
      "context" : "For A ∈ Rp×q and B ∈ Rm×n, the Kronecker product A⊗B ∈ Rpm×qn is defined as [41]",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "In this section, the n-persistent topic model is introduced and this imposes an additional constraint, known as topic persistence on the popular admixture model [4,5,42].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "In this section, the n-persistent topic model is introduced and this imposes an additional constraint, known as topic persistence on the popular admixture model [4,5,42].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 40,
      "context" : "In this section, the n-persistent topic model is introduced and this imposes an additional constraint, known as topic persistence on the popular admixture model [4,5,42].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 40,
      "context" : "This collection of vectors ai, i ∈ [q], is referred to as the population structure or the topic-word matrix [42].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "We now describe a linear representation of the n-persistent topic model, on lines of [6], but with extensions to incorporate persistence.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "For a given sparsity pattern, the class of population structure matrices is said to be generically identifiable [25], if all the non-identifiable matrices form a set of Lebesgue measure zero.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "Definition 4 (Kruskal rank, [15]).",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "Identifiability of this model has been studied earlier [7] and we recall it below.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "Remark 4 (Bag-of-words admixture model, [7]).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Given (2r)-th order observed moments with r ≥ 1, the structure of the popular bag-of-words admixture model and the (2r)-th order moment of hidden variables are identifiable, when A is full column rank and the following expansion condition holds [7] |NA(S)| ≥ |S|+ dmax(A), ∀S ⊆ Vh, |S| ≥ 2.",
      "startOffset" : 245,
      "endOffset" : 248
    }, {
      "referenceID" : 6,
      "context" : "Efficient l1-based recovery algorithms have been analyzed in [7,43] for the undercomplete case (n = 1).",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 41,
      "context" : "Efficient l1-based recovery algorithms have been analyzed in [7,43] for the undercomplete case (n = 1).",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "Exploiting additional structure present in A⊙n, for n > 1, such as rank-1 test devices proposed in [20] are interesting avenues for future investigation.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 41,
      "context" : "The identifiability result for the random bag-of-words admixture model is comparable to the result in [43], which considers exact recovery of sparsely-used dictionaries.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 39,
      "context" : "That is, A⊙n is the column-wise n order Kronecker product of n copies of A, and is known as the Khatri-Rao product [41].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 39,
      "context" : "For vectors ai ∈ R pi , i ∈ [n], the tensor outer product operator “◦” is defined as [41]",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 39,
      "context" : "This type of rank is called CP (Candecomp/Parafac) tensor rank in the literature [41].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "Finally, the Tucker and CP (Candecomp/Parafac) representations are defined as follows [10,41].",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 39,
      "context" : "Finally, the Tucker and CP (Candecomp/Parafac) representations are defined as follows [10,41].",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "Note that since we only require expansion on sets larger than Kruskal rank, the expansion condition (24) is a more relaxed condition compared to expansion condition proposed in [7, 43] for identifiability in the undercomplete regime.",
      "startOffset" : 177,
      "endOffset" : 184
    }, {
      "referenceID" : 41,
      "context" : "Note that since we only require expansion on sets larger than Kruskal rank, the expansion condition (24) is a more relaxed condition compared to expansion condition proposed in [7, 43] for identifiability in the undercomplete regime.",
      "startOffset" : 177,
      "endOffset" : 184
    }, {
      "referenceID" : 6,
      "context" : "The expansion condition for the bag-of-words admixture model is provided in (4), introduced in [7].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "Second, the expansion property (4), proposed in [7], needs to be satisfied for all subsets S with size |S| ≥ 2, which is a stricter condition than the one proposed here in (25), since we can have krank(A) ≫ 2.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "Note that for n ≥ 3, this full rank condition can be relaxed by Kruskal’s condition for uniqueness of CP decomposition [15] and its generalization to higher order tensors [44].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 42,
      "context" : "Note that for n ≥ 3, this full rank condition can be relaxed by Kruskal’s condition for uniqueness of CP decomposition [15] and its generalization to higher order tensors [44].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 6,
      "context" : "The result proposed in this lemma is similar to the parameter genericity condition in [7], but generalized for the n-gram matrix, A⊙n.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "2 in [7].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 43,
      "context" : "The following tail bound for the hypergeometric distribution is provided [45,46]",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 44,
      "context" : "Theorem 6 (Hall’s theorem, [47]).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "The most important and general uniqueness result of CP, called Kruskal’s condition, is provided in [15], where it is guaranteed that the above CP decomposition is unique if",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "One set of works assume that one of the components, say C, is full column rank [17, 18].",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "One set of works assume that one of the components, say C, is full column rank [17, 18].",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "it is shown in [18], for generic (fully dense) components A,B and C, if r ≤ q and r(r − 1) ≤ p(p− 1)s(s − 1)/2, then the CP decomposition in (41) is generically unique.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 17,
      "context" : "Thus, the identifiability results of [18] are applicable to our setting, if we assume generic (i.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "We note that the size bound specified in [18] is comparable to the size bound derived in this paper (for random structured matrices), but we have additional degree considerations for identifiability.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "Analyzing the regime where the uniqueness conditions of [18] are satisfied under sparsity constraints is an interesting question for future investigation.",
      "startOffset" : 56,
      "endOffset" : 60
    } ],
    "year" : 2013,
    "abstractText" : "Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of “higher order” expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allows for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition.",
    "creator" : "LaTeX with hyperref package"
  }
}