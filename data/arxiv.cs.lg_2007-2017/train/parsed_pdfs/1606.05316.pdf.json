{
  "name" : "1606.05316.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "gamir@cs.tau.ac.il", "roi.livni@mail.huji.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n05 31\n6v 1\n[ cs\n.L G\n] 1\n6 Ju\nn 20\nIn this work we give an online algorithm for ILN, which avoids the kernel trick assumption. More generally and of independent interest, we show that kernel methods in general can be exploited even when the kernel cannot be efficiently computed but can only be estimated via sampling.\nWe provide a regret analysis for our algorithm, showing that it matches the sample complexity of methods which have access to kernel values. Thus, our method is the first to demonstrate that the kernel trick is not necessary as such, and random features suffice to obtain comparable performance."
    }, {
      "heading" : "1 Introduction",
      "text" : "With the increasing success of highly non-convex and complex learning architectures such as neural networks, there is an increasing effort to further understand and explain the limits of training such hierarchical structures.\nRecently there have been attempts to draw mathematical insight from kernel methods in order to better understand deep learning as well as come up with new computationally learnable architectures. One such line of work consists on learning Infinite–Layer Networks (ILN) [8, 16]. An infinite layer network can be thought of as a network with infinitely many nodes in a hidden layer. A target function in an ILN class will be of the form:\nx → ∫ ψ(x;w)f(w)dµ(w), (1)\nHere ψ is some function of the input x and parameters w, and dµ(w) is a prior over the parameter space. For example, ψ(x;w) can be a single sigmoidal neuron or a complete\nconvolutional network. The integral can be thought of as an infinite sum over all such possible networks, and f(w) can be thought of as an infinite output weight vector to be trained.\nA Standard 1–hidden layer network with a finite set of units can be obtained from the above formalism as follows. First, choose ψ(x;w) = σ(x ·w) where σ is an activation function (e.g., sigmoid or relu). Next, set dµ(w) to be a discrete measure over a finite set w1, . . . ,wd.\n1 In this case, the integral results in a network with d hidden units, and the function f is the linear weights of the output layer. Namely:\nx → 1 d\nd ∑\ni=1\nf(wi) · σ(x ·wi).\nThe main challenge when training 1–hidden layer networks is of course to find the w1, . . . ,wd on which we wish to support our distribution. It is known [20], that due to hardness of learning intersection of halfspaces [19, 12], 1–hidden layer neural networks are computationally hard for a wide class of activation functions. Therefore, as the last example illustrates, the choice of µ is indeed crucial for performance.\nFor a fixed prior µ, the class of ILN functions is highly expressive, since f can be chosen to approximate any 1-hidden layer architecture to arbitrary precision (by setting f to delta functions around the weights of the network, as we did above for µ). However, this expressiveness comes at a cost. As argued in [16], ILNs will generalize well when there is a large mass of w parameters that attain a small loss.\nThe key observation that makes certain ILN tractable to learn is that Eq. 1 is a linear functional in f . In that sense it is a linear classifier and enjoys the rich theory and algorithmic toolbox for such classifiers. In particular, one can use the fact that linear classifiers can be learned via the kernel trick. In other words, we can reduce learning ILN to the problem of computing the kernel function between two examples. Specifically the problem reduces to computing integrals of the following form:\nk(x1,x2) =\n∫\nψ(x1;w) · ψ(x2;w)dµ(w) = E w̄∼µ [ψ(x1; w̄) · ψ(x2; w̄)] . (2)\nIn this work we extend this result to the case where no closed form kernel is available, and thus the kernel trick is not directly applicable. We thus turn our attention to the setting where features (i.e., w vectors) can be randomly sampled. In this setting, our main result shows that for the square loss, we can efficiently learn the above class – and surprisingly with a comparable computational cost w.r.t standard kernel methods. Our approach can thus be applied to any random feature based method.\nThe observation we begin with is that sampling random neurons, i.e. sampling random w, leads to an estimate of the kernel in Eq. 2. Thus, if for example, we ignore complexity issues and can sample infinitely many w’s, it is not surprising that we can avoid the need for exact computation of the kernel.\n1In δ function notation dµ(w) = 1 d ∑d i=1 δ(w −wi)dw\nOur results provide a much stronger and practical result. Given T training samples, the lower bound on achievable accuracy is O(1/ √ T ) (see [26]). We show that we can in fact achieve this rate, using Õ(T 2) calls2 to the random feature generator. For comparison, note that O(T 2) is the size of the kernel matrix, and is thus likely to be the cost of any algorithm that uses an explicit kernel matrix, where one is available (whether a kernel regression algorithm can use less than O(T 2) kernel entries is an open problem [5]. As we discuss later, our approach improves on previous random features based learning [9, 24] in terms of sample/computational complexity, and expressiveness."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We consider learning algorithms that learn a mapping from input instances x ∈ X to labels y ∈ Y. We focus on the regression case where Y is the interval [−1, 1]. Our starting point is a class of feature functions ψ(w;x) : Ω×X → R, parametrized by vectors w ∈ Ω. The functions ψ(w;x) may contain highly complex non linearities, such as multi-layer networks consisting of convolution and pooling layers. Our only assumption on ψ(w;x) is that for all w ∈ Ω and x ∈ X it holds that |ψ(w;x)| < 1.\nGiven a distribution µ on Ω, we denote by L2(Ω, µ) the class of square integrable functions over Ω.\nL2(Ω, µ) =\n{\nf :\n∫ f2(w)dµ(w) < ∞ } .\nWe will use functions f ∈ L2(Ω, µ) as mixture weights over the class Ω, where each f naturally defines a new regression function from x to R as follows:\nx → ∫ ψ(w;x)f(w)dµ(w). (3)\nOur key algorithmic assumption is that the learner can efficiently sample random w according to the distribution µ. Denote the time to generate one such sample by ρ.\nIn what follows it will be simpler to express the integrals as scalar products. Define the following scalar product on functions f ∈ L2(Ω, µ).\n〈f, g〉 = ∫ f(w)g(w)dµ(w) (4)\nWe denote the corresponding ℓ2 norm by ‖f‖ = √ 〈f, f〉. Also, given features x denote by Φ(x) the function in L2(Ω, µ) given by Φ(x)[w] = ψ(w;x). Then, the regression functions we are considering are of the form x → 〈f,Φ(x)〉.\nA subclass of norm bounded elements in L2(Ω, µ) induces a natural subclass of regression functions. Namely, we consider the following class:\nHBµ = {x → 〈f,Φ(x)〉 : ‖f‖ < B} . 2We use Õ notation to suppress logarithmic factors\nOur ultimate goal is to output a predictor f ∈ L2(Ω, µ) that is competitive, in terms of prediction, with the best target function in the class HBµ .\nWe will consider an online setting, and use it to derive generalization bounds via standard online to batch conversion. In our setting, at each round a learner chooses a target function ft ∈ L2(Ω, µ) and an adversary then reveals a sample xt and label yt. The learner then incurs a loss of\nℓt(ft) = 1\n2 (〈ft,Φ(xt)〉 − yt)2 . (5)\nThe objective of the learner is to minimize her T round regret w.r.t norm bounded elements in L2(Ω, µ). Namely:\nT ∑\nt=1\nℓt(ft)− min f∗∈HBµ\nT ∑\nt=1\nℓt(f ∗). (6)\nIn the statistical setting we assume that the sequence S = {(xi, yi}Ti=1 is generated IID according to some unknown distribution P. We then define the expected loss of a predictor as\nL(f) = E (x,y)∼P\n[\n1 2 (〈f,Φ(x)〉 − y)2\n]\n. (7)"
    }, {
      "heading" : "3 Main Results",
      "text" : "Theorem 1 states our result for the online model. The corresponding result for the statistical setting is given in Corollary 1. We will elaborate on the structure of the Algorithm later, but first provide the main result.\nAlgorithm 1: The SHRINKING GRADIENT algorithm.\nData: T, B > 1, η,m Result: Weights α(1), . . . , α(T+1) ∈ R(T ). Functions ft ∈ L2(Ω, µ) defined as\nft = ∑t i=1 α (t) i Φ(xi);\nInitialize α(1) = 0 ∈ RT .; for t = 1, . . . , T do\nObserve xt, yt; Set Et = EST SCALAR PROD(α (t),x1:t−1,xt,m); . if |Et| < 16B then α(t+1) = α(t);\nα (t+1) t = η(Et − yt);\nelse\nα(t+1) = 14α (t);\nAlgorithm 2: EST SCALAR PROD\nData: α, x1:t−1, x, m Result: Estimated scalar product E for k=1.. . . ,m do\nSample i from the distribution q(i) = |αi|∑ |αi| ; Sample parameter w̄ from µ. Set E(k) = sgn(αi)ψ(xi; w̄)ψ(x; w̄);\nSet E = ‖α‖1m ∑m k=1E (k)\nTheorem 1. Run Algorithm 1 with parameters T , B ≥ 1, η = B√ T and m = O\n( B4T log (BT ) ) ."
    }, {
      "heading" : "Then:",
      "text" : "1. For every sequence of squared losses ℓ1, . . . , ℓT observed by the algorithm we have for f1, . . . , fT :\nE\n[\nT ∑\nt=1\nℓt(ft)− min f∗∈HBµ ℓt(f ∗)\n]\n= O(B √ T )\n2. The run-time of the algorithm is Õ ( ρB4T 2 ) .3\n3. For each t = 1 . . . T and a new test example x, we can estimate 〈ft,Φ(x)〉 within accuracy ǫ0 by running Algorithm 2 with parameters α\n(t), {xi}ti=1, ,x and m = O(B\n4T ǫ2 0 log 1/δ). The resulting running time at test is then O(ρm).\nWe next turn to the statistical setting, where we provide bounds on the expected performance. Following standard online to batch conversion and Theorem 1 we can obtain the following Corollary (e.g., [25]):\nCorollary 1 (Statistical Setting). Let S = {xt, yt}Tt=1, be an IID sample drawn from some unknown distribution P The following holds for any ǫ > 0. Run Algorithm 1 as in Theorem 1, with T = O(B\nǫ2 ). Let fS = 1 T\n∑\nft. Then the expected loss satisfies:\nE S∼P [L(fS)] < inf f∗∈HBµ\nL(f∗) + ǫ.\nThe runtime of the algorithm, as well as estimation time on a test example are as defined in Theorem 1."
    }, {
      "heading" : "4 Related Work",
      "text" : "Learning random features can be traced to the early days of learning [22], and infinite networks have also been introduced more than 20 years ago [29, 17]. More recent works have considered learning neural nets (also multi-layer) with infinite hidden units using\n3Ignoring logarithmic factors in B and T .\nthe kernel trick [8, 13, 15, 16]. These works take a similar approach to ours but focus on computing the kernel for certain feature classes in order to invoke the kernel trick. Our work in contrast avoids using the kernel trick and applies to any feature class that can be randomly generated. All the above works are part of a broader effort of trying to circumvent hardness in deep learning by mimicking deep nets through kernels [21, 4, 2, 3], and developing general duality between neural networks and kernels [11].\nFrom a different perspective the relation between random features and kernels has been noted in [23] where the authors represent translation invariant kernels in terms of random features. This idea has been further studied in [1, 18] for other kernels as well. The focus of these works is mainly to allow scaling down of the feature space and representation of the final output classifier.\nThe idea is also present in [9], where the authors focus on tractability of large scale kernel methods. More relevant to our work is that one can show that the optimization method in [9] can be invoked whenever the kernel can be estimated using random features. In [9] the objective considered is of the regularized form:γ2‖f‖2 + R(f), with a corresponding sample complexity of O(1/(γ2ǫ2)) samples needed to achieve ǫ approximation with respect to the risk of the optimum of the regularized objective.\nTo relate the above results to ours, we begin by emphasizing that the bound in [9] holds for fixed γ, and refers to optimization of the regularized objective. Our objective is to minimize the risk R(f) which is the expected squared loss, for which we need to choose γ = O( ǫ\nB2 ) in order to attain accuracy ǫ [28]. Plugging this γ into the generalization\nbound in [9] we obtain that the algorithm in [9] needs O(B 4\nǫ4 ) samples to compete with\nthe optimal target function in the B-ball. Our algorithm needs O(B ǫ2 ) examples. We note that their method does extends to a larger class of losses, whereas our is restricted to the quadratic loss.\nIn [24], the authors consider embedding the domain into the feature space x → (ψ(w1;x), . . . , ψ(wm;x), where w1, . . . ,wm are IID random variables sampled according to some prior µ(w). They show that with O(B 2 log 1/δ\nǫ2 ) random features estimated on\nO(B 2 log 1/δ\nǫ2 ) samples they can compete with the class:\nHBµ max = { x → ∫ ψ(w;x)f(w)dµ(w) : |f(w)| ≤ B } . (8)\nOur algorithm relates to the mean square error cost function which does not meet the condition in [24], and is hence formally incomparable. Yet we can invoke our algorithm to compete against a larger class of target functions. Our main result shows that Algorithm 1, using Õ(B 8\nǫ4 ) estimated features and using O(B\n2\nǫ2 ) samples, will, in\nexpectation, output a predictor that is ǫ close to the best in HBµ . Note that |f(w)| < B implies Ew∼µ(f2(w)) < B2. Hence HBµ max ⊆ H B µ . Note howver, that the number of estimated features (as a function of B) is worse in our case. Our approach to the problem is to consider learning with a noisy estimate of the kernel. A related setting was studied in [7], where the authors considered learning with kernels when the data is corrupted. Noise in the data and noise in the scalar product\nestimation are not equivalent when there is non linearity in the kernel space embedding. There is also extensive research on linear regression with actively chosen attributes [6, 14]. The convergence rates and complexity of the algorithms are dimension dependent. It is interesting to see if their method can be extended from finite set of attributes to a continuum set of attributes."
    }, {
      "heading" : "5 Algorithm",
      "text" : "We next turn to present Algorithm 1, from which our main result is derived. The algorithm is similar in spirit to Online Gradient Descent (OGD) [31], but with some important modifications that are necessary for our analysis.\nWe first introduce the problem in the terminology of online convex optimization, as in [31]. At iteration t our algorithm outputs a hypothesis ft. It then receives as feedback (xt, yt), and suffers a loss ℓt(ft) as in Eq. 5.\nThe objective of the algorithm is to minimize the regret against a benchmark of B-bounded functions, as in Eq. 6.\nA classic approach to the problem is to exploit the OGD algorithm. Its simplest version would be to update ft+1 → ft − η∇t where η is a step size, and ∇t is the gradient of the loss wrt f at ft. In our case, ∇t is given by:\n∇t = (yt − 〈ft,Φ(xt)〉) Φ(xt) (9)\nApplying this update would also result in a function ft = ∑t\ni=1 αiΦ(xt) as we have in Algorithm 1 (but with different αi from ours). However, in our setting this update is not applicable since the scalar product 〈ft,Φ(xt)〉 is not available.\nOne alternative is to use a stochastic unbiased estimate of the gradient that we denote by ∇̄t. This induces an update step ft+1 → ft − η∇̄t. One can show that OGD with such an estimated gradient enjoys the following regret bound for every ‖f∗‖ ≤ B (see for example [25]):\nE\n[\n∑ ℓt(ft)− ℓt(f∗) ] ≤ B 2\nη + η\nT ∑\ni=1\nE [ ‖∇t‖2 ] + η\nT ∑\ni=1\nV [ ∇̄t ] . (10)\nWhere V [ ∇̄t ] = E [ ‖∇̄t −∇t‖2 ]\n. We can bound the first two terms using standard techniques applicable for the squared loss (as appeared for example in [30] or [27]). The third term depends on our choice of gradient estimate. There are multiple possible choices for such an estimate, and we use a version which facilitates our analysis.\nAssume that at iteration t, our function ft is given by ft = ∑t i=1 α (t) i Φ(xt). We now want to use sampling to obtain an unbiased estimate of 〈ft,Φ(xt)〉. We will do this via a two step sampling procedure, as described in Algorithm 2. First, sample an index i ∈ [1, . . . , t] by sampling according to the distribution q(i) ∝ |α(t)i |. Next, for the chosen i, sample w̄ according to µ, and use ψ(x; w̄)ψ(xi; w̄) to construct an estimate of\n〈Φ(xi),Φ(xt)〉. The resulting unbiased estimate of 〈Φ(xi),Φ(xt)〉 is denoted by Et and given by:\nEt = ‖α(t)‖1\nm\nm ∑\ni=1\nsgn(α (t) i )ψ(xi; w̄)ψ(xt; w̄) (11)\nThe corresponding unbiased gradient estimate is:\n∇̄t = (Et − yt)xt (12) The variance of ∇̄ affects the convergence rate and depends on both ‖α‖1 and m– the number of estimations. We wish to maintain m = O(T ) estimations per round, while achieving O( √ T ) regret.\nTo effectively regularize ‖α‖1, we modify the OGD algorithm so that whenever Et is larger then 16B, we do not perform the usual update. Instead, we perform a shrinking step that divides α(t) (and hence ft) by 4. Treating B as constant, this guarantees that ‖α‖1 = O(ηT ), and in turn Var(∇̄t) = O(η 2T 2 m ). Setting η = O(1/ √ T ), we have that m = O(T ) estimations are enough. The rationalle behind the shrinkage is that whenever Et is large, it indicates that ft is “far away” from the B-ball, and a shrinkage step, similar to projection, leads ft closer to the optimal element in the B-ball hence improves ft. However, due to stochasticity, the shrinkage step does add a further term to the regret bound that we would need to take care of."
    }, {
      "heading" : "5.1 Analysis",
      "text" : "In what follows we analyze the regret for Algorithm 1. We begin by modifying the regret bound for OGD in Eq. 10 to accommodate for steps that differ from the standard gradient update, such as shrinkage.\nLemma 1. Let ℓ1, . . . , ℓT be an arbitrary sequence of convex loss functions, and let f1, . . . , fT be random vectors, produced by an online algorithm. Assume ‖fi‖ ≤ BT for all i ≤ T . For each t let ∇̄t be an unbiased estimator of ∇ℓt(ft). Denote f̂t = ft−1 − η∇̄t−1 and let\nPt(f ∗) = P\n[ ‖ft − f∗‖ > ‖f̂t − f∗‖ ] . (13)"
    }, {
      "heading" : "For every ‖f∗‖ ≤ B it holds that:",
      "text" : "E\n[\nT ∑\nt=1\nℓt(ft)− ℓt(f∗) ] ≤ B 2\nη +η\nT ∑\nt=1\nE [ ‖∇t‖2 ] +η\nT ∑\nt=1\nV [ ∇̄t ] +\nT ∑\nt=1\n(BT +B) 2\nη E [Pt+1(f\n∗)]\n(14)\nAs discussed earlier, the first two terms in the RHS are the standard bound for OGD from Eq. 10. Note that in OGD we always choose ft = f̂t therefore Pt(f\n∗) = 0 and the last term disappears.\nThe third term will be bounded by controlling ‖α‖1. The last term Pt+1(f∗) is a penalty that results from updates that stir ft away from the standard update step f̂t. This will indeed happen for the shrinkage step. The next lemma bounds this term.\nLemma 2. Run Algorithm 1 with parameters T , B ≥ 1 and η < 1/8. Let ∇̄t be the unbiased estimator of ∇ℓt(ft) of the form\n∇̄t = (Et − yt)Φ(xt)."
    }, {
      "heading" : "Denote f̂t = ft − η∇̄t and define Pt(f∗) as in Eq. 13.",
      "text" : "Pt(f ∗) ≤ 2 exp\n(\n− m (3ηt)2\n)\nConsidering Lemma 2 and Lemma 1, we obtain that the last term in Eq. 14 can be\nbounded by ηT if we consider m = O\n(\n(ηT )2 log (\nBT η\n)2 )\nestimations (while again for\nsimplicity, treating B as constant). One can show that BT = O(ηT ), hence neglecting logarithmic factors and by choice of η = O( 1√\nT ), we obtain that m = Õ(T ) estimations\nper round suffice to bound the fourth term. A full proof of Theorem 1 and the above two Lemmas is given in the Appendix."
    }, {
      "heading" : "6 Discussion",
      "text" : "We presented a new online algorithm that employs kernels implicitly but avoids the kernel trick assumption. Namely, the algorithm can be invoked even when one has access to only estimations of the scalar product. The problem was motivated by kernels resulting from neural nets, but it can of course be applied to any scalar product of the form we described. To summarize, we can cast our result into a larger model that might be of independent interest. Consider a setting where a learner can observe an unbiased estimate of a coordinate in a kernel matrix, or alternatively the scalar product between any two observations. Our results imply that in this setting the above rates are applicable, and at least for the square loss, having no access to the true values in the kernel matrix is not necessarily prohibitive during training.\nThe results show that with sample size T we can achieve error of O( B√ T ). As demonstrated by [26] these rates are optimal, even when the scalar product is computable. To achieve this rate our algorithm needs to perform Õ(B4T 2) scalar product estimations. When the scalar product can be computed, existing kernelized algorithms need to observe a fixed proportion of the kernel matrix, hence they observe order of Ω(T 2) scalar products. In [5] it was shown that when the scalar product can be computed exactly, one would need access to at least Ω(T ) entries to the kernel matrix. It is still an open problem whether one has to access Ω(T 2) entries when the kernel can be computed exactly. However, as we show here, for fixed B even if the kernel can only be estimated Õ(T 2) estimations are enough. It would be interesting to further investigate and improve the performance of our algorithm in terms of the norm bound B."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The authors would like to thank Tomer Koren for helpful comments and suggestions. Roi Livni is a recipient of the Google Europe Fellowship in Learning Theory, and this research is supported in part by this Google Fellowship, and a Google Research Award."
    }, {
      "heading" : "A Estimation Procedure – Concentration Bounds",
      "text" : "In this section we provide concentration bounds for the estimation procedure in Algorithm 2.\nLemma 3. Run Algorithm 2 with α and, {xi}Ti=1 x, and m. Let f = ∑\nαiΦ(xi). Assume that |ψ(x;w)| < 1 for all w and x. Let E be the output of Algorithm 2. Then E is an unbiased estimator for 〈f,Φ(x)〉 and:\nP [|E − 〈f,Φ(x)〉| > ǫ] ≤ exp ( − mǫ 2\n‖α‖21\n)\n(15)\nProof. Consider the random variables ‖α‖1E(k) (where E(k) is as defined in Algorithm 2) and note that they are IID. One can show that E [ ‖α‖1E(k) ] = ∑ αiE [ψ(Φ(xi); w̄)ψ(Φ(x); w̄)] = 〈f,Φ(x)〉. By the bound on ψ(x;w) we have that ∣ ∣‖α‖1E(k) ∣\n∣ < ‖α‖1 with probability 1. Since E = 1m ∑ E(k) the result follows directly from Hoeffding’s inequality.\nNext, we relate these guarantees to the output of Algorithm 1:\nLemma 4. The α(t) obtained in Algorithm 1 satisfies:\n‖α(t)‖1 ≤ (16B + 1)ηt.\nAs a corollary of this and Lemma 3 we have that the function ft satisfies:\nP [|Et − 〈ft,Φ(xt)〉| > ǫ] ≤ exp ( − ǫ 2m\n((16B + 1)ηt)2\n)\n(16)\nProof. We prove the statement by induction. We separate into two cases, depending on whether the shrinkage step was performed or not.\nIf |Et| ≥ 16B the algorithm sets α(t+1) = 14α(t), and:\n‖α(t+1)‖1 = 1\n4 ‖α(t)‖1 ≤ (16B + 1)η(t + 1)\nIf |Et| < 16B the gradient update is performed. Since |yt| ≤ 1 we have that |Et−yt| < 16B + 1 and:\n‖α(t+1)‖1 ≤ ‖α(t)‖1 + η|Et − yi| ≤ (16B + 1)η(t + 1)."
    }, {
      "heading" : "B Proofs of Lemmas",
      "text" : ""
    }, {
      "heading" : "B.1 Proof of Lemma 1",
      "text" : "First, by convexity we have that\n2(ℓt(ft)− ℓt(f∗)) ≤ 2 〈∇t, ft − f∗〉 . (17)\nNext we upper bound 〈∇t, ft − f∗〉. Denote by E the event ‖ft+1−f∗‖ > ‖f̂t+1−f∗‖. Note that\nE [ ‖ft+1 − f∗‖2 ] ≤ E [ ‖f̂t+1 − f∗‖2 ] + E [ ‖ft+1 − f∗‖2 ∣ ∣E ] · Pt+1(f∗) ≤\nE\n[ ‖f̂t+1 − f∗‖2 ] + (B +BT ) 2Pt+1(f ∗)\nPlugging in f̂t+1 = ft − η∇̄t we get\nE [ ‖ft+1 − f∗‖2 ] ≤ E [ ‖ft − f∗‖2 ] +η2E [ ‖∇̄t‖2 ] −2ηE [ 〈∇̄t, ft − f∗〉 ] +(B+BT ) 2Pt+1(f ∗)\nDividing by η we have that:\n2E [ 〈∇̄t, ft − f∗〉 ] ≤ E [ ‖ft − f∗‖2 ] − E [ ‖ft+1 − f∗‖2 ]\nη + 1 η E [ (B +BT ) 2Pt+1(f ∗) ] +ηE [ ‖∇̄t‖2 ]\n(18) Taking 17 and 18 and summing we have:\n2E\n[\nT ∑\nt=1\nℓt(ft)− ℓt(f∗) ] ≤ 2E [ ∇⊤t (ft − f∗) ] = 2E [ ∇̄⊤t (ft − f∗) ] ≤\n‖f∗‖2 η + 1 η T ∑\nt=1\nE [ (B +BT ) 2Pt+1(f ∗) ] + η\nT ∑\nt=1\nE [ ‖∇̄t‖2 ] .\nFinally note that E [ ‖∇̄t‖2 ] = E [ ‖∇t‖2 ] + V [ ∇̄t ] to obtain the result."
    }, {
      "heading" : "B.2 Proof for Lemma 2",
      "text" : "To prove the bound in the lemma, we first bound the event Pt(f ∗) w.r.t to two possible events:\nLemma 5. Consider the setting as in Lemma 2. Run Algorithm 1 and for each t consider the following two events:\n• E t1 : |Et| > 16B and |Et| > 14η‖ft‖.\n• E t2 : |Et| > 16B and ‖ft‖ < 8B.\nFor every ‖f∗‖ < B we have that Pt(f∗) < P [ E t1 ∪ E t2 ] .\nProof. Denote the event |Et| > 16B by E t0. Note that if E t0 does not happen, then ft = f̂t. Hence trivially\nPt(f ∗) = P\n[ ‖ft − f∗‖ > ‖f̂t − f∗‖ ∧ E t0 ]\nWe will assume that:\n1. |Et| > 16B.\n2. |Et| < 14η‖ft‖.\n3. ‖ft‖ > 8B\nand show ‖ft+1 − f∗‖ ≤ ‖f̂t+1 − f∗‖. In other words, we will show that if E t0 happens and ‖ft+1 − f∗‖ > ‖f̂t+1 − f∗‖, then either E t2 or E t1 happened. This will conclude the proof. Fix t, note that since |ψ(x;w)| < 1 we have that ‖Φ(x)‖ < 1. We then have:\n‖f̂t+1‖ = ‖ft−η(Et−y)Φ(xt)‖ > ‖ft‖−‖η(Et−y)Φ(xt)‖ ≥ ‖ft‖−η|Et|−η ≥ 3\n4 ‖ft‖−η\nWhere the last inequality is due to assumption 2. We therefore have the following bound for every ‖f∗‖ < B:\n‖f̂t+1 − f∗‖ ≥ 3\n4 ‖ft‖ − η −B\nOn the other hand, if ft+1 6= f̂t+1 then by construction of the algorithm ft+1 = 14ft:\n‖ft+1 − f∗‖ ≤ ‖ft+1‖+ ‖f∗‖ ≤ ‖ft‖ 4 +B.\nNext note that η < 2B and assumption 3 states ‖ft‖ > 8B. Therefore:\n1 2 ‖ft‖ > 4B > η + 2B\nand we obtained the desired result:\n‖f̂t+1−f∗‖ ≥ 3\n4 ‖ft‖−η−B =\n1 4 ‖ft‖+\n(\n1 2 ‖ft‖ − η − 2B\n)\n+B ≥ 1 4 ‖ft‖+B ≥ ‖ft+1−f∗‖\nNext we upper bound P [ E t1 ∪ E t2 ] . In what follows the superscript t is dropped.\nA bound for P [E1 ∩ Ec2 ]: Assume that\n|Et − 〈ft,Φ(xt)〉| < ( 1\n4η − 1)8B.\nWe assume T is sufficiently large and η < 18 . We have 1 4η − 1 > 1. Since we assume E2 did not happen we must have ‖ft‖ > 8B and\n|Et − 〈ft,Φ(xt)〉| < ( 1\n4η − 1)‖f‖.\nWe continue:\nEt − ‖f‖ < |Et − 〈ft,Φ(xt)〉| < ( 1\n4η − 1)‖f‖.\nWhich leads to\nEt < 1\n4η ‖f‖.\nAnd we get that E1 did not happen. We conclude that if E1 and not E2 then:\n|Et − 〈ft,Φ(xt)〉| ≥ ( 1\n4η − 1)8B.\nSince 14η − 1 > 1 we have that:\n|Et − 〈ft,Φ(xt)〉| ≥ 8B.\nWe conclude that:\nP [E1 ∩ Ec2 ] ≤ P [|Et − 〈ft,Φ(xt)〉| ≥ 8B] . (19)\nA bound for P [E2]: If |Et| > 16B and ‖ft‖ < 8B then by normalization of Φ(xt) we have that 〈ft,Φ(xt)〉 < 8B and trivially we have that\n|Et − 〈ft,Φ(xt)〉| ≥ 8B.\nAnd again we have that:\nP [E2] ≤ P [|Et − 〈ft,Φ(xt)〉| ≥ 8B] . (20)\nTaking Eq. 19 and Eq. 20 we have that\nP [E2 ∪ E1] ≤ 2P [|Et − 〈ft,Φ(xt)〉| ≥ 8B] . (21)\nBy Lemma 4 we have that:\nP (|Et − 〈ft,Φ(xt)〉|) > 8B) <\nexp(− m(8B) 2\n((16B + 1)ηt)2 ) < exp\n(\n− m (3ηt)2\n)\nTaking the above upper bounds together with Lemma 5 we can prove Lemma 2."
    }, {
      "heading" : "C Proof of Main Result",
      "text" : "C.1 Some more technical Lemmas\nWe begin by deriving Corollary 2 that bounds the first two terms in the regret bound. As discussed, this section follows standard techniques. We begin with an upper bound on E(‖∇̄t‖2).\nLemma 6. Consider the setting as in Lemma 2. Then\nV [ ∇̄t ] ≤ ((16B + 1)ηt) 2\nm ,\nand, E [ ‖∇t‖2 ]\n≤ 2E [ℓt(ft)] . Proof. Begin by noting that since ‖Φ(x)‖ < 1, it follows from the definitions of ∇, ∇̄ that:\nV [ ∇̄t ] = E [ ‖∇̄t −∇t‖2 ] ≤ E [ (Et − 〈ft,Φ(xt)〉)2 ] = V [Et]\nBy construction (see Algorithm 2) we have that:\nV [Et] = 1 m V [ ‖α(t)‖21ψ(xi;w)ψ(xt;w) ]\nwhere the index i is sampled as in Algorithm 2, and ψ(xi;w)ψ(xt;w) is bounded by 1. By Lemma 4 we have that\nV [Et] ≤ ((16B + 1)ηt)2\nm .\nThis provides the required bound on V [ ∇̄t ] . Additionally, we have that\n‖∇t‖2 = (〈ft,Φ(xt)〉 − yt)2‖Φ(xt)‖2 ≤ 2ℓt(ft)\nand the result follows by taking expectation.\nWe thus have the following corollary that bounds the first three terms in Eq. 14:\nCorollary 2. With the notations and setting of Lemma 2 we have:\nB2\nη + η\nT ∑\nt=1\nE [ ‖∇t‖2 ] + η\nT ∑\nt=1\nV [ ∇̄t ] ≤ 2ηE [ T ∑\nt=1\nℓt(ft)− ℓt(f∗) ] + B2\nη (22)\n+2η\nT ∑\nt=1\nℓt(f ∗) + η\nT ∑\nt=1\n((16B + 1)ηT )2\nm"
    }, {
      "heading" : "C.2 Proof of Theorem 1",
      "text" : "Recall that Lemma 1 and Corollary 2 assume an upper bound BT on ‖ft‖. We begin by noting that BT can be bounded as follows, using Lemma 4:\nBT = max t ‖ft‖ ≤ max t ‖α(t)‖1 ≤ (16B + 1)ηT. (23)\nPlugging Corollary 2 into Eq. 14 we obtain:\n(1−2η)E [ T ∑\nt=1\nℓt(ft)− ℓt(f∗) ] ≤ B 2\nη +2η\nT ∑\nt=1\nℓt(f ∗)+η\nT ∑\nt=1\n((16B + 1)ηT )2\nm + (BT +B)\n2\nη\nT ∑\nt=1\nPt(f ∗)\n(24)\nTo bound the second term we note that:\nmin ‖f∗‖<B\nT ∑\nt=1\nℓt(f ∗) ≤\nT ∑\nt=1\nℓt(0) ≤ T. (25)\nWe next set η and m as in the statement of the theorem. Namely: η = B 2 √ T , and m = ((16B + 1)B)2T log γ, where γ = max (\n((16B+1)ηT+B)2 ) η2 , e ) .\nOur choice of m implies that m > ((16B+1)ηT )2, and hence the third term in Eq. 24 is bounded as follows:\nη T ∑\nt=1\n((16B + 1)ηT )2\nm ≤ ηT (26)\nNext we have that m > (3ηt)2 log γ for every t, and by the bound on BT we have\nthat γ > (B+BT ) 2\nη2 . Taken together with Lemma 2 we have that:\n(BT +B) 2\nη\nT ∑\nt=1\nPt(f ∗) ≤ ηT. (27)\nTaking Eq. 25, Eq. 27 and Eq. 26, plugging them into Eq. 24 we have that:\n(1− 2η)E [ T ∑\nt=1\nℓt(ft)− ℓt(f∗) ] ≤ B 2\nη + 2ηT + ηT + ηT\nFinally by choice of η, and dividing both sides by (1 − 2η) we obtain the desired result.\nIt remains to show that we can estimate each ft in the desired complexity (the result\nfor the averaged f is the same). Each ft has the form ft = ∑T t=1 α (t) i xi, By Lemma 3 and Lemma 4, running Algorithm 2 m iterations will lead to a random variable E such that:\nP [|E − 〈ft,x〉 |] ≤ exp ( − ǫ 2m\n((16B + 1)B √ T )2\n)\n.\nWe obtain that order of O(B 4T ǫ2 log 1/δ) estimations are enough."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>Infinite–Layer Networks (ILN) have recently been proposed as an architecture<lb>that mimics neural networks while enjoying some of the advantages of kernel meth-<lb>ods. ILN are networks that integrate over infinitely many nodes within a single<lb>hidden layer. It has been demonstrated by several authors that the problem of<lb>learning ILN can be reduced to the kernel trick, implying that whenever a certain<lb>integral can be computed analytically they are efficiently learnable.<lb>In this work we give an online algorithm for ILN, which avoids the kernel trick<lb>assumption. More generally and of independent interest, we show that kernel meth-<lb>ods in general can be exploited even when the kernel cannot be efficiently computed<lb>but can only be estimated via sampling.<lb>We provide a regret analysis for our algorithm, showing that it matches the<lb>sample complexity of methods which have access to kernel values. Thus, our method<lb>is the first to demonstrate that the kernel trick is not necessary as such, and random<lb>features suffice to obtain comparable performance.",
    "creator" : "LaTeX with hyperref package"
  }
}