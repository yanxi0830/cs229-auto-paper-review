{
  "name" : "1406.4802.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Charles Soussen", "Jérôme Idier", "Junbo Duan" ],
    "emails" : [ "charles.soussen@univ-lorraine.fr,", "david.brie@univ-lorraine.fr.", "jerome.idier@irccyn.ec-nantes.fr.", "junbo.duan@mail.xjtu.edu.cn." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n48 02\nv2 [\ncs .N\nA ]\n1 8\nM ar\nSparse signal restoration is usually formulated as the minimization of a quadratic cost function ‖y−Ax‖22, where A is a dictionary and x is an unknown sparse vector. It is well-known that imposing an ℓ0 constraint leads to an NP-hard minimization problem. The convex relaxation approach has received considerable attention, where the ℓ0-norm is replaced by the ℓ1-norm. Among the many efficient ℓ1 solvers, the homotopy algorithm minimizes ‖y −Ax‖2 2 + λ‖x‖1 with respect to x for a continuum of λ’s. It is inspired by the piecewise regularity of the ℓ1-regularization path, also referred to as the homotopy path. In this paper, we address the minimization problem ‖y −Ax‖2 2 + λ‖x‖0 for a continuum of λ’s and propose two heuristic search algorithms for ℓ0-homotopy. Continuation Single Best Replacement is a forward-backward greedy strategy extending the Single Best Replacement algorithm, previously proposed for ℓ0-minimization at a given λ. The adaptive search of the λ-values is inspired by ℓ1-homotopy. ℓ0 Regularization Path Descent is a more complex algorithm exploiting the structural properties of the ℓ0-regularization path, which is piecewise constant with respect to λ. Both algorithms are empirically evaluated for difficult inverse problems involving ill-conditioned dictionaries. Finally, we show that they can be easily coupled with usual methods of model order selection.\nThis work was carried out in part while C. Soussen was visiting IRCCyN during the academic year 2010-2011 with the\nfinancial support of CNRS.\nC. Soussen and D. Brie are with the Université de Lorraine and CNRS at the Centre de Recherche en Automatique de Nancy (UMR 7039).Campus Sciences, B.P. 70239, F-54506 Vandœuvre-lès-Nancy, France. Tel: (+33)-3 83 59 56 43, Fax: (+33)-3 83 68 44 62. E-mail: charles.soussen@univ-lorraine.fr, david.brie@univ-lorraine.fr.\nJ. Idier is with L’UNAM Université, Ecole Centrale Nantes and CNRS at the Institut de Recherche en Communications et Cybernétique de Nantes (UMR 6597), 1 rue de la Noë, BP 92101, F-44321 Nantes Cedex 3, France. Tel: (+33)-2 40 37 69 09, Fax: (+33)-2 40 37 69 30. E-mail: jerome.idier@irccyn.ec-nantes.fr.\nJ. Duan was with CRAN. He is now with the Department of Biomedical Engineering, Xi’an Jiaotong University. No. 28, Xianning West Road, Xi’an 710049, Shaanxi Province, China. Tel: (+86)-29-82 66 86 68, Fax: (+86)-29 82 66 76 67. E-mail: junbo.duan@mail.xjtu.edu.cn.\nMarch 19, 2015 DRAFT\nIndex Terms\nSparse signal estimation; ℓ0-regularized least-squares; ℓ0-homotopy; ℓ1-homotopy; stepwise algo-\nrithms; orthogonal least squares; model order selection.\nI. INTRODUCTION\nSparse approximation from noisy data is traditionally addressed as the constrained least-square problems\nmin x\n‖y −Ax‖22 subject to ‖x‖0 ≤ k (1)\nor\nmin x\n‖x‖0 subject to ‖y −Ax‖ 2 2 ≤ ε (2)\nwhere ‖x‖0 is the ℓ0-“norm” counting the number of nonzero entries in x, and the quadratic fidelity-todata term ‖y −Ax‖22 measures the quality of approximation. Formulation (1) is well adapted when one has a knowledge of the maximum number k of atoms to be selected in the dictionary A. On the contrary,\nthe choice of (2) is more appropriate when k is unknown but one has a knowledge of the variance of\nthe observation noise. The value of ε may then be chosen relative to the noise variance. Since both (1)\nand (2) are subset selection problems, they are discrete optimization problems. They are known to be\nNP-hard except for specific cases [1].\nWhen no knowledge is available on either k or ε, the unconstrained formulation\nmin x\n{J (x;λ) = ‖y −Ax‖22 + λ‖x‖0} (3)\nis worth being considered, where λ expresses the trade-off between the quality of approximation and the\nsparsity level [2]. In a Bayesian viewpoint, (3) can be seen as a (limit) maximum a posteriori formulation where ‖y−Ax‖22 and the penalty ‖x‖0 are respectively related to a Gaussian noise distribution and a prior distribution for sparse signals (a limit Bernoulli-Gaussian distribution with infinite Gaussian variance) [3]."
    }, {
      "heading" : "A. Classification of methods",
      "text" : "1) ℓ0-constrained least-squares: The discrete algorithms dedicated to problems (1)-(2) can be categorized into two classes. First, the forward greedy algorithms explore subsets of increasing cardinalities\nstarting from the empty set. At each iteration, a new atom is appended to the current subset, therefore\ngradually improving the quality of approximation [4]. Greedy algorithms include, by increasing order of\ncomplexity: Matching Pursuit (MP) [5], Orthogonal Matching Pursuit (OMP) [6], and Orthogonal Least\nMarch 19, 2015 DRAFT\nSquares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order\nRecursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10]. The\nsecond category are thresholding algorithms, where each iteration delivers a subset of same cardinality\nk. Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and\nCoSaMP [13].\nAmong these two categories, greedy algorithms are well-adapted to the resolution of (1) and (2) for\nvariable sparsity levels. Indeed, they yield a series of subsets for consecutive k (i.e., for decreasing\napproximation errors ε) since at each iteration, the current subset is increased by one element.\n2) ℓ0-penalized least-squares: In [3], we evidenced that the minimization of J (x;λ) using a descent algorithm leads to bidirectional extensions of forward (orthogonal) greedy algorithms. To be more specific,\nconsider a candidate subset S corresponding to the support of x. Including a new element into S yields a decrease of the square error, defined as the minimum of ‖y−Ax‖22 for x supported by S. On the other hand, the penalty term λ‖x‖0 is increased by λ. Overall, the cost function J (x;λ) decreases as soon as the square error variation exceeds λ. Similarly, a decrease of J (x;λ) occurs when an element is removed\nfrom S provided that the squared error increment is lower than λ. Because both inclusion and removal\noperations can induce a decrease of J , the formulation (3) allows one to design descent schemes allowing\na “forward-backward” search strategy, where each iteration either selects a new atom (forward selection)\nor de-selects an atom that was previously selected (backward elimination). The Bayesian OMP [14] and\nSingle Best Replacement (SBR) [3] algorithms have been proposed in this spirit. They are extensions of\nOMP and OLS, respectively. Their advantage over forward greedy algorithms is that an early wrong atom\nselection may be later cancelled. Forward-backward algorithms include the so-called stepwise regression\nalgorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14],\n[17].\n3) Connection with the continuous relaxation of the ℓ0 norm: The algorithms described so far are discrete search strategies dedicated to ℓ0-regularized least-squares. A classical alternative consists in relaxing the ℓ0-norm by a continuous function that is nondifferentiable at 0, and optimizing the resulting cost function. See, e.g., [18], [19] and [20]–[27] for convex (ℓ1) and nonconvex relaxation, respectively. The convex problem minx ‖y − Ax‖22 s.t. ‖x‖1 ≤ t is referred to as both Basis Pursuit Denoising (BPDN) and the LASSO. It is noticeable that BPDN leads to stepwise algorithms [18], [28] including\nthe popular ℓ1-homotopy [28]–[30], a forward-backward greedy search whose complexity is close to that of OMP. ℓ1-homotopy is closely connected to the Least Angle Regression (LARS), a simpler forward strategy allowing only atom selections. It is referred to as “LARS with the LASSO modification” in [30].\nMarch 19, 2015 DRAFT\nImportantly, ℓ1-homotopy solves the BPDN for a continuum of values of t."
    }, {
      "heading" : "B. Main idea",
      "text" : "Our approach is dedicated to ℓ0-penalized least-squares. It is based on the following geometrical\ninterpretation.\nFirst, for any subset S, we can define a linear function λ 7→ E(S) + λ|S|, where E(S) = ‖y −Ax‖22 is the corresponding least-square error and |S| stands for the cardinality of S. For each subset S, this\nfunction yields a line in the 2D domain (λ,J ), as shown on Fig. 1.\nSecond, the set of solutions to (3) is piecewise constant with respect to λ (see Appendix A for a proof).\nGeometrically, this result can be easily understood by noticing that the minimum of J (x;λ) with respect\nto x is obtained for all λ-values by considering the concave envelope of the set of lines λ 7→ E(S)+λ|S|\nfor all subsets S. The resulting piecewise affine curve is referred to as the ℓ0-curve (see Fig. 1). Its edges are related to the supports of the sparse solutions for all λ, and its vertices yield the breakpoints λ⋆i around which the set of optimal solutions argmin x J (x;λ) is changing.\nWe take advantage of this interpretation to propose two suboptimal greedy algorithms that address (3)\nfor a continuum of λ-values. Continuation Single Best Replacement (CSBR) repeatedly minimizes J (x;λ)\nwith respect to x for decreasing λ’s. ℓ0 Regularization Path Descent (ℓ0-PD) is a more complex algorithm maintaining a list of subsets so as to improve (decrease) the current approximation of the ℓ0 curve.\nMarch 19, 2015 DRAFT"
    }, {
      "heading" : "C. Related works",
      "text" : "1) Bi-objective optimization: The formulations (1), (2) and (3) can be interpreted as the same biobjective problem because they all intend to minimize both the approximation error ‖y −Ax‖22 and the sparsity measure ‖x‖0. Although x is continuous, the bi-objective optimization problem should rather be considered as a discrete one where both objectives reread E(S) and |S|. Indeed, the continuous solutions\ndeduce from the discrete solutions, x reading as a least-square minimizer among all vectors supported\nby S.\nFig. 2 is a classical bi-objective representation where each axis is related to a single objective [31], namely |S| and E(S). In bi-objective optimization, a point S is called Pareto optimal when no other point S′ can decrease both objectives [32]. In the present context, |S| takes integer values, thus the Pareto solutions are the minimizers of E(S) subject to |S| ≤ k for consecutive values of k. Equivalently, they minimize |S| subject to E(S) ≤ ε for some ε. They are usually classified as supported or non-supported. The former lay on the convex envelope of the Pareto frontier (the bullet points in Fig. 2) whereas the latter lay in the nonconvex areas (the square point). It is well known that a supported solution can be reached when minimizing the weighted sum of both objectives, i.e., when minimizing E(S) + λ|S| with respect to S for some weight λ. On the contrary, the non-supported solutions cannot [32]. Choosing between the weighting sum method and a more complex method is a nontrivial question. The answer depends on the problem at-hand and specifically, on the size of the nonconvex areas in the Pareto frontier.\nMarch 19, 2015 DRAFT\n2) ℓ1 and ℓ0-homotopy seen as a weighted sum method: It is important to notice that for convex objectives, the Pareto solutions are all supported. Consider the BPDN; because ‖y − Ax‖22 and ‖x‖1 are convex functions of x, the set of minimizers of ‖y − Ax‖22 + λ‖x‖1 for all λ coincides with the set of minimizers of ‖y − Ax‖22 s.t. ‖x‖1 ≤ t for all t [33]. Both sets are referred to as the (unique) “ℓ1-regularization path”. The situation is different with ℓ0-regularization. Now, the weighted sum formulation (3) may not yield the same solutions as the constrained formulations (1) and (2) because the\nℓ0-norm is nonconvex [2]. This will lead us to define two ℓ0-regularization paths, namely the “ℓ0-penalized path” and the “ℓ0-constrained path” (Section II).\nOn the algorithmic side, the ℓ0 problems are acknowledged to be difficult. Many authors actually discourage the direct optimization of J because there are a very large number of local minimizers [20],\n[23]. In [3], however, we showed that forward-backward extensions of OLS are able to escape from some\nlocal minimizers of J (x;λ) for a given λ. This motivates us to propose efficient OLS-based strategies\nfor minimizing J for variable λ-values.\n3) Positioning with respect to other stepwise algorithms: In statistical regression, the word “stepwise”\noriginally refers to Efroymson’s algorithm [15], proposed in 1960 as an empirical extension of forward\nselection (i.e., OLS). Other stepwise algorithms were proposed in the 1980’s [8, Chapter 3] among\nwhich Berk’s and Broersen’s algorithms [16], [34]. All these algorithms perform a single replacement\nper iteration, i.e., a forward selection or a backward elimination. They were originally applied to over-\ndetermined problems in which the number of columns of A is lower than the number of rows. Recent\nstepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36]. They all aim to\nfind subsets of cardinality k yielding a low approximation error E(S) for all k. Although our algorithms\nshare the same objective, they are inspired by (i) the ℓ1-homotopy algorithm; and (ii) the structural properties of the ℓ0-regularization paths. To the best of our knowledge, the idea of reconstructing an ℓ0-regularization path using ℓ0-homotopy procedures is novel.\nCSBR and ℓ0-PD both read as descent algorithms in different senses: CSBR, first sketched in [37], repeatedly minimizes J (x;λ) for decreasing λ’s. On the contrary, ℓ0-PD minimizes J (x;λ) for any λvalue simultaneously by maintaining a list of candidate subsets. The idea of maintaining a list of support\ncandidates was recently developed within the framework of forward selection [38], [39]. Our approach\nis different, because a family of optimization problems are being addressed together. In contrast, the\nsupports in the list are all candidate solutions to solve the same problem in [38], [39].\n4) Positioning with respect to continuation algorithms: The principle of continuation is to handle a\ndifficult problem by solving a sequence of simpler problems with warm start initialization, and gradually\nMarch 19, 2015 DRAFT\ntuning some continuous hyperparameter [40]. In sparse approximation, the word continuation is used in\ntwo opposite contexts.\nFirst, the BDPN problem involving the ℓ1-norm. BPDN is solved for decreasing hyperparameter values using the solution for each value as a warm starting point for the next value [4]. ℓ1-homotopy [28], [30], [41] exploits that the ℓ1 regularization path is piecewise affine and tracks the breakpoints between consecutive affine pieces. CSBR is designed in a similar spirit and can be interpreted as an “ℓ0-homotopy” procedure (although the ℓ0 minimization steps are solved in a sub-optimal way) working for decreasing λ-values.\nSecond, the continuous approximation of the (discrete) ℓ0 pseudo-norm [42] using a Graduated Non Convexity (GNC) approach [43]: a series of continuous concave metrics is considered leading to the reso-\nlution of continuous optimization problems with warm start initialization. Although the full reconstruction\nof the ℓ0-regularization path has been rarely addressed, it is noticeable that a GNC-like approach, called SparseNet, aims to gradually update some estimation of the regularization path induced by increasingly\nnon-convex sparsity measures [44]. This strategy relies on the choice of a grid of λ-values. Because\nthe influence of the grid is critical [33], it is useful to adapt the grid while the nonconvex measure is\nmodified [44]. On the contrary, our approach does not rely on a grid definition. The λ-values are rather\nadaptively computed similar to the ℓ1-homotopy principle [28], [30].\nThe paper is organized as follows. In Section II, we define the ℓ0-regularization paths and establish their main properties. The CSBR and ℓ0-PD algorithms are respectively proposed in Sections III and IV. In Section V, both algorithms are analyzed and compared with the state-of-art algorithms based on\nnonconvex penalties for difficult inverse problems. Additionally, we investigate the automatic choice of\nthe cardinality k using classical order selection rules.\nII. ℓ0-REGULARIZATION PATHS"
    }, {
      "heading" : "A. Definitions, terminology and working assumptions",
      "text" : "Let m×n denote the size of the dictionary A (usually, m ≤ n in sparse approximation). The observation\nsignal y and the weight vector x are of size m×1 and n×1, respectively. We assume that any min(m,n)\ncolumns of A are linearly independent so that for any subset S ⊂ {1, . . . , n}, the submatrix of A gathering\nthe columns indexed by S is full rank, and the least-square error E(S) can be numerically computed.\nThis assumption is however not necessary for the theoretical results provided hereafter.\nWe denote by |S| the cardinality of a subset S. We use the alternative notations “S+{i}” and “S−{i}”\nfor the forward selection S ∪ {i} and backward elimination S \\ {i}. We can then introduce the generic\nMarch 19, 2015 DRAFT\nnotation S ±{i} for single replacements: S ± {i} stands for S + {i} if i /∈ S, and S −{i} if i ∈ S. We\nwill frequently resort to the geometrical interpretation of Fig. 1. With a slight abuse of terminology, the\nline λ 7→ E(S) + λ|S| will be simply referred to as “the line S”.\nHereafter, we start by defining the ℓ0-regularized paths as the set of supports of the solutions to problems (1), (2) and (3) for varying hyperparameters. As seen in Section I, the solutions may differ whether\nthe ℓ0-regularization takes the form of a bound constraint or a penalty. This will lead us to distinguish the “ℓ0-constrained path” and the “ℓ0-penalized path”. We will keep the generic terminology “ℓ0-regularization paths” for statements that apply to both. The solutions delivered by our greedy algorithms will be referred\nto as the “approximate ℓ0-penalized path” since they are suboptimal algorithms."
    }, {
      "heading" : "B. Definition and properties of the ℓ0-regularized paths",
      "text" : "The continuous problems (1), (2) and (3) can be converted as the discrete problems:\nmin S E(S) subject to |S| ≤ k, (4)\nmin S |S| subject to E(S) ≤ ε, (5)\nmin S\n{ Ĵ (S;λ) , E(S) + λ|S| } , (6)\nwhere S stands for the support of x. The optimal solutions x to problems (1), (2) and (3) can indeed\nbe simply deduced from those of (4), (5) and (6), respectively, x reading as the least-square minimizers\namong all vectors supported by S. In the following, the formulation (5) will be omitted because it leads\nto the same ℓ0-regularization path as formulation (4) [2].\nLet us first define the set of solutions to (4) and (6) and the ℓ0-curve, related to the minimum value\nin (6) for all λ > 0.\nDefinition 1 For k ≤ min(m,n), let S⋆C(k) be the set of minimizers of the constrained problem (4).\nFor λ > 0, let S⋆P(λ) be the set of minimizers of the penalized problem (6). Additionally, we define the ℓ0-curve as the function λ 7→ minS{Ĵ (S;λ)}. It is the concave envelope of a finite number of linear functions. Thus, it is concave and piecewise affine. Let λ⋆I+1 , 0 < λ ⋆ I < . . . < λ ⋆ 1 < λ ⋆ 0 , +∞ delimit the affine intervals (I + 1 contiguous intervals; see Fig. 1 in the case where I = 2).\nEach set S⋆C(k) or S ⋆ P(λ) can be thought of as a single support (e.g., S ⋆ C(k) is reduced to the support S⋆a in the example of Fig. 2). They are defined as sets of supports because the minimizers of (4) and (6) might not be always unique. Let us now provide a key property of the set S⋆P(λ).\nMarch 19, 2015 DRAFT\nTheorem 1 S⋆P(λ) is a piecewise constant function of λ, being constant on each interval λ ∈ (λ ⋆ i+1, λ ⋆ i ).\nProof: See Appendix A.\nThis property allows us to define the ℓ0-regularization paths in a simple way.\nDefinition 2 The ℓ0-constrained path is the set (of sets) S⋆C = {S ⋆ C(k), k = 0, . . . ,min(m,n)}.\nThe ℓ0-penalized path is defined as S⋆P = {S ⋆ P(λ), λ > 0}. According to Theorem 1, S ⋆ P is composed\nof (I + 1) distinct sets S⋆P(λ), one for each interval λ ∈ (λ ⋆ i+1, λ ⋆ i ).\nS⋆C gathers the solutions to (4) for all k. As illustrated on Fig. 2, the elements of S ⋆ C are the Pareto solutions whereas the elements of S⋆P correspond to the convex envelope of the Pareto frontier. Therefore, both ℓ0-regularization paths may not coincide [2], [31]. As stated in Theorem 2, S⋆P ⊂ S ⋆ C, but the reverse inclusion is not guaranteed.\nTheorem 2 S⋆P ⊂ S ⋆ C. Moreover, for any λ /∈ {λ ⋆ I , . . . , λ ⋆ 0}, there exists k such that S ⋆ P(λ) = S ⋆ C(k).\nProof: See Appendix A."
    }, {
      "heading" : "C. Approximate ℓ0-penalized regularization path",
      "text" : "Let us introduce notations for the approximate ℓ0-penalized path delivered by our heuristic search algorithms. Throughout the paper, the ⋆ notation is reserved for optimal solutions (e.g., S⋆P). It is removed when dealing with numerical solutions. The outputs of our algorithms will be composed of a list λ =\n{λ1, . . . , λJ+1} of decreasing λ-values, and a list S = {S0, . . . , SJ} of candidate supports, with S0 = ∅. Sj is a suboptimal solution to (6) for λ ∈ (λj+1, λj). In the first interval λ > λ1, the solution is S0 = ∅. The reader shall keep in mind that each output Sj induces a suboptimal solution xj to (3) for λ ∈ (λj+1, λj). This vector is the least-square solution supported by Sj . It can be computed using the pseudo-inverse of the subdictionary indexed by the set of atoms in Sj .\nGeometrically, each support Sj yields a line segment. Appending these segments yields an approximate\nℓ0-curve covering the domain (λJ+1,+∞), as illustrated on Fig. 3."
    }, {
      "heading" : "III. GREEDY CONTINUATION ALGORITHM (CSBR)",
      "text" : "Our starting point is the Single Best Replacement algorithm [3] dedicated to the minimization of\nJ (x;λ) with respect to x, or equivalently to Ĵ (S;λ) = E(S)+λ|S| with respect to S. We first describe\nSBR for a given λ. Then, the CSBR extension is presented for decreasing and adaptive λ’s.\nMarch 19, 2015 DRAFT"
    }, {
      "heading" : "A. Single Best Replacement",
      "text" : "SBR is a deterministic descent algorithm dedicated to the minimization of Ĵ (S;λ) with the initial\nsolution S = ∅. An SBR iteration consists of three steps:\n1) Compute Ĵ (S ± {i};λ) for all possible single replacements S ± {i} (n insertion and removal\ntrials);\n2) Select the best replacement Sbest = S ± {ℓ}, with\nℓ ∈ argmin i∈{1,...,n} Ĵ (S ± {i};λ); (7)\n3) Update S ← Sbest.\nSBR terminates when Ĵ (Sbest;λ) ≥ Ĵ (S;λ), i.e., when no single replacement can decrease the cost function. This occurs after a finite number of iterations because SBR is a descent algorithm and there are\na finite number of possible subsets S ⊂ {1, . . . , n}. In the limit case λ = 0, we have Ĵ (S; 0) = E(S).\nOnly insertions can be performed since any removal increases the squared error E(S). SBR coincides with\nthe well-known OLS algorithm [7]. Generally, the n replacement trials necessitate to compute E(S+{i})\nfor all insertion trials and E(S − {i}) for all removals. In [3], we proposed a fast and stable recursive implementation based on the Cholesky factorization of the Gram matrix ATSAS when S is modified by one element (where AS stands for the submatrix of A gathering the active columns). SBR is summarized in Tab. I. The optional output parameters ℓadd and δEadd are unnecessary in the standard version. Their knowledge will be useful to implement the extended CSBR algorithm.\nLet us illustrate the behavior of SBR on a simple example using the geometrical interpretation of Fig. 4,\nwhere a single replacement is represented by a vertical displacement (from top to bottom) between the two lines S and S ± {ℓ}. Sinit = ∅ yields an horizontal line since Ĵ (∅;λ) = ‖y‖22 does not depend on\nMarch 19, 2015 DRAFT\nλ. At the first SBR iteration, a new dictionary atom ℓ = a is selected. The line related to the updated\nsupport S ← {a} is of slope |S| = 1. Similarly, some new dictionary atoms b and c are being selected\nin the next iterations, yielding the supports S ← {a, b} and S ← {a, b, c}. On Fig. 4, the dotted lines\nrelated to the latter supports have slopes equal to 2 and 3. At iteration 4, the single best replacement is\nthe removal ℓ = a. The resulting support S ← {b, c} is of cardinality 2, and the related line is parallel to\nthe line {a, b} found at iteration 2. During the fifth iteration, none of the n single replacements decreases\nĴ ({b, c};λ). SBR stops with output S = {b, c}."
    }, {
      "heading" : "B. Principle of the continuation search",
      "text" : "Our continuation strategy is inspired by ℓ1-homotopy which recursively computes the minimizers of ‖y−Ax‖22 +λ‖x‖1 when λ is continuously decreasing [28]–[30]. An iteration of ℓ1-homotopy consists in two steps:\n• Find the next value λnew < λcur for which the ℓ1 optimality conditions are violated with the current\nactive set S (λcur denotes the current value);\n• Compute the single replacement S ← S ± {i} allowing to fulfill the ℓ1 optimality conditions at\nλ = λnew.\nMarch 19, 2015 DRAFT\nCSBR follows the same principle. The first step is now related to some local ℓ0-optimality conditions, and the second step consists in calling SBR at λnew with the current active set as initial solution; see Fig. 5 for a sketch. A main difference with ℓ1-homotopy is that the ℓ0 solutions are suboptimal, i.e., they are local minimizers of J (x;λ) with respect to x.\n1) Local optimality conditions: Let us first reformulate the stopping conditions of SBR at a given λ.\nSBR terminates when a local minimum of Ĵ (S;λ) has been found:\n∀i ∈ {1, . . . , n}, Ĵ (S ± {i};λ) ≥ Ĵ (S;λ). (8)\nThis condition is illustrated on Fig. 6(a): all lines related to single replacements S ± {i} lay above the\nblack point representing the value of Ĵ (S;λ) for the current λ. By separating the conditions related to\ninsertions S + {i} and removals S − {i}, (8) rereads as the interval condition:\nλ ∈ [δEadd(S), δErmv(S)], (9)\nwhere\nδEadd(S) , max i/∈S\n{ E(S)− E(S + {i}) }\n(10a)\nδErmv(S) , min i∈S\n{ E(S − {i}) − E(S) }\n(10b)\nrefer to the maximum variation of the squared error when an atom is added in the support S (respectively,\nremoved from S).\nMarch 19, 2015 DRAFT\n2) Violation of the local optimality conditions: Consider the current output S = SBR(Sinit;λcur). The local optimality condition (9) is then met for λ = λcur, but also for any λ ∈ [δEadd(S), λcur]. The new value for which (9) is violated is λnew = δEadd(S) − c where c > 0 is arbitrarily small. The violation occurs for i = ℓadd, with\nℓadd ∈ argmax i/∈S {E(S) − E(S + {i})}. (11)\nMarch 19, 2015 DRAFT\nIn practice, λnew can be set to the limit value\nλnew = δEadd(S) (12)\nprovided that S is replaced with S + {ℓadd}.\nAs illustrated on Fig. 6(b), the line S+ {ℓadd} lays below all other parallel lines S+ {i}. It intersects line S at λnew. The vertical arrow represents the new call to SBR with inputs S + {ℓadd} and λnew. Because S and S+{ℓadd} both lead to the same value of Ĵ ( . ;λnew), the de-selection of ℓadd is forbidden in the first iteration of SBR."
    }, {
      "heading" : "C. CSBR algorithm",
      "text" : "CSBR is summarized in Tab. II. The repeated calls to SBR deliver subsets Sj for decreasing λj . As shown on Fig. 5, the solution Sj covers the interval (λj+1, λj ]. At the very first iteration, we have S0 = ∅, and (11)-(12) reread:\nℓadd ∈ argmax i∈{1,...,n}\n|〈y,ai〉|\n‖ai‖2 and λ1 =\n〈y,aℓadd〉 2\n‖aℓadd‖ 2 2\n. (13)\nAccording to Tab. II, CSBR stops when λj = 0, i.e., the whole domain λ ∈ R+ has been scanned. However, this choice may not be appropriate when dealing with noisy data and overcomplete dictionaries.\nIn such cases, ad hoc early stopping rules can be considered [28], [45]. A natural rule takes the form\nMarch 19, 2015 DRAFT\nλj ≤ λstop with λstop > 0. Alternative rules involve a maximum cardinality (|Sj | ≥ kstop) and/or a minimum squared error (E(Sj) ≤ εstop).\nFig. 5 shows a step-by-step illustration with the early stop λj ≤ λstop. The initial support Sinit = {ℓadd} and λ1 are precomputed in (13). In the first call S1 = SBR(Sinit;λ1), a number of single replacements updates S ← S±{ℓ} are carried out leading to S1 = S. This process is represented by the plain vertical arrow at λ1 linking both lines S0 and S1 (the line Sinit is not shown for readability reasons). Once S1 is obtained, the next value λ2 is computed. This process is represented by an oblique, dashed arrow joining λ1 and λ2. These two processes are being repeated alternatively at the second and third iterations of CSBR. Finally, CSBR terminates after λ4 has been computed because λ4 ≤ λstop.\nIV. ℓ0-REGULARIZATION PATH DESCENT (ℓ0-PD)\nOn the theoretical side, the ℓ0-penalized regularization path is piecewise constant (Theorem 1). It yields the ℓ0 curve which is piecewise affine, continuous and concave (Fig. 1). The curve related to the CSBR outputs does not fulfill this property since: (i) there might be jumps in this curve; and (ii) the slope of the\nline Sj is not necessarily increasing with j (see Fig. 5). This motivates us to propose another algorithm whose outputs are consistent with the structural properties of the ℓ0-curve.\nWe propose to gradually update a list S of candidate subsets Sj while imposing that the related curve is a concave polygon, obtained as the concave envelope of the set of lines Sj (see Fig. 7(a)). The subsets in S are updated so as to decrease at most the concave polygonal curve. In particular, we impose that\nthe least value is λJ+1 = 0, so that the concave envelope is computed over the whole domain λ ∈ R+."
    }, {
      "heading" : "A. Descent of the concave polygon",
      "text" : "The principle of ℓ0-PD is to perform a series of descent steps, where a new candidate subset Snew is considered and included in the list S only if the resulting concave polygon can be decreased. This\ndescent test is illustrated on Fig. 7 for two examples (top and bottom subfigures). For each example, the\ninitial polygon is represented in (a). It is updated when its intersection with the line Snew is non-empty (b). The new concave polygon (c) is obtained as the concave envelope of the former polygon and the\nline Snew. All subsets in S whose edges lay above the line Snew are removed from S .\nThis procedure is formally presented in Tab. III. Let us now specify how the new candidate subsets\nSnew are built.\nMarch 19, 2015 DRAFT"
    }, {
      "heading" : "B. Selection of the new candidate support",
      "text" : "We first need to assign a Boolean label Sj .expl to each subset Sj . It equals 1 if Sj has already been “explored” and 0 otherwise. The following exploration process is being carried out given a subset\nS = Sj: all the possible single replacements S±{i} are tested. The best insertion ℓadd and removal ℓrmv are both kept in memory, with ℓadd defined in (11) and similarly,\nℓrmv ∈ argmin i∈S {E(S − {i}) − E(S)}. (14)\nAt any ℓ0-PD iteration, the unexplored subset Sj of lowest cardinality (i.e., of lowest index j) is selected. ℓ0-PD attempts to include Sadd = Sj + {ℓadd} and Srmv = Sj − {ℓrmv} into S , so that the concave polygon can be decreased at most. The CCV Descent procedure (Tab. III) is first called with\nSnew ← Sadd leading to possible updates of S and λ. It is called again with Snew ← Srmv. Fig. 7 illustrates each of these calls: the slope of Snew is |Sj | + 1 and |Sj| − 1, respectively. When a support\nMarch 19, 2015 DRAFT\nSj has been explored, the new supports that have been included in S (if any) are tagged as unexplored."
    }, {
      "heading" : "C. ℓ0-PD algorithm",
      "text" : "ℓ0-PD is stated in Tab. IV. Initially, S is formed of the empty support S0 = ∅. The resulting concave polygon is reduced to a single horizontal edge. The corresponding endpoints are λ1 = 0 and (by extension) λ0 , +∞. In the first iteration, S0 is explored: the best insertion Sadd = {ℓadd} is computed in (13), and included in S during the call to CCV Descent. The updated set S is now composed of S0 = ∅ (explored) and S1 = Sadd (unexplored). The new concave polygon has two edges delimited by λ2 = 0, λ1 and λ0 = +∞, with λ1 given in (13). Generally, either 0, 1, or 2 new unexplored supports Sadd and Srmv may be included in S at a given iteration while a variable number of supports may be removed from S .\nℓ0-PD terminates when all supports in S have been explored. When this occurs, the concave polygon cannot decrease anymore with any single replacement Sj±{i}, with Sj ∈ S . Practically, the early stopping rule λj ≤ λstop can be adopted, where j denotes the unexplored subset having the least cardinality. This rule ensures that all candidate subsets Sj corresponding to the interval (λstop,+∞) have been explored. Similar to CSBR, alternative stopping conditions of the form |Sj | ≥ kstop or E(Sj) ≤ εstop can be adopted.\nMarch 19, 2015 DRAFT"
    }, {
      "heading" : "D. Fast implementation",
      "text" : "concave polygon S and a line Snew. Lemma 1 states that this intersection is empty in two simple situations. Hence, the call to intersect is not needed in these situations. This implementation detail is\nomitted in Tab. III for brevity reasons.\nLemma 1 Let S = {Sj , j = 0, . . . , J} be a list of supports associated to a continuous, concave polygon λ 7→ minj Ĵ (Sj ;λ) with J + 1 edges, delimited by λ = {λ0, . . . , λJ+1}. The following properties hold for all j:\n• If δEadd(Sj) < λj+1, then the line Sadd = Sj + {ℓadd} lays above the current concave polygon. • If δErmv(Sj) > λj , then the line Srmv = Sj − {ℓrmv} lays above the current concave polygon.\nMarch 19, 2015 DRAFT\nProof: We give a sketch of proof using geometrical arguments. Firstly, δEadd(Sj) is the λ-value of\nthe intersection point between lines Sj and Snew = Sj + {ℓadd}; see Fig. 7(b). Secondly, we notice that |Sj | ≤ |Sadd| ≤ |Sj+1| because the concave polygon is concave and |Sadd| = |Sj| + 1. It follows from these two facts that if δEadd(Sj) < λj+1, the line Sadd lays above Sj+1 for λ ≤ λj+1, and above Sj for λ ≥ λj+1.\nThis proves the first result. A similar sketch applies to the second result."
    }, {
      "heading" : "E. Main differences between CSBR and ℓ0-PD",
      "text" : "First, we stress that contrary to CSBR, the index j in λj does not identify with the iteration number anymore for ℓ0-PD. Actually, the current iteration of ℓ0-PD is related to an edge of the concave polygon, i.e., a whole interval (λj+1, λj), whereas the current iteration of CSBR is dedicated to a single value λj which is decreasing when the iteration number j increases.\nSecond, the computation of the next value λj+1 ≤ λj in CSBR is only based on the violation of the lower bound of (9), corresponding to atom selections. In ℓ0-PD, the upper bound is considered as well. This is the reason why the λ-values are not scanned in a decreasing order anymore. This may improve\nthe very sparse solutions found in the early iterations within an increased computation time, as we will\nsee hereafter."
    }, {
      "heading" : "V. NUMERICAL RESULTS",
      "text" : "The algorithms are evaluated on two kinds of problems involving ill-conditioned dictionaries. The be-\nhavior of CSBR and ℓ0-PD is first analyzed for simple examples. Then, we provide a detailed comparison with other nonconvex algorithms for many scenarii."
    }, {
      "heading" : "A. Two generic problems",
      "text" : "The sparse deconvolution problem takes the form y = h ∗ x⋆ + n where the impulse response h is\na Gaussian filter of standard deviation σ, and the noise n is assumed i.i.d. and Gaussian. The problem rereads y = Ax⋆+n where A is a convolution matrix. In the default setting, y and x are sampled at the\nsame frequency. h is approximated by a finite impulse response of length 6σ by thresholding the smallest values. A is a Toeplitz matrix of dimensions chosen so that any Gaussian feature h∗x⋆ if fully contained\nwithin the observation window {1, . . . ,m}. This implies that A is slightly undercomplete: m > n with m ≈ n. Two simulated data vectors y are represented in Fig. 8(a,b) where x⋆ are k-sparse vectors with\nMarch 19, 2015 DRAFT\nk = 10 and 30, and the signal-to-noise ratio (SNR) is equal to 25 and 10 dB, respectively. It is defined by SNR = 10 log(‖Ax⋆‖22/(mσ 2 n)) where σ 2 n is the variance of the noise process n.\nThe jump detection problem is illustrated on Fig. 8(c,d). Here, A is the squared dictionary (m = n) defined by Ai,j = 1 if i ≥ j, and 0 otherwise. The atom aj codes for a jump at location j, and x⋆j matches the height of the jump. When x⋆ is k-sparse, Ax⋆ yields a piecewise constant signal with k pieces, x⋆ being the first-order derivative of the signal Ax⋆.\nBoth generic problems involve either square or slightly undercomplete dictionaries. The case of over-\ncomplete dictionaries will be discussed as well, e.g., by considering the deconvolution problem with\nundersampled observations y. The generic problems are already difficult because neighboring columns of\nA are highly correlated, and a number of fast algorithms that are efficient for well-conditioned dictionaries may fail to recover the support of x⋆. The degree of difficulty of the deconvolution problem is controlled\nby the width σ of the Gaussian impulse response and the sparsity k: for large values of k and/or σ, the Gaussian features resulting from the convolution h∗x⋆ strongly overlap. For the jump detection problem,\nMarch 19, 2015 DRAFT\nall the step signals related to the atoms aj have overlapping supports."
    }, {
      "heading" : "B. Empirical behavior of CSBR and ℓ0-PD",
      "text" : "1) Example: Consider the problem shown on Fig. 8(c). Because CSBR and ℓ0-PD provide very similar results, we only show the CSBR results. CSBR delivers sparse solutions xj for decreasing λj , xj being the least-square solution supported by the j-th output of CSBR (Sj). Three sparse solutions xj are represented on Fig. 9. For the first solution (lowest value of |Sj |, largest λj), only the seven main jumps are being detected (Fig. 9(a)). The cardinality of Sj increases with j, and some other jumps are obtained together with possible false detections (Figs. 9(b,c)).\n2) Model order selection: It may often be useful to select a single solution xj . The proposed algorithms are compatible with most classical methods of model order selection [46], [47] because they are greedy\nalgorithms. Assuming that the variance of the observation noise is unknown, we distinguish two categories\nof cost functions for estimation of the order ‖xj‖0 = |Sj |. The first take the form minj{m log E(Sj) + α|Sj |} where α equals 2, logm, and 2 log logm for the Akaike, Minimum Description Length (MDL) and Hannan and Quinn criteria, respectively [46]. The second are cross-validation criteria [48], [49]. The\nsparse approximation framework allows one to derive simplified expressions of the latter up to the storage\nof intermediate solutions of greedy algorithms for consecutive cardinalities [8], [47], [50].\nFor the sparse deconvolution and jump detection problems, we found that the Akaike and cross\nvalidation criteria severely over-estimate the expected number of spikes. On the contrary, the MDL\nMarch 19, 2015 DRAFT\ncriterion yields quite accurate results. We found that the modified MDLc version dedicated to short data\nrecords (i.e., when the number of observations is moderately larger than the model order) [51] yields the\nbest results for all the scenarii we have tested. It reads:\nMarch 19, 2015 DRAFT\nfor noisy data, the spikes of smallest amplitudes are drowned in the noise. One cannot expect to detect\nthem.\n3) Further empirical observations: Fig. 11 is a typical display of the approximate ℓ0-curves yielded by CSBR and ℓ0-PD. The ℓ0-PD curve is structurally continuous and concave whereas for the CSBR curve, there are two kinds of breakpoints depicted with black and white circles. The former are “continuous”\nbreakpoints. This occurs when no single replacement is done during the call to SBR (SBR(Sinit;λj) returns Sj = Sinit; see Tab. II). Otherwise, a discontinuity breakpoint (white circle) appears. In Fig. 11, the CSBR and ℓ0-PD curves almost coincide for large λ’s, where only continuous breakpoints can be observed. For low λ’s, the ℓ0-PD curve lays below the CSBR curve, and discontinuity breakpoints appear in the latter curve.\nFig. 12 provides some insight on the CSBR and ℓ0-PD iterations for a sparse deconvolution problem with ‖x⋆‖0 = 17 and SNR = 20 dB. In the CSBR subfigures, the horizontal axis represents the number of single replacements: 60 replacements are being performed from the initial empty support during the\nsuccessive calls to SBR. For ℓ0-PD, the horizontal axis shows the iteration number. At most two new supports are being included in the list of candidate subsets at each iteration. The number of effective\nsingle replacements is therefore increased by 0, 1 or 2. During the first 25 iterations, ℓ0-PD mainly operates atom selections similar to CSBR. The explored subsets are thus of increasing cardinality and\nλ is decreasing (Figs. 12(c,d)). From iterations 25 to 40, the very sparse solutions previously found\n(k ≤ 20) are improved as a series of atom de-selections is performed. They are being improved again\naround iteration 80. On the contrary, the sparsest solutions are never improved with CSBR, which works\nfor decreasing λ’s (Figs. 12(a,b)). For ℓ0-PD, the early stopping parameter λstop may have a strong influence on the improvement of the sparsest solutions and the overall computation time. This point will\nbe further discussed below."
    }, {
      "heading" : "C. Extensive comparisons",
      "text" : "The proposed algorithms are compared with popular nonconvex algorithms for both problems intro-\nduced in subsection V-A with various parameter settings: problem dimension (m,n), ratio m/n, signal-tonoise ratio, cardinality of x⋆, and width σ of the Gaussian impulse response for the deconvolution problem.\nThe settings are listed on Table V for 10 scenarii. Because the proposed algorithms are orthogonal greedy\nalgorithms, they are better suited to problems in which the level of sparsity is moderate to high. We therefore restrict ourselves to the case where k = ‖x⋆‖0 ≤ 30.\nMarch 19, 2015 DRAFT\n1) Competing algorithms: We focus on the comparison with algorithms based on nonconvex penalties.\nIt is indeed increasingly acknowledged that the BPDN estimates are less accurate than sparse approxima-\ntion estimates based on nonconvex penalties. We do not consider forward greedy algorithms either; we\nalready showed that SBR is (unsurprisingly) more efficient than the simpler OMP and OLS algorithms [3].\nAmong the popular nonconvex algorithms, we consider:\n1) Iterative Reweighted Least Squares (IRLS) for ℓq minimization, q < 1 [52]; 2) Iterative Reweighted ℓ1 (IRℓ1) coupled with the penalty log(|xi|+ ε) [20], [23], [53];\nMarch 19, 2015 DRAFT\nWe resort to a penalized least-square implementation for all algorithms, the only algorithm directly\nworking with the ℓ0 penalty being L0LS-CD. We do not consider simpler thresholding algorithms (Iterative Hard Thresholding, CoSaMP, Subspace Pursuit) proposed in the context of compressive sensing since we\nfound that SBR behaves much better than these algorithms for ill-conditioned dictionaries [3]. We found\nthat L0LS-CD is more efficient than thresholding algorithms. Moreover, the cyclic descent approach is\nbecoming very popular in the recent sparse approximation literature [44], [56] although its speed of\nconvergence is sensitive to the quality of the initial solution. Here, we use the BPDN initial solution argmin x {‖y−Ax‖22+µ‖x‖1} where µ is set to half of the maximum tested λ-value (more details will be given hereafter). This simple ad hoc setting allows us to get a rough initial solution that is nonzero\nand very sparse within a fast computation time.\nThe three other considered algorithms work with sparsity measures depending on an arbitrary parameter.\nRegarding IRLS, we set q = 0.5 or 0.1 as suggested in [52]. We chose to run IRLS twice, with q = 0.5\nand then q = 0.1 (with the previous output at q = 0.5 as initial solution) so that IRLS is less sensitive to\nlocal solutions at q = 0.1. SL0 is a GNC-like algorithm working for increasingly non-convex penalties\nMarch 19, 2015 DRAFT\n(i.e., Gaussian functions of decreasing widths). For simplicity reasons, we set the lowest width relative to the knowledge of the smallest nonzero amplitude of the ground truth solution x⋆. The basic SL0\nimplementation is dedicated to noise-free problems [43]. There exist several adaptations in the noisy\nsetting [55], [57] including the precursory work [58]. We chose the efficient implementation of [57] in\nwhich the original pseudo-inverse calculations are replaced by a quasi-Newton strategy using limited\nmemory BFGS updates. Finally, the IRℓ1 implementation depends on both the choice of parameter ε (which controls the degree of nonconvexity) and the ℓ1 solver. We have tested two ℓ1 solvers: the incrowd algorithm [59] together with an empirical setting of ε > 0, and ℓ1 homotopy in the limit case ε → 0, following [53]. We found that ℓ1 homotopy is faster than in-crowd, mainly because the Matlab implementation of in-crowd (provided by the authors) makes calls to the quadprog built-in function,\nwhich is computationally expensive for large dimension problems.\n2) Numerical protocol: Because the competing algorithms work for a single λ value, we need to define a grid, denoted by {λGi , i = 1, . . . , Nλ}, for comparison purposes. Such grid is defined in logscale for each of the 10 scenarii (k,A,SNR) defined in Table V. The number of grid points is Nλ = 11. For a given scenario, T = 30 trials are being performed in which k-sparse vectors x⋆(t) and noise vector n(t) are randomly drawn. This leads us to simulate T observation vectors y(t) = Ax⋆(t) + n(t) with t ∈ {1, . . . , T}. Specifically, the location of the nonzero amplitudes in x⋆(t) are uniformly distributed and\nthe amplitude values are drawn according to an i.i.d. Gaussian distribution. For each trial t, all competing algorithms need to be run Nλ times with y(t) and λGi as inputs whereas CSBR and ℓ0-PD are run only once since they deliver estimates for a continuum of values of λ. Their solution for each λGi directly deduces from their set of output supports and the knowledge of both breakpoints surrounding λGi .\nThe algorithms are first evaluated in the optimization viewpoint: the related criteria are their capacity to\nreach a low value of J (x;λ) and the corresponding CPU time. In this viewpoint, the proposed methods\nmight be somehow favored since they are more directly designed with the criterion J (x;λ) in mind.\nOn the other hand, J (x;λ) appears to be a natural indicator because solving either ℓ0-minimization problem (1), (2) or (3) is the ultimate goal of any sparse approximation method. As detailed below,\nsome post-processing will be applied to the outputs of algorithms that do not rely on the ℓ0-norm so that they are not strongly disadvantaged. Practically, we store the value of J (x;λGi ) found for each trial and each λGi . Averaging this value over the trials t yields a table TabJ(a, λ G i ) where a denotes a candidate algorithm. Similarly, the CPU time is averaged over the trials t, leading to another table TabCPU(a, λGi ). Each table is represented separately as a 2D plot with a specific color for each algorithm: see, e.g.,\nFig. 13. CSBR and ℓ0-PD are represented with continuous curves because J (x;λ) is computed for a\nMarch 19, 2015 DRAFT\nminx J (x;λ) CPU Time (seconds) minx J (x;λ) CPU Time (seconds)\ncontinuum of λ’s, and the CPU time is computed only once.\nThe algorithms are also evaluated in terms of support recovery accuracy. For this purpose, let us first\ndefine the “support error” as the minimum over i of the distance\n|S⋆(t)\\S(t, a, λGi )|+ |S(t, a, λ G i )\\S ⋆(t)| (16)\nbetween the support S⋆(t) of the unknown sparse vector x⋆(t) and the support S(t, a, λGi ) of the sparse reconstruction at λGi with algorithm a. (16) takes into account both numbers of false negatives |S⋆(t)\\S(t, a, λGi )| and of false positives |S(t, a, λ G i )\\S ⋆(t)|. Denoting by S(t, a, λGopt) ← S(t, a, λ G i ) the solution support that is the closest to S⋆(t) according to (16), we further consider the number of true positives in S(t, a, λGopt), defined as |S ⋆(t) ∩ S(t, a, λGopt)|. We will thus report:\n• the support error;\n• the corresponding number of true positives; • the corresponding model order |S(t, a, λGopt)|.\nMarch 19, 2015 DRAFT\nAveraging these measures over T trials yields the support error score SE(a), the true positive score TP(a)\nand the model order, denoted by Order(a). The numbers of false positives (FP) and of true/false negatives\ncan be directly deduced, e.g., FP(a) = Order(a) − TP(a).\nThe underlying idea in this analysis is that when SE is small (respectively, TP is high), the algorithms\nare likely to perform well provided that λ is appropriately chosen. However, in practical applications,\nonly one estimate is selected using a suitable model selection criterion. We therefore provide additional\nevaluations of the MDLc estimate accuracy. For CSBR and ℓ0-PD, all output supports are considered to compute the MDLc estimate as described in subsection V-B. For other algorithms, it is equal to one of the sparse reconstructions obtained at λGi for i ∈ {1, . . . , Nλ}. The same three measures as above are computed for the MDLc estimate and averaged over T trials. They are denoted by MDLc-SE(a),\nMDLc-TP(a) and MDLc-Order(a).\n3) Technical adaptations for comparison purposes: Because IRLS and SL0 do not deliver sparse\nvectors in the strict sense, it is necessary to sparsify their outputs before computing their SE(a) score.\nThis is done by running one iteration of cyclic descent (L0LS-CD): most small nonzero amplitudes are\nthen thresholded to 0. Regarding the values of J (x;λ), a post-processing is performed for algorithms\nthat do not rely on the ℓ0-norm. This post-processing can be interpreted as a local descent of J (x;λ). It consists in: (i) running one iteration of cyclic descent (L0LS-CD); (ii) computing the squared error\nrelated to the output support. L0LS-CD is indeed a local descent algorithm dedicated to J (x;λ) but the\nconvergence towards a least-square minimizer is not reached in one iteration.\n4) Analysis in the optimization viewpoint: CSBR and ℓ0-PD are always among the most accurate to minimize the cost function, as illustrated on Figs. 13, 14 and 15. We can clearly distinguish two\ngroups of algorithms on these figures: IRLS, L0LS-CD and SL0 one the one hand, and the OLS-based\nalgorithms (SBR, CSBR, ℓ0-PD) and IRℓ1 on the other hand, which are the most accurate. We cannot clearly discriminate the accuracy of SBR and CSBR: one may behave slightly better than the other\ndepending on the scenarii. On the contrary, SBR and CSBR are often outperformed by ℓ0-PD. The obvious advantage of CSBR and ℓ0-PD over SBR and IRℓ1 is that they are ℓ0-homotopy algorithms, i.e., a set of solutions are delivered for many sparsity levels, and the corresponding λ-values are adaptively\nfound. On the contrary, the SBR output is related to a single λ whose tuning may be tricky. Another\nadvantage over IRℓ1 is that the structure of forward-backward algorithms is simpler, as no call to any ℓ1 solver is required. Moreover, the number of parameters to tune is lower: there is a single (early) stopping\nparameter λstop.\nThe price to pay for a better performance is an increase of the computation burden. On Figs. 13, 14\nMarch 19, 2015 DRAFT\nminx J (x;λ) CPU Time (seconds) minx J (x;λ) CPU Time (seconds)\nand 15, two lines are drawn for CSBR (respectively, for ℓ0-PD). They are horizontal because the algorithm is run only once per trial, so there is a single computation time measurement. The first line corresponds\nto the overall computation time, i.e., from the start to the termination of CSBR / ℓ0-PD. This time is often more expensive than for other algorithms. However, the latter times refer to a single execution for some λGi value. If one wants to recover sparse solutions for many λ G i ’s, they must be cumulated. This is the reason why we have drawn a second line for CSBR and ℓ0-PD corresponding to a normalization (by Nλ = 11) of the overall computation time. In this viewpoint, the CPU time of CSBR and ℓ0-PD are very reasonable.\nThe computation time depends on many factors among which the implementation of algorithms\n(including the memory storage) and the chosen stopping rules. We have followed an homogeneous\nimplementation of algorithms to make the CPU time comparisons meaningful. We have defined two\nsets of stopping rules depending on the problem dimension. The default parameters apply to medium\nsize problems (m = 300). They are relaxed for problems of larger dimension (m > 500) to avoid huge computational costs. The stopping rule of CSBR and ℓ0-PD is always λ ≤ λstop = αλG1 with α = 1\nMarch 19, 2015 DRAFT\nfor CSBR and 0.5 (medium size) or 0.8 (large size) for ℓ0-PD. For L0LS-CD, the maximum number of cyclic descents (update of every amplitude xi) is set to 60 or 10 depending on the dimension. For SL0, we have followed the default setting of [43] for the rate of deformation of the nonconvex penalty.\nThe number of BFGS iterations done in the local minimization steps for each penalty is set to L = 40\nor 5. It is set to 5L for the last penalty which is the most nonconvex. Regarding IRLS and IRLℓ1, we keep the same settings whatever the dimension since the computation times remain reasonable for large\ndimensions. Finally, SBR does not require any arbitrary stopping rule. The problems of large dimensions\ncorrespond to scenarii C and D. We observe on Fig. 13 that the comparison (trade-off performance vs\ncomputation time) is now clearly in favor of CSBR and ℓ0-PD. IRℓ1 remains very competitive although\nMarch 19, 2015 DRAFT\nthe average numerical cost becomes larger.\n5) Analysis in the support recovery viewpoint: The support recovery performance is only shown for\nthe scenarii E to J (Tabs. VI and VII). For noisy deconvolution problems, these results are omitted\nbecause the support error is often quite large and the true positive scores are low whatever the algorithm,\nespecially for scenarii B to D. Specifically, the least support error always exceeds 20, 10, 10 and 32 for\nthe scenarii A to D (k = 30, 10, 10 and 30, respectively). For such difficult problems, one can hardly\ndiscriminate algorithms based on simple binary tests such as the true positive rate. More sophisticated\nlocalization tests are non binary and would take into account the distance between the location of the true\nspikes and their wrong estimates [60]. It is noticeable, though, that the MDLc estimator delivers subsets\nof realistic cardinality for scenarii A to D (e.g., the subsets found with CSBR are of cardinalities 33,\n9, 15 and 38, the true cardinalities being 30, 10, 10 and 30). The model orders are also quite accurate\nfor the noisy jump detection problem (Tab. VI) whereas the true support is often partially detected by\nseveral of the considered algorithms. Here, CSBR and ℓ0-PD are among the best algorithms in terms of\nMarch 19, 2015 DRAFT\nsupport error.\nThe results of Tab. VII and Fig. 15 correspond to the deconvolution problem in noise-free case. The\ndata y are undersampled so that the dictionary A is overcomplete. The undersampling rate ∆ ≈ m/n is\nset to 2 in scenarii H and I and 4 in scenario J. Again, CSBR and ℓ0-PD are among the best (SE, TP, MDLc-order) especially for the most difficult problem J.\n6) Overcomplete dictionaries with noise: We now provide arguments indicating that the proposed\nalgorithms are competitive as well for noisy problems with overcomplete dictionaries. The detailed\nexperiments commented below are not reported for space reasons.\nWe have first considered the noisy deconvolution problem with ∆ = 2 or 4 leading to overcomplete\ndictionaries, the other parameters being set as in scenarii A to D. Although the data approximation is\nqualitatively good for CSBR and ℓ0-PD, the SE and TP scores are very weak. It is hard to discriminate the\nMarch 19, 2015 DRAFT\nperformance of algorithms because these measures are very weak for all considered algorithms. Moreover,\nthe values of J (λ) found for most algorithms are often similar.\nWe have also considered an adaptive spline approximation problem generalizing the jump detection\nproblem to the approximation of a signal using piecewise polynomials of degree P = 1 or 2 [3]. The jump\ndetection problem can indeed be thought of as the approximation with a piecewise constant signal (P = 0).\nThe generalized version [3] is inspired from the regression spline modeling in [61]. Now, the dictionary\natoms are related to the detection of the locations of jumps, changes of slopes and changes of curvatures in the signal y (subdictionaries A0, A1 and A2). The dictionary then takes the form A ← [A0,A1] or A ← [A0,A1,A2] where each sub-dictionary Ap (p ≤ P ) is formed of shifted versions of the one-sided power function i 7→ [max(i, 0)]p. The size of the full dictionary A is approximately m × (P + 1)m.\nHence, it becomes overcomplete as soon as P ≥ 1. We have shown [3] that SBR is competitive when\nP = 1 or 2. We have carried out new tests confirming that CSBR and ℓ0-PD are more efficient than their competitors in terms of values of J (λ). However, the rate of true positives is low for P ≥ 1 since the\nlocation of the change of slopes and of curvatures can hardly be exactly recovered from noisy data."
    }, {
      "heading" : "VI. SOFTWARE",
      "text" : "The Matlab implementation of the proposed CSBR and ℓ0-PD algorithms is available at\nwww.cran.univ-lorraine.fr/perso/charles.soussen/software.html including programs showing how to call\nthese functions."
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "The choice of a relevant sparse approximation algorithm relies on a trade-off between the desired\nperformance and the computation time one is ready to spend. The proposed algorithms are relatively\nexpensive but very well suited to inverse problems inducing highly correlated dictionaries. A reason is that they have the capacity to escape from local minimizers of J (x;λ) = ‖y−Ax‖22 +λ‖x‖0 [3]. This behavior is in contrast with other classical sparse algorithms.\nWe have shown the usefulness and efficiency of the two SBR extensions when the level of sparsity\nis moderate to high, i.e., k/min(m,n) is lower than 0.1. They remain competitive when k/min(m,n)\nranges between 0.1 and 0.2, and their performance gradually degrade for weaker levels of sparsity, which\nis an expected behavior for such greedy type algorithms. For a single λ, CSBR is as efficient as SBR,\nand ℓ0-PD improves the SBR and CSBR performance within a larger computation cost. The main benefit over SBR is that sparse solutions are provided for a continuum of λ-values, enabling the utilization of\nMarch 19, 2015 DRAFT\nany classical order selection method. We found that the MDL criterion yields very accurate estimates of\nthe cardinality ‖x‖0.\nOur perspectives include the proposal of forward-backward search algorithms that will be faster than\nSBR and potentially more efficient. In the standard version of SBR, CSBR and ℓ0-PD, a single replacement refers to the insertion or removal of a dictionary element. The cost of an iteration is essentially related to\nthe n linear system resolutions done to test single replacements for all dictionary atoms. The proposed\nalgorithms obviously remain valid when working with a larger neighborhood, e.g., when testing the\nreplacement of two atoms simultaneously, but their complexity becomes huge. To avoid such numerical\nexplosion, one may rather choose not to carry out all the replacement tests, but only some tests that are\nlikely to be effective. Extensions of OMP and OLS were recently proposed in this spirit [36] and deserve\nconsideration for proposing efficient forward-backward algorithms."
    }, {
      "heading" : "APPENDIX A",
      "text" : "PROPERTIES OF THE ℓ0 REGULARIZATION PATHS\nIn this appendix, we prove that the ℓ0-penalized path S⋆P (see Definition 2) is piecewise constant (Theorem 1) and is a subset of the ℓ0-constrained regularization path S⋆C (Theorem 2). We will denote the ℓ0-curve by λ 7→ J ⋆(λ) = minS{Ĵ (S;λ)}. Let us recall that this function is concave and affine on each interval (λ⋆i+1, λ ⋆ i ), with i ∈ {0, . . . , I} (Definition 1). Moreover, λ ⋆ I+1 = 0 and λ ⋆ 0 = +∞."
    }, {
      "heading" : "A. Proof of Theorem 1",
      "text" : "We prove Theorem 1 together with the following lemma, which is informative about the content of\nS⋆P(λ) for the breakpoints λ = λ ⋆ i .\nLemma 2 Let i ∈ {1, . . . , I − 1}. Then, for all λ ∈ (λ⋆i+1, λ ⋆ i ), S ⋆ P(λ) ⊂ S ⋆ P(λ ⋆ i+1) ∩ S ⋆ P(λ ⋆ i ).\nFor the first and last intervals, we have:\n• For all λ ∈ (0, λ⋆I), S ⋆ P(λ) ⊂ S ⋆ P(λ ⋆ I). • For all λ ∈ (λ⋆1,+∞), S ⋆ P(λ) = {∅} ⊂ S ⋆ P(λ ⋆ 1).\nProof of Theorem 1: By definition, the ℓ0-curve is the concave envelope of the (finite) set of lines\nS for all possible subsets S. Because it is affine on the i-th interval (λ⋆i+1, λ ⋆ i ), J ⋆(λ) coincides with Ĵ (Si;λ) = E(Si) + λ|Si|, where Si is some optimal subset for all λ ∈ (λ⋆i+1, λ ⋆ i ).\nLet λ ∈ (λ⋆i+1, λ ⋆ i ) and S ∈ S ⋆ P(λ). Then, Ĵ (S;λ) = Ĵ (Si;λ). It follows that both lines S and Si necessarily coincide; otherwise, they would intersect at λ, and line S would lay below Si on either\nMarch 19, 2015 DRAFT\ninterval (λ⋆i+1, λ) or (λ, λ ⋆ i ), which contradicts the definition of Si. We conclude that S ∈ S ⋆ P(λ ′) for all λ′ ∈ (λ⋆i+1, λ ⋆ i ).\nWe have shown that the content of S⋆P(λ) does not depend on λ when λ ∈ (λ ⋆ i+1, λ ⋆ i ).\nProof of Lemma 2: The first result S⋆P(λ) ⊂ S ⋆ P(λ ⋆ i+1) ∩ S ⋆ P(λ ⋆ i ) is obtained by slightly adapting\nthe proof of Theorem 1: replace (λ⋆i+1, λ ⋆ i ) by the closed interval [λ ⋆ i+1, λ ⋆ i ], and set λ ′ to both endpoints of this interval.\nThe second and third results are obtained similarly, by considering the intervals (0, λ⋆I ] and [λ ⋆ 1,+∞),\nand setting λ′ ← λ⋆I and λ ′ ← λ⋆1, respectively. It is obvious that S ⋆ P(λ) reduces to the empty support for λ > λ⋆1 since the ℓ0-curve is constant for λ > λ ⋆ 1."
    }, {
      "heading" : "B. Proof of Theorem 2",
      "text" : "The first result is straightforward: for any λ and for S ∈ S⋆P(λ), we have S ∈ S ⋆ C(|S|). Otherwise, there would exist S′ with |S′| ≤ |S| and E(S′) < E(S). Then, Ĵ (S′;λ) < Ĵ (S;λ) would contradict S ∈ S⋆P(λ).\nTo prove the second result, let us first show that for any i, ∃ki : ∀λ ∈ (λ⋆i+1, λ ⋆ i ), S ⋆ P(λ) ⊂ S ⋆ C(ki). Let S ∈ S⋆P(λ) for some λ ∈ (λ ⋆ i+1, λ ⋆ i ). Theorem 1 implies that S ∈ S ⋆ P(λ) for any λ ∈ (λ ⋆ i+1, λ ⋆ i ). Therefore, J ⋆(λ) = Ĵ (S;λ) for λ ∈ (λ⋆i+1, λ ⋆ i ), and the slope of line S, i.e., |S|, is constant whatever S ∈ S⋆P(λ) and λ ∈ (λ ⋆ i+1, λ ⋆ i ). Let us denote this constant by ki = |S|. According to the first paragraph of the proof, S ∈ S⋆P(λ) implies that S ∈ S ⋆ C(ki).\nLet us prove the reverse inclusion S⋆C(ki) ⊂ S ⋆ P(λ). Let λ ∈ (λ ⋆ i+1, λ ⋆ i ) and S ∈ S ⋆ C(ki). First, we have |S| ≤ ki. Second, for any S′ ∈ S⋆P(λ), we have |S ′| = ki by definition of ki. We also have that E(S′) = E(S) because S⋆P(λ) ⊂ S ⋆ C(ki). Finally, Ĵ (S ′;λ) ≥ Ĵ (S;λ). S′ ∈ S⋆P(λ) implies that S ∈ S⋆P(λ). This concludes the proof of the second result."
    } ],
    "references" : [ {
      "title" : "Sparse approximate solutions to linear systems",
      "author" : [ "B.K. Natarajan" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1995
    }, {
      "title" : "Description of the minimizers of least squares regularized with l0 norm. Uniqueness of the global minimizer",
      "author" : [ "M. Nikolova" ],
      "venue" : "SIAM J. Imaging Sci.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Computational methods for sparse solution of linear inverse problems”, Proc. IEEE, invited paper (Special Issue “Applications of sparse representation and compressive sensing”)",
      "author" : [ "J.A. Tropp", "S.J. Wright" ],
      "venue" : "vol. 98,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition",
      "author" : [ "Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad" ],
      "venue" : "in Proc. 27th Asilomar Conf. on Signals, Systems and Computers,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1993
    }, {
      "title" : "Orthogonal least squares methods and their application to non-linear system identification",
      "author" : [ "S. Chen", "S.A. Billings", "W. Luo" ],
      "venue" : "Int. J. Control,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1989
    }, {
      "title" : "Subset selection in regression",
      "author" : [ "A.J. Miller" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    }, {
      "title" : "Forward sequential algorithms for best basis selection",
      "author" : [ "S.F. Cotter", "J. Adler", "B.D. Rao", "K. Kreutz-Delgado" ],
      "venue" : "IEE Proc. Vision, Image and Signal Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Optimized orthogonal matching pursuit approach",
      "author" : [ "L. Rebollo-Neira", "D. Lowe" ],
      "venue" : "IEEE Signal Process. Lett., vol. 9,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "Iterative thresholding for sparse approximations",
      "author" : [ "T. Blumensath", "M.E. Davies" ],
      "venue" : "J. Fourier Anal. Appl.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Subspace pursuit for compressive sensing signal reconstruction",
      "author" : [ "W. Dai", "O. Milenkovic" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples",
      "author" : [ "D. Needell", "J.A. Tropp" ],
      "venue" : "Appl. Comp. Harmonic Anal., vol. 26,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Multiple regression analysis",
      "author" : [ "M.A. Efroymson" ],
      "venue" : "Mathematical Methods for Digital Computers, A. Ralston and H. S. Wilf, Eds., vol. 1, pp. 191–203. Wiley, New York",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1960
    }, {
      "title" : "Forward and backward stepping in variable selection",
      "author" : [ "K.N. Berk" ],
      "venue" : "J. Statist. Comput. Simul.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1980
    }, {
      "title" : "Adaptive forward-backward greedy algorithm for learning sparse representations",
      "author" : [ "T. Zhang" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems",
      "author" : [ "M.A.T. Figueiredo", "R.D. Nowak", "S.J. Wright" ],
      "venue" : "IEEE J. Sel. Top. Signal Process.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "l1 − l2 optimization in signal and image processing",
      "author" : [ "M. Zibulevsky", "M. Elad" ],
      "venue" : "IEEE Sig. Proc. Mag., vol. 27,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "Minimizing nonconvex functions for sparse vector reconstruction",
      "author" : [ "N. Mourad", "J.P. Reilly" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Iterative reweighted l1 and l2 methods for finding sparse solutions",
      "author" : [ "D.P. Wipf", "S. Nagarajan" ],
      "venue" : "IEEE J. Sel. Top. Signal Process. (Special issue on Compressive Sensing),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "A general framework for sparsity-based denoising and inversion",
      "author" : [ "A. Gholami", "S.M. Hosseini" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Universal regularizers for robust sparse coding and modeling",
      "author" : [ "I. Ramı́rez", "G. Sapiro" ],
      "venue" : "IEEE Trans. Image Process.,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Sparse signal recovery by difference of convex functions algorithms",
      "author" : [ "H.A. Le Thi", "B.T. Nguyen Thi", "H.M. Le" ],
      "venue" : "Intelligent Information and Database Systems, A. Selamat, N. T. Nguyen, and H. Haron, Eds., Berlin",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sparse signal estimation by maximally sparse convex optimization",
      "author" : [ "I. Selesnick", "I. Bayram" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "A new approach to variable selection in least squares problems",
      "author" : [ "M.R. Osborne", "B. Presnell", "B.A. Turlach" ],
      "venue" : "IMA Journal of Numerical Analysis, vol. 20, no. 3, pp. 389–403",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani" ],
      "venue" : "Ann. Statist., vol. 32,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2004
    }, {
      "title" : "A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems",
      "author" : [ "I. Das", "J.E. Dennis" ],
      "venue" : "Structural optimization,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2007
    }, {
      "title" : "Survey of multi-objective optimization methods for engineering",
      "author" : [ "R.T. Marler", "J.S. Arora" ],
      "venue" : "Structural and Multidisciplinary Optimization,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2004
    }, {
      "title" : "Probing the Pareto frontier for basis pursuit solutions",
      "author" : [ "E. van den Berg", "M.P. Friedlander" ],
      "venue" : "SIAM J. Sci. Comput.,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2008
    }, {
      "title" : "Subset regression with stepwise directed search",
      "author" : [ "P.M.T. Broersen" ],
      "venue" : "J. R. Statist. Soc. C, vol. 35, no. 2, pp. 168–177",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "A bidirectional greedy heuristic for the subspace selection problem”, in Engineering stochastic local search algorithms. Designing, implementing and analyzing effective heuristics",
      "author" : [ "D. Haugland" ],
      "venue" : "vol. 4638 of Lect. Notes Comput. Sci.,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2007
    }, {
      "title" : "Projection-based and look-ahead strategies for atom selection",
      "author" : [ "S. Chatterjee", "D. Sundman", "M. Vehkaperä", "M. Skoglund" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2012
    }, {
      "title" : "A continuation approach to estimate a solution path of mixed L2-L0 minimization problems”, in Signal Processing with Adaptive Sparse Structured Representations (SPARS workshop)",
      "author" : [ "J. Duan", "C. Soussen", "D. Brie", "J. Idier" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    }, {
      "title" : "Multipath matching pursuit",
      "author" : [ "S. Kwon", "J. Wang", "B. Shim" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2014
    }, {
      "title" : "The Viterbi algorithm for subset selection",
      "author" : [ "S. Maymon", "Y. Eldar" ],
      "venue" : "IEEE Signal Process. Lett., vol. 22,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2015
    }, {
      "title" : "Numerical solutions by the continuation method",
      "author" : [ "E. Wasserstrom" ],
      "venue" : "SIAM Rev., vol. 15,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 1973
    }, {
      "title" : "Homotopy continuation for sparse signal representation",
      "author" : [ "D.M. Malioutov", "M. Çetin", "A.S. Willsky" ],
      "venue" : "in Proc. IEEE ICASSP,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2005
    }, {
      "title" : "Highly undersampled magnetic resonance image reconstruction via homotopic  l0minimization",
      "author" : [ "J. Trzasko", "A. Manduca" ],
      "venue" : "IEEE Trans. Medical Imaging,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2009
    }, {
      "title" : "A fast approach for overcomplete sparse decomposition based on smoothed l norm",
      "author" : [ "G.H. Mohimani", "M. Babaie-Zadeh", "C. Jutten" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2009
    }, {
      "title" : "SparseNet: Coordinate descent with nonconvex penalties",
      "author" : [ "R. Mazumder", "J.H. Friedman", "T. Hastie" ],
      "venue" : "J. Acoust. Soc. Amer.,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2011
    }, {
      "title" : "Practical approximate solutions to linear operator equations when the data are noisy",
      "author" : [ "G. Wahba" ],
      "venue" : "SIAM J. Num. Anal., vol. 14, no. 4, pp. 651–667",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Generalized cross-validation as a method for choosing a good ridge parameter",
      "author" : [ "G.H. Golub", "M. Heath", "G. Wahba" ],
      "venue" : "Technometrics, vol. 21,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1979
    }, {
      "title" : "On the relation between sparse reconstruction and parameter estimation with model order selection",
      "author" : [ "C.D. Austin", "R.L. Moses", "J.N. Ash", "E. Ertin" ],
      "venue" : "IEEE J. Sel. Top. Signal Process.,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2010
    }, {
      "title" : "Improved iteratively reweighted least squares for unconstrained smoothed lq minimization",
      "author" : [ "M.-J. Lai", "Y. Xu", "W. Yin" ],
      "venue" : "SIAM J. Num. Anal.,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2013
    }, {
      "title" : "The adaptive Lasso and its oracle properties",
      "author" : [ "H. Zou" ],
      "venue" : "J. Acoust. Soc. Amer., vol. 101,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2006
    }, {
      "title" : "Sparse coloured system identification with guaranteed stability",
      "author" : [ "A.J. Seneviratne", "V. Solo" ],
      "venue" : "IEEE Conference on Decision and Control,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2012
    }, {
      "title" : "Robust-sl0 for stable sparse representation in noisy settings",
      "author" : [ "A. Eftekhari", "M. Babaie-Zadeh", "C. Jutten", "H.A. Moghaddam" ],
      "venue" : "in Proc. IEEE ICASSP,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2009
    }, {
      "title" : "lq sparsity penalized linear regression with cyclic descent",
      "author" : [ "G. Marjanovic", "V. Solo" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2014
    }, {
      "title" : "Sparse channel estimation of mimo-ofdm systems with unconstrained smoothed l0-norm-regularized least squares compressed sensing",
      "author" : [ "X. Ye", "W.-P. Zhu", "A. Zhang", "J. Yan" ],
      "venue" : "EURASIP J. Wireless Comm. and Networking,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2013
    }, {
      "title" : "Superresolution of noisy band-limited data by data adaptive regularization and its application to seismic trace inversion",
      "author" : [ "N. Saito" ],
      "venue" : "in Proc. IEEE ICASSP,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 1990
    }, {
      "title" : "A novel spike distance",
      "author" : [ "M.C. van Rossum" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2001
    }, {
      "title" : "Multivariate adaptive regression splines",
      "author" : [ "J.H. Friedman" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 1991
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "They are known to be NP-hard except for specific cases [1].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "is worth being considered, where λ expresses the trade-off between the quality of approximation and the sparsity level [2].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "At each iteration, a new atom is appended to the current subset, therefore gradually improving the quality of approximation [4].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : "Greedy algorithms include, by increasing order of complexity: Matching Pursuit (MP) [5], Orthogonal Matching Pursuit (OMP) [6], and Orthogonal Least",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 8,
      "context" : "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : ", [18], [19] and [20]–[27] for convex (l1) and nonconvex relaxation, respectively.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : ", [18], [19] and [20]–[27] for convex (l1) and nonconvex relaxation, respectively.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 21,
      "context" : ", [18], [19] and [20]–[27] for convex (l1) and nonconvex relaxation, respectively.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "It is noticeable that BPDN leads to stepwise algorithms [18], [28] including the popular l1-homotopy [28]–[30], a forward-backward greedy search whose complexity is close to that of OMP.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 23,
      "context" : "It is noticeable that BPDN leads to stepwise algorithms [18], [28] including the popular l1-homotopy [28]–[30], a forward-backward greedy search whose complexity is close to that of OMP.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "It is referred to as “LARS with the LASSO modification” in [30].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "2 is a classical bi-objective representation where each axis is related to a single objective [31], namely |S| and E(S).",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "In bi-objective optimization, a point S is called Pareto optimal when no other point S′ can decrease both objectives [32].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 25,
      "context" : "On the contrary, the non-supported solutions cannot [32].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "‖x‖1 ≤ t for all t [33].",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Now, the weighted sum formulation (3) may not yield the same solutions as the constrained formulations (1) and (2) because the l0-norm is nonconvex [2].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "Many authors actually discourage the direct optimization of J because there are a very large number of local minimizers [20], [23].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "3) Positioning with respect to other stepwise algorithms: In statistical regression, the word “stepwise” originally refers to Efroymson’s algorithm [15], proposed in 1960 as an empirical extension of forward selection (i.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : "Other stepwise algorithms were proposed in the 1980’s [8, Chapter 3] among which Berk’s and Broersen’s algorithms [16], [34].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "Other stepwise algorithms were proposed in the 1980’s [8, Chapter 3] among which Berk’s and Broersen’s algorithms [16], [34].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 28,
      "context" : "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 29,
      "context" : "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 30,
      "context" : "CSBR and l0-PD both read as descent algorithms in different senses: CSBR, first sketched in [37], repeatedly minimizes J (x;λ) for decreasing λ’s.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : "The idea of maintaining a list of support candidates was recently developed within the framework of forward selection [38], [39].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 32,
      "context" : "The idea of maintaining a list of support candidates was recently developed within the framework of forward selection [38], [39].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 31,
      "context" : "In contrast, the supports in the list are all candidate solutions to solve the same problem in [38], [39].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 32,
      "context" : "In contrast, the supports in the list are all candidate solutions to solve the same problem in [38], [39].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "tuning some continuous hyperparameter [40].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "BPDN is solved for decreasing hyperparameter values using the solution for each value as a warm starting point for the next value [4].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "l1-homotopy [28], [30], [41] exploits that the l1 regularization path is piecewise affine and tracks the breakpoints between consecutive affine pieces.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 34,
      "context" : "l1-homotopy [28], [30], [41] exploits that the l1 regularization path is piecewise affine and tracks the breakpoints between consecutive affine pieces.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 35,
      "context" : "Second, the continuous approximation of the (discrete) l0 pseudo-norm [42] using a Graduated Non Convexity (GNC) approach [43]: a series of continuous concave metrics is considered leading to the resolution of continuous optimization problems with warm start initialization.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 36,
      "context" : "Second, the continuous approximation of the (discrete) l0 pseudo-norm [42] using a Graduated Non Convexity (GNC) approach [43]: a series of continuous concave metrics is considered leading to the resolution of continuous optimization problems with warm start initialization.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 37,
      "context" : "Although the full reconstruction of the l0-regularization path has been rarely addressed, it is noticeable that a GNC-like approach, called SparseNet, aims to gradually update some estimation of the regularization path induced by increasingly non-convex sparsity measures [44].",
      "startOffset" : 272,
      "endOffset" : 276
    }, {
      "referenceID" : 26,
      "context" : "Because the influence of the grid is critical [33], it is useful to adapt the grid while the nonconvex measure is modified [44].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 37,
      "context" : "Because the influence of the grid is critical [33], it is useful to adapt the grid while the nonconvex measure is modified [44].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 23,
      "context" : "The λ-values are rather adaptively computed similar to the l1-homotopy principle [28], [30].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "In the following, the formulation (5) will be omitted because it leads to the same l0-regularization path as formulation (4) [2].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Therefore, both l0-regularization paths may not coincide [2], [31].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : "Therefore, both l0-regularization paths may not coincide [2], [31].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "SBR coincides with the well-known OLS algorithm [7].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 23,
      "context" : "Our continuation strategy is inspired by l1-homotopy which recursively computes the minimizers of ‖y−Ax‖2 +λ‖x‖1 when λ is continuously decreasing [28]–[30].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 38,
      "context" : "The second are cross-validation criteria [48], [49].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 39,
      "context" : "The second are cross-validation criteria [48], [49].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "The sparse approximation framework allows one to derive simplified expressions of the latter up to the storage of intermediate solutions of greedy algorithms for consecutive cardinalities [8], [47], [50].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 40,
      "context" : "The sparse approximation framework allows one to derive simplified expressions of the latter up to the storage of intermediate solutions of greedy algorithms for consecutive cardinalities [8], [47], [50].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 41,
      "context" : "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ ε) [20], [23], [53];",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ ε) [20], [23], [53];",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 42,
      "context" : "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ ε) [20], [23], [53];",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 43,
      "context" : "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 36,
      "context" : "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 44,
      "context" : "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 37,
      "context" : "Moreover, the cyclic descent approach is becoming very popular in the recent sparse approximation literature [44], [56] although its speed of convergence is sensitive to the quality of the initial solution.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 45,
      "context" : "Moreover, the cyclic descent approach is becoming very popular in the recent sparse approximation literature [44], [56] although its speed of convergence is sensitive to the quality of the initial solution.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 41,
      "context" : "1 as suggested in [52].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 36,
      "context" : "The basic SL0 implementation is dedicated to noise-free problems [43].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 44,
      "context" : "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 46,
      "context" : "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 47,
      "context" : "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 46,
      "context" : "We chose the efficient implementation of [57] in which the original pseudo-inverse calculations are replaced by a quasi-Newton strategy using limited memory BFGS updates.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 42,
      "context" : "We have tested two l1 solvers: the incrowd algorithm [59] together with an empirical setting of ε > 0, and l1 homotopy in the limit case ε → 0, following [53].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 36,
      "context" : "For SL0, we have followed the default setting of [43] for the rate of deformation of the nonconvex penalty.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 48,
      "context" : "More sophisticated localization tests are non binary and would take into account the distance between the location of the true spikes and their wrong estimates [60].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 49,
      "context" : "The generalized version [3] is inspired from the regression spline modeling in [61].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 29,
      "context" : "Extensions of OMP and OLS were recently proposed in this spirit [36] and deserve consideration for proposing efficient forward-backward algorithms.",
      "startOffset" : 64,
      "endOffset" : 68
    } ],
    "year" : 2015,
    "abstractText" : "Sparse signal restoration is usually formulated as the minimization of a quadratic cost function ‖y−Ax‖22, where A is a dictionary and x is an unknown sparse vector. It is well-known that imposing an l0 constraint leads to an NP-hard minimization problem. The convex relaxation approach has received considerable attention, where the l0-norm is replaced by the l1-norm. Among the many efficient l1 solvers, the homotopy algorithm minimizes ‖y −Ax‖ 2 + λ‖x‖1 with respect to x for a continuum of λ’s. It is inspired by the piecewise regularity of the l1-regularization path, also referred to as the homotopy path. In this paper, we address the minimization problem ‖y −Ax‖ 2 + λ‖x‖0 for a continuum of λ’s and propose two heuristic search algorithms for l0-homotopy. Continuation Single Best Replacement is a forward-backward greedy strategy extending the Single Best Replacement algorithm, previously proposed for l0-minimization at a given λ. The adaptive search of the λ-values is inspired by l1-homotopy. l0 Regularization Path Descent is a more complex algorithm exploiting the structural properties of the l0-regularization path, which is piecewise constant with respect to λ. Both algorithms are empirically evaluated for difficult inverse problems involving ill-conditioned dictionaries. Finally, we show that they can be easily coupled with usual methods of model order selection. This work was carried out in part while C. Soussen was visiting IRCCyN during the academic year 2010-2011 with the financial support of CNRS. C. Soussen and D. Brie are with the Université de Lorraine and CNRS at the Centre de Recherche en Automatique de Nancy (UMR 7039).Campus Sciences, B.P. 70239, F-54506 Vandœuvre-lès-Nancy, France. Tel: (+33)-3 83 59 56 43, Fax: (+33)-3 83 68 44 62. E-mail: charles.soussen@univ-lorraine.fr, david.brie@univ-lorraine.fr. J. Idier is with L’UNAM Université, Ecole Centrale Nantes and CNRS at the Institut de Recherche en Communications et Cybernétique de Nantes (UMR 6597), 1 rue de la Noë, BP 92101, F-44321 Nantes Cedex 3, France. Tel: (+33)-2 40 37 69 09, Fax: (+33)-2 40 37 69 30. E-mail: jerome.idier@irccyn.ec-nantes.fr. J. Duan was with CRAN. He is now with the Department of Biomedical Engineering, Xi’an Jiaotong University. No. 28, Xianning West Road, Xi’an 710049, Shaanxi Province, China. Tel: (+86)-29-82 66 86 68, Fax: (+86)-29 82 66 76 67. E-mail: junbo.duan@mail.xjtu.edu.cn. March 19, 2015 DRAFT",
    "creator" : "LaTeX with hyperref package"
  }
}