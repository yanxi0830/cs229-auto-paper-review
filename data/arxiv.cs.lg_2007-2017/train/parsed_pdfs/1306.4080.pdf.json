{
  "name" : "1306.4080.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Parallel Coordinate Descent Newton for Large-scale L1-Regularized Minimization",
    "authors" : [ "Yatao Bian", "Xiong Li", "Yuncai Liu" ],
    "emails" : [ "bianyatao@sjtu.edu.cn", "lixiong@sjtu.edu.cn", "whomliu@sjtu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n40 80\nv1 [\nKeywords: Coordinate Descent, Parallel Optimization, Large-scale Optimization, L1Regularized Minimization"
    }, {
      "heading" : "1. Introduction",
      "text" : "High dimensional L1-regularized models arise in a wide range of applications, such as sparse logistic regression (Ng, 2004), L1-regularized L2-loss SVM (Yuan et al., 2010) and compressed sensing (Li and Osher, 2009). Various optimization methods such as trust region (Lin and Moré, 1999), coordinate gradient descent (Tseng and Yun, 2009) and stochastic gradient (Shalev-Shwartz and Tewari, 2009) have been developed to solve L1-regularized\nmodels, among which Coordinate Descent Newton (CDN) (Yuan et al., 2010) is highly efficient and has shown superiority over others on some problems.\nLarge scale dataset with high dimensional features or large number of samples calls for highly scalable and parallelized optimization algorithms. Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richtárik and Takác (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features. However, SCDN has no guarantee on its convergence when the number of parallelly updated features is greater than a threshold, which limits its parallel ability. To pursue more parallelism, Scherrer et al Scherrer et al. (2012) proposed to preprocess the training data, such as feature clustering, which would introduce extra computation overhead.\nIs there a parallel coordinate descent algorithm with high parallelism and global convergence guarantee, but without needing data preprocessing? We in this paper present such an algorithm based on CDN of Yuan et al. (2010), termed as Parallel Coordinate Descent Newton (PCDN). PCDN first randomly partitions the feature set N into b subsets/bundles with the size of P , and then sequentially processes each bundle. For each bundle, PCDN first computes the descent direction for each feature in the bundle in parallel, then conducts P -dimensional line search to obtain the stepsize of the whole bundle. The P -dimensional line search strategy ensures the global convergence of PCDN, which will be justified theoretically. Further, we analyze the convergence rate of PCDN and show that, for any bundle size P , it is guaranteed to converge to a specified accuracy ǫ within the limited iteration number of Tǫ. Moreover, the iteration number Tǫ decreases along with the increasing of parallelism (bundle size P ). Extensive experiments over two L1-regularized problems and five real datasets demonstrate that PCDN is a highly parallelized approach with strong convergence guarantee and fast convergence rate. For readability, we here briefly summarize the mathematical notations in Table 1.\n2. L1-regularized minimization\nConsider the unconstrained L1-regularized minimization problem over the training set {(xi, yi)}si=1. It has a general form as follows:\nmin w∈Rn Fc(w) = min w∈Rn c\ns ∑\ni=1\nϕ(w;xi, yi) + ‖w‖1, (1)\nwhere ϕ(w;xi, yi) is a convex and non-negative loss function; c > 0 is the regularization parameter. For L1-regularized logistic regression, the overall loss can be expressed as,\nL(w) = c\ns ∑\ni=1\nϕlog(w;xi, yi) = c\ns ∑\ni=1\nlog(1 + e−yiw Txi). (2)\nFor L1-regularized L2-loss SVM, the overall loss is,\nL(w) = c s ∑\ni=1\nϕsvm(w;xi, yi) = c s ∑\ni=1\nmax(0, 1− yiwTxi)2. (3)\nA number of solvers are available for this problem. In this section, we focus on two effective solvers: Coordinate Descent Newton (CDN) Yuan et al. (2010) and its parallel variant, Shotgun CDN (SCDN) Bradley et al. (2011).\n2.1 Coordinate Descent Newton (CDN) for L1-regularized minimization\nYuan et al. (2010) have demonstrated that CDN is an efficient solver for large-scale L1regularized minimization. It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg. 1.\nGiven the current model w, for the selected feature j ∈ N , w is updated along the descent direction dj = d(w; j)ej , where,\nd(w; j) = argmin d\n{∇jL(w)d+ 1\n2 ∇2jjL(w)d2 + |wj + d|}. (4)\nArmijo rule according to Burke (1985) is adopted to determine the stepsize for the line search procedure. Let q be the line search step index, the stepsize α = α(w,d) is determined as follows,\nα(w,d) = max q=0,1,2,···\n{βq | Fc(w + βqd)− Fc(w) ≤ βqσ∆}, (5)\nwhere β ∈ (0, 1), σ ∈ (0, 1), βq denotes β to the power of q and,\n∆ = ∇L(w)Td+ γdTHd+ ‖w + d‖1 − ‖w‖1, (6)\nwhere H ≡ diag(∇2L(w)), γ ∈ [0, 1). This rule requires only function evaluations. According to Tseng and Yun (2009), larger stepsize will be accepted if we choose either σ near 0 or γ near 1.\nAlgorithm 1 Coordinate Descent Newton (CDN) (Yuan et al., 2010)\n1: initialize w0 = 0n×1. 2: for k = 0, 1, 2, · · · do 3: for all j ∈ N do 4: compute the descent direction dkj = d(w k,j; j) by solving Eq. (4). 5: find the stepsize αk,j = α(wk,j , dkj ej) by solving Eq. (5). //1-dimensional line search 6: wk,j+1 ← wk,j + αk,jdkj ej . 7: end for 8: end for\n2.2 Shotgun CDN (SCDN) for L1-regularized logistic regression\nFor L1-regularized logistic regression, Bradley et al. (2011) proposed SCDN, which is summarized in Alg. 2. It simply updates P̄ features in parallel, where each feature update corresponds to an inner iteration of CDN (see Alg. 1). However, its parallel updates for P̄ features might increase the risk of divergence, due to the correlation among features. Bradley et al. (2011) provided a problem-specific measure for SCDN’s potential of parallelization: the spectral radius ρ of XTX. With this measure, a upper bound is given to P̄ , i.e., P̄ ≤ n/ρ + 1 to achieve speedups linear in P̄ . However, ρ can be very large for most large-scale datasets, e.g. ρ = 20, 228, 800 for dataset gisette with n = 5000, which limits the parallel ability of SCDN. Therefore, algorithms with high parallelism are desired to deal with large-scale problems.\nAlgorithm 2 Shotgun CDN (SCDN) (Bradley et al., 2011)\n1: choose number of parallel updates P̄ ≥ 1 2: set w = 0n×1 3: while not converged do 4: In parallel on P̄ processors 5: choose j ∈ N uniformly at radom. 6: obtain dj = d(w; j)ej by solving Eq. (4). 7: find the stepsize αj = α(w,dj) by solving Eq. (5). //1-dimensional line search 8: w ← w + αjdj 9: end while"
    }, {
      "heading" : "3. Parallel coordinate descent newton (PCDN)",
      "text" : "SCDN places no guarantee on its convergence when the number of features to be updated in parallel is greater than a threshold, i.e., P̄ > n/ρ+ 1. To exploit higher parallelism, we propose a parallel algorithm based on high dimensional Armijo line search which potentially ensures its convergence. The overall procedure of the proposed algorithm, Parallel Coordinate Descent Newton (PCDN), is summarized in Algorithm 3. Note that, the key difference between PCDN and SCDN is the line search, i.e., PCDN performs P -dimensional\nline search for a bundle of features, while SCDN performs 1-dimensional Armijo line search for each feature.\nIn the k-th outer iteration, PCDN randomly partitions the feature index set N into b subsets in a Gauss-Seidel manner, such that,\nN = disjoint union of {Bkb,Bkb+1, · · · ,B(k+1)b−1}, ∀ k = 0, 1, 2, · · · (7)\nwhere B denotes the subset and is termed as bundle throughout this work; P = |B| is the bundle size; b = ⌈ n\nP ⌉ is the number of bundles partitioned fromN . Then PCDN sequentially\nprocesses each bundle in each inner iteration.\nIn the t-th inner iteration1, PCDN first computes the descent directions dtj (step 8) for P features in Bt in parallel, which constitutes the P -dimensional descent direction dt (dtj = 0,∀j 6∈ Bt). Then it performs P -dimensional Armijo line search (step 10) to obtain the stepsize αt of the bundle along dt. At last it updates the features in Bt (step 11).\nAlgorithm 3 Parallel Coordinate Descent Newton (PCDN)\n1: choose the bundle size P ∈ [1, n]. 2: initialize w0 = 0n×1. 3: for k = 0, 1, 2, · · · do 4: {Bkb,Bkb+1, · · · ,B(k+1)b−1} ← random partitions of N according to Eq. (7) 5: for t = kb, kb+ 1, · · · , kb+ b− 1 do 6: dt ← 0. 7: for all j ∈ Bt in parallel do 8: compute the descent direction dtj = d(w\nt; j) by solving Eq. (4). 9: end for\n10: find the stepsize αt = α(wt,dt) by solving Eq. (5). //P -dimensional line search 11: wt+1 ← wt + αtdt. 12: end for 13: end for\nThe P -dimensional line search is the key procedure that guarantees the convergence of our proposed PCDN. With P -dimensional line search, the objective function Fc(w) is ensured to be nonincreasing for any bundle Bt (see Lemma 1(3)). In general, the P -dimensional line search tends to return a large stepsize if the P features in Bt are less correlated, and return a small stepsize if the P features in Bt are highly correlated.\nPCDN can better exploit parallelism than SCDN. In step 8 of Alg. 3, the descent direction for P features can be computed in parallel on P threads. Theorem 3 shows that PCDN has a guarantee of global convergence, for any P ∈ [1, n]. Therefore, the bundle size P which measures the parallelism can be very large when the number of features n is large. In contrast, for SCDN, P̄ which measures the parallelism is no more than n/ρ+1 according to Bradley et al. (2011).\nFurthermore, PCDN will take less time for each outer iteration compared to CDN. First, the descent direction computing procedure (step 8 in Alg. 3) can be fully parallelized.\n1. Note that t is the cumulative inner iteration index. When referred to t-th iteration of PCDN, it indicates the inner iteration by default.\nSecond, in each outer iteration, PCDN conducts ⌈ n P ⌉ times of P -dimensional line search (step 10 in Alg. 3) while CDN conducts n times of 1-dimensional line search (step 5 in Alg. 1). However, the time cost of each P -dimensional line search is much less than P times of 1-dimensional line search time cost2. Therefore, the overall time cost of PCDN line search is lower than that of CDN. Thus, PCDN costs less time for each outer iteration compared to CDN, which will be verified by experiments in Section 5.2 and 5.3."
    }, {
      "heading" : "4. On the convergence of PCDN",
      "text" : "In this section, we will theoretically justify the convergence of the proposed PCDN from three aspects: the convergence of P -dimensional line search, the global convergence, the convergence rate. We enclose all the detailed proofs in Appendix A. Before analyzing the convergence of PCDN, we first give the following lemma.\nLemma 1 Let {wt}, {dt}, {αt} and {Bt} be sequences generated by Alg. 3; λ̄(Bt) be the maximum element of (XTX)jj where j ∈ Bt; λk be the k-th minimum element of (XTX)jj where j = 1, · · · , n. Then the following results hold.\n(1) EBt [λ̄(Bt)] is monotonically increasing w.r.t P ; EBt[λ̄(Bt)] is constant w.r.t P if λi is constant or λ1 = λ2 = · · · = λn; EBt [λ̄(Bt)]/P is monotonically decreasing w.r.t P .\n(2) For L1-regularized logistic regression in Eq. (2) and L1-regularized L2-loss SVM in Eq. (3), the diagonal elements of the Hessian of the loss function L(w) have positive lower bound h and upper bound h̄, and the upper bound only depends on the design matrix X. That is,\n∇2jjL(w) ≤ θc(XTX)jj = θc s ∑\ni=1\nx2ij , ∀ j ∈ N (8)\n0 < h ≤ ∇2jjL(w) ≤ h̄ = θcλ̄(N ), ∀ j ∈ N (9)\nwhere θ = 14 for logistic regression and θ = 2 for L2-loss SVM.\n(3) The objective {Fc(wt)} is nonincreasing and ∆t (Eq. (6)) in the Armijo line search rule satisfies\n∆t ≤ (γ − 1)dtTHtdt (10)\nFc(w t + αtdt)− Fc(wt) ≤ σαt∆t ≤ 0 (11)\n2. First, the time cost of one step of P -dimensional line search (denoted as tls) remains approximately constant with varying bundle size P . That is, tls is about the same as that in 1-dimensional line search in CDN. The reason is that in each line search step, it checks if Fc(w+αd)−Fc(w) ≤ ασ∆ is satisfied by objective value evaluation (Eq. (5)), the time cost of which is approximately constant with varying P when implemented properly (see Appendix B for details). Second, the P -dimensional line search in PCDN will terminate within finite steps (from Theorem 2), which is less than P times of line search steps in CDN.\nLemma 1(1) will be used to analyze the iteration number Tǫ given the expected accuracy ǫ. Lemma 1(2) will be used to prove Theorem 2, 3 and 4. Lemma 1(3) ensures the descent of the objective theoretically and gives upper bound for ∆t in the Armijo line search, and will be used to prove Theorem 2 and 4. Note that the upper bound (γ − 1)dtTHtdt is only related to the second order information.\nTheorem 2 (Convergence of P -dimensional line search) Let {Bt} be sequence generated by Alg. 3; λ̄(Bt) = max{(XTX)jj | j ∈ Bt}. Then the P-dimensional line search will converge in finite steps, and the expected line search step number in each iteration can be bounded as,\nE[qt] ≤ 1 + logβ−1 θc 2h(1− σ + σγ) + 1 2 logβ−1 P + logβ−1 E[λ̄(Bt)] (12)\nwhere the expectation is w.r.t the random choices of Bt; qt is the line search step number in the t-th iteration; β ∈ (0, 1), σ ∈ (0, 1) and γ ∈ [0, 1) are parameters of the Armijo rule (Eq. (5)); θ and h (the positive lower bound of ∇2jjL(w)) are in Lemma 1(2).\nBecause E[λ̄(Bt)] is monotonically increasing w.r.t P (Lemma 1(1)), Theorem 2 predicts that E[qt] increases along with the bundle size P .\nTheorem 3 (Global convergence) Let {wt} be the sequence genereated by Alg. 3, then every cluster point of {wt} is a stationary point of Fc(w).\nTheorem 3 guarantees that PCDN will converge globally for any bundle size P ∈ [1, n].\nTheorem 4 (Convergence rate) Let w∗ minimizes Eq. (1); {wt},{Bt} and {αt} be the sequences generated by Alg. 3; λ̄(Bt) = max{(XTX)jj | j ∈ Bt} and wT be the output of Alg. 3 after T + 1 iterations. Then,\nE[Fc(w T )]− Fc(w∗) ≤ nE[λ̄(Bt)] inft αtP (T + 1) [ θc 2 ‖w∗‖2 + θc supt α t 2σ(1 − γ)hFc(0) ]\nwhere the expectation is w.r.t the random choices of Bt; σ ∈ (0, 1) and γ ∈ [0, 1) are parameters in the Armijo rule (Eq. (5)); θ and h (the positive lower bound of ∇2jjL(w)) are in Lemma 1(2); E[λ̄(Bt)] is determined by the bundle size P and the design matrix X.\nBased on Theorem 4, we can immediately obtain the upper bound (T upǫ ) of the iteration number Tǫ satisfying a specified accuracy ǫ.\nTǫ ≤ nE[λ̄(Bt)] inft αtPǫ [ θc 2 ‖w∗‖2 + θc supt α t 2σ(1 − γ)hFc(0) ] ≡ T upǫ ∝ E[λ̄(Bt)] Pǫ (13)\nwhich says that PCDN can achieve iteration number speedups linear in the bundle size P compared to CDN if E[λ̄(Bt)] keeps constant3. In general, E[λ̄(Bt)] increases w.r.t P (Lemma 1(1)), thus making the speedup sublinear. Moreover, since E[λ̄(Bt)]/P decreases 3. If perform feature-wise normalization over the training data X to ensure λ1 = λ2 = · · · = λn, then\nE[λ̄(Bt)] keeps constant according to Lemma 1(1).\nw.r.t P from Lemma 1(1), T upǫ will decrease w.r.t P , so does Tǫ. Thus PCDN needs less iterations with larger bundle size P to converge to ǫ accuracy.\nTo verify the upper bound T upǫ (Eq. (13)) of the iteration number Tǫ for a given accuracy ǫ, we set ǫ = 10−3 and show the iteration number Tǫ as a function of the bundle size P in Fig. 1, where two document datasets: a9a and real-sim (see Section 5.1 for details) are tested. Because T upǫ ∝ E[λ̄(Bt)]/P , we draw E[λ̄(Bt)]/P instead of T upǫ in Fig. 1. The results match the upper bound in Eq. (13): for given ǫ, Tǫ (solid green lines) is positive correlated with E[λ̄(Bt)]/P (dotted blue lines). Also, Tǫ is decreasing w.r.t P . These results verify the conclusion that with larger bundle size P less iterations are needed by PCDN to converge to ǫ accuracy."
    }, {
      "heading" : "5. Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental setup",
      "text" : "Datasets Five public real datasets4 are used in our experiments. They are summarized in Table 2. news20, rcv1, a9a and real-sim are document datasets, whose samples are normalized to unit vectors. gisette is a handwriting digit problem from NIPS 2003 feature selection challenge, whose features are linearly scaled to the [-1,1] interval.\nFor each dataset, the optimal bundle size P ∗ under which PCDN has minimum runtime is set according to the following analysis.\nFor Alg. 3, the expected runtime of each iteration of PCDN can be approximated by,\nE[time(t)] ≈ (P/#thread) · tdc +E[qt] · tls (14)\nwhere #thread is the number of threads launched by PCDN, and is set to 23 in our experiments; tdc is the time cost of descent direction computing (step 8 in Alg. 3); tls is the time cost of a step of P -dimensional line search, and is approximately constant with varying P (see Appendix B for details). Because E[qt] will increase w.r.t the bundle size P (Theorem 2), Eq. (14) indicates that E[time(t)] increases w.r.t P . Plus the conclusion that (Section 4) PCDN needs less iterations for larger P to converge to ǫ accuracy, we need to make a tradeoff between the increasing runtime per iteration E[time(t)] and the decreasing iteration number Tǫ to select the optimal bundle size P\n∗. In practice, we run PCDN with varying P and empirically select the optimal P ∗ for each dataset, which is summaried in Table 3.\nImplementation We compare PCDN with three state-of-the-art L1-regularized solvers, CDN of Yuan et al. (2010), SCDN of Bradley et al. (2011) and TRON (Trust Region Newton Method) of Yuan et al. (2010). All these solvers are implemented in C/C++ language. For the Armijo line search procedures (Eq. (5)) in PCDN, CDN and SCDN, we set σ = 0.01, γ = 0 and β = 0.5 to keep fair comparison. OpenMP is used as the parallel programming model. Work in parallel is distributed among a team of threads using OpenMP parallel for construct, and the static scheduling of threads is used because it is proved to be very efficient in our experiments.\nThe stopping condition used in our implementation is similar to the outer stopping condition in Yuan et al. (2011). We run CDN with a strict stopping criteria ǫ = 10−8 to otain the optimal value Fc(w\n∗), which is used to compute the relative difference to the optimal function value (relative function value difference),\n[Fc(w)− Fc(w∗)]/Fc(w∗) (15)\nSome private implementation details are listed as follows:\n• CDN: we use the source code included in LIBLINEAR5. The shrinking procedure is modified so that it is consistent with the other parallel algorithms.\n• SCDN: we set P̄ = 8 for SCDN following Bradley et al. (2011). Though Bradley et al released the source code for SCDN, we reimplement it in C language based on CDN implementation in LIBLINEAR for fair comparison.\n5. version 1.7, http://www.csie.ntu.edu.tw/~cjlin/liblinear/oldfiles/\n• PCDN: we implement PCDN carefully, including the data type and the atomic operation. For atomic operation, we apply a compare-and-swap implementation using inline assembly.\n• TRON (Trust Region Newton Method (Yuan et al., 2010)): TRON is a L1-regularized L2-loss SVM solver by solving constrained optimization formulation. We set σ = 0.01 and β = 0.1 in the projected line search according to Yuan etal Yuan et al. (2010).\nPlatform All experiments are conducted on a 64-bit machine with Intel(R) Xeon(R) CPU (E5645 @ 2.40GHz) and 64GB main memory. We use GNU C/C++/Fortran compilers (version 4.6.0), and the ”-O3” optimization flag is set for each solver. We set #thread = 23 for PCDN on our 24-core machine, which is far less than the optimal bundle size P ∗ given in Table 3. Consequently, our 24-core machine is unable to fully exhibit its parallelism potential. Note that the descent direction computing (step 8 in Alg. 3) in PCDN can be fully parallelized on several hundreds even to thousand threads. PCDN will run even faster on a machine with more cores or with a GPU implementation.\n5.2 Results for L1-regularized L2-loss SVM\nRuntime comparison results of PCDN, CDN and TRON are shown in Fig. 2. We ran each algorithm on 3 datasets with the best regularization parameter c∗ (set according to Yuan et al Yuan et al. (2010)) and varying stopping criteria ǫ, which is set to be equivalent for the three solvers. As shown in Fig. 2 we can clearly see that PCDN was faster than\nCDN and TRON in most cases. As a feature-based parallel algorithm, the proposed PCDN performs especially well for sparse datasets with more features. This could be verified by the result on rcv1 and news20, which are both very sparse (training data sparsity is 99.85% and 99.97%) datasets with lots of features (47,236 and 1,355,191). In this circumstance, PCDN performed much better than TRON. In the best case on news20, PCDN was 29 times faster than TRON and 18 times faster than CDN. Note that for the dataset a9a, sometimes PCDN was slightly slower than TRON. The reason is that a9a is a relatively dense dataset with fewer features than samples (only 123 features with 26,049 samples).\n5.3 Results for L1-regularized logistic regression\nIn this section, we compare PCDN with SCDN and CDN on 5 datasets (we only put part of results here, see Appendix C for more results). In Fig. 3, we show the timing performance of relative function value difference (see Eq. (15)) and test accuracy in the first row and second row, respectively. To estimate the test accuracy, each dataset is split into one fifth for testing and the rest for training. The results indicate that PCDN is much faster than\nCDN and SCDN, where the best speedup compared to CDN is 17.49, it can be much higher if more threads were used for PCDN. The resutls highlight its high speedup and strong ability to pursue parallelism. Note that, for the dataset gisette shown in Fig. 3(b), SCDN was even slower than CDN. The reason accounting for this result is that SCDN is sensitive to correlation among features which makes the convergence rate lower. Moreover, SCDN does not have convergence guarantee for P̄ = 8 on the dataset news20, Fig. 3(c) shows that though SCDN converged faster at the beginning, it could not converge to the final model at limited time with a relatively strict stopping criteria ǫ = 10−7. Meanwhile, in all the cases, PCDN had strong convergence guarantee and fast convergence rate."
    }, {
      "heading" : "6. Discussion and conclusion",
      "text" : "We in this paper introduced a highly parallelized algorithm, Parallel Coordinate Descent Newton (PCDN), with strong convergence guarantee and fast convergence rate under any possible parallelism, for large-scale L1-regularized minimization. PCDN can be generalized to be a generic parallel framework to solve the problem of minimizing the sum of a convex loss term and a seperable term. In our experiment, the ability of PCDN is still limited by\nthe ordinary multi-core implementation where the number of cores as well as the number of threads are few. To fully utilize the potential of parallelism (i.e., the descent direction computing in step 8 in Algorithm 3), an implementation based on heterogeneous computing frameworks (Gaster et al., 2012) such as GPU and FPGA will be much more powerful."
    }, {
      "heading" : "Appendix A. Proofs",
      "text" : "In this appendix we prove the following lemmas and theorems from Section 4.\nProof of Lemma 1(1)\nProof\nWe first prove that EBt[λ̄(Bt)] is monotonically increasing w.r.t P and EBt[λ̄(Bt)] is constant w.r.t P , if λi is constant or λ1 = λ2 = · · · = λn.\nLet λk be the k-th minimum of (X TX)jj, j = 1, · · · , n,\nf(P ) = EBt [λ̄(Bt)] = 1\nCPn (λnC\nP−1 n−1 +λn−1C P−1 n−2 + · · ·+λkCP−1k−1 + · · ·+λPCP−1P−1 ), 1 ≤ P ≤ n\n(16) where CPn is the combinatorial number. For 1 ≤ P ≤ n− 1,\nf(P + 1)− f(P )\n= −λP CP−1P−1 CPn + P+1 ∑\nk=n\nλk( CPk−1 CP+1n − CP−1k−1 CPn )\n= −λP CP−1P−1 CPn + P+1 ∑\nk=n\nλk (P + 1)k − P (n+ 1) P (n− P ) CP−1k−1 CPn\nset k̄ = ⌈ (P+1)k P (n+1)⌉, then (P +1)k−P (n+1) ≥ 0,∀k ≥ k̄ and (P +1)k−P (n+1) ≤ 0,∀k < k̄. The above equation equals\nf(P + 1)− f(P )\n=\n\n\nk̄ ∑\nk=n\nλk (P + 1)k − P (n+ 1) P (n− P ) CP−1k−1 CPn\n\n−\n\n\nP+1 ∑\nk=k̄\nλk P (n + 1)− (P + 1)k P (n− P ) CP−1k−1 CPn\n − λP CP−1P−1 CPn\n≥\n\n\nk̄ ∑\nk=n\nλk̄ (P + 1)k − P (n+ 1) P (n− P ) CP−1k−1 CPn\n\n−\n\n\nP+1 ∑\nk=k̄\nλk̄ P (n + 1)− (P + 1)k P (n− P ) CP−1k−1 CPn\n − λk̄ CP−1P−1 CPn\n(17)\n= λk̄\n[\n− CP−1P−1 CPn + P+1 ∑\nk=n\n(P + 1)k − P (n+ 1) P (n− P ) CP−1k−1 CPn\n]\n= λk̄\n[\n− CP−1P−1 CPn + P+1 ∑\nk=n\n( CPk−1 CP+1n − CP−1k−1 CPn )\n]\n= λk̄\n[\nP+1 ∑\nk=n\nCPk−1 CP+1n − P ∑\nk=n\nCP−1k−1 CPn\n]\n= λk̄[1− 1] = 0\nwhere Eq. (17) comes from λk ≥ λk̄,∀k ≥ k̄ and λk ≤ λk̄,∀k < k̄. So, f(P + 1) − f(P ) ≥ 0, for1 ≤ P ≤ n− 1, that is, EBt λ̄(Bt) is monotonically increasing w.r.t P .\nObviously, from Eq. (16), if λ1 = λ2 = · · · = λn, EBt [λ̄(Bt)] = λ1, which is constant w.r.t P .\nThen we prove that EBt[λ̄(Bt)]/P is monotonically decreasing w.r.t P . Let λk be the k-th minimum of (X TX)jj, j = 1, · · · , n,\ng(P ) = EBt [λ̄(Bt)]\nP =\n1\nPCPn (λnC\nP−1 n−1 +λn−1C P−1 n−2 +· · ·+λkCP−1k−1 +· · ·+λPCP−1P−1 ), 1 ≤ P ≤ n\nFor 1 ≤ P ≤ n− 1,\ng(P + 1)− g(P )\n= −λP CP−1P−1 PCPn + P+1 ∑\nk=n\nλk( CPk−1\n(P + 1)CP+1n − CP−1k−1 PCPn )\n= −λP CP−1P−1 PCPn + P+1 ∑\nk=n\nλk k − n n− P CP−1k−1 PCPn\n≤ −λP CP−1P−1 PCPn + P+1 ∑\nk=n\nλP k − n n− P CP−1k−1 PCPn\n(18)\n= λP\n[\n− CP−1P−1 PCPn + P+1 ∑\nk=n\nk − n n− P CP−1k−1 PCPn λP\n]\n= λP\n[\nCP−1P−1 PCPn + P+1 ∑\nk=n\n( CPk−1\n(P + 1)CP+1n − CP−1k−1 PCPn )\n]\n= λP\n[\n1\nP + 1\nP+1 ∑\nk=n\nCPk−1 CP+1n − 1 P P ∑\nk=n\nCP−1k−1 CPn\n]\n= λP\n[\n1 P + 1 − 1 P\n]\n≤ 0 (19)\nwhere Eq. (18) results from k−n n−P ≤ 0 and λk ≥ λP ,∀k = n, · · · , P +1, Eq. (19) comes from λP ≥ 0 and 1P+1 − 1P < 0. So, g(P + 1)− g(P ) ≤ 0, for1 ≤ P ≤ n− 1, that is, E Bt [λ̄(Bt)] P\nis monotonically decreasing w.r.t P .\nProof of Lemma 1(2)\nProof For logistic regression,\n∇2jjL(w) = c s ∑\ni=1\nτ(yiw Txi)(1− τ(yiwTxi))x2ij (20)\nwhere τ(s) ≡ 11+e−s is the derivative of the logistic loss function. Because 0 < τ(s) < 1, one can get that 0 < ∇2jjL(w) ≤ 14c ∑s i=1 x 2 ij (”=” holds when τ(s) = 1 2), so θ = 1 4 for logistic regression. Also, because in practice |yiwTxi| < ∞, so there exist τ̄ and τ such that 0 < τ ≤ τ(yiwTxi) ≤ τ̄ < 1, so there exists a h > 0 such that 0 < h ≤ ∇2jjL(w).\nFor L2-loss SVM,\n∇2jjL(w) = 2c ∑ i∈I(w) x2ij ≤ 2c\ns ∑\ni=1\nx2ij (21)\nwhere I(w) = {i | yiwTxi < 1}. So θ = 2 for L2-loss SVM. To ensure that ∇2jjL(w) > 0, we add a very small positive number ν (ν = 10−12 in our program). So, h = ν > 0 for L2-loss SVM.\nProof of Lemma 1(3)\nProof We follow the proof in Tseng and Yun (2009), from Eq. (4) and the convexity of 1-norm, for any α ∈ (0, 1),\n∇L(w)Td+ 1 2 dTHd+ ‖w + d‖1 ≤ ∇L(w)T (αd) + 1 2 (αd)TH(αd) + ‖w + (αd)‖1\n= α∇L(w)Td+ 1 2 α2dTHd+ ‖α(w + d) + (1− α)w‖1 ≤ α∇L(w)Td+ 1 2 α2dTHd+ α‖w + d‖1 + (1− α)‖w‖1\nRearranging terms yields\n(1− α)∇L(w)Td+ (1− α)(‖w + d‖1 − ‖w‖1) ≤ − 1\n2 (1− α)(1 + α)dTHd\nDividing both sides by 1− α > 0 and taking α ↑ 1 yields\n∇L(w)Td+ ‖w + d‖1 − ‖w‖1 ≤ −dTHd\nso\n∆ = ∇L(w)Td+ γdTHd‖w + d‖1 − ‖w‖1 ≤ (γ − 1)dTHd\nand from the Armijo rule\nFc(w + αd)− Fc(w) ≤ σα∆ ≤ 0\nhence {Fc(wt)} is nonincreasing.\nProof of Theorem 2: convergence of P -dimensional line search\nProof\nFirst, we prove that (following Lemma 5(b) in Tseng and Yun (2009)) the descent condition in Eq. (5) Fc(w\nt + αdt) − Fc(wt) ≤ σαt∆t is satisfied for any σ ∈ (0, 1) whenever 0 ≤ αt ≤ min {\n1, 2h(1−σ+σγ) θc √ Pλ̄(Bt)\n}\n.\nFor any α ∈ [0, 1],\nFc(w + αd)− Fc(w) = L(w + αd)− L(w) + ‖w + αd‖1 − ‖w‖1\n= α∇L(w)Td+ ‖w + αd‖1 − ‖w‖1 + ∫ 1\n0 (∇L(w + tαd)−∇L(w))T (αd)dt\n≤ α∇L(w)Td+ α(‖w + d‖1 − ‖w‖1) + α ∫ 1\n0 ‖∇L(w + tαd)−∇L(w)‖‖d‖dt (22)\nwhere Eq. (22) is from the convexity of 1-norm and the Cauchy-Schwarz inequality, the 2-norm is only w.r.t j ∈ Bt because dj = 0,∀j 6∈ Bt. And\n‖∇L(w + tαd)−∇L(w)‖ ≤ ‖∇2L(w̄)‖‖tαd‖\n= tα\n√\n∑ j∈Bt (∇2jjL(w̄))2‖d‖\n≤ tα √\nP (θcλ̄(Bt))2‖d‖ (23) = tαθc √ Pλ̄(Bt)‖d‖\nwhere w̄ = t′(w + tαd) + (1 − t′)w, 0 ≤ t′ ≤ 1. Eq. (23) uses Lemma 1(2). Substitute the above inequality into Eq. (22) we get\nFc(w + αd) − Fc(w)\n≤ α∇L(w)Td+ α(‖w + d‖1 − ‖w‖1) + α2θc √ Pλ̄(Bt) ∫ 1\n0 t‖d‖2dt\n= α(∇L(w)Td+ ‖w + d‖1 − ‖w‖1) + α2θc √ Pλ̄(Bt) 2 ‖d‖2 = α(∇L(w)Td+ γdTHd+ ‖w + d‖1 − ‖w‖1) + α2θc √ Pλ̄(Bt) 2 ‖d‖2 − αγdTHd (24)\nIf we set α ≤ 2h(1−σ+σγ) θc √ P λ̄(Bt) ,then\nα2θc √ Pλ̄(Bt) 2 ‖d‖2 − αγdTHd ≤ α(h(1− σ + σγ)‖d‖2 − γdTHd) ≤ α((1 − σ + σγ)dTHd− γdTHd) (25) = α(1− σ)(1 − γ)dTHd ≤ −α(1− σ)∆ (26) = −α(1− σ)(∇L(w)Td+ γdTHd+ ‖w + d‖1 − ‖w‖1)\nwhere Eq. (25) comes from Eq. (9) in Lemma 1(2); Eq. (26) uses Lemma 1(3). The above equation together with Eq. (24) proves that Fc(w+αd)−Fc(w) ≤ σα∆ if α ≤ 2h(1−σ+σγ) θc √ Pλ̄(Bt) .\nThen, from the Armijo line search procedure, we can see that it tests different values of α from larger to smaller, it stops right after finding one value that satisfy Fc(w\nt + αdt)− Fc(w t) ≤ σαt∆t. So in the t-th iteration, the chosen stepsize αt will satisfy\nαt ≥ 2h(1− σ + σγ) θc √ Pλ̄(Bt)\n(27)\nFrom Eq. (5), αt = βq, thus the line search step number of the t-th iteration\nqt = 1 + logβ α t ≤ 1 + logβ−1\nθc √ Pλ̄(Bt)\n2h(1− σ + σγ) (28)\nTake expectation on both sides w.r.t the random choices of Bt\nE[qt] ≤ 1 + logβ−1 θc 2h(1− σ + σγ) + 1 2 logβ−1 P +EBt [logβ−1 λ̄(Bt)]\n≤ 1 + logβ−1 θc 2h(1− σ + σγ) + 1 2 logβ−1 P + logβ−1 EBt[λ̄(Bt)] (29)\nwhere Eq. (29) comes from Jensen’s inequality for concave function logβ−1(·).\nProof of Theorem 3: global convergence\nProof First, we prove that PCDN is a special case of CGD (see Tseng and Yun (2009)) with H = diag(∇2L(w)).\nNote that, the selection of bundle Bt in Eq. (7) is consistent with that used in CGD (Eq. (12) in Tseng and Yun (2009)). And for descent direction computing in a bundle in Alg. 3, we have,\ndt = ∑\nj∈Bt d(wt; j)ej\n= ∑\nj∈Bt argmin d {∇Lj(wt)Td+\n1 2 ∇2jjL(wt)d2 + |wtj + d|}ej (30)\n= argmin d {∇L(wt)Td+ 1 2 dTHd+ ‖w + d‖1 | dj = 0,∀j 6∈ Bt} (31) ≡ dH(wt;Bt) (32)\nwhere Eq. (30) is derived by considering the definition of d(w; j) in Eq. (4); Eq. (31) is obtained by applying the setting of H ≡ diag(∇2L(w)); Eq. (32) is defined by following the descent direction definition of Tseng etal (Eq. (6) in Tseng and Yun (2009)). Therefore the definition of direction computing is a CGD manner. Moreover, since PCDN conducts\nArmijo line search for dt, it is clear that PCDN is a special case of CGD by taking H = diag(∇2L(w)).\nThen, we can use Theorem 1(e) in Tseng and Yun (2009) to prove the global convergence, which requires that {Bt} is chosen under the Gauss-Seidel rule and supt αt < ∞. In Eq. (5), αt ≤ 1, t = 1, 2, ..., which satisfies supt αt < ∞. To ensure global convergence, Tseng etal made the following assumption,\n0 < h ≤ ∇2jjL(wt) ≤ h̄, ∀j = 1, · · · , n, t = 0, 1, ...\nwhich is fulfilled by Lemma 1(2). According to Theorem 1(e) in Tseng and Yun (2009), any cluster point of {wt} is a stationary point of Fc(w).\nProof of Theorem 4: convergence rate\nFor analysis of convergence rate, we transform Eq. (1) into an equivalent problem with a twice-differentiable regularizer following Shalev-Shwartz and Tewari (2009). Let ŵ ∈ R2n+ , use duplicated features6 x̂i ≡ [xi;−xi] ∈ R2n and the problem is\nmin ŵ∈R2n\n+\nFc(ŵ) ≡ c s ∑\ni=1\nϕ(ŵ; x̂i, yi) +\n2n ∑\nj=1\nŵj (33)\nNow, Eq. (4) becomes\nd̂(ŵ; j) ≡ argmin d̂\n{∇jL(ŵ)d̂+ 1\n2 ∇2jjL(ŵ)d̂2 + ŵj + d̂} = −(∇jL(ŵ) + 1)/∇2jjL(ŵ) (34)\nIn the following proof we omit the ”∧” above each variables for simplicity. Proof Define the potential function as\nΨ(w) = θcλ̄(Bt) 2 ‖w −w∗‖2 + θcλ̄(B t) supt α t 2σ(1 − γ)h Fc(w) = a‖w −w ∗‖2 + bFc(w) (35)\n6. Though our analysis uses duplicate features, they are not required for an implementation.\nthen\nΨ(w)−Ψ(w + αd) = a(‖w −w∗‖2 − ‖w + αd−w∗‖2) + b(Fc(w)− Fc(w + αd)) = aα(−2wTd+ 2w∗Td− αdTd) + b(Fc(w)− Fc(w + αd)) ≥ aα(−2wTd+ 2w∗Td− αdTd) + bσα(1 − γ)dTHd (36) = ∑\nj∈Bt aα(−2wjdj + 2w∗jdj − αd2j ) + bσα(1 − γ)∇2jjL(w)d2j\n= ∑ j∈Bt aα(−2wjdj + 2w∗jdj) + α[bσ(1 − γ)∇2jjL(w)− aα]d2j ≥ ∑\nj∈Bt aα(−2wj + 2w∗j )dj (37)\n= ∑\nj∈Bt\nθcλ̄(Bt)α ∇2jjL(w) (wj − w∗j )(∇jL(w) + 1) (38)\n≥ ∑\nj∈Bt\nλ̄(Bt)α (XTX)jj (wj − w∗j )(∇jL(w) + 1) (39)\n≥ α ∑ j∈Bt (wj − w∗j )(∇jL(w) + 1) (40)\nwhere Eq. (36) uses Lemma 1(3), Eq. (37) uses the fact that bσ(1 − γ)∇2jjL(w)− aα ≥ 0. Substitute a = θcλ̄(B\nt) 2 and dj = −(∇jL(w) + 1)/∇2jjL(w) into Eq. (37) we get Eq. (38).\nEq. (39) comes from Lemma 1(2) and Eq. (40) comes from the definition of λ̄(Bt). Take the expectation w.r.t the random choices of Bt on both sides\nEBt[Ψ(w)−Ψ(w + αd)] ≥ inf\nt αtEBt[\n∑ j∈Bt (wj − w∗j )(∇jL(w) + 1)] (41)\n= inf t αtPEj\n[ (wj − w∗j )(∇jL(w) + 1) ]\n= inf t αt\nP 2n (w −w∗)(∇L(w) + 1)\n≥ inf t αt\nP 2n (Fc(w)− Fc(w∗)) (42)\nwhere Eq. (41) results from Eq. (40) and Eq. (42) comes from the convexity of L(w).\nSum over T + 1 iterations, with an expectation over the random choices of Bt\nE[\nT ∑\nt=0\nΨ(wt)−Ψ(wt+1]\n≥ inf t αt\nP 2n E[\nT ∑\nt=0\nFc(w t)− Fc(w∗)]\n= inf t αt\nP 2n [E\nT ∑\nt=0\n[Fc(w t)]− (T + 1)Fc(w∗)]\n≥ inf t αt\nP (T + 1)\n2n [E[Fc(w\nT )]− Fc(w∗)] (43)\nwhere Eq. (43) comes from Lemma 1(b) that {Fc(wt)} is nonincreasing. Rearranging the above inequality gives\nE[Fc(w T )]− Fc(w∗)\n≤ 2n inft αtP (T + 1)\nE[ T ∑\nt=0\nΨ(wt)−Ψ(wt+1)]\n≤ 2n inft αtP (T + 1) E[Ψ(w0)−Ψ(wT+1)] ≤ 2n inft αtP (T + 1) E[Ψ(w0)] (44) = 2nEBt λ̄(Bt)\ninft αtP (T + 1)\n[\nθc 2 (‖w∗‖2) + θc supj αj 2σ(1 − γ)h (Fc(0) ]\n(45)\nwhere Eq. (44) comes from that Ψ(wT+1) ≥ 0; Eq. (45) comes from the fact that w0 is set to be 0."
    }, {
      "heading" : "Appendix B. Details about P -dimensional Armijo line search",
      "text" : "In this section we will show that the time cost of one step of our P -dimensional line search tls remains approximately constant with varying P by presenting the implementation details (in Alg. 4). Take w.l.o.g logistic regression for example, we maintain both dTxi and ew Txi , i = 1, · · · , s and follow the implementation technique of Fan etal (see Appendix G in Fan et al. (2008)). That is, using the following form of sufficient decrease condition:\nf(w+ βqd)− f(w)\n= ‖w+βqd‖1− ‖w‖1+ c( s ∑\ni=1\nlog( e(w+β q d)Txi + 1\ne(w+β qd)Txi + eβqdTxi\n) + βq ∑\ni:yi=−1 dTxi)\n≤ σβq(∇L(w)Td+ ‖w + d‖1 − ‖w‖1)\n(46)\nIn each line search step in Alg. 4, it check if Eq. (46) is satisfied, one can see that time cost\nAlgorithm 4 P -Dimensional Armijo Line Search Details of PCDN for Logistic Regression (γ = 0)\n1: Given β, σ,∇L(w),w,d and ewTxi , i = 1, ..., s. 2: ∆ ← ∇L(w)Td+ ‖w + d‖1 − ‖w‖1. 3: *Compute dTxi, i = 1, · · · , s. 4: for q = 0, 1, 2, · · · do 5: if Eq. (46) is satisfied then 6: w ← w + βqd. 7: ew T xi ← ewTxieβqdTxi , i = 1, · · · , s. 8: break 9: else\n10: ∆ ← β∆. 11: dTxi ← βdTxi, i = 1, · · · , s. 12: end if 13: end for\nof which remains constant w.r.t P . The only difference in the whole line search procedure is the computing of dTxi = ∑P j=1 djxij. However, d\nTxi in PCDN could be computed in parallel with P threads and a reduction-sum operation, so the time cost still remains constant."
    }, {
      "heading" : "Appendix C. More experimental results for logistic regression",
      "text" : "More experimental results are put in this section. Fig. 4 plots the trace of model nnz (number of nonzero elements in w, the first row) and function value Fc(w) (the second row) w.r.t runtime. Fig. 5 and Fig. 6 is the time performance of logistic regression on dataset a9a and real-sim, respectively."
    } ],
    "references" : [ {
      "title" : "Sparse online learning via truncated gradient",
      "author" : [ "John Langford", "Lihong Li", "Tong Zhang" ],
      "venue" : "OpenCL. Elsevier Science & Technology Books,",
      "citeRegEx" : "Langford et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2012
    }, {
      "title" : "Coordinate descent optimization for l1 minimization with",
      "author" : [ "Yingying Li", "Stanley Osher" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Li and Osher.,? \\Q2009\\E",
      "shortCiteRegEx" : "Li and Osher.",
      "year" : 2009
    }, {
      "title" : "Hogwild: A lock-free",
      "author" : [ "Niu", "Benjamin Recht", "Christopher Re", "Stephen J. Wright" ],
      "venue" : null,
      "citeRegEx" : "Niu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2004
    }, {
      "title" : "A coordinate gradient descent method for nonsmooth",
      "author" : [ "Paul Tseng", "Sangwoon Yun" ],
      "venue" : null,
      "citeRegEx" : "Tseng and Yun.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tseng and Yun.",
      "year" : 2009
    }, {
      "title" : "Slow learners are fast",
      "author" : [ "Alex Smola", "John Langford" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : ", 2010) and compressed sensing (Li and Osher, 2009).",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "Various optimization methods such as trust region (Lin and Moré, 1999), coordinate gradient descent (Tseng and Yun, 2009) and stochastic gradient (Shalev-Shwartz and Tewari, 2009) have been developed to solve L1-regularized 1",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "Algorithms in Langford et al. (2009); Niu et al.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al.",
      "startOffset" : 14,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems.",
      "startOffset" : 14,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richtárik and Takác (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem.",
      "startOffset" : 14,
      "endOffset" : 230
    }, {
      "referenceID" : 0,
      "context" : "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richtárik and Takác (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features.",
      "startOffset" : 14,
      "endOffset" : 408
    }, {
      "referenceID" : 0,
      "context" : "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richtárik and Takác (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features. However, SCDN has no guarantee on its convergence when the number of parallelly updated features is greater than a threshold, which limits its parallel ability. To pursue more parallelism, Scherrer et al Scherrer et al. (2012) proposed to preprocess the training data, such as feature clustering, which would introduce extra computation overhead.",
      "startOffset" : 14,
      "endOffset" : 753
    }, {
      "referenceID" : 0,
      "context" : "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richtárik and Takác (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features. However, SCDN has no guarantee on its convergence when the number of parallelly updated features is greater than a threshold, which limits its parallel ability. To pursue more parallelism, Scherrer et al Scherrer et al. (2012) proposed to preprocess the training data, such as feature clustering, which would introduce extra computation overhead. Is there a parallel coordinate descent algorithm with high parallelism and global convergence guarantee, but without needing data preprocessing? We in this paper present such an algorithm based on CDN of Yuan et al. (2010), termed as Parallel Coordinate Descent Newton (PCDN).",
      "startOffset" : 14,
      "endOffset" : 1096
    }, {
      "referenceID" : 3,
      "context" : "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg. 1. Given the current model w, for the selected feature j ∈ N , w is updated along the descent direction d = d(w; j)ej , where, d(w; j) = argmin d {∇jL(w)d+ 1 2 ∇jjL(w)d + |wj + d|}. (4) Armijo rule according to Burke (1985) is adopted to determine the stepsize for the line search procedure.",
      "startOffset" : 71,
      "endOffset" : 361
    }, {
      "referenceID" : 3,
      "context" : "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg. 1. Given the current model w, for the selected feature j ∈ N , w is updated along the descent direction d = d(w; j)ej , where, d(w; j) = argmin d {∇jL(w)d+ 1 2 ∇jjL(w)d + |wj + d|}. (4) Armijo rule according to Burke (1985) is adopted to determine the stepsize for the line search procedure. Let q be the line search step index, the stepsize α = α(w,d) is determined as follows, α(w,d) = max q=0,1,2,··· {β | Fc(w + βd)− Fc(w) ≤ βσ∆}, (5) where β ∈ (0, 1), σ ∈ (0, 1), β denotes β to the power of q and, ∆ = ∇L(w)d+ γdHd+ ‖w + d‖1 − ‖w‖1, (6) where H ≡ diag(∇2L(w)), γ ∈ [0, 1). This rule requires only function evaluations. According to Tseng and Yun (2009), larger stepsize will be accepted if we choose either σ near 0 or γ near 1.",
      "startOffset" : 71,
      "endOffset" : 796
    }, {
      "referenceID" : 3,
      "context" : "Proof of Lemma 1(3) Proof We follow the proof in Tseng and Yun (2009), from Eq.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "First, we prove that (following Lemma 5(b) in Tseng and Yun (2009)) the descent condition in Eq.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "Proof of Theorem 3: global convergence Proof First, we prove that PCDN is a special case of CGD (see Tseng and Yun (2009)) with H = diag(∇2L(w)).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "Proof of Theorem 3: global convergence Proof First, we prove that PCDN is a special case of CGD (see Tseng and Yun (2009)) with H = diag(∇2L(w)). Note that, the selection of bundle Bt in Eq. (7) is consistent with that used in CGD (Eq. (12) in Tseng and Yun (2009)).",
      "startOffset" : 101,
      "endOffset" : 265
    }, {
      "referenceID" : 3,
      "context" : "(6) in Tseng and Yun (2009)).",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "Then, we can use Theorem 1(e) in Tseng and Yun (2009) to prove the global convergence, which requires that {Bt} is chosen under the Gauss-Seidel rule and supt α < ∞.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "According to Theorem 1(e) in Tseng and Yun (2009), any cluster point of {wt} is a stationary point of Fc(w).",
      "startOffset" : 29,
      "endOffset" : 50
    } ],
    "year" : 2017,
    "abstractText" : "Parallel coordinate descent algorithms emerge with the growing demand for large-scale optimization. These algorithms are usually limited by their divergence under high parallelism or need data preprocessing to avoid divergence. In this paper, we propose a parallelized algorithm, termed as Parallel Coordinate Descent Newton (PCDN), to pursue more parallelism. It randomly partitions the feature set into b subsets/bundles with size of P , then it sequentially processes each bundle by first computing the descent directions for each feature in the bundle in parallel and then conducting P -dimensional line search to obtain the stepsize of the bundle. We will show that: (1) PCDN is guaranteed to converge globally; (2) PCDN can converge to the specified accuracy ǫ within the limited iteration number of Tǫ, and the iteration number Tǫ decreases along with the increasing of parallelism (bundle size P ). PCDN is applied to large-scale L1-regularized logistic regression and L2-loss SVM. Experimental evaluations over five public datasets indicate that PCDN can better exploit parallelism and outperforms state-of-the-art algorithms in speed, without losing test accuracy.",
    "creator" : "LaTeX with hyperref package"
  }
}