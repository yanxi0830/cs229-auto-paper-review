{
  "name" : "1406.2673.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mondrian Forests: Efficient Online Random Forests",
    "authors" : [ "Balaji Lakshminarayanan", "Daniel M. Roy" ],
    "emails" : [ "balaji@gatsby.ucl.ac.uk." ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Despite being introduced over a decade ago, random forests remain one of the most popular machine learning tools due in part to their accuracy, scalability, and robustness in real-world classification tasks [3]. (We refer to [5] for an excellent recent survey of random forests.) In this paper, we introduce a novel type of random forest—called Mondrian forests (MF), due to the fact that the underlying tree structure of each classifier in the ensemble is a so-called Mondrian process. Using the properties of Mondrian processes, we present an efficient online algorithm that agrees with its batch counterpart at each iteration. Not only are online Mondrian forests faster and more accurate than recent proposals for online random forest methods, but they nearly match the accuracy of state-of-the-art batch random forest methods trained on the same dataset.\nThe paper is organized as follows: In Section 2, we describe our approach at a high-level, and in Sections 3, 4, and 5, we describe the tree structures, label model, and incremental updates/predictions in more detail. We discuss related work in Section 6, demonstrate the excellent empirical performance of MF in Section 7, and conclude in Section 8 with a discussion about future work.\n∗Corresponding author. Email address: balaji@gatsby.ucl.ac.uk.\nar X\niv :1\n40 6.\n26 73\nv1 [\nst at\n.M L\n] 1\n0 Ju"
    }, {
      "heading" : "2 Approach",
      "text" : "Given N labeled examples (x1, y1), . . . , (xN , yN ) ∈ RD × Y as training data, our task is to predict labels y ∈ Y for unlabeled test points x ∈ RD. We will focus on multi-class classification where Y := {1, . . . ,K}, however, it is possible to extend the methodology to other supervised learning tasks such as regression. Let X1:n := (x1, . . . ,xn), Y1:n := (y1, . . . , yn), and D1:n := (X1:n, Y1:n).\nA Mondrian forest classifier is constructed much like a random forest: Given training data D1:N , we sample an independent collection T1, . . . , TM of so-called Mondrian trees, which we will describe in the next section. The prediction made by each Mondrian tree Tm is a distribution pTm(y|x,D1:N ) over the class label y for a test point x. The prediction made by the Mondrian forest is the average\n1\nM M∑ m=1 pTm(y|x,D1:N ) (1)\nof the individual tree predictions. As M →∞, the average converges at the standard rate to the expectation\nET∼MT(λ,D1:N )[ pT (y|x,D1:N )] +O(M −1/2), (2)\nwhere MT (λ,D1:N ) is the distribution of a Mondrian tree. As the limiting expectation does not depend on M , we would not expect to see overfitting behavior as M increases. A similar observation was made by Breiman in his seminal article [2] introducing random forests. Note that Eq. (2) is ensemble model combination, not Bayesian model averaging.\nIn the online learning setting, the training examples are presented one after another in a sequence of trials. Mondrian forests excel in this setting: at iteration n+ 1, each Mondrian tree T ∼ MT (λ,D1:n) is updated to incorporate the next labeled example (xn+1, yn+1) by sampling an extended tree T ′ from a distribution MTx(λ, T,Dn+1). Using properties of the Mondrian process, we can choose a probability distribution MTx such that T ′ = T on D1:n and T ′ is distributed according to MT (λ,D1:n+1), i.e.,\nT ∼ MT (λ,D1:n) T ′ | T,D1:n+1 ∼ MTx(λ, T,Dn+1)\nimplies T ′ ∼ MT (λ,D1:n+1) . (3)\nTherefore, the distribution of Mondrian trees trained on a dataset in an incremental fashion is the same as that of Mondrian trees trained on the same dataset in a batch fashion, irrespective of the order in which the data points are observed. To the best of our knowledge, none of the existing online random forests have this property. Moreover, we can sample from MTx(λ, T,Dn+1) efficiently: the complexity scales with the depth of the tree, which is typically logarithmic in n.\nWhile treating the online setting as a sequence of larger and larger batch problems is normally computationally prohibitive, this approach can be achieved efficiently with Mondrian forests. In the following sections, we define the Mondrian tree distribution MT (λ,D1:N ), the label distribution pT (y|x,D1:N ), and the update distribution MTx(λ, T,Dn+1)."
    }, {
      "heading" : "3 Mondrian trees",
      "text" : "For our purposes, a decision tree on RD will be a hierarchical, binary partitioning of RD and a rule for predicting the label of test points given training data. More carefully, a rooted, strictly-binary tree is a finite tree T such that every node in T is either a leaf or internal\nnode, and every node is the child of exactly one parent node, except for a distinguished root node, represented by , which has no parent. Let leaves(T) denote the set of leaf nodes in T. For every internal node j ∈ T \\ leaves(T), there are exactly two children nodes, represented by left(j) and right(j). To each node j ∈ T, we associate a block Bj ∈ RD of the input space as follows: We let B := RD. Each internal node j ∈ T \\ leaves(T) is associated with a split( δj , ξj ) , where δj ∈ {1, 2, . . . , D} denotes the dimension of the split and ξj denotes the location of the split along dimension δj . We then define\nBleft(j) := {x ∈ Bj : xδj ≤ ξj} and Bright(j) := {x ∈ Bj : xδj > ξj}. (4)\nWe may write Bj = ( `j1, uj1 ] × . . .× ( `jD, ujD ] , where `jd and ujd denote the `ower and upper limit, respectively, of the rectangular block Bj along dimension d. Put `j = {`j1, `j2, . . . , `jD} and uj = {uj1, uj2, . . . , ujD}. The decision tree structure is represented by the tuple T = (T, δ, ξ). We refer to Figure 1(a) for a simple illustration of a decision tree.\nLet parent(j) denote the parent of node j. Let N(j) denote the indices of training data points at node j, i.e., N(j) = {n ∈ {1, . . . , N} : xn ∈ Bj}. Let DN(j) = {XN(j), YN(j)} denote the features and labels of training data points at node j. Let `xjd and u x jd denote the lower and upper limit of training data points (hence the superscript x) respectively in node j along dimension d. Let Bxj = ( `xj1, u x j1 ] × . . .× ( `xjD, u x jD ] ⊆ Bj denote the smallest rectangle that encloses the training data points in node j."
    }, {
      "heading" : "3.1 Mondrian process distribution over decision trees",
      "text" : "Mondrian processes, introduced by Roy and Teh [14], are families {Mt : t ∈ [0,∞)} of random, hierarchical binary partitions of RD such that Mt is a refinement of Ms whenever t > s.1 Mondrian processes are natural candidates for the partition structure of random decision trees, but Mondrian processes on RD are, in general, infinite structures that we cannot represent all at once. Because we only care about the partition on a finite set of observed data, we introduce Mondrian trees, which are restrictions of Mondrian processes to a finite set of points. A Mondrian tree T can be represented by a tuple (T, δ, ξ, τ ), where (T, δ, ξ) is a decision tree, τ = {τj}j∈T, and τj ≥ 0 denotes the time of the split associated with node j. The time of split increases with depth, i.e., τj > τparent(j). We abuse notation and define τparent( ) = 0.\n1Roy and Teh [14] studied the distribution of {Mt : t ≤ λ} and refered to λ as the budget. See [13, Chp. 5] for more details. We will refer to t as time, not be confused with discrete time in the online learning setting.\nGiven a non-negative lifetime parameter λ and training data D1:n, the generative process for sampling Mondrian trees from MT (λ,D1:n) is described in the following two algorithms: Algorithm 1 SampleMondrianTree ( λ,D1:n\n) 1: Initialize: T = ∅, leaves(T) = ∅, δ = ∅, ξ = ∅, τ = ∅, N( ) = {1, 2, . . . , n} 2: SampleMondrianBlock ( ,DN( ), λ ) . Algorithm 2\nAlgorithm 2 SampleMondrianBlock ( j,DN(j), λ ) 1: Add j to T 2: For all d, set `xjd = min(XN(j),d), u x jd = max(XN(j),d) . dimension-wise min and max\n3: Sample E from exponential distribution with rate ∑ d(u x jd − `xjd) 4: if τparent(j) + E < λ then . j is an internal node 5: Set τj = τparent(j) + E 6: Sample split dimension δj , choosing d with probability proportional to u x jd − `xjd 7: Sample split location ξj uniformly from interval [` x jδj , uxjδj ] 8: Set N(left(j)) = {n ∈ N(j) : Xn,δj ≤ ξj} and N(right(j)) = {n ∈ N(j) : Xn,δj > ξj} 9: SampleMondrianBlock ( left(j),DN(left(j)), λ\n) 10: SampleMondrianBlock ( right(j),DN(right(j)), λ\n) 11: else . j is a leaf node 12: Set τj = λ and add j to leaves(T)\nThe procedure starts with the root node and recurses down the tree. In Algorithm 2, we first compute the `x and u x i.e. the lower and upper limits of B x , the smallest rectangle enclosing XN( ). We sample E from an exponential distribution whose rate is the so-called linear dimension of Bx , given by ∑ d(u x d − `x d). Since τparent( ) = 0, E + τparent( ) = E. If E ≥ λ, the time of split is not within the lifetime λ; hence, we assign to be a leaf node and the procedure halts. (Since E[E] = 1/ (∑ d(u x jd − `xjd) ) , bigger rectangles are less likely to be leaf nodes.) Else, is an internal node and we sample a split (δ , ξ ) in B x from the uniform split distribution in Bx . More precisely, we first sample the dimension δ , taking the value d with probability proportional to ux d − `x d, and then sample the split location ξ uniformly from the interval [`x δ , u x δ\n]. The procedure then recurses along the left and right children. Mondrian trees differ from standard decision trees (e.g. CART, C4.5) in the following: (i) the splits are sampled independent of the labels YN(j); (ii) every node j is associated with a split time τj ; (iii) the lifetime parameter λ controls the total number of splits (similar to the maximum depth parameter for standard decision trees); (iv) the split represented by an internal node j holds only within Bxj and not the whole of Bj . No commitment is made in Bj \\Bxj . Figure 1 illustrates the difference between Mondrian trees and decision trees.\nConsider the family of distributions MT (λ, F ), where F ranges over all possible finite sets of data points. Due to the fact that these distributions are derived from that of a Mondrian process on RD restricted to a set F of points, the family MT (λ, ·) will be projective. Intuitively, projectivity implies that the tree distributions possess a type of self-consistency in distribution. In words, if we sample a Mondrian tree T from MT (λ, F ) and then restrict the tree T to a subset F ′ ⊆ F of points, then the restricted tree T ′ has distribution MT (λ, F ′). This property follows from a similar property of Mondrian processes [13, 14]. Most importantly, projectivity gives us a consistent way to extend a Mondrian tree on a data set D1:n to a larger data set D1:n+1. We exploit this property to incrementally grow a Mondrian tree: even though MT (λ,D1:n) is defined on RD, we instantiate the Mondrian tree just on the regions where we have observed training data points so far; upon observing Dn+1, we extend the Mondrian by sampling from the conditional Mondrian distribution, referred to as MTx(λ, T,Dn+1) in (3), unveiling the Mondrian tree only where we have observed training data."
    }, {
      "heading" : "4 Label distribution: model, hierarchical prior, and pre-",
      "text" : "dictive posterior\nSo far, our discussion has been focused only on the tree structure. In this section, we focus on the label distribution pT (y|x,D1:N ). Intuitively, we want the label distribution at a node to be a smoothed estimate of the empirical distribution of labels at a node. We achieve this smoothing via a hierarchical Bayesian approach within each tree. For each tree T , we introduce latent parameters G which specify a distribution over y at each node, denoted by pT (y|x,G). Next, we define a hierarchical prior pT (G) that encourages label distribution at a node to be similar to that of its parent. Finally, we discuss how the likelihood and the hierarchical prior are combined to obtain the label distribution pT (y|x,D1:N ).\nLet leaf(x) denote the unique leaf node in T such that x ∈ Bleaf(x). As is common in the decision tree literature, we assume that the probability of labels within each block is independent of X given the tree structure T . Let Gj denote the distribution of labels at node j and G = {Gj : j ∈ T} denote the set of label distributions at all the nodes in the tree. Given a tree T , the likelihood for x is defined by the label distribution at the node leaf(x), i.e., pT (y|x,G) = Gleaf(x). In this paper, we focus on the case of categorical labels taking values in the set {1, . . . ,K}. Hence, Gj = [Gj,1, Gj,2, . . . , Gj,K ] is the discrete distribution, where Gj,k is the probability of label k at node j.\nWe model the collection Gj , for j ∈ T , as a hierarchy of normalized stable processes (NSP). A NSP prior is a distribution over distributions and is a special case of the Pitman-Yor process (PYP) prior where the concentration parameter is taken to zero.2 The discount parameter d ∈ (0, 1) controls how much the samples vary around the base distribution; if Gj ∼ NSP(d,H), then E[Gjk] = Hk and Var[Gjk] = (1 − d)Hk(1 −Hk). We use a hierarchical NSP (HNSP) prior over Gj as follows:\nG |H ∼ NSP(d , H), and Gj |Gparent(j) ∼ NSP(dj , Gparent(j)). (5)\nThis hierarchical prior was first proposed by Wood et al. [19]. Here we set dj = exp{−γ(τj − τparent(j))}, and the base distribution H to be the uniform distribution over the K labels.\nGiven training data D1:N , the predictive distribution pT (y|x,D1:N ) is obtained by integrating over G, i.e., pT (y|x,D1:N ) = EG∼pT (G|D1:N )[pT (y|x,G)] = EG∼pT (G|D1:N )[Gleaf(x),y] = Gleaf(x),y, where the posterior pT (G|D1:N ) ∝ pT (G) ∏N n=1 pT (yn|xn,G). Posterior inference in the HNSP, i.e., computation of the posterior means Gleaf(x), is a special case of posterior inference in hierarchical PYP (HPYP). In particular, Teh [17] considers the HPYP with multinomial likelihood (in the context of language modeling). The model considered here is a special case of [17]. Exact inference is intractable and hence we resort to approximations. In particular, we use a fast approximation known as the interpolated Kneser-Ney (IKN) smoothing [17], a popular technique for smoothing probabilities in language modeling [10]. The IKN approximation in [17] can be extended in a straightforward fashion to the online setting, and the computational complexity of adding a new training instance is linear in the depth of the tree. We refer the reader to Appendix A for further details."
    }, {
      "heading" : "5 Online training and prediction",
      "text" : "In this section, we describe the distribution MTx(λ, T,Dn+1) that incrementally adds the data point Dn+1 to a tree T . These updates are based on the conditional Mondrian algorithm [14], specialized to a finite set of points. In general, one or more of the following three operations\n2Taking the discount parameter to zero leads to a Dirichlet process. Hierarchies of NSPs admit more tractable approximations than hierarchies of Dirichlet processes, hence our choice here.\nAt iteration 1, we have two training data points, labeled as a, b. Figures 2(a) and 2(g) show the partition and tree structure of the Mondrian tree. Note that even though there is a split x2 > 0.23 at time t = 2.42, we commit this split only within Bxj (shown by the gray rectangle).\nAt iteration 2, a new data point c is added. Algorithm 3 starts with the root node and recurses down the tree. Algorithm 4 checks if the new data point lies within Bx by computing the additional extent e` and eu. In this case, c does not lie within Bx . Let Rab and Rabc respectively denote the small gray rectangle (enclosing a, b) and big gray rectangble (enclosing a, b, c) in Figure 2(b). While extending the Mondrian from Rab to Rabc, we could either introduce a new split in Rabc outside Rab or extend the split in Rab to the new range. To choose between these two options, we sample the time of this new split: we first sample E from an exponential distribution whose rate is the sum of the additional extent, i.e., ∑ d(e ` d + e u d), and set the time of the new split to E + τparent( ). If E+ τparent( ) ≤ τ , this new split in Rabc can precede the old split in Rab and a split is sampled in Rabc outside Rab. In Figures 2(c) and 2(h), E + τparent( ) = 1.01 + 0 ≤ 2.42, hence a new split x1 > 0.75 is introduced. The farther a new data point x is from Bxj , the higher the rate ∑ d(e ` d + e u d), and\nsubsequently the higher the probability of a new split being introduced, since E[E] = 1/ (∑\nd(e ` d+e u d ) ) .\nA new split in Rabc is sampled such that it is consistent with the existing partition structure in Rab (i.e., the new split cannot slice through Rab).\nIn the final iteration, we add data point d. In Figure 2(d), the data point d lies within the extent of the root node, hence we traverse to the left side of the root and update Bxj of the internal node containing {a, b} to include d. We could either introduce a new split or extend the split x2 > 0.23. In Figure 2(e), we extend the split x2 > 0.23 to the new extent, and traverse to the leaf node in Figure 2(h) containing b. In Figures 2(f) and 2(i), we sample E = 1.55 and since τparent(j) + E = 2.42 + 1.55 = 3.97 ≤ λ =∞, we introduce a new split x1 > 0.47.\nmay be executed while introducing a new data point: (i) introduction of a new split ‘above’ an existing split, (ii) extension of an existing split to the updated extent of the block and (iii) splitting an existing leaf node into two children. To the best of our knowledge, existing online decision trees use just the third operation, and the first two operations are unique to Mondrian trees. The complete pseudo-code for incrementally updating a Mondrian tree T with new data D according to MTx(λ, T,D) is described in the following two algorithms. Figure 2 walks through the algorithms on a toy dataset.\nAlgorithm 3 ExtendMondrianTree(T, λ,D) 1: Input: Tree T = (T, δ, ξ, τ ), new training instance D = (x, y) 2: ExtendMondrianBlock(T, λ, ,D) . Algorithm 4\nAlgorithm 4 ExtendMondrianBlock(T, λ, j,D) 1: Set e` = max(`xj − x, 0) and eu = max(x− uxj , 0) . e` = eu = 0D if x ∈ Bxj 2: Sample E from exponential distribution with rate ∑ d(e ` d + e u d)\n3: if τparent(j) + E < τj then . introduce new parent for node j 4: Sample split dimension δ, choosing d with probability proportional to e`d + e u d 5: Sample split location ξ uniformly from interval [uxj,δ, xδ] if xδ > u x j,δ else [xδ, ` x j,δ]. 6: Insert a new node ̃ just above node j in the tree, and a new leaf j′′, sibling to j, where 7: δ̃ = δ, ξ̃ = ξ, τ̃ = τparent(j) + E, ` x ̃ = min(` x j ,x), u x ̃ = max(u x j ,x) 8: j′′ = left(̃) iff xδ̃ ≤ ξ̃ 9: SampleMondrianBlock ( j′′,D, λ\n) 10: else 11: Update `xj ← min(`xj ,x),uxj ← max(uxj ,x) . update extent of node j 12: if j /∈ leaves(T) then . return if j is a leaf node, else recurse down the tree 13: if xδj ≤ ξj then child(j) = left(j) else child(j) = right(j) 14: ExtendMondrianBlock(T, λ, child(j),D) . recurse on child containing D\nIn practice, random forest implementations stop splitting a node when all the labels are identical and assign it to be a leaf node. To make our MF implementation comparable, we ‘pause’ a Mondrian block when all the labels are identical; if a new training instance lies within Bj of a paused leaf node j and has the same label as the rest of the data points in Bj , we continue pausing the Mondrian block. We ‘un-pause’ the Mondrian block when there is more than one unique label in a block. Algorithms 9 and 10 in the appendix discuss versions of SampleMondrianBlock and ExtendMondrianBlock for paused Mondrians.\nPrediction using Mondrian tree Let x denote a test data point. If x is already “contained” in the tree T , i.e., if x ∈ Bxj for some leaf j ∈ leaves(T ), then the prediction is taken to be Gleaf(x). Otherwise, we somehow need to incorporate x. One choice is to extend T by sampling T ′ from MTx(λ, T,x) as described in Algorithm 3, and set the prediction to Gj , where j ∈ leaves(T ′) is the leaf node containing x. A particular extension T ′ might lead to an overly confident prediction; hence, we average over every possible extension T ′. This integration can be carried out analytically and the computational complexity is linear in the depth of the tree. We refer the reader to Appendix B for further details."
    }, {
      "heading" : "6 Related work",
      "text" : "The literature on random forests is vast and we do not attempt to cover it comprehensively; we provide a brief review here and refer to [5] and [7] for a recent review of random forests in\nbatch and online settings respectively. Classic decision tree induction procedures choose the best split dimension and location from all candidate splits at each node by optimizing some suitable quality criterion (e.g. information gain) in a greedy manner. In a random forest, the individual trees are randomized to de-correlate their predictions. The most common strategies for injecting randomness are (i) bagging [1] and (ii) randomly subsampling the set of candidate splits within each node.\nTwo popular random forest variants in the batch setting are Breiman-RF [2] and Extremely randomized trees (ERT) [9]. Breiman-RF uses bagging and furthermore, at each node, a random k-dimensional subset of the original D features is sampled. ERT chooses a k dimensional subset of the features and then chooses one split location each for the k features randomly (unlike Breiman-RF which considers all possible split locations along a dimension). ERT does not use bagging. When k = 1, the ERT trees are totally randomized and the splits are chosen independent of the labels; hence the ERT-1 method is very similar to MF in the batch setting in terms of tree induction. (Note that unlike ERT, MF uses HNSP to smooth predictive estimates and allows a test point to branch off into its own node.) Perfect random trees (PERT), proposed by Cutler and Zhao [6] for classification problems, produce totally randomized trees similar to ERT-1, although there are some slight differences [9].\nExisting online random forests [7, 15] start with an empty tree and grow the tree incrementally. Every leaf of every tree maintains a list of k candidate splits and associated quality scores. When a new data point is added, the scores of the candidate splits at the corresponding leaf node are updated. To reduce the risk of choosing a sub-optimal split based on noisy quality scores, additional hyper parameters such as the minimum number of data points at a leaf node before a decision is made and the minimum threshold for the quality criterion of the best split, are used to assess ‘confidence’ associated with a split. Once these criteria are satisfied at a leaf node, the best split is chosen (making this node an internal node) and its two children are the new leaf nodes (with their own candidate splits), and the process is repeated. These methods could be memory inefficient for deep trees due to the high cost associated with maintaining candidate quality scores for the fringe of potential children [7].\nThere has been some work on incremental induction of decision trees, e.g. incremental CART [4], ITI [18], VFDT [8] and dynamic trees [16], but to the best of our knowledge, these are focused on learning decision trees and have not been generalized to online random forests. We do not compare MF to incremental decision trees, since random forests are known to outperform single decision trees."
    }, {
      "heading" : "7 Empirical evaluation",
      "text" : "The purpose of these experiments is to evaluate the predictive performance (test accuracy) of MF as a function of (i) fraction of training data and (ii) training time. We divide the training data into 100 mini-batches and we compare the performance of online random forests (MF, ORF-Saffari) to batch random forests (Breiman-RF, ERT-k, ERT-1) which are trained on the same fraction of the training data. Our scripts are implemented in Python. We implemented the ORF-Saffari algorithm as well as ERT in Python for timing comparisons. The scripts will be made publicly available. We did not implement the ORF-Denil algorithm since its performance is very similar to ORF-Saffari [7] and the computational complexity of the ORF-Denil algorithm is worse than that of ORF-Saffari. We used the Breiman-RF implementation in scikit-learn [12].3\n3The scikit-learn implementation uses highly optimized C code, hence we do not compare our runtimes with the scikit-learn implementation. The ERT implementation in scikit-learn achieves very similar test accuracy as our ERT implementation, hence we do not report those results here.\nWe evaluate on four of the five datasets used in [15] — we excluded the mushroom dataset as even very simple logical rules achieve > 99% accuracy on this dataset.4 We re-scaled the datasets such that each feature takes on values in the range [0, 1] (by subtracting the min value along that dimension and dividing by the range along that dimension, where range = max−min).\nAs common in the random forest literature, we set the number of trees M = 100. For Mondrian forests, we set λ =∞, γ = 10D. For ORF-Saffari, we set num epochs = 20 (number of passes through the training data) and set the other hyper parameters to the values used in [15]. For Breiman-RF and ERT, the hyper parameters are set to default values. We repeat each algorithm with five random initializations and report the mean performance. The results are shown in Figure 3. (The * in Breiman-RF* indicates scikit-learn implementation.)\nComparing test accuracy vs fraction of training data on usps, satimages and letter datasets, we observe that MF achieves accuracy very close to the batch RF versions (BreimanRF, ERT-k, ERT-1) trained on the same fraction of the data. MF significantly outperforms ORF-Saffari trained on the same fraction of training data. In batch RF versions, the same training data can be used to evaluate candidate splits at a node and its children. However, in the online RF versions (ORF-Saffari and ORF-Denil), incoming training examples are used to evaluate candidate splits just at a current leaf node and new training data are required to evaluate candidate splits every time a new leaf node is created. Saffari et al. [15] recommend multiple passes through the training data to increase the effective number of training samples. In a realistic streaming data setup, where training examples cannot be stored for multiple passes, MF would require significantly fewer examples than ORF-Saffari to achieve the same accuracy.\nComparing test accuracy vs training time on usps, satimages and letter datasets, we observe that MF is at least an order of magnitude faster than re-trained batch versions and ORF-Saffari. For ORF-Saffari, we plot test accuracy at the end of every additional pass; hence it contains additional markers compared to the top row which plots results after a single pass. Re-training batch RF using 100 mini-batches is unfair to MF; in a streaming data setup where the model is updated when a new training instance arrives, MF would be significantly faster than the re-trained batch versions. Assuming trees are balanced after adding each data point, it can be shown that computational complexity of MF scales as O(N logN) whereas the re-trained batch RF scales as O(N2 logN) (Appendix C).\nIt is remarkable that choosing splits independent of labels achieves competitive classification performance. This phenomenon has been observed by others as well—for example, Cutler and Zhao [6] demonstrate that their PERT classifier (which is similar to batch version of MF) achieves test accuracy comparable to Breiman-RF on many real world datasets. However, in the presence of irrelevant features, methods which choose splits independent of labels (MF, ERT-1) perform worse than Breiman-RF and ERT-k (but still better than ORF-Saffari) as indicated by the results on the dna dataset. We trained MF and ERT-1 using just the most relevant 60 attributes amongst the 180 attributes5—these results are indicated as MF† and ERT-1† in Figure 3. We observe that, as expected, filtering out irrelevant features significantly improves performance of MF and ERT-1."
    }, {
      "heading" : "8 Discussion",
      "text" : "We have introduced Mondrian forests, a new random forest variant which can be trained incrementally in an efficient manner. MF significantly outperforms existing online random forests in terms of training time as well as number of training instances required to achieve a\n4https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names 5https://www.sgi.com/tech/mlc/db/DNA.names\nparticular test accuracy. Remarkably, MF achieves competitive test accuracy to batch random forests trained on the same fraction of the data. MF is unable to handle lots of irrelevant features (since splits are chosen independent of the labels)—one way to use labels to guide splits is via recently proposed Sequential Monte Carlo algorithm for decision trees [11]. The computational complexity of MF is linear in the number of dimensions (since rectangles are represented explicitly) which could be expensive for high dimensional data; we will address this limitation in future work. Random forests have been tremendously influential in machine learning for a variety of tasks; hence lots of other interesting extensions of this work are possible, e.g. MF for regression, theoretical bias-variance analysis of MF, extensions of MF that use hyperplane splits instead of axis-aligned splits."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Charles Blundell, Gintare Dziugaite, Creighton Heaukulani, José Miguel Hernández-Lobato, Maria Lomeli, Alex Smola, Heiko Strathmann, and Srini Turaga for helpful discussions and feedback on drafts. BL gratefully acknowledges generous funding from the Gatsby Charitable Foundation. This research was carried out in part while DMR held a Research Fellowship at Emmanuel College, Cambridge, with funding also from a Newton International Fellowship through the Royal Society. YWT’s research leading to these results has received funding from the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617411."
    }, {
      "heading" : "A Posterior inference and prediction using the HNSP",
      "text" : "Recall that we use a hierarchical Bayesian approach to specify a smooth label distribution pT (y|x,D1:N ) for each tree T . The label prediction at a test point x will depend on where x falls relative to the existing data in the tree T . In this section, we assume that x lies within one of the leaf nodes in T , i.e., x ∈ Bxleaf(x), where leaf(x) ∈ leaves(T ). If x does not lie within any of the leaf nodes in T , i.e., x /∈ ∪j∈leaves(T )Bxj , one could extend the tree by sampling T ′ from MTx(λ, T,x), such that x lies within a leaf node in T ′ and apply the procedure described below using the extended tree T ′. Appendix B describes this case in more detail.\nGiven training data D1:N , a Mondrian tree T and the hierarchical prior over G, the predictive label distribution pT (y|x,D1:N ) is obtained by integrating over G, i.e.\npT (y|x,D1:N ) = EG∼pT (G|D1:N )[pT (y|x,G)] = EG∼pT (G|D1:N )[Gleaf(x),y] = Gleaf(x),y. (6)\nHence, the prediction is given by Gleaf(x), the posterior mean at leaf(x). The posterior mean Gleaf(x) can be computed using existing techniques, which we review in the rest of this section. Posterior inference in the HNSP is a special case of posterior inference in hierarchical PYP (HPYP). Teh [17] considers the HPYP with multinomial likelihood (in the context of language modeling)—the model considered here (HNSP with multinomial likelihood) is a special case of [17]. Hence, we just sketch the high level picture and refer the reader to [17] for further details. We first describe posterior inference given N data points D1:N (batch setting), and later explain how to adapt inference to the online setting. Finally, we describe the computation of the predictive posterior distribution.\nBatch setting\nPosterior inference is done using the Chinese restaurant process representation, wherein every node of the decision tree is a restaurant; the training data points are the customers seated in the tables associated with the leaf node restaurants; these tables are in turn customers at the tables in their corresponding parent level restaurant; the dish served at each table is the class label. Exact inference is intractable and hence we resort to approximations. In particular, we use the approximation known as the interpolated Kneser-Ney (IKN) smoothing, a popular smoothing technique for language modeling [10]. The IKN smoothing can be interpreted as an approximate inference scheme for the HPYP, where the number of tables serving a particular dish in a restaurant is at most one [17]. More precisely, if cj,k denotes the number of customers at restaurant j eating dish k and tabj,k denotes the number of tables at restaurant j serving dish k, the IKN approximation sets tabj,k = min(cj,k, 1). The counts cj,k and tabj,k can be computed in a single bottom-up pass as follows: for every leaf node j ∈ leaves(T), cj,k is simply the number of training data points with label k at node j; for every internal node j ∈ T \\ leaves(T), we set cj,k = tableft(j),k + tabright(j),k. For a leaf node j, this procedure is summarized in Algorithm 5. (Note that this pseudocode just serves as a reference; in practice, these counts are updated in an online fashion, as described in Algorithm 6.)\nPosterior inference: online setting\nIt is straightforward to extend inference to the online setting. Adding a new data point (x, y) affects only the counts along the path from the root to the leaf node of that data point. We update the counts in a bottom-up fashion, starting at the leaf node containing the data point,\nAlgorithm 5 InitializePosteriorCounts(j)\n1: For all k, set cjk = #{i ∈ N(j) : yi = k} 2: Initialize j′ = j 3: while True do 4: if j′ /∈ leaves(T) then 5: For all k, set cj′k = tableft(j′),k + tabright(j′),k\n6: For all k, set tabj′k = min(cj′k, 1) . IKN approximation 7: if j′ = then 8: return 9: else 10: j′ ← parent(j′)\nleaf(x). Due to the nature of the IKN approximation, we can stop at the internal node j where cj,y = 1 and need not traverse up till the root. This procedure is summarized in Algorithm 6.\nAlgorithm 6 UpdatePosteriorCounts(j, y)\n1: cjy ← cjy + 1 2: Initialize j′ = j 3: while True do 4: if tabj′y = 1 then . none of the counts above need to be updated 5: return 6: else 7: if j′ /∈ leaves(T) then 8: cj′y = tableft(j′),y + tabright(j′),y"
    }, {
      "heading" : "9: tabj′y = min(cj′y, 1) . IKN approximation",
      "text" : "10: if j′ = then 11: return 12: else 13: j′ ← parent(j′)\nPredictive posterior computation Given the counts cj,k and table assignments tabj,k, the predictive probability (i.e., posterior mean) at node j can be computed recursively as follows:\nGjk =\n{ cj,k−djtabj,k\ncj,· + djtabj,· cj,· Gparent(j),k cj,· > 0,\nGparent(j),k cj,· = 0, (7)\nwhere cj,· = ∑ k cj,k, tabj,· = ∑ k tabj,k, and dj := exp ( −γ(τj − τparent(j)) ) is the “discount” for node j, defined in Section 4. Informally, the discount interpolates between the counts c and the prior. If the discount dj ≈ 1, then Gj is more like its parent Gparent(j). If dj ≈ 0, then Gj weights the counts more. These predictive probabilities can be computed in a single top-down pass as shown in Algorithm 7."
    }, {
      "heading" : "B Prediction using Mondrian tree",
      "text" : "Let x denote a test data point. We are interested in the predictive probability of y at x, denoted by pT (y|x,D1:N ). As in typical decision trees, the process involves a top-down tree traversal, starting from the root. If x is already “contained” in the tree T , i.e., if x ∈ Bxj\nAlgorithm 7 ComputePosteriorPredictiveDistribution ( T,G ) 1: . Description of top-down pass to compute posterior predictive distribution given by (7) 2: . Gjk denotes the posterior probability of y = k at node j 3: Initialize the ordered set J = { } 4: while J not empty do 5: Pop the first element of J 6: if j = then 7: Gparent( ) = H\n8: Set d = exp ( −γ(τj − τparent(j)) ) 9: For all k, set Gjk = c −1 j,· ( cj,k − d tabj,k + d tabj,· Gparent(j),k\n) 10: if j /∈ leaves(T) then 11: Append left(j) and right(j) to the end of the ordered set J\nfor some leaf j ∈ leaves(T ), then the prediction is taken to be Gleaf(x), which is computed as described in Appendix A. Otherwise, we somehow need to incorporate x. One choice is to extend T by sampling T ′ from MTx(λ, T,D1:n,x) as described in Algorithm 3, and set the prediction to Gj , where j ∈ leaves(T ′) is the leaf node containing x. A particular extension T ′ might lead to an overly confident prediction; hence, we average over every possible extension T ′. This expectation can be carried out analytically, using properties of the Mondrian process, as we show below.\nLet ancestors(j) denote the set of all ancestors of node j. Let path(j) = {j} ∪ ancestors(j), that is, the set of all nodes along the ancestral path from j to the root. Recall that leaf(x) is the unique leaf node in T such that x ∈ Bleaf(x). If the test point x ∈ Bxleaf(x) (i.e., x lies within the ‘gray rectangle’ at the leaf node), it can never branch off; else, it can branch off at one or more points along the path from the root to leaf(x). More precisely, if x lies outside Bxj at node j, the probability that x will branch off into its own node at node j, denoted by 6 psj(x), is equal to the probability that a split exists in Bj outside B x j , which is\npsj(x) = 1− exp ( −∆jηj(x) ) , where ηj(x) = ∑ d ( max(xd − uxjd, 0) + max(`xjd − xd, 0) ) ,\nand ∆j = τj − τparent(j). Note that psj(x) = 0 if x lies within Bxj (i.e., if `xjd ≤ xd ≤ uxjd for all d). The probability of x not branching off before reaching node j is given by∏ j′∈ancestors(j)(1− psj′(x)).\nIf x ∈ Bxleaf(x), the prediction is given by Gleaf(x). If there is a split in Bj outside B x j , let ̃ denote the new parent of j and child(̃) denote the child node containing just the test data point,; in this case, the prediction is Gchild(̃). Averaging over the location where the test point branches off, we obtain\npT (y|x,D1:N ) = ∑\nj∈path(leaf(x)) ( ∏ j′∈ancestors(j) (1− psj′(x)) ) Fj(x), (8)\nwhere\nFj(x) = p s j(x)E∆̃ [ Gchild(̃) ] + 1[j = leaf(x)](1− psj(x))Gleaf(x). (9)\nThe second term in Fj(x) needs to be computed only for the leaf node leaf(x) and is simply the posterior mean of Gleaf(x) weighted by 1− psleaf(x)(x). The posterior mean of Gleaf(x), given\n6The superscript s in psj(x) is used to denote the fact that this split ‘separates’ the test data point x into its own leaf node.\nby Gleaf(x), can be computed using (7). The first term in Fj(x) is simply the posterior mean of Gchild(̃), averaged over ∆̃, weighted by p s j(x). Since no labels are observed in child(̃), cchild(̃),· = 0, hence from (7), we have Gchild(̃) = G̃. We compute G̃ using (7). We average over ∆̃ due to the fact that the discount in (7) for the node ̃ depends on τ̃ − τparent(̃) = ∆̃. To average over all valid split times τ̃, we compute expectation w.r.t. ∆̃ which is distributed according to a truncated exponential with rate ηj(x), truncated to the interval [0,∆j ].\nThe procedure for computing pT (y|x,D1:N ) for any x ∈ RD is summarized in Algorithm 8.\nAlgorithm 8 Predict ( T,x ) 1: . Description of prediction using a Mondrian tree, given by (8) 2: Initialize j = and pNotSeparatedYet = 1 3: Initialize s = 0K . s is K-dimensional vector where sk = pT (y = k|x,D1:N ) 4: while True do 5: Set ∆j = τj − τparent(j) and ηj(x) = ∑ d ( max(xd − uxjd, 0) + max(`xjd − xd, 0)\n) 6: Set psj(x) = 1− exp ( −∆jηj(x)\n) 7: if psj(x) > 0 then 8: . Let x branch off into its own node child(̃), creating a new node ̃ which is the\nparent of j and child(̃). Gchild(̃) = G̃ from (7) since cchild(̃),· = 0. 9: Compute expected discount d̄ = E∆[exp(−γ∆)] where ∆ is drawn from a truncated\nexponential with rate ηj(x), truncated to the interval [0,∆j ]. 10: For all k, set c̃,k = tab̃,k = min(cj,k, 1) 11: For all k, set G̃k = c −1 ̃,· ( c̃,k − d̄ tab̃,k + d̄ tab̃,· Gparent(̃),k ) . Algorithm 7 and\n(9) 12: For all k, update sk ← sk + pNotSeparatedYet psj(x)G̃k 13: if j ∈ leaves(T) then 14: For all k, update sk ← sk + pNotSeparatedYet(1− psj(x))Gjk . Algorithm 7 and (9) 15: return ŷ = argmaxk sk 16: else 17: pNotSeparatedYet ← pNotSeparatedYet(1− psj(x)) 18: if xδj ≤ ξj then j ← left(j) else j ← right(j) . recurse to the child where x lies"
    }, {
      "heading" : "C Computational complexity",
      "text" : "We discuss the computational complexity associated with a single Mondrian tree. The complexity of a forest is simply M times that of a single tree; however, this computation can be trivially parallelized since there is no interaction between the trees. Assume that the N data points are processed one by one. Assuming the data points form a balanced binary tree after each update, the computational cost of processing the nth data point is at most O(log n) (add the data point into its own leaf, update posterior counts for HNSP in bottom-up pass from leaf to root). The overall cost to process N data points is O( ∑N n=1 log n) = O(logN !), which for large N tends to O(N logN) (using Stirling approximation for the factorial function). For offline RF and ERT, the expected complexity with n data points is O(n log n). The complexity of the re-trained version is O( ∑N n=1 n log n) = O(log ∏N n=1 n\nn), which for large N tends to O(N2 logN) (using asymptotic expansion of the hyper factorial function)."
    }, {
      "heading" : "D Pseudocode for paused Mondrians",
      "text" : "Algorithm 9 SampleMondrianBlock ( j,DN(j), λ ) version that depends on labels\n1: Add j to T 2: ∀d, set `xjd = min(XN(j),d), uxjd = max(XN(j),d) . dimension-wise min and max 3: if AllLabelsIdentical(YN(j)) then 4: Set τj = λ . pause Mondrian 5: else 6: Sample E from exponential distribution with rate ∑ d(ujd − `jd) 7: Set τj = τparent(j) + E\n8: if τj < λ then 9: Sample δj with probability of choosing d proportional to u x jd − `xjd 10: Sample split location ξj along dimension δj from an uniform distribution over U [`xjd, uxjd] 11: Set N(left(j)) = {n ∈ N(j) : xnδj ≤ ξj} and N(right(j)) = {n ∈ N(j) : xnδj > ξj} 12: SampleMondrianBlock ( left(j),DN(left(j)), λ\n) 13: SampleMondrianBlock ( right(j),DN(right(j)), λ\n) 14: else 15: Set τj = λ and add j to leaves(T) . j is a leaf node 16: InitializePosteriorCounts(j) . Algorithm 5\nAlgorithm 10 ExtendMondrianBlock(T, λ, j,D) version that depends on labels 1: if AllLabelsIdentical(YN(j)) then . paused Mondrian leaf 2: Update extent `xj ← min(`xj ,x),uxj ← max(uxj ,x) 3: Append D to DN(j) . append x to XN(j) and y to YN(j) 4: if y = unique(YN(j)) then 5: UpdatePosteriorCounts(j, y) . Algortithm 6 6: return . continue pausing 7: else 8: Remove j from leaves(T) 9: SampleMondrianBlock ( j,DN(j), λ ) . un-pause Mondrian\n10: else 11: Set e` = max(`xj − x, 0) and eu = max(x− uxj , 0) . e` = eu = 0D if x ∈ Bxj 12: Sample E ∼ Exp( ∑ d(e ` d + e u d)) 13: if E + τparent(j) < τj then . introduce new parent for node j 14: Create new Mondrian block ̃ where `x̃ = min(` x j ,x) and u x ̃ = max(u x j ,x) 15: Sample δ̃ with Pr(δ̃ = d) proportional to e ` d + e u d 16: if xδ̃ > u x j,δ̃ , then sample ξ̃ from U [uxj,δ̃ , xδ̃ ], else sample ξ̃ from U([xδ̃ , ` x j,δ̃\n]) 17: if j = then . set ̃ as the new root 18: ← ̃ 19: else . set ̃ as child of parent(j) 20: if j = left(parent(j)), then left(parent(j))← ̃, else right(parent(j))← ̃ 21: if xδ̃ > ξ̃ then 22: Set left(̃) = j and SampleMondrianBlock ( right(̃),D, λ ) . create new leaf for x 23: else 24: Set right(̃) = j and SampleMondrianBlock ( left(̃),D, λ ) . create new leaf for x\n25: else 26: Update `xj ← min(`xj ,x),uxj ← max(uxj ,x) . update extent of node j 27: if j /∈ leaves(T) then . return if j is a leaf node, else recurse down the tree 28: if xδj ≤ ξj then child(j) = left(j) else child(j) = right(j) 29: ExtendMondrianBlock(T, λ, child(j),D) . recurse on child containing x"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "<lb>Ensembles of randomized decision trees, usually referred to as random forests, are<lb>widely used for classification and regression tasks in machine learning and statistics.<lb>Random forests achieve competitive predictive performance and are computationally<lb>efficient to train and test, making them excellent candidates for real-world prediction<lb>tasks. The most popular random forest variants (such as Breiman’s random forest and<lb>extremely randomized trees) operate on batches of training data. Online methods are<lb>now in greater demand. Existing online random forests, however, require more training<lb>data than their batch counterpart to achieve comparable predictive performance. In<lb>this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of<lb>random decision trees we call Mondrian forests. Mondrian forests can be grown in an<lb>incremental/online fashion and remarkably, the distribution of online Mondrian forests<lb>is the same as that of batch Mondrian forests. Mondrian forests achieve competitive<lb>predictive performance comparable with existing online random forests and periodically<lb>re-trained batch random forests, while being more than an order of magnitude faster,<lb>thus representing a better computation vs accuracy tradeoff.",
    "creator" : "LaTeX with hyperref package"
  }
}