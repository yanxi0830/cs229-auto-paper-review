{
  "name" : "1509.06095.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MULTILAYER BOOTSTRAP NETWORK FOR UNSUPERVISED SPEAKER RECOGNITION",
    "authors" : [ "Xiao-Lei Zhang" ],
    "emails" : [ "xiaolei.zhang9@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— multilayer bootstrap network, speaker recognition, unsupervised learning."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Speaker recognition aims to identify speakers from their voices. It is important in many speech systems, such as speaker diarization, language recognition, and speech recognition. Supervised methods include maximum a posteriori estimation [1, 2], linear discriminative analysis (LDA) [3, 4], support vector machines [2], deep neural networks [5, 6], etc.\nBecause constructing a manually-labeled corpus is laboring intensive and time-consuming, it is strongly needed to develop unsupervised speaker recognition methods. Existing methods mainly include principle component analysis (PCA), k-means clustering, Gaussian mixture model (GMM), agglomerative hierarchical clustering, and joint factor analysis. For example, Wooters and Huijbregts [7] used agglomerative clustering to merge speaker segments by Bayesian information criterion. Iso [8] used vector quantization to encode speech segments and used spectral clustering, which is a kmeans clustering applied to a low-dimensional subspace of data, for speaker recognition. Nwe et al. [9] used a group of GMM clusterings to improve individual base GMM clusterings. Some methods apply clustering techniques, e.g. variational Bayesian expectation-maximization (EM) GMM [10] and spectral clustering [11], to a low-dimensional total variability subspace [4] that is learned from high-dimensional supervectors by joint factor analysis [4]. Some methods compensate the total variability space with new items, e.g. [12].\nBecause little prior knowledge of data is known beforehand, an unsupervised method should satisfy the following conditions: (i) no need for manually-labeled training data; (ii) no hyperparameter tunning for a satisfied performance; and (iii) robustness to different data or modeling conditions. Due to these strict requirements, unsupervised speaker recognition is a very difficult task. In this paper, we present a multilayer bootstrap network (MBN) [13] based algorithm. MBN is a recent proposed unsupervised nonlinear dimensionality reduction algorithm. Experimental results show that the proposed method satisfies these requirements.\nThis paper is organized as follows. In Section 2, we present the MBN-based system. In Section 3, we present the MBN algorithm and its typical hyperparameter setting. In Section 4, we present the relationship between MBN and deep learning. In Section 5, we report comparison results. In Section 6, we conclude this paper."
    }, {
      "heading" : "2. SYSTEM",
      "text" : "Given an unlabeled speaker recognition corpus, we propose the following unsupervised algorithm:1\n• The first step trains a speaker- and session-independent unsupervised universal background model (UBM) [1] from an acoustic feature, which produces a ddimensional supervector for each utterance, denoted as x = [nT , fT ]T where n is the accumulation of the mixture occupation over all frames of the utterance and f is the vector form of the centered first order statistics.\n• The second step reduces the dimension of x from d to d̄ (d̄ d) by multilayer bootstrap network (MBN) which is introduced in Section 3.\n• The third step conducts k-means clustering on the lowdimensional data if the number of the underlying speakers is known, or agglomerative clustering if the number of the speakers is unknown.\n1The source code is downloadable from http://sites.google.com/site/ zhangxiaolei321/speaker_recognition\nar X\niv :1\n50 9.\n06 09\n5v 1\n[ cs\n.L G\n] 2\n1 Se\np 20\n15"
    }, {
      "heading" : "3. MULTILAYER BOOTSTRAP NETWORK",
      "text" : "The structure of MBN [13] is shown in Fig. 1. MBN is a multilayer localized PCA algorithm that gradually enlarges the area of a local region implicitly from the bottom hidden layer to the top hidden layer by high-dimensional sparse coding, and gets a low-dimensional feature explicitly by PCA at the output layer.\nEach hidden layer of MBN consists of a group of mutually independent k-centers clusterings. Each k-centers clustering has k output units, each of which indicates one cluster. The output units of all clusterings are concatenated as the input of their upper layer [13].\nMBN is trained layer-by-layer from bottom up. For training a hidden layer given a d-dimensional input X = {x1, . . . ,xn}, MBN trains each clustering independently [13]:\n• Random feature selection. The first step randomly selects d̂ dimensions of X (d̂ ≤ d) to form a new set X̂ = {x̂1, . . . , x̂n}. This step is controlled by a hyperparameter a = d̂/d.\n• Random sampling. The second step randomly selects k data points from X̂ as the k centers of the clustering, denoted as {w1, . . . ,wk}. This step is controlled by a hyperparameter k.\n• Random reconstruction. The third step randomly selects d′ dimensions of the k centers (d′ ≤ d̂/2) and does a one-step cyclic-shift as shown in Fig. 2. This step is controlled by a hyperparameter r = d′/d̂.\n• Sparse representation learning. The fourth step assigns the input x̂ to one of the k clusters and outputs a k-dimensional indicator vector h = [h1, . . . , hk]T . For example, if x̂ is assigned to the second cluster, then h = [0, 1, 0, . . . , 0]T . The assignment is calculated according to the similarities between x̂ and the k centers, in terms of some predefined similarity measurement at the bottom layer, such as the minimum squared loss arg minki=1 ‖wi− x̂‖2, or in terms of arg maxki=1 wTi x̂ at all other hidden layers [13]."
    }, {
      "heading" : "3.1. A typical hyperparameter setting",
      "text" : "MBN has five hyperparameters { V,L, {kl}Ll=1, a, r } where V is the number of k-centers clusterings per layer, L is the number of hidden layers, and kl is the hyperparameter k at the lth hidden layer. As shown in [13], MBN is robust to hyperparameter selection. Here we introduce a typical setting:\n• Setting hyperparameter k. (i) k1 should be as large as possible, i.e. k1 → n. Suppose the largest k supported by hardware is kmax, then k1 = min(0.9n, kmax). (ii) kl decays with a factor of, e.g. 0.5, with the increase of hidden layers. That is to say, kl = 0.5kl−1. (iii) kL should be larger than the number of speakers c. Typically, kL ≈ 1.5c. If c is unknown, we simply set kL to a relatively large number, e.g. 30, since c is unlikely larger than 30 in a practical dialog.\n• Setting hyperparameter r. When a problem is smallscale, e.g. k1 > 0.8n, then r = 0.5; otherwise, r = 0.\n• Setting other hyperparameters. Hyperparameter V should be at least larger than 100, typically V = 400. Hyperparameter a is fixed to 0.5. Hyperparameter L is determined by k."
    }, {
      "heading" : "4. RELATED WORK",
      "text" : "The proposed method learns multilayer nonlinear transforms, which is related to deep learning (a.k.a., multilayer neural networks)—a recent advanced topic in many speech processing fields, e.g. speaker recognition [5, 6], speech recognition [14], speech separation and enhancement [15–18], speech synthesis [19], and voice activity detection [20, 21]. The aforementioned deep learning methods are all supervised ones and limited to neural networks, while the proposed method is an unsupervised one and different from neural networks."
    }, {
      "heading" : "5. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "5.1. Experimental setup",
      "text" : "We used the training corpus of speech separation challenge (SSC) [22]. The training corpus contains 34 speakers, each\nof which has 500 clean utterances. We selected the first 100 utterances (a.k.a, sessions) of each speaker for evaluation, which amounts to 3400 utterances. We set the frame length to 25 milliseconds and frame shift to 10 milliseconds, and extract a 25-dimensional MFCC feature.\nFor the proposed MBN-based speaker recognition, we adopted the typical parameter setting of MBN. Specifically, V = 400, a = 0.5, r = 0.5, and k were set to 3060-1530-765-382-191-95. The output of PCA was set to {2, 3, 5, 10, 30, 50} dimensions respectively. We assumed that the number of speakers was known, and used k-means clustering for clustering the low-dimensional data.\nWe compared with PCA, k-means clustering, and an LDA-based system, where the first two methods are unsupervised and the third one is supervised. For the PCA-based method, we first used the same UBM as the MBN-based method to extract high-dimensional supervectors, then reduced the dimension of the supervectors to {2, 3, 5, 10, 30, 50} respectively, and finally evaluated the low-dimensional output of PCA by k-means clustering. For the k-means-clusteringbased method, we apply k-means clustering to the highdimensional supervectors directly.\nThe LDA-based system2 uses UBM to extract a highdimensional feature, then uses joint factor analysis to reduce the high-dimensional feature to an intermediately low dimensional representation in an unsupervised way, and finally uses LDA, a supervised dimensionality reduction method, to reduce the intermediate representation to a low-dimensional subspace where classification is conducted by a probabilistic LDA algorithm. Since factor analysis is an unsupervised dimensionality reduction method, we set its output to {2, 3, 5, 10, 30, 50} dimensions respectively for comparison. We constructed a training set from the SSC corpus for this supervised method: each speaker consists of 100 training utterances, which are selected from the 400 remaining utterances of the speaker.\nThe performance was measured by normalized mutual information (NMI) [23]. MNI was proposed to overcome the label indexing problem between the ground-truth labels and the predicted labels. It is one of the standard evaluation metrics of unsupervised learning. The higher the NMI is, the better the performance is. We also report the classification accuracy of the LDA-based system in the Supplementary Material3 where we can see that NMI is consistent with classification accuracy."
    }, {
      "heading" : "5.2. Results",
      "text" : "Because all comparison methods use UBM to extract speakerand session-independent supervectors, we need to study how they behave in different UBM settings, in terms of mixture number and expectation-maximization (EM) iterations. (i)\n2The source code is downloadable from http://research.microsoft.com/enus/downloads/a6262fec-03a7-4060-a08c-0b0d037a3f5b/\n3http://sites.google.com/site/ zhangxiaolei321/speaker_recognition\nThe mixture number of UBM reflects the capacity of UBM for modelling an underlying data distribution: if the mixture number of UBM is smaller than the number of speakers, UBM is likely underfitting, i.e. it cannot grasp the data distribution well. To study this effect, we set the mixture number of UBM to {1, 2, 4, 8, 16, 32, 64} respectively. (ii) The number of EM iterations of UBM reflects the quality of the acoustic feature produced by UBM: if the EM optimization is not sufficient, the acoustic feature is noisy. To study this effect, we set the number of EM iterations of UBM to {0, 20} respectively, where setting the number of iterations to 0 means that UBM is initialized with randomly sampled means without EM optimization, which is the worst case.\nFig. 3 and Supplementary-Fig. 1 give a comparison example between PCA and MBN in visualizing the first 10 speakers, where a 16-mixtures UBM with 20 and 0 EM iteration are used to generate their inputs respectively. From the figures, we can see that MBN produces ideal visualizations.\nFig. 4 reports results with respect to the mixture number of UBM. Fig. 5 reports results with respect to the number of output dimensions. Supplementary-Tables 1 and 3 report the detailed results of the two figures. From the figures and tables, we observe the following phenomena: (i) the MBNbased method outperforms the PCA- and k-means-clusteringbased methods and approaches to the supervised LDA system in all cases; (ii) the MBN-based method is less sensitive to different parameter settings of both UBM and MBN itself; (iii) the LDA-based system is less sensitive to the mixture number of UBM, but sensitive to the number of output dimensions; (iv) the PCA-based method is sensitive to both the mixture number of UBM and the number of output dimensions, and strongly relies on the effectiveness of UBM; (v) the performance of the k-means-clustering-based method is consistent with that of the PCA-based method.\nFig. 6 reports results of the MBN-based method with respect to the number of hidden layers. From the figure, we ob-\nserve that the accuracy improves gradually with the increase of the number of hidden layers."
    }, {
      "heading" : "6. CONCLUSIONS",
      "text" : "In this paper, we have proposed a multilayer bootstrap network based unsupervised speaker recognition algorithm. The method first uses UBM to extract a high-dimensional feature from the original MFCC acoustic feature, then uses MBN to reduce the high-dimensional feature to a low-dimensional space, and finally clustering the low-dimensional data. We have compared it with the PCA-, k-means-clustering-, and LDA-based methods, where the first two methods are unsupervised and the third method is supervised. Experimental results have shown that the proposed method outperforms\nthe unsupervised methods and approaches to the supervised method. Moreover, it is insensitive to different parameter settings of UBM and MBN, which facilitates its practical use."
    }, {
      "heading" : "7. ACKNOWLEDGEMENT",
      "text" : "The author thanks Prof DeLiang Wang for providing the Ohio Computing Center and Dr Ke Hu for helping with the SSC corpus."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Douglas A Reynolds, Thomas F Quatieri, and Robert B Dunn, “Speaker verification using adapted gaussian mixture models,” Digital Signal Process., vol. 10, no. 1, pp. 19–41, 2000.\n[2] William M Campbell, Douglas E Sturim, Douglas A Reynolds, and Alex Solomonoff, “SVM based speaker verification using a GMM supervector kernel and NAP variability compensation,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2006, vol. 1, pp. 97– 100.\n[3] Patrick Kenny, Gilles Boulianne, Pierre Ouellet, and Pierre Dumouchel, “Joint factor analysis versus eigenchannels in speaker recognition,” IEEE Trans. Audio, Speech, Lang. Process., vol. 15, no. 4, pp. 1435–1447, 2007.\n[4] Najim Dehak, Patrick Kenny, Réda Dehak, Pierre Dumouchel, and Pierre Ouellet, “Front-end factor analysis for speaker verification,” IEEE Trans. Audio, Speech, Lang. Process., vol. 19, no. 4, pp. 788–798, 2011.\n[5] Ke Chen and Ahmad Salman, “Learning speakerspecific characteristics with a deep neural architecture,” IEEE Trans. Neural Netw., vol. 22, no. 11, pp. 1744– 1756, 2011.\n[6] Xiaojia Zhao, Yuxuan Wang, and DeLiang Wang, “Cochannel speaker identification in anechoic and reverberant conditions,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 22, no. 11, pp. 1727–1736, 2015.\n[7] Chuck Wooters and Marijn Huijbregts, “The ICSI RT07s speaker diarization system,” in Multimodal Technologies for Perception of Humans, pp. 509–519. Springer, 2008.\n[8] Ken-ichi Iso, “Speaker clustering using vector quantization and spectral clustering,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2010, pp. 4986–4989.\n[9] Tin Lay Nwe, Hanwu Sun, Bin Ma, and Haizhou Li, “Speaker clustering and cluster purification methods for rt07 and rt09 evaluation meeting data,” IEEE Trans. Audio, Speech, Lang. Process., vol. 20, no. 2, pp. 461–473, 2012.\n[10] Stephen H Shum, Najim Dehak, Réda Dehak, and James R Glass, “Unsupervised methods for speaker diarization: An integrated and iterative approach,” IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 10, pp. 2015–2028, 2013.\n[11] Naohiro Tawara, Tetsuji Ogawa, and Tetsunori Kobayashi, “A comparative study of spectral clustering for i-vector-based speaker clustering under noisy conditions,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2015, pp. 2041–2045.\n[12] Kui Wu, Yan Song, Wu Guo, and Lirong Dai, “Intraconversation intra-speaker variability compensation for speaker clustering,” in Proc. Int. Sym. Chinese Spoken Lang. Process., 2012, pp. 330–334.\n[13] Xiao-Lei Zhang, “Nonlinear dimensionality reduction of data by multilayer bootstrap networks,” arXiv preprint arXiv:1408.0848, pp. 1–20, 2014.\n[14] George E Dahl, Dong Yu, Li Deng, and Alex Acero, “Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,” IEEE Trans. Audio, Speech, Lang. Process., vol. 20, no. 1, pp. 30–42, 2012.\n[15] Yuxuan Wang and DeLiang Wang, “Towards scaling up classification-based speech separation,” IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 7, pp. 1381– 1390, 2013.\n[16] Yong Xu, Jun Du, Li-Rong Dai, and Chin-Hui Lee, “A regression approach to speech enhancement based on deep neural networks,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 1, pp. 7–19, 2015.\n[17] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, and Paris Smaragdis, “Joint optimization of masks and deep recurrent neural networks for monaural source separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 12, pp. 2136–2147, 2015.\n[18] Xiao-Lei Zhang and DeLiang Wang, “Deep ensemble learning for monaural speech separation,” Tech. Rep. OSU-CISRC-8/15-TR13, Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA, 2015.\n[19] Zhen-Hua Ling, Li Deng, and Dong Yu, “Modeling spectral envelopes using restricted boltzmann machines and deep belief networks for statistical parametric speech synthesis,” IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 10, pp. 2129–2139, 2013.\n[20] Xiao-Lei Zhang and Ji Wu, “Deep belief networks based voice activity detection,” IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 4, pp. 697–710, 2013.\n[21] Xiao-Lei Zhang and DeLiang Wang, “Boosting contextual information for deep neural network based voice activity detection,” Tech. Rep. OSU-CISRC-5/15-TR06, Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA, 2015.\n[22] Martin Cooke and Te-Won Lee, “Speech separation challenge,” http://staffwww. dcs.shef.ac.uk/people/M.Cooke/ SpeechSeparationChallenge.htm, 2006.\n[23] Alexander Strehl and Joydeep Ghosh, “Cluster ensembles—a knowledge reuse framework for combining multiple partitions,” J. Mach. Learn. Res., vol. 3, pp. 583–617, 2003."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We apply multilayer bootstrap network (MBN), a recent pro-<lb>posed unsupervised learning method, to unsupervised speaker<lb>recognition. The proposed method first extracts supervectors<lb>from an unsupervised universal background model, then re-<lb>duces the dimension of the high-dimensional supervectors by<lb>multilayer bootstrap network, and finally conducts unsuper-<lb>vised speaker recognition by clustering the low-dimensional<lb>data. The comparison results with 2 unsupervised and 1 su-<lb>pervised speaker recognition techniques demonstrate the ef-<lb>fectiveness and robustness of the proposed method.",
    "creator" : "LaTeX with hyperref package"
  }
}