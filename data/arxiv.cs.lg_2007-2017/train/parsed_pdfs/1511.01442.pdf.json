{
  "name" : "1511.01442.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Weighted Tree Automata Approximation by Singular Value Truncation",
    "authors" : [ "Guillaume Rabusseau", "Borja Balle", "Shay B. Cohen" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Probabilistic context-free grammars (PCFG) provide a powerful statistical formalism for modeling important phenomena occurring in natural language. In fact, learning and parsing algorithms for PCFG are now standard tools in natural language processing pipelines. Most of these algorithms can be naturally extended to the superclass of weighted context-free grammars (WCFG), and closely related models like weighted tree automata (WTA) and latent probabilistic context-free grammars (LPCFG). The complexity of these algorithms depends on the size of the grammar/automaton, typically controlled by the number of rules/states. Being able to control this complexity is essential in operations like parsing, which is typically executed every time the model is used to make a prediction. In this paper we present an algorithm that given a WTA with n states and a target number of states n̂ < n, returns a WTA with n̂ states that is a good approximation of the original automaton. This can be interpreted as a lowrank approximation method for WTA through the direct connection between number of states of a WTA and the rank of its associated Hankel matrix. This opens the door to reducing the complexity of algorithms working with WTA at the price of incurring a small, controlled amount of error in the output of such algorithms.\nOur techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al., 2010; Cohen et al., 2014), and subsequent investigations into low-rank spectral learning for predictive state representations (Kulesza et al., 2014, 2015) and approximate minimization of weighted automata (Balle et al., 2015). In spectral learning algorithms, data is used to reconstruct a finite block of a Hankel matrix and an SVD of such matrix then reveals a low-dimensional\nar X\niv :1\n51 1.\n01 44\n2v 1\n[ cs\n.L G\n] 4\nN ov\n2 01\n5\nspace where a linear regression recovers the parameters of the model. In contrast, our approach computes the SVD of the infinite Hankel matrix associated with a WTA. Our main result is an efficient algorithm for computing this singular value decomposition by operating directly on the WTA representation of the Hankel matrix; that is, without the need to explicitly represent this infinite matrix at any point. Section 2 presents the main ideas underlying our approach. Add a comment to this line An efficient algorithmic implementation of these ideas is discussed in Section 3, and a theoretical analysis of the approximation error induced by our minimization method is given in Section 4. Proofs of all results stated in the paper can be found in appendix.\nThe idea of speeding up parsing with (L)PCFG by approximating the original model with a smaller one was recently studied in (Cohen and Collins, 2012; Cohen et al., 2013a), where a tensor decomposition technique was used in order to obtain the minimized model. We compare that approach to ours in the experiments presented in Section 5, where both techniques are used to compute approximations to a grammar learned from a corpus of real linguistic data. It was observed in (Cohen and Collins, 2012; Cohen et al., 2013a) that a side-effect of reducing the size of a grammar learned from data was a slight improvement in parsing performance. The number of parameters in the approximate models is smaller, and as such, generalization improves. We show in our experimental section that our minimization algorithms have the same effect in certain parsing scenarios. In addition, our approach yields models which give lower perplexity on an unseen set of sentences, and provides a better approximation to the original model in terms of `2 distance. It is important to remark that in contrast with the tensor decompositions in (Cohen and Collins, 2012; Cohen et al., 2013a) which are susceptible to local optima problems, our approach resembles a power-method approach to SVD, which yields efficient globally convergent algorithms. Overall, we observe in our experiments that this renders a more stable minimization method than the one using tensor decompositions."
    }, {
      "heading" : "1.1 Notation",
      "text" : "For an integer n, we write [n] = {1, . . . , n}. We use lower case bold letters (or symbols) for vectors (e.g. v ∈ Rd1), upper case bold letters for matrices (e.g. M ∈ Rd1×d2) and bold calligraphic letters for third order tensors (e.g. T ∈ Rd1×d2×d3). Unless explicitly stated, vectors are by default column vectors. The identity matrix will be written as I. Given i1 ∈ [d1], i2 ∈ [d2], i3 ∈ [d3] we use v(i1), M(i1, i2), and T (i1, i2, i3) to denote the corresponding entries. The ith row (resp. column) of a matrix M will be noted M(i, :) (resp. M(:, i)). This notation is extended to slices across the three modes of a tensor in the straightforward way. If v ∈ Rd1 and v′ ∈ Rd2 , we use v ⊗ v′ ∈ Rd1·d2 to denote the Kronecker product between vectors, and its straightforward extension to matrices and tensors. Given a matrix M ∈ Rd1×d2 we use vec(M) ∈ Rd1·d2 to denote the column vector obtained by concatenating the columns of M. Given a tensor T ∈ Rd1×d2×d3 and matrices Mi ∈ Rdi×d ′ i for i ∈ [3], we define a tensor T (M1,M2,M3) ∈ Rd ′ 1×d ′ 2×d ′ 3 whose entries are given by\nT (M1,M2,M3)(i1, i2, i3) = ∑\nj1,j2,j3\nT (j1, j2, j3)M1(j1, i1)M2(j2, i2)M3(j3, i3) .\nThis operation corresponds to contracting T with Mi across the ith mode of the tensor for each i."
    }, {
      "heading" : "2 Approximate Minimization of WTA and SVD of Hankel Matrices",
      "text" : "In this section we present the first contribution of the paper. Namely, the existence of a canonical form for weighted tree automata inducing the singular value decomposition of the infinite Hankel matrix associated with the automaton. We start by recalling several definitions and well-known facts about WTA that will be used in the rest of the paper. Then we proceed to establish the existence of the canonical form, which we call the singular value tree automaton. Finally we indicate how removing the states in this canonical form that correspond to the smallest singular values of the Hankel matrix leads to an effective procedure for model reduction in WTA."
    }, {
      "heading" : "2.1 Weighted Tree Automata",
      "text" : "Let Σ be a finite alphabet. We use Σ? to denote the set of all finite strings with symbols in Σ with λ denoting the empty string. We write |x| to denote the length of a string x ∈ Σ?. The number of occurences of a symbol σ ∈ Σ in a string x ∈ Σ? is denoted by |x|σ.\nThe set of all rooted full binary trees with leafs in Σ is the smallest set TΣ such that Σ ⊂ TΣ and (t1, t2) ∈ TΣ for any t1, t2 ∈ TΣ. We shall just write T when the alphabet Σ is clear from the context. The size of a tree t ∈ T is denoted by size(t) and defined recursively by size(σ) = 0 for σ ∈ Σ, and size((t1, t2)) = size(t1) + size(t2) + 1; that is, the number of internal nodes in the tree. The depth of a tree t ∈ T is denoted by depth(t) and defined recursively by depth(σ) = 0 for σ ∈ Σ, and depth((t1, t2)) = max{depth(t1), depth(t2)} + 1; that is, the distance from the root of the tree to the farthest leaf. The yield of a tree t ∈ T is a string 〈t〉 ∈ Σ∗ defined as the left-to-right concatenation of the symbols in the leafs of t, and can be recursively defined by 〈σ〉 = σ, and 〈(t1, t2)〉 = 〈t1〉 · 〈t2〉. The total number of nodes (internal plus leafs) of a tree t is denoted by |t| and satisfies |t| = size(t) + |〈t〉|.\nLet Σ′ = Σ∪{∗}, where ∗ is a symbol not in Σ. The set of rooted full binary context trees is the set CΣ = {c ∈ TΣ′ | |〈c〉|∗ = 1}; that is, a context c ∈ CΣ is a tree in TΣ′ in which the symbol ∗ occurs exactly in one leaf. Note that because given a context c = (t1, t2) ∈ CΣ with t1, t2 ∈ TΣ′ the symbol ∗ can only appear in one of the t1 and t2, we must actually have c = (c′, t) or c = (t, c′) with c′ ∈ CΣ and t ∈ TΣ. The drop of a context c ∈ C is the distance between the root and the leaf labeled with ∗ in c, which can be defined recursively as drop(∗) = 0, drop((c, t)) = drop((t, c)) = drop(c) + 1.\nWe usually think as the leaf with the symbol ∗ in a context as a placeholder where the root of another tree or another context can be inserted. Accordingly, given t ∈ T and c ∈ C, we can define c[t] ∈ T as the tree obtained by replacing the occurence of ∗ in c with t. Similarly, given c, c′ ∈ C we can obtain a new context tree c[c′] by replacing the occurence of ∗ in c with c′. See Figure 1 for some illustrative examples.\nA weighted tree automaton (WTA) over Σ is a tuple A = 〈α,T , {ωσ}σ∈Σ〉, where α ∈ Rn is the vector of initial weights, T ∈ Rn×n×n is the tensor of transition weights, and ωσ ∈ Rn is the vector of terminal weights associated with σ ∈ Σ. The dimension n is the number of states of the automaton, which we shall sometimes denote by |A|. A WTAA = 〈α,T , {ωσ}〉 computes a function fA : TΣ → R assigning to each tree t ∈ T the number computed as fA(t) = α>ωA(t), where ωA(t) ∈ Rn is obtained recursively as ωA(σ) = ωσ, and ωA((t1, t2)) = T (I,ωA(t1),ωA(t2)) — note the matching of dimensions in this last expression since contracting a third order tensor with a matrix in the first mode and\nvectors in the second and third mode yields a vector. In many cases we shall just write ω(t) when the automaton A is clear from the context. While WTA are usually defined over arbitrary ranked trees, only considering binary trees does not lead to any loss of generality since WTA on ranked trees are equivalent to WTA on binary trees (see (Bailly et al., 2010) for references). Additionally, one could consider binary trees where each internal node is labelled, which leads to the definition of WTA with multiple transition tensors. Our results can be extended to this case without much effort, but we state them just for WTA with only one transition tensor to keep the notation manageable.\nAn important observation is that there exist more than one WTA computing the same function — actually there exist infinitely many. An important construction along these lines is the conjugate of a WTA A with n states by an invertible matrix Q ∈ Rn×n. If A = 〈α,T , {ωσ}〉, its conjugate by Q is AQ = 〈Q>α,T (Q−>,Q,Q), {Q−1ωσ}〉, where Q−> denotes the inverse of Q>. To show that fA = fAQ one applies an induction argument on depth(t) to show that for every t ∈ T one has ωAQ(t) = Q−1ωA(t). The claim is obvious for trees of zero depth σ ∈ Σ, and for t = (t1, t2) one has\nωAQ((t1, t2)) = (T (Q−>,Q,Q))(I,ωAQ(t1),ωAQ(t2)) = (T (Q−>,Q,Q))(I,Q−1ωA(t1), Q−1ωA(t2)) = T (Q−>,ωA(t1),ωA(t2)) = Q−1T (I,ωA(t1),ωA(t2)) ,\nwhere we just used some simple rules of tensor algebra. An arbitrary function f : T → R is called rational if there exists a WTA A such that f = fA. The number of states of the smallest such WTA is the rank of f — we shall set rank(f) = ∞ if f is not rational. A WTA A with fA = f and |A| = rank(f) is called minimal. Given any f : T → R we define its Hankel matrix as the infinite matrix Hf ∈ RC×T with rows indexed by contexts, columns indexed by trees, and whose entries are given by Hf (c, t) = f(c[t]). Note that given a tree t′ ∈ T there are exactly |t′| different ways of splitting t′ = c[t] with c ∈ C and t ∈ T. This implies that Hf is a highly redundant representation for f , and it turns out that this redundancy is the key to proving the following fundamental result about rational tree functions.\nTheorem 1 ((Bozapalidis and Louscou-Bozapalidou, 1983)). For any f : T → R we have rank(f) = rank(Hf )."
    }, {
      "heading" : "2.2 Rank Factorizations of Hankel Matrices",
      "text" : "The theorem above can be rephrased as saying that the rank of Hf is finite if and only if f is rational. When the rank of Hf is indeed finite — say rank(Hf ) = n — one can\nfind two rank n matrices P ∈ RC×n, S ∈ Rn×T such that Hf = PS. In this case we say that P and S give a rank factorization of Hf . We shall now refine Theorem 1 by showing that when f is rational, the set of all possible rank factorizations of Hf is in direct correpondance with the set of minimal WTA computing f .\nThe first step is to show that any minimal WTA A = 〈α,T , {ωσ}〉 computing f induces a rank factorization Hf = PASA. We build SA ∈ Rn×T by setting the column corresponding to a tree t to SA(:, t) = ωA(t). In order to define PA we need to introduce a new mapping ΞA : C→ Rn×n assigning a matrix to every context as follows: ΞA(∗) = I, ΞA((c, t)) = T (I,ΞA(c),ωA(t)), and ΞA((t, c)) = T (I,ωA(t),ΞA(c)). If we now define αA : C→ Rn as αA(c)> = α>ΞA(c), we can set the row of PA corresponding to c to be PA(c, :) = αA(c)>. With these definitions one can easily show by induction on drop(c) that ΞA(c)ωA(t) = ωA(c[t]) for any c ∈ C and t ∈ T. Then it is immediate to check that Hf = PASA:\nn∑ i=1 PA(c, i)SA(i, t) = αA(c)>ωA(t) = α>ΞA(c)ωA(t)\n= α>ωA(c[t]) = fA(c[t]) = Hf (c, t) . (1)\nAs before, we shall sometimes just write Ξ(c) and α(c) when A is clear from the context. We can now state the main result of this section, which generalizes similar results in (Balle et al., 2015, 2014) for weighted automata on strings.\nTheorem 2. Let f : T→ R be rational. If Hf = PS is a rank factorization, then there exists a minimal WTA A computing f such that PA = P and SA = S.\nProof. See Appendix A."
    }, {
      "heading" : "2.3 Approximate Minimization with the Singular Value Tree Automaton",
      "text" : "Equation (1) can be interpreted as saying that given a fixed factorization Hf = PASA, the value fA(c[t]) is given by the inner product ∑ iαA(c)(i)ωA(t)(i). Thus, αA(c)(i) and ωA(t)(i) quantify the influence of state i in the computation of fA(c[t]), and by extension one can use ‖PA(:, i)‖ and ‖SA(i, :)‖ to measure the overall influence of state i in fA. Since our goal is to approximate a given WTA by a smaller WTA obtained by removing some states in the original one, we shall proceed by removing those states with overall less influence on the computation of f . But because there are infinitely many WTA computing f , we need to first fix a particular representation for f before we can remove the less influential states. In particular, we seek a representation where each state is decoupled as much as possible from each other state, and where there is a clear ranking of states in terms of overall influence. It turns out all this can be achieved by a canonical form for WTA we call the singular value tree automaton, which provides an implicit representation for the SVD of Hf . We now show conditions for the existence of such canonical form, and in the next section we develop an algorithm for computing the it efficiently.\nSuppose f : T→ R is a rank n rational function such that its Hankel matrix admits a reduced singular value decomposition Hf = UDV>. Then we have that P = UD1/2 and S = D1/2V> is a rank decomposition for Hf , and by Theorem 2 there exists some minimal WTA A with fA = f , PA = UD1/2 and SA = D1/2V>. We call such an A a\nsingular value tree automaton (SVTA) for f . However, these are not defined for every rational function f , because the fact that columns of U and V must be unitary vectors (i.e. U>U = V>V = I) imposes some restrictions on which infinite Hankel matrices Hf admit an SVD — this phenomenon is related to the distinction between compact and non-compact operators in functional analysis. Our next theorem gives a sufficient condition for the existence of an SVD of Hf .\nWe say that a function f : T → R is strongly convergent if the series ∑ t∈T |t||f(t)| converges. To see the intuitive meaning of this condition, assume that f is a probability distribution over trees in T. In this case, strong convergence is equivalent to saying that the expected size of trees generated from the distribution f is finite. It turns out strong convergence of f is a sufficient condition to guarantee the existence of an SVD for Hf .\nTheorem 3. If f : TΣ → R is rational and strongly convergent, then Hf admits a singular value decomposition.\nProof. See Appendix B.\nTogether, Theorems 2 and 3 imply that every rational strongly convergent f : T→ R can be represented by an SVTA A. If rank(f) = n, then A has n states and for every i ∈ [n] the ith state contributes to Hf by generating the ith left and right singular vectors weighted by √si, where si = D(i, i) is the ith singular value. Thus, if we desire to obtain a good approximation f̂ to f with n̂ states, we can take the WTA Â obtained by removing the last n − n̂ states from A, which corresponds to removing from f the contribution of the smallest singular values of Hf . We call such Â an SVTA truncation. Given an SVTA A = 〈α,T , {ωσ}〉 and Π = [I | 0] ∈ Rn̂×n, the SVTA truncation to n̂ states can be written as\nÂ = 〈Πα,T (Π>,Π>,Π>), {Πωσ}〉 .\nTheoretical guarantees on the error induced by the SVTA truncation method are presented in Section 4 ."
    }, {
      "heading" : "3 Computing the Singular Value WTA",
      "text" : "Previous section shows that in order to compute an approximation to a strongly convergent rational function f : T→ R one can proceed by truncating its SVTA. However, the only obvious way to obtain such SVTA is by computing the SVD of the infinite matrix Hf . In this section we show that if we are given an arbitrary minimal WTA A for f , then we can transform A into the corresponding SVTA efficiently.1 In other words, given a representation of Hf as a WTA, we can compute its SVD without the need to operate on infinite matrices. The key observation is to reduce the computation of the SVD of Hf to the computation of spectral properties of the Gram matrices GC = P>P and GT = SS> associated with the rank factorization Hf = PS induced by some minimal WTA computing f . In the case of weighted automata on strings, (Balle et al., 2015) recently showed a polynomial time algorithm for computing the Gram matrices of a string Hankel matrix by solving a system of linear equations. Unfortunately, extending their approach to the tree case requires obtaining a closed-form solution to a system of quadratic equations, which in general does not exist. Thus, we shall resort to a different\n1If the WTA given to the algorithm is not minimal, a pre-processing step can be used to minimize the input using the algorithm from (Kiefer et al., 2015).\nalgorithmic technique and show that GC and GT can be obtained as fixed points of a certain non-linear operator. This yields the iterative algorithm presented in Algorithm 2 which converges exponentially fast as shown in Theorem 5. The overall procedure to transform a WTA into the corresponding SVTA is presented in Algorithm 1.\nWe start with a simple linear algebra result showing exactly how to relate the eigendecompositions of GC and GT with the SVD of Hf .\nLemma 1. Let f : T→ R be a rational function such that its Hankel matrix Hf admits an SVD. Suppose Hf = PS is a rank factorization. Then the following hold:\n1. GC = P>P and GT = SS> are finite symmetric positive definite matrices with eigendecompositions GC = VCDCV>C and GT = VTDTV>T .\n2. If M = D1/2C V>C VTD 1/2 T has SVD M = ŨDṼ>, then Hf = UDV> is an SVD,\nwhere U = PVCD−1/2C Ũ, and V> = Ṽ>D −1/2 T V>TS.\nProof. The proof follows along the same lines as that of (Balle et al., 2015, Lemma 7).\nPutting together Lemma 1 and the proof of Theorem 2 we see that given a minimal WTA computing a strongly convergent rational function, Algorithm 1 below will compute the corresponding SVTA. Note the algorithm depends on a procedure for computing the Gram matrices GT and GC. In the remaining of this section we present one of our main results: a linearly convergent iterative algorithm for computing these matrices.\nAlgorithm 1 ComputeSVTA Input: A strongly convergent minimal WTA A Output: The corresponding SVTA\nGC,GT ← GramMatrices(A) Let GT = VTDTV>T and GC = VCDCV>C be the eigendecompositions of GT and GC Let M = D1/2C V>C VTD 1/2 T and let M = UDV\n> be the singular value decomposition of M Let Q = VCD−1/2C UD1/2 return AQ\nLet A = 〈α,T , {ωσ}〉 be a strongly convergent WTA of dimension n computing a function f . We now show how the Gram matrix GT can be approximated using a simple iterative scheme. Let A⊗ = 〈α⊗,T ⊗, {ω⊗σ }〉 where α⊗ = α⊗α, T ⊗ = T ⊗T ∈ Rn2×n2×n2 and ω⊗σ = ωσ ⊗ ωσ for all σ ∈ Σ. It is shown in (Berstel and Reutenauer, 1982) that A⊗ computes the function fA⊗(t) = f(t)2. Note we have GT = SS> =∑ t∈Tω(t)ω(t)>, hence s , vec(GT) = ∑ t∈Tω\n⊗(t) since ω⊗(t) = vec(ω(t)ω(t)>). Thus, computing the Gram matrix GT boils down to computing the vector s. The following theorem shows that this can be done by repeated applications of a non-linear operator until convergence to a fixed point.\nTheorem 4. Let F : Rn2 → Rn2 be the mapping defined by F (v) = T ⊗(I,v,v) +∑ σ∈Σω ⊗ σ . Then the following hold:\n(i) s is a fixed-point of F ; i.e. F (s) = s.\n(ii) 0 is in the basin of attraction of s; i.e. limk→∞ F k(0) = s.\n(iii) The iteration defined by s0 = 0 and sk+1 = F (sk) converges linearly to s; i.e. there exists 0 < ρ < 1 such that ‖sk − s‖2 ≤ O(ρk).\nProof. See Appendix C.\nThough we could derive a similar iterative algorithm for computing GC, it turns out that knowledge of s = vec(GT) provides an alternative, more efficient procedure for obtaining GC. Like before, we have GC = P>P = ∑ c∈Cα(c)α(c)> and α⊗(c) =\nα(c) ⊗ α(c) for all c ∈ C, hence q , vec(GC) = ∑ c∈Cα\n⊗(c). By defining the matrix E = T ⊗(I, s, I) + T ⊗(I, I, s) which only depends on T and s, we can use the expression α⊗ >(c) = α⊗>ΞA⊗(c) to see that:\nq> = ∑ c∈C (α⊗)>ΞA⊗(c) = (α⊗)> ∑ k≥0 Ek = (α⊗)>(I−E)−1 ,\nwhere we used the facts Ek = ∑ c∈Ck ΞA⊗(c) and ρ(E) < 1 shown in the proof of Theorem 4. Algorithm 2 summarizes the overall approximation procedure for the Gram matrices, which can be done to an arbitrary precision. There, reshape(·, n×n) is an operation that takes an n2-dimensional vector and returns the n×n matrix whose first column contains the first n entries in the vector and so on. Theoretical guarantees on the convergence rate of this algorithm are given in the following theorem.\nTheorem 5. There exists 0 < ρ < 1 such that after k iterations in Algorithm 2, the approximations ĜC and ĜT satisfy ‖GC − ĜC‖F ≤ O(ρk) and ‖GT − ĜT‖F ≤ O(ρk).\nProof. See Appendix D.\nAlgorithm 2 GramMatrices Input: A strongly convergent minimal WTA A = 〈α,T , {ωσ}〉 Output: Gram matrices ĜC ' ∑ c∈CαA(c)αA(c)> and ĜT ' ∑ t∈TωA(t)ωA(t)>\nLet T ⊗ = T ⊗ T ∈ Rn2×n2×n2 , and let ω⊗σ = ωσ ⊗ ωσ ∈ Rn 2 for all σ ∈ Σ. Let I be the n2 × n2 identity matrix and let s = 0 ∈ Rn2 repeat s← T ⊗(I, s, s) + ∑ σ∈Σω ⊗ σ until convergence q = (α⊗α)> ( I− T ⊗(I, I, s)− T ⊗(I, s, I)\n)−1 ĜT = reshape(s, n× n) ĜC = reshape(q, n× n) return ĜC, ĜT"
    }, {
      "heading" : "4 Approximation Error of an SVTA Truncation",
      "text" : "In this section, we analyze the approximation error induced by the truncation of an SVTA. We recall that given a SVTA A = 〈α,T , {ωσ}〉, its truncation to n̂ states is the automaton\nÂ = 〈Πα,T (Π>,Π>,Π>), {Πωσ}〉\nwhere Π = [I | 0] ∈ Rn̂×n is the projection matrix which removes the states associated with the n− n̂ smallest singular values of the Hankel matrix.\nIntuitively, the states associated with the smaller singular values are the ones with the less influence on the Hankel matrix, thus they should also be the states having the less effect on the computation of the SVTA. The following theorem support this intuition by showing a fundamental relation between the singular values of the Hankel matrix of a rational function f and the parameters of the SVTA computing it.\nTheorem 6. Let A = 〈α,T , {ωσ}σ∈Σ〉 be a SVTA with n states realizing a function f and let s1 ≥ s2 ≥ · · · ≥ sn be the singular values of the Hankel matrix Hf . Then, for any t ∈ T and i, j, k ∈ [n] the following hold:\n• |ω(t)i| ≤ √ si , • |αi| ≤ √ si , and\n• |T (i, j, k)| ≤ min{ √ si√ sj √ sk , √ sj√ si √ sk , √ sk√ si √ sj }.\nProof. See Appendix E.\nTwo important properties of SVTAs follow from this proposition. First, the fact that |ω(t)i| ≤ √ si implies that the weights associated with states corresponding to small singular values are small. Second, this proposition gives us some intuition on how the states of an SVTA interact with each other. To see this, let M = T (α, I, I) and remark that for a tree t = (t1, t2) ∈ T we have f(t) = ω(t1)>Mω(t2). Using the previous theorem one can show that\n|M(i, j)| ≤ n √\nmin{si, sj} max{si, sj} ,\nwhich tells us that two states corresponding to singular values far away from each other have very little interaction in the computations of the automata.\nTheorem 6 is key to proving the following theorem, which is the main result of this section. It shows how the approximation error induced by the truncation of an SVTA is impacted by the magnitudes of the singular values associated with the removed states.\nTheorem 7. Let A = 〈α,T , {ωσ}σ∈Σ〉 be a SVTA with n states realizing a function f and let s1 ≥ s2 ≥ · · · ≥ sn be the singular values of the Hankel matrix Hf . Let f̂ be the function computed by the SVTA truncation of A to n̂ states. The following holds for any ε > 0:\n• For any tree t ∈ T of size M , if M < log\n( 1\nsn̂+1\n) + log (ε)\n2 logn then |f(t)− f̂(t)| < ε.\n• Furthermore, if M < log\n( 1\nsn̂+1\n) + log(ε)\nlog(4|Σ|n2) − 1 then ∑\nt:size(t)<M |f(t)− f̂(t)| < ε.\nProof. See Appendix F.\nSince sn̂+1 > sn̂+2 > · · · > sn, this theorem shows that the smaller the singular values associated with the removed states are, the better will be the approximation. As a direct consequence, the error introduced by the truncation grows with the number of states removed. The dependence on the size of the trees comes from the propagation of the error during the contractions of the tensor T̂ of the truncated SVTA.\nThe decay of singular values can be very slow in the worst case, but in practice is not unusual to observe an exponential decay on the tail. For example, this is shown to be the case for the SVTA we compute in Section 5. Assuming such an exponential decay of the form si = Cθi for some 0 < θ < 1, the second bound above on the size of the trees for which ∑ size(t)<M |f(t)− f̂(t)| < ε specializes to\n(n̂+ 1) log(1/θ) + log(ε) + log(C) log(4|Σ|n2) .\nIt is interesting to observe that the dependence of this bound on the number of total/removed states is O(n̂/ log(n))."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we assess the performance of our method on a model arising from realworld data, by using a PCFG learned from a text corpus as our initial model. Before presenting our experimental setup and results, we recall the standard mapping between WCFG and WTA."
    }, {
      "heading" : "5.1 Converting WCFG to WTA",
      "text" : "A weighted context-free grammar (WCFG) in Chomsky normal form is a tuple G = 〈N ,Σ,R,weight〉 where N is the finite set of nonterminal symbols, Σ is the finite set of words, with Σ ∩ N = ∅, R is a set of rules having the form (a→ bc), (a→ x) or (→ a) for a, b, c ∈ N , x ∈ Σ, and weight : R → R is the weight function which is extended to the set of all possible rules by letting weight(δ) = 0 for all rules δ 6∈ R.\nA WCFG G assigns a weight to each derivation tree τ of the grammar given by weight(τ) = ∏ δ∈Rw(δ)]δ(τ) (where ]δ(τ) is the number of times the rule δ appears in\nτ), and it computes a function fG : Σ+ → R defined by fG(w) = ∑ τ∈T (w) weight(τ) for any w ∈ Σ+, where T (w) is the set of trees deriving the word w. Given a WCFG G, we can build a WTA that assigns to each binary tree t ∈ TΣ the sum of the weights of all derivation trees of G having the same topology as t. Let G = 〈N ,Σ,R, w〉 be a WCFG in normal form with N = [n]. Let A = 〈α,T , {ωσ}σ∈Σ〉 be the WTA with n states defined by α(i) = weight(→ i) for all i ∈ [n], T (i, j, k) = weight(i → jk) for all i, j, k ∈ [n], and ωσ(i) = weight(i → σ) for all i ∈ [n], σ ∈ Σ. Then for all w ∈ Σ+ we have fG(w) = ∑ t∈TΣ:〈t〉=w fA(t) . It is important to note that in this conversion the number of states in A corresponds to the number of non-terminals in G. A similar construction can be used to convert any WTA to a WCFG where each state in the WTA is mapped to a non-terminal in the WCFG."
    }, {
      "heading" : "5.2 Experimental Setup and Results",
      "text" : "In our experiments, we used the annotated corpus of german newspaper texts NEGRA (Skut et al., 1997). We use a standard setup, in which the first 18,602 sentences are used as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set Stest. All trees are binarized as described in (Cohen et al., 2013b). We extract a binary grammar in Chomsky normal form from the data, and then estimate its probabilities using maximum likelihood. The resulting PCFG has n = 211 nonterminals. We compare our method against the ones described in (Cohen et al.,\n2013a), who use tensor decomposition algorithms (Chi and Kolda, 2012) to decompose the tensors of an underlying PCFG.2\nWe used three evaluation measures: `2 distance (between the functions of type TΣ → R computed by the original WTA and the one computed by its approximation), perplexity on a test set, and parsing accuracy on a test set (comparing the tree topology of parses using the bracketing F-measure). Because the number of states on a WTA and the CP-rank of tensor decomposition method are not directly comparable, we plotted the results using the number of parameters needed to specify the model on the horizontal axis. This number is equal to n̂3 for a WTA with n̂ states, and it is equal to 3Rn when the tensor T is approximated with a tensor of CP-rank R (note in both cases these are the number of parameters needed to specify the tensor occurring in the model). The `2 distance between the original function f and its minimization f̂ , ‖f − f̂‖22 =∑ t∈T(f(t) − f̂(t))2, can be approximated to an arbitrary precision using the Gram matrices of the corresponding WTA (which follows from observing that (f − f̂)2 is rational). The perplexity of f̂ is defined by 2−Htest , where Htest = ∑ t∈Stest f(t) log2 f̂(t) and both f and f̂ have been normalized to sum to one over the test set. The results are plotted in Figure 2, where an horizontal dotted line represents the performance of the original model. We see that our method outperforms the tensor decomposition methods both in terms of `2 distance and perplexity. We also remark that our method obtains very smooth curves, which comes from the fact that it does not suffer from local optima problems like the tensor decomposition methods.\nFor parsing we use minimum Bayes risk decoding, maximizing the sum of the marginals for the nonterminals in the grammar, essentially choosing the best tree topology given a string (Goodman, 1996). The results for various length of sentences are shown in Figure 3, where we see that our method does not perform as well as the tensor decomposition methods in terms of parsing accuracy on long sentences. In this figure, we also plotted the results for a slight modification of our method (SVTA∗) that is able to achieve competitive performances. The SVTA∗ method gives more importance to long sentences in the minimization process. This is done by finding the highest constant γ > 0 such that the function fγ : t 7→ γsize(t)f(t) is still strongly convergent. This function is then approximated by a low-rank WTA computing f̂γ , and we let f̂ : t 7→ γ−size(t)f̂γ(t) (which is rational). In our experiment, we used γ = 2.4. While the SVTA∗ method improved the parsing accuracy, it had no significant repercussion on the `2 and per-\n2We use two tensor decomposition algorithms from the tensor Matlab toolbox: pqnr, which makes use of projected quasi-Newton and mu, which uses a multiplicative update. See http://www.sandia. gov/˜tgkolda/TensorToolbox/index-2.6.html.\nplexity measures. We believe that the parsing accuracy of our method could be further improved. Seeking techniques that combines the benefits of SVTA and previous works is a promising direction."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We described a technique for approximate minimization of WTA, yielding a model smaller than the original one which retains good approximation properties. Our main algorithm relies on a singular value decomposition of an infinite Hankel matrix induced by the WTA. We provided theoretical guarantees on the error induced by our minimization method. Our experiments with real-world parsing data show that the minimized WTA, depending on the number of singular values used, approximates well the original WTA on three measures: perplexity, bracketing accuracy and `2 distance of the tree weights. Our work has connections with spectral learning techniques for WTA, and exhibits similar properties as those algorithms; e.g. absence of local optima. In future work we plan to investigate the applications of our approach to the design and analysis of improved spectral learning algorithms for WTA."
    }, {
      "heading" : "A Proof of Theorem 2",
      "text" : "Theorem. Let f : T → R be rational. If Hf = PS is a rank factorization, then there exists a minimal WTA A computing f such that PA = P and SA = S.\nProof. Let n = rank(f). Let B be an arbitrary minimal WTA computing f . Suppose B induces the rank factorization Hf = P′S′. Since the columns of both P and P′ are basis for the column-span of Hf , there must exists a change of basis Q ∈ Rn×n between P and P′. That is, Q is an invertible matrix such that P′Q = P. Furthermore, since P′S′ = Hf = PS = P′QS and P′ has full column rank, we must have S′ = QS, or equivalently, Q−1S′ = S. Thus, we let A = BQ, which immediately verifies fA = fB = f . It remains to be shown that A induces the rank factorization Hf = PS. Note that when proving the equivalence fA = fB we already showed ωA(t) = Q−1ωB(t), which means we have SA = Q−1S′ = S. To show PA = P′Q we need to show that for any c ∈ C we have αA(c)> = αB(c)>Q. This will immediately follow if we show that ΞA(c) = Q−1ΞB(c)Q. If we proceed by induction on drop(c), we see the case c = ∗ is immediate, and for c = (c′, t) we get\nΞA((c′, t)) = (T (Q−>,Q,Q))(I,ΞA(c′),ωA(t)) = (T (Q−>,Q,Q))(I,Q−1ΞB(c′)Q,Q−1ωB(t)) = T (Q−>,ΞB(c′)Q,ωB(t)) = Q−1T (I,ΞB(c′),ωB(t))Q .\nApplying the same argument mutatis mutandis for c = (t, c′) completes the proof."
    }, {
      "heading" : "B Proof of Theorem 3",
      "text" : "Theorem. If f : TΣ → R is rational and strongly convergent, then Hf admits a singular value decomposition.\nProof. The result will follow if we show that Hf is the matrix of a compact operator on a Hilbert space (Conway, 1990). The main obstruction to this approach is that the rows and columns of Hf are indexed by different objects (trees vs. contexts). Thus, we will need to see Hf as an operator on a larger space that contains both these objects.\nRecall we have TΣ ⊂ TΣ′ and CΣ ⊂ TΣ′ . Given two functions g, g′ : TΣ′ → R we define their inner product to be 〈g, g′〉 = ∑ t′∈TΣ′ g(t ′)g′(t′). Let ‖g‖ = √ | 〈g, g〉 | be the induced norm and let T be the space of all functions g : TΣ′ → R such that ‖g‖ <∞. Note that T with a Hilbert space, and that since TΣ′ is countable, it actually is a separable Hilbert space isomorphic to `2, the spaces of infinite square summable sequences. Given set X ⊂ TΣ′ we define T(X) = {g ∈ T | g(t′) = 0, t′ ∈ TΣ′ \\ X}.\nNow let Cf : T→ T be the linear operator on T given by\n(Cfg)(t′) = {∑ t∈TΣ f(t ′[t])g(t) if t′ ∈ CΣ\n0 if t′ /∈ CΣ .\nNow note that by construction we have T(TΣ) ⊆ Ker(Cf ) and Im(Cf ) ⊆ T(CΣ). Hence, a simple calculation shows that given the decompositions Cf : T(TΣ)⊥ ⊕ T(TΣ) → T(CΣ)⊕ T(CΣ)⊥, the matrix of Cf is\nCf = [\nHf 0 0 0\n] .\nThus, if Cf is a compact operator, then Hf admits an SVD. Since Hf has finite rank, we only need to show that Cf is a bounded operator.\nGiven c ∈ CΣ we define fc ∈ T(TΣ) given by fc(t) = f(c[t]) for t ∈ TΣ. Now let g ∈ T with ‖g‖ = 1 and recall Cf is bounded if ‖Cfg‖ < ∞ for every g ∈ T with ‖g‖ = 1. Indeed, because f is strongly convergent we have:\n‖Cfg‖2 = ∑ t′∈TΣ′ (Cfg)(t′)2\n= ∑ c∈CΣ (Cfg)(c)2 = ∑ c∈CΣ ∑ t∈TΣ f(c[t])g(t) 2\n= ∑ c∈CΣ 〈fc, g〉2\n≤ ‖g‖2 ∑ c∈CΣ ‖fc‖2\n= ∑ c∈CΣ ∑ t′∈TΣ′ fc(t′)2\n= ∑ c∈CΣ ∑ t∈TΣ f(c[t])2\n= ∑ t∈TΣ |t|f(t)2\n≤ sup t∈TΣ |f(t)| · ∑ t∈TΣ |t||f(t)| <∞ ,\nwhere we used the Cauchy–Schwarz inequality, and the fact that supt∈TΣ |f(t)| is bounded when f is strongly convergent."
    }, {
      "heading" : "C Proof of Theorem 4",
      "text" : "Theorem. Let F : Rn2 → Rn2 be the mapping defined by F (v) = T ⊗(I,v,v) +∑ σ∈Σω ⊗ σ . Then the following hold:\n(i) s is a fixed-point of F ; i.e. F (s) = s.\n(ii) 0 is in the basin of attraction of s; i.e. limk→∞ F k(0) = s.\n(iii) The iteration defined by s0 = 0 and sk+1 = F (sk) converges linearly to s; i.e. there exists 0 < ρ < 1 such that ‖sk − s‖2 ≤ O(ρk).\nProof. (i) We have T ⊗(I, s, s) = ∑ t,t′∈T T ⊗(I,ω⊗(t),ω⊗(t′)) = ∑ t,t′∈Tω\n⊗((t, t′)) =∑ t∈T≥1 ω\n⊗(t) where T≥1 is the set of trees of depth at least one. Hence F (s) =∑ t∈T≥1 ω ⊗(t) + ∑ σ∈Σω ⊗ σ = s.\n(ii) Let T≤k denote the set of all trees with depth at most k. We prove by induction on k that F k(0) = ∑ t∈T≤k ω ⊗(t), which implies that limk→∞ F k(0) = s. This is\nstraightforward for k = 0. Assuming it is true for all naturals up to k − 1, we have\nF k(0) = T ⊗(I, F k−1(0), F k−1(0)) + ∑ σ∈Σ ω⊗σ\n= ∑\nt,t′∈T≤k−1 T ⊗(I,ω⊗(t),ω⊗(t′)) + ∑ σ∈Σ ω⊗σ\n= ∑\nt,t′∈T≤k−1 ω⊗((t, t′)) + ∑ σ∈Σ ω⊗σ\n= ∑ t∈T≤k ω⊗(t) .\n(iii) Let E be the Jacobian of F around s, we show that the spectral radius ρ(E) of E is less than one, which implies the result by Ostrowski’s theorem (see (Ortega, 1990, Theorem 8.1.7)).\nSince A is minimal, there exists trees t1, · · · , tn ∈ T and contexts c1, · · · , cn ∈ C such that both {ω(ti)}i∈[n] and {α(ci)}i∈[n] are sets of linear independent vectors in Rn (Bailly et al., 2010). Therefore, the sets {ω(ti)⊗ω(tj)}i,j∈[n] and {α(ci)⊗α(cj)}i,j∈[n] are sets of linear independent vectors in Rn2 . Let v ∈ Rn2 be an eigenvector of E with eigenvalue λ 6= 0, and let v = ∑ i,j∈[n] βi,j(ω(ti) ⊗ ω(tj)) be its expression in terms of the basis given by {ω(ti)⊗ ω(tj)}. For any vector u ∈ {α(ci)⊗α(cj)} we have\nlim k→∞ u>Ekv ≤ lim k→∞ |u>Ekv| ≤ ∑ i,j∈[n] |βi,j | lim k→∞ |u>Ek(ω(ti)⊗ ω(tj))| = 0 ,\nwhere we used Lemma 2 in the last step. Since this is true for any vector u in the basis {α(ci) ⊗ α(cj)}, we have limk→∞Ekv = limk→∞ |λ|kv = 0, hence |λ| < 1. This reasoning holds for any eigenvalue of E, hence ρ(E) < 1.\nLemma 2. Let A = 〈α,T , {ωσ}〉 be a minimal WTA of dimension n computing the strongly convergent function f , and let E ∈ Rn2×n2 be the Jacobian around s =∑ t∈Tω(t) ⊗ ω(t) of the mapping F : v → T ⊗(I,v,v) + ∑ σ∈Σω ⊗ σ . Then for any c1, c2 ∈ C and any t1, t2 ∈ T we have limk→∞ |(α(c1)⊗α(c2))>Ek(ω(t1)⊗ ω(t2))| = 0.\nProof. Let Ξ⊗ : C→ Rn2×n2 be the context mapping associated with the WTA A⊗; i.e. Ξ⊗ = ΞA⊗ . We start by proving by induction on drop(c) that Ξ⊗(c) = Ξ(c)⊗Ξ(c) for all c ∈ C. Let Cd denote the set of contexts c ∈ C with drop(c) = d. The statement is trivial for c ∈ C0. Assume the statement is true for all naturals up to d − 1 and let c = (t, c′) ∈ Cd for some t ∈ T and c′ ∈ Cd−1. Then using our inductive hypothesis we have that\nΞ⊗(c) = T ⊗(In2 ,ω(t)⊗ ω(t),Ξ(c′)⊗Ξ(c′)) = T (In,ω(t),Ξ(c′))⊗ T (In,ω(t),Ξ(c′)) = Ξ(c)⊗Ξ(c) .\nThe case c = (c′, t) follows from an identical argument. Next we use the multi-linearity of F to expand F (s+h) for a vector h ∈ Rn2 . Keeping the terms that are linear in h we obtain that E = T ⊗(I, s, I) + T ⊗(I, I, s). It follows that E = ∑ c∈C1 Ξ⊗(c), and it can be shown by induction on k that Ek = ∑ c∈Ck Ξ⊗(c).\nWriting dc = min(drop(c1),drop(c2)) and dt = min(depth(t1), depth(t2)), we can see that∣∣∣(α(c1)⊗α(c2))>Ek(ω(t1)⊗ ω(t2))∣∣∣ =\n∣∣∣∣∣∣ ∑ c∈Ck (α(c1)⊗α(c2))>Ξ⊗(c)(ω(t1)⊗ ω(t2)) ∣∣∣∣∣∣ =\n∣∣∣∣∣∣ ∑ c∈Ck (α(c1)>Ξ(c)ω(t1)) · (α(c2)>Ξ(c)ω(t2)) ∣∣∣∣∣∣ =\n∣∣∣∣∣∣ ∑ c∈Ck f(c1[c[t1]])f(c2[c[t2]]) ∣∣∣∣∣∣ ≤\n∑ c∈Ck |f(c1[c[t1]])| ∑ c∈Ck |f(c2[c[t2]])|  ≤\n ∑ t∈T≥dc+dt+k |t||f(t)| 2 , which tends to 0 with k →∞ since f is strongly convergent. To prove the last inequality, check that any tree of the form t′ = c[c′[t]] satisfies depth(t′) ≥ drop(c) + drop(c′) + depth(t), and that for fixed c ∈ C and t, t′ ∈ T we have |{c′ ∈ C : c[c′[t]] = t′}| ≤ |t′| (indeed, a factorization t′ = c[c′[t]] is fixed once the root of t is chosen in t′, which can be done in at most |t′| different ways)."
    }, {
      "heading" : "D Proof of Theorem 5",
      "text" : "Theorem. There exists 0 < ρ < 1 such that after k iterations in Algorithm 2, the approximations ĜC and ĜT satisfy ‖GC − ĜC‖F ≤ O(ρk) and ‖GT − ĜT‖F ≤ O(ρk).\nProof. The result for the Gram matrix GT directly follows from Theorem 4. We now show how the error in the approximation of GT = reshape(s, n× n) affects the approximation of q = (α⊗)>(I − E)−1 = vec(GC). Let ŝ ∈ Rn be such that ‖s − ŝ‖ ≤ ε, let Ê = T ⊗(I, ŝ, I) + T ⊗(I, I, ŝ) and let q = (α⊗)>(I− Ê)−1. We first bound the distance between E and Ê. We have\n‖E− Ê‖F = ‖T ⊗(I, s− ŝ, I) + T ⊗(I, I, s− ŝ)‖F ≤ 2‖T ⊗‖F ‖s− ŝ‖ = O(ε) ,\nwhere we used the bounds ‖T (I, I,v)‖F ≤ ‖T ‖F ‖v‖ and ‖T (I,v, I)‖F ≤ ‖T ‖F ‖v‖. Let δ = ‖E− Ê‖ and let σ be the smallest nonzero eigenvalue of the matrix I− E. It follows from (El Ghaoui, 2002, Equation (7.2)) that if δ < σ then ‖(I− E)−1 − (I− Ê)−1‖ ≤ δ/(σ(σ − δ)). Since δ = O(ε) from our previous bound, the condition δ ≤ σ/2 will be eventually satisfied as ε→ 0, in which case we can conclude that\n‖GC − ĜC‖F = ‖q − q̂‖ ≤ ‖(I−E)−1 − (I− Ê)−1‖‖α⊗‖\n≤ 2δ σ2 ‖α⊗‖ = O(ε) ."
    }, {
      "heading" : "E Proof of Theorem 6",
      "text" : "Let A = 〈α,T , {ωσ}σ∈Σ〉 be a SVTA with n states realizing a function f and let s1 ≥ s2 ≥ · · · ≥ sn be the singular values of the Hankel matrix Hf .\nTheorem 6 relies on the following lemma, which explores the consequences that the fixed-point equations used to compute GT and GC have for an SVTA.\nLemma 3. For all i ∈ [n], the following hold:\n1. si = ∑ σ∈Σωσ(i)2 + ∑n j,k=1 T (i, j, k)2sjsk ,\n2. si = α(j)2 + ∑n j,k=1(T (j, i, k)2 + T (j, k, i)2)sjsk .\nProof. Let GT and GC be the Gram matrices associated with the rank factorization of Hf . Since A is a SVTA we have GT = GC = D where D = diag(s1, · · · , sn) is a diagonal matrix with the Hankel singular values on the diagonal. The first equality then follows from the following fixed point characterization of GT:\nGT = ∑ t∈T ω(t)ω(t)>\n= ∑ σ∈Σ ωσω > σ\n+ ∑\nt1,t2∈T T (I,ω(t1),ω(t2))T (I,ω(t1),ω(t2))>\n= ∑ σ∈Σ ωσω > σ + T(1)(GT ⊗GT)T>(1) ,\n(where T(i) denotes the matricization of the tensor T along the ith mode). The second equality follows from the following fixed point characterization of GC:\nGC = ∑ c∈C α(c)α(c)>\n= αα> + ∑\nc∈C,t∈T T (α(c),ω(t), I)T (α(c),ω(t), I)>\n+ ∑\nc∈C,t∈T T (α(c), I,ω(t))T (α(c), I,ω(t))>\n= αα>\n+ T(2)(GC ⊗GT)T>(2) + T(3)(GC ⊗GT)T>(3) .\nTheorem. For any t ∈ T, c ∈ C and i, j, k ∈ [n] the following hold:\n• |ω(t)i| ≤ √ si , • |α(c)i| ≤ √ si , and\n• |T (i, j, k)| ≤ min{ √ si√ sj √ sk , √ sj√ si √ sk , √ sk√ si √ sj }.\nProof. The third point is a direct consequence of the previous Lemma. For the first point, let UDV> be the SVD of Hf . Since A is a SVTA we have\nω(t)2i = (D1/2V>)2i,t = siV(t, i)2\nand since the rows of V are orthonormal we have V(t, i)2 ≤ 1. The inequality for contexts is proved similarly by reasoning on the rows of UD1/2."
    }, {
      "heading" : "F Proof of Theorem 7",
      "text" : "To prove Theorem 7, we will show how the computation of a WTA on a give tree t can be seen as an inner product between two tensors, one which is a function of the topology of the tree, and one which is a function of the labeling of its leafs (Proposition 1). We will then show a fundamental relation between the components of the first tensor and the singular values of the Hankel matrix when the WTA is in SVTA normal form (Proposition 2); this proposition will allow us to show Lemma 4 that bounds the difference between components of this first tensor for the original SVTA and its truncation. We will finally use this lemma to bound the absolute error introduced by the truncation of an SVTA (Propositions 3 and 4).\nWe first introduce another kind of contexts than the one introduced in Section 2, where every leaf of a binary tree is labeled by the special symbol ∗ (which still acts as a place holder). Let B be the set of binary trees on the one-letter alphabet {∗}. We will call a tree b ∈ B a multicontext. For any integer M ≥ 1 we let\nBM = {b ∈ B : |〈b〉| = M}\nbe the subset of multicontexts with M leaves (equivalently, BM is the subset of multicontexts of size M − 1). Given a word w = w1 · · ·wM ∈ Σ∗ and a multicontext b ∈ BM , we denote by b[w1, · · · , wM ] ∈ TΣ the tree obtained by replacing the ith occurrence of ∗ in b by wi for i ∈ [M ]. Let b ∈ BM , for any integer m ∈ [M ] we denote by bJmK ∈ BM+1 the multicontext obtained by replacing the mth occurence of ∗ in b by the tree (∗, ∗). Let M > 1, it is easy to check that for any b′ ∈ BM , there exist b ∈ BM−1 and m ∈ [M − 1] satisfying b′ = bJmK. See Figure 4 for some illustrative examples.\nWe now show how the computation of a WTA on a given tree with M leaves can be seen as an inner product between two Mth order tensors: the first one depends on the topology of the tree, while the second one depends on the labeling of its leaves.\nLet A = 〈α,T , {ωσ}σ∈Σ〉 be a WTA with n states computing a function f . Given a multicontext b ∈ BM , we denote by βA(b) ∈ ⊗M i=1 Rn the Mth order tensor inductively defined by βA(∗) = α and\nβA(bJmK)i1···iM = n∑ k=1 βA(b)i1···im−1kim+2···iMT kimim+1\nfor any b ∈ BM−1, m ∈ [M −1] and i1, · · · , iM ∈ [n] (i.e. βA(bJmK) is the contraction of βA(b) along the mth mode and T along the first mode). Given a word w = w1 · · ·wM ∈ Σ∗, we let ψA(w) ∈ ⊗M i=1 Rn be the Mth order tensor defined by\nψA(w)i1···iM = ω(w1)i1ω(w2)i2 · · ·ω(wM )iM = M∏ m=1 ω(wm)im\nfor i1, · · · , iM ∈ [n] (i.e. ψA(w) is the tensor product of the ω(wi)’s). We will simply write β and ψ when the automaton is clear from context.\nProposition 1. For any multicontext b ∈ BM and any word w = w1 · · ·wM ∈ Σ∗ we have\nf(b[w1, · · · , wM ]) = 〈β(b),ψ(w)〉 ,\nwhere the inner product between two M th order tensors U and V is defined by 〈U ,V〉 =∑ i1···iM U(i1, · · · , iM )V(i1, · · · , iM ).\nSketch of proof. Let b ∈ BM and w = w1 · · ·wM ∈ Σ∗. Let b1 ∈ BM−1 and m ∈ [M −1] be such that b = b1JmK. In order to lighten the notations and without loss of generality we assume that m = 1. One can check that\n〈β(b),ψ(w)〉 =β(b)(ωw1 , · · · ,ωwM ) =β(b1J1K)(ωw1 , · · · ,ωwM ) =β(b1)(ω((w1, w2)),ωw3 , · · · ,ωwM ) .\nThe same reasoning can now be applied to b1. Assume for example that b1 = b2J1K for some b2 ∈ BM−2, we would have\n〈β(b),ψ(w)〉 = β(b1)(ω((w1, w2)),ωw3 , · · · ,ωwM ) = β(b2J1K)(ω((w1, w2)),ωw3 , · · · ,ωwM ) = β(b2)(ω(((w1, w2), w3)),ωw4 , · · · ,ωwM ) .\nBy applying the same argument again and again we will eventually obtain\n〈β(b),ψ(w)〉 = β(bM−1)(ω(b[w1, · · · , wM ])) = β(∗)(ω(b[w1, · · · , wM ])) = α>ω(b[w1, · · · , wM ]) = f(b[w1, · · · , wM ]) .\nSuppose now that A = 〈α,T , {ωσ}σ∈Σ〉 is an SVTA with n states for f and let s1 ≥ s2 ≥ · · · ≥ sn be the singular values of the Hankel matrix Hf . The following proposition shows a relation — similar to the one presented in Theorem 6 — between the components of the tensor β(b) (for any multicontext b) and the singular values of the Hankel matrix.\nProposition 2. If A = 〈α,T , {ωσ}σ∈Σ〉 is an SVTA, then for any b ∈ BM and any i1, · · · , iM ∈ [n] the following holds:\n|β(b)i1···iM | ≤ nM−1 min p∈[M ] {sip} M∏ m=1 1 √ sim .\nProof. We proceed by induction on M . If M = 1 we have b = ∗ and\n|β(∗)i| = |αi| ≤ √ si = si√ si .\nSuppose the result holds for multicontexts in BM−1 and let b′ ∈ BM . Let m ∈ [M ] and b ∈ BM−1 be such that b′ = bJmK. Without loss of generality and to lighten the notations we assume that m = 1. Start by writing:\n|β(b′)i1···iM | = |β(bJ1K)i1···iM | = ∣∣∣∣∣ n∑ k=1 βA(b)ki3···iMT ki1i2 ∣∣∣∣∣ ≤ n∑ k=1 |βA(b)ki3···iMT ki1i2 |\nRemarking that the third inequality in Theorem 6 can be rewritten as |T ijk| ≤ min{si,sj ,sk}√si√sj√sk , we have for any k ∈ [n]:\n|βA(b)ki3···iMT ki1i2 | ≤n M−2 min{sk, si3 , · · · , siM } 1 √ sk M∏ m=3 1 √ sim min{sk, si1 , si2}√ sk √ si1 √ si2\n=nM−2 1 sk M∏ m=1 1 √ sim min{sk, si3 , · · · , siM }min{sk, si1 , si2}\n≤nM−2 min p∈[M ] {sip} M∏ m=1 1 √ sim ,\nwhere we used that\nmin{sk, si3 , · · · , siM }min{sk, si1 , si2} ≤ sk min{si1 , · · · , siM }\nSumming over k yields the desired bound.\nLet f̂ be the function computed by the SVTA truncation of A to n̂ states. Let Π ∈ Rn×n be the diagonal matrix defined by Π(i, i) = 1 if i ≤ n̂ and 0 otherwise. It is easy to check that the WTA Â = 〈α̂, T̂ , ω̂σ〉, where α̂ = Πα, T̂ = T (I,Π,Π) and ω̂σ = ωσ, computes the function f̂ . We let ω̂(t) = ωÂ(t) for any tree t and similarly for α̂(c), ψ̂(w) and β̂(c).\nWe can now prove the following Lemma that bounds the absolute difference between the components of the tensors β(b) and β̂(b) for a given multicontext b.\nLemma 4. For any b ∈ BM and any i1, · · · iM ∈ [n] we have\n|(β(b)− β̂(b))i1···iM | ≤ sn̂+1 nM−1 M∏ m=1 1 √ sim .\nProof. It is easy to check that when there exists at least one m ∈ [M ] such that im > n̂, we have β̂(b)i1···iM = 0, hence\n|(β(b)− β̂(b))i1···iM | = |β(b)i1···iM |\nand the result directly follows from Proposition 2. Suppose i1, · · · , iM ∈ [n̂], we proceed by induction on M . If M = 1 then b = ∗, thus\n|β(∗)i − β̂(∗)i| = |αi − α̂i| = 0\nfor all i ∈ [n̂]. Suppose the result holds for multicontexts in BM−1 and let b′ ∈ BM . Let b ∈ BM−1 and m ∈ [M − 1] be such that b′ = bJmK. To lighten the notations we assume without loss of generality that m = 1. We have\n|(β(b′)− β̂(b′))i1···iM | =|(β(bJ1K)− β̂(bJ1K))i1···iM | (2)\n≤ n̂∑ k=1 |T ki1i2 | |(β(b)− β̂(b))ki3···iM | (3)\n+ n∑\nk=n̂+1 |T ki1i2 | |β(b)ki3···iM | (4)\n≤ n̂∑ k=1 √ sk√ si1si2 · sn̂+1 n M−2 √ sk √ si3 · · · √ siM\n(5)\n+ n∑\nk=n̂+1\n√ sk√\nsi1si2 · min{sk, si3 , · · · , siM } n M−2 √ sk √ si3 · · · √ siM\n(6)\n≤ sn̂+1 nM−1 M∏ m=1 1 √ sim . (7)\nTo decompose (2) in (3) and (4) we used the fact that T ki1i2 = T̂ ki1i2 whenever k ≤ n̂ and β̂(b)ki3···iM = 0 whenever k > n̂. We bounded (3) by (5) using the induction hypothesis, while we used Proposition 2 to bound (4) by (6).\nProposition 3. Let t ∈ T be a tree of size M , then\n|f(t)− f̂(t)| ≤ n2M−1sn̂+1 .\nProof. Let t ∈ T be a tree of size M − 1, then there exists a (unique) b ∈ BM and a (unique) word w = w1 · · ·wM ∈ Σ∗ such that t = b[w1, · · · , wM ]. Since ωσ = ω̂σ for all σ ∈ Σ, we have ψ(x) = ψ̂(x) for all x ∈ Σ∗. Furthermore, since ωσ(i)2 ≤ si for all i ∈ [n], we have\n|ψ(w)i1···iM | ≤ M∏ m=1 √ sim .\nIt follows that |f(t)− f̂(t)| = ∣∣∣〈β(b),ψ(w)〉 − 〈β̂(b), ψ̂(w)〉∣∣∣\n= ∣∣∣〈β(b)− β̂(b),ψ(w)〉∣∣∣ ≤\nn∑ i1=1 · · · n∑ iM=1 |(β(b)− β̂(b))i1···iM | |ψ(w)i1···iM |\n≤ n∑\ni1=1 · · · n∑ iM=1 sn̂+1 n M−1 M∏ m=1 1 √ sim · M∏ m=1 √ sim\n=n2M−1sn̂+1\nProposition 4. Let S = |Σ| be the size of the alphabet. For any integer M we have\n∑ t∈T:\nsize(t)<M\n|f(t)− f̂(t)| ≤ (4Sn 2)M+1 − 1\n(4Sn2)− 1 sn̂+1 .\nProof. For any integer m there are less than 4m binary trees with m internal nodes (which is a bound on the m-th Catalan number) and each one of these trees has m+ 1 leaves, thus Sm+1 possible labelling of the leaves. Using the previous proposition we get\n∑ t∈T:\nsize(t)<M\n|f(t)− f̂(t)| = M−1∑ m=0 ∑ t∈T:\nsize(t)=m\n|f(t)− f̂(t)|\n≤ M−1∑ m=0 4mSm+1 · n2(m−1)sn̂+1\n≤ M∑ m=1 (4Sn2)msn̂+1 ≤ (4Sn 2)M+1 − 1\n(4Sn2)− 1 sn̂+1.\nTheorem. Let A = 〈α,T , {ωσ}σ∈Σ〉 be a SVTA with n states realizing a function f and let s1 ≥ s2 ≥ · · · ≥ sn be the singular values of the Hankel matrix Hf . Let f̂ be the function computed by the SVTA truncation of A to n̂ states.\nLet S = |Σ| be the size of the alphabet, let M be an integer and let ε > 0.\n• For any tree t ∈ T of size M , if M < log\n( 1\nsn̂+1\n) + log (ε)\n2 logn then |f(t)− f̂(t)| < ε.\n• If M < log\n( 1\nsn̂+1\n) + log(ε)\nlog(4Sn2) − 1 then ∑\nt:size(t)<M |f(t)− f̂(t)| < ε.\nProof. For the first bound, it is easy to check that if\nM < log\n( 1\nsn̂+1\n) + log (ε)\n2 logn\nthen n2Msn̂+1 < ε and the result follows from Proposition 3. For the second one, if\nM < log\n( 1\nsn̂+1\n) + log(ε)\nlog(4Sn2) − 1\nthen (4Sn 2)M+1−1\n(4Sn2)−1 sn̂+1 < ε and the result follows from Proposition 4."
    } ],
    "references" : [ {
      "title" : "Grammatical inference as a principal component analysis problem",
      "author" : [ "R. Bailly", "F. Denis", "L. Ralaivola" ],
      "venue" : "In Proceedings of ICML",
      "citeRegEx" : "Bailly et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bailly et al\\.",
      "year" : 2009
    }, {
      "title" : "A spectral approach for probabilistic grammatical inference on trees",
      "author" : [ "R. Bailly", "A. Habrard", "F. Denis" ],
      "venue" : "In Proceedings of ALT",
      "citeRegEx" : "Bailly et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bailly et al\\.",
      "year" : 2010
    }, {
      "title" : "Spectral learning of weighted automata: A forward-backward perspective",
      "author" : [ "B. Balle", "X. Carreras", "F. Luque", "A. Quattoni" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Balle et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Balle et al\\.",
      "year" : 2014
    }, {
      "title" : "A canonical form for weighted automata and applications to approximate minimization",
      "author" : [ "B. Balle", "P. Panangaden", "D. Precup" ],
      "venue" : "In Proceedings of LICS",
      "citeRegEx" : "Balle et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Balle et al\\.",
      "year" : 2015
    }, {
      "title" : "Recognizable formal power series on trees",
      "author" : [ "J. Berstel", "C. Reutenauer" ],
      "venue" : "Theoretical Computer Science",
      "citeRegEx" : "Berstel and Reutenauer,? \\Q1982\\E",
      "shortCiteRegEx" : "Berstel and Reutenauer",
      "year" : 1982
    }, {
      "title" : "Closing the learning planning loop with predictive state representations",
      "author" : [ "B. Boots", "S. Siddiqi", "G. Gordon" ],
      "venue" : "International Journal of Robotics Research",
      "citeRegEx" : "Boots et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boots et al\\.",
      "year" : 2011
    }, {
      "title" : "The rank of a formal tree power series",
      "author" : [ "S. Bozapalidis", "O. Louscou-Bozapalidou" ],
      "venue" : "Theoretical Computer Science",
      "citeRegEx" : "Bozapalidis and Louscou.Bozapalidou,? \\Q1983\\E",
      "shortCiteRegEx" : "Bozapalidis and Louscou.Bozapalidou",
      "year" : 1983
    }, {
      "title" : "On tensors, sparsity, and nonnegative factorizations",
      "author" : [ "E.C. Chi", "T.G. Kolda" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications",
      "citeRegEx" : "Chi and Kolda,? \\Q2012\\E",
      "shortCiteRegEx" : "Chi and Kolda",
      "year" : 2012
    }, {
      "title" : "Tensor decomposition for fast parsing with latentvariable PCFGs",
      "author" : [ "S.B. Cohen", "M. Collins" ],
      "venue" : "In Proceedings of NIPS",
      "citeRegEx" : "Cohen and Collins,? \\Q2012\\E",
      "shortCiteRegEx" : "Cohen and Collins",
      "year" : 2012
    }, {
      "title" : "Approximate PCFG parsing using tensor decomposition",
      "author" : [ "S.B. Cohen", "G. Satta", "M. Collins" ],
      "venue" : "In Proceedings of NAACL",
      "citeRegEx" : "Cohen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2013
    }, {
      "title" : "Experiments with spectral learning of latent-variable PCFGs",
      "author" : [ "S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar" ],
      "venue" : "In Proceedings of NAACL",
      "citeRegEx" : "Cohen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2013
    }, {
      "title" : "Spectral learning of latent-variable PCFGs: Algorithms and sample complexity",
      "author" : [ "S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Cohen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2014
    }, {
      "title" : "A course in functional analysis",
      "author" : [ "J.B. Conway" ],
      "venue" : null,
      "citeRegEx" : "Conway,? \\Q1990\\E",
      "shortCiteRegEx" : "Conway",
      "year" : 1990
    }, {
      "title" : "Inversion error, condition number, and approximate inverses of uncertain matrices. Linear algebra and its applications",
      "author" : [ "L. El Ghaoui" ],
      "venue" : null,
      "citeRegEx" : "Ghaoui,? \\Q2002\\E",
      "shortCiteRegEx" : "Ghaoui",
      "year" : 2002
    }, {
      "title" : "Parsing algorithms and metrics",
      "author" : [ "J. Goodman" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Goodman,? \\Q1996\\E",
      "shortCiteRegEx" : "Goodman",
      "year" : 1996
    }, {
      "title" : "A spectral algorithm for learning hidden Markov models",
      "author" : [ "D. Hsu", "S.M. Kakade", "T. Zhang" ],
      "venue" : "Journal of Computer and System Sciences",
      "citeRegEx" : "Hsu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2012
    }, {
      "title" : "Minimisation of Multiplicity Tree Automata",
      "author" : [ "S. Kiefer", "I. Marusic", "J. Worrell" ],
      "venue" : null,
      "citeRegEx" : "Kiefer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kiefer et al\\.",
      "year" : 2015
    }, {
      "title" : "Low-rank spectral learning with weighted loss functions",
      "author" : [ "A. Kulesza", "N. Jiang", "S. Singh" ],
      "venue" : "In Proceedings of AISTATS",
      "citeRegEx" : "Kulesza et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulesza et al\\.",
      "year" : 2015
    }, {
      "title" : "Low-Rank Spectral Learning",
      "author" : [ "A. Kulesza", "N.R. Rao", "S. Singh" ],
      "venue" : "In Proceedings of AISTATS",
      "citeRegEx" : "Kulesza et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kulesza et al\\.",
      "year" : 2014
    }, {
      "title" : "Numerical analysis: a second course. Siam",
      "author" : [ "J.M. Ortega" ],
      "venue" : null,
      "citeRegEx" : "Ortega,? \\Q1990\\E",
      "shortCiteRegEx" : "Ortega",
      "year" : 1990
    }, {
      "title" : "An annotation scheme for free word order languages",
      "author" : [ "W. Skut", "B. Krenn", "T. Brants", "H. Uszkoreit" ],
      "venue" : "In Conference on Applied Natural Language Processing",
      "citeRegEx" : "Skut et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Skut et al\\.",
      "year" : 1997
    }, {
      "title" : "T ‖F ‖v‖ and ‖T (I,v, I)‖F ≤ ‖T ‖F ‖v‖. Let δ = ‖E− Ê‖ and let σ be the smallest nonzero eigenvalue of the matrix I− E. It follows from (El Ghaoui, 2002, Equation (7.2)) that if δ < σ then ‖(I− E)−1 − (I− Ê)−1‖ ≤ δ/(σ(σ − δ))",
      "author" : [ "v)‖F" ],
      "venue" : "Since δ = O(ε)",
      "citeRegEx" : "I and ≤,? \\Q2002\\E",
      "shortCiteRegEx" : "I and ≤",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.",
      "startOffset" : 128,
      "endOffset" : 207
    }, {
      "referenceID" : 0,
      "context" : "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.",
      "startOffset" : 128,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.",
      "startOffset" : 128,
      "endOffset" : 207
    }, {
      "referenceID" : 2,
      "context" : "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.",
      "startOffset" : 128,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : ", 2014) and trees (Bailly et al., 2010; Cohen et al., 2014), and subsequent investigations into low-rank spectral learning for predictive state representations (Kulesza et al.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : ", 2014) and trees (Bailly et al., 2010; Cohen et al., 2014), and subsequent investigations into low-rank spectral learning for predictive state representations (Kulesza et al.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : ", 2014, 2015) and approximate minimization of weighted automata (Balle et al., 2015).",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "The idea of speeding up parsing with (L)PCFG by approximating the original model with a smaller one was recently studied in (Cohen and Collins, 2012; Cohen et al., 2013a), where a tensor decomposition technique was used in order to obtain the minimized model.",
      "startOffset" : 124,
      "endOffset" : 170
    }, {
      "referenceID" : 8,
      "context" : "It was observed in (Cohen and Collins, 2012; Cohen et al., 2013a) that a side-effect of reducing the size of a grammar learned from data was a slight improvement in parsing performance.",
      "startOffset" : 19,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "It is important to remark that in contrast with the tensor decompositions in (Cohen and Collins, 2012; Cohen et al., 2013a) which are susceptible to local optima problems, our approach resembles a power-method approach to SVD, which yields efficient globally convergent algorithms.",
      "startOffset" : 77,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "While WTA are usually defined over arbitrary ranked trees, only considering binary trees does not lead to any loss of generality since WTA on ranked trees are equivalent to WTA on binary trees (see (Bailly et al., 2010) for references).",
      "startOffset" : 198,
      "endOffset" : 219
    }, {
      "referenceID" : 6,
      "context" : "Theorem 1 ((Bozapalidis and Louscou-Bozapalidou, 1983)).",
      "startOffset" : 11,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "In the case of weighted automata on strings, (Balle et al., 2015) recently showed a polynomial time algorithm for computing the Gram matrices of a string Hankel matrix by solving a system of linear equations.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "Thus, we shall resort to a different 1If the WTA given to the algorithm is not minimal, a pre-processing step can be used to minimize the input using the algorithm from (Kiefer et al., 2015).",
      "startOffset" : 169,
      "endOffset" : 190
    }, {
      "referenceID" : 4,
      "context" : "It is shown in (Berstel and Reutenauer, 1982) that A⊗ computes the function fA⊗(t) = f(t)2.",
      "startOffset" : 15,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "2 Experimental Setup and Results In our experiments, we used the annotated corpus of german newspaper texts NEGRA (Skut et al., 1997).",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "2013a), who use tensor decomposition algorithms (Chi and Kolda, 2012) to decompose the tensors of an underlying PCFG.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "For parsing we use minimum Bayes risk decoding, maximizing the sum of the marginals for the nonterminals in the grammar, essentially choosing the best tree topology given a string (Goodman, 1996).",
      "startOffset" : 180,
      "endOffset" : 195
    } ],
    "year" : 2017,
    "abstractText" : "We describe a technique to minimize weighted tree automata (WTA), a powerful formalisms that subsumes probabilistic context-free grammars (PCFGs) and latent-variable PCFGs. Our method relies on a singular value decomposition of the underlying Hankel matrix defined by the WTA. Our main theoretical result is an efficient algorithm for computing the SVD of an infinite Hankel matrix implicitly represented as a WTA. We provide an analysis of the approximation error induced by the minimization, and we evaluate our method on real-world data originating in newswire treebank. We show that the model achieves lower perplexity than previous methods for PCFG minimization, and also is much more stable due to the absence of local optima.",
    "creator" : "TeX"
  }
}