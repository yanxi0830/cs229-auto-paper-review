{
  "name" : "1106.3397.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Stéphane CANU" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 6.\n33 97\nv1 [\ncs .L\nG ]\n1 7\nJu n\n20 11\nIndex Terms— support vector machines, maximal margin algorithm, uncertain labels.\n1. INTRODUCTION\nIn the mainstream supervised classification scheme, an expert is required for labelling a set of data used then as inputs for training the classifier. However, even for an expert, this labeling task is likely to be difficult in many applications. In the end the training data set may contain inaccurate classes for some examples, which leads to non robust classifiers[1]. For instance, this is often the case in medical imaging where radiologists have to outline what they think are malignant tissues over medical images without access to the reference histopatologic information. We propose to deal with these uncertainties by introducing probabilistic labels in the learning stage so as to: 1. stick to the real life annotation problem, 2. avoid discarding uncertain data, 3. balance the influence of uncertain data in the classification process. Our study focuses on the widely used Support Vector Machines (SVM) two-class classification problem [2]. This method aims a finding the separating hyperplane maximizing the margin between the examples of both classes. Several mappings from SVM scores to class membership probabilities have been proposed in the literature [3, 4]. In our\napproach, we propose to use both labels and probabilities as input thus learning simultaneously a classifier and a probabilistic output. Note that the output of our classifier may be transformed to probability estimations without using any mapping algorithm. In section 2 we define our new SVM problem formulation (referred to as P-SVM) to deal with certain and probabilistic labels simultaneously. Section 3 describes the whole framework of P-SVM and presents the associated quadratic problem. Finally, in section 5 we compare its performances to the classical SVM formulation (C-SVM) over different data sets to demonstrate its potential.\n2. PROBLEM FORMULATION\nWe present below a new formulation for the two-class classification problem dealing with uncertain labels. Let X be a feature space. We define (xi, li)i=1...m the learning dataset of input vectors (xi)i=1...m ∈ X along with their corresponding labels (li)i=1...m, the latter of which being\n• class labels: li = yi ∈ {−1,+1} for i = 1 . . . n (in classification),\n• real values: li = pi ∈ [0, 1] for i = n + 1 . . .m (in regression).\npi, associated to point xi allows to consider uncertainties about point xi’s class. We define it as the posterior probability for class 1.\npi = p(xi) = P(Yi = 1 | Xi = xi).\nWe define the associated pattern recognition problem as min w 1 2‖w‖ 2 (1)\nsubject to\n{\nyi(w ⊤xi + b) ≥ 1, i = 1...n z−i ≤ w ⊤xi + b ≤ z + i , i = n+ 1...m\nWhere boundaries z−i , z + i directly depend on pi. This formulation consists in minimizing the complexity of the model while forcing good classification and good probability estimation (close to pi). Obviously, if n = m, we are brought back to the classical SVM problem formulation.\nFollowing the idea of soft margin introduced in regular SVM to deal with the case of inseparable data, we introduce\nslack variables ξi. This measure the degree of misclassification of the datum xi thus relaxing hard constraints of the initial optimization problem which becomes\nmin w,ξ,ξ−,ξ+\n1 2 ‖w‖2 + C\nn∑\ni=1\nξi + C̃\nm∑\ni=n+1\n(ξ−i + ξ + i ) (2)\nsubject to \n \n\nyi(w ⊤xi + b) ≥ 1− ξi, i = 1...n z−i − ξ − i ≤ w ⊤xi + b ≤ z + i + ξ + i , i = n+ 1...m 0 ≤ ξi, i = 1...n 0 ≤ ξ−i and 0 ≤ ξ + i , i = n+ 1...m\nParametersC and C̃ are predefined positive real numbers controlling the relative weighting of classification and regression performances. Let ε be the labelling precision and δ the confidence we have in the labelling. Let’s define η = ε + δ. Then, the regression problem consists in finding optimal parameters w and b such that\n| 1\n1 + e−a(w ⊤xi+b)\n− pi |< η ,\nThus constraining the probability prediction for point xi to remain around to 1\n1+e−a(w ⊤xi+b)\nwithin distance η [5, 6, 7].\nThe boundaries (where w⊤xi + b = ±1), define parameter a as:\na = ln( 1η − 1)\nFinally: max(0, pi − η) ≤\n1\n1 + e−a(w ⊤xi+b)\n< min(pi + η, 1),\n⇐⇒ z−i ≤ w ⊤xi + b < z + i ,\nwhere z−i = − 1 a ln( 1 pi−η − 1) and z+i = − 1 a ln( 1 pi+η − 1).\n3. DUAL FORMULATION\nWe can rewrite the problem in its dual form, introducing Lagrange multipliers. We are looking for a stationary point for the Lagrange function L defined as\nL(w, b, ξ, ξ−, ξ+, α, β, µ+, µ−, γ+, γ−) =\n1 2‖w‖ 2 + C\nn∑\ni=1\nξi + C̃\nm∑\ni=n+1\n(ξ−i + ξ + i )\n− n∑\ni=1\nαi(yi(w ⊤xi + b)− (1 − ξi))−\nn∑\ni=1\nβiξi\n−\nm∑\ni=n+1\nµ−i ((w ⊤xi + b)− (z − i − ξ − i ))−\nm∑\ni=n+1\nγ−i ξ − i\n−\nm∑\ni=n+1\nµ+i ((z + i + ξ + i )− (w ⊤xi + b))−\nm∑\ni=n+1\nγ+i ξ + i\nwith α ≥ 0, β ≥ 0, µ+ ≥ 0, µ− ≥ 0,γ+ ≥ 0 and γ− ≥ 0 Computing the derivatives of L with respect to w, b, ξ, ξ− and\nξ+ leads to the following optimality conditions:\n  \n \n0 ≤ αi ≤ C, i = 1...n 0 ≤ µ+i ≤ C̃, i = n+ 1...m 0 ≤ µ−i ≤ C̃, i = n+ 1...m\nw = n∑\ni=1\nαiyixi − m∑\ni=n+1\n(µ+i − µ − i )xi\ny⊤α = m∑\ni=n+1\n(µ+i − µ − i )\nwhere e1 = [1 . . . 1︸ ︷︷ ︸ n times 0 . . . 0 ︸ ︷︷ ︸ (m-n) times ]⊤ and e2 = [0 . . . 0︸ ︷︷ ︸ n times 1 . . . 1 ︸ ︷︷ ︸ (m-n) times ]⊤. Calculations simplifications then lead to L(w, b, ξ, ξ−, ξ+, α, β, µ, γ+, γ−) =\n− 12w ⊤w +\nn∑\ni=1\nαi + m∑\ni=n+1\nµ−i z − i −\nm∑\ni=n+1\nµ+i z + i\nFinally, let Γ = [α1 . . . αn µ + n+1 . . . µ + m µ − n+1 . . . µ − m] ⊤ be a vector of dimension 2m− n. Then\nw⊤w = Γ⊤ G Γ where\nG =\n\n K1 − K2 K2 − K⊤2 K3 − K3\nK⊤2 − K3 K3\n\n\nwith K1 = (yiyjx ⊤ i xj)i,j=1...n,\nK2 = (x ⊤ i xjyi)i=1...n,j=n+1...m, K3 = (x ⊤ i xj)i,j=n+1...m,\nThe dual formulation becomes\n  \n \nmin Γ\n1 2Γ ⊤GΓ− ẽ⊤Γ,\nf⊤Γ = 0 with ẽ = [ 1 . . . 1\n︸ ︷︷ ︸\nn times\n−z+n+1 · · · − z + m ︸ ︷︷ ︸\nn-m times\nz−n+1 . . . z − m ︸ ︷︷ ︸\nn-m times\n]\nwith f⊤ = [y⊤,−1 · · · − 1 ︸ ︷︷ ︸\nn-m times\n, 1 . . . 1 ︸ ︷︷ ︸\nn-m times\n]\nand 0 ≤ Γ ≤ [C . . .C ︸ ︷︷ ︸\nn times\nC̃ . . . C̃ ︸ ︷︷ ︸\nn-m times\nC̃ . . . C̃ ︸ ︷︷ ︸\nn-m times\n]⊤\n(3)\n4. KERNELIZATION\nFormulations (2) and (3) can be easily generalized by introducing kernel functions. Let k be a positive kernel satisfying Mercer’s condition and H the associated Reproducing Kernel Hilbert Space (RKHS). Within this framework equation (2) becomes\nmin f,b,ξ,ξ−,ξ+\n1 2 ‖f‖2H + C\nn∑\ni=1\nξi + C̃\nm∑\ni=n+1\n(ξ−i + ξ + i ) (4)\nsubject to\n  \n\nyi(f(xi) + b) ≥ 1− ξi, i = 1...n z−i − ξ − i ≤ f(xi) + b ≤ z + i + ξ + i , i = n+ 1...m 0 ≤ ξi, i = 1...n 0 ≤ ξ−i and 0 ≤ ξ + i i = n+ 1...m\nFormulation (3) remains identical, with K1 = (yiyjk(xi, xj))i,j=1...n, K2 = (k(xi, xj)yi)i=1...n,j=n+1...m, K3 = (k(xi, xj))i,j=n+1...m,\n5. EXAMPLES\nIn order to experimentally evaluate the proposed method for handling uncertain labels in SVM classification, we have simulated different data sets described below. In these numerical examples, a RBF kernel k(u, v) = e−‖u−v‖\n2/2σ2 is used and C = C̃ = 100. We implemented our method using the SVMKM Toolbox [8]. We compare the classification performances and probabilistic predictions of the C-SVM and P-SVM approaches. In the first case, probabilities are estimated by using Platt’s scaling algorithm [3] while in the second case, probabilities are directly estimated via the formula defined in (2): P (y = 1|x) = 1\n1+e−a(w⊤x+b) . Performances are evaluated by\ncomputing • Accuracy (Acc)\nProportion of well predicted examples in the test set (for evaluating classification).\n• Kullback Leibler distance (KL)\nDKL(P ||Q) =\nn∑\ni=1\nP (yi = 1|xi) log( P (yi = 1|xi)\nQ(yi = 1|xi) )\nfor probability distributions P and Q (for evaluating probability estimation).\n5.1. Probability estimation\nWe generate two unidimensional datasets, labelled ’+1’ and ’-1’, from normal distributions of variances σ2−1= σ 2 1=0.3 and means µ−1=-0.5 and µ1=+0.5. Let’s (xli)i=1...nl denote the learning data set (nl=200) and (xti)i=1...nt the test set (nt=1000). We compute, for each point xi, its true probability P (yi = +1|xi) to belong to class ’+1’. From here on, learning data are labelled in two ways, as follows\na) For i = 1 . . . nl, we get the regular SVM dataset by simply using a probability of 0.5 as the threshold for assigning class labels yi associated to point xi. This is what would be done in practical cases when the data contains class membership probabilities and a SVM classifier is used.\nif P (yli = 1|x l i) > 0.5, then y l i = 1, if P (yli = 1|x l i) ≤ 0.5, then y l i = −1 (5)\nThis dataset (xli, y l i)i=1...nl is used to train the C-SVM\nclassifier. b) We define another data set (xli, ŷ l i)i=1...nl such that, for\ni = 1 . . . n,\nif P (yli = 1|x l i) > 1− η, then ŷ l i = 1, if P (yli = 1|x l i) < η, then ŷ l i = −1,\nŷli = P (y l i = 1|x l i) otherwise.\n(6) If the probability values are sufficiently close to 0 or 1 (closeness being defined by the precision and confidence), we admit that they belong respectively to class -1 or 1. This probabilistic dataset (xli, ŷ l i)i=1...nl is used to\ntrain the P-SVM algorithm. We compare our two approaches using the test set (xti)i=1...nt . As we know the true probabilities (P (yti = 1|x t i))i=1...nt , we can estimate the probability prediction error (KL). Figure 1 shows the probability predictions performances improvement shown by the P-SVM: the true probabilities (black) and P-SVM estimations (red) are quasi-superimposed (KL=0.2) whereas Platt’s estimations are less accurate (KL=11.3).\n5.2. Noise robustness\nWe generate two 2D datasets, labelled ’+1’ and ’-1’, from normal distributions of variances σ2−1=σ 2 1=0.7 and means µ−1 = (-0.3, -0.5) and µ1=(+0.3, +0.5). As in the previous experiment, we compute class ’1’ membership probability for each point xl of the learning data set. We simulate classification error by artificially adding a centered uniform noise (δ of amplitude 0.1), to the probabilities, such that for i = 1 . . . n,\nP̂ (yi = 1|xi) = P (yi = 1|xi) + δi.\nWe then label learning data following the same scheme as described in (5) and (6). Figure 2 shows the margin location and probabilities estimations using the two methods over a grid of values. Far from learning data points, both probability estimations are less accurate, this being directly linked to\nthe choice of a gaussian kernel. However, P-SVM classification and probability estimations obtained for 1000 test points, are clearly more alike the ground truth (AccP-SVM = 99% , KLP-SVM = 3.6) than C-SVM (AccC-SVM = 95%, KLC-SVM = 95). Contrary to P-SVM which, by combining both classification and regression, predicts good probabilities, C-SVM is sensitive to classification noise and is no more converging to the Bayes rule as seen in [1].\nFigure 3 shows the impact of noise amplitude on classifiers performances (values are averaged over 30 random simulations). Even if noise increases, classifications and probability predictions performances of the P-SVM remain significantly higher than those of C-SVM.\n6. CONCLUSION\nThis paper has presented a new way to take into account both qualitative and quantitative target data by shrewdly combin-\ning both SVM classification and regression loss. Experimental results show that our formulation can perform very well on simulated data for discrimination as well as posterior probability estimation. This approach will soon be applied on clinical data thus allowing to assess its usefulness in computer assisted diagnosis for prostate cancer. Note that this framework initially designed for probabilistic labels can also be generalized to other dataset involving quantitative data as it can be used for instance to estimate a conditional cumulative distribution function.\n7. REFERENCES\n[1] G. Stempfel and L. Ralaivola, “Learning SVMs from Sloppily Labeled Data,” Artificial Neural Networks–ICANN 2009, pp. 884–893, 2009.\n[2] Bernhard Schölkopf and Alexander J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning), The MIT Press, 1st edition, December 2001.\n[3] John C. Platt, “Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods,” in Advances in large margin classifiers. 1999, pp. 61–74, MIT Press.\n[4] Peter Sollich, “Probabilistic methods for support vector machines,” in Advances in Neural Information Processing Systems 12. 2000, pp. 349–355, MIT Press.\n[5] S. Rüping, “A Simple Method For Estimating Conditional Probabilities For SVMs,” in LWA 2004, Bickel S. Brefeld U. Drost I. Henze N. Herden O. Minor M. Scheffer T. Stojanovic L. Abecker, A. and S. Weibelza hl, Eds., 2004.\n[6] Y. Grandvalet, J. Mariéthoz, and S. Bengio, “A probabilistic interpretation of SVMs with an application to unbalanced classification,” Advances in Neural Information Processing Systems, vol. 18, pp. 467, 2006.\n[7] S. Rueping, “SVM Classifier Estimation from Group Probabilities,” in ICML 2010.\n[8] S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotomamonjy, “Svm and kernel methods matlab toolbox,” Perception Systèmes et Information, INSA de Rouen, Rouen, France, 2005.\n−1.5 −1 −0.5 0 0.5 1 1.5 0\n0.5\n1\nx\npr ob\nab ili\nty\nP(x|y=1) P(x|y=−1) P(y=1|x)\n−1.5 −1 −0.5 0 0.5 1 1.5 0\n0.5\n1\nx\npr ob\nab ili\nty\nP(y=1|x) P(y=1|x) C−SVM + Platt P(y=1|x) P−SVM\n−1.5 −1 −0.5 0 0.5 1 1.5 0\n0.2\n0.4\n0.6\n0.8\n1\nx\npr ob\nab ili\nty\nP(x|y=1) P(x|y=−1) P(y=1|x)\n−1.5 −1 −0.5 0 0.5 1 1.5 0\n0.2\n0.4\n0.6\n0.8\n1\nx\npr ob\nab ili\nty\nP(y=1|x) P(y=1|x) C−SVM + Platt P(y=1|x) P−SVM\n0.5\n0.5\n0.5\n0.5\nC−SVM + Platt probability estimates\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nP−SVM probability estimates\n0.5\n0.5\n0.5\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nTrue probabilities\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nC−SVM + Platt probability estimates\n−1 0 1\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nP−SVM probability estimates\n−1 0 1\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nTrue probabilities\n−1 0 1\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nPlatt probability estimates\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.5\n0.5\n0.5\n0.5\noutputs of the extended SVM\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.5\n0.5\n0.5\n0.5\nTrue probabilities\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0. 5\n0.5\n0.50.5\nPlatt probability estimates\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.5\n0.5\n0.5\n0.5\noutputs of the extended SVM\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n0.5\n0.5\n0.5\n0.5\nTrue probabilities\n−1.5 −1 −0.5 0 0.5 1 1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5"
    } ],
    "references" : [ {
      "title" : "Learning SVMs from Sloppily Labeled Data,",
      "author" : [ "G. Stempfel", "L. Ralaivola" ],
      "venue" : "Artificial Neural Networks–ICANN",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning)",
      "author" : [ "Bernhard Schölkopf", "Alexander J. Smola" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods,” in Advances in large margin classifiers",
      "author" : [ "John C. Platt" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1999
    }, {
      "title" : "Probabilistic methods for support vector machines,",
      "author" : [ "Peter Sollich" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "A Simple Method For Estimating Conditional Probabilities For SVMs,",
      "author" : [ "S. Rüping" ],
      "venue" : "LWA",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "and S",
      "author" : [ "Y. Grandvalet", "J. Mariéthoz" ],
      "venue" : "Bengio, “A probabilistic interpretation of SVMs with an application to unbalanced classification,” Advances in Neural Information Processing Systems, vol. 18, pp. 467",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "SVM Classifier Estimation from Group Probabilities,",
      "author" : [ "S. Rueping" ],
      "venue" : "ICML",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "and A",
      "author" : [ "S. Canu", "Y. Grandvalet", "V. Guigue" ],
      "venue" : "Rakotomamonjy, “Svm and kernel methods matlab toolbox,” Perception Systèmes et Information, INSA de Rouen, Rouen, France",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In the end the training data set may contain inaccurate classes for some examples, which leads to non robust classifiers[1].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "Our study focuses on the widely used Support Vector Machines (SVM) two-class classification problem [2].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "Several mappings from SVM scores to class membership probabilities have been proposed in the literature [3, 4].",
      "startOffset" : 104,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Several mappings from SVM scores to class membership probabilities have been proposed in the literature [3, 4].",
      "startOffset" : 104,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "n (in classification), • real values: li = pi ∈ [0, 1] for i = n + 1 .",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e ⊤xi+b) − pi |< η , Thus constraining the probability prediction for point xi to remain around to 1 1+e−a(w xi+b) within distance η [5, 6, 7].",
      "startOffset" : 227,
      "endOffset" : 236
    }, {
      "referenceID" : 5,
      "context" : "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e ⊤xi+b) − pi |< η , Thus constraining the probability prediction for point xi to remain around to 1 1+e−a(w xi+b) within distance η [5, 6, 7].",
      "startOffset" : 227,
      "endOffset" : 236
    }, {
      "referenceID" : 6,
      "context" : "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e ⊤xi+b) − pi |< η , Thus constraining the probability prediction for point xi to remain around to 1 1+e−a(w xi+b) within distance η [5, 6, 7].",
      "startOffset" : 227,
      "endOffset" : 236
    }, {
      "referenceID" : 7,
      "context" : "We implemented our method using the SVMKM Toolbox [8].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "In the first case, probabilities are estimated by using Platt’s scaling algorithm [3] while in the second case, probabilities are directly estimated via the formula defined in (2): P (y = 1|x) = 1 1+e−a(w⊤x+b) .",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Contrary to P-SVM which, by combining both classification and regression, predicts good probabilities, C-SVM is sensitive to classification noise and is no more converging to the Bayes rule as seen in [1].",
      "startOffset" : 201,
      "endOffset" : 204
    } ],
    "year" : 2011,
    "abstractText" : "This paper addresses the pattern classification problem arising when available target data include some uncertainty information. Target data considered here is either qualitative (a class label) or quantitative (an estimation of the posterior probability). Our main contribution is a SVM inspired formulation of this problem allowing to take into account class label through a hinge loss as well as probability estimates using ε-insensitive cost function together with a minimum norm (maximum margin) objective. This formulation shows a dual form leading to a quadratic problem and allows the use of a representer theorem and associated kernel. The solution provided can be used for both decision and posterior probability estimation. Based on empirical evidence our method outperforms regular SVM in terms of probability predictions and classification performances.",
    "creator" : "LaTeX with hyperref package"
  }
}