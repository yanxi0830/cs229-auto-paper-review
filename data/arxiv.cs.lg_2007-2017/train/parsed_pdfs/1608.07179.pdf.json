{
  "name" : "1608.07179.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Minimizing Quadratic Functions in Constant Time",
    "authors" : [ "Kohei Hayashi" ],
    "emails" : [ "hayashi.kohei@gmail.com", "yyoshida@nii.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n07 17\n9v 1\n[ cs\n.L G\n] 2\n5 A"
    }, {
      "heading" : "1 Introduction",
      "text" : "A quadratic function is one of the most important function classes in machine learning, statistics, and data mining. Many fundamental problems such as linear regression, k-means clustering, principal component analysis, support vector machines, and kernel methods [14] can be formulated as a minimization problem of a quadratic function.\nIn some applications, it is sufficient to compute the minimum value of a quadratic function rather than its solution. For example, Yamada et al. [21] proposed an efficient method for estimating the Pearson divergence, which provides useful information about data, such as the density ratio [18]. They formulated the estimation problem as the minimization of a squared loss and showed that the Pearson divergence can be estimated from the minimum value. The least-squares mutual information [19] is another example that can be computed in a similar manner.\nDespite its importance, the minimization of a quadratic function has the issue of scalability. Let n ∈ N be the number of variables (the “dimension” of the problem). In general, such a minimization problem can be solved by quadratic programming (QP), which requires poly(n) time. If the problem is convex and there are no constraints, then the problem is reduced to solving a system of linear equations, which requires O(n3) time. Both methods easily become infeasible, even for mediumscale problems, say, n > 10000.\nAlthough several techniques have been proposed to accelerate quadratic function minimization, they require at least linear time in n. This is problematic when handling problems with an ultrahigh dimension, for which even linear time is slow or prohibitive. For example, stochastic gradient descent (SGD) is an optimization method that is widely used for large-scale problems. A nice property of this method is that, if the objective function is strongly convex, it outputs a point that is sufficiently close to an optimal solution after a constant number of iterations [5]. Nevertheless, in each iteration, we need at least O(n) time to access the variables. Another technique is lowrank approximation such as Nyström’s method [20]. The underlying idea is the approximation of the problem by using a low-rank matrix, and by doing so, we can drastically reduce the time\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\ncomplexity. However, we still need to compute the matrix–vector product of size n, which requires O(n) time. Clarkson et al. [7] proposed sublinear-time algorithms for special cases of quadratic function minimization. However, it is “sublinear” with respect to the number of pairwise interactions of the variables, which is O(n2), and their algorithms require O(n logc n) time for some c ≥ 1.\nOur contributions: Let A ∈ Rn×n be a matrix and d, b ∈ Rn be vectors. Then, we consider the following quadratic problem:\nminimize v∈Rn pn,A,d,b(v), where pn,A,d,b(v) = 〈v, Av〉 + n〈v, diag(d)v〉+ n〈b,v〉. (1)\nHere, 〈·, ·〉 denotes the inner product and diag(d) denotes the matrix whose diagonal entries are specified by d. Note that a constant term can be included in (1); however, it is irrelevant when optimizing (1), and hence we ignore it.\nLet z∗ ∈ R be the optimal value of (1) and let ǫ, δ ∈ (0, 1) be parameters. Then, the main goal of this paper is the computation of z with |z− z∗| = O(ǫn2) with probability at least 1− δ in constant time, that is, independent of n. Here, we assume the real RAM model [6], in which we can perform basic algebraic operations on real numbers in one step. Moreover, we assume that we have query accesses to A, b, and d, with which we can obtain an entry of them by specifying an index. We note that z∗ is typically Θ(n2) because 〈v, Av〉 consists of Θ(n2) terms, and 〈v, diag(d)v〉 and 〈b,v〉 consist of Θ(n) terms. Hence, we can regard the error of Θ(ǫn2) as an error of Θ(ǫ) for each term, which is reasonably small in typical situations.\nLet ·|S be an operator that extracts a submatrix (or subvector) specified by an index set S ⊂ N; then, our algorithm is defined as follows, where the parameter k := k(ǫ, δ) will be determined later.\nAlgorithm 1\nInput: An integer n ∈ N, query accesses to the matrix A ∈ Rn×n and to the vectors d, b ∈ Rn, and ǫ, δ > 0\n1: S ← a sequence of k = k(ǫ, δ) indices independently and uniformly sampled from {1, 2, . . . , n}. 2: return n 2\nk2 minv∈Rn pk,A|S,d|S,b|S (v).\nIn other words, we sample a constant number of indices from the set {1, 2, . . . , n}, and then solve the problem (1) restricted to these indices. Note that the number of queries and the time complexity are O(k2) and poly(k), respectively. In order to analyze the difference between the optimal values of pn,A,d,b and pk,A|S ,d|S,b|S , we want to measure the “distances” between A and A|S , d and d|S , and b and b|S , and want to show them small. To this end, we exploit graph limit theory, initiated by Lovász and Szegedy [11] (refer to [10] for a book), in which we measure the distance between two graphs on different number of vertices by considering continuous versions. Although the primary interest of graph limit theory is graphs, we can extend the argument to analyze matrices and vectors.\nUsing synthetic and real settings, we demonstrate that our method is orders of magnitude faster than standard polynomial-time algorithms and that the accuracy of our method is sufficiently high.\nRelated work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25]. However, as far as we know, no such algorithm is known for continuous optimization problems.\nA related notion is property testing [9, 17], which aims to design constant-time algorithms that distinguish inputs satisfying some predetermined property from inputs that are “far” from satisfying it. Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].\nOrganization In Section 2, we introduce the basic notions from graph limit theory. In Section 3, we show that we can obtain a good approximation to (a continuous version of) a matrix by sampling a constant-size submatrix in the sense that the optimizations over the original matrix and the submatrix are essentially equivalent. Using this fact, we prove the correctness of Algorithm 1 in Section 4. We show our experimental results in Section 5."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "For an integer n, let [n] denote the set {1, 2, . . . , n}. The notation a = b± c means that b− c ≤ a ≤ b+ c. In this paper, we only consider functions and sets that are measurable.\nLet S = (x1, . . . , xk) be a sequence of k indices in [n]. For a vector v ∈ Rn, we denote the restriction of v to S by v|S ∈ Rk; that is, (v|S)i = vxi for every i ∈ [k]. For the matrix A ∈ R\nn×n, we denote the restriction of A to S by A|S ∈ Rk×k; that is, (A|S)ij = Axixj for every i, j ∈ [k]."
    }, {
      "heading" : "2.1 Dikernels",
      "text" : "Following [12], we call a (measurable) function f : [0, 1]2 → R a dikernel. A dikernel is a generalization of a graphon [11], which is symmetric and whose range is bounded in [0, 1]. We can regard a dikernel as a matrix whose index is specified by a real value in [0, 1]. We stress that the term dikernel has nothing to do with kernel methods. For two functions f, g : [0, 1] → R, we define their inner product as 〈f, g〉 = ∫ 1 0 f(x)g(x)dx. For a dikernel W : [0, 1]2 → R and a function f : [0, 1] → R, we define a function Wf : [0, 1] → R as (Wf)(x) = 〈W (x, ·), f〉.\nLet W : [0, 1]2 → R be a dikernel. The Lp norm ‖W‖p for p ≥ 1 and the cut norm ‖W‖ of W are defined as ‖W‖p = (∫ 1\n0 ∫ 1 0 |W (x, y)|pdxdy )1/p and ‖W‖ = supS,T⊆[0,1] ∣∣∣ ∫ S ∫ T W (x, y)dxdy ∣∣∣, respectively, where the supremum is over all pairs of subsets. We note that these norms satisfy the triangle inequalities and ‖W‖ ≤ ‖W‖1.\nLet λ be a Lebesgue measure. A map π : [0, 1] → [0, 1] is said to be measure-preserving, if the pre-image π−1(X) is measurable for every measurable set X , and λ(π−1(X)) = λ(X). A measure-preserving bijection is a measure-preserving map whose inverse map exists and is also measurable (and then also measure-preserving). For a measure preserving bijection π : [0, 1] → [0, 1] and a dikernel W : [0, 1]2 → R, we define the dikernel π(W ) : [0, 1]2 → R as π(W )(x, y) = W (π(x), π(y))."
    }, {
      "heading" : "2.2 Matrices and Dikernels",
      "text" : "Let W : [0, 1]2 → R be a dikernel and S = (x1, . . . , xk) be a sequence of elements in [0, 1]. Then, we define the matrix W |S ∈ Rk×k so that (W |S)ij = W (xi, xj).\nWe can construct the dikernel Â : [0, 1]2 → R from the matrix A ∈ Rn×n as follows. Let I1 = [0, 1n ], I2 = ( 1 n , 2 n ], . . . , In = ( n−1 n , . . . , 1]. For x ∈ [0, 1], we define in(x) ∈ [n] as a unique integer such that x ∈ Ii. Then, we define Â(x, y) = Ain(x)in(y). The main motivation for creating a dikernel from a matrix is that, by doing so, we can define the distance between two matrices A and B of different sizes via the cut norm, that is, ‖Â− B̂‖ .\nWe note that the distribution of A|S , where S is a sequence of k indices that are uniformly and independently sampled from [n] exactly matches the distribution of Â|S , where S is a sequence of k elements that are uniformly and independently sampled from [0, 1]."
    }, {
      "heading" : "3 Sampling Theorem and the Properties of the Cut Norm",
      "text" : "In this section, we prove the following theorem, which states that, given a sequence of dikernels W 1, . . . ,WT : [0, 1]2 → [−L,L], we can obtain a good approximation to them by sampling a sequence of a small number of elements in [0, 1]. Formally, we prove the following:\nTheorem 3.1. Let W 1, . . . ,WT : [0, 1]2 → [−L,L] be dikernels. Let S be a sequence of k elements uniformly and independently sampled from [0, 1]. Then, with a probability of at least 1− exp(−Ω(kT/ log2 k)), there exists a measure-preserving bijection π : [0, 1] → [0, 1] such that, for any functions f, g : [0, 1] → [−K,K] and t ∈ [T ], we have\n|〈f,W tg〉 − 〈f, π(Ŵ t|S)g〉| = O ( LK2 √ T/ log2 k ) .\nWe start with the following lemma, which states that, if a dikernel W : [0, 1]2 → R has a small cut norm, then 〈f,Wf〉 is negligible no matter what f is. Hence, we can focus on the cut norm when proving Theorem 3.1.\nLemma 3.2. Let ǫ ≥ 0 and W : [0, 1]2 → R be a dikernel with ‖W‖ ≤ ǫ. Then, for any functions f, g : [0, 1] → [−K,K], we have |〈f,Wg〉| ≤ ǫK2.\nProof. For τ ∈ R and the function h : [0, 1] → R, let Lτ (h) := {x ∈ [0, 1] | h(x) = τ} be the level set of h at τ . For f ′ = f/K and g′ = g/K , we have\n|〈f,Wg〉| = K2|〈f ′,Wg′〉| = K2 ∣∣∣ ∫ 1\n−1\n∫ 1\n−1\nτ1τ2\n∫\nLτ1(f ′)\n∫\nLτ2 (g ′)\nW (x, y)dxdydτ1dτ2 ∣∣∣\n≤ K2 ∫ 1\n−1\n∫ 1\n−1\n|τ1||τ2| ∣∣∣∣∣ ∫\nLτ1(f ′)\n∫\nLτ2(g ′)\nW (x, y)dxdy ∣∣∣∣∣ dτ1dτ2\n≤ ǫK2 ∫ 1\n−1\n∫ 1\n−1\n|τ1||τ2|dτ1dτ2 = ǫK 2.\nTo introduce the next technical tool, we need several definitions. We say that the partition Q is a refinement of the partitionP = (V1, . . . , Vp) if Q is obtained by splitting each set Vi into one or more parts. The partition P = (V1, . . . , Vp) of the interval [0, 1] is called an equipartition if λ(Vi) = 1/p for every i ∈ [p]. For the dikernel W : [0, 1]2 → R and the equipartition P = (V1, . . . , Vp) of [0, 1], we define WP : [0, 1]2 → R as the function obtained by averaging each Vi × Vj for i, j ∈ [p]. More formally, we define\nWP (x, y) = 1\nλ(Vi)λ(Vj)\n∫\nVi×Vj\nW (x′, y′)dx′dy′ = p2 ∫\nVi×Vj\nW (x′, y′)dx′dy′,\nwhere i and j are unique indices such that x ∈ Vi and y ∈ Vj , respectively.\nThe following lemma states that any function W : [0, 1]2 → R can be well approximated by WP for the equipartition P into a small number of parts.\nLemma 3.3 (Weak regularity lemma for functions on [0, 1]2 [8]). Let P be an equipartition of [0, 1] into k sets. Then, for any dikernel W : [0, 1]2 → R and ǫ > 0, there exists a refinement Q of P with |Q| ≤ k2C/ǫ 2 for some constant C > 0 such that\n‖W −WQ‖ ≤ ǫ‖W‖2.\nCorollary 3.4. Let W 1, . . . ,WT : [0, 1]2 → R be dikernels. Then, for any ǫ > 0, there exists an equipartition P into |P| ≤ 2CT/ǫ 2 parts for some constant C > 0 such that, for every t ∈ [T ],\n‖W t −W tP‖ ≤ ǫ‖W t‖2.\nProof. Let P0 be a trivial partition, that is, a partition consisting of a single part [n]. Then, for each t ∈ [T ], we iteratively apply Lemma 3.3 with Pt−1, W t, and ǫ, and we obtain the partition Pt into at most |Pt−1|2C/ǫ 2\nparts such that ‖W t −W tPt‖ ≤ ǫ‖W t‖2. Since Pt is a refinement of Pt−1,\nwe have ‖W i −W iPt‖ ≤ ‖W i −W iPt−1‖ for every i ∈ [t − 1]. Then, P T satisfies the desired property with |PT | ≤ (2C/ǫ 2 )T = 2CT/ǫ 2 .\nAs long as S is sufficiently large, W and Ŵ |S are close in the cut norm:\nLemma 3.5 ((4.15) of [4]). Let W : [0, 1]2 → [−L,L] be a dikernel and S be a sequence of k elements uniformly and independently sampled from [0, 1]. Then, we have\n− 2L\nk ≤ ES‖Ŵ |S‖ − ‖W‖ <\n8L\nk1/4 .\nFinally, we need the following concentration inequality.\nLemma 3.6 (Azuma’s inequality). Let (Ω, A, P ) be a probability space, k be a positive integer, and C > 0. Let z = (z1, . . . , zk), where z1, . . . , zk are independent random variables, and zi takes values in some measure space (Ωi, Ai). Let f : Ω1 × · · · × Ωk → R be a function. Suppose that |f(x)− f(y)| ≤ C whenever x and y only differ in one coordinate. Then\nPr [ |f(z)−Ez[f(z)]| > λC ] < 2e−λ 2/2k.\nNow we prove the counterpart of Theorem 3.1 for the cut norm.\nLemma 3.7. Let W 1, . . . ,WT : [0, 1]2 → [−L,L] be dikernels. Let S be a sequence of k elements uniformly and independently sampled from [0, 1]. Then, with a probability of at least 1− exp(−Ω(kT/ log2 k)), there exists a measure-preserving bijection π : [0, 1] → [0, 1] such that, for every t ∈ [T ], we have\n‖W t − π(Ŵ t|S)‖ = O ( L √ T/ log2 k ) .\nProof. First, we bound the expectations and then prove their concentrations. We apply Corollary 3.4 to W 1, . . . ,WT and ǫ, and let P = (V1, . . . , Vp) be the obtained partition with p ≤ 2CT/ǫ 2\nparts such that\n‖W t −W tP‖ ≤ ǫL.\nfor every t ∈ [T ]. By Lemma 3.5, for every t ∈ [T ], we have\nES‖Ŵ tP |S − Ŵ t|S‖ = ES‖(W t P −W t)|Ŝ‖ ≤ ǫL+ 8L\nk1/4 .\nThen, for any measure-preserving bijection π : [0, 1] → [0, 1] and t ∈ [T ], we have\nES‖W t − π(Ŵ t|S)‖ ≤ ‖W t −W tP‖ +ES‖W t P − π(Ŵ t P |S)‖ +ES‖π(Ŵ t P |S)− π(Ŵ t|S)‖\n≤ 2ǫL+ 8L\nk1/4 +ES‖W\nt P − π(Ŵ t P |S)‖ . (2)\nThus, we are left with the problem of sampling from P . Let S = {x1, . . . , xk} be a sequence of independent random variables that are uniformly distributed in [0, 1], and let Zi be the number of points xj that fall into the set Vi. It is easy to compute that\nE[Zi] = k\np and Var[Zi] = (1 p − 1 p2 ) k < k p .\nThe partition P ′ of [0, 1] is constructed into the sets V ′1 , . . . , V ′ p such that λ(V ′ i ) = Zi/k and λ(Vi ∩ V ′i ) = min(1/p, Zi/k). For each t ∈ [T ], we construct the dikernel W t : [0, 1] → R such that the value of W t\non V ′i × V ′ j is the same as the value of W t P on Vi × Vj . Then, W t agrees with W tP on\nthe set Q = ⋃\ni,j∈[p](Vi∩V ′ i )×(Vj∩V ′ j ). Then, there exists a bijection π such that π(Ŵ t P |S) = W\nt\nfor each t ∈ [T ]. Then, for every t ∈ [T ], we have\n‖W tP − π(Ŵ t P |S)‖ = ‖W t P −W\nt ‖ ≤ ‖W\nt P −W t ‖1 ≤ 2L(1− λ(Q))\n= 2L ( 1− (∑\ni∈[p]\nmin (1 p , Zi k ))2) ≤ 4L ( 1− ∑\ni∈[p]\nmin (1 p , Zi k ))\n= 2L ∑\ni∈[p]\n∣∣∣1 p − Zi k ∣∣∣ ≤ 2L ( p ∑\ni∈[p]\n(1 p − Zi k )2)1/2 ,\nwhich we rewrite as\n‖W tP − π(Ŵ t P |S)‖ 2 ≤ 4L2p\n∑\ni∈[p]\n(1 p − Zi k )2 .\nThe expectation of the right hand side is (4L2p/k2) ∑\ni∈[p] Var(Zi) < 4L 2p/k. By the Cauchy-\nSchwartz inequality, E‖W tP − π(Ŵ t P |S)‖ ≤ 2L\n√ p/k.\nInserted this into (2), we obtain\nE‖W t − π(Ŵ t|S)‖ ≤ 2ǫL+ 8L\nk1/4 + 2L\n√ p\nk ≤ 2ǫL+\n8L\nk1/4 +\n2L\nk1/2 2CT/ǫ\n2\n.\nChoosing ǫ = √ CT/(log2 k 1/4) = √ 4CT/(log2 k), we obtain the upper bound\nE‖W t − π(Ŵ t|S)‖ ≤ 2L\n√ 4CT\nlog2 k +\n8L\nk1/4 +\n2L\nk1/4 = O\n( L\n√ T\nlog2 k\n) .\nObserving that ‖W t − π(Ŵ t|S)‖ changes by at most O(L/k) if one element in S changes, we apply Azuma’s inequality with λ = k √ T/ log2 k and the union bound to complete the proof.\nThe proof of Theorem 3.1 is immediately follows from Lemmas 3.2 and 3.7."
    }, {
      "heading" : "4 Analysis of Algorithm 1",
      "text" : "In this section, we analyze Algorithm 1. Because we want to use dikernels for the analysis, we introduce a continuous version of pn,A,d,b (recall (1)). The real-valued function Pn,A,d,b on the functions f : [0, 1] → R is defined as\nPn,A,d,b(f) = 〈f, Âf〉+ 〈f 2, d̂1⊤1〉+ 〈f, b̂1⊤1〉,\nwhere f2 : [0, 1] → R is a function such that f2(x) = f(x)2 for every x ∈ [0, 1] and 1 : [0, 1] → R is the constant function that has a value of 1 everywhere. The following lemma states that the minimizations of pn,A,d,b and Pn,A,d,b are equivalent:\nLemma 4.1. Let A ∈ Rn×n be a matrix and d, b ∈ Rn×n be vectors. Then, we have\nmin v∈[−K,K]n\npn,A,d,b(v) = n 2 · inf\nf :[0,1]→[−K,K] Pn,A,d,b(f).\nfor any K > 0.\nProof. First, we show that n2 · inff :[0,1]→[−K,K] Pn,A,d,b(f) ≤ minv∈[−K,K]n pn,A,d,b(v). Given a vector v ∈ [−K,K]n, we define f : [0, 1] → [−K,K] as f(x) = vin(x). Then,\n〈f, Âf〉 = ∑\ni,j∈[n]\n∫\nIi\n∫\nIj\nAijf(x)f(y)dxdy = 1\nn2\n∑\ni,j∈[n]\nAijvivj = 1\nn2 〈v, Av〉,\n〈f2, d̂1⊤1〉 = ∑\ni,j∈[n]\n∫\nIi\n∫\nIj\ndif(x) 2dxdy =\n∑\ni∈[n]\n∫\nIi\ndif(x) 2dx =\n1\nn\n∑\ni∈[n]\ndiv 2 i =\n1 n 〈v, diag(d)v〉,\n〈f, b̂1⊤1〉 = ∑\ni,j∈[n]\n∫\nIi\n∫\nIj\nbif(x)dxdy = ∑\ni∈[n]\n∫\nIi\nbif(x)dx = 1\nn\n∑\ni∈[n]\nbivi = 1\nn 〈v, b〉.\nThen, we have n2Pn,A,d,b(f) ≤ pn,A,d,b(v).\nNext, we show that minv∈[−K,K]n pn,A,d,b(v) ≤ n2 · inff :[0,1]→[−K,K] Pn,A,d,b(f). Let f : [0, 1] → [−K,K] be a measurable function. Then, for x ∈ [0, 1], we have\n∂Pn,A,d,b(f(x))\n∂f(x) =\n∑\ni∈[n]\n∫\nIi\nAiin(x)f(y)dy + ∑\nj∈[n]\n∫\nIj\nAin(x)jf(y)dy + 2din(x)f(x) + bin(x).\nNote that the form of this partial derivative only depends on in(x); hence, in the optimal solution f∗ : [0, 1] → [−K,K], we can assume f∗(x) = f∗(y) if in(x) = in(y). In other words, f∗ is constant on each of the intervals I1, . . . , In. For such f∗, we define the vector v ∈ Rn as vi = f ∗(x), where x ∈ [0, 1] is any element in Ii. Then, we have\n〈v, Av〉 = ∑\ni,j∈[n]\nAijvivj = n 2\n∑\ni,j∈[n]\n∫\nIi\n∫\nIj\nAijf ∗(x)f∗(y)dxdy = n2〈f∗, Âf∗〉,\n〈v, diag(d)v〉 = ∑\ni∈[n]\ndiv 2 i = n\n∑\ni∈[n]\n∫\nIi\ndif ∗(x)2dx = n〈(f∗)2, d̂1T 1〉,\n〈v, b〉 = ∑\ni∈[n]\nbivi = n ∑\ni∈[n]\n∫\nIi\nbif ∗(x)dx = n〈f∗, b̂1T 1〉.\nFinally, we have pn,A,d,b(v) ≤ n2Pn,A,d,b(f∗).\nNow we show that Algorithm 1 well-approximates the optimal value of (1) in the following sense:\nTheorem 4.2. Let v∗ and z∗ be an optimal solution and the optimal value, respectively, of problem (1). By choosing k(ǫ, δ) = 2Θ(1/ǫ\n2) + Θ(log 1δ log log 1 δ ), with a probability of at least\n1 − δ, a sequence S of k indices independently and uniformly sampled from [n] satisfies the following: Let ṽ∗ and z̃∗ be an optimal solution and the optimal value, respectively, of the problem minv∈Rk pk,A|S ,d|S,b|S (v). Then, we have\n∣∣∣n 2\nk2 z̃∗ − z∗\n∣∣∣ ≤ ǫLK2n2,\nwhere K = max{maxi∈[n] |v∗i |,maxi∈[n] |ṽ ∗ i |} and L = max{maxi,j |Aij |,maxi |di|,maxi |bi|}.\nProof. We instantiate Theorem 3.1 with k = 2Θ(1/ǫ 2) + Θ(log 1δ log log 1 δ ) and the dikernels Â, d̂1⊤, and b̂1⊤. Then, with a probability of at least 1− δ, there exists a measure preserving bijection π : [0, 1] → [0, 1] such that\nmax { |〈f, (Â− π(Â|S))f〉|, |〈f 2, (d̂1⊤ − π(d̂1⊤|S))1〉|, |〈f, (b̂1⊤ − π(b̂1⊤|S))1〉| } ≤ ǫLK2\n3\nfor any function f : [0, 1] → [−K,K]. Then, we have\nz̃∗ = min v∈Rk pk,A|S ,d|S,b|S(v) = min v∈[−K,K]k pk,A|S,d|S ,b|S(v)\n= k2 · inf f :[0,1]→[−K,K] Pk,A|S ,d|S,b|S(f) (By Lemma 4.1)\n= k2 · inf f :[0,1]→[−K,K]\n( 〈f, (π(Â|S)− Â)f〉+ 〈f, Âf〉+ 〈f 2, (π(d̂1⊤|S)− d̂1⊤)1〉+ 〈f 2, d̂1⊤1〉+\n〈f, (π(b̂1⊤|S)− b̂1⊤)1〉+ 〈f, b̂1⊤1〉 )\n≤ k2 · inf f :[0,1]→[−K,K]\n( 〈f, Âf〉+ 〈f2, d̂1⊤1〉+ 〈f, b̂1⊤1〉 ± ǫLK2 )\n= k2\nn2 · min v∈[−K,K]n\npn,A,d,b(v)± ǫLK 2k2. (By Lemma 4.1)\n= k2\nn2 · min v∈Rn\npn,A,d,b(v)± ǫLK 2k2 =\nk2 n2 z∗ ± ǫLK2k2.\nRearranging the inequality, we obtain the desired result.\nWe can show that K is bounded when A is symmetric and full rank. To see this, we first note that we can assume A + ndiag(d) is positive-definite, as otherwise pn,A,d,b is not bounded and the problem is uninteresting. Then, for any set S ⊆ [n] of k indices, (A + ndiag(d))|S is again positive-definite because it is a principal submatrix. Hence, we have v∗ = (A + ndiag(d))−1nb/2 and ṽ∗ = (A|S + ndiag(d|S))−1nb|S/2, which means that K is bounded."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we demonstrate the effectiveness of our method by experiment. All experiments were conducted on an Amazon EC2 c3.8xlarge instance. Error bars indicate the standard deviations over ten trials with different random seeds.\nNumerical simulation We investigated the actual relationships between n, k, and ǫ. To this end, we prepared synthetic data as follows. We randomly generated inputs as Aij ∼ U[−1,1], di ∼ U[0,1], and bi ∼ U[−1,1] for i, j ∈ [n], where U[a,b] denotes the uniform distribution with the support [a, b]. After that, we solved (1) by using Algorithm 1 and compared it with the exact solution obtained by QP.1 The result (Figure 1) show the approximation errors were evenly controlled regardless of n, which meets the error analysis (Theorem 4.2).\nApplication to kernel methods Next, we considered the kernel approximation of the Pearson divergence [21]. The problem is defined as follows. Suppose we have the two different data sets x = (x1, . . . , xn) ∈ R n and x′ = (x′1, . . . , x ′ n′) ∈ R n′ where n, n′ ∈ N. Let H ∈ Rn×n be a gram matrix such that Hl,m = αn ∑n i=1 φ(xi, xl)φ(xi, xm) + 1−α n′ ∑n′ j=1 φ(x ′ j , xl)φ(x ′ j , xm), where φ(·, ·) is a kernel function and α ∈ (0, 1) is a parameter. Also, let h ∈ Rn be a vector such that hl = 1n ∑n i=1 φ(xi, xl). Then, an estimator of the α-relative Pearson divergence between the distributions of x and x′ is obtained by − 12 − minv∈Rn 1 2 〈v, Hv〉 − 〈h,v〉 + λ 2 〈v,v〉. Here, λ > 0 is a regularization parameter. In this experiment, we used the Gaussian kernel φ(x, y) = exp((x− y)2/2σ2) and set n′ = 200 and α = 0.5; σ2 and λ were chosen by 5-fold cross-validation as suggested in [21]. We randomly generated the data sets as xi ∼ N(1, 0.5) for i ∈ [n] and x′j ∼ N(1.5, 0.5) for j ∈ [n\n′] where N(µ, σ2) denotes the Gaussian distribution with mean µ and variance σ2.\nWe encoded this problem into (1) by setting A = 12H , b = −h, and d = λ 2n1n, where 1n denotes the n-dimensional vector whose elements are all one. After that, given k, we computed the second step of Algorithm 1 with the pseudoinverse of A|S+kdiag(d|S). Absolute approximation errors and runtimes were compared with Nyström’s method whose approximated rank was set to k. In terms of accuracy, our method clearly outperformed Nyström’s method (Table 2). In addition, the runtimes of our method were nearly constant, whereas the runtimes of Nyström’s method grew linearly in k (Table 1).\n1We used GLPK (https://www.gnu.org/software/glpk/) for the QP solver."
    }, {
      "heading" : "6 Acknowledgments",
      "text" : "We would like to thank Makoto Yamada for suggesting a motivating problem of our method. K. H. is supported by MEXT KAKENHI 15K16055. Y. Y. is supported by MEXT Grant-in-Aid for Scientific Research on Innovative Areas (No. 24106001), JST, CREST, Foundations of Innovative Algorithms for Big Data, and JST, ERATO, Kawarabayashi Large Graph Project."
    } ],
    "references" : [ {
      "title" : "Random sampling and approximation of MAX- CSP problems",
      "author" : [ "N. Alon", "W.F. de la Vega", "R. Kannan", "M. Karpinski" ],
      "venue" : "In STOC,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "A combinatorial characterization of the testable graph properties: It’s all about regularity",
      "author" : [ "N. Alon", "E. Fischer", "I. Newman", "A. Shapira" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Graph limits and parameter testing",
      "author" : [ "C. Borgs", "J. Chayes", "L. Lovász", "V.T. Sós", "B. Szegedy", "K. Vesztergombi" ],
      "venue" : "In STOC,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing",
      "author" : [ "C. Borgs", "J.T. Chayes", "L. Lovász", "V.T. Sós", "K. Vesztergombi" ],
      "venue" : "Advances in Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Stochastic learning",
      "author" : [ "L. Bottou" ],
      "venue" : "In Advanced Lectures on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Feasible real random access machines",
      "author" : [ "V. Brattka", "P. Hertling" ],
      "venue" : "Journal of Complexity,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Sublinear optimization for machine learning",
      "author" : [ "K.L. Clarkson", "E. Hazan", "D.P. Woodruff" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "The regularity lemma and approximation schemes for dense problems",
      "author" : [ "A. Frieze", "R. Kannan" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1996
    }, {
      "title" : "Property testing and its connection to learning and approximation",
      "author" : [ "O. Goldreich", "S. Goldwasser", "D. Ron" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Large Networks and Graph Limits",
      "author" : [ "L. Lovász" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Limits of dense graph sequences",
      "author" : [ "L. Lovász", "B. Szegedy" ],
      "venue" : "Journal of Combinatorial Theory, Series B,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Non-deterministic graph property testing",
      "author" : [ "L. Lovász", "K. Vesztergombi" ],
      "venue" : "Combinatorics, Probability and Computing,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Yet another algorithm for dense max cut: go greedy",
      "author" : [ "C. Mathieu", "W. Schudy" ],
      "venue" : "In SODA,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "Machine learning: a probabilistic perspective",
      "author" : [ "K.P. Murphy" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Constant-time approximation algorithms via local improvements",
      "author" : [ "H.N. Nguyen", "K. Onak" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "A near-optimal sublinear-time algorithm for approximating the minimum vertex cover size",
      "author" : [ "K. Onak", "D. Ron", "M. Rosen", "R. Rubinfeld" ],
      "venue" : "In SODA,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Robust characterizations of polynomials with applications to program testing",
      "author" : [ "R. Rubinfeld", "M. Sudan" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1996
    }, {
      "title" : "Density Ratio Estimation in Machine Learning",
      "author" : [ "M. Sugiyama", "T. Suzuki", "T. Kanamori" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Least-Squares Independent Component Analysis",
      "author" : [ "T. Suzuki", "M. Sugiyama" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Using the nyström method to speed up kernel machines",
      "author" : [ "C.K.I. Williams", "M. Seeger" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "Relative density-ratio estimation for robust distribution comparison",
      "author" : [ "M. Yamada", "T. Suzuki", "T. Kanamori", "H. Hachiya", "M. Sugiyama" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Optimal constant-time approximation algorithms and (unconditional) inapproximability results for every bounded-degree CSP",
      "author" : [ "Y. Yoshida" ],
      "venue" : "In STOC,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "A characterization of locally testable affine-invariant properties via decomposition theorems",
      "author" : [ "Y. Yoshida" ],
      "venue" : "In STOC,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Gowers norm, function limits, and parameter estimation",
      "author" : [ "Y. Yoshida" ],
      "venue" : "In SODA,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Improved constant-time approximation algorithms for maximum matchings and other optimization problems",
      "author" : [ "Y. Yoshida", "M. Yamamoto", "H. Ito" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Many fundamental problems such as linear regression, k-means clustering, principal component analysis, support vector machines, and kernel methods [14] can be formulated as a minimization problem of a quadratic function.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "[21] proposed an efficient method for estimating the Pearson divergence, which provides useful information about data, such as the density ratio [18].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[21] proposed an efficient method for estimating the Pearson divergence, which provides useful information about data, such as the density ratio [18].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "The least-squares mutual information [19] is another example that can be computed in a similar manner.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "A nice property of this method is that, if the objective function is strongly convex, it outputs a point that is sufficiently close to an optimal solution after a constant number of iterations [5].",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 19,
      "context" : "Another technique is lowrank approximation such as Nyström’s method [20].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "[7] proposed sublinear-time algorithms for special cases of quadratic function minimization.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "Here, we assume the real RAM model [6], in which we can perform basic algebraic operations on real numbers in one step.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "To this end, we exploit graph limit theory, initiated by Lovász and Szegedy [11] (refer to [10] for a book), in which we measure the distance between two graphs on different number of vertices by considering continuous versions.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "To this end, we exploit graph limit theory, initiated by Lovász and Szegedy [11] (refer to [10] for a book), in which we measure the distance between two graphs on different number of vertices by considering continuous versions.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].",
      "startOffset" : 155,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].",
      "startOffset" : 155,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].",
      "startOffset" : 197,
      "endOffset" : 204
    }, {
      "referenceID" : 21,
      "context" : "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].",
      "startOffset" : 197,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].",
      "startOffset" : 235,
      "endOffset" : 247
    }, {
      "referenceID" : 15,
      "context" : "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].",
      "startOffset" : 235,
      "endOffset" : 247
    }, {
      "referenceID" : 24,
      "context" : "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].",
      "startOffset" : 235,
      "endOffset" : 247
    }, {
      "referenceID" : 8,
      "context" : "A related notion is property testing [9, 17], which aims to design constant-time algorithms that distinguish inputs satisfying some predetermined property from inputs that are “far” from satisfying it.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "A related notion is property testing [9, 17], which aims to design constant-time algorithms that distinguish inputs satisfying some predetermined property from inputs that are “far” from satisfying it.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 22,
      "context" : "Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 23,
      "context" : "Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 11,
      "context" : "1 Dikernels Following [12], we call a (measurable) function f : [0, 1] → R a dikernel.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "1 Dikernels Following [12], we call a (measurable) function f : [0, 1] → R a dikernel.",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "A dikernel is a generalization of a graphon [11], which is symmetric and whose range is bounded in [0, 1].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "A dikernel is a generalization of a graphon [11], which is symmetric and whose range is bounded in [0, 1].",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "We can regard a dikernel as a matrix whose index is specified by a real value in [0, 1].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "For two functions f, g : [0, 1] → R, we define their inner product as 〈f, g〉 = ∫ 1 0 f(x)g(x)dx.",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "For a dikernel W : [0, 1] → R and a function f : [0, 1] → R, we define a function Wf : [0, 1] → R as (Wf)(x) = 〈W (x, ·), f〉.",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "For a dikernel W : [0, 1] → R and a function f : [0, 1] → R, we define a function Wf : [0, 1] → R as (Wf)(x) = 〈W (x, ·), f〉.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "For a dikernel W : [0, 1] → R and a function f : [0, 1] → R, we define a function Wf : [0, 1] → R as (Wf)(x) = 〈W (x, ·), f〉.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "Let W : [0, 1] → R be a dikernel.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "The Lp norm ‖W‖p for p ≥ 1 and the cut norm ‖W‖ of W are defined as ‖W‖p = (∫ 1 0 ∫ 1 0 |W (x, y)|dxdy )1/p and ‖W‖ = supS,T⊆[0,1] ∣∣ ∫ S ∫ T W (x, y)dxdy ∣∣, respectively, where the supremum is over all pairs of subsets.",
      "startOffset" : 125,
      "endOffset" : 130
    }, {
      "referenceID" : 0,
      "context" : "A map π : [0, 1] → [0, 1] is said to be measure-preserving, if the pre-image π(X) is measurable for every measurable set X , and λ(π(X)) = λ(X).",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "A map π : [0, 1] → [0, 1] is said to be measure-preserving, if the pre-image π(X) is measurable for every measurable set X , and λ(π(X)) = λ(X).",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "For a measure preserving bijection π : [0, 1] → [0, 1] and a dikernel W : [0, 1] → R, we define the dikernel π(W ) : [0, 1] → R as π(W )(x, y) = W (π(x), π(y)).",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "For a measure preserving bijection π : [0, 1] → [0, 1] and a dikernel W : [0, 1] → R, we define the dikernel π(W ) : [0, 1] → R as π(W )(x, y) = W (π(x), π(y)).",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "For a measure preserving bijection π : [0, 1] → [0, 1] and a dikernel W : [0, 1] → R, we define the dikernel π(W ) : [0, 1] → R as π(W )(x, y) = W (π(x), π(y)).",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "For a measure preserving bijection π : [0, 1] → [0, 1] and a dikernel W : [0, 1] → R, we define the dikernel π(W ) : [0, 1] → R as π(W )(x, y) = W (π(x), π(y)).",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "2 Matrices and Dikernels Let W : [0, 1] → R be a dikernel and S = (x1, .",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : ", xk) be a sequence of elements in [0, 1].",
      "startOffset" : 35,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "We can construct the dikernel Â : [0, 1] → R from the matrix A ∈ R as follows.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "For x ∈ [0, 1], we define in(x) ∈ [n] as a unique integer such that x ∈ Ii.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "We note that the distribution of A|S , where S is a sequence of k indices that are uniformly and independently sampled from [n] exactly matches the distribution of Â|S , where S is a sequence of k elements that are uniformly and independently sampled from [0, 1].",
      "startOffset" : 256,
      "endOffset" : 262
    }, {
      "referenceID" : 0,
      "context" : ",W : [0, 1] → [−L,L], we can obtain a good approximation to them by sampling a sequence of a small number of elements in [0, 1].",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : ",W : [0, 1] → [−L,L], we can obtain a good approximation to them by sampling a sequence of a small number of elements in [0, 1].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : ",W : [0, 1] → [−L,L] be dikernels.",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "Let S be a sequence of k elements uniformly and independently sampled from [0, 1].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "Then, with a probability of at least 1− exp(−Ω(kT/ log2 k)), there exists a measure-preserving bijection π : [0, 1] → [0, 1] such that, for any functions f, g : [0, 1] → [−K,K] and t ∈ [T ], we have |〈f,W g〉 − 〈f, π(Ŵ |S)g〉| = O ( LK √ T/ log2 k ) .",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "Then, with a probability of at least 1− exp(−Ω(kT/ log2 k)), there exists a measure-preserving bijection π : [0, 1] → [0, 1] such that, for any functions f, g : [0, 1] → [−K,K] and t ∈ [T ], we have |〈f,W g〉 − 〈f, π(Ŵ |S)g〉| = O ( LK √ T/ log2 k ) .",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "Then, with a probability of at least 1− exp(−Ω(kT/ log2 k)), there exists a measure-preserving bijection π : [0, 1] → [0, 1] such that, for any functions f, g : [0, 1] → [−K,K] and t ∈ [T ], we have |〈f,W g〉 − 〈f, π(Ŵ |S)g〉| = O ( LK √ T/ log2 k ) .",
      "startOffset" : 161,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : "We start with the following lemma, which states that, if a dikernel W : [0, 1] → R has a small cut norm, then 〈f,Wf〉 is negligible no matter what f is.",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Let ǫ ≥ 0 and W : [0, 1] → R be a dikernel with ‖W‖ ≤ ǫ.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Then, for any functions f, g : [0, 1] → [−K,K], we have |〈f,Wg〉| ≤ ǫK.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "For τ ∈ R and the function h : [0, 1] → R, let Lτ (h) := {x ∈ [0, 1] | h(x) = τ} be the level set of h at τ .",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "For τ ∈ R and the function h : [0, 1] → R, let Lτ (h) := {x ∈ [0, 1] | h(x) = τ} be the level set of h at τ .",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : ", Vp) of the interval [0, 1] is called an equipartition if λ(Vi) = 1/p for every i ∈ [p].",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "For the dikernel W : [0, 1] → R and the equipartition P = (V1, .",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : ", Vp) of [0, 1], we define WP : [0, 1] → R as the function obtained by averaging each Vi × Vj for i, j ∈ [p].",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : ", Vp) of [0, 1], we define WP : [0, 1] → R as the function obtained by averaging each Vi × Vj for i, j ∈ [p].",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "The following lemma states that any function W : [0, 1] → R can be well approximated by WP for the equipartition P into a small number of parts.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "3 (Weak regularity lemma for functions on [0, 1] [8]).",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "3 (Weak regularity lemma for functions on [0, 1] [8]).",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "Let P be an equipartition of [0, 1] into k sets.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Then, for any dikernel W : [0, 1] → R and ǫ > 0, there exists a refinement Q of P with |Q| ≤ k2 2 for some constant C > 0 such that ‖W −WQ‖ ≤ ǫ‖W‖2.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : ",W : [0, 1] → R be dikernels.",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "15) of [4]).",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "Let W : [0, 1] → [−L,L] be a dikernel and S be a sequence of k elements uniformly and independently sampled from [0, 1].",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "Let W : [0, 1] → [−L,L] be a dikernel and S be a sequence of k elements uniformly and independently sampled from [0, 1].",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : ",W : [0, 1] → [−L,L] be dikernels.",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "Let S be a sequence of k elements uniformly and independently sampled from [0, 1].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "Then, with a probability of at least 1− exp(−Ω(kT/ log2 k)), there exists a measure-preserving bijection π : [0, 1] → [0, 1] such that, for every t ∈ [T ], we have ‖W t − π(Ŵ |S)‖ = O ( L √ T/ log2 k ) .",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "Then, with a probability of at least 1− exp(−Ω(kT/ log2 k)), there exists a measure-preserving bijection π : [0, 1] → [0, 1] such that, for every t ∈ [T ], we have ‖W t − π(Ŵ |S)‖ = O ( L √ T/ log2 k ) .",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "Then, for any measure-preserving bijection π : [0, 1] → [0, 1] and t ∈ [T ], we have ES‖W t − π(Ŵ |S)‖ ≤ ‖W t −W t P‖ +ES‖W t P − π(Ŵ t P |S)‖ +ES‖π(Ŵ t P |S)− π(Ŵ |S)‖ ≤ 2ǫL+ 8L k1/4 +ES‖W t P − π(Ŵ t P |S)‖ .",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "Then, for any measure-preserving bijection π : [0, 1] → [0, 1] and t ∈ [T ], we have ES‖W t − π(Ŵ |S)‖ ≤ ‖W t −W t P‖ +ES‖W t P − π(Ŵ t P |S)‖ +ES‖π(Ŵ t P |S)− π(Ŵ |S)‖ ≤ 2ǫL+ 8L k1/4 +ES‖W t P − π(Ŵ t P |S)‖ .",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : ", xk} be a sequence of independent random variables that are uniformly distributed in [0, 1], and let Zi be the number of points xj that fall into the set Vi.",
      "startOffset" : 86,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "The partition P ′ of [0, 1] is constructed into the sets V ′ 1 , .",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "For each t ∈ [T ], we construct the dikernel W t : [0, 1] → R such that the value of W t on V ′ i × V ′ j is the same as the value of W t P on Vi × Vj .",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "The real-valued function Pn,A,d,b on the functions f : [0, 1] → R is defined as Pn,A,d,b(f) = 〈f, Âf〉+ 〈f , d̂1⊤1〉+ 〈f, b̂1⊤1〉, where f : [0, 1] → R is a function such that f(x) = f(x) for every x ∈ [0, 1] and 1 : [0, 1] → R is the constant function that has a value of 1 everywhere.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "The real-valued function Pn,A,d,b on the functions f : [0, 1] → R is defined as Pn,A,d,b(f) = 〈f, Âf〉+ 〈f , d̂1⊤1〉+ 〈f, b̂1⊤1〉, where f : [0, 1] → R is a function such that f(x) = f(x) for every x ∈ [0, 1] and 1 : [0, 1] → R is the constant function that has a value of 1 everywhere.",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "The real-valued function Pn,A,d,b on the functions f : [0, 1] → R is defined as Pn,A,d,b(f) = 〈f, Âf〉+ 〈f , d̂1⊤1〉+ 〈f, b̂1⊤1〉, where f : [0, 1] → R is a function such that f(x) = f(x) for every x ∈ [0, 1] and 1 : [0, 1] → R is the constant function that has a value of 1 everywhere.",
      "startOffset" : 199,
      "endOffset" : 205
    }, {
      "referenceID" : 0,
      "context" : "The real-valued function Pn,A,d,b on the functions f : [0, 1] → R is defined as Pn,A,d,b(f) = 〈f, Âf〉+ 〈f , d̂1⊤1〉+ 〈f, b̂1⊤1〉, where f : [0, 1] → R is a function such that f(x) = f(x) for every x ∈ [0, 1] and 1 : [0, 1] → R is the constant function that has a value of 1 everywhere.",
      "startOffset" : 214,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : "Then, we have min v∈[−K,K]n pn,A,d,b(v) = n 2 · inf f :[0,1]→[−K,K] Pn,A,d,b(f).",
      "startOffset" : 55,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "First, we show that n · inff :[0,1]→[−K,K] Pn,A,d,b(f) ≤ minv∈[−K,K]n pn,A,d,b(v).",
      "startOffset" : 30,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Given a vector v ∈ [−K,K], we define f : [0, 1] → [−K,K] as f(x) = vin(x).",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Next, we show that minv∈[−K,K]n pn,A,d,b(v) ≤ n · inff :[0,1]→[−K,K] Pn,A,d,b(f).",
      "startOffset" : 56,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "Let f : [0, 1] → [−K,K] be a measurable function.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "Then, for x ∈ [0, 1], we have ∂Pn,A,d,b(f(x)) ∂f(x) = ∑",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "Note that the form of this partial derivative only depends on in(x); hence, in the optimal solution f : [0, 1] → [−K,K], we can assume f(x) = f(y) if in(x) = in(y).",
      "startOffset" : 104,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "For such f, we define the vector v ∈ R as vi = f (x), where x ∈ [0, 1] is any element in Ii.",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "Then, with a probability of at least 1− δ, there exists a measure preserving bijection π : [0, 1] → [0, 1] such that",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "Then, with a probability of at least 1− δ, there exists a measure preserving bijection π : [0, 1] → [0, 1] such that",
      "startOffset" : 100,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "max { |〈f, (Â− π(Â|S))f〉|, |〈f , (d̂1⊤ − π(d̂1|S))1〉|, |〈f, (b̂1⊤ − π(b̂1|S))1〉| } ≤ ǫLK 3 for any function f : [0, 1] → [−K,K].",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "Then, we have z̃ = min v∈Rk pk,A|S ,d|S,b|S(v) = min v∈[−K,K]k pk,A|S,d|S ,b|S(v) = k · inf f :[0,1]→[−K,K] Pk,A|S ,d|S,b|S(f) (By Lemma 4.",
      "startOffset" : 95,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "1) = k · inf f :[0,1]→[−K,K] ( 〈f, (π(Â|S)− Â)f〉+ 〈f, Âf〉+ 〈f , (π(d̂1|S)− d̂1⊤)1〉+ 〈f , d̂1⊤1〉+",
      "startOffset" : 16,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "≤ k · inf f :[0,1]→[−K,K] ( 〈f, Âf〉+ 〈f, d̂1⊤1〉+ 〈f, b̂1⊤1〉 ± ǫLK )",
      "startOffset" : 13,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "We randomly generated inputs as Aij ∼ U[−1,1], di ∼ U[0,1], and bi ∼ U[−1,1] for i, j ∈ [n], where U[a,b] denotes the uniform distribution with the support [a, b].",
      "startOffset" : 53,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "Application to kernel methods Next, we considered the kernel approximation of the Pearson divergence [21].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "5; σ and λ were chosen by 5-fold cross-validation as suggested in [21].",
      "startOffset" : 66,
      "endOffset" : 70
    } ],
    "year" : 2016,
    "abstractText" : "A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following n-dimensional quadratic minimization problem in constant time, which is independent of n: z = minv∈Rn〈v, Av〉 + n〈v, diag(d)v〉 + n〈b,v〉, where A ∈ R is a matrix and d, b ∈ R are vectors. Our theoretical analysis specifies the number of samples k(δ, ǫ) such that the approximated solution z satisfies |z − z| = O(ǫn) with probability 1− δ. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments.",
    "creator" : "LaTeX with hyperref package"
  }
}