{
  "name" : "1506.02216.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Recurrent Latent Variable Model for Sequential Data",
    "authors" : [ "Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron Courville", "Yoshua Bengio" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Learning generative models of sequences is a long-standing machine learning challenge and historically the domain of dynamic Bayes networks (DBNs) such as hidden Markov models (HMMs) and Kalman filters. The dominance of DBN-based approaches has been recently overturned by a resurgence of interest in recurrent neural network (RNN) based approaches. An RNN is a special type of neural network that is able to handle both variable-length input and output. By training an RNN to predict the next output in a sequence, given all previous outputs, it can be used to model the joint probability distribution over sequences.\nBoth RNNs and DBNs consist of two parts: (1) a transition function that determines the evolution of the internal latent state, and (2) a mapping from the state to the output. There are, however also a few important differences between RNNs and DBNs.\nDBNs have typically been limited to either relatively simple state transition structures (e.g., linear models in the case of the Kalman filter) or to relatively simple internal state structure (e.g., the HMM state space consists of a single set of mutually exclusive states). RNNs, on the other hand, typically possess both a rich distributed internal state representation as well as flexible non-linear transition functions. These differences give RNNs extra expressive power in comparison to DBNs. This expressive power and the ability to train via error backpropagation are key reasons why RNNs have gained popularity as generative models for richly-structured sequence data.\nIn this paper we focus on another important difference between DBNs and RNN. While latent state in DBNs is expressed in terms of random variables, the standard RNN state transition structure is entirely deterministic. The only source of randomness or variability in the RNN is found in the conditional output probability model. We suggest that this can be an inappropriate way to model of the kind of variability observed in highly-structured data, such as natural speech, which is characterized by strong and complex dependencies among the output variables at each timestep of the sequence. We argue, as have others [4, 2], that these complex dependencies cannot be modeled efficiently by ∗Kratarth Goel is visiting Université de Montréal †CIFAR Senior Fellow\nar X\niv :1\n50 6.\n02 21\n6v 1\n[ cs\n.L G\n] 7\nJ un\nstandard RNN output models, which include either a simple unimodal distribution or a mixture of simple unimodal distributions.\nWe propose the use of high-level random latent variables to model the variability observed in the data. In the context of standard neural network models for non-sequential data, the recently introduced variational autoencoder [11] offers an interesting combination highly flexible non-linear mapping between the latent random state and the observed output and effective approximate inference. In this paper, we propose to extend the variational autoencoder (VAE) into an recurrent framework for modeling high-dimensional sequences. Our justification for the choice of VAE is two-fold. First, the VAE can model complex multimodal distributions, which will help when the underlying true data distribution consists of multimodal conditional distributions. We call this joint model a variational RNN, or VRNN.\nA natural question to ask is: how do we encode observed variability via latent variables? The answer to this question depends on the nature of the data itself. In this work, we are mainly interested in highly-structured data that often arises in AI applications. By highly-structured, we mean that the data is characterized by two properties. First, there is a relatively high signal to noise ratio, meaning that the vast majority of the variability observed in the data is due to the signal itself and cannot reasonably be considered noise. Second, there exists a complex relationship between the underlying factors of variation and the observed data. For example, in speech, the vocal qualities of the speaker have a strong but complicated influence on the audio waveform, affecting the waveform in a consistent manner across frames.\nWith these considerations in mind, we suggest that our model variability should induce dependencies across timesteps. Thus, like DBN models such as HMMs and Kalman filters, we model dependencies between the latent variables across timesteps. While we are not the first to propose to integrate random variables into the RNN latent state, Boulanger-Lewandowski et al. [4], Bayer and Osendorfer [2], Fabius and van Amersfoort [6], we believe we are the first to integrate these dependencies between the random variables at neighboring timesteps.\nWe evaluate the proposed VRNN against other RNN-based models – including RNNs with Gaussian mixtures as output models – on two challenging sequential data types: natural speech and online handwriting. We also compare to an alternative method of incorporating latent random variables into a recurrent structure [2]. We demonstrate that for speech modeling the proposed VRNN significantly outperforms both RNNs and a similar model that does not integrate temporal dependencies between random variables."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Sequence Modeling with Recurrent Neural Networks",
      "text" : "An RNN is a type of neural network that can take as input a variable-length sequence x = (x1,x2, . . . ,xT ) by recursively processing each symbol while maintaining its internal hidden state h. At each timestep t, an RNN reads the symbol xt ∈ Rd and updates its hidden state ht ∈ Rp by\nht =fθ (xt,ht−1) , (1)\nwhere f is a non-linear transition function, usually implemented as an affine transformation followed by tanh or a gated activation function such as a long short-term memory unit [LSTM, 8] or gated recurrent unit [GRU, 5]. RNNs model sequences by parameterizing a factorization of the joint sequence probability distribution as a product of conditional probabilities such that\np(x1,x2, . . . ,xT ) = T∏ t=1 p(xt | x<t),\np(xt | x<t) = gτ (ht), (2) where g is a function that maps the RNN state ht to a probability distribution over possible outputs.\nOne of the main factors which determines the representation power of a recurrent neural network (RNN) is the output function g in Eq. (2). With a deterministic transition function, the choice of g effectively defines the family of joint probability distributions p(x1, . . . ,xt) that can be expressed by the RNN.\nWe can express the output function g in Eq. (2) as being composed of two parts. The first part ϕτ returns the parameters φt given the internal state ht, φt = ϕτ (ht), while the second part g returns the density of xt, i.e. pφt(xt | x<t). For the kind of real-valued, high-dimensional modeling tasks in which we are interested (i.e. speech and handwriting), one reasonable choice of observation model is the unimodal Gaussian distribution. In this case, ϕτ represents the mean vector µt and covariance matrix Σt. The probability of xt given these parameters is\npµt,Σt(xt|x<t) = N (xt;µt,Σt) , where N specifies a multivariate Gaussian distribution. Graves [7] recently used a Gaussian mixture model (GMM) to model real-valued sequences in the context of, e.g., handwriting generation. In the case of a Gaussian mixture model (GMM), ϕτ returns a set of mixture coefficients αt, means µ·,t and covariances Σ·,t of the corresponding mixture components. The probability of xt under the mixture distribution is\npαt,µ·,t,Σ·,t(xt|x<t) = ∑ j αj,tN ( xt;µj,t,Σj,t ) .\nWith the notable exception of [7], there have been few works investigating the structured output density model for RNNs with real-valued sequences.\nThere is potentially a significant issue in the way the RNN models output variability. Given a deterministic transition function, the only source of variability is in this output probability density. This can present problems when modeling sequences that are at once highly variable and highly structured. To effectively capture this kind of data, the RNN must be capable of mapping very small variations in x (the only source of randomness) to potentially very large variations in the internal state ht. Limiting the capacity of the network (as must be done to guard against overfitting) will force a compromise between the generation of a clean signal (i.e. with high signal to noise ratio) and encoding sufficient input variability to capture the high-level variability both within a single observed sequence and across data examples.\nThe need for highly-structured output functions in an RNN has been previously noted. BoulangerLewandowski et al. [4] extensively tested NADE and RBM-based output densities for modeling sequences of binary vector representations of music. Bayer and Osendorfer [2] introduced a sequence of independent latent variables corresponding to the states of the RNN. Their model, called STORN, first generates a sample (z1, . . . , zT ) from the sequence of independent latent variables. At each timestep, the transition function f from Eq. (1) computes the next hidden state ht based on both the previous state ht−1, the previous output xt−1 as well as the sampled latent variable zt. They proposed to train this model based on the variational autoencoder principle (see Sec. 2.2.) Similarly, Pachitariu and Sahani [14] earlier proposed both a sequence of independent latent variables and a stochastic internal state for the RNN.\nThese approaches are closely related to the approach proposed in this paper. However, there is a major difference in how the prior distribution over the latent variable z is modeled. Unlike them, our approach makes the prior distribution of the latent variable at time t dependent on all the preceding inputs via the RNN state ht (see Eq. (5)). The introduction of temporal structure into the prior distribution is expected to improve the representational power of the model, which we empirically observe in the experiments (See Table 1). However, it is important to note that any approach based on having stochastic hidden states is orthogonal to having a structured output function, and these two can be used together to form a single model."
    }, {
      "heading" : "2.2 Variational Autoencoder",
      "text" : "For non-sequential data, variational autoencoders [11, 15] have recently been shown to be an effective modeling paradigm to recover complex multimodal distributions over the data space.\nA variational autoencoder introduces a set of latent variables z, designed to capture the variations in the observed variables x. As an example of a directed graphical model, the joint distribution is defined as\np(x, z) = p(x | z)p(z). (3)\nwhere the prior over the latent variables, p(z), is generally chosen to be a simple Gaussian distribution and the conditional p(x | z) is an arbitrary observation model whose parameters are computed by a parametric function of z. Importantly, the variational autoencoder typically parameterizes p(x | z) with a highly flexible function approximator such as a neural network. While latent variable models of the form given in Eq. 3 are not uncommon, endowing the conditional p(x | z) as a potentially highly non-linear mapping from z to x is a rather unique feature of the VAE.\nHowever, introducing a highly non-linear mapping form z to x results in intractable inference of the posterior p(z | x). Instead VAE uses a variational approximation q(z | x) of the posterior that enables the use of the lower bound\nlog p(x) ≥ −KL(q(z | x)‖p(z)) + Eq(z|x) [log(p(x | z))] , (4) where KL(Q‖P ) is Kullback-Leibler divergence between two distributions Q and P . In Kingma and Welling [11], the variational posterior q(z | x) is a GaussianN (µ, diag(σ2)) whose mean µ and log-variance log(σ2) are the output of a highly non-linear function of x, once again typically a neural network.\nThe generative model p(x | z) and inference model q(z | x) are then trained jointly by maximizing the variational lower bound with respect to their respective parameters, where the integral with respect to q(z | x) is approximated stochastically. The gradient of this estimate can have a low variance estimate, by reparametrizing z = µ + σ where is a standard Gaussian variable and rewriting\nEq(z|x) [log p(x | z)] = Ep( ) [log p(x | z = µ + σ )] . the inference model can then be trained through standard backpropagation technique for stochastic gradient descent."
    }, {
      "heading" : "3 Variational Recurrent Neural Network",
      "text" : "In this section, we introduce a recurrent version of the variational autoencoder for the purpose of modeling sequences. Drawing inspiration from simpler dynamic Bayes networks such as HMMs and Kalman filters, the proposed variational recurrent neural network (VRNN) explicitly models the dependency between latent random variables across subsequent timesteps. However, unlike these simpler DBN models, the VRNN retains the flexibility to model highly non-linear dynamics.\nGeneration The VRNN contains a variational autoencoder at every timestep. However, these variational autoencoders are conditioned on the state variable ht−1 of a recurrent network. This addition will help us to take into account the temporal structure of the sequential data. Unlike a typical variational autoencoder, the prior on the latent variable is no longer a standard normal distribution, but follow the distribution\nzt ∼ N (µ0,t, diag(σ20,t)) , where [µ0,t,σ0,t] = ϕpriorτ (ht−1). (5)\nMoreover, the generating distribution will not only be conditioned on zt but also on ht−1 to be\nxt | zt ∼ N (µx,t, diag(σ2x,t)) , where [µx,t,σx,t] = ϕdecτ (ϕzτ (zt),ht−1). (6)\nAgain ϕpriorτ and ϕdecτ can be any parametric function and will typically be neural networks. ϕ x τ and ϕzτ are also non-linear functions, e.g., neural networks, that will extract complex feature from xt and zt, respectively. We found that these feature extractors are crucial for enabling the learning of complex sequences for the model.\nThe recurrent network, which we will choose to be an RNN, will follow the recurrence equation\nht =fθ (ϕ x τ (xt), ϕ z τ (zt),ht−1) , (7)\nwhere fθ is a transition function from Eq. (1).\nThis parametrization of the generative model results in and – was motivated by – the factorization\np(z≤T ,x≤T ) = T∏ t=1 p(zt | x<t, z<t)p(xt | z≤t,x<t). (8)\nInference In a similar fashion, the approximate posterior will not only be a function of xt but also of ht−1 to follow the equation\nzt | xt ∼ N (µz,t, diag(σ2z,t)) , where [µz,t,σz,t] = ϕencτ (ϕxτ (xt),ht−1). (9)\nWe can notice that the encoding of the approximate posterior and the decoding for generation are also tied through the hidden state ht−1.\nWe can also observe that this results in the factorization\nq(z≤T | x≤T ) = T∏ t=1 q(zt | z<t,x≤t) (10)\nLearning This factorization is crucial in breaking the variational lower bound timestep-wise∫ log ( p(x≤T , z≤T )\nq(z≤T | x≤T )\n) dq(z≤T | x≤T ) = T∑ t=1 −KL(q(zt | x≤t, z<t)‖p(zt | z<t,x<t))\n+Eq(zt|x≤t,z<t) [log(p(xt | z≤t,x<t))]\nAs in the regular variational autoencoder, we learn the generating and inference model jointly by maximizing the variational lower bound with respect to their parameters. The schematic view of VRNN is shown in Fig. 1, where the network in (a) assumes independent priors across time (VRNNI) and the network in (b) has temporal conditioning in its prior (VRNN, see Eq. 5). STORN [2] model can be considered an instance of the VRNN-I model family. In fact, STORN makes further restrictions on the dependency structure of the approximate inference model. We include this version of the model in our experimental evaluation in order to directly study the impact of including the temporal dependency structure in the prior over the latent random variables."
    }, {
      "heading" : "4 Experiment Settings",
      "text" : "We evaluate the proposed VRNN model on two tasks: (1) modeling natural speech directly from the raw audio waveform; (2) modeling the dynamic handwriting process.\nSpeech Modeling We train the models to directly model raw audio, represented as a sequence of 200-dimensional frames. Each frame corresponds to the real-valued amplitudes of 200 consecutive raw acoustic samples. Note that this is unlike the conventional approach to modeling speech, often used in speech synthesis where models are expressed over representations such as spectral features [see, e.g., 16, 3, 12].\nWe evaluate the models on the following four speech datasets:\nFor the Blizzard and Accent datasets, we process the data so that each sample duration is 0.5s (the sampling frequency used is 16kHz). We use truncated backpropagation through time and initialize the hidden state with the final hidden state of previous minibatch, resetting to a zero-vector every four updates. Excluding TIMIT, these datasets do not have predefined train/test splits. We shuffle and divide the data into train/validation/test splits using a fraction of 0.9/0.05/0.05. See supplementary material for more details on datasets and experimental settings.\nHandwriting Generation We let each model to learn a sequence of (x, y)-coordinates together with binary indicators of pen up / pen down, using the IAM-OnDB dataset which consists of 13, 040 handwritten lines written by 500 writers [13]. We preprocess and split the dataset as done in [7].\nPreprocessing and Training The only preprocessing used in the experiments is normalizing each vector of a sequence by using the global mean and standard deviation computed from the training set. We trained each model by the stochastic gradient descent on the log-likelihood using the recently proposed Adam optimizer [10], with learning rate of 0.001 for TIMIT and Accent, 0.0003 for the rest. We used minibatch size of 128 for Blizzard and Accent, and 64 for the rest. The final model was chosen with early-stopping the training based on the validation performance.\nModels We compare the proposed VRNN with a standard RNN. For each architecture, we evaluate two different output function: unimodal Gaussian distribution (Gauss) and the Gaussian mixture model (GMM). For each task, we conduct additional set of experiments of VRNN without temporal conditioning.\nWe fix the size of the RNN of each model to have one recurrent layer with 2000 LSTM units (in the case of Blizzard, 4000 and for IAM-OnDB, 1200). All the ϕτ in Eqs. (5)–(9) have four hidden layers using rectified linear units (for IAM-OnDB, one hidden).\nFor the RNN-GMM and VRNN, we match the number of parameters of each function as closely as possible to an RNN-Gauss model having 600 rectified linear units for all feedforward layers of ϕτ in Eqs. (7)–(9) (800 for Blizzard). All the models using GMM as output function has 20 mixture components. The standard RNN model only has ϕxτ and ϕ dec τ , while the proposed VRNN has ϕ z τ , ϕencτ and ϕ prior τ as well.\n1 This dataset has been provided by Ubisoft."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "We evaluate the average log-probability of test examples assigned by each model and report in Table 1. With RNN-Gauss and RNN-GMM, we report exact log-probabilities, while in the case of VRNNs we report the variational lower bound (see Eq. (4)) and approximated marginal loglikelihood based on importance sampling using 40 samples as in Rezende et al. [15]. In all cases, higher scores are better. Our results show the proposed VRNNs with latent random variables have better log-probability performance. VRNN-Gauss performs well (compared to VRNN-GMM) using only a unimodal output function, which does not happen in the standard RNN case.\nLatent Space Analysis After observing the improvements achieved by the proposed VRNN, we were curious on what kind of dynamics of the latent variables have learned by the model. In Fig. 2, we show some analysis of the latent variables. We compute ∑ j(µ j z,t − µ j z,t−1)\n2 at every timestep t and plot in the top row of Fig. 2. Here we see the difference between µz of consecutive timesteps, which seems to reflect changes of modality. The middle row shows the KL divergence computed between the approximate posterior and the prior. When there is a transition, the KL tends to grow.\nFor qualitative analysis for speech, we train with larger models again controlling the number of parameters to generate sequences. For all models we use RNNs with three hidden layers, each contains 3000 LSTM units, matching the number of parameters to an RNN-Gauss model having 3200 rectified linear units for all feedforward layers of ϕτ .\nSpeech Generation We generate waveforms with 2.0s duration from the models that were trained on Blizzard. From Fig. 3 we can clearly see that the waveforms from the VRNN are much less noisy and have less spurious peaks than those from the RNN-Gauss (not shown) and RNN-GMM. We suggest that the small about of noise apparent in the RNN-GMM model is a consequence of the compromise these models must make between representing a clean signal (i.e. with a high signal to noise ratio) consistent with the training data, and encoding sufficient input variability to capture the variations across data examples. The latent random variable models (both VRNN-I and VRNN) can avoid this compromise by adding variability in the latent space which can always be mapped to a point close to a relatively clean sample.\nHandwriting Generation Visual inspection of generated handwriting from the trained models reveals that the proposed VRNN tends to generate with more diverse and consistent writing styles, when compared to the RNN-GMM. Fig 4 depicts handwriting from the training examples, RNNGauss, RNN-GMM and VRNN. The VRNN-Gauss model was not able to generate well-formed handwriting, and was excluded from this plot."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose a novel model of complex sequential data that incorporates latent variables into a recurrent neural network (RNN) architecture. We show that by modeling the dependencies between these latent variables, we are able to provide a model that more naturally reflects the kinds of variability seen in many sequential processes.\nOur experiments focus on unconditional speech generation involving various real-valued datasets as well as unconstrained handwriting generation. We find the introduction of latent variables provides a significant performance increase for unconditional speech modeling and further show the importance of temporal conditioning of these latent variables. Samples from VRNN models are qualitatively competitive with existing methods, and appear to show stylistic consistency over the course of generation. This is especially apparent in handwriting samples."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank the developers of Theano [1]. Also, the authors thank Kyunghyun Cho for insightful comments and discussion. We acknowledge the support of the following agencies for research funding and computing support: Ubisoft, NSERC, Calcul Québec, Compute Canada, the Canada Research Chairs and CIFAR."
    } ],
    "references" : [ {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Learning stochastic recurrent networks",
      "author" : [ "J. Bayer", "C. Osendorfer" ],
      "venue" : "arXiv preprint arXiv:1411.7610,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Unsupervised learning of auditory filter banks using nonnegative matrix factorisation",
      "author" : [ "A. Bertrand", "K. Demuynck", "V. Stouten" ],
      "venue" : "In Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription",
      "author" : [ "N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent" ],
      "venue" : "In ICML’2012,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Variational Recurrent Auto-Encoders",
      "author" : [ "O. Fabius", "J.R. van Amersfoort" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "Technical report,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "The blizzard challenge",
      "author" : [ "S. King", "V. Karaiskos" ],
      "venue" : "In The Ninth annual Blizzard Challenge,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Unsupervised feature learning for audio classification using convolutional deep belief networks",
      "author" : [ "H. Lee", "P. Pham", "Y. Largman", "A. Ng" ],
      "venue" : "In NIPS’09,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Iam-ondb-an on-line english sentence database acquired from handwritten text on a whiteboard",
      "author" : [ "M. Liwicki", "H. Bunke" ],
      "venue" : "In Document Analysis and Recognition,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Learning visual motion in recurrent neural networks",
      "author" : [ "M. Pachitariu", "M. Sahani" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D.J. Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : "In ICML’2014,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Speech synthesis based on hidden markov models",
      "author" : [ "K. Tokuda", "Y. Nankaku", "T. Toda", "H. Zen", "J. Yamagishi", "K. Oura" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "The speech accent archieve",
      "author" : [ "S. Weinberger" ],
      "venue" : "http://accent.gmu.edu/,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "We argue, as have others [4, 2], that these complex dependencies cannot be modeled efficiently by",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "We argue, as have others [4, 2], that these complex dependencies cannot be modeled efficiently by",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "In the context of standard neural network models for non-sequential data, the recently introduced variational autoencoder [11] offers an interesting combination highly flexible non-linear mapping between the latent random state and the observed output and effective approximate inference.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "[4], Bayer and Osendorfer [2], Fabius and van Amersfoort [6], we believe we are the first to integrate these dependencies between the random variables at neighboring timesteps.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[4], Bayer and Osendorfer [2], Fabius and van Amersfoort [6], we believe we are the first to integrate these dependencies between the random variables at neighboring timesteps.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "[4], Bayer and Osendorfer [2], Fabius and van Amersfoort [6], we believe we are the first to integrate these dependencies between the random variables at neighboring timesteps.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "We also compare to an alternative method of incorporating latent random variables into a recurrent structure [2].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "Graves [7] recently used a Gaussian mixture model (GMM) to model real-valued sequences in the context of, e.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 6,
      "context" : "With the notable exception of [7], there have been few works investigating the structured output density model for RNNs with real-valued sequences.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "[4] extensively tested NADE and RBM-based output densities for modeling sequences of binary vector representations of music.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Bayer and Osendorfer [2] introduced a sequence of independent latent variables corresponding to the states of the RNN.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : ") Similarly, Pachitariu and Sahani [14] earlier proposed both a sequence of independent latent variables and a stochastic internal state for the RNN.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "For non-sequential data, variational autoencoders [11, 15] have recently been shown to be an effective modeling paradigm to recover complex multimodal distributions over the data space.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "For non-sequential data, variational autoencoders [11, 15] have recently been shown to be an effective modeling paradigm to recover complex multimodal distributions over the data space.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "In Kingma and Welling [11], the variational posterior q(z | x) is a GaussianN (μ, diag(σ)) whose mean μ and log-variance log(σ) are the output of a highly non-linear function of x, once again typically a neural network.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "STORN [2] model can be considered an instance of the VRNN-I model family.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "Blizzard: This text-to-speech dataset made available by the Blizzard Challenge 2013 contains 300 hours of English spoken by a single female speaker [9].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "Accent: This dataset contains English paragraphs read by 2, 046 different native and nonnative English speakers [17].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "Handwriting Generation We let each model to learn a sequence of (x, y)-coordinates together with binary indicators of pen up / pen down, using the IAM-OnDB dataset which consists of 13, 040 handwritten lines written by 500 writers [13].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 6,
      "context" : "We preprocess and split the dataset as done in [7].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "We trained each model by the stochastic gradient descent on the log-likelihood using the recently proposed Adam optimizer [10], with learning rate of 0.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "[15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "The authors would like to thank the developers of Theano [1].",
      "startOffset" : 57,
      "endOffset" : 60
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.",
    "creator" : "LaTeX with hyperref package"
  }
}