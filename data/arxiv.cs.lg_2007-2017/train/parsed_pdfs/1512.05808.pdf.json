{
  "name" : "1512.05808.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Successive Ray Refinement and Its Application to Coordinate Descent for LASSO",
    "authors" : [ "Jun Liu", "Zheng Zhao", "Ruiwen Zhang" ],
    "emails" : [ "junliu.nt@gmail.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "and its extensions due to its simplicity and efficiency. When applying coordinate descent to solving Lasso, we update one coordinate at a time while fixing the remaining coordinates. Such an update, which is usually easy to compute, greedily decreases the objective function value. In this paper, we aim to improve its computational efficiency by reducing the number of coordinate descent iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). SRR makes use of the following ray continuation property on the successive iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, it lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, it achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point and show that the refined search point can be efficiently obtained. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially for small Lasso regularization parameters."
    }, {
      "heading" : "1 Introduction",
      "text" : "Lasso [12] is an effective technique for analyzing high-dimensional data. It has been applied successfully in various areas, such as machine learning, signal processing, image processing, medical imaging, and so on. LetX = [x1,x2, . . . ,xp] ∈ Rn×p denote the data matrix composed of n samples with p variables, and let y ∈ Rn×1 be the response vector. In Lasso, we compute the β that optimizes\nmin β f(β) =\n1 2 ‖Xβ − y‖22 + λ‖β‖1, (1)\n∗Corresponding author. E-mail address: junliu.nt@gmail.com. †This work was done when Zheng Zhao was with SAS.\nar X\niv :1\n51 2.\n05 80\n8v 1\n[ cs\n.L G\n] 1\n7 D\nwhere the first term measures the discrepancy between the prediction and the response and the second term controls the sparsity of β with `1 regularization. The regularization parameter λ is nonnegative, and a larger λ usually leads to a sparser solution.\nResearchers have developed many approaches for solving Lasso in Equation (1). Least Angle Regression (LARS) [3] is one of the most well-known homotopy approaches for Lasso. LARS adds or drops one variable at a time, generating a piecewise linear solution path for Lasso. Unlike LARS, other approaches usually solve Equation (1) according to some prespecified regularization parameters. These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on. Among these approaches, coordinate descent is one of the most popular approaches due to its simplicity and efficiency. When applying coordinate descent to Lasso, we update one coordinate at a time while fixing the remaining coordinates. This type of update, which is easy to compute, can effectively decrease the objective function value in a greedy way.\nTo improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19]. Screening 1) identifies and removes the variables that have zero entries in the solution β and 2) solves Equation (1) by using only the kept variables. When one is able to discard the variables that have zero entries in the final solution β and identify the signs of the nonzero entries, the Lasso problem in Equation (1) becomes a standard quadratic programming problem. However, it is usually very hard to identify all the zero entries, especially when the regularization parameter is small. In addition, the computational cost of Lasso usually increases as the the regularization parameter decreases. The computational cost increase motivates us to come up with an approach that can accelerate the computation of Lasso for small regularization parameters.\nIn this paper, we aim to improve the computational efficiency of coordinate descent by reducing its iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). Our proposed SRR is motivated by an interesting ray-continuation property on the coordinate descent iterations: for a given coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Figure 1 illustrates the ray-continuation property by using the data specified in Section 2. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, the search point lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, the search point achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point, and we show that the refined search point can be efficiently computed. Experimental results on both synthetic and real data sets demonstrate that the proposed SRR can greatly accelerate the convergence of coordinate descent for Lasso, especially when the regularization parameter is small.\nOrganization The rest of this paper is organized as follows. We introduce the traditional coordinate descent for Lasso and present the ray-continuation property that motivates this paper in Section 2, propose the SRR technique in Section 3, discuss the efficient computation of the refinement factor that is used in SRR in Section 4, conduct\ni + (1 − αki )βki . Ray-continuation\nproperty: for a given coordinate i, the value obtained in the next iteration denoted by βk+1i almost always lies on a ray that starts at its previous iteration, β k−1 i , and passes through the current iteration, αki . For numerical details of plot (a) and plot (b), see Table 1 and Table 2.\nan eigenvalue analysis on the proposed SRR in Section 5, and compare SRR with related work in Section 6. We report experimental results on both synthetic and real data sets in Section 7, and we conclude this paper in Section 8.\nNotations Throughout this paper, scalars are denoted by italic letters and vectors by bold face letters. Let ‖ · ‖1 denote the `1 norm, let ‖ · ‖2 denote the Euclidean norm, and let ‖ · ‖∞ denote the infinity norm. Let 〈x,y〉 denote the inner product between x and y. Let a superscript denote the iteration number, and let a subscript denote the index of the variable or coordinate. We assume that X does not contain a zero column; that is, ‖xi‖2 6= 0,∀i."
    }, {
      "heading" : "2 Coordinate Descent For Lasso",
      "text" : "In this section, we first review the coordinate descent method for solving Lasso, and then analyze the adjacent iterations to motivate the proposed SRR technique.\nLet βki denote the ith element of β, which is obtained at the kth iteration of coordinate descent. In coordinate descent, we compute βki while fixing βj = β k j , 1 ≤ j < i, and βj = βk−1j , i < j ≤ p. Specifically, βki is computed as the minimizer to the following univariate optimization problem:\nβki = arg min β f([βk1 , . . . , β k i−1, β, β k−1 i+1 , . . . , β k−1 p ] T ).\nIt can be computed in a closed form as:\nβki = S(xTi y −\n∑ j<i x T i xjβ k j − ∑ j>i x T i xjβ k−1 j , λ)\n‖xi‖22 , (2)\nwhere S(·, ·) is the shrinkage function\nS(x, λ) =  x− λ x > λx+ λ x < −λ 0 |x| ≤ λ.\n(3)\nLet rki = y −X[βk1 , . . . , βki−1, βki , βk−1i+1 , . . . , β k−1 p ] T (4)\ndenote the residual obtained after updating βk−1i to β k i . With Equation (4), we can rewrite Equation (2) as\nβki = S(β k−1 i +\nxTi r k i−1\n‖xi‖22 ,\nλ\n‖xi‖22 ). (5)\nIn addition, with the updated βki , we can update the residual from r k i−1 to r k i as\nrki = r k i−1 + xi(β k−1 i − β k i ). (6)\nAlgorithm 1 illustrates solving Lasso via coordinate descent. Since the non-smooth `1 penalty in Equation (1) is separable, the algorithm is guaranteed to converge [14].\nAlgorithm 1 Coordinate Descent for Lasso Input: X , y, λ Output: βk\n1: k = 0, β0 = 0, r0 = y 2: repeat 3: Set k = k + 1, rk = rk−1 4: for i = 1 to p do 5: Compute βki = S(β k−1 i + xTi r k\n‖xi‖22 , λ‖xi‖22 )\n6: Update residual rk = rk + xi(βk−1i − βki ) 7: end for 8: until convergence criterion satisfied"
    }, {
      "heading" : "30 -0.083806 -0.141752 0.468749 0.033883 0.218827 0.000082 ...",
      "text" : ""
    }, {
      "heading" : "29 -0.082490 -0.142044 0.468368 0.032406 0.218288 0.000093",
      "text" : ""
    }, {
      "heading" : "28 -0.081090 -0.142355 0.467964 0.030835 0.217715 0.000106",
      "text" : ""
    }, {
      "heading" : "10 -0.031899 -0.149927 0.454929 -0.020507 0.197895 0.000950 ...",
      "text" : "We demonstrate Algorithm 1 using the following randomly generated X and y:\nX =  −0.204708 0.478943 −0.519439 −0.555730 1.965781 1.393406 0.092908 0.281746 0.769023 1.246435 1.007189 −1.296221 0.274992 0.228913 1.352917 0.886429 −2.001637 −0.371843 1.669025 −0.438570 −0.539741 0.476985 3.248944 −1.021228 −0.577087  , (7)\ny = [0.124121, 0.302614, 0.523772, 0.000940, 1.343810]T . (8)\nWe show the iterations of coordinate descent for Lasso with λ = 0 in Table 1 and Figure 1 (a). We set λ = 0 to facilitate the eigenvalue analysis in Section 5. Note that the results reported here also generalize to Lasso, because if we know the sign of the optimal solution β∗, the nonzero entries of β∗ can be solved by the following equivalent convex smooth problem:\nmin β\n1 2 ‖ ∑ i:si 6=0 xiβi − y‖22 + λ ∑ i:si 6=0 βisi, (9)\nwhere si = 0 if β∗i = 0, si = 1 if β ∗ i > 0, and si = −1 if β∗i < 0.\nIt can be observed from the results in Table 1 and Figure 1 (a) that we can obtain an approximate solution with a small objective function value within a few iterations. However, achieving a solution with high precision takes quite a few iterations for this example. More interestingly, for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. To show this, we compute αki that satisfies the following equation:\nβk+1i = α k i β k−1 i + (1− α k i )β k i . (10)\nTable 2 and Figure 1 (b) show the values of αki for different iterations. It can be observed that the values of αki are almost always positive except α 2 1 for this example. In addition, most of the values of αki are larger than 1. We tried quite a few synthetic data and observed a similar phenomenon.\nFor a particular iteration number k, if αki = α,∀i, we can easily achieve βk+1 = αβk−1 + (1−α)βk without needing to perform any coordinate descent iteration. This motivated us to come up with the successive ray refinement technique to be discussed in the next section."
    }, {
      "heading" : "3 Successive Ray Refinement",
      "text" : "In the proposed SRR technique, we make use of the ray-continuation property shown in Figure 1, Table 1, and Table 2. Our idea is as follows: To obtain βk+1, we perform coordinate descent based on a refined search point sk rather than on its previous solution βk. We propose setting the refined search point as:\nsk = (1− αk)hk + αkβk, (11)"
    }, {
      "heading" : "10 1.897021 1.240529 2.128716 2.068764 1.924043 ...",
      "text" : ""
    }, {
      "heading" : "28 1.939534 1.939949 1.939646 1.939628 1.939556",
      "text" : ""
    }, {
      "heading" : "29 1.939545 1.939821 1.939620 1.939608 1.939560",
      "text" : ""
    }, {
      "heading" : "30 1.939552 1.939736 1.939602 1.939594 1.939562",
      "text" : "where hk is a properly chosen history solution, βk is the current solution, and αk is an optimal refinement factor that optimizes the following univariate optimization problem:\nαk = arg min α {g(α) = f((1− α)hk + αβk)}. (12)\nThe setting of hk to one of the history solutions is based on the following two considerations. First, we aim to use the ray-continuation property to reduce the number of iterations. Second, we need to ensure that the univariate optimization problem in Equation (12) can be efficiently computed. We discuss the computation of Equation (12) in Section 4.\nFigure 2 illustrates the proposed SRR technique. When αk = 1, we have sk = βk; that is, the refined search point becomes the current solution βk. When αk = 0, we\nhave sk = hk; that is, the refined search point becomes the specified history solution hk. However, our next theorem shows that sk 6= hk because αk is always positive. In other words, the search point always lies on a ray that starts with the history point hk and passes through the current solution βk.\nTheorem 1 Assume that the history point hk satisfies\nf(hk) > f(βk). (13)\nThen, αk that minimizes Equation (12) is positive. In addition, if Xhk 6= Xβk, αk is unique.\nProof It is easy to verify that g(α) is convex. Therefore, αk that minimizes Equation (12) has at least one solution. Equation (13) leads to\ng(1) < g(0). (14)\nTherefore, the global refinement factor αk 6= 0. Next, we show that αk cannot be negative.\nIf αk < 0, due to the convexity of g(α), we have\ng((1− θ)αk + θ) ≤ (1− θ)g(αk) + θg(1),∀θ ∈ [0, 1]. (15)\nSetting θ = α k\nαk−1 , we have\ng(0) ≤ −1 αk − 1 g(αk) + αk αk − 1 g(1),∀θ ∈ [0, 1]. (16)\nMaking use of Equation (14), we have g(1) < g(αk). This contradicts the fact that αk minimizes Equation (12). Therefore, αk is always positive. IfXhk 6= Xβk, g(α) is strongly convex and thus αk is unique. This ends the proof of this theorem. For coordinate descent, the condition in Equation (13) always holds, because the objective function value keeps decreasing. The selection of an appropriate hk is key to the success of the proposed SRR, and the following theorem says that if hk is good enough, the refined search solution sk is an optimal solution to Equation (1).\nTheorem 2 Let β∗ be an optimal solution to Equation (1). If\nβ∗ − hk = γ(βk − hk), (17)\nfor some positive γ, sk achieved by SRR in Equation (11) satisfies f(sk) = f(β∗).\nProof When setting αk = γ, we have sk = β∗ under the assumption in Equation (17). Therefore, with the SRR technique, we can obtain a refined solution sk that is an optimal solution to Equation (1).\nIn the following subsections, we discuss two schemes for choosing the history solution hk."
    }, {
      "heading" : "3.1 Successive Ray Refinement Chain",
      "text" : "In the first scheme, we set\nhk = sk−1. (18)\nThat is, the history point is set to the most recent refined search point. Figure 3 demonstrates this scheme. Since the generated points follow a chain structure, we call this scheme the Successive Ray Refinement Chain (SRRC). In SRRC, sk−1, βk, and sk lie on the same line. In addition, coordinate descent (CD) controls the direction of the chain. In this illustration, it is assumed that the optimal refinement factor αk is larger than 1 in each step. According to Theorem 1, αk > 0. When αk ∈ (0, 1), sk lies between sk−1 and βk. When αk = 1, sk coincides with βk.\nIn Algorithm 2, we apply the proposed SRRC to coordinate descent for Lasso. Compared with the traditional coordinate descent in Algorithm 1, the coordinate update is based on the search point sk−1 rather than on the previous solution βk−1. When αk in line 9 of Algorithm 2 is set to 1, Algorithm 2 becomes identical to Algorithm 1. Table 3 illustrates Algorithm 2 with the same input X and y that are used in Table 1. Comparing Table 3 with Table 1, we can see that the number of iterations can be significantly reduced with the usage of the SRRC technique. Specifically, to achieve a function value of below 10−3, the traditional coordinate descent takes 10 iterations, whereas the one with the SRRC technique takes 7 iterations; to achieve a function value below 10−4, the traditional coordinate descent takes 29 iterations, whereas the one with the SRRC technique 14 iterations; and to achieve a function value below 10−8, the traditional coordinate descent takes 103 iterations, whereas the one with the SRRC technique takes 16 iterations.\nAs can be seen from Figure 3, we generate two sequences: {sk} and {βk}. At iteration k, the SRRC technique is very greedy in that it constructs the search point sk by using the two existing points sk−1 and βk to achieve the lowest objective function\nAlgorithm 2 Coordinate Descent plus SRRC (CD+SRRC) for Lasso Input: X , y, λ Output: βk\n1: Set k = 0, s0 = 0, r0s = y 2: repeat 3: Set k = k + 1, rk = rk−1s 4: for i = 1 to p do 5: Compute βki = S(s k−1 i + xTi r k\n‖xi‖22 , λ‖xi‖22 )\n6: Obtain rk = rk + xi(sk−1i − βki ) 7: end for 8: if convergence criterion not satisfied then 9: Set αk = arg minα f((1− α)sk−1 + αβk)\n10: Set sk = (1− αk)sk−1 + αkβk 11: Set rks = (1− αk)rk−1s + αkrk 12: end if 13: until convergence criterion satisfied\nvalue. If the search point sk−1 is dense at some iteration number k and αk 6= 1, it can be shown that sk is also dense. This is not good for Lasso, which usually has a sparse solution. Interestingly, our empirical simulations show that Algorithm 2 can set αk = 1 in some iterations, leading to a sparse search point."
    }, {
      "heading" : "3.2 Successive Ray Refinement Triangle",
      "text" : "In the second scheme, we set\nhk = βk−1. (19)\nFigure 4 demonstrates this scheme. Since the generated points follow a triangle structure, we call this scheme the Successive Ray Refinement Triangle (SRRT). SRRT is less greedy compared to SRRC because βk−1 leads to a higher objective function value than sk−1 leads to. However, SRRT can sometimes outperform SRRC in solving Lasso.\nAlgorithm 3 shows the application of the proposed SRRT technique to coordinate descent for Lasso. Similar to Algorithm 2, ifαk in line 9 is set to 1, Algorithm 3 reduces to the traditional coordinate descent in Algorithm 1. Table 4 illustrates Algorithm 3. Similar to SRRC, SRRT greatly reduces the number of iterations used in coordinate descent for Lasso."
    }, {
      "heading" : "3.3 Convergence of CD plus SRR",
      "text" : "In this subsection, we show that both the combination of CD and SRRC (CD+SRRC) and the combination of CD and SRRT (CD+SRRT) are guaranteed to converge.\nTheorem 3 For the sequence s0,β1, s1,β2, s2,β3, . . . generated by CD+SRRC and CD+SRRT, the objective function value is monotonically decreasing until convergence; that is,\nf(sk−1) ≥ f(βk) ≥ f(sk) ≥ f(βk+1). (20)\nAlgorithm 3 Coordinate Descent plus SRRT (CD+SRRT) for Lasso Input: X , y, λ Output: βk\n1: Set k = 0, s0 = 0, r0s = y 2: repeat 3: Set k = k + 1, rk = rk−1s 4: for i = 1 to p do 5: Compute βki = S(s k−1 i + xTi r k\n‖xi‖22 , λ‖xi‖22 )\n6: Obtain rk = rk + xi(sk−1i − βki ) 7: end for 8: if convergence criterion not satisfied then 9: Set αk = arg minα f((1− α)βk−1 + αβk)\n10: Set sk = (1− αk)βk−1 + αkβk 11: Set rks = (1− αk)rk−1 + αkrk 12: end if 13: until convergence criterion satisfied\nIn addition, if f(sk−1) = f(βk), we have sk−1 = βk and βk is an optimal solution; that is,\nf(βk) = min β f(β). (21)\nTherefore, we have lim k→∞ f(βk) = min β f(β). (22)\nProof βk is computed by applying coordinate descent based on sk−1; that is,\nβki = arg min β f([βk1 , . . . , β k i−1, β, s k−1 i+1 , . . . , s k−1 p ] T ), (23)\nor equivalently\nβki = S(xTi y −\n∑ j<i x T i xjβ k j − ∑ j>i x T i xjs k−1 j , λ)\n‖xi‖22 . (24)\nTherefore, we have\nf([βk1 , . . . , β k i−1, β k i , s k−1 i+1 , . . . , s k−1 p ] T )\n≤ f([βk1 , . . . , βki−1, sk−1i , s k−1 i+1 , . . . , s k−1 p ]\nT ). (25)\nfor all i. Since ‖xi‖2 6= 0, f([βk1 , . . . , βki−1, β, s k−1 i+1 , . . . , s k−1 p ]\nT ) is strongly convex in β. As a result, if the equality in Equation (25) holds, we have βki = s k−1 i . Recursively applying Equation (25), we have the following two facts: f(βk) ≤ f(sk−1) and if f(βk) = f(sk−1), then sk−1 = βk.\nIf sk−1 = βk, it follows from Equation (24) that ‖xi‖22βki = S(xTi y − ∑ j 6=i xTi xjβ k j , λ)\n= S(xTi y − xTi Xβk + ‖xi‖22βki , λ), (26)\nwhich leads to xTi y − xTi Xβk ∈ SGN(βki ), (27)\nwhere\nSGN(t) =  {1}, t > 0{−1}, t < 0 [−1, 1] , t = 0.\n(28)\nSince β∗ is an optimal solution to Equation (1) if and only if\nxTi y − xTi Xβ∗ ∈ SGN(β∗i ),∀i, (29)\nit follows from Equation (27) that βk is an optimal solution to Equation (1). The relationship f(βk) ≥ f(sk) is guaranteed by the univariate optimization problem in Equation (12). Therefore, the sequence {f(βk)} is decreasing. Meanwhile, the squence {f(βk)} has a lower bound minβ f(β). According to the well-known monotone convergence theorem, we have Equation (22).\nThis completes the proof of this theorem."
    }, {
      "heading" : "4 Efficient Refinement Factor Computation",
      "text" : "In this section, we discuss how to efficiently compute the refinement factor αk in Equation (12). The function g(α) can be written as:\ng(α) = 1\n2\n∥∥X((1− α)hk + αβk)− y∥∥2 2 + λ‖(1− α)hk + αβk‖1\n= 1\n2 ∥∥rkh − α(rkh − rk)∥∥22 + λ‖hk − α(hk − βk)‖1, (30) where rkh = y −Xhk and rk = y −Xβk are the residuals that correspond to hk and βk, respectively. Note that 1) rkh = r k−1 s for SRRC and r k h = r\nk−1 for SRRT, and 2) both rkh and r\nk have been obtained before line 8 of Algorithm 2 and Algorithm 3. Before the convergence, we have rkh 6= rk. Therefore, g(α) is strongly convex in α, and αk, the minimizer to Equation (12), is unique.\nWhen λ = 0, Equation (12) has a nice closed form solution,\nαk = 〈rkh, rkh − rk〉 ‖rkh − rk‖22 . (31)\nNext, we discuss the case λ > 0. The subgradient of g(α) with regard to α can be computed as\n∂g(α) = α‖rkh − rk‖22 − 〈rkh, rkh − rk〉\n+ λ p∑ i=1 (βki − hi)SGN(hi − α(hi − βki )). (32)\nCompute αk is a root-finding problem. According to Theorem 1, we have αk > 0. Next, we consider only α > 0 for ∂g(α). We consider the following three cases:\n1. If hi = 0, we have\n(βki − hi)SGN(hi − α(hi − βki )) = {|βki |}.\n2. If hi(βki − hi) > 0, we have\n(βki − hi)SGN(hi − α(hi − βki )) = {|βki − hi|}.\n3. If hi(βki − hi) < 0, we let\nwi = hi\nhi − βki , (33)\nand we have (βki − hi)SGN(hi − α(hi − βki )) = {−|β k i − hi|} α ∈ (0, wi)\n{|βki − hi|} α ∈ (wi,+∞) |βki − hi|{[−1, 1]} α = wi.\n(34)\nFor the first two cases, the set SGN(hi − α(hi − βki )) is deterministic. For the third case, SGN(hi − α(hi − βki )) is deterministic when α 6= wi. Define\nΩ(hk,βk) = {i : hi(βki − hi) < 0}. (35)\nFigure 5 illustrates the function ∂g(α), α > 0. It can be observed that ∂g(α) is a piecewise linear function. If Ω(hk,βk) is empty, ∂g(α) is continuous; otherwise, ∂g(α) is not continuous at α = wi, i ∈ Ω(hk,βk)."
    }, {
      "heading" : "4.1 An Algorithm Based on Sorting",
      "text" : "To compute the refinement factor, one approach is to sort wi as follows:\nFirst, we sort wi, i ∈ Ω(hk,βk), and assume wi0 ≤ wi1 ≤ . . . ≤ wi|Ω(hk,βk)| . Second, for j = 1, 2, . . . , |Ω(hk,βk)|, we evaluate ∂g(α) at α = wij with the\nfollowing three cases:\n1. If 0 ∈ ∂g(wij ), we have αk = wij and terminate the search.\n2. If an element in ∂g(wij ) is positive, α k lies in the piecewise line starting α =\nwij−1 and ending α = wij , and it can be analytically computed.\n3. If all elements in ∂g(wij ) are negative, we set j = j+1 and continue the search.\nFinally, if all elements in ∂g(wij ) are negative when j = |Ω(hk,βk)|, αk lies on the piecewise line that starts at α = wij . Thus, α̃\nk can be analytically computed. With a careful implementation, the naive approach can be completed in O(p + m log(m)), where m = |Ω(hk,βk)|. In Lasso, the solution is usually sparse, and thus m is much smaller than p, the number of variables."
    }, {
      "heading" : "4.2 An Algorithm Based on Bisection",
      "text" : "A second approach is to make use of the improved bisection proposed in [7]. The idea is to 1) determine an initial guess of the interval [α1, α2] to which the root belongs, where all elements in ∂g(α1) are negative and all elements in ∂g(α2) are positive, 2) evaluate ∂g(α) at α = α1+α22 and update the interval to [α1, α) if all the elements in ∂g(α) are positive or to [α, α2) if all the elements in ∂g(α) are negative, 3) set the value of α to the largest value of wi that satisfy wi < α if all the elements in ∂g(α) are positive or to the smallest value of wi that satisfy wi > α if all the elements in ∂g(α) are negative, and 4) repeat 2) and 3) until finding the root of ∂g(α). With a similar implementation as in [7], the improved bisection approach has a time complexity of O(p)."
    }, {
      "heading" : "5 An Eigenvalue Analysis on the Proposed SRR",
      "text" : "Let A = XXT = L+D + U, (36)\nwhere D is A’s diagonal part, L is A’s strictly lower triangular part, and U is A’s strictly upper triangular part. It is easy to see that\nLij =\n{ xTi xj i < j\n0 i ≥ j, (37)\nDij =\n{ xTi xi i = j\n0 i 6= j, (38)\nUij =\n{ xTi xj i > j\n0 i ≤ j. (39)\nWe can rewrite Equation (2) as\nDiiβ k i = S(x T i y − Li:βk − Ui:βk−1, λ), (40)\nwhere Li: and Ui: denote the ith row of L and U , respectively. Therefore, we can write coordinate descent iteration as:\nDβk = S(XTy − Lβk − Uβk−1, λ). (41)\nWhen λ = 0, Equation (41) becomes\n(L+D)βk+1 = XTy − Uβk, (42)\nwhich is the Gauss-Seidel method for solving\nXTXβ = (L+D + U)β = XTy. (43)\nEquation (43) is also the optimality condition for Equation (1) when λ = 0. Our next discussion is for the case λ = 0 because it is easy to write the linear systems for the iterations.\nDenote G = −(L+D)−1U. (44)\nLet G have the following eigendecomposition:\nG = P∆P−1, (45)\nwhere ∆ = diag(δ1, δ2, . . . , δp) is a diagonal matrix consisting of its eigenvalues.\nLemma 1 The magnitudes of the eigenvalues of G are all less than or equal to 1; that is,\n|δi| ≤ 1,∀i. (46)\nProof Let Gz = σz, (47)\nwhere σ is an eigenvalue of G with the corresponding eigenvector being z. Note that σ and the entries in z can be complex. Using Equation (36) and Equation (44), we have\n(L+D −XTX)z = −Uz = (L+D)σz. (48)\nwhich leads to (L+D)(1− σ)z = XTXz. (49)\nIf σ = 1, the corresponding eigenvector z is in the null space of XTX . If σ 6= 1, we have\n(L+D)z = 1\n(1− σ) XTXz. (50)\nPremultiplying Equation (50) by zH , the conjugate transpose of z, we have\nzH(L+D)z = 1\n(1− σ) zHXTXz. (51)\nTaking the conjugate transpose of Equation (51), we have\nzH(U +D)z = 1\n(1− σ̄) zHXTXz, (52)\nwhere σ̄ denotes the conjugate of σ. Adding Equation (51) and Equation (52) and subtracting zHXTXz, we have\nzHDz =\n( 1\n(1− σ̄) +\n1 (1− σ) − 1 ) zHXTXz. (53)\nSince zHDz > 0 and zHXTXz ≥ 0, we have\n0 < 1\n(1− σ̄) +\n1 (1− σ) − 1 = 1− |σ| 2 |1− σ|2 . (54)\nTherefore, we have 1− |σ|2 > 0 or equivalently |σ| < 1. This ends the proof of this lemma."
    }, {
      "heading" : "5.1 An Eigenvalue Analysis on CD+SRRC",
      "text" : "For CD+SRRC in Algorithm 2, when λ = 0 we have\nβk = (L+D)−1[XTy − Usk−1] (55)\nsk = (1− αk)sk−1 + αkβk. (56)\nIt can be shown that sk − sk−1 = αk [ −(L+D)−1U + 1− α k−1\nαk−1 I\n] (sk−1 − sk−2), (57)\nβk − βk−1 = −(L+D)−1U(sk−1 − sk−2). (58)\nWhen k ≥ 2, we denote Ak = αk [ −(L+D)−1U + 1− α k−1\nαk−1 I\n] . (59)\nIt can be shown that Ak = PΣkP−1, (60)\nwhere Σk = diag(σk1 , σ k 2 , . . . , σ k p) is a diagonal matrix and\nσki = α k(δi +\n1− αk−1\nαk−1 ). (61)\nTherefore, we have sk − sk−1 = ( Πki=2A k ) (s1 − s0) = P (Πki=2Σk)P−1(s1 − s0), (62)\nFor discussion convenience, we let Σ1 = ∆ and\nT k = diag(tk1 , t k 2 , . . . , t k p) = Π k i=1Σ k, (63)\nwhere tki = Π k i=1σ k i . (64)\nWe have βk − βk−1 = PT k−1P−1(s1 − s0). (65)\nFor the traditional coordinate descent in Algorithm 1, αk = 1,∀k. For the proposed CD+SRRC in Algorithm 2, αk optimizes Equation (12).\nFor the data used in Table 1, the eigenvalues of −(L+D)−1U are\nδ1 = 0, δ2 = 0.00219338, δ3 = 0.12412229,\nδ4 = 0.62606165, δ5 = 0.93956707.\nFor the traditional coordinate descent in Algorithm 1, when k = 30, we have\nt291 = 0, t 29 2 < 0.000001, t 29 3 < 0.000001,\nt294 = 0.000002, t 29 5 = 0.164023.\nFor the coordinate descent with SRRC in Algorithm 2, we have\nt291 = 0, t 29 2 < 0.000001, t 29 3 < 0.000001,\nt294 < 0.000001, t 29 5 = 0.000008.\nThis explains why the proposed SRRC can greatly accelerate the convergence of the coordinate descent method."
    }, {
      "heading" : "5.2 An Eigenvalue Analysis on CD+SRRT",
      "text" : "For coordinate descent with SRRT in Algorithm 3, when λ = 0 we have\nβk = (L+D)−1[XTy − Usk−1] (66)\nsk = (1− αk)βk−1 + αkβk. (67)\nWhen k ≥ 3, it can be shown that\nβk − βk−1 = G[(1− αk−2)(βk−2 − βk−3) + αk−1(βk−1 − βk−2)]. (68)\nWhen k = 2, we have β2 − β1 = α1G(β1 − β0). (69)\nUsing the recursion in Equation (68), we can get\nβ3 − β2 = [(1− α1)G+ α1Gα2G](β1 − β0). (70)\nβ4 − β3 =[α1G(1− α2)G+ (1− α1)Gα3G + α1Gα2Gα3G](β1 − β0).\n(71)\nGenerally speaking, we can write\nβk − βk−1 = PT k−1P−1(β1 − β0), (72)\nwhere T k = diag(tk1 , t k 2 , . . . , t k p) is a diagonal matrix. For t k i , it is a polynomial function of δi; that is, tki = φk(δi), where\nφk(t) = t× . . .× t︸ ︷︷ ︸ dk/2e k−dk/2e∑ i=0 ci × t× . . .× t︸ ︷︷ ︸ i ), (73)\nand c0, c1, . . . , ck−dk/2e are dependent on α1, α2, . . . , αk−1. When k = 2, we have\nc0 = α 1. (74)\nWhen k = 3, we have c0 = 1− α1, c1 = α 1α2. (75)\nWhen k = 4, we have\nc0 = α 1(1− α2) + α3(1− α1), c1 = α 1α2α3.\n(76)\nWhen k = 5, we have\nc0 = (1− α2)(1− α3), c1 = α\n1(1− α2)α4 + α3(1− α1)α4 + α1α2(1− α3), c2 = α 1α2α3α4. (77)\nFor the coordinate descent with SRRT in Algorithm 3, we have\nt291 = 0, t 29 2 < 0.000001, t 29 3 < 0.000001,\nt294 = 0.000002, t 29 5 = 0.000393,\nwhich are smaller than the ones in the traditional coordinate descent shown in Section 5.1."
    }, {
      "heading" : "6 Related Work",
      "text" : "In this section, we compare our proposed SRR with successive over-relaxation [17] and the accelerated gradient descent method [9]."
    }, {
      "heading" : "6.1 Relationship between SRRC and Successive Over-Relaxation",
      "text" : "Successive over-relaxation (SOR) is a classical approach for accelerating the GaussSeidel approach. Our discussion in this section considers only λ = 0, because SOR targets the acceleration of the Gauss-Seidel approach.\nFrom Equation (43), we have\n(wL+D)β = wXTy − wUβ − (w − 1)Dβ,∀w > 0. (78)\nThe iteration used in successive over-relaxation is:\nβk = (wL+D)−1[wXT y − [wU + (w − 1)D]βk−1, (79)\nwhich can be obtained by plugging βk and βk−1 into Equation (78). Equation (79) can be rewritten as:\nβk = βk−1 − w(wL+D)−1[XTXβk−1 −XTy]. (80)\nFor the proposed CD+SRRC in Algorithm 2, when λ = 0 we have\nsk = (1− αk)sk−1 + αk(L+D)−1 [ XTy − Usk−1 ] = sk−1 − αk(L+D)−1 [ XTXsk−1 −XTy ] .\n(81)\nWhen w = 1 and αk = 1, both SOR and CD+SRRC reduce to the traditional coordinate descent. Equation (79) and Equation (81) share the following two similaries: 1) both make use of the gradient in the recursive iterations in that XTXβk−1 −XTy is the gradient of 12‖Xβ − y‖ 2 2 at β\nk−1 and XTXsk−1 − XTy is the gradient of 1 2‖Xs\nk−1 − y‖22 at sk−1, and 2) both use a precondition matrix in that SOR uses (wL + D) whereas SRRC uses (L + D). A key difference is that the precondition matrix used in SRRC is parameter-free whereas the one used in SOR has a parameter. As a result, we can perform an inexpensive univariate search to find the optimal αk used in SRRC whereas it is usually expensive for SOR to search for an optimal w in the same way as Equation (12).\nWhen the design matrix has some special structures, it has been shown in [17] that the optimal value of w can be found for SOR. However, for the general design matrix X , it is hard to obtain the optimal w used for SOR. This might be a major reason that SOR is not widely used in solving Lasso with coordinate descent. For our proposed SRRC, the criterion in Equation (12) enables us to adaptively set the refinement factor αk."
    }, {
      "heading" : "6.2 Relationship between SRRT and the Nesterov’s Method",
      "text" : "The SRRT scheme presented in Figure 4 is similar to the Nesterov’s method in that both make use of a search point in the iterations. In addition, both set the search point using\nsk = (1− αk)βk−1 + αkβk. (82)\nHowever, the key difference is that the αk used in the Nesterov’s method is predefined according to a specified formula, whereas the αk used in SRRT is set to optimize the objective function as shown in Equation (12). Note that if the Nesterov’s method sets the αk to optimize the objective function, it reduces to the traditional steepest descent method thus the good acceleration property of the Nesterov’s method is gone."
    }, {
      "heading" : "7 Experiments",
      "text" : "In this section, we report experimental results for synthetic and real data sets, studying the number of iterations of CD, CD+SRRC and CD+SRRT for solving Lasso. The consumed computational time is proportional to the number of iterations.\nSynthetic Data Sets We generate the synthetic data as follows. The entries in the n×p design matrix X and the n× 1 response y are drawn from a Gaussian distribution. We try the following three settings of n and p: 1) n = 500, p = 1000, 2) n = 1000, p = 1000, and 3) n = 1000, p = 500.\nReal Data Sets We make use of the following three real data sets provided in [2]: leukemia, colon, and gisette. The leukemia data set has n = 38 samples and p = 7129 variables. The colon data set has n = 62 samples and p = 2000 variables. The gisette data set has n = 6000 samples and p = 5000 variables.\nExperimental Settings For the value of the regularization parameter, we try λ = r‖XTy‖∞, where r = 0.5, 0.1, 0.05, 0.01. For the synthetic data sets, the reported results are averaged over 10 runs. For a particular regularization parameter, we first run CD in Algorithm 1 until ‖βk − βk−1‖2 ≤ 10−6, and then run CD+SRRC and CD+SRRT until the obtained objective function value is less than or equal to the one obtained by CD.\nResults Table 5 and Table 6 show the results for the synthetic and real data sets, respectively. The last column of each table shows the sparsity of the obtained Lasso solution, which is defined as the number of zero entries in the solution divided by the number of variables p. Figure 6 visualizes the results in these two tables. We can see that when the solution is very sparse (for example, λ = 0.5‖XTy‖∞), the proposed CD+SRRC and CD+SRRT consume comparable number of iterations to the traditional CD. The reason is that the optimal refinement factor computed by SRR in Equation (12) is equal to or close to 1, and thus CD+SRRC and CD+SRRT is very close to the traditional CD. Note that a regularization parameter λ = 0.5‖XTy‖∞ is usually too large for practical applications because it selects too few variables, and we usually need to try a smaller λ = r‖XTy‖∞ for example, r = 0.01. It can be observed that the proposed CD+SRRC and CD+SRRT requires much fewer iterations, especially for smaller regularization parameters."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we propose a novel technique called successive ray refinement. Our proposed SRR is motivated by an interesting ray-continuation property on the coordinate descent iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. We propose two schemes for SRR and apply them to solving Lasso with coordinate descent. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially when the regularization parameter is small.\nWe have established the convergence of CD+SRR, and it is interesting to study the convergence rate. We focus on a least squares loss function in (1), and we plan to apply the SRR technique to solving the generalized linear models. We compute the refinement factor as an optimal solution to Equation (12), and we plan to obtain the refinement factor as an approximate solution, especially in the case of generalized linear models."
    } ],
    "references" : [ {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "C.C. Chang", "C.J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Regularization paths for generalized linear models via coordinate descent",
      "author" : [ "J.H. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Safe feature elimination in sparse supervised learning",
      "author" : [ "L. Ghaoui", "V. Viallon", "T. Rabbani" ],
      "venue" : "Pacific Journal of Optimization,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "An interior-point method for large-scale l1regularized logistic regression",
      "author" : [ "K. Koh", "S. Kim", "S. Boyd" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Efficient euclidean projections in linear time",
      "author" : [ "J. Liu", "J. Ye" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Safe screening with variational inequalities and its application to lasso",
      "author" : [ "J. Liu", "Z. Zhao", "J. Wang", "J. Ye" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Introductory lectures on convex optimization : a basic course",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Applied optimization. Kluwer Academic Publ.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "Safe screening of non-support vectors in pathwise SVM computation",
      "author" : [ "K. Ogawa", "Y. Suzuki", "I. Takeuchi" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Stochastic methods for `1 regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "Strong rules for discarding predictors in lasso-type problems",
      "author" : [ "R. Tibshirani", "J. Bien", "J.H. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R.J. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society: Series B,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Convergence of a block coordinate descent method for nondifferentiable minimization",
      "author" : [ "P. Tseng", "Communicated O.L. Mangasarian" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2001
    }, {
      "title" : "Lasso screening rules via dual polytope projection",
      "author" : [ "J. Wang", "B. Lin", "P. Gong", "P. Wonka", "J. Ye" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Sparse reconstruction by separable approximation",
      "author" : [ "S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Iterative methods for solving partial difference equations of elliptical type",
      "author" : [ "D.M. Yong" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1950
    }, {
      "title" : "An improved glmnet for l1-regularized logistic regression",
      "author" : [ "G.X. Yuan", "C.H. Ho", "C.J. Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Learning sparse representations of high dimensional data on large scale dictionaries",
      "author" : [ "J.X. Zhen", "X. Hao", "J.R. Peter" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Lasso [12] is an effective technique for analyzing high-dimensional data.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : "Least Angle Regression (LARS) [3] is one of the most well-known homotopy approaches for Lasso.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 12,
      "context" : "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "Since the non-smooth `1 penalty in Equation (1) is separable, the algorithm is guaranteed to converge [14].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "If α < 0, due to the convexity of g(α), we have g((1− θ)α + θ) ≤ (1− θ)g(α) + θg(1),∀θ ∈ [0, 1].",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "Setting θ = α k αk−1 , we have g(0) ≤ −1 αk − 1 g(α) + α αk − 1 g(1),∀θ ∈ [0, 1].",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "2 An Algorithm Based on Bisection A second approach is to make use of the improved bisection proposed in [7].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "With a similar implementation as in [7], the improved bisection approach has a time complexity of O(p).",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "In this section, we compare our proposed SRR with successive over-relaxation [17] and the accelerated gradient descent method [9].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "In this section, we compare our proposed SRR with successive over-relaxation [17] and the accelerated gradient descent method [9].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "When the design matrix has some special structures, it has been shown in [17] that the optimal value of w can be found for SOR.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Real Data Sets We make use of the following three real data sets provided in [2]: leukemia, colon, and gisette.",
      "startOffset" : 77,
      "endOffset" : 80
    } ],
    "year" : 2015,
    "abstractText" : "Coordinate descent is one of the most popular approaches for solving Lasso and its extensions due to its simplicity and efficiency. When applying coordinate descent to solving Lasso, we update one coordinate at a time while fixing the remaining coordinates. Such an update, which is usually easy to compute, greedily decreases the objective function value. In this paper, we aim to improve its computational efficiency by reducing the number of coordinate descent iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). SRR makes use of the following ray continuation property on the successive iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, it lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, it achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point and show that the refined search point can be efficiently obtained. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially for small Lasso regularization parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}