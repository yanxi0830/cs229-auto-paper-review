{
  "name" : "1502.06531.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Scalable Variational Inference in Log-supermodular Models",
    "authors" : [ "Josip Djolonga", "Andreas Krause" ],
    "emails" : [ "JOSIPD@INF.ETHZ.CH", "KRAUSEA@ETHZ.CH" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Performing inference in probabilistic models is one of the central challenges in machine learning, providing a foundation for making decisions with uncertain data. Unfortunately, the general problem is intractable and one must resort to approximate inference techniques. The importance of this problem is witnessed by the amount of interest it has attracted in the research community, which has resulted in a large family of approximations, most notably the\nmean-field (Wainwright & Jordan, 2008) and belief propagation (Pearl, 1986) algorithms and their variants. One major drawback of these and many other techniques is the exponential dependence on the size of the largest factor which restricts the class of models one can use. In addition, these methods generally involve non-convex objectives, resulting in local optima (or even non-convergence).\nWe consider the problem of inference in distributions over sets, also known as point processes. Formally, we have some finite ground set V and a measure P that assigns some probability P (A) to every subset A ⊆ V . We would like to point out that we can equivalently see such distributions as being defined over |V | Bernoulli random variables Xi ∈ {0, 1}, one for every element in the ground set i ∈ V indicating if element i has been selected. As a concrete example showing this equivalence consider the task of image segmentation in computer vision, where one wants to separate the foreground from the background pixels. Traditionally, one defines one random variable Xp ∈ {0, 1} for each pixel p indicating if the pixel is in the foreground or the background. We can also isomorphically treat the distribution as being defined over subsets of the set of all pixels V . In this case, for any subset A ⊆ V the quantity P (A) is the probability of pixels A belonging to the foreground. In the remaining of the paper we will employ this latter view of distributions over sets. The additional assumption that we make is that the distribution is log-supermodular, i.e. can be written as P (A) = 1Z exp(−F (A)), where F is some submodular function.\nRelated work. Submodular functions are a family of set functions exhibiting a natural diminishing returns property, originating first in the field of combinatorial optimization (Edmonds, 1970). They have been applied to many problems in machine learning, including clustering (Narasimhan et al., 2005), variable selection (Krause & Guestrin, 2005), structured norms (Bach, 2010), dictionary learning (Cevher & Krause, 2011), etc. Submodular functions have huge implications for the tractability of (approximate) optimization, akin to convexity and concavity in continuous domains. While the major emphasis has consequently been on optimization, submodular functions can\nar X\niv :1\n50 2.\n06 53\n1v 2\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n15\nbe also employed to define probabilistic models. Special cases include Ising models used in computer models and the determinantal point process (DPP) (Kulesza & Taskar, 2012) used for modeling diversity. Alas, submodularity does not render the inference problem tractable, which remains extremely difficult even for the Ising model (Goldberg & Jerrum, 2007; Jerrum & Sinclair, 1993) which has only pairwise interactions.\nThe study of approximate Bayesian inference in general log-supermodular models has been recently initiated by Djolonga & Krause (2014). They provide a general variational approach –L-FIELD– that optimizes bounds on the partition function via the differentials of submodular functions. While their approach leads to optimization problems that can be solved exactly in polynomial time for arbitrary high order interactions, presently the approach is slow, and impractical for large scale inference tasks such as those arising in computer vision.\nOur contributions. We improve over their result in several ways. First, by showing an equivalence of L-FIELD with a classical problem in submodular minimization – the minimum norm point problem – we obtain access to a large family of specially crafted algorithms that can handle models with very large numbers of variables. In the experimental section we indeed perform inference over images, which have hundreds of thousands of variables. This insight also implies, for example, that the approximation agrees on the mode of the distribution, hence the MAP problem is solved for free. Secondly, by establishing another important connection, namely to a specific type of information divergence, we shed light on the type of approximations that result from this method. Thirdly, we show how special structure of many real-world log-supermodular models (such as those in image segmentation with high-order potentials) enable a highly efficient parallel message passing algorithm that converges to the global optimum at a linear rate. Lastly, we perform extensive experiments on a challenging image segmentation task, demonstrating that our approach is scalable, provides more accurate marginals than existing techniques, and provides evidence on the effectiveness of models using high-order interactions."
    }, {
      "heading" : "2. Background: Submodularity and log-supermodular models",
      "text" : "Formally, a function F : 2V → R is said to be submodular if for any pair of sets A ⊆ B and x /∈ B it holds that\nF ({x} ∪A)− F (A) ≥ F ({x} ∪B)− F (B).\nIn other words, the gain of adding any element x decreases as the context grows, which is the diminishing returns property already mentioned. Additionally, without\nany loss of generality we assume that F is normalized so that F (∅) = 0. We will consider Gibbs distributions that arise from these models, more specifically probability measures of the form\nP (S) = 1\nZ exp(−F (S)),\nfor some submodular F : 2V → R. These models are called log-supermodular or attractive for reasons explained below.\nExamples. A typical example of such models is the regular Ising model, which can be used for the image segmentation task from the introduction. Continuing with that example, we define a set of edges E that connect neighboring pixels, and for every pair of neighbors {p, p′} we specify a weight w{p,p′} ≥ 0 that quantifies their similarity. To model the preference of neighbors to be assigned to the same segment, we use the cut function\n∀A ⊆ V : F (A) = ∑\n{p,p′}∈E\n1|A∩{p,p′}|=1w{p,p′}.\nHence, if we place two neighboring pixels p and p′ in different segments, we will cut the edge {p, p′} and be “penalized” by the corresponding weight, which explains the attractive behavior of the model. We can go one step further and define regions Pi ⊆ V which we would prefer to be in the same segment. One strategy to generate the regions, used by Kohli et al. (2009) is to generate superpixels, as illustrated on Figure 1. We can then modify the model to incorporate these higher order potentials by adding terms of the form φ(|Pi ∩A|/|Pi|) for some concave function φ. As a concrete example, consider φ(z) = z(1 − z), which assigns a value of 0 when the pixels in the superpixel are in the same segment, and assigns a larger penalty otherwise, which is maximal when the pixels are equally split between the two segments.\nModular functions. The simplest family of submodular functions are modular functions, which can be seen as the\ndiscrete analogue of linear functions. Namely, a function s : 2V → R is said to be modular if for all A ⊆ V it holds that s(A) = ∑ i∈A s({i}). The family of distributions that arise from these functions are exactly the family of completely factorized distributions1, because\nP (S) ∝ exp(−s(S)) = ∏ i∈S exp(−si).\nAs evident from their definition, modular functions are uniquely defined through the quantities s({i}) for all i ∈ V . It is very useful to view modular functions as vectors s ∈ RV with coordinates si = s({i}).\nSubmodular polyhedra. There are several polyhedra that contain some of these modular functions (in their vectorial representation) that we will make use of. More specifically, we are interested in the submodular polyhedron P (F ) and the base polytope B(F ), which are defined as\nP (F ) = {s ∈ RV | ∀A ⊆ V : s(A) ≤ F (A)}, (1) B(F ) = P (F ) ∩ {s ∈ RV | s(V ) = F (V )}. (2)\nIn other words, P (F ) is the set of all modular lower bounds of the function F , while B(F ) adds the further restriction that the bound must be tight at the ground set V . It can be shown that these polyhedra are not empty and their geometry is also well understood (Fujishige, 2005; Bach, 2013). Moreover, what is especially surprising, is that even though B(F ) is defined with exponentially many inequalities, we can optimize linear functions over it in O(|V | log |V |) time with a simple greedy strategy (Edmonds, 1970).\nMAP estimation and the minimum norm point. A very natural question that arises for any probabilistic model is that of finding a MAP configuration, which for logsupermodular distribution amounts to minimizing the function F . This is a problem that has been studied in much detail and has resulted in numerous approaches. The fastest known combinatorial algorithm due to Orlin (2009) has a bound of O(n6 + τn5), where τ is the cost of evaluating the function, and can be prohibitively expensive to run for larger ground sets. An algorithm that performs better in practice, but only has a pseudopolynomial running time guarantee (Chakrabarty et al., 2014), is the Fujishige-Wolfe algorithm (Fujishige, 1980). This method approaches the problem by solving the following convex program, known as the minimum norm problem.\nminimize s∈B(F )\n‖s‖2. (3)\n1Because we use Gibbs distributions, note that they can not assign zero probabilities.\nOne can extract the solution to the submodular minimization problem from the solution to the above problem by thresholding, which is formalized in the following theorem. Theorem 1 (Fujishige (2005)). If s∗ is the optimal solution to problem (3), define the following sets\nA− = {v | v ∈ V and s∗v < 0}, and A0 = {v | v ∈ V and s∗v ≤ 0}.\nThenA− andA0 are the unique minimal and maximal minimizers of F ."
    }, {
      "heading" : "3. Variational inference with L-FIELD",
      "text" : "The main barrier to performing inference in logsupermodular models is the computation of the normalizing factor Z , also known as the partition function in the statistical physics literature. We cannot compute it directly as we have to sum up over all S ⊆ V , so we have to use approximative techniques. One common approach is to define an optimization problem over some variational parameter q, so that we can compute the quantity of interest by optimizing this problem.\nWe now review the variational approximation technique recently introduced by Djolonga & Krause (2014). Their method relies on two main observations: (i) modular functions have analytical log-partition functions and (ii) submodular functions can be lower-bounded by modular functions. The main idea is the following: if it holds that ∀A ⊆ V : s(A) ≤ F (A), then it will certainly be the case that\nlog ∑ A⊆V e−F (A) ≤ log ∑ A⊆V e−s(A) = ∑ i∈V log(1 + e−si).\nWe thus have a family of variational upper bounds on the partition function parametrized by the modular functions s, over which we can optimize to minimize the right hand side of the inequality. As shown by Djolonga & Krause (2014) this variational problem can be reduced to the following convex separable optimization problem over the base polytope\nminimize s∈B(F ) ∑ i∈V log(1 + exp(−si)). (4)\nThis problem – L-FIELD – can be then solved using the divide-and-conquer algorithm (Bach, 2013; Jegelka et al., 2013) by solving at most O(min{|V |, log 1 }) MAP problems, where is the tolerated error on the marginals. It can be also approximately solved using the Frank-Wolfe algorithm at a convergence rate of O(1/k). While these results establish tractability of the variational approach, in general solving even one MAP problem requires submodular minimization – an expensive task, and repeated solution may be too costly. Convergence of the Frank-Wolfe method is empirically slow."
    }, {
      "heading" : "4. L-FIELD ≡Minimum norm point.",
      "text" : "Our first main contribution is the following, perhaps surprising, result:\nTheorem 2. Problems (4) and (3) have the same solution.\nThe proof of this theorem (given in the appendix) crucially depends on the peculiar characteristics of the base polytope. Similar results have been shown (for other objectives) by Nagano & Aihara (2012). This theorem has three immediate, extremely important consequences. First, since the minimum-norm point approach is often the method of choice for submodular minimization anyway, this insight reduces the cost from solving many MAP problems to a single minimum norm point problem, which leads to substantial performance gains – a factor of O(|V |) if we seek the optimal variational solution! Secondly, given this equivalence and Theorem 1, we can immediately see that we can in fact extract the MAP solution by thresholding the marginals at 1/2.\nCorollary 1. We can extract the unique minimal and maximal MAP solutions by thresholding the optimal marginal vector at 1/2.\nThus, the L-FIELD approach results in the exact MAP solution in addition to approximate marginals and an upper bound on the partition function. Thirdly, since the minimum norm point problem is well studied, faster algorithms for important special cases become available. In particular, in §6, we demonstrate how certain types of logsupermodular distributions enable extremely efficient parallel inference."
    }, {
      "heading" : "5. The divergence minimization perspective",
      "text" : "The L-FIELD approach attacks the partition function directly. One can of course employ the factorized distribution parametrized by the minimizer s∗ of the upper bound to obtain approximate marginals. However, it is not immediately clear what properties the resulting distribution has, apart from agreeing on the mode (as shown by Corollary 1). To this end, we turn to the theory of divergence measures as that will enable us to understand the solutions preferred by the method. Divergence measures are functions D(P ‖Q) of two probability distributions P and Q that quantify the degree of dissimilarity between the arguments. Once we have picked a divergence measure D, we are interesting in minimizing D(P ‖Q) among some set of approximative distributions Q ∈ Q. The family which is of particular interest to us is that of completely factorized distributions that assign positive probabilities, which we now formally define.\nDefinition 1. We define the set Q of completely factorized positive distributions as\nQ = {Q | Q(S) ∝ ∏ i∈S exp(−qi) for some q ∈ RV }.\nThere are many choices for a divergence measure, the most prominent examples being the KL-divergence and the family of Rényi divergences (Rényi, 1961). Of particular interest for our analysis is the special infinite order of the Rényi divergence, defined as follows: Definition 2 (Van Erven & Harremoës (2012)). Define the Rényi divergence of infinite order between P (S) and Q(S)\nD∞(P ‖Q) = log sup S⊆V\nP (S) Q(S) . (5)\nIn the terminology of Minka et al. (2005) we can see that the D∞ divergence is inclusive, which means that it would try to “cover” as much as possible from the distribution: The variational approximation is conservative in the sense that it attempts to spread mass over all sets that achieve substantial mass under the true distribution. As we now show, it turns out that when we minimize this divergence for logsupermodular distributions we can focus our attention only on some specific factorized distributions. Lemma 1. When P is log-supermodular, to solve minimizeQ∈QD∞(P ‖Q) we have to only optimize over modular functions q that are global lower bounds of F .\nWhat this lemma essentially says, is that a minimizing distribution q∗ can be always found in P (F ). This result also implies the central result of this section, that the variational approach we have considered essentially minimizes the infinite divergence. Theorem 3. When P is log-supermodular, the problem minimizeQ∈QD∞(P ‖Q) is equivalent to problem (4).\nThis theorem has the following immediate consequence: Corollary 2. For log-supermodular models, problem minimizeQ∈QD∞(P ‖Q) is polynomial-time tractable via O(|V |) MAP (submodular minimization) problems.\nHence, any log-supermodular distribution has the property that we can find the closest factorized distribution to it w.r.t. this specific divergence in polynomial time, irrespective of whether the distribution factorizes into smaller factors or not. We would like to point out that the above criterion does not necessarily hold in general for non-logsupermodular distributions, which we formally show. Lemma 2. Lemma 1 does not hold for general point processes. Specifically, there exists a log-submodular counter example.\nThe proofs of all claims are provided in the supplemental material."
    }, {
      "heading" : "6. Parallel inference for decomposable models",
      "text" : "Very often the submodular functionF has structure that one can exploit to procure faster inference algorithms. In particular, the function often decomposes, i.e., can be written as a sum of (simpler) functions as\nF (S) = R∑ i=1 Fi(S ∩ Vi),\nwhere Fi : 2Vi → R are submodular functions with ground sets Vi. This setting has been considered, e.g., by Stobbe & Krause (2010) and Jegelka et al. (2013). The decomposition implies that the corresponding distribution factorizes as follows\nP (S) ∝ R∏ i=1 exp(−Fi(S ∩ Vi)). (6)\nIn fact, the examples we discussed in §2 both have this form, factorizing either into pairwise potentials or into the potentials defined by the superpixels. Such models can be naturally represented via a factor graph G that has as nodes the union of the ground sets Vi and the factors F1, . . . , FR. We then add edges E in a bipartite way by connecting each factor Fi to the elements Vi that participate in it (e.g. Fi is connected to v iff v ∈ Vi). For any node w in the graph (either a factor, or variable node), we will denote its neighbors by δ(w).\nWhen the function enjoys such a decomposition, the base polytope can be written as the Minkowski sum of the base polytopes of the summands, or formally 2\nB(F ) = R∑ i=1 B(Fi).\nHence, the minimum norm problem (3) that we are interested in can be rewritten as the following problem.\nminimize qi∈B(Fi) ∑ v∈V ( ∑\nFi∈δ(v)\nqi,v) 2.\nIn the following, we discuss two natural message passing algorithms exploiting this structure.\nExpectation propagation. A very natural approach would be to perform block coordinate descent one factor at a time. If we look through the lens of divergence measures, as introduced in §5, we can make a clear connection to (a variant of) expectation propagation3, the message passing approach of Minka et al. (2005) specialized\n2If v /∈ Vi, then the elements from B(Fi) are treated as having a zero for that coordinate.\n3Typically, expectation propagation is defined w.r.t. the KLdivergence.\nto minimizing the divergence D∞(P ‖Q), which we now briefly describe. The main idea is to approximate each factor exp(−Fi(S∩Vi)) with a completely factorized distributionQi(S) ∝ exp(−qi(S)), such that the product ∏R i=1Qi is a good approximation to the true distribution in terms of the given divergence. Then, we optimize iteratively using the following procedure.\n1. Pick a factor Fi.\n2. Replace the other factors Fj for j 6= i with their approximations Qj and minimize\nD∞( 1\nZi exp(−Fi(S)) ∏ j 6=i Qj ‖ R∏ j=1 Qj)\nover the factorized approximation Qi.\nIn other words, we choose a factor and minimize the infinite divergence for that factor, but instead of using the true factors exp(−Fj(S)) for j 6= i, we replace them with their current modular approximations Qj .\nA parallel approach. One downside of the approach presented above is that it has to be applied sequentially, i.e., one factor has to be updated at a time to ensure convergence. An alternative is to apply an approach used by Jegelka et al. (2013), which allows to perform message passing in parallel without losing the convergence guarantees. Jegelka et al. (2013) assume that all factors depend on all variables (i.e. Vi = V ). In the following, we generalize their setting in order to allow Vi 6= V . By changing the dual problem they consider (shown in detail in the appendix) we arrive at a form that is more natural to our setting and can be seen as performing message passing in the factor graph. To describe the messages, we have to define the following pair of norms that arise from the structure of the factor graph.\nDefinition 3. For any xS ∈ RS , where S ⊆ V , we define the following pair of norms.\n‖xS‖2G = ∑ v∈S 1 |δ(v)| x2v, and ‖xS‖2G∗ = ∑ v∈S |δ(v)|x2v.\nThe messages from variables to factors are simple sums, similar to those in standard belief propagation\nµt+1v→Fi = 1 |δ(v)| ∑\nFj∈δ(v)\nµtFj→v.\nThe factors always keep some vector on their base polyhedron, which at iteration t will be denoted by qti ∈ B(Fi). Then, based on the incoming messages, they update this vector by solving a convex problem, which is much cheaper\nthan the exhaustive computation one has to do for belief propagation (which is exponential in the factor size). We will denote the message sent from node u to node w at iteration t by µtu→w. If m t i ∈ RVi is the vector of messages received at iteration t at node Fi (one message from each v ∈ Vi), then the factor solves a projection problem parametrized by (mti,x t i), whose solution is assigned to xt+1i . Written formally, we have\nqt+1i = argmin qi∈B(Fi) ‖qi − (qti −mti)‖2G∗.\nAs this is a convex separable problem on the base polytope, it can be solved for example using the divide-and-conquer algorithm (Bach, 2013). Having solved this problem, the factor sends the following messages to its neighbours\nµt+1Fi→v = q t+1 v .\nStated differently, it will send to every variable node v the coordinate of the stored vector corresponding to that variable. At every iteration t we can extract the current factorized approximation to the full distribution by simply considering the incoming messages at the variable nodes. Specifically, the approximation qt at time step has in the v-th coordinate the sum of incoming messages at the node v, or formally\nqtv = ∑\nFi∈δ(Fj)\nµtFi→v.\nBecause the algorithm can be seen as performing block coordinate descent on a specific problem (discussed in the appendix), the message passing algorithm described above possesses strong convergence guarantees that depend on the structure of the factor graph. These guarantees even hold if all messages from nodes to factors, and all messages from factors to nodes are each computed in parallel. An important quantity that appears in the convergence rate is the maximal variable connectivity ∆V = maxv∈V |δ(v)|. Based on recent new results by Nishihara et al. (2014) on block coordinate descent for a similar dual (assuming that all factors depend on all variables, as considered by Jegelka et al. (2013)), we extend their analysis to obtain a linear convergence rate for our message passing scheme.\nTheorem 4 (Extension of Nishihara et al. (2014)). If the graph is ∆V -regular, s.t. every variable appears in exactly ∆V factors, then the message passing algorithm converges linearly with rate (1− 1|V |∆V ) 2 . More specifically\n‖qt − q∗‖ ≤ 2‖q0 − q∗‖∞ √ ∆V E(1− 1\n|V |2∆2V )t,\nwhere q∗ is the optimal point, q0 is the initial point and E is the number of edges in the factor graph."
    }, {
      "heading" : "7. Experiments",
      "text" : "We now report experimental results on applying our parallel variational inference scheme to a challenging image segmentation problem as motivated in §2. The goal of our experiments is to test the scalability of our approach to large problems, and to evaluate the quality of the marginals both qualitatively and quantitatively. We used the data from Jegelka & Bilmes (2011), which contains a total of 36 images, each with a highly detailed (pixel-level precision) ground truth segmentation. Due to intractability, we cannot compute the exact marginals against which we would ideally wish to compare. As a proxy for measuring the quality of the approximations, we use the area under the ROC curve (AUC) as compared to the ground truth segmentation. We classify each pixel independently as fore- or background by comparing its approximate marginal against a threshold, which we vary to obtain the ROC curve. We have used the following model, which contains both pairwise and higher-order interactions.\nF (A) = αm(A) + βFcut(A) + γ ∑ Pi∈P φ ( |A ∩ Pi| |Pi| ) ,\nwhere\n• the unary potentials m(·) were learned from labeled data using a 5 component GMM;\n• the pairwise potentials Fcut connect neighboring pixels x and x′ with weights w(x,x′) = exp(−θ‖x − x‖2), where x and x′ are the RGB values of the pixels;\n• the higher order potentials were generated using the mean-shift algorithm of Comaniciu & Meer (2002). We have used two overlapping layers of superpixels, each layer with different granularity. The concave function was defined as φ(z) = z(1− z).\nWe compared the following inference techniques. The reported typical running times are for an image of size 427x640 pixels on a quad core machine and we report the wall clock time of the inference code (without setting up the factor graph or generating the superpixels).\n• Unary potentials only with independent predictions, i.e., β = γ = 0.\n• Belief propagation (BP), mean-field (MF) and fractional belief propagation (FBP) for the pairwise model (i.e. γ = 0). We have used the implementation from libDAI (Mooij, 2010). The maximum number of iterations was set to 30. We note that this code is not parallelized. When we observe fast convergence, for example BP can converge in 3 iterations, it takes\nabout 45 seconds. Even though we have set a relatively low number of iterations, the running times can be extremely slow if the methods do not converge. For example, running mean-field for 30 iterations can take more than 3 minutes.\n• Our approach using only pairwise potentials (γ = 0), solved using the total variation Douglas-Rachford (DR) code from (Barbero & Sra, 2011; 2014; Jegelka et al., 2013). We ran for at most 100 iterations. The inference takes typically less than a second.\n• Our approach with higher order potentials (HOP) only (β = 0). The inference takes less than 12 seconds.\nFor every method we tested several variants using different combinations for α, β, γ and θ (exact numbers provided in the appendix). Then, we performed a leave-one-out crossvalidation for estimating the average AUC. We have also generated a sequence of 10 trimaps by growing the boundary around the true foreground to estimate accuracy over the hardest pixels, namely those at the boundary.\nAccuracy. We first wish to quantitatively compare the accuracy of the approximate marginals. We report the aggregate results in Figure 3, and the ROC curves in Figure 4.\nWe can clearly see that our approach outperforms the traditional inference methods for both objectives — the AUC over the whole image and over the challenging boundary (trimaps). Sometimes we see very poor behavior of the alternative methods, which can be attributed to either their over-confidence (as verified below), or the fact that they optimize non-convex objectives and can fail to converge within the given number of iterations. Lastly, capturing high-order interactions leads to higher accuracy (in particular around the boundary) than pairwise potentials only.\nProperties of marginals. We would also like to understand the qualitative characteristics of the resulting marginals of our methods when compared with the traditional techniques. From the discussion on the divergence minimization in §5, we would expect the approximate marginals to avoid assigning low probabilities and rather prefer to err conservatively, i.e., on the side of causing false positives. On the other hand, it is known that the results of belief propagation are often over-confident. For this purpose, we provide a visual comparison in Figure 2. Namely, each of the four FBP/DR pairs are results using the respective algorithms for the same parameters of the model. We observe exactly what the theory predicts — the distribution obtained via L-FIELD is less concentrated around the object and mass is spread around more. The contrast is starkest on Figures 2 (b) and (h), where we use a very strong pairwise prior (high β). On Figures 2 (e) and (k) we have used a very weak pairwise prior (low β), and as expected the resulting marginals are mainly determined by the unary part and the choice of inference procedure does not make a difference. The results in the last column are from the higher order model, with two different values of γ (the strength of the higher order potential). We can see that the resulting probabilities better preserve the boundaries of the object and the fine details, which is one of the main benefits of using these models."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We have addressed the problem of variational inference in log-supermodular distributions. In particular, building on the L-FIELD approach of Djolonga & Krause (2014), we established two natural, important interpretations of their method. First, we showed how L-FIELD can be reduced to solving the well-studied minimum norm point problem, making a wealth of tools from submodular optimization suddenly available for approximate Bayesian inference. Secondly, we showed that the factorized distributions returned by L-FIELD minimize a particular type of information divergence. Both of these theoretical connections are immediately algorithmically useful. In particular, for the common case of decomposable models, both connections lead to efficient message passing algorithms. Exploiting the minimum norm connection, we proved strong convergence rates for a natural parallel approach, with convergence rates dependent on the factor graph structure. Lastly, we demonstrate our approach on a challenging image segmentation task. Our results demonstrate the accuracy of our marginals (in terms of AUC score) compared to those produced by classical techniques like belief propagation, mean field and variants, on models where these can be applied. We also show that performance can be further improved by moving to high-order potentials, leading to models where classical marginal inference techniques become intractable. We believe our results provide an important step towards practical, efficient inference in models with complex, high-order variable interactions."
    }, {
      "heading" : "A. Theory",
      "text" : "A.1. Equivalence between the minimum norm and inference problems\nWe will show a stronger result from which Theorem 2 follows by taking wi = 1 and yi = 0. Lemma 3. For positive weights wi > 0 the objectives ∑ i∈V wi(xi− yi)2 and ∑ i∈V 1 wi\nlog(exp(−wixi) + exp(−wiyi)) have the same optimum under the constraint x ∈ B(F ).\nProof. For the second objective we have that∑ i∈V 1 wi log(exp(−wixi) + exp(−wiyi)) z=−x = ∑ i∈V 1 wi log(exp(wizi) + exp(−wiyi)).\nHence, the optimum of the problem is the negative of the problem on−B(F ), which is the base polytope of the submodular function F (A) = F (V − A) − F (V ). The gradient of this objective with respect to any zi is equal to σ(wi(zi + yi)), where σ(u) = 1/(1 + e−u) is the sigmoid function. As the weights wi are positive, we have that\nσ(wi(zi + yi)) ≤ σ(wj(zj + yj)) ⇐⇒ wi(zi + yi) ≤ wj(zj + yj). From Fujishige (2005)[Theorem 8.1], it follows that the above problem and ∑ i wi(zi + yi) 2 have the same solution z∗ on −B(F ) = B(F ). However, if z∗ is the projection of −y onto −B(F ), then x = −z∗ is the projection of y onto B(F ).\nA.2. Connection to the infinite Rényi divergences\nIf we expand the Rényi infinite divergence for P (S) = 1Zp exp(−F (S)) and Q(S) = 1 Zq exp(−q(S)) for modular q(·) we get the following\nD∞(P ‖Q) = log sup A⊆V\nP (A) Q(A) = log sup\nA⊆V exp(−F (A))/Zp exp(−q(A))/Zq = logZq − logZp + sup A⊆V {q(A)− F (A)}. (7)\nAs we will consider the problem of minimizing the above quantity with respect to the modular function q we can ignore the constant logZp. We will also expand the log-partition function of q, logZq = ∑ i∈V log(1 + exp(−qi)) and introduce a new variable t capturing the supremum above to arrive at the following formulation.\nminimize ∑ i∈V log(1 + exp(−qi)) + t\nsubject to q(A) ≤ F (A) + t for all A ⊆ V (8)\nDefinition 4. For any normalized submodular function F , define L∗(F ) to be the optimum value of minimizing∑ i∈V log(1 + exp(−si)) subject to s ∈ B(F ).\nTo show the connection, we will need the following two lemmas.\nLemma 4 (Djolonga & Krause (2014)). By strong Fenchel duality we have that\nL∗(F ) = min s∈B(F ) ∑ i∈V log(1 + exp(−si)) = sup p∈[0,1]V H[p]− f(p),\nwhere f is the Lovász extension of F and H[p] is the entropy of a random vector of independent Bernoulli random variables.\nThe following lemma is already known, as such functions have been already used, but we prove it for completeness.\nLemma 5. For any normalized submodular function F : 2V → R define Fβ : 2V → R as follows.\nFβ(S) = { 0 if S = ∅ F (S) + β if S 6= ∅\nThen, Fβ is submodular, normalized and has Lovász extension fβ(w) = f(w) + βmaxi wi.\nProof. To show that F is submodular we need to show that (see e.g. (Bach, 2013)[Prop. 2.3]) for anyA ⊆ V and any j, k ∈ V −A we have that Fβ(A∪{k})−Fβ(A) ≥ Fβ(A∪{j, k})−Fβ(A∪{j}). If A 6= ∅, then the above inequality follows immediately from the submodularity of F . Otherwise, we have the inequality F ({k}) +β−F (∅) ≥ F ({j, k})−F ({j}), which again easily follows from the submodularity of F and the fact that β ≥ 0. The Lovász extension of Fβ is defined as\nfβ(w) = |V |∑ k=1 wjk ( Fβ({j1, . . . , jk})− Fβ({j1, . . . , jk−1}) ) ,\nwhere the indices are chosen so that wj1 ≥ wj2 ≥ . . . ≥ wj|V | . Then, every term in the sum will be the same if we replace Fβ with F (because β will be added and subtracted), except for the first term when k = 1. This term is equal to wj1(F ({j1}) + β − F (∅)), and the result follows immediately as wj1 = maxi wi.\nLemma 6. For submodular functions Problem (8) reaches the minimum for t = 0.\nProof. Define OPTβ to be the optimum value of problem (8) for t = β. Note that OPTβ = L∗(Fβ) + β. Then\nOPT0 Lem.4 = H[p]− f(p) ≤ H[p]− f(p) + β (1−max i pi)︸ ︷︷ ︸\n≥0\nLem.5 = H[p]− fβ(p) + β Lem.4 ≤ L∗(fβ) + β = OPTβ .\nAnd this immediately implies Lemma 1 and Theorem 3. We will now prove Lemma 2 by giving a specific counterexample.\nLemma 7. There is a supermodular function for which the minimum is achieved for some t > 0.\nProof. For t = 0, the Lagrange dual problem of Problem (8) is maximize− ∑ A⊆V F (A)λA + ∑ i∈V h( ∑ A3i λA)\nsubject to 0 ≤ ∑ A3i λA ≤ 1 for all i ∈ V, (9)\nwhere h(p) = −p log p − (1 − p) log(1 − p) is the binary entropy function (defined so that h(0) = h(1) = 0). The Lagrange dual is easily derived if we note that −h(−u) is the covex conjugate of the primal objective and use (Boyd & Vandenberghe, 2004)[§5.1.6]. Consider the function\nF (∅) = 0, F ({1}) = −20, F ({2}) = −8, F ({1, 2}) = −16,\nwhich is supermodular as\nF (1 | {2}) = −16− (−8) = −8 > −20 = F ({1}), and F (2 | {1}) = −16− (−20) = 4 > 8 = F ({2}).\nFor t = 1 consider the primal feasible variable x = (−19,−7), which has an objective value < 27.1. For t = 0 take the dual variable λ∅ = λV = 0, λ1 = 1 = λ2 = 1, with a value of −F (1)λ1 − F (0)λ0 + 0 = 28 > 27.1 and thus a strictly better value is achieved for t = 1."
    }, {
      "heading" : "B. Proofs for Section 6",
      "text" : "We will first define a dual formulation of the minimization problem, for which the claimed message passing scheme does BCD. We will work with the set of all valid Lagrange multipliers Λ = {λ ∈ R ∑R i=1 |Vi| : (∀v ∈ V ) ∑ i∈δ(v) λv,i = 0} and the product of base polytopes B = ⊗Ri=1B(Fi). For any element y of either of these sets we will denote by yi,v the coordinate corresponding to variable v ∈ Vi of the i-th block (1 ≤ i ≤ R). Moreover, for any vector x ∈ RV we will denote by x|S its restriction to the coordinates S ⊆ V .\nTheorem 5 (Following (Jegelka et al., 2013)). The dual problem of\nminimizexf(x) + 1\n2 ‖x‖2 = minimizex R∑ i=1 (f(x|Vi) + 1 2 ‖x|Vi‖2G) (10)\nis equal to\nmaximize λ∈Λ,yi∈B(Fi) R∑ i=1 −1 2 ‖yi − λi‖2G∗, (11)\nProof. The proof is based on that of (Jegelka et al., 2013)[Lemma 1]. The considered problem is the following.\nmin x f(x) +\n1 2 ‖x|22 = min x R∑ i=1 (fi(xVi) + 1 2 ‖xVi‖2G) s.t. xVi = x|Vi\n= min x,xVi max λi R∑ i=1 (fi(xVi) + 1 2 ‖xVi‖2G − λTi (xVi − x|Vi)).\nBecause we have zero duality gap, we can change the order of optimization. Then, if we optimize for x, we see that the Lagrange multipliers have to belong to Λ, which was defined above. Hence, we have the following problem\nmax λ∈Λ ∑ i min xVi max yi∈B(Fi) (xTViyi + 1 2 ‖xVi‖2G − λTi xVi) = max λ∈Λ ∑ i max yi∈B(Fi) min xVi (xTViyi + 1 2 ‖xVi‖2G − λTi xVi).\nConsider the inner problem, i.e.\nminimizexVi x T Viyi +\n1 2 ‖xVi‖2G − λTi xVi = minimizexVi x T Vi(yi − λi) + 1 2 ‖xVi‖2G,\nwhich is exactly the negative of the convex conjugate of 12‖·‖ 2 G evaluated at λi−yi. Because the convex conjugate is equal to 12‖ · ‖ 2 G∗ (see e.g. (Boyd & Vandenberghe, 2004)[Ex. 3.22]), the above minimum is equal to − 12‖yi − λi‖ 2 G∗, which we had to show.\nWe can now easily see that the message passing algorithm is doing BCD for the dual — each node Fi is first projecting to Λ by subtracting the component-wise mean, and then clearly projecting ontoB(Fi) under the norm defined in Definition 3.\nWe will now show the linear convergence rate. Because we consider the k-regular case, the primal can be written in the following simpler form\nminimize x r∑ i=1 f(x|Vi) + 1 2k ‖x|Vi‖2,\nand the decomposed dual becomes the problem of finding the closest points between Λ and B\nmaximize λ∈Λ,yi∈B(Fi) ∑ i −k 2 ‖yi − λi‖2. (12)\nWe now use exactly the same argument as in (Nishihara et al., 2014)[§3.3] with some small changes necessary to accommodate our different definitions of Λ and B. Please refer to that paper and references therein for the terminology used in the remaining of the proof. We will show that the Friedrich’s angle between any two faces of B and Λ is at most 2k2|V |2 , which combined with (Nishihara et al., 2014)[Thm. 2 and Cor. 5] implies the theorem. To make the notation easier to parse, we will assume that the elements in B and Λ are ordered so that first come the |V1| elements corresponding to F1, then the |V2| elements corresponding to F2 and so forth. Under this ordering, the vector space Λ can be written as the nullspace of the following matrix\nS = 1√ k\n( S1︸︷︷︸\n∈RV×V1 . . . SR︸︷︷︸ ∈RV×VR\n) ,where [Vi]v,v′ = [v = v′]. (13)\nAs noted in (Nishihara et al., 2014), using (Bach, 2013)[Prop. 4.7] we can express the affine hull aff0(Bz) of any face Bz as (where for each i ∈ {1, . . . , R} the sets Ar,1, . . . , Ar,Mr form a partition of Vi)\naff0(Bz) = R⋂ r=1 Mr⋂ m=1 {(y1, . . . ,yR) : yr(Ar,1 ∪ . . . ∪Ar,m) = 0},where yi ∈ B(Fi).\nThis set can be also written as the nullspace of the following matrix\nT =  1TA1,1√ |A1,1| 1TA1,2√ |A1,2| ... 1TA1,M1√ |A1,M1 | . . . 1TA1,1√ |AR,1| 1TA1,2√ |AR,2|\n... 1TA1,MR√ |AR,MR |\n .\nTo compute the Friedrich’s angle we are interested in the singular values of STT (Nishihara et al., 2014)[Lemma 6], which is equal to\nSTT = 1√ k ( 1A1,1√ |A1,1| , . . . , 11,M1√ |A1,M1 | , . . . 1AR,1√ |AR,1| , . . . , 1R,MR√ |AR,MR | ) .\nHence, we have to analyze the eigenvalues of the square matrix (STT )T (STT ), whose rows and columns are indexed by I = {(r,m) : r ∈ [R],m ∈ [Mr]}, and whose ((ri,mi), (rj ,mj)) entry is\n1\nk |Ari,mi ∩Arj ,mj |√ |Ari,mi ||Arj ,mj | .\nWe create a graph with vertices I and we add edges between distinct (ri,mi) and (rj ,mj) with weight |Ari,mi ∩Arj ,mj | (zero weight means that we do not add that edge). The normalized graph Laplacian of this graph is equal to\n[L](ri,mi),(rj ,mj) = 1 if (ri,mi) = (rj ,mj)− 1k−1 |Ari,mi∩Arj,mj |√|Ari,mi ||Arj,mj | otherwise. Hence, (STT )T (STT ) = I− k−1k L. We want to lower-bound the Cheeger constant h of this graph. Because the spectrum of a graph is the union of the spectra of the connected components we will assume that the graph is connected, as we can apply the same argument to every component. From the definition of h we have that\nh ≥ 2minimum cut volume ,where volume = ∑ v∈V |δ(v)|(|δ(v)| − 1) = |V |(k2 − k).\nWhat remains is to bound the minimum cut from below. Because the graph is connected, for any cut U there must exist some v that is in sets on both sides of the cut. Let m be the number of sets in U that contain it, and let k−m be the number of sets in the complement that contain it. Then, the cut is of size is at least m(k −m) ≥ k − 1. Hence\nh ≥ 2 k − 1 |V |(k2 − k) = 2 |V |k .\nAnd by Cheeger’s inequality the smallest positive eigenvalue λ2 of the Laplacian L is at least 2k2|V |2 , which from the relationship (STT )T (STT ) = I − k−1k L implies that the squared Friedrich’s angle c 2 F between Λ and B is at most\nc2F ≤ cF = 1− k − 1 k λ2 ≤ 1− 1 |V |2k2 ,\nwhich completes the proof."
    }, {
      "heading" : "C. Experiments",
      "text" : "We have used the parameter values in the table below.\nParameter Values\nθ 0.1, 0.001, 0.0001 α 1, 0.1, 0.01, 0.001 β 10, 1, 0.1, 0.01, 0.001 γ 10, 1, 0.1, 0.01, 0.001"
    } ],
    "references" : [ {
      "title" : "Learning with submodular functions: a con",
      "author" : [ "Bach", "Francis" ],
      "venue" : null,
      "citeRegEx" : "Bach and Francis.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bach and Francis.",
      "year" : 2010
    }, {
      "title" : "Modular proximal optimization for multidimensional total-variation regularization",
      "author" : [ "Barbero", "Álvaro", "Sra", "Suvrit" ],
      "venue" : null,
      "citeRegEx" : "Barbero et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Barbero et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast Newton-type methods for total variation regularization",
      "author" : [ "Barbero", "lvaro", "Sra", "Suvrit" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Barbero et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Barbero et al\\.",
      "year" : 2011
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Boyd", "Stephen P", "Vandenberghe", "Lieven" ],
      "venue" : null,
      "citeRegEx" : "Boyd et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2004
    }, {
      "title" : "Greedy dictionary selection for sparse representation",
      "author" : [ "Cevher", "Volkan", "Krause", "Andreas" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing,",
      "citeRegEx" : "Cevher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cevher et al\\.",
      "year" : 2011
    }, {
      "title" : "Provable submodular minimization using Wolfe’s algorithm",
      "author" : [ "Chakrabarty", "Deeparnab", "Jain", "Prateek", "Kothari", "Pravesh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chakrabarty et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chakrabarty et al\\.",
      "year" : 2014
    }, {
      "title" : "Mean shift: A robust approach toward feature space analysis",
      "author" : [ "Comaniciu", "Dorin", "Meer", "Peter" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Comaniciu et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Comaniciu et al\\.",
      "year" : 2002
    }, {
      "title" : "From MAP to marginals: Variational inference in Bayesian submodular models",
      "author" : [ "Djolonga", "Josip", "Krause", "Andreas" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Djolonga et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Djolonga et al\\.",
      "year" : 2014
    }, {
      "title" : "Submodular functions, matroids, and certain polyhedra",
      "author" : [ "Edmonds", "Jack" ],
      "venue" : "Combinatorial structures and their applications,",
      "citeRegEx" : "Edmonds and Jack.,? \\Q1970\\E",
      "shortCiteRegEx" : "Edmonds and Jack.",
      "year" : 1970
    }, {
      "title" : "Lexicographically optimal base of a polymatroid with respect to a weight vector",
      "author" : [ "Fujishige", "Satoru" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Fujishige and Satoru.,? \\Q1980\\E",
      "shortCiteRegEx" : "Fujishige and Satoru.",
      "year" : 1980
    }, {
      "title" : "Submodular functions and optimization, volume",
      "author" : [ "Fujishige", "Satoru" ],
      "venue" : "Annals of Discrete Mathematics",
      "citeRegEx" : "Fujishige and Satoru.,? \\Q2005\\E",
      "shortCiteRegEx" : "Fujishige and Satoru.",
      "year" : 2005
    }, {
      "title" : "The complexity of ferromagnetic ising with local fields",
      "author" : [ "Goldberg", "Leslie Ann", "Jerrum", "Mark" ],
      "venue" : "Combinatorics, Probability and Computing,",
      "citeRegEx" : "Goldberg et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Goldberg et al\\.",
      "year" : 2007
    }, {
      "title" : "Submodularity beyond submodular energies: coupling edges in graph cuts",
      "author" : [ "Jegelka", "Stefanie", "Bilmes", "Jeff" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Jegelka et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jegelka et al\\.",
      "year" : 2011
    }, {
      "title" : "Reflection methods for user-friendly submodular optimization",
      "author" : [ "Jegelka", "Stefanie", "Bach", "Francis", "Sra", "Suvrit" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jegelka et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jegelka et al\\.",
      "year" : 2013
    }, {
      "title" : "Polynomial-time approximation algorithms for the ising model",
      "author" : [ "Jerrum", "Mark", "Sinclair", "Alistair" ],
      "venue" : "SIAM Journal on computing,",
      "citeRegEx" : "Jerrum et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Jerrum et al\\.",
      "year" : 1993
    }, {
      "title" : "Robust higher order potentials for enforcing label consistency",
      "author" : [ "Kohli", "Pushmeet", "Ladický", "L’ubor", "Torr", "Philip H.S" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Kohli et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kohli et al\\.",
      "year" : 2009
    }, {
      "title" : "Near-optimal nonmyopic value of information in graphical models",
      "author" : [ "Krause", "Andreas", "Guestrin", "Carlos" ],
      "venue" : "In Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Krause et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2005
    }, {
      "title" : "Determinantal point processes for machine learning",
      "author" : [ "A. Kulesza", "B. Taskar" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Kulesza and Taskar,? \\Q2012\\E",
      "shortCiteRegEx" : "Kulesza and Taskar",
      "year" : 2012
    }, {
      "title" : "Divergence measures and message passing",
      "author" : [ "Minka", "Tom" ],
      "venue" : "Technical report, Technical report, Microsoft Research,",
      "citeRegEx" : "Minka and Tom,? \\Q2005\\E",
      "shortCiteRegEx" : "Minka and Tom",
      "year" : 2005
    }, {
      "title" : "libDAI: A free and open source C++ library for discrete approximate inference in graphical models",
      "author" : [ "Mooij", "Joris M" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Mooij and M.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mooij and M.",
      "year" : 2010
    }, {
      "title" : "Equivalence of convex minimization problems over base polytopes",
      "author" : [ "Nagano", "Kiyohito", "Aihara", "Kazuyuki" ],
      "venue" : "Japan journal of industrial and applied mathematics,",
      "citeRegEx" : "Nagano et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nagano et al\\.",
      "year" : 2012
    }, {
      "title" : "On the convergence rate of decomposable submodular function minimization",
      "author" : [ "Nishihara", "Robert", "Jegelka", "Stefanie", "Jordan", "Michael I" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Nishihara et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Nishihara et al\\.",
      "year" : 2014
    }, {
      "title" : "A faster strongly polynomial time algorithm for submodular function minimization",
      "author" : [ "Orlin", "James B" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Orlin and B.,? \\Q2009\\E",
      "shortCiteRegEx" : "Orlin and B.",
      "year" : 2009
    }, {
      "title" : "Fusion, propagation, and structuring in belief networks",
      "author" : [ "Pearl", "Judea" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Pearl and Judea.,? \\Q1986\\E",
      "shortCiteRegEx" : "Pearl and Judea.",
      "year" : 1986
    }, {
      "title" : "On measures of entropy and information",
      "author" : [ "Rényi", "Alfréd" ],
      "venue" : "In Fourth Berkeley symposium on mathematical statistics and probability,",
      "citeRegEx" : "Rényi and Alfréd.,? \\Q1961\\E",
      "shortCiteRegEx" : "Rényi and Alfréd.",
      "year" : 1961
    }, {
      "title" : "Efficient minimization of decomposable submodular functions",
      "author" : [ "Stobbe", "Peter", "Krause", "Andreas" ],
      "venue" : "In Proc. Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Stobbe et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Stobbe et al\\.",
      "year" : 2010
    }, {
      "title" : "Rényi divergence and Kullback-Leibler divergence",
      "author" : [ "Van Erven", "Tim", "Harremoës", "Peter" ],
      "venue" : "arXiv preprint arXiv:1206.2459,",
      "citeRegEx" : "Erven et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Erven et al\\.",
      "year" : 2012
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "Wainwright", "Martin J", "Jordan", "Michael I" ],
      "venue" : "Found. Trends Mach. Learn.,",
      "citeRegEx" : "Wainwright et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright et al\\.",
      "year" : 2008
    }, {
      "title" : "To compute the Friedrich’s angle we are interested in the singular values of ST (Nishihara et al., 2014)[Lemma",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : ".,? \\Q2014\\E",
      "shortCiteRegEx" : ".",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "One strategy to generate the regions, used by Kohli et al. (2009) is to generate superpixels, as illustrated on Figure 1.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "An algorithm that performs better in practice, but only has a pseudopolynomial running time guarantee (Chakrabarty et al., 2014), is the Fujishige-Wolfe algorithm (Fujishige, 1980).",
      "startOffset" : 102,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "This problem – L-FIELD – can be then solved using the divide-and-conquer algorithm (Bach, 2013; Jegelka et al., 2013) by solving at most O(min{|V |, log 1 }) MAP problems, where is the tolerated error on the marginals.",
      "startOffset" : 83,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : ", by Stobbe & Krause (2010) and Jegelka et al. (2013). The decomposition implies that the corresponding distribution factorizes as follows",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "An alternative is to apply an approach used by Jegelka et al. (2013), which allows to perform message passing in parallel without losing the convergence guarantees.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "An alternative is to apply an approach used by Jegelka et al. (2013), which allows to perform message passing in parallel without losing the convergence guarantees. Jegelka et al. (2013) assume that all factors depend on all variables (i.",
      "startOffset" : 47,
      "endOffset" : 187
    }, {
      "referenceID" : 19,
      "context" : "Based on recent new results by Nishihara et al. (2014) on block coordinate descent for a similar dual (assuming that all factors depend on all variables, as considered by Jegelka et al.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "(2014) on block coordinate descent for a similar dual (assuming that all factors depend on all variables, as considered by Jegelka et al. (2013)), we extend their analysis to obtain a linear convergence rate for our message passing scheme.",
      "startOffset" : 123,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "(2014) on block coordinate descent for a similar dual (assuming that all factors depend on all variables, as considered by Jegelka et al. (2013)), we extend their analysis to obtain a linear convergence rate for our message passing scheme. Theorem 4 (Extension of Nishihara et al. (2014)).",
      "startOffset" : 123,
      "endOffset" : 288
    }, {
      "referenceID" : 13,
      "context" : "• Our approach using only pairwise potentials (γ = 0), solved using the total variation Douglas-Rachford (DR) code from (Barbero & Sra, 2011; 2014; Jegelka et al., 2013).",
      "startOffset" : 120,
      "endOffset" : 169
    } ],
    "year" : 2015,
    "abstractText" : "We consider the problem of approximate Bayesian inference in log-supermodular models. These models encompass regular pairwise MRFs with binary variables, but allow to capture highorder interactions, which are intractable for existing approximate inference techniques such as belief propagation, mean field, and variants. We show that a recently proposed variational approach to inference in log-supermodular models –L-FIELD– reduces to the widely-studied minimum norm problem for submodular minimization. This insight allows to leverage powerful existing tools, and hence to solve the variational problem orders of magnitude more efficiently than previously possible. We then provide another natural interpretation of L-FIELD, demonstrating that it exactly minimizes a specific type of Rényi divergence measure. This insight sheds light on the nature of the variational approximations produced by L-FIELD. Furthermore, we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate, without having to sum up over all the configurations of the factor. Finally, we apply our approach to a challenging image segmentation task. Our experiments confirm scalability of our approach, high quality of the marginals, and the benefit of incorporating higher-order potentials.",
    "creator" : "LaTeX with hyperref package"
  }
}