{
  "name" : "1202.3639.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Finding a most biased coin with fewest flips",
    "authors" : [ "Karthekeyan Chandrasekaran", "Richard Karp" ],
    "emails" : [ "karthe@seas.harvard.edu,", "karp@icsi.berkeley.edu," ],
    "sections" : [ {
      "heading" : null,
      "text" : "whose posterior probability of being most biased is at least 1 − δ for a given δ. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategy – a strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs tools from the field of Markov games."
    }, {
      "heading" : "1 Introduction",
      "text" : "The multi-armed bandit problem is a classical decision-theoretic problem with applications in bioinformatics, medical trials, stochastic algorithms, etc. [18]. The input to the problem is a set of arms, each associated with an unknown stochastic reward. At each step, an agent chooses an arm and receives a reward. The objective is to find a strategy for choosing the arms in order to achieve the best expected reward asymptotically. This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].\nThe motivation to identify the best bandit arm arises from problems where one would like to minimize regret within a fixed budget. In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret. Here regret is defined to be the difference between the expected reward of the chosen arm and the expected reward of the optimal arm. The work of [8] suggested that the exploration-exploitation trade offs for this setting are much different from the setting where the number of steps is asymptotic. Following this, Audibert et al. [1] proposed exploration strategies to perform essentially as well as the best strategy that knows all distributions up to permutations of the arms. Gabillon et al. [17] addressed the problem of identifying the best arm for each bandit among a collection of bandits within a fixed budget. They proposed strategies that focus on arms whose expected rewards are closer to that of the optimal arm and show an upper bound on the probability of error for these strategies that decreases exponentially with the number of steps allowed.\n∗karthe@seas.harvard.edu, Harvard University. This work was done while the author was a visiting researcher at ICSI, Berkeley. †karp@icsi.berkeley.edu, University of California, Berkeley.\nar X\niv :1\n20 2.\n36 39\nv3 [\ncs .D\nS] 7\nS ep\n2 01\n3\nIn contrast, one could also attempt to optimize the budget subject to the quality of the arm to be identified. This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem – given any δ > 0, identify the arm with maximum expected reward with error probability at most δ while minimizing the total number of steps needed. This PAC-style learning formulation was introduced by Even-Dar et al. [14]. Given a collection of n arms, Even-Dar et al. [14] showed that a total of O((n/ 2) log(1/δ)) steps is sufficient to identify an arm whose expected reward is at most away from the optimal arm with correctness at least 1 − δ. Mannor and Tsitsiklis [22] showed lower bounds matching up to constant factors under various settings of the rewards. We attempt to bridge the constant factor gap by addressing the problem from a decision-theoretic perspective. Given the history of outcomes, does there exist a strategy to choose an arm so that the expected number of steps needed to learn the best arm is minimized? Our notion of learning the best arm is to identify an arm whose posterior probability of being the most-rewarding is at least 1− δ.\nAlthough the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the “ranking and selection problem”. It was introduced for normally distributed rewards by Bechhofer [4]. Adaptive strategies for this problem, known as “sequential selection”, can be traced back to Paulson [24]. Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25]. A simple and interesting case of the problem is when the most rewarding arm and the second-most rewarding arm differ in their mean rewards by at least > 0. This special case is known as the “indifference-zone” assumption [4]. Strategies and their measure of optimality are known for various relaxations of independence, normality, equal and known variances and indifference-zone assumptions [20]. In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9]. In this work, we address a particular Bayesian setting for Bernoulli rewards satisfying the indifference-zone assumption.\nIf the rewards from the bandit arms are Bernoulli, then learning the arm with the maximum expected reward is equivalent to learning the most biased coin by tossing them adaptively. So, we will focus on this problem for the rest of the paper. Under the indifference zone assumption, Chernoff bound leads to a trivial upper bound on the number of tosses in the non-adaptive setting – toss each coin (4/ 2) log (n/δ) times and output the coin with the maximum number of heads outcomes. Let p̂i denote the empirical probability of heads for the ith coin. By Chernoff bound, |p̂i− pi| ≤ /2 with probability at least 1− δ/n. Therefore, by the union bound, it follows that this trivial toss-each-coin-k-times strategy outputs the most biased coin with probability at least 1− δ.\nIn this work, we give a simple yet optimal strategy for choosing coins to toss in a particular Bayesian setting. Our strategy is optimal in the sense that given a current history of outcomes of all coins and a threshold, it minimizes the expected number of tosses to find a coin whose posterior probability of being a most-biased coin is at or above the threshold. Our main contribution is a proof of optimality by employing tools from the field of Markov games. We also bound the expected number of coin tosses performed by our strategy. To the best of our knowledge, this is the first provably optimal strategy under a Bayesian setting of the problem with indifference zone assumption.\nSetting. A coin is said to be heavy if the probability of heads for the coin is p+ and not-heavy if the heads probability is p− for some given ∈ (0, 1/2) and p ∈ [ , 1− ]. We are given an infinite\ncollection of coins where each coin in the collection is heavy with probability α and not-heavy with probability 1 − α. Given δ > 0, the algorithm is allowed to toss coins adaptively and has to necessarily perform a coin toss until it identifies a coin whose posterior probability of being heavy is at least 1 − δ (i.e., a coin i for which Pr (Coin i is heavy | Outcomes all coin tosses) ≥ 1 − δ). The goal is to minimize the expected number of tosses required.\nAn adaptive strategy is allowed to choose which coin to toss after observing the history of outcomes of all previous coin tosses. Given the history of outcomes of coin tosses, the cost of an adaptive strategy is equal to the expected number of future coin tosses needed by following this strategy so that it identifies a coin whose posterior probability of being heavy is at least 1− δ. An adaptive strategy is said to be optimal if it has the minimum cost."
    }, {
      "heading" : "1.1 Results",
      "text" : "Our main result is an optimal adaptive algorithm for the above setting.\nTheorem 1. Given δ > 0, there exists an algorithm A that employs an optimal adaptive strategy in tossing coins to identify a coin whose posterior probability of being heavy is at least 1 − δ. At any step, the time taken by A to identify the coin to toss is O(1).\nWe also quantify the number of tosses performed by our optimal adaptive algorithm. We assume an infinite supply of coins under the same probabilistic setting. Let q := 1 − p, ∆H := log ((p+ )/(p− )), ∆T := log ((q + )/(q − )), B(δ) := log ((1− α)(1− δ)/αδ). Let δ0 be determined as follows: Consider the unique real value ρ ∈ (0, 1) such that ρ∆H (p+ ) + ρ−∆T (q− ) = 1 (the existence and uniqueness of ρ is elaborated in Section 5). Fix δ0 to be the largest real value such that (1− ρB(δ)+∆H )/(1− ρB(δ)+∆T ) < 2 and B(δ) ≥ ∆H .\nTheorem 2. For every δ ∈ (0, δ0], the expected number of tosses performed by A to identify a coin whose posterior probability of being heavy is at least 1− δ in the above setting, is at most\n16\n2 ( 1− α α + log ( (1− α)(1− δ) αδ )) .\nThe implications of our upper bound when the number of coins is bounded but much larger than 1/α needs to be contrasted with the lower bounds by [22]. In this case, setting n = c/α in the above expression suggests that our algorithm beats the lower bound shown in Theorem 9 of [22]. We observe that Theorem 9 of [22] shows a lower bound in the most general Bayesian setting – there exists a prior distribution of the probabilities of the n coins so that any algorithm requires at least O((n/ 2) log (1/δ)) tosses in expectation. However, our algorithm works in a particular Bayesian setting by exploiting prior knowledge about this setting."
    }, {
      "heading" : "1.2 Algorithm",
      "text" : "At any stage of the algorithm, let the history of outcomes of a coin i be given by Di := (hi, ti) where hi and ti refer to the number of outcomes that were heads and tails respectively. Given the history Di, we define the likelihood ratio of the coin to be\nLi := Pr (Coin i is heavy|Di)\nPr (Coin i is not-heavy|Di) =\n( p+\np− )hi (q − q + )ti .\nAlgorithm Likelihood-Toss\n1. Initialize Li = 1 for the i’th coin.\n2. While (Li < (1− α)(1− δ)/αδ ∀ i ∈ [n])\n(a) Toss coin i∗ such that i∗ = arg max{Li : i ∈ [n]}. (Break ties arbitrarily). Let\nbi∗ =\n{ 1 if outcome is heads,\n0 if outcome is tails.\n(b) Update Li∗ ← Li∗ ( p+ p− )bi∗ (1−p− 1−p+ )1−bi∗ .\n3. Output the coin i with maximum Li."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Our proof of optimality is based on an optimal strategy for multitoken Markov games. We now formally define the multitoken Markov game and state the optimal strategy that has been studied for this game. We use the notation and results from [12].\nA Markov system S = (V, P,C, s, t) consists of a state space V , a transition probability function P : V ×V → [0, 1], a positive real cost Cv associated with each state v, a start state s and a target state t. Let v(0), v(1), . . . , v(k) denote a set of states taken by following the Markov system for k steps. The cost of such a trip on S is the sum ∑k−1 i=0 Cv(i) of the costs of the exited states.\nLet S1, . . . , Sn be n Markov systems, each of which has a token on its starting state. A simple multitoken Markov game G = S1 ◦ S2 ◦ · · · ◦ Sn consists of a succession of steps in which we choose one of the n tokens, which takes a random step in its system (i.e., according to its Pi). After choosing a token i on state u say, we pay the cost Ci(u) associated with the state u of the system Si. We terminate as soon as one of the tokens reaches its target state for the first time. A strategy denotes the policy employed to pick a token given the state of the n Markov systems. The cost of such a game E[G] is the minimum expected cost taken over all possible strategies. The strategy that achieves the minimum expected cost is said to be optimal. A strategy is said to be pure if the choice of the token at any step is deterministic (entirely determined by the state of all Markov systems).\nTheorem 3. [12] Every Markov game has a pure optimal strategy.\nFor any strategy π for a Markov game G, we denote the expected cost incurred by playing π on G by Eπ[G].\nThe pure optimal strategy in the multitoken Markov game is completely determined by the grade γ of the states of the systems. The grade γ of a state is defined as follows: Given a Markov system S = (V, P,C, s, t) and state u, let S(u) = (V, P,C, u, t) denote the Markov system whose starting state is u. Consider the Markov game Sg(u) – where at any step of the game one is allowed to either play in S(u) or quit. Quitting incurs a cost of g. Playing in S(u) is equivalent to taking a step following the Markov system S incurring the cost associated with the state of\nthe system. The game stops once the target state is reached or once we quit. The grade γ(u) of state u is defined to be the smallest real value g such that there exists an optimal strategy σ that plays in S(u) in the first step. We note that, by definition, the cost of the game Sγ(u)(u) is E[Sγ(u)(u)] = γ(u) = Eσ[Sγ(u)(u)].\nTheorem 4. [12] Given the states u1, . . . , un of the Markov systems in the multitoken Markov game, the unique optimal strategy is to pick the token i such that γ(ui) is minimal.\nWe observe that the above results can be extended in a straightforward manner to the case where (1) the number of Markov systems is countably infinite, i.e., n = ∞ and (2) the Markov systems have infinite state space but all states are locally finite (i.e., the number of possible transitions from any fixed state is finite), by working through the proofs in [12]. The Markov systems that will be considered for our purpose will satisfy these two properties.\nWe use the following results from [13] to bound the number of tosses.\nTheorem 5. [13] Let X ∈ [−ν, µ] be the random variable that determines the step-sizes of a one dimensional random walk with absorbing barriers at −L and W such that Pr (X > 0) > 0, Pr (X < 0) > 0, E (X) 6= 0. Let L∗ = L+ ν, W ∗ = W + µ and φ(ρ) := E ( ρX ) .\n1. The function φ(ρ) is convex. If E (X) 6= 0, there exists a unique ρ0 ∈ (0, 1)∪ (1,∞) such that φ(ρ0) = 1. If E (X) < 0, then ρ0 > 1 and if E (X) > 0, then ρ0 < 1.\n2.\nPr (Absorption at W ) ≥ 1− ρ L 0\n1− ρL+W ∗0 .\n3. If E (X) < 0, then\nE (Number of steps to absorption) ≤ L ∗\n|E (X)| .\n4. If E (X) > 0, then\nE (Number of steps to absorption) ≤ (L+W ∗)\nE (X)\n( 1− ρL∗0\n1− ρL∗+W0\n) ."
    }, {
      "heading" : "3 Correctness",
      "text" : "We first argue the correctness of the algorithm.\nLemma 6. Given the history Di for a coin i, Pr (Coin i is heavy|Di) ≥ 1− δ if and only if Li ≥ (\n1− δ δ )( 1− α α ) .\nProof. The lemma is a straightforward application of Bayes’ theorem.\nPr (Coin i is heavy|Di) = Pr (Di|Coin i is heavy)Pr (Coin i is heavy)\nPr (Di)\n= α(p+ )hi(q − )ti\nα(p+ )hi(q − )ti + (1− α)(p− )hi(q + )ti\n= αLi\nαLi + (1− α) .\nThus, it follows that Pr (Coin i is heavy|Di) ≥ 1− δ if and only if Li ≥ (\n1− δ δ )( 1− α α ) .\nThe algorithm computes the likelihood ratio Li for each coin i based on the history of outcomes of the coin. The algorithm repeatedly tosses coins until there exists i∗ such that Li∗ ≥ (1−α)(1−δ)/αδ. Thus, if i∗ is output by Algorithm Likelihood-Toss, then\nPr (Coin i∗ is heavy|Di∗) ≥ 1− δ."
    }, {
      "heading" : "4 Optimality of the Algorithm",
      "text" : "Consider the log-likelihood of a coin i defined as Xi := logLi. Given the history of a coin, the log-likelihood of the coin is determined uniquely. In the beginning, the history is empty and hence all log-likelihoods are identically zero. The influence of a toss on the log-likelihood is a random step for Xi – if the outcome of the toss is a head, then Xi ← Xi + ∆H and if the outcome is a tail, then Xi ← Xi − ∆T . Thus, the toss outcomes of the coin leads to a 1-dimensional random-walk of the log-likelihood function associated with the coin. Further, since we stop tossing as soon as the log-likelihood of a coin is greater than B = log (1− α)(1− δ)/αδ, the random-walk has an absorbing barrier at B. We observe that the random walks performed by the coins are independent of each other since each coin being heavy is independent of the rest of the coins.\nThus, we have infinitely many identical Markov systems S1, S2, . . . , with each one starting in state Xi = 0. Each Markov system also has a target state, namely the boundary B. A strategy to pick a coin to toss is equivalent to picking a Markov system i. Each toss outcome is equivalent to the corresponding system taking a step following the transition probability and step size of the system. The goal to minimize the expected number of future tosses is equivalent to minimizing the expected number of steps for one of the Markov systems to reach the target state.\nTherefore, we are essentially seeking an optimal strategy to play a multitoken Markov game. We show that the strategy employed by Algorithm Likelihood-Toss is an optimal strategy to play the multitoken Markov game that arises in our setting.\nLet the Markov system associated with the one-dimensional random walk of the log-likelihood function of the history of the coin be S = (V, P,C, s, t). Here, the state space V consists of every possible real value that is at most B. The target state is a special state determined by t = B. The starting state is s = 0. Given the current state X, the transition cost incurred is one while transition probabilities are defined as follows:\nX → { min{X + ∆H , B} with probability Pr (Heads|X), X −∆T with probability 1− Pr (Heads|X)\nwhere\nPr (Heads|X) = Pr (Heads|Heavy coin)Pr (Heavy coin|X) + Pr (Heads|Non-heavy coin)Pr (Non-heavy coin|X)\n= (p+ )αeX\nαeX + (1− α) + (p− )(1− α) αeX + (1− α) .\nWe observe that the transition probabilities in this random-walk vary with the state of the system (as opposed to the well-known random-walk under uniform transition probability). It is clear that this Markov system is locally finite – the number of possible states reachable using one transition from any fixed state is only two. In this modeling of the Markov System for the log-likelihood of each coin, we do not condition on the coin being heavy or not-heavy. We are postponing this decision by conditioning based on the history."
    }, {
      "heading" : "4.1 Proof of Optimality",
      "text" : "We now show that the grade is a monotonically non-increasing function of the log-likelihood.\nLemma 7. Consider the Markov System S = (V, P,C, s, t) associated with the log-likelihood function. Let X,Y ∈ V such that X ≥ Y . Then γ(X) ≤ γ(Y ).\nProof. Let γ(Y ) = g. Then, by definition of grade, it follows that there exists a pure optimal strategy σ that chooses to toss the coin in the first step in Sg(Y ) and Eσ[Sg(Y )] = g. We will specify a mixed strategy π for Sg(X) such that Eπ[Sg(X)] ≤ g and π chooses to play in the system S(X) in the first step. It follows by definition that γ(X) ≤ g.\nThe pure strategy σ can be expressed by a (possibly infinite) binary decision tree Dσ as follows: Each node u has an associated label l(u) ∈ R. Each edge has a label from {H,T}. The root node v is labeled l(v) = Y . On reaching l(u) < B, if σ chooses to play in the system, then u has two children - the left and right children uL, uR are labeled l(uL) = l(u) + ∆H and l(uR) = l(u)−∆T respectively. The edges (u, uL), (u, uR) are labeled H and T respectively. On reaching l(u) < B, if σ decides to quit, then u is a leaf node. Finally, if l(u) ≥ B, then u is a leaf node. We observe that since σ plays in the system Sg(Y ) in the first step, the root of Dσ is not a leaf. (See Figure 1 for an example.)\nWe obtain a mixed strategy π for Sg(X) by considering the following ternary tree Dπ derived from Dσ: Each node u in Dπ has an associated label (lX(u), lY (u)) ∈ R2. Each edge in Dπ has a label from {HH,HT, TT}. There is an onto mapping m(u) from each node u ∈ Dπ to a node in Dσ. The root node u is labeled (lX(u) = X, lY (u) = Y ) and m(u) =Root(Dσ). For any node u, if m(u) is a leaf, then u is a leaf. Let u be a node such that v = m(u) is not a leaf. Let vH and vT denote the left and right children of v. Create children uHH , uHT , uTT as nodes adjacent to edges\nlabeled HH,HT, TT respectively. Define the mapping m(uHH) = vH , m(uHT ) = vT , m(uTT ) = vT and set\nlX(uHH) = lX(u) + ∆H , lX(uHT ) = lX(u) + ∆H , lX(uTT ) = lX(u)−∆T , lY (uHH) = lY (u) + ∆H , lY (uHT ) = lY (u)−∆T , lY (uTT ) = lY (u)−∆T .\nBy construction of Dπ, it follows that if X ≥ Y , then at any node u in Dπ, lX(u) ≥ lY (u) and hence, Pr (Heads|lX(u)) ≥ Pr (Heads|lY (u)).\nOur mixed strategy π for Sg(X) is based on Dπ. The strategy at any step maintains a pointer to some node u in Dπ. Initialize the pointer to the root node u. If the pointer is at a non-leaf node u, then π chooses to play in the system. If the step in the system is a backward step (outcome of coin toss is a tail), then π moves the pointer to uTT . If the step in the system is a forward step (outcome of coin toss is a head), then π generates a random number r ∈ [0, 1] and moves the pointer to the node uHH if r < Pr (Heads|lY (u)) /Pr (Heads|lX(u)) and to the node uHT if r ≥ Pr (Heads|lY (u)) /Pr (Heads|lX(u)). If the pointer is at a leaf node u such that lY (u) < B, then π quits the system. Otherwise, lY (u) ≥ B and hence lX(u) ≥ B. Thus, the strategy π is a valid mixed strategy for Sg(X) and π plays in the system Sg(X) in the first step since σ plays in the system Sg(Y ) in the first step.\nIt only remains to show that Eπ[Sg(X)] ≤ g. This is shown in Claim 8.\nClaim 8. Eπ[Sg(X)] ≤ g.\nProof. The cost of using σ for Sg(Y ) can be simulated by running a random process in Dσ and considering an associated cost. For each non-leaf node in Dσ associate a cost of 1 and for each leaf node u in Dσ such that l(u) < B, associate a cost of g. Consider the following random process RP1(u) for a node u ∈ Dσ: Begin at node u of Dσ. On reaching a non-leaf node v, repeatedly traverse the tree Dσ by taking the left child with probability Pr (Heads|l(v)) and the right child with the remaining probability until a leaf node is reached. The cost of the random process is the sum of the cost incurred along the nodes in the path traversed by the random process. Let E[Dσ(u)] denote the expected cost. Then, by construction of Dσ, it follows that E[Dσ(r)] = Eσ[Sg(l(r))] = g for the root node r in Dσ.\nNext, we give a random process RP2 on Dπ that relates the expected cost of following strategy π on Sg(X) and the expected cost of following strategy σ on Sg(Y ). We first associate a cost with each node u in Dπ: For each non-leaf node u, if lX(u) < B, then cost cX(u) = 1, and if lY (u) < B, then cost cY (u) = 1. For each leaf node u, if lX(u) < B, then cost cX(u) = g and if lY (u) < B, then cost cY (u) = g. The remaining costs are zero. Here, we observe that cX(u) ≤ cY (u) for every node u ∈ Dπ.\nWe define the random process RP2(v) for a node v ∈ Dπ as follows: Begin at node v and repeatedly traverse the tree Dπ by taking one of the three children at each non-leaf node until a leaf node is reached. On reaching a non-leaf node u, traverse to uHH with probability Pr (Heads|lY (u)), to vHT with probability Pr (Heads|lX(u))− Pr (Heads|lY (u)) and to uTT with the remaining probability. Let P (v) be the set of nodes in the path traversed by the random process RP2(v). Let the cost incurred be cX(v) = ∑ u∈P (v) cX(u) and cY (v) = ∑ u∈P (v) cY (u). Now, the cost incurred by following strategy π for Sg(X) is the same as the cost cX(r) incurred by the random process RP2(r), where r is the root node in Dπ.\nBy construction of Dπ from Dσ, it follows that for each node v ∈ Dπ, the expected cost cY (v) of the random process RP2(v) is equal to the expected cost of the random process RP1(m(v)). Hence, E[cY (r)] = E[Dσ(m(r))] = g for the root node r in Dπ. Next, since cX(u) ≤ cY (u) for every node u, it follows that E[cX(r)] ≤ E[cY (r)] = g. Finally, the expected cost incurred by following mixed strategy π for Sg(X) is exactly equal to E[cX(r)].\nProof of Theorem 1. We use Algorithm Likelihood-Toss. By Lemma 6, the optimal adaptive strategy also minimizes the expected number of tosses to identify a coin i such that the log-likelihood Xi ≥ B.\nThe strategy adopted by Algorithm Likelihood-Toss at any stage is to toss the coin with maximum log-likelihood. Let the Markov system associated with the one-dimensional random walk of the log-likelihood function of the history of the coin be S = (V, P,C, s, t). We have infinitely many independent and identical Markov systems S1 = S2 = . . . = S associated with the log-likelihood function of the respective coin. By Theorem 4, the optimal strategy to minimize the expected number of tosses to identify a coin i such that the log-likelihood Xi ≥ B is to toss the coin i such that γ(Xi) is minimal. Lemma 7 shows that the grade function γ(X) is monotonically non-increasing. Thus, tossing the coin with maximum log-likelihood is an optimal strategy.\nBy the description of the algorithm, it is clear that the algorithm starts tossing a fresh/new coin only if the log-likelihood of the current coin decreases below zero. The time to update the likelihood ratio of the current coin after a coin toss is only a constant and hence the time to identify the coin to toss is O(1)."
    }, {
      "heading" : "5 Number of Coin Tosses",
      "text" : "In this section, we give an upper bound on the number of coin tosses performed by Algorithm Likelihood-Toss. The algorithm repeatedly tosses a coin while the log-likelihood of the coin is at least zero and starts with a fresh coin if the log-likelihood of the coin is less than zero. The algorithm terminates if the log-likelihood of a coin is at least B.\nConsider the random walk of the log-likelihood function. The random walk has absorbing barriers at B and at every state less than 0.\nLemma 9. Let C and D denote the expected number of tosses to get absorbed for a non-heavy and heavy coin respectively. Let π denote the probability that a heavy coin gets absorbed at B. Then, under the assumptions of Theorem 2,\n1.\nπ ≥ ∆H(p+ )−∆T (q − ) 2(∆H + ∆T ) .\n2. D π ≤ (\n8B\n∆H(p+ )−∆T (q − ) )( ∆H + ∆T ∆H(p+ ) ) .\n3.\nC ≤ 2(∆H + ∆T ) ∆T (q + )−∆H(p− ) .\nProof. Consider a modified random walk where the starting state is ∆H as opposed to zero. Let C ′ and D′ denote the expected number of tosses for the modified walk to get absorbed using a non-heavy and heavy coin respectively. Let π′ denote the probability that the modified walk gets absorbed at B using a heavy coin. Then, D ≤ D′ + 1 ≤ 2D′, C ≤ C ′ + 1 ≤ 2C ′, π = (p+ )π′.\nWe use Theorem 5. For the modified random walk, we have that L = ∆H , W = B − ∆H , ν = ∆T , µ = ∆H . For the modified random walk using a heavy coin, the step sizes are\nX =\n{ ∆H with probability p+\n−∆T with probability q − ,\nand for the modified random walk using a non-heavy coin, the step sizes are\nY = { ∆H with probability p− −∆T with probability q + ,\nFor > 0, we have that E (Y ) < 0. Therefore,\nC ′ ≤ ∆H + ∆T ∆T (q + )−∆H(p− )\nand hence we have the bound on C. Now consider the modified random walk using a heavy coin. For > 0, we have that E (X) > 0. Let ρ0 < 1 be the unique real value such that E ( ρX0 ) = 1. Thus,\nπ′ ≥ 1− ρ ∆H 0\n1− ρB+∆H0\nD′ ≤ (∆H +B) E (X) ( 1− ρ∆H+∆T0 1− ρB+∆T0 ) .\nSince φ(ρ) is convex, it can be shown that the minimum value of φ(ρ) occurs at\nρmin = ( ∆T (q − ) ∆H(p+ ) ) 1 ∆H+∆T\nand hence, ρ0 < ρmin < 1. Thus,\nD′\nπ′ ≤ (∆H +B) E (X) ( 1− ρB+∆H0 1− ρB+∆T0 )( 1− ρ∆H+∆T0 1− ρ∆H0 )\n≤ 2B E (X)\n( 1− ρ∆H+∆T0\n1− ρ∆H0\n) (by the assumption δ < δ0)\n< 2B\nE (X)\n( 1− ρ∆H+∆Tmin\n1− ρ∆Hmin\n) (since ρ0 < ρmin)\n= 2B\n∆H(p+ )  1 1− ( ∆T (q− ) ∆H(p+ ) ) ∆H ∆H+∆T  ≤ 4B(∆H + ∆T )\nE (X) ∆H .\nand we obtain the bound on the ratio D/π. Finally, to lower bound π′, we observe that\nπ′ ≥ 1− ρ ∆H 0\n1− ρB+∆H0\n≥ 1− ρ∆Hmin\n1− ρB+∆Hmin ≥ 1− ρ∆Hmin ≥ E (X) 2(∆H + ∆T )(p+ ) .\nProof of Theorem 2. . We use Algorithm Likelihood-Toss. Consider the one-dimensional random walk of the log-likelihood function. The random walk has absorbing barriers at B and at every state less than 0. Let C and D denote the expected number of tosses to get absorbed for a non-heavy and heavy coin respectively. Let π denote the probability that a heavy coin gets absorbed at B. Let D0 and D1 denote the expected number of tosses of a heavy coin to get absorbed at 0 and B respectively. Then, D = (1− π)D0 + πD1.\nLet E denote the expected number of tosses performed by algorithm Likelihood-Toss. Then,\nE ≤ (1− α)(C + E) + α((1− π)(D0 + E) + πD1)\n⇒ E ≤ (1− α) α C π + D π .\nBy Lemma 9, we have that E ≤ ( 4(∆H + ∆T )\n∆H(p+ )−∆T (q − ) )(( 1− α α )( ∆H + ∆T ∆T (q + )−∆H(p− ) ) + ( 2B ∆H(p+ ) )) .\nThe final upper bound follows by substituting for ∆H ,∆T and B and using the following inequalities (derived by straightforward calculus),\n2\n≥ max\n{ ∆H + ∆T\n∆H(p+ )−∆T (q − ) , ∆H + ∆T ∆T (q + )−∆H(p− )\n} ,\n∆H ≥\np− ."
    }, {
      "heading" : "6 Discussion",
      "text" : "We gave an adaptive strategy that tosses coins in order to achieve a certain stopping condition, namely, the existence of a coin whose posterior probability of being heavy is at least a given threshold. Our strategy has minimum cost where cost is measured by the expected number of future tosses by following the strategy to attain the stopping condition. We achieved this by performing\nthe best possible action after observing the outcome of each coin toss. We note that our algorithm can also be modified to start from any fixed history of outcomes by appropriately modifying the initialization step. The optimality of the action is exhibited using tools from the field of Markov games. A major limitation of our algorithm is that it is optimal only in the setting where the coins are independently heavy and non-heavy. It would be very interesting to devise an adaptive strategy where the coins are not necessarily independent – say we have n coins with exactly one heavy coin and the goal is to attain the stopping condition. In this setting, we note that the posterior probability of a fixed coin being heavy depends on the outcomes of the tosses of all the coins and not just any fixed coin.\nAcknowledgment. We thank Santosh Vempala for valuable comments."
    } ],
    "references" : [ {
      "title" : "Best Arm Identification in Multi-Armed Bandits",
      "author" : [ "J.-Y. Audibert", "S. Bubeck", "R. Munos" ],
      "venue" : "In Proceedings of the Twenty-third Conference on Learning Theory, COLT",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Finite-time Analysis of the Multiarmed Bandit Problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "The Nonstochastic Multiarmed Bandit Problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM Journal of Computing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "A Single-Sample Multiple Decision Procedure for Ranking Means of Normal Populations with known Variances",
      "author" : [ "R.E. Bechhofer" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1954
    }, {
      "title" : "Design and Analysis of Experiments for Statistical Selection, Screening, and Multiple Comparisons",
      "author" : [ "R.E. Bechhofer", "T.J. Santner", "D.M. Goldsman" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1995
    }, {
      "title" : "Bandit Problems: Sequential Allocation of Experiments (Monographs on Statistics and Applied Probability)",
      "author" : [ "D.A. Berry", "B. Fristedt" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1985
    }, {
      "title" : "Using Ranking and Selection to “Clean Up” after Simulation Optimization",
      "author" : [ "J. Boesel", "B.L. Nelson", "S.-H. Kim" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Proceedings of the 20th international conference on Algorithmic learning theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Economic Analysis of Simulation Selection Problems",
      "author" : [ "S.E. Chick", "N. Gans" ],
      "venue" : "Management Science,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "New Two-Stage and Sequential Procedures for Selecting the Best Simulated System",
      "author" : [ "S.E. Chick", "K. Inoue" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "The Max k-Armed Bandit: A New Model for Exploration Applied to Search Heuristic Selection",
      "author" : [ "V. Cicirello", "S. Smith" ],
      "venue" : "In 20th National Conference on Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "On Playing Golf with Two Balls",
      "author" : [ "I. Dumitriu", "P. Tetali", "P. Winkler" ],
      "venue" : "SIAM Journal of Discrete Mathematics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "Bounds on Gambler’s Ruin Probabilities in Terms of Moments",
      "author" : [ "S.N. Ethier", "D. Khoshnevisan" ],
      "venue" : "Methodology and Computing in Applied Probability,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "PAC Bounds for Multi-armed Bandit and Markov Decision Processes",
      "author" : [ "E. Even-Dar", "S. Mannor", "Y. Mansour" ],
      "venue" : "In Proceedings of the 15th Annual Conference on Computational Learning Theory, COLT",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems",
      "author" : [ "E. Even-Dar", "S. Mannor", "Y. Mansour" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "A Knowledge-Gradient Policy for Sequential Information Collection",
      "author" : [ "P.I. Frazier", "W.B. Powell", "S. Dayanik" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Multi-Bandit Best Arm Identification",
      "author" : [ "V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Multi-armed Bandit Allocation Indices",
      "author" : [ "J. Gittins", "K. Glazebrook", "R. Weber" ],
      "venue" : "Wiley, 2nd edition,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Bayesian look ahead one-stage sampling allocations for selection of the best population",
      "author" : [ "S.S. Gupta", "K.J. Miescke" ],
      "venue" : "Journal of Statistical Planning and Inference,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1996
    }, {
      "title" : "Selecting the best system",
      "author" : [ "S.H. Kim", "B.L. Nelson" ],
      "venue" : "Handbooks in Operations Research and Management Science: Simulation,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Asymptotically Efficient Adaptive Allocation Rules",
      "author" : [ "T. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1985
    }, {
      "title" : "The Sample Complexity of Exploration in the Multi-Armed Bandit Problem",
      "author" : [ "S. Mannor", "J.N. Tsitsiklis" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2004
    }, {
      "title" : "Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation",
      "author" : [ "O. Maron", "A. Moore" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1994
    }, {
      "title" : "A Sequential Procedure for Selecting the Population with the Largest Mean from k Normal Populations",
      "author" : [ "E. Paulson" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1964
    }, {
      "title" : "Selection-of-the-best procedures for optimization via simulation",
      "author" : [ "J. Pichitlamken", "B.L. Nelson" ],
      "venue" : "In Proceeding of the 2001 Winter Simulation Conference,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "[18].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].",
      "startOffset" : 121,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].",
      "startOffset" : 121,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].",
      "startOffset" : 121,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].",
      "startOffset" : 121,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "The work of [8] suggested that the exploration-exploitation trade offs for this setting are much different from the setting where the number of steps is asymptotic.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "[1] proposed exploration strategies to perform essentially as well as the best strategy that knows all distributions up to permutations of the arms.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "[17] addressed the problem of identifying the best arm for each bandit among a collection of bandits within a fixed budget.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem – given any δ > 0, identify the arm with maximum expected reward with error probability at most δ while minimizing the total number of steps needed.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem – given any δ > 0, identify the arm with maximum expected reward with error probability at most δ while minimizing the total number of steps needed.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem – given any δ > 0, identify the arm with maximum expected reward with error probability at most δ while minimizing the total number of steps needed.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "[14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] showed that a total of O((n/ 2) log(1/δ)) steps is sufficient to identify an arm whose expected reward is at most away from the optimal arm with correctness at least 1 − δ.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "Mannor and Tsitsiklis [22] showed lower bounds matching up to constant factors under various settings of the rewards.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the “ranking and selection problem”.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 21,
      "context" : "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the “ranking and selection problem”.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the “ranking and selection problem”.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the “ranking and selection problem”.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the “ranking and selection problem”.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 16,
      "context" : "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the “ranking and selection problem”.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "It was introduced for normally distributed rewards by Bechhofer [4].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 23,
      "context" : "Adaptive strategies for this problem, known as “sequential selection”, can be traced back to Paulson [24].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 23,
      "context" : "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 24,
      "context" : "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "This special case is known as the “indifference-zone” assumption [4].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 19,
      "context" : "Strategies and their measure of optimality are known for various relaxations of independence, normality, equal and known variances and indifference-zone assumptions [20].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 18,
      "context" : "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 21,
      "context" : "The implications of our upper bound when the number of coins is bounded but much larger than 1/α needs to be contrasted with the lower bounds by [22].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "In this case, setting n = c/α in the above expression suggests that our algorithm beats the lower bound shown in Theorem 9 of [22].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 21,
      "context" : "We observe that Theorem 9 of [22] shows a lower bound in the most general Bayesian setting – there exists a prior distribution of the probabilities of the n coins so that any algorithm requires at least O((n/ 2) log (1/δ)) tosses in expectation.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "We use the notation and results from [12].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "A Markov system S = (V, P,C, s, t) consists of a state space V , a transition probability function P : V ×V → [0, 1], a positive real cost Cv associated with each state v, a start state s and a target state t.",
      "startOffset" : 110,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "[12] Every Markov game has a pure optimal strategy.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Given the states u1, .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : ", the number of possible transitions from any fixed state is finite), by working through the proofs in [12].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "We use the following results from [13] to bound the number of tosses.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "[13] Let X ∈ [−ν, μ] be the random variable that determines the step-sizes of a one dimensional random walk with absorbing barriers at −L and W such that Pr (X > 0) > 0, Pr (X < 0) > 0, E (X) 6= 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "If the step in the system is a forward step (outcome of coin toss is a head), then π generates a random number r ∈ [0, 1] and moves the pointer to the node uHH if r < Pr (Heads|lY (u)) /Pr (Heads|lX(u)) and to the node uHT if r ≥ Pr (Heads|lY (u)) /Pr (Heads|lX(u)).",
      "startOffset" : 115,
      "endOffset" : 121
    } ],
    "year" : 2013,
    "abstractText" : "We study the problem of learning a most biased coin among a set of coins by tossing the coins adaptively. The goal is to minimize the number of tosses until we identify a coin i∗ whose posterior probability of being most biased is at least 1 − δ for a given δ. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategy – a strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs tools from the field of Markov games.",
    "creator" : "LaTeX with hyperref package"
  }
}