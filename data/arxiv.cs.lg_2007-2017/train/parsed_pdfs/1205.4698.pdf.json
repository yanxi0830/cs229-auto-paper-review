{
  "name" : "1205.4698.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Role of Weight Shrinking in Large Margin Perceptron Learning",
    "authors" : [ "Constantinos Panagiotakopoulos" ],
    "emails" : [ "costapan@eng.auth.gr,", "petroula@gen.auth.gr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 5.\n46 98\nv2 [\ncs .L\nG ]\n7 F\neb 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "It is widely accepted that the generalization ability of learning machines improves as the margin of the solution hyperplane increases [23]. The simplest online learning algorithm for binary linear classification, the perceptron [18, 13], does not aim at any margin. The problem, instead, of finding the optimal separating hyperplane is central to Support Vector Machines (SVMs) [23, 2].\nSVMs obtain large margin solutions by solving a constrained quadratic optimization problem using dual variables. In the early days, however, efficient implementation of SVMs was hindered by the quadratic dependence of their memory requirements on the number of training examples. To overcome this obstacle decomposition methods [17, 7] were developed that apply optimization only to a subset of the training set. Although such methods led to considerable improvement the problem of excessive runtimes when processing very large datasets remained. Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.\nThe drawbacks of the dual formulation motivated research long before the advent of linear SVMs in alternative large margin classifiers naturally formulated in primal space. Having the perceptron as a prototype they focus on the primal problem by updating a weight vector which represents their current state whenever a data point presented to them satisfies a specific condition. By exploiting\ntheir ability to process one example at a time1 they save time and memory and acquire the potential to handle large datasets. The first algorithm of the kind is the perceptron with margin [4] the solutions of which provably possess only up to 1/2 of the maximum margin [11]. Subsequently, various others succeeded in approximately attaining maximum margin by employing modified perceptron-like update rules. For ROMMA [12] such a rule is the result of a relaxed optimization which reduces all constraints to just two. In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a “projection” mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates. Very recently, the margitron [15] and the perceptron with dynamic margin (PDM) [16] using modified conditions managed to approximately reach maximum margin solutions while maintaining the original perceptron update.\nA somewhat different approach from the hard margin one adopted by most of the algorithms above was also developed which focuses on the minimization of the regularized 1-norm soft margin loss through stochastic gradient descent. Notable representatives of this approach are the pioneer NORMA [10] and Pegasos [19]. Stochastic gradient descent gives rise to perceptron-like updates an important ingredient of which is the “shrinking” of the current weight vector. Shrinking is always imposed when a pattern is presented to the algorithm with it being the only modification suffered by the weight vector in the event that its condition is violated and as a consequence no loss is incurred. The cummulative effect of shrinking is to gradually diminish the impact of the earlier contributions to the weight vector. Shrinking has also been employed by algorithms which do not have their origin in stochastic gradient descent as an accompanying mechanism in perceptron-based budget scenarios for classification [3] or tracking [1].\nOur purpose in the present work is to investigate the role that shrinking of the weight vector might play in large margin perceptron learning. This is motivated by the observation that such a mechanism naturally emerges in attempts to attack the 1-norm soft margin task through stochastic gradient descent. If we accept that algorithms like NORMA succeed in minimizing the regularized 1-norm soft margin loss they should be able to solve the hard margin problem as well for sufficiently small non-zero values of the regularization parameter which also controls the strength of shrinking. Thus shrinking, as weak as it may be, when introduced into the perceptron algorithm with margin might prove beneficial. Another factor to be taken into account is that the shrinking mechanism in the algorithms considered here is operative only for erroneous trials, a feature that offers them the possibility to terminate in a finite number of steps. Therefore, shrinking in such algorithms may need to be strengthened relative to algorithms like NORMA to compensate for the fact that the latter shrink the weight vector even when the condition is violated. In conclusion, the amount of shrinking that a perceptron with margin could tolerate without it destroying the conservativeness of the update might be sufficient to raise the theoretically\n1 The conversion of online algorithms to the batch setting is done by cycling repeatedly through the dataset and using the last hypothesis for prediction.\nguaranteed fraction of the maximum margin achieved to a value larger than 1/2. It turns out that this is actually the case.\nThe remaining of this paper is organized as follows. Section 2 contains some preliminaries and a description of the algorithms. In Sect. 3 we present a theoretical analysis of the algorithms. Section 4 is devoted to implementational issues and a brief experimental evaluation while Sect. 5 contains our conclusions."
    }, {
      "heading" : "2 The Algorithms",
      "text" : "Let us consider a linearly separable training set {(xk, lk)}mk=1, with vectors xk ∈ IRd and labels lk ∈ {+1,−1}. This training set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality [23, 2]. By placing xk in the same position at a distance ρ in an additional dimension, i.e., by extending xk to [xk, ρ], we construct an embedding of our data into the socalled augmented space [4]. This way, we construct hyperplanes possessing bias in the non-augmented feature space. Following the augmentation, a reflection with respect to the origin of the negatively labeled patterns is performed. This allows for a uniform treatment of both categories of patterns. Also, R ≡ max\nk ‖yk‖ with\nyk ≡ [lkxk, lkρ] the kth augmented and reflected pattern. The relation characterizing optimally correct classification of the training patterns yk by a weight vector u of unit norm in the augmented space is\nu · yk ≥ γd ≡ max u′:‖u′‖=1 min i {u′ · yi} ∀k . (1)\nWe shall refer to γd as the maximum directional margin. It coincides with the maximum margin in the augmented space with respect to hyperplanes passing through the origin. The maximum directional margin γd is upper bounded by the maximum geometric margin γ in the non-augmented space and tends to it as ρ → ∞ [20].\nWe consider algorithms in which the augmented weight vector ast is initially set to zero, i.e. as0 = 0, and is updated according to the perceptron-like rule\na s t+1 = c s ta s t + ηyk (2)\neach time the “misclassification” condition\nc̄sta s t · yk ≤ b (3)\nis satisfied by a training pattern yk, i.e., whenever a margin error is made on yk. Here 0 < cst , c̄ s t ≤ 1 are “shrinking factors” which may vary with the number t of updates, η > 0 is a constant learning rate and b > 0 acts as a margin threshold in the misclassification condition. If we set cst = c̄ s t = 1 we recover the classical perceptron algorithm with margin. The role of cst is to shrink the current weight vector as a first step of the update, thereby enhancing the importance of the current update relative to the previous one. At the same time such a shrinking\nacts as a mechanism of effectively increasing the margin threshold of the condition, an effect that may be further strengthened through the introduction of the factor c̄st in (3). In fact, for appropriate choices of c s t , c̄ s t , to which we confine our interest here, it is possible to equivalently introduce shrinking into the perceptron with margin via a learning rate and margin threshold which both increase with t. Notice that we denote by ast the weight vector of the algorithms with shrinking to reserve the notation at for the weight vector of the equivalent algorithms with variable learning rate and margin threshold.\nThe Margin Perceptron with Constant Shrinking\nInput: A linearly separable augmented dataset S = (y1, . . . ,yk, . . . ,ym) with reflection assumed Fix: η, λ, b Define: cs = 1− ηλ Initialize: t = 0, a0 = 0, η0 = η, b0 = c\ns b repeat\nfor k = 1 to m do if at · yk ≤ bt then\nat+1 = at + ηtyk ηt+1 = ηt/c s bt+1 = bt/c s t← t+ 1\nuntil no update made within the for loop\nWe investigate the impact of shrinking on large margin perceptron learning by considering both constant and variable shrinking factors. If shrinking does not depend on t we set c̄st = 1 since a constant c̄st may be absorbed into a redefinition of b. We also express cst in terms of a “shrinking parameter” λ < 1/η as cst = 1 − ηλ. Then (2) becomes the update of NORMA for ast ·yk ≤ b. NORMA, however, updates its weight vector even when ast · yk > b. In this case the update reduces to pure shrinking ast+1 = (1−ηλ)ast . This\nis the important difference from our algorithm in which an update occurs only if the misclassification condition is satisfied, thereby making convergence in a finite number of steps possible.\nLet us divide the update rule (2) with (1 − ηλ)t and condition (3) with (1−ηλ)t−1. Also let at = ast/(1−ηλ)t−1. Then, we obtain a completely equivalent algorithm with update\nat+1 = at + η\n(1− ηλ)t yk (4)\nand condition\nat · yk ≤ b\n(1− ηλ)t−1 . (5)\nAn algorithm with variable shrinking is obtained if we choose cst = c̄ s t = (t/(t+ 1)) n , where n ≥ 0 is an integer. For n = 1 the shrinking factor cst entering the update is the one encountered in Pegasos. Pegasos, however, has variable learning rate, c̄st = 1 and performs, just like NORMA, a pure shrinking update when its condition is violated. In addition, its update ends with a projection step. A variable shrinking factor t/(t+ λ) is also employed by SPA [1] in which b = 0. Such a factor is related to ours for λ = n even if n 6= 1 since for t ≫ n\nt\nt+ n =\nn−1 ∏\nk=0\nt+ k\nt+ k + 1 =\n(\nt\nt+ 1\n)n n−1 ∏\nk=0\n(\n1 + k\nt(t+ k + 1)\n) ≈ ( t\nt+ 1\n)n\n.\nLet us multiply both the update rule (2) and condition (3) with (t+1)n and set at = t n a s t . Then, we obtain a completely equivalent algorithm with update\nat+1 = at + η(t+ 1) n yk (6)\nand condition at · yk ≤ b(t+ 1)n . (7)\nThe Margin Perceptron with Variable Shrinking\nInput: A linearly separable augmented dataset S = (y1, . . . ,yk, . . . ,ym) with reflection assumed Fix: η, b, n Initialize: t = 0, a0 = 0 repeat\nfor k = 1 to m do tn = (t+ 1) n\nbt = btn ηt = ηtn if at · yk ≤ bt then\nat+1 = at + ηtyk t← t+ 1\nuntil no update made within the for loop\nIf we had chosen c̄st = 1 we should have multiplied (3) with tn. As a result the threshold in (7) would have been btn, a difference that does not seem to be of paramount importance. However, the choice c̄st = (t/(t+ 1)) n prevailed for the sake of convenience. The choice, instead, cst = c̄ s t = t/(t + n)= Pnt /P n t+1 with P n t ≡ ∏n−1 k=0 (t+ k) would have led to at = P n t a s t and to the replacement of (t+1)n with Pnt+1 in (6) and (7).\nWe shall refer to the algorithm with update (4) and condition (5)\nas the margin perceptron with constant shrinking. The algorithm, instead, with update (6) and condition (7) will be called the margin perceptron with variable shrinking. The above formulations of the algorithms are the ones that will henceforth be considered in place of the original formulations of (2) and (3)."
    }, {
      "heading" : "3 Theoretical Analysis",
      "text" : "We begin with the analysis of the margin perceptron with constant shrinking.\nTheorem 1. The margin perceptron with constant shrinking converges in a finite number tc of updates satisfying the bound\ntc ≤ 1 δ(1− ǫ) R2\nγ2d ln\n4− (2 + δ)ǫ+ δ (2 + δ)ǫ − δ (8)\nprovided δ ≡ ηR2/b ≤ 2 and ǫ ≡ 1−λb/γ2d obey the constraint δ/(2+ δ) < ǫ < 1. Moreover, the zero-threshold solution hyperplane possesses margin γ′d which is a fraction f of the maximum margin γd obeying the inequality\nf ≡ γ ′ d\nγd >\n1\n2 + δ + 1− ǫ 2 . (9)\nFinally, an after-run lower bound on f involving the margin γ′d achieved, the length ‖atc‖ of the solution weight vector atc and the number tc of updates is given by\nf ≥ 1− (1 − ηλ) tc λ(1− ηλ)tc−1 γ′d ‖atc‖ . (10)\nProof. Taking the inner product of (4) with the optimal direction u and using (1) we get\nu · at+1 − u · at = η (1− ηλ)tu · yk ≥ η (1 − ηλ)t γd\na repeated application of which, taking into account that a0 = 0, gives\n‖at‖ ≥ u · at ≥ t−1 ∑\nk=0\nηγd (1 − ηλ)k = 1− (1− ηλ)t λ(1− ηλ)t−1 γd . (11)\nHere we made use of ∑t−1 k=0 α k = (αt − 1)/(α− 1). From (4) and (5) we obtain\n‖at+1‖2 − ‖at‖2 = η2\n(1− ηλ)2t ‖yk‖ 2 +\n2η (1− ηλ)tat · yk ≤ η2R2 + 2η(1− ηλ)b (1 − ηλ)2t .\nA repeated application of the above inequality, assuming a0 = 0, leads to\n‖at‖2 ≤ t−1 ∑\nk=0\nη2R2 + 2η(1− ηλ)b (1 − ηλ)2k =\n( 1− (1− ηλ)2t ) ( ηR2 + 2(1− ηλ)b )\nλ(2 − ηλ)(1 − ηλ)2(t−1) .\n(12)\nComparing the lower bound on ‖at‖2 from (11) with its upper bound (12) we get\n(1− (1− ηλ)t)2\nλ γ2d ≤ 1− (1− ηλ)2t 2− ηλ ( ηR2 + 2(1− ηλ)b )\n(13)\nor, noticing that 1− (1− ηλ)2t = (1− (1 − ηλ)t) (1 + (1− ηλ)t), we obtain\n1− (1− ηλ)t ≤ ( 1 + (1− ηλ)t ) A . (14)\nHere\nA ≡ λ ( b\nγ2d\n)\nηR2/b+ 2(1− ηλ) 2− ηλ < 1 . (15)\nThe condition A < 1 ensures that (14) does lead to an upper bound on the number of updates since otherwise (14) is always satisfied. This translates into a very restrictive upper bound on the shrinking parameter λ. This upper bound depends on the values of the remaining parameters but is never larger than γ2d/b. From (14), provided A < 1, we easily derive the following upper bound on the number of updates\nt ≤ tb ≡ 1 ln(1− ηλ)−1 ln 1 +A 1−A . (16)\nFor δ ≤ 2 it holds that δ + 2(1− ηλ)\n2− ηλ ≤ 1 + δ 2 (17)\nand\nA = (1− ǫ) ( δ + 2(1− ηλ) 2− ηλ ) ≤ (1− ǫ) ( 1 + δ 2 ) = 1− (2 + δ)ǫ− δ 2 . (18)\nAs a consequence, ǫ > δ/(2 + δ) ensures that A < 1. In addition\nln(1− ηλ)−1 ≥ ηλ = δ(1− ǫ) γ 2 d\nR2 . (19)\nCombining (16), (18) and (19) we finally arrive at the slightly simplified upper bound on the number of updates given by (8).\nUpon convergence of the algorithm in tc updates condition (5) is violated by all patterns. Therefore, the achieved margin γ′d > b/ ( (1− ηλ)tc−1 ‖atc‖ ) . Thus,\nf2 = γ′d\n2\nγ2d >\nb2 (1− ηλ)2(tc−1) ‖atc‖2 γ2d ≥ λ(2− ηλ)b 2 (1− (1 − ηλ)2tc) (ηR2 + 2(1− ηλ)b) γ2d\n≥ λ(2 − ηλ)b 2\n(1− (1− ηλ)2tb ) (ηR2 + 2(1− ηλ)b) γ2d =\n(\nλb\n(1− (1− ηλ)tb ) γ2d\n)2\n,\nwhere use has been made of the upper bound (12) on ‖atc‖2 and of the fact that (13) at t = tb holds as an equality. Taking the square root and making use of the definition of tb from (16) the previous inequality becomes\nf > λ b\nγ2d\n(\n1 +A 2A\n)\n= 1\n2\n(\nλb\nAγ2d + λ\nb\nγ2d\n)\n= 1\n2\n(\n2− ηλ δ + 2(1− ηλ) + 1− ǫ\n)\n.\nFor δ ≤ 2 the above inequality gives rise to (9) because of (17). Finally, (10) is readily obtained if in the ratio γ′d/γd we employ the upper bound on γd derivable from (11). ⊓⊔\nRemark 1. The parameters δ and ǫ are independent. Therefore, we may consider choosing δ ≪ 1 while keeping ǫ fixed. In this case the upper bound (8) on the number of updates becomes O (\nδ−1R2/γ2d )\nand from (9) the before-run lower bound on f approaches as δ → 0 the value 1 − ǫ/2. This generalizes the wellknown result that the classical perceptron algorithm with margin (obtainable when λ → 0 or ǫ → 1) has in the limit δ → 0 a theoretically guaranteed beforerun value of f equal to 1/2. By subsequently letting ǫ → 0 (i.e., λ → γ2d/b) we may approach solutions with maximum margin.\nRemark 2. To facilitate comparison with other large margin classifiers we may relate the independent parameters δ and ǫ and obtain a single parameter ζ < 1/ √ 2 through the relations δ = 2ζ, ǫ = δ(1+δ)/(2+δ) = ζ(1+2ζ)/(1+ζ). Then, from (8) and (9) we have that the margin perceptron with constant shrinking achieves “accuracy” ζ, i.e., f > 1− ζ , in a number tc of updates satisfying the bound\ntc ≤ 1\nζ\n(\n1 + ζ 1− 2ζ2 ) R2\nγ2d ln\n√\n1− ζ2 ζ .\nNotice that the quantity R/γd does not enter the logarithm. In this sense the above bound, which is O (\n(ζ−1R2/γ2d) ln ζ −1 ) for ζ ≪ 1, is the best among the\nbounds of perceptron-like maximum margin algorithms. Typically, algorithms which require at least an approximate knowledge of the value of γd to tune their parameters have bounds O (\n(ζ−1R2/γ2d) ln(ζ −1 (R/γd)\nk ) ) with k = 1, 2 while\nalgorithms which do not assume such a knowledge have bounds O ( ζ−2R2/γ2d ) .\nRemark 3. Suppose we are given γ̄d < γd. It may be expressed as γ̄d = (1−ξ)γd. Setting λ = (2/(2+ δ))γ̄2d/b it holds that ǫ = 1− (2/(2+ δ))(1− ξ)2 > δ/(2+ δ). Then (9) gives γ′d/γd > 1− ξ + ( ξ2 − δ(1− ξ) )\n/(2 + δ). Thus, for δ(1− ξ) ≤ ξ2 a solution with margin γ′d > γ̄d is obtained which provides a better lower bound on γd than the one used as an input. A repeated application of this procedure starting, e.g., with γ̄d = 0, ξ = 1, λ = 0 gives solutions possessing margin which is any desirable approximation of γd without prior knowledge of its value. An estimate of the quality of the approximation at each stage may be obtained via the after-run lower bound (10) on γ′d/γd which provides an upper bound on γd. In practice, only a few repetitions of this procedure are required to obtain a satisfactory approximation of the optimal solution because the margin actually achieved by the algorithm is considerably larger than the one suggested by (9).\nRemark 4. From (12) we see that for the algorithm described by (2) and (3) with cst = 1 − ηλ, c̄st = 1 it holds that ‖ast‖2 = ‖at‖2 (1 − ηλ)2(t−1) ≤ (\nηR2 + 2(1− ηλ)b ) / (λ(2− ηλ)). Thus, it is confirmed in this context the wellknown fact that constant shrinking leads to bounded length of the weight vector.\nTo proceed with our analysis of the margin perceptron with variable shrinking we need some inequalities involving sums of powers of integers which we present in the form of lemmas. Their proofs can be found in the Appendix.\nLemma 1. Let n ≥ 0 be an integer. Then, it holds that\n(n+ 1)\nt ∑\nk=1\nkn ≤ t(t+ 1)n . (20)\nLemma 2. Let n ≥ 0 be an integer. Then, it holds that\n(n+ 1) t ∑\nk=1\nkn ≥ (t+ 1)n+1 − (n+ 1) 2\n2n+ 1 (t+ 1)n . (21)\nLemma 3. Let n ≥ 0 be an integer. Then, it holds that\n(2n+ 1)t\nt ∑\nk=1\nk2n ≤ (n+ 1)2 ( t ∑\nk=1\nkn\n)2\n. (22)\nNow we are ready to move on with the analysis of the variable shrinking case.\nTheorem 2. The margin perceptron with variable shrinking converges in a finite number tc of updates satisfying the bound\ntc ≤ tb ≡ (n+ 1)2\n2n+ 1\n(\n1 + 2b\nηR2\n)\nR2 γ2d . (23)\nMoreover, the zero-threshold solution hyperplane possesses margin γ′d which is a fraction f of the maximum margin γd obeying the inequality\nf ≡ γ ′ d\nγd >\n2n+ 1\n2n+ 2\n(\n1 + ηR2\n2b\n)−1\n. (24)\nFinally, an after-run lower bound on f involving the margin γ′d achieved, the length ‖atc‖ of the solution weight vector atc and the number tc of updates is given by\nf ≥ η tc ∑\nk=1\nkn γ′d\n‖atc‖ . (25)\nProof. Taking the inner product of (6) with the optimal direction u and using (1) we get u · at+1 − u · at = η(t+ 1)nu · yk ≥ η(t+ 1)nγd a repeated application of which, taking into account that a0 = 0, gives\n‖at‖ ≥ u · at ≥ ηγd t ∑\nk=1\nkn . (26)\nFrom (6) and (7) we obtain\n‖at+1‖2−‖at‖2 = η2(t+1)2n ‖yk‖2+2η(t+1)nat ·yk ≤ (η2R2+2ηb)(t+1)2n .\nA repeated application of the above inequality, assuming a0 = 0, leads to\n‖at‖2 ≤ (η2R2 + 2ηb) t ∑\nk=1\nk2n . (27)\nCombining (26) and (27) we obtain\nη2γ2d\n(\nt ∑\nk=1\nkn\n)2\n≤ ‖at‖2 ≤ (η2R2 + 2ηb) t ∑\nk=1\nk2n (28)\nor\nt ≤ ( R2 + 2b/η\nγ2d\n)\n(\nt t ∑\nk=1\nk2n\n)(\nt ∑\nk=1\nkn\n)−2\nwhich by virtue of (22) gives (23). Upon convergence of the algorithm in tc updates condition (7) is violated by all patterns. Therefore, the margin γ′d achieved satisfies γ ′ d > b(tc + 1)\nn/‖atc‖. Thus,\nf2 = γ′d\n2\nγ2d >\nb2(tc + 1) 2n\nγ2d ‖atc‖ 2 ≥\nb2(tc + 1) 2n\nγ2d(η 2R2 + 2ηb) ∑tc k=1 k\n2n ≥ (2n+ 1)b\n2\nγ2d(ηR 2 + 2b)ηtc\n.\n(29)\nHere we replaced ‖atc‖2 with its upper bound (η2R2 +2ηb) ∑tc k=1 k 2n from (27) and ∑tc\nk=1 k 2n with its upper bound tc(tc + 1) 2n/(2n + 1) from (20). Overapproximating tc by tb in (29) and substituting the value of the latter from (23) we get\nf2 > (2n+ 1)b2\nγ2d(ηR 2 + 2b)ηtb\n=\n(\n(2n+ 1)\n(n+ 1)\nb\n(ηR2 + 2b)\n)2\nfrom where by taking the square root we obtain (24). Finally, (25) is readily obtained if in the ratio γ′d/γd we employ the upper bound on γd derivable from (26). ⊓⊔\nRemark 5. Let us define δ ≡ ηR2/b and ǫ ≡ (n + 1)−1. Then, (23) and (24) become\ntc ≤ 1\nǫδ\n(\n1 + δ/2 1− ǫ/2\n)\nR2 γ2d\nand\nf > 1− ǫ/2 1 + δ/2 ,\nrespectively. The perceptron with margin corresponds to n = 0 or ǫ = 1. If we choose δ ≪ 1 while keeping ǫ (i.e., n) fixed the upper bound on the number of updates becomes O (\nδ−1R2/γ2d )\nand the before-run lower bound on f approaches as δ → 0 the value 1 − ǫ/2. Then, by allowing ǫ → 0 (i.e., n → ∞) maximum margin solutions are approximated. If we set, instead, δ = ǫ ≪ 1 then f > 1− ǫ and the algorithm achieves “accuracy” ǫ in at most ǫ−2R2/γ2d + O ( ǫ−1R2/γ2d ) updates. This is among the best bounds of perceptron-like approximate maximum margin classifiers which do not assume knowledge of the value of γd in any way. For comparison, ALMA’s bound is ≃ 8ǫ−2R2/γ2d.\nRemark 6. Given that f2 ≤ 1 (29) leads to a lower bound on the number tc of updates required for convergence of the margin perceptron with variable shrinking which in terms of the parameters δ and ǫ reads\ntc > 1\nǫδ\n(\n1− ǫ/2 1 + δ/2\n)\nR2 γ2d .\nAs δ, ǫ → 0 the ratio of the above lower bound to the upper bound tends to 1 and the algorithm approaches the optimal solution in ≃ (ǫδ)−1R2/γ2d updates.\nRemark 7. Theorems 1 and 2 hold also for the algorithms described by (2) and (3) as appropriate provided, of course, that ‖atc‖ is replaced in (10) and (25) with ∥\n∥a s tc\n∥ ∥ by making use of the relation connecting these two quantities.\nRemark 8. The after-run lower bounds on f given by (10) and (25) typically provide estimates of the margin achieved which are much more accurate than the ones obtained from the before-run bounds of (9) and (24), respectively. Our experience based on such estimates suggests that a satisfactory approximation of the maximum margin solution can be obtained without the need to resort to\nvery small values of the parameter ǫ. In other words, although the theoretically guaranteed before-run fraction of the maximum margin for δ ≪ 1 is close to 1− ǫ/2 both the estimated after-run fraction and the one actually achieved are larger. This is a generic feature of the perceptron with margin and its generalizations. It turns out that in most cases ǫ ≃ 0.2− 0.3 is sufficiently small for the algorithm to obtain for δ ≪ 1 solutions possessing 99% of the maximum margin. Thus, for constant shrinking a very accurate knowledge of the value of γd is not required while for variable shrinking very low values of n are sufficient."
    }, {
      "heading" : "4 Implementation and Experiments",
      "text" : "To reduce the computational cost we adopt a two-member nested sequence of reduced “active sets” of data points as described in detail in [15]. The parameter c̄ which multiplies the threshold of the misclassification condition when this condition is used to select the points of the first-level active set is given the value c̄ = 1.01. The parameters, instead, determining the number of times the active sets are presented to the algorithm are set to the values Nep1 = Nep2 = 5.\nAn additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14–16] once a data point is presented to the algorithm. It is understood, of course, that a multiple update should be equivalent to a certain number of updates occurring as a result of repeatedly presenting to the algorithm the data point in question. Thus, the maximal multiplicity of such an update will be determined by the requirement that the pattern yk which satisfies the misclassification condition will just violate it as a result of the multiple update. For constant shrinking a multiple update is\nat+µ = at + 1− (1− ηλ)µ λ(1 − ηλ)t+µ−1 yk\nwith\nµ ≤ [\n1 ln(1− ηλ)−1 ln ( 1 + λ b − (1− ηλ)t−1at · yk\n‖yk‖2 − λb\n)]\n+ 1 .\nHere [x] is the integer part of x ≥ 0. For variable shrinking, instead, finding the maximal multiplicity of the update involves solving a (n+1)-th degree equation for which there is no general formula unless n ≤ 3. However, this does not pose a serious problem for several reasons. First of all, as we have already pointed out in Remark 8, we may reach very good approximations of the maximal margin hyperplane with low values of n. In addition, even if we choose a larger n we may obtain satisfactory performance with updates having multiplicity lower than the maximal one. Thus, it suffices to find a lower bound on the relevant root of the (n+1)-th degree equation. Moreover, even when the exact root is available it is often preferable to set an upper bound ℓup on the multiplicity of the updates.\nThe aim of our experiments is to assess the ability of the margin perceptron with constant shrinking (MPCS) and the margin perceptron with variable shrinking (MPVS) to achieve fast convergence to a certain approximation of the\noptimal solution in the feature space where the patterns are linearly separable. For linearly separable data the feature space is the initial instance space. For inseparable data, instead, a space extended by m dimensions, as many as the instances, is considered where each instance is placed at a distance ∆ from the origin in the corresponding dimension2 [5]. This extension generates a margin of at least∆/ √ m and its employment relies on the well-known equivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function ‖w‖2 +∆−2 ∑\niξi 2\ninvolving the weight vector w and the 2-norm of the slacks ξi [2].\nIn the experiments the augmentation parameter ρ was set to the value ρ = 1. The values of the parameter ∆ together with the number of instances and attributes of the datasets used are given in Table 1. Further details may be found in [16]. The experiments, like the ones of [16], were conducted on a 2.5 GHz Intel Core 2 Duo processor with 3 GB RAM running Windows Vista. Therefore, the runtimes reported here can be directly compared to the ones of [16]. Our codes written in C++ were compiled using the g++ compiler under Cygwin. They are available at http://users.auth.gr/costapan.\nIn the numerical experiments the results of which we report in Table 1 the algorithms MPCS and MPVS were required to obtain solutions possessing 99% of the maximum margin γd. Additionally, we imposed a cut-off value ℓup = 1000 on the multiplicity of the updates. We set b = R2 such that δ = ηR2/b = η for both algorithms. For MPCS assuming knowledge of γd we chose λ ≃ 0.75γ2d/b such that ǫ ≃ 0.25. In the case of MPVS we set n = 3 giving ǫ = (n+1)−1 = 0.25. Thus, for both algorithms the asymptotic value of the theoretically guaranteed fraction of γd that they were able to achieve in the limit δ → 0 was 1 − ǫ/2 ≃ 0.875. The lower bound on the fraction f reported is the after-run bound of (10)\n2 yk = [ȳk, lk∆δ1k, . . . , lk∆δmk], where δij is Kronecker’s δ and ȳk the projection of the kth extended instance yk (multiplied by its label lk) onto the initial instance space. The feature space mapping defined by the extension commutes with a possible augmentation (with parameter ρ) in which case ȳk = [lkx̄k, lkρ]. Here x̄k represents the kth data point.\nand (25) which turns out in most cases to be ≃ 0.99 and certainly much larger than the before-run fraction ≃ 0.875 in accordance with our earlier discussion in Remark 8. The required value of the margin was achieved by sufficiently lowering the value of η having knowledge of the target value. However, even if such a knowledge were not available we could have reached our goal guided by the after-run lower bound on f . From Table 1 we see that the runtimes (in seconds) of MPCS and MPVS for the same value γ′d of the margin achieved are comparable. More important, though, is a comparison with the results obtained with other large margin classifiers as reported in [16]. We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVMlight [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14]. We should note, however, that unlike our algorithms linear SVMs are not primal and strictly online.\nFinally, we would like to point out that in practice it is possible to set at one stage the parameter λ of MPCS without prior knowledge of the value of γd. A preliminary run of MPCS with an almost vanishing λ provides a lower bound on γd which is the margin γ ′ d achieved and an upper bound from the after-run lower bound on f . Actually, γd usually lies closer to its upper bound. This information is sufficient to choose λ given that the algorithm is not extremely sensitive to this choice provided, of course, that λ remains below its maximal allowed value."
    }, {
      "heading" : "5 Conclusions",
      "text" : "Motivated by the presence of weight shrinking in most attempts at solving the L1-SVM problem via stochastic gradient descent we introduced this feature into the classical perceptron algorithm with margin. In the case of constant weight decay parameter λ and constant learning rate we demonstrated that convergence to solutions with approximately maximum margin requires λ to approach a margin-dependent maximal allowed value. Scenarios with variable shrinking strength were also considered and proven not to be subject to such limitations. The theoretical analysis was corroborated by an experimental investigation with massive datasets which involved searching for large margin solutions in an extended feature space, a problem equivalent to the 2-norm soft margin one. As a final conclusion of our study we may say that shrinking of the current weight vector as a first step of the update is able to elevate the margin perceptron to a very effective primal online large margin classifier."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "Proof. We proceed by induction in the integer t. For t = 1 inequality (20) reduces to (n + 1) ≤ 2n which holds since 2n = (1 + 1)n ≥ 1 + n. Now let us assume that (20) holds and prove that (n + 1)\n∑t+1 k=1 k n ≤ (t + 1)(t + 2)n\nor (n + 1) ( (t+ 1)n + ∑t k=1 k n ) ≤ (t + 1)(t + 2)n. Given that (20) holds it\nsuffices to prove that (n + 1)(t + 1)n + t(t + 1)n ≤ (t + 1)(t + 2)n or that (t+ 2)n ≥ (t + 1)n−1(n+ 1 + t). Indeed, (t + 2)n = (t+ 1)n ( 1 + (t+ 1)−1 )n ≥ (t+ 1)n ( 1 + n(t+ 1)−1 ) = (t+ 1)n−1(t+ 1 + n). ⊓⊔"
    }, {
      "heading" : "B Proof of Lemma 2",
      "text" : "Proof. We proceed by induction in the integer t. For t = 1 inequality (21) reduces to (n + 1)(2n+ 1) ≥ 2n (1− n(n− 2)) which holds ∀n ≥ 0. Now let us assume that (21) holds and prove that\n∑t+1 k=1 k n ≥ 1 n+1 (t+ 2) n+1 − n+12n+1 (t+ 2)n. Using (21) we have\n∑t+1 k=1 k n = (t + 1)n + ∑t k=1 k n ≥ (t + 1)n + 1 n+1 (t + 1) n+1 −\nn+1 2n+1 (t + 1) n = 1 n+1 (t + 1) n+1 + n2n+1 (t + 1) n. Thus, it suffices to prove that F (t) ≡ n+12n+1 (t+ 2)n + n2n+1 (t+ 1)n − 1n+1 ( (t+ 2)n+1 − (t+ 1)n+1 )\n≥ 0 or that F (t)/tn ≥ 0. By virtue of the binomial formula F (t)/tn admits the expansion\nF (t)\ntn =\nn ∑\nl=0\nn!\nl!(n− l)!\n(\n(n+ 1)2l + n\n2n+ 1 − 2 l+1 − 1 l + 1\n)\nt−l .\nGiven that ((n + 1)2l + n)(l + 1) − (2l+1 − 1)(2n + 1) = ((l − 3)2l + l + 3)n + (l − 1)2l + 1 ≥ 0 ∀l ≥ 0 the terms in the above expansion are all non-negative implying F (t)/tn ≥ 0. ⊓⊔"
    }, {
      "heading" : "C Proof of Lemma 3",
      "text" : "Proof. We proceed by induction in the integer t. For t = 1 inequality (22) reduces to 2n+1 ≤ (n+1)2 which obviously holds ∀n ≥ 0. Now let us assume that (22) holds and prove that (2n+1)(t+1)\n∑t+1 k=1 k\n2n ≤ (n+1)2 (\n∑t+1 k=1 k\nn )2\n. Using (22)\nwe have (2n + 1)(t + 1) ∑t+1 k=1 k 2n = (2n + 1)(t + 1)\n(\n(t+ 1)2n + ∑t k=1 k 2n ) =\n(2n + 1)(t + 1)2n+1 + t+1 t (2n + 1)t\n∑t\nk=1 k 2n ≤ (2n + 1)(t + 1)2n+1 + t+1 t (n +\n1)2 ( ∑t k=1 k n )2 . Also (n+1)2 ( ∑t+1 k=1 k n )2 = (n+1)2 ( (t+ 1)n + ∑t k=1 k n )2 = (n + 1)2(t+ 1)2n + (n+ 1)2 ( ∑t k=1 k n )2 + 2(n + 1)2(t+ 1)n ∑t k=1 k n. Thus, it\nsuffices to prove that (2n+1)(t+1)2n+1+ t+1 t (n+1)2\n(\n∑t k=1 k n )2 ≤ (n+1)2(t+\n1)2n + (n + 1)2 ( ∑t k=1 k n )2 + 2(n + 1)2(t + 1)n ∑t k=1 k n or, equivalently, that (n + 1) ( 2(n+ 1)t(t+ 1)n − (n+ 1) ∑t\nk=1 k n ) ∑t k=1 k n + (n + 1)2t(t + 1)2n −\n(2n+1)t(t+1)2n+1 ≥ 0. Replacing in the above inequality (n+1) ∑t k=1 k n with its upper bound t(t+ 1)n from (20) we end up with the inequality (n+ 1)(2n+ 1)t(t+1)n ∑t\nk=1 k n+(n+1)2t(t+1)2n−(2n+1)t(t+1)2n+1 ≥ 0 to prove which,\nhowever, is equivalent to (21). ⊓⊔"
    } ],
    "references" : [ {
      "title" : "Tracking the best hyperplane with a simple budget perceptron",
      "author" : [ "N. Cesa-Bianchi", "C. Gentile" ],
      "venue" : "COLT, pp. 483-498",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An introduction to support vector machines",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor" ],
      "venue" : "Cambridge University Press, Cambridge",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The forgetron: A kernel-based perceptron on a fixed budget",
      "author" : [ "O. Dekel", "S. Shalev-Shwartz", "Singer. Y." ],
      "venue" : "NIPS, 18 pp. 259-266",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Pattern classsification and scene analysis",
      "author" : [ "R.O. Duda", "P.E. Hart" ],
      "venue" : "Wiley, Chichester",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Large margin classification using the perceptron algorithm",
      "author" : [ "Y. Freund", "R.E. Shapire" ],
      "venue" : "Machine Learning 37(3), 277-296",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A new approximate maximal margin classification algorithm",
      "author" : [ "C. Gentile" ],
      "venue" : "Journal of Machine Learning Research 2, 213-242",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Making large-scale SVM learning practical",
      "author" : [ "T. Joachims" ],
      "venue" : "Advances in Kernel Methods-Support Vector Learning. MIT Press, Cambridge",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "KDD pp. 217-226",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A dual coordinate descent method for large-scale linear SVM",
      "author" : [ "Hsieh", "C.-J.", "Chang", "K.-W.", "Lin", "C.-J.", "S.S. Keerthi", "S. Sundararajan" ],
      "venue" : "ICML pp. 408-415",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Online learning with kernels",
      "author" : [ "J. Kivinen", "A. Smola", "R. Williamson" ],
      "venue" : "IEEE Transactions on Signal Processing 52(8), 2165-2176",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning algorithms with optimal stability in neural networks",
      "author" : [ "W. Krauth", "M. Mézard" ],
      "venue" : "Journal of Physics A20, L745-L752",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "The relaxed online maximummargin algorithm",
      "author" : [ "Y. Li", "P. Long" ],
      "venue" : "Machine Learning, 46(1-3), 361-387",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "On convergence proofs on perceptrons",
      "author" : [ "A.B.J. Novikoff" ],
      "venue" : "Proc. Symp. Math. Theory Automata, vol. 12, pp. 615-622",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "The margin perceptron with unlearning",
      "author" : [ "C. Panagiotakopoulos", "P. Tsampouka" ],
      "venue" : "ICML pp. 855–862",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The margitron: A generalized perceptron with margin",
      "author" : [ "C. Panagiotakopoulos", "P. Tsampouka" ],
      "venue" : "IEEE Transactions on Neural Networks 22(3), 395-407",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The perceptron with dynamic margin",
      "author" : [ "C. Panagiotakopoulos", "P. Tsampouka" ],
      "venue" : "Kivinen, J., et. al. (eds.) ALT 2011. LNCS (LNAI) vol. 6925, pp. 204-218. Springer, Heidelberg",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sequential minimal optimization: A fast algorithm for training support vector machines",
      "author" : [ "J.C. Platt" ],
      "venue" : "Microsoft Res. Redmond WA, Tech. Rep. MSR-TR-98-14",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The perceptron: A probabilistic model for information storage and organization in the brain",
      "author" : [ "F. Rosenblatt" ],
      "venue" : "Psychological Review, 65(6), 386-408",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1958
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for SVM",
      "author" : [ "S. Shalev-Schwartz", "Y. Singer", "N. Srebro" ],
      "venue" : "ICML pp. 807-814",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Analysis of generic perceptron-like large margin classifiers",
      "author" : [ "P. Tsampouka", "J. Shawe-Taylor" ],
      "venue" : "Gama, J., et. al. (eds.) ECML 2005. LNCS (LNAI) vol. 3720, pp. 750-758. Springer, Heidelberg",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Constant rate approximate maximum margin algorithms",
      "author" : [ "P. Tsampouka", "J. Shawe-Taylor" ],
      "venue" : "Fürnkranz, J., et. al. (eds.) ECML 2006. LNCS (LNAI) vol. 4212, pp. 437-448. Springer, Heidelberg",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Approximate maximum margin algorithms with rules controlled by the number of mistakes",
      "author" : [ "P. Tsampouka", "J. Shawe-Taylor" ],
      "venue" : "ICML pp. 903–910",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Statistical learning theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "Wiley, Chichester",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "It is widely accepted that the generalization ability of learning machines improves as the margin of the solution hyperplane increases [23].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 17,
      "context" : "The simplest online learning algorithm for binary linear classification, the perceptron [18, 13], does not aim at any margin.",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "The simplest online learning algorithm for binary linear classification, the perceptron [18, 13], does not aim at any margin.",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : "The problem, instead, of finding the optimal separating hyperplane is central to Support Vector Machines (SVMs) [23, 2].",
      "startOffset" : 112,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "The problem, instead, of finding the optimal separating hyperplane is central to Support Vector Machines (SVMs) [23, 2].",
      "startOffset" : 112,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "To overcome this obstacle decomposition methods [17, 7] were developed that apply optimization only to a subset of the training set.",
      "startOffset" : 48,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "To overcome this obstacle decomposition methods [17, 7] were developed that apply optimization only to a subset of the training set.",
      "startOffset" : 48,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.",
      "startOffset" : 40,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.",
      "startOffset" : 40,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.",
      "startOffset" : 40,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "The first algorithm of the kind is the perceptron with margin [4] the solutions of which provably possess only up to 1/2 of the maximum margin [11].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "The first algorithm of the kind is the perceptron with margin [4] the solutions of which provably possess only up to 1/2 of the maximum margin [11].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "For ROMMA [12] such a rule is the result of a relaxed optimization which reduces all constraints to just two.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a “projection” mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a “projection” mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a “projection” mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "Very recently, the margitron [15] and the perceptron with dynamic margin (PDM) [16] using modified conditions managed to approximately reach maximum margin solutions while maintaining the original perceptron update.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "Very recently, the margitron [15] and the perceptron with dynamic margin (PDM) [16] using modified conditions managed to approximately reach maximum margin solutions while maintaining the original perceptron update.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "Notable representatives of this approach are the pioneer NORMA [10] and Pegasos [19].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "Notable representatives of this approach are the pioneer NORMA [10] and Pegasos [19].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "Shrinking has also been employed by algorithms which do not have their origin in stochastic gradient descent as an accompanying mechanism in perceptron-based budget scenarios for classification [3] or tracking [1].",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "Shrinking has also been employed by algorithms which do not have their origin in stochastic gradient descent as an accompanying mechanism in perceptron-based budget scenarios for classification [3] or tracking [1].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 22,
      "context" : "This training set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality [23, 2].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "This training set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality [23, 2].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : ", by extending xk to [xk, ρ], we construct an embedding of our data into the socalled augmented space [4].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "The maximum directional margin γd is upper bounded by the maximum geometric margin γ in the non-augmented space and tends to it as ρ → ∞ [20].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "A variable shrinking factor t/(t+ λ) is also employed by SPA [1] in which b = 0.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "To reduce the computational cost we adopt a two-member nested sequence of reduced “active sets” of data points as described in detail in [15].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14–16] once a data point is presented to the algorithm.",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14–16] once a data point is presented to the algorithm.",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14–16] once a data point is presented to the algorithm.",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "For inseparable data, instead, a space extended by m dimensions, as many as the instances, is considered where each instance is placed at a distance ∆ from the origin in the corresponding dimension [5].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "This extension generates a margin of at least∆/ √ m and its employment relies on the well-known equivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function ‖w‖ +∆ ∑ iξi 2 involving the weight vector w and the 2-norm of the slacks ξi [2].",
      "startOffset" : 331,
      "endOffset" : 334
    }, {
      "referenceID" : 15,
      "context" : "Further details may be found in [16].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "The experiments, like the ones of [16], were conducted on a 2.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "Therefore, the runtimes reported here can be directly compared to the ones of [16].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "More important, though, is a comparison with the results obtained with other large margin classifiers as reported in [16].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 13,
      "context" : "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].",
      "startOffset" : 200,
      "endOffset" : 204
    } ],
    "year" : 2013,
    "abstractText" : "We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}