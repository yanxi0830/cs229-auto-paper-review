{
  "name" : "1511.07340.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Modular Autoencoders for Ensemble Feature Extraction",
    "authors" : [ "Henry W J Reeve", "Gavin Brown", "Afshin Rostamizadeh" ],
    "emails" : [ "henrywjreeve@gmail.com", "gavin.brown@manchester.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Modularity, Autoencoders, Diversity, Unsupervised, Ensembles"
    }, {
      "heading" : "1. Introduction",
      "text" : "In a wide variety of Machine Learning problems we wish to extract information from high dimensional data sets such as images or documents. Dealing with high dimensional data creates both computational and statistical challenges. One approach to overcoming these challenges is to extract a small set of highly informative features. These features may then be fed into a task dependent learning algorithm. In representation learning these features are learnt directly from the data (Bengio et al., 2013).\nWe consider a modular approach to representation learning. Rather than extracting a single set of features, we extract multiple sets of features. Each of these sets of features is then fed into a separate learning module. These modules may then be trained independently, which addresses both computational challenges, by being easily distributable, and statistical challenges, since each module is tuned to just a small set of features. The outputs of the different classifiers are then combined, giving rise to a classifier ensemble.\nEnsemble methods combine the outputs of a multiplicity of models in order to obtain an enriched hypothesis space whilst controlling variance (Friedman et al., 2001). In this work we shall apply ensemble methods to representation learning in order to extract several subsets of features for an effective classifier ensemble. Successful ensemble learning results from a fruitful trade-off between accuracy and diversity within the ensemble. Diversity\nc©2015 Henry Reeve and Gavin Brown.\nar X\niv :1\n51 1.\n07 34\n0v 1\n[ cs\n.L G\n] 2\nis typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005).\nWe investigate an unsupervised approach to learning a set of diverse but complementary representations from unlabelled data. As such, we move away from the recent trend towards coupled dimensionality reduction in which the tasks of feature extraction and supervised learning are performed in unison Gönen (2014); Storcheus et al. (2015). Whilst coupled dimensionality reduction has been shown to improve accuracy for certain classification tasks Gönen (2014), the unsupervised approach allows us to use unlabelled data to learn a transferable representation which may be used on multiple tasks without the need for retraining Bengio et al. (2013).\nWe show that one can improve the performance of a classifier ensemble by first learning a diverse collection of modular feature extractors in a purely unsupervised way (see Section 4) and then training a set of classifiers independently. Features are extracted using a Modular Autoencoder trained to simultaneously minimise reconstruction error and maximise diversity amongst reconstructions (see Section 2). Though the MAE framework is entirely general to any activation function, in the present paper we focus on the linear case and provide an efficient learning algorithm that converges several orders of magnitude faster than gradient descent (see Section 3). The training scheme involves a hyper-parameter λ. We provide an upper bound on λ, enabling a meaningful trade off between reconstruction error and diversity (see Section 2.2)."
    }, {
      "heading" : "2. Modular Autoencoders",
      "text" : "A Modular Autoencoder consists of an ensemble W = {(Ai,Bi)}Mi=1 consisting of M autoencoder modules (Ai,Bi), where each module consists of an encoder map Bi : RD → RH from a D-dimensional feature space RD to an H-dimensional representation space RH , and a decoder map Ai : RD → RH . For reasons of brevity we focus on the linear case, where Ai ∈MD×H (R) and Bi ∈MH×D (R) are matrices. See Figure 1.\nIn order to train our Modular Autoencoders W we introduce the following loss function\nLλ (W,x) := 1\nM M∑ i=1 reconstruction error︷ ︸︸ ︷ ||AiBix− x||2 −λ · 1 M M∑ i=1 diversity︷ ︸︸ ︷∣∣∣∣∣∣ ∣∣∣∣∣∣AiBix− 1M M∑ j=1 AjBjx ∣∣∣∣∣∣ ∣∣∣∣∣∣ 2 , (1)\nfor feature vectors x ∈ RD. The loss function Lλ (W,x) is inspired by (but not identical to) the Negative Correlation Learning approach of by Liu and Yao for training supervised ensembles of neural networks (Liu and Yao, 1999)1. The first term corresponds to the squared reconstruction error typically minimised by Autoencoders (Bengio et al., 2013). The second term encourages the reconstructions to be diverse, with a view to capturing different factors of variation within the training data. The hyper-parameter λ, known as the diversity parameter, controls the degree of emphasis placed upon these two terms. We discuss its properties in Sections 2.1 and 2.2. Given a data set D ⊂ RD we train a Modular Autoencoder to minimise the error Eλ (W,D), the loss function Lλ (W,x) averaged across the data x ∈ D."
    }, {
      "heading" : "2.1 Between two extremes",
      "text" : "To understand the role of the diversity parameter λ we first look at the two extremes of λ = 0 and λ = 1. If λ = 0 then no emphasis is placed upon diversity. Consequently L0 (W,x) is precisely the average squared error of the individual modules (Ai,Bi). Since there is no interaction term, minimising L0 (W,x) over the training data is equivalent to training each of the auto-encoder modules independently, to minimise squared error. Hence, in the linear case E0 (W,D) is minimised by taking each Bi to be the projection onto the first H principal components of the data covariance (Baldi and Hornik, 1989).\nIf λ = 1 then, by the Ambiguity Decomposition (Krogh et al., 1995),\nL1 (W,x) = ∣∣∣∣∣ ∣∣∣∣∣ 1M M∑ i=1 AiBix− x ∣∣∣∣∣ ∣∣∣∣∣ 2 .\nHence, minimising L1 (W) is equivalent to minimising squared error for a single large Autoencoder (A,B) with an M ·H-dimensional hidden layer, where B = [BT1 , · · · ,BTM ]T and A = M−1[A1, · · · ,AM ].\nConsequently, moving λ between 0 and 1 corresponds to moving from training each of our autoencoder modules independently through to training the entire network as a single monolithic autoencoder."
    }, {
      "heading" : "2.2 Bounds on the diversity parameter",
      "text" : "The diversity parameter λ may be set by optimising the performance of a task-specific system using the extracted sets of features on a validation set. Theorem 1 shows that the search region may be restricted to the closed unit interval [0, 1].\nTheorem 1 Suppose we have a data set D. The following dichotomy holds:\n• If λ ≤ 1 then inf Eλ (W,D) ≥ 0.\n• If λ > 1 then inf Eλ (W,D) = −∞."
    }, {
      "heading" : "In both cases the infimums range over possible parametrisations for the ensemble W.",
      "text" : "Moreover, if the diversity parameter λ > 1 there exist ensembles W with arbitrarily low error Eλ (W,D) and arbitrarily high average reconstruction error.\n1. See the Appendix for details.\nTheorem 1 is a special case of Theorem 3, which is proved the appendix."
    }, {
      "heading" : "3. An Efficient Algorithm for Training Linear Modular Autoencoders",
      "text" : "One method to minimise the error Eλ (W,D) would be to apply some form of gradient descent. However, Linear Modular Autoencoders we can make use of the Singular Value Decomposition to obtain a fast iterative algorithm for minimising the error Eλ (W,D) (see Algorithm 1).\nAlgorithm 1 Backfitting for Linear Modular Autoencoders\nInputs: D × N data matrix X, diversity parameter λ, number of hidden nodes per module H, number of modules M , maximal number of epochs T, Randomly generate {(Ai,Bi)}Mi=1 and set Σ←XXT , for t = 1 to T do\nfor i = 1 to M do Zi ←M−1 ∑ j 6=iAjBj\nΦ← (ID − λ ·Zi) Σ (ID − λ ·Zi)T , where ID denotes the D ×D identity matrix. Ai ← [u1, · · · ,uH ], where {u1, · · · ,uH} are the top eigenvectors of Φ. Bi ← (1− λ · (M − 1)/M)−1 ·AiT (ID − λ ·Zi)\nend for end for return Decoder-Encoder pairs (Ai,Bi) M i=1\nAlgorithm 1 is a simple greedy procedure reminiscent of the back-fitting algorithm for additive models (Friedman et al., 2001). Each module is optimised in turn, leaving the parameters for the other modules fixed. The error Eλ (W,D) decreases every epoch until a critical point is reached.\nTheorem 2 Suppose that Σ = XXT is of full rank. Let (Wt)Tt=1 be a sequence of parameters obtained by Algorithm 1. For every epoch t = {1, · · · , T}, we have Eλ (Wt+1,D) < Eλ (Wt,D), unless Wt is a critical point for Eλ (·,D), in which case Eλ (Wt+1,D) ≤ Eλ (Wt,D).\nTheorem 2 justifies the procedure in Algorithm 1. The proof is given in Appendix B. We compared Algorithm 1 with (batch) gradient descent on an artificial data set consisting of 1000 data points randomly generated from a Gaussian mixture data set consisting of equally weighted spherical Gaussians with standard deviation 0.25 and a mean drawn from a standard multivariate normal distribution. We measured the time for the cost to stop falling by at least = 10−5 per epoch for both Algorithm 1 and (batch) gradient descent. The procedure was repeated ten times. The two algorithms performed similarly in terms of minimum cost attained, with Algorithm 1 attaining slightly lower costs on average. However, as we can see from Table 1, Algorithm 1 converged several orders of magnitude faster than gradient descent."
    }, {
      "heading" : "4. Empirical results",
      "text" : "In this section we demonstrate the efficacy of Modular Autoencoders for extracting useful sets features for classification tasks. In particular, we demonstrate empirically that we can improve the performance of a classifier ensemble by first learning a diverse collection of modular feature extractors in an unsupervised way.\nOur methodology is as follows. We take a training data set D = {(xn, yn)}Nn=1 consisting of pairs of feature vectors xn and class labels yn. The data set D is pre-processed so each of the features have zero mean. We first train a Modular Autoencoder W = (Ai,Bi). For each module i we take Ci to be the 1-nearest neighbour classifier with the data set Di = {(Bixn, yn)}Nn=1. The combined prediction of the ensemble on a test point x is defined by taking a modal average of the class predictions {Ci(Bix)}Mi=1.\nWe use a collection of six image data sets from Larochelle et al. (2007), Basic, Rotations, Background Images and Background Noise variants of MNIST as well as Rectangles and Convex. In each case we use a Modular Autoencoder consisting of ten modules (M = 10), each consisting of ten hidden nodes (H = 10). The five-fold cross-validated test error is shown as a function of the diversity parameter λ. We contrast with a natural baseline approach Bagging Autoencoders (BAE) in which we proceed as described, but the modules (Ai,Bi) are trained independently on bootstrapped samples from the data. In all cases, as the diversity parameter increases from zero the test error for features extracted using Modular Autoencoders falls well below the level attained by Bagging Autoencoders. As λ→ 1 the ensemble error begins to rise, sometimes sharply."
    }, {
      "heading" : "5. Understanding Modular Autoencoders",
      "text" : "In this section we analyse the role of encouraging diversity in an unsupervised way with Modular Autoencoders and the impact this has upon supervised classification."
    }, {
      "heading" : "5.1 A more complex decision boundary",
      "text" : "We begin by considering a simple two-dimensional example consisting of a Gaussian mixture with three clusters. In this setting we use a Linear Modular Autoencoder consisting of two modules, each with a single hidden node, so each of the feature extractors is simply a projection onto a line. We use a linear Softmax classifier on each of the extracted features. The probabilistic outputs of the individual classifiers are then combined by taking the mean average. The predicted label is defined to be the one with the highest probability. Once again we observe the same trend as we saw in Section 4 - encouraging diversity leads to a substantial drop in the test error of our ensemble, with a test error of 21.3± 1.3% for λ = 0 and 12.8± 1.0% for λ = 0.5.\nTo see why this is the case we contrast the features extracted when λ = 0 with those extracted when λ = 0.5. Figure 3 shows the projection of the class densities onto the two extracted features when λ = 0. No emphasis is placed upon diversity the two modules are trained independently to maximise reconstruction. Hence, the features extract identical information and there is no ensemble gain. Figure 5 shows the resultant decision boundary; a simple linear decision boundary based upon a single one-dimensional classification.\nIn contrast, when λ = 0.5 the two features yield diverse and complementary information. As we can see from Figure 4, one feature separates class 1 from classes 2 and 3, and the other separates class 3 from classes 1 and 2. As we can see from the right of Figure 5, the resulting decision boundary accurately reflects the true class boundaries, despite being based upon two independently trained one-dimensional classifiers. This leads to the reduction in test error for λ = 0.5.\nIn general, Modular Autoencoders trained with the loss function defined in (1) extract diverse and complementary sets of features, whilst reflecting the main factors of variation within the data. Simple classifiers may be trained independently based upon these sets of features, so that the combined ensemble system gives rise to a complex decision boundary."
    }, {
      "heading" : "5.2 Diversity of feature extractors",
      "text" : "In this Section we give further insight into the effect of diversity upon Modular Autoencoders. We return to the empirical framework of Section 4. Figure 6 plots two values test error for features extracted with Linear Modular Autoencoders. We plot both the average individual error of the classifiers (without ensembling the outputs) and the test error of the ensemble. In every case the average individual error rises as the diversity parameter moves\naway from zero. Nonetheless, the ensemble error falls as the diversity parameter increases (at least initially).\nTo see why the ensemble error falls whilst the average individual error rises we consider the metric structure of the different sets of extracted features. To compare the metric structure captured by different feature extractors, both with one another, and with the original feature space, we use the concept of distance correlation introduced by (Székely et al., 2007).\nGiven a feature extractor map F (such as x 7→ Bix) we compute D (F,D), the distance correlation based upon the pairs {(F (x), x) : x ∈ D}. The quantity D (F,D) tells us how faithfully the extracted feature space for a feature map F captures the metric structure of the original feature space. For each of our data sets we compute the average value of D (F,D) across the different feature extractors. To reduce computational cost we restrict ourselves to a thousand examples of both train and test data, Dred. Figure 7 shows how the average value of M−1 ∑M i=1D ( Bi,Dred ) varies as a function of the diversity parameter. As we increase the diversity parameter λ we also reduce the emphasis on reconstruction accuracy. Hence, increasing λ reduces the degree to which the extracted features accurately\nreflect the metric structure of the original feature space. This explains the fall in individual classification accuracy we observed in Figure 6.\nGiven feature extractor maps F and G (such as x 7→ Bix and x 7→ Bjx), on a data set D we compute C (F,G,D), the distance correlation based upon the pairs {(F (x), G(x)) : x ∈ D}. The quantity C (F,G,D) gives us a measure of the correlation between the metric structures induced by F and G. Again, to reduce computational cost we restrict ourselves to a thousand examples of both train and test data, Dred. To measure the degree of diversity between our different sets of extracted features we compute the average pairwise correlation C ( Bi,Bj ,Dred ) , averaged across all pairs of distinct feature maps Bi,Bj with i 6= j. Again we restrict ourselves to a thousand out-of-sample examples. Figure 8 shows how the degree of metric correlation between the different sets of extracted features falls as we increase the diversity parameter λ. Increasing λ places an increasing level of emphasis on a diversity of reconstructions. This diversity results in the different classifiers making different errors from one another enabling the improved ensemble performance we observed in Section 4."
    }, {
      "heading" : "6. Discussion",
      "text" : "We have introduced a modular approach to representation learning where an ensemble of auto-encoder modules is learnt so as to achieve a diversity of reconstructions, as well\nas maintaining low reconstruction error for each individual module. We demonstrated empirically, using six benchmark data sets, that we can improve the performance of a classifier ensemble by first learning a diverse collection of modular feature extractors in an unsupervised way. We explored Linear Modular Autoencoders and derived an SVDbased algorithm which converges three orders of magnitude faster than gradient descent. In forthcoming work we extend this concept beyond the realm of auto-encoders and into a broader framework of modular manifold learning."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The research leading to these results has received funding from EPSRC Centre for Doctoral Training grant EP/I028099/1, the EPSRC Anyscale project EP/L000725/1 and from the AXLE project funded by the European Union’s Seventh Framework Programme (FP7/20072013) under grant agreement no 318633. We would also like to thank Kostas Sechidis, Nikolaos Nikolaou, Sarah Nogueira and Charlie Reynolds for their useful comments, and the anonymous referee for suggesting several useful references."
    }, {
      "heading" : "Appendix A. Modular Regression Networks",
      "text" : "We shall consider the more general framework of Modular Regression Networks (MRN) which encompasses Modular Autoencoders (MAE).\nA Modular Regression Network F = {Fi}Mi=1 is an ensemble system consisting of M mappings Fi : RD → RQ. The MRN F is trained using the following loss,\nLλ (F ,x,y) := 1\nM M∑ i=1 error︷ ︸︸ ︷ ||Fi (x)− y||2−λ · 1 M M∑ i=1 diversity︷ ︸︸ ︷∣∣∣∣Fi(x)− F (x)∣∣∣∣2, (2) where x is a feature vector, y a corresponding output, and F denotes the arithmetic average F := 1M ∑M i=1 Fi. Given a data set D = {(xn,yn)} N n=1 we let Eλ (F ,D) denote the loss Lλ (F ,x,y) averaged over (x,y) ∈ D. The MRN F is trained to minimise Eλ (F ,D).\nA.1 Investigating the loss function\nProposition 1 Given λ ∈ [0,∞) and an MRN F , for each example (x,y) we have\nLλ (F ,x,y) = (1− λ) · 1\nM ∑ i ||Fi (x)− y||2 + λ · ∣∣∣∣F (x)− y∣∣∣∣2 .\nProof The result may be deduced from the Ambiguity Decomposition (Krogh et al., 1995).\nThe following proposition relates MRNs to Negative Correlation Learning (Liu and Yao, 1999).\nProposition 2 Given an MRN F and (x,y) ∈ D we have\n∂Lλ (F ,x,y) ∂Fi = 2 M · ( (Fi (x)− y)− λ · ( Fi (x)− F (x) )) .\nProof This follows from the definitions of Lλ (F ,x,y) and F (x).\nIn Negative Correlation Learning each network Fi is updated in parallel with rule\nθi ← θi − α · ∂Fi ∂θi\n( (Fi (x)− y)− λ · ( Fi (x)− F (x) )) ,\nfor each example (x,y) ∈ D in turn, where θi denotes the parameters of Fi and α denotes the learning rate (Liu and Yao, 1999, Equation 4). By Proposition 2 this is equivalent to training a MRN F to minimise Eλ (F ,D) with stochastic gradient descent, using the learning rate M/2 · α.\nA.2 An upper bound on the diversity parameter\nWe now focus on a particular class of networks. Suppose that there exists a vector-valued function ϕ (x; ρ), parametrised by ρ. We assume that ϕ is sufficiently expressive that for\neach possible feature vector x ∈ RD there exists a choice of parameters ρ with ϕ (x; ρ) 6= 0. Suppose that for each i there exists a weight matrix W i and parameter vector ρi such that Fi (x) = W iϕ ( x; ρi ) . We refer to such networks as Modular Linear Top Layer Networks (MLT). This is the natural choice in the context of regression and includes Modular Autoencoders with linear outputs.\nTheorem 3 Suppose we have a MLT F and a dataset D. The following dichotomy holds:\n• If λ ≤ 1 then inf Eλ (F ,D) ≥ 0.\n• If λ > 1 then inf Eλ (F ,D) = −∞."
    }, {
      "heading" : "In both cases the infimums range over possible parametrisations for the MRN F .",
      "text" : "Moreover, if λ > 1 there exists parametrisations of F with arbitrarily low error Eλ (F ,D) and arbitrarily high squared loss for the ensemble output F and average squared loss for the individual regression networks F .\nProof It follows from Proposition 1 that whenever λ ≤ 1, Lλ (F ,x,y) ≥ 0 for all choices of F and all (x,y) ∈ D. This implies the consequent in the case where λ ≤ 1.\nWe now address the implications of λ > 1. Take (x̃, ỹ) ∈ D. By the (MLT) assumption we may find parameters ρ so that ϕ (x̃; ρ) 6= 0. Without loss of generality we may assume that 0 6= c = ϕ1 (x̃; ρ), where ϕ1 (x; ρ) denotes the first coordinate of ϕ (x; ρ). We shall leave ρ fixed and obtain a sequence (Fq)q∈N, where for each q we have F qi (x) = W\n(i,q)ϕ (x; ρ) by choosing W (i,q). First take W (i,q) = 0 for all i = 3, · · · ,M , so\nF (x) = 1\nM (F1(x) + F2(x)) .\nIn addition we choose W (1,q) kl = W (2,q) kl = 0 for all k > 1 or l > 1. Finally we take W (1,q) 11 = c · ( q2 + q ) and W (2,q) 11 = −c · q2. It follows that for each (x,y) ∈ D we have,\nF1(x) = W (1,q)ϕ (x, ρ) = ( c(q2 + q)ϕ1 (x; ρ) , 0, · · · , 0 ) F1(x) = W (2,q)ϕ (x, ρ) = ( −cq2ϕ1 (x; ρ) , 0, · · · , 0 ) ,\nF (x) = ( M−1cqϕ1 (x; ρ) , 0, · · · , 0 ) .\nNoting that ϕ1 (x; ρ) = c 6= 0 and (x̃, ỹ) ∈ D we see that,\n1\nN N∑ n=1 ( F (xn)− yn )2 = Ω(q2). (3)\nOn the other hand we have,\n1\nN N∑ n=1 (F1(xn)− yn)2 = Ω(q4),\nand clearly for all i,\n1\nN N∑ n=1 (F1(xn)− yn)2 > 0.\nHence,\n1\nN N∑ n=1 1 M M∑ i=1 (F1(xn)− yn)2 = Ω(q4).\nCombining with Equation (3) this gives\nEλ (Fq,D) = (1− λ) · Ω(q4) + λ · Ω(q2).\nSince λ > 1 this implies\nEλ (Fq,D) = −Ω(q4). (4)\nBy Equations 3 and 4 we see that for any Q1, Q2 > 1, by choosing q sufficiently large we have\nEλ (Fq,D) < −Q1,\nand\n1\nN N∑ n=1 1 M M∑ i=1 (Fi(xn)− yn)2 ≥ 1 N N∑ n=1 ( F (xn)− yn )2 > Q2,\nThis proves the second item.\nThe impact of Theorem 3 is that whenever λ > 1, minimising Eλ will result in one or more parameters diverging. Moreover, the resultant solutions may be arbitrarily bad in terms of training error, leading to very poor choices of parameters.\nTheorem 4 Suppose we have a MLT F on a data set D. Suppose we choose i ∈ {1, · · · ,M} and fix Fj for all j 6= i. The following dichotomy holds:\n• If λ < MM−1 then inf Eλ (F ,D) > −∞.\n• If λ > MM−1 then inf Eλ (F ,D) = −∞.\nIn both cases the infimums range over possible parameterisations for the function fi, with fj fixed for j 6= i.\nProof We fix Fj for j 6= i. For each pair (x,y) ∈ D we consider Lλ (F) for large ||Fi(x)||. By Proposition 1 we have\nLλ (F ,x,y) = (1− λ) · 1 M Ω ( ||Fi(x)||2 ) + λ · Ω ( || 1 M Fi(x)||2 ) = ( 1− λ · M − 1\nM\n) · Ω ( ||Fi(x)||2 ) .\nHence, if λ < MM−1 we see that Lλ (F ,x,y) is bounded from below for each example (x,y), for all choices of Fi. This implies the first case.\nIn addition, the fact that ϕ(x1, ρ) 6= 0 for some choice of parameters ρ means that we may choose a sequence of parameters such that ||Fi(x)|| → ∞ for one or more examples (x,y) ∈ D. Hence, if λ > MM−1 , we may choose weights so that Lλ (F ,x,y) → −∞ for some examples (x,y) ∈ D. The above asymptotic formula also implies that Lλ (F ,x,y) is uniformly bounded from above when λ > MM−1 . Thus, we have inf Eλ (F ,D) = −∞."
    }, {
      "heading" : "Appendix B. Derivation of the Linear Modular Autoencoder Training",
      "text" : "Algorithm\nIn what follows we fix D,N , and H < D and define\nCD,H := { (A,B) : A ∈ RD×H ,B ∈ RH×D } .\nWe data set D ⊂ RD, with D features and N examples, and let X denote the D×N matrix given by X = [x1, · · · ,xN ].\nGiven any λ ∈ [0,∞) we define our error function by\nEλ (W,D) = 1\nN N∑ n=1  1 M M∑ j=1 ||xn −AjBjxn||2 − λ · ∣∣∣∣∣ ∣∣∣∣∣AjBjxn − 1M M∑ k=1 AkBkxn ∣∣∣∣∣ ∣∣∣∣∣ 2 \n= 1\nN ·M M∑ i=1 ||X −AjBjX||2 − λ · ∣∣∣∣∣ ∣∣∣∣∣AjBjX − 1M M∑ k=1 AkBkX ∣∣∣∣∣ ∣∣∣∣∣ 2  ,\nwhere W = ((Ai,Bi))Mi=1 ∈ (CD,P ) M , and ||·|| denotes the Frobenius matrix norm.\nProposition 3 Suppose we take X so that Σ = XXT has full rank D and choose λ < M/(M − 1). We pick some i ∈ {1, · · · ,M}, and fix Aj ,Bj for each j 6= i. Then we find (Ai,Bi) which minimises Eλ (W,D) by\n1. Taking Ai to be the matrix whose columns consist of the D unit eigenvectors with largest eigenvalues for the matrixID − λ\nM ∑ j 6=i AjBj\nΣ ID − λ\nM ∑ j 6=i AjBj T , 2. Choosing Bi so that\nBi =\n( 1− λ · M − 1\nM\n)−1 ·ATi ID − λ M ∑ j 6=i AjBj  .\nMoreover, for any other decoder-encoder pair ( Ãi, B̃i ) which also minimises Eλ (W,D)\n(with the remaining pairs Aj ,Bj fixed) we have ÃiB̃i = AiBi.\nProposition 3 implies the following proposition from Section 3.\nTheorem 2 Suppose that Σ is of full rank. Let (Wt)Tt=1 be a sequence of parameters obtained by Algorithm 1. For every epoch t = {1, · · · , T}, we have Eλ (Wt+1,D) < Eλ (Wt,D), unless Wt is a critical point for Eλ (·,D), in which case Eλ (Wt+1,D) ≤ Eλ (Wt,D).\nProof By Proposition 3, each update in Algorithm 1 modifies a decoder-encoder pair (Ai,Bi) so as to minimise Eλ (W,D), subject to the condition that (Aj ,Bj) remain fixed for j 6= i. Hence, Eλ (Wt+1,D) ≤ Eλ (Wt,D).\nNow suppose Eλ (Wt+1,D) = Eλ (Wt,D) for some t. Note that Eλ (Wt+1,D) is a function of C = {Ci}Mi=1 where Ci = AiBi for i = 1, · · · ,M . We shall show that Ct is a critical point in terms for Eλ. Since Eλ (Wt+1,D) = Eλ (Wt,D) we must have Ct+1i = Cti for i = 1, · · · ,M . Indeed, Proposition 3 implies that Algorithm 1 only modifies Ci when Eλ (W,D) is reduced (although the individual matrices Ai and Bi may be modified). Since Ct+1i = C t i we may infer that C t i attains the minimum value of Eλ (W,D) over the set of parameters such that Cj = C t j for all j 6= i. Hence, at the point Ct we have ∂Eλ/∂Ci = 0 for each i = 1, · · · ,M . Thus, ∂Eλ/∂Ai = 0 and ∂Eλ/∂Bi = 0, for each i, by the chain rule.\nTo prove Proposition 3 we require two intermediary lemmas. The first is a theorem concerning Rank Restricted Linear Regression.\nTheorem 5 Suppose we have D×N data matrices X,Y . We define a function E : CD,H → R by\nE (A,B) = ||Y −ABX||2 .\nSuppose that the matrix XXT is invertible and define Σ := (Y XT )(XXT )−1(XY T ). Let U denote the N × D matrix who’s columns are the D unit eigenvectors of Σ with largest eigen-values. Then the minimum for E is attained by taking,\nA = U B = UT (Y XT )(XXT )−1.\nProof See Baldi and Hornik (1989, Fact 4).\nNote that the minimal solution is not unique. Indeed if A,B attain the minimum, then so does AC, C−1B for any invertible H ×H matrix C.\nLemma 6 Suppose we have D × N matrices X and Y1, · · · ,YQ, and scalars α1, · · · , αQ such that ∑Q q=1 αq > 0. Then we have\narg min (A,B)∈CD,H  Q∑ q=1 αq||Yq −ABX||2 \n= arg min (A,B)∈CD,H ||  Q∑ q=1 α̃qY −ABX||2  ,\nwhere α̃q = αq/ (∑Q q′=1 αq′ ) .\nProof We use the fact that under the Frobenius matrix norm, ||M ||2 = tr(MMT ) for matrices M , where tr denotes the trace operator. Note also that the trace operator is linear and invariant under matrix transpositions. Hence, we have\nQ∑ q=1 αq ||Yq −ABX||2\n= Q∑ q=1 αq · tr ( (Yq −ABX)(Yq −ABX)T ) =\nQ∑ q=1 αq · tr ( YqY T q − 2(AB)XY Tq + (AB)XX(AB)T )\n= Q∑ q=1 αq ||Yq||2 − tr 2(AB)X  Q∑ q=1 αqYq T + tr  Q∑ q=1 αq  (AB)XXT (AB)T  .\nNote that we may add constant terms (ie. terms not depending on A or B) and multiply by positive scalars without changing the minimising argument. Hence, dividing by ∑Q q=1 αq > 0 and adding a constant we see that the minimiser of the above expression is equal to the minimiser of tr ( (AB)XXT (AB)T ) + tr\n2(AB)X  Q∑ q=1 α̃qYq T + tr   Q∑ q=1 α̃qYq  Q∑ q=1 α̃qYq T  .\nMoreover, by the linearity of the trace operator this expression is equal to∣∣∣∣∣∣ ∣∣∣∣∣∣  Q∑ q=1 α̃qYq −ABX ∣∣∣∣∣∣ ∣∣∣∣∣∣ 2 .\nThis proves the lemma.\nProof [Proposition 3] We begin observing that if we fix Aj ,Bj for j 6= i, then minimising Eλ (W,D) is equivalent to minimising\n||X −AiBiX||2 − λ (\n1− 1 M )2 ∣∣∣∣∣∣∣∣ 1M − 1 · S−i −AiBiX ∣∣∣∣∣∣∣∣2−\nλ\nM2 ∑ j 6=i ||(MAjBjX − S−i)−AiBiX||2 ,\nwhere S−i = ∑\nj 6=iAjBjX. This holds as the above expression differs from Eλ (W,D) only by a multiplicative factor of NM and some constant terms which do not depend upon Ai,Bi.\nBy Lemma 6, minimising the above expression in terms of Ai,Bi is equivalent to minimising\n||Y −AiBiX||, (5)\nwith\nY = ( 1− λ (( 1− 1\nM )2 + M − 1 M2 ))−1\n· X − λ · (1− 1\nM )2 1 M − 1 · S−i + 1 M2 ∑ j 6=i (MAjBjX − S−i)  . Here we use the fact that λ < M/(M − 1), so\n1− λ (( 1− 1\nM )2 + M − 1 M2 ) = 1− λ · M − 1 M > 0.\nWe may simplify our expression for Y as follows,\nY = ( 1− λ · M − 1\nM\n)−1 · ID − λ M ∑ j 6=i AjBj X. By Theorem 5, we may minimise the expression in 5 by taking Ai to be the matrix whose columns consist of the D unit eigenvectors with largest eigenvalues for the matrixID − λ\nM ∑ j 6=i AjBj\n(XXT ) ID − λ\nM ∑ j 6=i AjBj T , and setting\nBi =\n( 1− λ · M − 1\nM\n)−1 ·ATi ID − λ M ∑ j 6=i AjBj  . This completes the proof of the proposition."
    } ],
    "references" : [ {
      "title" : "Neural networks and principal component analysis: Learning from examples without local minima",
      "author" : [ "Pierre Baldi", "Kurt Hornik" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "Baldi and Hornik.,? \\Q1989\\E",
      "shortCiteRegEx" : "Baldi and Hornik.",
      "year" : 1989
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Yoshua Bengio", "Aaron Courville", "Pierre Vincent" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Diversity creation methods: a survey and categorisation",
      "author" : [ "Gavin Brown", "Jeremy Wyatt", "Rachel Harris", "Xin Yao" ],
      "venue" : "Information Fusion,",
      "citeRegEx" : "Brown et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2005
    }, {
      "title" : "The elements of statistical learning, volume 1. Springer series in statistics",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2001
    }, {
      "title" : "Coupled dimensionality reduction and classification for supervised and semi-supervised multilabel learning",
      "author" : [ "Mehmet Gönen" ],
      "venue" : "Pattern recognition letters,",
      "citeRegEx" : "Gönen.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gönen.",
      "year" : 2014
    }, {
      "title" : "Neural network ensembles, cross validation, and active learning",
      "author" : [ "Anders Krogh", "Jesper Vedelsby" ],
      "venue" : null,
      "citeRegEx" : "Krogh and Vedelsby,? \\Q1995\\E",
      "shortCiteRegEx" : "Krogh and Vedelsby",
      "year" : 1995
    }, {
      "title" : "An empirical evaluation of deep architectures on problems with many factors of variation",
      "author" : [ "Hugo Larochelle", "Dumitru Erhan", "Aaron Courville", "James Bergstra", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Larochelle et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Larochelle et al\\.",
      "year" : 2007
    }, {
      "title" : "Ensemble learning via negative correlation",
      "author" : [ "Yong Liu", "Xin Yao" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Liu and Yao.,? \\Q1999\\E",
      "shortCiteRegEx" : "Liu and Yao.",
      "year" : 1999
    }, {
      "title" : "Foundations of coupled nonlinear dimensionality reduction",
      "author" : [ "Dmitry Storcheus", "Mehryar Mohri", "Afshin Rostamizadeh" ],
      "venue" : "arXiv preprint arXiv:1509.08880,",
      "citeRegEx" : "Storcheus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Storcheus et al\\.",
      "year" : 2015
    }, {
      "title" : "Measuring and testing dependence by correlation of distances",
      "author" : [ "Gábor J Székely", "Maria L Rizzo", "Nail K Bakirov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Székely et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Székely et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In representation learning these features are learnt directly from the data (Bengio et al., 2013).",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Ensemble methods combine the outputs of a multiplicity of models in order to obtain an enriched hypothesis space whilst controlling variance (Friedman et al., 2001).",
      "startOffset" : 141,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : "is typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005).",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "is typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005). We investigate an unsupervised approach to learning a set of diverse but complementary representations from unlabelled data. As such, we move away from the recent trend towards coupled dimensionality reduction in which the tasks of feature extraction and supervised learning are performed in unison Gönen (2014); Storcheus et al.",
      "startOffset" : 125,
      "endOffset" : 458
    }, {
      "referenceID" : 1,
      "context" : "is typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005). We investigate an unsupervised approach to learning a set of diverse but complementary representations from unlabelled data. As such, we move away from the recent trend towards coupled dimensionality reduction in which the tasks of feature extraction and supervised learning are performed in unison Gönen (2014); Storcheus et al. (2015). Whilst coupled dimensionality reduction has been shown to improve accuracy for certain classification tasks Gönen (2014), the unsupervised approach allows us to use unlabelled data to learn a transferable representation which may be used on multiple tasks without the need for retraining Bengio et al.",
      "startOffset" : 125,
      "endOffset" : 483
    }, {
      "referenceID" : 1,
      "context" : "is typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005). We investigate an unsupervised approach to learning a set of diverse but complementary representations from unlabelled data. As such, we move away from the recent trend towards coupled dimensionality reduction in which the tasks of feature extraction and supervised learning are performed in unison Gönen (2014); Storcheus et al. (2015). Whilst coupled dimensionality reduction has been shown to improve accuracy for certain classification tasks Gönen (2014), the unsupervised approach allows us to use unlabelled data to learn a transferable representation which may be used on multiple tasks without the need for retraining Bengio et al.",
      "startOffset" : 125,
      "endOffset" : 605
    }, {
      "referenceID" : 1,
      "context" : "Whilst coupled dimensionality reduction has been shown to improve accuracy for certain classification tasks Gönen (2014), the unsupervised approach allows us to use unlabelled data to learn a transferable representation which may be used on multiple tasks without the need for retraining Bengio et al. (2013). We show that one can improve the performance of a classifier ensemble by first learning a diverse collection of modular feature extractors in a purely unsupervised way (see Section 4) and then training a set of classifiers independently.",
      "startOffset" : 288,
      "endOffset" : 309
    }, {
      "referenceID" : 7,
      "context" : "The loss function Lλ (W,x) is inspired by (but not identical to) the Negative Correlation Learning approach of by Liu and Yao for training supervised ensembles of neural networks (Liu and Yao, 1999)1.",
      "startOffset" : 179,
      "endOffset" : 198
    }, {
      "referenceID" : 1,
      "context" : "The first term corresponds to the squared reconstruction error typically minimised by Autoencoders (Bengio et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "Hence, in the linear case E0 (W,D) is minimised by taking each Bi to be the projection onto the first H principal components of the data covariance (Baldi and Hornik, 1989).",
      "startOffset" : 148,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "Algorithm 1 is a simple greedy procedure reminiscent of the back-fitting algorithm for additive models (Friedman et al., 2001).",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "We use a collection of six image data sets from Larochelle et al. (2007), Basic, Rotations, Background Images and Background Noise variants of MNIST as well as Rectangles and Convex.",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "To compare the metric structure captured by different feature extractors, both with one another, and with the original feature space, we use the concept of distance correlation introduced by (Székely et al., 2007).",
      "startOffset" : 191,
      "endOffset" : 213
    }, {
      "referenceID" : 7,
      "context" : "The following proposition relates MRNs to Negative Correlation Learning (Liu and Yao, 1999).",
      "startOffset" : 72,
      "endOffset" : 91
    } ],
    "year" : 2015,
    "abstractText" : "We introduce the concept of a Modular Autoencoder (MAE), capable of learning a set of diverse but complementary representations from unlabelled data, that can later be used for supervised tasks. The learning of the representations is controlled by a trade off parameter, and we show on six benchmark datasets the optimum lies between two extremes: a set of smaller, independent autoencoders each with low capacity, versus a single monolithic encoding, outperforming an appropriate baseline. In the present paper we explore the special case of linear MAE, and derive an SVD-based algorithm which converges several orders of magnitude faster than gradient descent.",
    "creator" : "LaTeX with hyperref package"
  }
}