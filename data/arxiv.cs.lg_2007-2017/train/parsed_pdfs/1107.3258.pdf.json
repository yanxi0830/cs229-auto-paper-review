{
  "name" : "1107.3258.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Learning Discrete Graphical Models Using Greedy Methods",
    "authors" : [ "Ali Jalali" ],
    "emails" : [ "alij@mail.utexas.edu", "cjohnson@cs.utexas.edu", "pradeepr@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 7.\n32 58"
    }, {
      "heading" : "1 Introduction",
      "text" : "Undirected graphical models, also known as Markov random fields, are used in a variety of domains, including statistical physics, natural language processing and image analysis among others. In this paper we are concerned with the task of estimating the graph structure G of a Markov random field (MRF) over a discrete random vector X = (X1, X2, . . . , Xp), given n independent and identically distributed samples\n{x(1), x(2), . . . , x(n)}. This underlying graph structure encodes conditional independence assumptions among subsets of the variables, and thus plays an important role in a broad range of applications of MRFs. Existing approaches: Neighborhood Estimation, Greedy Local Search. Methods for estimating such graph structure include those based on constraint and hypothesis testing [22], and those that estimate restricted classes of graph structures such as trees [8], polytrees [11], and hypertrees [23]. A recent class of successful approaches for graphical model structure learning are based on estimating the local neighborhood of each node. One subclass of these for the special case of bounded degree graphs involve the use of exhaustive search so that their computational complexity grows at least as quickly as O(pd), where d is the maximum neighborhood size in the graphical model [1, 4, 9]. Another subclass use convex programs to learn the neighborhood structure: for instance [20, 17, 16] estimate the neighborhood set for each vertex r ∈ V by optimizing its ℓ1-regularized conditional likelihood; [15, 10] use ℓ1/ℓ2-regularized conditional likelihood. Even these methods, however need to solve regularized convex programs with typically polynomial computational cost of O(p4) or O(p6), are still expensive for large problems. Another popular class of approaches are based on using a score metric and searching for the best scoring structure from a candidate set of graph structures. Exact search is typically NP-hard [7]; indeed for general discrete MRFs, not only is the search space intractably large, but calculation of typical score metrics itself is computationally intractable since they involve computing the partition function associated with the Markov random field [26]. Such methods thus have to use approximations and search heuristics for tractable computation. Question: Can one use local procedures that are as inexpensive as the heuristic greedy approaches, and yet come with the strong statistical guarantees of the regularized convex program based approaches? High-dimensional Estimation; Greedy Methods. There has been an increasing focus in recent years on high-dimensional statistical models where the number of parameters p is comparable to or even larger than the number of observations n. It is now well understood that consistent estimation is possible even under such high-dimensional scaling if some low-dimensional structure is imposed on the model space. Of relevance to graphical model structure learning is the structure of sparsity, where a sparse set of non-zero parameters entail a sparse set of edges. A surge of recent work [5, 12] has shown that ℓ1-regularization for learning such sparse models can lead to practical algorithms with strong theoretical guarantees. A line of recent work (cf. paragraph above) has thus leveraged this sparsity inducing nature of ℓ1-regularization, to propose and analyze convex programs based on regularized log-likelihood functions. A related line of recent work on learning sparse models has focused on “stagewise” greedy algorithms. These perform simple forward steps (adding parameters greedily), and possibly also backward steps (removing parameters greedily), and yet provide strong statistical guarantees for the estimate after a finite number of greedy steps. The forward greedy variant which performs just the forward step has appeared in various guises in multiple communities: in machine learning as boosting [13], in function approximation [24], and in signal processing as basis pursuit [6]. In the context of statistical model estimation, Zhang [28] analyzed the forward greedy algorithm for the case of sparse linear regression; and showed that the forward greedy algorithm is sparsistent (consistent for\nmodel selection recovery) under the same “irrepresentable” condition as that required for “sparsistency” of the Lasso. Zhang [27] analyzes a more general greedy algorithm for sparse linear regression that performs forward and backward steps, and showed that it is sparsistent under a weaker restricted eigenvalue condition. Here we ask the question: Can we provide an analysis of a general forward backward algorithm for parameter estimation in general statistical models? Specifically, we need to extend the sparsistency analysis of [28] to general non-linear models, which requires a subtler analysis due to the circular requirement of requiring to control the third order terms in the Taylor series expansion of the log-likelihood, that in turn requires the estimate to be well-behaved. Such extensions in the case of ℓ1-regularization occur for instance in [20, 25, 3]. Our Contributions. In this paper, we address both questions above. In the first part, we analyze the forward backward greedy algorithm [28] for general statistical models. We note that even though we consider the general statistical model case, our analysis is much simpler and accessible than [28], and would be of use even to a reader interested in just the linear model case of Zhang [28]. In the second part, we use this to show that when combined with neighborhood estimation, the forward backward variant applied to local conditional log-likelihoods provides a simple computationally tractable method that adds and deletes edges, but comes with strong sparsistency guarantees. We reiterate that the our first result on the sparsistency of the forward backward greedy algorithm for general objectives is of independent interest even outside the context of graphical models. As we show, the greedy method is better than the ℓ1-regularized counterpart in [20] theoretically, as well as experimentally. The sufficient condition on the parameters imposed by the greedy algorithm is a restricted strong convexity condition [19], which is weaker than the irrepresentable condition required by [20]. Further, the number of samples required for sparsistent graph recovery scales as O(d2 log p), where d is the maximum node degree, in contrast to O(d3 log p) for the ℓ1-regularized counterpart. We corroborate this in our simulations, where we find that the greedy algorithm requires fewer observations than [20] for sparsistent graph recovery."
    }, {
      "heading" : "2 Review, Setup and Notation",
      "text" : ""
    }, {
      "heading" : "2.1 Markov Random Fields",
      "text" : "Let X = (X1, . . . , Xp) be a random vector, each variable Xi taking values in a discrete set X of cardinality m. Let G = (V,E) denote a graph with p nodes, corresponding to the p variables {X1, . . . , Xp}. A pairwise Markov random field over X = (X1, . . . , Xp) is then specified by nodewise and pairwise functions θr : X 7→ R for all r ∈ V , and θrt : X × X 7→ R for all (r, t) ∈ E:\nP(x) ∝ exp {∑\nr∈V\nθr(xr) + ∑\n(r,t)∈E\nθrt(xr, xt) } . (1)\nIn this paper, we largely focus on the case where the variables are binary with X = {−1,+1}, where we can rewrite (1) to the Ising model form [14] for some set of parameters {θr} and {θrt} as\nP(x) ∝ exp {∑\nr∈V\nθrxr + ∑\n(r,t)∈E\nθrtxrxt } . (2)"
    }, {
      "heading" : "2.2 Graphical Model Selection",
      "text" : "Let D := {x(1), . . . , x(n)} denote the set of n samples, where each p-dimensional vector x(i) ∈ {1, . . . ,m}p is drawn i.i.d. from a distribution Pθ∗ of the form (1), for parameters θ∗ and graph G = (V,E∗) over the p variables. Note that the true edge set E∗ can also be expressed as a function of the parameters as\nE∗ = {(r, t) ∈ V × V : θ∗st 6= 0}. (3)\nThe graphical model selection task consists of inferring this edge set E∗ from the samples D. The goal is to construct an estimator Ên for which P[Ên = E∗] → 1 as n → ∞. Denote by N ∗(r) the set of neighbors of a vertex r ∈ V , so that N ∗(r) = {t : (r, t) ∈ E∗}. Then the graphical model selection problem is equivalent to that of estimating the neighborhoods N̂n(r) ⊂ V , so that P[N̂n(r) = N ∗(r); ∀r ∈ V ] → 1 as n → ∞.\nFor any pair of random variables Xr and Xt, the parameter θrt fully characterizes whether there is an edge between them, and can be estimated via its conditional likelihood. In particular, defining Θr := (θr1, . . . , θrp), our goal is to use the conditional likelihood of Xr conditioned on XV \\r to estimate Θr and hence its neighborhood N (r). This conditional distribution of Xr conditioned on XV \\r generated by (2) is given by the logistic model\nP ( Xr = xr ∣∣∣XV \\r = xV \\r ) =\nexp(θrxr + ∑\nt∈V \\r θrtxrxt)\n1 + exp(θr + ∑ r∈V \\r θrtxr) .\nGiven the n samples D, the corresponding conditional log-likelihood is given by\nL(Θr;D) = 1 n\nn∑\ni=1\n  log  1+ exp  θrx(i)+ ∑\nt∈V \\r\nθrtx (i) r x (i) t\n\n\n −θrx(i)r − ∑\nt∈V \\r\nθrtx (i) r x (i) t\n   .\n(4)\nIn Section 4, we study a greedy algorithm (Algorithm 2) that finds these node neighborhoods N̂n(r) = Supp(Θ̂r) of each random variable Xr separately by a greedy stagewise optimization of the conditional log-likelihood of Xr conditioned on XV \\r.\nThe algorithm then combines these neighborhoods to obtain a graph estimate Ê using an “OR” rule: Ên = ∪r{(r, t) : t ∈ N̂n(r)}. Other rules such as the “AND” rule, that add an edge only if it occurs in each of the respective node neighborhoods, could be used to combine the node-neighborhoods to a graph estimate. We show in Theorem 2 that the neighborhood selection by the greedy algorithm succeeds in recovering the exact node-neighborhoods with high probability, so that by a union bound, the graph estimates using either the AND or OR rules would be exact with high probability as well.\nBefore we describe this greedy algorithm and its analysis in Section 4 however, we first consider the general statistical model case in the next section. We first describe the forward backward greedy algorithm of Zhang [28] as applied to general statistical models, followed by a sparsistency analysis for this general case. We then specialize these general results in Section 4 to the graphical model case. The next section is thus of independent interest even outside the context of graphical models.\nAlgorithm 1 Greedy forward-backward algorithm for finding a sparse optimizer of L(·) Input: Data D := {x(1), . . . , x(n)}, Stopping Threshold ǫS , Backward Step Factor ν ∈ (0, 1)\nOutput: Sparse optimizer θ̂\nθ̂(0) ←− 0 and Ŝ(0) ←− φ and k ←− 1\nwhile true do {Forward Step} (j∗, α∗) ←− arg min\nj∈(Ŝ(k−1))c ;α L(θ̂(k−1)+αej ;D)\nŜ(k) ←− Ŝ(k−1) ∪ {j∗} δ (k) f ←− L(θ̂(k−1);D)− L(θ̂(k−1) + α∗ej∗ ;D) if δ(k)f ≤ ǫS then break end if\nθ̂(k) ←− argmin θ\nL ( θ Ŝ(k) ;D )\nk ←− k + 1\nwhile true do {Backward Step} j∗ ←− arg min\nj∈Ŝ(k−1) L(θ̂(k−1) − θ̂(k−1)j ej ;D)\nif L ( θ̂(k−1) − θ̂(k−1)j∗ ej∗ ;D ) − L ( θ̂(k−1);D ) > νδ (k) f then\nbreak end if\nŜ(k−1) ←− Ŝ(k) − {j∗} θ̂(k−1) ←− argmin\nθ L ( θ Ŝ(k−1) ;D )\nk ←− k − 1 end while\nend while"
    }, {
      "heading" : "3 Greedy Algorithm for General Losses",
      "text" : "Consider a random variable Z with distribution P, and let Zn1 := {Z1, . . . , Zn} denote n observations drawn i.i.d. according to P. Suppose we are interested in estimating some parameter θ∗ ∈ Rp of the distribution P that is sparse; denote its number of nonzeroes by s∗ := ‖θ∗‖0. Let L : Rp ×Zn 7→ R be some loss function that assigns a cost to any parameter θ ∈ Rp, for a given set of observations Zn1 . For ease of notation, in the sequel, we adopt the shorthand L(θ) for L(θ;Zn1 ). We assume that θ∗ satisfies EZ [∇L(θ∗)] = 0.\nWe now consider the forward backward greedy algorithm in Algorithm 1 that rewrites the algorithm in [27] to allow for general loss functions. The algorithm starts with an empty set of active variables Ŝ(0) and gradually adds (and removes) vairables\nto the active set until it meets the stopping criterion. This algorithm has two major steps: the forward step and the backward step. In the forward step, the algorithm finds the best next candidate and adds it to the active set as long as it improves the loss function at least by ǫS , otherwise the stopping criterion is met and the algorithm terminates. Then, in the backward step, the algorithm checks the influence of all variables in the presence of the new added variable. If one or some of the previously added variables do not contribute at least νǫS to the loss function, then the algorithm removes them from the active set. This procedure ensures that at each round, the loss function is improved by at least (1− ν)ǫS and hence it terminates within a finite number of steps.\nWe state the assumptions on the loss function so that sparsistency could be guaranteed. Let us first recall the definition of restricted strong convexity from Negahban et al. [18]. Specifically, for a given set S, the loss function is said to satisfy restricted strong convexity (RSC) with parameter κl if\nL(θ +∆;Zn1 )− L(θ;Zn1 )− 〈∇L(θ;Zn1 ),∆〉 ≥ κl 2 ‖∆‖22 for all ∆ ∈ S. (5)\nWe can now define sparsity restricted strong convexity as follows. Specifically, we say that the loss function L satisfies RSC(k) with parameter κl if it satisfies RSC with parameter κl for all sets S ⊆ {1, . . . , p} such that ‖S‖0 ≤ k.\nIn contrast, we say the loss function satisfies restricted strong smoothness (RSS) with parameter κu, for a given set S if\nL(θ +∆;Zn1 )− L(θ;Zn1 )− 〈∇L(θ;Zn1 ),∆〉 ≤ κu 2 ‖∆‖22 for all ∆ ∈ S.\nWe can define RSS(k) similarly: the loss function L satisfies RSS(k) with parameter κu if it satisfies RSS with parameter κu for all sets S ⊆ {1, . . . , p} such that ‖S‖0 ≤ k at all points θ with ‖θ‖0 ≤ k. Given any constants κl and κu, and a sample based loss functionL, we can typically use concentration based arguments to obtain bounds on the sample size required so that the RSS and RSC conditions hold with high probability.\nAnother property of the loss function that we require is an upper bound λn on the ℓ∞ norm of the gradient of the loss at the true parameter θ∗, i.e., λn ≥ ‖∇L(θ∗)‖∞. This captures the “noise level” of the samples with respect to the loss. Here too, we can typically use concentration arguments to show for instance that λn ≤ cn(log(p)/n)1/2, for some constant cn > 0 with high probability.\nTheorem 1 (Sparsistency). Suppose the loss function L(·) satisfies RSC (η s∗) and RSS (η s∗) with parameters κl and κu for some η ≥ 2 + 4ρ2( √ (ρ2 − ρ)/s∗ + √ 2)2 with ρ = κu/κl. Moreover, suppose that the true parameters θ∗ satisfy minj∈S∗ |θ∗j | >√ 32ρǫS/κl. Then if we run Algorithm 1 with stopping threshold ǫS ≥ (8ρη/κl) s∗λ2n, the output θ̂ with support Ŝ satisfies:\n(a) Error Bound: ‖θ̂ − θ∗‖2 ≤ 2κl √ s∗ (λn √ η + √ ǫS √ 2κu).\n(b) No False Exclusions: S∗ − Ŝ = ∅.\n(c) No False Inclusions: Ŝ − S∗ = ∅.\nProof. The proof theorem hinges on three main lemmas: Lemmas 5 and 7 are simple consequences of the forward and backward steps failing when the greedy algorithm stops, and Lemma 6 which uses these two lemmas and extends techniques from [21] and [19] to obtain an ℓ2 error bound on the error. Provided these lemmas hold, we then show below that the greedy algorithm is sparsistent. However, these lemmas require apriori that the RSC and RSS conditions hold for sparsity size |S∗ ∪ Ŝ|. Thus, we use the result in Lemma 8 that if RSC(ηs∗) holds, then the solution when the algorithm terminates satisfies |Ŝ| ≤ (η − 1)s∗, and hence |Ŝ ∪ S∗| ≤ ηs∗. Thus, we can then apply Lemmas 5, 7 and Lemma 6 to complete the proof as detailed below.\n(a) The result follows directly from Lemma 6, and noting that |Ŝ∪S∗| ≤ ηs∗. In that Lemma, we show that the upper bound holds by drawing from fixed point techniques in [21] and [19], and by using a simple consequence of the forward step failing when the greedy algorithm stops.\n(b) Following the argument in [27], we use the chaining argument. For any τ ∈ R, we have\nτ |{j ∈ S∗ − Ŝ : |θ∗j |2 > τ}| ≤ ‖θ∗S∗−Ŝ‖ 2 2 ≤ ‖θ∗ − θ̂‖22\n≤ 8ηs ∗λ2n κ2l + 16κuǫS κ2l |S∗ − Ŝ|,\nwhere the last inequality follows from part (a) and the inequality (a + b)2 ≤ 2a2 + 2b2. Now, setting τ = 32κuǫS\nκ2 l\n, and dividing both sides by τ/2 we get\n2|{j ∈ S∗ − Ŝ : |θ∗j |2 > τ}| ≤ ηs∗λ2n 2κuǫS + |S∗ − Ŝ|.\nSubstituting |{j ∈ S∗ − Ŝ : |θ∗j |2 > τ}| = |S∗ − Ŝ| − |{j ∈ S∗ − Ŝ : |θ∗j |2 ≤ τ}|, we get\n|S∗ − Ŝ| ≤ |{j ∈ S∗ − Ŝ : |θ∗j |2 ≤ τ}|+ ηs∗λ2n 2κuǫS ≤ |{j ∈ S∗ − Ŝ : |θ∗j |2 ≤ τ}|+ 1/2,\ndue to the setting of the stopping threshold ǫS . This in turn entails that\n|S∗ − Ŝ| ≤ |{j ∈ S∗ − Ŝ : |θ∗j |2 ≤ τ}| = 0,\nby our assumption on the size of the minimum entry of θ∗.\n(c) From Lemma 7, which provides a simple consequence of the backward step failing when the greedy algorithm stops, for ∆̂ = θ̂ − θ∗, we have ǫS/κu|Ŝ − S∗| ≤ ‖∆̂Ŝ−S∗‖22 ≤ ‖∆̂‖22, so that using Lemma 6 and that |S∗ − Ŝ| = 0, we obtain that |Ŝ − S∗| ≤ 4ηs ∗λ2nκu\nǫSκ 2 l\n≤ 1/2, due to the setting of the stopping threshold ǫS ."
    }, {
      "heading" : "3.1 Lemmas for Theorem 1",
      "text" : "We list the simple lemmas that characterize the solution obtained when the algorithm terminates, and on which the proof of Theorem 1 hinges.\nAlgorithm 2 Greedy forward-backward algorithm for pairwise discrete graphical model learning\nInput: Data D := {x(1), . . . , x(n)}, Stopping Threshold ǫS , Backward Step Factor ν ∈ (0, 1) Output: Estimated Edges Ê\nfor r ∈ V do Run Algorithm 1 with L(·) described by (4) to get Θr and its support N̂r end for\nOutput Ê = ⋃\nr\n{ (r, t) : t ∈ N̂r }\nLemma 1 (Stopping Forward Step). When the algorithm 1 stops with parameter θ̂ supported on Ŝ, we have\n∣∣∣L ( θ̂ ) −L (θ∗) ∣∣∣ < √ 2 |S∗ − Ŝ|κu ǫS ∥∥∥θ̂ − θ∗ ∥∥∥ 2 .\nLemma 2 (Stopping Backward Step). When the algorithm 1 stops with parameter θ̂ supported on Ŝ, we have\n∥∥∥∆̂Ŝ−S∗ ∥∥∥ 2\n2 ≥ ǫS κu\n∣∣∣Ŝ − S∗ ∣∣∣ .\nLemma 3 (Stopping Error Bound). When the algorithm 1 stops with parameter θ̂ supported on Ŝ, we have\n∥∥∥θ̂ − θ∗ ∥∥∥ 2 ≤ 2 κl\n( λn √∣∣∣S∗ ∪ Ŝ ∣∣∣+ √ 2 ∣∣∣S∗ − Ŝ ∣∣∣κuǫS ) .\nLemma 4 (Stopping Size). If ǫS > λ2n κu\n(√ 2\nη−1 − √ 2 η )−2 and RSC (ηs∗) holds for\nsome η ≥ 2 + 4ρ2 (√\nρ2−ρ s∗\n+ √ 2 )2 , then the algorithm 1 stops with k ≤ (η − 1)s∗.\nNotice that if ǫS ≥ (8ρη/κl) (η2/(4ρ2)) λ2n, then, the assumption of this lemma is satisfied. Hence for large value of s∗ ≥ 8ρ2 > η2/(4ρ2), it suffices to have ǫS ≥ (8ρη/κl) s ∗λ2n."
    }, {
      "heading" : "4 Greedy Algorithm for Pairwise Graphical Models",
      "text" : "Suppose we are given set of n i.i.d. samples D := {x(1), . . . , x(n)}, drawn from a pairwise Ising model as in (2), with parameters θ∗, and graph G = (V,E∗). It will be useful to denote the maximum node-degree in the graph E∗ by d. As we will show, our model selection performance depends critically on this parameter d. We then propose the Algorithm 2 for estimating the underlying graphical model from the n samples D.\nTheorem 2 (Pairwise Sparsistency). Suppose we run Algorithm 2 with stopping threshold ǫS ≥ c1 d log pn , where, d is the maximum node degree in the graphical model, and the true parameters θ∗ satisfy c3√\nd > minj∈S∗ |θ∗j | > c2\n√ ǫS , and further that number\nof samples scales as n > c4 d 2 log p,\nfor some constants c1, c2, c3, c4. Then, with probability at least 1 − c′ exp(−c′′n), the output θ̂ supported on Ŝ satisfies:\n(a) No False Exclusions: E∗ − Ê = ∅.\n(b) No False Inclusions: Ê − E∗ = ∅.\nProof. This theorem is a corollary to our general Theorem 1. We first show that the conditions of Theorem 1 hold under the assumptions in this corollary. RSC, RSS. We first note that the conditional log-likelihood loss function in (4) corresponds to a logistic likelihood. Moreover, the covariates are all binary, and bounded, and hence also sub-Gaussian. [19, 2] analyze the RSC and RSS properties of generalized linear models, of which logistic models are an instance, and show that the following result holds if the covariates are sub-Gaussian. Let ∂L(∆; θ∗) = L(θ∗ + ∆)−L(θ∗)−〈∇L(θ∗),∆〉 be the second order Taylor series remainder. Then, Proposition 2 in [19] states that that there exist constants κl1 and κ l 2, independent of n, p such that with probability at least 1− c1 exp(−c2n), for some constants c1, c2 > 0,\n∂L(∆; θ∗) ≥ κl1‖∆‖2 { ‖∆‖2 − κl2 √ log(p)\nn ‖∆‖1\n} for all ∆ : ‖∆‖2 ≤ 1.\nThus, if ‖∆‖0 ≤ k := ηd, then ‖∆‖1 ≤ √ k‖∆‖2, so that\n∂L(∆; θ∗) ≥ ‖∆‖22 ( κl1 − κl2 √ k log p\nn\n) ≥ κ l 1\n2 ‖∆‖22,\nif n > 4(κl2/κ l 1) 2 ηd log(p). In other words, with probability at least 1−c1 exp(−c2n), the loss functionL satisfies RSC(k) with parameter κl1 providedn > 4(κl2/κl1)2 ηd log(p). Similarly, it follows from [19, 2] that there exist constants κu1 and κ u 2 such that with probability at least 1− c′1 exp(−c′2n),\n∂L(∆; θ∗) ≤ κu1‖∆‖2{‖∆‖2 − κu2‖∆‖1} for all ∆ : ‖∆‖2 ≤ 1,\nso that by a similar argument, with probability at least 1 − c′1 exp(−c′2n), the loss function L satisfies RSS(k) with parameter κu1 provided n > 4(κu2/κu1 )2 ηd log(p). Noise Level. Next, we obtain a bound on the noiselevel λn ≥ ‖∇L(θ∗)‖∞ following similar arguments to [20]. Let W denote the gradient ∇L(θ∗) of the loss function (4). Any entry of W has the form Wt = 1n ∑n i=1 Z (i) rt , where Z (i) rt = x (i) t (x (i) r − P(xr = 1|x(i)\\s )) are zero-mean, i.i.d. and bounded |Z (i) rt | ≤ 1. Thus, an application of Hoeffding’s inequality yields that P[|Wt| > δ] ≤ 2 exp(−2nδ2). Applying a union bound\nover indices in W , we get P[‖W‖∞ > δ] ≤ 2 exp(−2nδ2 + log(p)). Thus, if λn = (log(p)/n)1/2, then ‖W‖∞ ≤ λn with probability at least 1− exp(−nλ2n + log(p)).\nWe can now verify that under the assumptions in the corollary, the conditions on the stopping size ǫS and the minimum absolute value of the non-zero parameters minj∈S∗ |θ∗j | are satisfied. Moreover, from the discussion above, under the sample size scaling in the corollary, the required RSC and RSS conditions hold as well. Thus, Theorem 1 yields that each node neighborhood is recovered with no false exclusions or inclusions with probability at least 1− c′ exp(−c′′n). An application of a union bound over all nodes completes the proof.\nRemarks. The sufficient condition on the parameters imposed by the greedy algorithm is a restricted strong convexity condition [19], which is weaker than the irrepresentable condition required by [20]. Further, the number of samples required for sparsistent graph recovery scales as O(d2 log p), where d is the maximum node degree, in contrast to O(d3 log p) for the ℓ1 regularized counterpart. We corroborate this in our simulations, where we find that the greedy algorithm requires fewer observations than [20] for sparsistent graph recovery.\nWe also note that the result can also be extended to the general pairwise graphical model case, where each random variable takes values in the range {1, . . . ,m}. In that case, the conditional likelihood of each node conditioned on the rest of the nodes takes the form of a multiclass logistic model, and the greedy algorithm would take the form of a “group” forward-backward greedy algorithm, which would add or remove all the parameters corresponding to an edge as a group. Our analysis however naturally extends to such a group greedy setting as well. The analysis for RSC and RSS remains the same and for bounds on λn, see equation (12) in [15]. We defer further discussion on this due to the lack of space."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "We now present experimental results that illustrate the power of Algorithm 2 and support our theoretical guarantees. We simulated structure learning of several different graph structures and compared the learning rates of our method against that of a standard ℓ1-logistic regression method as outlined in [20].\nWe performed experiments using 3 different graph structures: (a) chain (line graph), (b) 4-nearest neighbor (grid graph) and (c) star graph. For each experiment, we assumed a pairwise binary Ising model in which each θ∗rt = ±1 randomly. For each graph type, we generated a set of n samples x(1), ..., x(n) using Gibbs sampling. We then attempted to learn the structure of the model using both Algorithm 2 as well as ℓ1-logistic regression. We then compared the actual graph structure with the empirically learned graph structures. If the graph structures matched completely then we declared the result a success otherwise we declared the result a failure. We compared these results over a range of sample sizes (n) and averaged the results for each sample size over a batch of size 10. For all greedy experiments we set the stopping threshold ǫS = c log(np) n , where c is a tuning constant, as suggested by Theorem 2, and set the\nbackwards step threshold ν = 0.5. For all ℓ1-logistic regression experiments we set the regularization parameter λn = c′ √ log(p)/n, where c′ is set via cross-validation.\nFigure 1 shows the results for the chain (d = 2), grid (d = 4) and star (d = 0.1p) graphs using both Algorithm 2 and ℓ1-logistic regression for three different graph sizes p ∈ {36, 64, 100} with mixed (random sign) couplings. For each sample size, we generated a batch of 10 different graphical models and averaged the probability of success (complete structure learned) over the batch. Each curve then represents the probability of success versus the control parameter β(n, p, d) = n/[20d log(p)] which increases with the sample size n. These results support our theoretical claims and demonstrate the efficiency of the greedy method in comparison to node-wise logistic regression [20]."
    }, {
      "heading" : "A Auxiliary Lemmas for Theorem 1",
      "text" : "In this section, we prove the Lemmas used in the proof of Theorem 1. Note that when the algorithm terminates, the forward step fails to go through. This entails that\nL(θ̂)− inf j∈Ŝc,α∈R L(θ̂ + αej) < ǫS . (6)\nThe next lemma shows that this has the consequence of upper bounding the deviation in loss between the estimated parameters θ̂ and the true parameters θ∗.\nLemma 5 (Stopping Forward Step). When the algorithm stops with parameter θ̂ supported on Ŝ, we have\n∣∣∣L ( θ̂ ) − L (θ∗) ∣∣∣ < √ 2 |S∗ − Ŝ|κu ǫS ∥∥∥θ̂ − θ∗ ∥∥∥ 2 . (7)\nProof. Let ∆̂ = θ∗ − θ̂. For any η ∈ R, we have\nL ( θ̂ + η∆̂jej ) ≤ L ( θ̂ ) + η∇jL ( θ̂ ) ∆̂j + η\n2κu 2 ∆̂2j .\nThus, we can establish\n−|S∗ − Ŝ|ǫS < ∑\nj∈S∗−Ŝ\n( L ( θ̂ + η∆̂jej ) − L ( θ̂ ))\n≤ η ( L (θ∗)− L ( θ̂ ))\n+ η2 κu 2\n∥∥∥∆̂ ∥∥∥ 2\n2 .\nOptimizing the RHS over η, we obtain\n−|S∗ − Ŝ| ǫS < −\n( L(θ∗)− L ( θ̂ ))2\n2 κu ‖∆̂‖22 ,\nwhence the lemma follows.\nLemma 6 (Stopping Error Bound). When the algorithm stops with parameter θ̂ supported on Ŝ, we have\n‖θ̂ − θ∗‖2 ≤ 2\nκl\n( λn √∣∣∣S∗ ∪ Ŝ ∣∣∣+ √ 2 ∣∣∣S∗ − Ŝ ∣∣∣κuǫS ) . (8)\nProof. For ∆ ∈ R, let\nG(∆) = L (θ∗ +∆)− L (θ∗)− √ 2 ∣∣∣S∗ − Ŝ ∣∣∣ κu ǫS ‖∆‖2 .\nIt can be seen that G(0) = 0, and from the previous lemma, G(∆̂) ≤ 0. Further, G(∆) is sub-homogeneous (over a limited range): G(t∆) ≤ tG(∆) for t ∈ [0, 1]. Thus,\nfor a carefully chosen r > 0, if we show that G(∆) > 0 for all ∆ ∈ {∆ : ‖∆‖2 ≤ r, ‖∆‖0 ≤ |S|}, where S = |Ŝ ∪ S∗|, then it follows that ‖∆̂‖2 ≤ r. If not, then there would exist some t ∈ [0, 1) such that ‖t∆̂‖ = r, whence we would arrive at the contradiction\n0 < G(t∆̂) ≤ tG(∆̂) ≤ 0.\nThus, it remains to show that G(∆) > 0 for all ∆ ∈ {∆ : ‖∆‖2 ≤ r, ‖∆‖0 ≤ |S|}. By restricted strong convexity property of L, we have\nL(θ∗ +∆)− L(θ∗) ≥ 〈∇L(θ∗),∆〉+ κl 2 ‖∆‖22 .\nWe can establish\n〈∇L(θ∗),∆〉 ≥ − |〈∇L(θ∗),∆〉| ≥ −‖∇L(θ∗)‖∞ ‖∆‖1 = λn ‖∆‖1 ,\nand hence,\nG(θ∗ +∆) ≥ −λn‖∆‖1 + κl 2 ‖∆‖22 −\n√ 2 ∣∣∣S∗ − Ŝ ∣∣∣ κuǫS‖∆‖2\n> ‖∆‖2 ( κl 2 ‖∆‖2 − λn √∣∣∣S∗ ∪ Ŝ ∣∣∣− √ 2 ∣∣∣S∗ − Ŝ ∣∣∣ κuǫS )\n> 0,\nif ‖∆‖2 = r for\nr = 2\nκl\n( λn √∣∣∣S∗ ∪ Ŝ ∣∣∣+ √ 2 ∣∣∣S∗ − Ŝ ∣∣∣κuǫS ) .\nThis concludes the proof of the lemma.\nNext, we note that when the algorithm terminates, the backward step with the current parameters has failed to go through. This entails that\ninf j∈Ŝ\nL(θ̂ − θ̂jej)− L(θ̂) > ǫS/2. (9)\nThe next lemma shows the consequence of this bound.\nLemma 7 (Stopping Backward Step). When the algorithm stops with parameter θ̂ supported on Ŝ, we have\n∥∥∥∆̂Ŝ−S∗ ∥∥∥ 2 2 ≥ ǫS κu ∣∣∣Ŝ − S∗ ∣∣∣ . (10)\nProof. We have\n|Ŝ − S∗| inf j∈Ŝ\nL(θ̂ − θ̂jej) ≤ ∑\nj∈Ŝ−S∗ L(θ̂ − θ̂jej)\n≤ |Ŝ − S∗|L(θ̂) + ∑\nj∈Ŝ−S∗\n( ∇jL(θ̂) θ̂j +\nκu 2 θ̂2j\n)\n≤ |Ŝ − S∗|L(θ̂) + κu 2 ∥∥∥∆̂Ŝ−S∗ ∥∥∥ 2 2 ,\nwhere the second inequality uses the fact that [∇L(θ̂)]Ŝ = 0. Substituting (9) above, the lemma follows."
    }, {
      "heading" : "B Lemmas on the Stopping Size",
      "text" : "Lemma 8. If ǫS > λ2n κu\n( 1 2ρ √ γ− √ ρ2−ρ k∗√\n1+γ − √ 2 2+γ )−2 and RSC ((2 + γ)k∗) holds for\nsome γ ≥ 4ρ2 (√\nρ2−ρ k∗ +\n√ 2 )2 , then the algorithm stops with k ≤ (1 + γ)k∗.\nProof. Consider the first time the algorithm reaches k = (1 + γ)k∗ + 1, then by Lemma 9 and 11, we have\n√ k − 1− k∗ k − 1 ≤ √ |Ŝ(k−1) − S∗| |Ŝ(k−1) ∪ S∗| ≤ 2κu √ κu(κu − κl)\nκ2l\n√ |Ŝ(k−1) ∪ S∗|\n+ 2κu κl ( λn√ κuǫS + √ 2|S∗ − Ŝ(k−1)| |S∗ ∪ Ŝ(k−1)| )\n≤ 2κuκl\n√( κu κl )2 − κuκl√\nk − 1 + 2κu κl ( λn√ κuǫS + √ 2k∗ k + k∗ − 1 ) .\nHence, we get 1 2ρ √ γ − √ ρ2−ρ k∗√\n1 + γ − √ 2 2 + γ ≤ λn√ κuǫS .\nFor γ ≥ 4ρ2 (√\nρ2−ρ k∗ +\n√ 2 )2 , the LHS is positive and we arrive to a contradiction\nwith the assumption on ǫS .\nWhen the algorithm reaches the support size of k at the beginning of the forward step, i.e., we added the kth variable to the support and the backward step did not remove any variable, let θ̂(k) denote the current parameter and Ŝ(k) = Supp(θ̂(k)) with k = |Ŝ(k)|. Let θ∗ be the target parameter matrix (i.e., E [∇L(θ∗)] = 0), with S∗ = Supp(θ∗) and k∗ = |S∗|. Lemmas 9, 10 and 11 follow along similar lines to\ntheir counterparts in Lemmas 7, 5 and 6 respectively: the latter held when the algorithm terminates, while the lemmas below hold at any iterate θ̂(k) where we have first added the kth variable to the support. We provide their detailed proofs for completeness.\nLemma 9 (General Backward Step). The first time the algorithm reaches a support size of k > k∗+4 ( κu κl )4 +1 at the beginning of the forward step, assumingRSC ( |Ŝ(k) ∪ S∗| ) holds, we have\n∥∥∥θ̂(k−1) Ŝ(k−1)−S∗ ∥∥∥ 2 2 ≥\n  √\n|Ŝ(k−1) − S∗| κu − 2κu √ κu − κl κ2l\n  2\nδ (k) f . (11)\nProof. Under the assumption of the lemma, the immediate previous backward step has not gone through and hence,\ninf j∈Ŝ(k)−S∗\nL ( θ̂(k) − θ̂(k)j ej ) − L ( θ̂(k) ) ≥ δ (k) f\n2 .\nConsequently, we get\n|Ŝ(k−1) − S∗| δ (k) f\n2 ≤\n∑\nj∈Ŝ(k−1)−S∗ L(θ̂(k) − θ̂(k)j ej)− L(θ̂(k))\n≤ κu 2 ∥∥∥θ̂(k) Ŝ(k−1)−S∗ ∥∥∥ 2 2 ≤ κu 2 (∥∥∥θ̂(k−1) Ŝ(k−1)−S∗ ∥∥∥ 2 + ∥∥∥∆(k) ∥∥∥ 2 )2 ,\nwhere, ∆(k) = θ̂(k) Ŝ(k−1) − θ̂(k−1). This entails that   √\n|Ŝ(k−1) − S∗| κu δ (k) f − ∥∥∥∆(k) ∥∥∥ 2\n  2 ≤ ∥∥∥θ̂(k−1)\nŜ(k−1)−S∗\n∥∥∥ 2\n2 .\nThus, it suffices to show that ∥∥∆(k) ∥∥ 2 ≤ 2κu\nκ2 l\n√ (κu − κl)δ(k)f .\nFrom the forward step, we have\nL ( θ̂(k−1) ) − inf j /∈Ŝ(k−1),α∈R L ( θ̂(k−1) + αej ) = δ (k) f .\nLet (j∗, α∗ 6= 0) be the optimizer of the equation above. Now, we have κl 2 ∥∥∥∆(k) ∥∥∥ 2 2 ≤ L ( θ̂ (k) Ŝ(k−1) ) − L ( θ̂(k−1) )\n≤ L ( θ̂ (k)\nŜ(k−1)\n) − L ( θ̂(k) ) + L ( θ̂(k) ) − L ( θ̂(k−1) )\n≤ κu 2 ∣∣∣θ̂(k)j∗ ∣∣∣ 2 − κl 2 ∥∥∥∆(k) ∥∥∥ 2 2 − κl 2 ∣∣∣θ̂(k)j∗ ∣∣∣ 2 .\nHence, ∥∥∆(k) ∥∥2 2 ≤ κu−κl2κl ∣∣∣θ̂(k)j∗ ∣∣∣ 2 and we only need to show that ∣∣∣θ̂(k)j∗ ∣∣∣ ≤ 2κuκl √ 2 κl δ (k) f .\nSince ∣∣∣θ̂(k)j∗ ∣∣∣ ≤ ∣∣∣θ̂(k)j∗ − α∗ ∣∣∣ + |α∗|, we can equivalently control the latter two terms. First, by forward step construction, κl2 |α∗| 2 ≤ L ( θ̂(k−1) ) − L ( θ̂(k−1) + α∗ej∗ ) =\nδ (k) f and hence |α∗| ≤ √ 2 κl δ (k) f . Second, we claim that ∣∣∣θ̂(k)j∗ − α∗ ∣∣∣ ≤ 2κu−κlκl |α∗| and we are done.\nIn contrary, suppose ∣∣∣θ̂(k)j∗ − α∗ ∣∣∣ 2 > ( 2κu−κl κl )2 |α∗|2 ≥ κuκl |α∗| 2. We have\nκl 2 ∣∣∣θ̂(k)j∗ − α∗ ∣∣∣ 2 > κu 2 |α∗|2\n≥ L ( θ̂(k) − α∗ej∗ ) − L ( θ̂(k) ) ≥ L ( θ̂(k) − α∗ej∗ ) − L ( θ̂(k−1) ) + L ( θ̂(k−1) ) − L ( θ̂(k) )\n≥ κl 2\n∥∥∥∆(k) ∥∥∥ 2\n2 + κl 2 ∣∣∣θ̂(k)j∗ − α∗ ∣∣∣ 2 +∇j∗L ( θ̂(k−1) )( θ̂ (k) j∗ − α∗ )\n+ κl 2\n∥∥∥∆(k) ∥∥∥ 2\n2 + κl 2 ∣∣∣θ̂(k)j∗ ∣∣∣ 2 .\nThis is a contradiction provided that κl2 ∣∣∣θ̂(k)j∗ ∣∣∣ 2 + ∇j∗L ( θ̂(k−1) )( θ̂ (k) j∗ − α∗ )\n≥ 0. Later, we will show that Sign ( ∇j∗L ( θ̂(k−1) )) = −Sign (α∗) and κl|α∗| ≤ ∣∣∣∇j∗L ( θ̂(k−1) )∣∣∣ ≤ κu|α∗|. With these, if θ̂ (k) j∗\nα∗ ≤ 1, we have ∇j∗L\n( θ̂(k−1) )( θ̂ (k) j∗ − α∗ )\n≥ 0 and the claim follows. Otherwise, we have ∣∣∣θ̂(k)j∗ ∣∣∣ ≥ ∣∣∣θ̂(k)j∗ ∣∣∣ − |α∗| =\n∣∣∣θ̂(k)j∗ − α∗ ∣∣∣ so that∣∣∣θ̂(k)j∗ ∣∣∣ ≥ 2κuκl |α∗| and hence,\nκl 2 ∣∣∣θ̂(k)j∗ ∣∣∣ 2 +∇j∗L ( θ̂(k−1) )( θ̂ (k) j∗ − α∗ ) ≥ κl 2 2κu κl |α∗| ∣∣∣θ̂(k)j∗ − α∗ ∣∣∣− κu |α∗| ∣∣∣θ̂(k)j∗ − α∗ ∣∣∣\n= 0.\nTo get the claimed properties of ∇j∗L ( θ̂(k−1) ) , note that\nκl 2 |α∗|2 ≤ L\n( θ̂(k−1) ) − L ( θ̂(k−1) + α∗ej∗ )\n≤ −κl 2 |α∗|2 −∇j∗L\n( θ̂(k−1) ) α∗ ,\nand hence Sign ( ∇j∗L ( θ̂(k−1) )) = −Sign (α∗) and κl|α∗| ≤ ∣∣∣∇j∗L ( θ̂(k−1) )∣∣∣. Also, we can establish\nκu 2\n|α∗|2 ≥ L ( θ̂(k−1) ) − L ( θ̂(k−1) + α∗ej∗ )\n≥ −κu 2\n|α∗|2 −∇j∗L ( θ̂(k−1) ) α∗ .\nSince −∇j∗L ( θ̂(k−1) ) α∗ ≥ 0, we can conclude that ∣∣∣∇j∗L ( θ̂(k−1) )∣∣∣ ≤ κu|α∗|. This concludes the proof of the lemma.\nLemma 10 (General Forward Step). The first time the algorithm reaches a support size of k at the beginning of the forward step, we have\n∣∣∣L (θ∗)− L ( θ̂(k−1) )∣∣∣ ≤ √ 2 ∣∣∣S∗ − Ŝ(k−1) ∣∣∣ κu δ(k)f ∥∥∥θ∗ − θ̂(k−1) ∥∥∥ 2 .\nProof. Under the assumption of the lemma, we have\nL ( θ̂(k−1) ) − inf j /∈Ŝ(k−1),α∈R L ( θ̂(k−1) + αej ) = δ (k) f .\nFor any η ∈ R, we have\n− ∣∣∣S∗ − Ŝ(k−1) ∣∣∣ δ(k)f ≤ ∑ j∈S∗−Ŝ(k−1) L ( θ̂(k−1) + ηθ∗j ej ) − L ( θ̂(k−1) )\n≤ η ∑\nj∈S∗−Ŝ(k−1) ∇jL\n( θ̂(k−1) ) θ∗j + η\n2κu 2\n∥∥∥θ∗ − θ̂(k−1) ∥∥∥ 2\n2\n≤ η ( L (θ∗)− L ( θ̂(k−1) )) + η2\nκu 2\n∥∥∥θ∗ − θ̂(k−1) ∥∥∥ 2\n2 .\nOptimizing the RHS over η, we obtain\n|S∗ − Ŝ(k−1)|δ(k)f ≥\n( L (θ∗)− L ( θ̂(k−1) ))2\n2κu ‖θ∗ − θ̂(k−1)‖22 .\nThis concludes the proof of the lemma.\nLemma 11 (General Error Bound). The first time the algorithm reaches a support size of k at the beginning of the forward step, assuming RSC ( |Ŝ(k) ∪ S∗| ) holds, we have\n∥∥∥θ̂(k−1) Ŝ(k−1)−S∗ ∥∥∥ 2 2 ≤ 4κu|S∗ ∪ Ŝ(k−1)|δ(k)f κ2l ( λn√ κuǫS + √ 2|S∗ − Ŝ(k−1)| |S∗ ∪ Ŝ(k−1)| )2 .\nProof. Let\nG (∆) := L(θ∗ +∆)− L(θ∗)− √ 2|S∗ − Ŝ(k−1)|κu δ(k)f ‖∆‖2.\nIt can be seen that G(0) = 0, and from Lemma 10, G(θ̂(k−1)−θ∗) ≤ 0. Further, G(∆) is sub-homogeneous (over a limited range): G(t∆) ≤ tG(∆) for t ∈ [0, 1]. Thus, for\na carefully chosen r > 0, if we show that G(∆) > 0 for all ∆ ∈ {∆ : ‖∆‖2 ≤ r, ‖∆‖0 ≤ |S|}, where S = |Ŝ(k) ∪ S∗|, then it follows that ‖θ̂(k) − θ∗‖2 ≤ r. If not, then there would exist some t ∈ [0, 1) such that ‖t(θ̂(k) − θ∗)‖2 = r, whence we would arrive at the contradiction\n0 < G ( t(θ̂(k) − θ∗) ) ≤ tG ( θ̂(k) − θ∗ ) ≤ 0.\nThus, it remains to show that G(∆) > 0 for all ∆ ∈ {∆ : ‖∆‖2 ≤ r, ‖∆‖0 ≤ |S|}. By RSC, we have\nL(θ∗ +∆)− L(θ∗) ≥ ∇L(θ∗) ·∆+ κl 2 ‖∆‖22.\nWe can establish\n∇L(θ∗) ·∆ ≥ −|∇L(θ∗) ·∆| ≥ −‖∇L(θ∗)‖∞‖∆‖1 = −λn‖∆‖1,\nand hence,\nG(θ∗ +∆) ≥ −λn‖∆‖1 + κl 2 ‖∆‖22 −\n√ 2|S∗ − Ŝ(k−1)|κuδ(k)f ‖∆‖2\n≥ ‖∆‖2 ( κl 2 ‖∆‖2 − λn √ |S∗ ∪ Ŝ(k)| − √ 2|S∗ − Ŝ(k−1)|κuδ(k)f )\n> 0,\nif ‖∆‖2 = r for\nr = 2\nκl\n( λn √ |S∗ ∪ Ŝ(k)|+ √ 2|S∗ − Ŝ(k−1)|κuδ(k)f ) .\nHence,\n∥∥∥θ̂(k−1) Ŝ(k−1)−S∗ ∥∥∥ 2 2 ≤ 4κu|S∗ ∪ Ŝ(k−1)|δ(k)f κ2l\n  λn√\nκuδ (k) f\n+ √ 2|S∗ − Ŝ(k−1)| |S∗ ∪ Ŝ(k−1)|   2 .\nFinally, consider the fact that δ(k)f ≥ ǫS . This concludes the proof of the lemma."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "<lb>In this paper, we address the problem of learning the structure of a pairwise<lb>graphical model from samples in a high-dimensional setting. Our first main re-<lb>sult studies the sparsistency, or consistency in sparsity pattern recovery, properties<lb>of a forward-backward greedy algorithm as applied to general statistical models.<lb>As a special case, we then apply this algorithm to learn the structure of a discrete<lb>graphical model via neighborhood estimation. As a corollary of our general result,<lb>we derive sufficient conditions on the number of samples n, the maximum node-<lb>degree d and the problem size p, as well as other conditions on the model param-<lb>eters, so that the algorithm recovers all the edges with high probability. Our result<lb>guarantees graph selection for samples scaling as n = Ω(d log(p)), in contrast<lb>to existing convex-optimization based algorithms that require a sample complexity<lb>of Ω(d log(p)). Further, the greedy algorithm only requires a restricted strong<lb>convexity condition which is typically milder than irrepresentability assumptions.<lb>We corroborate these results using numerical simulations at the end.",
    "creator" : "dvips(k) 5.98 Copyright 2009 Radical Eye Software"
  }
}