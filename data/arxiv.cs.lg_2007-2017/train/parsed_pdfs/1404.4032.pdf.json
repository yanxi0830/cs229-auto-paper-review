{
  "name" : "1404.4032.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recovery of Coherent Data via Low-Rank Dictionary Pursuit",
    "authors" : [ "Guangcan Liu", "Ping Li" ],
    "emails" : [ "guangcan.liu@rutgers.edu", "pingli@stat.rutgers.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 4.\n40 32\nv2 [\nst at\n.M E\n] 1\n6 Ju"
    }, {
      "heading" : "1. Introduction",
      "text" : "Nowadays our data is often high-dimensional, massive and full of gross errors (e.g., corruptions, outliers and missing measurements). In the presence of gross errors, the classical Principal Component Analysis (PCA) method, which is probably the most widely used tool for data analysis and dimensionality reduction, becomes brittle — A single gross error could render the estimate produced by PCA arbitrarily far from the desired estimate. As a consequence, it is crucial to develop new statistical tools for robustifying PCA. A variety of methods have been proposed and explored in the literature over several decades, e.g., (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010). One of the most exciting methods is probably the so-called RPCA (Robust Principal Component Analysis) method by Candès et al. (2011), built upon the exploration of the following low-rank matrix recovery problem:\nProblem 1 (Low-Rank Matrix Recovery) Suppose we have a data matrix X ∈ Rm×n and we know it can be decomposed as\nX = L0 + S0, (1.1)\nwhere L0 ∈ Rm×n is a low-rank matrix in which each column is a data point drawn from some low-dimensional subspace, and S0 ∈ Rm×n is a sparse matrix supported on Ω ⊆ {1, · · · ,m} × {1, · · · , n}. Except these mild restrictions, both components are arbitrary. The rank of L0 is unknown, the support set Ω (i.e., the locations of the nonzero entries of S0) and its cardinality (i.e., the amount of the nonzero entries of S0) are unknown either. In particular, the magnitudes of the nonzero entries in S0 may be arbitrarily large. Given X, can we recover both L0 and S0, in a scalable and exact fashion?\nThe theory of RPCA tells us that, very generally, when the low-rank matrix L0 satisfies some incoherent conditions (i.e., the coherence parameters of L0 are small), both the low-rank and the sparse matrices can be exactly recovered by using the following convex, potentially scalable program:\nmin L,S\n‖L‖∗ + λ‖S‖1, s.t. X = L+ S, (1.2)\nwhere ‖ · ‖∗ is the nuclear norm (Fazel, 2002) of a matrix, ‖ · ‖1 denotes the ℓ1 norm of a matrix seen as a long vector, and λ > 0 is a parameter. Besides of its elegance in theory, RPCA also has good empirical performance in many practical areas, e.g., image processing (Zhang et al., 2012), computer vision (Peng et al., 2012), radar imaging (Borcea et al., 2012), magnetic resonance imaging (Otazo et al., 2012), etc.\nWhile complete in theory and powerful in reality, RPCA cannot be an ultimate solution to the low-rank matrix recovery Problem 1. Indeed, the method might not produce perfect recovery even when the latent matrix L0 is strictly low-rank. This is because, seen from the aspect of mathematics, RPCA requires L0 to satisfy some incoherent conditions, which, however, might not hold in reality. In a physical sense, the reason is that RPCA captures only the low-rankness property, which should not be the only property of our data, but essentially ignores the extra structures (beyond low-rankness) widely existed in data: Given the situation that L0 is low-rank, i.e., the column vectors of L0 locate on a low-dimensional subspace, it is quite normal that L0 may have some extra structures, which specify in more detail how the data points (i.e., the column vectors of L0) locate on the subspace.\nFigure 1 demonstrates a typical example of extra structures; that is, the clustering structure which is ubiquitous in modern applications (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Soltanolkotabi et al., 2013). Whenever the data is exhibiting some clustering structure, the coherence parameters might be large and therefore RPCA might be unsatisfactory. More precisely, as will be shown in this paper, while the rank of L0 is fixed and the underlying cluster number goes large, the coherence of L0 keeps heightening and thus, arguably, the performance of RPCA drops.\nTo well handle coherent data1, a straightforward approach is to avoid the coherence parameters of L0. Nevertheless, as explained in (Candès et al., 2011; Candès and Recht, 2009), the coherence parameters are indeed necessary for matrix recovery (if there is no additional condition available). Even more, this paper shall further indicate that the coherence parameters are related in nature to some extra structures intrinsically existed in L0 and therefore cannot be discarded simply. Interestingly, we show that it is possible to avoid the coherence parameters by imposing some additional conditions, which are easy to obey in supervised environments and can also be approximately satisfied in unsupervised environments. Our study is based on the following convex program termed Low-Rank Representation (LRR) (Liu et al., 2013):\nmin Z,S\n‖Z‖∗ + λ‖S‖1, s.t. X = AZ + S, (1.3)\nwhere A ∈ Rm×d is a size-d dictionary matrix constructed in advance2, and λ > 0 is a parameter. In order for LRR to avoid the coherence parameters which have potential to\n1. Generally, coherent (resp. incoherent) data refers to the matrices whose coherence parameters are relatively large (resp. small). Yet there is no deterministic threshold to divide all matrices into coherent matrices and incoherent ones. To avoid confusion, in this paper we say that a matrix is incoherent if and only if the column vectors of the matrix are sampled from a single subspace uniformly at random. Apart from this particular case, the matrix is said to be coherent. In that sense, strictly speaking, the “incoherent data” stated in this paper does not exist in realistic environments. 2. Note that it is unimportant to determine the value of d. Suppose Z∗ is the optimal solution with respect to Z. Then LRR uses AZ∗ to restore L0. It is easy to see that LRR falls back to RPCA when A = I (identity matrix), and it can actually be further proved that the recovery produced by LRR is the same as RPCA whenever the dictionary A is orthogonal.\nbe large in the presence of extra structures, we prove that it is sufficient to construct in advance a dictionary matrix A which is low-rank by itself. This additional condition (i.e., the dictionary A is low-rank) gives a generic prescription to defend the possible infections raised by coherent data, providing an elementary criterion for learning the dictionary matrix A. Subsequently, we propose a simple and effective algorithm that utilizes the output of RPCA to construct the dictionary in LRR. Our extensive experiments demonstrated on randomly generated matrices and motion data show promising results. In summary, the contributions of this paper include:\n⋄ For the first time, this paper studies the problem of recovering low-rank, but coherent matrices from their corrupted versions. We investigate the physical regime where coherent data arises — The widely existed clustering structure is a typical example that leads to coherent data. We prove some basic theories for resolving the problem of recovering coherent data, and also establish a practical algorithm that works better than RPCA in our experiments.\n⋄ The studies of this paper help to understand the physical meaning of coherence, which is now standard and widely used in various literatures, e.g., (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Xu et al., 2010; Liu et al., 2012). We show that the coherence parameters are not “assumptions” for accomplishing a proof, but rather some excellent quantities that relate in nature to the extra structures (beyond low-rankness) intrinsically existed in L0.\n⋄ This paper provides insights regarding the LRR model proposed by (Liu et al., 2013). While the special case of A = X has been extensively studied, the LRR model (1.3) with general dictionaries was not fully understood. We show that LRR (1.3) equipped with proper dictionaries could well handle coherent data.\n⋄ The idea of replacing L with AZ is essentially related to the spirit of matrix factorization which has been explored for long, e.g., (Srebro and Jaakkola, 2005; Weimer et al., 2007). In that sense, the explorations of this paper help to understand why factorization techniques are useful.\nThe remainder of this paper is organized as follows. Section 2 summarizes mathematical notations used throughout this paper. In Section 3, we explore the problem of recovering coherent data from corrupted observations, providing some theories and an algorithm for resolving the problem. Section 4 presents the complete proof procedure of our main result. Section 5 demonstrates experimental results and Section 6 concludes this paper."
    }, {
      "heading" : "2. Summary of Main Notations",
      "text" : "Capital letters such as M are used to represent matrices, and accordingly, [M ]ij denotes its (i, j)th entry. Letters U , V , Ω and their variants (complements, subscripts, etc.) are reserved for left singular vectors, right singular vectors and support set, respectively. We slightly abuse the notation U (resp. V ) to denote the linear space spanned by the columns of U (resp. V ), i.e., the column space (resp. row space). The projection onto the column space U , is denoted by PU and given by PU (M) = UUTM , and similarly for the row space\nPV (M) = MV V T . We also abuse the notation Ω to denote the linear space of matrices supported on Ω. Then PΩ and PΩ⊥ respectively denote the projections onto Ω and Ωc such that PΩ + PΩ⊥ = I, where I is the identity operator. The symbol (·)+ denotes the Moore-Penrose pseudoinverse of a matrix: M+ = VMΣ −1 M U T M for a matrix M with Singular Value Decomposition (SVD)3 UMΣMV T M .\nSix different matrix norms are used in this paper. The first three norms are functions of the singular values: 1) The operator norm (i.e., the largest singular value) denoted by ‖M‖, 2) the Frobenius norm (i.e., square root of the sum of squared singular values) denoted by ‖M‖F , and 3) the nuclear norm (i.e., the sum of singular values) denoted by ‖M‖∗. The other three are the ℓ1, ℓ∞ (i.e., sup-norm) and ℓ2,∞ norms of a matrix: ‖M‖1 = ∑\ni,j |[M ]ij |, ‖M‖∞ = maxi,j{|[M ]ij |} and ‖M‖2,∞ = maxj{ √ ∑ i[M ] 2 ij}.\nThe Greek letter µ and its variants (e.g., subscripts and superscripts) are reserved to denote the coherence parameters of a matrix. We shall also reserve two lower case letters, m and n, to respectively denote the data dimension and the number of data points, and we use the following two symbols throughout this paper:\nn1 = max(m,n) and n2 = min(m,n).\nA complete list of notations can be found in Appendix A for convenience of readers."
    }, {
      "heading" : "3. On the Recovery of Coherent Data",
      "text" : "In this section, we shall firstly investigate the physical regime that raises coherent data, and then discuss the problem of recovering coherent data from corrupted observations, providing some basic principles and an algorithm for resolving the problem."
    }, {
      "heading" : "3.1 Coherence Parameters and Their Properties",
      "text" : "Notice that the rank function cannot fully capture all characteristics of L0, and thus it is indeed necessary to define some quantities for measuring the effects of various extra structures (beyond low-rankness) such as the clustering structure demonstrated in Figure 1. The coherence parameters defined in (Candès and Recht, 2009; Candès et al., 2011) are excellent exemplars of such quantities."
    }, {
      "heading" : "3.1.1 µ1 and µ2",
      "text" : "For an m × n matrix L0 with rank r0 and SVD L0 = U0Σ0V T0 , some of its important properties can be characterized by two coherence parameters, denoted as µ1 and µ2. The first coherence parameter, 1 ≤ µ1 ≤ m, which characterizes the column space identified by U0, is defined as\nµ1(L0) = m\nr0 max 1≤i≤m\n‖UT0 ei‖22, (3.4)\n3. In this paper, SVD always refers to skinny SVD. For a rank-r matrix M ∈ Rm×n, its SVD is of the form UMΣMV T M , with UM ∈ R m×r,ΣM ∈ Rr×r and VM ∈ Rn×r.\nwhere ei denotes the ith standard basis. The second coherence parameter, 1 ≤ µ2 ≤ n, which characterizes the row space identified by V0, is defined as\nµ2(L0) = n\nr0 max 1≤j≤n\n‖V T0 ej‖22. (3.5)\nIn (Candès et al., 2011), another coherence parameter, called as the third coherence parameter and denoted as 1 ≤ µ3 ≤ mn, is also introduced:\nµ3(L0) = mn\nr0 (‖U0V T0 ‖∞)2 =\nmn\nr0 max i,j\n(|〈UT0 ei, V T0 ej〉|)2.\nNotice that µ3 is not indispensable, as it is actually a “derivative” of µ1 and µ2: Simple calculations give that µ3 ≤ r0µ1µ2. The analysis of work does not need to access µ3. We include it just for the sake of consistence with (Candès et al., 2011).\nThe analysis in (Candès et al., 2011) merges the above three parameters into a single one: µ(L0) = max{µ1(L0), µ2(L0), µ3(L0)}. As will be seen later, the behaviors of those three coherence parameters are different from each other, and thus it is indeed more adequate to consider them individually."
    }, {
      "heading" : "3.1.2 µ2-phenomenon",
      "text" : "Candès et al. (2011) have proven that the success condition (regarding L0) of RPCA is\nrank (L0) ≤ n2\ncrµ(L0)(log n1)2 , (3.6)\nwhere µ(L0) = max{µ1(L0), µ2(L0), µ3(L0)} and cr > 1 is some numerical constant. So, RPCA will be less successful when the coherence parameters are considerably larger: The success condition (3.6) is narrowed when µ(L0) goes large. As an extreme example, consider the case where the latent matrix L0 is one in only one column and zero everywhere else. Such a matrix produces µ2(L0) = n ≥ n2, and thus the success condition (3.6) is invalid. In this subsection, we shall further show that the widely existed clustering structure can enlarge the coherence parameters and, accordingly, degrades the performance of RPCA.\nGiven the situation that L0 is low-rank, i.e., rank (L0) ≡ r0 ≪ n2, the data points (i.e., column vectors of L0) should be sampled from a r0-dimensional subspace. Yet the sampling is unnecessary to be uniform. Indeed, a more realistic interpretation is to consider the data points as samples from the union of k number of subspaces (i.e., clusters), and the sum of those multiple subspaces together has a dimension r0. That is to say, there are multiple “small” subspaces inside one r0-dimensional “large” subspace, as exemplified in Figure 1. It is arguable that such a structure of multiple subspaces exists widely in various domains, e.g., face, texture and motion (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Liu et al., 2010a). Whenever the low-rank matrix L0 is exhibiting such clustering behaviors, the second coherence parameter µ2(L0) will increase with the cluster number underlying L0, as shown in Figure 2. When the coherence is heightening, (3.6) suggests that the performance of RPCA will drop, as verified in Figure 2(d). For the ease of citation, we call the phenomena shown in Figure 2(b)∼(d) as the “µ2-phenomenon”.\nTo see why the second coherence parameter increases with the cluster number underlying L0, please refer to Appendix B. As can be seen from Figure 2(a), the first coherence\nparameter µ1 is invariant to the variation of the clustering number. This is because the behaviors of the data points (i.e., column vectors) can only affect the row space, while µ1 is defined on the column space. Nevertheless, if the row vectors of L0 also own some clustering structure, µ1 could be large as well. This kind of data exists widely in text documents and we leave it as future work."
    }, {
      "heading" : "3.2 Avoiding µ2 by LRR",
      "text" : "To accurately recover coherent matrices from their corrupted versions, one may establish some parametric models to capture the extra structures which produce high coherence. However, it is usually hard, if not impossible, to know in advance what kind of extra structures there are and which models are appropriate to use. Even if the modalities of the extra structure are known, e.g., the mixture of multiple subspaces shown in Figure 1, such a strategy still needs to face some difficult problems, e.g., the estimate of the cluster number. In sharp contrast, it is much simpler to devise an approach that can avoid the second coherence parameter µ2. Unfortunately, as explained in (Candès and Recht, 2009; Candès et al., 2011; Liu et al., 2012), the coherence parameters are necessary for identifying accurately the success conditions of matrix recovery. Even more, the µ2-phenomenon actually implies that µ2 is related in nature to some intrinsic structures of L0 and thus cannot be eschewed freely. Interestingly, we shall show that LRR can avoid µ2 by using some additional conditions, which are possible to obey in both supervised and unsupervised environments.\nMain Result: We shall show that, when the dictionary matrix A itself is low-rank, the recovery performance of LRR does not depend on µ2. Our main result is presented in the following theorem (The detailed proof procedure is deferred until Section 4).\nTheorem 1 (Noiseless) Let A ∈ Rm×d with SVD A = UAΣAV TA be a column-wisely unitnormed (i.e., ‖Aei‖2 = 1,∀i) dictionary matrix which satisfies PUA(U0) = U0 (i.e., U0 is a subspace of UA). For any 0 < ǫ < 0.5 and some numerical constant ca > 1, if\nrank (L0) ≤ rank (A) ≤ ǫ2n2\ncaµ1(A) log n1 and |Ω| ≤ (0.5− ǫ)mn, (3.7)\nthen with probability at least 1− n−101 , the optimal solution to the LRR problem (1.3) with λ = 1/ √ n1 is unique and exact, in a sense that\nZ∗ = A+L0 and S ∗ = S0,\nwhere (Z∗, S∗) is the optimal solution to (1.3).\nBy U0 ⊂ UA, the column space of A should approximately have the same properties as L0, and thus, roughly, µ1(A) ≈ µ1(L0). So, as aforementioned, this paper needs to assume that the first coherence parameter of L0 is small and only addresses the cases where the second coherence parameter might be large. It is worth noting that the restriction rank (L0) ≤ O(n2/ log n1) is looser than that of PRCA\n4, which requires rank (L0) ≤ O(n2/(log n1)2). The requirement of column-wisely unit-normed dictionary (i.e., ‖Aei‖2 = 1,∀i) is purely for complying the parameter estimate of λ = 1/ √ n1, which is consistent with RPCA. The condition PUA(U0) = U0, i.e., U0 is a subspace of UA, is indispensable if we ask for exact recovery, because PUA(U0) = U0 is implied by the equality AZ∗ = L0. This necessary condition, together with the condition that A is low-rank, indeed provides an elementary criterion for learning the dictionary matrix A in LRR. Figure 3 presents an example, which further confirms our main result: LRR is able to avoid µ2 as long as U0 ⊂ UA and A is low-rank. Note that it is unnecessary for the dictionary A to strictly satisfy UA = U0, and LRR is actually tolerant to the “errors” possibly existing in the dictionary.\nThe LRR program (1.3) is designed for the cases where the uncorrupted observations are noiseless. In reality this is often not true and all entries of X can be contaminated by a small amount of noises, i.e., X = L0 + S0 + N , where N is a matrix of dense Gaussian noises. In this case, the formula of LRR (1.3) need be modified to\nmin Z,S\n‖Z‖∗ + λ‖S‖1, s.t. ‖X −AZ − S‖F ≤ ε, (3.8)\nwhere ε is a parameter that measures the noise level of data. In the experiments of this paper, we consistently use ε = 10−6‖X‖F . In the presence of dense noises, the latent matrices, L0 and S0, cannot be exactly restored. Yet we have the following theorem to guarantee the near recovery property of the solution produced by (3.8) (please refer to Appendix C for the proof):\nTheorem 2 (Noisy) Suppose ‖X−L0−S0‖F ≤ ε. Let A ∈ Rm×d with SVD A = UAΣAV TA be a column-wisely unit-normed dictionary matrix which satisfies PUA(U0) = U0. For any 0 < ǫ < 0.35 and some numerical constant ca > 1, if\nrank (L0) ≤ rank (A) ≤ ǫ2n2\ncaµ1(A) log n1 and |Ω| ≤ (0.35 − ǫ)mn, (3.9)\n4. In terms of exact recovery, O(n2/ log n1) is probably the “finest” bound one could accomplish in theory.\nthen with probability at least 1−n−101 , any solution (Z∗, S∗) to the LRR program (3.8) with λ = 1/ √ n1 gives a near recovery to (L0, S0), in a sense that ‖AZ∗ − L0‖F ≤ 8 √ mnε and\n‖S∗ − S0‖F ≤ (8 √ mn+ 2)ε."
    }, {
      "heading" : "3.3 An Unsupervised Algorithm for Matrix Recovery",
      "text" : "To well handle coherent data, Theorem 1 suggests that, ideally, the dictionary matrix A should be low-rank and satisfy U0 ⊂ UA. In certain supervised environment, this would be easy as one could use clear, well-processed training data to construct the dictionary. In unsupervised environments, however, it is challenging to purse a low-rank dictionary that can also satisfy U0 ⊂ UA, since U0 ⊂ UA is essentially some kind of “weak” supervision information: As long as the dictionary matrix A is low-rank, U0 ⊂ UA forms a prior that L0 is known to be contained by a low-rank subspace identified by UA. Interestingly, as will be shown later, it is possible to approximate the desired dictionary even when no prior about L0 is given.\nWe shall introduce a heuristic algorithm that works distinctly better than RPCA in our experiments. As can be seen from (3.6), RPCA is actually not brittle with respect to coherent data: Except for the extreme case where the coherence parameters reach the upper bound n (or m), RPCA could own a valid condition (although the condition is narrowed) to be exactly successful even when the coherence parameters are considerably large. Based on this, we propose a pretty simple algorithm, as summarized in Algorithm 1, to achieve a solid improvement over RPCA. Our idea is straightforward: We firstly obtain an estimate of L0 by using RPCA and then utilize the estimate to construct the dictionary matrix A. The post-processing steps (Step 2 and Step 3) that slightly modify the solution of RPCA are designed to encourage well-conditioned dictionary, which is the favorite circumstance indicated by Theorem 1.\nAlgorithm 1 Matrix Recovery\ninput: Observed data matrix X ∈ Rm×n. adjustable parameter: λ. 1. Solve for L̂0 by optimizing the RPCA problem (1.2) with λ = 1/ √ n1. 2. Estimate the rank of L̂0 by\nr̂0 = #{i : σi > 10−3σ1},\nwhere σ1, σ2, · · · , σn2 are the singular values of L̂0. 3. Form L̃0 by using the rank-r̂0 approximation of L̂0. That is,\nL̃0 = argmin L\n‖L− L̂0‖2F , s.t. rank (L) ≤ r̂0,\nwhich is solved by SVD. 4. Construct a dictionary Â from L̃0 by normalizing the column vectors of L̃0:\n[Â]:,i = [L̃0]:,i\n‖[L̃0]:,i‖2 , i = 1, · · · , n,\nwhere [·]:,i denotes the ith column of a matrix. 5. Solve for Z∗ by optimizing the LRR problem (1.3) with A = Â and λ = 1/ √ n1. output: ÂZ∗.\nWhenever the recovery produced by RPCA is already exact, the claim in Theorem 1 gives that the recovery produced by our Algorithm 1 is exact as well. When RPCA fails to exactly recover L0, the produced dictionary is still possible to satisfy the success conditions required by Theorem 1, namely A is low-rank and U0 ⊂ UA. This is because those conditions are weaker than A = L0. Thus, in terms of exactly recovering L0 from a given X, the success probability of our Algorithm 1 is greater than or equal to that of RPCA. Also, in a computational sense, Algorithm 1 does not double RPCA, although there are two convex programs in our algorithm. In fact, according to our simulations, usually the computational time of Algorithm 1 is just 1.2 times as much as RPCA. The reason is that, as has been explored by (Liu et al., 2013), the complexity of solving the LRR problem (1.3) is O(n2rA) (assume m = n), which is much lower than that of RPCA (which requires O(n3)) provided that the obtained dictionary matrix A is fairly low-rank (i.e., rA is small).\nOne may have noticed that the procedure of Algorithm 1 could be made iterative, i.e., one can consider ÂZ∗ as a new estimate of L0 and use it to further update the dictionary matrix A, and so on. Nevertheless, we empirically find that such an iterative procedure often converges within two iterations. Hence, for the sake of simplicity, we do not consider the iterative strategies in this paper."
    }, {
      "heading" : "4. Proof of Theorem 1",
      "text" : ""
    }, {
      "heading" : "4.1 Settings and Some Basic Lemmas",
      "text" : "The same as in RPCA (Candès et al., 2011), we assume that the locations of the corrupted entries are selected uniformly at random. In more details, we work with the Bernoulli model Ω = {(i, j) : δij = 1}, where δij ’s are i.i.d. variables taking value one with probability ρ0 = |Ω|/(mn) and zero with probability (1 − ρ0), so that the expected cardinality of Ω is ρ0mn. For the ease of presentation, we assume that the signs of the nonzero entries of S0 are symmetric Bernoulli ±1 values:\n[sign(S0)]ij =\n\n\n 1, with probability ρ02 , 0, with probability 1− ρ0, −1, with probability ρ02 .\nFor general sign matrices, the same as in RPCA (Candès et al., 2011), our Theorem 1 can still be proved by globally placing an elimination theorem and a derandomization scheme. Yet the success conditions in Theorem 2 have not been proven when sign(S0) has an arbitrary distribution, because the elimination theorem does not hold in the noisy case.\nThe following two lemmas are well-known and will be used multiple times in the proof.\nLemma 3 For any matrix M , the following holds:\n1. Let the SVD of M be UMΣMV T M . Then we have ∂‖M‖∗ = {UMV TM + W |UTMW =\n0,WVM = 0, ‖W‖ ≤ 1}.\n2. Let the support set of M be ΩM . Then we have ∂‖M‖1 = {sign(M) + F |PΩM (F ) = 0, ‖F‖∞ ≤ 1}.\nLemma 4 For any matrices M and N of consistent sizes,\n|〈M,N〉| ≤ ‖M‖∞‖N‖1, |〈M,N〉| ≤ ‖M‖F ‖N‖F , ‖MN‖F ≤ ‖M‖‖N‖F , ‖MN‖2,∞ ≤ ‖M‖|N‖2,∞."
    }, {
      "heading" : "4.2 Critical Lemmas",
      "text" : "First of all, we would like to prove that the sparse matrix S0 does not locate in the column space of the dictionary A, i.e., UA ∩ Ω = {0} or ‖PUAPΩ‖ < 1 as equal. Provided that A ∈ Rm×d is fairly low-rank, the analysis in (Candès et al., 2011) gives that\n‖PTAPΩ‖ ≤ √ |Ω| mn + ǫ\nholds with high probability for any ǫ > 0, where TA denotes the linear space given by PUA +PVA −PUAPVA . Since UA ⊂ TA and ‖PUAPΩ‖ ≤ ‖PTAPΩ‖, it is natural to anticipate that ‖PUAPΩ‖ is smaller than 1 with high probability. The difference is that we only need the first coherence parameter µ1 to finish the proof. Following the techniques in (Candès et al., 2011), we have the following lemma to bound the operator norm of PUAPΩ.\nLemma 5 Suppose Ω ∼ Ber(ρ0) with ρ0 < 1. Then for any ǫ > 0,\n‖PUAPΩ‖ ≤ √ ρ0 + ǫ\nholds with probability at least 1− n−101 , provided that\nrank (A) ≤ ǫ 2n2\ncaµ1(A) log n1 .\nProof For any matrix M , we have\nPUA(M) = ∑\ni,j\n〈PUA(M), eieTj 〉eieTj ,\nand so\nPΩ⊥PUA(M) = ∑\ni,j\n(1− δij)〈PUA(M), eieTj 〉eieTj ,\nwhich gives\nPUAPΩ⊥PUA(M) = ∑\ni,j\n(1− δij)〈PUA(M), eieTj 〉PUA(eieTj )\n= ∑\ni,j\n(1− δij)〈M,PUA(eieTj )〉PUA(eieTj ).\nNote that the Frobenius norm of a matrix is equivalent to the vector ℓ2 norm, while considering the matrix as a long vector. In that sense, we have\nPUAPΩ⊥PUA = ∑\ni,j\n(1− δij)PUA(eieTj )⊗ PUA(eieTj ).\nThe definition of µ1(A) gives\n‖PUA(eieTj )‖2F ≤ µ1(A)rA\nm .\nThen by using the results in (Rudelson, 1999) and following the proof procedure of (Candès and Recht, 2009), we have that\n‖(1 − ρ0)PUA − PUAPΩ⊥PUA‖ ≤ (1− ρ0)(φ1\n√\nµ1(A)rA log n1 n2 + φ2\n√\nµ1(A)βrA log n1 n2 )\n≤ φ1\n√\nµ1(A)rA log n1 n2 + φ2\n√\nµ1(A)βrA log n1 n2\nholds with probability at least 1 − n−β1 for some numerical constants φ1 and φ2. For any ǫ > 0, setting β = 10 and ca = (φ1 + √ 10φ2) 2 gives that\n‖(1− ρ0)PUA −PUAPΩ⊥PUA‖ ≤ ǫ\nholds with probability at least 1− n−101 , provided that rA ≤ ǫ2n2/(caµ1(A) log n1). By PUAPΩPUA = −ρ0PUA − ((1− ρ0)PUA − PUAPΩ⊥PUA) and the triangle inequality,\n‖PUAPΩPUA‖ ≤ ‖ρ0PUA‖+ ‖(1 − ρ0)PUA − PUAPΩ⊥PUA‖\n≤ ρ0 + ǫ = |Ω| mn + ǫ.\nFinally, the fact ‖PUAPΩPUA‖ = ‖PUAPΩ‖2 completes the proof.\nWhile the above Lemma implies that ‖PUAPΩ(M)‖F ≤ (ρ0 + ǫ)‖M‖F , we often need to bound the sup-norm of PUAPΩ(M). The next lemma will show that, when the signs of the matrix entries are independent symmetric Bernoulli variables, the sup-norm could be arbitrarily small.\nLemma 6 Suppose P is a symmetric linear projection with ‖P‖ ≤ 2, and Ψ ∈ Rm×n is a random sign matrix with i.i.d. entries distributed as\n[Ψ]ij =\n{\n1, with probability 12 , −1, with probability 12 .\nFor any ǫ > 0,\n‖PUAPPUAPΩ(Ψ)‖∞ ≤ ǫ\nholds with high probability as long as\nrank (A) ≤ ǫ 2n2\ncaµ1(A) log n1 .\nProof Let ξij = [Ψ]ij and\nQ = PUAPPUAPΩ(Ψ) = PUAPPUA( ∑\ni,j\nδijξijeie T j )\n= ∑\ni,j\nδijξijPUAPPUA(eieTj ).\nThen it can be seen that each entry of Q is a sum of independent random variables:\n[Q]i1j1 = ∑\ni,j\nyij with\nyij = δijξij〈PUAPPUA(eieTj ), ei1eTj1〉.\nNote here that the variables δij ’s are fixed and the randomness comes from ξij’s.\nIt is easy to see that E(yij) = 0. We have\n|yij − E(yij)| = |δijξij〈PUAPPUA(eieTj ), ei1eTj1〉| = |δijξij〈PUA(eieTj ),PPUA(ei1eTj1)〉| ≤ ‖PUA(eieTj )‖F ‖PPUA(ei1eTj1)‖F ≤ ‖PUA(eieTj )‖F ‖P‖‖PUA(ei1eTj1)‖F\n≤ 2u1(A)rA m .\nWe also have ∑\ni,j\nV ar(yij) = ∑\nij\n|δij〈PUAPPUA(eieTj ), ei1eTj1〉| 2V ar(ξij)\n= ∑\ni,j\n(δij) 2|〈PUAPPUA(eieTj ), ei1eTj1〉| 2\n= ∑\ni,j\n(δij) 2|〈eieTj ,PUAPPUA(ei1eTj1)〉| 2\n≤ ∑\ni,j\n|〈eieTj ,PUAPPUA(ei1eTj1)〉| 2\n= ‖PUAPPUA(ei1eTj1)‖ 2 F ≤ ‖PUAP‖2‖PUA(ei1eTj1)‖ 2 F ≤ 4µ1(A)rA m .\nThen the proof is finished by using Bernstein’s inequality, which states that for a collection of uniformly bounded independent random variables {yi}pi=1 with |yi − E(yi)| < c,\nPr\n(∣\n∣ ∣ ∣ ∣\np ∑\ni=1\n(yi − E(yi)) ∣ ∣ ∣ ∣\n∣\n> t\n)\n≤ exp ( − 0.5t 2\n∑p i=1 V ar(yi) + ct/3\n)\n.\nThus we have\nPr(|[Q]i1j1 | > ǫ) ≤ exp ( − 0.5ǫ 2\n4µ1(A)rA m + 2ǫµ1(A)rA3m\n)\n≤ exp ( − 1.5ǫ 2m\n(12 + 2ǫ)µ1(A)rA\n)\n.\nBy union bound,\nPr (‖Q‖∞ ≤ ǫ) ≥ 1− n21 exp ( − 1.5ǫ 2m\n(12 + 2ǫ)µ1(A)rA\n)\n≥ 1− n−101 ,\nprovided that rA ≤ ǫ2n2/(caµ1(A) log n1) with ca ≥ 104."
    }, {
      "heading" : "4.3 Dual Conditions",
      "text" : "It remains to prove Theorem 1 by two steps:\n1. Dual Conditions: Identify the sufficient conditions for (Z = A+L0, S = S0) to be the unique optimal solution to the LRR problem (1.3).\n2. Dual Certificates: Show that the dual conditions can be satisfied, that is to say, construct the dual certificates.\nThe dual conditions are presented in the following lemma.\nLemma 7 Let the SVD of A+L0 be UΣV T . Suppose PUA(U0) = U0 and UA ∩ Ω = {0}. Then (A+L0, S0) is the unique optimal solution to (1.3) if there exists a matrix F that obeys\n(a) UV T = λAT (sign(S0) + F ), (b) PΩ(F ) = 0, (c) ‖PΩ⊥(F )‖∞ < 1.\nProof By standard convexity arguments (Rockafellar, 1970), (A+L0, S0) is an optimal solution to (1.3) if\n0 ∈ ∂‖A+L0‖∗ − λAT∂‖S0‖1.\nNote that UV T ∈ ∂‖A+L0‖∗. Furthermore, (b) and (c) imply that sign(S0) +F ∈ ∂‖S0‖1. Thus, the conditions (a), (b) and (c) are sufficient to conclude that (A+L0, S0) is an optimal (but may not be unique) solution to (1.3).\nNext, we shall consider a feasible perturbation (A+L0 +∆1, S0 −∆) and show that the objective strictly increases whenever ∆ 6= 0. By L0 + S0 = X = A(A+L0 +∆1) + S0 −∆,\n∆ = A∆1 and so ∆ ∈ PUA .\nLet H = −PΩ⊥(sign(∆)). Then by Lemma 3, sign(S0) +H is a subgradient of ‖S0‖1. By the convexity of the nuclear norm and ℓ1 norm,\n‖A+L0 +∆1‖∗ + λ‖S0 −∆‖1 ≥‖A+L0‖∗ + λ‖S0‖1 + 〈UV T ,∆1〉 − λ〈sign(S0) +H,∆〉 =‖A+L0‖∗ + λ‖S0‖1 + 〈UV T − λAT sign(S0),∆1〉 − λ〈H,∆〉 =‖A+L0‖∗ + λ‖S0‖1 + 〈UV T − λAT sign(S0),∆1〉+ λ‖PΩ⊥(∆)‖1 =‖A+L0‖∗ + λ‖S0‖1 + λ〈ATF,∆1〉+ λ‖PΩ⊥(∆)‖1 =‖A+L0‖∗ + λ‖S0‖1 + λ〈F,A∆1〉+ λ‖PΩ⊥(∆)‖1 =‖A+L0‖∗ + λ‖S0‖1 + λ〈F,∆〉+ λ‖PΩ⊥(∆)‖1 =‖A+L0‖∗ + λ‖S0‖1 + λ〈PΩ⊥(F ),PΩ⊥(∆)〉+ λ‖PΩ⊥(∆)‖1 ≥‖A+L0‖∗ + λ‖S0‖1 − λ‖PΩ⊥(F )‖∞‖PΩ⊥(∆)‖1 + λ‖PΩ⊥(∆)‖1 =‖A+L0‖∗ + λ‖S0‖1 + λ(1− ‖PΩ⊥(F )‖∞)‖PΩ⊥(∆)‖1.\nBy ∆ ∈ PUA , ‖PΩ⊥(F )‖∞ < 1 and the assumption UA ∩Ω = {0}, we have ‖PΩ⊥(∆)‖1 > 0. Thus, we have\n‖A+L0‖∗ + λ‖S0‖1 + λ(1− ‖PΩ⊥(F )‖∞)‖PΩ⊥(∆)‖1 > ‖A+L0‖∗ + λ‖S0‖1\nstrictly holds unless ∆ = 0. As long as A∆1 = 0, Theorem 4.1 of (Liu et al., 2013) gives that ‖A+L0 + ∆1‖∗ > ‖A+L0‖∗ strictly holds unless ∆1 = 0. Hence, (A+L0, S0) is the unique optimal solution to the LRR problem (1.3)."
    }, {
      "heading" : "4.4 Dual Certificates",
      "text" : "To construct a matrix F which satisfies the dual conditions listed in Lemma 7, we need the inverse of PUAPΩ⊥PUA . The following lemma shows that (PUAPΩ⊥PUA)\n−1 is well defined and has a small operator norm.\nLemma 8 If ‖PUAPΩ‖ < 1, then the operator PUAPΩ⊥PUA is an injection from PUA to PUA, and its inverse operator is given by\nI + ∞ ∑\ni=1\n(PUAPΩPUA)i.\nProof Let ψ ≡ ‖PUAPΩ‖. By ‖PUAPΩPUA‖ = ‖PUAPΩ‖2 = ψ2 < 1, we have that I+∑∞i=1(PUAPΩPUA)i is well defined and has an operator norm not larger than 1/(1−ψ2).\nNote that\nPUAPΩ⊥PUA = PUA(I − PΩ)PUA = PUA(I − PUAPΩPUA).\nThus for any M ∈ PUA the following holds:\nPUAPΩ⊥PUA(I + ∞ ∑\ni=1\n(PUAPΩPUA)i)(M)\n= PUA(I − PUAPΩPUA)(I + ∞ ∑\ni=1\n(PUAPΩPUA)i)(M)\n= PUA(I + ∞ ∑\ni=1\n(PUAPΩPUA)i − PUAPΩPUA − ∞ ∑\ni=2\n(PUAPΩPUA)i)(M)\n= PUA(I + ∞ ∑\ni=1\n(PUAPΩPUA)i − ∞ ∑\ni=1\n(PUAPΩPUA)i)(M)\n= PUA(M) = M.\nThe next lemma completes the construction of the dual certificates.\nLemma 9 Let\nF = PΩ⊥PUA\n(\nI + ∞ ∑\ni=1\n(PUAPΩPUA)i ) ( 1\nλ (AT )+UV T − PUA(sign(S0))\n)\n,\nwhere U and V are the left and right singular vectors of A+L0, respectively. If the conditions stated in (3.7) are obeyed, then the above F using λ = 1/ √ n1 satisfies (with high probability) the dual conditions (a), (b) and (c) in Lemma 7.\nProof (a): We have\nλAT (sign(S0) + F )\n=λAT sign(S0) + λA TPUA(F )\n=λAT sign(S0) + λA TPUAPΩ⊥PUA(I +\n∞ ∑\ni=1\n(PUAPΩPUA)i)( 1\nλ (AT )+UV T − PUA(sign(S0)))\n=λAT sign(S0) + λA T (\n1 λ (AT )+UV T − PUA(sign(S0)))\n=λAT sign(S0) + VAV T A UV T − λATPUA(sign(S0)) =λAT sign(S0)− λATPUA(sign(S0)) + VAV TA UV T =VAV T A UV T = UV T ,\nwhere the last equality follows from Theorem 4.3 of (Liu et al., 2013).\n(b): It is easy to verify that PΩ(F ) = 0.\n(c): Let P = I +∑∞i=1(PUAPΩPUA)i and F = PΩ⊥(F1 − F2), where\nF1 = PUAPPUA( 1\nλ (AT )+UV T ), F2 = PUAPPUA(sign(S0)).\nIn the following, we shall bound the sup-norm of each term individually. The proof for ‖F2‖∞ needs to access the distribution of sign(S0). When the signs of the nonzero entries of S0 are Bernoulli ±1 values, i.e., sign(S0) = PΩ(Ψ) with Ψ being a random sign matrix as in Lemma 6, we have indeed proven\n‖F2‖∞ = ‖PUAPPUAPΩ(Ψ)‖∞ < ǫ,\nprovided that ‖P‖ ≤ 1/(1 − ρ0 − ǫ) ≤ 2, which follows from the condition of ρ0 < 0.5− ǫ. So it remains to prove that\n‖F1‖∞ < 1− ǫ.\nThis seems easy because we could set λ → + ∞. Nevertheless, to prove our main result, Theorem 1, with λ = 1/ √ n1 (which is a good choice in general), one essentially needs to establish an accurate bound for ‖F1‖∞. Even more, the golfing scheme widely adopted by previous literatures is indeed not easy to work with in our setting. Fortunately, we can\nmake use of the particular structure of F1 and devise a simple approach to accomplish the proof. Our idea is based on the following observation: For any matrix Q, the (i1, j1)th entry of the matrix PUAPΩ(Q) is\n[PUAPΩ(Q)]i1j1 = ∑\ni,j\nδij [Q]ij〈eieTj ,PUA(ei1eTj1)〉 = ∑\ni\nδij1 [Q]ij1 [UAU T A ]ii1 ,\nwhich reveals the fact that the absolute value of [F1]i1j1 closely relates to the length of the j1th column of (A\nT )+UV T . So it may not lose much accuracy to use the relaxation of ‖ · ‖∞ ≤ ‖ · ‖2,∞. For the sake of consistency, we use the ℓ2,∞ norm to define as follows the third coherence parameter of L0, associating with a dictionary matrix A:\nDefinition 1 For L0 ∈ Rm×n of rank r0, its third coherence parameter, associating with a non-orthonormal, column-wisely unit-normed dictionary matrix A which also satisfies PUA(U0) = U0, is defined as\nµA3 (L0) = n2(‖(AT )+UV T ‖2,∞)2\n(log n)2r0γA , (4.10)\nwhere U and V are the left and right singular vectors of A+L0, respectively, and γA is the condition number of the matrix A.\nFigure 4 demonstrates some properties about this particular coherence parameter, µA3 . It can be seen that µA3 is approximately a numerical constant equaling to 1, as long as the rank is not too high such that the dictionary matrix A is well-conditioned.\nBy Lemma 5, ‖P‖ ≤ 1/(1 − ρ0 − ǫ). By (4.10),\n‖(AT )+UV T )‖2,∞ ≤\n√\nγAµ A 3 (L0)r0 log n\nn .\nThus we have\n‖F1‖∞ = ‖UAUTAPPUA( 1\nλ (AT )+UV T )‖∞\n≤ max i\n‖eTi UA‖2‖UTAPPUA( 1\nλ (AT )+UV T )‖2,∞\n≤ √\nµ1(A)rA m ‖P‖‖( 1 λ (AT )+UV T )‖2,∞\n≤\n√\nµ1(A)µA3 (L0)γArAr0 log n\nλ √ mn(1− ρ0 − ǫ)\n.\nBy r0 ≤ rA ≤ ǫ2n2/(caµ1(A) log n1) and setting λ = √ µA3 (L0)γA/(µ1(A)n1),\n‖F1‖∞ ≤ ǫ2n2\n√ n1 log n\nca(1 − ρ0 − ǫ) √ mn log n1\n≤ ǫ 2\nca(1− ρ0 − ǫ) .\nSince µA3 (L0)γA/µ1(A) ≈ 1 (provided that A is well-conditioned), we claim λ = 1/ √ n1 for the sake of simplicity5. Now the dual condition ‖PΩ⊥(F )‖∞ < 1 is proved by\n‖F‖∞ < 1,\n← ǫ 2\nca(1− ρ0 − ǫ) < 1− ǫ,\n← ρ0 < 1− 2ǫ, ← ρ0 < 0.5 − ǫ.\nWe claim ρ0 < 0.5 − ǫ instead of ρ0 < 1 − 2ǫ because Lemma 6 requires ‖P‖ ≤ 2, which follows from ρ0 < 0.5− ǫ."
    }, {
      "heading" : "5. Experiments",
      "text" : "Our main result, Theorem 1, is useful in both supervised and unsupervised environments. For the fair of comparison, in the experiments of this paper we shall focus on demonstrating the superiorities of our unsupervised Algorithm 1 over RPCA."
    }, {
      "heading" : "5.1 Results on Randomly Generated Matrices",
      "text" : "We first verify the effectiveness of our Algorithm 1 on randomly generated matrices. We generate a collection of 200×1000 data matrices according to the model of X = PΩ⊥(L0)+ PΩ(S0): Ω is a support set chosen at random; L0 is created by sampling 200 data points from each of 5 randomly generated subspaces, and its values are normalized such that ‖L0‖∞ = 1; S0 is consisting of random values from Bernoulli ±1. The dimension of each subspace varies from 1 to 20 with step size 1, and thus the rank of L0 varies from 5 to 100\n5. This detail also suggests that λ = 1/ √ n1 may not be the “best” choice.\nwith step size 5. The fraction |Ω|/(mn) varies from 2.5% to 50% with step size 2.5%. For each pair of rank and support size (r0, |Ω|), we run 10 trials, resulting in a total of 4000 (20× 20× 10) trials.\nFigure 5 compares our Algorithm 1 to RPCA, both using λ = 1/ √ n1. It can be seen that the learnt dictionary matrix works distinctly better than the identity dictionary adopted by RPCA. Namely, the success area (i.e., the area of the white region) of our algorithm is 46% wider than that of RPCA! One may have noticed that RPCA owns a region to be exactly successful. This is because in these experiments the coherence parameters are not too large, namely µ1(L0) ≤ 3.5 and µ2(L0) ≤ 13.7. Whenever µ2 reaches the upper bound n, e.g., the example shown in Figure 3, the success region of RPCA will vanish."
    }, {
      "heading" : "5.2 Results on Corrupted Motion Sequences",
      "text" : "We now experiment with 11 additional sequences attached to the Hopkins155 (Tron and Vidal, 2007) database. In those sequences, about 10% of the entries in the data matrix of trajectories are unobserved (i.e., missed) due to visual occlusion. We replace each missing entry with a number from Bernoulli ±1, resulting in a collection of corrupted trajectory matrices for evaluating the effectiveness of matrix recovery algorithms. We perform subspace clustering on both the corrupted trajectory matrices and the recovered versions, and use the clustering error rates produced by existing subspace clustering methods as the evaluation metrics. We consider three state-of-the-art subspace clustering methods: Shape Interaction Matrix (SIM) (Costeira and Kanade, 1998), Low-Rank Representation with A = X (Liu et al., 2010b) (which is referred to as “LRRx”) and Sparse Subspace Clustering (SSC) (Elhamifar and Vidal, 2009).\nTable 1 shows the error rates of various algorithms. Without the preprocessing of matrix recovery, all the subspace clustering methods fail to accurately categorize the trajectories of motion objects, producing error rates higher than 20%. This illustrates that it is important\nfor motion segmentation to correct the gross corruptions possibly existing in the data matrix of trajectories. By using RPCA (λ = 1/ √ n1) to correct the corruptions, the clustering performances of all considered methods are improved dramatically. For example, the error rate of SSC is reduced from 22.9% to 9.5%. By choosing a better dictionary (than the identity) for LRR (λ = 1/ √ n1), the error rates can be reduced again, namely from 9.5% to 5.7%, which is a 40% improvement. These results verify the effectiveness of our dictionary learning strategy in realistic environments."
    }, {
      "heading" : "6. Conclusion and Future Work",
      "text" : "In this paper, we studied the problem of disentangling the low-rank (L0) and sparse (S0) components in a given data matrix. Whenever the low-rank component owns some extra structures, the state-of-the-art RPCA method might fail even if L0 is strictly low-rank. As a typical example, consider the case where there is a mixture structure of multiple subspaces underlying L0. When the subspace (i.e., cluster) number goes large, the second coherence parameter will enlarge and thus the performance of RPCA degrades. To overcome the challenges arising from coherent data, theoretically, one needs to capture the extra structures that produce high coherence. Nevertheless, such a strategy suffers several practical issues and is therefore infeasible. In sharp contrast, it is much simpler to solve the problem by LRR: When the dictionary matrix A in LRR satisfies certain conditions, namely A is low-rank and U0 ⊂ UA, LRR can avoid the second coherence parameter that has potential to be large. Furthermore, we established a heuristic algorithm that utilizes RPCA to approximately pursue a qualified dictionary. Experimental results showed that our algorithm performed better than RPCA. However, there still remain several problems for future work.\n⋄ By AZ∗ = L0, the column space of the dictionary A approximately has the same properties as L0, and thus, roughly, µ1(A) ≈ µ1(L0). So this paper still needs to assume that the first coherence parameter µ1 is small and only addresses the cases where the second coherence parameter µ2 might be large. In some domains such as the text documents, both the row space and column space can own some clustering structures, and thus both µ1 and µ2 can be large. New models are required to well handle such coherent data.\n⋄ It is possible to prove that Algorithm 1 is superior over RPCA in theory, because the conditions (i.e., A is low-rank and U0 ⊂ UA) required by Algorithm 1 to succeed are weaker than A = L0. It is significant to accurately identify in which conditions RPCA can produce a solution that is able to meet those conditions.\n⋄ While theorem 1 points out a generic direction for learning the dictionary matrix in LRR, the specific learning procedure is not unique and our Algorithm 1 is not exclusive either. For example, one may drive some kind of optimization framework to jointly compute the variables A and Z."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Guangcan Liu is a Postdoctoral Researcher supported by nsf-dms0808864, nsf-ses1131848, nsf-eager1249316, AFOSR-FA9550-13-1-0137, and ONR-N00014-13-1-0764. Ping Li is also partially supported by nsf-iii1360971 and nsf-bigdata1419210."
    }, {
      "heading" : "Appendix A. List of Notations",
      "text" : "(·)+ Moore-Penrose pseudoinverse of a matrix. ⊗ Kronecker product. ei The ith standard basis. [·]ij The (i, j)th entry of a matrix. X ∈ Rm×n The observed data matrix. A, UAΣAV T A The dictionary matrix, and its SVD L0, U0Σ0V T 0 The ground truth of the data matrix, and its SVD. S0 ∈ Rm×n The ground truth of the corruption matrix. UΣV T The SVD of A+L0. r0, rA The ranks of L0 and A. γA The condition number of A. µ1, µ2, µ3 The first, second and third coherence parameters of a matrix. µA3 (·) The third coherence parameter of a matrix, associating with A. n1, n2 n1 = max(m,n),n2 = min(m,n). Ω Locations of the nonzero entries of S0. Ωc The complement of Ω. PU0 , PV0 The projections onto the space spanned by U0 (resp. V0). PΩ, PΩ⊥ The projections onto the space of matrices supported on Ω (resp. Ωc). I,I The identity matrix and the identity operator. |Ω| The cardinality of Ω, i.e., the number of nonzero entries in S0. sign(·) The signum function. ∂ The subgradient of a function. ‖ · ‖2 The ℓ2 norm of a vector. 〈·〉 The inner product of two matrices or vectors. ‖ · ‖ The operator norm or 2-norm of a matrix, i.e., largest singular value. ‖ · ‖∗ The nuclear norm of a matrix. ‖ · ‖F The Frobenius norm of a matrix. ‖ · ‖2,∞ The ℓ2,∞ norm, i.e., the largest ℓ2 norm of the columns of a matrix. ‖ · ‖1 The ℓ1 norm of a matrix seen as a long vector. ‖ · ‖∞ The sup-norm of a matrix seen as a long vector. Ber(ρ) A Bernoulli distribution with expected value ρ and variance ρ(1− ρ)."
    }, {
      "heading" : "Appendix B. Why Does µ2 Increase with the Cluster Number?",
      "text" : "B.1 Zipf’s Law\nWhen the data points are sampled from a low-rank subspace uniformly at random, it has been proven by (Candès and Recht, 2009) that the first and second coherence parameters are bounded. Namely, µ1(L0) ≤ c and µ2(L0) ≤ c for some numerical constant c independent of the characteristics of L0. Although correct, such a property is not enough to interpret the phenomenon that the coherence parameters increase with the cluster number underlying L0. Hence, it is necessary to establish a more accurate rule to characterize the coherence parameters. Through extensive experiments, we find that the first and second coherence parameters actually follow the well-known Zipf’s law. More precisely, if the data\npoints (which form the column vectors of L0 ∈ Rm×n) are uniformly sampled from a r0dimensional subspace, then, roughly, the logarithm of coherence is inversely proportional to the logarithm of 1 + r0. That is,\nlog(µ1(L0)) log(1 + r0) ≈ c1 and log(µ2(L0)) log(1 + r0) ≈ c2, (B.11)\nwhere c1 and c2 are two constants. The results in Figure 6 verify the above Zipf’s law. Note that the Zipf’s law (B.11) can also induce the boundedness property proved by (Candès and Recht, 2009). Namely, (B.11) approximately gives that µ1(L0) ≤ exp(c1/ log 2) and µ2(L0) ≤ exp(c2/ log 2).\nThe above Zipf’s law suggests that the coherence must be inversely proportional to the rank of data. This is intuitively interpretable. Let yj = [U0]ij and Cr0 = ‖UT0 ei‖22 = ∑r0\nj=1 y 2 j . Then it can be seen that Cr0 is the squared Euclidean length of the first r0 components of a unit vector distributed on the m-dimensional unit sphere. With these notations, it can be seen that µ1 is the largest order statistic of Cr0 divided by the expectation of Cr0 :\nµ1(L0) = m\nr0 max i ‖UT0 ei‖22 = maxi ‖UT0 ei‖22 r0 m = max(Cr0) E(Cr0) .\nNow it is unfolded that the first (and second) coherence parameter of a matrix with rank r0 is actually some kind of uncertainty of the first r0 components of a unit-normed, mdimensional random vector. Thus if r0 = m (i.e., L0 is full rank), then the uncertainty vanishes and µ1(L0) = 1. Similarly if r0 = 1, the uncertainty measured by max(Cr0)/E(Cr0) is as high as that of a single random number.\nThe Zipf’s law (B.11) is useful, because it provides us a trackable approach to estimate the coherence parameters when the data points are not uniformly sampled, as will be shown in the next section.\nB.2 An Explanation to the µ2-phenomenon\nIdeally, if the values in U0 and V0 are perfectly spreading out, namely [U0]ij = [U0]i1j1 and [V0]ij = [V0]i1j1 ,∀i, j, i1, j1, then µ1(L0) = µ2(L0) = 1. However, this is unlikely for µ2(L0) to happen, as it is provable that the row projector V0V T 0 , which is also known as Shape Interaction Matrix (SIM) in subspace clustering, measures the subspace membership of the data points (Costeira and Kanade, 1998; Liu et al., 2013). More precisely, if the data points in L0 are sampled from k number of independent subspaces, saying L0 = [L (1) 0 , · · · , L (k) 0 ], where L (i) 0 with SVD UiΣiV T i is a matrix of data points from the ith subspace, then V0 is equivalent to a block-diagonal matrix that has nonzero entries only on k number of blocks:\nV0 ∼\n\n   \nV1 0 0 0 0 V2 0 0\n0 0 . . . 0 0 0 0 Vk\n\n    .\nIn this case, it is demonstrable that the second coherence parameter µ2(L0) depends on the cluster number k. For the convenience of analysis, we assume that the dimensions of\nall subspaces are equal, i.e., rank (\nL (i) 0\n)\n= r0/k,∀, i = 1, · · · , k, and the sampling in each subspace is uniform. Then the Zipf’s law (B.11) gives\nµ2(L0) = max i\nµ2(L (i) 0 ) ≈ exp( c2 log(1 + r0\nk ) ), (B.12)\nwhere k is the cluster number. Hence, approximately, the second coherence parameter µ2(L0) will increase with the cluster number underlying L0."
    }, {
      "heading" : "Appendix C. Proof of Theorem 2",
      "text" : "Proof Let (Z∗, S∗) be an optimal solution to (3.8). Denote NL = AZ ∗−L0, NS = S∗−S0 and E = NL +NS. Then we have\n‖E‖F = ‖(X − L0 − S0)− (X −AZ∗ − S∗)‖F ≤ ‖(X − L0 − S0)‖F + ‖(X −AZ∗ − S∗)‖F ≤ 2ε.\nProvided that |Ω| < (0.35 − ǫ)mn, the proof process of Lemma 9 shows that\n‖F‖∞ < 0.5.\nBy the optimality of (Z∗, S∗),\n‖A+L0‖∗ + λ‖S0‖1 ≥ ‖Z∗‖∗ + λ‖S∗‖1 ≥ ‖A+L0‖∗ + λ‖S0‖1 + 〈UV T , Z∗ −A+L0〉+ λ〈sign(S0) +H,NS〉 = ‖A+L0‖∗ + λ‖S0‖1 + λ〈sign(S0) + F,NL〉+ λ〈sign(S0) +H,NS〉,\nwhich leads to\n0 ≥ 〈sign(S0) + F,NL〉+ 〈sign(S0) +H,NS〉 = 〈sign(S0) + F,NL〉+ 〈sign(S0) +H,E −NL〉 = 〈F −H,NL〉+ 〈sign(S0) +H,E〉 ≥ 0.5‖PΩ⊥(NL)‖1 − ‖E‖1.\nHence,\n‖PΩ⊥(NL)‖F ≤ ‖PΩ⊥(NL)‖1 ≤ 2‖E‖1 ≤ 2 √ mn‖E‖F ≤ 4 √ mnε.\nBy NL = AZ ∗ − L0 ∈ PUA ,\nNL = PPUAPΩ⊥(NL),\nwhere P = I +∑∞i=1(PUAPΩPUA)i. By ‖P‖ ≤ 2,\n‖NL‖F ≤ ‖P‖‖PUAPΩ⊥(NL)‖F ≤ ‖P‖‖PΩ⊥ (NL)‖F ≤ 8 √ mnε."
    }, {
      "heading" : "Appendix D. Optimization Procedure",
      "text" : "In this work, we use the exact ALM method to solve the optimization problem (1.3). We first convert (1.3) to the following equivalent problem:\nmin Z,S,J\n‖J‖∗ + λ‖S‖1, s.t. X = AZ + S,Z = J.\nThis problem can be solved by the ALM method, which minimizes the following augmented Lagrange function:\n‖J‖∗ + λ‖S‖1 + 〈Y,X −AZ − S〉+ 〈W,Z − J〉+ θ\n2 (‖X −AZ − S‖2F + ‖Z − J‖ 2 F )\nwith respect to J , Z and S, respectively, by fixing the other variables, and then updating the Lagrange multipliers Y and W . Algorithm 2 summarizes the whole procedure of the optimization procedure.\nAlgorithm 2 Solving Problem (1.3) by Exact ALM\nInput: data matrix X, dictionary matrix A, parameter λ. Initialization: Z = J = 0, S = 0, Y = 0,W = 0, θ = 0.1, τ = 5. while not converged do\n1. Alternating minimization: while not converged do\n1.1. fix the others and update J by\nJ = argmin 1\nθ ||J ||∗ +\n1 2 ||J − (Z +W/θ)||2F .\n1.2. fix the others and update Z by\nZ = (I+ATA)−1(AT (X − S) + J + (ATY −W )/θ).\n1.3. fix the others and update S by\nS = argmin λ\nθ ‖S‖1 +\n1 2 ||S − (X −AZ + Y/θ)||2F .\nend while 2. update the Lagrange multipliers and the parameter θ\nY = Y + θ(X −AZ − S), W = W + θ(Z − J), θ = θτ.\nend while"
    } ],
    "references" : [ {
      "title" : "Synthetic aperture radar imaging and motion estimation via robust principle component analysis",
      "author" : [ "Liliana Borcea", "Thomas Callaghan", "George Papanicolaou" ],
      "venue" : "Arxiv,",
      "citeRegEx" : "Borcea et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Borcea et al\\.",
      "year" : 2012
    }, {
      "title" : "Matrix completion with noise",
      "author" : [ "Emmanuel Candès", "Yaniv Plan" ],
      "venue" : "In IEEE Proceeding,",
      "citeRegEx" : "Candès and Plan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Candès and Plan.",
      "year" : 2010
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "Emmanuel J. Candès", "Xiaodong Li", "Yi Ma", "John Wright" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Candès et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2011
    }, {
      "title" : "A multibody factorization method for independently moving objects",
      "author" : [ "Joao Costeira", "Takeo Kanade" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Costeira and Kanade.,? \\Q1998\\E",
      "shortCiteRegEx" : "Costeira and Kanade.",
      "year" : 1998
    }, {
      "title" : "Sparse subspace clustering",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Elhamifar and Vidal.,? \\Q2009\\E",
      "shortCiteRegEx" : "Elhamifar and Vidal.",
      "year" : 2009
    }, {
      "title" : "Matrix rank minimization with applications",
      "author" : [ "M. Fazel" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Fazel.,? \\Q2002\\E",
      "shortCiteRegEx" : "Fazel.",
      "year" : 2002
    }, {
      "title" : "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography",
      "author" : [ "Martin Fischler", "Robert Bolles" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Fischler and Bolles.,? \\Q1981\\E",
      "shortCiteRegEx" : "Fischler and Bolles.",
      "year" : 1981
    }, {
      "title" : "Robust estimates, residuals, and outlier detection with multiresponse data",
      "author" : [ "R. Gnanadesikan", "J.R. Kettenring" ],
      "venue" : null,
      "citeRegEx" : "Gnanadesikan and Kettenring.,? \\Q1972\\E",
      "shortCiteRegEx" : "Gnanadesikan and Kettenring.",
      "year" : 1972
    }, {
      "title" : "Recovering low-rank matrices from few coefficients in any basis",
      "author" : [ "D. Gross" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Gross.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gross.",
      "year" : 2011
    }, {
      "title" : "Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming",
      "author" : [ "Qifa Ke", "Takeo Kanade" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Ke and Kanade.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ke and Kanade.",
      "year" : 2005
    }, {
      "title" : "A framework for robust subspace learning",
      "author" : [ "Fernando De la Torre", "Michael J. Black" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Torre and Black.,? \\Q2003\\E",
      "shortCiteRegEx" : "Torre and Black.",
      "year" : 2003
    }, {
      "title" : "Unsupervised object segmentation with a hybrid graph model (hgm)",
      "author" : [ "Guangcan Liu", "Zhouchen Lin", "Xiaoou Tang", "Yong Yu" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Liu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "Robust subspace segmentation by low-rank representation",
      "author" : [ "Guangcan Liu", "Zhouchen Lin", "Yong Yu" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Liu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "Exact subspace segmentation and outlier detection by low-rank representation",
      "author" : [ "Guangcan Liu", "Huan Xu", "Shuicheng Yan" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Liu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Robust recovery of subspace structures by low-rank representation",
      "author" : [ "Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Liu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "Spectral regularization algorithms for learning large incomplete matrices",
      "author" : [ "Rahul Mazumder", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Mazumder et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mazumder et al\\.",
      "year" : 2010
    }, {
      "title" : "Low-rank and sparse matrix decomposition for accelerated dynamic mri with separation of background and dynamic components",
      "author" : [ "Ricardo Otazo", "Emmanuel Candès", "Daniel K. Sodickson" ],
      "venue" : "Arxiv,",
      "citeRegEx" : "Otazo et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Otazo et al\\.",
      "year" : 2012
    }, {
      "title" : "Rasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images",
      "author" : [ "YiGang Peng", "Arvind Ganesh", "John Wright", "Wenli Xu", "Yi Ma" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Peng et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2012
    }, {
      "title" : "Convex Analysis",
      "author" : [ "R. Rockafellar" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar.,? \\Q1970\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1970
    }, {
      "title" : "Random vectors in the isotropic position",
      "author" : [ "M. Rudelson" ],
      "venue" : "Journal of Functional Analysis,",
      "citeRegEx" : "Rudelson.,? \\Q1999\\E",
      "shortCiteRegEx" : "Rudelson.",
      "year" : 1999
    }, {
      "title" : "Generalization error bounds for collaborative prediction with low-rank matrices",
      "author" : [ "Nathan Srebro", "Tommi Jaakkola" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Srebro and Jaakkola.,? \\Q2005\\E",
      "shortCiteRegEx" : "Srebro and Jaakkola.",
      "year" : 2005
    }, {
      "title" : "A benchmark for the comparison of 3-d motion segmentation algorithms",
      "author" : [ "Roberto Tron", "Rene Vidal" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Tron and Vidal.,? \\Q2007\\E",
      "shortCiteRegEx" : "Tron and Vidal.",
      "year" : 2007
    }, {
      "title" : "Cofi rank maximum margin matrix factorization for collaborative ranking",
      "author" : [ "Markus Weimer", "Alexandros Karatzoglou", "Quoc V. Le", "Alex J. Smola" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Weimer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Weimer et al\\.",
      "year" : 2007
    }, {
      "title" : "Robust pca via outlier pursuit",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Sujay Sanghavi" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Xu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2010
    }, {
      "title" : "Outlier-robust pca: The highdimensional case",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Shie Mannor" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Xu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2013
    }, {
      "title" : "Tilt: Transform invariant low-rank textures",
      "author" : [ "Zhengdong Zhang", "Arvind Ganesh", "Xiao Liang", "Yi Ma" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Abstract The recently established RPCA (Candès et al., 2011) method provides us a convenient way to restore low-rank matrices from grossly corrupted observations.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "We show that it is possible for Low-Rank Representation (LRR) (Liu et al., 2013) to overcome the challenges raised by coherent data, as long as the dictionary in LRR is configured appropriately.",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 2,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 3,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 7,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 8,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 9,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 10,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 25,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 15,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 16,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 24,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).",
      "startOffset" : 2,
      "endOffset" : 296
    }, {
      "referenceID" : 1,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010). One of the most exciting methods is probably the so-called RPCA (Robust Principal Component Analysis) method by Candès et al. (2011), built upon the exploration of the following low-rank matrix recovery problem:",
      "startOffset" : 3,
      "endOffset" : 431
    }, {
      "referenceID" : 6,
      "context" : "where ‖ · ‖∗ is the nuclear norm (Fazel, 2002) of a matrix, ‖ · ‖1 denotes the l1 norm of a matrix seen as a long vector, and λ > 0 is a parameter.",
      "startOffset" : 33,
      "endOffset" : 46
    }, {
      "referenceID" : 26,
      "context" : ", image processing (Zhang et al., 2012), computer vision (Peng et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : ", 2012), computer vision (Peng et al., 2012), radar imaging (Borcea et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : ", 2012), radar imaging (Borcea et al., 2012), magnetic resonance imaging (Otazo et al.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : ", 2012), magnetic resonance imaging (Otazo et al., 2012), etc.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Figure 1 demonstrates a typical example of extra structures; that is, the clustering structure which is ubiquitous in modern applications (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Soltanolkotabi et al., 2013).",
      "startOffset" : 138,
      "endOffset" : 221
    }, {
      "referenceID" : 5,
      "context" : "Figure 1 demonstrates a typical example of extra structures; that is, the clustering structure which is ubiquitous in modern applications (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Soltanolkotabi et al., 2013).",
      "startOffset" : 138,
      "endOffset" : 221
    }, {
      "referenceID" : 3,
      "context" : "Nevertheless, as explained in (Candès et al., 2011; Candès and Recht, 2009), the coherence parameters are indeed necessary for matrix recovery (if there is no additional condition available).",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, as explained in (Candès et al., 2011; Candès and Recht, 2009), the coherence parameters are indeed necessary for matrix recovery (if there is no additional condition available).",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "Our study is based on the following convex program termed Low-Rank Representation (LRR) (Liu et al., 2013): min Z,S ‖Z‖∗ + λ‖S‖1, s.",
      "startOffset" : 88,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Xu et al., 2010; Liu et al., 2012).",
      "startOffset" : 2,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Xu et al., 2010; Liu et al., 2012).",
      "startOffset" : 2,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Xu et al., 2010; Liu et al., 2012).",
      "startOffset" : 2,
      "endOffset" : 105
    }, {
      "referenceID" : 24,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Xu et al., 2010; Liu et al., 2012).",
      "startOffset" : 2,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : ", (Candès and Plan, 2010; Candès and Recht, 2009; Candès et al., 2011; Xu et al., 2010; Liu et al., 2012).",
      "startOffset" : 2,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "⋄ This paper provides insights regarding the LRR model proposed by (Liu et al., 2013).",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : ", (Srebro and Jaakkola, 2005; Weimer et al., 2007).",
      "startOffset" : 2,
      "endOffset" : 50
    }, {
      "referenceID" : 23,
      "context" : ", (Srebro and Jaakkola, 2005; Weimer et al., 2007).",
      "startOffset" : 2,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "The coherence parameters defined in (Candès and Recht, 2009; Candès et al., 2011) are excellent exemplars of such quantities.",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "The coherence parameters defined in (Candès and Recht, 2009; Candès et al., 2011) are excellent exemplars of such quantities.",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "In (Candès et al., 2011), another coherence parameter, called as the third coherence parameter and denoted as 1 ≤ μ3 ≤ mn, is also introduced: μ3(L0) = mn r0 (‖U0V T 0 ‖∞) = mn r0 max i,j (|〈U 0 ei, V T 0 ej〉|).",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "We include it just for the sake of consistence with (Candès et al., 2011).",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "The analysis in (Candès et al., 2011) merges the above three parameters into a single one: μ(L0) = max{μ1(L0), μ2(L0), μ3(L0)}.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "2 μ2-phenomenon Candès et al. (2011) have proven that the success condition (regarding L0) of RPCA is rank (L0) ≤ n2 crμ(L0)(log n1) , (3.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : ", face, texture and motion (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Liu et al., 2010a).",
      "startOffset" : 27,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : ", face, texture and motion (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Liu et al., 2010a).",
      "startOffset" : 27,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "Unfortunately, as explained in (Candès and Recht, 2009; Candès et al., 2011; Liu et al., 2012), the coherence parameters are necessary for identifying accurately the success conditions of matrix recovery.",
      "startOffset" : 31,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "Unfortunately, as explained in (Candès and Recht, 2009; Candès et al., 2011; Liu et al., 2012), the coherence parameters are necessary for identifying accurately the success conditions of matrix recovery.",
      "startOffset" : 31,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "Unfortunately, as explained in (Candès and Recht, 2009; Candès et al., 2011; Liu et al., 2012), the coherence parameters are necessary for identifying accurately the success conditions of matrix recovery.",
      "startOffset" : 31,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "The reason is that, as has been explored by (Liu et al., 2013), the complexity of solving the LRR problem (1.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "1 Settings and Some Basic Lemmas The same as in RPCA (Candès et al., 2011), we assume that the locations of the corrupted entries are selected uniformly at random.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "For general sign matrices, the same as in RPCA (Candès et al., 2011), our Theorem 1 can still be proved by globally placing an elimination theorem and a derandomization scheme.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "Provided that A ∈ R is fairly low-rank, the analysis in (Candès et al., 2011) gives that ‖PTAPΩ‖ ≤ √",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "Following the techniques in (Candès et al., 2011), we have the following lemma to bound the operator norm of PUAPΩ.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "Then by using the results in (Rudelson, 1999) and following the proof procedure of (Candès and Recht, 2009), we have that",
      "startOffset" : 29,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "Then by using the results in (Rudelson, 1999) and following the proof procedure of (Candès and Recht, 2009), we have that",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 19,
      "context" : "Proof By standard convexity arguments (Rockafellar, 1970), (AL0, S0) is an optimal solution to (1.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "1 of (Liu et al., 2013) gives that ‖AL0 + ∆1‖∗ > ‖AL0‖∗ strictly holds unless ∆1 = 0.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "3 of (Liu et al., 2013).",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 22,
      "context" : "2 Results on Corrupted Motion Sequences We now experiment with 11 additional sequences attached to the Hopkins155 (Tron and Vidal, 2007) database.",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "We consider three state-of-the-art subspace clustering methods: Shape Interaction Matrix (SIM) (Costeira and Kanade, 1998), Low-Rank Representation with A = X (Liu et al.",
      "startOffset" : 95,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : ", 2010b) (which is referred to as “LRRx”) and Sparse Subspace Clustering (SSC) (Elhamifar and Vidal, 2009).",
      "startOffset" : 79,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : "1 Zipf’s Law When the data points are sampled from a low-rank subspace uniformly at random, it has been proven by (Candès and Recht, 2009) that the first and second coherence parameters are bounded.",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "11) can also induce the boundedness property proved by (Candès and Recht, 2009).",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "However, this is unlikely for μ2(L0) to happen, as it is provable that the row projector V0V T 0 , which is also known as Shape Interaction Matrix (SIM) in subspace clustering, measures the subspace membership of the data points (Costeira and Kanade, 1998; Liu et al., 2013).",
      "startOffset" : 229,
      "endOffset" : 274
    }, {
      "referenceID" : 15,
      "context" : "However, this is unlikely for μ2(L0) to happen, as it is provable that the row projector V0V T 0 , which is also known as Shape Interaction Matrix (SIM) in subspace clustering, measures the subspace membership of the data points (Costeira and Kanade, 1998; Liu et al., 2013).",
      "startOffset" : 229,
      "endOffset" : 274
    } ],
    "year" : 2014,
    "abstractText" : "The recently established RPCA (Candès et al., 2011) method provides us a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA may be not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when the data is strictly low-rank. This is because RPCA prefers incoherent data, which, however, could be inconsistent with some natural structures of data. As a typical example, consider the clustering structure which is ubiquitous in modern applications. As the number of cluster grows, the coherence parameters of data keep increasing, and accordingly, the recovery performance of RPCA degrades. We show that it is possible for Low-Rank Representation (LRR) (Liu et al., 2013) to overcome the challenges raised by coherent data, as long as the dictionary in LRR is configured appropriately. Namely, we mathematically prove that if the dictionary itself is low-rank then LRR can avoid the coherence parameters which have potential to be large. This provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments. Our extensive experiments on randomly generated matrices and real motion sequences show promising results.",
    "creator" : "LaTeX with hyperref package"
  }
}