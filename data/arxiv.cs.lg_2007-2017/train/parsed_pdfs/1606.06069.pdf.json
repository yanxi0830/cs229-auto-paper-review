{
  "name" : "1606.06069.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Relative Natural Gradient for Learning Large Complex Models",
    "authors" : [ "Ke Sun", "Frank Nielsen" ],
    "emails" : [ "sunk.edu@gmail.com", "Frank.Nielsen@acm.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The Fisher Information Metric (FIM) I(Θ) = (Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D ×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients\nIij = Ep [ ∂l\n∂Θi\n∂l\n∂Θj\n] , (1)\nwhere l(Θ) denotes the log-likelihood function ln p(x |Θ). Under light regularity conditions, Equation (1) can be rewritten equivalently as\nIij = −Ep [ ∂2l\n∂Θi∂Θj\n] = 4 ∫ ∂ √ p(x |Θ) ∂Θi ∂ √ p(x |Θ) ∂Θj dx. (2)\nFor regular natural exponential families (NEFs) l(Θ) = Θᵀt(x)− F (Θ) (log-linear models with sufficient statistics t(x)), the FIM is I(Θ) = ∇2F (Θ) 0, the Hessian of the moment generating function (mgf). Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the mgf may not be available in closed-form nor computationally tractable (Montanari, 2015). Besides the fact that learning machines usually have often singularities (Watanabe, 2009) (|I(Θ)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large learning system is very challenging due to the finiteness of data, and the large number D(D+1)2 of matrix coefficients to evaluate. Moreover,\nar X\niv :1\n60 6.\n06 06\n9v 1\n[ cs\n.L G\n] 2\n0 Ju\nn 20\ngradient descent techniques require to invert this large matrix and to tune the learning rate. The FIM is not invariant and depends on the parameterization: IΘ(Θ) = JᵀIΛ(Λ)J where J is the Jacobian matrix Jij = ∂Λi∂Θj . Therefore one may ponder whether we can always find a suitable parameterization that yields a diagonal FIM that is straightforward to invert. This fundamental problem of parameter orthogonalization was first investigated by Jeyffreys (1961) for decorrelating the estimation of the parameters of interest from the nuisance parameters. Fisher diagonalization yields parameter orthogonalization (Cox and Reid, 1987), and prove useful when estimating Θ̂ using MLE that is asymptotically normally distributed, Θ̂n = G(Θ, I−1(Θ)/ √ n), where G(θ1,θ2) denotes a univariate or multivariate Gaussian distribution with mean θ1 and variance θ2, and efficient since the variance of the estimator matches the Cramér-Rao lower bound. Using the chain rule of differentiation of calculus, this amounts to find a suitable parameterization Ω = Ω(Θ) satisfying ∑\ni,j\nE\n[ ∂2\n∂Θi∂Θj ∂l(x; θ) ] ∂Θi ∂Ωk ∂Θj ∂Ωl = 0, ∀k 6= l.\nThus in general, we end up with ( D 2 ) = D(D−1)2 (non-linear) partial differential equations to\nsatisfy (Huzurbazar, 1950). Therefore, in general there is no solution when ( D 2 ) > D, that is when D > 3. When D = 2, the single differential equations is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1σp0( x−µ σ )} that include the Gaussian family and the Cauchy family. Sometimes, the structure of the differential equation system yields a solution: For example, Jeffreys (1961) reported a parameter orthogonalization for Pearson’s distributions of type I which is of order D = 4. Cox and Reid (1987) further investigate this topic with application to conditional inference, and provide examples (including the Weibull distribution).\nFrom the viewpoint of geometry, the FIM induces a Riemannian manifold with metric tensor g(Θ) = I(Θ). When the FIM may be degenerate, this yields a pseudo-Riemannian manifold (Thomas, 2014). In differential geometry, orthogonalization amounts to transform the square length infinitesimal element gijdΘiΘj of a Riemannian geometry into an orthogonal system ω with matching square length infinitesimal element ΩiidΩidΩj . However, such a global orthogonal metric does not exist (Huzurbazar, 1950) when D > 3 for an arbitrary metric tensor although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant and Vickers, 2009). For NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016) (Θ1:k,Hk+1:D), where H = Ep[t(x)] = ∇F (Θ) is the moment parameter, for any k ∈ {1, ..., D−1}, where v[b:e] denotes the subvector (vb, ..., ve)ᵀ of v. The geometry of NEFs is a dually flat structure (Amari, 2016) induced by the convex mgf, the potential function. It defines a dual affine coordinate systems ei = ∂i = ∂∂Hi and ej = ∂ j = ∂∂Θj that are orthogonal: 〈e i, ej〉 = δij , where δij = 1 iff i = j and δij = 0 otherwise. Those dual affine coordinate systems are defined up to an affine invertible transformation A: Θ̃ = AΘ + b, H̃ = A−1H + c, where b and c are constants. In particular, for any order-2 NEF (D = 2), we can always obtain two mixed parameterizations (Θ1, H2) or (H1,Θ2).\nThe FIM g(Θ) or I(Θ) by definition is an expectation. If its Hessian form in eq. (2) is computed based on a set of empirical observations {xk}, as\nḡ(Θ) = − ∂ 2lk\n∂Θi∂Θj , (3)\nwhere “ · ” denotes the sample average over {xk}, the resulting metric is called the observed FIM (Efron and Hinkley, 1978). It is useful when the underlying distribution is not available.\nWhen the number of observations increases, the observed FIM becomes more and more close to the FIM.\nPast works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015). This global approach faces the analytical complexity of learning systems. The approximation error increases as the system scales up and as complex and dynamic structures emerge.\nThis work aims at a different local approach. The idea is to accurately describe the information geometry in a local subsystem of the big learning system, which is invariant to the scaling up and structural change of the global system, so that the local machinery, including optimization, can be discussed regardless of the other parts.\nFor this purpose, a novel concept, Relative Fisher Information Metric (RFIM), is defined. Unlike the traditional geometric view of a high-dimensional parameter manifold, RFIMs defines multiple projected low-dimensional geometry of subsystems. This geometry is correlated to the parameters beyond the subsystem and is therefore considered dynamic. It can be used to characterize the efficiency of a local learning process. Taking this stance has potentials in deep learning because a big learning system can be decomposed into many local components, i.e. layers. This paper will make clear the concept of RFIM, provide proof-of-concept experiments, and discuss its theoretical advantages.\nThe paper is organized as follows. Sec. 2 reviews natural gradient within the context of MultiLayer Perceptrons (MLP). Sec. 3 presents the concept of RFIM, and gives detailed formulations of several commonly used subsystems. Sec. 4 discusses the advantages of using RFIM as compared to FIM. Sec. 5 shows how to use the RFIMs given by Sec. 3 to optimize neural networks, with an algorithm framework and several proof-of-concept experiments. Sec. 6 concludes this work and further hint at perspectives."
    }, {
      "heading" : "2 Natural Gradient of Neural Networks",
      "text" : "Consider a MLP as depicted in fig. 1, whose statistical model is the following conditional distribution\np(y |x,Θ) = ∑\nh1,··· ,hL−1\np(y |hL−1,θL) · · · p(h2 |h1,θ2)p(h1 |x,θ1), (4)\nwhere the often intractable sum over h1, · · · ,hL−1 can be get rid off by deteriorating p(h1 |x,θ1), · · · , p(hL−1 |hL−2,θL−1) to Dirac’s deltas δ, and let merely the last layer p(y |hL−1,θL) be stochastic. Note that Restricted Boltzmann Machines (Nair and Hinton, 2010; Montavon and Müller, 2012) (RBMs), and dropout (Wager et al., 2013) do consider h to be stochastic.\nThe tensor metric of the neuromanifold (Amari, 1995)MΘ, consisting of all MLPs with the same architecture but different parameter values, is locally defined by the FIM. Because that a MLP corresponds to a conditional distribution, its FIM by eq. (1) is a function of the input x. By taking an empirical average over the input samples {xi}, the FIM of a MLP has the following expression\ng(Θ) = 1\nn n∑ i=1 Ep(y |xi,Θ) [ ∂li ∂Θ ∂li ∂Θᵀ ] = − 1 n n∑ i=1 Ep(y |xi,Θ) [ ∂2li ∂Θ∂Θᵀ ] , (5)\nwhere li(Θ) = ln p(y |xi, Θ) denotes the conditional log-likelihood function wrt xi.\nJust like a Mahalanobis metric, g(Θ) can be used to measure the distance between two neural networks locally around Θ ∈MΘ. A learning step makes a tiny movement δΘ onMΘ from Θ to Θ + δΘ. According to the FIM, the infinitesimal square distance\n〈δΘ, δΘ〉g(Θ) = δΘᵀg(Θ)δΘ = 1\nn n∑ i=1 Ep(y |xi,Θ) [ δΘᵀ ∂li ∂Θ ]2 (6)\nmeasures how much δΘ (with a radius constraint) is statistically along ∂l∂Θ , or equivalently how much δΘ affects intrinsically the conditional distribution p(y |x, Θ).\nConsider the negative log-likelihood function L(Θ) = − ∑ i ln p(yi |xi,Θ) wrt the observed pairs {(xi,yi)}, we try to minimize the loss while maintaining a small cost, measured geometrically by the square distance 〈δΘ, δΘ〉g(Θ) onMΘ. At Θt ∈MΘ, the target is to minimize wrt δΘ\nL(Θt + δΘ) + 1\n2γ 〈δΘ, δΘ〉g(Θt) ≈ L(Θt) + δΘ\nᵀ 5 L(Θt) + 1\n2γ δΘᵀg(Θt)δΘ, (7)\nwhere γ > 0 is a learning rate. The optimal solution of the above eq. (7) gives a learning step\nδΘt = −γg−1(Θt)5 L(Θt).\nIn this update procedure, the term g−1(Θt) 5 L(Θt) replaces the role of the usual gradient 5L(Θt) and is called the natural gradient (Amari, 1997).\nAlthough the FIM depends on the chosen parameterization, the natural gradient is invariant to reparametrization. Let Λ be another coordinate system and J be the Jacobian matrix Θ→ Λ. Then we have\ng−1(Θ)5 L(Θ) = (Jᵀg(Λ)J)−1 Jᵀ 5 L(Λ) = J−1g−1(Λ)5 L(Λ). (8)\nThe left-hand-side and right-hand-side of eq. (8) correspond to exactly the same infinitesimal movement alongMΘ. However, as the learning rate γ is not infinitesimal in practice, natural gradient descent actually depends on the coordinate system in practice. Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaus of the error surface, and attaining Fisher efficiency in online learning (see Sec. 4 (Amari, 1998)).\nFor sake of simplicity, we omit to discuss the case when the FIM is singular. That is, the set of parameters Θs with zero metric. This set of parameters forms an analytic variety (Watanabe, 2009), and technically the MLP as a statistical model is said non-regular (and the parameter Θ is not identifiable). The natural gradient has been extended (Thomas, 2014) to cope with singular FIMs having positive semi-definite matrices by taking the Moore-Penrose pseudo-inverse (that coincides with the inverse matrix for full rank matrices).\nIn the family of 2nd-order optimization methods, a line can be drawn from natural gradient from the Newton methods, e.g. (Martens, 2010), by checking whether the computation of the Hessian term depends on the cost function. Taking a MLP as an example, the computation of the FIM does not need the given {yi}, but averages over all possible y generated according to {xi} and the current model Θ. One advantage of natural gradient is that the FIM is guaranteed to be psd while the Hessian may not. We refer the reader to related references (Amari et al., 2000) for more details.\nBonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a step gradient descent, thus ensuring to stay on the manifold for any chosen learning rate. Convergence is proven for Hadamard manifolds (of negative curvatures). However, it is not mathematically tractable to express the exponential map of hierarchical model manifolds."
    }, {
      "heading" : "3 Relative Fisher Information Metric of Subsystems",
      "text" : "In general, for large parametric matrices, it is impossible to diagonalize or decorrelate all the parameters as mentioned above, so that we split instead all random variables in the learning system into three parts θf , θ, h. The reference, θf , consists of the majority of the random variables that are considered fixed. θ is the internal parameters of a subsystem wrt its structure. The response h is the interface of this subsystem to the rest of the learning system. This h usually carries sample-wise information to be summarized into the internal parameters θ, so it is like “pseudo-observations”, or hidden variables, to the subsystem. Once θf is given, the subsystem can characterized by the conditional distribution p(h |θ,θf ). We made the following definition.\nDefinition 1 (RFIM). Given θf , the RFIM 1 of θ wrt h is\ngh (θ |θf ) def = Ep(h | θ, θf )\n[ ∂\n∂θ ln p(h |θ, θf )\n∂\n∂θᵀ ln p(h |θ, θf )\n] , (9)\nor simply gh (θ), corresponding to the estimation of θ based on observations of h given θf .\nWhen we choose h to be the observables, usually denoted by x, and choose θ to be all free parameters Θ in the learning system, then RFIM becomes FIM: g(Θ) = gx(Θ). What is novel is that we can choose the response h to be other than the raw observables to compute Fisher informations of subsystems, specially dynamically during the learning of machines. To see the meaning of RFIM, similar to eq. (6), the infinitesimal square distance\n〈δθ, δθ〉gh(θ) = Ep(h | θ, θf ) [ δθᵀ ∂\n∂θ ln p(h |θ, θf )\n]2 (10)\nmeasures how much δθ impacts intrinsically the conditional distribution featuring the subsystem. We also have the following proposition, following straightforwardly from definition 1.\n1We use the same term “relative FIM” (Zegers, 2015) with a different definition.\np(y |Θ,x) = ∑ h1 ∑ h2 p(h1 |θ1,x) p(h2 |θ2,h1) p(y |θ3,h2)\nProposition 2. If θ1 consists of a subset of θ2 so that θ2 = (θ1, θ̃1), thenMθ1 with the metric gh (θ1) has the same Riemannian geometry with a sub-manifold ofMθ2 with the metric gh (θ2), when θ̃1 is given.\nWhen the response h is chosen, then different splits of (θ,θf ) correspond to the same ambient geometry. In fact, the particular case of a mixed coordinate system (that is not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations. Our splits in RFIMs consider non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that are the leaves of the foliation (Amari and Nagaoka, 2000).\nFigure 2 shows the traditional global geometry of a learning system, as compared to the information geometry of subsystems defined by RFIMs. The red arrows means that the pointed geometry structure is dynamic and varies with the reference variable. In MLPs, the subsystems, i.e. layers, are supervised. Their reference θf also carries sample-wise information.\nOne should not confuse RFIM with the diagonal blocks of FIM. Both their meaning and expression are different. RFIM is computed by integrating out hidden variables, or the output of subsystems. FIM is always computed by integrating out the observables.\nIn the following we analyze accurately several commonly used RFIMs. Note that, the FIMs of small parametric structures such as single neurons have been studied for a long time (Amari, 1997). Although with similar expressions, we are looking at a component embedded in a large system rather than a small single component system. These are two different concepts, and only the former can be used to guide large learning systems."
    }, {
      "heading" : "3.1 RFIMs of One Neuron",
      "text" : "We start from the RFIM of single neuron models."
    }, {
      "heading" : "3.1.1 Hyperbolic tangent activation",
      "text" : "Consider a neuron with input x, weights w, a hyperbolic tangent activation function, and a stochastic output y ∈ {−1, 1}, given by\np(y = 1) = 1 + tanh(wᵀx̃)\n2 , tanh(t) = exp(t)− exp(−t) exp(t) + exp(−t) . (11)\nFor convenience, throughout this paper x̃ = (xᵀ, 1)ᵀ denotes the augmented vector of x (homogeneous coordinates) so that wᵀx̃ contains a bias term, and a general linear transformation can be written simply as Ax̃. By definition 1 and some simple analysis 2, we get\ngy(w |x) = νtanh(w,x)x̃x̃ᵀ, νtanh(w,x) = 1− tanh2(wᵀx̃). (12)"
    }, {
      "heading" : "3.1.2 Sigmoid activation",
      "text" : "Similarly, the RFIM of a neuron with input x, weights w, a sigmoid activation function, and a stochastic binary output y ∈ {0, 1}, where\np(y = 1) = sigm(wᵀx̃), sigm(t) = 1\n1 + exp(−t) , (13)\nis given by\ngy(w |x) = νsigm(w,x)x̃x̃ᵀ, νsigm(w,x) = sigm (w ᵀx̃) [ 1− sigm (wᵀx̃) ] . (14)\nA neuron with sigmoid activation but continuous output was discussed earlier (Amari, 1997)."
    }, {
      "heading" : "3.1.3 Parametric Rectified Linear Unit",
      "text" : "Another commonly used activation function is Parametric Rectified Linear Unit (PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) as a special case. To compute the RFIM, we formulate PReLU into a conditional distribution given by\np(y |w,x) = G(y | relu(wᵀx̃), σ2), relu(t) = { t if t ≥ 0 ιt if t < 0. (0 ≤ ι < 1) (15)\nWhen ι = 0, eq. (15) becomes ReLU. By definition 1, the corresponding RFIM is\ngy(w |x) =  1 σ2 x̃x̃ ᵀ if wᵀx̃ > 0 undefined if wᵀx̃ = 0 ι2\nσ2 x̃x̃ ᵀ if wᵀx̃ < 0\n(16)\nThis RFIM is discontinuous at wᵀx̃ = 0. To obtain a smoother RFIM, a trick is to replace relu(wᵀx̃) on the left-hand-side of eq. (15) with reluω(wᵀx̃), where\nreluω(t) = ω ln\n( exp ( ιt\nω\n) + exp ( t\nω\n)) , (17)\n2See the appendix for detailed derivations.\nand ω > 0 is a hyper-parameter so that limω→0+ reluω = relu. Then, PReLU’s RFIM is given by\ngy(w |x) = νrelu(w,x)x̃x̃ᵀ,\nνrelu(w,x) = 1\nσ2\n[ ι+ (1− ι)sigm ( 1− ι ω wᵀx̃ )]2 , (18)\nwhich is simply a smoothed version of eq. (16). If we set empirically σ = 1, ι = 0, then νrelu(w,x) = sigm\n2 (\n1 ωw\nᵀx̃ ) is close to 1 when wᵀx̃ > 0, and is close to 0 otherwise."
    }, {
      "heading" : "3.1.4 Exponential Linear Unit",
      "text" : "A stochastic exponential linear unit (ELU) (Clevert et al., 2015) with α > 0 is p(y |w, x) = G(y | elu(wᵀx̃), σ2), elu(t) = { t if t ≥ 0 α (exp(t)− 1) if t < 0. (19)\nIts RFIM is given by\ngy(w |x) = νelu(w,x)x̃x̃ᵀ,\nνelu(w,x) =\n{ 1 σ2 if w\nᵀx̃ ≥ 0 α2 σ2 exp (2w ᵀx̃) if wᵀx̃ < 0.\n(20)\nThe coefficient function νelu(w,x) is continuous but non-differentiable at wᵀx̃ = 0."
    }, {
      "heading" : "3.1.5 A generic expression of one-neuron RFIMs",
      "text" : "Denote f ∈ {tanh, sigm, relu, elu} to be an element-wise nonlinear activation function. By eqs. (12), (14) and (18), the RFIMs of single neurons have a common form\ngy(w |x) = νf (w,x)x̃x̃ᵀ, (21)\nwhere νf (w,x) is a positive coefficient with large values in the linear region, or the effective learning zone of the neuron."
    }, {
      "heading" : "3.2 RFIM of One Layer",
      "text" : "A linear layer with input x, connection weights W = [w1, · · · ,wDy ], and stochastic output y can be represented by p(y |W ,x) = G(y |W ᵀx̃, σ2I), where I is the identity matrix, and σ is the scale of the observation noise. We vectorize W by stacking its columns {wi}, then gy(W |x) is a tensor of size (Dx + 1)Dy × (Dx + 1)Dy, where D denotes the dimension of the corresponding variable. Fortunately, due to conditional independence of y’s dimensions given W and x, the RFIM has a simple block diagonal form, given by\ngy(W |x) = 1 σ2 diag [x̃x̃ᵀ, · · · , x̃x̃ᵀ] , (22)\nwhere diag(·) means the (block) diagonal matrix constructed by the given matrix entries. A nonlinear layer increments a linear layer by adding an element-wise activation function applied on W ᵀw̃, and randomized wrt the choice of the activation (Bernoulli for tanh and sigm; Gaussian for relu). By definition 1, its RFIM is given by\ngy (W |x) = diag [ νf (w1,x)x̃x̃ᵀ, · · · , νf (wm,x)x̃x̃ᵀ ] , (23)\nwhere νf (wi,x) depends on the activation function f as discussed in Subsec. 3.1. A softmax layer, which often appears as the last layer of a MLP, is given by y ∈ {1, . . . ,m}, where p(y) = ηy =\nexp(wyx̃)∑m i=1 exp(wix̃) . (24)\nIts RFIM is not block diagonal any more but given by\ngy(W ) =  (η1 − η21)x̃x̃ᵀ −η1η2x̃x̃ᵀ · · · −η1ηmx̃x̃ᵀ −η2η1x̃x̃ᵀ (η2 − η22)x̃x̃ᵀ · · · −η2ηmx̃x̃ᵀ ... ... . . . ...\n−ηmη1x̃x̃ᵀ −ηmη2x̃x̃ᵀ · · · (ηm − η2m)x̃x̃ᵀ  . (25) Notice that its i’th diagonal block (ηi − η2i )x̃x̃ᵀ resembles the RFIM of a single sigm neuron."
    }, {
      "heading" : "3.3 RFIM of Two Layers",
      "text" : "By eq. (23), the one-layer RFIM is a product metric (Jost, 2011) and does not consider the inter-neuron correlations. We must look at a larger subsystem to obtain such correlations. Consider a two-layer model with stochastic output y around the mean vector f (Cᵀh), where h = f (W ᵀx), as shown in fig. 3. For simplicity, we ignore inter-layer correlations between the first layer and the second layer and focus on the inter-neuron correlations within the first layer. To do this, both x and C are considered as references to compute the RFIM of W . By definition 1, gy(W |x,C) = [Gij ]Dh×Dh and each block\nGij = Dy∑ l=1 cilcjlνf (cl,h)νf (wi,x)νf (wj ,x)x̃x̃ ᵀ, ∀1 ≤ i ≤ Dh, ∀1 ≤ j ≤ Dh. (26)\nConsider the computational difficulty of eq. (26) that is only listed here as an analytical contribution with possible empirical extensions."
    }, {
      "heading" : "4 Advantages of RFIM",
      "text" : "This section discusses theoretical advantages of RFIM. Consider wlog a MLP with one Bernoulli output y, whose mean µ is a deterministic function depending on the input x and the network\nparameters Θ. By Sec. 2, the FIM of the MLP can be computed as\ng(Θ) = µi ∂ lnµi ∂Θ ∂ lnµi ∂Θᵀ + (1− µi) ∂ ln(1− µi) ∂Θ ∂ ln(1− µi) ∂Θᵀ = 1 µi(1− µi) ∂µi ∂Θ ∂µi ∂Θᵀ . (27)\nTherefore rank(g(Θ)) ≤ n, as each sample contributes maximally 1 to the rank of g(Θ). A small diagonal block of g(Θ), representing one layer, is likely to have a rank much lower than the sample size. If the number of parameters is greater than the sample size, which can be achieved especially with deep learning (Szegedy et al., 2015), then g(Θ) is guaranteed to be singular. All methods trying to approximate FIM suffers from this problem and should use proper regularizations. Comparatively, RFIM decomposes the network in a layer-wise manner. In each layer h = f(W ᵀx), by eq. (23), rank(gh(W )) ≤ nDh, which is an achievable upper bound. Therefore, RFIM is expected to have a much higher rank than FIM. Higher rank means less singularity, and more information is captured, and easier to reparameterize to achieve good optimization properties. Essentially, RFIM integrates the internal stochasticity (Bengio, 2013) of the neural system by considering the output of each layer as random variables. In theory, the computation of FIM should also consider stochastic neurons. However it requires to marginalize the joint distribution of h1, h2, · · · , y. This makes the already infeasible computation even more difficult.\nRFIM is accurate, for that the geometry of θ is defined wrt to its direct response h in the system, or adjacent nodes in a graphical model. By the example in Sec. 2 and eq. (9), the RFIM gy(θL) is exactly the corresponding block in the FIM I(Θ), because they both investigate how θL affects the last layer hL−1 → y. They start to diverge from the second last layer. To compute the geometry of θL−1, RFIM looks at how θL−1 affects the local mapping hL−1 → hL. Intuitively, this local relationship can be more reliably measured regardless of the rest of the system. In contrast, FIM examines how θL−1 affects the global mapping hL−1 → y. This task is much more difficult, because it must consider the correlation between different layers. This is hard to perform without approximation techniques. As a commonly used approximation, the block diagonalized version of FIM will ignore such correlations and loose accuracy.\nThe measurement of RFIM makes it possible to maintain global stability of the learning system by balancing different subsystems. Consider two connected subsystems with internal parameters θ1 and θ2 and corresponding responses h1 and h2. A learning step is given by θ1 ← θ1 + δθ1 and θ2 ← θ2 + δθ2, with ‖δθ1‖ ≤ γ and ‖δθ2‖ ≤ γ constrained by a pre-chosen maximum radius γ. To balance the system, we constrain gh1(θ1) and gh2(θ2) to have similar scales, e.g. by normalizing their largest eigenvalue to 1. This can be done through reparametrization tricks. Then the intrinsic changes, or variations, of the responses h1 and h2 also have similar scales and is upper bounded. Note that the responses h1 and h2 are the interfaces of subsystems. For example, in the MLP shown in fig. 2, the response h1 in subsystem 1 becomes the reference in subsystem 2. By bounding the variation of h1, the Riemannian metric gh2(θ2 |h1) will present less variations during learning."
    }, {
      "heading" : "5 Relative Natural Gradient Descent (RNGD)",
      "text" : "The traditional non-parametric way of applying natural gradient requires to re-calculate the FIM and solving a large linear system in each learning step. Besides the huge computational cost, it meets some difficulties. For example, in an online learning scenario, a mini batch of samples cannot faithfully reflect the “true” geometry, which has to integrate the risk of sample variations. That is, the FIM of a mini batch is likely to be singular or with bad conditions.\nA recent series of efforts (Montavon and Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach of applying natural gradient, which memorizes\nand learns a geometry. For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers to parametrize the geometry of the neural manifold.\nBy dividing the learning system into subsystems, RFIM makes this parametric approach much more implementable. The memory complexity of storing the Riemannian metric has been reduced from O(#Θ2) to O( ∑ i #θ 2 i ), where each θi corresponds to a subsystem, and #Θ means the dimensionality of Θ. The computational complexity has been reduced from O(#Θ%) (% ≈ 2.373, Williams (2012)) to O( ∑ i #θ % i ). Approximation techniques of FIM to improve these complexities can be applied to RFIM. Optimization based on RFIM will be called Relative Natural Gradient Descent (RNGD). In the following of this section, we present two examples of RNGD. The objective is to demonstrate its advantages and mechanisms.\n5.1 RNGD with a Single sigm Neuron The first experiment is a single neuron model to implement logistic regression (Minka, 2003). The focus of the experiment is to demonstrate the mechanisms of RNGD and to show on a small scale model the improvement made by RNGD over feature whitening techniques, so that we can expect the same improvement on a larger model.\nTo classify a sample set {(xi, yi)} with features x and class labels y ∈ {0, 1}, a statistical model is assumed as\np(y = 1) = sigm(θᵀz),\nz = ((x− a)ᵀAᵀ, 1)ᵀ. (28)\nIn eq. (28), θ is the canonical parameters or the link weights of the neuron. A and a are for feature whitening (Montavon and Müller, 2012; Desjardins et al., 2015). They are precomputed and fixed during learning, so that the transformed samples {A(xi − a)} have zero mean and unit covariance except singular dimensions. The learning cost function is the average cross entropy\nL(θ) = −yi ln sigm(θᵀzi)− (1− yi) ln [1− sigm(θᵀzi)], (29)\nwhose gradient is simply 5L = (sigm(θᵀzi)− yi) zi. To apply RNGD, by eq. (14), we have gy(θ) = νsigm (θ, zi) ziz ᵀ i . In each learning step, we update θ based on\nθnew ← θold − γ(gy(θ) + I)−1 5 L, (30)\nwhere γ is a learning rate, and ε > 0 is a hyper-parameter to guarantee that (gy(θ) + I) is invertible. We choose empirically to be εtr(gy(θ))/D, where ε = 10−2.\nBased on a gradient descent optimizer with constant learning rate and momentum, we compare four different methods:\n1. GD fixes A = I, a = 0, and applies gradient descent;\n2. WhiteGD performs feature whitening by pre-computing A and a as well as gradient descent;\n3. NGD fixes A = I, a = 0, and updates θ based on eq. (30);\n4. WhiteNGD performs both feature whitening and the updating scheme in eq. (30).\nBased on the MNIST dataset (LeCun et al., 1998), fig. 4 shows their learning curves on binary classification problems “3” vs “5” and “4” vs “9” on two different training sizes. For each method, the best curve which achieved the minimal training error after 100 epochs is shown. The configuration\ngrid is given as follows. The candidate learning rate is in the range {10−2, 10−1, 1, 10, 100}. The candidate momentum is in the range {0, 0.8}.\nIn different cases, WhiteNGD and NGD can consistently reach deeper in the error surface within a reasonable number of epochs as compared to WhiteGD and GD. Among all methods, WhiteNGD performs best, which demonstrates the dependency of natural gradient and RFIM on the coordinate system. In a whitened coordinate system, RFIM is expected to have better conditions in average, and therefore leading to better optimization. Intuitively, in eq. (14), the term νsigm (θ,x) serves as a “selector”, highlighting a subset of {xi} in the linear region of the perceptron. The updating rule in eq. (30) will zoom in and de-correlate the sample variance in this region, and let the classifier focus on the discriminative samples.\nGradient descent depends much more on the choice of the coordinate system. There is a\nsignificant improvement from GD to WhiteGD. A whitened coordinate system allows larger learning rates. In the first several epochs, WhiteGD can even learn faster than NGD and WhiteNGD, as shown in fig. 4c.\n5.2 RNGD with a relu MLP The good performance of batch normalization (BN) (Ioffe and Szegedy, 2015) can be explained using RFIM. Basically, BN uses an inter-sample normalization layer to transform the input of each layer, denoted as x, to be zero mean and unit variance and thus reduces “internal covariate shift”. In a typical case, above this normalization layer is a linear layer given by y = W ᵀx. By eq. (22), if x is normalized, then the diagonal entries of gy(W ) becomes uniform. Therefore the geometry of the parameter manifold is conditioned. In this case, BN helps to condition the RFIM of a linear layer.\nThis subsection looks at a larger subsystem consisting of a linear layer plus a nonlinear activation layer above it, given by y = f(W ᵀx). By eq. (23), its RFIM is diag [ νf (w1,x)x̃x̃ ᵀ, · · · , νf (wm,x)x̃x̃ᵀ ]. To perform RNGD, one need to update this layer by\nwnew1 ← wold1 − νf (w1,xi)x̃ix̃ ᵀ i + I\n−1 ∂E\n∂w1 ,\n· · ·\nwnewm ← woldm − νf (wm,xi)x̃ix̃ ᵀ i + I\n−1 ∂E\n∂wm , (31)\nwhere E is the cost function, and > 0 is a hyper parameter to avoid singularity. However, this update requires solving many linear subsystems and is too expensive to compute. Moreover, we only have a mini batch which contains not enough information to compute the RFIM. To tackle these difficulties, we maintain an exponentially moving average of Gl, the RFIM of the l’th neuron in this layer. Initially, Gl is initialized to identity. At each iteration, it is updated by\nGnewl ← λGoldl + (1− λ)νf (wl,xi)x̃ix̃ ᵀ i + I, (32)\nwhere the average is taken over all samples in this mini batch, and λ is a decaying rate. Every T iterations, we recompute G−1l based on the most current Gl, and store the resulting G −1 l . In the next T iterations, this G−1l will remain constant and be used as an approximation of inverse RFIM. Then, the updating rule of the layer is given by\nwnew1 ← wold1 −G−11 ∂E ∂w1 · · · wnewm ← woldm −G−1m ∂E ∂wm . (33)\nFor the input layer which scales with the number of input features, and the final soft-max layer, we apply instead the RFIM of the corresponding linear layer for the consideration of the computational efficiency.\nWe implemented the proposed method using TensorFlow (Abadi et al., 2015) and applied it to classify MNIST digits. The network has shape 784-64-64-64-10, with relu activation units, a final soft-max layer, and uses average cross-entropy as the cost function.\nFigure 5 shows the learning curves of different methods. SGD is stochastic gradient descent. ADAM is the Adam optimizer (Kingma and Ba, 2014). PLAIN means a plain MLP without batch normalization. BNA and BNB are two different implementations of BN, depending on whether BN is performed right before (BNB) or right after (BNA) the activation of the hidden units. They both use a re-scaling parameter to ensure enough flexibility of the parametric structure (Ioffe and Szegedy, 2015). The epsilon parameter of both BNA and BNB is set to 10−3. For RNGD, we\nset empirically T = 100 and λ = 0.995. The right table shows the mean and standard deviation of the τ -sharp ratio, defined as the training cost over the last τM mini-batch iterations, where M = 55000 is the total number of iterations.\nWe can see that RNGD can significantly improve the learning curve over the other methods and achieve a smaller sharp ratio. The mechanism is similar to the first experiment. By eq. (18), νrelu(wi,x) is approximately binary, emphasizing such informative samples with wᵀi x̃ > 0, which are the ones contributing to the learning of wi with non-zero gradient values. Each output neuron has a different subset of informative samples. While BN normalizes the layer input x regardless of the output neurons, RNGD normalizes x differently wrt different output, so that the informative samples for each output neuron is centered and decorrelated.\nAs shown in fig. 5, RNGD appears to have a larger variation during learning. This is because of the non-smooth update of the inverse metric. One can however see from the right table that the variation is actually not much, as the y-axis is in log-scale.\nBy the results of RNGD, it is clear that this MLP structure overfits the input data. It is a fundamental trade-off between fitting the input data and generalizing. This objective of this experiment is mainly focused on improving learning optimization. The overfitting can be tackled by early stopping.\nRNGD’s computational time per each iteration is much more than the other methods. In our experiments on a GPU instance of Amazon EC2, RNGD costs around half a minute per each epoch, while the other methods only costs seconds. On CPUs RNGD is even more time consuming. This is both due to the inefficiency of our implementation and its algorithm complexity. To seek efficient RNGD implementations is left for future work."
    }, {
      "heading" : "6 Conclusion and discussion",
      "text" : "We propose to investigate local structures of large learning systems using the new concept of Relative Fisher Information Matrix (RFIM). The key advantage of this approach is that the local dynamics can be analyzed in an accurate way without approximation. We present a core list of such local structures in neuron networks, and give their corresponding RFIMs. This list of recipes can be used to provide guiding principles to neuron networks. As a first example, we demonstrated how to apply single neuron RFIM and one layer RFIM to improve the learning curve by using the relative natural gradient descent.\nOur work applies to mirror descent as well since natural gradient is related to mirror descent (Raskutti and Mukherjee, 2015) as follows: In mirror descent, given a strictly convex distance function D(·, ·) in the first argument (playing the role of the proximity function), we express the gradient descent step as :\nΘt+1 = arg min Θ {Θ>∇F (Θt) +\n1 γ D(Θ,Θt)}.\nWhen D(Θ,Θ′) is chosen as a Bregman divergence BF ((Θ,Θ′) = F (Θ)−F (Θ′)−(Θ−Θ′)>∇F (Θ), it has been proved that the mirror descent on the Θ-parameterization is equivalent (Raskutti and Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (∇2F (Θ)) parameterized by the dual coordinate system H = ∇F (Θ). In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(Θ) on the manifold, one needs to choose a proper metric tensor given in matrix form G(Θ). Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I, the identity matrix) converges. Recently, Thomas et al. (2016) proposed a new kind of descent method based on what they called the Energetic Natural Gradient that generalizes the natural gradient. The energy distance DE(p(Θ1), p(Θ2))2 = E[2dp(Θ1)(X,Y )− dp(Θ1)(X,X\n′) − dp(Θ1)(Y, Y ′)] where X,X ′ ∼ p(Θ1) and Y, Y ′ ∼ p(Θ2), where dp(Θ1)(·, ·) is a distance metric over the support. Using a Taylor’s expansion on their energy distance, they get the Energy Information Matrix (in a way similar to recovering the FIM from a Taylor’s expansion of any f -divergence like the Kullback-Leibler divergence). Their idea is to incorporate prior knowledge on the structure of the support (observation space) to define energy distance. Twisting the geometry of the support (say, Wassertein’s optimal transport) with the geometry of the parametric distributions (Fisher-Rao geodesic distances) is indeed important (Chizat et al., 2015). In information geometry, invariance on the support is provided by a Markov morphism that is a probabilistic mapping of support to itself (Čencov, 1982). Markov morphism include deterministic transformation of a random variable by a statistic. It is well-known that IT (Θ) ≺ IX(Θ) with equality iff. T = T (X) is a sufficient statistic of X. Thus to get the same invariance for the energy distance (Thomas et al., 2016), one shall further require dp(Θ)(T (X), T (Y )) = dp(Θ)(X,Y ).\nIn the foreseeable future, we believe that Relative Fisher Information Metrics (RFIMs) will provide a sound methodology to build further efficient learning techniques in deep learning.\nOur implementation is available at https://www.lix.polytechnique.fr/~nielsen/RFIM."
    }, {
      "heading" : "A Non-linear Activation Functions",
      "text" : "By definition,\ntanh(t) def = exp(t)− exp(−t) exp(t) + exp(−t) , (34)\nand\nsech(t) def =\n2\nexp(t) + exp(−t) . (35)\nIt is easy to verify that\nsech2(t) = [1 + tanh(t)] [1− tanh(t)] = 1− tanh2(t). (36)\nBy eq. (34),\ntanh′(t) = exp(t) + exp(−t) exp(t) + exp(−t) − exp(t)− exp(−t) [exp(t) + exp(−t)]2 [exp(t)− exp(−t)]\n= [exp(t) + exp(−t)]2 − [exp(t)− exp(−t)]2\n[exp(t) + exp(−t)]2 =\n4\n[exp(t) + exp(−t)]2 = sech2(t). (37)\nBy definition,\nsigm(t) def =\n1\n1 + exp(−t) . (38)\nTherefore\nsigm′(t) = − 1 [1 + exp(−t)]2 (− exp(−t)) = exp(−t) [1 + exp(−t)]2 = sigm(t) [1− sigm(t)] . (39)\nBy definition,\nreluω(t) def = ω ln ( exp ( ιt\nω\n) + exp ( t\nω\n)) , (40)\nwhere ω > 0 and 0 ≤ ι < 1. Then,\nrelu′ω(t) = ω 1 exp ( ιt ω ) + exp ( t ω ) ( ι ω exp ( ιt ω ) + 1 ω exp ( t ω )) = ι exp ( ιt ω ) + exp ( t ω ) exp ( ιt ω ) + exp ( t ω\n) = ι+ (1− ι) exp ( t ω ) exp ( ιt ω ) + exp ( t ω\n) = ι+ (1− ι) 1\nexp ( (ι− 1) tω ) + 1\n= ι+ (1− ι)sigm (\n1− ι ω t\n) . (41)\nBy definition,\nelu(t) = { t if t ≥ 0 α (exp(t)− 1) if t < 0. (42)\nTherefore elu′(t) = { 1 if t ≥ 0 α exp(t) if t < 0. (43)\nB A Single tanh Neuron Consider a neuron with parameters w and a Bernoulli output y ∈ {+,−}, p(y = +) = p+, p(y = −) = p−, and p+ + p− = 1. By the definition of RFIM, we have\ngy(w) = p+ ∂ ln p+\n∂w\n∂ ln p+\n∂wᵀ + p−\n∂ ln p−\n∂w\n∂ ln p−\n∂wᵀ\n= 1 p+ ∂p+ ∂w ∂p+ ∂wᵀ + 1 p− ∂p− ∂w ∂p− ∂wᵀ . (44)\nSince p+ + p− = 1,\n∂p+ ∂w + ∂p− ∂w = 0. (45)\nTherefore, the RFIM of a Bernoulli neuron has the general form\ngy(w) =\n( 1\np+ +\n1\np−\n) ∂p+\n∂w\n∂p+ ∂wᵀ = 1 p+p− ∂p+ ∂w ∂p+ ∂wᵀ . (46)\nA single tanh neuron with stochastic output y ∈ {−1, 1} is given by\np(y = −1) = 1− µ(x) 2 , (47)\np(y = 1) = 1 + µ(x)\n2 , (48)\nµ(x) = tanh(wᵀx̃). (49)\nBy eq. (46),\ngy(w) = 1\n1−µ(x) 2 1+µ(x) 2\n( 1\n2\n∂µ\n∂w\n)( 1\n2\n∂µ\n∂wᵀ ) = 1\n(1− µ(x)) (1 + µ(x)) [ 1− µ2(x) ]2 x̃x̃ᵀ\n= [ 1− µ2(x) ] x̃x̃ᵀ\n= [ 1− tanh2(wᵀx̃) ] x̃x̃ᵀ\n= sech2(wᵀx̃)x̃x̃ᵀ. (50)\nAn alternatively analysis is given as follows. By eqs. (47) to (49),\np(y = −1) = exp(−w ᵀx̃)\nexp(wᵀx) + exp(−wᵀx) , (51)\np(y = 1) = exp(wᵀx̃)\nexp(wᵀx̃) + exp(−wᵀx̃) . (52)\nThen,\ngy(w) = Ey∼p(y |x) ( −∂ 2 ln p(y)\n∂w∂wᵀ ) = ∂2\n∂w∂wᵀ ln [exp(wᵀx̃) + exp(−wᵀx̃)] (first linear term vanishes)\n= ∂\n∂wᵀ [ exp(wᵀx̃)− exp(−wᵀx̃) exp(wᵀx̃) + exp(−wᵀx̃) ] x̃\n= ∂\n∂wᵀ tanh(wᵀx̃)x̃\n= sech2(wᵀx̃)x̃x̃ᵀ. (53)\nThe intuitive meaning of gy(w) is a weighted covariance to emphasize such “informative” x’s that\n• are in the linear region of tanh\n• contain “ambiguous” samples\nWe will need at least dim(w) samples to make gy(w) full rank.\nC A Single sigm Neuron A single sigm neuron is given by\np(y = 0) = 1− µ(x), (54) p(y = 1) = µ(x), (55)\nµ(x) = sigm(wᵀx̃). (56)\nBy eq. (46),\ngy(w) = 1\np(y = 0)p(y = 1)\n∂p(y = 1)\n∂w\n∂p(y = 1)\n∂wᵀ\n= 1 µ(x)(1− µ(x)) ∂µ ∂w ∂µ ∂wᵀ\n= 1\nµ(x)(1− µ(x)) µ2(x)(1− µ(x))2x̃x̃ᵀ\n= µ(x)(1− µ(x))x̃x̃ᵀ\n= sigm(wᵀx̃) [1− sigm(wᵀx̃)] x̃x̃ᵀ. (57)\nD A Single relu Neuron Consider a single neuron with Gaussian output p(y |w,x) = G(y |µ(w,x), σ2). Then\ngy(w |x) = Ep(y |w,x) [ ∂ lnG(y |µ, σ2)\n∂w\n∂ lnG(y |µ, σ2) ∂wᵀ ] = Ep(y |w,x) [ ∂\n∂w\n( − 1\n2σ2 (y − µ)2\n) ∂\n∂wᵀ\n( − 1\n2σ2 (y − µ)2 )] = Ep(y |w,x) [( − 1 σ2 (µ− y) )2 ∂µ ∂w ∂µ ∂wᵀ ]\n= 1\nσ4 Ep(y |w,x) (µ− y)\n2 ∂µ\n∂w\n∂µ\n∂wᵀ\n= 1 σ2 ∂µ ∂w ∂µ ∂wᵀ . (58)\nA single relu neuron is given by\nµ(w,x) = reluω(w ᵀx̃). (59)\nBy eqs. (41) and (58),\ngy(w) = 1\nσ2\n[ ι+ (1− ι)sigm ( 1− ι ω wᵀx̃ )]2 x̃x̃ᵀ. (60)\nE A Single elu Neuron Similar to the analysis in appendix D, a single elu neuron is given by\nµ(w,x) = elu(wᵀx̃). (61)\nBy eq. (43), ∂µ\n∂w = { x̃ if wᵀx̃ ≥ 0 α exp(wᵀx̃)x̃ if wᵀx̃ < 0. (62)\nBy eq. (58),\ngy(w) =\n{ 1 σ2 x̃x̃\nᵀ if wᵀx̃ ≥ 0 1 σ2 (α exp(w ᵀx̃)) 2 x̃x̃ᵀ if wᵀx̃ < 0.\n(63)"
    }, {
      "heading" : "F RFIM of a Linear Layer",
      "text" : "Consider a linear layer\np(y) = G ( y |W ᵀx̃, σ2I ) , (64)\nwhere W = (w1, · · · ,wDy ). By the definition of multivariate Gaussian distribution,\nln p(y) = −1 2 ln 2π − Dy 2 lnσ2 − 1 2σ2 Dy∑ i=1 (yi −wᵀi x̃) 2 . (65)\nTherefore,\n∀i, ∂ ∂wi ln p(y) = − 1 σ2 (wᵀi x̃− yi) x̃. (66)\nTherefore,\n∀i,∀j ∂ ∂wi ln p(y) ∂ ∂wᵀj ln p(y) = 1 σ4 (yi −wᵀi x̃)\n( yj −wᵀj x̃ ) x̃x̃ᵀ. (67)\nW is vectorized by stacking its columns {wi} Dy i=1. In the followingW will be used interchangeably to denote either the matrix or its vector form. Correspondingly, the RFIM gy(W ) has Dy ×Dy blocks, where the off-diagonal blocks are\n∀i 6= j, Ep(y)\n( ∂\n∂wi ln p(y)\n∂\n∂wᵀj ln p(y)\n) = 1\nσ4 Ep(y)\n[ (yi −wᵀi x̃) ( yj −wᵀj x̃ )] x̃x̃ᵀ = 0, (68)\nand the diagonal blocks are ∀i, Ep(y) ( ∂\n∂wi ln p(y)\n∂\n∂wᵀi ln p(y)\n) = 1\nσ4 Ep(y) (yi −wᵀi x̃) 2 x̃x̃ᵀ =\n1\nσ2 x̃x̃ᵀ. (69)\nIn summary,\ngy(W ) = 1\nσ2 diag [x̃x̃ᵀ, · · · , x̃x̃ᵀ] . (70)"
    }, {
      "heading" : "G RFIM of a Non-Linear Layer",
      "text" : "The statistical model of a non-linear layer is\np(y |W ,x) = Dy∏ i=1 p(yi |wi,x). (71)\nThen,\nln p(y |W ,x) = Dy∑ i=1 ln p(yi |wi,x). (72)\nTherefore,\n∂2\n∂W ∂W ᵀ ln p(y |W ,x) =\n ∂2 ∂w1∂w ᵀ 1 ln p(y1 |w1,x) . . .\n∂2\n∂wDy∂w ᵀ Dy\nln p(yDy |wDy ,x)\n . (73)\nTherefore RFIM gy(W ) is a block-diagonal matrix, with the i’th block given by\n−Ep(y |W ,x) [ ∂2\n∂wi∂w ᵀ i\nln p(yi |wi,x) ] = −Ep(yi |wi,x) [ ∂2\n∂wi∂w ᵀ i\nln p(yi |wi,x) ] , (74)\nwhich is simply the single neuron RFIM of the i’th neuron."
    }, {
      "heading" : "H RFIM of a Softmax Layer",
      "text" : "Recall that\n∀i ∈ {1, · · · ,m} , p(y = i) = exp(wix̃)∑m i=1 exp(wix̃) . (75)\nThen\n∀i, ln p(y = i) = wix̃− ln m∑ i=1 exp(wix̃). (76)\nHence ∀i, ∀j, ∂ ln p(y = i)\n∂wj = δijx̃− exp(wjx̃)∑m i=1 exp(wix̃) x̃, (77)\nwhere δij = 1 if and only if i = j and δij = 0 otherwise. Then\n∀i, ∀j, ∀k, ∂ 2 ln p(y = i)\n∂wj∂w ᵀ k = −δjk exp(wjx̃)∑m i=1 exp(wix̃) x̃x̃ᵀ + exp(wjx̃) ( ∑m i=1 exp(wix̃)) 2 exp(wkx̃)x̃x̃ ᵀ\n= (−δjkηj + ηjηk) x̃x̃ᵀ. (78)\nThe right-hand-side of eq. (78) does not depend on i. Therefore\ngy(W ) =  (η1 − η21)x̃x̃ᵀ −η1η2x̃x̃ᵀ · · · −η1ηmx̃x̃ᵀ −η2η1x̃x̃ᵀ (η2 − η22)x̃x̃ᵀ · · · −η2ηmx̃x̃ᵀ ... ... . . . ...\n−ηmη1x̃x̃ᵀ −ηmη2x̃x̃ᵀ · · · (ηm − η2m)x̃x̃ᵀ\n . (79)"
    }, {
      "heading" : "I RFIM of Two layers",
      "text" : "Consider a two layer structure, where the output y satisfies a multivariate Bernoulli distribution with independent dimensions. By a similar analysis to appendix B, we have\ngy(W ) = Dy∑ l=1 νf (cl,h) ∂cᵀl h ∂W ∂cᵀl h ∂W ᵀ . (80)\nIt can be written block by block as gy(W ) = [Gij ]Dh×Dh , where each block Gij means the correlation between the i’th hidden neuron with weights wi and the j’th hidden neuron with\nweights wj . By eq. (80),\nGij = Dy∑ l=1 νf (cl,h) ∂cᵀl h ∂wi ∂cᵀl h ∂wᵀj = Dy∑ l=1 νf (cl,h) ∂cilhi ∂wi ∂cjlhj ∂wᵀj\n= Dy∑ l=1 νf (cl,h)cilcjl ∂hi ∂wi ∂hj ∂wᵀj = Dy∑ l=1 νf (cl,h)cilcjl (νf (wi,x)x̃) (νf (wj ,x)x̃ ᵀ)\n= Dy∑ l=1 cilcjlνf (cl,h)νf (wi,x)νf (wj ,x)x̃x̃ ᵀ. (81)\nThe proof of the other case, where two relu layers have stochastic output y satisfying a multivariate Gaussian distribution with independent dimensions, is very similar and is omitted."
    } ],
    "references" : [ {
      "title" : "Our implementation is available at https://www.lix.polytechnique.fr/~nielsen/RFIM",
      "author" : [ "M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo" ],
      "venue" : null,
      "citeRegEx" : "Abadi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Abadi et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural learning in structured parameter spaces – natural Riemannian gradient",
      "author" : [ "S. Amari" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Amari.,? \\Q1997\\E",
      "shortCiteRegEx" : "Amari.",
      "year" : 1997
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "S. Amari" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Amari.,? \\Q1998\\E",
      "shortCiteRegEx" : "Amari.",
      "year" : 1998
    }, {
      "title" : "Information Geometry and its Applications, volume 194 of Applied Mathematical Sciences",
      "author" : [ "S. Amari" ],
      "venue" : null,
      "citeRegEx" : "Amari.,? \\Q2016\\E",
      "shortCiteRegEx" : "Amari.",
      "year" : 2016
    }, {
      "title" : "Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs",
      "author" : [ "S. Amari", "H. Nagaoka" ],
      "venue" : "AMS and OUP,",
      "citeRegEx" : "Amari and Nagaoka.,? \\Q2000\\E",
      "shortCiteRegEx" : "Amari and Nagaoka.",
      "year" : 2000
    }, {
      "title" : "Adaptive method of realizing natural gradient learning for multilayer perceptrons",
      "author" : [ "S. Amari", "H. Park", "K. Fukumizu" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Amari et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Amari et al\\.",
      "year" : 2000
    }, {
      "title" : "Estimating or propagating gradients through stochastic neurons",
      "author" : [ "Y. Bengio" ],
      "venue" : "CoRR, abs/1305.2982,",
      "citeRegEx" : "Bengio.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2013
    }, {
      "title" : "Stochastic gradient descent on Riemannian manifolds",
      "author" : [ "S. Bonnabel" ],
      "venue" : "IEEE Trans. Automat. Contr.,",
      "citeRegEx" : "Bonnabel.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bonnabel.",
      "year" : 2013
    }, {
      "title" : "Statistical decision rules and optimal inference, volume 53 of Translations of Mathematical Monographs",
      "author" : [ "N.N. Čencov" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "Čencov.,? \\Q1982\\E",
      "shortCiteRegEx" : "Čencov.",
      "year" : 1982
    }, {
      "title" : "An Interpolating Distance between Optimal Transport and Fisher-Rao",
      "author" : [ "L. Chizat", "B. Schmitzer", "G. Peyré", "F.-X. Vialard" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Chizat et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chizat et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast and accurate deep network learning by exponential linear units (elus)",
      "author" : [ "D. Clevert", "T. Unterthiner", "S. Hochreiter" ],
      "venue" : "CoRR, abs/1511.07289,",
      "citeRegEx" : "Clevert et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clevert et al\\.",
      "year" : 2015
    }, {
      "title" : "Estimation and moment recursion relations for multimodal distributions of the exponential family",
      "author" : [ "L. Cobb", "P. Koppstein", "N.H. Chen" ],
      "venue" : "JASA, 78(381):124–130,",
      "citeRegEx" : "Cobb et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Cobb et al\\.",
      "year" : 1983
    }, {
      "title" : "Parameter orthogonality and approximate conditional inference",
      "author" : [ "D.R. Cox", "N. Reid" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Cox and Reid.,? \\Q1987\\E",
      "shortCiteRegEx" : "Cox and Reid.",
      "year" : 1987
    }, {
      "title" : "Natural neural networks",
      "author" : [ "G. Desjardins", "K. Simonyan", "R. Pascanu", "K. Kavukcuoglu" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Desjardins et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Desjardins et al\\.",
      "year" : 2015
    }, {
      "title" : "Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information",
      "author" : [ "B. Efron", "D.V. Hinkley" ],
      "venue" : null,
      "citeRegEx" : "Efron and Hinkley.,? \\Q1978\\E",
      "shortCiteRegEx" : "Efron and Hinkley.",
      "year" : 1978
    }, {
      "title" : "Block diagonalization of four-dimensional metrics",
      "author" : [ "J.D. Grant", "J. Vickers" ],
      "venue" : "Classical and Quantum Gravity,",
      "citeRegEx" : "Grant and Vickers.,? \\Q2009\\E",
      "shortCiteRegEx" : "Grant and Vickers.",
      "year" : 2009
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Probability distributions and orthogonal parameters",
      "author" : [ "V.S. Huzurbazar" ],
      "venue" : "In Mathematical Proceedings of the Cambridge Philosophical Society,",
      "citeRegEx" : "Huzurbazar.,? \\Q1950\\E",
      "shortCiteRegEx" : "Huzurbazar.",
      "year" : 1950
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "In ICML; JMLR: W&CP",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Theory of Probability",
      "author" : [ "H. Jeffreys" ],
      "venue" : null,
      "citeRegEx" : "Jeffreys.,? \\Q1961\\E",
      "shortCiteRegEx" : "Jeffreys.",
      "year" : 1961
    }, {
      "title" : "Riemannian Geometry and Geometric Analysis",
      "author" : [ "J. Jost" ],
      "venue" : "Springer, 6th edition,",
      "citeRegEx" : "Jost.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jost.",
      "year" : 2011
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Deep learning via Hessian-free optimization",
      "author" : [ "J. Martens" ],
      "venue" : "In ICML, pages 735–742,",
      "citeRegEx" : "Martens.,? \\Q2010\\E",
      "shortCiteRegEx" : "Martens.",
      "year" : 2010
    }, {
      "title" : "Optimizing neural networks with Kronecker-factored approximate curvature",
      "author" : [ "J. Martens", "R. Grosse" ],
      "venue" : "In ICML; JMLR: W&CP",
      "citeRegEx" : "Martens and Grosse.,? \\Q2015\\E",
      "shortCiteRegEx" : "Martens and Grosse.",
      "year" : 2015
    }, {
      "title" : "A comparison of numerical optimizers for logistic regression",
      "author" : [ "T.P. Minka" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Minka.,? \\Q2003\\E",
      "shortCiteRegEx" : "Minka.",
      "year" : 2003
    }, {
      "title" : "Computational implications of reducing data to sufficient statistics",
      "author" : [ "A. Montanari" ],
      "venue" : "Electron. J. Statist.,",
      "citeRegEx" : "Montanari.,? \\Q2015\\E",
      "shortCiteRegEx" : "Montanari.",
      "year" : 2015
    }, {
      "title" : "Deep Boltzmann machines and the centering trick",
      "author" : [ "G. Montavon", "K.R. Müller" ],
      "venue" : "In Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "Montavon and Müller.,? \\Q2012\\E",
      "shortCiteRegEx" : "Montavon and Müller.",
      "year" : 2012
    }, {
      "title" : "Rectified linear units improve restricted Boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Revisiting natural gradient for deep networks",
      "author" : [ "R. Pascanu", "Y. Bengio" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Pascanu and Bengio.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pascanu and Bengio.",
      "year" : 2014
    }, {
      "title" : "Deep learning made easier by linear transformations in perceptrons",
      "author" : [ "T. Raiko", "H. Valpola", "Y. LeCun" ],
      "venue" : "In AISTATS; JMLR W&CP",
      "citeRegEx" : "Raiko et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Raiko et al\\.",
      "year" : 2012
    }, {
      "title" : "The information geometry of mirror descent",
      "author" : [ "G. Raskutti", "S. Mukherjee" ],
      "venue" : "ISBN 978-3-319-25039-7",
      "citeRegEx" : "Raskutti and Mukherjee.,? \\Q2015\\E",
      "shortCiteRegEx" : "Raskutti and Mukherjee.",
      "year" : 2015
    }, {
      "title" : "Topmoumoute online natural gradient algorithm",
      "author" : [ "N.L. Roux", "P. Manzagol", "Y. Bengio" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Roux et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2008
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "GeNGA: A generalization of natural gradient ascent with positive and negative convergence results",
      "author" : [ "P. Thomas" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Thomas.,? \\Q2014\\E",
      "shortCiteRegEx" : "Thomas.",
      "year" : 2014
    }, {
      "title" : "Energetic natural gradient descent",
      "author" : [ "P. Thomas", "B.C. da Silva", "C. Dann", "E. Brunskill" ],
      "venue" : "In Proceedings of the 33st International Conference on Machine Learning",
      "citeRegEx" : "Thomas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Thomas et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropout training as adaptive regularization",
      "author" : [ "S. Wager", "S. Wang", "P.S. Liang" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Wager et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wager et al\\.",
      "year" : 2013
    }, {
      "title" : "Algebraic Geometry and Statistical Learning Theory, volume 25 of Cambridge Monographs on Applied and Computational Mathematics",
      "author" : [ "S. Watanabe" ],
      "venue" : null,
      "citeRegEx" : "Watanabe.,? \\Q2009\\E",
      "shortCiteRegEx" : "Watanabe.",
      "year" : 2009
    }, {
      "title" : "Multiplying matrices faster than coppersmith-Winograd",
      "author" : [ "V.V. Williams" ],
      "venue" : "In Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Williams.,? \\Q2012\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the mgf may not be available in closed-form nor computationally tractable (Montanari, 2015).",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : ", 1983), the mgf may not be available in closed-form nor computationally tractable (Montanari, 2015).",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 36,
      "context" : "Besides the fact that learning machines usually have often singularities (Watanabe, 2009) (|I(Θ)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large learning system is very challenging due to the finiteness of data, and the large number D(D+1) 2 of matrix coefficients to evaluate.",
      "startOffset" : 73,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "Fisher diagonalization yields parameter orthogonalization (Cox and Reid, 1987), and prove useful when estimating Θ̂ using MLE that is asymptotically normally distributed, Θ̂n = G(Θ, I−1(Θ)/ √ n), where G(θ1,θ2) denotes a univariate or multivariate Gaussian distribution with mean θ1 and variance θ2, and efficient since the variance of the estimator matches the Cramér-Rao lower bound.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "Thus in general, we end up with ( D 2 ) = D(D−1) 2 (non-linear) partial differential equations to satisfy (Huzurbazar, 1950).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 33,
      "context" : "When the FIM may be degenerate, this yields a pseudo-Riemannian manifold (Thomas, 2014).",
      "startOffset" : 73,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "However, such a global orthogonal metric does not exist (Huzurbazar, 1950) when D > 3 for an arbitrary metric tensor although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant and Vickers, 2009).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "However, such a global orthogonal metric does not exist (Huzurbazar, 1950) when D > 3 for an arbitrary metric tensor although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant and Vickers, 2009).",
      "startOffset" : 218,
      "endOffset" : 243
    }, {
      "referenceID" : 3,
      "context" : "For NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016) (Θ1:k,Hk+1:D), where H = Ep[t(x)] = ∇F (Θ) is the moment parameter, for any k ∈ {1, .",
      "startOffset" : 89,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "The geometry of NEFs is a dually flat structure (Amari, 2016) induced by the convex mgf, the potential function.",
      "startOffset" : 48,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Thus in general, we end up with ( D 2 ) = D(D−1) 2 (non-linear) partial differential equations to satisfy (Huzurbazar, 1950). Therefore, in general there is no solution when ( D 2 ) > D, that is when D > 3. When D = 2, the single differential equations is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1 σp0( x−μ σ )} that include the Gaussian family and the Cauchy family.",
      "startOffset" : 107,
      "endOffset" : 355
    }, {
      "referenceID" : 12,
      "context" : "Thus in general, we end up with ( D 2 ) = D(D−1) 2 (non-linear) partial differential equations to satisfy (Huzurbazar, 1950). Therefore, in general there is no solution when ( D 2 ) > D, that is when D > 3. When D = 2, the single differential equations is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1 σp0( x−μ σ )} that include the Gaussian family and the Cauchy family. Sometimes, the structure of the differential equation system yields a solution: For example, Jeffreys (1961) reported a parameter orthogonalization for Pearson’s distributions of type I which is of order D = 4.",
      "startOffset" : 107,
      "endOffset" : 608
    }, {
      "referenceID" : 9,
      "context" : "Cox and Reid (1987) further investigate this topic with application to conditional inference, and provide examples (including the Weibull distribution).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "where “ · ” denotes the sample average over {xk}, the resulting metric is called the observed FIM (Efron and Hinkley, 1978).",
      "startOffset" : 98,
      "endOffset" : 123
    }, {
      "referenceID" : 31,
      "context" : "Past works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015).",
      "startOffset" : 152,
      "endOffset" : 238
    }, {
      "referenceID" : 22,
      "context" : "Past works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015).",
      "startOffset" : 152,
      "endOffset" : 238
    }, {
      "referenceID" : 28,
      "context" : "Past works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015).",
      "startOffset" : 152,
      "endOffset" : 238
    }, {
      "referenceID" : 23,
      "context" : "Past works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015).",
      "startOffset" : 152,
      "endOffset" : 238
    }, {
      "referenceID" : 27,
      "context" : "Note that Restricted Boltzmann Machines (Nair and Hinton, 2010; Montavon and Müller, 2012) (RBMs), and dropout (Wager et al.",
      "startOffset" : 40,
      "endOffset" : 90
    }, {
      "referenceID" : 26,
      "context" : "Note that Restricted Boltzmann Machines (Nair and Hinton, 2010; Montavon and Müller, 2012) (RBMs), and dropout (Wager et al.",
      "startOffset" : 40,
      "endOffset" : 90
    }, {
      "referenceID" : 35,
      "context" : "Note that Restricted Boltzmann Machines (Nair and Hinton, 2010; Montavon and Müller, 2012) (RBMs), and dropout (Wager et al., 2013) do consider h to be stochastic.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "In this update procedure, the term g(Θt) 5 L(Θt) replaces the role of the usual gradient 5L(Θt) and is called the natural gradient (Amari, 1997).",
      "startOffset" : 131,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "4 (Amari, 1998)).",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 36,
      "context" : "This set of parameters forms an analytic variety (Watanabe, 2009), and technically the MLP as a statistical model is said non-regular (and the parameter Θ is not identifiable).",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 33,
      "context" : "The natural gradient has been extended (Thomas, 2014) to cope with singular FIMs having positive semi-definite matrices by taking the Moore-Penrose pseudo-inverse (that coincides with the inverse matrix for full rank matrices).",
      "startOffset" : 39,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "(Martens, 2010), by checking whether the computation of the Hessian term depends on the cost function.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 5,
      "context" : "We refer the reader to related references (Amari et al., 2000) for more details.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "Bonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a step gradient descent, thus ensuring to stay on the manifold for any chosen learning rate.",
      "startOffset" : 9,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "In fact, the particular case of a mixed coordinate system (that is not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations.",
      "startOffset" : 132,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "Our splits in RFIMs consider non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that are the leaves of the foliation (Amari and Nagaoka, 2000).",
      "startOffset" : 179,
      "endOffset" : 204
    }, {
      "referenceID" : 1,
      "context" : "Note that, the FIMs of small parametric structures such as single neurons have been studied for a long time (Amari, 1997).",
      "startOffset" : 108,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "A neuron with sigmoid activation but continuous output was discussed earlier (Amari, 1997).",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : "3 Parametric Rectified Linear Unit Another commonly used activation function is Parametric Rectified Linear Unit (PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) as a special case.",
      "startOffset" : 121,
      "endOffset" : 138
    }, {
      "referenceID" : 27,
      "context" : ", 2015), which includes Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) as a special case.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "4 Exponential Linear Unit A stochastic exponential linear unit (ELU) (Clevert et al., 2015) with α > 0 is",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "(23), the one-layer RFIM is a product metric (Jost, 2011) and does not consider the inter-neuron correlations.",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "If the number of parameters is greater than the sample size, which can be achieved especially with deep learning (Szegedy et al., 2015), then g(Θ) is guaranteed to be singular.",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "Essentially, RFIM integrates the internal stochasticity (Bengio, 2013) of the neural system by considering the output of each layer as random variables.",
      "startOffset" : 56,
      "endOffset" : 70
    }, {
      "referenceID" : 26,
      "context" : "A recent series of efforts (Montavon and Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach of applying natural gradient, which memorizes",
      "startOffset" : 27,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "A recent series of efforts (Montavon and Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach of applying natural gradient, which memorizes",
      "startOffset" : 27,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "A recent series of efforts (Montavon and Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach of applying natural gradient, which memorizes",
      "startOffset" : 27,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers to parametrize the geometry of the neural manifold.",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers to parametrize the geometry of the neural manifold. By dividing the learning system into subsystems, RFIM makes this parametric approach much more implementable. The memory complexity of storing the Riemannian metric has been reduced from O(#Θ) to O( ∑ i #θ 2 i ), where each θi corresponds to a subsystem, and #Θ means the dimensionality of Θ. The computational complexity has been reduced from O(#Θ) (% ≈ 2.373, Williams (2012)) to O( ∑ i #θ % i ).",
      "startOffset" : 38,
      "endOffset" : 571
    }, {
      "referenceID" : 24,
      "context" : "1 RNGD with a Single sigm Neuron The first experiment is a single neuron model to implement logistic regression (Minka, 2003).",
      "startOffset" : 112,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : "A and a are for feature whitening (Montavon and Müller, 2012; Desjardins et al., 2015).",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "A and a are for feature whitening (Montavon and Müller, 2012; Desjardins et al., 2015).",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : "2 RNGD with a relu MLP The good performance of batch normalization (BN) (Ioffe and Szegedy, 2015) can be explained using RFIM.",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "We implemented the proposed method using TensorFlow (Abadi et al., 2015) and applied it to classify MNIST digits.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "ADAM is the Adam optimizer (Kingma and Ba, 2014).",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "They both use a re-scaling parameter to ensure enough flexibility of the parametric structure (Ioffe and Szegedy, 2015).",
      "startOffset" : 94,
      "endOffset" : 119
    }, {
      "referenceID" : 30,
      "context" : "Our work applies to mirror descent as well since natural gradient is related to mirror descent (Raskutti and Mukherjee, 2015) as follows: In mirror descent, given a strictly convex distance function D(·, ·) in the first argument (playing the role of the proximity function), we express the gradient descent step as :",
      "startOffset" : 95,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : "When D(Θ,Θ′) is chosen as a Bregman divergence BF ((Θ,Θ′) = F (Θ)−F (Θ′)−(Θ−Θ′)>∇F (Θ), it has been proved that the mirror descent on the Θ-parameterization is equivalent (Raskutti and Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (∇F (Θ)) parameterized by the dual coordinate system H = ∇F (Θ).",
      "startOffset" : 171,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : "Twisting the geometry of the support (say, Wassertein’s optimal transport) with the geometry of the parametric distributions (Fisher-Rao geodesic distances) is indeed important (Chizat et al., 2015).",
      "startOffset" : 177,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "In information geometry, invariance on the support is provided by a Markov morphism that is a probabilistic mapping of support to itself (Čencov, 1982).",
      "startOffset" : 137,
      "endOffset" : 151
    }, {
      "referenceID" : 34,
      "context" : "Thus to get the same invariance for the energy distance (Thomas et al., 2016), one shall further require dp(Θ)(T (X), T (Y )) = dp(Θ)(X,Y ).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 28,
      "context" : "When D(Θ,Θ′) is chosen as a Bregman divergence BF ((Θ,Θ′) = F (Θ)−F (Θ′)−(Θ−Θ′)>∇F (Θ), it has been proved that the mirror descent on the Θ-parameterization is equivalent (Raskutti and Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (∇F (Θ)) parameterized by the dual coordinate system H = ∇F (Θ). In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(Θ) on the manifold, one needs to choose a proper metric tensor given in matrix form G(Θ). Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I, the identity matrix) converges.",
      "startOffset" : 172,
      "endOffset" : 555
    }, {
      "referenceID" : 28,
      "context" : "When D(Θ,Θ′) is chosen as a Bregman divergence BF ((Θ,Θ′) = F (Θ)−F (Θ′)−(Θ−Θ′)>∇F (Θ), it has been proved that the mirror descent on the Θ-parameterization is equivalent (Raskutti and Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (∇F (Θ)) parameterized by the dual coordinate system H = ∇F (Θ). In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(Θ) on the manifold, one needs to choose a proper metric tensor given in matrix form G(Θ). Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I, the identity matrix) converges. Recently, Thomas et al. (2016) proposed a new kind of descent method based on what they called the Energetic Natural Gradient that generalizes the natural gradient.",
      "startOffset" : 172,
      "endOffset" : 730
    } ],
    "year" : 2016,
    "abstractText" : "Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks. However related analysis becomes more and more difficult as the learner’s structure turns large and complex. This paper makes a preliminary step towards a new direction. We extract a local component of a large neuron system, and defines its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system. This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks. We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization.",
    "creator" : "LaTeX with hyperref package"
  }
}