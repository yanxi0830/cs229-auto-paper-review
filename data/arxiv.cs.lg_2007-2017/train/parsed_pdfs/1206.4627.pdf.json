{
  "name" : "1206.4627.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models",
    "authors" : [ "Jean Honorio" ],
    "emails" : [ "jhonorio@cs.sunysb.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Structure learning aims to discover the topology of a probabilistic network of variables such that this network represents accurately a given dataset while maintaining low complexity. Accuracy of representation is measured by the likelihood that the model explains the observed data, while complexity of a graphical model is measured by its number of parameters.\nOne challenge of structure learning is that the number of possible structures is super-exponential in the number of variables. For Ising models, the number of parameters, the number of edges in the structure and\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nthe number of non-zero elements in the ferro-magnetic coupling matrix are equivalent measures of model complexity. Therefore a computationally tractable approach is to use sparseness promoting regularizers (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009).\nOne additional challenge for Ising models (and Markov random fields in general) is that computing the likelihood of a candidate structure is NP-hard. For this reason, several researchers propose exact optimization of approximate objectives, such as `1-regularized logistic regression (Wainwright et al., 2006), greedy optimization of the conditional log-likelihoods (Jalali et al., 2011), pseudo-likelihood (Besag, 1975) and a sequence of first-order approximations of the exact log-likelihood (Höfling & Tibshirani, 2009). Several convex upper bounds and approximations to the logpartition function have been proposed for maximum likelihood estimation, such as the log-determinant relaxation (Banerjee et al., 2008), the cardinality bound (El Ghaoui & Gueye, 2008), the Bethe entropy (Lee et al., 2006; Parise & Welling, 2006), treereweighted approximations and general weighted freeenergy (Yang & Ravikumar, 2011).\nIn this paper, we focus on the stochastic optimization of the exact log-likelihood as our motivating problem. The use of stochastic maximum likelihood dates back to (Geyer, 1991; Younes, 1988), in which Markov chain Monte Carlo (MCMC) was used for approximating the gradient. For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al., 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al., 2010).\nEmpirical results in (Marlin et al., 2010) suggests that stochastic maximum likelihood is superior to con-\ntrastive divergence, pseudo-likelihood, ratio matching and generalized score matching for learning restricted Boltzmann machines, in the sense that it produces a higher test set log-likelihood, and more consistent classification results across datasets.\nLearning sparse Ising models leads to the use of stochastic optimization with biased estimates of the gradient. Most work in stochastic optimization assumes the availability of unbiased estimates (Duchi & Singer, 2009; Duchi et al., 2010; Hu et al., 2009; Nemirovski et al., 2009). Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d’Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level. Similarly, Devolder (2012) analyzed the case of stochastic errors with fixed bias and variance and show convergence up to a constant level.\nNotable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions. In our work, we restrict our analysis to exact projection steps and do not assume strong convexity. Both assumptions are natural for learning sparse models under the `1 regularization. Friedlander & Schmidt (2011) provides convergence rates in expected value for PG with stochastic errors that decrease over time in expected value. Friedlander & Schmidt (2011) proposes a growing sample-size strategy for approximating the gradient, i.e. by picking an increasing number of training samples in order to better approximate the gradient. In contrast, our work is for NP-hard gradients and we provide bounds with high probability, by taking into account the bias and the variance of the errors. Duchi et al. (2011) analyzed mirror descent (a generalization that includes forward-backward splitting) and show convergence rates in expected value and with high probability with respect to the mixing time of the sampling distribution. We argue that practitioners usually terminate Markov chains before properly mixing, and therefore we motivate our analysis for a controlled increasing number of random samples.\nRegarding our contribution in optimization, we provide a convergence-rate analysis of deterministic errors for three different flavors of forward-backward splitting (FBS): robust (Nemirovski et al., 2009), basic and random (Duchi & Singer, 2009). We extend our analysis to biased stochastic errors, by first characterizing a family of samplers (including importance sampling and\nMCMC) and providing a high probability bound that is useful for understanding the convergence of not only FBS, but also PG (Schmidt et al., 2011). Our analysis shows the bias/variance term and allow to derive some interesting conclusions. First, FBS for deterministic or biased stochastic errors requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate). More interestingly, we found that the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG. We also found that accelerated PG is not guaranteed to converge in the biased stochastic setting.\nRegarding our contribution in structure learning, we show that the optimal solution of maximum likelihood estimation is bounded (to the best of our knowledge, this has not been shown before). Our analysis shows provable convergence guarantees for finite iterations and finite number of random samples. Note that while consistency in structure recovery has been established (e.g. Wainwright et al. (2006)), convergence rates of parameter learning for fixed structures is up to now unknown. Our analysis can be easily extended to Markov random fields with higher order cliques as well as parameter learning for fixed structures by using a `22 regularizer instead."
    }, {
      "heading" : "2. Our Motivating Problem",
      "text" : "In this section, we introduce the problem of learning sparse Ising models and discuss its properties. Our discussion will motivate a set of bounds and assumptions for a more general convergence rate analysis."
    }, {
      "heading" : "2.1. Problem Setup",
      "text" : "An Ising model is a Markov random field on binary variables with pairwise interactions. It first arose in statistical physics as a model for the energy of a physical system of interacting atoms (Koller & Friedman, 2009). Formally, the probability mass function (PMF) of an Ising model parameterized by θ = (W,b) is defined as:\npθ(x) = 1 Z(W,b)e xTWx+bTx (1)\nwhere the domain for the binary variables is x ∈ {−1, +1}N , W ∈ RN×N is symmetric with zero diagonal, b ∈ RN and partition function is defined as Z(W,b) = ∑x ex\nTWx+bTx. For clarity of the convergence rate analysis, we also define θ ∈ RM where M = N2.\nIn the physics literature, W and b are called ferromagnetic coupling and external magnetic field respec-\ntively. W defines the topology of the Markov random field, i.e. the graph G = (V, E) is defined as V = {1, . . . , N} and E = {(n1, n2) | n1 < n2 ∧ wn1n2 6= 0}. It is well known that, for an Ising model with arbitrary topology, computing the partition function Z is NPhard (Barahona, 1982). It is also NP-hard to approximate Z with high probability and arbitrary precision (Chandrasekaran et al., 2008).\nThe number of edges |E| or equivalently the cardinality (number of non-zero entries) of W is a measure of model complexity, and it can be used as a regularizer for maximum likelihood estimation. The main disadvantage of using such penalty is that it leads to a NP-hard problem, regardless of the computational complexity of the log-likelihood.\nNext, we formalize the problem of finding a sparse Ising model by regularized maximum likelihood estimation. We replace the cardinality penalty by the `1- norm regularizer as in (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009).\nGiven a complete dataset with T i.i.d. samples x(1), . . . ,x(T ), and a sparseness parameter ρ > 0 the `1-regularized maximum likelihood estimation for the Ising model in eq.(1) becomes:\nmin W,b\nL(W,b) +R(W) (2)\nwhere the negative (average) log-likelihood L(W,b) = − 1T ∑ t log pθ(x\n(t)) = logZ(W,b) − 〈Σ̂,W〉 − µ̂Tb, the empirical second-order moment Σ̂ = 1 T ∑ t x (t)x(t) T − I, the empirical first-order moment\nµ̂ = 1T ∑ t x (t) and the regularizer R(W) = ρ‖W‖1.\nThe objective function in eq.(2) is convex, given the convexity of the log-partition function (Koller & Friedman, 2009), linearity of the scalar products and convexity of the non-smooth `1-norm regularizer. As discussed before, computing the partition function Z is NP-hard, and so is computing the objective function in eq.(2)."
    }, {
      "heading" : "2.2. Bounds",
      "text" : "In what follows, we show boundedness of the optimal solution and the gradients of the maximum likelihood problem. Both are important ingredients for showing convergence and are largely used assumptions in optimization. In this paper, we follow the original formulation of the problem given in (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009), which does not regularize b. We found interesting to show that this problem has bounds for ‖b∗‖1 unlike other stochastic optimization problems,\ne.g. SVMs (Shalev-Shwartz et al., 2007).\nFirst, we make some observations that will help us derive our bounds. The empirical second-order moment Σ̂ and first-order moment µ̂ in eq.(2) are computed from binary variables in {−1,+1}, therefore ‖Σ̂‖∞ ≤ 1 and ‖µ̂‖∞ ≤ 1. Assumption 1. It is reasonable to assume that the empirical first-order moment of every variable is not equal to −1 (or +1), since this would be equivalent to observe a constant value −1 (or +1) for such variables in every sample in the dataset, i.e. (∃n) |µ̂n| = 1 ⇔ (∀t) x(t)n = −1 ∨ (∀t) x(t)n = 1. Therefore, we assume ‖µ̂‖∞ < 1 ⇔ (∀n)− 1 < µ̂n < +1.\nGiven those observations, we state our bounds in the following theorem. For clarity of the convergence rate analysis, we also define the bound D of the optimal solution. Theorem 2. The optimal solution θ∗ = (W∗,b∗) of the maximum likelihood problem in eq.(2) is bounded as follows:\ni. ‖W∗‖1 ≤ N log 2ρ ii. ‖b∗‖1 ≤ N log 2(ρ+1+‖Σ̂‖∞)ρ(1−‖µ̂‖∞) iii. ‖θ∗‖2 ≤ D\n(3)\nwhere D2 = (\nN log 2 ρ\n)2 ( 1 + ( ρ+1+‖Σ̂‖∞\n1−‖µ̂‖∞\n)2) .\nProof Sketch. Claim i and ii follow from the fact that the function evaluated at (W∗,b∗) is less than at (0,0). Additionally, Claim i follows from nonnegativity of the negative log-likelihood in eq.(2), while Claim ii follows from non-negativity of the regularizer and from Assumption 1. Claim iii follows from norm inequalities and Claims i and ii.\n(Please, see Appendix C for detailed proofs.)\nIf we choose to add the regularizer ρ‖b‖1 in eq.(2), it is easy to conclude that ‖W∗‖1 +‖b∗‖1 ≤ N log 2ρ as in Claim i of Theorem 2.\nThe gradient of the objective function of the maximum likelihood problem in eq.(2) is defined as:\ni. ∂ logZ/∂W = EP [xxT] ii. ∂ logZ/∂b = EP [x] iii. ∂L/∂W = ∂ logZ/∂W − Σ̂ iv. ∂L/∂b = ∂ logZ/∂b− µ̂\n(4)\nwhere P is the probability distribution with PMF pθ(x). The expression in eq.(4) uses the fact that EP [xxT] = ∑ x xx Tpθ(x) and EP [x] = ∑ x xpθ(x).\nIt is well known that computing the gradients ∂ logZ/∂W and ∂ logZ/∂b is NP-hard. The complexity results in (Chandrasekaran et al., 2008) imply that approximating those gradients with high probability and arbitrary precision is also NP-hard.\nNext, we state some properties of the gradient of the exact log-likelihood. For clarity of the convergence rate analysis, we also define the Lipschitz constant G. Lemma 3. The objective function of the maximum likelihood problem in eq.(2) has the following Lipschitz continuity properties:\ni. ‖∂ logZ/∂W‖∞ , ‖∂ logZ/∂b‖∞ ≤ 1 ii. ‖∂L/∂W‖∞ ≤ 1 + ‖Σ̂‖∞ iii. ‖∂L/∂b‖∞ ≤ 1 + ‖µ̂‖∞ iv. ‖∂R/∂W‖∞ ≤ ρ v. ‖∂L/∂θ‖2 , ‖∂R/∂θ‖2 ≤ G (5)\nwhere G2 = N2 max((1+‖Σ̂‖∞)2+ 1N (1+‖µ̂‖∞)2, ρ2).\nProof Sketch. Claims i to iii follow from the fact that the terms ∂ logZ/∂W and ∂ logZ/∂b in eq.(4) are the second and first-order moment of binary variables in {−1, +1}. Claim iv follows from the definition of subgradients. Claim v follows from norm inequalities and Claims ii to iv."
    }, {
      "heading" : "2.3. Approximating the Gradient of the Log-Partition Function",
      "text" : "Suppose one wants to evaluate the expression EP [xxT] in eq.(4) which is the gradient of the log-partition function. Let assume we know the distribution pθ(x) up to a constant factor, i.e. p′θ(x) = e\nxTWx+bTx. Importance sampling draws S samples x(1), . . . ,x(S) from a trial distribution with PMF q(x), calculates the importance weights α(s) = p′θ(x\n(s))/q(x(s)) and produces the estimate ( ∑ s α (s)x(s)x(s) T )/ ∑ s α\n(s). On the other hand, MCMC generates S samples x(1), . . . ,x(S) from the distribution pθ(x) based on constructing a Markov chain whose stationary distribution is pθ(x). Thus, the estimate becomes 1S ∑ s x (s)x(s) T .\nIn what follows, we characterize a family of samplers that includes importance sampling and MCMC as shown in (Peskun, 1973; Liu, 2001). Definition 4. A (B, V, S, D)-sampler takes S random samples from a distribution Q and produces biased estimates of the gradient of the log-partition function ∂ logZ/∂θ + ξ, with error ξ that has bias and variance:\ni. EQ[‖ξ‖2] ≤ BS +O( 1S2 ) ii. VarQ[‖ξ‖2] ≤ VS +O( 1S2 )\n(6)\nfor B ≥ 0, V ≥ 0 and (∀θ) ‖θ‖2 ≤ D.\nNote that a (B, V, S,D)-sampler is asymptotically unbiased with asymptotically vanishing variance, i.e. S → +∞ ⇒ BS → 0 ∧ VS → 0. Unfortunately, analytical approximations of the constants B and V are difficult to obtain even for specific classes, e.g. Ising models. The theoretical analysis implies that such constants B and V exist (Peskun, 1973; Liu, 2001) for importance sampling and MCMC. We argue that this apparent disadvantage does not diminish the relevance of our analysis, since we can reasonably expect that more refined samplers lead to lower B and V .\nNote that Definition 4 does not contradict the complexity results in (Chandrasekaran et al., 2008) that show that it is likely impossible to approximate Z (and therefore its gradient) with probability greater than 1 − δ and arbitrary precision ε in time polynomial in log 1δ and 1 ε . Definition 4 assumes biasedness and a polynomial decay instead of an exponential decay (which is a more stringent condition) and cannot be used to derive two-sided high probability bounds that are both O(log 1δ ) and O( 1S ). Therefore, Definition 4 cannot be used to obtain polynomial-time algorithms as the ones considered in (Chandrasekaran et al., 2008).\nAssumption 5. It is reasonable to assume that the estimates of the gradient of the log-partition function are inside [−1;+1] since they are approximations of the second and first-order moment of binary variables in {−1, +1}. Furthermore, it is straightforward to enforce Lipschitz continuity (condition i of Lemma 3) for any sampler (e.g. importance sampling, MCMC or any conceivable method) by limiting its output to be inside [−1; +1]. More formally, we have:\ni. ‖∂ logZ/∂θ + ξ‖∞ ≤ 1 ii. ‖∂L/∂θ + ξ‖2 ≤ G (7)"
    }, {
      "heading" : "3. Biased Stochastic Optimization",
      "text" : "In this section, we analyze the convergence rates of forward-backward splitting. Our results apply to any problem that fulfills the following largely used assumptions in optimization:\n• the objective function is composed by a smooth function L(θ) and non-smooth regularizer R(θ) • the optimal solution is bounded, i.e. ‖θ∗‖2 ≤ D • each visited point is at a bounded distance from\nthe optimal solution, i.e. (∀k) ‖θ(k) − θ∗‖2 ≤ D • both L and R are Lipschitz continuous, i.e. ‖∂L/∂θ‖2 , ‖∂R/∂θ‖2 ≤ G • the non-smooth regularizer vanishes at zero, i.e. R(0) = 0\nWe additionally require that the errors do not change the Lipschitz continuity properties, i.e. ‖∂L/∂θ + ξ‖2 ≤ G (as discussed in Assumption 5)."
    }, {
      "heading" : "3.1. Algorithm",
      "text" : "We analyze forward-backward splitting (Duchi & Singer, 2009) for deterministic as well as biased stochastic errors, for non-increasing step sizes of the form ηk ∈ O( 1kr ) for 0 < r < 1. This method is equivalent to basic proximal gradient (Schmidt et al., 2011) for r = 0 (constant step size). We point out that FBS has O( 1√\nK ) convergence for r = 12 , while basic\nPG has O( 1K ) convergence, and accelerated PG has O( 1K2 ) convergence. Thus, PG methods have faster convergence but they are more sensitive to errors.\nFBS performs gradient descent steps for the smooth part of the objective function, and (closed form) projection steps for the non-smooth part. Here we assume that at each iteration k, we approximate the gradient with some (deterministic or biased stochastic) error ξ(k). For our objective function in eq.(2), one iteration of the algorithm is equivalent to:\ni. θ(k+ 1 2 ) = θ(k) − ηk(g(k)L + ξ(k)) ii. θ(k+1) = arg minθ( 1 2‖θ − θ(k+ 1 2 )‖22 + ηk+1R(θ))\n(8) where g(k)L = ∂L ∂θ (θ\n(k)), and ξ(k) is the error in the gradient approximation. Step ii is a projection step for the non-smooth regularizer R(θ). For the regularizer in our motivating problem R(W) = ρ‖W‖1, Step ii decomposes into N2 independent lasso problems."
    }, {
      "heading" : "3.2. Convergence Rates for Deterministic Errors",
      "text" : "In what follows, we analyze three different flavors of forward-backward splitting: robust which outputs the weighted average of all visited points by using the step sizes as in robust stochastic approximation (Nemirovski et al., 2009), basic which outputs the average of all visited points as in (Duchi & Singer, 2009), or random which outputs a point chosen uniformly at random from the visited points. Here we assume that at each iteration k, we approximate the gradient with some deterministic error ξ(k). Our results in this subsection will allow us to draw some conclusions regarding not only FBS but also proximal gradient.\nIn order to make our bounds more general for different choices of step size ηk ∈ O( 1kr ) for some 0 < r < 1, we use generalized harmonic numbers Hr,K = ∑K k=1 1 kr and therefore H0,K = K, Hr,K ≈ K1−r1−r for 0 < r < 1, H1,K ≈ log K and Hr,K ≈ 1−K1−rr−1 for r > 1.\nAdditionally, we define a weighted error term that will be used for our analysis of deterministic as well as biased stochastic errors. Given a sequence of errors ξ(1), . . . , ξ(K) and a set of arbitrary weights γk such that ∑ k γk = 1, the error term is defined as:\nAγ,ξ ≡ ∑ k γk‖ξ(k)‖2 (9)\nFirst, we show the convergence rate of robust FBS.\nTheorem 6. For a sequence of deterministic errors ξ(1), . . . , ξ(K), step size ηk = βGkr for 0 < r < 1, initial point θ(1) = 0, the objective function evaluated at the weighted average of all visited points converges to the optimal solution with rate:\nL(θ) +R(θ)− L(θ∗)−R(θ∗) ≤ πη(K) ≤ D2G2βHr,K + 2DAγ,ξ + 4βGH2r,K Hr,K\n(10)\nwhere θ = ∑ k ηkθ (k)\n∑ k ηk , the weighted average regret\nπη(K) = ∑\nk ηk(L(θ(k))+R(θ(k)))∑ k ηk − L(θ∗) − R(θ∗), the error term Aγ,ξ is defined as in eq.(9), and the error weights γk = 1/kr\nHr,K such that ∑ k γk = 1.\nProof Sketch. By Jensen’s inequality L(θ) + R(θ) ≤∑ k ηk(L(θ(k)) +R(θ(k)))/ ∑ k ηk. Then we apply a technical lemma for bounding consecutive steps (Please, see Appendix B).\nSecond, we show the convergence rate of basic FBS.\nTheorem 7. For a sequence of deterministic errors ξ(1), . . . , ξ(K), step size ηk = βGkr for 0 < r < 1, initial point θ(1) = 0, the objective function evaluated at the average of all visited points converges to the optimal solution with rate:\nL(θ) +R(θ)− L(θ∗)−R(θ∗) ≤ π(K) ≤ D2G(K+1)r2βK + 21+rDAγ,ξ + 22+rβGHr,K K\n(11)\nwhere θ = ∑ k θ (k)\nK , the average regret π(K) =∑ k (L(θ(k))+R(θ(k)))\nK − L(θ∗) − R(θ∗), the error term Aγ,ξ is defined as in eq.(9), and the error weights γk = 1K such that ∑ k γk = 1.\nProof Sketch. By Jensen’s inequality L(θ) + R(θ) ≤∑ k (L(θ(k)) +R(θ(k)))/K. Then we apply a technical lemma for bounding consecutive steps (Please, see Appendix B).\nFinally, we show the convergence rate of random FBS.\nTheorem 8. For a sequence of deterministic errors ξ(1), . . . , ξ(K), step size ηk = βGkr for 0 < r < 1, initial point θ(1) = 0 and some confidence parameter 0 < ε < 1, the objective function evaluated at a point k chosen uniformly at random from the visited points converges, with probability at least 1 − ε, to the optimal solution with rate:\nL(θ(k)) +R(θ(k))− L(θ∗)−R(θ∗) ≤ 1ε ( D2G(K+1)r 2βK + 2 1+rDAγ,ξ + 22+rβGHr,K K ) (12)\nwhere the error term Aγ,ξ is defined as in eq.(9), and the error weights γk = 1K such that ∑ k γk = 1.\nProof Sketch. Since the distribution is uniform on k, the expected value of the objective function is equal to the average of the objective function evaluated at all visited points, i.e. the average regret π(K). The final result follows from Markov’s inequality and the upper bound of π(K) given in Theorem 7.\nThe convergence rates in Theorems 6, 7 and 8 lead to an error term Aγ,ξ that is linear, while the error term is quadratic in the analysis of proximal gradient (Schmidt et al., 2011). In basic PG, the error term can be written as:\n1 K ( ∑ k ‖ξ(k)‖2)2 = K(Aγ,ξ)2 (13)\nwhere the error weights γk = 1K such that ∑\nk γk = 1. In accelerated PG, the error term can be written as:\n4 (K+1)2 ( ∑ k k‖ξ(k)‖2)2 = K2(Aγ,ξ)2 (14)\nwhere the error weights γk = k/ ( K 2 ) so that ∑ k γk = 1.\nNote that both PG methods contain terms K and K2, which are not in our analysis. As noted in (Schmidt et al., 2011), errors have a greater effect on the accelerated method than on the basic method. This observation suggests that, unlike in the error-free case, accelerated PG is not necessarily better than the basic method due to a higher sensitivity to errors (Devolder et al., 2011).\nIntuitively speaking, basic PG is similar to basic FBS in the sense that errors from all iterations have the same effect on the convergence rate, i.e. γk is constant. In robust FBS, errors in the last iterations have a lower effect on the convergence rate than errors in the beginning, i.e. γk is decreasing. In accelerated PG, errors in the last iterations have a bigger effect on the convergence rate than errors in the beginning, i.e. γk is increasing.\nThe analysis of Schmidt et al. (2011) for deterministic errors implies that in order to have convergence,\nthe errors must decrease at a rate ‖ξ(k)‖2 ∈ O( 1k1/2+² ) for some ² > 0 in the case of basic PG, and O( 1k1+² ) for accelerated PG. In contrast, our analysis of FBS show that we only need logarithmically decreasing errors O( 1log k ) in order to have convergence. Regarding O( 1√\nK ) convergence of the error term Aγ,ξ, basic and\nrobust FBS requires errors O( 1 k1/2+² ) (the minimum required for convergence in basic PG). Table 1 summarizes the requirements for different convergence rates of the error term Aγ,ξ of FBS as well as the error terms of basic PG in eq.(13) and accelerated PG in eq.(14).\nFor an informal (and incomplete) analysis of the results in (Schmidt et al., 2011) for biased stochastic optimization, consider each error bounded by its bias and variance ‖ξ(k)‖2 ≤ B/Sk+c √ V/Sk for some c > 0 and an increasing number of random samples Sk that allows to obtain decreasing errors. Without noting the possible need of “uniform convergence” of the bound for all K iterations (making c a function of K), the number of random samples must increase (at least) at a rate that is quadratic of the rate of the errors. For instance, in order to have O( 1K ) convergence, basic PG requires errors to be O( 1k1+² ) and therefore it would require (at least) an increasing number of random samples Sk ∈ O(k2+²) for some ² > 0. Accelerated PG would require (at least) Sk ∈ O(k4+²) in order to obtain O( 1K2 ) convergence. If we include the fact that c is a function of K, then the required number of random samples would be “worse than quadratic” of the required rate of the errors. Fortunately, a formal analysis in the next subsection shows that this is not the case for all methods except accelerated PG."
    }, {
      "heading" : "3.3. Bounding the Error Term for Biased Stochastic Optimization",
      "text" : "In what follows, we focus in the analysis of stochastic errors in order to see if better convergence rates can be obtained than the ones informally outlined in the previous subsection. A formal analysis of the er-\nror terms show that forward-backward splitting for biased stochastic errors requires only a logarithmically increasing number of random samples in order to converge, i.e. Sk ∈ O(log k). More interestingly, we found that the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG. On the negative side, we found that accelerated PG is not guaranteed to converge in the biased stochastic setting.\nNext, we present our high probability bound for the error term for biased stochastic optimization. One way to bound the error term Aγ,ξ would be to rely on “uniform convergence” arguments, i.e. to bound the error of each iteration ‖ξ(k)‖2 and then use the well-known union bound. We chose to bound the error term itself, by using the fact that errors become independent (but not identically distributed) conditioned to the parameters θ(1), . . . , θ(K). We also allow for a different number of random samples Sk for each iteration k.\nTheorem 9. Given K (B, V, Sk, D)-samplers each producing estimates with an error ξ(k), and given a set of arbitrary weights γk such that ∑ k γk = 1. For some confidence parameter 0 < δ < 1, with probability at least 1− δ, the error term is bounded as follows:\nAγ,ξ ≤ λ1 + 2 √ M 3K log 1 δ + √ 2λ2 log 1δ + 4M 9K2 log 2 1 δ\n(15) where the bias term λ1 = min(2 √ M,B ∑ k γk Sk ) and the variance term λ2 = min(4M, V ∑ k γ2k Sk ).\nProof Sketch. The bias and variance for each ‖ξ(k)‖2 are bounded by BSk and V Sk\nby Definition 4. By Lemma 3 and Assumption 5 we have ‖ξ(k)‖2 ≤ 2 √ M which is the maximum bias, and its square is the maximum variance. By the definition of marginal distribution, we make ‖ξ(1)‖2, . . . , ‖ξ(K)‖2 independent (but not identically distributed) conditioned to the parameters θ(1), . . . , θ(K). We then invoke Bernstein inequality for properly defined variables such that it applies to the weighted average Aγ,ξ.\nIt is interesting to note what happens for a fixed number of random samples Sk ∈ O(1). In this case, the bias term λ1 ∈ O(1) and therefore FBS will not converge. For robust FBS, the variance term λ2 ∈ O(H2r,K/(Hr,K)2) which for instance for r = 12 we have λ2 ∈ O( log KK ). For basic FBS, the variance term λ2 ∈ O( 1K ). Therefore, for the constant number of random samples, the lack of convergence of FBS is explained only by the bias of the sampler and not its variance.\nTable 2 summarizes the requirements for different convergence rates of the error term Aγ,ξ of FBS as well as the error terms of basic PG in eq.(13) and accelerated PG in eq.(14). Note that convergence for FBS is guaranteed for a logarithmically increasing number of random samples Sk ∈ O(log k). Moreover, in order to obtain convergence rates of O( 1√\nK ) and O( 1K ), the\nrequired number of random samples is just the inverse of the required rate of the errors for the deterministic case, and not “worse than quadratic” as outlined in our informal analysis of the previous subsection.\nOne important conclusion from Theorem 9 is that the upper bound of the error term is Ω( 1K ) independently of the bias term λ1 and the variance term λ2. This implies that the error term is O( 1K ) for any setting of error weights γk and number of random samples Sk. The main implication is that the error term in accelerated PG in eq.(14) is constant and therefore the accelerated method is not guaranteed to converge."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "We illustrate our theoretical findings with a small synthetic experiment (N = 15 variables) since we want to report the log-likelihood at each iteration. We performed 10 repetitions. For each repetition, we generate edges in the ground truth model Wg with a 50% density. The weight of each edge is generated uniformly at random from [−1;+1]. We set bg = 0. We finally generate a dataset of 50 samples. We used a “Gibbs sampler” by first finding the mean field distribution and then performing 5 Gibbs iterations. We used a step size factor β = 1 and regularization parameter ρ = 1/16. We also include a two-step algorithm, by first learning the structure by `1-regularized logistic regression (Wainwright et al., 2006) and then learning the parameters by using FBS with belief propagation for gradient approximation. We summarize our results in Figure 1.\nOur experiments suggest that stochastic optimiza-\ntion converges to the maximum likelihood estimate. We also show the Kullback-Leibler divergence to the ground truth, and more pronounced effects for importance sampling (Please, see Appendix D).\nConcluding Remarks. Although we focused on Ising models, the ideas developed in the current paper could be applied to Markov random fields with higher order cliques. Our analysis can be easily extended to parameter learning for fixed structures by using a `22 regularizer instead. Although we show that accelerated proximal gradient is not guaranteed to converge in our specific biased stochastic setting, necessary conditions for its convergence needs to be investigated.\nAcknowledgments. This work was done while the author was supported in part by NIH Grants 1 R01 DA020949 and 1 R01 EB007530."
    } ],
    "references" : [ {
      "title" : "Particle filtered MCMC-MLE with connections to contrastive divergence",
      "author" : [ "A. Asuncion", "Q. Liu", "A. Ihler", "P. Smyth" ],
      "venue" : null,
      "citeRegEx" : "Asuncion et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Asuncion et al\\.",
      "year" : 2010
    }, {
      "title" : "Estimate sequence methods: extensions and approximations",
      "author" : [ "M. Baes" ],
      "venue" : "IFOR internal report, ETH Zurich,",
      "citeRegEx" : "Baes,? \\Q2009\\E",
      "shortCiteRegEx" : "Baes",
      "year" : 2009
    }, {
      "title" : "Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data",
      "author" : [ "O. Banerjee", "L. El Ghaoui", "A. d’Aspremont" ],
      "venue" : null,
      "citeRegEx" : "Banerjee et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2008
    }, {
      "title" : "On the computational complexity of Ising spin glass models",
      "author" : [ "F. Barahona" ],
      "venue" : "Journal of Physics A: Mathematical, Nuclear and General,",
      "citeRegEx" : "Barahona,? \\Q1982\\E",
      "shortCiteRegEx" : "Barahona",
      "year" : 1982
    }, {
      "title" : "Statistical analysis of non-lattice data",
      "author" : [ "J. Besag" ],
      "venue" : "The Statistician,",
      "citeRegEx" : "Besag,? \\Q1975\\E",
      "shortCiteRegEx" : "Besag",
      "year" : 1975
    }, {
      "title" : "Complexity of inference in graphical models",
      "author" : [ "V. Chandrasekaran", "N. Srebro", "P. Harsha" ],
      "venue" : null,
      "citeRegEx" : "Chandrasekaran et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chandrasekaran et al\\.",
      "year" : 2008
    }, {
      "title" : "Smooth optimization with approximate gradient",
      "author" : [ "A. d’Aspremont" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "d.Aspremont,? \\Q2008\\E",
      "shortCiteRegEx" : "d.Aspremont",
      "year" : 2008
    }, {
      "title" : "Parallel tempering for training of restricted Boltzmann machines",
      "author" : [ "G. Desjardins", "A. Courville", "Y. Bengio", "P. Vincent", "O. Delalleau" ],
      "venue" : null,
      "citeRegEx" : "Desjardins et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Desjardins et al\\.",
      "year" : 2010
    }, {
      "title" : "Stochastic first order methods in smooth convex optimization",
      "author" : [ "O. Devolder" ],
      "venue" : "CORE Discussion Papers 2012/9,",
      "citeRegEx" : "Devolder,? \\Q2012\\E",
      "shortCiteRegEx" : "Devolder",
      "year" : 2012
    }, {
      "title" : "First-order methods of smooth convex optimization with inexact oracle",
      "author" : [ "O. Devolder", "F. Glineur", "Y. Nesterov" ],
      "venue" : "CORE Discussion Papers 2011/2,",
      "citeRegEx" : "Devolder et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Devolder et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "J. Duchi", "Y. Singer" ],
      "venue" : null,
      "citeRegEx" : "Duchi and Singer,? \\Q2009\\E",
      "shortCiteRegEx" : "Duchi and Singer",
      "year" : 2009
    }, {
      "title" : "A convex upper bound on the log-partition function for binary graphical models",
      "author" : [ "L. El Ghaoui", "A. Gueye" ],
      "venue" : null,
      "citeRegEx" : "Ghaoui and Gueye,? \\Q2008\\E",
      "shortCiteRegEx" : "Ghaoui and Gueye",
      "year" : 2008
    }, {
      "title" : "Hybrid deterministicstochastic methods for data fitting",
      "author" : [ "M. Friedlander", "M. Schmidt" ],
      "venue" : null,
      "citeRegEx" : "Friedlander and Schmidt,? \\Q2011\\E",
      "shortCiteRegEx" : "Friedlander and Schmidt",
      "year" : 2011
    }, {
      "title" : "Markov chain Monte Carlo maximum likelihood",
      "author" : [ "C. Geyer" ],
      "venue" : "Computing Science and Statistics,",
      "citeRegEx" : "Geyer,? \\Q1991\\E",
      "shortCiteRegEx" : "Geyer",
      "year" : 1991
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton,? \\Q2002\\E",
      "shortCiteRegEx" : "Hinton",
      "year" : 2002
    }, {
      "title" : "Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods",
      "author" : [ "H. Höfling", "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Höfling and Tibshirani,? \\Q2009\\E",
      "shortCiteRegEx" : "Höfling and Tibshirani",
      "year" : 2009
    }, {
      "title" : "Accelerated gradient methods for stochastic optimization and online learning",
      "author" : [ "C. Hu", "J. Kowk", "W. Pan" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2009
    }, {
      "title" : "On learning discrete graphical models using greedy methods",
      "author" : [ "A. Jalali", "C. Johnson", "P. Ravikumar" ],
      "venue" : null,
      "citeRegEx" : "Jalali et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jalali et al\\.",
      "year" : 2011
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedman,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "Efficient structure learning of Markov networks using `1-regularization",
      "author" : [ "S. Lee", "V. Ganapathi", "D. Koller" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2006
    }, {
      "title" : "Monte Carlo Strategies in Scientific Computing",
      "author" : [ "J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Liu,? \\Q2001\\E",
      "shortCiteRegEx" : "Liu",
      "year" : 2001
    }, {
      "title" : "Inductive principles for restricted Boltzmann machine learning",
      "author" : [ "B. Marlin", "K. Swersky", "B. Chen", "N. de Freitas" ],
      "venue" : null,
      "citeRegEx" : "Marlin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Marlin et al\\.",
      "year" : 2010
    }, {
      "title" : "Bayesian learning in undirected graphical models: Approximate MCMC algorithms",
      "author" : [ "I. Murray", "Z. Ghahramani" ],
      "venue" : null,
      "citeRegEx" : "Murray and Ghahramani,? \\Q2004\\E",
      "shortCiteRegEx" : "Murray and Ghahramani",
      "year" : 2004
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "Structure learning in Markov random fields",
      "author" : [ "S. Parise", "M. Welling" ],
      "venue" : null,
      "citeRegEx" : "Parise and Welling,? \\Q2006\\E",
      "shortCiteRegEx" : "Parise and Welling",
      "year" : 2006
    }, {
      "title" : "Optimum Monte Carlo sampling using Markov chains",
      "author" : [ "P. Peskun" ],
      "venue" : null,
      "citeRegEx" : "Peskun,? \\Q1973\\E",
      "shortCiteRegEx" : "Peskun",
      "year" : 1973
    }, {
      "title" : "Learning in Markov random fields using tempered transitions",
      "author" : [ "R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Salakhutdinov,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov",
      "year" : 2009
    }, {
      "title" : "Learning deep Boltzmann machines using adaptive MCMC",
      "author" : [ "R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Salakhutdinov,? \\Q2010\\E",
      "shortCiteRegEx" : "Salakhutdinov",
      "year" : 2010
    }, {
      "title" : "Convergence rates of inexact proximal-gradient methods for convex optimization",
      "author" : [ "M. Schmidt", "N. Le Roux", "F. Bach" ],
      "venue" : null,
      "citeRegEx" : "Schmidt et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2011
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for SVM",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2007
    }, {
      "title" : "Training restricted Boltzmann machines using approximations to the likelihood gradient",
      "author" : [ "T. Tieleman" ],
      "venue" : null,
      "citeRegEx" : "Tieleman,? \\Q2008\\E",
      "shortCiteRegEx" : "Tieleman",
      "year" : 2008
    }, {
      "title" : "High dimensional graphical model selection using `1-regularized logistic regression",
      "author" : [ "M. Wainwright", "P. Ravikumar", "J. Lafferty" ],
      "venue" : null,
      "citeRegEx" : "Wainwright et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Wainwright et al\\.",
      "year" : 2006
    }, {
      "title" : "On the use of variational inference for learning discrete graphical models",
      "author" : [ "E. Yang", "P. Ravikumar" ],
      "venue" : null,
      "citeRegEx" : "Yang and Ravikumar,? \\Q2011\\E",
      "shortCiteRegEx" : "Yang and Ravikumar",
      "year" : 2011
    }, {
      "title" : "Estimation and annealing for Gibbsian fields",
      "author" : [ "L. Younes" ],
      "venue" : "Annales de l’Institut Henri Poincaré,",
      "citeRegEx" : "Younes,? \\Q1988\\E",
      "shortCiteRegEx" : "Younes",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Therefore a computationally tractable approach is to use sparseness promoting regularizers (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009).",
      "startOffset" : 91,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "Therefore a computationally tractable approach is to use sparseness promoting regularizers (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009).",
      "startOffset" : 91,
      "endOffset" : 167
    }, {
      "referenceID" : 31,
      "context" : "For this reason, several researchers propose exact optimization of approximate objectives, such as `1-regularized logistic regression (Wainwright et al., 2006), greedy optimization of the conditional log-likelihoods (Jalali et al.",
      "startOffset" : 134,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : ", 2006), greedy optimization of the conditional log-likelihoods (Jalali et al., 2011), pseudo-likelihood (Besag, 1975) and a sequence of first-order approximations of the exact log-likelihood (Höfling & Tibshirani, 2009).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : ", 2011), pseudo-likelihood (Besag, 1975) and a sequence of first-order approximations of the exact log-likelihood (Höfling & Tibshirani, 2009).",
      "startOffset" : 27,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "Several convex upper bounds and approximations to the logpartition function have been proposed for maximum likelihood estimation, such as the log-determinant relaxation (Banerjee et al., 2008), the cardinality bound (El Ghaoui & Gueye, 2008), the Bethe entropy (Lee et al.",
      "startOffset" : 169,
      "endOffset" : 192
    }, {
      "referenceID" : 19,
      "context" : ", 2008), the cardinality bound (El Ghaoui & Gueye, 2008), the Bethe entropy (Lee et al., 2006; Parise & Welling, 2006), treereweighted approximations and general weighted freeenergy (Yang & Ravikumar, 2011).",
      "startOffset" : 76,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "The use of stochastic maximum likelihood dates back to (Geyer, 1991; Younes, 1988), in which Markov chain Monte Carlo (MCMC) was used for approximating the gradient.",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 33,
      "context" : "The use of stochastic maximum likelihood dates back to (Geyer, 1991; Younes, 1988), in which Markov chain Monte Carlo (MCMC) was used for approximating the gradient.",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al.",
      "startOffset" : 216,
      "endOffset" : 230
    }, {
      "referenceID" : 30,
      "context" : "For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al.",
      "startOffset" : 266,
      "endOffset" : 282
    }, {
      "referenceID" : 26,
      "context" : "For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al., 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al.",
      "startOffset" : 298,
      "endOffset" : 344
    }, {
      "referenceID" : 7,
      "context" : "For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al., 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al.",
      "startOffset" : 298,
      "endOffset" : 344
    }, {
      "referenceID" : 27,
      "context" : ", 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : ", 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al., 2010).",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "Empirical results in (Marlin et al., 2010) suggests that stochastic maximum likelihood is superior to con-",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "Most work in stochastic optimization assumes the availability of unbiased estimates (Duchi & Singer, 2009; Duchi et al., 2010; Hu et al., 2009; Nemirovski et al., 2009).",
      "startOffset" : 84,
      "endOffset" : 168
    }, {
      "referenceID" : 23,
      "context" : "Most work in stochastic optimization assumes the availability of unbiased estimates (Duchi & Singer, 2009; Duchi et al., 2010; Hu et al., 2009; Nemirovski et al., 2009).",
      "startOffset" : 84,
      "endOffset" : 168
    }, {
      "referenceID" : 6,
      "context" : "Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d’Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level.",
      "startOffset" : 135,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : "Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d’Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level.",
      "startOffset" : 135,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d’Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level.",
      "startOffset" : 135,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : "Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d’Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level. Similarly, Devolder (2012) analyzed the case of stochastic errors with fixed bias and variance and show convergence up to a constant level.",
      "startOffset" : 155,
      "endOffset" : 262
    }, {
      "referenceID" : 28,
      "context" : "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 28,
      "context" : "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al.",
      "startOffset" : 43,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al.",
      "startOffset" : 43,
      "endOffset" : 116
    }, {
      "referenceID" : 28,
      "context" : "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions.",
      "startOffset" : 43,
      "endOffset" : 139
    }, {
      "referenceID" : 28,
      "context" : "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions. In our work, we restrict our analysis to exact projection steps and do not assume strong convexity. Both assumptions are natural for learning sparse models under the `1 regularization. Friedlander & Schmidt (2011) provides convergence rates in expected value for PG with stochastic errors that decrease over time in expected value.",
      "startOffset" : 43,
      "endOffset" : 539
    }, {
      "referenceID" : 28,
      "context" : "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions. In our work, we restrict our analysis to exact projection steps and do not assume strong convexity. Both assumptions are natural for learning sparse models under the `1 regularization. Friedlander & Schmidt (2011) provides convergence rates in expected value for PG with stochastic errors that decrease over time in expected value. Friedlander & Schmidt (2011) proposes a growing sample-size strategy for approximating the gradient, i.",
      "startOffset" : 43,
      "endOffset" : 686
    }, {
      "referenceID" : 28,
      "context" : "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions. In our work, we restrict our analysis to exact projection steps and do not assume strong convexity. Both assumptions are natural for learning sparse models under the `1 regularization. Friedlander & Schmidt (2011) provides convergence rates in expected value for PG with stochastic errors that decrease over time in expected value. Friedlander & Schmidt (2011) proposes a growing sample-size strategy for approximating the gradient, i.e. by picking an increasing number of training samples in order to better approximate the gradient. In contrast, our work is for NP-hard gradients and we provide bounds with high probability, by taking into account the bias and the variance of the errors. Duchi et al. (2011) analyzed mirror descent (a generalization that includes forward-backward splitting) and show convergence rates in expected value and with high probability with respect to the mixing time of the sampling distribution.",
      "startOffset" : 43,
      "endOffset" : 1036
    }, {
      "referenceID" : 23,
      "context" : "Regarding our contribution in optimization, we provide a convergence-rate analysis of deterministic errors for three different flavors of forward-backward splitting (FBS): robust (Nemirovski et al., 2009), basic and random (Duchi & Singer, 2009).",
      "startOffset" : 179,
      "endOffset" : 204
    }, {
      "referenceID" : 28,
      "context" : "FBS, but also PG (Schmidt et al., 2011).",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 31,
      "context" : "Wainwright et al. (2006)), convergence rates of parameter learning for fixed structures is up to now unknown.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "It is well known that, for an Ising model with arbitrary topology, computing the partition function Z is NPhard (Barahona, 1982).",
      "startOffset" : 112,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "It is also NP-hard to approximate Z with high probability and arbitrary precision (Chandrasekaran et al., 2008).",
      "startOffset" : 82,
      "endOffset" : 111
    }, {
      "referenceID" : 31,
      "context" : "We replace the cardinality penalty by the `1norm regularizer as in (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009).",
      "startOffset" : 67,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "We replace the cardinality penalty by the `1norm regularizer as in (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009).",
      "startOffset" : 67,
      "endOffset" : 143
    }, {
      "referenceID" : 31,
      "context" : "In this paper, we follow the original formulation of the problem given in (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009), which does not regularize b.",
      "startOffset" : 74,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "In this paper, we follow the original formulation of the problem given in (Wainwright et al., 2006; Banerjee et al., 2008; Höfling & Tibshirani, 2009), which does not regularize b.",
      "startOffset" : 74,
      "endOffset" : 150
    }, {
      "referenceID" : 29,
      "context" : "SVMs (Shalev-Shwartz et al., 2007).",
      "startOffset" : 5,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "plexity results in (Chandrasekaran et al., 2008) imply that approximating those gradients with high probability and arbitrary precision is also NP-hard.",
      "startOffset" : 19,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "In what follows, we characterize a family of samplers that includes importance sampling and MCMC as shown in (Peskun, 1973; Liu, 2001).",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "In what follows, we characterize a family of samplers that includes importance sampling and MCMC as shown in (Peskun, 1973; Liu, 2001).",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : "The theoretical analysis implies that such constants B and V exist (Peskun, 1973; Liu, 2001) for importance sampling and MCMC.",
      "startOffset" : 67,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "The theoretical analysis implies that such constants B and V exist (Peskun, 1973; Liu, 2001) for importance sampling and MCMC.",
      "startOffset" : 67,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Note that Definition 4 does not contradict the complexity results in (Chandrasekaran et al., 2008) that show that it is likely impossible to approximate Z (and therefore its gradient) with probability greater than 1 − δ and arbitrary precision ε in time polynomial in log 1δ and 1 ε .",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "Therefore, Definition 4 cannot be used to obtain polynomial-time algorithms as the ones considered in (Chandrasekaran et al., 2008).",
      "startOffset" : 102,
      "endOffset" : 131
    }, {
      "referenceID" : 28,
      "context" : "This method is equivalent to basic proximal gradient (Schmidt et al., 2011) for r = 0 (constant step size).",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "In what follows, we analyze three different flavors of forward-backward splitting: robust which outputs the weighted average of all visited points by using the step sizes as in robust stochastic approximation (Nemirovski et al., 2009), basic which outputs the average of all visited points as in (Duchi & Singer, 2009), or random which outputs a point chosen uniformly at random from the visited points.",
      "startOffset" : 209,
      "endOffset" : 234
    }, {
      "referenceID" : 28,
      "context" : "The convergence rates in Theorems 6, 7 and 8 lead to an error term Aγ,ξ that is linear, while the error term is quadratic in the analysis of proximal gradient (Schmidt et al., 2011).",
      "startOffset" : 159,
      "endOffset" : 181
    }, {
      "referenceID" : 28,
      "context" : "As noted in (Schmidt et al., 2011), errors have a greater effect on the accelerated method than on the basic method.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "This observation suggests that, unlike in the error-free case, accelerated PG is not necessarily better than the basic method due to a higher sensitivity to errors (Devolder et al., 2011).",
      "startOffset" : 164,
      "endOffset" : 187
    }, {
      "referenceID" : 28,
      "context" : "The analysis of Schmidt et al. (2011) for deterministic errors implies that in order to have convergence, Table 1.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "For an informal (and incomplete) analysis of the results in (Schmidt et al., 2011) for biased stochastic optimization, consider each error bounded by its bias and variance ‖ξ‖2 ≤ B/Sk+c √ V/Sk for some c > 0 and an increasing number of random samples Sk that allows to obtain decreasing errors.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "We also include a two-step algorithm, by first learning the structure by `1-regularized logistic regression (Wainwright et al., 2006) and then learning the parameters by using FBS with belief propagation for gradient approximation.",
      "startOffset" : 108,
      "endOffset" : 133
    } ],
    "year" : 2012,
    "abstractText" : "We study the convergence rate of stochastic optimization of exact (NP-hard) objectives, for which only biased estimates of the gradient are available. We motivate this problem in the context of learning the structure and parameters of Ising models. We first provide a convergence-rate analysis of deterministic errors for forward-backward splitting (FBS). We then extend our analysis to biased stochastic errors, by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS, but also proximal gradient (PG) methods. We derive some interesting conclusions: FBS requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate); the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG; accelerated PG is not guaranteed to converge in the biased stochastic setting.",
    "creator" : " TeX output 2012.05.22:0015"
  }
}