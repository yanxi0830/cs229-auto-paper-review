{
  "name" : "1611.04982.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Oracle Complexity of Second-Order Methods for Finite-Sum Problems",
    "authors" : [ "Yossi Arjevani" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n04 98\n2v 1\n[ m\nat h.\nO C\n] 1\n5 N"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider finite-sum problems of the form\nmin w∈W\nF (w) = 1\nn\nn ∑\ni=1\nfi(w), (1)\nwhere W is a closed convex subset of some Euclidean or Hilbert space, each fi is convex and µ-smooth, and F is λ-strongly convex1. Such problems are ubiquitous in machine learning, for example in order to perform empirical risk minimization using convex losses.\nTo study the complexity of this and other optimization problems, it is common to consider an oracle model, where the optimization algorithm has no a-priori information about the objective function, and obtains information from an oracle which provides values and derivatives of the function at various domain points [Nemirovsky and Yudin, 1983]. The complexity of the algorithm is measured in terms of the number of oracle calls required to optimize the function to within some prescribed accuracy.\nExisting lower bounds for finite-sum problems show that using a first-order oracle, which given a point w and index i = 1, . . . , n returns fi(w) and ∇fi(w), the number of oracle queries required to find an ǫ-optimal solution is at least of order\nΩ\n(\nn+\n√\nnµ\nλ log\n(\n1\nǫ\n))\n,\n1For a twice-differentiable function f , it is µ-smooth if ∇2f(w) µI for all w ∈ W , and λ-strongly convex if ∇2f(w) λI for all w ∈ W .\neither under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general.\nAn alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form\nwt+1 = wt − αt ( ∇2F (w) )−1∇F (w), (2)\nwhere ∇F (w),∇2F (w) are the gradient and the Hessian of F at w, and αt is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d× d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = ℓi(〈w,xi〉), where xi is a training instance and ℓi is some loss function. In that case, assuming ℓi is twice-differentiable, the Hessian has the rank-1 form ℓ′′i (〈w,xi〉)xix⊤i . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems.\nBuilding on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016], Xu et al. [2016] and references therein). These can all be viewed as approximate Newton methods, which replace the actual Hessian ∇2F (w) = 1n ∑n i=1 ∇2fi(w) in Eq. (2) by some approximation, based for instance on the Hessians of a few individual functions fi sampled at random. One may hope that such methods can inherit the favorable properties of second-order methods, and improve on the performance of commonly used first-order methods.\nIn this paper, we consider the opposite direction, and study lower bounds on the number of iterations required by algorithms using second-order (or possibly even higher-order) information, focusing on finitesum problems which are strongly-convex and smooth. We make the following contributions:\n• First, as a more minor contribution, we prove that in the standard setting of optimizing a single smooth and strongly convex function, second-order information cannot improve the oracle complexity compared to first-order methods (at least in high dimensions). Although this may seem unexpected at first, the reason is that the smoothness constraint must be extended to higher-order derivatives, in order for higher-order information to be useful. We note that this observation in itself is not new, and is briefly mentioned (without proof) in Nemirovsky and Yudin [1983, Section 7.2.6]. Our contribution here is in providing a clean, explicit statement and proof of this result.\n• We then turn to present our main results, which state (perhaps surprisingly) that under some mild algorithmic assumptions, and if the dimension is sufficiently large, the oracle complexity of second-\n2Depending on how ǫ-optimality is defined precisely, and where the algorithm is assumed to start, these bounds may have additional factors inside the log. For simplicity, we present the existing bounds assuming ǫ is sufficiently small, so that a log(1/ǫ) term dominates.\norder methods for finite-sum problems is no better than first-order methods, even if the finite-sum problem is composed of quadratics (which are trivially smooth to any order).\n• Despite this pessimistic conclusion, our results also indicate what assumptions and algorithmic approaches might be helpful in circumventing it. In particular, it appears that better, dimension-dependent performance may be possible, if the dimension is moderate and the n individual functions in Eq. (1) are accessed in a non-oblivious way, which depends on the functions rather than fixed in advance (e.g. sampling them from a non-uniform distribution depending on their Hessians, as opposed to sampling them uniformly at random). This provides evidence to the necessity of non-oblivious sampling schemes, and a dimension-dependent analysis, which indeed accords with some recently proposed algorithms and derivations, e.g. Agarwal et al. [2016], Xu et al. [2016]. We note that the limitations arising from oblivious optimization schemes (in a somewhat stronger sense) was also explored in Arjevani and Shamir [2016a,b].\nThe paper is structured as follows: We begin in Sec. 2 with a lower bound for algorithms utilizing second-order information, in the simpler setting where there is a single function F to be optimized, rather than a finite-sum problem. We then turn to provide our main lower bounds in Sec. 3, and discuss their applicability to some existing approaches in Sec. 4. We conclude in Sec. 5, where we also discuss possible approaches to circumvent our lower bounds. The proofs of our results appear in Appendix A."
    }, {
      "heading" : "2 Strongly Convex and Smooth Optimization with a Second-Order Oracle",
      "text" : "Before presenting our main results for finite-sum optimization problems, we consider the simpler problem of minimizing a single strongly-convex and smooth function F (or equivalently, Eq. (1) when n = 1), and prove a result which may be of independent interest.\nTo formalize the setting, we follow a standard oracle model, and assume that the algorithm does not have a-priori information on the objective function F , except the strong-convexity parameter λ and smoothness parameter µ. Instead, it has access to an oracle, which given a point w ∈ W , returns values and derivatives of F at w (either ∇F (w) for a first-order oracle, or ∇F (w),∇2F (w) for a second-order oracle). The algorithm sequentially queries the oracle using w1,w2, . . . ,wT−1, and returns the point wT . Our goal is to lower bound the number of oracle calls T , required to ensure that wT is an ǫ-suboptimal solution.\nGiven a first-order oracle and a strongly convex and smooth objective in sufficiently high dimensions, it is well-known that the worst-case oracle complexity is Ω( √\nµ/λ · log(1/ǫ)) [Nemirovsky and Yudin, 1983]. What if we replace this by a second-order oracle, which returns both ∇2F (w) on top of F (w),∇F (w)?\nPerhaps unexpectedly, it turns out that this additional information does not substantially improve the worst-case oracle complexity bound, as evidenced by the following theorem:\nTheorem 1. For any µ, λ such that µ > 8λ > 0, any ǫ ∈ (0, 1), and any deterministic algorithm, there exists a µ-smooth, λ strongly-convex function F on Rd (for d = Õ( √\nµ/λ), hiding factors logarithmic in µ, λ, ǫ), such that the number of calls T to a second-order oracle, required to ensure that F (wT ) − min\nw∈Rd F (w) ≤ ǫ · (F (0) −minw∈Rd F (w)), must be at least\nc\n( √\nµ\n8λ − 1\n) · log ( (λ/µ)3/2\nc′ǫ\n)\n,\nwhere c, c′ are positive universal constants.\nFor sufficiently large µλ and small ǫ, this complexity lower bound is Ω (√ µ λ · log ( 1 ǫ ) ) , which matches existing lower and upper bounds for optimizing strongly-convex and smooth functions using first-order methods. As mentioned earlier, the observation that such first-order oracle bounds can be extended to higherorder oracles is also briefly mentioned (without proof) in Nemirovsky and Yudin [1983, Section 7.2.6]. Also, the theorem considers deterministic algorithms (which includes standard second-order methods, such as the Newton method), but otherwise makes no assumption on the algorithm. Generalizing this result to randomized algorithms should be quite doable, based on the techniques developed in Woodworth and Srebro [2016]. We leave a formal derivation to future work.\nAlthough this result may seem surprising at first, it has a simple explanation: In order for Hessian information, which is local in nature, to be useful, there should be some Lipschitz constraint on the Hessian, which ensures that it cannot change arbitrarily quickly as we move around the domain (e.g. ‖∇2F (w) − ∇2F (w′)‖ ≤ L‖w−w′‖ for some constant L). Indeed, the construction relies on a function which does not have Lipschitz Hessians: It is based on a standard lower bound construction for first-order oracles, but the function is locally “flattened” in certain directions around points which are to be queried by the algorithm. This is done in such a way, that the Hessian observed by the algorithm does not provide more information than the gradient, and cannot be used to improve the algorithm’s performance."
    }, {
      "heading" : "3 Second-Order Oracle Complexity Bounds for Finite-Sum Problems",
      "text" : "We now turn to study finite-sum problems of the form given in Eq. (1), and provide lower bounds on the number of oracle calls required to solve them, assuming a second-order oracle. To adapt the setting to a finite-sum problem, we assume that the second-order oracle is given both a point w and an index i ∈ {1, . . . , n}, and returns {fi(w),∇fi(w),∇2fi(w)}. The algorithm iteratively produces and queries the oracle with point-index pairs {(wt, it)}Tt=1, with the goal of making the suboptimality (or expected suboptimality, if the algorithm is randomized) smaller than ǫ using a minimal number of oracle calls T .\nIn fact, the lower bound construction we use is such that each function fi is quadratic. Unlike the construction of the previous section, such functions have a constant (and hence trivially Lipschitz) Hessian. Moreover, since any p-order derivative of a quadratic for p > 2 is zero, this means that our lower bounds automatically hold even if the oracle provides p-th order derivatives at any w, for arbitrarily large p.\nHowever, in order to provide a lower bound using quadratic functions, it is necessary to pose additional assumptions on the structure of the algorithm (unlike Thm. 1 which is purely information-based). To see why, note that without computational constraints, the algorithm can simply query the Hessians and gradients of each fi(w) at w = 0, take the average to get ∇F (0) = 1n ∑n i=1∇fi(0) and ∇2F (0) = 1 n ∑n i=1∇2fi(0), and return the exact optimum, which for quadratics equals ∇2F (0)−1∇F (0). Therefore, with second-order information, the best possible information-based lower bound for quadratics is no better than Ω(n). This is not a satisfying bound, since in order to attain it we need to invert the (possibly high-rank) d × d matrix ∇2F (0). Therefore, if we are interested in bounds for computationally-efficient algorithms, we need to forbid such operations.\nSpecifically, we will make two assumptions about the algorithm, which are stated below (their applicability to existing algorithms is discussed in the next section). The first assumption constrains the indices chosen by the algorithm to be oblivious, in the following sense:\nAssumption 1 (Index Obliviousness). The indices i1, i2, . . . chosen by the algorithm are independent of f1, . . . , fn.\nTo put this assumption differently, the indices may just as well be chosen before the algorithm begins querying the oracle. This can include, for instance, sampling functions fi uniformly at random from f1, . . . , fn, and performing deterministic passes over f1, . . . , fn in order. All optimal first-order algorithms, as well as most second-order methods we are aware of, fall into this framework. Some second-order methods are based on non-uniform sampling schemes, which are covered by the following relaxation:\nAssumption 1a (Partial Index Obliviousness). The indices i1, . . . , iT chosen by the algorithm depend on f1, . . . , fn only through {∇2f1(w), . . . ,∇2fn(w)}w∈W .\nWe will show that one can use Assumption 1a in lieu of Assumption 1, at the cost of a slightly more complicated construction.\nThe second assumption we use constrains the algorithm to query and return points w which are computable using linear-algebraic manipulations of previous points, gradients and Hessians. Moreover, these manipulations can only depend on (at most) the last ⌊n/2⌋ Hessians returned by the oracle. As discussed previously, this assumption is necessary to prevent the algorithm from computing and inverting the full Hessian of F , which is computationally prohibitive. Formally, the assumption is the following:\nAssumption 2 (Linear-Algebraic Computations). wt belongs to the set Wt ⊆ Rd, defined recursively as follows: W1 = {0}, and Wt+1 is the closure of the set of vectors derived from Wt ∪ {∇fit(wt)} by a finite number of operations of the following form:\n• w,w′ → αw + α′w′, where α,α′ are arbitrary.\n• w → Hw, where H is any d× d which has the same block-diagonal structure as t\n∑\nτ=max{1,t−⌊n/2⌋+1} ατ∇2fiτ (wτ ), (3)\nfor some arbitrary {ατ}.\nThe first bullet allows to take arbitrary linear combinations of previous points and gradients, and already covers standard first-order methods and their variants. As to the second bullet, by “same block-diagonal structure”, we mean that if the matrix in Eq. (3) can be decomposed to r diagonal blocks of size d1, . . . , dr in order, then H can also be decomposed into r blocks of size d1, . . . , dr in order (note that this does not exclude the possibility that each such block is composed of additional sub-blocks). To give a few examples, if we let Ht be the matrix in Eq. (3), then we may have H = Ht, H = H −1 t (if Ht is invertible), H = (Ht + D) −1 (where D is some arbitrary diagonal matrix, possibly acting as a regularizer), H can be a truncated SVD decomposition of Ht (or again, Ht + D or (Ht + D)−1 for some arbitrary diagonal matrix D) or its pseudoinverse, etc. Also, note that the assumption places no limits on the number of such operations allowed between oracle calls. However, crucially, all these operations can be performed starting from a linear combination of at most ⌊n/2⌋ recent Hessians. As mentioned earlier, this is necessary, since if we could compute the average of all Hessians, then we could implement the Newton method. It is also realistic, as existing computationally-efficient methods seek to use ≪ n individual Hessians. We note that the choice of ⌊n/2⌋ is rather arbitrary, and can be replaced by αn for any constant α ∈ (0, 1).\nFinally, we mention that the way the assumption is formulated, the algorithm is assumed to “start” from the origin 0. However, this is merely for simplicity, and can be replaced by any other fixed vector (the lower bound will hold by shifting the constructed “hard” function appropriately).\nWith these assumptions stated, we can finally turn to present the main result of this section:\nTheorem 2. For any n > 1, any µ, λ such that µ ≥ 2λn > 0, any ǫ ∈ (0, c) (for some universal constant c > 0), and any (possibly randomized) algorithm satisfying Assumptions 1 and 2, there exists µ-smooth, λ-strongly convex quadratic functions f1, . . . , fn : Rd → R (for d = Õ(1 + √\nµ/λn), hiding factors logarithmic in n, µ, λ, ǫ), such that the number of calls T to a second-order oracle, so that E [F (wT )−minw∈Rd F (w)] ≤ ǫ · (F (0) −minw∈Rd F (w)), must be at least\nΩ\n(\nn+\n√\nnµ\nλ · log\n( (λ/µ)3/2 √ n\nǫ\n))\nComparing this with the (tight) first-order oracle complexity bounds discussed in the introduction, we see that the lower bound is the same up to log-factors, despite the availability of second-order information. In particular, the lower bound exhibits none of the favorable properties associated with full second-order methods, which can compute and invert Hessians of F : Whereas the full Newton method can attain O(log log(1/ǫ)) rates, and be independent of µ, λ if F satisfies a self-concordance property [Boyd and Vandenberghe, 2004], here we only get a linear O(log(1/ǫ)) rate, and there is a strong dependence on µ, λ, even though the function is quadratic and hence self-concordant.\nThe proof of the theorem is based on a randomized construction, which can be sketched as follows: We choose indices j1, . . . , jd−1 ∈ {1, . . . , n} independently and uniformly at random, and define\nfi(w) = a · w21 + â · d−1 ∑\nl=1\n1jl=i(wl − wl+1)2 + ā · w2d − ã · w1 + λ\n2 ‖w‖2,\nwhere 1A is the indicator function of the event A, and a, â, ā, ã are parameters chosen based on λ, µ, n. The average function F (w) = 1n ∑n i=1 fi(w) equals\nF (w) = a · w21 + â n · d−1 ∑\nl=1\n(wl −wl+1)2 + ā · w2d − ã · w1 + λ\n2 ‖w‖2.\nBy setting the parameters appropriately, it can be shown that F is λ-strongly convex and each fi is µ-smooth. Moreover, the optimum of F has the form (q, q2, q3, . . . , qd) for some q > 0. The proof is based on arguing that after T oracle calls, the points computable by any algorithm satisfying Assumptions 1 and 2 must have 0 values at all coordinates larger than some lT , hence the squared distance of wT from the optimum must be at least\n∑d i=lT+1 q2i, which leads to our lower bound. Thus, the proof revolves around upper bounding lT . We note that a similar construction of F was used in some previous first-order lower bounds under algorithmic assumptions (e.g. Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context). The main difference is in how we construct the individual functions fi, and in analyzing the effect of second-order rather than just first-order information.\nTo upper bound lT , we let lt (where t = 1, . . . , T ) be the largest non-zero coordinate in wt, and track how lt increases with t. The key insight is that if w1, . . . ,wt−1 are zero beyond some coordinate l, then any linear combinations of them, as well as multiplying them by matrices based on second-order information, as specified in Assumption 2, will still result in vectors with zeros beyond coordinate l. The only way to “advance” and increase the set of non-zero coordinates is by happening to query the function fjl . However, since the indices of the queried functions are chosen obliviously, whereas each jl is chosen uniformly at random, the probability of this happening is quite small, of order 1/n. Moreover, we show that even if this event occurs, we are unlikely to “advance” by more than O(1) coordinates at a time. Thus, the algorithm essentially needs to make Ω(n) oracle calls in expectation, in order to increase the number of non-zero\ncoordinates by O(1). It can be shown that the number of coordinates needed to get an ǫ-optimal solution is Ω̃( √\nµ/nλ · log(1/ǫ)) (hiding some log-factors). Therefore, the total number of oracle calls is about n times larger, namely Ω̃( √\nnµ/λ · log(1/ǫ)). To complete the theorem, we also provide a simple and separate Ω(n) lower bound, which holds since each oracle call gives us information on just one of the n individual functions f1, . . . , fn, and we need some information on most of them in order to get a close-to-optimal solution.\nAs mentioned previously, we can provide a similar result if we replace Assumption 1 by the milder Assumption 1a, which allows the choice of indices to depend on the Hessians of the individual functions:\nTheorem 3. Thm. 2 also holds (for some finite dimension) if we replace Assumption 1 by the relaxed assumption 1a.\nThe cost for relaxing Assumption 1 is that the dimension needs to be much higher (although still finite). The proof is rather straightforward: Making the dependence on the random indices j1, . . . , jd−1 explicit, the quadratic construction used in the previous theorem can be written as\nF j1,...,jd−1(w) = 1\nn\nn ∑\ni=1\nf j1,...,jd−1 i (w) =\n1\nn\nn ∑\ni=1\nw ⊤A j1,...,jd−1 i w − ã〈e1,w〉+\nλ 2 ‖w‖2\nfor some d × d matrix Aj1,...,jd−1i dependent on j1, . . . , jd−1, and a fixed parameter ã. Crucially, note that the linear term in the function above does not depend on j1, . . . , jd−1. Now, we create n huge blockdiagonal matrices A1, . . . , An, where each Ai contains A j1,...,jd−1 i for each of the n\nd−1 possible choices of j1, . . . , jd−1 along its diagonal (in some canonical order). We then choose r ∈ {1, 1 + d, 1 + 2d, . . . , 1 + (nd−1 − 1)d} uniformly at random, and let\nF (w) = 1\nn\nn ∑\ni=1\nfi(w) = 1\nn\nn ∑\ni=1\nw ⊤Aiw − c〈er,w〉+\nλ 2 ‖w‖2.\nDue to the block-diagonal structure of each Ai, this function inherits the strong-convexity and smoothness properties of the original construction. Moreover, depending on the choice of r, the function is equivalent to the construction involving some j1, . . . , jd−1 on some subset of d coordinates (and along the other coordinates, the optimum is simply 0). The only main difference is that the quadratic terms Ai are no longer random – all the randomness is “pushed” to the linear term er, and the random r encodes which choices of j1, . . . , jd−1 are used. Thus, even if the indices i1, i2, . . . used by the algorithm depends on the Hessians Ai, they are independent of r, and hence independent of j1, . . . , jd−1 corresponding to the construction. Therefore, the analysis of Thm. 2 (which assumes such independence via Assumption 1) holds verbatim, and the result follows."
    }, {
      "heading" : "4 Comparison to Existing Approaches",
      "text" : "As discussed in the introduction, there has been a recent burst of activity involving second-order methods for solving finite-sum problems, relying on Hessians of individual functions fi. In this section, we review the main algorithmic approaches and compare them to our results. The bottom line is that most existing approaches satisfy the assumptions stated in Sec. 3, and therefore our lower bounds will apply, at least in a worst-case sense. A possible exception to this is the Newton sketch algorithm [Pilanci and Wainwright, 2015], which relies on random projections, but on the flip side is computationally expensive.\nTurning to the details, existing approaches are based on taking the standard Newton iteration for such problems,\nwt+1 = wt − αt ( ∇2F (w) )−1 ∇F (w) = wt − αt\n(\n1\nn\nn ∑\ni=1\n∇2fi(w) )−1( 1\nn\nn ∑\ni=1\n∇fi(w) ) ,\nand replacing the inverse Hessian term ( 1 n ∑n i=1∇2fi(w) )−1 (and sometimes the vector term 1n ∑n i=1 ∇fi(w) as well) by some approximation which is computationally cheaper to compute. One standard and wellknown approach is to use only gradient information to construct such an approximation, leading to the family of quasi-Newton methods [Nocedal and Wright, 2006]. However, as they rely on first-order rather than second-order information, they are orthogonal to the topic of our work, and are already covered by existing complexity lower bounds for first-order oracles.\nTurning to consider Hessian approximation techniques using second-order information, perhaps the simplest and most intuitive approach is sampling: Since the Hessian equals the average of many individual Hessians (∇2F (w) = 1n ∑n i=1∇2fi(w)), we can approximate it by taking a sample S of indices in {1, . . . , n} uniformly at random, compute the Hessians of the corresponding individual functions, and use the approximation\n1\n|S| ∑ i∈S ∇2fi(w).\nIf |S| is large enough, then by concentration of measure arguments, this sample average should be close to the actual Hessian ∇2F (w). On the other hand, if |S| is not too large, then the resulting matrix is easier to invert (e.g. because it has a rank of only O(|S|), if each individual Hessian has rank O(1), as in the case of linear predictors). Thus, one can hope that the right sample size will lead to computational savings. There have been several rigorous studies of such “subsampled Newton” methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound.\nXu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more “important”. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 1, as the sampled indices are now chosen in a way dependent on the individual functions, but since this dependence is only through the Hessians, it still satisfies Assumption 1a. Therefore, our lower bound as stated in Thm. 3 still applies to such a method.\nA variant of the subsampled Newton approach, studied in Erdogdu and Montanari [2015], uses a lowrank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself. However, this still falls in the framework of Assumption 2, and our lower bound still applies.\nA different approach to approximate the full Hessian is via randomized sketching techniques, which replace the Hessian ∇2F (w) by a low-rank approximation of the form (∇2F (w))1/2SS⊤(∇2F (w))1/2, where S ∈ Rd×m,m ≪ d is a random sketching matrix, and (∇2F (w))1/2 is the matrix square root of ∇2F (w). This approach forms the basis of the Newton sketch algorithm proposed in Pilanci and Wainwright [2015]. This approach currently escapes our lower bound, since it violates Assumption 2. That being said, this approach is inherently expensive in terms of computational resources, as it requires us to compute the square root of the full Hessian matrix. Even under favorable conditions, this requires us to perform a full pass over all functions f1, . . . , fn at every iteration. Moreover, existing iteration complexity upper bounds\nhave a strong dependence on both µ/λ as well as the dimension d, and are considerably worse than the lower bound of Thm. 2. Therefore, we conjecture that this approach cannot lead to better worst-case results.\nAgarwal et al. [2016] develop another line of stochastic second-order methods, which are based on the observation that the Newton step (∇2F (w))−1∇F (w) is the solution of the system of linear equations ∇2F (w)x = ∇F (w). Thus, one can reduce the optimization problem to solving this system as efficiently as possible. The basic variant of their algorithm (denoted as LiSSA) relies on operations of the form w 7→ (I − ∇2fi(w))w (for i sampled uniformly at random), as well as linear combinations of such vectors, which satisfy our assumptions. A second variant, LiSSA-Quad, re-phrases this linear system as the finitesum optimization problem\nmin x\nx ⊤∇2F (w)x+∇F (w)⊤x = 1\nn\nn ∑\ni=1\nx ⊤∇2fi(w)x+∇fi(w)⊤x,\nand uses some first-order method for finite-sum problems in order to solve it. Since individual gradients of this objective are of the form ∇2fi(w)x + ∇fi(w), and most state-of-the-art first-order methods pick indices i obliviously, this approach also satisfies our assumptions, and our lower bounds apply. Yet another proposed algorithm, LiSSA-Sample, is based on replacing the optimization problem above by\nmin x\nx ⊤∇2F (w)B−1x+∇F (w)⊤x, (4)\nwhere B is some invertible matrix, solving it (with the optimum being equal to B(∇2F (w))−1∇F (w)), and multiplying the solution by B−1 to recover the solution (∇2F (w))−1∇F (w) to the original problem. In order to get computational savings, B is chosen to be a linear combination of O(d log(d)) sampled individual hessians ∇2fi(w), where it is assumed that d log(d) ≪ n, and the sampling and weighting is carefully chosen (based on the Hessians) so that Eq. (4) has strong convexity and smoothness parameters within a constant of each other. As a result, Eq. (4) can be solved quickly using standard gradient descent, taking steps along the gradient, which equals ∇2F (w)B−1x + ∇F (w) at any point x. This gradient is again computable under Assumptions 1a and 2 (using O(n) oracle calls), since B is a linear combination of d log(d) ≪ n sampled individual Hessians, with the sampling determined based on the Hessians. Thus, our lower bound (in the form of Thm. 3) still applies to such methods.\nThat being said, it is important to note that the complexity upper bound attained in Agarwal et al. [2016] for LiSSA-Sample is on the order of Õ((n+ √\ndµ/λ) log(1/ǫ)) (asymptotically as ǫ → 0), which is better than our lower bound if d ≪ n. There is no contradiction, since the lower bound in Thm. 3 only applies for a dimension d much larger than n. Interestingly, note that if Õ((n + √\ndµ/λ) log(1/ǫ)) was attainable with an oblivious sampling scheme, in the regime where d ≪ n, it would violate Thm. 2, which establishes a lower bound of Õ(n + √ nµ/λ) even if the dimension is quite moderate (d = Õ(1 + √\nµ/λn), which is ≪ n under the mild assumption that µ/λ ≪ n3). This indicates that a non index-oblivious sampling scheme is indeed necessary to get such improved results.\nThe observation that a non-oblivious scheme (breaking assumption 1) can help performance when d ≪ n is also seen in the lower bound construction used to prove Thm. 2: If µ, λ, n are such that the required dimension d is ≪ n, then it means that only the functions fj1 , . . . , fjd−1 , which are a small fraction of all n individual functions, are informative and help us reduce the objective value. Thus, sampling these functions in an adaptive manner is imperative to get better complexity than the bound in Thm. 2. Based on the fact that only at most d − 1 out of n functions are relevant in the construction, we conjecture that the possible improvement in the oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d, which is indeed the type of improvement attained (for small enough ǫ) in Agarwal et al. [2016].\nFinally, we note that Agarwal et al. [2016] proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem. However, it requires performing ≥ 1 full Newton steps, so the runtime is prohibitive for large-scale problems (indeed, for quadratics as used in our lower bounds, even a single Newton step suffices to compute an exact solution)."
    }, {
      "heading" : "5 Summary and Discussion",
      "text" : "In this paper, we studied the oracle complexity for optimization problems, assuming availability of a secondorder oracle. This is in contrast to most existing oracle complexity results, which focus on a first-order oracle. First, we formally proved that in the standard setting of strongly-convex and smooth optimization problems, second-order information does not significantly improve the oracle complexity, and further assumptions (i.e. Lipschitzness of the Hessians) are in fact necessary. We then presented our main lower bounds, which show that for finite-sum problems with a second-order oracle, under some reasonable algorithmic assumptions, the resulting oracle complexity is – again – not significantly better than what can be obtained using a first-order oracle. Moreover, this is shown using quadratic functions, which have 0 derivatives of order larger than 2. Hence, our lower bounds apply even if we have access to an oracle returning derivatives of order p for all p ≥ 0, and the function is smooth to any order. In Sec. 4, we studied how our framework and lower bounds are applicable to most existing approaches.\nAlthough this conclusion may appear very pessimistic, they are actually useful in pinpointing potential assumptions and approaches which may circumvent these lower bounds. In particular:\n• Our lower bound for algorithms employing non index-oblivious sampling schemes (Thm. 3) only hold when the dimension d is very large. This leaves open the possibility of better (non index-oblivious) algorithms when d is moderate, as was recently demonstrated in the context of the LiSSA-Sample algorithm of Agarwal et al. [2016] (at least for small enough ǫ). As discussed in the previous section, we conjecture that the possible improvement in the oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d.\n• It might be possible to construct algorithms breaking the one of the assumptions in Sec. 3, e.g. by using operations other than the linear-algebraic ones specified in Assumption 2. However, we currently conjecture that these assumptions can be considerably relaxed, and similar results would hold for any algorithm which has “significantly” cheaper iterations (in terms of runtime) compared to the Newton method. Nevertheless, our conjecture could be false.\n• Our lower bounds are worst-case over smooth and strongly-convex individual functions fi. It could be that by assuming more structure, better bounds can be obtained. For example, as discussed in the introduction, an important special case is when fi(w) = ℓi(x⊤i w) for some scalar function ℓi and vector xi. Our construction in Thm. 2 does not quite fit this structure, although it is easy to show that we still get functions of the form fi(w) = ℓi(X⊤i w), where Xi has O(1 + d/n) = Õ(1 + √\nµ/λn3) rows in expectation, which is Õ(1) under a broad parameter regime. We believe that the difference between Õ(1) rows and 1 row is not significant in terms of the attainable oracle complexity, but we may be wrong. Another possibility is to provide results depending on more delicate spectral properties of the function, beyond its strong convexity and smoothness, which may lead to better results and algorithms under favorable assumptions.\n• Our lower bounds in Sec. 3, which establish a linear convergence rate (logarithmic dependence on log(1/ǫ)), are non-trivial only if the optimization error ǫ is sufficiently small. This does not preclude the possibility of attaining better initial performance when ǫ is relatively large. Indeed, some of the recent work mentioned previously (e.g. Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.\nIn any case, we believe our work lays the foundation for a fuller study of the complexity of efficient second-order methods, for finite-sum and related optimization and learning problems."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported in part by an FP7 Marie Curie CIG grant and an Israel Science Foundation grant 425/13."
    }, {
      "heading" : "A Proofs",
      "text" : ""
    }, {
      "heading" : "A.1 Auxiliary Lemmas",
      "text" : "The following lemma was essentially proven in Lan [2015], Nesterov [2013], but we provide a proof for completeness:\nLemma 1. Fix µ̃ ≥ λ̃ > 0, and consider the following function on Rd:\nF (w) = λ̃(κ̃− 1)\n8\n(\nw21 +\nd−1 ∑\ni=1\n(wi − wi+1)2 + (aκ̃ − 1)w2d − w1 ) + λ̃\n2 ‖w‖2,\nwhere κ̃ = µ̃/λ̃ and aκ̃ = √ κ̃+3√ κ̃+1 . Then F is λ̃ strongly convex, µ̃-smooth, and has a unique minimum at (q, q2, q3, . . . , qd) where q = √ κ̃−1√ κ̃+1 .\nProof. The function is equivalent to\nF (w) = λ̃(κ̃− 1)\n8\n(\nw ⊤Aw −w1\n) + λ̃\n2 ‖w‖2,\nwhere\nA =\n\n      \n2 −1 −1 2 −1\n−1 . . . . . . . . . 2 −1\n−1 aκ̃\n\n      \nis a symmetric tridiagonal matrix with positive entries on the diagonal. Since it is diagonally dominant, it follows that it is positive semidefinite, and therefore F is λ̃ strongly-convex due to the λ̃2‖w‖2 term.\nMoreover, for any w, using the fact that (a+ b)2 ≤ 2(a2 + b2), we have\nw ⊤Aw = w21 +\nd−1 ∑\ni=1\n(wi − wi+1)2 + (aκ̃ − 1)w2d\n≤ w21 + 2 d−1 ∑\ni=1\n(w2i + w 2 i+1) + (aκ̃ − 1)w2d\n≤ 4w21 + 4 d−1 ∑\ni=2\nw2i + (aκ̃ + 1)w 2 d ≤ 4‖w‖2,\nwhere in the last inequality we used the fact that aκ̃ ≤ 3 by definition of aκ̃. Therefore, 4I A 0, and hence the function w 7→ λ̃(κ̃−1)8 ( w ⊤Aw − w1 )\nis a convex, λ̃(κ̃− 1)-smooth function. Since λ̃2‖w‖2 is λ̃ strongly-convex, it follows that F is λ̃ strongly convex and λ̃(κ̃− 1) + λ̃ = λ̃κ̃ = µ̃ smooth.\nIt remains to compute the optimum of F . By differentiating F and setting to zero, we get that the optimum w must satisfy the following set of equations:\nw2 − 2 · κ̃+ 1\nκ̃− 1 · w1 + 1 = 0\nwi+1 − 2 · κ̃+ 1\nκ̃− 1 · wi + wi−1 = 0 ∀ i = 2, . . . , d− 1 (\naκ̃ + 4\nκ̃− 1\n)\nwd − wd−1 = 0.\nIt is easily verified that this is satisfied by the vector (q, q2, q3, . . . , qd), where q = √ κ̃−1√ κ̃+1\n. Since F is strongly convex, this stationary point must be the unique global optimum of F .\nLemma 2. For some q ∈ (0, 1) and positive d, define\ng(z) =\n{\nq2(z+1) z < d 0 z ≥ d .\nLet l be a non-negative random variable, and suppose d ≥ 2E[l]. Then E[g(l)] ≥ 12q4E[l]+4.\nProof. Since q ∈ (0, 1), the function z 7→ qz is convex for non-negative z. Therefore, by definition of g, Jensen’s inequality, and the law of total expectation, we have\nE[g(l)] = Pr(l < d) · E[q2(l+1)|l < d] + Pr(l ≥ d) · 0 ≥ Pr(l < d) · qE[2(l+1)|l<d]\n= Pr(l < d) · q(E[2(l+1)]−Pr(l≥d)·E[2(l+1)|l≥d])/Pr(l<d)\n≥ Pr(l < d) · qE[2(l+1)]/Pr(l<d).\nBy Markov’s inequality, Pr(l < d) = 1 − Pr(l ≥ d) ≥ 1 − E[l]d ≥ 12 . Plugging into the above, we get the lower bound 12q 4(E[l+1]) = 12q 4E[l]+4 as required."
    }, {
      "heading" : "A.2 Proof of Thm. 1",
      "text" : "The proof is inspired by a technique introduced in Woodworth and Srebro [2016] for analyzing randomized first-order methods, in which a quadratic function is “locally flattened” in order to make first-order (gradient) information non-informative. We use a similar technique to make second-order (Hessian) information noninformative, hence preventing second-order methods from having an advantage over first-order methods.\nGiven a (deterministic) algorithm and a bound T on the number of oracle calls, we construct the function F in the following manner. We first choose some dimension d ≥ 2T . We then define\nκ = µ\n8λ , q = √ κ− 1√ κ+ 1 ,\nand choose r > 0 sufficiently small so that\nTµr2\n8λ ≤ 1 and\n√\nTµr2\n16λ ≤ 1 2 qT .\nWe also let v1, . . . ,vT be orthonormal vectors in Rd (to be specified later). We finally define our function as\nF (w) = H(w) + λ\n2 ‖w‖2,\nwhere\nH(w) = λ(κ− 1)\n8\n( 〈v1,w〉2 + T−1 ∑\ni=1\nφr(〈vi − vi+1,w〉) + (aκ − 1)φr(〈vT ,w〉)− 〈v1,w〉 ) ,\naκ = √ κ+3√ κ+1 , and\nφr(z) =\n\n \n  0 |z| ≤ r 2(|z| − r)2 r < |z| ≤ 2r z2 − 2r2 |z| > 2r .\nIt is easy to show that φr is 4-smooth and satisfies 0 ≤ z2 − φr(z) ≤ 2r2 for all z. First, we establish that F is indeed strongly convex and smooth as required:\nLemma 3. F as defined above is λ-strongly convex and µ-smooth.\nProof. Since φr is convex, and the composition of a convex and linear function is convex, we have that w 7→ φr(〈vi − vi+1,w〉) are convex for all i, as well as w 7→ 〈v1,w〉2 and w 7→ φr(〈vT ,w〉). Therefore, H(w) is convex. As a result, F is λ-strongly convex due to the λ2‖w‖2 term. As to smoothness, note first that H(w) can be equivalently written as H̃(Vw), where V is some orthogonal d × d matrix with the first T rows equal to v1, . . . ,vT , and\nH̃(x) = λ(κ− 1)\n8\n( x21 + T−1 ∑\ni=1\nφr(xi − xi+1) + (aκ − 1)φr(xT )− x1 ) .\nTherefore, ∇2F (w) = ∇2H(w) + λI = V ⊤∇2H̃(Vw)V + λI . It is easily verified that ∇2H̃ at any point (and in particular Vw) is tridiagonal, with each element having absolute value at most 2λ(κ−1). Therefore,\nusing the orthogonality of V and the fact that (a+ b)2 ≤ 2(a2 + b2),\nsup x:‖x‖=1\nx ⊤∇2F (w)x = sup x:‖x‖=1 x ⊤(V ⊤∇2H̃(Vw)V + λI)x\n= sup x:‖x‖=1\nx ⊤∇2H̃(Vw)x+ λ\n≤ sup x:‖x‖=1\n2λ(κ− 1) ( d ∑\ni=1\nx2i + 2\nd−1 ∑\ni=1\n|xixi+1| ) + λ\n≤ sup x:‖x‖=1\n2λ(κ− 1) d−1 ∑\ni=1\n(|xi|+ |xi+1|)2 + λ\n≤ sup x:‖x‖=1\n4λ(κ− 1) d−1 ∑\ni=1\n(x2i + x 2 i+1) + λ\n≤ 8λ(κ − 1) + λ ≤ 8λκ.\nPlugging in the definition of κ, this equals µ. Therefore, the spectral norm of the Hessian of F at any point is at most µ, and therefore F is µ-smooth.\nBy construction, the function F also has the following key property:\nLemma 4. For any w ∈ Rd orthogonal to vt,vt+1, . . . ,vT (for some t ∈ {1, 2, . . . , T − 1}), it holds that F (w),∇F (w),∇2F (w) do not depend on vt+1,vt+2, . . . ,vT .\nProof. Recall that F is derived from H by adding a λ2‖w‖2 term, which clearly does not depend on v1, . . . ,vT . Therefore, it is enough to prove the result for H(w),∇H(w),∇2H(w). By taking the definition of H and differentiating, we have that H(w) is proportional to\n〈v1,w〉2 + T−1 ∑\ni=1\nφr(〈vi − vi+1,w〉) + (aκ − 1)φr(〈vT ,w〉)− 〈v1,w〉,\n∇H(w) is proportional to\n2〈v1,w〉v1 + T−1 ∑\ni=1\nφ′r(〈vi − vi+1,w〉)(vi − vi+1) + (aκ − 1)φ′r(〈vT ,w〉)vT − v1,\nand ∇2H(w) is proportional to\n2v1v ⊤ 1 +\nT−1 ∑\ni=1\nφ′′r(〈vi − vi+1,w〉)(vi − vi+1)(vi − vi+1)⊤ + (aκ − 1)φ′′r (〈vT ,w〉)vTv⊤T .\nBy the assumption 〈vt,w〉 = 〈vt+1,w〉 = . . . = 〈vT ,w〉 = 0, and the fact that φr(0) = φ′r(0) = φ′′r(0) = 0, we have φr(〈vi−vi+1,w〉) = φ′r(〈vi−vi+1,w〉) = φ′′r (〈vi−vi+1,w〉) = 0 for all i ∈ {t, t+1, . . . , T}, as well as φr(〈vT ,w〉) = φ′r(〈vT ,w〉) = φ′′r (〈vT , bw〉) = 0. Therefore, it is easily verified that the expressions above indeed do not depend on vt+1, . . . ,vT .\nWith this lemma at hand, we now turn to describe how v1, . . . ,vT are constructed:\n• First, we compute w1 (which is possible since the algorithm is deterministic and w1 is chosen before any oracle calls are made).\n• We pick v1 to be some unit vector orthogonal to w1. Assuming v2, . . . ,vT will also be orthogonal to w1 (which will be ensured by the construction which follows), we have by Lemma 4 that the information F (w1),∇F (w1),∇2F (w1) provided by the oracle to the algorithm does not depend on {v2, . . . ,vT }, and thus depends only on v1 which was already fixed. Since the algorithm is deterministic, this fixes the next query point w2.\n• For t = 2, 3, . . . , T − 1, we repeat the process above: We compute wt, and pick vt to be some unit vectors orthogonal to w1,w2, . . . ,wt, as well as all previously constructed v’s (this is always possible since the dimension is sufficiently large). By Lemma 4, as long as all vectors thus constructed are orthogonal to wt, the information {F (wt),∇F (wt),∇2F (wt)} provided to the algorithm does not depend on vt+1, . . . ,vT , and only depends on v1, . . . ,vt which were already determined. Therefore, the next query point wt+1 is fixed.\n• At the end of the process, we pick vT to be some unit vector orthogonal to all previously chosen v’s as well as w1, . . . ,wT .\nBased on this construction, the following lemma is self-evident:\nLemma 5. It holds that 〈wT ,vT 〉 = 0.\nBased on this lemma, we now turn to argue that wT must be a sub-optimal point. We first establish the following result:\nLemma 6. Letting w⋆ = argminw F (w), it holds that ∥\n∥ ∥ ∥ ∥ w ⋆ −\nT ∑\ni=1\nqivi\n∥ ∥ ∥ ∥ ∥ ≤ √ Tµr2 16λ\nwhere q = √ κ−1√ κ+1 .\nProof. Let Fr denote F , where we make the dependence on the parameter r explicit. We first argue that\nsup w∈Rd\n|Fr(w)− F0(w)| ≤ Tµr2\n32 . (5)\nThis is because\n|Fr(w)− F0(w)| ≤ λ(κ− 1)\n8\n( T−1 ∑\ni=1\n|φr(〈vi − vi+1,w〉)− φ0(〈vi − vi+1,w〉)|\n+ |φr(〈vT ,w〉)− φ0(〈vT ,w〉)| ) ,\nand since supz∈R |φr(z)− φ0(z)| = supz∈R |φr(z)− z2| ≤ 2r2, the above is at most λ(κ−1)4 Tr2 ≤ λκ4 Tr2. Recalling that κ = µ/8λ, Eq. (5) follows.\nLet wr = argminFr(w). By λ-strong convexity of F0 and Fr ,\nF0(wr)− F0(w0) ≥ λ\n2 ‖wr −w0‖2 , Fr(w0)− Fr(wr) ≥\nλ 2 ‖w0 −wr‖2.\nSumming the two inequalities and using Eq. (5),\nλ‖wr −w0‖2 ≤ F0(wr)− Fr(wr) + Fr(w0)− F0(w0) ≤ Tµr2\n16 ,\nand therefore\n‖wr −w0‖2 ≤ Tµr2\n16λ . (6)\nBy definition, wr = w⋆ from the statement of our lemma, so it only remains to prove that w0 = argminF0(w) equals\n∑T i=1 q i vi. To see this, note that F0(w) can be equivalently written as F̃ (Vw), where V is some\northogonal d× d matrix with its first T rows equal to v1, . . . ,vT , and\nF̃ (x) = λ(κ− 1)\n8\n( x21 + T−1 ∑\ni=1\n(xi − xi+1)2 + (aκ − 1)x2T − w1 ) + λ\n2 ‖x‖2.\nBy an immediate corollary of Lemma 1, F̃ (·) is minimized at (q, q2, . . . , qT , 0, . . . , 0), where q = √ κ−1√ κ+1 , and therefore F (w) = F̃ (Vw) is minimized at V ⊤(q, q2, . . . , qT , 0, . . . , 0), which equals ∑T\ni=1 q i vi as\nrequired.\nNote that this lemma also allows us to bound the norm of w⋆ = argminF (w), since it implies that\n‖w⋆‖ ≤ ∥ ∥ ∥\n∥ ∥\nT ∑\ni=1\nqivi\n∥ ∥ ∥ ∥ ∥ + √ Tµr2 16λ ,\nand since (a+ b)2 ≤ 2a2 + 2b2 and q < 1, we have\n‖w⋆‖2 ≤ 2 ∥ ∥ ∥\n∥ ∥\nT ∑\ni=1\nqivi\n∥ ∥ ∥ ∥ ∥\n2\n+ Tµr2\n8λ = 2\nT ∑\ni=1\nq2i + Tµr2\n8λ\n≤ 2 ∞ ∑\ni=1\nq2i + Tµr2\n8λ =\n2q2 1− q2 + Tµr2 8λ\n≤ 2 1− q +\nTµr2\n8λ =\n√ κ+ 1 + Tµr2\n8λ ,\nwhich is at most √ κ + 2 ≤ 3√κ, since we assume that c is sufficiently small so that Tµr28λ ≤ 1, and that κ = µ/8λ ≥ 1. The proof of the theorem follows by combining Lemma 5 and Lemma 6. Specifically, Lemma 5 (which states that 〈wT ,vT 〉 = 0) and the fact that v1, . . . ,vT are orthonormal tells us that ∥\n∥ ∥ ∥ ∥\nwT − T ∑\ni=1\nqivi\n∥ ∥ ∥ ∥ ∥\n2\n=\n∥ ∥ ∥ ∥ ∥ ( wT − T−1 ∑\ni=1\nqivi\n)\n− qTvT\n∥ ∥ ∥ ∥ ∥\n2\n=\n∥ ∥ ∥ ∥ ∥ wT − T−1 ∑\ni=1\nqivi\n∥ ∥ ∥ ∥ ∥\n2\n+ ‖qTvT ‖2\n≥ ‖qTvT ‖2 = q2T ,\nand hence ∥\n∥ ∥ ∥ ∥\nwT − T ∑\ni=1\nqivi\n∥ ∥ ∥ ∥ ∥ ≥ qT .\nOn the other hand, Lemma 6 states that ∥\n∥ ∥ ∥ ∥ w ⋆ −\nT ∑\ni=1\nqivi\n∥ ∥ ∥ ∥ ∥ ≤ √ Tµr2 16λ .\nCombining the last two displayed equations by the triangle inequality, we get that\n‖wT −w⋆‖ ≥ qT − √ Tµr2\n16λ .\nBy the assumption that c is sufficiently small so that √ Tµr2\n16λ ≤ 12qT , the left hand side is at least 12qT . Squaring both sides, we get\n‖wT −w⋆‖2 ≥ 1\n4 q2T ,\nso by strong convexity of F ,\nF (wT )− F (w⋆) ≥ λ\n2 ‖wT −w⋆‖2 ≥\nλ 8 q2T .\nPlugging in the value of q, we get\nF (wT )− F (w⋆) ≥ λ\n8 (√ κ− 1√ κ+ 1 )2T .\nOn the other hand, we showed earlier that ‖w⋆‖2 ≤ 3√κ, so by smoothness, F (0)− F (w⋆) ≤ µ2‖w⋆‖2 ≤ 3µ 2 √ κ. Therefore,\nF (wT )− F (w⋆) F (0)− F (w⋆) ≥\nλ\n12µ √ κ (√ κ− 1√ κ+ 1 )2T\nTo make the right-hand side less than ǫ, T must be such that (√\nκ− 1√ κ+ 1\n)2T ≤ 12µ √ κǫ\nλ ,\nwhich is equivalent to\n2T · log (√\nκ+ 1√ κ− 1\n) ≥ log (\nλ\n12µ √ κǫ\n)\n.\nSince log (√\nκ+1√ κ−1\n) = log (\n1 + 2√ κ−1\n)\n≤ 2√ κ−1 , it follows that T must be such that\n4T√ κ− 1 ≥ log\n(\nλ\n12µ √ κǫ\n)\n.\nPlugging in κ = µ/8λ and simplifying a bit, we get that\nT ≥ 1 4\n(√\nµ\n8λ − 1\n) · log (√ 8(λ/µ)3/2\n12ǫ\n)\n,\nfrom which the result follows."
    }, {
      "heading" : "A.3 Proof of Thm. 2",
      "text" : "We will define a randomized choice of quadratic functions f1, . . . , fn, and prove a lower bound on the expected optimization error of any algorithm (where the expectation is over both the algorithm and the randomized functions). This implies that for any algorithm, the same lower bound (in expectation over the algorithm only) holds for some deterministic choice of f1, . . . , fn.\nThere will actually be two separate constructions, one leading to a lower bound of Ω(n), and one leading\nto a lower bound of Ω (√\nnµ λ · log\n( (λ/µ)3/2 √ n\nǫ\n))\n. Choosing the construction which leads to the larger lower\nbound, the theorem follows."
    }, {
      "heading" : "A.3.1 An Ω(n) Lower Bound",
      "text" : "Starting with the Ω(n) lower bound, let δi, where i ∈ {1, . . . , n}, be chosen uniformly at random from {−1,+1}, and define\nfi(w) = −δiw1 + λ\n2 ‖w‖2.\nClearly, these are λ-smooth (and hence µ-smooth) functions, as well as λ-strongly convex. Also, the optimum of F (w) = µn ∑n i=1 fi(w) equals w ⋆ = ( 1 nλ ∑n i=1 δi ) e1, where e1 is the first unit vector. As a result, ‖w⋆‖2 = 1 λ2 ( 1 n ∑n i=1 δi )2 , so by λ-smoothness of F\nF (0)− F (w⋆) ≤ λ 2 ‖w⋆‖2 = 1 2λ\n(\n1\nn\nn ∑\ni=1\nδi\n)2\n.\nSince δi are i.i.d., we have by Hoeffding’s bound that with probability at least 3/4, ∣ ∣ 1 n ∑n i=1 δi ∣ ∣ is at most √\n2 log(8/3)/n ≤ √ 2/n. Plugging into the equation above, we get that with probability at least 3/4,\nF (0)− F (w⋆) ≤ 1 λn . (7)\nTurning to lower bound F (wT )− F (w⋆), we have by strong convexity that\nF (wT )− F (w⋆) ≥ λ\n2 ‖wT −w⋆‖2 ≥\nλ 2 (wT,1 − w⋆1)2\n= 1\n2λ\n(\nλwT,1 − 1\nn\nn ∑\ni=1\nδi\n)2\n.\nNow, if at most ⌊n/2⌋ indices {1, . . . , n} were queried by the algorithm, then the wT returned by the algorithm must be independent of at least ⌈n/2⌉ random variables δj1 , . . . , δj⌈n/2⌉ (for some distinct indices j1, j2, . . . depending on the algorithm’s behavior, but independent of the values of δj1 , . . . , δj⌈n/2⌉). Therefore, conditioned on j1, . . . , j⌈n/2⌉ and the values of δj1 , . . . , δj⌈n/2⌉ , the expression above can be written as\n1\n2λ\n\nη − 1 n\n∑\ni/∈{j1,...,j⌈n/2⌉} δi\n\n\n2\n,\nwhere η is a fixed quantity independent of the values of δi for i /∈ {j1, . . . , j⌈n/2⌉}. By a standard anticoncentration argument, with probability at least 3/4, this expression will be at least 12λ ( c′√ n )2 = c ′2 2λn for\nsome universal positive c′ > 0. Since this is true for any j1, . . . , j⌈n/2⌉ and δj1 , . . . , δj⌈n/2⌉ , we get that with probability at least 3/4 over δ1, . . . , δn,\nF (wT )− F (w⋆) ≥ c′2\n2λn .\nCombining this with Eq. (7) using a union bound, we have that with probability at least 1/2,\nF (wT )− F (w⋆) F (0) − F (w⋆) ≥ c′2λn 2λn = c′2 2 .\nAs a result, since the ratio above is always a non-negative quantity,\nE\n[\nF (wT )− F (w⋆) F (0)− F (w⋆)\n] ≥ c ′2\n4 .\nUsing the assumption stated in the theorem (taking c = c′2/4), we have that the right hand side cannot be smaller than ǫ, unless more than ⌊n/2⌋ = Ω(n) oracle calls are made.\nA.3.2 An Ω (√\nnµ λ · log\n( (λ/µ)3/2 √ n\nǫ\n))\nLower Bound\nWe now turn to prove the Ω (√\nnµ λ · log\n(\nλ ǫ\n)\n)\nlower bound, using a different function construction: Let\nj1, . . . , jd−1 be chosen uniformly and independently at random from {1, . . . , n}, and define\nfi(w) = λn(κ− 1)\n8\n( d−1 ∑\nl=1\n1jl=i(wl − wl+1)2 + 1\nn\n( w21 + (aκ − 1)w2d − w1 )\n)\n+ λ\n2 ‖w‖2.\nwhere 1A is the indicator of the event i, and κ = µ λn (which is ≥ 2 by assumption). Note that these are all λ-strongly convex functions, as all terms in their definition are convex in w, and there is an additional λ 2‖w‖2 term. Moreover, they are also µ-smooth: To see this, note that ∇2fi(w) λn(κ−1) 4 A+ λ 2 I , where A 4I is as defined in the proof of Lemma 1. Therefore, the smoothness parameter is at most\nλn(κ− 1) 4 · 4 + λ ≤ λnκ ≤ µ.\nThe average function F (w) = 1n ∑n i=1 fi(w) equals\nF (w) = λ(κ− 1)\n8\n( w21 + d−1 ∑\ni=1\n(wi − wi+1)2 + (aκ − 1)w2d − w1 ) + λ\n2 ‖w‖2,\nwhich is λ-strongly convex and is the same as F from Lemma 1 (with λ̃ = λ, µ̃ = µ/n, and κ̃ = κ). Therefore, by Lemma 1, the global minimum w⋆ of F equals (q, q2, . . . , qd), where q = √ κ−1√ κ+1\n. Note that since q < 1 and κ ≥ 2, the squared norm of w⋆ is at most\nd ∑\ni=1\nq2i ≤ ∞ ∑\ni=1\nq2i = q2 1− q2 ≤ 1 1− q = √ κ+ 1 2 ≤ √ κ,\nhence by smoothness,\nF (0)− F (w⋆) ≤ µ 2 ‖w⋆‖2 ≤ µ 2\n√ κ. (8)\nWith these preliminaries out of the way, we now turn to compute a lower bound on the expected optimization error. The proof is based on arguing that wT can only have a first few coordinates being non-zero. To see how this gives a lower bound, let lT ∈ {1, . . . , d} be the largest index of a non-zero coordinate of wT (or 0 if wT = 0). By definition of w⋆, we have\n‖wT −w⋆‖2 ≥ d ∑\ni=lT+1\nq2i ≥ g(lT ),\nwhere\ng(z) =\n{\nq2(z+1) z < d 0 z ≥ d .\nBy strong convexity of F , this implies that\nF (wT )− F (w⋆) ≥ λ\n2 ‖wT −w⋆‖2 ≥\nλ 2 g(lT ).\nFinally, taking expectation over the randomness of j1, . . . , jd−1 above (and over the internal randomness of the algorithm, if any), applying Lemma 2, and choosing the dimension d = ⌈2E[lT ]⌉ (which we will later show to equal the value specified in the theorem), we have\nE [F (wT )− F (w⋆)] ≥ λ\n4 q4E[lT ]+4 =\nλ\n4 (√ κ− 1√ κ+ 1 )4E[lT ]+4 .\nCombined with Eq. (8), this gives\nE\n[\nF (wT )− F (w⋆) F (0)− F (w⋆)\n]\n≥ λ 2µ √ κ (√ κ− 1√ κ+ 1 )4E[lT ]+4 . (9)\nThus, it remains to upper bound E[lT ]. To get a bound, we rely on the following key lemma (where ei is the i-th unit vector, and recall that Wt defines the set of allowed query points wt, and j1, . . . , jd are the random indices used in constructing f1, . . . , fn):\nLemma 7. For all t, it holds that Wt ⊆ span{ed, e1, e2, e3, . . . , eℓt} for all t, where ℓt is defined recursively as follows: ℓ1 = 1, and ℓt+1 equals the largest number in {1, . . . , d−1} such that {jℓt , jℓt+1, . . . , jℓt+1−1} ⊆ {it, it−1, . . . , imax{1,t−⌊n/2⌋+1}} (and ℓt+1 = ℓt if no such number exists).\nAs will be seen later, ℓT (which is a random variable as a function of the random indices j1, . . . , jd) upper-bounds the number of non-zero coordinates of wT , and therefore we can upper bound E[lT ] by E[ℓT ].\nProof. The proof is by induction over t. Since W1 = {0} ⊆ span(ed), the result trivially holds for t = 1. Now, suppose that Wt ⊆ span{ed, e1, . . . , eℓt} for some t and ℓt. Note that in particular, this means that wt is non-zero only in its first ℓt coordinates. By definition of fi for any i,\n∇fi(w) = λn(κ− 1)\n8\n(\n2\nd−1 ∑\nl=1\n1jl=i(wl − wl+1)(el − el+1) + 1\nn (2w1e1 + 2(aκ − 1)wded − e1)\n)\n+ λw\n∇2fi(w) = λn(κ− 1)\n8\n( d−1 ∑\nl=1\n1jl=i(2El,l − El+1,l − El,l+1) + 1\nn (2E1,1 + 2(aκ − 1)Ed,d)\n)\n+ λI,\nwhere Er,s is the d × d which is all zeros, except for an entry of 1 in location (r, s). It is easily seen that these expressions imply the following:\n• If jℓt 6= it, then ∇fit(wt) ∈ span{ed, e1, . . . , eℓt}, otherwise ∇fit(wt) ∈ span{ed, e1, . . . , eℓt+1}.\n• For any w and l ∈ {1, . . . , d− 1}, if jl 6= i, then ∇2fi(w) is block-diagonal, with a block in the first l× l entries. In other words, any entry (r, s) in the matrix, where r ≤ l and s > l (or r > l and s ≤ l) is zero.\n• As a result, if jl /∈ {it, it−1, . . . , imax{1,t−⌊n/2⌋+1}}, then ∑t τ=max{1,t−⌊n/2⌋+1} ατ∇2fiτ (wτ ), for arbitrary scalars τ , is block-diagonal with a block in the first l × l entries. The same clearly holds for any matrix with the same block-diagonal structure.\nTogether, these observations imply that the operations specified in Assumption 2 can lead to vectors outside span{ed, e1, . . . , eℓt}, only if jℓt ∈ {it, it−1, . . . , imax{1,t−⌊n/2⌋+1}}. Moreover, these vectors must belong to span{ed, e1, . . . , eℓt+1}, where ℓt+1 is as specified in the lemma: By definition, jℓt+1 is not in {it, it−1, . . . , imax{1,t−⌊n/2⌋+1}}, and therefore all relevant Hessians have a block in the first ℓt+1× ℓt+1 entries, hence it is impossible to create a vector with non-zero coordinates (using the operations of Assumption 2) beyond the first ℓt+1.\nSince wT ⊆ WT , the lemma above implies that E[lT ] from Eq. (9) (where lT is the largest index of a non-zero coordinate of wT ) can be upper-bounded by E[ℓT ], where the expectation is over the random draw of the indices j1, . . . , jd−1. This can be bounded using the following lemma: Lemma 8. It holds that E[ℓT ] ≤ 1 + 2(T−1)n . Proof. By definition of ℓt and linearity of expectation, we have\nE[ℓT ] = E\n[ T−1 ∑\nt=1\n(ℓt+1 − ℓt) ] + ℓ1 = T−1 ∑\nt=1\nE[ℓt+1 − ℓt] + 1. (10)\nLet us consider any particular term in the sum above. Since ℓt+1 − ℓt is a non-negative integer, we have E[ℓt+1 − ℓt] = Pr (ℓt+1 > ℓt) · E [ℓt+1 − ℓt | ℓt+1 > ℓt] .\nBy definition of ℓt, the event ℓt+1 > ℓt can occur only if jℓt /∈ {it−1, it−2, . . . , imax{1,t−⌊n/2⌋}}, yet jℓt ∈ {it, it−1, . . . , imax{1,t−⌊n/2⌋+1}}. This is equivalent to jℓt = it (that is, in iteration t we happened to choose the index jℓt of the unique individual function, which contains the block linking coordinate ℓt and ℓt + 1, hence allowing us to “advance” and have more non-zero coordinates). But since the algorithm is oblivious, it is fixed whereas jℓt is chosen uniformly at random, hence the probability of this event is 1/n. Therefore, Pr (ℓt+1 > ℓt) ≤ 1/n. Turning to the conditional expectation of ℓt+1 − ℓt above, it equals the expected number of indices jℓt , jℓt+1, . . . belonging to {it, it−1, . . . , imax{1,t−⌊n/2⌋+1}}, conditioned on jℓt belonging to that set. But since the i indices are fixed and the j indices are chosen uniformly at random, this equals one plus the expected number of times where a randomly drawn j ∈ {1, . . . , n} belongs to {it, it−1, . . . , it−⌊n/2⌋+1}. Since this set contains at most ⌊n/2⌋ distinct elements in {1, . . . , n}, this is equivalent to (one plus) the expectation of a geometric random variable, where the success probability is at most 1/2. By a standard derivation, this is at most 1 + 1/21−1/2 = 2. Plugging into the displayed equation above, we get that\nE[ℓt+1 − ℓt] ≤ 1 n · 2 = 2 n ,\nand therefore the bound in Eq. (10) is at most 2(T−1)n + 1 as required.\nPlugging this bound into Eq. (9), we get\nE\n[\nF (wT )− F (w⋆) F (0)− F (w⋆)\n]\n≥ λ 2µ √ κ (√ κ− 1√ κ+ 1 ) 8(T−1) n +8 .\nTo make the right-hand side less than ǫ, T must be such that\n(√ κ− 1√ κ+ 1 ) 8(T−1) n +8 ≤ 2µ √ κǫ λ ,\nwhich is equivalent to (\n8(T − 1) n + 8\n)\nlog (√ κ+ 1√ κ− 1 ) ≥ log ( λ 2µ √ κǫ ) .\nSince log (√\nκ+1√ κ−1\n) = log (\n1 + 2√ κ−1\n)\n≤ 2√ κ−1 , it follows that T must be such that\n(\n8(T − 1) n + 8\n)\n2√ κ− 1 ≥ log\n(\nλ\n2µ √ κǫ\n)\n.\nPlugging in κ = µ/λn and simplifying, we get that\nT ≥ 1 + n 8\n(\n√\nµ/λn− 1 2 · log (\n(λ/µ)3/2 √ n\n2ǫ\n) − 8 ) .\nUsing asymptotic notation, and recalling the assumptions µ/λn ≥ 2, the right-hand side equals\nΩ\n(\nn · √ µ\nλn · log\n( (λ/µ)3/2 √ n\nǫ\n))\n= Ω\n(\n√\nnµ\nλ · log\n( (λ/µ)3/2 √ n\nǫ\n))\nas required. The bound on the dimension d follows from the fact that we chose it to be O(E[lT ]) = O(1 + T/n), and to make the lower bound valid it is enough to pick some T = O (√\nnµ λ · log\n( (λ/µ)3/2 √ n\nǫ\n))\n."
    } ],
    "references" : [ {
      "title" : "A lower bound for the optimization of finite sums",
      "author" : [ "Alekh Agarwal", "Leon Bottou" ],
      "venue" : "arXiv preprint arXiv:1410.0723,",
      "citeRegEx" : "Agarwal and Bottou.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal and Bottou.",
      "year" : 2014
    }, {
      "title" : "Second order stochastic optimization in linear time",
      "author" : [ "Naman Agarwal", "Brian Bullins", "Elad Hazan" ],
      "venue" : "arXiv preprint arXiv:1602.03943,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2016
    }, {
      "title" : "Communication complexity of distributed convex learning and optimization",
      "author" : [ "Yossi Arjevani", "Ohad Shamir" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Arjevani and Shamir.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arjevani and Shamir.",
      "year" : 2015
    }, {
      "title" : "Dimension-free iteration complexity of finite sum optimization problems",
      "author" : [ "Yossi Arjevani", "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1606.09333,",
      "citeRegEx" : "Arjevani and Shamir.,? \\Q2016\\E",
      "shortCiteRegEx" : "Arjevani and Shamir.",
      "year" : 2016
    }, {
      "title" : "On the iteration complexity of oblivious first-order optimization algorithms",
      "author" : [ "Yossi Arjevani", "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1605.03529,",
      "citeRegEx" : "Arjevani and Shamir.,? \\Q2016\\E",
      "shortCiteRegEx" : "Arjevani and Shamir.",
      "year" : 2016
    }, {
      "title" : "Exact and inexact subsampled newton methods for optimization",
      "author" : [ "Raghu Bollapragada", "Richard Byrd", "Jorge Nocedal" ],
      "venue" : "arXiv preprint arXiv:1609.08502,",
      "citeRegEx" : "Bollapragada et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bollapragada et al\\.",
      "year" : 2016
    }, {
      "title" : "Convergence rates of sub-sampled newton methods",
      "author" : [ "Murat A Erdogdu", "Andrea Montanari" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Erdogdu and Montanari.,? \\Q2015\\E",
      "shortCiteRegEx" : "Erdogdu and Montanari.",
      "year" : 2015
    }, {
      "title" : "An optimal randomized incremental gradient method",
      "author" : [ "Guanghui Lan" ],
      "venue" : "arXiv preprint arXiv:1507.02000,",
      "citeRegEx" : "Lan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lan.",
      "year" : 2015
    }, {
      "title" : "Problem Complexity and Method Efficiency in Optimization",
      "author" : [ "A. Nemirovsky", "D. Yudin" ],
      "venue" : "WileyInterscience,",
      "citeRegEx" : "Nemirovsky and Yudin.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nemirovsky and Yudin.",
      "year" : 1983
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course, volume 87",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Nesterov.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2013
    }, {
      "title" : "Newton sketch: A linear-time optimization algorithm with linearquadratic convergence",
      "author" : [ "Mert Pilanci", "Martin J Wainwright" ],
      "venue" : "arXiv preprint arXiv:1505.02250,",
      "citeRegEx" : "Pilanci and Wainwright.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pilanci and Wainwright.",
      "year" : 2015
    }, {
      "title" : "Sub-sampled newton methods i: globally convergent algorithms",
      "author" : [ "Farbod Roosta-Khorasani", "Michael W Mahoney" ],
      "venue" : "arXiv preprint arXiv:1601.04737,",
      "citeRegEx" : "Roosta.Khorasani and Mahoney.,? \\Q2016\\E",
      "shortCiteRegEx" : "Roosta.Khorasani and Mahoney.",
      "year" : 2016
    }, {
      "title" : "Sub-sampled newton methods ii: Local convergence rates",
      "author" : [ "Farbod Roosta-Khorasani", "Michael W Mahoney" ],
      "venue" : "arXiv preprint arXiv:1601.04738,",
      "citeRegEx" : "Roosta.Khorasani and Mahoney.,? \\Q2016\\E",
      "shortCiteRegEx" : "Roosta.Khorasani and Mahoney.",
      "year" : 2016
    }, {
      "title" : "Tight complexity bounds for optimizing composite objectives",
      "author" : [ "Blake Woodworth", "Nathan Srebro" ],
      "venue" : "arXiv preprint arXiv:1605.08003,",
      "citeRegEx" : "Woodworth and Srebro.,? \\Q2016\\E",
      "shortCiteRegEx" : "Woodworth and Srebro.",
      "year" : 2016
    }, {
      "title" : "Sub-sampled newton methods with non-uniform sampling",
      "author" : [ "Peng Xu", "Jiyan Yang", "Farbod Roosta-Khorasani", "Christopher Ré", "Michael W Mahoney" ],
      "venue" : "arXiv preprint arXiv:1607.00559,",
      "citeRegEx" : "Xu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "To study the complexity of this and other optimization problems, it is common to consider an oracle model, where the optimization algorithm has no a-priori information about the objective function, and obtains information from an oracle which provides values and derivatives of the function at various domain points [Nemirovsky and Yudin, 1983].",
      "startOffset" : 316,
      "endOffset" : 344
    }, {
      "referenceID" : 0,
      "context" : "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt − αt ( ∇F (w) )∇F (w), (2) where ∇F (w),∇2F (w) are the gradient and the Hessian of F at w, and αt is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d× d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(〈w,xi〉), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l′′ i (〈w,xi〉)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al.",
      "startOffset" : 87,
      "endOffset" : 2162
    }, {
      "referenceID" : 0,
      "context" : "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt − αt ( ∇F (w) )∇F (w), (2) where ∇F (w),∇2F (w) are the gradient and the Hessian of F at w, and αt is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d× d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(〈w,xi〉), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l′′ i (〈w,xi〉)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.",
      "startOffset" : 87,
      "endOffset" : 2185
    }, {
      "referenceID" : 0,
      "context" : "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt − αt ( ∇F (w) )∇F (w), (2) where ∇F (w),∇2F (w) are the gradient and the Hessian of F at w, and αt is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d× d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(〈w,xi〉), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l′′ i (〈w,xi〉)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.",
      "startOffset" : 87,
      "endOffset" : 2216
    }, {
      "referenceID" : 0,
      "context" : "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt − αt ( ∇F (w) )∇F (w), (2) where ∇F (w),∇2F (w) are the gradient and the Hessian of F at w, and αt is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d× d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(〈w,xi〉), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l′′ i (〈w,xi〉)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016], Xu et al.",
      "startOffset" : 87,
      "endOffset" : 2284
    }, {
      "referenceID" : 0,
      "context" : "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt − αt ( ∇F (w) )∇F (w), (2) where ∇F (w),∇2F (w) are the gradient and the Hessian of F at w, and αt is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d× d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(〈w,xi〉), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l′′ i (〈w,xi〉)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016], Xu et al. [2016] and references therein).",
      "startOffset" : 87,
      "endOffset" : 2302
    }, {
      "referenceID" : 8,
      "context" : "Given a first-order oracle and a strongly convex and smooth objective in sufficiently high dimensions, it is well-known that the worst-case oracle complexity is Ω( √ μ/λ · log(1/ǫ)) [Nemirovsky and Yudin, 1983].",
      "startOffset" : 182,
      "endOffset" : 210
    }, {
      "referenceID" : 1,
      "context" : "Agarwal et al. [2016], Xu et al.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "Agarwal et al. [2016], Xu et al. [2016]. We note that the limitations arising from oblivious optimization schemes (in a somewhat stronger sense) was also explored in Arjevani and Shamir [2016a,b].",
      "startOffset" : 0,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "As mentioned earlier, the observation that such first-order oracle bounds can be extended to higherorder oracles is also briefly mentioned (without proof) in Nemirovsky and Yudin [1983, Section 7.2.6]. Also, the theorem considers deterministic algorithms (which includes standard second-order methods, such as the Newton method), but otherwise makes no assumption on the algorithm. Generalizing this result to randomized algorithms should be quite doable, based on the techniques developed in Woodworth and Srebro [2016]. We leave a formal derivation to future work.",
      "startOffset" : 158,
      "endOffset" : 521
    }, {
      "referenceID" : 5,
      "context" : "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 4,
      "context" : "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).",
      "startOffset" : 40,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "A possible exception to this is the Newton sketch algorithm [Pilanci and Wainwright, 2015], which relies on random projections, but on the flip side is computationally expensive.",
      "startOffset" : 60,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "There have been several rigorous studies of such “subsampled Newton” methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.",
      "startOffset" : 86,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "There have been several rigorous studies of such “subsampled Newton” methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein.",
      "startOffset" : 156,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "There have been several rigorous studies of such “subsampled Newton” methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more “important”.",
      "startOffset" : 156,
      "endOffset" : 428
    }, {
      "referenceID" : 5,
      "context" : "There have been several rigorous studies of such “subsampled Newton” methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more “important”. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 1, as the sampled indices are now chosen in a way dependent on the individual functions, but since this dependence is only through the Hessians, it still satisfies Assumption 1a. Therefore, our lower bound as stated in Thm. 3 still applies to such a method. A variant of the subsampled Newton approach, studied in Erdogdu and Montanari [2015], uses a lowrank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself.",
      "startOffset" : 156,
      "endOffset" : 1080
    }, {
      "referenceID" : 5,
      "context" : "There have been several rigorous studies of such “subsampled Newton” methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more “important”. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 1, as the sampled indices are now chosen in a way dependent on the individual functions, but since this dependence is only through the Hessians, it still satisfies Assumption 1a. Therefore, our lower bound as stated in Thm. 3 still applies to such a method. A variant of the subsampled Newton approach, studied in Erdogdu and Montanari [2015], uses a lowrank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself. However, this still falls in the framework of Assumption 2, and our lower bound still applies. A different approach to approximate the full Hessian is via randomized sketching techniques, which replace the Hessian ∇2F (w) by a low-rank approximation of the form (∇2F (w))1/2SS⊤(∇2F (w))1/2, where S ∈ Rd×m,m ≪ d is a random sketching matrix, and (∇2F (w))1/2 is the matrix square root of ∇2F (w). This approach forms the basis of the Newton sketch algorithm proposed in Pilanci and Wainwright [2015]. This approach currently escapes our lower bound, since it violates Assumption 2.",
      "startOffset" : 156,
      "endOffset" : 1699
    }, {
      "referenceID" : 1,
      "context" : "Agarwal et al. [2016] develop another line of stochastic second-order methods, which are based on the observation that the Newton step (∇2F (w))−1∇F (w) is the solution of the system of linear equations ∇2F (w)x = ∇F (w).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "That being said, it is important to note that the complexity upper bound attained in Agarwal et al. [2016] for LiSSA-Sample is on the order of Õ((n+ √ dμ/λ) log(1/ǫ)) (asymptotically as ǫ → 0), which is better than our lower bound if d ≪ n.",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "Based on the fact that only at most d − 1 out of n functions are relevant in the construction, we conjecture that the possible improvement in the oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d, which is indeed the type of improvement attained (for small enough ǫ) in Agarwal et al. [2016]. 9",
      "startOffset" : 319,
      "endOffset" : 341
    }, {
      "referenceID" : 1,
      "context" : "Finally, we note that Agarwal et al. [2016] proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Finally, we note that Agarwal et al. [2016] proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem. However, it requires performing ≥ 1 full Newton steps, so the runtime is prohibitive for large-scale problems (indeed, for quadratics as used in our lower bounds, even a single Newton step suffices to compute an exact solution). 5 Summary and Discussion In this paper, we studied the oracle complexity for optimization problems, assuming availability of a secondorder oracle. This is in contrast to most existing oracle complexity results, which focus on a first-order oracle. First, we formally proved that in the standard setting of strongly-convex and smooth optimization problems, second-order information does not significantly improve the oracle complexity, and further assumptions (i.e. Lipschitzness of the Hessians) are in fact necessary. We then presented our main lower bounds, which show that for finite-sum problems with a second-order oracle, under some reasonable algorithmic assumptions, the resulting oracle complexity is – again – not significantly better than what can be obtained using a first-order oracle. Moreover, this is shown using quadratic functions, which have 0 derivatives of order larger than 2. Hence, our lower bounds apply even if we have access to an oracle returning derivatives of order p for all p ≥ 0, and the function is smooth to any order. In Sec. 4, we studied how our framework and lower bounds are applicable to most existing approaches. Although this conclusion may appear very pessimistic, they are actually useful in pinpointing potential assumptions and approaches which may circumvent these lower bounds. In particular: • Our lower bound for algorithms employing non index-oblivious sampling schemes (Thm. 3) only hold when the dimension d is very large. This leaves open the possibility of better (non index-oblivious) algorithms when d is moderate, as was recently demonstrated in the context of the LiSSA-Sample algorithm of Agarwal et al. [2016] (at least for small enough ǫ).",
      "startOffset" : 22,
      "endOffset" : 2102
    }, {
      "referenceID" : 5,
      "context" : "Bollapragada et al. [2016], Xu et al.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.",
      "startOffset" : 0,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.",
      "startOffset" : 0,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "1 Auxiliary Lemmas The following lemma was essentially proven in Lan [2015], Nesterov [2013], but we provide a proof for completeness: Lemma 1.",
      "startOffset" : 65,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "1 Auxiliary Lemmas The following lemma was essentially proven in Lan [2015], Nesterov [2013], but we provide a proof for completeness: Lemma 1.",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "1 The proof is inspired by a technique introduced in Woodworth and Srebro [2016] for analyzing randomized first-order methods, in which a quadratic function is “locally flattened” in order to make first-order (gradient) information non-informative.",
      "startOffset" : 53,
      "endOffset" : 81
    } ],
    "year" : 2017,
    "abstractText" : "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer – perhaps surprisingly – is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.",
    "creator" : "LaTeX with hyperref package"
  }
}