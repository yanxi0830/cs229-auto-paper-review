{
  "name" : "1609.01000.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convexified Convolutional Neural Networks",
    "authors" : [ "Yuchen Zhang", "Percy Liang", "Martin J. Wainwright" ],
    "emails" : [ "zhangyuc@cs.stanford.edu.", "pliang@cs.stanford.edu.", "wainwrig@eecs.berkeley.edu." ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37]. There are two principal advantages of a CNN over a fully-connected neural network: (i) sparsity—that each nonlinear convolutional filter acts only on a local patch of the input, and (ii) parameter sharing—that the same filter is applied to each patch.\nHowever, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard [6]. In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation [7]. This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper [19]), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.\nIn this paper, with the goal of addressing these two challenges, we propose a new model class known as convexified convolutional neural networks (CCNNs). These models have two desirable features. First, training a CCNN corresponds to a convex optimization problem, which can be solved efficiently and optimally via a projected gradient algorithm. Second, the statistical properties of CCNN models can be studied in a precise and rigorous manner. We obtain CCNNs by convexifying\n∗Computer Science Department, Stanford University, Stanford, CA 94305. Email: zhangyuc@cs.stanford.edu. †Computer Science Department, Stanford University, Stanford, CA 94305. Email: pliang@cs.stanford.edu. ‡Department of Electrical Engineering and Computer Science and Department of Statistics, University of California\nBerkeley, Berkeley, CA 94720. Email: wainwrig@eecs.berkeley.edu.\nar X\niv :1\n60 9.\n01 00\n0v 1\n[ cs\n.L G\n] 4\nS ep\n2 01\ntwo-layer CNNs; doing so requires overcoming two challenges. First, the activation function of a CNN is nonlinear. In order to address this issue, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by our earlier work [48], involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks. Second, the parameter sharing induced by CNNs is crucial to its effectiveness and must be preserved. We show that CNNs with RKHS filters can be parametrized by a low-rank matrix. Further relaxing the low-rank constraint to a nuclear norm constraint leads to our final formulation of CCNNs.\nOn the theoretical front, we prove an oracle inequality on generalization error achieved by our class of CCNNs, showing that it is upper bounded by the best possible performance achievable by a two-layer CNN given infinite data—a quantity to which we refer as the oracle risk—plus a model complexity term that decays to zero polynomially in the sample size. Our results show that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network [48], highlighting the importance of parameter sharing. For models with more than one hidden layer, our theory does not apply, but we provide encouraging empirical results using a greedy layer-wise training heuristic. We then apply CCNNs to the MNIST handwritten digit dataset as well as four variation datasets [43], and find that it achieves the state-of-the-art performance. On the CIFAR-10 dataset, CCNNs outperform CNNs of the same depths, as well as other baseline methods that do not involve nonconvex optimization. We also demonstrate that building CCNNs on top of existing CNN filters improves the performance of CNNs.\nThe remainder of this paper is organized as follows. We begin in Section 2 by introducing convolutional neural networks, and setting up the empirical risk minimization problem studied in this paper. In Section 3, we describe the algorithm for learning two-layer CCNNs, beginning with the simple case of convexifying CNNs with a linear activation function, then proceeding to convexify CNNs with a nonlinear activation. We show that the generalization error of a CCNN converges to that of the best possible CNN. In Section 4, we describe several extensions to the basic CCNN algorithm, including averaging pooling, multi-channel input processing, and the layer-wise learning of multi-layer CNNs. In Section 5, we report the empirical evaluations of CCNNs. We survey related work in Section 6 and conclude the paper in Section 7.\nNotation. For any positive integer n, we use [n] as a shorthand for the discrete set {1, 2, . . . , n}. For a rectangular matrix A, let ‖A‖∗ be its nuclear norm, ‖A‖2 be its spectral norm (i.e., maximal singular value), and ‖A‖F be its Frobenius norm. We use `2(N) to denote the set of countable dimensional vectors v = (v1, v2, . . . ) such that ∑∞ `=1 v 2 ` < ∞. For any vectors u, v ∈ `2(N), the\ninner product 〈u, v〉 := ∑∞ `=1 uivi and the `2-norm ‖u‖2 := √ 〈u, u〉 are well defined."
    }, {
      "heading" : "2 Background and problem set-up",
      "text" : "In this section, we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem."
    }, {
      "heading" : "2.1 Convolutional neural networks.",
      "text" : "At a high level, a two-layer CNN1 is a particular type of function that maps an input vector x ∈ Rd0 (e.g., an image) to an output vector in y ∈ Rd2 (e.g., classification scores for the d2 classes). This mapping is formed in the following manner:\n• First, we extract a collection of P vectors {zp(x)}Pj=1 of the full input vector x. Each vector zp(x) ∈ Rd1 is referred to as a patch, and these patches may depend on overlapping components of x.\n• Second, given some choice of activation function σ : R → R and a collection of weight vectors {wj}rj=1 in Rd1 , we compute the functions\nhj(z) := σ(w > j z) for each patch z ∈ Rd1 . (1)\nEach function hj (for j ∈ [r]) is known as a filter, and note that the same filters are applied to each patch—this corresponds to the parameter sharing of a CNN.\n• Third, for each patch index p ∈ [P ], filter index j ∈ [r], and output coordinate k ∈ [d2], we introduce a coefficient αk,j,p ∈ R that governs the contribution of the filter hj on patch zp(x) to output fk(x). The final form of the CNN is given by f(x) : = (f1(x), . . . , fd2(x)), where the k th\ncomponent is given by\nfk(x) := r∑ j=1 P∑ p=1 αk,j,phj(zp(x)). (2)\nTaking the patch functions {zp}Pp=1 and activation function σ as fixed, the parameters of the CNN are the filter vectors w := {wj ∈ Rd1 : j ∈ [r]} along with the collection of coefficient vectors α := {αk,j ∈ RP : k ∈ [d2], j ∈ [r]}. We assume that all patch vectors zp(x) ∈ Rd1 are contained in the unit `2-ball. This assumption can be satisfied without loss of generality by normalization: By multiplying a constant γ > 0 to every patch zp(x) and multiplying 1/γ to the filter vectors w, the assumption will be satisfied without changing the the output of the network.\nGiven some positive radii B1 and B2, we consider the model class Fcnn(B1, B2) := { f of the form (2) : max\nj∈[r] ‖wj‖2 ≤ B1 and max k∈[d2],j∈[r] ‖αk,j‖2 ≤ B2\n} . (3)\nWhen the radii (B1, B2) are clear from context, we adopt Fcnn as a convenient shorthand."
    }, {
      "heading" : "2.2 Empirical risk minimization.",
      "text" : "Given an input-output pair (x, y) and a CNN f , we let L(f(x); y) denote the loss incurred when the output y is predicted via f(x). We assume that the loss function L is convex and L-Lipschitz in its first argument given any value of its second argument. As a concrete example, for multiclass classification with d2 classes, the output vector y takes values in the discrete set [d2] = {1, 2, . . . , d2}.\n1Average pooling and multiple channels are also an integral part of CNNs, but these do not present any new technical challenges, so that we defer these extensions to Section 4.\nFor example, given a vector f(x) = (f1(x), . . . , fd2(y)) ∈ Rd2 of classification scores, the associated multiclass logistic loss for a pair (x, y) is given by L(f(x); y) := −fy(x) + log (∑d2 y′=1 exp(fy′(x)) ) .\nGiven n training examples {(xi, yi)}ni=1, we would like to compute an empirical risk minimizer\nf̂cnn ∈ arg min f∈Fcnn n∑ i=1 L(f(xi); yi). (4)\nRecalling that functions f ∈ Fcnn depend on the parameters w and α in a highly nonlinear way (2), this optimization problem is nonconvex. As mentioned earlier, heuristics based on stochastic gradient methods are used in practice, which makes it challenging to gain a theoretical understanding of their behavior. Thus, in the next section, we describe a relaxation of the class Fcnn that allows us to obtain a convex formulation of the associated empirical risk minimization problem."
    }, {
      "heading" : "3 Convexifying CNNs",
      "text" : "We now turn to the development of the class of convexified CNNs. We begin in Section 3.1 by illustrating the procedure for the special case of the linear activation function. Although the linear case is not of practical interest, it provides intuition for our more general convexification procedure, described in Section 3.2, which applies to nonlinear activation functions. In particular, we show how embedding the nonlinear problem into an appropriately chosen reproducing kernel Hilbert space (RKHS) allows us to again reduce to the linear setting."
    }, {
      "heading" : "3.1 Linear activation functions: low rank relaxations",
      "text" : "In order to develop intuition for our approach, let us begin by considering the simple case of the linear activation function σ(t) = t. In this case, the filter hj when applied to the patch vector zp(x) outputs a Euclidean inner product of the form hj(zp(x)) = 〈zp(x), wj〉. For each x ∈ Rd0 , we first define the P × d1-dimensional matrix\nZ(x) := z1(x) >\n... zP (x) >  . (5) We also define the P -dimensional vector αk,j := (αk,j,1, . . . , αk,j,P )\n>. With this notation, we can rewrite equation (2) for the kth output as\nfk(x) = r∑ j=1 P∑ p=1 αk,j,p〈zp(x), wj〉 = r∑ j=1 α>k,jZ(x)wj = tr ( Z(x) ( r∑ j=1 wjα > k,j )) = tr(Z(x)Ak), (6)\nwhere in the final step, we have defined the d1×P -dimensional matrix Ak := ∑r j=1wjα > k,j . Observe that fk now depends linearly on the matrix parameter Ak. Moreover, the matrix Ak has rank at most r, due to the parameter sharing of CNNs. See Figure 1 for a graphical illustration of this model structure.\nLetting A := (A1, . . . , Ad2) be a concatenation of these matrices across all d2 output coordinates, we can then define a function fA : Rd1 → Rd2 of the form\nfA(x) := (tr(Z(x)A1), . . . , tr(Z(x)Ad2)). (7)\nNote that these functions have a linear parameterization in terms of the underlying matrix A. Our model class corresponds to a collection of such functions based on imposing certain constraints on the underlying matrix A: in particular, we define\nFcnn(B1, B2) := { fA : max\nj∈[r] ‖wj‖2 ≤ B1 and max\nk∈[d2] j∈[r]\n‖αk,j‖2 ≤ B2\n︸ ︷︷ ︸ Constraint (C1)\nand rank(A) = r︸ ︷︷ ︸ Constraint (C2)\n} .\nThis is simply an alternative formulation of our original class of CNNs. Notice that if the filter weights wj are not shared across all patches, then the constraint (C1) still holds, but constraint (C2) no longer holds. Thus, the parameter sharing of CNNs is realized by the low-rank constraint (C2). The matrix A of rank r can be decomposed as A = UV >, where both U and V have r columns. The column space of matrix A contains the convolution parameters {wj}, and the row space of A contains to the output parameters {αk,j}.\nThe rank-r matrices satisfying constraints (C1) and (C2) form a nonconvex set. A standard convex relaxation of a rank constraint is based on the nuclear norm ‖A‖∗ corresponding to the sum of the singular values of A. It is straightforward to verify that any matrix A satisfying the constraints (C1) and (C2) must have nuclear norm bounded as ‖A‖∗ ≤ B1B2r √ d2. Consequently, if we define the function class\nFccnn := { fA : ‖A‖∗ ≤ B1B2r √ d2 } , (8)\nthen we are guaranteed that Fccnn ⊇ Fcnn. Overall, we propose to minimize the empirical risk (4) over Fccnn instead of Fcnn; doing so\ndefines a convex optimization problem over this richer class of functions\nf̂ccnn := arg min fA∈Fccnn n∑ i=1 L(fA(xi); yi). (9)\nIn Section 3.3, we describe iterative algorithms that can be used to solve this form of convex program in the more general setting of nonlinear activation functions."
    }, {
      "heading" : "3.2 Nonlinear activations: RKHS filters",
      "text" : "For nonlinear activation functions σ, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). As we show, this relaxation allows us to reduce the problem to the linear activation case.\nLet K : Rd1 × Rd1 → R be a positive semidefinite kernel function. For particular choices of kernels (e.g., the Gaussian RBF kernel) and some sufficiently smooth activation function σ, we are able to show that the filter h : z 7→ σ(〈w, z〉) is contained in the RKHS induced by the kernel function K. See Section 3.4 for the choice of the kernel function and the activation function. Let S := {zp(xi) : p ∈ [P ], i ∈ [n]} be the set of patches in the training dataset. The representer theorem then implies that for any patch zp(xi) ∈ S, the function value can be represented by\nh(zp(xi)) = ∑\n(i′,p′)∈[n]×[P ]\nci′,p′k(zp(xi), zp′(xi′)) (10)\nfor some coefficients {ci′,p′}(i′,p′)∈[n]×[P ]. Filters taking the form (10) are members of the RKHS, because they are linear combinations of basis functions z 7→ k(z, zp′(xi′)). Such filters are parametrized by a finite set of coefficients, which can be estimated via empirical risk minimization.\nLet K ∈ RnP×nP be the symmetric kernel matrix, where with rows and columns indexed by the example-patch index pair (i, p) ∈ [n]× [P ]. The entry at row (i, p) and column (i′, p′) of matrix K is equal to K(zp(xi), zp′(xi′)). So as to avoid re-deriving everything in the kernelized setting, we perform a reduction to the linear setting of Section 3.1. Consider a factorization K = QQ> of the kernel matrix, where Q ∈ RnP×m; one example is the Cholesky factorization with m = nP . We can interpret each row Q(i,p) ∈ Rm as a feature vector in place of the original zp(xi) ∈ Rd1 , and rewrite equation (10) as\nh(zp(xi)) = 〈Q(i,p), w〉 where w := ∑ (i′,p′) ci′,p′Q(i′,p′).\nIn order to learn the filter h, it suffices to learn the m-dimensional vector w. To do this, define patch matrices Z(xi) ∈ RP×m for each i ∈ [n] so that its p-th row is Q(i,p). Then we carry out all of Section 3.1; solving the ERM gives us a parameter matrix A ∈ Rm×Pd2 . The only difference is that the B1 norm constraint needs to be relaxed as well. See Appendix B for details.\nAt test time, given a new input x ∈ Rd0 , we can compute a patch matrix Z(x) ∈ RP×m as follows:\n• The p-th row of this matrix is the feature vector for patch p, which is equal to Q†v(zp(x)) ∈ Rm. Here, for any patch z, the vector v(z) is defined as a nP -dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). We note that if x is an instance xi in the training set, then the vector Q†v(zp(x)) is exactly equal to Q(i,p). Thus the mapping Z(x) applies to both training and testing.\nAlgorithm 1: Learning two-layer CCNNs\nInput: Data {(xi, yi)}ni=1, kernel function K, regularization parameter R > 0, number of filters r.\n1. Construct a kernel matrix K ∈ RnP×nP such that the entry at column (i, p) and row (i′, p′) is equal to K(zp(xi), zp′(xi′)). Compute a factorization K = QQ> or an approximation K ≈ QQ>, where Q ∈ RnP×m.\n2. For each xi, construct patch matrix Z(xi) ∈ RP×m whose p-th row is the (i, p)-th row of Q, where Z(·) is defined in Section 3.2.\n3. Solve the following optimization problem to obtain a matrix Â = (Â1, . . . , Âd2):\nÂ ∈ argmin ‖A‖∗≤R L̃(A) where L̃(A) := n∑ i=1 L (( tr(Z(xi)A1), . . . , tr(Z(xi)Ad2) ) ; yi ) . (12)\n4. Compute a rank-r approximation Ã ≈ Û V̂ > where Û ∈ Rm×r and V̂ ∈ RPd2×r.\nOutput: Return the predictor f̂ccnn(x) := ( tr(Z(x)Â1), . . . , tr(Z(x)Âd2) ) and the convolutional layer output H(x) := Û>(Z(x))>.\n• We can then compute the predictor fk(x) = tr(Z(x)Ak) via equation (6). Note that we do not explicitly need to compute the filter values hj(zp(x)) to compute the output under the CCNN.\nRetrieving filters. However, when we learn multi-layer CCNNs, we need to compute the filters explicitly. Recall from Section 3.1 that the column space of matrix A corresponds to parameters of the convolutional layer, and the row space of A corresponds to parameters of the output layer. Thus, once we obtain the parameter matrix A, we compute a rank-r approximation A ≈ Û V̂ >. Then set the j-th filter hj to the mapping\nz 7→ 〈Ûj , Q†v(z)〉 for any patch z ∈ Rd1 , (11)\nwhere Ûj ∈ Rm is the j-th column of matrix Û , and Q†v(z) represents the feature vector for patch z. The matrix V̂ > encodes parameters of the output layer, thus doesn’t appear in the filter expression (11). It is important to note that the filter retrieval is not unique, because the rankr approximation of the matrix A is not unique. One feasible way is to form the singular value decomposition A = UΛV >, then define Û to be the first r columns of U , and define V̂ > to be the first r rows of ΛV >.\nWhen we apply all of the r filters to all patches of an input x ∈ Rd0 , the resulting output is H(x) := Û>(Z(x))> — this is an r × P matrix whose element at row j and column p is equal to hj(zp(x))."
    }, {
      "heading" : "3.3 Algorithm",
      "text" : "The algorithm for learning a two-layer CCNN is summarized in Algorithm 1; it is a formalization of the steps described in Section 3.2. In order to solve the optimization problem (12), the simplest\napproach is to via projected gradient descent: At iteration t, using a step size ηt > 0, it forms the new matrix At+1 based on the previous iterate At according to:\nAt+1 = ΠR ( At − ηt ∇AL̃(At) ) . (13)\nHere ∇AL̃ denotes the gradient of the objective function defined in (12), and ΠR denotes the Euclidean projection onto the nuclear norm ball {A : ‖A‖∗ ≤ R}. This nuclear norm projection can be obtained by first computing the singular value decomposition of A, and then projecting the vector of singular values onto the `1-ball. This latter projection step can be carried out efficiently by the algorithm of Duchi et al. [16]. There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46]. All these algorithms can be executed in a stochastic fashion, so that each gradient step processes a mini-batch of examples.\nThe computational complexity of each iteration depends on the width m of the matrix Q. Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nyström approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q ∈ RnP×m such that K ≈ QQ>. Typically, the parameter m is chosen to be much smaller than nP . In order to compute the matrix Q, the Nyström approximation method takes O(m2nP ) time. The random feature approximation takes O(mnPd1) time, but can be improved to O(mnP log d1) time using the fast Hadamard transform [27]. The complexity of computing a gradient vector on a batch of b images is O(mPd2b). The complexity of projecting the parameter matrix onto the nuclear norm ball is O(min{m2Pd2,mP 2d22}). Thus, the approximate algorithms provide substantial speed-ups on the projected gradient descent steps."
    }, {
      "heading" : "3.4 Theoretical results",
      "text" : "In this section, we upper bound the generalization error of Algorithm 1, proving that it converges to the best possible generalization error of CNN. We focus on the binary classification case where the output dimension is d2 = 1. 2\nThe learning of CCNN requires a kernel function K. We consider kernel functions whose associated RKHS is large enough to contain any function taking the following form: z 7→ q(〈w, z〉), where q is an arbitrary polynomial function and w ∈ Rd1 is an arbitrary vector. As a concrete example, we consider the inverse polynomial kernel:\nK(z, z′) := 1 2− 〈z, z′〉 , ‖z‖2 ≤ 1, ‖z′‖2 ≤ 1. (14)\nThis kernel was studied by Shalev-Shwartz et al. [36] for learning halfspaces, and by Zhang et al. [48] for learning fully-connected neural networks. We also consider the Gaussian RBF kernel:\nK(z, z′) := exp(−γ‖z − z′‖22), ‖z‖2 = ‖z′‖2 = 1, γ > 0. (15)\nAs we show in Appendix A, the inverse polynomial kernel and the Gaussian kernel satisfy the above notion of richness. We focus on these two kernels for the theoretical analysis.\n2We can treat the multiclass case by performing a standard one-versus-all reduction to the binary case.\nLet f̂ccnn be the CCNN that minimizes the empirical risk (12) using one of the two kernels above. Our main theoretical result is that for suitably chosen activation functions, the generalization error of f̂ccnn is comparable to that of the best CNN model. In particular, the following theorem applies to activation functions σ of the following types:\n(a) arbitrary polynomial functions (e.g., used by [10, 29]).\n(b) sinusoid activation function σ(t) := sin(t) (e.g., used by [39, 22]). (c) erf function σerf(t) := 2/ √ π ∫ t 0 e −z2dz, which represents an approximation to the sigmoid\nfunction (See Figure 2(a)). (d) a smoothed hinge loss σsh(t) := ∫ t −∞ 1 2(σerf(z) + 1)dz, which represents an approximation to\nthe ReLU function (See Figure 2(b)).\nTo understand why these activation functions pair with our choice of kernels, we consider polynomial expansions of the above activation functions: σ(t) = ∑∞ j=0 ajt\nj , and note that the smoothness of these functions are characterized by the rate of their coefficients {aj}∞j=0 converging to zero. If σ is a polynomial in category (a), then the richness of the RKHS guarantees that it contains the class of filters activated by function σ. If σ is a non-polynomial function in categories (b),(c),(d), then as Appendix A shows, the RKHS contains the filter only if the coefficients {aj}∞j=0 converge quickly enough to zero (the criterion depends on the choice of the kernel). Concretely, the inverse polynomial kernel is shown to capture all of the four categories of activations: they are referred as valid activation functions for the inverse polynomial kernel. The Gaussian kernel induces a smaller RKHS, and is shown to capture categories (a),(b), so that these functions are referred as valid activation functions for the Gaussian kernel. In contrast, the sigmoid function and the ReLU function are not valid for either kernel, because their polynomial expansions fail to converge quickly enough, or more intuitively speaking, because they are not smooth enough functions to be contained in the RKHS.\nWe are ready to state the main theoretical result. In the theorem statement, we use K(X) ∈ RP×P to denote the random kernel matrix obtained from an input vector X ∈ Rd0 drawn randomly from the population. More precisely, the (p, q)-th entry of K(X) is given by K(zp(X), zq(X)).\nTheorem 1. Assume that the loss function L(·; y) is L-Lipchitz continuous for every y ∈ [d2] and that K is the inverse polynomial kernel or the Gaussian kernel. For any valid activation function σ, there is a constant Cσ(B1) such that with the radius R := Cσ(B1)B2r, the expected generalization error is at most\nEX,Y [L(f̂ccnn(X);Y )] ≤ inf f∈Fcnn\nEX,Y [L(f(X);Y )] + c LCσ(B1)B2r √ log(nP ) EX [‖K(X)‖2]√\nn , (16)\nwhere c > 0 is a universal constant.\nProof sketch The proof of Theorem 1 consists of two parts: First, we consider a larger function class that contains the class of CNNs. This function class is defined as:\nFccnn := { x 7→ r∗∑ j=1 P∑ p=1 αj,phj(zp(x)) : r ∗ <∞ and r∗∑ j=1 ‖αj‖2‖hj‖H ≤ Cσ(B1)B2d2 } .\nwhere ‖·‖H is the norm of the RKHS associated with the kernel. This new function class relaxes the class of CNNs in two ways: 1) the filters are relaxed to belong to the RKHS, and 2) the `2-norm bounds on the weight vectors are replaced by a single constraint on ‖αj‖2 and ‖hj‖H. We prove the following property for the predictor f̂ccnn: it must be an empirical risk minimizer of Fccnn, even though the algorithm has never explicitly optimized the loss within this nonparametric function class.\nSecond, we characterize the Rademacher complexity of this new function class Fccnn, proving an upper bound for it based on the matrix concentration theory. Combining this bound with the classical Rademacher complexity theory [4], we conclude that the generalization loss of f̂ccnn converges to the least possible generalization error of Fccnn. The later loss is bounded by the generalization loss of CNNs (because Fcnn ⊆ Fccnn), which establishes the theorem. See Appendix C for the full proof of Theorem 1.\nRemark on activation functions. It is worth noting that the quantity Cσ(B1) depends on the activation function σ, and more precisely, depends on the convergence rate of the polynomial expansion of σ. Appendix A shows that if σ is a polynomial function of degree `, then Cσ(B1) = O(B`1). If σ is the sinusoid function, the erf function or the smoothed hinge loss, then the quantity Cσ(B1) will be exponential in B1. In an algorithmic perspective, we don’t need to know the activation function for executing Algorithm 1. In a theoretical perspective, however, the choice of σ is relevant from the point of Theorem 1 to compare f̂ccnn with the best CNN, whose representation power is characterized by the choice of σ. Therefore, if a CNN with a low-degree polynomial σ performs well on a given task, then CCNN also enjoys correspondingly strong generalization. Empirically, this is actually borne out: in Section 5, we show that the quadratic activation function performs almost as well as the ReLU function for digit classification.\nRemark on parameter sharing. In order to demonstrate the importance of parameter sharing, consider a CNN without parameter sharing, so that we have filter weights wj,p for each filter index\nj and patch index p. With this change, the new CNN output (2) is\nf(x) = r∑ j=1 P∑ p=1 αj,pσ(w > j,pzp(x)), where αj,p ∈ R and wj,p ∈ Rd1 . (17)\nNote that the hidden layer of this new network has P times more parameters than that of the convolutional neural network with parameter sharing. These networks without parameter sharing can be learned by the recursive kernel method proposed by Zhang et al. [48]. This paper shows that under the norm constraints ‖wj‖2 ≤ B′1 and ∑r j=1 ∑P p=1 |αj,p| ≤ B′2, the excess risk of the\nrecursive kernel method is at most O(LCσ(B′1)B′2 √ Kmax/n), where Kmax = maxz:‖z‖2≤1K(z, z) is the maximal value of the kernel function. Plugging in the norm constraints of the function class Fcnn, we have B′1 = B1 and B′2 = B2r √ P . Thus, the expected risk of the estimated f̂ is bounded by:\nEX,Y [L(f̂(X);Y )] ≤ inf f∈Fcnn\nEX,Y [L(f(X);Y )] + c LCσ(B1)B2r √ PKmax√\nn . (18)\nComparing this bound to Theorem 1, we see that (apart from the logarithmic terms) they differ in the multiplicative factors of √ P Kmax versus √ E[‖K(X)‖2]. Since the matrix K(X) is P -dimensional, we have\n‖K(X)‖2 ≤ max p∈[P ] ∑ q∈[P ] |K(zp(X), zq(X))| ≤ P Kmax.\nThis demonstrates that √ P Kmax is always greater than √ E[‖K(X)‖2]. In general, the first term\ncan be up to factor of √ P times greater, which implies that the sample complexity of the recursive kernel method is up to P times greater than that of the CCNN. This difference corresponds to the fact that the recursive kernel method learns a model with P times more parameters. Although comparing the upper bounds doesn’t rigorously show that one method is better than the other, it gives the right intuition for understanding the importance of parameter sharing."
    }, {
      "heading" : "4 Learning multi-layer CCNNs",
      "text" : "In this section, we describe a heuristic method for learning CNNs with more layers. The idea is to estimate the parameters of the convolutional layers incrementally from bottom to the top. Before presenting the multi-layer algorithm, we present two extensions, average pooling and multi-channel inputs.\nAverage pooling. Average pooling is a technique to reduce the output dimension of the convolutional layer from dimensions P × r to dimensions P ′ × r with P ′ < P . Suppose that the filter hj applied to all the patch vectors produces the output vector Hj(x) := (hj(z1(x)), · · · , hj(zP (x))) ∈ RP×r. Average pooling produces a P ′ × r matrix, where each row is the average of the rows corresponding to a small subset of the P patches. For example, we might average every pair of adjacent patches, which would produce P ′ = P/2 rows. The operation of average pooling can be represented via left-multiplication using a fixed matrix G ∈ RP ′×P .\nAlgorithm 2: Learning multi-layer CCNNs\nInput:Data {(xi, yi)}ni=1, kernel function K, number of layers m, regularization parameters R1, . . . , Rm, number of filters r1, . . . , rm. Define H1(x) = x. For each layer s = 2, . . . ,m:\n• Train a two-layer network by Algorithm 1, taking {(Hs−1(xi), yi)}ni=1 as training examples and Rs, rs as parameters. Let Hs be the output of the convolutional layer and f̂s be the predictor.\nOutput: Predictor f̂m and the top convolutional layer output Hm.\nFor the CCNN model, if we apply average pooling after the convolutional layer, then the kth output of the CCNN model becomes tr(GZ(x)Ak) where Ak ∈ Rm×P ′ is the new (smaller) parameter matrix. Thus, performing a pooling operation requires only replacing every matrix Z(xi) in problem (12) by the pooled matrix GZ(xi). Note that the linearity of the CCNN allows us to effectively pool before convolution, even though for the CNN, pooling must be done after applying the nonlinear filters. The resulting ERM problem is still convex, and the number of parameters have been reduced by P/P ′-fold. Although average pooling is straightforward to incorporate in our framework, unfortunately, max pooling does not fit into our framework due to its nonlinearity.\nProcessing multi-channel inputs. If our input has C channels (corresponding to RGB colors, for example), then the input becomes a matrix x ∈ RC×d0 . The c-th row of matrix x, denoted by x[c] ∈ Rd0 , is a vector representing the c-th channel. We define the multi-channel patch vector as a concatenation of patch vectors for each channel:\nzp(x) := (zp(x[1]), . . . , zp(x[C])) ∈ RCd1 .\nThen we construct the feature matrix Z(x) using the concatenated patch vectors {zp(x)}Pp=1. From here, everything else of Algorithm 1 remains the same. We note that this approach learns a convex relaxation of filters taking the form σ( ∑C c=1〈wc, zp(x[c])〉), parametrized by the vectors {wc}Cc=1.\nMulti-layer CCNN. Given these extensions, we are ready to present the algorithm for learning multi-layer CCNNs. The algorithm is summarized in Algorithm 2. For each layer s, we call Algorithm 1 using the output of previous convolutional layers as input—note that this consists of r channels (one from each previous filter) and thus we must use the multi-channel extension. Algorithm 2 outputs a new convolutional layer along with a prediction function, which is kept only at the last layer. We optionally use averaging pooling after each successive layer. to reduce the output dimension of the convolutional layers."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we compare the CCNN approach with other methods. The results are reported on the MNIST dataset and its variations for digit recognition, and on the CIFAR-10 dataset for object classification."
    }, {
      "heading" : "5.1 MNIST and variations",
      "text" : "Since the basic MNIST digits are relatively easy to classify, we also consider more challenging variations [43]. These variations are known to be hard for methods without a convolution mechanism (for instance, see the paper [44]). Figure 3 shows a number of sample images from these different datasets. All the images are of size 28 × 28. For all datasets, we use 10,000 images for training, 2,000 images for validation and 50,000 images for testing. This 10k/2k/50k partitioning is standard for MNIST variations [43].\nImplementation details. For the CCNN method and the baseline CNN method, we train twolayer and three-layer models respectively. The models with k convolutional layers are denoted by CCNN-k and CNN-k. Each convolutional layer is constructed on 5 × 5 patches with unit stride, followed by 2× 2 average pooling. The first and the second convolutional layers contains 16 and 32 filters, respectively. The loss function is chosen as the 10-class logistic loss. We use the Gaussian kernel K(z, z′) = exp(−γ‖z − z′‖22) and set hyperparameters γ = 0.2 for the first convolutional layer and γ = 2 for the second. The feature matrix Z(x) is constructed via random feature approximation [33] with dimension m = 500 for the first convolutional layer and m = 1000 for the second. Before training each CCNN layer, we preprocess the input vectors zp(xi) using local contrast normalization and ZCA whitening [12]. The convex optimization problem is solved by projected SGD with mini-batches of size 50.\nAs a baseline approach, the CNN models are activated by the ReLU function σ(t) = max{0, t} or the quadratic function σ(t) = t2. We train them using mini-batch SGD. The input images are preprocessed by global contrast normalization and ZCA whitening [see, e.g. 40]. We compare our method against several alternative baselines. The CCNN-1 model is compared against an SVM with the Gaussian RBF kernel (SVMrbf ) and a fully connected neural network with one hidden layer (NN-1). The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].\nResults. Table 1 shows the classification errors on the test set. The models are grouped with respect to the number of layers that they contain. For models with one convolutional layer, the\nerrors of CNN-1 are significantly lower than that of NN-1, highlighting the benefits of parameter sharing. The CCNN-1 model outperforms CNN-1 on all datasets. For models with two or more hidden layers, the CCNN-2 model outperforms CNN-2 on all datasets, and is competitive against the state-of-the-art. In particular, it achieves the best accuracy on the rand, img and img+rot dataset, and is comparable to the state-of-the-art on the remaining two datasets.\nIn order to understand the key factors that affect the training of CCNN filters, we evaluate five variants:\n(1) replace the Gaussian kernel by a linear kernel; (2) remove the ZCA whitening in preprocessing; (3) use fewer random features (m = 200 rather than m = 500) to approximate the kernel matrix; (4) regularize the parameter matrix by the Frobenius norm instead of the nuclear norm; (5) stop the mini-batch SGD early before it converges.\nWe evaluate the obtained filters by training a second convolutional layer on top of them, then evaluating the classification error on the hardest dataset img+rot. As Table 2 shows, switching to the linear kernel or removing the ZCA whitening significantly degenerates the performance. This is because that both variants equivalently modify the kernel function. Decreasing the number of random features also has a non-negligible effect, as it makes the kernel approximation less accurate. These observations highlight the impact of the kernel function. Interestingly, replacing the nuclear\nnorm by a Frobenius norm or stopping the algorithm early doesn’t hurt the performance. To see their impact on the parameter matrix, we compute the effective rank (ratio between the nuclear norm and the spectral norm, see [18]) of matrix Â. The effective rank obtained by the last two variants are equal to 77 and 24, greater than that of the original CCNN (equal to 12). It reveals that the last two variants have damaged the algorithm’s capability of enforcing a low-rank solution. However, the CCNN filters are retrieved from the top-r singular vectors of the parameter matrix, hence the performance will remain stable as long as the top singular vectors are robust to the variation of the matrix.\nIn Section 3.4, we showed that if the activation function is a polynomial function, then the CCNN requires lower sample complexity to match the performance of the best possible CNN. More precisely, if the activation function is degree-` polynomial, then Cσ(B) in the upper bound will be controlled by O(B`). This motivates us to study the performance of low-degree polynomial activations. Table 1 shows that the CNN-2 model with a quadratic activation function achieves error rates comparable to that with a ReLU activation: CNN-2 (Quad) outperforms CNN-2 (ReLU) on the basic and rand datasets, and is only slightly worse on the rot and img dataset. Since the performance of CCNN matches that of the best possible CNN, the good performance of the quadratic activation in part explains why the CCNN is also good."
    }, {
      "heading" : "5.2 CIFAR-10",
      "text" : "In order to test the capability of CCNN in complex classification tasks, we report its performance on the CIFAR-10 dataset [24]. The dataset consists of 60000 images divided into 10 classes. Each image has 32×32 pixels in RGB colors. We use 50k images for training and 10k images for testing.\nImplementation details. We train CNN and CCNN models with two, three, and four layers Each convolutional layer is constructed on 5× 5 patches with unit stride, followed by 3× 3 average pooling with two-pixel stride. We train 32, 32, 64 filters for the three convolutional layers from bottom to the top. For any s× s input, zero pixels are padded on its borders so that the input size becomes (s+4)× (s+4), and the output size of the convolutional layer is (s/2)× (s/2). The CNNs are activated by the ReLU function. For CCNNs, we use the Gaussian kernel with hyperparameter γ = 1, 2, 2 (for the three convolutional layers). The feature matrix Z(x) is constructed via random feature approximation with dimension m = 2000. The preprocessing steps are the same as in the MNIST experiments. It was known that the generalization performance of the CNN can be improved by training on random crops of the original image [25], so we train the CNN on random 24×24 patches of the image, and test on the central 24×24 patch. We also apply random cropping to training the the first and the second layer of the CCNN.\nWe compare the CCNN against other baseline methods that don’t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].\nResults. We report the classification errors on in Table 3. For models of all depths, the CCNN model outperforms the CNN model. The CCNN-2 and the CCNN-3 model also outperform the three baseline methods. The advantage of the CCNN is substantial for learning two-layer networks, when the optimality guarantee of CCNN holds. The performance improves as more layers are stacked, but as we observe in Table 3, the marginal gain of CCNN diminishes as the network grows deeper. We suspect that this is due to the greedy fashion in which the CCNN layers are constructed. Once\ntrained, the low-level filters of the CCNN are no longer able to adapt to the final classifier. In contrast, the low-level filters of the CNN model are continuously adjusted via backpropagation.\nIt is worth noting that the performance of the CNN can be further improved by adding more layers, switching from average pooling to max pooling, and being regularized by local response normalization and dropout (see, e.g. [25]). The figures in Table 3 are by no means the state-ofthe-art result on CIFAR-10. However, it does demonstrate that the convex relaxation is capable of improving the performance of convolutional neural networks. For future work, we propose to study a better way for convexifying deep CNNs.\nIn Figure 4, we compare the computational efficiency of CNN-3 to its convexified version CCNN3. Both models are trained by mini-batch SGD (with batchsize equal to 50) on a single processor. We optimized the choice of step-size for each algorithm. From the plot, it is easy to identify the three stages of the CCNN-3 curve for training the three convolutional layers from bottom to the top. We also observe that the CCNN converges faster than the CNN. More precisely, the CCNN takes half the runtime of the CNN to reach an error rate of 28%, and one-fifth of the runtime to reach an error rate of 23%. The per-iteration cost for training the first layer of CCNN is about 109% of the per-iteration cost of CNN, but the per-iteration cost for training the remaining two layers are about 18% and 7% of that of CNN. Thus, training CCNN scales well to large datasets.\nTraining a CCNN on top of a CNN. Instead of training a CCNN from scratch, we can also train CCNNs on top of existing CNN layers. More concretely, once a CNN-k model is obtained, we train a two-layer CCNN by taking the (k− 1)-th hidden layer of CNN as input. This approach preserves the low-level features learned by CNN, only convexifying its top convolutional layer. The underlying motivation is that the traditional CNN is good at learning low-level features through\nbackpropagation, while the CCNN is optimal in learning two-layer networks. In this experiment, we convexify the top convolutional layer of CNN-2 and CNN-3 using the CCNN approach, with a smaller Gaussian kernel parameter (i.e. γ = 0.1) and keeping other hyperparameters the same as in the training of CCNN-2 and CCNN-3. The results are shown in Table 4. The convexified CNN achieves better accuracy on all network depths. It is worth noting that the time for training a convexified layer is only a small fraction of the time for training the original CNN."
    }, {
      "heading" : "6 Related work",
      "text" : "With the empirical success of deep neural networks, there has been an increasing interest in theoretical understanding. Bengio et al. [5] showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters. This perspective encourages incrementally adding neurons to the network, whose generalization error was studied by Bach [3]. Zhang et al. [47] propose a polynomial-time ensemble method for learning fully-connected neural networks, but their approach handles neither parameter sharing nor the convolutional setting. Other relevant works for learning fully-connected networks include [35, 23, 29]. Aslan et al. [1, 2] propose a method for learning multi-layer latent variable models. They showed that for certain activation functions, the proposed method is a convex relaxation for learning the fully-connected neural network.\nAnother line of work is devoted to understanding the energy landscape of a neural network. Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11]. If this property holds, then gradient descent can find a solution that is “good enough”. Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20]. However, these results are not applicable to a CNN, since the underlying assumptions are not satisfied by CNNs.\nPast work has studied learning translation invariant features without backpropagation. Mairal et al. [30] present convolutional kernel networks. They propose a translation-invariant kernel whose feature mapping can be approximated by a composition of the convolution, non-linearity and pooling operators, obtained through unsupervised learning. However, this method is not equipped with the optimality guarantees that we have provided for CCNNs in this paper, even for learning one convolution layer. The ScatNet method [8] uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, unlike the analysis in this paper. Daniely et al. [13] show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from a random initialization."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we have shown how convex optimization can be used to efficiently optimize CNNs as well as understand them statistically. Our convex relaxation consists of two parts: the nuclear norm relaxation for handling parameter sharing, and the RKHS relaxation for handling non-linearity. For the two-layer CCNN, we proved that its generalization error converges to that of the best possible two-layer CNN. We handled multi-layer CCNNs only heuristically, but observed that adding more\nlayers improves the performance in practice. On real data experiments, we demonstrated that CCNN outperforms the traditional CNN of the same depth, is computationally efficient, and can be combined with the traditional CNN to achieve better performance. A major open problem is to formally study the convex relaxation of deep CNNs."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was partially supported by Office of Naval Research MURI grant DOD-002888, Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, Office of Naval Research grant ONR-N00014, National Science Foundation Grant CIF-31712-23800, as well as a Microsoft Faculty Research Award to the second author.\nA Inverse polynomial kernel and Gaussian kernel\nIn this appendix, we describe the properties of the two types of kernels — the inverse polynomial kernel (14) and the Gaussian RBF kernel (15). We prove that the associated reproducing kernel Hilbert Spaces (RKHS) of these kernels contain filters taking the form h : z 7→ σ(〈w, z〉) for particular activation functions σ.\nA.1 Inverse polynomial kernel\nWe first verify that the function (14) is a kernel function. This holds since that we can find a mapping ϕ : Rd1 → `2(N) such that K(z, z′) = 〈ϕ(z), ϕ(z′)〉. We use zi to represent the i-th coordinate of an infinite-dimensional vector z. The (k1, . . . , kj)-th coordinate of ϕ(z), where j ∈ N and k1, . . . , kj ∈ [d1], is defined as 2− j+1 2 xk1 . . . xkj . By this definition, we have\n〈ϕ(x), ϕ(y)〉 = ∞∑ j=0 2−(j+1) ∑ (k1,...,kj)∈[d1]j zk1 . . . zkjz ′ k1 . . . z ′ kj . (19)\nSince ‖z‖2 ≤ 1 and ‖z′‖2 ≤ 1, the series on the right-hand side is absolutely convergent. The inner term on the right-hand side of equation (19) can be simplified to∑\n(k1,...,kj)∈[d1]j zk1 . . . zkjz\n′ k1 . . . z ′ kj = (〈z, z′〉)j . (20)\nCombining equations (19) and (20) and using the fact that |〈z, z′〉| ≤ 1, we have\n〈ϕ(z), ϕ(z′)〉 = ∞∑ j=0 2−(j+1)(〈z, z′〉)j (i)= 1 2− 〈z, z′〉 = K(z, z′),\nwhich verifies that K is a kernel function and ϕ is the associated feature map. Next, we prove that the associated RKHS contains the class of nonlinear filters. The lemma was proved by Zhang et al. [48]. We include the proof to make the paper self-contained.\nLemma 1. Assume that the function σ(x) has a polynomial expansion σ(t) = ∑∞\nj=0 ajt j. Let Cσ(λ) := √∑∞ j=0 2 j+1a2jλ\n2j. If Cσ(‖w‖2) <∞, then the RKHS induced by the inverse polynomial kernel contains function h : z 7→ σ(〈w, z〉) with Hilbert norm ‖h‖H = Cσ(‖w‖2).\nProof. Let ϕ be the feature map that we have defined for the polynomial inverse kernel. We define vector w ∈ `2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j ∈ N and k1, . . . , kj ∈ [d1], is equal to 2\nj+1 2 ajwk1 . . . wkj . By this definition, we have\nσ(〈w, z〉) = ∞∑ t=0 aj(〈w, z〉)j = ∞∑ j=0 aj ∑ (k1,...,kj)∈[d1]j wk1 . . . wkjzk1 . . . zkj = 〈w,ϕ(z)〉, (21)\nwhere the first equation holds since σ(x) has a polynomial expansion σ(x) = ∑∞\nj=0 ajx j , the second\nby expanding the inner product, and the third by definition of w and ϕ(z). The `2-norm of w is equal to:\n‖w‖22 = ∞∑ j=0 2j+1a2j ∑ (k1,...,kj)∈[d1]j w2k1w 2 k2 · · ·w 2 kj = ∞∑ j=0 2j+1a2j‖w‖ 2j 2 = C 2 σ(‖w‖2) <∞. (22)\nBy the basic property of the RKHS, the Hilbert norm of h is equal to the `2-norm of w. Combining equations (21) and (22), we conclude that h ∈ H and ‖h‖H = ‖w‖2 = Cσ(‖w‖2).\nAccording to Lemma 1, it suffices to upper bound Cσ(λ) for a particular activation function σ. To make Cσ(λ) < ∞, the coefficients {aj}∞j=0 must quickly converge to zero, meaning that the activation function must be sufficiently smooth. For polynomial functions of degree `, the definition of Cσ implies that Cσ(λ) = O(λ`). For the sinusoid activation σ(t) := sin(t), we have\nCσ(λ) = √√√√ ∞∑ j=0\n22j+2\n((2j + 1)!)2 · (λ2)2j+1 ≤ 2eλ2 .\nFor the erf function and the smoothed hinge loss function defined in Section 3.4, Zhang et al. [48, Proposition 1] proved that Cσ(λ) = O(ecλ 2 ) for universal numerical constant c > 0.\nA.2 Gaussian kernel\nThe Gaussian kernel also induces an RKHS that contains a particular class of nonlinear filters. The proof is similar to that of Lemma 1.\nLemma 2. Assume that the function σ(x) has a polynomial expansion σ(t) = ∑∞\nj=0 ajt j. Let Cσ(λ) := √∑∞ j=0 j!e2γ (2γ)j a2jλ\n2j. If Cσ(‖w‖2) < ∞, then the RKHS induced by the Gaussian kernel contains the function h : z 7→ σ(〈w, z〉) with Hilbert norm ‖h‖H = Cσ(‖w‖2).\nProof. When ‖z‖2 = ‖z′‖2 = 1, It is well-known [see, e.g. 41] the following mapping ϕ : Rd1 → `2(N) is a feature map for the Gaussian RBF kernel: the (k1, . . . , kj)-th coordinate of ϕ(z), where j ∈ N and k1, . . . , kj ∈ [d1], is defined as e−γ((2γ)j/j!)1/2xk1 . . . xkj . Similar to equation (21), we define a vector w ∈ `2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j ∈ N and k1, . . . , kj ∈ [d1], is equal to eγ((2γ)j/j!)−1/2ajwk1 . . . wkj . By this definition, we have\nσ(〈w, z〉) = ∞∑ t=0 aj(〈w, z〉)j = ∞∑ j=0 aj ∑ (k1,...,kj)∈[d1]j wk1 . . . wkjzk1 . . . zkj = 〈w,ϕ(z)〉. (23)\nThe `2-norm of w is equal to:\n‖w‖22 = ∞∑ j=0 j!e2γ (2γ)j a2j ∑ (k1,...,kj)∈[d1]j w2k1w 2 k2 · · ·w 2 kj = ∞∑ j=0 j!e2γ (2γ)j a2j‖w‖ 2j 2 = C 2 σ(‖w‖2) <∞. (24)\nCombining equations (21) and (22), we conclude that h ∈ H and ‖h‖H = ‖w‖2 = Cσ(‖w‖2).\nComparing Lemma 1 and Lemma 2, we find that the Gaussian kernel imposes a stronger condition on the smoothness of the activation function. For polynomial functions of degree `, we still have Cσ(λ) = O(λ`). For the sinusoid activation σ(t) := sin(t), it can be verified that\nCσ(λ) = √√√√e2γ ∞∑ j=0\n1 (2j + 1)! · (λ2 2γ )2j+1 ≤ eλ2/(4γ)+γ .\nHowever, the value of Cσ(λ) is infinite when σ is the erf function or the smoothed hinge loss, meaning that the Gaussian kernel’s RKHS doesn’t contain filters activated by these two functions."
    }, {
      "heading" : "B Convex relaxation for nonlinear activation",
      "text" : "In this appendix, we provide a detailed derivation of the relaxation for nonlinear activation functions that we previously sketched in Section 3.2. Recall that the filter output is σ(〈wj , z〉). Appendix A shows that given a sufficiently smooth activation function σ, we can find some kernel function K : Rd1×Rd1 → R and a feature map ϕ : Rd1 → `2(N) satisfying K(z, z′) ≡ 〈ϕ(z), ϕ(z′)〉, such that\nσ(〈wj , z〉) ≡ 〈wj , ϕ(z)〉. (25)\nHere wj ∈ `2(N) is a countable-dimensional vector and ϕ := (ϕ1, ϕ2, . . . ) is a countable sequence of functions. Moreover, the `2-norm of wj is bounded as ‖wj‖2 ≤ Cσ(‖wj‖2) for a monotonically increasing function Cσ that depends on the kernel (see Lemma 1 and Lemma 2). As a consequence, we may use ϕ(z) as the vectorized representation of the patch z, and use wj as the linear transformation weights, then the problem is reduced to training a CNN with the identity activation function.\nThe filter is parametrized by an infinite-dimensional vector wj . Our next step is to reduce the original ERM problem to a finite-dimensional one. In order to minimize the empirical risk, one only needs to concern the output on the training data, that is, the output of 〈wj , ϕ(zp(xi))〉 for all (i, p) ∈ [n] × [P ]. Let T be the orthogonal projector onto the linear subspace spanned by the vectors {ϕ(zp(xi)) : (i, p) ∈ [n]× [P ]}. Then we have\n∀ (i, p) ∈ [n]× [P ] : 〈wj , ϕ(zp(xi))〉 = 〈wj , Tϕ(zp(xi))〉 = 〈Twj , ϕ(zp(xi))〉.\nThe last equation follows since the orthogonal projector T is self-adjoint. Thus, for empirical risk minimization, we can without loss of generality assume that wj belongs to the linear subspace spanned by {ϕ(zp(xi)) : (i, p) ∈ [n]× [P ]} and reparametrize it by:\nwj = ∑\n(i,p)∈[n]×[P ]\nβj,(i,p)ϕ(zp(xi)). (26)\nLet βj ∈ RnP be a vector whose whose (i, p)-th coordinate is βj,(i,p). In order to estimate wj , it suffices to estimate the vector βj . By definition, the vector satisfies the relation β > j Kβj = ‖wj‖22, where K is the nP × nP kernel matrix defined in Section 3.2. As a consequence, if we can find a matrix Q such that QQ> = K, then we have the norm constraint\n‖Q>βj‖2 = √ β>j Kβj = ‖wj‖2 ≤ Cσ(‖wj‖2) ≤ Cσ(B). (27)\nLet v(z) ∈ RnP be a vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then by equations (25) and (26), the filter output can be written as\nσ ( 〈wj , z〉 ) ≡ 〈wj , ϕ(z)〉 ≡ 〈βj , v(z)〉. (28)\nFor any patch zp(xi) in the training data, the vector v(zp(xi)) belongs to the column space of the kernel matrix K. Therefore, letting Q† represent the pseudo-inverse of matrix Q, we have\n∀ (i, p) ∈ [n]× [P ] : 〈βj , v(zp(xi))〉 = β>j QQ†v(zp(xi)) = 〈(Q>)†Q>βj , v(zp(xi))〉.\nIt means that if we replace the vector βj on the right-hand side of equation (28) by the vector (Q>)†Q>βj , then it won’t change the empirical risk. Thus, for ERM we can parametrize the filters by\nhj(z) := 〈(Q>)†Q>βj , v(z)〉 = 〈Q†v(z), Q>βj〉. (29)\nLet Z(x) be an P × nP matrix whose p-th row is equal to Q†v(zp(x)). Similar to the steps in equation (6), we have\nfk(x) = r∑ j=1 α>k,jZ(x)K 1/2βj = tr ( Z(x) ( r∑ j=1 K1/2βjα > k,j )) = tr(Z(x)Ak),\nwhere Ak := ∑r j=1Q >βjα > k,j . If we let A := (A1, . . . , Ad2) denote the concatenation of these matrices, then this larger matrix satisfies the constraints:\nConstraint (C1): max j∈[r] ‖Q>βj‖2 ≤ Cσ(B1) and max (k,j)∈[d2]×[r] ‖αk,j‖2 ≤ B2.\nConstraint (C2): The matrix A has rank at most r.\nWe relax these two constraints to the nuclear norm constraint: ‖A‖∗ ≤ Cσ(B1)B2r √ d2. (30)\nBy comparing constraints (8) and (30), we see that the only difference is that the term B1 in the norm bound has been replaced by Cσ(B1). This change is needed because we have used the kernel trick to handle nonlinear activation functions."
    }, {
      "heading" : "C Proof of Theorem 1",
      "text" : "Since the output is one-dimensional in this case, we can adopt the simplified notation (A,αj,p) for the matrix (A1, α1,j,p). Letting H be the RKHS associated with the kernel function K, and letting ‖·‖H be the associated Hilbert norm, consider the function class\nFccnn := { x 7→ r∗∑ j=1 P∑ p=1 αj,phj(zp(x)) : r ∗ <∞ and r∗∑ j=1 ‖αj‖2‖hj‖H ≤ Cσ(B1)B2d2 } . (31)\nHere αj,p denotes the p-th entry of vector αj ∈ RP , whereas the quantity Cσ(B1) only depends on B1 and the activation function σ. The following lemma shows that the function class Fccnn is rich enough so that it contains family of CNN predictors as a subset. The reader should recall the notion of a valid activation function, as defined prior to the statement of Theorem 1.\nLemma 3. For any valid activation function σ, there is a quantity Cσ(B1), depending only on B1 and σ, such that Fcnn ⊂ Fccnn.\nSee Appendix C.1 for the proof.\nNext, we connect the function class Fccnn to the CCNN algorithm. Recall that f̂ccnn is the predictor trained by the CCNN algorithm. The following lemma shows that f̂ccnn is an empirical risk minimizer within Fccnn.\nLemma 4. With the CCNN hyper-parameter R = Cσ(B1)B2d2, the predictor f̂ccnn is guaranteed to satisfy the inclusion\nf̂ccnn ∈ arg min f∈Fccnn n∑ i=1 L(f(xi); yi).\nSee Appendix C.2 for the proof.\nOur third lemma shows that the function class Fccnn is not “too big”, which we do by upper bounding its Rademacher complexity. The Rademacher complexity of a function class F = {f : X → R} with respect to n i.i.d. samples {Xi}ni=1 is given by\nRn(F) := EX, [ sup f∈F 1 n n∑ i=1 if(Xi) ] ,\nwhere { i}ni=1 are an i.i.d. sequence of uniform {−1,+1}-valued variables. Rademacher complexity plays an important role in empirical process theory, and in particular can be used to bound the generalization loss of our empirical risk minimization problem. We refer the reader to Bartlett and Mendelson [4] for an introduction to the theoretical properties of Rademacher complexity.\nThe following lemma involves the kernel matrix K(x) ∈ RP×P whose (i, j)-th entry is equal to K(zi(x), zj(x)), as well as the expectation E[‖K(X)‖2] of the spectral norm of this matrix when X is drawn randomly.\nLemma 5. There is a universal constant c such that\nRn(Fccnn) ≤ c Cσ(B1)B2r √ log(nP )E[‖K(X)‖2]√ n . (32)\nSee Appendix C.3 for the proof of this claim.\nCombining Lemmas 3 through 5 allows us to compare the CCNN predictor f̂ccnn against the best model in the CNN class. Lemma 4 shows that f̂ccnn is the empirical risk minimizer within function class Fccnn. Thus, the theory of Rademacher complexity [4] guarantees that\nE[L(Fccnn(X);Y )] ≤ inf f∈Fccnn E[L(f(x); y)] + 2L · Rn(Fccnn) + c√ n , (33)\nwhere c is a universal constant. By Lemma 3, we have\ninf f∈Fccnn E[L(f(X);Y )] ≤ inf f∈Fcnn E[L(f(X);Y )].\nPlugging this upper bound into inequality (33) and applying Lemma 5 completes the proof.\nC.1 Proof of Lemma 3\nWith the activation functions specified in the lemma statement, Lemma 1 and Lemma 2 show that there is a quantity Cσ(B1), such any filter of CNN belongs to the reproducing kernel Hilbert space H and its Hilbert norm is bounded by Cσ(B1). As a consequence, any function f ∈ Fcnn can be represented by\nf(x) := r∑ j=1 P∑ p=1 αj,phj(zp(x)) where ‖hj‖H ≤ Cσ(B1) and ‖αj‖2 ≤ B2.\nIt is straightforward to verify that function f satisfies the constraint in equation (31), and consequently belongs to Fccnn.\nC.2 Proof of Lemma 4 Let CR denote the function class { x 7→ tr(Z(x)A) : ‖A‖∗ ≤ R } . We first prove that CR ⊂ Fccnn. Consider an arbitrary function fA(x) : = tr(Z(x)A) belonging to CR. Note that the matrix A has a singular value decomposition (SVD) A = ∑r∗ j=1 λjwju > j for some r\n∗ < ∞, where wj and uj are unit vectors and λj are real numbers. Using this notation, the function fA can be represented as the sum\nfA(x) = r∗∑ j=1 λju > j Z(x)wj .\nLet v(z) be an nP -dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then Q†v(zp(x)) is the p-th row of matrix Z(x). Letting hj denote the mapping z 7→ 〈Q†v(z), wj〉, we have\nfA(x) = r∗∑ j=1 P∑ p=1 λjuj,phj(zp(x)).\nThe function hj can also be written as z 7→ 〈(Q>)†wj , v(z)〉. Equation (27) implies that the Hilbert norm of this function is equal to ‖Q>(Q>)†wj‖2, which is bounded by ‖wj‖2 = 1. Thus we have\nr∗∑ j=1 ‖λjuj‖2‖hj‖H = r∗∑ j=1 |λj | = ‖A‖∗ ≤ Cσ(B1)B2r,\nwhich implies that fA ∈ Fccnn. Next, it suffices to prove that for some empirical risk minimizer f in function class Fccnn, it also belongs to the function class CR. Recall that any function f ∈ Fccnn can be represented in the form\nf(x) = r∗∑ j=1 P∑ p=1 αj,phj(zp(x)).\nwhere the filter hj belongs to the RKHS. Let ϕ : Rd1 → `2(N) be a feature map of the kernel function. The function hj(z) can be represented by 〈w, ϕ(z)〉 for some w ∈ `2(N). In Appendix B, we have shown that any function taking this form can be replaced by 〈(Q>)†Q>βj , v(z)〉 for some vector βj ∈ RnP without changing the output on the training data. Thus, there exists at least one empirical risk minimizer f of Fccnn such that all of its filters take the form\nhj(z) = 〈(Q>)†Q>βj , v(z)〉. (34)\nBy equation (27), the Hilbert norm of these filters satisfy:\n‖hj‖H = ‖Q>(Q>)†Q>βj‖2 = ‖Q>βj‖2.\nAccording to Appendix B, if all filters take the form (34), then the function f can be represented by tr(Z(x)A) for matrix A := ∑r∗ j=1Q >βjα > j . Consequently, the nuclear norm is bounded as\n‖A‖∗ ≤ r∗∑ j=1 ‖αj‖2‖Q>βj‖2 = r∗∑ j=1 ‖αj‖2‖hj‖H ≤ Cσ(B1)B2r = R,\nwhich establishes that the function f belongs to the function class CR.\nC.3 Proof of Lemma 5\nThroughout this proof, we use the shorthand notation R := Cσ(B1)B2r. Recall that any function f ∈ Fccnn can be represented in the form\nf(x) = r∗∑ j=1 P∑ p=1 αj,phj(zp(x)) where hj ∈ H, (35)\nand the Hilbert space H is induced by the kernel function K. Since any patch z belongs to the compact space {z : ‖z‖2 ≤ 1} and K is a continuous kernel satisfying K(z, z) ≤ 1, Mercer’s theorem [41, Theorem 4.49] implies that there is a feature map ϕ : Rd1 → `2(N) such that ∑∞ `=1 ϕ`(z)ϕ`(z\n′) converges uniformly and absolutely to K(z, z′). Thus, we can write K(z, z′) = 〈ϕ(z), ϕ(z′)〉. Since ϕ is a feature map, every any function h ∈ H can be written as h(z) = 〈β, ϕ(z)〉 for some β ∈ `2(N), and the Hilbert norm of h is equal to ‖β‖2.\nUsing this notation, we can write the filter hj in equation (35) as hj(z) = 〈βj , ϕ(z)〉 for some vector βj ∈ `2(N), with Hilbert norm ‖hj‖H = ‖βj‖2. For each x, let Ψ(x) denote the linear operator that maps any sequence θ ∈ `2(N) to the vector in RP with elements[\n〈θ, ϕ(z1(x))〉 . . . 〈θ, ϕ(zP (x))〉 ]T .\nInformally, we can think of Ψ(x) as a matrix whose p-th row is equal to ϕ(zp(x)). The function f can then be written as\nf(xi) = r∗∑ j=1 α>j Ψ(xi)βj = tr Ψ(xi)( r∗∑ j=1 βjα > j ) . (36) The matrix ∑r∗ j=1 βjα\n> j satisfies the constraint∥∥∥∥∥∥ r∗∑ j=1 βjα > j ∥∥∥∥∥∥ ∗ ≤ r∗∑ j=1 ‖αj‖2 · ‖βj‖2 = r∗∑ j=1 ‖αj‖2 · ‖hj‖H ≤ R. (37)\nCombining equation (36) and inequality (37), we find that the Rademacher complexity is bounded by\nRn(Fccnn) = 1\nn E\n[ sup\nf∈Fccnn n∑ i=1 if(xi) ] ≤ 1 n E [ sup A:‖A‖∗≤R tr (( n∑ i=1 iΨ(xi) ) A )]\n= R\nn E [∥∥∥∥∥ n∑ i=1 iΨ(xi) ∥∥∥∥∥ 2 ] , (38)\nwhere the last equality uses Hölder’s inequality—that is, the duality between the nuclear norm and the spectral norm.\nAs noted previously, we may think informally of the quantity ∑n\ni=1 iΨ(xi) as a matrix with P rows and infinitely many columns. Let Ψ(d)(xi) denote the submatrix consisting of the first d columns of Ψ(xi) and let Ψ (−d)(xi) denote the remaining sub-matrix. We have\nE [∥∥∥∥∥ n∑ i=1 iΨ(xi) ∥∥∥∥∥ 2 ] ≤ E [∥∥∥∥∥ n∑ i=1 iΨ (d)(xi) ∥∥∥∥∥ 2 ] + E ∥∥∥∥∥ n∑ i=1 iΨ (−d)(xi) ∥∥∥∥∥ 2 F 1/2\n≤ E [∥∥∥∥∥ n∑ i=1 iΨ (d)(xi) ∥∥∥∥∥ 2 ] + ( nP · E [ ∞∑ `=d+1 ϕ2` (z) ])1/2 .\nSince ∑∞\n`=1 ϕ 2 ` (z) uniformly converges to K(z, z), the second term on the right-hand side converges\nto zero as d → ∞. Thus it suffices to bound the first term and take the limit. In order to upper bound the spectral norm ∥∥∑n i=1 iΨ (d)(xi) ∥∥ 2 , we use a matrix Bernstein inequality due to Minsker [31, Theorem 2.1]. In particular, whenever tr(Ψ(d)(xi)(Ψ (d)(xi))\n>) ≤ C1, there is a universal constant c such that the expected spectral norm is upper bounded as\nE [∥∥∥∥∥ n∑ i=1 iΨ (d)(xi) ∥∥∥∥∥ 2 ] ≤ c √ log(nC1)E [( n∑ i=1 ‖Ψ(d)(xi)(Ψ(d)(xi))>‖2 )1/2]\n≤ c √ log(nC1) ( nE[‖Ψ(X)Ψ>(X)‖2] )1/2 .\nNote that the uniform kernel expansion K(z, z′) = ∑∞\n`=1 ϕ`(z)ϕ`(z ′) implies the trace norm bound\ntr(Ψ(d)(xi)(Ψ (d)(xi)) >) ≤ tr(K(xi)). Since all patches are contained in the unit `2-ball, the kernel function K is uniformly bounded by 1, and hence C1 ≤ P . Taking the limit d→∞, we find that\nE [∥∥∥∥∥ n∑ i=1 iΨ(xi) ∥∥∥∥∥ 2 ] ≤ c √ log(nP ) ( nE[‖K(X)‖2] )1/2 .\nFinally, substituting this upper bound into inequality (38) yields the claimed bound (32)."
    } ],
    "references" : [ {
      "title" : "Convex two-layer modeling",
      "author" : [ "Ö. Aslan", "H. Cheng", "X. Zhang", "D. Schuurmans" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Convex deep learning via normalized kernels",
      "author" : [ "Ö. Aslan", "X. Zhang", "D. Schuurmans" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Breaking the curse of dimensionality with convex neural networks",
      "author" : [ "F. Bach" ],
      "venue" : "arXiv preprint arXiv:1412.8690,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Convex neural networks",
      "author" : [ "Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Training a 3-node neural network is NP-complete",
      "author" : [ "A.L. Blum", "R.L. Rivest" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1992
    }, {
      "title" : "Online learning and stochastic approximations",
      "author" : [ "L. Bottou" ],
      "venue" : "On-line learning in neural networks,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Pcanet: A simple deep learning baseline for image classification",
      "author" : [ "T.-H. Chan", "K. Jia", "S. Gao", "J. Lu", "Z. Zeng", "Y. Ma" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "D. Chen", "C.D. Manning" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "The loss surface of multilayer networks",
      "author" : [ "A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "A. Coates", "H. Lee", "A.Y. Ng" ],
      "venue" : "Ann Arbor,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity",
      "author" : [ "A. Daniely", "R. Frostig", "Y. Singer" ],
      "venue" : "arXiv preprint arXiv:1602.05897,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "On the Nyström method for approximating a Gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M.W. Mahoney" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "Efficient projections onto the `1ball for learning in high dimensions",
      "author" : [ "J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Compressed sensing: theory and applications",
      "author" : [ "Y.C. Eldar", "G. Kutyniok" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "An empirical study of learning speed in back-propagation networks",
      "author" : [ "S.E. Fahlman" ],
      "venue" : "Journal of Heuristics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1988
    }, {
      "title" : "Global optimality in tensor factorization, deep learning, and beyond",
      "author" : [ "B.D. Haeffele", "R. Vidal" ],
      "venue" : "arXiv preprint arXiv:1506.07540,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Suitable mlp network activation functions for breast cancer and thyroid disease detection",
      "author" : [ "I. Isa", "Z. Saad", "S. Omar", "M. Osman", "K. Ahmad", "H.M. Sakim" ],
      "venue" : "In 2010 Second International Conference on Computational Intelligence, Modelling and Simulation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Generalization bounds for neural networks through tensor factorization",
      "author" : [ "M. Janzamin", "H. Sedghi", "A. Anandkumar" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : "Master Thesis,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Face recognition: A convolutional neural-network approach",
      "author" : [ "S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1997
    }, {
      "title" : "Fastfood-approximating kernel expansions in loglinear time",
      "author" : [ "Q. Le", "T. Sarlós", "A. Smola" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1998
    }, {
      "title" : "On the computational efficiency of training neural networks",
      "author" : [ "R. Livni", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "Convolutional kernel networks",
      "author" : [ "J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "On some extensions of Bernstein’s inequality for self-adjoint operators",
      "author" : [ "S. Minsker" ],
      "venue" : "arXiv preprint arXiv:1112.5448,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2007
    }, {
      "title" : "On the quality of the initial basin in overspecified neural networks",
      "author" : [ "I. Safran", "O. Shamir" ],
      "venue" : "arXiv preprint arXiv:1511.04210,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    }, {
      "title" : "Provable methods for training neural networks with sparse connectivity",
      "author" : [ "H. Sedghi", "A. Anandkumar" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2014
    }, {
      "title" : "Learning kernel-based halfspaces with the 0-1 loss",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "K. Sridharan" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2011
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Learning invariant representations with local transformations",
      "author" : [ "K. Sohn", "H. Lee" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2012
    }, {
      "title" : "Neural networks with periodic and monotonic activation functions: a comparative study in classification problems",
      "author" : [ "J.M. Sopena", "E. Romero", "R. Alquezar" ],
      "venue" : "In ICANN",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1999
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 1929
    }, {
      "title" : "Support vector machines",
      "author" : [ "I. Steinwart", "A. Christmann" ],
      "venue" : null,
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2008
    }, {
      "title" : "Deep learning using linear support vector machines",
      "author" : [ "Y. Tang" ],
      "venue" : "arXiv preprint arXiv:1306.0239,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2013
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2010
    }, {
      "title" : "End-to-end text recognition with convolutional neural networks",
      "author" : [ "T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng" ],
      "venue" : "In Pattern Recognition (ICPR),",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2012
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "L. Xiao", "T. Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2014
    }, {
      "title" : "Learning halfspaces and neural networks with random initialization",
      "author" : [ "Y. Zhang", "J.D. Lee", "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "arXiv preprint arXiv:1511.07948,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2015
    }, {
      "title" : "`1-regularized neural networks are improperly learnable in polynomial time",
      "author" : [ "Y. Zhang", "J.D. Lee", "M.I. Jordan" ],
      "venue" : "In Proceedings on the 33rd International Conference on Machine Learning,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 27,
      "context" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 24,
      "context" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 25,
      "context" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 20,
      "context" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 43,
      "context" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 31,
      "context" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].",
      "startOffset" : 264,
      "endOffset" : 272
    }, {
      "referenceID" : 36,
      "context" : "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].",
      "startOffset" : 264,
      "endOffset" : 272
    }, {
      "referenceID" : 5,
      "context" : "However, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard [6].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : "In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation [7].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 18,
      "context" : "This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper [19]), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 46,
      "context" : "This approach is inspired by our earlier work [48], involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 46,
      "context" : "Our results show that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network [48], highlighting the importance of parameter sharing.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "[16].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 44,
      "context" : "There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : "Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nyström approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q ∈ RnP×m such that K ≈ QQ>.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 32,
      "context" : "Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nyström approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q ∈ RnP×m such that K ≈ QQ>.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 26,
      "context" : "The random feature approximation takes O(mnPd1) time, but can be improved to O(mnP log d1) time using the fast Hadamard transform [27].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 35,
      "context" : "[36] for learning halfspaces, and by Zhang et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 46,
      "context" : "[48] for learning fully-connected neural networks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : ", used by [10, 29]).",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 28,
      "context" : ", used by [10, 29]).",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 38,
      "context" : ", used by [39, 22]).",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 21,
      "context" : ", used by [39, 22]).",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "Combining this bound with the classical Rademacher complexity theory [4], we conclude that the generalization loss of f̂ccnn converges to the least possible generalization error of Fccnn.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 46,
      "context" : "[48].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "We define the multi-channel patch vector as a concatenation of patch vectors for each channel: zp(x) := (zp(x[1]), .",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 42,
      "context" : "These variations are known to be hard for methods without a convolution mechanism (for instance, see the paper [44]).",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : "The feature matrix Z(x) is constructed via random feature approximation [33] with dimension m = 500 for the first convolutional layer and m = 1000 for the second.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "Before training each CCNN layer, we preprocess the input vectors zp(xi) using local contrast normalization and ZCA whitening [12].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 37,
      "context" : "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 42,
      "context" : "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 7,
      "context" : "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].",
      "startOffset" : 260,
      "endOffset" : 263
    }, {
      "referenceID" : 8,
      "context" : "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 42,
      "context" : "basic rand rot img img+rot SVMrbf [44] 3.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 42,
      "context" : "18% NN-1 [44] 4.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 37,
      "context" : "28% TIRBM [38] - 4.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 42,
      "context" : "50% SDAE-3 [44] 2.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "76% ScatNet-2 [8] 1.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "48% PCANet-2 [9] 1.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 17,
      "context" : "To see their impact on the parameter matrix, we compute the effective rank (ratio between the nuclear norm and the spectral norm, see [18]) of matrix Â.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 23,
      "context" : "2 CIFAR-10 In order to test the capability of CCNN in complex classification tasks, we report its performance on the CIFAR-10 dataset [24].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 24,
      "context" : "It was known that the generalization performance of the CNN can be improved by training on random crops of the original image [25], so we train the CNN on random 24×24 patches of the image, and test on the central 24×24 patch.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 26,
      "context" : "We compare the CCNN against other baseline methods that don’t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "We compare the CCNN against other baseline methods that don’t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 29,
      "context" : "We compare the CCNN against other baseline methods that don’t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 26,
      "context" : "52% SVMFastfood [27] 36.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "90% PCANet-2 [9] 22.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 29,
      "context" : "86% CKN [30] 21.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 24,
      "context" : "[25]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5] showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "This perspective encourages incrementally adding neurons to the network, whose generalization error was studied by Bach [3].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 45,
      "context" : "[47] propose a polynomial-time ensemble method for learning fully-connected neural networks, but their approach handles neither parameter sharing nor the convolutional setting.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "Other relevant works for learning fully-connected networks include [35, 23, 29].",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "Other relevant works for learning fully-connected networks include [35, 23, 29].",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "Other relevant works for learning fully-connected networks include [35, 23, 29].",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "[1, 2] propose a method for learning multi-layer latent variable models.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "[1, 2] propose a method for learning multi-layer latent variable models.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 10,
      "context" : "Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 33,
      "context" : "Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 29,
      "context" : "[30] present convolutional kernel networks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "The ScatNet method [8] uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, unlike the analysis in this paper.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "[13] show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from a random initialization.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 46,
      "context" : "[48].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "We refer the reader to Bartlett and Mendelson [4] for an introduction to the theoretical properties of Rademacher complexity.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "Thus, the theory of Rademacher complexity [4] guarantees that E[L(Fccnn(X);Y )] ≤ inf f∈Fccnn E[L(f(x); y)] + 2L · Rn(Fccnn) + c √ n , (33)",
      "startOffset" : 42,
      "endOffset" : 45
    } ],
    "year" : 2016,
    "abstractText" : "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.",
    "creator" : "LaTeX with hyperref package"
  }
}