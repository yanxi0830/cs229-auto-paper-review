{
  "name" : "1606.03212.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Discovery of Latent Factors in High-dimensional Data Using Tensor Methods",
    "authors" : [ "Furong Huang", "Shaoyun Liu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n03 21\n2v 1\n[ cs\n.L G\n] 1\n0 Ju\nn 20\n16"
    }, {
      "heading" : "UNIVERSITY OF CALIFORNIA, IRVINE",
      "text" : "Discovery of Latent Factors in High-dimensional Data Using Tensor Methods"
    }, {
      "heading" : "DISSERTATION",
      "text" : "submitted in partial satisfaction of the requirements for the degree of"
    }, {
      "heading" : "DOCTOR OF PHILOSOPHY",
      "text" : "in Electrical and Computer Engineering\nby\nFurong Huang\nDissertation Committee: Assistant Professor Animashree Anandkumar, Chair\nProfessor Carter Butts Associate Professor Athina Markopoulou\n2016\nAll materials c© 2016 Furong Huang"
    }, {
      "heading" : "DEDICATION",
      "text" : "To Jinsong Huang and Shaoyun Liu\nii\nTABLE OF CONTENTS\nPage"
    }, {
      "heading" : "LIST OF FIGURES vii",
      "text" : ""
    }, {
      "heading" : "LIST OF TABLES ix",
      "text" : ""
    }, {
      "heading" : "LIST OF ALGORITHMS x",
      "text" : ""
    }, {
      "heading" : "ACKNOWLEDGMENTS xi",
      "text" : "CURRICULUM VITAE xiii"
    }, {
      "heading" : "ABSTRACT OF THE DISSERTATION xvi",
      "text" : ""
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Summary of Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.1.1 Globally Guaranteed Online Tensor Decomposition . . . . . . . . . . 3 1.1.2 Deployment of Scalable Tensor Decomposition Framework . . . . . . 4 1.1.3 Learning Invariant Models Using Convolutional Tensor Decomposition 6 1.1.4 Learning Latent Tree Models Using Hierarchical Tensor Decomposition 7 1.1.5 Discovering Neuronal Cell Types Using Spectral Methods . . . . . . . 8\n1.2 Tensor Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.3 Background and Related Works . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.3.1 Online Stochastic Gradient for Tensor Decomposition . . . . . . . . . 12 1.3.2 Applying Online Tensor Methods for Learning Latent Variable Models 15 1.3.3 Dictionary Learning through Convolutional Tensor Decomposition . . 17 1.3.4 Latent Tree Model Learning via Hierarchical Tensor Decomposition . 22\n1.4 Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24"
    }, {
      "heading" : "2 Online Stochastic Gradient for Tensor Decomposition 25",
      "text" : "2.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.2 Stochastic Gradient Descent for Strict saddle Function . . . . . . . . . . . . 29\n2.2.1 Strict saddle Property . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.2.2 Proof Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 2.2.3 Constrained Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.3 Online Tensor Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 36 2.3.1 Optimization Problem for Tensor Decomposition . . . . . . . . . . . . 36\niii\n2.3.2 Implementing Stochastic Gradient Oracle . . . . . . . . . . . . . . . . 38 2.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 2.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42"
    }, {
      "heading" : "3 Applying Online Tensor Methods for Learning Latent Variable Models 43",
      "text" : "3.1 Tensor Forms for Topic and Community Models . . . . . . . . . . . . . . . . 45\n3.1.1 Topic Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.1.2 Mixed Membership Model . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.2 Learning using Third Order Moment . . . . . . . . . . . . . . . . . . . . . . 50 3.2.1 Dimensionality Reduction and Whitening . . . . . . . . . . . . . . . . 51 3.2.2 Stochastic Tensor Gradient Descent . . . . . . . . . . . . . . . . . . . 52 3.2.3 Post-processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.3 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.3.1 Symmetrization Step to Compute M2 . . . . . . . . . . . . . . . . . . 54 3.3.2 Efficient Randomized SVD Computations . . . . . . . . . . . . . . . 55 3.3.3 Stochastic Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 3.3.4 Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . 61 3.4 Validation methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.4.1 P -value Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.4.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 3.5 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76"
    }, {
      "heading" : "4 Dictionary Learning through Convolutional Tensor Decomposition 78",
      "text" : "4.1 Model and Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n4.1.1 Convolutional Dictionary Learning/ICA Model . . . . . . . . . . . . 81 4.2 Form of Cumulant Moment Tensors . . . . . . . . . . . . . . . . . . . . . . . 82 4.3 Alternating Least Squares for Convolutional Tensor Decomposition . . . . . 84 4.4 Algorithm Optimization to Reduce Memory and Computational Costs . . . . 87\n4.4.1 Challenge: Computing ((H⊤H). ⋆ (G⊤G))† . . . . . . . . . . . . . . . 88 4.4.2 Challenge: Computing M = C3(H⊙ G) · ((H⊤H). ⋆ (G⊤G))† . . . . . 89\n4.5 Experiments: Comparison with Alternating Minimization . . . . . . . . . . . 91 4.6 Application: Learning Word-sequence Embeddings . . . . . . . . . . . . . . . 92\n4.6.1 Word-Sequence Modeling and Formulation . . . . . . . . . . . . . . . 92 4.6.2 Evaluating Embeddings through Downstream Tasks . . . . . . . . . . 97\n4.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102"
    }, {
      "heading" : "5 Latent Tree Model Learning through Hierarchical Tensor Decomposition103",
      "text" : "5.1 Latent Tree Graphical Model Preliminaries . . . . . . . . . . . . . . . . . . . 105 5.2 Overview of Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 5.3 Structure Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 5.4 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 5.5 Integrated Structure and Parameter Estimation . . . . . . . . . . . . . . . . 111\n5.5.1 Local Recursive Grouping with Tensor Decomposition . . . . . . . . . 111 5.5.2 Merging and Alignment Correction . . . . . . . . . . . . . . . . . . . 113\niv\n5.6 Theoretical Gaurantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 5.7 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 5.7.1 Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 5.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122"
    }, {
      "heading" : "6 Discovering Cell Types with Spatial Point Process Mixture Model 123",
      "text" : "6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n6.1.1 Motivations and Goals . . . . . . . . . . . . . . . . . . . . . . . . . . 124 6.1.2 Previous Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.2 Modeling Cell-types Using Spatial Point Process Features . . . . . . . . . . . 129 6.2.1 The Marked Spatial Point Process Representation of ISH Images . . . 129 6.2.2 Representing Spatial Point Processes Using Joint Feature Histograms 130 6.3 Un-mixing Spatial Point Processes to Discover Cell-types . . . . . . . . . . . 131 6.3.1 Generative Model: A Variation of Latent Dirichlet Allocation . . . . 131 6.3.2 Estimating the Cell-type Dependent Gene Expression Profile β . . . . 132 6.3.3 Estimating the Cell-type Dependent Spatial Point Process Histogram h 133 6.4 Results and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 6.4.1 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . 134 6.4.2 Evaluating Cell-type Gene Expression Profile Predictions . . . . . . . 135 6.4.3 Comparison to Standard Average Gene Expression Features . . . . . 136 6.4.4 A Brief Analysis of Recovered Cell Types in Somatosensory Cortex . 138 6.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139"
    }, {
      "heading" : "7 Conclusion and Outlook 141",
      "text" : "7.1 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 7.2 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\nBibliography 143"
    }, {
      "heading" : "A Appendix for Online Stochastic Gradient for Tensor Decomposition 156",
      "text" : "A.1 Detailed Analysis for Section 2.2 in Unconstrained Case . . . . . . . . . . . . 156 A.2 Detailed Analysis for Section 2.2 in Constrained Case . . . . . . . . . . . . . 172\nA.2.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 A.2.2 Geometrical Lemmas Regarding Constraint Manifold . . . . . . . . . 178 A.2.3 Main Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\nA.3 Detailed Proofs for Section 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . 195 A.3.1 Warm Up: Maximum Eigenvalue Formulation . . . . . . . . . . . . . 195 A.3.2 New Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 A.3.3 Extending to Tensors of Different Order . . . . . . . . . . . . . . . . 209"
    }, {
      "heading" : "B Appendix for Applying Online Tensor Methods for Learning LVMs 212",
      "text" : "B.1 Stochastic Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 B.2 Proof of Algorithm Correctness . . . . . . . . . . . . . . . . . . . . . . . . . 214 B.3 GPU Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 B.4 Results on Synthetic Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . 219\nv\nB.5 Comparison of Error Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . 220"
    }, {
      "heading" : "C Appendix for Dictionary Learning via Convolutional Tensor Method 224",
      "text" : "C.1 Cumulant Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 C.2 Proof for Main Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 C.3 Parallel Inversion of Ψ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226"
    }, {
      "heading" : "D Appendix for Latent Tree Learning via Hierarchical Tensor Method 228",
      "text" : "D.1 Additivity of the Multivariate Information Distance . . . . . . . . . . . . . . 228 D.2 Local Recursive Grouping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 D.3 Proof Sketch for Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 231 D.4 Proof of Correctness for LRG . . . . . . . . . . . . . . . . . . . . . . . . . . 233 D.5 Cross Group Alignment Correction . . . . . . . . . . . . . . . . . . . . . . . 235 D.6 Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 D.7 Sample Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 D.8 Efficient SVD Using Sparsity and Dimensionality Reduction . . . . . . . . . 238"
    }, {
      "heading" : "E Appendix for Spatial Point Process Mixture model Learning 240",
      "text" : "E.1 Morphological Basis Extraction . . . . . . . . . . . . . . . . . . . . . . . . . 240\nE.1.1 Gaussian Prior Convolutional Sparse Coding . . . . . . . . . . . . . . 241 E.1.2 Image Registration/Alignment . . . . . . . . . . . . . . . . . . . . . . 242\nvi\nLIST OF FIGURES\nPage\n1.1 Unsupervised learning general framework . . . . . . . . . . . . . . . . . . . . 3 1.2 Tensor decomposition framework is versatile . . . . . . . . . . . . . . . . . . 5 1.3 Tensor decomposition vs variational inference on PubMed . . . . . . . . . . . 5 1.4 Tensor decomposition vs variational inference on social networks . . . . . . . 5 1.5 Word embedding and sentence embedding . . . . . . . . . . . . . . . . . . . 6 1.6 Hierarchical tensor decomposition. . . . . . . . . . . . . . . . . . . . . . . . . 7 1.7 Examples of brain slices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.8 Orthogonal matrix decomposition is not unique without eigenvalue gap. . . . 10 1.9 Orthogonal tensor decomposition is unique with or without eigenvalue gap. . 11 1.10 Flat multi-view vs hierarchical latent variable graphical model . . . . . . . . 23 2.1 Comparison of different objective functions . . . . . . . . . . . . . . . . . . . 41 2.2 Comparison of different objective functions . . . . . . . . . . . . . . . . . . . 42\n3.1 Efficient computation in smart order . . . . . . . . . . . . . . . . . . . . . . 55 3.2 Data transfer between CPU and GPU . . . . . . . . . . . . . . . . . . . . . . 58 3.3 STGD running time comparison . . . . . . . . . . . . . . . . . . . . . . . . . 60 3.4 P -value matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.5 Yelp result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 3.6 Facebook result tunning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 4.1 Block structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 4.2 Error and running time comparison . . . . . . . . . . . . . . . . . . . . . . . 92 4.3 Principal component projection . . . . . . . . . . . . . . . . . . . . . . . . . 93 4.4 Overview of our ConvDic+DeconvDec framework . . . . . . . . . . . . . . . . 94 4.5 Tensor decomposition for learning convolutional ICA models . . . . . . . . . 95 4.6 Third order cumulant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.1 Latent tree and hierarchical tensor decomposition . . . . . . . . . . . . . . . 104 5.2 Overall approach illustrated in a toy example . . . . . . . . . . . . . . . . . 107 5.3 Running time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 5.4 Subtree 1 of estimated human disease hierarchy . . . . . . . . . . . . . . . . 120 5.5 Subtree 1 of estimated human disease hierarchy . . . . . . . . . . . . . . . . 121 6.1 Overview of the proposed framework . . . . . . . . . . . . . . . . . . . . . . 128 6.2 Synthetic results and comparison with average gene expression level baseline. 135\nvii\n6.3 Estimated β on marker genes for 8 cell types . . . . . . . . . . . . . . . . . . 137 6.4 Detected 8 cell type feature visualization . . . . . . . . . . . . . . . . . . . . 138\nviii\nLIST OF TABLES\nPage\n3.1 Linear algebraic operation counts . . . . . . . . . . . . . . . . . . . . . . . . 59 3.2 Time and space complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 3.3 New York Times results: topics . . . . . . . . . . . . . . . . . . . . . . . . . 72 3.4 New York Times results: words . . . . . . . . . . . . . . . . . . . . . . . . . 72 3.5 Datasets summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 3.6 Compare community detection results against variational method . . . . . . 74 3.7 Membership recovery in Yelp review data . . . . . . . . . . . . . . . . . . . . 75 4.1 Summary statistics of the datasets used. . . . . . . . . . . . . . . . . . . . . 97 4.2 Classification tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.3 Paraphrase detection tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.4 STS task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.1 Worst-case computational complexity of our algorithm . . . . . . . . . . . . 117 5.2 Robinson Foulds (RF) metric . . . . . . . . . . . . . . . . . . . . . . . . . . 119\nix\nLIST OF ALGORITHMS\nPage 1 Noisy Stochastic Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2 Projected Noisy Stochastic Gradient . . . . . . . . . . . . . . . . . . . . . . 34 3 Moment-based spectral learning of latent variable models . . . . . . . . . . . 51 4 Randomized tall-thin SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 5 Randomized Pseudoinverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 6 LRG with Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . 112 7 Merging and Alignment Correction (MAC) . . . . . . . . . . . . . . . . . . . . 113 8 Parameter Alignment Correction . . . . . . . . . . . . . . . . . . . . . . . . 115\nx"
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "First and foremost I want to thank my advisor Animashree Anandkumar, who has been my role model as a successful female professor in machine learning. It has been an honor to be her first Ph.D. student. I appreciate all the efforts she put to help build my confidence, guide me through my early research career, and make my graduate study experience productive and stimulating. Her endless enthusiasm for research has been contagious and a source of motivation. She has also continually and convincingly conveyed a spirit of adventure with regard to research and scholarship. Anima is not only a role model, a career guide but also a friend who shares life experience and offers excellent advice. I couldn’t have fought through the tough times in my Ph.D. pursuit without her inspiration or support.\nDuring my graduate studies, I have been lucky to have collaborated with some smart and innovative minds who inspired me profoundly. My collaborator Rong Ge has impressed me by his enthusiasm, intensity and incredible ability to disentangle complicated research problems. I would also like to acknowledge Chi Jin and Yang Yuan for always being available for discussions and brainstorming. I am especially grateful for working with Srini Turaga and Ernest Fraenkel. They provided comments and advice from fresh angles and stimulated me to think differently. I appreciate insightful and sparkling discussions with Sham Kakade, Daniel Hsu, David Mimno, David Blei, Qirong Ho, Alex Smola, Paul Mineiro, Nikos Karampatziakis and others. During my internship in Microsoft Research New England, I have met the most wonderful mentors Jennifer Chayes and Christian Borgs, whose support has powered me to chase my academic dreams.\nI would like to thank my committee members, Professor Athina Markopoulou, and Professor Carter Butts, who are always there for me whenever I need advice. In addition, a thank you to Professor Max Welling and Professor Alexander Ihler, who introduced me to machine learning, and stimulated my long lasting enthusiasm for machine learning. I also appreciate the efforts of Professor Padhraic Smyth, who started the Data Science Initiative, a growing interdisciplinary machine learning community, in UC Irvine.\nThe members of the MEGADatA group, Majid Janzamin, Hanie Sedghi, Niranjan UN, Forough Arabshahi, Yang Shi, Kamyar Azizzade, and Saeed Karimi Bidhendi, have brought immense amount of joy to my personal and professional time at UC Irvine. I am grateful for the nights we spent working on paper deadlines, as well as the fun times we had wearing bean sprout hair clips in the lab and posing for group profile pictures. The group has been a source of friendships and collaborations.\nI thank MIT Press for permission to include Chapter 2 of my thesis, which was originally published in Conference of Learning Theory. And I thank MIT Press for permission to include Chapter 3 and 4 of my thesis, which was originally published in Journal of Machine Learning. I gratefully acknowledge the funding sources that made my Ph.D. work possible. I was funded by the EECS Department fellowship. My work was also supported by the National Science Foundation BIGDATA award.\nxi\nLastly, I would like to thank my family for all their unconditional love and faithful support. Thank my parents, Jinsong Huang and Shaoyun Liu, for raising me with hard-working spirit and a love of science. Wenchao Xi, thank you for always being by my side, sharing joy and sorrow, in the years of adventure.\nxii\nCURRICULUM VITAE\nFurong Huang"
    }, {
      "heading" : "EDUCATION",
      "text" : "Doctor of Philosophy in ECE 2016 University of California Irvine Irvine, CA, USA Master of Science in ECE 2012 University of California Irvine Irvine, CA, USA Bachelor of Science in EECS 2010 Zhejiang University Hangzhou, Zhejiang, China"
    }, {
      "heading" : "RESEARCH EXPERIENCE",
      "text" : "Graduate Research Assistant 2010–2016 University of California Irvine Irvine, California\nResearch Intern 2014.3–2014.5 Microsoft Research Redmond, Washington\nResearch Intern 2014.6–2014.12 Microsoft Research New England Cambridge, Massachusetts"
    }, {
      "heading" : "REFEREED JOURNAL PUBLICATIONS",
      "text" : "F. Huang, U.N. Niranjan, M.U. Hakeem and A. Anandkumar, “Online Tensor Methods for Learning Latent Variable Models”\n2014"
    }, {
      "heading" : "Journal of Machine Learning",
      "text" : ""
    }, {
      "heading" : "A. Anandkumar, V.Y.F Tan, F. Huang and A.S. Willsky, “HighDimensional Structure Learning of Ising Models: Local Separation",
      "text" : "Criterion”\n2012"
    }, {
      "heading" : "Annals of Statistics",
      "text" : ""
    }, {
      "heading" : "A. Anandkumar, V.Y.F Tan, F. Huang and A.S. Willsky, “HighDimensional Gaussian Graphical Model Selection: Walk-Summability",
      "text" : "and Local Separation Criterion”\n2012\nJournal of Machine Learning\nxiii"
    }, {
      "heading" : "REFEREED CONFERENCE PUBLICATIONS",
      "text" : ""
    }, {
      "heading" : "F. Huang, A. Anandkumar, C. Borgs, J. Chayes, E. Fraenkel, M. Hawrylycz, E. Lein, A. Ingrosso, S. Turaga, “Discovering Neuronal Cell Types and Their Gene Expression Profiles Using a Spatial Point",
      "text" : "Process Mixture Model”\n2015"
    }, {
      "heading" : "NIPS BigNeuro workshop 2015",
      "text" : ""
    }, {
      "heading" : "F. Huang, U.N. Niranjan, J. Perros, R. Chen, J. Sun, A. Anandkumar,“Scalable Latent Tree Model and its Application to Health",
      "text" : "Analytics”\n2015\nNIPS 2015 Workshop on Machine Learning in Healthcare\nF. Huang, A. Anandkumar, “Convolutional Dictionary Learning through Tensor Factorization”\n2015\nJMLR conference and workshop proceedings"
    }, {
      "heading" : "F. Arabshahi, F. Huang, A. Anandkumar, C. Butts, “Are you going to the party: depends, who else is coming? –Learning hidden group",
      "text" : "dynamics via conditional latent tree models”\n2015"
    }, {
      "heading" : "2015 IEEE International Conference on Data Mining (ICDM)",
      "text" : ""
    }, {
      "heading" : "F. Huang, S. Matusevych, A.Anandkumar, N. Karampatziakism and",
      "text" : "P. Mineiro, “Distributed Latent Dirichlet Allocation via Tensor Factorization”\n2014"
    }, {
      "heading" : "NIPS Optimization for Machine Learning workshop",
      "text" : ""
    }, {
      "heading" : "A. Anandkumar, D. Hsu, F. Huang and S.M. Kakade, “Learning",
      "text" : "High-Dimensional Mixtures of Graphical Models”\n2012"
    }, {
      "heading" : "Proc. of NIPS 2012",
      "text" : "F. Huang and A. Anandkumar, “FCD: Fast-Concurrent-Distributed Load Balancing under Switching Costs and Imperfect Observations”\n2013"
    }, {
      "heading" : "In Proc. of the 32nd IEEE INFOCOM",
      "text" : "F. Huang, W. Wang and Z. Zhang, “Prediction-based Spectrum Aggregation with Hardware Limitation in Cognitive Radio Networks”\n2010\nIEEE Vehicular Technology Conference\nSOFTWARE\nxiv\nTensorDecom4TopicModeling Link to Github repository C++ algorithm that solves topic modeling LDA using tensor decomposition on single node workstations.\nOnlineTensorCommunity Link to Github repository C++ and CUDA algorithms that solves community detection problem using tensor decomposition on single node CPU and GPU.\nSpectralLDA-TensorSpark Link to Github repository Spark spectral LDA algorithms in Scala that solves large scale tensor decomposition.\nConvDicLearnTensorFactor Link to Github repository Tensor decomposition algorithms that learns convolutional dictionary models."
    }, {
      "heading" : "AWARDS",
      "text" : "MLconf Industry Impact Student Research Winner 2015 Google San Francisco, California\nTravel Grant 2015 NIPS Montreal, Canada\nTravel Grant 2013 WiML Lake Tahoe, Nevada\nFellowship 2010 University of California Irvine Irvine, California\nxv"
    }, {
      "heading" : "ABSTRACT OF THE DISSERTATION",
      "text" : "Discovery of Latent Factors in High-dimensional Data Using Tensor Methods\nBy\nFurong Huang\nDoctor of Philosophy in Electrical and Computer Engineering\nUniversity of California, Irvine, 2016\nAssistant Professor Animashree Anandkumar, Chair\nUnsupervised learning aims at the discovery of hidden structure that drives the observations in the real world. It is essential for success in modern machine learning and artificial intelligence. Latent variable models are versatile in unsupervised learning and have applications in almost every domain, e.g., social network analysis, natural language processing, computer vision and computational biology. Training latent variable models is challenging due to the non-convexity of the likelihood objective function. An alternative method is based on the spectral decomposition of low order moment matrices and tensors. This versatile framework is guaranteed to estimate the correct model consistently. My thesis spans both theoretical analysis of tensor decomposition framework and practical implementation of various applications.\nThis thesis presents theoretical results on convergence to globally optimal solution of tensor decomposition using the stochastic gradient descent, despite non-convexity of the objective. This is the first work that gives global convergence guarantees for the stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points.\nThis thesis also presents large-scale deployment of spectral methods (matrix and tensor decomposition) carried out on CPU, GPU and Spark platforms. Dimensionality reduction techniques such as random projection are incorporated for a highly parallel and scalable\nxvi\ntensor decomposition algorithm. We obtain a gain in both accuracies and in running times by several orders of magnitude compared to the state-of-art variational methods.\nTo solve real world problems, more advanced models and learning algorithms are proposed. After introducing tensor decomposition framework under latent Dirichlet allocation (LDA) model, this thesis discusses generalization of LDA model to mixed membership stochastic block model for learning hidden user commonalities or communities in social network, convolutional dictionary model for learning phrase templates and word-sequence embeddings, hierarchical tensor decomposition and latent tree structure model for learning disease hierarchy in healthcare analytics, and spatial point process mixture model for detecting cell types in neuroscience.\nxvii\nChapter 1"
    }, {
      "heading" : "Introduction",
      "text" : "There has been tremendous excitement about machine learning and artificial intelligence over the last few years. We are now able to do automated classification of images, where there are a predefined set of image categories. Due to the enormous amount of available labeled data, and powerful computation resources, we can train massive neural networks and obtain features for classification in domains such as image classification, speech recognition, and text understanding. However, all these tasks fall under what we call supervised learning, where the training data provides label information. What if such labeled information about the categories is absent? Can we have automated discovery of the features and categories?\nThis problem is known as unsupervised learning, and experts agree that it is one of the hardest problems in machine learning. Unsupervised learning is usually the foundation for the success of supervised learning in many real world problems, and it aims at summarizing key features in the data. Human beings are known to be good at unsupervised learning, as we accumulate “general knowledge” or “common sense.” But can we have “intelligent” machines that mimic such capabilities?\n1\nWe live in a world with explosively growing data; as we receive more data, not only do we get more information but also are we confronted with more variables or “unknowns”. In other words, as the data grows, the number of variables also grows, and this is known as the high-dimensional regime. Learning the data patterns or the model in high dimensions is extremely challenging due to curse of dimensionality. However, the useful information that we need to gain an insightful understanding of the data usually hides in a low dimensional space. Finding these hidden structures is computationally challenging since it is akin to finding “a needle in a haystack”.\nThe hidden structures in data can be efficiently expressed with the use of probabilistic latent variable models. The computational task of searching for hidden structures is then expressed as learning a probabilistic latent variable model. Once the model is learned, the hidden variables can be inferred based on the model parameters, as depicted in Figure 1.1.\nThere exit numerous popular approaches for probabilistic latent variable model learning algorithms, among which two families of approaches are particularly successful: randomized algorithms (such as MCMC) and deterministic algorithms (such as maximum likelihood based variational inference). However, randomized algorithms are typically slow due to the exponential mixing time. The deterministic maximum likelihood based estimators tend to be faster than randomized algorithms, but the likelihood function is often intractable. One solution is to substitute the likelihood objective with its approximation and search for the optima. However, local search methods are susceptible to spurious local optima as the surrogate likelihoods are usually non-convex.\nIn this thesis, we analyze and deploy an alternative tensor decomposition framework for learning latent variable models. The basic paradigm of tensor decomposition framework dates back to 1894 when Pearson [135] proposed the method of moments, a classical parameter estimation technique using data statistics. The method of moments identifies the model whose parameters give rise to the observed aggregated statistics of the data (such\n2\nas empirical moments) [12]. Although matching the model parameters to the observed moments may involve solving computationally intractable systems of multivariate polynomial equations, low-order moments (typically third or fourth order) completely characterize the distribution for many classes of latent variable models [37, 36, 38, 128, 81, 15, 80], and decomposition of the low-order statistics of the data (tensors) reveals the consistent model parameters asymptotically. Therefore, the inverse method of moments is solved efficiently with consistency guarantees (both in terms of computational and sample complexity), in contrast to the computationally prohibitive maximum likelihood estimators which require non-convex optimization and are subject to local optimality."
    }, {
      "heading" : "1.1 Summary of Contributions",
      "text" : ""
    }, {
      "heading" : "1.1.1 Globally Guaranteed Online Tensor Decomposition",
      "text" : "Learning latent variable models via method of moments involves a challenging non-convex optimization problem in the high-dimensional regime as tensor decomposition is NP-hard in general. We identify strict saddle property for non-convex problem that allows for efficient optimization. Using this property, we show that from an arbitrary starting point, noisy stochastic gradient descent converges to a local minimum in a polynomial number of iterations. To the best of our knowledge, this is the first work that gives global convergence\n3\nguarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. Our analysis is applied to orthogonal tensor decomposition, and we propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result, we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee [64]. By employing this algorithm, we obtain an efficient unsupervised learning algorithm for a wide class of latent variable models."
    }, {
      "heading" : "1.1.2 Deployment of Scalable Tensor Decomposition Framework",
      "text" : "Tensor decomposition framework is tailored for automated categorization of documents (that is finding the hidden topics of articles) and prediction of social actors’ common interests or communities (using the connectivity graph) in social networks efficiently, see Figure 1.2. Compared to the state of the art variational inference, which optimizes the lower bound on the likelihood, our results are surprisingly accurate and much faster [84, 86]. For instance, we implemented our tensor decomposition on spark to learn topics in the PubMed data, which consists of 8 million documents and 700 million words. Tensor method achieves much more accurate results (better likelihood) compared to variational inference although we never compute or optimize over the likelihood function. Furthermore, tensor method requires much less computation time and is at least an order of magnitude faster.\nAnother comparison is carried out on graph data to evaluate the performance of discovering hidden communities. On the Facebook friendship network, yelp bipartite review graph and DBLP co-authorship system, tensor decomposition framework continues to be both accuracy and fast compared to the state-of-the-art variational methods [86].\n4\n5"
    }, {
      "heading" : "1.1.3 Learning Invariant Models Using Convolutional Tensor De-",
      "text" : "composition\nTensor methods can also be extended to solving the problem of learning shift invariant dictionary elements. The data is modeled as linear combinations of filters/templates convolved with activation maps. The filters are shift invariant dictionary elements due to the convolution. A tensor decomposition algorithm with additional shift invariance constraints on the factors is introduced, and it converges to models with better reconstruction error and is much faster, compared to the popular alternating minimization heuristic, where the filters and activation maps are alternately updated.\nThis convolutional tensor decomposition framework successfully solves challenging natural language processing tasks such as learning phrase templates and extracting word-sequence embeddings, as in Figure 1.5. Convolutional tensor decomposition learns a good set of filters/templates [82] and discriminative features (such as word-sequence embeddings) which yield successful automated understanding and classification of word-sequences.\n6"
    }, {
      "heading" : "1.1.4 Learning Latent Tree Models Using Hierarchical Tensor De-",
      "text" : "composition\nTensor decomposition framework is also extended to learning models with hierarchy. This thesis presents an integrated approach to structure and parameter estimation in latent tree models. The proposed algorithm automatically learns the latent variables and their locations and achieves consistent structure estimation with logarithmic computational complexity. Meanwhile, the inverse method of moments is carried out on smartly selected local neighborhoods with linear computational complexity. A rigorous proof of the global consistency of the structure and parameter estimation under the “divide-and-conquer” framework is presented. The consistency guarantees apply to a broad class of linear multivariate latent tree models including discrete distributions, continuous multivariate distributions (e.g. Gaussian), and mixed distributions such as Gaussian mixtures [88]. This model class is much more general than discrete models, prevalent in most of the previous works on latent tree models [128, 127, 59, 17].\nThis efficient approach is shown to be useful in healthcare analytics [88], where we account for the co-occurrence of diseases on individuals and learn a clinical meaningful human disease hierarchy, using big electronic hospital records which involve millions of patients, hundreds of millions diagnostic events, and tens of thousands of diseases. The learned hierarchy\n7\non human diseases is clinically meaningful and can help doctors prevent potential diseases according to partial information on patients’ health condition."
    }, {
      "heading" : "1.1.5 Discovering Neuronal Cell Types Using Spectral Methods",
      "text" : "The above advances in unsupervised learning have rich applications in neuroscience. Using spectral decomposition framework, we analyze challenging tasks. For instance, cataloging neuronal cell types in the brain, which has been the number one goal of the brain initiative and modern neuroscience. It is an extremely challenging task partly due to the petabytescale size brain-wide single-cell resolution in situ hybridization imagery. Previous methods average over image intensity in local voxels for a rough estimation of gene expression levels. The success of these methods rely on a precise neuron level image alignment across different brains, which is computationally prohibitive.\nIn this thesis, we resolve the above problem using a spatial point process mixture model. We measure the spatial distribution of neurons labeled in the ISH image for each gene and model it as a spatial point process mixture, whose mixture weights are given by the cell types which\n8\nexpress that gene. By fitting a point process mixture model jointly to the ISH images, we infer both the spatial point process distribution for each cell type and their gene expression profile. We validate our predictions of cell type-specific gene expression profiles using single cell RNA sequencing data, recently published for the mouse somatosensory cortex. Jointly with the gene expression profiles, cell features such as cell size, orientation, intensity and local density level are inferred per cell type. Compared with the state-of-the-art approaches, our method [83] yields lower/better perplexity scores. In addition, 8 cell types are detected and their cell features are estimated."
    }, {
      "heading" : "1.2 Tensor Preliminaries",
      "text" : "What is a tensor? A pth order tensor is a p-dimensional array. We will use 4th order tensor as an example. If T ∈ Rd4 is a 4th order tensor, we use Ti1,i2,i3,i4(i1, ..., i4 ∈ [d]) to denote its (i1, i2, i3, i4) th entry.\nTensors can be constructed from tensor products. We use (u ⊗ v) to denote a 2nd order tensor where (u ⊗ v)i,j = uivj . This generalizes to higher order and we use u⊗4 to denote the 4th order tensor\n[u⊗4]i1,i2,i3,i4 = ui1ui2ui3ui4.\nWe say a 4th order tensor T ∈ Rd4 has an orthogonal decomposition if it can be written as\nT =\nd∑\ni=1\na⊗4i , (1.1)\nwhere ai’s are orthonormal vectors that satisfy ‖ai‖ = 1 and aTi aj = 0 for i 6= j. We call the vectors ai’s the components of this decomposition. Such a decomposition is unique up to permutation of ai’s and sign-flips.\n9\nA tensor also defines a multilinear form (just as a matrix defines a bilinear form), for a pth order tensor T ∈ Rdp and matrices Mi ∈ Rd×ni , i ∈ [p], we define\n[T (M1,M2, ...,Mp)]i1,i2,...,ip = ∑\nj1,j2,...,jp∈[d] Tj1,j2,...,jp\n∏ t∈[p] Mt[jt, it].\nThat is, the result of the multilinear form T (M1,M2, ...,Mp) is another tensor in R n1×n2×···×np. We will most often use vectors or identity matrices in the multilinear form. In particular, for a 4th order tensor T ∈ Rd4 we know T (I, u, u, u) is a vector and T (I, I, u, u) is a matrix. In particular, if T has the orthogonal decomposition in (1.1), we know T (I, u, u, u) = ∑d\ni=1(u Tai) 3ai\nand T (I, I, u, u) = ∑d\ni=1(u Tai) 2aia T i .\nWhy are tensors powerful? Let us start with the simple matrix decomposition, where the goal is to discover the orthogonal eigenvectors of a matrix. However, it is known that if the eigenvalues of the matrix are equal to each other, one can not uniquely identify the eigenvectors. For instance, an identity matrix can be decomposed as the set of basis vector e1 and e2, as well as u1 and u2, who are 45 degree rotated e1 and e2:\n  1 0\n0 1\n  = e1e⊤1 + e2e⊤2 = u1u⊤1 + u2u⊤2 .\n10\nHowever, in tensors, there exists a unique decomposition even without eigenvalue gap. Let a third order tensor (a cube) be decomposed as a linear combination of 2 rank-1 tensors as in red and blue, see Figure 1.9a. The eigenvectors of the tensor are this red vector and this blue vector who are orthogonal to each other, and the eigenvalues of the tensor are equal. Consider taking a slice of the tensor, which yields matrix. This matrix shares the same eigenvectors with the tensor, but the eigenvalues of this matrix will be different depending on the direction of the slice. Therefore, the slice of tensor has eigenvalue gap. And thus we are able to identify the eigenvectors for the tensor uniquely. Since higher order tensors have additional dimensions and contains more information, it is more powerful than second-order matrices.\nOrthogonal tensor decomposition Given a tensor T with an orthogonal decomposition, the orthogonal tensor decomposition problem asks to find the individual components a1, ..., ad. This is a central problem in learning many latent variable models, including Hidden Markov Model, multi-view models, topic models, mixture of Gaussians and Independent Component Analysis (ICA). See the discussion and citations in [13]. Orthogonal tensor decomposition problem can be solved by many algorithms even when the input is a noisy estimation T̃ ≈ T [77, 105, 13]. In practice this approach has been successfully applied to ICA [49], topic models [171] and community detection [87].\n11"
    }, {
      "heading" : "1.3 Background and Related Works",
      "text" : ""
    }, {
      "heading" : "1.3.1 Online Stochastic Gradient for Tensor Decomposition",
      "text" : "Stochastic gradient descent is one of the basic algorithms in optimization. It is often used to solve the following stochastic optimization problem\nw = arg min w∈Rd f(w), where f(w) = Ex∼D[φ(w, x)] (1.2)\nHere x is a data point that comes from some unknown distribution D, and φ is a loss function that is defined for a pair (x, w) of sample and parameters. We hope to minimize the expected loss E[φ(w, x)].\nWhen the function f(w) is convex, convergence of stochastic gradient descent is well-understood [147, 138]. However, the stochastic gradient descent is not only limited to convex functions. Especially, in the context of neural networks, the stochastic gradient descent is known as the “backpropagation” algorithm [141], and has been the main algorithm that underlies the success of deep learning [28]. However, the guarantees in the convex setting do not transfer to the non-convex settings.\nOptimizing a non-convex function is NP-hard in general. The difficulty comes from two aspects. First, the function may have many local minima, and it can be hard to find the best one (global minimum) among them. Second, even finding a local minimum can be hard as there can be many saddle points which have 0-gradient but are not local minima1. In the most general case, there is no known algorithm that guarantees to find a local minimum in a polynomial number of steps. The discrete analog (finding a local minimum in domains like {0, 1}n) has been studied in complexity theory and is PLS-complete [96]. 1See Section 2.2 for the definition of saddle points.\n12\nIn many cases, especially in those related to deep neural networks [53] [43], the main bottleneck in optimization is not due to local minima, but the existence of many saddle points. Gradient-based algorithms are in particular susceptible to saddle point problems as they only rely on the gradient information. The saddle point problem is alleviated for second-order methods that also rely on the Hessian information [53].\nHowever, using Hessian information usually increases the memory requirement and computation time per iteration. As a result, many applications still use stochastic gradient and empirically get reasonable results. In this paper we investigate why stochastic gradient methods can be effective even in presence of saddle point, in particular, we answer the following question:\nQuestion: Given a non-convex function f with many saddle points, what properties of f will guarantee stochastic gradient descent to converge to a local minimum efficiently?\nWe identify a property of non-convex functions which we call strict saddle. Intuitively, it guarantees local progress if we have access to the Hessian information. Surprisingly we show that, with only first order (gradient) information, the stochastic gradient escape from the saddle points efficiently. We provide a framework for analyzing stochastic gradient in both unconstrained and equality-constrained case using this property.\nWe apply our framework to orthogonal tensor decomposition, which is a core problem in learning many latent variable models. The tensor decomposition problem is inherently susceptible to the saddle point issues, as the problem asks to find d different components and any permutation of the true components yields a valid solution. Such symmetry creates exponentially many local minima and saddle points in the optimization problem. Using our new analysis of stochastic gradient, we give the first online algorithm for orthogonal tensor decomposition with global convergence guarantee. This is a key step towards making tensor decomposition algorithms more scalable.\n13\nRelaxed notions of convexity In optimization theory and economics, there are extensive works on understanding functions that behave similarly to convex functions (and in particular can be optimized efficiently). Such notions involve pseudo-convexity [117], quasi-convexity [104], invexity[75] and their variants. More recently there are also works that consider classes that admit more efficient optimization procedures like RSC (restricted strong convexity) [3]. Although these classes involve functions that are non-convex, the function (or at least the function restricted to the region of analysis) still has a unique stationary point that is the desired local/global minimum. Therefore, these works cannot be used to prove global convergence for problems like tensor decomposition, where there are exponentially many local minima and saddle points by the symmetry of the problem.\nSecond-order algorithms The most popular second-order method is the Newton’s method. Although Newton’s method converges fast near a local minimum, its global convergence properties are less understood in the more general case. For non-convex functions, [63] gave a concrete example where second-order method converges to the desired local minimum in a polynomial number of steps (interestingly the function of interest is trying to find one component in a 4th order orthogonal tensor, which is a simpler case of our application). As Newton’s method often converges also to saddle points, to avoid this behavior, different trusted-region algorithms are applied [53].\nStochastic gradient and symmetry The tensor decomposition problem we consider in this paper has the following symmetry: the solution is a set of d vectors v1, ..., vd. If (v1, v2, ..., vd) is a solution, then for any permutation π and any sign flips κ ∈ {±1}d, (.., κivπ(i), ...) is also a valid solution. In general, symmetry is known to generate saddle points, and variants of gradient descent often perform reasonably in these cases (see [143], [139], [92]). The settings in these work are different from ours, and none of them give bounds on number of steps required for convergence.\n14\nMany other problems have the same symmetric structure as the tensor decomposition problem, including the sparse coding problem [132] and many deep learning applications [28]. In these problems, the goal is to learn multiple “features” where the solution is invariant under permutation. Note that there are many recent papers on iterative/gradient-based algorithms for problems related to matrix factorization [93, 145]. These problems often have very different symmetry, as if Y = AX then for any invertible matrix R we know Y = (AR)(R−1X). In this case, all the equivalent solutions are in a connected low dimensional manifold, and there need not be saddle points between them."
    }, {
      "heading" : "1.3.2 Applying Online Tensor Methods for Learning Latent Vari-",
      "text" : "able Models\nThe spectral or moment-based approach involves decomposition of certain empirical moment tensors, estimated from observed data to obtain the parameters of the proposed probabilistic model. Unsupervised learning for a wide range of latent variable models can be carried out efficiently via tensor-based techniques with low sample and computational complexities [10]. In contrast, usual methods employed in practice such as expectation maximization (EM) and variational Bayes do not have such consistency guarantees. While the previous works [8] focused on theoretical guarantees, in chapter 3 of this thesis, we focus on the implementation of the tensor methods, study its performance on several datasets.\nWe introduce an online tensor decomposition based approach for two latent variable modeling problems namely, (1) community detection, in which we learn the latent communities that the social actors in social networks belong to, and (2) topic modeling, in which we infer hidden topics of text articles. We consider decomposition of moment tensors using stochastic gradient descent. We conduct optimization of multilinear operations in SGD and avoid directly forming the tensors, to save computational and storage costs. We present opti-\n15\nmized algorithm in two platforms. Our GPU-based implementation exploits the parallelism of SIMD architectures to allow for maximum speed-up by a careful optimization of storage and data transfer, whereas our CPU-based implementation uses efficient sparse matrix computations and is suitable for large sparse data sets. For the community detection problem, we demonstrate accuracy and computational efficiency on Facebook, Yelp, and DBLP data sets, and for the topic modeling problem, we also demonstrate good performance on the New York Times data set. We compare our results to the state-of-the-art algorithms such as the variational method and report a gain of accuracy and a gain of several orders of magnitude in the execution time.\nChapter 3 builds on the recent works of Anandkumar et al [10, 8] which establishes the correctness of tensor-based approaches for learning MMSB [5] models and other latent variable models. While, the earlier works provided a theoretical analysis of the method, the current paper considers a careful implementation of the method. Moreover, there are a number of algorithmic improvements in this thesis. For instance, while [10, 8] consider tensor power iterations, based on batch data and deflations performed serially, here, we adopt a stochastic gradient descent approach for tensor decomposition, which provides the flexibility to tradeoff sub-sampling with accuracy. Moreover, we use randomized methods for dimensionality reduction in the preprocessing stage of our method which enables us to scale our method to graphs with millions of nodes.\nThere are other known methods for learning the stochastic block model based on techniques such as spectral clustering [120] and convex optimization [39]. However, these methods are not applicable for learning overlapping communities. We note that learning the mixed membership model can be reduced to a matrix factorization problem [169]. While collaborative filtering techniques such as [126, 144] focus on matrix factorization and the prediction accuracy of recommendations on an unseen test set, we recover the underlying latent com-\n16\nmunities, which helps with the interpretability, and the statistical model can be employed for other tasks.\nAlthough there have been other fast implementations for community detection before [152, 112], these methods are not statistical and do not yield descriptive statistics such as bridging nodes [129], and cannot perform predictive tasks such as link classification which are the main strengths of the MMSB model. With the implementation of our tensor-based approach, we record huge speed-ups compared to existing approaches for learning the MMSB model.\nTo the best of our knowledge, while stochastic methods for matrix decomposition have been considered earlier [130, 18], this is the first work incorporating stochastic optimization for tensor decomposition, and paves the way for further investigation on many theoretical and practical issues. We also note that we never explicitly form or store the subgraph count tensor, of size O(n3) where n is the number of nodes, in our implementation, but directly manipulate the neighborhood vectors to obtain tensor decompositions through stochastic updates. This is a crucial departure from other works on tensor decompositions on GPUs [25, 146], where the tensor needs to be stored and manipulated directly."
    }, {
      "heading" : "1.3.3 Dictionary Learning through Convolutional Tensor Decom-",
      "text" : "position\nFeature or representation learning forms a cornerstone of modern machine learning. Representing the data in the relevant feature space is critical to obtaining good performance in challenging machine learning tasks in speech, computer vision and natural language processing. A popular representation learning framework is based on dictionary learning. Here, the input data is modeled as a linear combination of dictionary elements. However, this model fails to incorporate natural domain-specific invariances such as shift invariance and results in highly redundant dictionary elements, which makes inference in these models expensive.\n17\nThese shortcomings can be remedied by incorporating invariances into the dictionary model, and such models are known as convolutional models. Convolutional models are ubiquitous in machine learning for image, speech and sentence representations [167, 101, 33], and in neuroscience for modeling neural spike trains [131, 58]. Deep convolutional neural networks are a multi-layer extension of these models with non-linear activations. Such models have revolutionized performance in image, speech and natural language processing [167, 97]. The convolutional dictionary learning model posits that the input signal x is generated as a linear combination of convolutions of unknown dictionary elements or filters f ∗1 , . . . f ∗ L and unknown activation maps w∗1, . . . w ∗ L:\nx = ∑\ni∈[L] f ∗i ∗w∗i , (1.3)\nwhere [L] := 1, . . . , L. The vector w∗i denotes the activations at locations, where the corresponding filter f ∗i is active.\nIn order to learn the model in (1.3), usually a square loss reconstruction criterion is employed:\nmin fi,wi:‖fi‖=1\n‖x− ∑\ni∈[L] fi∗wi‖2. (1.4)\nThe constraints (‖fi‖ = 1) are enforced, since otherwise, the scaling can be exchanged between the filters fi and the activation maps wi. Also, an additional regularization term (for example an ℓ1 term on the w ′ is) is usually added to the above objective to promote sparsity on wi.\nA popular heuristic for solving (1.4) is based on alternating minimization [34], where the filters fi are optimized, while keeping the activations wi fixed, and vice versa. Each alternating update can be solved efficiently (since it is linear in each of the variables). However, the method is computationally expensive in the large sample setting since each iteration re-\n18\nquires a pass over all the samples, and in modern machine learning applications, the number of samples can run into billions. Moreover, alternating minimization has multiple spurious local optima, and reaching the global optimum of (1.4) is NP-hard in general. This problem is severely amplified in the convolutional setting due to additional symmetries, compared to the usual dictionary learning setting (without the convolutional operation). Due to shift invariance of the convolutional operator, shifting a filter fi by some amount, and applying a corresponding negative shift on the activation wi leaves the objective in (1.4) unchanged. Can we design alternative methods for convolutional dictionary learning that are scalable to huge datasets?\nThe special case of (1.3) with one filter (L = 1) is a well studied problem, and is referred to as blind deconvolution [90]. In general, this problem is not identifiable, i.e. multiple equivalent solutions can exist [44]. It has been documented that in many cases alternating minimization produces trivial solutions, where the filter f = x is the signal itself and the activation is the identity function [116]. Therefore, alternative techniques have been proposed, such as convex programs, based on nuclear norm minimization [4] and imposing hierarchical Bayesian priors for activation maps [163]. However, there is no analysis for settings with more than one filter. Incorporating Bayesian priors has shown to reduce the number of local optima, but not eliminate them [163, 109]. Moreover, Bayesian techniques are in general more expensive than alternating minimization.\nThe extension of blind deconvolution to multiple filters is known as convolutive blind source separation or convolutive independent component analysis (ICA) [90]. Previous methods directly reformulate convolutive ICA as an ICA model, without incorporating the shift constraints. Moreover, reformulation leads to an increase in the number of hidden sources from L to nL in the new model, where n is the input dimension, which is harder to separate and computationally more expensive. Other methods are based on performing ICA in the Fourier domain, but the downside is that the new mixing matrix depends on the angular fre-\n19\nquency, and leads to permutation and sign indeterminacies of the sources across frequencies. Complicated interpolation methods [90] overcome these indeterminacies. In contrast, our method avoids all these issues. We do not perform Fourier transform on the input. Instead, we employ FFTs at different iterations of our method to estimate the filters efficiently.\nThe dictionary learning problem without convolution has received much attention. Recent results show that simple iterative methods can learn the globally optimal solution [2, 19]. Also, tensor decomposition methods provably learn the model, when the activations are independently drawn (the ICA model) [12] or are sparse (the sparse coding model) [14]. In this work, we extend the tensor decomposition methods to efficiently incorporate the shift invariance constraints imposed by the convolution operator. This framework is applied to word-sequence embedding learning in natural language processing.\nWe have recently witnessed the tremendous success of word embeddings or word vector representations in natural language processing. This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space [29, 47, 48, 124, 136]. Word embeddings have attained state-of-the-art performance in tasks such as part-of-speech (POS) tagging, chunking, named entity recognition (NER), and semantic role labeling. Despite this impressive performance, word embeddings do not suffice for more advanced tasks which require context-aware information or word orders, e.g. paraphrase detection, sentiment analysis, plagiarism detection, information retrieval and machine translation. Therefore, extracting word-sequence vector representations is crucial for expanding the realm of automated text understanding.\nPrevious works on word-sequence embeddings are based on a variety of mechanisms. A popular method is to learn the composition operators in sequences [125, 166]. The complexity of the compositionality varies widely: from simple operations such as addition [125, 166] to complicated recursive neural networks [149, 150, 27], convolutional neural networks [97, 97], long short-term memory (LSTM) recurrent neural networks [154], or combinations of\n20\nthese architectures [161]. All these methods produce sentence representations that depend on a supervised task, and the class labels are back-propagated to update the composition weights [98].\nSince the above methods rely heavily on the downstream task and the domain of the training samples, they can hardly be used as universal embeddings across domains, and require intensive pre-training and hyper-parameter tuning. The state-of-the-art unsupervised framework is Skip-thought [103], based on an objective function that abstracts the skip-gram model to the sentence level, and encodes a sentence to predict the sentences around it. However, the skip-thought model requires a large corpus of contiguous text, such as the book corpus with more than 74 million sentences. Can we instead efficiently learn sentence embeddings using small amounts of samples without supervision/labels or annotated features(such as parse trees)? Also, can the sentence embeddings be context-aware, can handle variable lengths, and is not limited to specific domains?\nWe propose an unsupervised ConvDic+DeconvDec framework that satisfies all the above constraints. It is composed of two phases, a comprehension phase which summarizes template phrases using convolutional dictionary elements, followed by a feature-extraction phase which extracts activations using deconvolutional decoding. We propose a novel learning algorithm for the comprehension phase based on convolutional tensor decomposition. Note that in the comprehension phase, phrase templates are learned over fixed length small patches (patch length is equal to phrase template length), whereas entire word-sequence is decoded to get the final word-sequence embedding in the feature-extraction phase.\nWe employ our sentence embeddings in the tasks of sentiment classification, semantic textual similarity estimation, and paraphrase detection over eight datasets from various domains. These are challenging tasks since they require a contextual understanding of text relationships rather than bags of words. We learn the embeddings from scratch without using any auxiliary information. While previous works use information such as parse trees, Wordnet\n21\nor pre-train on a much larger corpus, we train from scratch on small amounts of text and obtain competitive results, which are close or even better than the state-of-the-art.\nThis is due to the combination of efficient modeling and learning approaches in our work. The convolutional model incorporates word orders and phrase representations, and our tensor decomposition algorithm can efficiently learn a set of parameters (phrase templates) for the convolutional model."
    }, {
      "heading" : "1.3.4 Latent Tree Model Learning through Hierarchical Tensor",
      "text" : "Decomposition\nLatent variable graphical models span flat models and hierarchical models, see Figure 1.10 for a flat multi-view model and a hierarchical model. Latent tree graphical models are a popular class of latent variable models, where a probability distribution involving observed and hidden variables are Markovian on a tree. Due to the fact that structure of (observable and hidden) variable interactions are approximated as a tree, inference on latent trees can be carried out exactly through a simple belief propagation [134]. Therefore, latent tree graphical models present a good trade-off between model accuracy and computational complexity. They are applicable in many domains, where it is natural to expect hierarchical or sequential relationships among the variables (through a hidden-Markov model). For instance, latent tree models have been employed for phylogenetic reconstruction [56], object recognition [40], [42] and human pose estimation [157].\nThe task of learning a latent tree model consists of two parts: learning the tree structure, and learning the parameters of the tree. There exist many challenges which prohibit efficient or guaranteed learning of the latent tree graphical model, which will be addressed in this thesis:\n22\n1. The location and the number of latent variables are hidden, and the marginalized graph\nover the observable variables no longer conforms to a tree structure.\n2. Structure learning algorithms are typically of computational complexity polynomial\nwith p (number of variables) as discussed in [6, 41]. These methods are serial in nature and therefore are not scalable for large p. 3. Parameter estimation in latent tree models is typically carried out through Expecta-\ntion Maximization (EM) or other local search heuristics [41]. These methods have no consistency guarantees, suffer from the problem of local optima and are not easily parallelizable. 4. Typically structure learning and parameter estimation are carried out one after an-\nother.\nThere has been widespread interest in developing distributed learning techniques, e.g., the recent works of [148] and [160]. These works consider parameter estimation via likelihoodbased optimizations such as Gibbs sampling, while our method involves more challenging tasks where both the structure and the parameters are estimated. Simple methods such as local neighborhood selection through ℓ1-regularization [121] or local conditional independence testing [16] can be parallelized, but these methods do not incorporate hidden variables. Finally, note that the latent tree models provide a statistical description, in addition to\n23\nrevealing the hierarchy. In contrast, hierarchical clustering techniques are not based on a statistical model [108] and cannot provide valuable information such as the level of correlation between observed and hidden variables."
    }, {
      "heading" : "1.4 Thesis Structure",
      "text" : "In my thesis, I will first prove that simple noisy gradient descent on a carefully selected objective function yields global convergence guarantee in chapter 2. Based on the theoretical guarantees, I will show how to make tensor decomposition highly scalable, highly parallel in chapter 3. Furthermore, I extend the framework to learn dictionary or templates with additional constraints such as shift invariance in image or text dictionary learning using convolutional dictionary tensor decomposition in chapter 4. I do not limit myself to shallow models where observations are conditional independent on the hidden dimension. On the contrary, I extend the multi-view tensor decomposition framework to a hierarchical tensor decomposition framework to analyze data with complicated hierarchical structure. A latent tree model is therefore proposed in chapter 5, where latent variable graphical model structure learning technique is combined with hierarchical tensor decomposition for a consistent learning of the hierarchical model structure and parameter. Finally, I conclude my thesis with a challenging but important task in chapter 6, discovering cell types in the brain. This work brings together the techniques used in all previous chapters, such as image processing to extract cells and cell features from brain slices, learning a point process admixture model.\n24\nChapter 2"
    }, {
      "heading" : "Online Stochastic Gradient for Tensor",
      "text" : ""
    }, {
      "heading" : "Decomposition",
      "text" : "It is established in the previous work [13] that a wide class of latent variable graphical models can be learned through tensor decomposition, and model parameters are obtained by decomposing higher order data aggregates or modified data moments. Therefore, learning latent variable graphical model is reduced to tensor decomposition problem. Tensor decomposition is a non-convex optimization problem, and it is known that non-convex optimization problem is NP hard in general. Now the question is: could we use efficient methods such as stochastic gradient descent to reach local optima for a class of function under mild conditions? Could we fit tensor decomposition problem into the class of function?\nWe analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in saddle points. In this chapter we identify strict saddle property for non-convex problem that allows for efficient optimization. Using this property we show that from an arbitrary starting point, stochastic gradient descent converges to a\n25\nlocal minimum in a polynomial number of iterations. To the best of our knowledge this is the first work that gives global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points.\nOur analysis can be applied to orthogonal tensor decomposition, which is widely used in learning a rich class of latent variable models. We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee."
    }, {
      "heading" : "Strict saddle functions",
      "text" : "Given a function f(w) that is twice differentiable, we call w a stationary point if ∇f(w) = 0. A stationary point can either be a local minimum, a local maximum or a saddle point. We identify an interesting class of non-convex functions which we call strict saddle. For these functions the Hessian of every saddle point has a negative eigenvalue. In particular, this means that local second-order algorithms which are similar to the ones in [53] can always make some progress.\nIt may seem counter-intuitive why stochastic gradient can work in these cases: in particular if we run the basic gradient descent starting from a stationary point then it will not move. However, we show that the saddle points are not stable and that the randomness in stochastic gradient helps the algorithm to escape from the saddle points.\nTheorem 2.1 (informal). Suppose f(w) is strict saddle (see Definition 2.3), Noisy Gradient Descent (Algorithm 1) outputs a point that is close to a local minimum in polynomial number of steps.\nOnline tensor decomposition Requiring all saddle points to have a negative eigenvalue may seem strong, but it already allows non-trivial applications to natural non-convex optimization\n26\nproblems. As an example, we consider the orthogonal tensor decomposition problem. This problem is the key step in spectral learning for many latent variable models.\nWe design a new objective function for tensor decomposition that is strict saddle.\nTheorem 2.2. Given random variables X such that T = E[g(X)] ∈ Rd4 is an orthogonal 4-th order tensor, there is an objective function f(w) = E[φ(w,X)] w ∈ Rd×d such that every local minimum of f(w) corresponds to a valid decomposition of T . Further, function f is strict saddle.\nCombining this new objective with our framework for optimizing strict saddlefunctions, we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee."
    }, {
      "heading" : "2.1 Preliminaries",
      "text" : "The stochastic gradient aims to solve the stochastic optimization problem (1.2), which we restate here:\nw = arg min w∈Rd f(w), where f(w) = Ex∼D[φ(w, x)].\nRecall φ(w, x) denotes the loss function evaluated for sample x at point w. The algorithm follows a stochastic gradient\nwt+1 = wt − η∇wtφ(wt, xt), (2.1)\nwhere xt is a random sample drawn from distribution D and η is the learning rate.\nIn the more general setting, stochastic gradient descent can be viewed as optimizing an arbitrary function f(w) given a stochastic gradient oracle.\n27\nDefinition 2.1. For a function f(w) : Rd → R, a function SG(w) that maps a variable to a random vector in Rd is a stochastic gradient oracle if E[SG(w)] = ∇f(w) and ‖SG(w)− ∇f(w)‖ ≤ Q.\nIn this case the update step of the algorithm becomes wt+1 = wt − ηSG(wt).\nSmoothness and Strong Convexity Traditional analysis for stochastic gradient often assumes the function is smooth and strongly convex. A function is β-smooth if for any two points w1, w2,\n‖∇f(w1)−∇f(w2)‖ ≤ β‖w1 − w2‖. (2.2)\nWhen f is twice differentiable this is equivalent to assuming that the spectral norm of the Hessian matrix is bounded by β. We say a function is α-strongly convex if the Hessian at any point has smallest eigenvalue at least α (λmin(∇2f(w)) ≥ α).\nUsing these two properties, previous work [138] shows that stochastic gradient converges at a rate of 1/t. In this thesis we consider non-convex functions, which can still be β-smooth but cannot be strongly convex.\nSmoothness of Hessians It is common to assume the Hessian of the function f to be smooth. We say a function f(w) has ρ-Lipschitz Hessian if for any two points w1, w2 we have\n‖∇2f(w1)−∇2f(w2)‖ ≤ ρ‖w1 − w2‖. (2.3)\nThis is a third order condition that is true if the third order derivative exists and is bounded.\n28"
    }, {
      "heading" : "2.2 Stochastic Gradient Descent for Strict saddle Func-",
      "text" : "tion\nIn this section we discuss the properties of saddle points, and show if all the saddle points are well-behaved then stochastic gradient descent finds a local minimum for a non-convex function in polynomial time.\nNotation Throughout the chapter we use [d] to denote set {1, 2, ..., d}. We use ‖ · ‖ to denote the ℓ2 norm of vectors and spectral norm of matrices. For a matrix we use λmin to denote its smallest eigenvalue. For a function f : Rd → R, ∇f and ∇2f denote its gradient vector and Hessian matrix."
    }, {
      "heading" : "2.2.1 Strict saddle Property",
      "text" : "For a twice differentiable function f(w), we call a point stationary point if its gradient is equal to 0. Stationary points could be local minima, local maxima or saddle points. By local optimality conditions [164], in many cases we can tell what type a point w is by looking at its Hessian: if ∇2f(w) is positive definite then w is a local minimum; if ∇2f(w) is negative definite then w is a local maximum; if ∇2f(w) has both positive and negative eigenvalues then w is a saddle point. These criteria do not cover all the cases as there could be degenerate scenarios: ∇2f(w) can be positive semidefinite with an eigenvalue equal to 0, in which case the point could be a local minimum or a saddle point.\nIf a function does not have these degenerate cases, then we say the function is strict saddle:\nDefinition 2.2. A twice differentiable function f(w) is strict saddle, if all its local minima have ∇2f(w) ≻ 0 and all its other stationary points satisfy λmin(∇2f(w)) < 0.\n29\nIntuitively, if we are not at a stationary point, then we can always follow the gradient and reduce the value of the function. If we are at a saddle point, we need to consider a second order Taylor expansion:\nf(w +∆w) ≈ w + (∆w)T∇2f(w)(∆w) +O(‖∆w‖3).\nSince the strict saddle property guarantees ∇2f(w) to have a negative eigenvalue, there is always a point that is near w and has strictly smaller function value. It is possible to make local improvements as long as we have access to second order information. However it is not clear whether the more efficient stochastic gradient updates can work in this setting.\nTo make sure the local improvements are significant, we use a robust version of the strict saddle property:\nDefinition 2.3. A twice differentiable function f(w) is (α, γ, ǫ, δ)-strict saddle, if for any point w at least one of the following is true\n1. ‖∇f(w)‖ ≥ ǫ.\n2. λmin(∇2f(w)) ≤ −γ.\n3. There is a local minimum w⋆ such that ‖w−w⋆‖ ≤ δ, and the function f(w′) restricted\nto 2δ neighborhood of w⋆ (‖w′ − w⋆‖ ≤ 2δ) is α-strongly convex.\nIntuitively, this condition says for any point whose gradient is small, it is either close to a robust local minimum, or is a saddle point (or local maximum) with a significant negative eigenvalue.\nWe purpose a simple variant of stochastic gradient algorithm, where the only difference to the traditional algorithm is we add an extra noise term to the updates. The main benefit of this additional noise is that we can guarantee there is noise in every direction, which allows the\n30\nProcedure 1 Noisy Stochastic Gradient Input: Stochastic gradient oracle SG(w), initial point w0, desired accuracy κ. Output: wt that is close to some local minimum w\n⋆. 1: Choose η = min{Õ(κ2/ log(1/κ)), ηmax} 2: for t = 0 to Õ(1/η2) do 3: Sample noise n uniformly from unit sphere. 4: wt+1 ← wt − η(SG(w) + n)\nalgorithm to effectively explore the local neighborhood around saddle points. If the noise from stochastic gradient oracle already has nonnegligible variance in every direction, our analysis also applies without adding additional noise. We show noise can help the algorithm escape from saddle points and optimize strict saddle functions.\nTheorem 2.3 (Main Theorem). Suppose a function f(w) : Rd → R that is (α, γ, ǫ, δ)-strict saddle, and has a stochastic gradient oracle with radius at most Q. Further, suppose the function is bounded by |f(w)| ≤ B, is β-smooth and has ρ-Lipschitz Hessian. Then there exists a threshold ηmax = Θ̃(1), so that for any ζ > 0, and for any η ≤ ηmax/max{1, log(1/ζ)}, with probability at least 1− ζ in t = Õ(η−2 log(1/ζ)) iterations, Algorithm 1 (Noisy Gradient Descent) outputs a point wt that is Õ( √ η log(1/ηζ))-close to some local minimum w⋆.\nHere (and throughout the rest of the chapter) Õ(·) (Ω̃, Θ̃) hides the factor that is polynomially dependent on all other parameters (including Q, 1/α, 1/γ, 1/ǫ, 1/δ, B, β, ρ, and d), but independent of η and ζ . So it focuses on the dependency on η and ζ . Our proof technique can give explicit dependencies on these parameters however we hide these dependencies for simplicity of presentation. 1\nRemark (Decreasing learning rate). Often analysis of stochastic gradient descent uses decreasing learning rates and the algorithm converges to a local (or global) minimum. Since the function is strongly convex in the small region close to local minimum, we can use Theorem 2.3 to first find a point that is close to a local minimum, and then apply standard analysis\n1 Currently, our number of iteration is a large polynomial in the dimension d. We have not tried to optimize the degree of this polynomial. Empirically the dependency on d is much better, whether the dependency on d can be improved to poly log d is left as an open problem.\n31\nof SGD in the strongly convex case (where we decrease the learning rate by 1/t and get 1/ √ t convergence in ‖w − w⋆‖).\nIn the next part we sketch the proof of the main theorem. Details are deferred to Appendix A.1."
    }, {
      "heading" : "2.2.2 Proof Sketch",
      "text" : "In order to prove Theorem 2.3, we analyze the three cases in Definition 2.3. When the gradient is large, we show the function value decreases in one step (see Lemma 2.1); when the point is close to a local minimum, we show with high probability it cannot escape in the next polynomial number of iterations (see Lemma 2.2).\nLemma 2.1 (Gradient). Under the assumptions of Theorem 2.3, for any point with ‖∇f(wt)‖ ≥ C√η (where C = Θ̃(1)) and C√η ≤ ǫ, after one iteration we have E[f(wt+1)] ≤ f(wt)− Ω̃(η2).\nThe proof of this lemma is a simple application of the smoothness property.\nLemma 2.2 (Local minimum). Under the assumptions of Theorem 2.3, for any point wt that is Õ( √ η) < δ close to local minimum w⋆, in Õ(η−2 log(1/ζ)) number of steps all future wt+i’s are Õ( √ η log(1/ηζ))-close with probability at least 1− ζ/2.\nThe proof of this lemma is similar to the standard analysis [138] of stochastic gradient descent in the smooth and strongly convex setting, except we only have local strong convexity. The proof appears in Appendix A.1.\nThe hardest case is when the point is “close” to a saddle point: it has gradient smaller than ǫ and smallest eigenvalue of the Hessian bounded by −γ. In this case we show the noise in our algorithm helps the algorithm to escape:\n32\nLemma 2.3 (Saddle point). Under the assumptions of Theorem 2.3, for any point wt where ‖∇f(wt)‖ ≤ C√η (for the same C as in Lemma 2.1), and λmin(∇2f(wt)) ≤ −γ, there is a number of steps T that depends on wt such that E[f(wt+T )] ≤ f(wt)− Ω̃(η). The number of steps T has a fixed upper bound Tmax that is independent of wt where T ≤ Tmax = Õ(1/η).\nIntuitively, at point wt there is a good direction that is hiding in the Hessian. The hope of the algorithm is that the additional (or inherent) noise in the update step makes a small step towards the correct direction, and then the gradient information will reinforce this small perturbation and the future updates will “slide” down the correct direction.\nTo make this more formal, we consider a coupled sequence of updates w̃ such that the function to minimize is just the local second order approximation\nf̃(w) = f(wt) +∇f(wt)T (w − wt) + 1\n2 (w − wt)T∇2f(wt)(w − wt).\nThe dynamics of stochastic gradient descent for this quadratic function is easy to analyze as w̃t+i can be calculated analytically. Indeed, we show the expectation of f̃(w̃) will decrease. More concretely we show the point w̃t+i will move substantially in the negative curvature directions and remain close to wt in positive curvature directions. We then use the smoothness of the function to show that as long as the points did not go very far from wt, the two update sequences w̃ and w will remain close to each other, and thus f̃(w̃t+i) ≈ f(wt+i). Finally we prove the future wt+i’s (in the next T steps) will remain close to wt with high probability by Martingale bounds. The detailed proof appears in Appendix A.1.\nWith these three lemmas it is easy to prove the main theorem. Intuitively, as long as there is a small probability of being Õ( √ η)-close to a local minimum, we can always apply Lemma 2.1 or Lemma 2.3 to make the expected function value decrease by Ω̃(η) in at most Õ(1/η) iterations, this cannot go on for more than Õ(1/η2) iterations because in that case\n33\nthe expected function value will decrease by more than 2B, but max f(x)−min f(x) ≤ 2B by our assumption. Therefore in Õ(1/η2) steps with at least constant probability wt will become Õ( √ η)-close to a local minimum. By Lemma 2.2 we know once it is close it will almost always stay close, so after q epochs of Õ(1/η2) iterations each, the probability of success will be 1 − exp(−Ω(q)). Taking q = O(log(1/ζ)) gives the result. More details appear in Appendix A.1."
    }, {
      "heading" : "2.2.3 Constrained Problems",
      "text" : "In many cases, the problem we are facing are constrained optimization problems. In this part we briefly describe how to adapt the analysis to problems with equality constraints (which suffices for the tensor application). Dealing with general inequality constraint is left as future work.\nFor a constrained optimization problem:\nmin w∈Rd\nf(w) (2.4)\ns.t. ci(w) = 0, i ∈ [m]\nin general we need to consider the set of points in a low dimensional manifold that is defined by the constraints. In particular, in the algorithm after every step we need to project back to this manifold (see Algorithm 2 where ΠW is the projection to this manifold).\nProcedure 2 Projected Noisy Stochastic Gradient Input: Stochastic gradient oracle SG(w), initial point w0, desired accuracy κ. Output: wt that is close to some local minimum w\n⋆. 1: Choose η = min{Õ(κ2/ log(1/κ)), ηmax} 2: for t = 0 to Õ(1/η2) do 3: Sample noise n uniformly from unit sphere. 4: vt+1 ← wt − η(SG(w) + n) 5: wt+1 = ΠW(vt+1)\n34\nFor constrained optimization it is common to consider the Lagrangian:\nL(w, λ) = f(w)− m∑\ni=1\nλici(w). (2.5)\nUnder common regularity conditions, it is possible to compute the value of the Lagrangian multipliers:\nλ∗(w) = argmin λ ‖∇wL(w, λ)‖.\nWe can also define the tangent space, which contains all directions that are orthogonal to all the gradients of the constraints: T (w) = {v : ∇ci(w)Tv = 0; i = 1, · · · , m}. In this case the corresponding gradient and Hessian we consider are the first-order and second-order partial derivative of Lagrangian L at point (w, λ∗(w)):\nχ(w) = ∇wL(w, λ)|(w,λ∗(w)) = ∇f(w)− m∑\ni=1\nλ∗i (w)∇ci(w) (2.6)\nM(w) = ∇2wwL(w, λ)|(w,λ∗(w)) = ∇2f(w)− m∑\ni=1\nλ∗i (w)∇2ci(w) (2.7)\nWe replace the gradient and Hessian with χ(w) andM(w), and when computing eigenvectors of M(w) we focus on its projection on the tangent space. In this way, we can get a similar definition for strict saddle (see Appendix A.2), and the following theorem.\nTheorem 2.4. (informal) Under regularity conditions and smoothness conditions, if a constrained optimization problem satisfies strict saddle property, then for a small enough η, in Õ(η−2 log 1/ζ) iterations Projected Noisy Gradient Descent (Algorithm 2) outputs a point w that is Õ( √ η log(1/ηζ)) close to a local minimum with probability at least 1− ζ.\nDetailed discussions and formal version of this theorem are deferred to Appendix A.2.\n35"
    }, {
      "heading" : "2.3 Online Tensor Decomposition",
      "text" : "In this section we describe how to apply our stochastic gradient descent analysis to tensor decomposition problems. We first give a new formulation of tensor decomposition as an optimization problem, and show that it satisfies the strict saddle property. Then we explain how to compute stochastic gradient in a simple example of Independent Component Analysis (ICA) [91]."
    }, {
      "heading" : "2.3.1 Optimization Problem for Tensor Decomposition",
      "text" : "Given a tensor T ∈ Rd4 that has an orthogonal decomposition\nT =\nd∑\ni=1\na⊗4i , (2.8)\nwhere the components ai’s are orthonormal vectors (‖ai‖ = 1, aTi aj = 0 for i 6= j), the goal of orthogonal tensor decomposition is to find the components ai’s. This problem has inherent symmetry: for any permutation π and any set of κi ∈ {±1}, i ∈ [d], we know ui = κiaπ(i) is also a valid solution. This symmetry property makes the natural optimization problems non-convex.\nIn this section we will give a new formulation of orthogonal tensor decomposition as an optimization problem, and show that this new problem satisfies the strict saddle property. Previously, [63] solves the problem of finding one component, with the following objective function\nmax ‖u‖2=1 T (u, u, u, u). (2.9)\n36\nIn Appendix A.3.1, as a warm-up example we show this function is indeed strict saddle, and we can apply Theorem 2.4 to prove global convergence of stochastic gradient descent algorithm.\nIt is possible to find all components of a tensor by iteratively finding one component, and do careful deflation, as described in [13] or [20]. However, in practice the most popular approaches like Alternating Least Squares [50] or FastICA [89] try to use a single optimization problem to find all the components. Empirically these algorithms are often more robust to noise and model misspecification.\nThe most straight-forward formulation of the problem aims to minimize the reconstruction error\nmin ∀i,‖ui‖2=1\n‖T − d∑\ni=1\nu⊗4i ‖2F . (2.10)\nHere ‖ · ‖F is the Frobenius norm of the tensor which is equal to the ℓ2 norm when we view the tensor as a d4 dimensional vector. However, it is not clear whether this function satisfies the strict saddle property, and empirically stochastic gradient descent is unstable for this objective.\nWe propose a new objective that aims to minimize the correlation between different components:\nmin ∀i,‖ui‖2=1\n∑\ni 6=j T (ui, ui, uj, uj), (2.11)\nTo understand this objective intuitively, we first expand vectors uk in the orthogonal basis formed by {ai}’s. That is, we can write uk = ∑d i=1 zk(i)ai, where zk(i) are scalars that correspond to the coordinates in the {ai} basis. In this way we can rewrite T (uk, uk, ul, ul) =\n37\n∑d i=1(zk(i)) 2(zl(i)) 2. From this form it is clear that the T (uk, uk, ul, ul) is always nonnegative, and is equal to 0 only when the support of zk and zl do not intersect. For the objective function, we know in order for it to be equal to 0 the z’s must have disjoint support. Therefore, we claim that {uk}, ∀k ∈ [d] is equivalent to {ai}, ∀i ∈ [d] up to permutation and sign flips when the global minimum (which is 0) is achieved.\nWe further show that this optimization program satisfies the strict saddle property and all its local minima in fact achieves global minimum value. The proof is deferred to Appendix A.3.2.\nTheorem 2.5. The optimization problem (2.11) is (α, γ, ǫ, δ)-strict saddle, for α = 1 and γ, ǫ, δ = 1/poly(d). Moreover, all its local minima have the form ui = κiaπ(i) for some κi = ±1 and permutation π(i).\nNote that we can also generalize this to handle 4th order tensors with different positive weights on the components, or other order tensors, see Appendix A.3.3."
    }, {
      "heading" : "2.3.2 Implementing Stochastic Gradient Oracle",
      "text" : "To design an online algorithm based on objective function (2.11), we need to give an implementation for the stochastic gradient oracle.\nIn applications, the tensor T is oftentimes the expectation of multilinear operations of samples g(x) over x where x is generated from some distribution D. In other words, for any x ∼ D, the tensor is T = E[g(x)]. Using the linearity of the multilinear map, we know E[g(x)](ui, ui, uj, uj) = E[g(x)(ui, ui, uj, uj)]. Therefore we can define the loss function φ(u, x) = ∑\ni 6=j g(x)(ui, ui, uj, uj), and the stochastic gradient oracle SG(u) = ∇uφ(u, x).\n38\nFor concreteness, we look at a simple ICA example. In the simple setting we consider an unknown signal x that is uniform2 in {±1}d, and an unknown orthonormal linear transformation3 A (AAT = I). The sample we observe is y := Ax ∈ Rd. Using standard techniques (see [35]), we know the 4-th order cumulant of the observed sample is a tensor that has orthogonal decomposition. Here for simplicity we don’t define 4-th order cumulant, instead we give the result directly.\nDefine tensor Z ∈ Rd4 as follows:\nZ(i, i, i, i) = 3, ∀i ∈ [d] Z(i, i, j, j) = Z(i, j, i, j) = Z(i, j, j, i) = 1, ∀i 6= j ∈ [d]\nwhere all other entries of Z are equal to 0. The tensor T can be written as a function of the auxiliary tensor Z and multilinear form of the sample y. Lemma 2.4. The expectation E[1 2 (Z − y⊗4)] = ∑di=1 a⊗4i = T , where ai’s are columns of the unknown orthonormal matrix A.\nThis lemma is easy to verify, and is closely related to cumulants [35]. Recall that φ(u, y) denotes the loss (objective) function evaluated at sample y for point u. Let φ(u, y) = ∑ i 6=j 1 2 (Z − y⊗4)(ui, ui, uj, uj). By Lemma 2.4, we know that E[φ(u, y)] is equal to the objective function as in Equation (2.11). Therefore we rewrite objective (2.11) as the following stochastic optimization problem\nmin ∀i,‖ui‖2=1\nE[φ(u, y)], where φ(u, y) = ∑\ni 6=j\n1 2 (Z − y⊗4)(ui, ui, uj, uj)\n2In general ICA the entries of x are independent, non-Gaussian variables. 3In general (under-complete) ICA this could be an arbitrary linear transformation, however usually after\nthe “whitening” step (see [35]) the linear transformation becomes orthonormal.\n39\nThe stochastic gradient oracle is then\n∇uiφ(u, y) = ∑\nj 6=i\n( 〈uj, uj〉ui + 2 〈ui, uj〉uj − 〈uj, y〉2 〈ui, y〉 y ) . (2.12)\nNotice that computing this stochastic gradient does not require constructing the 4-th order tensor T − y⊗4. In particular, this stochastic gradient can be computed very efficiently:\nRemark. The stochastic gradient (2.12) can be computed for all ui’s in O(d 3) time for one sample or O(d3 + d2k) for average of k samples.\nProof. The proof is straight forward as the first two terms on the right hand side take O(d3) and is shared by all samples. The third term can be efficiently computed once the innerproducts between all the y’s and all the ui’s are computed (which takes O(kd 2) time)."
    }, {
      "heading" : "2.4 Experiments",
      "text" : "We run simulations for Projected Noisy Gradient Descent (Algorithm 2) applied to orthogonal tensor decomposition. The results show that the algorithm converges from random initial points efficiently (as predicted by the theorems), and our new formulation (2.11) performs better than reconstruction error (2.10) based formulation.\nSettings We set dimension d = 10, the input tensor T is a random tensor in R10 4 that has orthogonal decomposition (1.1). The step size is chosen carefully for respective objective functions. The performance is measured by normalized reconstruction error E = ( ‖T −∑di=1 u⊗4i ‖2F ) /‖T‖2F .\nSamples and stochastic gradients We use two ways to generate samples and compute stochastic gradients. In the first case we generate sample x by setting it equivalent to d 1 4ai with\n40\nprobability 1/d. It is easy to see that E[x⊗4] = T . This is a very simple way of generating samples, and we use it as a sanity check for the objective functions.\nIn the second case we consider the ICA example introduced in Section 2.3.2, and use Equation (2.12) to compute a stochastic gradient. In this case the stochastic gradient has a large variance, so we use mini-batch of size 100 to reduce the variance.\nComparison of objective functions We use the simple way of generating samples for our new objective function (2.11) and reconstruction error objective (2.10). The result is shown in Figure 2.1. Our new objective function is empirically more stable (always converges within 10000 iterations); the reconstruction error do not always converge within the same number of iterations and often exhibits long periods with small improvement (which is likely to be caused by saddle points that do not have a significant negative eigenvalue).\nSimple ICA example As shown in Figure 2.2, our new algorithm also works in the ICA setting. When the learning rate is constant the error stays at a fixed small value. When we decrease the learning rate the error converges to 0.\n41"
    }, {
      "heading" : "2.5 Conclusion",
      "text" : "In this chapter we identify the strict saddle property and show stochastic gradient descent converges to a local minimum under this assumption. This leads to new online algorithm for orthogonal tensor decomposition. We hope this is a first step towards understanding stochastic gradient for more classes of non-convex functions. We believe strict saddle property can be extended to handle more functions, especially those functions that have similar symmetry properties.\n42\nChapter 3"
    }, {
      "heading" : "Applying Online Tensor Methods for",
      "text" : ""
    }, {
      "heading" : "Learning Latent Variable Models",
      "text" : "In Chapter 2, we have established a guaranteed online stochastic gradient descent algorithm for tensor decomposition. Theoretically, it is solid and well justified. We will now fill in the gap of theoretical findings and practical applications by applying the algorithm to real world problems.\nWe consider two problems: (1) community detection (wherein we compute the decomposition of a tensor which relates to the count of 3-stars in a graph) and (2) topic modeling (wherein we consider the tensor related to co-occurrence of triplets of words in documents); decomposition of the these tensors allows us to learn the hidden communities and topics from observed data.\nCommunity detection: We recover hidden communities in several real datasets with high accuracy. When ground-truth communities are available, we propose a new error score based on the hypothesis testing methodology involving p-values and false discovery rates [153] to validate our results. The use of p-values eliminates the need to carefully tune the number of communities output by our algorithm, and hence, we obtain a flexible trade-off between the\n43\nfraction of communities recovered and their estimation accuracy. We find that our method has very good accuracy on a range of network datasets: Facebook, Yelp and DBLP. We summarize the datasets used in this chapter in Table 3.5. To get an idea of our running times, let us consider the larger DBLP collaborative data set for a moment. It consists of 16 million edges, one million nodes and 250 communities. We obtain an error of 10% and the method runs in about two minutes, excluding the 80 minutes taken to read the edge data from files stored on the hard disk and converting it to sparse matrix format.\nCompared to the state-of-the-art method for learning MMSB models using the stochastic variational inference algorithm of [70], we obtain several orders of magnitude speed-up in the running time on multiple real datasets. This is because our method consists of efficient matrix operations which are embarrassingly parallel. Matrix operations are carried out in the sparse format which is efficient especially for social network settings involving large sparse graphs. Moreover, our code is flexible to run on a range of graphs such as directed, undirected and bipartite graphs, while the code of [70] is designed for homophilic networks, and cannot handle bipartite graphs in its present format. Note that bipartite networks occur in the recommendation setting such as the Yelp data set. Additionally, the variational implementation in [70] assumes a homogeneous connectivity model, where any pair of communities connect with the same probability and the probability of intra-community connectivity is also fixed. Our framework does not suffer from this restriction. We also provide arguments to show that the Normalized Mutual Information (NMI) and other scores, previously used for evaluating the recovery of overlapping community, can underestimate the errors.\nTopic modeling: We also employ the tensor method for topic-modeling, and there are many similarities between the topic and community settings. For instance, each document has multiple topics, while in the network setting, each node has membership in multiple communities. The words in a document are generated based on the latent topics in the document, and similarly, edges are generated based on the community memberships of the node pairs.\n44\nThe tensor method is even faster for topic modeling, since the word vocabulary size is typically much smaller than the size of real-world networks. We learn interesting hidden topics in New York Times corpus from UCI bag-of-words data set1 with around 100, 000 words and 300, 000 documents in about two minutes. We present the important words for recovered topics, as well as interpret “bridging” words, which occur in many topics.\nImplementations: We present two implementations, viz., a GPU-based implementation which exploits the parallelism of SIMD architectures and a CPU-based implementation for larger datasets, where the GPU memory does not suffice. We discuss various aspects involved such as implicit manipulation of tensors since explicitly forming tensors would be unwieldy for large networks, optimizing for communication bottlenecks in a parallel deployment, the need for sparse matrix and vector operations since real world networks tend to be sparse, and a careful statistical approach to validating the results, when ground truth is available."
    }, {
      "heading" : "3.1 Tensor Forms for Topic and Community Models",
      "text" : "In this section, we briefly recap the topic and community models, as well as the tensor forms for their exact moments, derived in [10, 8]."
    }, {
      "heading" : "3.1.1 Topic Modeling",
      "text" : "In topic modeling, a document is viewed as a bag of words. Each document has a latent set of topics, and h = (h1, h2, . . . , hk) represents the proportions of k topics in a given document. Given the topics h, the words are independently drawn and are exchangeable, and hence, the term “bag of words” model. We represent the words in the document by d-dimensional random vectors x1, x2, . . . xl ∈ Rd, where xi are coordinate basis vectors in Rd and d is the 1https://archive.ics.uci.edu/ml/datasets/Bag+of+Words\n45\nsize of the word vocabulary. Conditioned on h, the words in a document satisfy E[xi|h] = µh, where µ := [µ1, . . . , µk] is the topic-word matrix. And thus µj is the topic vector satisfying µj = Pr (xi|hj), ∀j ∈ [k]. Under the Latent Dirichlet Allocation (LDA) topic model [31], h is drawn from a Dirichlet distribution with concentration parameter vector α = [α1, . . . , αk]. In other words, for each document u, hu iid∼ Dir(α), ∀u ∈ [n] with parameter vector α ∈ Rk+. We define the Dirichlet concentration (mixing) parameter\nα0 := ∑\ni∈[k] αi.\nThe Dirichlet distribution allows us to specify the extent of overlap among the topics by controlling for sparsity in topic density function. A larger α0 results in more overlapped (mixed) topics. A special case of α0 = 0 is the single topic model.\nDue to exchangeability, the order of the words does not matter, and it suffices to consider the frequency vector for each document, which counts the number of occurrences of each word in a document. Let ct := (c1,t, c2,t, . . . , cd,t) ∈ Rd denote the frequency vector for tth document, and let n be the number of documents.\n46\nWe consider the first three order empirical moments, given by\nMTop1 := 1\nn\nn∑\nt=1\nct (3.1)\nMTop2 := α0 + 1\nn\nn∑\nt=1\n(ct ⊗ ct − diag (ct))− α0MTop1 ⊗MTop1 (3.2)\nMTop3 := (α0 + 1)(α0 + 2)\n2n\nn∑\nt=1\n ct ⊗ ct ⊗ ct − d∑\ni=1\nd∑\nj=1\nci,tcj,t(ei ⊗ ei ⊗ ej)\n− d∑\ni=1\nd∑\nj=1\nci,tcj,t(ei ⊗ ej ⊗ ei)− d∑\ni=1\nd∑\nj=1\nci,tcj,t(ei ⊗ ej ⊗ ej) + 2 d∑\ni=1\nci,t(ei ⊗ ei ⊗ ei)\n \n− α0(α0 + 1) 2n\nn∑\nt=1\n[ d∑\ni=1\nci,t(ei ⊗ ei ⊗MTop1 ) + d∑\ni=1\nci,t(ei ⊗MTop1 ⊗ ei)\n+ d∑\ni=1\nci,t(M Top 1 ⊗ ei ⊗ ei) ] + α20M Top 1 ⊗M Top 1 ⊗M Top 1 . (3.3)\nWe recall Theorem 3.5 of [10]:\nLemma 3.1. The exact moments can be factorized as\nE[MTop1 ] =\nk∑\ni=1\nαi α0 µi (3.4)\nE[MTop2 ] =\nk∑\ni=1\nαi α0 µi ⊗ µi (3.5)\nE[MTop3 ] =\nk∑\ni=1\nαi α0 µi ⊗ µi ⊗ µi. (3.6)\nwhere µ = [µ1, . . . , µk] and µi = Pr (xt|h = i), ∀t ∈ [l]. In other words, µ is the topic-word matrix.\nFrom the Lemma 3.1, we observe that the first three moments of a LDA topic model have a simple form involving the topic-word matrix µ and Dirichlet parameters αi. In [10], it is shown that these parameters can be recovered under a weak non-degeneracy assumption. We will employ tensor decomposition techniques to learn the parameters.\n47"
    }, {
      "heading" : "3.1.2 Mixed Membership Model",
      "text" : "In the mixed membership stochastic block model (MMSB), introduced by [5], the edges in a social network are related to the hidden communities of the nodes. A batch tensor decomposition technique for learning MMSB was derived in [8].\nLet n denote the number of nodes, k the number of communities and G ∈ Rn×n the adjacency matrix of the graph. Each node i ∈ [n] has an associated community membership vector πi ∈ Rk, which is a latent variable, and the vectors are contained in a simplex, i.e.,\n∑ i∈[k] πu(i) = 1, ∀u ∈ [n]\nwhere the notation [n] denotes the set {1, . . . , n}. Membership vectors are sampled from the Dirichlet distribution πu iid∼ Dir(α), ∀u ∈ [n] with parameter vector α ∈ Rk+ where α0 := ∑ i∈[k] αi. As in the topic modeling setting, the Dirichlet distribution allows us to specify the extent of overlap among the communities by controlling for sparsity in community membership vectors. A larger α0 results in more overlapped (mixed) memberships. A special case of α0 = 0 is the stochastic block model [8].\nThe community connectivity matrix is denoted by P ∈ [0, 1]k×k where P (a, b) measures the connectivity between communities a and b, ∀a, b ∈ [k]. We model the adjacency matrix entries as either of the two settings given below:\nBernoulli model: This models a network with unweighted edges. It is used for Facebook and DBLP datasets in Section 3.5 in our experiments.\nGij iid∼ Ber(π⊤i Pπj), ∀i, j ∈ [n].\n48\nPoisson model [100]: This models a network with weighted edges. It is used for the Yelp data set in Section 3.5 to incorporate the review ratings.\nGij iid∼ Poi(π⊤i Pπj), ∀i, j ∈ [n].\nThe tensor decomposition approach involves up to third order moments, computed from the observed network. In order to compute the moments, we partition the nodes randomly into sets X,A,B, C. Let FA := Π ⊤ AP ⊤, FB := Π ⊤ BP ⊤, FC := Π ⊤ CP ⊤ (where P is the community connectivity matrix and Π is the membership matrix) and α̂ := (\nα1 α0 , . . . , αk α0\n)\ndenote the normalized Dirichlet concentration parameter. We define pairs over Y1 and Y2 as Pairs(Y1, Y2) := G ⊤ X,Y1 ⊗G⊤X,Y2 . Define the following matrices\nZB := Pairs (A,C) (Pairs (B,C)) † , (3.7) ZC := Pairs (A,B) (Pairs (C,B)) † . (3.8)\nWe consider the first three empirical moments, given by\nM1 Com :=\n1\nnX\n∑ x∈X G⊤x,A (3.9)\nM2 Com :=\nα0 + 1\nnX\n∑ x∈X ZCG ⊤ x,CGx,BZ ⊤ B − α0M1ComM1Com ⊤ (3.10)\nM3 Com :=\n(α0 + 1)(α0 + 2)\n2nX\n∑ x∈X G⊤x,A ⊗ ZBG⊤x,B ⊗ ZCG⊤x,C\n+ α20M1 Com ⊗M1Com ⊗M1Com − α0(α0 + 1) 2nX ∑\nx∈X\n( G⊤x,A ⊗ ZBG⊤x,B ⊗M1Com +G⊤x,A ⊗M1Com ⊗ ZCG⊤x,C\n+M1 Com ⊗ ZBG⊤x,B ⊗ ZCG⊤x,C\n) (3.11)\n49\nWe now recap Proposition 2.2 of [9] which provides the form of these moments under expectation.\nLemma 3.2. The exact moments can be factorized as\nE[M1 Com|ΠA,ΠB,ΠC ] :=\n∑ i∈[k] α̂i(FA)i (3.12)\nE[M2 Com|ΠA,ΠB,ΠC ] :=\n∑ i∈[k] α̂i(FA)i ⊗ (FA)i (3.13)\nE[M3 Com|ΠA,ΠB,ΠC ] :=\n∑ i∈[k] α̂i(FA)i ⊗ (FA)i ⊗ (FA)i (3.14)\nwhere ⊗ denotes the Kronecker product and (FA)i corresponds to the ith column of FA.\nWe observe that the moment forms above for the MMSB model have a similar form as the moments of the topic model in the previous section. Thus, we can employ a unified framework for both topic and community modeling involving decomposition of the third order moment tensors MTop3 and M Com 3 . Second order moments M Top 2 and M Com 2 are used for preprocessing of the data (i.e., whitening, which is introduced in detail in Section 3.2.1). For the sake of the simplicity of the notation, in the rest of the chapter, we will use M2 to denote empirical second order moments for both MTop2 in topic modeling setting, and M Com 2 in the mixed membership model setting. Similarly, we will use M3 to denote empirical third order moments for both MTop3 and M Com 3 ."
    }, {
      "heading" : "3.2 Learning using Third Order Moment",
      "text" : "Our learning algorithm uses up to the third-order moment to estimate the topic word matrix µ or the community membership matrix Π. First, we obtain co-occurrence of triplet words or subgraph counts (implicitly). Then, we perform preprocessing using second order moment\n50\nM2. Then we perform tensor decomposition efficiently using stochastic gradient descent [111] on M3. We note that, in our implementation of the algorithm on the Graphics Processing Unit (GPU), linear algebraic operations are extremely fast. We also implement our algorithm on the CPU for large datasets which exceed the memory capacity of GPU and use sparse matrix operations which results in large gains in terms of both the memory and the running time requirements. The overall approach is summarized in Algorithm 3.\nProcedure 3 Overall approach for learning latent variable models via a moment-based approach. Input: Observed data: social network graph or document samples. Output: Learned latent variable model and infer hidden attributes. 1: Estimate the third order moments tensor M3 (implicitly). The tensor is not formed\nexplicitly as we break down the tensor operations into vector and matrix operations. 2: Whiten the data, via SVD of M2, to reduce dimensionality via symmetrization and\northogonalization. The third order moments M3 are whitened as T . 3: Use stochastic gradient descent to estimate spectrum of whitened (implicit) tensor T . 4: Apply post-processing to obtain the topic-word matrix or the community memberships.\n5: If ground truth is known, validate the results using various evaluation measures."
    }, {
      "heading" : "3.2.1 Dimensionality Reduction and Whitening",
      "text" : "Whitening step utilizes linear algebraic manipulations to make the tensor symmetric and orthogonal (in expectation). Moreover, it leads to dimensionality reduction since it (implicitly) reduces tensor M3 of size O(n 3) to a tensor of size k3, where k is the number of communities. Typically we have k ≪ n. The whitening step also converts the tensor M3 to a symmetric orthogonal tensor. The whitening matrix W ∈ RnA×k satisfies W⊤M2W = I. The idea is that if the bilinear projection of the second order moment onto W results in the identity matrix, then a trilinear projection of the third order moment onto W would result in an orthogonal tensor. We use multilinear operations to get an orthogonal tensor T := M3(W,W,W ).\n51\nThe whitening matrix W is computed via truncated k−svd of the second order moments.\nW = UM2Σ −1/2 M2 ,\nwhere UM2 and ΣM2 = diag(σM2,1, . . . , σM2,k) are the top k singular vectors and singular values of M2 respectively. We then perform multilinear transformations on the triplet data using the whitening matrix. The whitened data is thus\nytA := 〈 W, ct 〉 , ytB := 〈 W, ct 〉 , ytC := 〈 W, ct 〉 ,\nfor the topic modeling, where t denotes the index of the documents. Note that ytA, y t B and ytC ∈ Rk. Implicitly, the whitened tensor is T = 1nX ∑ t∈X ytA ⊗ ytB ⊗ ytC and is a k × k × k dimension tensor. Since k ≪ n, the dimensionality reduction is crucial for our speedup."
    }, {
      "heading" : "3.2.2 Stochastic Tensor Gradient Descent",
      "text" : "In [8] and [10], the power method with deflation is used for tensor decomposition where the eigenvectors are recovered by iterating over multiple loops in a serial manner. Furthermore, batch data is used in their iterative power method which makes that algorithm slower than its stochastic counterpart. In addition to implementing a stochastic spectral optimization algorithm, we achieve further speed-up by efficiently parallelizing the stochastic updates.\nLet v = [v1|v2| . . . |vk] be the true eigenvectors. Denote the cardinality of the sample set as nX, i.e., nX := |X|. Now that we have the whitened tensor, we propose the Stochastic Tensor Gradient Descent (STGD) algorithm for tensor decomposition. Consider the tensor\n52\nT ∈ Rk×k×k using whitened samples, i.e.,\nT = 1 nX\n∑ t∈X T t = (α0 + 1)(α0 + 2) 2nX ∑ t∈X ytA ⊗ ytB ⊗ ytC\n− α0(α0 + 1) 2nX\n∑\nt∈X\n[ ytA ⊗ ytB ⊗ ȳC + ytA ⊗ ȳB ⊗ ytC + ȳA ⊗ ytB ⊗ ytC ] + α20ȳA ⊗ ȳB ⊗ ȳC ,\nwhere t ∈ X and denotes the index of the online data and ȳA, ȳB, and ȳC denote the mean of the whitened data. Our goal is to find a symmetric CP decomposition of the whitened tensor, and this will be extensively discussed in the next chapter.\nAfter learning the decomposition of the third order moment, we perform post-processing to estimate Π̂."
    }, {
      "heading" : "3.2.3 Post-processing",
      "text" : "Eigenvalues Λ := [λ1, λ2, . . . , λk] are estimated as the norm of the eigenvectors λi = ‖φi‖3.\nLemma 3.3. After we obtain Λ and Φ, the estimate for the topic-word matrix is given by\nµ̂ = W⊤ † Φ,\nand in the community setting, the community membership matrix is given by\nΠ̂Ac = diag(γ) 1/3 diag(Λ)−1Φ⊤Ŵ⊤GA,Ac .\nwhere Ac := X ∪ B ∪ C. Similarly, we estimate Π̂A by exchanging the roles of X and A. Next, we obtain the Dirichlet distribution parameters\nα̂i = γ 2λ−2i , ∀i ∈ [k].\n53\nwhere γ2 is chosen such that we have normalization ∑ i∈[k] α̂i := ∑ i∈[k] αi α0 = 1.\nThus, we perform STGDmethod to estimate the eigenvectors and eigenvalues of the whitened tensor, and then use these to estimate the topic word matrix µ and community membership matrix Π̂ by thresholding."
    }, {
      "heading" : "3.3 Implementation Details",
      "text" : ""
    }, {
      "heading" : "3.3.1 Symmetrization Step to Compute M2",
      "text" : "Note that for the topic model, the second order moment M2 can be computed easily from the word-frequency vector. On the other hand, for the community setting, computing M2 requires additional linear algebraic operations. It requires computation of matrices ZB and ZC in equation (3.7). This requires computation of pseudo-inverses of “Pairs” matrices. Now, note that pseudo-inverse of (Pairs (B,C)) in Equation (3.7) can be computed using rank k-SVD:\nk-SVD (Pairs (B,C)) = UB(:, 1 : k)ΣBC(1 : k)VC(:, 1 : k) ⊤.\nWe exploit the low rank property to have efficient running times and storage. We first implement the k-SVD of Pairs, given by G⊤X,CGX,B. Then the order in which the matrix products are carried out plays a significant role in terms of both memory and speed. Note that ZC involves the multiplication of a sequence of matrices of sizes R nA×nB , RnB×k, Rk×k, Rk×nC , G⊤x,CGx,B involves products of sizes R nC×k, Rk×k, Rk×nB , and ZB involving products of sizes RnA×nC , RnC×k, Rk×k, Rk×nB . While performing these products, we avoid products of sizes RO(n)×O(n) and RO(n)×O(n). This allows us to have efficient storage requirements. Such manipulations are represented in Figure 3.1.\n54\nWe then orthogonalize the third order moments to reduce the dimension of its modes to k. We perform linear transformations on the data corresponding to the partitions A, B and C using the whitening matrix. The whitened data is thus ytA := 〈 W,G⊤t,A 〉 , ytB := 〈 W,ZBG ⊤ t,B 〉 , and ytC := 〈 W,ZCG ⊤ t,C 〉 , where t ∈ X and denotes the index of the online data. Since k ≪ n, the dimensionality reduction is crucial for our speedup."
    }, {
      "heading" : "3.3.2 Efficient Randomized SVD Computations",
      "text" : "When we consider very large-scale data, the whitening matrix is a bottleneck to handle when we aim for fast running times. We obtain the low rank approximation of matrices using random projections. In the CPU implementation, we use tall-thin SVD (on a sparse matrix) via the Lanczos algorithm after the projection and in the GPU implementation,\n55\nwe use tall-thin QR. We give the overview of these methods below. Again, we use graph community membership model without loss of generality.\nRandomized low rank approximation: From [66], for the k-rank positive semi-definite matrix M2 ∈ RnA×nA with nA ≫ k, we can perform random projection to reduce dimensionality. More precisely, if we have a random matrix S ∈ RnA×k̃ with unit norm (rotation matrix), we project M2 onto this random matrix to get R n×k̃ tall-thin matrix. Note that we choose k̃ = 2k in our implementation. We will obtain lower dimension approximation of M2 in Rk̃×k̃. Here we emphasize that S ∈ Rn×k̃ is a random matrix for dense M2. However for sparse M2, S ∈ {0, 1}n×k̃ is a column selection matrix with random sign for each entry.\nAfter the projection, one approach we use is SVD on this tall-thin (Rn×k̃) matrix. Define O := M2S ∈ Rn×k̃ and Ω := S⊤M2S ∈ Rk̃×k̃. A low rank approximation of M2 is given by OΩ†O⊤ [66]. Recall that the definition of a whitening matrix W is that W⊤M2W = I. We can obtain the whitening matrix of M2 without directly doing a SVD on M2 ∈ RnA×nA.\nTall-thin SVD: This is used in the CPU implementation. The whitening matrix can be obtained by\nW ≈ (O†)⊤(Ω 12 )⊤. (3.15)\nThe pseudo code for computing the whitening matrix W using tall-thin SVD is given in Algorithm 4. Therefore, we only need to compute SVD of a tall-thin matrix O ∈ RnA×k̃. Note that Ω ∈ Rk̃×k̃, its square-root is easy to compute. Similarly, pseudoinverses can also be obtained without directly doing SVD. For instance, the pseudoinverse of the Pairs (B,C) matrix is given by\n(Pairs (B,C))† = (J†)⊤ΨJ†,\n56\nProcedure 4 Randomized Tall-thin SVD Input: Second moment matrix M2. Output: Whitening matrix W . 1: Generate random matrix S ∈ Rn×k̃ if M2 is dense. 2: Generate column selection matrix with random sign S ∈ {0, 1}n×k̃ if M2 is sparse. 3: O = M2S ∈ Rn×k̃ 4: [UO, LO, VO] =SVD(O)\n5: Ω = S⊤O ∈ Rk̃×k̃ 6: [UΩ, LΩ, VΩ] =SVD(Ω) 7: W = UOL −1 O V ⊤ O VΩL 1 2 ΩU ⊤ Ω\nwhere Ψ = S⊤ (Pairs (B,C))S and J = (Pairs (B,C))S. The pseudo code for computing pseudoinverses is given in Algorithm 5.\nProcedure 5 Randomized Pseudoinverse Input: Pairs matrix Pairs (B,C). Output: Pseudoinverse of the pairs matrix (Pairs (B,C))†. 1: Generate random matrix S ∈ Rn,k if M2 is dense. 2: Generate column selection matrix with random sign S ∈ {0, 1}n×k if M2 is sparse. 3: J = (Pairs (B,C))S 4: Ψ = S⊤J 5: [UJ , LJ , VJ ] =SVD(J) 6: (Pairs (B,C))† = UJL −1 J V ⊤ J ΨVJL −1 J U ⊤ J\nThe sparse representation of the data allows for scalability on a single machine to datasets having millions of nodes. Although the GPU has SIMD architecture which makes parallelization efficient, it lacks advanced libraries with sparse SVD operations and out-of-GPU-core implementations. We therefore implement the sparse format on CPU for sparse datasets. We implement our algorithm using random projection for efficient dimensionality reduction [45] along with the sparse matrix operations available in the Eigen toolkit2, and we use the SVDLIBC [30] library to compute sparse SVD via the Lanczos algorithm. Theoretically, the Lanczos algorithm [69] on a n×n matrix takes around (2d+8)n flops for a single step where d is the average number of non-zero entries per row.\n2http://eigen.tuxfamily.org/index.php?title=Main_Page\n57\nTall-thin QR: This is used in the GPU implementation due to the lack of library to do sparse tall-thin SVD. The difference is that we instead implement a tall-thin QR on O, therefore the whitening matrix is obtained as\nW ≈ Q(R†)⊤(Ω 12 )⊤.\nThe main bottleneck for our GPU implementation is device storage, since GPU memory is highly limited and not expandable. Random projections help in reducing the dimensionality from O(n× n) to O(n× k) and hence, this fits the data in the GPU memory better. Consequently, after the whitening step, we project the data into k-dimensional space. Therefore, the STGD step is dependent only on k, and hence can be fit in the GPU memory. So, the main bottleneck is computation of large SVDs. In order to support larger datasets such as the DBLP data set which exceed the GPU memory capacity, we extend our implementation with out-of-GPU-core matrix operations and the Nystrom method [66] for the whitening matrix computation and the pseudoinverse computation in the pre-processing module.\n58"
    }, {
      "heading" : "3.3.3 Stochastic Updates",
      "text" : "STGD can potentially be the most computationally intensive task if carried out naively since the storage and manipulation of a O(n3)-sized tensor makes the method not scalable. However we overcome this problem since we never form the tensor explicitly; instead, we collapse the tensor modes implicitly. We gain large speed up by optimizing the implementation of STGD.To implement the tensor operations efficiently we convert them into matrix and vector operations so that they are implemented using BLAS routines. We obtain whitened vectors yA, yB and yC and manipulate these vectors efficiently to obtain tensor eigenvector updates using the gradient scaled by a suitable learning rate.\nEfficient STGD via stacked vector operations: We convert the BLAS II into BLAS III operations by stacking the vectors to form matrices, leading to more efficient operations. Although the updating equation for the stochastic gradient update is presented serially, we can update the k eigenvectors simultaneously in parallel. The basic idea is to stack the k eigenvectors φi ∈ Rk into a matrix Φ, then using the internal parallelism designed for BLAS III operations.\nOverall, the STGD step involves 1 + k + i(2 + 3k) BLAS II over Rk vectors, 7N BLAS III over Rk×k matrices and 2 QR operations over Rk×k matrices, where i denotes the number of iterations. We provide a count of BLAS operations for various steps in Table 3.1."
    }, {
      "heading" : "Module BLAS I BLAS II BLAS III SVD QR",
      "text" : "Reducing communication in GPU implementation: In STGD, note that the storage needed for the iterative part does not depend on the number of nodes in the data set, rather,\n59\nit depends on the parameter k, i.e., the number of communities to be estimated, since whitening performed before STGD leads to dimensionality reduction. This makes it suitable for storing the required buffers in the GPU memory, and using the CULA device interface for the BLAS operations. In Figure 3.2, we illustrate the data transfer involved in the GPU standard and device interface codes. While the standard interface involves data transfer (including whitened neighborhood vectors and the eigenvectors) at each stochastic iteration between the CPU memory and the GPU memory, the device interface involves allocating and retaining the eigenvectors at each stochastic iteration which in turn speeds up the spectral estimation.\n60\nWe compare the running time of the CULA device code with the MATLAB code (using the tensor toolbox [23]), CULA standard code and Eigen sparse code in Figure 3.3. As expected, the GPU implementations of matrix operations are much faster and scale much better than the CPU implementations. Among the CPU codes, we notice that sparsity and optimization offered by the Eigen toolkit gives us huge gains. We obtain orders of magnitude of speed up for the GPU device code as we place the buffers in the GPU memory and transfer minimal amount of data involving the whitened vectors only once at the beginning of each iteration. The running time for the CULA standard code is more than the device code because of the CPU-GPU data transfer overhead. For the same reason, the sparse CPU implementation, by avoiding the data transfer overhead, performs better than the GPU standard code for very small number of communities. We note that there is no performance degradation due to the parallelization of the matrix operations. After whitening, the STGD requires the most code design and optimization effort, and so we convert that into BLAS-like routines."
    }, {
      "heading" : "3.3.4 Computational Complexity",
      "text" : "We partition the execution of our algorithm into three main modules namely, pre-processing, STGD and post-processing, whose various matrix operation counts are listed above in Table 3.1.\n61\nThe theoretical asymptotic complexity of our method is summarized in Table 3.2 and is best addressed by considering the parallel model of computation [94], i.e., wherein a number of processors or compute cores are operating on the data simultaneously in parallel. This is justified considering that we implement our method on GPUs and matrix products are embarrassingly parallel. Note that this is different from serial computational complexity. We now break down the entries in Table 3.2. First, we recall a basic lemma regarding the lower bound on the time complexity for parallel addition along with the required number of cores to achieve a speed-up.\nLemma 3.4. [94] Addition of s numbers in serial takes O(s) time; with Ω(s/ log s) cores, this can be improved to O(log s) time in the best case.\nEssentially, this speed-up is achieved by recursively adding pairs of numbers in parallel.\nLemma 3.5. [94] Consider M ∈ Rp×q and N ∈ Rq×r with s non-zeros per row/column. Naive serial matrix multiplication requires O(psr) time; with Ω(psr/ log s) cores, this can be improved to O(log s) time in the best case.\nLemma 3.5 follows by simply parallelizing the sparse inner products and applying Lemma 3.4 for the addition in the inner products. Note that, this can be generalized to the fact that given c cores, the multiplication can be performed in O(max(psr/c, log s)) running time."
    }, {
      "heading" : "Pre-processing",
      "text" : "Random projection: In preprocessing, given c compute cores, we first do random projection using matrix multiplication. We multiply an O(n)× O(n) matrix M2 with an O(n)× O(k) random matrix S. Therefore, this requires O(nsk) serial operations, where s is the number of non-zero elements per row/column of M2. Using Lemma 3.5, given c = nsk log s cores, we could\n62\nachieve O(log s) computational complexity. However, the parallel computational complexity is not further reduced with more than nsk log s cores.\nAfter the multiplication, we use tall-thin SVD for CPU implementation, and tall-thin QR for GPU implementation.\nTall-thin SVD: We perform Lanczos SVD on the tall-thin sparse O(n)×O(k) matrix, which involves a tri-diagonalization followed with the QR on the tri-diagonal matrix. Given c = nsk log s cores, the computational complexity of the tri-diagonalization is O(log s). We then do QR on the tridiagonal matrix which is as cheap as O(k2) serially. Each orthogonalization requires O(k) inner products of constant entry vectors, and there are O(k) such orthogonalizations to be done. Therefore given O(k) cores, the complexity is O(k). More cores does not help since the degree of parallelism is k.\nTall-thin QR: Alternatively, we perform QR in the GPU implementation which takes O(sk2). To arrive at the complexity of obtaining Q, we analyze the Gram-Schmidt orthonormalization procedure under sparsity and parallelism conditions. Consider a serial Gram-Schmidt on k columns (which are s-dense) of O(n) × O(k) matrix. For each of the columns 2 to k, we perform projection on the previously computed components and subtract it. Both inner product and subtraction operations are on the s-dense columns and there are O(s) operations which are done O(k2) times serially. The last step is the normalization of k s-dense vectors with is an O(sk) operation. This leads to a serial complexity of O(sk2+sk) = O(sk2). Using this, we may obtain the parallel complexity in different regimes of the number of cores as follows.\nParallelism for inner products : For each component i, we need i − 1 projections on previous components which can be parallel. Each projection involves scaling and inner product operations on a pair of s-dense vectors. Using Lemma 3.4, projection for component i can\n63\nbe performed in O(max( sk c , log s)) time. O(log s) complexity is obtained using O(sk/ log s) cores.\nParallelism for subtractions : For each component i, we need i − 1 subtractions on a sdense vector after the projection. Serially the subtraction requires O(sk) operations, and this can be reduced to O(log k) with O(sk/ log k) cores in the best case. The complexity is O(max( sk c , log k)). Combing the inner products and subtractions, the complexity is O ( max( sk\nc , log s)\n+max( sk c , log k)\n) for component i. There are k components in total, which can not be\nparallel. In total, the complexity for the parallel QR is O ( max( sk 2\nc , log s) + max( sk\n2\nc , log k)\n) .\nShort-thin SVD: SVD of the smaller O(Rk×k) matrix time requires O(k3) computations in serially. We note that this is the bottleneck for the computational complexity, but we emphasize that k is sufficiently small in many applications. Furthermore, this k3 complexity can be reduced by using distributed SVD algorithms e.g. [99, 62]. An analysis with respect to Lanczos parallel SVD is similar with the discussion in the Tall-thin SVD paragraph. The complexity is O(max(k3/c, log k)+max(k2/c, k)). In the best case, the complexity is reduced to O(log k + k).\nThe serial time complexity of SVD is O(n2k) but with randomized dimensionality reduction [66] and parallelization [51], this is significantly reduced."
    }, {
      "heading" : "STGD",
      "text" : "In STGD, we perform implicit stochastic updates, consisting of a constant number of matrixmatrix and matrix-vector products, on the set of eigenvectors and whitened samples which is of size k × k. When c ∈ [1, k3/ log k], we obtain a running time of O(k3/c) for computing inner products in parallel with c compute cores since each core can perform an inner product\n64\nto compute an element in the resulting matrix independent of other cores in linear time. For c ∈ (k3/ log k,∞], using Lemma 3.4, we obtain a running time of O(log k). Note that the STGD time complexity is calculated per iteration."
    }, {
      "heading" : "Post-processing",
      "text" : "Finally, post-processing consists of sparse matrix products as well. Similar to pre-processing, this consists of multiplications involving the sparse matrices. Given s number of non-zeros per column of an O(n) × O(k) matrix, the effective number of elements reduces to O(sk). Hence, given c ∈ [1, nks/ log s] cores, we need O(nsk/c) time to perform the inner products for each entry of the resultant matrix. For c ∈ (nks/ log s,∞], using Lemma 3.4, we obtain a running time of O(log s).\nNote that nk2 is the complexity of computing the exact SVD and we reduce it to O(k) when there are sufficient cores available. This is meant for the setting where k is small. This k3 complexity of SVD on O(k × k) matrix can be reduced to O(k) using distributed SVD algorithms e.g. [99, 62]. We note that the variational inference algorithm complexity, by Gopalan and Blei [71], is O(mk) for each iteration, where m denotes the number of edges in the graph, and n < m < n2. In the regime that n ≫ k, our algorithm is more efficient. Moreover, a big difference is in the scaling with respect to the size of the network and ease of parallelization of our method compared to variational one.\n65"
    }, {
      "heading" : "3.4 Validation methods",
      "text" : ""
    }, {
      "heading" : "3.4.1 P -value Testing",
      "text" : "We recover the estimated community membership matrix Π̂ ∈ Rk̂×n, where k̂ is the number of communities specified to our method. Recall that the true community membership matrix is Π, and we consider datasets where ground truth is available. Let i-th row of Π̂ be denoted by Π̂i. Our community detection method is unsupervised, which inevitably results in row permutations between Π and Π̂ and k̂ may not be the same as k. To validate the results, we need to find a good match between the rows of Π̂ and Π. We use the notion of p-values to test for statistically significant dependencies among a set of random variables. The p-value denotes the probability of not rejecting the null hypothesis that the random variables under consideration are independent and we use the Student’s3 t-test statistic [60] to compute the p-value. We use multiple hypothesis testing for different pairs of estimated and ground-\n3Note that Student’s t-test is robust to the presence of unequal variances when the sample sizes of the two are equal which is true in our setting.\n66\ntruth communities Π̂i,Πj and adjust the p-values to ensure a small enough false discovery rate (FDR) [153].\nThe test statistic used for the p-value testing of the estimated communities is\nTij := ρ ( Π̂i,Πj )√ n− 2\n√ 1− ρ ( Π̂i,Πj )2 .\nThe right p-value is obtained via the probability of obtaining a value (say tij) greater than the test statistic Tij , and it is defined as\nPval(Πi, Π̂j) := 1− P (tij > Tij) .\nNote that Tij has Student’s t-distribution with degree of freedom n − 2 (i.e. Tij ∼ tn−2). Thus, we obtain the right p-value4.\nIn this way, we compute the Pval matrix as\nPval(i, j) := Pval [ Π̂i,Πj ] , ∀i ∈ [k] and j ∈ [k̂]."
    }, {
      "heading" : "3.4.2 Evaluation Metrics",
      "text" : "Recovery ratio: Validating the results requires a matching of the true membership Π with estimated membership Π̂. Let Pval(Πi, Π̂j) denote the right p-value under the null hypothesis that Πi and Π̂j are statistically independent. We use the p-value test to find out pairs Πi, Π̂j which pass a specified p-value threshold, and we denote such pairs using a bipartite graph\n4The right p-value accounts for the fact that when two communities are anti-correlated they are not paired up. Hence note that in the special case of block model in which the estimated communities are just permuted version of the ground truth communities, the pairing results in a perfect matching accurately.\n67\nG{Pval}. Thus, G{Pval} is defined as\nG{Pval} := ({ V (1) {Pval}, V (2) {Pval} } , E{Pval} ) ,\nwhere the nodes in the two node sets are\nV (1) {Pval} = {Π1, . . . ,Πk} , V (2) {Pval} = { Π̂1, . . . , Π̂k̂ }\nand the edges of G{Pval} satisfy\n(i, j) ∈ E{Pval} s.t. Pval [ Π̂i,Πj ] ≤ 0.01.\nA simple example is shown in Figure 3.4, in which Π2 has statistically significant dependence with Π̂1, i.e., the probability of not rejecting the null hypothesis is small (recall that null hypothesis is that they are independent). If no estimated membership vector has a significant overlap with Π3, then Π3 is not recovered. There can also be multiple pairings such as for Π1 and {Π̂2, Π̂3, Π̂6}. The p-value test between Π1 and {Π̂2, Π̂3, Π̂6} indicates that probability of not rejecting the null hypothesis is small, i.e., they are independent. We use 0.01 as the threshold. The same holds for Π2 and {Π̂1} and for Π4 and {Π̂4, Π̂5}. There can be a perfect one to one matching like for Π2 and Π̂1 as well as a multiple matching such as for Π1 and {Π̂2, Π̂3, Π̂6}. Or another multiple matching such as for {Π1,Π2} and Π̂3.\nLet Degreei denote the degree of ground truth community i ∈ [k] in G{Pval}, we define the recovery ratio as follows.\nDefinition 3.1. The recovery ratio is defined as\nR := 1 k\n∑\ni\nI {Degreei > 0} , i ∈ [k]\n68\nwhere I(x) is the indicator function whose value equals one if x is true.\nThe perfect case is that all the memberships have at least one significant overlapping estimated membership, giving a recovery ratio of 100%. Error function: For performance analysis of our learning algorithm, we use an error function given as follows:\nDefinition 3.2. The average error function is defined as\nE := 1 k\n∑\n(i,j)∈E{P val }\n   1 n ∑\nx∈|X|\n∣∣∣∣ Π̂i(x)−Πj(x) ∣∣∣∣    ,\nwhere E{Pval} denotes the set of edges based on thresholding of the p-values.\nThe error function incorporates two aspects, namely the l1 norm error between each estimated community and the corresponding paired ground truth community, and the error induced by false pairings between the estimated and ground-truth communities through p-value testing. For the former l1 norm error, we normalize with n which is reasonable and results in the range of the error in [0, 1]. For the latter, we define the average error function as the summation of all paired memberships errors divided by the true number of communities k. In this way we penalize falsely discovered pairings by summing them up. Our error function can be greater than 1 if there are too many falsely discovered pairings through p-value testing (which can be as large as k × k̂).\nBridgeness: Bridgeness in overlapping communities is an interesting measure to evaluate. A bridge is defined as a vertex that crosses structural holes between discrete groups of people and bridgeness analyzes the extent to which a given vertex is shared among different\n69\ncommunities [129]. Formally, the bridgeness of a vertex i is defined as\nbi := 1− √√√√ k̂ k̂ − 1 k̂∑\nj=1\n( Π̂i(j)− 1\nk̂\n)2 . (3.16)\nNote that centrality measures should be used in conjunction with bridge score to distinguish outliers from genuine bridge nodes [129]. The degree-corrected bridgeness is used to evaluate our results and is defined as\nBi := Dibi, (3.17)\nwhere Di is degree of node i."
    }, {
      "heading" : "3.5 Experimental Results",
      "text" : ""
    }, {
      "heading" : "Results on Synthetic Datasets:",
      "text" : "We perform experiments for both the stochastic block model (α0 = 0) and the mixed membership model. For the mixed membership model, we set the concentration parameter α0 = 1. We note that the error is around 8%−14% and the running times are under a minute, when n ≤ 10000 and n ≫ k.\nWe observe that more samples result in a more accurate recovery of memberships which matches intuition and theory. Overall, our learning algorithm performs better in the stochastic block model case than in the mixed membership model case although we note that the accuracy is quite high for practical purposes. Theoretically, this is expected since smaller concentration parameter α0 is easier for our algorithm to learn [8]. Also, our algorithm is scalable to an order of magnitude larger in n as illustrated by experiments on real-world large-scale datasets.\n70\nNote that we threshold the estimated memberships to clean the results. There is a tradeoff between match ratio and average error via different thresholds. In synthetic experiments, the tradeoff is not evident since a perfect matching is always present. However, we need to carefully handle this in experiments involving real data.\nResults on Topic Modeling: We perform experiments for the bag of words data set [22] for The New York Times. We set the concentration parameter to be α0 = 1 and observe top recovered words in numerous topics. The results are in Table 3.3. Many of the results are expected. For example, the top words in topic # 11 are all related to some bad personality.\nWe also present the words with most spread membership, i.e., words that belong to many topics as in Table 3.4. As expected, we see minutes, consumer, human, member and so on. These words can appear in a lot of topics, and we expect them to connect topics.\nResults on Real-world Graph Datasets: We describe the results on real datasets summarized in Table 3.5 in detail below. The simulations are summarized in Table 3.6.\nThe results are presented in Table 3.6. We note that our method, in both dense and sparse implementations, performs very well compared to the state-of-the-art variational method. For the Yelp dataset, we have a bipartite graph where the business nodes are on one side and user nodes on the other and use the review stars as the edge weights. In this bipartite setting, the variational code provided by Gopalan et al [70] does not work on since it is not applicable to non-homophilic models. Our approach does not have this restriction. Note that we use our dense implementation on the GPU to run experiments with large number of communities k as the device implementation is much faster in terms of running time of the STGD step.On the other hand, the sparse implementation on CPU is fast and memory efficient in the case of sparse graphs with a small number of communities while the dense implementation on GPU is faster for denser graphs such as Facebook. Note that data reading time for DBLP is around 4700 seconds, which is not negligible as compared to other\n71\n72\ngraph, |E| is the number of edges, GD is the graph density given by 2|E||V |(|V |−1) , k is the number of communities, AB is the average bridgeness and ADCB is the average degreecorrected bridgeness(explained in Section 3.4).\ndatasets (usually within a few seconds). Effectively, our algorithm, excluding the file I/O time, executes within two minutes for k = 10 and within ten minutes for k = 100.\nInterpretation on Yelp Dataset: The ground truth on business attributes such as location and type of business are available (but not provided to our algorithm) and we provide the distribution in Figure 3.5 on the left side. There is also a natural trade-off between recovery ratio and average error or between attempting to recover all the business communities and the accuracy of recovery. We can either recover top significant communities with high accuracy or recover more with lower accuracy. We demonstrate the trade-off in Figure 3.5 on the right side.\n73\nWe select the top ten categories recovered with the lowest error and report the business with highest weights in Π̂. Among the matched communities, we find the business with the highest membership weight (Table 3.7). We can see that most of the “top” recovered businesses are rated high. Many of the categories in the top ten list are restaurants as they have a large number of reviewers. Our method can recover restaurant category with high accuracy, and the specific restaurant in the category is a popular result (with high number of stars). Also, our method can also recover many of the categories with low review counts accurately like hobby shops, yoga, churches, galleries and religious organizations which are the “niche” categories with a dedicated set of reviewers, who mostly do not review other categories.\nOur algorithm can also recover the attributes of users. However, the ground truth available about users is far more limited than businesses, and we only have information on gender, average review counts and average stars (we infer the gender of the users through their\n74\nnames). Our algorithm can recover all these attributes. We observe that gender is the hardest to recover while review counts is the easiest. We see that the other user attributes recovered by our algorithm correspond to valuable user information such as their interests, location, age, lifestyle, etc. This is useful, for instance, for businesses studying the characteristics of their users, for delivering better personalized advertisements for users, and so on.\nFacebook Dataset: A snapshot of the Facebook network of UNC [155] is provided with user attributes. The ground truth communities are based on user attributes given in the dataset which are not exposed to the algorithm. There are 360 top communities with sufficient (at least 20) users. Our algorithm can recover these attributes with high accuracy compared with variational inference result [70].\nWe also obtain results for a range of values of α0 (Figure 3.6). We observe that the recovery ratio improves with larger α0 since a larger α0 can recover overlapping communities more efficiently while the error score remains relatively the same.\nFor the Facebook dataset, the top ten communities recovered with lowest error consist of certain high schools, second majors and dorms/houses. We observe that high school attributes are easiest to recover and second major and dorm/house are reasonably easy to recover by looking at the friendship relations in Facebook. This is reasonable: college students from\n75\nthe same high school have a high probability of being friends; so do colleges students from the same dorm."
    }, {
      "heading" : "DBLP Dataset:",
      "text" : "The DBLP data contains bibliographic records5 with various publication venues, such as journals and conferences, which we model as communities. We then consider authors who have published at least one paper in a community (publication venue) as a member of it. Co-authorship is thus modeled as link in the graph in which authors are represented as nodes. In this framework, we could recover the top authors in communities and bridging authors."
    }, {
      "heading" : "3.6 Conclusion",
      "text" : "In this chapter, we presented a fast and unified moment-based framework for learning overlapping communities as well as topics in a corpus. There are several key insights involved. Firstly, our approach follows from a systematic and guaranteed learning procedure in contrast to several heuristic approaches which may not have strong statistical recovery guarantees.\n5http://dblp.uni-trier.de/xml/Dblp.xml\n76\nSecondly, though using a moment-based formulation may seem computationally expensive at first sight, implementing implicit “tensor” operations leads to significant speed-ups of the algorithm. Thirdly, employing randomized methods for spectral methods is promising in the computational domain, since the running time can then be significantly reduced.\nThis work paves the way for several interesting directions for further research. While our current deployment incorporates community detection in a single graph, extensions to multigraphs and hypergraphs are possible in principle. A careful and efficient implementation for such settings will be useful in a number of applications. It is natural to extend the deployment to even larger datasets by having cloud-based systems. The issue of efficient partitioning of data and reducing communication between the machines becomes significant there. Combining our approach with other simple community detection approaches to gain even more speedups can be explored.\n77\nChapter 4"
    }, {
      "heading" : "Dictionary Learning through",
      "text" : ""
    }, {
      "heading" : "Convolutional Tensor Decomposition",
      "text" : "In this chapter, we extend tensor decomposition framework to models with invariances, such as convolutional dictionary models. Learning invariant dictionary elements is crucial to remove unnecessary model redundancy in a lot of settings. For instance, in image filter bank learning where image filters’ activation locations in the image are ignored, in natural language process where the phrase templates are not distinguished by their location in the sentence, and in neural science where neural spikes consist of template spikes activated at different time.\nWe propose a tensor decomposition algorithm to solve this problem of learning shift invariant dictionary elements. Our tensor decomposition algorithm is based on the popular alternating least squares (ALS) method, but with additional shift invariance constraints on the factors. We demonstrate that each ALS update can be computed efficiently using simple operations such as fast Fourier transforms and matrix multiplications. Our algorithm converges to mod-\n78\nels with better reconstruction error and is much faster, compared to the popular alternating minimization heuristic, where the filters and activation maps are alternately updated.\nWe propose a novel framework for learning convolutional models through tensor decomposition. We consider inverse method of moments to estimate the model parameters via decomposition of higher order (third or fourth order) moment tensors. When the inputs x are generated from a convolutional model in (1.3), with independent activation maps w∗i , i.e. a convolutional ICA model, we show that the cumulant tensors have a CP decomposition, whose components correspond to filters and their circulant shifts. We propose a novel method for tensor decomposition when such circulant constraints are imposed on the components of the tensor decomposition.\nOur tensor decomposition method is a constrained form of the popular alternating least squares (ALS) method1. We show that the resulting optimization problem in each tensor ALS iteration can be solved in closed form, and uses simple operations such as Fast Fourier transforms (FFT) and matrix multiplications. These operations have a high degree of parallelism: for estimating L filters, each of length n, we require O(logn + logL) time and O(L2n3) processors. Note that there is no dependence on the number of data samples N , since the empirical moment tensor can be computed in one data pass, and the ALS iterations only updates the filters. This is a huge saving in running time, compared to the alternate minimization method which requires a pass over data in each step to decode all the activation maps wi. The running time of alternating minimization is O(max(logn logL, log n logN)) per iteration with O(max( nNL logN , nNL logL )) processors, and when N ≫ Ln2, which is the typical scenario, our method is hugely advantageous. Our method avoids decoding the activation maps in each iteration since they are averaged out in the input moment tensor, on which the ALS method operates and we only estimate the filters fi in the learning step. In other\n1The ALS method for tensor decomposition is not to be confused with the alternating minimization method for solving (1.4). While (1.4) acts on data samples and alternates between updating filters and activation maps, tensor ALS operates on averaged moment tensors and alternates between different modes of the tensor decomposition.\n79\nwords, the activation maps wi’s are easily estimated using (1.4) in one data pass after filter estimation. Thus, our method is highly parallel and scalable to huge datasets.\nWe carefully optimize computation and memory costs by exploiting tensor algebra and circulant structure, due to the shift invariance of the convolutional model. We implicitly carry out many of the operations and do not form large (circulant) matrices and minimize storage requirements. Preliminary experiments further demonstrate superiority of our method compared to alternating minimization. Our algorithm converges accurately and much faster to the true underlying filters compared to alternating minimization. Moreover, it results in much lower reconstruction error, while alternating minimization tends to get stuck in spurious local optima. Our algorithm is also orders of magnitude faster than the alternating minimization."
    }, {
      "heading" : "4.1 Model and Formulation",
      "text" : "Notation Let [n] := {1, 2, . . . , n}. For a vector v, denote the ith element as v(i). For a matrix M , denote the ith row as M i and jth column as Mj . For a tensor T ∈ Rn×n×n, its (i1, i2, i3) th entry is denoted by [T ]i1,i2,i3 . A column-stacked matrix M consisting of M ′ is (with same number of rows) is M := [M1,M2, . . . ,ML]. Similarly, a row-stacked matrix M from M ′is (with same number of columns) is M := [M1;M2; . . . ;ML].\nCyclic Convolution The 1-dimensional (1-D) n-cyclic convolution f ∗w between vectors f and w is defined as v = f ∗n w, v(i) = ∑j∈[n] f(j)w((i − j + 1) mod n). Note that the linear convolution is the combination without the modulo operation (i.e. cyclic shifts) above. n-Cyclic convolution is equivalent to linear convolution, when n is at least twice the support length of both f and w [133], which will be assumed. We drop the notation n in ∗ for\n80\nconvenience. Cyclic convolution in (4.1) is equivalent to f ∗w = Cir(f) · w, and\nCir(f) := ∑\np\nf(p)Gp ∈ Rn×n, (Gp)ij := δ {((i− j) mod n) = p− 1} , ∀p ∈ [n]. (4.1)\ndefines a circulant matrix. A circulant matrix Cir(f) is characterized by the vector f , and each column corresponds to a cyclic shift of f .\nProperties of circulant matrices Let F be the discrete Fourier transform matrix whose (m, k)-th entry is Fmk = ω (m−1)(k−1) n , ∀m, k ∈ [n] where ωn = exp(−2πin ). If U := √ nF−1, U is the set of eigenvectors for all n × n circulant matrices [73]. Let the Discrete Fourier Transform of a vector f be FFT(f), we express the circulant matrix Cir(f) as\nCir(f) = U Diag(F · f)UH = U Diag(FFT(f))UH. (4.2)\nThis is an important property we use in algorithm optimization to improve computational efficiency.\nColumn stacked circulant matrices We will extensively use column stacked circulant matrices F := [Cir(f1), . . . ,Cir(fL)], where Cir(fj) is the circulant matrix corresponding to filter fj ."
    }, {
      "heading" : "4.1.1 Convolutional Dictionary Learning/ICA Model",
      "text" : "We assume that the input x ∈ Rn is generated as\nx = ∑\nj∈[L] f ∗j ∗w∗j =\n∑ j∈[L] Cir(f ∗j )w ∗ j = F∗ · w∗, (4.3)\nwhere F∗ := [Cir(f ∗1 ),Cir(f ∗2 ), . . . ,Cir(f ∗L)] is the concatenation or column stacked version of circulant matrices and w∗ is the row-stacked vector w∗ := [w∗1;w ∗ 2; . . . w ∗ L] ∈ RnL. Recall that Cir(f ∗l ) is circulant matrix corresponding to filter f ∗ l , as given by (4.2). Note that although\n81\nF∗ is a n by nL matrix, there are only nL free parameters. We never explicitly form the estimates F of F∗, but instead use filter estimates fl’s to characterize F . In addition, we can handle additive Gaussian noise in (4.17), but do not incorporate it for simplicity. Activation Maps:For each observed sample x, the activation map w∗i in (4.17) indicates the locations where each filter f ∗i is active and w ∗ is the row-stacked vector w∗ := [w∗1;w ∗ 2; . . . w ∗ L]. We assume that the coordinates of w∗ are drawn from some product distribution, i.e. different entries are independent of one another and we have the independent component analysis (ICA) model in (4.17). When the distribution encourages sparsity, e.g. Bernoulli-Gaussian, only a small subset of locations are active, and we have the sparse coding model in that case. We can also extend to dependent distributions such as Dirichlet for w∗, along the lines of [32], but limit ourselves to ICA model for simplicity. Learning Problem:Given access to N i.i.d. samples, X := [x1, x2, . . . , xN ] ∈ Rn×N , generated according to the above model, we aim to estimate the true filters f ∗i , for i ∈ [L]. Once the filters are estimated, we can use standard decoding techniques, such as the square loss criterion in (1.4) to learn the activation maps for the individual maps. We focus on developing a novel method for filter estimation in this chapter."
    }, {
      "heading" : "4.2 Form of Cumulant Moment Tensors",
      "text" : "Tensor Preliminaries We consider 3rd order tensors in this chapter but the analysis is easily extended to higher order tensors. For tensor T ∈ Rn×n×n, its (i1, i2, i3)th entry is denoted by [T ]i1,i2,i3, ∀i1 ∈ [n], i2 ∈ [n], i3 ∈ [n]. A flattening or unfolding of tensor T ∈ R is the columnstacked matrix of all its slices, given by unfold(T ) := [[T ]:,:,1, [T ]:,:,2, . . . , [T ]:,:,n] ∈ Rn×n2. Define the Khatri-Rao product for vectors u ∈ Ra and v ∈ Rb as a row-stacked vector [u ⊙ v] := [u(1)v; u(2)v; . . . ; u(a)v] ∈ Rab. Khatri-Rao product is also defined for matrices with same columns. For M ∈ Ra×c and M ′ ∈ Rb×c, M ⊙M ′ := [M1 ⊙M ′1, . . . ,Mc ⊙M ′c, ] ∈\n82\nR ab×c, where Mi denotes the i th column of M . CumulantThe third order cumulant of a multivariate distribution is a third order tensor, which uses (raw) moments up to third order. Let C3 ∈ Rn×n2 denote the unfolded version of third order cumulant tensor, it is given by\nC3 := E[x(x ⊙ x)⊤]− unfold(Z) (4.4)\nwhere [Z]a,b,c := E[xa]E[xbxc] +E[xb]E[xaxc] +E[xc]E[xaxb]− 2E[xa]E[xb]E[xc], ∀a, b, c ∈ [n].\nUnder the convolution ICA model in Section 4.1.1, we show that the third order cumulant has a nice tensor form, as given below.\nLemma 4.1 (Form of Cumulants). The unfolded third order cumulant C3 in (4.4) has the following decomposition form\nC3 = ∑\nj∈[nL] λ∗jF∗j (F∗j ⊙F∗j )⊤ = F∗Λ∗ (F∗ ⊙ F∗)⊤ , where Λ∗ := Diag(λ∗1, λ∗2, . . . , λ∗nL) (4.5)\nwhere F∗j denotes the jth column of the column-stacked circulant matrix F∗ and λ∗j is the third order cumulant corresponding to the (univariate) distribution of w∗(j).\nFor example, if the lth activation is drawn from a Poisson distribution with mean λ̃, we have that λ∗l = λ̃. Note that if the third order cumulants of the activations, i.e. λ ∗ j ’s, are zero, we need to consider higher order cumulants. This holds for zero-mean activations and we need to use fourth order cumulant instead. Our method extends in a straightforward manner for higher order cumulants.\nThe decomposition form in (4.5) is known as the CANDECOMP/PARAFAC (CP) decomposition form [12] (the usual form has the decomposition of the tensor and not its unfolding, as above). We now attempt to recover the unknown filters f ∗i through decomposition of the third order cumulants C3. This is formally stated below.\n83\nObjective Function: Our goal is to obtain filter estimates fi’s which minimize the Frobenius norm ‖ · ‖F of reconstruction of the cumulant tensor C3,\nmin F\n‖C3 −FΛ (F ⊙F)⊤‖2F ,\ns.t. blkl(F) = U Diag(FFT(fl))UH, ‖fl‖2 = 1, ∀l ∈ [L], Λ = Diag(λ). (4.6)\nwhere blkl(F) denotes the lth circulant matrix in F . The conditions in (4.6) enforce blkl(F) to be circulant and for the filters to be normalized. Recall that U denotes the eigenvectors for circulant matrices. The rest of the chapter is devoted to devising efficient methods to solve (4.6).\nThroughout the chapter, we will use Fj to denote the jth column of F , and blkl(F) to denote the lth circulant matrix block in F . Note that F ∈ Rn×nL, Fj ∈ Rn and blkl(F) ∈ Rn×n."
    }, {
      "heading" : "4.3 Alternating Least Squares for Convolutional Ten-",
      "text" : "sor Decomposition\nTo solve the non-convex optimization problem in (4.6), we consider the alternating least squares (ALS) method with column stacked circulant constraint. We first consider the asymmetric relaxation of (4.6) and introduce separate variables F ,G and H for filter es-\n84\ntimates along each of the modes to fit the third order cumulant tensor C3. We then perform alternating updates by fixing two of the modes and updating the third one.\nmin F\n‖C3−FΛ (H⊙ G)⊤‖2F s.t. blkl(F) = U ·Diag(FFT(fl))·UH, ‖fl‖22 = 1, ∀l ∈ [L] (4.7)\nSimilarly, G and H have the same column-stacked circulant matrix constraint and are updated similarly in alternating steps. The diagonal matrix Λ is updated through normalization.\nWe now introduce the Convolutional Tensor (CT) Decomposition algorithm to efficiently solve (4.7) in closed form, using simple operations such as matrix multiplications and fast Fourier Transform (FFT). We do not form matrices F ,G and H ∈ Rn×nL, which are large, but only update them using filter estimates f1, . . . , fL, g1, . . . , gL, h1, . . . hL. Denote\nM := C3((H⊙ G)⊤)†, (4.8)\nwhere † denotes pseudoinverse. Let blkl(M) and blkl(Λ) denote the lth blocks of M and Λ. We have a closed form solution for filter update, once we have computed M , and we present the main result as follows.\nTheorem 4.1. [Closed form updates] The optimal solution f optl for (C.9) is given by\nf optl (p) =\n∑ i,j∈[n]\n‖blkl(M)j‖−1 · blkl(M)ij · Iqp−1 ∑\ni,j∈[n] Iqp−1\n, ∀p ∈ [n], q := (i− j) mod n. (4.9)\nFurther Λ = Diag(λ) is updated as λ(i) = ‖Mi‖, for all i ∈ [nL]. Note that Iqp−1 denotes the (q, (p− 1))th element of the identity matrix.\n85\nProof Sketch: Using the property of least squares, the optimization problem in (4.7) is equivalent to\nmin F\n‖C3((H⊙ G)⊤)†Λ†−F‖2F s.t. blkl(F) = U ·Diag(FFT(fl))·UH, ‖fl‖22 = 1, ∀l ∈ [L] (4.10)\nwhen (H ⊙ G) and Λ are full column rank. The full rank condition requires nL < n2 or L < n, and it is a reasonable assumption since otherwise the filter estimates are redundant. In practice, we can additionally regularize the update to ensure full rank condition is met. Since (C.8) has block constraints, it can be broken down in to solving L independent subproblems\nmin fl\n∥∥blkl(M) · blkl(Λ)† − U ·Diag(FFT(fl)) · UH ∥∥2 F\ns.t. ‖fl‖22 = 1, ∀l ∈ [L] (4.11)\nOur proof for the closed form solution is similar to the analysis in [57], where they proposed a closed form solution for finding the closest circulant/toeplitz matrix. For a detailed proof of Theorem 4.1, see Appendix C.2.\nThus, the reformulated problem in (C.9) can be solved in closed form efficiently. A bulk of the computational effort will go into computing M in (4.8). Computation of M requires 2L fast Fourier Transforms of length n filters and simple matrix multiplications without explicitly forming G or H. We make this concrete in the next section. The closed form update after getting M is highly parallel. With O(n2L/ log n) processors, it takes O(logn) time.\n86"
    }, {
      "heading" : "4.4 Algorithm Optimization to Reduce Memory and",
      "text" : "Computational Costs\nWe now focus on estimating M := C3((H ⊙ G)⊤)† in (4.8). If done naively, this requires inverting n2 × nL matrix and multiplication of n × n2 and n2 × nL matrices with O(n6) time. However, forming and computing with these matrices is very expensive when n (and L) are large. Instead, we utilize the properties of circulant matrices and the Khatri-Rao product ⊙ to efficiently carry out these computations implicitly. We present our final result on computational complexity of the proposed method. Recall that n is the filter size and L is the number of filters.\nLemma 4.2. [Computational Complexity] With multi-threading, the running time of our algorithm for n dimensional input and L number of filters is O(logn+logL) per iteration using O(L2n3) processors.\nNote that before the iterative updates, we compute the third order cumulant2 C3 once whose computational complexity is O(logN) with N logN processors, where N is the number of samples. However, this operation is not iterative. In contrast, alternating minimization (AM) requires pass over all the data samples in each iteration, while our algorithm requires only one pass of the data.\nThe parallel computational complexity of AM is as follows. In each iteration of AM, computing the derivative with respect to either filters or activation maps requires NL number of FFTs (requires O(NLn log n) serial time), and the degrees of parallelism are O(Nn logL) and O(Nn logn) respectively. Therefore with multi-threading, the running time of AM is O(max(logn logL, logn logN)) per iteration using O(max( nNL logN , nNL logL )) processors. Compar-\n2Instead of computing the cumulant tensor C3, a randomized sketch can be computed efficiently, following the recent work of [159], and the ALS updates can be performed efficiently without forming the cumulant tensor C3.\n87\ning with Lemma 4.2, we find that our algorithm is advantageous in the regime of N ≥ Ln2, which is the typical regime in applications.\nLet us describe how we utilize various algebraic structures to obtain efficient computation. Property 1 (Khatri-Rao product): ((H⊙G)⊤)† = (H⊙G)((H⊤H).⋆(G⊤G))†, where .⋆ denotes element-wise product.\nComputational Goals: Find ((H⊤H). ⋆ (G⊤G))† first and multiply the result with C3(H⊙G) to find M .\nWe now describe in detail how to carry out each of these steps."
    }, {
      "heading" : "4.4.1 Challenge: Computing ((H⊤H). ⋆ (G⊤G))†",
      "text" : "A naive implementation to find the matrix inversion ((H⊤H). ⋆ (G⊤G))† is very expensive. However, we incorporate the stacked circulant structure of G and H to reduce computation. Note that this is not completely straightforward since although G and H are column stacked circulant matrices, the resulting product whose inverse is required, is not circulant. Below, we show that however, it is partially circulant along different rows and columns.\nProperty 2 (Block circulant matrix): The matrix (H⊤H). ⋆ (G⊤G) consists of row and\ncolumn stacked circulant matrices.\nWe now make the above property precise by introducing some new notations. Define column stacked identity matrix I := [I, . . . , I] ∈ Rn×nL, where I is n × n identity matrix. Let U := Blkdiag(U, U, . . . U) ∈ RnL×nL be the block diagonal matrix with U along the diagonal. The first thing to note is that G and H, which are column stacked circulant matrices, can\n88\nbe written as\nG = I ·U · Diag(v) ·UH , v := [FFT(g1); FFT(g2); . . . ; FFT(gL)], (4.12)\nwhere g1, . . . , gL are the filters corresponding to G, and similarly for H, where the diagonal matrix consists of FFT coefficients of the respective filters h1, . . . , hL.\nBy appealing to the above form, we have the following result. We use the notation blkij(Ψ) for a matrix Ψ ∈ RnL×nL to denote (i, j)th block of size n× n.\nLemma 4.3 (Form of (H⊤H). ⋆ (G⊤G) ). We have\n((H⊤H). ⋆ (G⊤G))† = U ·Ψ† ·UH, (4.13)\nwhere Ψ ∈ RnL×nL has L by L blocks, each block of size n× n. Its (j, l)th block is given by\nblkjl (Ψ) = Diag(FFT(γ(gj, gl). ∗ γ(hj , hl))) ∈ Rn×n (4.14)\nwhere γ(gj, gl) := reverse(reverse(gj)∗ gl) and γ(hj, hl) := reverse(reverse(hj)∗hl).\nTherefore, the inversion of (H⊤H).⋆(G⊤G) can be reduced to the inversion of row-and-column stacked set of diagonal matrices which form Ψ. Computing Ψ simply requires FFT on all 2L filters g1, . . . , gL and h1, . . . , hL, i.e. 2L FFTs, each on length n vector. We propose an efficient iterative algorithm to compute Ψ† via block matrix inversion theorem[68] in Appendix C.3."
    }, {
      "heading" : "4.4.2 Challenge: Computing M = C3(H⊙ G) · ((H⊤H). ⋆ (G⊤G))†",
      "text" : "Now that we have computed ((H⊤H). ⋆ (G⊤G))† efficiently, we need to compute the resulting matrix with C3(H⊙ G) to obtain M . We observe that the mth row of the result M is given\n89\nby\nMm = ∑\nj∈[nL] Uj DiagH (z) Φ(m) Diag (v) (Uj)HUjΨ†UH, ∀m ∈ [nL], (4.15)\nwhere v := [FFT(g1); . . . ; FFT(gL)], z := [FFT(h1); . . . ; FFT(hL)] are concatenated FFT coefficients of the filters, and\nΦ(m) := UHI⊤Γ(m)IU, [Γ(m)]ij := [C3] m i+(j−1)n, ∀i, j,m ∈ [n] (4.16)\nNote that Φ(m) and Γ(m) are fixed for all iterations and need to be computed only once. Note that Γ(m) is the result of taking mth row of the cumulant unfolding C3 and matricizing it. Equation (4.15) uses the property that Cm3 (H ⊙ G) is equal to the diagonal elments of H⊤Γ(m)G.\nWe now bound the cost for computing (4.15). (1) Inverting Ψ takes O(logL+logn) time with O(n2L2/(logn+logL)) processors according to appendix C.3. (2) Since Diag(v) and Diag(z) are diagonal and Ψ is a matrix with diagonal blocks, the overall matrix multiplication in equation (4.15) takes O(L2n2) time serially with O(L2n2) degree of parallelism for each row. Therefore the overall serial computation cost is O(L2n3) with O(L2n3) degree of parallelism. With multi-threading, the running time is O(1) per iteration using O(L2n3) processes. (3) FFT requires O(n logn) serial time, with O(n) degree of parallelism. Therefore computing 2L FFT’s takes O(logn) time with O(Ln) processors.\nCombining the above discussion, it takes O(logL+ log n) time with O(L2n3) processors.\n90"
    }, {
      "heading" : "4.5 Experiments: Comparison with Alternating Mini-",
      "text" : "mization\nWe compare our convolutional tensor decomposition framework with solving equation (1.4) using alternating (between filters and activation map) minimization method where gradient descent is employed to update fi and wi alternatively. The error comparison between our proposed convolutional tensor algorithm and the alternating minimization algorithm is in figure 4.2a. We evaluate the errors for both algorithms by comparing the reconstruction of error and filter recovery error3. Our algorithm converges much faster to the solution than the alternating minimization algorithm. In fact, alternating minimization leads to spurious solution where the reconstruction error is significantly larger compared to the error achieved by the tensor method. The error bump in the reconstruction error curve in figure 4.2a for tensor method is due to the random initialization following deflation of one filter, and estimation of the second one. The running time is also reported in figure 4.2b and 4.2c between our proposed convolutional tensor algorithm and the alternating minimization. Our algorithm is orders of magnitude faster than the alternating minimization. Both our algorithm and alternating minimization scale linearly with number of filters. However convolutional tensor algorithm is almost constant time with respect to the number of samples, whereas the alternating minimization scales linearly. This results in huge savings in running time for large datasets.\n3Note that circulant shifts of the filters result in the same reconstruction error, and we report the lowest error between the estimated filters and all circulant shifts of the ground-truth.\n91"
    }, {
      "heading" : "4.6 Application: LearningWord-sequence Embeddings",
      "text" : ""
    }, {
      "heading" : "4.6.1 Word-Sequence Modeling and Formulation",
      "text" : "Our ConvDic+DeconvDec framework focuses on a convolutional dictionary model to summarize phrase templates, and then decode word-sequence signals to obtain the word-sequence embeddings. The first question is how to encode the word sequence into a signal, to be input to the convolutional model and we discuss that below."
    }, {
      "heading" : "From raw text to signals",
      "text" : "Word encoding: A word is represented as a one-hot encoding vector, i.e. with vector ei ∈ Rd whose ith entry is 1 and other entries are 0, where i is the index of the word in the dictionary. Alternatively, one could use the word2vec embeddings instead of one-hot encodings. We then stack the one-hot encoding vectors of each sentence together to form a encoding matrix. The stacking order conforms the word-sequence order.\n92\nTo be precise, let us consider sentenc with N words. The encoding matrix of this wordsequence Sseq is Sseq := [sword1 , sword2, . . . , swordN ] ∈ Rd×N .\nPrincipal components: Now that we have encoded words in each sentence, we want to find a compact representation of them in terms of a dictionary model. However, the encoding matrices are too sparse to fit a convolutional model in the word space. Instead, we perform dimensionality reduction through PCA and carry out dictionary modeling in the projected space.\nConcretely, we stack the encoding matrices side by side as S := [Sseq1 ,Sseq2, . . . ,SseqM ] ∈ R d×( ∑M i=1 Ni), assuming there are M number of sentences in the collection of varying lengths N1, N2 and so on. Let U ∈ Rd×k denote the top k left eigenvectors of S. We consider Yi := U⊤Sseq1 ∈ Rk×Ni, for each sentence i. We treat the rows of Yi independently in parallel and fit convolutional model to each row. Denote jth row of Yi as y(j)i , and thus\nYi =   y (1) i ...\ny (k) i\n  .\n93\nEach y (j) i is generated through a convolutional dictionary model over phrase templates and activation maps. Our goal in the learning phase is to learn template phrases for the collection of [y (j) i ] over all word-sequences ∀i ∈ [M ] across all parallel directions ∀j ∈ [k]. We will state the learning problem formally in the next section. Since all the coordinates are independent and the phrase templates are learned in parallel over all the coordinates, we drop the index j to denote a coordinate of the ith word sequence y (j) i . In the following subsection, a patch from y (j) i will be denoted as x."
    }, {
      "heading" : "Comprehension Phase – Learning Phrase Templates",
      "text" : "A word sequence is composed of superposition of overlapping patches, therefore we are interested in learning a generative model over overlapping patches. We can also view these patches as phrases. A length n patch x is generated as the superposition of L phrase embeddings f ∗l convolved at L activation maps w ∗ l , ∀l ∈ [L]. Due to the property of the convolution, the convolution is reformulated as the multiplication of F∗ and w∗, where F∗ := [Cir(f ∗1 ),Cir(f ∗2 ), . . . ,Cir(f ∗L)] is the concatenation of circulant matrices and w∗ is the\n94\nrow-stacked vector w∗ :=   w∗1 w∗2 ...\nw∗L\n  ∈ RnL. To be precise, a patch\nx = ∑\nl∈[L] f ∗l ∗wl∗ = F∗ · w∗, (4.17)\nThis is illustrated in Fig 4.6(a). Cir(f ∗l ) is circulant matrix corresponding to phrase template f ∗l , whose columns are shifted versions of f ∗ l as shown in Fig 4.6(a). Note that although F∗ is a n by nL matrix, there are only nL free parameters. Given access to the collection of word-sequence sample patches, X := [x1, x2, . . .], generated according to the above model, we aim to estimate the true template phrases f ∗i , for i ∈ [L].\n95\nIf the patches are in the same coordinate of the word sequence, these patches share a common set of phase templates, but their activation maps are different. The activation maps are the discriminative features that distinguish different patches. Once the template phrases are estimated, we can use standard decoding techniques, such as the square loss criterion in (1.4) to learn the activation maps for the individual maps."
    }, {
      "heading" : "Feature-extraction Phase – Word-sequence Embeddings",
      "text" : "Activation maps in a coordination: After learning a good set of phrase templates {f1, . . . , fL} and thus F , we use the deconvolutional decoding (DeconvDec) to obtain the activation maps for the jth coordinate. For each observed coordinate of the word-sequence y (j) i , the activation map w∗l in (4.17) indicates the locations where i th template phrase f ∗l is activated and w ∗ is the row-stacked vector w∗ := [w∗1;w ∗ 2; . . . w ∗ L]. An estimation of w ∗, w (j) i , is achieved as follows\nw (j) i = F †y(j)i\n⊤ . (4.18)\nNote that the estimated phrase templates are zero padded to match the length of the wordsequence.\nWe assume that the elements of w∗ are drawn from some product distribution, i.e. different entries are independent of one another, and we have the independent component analysis (ICA) model in (4.17). When the distribution encourages sparsity, e.g. Bernoulli-Gaussian, only a small subset of locations are active, and we have the sparse coding model in that case. We can also extend to dependent distributions such as Dirichlet for w∗, along the lines of [32], but limit ourselves to ICA model for simplicity. This activation map w (j) i ∈ RNi·L contains sequence embeddings from coordinate j only, and will be used as one coordinate of our final word-sequence embeddings.\n96\nVarying sentence length: One difficulty in learning the template phrases using our convolutional tensor decomposition model is that different word-sequence has a different length Ni, therefore the activation maps are of varying length as well. We resolved this problem by max-k pooling. In other words, we extract most informative global discriminative features from the activation maps, as illustrated in Figure 4.4. Finally, we concatenate all the max-k pooled coordinate sequence embeddings as a long vector as the final word-sequence embedding.\nThe overall framework flow is depicted in Fig 4.4."
    }, {
      "heading" : "4.6.2 Evaluating Embeddings through Downstream Tasks",
      "text" : "We evaluate the quality of our word sequence embeddings using three challenging natural language process tasks: sentiment classification, paraphrase detection, and semantic textual similarity estimation. Eight datasets which cover various domains are used as shown in Table 4.1.\nFor all the datasets, we train a simple logistic regression model on the training samples and report test classification accuracy using a 10-fold cross validation. Sentiment analysis and paraphrase detection belong to binary classification tasks. In a binary classification task, either accuracy or F score is used as evaluate metric. Recall that F-score is the harmonic\n97\nmean of precision and recall, i.e., F = 2 · (precision · recall)/precision + recall. Precision is the number of true positives divided by the total number of elements labeled as belonging to the positive class, and recall is the number of true positives divided by the total number of elements that belong to the positive class.\nOur ConvDic+DeconvDec learns word-sequence embeddings from scratch and requires no pre-training. When working on a new dataset from a new domain, we train fresh set of phrase templates as called domain phrase templates. Using these domain phrase templates, we decode activation maps and then form phrase-embeddings. Our approach is different from skip thoughts, where universal phrase embeddings are generated [103]."
    }, {
      "heading" : "Evaluation Task: Sentiment Classification",
      "text" : "Sentiment analysis is an important task in natural language process as automated labeling of word sequences into positive and negative opinions is used in various settings. We evaluate our sentence embeddings on two datasets from different domains, such as movie review and subjective and objective comments, as in Table 4.1. Using word-sequence embeddings combined with NB features, we obtain the state-of-the-art classification results for both these datasets as in Table 4.2."
    }, {
      "heading" : "Evaluation Task: Paraphrase Detection",
      "text" : "We consider the paraphrase detection task on the Microsoft paraphrase corpus [137, 55]. We employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on the ground truth binary labels with our learned sentence embeddings. The remaining test data is used to calculate classification error.\n4The word similarities information they use are either trained in Wikipedia (4.4 million articles in contrast to the 4076 sentences of paraphrase dataset we use) or from WordNet with expert knowledge.\n98"
    }, {
      "heading" : "Method MR SUBJ",
      "text" : "99\nAs discussed in [154], we combine the pair of sentence embeddings produced earlier wL and wR, i.e., the embedding for the right and the left sentences. We generate features for classification using both the distance (absolute difference) and the product between the pair (wL, wR): [wL ⊙ wR, ‖wL − wR‖], where ⊙ denotes the element-wise multiplication.\nIn contrast to other unsupervised methods which are trained using outside information such as wordnet and parse trees, our unsupervised approach use no extra information, and still achieves comparable results with the state of art [162] as in table 4.3. We show some examples of paraphrase and non-paraphrase we identified.\nParaphrase detected: (1) Amrozi accused his brother, whom he called ”the witness”, of deliberately distorting his evidence. (2) Referring to him as only ”the witness”, Amrozi accused his brother of deliberately distorting his evidence. The two sentences are the “difficult sentence” to show how our algorithm detect paraphrases since they are not simple switching of clauses, and the sentence structures differ quite significantly in the two sentences.\nNon-paraphrase detected : (1) I never organised a youth camp for the diocese of Bendigo. (2) I never attended a youth camp organised by that diocese. Similarly with non-paraphrase detection, the two sentences share common words such as youth camp and organized, but our method is able to successfully detect them as non-paraphrase."
    }, {
      "heading" : "Evaluation Task: Semantic Textual Similarity Estimation",
      "text" : "For the Semantic Textual Similarity (STS) task, the goal is to predict a real-valued similarity score in a range [1, K] given a sentence pair. We include datasets from STS task in various domains including news, image and video description, glosses from WordNet/OntoNotes, the output of machine translation systems with reference translation.\n100\nTo frame semantic test similarity estimation task into the multi-class classification framework, the gold rating τ ∈ [K1, K2] is discretized as p ∈ ∆K2−K1 in the follow manner [154], pi = ⌊τ⌋ − τ + 1 if i = ⌊τ⌋+ 1−K1, pi = τ − ⌊τ⌋ if i = ⌊τ⌋+ 2−K1, and pi = 0 otherwise. This reduces to finding a predicted p̂θ ∈ ∆K2−K1 given model parameters θ to be closest to p in terms of KL divergence [154]. We use a logistic regression classifier to predict p̂θ and estimate τ̂θ = [K1, . . . , K2]p̂.\nResults on STS task datasets are illustrated in Table 4.4. As in [161], Pearson’s r of the median, 75th percentile, and highest score from the official task rankings are showed. We then compare our method against the performance of supervised models in [161]: PARAGRAMPHRASE (PP), projection (proj.), deep-averaging network (DAN), recurrent neural network (RNN) and LSTM; as well as the state-of-the-art unsupervised model skip-thought vectors [103].\nAs we can see from the table, LST is performing poorly even though a back-propagation after seeing the training labelings is carried out for sequence embedding learning. Our method is an unsupervised approach as in skip-thought vectors. However, our algorithm doesn’t output universal word-sequence embeddings across domains. We train a fresh model and a new set of domain phrase templates from scratch. Therefore our algorithm is performing better for these individual datasets on the STS task.\n101"
    }, {
      "heading" : "4.7 Conclusion",
      "text" : "In this chapter, we proposed a novel tensor decomposition framework for learning convolutional dictionary models. Unlike the popular alternating minimization, our method avoids expensive decoding of activation maps in each step and can reach better solutions with faster run times. We derived efficient updates for tensor decomposition based on modified alternating least squares, and it consists of simple operations such as FFTs and matrix multiplications. Our framework easily extends to convolutional models for higher dimensional signals (such as images), where the circulant matrix is replaced with block circulant matrices [73]. More generally, our framework can handle general group structure, by replacing the FFT operation with the appropriate group FFT [106]. By combining the advantages of tensor methods with a general class of invariant representations, we thus have a powerful paradigm for learning efficient latent variable models and embeddings in a variety of domains.\n102\nChapter 5"
    }, {
      "heading" : "Latent Tree Model Learning through",
      "text" : ""
    }, {
      "heading" : "Hierarchical Tensor Decomposition",
      "text" : "In previous chapters, we introduced latent dirichlet allocation and its variations to model data with “shallow” structure, for instance, multi-view model. However, real world data is usually generated through more complicated models such as a latent (hierarchical) tree graphical model. Latent tree graphical models characterize a probability distribution involving observed and hidden variables which are Markovian on a tree. Learning is challenging as the number of latent variables and the location of them are not observed. We present an integrated approach to structure and parameter estimation in latent tree graphical models, where some nodes are hidden.\nWe present an integrated approach to structure and parameter estimation in latent tree models. Our method overcomes all the above shortcomings simultaneously. First, it automatically learns the latent variables and their locations. Second, our method achieves consistent structure estimation with log(p) computational complexity with enough computational resources via “divide-and-conquer” manner. We also present a rigorous proof on the\n103\nreplacements\n(a) Latent tree\n= + +\n= + + = + + = + +\n(b) Hierarchical tensor decomposition\nFigure 5.1: Learning hierarchical latent variable graphical model parameter using hierarchical tensor decomposition.\nglobal consistency of the structure and parameter estimation under the “divide-and-conquer” framework. Our consistency guarantees are applicable to a broad class of linear multivariate latent tree models including discrete distributions, continuous multivariate distributions (e.g. Gaussian), and mixed distributions such as Gaussian mixtures. This model class is much more general than discrete models, prevalent in most of the previous works on latent tree models [128, 127, 59, 17]. Third, our algorithm considers the inverse method of moments, and estimates the model parameters via tensor decomposition with low perturbation guarantees. Moreover, we carefully integrate structure learning with parameter estimation, based on tensor spectral decompositions [11]. Finally, our approach has a high degree of parallelism, and is bulk asynchronous parallel [65].\nIn addition to the aforementioned technical contributions, we showcase the impact of our work by applying it to two real datasets originating from the healthcare domain. The algorithm was used to discover hidden patterns, or concepts reflecting co-occurrences of particular diagnoses in patients in outpatient and intensive care settings. While such a task is currently done through manual analysis of the data, our method provides an automated method for the discovery of novel clinical concepts from high dimensional, multi-modal data.\n104\nOur overall approach follows a “divide-and-conquer” strategy that learns models over small groups of variables and iteratively merges into a global solution. The structure learning involves combinatorial operations such as minimum spanning tree construction and local recursive grouping; the parameter learning is based on the method of moments and on tensor decompositions. Our method is guaranteed to correctly recover the unknown tree structure and the model parameters with low sample complexity for the class of linear multivariate latent tree models which includes discrete and Gaussian distributions, and Gaussian mixtures. Our bulk asynchronous parallel algorithm is implemented in parallel using the OpenMP framework and scales logarithmically with the number of variables and linearly with dimensionality of each variable.\nOur experiments confirm a high degree of efficiency and accuracy on large datasets of electronic health records. We use latent tree model for discovering a hierarchy among diseases based on comorbidities exhibited in patients’ health records, i.e. co-occurrences of diseases in patients. In particular, two large healthcare datasets of 30K and 1.6M patients are used to build the latent disease trees, where clinically meaningful disease clusters are identified as shown in fig 5.4 and 5.5. The proposed algorithm also generates intuitive and clinically meaningful disease hierarchies."
    }, {
      "heading" : "5.1 Latent Tree Graphical Model Preliminaries",
      "text" : "We denote [n] := {1, . . . , n}. Let T := (V, E) denote an undirected tree with vertex set V and edge set E . The neighborhood of a node vi, nbd(vi), is the set of nodes to which vi is directly connected on the tree. Leaves which have a common neighboring node are known as siblings, and the common node is referred to as their parent. Let N denote the number of samples. An example of latent tree is depicted in Figure 5.2(a).\n105\nThere are two types of variables on the nodes, namely, the observable variables, denoted by X := {x1, . . . , xp} (p := |X |), and hidden variables, denoted by H := {h1, . . . , hm} (m := |H|). Let Y := X ∪ H denote the complete set of variables and let yi denote the random variable at node vi ∈ V, and similarly let yA denote the set of random variables in set A.\nA graphical model is defined as follows: given the neighborhood nbd(vi) of any node vi ∈ V, the variable yi is conditionally independent of the rest of the variables in V, i.e., yi ⊥ yj|ynbd(vi), ∀vj ∈ V\\ {vi ∪ nbd(vi)}.\nLinear Models We consider the class of linear latent tree models. The observed variables xi are random vectors of length di, i.e., xi ∈ Rdi , ∀i ∈ [p] while the latent nodes are k-state categorical variables, i.e., hi ∈ {e1, . . . , ek}, where ej ∈ Rk is the jth standard basis vector. Although di can vary across variables, we use d for notation simplicity. In other words, for notation simplicity, xi ∈ Rd, ∀i ∈ [p] is equivalent to xi ∈ Rdi , ∀i ∈ [p]. For any variable yi with neighboring hidden variable hj , we assume a linear relationship:\nE[yi|hj] = Ayi|hjhj , (5.1)\nwhere transition matrix Ayi|hj ∈ R d×k is assumed to have full column rank, ∀yi, hj ∈ V. This implies that k ≤ d, which is natural if we want to enforce a parsimonious model for fitting the observed data.\nFor a pair of (observed or hidden) variables ya and yb, consider the pairwise correlation matrix E [ yay ⊤ b ] where the expectation is over samples. Since our model assumes that two observable variables interact through at least a hidden variable, we have\nE[yay ⊤ b ] :=\n∑\nei\nE[hj = ei]Aya|hj=eiA ⊤ yb|hj=ei (5.2)\n106\nWe see that E[yay ⊤ b ] is of rank k since Aya|hj=ei or Ayb|hj=ei is of rank k."
    }, {
      "heading" : "5.2 Overview of Approach",
      "text" : "The overall approach is depicted in Figure 5.2, where (a) and (b) show the data preprocessing step, (c) - (e) illustrate the divide-and-conquer step for structure and parameter learning.\nMore specifically, we start with the parallel computation of pairwise multivariate information distances. Information distance roughly measures the extent of correlation between different pairs of observed variables and requires SVD computations in step (a). Then in step (b) a Minimum Spanning Tree (MST) is constructed over observable variables in parallel [24] using the multivariate information distance. The local groups are also obtained through MST so that they are available for the structure and parameter learning step that follows.\nThe structure and parameter learning is done jointly through a divide-and-conquer strategy. Step-(c) illustrates the divide step (or local learning), where local structure and parameter estimation is performed. It also performs the local merge to obtain group level structure and parameter estimates. After the local structure and parameter learning is finished within the\n107\ngroups, we perform merge operations among groups, again guided by the Minimum Spanning Tree structure. For the structure estimation it consists of a union operation of sub-trees; for the parameter estimation, it consists of linear algebraic operations. Since our method is unsupervised, an alignment procedure of the hidden states is carried out which finalizes the global estimates of the tree structure and the parameters."
    }, {
      "heading" : "5.3 Structure Learning",
      "text" : "Structure learning in graphical models involves finding the underlying Markov graph, given the observed samples. For latent tree models, structure can be estimated via distance based methods. This involves computing certain information distances between any pair of observed variables, and then finding a tree which fits the computed distances.\nMultivariate information distances: We propose an additive distance for multivariate linear latent tree models. For a pair of (observed or hidden) variables ya and yb, consider the pairwise correlation matrix E [ yay ⊤ b ] (the expectation is over samples). Note that its rank is k, dimension of the hidden variables.\nDefinition 5.1. The multivariate information distance between nodes i and j is defined as\ndist(va, vb) := − log\nk∏ i=1 σi ( E(yay ⊤ b ) )\n√ det(E(yay⊤a )) det(E(yby ⊤ b ))\n(5.3)\nwhere {σ1(·), . . . , σk(·)} are the top k singular values.\nNote that definition 5.1 suggests that this multivariate information distance allows heterogeneous settings where the dimensions of ya and yb are different (and ≥ k).\n108\nFor latent tree models, we can find information distances which are provably additive on the underlying tree in expectation, i.e. the expected distance between any two nodes in the tree is the sum of distances along the path between them.\nLemma 5.1. The multivariate information distance is additive on the tree T , i.e., dist(va, vc) = dist(va, vb) + dist(vb, vc), where vb is a node in the path from va to vc and va,vb,vc ∈ V.\nRefer to Appendix D.1 for proof. The empirical distances can be computed via rank-k SVD of the empirical pairwise moment matrix Ê[yay ⊤ b ] Note that the distances for all the pairs can be computed in parallel.\nFormation of local groups via MST: Once the empirical distances are computed, we construct a Minimum Spanning Tree (MST), based on those distances. Note that the MST can be computed efficiently in parallel [156, 122]. We now form groups of observed variables over which we carry out learning independently, without any coordination. These groups are obtained by the (closed) neigborhoods in the MST, i.e. an internal node and its one-hop neighbors form a group. The corresponding internal node is referred to as the group leader. See Figure 5.2(b).\nLocal recursive grouping (LRG): Once the groups are constructed via neighborhoods of MST, we construct a sub-tree with hidden variables in each group (in parallel) using the recursive grouping introduced in [41]. The recursive grouping uses the multivariate information distances and decides the locations and numbers of hidden nodes. It proceeds by deciding which nodes are siblings, which proceeds as follows: consider two observed nodes vi, vj which are siblings on the tree with a common parent vl, and consider any other observed node va. From additivity of the (expected) information distances, we have dist(vi, va) = dist(vi, vl) + dist(vl, va) and similarly for dist(vj , va). Thus, we have Φ(vi, vj; va) := dist(vi, va) − dist(vj, va) = dist(vi, vl) − dist(vj , vl), which is independent of node va. Thus, comparing the quantity Φ(vi, vj ; va) for different nodes va allows us to\n109\nconclude that vi and vj are siblings. Once the siblings are inferred, the hidden nodes are introduced, and the same procedure repeats to construct the higher layers. Note that whenever we introduce a new hidden node hnew as a parent, we need to estimate multivariate information distance between hnew and nodes in active set Ω. This is discussed in [41] with details.\nWe will describe the LRG in details with integrated parameters estimation in Procudure 6 in Section 5.5. In the end, we obtain a sub-tree over the local group of variables. After this local recursive grouping test, we store the neighborhood relationship for the leader vi using an adjacency list N i. We call the resultant local structure as latent sub-tree."
    }, {
      "heading" : "5.4 Parameter Estimation",
      "text" : "Along with the structure learning, we adopt a moment-based spectral learning technique for parameter estimation. This is a guaranteed and fast approach to recover parameters via moment matching for third order moments of the observed data. In contrast, traditional approaches such as Expectation Maximization (EM) suffer from spurious local optima and cannot provably recover the parameters.\nA latent tree with three leaves: We first consider an example of three observable leaves x1, x2, x3 (i.e., a triplet) with a common hidden parent h. We then clarify how this can be generalized to learn the parameters of the latent tree model. Let ⊗ denote for the tensor product. For example, if x1, x2, x3 ∈ Rd, we have x1 ⊗ x2 ⊗ x3 ∈ Rd×d×d.\nProperty 5.1 (Tensor decomposition for triplets). For a linear latent tree model with three observed nodes v1, v2, v3 with joint hidden node h, we have\nE(x1 ⊗ x2 ⊗ x3) = k∑\nr=1\nP[h = er]A r x1|h ⊗Arx2|h ⊗ Arx3|h, (5.4)\n110\nwhere Arxi|h = E(xi|h = er), i.e., rth column of the transition matrices from h to xi. The tensor decomposition method of [11] provably recovers the parameters Axi|h, ∀i ∈ [3], and P[h].\nTensor decomposition for learning latent tree models: We employ the above approach for learning latent tree model parameters as follows: for every triplet of variables ya, yb, and yc (hidden or observed), we consider the hidden variable hi which is the joining point of ya, yb and yc on the tree. They form a triplet model, for which we employ the tensor decomposition procedure. However, it is wasteful to do it over all the triplets in the latent tree.\nIn the next section, we demonstrate how we efficiently estimate the parameters as we learn the structure, and minimize the tensor decompositions required for estimation. Issues such as alignment of hidden labels across different decompositions will also be addressed."
    }, {
      "heading" : "5.5 Integrated Structure and Parameter Estimation",
      "text" : "So far, we described high-level procedures of structure estimation through local recursive grouping (LRG) and parameter estimation through tensor decomposition over triplets of variables, respectively. We now describe an integrated and efficient approach which brings all these ingredients together. In addition, we provide merging steps to obtain a global model, using the sub-trees and parameters learnt over local groups."
    }, {
      "heading" : "5.5.1 Local Recursive Grouping with Tensor Decomposition",
      "text" : "Next we present an integrated procedure where the parameter estimation goes hand-in-hand with structure estimation. Intuitively, we find efficient groups of triplets to carry out tensor decomposition simultaneously, as we estimate the structure through recursive grouping. In\n111\nrecursive grouping, pairs of nodes are recursively grouped as siblings or as parent-child. As this process continues, we carry out tensor decompositions whenever there are siblings present as triplets. If there are only a pair of siblings, we find an observed node with closest distance to the pair. Once the tensor decompositions are carried out on the observed nodes, we proceed to structure and parameter estimation of the added hidden variables. The samples of the hidden variables can be obtained via the posterior distribution, which is learnt earlier through tensor decomposition. This allows us to predict information distances and third order moments among the hidden variables as process continues. The full algorithm is given in Procedure 6.\nProcedure 6 LRG with Parameter Estimation Input: for each vi ∈ Xint, active set Ω := nbd[vi;MST]. Output: for each vi ∈ Xint, local sub-tree adjacency matrix N i, and E[ya|yb] for all (va, vb) ∈\nN i. 1: Active set Ω ← nbd[vi;MST] 2: while |Ω| > 2 do 3: for all va, vb ∈ Ω do 4: if Φ(va, vb; vc) = dist(va, vb), ∀ vc ∈ Ω\\{va, vb} then 5: va is a leaf node and vb is its parent, 6: Eliminate va from Ω. 7: if −dist(va, vb) < Φ(va, vb; vc) = Φ(va, vb; v′c) < dist(va, vb), ∀vc, v′c ∈ Ω\\{va, vb}\nthen 8: va and vb are siblings,eliminate va and vb from Ω, add hnew to Ω. 9: Introduce new hidden node hnew as parent of va and vb. 10: if more than 3 siblings under hnew then 11: find vc in siblings, 12: else 13: find vc = argminvc∈Ω dist(va, vc). 14: Estimate empirical third order moments Ê(ya ⊗ yb ⊗ yc) 15: Decompose Ê(ya ⊗ yb ⊗ yc) to get Pr[hnew] and E(yr|hnew), ∀r = {a, b, c}.\nThe divide-and-conquer local spectral parameter estimation is superior compared to popular EM-based method [41], which is slow and prone to local optima. More importantly, EM can only be applied on a stable structure since it is a global update procedure. Our proposed spectral learning method, in contrast, is applied locally over small groups of variables, and is a guaranteed learning with sufficient number of samples [11]. Moreover, since\n112\nwe integrate structure and parameter learning, we avoid recomputing the same quantities, e.g. SVD computations are required both for structure estimation (for computing distances) and parameter estimation (for whitening the tensor). Combining these operations results in huge computational savings (see Section 5.6 for the exact computational complexity of our method).\nProcedure 7 Merging and Alignment Correction (MAC) Input: Latent sub-trees N i for all internal nodes i. Output: Global latent tree T structure and parameters. 1: for N i and N j in all the sub-trees do 2: if there are common nodes between N i and N j then 3: Find the shortest path path(vi, vj;N i) between vi and vj on N i and path(vi, vj;N j)\nin N j ; 4: Union the only conflicting path(vi, vj ;N i) and path(vi, vj;N j) according to equation (5.7) ; 5: Attach other nodes in N i and N j to the union path; 6: Perform alignment correction as described in Procedure 8."
    }, {
      "heading" : "5.5.2 Merging and Alignment Correction",
      "text" : "We have so far learnt sub-trees and parameters over local groups of variables, where the groups are determined by the neighborhoods of the MST. The challenge now is to combine them to obtain a globally consistent estimate. There are non-trivial obstacles to achieving this: first, the constructed local sub-trees span overlapping groups of observed nodes, and possess conflicting paths. Second, local parameters need to be re-aligned as we merge the subtrees to obtain globally consistent estimates due to the nature of unsupervised learning. To be precise, different tensor decompositions lead to permutation of the hidden labels (i.e. columns of the transition matrices) across triplets. Thus, we need to find the permutation matrix correcting the alignment of hidden states of the transition matrices, so as to guarantee global consistency.\n113\nStructure Union: We now describe the procedure to merge the local structures. We merge them in pairs to obtain the final global latent tree. Recall that N i denotes a sub-tree constructed locally over a group, whose leader is node vi. Consider a pair of subtrees N i and N j, whose group leaders vi and vj are neighbors on the MST. Since vi and vj are neighbors, both the sub-trees contain them, and have different paths between them (with hidden variables added). Moreover, note that this is the only conflicting path in the two subtrees. We now describe how we can resolve this: in N i, let hi1 be the neighboring hidden node for vi and h i 2 be the neighbor of vj . There could be more hidden nodes between h i 1 and hi2. Similarly, in N i, let hj1 and hj2 be the corresponding nodes in N j. The shortest path between vi and vj in the two sub-trees are given as follows:\npath(vi, vj;N i) := [vi − hi1 − . . .− hi2 − vj] (5.5) path(vi, vj ;N j) := [vi − hj1 − . . .− hj2 − vj] (5.6)\nThen the union path is formed as follows:\nmerge(path(vi, vj;N i), path(vi, vj;N j))\n:= [vi − hi1 − . . .− hi2 − hj1 . . . hj2 − vj] (5.7)\nIn other words, we retain the immediate hidden neighbor of each group leader, and break the paths on the other end. For example in Figure 5.2(d1,d2), we have the path v3−h1− v5 in N 3 and path v3 − h3 − h2 − v5 in N 5. The resulting path is v3 − h1 − h3 − h2 − v5, as see in Figure 5.2(e). After the union of the conflicting paths, the other nodes are attached to the resultant latent tree. We present the pseudo code in Procedure 7 in Appendix D.5.\nParameter Alignment Correction: As mentioned before, our parameter estimation is unsupervised, and therefore, columns of the estimated transition matrices may be permuted for different triplets over which tensor decomposition is carried out. Note that the parameter\n114\nProcedure 8 Parameter Alignment Correction (Gr denotes reference group, Go denotes the list of other groups, each group has a reference node denoted as Rl, and the reference node in Gr is Rg. The details on alignment at line 8 is in Appendix D.5.) Input: Triplets and unaligned parameters estimated for these triplets, denoted as Trip(yi, yj, yk). Output: Aligned parameters for the entire latent tree T . 1: Select Gr which has sufficient children; 2: Select refer node Rg in Gr; 3: for all a, b in Gr do 4: Align Tripin(ya, yb,Rg); 5: for all ig in Go do 6: Select refer node Rl in Go[ig]; 7: Align Tripout(Rg, ya,Rl) and Tripout(Rl, yi,Rg); 8: for all i, j in Go[ig] do 9: Align Trip(yi, yj,Rl);\nestimation within the triplet is automatically acquired through the tensor decomposition technique, so that the alignment issue only arises across triplets. We refer to this as the alignment issue and it is required at various levels.\nThere are two types of triplets, namely, in-group and out-group triplets. A triplet of nodes Trip(yi, yj, yl) is said to be in-group (denoted by Tripin(yi, yj, yl) ) if its containing nodes share a joint node hk and there are no other hidden nodes in path(yi, hk), path(yj, hk) or path(yl, hk). Otherwise, this triplet is out-group denoted by Tripout(yi, yj, yl). We define a group as sufficient children group if it contains at least three in-group nodes.\nDesigning an in-group alignment correction with sufficient children is relatively simple: we achieve this by including a local reference node for all the in-group triplets. Thus, all the triplets are aligned with the reference node. The alignment correction is more challenging if lacking sufficient children. We propose out-group alignment to solve this problem. We first assign one group as a reference group, and the local reference node in that reference group becomes the global reference node. In this way, we align all recovered transition matrices\n115\nin the same order of hidden states as in the reference node. Overall, we merge the local structures and align the parameters from LRG local sub-trees using Procedure 7 and 8."
    }, {
      "heading" : "5.6 Theoretical Gaurantees",
      "text" : "Correctness of Proposed Parallel Algorithm: We now provide the main result of this chapter on global consistency for our method, despite the high degree of parallelism.\nTheorem 5.1. Given samples from an identifiable latent tree model, the proposed method consistently recovers the structure with O(log p) sample complexity and parameters with O(poly p) sample complexity.\nThe proof sketch is in Appendix D.3.\nComputational Complexity: We recall some notations here: d is the observable node dimension, k is the hidden node dimension (k ≪ d), N is the number of samples, p is the number of observable nodes, and z is the number of non-zero elements in each sample.\nLet Γ denote the maximum size of the groups, over which we operate the local recursive grouping procedure. Thus, Γ affects the degree of parallelism for our method. Recall that it is given by the neighborhoods on MST, i.e., Γ := maxi|nbd[i;MST]|. Below, we provide a bound on Γ.\nLemma 5.2. The maximum size of neighborhoods on MST, denoted as Γ, satisfies\nΓ ≤ ∆1+ ud ld δ , (5.8)\nwhere δ := maxi{minj{path(vi, vj; T )}} is the effective depth, ∆ is the maximum degree of T , and the ud and ld are the upper and lower bound of information distances between neighbors on T .\n116\nThus, we see that for many natural cases, where the degree and the depth in the latent tree are bounded (e.g. the hidden Markov model), and the parameters are mostly homogeneous (i.e., ud/ld is small), the group sizes are bounded, leading to a high degree of parallelism.\nWe summarize the computational complexity in Table 5.1. Details can be found in Appendix D.6."
    }, {
      "heading" : "5.7 Experiments",
      "text" : "Setup Experiments are conducted on a server running the Red Hat Enterprise 6.6 with 64 AMD Opteron processors and 265 GBRAM. The program is written in C++, coupled with the multi-threading capabilities of the OpenMP environment [52] (version 1.8.1). We use the Eigen toolkit1 where BLAS operations are incorporated. For SVDs of large matrices, we use randomized projection methods [66] as described in Appendix D.8.\nHealthcare data analysis The goal of our analysis is to discover a disease hierarchy based on their co-occurring relationships in the patient records. In general, longitudinal patient records store the diagnosed diseases on patients over time, where the diseases are encoded with International Classification of Diseases (ICD) code.\n1http://eigen.tuxfamily.org/index.php?title=Main_Page\n117\nData description We used two large patient datasets of different sizes with respect to the number of samples, variables and dimensionality.\n(1) MIMIC2: The MIMIC2 dataset record disease history of 29,862 patients where a overall of 314,647 diagnostic events over time representing 5675 diseases are logged. We consider patients as samples and groups of diseases as variables. We analyze and compare the results by varying the group size (therefore varying d and p).\n(2) CMS: The CMS dataset includes 1.6 million patients, for whom 15.8 million medical encounter events are logged. Across all events, 11,434 distinct diseases (represented by ICD codes) are logged. We consider patients as samples and groups of diseases as variables. We consider specific diseases within each group as dimensions. We analyze and compare the results by varying the group size (therefore varying d and p). While the MIMIC2 dataset and CMS dataset both contain logged diagnostic events, the larger volume of data in CMS provides an opportunity for testing the algorithm’s scalability. We qualitatively evaluate biological implications on MIMIC2 and quantitatively evaluate algorithm performance and scalability on CMS.\nTo learn the disease hierarchy from data, we also leverage some existing domain knowledge about diseases. In particular, we use an existing mapping between ICD codes and higherlevel Phenome-wide Association Study (PheWAS) codes [54]. We use (about 200) PheWAS codes as observed nodes and the observed node dimension is set to be binary (d = 2) or the maximum number of ICD codes within a pheWAS code (d = 31). The goal is to learn the latent nodes and the disease hierarchy and associated parameters from data."
    }, {
      "heading" : "5.7.1 Validation",
      "text" : "We conduct both quantitative and qualitative validation of the resulting disease hierarchy.\n118\nQuantitative Analysis We first compare our resulting hierarchy with a ground truth tree based on medical knowledge2. The standard Robinson Foulds (RF) metric [140](between our estimated latent tree and the ground truth tree) is computed to evaluate the structure recovery in Table 5.2. The smaller the metric is, the better the recovered tree is. We also compare our results with a baseline: the agglomerative clustering. The proposed method are slightly better than the baseline and the advantage is increased with more nodes. However, the proposed method provides an efficient probabilistic graphical model that can support general inference which is beyond the baseline.\nQualitative analysis The qualitative analysis is done by a senior MD-PhD student in our team.\n2The ground truth tree is the PheWAS hierarchy provided in the clinical study [54]\n119\n(a) Case d=2: Here we report the results from the 2-dimensional case (i.e., observed variable is binary). In figure 5.4, we show a portion of the learned tree using the MIMIC2 healthcare\ndata. The yellow nodes are latent nodes from the learned subtrees while the blue nodes represent observed nodes(diagnosis codes) in the original dataset. Diagnoses that are similar were generally grouped together. For example, many neoplastic diseases were grouped under the same latent node (node 1135). While some dissimilar diseases were grouped together, there usually exists a known or plausible association of the diseases in the clinical setting. For example, in figure 5.4, clotting-related diseases and altered mental status were grouped under the same latent node as several neoplasms. This may reflect the fact that altered mental status and clotting conditions such as thrombophlebitis can occur as complications of neoplastic diseases [61]. The association of malignant neoplasms of prostate and colon polyps, two common cancers in males, is captured under latent node 1136 [74].\n120\n(b) Case d =31: We also learn a tree from the MIMIC2 dataset, in which we grouped diseases into 163 pheWAS codes and up to 31 dimensions per variable. Figure 5.5 shows a portion of the learned tree of four subtrees which all reflect similar diseases relating to trauma. A majority of the learned subtrees reflected clinically meaningful concepts, in that related and commonly co-occurring diseases tended to group together in the same subtrees or in nearby subtrees. We also learn the disease tree from the larger CMS dataset, in which we group diseases into 168 variables and up to 31 dimensions per variable. Similar to the case from the MIMIC2 dataset, a majority of learned subtrees reflected clinically meaningful concepts.\nFor both the MIMIC2 and CMS datasets, we performed a qualitative comparison of the resulting trees while varying the hidden dimension k for the algorithm. The resulting trees for different values of k did not exhibit significant differences. This implies that our algorithm is robust with different choices of hidden dimensions. The estimated model parameters are also robust for different values of k based on the results.\n121\nScalability Our algorithm is scalable w.r.t. varying characteristics of the input data. First, it can handle a large number of patients efficiently, as shown in Figure 5.3(a). It has also a linear scaling behavior as we vary the number observed nodes, as shown in Figure 5.3(b). Furthermore, even in cases where the number of observed variables is large, our method maintains an almost linear scale-up as we vary the computational power available, as shown in Figure 5.3(c). As such, by providing the respective resources, our algorithm is practical under any variation of the input data characteristics."
    }, {
      "heading" : "5.8 Conclusion",
      "text" : "We present an integrated approach to structure and parameter estimation in latent tree models. Our method overcomes challenges such as uncertainty of location and number of hidden variables, problem of local optima with no consistency guarantees, difficulty in scalability with respect to number of variables. The proposed algorithm is ideal for parallel computing and highly scalable. We successfully applied the algorithm to a real application for disease hierarchy discovery using large patient data for 1.6m patients.\n122\nChapter 6"
    }, {
      "heading" : "Discovering Cell Types with Spatial",
      "text" : ""
    }, {
      "heading" : "Point Process Mixture Model",
      "text" : "Cataloging the neuronal cell types that comprise circuitry of individual brain regions is a major goal of modern neuroscience and the BRAIN initiative. Single-cell RNA sequencing can now be used to measure the gene expression profiles of individual neurons and to categorize neurons based on their gene expression profiles. While the single-cell techniques are extremely powerful and hold great promise, they are currently still labor intensive, have a high cost per cell, and, most importantly, do not provide information on spatial distribution of cell types in specific regions of the brain. We propose a complementary approach that uses computational methods to infer the cell types and their gene expression profiles through analysis of brain-wide single-cell resolution in situ hybridization (ISH) imagery contained in the Allen Brain Atlas (ABA). We measure the spatial distribution of neurons labeled in the ISH image for each gene and model it as a spatial point process mixture, whose mixture weights are given by the cell types which express that gene. By fitting a point process mixture model jointly to the ISH images, we infer both the spatial point process distribution for each cell type and their gene expression profile. We validate our predictions of cell type-\n123\nspecific gene expression profiles using single cell RNA sequencing data, recently published for the mouse somatosensory cortex. Jointly with the gene expression profiles, cell features such as cell size, orientation, intensity and local density level are inferred per cell type. This work brings together the techniques used in all previous chapters, such as image processing to extract cells and cell features from brain slices, learning a point process admixture model."
    }, {
      "heading" : "6.1 Introduction",
      "text" : ""
    }, {
      "heading" : "6.1.1 Motivations and Goals",
      "text" : "The human brain comprises about one hundred billion neurons and one trillion supporting glial cells. These cells are specialized into a surprising diversity of cell types. The retina alone boasts well over 50 cell types, and it is an active area of research to perform a census of the various neuronal cell types that comprise the central nervous system. Many criteria have been used to categorize neuronal cell types, from neuronal morphology and connectivity to their functional response properties. Neurons can also be categorized based on the proteins they make. Immunohistochemistry has been used with great success for many decades to differentiate excitatory neurons from inhibitory neurons by labeling for known proteins involved in the synthesis and regulation of glutamate and GABA, the primary excitatory and inhibitory neurotransmitters respectively.\nMore recently, there has been an effort to systematically measure the complete transcriptome of single neurons. Single-cell RNA sequencing (RNA-Seq) is an extremely powerful technique that can quantitatively determine the expression level of every gene that is expressed in individual neurons. This so-called transcriptome or gene expression / transcription profile can then be used to define cell types by clustering. A recent study produced the most comprehensive census of cell types to date in the mouse somatosensory cortex and hippocampus\n124\nby performing single-cell RNA-Seq on over 3000 neurons [168]. While this study is quite exciting, tyring to replicate it for all brain regions might well require the equivalent of a thousand such experiments. Thus, it is likely that the unprecedented insights that RNA-Seq can provide will be slow to arrive. More importantly, single cell sequencing methods are not currently able to capture the precise three-dimensional location of the individual neurons.\nHere we propose a complementary approach that uses computational strategies to identify cell types and their spatial distribution by re-analysing data published by the Allen Institute for Brain Research. The Allen Brain Atlas (ABA) contains cellular resolution brain-wide insitu hybridization (ISH) images for 20,000 genes1. ISH is a histological technique that labels the mRNA in all cells expressing the corresponding gene in a manner roughly proportion to the gene expression level. An example of an ISH image can be seen in figure 6.1(a).\nThe ABA contains genome-wide and brain-wide ISH images of the adult mouse brain. These images were generated by slicing the brain into a series of 25 µm thin sections and performing ISH. Image series of ISH performed for different genes come from different mouse brains, since ISH can only be performed for one gene at a time. The ISH image series for different genes were then computational aligned into a common reference brain coordinate system. Such data have been productively used to infer the average transcriptomes corresponding to different brain regions.\nIt is commonly thought that the ABA cannot be used to infer the transcriptomes of individual cells in a given brain region since mouse brains cannot be aligned to the precision of a single cell. This is because there is individual variation in the precise number and location of neurons from brain to brain. However, we expect that the average number and spatial distribution of neurons from each cell type to be conserved from brain to brain, for a given brain area. More concretely, we might expect that parvalbumin-expressing (PV) inhibitory\n1 Although the Atlas contains ISH data for approximately 20,000 distinct mouse genes, we focus on the top 1743 reliable genes whose sagittal and coronal experiments are highly correlated.\n125\ninterneurons in layer 2/3 of the mouse somatosensory cortex comprise approximately 7% of all neurons and have a conserved spatial and size distribution from brain to brain. We use this fact to derive a method for simultaneously inferring the cell types in a given brain region and their gene expression profiles from the ABA.\nWe propose to model the spatial distribution of neurons in a brain as being generated by sampling from an unknown but consistent brain-region and cell-type dependent spatial point process distribution. And since each gene might only be expressed in a subset of cell types, an ISH image for a single gene can be thought of as a mixture of spatial point processes where the mixture weights represent the individual cell types expressing that gene. We infer cell types, their gene expression profiles and their spatial distribution by unmixing the spatial point processes corresponding to the ISH images for 1743 genes. This is in notable contrast to the information provided by single-cell RNA sequencing which can only measure the gene expression profile of individual cells to high accuracy but where, due to the destructive measurement process, all information about the spatial position and distribution of cell types is lost."
    }, {
      "heading" : "6.1.2 Previous Work",
      "text" : "Allen Brain Atlas (ABA) [115] is a landmark study which mapped the gene expression of about 20,000 genes across the entire mouse brain. The ABA dataset consists of cellular high-resolution 2d imagery of in-situ hybridized series of brain sections, digitally aligned to a common reference atlas. However, since the in-situ images for each gene come from different mouse brains and since there is significant variability in the individual locations of labeled cells, it is not possible to register brain-wide gene expression at a resolution higher than about 250µm. Therefore, the cellular resolution detail was down-sampled to construct\n126\na coarser 3d representation of the average gene expression level in 250µm× 250µm× 250µm voxels.\nThe coarse-resolution averaged gene expression representation has been widely used and analyzed to understand differences in gene expression at the level of brain region. Hawrylycz et al [79] analyzed the correlational structure of gene expression at this scale, across the entire mouse brain. However, due to the poor resolution of the average gene expression representation, it has proven challenging to use the ABA to discover the microstructure of gene expression within a brain region. To address this issue from a complementary perspective, Grange et al [72] used the gene expression profiles of 64 known cell-types, combined with linear unmixing to determine the spatial distribution of these known cell-types. However, such an approach can be confounded by the presence of cell-types whose expression profiles have yet to be characterized, and limited by the resolution of the averaged gene expression representation.\nIn contrast to previous approaches, we aim to solve the difficult problem of automatically discovering the gene expression profiles of cell-types within a brain region by analyzing the original cellular resolution ISH imagery. We propose to use the spatial distributions of labeled cells, and their shapes and sizes, which are a far richer representation than simply the average expression level in 250µm× 250µm× 250µm voxels. This spatial point process is then un-mixed to determine the gene expression profile of cell types.\nMost previous work on unmixing point process mixtures adopted parametric generative models where the point process is limited to some distribution family such as Poisson or Gaussian [95, 107]. However, since we are not interested in building a generative model of a point process, but rather care more about inferring the mixing proportions (gene expression profile), we take a simpler parameter-free approach. This approach models only the statistics of the point process, but is not a generative model, and so cannot be use to model individual points/cells.\n127\n128"
    }, {
      "heading" : "6.2 Modeling the Spatial Distribution of Cell-types Us-",
      "text" : "ing Spatial Point Process Features\nMost analyses of the ABA in situ hybridization dataset have utilized a simple measure of average expression level in relatively large 250µm× 250µm× 250µm voxels of brain tissue. Due to the large volume over which the expression level is averaged, such a representation cannot distinguish between large numbers of cells expressing small amounts of RNA vs. small numbers of cells expressing large amounts of RNA. All information about the spatial organization of labeled cells, their shapes, sizes and spatial density are lost and summarized by a single scalar number. Here, we describe a more sophisticated representation of the labeled cells in an ISH image based on marked spatial point processes."
    }, {
      "heading" : "6.2.1 The Marked Spatial Point Process Representation of ISH",
      "text" : "Images\nOur approach requires processing the high-resolution ISH images to detect individual labeled cells and their visual characteristics. We developed a cell detection algorithm described in the Supplementary section. Our algorithm additionally also estimates the expression level of each detected cell, its shape, size and orientation. Figure 6.1(a) and Figure 6.1(b) illustrate the results of our cell detection algorithm.\nSince cell-types differ not only in terms of gene expression pattern, but also display a diversity of shapes, sizes and spatial densities, we sought to characterize these properties. We measured: (1) cell size s = [r1, r2]: the radius in two principal directions of an ellipse fit to each cell; (2) cell orientation o: the orientation of the first principle axis of the ellipse; (3) gene intensity level p: intensity of labeling of a cell relative to the image background; (4)\n129\nspatial distribution c: the number of cells within a local area centered around the cell, which can be regarded as a measure of the local cell density.\nThe collection of detected cells within an atlas-defined brain region, along with their features, constitutes a marked spatial point process. This point process is considered “marked”, because each point is characterized by the shape, size, expression level and local density features, in addition to just their location in space."
    }, {
      "heading" : "6.2.2 A Model-free Approach to Representing Spatial Point Pro-",
      "text" : "cesses Using Joint Feature Histograms\nThe statistical modeling of repulsive spatial point processes such as those that arise in biology is non-trivial, and many generative models such as determinantal point processes [110]and Matern point processes have high computational complexity. But since we are not interested in directly modeling the individual labeled cells, but instead in modeling only their aggregate spatial statistics, and in inferring their gene expression profiles, we can take a simpler approach.\nWe use a joint histogram simple statistics of the collection of detected cells to characterize the underlying point process from which they are drawn. This is an empirical moment approach which side-steps the need to carefully define a generative point process distribution.\nAs we describe in the next section, we propose to model the point process measured from the ISH image for each gene as a mixture of point processes belonging to individual cell-types. For this, we use a linear mixing model, the Latent Dirichlet Allocation model. The use of this model is greatly simplified if we carefully choose our feature representation such that the linear mixture of point processes results in a linear mixture of histogram statistics. This is clearly the case for the features we have chosen. For instance, if we sample equally from\n130\ntwo point process distributions P1 and P2 with average densities of d1 and d2, the addition of these two point processes P = P1+P2 results in the addition of the two densities d = d1+d2. This is not the case for second order features, such as the distances to the nearest neighbors, which would have a more nonlinear relationship.\nIn figure 6.1(c), we display marginal histograms corresponding to the joint histogram for two genes, Pvalb and Rasgrf2, which are well-known markers for a specific class of inhibitory and excitatory cortical neuronal cell-types respectively."
    }, {
      "heading" : "6.3 Un-mixing Spatial Point Processes to Discover Cell-",
      "text" : "types"
    }, {
      "heading" : "6.3.1 Generative Model: A Variation of Latent Dirichlet Alloca-",
      "text" : "tion\nThe spatial point process histogram representation of the ABA ISH dataset results, for each brain region, is an NF × NG matrix [xmn ], where NF is the total number of histogram bins (henceforward called the number of histogram features) 2, NG is the number of genes, and xmn is the number of cells expressing gene n in histogram bin m.\nWe model the gene-spatial histogram matrix [xmn ] by assuming it is generated by a Variation of Latent Dirichlet Allocation (vLDA) [32] model of cell types. This matrix factorization based latent variable model assumes that the ISH histograms are generated from a small number of cell-types, K, and each cell-type i is associated with a type-dependent spatial point process histogram hi and a gene expression profile βi.\n2Note that there are two types of features – the features characterizing each detected cell, and the features characterizing the collection of detected cells that constitute a single sample from a spatial point process\n131\nOur generative model for each histogram bin m (characterizing a particular bin in the size/ orientation/ gene profile/ spatial distribution) is as follows: Let Lm = ∑NG\nn x m n be the\ndetected number of cells in the joint histogram bin m. For each cell l in this bin, its cell-type t is sampled from the multinomial distribution hm. And given the cell-type t of cell l, the genes n expressed by this cell are sampled from a multinomial distribution given by the type-dependent gene expression profile/distribution βt. For a given gene n and histogram bin m, this generative process determines the number of cells that would be detected xmn .\nWe further place a Dirichlet prior over hm ∼ Dir(α), with the concentration parameter α which determines the prior probability over the number of cell-types present in a given histogram bin m. This prior represents our prior knowledge of how many cell-types express each gene, and also how well our feature representation separates cells of different types into different histogram bins. In principle, we could generalize this to be a gene-specific prior, if we had such information available. We could also use α to incorporate information about our prior knowledge over the distribution of cells from each cell-type, for instance that excitatory neurons greatly outnumber inhibitory neurons in a roughly 5 : 1 ratio.\nWe now describe how we estimate the model parameters – the cell-type specific multinomial gene expression profile β and the cell-type specific spatial point process histogram h from the gene-specific spatial point process histograms measured from the ISH images."
    }, {
      "heading" : "6.3.2 Estimating the Cell-type Dependent Gene Expression Pro-",
      "text" : "file β\nAfter testing several estimation methods for the parameters of our model, we found that non-negative matrix factorization (NMF) performed well in estimating the cell-type specific\n132\ngene expression profiles β, see Figure 6.2a. We solve the following optimization problem:\nmin β,h\nNF∑\nm\nNG∑\nn\n(xmn − K∑\nt\nhmt β t nL m)2, s.t. βtn ≥ 0, NG∑\nn\nβtn = 1, h m t ≥ 0,\nK∑\nt\nhmt = 1 (6.1)\nHere, the non-negativity and sum-to-one constraints on hmt and β t n ensure that h and β result in properly normalized multinomial distributions. While this estimation procedure results in joint estimates for h and β, it does not enforce the Dirichlet prior over h. So we refine our NMF-derived estimates for h using variational inference [32]."
    }, {
      "heading" : "6.3.3 Estimating the Cell-type Dependent Spatial Point Process",
      "text" : "Histogram h\nWe use a standard maximum likelihood estimation procedure for h [32]. Iteratively, we refine the inference of the cell type membership hm ∈ ∆k under each joint histogram feature m. We update hmi until convergence [148].\nhmi ← 1\nLm + ∑K\nt αt\nNG∑\nn=1\nxmn hmi β i n\nK∑ l=1 hml β l n\n+ αi, ∀i ∈ [K], m ∈ [NF ] (6.2)\nRecall that the Dirichlet prior α encodes the number of cell-types that we expect on average to express each gene. We set α to be a symmetric Dirichlet with α1 = α2 = . . . = αK , and ∑ t αt = 0.01 for all cell-types t. In practice, we observe that our estimates of h are fairly insensitive to the specific choice for α as long as ∑\nt αt is small enough. The smaller α is,\nthe fewer cell-types expressing a given gene we expect to observe in a single histogram bin.\n133"
    }, {
      "heading" : "6.4 Results and Evaluation",
      "text" : ""
    }, {
      "heading" : "6.4.1 Implementation Details",
      "text" : "We tested our proposed cell-type discovery algorithm using the high-resolution in situ hybridization image series for 1743 of the most reliably imaged and annotated genes in the ABA. Individual cells were detected in the cellular resolution ISH images using custom algorithms (detailed in Supplementary Information). For each detected cell, we fit ellipses and extract several local features: (a) size and shape represented as the diameters along the principle axes of the ellipse, (b) orientation of the first principle axis, (c) gene intensity level as measured by the intensity of labeling of the cell body, and (d) the number of cells detected with-in a 100 µm radius around the cell, which is a measure of the local cell density. We aligned the ISH images to the ABA reference atlas and, for this paper, focused our attention on cells in the somatosensory cortex, since independent RNA-Seq data exist for this region the can be used to evaluate our approach. We computed joint histograms for the collection of cells found with-in the somatosensory cortex, resulting in a spatial point process feature vector of NF = 10010 histogram bins per gene.\nSynthetic experiment: The vLDA model we proposed is then fit to NG × NF gene point process histogram matrix to estimate the cell-type gene expression profile matrix β using the non-negative matrix factorization (NNMF) algorithm. The reason why we choose NNMF over Variational Inference (which is a popular approach for LDA) for β estimation is that NNMF produces more accurate β estimation in simulated data, illustrated in Fig 6.2a. In the synthetic experiment, we simulate point process data ( with some predefined golden standard β) and use the data to estimate β̂. The errors were computed after pairing the estimated columns of β with a closest golden standard β column via hypothesis testing. Note that the columns of β are normalized to 1, so the errors are bounded.\n134"
    }, {
      "heading" : "6.4.2 Evaluating Cell-type Gene Expression Profile Predictions",
      "text" : "A recent study performed single-cell RNA sequencing on 1691 neurons isolated from mouse somatosensory cortex. We use this dataset to evaluate the quality of the cell-types we discover.\nThe single cell RNA-seq data, G := [g1|g2| . . . |gNC ] ∈ RNG×NC , contains the gene expression profiles for NC = 1691 cells. We infer the cell types h i for these cells using equation (6.2), and then compute the likelihood Li of observing each for each cell under our estimated cell-type dependent gene expression profile matrix β using equation (6.4). We can then evaluate the\n135\nperplexity, a commonly used measure of goodness of fit under the vLDA model, of single cell RNA-seq data on the model we learned from our spatial point process data.\nThe perplexity score is a standard metric, which is defined as the geometric mean per-cell likelihood. It is a monotonically decreasing function of the log-likelihood L(G) of test data G.\nperplexity(G) = exp(− ∑NC\ni=1 log p(g i)∑NC\ni=1 L i\n) (6.3)\nwhere the likelihood is evaluated as\np(gm|hm, α, β) = Γ ( ∑\ni αi)∏ i Γ (αi)\nk∏\ni=1\n(hmi ) αi−1\nLm∏\nj=1\n( k∑\ni=1\nNG∑\nn=1\nδgmj ,enh m i β i n\n) . (6.4)\nwhere δi,j is the Kronecker delta, δi,j = 1 when i = j and 0 otherwise. e n is the nth basis vector."
    }, {
      "heading" : "6.4.3 Comparison to Standard Average Gene Expression Features",
      "text" : "Baseline and a Permutation Test for Significance\nHere we demonstrate the superiority of our method and its statistical significance in two ways. First we compared the perplexity of the single-cell RNA seq dataset G under our model (figure 6.2b, solid blue) against the perplexity of a surrogate dataset with the same marginal statistics, but whose gene-cell correlations were destroyed (figure 6.2b, dashed blue). We generated this surrogate dataset by randomly permuting the gene expression levels for each gene across cells. This permuted dataset had a significantly higher (worse) perplexity than the true single-cell dataset. This demonstrates that our model trained to un-mix the ISH-derived spatial point processes discovered cell-types whose gene expression profiles are significantly better match to single-cells than by chance.\n136\nWe also compared the predictions of cell-type gene expression profiles derived by un-mixing our spatial point process features against gene expression profiles derived by un-mixing the more standard 250µm × 250µm × 250µm averaged gene expression level features. We see a very large improvement in perplexity by switching from the standard simple averaging of gene expression, to extracting spatial point process features (figure 6.2b). The singlecell RNA seq dataset analysis from figure 6.2b shows that the perplexity of our recovered cell-types rapidly flattens after we recover approximately 10 clusters (K = 10).\n137"
    }, {
      "heading" : "6.4.4 A Brief Analysis of Recovered Cell Types in Somatosensory",
      "text" : "Cortex\nIn this section we describe the representative spatial point process statistics and gene expressions for 8 cell-types we recovered. We attempted to align our 8 clusters to cell-types defined by [168] in the single-cell RNA sequencing paper. We found high overlap in the gene expression profiles for all 8 clusters with known cell-types defined in [168], Interneurons, S1\n138\nPyramidal, Mural, Endothelial, Microglia, Ependymal, Astrocytes and Oligodendrocytes, in Figure 6.3.\nThe estimate of β was combined with MLE to infer the cell-type specific spatial point process representation hml . In examining the spatial point process distributions that we predict for each of these cell types, we discover that while the distribution of cell body orientations is quite broad and similar across cell types, the cell count distribution, which is a measure of cell density, varies in a systematic way from one cell type to another. Fig 6.4d shows that inhibitory Interneurons are less dense than S1Pyramidal neurons. This is consistent with their known prevalence, roughly 20% of all neurons are GABAergic interneurons [118], while the remaining 80% are excitatory glutamatergic pyramidal neurons. As expected, this excitatory neuronal category of S1Pyramidal is the most common and hence most dense class of neuronal cells. They also have slightly larger cell bodies, compared to interneurons, as can be seen in Fig 6.4a. The remaining 6 cell types correspond to various glial sub-types."
    }, {
      "heading" : "6.5 Conclusion",
      "text" : "We developed a computational method for discovering cell types in a brain region by analyzing the high-resolution in situ hybridization image series from the Allen Brain Atlas. Under the assumption that cell types have unique spatial distributions and gene expression profiles, we used a varied latent Dirichlet allocation (vLDA) based on spatial point process process mixture model to simultaneously infer the cell feature spatial distribution and gene expression profiles of cell types. By comparing our gene expression profile predictions to a single-cell RNA sequencing dataset, we demonstrated that our model improves significantly on state of the art.\n139\nThe accuracy of our method relies heavily on the assumption that cell-types differ in their spatial distribution, and that our point process features perform a good job of distinguishing these differences. Thus the performance of our method can be improved by better estimates of better features. We would expect our method to perform better for large brain areas, which can be more accurately aligned, and which have more cells to estimate point process features.\nThere are several modifications to our vLDA model which might improve the faithfulness of our generative model to the biology. We place a symmetric Dirichlet prior over cell-type multinomial distribution hm for a given histogram bin m. This assumes that the number of cell-types expressing each gene is the same for all genes. But since some genes are expressed more commonly and non-specifically than others, we might expect a gene-specific prior to be a better model. Further, the symmetric Dirichlet assumes that all cell-types have equal proportions of cells. But evidence suggests that excitatory neurons are more common than inhibitory neurons in cortex [76], and using a non-uniform Dirichlet prior could account for this.\n140\nChapter 7"
    }, {
      "heading" : "Conclusion and Outlook",
      "text" : ""
    }, {
      "heading" : "7.1 Conclusion",
      "text" : "Now that we are at the end of the dissertation, we are convinced that spectral methods including tensor decomposition are good candidates for unsupervised learning. They reveal hidden structure using transformations and extract useful and clean information to characterize the complicated data. Spectral methods are proved to be potential in various application. For instance, text and image processing, social networks, healthcare analytics and neuroscience.\nSpectral methods especially matrix/tensor decomposition framework is versatile. They are straightforward to apply to flat models, such as exchangeable model, multi-view model, and hidden Markov model, but they are also amendable to learn models with a hierarchy such as a mixture of trees and latent tree model. Spectral methods not only perform well on traditional multiplicative sparse coding models but also outperforms the state-of-the-art on models with group invariance. The tensor decomposition framework is efficient and is guaranteed to converge to global optima.\n141"
    }, {
      "heading" : "7.2 Outlook",
      "text" : "Now the question is what is beyond? Could we further push the boundaries of spectral methods? Can we have a tensor library with optimal hardware support for tensor operations? In the region of high dimensional hidden space, could we develop approximated algorithms that are computational more efficient? Could we have tensor sketching where the decomposition happens in a sketching vector space, and the tensor is never explicitly formed? Furthermore, could we use tensor decomposition to train models with other invariances (such as rotation invariance and scaling invariance) or general invariance constraints?\nIn the real world, we could push our framework further for more challenging tasks. In neuroscience, we would like to understand the brain; that is to systematically model and learn brain neural system and sort out its relationship to body functions. We know that deep neural network system inspired by the architecture of neural circuits have been hugely successful empirically. Could we utilize the neural network techniques to foster understanding of the brain neural circuits? Or could we use our knowledge of the brain neural circuits to understand fundamental reasons for a certain structure of a deep neural network system in machine learning? Even in healthcare analytics, simple usage of the co-occurrence of diseases is not as informative as considering other factors such as symptoms. With more information, the model gets more complicated, but we hope to achieve personalized identification of diseases or curing plans.\nOverall, there are numerous exciting open problems ahead. Graduation is not an end; rather it is a fresh start. I am looking forward to the uncertainty of the future career. Keep curious and continue exploring. May the world be more intelligent!\n142"
    }, {
      "heading" : "Appendix A",
      "text" : ""
    }, {
      "heading" : "Appendix for Online Stochastic",
      "text" : ""
    }, {
      "heading" : "Gradient for Tensor Decomposition",
      "text" : ""
    }, {
      "heading" : "A.1 Detailed Analysis for Section 2.2 in Unconstrained",
      "text" : "Case\nIn this section we give detailed analysis for noisy gradient descent, under the assumption that the unconstrained problem satisfies (α, γ, ǫ, δ)-strict saddle property.\nThe algorithm we investigate in Algorithm 1, we can combine the randomness in the stochastic gradient oracle and the artificial noise, and rewrite the update equation in form:\nwt = wt−1 − η(∇f(wt−1) + ξt−1) (A.1)\nwhere η is step size, ξ = SG(wt−1) − ∇f(wt−1) + n (recall n is a random vector on unit sphere) is the combination of two source of noise.\n156\nBy assumption, we know ξ’s are independent and they satisfying Eξ = 0, ‖ξ‖ ≤ Q+ 1. Due to the explicitly added noise in Algorithm 1, we further have EξξT ≻ 1 d I. For simplicity, we assume EξξT = σ2I, for some constant σ = Θ̃(1), then the algorithm we are running is exactly the same as Stochastic Gradient Descent (SGD). Our proof can be very easily extended to the case when 1 d I E[ξξT ] (Q + 1 d )I because both the upper and lower bounds are Θ̃(1).\nWe first restate the main theorem in the context of stochastic gradient descent.\nTheorem A.1 (Main Theorem). Suppose a function f(w) : Rd → R that is (α, γ, ǫ, δ)strict saddle, and has a stochastic gradient oracle where the noise satisfy EξξT = σ2I. Further, suppose the function is bounded by |f(w)| ≤ B, is β-smooth and has ρ-Lipschitz Hessian. Then there exists a threshold ηmax = Θ̃(1), so that for any ζ > 0, and for any η ≤ ηmax/max{1, log(1/ζ)}, with probability at least 1− ζ in t = Õ(η−2 log(1/ζ)) iterations, SGD outputs a point wt that is Õ( √ η log(1/ηζ))-close to some local minimum w⋆.\nRecall that Õ(·) (Ω̃, Θ̃) hides the factor that has polynomial dependence on all other parameters, but is independent of η and ζ . So it focuses on the dependency on η and ζ . Throughout the proof, we interchangeably use both H(w) and ∇2f(w) to represent the Hessian matrix of f(w).\nAs we discussed in the proof sketch in Section 2.2, we analyze the behavior of the algorithm in three different cases. The first case is when the gradient is large.\nLemma A.1. Under the assumptions of Theorem A.1, for any point with ‖∇f(w0)‖ ≥ √\n2ησ2βd where √ 2ησ2βd < ǫ, after one iteration we have:\nEf(w1)− f(w0) ≤ −Ω̃(η2) (A.2)\n157\nProof. Our assumption can guarantee ηmax < 1 β , then by update equation Eq.(A.1), we have:\nEf(w1)− f(w0) ≤ ∇f(w0)TE(w1 − w0) + β\n2 E‖w1 − w0‖2\n= ∇f(w0)TE (−η(∇f(w0) + ξ0)) + β\n2 E ‖−η(∇f(w0) + ξ0)‖2\n= −(η − βη 2\n2 )‖∇f(w0)‖2 +\nη2σ2βd\n2\n≤ −η 2 ‖∇f(w0)‖2 +\nη2σ2βd\n2 ≤ −η\n2σ2βd\n2 (A.3)\nwhich finishes the proof.\nLemma A.2. Under the assumptions of Theorem A.1, for any initial point w0 that is Õ( √ η) < δ close to a local minimum w⋆, with probability at least 1 − ζ/2, we have following holds simultaneously:\n∀t ≤ Õ( 1 η2 log 1 ζ ), ‖wt − w⋆‖ ≤ Õ(\n√ η log 1\nηζ ) < δ (A.4)\nwhere w⋆ is the locally optimal point.\nProof. We shall construct a supermartingale and use Azuma’s inequality [21] to prove this result.\nLet filtration Ft = σ{ξ0, · · · ξt−1}, and note σ{∆0, · · · ,∆t} ⊂ Ft, where σ{·} denotes the sigma field. Let event Et = {∀τ ≤ t, ‖wτ − w⋆‖ ≤ µ √ η log 1 ηζ < δ}, where µ is independent of (η, ζ), and will be specified later. To ensure the correctness of proof, Õ notation in this proof will never hide any dependence on µ. Clearly there’s always a small enough choice of ηmax = Θ̃(1) to make µ √ η log 1 ηζ < δ holds as long as η ≤ ηmax/max{1, log(1/ζ)}. Also note Et ⊂ Et−1, that is 1Et ≤ 1Et−1.\n158\nBy Definition 2.3 of (α, γ, ǫ, δ)-strict saddle, we know f is locally α-strongly convex in the 2δ-neighborhood of w⋆. Since ∇f(w⋆) = 0, we have\n∇f(wt)T (wt − w⋆)1Et ≥ α‖wt − w⋆‖21Et (A.5)\nFurthermore, with ηmax < α β2 , using β-smoothness, we have:\nE[‖wt − w⋆‖21Et−1 |Ft−1] =E[‖wt−1 − η(∇f(wt−1) + ξt−1)− w⋆‖2|Ft−1]1Et−1\n= [ ‖wt−1 − w⋆‖2 − 2η∇f(wt−1)T (wt−1 − w⋆)\n+η2‖∇f(wt−1)‖2 + η2dσ2 ] 1Et−1\n≤[(1− 2ηα + η2β2)‖wt−1 − w⋆‖2 + η2dσ2]1Et−1 ≤[(1− ηα)‖wt−1 − w⋆‖2 + η2dσ2]1Et−1 (A.6)\nTherefore, we have:\n[ E[‖wt − w⋆‖2|Ft−1]− ηdσ2\nα\n] 1Et−1 ≤ (1− ηα) [ ‖wt−1 − w⋆‖2 − ηdσ2\nα\n] 1Et−1 (A.7)\nThen, let Gt = max{(1− ηα)−t(‖wt − w⋆‖2 − ηdσ 2 α ), 0}, we have:\nE[Gt1Et−1|Ft−1] ≤ Gt−11Et−1 ≤ Gt−11Et−2 (A.8)\nwhich means Gt1Et−1 is a supermartingale.\n159\nTherefore, with probability 1, we have:\n|Gt1Et−1 − E[Gt1Et−1|Ft−1]|\n≤(1− ηα)−t[ ‖wt−1 − η∇f(wt−1)− w⋆‖ · η‖ξt−1‖+ η2‖ξt−1‖2 + η2dσ2 ]1Et−1 ≤(1− ηα)−t · Õ(µη1.5 log 12 1 ηζ ) = dt (A.9)\nLet\nct =\n√√√√ t∑\nτ=1\nd2τ = Õ(µη 1.5 log\n1 2 1\nηζ )\n√√√√ t∑\nτ=1\n(1− ηα)−2τ (A.10)\nBy Azuma’s inequality, with probability less than Õ(η3ζ), we have:\nGt1Et−1 > Õ(1)ct log 1 2 (\n1\nηζ ) +G0 (A.11)\nWe know Gt > Õ(1)ct log 1 2 ( 1\nηζ ) +G0 is equivalent to:\n‖wt − w⋆‖2 > Õ(η) + Õ(1)(1− ηα)tct log 1 2 (\n1\nηζ ) (A.12)\nWe know:\n(1− ηα)tct log 1 2 (\n1 ηζ ) = µ · Õ(η1.5 log 1 ηζ )\n√√√√ t∑\nτ=1\n(1− ηα)2(t−τ)\n=µ · Õ(η1.5 log 1 ηζ )\n√√√√ t−1∑\nτ=0\n(1− ηα)2τ ≤ µ · Õ(η1.5 log 1 ηζ )\n√ 1\n1− (1− ηα)2\n= µ · Õ(η log 1 ηζ ) (A.13)\n160\nThis means Azuma’s inequality implies, there exist some C̃ = Õ(1) so that:\nP ( Et−1 ∩ { ‖wt − w⋆‖2 > µ · C̃η log 1\nηζ )\n}) ≤ Õ(η3ζ) (A.14)\nBy choosing µ > C̃, this is equivalent to:\nP ( Et−1 ∩ { ‖wt − w⋆‖2 > µ2η log 1\nηζ\n}) ≤ Õ(η3ζ) (A.15)\nThen we have:\nP (Et) = P ( Et−1 ∩ { ‖wt − w⋆‖ > µ √ η log 1\nηζ\n}) + P (Et−1) ≤ Õ(η3ζ) + P (Et−1)\n(A.16)\nBy initialization conditions, we know P (E0) = 0, and thus P (Et) ≤ tÕ(η3ζ). Take t = Õ( 1 η2 log 1 ζ ), we have P (Et) ≤ Õ(ηζ log 1ζ ). When ηmax = Õ(1) is chosen small enough, and η ≤ ηmax/ log(1/ζ), this finishes the proof.\nLemma A.3. Under the assumptions of Theorem A.1, for any initial point w0 where ‖∇f(w0)‖ ≤ √ 2ησ2βd < ǫ, and λmin(H(w0)) ≤ −γ, then there is a number of steps T that depends on w0 such that:\nEf(wT )− f(w0) ≤ −Ω̃(η) (A.17)\nThe number of steps T has a fixed upper bound Tmax that is independent of w0 where T ≤ Tmax = O((log d)/γη).\nRemark. In general, if we relax the assumption EξξT = σ2I to σ2minI EξξT σ2maxI, the upper bound Tmax of number of steps required in Lemma A.3 would be increased to Tmax = O( 1 γη (log d+ log σmax σmin ))\n161\nAs we described in the proof sketch, the main idea is to consider a coupled update sequence that correspond to the local second-order approximation of f(x) around w0. We characterize this sequence of update in the next lemma.\nLemma A.4. Under the assumptions of Theorem A.1. Let f̃ defined as local second-order approximation of f(x) around w0:\nf̃(w) . = f(w0) +∇f(w0)T (w − w0) +\n1 2 (w − w0)TH(w0)(w − w0) (A.18)\n{w̃t} be the corresponding sequence generated by running SGD on function f̃ , with w̃0 = w0. For simplicity, denote H = H(w0) = ∇2f(w0), then we have analytically:\n∇f̃(w̃t) = (1− ηH)t∇f(w0)− ηH t−1∑\nτ=0\n(1− ηH)t−τ−1ξτ (A.19)\nw̃t − w0 = −η t−1∑\nτ=0\n(1− ηH)τ∇f(w0)− η t−1∑\nτ=0\n(1− ηH)t−τ−1ξτ (A.20)\nFurthermore, for any initial point w0 where ‖∇f(w0)‖ ≤ Õ(η) < ǫ, and λmin(H(w0)) = −γ0. Then, there exist a T ∈ N satisfying:\nd\nηγ0 ≤\nT−1∑\nτ=0\n(1 + ηγ0) 2τ <\n3d\nηγ0 (A.21)\nwith probability at least 1− Õ(η3), we have following holds simultaneously for all t ≤ T :\n‖w̃t − w0‖ ≤ Õ(η 1 2 log\n1 η ); ‖∇f̃(w̃t)‖ ≤ Õ(η 1 2 log 1 η ) (A.22)\nProof. Denote H = H(w0), since f̃ is quadratic, clearly we have:\n∇f̃(w̃t) = ∇f̃(w̃t−1) +H(w̃t − w̃t−1) (A.23)\n162\nSubstitute the update equation of SGD in Eq.(A.23), we have:\n∇f̃(w̃t) = ∇f̃(w̃t−1)− ηH(∇f̃(w̃t−1) + ξt−1)\n= (1− ηH)∇f̃(w̃t−1)− ηHξt−1 = (1− ηH)2∇f̃(w̃t−2)− ηHξt−1 − ηH(1− ηH)ξt−2 = · · · = (1− ηH)t∇f(w0)− ηH t−1∑\nτ=0\n(1− ηH)t−τ−1ξτ (A.24)\nTherefore, we have:\nw̃t − w0 = −η t−1∑\nτ=0\n(∇f̃(w̃τ ) + ξτ)\n= −η t−1∑\nτ=0\n( (1− ηH)τ∇f(w0)− ηH τ−1∑\nτ ′=0\n(1− ηH)τ−τ ′−1ξτ ′ + ξτ )\n= −η t−1∑\nτ=0\n(1− ηH)τ∇f(w0)− η t−1∑\nτ=0\n(1− ηH)t−τ−1ξτ (A.25)\nNext, we prove the existence of T in Eq.(A.21). Since ∑t\nτ=0(1 + ηγ0) 2τ is monotonically\nincreasing w.r.t t, and diverge to infinity as t → ∞. We know there is always some T ∈ N gives d ηγ0 ≤ ∑T−1τ=0 (1 + ηγ0)2τ . Let T be the smallest integer satisfying above equation. By assumption, we know γ ≤ γ0 ≤ L, and\nt+1∑\nτ=0\n(1 + ηγ0) 2τ = 1 + (1 + ηγ0)\n2 t∑\nτ=0\n(1 + ηγ0) 2τ (A.26)\nwe can choose ηmax < min{( √ 2− 1)/L, 2d/γ} so that\nd\nηγ0 ≤\nT−1∑\nτ=0\n(1 + ηγ0) 2τ ≤ 1 + 2d ηγ0 ≤ 3d ηγ0 (A.27)\n163\nFinally, by Eq.(A.21), we know T = O(log d/γ0η), and (1 + ηγ0) T ≤ Õ(1). Also because Eξ = 0 and ‖ξ‖ ≤ Q = Õ(1) with probability 1, then by Hoeffding inequality, we have for each dimension i and time t ≤ T :\nP ( |η t−1∑\nτ=0\n(1− ηH)t−τ−1ξτ,i| > Õ(η 1 2 log\n1 η )\n) ≤ e−Ω̃(log2 1η ) ≤ Õ(η4) (A.28)\nthen by summing over dimension d and taking union bound over all t ≤ T , we directly have:\nP ( ∀t ≤ T, ‖η t−1∑\nτ=0\n(1− ηH)t−τ−1ξτ‖ > Õ(η 1 2 log\n1 η )\n) ≤ Õ(η3). (A.29)\nCombine this fact with Eq.(A.24) and Eq.(A.25), we finish the proof.\nNext we need to prove that the two sequences of updates are always close.\nLemma A.5. Under the assumptions of Theorem A.1. and let {wt} be the corresponding sequence generated by running SGD on function f . Also let f̃ and {w̃t} be defined as in Lemma A.4. Then, for any initial point w0 where ‖∇f(w0)‖ ≤ Õ(η) < ǫ, and λmin(∇2f(w0)) = −γ0. Given the choice of T as in Eq.(A.21), with probability at least 1− Õ(η2), we have following holds simultaneously for all t ≤ T :\n‖wt − w̃t‖ ≤ Õ(η log2 1\nη ); ‖∇f(wt)−∇f̃(w̃t)‖ ≤ Õ(η log2\n1 η ) (A.30)\nProof. First, we have update function of gradient by:\n∇f(wt) =∇f(wt−1) + ∫ 1\n0\nH(wt−1 + t(wt − wt−1))dt · (wt − wt−1)\n=∇f(wt−1) +H(wt−1)(wt − wt−1) + θt−1 (A.31)\n164\nwhere the remainder:\nθt−1 ≡ ∫ 1\n0\n[H(wt−1 + t(wt − wt−1))−H(wt−1)] dt · (wt − wt−1) (A.32)\nDenote H = H(w0), and H′t−1 = H(wt−1)−H(w0). By Hessian smoothness, we immediately have:\n‖H′t−1‖ = ‖H(wt−1)−H(w0)‖ ≤ ρ‖wt−1 − w0‖ ≤ ρ(‖wt − w̃t‖+ ‖w̃t − w0‖) (A.33) ‖θt−1‖ ≤ ρ\n2 ‖wt − wt−1‖2 (A.34)\nSubstitute the update equation of SGD (Eq.(A.1)) into Eq.(A.31), we have:\n∇f(wt) = ∇f(wt−1)− η(H +H′t−1)(∇f(wt−1) + ξt−1) + θt−1\n= (1− ηH)∇f(wt−1)− ηHξt−1 − ηH′t−1(∇f(wt−1) + ξt−1) + θt−1 (A.35)\nLet ∆t = ∇f(wt)−∇f̃(w̃t) denote the difference in gradient, then from Eq.(A.24), Eq.(A.35), and Eq.(A.1), we have:\n∆t = (1− ηH)∆t−1 − ηH′t−1[∆t−1 +∇f̃(w̃t−1) + ξt−1] + θt−1 (A.36) wt − w̃t = −η t−1∑\nτ=0\n∆τ (A.37)\nLet filtration Ft = σ{ξ0, · · · ξt−1}, and note σ{∆0, · · · ,∆t} ⊂ Ft, where σ{·} denotes the sigma field. Also, let event Kt = {∀τ ≤ t, ‖∇f̃(w̃τ)‖ ≤ Õ(η 1 2 log 1 η ), ‖w̃τ − w0‖ ≤ Õ(η 1 2 log 1\nη )}, and Et = {∀τ ≤ t, ‖∆τ‖ ≤ µη log2 1η}, where µ is independent of (η, ζ),\nand will be specified later. Again, Õ notation in this proof will never hide any dependence\n165\non µ. Clearly, we have Kt ⊂ Kt−1 (Et ⊂ Et−1), thus 1Kt ≤ 1Kt−1 (1Et ≤ 1Et−1), where 1K is the indicator function of event K.\nWe first need to carefully bounded all terms in Eq.(A.36), conditioned on event Kt−1 ∩Et−1, by Eq.(A.33), Eq.(A.34)), and Eq.(A.37), with probability 1, for all t ≤ T ≤ O(log d/γ0η), we have:\n‖(1− ηH)∆t−1‖ ≤ Õ(µη log2 1 η ) ‖ηH′t−1(∆t−1 +∇f̃(w̃t−1))‖ ≤ Õ(η2 log2 1 η )\n‖ηH′t−1ξt−1‖ ≤ Õ(η1.5 log 1\nη ) ‖θt−1‖ ≤ Õ(η2) (A.38)\nSince event Kt−1 ⊂ Ft−1,Et−1 ⊂ Ft−1 thus independent of ξt−1, we also have:\nE[((1− ηH)∆t−1)TηH′t−1ξt−11Kt−1∩Et−1 | Ft−1]\n=1Kt−1∩Et−1((1− ηH)∆t−1)TηH′t−1E[ξt−1 | Ft−1] = 0 (A.39)\nTherefore, from Eq.(A.36) and Eq.(A.38):\nE[‖∆t‖221Kt−1∩Et−1 | Ft−1]\n≤ [ (1 + ηγ0) 2‖∆t−1‖2 + (1 + ηγ0)‖∆t−1‖Õ(η2 log2 1\nη ) + Õ(η3 log2\n1 η )\n] 1Kt−1∩Et−1\n≤ [ (1 + ηγ0) 2‖∆t−1‖2 + Õ(µη3 log4 1\nη )\n] 1Kt−1∩Et−1 (A.40)\nDefine\nGt = (1 + ηγ0) −2t[ ‖∆t‖2 + αη2 log4\n1 η ] (A.41)\n166\nThen, when ηmax is small enough, we have:\nE[Gt1Kt−1∩Et−1 | Ft−1] = (1 + ηγ0)−2t [ E[‖∆t‖221Kt−1∩Et−1 | Ft−1] + αη2 log3 1\nη\n] 1Kt−1∩Et−1\n≤ (1 + ηγ0)−2t [ (1 + ηγ0) 2‖∆t−1‖2 + Õ(µη3 log4 1\nη ) + αη2 log4\n1 η\n] 1Kt−1∩Et−1\n≤ (1 + ηγ0)−2t [ (1 + ηγ0) 2‖∆t−1‖2 + (1 + ηγ0)2αη2 log4 1\nη\n] 1Kt−1∩Et−1\n= Gt−11Kt−1∩Et−1 ≤ Gt−11Kt−2∩Et−2 (A.42)\nTherefore, we have E[Gt1Kt−1∩Et−1 | Ft−1] ≤ Gt−11Kt−2∩Et−2 which means Gt1Kt−1∩Et−1 is a supermartingale.\nOn the other hand, we have:\n∆t = (1− ηH)∆t−1 − ηH′t−1(∆t−1 +∇f̃(w̃t−1))− ηH′t−1ξt−1 + θt−1 (A.43)\nOnce conditional on filtration Ft−1, the first two terms are deterministic, and only the third and fourth term are random. Therefore, we know, with probability 1:\n| ‖∆t‖22 − E[‖∆t‖22|Ft−1] |1Kt−1∩Et−1 ≤ Õ(µη2.5 log3 1\nη ) (A.44)\nWhere the main contribution comes from the product of the first term and third term. Then, with probability 1, we have:\n|Gt1Kt−1∩Et−1 − E[Gt1Kt−1∩Et−1 | Ft−1]|\n=(1 + 2ηγ0) −2t · | ‖∆t‖22 − E[‖∆t‖22|Ft−1] | · 1Kt−1∩Et−1 ≤ Õ(µη2.5 log3\n1 η ) = ct−1 (A.45)\n167\nBy Azuma-Hoeffding inequality, with probability less than Õ(η3), for t ≤ T ≤ O(log d/γ0η):\nGt1Kt−1∩Et−1 −G0 · 1 > Õ(1)\n√√√√ t−1∑\nτ=0\nc2τ log( 1\nη ) = Õ(µη2 log4\n1 η ) (A.46)\nThis means there exist some C̃ = Õ(1) so that:\nP ( Gt1Kt−1∩Et−1 ≥ C̃µη2 log4 1\nη\n) ≤ Õ(η3) (A.47)\nBy choosing µ > C̃, this is equivalent to:\nP ( Kt−1 ∩ Et−1 ∩ { ‖∆t‖2 ≥ µ2η2 log4 1\nη\n}) ≤ Õ(η3) (A.48)\nTherefore, combined with Lemma A.4, we have:\nP ( Et−1 ∩ { ‖∆t‖ ≥ µη log2 1\nη\n})\n=P ( Kt−1 ∩ Et−1 ∩ { ‖∆t‖ ≥ µη log2 1\nη\n}) + P ( Kt−1 ∩ Et−1 ∩ { ‖∆t‖ ≥ µη log2 1\nη\n})\n≤Õ(η3) + P (Kt−1) ≤ Õ(η3) (A.49)\nFinally, we know:\nP (Et) = P ( Et−1 ∩ { ‖∆t‖ ≥ µη log2 1\nη\n}) + P (Et−1) ≤ Õ(η3) + P (Et−1) (A.50)\nBecause P (E0) = 0, and T ≤ Õ( 1η ), we have P (ET ) ≤ Õ(η2). Due to Eq.(A.37), we have ‖wt − w̃t‖ ≤ η ∑t−1 τ=0 ‖∆τ‖, then by the definition of ET , we finish the proof.\nUsing the two lemmas above we are ready to prove Lemma A.3\n168\nProof of Lemma A.3. Let f̃ and {w̃t} be defined as in Lemma A.4. and also let λmin(H(w0)) = −γ0. Since H(w) is ρ-Lipschitz, for any w,w0, we have:\nf(w) ≤ f(w0) +∇f(w0)T (w −w0) + 1\n2 (w −w0)TH(w0)(w− w0) +\nρ 6 ‖w −w0‖3 (A.51)\nDenote δ̃ = w̃T − w0 and δ = wT − w̃T , we have:\nf(wT )− f(w0) ≤ [ ∇f(w0)T (wT − w0) + 1\n2 (wT − w0)TH(w0)(wT − w0) +\nρ 6 ‖wT − w0‖3\n]\n= [ ∇f(w0)T (δ̃ + δ) + 1\n2 (δ̃ + δ)TH(δ̃ + δ) + ρ 6 ‖δ̃ + δ‖3\n]\n= [ ∇f(w0)T δ̃ + 1\n2 δ̃THδ̃\n] + [ ∇f(w0)T δ + δ̃THδ + 1\n2 δTHδ + ρ 6 ‖δ̃ + δ‖3\n]\n(A.52)\nWhere H = H(w0). Denote Λ̃ = ∇f(w0)T δ̃+ 12 δ̃THδ̃ be the first term, and Λ = ∇f(w0)T δ+ δ̃THδ + 1 2 δTHδ + ρ 6 ‖δ̃ + δ‖3 be the second term. We have f(wT )− f(w0) ≤ Λ̃ + Λ.\nLet Et = {∀τ ≤ t, ‖w̃τ − w0‖ ≤ Õ(η 1 2 log 1 η ), ‖wt − w̃t‖ ≤ Õ(η log2 1η )}, by the result of Lemma A.4 and Lemma A.5, we know P (ET ) ≥ 1− Õ(η2). Then, clearly, we have:\nEf(wT )− f(w0) =E[f(wT )− f(w0)]1ET + E[f(wT )− f(w0)]1ET ≤EΛ̃1ET + EΛ1ET + E[f(wT )− f(w0)]1ET =EΛ̃ + EΛ1ET + E[f(wT )− f(w0)]1ET − EΛ̃1ET (A.53)\nWe will carefully caculate EΛ̃ term first, and then bound remaining term as “perturbation” to first term.\n169\nLet λ1, · · · , λd be the eigenvalues of H. By the result of lemma A.4 and simple linear algebra, we have:\nEΛ̃ = −η 2\nd∑\ni=1\n2T−1∑\nτ=0\n(1− ηλi)τ |∇if(w0)|2 + 1\n2\nd∑\ni=1\nλi\nT−1∑\nτ=0\n(1− ηλi)2τη2σ2\n≤ 1 2\nd∑\ni=1\nλi\nT−1∑\nτ=0\n(1− ηλi)2τη2σ2\n≤ η 2σ2\n2 [ d− 1 η − γ0 T−1∑\nτ=0\n(1 + ηγ0) 2τ ] ≤ −ησ 2\n2 (A.54)\nThe last inequality is directly implied by the choice of T as in Eq.(A.21). Also, by Eq.(A.21), we also immediately have that T = O(log d/γ0η) ≤ O(log d/γη). Therefore, by choose Tmax = O(log d/γη) with large enough constant, we have T ≤ Tmax = O(log d/γη).\nFor bounding the second term, by definition of Et, we have:\nEΛ1ET = E [ ∇f(w0)T δ + δ̃THδ + 1\n2 δTHδ + ρ 6 ‖δ̃ + δ‖3\n] 1ET ≤ Õ(η1.5 log3 1\nη ) (A.55)\nOn the other hand, since noise is bounded as ‖ξ‖ ≤ Õ(1), from the results of Lemma A.4, it’s easy to show ‖w̃ − w0‖ = ‖δ̃‖ ≤ Õ(1) is also bounded with probability 1. Recall the assumption that function f is also bounded, then we have:\nE[f(wT )− f(w0)]1ET − EΛ̃1ET\n=E[f(wT )− f(w0)]1ET − E [ ∇f(w0)T δ̃ + 1\n2 δ̃THδ̃\n] 1ET ≤ Õ(1)P (ET ) ≤ Õ(η 2) (A.56)\nFinally, substitute Eq.(A.54), Eq.(A.55) and Eq.(A.56) into Eq.(A.53), we finish the proof.\nFinally, we combine three cases to prove the main theorem.\n170\nProof of Theorem A.1. Let’s set L1 = {w | ‖∇f(w)‖ ≥ √ 2ησ2βd}, L2 = {w | ‖∇f(w)‖ ≤ √ 2ησ2βd and λmin(H(w)) ≤ −γ}, and L3 = Lc1 ∪ Lc2. By choosing small enough ηmax, we could make √ 2ησ2βd < min{ǫ, αδ}. Under this choice, we know from Definition 2.3 of (α, γ, ǫ, δ)-strict saddlethat L3 is the locally α-strongly convex region which is Õ(√η)-close to some local minimum.\nWe shall first prove that within Õ( 1 η2 log 1 ζ ) steps with probability at least 1− ζ/2 one of wt is in L3. Then by Lemma A.2 we know with probability at most ζ/2 there exists a wt that is in L3 but the last point is not. By union bound we will get the main result.\nTo prove within Õ( 1 η2 log 1 ζ ) steps with probability at least 1− ζ/2 one of wt is in L3, we first show starting from any point, in Õ( 1 η2 ) steps with probability at least 1/2 one of wt is in L3. Then we can repeat this log 1/ζ times to get the high probability result.\nDefine stochastic process {τi} s.t. τ0 = 0, and\nτi+1 =    τi + 1 if wτi ∈ L1 ∪ L3 τi + T (wτi) if wτi ∈ L2 (A.57)\nWhere T (wτi) is defined by Eq.(A.21) with γ0 = λmin(H(wτi))and we know T ≤ Tmax = Õ( 1η ).\nBy Lemma A.1 and Lemma A.3, we know:\nE[f(wτi+1)− f(wτi)|wτi ∈ L1,Fτi−1] = E[f(wτi+1)− f(wτi)|wτi ∈ L1] ≤ −Õ(η2) (A.58) E[f(wτi+1)− f(wτi)|wτi ∈ L2,Fτi−1] = E[f(wτi+1)− f(wτi)|wτi ∈ L2] ≤ −Õ(η) (A.59)\nTherefore, combine above equation, we have:\nE[f(wτi+1)− f(wτi)|wτi 6∈ L3,Fτi−1] = E[f(wτi+1)− f(wτi)|wτi 6∈ L3] ≤ −(τi+1− τi)Õ(η2)\n(A.60)\n171\nDefine event Ei = {∃j ≤ i, wτj ∈ L3}, clearly Ei ⊂ Ei+1, thus P (Ei) ≤ P (Ei+1). Finally, consider f(wτi+1)1Ei, we have:\nEf(wτi+1)1Ei − Ef(wτi)1Ei−1 ≤ B · P (Ei − Ei−1) + E[f(wτi+1)− f(wτi)|Ei] · P (Ei)\n≤ B · P (Ei − Ei−1)− (τi+1 − τi)Õ(η2)P (Ei) (A.61)\nTherefore, by summing up over i, we have:\nEf(wτi)1Ei − f(w0) ≤ BP (Ei)− τiÕ(η2)P (Ei) ≤ B − τiÕ(η2)P (Ei) (A.62)\nSince |f(wτi)1Ei| < B is bounded, as τi grows to as large as 6Bη2 , we must have P (Ei) < 12 . That is, after Õ( 1 η2 ) steps, with at least probability 1/2, {wt} have at least enter L3 once. Since this argument holds for any starting point, we can repeat this log 1/ζ times and we know after Õ( 1 η2 log 1/ζ) steps, with probability at least 1− ζ/2, {wt} have at least enter L3 once.\nCombining with Lemma A.2, and by union bound we know after Õ( 1 η2 log 1/ζ) steps, with probability at least 1−ζ , wt will be in the Õ( √ η log 1\nηζ ) neigborhood of some local minimum."
    }, {
      "heading" : "A.2 Detailed Analysis for Section 2.2 in Constrained",
      "text" : "Case\nSo far, we have been discussed all about unconstrained problem. In this section we extend our result to equality constraint problems under some mild conditions.\n172\nConsider the equality constrained optimization problem:\nmin w\nf(w) (A.63)\ns.t. ci(w) = 0, i = 1, · · · , m\nDefine the feasible set as the set of points that satisfy all the constraints W = {w | ci(w) = 0; i = 1, · · · , m}.\nIn this case, the algorithm we are running is Projected Noisy Gradient Descent. Let function ΠW(v) to be the projection to the feasible set where the projection is defined as the global solution of minw∈W ‖v − w‖2.\nWith same argument as in the unconstrained case, we could slightly simplify and convert it to standard projected stochastic gradient descent (PSGD) with update equation:\nvt = wt−1 − η∇f(wt−1) + ξt−1 (A.64)\nwt = ΠW(vt) (A.65)\nAs in unconstrained case, we are interested in noise ξ is i.i.d satisfying Eξ = 0, EξξT = σ2I and ‖ξ‖ ≤ Q almost surely. Our proof can be easily extended to Algorithm 2 with 1 d I EξξT (Q + 1 d )I. In this section we first introduce basic tools for handling constrained optimization problems (most these materials can be found in [164]), then we prove some technical lemmas that are useful for dealing with the projection step in PSGD, finally we point out how to modify the previous analysis.\n173"
    }, {
      "heading" : "A.2.1 Preliminaries",
      "text" : "Often for constrained optimization problems we want the constraints to satisfy some regularity conditions. LICQ (linear independent constraint quantification) is a common assumption in this context.\nDefinition A.1 (LICQ). In equality-constraint problem Eq.(A.63), given a point w, we say that the linear independence constraint qualification (LICQ) holds if the set of constraint gradients {∇ci(x), i = 1, · · · , m} is linearly independent.\nIn constrained optimization, we can locally transform it to an unconstrained problem by introducing Lagrangian multipliers. The Langrangian L can be written as\nL(w, λ) = f(w)− m∑\ni=1\nλici(w) (A.66)\nThen, if LICQ holds for all w ∈ W, we can properly define function λ∗(·) to be:\nλ∗(w) = argmin λ\n‖∇f(w)− m∑\ni=1\nλi∇ci(w)‖ = argmin λ ‖∇wL(w, λ)‖ (A.67)\nwhere λ∗(·) can be calculated analytically: let matrix C(w) = (∇c1(w), · · · ,∇cm(w)), then we have:\nλ∗(w) = C(w)†∇f(w) = (C(w)TC(w))−1C(w)T∇f(w) (A.68)\nwhere (·)† is Moore-Penrose pseudo-inverse.\nIn our setting we need a stronger regularity condition which we call robust LICQ (RLICQ).\nDefinition A.2 ( αc-RLICQ ). In equality-constraint problem Eq.(A.63), given a point w, we say that αc-robust linear independence constraint qualification ( αc-RLICQ ) holds if the\n174\nminimum singular value of matrix C(w) = (∇c1(w), · · · ,∇cm(w)) is greater or equal to αc, that is σmin(C(w)) ≥ αc.\nRemark. Given a point w ∈ W, αc-RLICQ implies LICQ. While LICQ holds for all w ∈ W is a necessary condition for λ∗(w) to be well-defined; it’s easy to check that αc-RLICQ holds for all w ∈ W is a necessary condition for λ∗(w) to be bounded. Later, we will also see αc-RLICQ combined with the smoothness of {ci(w)}mi=1 guarantee the curvature of constraint manifold to be bounded everywhere.\nNote that we require this condition in order to provide a quantitative bound, without this assumption there can be cases that are exponentially close to a function that does not satisfy LICQ.\nWe can also write down the first-order and second-order partial derivative of Lagrangian L at point (w, λ∗(w)):\nχ(w) = ∇wL(w, λ)|(w,λ∗(w)) = ∇f(w)− m∑\ni=1\nλ∗i (w)∇ci(w) (A.69)\nM(w) = ∇2wwL(w, λ)|(w,λ∗(w)) = ∇2f(w)− m∑\ni=1\nλ∗i (w)∇2ci(w) (A.70)\nDefinition A.3 (Tangent Space and Normal Space). Given a feasible point w ∈ W, define its corresponding Tangent Space to be T (w) = {v | ∇ci(w)Tv = 0; i = 1, · · · , m}, and Normal Space to be T c(w) = span{∇c1(w) · · · ,∇cm(w)}\nIf w ∈ Rd, and we have m constraint satisfying αc-RLICQ , the tangent space would be a linear subspace with dimension d −m; and the normal space would be a linear subspace with dimension m. We also know immediately that χ(w) defined in Eq.(A.69) has another interpretation: it’s the component of gradient ∇f(w) in tangent space.\n175\nAlso, it’s easy to see the normal space T c(w) is the orthogonal complement of T . We can also define the projection matrix of any vector onto tangent space (or normal space) to be PT (w) (or PT c(w)). Then, clearly, both PT (w) and PT c(w) are orthoprojector, thus symmetric. Also by Pythagorean theorem, we have:\n‖v‖2 = ‖PT (w)v‖2 + ‖PT c(w)v‖2, ∀v ∈ Rd (A.71)\nTaylor Expansion Let w,w0 ∈ W, and fix λ∗ = λ∗(w0) independent of w, assume∇2wwL(w, λ∗) is ρL-Lipschitz, that is ‖∇2wwL(w1, λ∗)−∇2wwL(w2, λ∗)‖ ≤ ρL‖w1−w2‖ By Taylor expansion, we have:\nL(w, λ∗) ≤L(w0, λ∗) +∇wL(w0, λ∗)T (w − w0)\n+ 1\n2 (w − w0)T∇2wwL(w0, λ∗)(w − w0) + ρL 6 ‖w − w0‖3 (A.72)\nSince w,w0 are feasible, we know: L(w, λ∗) = f(w) and L(w0, λ∗) = f(w0), this gives:\nf(w) ≤ f(w0) + χ(w0)T (w − w0) + 1\n2 (w − w0)TM(w0)(w − w0) + ρL 6 ‖w − w0‖3\n(A.73)\nDerivative of χ(w) By taking derative of χ(w) again, we know the change of this tangent gradient can be characterized by:\n∇χ(w) = H− m∑\ni=1\nλ∗i (w)∇2ci(w)− m∑\ni=1\n∇ci(w)∇λ∗i (w)T (A.74)\nDenote\nN(w) = − m∑\ni=1\n∇ci(w)∇λ∗i (w)T (A.75)\n176\nWe immediately know that ∇χ(w) = M(w) +N(w).\nRemark. The additional term N(w) is not necessary to be even symmetric in general. This is due to the fact that χ(w) may not be the gradient of any scalar function. However, N(w) has an important property that is: for any vector v ∈ Rd, N(w)v ∈ T c(w).\nFinally, for completeness, we state here the first/second-order necessary (or sufficient) conditions for optimality. Please refer to [164] for the proof of those theorems.\nTheorem A.2 (First-Order Necessary Conditions). In equality constraint problem Eq.(A.63), suppose that w† is a local solution, and that the functions f and ci are continuously differentiable, and that the LICQ holds at w†. Then there is a Lagrange multiplier vector λ†, such that:\n∇wL(w†, λ†) = 0 (A.76)\nci(w †) = 0, for i = 1, · · · , m (A.77)\nThese conditions are also usually referred as Karush-Kuhn-Tucker (KKT) conditions.\nTheorem A.3 (Second-Order Necessary Conditions). In equality constraint problem Eq.(A.63), suppose that w† is a local solution, and that the LICQ holds at w†. Let λ† Lagrange multiplier vector for which the KKT conditions are satisfied. Then:\nvT∇2xxL(w†, λ†)v ≥ 0 for all v ∈ T (w†) (A.78)\nTheorem A.4 (Second-Order Sufficient Conditions). In equality constraint problem Eq.(A.63), suppose that for some feasible point w† ∈ Rd, and there’s Lagrange multiplier vector λ† for which the KKT conditions are satisfied. Suppose also that:\nvT∇2xxL(w†, λ†)v > 0 for all v ∈ T (w†), v 6= 0 (A.79)\n177\nThen w† is a strict local solution.\nRemark. By definition Eq.(A.68), we know immediately λ∗(w†) is one of valid Lagrange multipliers λ† for which the KKT conditions are satisfied. This means χ(w†) = ∇wL(w†, λ†) and M(w†) = L(w†, λ†).\nTherefore, Theorem A.2, A.3, A.4 gives strong implication that χ(w) and M(w) are the right thing to look at, which are in some sense equivalent to ∇f(w) and ∇2f(w) in unconstrained case."
    }, {
      "heading" : "A.2.2 Geometrical Lemmas Regarding Constraint Manifold",
      "text" : "Since in equality constraint problem, at each step of PSGD, we are effectively considering the local manifold around feasible point wt−1. In this section, we provide some technical lemmas relating to the geometry of constraint manifold in preparsion for the proof of main theorem in equality constraint case.\nWe first show if two points are close, then the projection in the normal space is much smaller than the projection in the tangent space.\nLemma A.6. Suppose the constraints {ci}mi=1 are βi-smooth, and αc-RLICQ holds for all w ∈ W. Then, let ∑mi=1 β2i α2c = 1 R2 , for any w,w0 ∈ W, let T0 = T (w0), then\n‖PT c0 (w − w0)‖ ≤ 1\n2R ‖w − w0‖2 (A.80)\nFurthermore, if ‖w − w0‖ < R holds, we additionally have:\n‖PT c0 (w − w0)‖ ≤ ‖PT0(w − w0)‖2\nR (A.81)\n178\nProof. First, since for any vector v̂ ∈ T0, we have ‖C(w0)T v̂‖ = 0, then by simple linear algebra, it’s easy to show:\n‖C(w0)T (w − w0)‖2 =‖C(w0)TPT c0 (w − w0)‖2 ≥ σ2min‖PT c0 (w − w0)‖2\n≥α2c‖PT c0 (w − w0)‖2 (A.82)\nOn the other hand, by βi-smooth, we have:\n|ci(w)− ci(w0)−∇ci(w0)T (w − w0)| ≤ βi 2 ‖w − w0‖2 (A.83)\nSince w,w0 are feasible points, we have ci(w) = ci(w0) = 0, which gives:\n‖C(w0)T (w − w0)‖2 = m∑\ni=1\n(∇ci(w0)T (w − w0))2 ≤ m∑\ni=1\nβ2i 4 ‖w − w0‖4 (A.84)\nCombining Eq.(A.82) and Eq.(A.84), and the definition of R, we have:\n‖PT c0 (w − w0)‖2 ≤ 1\n4R2 ‖w − w0‖4 =\n1\n4R2 (‖PT c0 (w − w0)‖2 + ‖PT0(w − w0)‖2)2 (A.85)\nSolving this second-order inequality gives two solution\n‖PT c0 (w − w0)‖ ≤ ‖PT0(w − w0)‖2\nR or ‖PT c0 (w − w0)‖ ≥ R (A.86)\nBy assumption, we know ‖w − w0‖ < R (so the second case cannot be true), which finishes the proof.\nHere, we see the √∑m\ni=1 β2i α2c = 1 R serves as a upper bound of the curvatures on the constraint\nmanifold, and equivalently, R serves as a lower bound of the radius of curvature. αc-RLICQ and smoothness guarantee that the curvature is bounded.\n179\nNext we show the normal/tangent space of nearby points are close.\nLemma A.7. Suppose the constraints {ci}mi=1 are βi-smooth, and αc-RLICQ holds for all w ∈ W. Let ∑mi=1 β2i α2c = 1 R2 , for any w,w0 ∈ W, let T0 = T (w0). Then for all v̂ ∈ T (w) so that ‖v̂‖ = 1, we have\n‖PT c0 · v̂‖ ≤ ‖w − w0‖\nR (A.87)\nProof. With similar calculation as Eq.(A.82), we immediately have:\n‖PT c0 · v̂‖2 ≤ ‖C(w0)T v̂‖2 σ2min(C(w)) ≤ ‖C(w0) T v̂‖2 α2c (A.88)\nSince v̂ ∈ T (w) , we have C(w)T v̂ = 0, combined with the fact that v̂ is a unit vector, we have:\n‖C(w0)T v̂‖2 =‖[C(w0)− C(w)]T v̂‖2 = m∑\ni=1\n([∇ci(w0)−∇ci(w)]T v̂)2\n≤ m∑\ni=1\n‖∇ci(w0)−∇ci(w)‖2‖v̂‖2 ≤ m∑\ni=1\nβ2i ‖w0 − w‖2 (A.89)\nCombining Eq.(A.88) and Eq.(A.89), and the definition of R, we concludes the proof.\nLemma A.8. Suppose the constraints {ci}mi=1 are βi-smooth, and αc-RLICQ holds for all w ∈ W. Let ∑mi=1 β2i α2c = 1 R2 , for any w,w0 ∈ W, let T0 = T (w0). Then for all v̂ ∈ T c(w) so that ‖v̂‖ = 1, we have\n‖PT0 · v̂‖ ≤ ‖w − w0‖\nR (A.90)\nProof. By definition of projection, clearly, we have PT0 · v̂ + PT c0 · v̂ = v̂. Since v̂ ∈ T c(w), without loss of generality, assume v̂ = ∑m i=1 λi∇ci(w). Define d̃ = ∑m i=1 λi∇ci(w0), clearly\n180\nd̃ ∈ T c0 . Since projection gives the closest point in subspace, we have:\n‖PT0 · v̂‖ =‖PT c0 · v̂ − v̂‖ ≤ ‖d̃− v̂‖\n≤ m∑\ni=1\nλi‖∇ci(w0)−∇ci(w)‖ ≤ m∑\ni=1\nλiβi‖w0 − w‖ (A.91)\nOn the other hand, let λ = (λ1, · · · , λm)T , we know C(w)λ = v̂, thus:\nλ = C(w)†v̂ = (C(w)TC(w))−1C(w)T v̂ (A.92)\nTherefore, by αc-RLICQ and the fact v̂ is unit vector, we know: ‖λ‖ ≤ 1αc . Combined with Eq.(A.91), we finished the proof.\nUsing the previous lemmas, we can then prove that: starting from any point w0 on constraint manifold, the result of adding any small vector v and then projected back to feasible set, is not very different from the result of adding PT (w0)v.\nLemma A.9. Suppose the constraints {ci}mi=1 are βi-smooth, and αc-RLICQ holds for all w ∈ W. Let ∑mi=1 β2i α2c = 1 R2 , for any w0 ∈ W, let T0 = T (w0). Then let w1 = w0 + ηv̂, and w2 = w0 + ηPT0 · v̂, where v̂ ∈ Sd−1 is a unit vector. Then, we have:\n‖ΠW(w1)− w2‖ ≤ 4η2\nR (A.93)\nWhere projection ΠW(w) is defined as the closet point to w on feasible set W.\nProof. First, note that ‖w1 − w0‖ = η, and by definition of projection, there must exist a project ΠW(w) inside the ball Bη(w1) = {w | ‖w − w1‖ ≤ η}.\n181\nDenote u1 = ΠW(w1), and clearly u1 ∈ W. we can formulate u1 as the solution to following constrained optimization problems:\nmin u\n‖w1 − u‖2 (A.94)\ns.t. ci(u) = 0, i = 1, · · · , m\nSince function f(u) = ‖w1−u‖2 and ci(u) are continuously differentiable by assumption, and the condition αc-RLICQ holds for all w ∈ W implies that LICQ holds for u1. Therefore, by Karush-Kuhn-Tucker necessary conditions, we immediately know (w1 − u1) ∈ T c(u1).\nSince u1 ∈ Bη(w1), we know ‖w0 − u1‖ ≤ 2η, by Lemma A.8, we immediately have:\n‖PT0(w1 − u1)‖ = ‖PT0(w1 − u1)‖\n‖w1 − u1‖ ‖w1 − u1‖ ≤\n1 R ‖w0 − u1‖ · ‖w1 − u1‖ ≤ 2 R η2 (A.95)\nLet v1 = w0 + PT0(u1 − w0), we have:\n‖v1 − w2‖ =‖(v1 − w0)− (w2 − w0)‖ = ‖PT0(u1 − w0)− PT0(w1 − w0)‖\n=‖PT0(w1 − u1)‖ ≤ 2\nR η2 (A.96)\nOn the other hand by Lemma A.6, we have:\n‖u1 − v1‖ = ‖PT c0 (u1 − w0)‖ ≤ 1\n2R ‖u1 − w0‖2 ≤\n2 R η2 (A.97)\nCombining Eq.(A.96) and Eq.(A.97), we finished the proof.\n182"
    }, {
      "heading" : "A.2.3 Main Theorem",
      "text" : "Now we are ready to prove the main theorems. First we revise the definition of strict saddle in the constrained case.\nDefinition A.4. A twice differentiable function f(w) with constraints ci(w) is (α, γ, ǫ, δ)strict saddle, if for any point w one of the following is true\n1. ‖χ(w)‖ ≥ ǫ.\n2. v̂TM(w)v̂ ≤ −γ for some v̂ ∈ T (w), ‖v̂‖ = 1\n3. There is a local minimum w⋆ such that ‖w − w⋆‖ ≤ δ, and for all w′ in the 2δ neigh-\nborhood of w⋆, we have v̂TM(w′)v̂ ≥ α for all v̂ ∈ T (w′), ‖v̂‖ = 1\nNext, we prove a equivalent formulation for PSGD.\nLemma A.10. Suppose the constraints {ci}mi=1 are βi-smooth, and αc-RLICQ holds for all w ∈ W. Furthermore, if function f is L-Lipschitz, and the noise ξ is bounded, then running PSGD as in Eq.(A.64) is equivalent to running:\nwt = wt−1 − η · (χ(wt−1) + PT (wt−1)ξt−1) + ιt−1 (A.98)\nwhere ι is the correction for projection, and ‖ι‖ ≤ Õ(η2).\nProof. Lemma A.10 is a direct corollary of Lemma A.9.\nThe intuition behind this lemma is that: when {ci}mi=1 are smooth and αc-RLICQ holds for all w ∈ W, then the constraint manifold has bounded curvature every where. Then, if we only care about first order behavior, it’s well-approximated by the local dynamic in tangent plane, up to some second-order correction.\n183\nTherefore, by Eq.(A.98), we see locally it’s not much different from the unconstrainted case Eq.(A.1) up to some negeligable correction. In the following analysis, we will always use formula Eq.(A.98) as the update equation for PSGD.\nSince most of following proof bears a lot similarity as in unconstrained case, we only pointed out the essential steps in our following proof.\nTheorem A.5 (Main Theorem for Equality-Constrained Case). Suppose a function f(w) : Rd → R with constraints ci(w) : Rd → R is (α, γ, ǫ, δ)-strict saddle, and has a stochastic gradient oracle with radius at most Q, also satisfying Eξ = 0 and EξξT = σ2I. Further, suppose the function function f is B-bounded, L-Lipschitz, β-smooth, and has ρ-Lipschitz Hessian, and the constraints {ci}mi=1 is Li-Lipschitz, βi-smooth, and has ρi-Lipschitz Hessian. Then there exists a threshold ηmax = Θ̃(1), so that for any ζ > 0, and for any η ≤ ηmax/max{1, log(1/ζ)}, with probability at least 1− ζ in t = Õ(η−2 log(1/ζ)) iterations, PSGD outputs a point wt that is Õ( √ η log(1/ηζ))-close to some local minimum w⋆.\nFirst, we proof the assumptions in main theorem implies the smoothness conditions for M(w), N(w) and ∇2wwL(w, λ∗(w′)).\nLemma A.11. Under the assumptions of Theorem A.5, there exists βM , βN , ρM , ρN , ρL polynomial related to B,L, β, ρ, 1 αc and {Li, βi, ρi}mi=1 so that:\n1. ‖M(w)‖ ≤ βM and ‖N(w)‖ ≤ βN for all w ∈ W.\n2. M(w) is ρM -Lipschitz, and N(w) is ρN -Lipschitz, and ∇2wwL(w, λ∗(w′)) is ρL-Lipschitz\nfor all w′ ∈ W.\nProof. By definition of M(w), N(w) and ∇2wwL(w, λ∗(w′)), the above conditions will holds if there exists Bλ, Lλ, βλ bounded by Õ(1), so that λ ∗(w) is Bλ-bounded, Lλ-Lipschitz, and βλ-smooth.\n184\nBy definition Eq.(A.68), we have:\nλ∗(w) = C(w)†∇f(w) = (C(w)TC(w))−1C(w)T∇f(w) (A.99)\nBecause f is B-bounded, L-Lipschitz, β-smooth, and its Hessian is ρ-Lipschitz, thus, eventually, we only need to prove that there exists Bc, Lc, βc bounded by Õ(1), so that the pseudo-inverse C(w)† is Bc-bounded, Lc-Lipschitz, and βc-smooth.\nSince αc-RLICQ holds for all feasible points, we immediately have: ‖C(w)†‖ ≤ 1αc , thus bounded. For simplicity, in the following context we use C† to represent C†(w) without ambiguity. By some calculation of linear algebra, we have the derivative of pseudo-inverse:\n∂C(w)†\n∂wi = −C†∂C(w) ∂wi C† + C†[C†]T\n∂C(w)T\n∂wi (I − CC†) (A.100)\nAgain, αc-RLICQ holds implies that derivative of pseudo-inverse is well-defined for every feasible point. Let tensor E(w), Ẽ(w) to be the derivative of C(w), C†(w), which is defined as:\n[E(w)]ijk = ∂[C(w)]ik\n∂wj [Ẽ(w)]ijk = ∂[C(w)†]ik ∂wj\n(A.101)\nDefine the transpose of a 3rd order tensor ETi,j,k = Ek,j,i, then we have\nẼ(w) = −[E(w)](C†, I, C†) + [E(w)T ](C†[C†]T , I, (I − CC†)) (A.102)\nwhere by calculation [E(w)](I, I, ei) = ∇2ci(w).\nFinally, since C(w)† and ∇2ci(w) are bounded by Õ(1), by Eq.(A.102), we know Ẽ(w) is bounded, that is C(w)† is Lipschitz. Again, since both C(w)† and ∇2ci(w) are bounded, Lipschitz, by Eq.(A.102), we know Ẽ(w) is also Õ(1)-Lipschitz. This finishes the proof.\n185\nFrom now on, we can use the same proof strategy as unconstraint case. Below we list the corresponding lemmas and the essential steps that require modifications.\nLemma A.12. Under the assumptions of Theorem A.5, with notations in Lemma A.11, for any point with ‖χ(w0)‖ ≥ √ 2ησ2βM(d−m) where √ 2ησ2βM(d−m) < ǫ, after one iteration we have:\nEf(w1)− f(w0) ≤ −Ω̃(η2) (A.103)\nProof. Choose ηmax < 1\nβM , and also small enough, then by update equation Eq.(A.98), we\nhave:\nEf(w1)− f(w0) ≤ χ(w0)TE(w1 − w0) + βM 2 E‖w1 − w0‖2\n≤ −(η − βMη 2\n2 )‖χ(w0)‖2 + η2σ2βM(d−m) 2 + Õ(η2)‖χ(w0)‖+ Õ(η3)\n≤ −(η − Õ(η1.5)− βMη 2\n2 )‖χ(w0)‖2 + η2σ2βM(d−m) 2 + Õ(η3)\n≤ −η 2σ2βMd\n4 (A.104)\nWhich finishes the proof.\nTheorem A.6. Under the assumptions of Theorem A.5, with notations in Lemma A.11, for any initial point w0 that is Õ( √ η) < δ close to a local minimum w⋆, with probability at least 1− ζ/2, we have following holds simultaneously:\n∀t ≤ Õ( 1 η2 log 1 ζ ), ‖wt − w⋆‖ ≤ Õ(\n√ η log 1\nηζ ) < δ (A.105)\nwhere w⋆ is the locally optimal point.\n186\nProof. By calculus, we know\nχ(wt) =χ(w ⋆) +\n∫ 1\n0\n(M+N)(w⋆ + t(wt − w⋆))dt · (wt − w⋆) (A.106)\nLet filtration Ft = σ{ξ0, · · · ξt−1}, and note σ{∆0, · · · ,∆t} ⊂ Ft, where σ{·} denotes the sigma field. Let event Et = {∀τ ≤ t, ‖wτ − w⋆‖ ≤ µ √ η log 1 ηζ < δ}, where µ is independent of (η, ζ), and will be specified later.\nBy Definition A.4 of (α, γ, ǫ, δ)-strict saddle, we know M(w) is locally α-strongly convex restricted to its tangent space T (w). in the 2δ-neighborhood of w⋆. If ηmax is chosen small enough, by Remark A.2.1 and Lemma A.6, we have in addition:\nχ(wt) T (wt − w⋆)1Et = (wt − w⋆)T\n∫ 1\n0\n(M+N)(w⋆ + t(wt − w⋆))dt · (wt − w⋆)1Et\n≥ [α‖wt − w⋆‖2 − Õ(‖wt − w⋆‖3)]1Et ≥ 0.5α‖wt − w⋆‖21Et (A.107)\nThen, everything else follows almost the same as the proof of Lemma A.2.\nLemma A.13. Under the assumptions of Theorem A.5, with notations in Lemma A.11, for any initial point w0 where ‖χ(w0)‖ ≤ Õ(η) < ǫ, and v̂TM(w0)v̂ ≤ −γ for some v̂ ∈ T (w), ‖v̂‖ = 1, then there is a number of steps T that depends on w0 such that:\nEf(wT )− f(w0) ≤ −Ω̃(η) (A.108)\nThe number of steps T has a fixed upper bound Tmax that is independent of w0 where T ≤ Tmax = O((log(d−m))/γη).\nSimilar to the unconstrained case, we show this by a coupling sequence. Here the sequence we construct will only walk on the tangent space, by Lemmas in previous subsection, we\n187\nknow this is not very far from the actual sequence. We first define and characterize the coupled sequence in the following lemma:\nLemma A.14. Under the assumptions of Theorem A.5, with notations in Lemma A.11. Let f̃ defined as local second-order approximation of f(x) around w0 in tangent space T0 = T (w0):\nf̃(w) . = f(w0) + χ(w0) T (w − w0) + 1\n2 (w − w0)T [P TT0M(w0)PT0](w − w0) (A.109)\n{w̃t} be the corresponding sequence generated by running SGD on function f̃ , with w̃0 = w0, and noise projected to T0, (i.e. w̃t = w̃t−1 − η(χ̃(w̃t−1) + PT0ξt−1). For simplicity, denote χ̃(w) = ∇f̃(w), and M̃ = P TT0M(w0)PT0, then we have analytically:\nχ̃(w̃t) = (1− ηM̃)tχ̃(w̃0)− ηM̃ t−1∑\nτ=0\n(1− ηM̃)t−τ−1PT0ξτ (A.110)\nw̃t − w0 = −η t−1∑\nτ=0\n(1− ηM̃)τ χ̃(w̃0)− η t−1∑\nτ=0\n(1− ηM̃)t−τ−1PT0ξτ (A.111)\nFurther, for any initial point w0 where ‖χ(w0)‖ ≤ Õ(η) < ǫ, and minv̂∈T (w),‖v̂‖=1 v̂TM(w0)v̂ = −γ0. There exist a T ∈ N satisfying:\nd−m ηγ0 ≤ T−1∑\nτ=0\n(1 + ηγ0) 2τ < 3(d−m) ηγ0\n(A.112)\nwith probability at least 1− Õ(η3), we have following holds simultaneously for all t ≤ T :\n‖w̃t − w0‖ ≤ Õ(η 1 2 log\n1 η ); ‖χ̃(w̃t)‖ ≤ Õ(η 1 2 log 1 η ) (A.113)\nProof. Clearly we have:\nχ̃(w̃t) = χ̃(w̃t−1) + M̃(w̃t − w̃t−1) (A.114)\n188\nand\nw̃t = w̃t−1 − η(χ̃(w̃t−1) + PT0ξt−1) (A.115)\nThis lemma is then proved by a direct application of Lemma A.4.\nThen we show the sequence constructed is very close to the actual sequence.\nLemma A.15. Under the assumptions of Theorem A.5, with notations in Lemma A.11. Let {wt} be the corresponding sequence generated by running PSGD on function f . Also let f̃ and {w̃t} be defined as in Lemma A.14. Then, for any initial point w0 where ‖χ(w0)‖2 ≤ Õ(η) < ǫ, and minv̂∈T (w),‖v̂‖=1 v̂ TM(w0)v̂ = −γ0. Given the choice of T as in Eq.(A.112), with probability at least 1− Õ(η2), we have following holds simultaneously for all t ≤ T :\n‖wt − w̃t‖ ≤ Õ(η log2 1\nη ); (A.116)\nProof. First, we have update function of tangent gradient by:\nχ(wt) =χ(wt−1) +\n∫ 1\n0\n∇χ(wt−1 + t(wt − wt−1))dt · (wt − wt−1)\n=χ(wt−1) +M(wt−1)(wt − wt−1) +N(wt−1)(wt − wt−1) + θt−1 (A.117)\nwhere the remainder:\nθt−1 ≡ ∫ 1\n0\n[∇χ(wt−1 + t(wt − wt−1))−∇χ(wt−1)] dt · (wt − wt−1) (A.118)\n189\nProject it to tangent space T0 = T (w0). Denote M̃ = P TT0M(w0)PT0, and M̃′t−1 = P TT0[M(wt1)− M(w0) ]PT0 . Then, we have:\nPT0 · χ(wt) =PT0 · χ(wt−1) + PT0(M(wt−1) +N(wt−1))(wt − wt−1) + PT0θt−1\n=PT0 · χ(wt−1) + PT0M(wt−1)PT0(wt − wt−1)\n+ PT0M(wt−1)PT c0 (wt − wt−1) + PT0N(wt−1)(wt − wt−1) + PT0θt−1\n=PT0 · χ(wt−1) + M̃(wt − wt−1) + φt−1 (A.119)\nWhere\nφt−1 = [ M̃ ′ t−1 + PT0M(wt−1)PT c0 + PT0N(wt−1) ](wt − wt−1) + PT0θt−1 (A.120)\nBy Hessian smoothness, we immediately have:\n‖M̃′t−1‖ = ‖M(wt1)−M(w0)‖ ≤ ρM‖wt−1 − w0‖ ≤ ρM(‖wt − w̃t‖+ ‖w̃t − w0‖)\n(A.121)\n‖θt−1‖ ≤ ρM + ρN\n2 ‖wt − wt−1‖2 (A.122)\nSubstitute the update equation of PSGD (Eq.(A.98)) into Eq.(A.119), we have:\nPT0 · χ(wt) = PT0 · χ(wt−1)− ηM̃(PT0 · χ(wt−1) + PT0 · PT (wt−1)ξt−1) + M̃ · ιt−1 + φt−1\n= (1− ηM̃)PT0 · χ(wt−1)− ηM̃PT0ξt−1 + ηM̃PT0 · PT c(wt−1)ξt−1 + M̃ · ιt−1 + φt−1 (A.123)\n190\nLet ∆t = PT0 · χ(wt)− χ̃(w̃t) denote the difference of tangent gradient in T (w0), then from Eq.(A.114), Eq.(A.115), and Eq.(A.123) we have:\n∆t = (1− ηH)∆t−1 + ηM̃PT0 · PT c(wt−1)ξt−1 + M̃ · ιt−1 + φt−1 (A.124) PT0 · (wt − w0)− (w̃t − w0) = −η t−1∑\nτ=0\n∆τ + η t−1∑\nτ=0\nPT0 · PT c(wτ )ξτ + t−1∑\nτ=0\nιτ (A.125)\nBy Lemma A.6, we know if ∑m\ni=1 β2i α2c = 1 R2 , then we have:\n‖PT c0 (wt − w0)‖ ≤ ‖wt − w0‖2\n2R (A.126)\nLet filtration Ft = σ{ξ0, · · · ξt−1}, and note σ{∆0, · · · ,∆t} ⊂ Ft, where σ{·} denotes the sigma field. Also, let event Kt = {∀τ ≤ t, ‖χ̃(w̃τ)‖ ≤ Õ(η 1 2 log 1 η ), ‖w̃τ−w0‖ ≤ Õ(η 1 2 log 1 η )}, and denote Γt = η ∑t−1 τ=0 PT0 · PT c(wτ )ξτ , let Et = {∀τ ≤ t, ‖∆τ‖ ≤ µ1η log2 1η , ‖Γτ‖ ≤ µ2η log 2 1 η , ‖wτ − w̃τ‖ ≤ µ3η log2 1η} where (µ1, µ2, µ3) are is independent of (η, ζ), and will be determined later. To prevent ambiguity in the proof, Õ notation will not hide any dependence on µ. Clearly event Kt−1 ⊂ Ft−1,Et−1 ⊂ Ft−1 thus independent of ξt−1.\nThen, conditioned on event Kt−1 ∩ Et−1, by triangle inequality, we have ‖wτ − w0‖ ≤ Õ(η 1 2 log 1\nη ), for all τ ≤ t − 1 ≤ T − 1. We then need to carefully bound the following\nbound each term in Eq.(A.124). We know wt − wt−1 = −η · (χ(wt−1) + PT (wt−1)ξt−1) + ιt−1,\n191\nand then by Lemma A.8 and Lemma A.7, we have:\n‖ηM̃PT0 · PT c(wt−1)ξt−1‖ ≤ Õ(η1.5 log 1\nη )\n‖M̃ · ιt−1‖ ≤ Õ(η2)\n‖[ M̃′t−1 + PT0M(wt−1)PT c0 + PT0N(wt−1) ](−η · χ(wt−1))‖ ≤ Õ(η2 log 2 1\nη )\n‖[ M̃′t−1 + PT0M(wt−1)PT c0 + PT0N(wt−1) ](−ηPT (wt−1)ξt−1)‖ ≤ Õ(η1.5 log 1\nη )\n‖[ M̃′t−1 + PT0M(wt−1)PT c0 + PT0N(wt−1) ]ιt−1‖ ≤ Õ(η2)\n‖PT0θt−1‖ ≤ Õ(η2) (A.127)\nTherefore, abstractly, conditioned on event Kt−1 ∩ Et−1, we could write down the recursive equation as:\n∆t = (1− ηH)∆t−1 + A+B (A.128)\nwhere ‖A‖ ≤ Õ(η1.5 log 1 η ) and ‖B‖ ≤ Õ(η2 log2 1 η ), and in addition, by independence, easy to check we also have E[(1 − ηH)∆t−1A|Ft−1] = 0. This is exactly the same case as in the proof of Lemma A.5. By the same argument of martingale and Azuma-Hoeffding, and by choosing µ1 large enough, we can prove\nP ( Et−1 ∩ { ‖∆t‖ ≥ µ1η log2 1\nη\n}) ≤ Õ(η3) (A.129)\nOn the other hand, for Γt = η ∑t−1 τ=0 PT0 · PT c(wτ )ξτ , we have:\nE[Γt1Kt−1∩Et−1 |Ft−1] = [ Γt−1 + ηE[PT0 · PT c(wt−1)ξt−1|Ft−1] ] 1Kt−1∩Et−1\n= Γt−11Kt−1∩Et−1 ≤ Γt−11Kt−2∩Et−2 (A.130)\n192\nTherefore, we have E[Γt1Kt−1∩Et−1 | Ft−1] ≤ Γt−11Kt−2∩Et−2 which means Γt1Kt−1∩Et−1 is a supermartingale.\nWe also know by Lemma A.8, with probability 1:\n|Γt1Kt−1∩Et−1 − E[Γt1Kt−1∩Et−1 | Ft−1]| = |ηPT0 · PT c(wt−1)ξt−1| · 1Kt−1∩Et−1\n≤Õ(η)‖wt−1 − w0‖1Kt−1∩Et−1 ≤ Õ(η1.5 log 1\nη ) = ct−1 (A.131)\nBy Azuma-Hoeffding inequality, with probability less than Õ(η3), for t ≤ T ≤ O(log(d − m)/γ0η):\nΓt1Kt−1∩Et−1 − Γ0 · 1 > Õ(1)\n√√√√ t−1∑\nτ=0\nc2τ log( 1\nη ) = Õ(η log2\n1 η ) (A.132)\nThis means there exists some C̃2 = Õ(1) so that:\nP ( Kt−1 ∩ Et−1 ∩ { ‖Γt‖ ≥ C̃2η log2 1\nη\n}) ≤ Õ(η3) (A.133)\nby choosing µ2 > C̃2, we have:\nP ( Kt−1 ∩ Et−1 ∩ { ‖Γt‖ ≥ µ2η log2 1\nη\n}) ≤ Õ(η3) (A.134)\nTherefore, combined with Lemma A.14, we have:\nP ( Et−1 ∩ { ‖Γt‖ ≥ µ2η log2 1\nη\n}) ≤ Õ(η3) + P (Kt−1) ≤ Õ(η3) (A.135)\nFinally, conditioned on event Kt−1 ∩ Et−1, if we have ‖Γt‖ ≤ µ2η log2 1η , then by Eq.(A.125):\n‖PT0 · (wt − w0)− (w̃t − w0)‖ ≤ Õ ( (µ1 + µ2)η log 2 1\nη\n) (A.136)\n193\nSince ‖wt−1 − w0‖ ≤ Õ(η 1 2 log 1 η ), and ‖wt − wt−1‖ ≤ Õ(η), by Eq.(A.126):\n‖PT c0 (wt − w0)‖ ≤ ‖wt − w0‖2 2R ≤ Õ(η log2 1 η ) (A.137)\nThus:\n‖wt − w̃t‖2 =‖PT0 · (wt − w̃t)‖2 + ‖PT c0 · (wt − w̃t)‖2\n=‖PT0 · (wt − w0)− (w̃t − w0)‖2 + ‖PT c0 (wt − w0)‖2 ≤ Õ((µ1 + µ2)2η2 log 4 1\nη )\n(A.138)\nThat is there exist some C̃3 = Õ(1) so that ‖wt − w̃t‖ ≤ C̃3(µ1 + µ2)η log2 1η Therefore, conditioned on event Kt−1∩Et−1, we have proved that if choose µ3 > C̃3(µ1+µ2), then event {‖wt − w̃t‖ ≥ µ3η log2 1η} ⊂ {‖Γt‖ ≥ µ2η log 2 1 η }. Then, combined this fact with Eq.(A.129), Eq.(A.135), we have proved:\nP ( Et−1 ∩ Et ) ≤ Õ(η3) (A.139)\nBecause P (E0) = 0, and T ≤ Õ( 1η ), we have P (ET ) ≤ Õ(η2), which concludes the proof.\nThese two lemmas allow us to prove the result when the initial point is very close to a saddle point.\nProof of Lemma A.13. Combine Talyor expansion Eq.A.73 with Lemma A.14, Lemma A.15, we prove this Lemma by the same argument as in the proof of Lemma A.3.\nFinally the main theorem follows.\n194\nProof of Theorem A.5. By Lemma A.12, Lemma A.13, and Lemma A.6, with the same argument as in the proof Theorem A.1, we easily concludes this proof."
    }, {
      "heading" : "A.3 Detailed Proofs for Section 2.3",
      "text" : "In this section we show two optimization problems (2.9) and (2.11) satisfy the (α, γ, ǫ, δ)strict saddle propery."
    }, {
      "heading" : "A.3.1 Warm Up: Maximum Eigenvalue Formulation",
      "text" : "Recall that we are trying to solve the optimization (2.9), which we restate here.\nmax T (u, u, u, u), (A.140) ‖u‖2 = 1.\nHere the tensor T has orthogonal decomposition T = ∑d\ni=1 a ⊗4 i . We first do a change of\ncoordinates to work in the coordinate system specified by (ai)’s (this does not change the dynamics of the algorithm). In particular, let u = ∑d\ni=1 xiai (where x ∈ Rd), then we can see\nT (u, u, u, u) = ∑d\ni=1 x 4 i . Therefore let f(x) = −‖x‖44, the optimization problem is equivalent\nto\nmin f(x) (A.141)\ns.t. ‖x‖22 = 1\nThis is a constrained optimization, so we apply the framework developed in Section 2.2.3.\n195\nLet c(x) = ‖x‖22 − 1. We first compute the Lagrangian\nL(x, λ) = f(x)− λc(x) = −‖x‖44 − λ(‖x‖22 − 1). (A.142)\nSince there is only one constraint, and the gradient when ‖x‖ = 1 always have norm 2, we know the set of constraints satisfy 2-RLICQ. In particular, we can compute the correct value of Lagrangian multiplier λ,\nλ∗(x) = argmin λ ‖∇xL(x, λ)‖ = argmin λ\nd∑\ni=1\n(2x3i + λxi) 2 = −2‖x‖44 (A.143)\nTherefore, the gradient in the tangent space is equal to\nχ(x) = ∇xL(x, λ)|(x,λ∗(x)) = ∇f(x)− λ∗(x)∇c(x)\n= −4(x31, · · · , x3d)T − 2λ∗(x)(x1, · · · , xd)T = 4 ( (x21 − ‖x‖44)x1, · · · , (x2d − ‖x‖44)xd ) (A.144)\nThe second-order partial derivative of Lagrangian is equal to\nM(x) = ∇2xxL(x, λ)|(x,λ∗(x)) = ∇2f(x)− λ∗(x)∇2c(x)\n= −12diag(x21, · · · , x2d)− 2λ∗(x)Id = −12diag(x21, · · · , x2d) + 4‖x‖44Id (A.145)\n196\nSince the variable x has bounded norm, and the function is a polynomial, it’s clear that the function itself is bounded and all its derivatives are bounded. Moreover, all the derivatives of the constraint are bounded. We summarize this in the following lemma.\nLemma A.16. The objective function (2.9) is bounded by 1, its p-th order derivative is bounded by O( √ d) for p = 1, 2, 3. The constraint’s p-th order derivative is bounded by 2, for p = 1, 2, 3.\nTherefore the function satisfy all the smoothness condition we need. Finally we show the gradient and Hessian of Lagrangian satisfy the (α, γ, ǫ, δ)-strict saddle property. Note that we did not try to optimize the dependency with respect to d.\nTheorem A.7. The only local minima of optimization problem (2.9) are ±ai (i ∈ [d]). Further it satisfy (α, γ, ǫ, δ)-strict saddle for γ = 7/d, α = 3 and ǫ, δ = 1/poly(d).\nIn order to prove this theorem, we consider the transformed version Eq.A.141. We first need following two lemma for points around saddle point and local minimum respectively. We choose\nǫ0 = (10d) −4, ǫ = 4ǫ20, δ = 2dǫ0, S(x) = {i | |xi| > ǫ0} (A.146)\nWhere by intuition, S(x) is the set of coordinates whose value is relative large.\nLemma A.17. Under the choice of parameters in Eq.(A.146), suppose ‖χ(x)‖ ≤ ǫ, and |S(x)| ≥ 2. Then, there exists v̂ ∈ T (x) and ‖v̂‖ = 1, so that v̂TM(x)v̂ ≤ −7/d.\nProof. Suppose |S(x)| = p, and 2 ≤ p ≤ d. Since ‖χ(x)‖ ≤ ǫ = 4ǫ20, by Eq.(A.144), we have for each i ∈ [d], |[χ(x)]i| = 4|(x2i − ‖x‖44)xi| ≤ 4ǫ20. Therefore, we have:\n∀i ∈ S(x), |x2i − ‖x‖44| ≤ ǫ0 (A.147)\n197\nand thus:\n|‖x‖44 − 1\np | = |‖x‖44 −\n1 p\n∑\ni\nx2i |\n≤|‖x‖44 − 1\np\n∑\ni∈S(x) x2i |+ |\n1 p\n∑\ni∈[d]−S(x) x2i | ≤ ǫ0 +\nd− p p ǫ20 ≤ 2ǫ0 (A.148)\nCombined with Eq.A.147, this means:\n∀i ∈ S(x), |x2i − 1\np | ≤ 3ǫ0 (A.149)\nBecause of symmetry, WLOG we assume S(x) = {1, · · · , p}. Since |S(x)| ≥ 2, we can pick v̂ = (a, b, 0, · · · , 0). Here a > 0, b < 0, and a2 + b2 = 1. We pick a such that ax1 + bx2 = 0. The solution is the intersection of a radius 1 circle and a line which passes (0, 0), which always exists. For this v̂, we know ‖v̂‖ = 1, and v̂Tx = 0 thus v̂ ∈ T (x). We have:\nv̂TM(x)v̂ = −(12x21 + 4‖x‖44)a2 − (12x22 + 4‖x‖44)b2\n=− 8x21a2 − 8x22b2 − 4(x21 − ‖x‖44))a2 − 4(x22 − ‖x‖44))b2 ≤− 8 p + 24ǫ0 + 4ǫ0 ≤ −7/d (A.150)\nWhich finishes the proof.\nLemma A.18. Under the choice of parameters in Eq.(A.146), suppose ‖χ(x)‖ ≤ ǫ, and |S(x)| = 1. Then, there is a local minimum x⋆ such that ‖x− x⋆‖ ≤ δ, and for all x′ in the 2δ neighborhood of x⋆, we have v̂TM(x′)v̂ ≥ 3 for all v̂ ∈ T (x′), ‖v̂‖ = 1\nProof. WLOG, we assume S(x) = {1}. Then, we immediately have for all i > 1, |xi| ≤ ǫ0, and thus:\n1 ≥ x21 = 1− ∑\ni>1\nx2i ≥ 1− dǫ20 (A.151)\n198\nTherefore x1 ≥ √ 1− dǫ20 or x1 ≤ − √ 1− dǫ20. Which means x1 is either close to 1 or close to −1. By symmetry, we know WLOG, we can assume the case x1 ≥ √ 1− dǫ20. Let e1 = (1, 0, · · · , 0), then we know:\n‖x− e1‖2 ≤ (x1 − 1)2 + ∑\ni>1\nx2i ≤ 2dǫ20 ≤ δ2 (A.152)\nNext, we show e1 is a local minimum. According to Eq.A.145, we know M(e1) is a diagonal matrix with 4 on the diagonals except for the first diagonal entry (which is equal to −8), since T (e1) = span{e2, · · · , ed}, we have:\nvTM(e1)v ≥ 4‖v‖2 > 0 for all v ∈ T (e1), v 6= 0 (A.153)\nWhich by Theorem A.4 means e1 is a local minimum.\nFinally, denote T1 = T (e1) be the tangent space of constraint manifold at e1. We know for all x′ in the 2δ neighborhood of e1, and for all v̂ ∈ T (x′), ‖v̂‖ = 1:\nv̂TM(x′)v̂ ≥v̂TM(e1)v̂ − |v̂TM(e1)v̂ − v̂TM(x′)v̂|\n=4‖PT1 v̂‖2 − 8‖PT c1 v̂‖2 − ‖M(e1)−M(x′)‖‖v̂‖2 =4− 12‖PT c1 v̂‖2 − ‖M(e1)−M(x′)‖ (A.154)\nBy lemma A.7, we know ‖PT c1 v̂‖2 ≤ ‖x′ − e1‖2 ≤ 4δ2. By Eq.(A.145), we have:\n‖M(e1)−M(x′)‖ ≤ ‖M(e1)−M(x′)‖ ≤ ∑\n(i,j)\n|[M(e1)]ij − [M(x′)]ij |\n≤ ∑\ni\n∣∣−12[e1]2i + 4‖e1‖44 − 12x2i + 4‖x‖44 ∣∣ ≤ 64dδ (A.155)\nIn conclusion, we have v̂TM(x′)v̂ ≥ 4− 48δ2 − 64dδ ≥ 3 which finishs the proof.\n199\nFinally, we are ready to prove Theorem A.7.\nProof of Theorem A.7. According to Lemma A.17 and Lemma A.18, we immediately know the optimization problem satisfies (α, γ, ǫ, δ)-strict saddle.\nThe only thing remains to show is that the only local minima of optimization problem (2.9) are ±ai (i ∈ [d]). Which is equivalent to show that the only local minima of the transformed problem is ±ei (i ∈ [d]), where ei = (0, · · · , 0, 1, 0, · · · , 0), where 1 is on i-th coordinate.\nBy investigating the proof of Lemma A.17 and Lemma A.18, we know these two lemmas actually hold for any small enough choice of ǫ0 satisfying ǫ0 ≤ (10d)−4, by pushing ǫ0 → 0, we know for any point satisfying |χ(x)| ≤ ǫ → 0, if it is close to some local minimum, it must satisfy 1 = |S(x)| → supp(x). Therefore, we know the only possible local minima are ±ei (i ∈ [d]). In Lemma A.18, we proved e1 is local minimum, by symmetry, we finishes the proof."
    }, {
      "heading" : "A.3.2 New Formulation",
      "text" : "In this section we consider our new formulation (2.11). We first restate the optimization problem here:\nmin ∑\ni 6=j T (u(i), u(i), u(j), u(j)), (A.156)\n∀i ‖u(i)‖2 = 1.\nNote that we changed the notation for the variables from ui to u (i), because in later proofs we will often refer to the particular coordinates of these vectors.\n200\nSimilar to the previous section, we perform a change of basis. The effect is equivalent to making ai’s equal to basis vectors ei (and hence the tensor is equal to T = ∑d i=1 e ⊗4 i . After the transformation the equations become\nmin ∑\n(i,j):i 6=j h(u(i), u(j)) (A.157)\ns.t. ‖u(i)‖2 = 1 ∀i ∈ [d]\nHere h(u(i), u(j)) = ∑d\nk=1(u (i) k u (j) k ) 2, (i, j) ∈ [d]2. We divided the objective function by 2 to\nsimplify the calculation.\nLet U ∈ Rd2 be the concatenation of {u(i)} such that Uij = u(i)j . Let ci(U) = ‖u(i)‖2 − 1 and f(U) = 1 2 ∑ (i,j):i 6=j h(u (i), u(j)). We can then compute the Lagrangian\nL(U, λ) = f(U)− d∑\ni=1\nλici(U) = 1\n2\n∑\n(i,j):i 6=j h(u(i), u(j))−\nd∑\ni=1\nλi(‖u(i)‖2 − 1) (A.158)\nThe gradients of ci(U)’s are equal to (0, · · · , 0, 2u(i), 0, · · · , 0)T , all of these vectors are orthogonal to each other (because they have disjoint supports) and have norm 2. Therefore the set of constraints satisfy 2-RLICQ. We can then compute the Lagrangian multipiers λ∗ as follows\nλ∗(U) = argmin λ ‖∇UL(U, λ)‖ = argmin λ\n4 ∑\ni\n∑\nk\n( ∑\nj:j 6=i U2jkUik − λiUik)2 (A.159)\nwhich gives:\nλ∗i (U) = argmin λ\n∑\nk\n( ∑\nj:j 6=i U2jkUik − λiUik)2 =\n∑ j:j 6=i h(u(j), u(i)) (A.160)\n201\nTherefore, gradient in the tangent space is equal to\nχ(U) = ∇UL(U, λ)|(U,λ∗(U)) = ∇f(U)− n∑\ni=1\nλ∗i (U)∇ci(U). (A.161)\nThe gradient is a d2 dimensional vector (which can be viewed as a d×d matrix corresponding to entries of U), and we express this in a coordinate-by-coordinate way. For simplicity of later proof, denote:\nψik(U) = ∑\nj:j 6=i [U2jk − h(u(j), u(i))] =\n∑ j:j 6=i [U2jk − d∑ l=1 U2ilU 2 jl] (A.162)\nThen we have:\n[χ(U)]ik = 2( ∑\nj:j 6=i U2jk − λ∗i (U))Uik\n= 2Uik ∑\nj:j 6=i (U2jk − h(u(j), u(i)))\n= 2Uikψik(U) (A.163)\nSimilarly we can compute the second-order partial derivative of Lagrangian as\nM(U) = ∇2f(U)− d∑\ni=1\nλ∗i∇2ci(U). (A.164)\n202\nThe Hessian is a d2 × d2 matrix, we index it by 4 indices in [d]. The entries are summarized below:\n[M(U)]ik,i′k′ = ∂\n∂Ui′k′ [∇UL(U, λ)]ik ∣∣∣∣ (U,λ∗(U)) = ∂ ∂Ui′k′ [2( ∑ j:j 6=i U2jk − λ)Uik] ∣∣∣∣∣ (U,λ∗(U))\n=    2( ∑ j:j 6=iU 2 jk − λ∗i (U)) if k = k′, i = i′ 4Ui′kUik if k = k ′, i 6= i′\n0 if k 6= k′\n=    2ψik(U) if k = k ′, i = i′ 4Ui′kUik if k = k ′, i 6= i′ 0 if k 6= k′\n(A.165)\nSimilar to the previous case, it is easy to bound the function value and derivatives of the function and the constraints.\nLemma A.19. The objective function (2.11) and p-th order derivative are all bounded by poly(d) for p = 1, 2, 3. Each constraint’s p-th order derivative is bounded by 2, for p = 1, 2, 3.\nTherefore the function satisfy all the smoothness condition we need. Finally we show the gradient and Hessian of Lagrangian satisfy the (α, γ, ǫ, δ)-strict saddle property. Again we did not try to optimize the dependency with respect to d.\nTheorem A.8. Optimization problem (2.11) has exactly 2d · d! local minimum that corresponds to permutation and sign flips of ai’s. Further, it satisfy (α, γ, ǫ, δ)-strict saddle for α = 1 and γ, ǫ, δ = 1/poly(d).\nAgain, in order to prove this theorem, we follow the same strategy: we consider the transformed version Eq.A.157. and first prove the following lemmas for points around saddle\n203\npoint and local minimum respectively. We choose\nǫ0 = (10d) −6, ǫ = 2ǫ60, δ = 2dǫ0, γ = ǫ 4 0/4, S(u) = {k | |uk| > ǫ0} (A.166)\nWhere by intuition, S(u) is the set of coordinates whose value is relative large.\nLemma A.20. Under the choice of parameters in Eq.(A.166), suppose ‖χ(U)‖ ≤ ǫ, and there exists (i, j) ∈ [d]2 so that S(u(i)) ∩ S(u(j)) 6= ∅. Then, there exists v̂ ∈ T (U) and ‖v̂‖ = 1, so that v̂TM(U)v̂ ≤ −γ.\nProof. Again, since ‖χ(x)‖ ≤ ǫ = 2ǫ60, by Eq.(A.163), we have for each i ∈ [d], |[χ(x)]ik| = 2|Uikψik(U)| ≤ 2ǫ60. Therefore, have:\n∀k ∈ S(u(i)), |ψik(U)| ≤ ǫ50 (A.167)\nThen, we prove this lemma by dividing it into three cases. Note in order to prove that there exists v̂ ∈ T (U) and ‖v̂‖ = 1, so that v̂TM(U)v̂ ≤ −γ; it suffices to find a vector v ∈ T (U) and ‖v‖ ≤ 1, so that vTM(U)v ≤ −γ.\nCase 1 : |S(u(i))| ≥ 2, |S(u(j))| ≥ 2, and |S(u(i)) ∩S(u(j))| ≥ 2.\n204\nWLOG, assume {1, 2} ∈ S(u(i)) ∩S(u(j)), choose v to be vi1 = Ui24 , vi2 = −Ui14 , vj1 = Uj2 4 and vj2 = −Uj14 . All other entries of v are zero. Clearly v ∈ T (U), and ‖v‖ ≤ 1. On the other hand, we know M(U) restricted to these 4 coordinates (i1, i2, j1, j2) is\n  2ψi1(U) 0 4Ui1Uj1 0 0 2ψi2(U) 0 4Ui2Uj2 4Ui1Uj1 0 2ψj1(U) 0\n0 4Ui2Uj2 0 2ψj2(U)\n \n(A.168)\nBy Eq.(A.167), we know all diagonal entries are ≤ 2ǫ50.\nIf Ui1Uj1Ui2Uj2 is negative, we have the quadratic form:\nvTM(U)v =Ui1Uj1Ui2Uj2 + 1\n8 [U2i2ψi1(U) + U 2 i1ψi2(U) + U 2 j2ψj1(U) + U 2 j1ψj2(U)]\n≤− ǫ40 + ǫ50 ≤ − 1\n4 ǫ40 = −γ (A.169)\nIf Ui1Uj1Ui2Uj2 is positive we just swap the sign of the first two coordinates vi1 = −Ui22 , vi2 = Ui1 2 and the above argument would still holds.\nCase 2 : |S(u(i))| ≥ 2, |S(u(j))| ≥ 2, and |S(u(i)) ∩S(u(j))| = 1.\n205\nWLOG, assume {1, 2} ∈ S(u(i)) and {1, 3} ∈ S(u(j)), choose v to be vi1 = Ui24 , vi2 = −Ui14 , vj1 = Uj3 4 and vj3 = −Uj14 . All other entries of v are zero. Clearly v ∈ T (U) and ‖v‖ ≤ 1. On the other hand, we know M(U) restricted to these 4 coordinates (i1, i2, j1, j3) is\n  2ψi1(U) 0 4Ui1Uj1 0 0 2ψi2(U) 0 0 4Ui1Uj1 0 2ψj1(U) 0\n0 0 0 2ψj3(U)\n \n(A.170)\nBy Eq.(A.167), we know all diagonal entries are ≤ 2ǫ50. If Ui1Uj1Ui2Uj3 is negative, we have the quadratic form:\nvTM(U)v = 1\n2 Ui1Uj1Ui2Uj3 +\n1 8 [U2i2ψi1(U) + U 2 i1ψi2(U) + U 2 j3ψj1(U) + U 2 j1ψj3(U)]\n≤− 1 2 ǫ40 + ǫ 5 0 ≤ − 1 4 ǫ40 = −γ (A.171)\nIf Ui1Uj1Ui2Uj3 is positive we just swap the sign of the first two coordinates vi1 = −Ui22 , vi2 = Ui1 2 and the above argument would still holds.\nCase 3 : Either |S(u(i))| = 1 or |S(u(j))| = 1.\nWLOG, suppose |S(u(i))| = 1, and {1} = S(u(i)), we know:\n|(u(i)1 )2 − 1| ≤ (d− 1)ǫ20 (A.172)\nOn the other hand, since S(u(i)) ∩S(u(j)) 6= ∅, we have S(u(i)) ∩S(u(j)) = {1}, and thus:\n|ψj1(U)| = | ∑\ni′:i′ 6=j U2i′1 −\n∑\ni′:i′ 6=j h(u(i ′), u(j))| ≤ ǫ50 (A.173)\n206\nTherefore, we have:\n∑\ni′:i′ 6=j h(u(i\n′), u(j)) ≥ ∑\ni′:i′ 6=j U2i′1 − ǫ50 ≥ U2i1 − ǫ50 ≥ 1− dǫ20 (A.174)\nand\nd∑\nk=1\nψjk(U) = ∑\ni′:i′ 6=j\nd∑\nk=1\nU2i′k − d ∑\ni′:i′ 6=j h(u(i ′), u(j))\n≤d− 1− d(1− dǫ20) = −1 + d2ǫ20 (A.175)\nThus, we know, there must exist some k′ ∈ [d], so that ψjk′(U) ≤ −1d + dǫ20. This means we have “large” negative entry on the diagonal of M. Since |ψj1(U)| ≤ ǫ50, we know k′ 6= 1. WLOG, suppose k′ = 2, we have |ψj2(U)| > ǫ50, thus |Uj2| ≤ ǫ0.\nChoose v to be vj1 = Uj2 2 , vj2 = −Uj12 . All other entries of v are zero. Clearly v ∈ T (U) and ‖v‖ ≤ 1. On the other hand, we know M(U) restricted to these 2 coordinates (j1, j2) is\n  2ψj1(U) 0\n0 2ψj2(U)\n  (A.176)\nWe know |Uj1| > ǫ0, |Uj2| ≤ ǫ0, |ψj1(U)| ≤ ǫ50, and ψj2(U) ≤ −1d + dǫ20. Thus:\nvTM(U)v = 1\n2 ψj1(U)U\n2 j2 +\n1 2 ψj2(U)U 2 j1\n≤ǫ70 − ( 1\nd − dǫ20)ǫ20 ≤ −\n1\n2d ǫ20 ≤ −γ (A.177)\nSince by our choice of v, we have ‖v‖ ≤ 1, we can choose v̂ = v/‖v‖, and immediately have v̂ ∈ T (U) and ‖v̂‖ = 1, and v̂TM(U)v̂ ≤ −γ.\nLemma A.21. Under the choice of parameters in Eq.(A.166), suppose ‖χ(U)‖ ≤ ǫ, and for any (i, j) ∈ [d]2 we have S(u(i))∩S(u(j)) = ∅. Then, there is a local minimum U⋆ such that\n207\n‖U − U⋆‖ ≤ δ, and for all U ′ in the 2δ neighborhood of U⋆, we have v̂TM(U ′)v̂ ≥ 1 for all v̂ ∈ T (U ′), ‖v̂‖ = 1\nProof. WLOG, we assume S(u(i)) = {i} for i = 1, · · · , d. Then, we immediately have:\n|u(i)j | ≤ ǫ0, |(u(i)i )2 − 1| ≤ (d− 1)ǫ20, ∀(i, j) ∈ [d]2, j 6= i (A.178)\nThen u (i) i ≥ √ 1− dǫ20 or u(i)i ≤ − √ 1− dǫ20. Which means u(i)i is either close to 1 or close to −1. By symmetry, we know WLOG, we can assume the case u(i)i ≥ √ 1− dǫ20 for all i ∈ [d].\nLet V ∈ Rd2 be the concatenation of {e1, e2, · · · , ed}, then we have:\n‖U − V ‖2 = d∑\ni=1\n‖u(i) − ei‖2 ≤ 2d2ǫ20 ≤ δ2 (A.179)\nNext, we show V is a local minimum. According to Eq.A.165, we know M(V ) is a diagonal matrix with d2 entries:\n[M(V )]ik,ik = 2ψik(V ) = 2 ∑\nj:j 6=i [V 2jk −\nd∑\nl=1\nV 2ilV 2 jl] =    2 if i 6= k\n0 if i = k\n(A.180)\nWe know the unit vector in the direction that corresponds to [M(V )]ii,ii is not in the tangent space T (V ) for all i ∈ [d]. Therefore, for any v ∈ T (V ), we have\nvTM(e1)v ≥ 2‖v‖2 > 0 for all v ∈ T (V ), v 6= 0 (A.181)\nWhich by Theorem A.4 means V is a local minimum.\n208\nFinally, denote TV = T (V ) be the tangent space of constraint manifold at V . We know for all U ′ in the 2δ neighborhood of V , and for all v̂ ∈ T (x′), ‖v̂‖ = 1:\nv̂TM(U ′)v̂ ≥v̂TM(V )v̂ − |v̂TM(V )v̂ − v̂TM(U ′)v̂|\n=2‖PTV v̂‖2 − ‖M(V )−M(U ′)‖‖v̂‖2 =2− 2‖PT cV v̂‖ 2 − ‖M(V )−M(U ′)‖ (A.182)\nBy lemma A.7, we know ‖PT cV v̂‖2 ≤ ‖U ′ − V ‖2 ≤ 4δ2. By Eq.(A.165), we have:\n‖M(V )−M(U ′)‖ ≤ ‖M(V )−M(U ′)‖ ≤ ∑\n(i,j,k)\n|[M(V )]ik,jk − [M(U ′)]ik,jk| ≤ 100d3δ\n(A.183)\nIn conclusion, we have v̂TM(U ′)v̂ ≥ 2− 8δ2 − 100d3δ ≥ 1 which finishs the proof.\nFinally, we are ready to prove Theorem A.8.\nProof of Theorem A.8. Similarly, (α, γ, ǫ, δ)-strict saddleimmediately follows from Lemma A.20 and Lemma A.21.\nThe only thing remains to show is that Optimization problem (2.11) has exactly 2d · d! local minimum that corresponds to permutation and sign flips of ai’s. This can be easily proved by the same argument as in the proof of Theorem A.7."
    }, {
      "heading" : "A.3.3 Extending to Tensors of Different Order",
      "text" : "In this section we show how to generalize our algorithm to tensors of different orders. As a 8th order tensor (and more generally, 4pth order tensor for p ∈ N+) can always be considered\n209\nto be a 4th order tensor with components a⊗i ai ( a ⊗p i in general), so it is trivial to generalize our algorithm to 8th order or any 4pth order.\nFor tensors of other orders, we need to apply some transformation. As a concrete example, we show how to transform an orthogonal 3rd order tensor into an orthogonal 4th order tensor.\nWe first need to define a few notations. For third order tensors A,B ∈ Rd3 , we define (A ⊗ B)i1,i2,...,i6 = Ai1,i2,i3Bi4,i5,i6(i1, ..., i6 ∈ [d]). We also define the partial trace operation that maps a 6-th order tensor T ∈ Rd6 to a 4-th order tensor in Rd4 :\nptrace(T )i1,i2,i3,i4 = d∑\ni=1\nT (i, i1, i2, i, i3, i4).\nBasically, the operation views the tensor as a d3×d3 matrix with d2×d2 d×d matrix blocks, then takes the trace of each matrix block. Now given a random variable X ∈ Rd3 whose expectation is an orthogonal third order tensor, we can use these operations to construct an orthogonal 4-th order tensor:\nLemma A.22. Suppose the expectation of random variable X ∈ Rd3 is an orthogonal 3rd order tensor:\nE[X ] = d∑\ni=1\na⊗3i ,\nwhere ai’s are orthonormal vectors. Let X ′ be an independent sample of X, then we know\nE[ptrace(X ⊗X ′)] = d∑\ni=1\na⊗4i .\nIn other words, we can construct random samples whose expectation is equal to a 4-th order orthogonal tensor.\n210\nProof. Since ptrace and ⊗ are all linear operations, by linearity of expectation we know\nE[ptrace(X ⊗X ′)] = ptrace(E[X ]⊗ E[X ′]) = ptrace(( d∑\ni=1\na⊗3i )⊗ ( d∑\ni=1\na⊗3i )).\nWe can then expand out the product:\n( d∑\ni=1\na⊗3i )⊗ ( d∑\ni=1\na⊗3i ) = d∑\ni=1\na⊗6i + ∑\ni 6=j a⊗3i ⊗ a⊗3j .\nFor the diagonal terms, we know ptrace(a⊗i 6) = ‖ai‖2a⊗i 4 = a⊗i 4. For the i 6= j terms, we know ptrace(a⊗3i ⊗ a⊗3j ) = 〈ai, aj〉 a⊗i 2⊗ a⊗j 2 = 0 (since ai, aj are orthogonal). Therefore we must have\nptrace((\nd∑\ni=1\na⊗3i )⊗ ( d∑\ni=1\na⊗3i )) =\nd∑\ni=1\nptrace(a⊗6i ) + ∑\ni 6=j ptrace(a⊗3i ⊗ a⊗3j ) =\nd∑\ni=1\na⊗4i .\nThis gives the result.\nUsing similar operations we can easily convert all odd-order tensors into order 4p(p ∈ N+). For tensors of order 4p+ 2(p ∈ N+), we can simply apply the partial trace and get a tensor of order 4p with desirable properties. Therefore our results applies for all orders of tensors.\n211"
    }, {
      "heading" : "Appendix B",
      "text" : ""
    }, {
      "heading" : "Appendix for Applying Online Tensor",
      "text" : ""
    }, {
      "heading" : "Methods for Learning Latent Variable",
      "text" : ""
    }, {
      "heading" : "Models",
      "text" : ""
    }, {
      "heading" : "B.1 Stochastic Updates",
      "text" : "After obtaining the whitening matrix, we whiten the data G⊤x,A, G ⊤ x,B and G ⊤ x,C by linear operations to get ytA, y t B and y t C ∈ Rk:\nytA := 〈 G⊤x,A,W 〉 , ytB := 〈 ZBG ⊤ x,B,W 〉 , ytC := 〈 ZCG ⊤ x,C,W 〉 .\nwhere x ∈ X and t denotes the index of the online data.\n212\nThe stochastic gradient descent algorithm is obtained by taking the derivative of the loss function ∂L t(v) ∂vi :\n∂Lt(v)\n∂vi =θ\nk∑\nj=1\n〈vj , vi〉2 vj − (α0 + 1)(α0 + 2)\n2\n〈 vi, y t A 〉 〈 vi, y t B 〉 ytC − α20 〈 φti, ȳA 〉 〈 φti, ȳ t B 〉 ȳC\n+ α0(α0 + 1)\n2\n〈 φti, y t A 〉 〈 φti, y t B 〉 ȳC + α0(α0 + 1)\n2\n〈 φti, y t A 〉 〈 φti, ȳB 〉 yC\n+ α0(α0 + 1)\n2\n〈 φti, ȳA 〉 〈 φti, y t B 〉 yC\nfor i ∈ [k], where ytA, ytB and ytC are the online whitened data points as discussed in the whitening step and θ is a constant factor that we can set.\nThe iterative updating equation for the stochastic gradient update is given by\nφt+1i ← φti − βt ∂Lt\n∂vi ∣∣∣∣ φti\n(B.1)\nfor i ∈ [k], where βt is the learning rate, φti is the last iteration eigenvector and φti is the updated eigenvector. We update eigenvectors through\nφt+1i ← φti − θβt k∑\nj=1\n[〈 φtj, φ t i 〉2 φtj ] + shift[βt 〈 φti, y t A 〉 〈 φti, y t B 〉 ytC] (B.2)\nNow we shift the updating steps so that they correspond to the centered Dirichlet moment forms, i.e.,\nshift[βt 〈 φti, y t A 〉 〈 φti, y t B 〉 ytC] := β t (α0 + 1)(α0 + 2)\n2\n〈 φti, y t A 〉 〈 φti, y t B 〉 ytC\n+ βtα20 〈 φti, ȳA 〉 〈 φti, ȳB 〉 ȳC − βt α0(α0 + 1)\n2\n〈 φti, y t A 〉 〈 φti, y t B 〉 ȳC\n− βtα0(α0 + 1) 2\n〈 φti, y t A 〉 〈 φti, ȳB 〉 yC − βt α0(α0 + 1)\n2\n〈 φti, ȳA 〉 〈 φti, y t B 〉 yC, (B.3)\nwhere ȳA := Et[y t A] and similarly for ȳB and ȳC .\n213"
    }, {
      "heading" : "B.2 Proof of Algorithm Correctness",
      "text" : "We now prove the correctness of our algorithm.\nFirst, we compute M2 as just\nEx [ G̃⊤x,C ⊗ G̃⊤x,B|ΠA,ΠB,ΠC ]\nwhere we define\nG̃⊤x,B := Ex [ G⊤x,A ⊗G⊤x,C ∣∣∣∣ ΠA,ΠC ]( Ex [ G⊤x,B ⊗G⊤x,C ∣∣∣∣ ΠB,ΠC ])† G⊤x,B\nG̃⊤x,C := Ex [ G⊤x,A ⊗G⊤x,B ∣∣∣∣ ΠA,ΠB ]( Ex [ G⊤x,C ⊗G⊤x,B ∣∣∣∣ ΠB,ΠC ])† G⊤x,C.\nDefine FA as FA := Π ⊤ AP ⊤, we obtain M2 = E [ G⊤x,A ⊗G⊤x,A ] = Π⊤AP ⊤ ( Ex[πxπ ⊤ x ] ) PΠA = FA ( Ex[πxπ ⊤ x ] ) F⊤A . Note that P is the community connectivity matrix defined as P ∈ [0, 1]k×k. Now that we know M2, E [π 2 i ] = αi(αi+1) α0(α0+1) , and E [πiπj ] = αiαj α0(α0+1) ∀i 6= j, we can get the centered second order moments PairsCom as\nPairsCom := FA diag\n([ α1α1 + 1\nα0(α0 + 1) , . . . ,\nαkαk + 1\nα0(α0 + 1)\n]) F⊤A (B.4)\n= M2 − α0\nα0 + 1 FA\n( α̂α̂⊤ − diag ( α̂α̂⊤ )) F⊤A (B.5)\n= 1\nnX\n∑ x∈X ZCG ⊤ x,CGx,BZ ⊤ B − α0 α0 + 1 ( µAµ ⊤ A − diag ( µAµ ⊤ X→A )) (B.6)\nThus, our whitening matrix is computed. Now, our whitened tensor is T is given by\nT = T Com(W,W,W ) = 1 nX\n∑\nx\n[ (W⊤FAπ α0 x )⊗ (W⊤FAπα0x )⊗ (W⊤FAπα0x ) ] ,\n214\nwhere πα0x is the centered vector so that E[π α0 x ⊗ πα0x ⊗ πα0x ] is diagonal. We then apply the stochastic gradient descent technique to decompose the third order moment."
    }, {
      "heading" : "B.3 GPU Architecture",
      "text" : "The algorithm we propose is very amenable to parallelization and is scalable which makes it suitable to implement on processors with multiple cores in it. Our method consists of simple linear algebraic operations, thus enabling us to utilize Basic Linear Algebra Subprograms (BLAS) routines such as BLAS I (vector operations), BLAS II (matrix-vector operations), BLAS III (matrix-matrix operations), Singular Value Decomposition (SVD), and iterative operations such as stochastic gradient descent for tensor decomposition that can easily take advantage of Single Instruction Multiple Data (SIMD) hardware units present in the GPUs. As such, our method is amenable to parallelization and is ideal for GPU-based implementation.\nOverview of code design: From a higher level point of view, a typical GPU based computation is a three step process involving data transfer from CPU memory to GPU global memory, operations on the data now present in GPU memory and finally, the result transfer from the GPU memory back to the CPU memory. We use the CULA library for implementing the linear algebraic operations.\nGPU compute architecture: The GPUs achieve massive parallelism by having hundreds of homogeneous processing cores integrated on-chip. Massive replication of these cores provides the parallelism needed by the applications that run on the GPUs. These cores, for the Nvidia GPUs, are known as CUDA cores, where each core has fully pipelined floating-point and integer arithmetic logic units. In Nvidia’s Kepler architecture based GPUs, these CUDA cores are bunched together to form a Streaming Multiprocessor (SMX). These SMX units\n215\nact as the basic building block for Nvidia Kepler GPUs. Each GPU contains multiple SMX units where each SMX unit has 192 single-precision CUDA cores, 64 double-precision units, 32 special function units, and 32 load/store units for data movement between cores and memory.\nEach SMX has L1, shared memory and a read-only data cache that are common to all the CUDA cores in that SMX unit. Moreover, the programmer can choose between different configurations of the shared memory and L1 cache. Kepler GPUs also have an L2 cache memory of about 1.5MB that is common to all the on-chip SMXs. Apart from the above mentioned memories, Kepler based GPU cards come with a large DRAM memory, also known as the global memory, whose size is usually in gigabytes. This global memory is also visible to all the cores. The GPU cards usually do not exist as standalone devices. Rather they are part of a CPU based system, where the CPU and GPU interact with each other via PCI (or PCI Express) bus.\nIn order to program these massively parallel GPUs, Nvidia provides a framework known as CUDA that enables the developers to write programs in languages like C, C++, and Fortran etc. A CUDA program constitutes of functions called CUDA kernels that execute across many parallel software threads, where each thread runs on a CUDA core. Thus the GPU’s performance and scalability is exploited by the simple partitioning of the algorithm into fixed sized blocks of parallel threads that run on hundreds of CUDA cores. The threads running on an SMX can synchronize and cooperate with each other via the shared memory of that SMX unit and can access the Global memory. Note that the CUDA kernels are launched by the CPU but they get executed on the GPU. Thus compute architecture of the GPU requires CPU to initiate the CUDA kernels.\nCUDA enables the programming of Nvidia GPUs by exposing low level API. Apart from CUDA framework, Nvidia provides a wide variety of other tools and also supports third party libraries that can be used to program Nvidia GPUs. Since a major chunk of the\n216\nscientific computing algorithms is linear algebra based, it is not surprising that the standard linear algebraic solver libraries like BLAS and Linear Algebra PACKage (LAPACK) also have their equivalents for Nvidia GPUs in one form or another. Unlike CUDA APIs, such libraries expose APIs at a much higher-level and mask the architectural details of the underlying GPU hardware to some extent thus enabling relatively faster development time.\nConsidering the tradeoffs between the algorithm’s computational requirements, design flexibility, execution speed and development time, we choose CULA-Dense as our main implementation library. CULA-Dense provides GPU based implementations of the LAPACK and BLAS libraries for dense linear algebra and contains routines for systems solvers, singular value decompositions, and eigen-problems. Along with the rich set of functions that it offers, CULA provides the flexibility needed by the programmer to rapidly implement the algorithm while maintaining the performance. It hides most of the GPU architecture dependent programming details thus making it possible for rapid prototyping of GPU intensive routines.\nThe data transfers between the CPU memory and the GPU memory are usually explicitly initiated by CPU and are carried out via the PCI (or PCI Express) bus interconnecting the CPU and the GPU. The movement of data buffers between CPU and GPU is the most taxing in terms of time. The buffer transaction time is shown in the plot in Figure B.1. Newer GPUs, like Kepler based GPUs, also support useful features like GPU-GPU direct data transfers without CPU intervention.\n217\nCULA exposes two important interfaces for GPU programming namely, standard and device. Using the standard interface, the developer can program without worrying about the underlying architectural details of the GPU as the standard interface takes care of all the data movements, memory allocations in the GPU and synchronization issues. This however comes at a cost. For every standard interface function call the data is moved in and out of the GPU even if the output result of one operation is directly required by the subsequent operation. This unnecessary movement of intermediate data can dramatically impact the performance of the program. In order to avoid this, CULA provides the device interface. We use the device interface for STGD in which the programmer is responsible for data buffer allocations in the GPU memory, the required data movements between the CPU and GPU, and operates only on the data in the GPU. Thus the subroutines of the program that are iterative in nature are good candidates for device implementation.\nPre-processing and post-processing: The pre-processing involves matrices whose leading dimension is of the order of number of nodes. These are implemented using the CULA standard interface BLAS II and BLAS III routines.\nPre-processing requires SVD computations for the Moore-Penrose pseudoinverse calculations. We use CULA SVD routines since these SVD operations are carried out on matrices of\n218\nmoderate size. We further replaced the CULA SVD routines with more scalable SVD and pseudo inverse routines using random projections [66] to handle larger datasets such as DBLP dataset in our experiment.\nAfter STGD, the community membership matrix estimates are obtained using BLAS III routines provided by the CULA standard interface. The matrices are then used for hypothesis testing to evaluate the algorithm against the ground truth."
    }, {
      "heading" : "B.4 Results on Synthetic Datasets",
      "text" : "Homophily is an important factor in social interactions [119]; the term homophily refers to the tendency that actors in the same community interact more than across different communities. Therefore, we assume diagonal dominated community connectivity matrix P with diagonal elements equal to 0.9 and off-diagonal elements equal to 0.1. Note that P need neither be stochastic nor symmetric. Our algorithm allows for randomly generated community connectivity matrix P with support [0, 1]. In this way, we look at general directed social ties among communities.\nWe perform experiments for both the stochastic block model (α0 = 0) and the mixed membership model. For the mixed membership model, we set the concentration parameter α0 = 1.\n219\nWe note that the error is around 8%−14% and the running times are under a minute, when n ≤ 10000 and n ≫ k.\nThe results are given in Table B.1. We observe that more samples result in a more accurate recovery of memberships which matches intuition and theory. Overall, our learning algorithm performs better in the stochastic block model case than in the mixed membership model case although we note that the accuracy is quite high for practical purposes. Theoretically, this is expected since smaller concentration parameter α0 is easier for our algorithm to learn [8]. Also, our algorithm is scalable to an order of magnitude more in n as illustrated by experiments on real-world large-scale datasets."
    }, {
      "heading" : "B.5 Comparison of Error Scores",
      "text" : "Normalized Mutual Information (NMI) score [113] is another popular score which is defined differently for overlapping and non-overlapping community models. For non-overlapping block model, ground truth membership for node i is a discrete k-state categorical variable Πblock ∈ [k] and the estimated membership is a discrete k̂-state categorical variable Π̂block ∈ [k̂]. The empirical distribution of ground truth membership categorical variable Πblock is easy to obtain. Similarly is the empirical distribution of the estimated membership categorical variable Π̂block. NMI for block model is defined as\nNblock(Π̂block : Πblock) := H(Πblock) +H(Π̂block)−H(Πblock, Π̂block)(\nH(Πblock) +H(Π̂block) ) /2\n.\nThe NMI for overlapping communities is a binary vector instead of a categorical variable [113]. The ground truth membership for node i is a binary vector of length k, Πmix, while the estimated membership for node i is a binary vector of length k̂, Π̂mix. This notion\n220\ncoincides with one column of our membership matrices Π ∈ Rk×n and Π̂ ∈ Rk̂×n except that our membership matrices are stochastic. In other words, we consider all the nonzero entries of Π as 1’s, then each column of our Π is a sample for Πmix. The m-th entry of this binary vector is the realization of a random variable Πmixm = (Πmix)m, whose probability distribution is\nP (Πmixm = 1) = nm n , P (Πmixm = 0) = 1− nm n ,\nwhere nm is the number of nodes in community m. The same holds for Π̂mixm . The normalized conditional entropy between Πmix and Π̂mix is defined as\nH(Π̂mix|Πmix)norm := 1\nk\n∑ j∈[k] min i∈[k̂]\nH ( Π̂mixi|Πmixj )\nH(Πmixj ) (B.7)\nwhere Πmixj denotes the j th entry of Πmix and similarly for Π̂mixi. The NMI for overlapping community is\nNmix(Π̂mix : Πmix) := 1− 1\n2\n[ H(Πmix|Π̂mix)norm +H(Π̂mix|Πmix)norm ] .\nThere are two aspects in evaluating the error. The first aspect is the l1 norm error. According to Equation (B.7), the error function used in NMI score is H(Π̂mixi |Πmixj )\nH(Πmixj ) . NMI is\nnot suitable for evaluating recovery of different sized communities. In the special case of a pair of extremely sparse and dense membership vectors, depicted in Figure B.2, H(Πmixj ) is the same for both the dense and the sparse vectors since they are flipped versions of each other (0s flipped to 1s and vice versa). However, the smaller sized community (i.e. the sparser community vector), shown in red in Figure B.2, is significantly more difficult to recover than the larger sized community shown in blue in Figure B.2. Although this example is an extreme scenario that is not seen in practice, it justifies the drawbacks of the NMI.\n221\nThus, NMI is not suitable for evaluating recovery of different sized communities. In contrast,\nwhich is equal to P (Πmix2 = 1) = # of 1s in Π2 n . Similarly, P (Πmix1 = 1) = # of 1s in Π1 n which is equal to P (Πmix2 = 0) = # of 0s in Π2 n . Therefore, H(Πmix1) = H(Πmix2).\nour error function employs a normalized l1 norm error which penalizes more for larger sized communities than smaller ones.\nThe second aspect is the error induced by false pairings of estimated and ground-truth communities. NMI score selects only the closest estimated community through normalized conditional entropy minimization and it does not account for statistically significant dependence between an estimated community and multiple ground truth communities and vice-versa, and therefore it underestimates error. However, our error score does not limit to a matching between the estimated and ground truth communities: if an estimated community is found to have statistically significant correlation with multiple ground truth communities (as evaluated by the p-value), we penalize for the error over all such ground truth communities. Thus, our error score is a harsher measure of evaluation than NMI. This notion of “soft-matching” between ground-truth and estimated communities also enables validation of recovery of a combinatorial union of communities instead of single ones.\n222\nA number of other scores such as “separability”, “density”, “cohesiveness” and “clustering coefficient” [165] are non-statistical measures of faithful community recovery. The scores of [165] intrinsically aim to evaluate the level of clustering within a community. However our goal is to measure the accuracy of recovery of the communities and not how well-clustered the communities are.\nBanerjee and Langford [26] proposed an objective evaluation criterion for clustering which use classification performance as the evaluation measure. In contrast, we look at how well the method performs in recovering the hidden communities, and we are not evaluating predictive performance. Therefore, this measure is not used in our evaluation.\nFinally, we note that cophenetic correlation is another statistical score used for evaluating clustering methods, but note that it is only valid for hierarchical clustering and it is a measure of how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points [151]. Hence, it is not employed in this paper.\n223"
    }, {
      "heading" : "Appendix C",
      "text" : ""
    }, {
      "heading" : "Appendix for Dictionary Learning via",
      "text" : ""
    }, {
      "heading" : "Convolutional Tensor Method",
      "text" : ""
    }, {
      "heading" : "C.1 Cumulant Form",
      "text" : "In [12], it is proved that in ICA model, the cumulant of observation x is decomposed into multi-linear transform of a diagonal cumulant of h. Therefore, we aim to find the third order cumulant for input x.\nAs we know that the rth order moments for variable x is defined as\nµr := E[x r] ∈ Rn×n×n (C.1)\nLet us use [µ3]i,j,k to denote the (i, j, k) th entry of the third order moment. The relationship between 3th order cumulant κ3and 3 th order moment µ3is\n[κ3]i,j,k = [µ3]i,j,k − [µ2]i,j[µ1]k − [µ2]i,k[µ1]j − [µ2]j,k[µ1]i + 2[µ1]i[µ1]j [µ1]k (C.2)\n224\nTherefore the shift tensor is in this format: We know that the shift term\n[Z]a,b,c := E[x i a]E[x i bx i c] + E[xb]E[xax i c] + E[xc]E[xaxb]− 2E[xa]E[xb]E[xc], a, b, c ∈ [n]\n(C.3)\nIt is known from [12] that cumulant decomposition in the 3 order tensor format is\nE[x⊗ x⊗ x]− Z = ∑\nj∈[nL] λ∗jF∗j ⊗F∗j ⊗ F∗j (C.4)\nTherefore using the Khatri-Rao product property,\nunfold( ∑\nj∈[nL] λ∗jF∗j ⊗F∗j ⊗ F∗j ) =\n∑\nj∈[nL] λ∗jF∗j (F∗j ⊙ F∗j )⊤ = F∗Λ∗ (F∗ ⊙F∗)⊤ (C.5)\nTherefore the unfolded third order cumulant is decomposed as C3 = F∗Λ∗ (F∗ ⊙ F∗)⊤."
    }, {
      "heading" : "C.2 Proof for Main Theorem 4.1",
      "text" : "Our optimization problem is\nmin F\n‖C3−FΛ (H⊙ G)⊤‖2F s.t. blkl(F) = U ·Diag(FFT(fl))·UH, ‖fl‖22 = 1, ∀l ∈ [L], (C.6)\nwhere we denote D := Λ (H⊙ G)⊤ for simplicity. Therefore the objective is to minimize ‖C3 − FD‖2F . Let the SVD of D be D = PΣQ⊤. Since the Frobenius norm remains invariant under orthogonal transformations and full rank diagonal matrix [57], it is obtained\n225\nthat\n‖C3 − FD‖2F = ‖C3 −FPΣQ⊤‖2F = ‖C3QΣ† − FP‖2F = ‖C3QΣ†P⊤ − F‖2F (C.7)\nTherefore the optimization problem in (4.7) is equivalent to\nmin F\n‖C3((H⊙ G)⊤)†Λ†−F‖2F s.t. blkl(F) = U ·Diag(FFT(fl))·UH, ‖fl‖22 = 1, ∀l ∈ [L] (C.8)\nwhen (H⊙ G) and Λ are full column rank.\nThe full rank condition requires nL < n2 or L < n, and it is a reasonable assumption since otherwise the filter estimates are redundant. Since (C.8) has block constraints, it can be broken down in to solving L independent sub-problems\nmin fl\n∥∥blkl(M) · blkl(Λ)† − U ·Diag(FFT(fl)) · UH ∥∥2 F\ns.t. ‖fl‖22 = 1, ∀l ∈ [L]. (C.9)"
    }, {
      "heading" : "C.3 Parallel Inversion of Ψ",
      "text" : "We propose an efficient iterative algorithm to computeΨ† via block matrix inversion theorem[68].\nLemma C.1. (Parallel Inversion of row and column stacked diagonal matrix) Let JL = Ψ be partitioned into a block form:\nJL =   JL−1 O\nR blkLL(Ψ)\n  , (C.10)\n226\nwhere O :=   blk1L(Ψ) ...\nblkL−1L (Ψ)\n  , and R := [ blk1L−1(Ψ), . . . , blk L L−1(Ψ) ] . After inverting blkLL(Ψ)\nwhich takes O(1) time using O(n) processors, there inverse of Ψ is achieved by\nΨ† =   (JL−1 − OblkLL(Ψ) −1 R)−1 −(JL−1)−1O(blkLL(Ψ)− R(JL−1) −1 O)−1\n−blkLL(Ψ) −1 R(JL−1 − OblkLL(Ψ) −1 R)−1 (blkLL(Ψ)−R(JL−1) −1 O)−1\n \n(C.11)\nassuming that JL−1 and blkLLΨ are invertible.\nThis again requires inverting R, O and JL−1. Recursively applying these block matrix inversion theorem, the inversion problem is reduced to inverting L2 number of n by n diagonal matrices with additional matrix multiplications as indicated in equation (C.11).\nInverting a diagonal matrix results in another diagonal one, and the complexity of inverting n × n diagonal matrix is O(1) with O(n) processors. We can simultaneous invert all blocks. Therefore with O(nL2) processors, we invert all the diagonal matrices in O(1) time. The recursion takes L steps, for step i ∈ [L] matrix multiplication cost is O(lognL) with O(n2L/ log(nL)) processors. With L iteration, one achieves O(logn + logL) running time with O(n2L2/(logL+ logn)) processors.\n227"
    }, {
      "heading" : "Appendix D",
      "text" : ""
    }, {
      "heading" : "Appendix for Latent Tree Learning",
      "text" : "via Hierarchical Tensor Method"
    }, {
      "heading" : "D.1 Additivity of the Multivariate Information Dis-",
      "text" : "tance\nRecall that the additive information distance between nodes two categorical variables xi and xj was defined in [41]. We extend the notation of information distance to high dimensional variables via Definition 5.1 and present the proof of its additivity in Lemma 5.1 here."
    }, {
      "heading" : "Proof.",
      "text" : "E[xax ⊤ c ] = E[E[xax ⊤ c |xb]] = AE[xbx⊤b ]B⊤\nConsider three nodes a, b, c such that there are edges between a and b, and b and c. Let the A = E(xa|xb) and B = E(xc|xb). From Definition 5.1, we have, assuming that E(xax⊤a ),\n228\nE(xbx ⊤ b ) and E(xcx ⊤ c ) are full rank.\ndist(va, vc) = − log\nk∏ i=1 σi(E(xax ⊤ c ))\n√ det(E(xax⊤a )) det(E(xcx ⊤ c ))\ne−dist(va,vc) = det ( E(xax ⊤ a ) −1/2U⊤E(xax ⊤ c )V E(xcx ⊤ c ) −1/2)\nwhere k-SVD((E(xax ⊤ c )) = UΣV ⊤). Similarly,\ne−dist(va,vb) = det ( E(xax ⊤ a ) −1/2U⊤E(xax ⊤ b )WE(xbx ⊤ b ) −1/2) e−dist(vb,vc) = det ( E(xbx ⊤ b ) −1/2W⊤E(xbx ⊤ c )V E(xcx ⊤ c ) −1/2)\nwhere k-SVD((E(xax ⊤ b )) = UΣW ⊤) and k-SVD((E(xbx ⊤ c )) = WΣV ⊤).\nTherefore,\ne−(dist(a,b)+dist(b,c)) = det(E(xax ⊤ a ) −1/2U⊤E(xax ⊤ b )E(xbx ⊤ b ) −1/2−1/2 E(xbx ⊤ c )V E(xcx ⊤ c ) −1/2)\n= det(E(xax ⊤ a ) −1/2U⊤AE(xbx ⊤ b )B ⊤V E(xcx ⊤ c ) −1/2) = e−dist(va,vc)\nWe conclude that the multivariate information distance is additive. Note that E [ xax ⊤ b ] = E ( E ( xax ⊤ b |xb )) = E ( Axbx ⊤ b ) = AE(xbx ⊤ b ).\nWe note that when the second moments are not full rank, the above distance can be extended as follows:\ndist(va, vc) = − log\nk∏ i=1 σi(E(xax ⊤ c ))\n√ k∏\ni=1\nσi(E(xax⊤a )) k∏\ni=1\nσi(E(xcx⊤c ))\n.\n229"
    }, {
      "heading" : "D.2 Local Recursive Grouping",
      "text" : "The Local Recursive Grouping (LRG) algorithm is a local divide and conquer procedure for learning the structure and parameter of the latent tree (Algorithm 6). We perform recursive grouping simultaneously on the sub-trees of the MST. Each of the sub-tree consists of an internal node and its neighborhood nodes. We keep track of the internal nodes of the MST, and their neighbors. The resultant latent sub-trees after LRG can be merged easily to recover the final latent tree. Consider a pair of neighboring sub-trees in the MST. They have two common nodes (the internal nodes) which are neighbors on MST. Firstly we identify the path from one internal node to the other in the trees to be merged, then compute the multivariate information distances between the internal nodes and the introduced hidden nodes. We recover the path between the two internal nodes in the merged tree by inserting the hidden nodes closely to their surrogate node. Secondly, we merge all the leaves which are not in this path by attaching them to their parent. Hence, the recursive grouping can be done in parallel and we can recover the latent tree structure via this merging method.\nLemma D.1. If an observable node vj is the surrogate node of a hidden node hi, then the hidden node hi can be discovered using vj and the neighbors of vj in the MST.\nThis is due to the additive property of the multivariate information distance on the tree and the definition of a surrogate node. This observation is crucial for a completely local and parallel structure and parameter estimation. It is also easy to see that all internal nodes in the MST are surrogate nodes.\nAfter the parallel construction of the MST, we look at all the internal nodes Xint. For vi ∈ Xint, we denote the neighborhood of vi on MST as nbdsub(vi;MST) which is a small sub-tree. Note that the number of such sub-trees is equal to the number of internal nodes in MST.\n230\nFor any pair of sub-trees, nbdsub(vi;MST) and nbdsub(vj ;MST), there are two topological relationships, namely overlapping (i.e., when the sub-trees share at least one node in common) and non-overlapping (i.e., when the sub-trees do not share any nodes).\nSince we define a neighborhood centered at vi as only its immediate neighbors and itself on MST, the overlapping neighborhood pair nbdsub(vi;MST) and nbdsub(vj;MST) can only have conflicting paths, namely path(vi, vj;N i) and path(vi, vj ;N j), if vi and vj are neighbors in MST.\nWith this in mind, we locally estimate all the latent sub-trees, denoted as N i, by applying Recursive Grouping [41] in a parallel manner on nbdsub(vi;MST), ∀vi ∈ Xint. Note that the latent nodes automatically introduced by RG(vi) have vi as their surrogate. We update the tree structure by joining each level in a bottom-up manner. The testing of the relationship among nodes [41] uses the additive multivariate information distance metric (Appendix D.1) Φ(vi, vj; k) = dist(vi, vk)− dist(vi, vk) to decide whether the nodes vi and vj are parent-child or siblings. If they are siblings, they should be joined by a hidden parent. If they are parent and child, the child node is placed as a lower level node and we add the other node as the single parent node, which is then joined in the next level.\nFinally, for each internal edge of MST connecting two internal nodes vi and vj , we consider merging the latent sub-trees. In the example of two local estimated latent sub-trees in Figure 5.2, we illustrate the complete local merging algorithm that we propose."
    }, {
      "heading" : "D.3 Proof Sketch for Theorem 5.1",
      "text" : "We argue for the correctness of the method under exact moments. The sample complexity follows from the previous works. In order to clarify the proof ideas, we define the notion of surrogate node [41] as follows.\n231\nDefinition D.1. Surrogate node for hidden node hi on the latent tree T = (V, E) is defined as Sg(hi; T ) := arg min vj∈X dist(vi, vj).\nIn other words, the surrogate for a hidden node is an observable node which has the minimum multivariate information distance from the hidden node. See Figure 5.2(a), the surrogate node of h1, Sg(h1; T ), is v3, Sg(h2; T ) = Sg(h3; T ) = v5. Note that the notion of the surrogate node is only required for analysis, and our algorithm does not need to know this information.\nThe notion of surrogacy allows us to relate the constructed MST (over observed nodes) with the underlying latent tree. It can be easily shown that contracting the hidden nodes to their surrogates on latent tree leads to MST. Local recursive grouping procedure can be viewed as reversing these contractions, and hence, we obtain consistent local sub-trees.\nWe now argue the correctness of the structure union procedure, which merges the local subtrees. In each reconstructed sub-tree Ni, where vi is the group leader, the discovered hidden nodes {hi} form a surrogate relationship with vi, i.e. Sg(hi; T ) = vi. Our merging approach maintains these surrogate relationships. For example in Figure 5.2(d1,d2), we have the path v3−h1−v5 inN 3 and path v3−h3−h2−v5 inN 5. The resulting path is v3−h1−h3−h2−v5, as seen in Figure 5.2(e). We now argue why this is correct. As discussed before, Sg(h1; T ) = v3 and Sg(h2; T ) = Sg(h3; T ) = v5. When we merge the two subtrees, we want to preserve the paths from the group leaders to the added hidden nodes, and this ensures that the surrogate relationships are preserved in the resulting merged tree. Thus, we obtain a global consistent tree structure by merging the local structures. The correctness of parameter learning comes from the consistency of the tensor decomposition techniques and careful alignments of the hidden labels across different decompositions. Refer to Appendix D.4, D.7 for proof details and the sample complexity.\n232"
    }, {
      "heading" : "D.4 Proof of Correctness for LRG",
      "text" : "Definition D.2. A latent tree T≥3 is defined to be a minimal (or identifiable) latent tree if it satisfies that each latent variable has at least 3 neighbors.\nDefinition D.3. Surrogate node for hidden node hi in latent tree T = (V, E) is defined as\nSg(hi; T ) := arg min vj∈X dist(vi, vj).\nThere are some useful observations about the MST in [41] which we recall here.\nProperty D.1 (MST − surrogate neighborhood preservation). The surrogate nodes of any two neighboring nodes in E are also neighbors in the MST. I.e.,\n(hi, hj) ∈ E ⇒ (Sg(hi), Sg(hj)) ∈ MST.\nProperty D.2 (MST − surrogate consistency along path). If vj ∈ X and vh ∈ Sg−1(vj), then every node along the path connecting vj and vh belongs to the inverse surrogate set Sg−1(vj), i.e.,\nvi ∈ Sg−1(vj), ∀vi ∈ Path(vj, vh)\nif\nvh ∈ Sg−1(vj).\nThe MST properties observed connect the MST over observable nodes with the original latent tree T . We obtain MST by contracting all the latent nodes to its surrogate node.\n233\nGiven that the correctness of CLRG algorithm is proved in [41], we prove the equivalence between the CLRG and PLRG.\nLemma D.2. For any sub-tree pairs nbd[vi;MST] and nbd[vi;MST], there is at most one overlapping edge. The overlapping edge exists if and only if vi ∈ nbd(vj ;MST).\nThis is easy to see.\nLemma D.3. Denote the latent tree recovered from nbd[vi;MST] as N i and similarly for nbd[vj ;MST]. The inconsistency, if any, between N i and N j occurs in the overlapping path(vi, vj;N i) in and path(vi, vj;N j) after LRG implementation on each subtrees.\nWe now prove the correctness of LRG. Let us denote the latent tree resulting from merging a subset of small latent trees as TLRG(S), where S is the set of center of subtrees that are merged pair-wisely. CLRG algorithm in [41] implements the RG in a serial manner. Let us denote the latent tree learned at iteration i from CLRG is TCLRG(S), where S is the set of internal nodes visited by CLRG at current iteration . We prove the correctness of LRG by induction on the iterations.\nAt the initial step S = ∅: TCLRG = MST and TLRG = MST , thus TCLRG = TLRG.\nNow we assume that for the same set Si−1, TCLRG = TLRG is true for r = 1, . . . , i − 1. At iteration r = i where CLRG employs RG on the immediate neighborhood of node vi on TCLRG(Si−1), let us assume that Hi is the set of hidden nodes who are immediate neighbors of i− 1. The CLRG algorithm thus considers all the neighbors and implements the RG. We know that the surrogate nodes of every latent node in Hi belong to previously visited nodes Si−1. According to Property D.1 and D.2, if we contract all the hidden node neighbors to their surrogate nodes, CLRG thus is a RG on neighborhood of i on MST.\nAs for our LRG algorithm at this step, TLRG(Si) is the merging between TLRG(Si−1)and N i. The latent nodes whose surrogate node is j are introduced between the edge (i− 1, i). Now\n234\nthat we know N i is the RG output from immediate neighborhood of i on MST. Therefore, we proved that TCLRG(Si) = TLRG(Si)."
    }, {
      "heading" : "D.5 Cross Group Alignment Correction",
      "text" : "In order to achieve cross group alignments, tensor decompositions on two cross group triplets have to be computed. The first triplet is formed by three nodes: reference node in group 1, x1, non-reference node in group 1, x2, and reference node in group 2, x3. The second triplet is formed by three nodes as well: reference node in group 2, x3, non-reference node in group 2, x4 and reference node in group 1, x1. Let us use h1 to denote the parent node in group 1, and h2 the parent node in group 2.\nFrom Trip(x1, x2, x3), we obtain P (h1|x1) = Ã, P (x2|h1) = B and P (x3|h1) = P (x3|h2)P (h2|h1) = DE. From Trip(x3, x4, x1), we know P (x3|h2) = DΠ, P (x4|h2) = CΠ and P (h2|x1) = P (h2|h1)P (h1|x1) = ΠEÃ, where Π is a permutation matrix. We compute Π as Π =√ (ΠEÃ)(Ã)†(DE)†(DΠ) so that D = (DΠ)Π† is aligned with group 1. Thus, when all the parameters in the two groups are aligned by permute group 2 parameters using Π, thus the alignment is completed.\nSimilarly, the alignment correction can be done by calculating the permutation matrices while merging different threads.\nOverall, we merge the local structures and align the parameters from LRG locla sub-trees using Procedure 7 and 8.\n235"
    }, {
      "heading" : "D.6 Computational Complexity",
      "text" : "We recall some notations here: d is the observable node dimension, k is the hidden node dimension (k ≪ d), N is the number of samples, p is the number of observable nodes, and z is the number of non-zero elements in each sample.\nMultivariate information distance estimation involves sparse matrix multiplications to compute the pairwise second moments. Each observable node has a d×N sample matrix with z non-zeros per column. Computing the product x1x T 2 from a single sample for nodes 1 and 2 requires O(z) time and there are N such sample pair products leading to O(Nz) time. There are O(p2) node pairs and hence the degree of parallelism is O(p2). Next, we perform the k-rank SVD of each of these matrices. Each SVD takes O(d2k) time using classical methods. Using randomized methods [66], this can be improved to O(d+ k3).\nNext on, we construct the MST in O(log p) time per worker with p2 workers. The structure learning can be done in O(Γ3) per sub-tree and the local neighborhood of each node can be processed completely in parallel. We assume that the group sizes Γ are constant (the sizes are determined by the degree of nodes in the latent tree and homogeneity of parameters across different edges of the tree. The parameter estimation of each triplet of nodes consists of implicit stochastic updates involving products of k × k and d× k matrices. Note that we do not need to consider all possible triplets in groups but each node must be take care by a triplet and hence there are O(p) triplets. This leads to a factor of O(Γk3 + Γdk2) time per worker with p/Γ degree of parallelism.\nAt last, the merging step consists of products of k × k and d × k matrices for each edge in the latent tree leading to O(dk2) time per worker with p/Γ degree of parallelism.\n236"
    }, {
      "heading" : "D.7 Sample Complexity",
      "text" : "From [6], we recall the number of samples required for the recovery of the tree structure that is consistent with the ground truth (for a precise definition of consistency, refer to Definition 2 of [41])."
    }, {
      "heading" : "Lemma D.4. If",
      "text" : "N > 200k2B2t\n( γ2min γmax (1− distmax) )2 + 7kM2t γ2min γmax (1− distmax) , (D.1)\nthen with probability at least 1− η, proposed algorithm returns T̂ = T , where\nB := max xi,xj∈X\n{√ max{‖E[‖xi‖2xjx⊤j ]‖},max{‖E[‖xj‖2xix⊤i ]‖} } ,\nM := max xi∈X\n{‖xi‖} ,\nt := max xi,xj∈X\n{ 4 ln(4\nE[‖xi‖2‖xj‖2]− Tr(E[xix⊤j ]E[xjx⊤i ]) max{‖E[‖xj‖2xix⊤i ]‖, ‖E[‖xi‖2xjx⊤j ]‖} n/η)\n} .\nγmin := min {x1,x2}\n{σ ( E[x1x ⊤ 2 ] ) }\nγmax := max {x1,x2}\n{σ ( E[x1x ⊤ 2 ] ) }\nFrom [7], we recall the sample complexity for the faithful recovery of parameters via tensor decomposition methods.\nWe define ǫP to be the noise raised between empirical estimation of the second order moments and exact second order moments, and ǫT to be the noise raised between empirical estimation of the third order moments and the exact third order moments.\n237\nLemma D.5. Consider positive constants C, C ′, c and c′, the following holds. If\nǫP ≤ c λk λ1\nk , ǫT ≤ c′\nλkσ 3/2 k\nk N ≥ C ( log(k) + log ( log ( λ1σ 3/2 k\nǫT +\n1\nǫP\n)))\nL ≥ poly(k) log(1/δ),\nthen with probability at least 1 − δ, tensor decomposition returns (v̂i, λi) : i ∈ [k] satisfying, after appropriate reordering,\n‖v̂i − vi‖2 ≤ C ′ ( 1\nλi\n1\nσ2k ǫT + ( λ1 λi 1√ σk + 1 ) ǫP )\n|λ̂i − λi| ≤ C ′ ( 1\nσ 3/2 k\nǫT + λ1ǫP\n)\nfor all i ∈ [k].\nWe note that σ1 ≥ σ2 ≥ . . . σk > 0 are the non-zero singular values of the second order moments, λ1 ≥ λ2 ≥ . . . ≥ λk > 0 are the ground-truth eigenvalues of the third order moments, and vi are the corresponding eigenvectors for all i ∈ [k]."
    }, {
      "heading" : "D.8 Efficient SVD Using Sparsity and Dimensionality",
      "text" : "Reduction\nWithout loss of generality, we assume that a matrix whose SVD we aim to compute has no row or column which is fully zeros, since, if it does have zero entries, such row and columns can be dropped.\n238\nLet A ∈ Rn×n be the matrix to do SVD. Let Φ ∈ Rd×k̃, where k̃ = αk with α is a scalar, usually, in the range [2, 3]. For the ith row of Φ, if ∑ i |Φ|(i, :) 6= 0 and ∑ i |Φ|(:, i) 6= 0, then there is only one non-zero entry and that entry is uniformly chosen from [k̃]. If either ∑ i |Φ|(i, :) = 0 or ∑ i |Φ|(:, i) = 0, we leave that row blank. Let D ∈ Rd×d be a diagonal matrix with iid Rademacher entries, i.e., each non-zero entry is 1 or −1 with probability 1 2 . Now, our embedding matrix [46] is S = DΦ, i.e., we find AS and then proceed with the Nystrom [85] method. Unlike the usual Nystrom method [67] which uses a random matrix for computing the embedding, we improve upon this by using a sparse matrix for the embedding since the sparsity improves the running time and the memory requirements of the algorithm.\n239"
    }, {
      "heading" : "Appendix E",
      "text" : ""
    }, {
      "heading" : "Appendix for Spatial Point Process",
      "text" : ""
    }, {
      "heading" : "Mixture model Learning",
      "text" : ""
    }, {
      "heading" : "E.1 Morphological Basis Extraction",
      "text" : "We aim to characterize the morphological basis for all cells with different size, orientation, expression profiles and spatial distribution. The traditional sparse coding introduces too many free parameters and is not suitable for compact morphological basis learning. We instead propose Gaussian prior convolutional sparse coding (GPCSC). The intuition for using convolution is due to the frequent replication of cells of similar shapes and the translation invariance property. Traditional sparse coding would learn both the shape of the cell and the location of the cell. But the convolutional sparse coding would only learn the shape here. We characterize cell spatial distribution via decoding the sparse activation map.\nTo formulate the problem formally: let I be the image observed, then the convolutional sparse coding model generates observed image I using filters (resembling cell shapes)F superposed\n240\nat locations indicated by the activation map M (whose sparsity pattern indicates cell spatial distribution and activation amplitude indicates gene expression profiles. )\nOur goals of segmenting cells, extracting cell basis, and estimating gene profiles and cell locations are reduced to this optimization learning problem:\nmin Fm,Mnm ∥∥∥∥∥ ∑\nn\nIn − k∑\nm=1\nFm ⋆ M n m ∥∥∥∥∥ 2\nF\n+ ∑\nn\n∑\nm\nλ ‖Mnm‖0 ,\ns.t. Fm(x, y) ≥ 0, ‖Fm‖2F = 1,M (n)m (x, y) ≥ 0. (E.1)\nwhere In is the nth image associated with the gene we are interested in with Dx ×Dy pixels, i.e., In ∈ RD×D.\nWe call the Fm ∈ Rd×d filter, where d is set to capture the local cell morphological information. The spatial coefficient for image In is denoted as H (n) m ∈ R(D−d+1)×(D−d+1) which represents the position of the filter Fm being active on image I n. More precisely, if Hnm(x, y) = 1, then Fm is active at I n(x : x+ d− 1, y : y + d− 1)."
    }, {
      "heading" : "E.1.1 Gaussian Prior Convolutional Sparse Coding",
      "text" : "The popular alternating approach between matching pursuit to learn activation map M and k-SVD to learn F is general applicable to any object detection problem in image processing. However, this approach causes inexact cell number estimation as filters with multi-modality (i.e., multiple cells) are learnt. We resolve this issue by proposing an Gaussian probability density function prior on the filters to guarantee single cell detection and achieve accurate cell number estimation. The support of M is also limited to the local maxima indicating cell centers. Note that our cell are not donut shaped, and it is reasonable to assume the darkest point being the cell center.\n241\nTherefore, we optimize over the objective min ‖∑n In − ∑ m Fm ⋆ M n m‖22+ ∑ n ∑ m λ ‖Mnm‖0 such that Fm are 2 −D Gaussian densities with priori set top 2 principal radius and orientation. Alternating Minimization is used to solving the optimization problem. If we define the residual as ∑ n I n − ∑n ∑ m F̂m ⋆ M̂ n m, the gradient of the objective reduced to an iterative approach of updating filters, compute residual, optimizing activation map based on residual, compute residual and updating filters again. It is easy to see that both ∂L ∂Fm (i, j) and ∂L ∂Hm (i, j) are convolution of the residual and the other variable rotated by angle π.\nE.1.2 Image Registration/Alignment\nA structure represents a neuronanatomical region of interest. Structures are grouped into ontologies and organized in a hierarchy or structure graph. We are interested in the somatosensory cortex area. So we use the affine transform from Allen Brain Institute [1, 115] to align all the in-situ hybridization images with the Atlas brain to extract the correct region.\n242"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "OF THE DISSERTATION<lb>Discovery of Latent Factors in High-dimensional Data Using Tensor Methods<lb>By<lb>Furong Huang<lb>Doctor of Philosophy in Electrical and Computer Engineering<lb>University of California, Irvine, 2016<lb>Assistant Professor Animashree Anandkumar, Chair Unsupervised learning aims at the discovery of hidden structure that drives the observations<lb>in the real world. It is essential for success in modern machine learning and artificial intel-<lb>ligence. Latent variable models are versatile in unsupervised learning and have applications<lb>in almost every domain, e.g., social network analysis, natural language processing, computer<lb>vision and computational biology. Training latent variable models is challenging due to the<lb>non-convexity of the likelihood objective function. An alternative method is based on the<lb>spectral decomposition of low order moment matrices and tensors. This versatile framework<lb>is guaranteed to estimate the correct model consistently. My thesis spans both theoretical<lb>analysis of tensor decomposition framework and practical implementation of various appli-<lb>cations. This thesis presents theoretical results on convergence to globally optimal solution of tensor<lb>decomposition using the stochastic gradient descent, despite non-convexity of the objective.<lb>This is the first work that gives global convergence guarantees for the stochastic gradient<lb>descent on non-convex functions with exponentially many local minima and saddle points. This thesis also presents large-scale deployment of spectral methods (matrix and tensor<lb>decomposition) carried out on CPU, GPU and Spark platforms. Dimensionality reduction<lb>techniques such as random projection are incorporated for a highly parallel and scalable<lb>xvi tensor decomposition algorithm. We obtain a gain in both accuracies and in running times<lb>by several orders of magnitude compared to the state-of-art variational methods. To solve real world problems, more advanced models and learning algorithms are proposed.<lb>After introducing tensor decomposition framework under latent Dirichlet allocation (LDA)<lb>model, this thesis discusses generalization of LDA model to mixed membership stochastic<lb>block model for learning hidden user commonalities or communities in social network, con-<lb>volutional dictionary model for learning phrase templates and word-sequence embeddings,<lb>hierarchical tensor decomposition and latent tree structure model for learning disease hierar-<lb>chy in healthcare analytics, and spatial point process mixture model for detecting cell types<lb>in neuroscience.",
    "creator" : "LaTeX with hyperref package"
  }
}