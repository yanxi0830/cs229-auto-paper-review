{
  "name" : "1607.00360.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A scaled Bregman theorem with applications",
    "authors" : [ "Richard Nock", "Aditya Krishna Menon", "Cheng Soon Ong" ],
    "emails" : [ "chengsoon.ong}@data61.csiro.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning."
    }, {
      "heading" : "1 Introduction: Bregman divergences as a reduction tool",
      "text" : "Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. In recent years, Bregman divergences have arisen in procedures for convex optimisation [Beck and Teboulle, 2003], online learning [Cesa-Bianchi and Lugosi, 2006, Chapter 11] clustering [Banerjee et al., 2005], matrix approximation [Dhillon and Tropp, 2008], class-probability estimation [Buja et al., 2005, Nock and Nielsen, 2009, Reid and Williamson, 2010, 2011], density ratio estimation [Sugiyama et al., 2012], boosting [Collins et al., 2002], variational inference [Hernández-Lobato et al., 2016], and computational geometry [Boissonnat et al., 2010]. Despite these being very different applications, many of these algorithms and their analyses basically rely on three beautiful analytic properties of Bregman divergences, properties that we summarize for differentiable scalar convex functions ϕ with derivative ϕ′, conjugate ϕ?, and divergence Dϕ:\n• the triangle equality: Dϕ(x‖y) +Dϕ(y‖z)−Dϕ(x‖z) = (ϕ′(z)− ϕ′(y))(x− y);\n• the dual symmetry property: Dϕ(x‖y) = Dϕ?(ϕ′(y)‖ϕ′(x));\n• the right-centroid (population minimizer) is the average: arg minµ E[Dϕ(X‖µ)] = E[X].\nCasting a problem as a Bregman minimisation allows one to employ these properties to simplify analysis; for example, by interpreting mirror descent as applying a particular Bregman regulariser, Beck and Teboulle [2003] relied on the triangle equality above to simplify its proof of convergence.\nAnother intriguing possibility is that one may derive reductions amongst learning problems by connecting their underlying Bregman minimisations. Menon and Ong [2016] recently established how (binary) density ratio estimation (DRE) can be exactly reduced to class-probability estimation (CPE). This was facilitated by interpreting CPE as a Bregman minimisation [Buja et al., 2005, Section 19], and a new property of Bregman divergences —\nar X\niv :1\n60 7.\n00 36\n0v 1\n[ cs\n.L G\n] 1\nJ ul\n2 01\n6\nMenon and Ong [2016, Lemma 2] showed that for any twice differentiable scalar convex ϕ, for g(x) = 1 + x and ϕ†(x) . = g(x) · ϕ(x/g(x)), g(x) ·Dϕ(x/g(x)‖y/g(y)) = Dϕ†(x‖y). (1) Since the binary class-probability function η(x) = Pr(Y = 1|X = x) is related to the class-conditional density ratio r(x) = Pr(X = x|Y = 1)/Pr(X = x|Y = −1) via Bayes’ rule as η(x) = r(x)/g(r(x)), any η̂ with small Dϕ(η‖η̂) implicitly produces an r̂ with low Dϕ†(r‖r̂) i.e. a good estimate of the density ratio. The Bregman property of Equation 1 thus establishes a reduction from DRE to CPE. Two natural questions arise from this analysis: can we generalise Equation 1 to other g(·), and if so, can we similarly relate other problems to each other?\nThis paper presents a new Bregman identity (Theorem 1), the scaled Bregman theorem, a significant generalisation of Menon and Ong [2016, Lemma 2]. It shows that general distortions Dϕ† – which are not necessarily convex, positive, bounded or symmetric – may be re-expressed as a Bregman divergence Dϕ computed over transformed data, where this transformation can be as simple as a projection or normalisation by a gauge, or more involved like the exponential map on lifted coordinates for a curved manifold. Interestingly, candidate distortions include geodesic distances on curved manifolds. Equivalently, Theorem 1 shows various distortions can be “reverse engineered” as Bregman divergences (despite appearing prima facie to be a very different object), and thus inherit their good properties. Hence, Bregman divergences can embed several distances in a different — and arguably less involved — way than the transformations known to date [Acharyya et al., 2013].\nAs with the aforementioned key properties of Bregman divergences, Theorem 1 has potentially wide applicability. We present three such novel applications (see Table 1) to vastly different problems:\n• a reduction of multiple density ratio estimation to multiclass-probability estimation (§3), generalising the results of Menon and Ong [2016] for the binary label case,\n• a projection-free yet norm-enforcing mirror gradient algorithm (enforced norms are those of mirrored vectors and of the offset) with guarantees for adaptive filtering (§4), and\n• a seeding approach for clustering on positively or negatively (constant) curved manifolds based on a popular seeding for flat manifolds and with the same approximation guarantees (§5).\nExperiments on each of these domains (§6) validate our analysis. The Supplementary Material details the proofs of all results, provides the experimental results in extenso and some additional (nascent) applications of the scaled Bregman theorem to exponential families and computational geometry."
    }, {
      "heading" : "2 Main result: the scaled Bregman theorem",
      "text" : "In the remaining, [k] .= {0, 1, ..., k} and [k]∗ .= {1, 2, ..., k} for k ∈ N. For any differentiable (but not necessarily convex) ϕ : X→ R, we define the Bregman “distortion” Dϕ as\nDϕ(x‖y) .= ϕ(x)− ϕ(y)− (x− y)>∇ϕ(y) . (2)\nWhen ϕ is convex, Dϕ is the familiar Bregman divergence with generator ϕ.\nWithout further ado, we present our main result.\nTheorem 1 Let, ϕ : X→ R be convex differentiable, and g : X→ R∗ be differentiable. Then,\ng(x) ·Dϕ ( x/g(x) ∥∥ y/g(y) ) = Dϕ† ( x ∥∥ y ) ,∀x,y ∈ X , (3)\nwhere ϕ†(x) .= g(x) · ϕ (x/g(x)) , (4)\nif and only if (i) g is affine on X, and/or (ii) for every z ∈ Xg .= {(1/g(x)) · x : x ∈ X},\nϕ (z) = z>∇ϕ(z) . (5)\nTable 2 presents some examples of (sometimes involved) triplets (Dϕ, Dϕ† , g) for which Equation 3 holds; related proofs are in Appendix C. If we fold g into Dϕ in the left hand-side of eq. (3), then Theorem 1 states a scaled isodistortion (sometimes it turns out to be equivalently an adaptive scaled isometry, see Appendix I) property between X and Xg . Because Dϕ is such an important object, we do not perform this folding and refer to Theorem 1 as the scaled Bregman theorem for short.\nRemark. If Xg is a vector space, ϕ satisfies Equation 5 if and only if it is positive homogeneous of degree 1 on Xg (i.e. ϕ(αz) = α · ϕ(z) for any α > 0) from Euler’s homogenous function theorem. When Xg is not a vector space, this only holds for α such that αz ∈ Xg as well. We thus call the gradient condition of Equation 5 “restricted positive homogeneity” for simplicity.\nRemark. Appendix D gives a “deep composition” extension of Theorem 1.\nFor the special case where X = R, and g(x) = 1 + x, Theorem 1 is exactly Menon and Ong [2016, Lemma 2] (c.f. Equation 1). We wish to highlight a few points with regard to our more general result. First, the “distortion” generator ϕ† may be1 non-convex, as the following illustrates.\nExample. Suppose ϕ(x) = 12‖x‖22 corresponds to the generator for squared Euclidean distance. Then, for g(x) = 1 + 1>x, we have ϕ†(x) = 12 · ‖x‖22 1+1>x , which is non-convex on X = R d.\nWhen ϕ† is non-convex, the right hand side in Equation 3 is an object that ostensibly bears only a superficial similarity to a Bregman divergence; it is somewhat remarkable that Theorem 1 shows this general “distortion” between a pair (x,y) to be entirely equivalent to a (scaling of a) Bregman divergence between some transformation of the points. Second, when g is linear, Equation 3 holds for any convex ϕ. (This was the case considered in Menon and Ong [2016].) When g is non-linear, however, ϕ must be chosen carefully so that (ϕ, g) satisfies the restricted homogeneity conditon2 of Equation 5. In general, given a convex ϕ, one can “reverse engineer” a suitable g to guarantee this conditon, as illustrated by the following example.\nExample. Suppose3 ϕ(x) = (1 + ‖x‖22)/2. Then, Equation 5 requires that ‖x‖22 = 1 for every x ∈ Xg , i.e. Xg is (a subset of) the unit sphere. This is afforded by the choice g(x) = ‖x‖2. Third, Theorem 1 is not merely a mathematical curiosity: we now show that it facilitates novel results in three very different domains, namely estimating multiclass density ratios, constrained online optimisation, and clustering data on a manifold with non-zero curvature. We discuss nascent applications to exponential families and computational geometry in Appendices E and F."
    }, {
      "heading" : "3 Multiclass density-ratio estimation via class-probability estimation",
      "text" : "Given samples from a number of densities, density ratio estimation concerns estimating the ratio between each density and some reference density. This has applications in the covariate shift problem wherein the train and test distributions over instances differ [Shimodaira, 2000]. Our first application of Theorem 1 is to show how density ratio estimation can be reduced to class-probability estimation [Buja et al., 2005, Reid and Williamson, 2010].\n1Evidently, ϕ† is convex iff g is non-negative, by Equation (3) and the fact that a function is convex iff its Bregman “distortion” is nonnegative [Boyd and Vandenberghe, 2004, Section 3.1.3].\n2We stress that this condition only needs to hold on Xg ⊆ X.; it would not be really interesting in general for ϕ to be homogeneous everywhere in its domain, since we would basically have ϕ† = ϕ.\n3The constant 1/2 added in ϕ does not change Dϕ, since a Bregman divergence is invariant to affine terms; removing this however would make the divergences Dϕ and Dϕ† differ by a constant.\nTo proceed, we fix notation. For some integer C ≥ 1, consider a distribution P(X,Y) over an (instance, label) space X × [C]. Let ({Pc}Cc=1,π) be densities giving P(X|Y = c) and P(Y = c) respectively, and M giving P(X) accordingly. Fix c∗ ∈ [C] a reference class, and suppose for simplicity that c∗ = C. Let π̃ ∈ 4C−1 such that π̃c . = πc/(1 − πC). Density ratio estimation [Sugiyama et al., 2012] concerns inferring the vector r(x) ∈ RC−1 of density ratios relative to C, with rc(x) .= P(X = x|Y = c)/P(X = x|Y = C) , while classprobability estimation [Buja et al., 2005] concerns inferring the vector η(x) ∈ RC−1 of class-probabilities, with ηc(x) . = P(Y = c|X = x)/π̃c . In both cases, we estimate the respective quantities given an iid sample S ∼ P(X,Y)N . The genesis of the reduction from density ratio to class-probability estimation is the fact that r(x) = (πC/(1− πC)) · η(x)/ηC(x). In practice one will only have an estimate η̂, typically derived by minimising a suitable loss on the given S [Williamson et al., 2014], with a canonical example being multiclass logistic regression. Given η̂, it is natural to estimate the density ratio via:\nr̂(x) = η̂(x)\nη̂C(x) . (6)\nWhile this estimate is intuitive, to establish a formal reduction we must relate the quality of r̂ to that of η̂. Since the minimisation of a suitable loss for class-probability estimation is equivalent to a Bregman minimisation [Buja et al., 2005, Section 19], [Williamson et al., 2014, Proposition 7], this is however immediate by Theorem 1, as shown below.\nLemma 2 Given a class-probability estimator η̂ : X → [0, 1]C−1, let the density ratio estimator r̂ be as per Equation 6. Then for any convex differentiable ϕ : [0, 1]C−1 → R,\nEX∼M [Dϕ(η(X)‖η̂(X))] = (1− πC) · EX∼PC [ Dϕ†(r(X)‖r̂(X)) ] (7)\nwhere ϕ† is as per Equation 4 with g(x) .= πC/(1− πC) + π̃>x .\nLemma 2 generalises Menon and Ong [2016, Proposition 3], which focussed on the binary case with π = 1/2. (See Appendix G for a review of that result.) Unpacking the Lemma, the LHS in Equation 7 represents the object minimised by some suitable loss for class-probability estimation. Since g is affine, we can use any convex, differentiable ϕ, and so can use any suitable class-probability loss to estimate η̂. Lemma 2 thus implies that producing η̂ by minimising any class-probability loss equivalently produces an r̂ as per Equation 6 that minimises a Bregman divergence to the true r. Thus, Theorem 1 provides a reduction from density ratio to multiclass probability estimation.\nWe now detail two applications where g(·) is no longer affine, and ϕ must be chosen more carefully."
    }, {
      "heading" : "4 Dual norm mirror descent: projection-free online learning on Lp balls",
      "text" : "A substantial amount of work in the intersection of machine learning and convex optimisation has focused on constrained optimisation within a ball [Shalev-Shwartz et al., 2007, Duchi et al., 2008]. This optimisation is typically via projection operators that can be expensive to compute [Hazan and Kale, 2012, Jaggi, 2013]. We now show that gauge functions can be used as an inexpensive alternative, and that Theorem 1 easily yields guarantees for this procedure in online learning.\nWe consider the adaptive filtering problem, closely related to the online least squares problem with linear predictors [Cesa-Bianchi and Lugosi, 2006, Chapter 11]. Here, over a sequence of T rounds, we observe some xt ∈ X. We must the predict a target value ŷt = w>t−1xt using our current weight vectorwt−1. The true target yt = u\n>xt + t is then revealed, where t is some unknown noise, and we may update our weight to wt. Our goal is to minimise the regret of the sequence {wt}Tt=0,\nR(w1:T |u) .= T∑\nt=1\n( u>xt −w>t−1xt )2 − T∑\nt=1\n( u>xt − yt )2 . (8)\nLet q ∈ (1, 2] and p be such that 1/p + 1/q = 1. For ϕ .= 12 · ‖x‖2q and loss `t(w) = 12 · (yt − w>xt)2, the p-LMS algorithm [Kivinen et al., 2006] employs the stochastic mirror gradient updates\nwt . = argmin w ηt · `t(w) +Dϕ(w‖wt−1) = (∇ϕ)−1 (∇ϕ(wt−1)− ηt · ∇`t) , (9)\nwhere ηt is a learning rate to be specified by the user. Kivinen et al. [2006, Theorem 2] shows that for appropriate ηt, one has R(w1:T |u) ≤ (p− 1) ·maxx∈X ‖x‖2p · ‖u‖2q . The p-LMS updates do not provide any explicit control on ‖wt‖, i.e. there is no regularisation. Experiments (§6) suggest that leaving ‖wt‖ uncontrolled may not be a good idea as the increase of the norm sometimes prevents (significant) updates (9). Also, the wide success of regularisation in machine learning calls for regularised variants that retain the regret guarantees and computational efficiency of p-LMS. (Adding a projection step to Equation 9 would not achieve both.) We now do just this. For fixed W > 0, let ϕ .= (1/2)(W 2 + ‖x‖2q), a translation of that used in p-LMS. Invoking Theorem 1 with the admissible gq(x) = ||x||q/W yields ϕ† .= ϕ†q = W‖x‖q (see Table 2). Using the fact that Lp and Lq norms are dual of each other, we replace Equation 9 by:\nwt . = ∇ϕ†p ( ∇ϕ†q(wt−1)− ηt · ∇`t ) . (10)\nSee Lemma 6 of the Appendix for the simple forms of ∇ϕ†{p,q}. We call update (10) the dual norm p-LMS (DN-pLMS) algorithm, noting that the dual refers to the polar transform of the norm, and g stems from a gauge normalization for Bq(W ), the closed Lq ball with radiusW > 0. Namely, we have γGAU(x) = W/‖x‖q = g(x)−1 for the gauge γGAU(x) . = sup{z ≥ 0 : z · x ∈ Bq(W )}, so that ϕ†q implicitly performs gauge normalisation of the data. This update is no more computationally expensive than Equation 9 — we simply need to compute the p- and q-norms of appropriate terms — but, crucially, automatically constrains the norms of wt and its image by∇ϕ†q .\nLemma 3 For the update in Equation 10, ‖wt‖q = ‖∇ϕ†q(wt)‖p = W, ∀t > 0.\nLemma 3 is remarkable, since nowhere in Equation 10 do we project onto the Lq ball. Nonetheless, for the DN-pLMS updates to be principled, we need a similar regret guarantee to the original p-LMS. Fortunately, this may be done using Theorem 1 to exploit the original proof of Kivinen et al. [2006]. For any u ∈ Rd, define the q-normalised regret of {wt}Tt=0 by\nRq(w1:T |u) .= T∑\nt=1\n( (1/gq(u)) · u>xt −w>t−1xt )2 − T∑\nt=1\n( (1/gq(u)) · u>xt − yt )2 . (11)\nWe have the following bound on Rq for the DN-pLMS updates. (We cannot expect a bound on the unnormalised R(·) of Equation 8, since by Lemma 3 we can only compete against norm W vectors.)\nLemma 4 Pick any u ∈ Rd, p, q satisfying 1/p + 1/q = 1 and p > 2, and W > 0. Suppose ‖xt‖p ≤ Xp and |yt| ≤ Y,∀t ≤ T . Let {wt} be as per Equation 10, using learning rate\nηt . = γt ·\nW\n4(p− 1) max{W,Xp}XpW + |yt −w>t−1xt|Xp , (12)\nfor any desired γt ∈ [1/2, 1]. Then,\nRq(w1:T |u) ≤ 4(p− 1)X2pW 2 + (16p− 8) max{W,Xp}X2pW + 8Y X2p . (13)\nSeveral remarks can be made. First, the bound depends on the maximal signal value Y , but this is the maximal signal in the observed sequence, so it may not be very large in practice; if it is comparable to W , then our bound is looser than Kivinen et al. [2006] by just a constant factor. Second, the learning rate is adaptive in the sense that its choice depends on the last mistake made. There is a nice way to represent the “offset” vector ηt · ∇`t in eq. (10), since we have, for Q′′ .= 4(p− 1) max{W,Xp}XpW ,\nηt · ∇`t = W · |yt −w>t−1xt|Xp\nQ′′ + |yt −w>t−1xt|Xp · sign(yt −w>t−1xt) ·\n( 1 Xp · x ) , (14)\nso the Lp norm of the offset is actually equal to W ·Q, where Q ∈ [0, 1] is all the smaller as the vector w. gets better. Hence, the update in eq. (10) controls in fact all norms (that of w., its image by∇ϕ†q and the offset). Third, because of the normalisation of u, the bound actually does not depend on u, but on the radius W chosen for the Lq ball."
    }, {
      "heading" : "5 Clustering on a manifold via data transformation",
      "text" : "Our final application can be related to two problems that have received a steadily growing interest over the past decade in unsupervised machine learning: clustering on a non-linear manifold [Dhillon and Modha, 2001], and subspace custering [Vidal, 2011]. We consider two fundamental manifolds investigated by Galperin [1993] to compute centers of mass from relativistic theory: the sphere Sd and the hyperboloid Hd, the former being of positive curvature, and the latter of negative curvature. Applications involving these specific manifolds are numerous in text processing, computer vision, geometric modelling, computer graphics, to name a few [Buss and Fillmore, 2001, Dhillon and Modha, 2001, Endo and Miyamoto, 2015, Kuang et al., 2014, Rong et al., 2010, Shahani et al., 2015, Straub et al., 2015a,b,c, 2014]. We emphasize the fact that the clustering problem has significant practical impact for d as small as 2 in computer vision [Straub et al., 2014].\nThe problem is non-trivial for two separate reasons. First, the ambient space, i.e. the space of registration of the input data, is often implicitly Euclidean and therefore not the manifold [Dhillon and Modha, 2001]: if the mapping to the manifold is not carefully done, then geodesic distances measured on the manifold may be inconsistent with respect to the ambient space. Second, the fact that the manifold has non-zero curvature essentially prevents the direct use of Euclidean optimization algorithms [Zhang and Sra, 2016] — put simply, the average of two points\nthat belong to a manifold does not necessarily belong to the manifold, so we have to be careful on how to compute centroids for hard clustering [Galperin, 1993, Nock et al., 2016, Rong et al., 2010, Schwander and Nielsen, 2013].\nWhat we show now is that Riemannian manifolds with constant sectional curvature may be clustered with the k-means++ seeding for flat manifolds [Arthur and Vassilvitskii, 2007], without even touching a line of the algorithm. To formalise the problem, we need three key components of Riemannian geometry: tangent planes, exponential map and geodesics [Amari and Nagaoka, 2000]. We assume that the ambient space is a tangent plane to the manifold M, which conveniently makes it look Euclidean (see Figure 1). The point of tangency is called q, and the tangent plane TqM. The exponential map, expq : TqM→M, performs a distance preserving mapping: the geodesic length between q and expq(x) in M is the same as the Euclidean length between q and x in TqM. Our clustering objective is to find C .= {c1, c2, ...ck} ⊂M such that Drec(S : C) = infC′⊂M,|C′|=kDrec(S,C′), with\nDrec(S,C) . = ∑ i∈[m]∗ minj∈[k]∗ Drec(expq(xi), cj) , (15)\nwhere Drec is a reconstruction loss, a function of the geodesic distance between expq(xi) and cj . We use two loss functions defined from Galperin [1993] and used in machine learning for more than a decade [Dhillon and Modha, 2001]:\nR+ 3 Drec(y, c) .= {\n1− cosDG(y, c) for M = Sd coshDG(y, c)− 1 for M = Hd . (16)\nHere, DG(y, c) is the corresponding geodesic distance of M between y and c. Figure 1 shows that Drec(y, c) is the orthogonal distance between TcM and y when M = Sd. The solution to the clustering problem in eq. (15) is therefore the one that minimizes the error between tangent planes defined at the centroids, and points on the manifold.\nIt turns out that both distances in 16 can be engineered as Bregman divergences via Theorem 1, as seen in Table 2. Furthermore, they imply the same ϕ, which is just the generator of Mahalanobis distortion, but a different g. The construction involves a third party, a lifting map (lift(.)) that increases the dimension by one. The Sphere lifting map Rd 3 x 7→ xS ∈ Rd+1 is indicated in Table 3 (left). The new coordinate depends on the norm of x. The Hyperbolic lifting map, Rd 3 x 7→ xH ∈ Rd × C, involves a pure imaginary additional coordinate, is indicated in in Table 3 (right, with a slight abuse of notation) and Figure 1. Both xS and xH live on a d-dimensional manifold, depicted in Figure 1. When they are scaled by the corresponding g.(.), they happen to be mapped to Sd or Hd, respectively, by what happens to be the manifold’s exponential map for the original x (see Appendix C).\nTheorem 1 is interesting in this case because ϕ corresponds to a Mahalanobis distortion: this shows that kmeans++ seeding [Arthur and Vassilvitskii, 2007, Nock et al., 2008] can be used directly on the scaled coordinates (g−1{S,H}(x\n{S,H}) · x{S,H}) to pick centroids that yield an approximation of the global optimum for the clustering problem on the manifold which is just as good as the original Euclidean approximation bound [Arthur and Vassilvitskii, 2007].\nLemma 5 The expected potential of Sk-means++ seeding over the random choices of C+ satisfies:\nE[Drec(S : C)] ≤ 8(2 + log k) · inf C′∈Sd Drec(S : C ′) . (17)\nThe same approximation bounds holds for Hk-means++ seeding on the hyperboloid (C′,C+ ∈ Hd).\nLemma 5 is notable since it was only recently shown that such a bound is possible for the sphere [Endo and Miyamoto, 2015], and to our knowledge, no such approximation quality is known for clustering on the hyperboloid [Rong et al., 2010, Schwander and Nielsen, 2013]. Notice that Lloyd iterations on non-linear manifolds would require repetitive renormalizations to keep centers on the manifold [Dhillon and Modha, 2001], an additional disadvantage compared to clustering on flat manifolds that {G,K}-means++ seedings do not bear."
    }, {
      "heading" : "6 Experimental validation",
      "text" : "We present some experiments validating our theoretical analysis for the applications above.\nMultiple density ratio estimation. See Appendix H.1 for experiments in this domain.\nDual norm p-LMS (DN-p-LMS). We ran p-LMS and the DN-pLMS of §4 on the experimental setting of Kivinen et al. [2006]. We refer to that paper for an exhaustive description of the experimental setting, which we briefly summarize: it is a noisy signal processing setting, involving a dense or a sparse target. We compute, over the signal received, the error of our predictor on the signal. We keep all parameters as they are in [Kivinen et al., 2006], except for one: we make sure that data are scaled to fit in a Lp ball of prescribed radius, to test the assumption related in [Kivinen et al., 2006] that fixing the learning rate ηt is not straightforward in p-LMS. Knowing the true value of Xp, we then scale it by a misestimation factor ρ, typically in [0.1, 1.7]. We use the same misestimation in DN-p-LMS. Thus, both algorithms suffer the same source of uncertainty. Also, we periodically change the signal (each 1000 iterations), to assess the performances of the algorithms in tracking changes in the signal.\nExperiments, given in extenso in Appendix H.2, are sumarized in Table 4. The following trends emerge: in the mid to long run, DN-p-LMS is never beaten by p-LMS by more than a fraction of percent. On the other hand, DN-p-LM can beat p-LMS by very significant differences (exceeding 40%), in particular when p < 2, i.e. when we are outside the regime of the proof of Kivinen et al. [2006]. This indicates that significantly stronger and more general results than the one of Lemma 4 may be expected. Also, it seems that the problem of p-LMS lies in an “exploding” norm problem: in various cases, we observe that ‖wt‖ (in any norm) blows up with t, and this correlates with a very significant degradation of its performances. Clearly, DN-p-LMS does not have this problem since all relevant norms are under tight control. Finally, even when the norm does not explode, DN-p-LMS can still beat p-LMS, by less important differences though. Of course, the output of p-LMS can repeatedly be normalised, but the normalisation would escape the theory of Kivinen et al. [2006] and it is not clear which kind of normalisation would bring the best results.\nClustering on the sphere. For k ∈ [50]∗, we simulate on T0S2 a mixture of spherical Gaussian and uniform densities with 2k components. We run three algorithms: (i) SKM [Dhillon and Modha, 2001] on the data embedded on S2 with random (Forgy) initialization, (ii), Sk-means++ and (iii) SKM with Sk-means++ initialisation. Results are averaged over the algorithms’ runs.\nTable 5 (left) displays that using Sk-means++ as initialization for SKM brings a very significant leverage over SKM alone, since we almost divide the k-means potential by a factor 2 on some runs. The right plot of Table 5 shows that S-k-means++ consistently reduces the k-means potential by at least a factor 2 over Forgy. The left plot in Table 6 displays that even when it has converged, SKM does not necessarily beat Sk-means++. Finally, the center+right plots in Table 6 display that even when it does beat Sk-means++ when it has converged, the iteration number after which SKM beats Sk-means++ increases with k, and in the worst case may exceed the average number of iterations needed for SKM to converge (we stopped SKM if relative improvement is not above 1o/oo)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented a new scaled Bregman identity (Theorem 1), and used it to derive novel results in multiple density ratio estimation, adaptive filtering, and clustering on curved manifolds. We believe that, like other established properties of Bregman divergences, there is potential for several other applications of the result; Appendix E, F present preliminary thoughts in this direction."
    }, {
      "heading" : "A Additional helper lemmas",
      "text" : "We begin with some helper lemmas that will be used in some of the proofs. In what follows, let\nϕq(w) = (1/2)(W 2 + ‖w‖2q) ϕ†q(w) = W · ‖w‖q\nfor some W > 0 and p, q ∈ (1,∞) such that 1/p+ 1/q = 1.\nA.1 Properties of ϕq and ϕ†q\nWe use the following properties of ϕ,ϕ†.\nLemma 6 For any w,\n∇ϕq(w) = ‖w‖2−qq · sign(w)⊗ |w|q−1 ∇ϕ†q(w) = W · ‖w‖1−qq · sign(w)⊗ |w|q−1,\nwhere ⊗ denotes Hadamard product.\nProof The first identity was shown in Kivinen et al. [2006, Example 1]. The second identity follows from a simple calculation.\nThis implies the follows useful relation between the gradients of ϕq and ϕ†q .\nCorollary 7 For any w,\n∇ϕq(w) = (‖w‖q/W ) · ∇ϕ†q(w) ‖∇ϕ†q(w)‖p = W ‖∇ϕq(w)‖p = ‖w‖q.\nProof [Proof of Corollary 7] The proof follows by direct application of Lemma 6 and the definition of p, q. Note the third identity was shown in Kivinen et al. [2006, Appendix I].\nAs a consequence, we conclude that the gradients of ϕ and ϕ† coincide when considering vectors on the W -sphere.\nLemma 8 For any ‖w‖q = W , ∇ϕq (w) = ∇ϕ†q (w) .\nProof This follows from the relation between∇ϕq and ∇ϕ†q from Lemma 6.\nFinally, we have the following result about the composition of gradients.\nLemma 9 For any w,\n∇ϕq ◦ ∇ϕ†p(w) = ∇ϕ†q ◦ ∇ϕ†p(w) = W\n‖w‖p ·w.\nProof For the first identity, applying Lemma 6 twice,\n∇ϕq ◦ ∇ϕ†p(w) = 1 ‖∇ϕ†p(w)‖q−2q · sign(∇ϕ†p(w))⊗ |∇ϕ†p(w)|q−1\n= 1 W q−2q · sign(w)⊗ W\nq−1\n‖w‖(p−1)(q−1)p · |w|(p−1)(q−1)\n= W\n‖w‖p ·w . (18)\nFor the second identity, use Corollary 7 to conclude that\n∇ϕ†q ◦ ∇ϕ†p(w) = W ‖∇ϕ†p(w)‖q · ∇ϕq(∇ϕ†p(w))\n= W · w‖w‖p .\nA.2 Bound on successive iterate divergence\nThe following Lemma extends [Kivinen et al., 2006, Appendix I] to ϕ†.\nLemma 10 For any w and δ,\nDϕ†q\n( w‖∇ϕ†p ( ∇ϕ†q(w) + δ ))\n≤ (p− 1)‖w‖qW 2 · ∥∥∥∥∥\n1 ‖∇ϕ†q(w) + δ‖p · ( ∇ϕ†q(w) + δ ) − 1 W · ∇ϕ†q (w) ∥∥∥∥∥ 2\np\n. (19)\nProof [Proof of Lemma 10] In this proof, ◦ denotes composition and ⊗ is Hadamard product. The key step in the proof is the use of Theorem 1 to “branch” on the proof of [Kivinen et al., 2006, Appendix I] on the first following identity (letting ϕq(w) . = (1/2) · (W 2 + ‖w‖2q)). We also make use of the dual symmetry of Bregman divergences and we obtain third identity of:\nDϕ†q\n( w‖∇ϕ†p ( ∇ϕ†q(w) + δ ))\n= ‖w‖q W ·Dϕq\n( W ‖w‖q ·w ∥∥∥∥∥\nW\n‖∇ϕ†p(∇ϕ†q(w) + δ)‖q · ∇ϕ†p\n( ∇ϕ†q(w) + δ\n))\n= ‖w‖q W\n·Dϕq ( W\n‖w‖q ·w ∥∥∇ϕ†p ( ∇ϕ†q(w) + δ\n)) (20)\n= ‖w‖q W\n·Dϕp ( ∇ϕq ◦ ∇ϕ†p ( ∇ϕ†q(w) + δ ) ∥∥∥∥∇ϕq ( W ‖w‖q ·w )) by dual symmetry\n= ‖w‖q W ·Dϕp\n( W\n‖∇ϕ†q(w) + δ‖p · ( ∇ϕ†q(w) + δ\n) ∥∥∥∥ W\n‖w‖q · ∇ϕq (w)\n) (21)\n= ‖w‖q W ·Dϕp\n( W\n‖∇ϕ†q(w) + δ‖p · ( ∇ϕ†q(w) + δ\n) ∥∥∇ϕ†q (w) )\n(22)\n= ‖w‖qW ·Dϕp\n( 1\n‖∇ϕ†q(w) + δ‖p · ( ∇ϕ†q(w) + δ\n) ∥∥∥∥ 1\nW · ∇ϕ†q (w)\n) . (23)\nEquations (20) – (22) hold because of Corollary 7. We now use Appendix I4 in Kivinen et al. [2006] on Equation (23) and obtain\nDϕ†q\n( w‖∇ϕ†p ( ∇ϕ†q(w) + δ ))\n≤ (p− 1)‖w‖qW 2 · ∥∥∥∥∥\n1 ‖∇ϕ†q(w) + δ‖p · ( ∇ϕ†q(w) + δ ) − 1 W · ∇ϕ†q (w) ∥∥∥∥∥ 2\np\n,\nas claimed.\nA.3 Bound on successive iterate divergence to target\nIn what follows, we write the DN-pLMS updates as wt = ∇ϕ†p(θt), where\nθt . = ∇ϕ†q(wt−1)−∆t\nfor ∆t = ηt · (w>t−1xt − yt) · xt. Further, for notational ease, we write\nū . =\nu\ngq(u)\nand θ̄t . =\nθt ‖θt‖p .\nWe have the following preliminary bound on the distance from iterates of DN-pLMS to the (normalised) target.\nLemma 11 Fix any learning rate sequence {ηt}Tt=1. Pick any u, and consider iterates {wt}Tt=0 as per Equation 10. Denote st\n. = (ū−wt−1)>xt, rt .= ū>xt − yt, and αt .= W‖θt‖p . Suppose ‖xt‖p ≤ Xp. Then,\nDϕq (ū ‖wt−1 )−Dϕq (ū ‖wt ) ≥ Q+R+ S + T ,\nwith\nQ . =\nαt\n2 ηt(s\n2 t − r2t ),\nR . = (1− αt) · ( W 2 − ū>∇ϕ†q(wt−1) ) ︸ ︷︷ ︸\n∈[0,2W 2]\n,\nS . = p− 1 2 · ( 2α2tη 2 t (st − rt)2X2p − ∥∥(st − rt)ηtαt · xt − (1− αt) · ∇ϕ†q(wt−1) ∥∥2 p )\n︸ ︷︷ ︸ ≥−2(1−αt)2W 2\n,\nT . =\nαt\n2 ηt(st − rt)2\n( 1− 2(p− 1)ηtαtX2p ) .\nProof [Proof of Lemma 11] The Bregman triangle equality (also called the three points property) [Boissonnat et al., 2010, Property 5], [Cesa-Bianchi and Lugosi, 2006, Lemma 11.1] brings:\nDϕq (ū ‖wt−1 )−Dϕq (ū ‖wt ) = (ū−wt−1)> (∇ϕq (wt)−∇ϕq (wt−1))−Dϕq (wt−1 ‖wt ) = (ū−wt−1)> ( ∇ϕ†q (wt)−∇ϕ†q (wt−1) ) −Dϕ†q (wt−1 ‖wt ) by Lemmas 3, 8 .\n4This result is stated as a bound on Dϕq (w‖(∇ϕq)−1(∇ϕq(w) + δ)), which by the Bregman dual symmetry property is equivalent to a bound on Dϕp (∇ϕq(w) + δ‖∇ϕq(w)).\nWe now have\n∇ϕ†q (wt) = ∇ϕ†q ◦ ∇ϕ†p(θt) = W · θ̄t\nby Corollary 7. We get\nDϕq (ū ‖wt−1 )−Dϕq (ū ‖wt )\n≥ (ū−wt−1)> ( W · θ̄t −∇ϕ†q(wt−1) ) − (p− 1)W 2 2 · ∥∥∥∥θ̄t − 1 W · ∇ϕ†q (wt−1) ∥∥∥∥ 2\np\n= (ū−wt−1)> ( W · θ̄t −∇ϕ†q(wt−1) ) − p− 1 2 · ∥∥W · θ̄t −∇ϕ†q (wt−1) ∥∥2 p .\nNow, note that θt = ∇ϕ†q(wt−1) + ηt · (st − rt) · xt.\nWe can thus rewrite the above as\nDϕq (ū ‖wt−1 )−Dϕq (ū ‖wt ) ≥ st(st − rt)ηtαt + (1− αt) ( w>t−1∇ϕ†q(wt−1)− ū>∇ϕ†q(wt−1) )\n−p− 1 2 · ∥∥(st − rt)ηtαt · xt − (1− αt) · ∇ϕ†q(wt−1) ∥∥2 p\n= st(st − rt)ηtαt + (1− αt) ( W 2 − ū>∇ϕ†q(wt−1) )\n−p− 1 2 · ∥∥(st − rt)ηtαt · xt − (1− αt) · ∇ϕ†q(wt−1) ∥∥2 p\nby definition of ∇ϕ†q = Q+R+ S + T .\nWe can show that the sum R+ S + T ≥ 0. This proof involves chaining together multiple simple inequalities. We give a high level overview in Figure 2.\nLemma 12 Let R,S, T be as per Lemma 11. Suppose we fix\nηt = γ · W\n4(p− 1)MXpW + |yt −w>t−1xt|Xp , (24)\nfor any γ ∈ [1/2, 1], and M .= max{W,Xp}. Then, T +R+ S ≥ 0.\nProof The triangle inequality and the fact that ‖∇ϕ†q(wt−1)‖p = W brings\nαt ∈ [\nW\n‖∇ϕ†q(wt−1)‖p + ηt|st − rt| · ‖xt‖p ,\nW\n‖∇ϕ†q(wt−1)‖p − ηt|st − rt| · ‖xt‖p\n]\n⊆ [\nW\nW + ηt|st − rt|Xp ,\nW\nW − ηt|st − rt| ·Xp\n] , (25)\nassuming that ηt is chosen so that\nηt ≤ W\n|st − rt| ·Xp , (26)\nso that the right bound is non negative. To indeed ensure this, suppose that for some 0 < t ≤ 1/2, we fix\nηt ≤ t 1− t · W|st − rt|Xp . (27)\nWe would in addition obtain from Equation (25) that αt ∈ [1− t, 1 + t]. Suppose ηt is also fixed to ensure\nηt ∈ [\nW\n2(p− 1)κtXpW 2 − |st − rt|Xp ,\nW\n4(p− 1)XpW 2 + |st − rt|Xp\n] , (28)\nfor some κt such that\nκt ≥ 2 + |st − rt|\n(p− 1)W 2 . (29)\nNotice that constraint on κt makes the interval non empty and its left bound strictly positive. Assuming (28) holds, we would have\nαtηt ∈ [\n1\n2(p− 1)κtX2p ,\n1\n4(p− 1)X2p\n] . (30)\nThe left bound of (30) holds because\nαtηt ≥ ηt · W\nW + ηt|st − rt|Xp ≥ 1\n2(p− 1)κtX2p . (31)\nThe first inequality holds because of (25) and the second one holds because of (28). The right bound of (30) holds because of (25), and so\nαtηt ≤ ηt · W\nW − ηt|st − rt|Xp ≤ 1\n4(p− 1)X2p , (32)\nwhere the last inequality is due to (28).\nEquation (30) makes that T (ηtαt) is at least its value when αtηt attains the lower bound of (31), that is,\nT (ηtαt) ≥ κt − 1 κt · (st − rt) 2 8(p− 1)X2p . (33)\nNow, to guarantee αt ∈ [1− t, 1 + t], it is sufficient that the right-hand side of inequality (27) belongs to interval (28) and we pick ηt within the interval [left bound (28), right-hand side (27)]. To guarantee that the right-hand side of inequality (27) falls in interval (28), we need first,\nW\n2(p− 1)κtXpW 2 − |st − rt|Xp ≤ t 1− t · W|st − rt|Xp , (34)\nthat is,\nκt ≥ 1 t · |st − rt| 2(p− 1)W 2 . (35)\nTo guarantee that the right-hand side of inequality (27) falls in interval (28) we need then\nW\n4(p− 1)XpW 2 + |st − rt|Xp ≥ t 1− t · W|st − rt|Xp , (36)\nthat is,\nt ≤ |st − rt|\n2|st − rt|+ 4(p− 1)W 2 . (37)\nTo summarize, if we pick any strictly positive t following inequality (37) (note t < 1) and\nκt . = 2 +\n1 t · |st − rt| (p− 1)W 2 , (38)\nthen we shall have both αt ∈ [1− t, 1 + t] and inequality (33) holds as well. In this case, we shall have\nT +R+ S ≥  1− 1\n2 + 1 t · |st−rt| (p−1)W 2\n  · (st − rt) 2\n8(p− 1)X2p − 2 tW 2 − (p− 1) 2tW 2\n≥ (\n1− 1 2\n) · (st − rt) 2\n8(p− 1)X2p − 2 tW 2 − (p− 1) 2tW 2\n= (st − rt)2\n16(p− 1)X2p − 2 tW 2 − (p− 1) 2tW 2 . (39)\nTo finish up, we want to solve for t the right-hand side such that it is non negative, and we find that t has to satisfy\nt ≤ 1 p− 1 · ( 1 + √ 1 + (st − rt)2 16X2pW 2 ) . (40)\nSince √ 1 + x ≥ √x, a sufficient condition is\nt ≤ |st − rt|\n4(p− 1)XpW . (41)\nTo ensure this and inequality (37), it is sufficient that we fix\nt . = |st − rt| 2|st − rt|+ 4(p− 1)WM , (42)\nwhere M .= max{W,Xp}. With this expression for t, we get from (38),\nκt . = 2 +\n4M\nW + 2|st − rt| (p− 1)W 2 . (43)\nFor these choices, Lemma 13 implies that the given ηt is feasible.\nLemma 13 Suppose t satisfies (42) and κt satisfies (43). Then, a sufficient condition for ηt to satisfy both (27) and (28) is\nηt = γ · W\n4(p− 1)MXpW + |yt −w>t−1xt|Xp ,\nfor any γ ∈ [1/2, 1].\nProof [Proof of Lemma 13] Notice the range of values authorized for ηt: ηt ∈ [ W\n2(p− 1)κtXpW 2 − |st − rt|Xp , t 1− t · W|st − rt|Xp\n]\n=\n  W\n2(p− 1) (\n2 + 4MW + 2|st−rt| (p−1)W 2 ) XpW 2 − |st − rt|Xp\n, W\n4(p− 1)MXpW + |st − rt|Xp\n \n=\n[ W\n2(2(p− 1)W 2 + 4M(p− 1)W + 2|st − rt|)Xp − |st − rt|Xp ,\nW\n4(p− 1)MXpW + |st − rt|Xp\n]\n=\n[ W\n4(p− 1)XpW 2 + 8(p− 1)MXpW + 3|st − rt|Xp ,\nW\n4(p− 1)MXpW + |st − rt|Xp\n]\n⊃ [\nW\n8(p− 1)MXpW + 2|st − rt|Xp ,\nW\n4(p− 1)MXpW + |st − rt|Xp\n] . (44)\nA sufficient condition for ηt to fall in interval (44) is\nηt = γ · W\n4(p− 1)MXpW + |yt −w>t−1xt|Xp ,\nfor any γ ∈ [1/2, 1].\nLemma 14 Suppose we fix the learning rate as per (24). Pick any u, and consider iterates {wt}Tt=0 as per Equation 10. Suppose ‖xt‖p ≤ Xp and |yt| ≤ Y,∀t ≤ T . Then, for any t,\nDϕq (ū ‖wt−1 )−Dϕq (ū ‖wt ) ≥ 1\n4(p− 1) (\n2 + 4MW + 2(Y+XpW ) (p−1)W 2 ) X2p · (s2t − r2t )\nwhere st . = (ū−wt−1)>xt, rt .= ū>xt − yt.\nProof [Proof of Lemma 14] We start from the bound of Lemma 11:\nDϕq (ū ‖wt−1 )−Dϕq (ū ‖wt ) ≥ Q+R+ S + T ≥ Q by Lemma 12 = αt\n2 ηt(s\n2 t − r2t ) by definition\n≥ 1 4(p− 1)κtX2p · (s2t − r2t )\n≥ 1 4(p− 1) ( 2 + 4MW + 2 maxt |yt−w>t−1xt| (p−1)W 2 ) X2p · (s2t − r2t )\n≥ 1 4(p− 1) ( 2 + 4MW + 2(Y+XpW ) (p−1)W 2 ) X2p · (s2t − r2t ) . (45)\nThe last constraint to check for this bound to be valid is our t in (42) has to be < 1/2 from inequality (26), which trivially holds since 4(p− 1)WM ≥ 0. We conclude by noting Lemma 13 provides a feasible value of ηt.\nA.4 Gauge normalisation\nThe following lemma about the gauge of x will be useful.\nLemma 15 Let gq(x) = ‖x‖q/W for some W > 0. Then, for the iterates {wt} as per Equation 9, gq(wt) = 1.\nProof We have\ngq(wt) = ‖wt‖q W\n= W\nW by Lemma 3\n= 1 ,∀t ≥ 1 . (46)"
    }, {
      "heading" : "B Proofs of results in main body",
      "text" : "We present proofs of all results in the main body.\nProof [Proof of Theorem 1] Let J : X → Xg denote the Jacobian of h : x 7→ (1/g(x)) · x. By an elementary calculation, g(x) · J = Id − (1/g(x)) · x∇g(x)>, which by the chain rule brings the following expression for the gradient of ϕ†(y) = g(y) · (ϕ ◦ h)(y):\n∇ϕ† (y) = ∇g(y) · (ϕ ◦ h)(y) + g(y) · ∇(ϕ ◦ h)(y) = ∇g(y) · (ϕ ◦ h)(y) + g(y) · J>∇ϕ(h(y)) = ∇g(y) · (ϕ ◦ h)(y) +∇ϕ(h(y))− (1/g(y)) · ∇g(y)y>∇ϕ(h(y))\n= ∇ϕ ( 1 g(y) · y ) + ( ϕ ( 1 g(y) · y ) − 1 g(y) · y>∇ϕ ( 1 g(y) · y )) · ∇g(y) . (47)\nFor simplicity, let u = x/g(x) and v = y/g(y), so that ϕ†(x) = g(x) · ϕ(u) and ϕ†(y) = g(y) · ϕ(v). The above then reads\n∇ϕ† (y) = ∇ϕ (v) + ( ϕ (v)− v>∇ϕ (v) ) · ∇g(y). (48)\nNow, the LHS of Equation (3) is\ng(x) ·Dϕ ( 1\ng(x) · x ∥∥ 1 g(y)\n· y ) = g(x) ·Dϕ (u‖v)\n= g(x) · ϕ(u)− g(x) · ϕ(v)− g(x) · ∇ϕ(v)>(u− v) = ϕ†(x)− g(x) · ϕ(v)−∇ϕ(v)>(x− g(x) · v) = ϕ†(x)− g(x) · (ϕ(v)−∇ϕ(v)>v)−∇ϕ(v)>x,\nwhile the RHS is\nDϕ† ( x ∥∥ y )\n= ϕ†(x)− ϕ†(y)−∇ϕ†(y)>(x− y) = ϕ†(x)− g(y) · ϕ(v)−∇ϕ (v)> (x− y)− ( ϕ (v)− v>∇ϕ (v) ) · ∇g(y)>(x− y).\nCancelling the common ϕ†(x) and ∇ϕ (v)> y terms, the difference ∆ = RHS− LHS is\n∆ = g(x) · (ϕ(v)−∇ϕ(v)>v)− g(y) · ϕ(v) +∇ϕ (v)> y − ( ϕ (v)− v>∇ϕ (v) ) · ∇g(y)>(x− y)\n= g(x) · (ϕ(v)−∇ϕ(v)>v)− g(y) · ϕ(v) + g(y) · ∇ϕ (v)> v − ( ϕ (v)− v>∇ϕ (v) ) · ∇g(y)>(x− y) = g(x) · (ϕ(v)−∇ϕ(v)>v)− g(y) · ( ϕ(v)−∇ϕ (v)> v ) − ( ϕ (v)− v>∇ϕ (v) ) · ∇g(y)>(x− y)\n= (ϕ(v)−∇ϕ(v)>v) · (g(x)− g(y)−∇g(y)>(x− y)) = (ϕ(v)−∇ϕ(v)>v) ·Bg(x‖y).\nThus, the identity holds, if and only if either ϕ(v) = ∇ϕ(v)>v for every v ∈ Xg, or Bg(x‖y) = 0. The latter is true if and only if g is affine from Equation 2. The result follows.\nIt is easy to check that Theorem 1 in fact holds for separable (matrix) trace divergences [Kulis et al., 2009] of the form\nDϕ(X‖Y) .= ϕ(X)− ϕ(Y)− tr ( ∇ϕ(Y)>(X − Y) ) , (49)\nwith ϕ, g : S(d) → R (for S(d) the set of symmetric real matrices), with ϕ convex. In this case, the restricted positive homogeneity property becomes\nϕ (U) = tr ( ∇ϕ(U)>U ) ,∀U ∈ Xg . (50)\nProof [Proof of Lemma 2] Note that by construction, g(r(x)) = P(X = x)/((1− πC) · P(X = x|Y = C)), and so (\n1\ng(r(x)) · r(x)\n)\nc\n= (1− πC) · P(X = x|Y = C) P(X = x) · P(X = x|Y = c) P(X = x|Y = C)\n= (1− πC) πc · πcP(X = x|Y = c)\nP(X = x) = η(x) . (51)\nFurthermore,\nP(X = x) = C∑\nc=1\nπcP(X = x|Y = c)\n= (1− πC) · (\nπC 1− πC\n+ ∑\nc<C\nπc 1− πC · P(X = x|Y = c) P(X = x|Y = C)\n) · P(X = x|Y = C)\n= (1− πC) · g(r(x)) · P(X = x|Y = C) . (52) Now let\nr̂(x) = 1\nη̂C(x) · η̂(x).\nIt then comes\nEM [Dϕ(η(X)‖η̂(X))] = (1− πC) · EPC [g(r(x)) ·Dϕ(η(X)‖η̂(X))]\n= (1− πC) · EPC [ g(r(x)) ·Dϕ ( 1\ng(r(x)) · r(x)\n∥∥∥∥ η̂(X) )]\n= (1− πC) · EPC [ g(r(x)) ·Dϕ ( 1\ng(r(x)) · r(x)\n∥∥∥∥ 1\ng(r̂(X)) · r̂(X)\n)]\n= (1− πC) · EPC [ Dϕ†(r(X)‖r̂(X)) ] ,\nas claimed.\nProof [Proof of Lemma 3] For any x, ‖∇ϕ†p(x)‖q = W by Corollary 7. Since wt = ∇ϕ†p(θt−1) for suitable θt−1, the result follows. The result for ‖∇ϕ†q(wt)‖p follows similarly by Corollary 7. Note that while ‖wt‖q = ‖∇ϕ(wt)‖p for the standard p-LMS update [Kivinen et al., 2006, Appendix I], these norms may vary with each iteration i.e. wt may not lie in the Lq ball.\nProof [Proof of Lemma 4] Similarly to the proof of Lemma 10, a key to the proof of Lemma 4 relies on branching on Kivinen et al. [2006] through the use of Theorem 1. We first note that Dϕ†q (u‖w0) = W · ‖u‖q since w0 = 0, and Dϕ†q (u‖wT+1) ≥ 0, and so\nW · ‖u‖q ≥ Dϕ†q (u‖w0)−Dϕ†q (u‖wT+1)\n=\nT∑\nt=1\n{ Dϕ†q (u‖wt−1)−Dϕ†q (u‖wt) } by telescoping property\n= gq(u) · T∑\nt=1\n{ Dϕq ( u\ngq(u)\n∥∥∥∥ wt−1\ngq(wt−1)\n) −Dϕq ( u\ngq(u)\n∥∥∥∥ wt\ngq(wt)\n)} by Theorem 1\n= gq(u) · T∑\nt=1\n{ Dϕq ( u\ngq(u) ‖wt−1\n) −Dϕq ( u\ngq(u) ‖wt\n)} by Lemma 15 . (53)\nRecall from Lemma 14 that\nDϕq\n( u\ngq(u) ‖wt−1\n) −Dϕq ( u\ngq(u) ‖wt\n) ≥ 1\n4(p− 1) (\n2 + 4MW + 2(Y+XpW ) (p−1)W 2 ) X2p · (s2t − r2t )\nwhere\nst . = ((1/gq(u)) · u−wt−1)>xt rt . = (1/gq(u)) · u>xt − yt.\nNote that Rq(w1:T |u) = ∑T t=1(s 2 t − r2t ) by definition. Summing the above for t = 1, 2, ..., T and telescoping sums yields\nRq(w1:T |u) ≤ 4(p− 1) ( 2 + 4M\nW +\n2(Y +XpW ) (p− 1)W 2 ) X2pW 2\n= 4(p− 1)X2pW 2 + 16(p− 1)MX2pW + 8(Y +XpW )X2p ≤ 4(p− 1)X2pW 2 + (16p− 8)MX2pW + 8Y X2p . (54)\nSee Figure 3 for some geometric intuition about the updates.\nProof [Proof of Lemma 5] We start by the sphere. Let ϕ(x) .= (1/2) · ‖x‖22. Since a Bregman divergence is invariant to linear transformation, it comes from Table 7 that\nDϕ\n( xS\ngS(xS)\n∥∥ c S\ngS(cS)\n) =\n1\ngS(cS) ·Dϕ†(x‖c) = 1− cosDG(x, c),\nwhere we recall that DG denotes the geodesic distance on the sphere (see Figure 1 and Appendix C). Equivalently, ∥∥∥∥ 1\ngS(xS) · xS − 1 gS(cS) · cS\n∥∥∥∥ 2\n2\n= 1− cosDG(x, c) . (55)\nThis equality allows us to use k-means++ using the LHS of (55) to compute the distribution that picks a center. The key to using the approximation property of k-means++ relies on the existence of a coordinate system on the sphere for which the cluster centroid is just the average of the cluster points (polar coordinates), an average that eventually has to be rescaled if the coordinate system is not that one [Dhillon and Modha, 2001, Endo and Miyamoto, 2015]. The existence of this coordinate system makes that the proof of Arthur and Vassilvitskii [2007] (and in particular the key Lemmata 3.2 and 3.3) can be carried out without modification to yield the same approximation ratio as that of Arthur and Vassilvitskii [2007] if the distortion at hand is the squared Euclidean distance, which turns out to be Drec(. : .) from eq. (55). The case of the hyperboloid follows the exact same path, but starts from the fact that Table 7 now brings\nDϕ\n( xH\ngH(xH)\n∥∥ c H\ngH(cH)\n) = coshDG(y, c)− 1 = ∥∥∥∥ 1\ngH(xH) · xH − 1 gH(cH) · cH\n∥∥∥∥ 2\n2\n.\nTo finish, in the same way as for the Sphere, we just need the existence of a coordinate system for which the centroid is an average of the cluster points, which can be obtained from hyperbolic barycentric coordinates [Ungar, 2014, Section 18]."
    }, {
      "heading" : "C Working out examples of Table 7",
      "text" : "We fill in the details justifying each of the examples of Equation 3 provided in Table 2. We also provide the form of the corresponding divergences Dϕ and distortions Dϕ† in the augmented Table 7.\nRow I — for X = Rd, consider ϕ(x) = (1 + ‖x‖22)/2 and g(x) = ‖x‖2 (we project on the Euclidean sphere). It comes\nϕ†(x) = ‖x‖2 ·   1 + ∥∥∥ 1‖x‖2 · x ∥∥∥ 2 2\n2\n  = ‖x‖2 . (56)\ng is not linear (but it is homogeneous of degree 1), but we have\nϕ(x) = 1 = x>∇ϕ(x) ,∀x : ‖x‖2 = 1 , (57)\nso ϕ is 1-homogeneous on the Euclidean sphere, and we can apply Theorem 1. We have\ng(x) ·Dϕ ( 1\ng(x) · x‖ 1 g(y) · y ) = ‖x‖2 2 · ∥∥∥∥ 1 ‖x‖2 · x− 1‖y‖2 · y ∥∥∥∥ 2\n2\n= ‖x‖2 · ( 1− x >y\n‖x‖2‖y‖2\n) = ‖x‖2 · (1− cos(x,y)) , (58)\nand we also have\nDϕ† (x‖y) = ‖x‖2 − ‖y‖2 − 1 ‖y‖2 · (x− y)>y\n= ‖x‖2 − ‖y‖2 − x>y ‖y‖2 + ‖y‖2 (59) = ‖x‖2 · ( 1− x >y\n‖x‖2‖y‖2\n) = ‖x‖2 · (1− cos(x,y)) , (60)\nwhich is equal to Equation (58), so we check that Theorem 1 applies in this case. Dϕ† has some properties. One is a weak form of triangle inequality.\nLemma 16 Dϕ† (x‖y) +Dϕ† (y‖z) ≤ Dϕ† (x‖z), ∀x,y, z such that ‖y‖2 ≤ ‖x‖2.\nProof\nDϕ† (x‖y) +Dϕ† (y‖z) = ‖x‖2 · (1− cos(x,y)) + ‖y‖2 · (1− cos(y, z)) = ‖x‖2 · ((1− cos(x,y)) + (1− cos(y, z))) + (‖y‖2 − ‖x‖2) · (1− cos(y, z)) ≤ ‖x‖2 · (1− cos(x, z)) + (‖y‖2 − ‖x‖2) · (1− cos(y, z)) ≤ Dϕ† (x‖z) + (‖y‖2 − ‖x‖2) · (1− cos(y, z)) ≤ Dϕ† (x‖z) , (61)\nsince ‖y‖2 ≤ ‖x‖2. We have used the fact that (1 − cos(x,y)) is half the Euclidean distance between unitnormalized vectors.\nIt turns out that Dϕ†(x‖µ) can be related to the log-likelihood of a von Mises-Fisher distribution with expected direction µ, which is useful in text analysis [Reisinger et al., 2010].\nRow II — Let ϕ(x) .= (1/2) · (u2 + ‖x‖2q), for q > 1 [Kivinen et al., 2006]. We have\nϕ\n( 1 g(x) · x ) = u2 2 + 1 2 · ∥∥∥∥ 1 g(x) · x ∥∥∥∥ 2\nq\n= u2\n2 +\n1\n2g2(x) · ‖x‖2q . (62)\nWe also have\n( 1 g(x) · x )> ∇ϕ ( 1 g(x) · x ) = 1 g(x) · ∑\ni\nxi · sign ( 1 g(x) · xi ) ∣∣∣ 1g(x) · xi ∣∣∣ q−1\n∥∥∥ 1g(x) · x ∥∥∥ q−2\nq\n= ∑\ni\n∣∣∣ 1g(x) · xi ∣∣∣ q\n∥∥∥ 1g(x) · x ∥∥∥ q−2\nq\n= 1\ng2(x) · ‖x‖2q . (63)\nTo have the condition of Theorem 1 satisfied, we therefore need\n‖x‖q = ug(x) , (64)\nSo we use g(x) = ‖x‖q/W and u = W , observing that ϕ is 1-homogeneous on the Lp sphere. We check that\nϕ†(x) = W · ‖x‖q . (65)\nand we obtain\nDϕ†(w‖w′) = W · ‖w‖q −W · ∑\ni\nwi · sign(w′i) · |w′i|q−1 ‖w′‖q−1q . (66)\nRow III — As in Buss and Fillmore [2001], we assume ‖x‖2 ≤ π, or we renormalize or change the radius of the ball) We first lift the data points using the Sphere lifting map Rd 3 x 7→ xS ∈ Rd+1:\nxS . = [x1 x2 · · · xd rx cot rx]> , (67)\nwhere rx . = ‖x‖2 is the Euclidean norm of x. Notice that the last coordinate is a coordinate of the Hessian of the geodesic distance to the origin on the sphere [Buss and Fillmore, 2001]. We then let g(xS) .= rx/ sin rx (notice that g is computed uding the first d coordinates). Finally, for X = Rd+1 and u > 1, consider ϕ(xS) = (u2 +‖xS‖22)/2. The set of points for which ϕ(xS) = (xS)>∇ϕ(xS) is equivalently the subset Xg ⊆ Rd+1 such that\nXg . = {xS : g2(xS) = u2} . (68)\nSo ϕ satisfies the restricted positive homogeneity of degree 1 on Xg and we can apply Theorem 1. We first remark that:\n‖xS‖22 = r2x + r2x cot2 rx\n= r2x\nsin r2x = g2(xS) , (69)\nand\nϕ†(xS) = rx sin rx · ϕ ( sin rx rx · xS ) = rx sin rx · ( sin rx rx )2 · ‖xS‖22 = ‖xS‖2 , (70)\nand finally, because of the spherical law of cosines,\nsin rx sin ry cos(x,y) + cos rx cos ry = cosDG(x,y) , (71)\nwhere we recall from eq. (16) that DG(x,y) is the geodesic distance between the image of the exponential maps of x and y on the sphere.\nWe then derive\ng(xS) ·Dϕ ( 1\ng(xS) · xS‖ 1 g(yS) · yS\n)\n= rx 2 sin rx · ∥∥∥∥ sin rx rx · xS − sin ry ry · yS ∥∥∥∥ 2\n2\n= rx 2 sin rx · ( sin2 rx ‖x‖22 · ‖xS‖22 + sin2 ry ‖y‖22 · ‖yS‖22 − 2 · sin rx rx · sin ry ry · (xS)>yS )\n= rx sin rx · ( 1− sin rx rx · sin ry ry · (xS)>yS )\n(72)\n= rx sin rx · ( 1− sin rx rx · sin ry ry · ( x>y + rxry cot rx cot ry )) (73) = rx\nsin rx · (1− sin rx sin ry · (cos(x,y) + cot rx cot ry))\n= rx\nsin rx · (1− (sin rx sin ry cos(x,y) + cos rx cos ry))\n= rx\nsin rx · (1− cosDG(x,y)) . (74)\nIn Equation (72), we use Equation (69), and we use Equation (71) in Equation (74). We also check\nDϕ†(x S‖yS) = ‖xS‖2 − ‖yS‖2 −\n1\n‖yS‖2 · (xS − yS)>yS\n= ‖xS‖2 − 1 ‖yS‖2 · (xS)>yS = ‖xS‖2 · ( 1− (x S)>yS\n‖xS‖2‖yS‖2\n) (75)\n= rx\nsin rx · (1− cosDG(x,y)) . (76)\nTo obtain (76), we use the fact that\n(xS)>yS\n‖xS‖2‖yS‖2 = sin rx rx · sin ry ry\n· ( x>y + rxry cot rx cot ry ) , (77)\nand then plug it into Equation (76), which yields the identity between Equation (73) (and thus (74)) and (76). So Theorem 1 holds in this case as well.\nWe remark that (1/g(xS)) · xS = exp0(x) is the exponential map for the sphere [Buss and Fillmore, 2001].\nRow IV — In the same way as we did for row IV, we first create a lifting map, but this time complex valued, the Hyperboloid lifting map H: Rd 3 x 7→ xH ∈ Rd × C. With an abuse of notation, it is given by\nxH . = [x1 x2 · · · xd irx coth rx]> , (78)\nand we let g(xH) .= −rx/ sinh rx, with coth and sinh defining respectively the hyperbolic cotangent and hyperbolic sine. We let 0 coth 0 = 0/ sinh 0 = 1. Notice that the complex number is pure imaginary and so H defines a d dimensional manifold that lives in Rd+1 assuming that the last coordinate is the imaginary axis. Let expq(x) . = (1/g(xH) · xH . Notice that\n∥∥expq(x) ∥∥2\n2 = sinh2 rx r2x · ( r2x + i 2r2x coth 2 rx )\n= sinh2 rx + i 2 cosh2 rx = sinh2 rx − cosh2 rx = −1 , (79)\nso expq(x) defines a lifting map from Rd to the hyperboloid model Hd of hyperbolic geomety Galperin [1993]. In fact, it defines the exponential map for the plane TqHd tangent to Hd in point q . = [0 0 · · · 0 i] = 0H . To see this, remark that we can express the geodesic distance DG with the hyperbolic metric between xH and yH as\nDG(x H ,yH) . = cosh−1(−(xH)>yH) , (80)\nwhere cosh−1 is the inverse hyperbolic cosine. So, for any x ∈ TqHd, since rx = ‖x− 0‖2, we have\nDG(expq(x), q) = cosh −1(−(xH)>0H)\n= cosh−1(−i2 cosh rx) = rx = ‖x− 0‖2 , (81)\nand expq(x) is indeed the exponential map for TqHd. Now, remark that\nexpq(x) > expq(y) = sinh rx rx · sinh ry ry · (xH)>yH\n= sinh rx rx · sinh ry ry\n· ( x>y + i2rxry coth rx coth ry )\n= sinh rx sinh ry · (cos(x,y)− coth rx coth ry) = sinh rx sinh ry cos(x,y)− cosh rx cosh ry = − coshDG(xH ,yH) . (82)\nEq. (82) holds by the hyperbolic law of cosines. Now, we let ϕ(xH) = (u2 + ‖xH‖22)/2 and\nXg . = {xH : ‖xH‖22 = u2} . (83)\nWe check that ϕ(xH) = u2 = (xH)>∇ϕ(xH) for any xH ∈ Xg, so we can apply Theorem 1. We then use eqs. (79) and (82) and derive\ng(xH) ·Dϕ ( 1\ng(xH) · xH‖ 1 g(yH) · yH\n)\n= − rx 2 sinh rx\n· ∥∥expq(x)− expq(y) ∥∥2 2\n= − rx 2 sinh rx\n· (∥∥expq(x) ∥∥2 2 + ∥∥expq(y) ∥∥2 2 − 2 expq(x)> expq(y) )\n= − rx sinh rx\n· ( coshDG(x H ,yH)− 1 ) . (84)\nNote that eq. (84) is a negative-valued and concave distortion.\nRow V — for X = Rd+∗, consider ϕ(x) = ∑ i xi log xi − xi and g(x) = 1>x (we normalize on the simplex). Since g is linear, we do not need to check for the homogeneity of ϕ, and we directly obtain:\ng(x) ·Dϕ ( 1\ng(x) · x‖ 1 g(y) · y ) = ∑\ni\nxi log xi − (1>x) log(1>x)− 1>x\n−1 >x 1>y · ∑\ni\nyi log yi − (1>x) log(1>y) + 1>x\n−(1>x) · ∑\ni\n( xi\n1>x − yi 1>y\n) · log yi\n1>y\n= ∑\ni\nxi log xi yi − (1>x) · log 1 >x 1>y . (85)\nFurthermore,\nϕ†(x) = 1>x · (∑\ni\nxi 1>x · log xi 1>x\n− 1 ) = ∑\ni\nxi log xi − (1>x) log(1>x)− 1>x . (86)\nNoting that ϕ†(x) is the sum of three terms, one of which is linear and can be removed for the divergence, so the divergence is just the sum of the two divergences with the two generators, which is found to be Equation (85) as well. Remark that while the KL divergence is convex in its both arguments, Dϕ†(x‖y) may not be (jointly) convex. Indeed, its Hessian in y equals:\nHy(Dϕ†) = Diag({xi/y2i }i)− 1>x\n(1>y)2 · 11> , (87)\nwhich may be indefinite.\nRow VI — for X = Rd+∗, consider ϕ(x) = −d− ∑ i log xi and g(x) = (πx) 1/d, where we let πx . = ∏ i xi (we normalize with the geometric average). It comes\nϕ†(x) = (πx) 1/d · ( −d− ∑\ni\nlog xi\n(πx)1/d\n) = −d · (πx)1/d . (88)\ng is not linear (but it is homogeneous of degree 1), and we have\nϕ(x) = −d = x>∇ϕ(x) ,∀x : ∏\ni\nxi = 1 , (89)\nso ϕ is 1-homogeneous on Xg , and we can apply Theorem 1. We have\ng(x) ·Dϕ ( 1\ng(x) · x‖ 1 g(y) · y )\n= (πx) 1/d ·\n∑\ni\n( xi(πy) 1/d\nyi(πx)1/d − log xi(πy)\n1/d\nyi(πx)1/d\n) − d(πx)1/d\n= ∑\ni\nxi(πy) 1/d\nyi − d(πx)1/d log(πx)1/d − (πx)1/d log πy + (πx)1/d log πy\n+d(πx) 1/d log(πx) 1/d − d(πx)1/d\n= ∑\ni\nxi(πy) 1/d\nyi − d(πx)1/d . (90)\nWe also have\n∂\n∂xi ϕ†(x) = −(1/xi) · (πx)1/d , (91)\nand so\nDϕ† (x‖y) = −d(πx)1/d + d(πy)1/d + · ∑\ni\n(xi − yi) · (πy)\n1/d\nyi\n= −d(πx)1/d + d(πy)1/d + ∑\ni\nxi(πy) 1/d\nyi − d(πy)1/d\n= ∑\ni\nxi(πy) 1/d\nyi − d(πx)1/d , (92)\nwhich is equal to Equation (90), so we check that Theorem 1 applies in this case.\nRow VII — We use the following fact Kulis et al. [2009]. Let X = ULU> and Y = VTV> be the eigendecomposition of symmetric positive definite matrices X and Y, with L .= Diag(l), T .= Diag(t), and U .= [u1|u2| · · · |ud], V .= [v1|v2| · · · |vd] orthonormal; let ϕ = tr (X log X − X). Then we have\nDϕ (X‖Y) = ∑\ni,j\n(u>i vj) 2 ·Dϕ2(li‖tj) , (93)\nwith ϕ2(x) = x log x− x. We pick g(X) = tr (X) = ∑ i li, which brings from Equation (85)\ng(X) ·Dϕ ( 1\ng(X) · X‖ 1 g(Y) · Y )\n= ∑\ni,j\n(u>i vj) 2 · tr (X) ·Dϕ2\n( li\ntr (X) ‖ tj tr (Y)\n)\n= ∑\ni,j\n(u>i vj) 2 · tr (X) ·\n( li\ntr (X) · log li · tr (Y) tj · tr (X) − li tr (X) + tj tr (Y)\n)\n= ∑\ni,j\n(u>i vj) 2 · ( li log\nli tj − li + tj\n) + log ( tr (Y)\ntr (X)\n) · ∑\ni,j\n(u>i vj) 2 · li\n+ tr (X) tr (Y) · ∑\ni,j\n(u>i vj) 2 · tj −\n∑\ni,j\n(u>i vj) 2 · tj . (94)\nBecause U, V are orthonormal, we also get ∑ i,j(u > i vj) 2 · li = ∑ i li ∑ j cos 2(ui,vj) = ∑ i li = tr (X) and∑\ni,j(u > i vj) 2 · tj = tr (y), and so Equation (94) becomes\ng(X) ·Dϕ ( 1\ng(X) · X‖ 1 g(Y) · Y )\n= tr (X log X − X log Y)− tr (X) + tr (Y) + tr (X) · log ( tr (Y)\ntr (X)\n) + tr (X)− tr (Y)\n= tr (X log X − X log Y)− tr (X) · log ( tr (X)\ntr (Y)\n) . (95)\nWe also check that\nϕ†(X) = tr (X) · tr ( 1\ntr (X) · X log\n( 1 tr (X) · X ) − 1 tr (X) · X )\n= tr ( X log ( 1 tr (X) · X )) − tr (X) , (96)\nand\nX log\n( 1 tr (X) · X ) = ULU>U log ( 1 1>l · L ) U>\n= UL log\n( 1 1>l · L ) U> (97)\n= UL log LU> − log tr (X) · ULU> , (98)\nso that ϕ†(X) = tr (X log X − X) − tr (X) · log tr (X). Let ϕ3(X) .= tr (X) · log tr (X). We have ∇ϕ3(X) = (1 + log tr (X)) · I. Since a (Bregman) divergence involving a sum of generators is the sum of (Bregman) divergences, we get\nDϕ† (X‖Y) = tr (X log X − X log Y − X + Y)− tr (X) · log tr (X) + tr (Y) · log tr (Y) +(1 + log tr (Y)) · tr (X − Y)\n= tr (X log X − X log Y)− tr (X) · log tr (X) + tr (X) · log tr (Y) = tr (X log X − X log Y)− tr (X) · log ( tr (X)\ntr (Y)\n) , (99)\nwhich is Equation (95).\nRow VIII — We have the same property as for Row V, but this time with ϕ2 = −d− log x [Kulis et al., 2009]. We check that whenever det(X) = 1, we have\nϕ(X) = −d− log det(X) = −d = − det(X)tr (I) = tr ( det(X)X−1X ) = tr ( ∇ϕ(X)>X ) . (100)\nFor g(X) .= det X1/d, we get:\nϕ†(X) = det X1/d · ( −d− log det ( 1 det X1/d · X ))\n= det X1/d · ( −d− log 1\ndet X · det X\n) = −d · det X1/d , (101)\nand furthermore\n∇ϕ†(X) = −d · ∇(det X1/d)(X) = −det(X1/d) · X−1 (102)\nSo,\nDϕ† (X‖Y) = −d · det X1/d + d · det Y1/d + tr ( det(Y1/d) · Y−1(X − Y) )\n= −d · det X1/d + d · det Y1/d + det(Y1/d)tr ( XY−1 ) − d · det Y1/d = det(Y1/d)tr ( XY−1 ) − d · det X1/d . (103)\nWe check that it is equal to:\ng(X) ·Dϕ ( 1\ng(X) · X‖ 1 g(Y) · Y )\n= det X1/d · ∑\ni,j\n(u>i vj) 2 · ( li det Y 1/d\ntj det X1/d − log li det Y\n1/d tj det X1/d − d ) . (104)\nTo check it, we use the fact that, since U and V are orthonormal,\n∑\ni,j\n(u>i vj) 2 · log li det Y\n1/d\ntj det X1/d\n= ∑\ni,j\n(u>i vj) 2 · log li −\n∑\ni,j\n(u>i vj) 2 · log det X1/d\n+ ∑\ni,j\n(u>i vj) 2 · log det Y1/d −\n∑\ni,j\n(u>i vj) 2 · log tj\n= ∑\ni\nlog li − d · log det X1/d\n︸ ︷︷ ︸ =0\n+ d · log det Y1/d − ∑\nj\nlog lj\n︸ ︷︷ ︸ =0\n= 0 , (105)\nwhich yields\ng(X) ·Dϕ ( 1\ng(X) · X‖ 1 g(Y) · Y ) = det X1/d · ∑\ni,j\n(u>i vj) 2 · ( li det Y 1/d tj det X1/d − d )\n= det Y1/d · ∑\ni,j\n(u>i vj) 2 · li tj − d · det X1/d\n= det Y1/d · tr ( XY−1 ) − d · det X1/d , (106)\nwhich is equal to Equation (103)."
    }, {
      "heading" : "D Going deep: higher-order identities",
      "text" : "We can generalize Theorem 1 to higher order identities. For this, consider k > 0 an integer, and let g1, g2, ..., gk : X → R∗ be a sequence of differentiable functions. For any `, `′ ∈ [k]∗ such that ` ≤ `′, we let g̃`,`′ be defined recursively as:\ng̃`,`′(x) . =\n{ g̃`−1,`′(x) · g`′−(`−1) ( 1 g̃`−1,`′ (x) · x )\nif 1 < ` ≤ `′ , g`′(x) if ` = 1 ,\n(107)\nand, for any ` ∈ [k],\nϕ†(`)(x) . =\n{ g`(x) · ϕ†(`−1) ( 1 g`(x) · x )\nif 0 < ` ≤ k , ϕ(x) if ` = 0 .\n(108)\nNotice that even when all g. are linear, this does not guarantee that some g̃`,`′ for ` 6= 1 is going to be linear. However, if for example g`′ is linear and all “preceeding” g` (` ≤ `′) are homogeneous of degree 1, then all g̃`,`′ (∀` ≤ `′) are linear.\nCorollary 17 For any k ∈ N∗, let ϕ : X→ R be convex differentiable, and g` : X→ R∗ (` ∈ [k]) a sequence of k differentiable functions. Then the following relationship holds, for any `, `′ ∈ [k]∗ with ` ≤ `′:\ng̃`,`′(x) ·Dϕ†(`′−`) ( 1\ng̃`,`′(x) · x‖ 1 g̃`,`′(y) · y ) = Dϕ†(`′) (x‖y) ,∀x,y ∈ X , (109)\nwith g̃`,`′ defined as in Equation (107) and ϕ†(` ′) defined as in Equation (108), if and only if at least one of the two following conditions hold:\n(i) g̃`,`′ is linear on X;\n(ii) ϕ†(` ′−`) is positive homogeneous of degree 1 on X`,`′ . = {(1/g̃`,`′(x)) · x : x ∈ X}.\nWe check that whenever ϕ is convex and all g. are non-negative, then all ϕ†(`) are convex (∀` ∈ [k]). To prove this, we choose `′ = ` and rewrite Equation (3), which brings, since ϕ†(` ′−`) = ϕ†(0) = ϕ,\ng̃`,`(x) ·Dϕ ( 1\ng̃`,`(x) · x‖ 1 g̃`,`(y) · y ) = Dϕ†(`) (x‖y) ,∀x,y ∈ X . (110)\nSince ϕ is convex, a sufficient condition to prove our result is to show that g̃`,` is non-negative — which will prove that the right hand side of (110) is non-negative, and therefore ϕ†(`) is convex —. This can easily be proven by induction from the expression of g̃`,`′ in (107) and the fact that all g. are non-negative.\nOne interesting candidate for simplification is when all g. are the same linear function, say g`(x) = a>x+ b,∀` ∈ [k]. In this case, we have indeed:\ng̃`,`′(x) = a >x+ b · g̃`−1,`′(x)\n= b` + a>x · `−1∑\nj=1\nbj , (111)\nϕ†(` ′)(x) =  b`′ + a>x · `′−1∑\nj=1\nbj   · ϕ ( 1 b`′ + a>x ·∑`′−1j=1 bj · x ) . (112)\nProof [Proof of Corollary 17] To check eq. (4), we first remark (`′ being fixed) that it holds for ` = 1 (this is eq. (4)), and then proceed by an induction from the induction base hypothesis that, for some ` ≤ `′,\nϕ†(` ′)(x) = g̃`,`′(x) · ϕ†(`\n′−`) ( 1 g̃`,`′(x) · x ) . (113)\nWe now have\nϕ†(` ′)(x)\n= g̃`+1,`′(x)\ng`′−` (\n1 g̃`,`′ (x)\n· x ) · ϕ†(`′−`)\n( 1 g̃`,`′(x) · x )\n(114)\n= g̃`+1,`′(x)\ng`′−` (\n1 g̃`,`′ (x)\n· x ) · g`′−`\n( 1 g̃`,`′(x) · x ) · ϕ†(`′−(`+1))   1 g̃`,`′(x)g`′−` ( 1 g̃`,`′ (x) · x ) · x   (115)\n= g̃`+1,`′(x) · ϕ†(` ′−(`+1))   1 g̃`,`′(x)g`′−` ( 1 g̃`,`′ (x) · x ) · x  \n= g̃`+1,`′(x) · ϕ†(` ′−(`+1))\n( 1 g̃`+1,`′(x) · x ) . (116)\nEq. (114) comes from eq. (113) and the definition of g̃` in (107), eq. (115) comes from the definition of ϕ†(` ′−`) in (108), eq. (116) is a second use of the definition of g̃` in (107).\nNotice the eventual high non-linearities introduced by the composition in eqs (107,108), which justifies the \"deep\" characterization."
    }, {
      "heading" : "E Additional application: exponential families",
      "text" : "Let ϕ be the cumulant function of a regular ϕ-exponential family with pdf pϕ(.|θ), where θ ∈ X is its natural parameter. Let Ω(.) be a norm on X. Let XΩ be the image of the application from X onto the Ω-ball of unit norm defined by x 7→ (1/Ω(x)) · x. Let θΩ be the image of θ ∈ X. For any two θ,θ′ ∈ X, let\nKLϕ(θ‖θ′) .= ∫ pϕ(x|θ) log\npϕ(x|θ) pϕ(x|θ′) dx (117)\nbe the KL divergence between the two densities pϕ(.|θ) and pϕ(.|θ′).\nLemma 18 For any convex ϕ which is restricted positive 1-homogeneous on XΩ, the KL-divergence between two members of the same ϕ-exponential family satisfies:\nKLϕ(θΩ‖θ′Ω) = 1 Ω(θ) ·Dϕ†(θ′‖θ) . (118)\nProof We know that KL(θ‖θ′) = Dϕ(θ′‖θ) Boissonnat et al. [2010]. Hence,\nDϕ†(θ ′‖θ) = Ω(θ) ·Dϕ\n( 1\nΩ(θ) · θ′‖ 1 Ω(θ′) · θ )\n= Ω(θ) ·Dϕ(θ′Ω‖θΩ) = Ω(θ) ·KLϕ(θΩ‖θ′Ω) , (119)\nas claimed.\nThe interest in Lemma 18 is to provide an integral-free expression of the KL-divergence when natural parameters are scaled by non-trivial transformations. Furthermore, even when Dϕ† may not be a Bregman divergence, it still bears the same analytical form, which still may be useful for formal derivations."
    }, {
      "heading" : "F Additional application: computational geometry, nearest neighbor rules",
      "text" : "Two important objects of central importance in (computational) geometry are balls and Voronoi diagrams induced by a distortion, with which we can characterize the topological and computational aspects of major structures (Voronoi diagrams, triangulations, nearest neighbor topologies, etc.) [Boissonnat et al., 2010].\nSince a Bregman divergence is not necessarily symmetric, there are two types of (dual) balls that can be defined, the first or second types, where the variable x is respectively placed in the left or right position. The first type Bregman balls are convex while the second type are not necessarily convex. A Bregman ball of the second type (with center c and \"radius\" r) is defined as:\nB′(c, r|X, ϕ) .= {x ∈ X : Dϕ(c‖x) ≤ r} . (120)\nIt turns out that any divergence Dϕ† induces a ball of the second type, which is not necessarily analytically a Bregman ball (when ϕ† is not convex), but turns out to define the same ball as a Bregman ball over transformed coordinates. In other words and to be a little bit more specific,\n\"any x belongs to the ball of the second type induced by Dϕ† over X iff (1/g(x)) · x (∈ Xg) belongs to the Bregman ball of the second type induced by Dϕ over Xg .\"\nTheorem 19 Let (ϕ, g, ϕ†) satisfy the conditions of Theorem 1, with g non negative. Then\nB′(c, r|ϕ†,X) = B′ ( c, r\ng(c)\n∥∥∥∥ϕ,Xg ) . (121)\nProof From Theorem 1, we have\nDϕ†(c‖x) ≤ r (122)\niff\nDϕ\n( 1 g(c) · c ∥∥∥∥ 1 g(c) · x ) ≤ 1 g(c) · r . (123)\nHence,\nB′(c, r|ϕ†,X) = {x ∈ Xg : Dϕ(c/g(c)‖x/g(x)) ≤ r/g(c)} = B′(c, r/g(c)|ϕ,Xg) , (124)\nas claimed.\nThis property is not true for balls of the first type. What Theorem 19 says is that the topology induced by Dϕ† over X is just no different from that induced by Dϕ over Xg .\nLet us now investigate Bregman Voronoi diagrams. In the same way as there exists two types of Bregman balls, we can define two types of Bregman Voronoi diagrams that depend on the equation of the Bregman bisector [Boissonnat et al., 2010]. Of particular interest is the Bregman bisector of the first type:\nBBϕ(x,y|X) = {z ∈ X : Dϕ(z‖x) = Dϕ(z‖y)} . (125)\nIt turns out that any divergence Dϕ† induces a bisector of the first type which is not necessarily analytically a Bregman bisector (when ϕ† is not convex), but turns out to define the same bisector as a Bregman bisector over transformed coordinates. Again, we get more precisely\n\"any x belongs to a Bregman bisector of the first type induced by Dϕ† over X iff (1/g(x)) · x (∈ Xg) belongs to the corresponding Bregman bisector of the first type induced by Dϕ over Xg .\"\nTheorem 20 Let (ϕ, g, ϕ†) satisfy the conditions of Theorem 1. Then\nBBϕ†(x,y|X) = BBϕ(x,y|Xg) . (126)\n(proof similar to Theorem 19) This property is not true for Bregman bisectors of the second type. Theorems 19, 20 have several important algorithmic consequences, some of which are listed now:\n• the Voronoi diagram (resp. Delaunay triangulation) of the first type associated to ϕ† can be constructed via the Voronoi diagram (resp. Delaunay triangulation) of the first type associated to ϕ [Boissonnat et al., 2010];\n• range search using ball trees on Dϕ† can be efficiently implemented using Bregman divergence Dϕ on Xg [Cayton, 2009];\n• the minimum enclosing ball problem, the one-class clustering problem (an important problem in machine learning), with balls of the second type on Dϕ† can be solved via the minimum Bregman enclosing ball problem on Dϕ [Nock and Nielsen, 2005]."
    }, {
      "heading" : "G Review: binary density ratio estimation",
      "text" : "For completeness, we quickly review the central result of Menon and Ong [2016, Proposition 3]. Let (P,Q, π) be densities giving P(X|Y = 1),P(X = x|Y = −1),P(Y = 1) respectively, and M giving P(X = x) accordingly. Let r(x) .= P(X = x|Y = 1)/P(X = x|Y = −1) be the density ratio of the class-conditional densities, and η(x) . = P[Y = 1|X = x] be the class-probability function. Then, we have the following, which extends [Menon and Ong, 2016, Proposition 6] for the case π 6= 12 .\nLemma 21 Given a class-probability estimator η̂ : X→ [0, 1], let the density ratio estimator r̂ be\nr̂(x) = 1− π π · η̂(x) 1− η̂(x) . (127)\nThen for any convex differentiable ϕ : [0, 1]→ R,\nEX∼M [Dϕ(η(X)‖η̂(X))] = π · EX∼Q [ Dϕ†(r(X)‖r̂(X)) ] . (128)\nwhere ϕ† is as per Equation 4 with g(z) .= 1−ππ + z .\nProof [Proof of Lemma 21] Note that\n1 g(r(x)) · r(x) = πP(X = x|Y = −1) P(X = x) · P(X = x|Y = 1) P(X = x|Y = −1)\n= πP(X = x|Y = 1) P(X = x) = η(x) , (129)\nand furthermore\nP(X = x) = (1− π)P(X = x|Y = −1) + πP(X = x|Y = 1)\n= π · (\n1− π π + P(X = x|Y = 1) P(X = x|Y = −1)\n) · P(X = x|Y = −1)\n= π · g(r(x)) · P(X = x|Y = −1) . (130)\nSo,\nEM [Dϕ(η(X)‖η̂(X))] = π · EQ [g(r(X)) ·Dϕ(η(X)‖η̂(X))] (131)\n= π · EQ [ g(r(X)) ·Dϕ ( 1\ng(r(X)) · r(X)\n∥∥∥∥ η̂(X) )]\n(132)\n= π · EQ [ g(r(X)) ·Dϕ ( 1\ng(r(X)) · r(X)\n∥∥∥∥ 1\ng(r̂(X)) · r̂(X)\n)] (133)\n= π · EQ [ Dϕ†(r(X)‖r̂(X)) ] , (134)\nas claimed. Equation (131) comes from (130), Equation (132) comes from (129), Equation (133) comes from (127) and the definition of g. Equation (134) comes from Theorem 1, noting that g is linear."
    }, {
      "heading" : "H Additional experiments",
      "text" : "H.1 Multiclass density ratio experiments\nWe consider a synthetic multiclass density ratio estimation problem. We fix X = R2, and consider C = 3 classes. We consider a distribution where the class-conditionals Pr(X|Y = c) are multivariate Gaussians with means µc and covariance σ2c · Id. As the class-conditionals have a closed form, we can explicitly compute η, as well the density ratio r to the reference class c∗ = C.\nFor fixed class prior π = Pr(Y = c), we draw NTr samples from Pr(X,Y). From this, we estimate the classprobability η̂ using multiclass logistic regression. This can be seen as minimising EX∼M [Dϕ(η(X)‖η̂(X))] where ϕ(z) = ∑ i zi log zi is the generator for the KL-divergence.\nWe then use Equation 6 to estimate the density ratios r̂ from η̂. On a fresh sample of Nte instances from Pr(X,Y), we estimate the right hand side of Lemma 2, viz. EX∼PC [ Dϕ†(r(X)‖r̂(X)) ] , where ϕ† uses the g as specified in Lemma 2. From the result of Lemma 2, we expect this divergence to be small when η̂ is a good estimator of η.\nWe perform the above for sample sizes N ∈ {44, 45, . . . , 410}, with NTr = 0.8N and NTe = 0.2N . For each sample size, we perform T = 25 trials, where in each trial we randomly draw π uniformly over (1/C)1 + (1− 1/C) · [0, 1]C , µc from 0.1 ·N (0, 1), and σc uniformly from [0.5, 1]. Figure 4 summarises the mean divergence across the T trials for each sample size. We see that, as expected, with more training samples the divergence decreases in a monotone fashion.\nH.2 Adaptive filtering experiments\nTables 8 – 13 present in extenso the experiments of p-LMS vs DN-p-LMS, as a function of (p, q), whether target u is sparse or not, and the misestimation factor ρ for Xp. We refer to Kivinen et al. [2006] for the formal definitions used for sparse / dense targets as well as for the experimental setting, which we have reproduced with the sole difference that the signal changes periodically each 1 000 iterations."
    }, {
      "heading" : "I Comment: Theorem 1 is a scaled isometry in disguise (sometimes)",
      "text" : "Theorem 1 states in fact an isometry under some conditions, but an adaptive one in the sense that metrics involved rely on all parameters, and in particular on the points involved in the divergences (See Figure 5). Indeed, a simple Taylor expansion of the equation (2) (main file) shows that any such Bregman distortion with a twice differentiable generator can be expressed as:\nDϕ(x‖y) = 1 2 · (x− y)>Hϕ(x− y) , (135)\nfor some value of the Hessian Hϕ depending on x,y (see for example [Kivinen et al., 2006, Appendix I], [Amari and Nagaoka, 2000]). Hence, under the constraint that both ϕ and ϕ† are twice differentiable, eq. (3) becomes\ng(x) · ( 1\ng(x) · x− 1 g(y) · y )> Hϕ ( 1 g(x) · x− 1 g(y) · y ) = (x− y)>Hϕ†(x− y) . (136)\nAssuming g non-negative (which, by the way, enforces the convexity of ϕ†), we get by taking square roots,\n√ g(x) · ∥∥∥∥ 1 g(x) · x− 1 g(y) · y ∥∥∥∥\nHϕ = ‖x− y‖H ϕ† , (137)\nwhich is a scaled isometry relationship between Xg (left) and X (right), but again the metrics involved depend on the arguments. Nevertheless, eq. (137) displays a sophisticated relationship between distances in Xg and in X which may prove useful in itself."
    } ],
    "references" : [ {
      "title" : "Bregman divergences and triangle inequality",
      "author" : [ "S. Acharyya", "A. Banerjee", "D. Boley" ],
      "venue" : "In SDM,",
      "citeRegEx" : "Acharyya et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Acharyya et al\\.",
      "year" : 2013
    }, {
      "title" : "Loss functions for binary class probability estimation and classification",
      "author" : [ "J.-D. Boissonnat", "F. Nielsen", "R. Nock" ],
      "venue" : "Bregman Voronoi diagrams. DCG,",
      "citeRegEx" : "Boissonnat et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Boissonnat et al\\.",
      "year" : 2003
    }, {
      "title" : "Spherical averages and applications to spherical splines and interpolation",
      "author" : [ "S.-R. Buss", "J.-P. Fillmore" ],
      "venue" : "Structure and applications,",
      "citeRegEx" : "Buss and Fillmore.,? \\Q2005\\E",
      "shortCiteRegEx" : "Buss and Fillmore.",
      "year" : 2005
    }, {
      "title" : "Spherical k-means++ clustering",
      "author" : [ "Y. Endo", "S. Miyamoto" ],
      "venue" : "In Proc. of the 12 MDAI, pages 103–114,",
      "citeRegEx" : "Endo and Miyamoto.,? \\Q2008\\E",
      "shortCiteRegEx" : "Endo and Miyamoto.",
      "year" : 2008
    }, {
      "title" : "Revisiting Frank-Wolfe: Projection-free sparse convex optimization",
      "author" : [ "M. Jaggi" ],
      "venue" : "ICML,",
      "citeRegEx" : "Jaggi.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jaggi.",
      "year" : 2013
    }, {
      "title" : "Linking losses for class-probability and density ratio estimation",
      "author" : [ "Menon", "C.-S. Ong" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Menon and Ong.,? \\Q2009\\E",
      "shortCiteRegEx" : "Menon and Ong.",
      "year" : 2009
    }, {
      "title" : "Bregman divergences and surrogates for learning",
      "author" : [ "R. Nock", "F. Nielsen" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Nock and Nielsen.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nock and Nielsen.",
      "year" : 2005
    }, {
      "title" : "Information, divergence and risk for binary experiments",
      "author" : [ "M. Reid", "R. Williamson" ],
      "venue" : "JMLR, 12:731–817,",
      "citeRegEx" : "Reid and Williamson.,? \\Q2011\\E",
      "shortCiteRegEx" : "Reid and Williamson.",
      "year" : 2011
    }, {
      "title" : "Hyperbolic centroidal Voronoi tessellation",
      "author" : [ "G. Rong", "M. Jin", "X. Guo" ],
      "venue" : "th ACM SPM,",
      "citeRegEx" : "2010",
      "shortCiteRegEx" : "2010",
      "year" : 2010
    }, {
      "title" : "The dynamics of coarsening in highly anisotropic systems: Si particles in Al−Si liquids",
      "author" : [ "A.-J. Shahani", "E.-B. Gulsoy", "V.-J. Roussochatzakis", "J.-W. Gibbs", "J.-L. Fife", "P.-W. Voorhees" ],
      "venue" : "Acta Materialia,",
      "citeRegEx" : "Shahani et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shahani et al\\.",
      "year" : 2015
    }, {
      "title" : "A mixture of Manhattan frames: Beyond the Manhattan world",
      "author" : [ "J. Straub", "G. Rosman", "O. Freifeld", "J.-J. Leonard", "J.-W. Fisher III" ],
      "venue" : "In Proc. of the 27 IEEE CVPR,",
      "citeRegEx" : "Straub et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Straub et al\\.",
      "year" : 2014
    }, {
      "title" : "Real-time Manhattan world rotation estimation in 3d",
      "author" : [ "J. Straub", "N. Bhandari", "J.-J. Leonard", "J.-W. Fisher III" ],
      "venue" : "In Proc. of the 27 IROS,",
      "citeRegEx" : "Straub et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Straub et al\\.",
      "year" : 2015
    }, {
      "title" : "Small-variance nonparametric clustering on the hypersphere",
      "author" : [ "J. Straub", "T. Campbell", "J.-P. How", "J.-W. Fisher III" ],
      "venue" : "In Proc. of the 28 IEEE CVPR,",
      "citeRegEx" : "Straub et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Straub et al\\.",
      "year" : 2015
    }, {
      "title" : "A Dirichlet process mixture model for spherical data",
      "author" : [ "J. Straub", "J. Chang", "O. Freifeld", "J.-W. Fisher III" ],
      "venue" : "In Proc. of the 18 AISTATS,",
      "citeRegEx" : "Straub et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Straub et al\\.",
      "year" : 2015
    }, {
      "title" : "Subspace clustering",
      "author" : [ "R. Vidal" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Vidal.,? \\Q2011\\E",
      "shortCiteRegEx" : "Vidal.",
      "year" : 2011
    }, {
      "title" : "First-order methods for geodesically convex optimization",
      "author" : [ "H. Zhang", "S. Sra" ],
      "venue" : null,
      "citeRegEx" : "Zhang and Sra.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang and Sra.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : ", 2016], and computational geometry [Boissonnat et al., 2010]. Despite these being very different applications, many of these algorithms and their analyses basically rely on three beautiful analytic properties of Bregman divergences, properties that we summarize for differentiable scalar convex functions φ with derivative φ′, conjugate φ, and divergence Dφ: • the triangle equality: Dφ(x‖y) +Dφ(y‖z)−Dφ(x‖z) = (φ′(z)− φ′(y))(x− y); • the dual symmetry property: Dφ(x‖y) = Dφ?(φ′(y)‖φ′(x)); • the right-centroid (population minimizer) is the average: arg minμ E[Dφ(X‖μ)] = E[X]. Casting a problem as a Bregman minimisation allows one to employ these properties to simplify analysis; for example, by interpreting mirror descent as applying a particular Bregman regulariser, Beck and Teboulle [2003] relied on the triangle equality above to simplify its proof of convergence.",
      "startOffset" : 37,
      "endOffset" : 799
    }, {
      "referenceID" : 1,
      "context" : ", 2016], and computational geometry [Boissonnat et al., 2010]. Despite these being very different applications, many of these algorithms and their analyses basically rely on three beautiful analytic properties of Bregman divergences, properties that we summarize for differentiable scalar convex functions φ with derivative φ′, conjugate φ, and divergence Dφ: • the triangle equality: Dφ(x‖y) +Dφ(y‖z)−Dφ(x‖z) = (φ′(z)− φ′(y))(x− y); • the dual symmetry property: Dφ(x‖y) = Dφ?(φ′(y)‖φ′(x)); • the right-centroid (population minimizer) is the average: arg minμ E[Dφ(X‖μ)] = E[X]. Casting a problem as a Bregman minimisation allows one to employ these properties to simplify analysis; for example, by interpreting mirror descent as applying a particular Bregman regulariser, Beck and Teboulle [2003] relied on the triangle equality above to simplify its proof of convergence. Another intriguing possibility is that one may derive reductions amongst learning problems by connecting their underlying Bregman minimisations. Menon and Ong [2016] recently established how (binary) density ratio estimation (DRE) can be exactly reduced to class-probability estimation (CPE).",
      "startOffset" : 37,
      "endOffset" : 1041
    }, {
      "referenceID" : 0,
      "context" : "Hence, Bregman divergences can embed several distances in a different — and arguably less involved — way than the transformations known to date [Acharyya et al., 2013].",
      "startOffset" : 144,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : "Hence, Bregman divergences can embed several distances in a different — and arguably less involved — way than the transformations known to date [Acharyya et al., 2013]. As with the aforementioned key properties of Bregman divergences, Theorem 1 has potentially wide applicability. We present three such novel applications (see Table 1) to vastly different problems: • a reduction of multiple density ratio estimation to multiclass-probability estimation (§3), generalising the results of Menon and Ong [2016] for the binary label case, • a projection-free yet norm-enforcing mirror gradient algorithm (enforced norms are those of mirrored vectors and of the offset) with guarantees for adaptive filtering (§4), and • a seeding approach for clustering on positively or negatively (constant) curved manifolds based on a popular seeding for flat manifolds and with the same approximation guarantees (§5).",
      "startOffset" : 145,
      "endOffset" : 509
    }, {
      "referenceID" : 5,
      "context" : "For the special case where X = R, and g(x) = 1 + x, Theorem 1 is exactly Menon and Ong [2016, Lemma 2] (c.f. Equation 1). We wish to highlight a few points with regard to our more general result. First, the “distortion” generator φ† may be1 non-convex, as the following illustrates. Example. Suppose φ(x) = 2‖x‖2 corresponds to the generator for squared Euclidean distance. Then, for g(x) = 1 + 1>x, we have φ†(x) = 12 · ‖x‖22 1+1>x , which is non-convex on X = R . When φ† is non-convex, the right hand side in Equation 3 is an object that ostensibly bears only a superficial similarity to a Bregman divergence; it is somewhat remarkable that Theorem 1 shows this general “distortion” between a pair (x,y) to be entirely equivalent to a (scaling of a) Bregman divergence between some transformation of the points. Second, when g is linear, Equation 3 holds for any convex φ. (This was the case considered in Menon and Ong [2016].) When g is non-linear, however, φ must be chosen carefully so that (φ, g) satisfies the restricted homogeneity conditon2 of Equation 5.",
      "startOffset" : 73,
      "endOffset" : 930
    }, {
      "referenceID" : 14,
      "context" : "Our final application can be related to two problems that have received a steadily growing interest over the past decade in unsupervised machine learning: clustering on a non-linear manifold [Dhillon and Modha, 2001], and subspace custering [Vidal, 2011].",
      "startOffset" : 241,
      "endOffset" : 254
    }, {
      "referenceID" : 10,
      "context" : "We emphasize the fact that the clustering problem has significant practical impact for d as small as 2 in computer vision [Straub et al., 2014].",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "Second, the fact that the manifold has non-zero curvature essentially prevents the direct use of Euclidean optimization algorithms [Zhang and Sra, 2016] — put simply, the average of two points",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "Our final application can be related to two problems that have received a steadily growing interest over the past decade in unsupervised machine learning: clustering on a non-linear manifold [Dhillon and Modha, 2001], and subspace custering [Vidal, 2011]. We consider two fundamental manifolds investigated by Galperin [1993] to compute centers of mass from relativistic theory: the sphere S and the hyperboloid H, the former being of positive curvature, and the latter of negative curvature.",
      "startOffset" : 242,
      "endOffset" : 326
    }, {
      "referenceID" : 3,
      "context" : "The key to using the approximation property of k-means++ relies on the existence of a coordinate system on the sphere for which the cluster centroid is just the average of the cluster points (polar coordinates), an average that eventually has to be rescaled if the coordinate system is not that one [Dhillon and Modha, 2001, Endo and Miyamoto, 2015]. The existence of this coordinate system makes that the proof of Arthur and Vassilvitskii [2007] (and in particular the key Lemmata 3.",
      "startOffset" : 325,
      "endOffset" : 447
    }, {
      "referenceID" : 3,
      "context" : "The key to using the approximation property of k-means++ relies on the existence of a coordinate system on the sphere for which the cluster centroid is just the average of the cluster points (polar coordinates), an average that eventually has to be rescaled if the coordinate system is not that one [Dhillon and Modha, 2001, Endo and Miyamoto, 2015]. The existence of this coordinate system makes that the proof of Arthur and Vassilvitskii [2007] (and in particular the key Lemmata 3.2 and 3.3) can be carried out without modification to yield the same approximation ratio as that of Arthur and Vassilvitskii [2007] if the distortion at hand is the squared Euclidean distance, which turns out to be Drec(.",
      "startOffset" : 325,
      "endOffset" : 616
    }, {
      "referenceID" : 2,
      "context" : "Row III — As in Buss and Fillmore [2001], we assume ‖x‖2 ≤ π, or we renormalize or change the radius of the ball) We first lift the data points using the Sphere lifting map R 3 x 7→ x ∈ R: x .",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Proof We know that KL(θ‖θ′) = Dφ(θ′‖θ) Boissonnat et al. [2010]. Hence, Dφ†(θ ′‖θ) = Ω(θ) ·Dφ ( 1 Ω(θ) · θ′‖ 1 Ω(θ′) · θ )",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : ", 2010]; • range search using ball trees on Dφ† can be efficiently implemented using Bregman divergence Dφ on Xg [Cayton, 2009]; • the minimum enclosing ball problem, the one-class clustering problem (an important problem in machine learning), with balls of the second type on Dφ† can be solved via the minimum Bregman enclosing ball problem on Dφ [Nock and Nielsen, 2005].",
      "startOffset" : 348,
      "endOffset" : 372
    } ],
    "year" : 2016,
    "abstractText" : "Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. This paper explores the use of Bregman divergences to establish reductions between such algorithms and their analyses. We present a new scaled isodistortion theorem involving Bregman divergences (scaled Bregman theorem for short) which shows that certain “Bregman distortions” (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation, while admissible data include scalars, vectors and matrices. Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning.",
    "creator" : "LaTeX with hyperref package"
  }
}