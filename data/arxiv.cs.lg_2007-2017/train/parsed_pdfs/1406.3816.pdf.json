{
  "name" : "1406.3816.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning",
    "authors" : [ "Francesco Orabona" ],
    "emails" : [ "francesco@orabona.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Stochastic Gradient Descent (SGD) algorithms are gaining more and more importance in the Machine Learning community as efficient and scalable machine learning tools. There are two possible ways to use a SGD algorithm: to optimize a batch objective function, e.g. [23], or to directly optimize the generalization performance of a learning algorithm, in a stochastic approximation way [20]. The second use is the one we will consider in this paper. It allows learning over streams of data, coming Independent and Identically Distributed (IID) from a stochastic source. Moreover, it has been advocated that SGD theoretically yields the best generalization performance in a given amount of time compared to other more sophisticated optimization algorithms [6].\nYet, both in theory and in practice, the convergence rate of SGD for any finite training set critically depends on the step sizes used during training. In fact, often theoretical analysis assumes the use of optimal step sizes, rarely known in reality, and in practical applications wrong step sizes can result in arbitrary bad performance. While in finite hypothesis spaces simple optimal strategies are known [2], in infinite dimensional spaces the only attempts to solve this problem achieve convergence only in the realizable case, e.g. [25], or assume prior knowledge of intrinsic (and unknown) characteristic of the problem [24, 34, 33, 31, 29]. The only known practical and theoretical way to achieve optimal rates in infinite Reproducing Kernel Hilbert Space (RKHS) is to use some form of cross-validation to select the step size that corresponds to a form of model selection [26, Chapter 7.4]. However, cross-validation techniques would result in a slower training procedure partially neglecting the advantage of the stochastic training. A notable exception is the algorithm in [21], that keeps the step size constant and uses the number of epochs on the training set as a regularization procedure. Yet, the number of epochs is decided through the use of a validation set [21].\nar X\niv :1\n40 6.\n38 16\nv1 [\ncs .L\nG ]\nNote that the situation is exactly the same in the batch setting where the regularization takes the role of the step size. Even in this case, optimal rates can be achieved only when the regularization is chosen in a problem dependent way [11, 32, 27, 16].\nOn a parallel route, the Online Convex Optimization (OCO) literature studies the possibility to learn in a scenario where the data are not IID [36, 9]. It turns out that this setting is strictly more difficult than the IID one and OCO algorithms can also be used to solve the corresponding stochastic problems [8]. The literature on OCO focuses on the adversarial nature of the problem and on various ways to achieve adaptivity to its unknown characteristics [1, 13].\nThis paper is in between these two different worlds: We extend tools from OCO to design a novel stochastic parameter-free algorithm able to obtain optimal finite sample convergence bounds in infinite dimensional RKHS. This new algorithm, called Parameter-free STOchastic Learning (PiSTOL), has the same complexity as the plain stochastic gradient descent procedure and implicitly achieves the model selection while training, with no parameters to tune nor the need for cross-validation. The core idea is to change the step sizes over time in a data-dependent way. As far as we know, this is the first algorithm of this kind to have provable optimal convergence rates.\nThe rest of the paper is organized as follows. After introducing some basic notations (Sec. 2), we will explain the basic intuition of the proposed method (Sec. 3). Next, in Sec. 4 we will describe the PiSTOL algorithm and its regret bounds in the adversarial setting and in Sec. 5 we will show its convergence results in the stochastic setting. The detailed discussion of related work is deferred to Sec. 6. Finally, we show some empirical results and draw the conclusions in Sec. 7."
    }, {
      "heading" : "2 Problem Setting and Definitions",
      "text" : "Let X ⊂ Rd a compact set and HK the RKHS associated to a Mercer kernel K : X × X → R implementing the inner product 〈· , ·〉K . The inner product is defined so that it satisfies the reproducing property, 〈K(x, ·) , f(·)〉K = f(x).\nPerformance is measured w.r.t. a loss function ` : R → R+. We will consider L-Lipschitz losses, that is |`(x) − `(x′)| ≤ L|x − x′|, ∀x, x′ ∈ R, and H-smooth losses, that is differentiable losses with the first derivative H-Lipschitz. Note that a loss can be both Lipschitz and smooth. A vector x is a subgradient of a convex function ` at v if `(u)− `(v) ≥ 〈u− v,x〉 for any u in the domain of `. The differential set of ` at v, denoted by ∂`(v), is the set of all the subgradients of ` at v. 1(Φ) will denote the indicator function of a Boolean predicate Φ.\nIn the OCO framework, at each round t the algorithm receives a vector xt ∈ X , picks a ft ∈ HK , and pays `t(ft(xt)), where `t is a loss function. The aim of the algorithm is to minimize the regret, that is the difference between the cumulative loss of the algorithm, ∑T t=1 `t(ft(xt)), and the cumulative loss of an\narbitrary and fixed competitor h ∈ HK , ∑T t=1 `t(h(xt)).\nFor the statistical setting, let ρ a fixed but unknown distribution on X × Y , where Y = [−1, 1]. A training set {xt, yt}Tt=1 will consist of samples drawn IID from ρ. Denote by fρ(x) := ∫ Y ydρ(y|x) the regression function, where ρ(·|x) is the conditional probability measure at x induced by ρ. Denote by ρX the marginal probability measure on X and let L2ρX be the space of square integrable functions with respect to ρX , whose norm is denoted by ‖f‖L2ρX := √∫ X f\n2(x)dρX . Note that fρ ∈ L2ρX . Define the `-risk of f , as E`(f) := ∫ X×Y `(yf(x))dρ. Also, define f ` ρ(x) := arg mint∈R ∫ Y `(yt)dρ(y|x), that gives the optimal `-risk, E`(f `ρ) = inff∈L2ρX E `(f). In the binary classification case, define the misclassification risk of f as R(f) := P (y 6= sign(f(x))). The infimum of the misclassification risk over all measurable f will be called Bayes risk and fc := sign(fρ), called the Bayes classifier, is such thatR(fc) = inff∈L2ρX R(f).\nLet LK : L2ρX → HK the integral operator defined by (LKf)(x) = ∫ X K(x, x ′)f(x′)dρX (x ′). There exists an orthonormal basis {Φ1,Φ2, · · · } of L2ρX consisting of eigenfunctions of LK with corresponding non-negative eigenvalues {λ1, λ2, · · · } and the set {λi} is finite or λk → 0 when k →∞ [12, Theorem 4.7].\nSinceK is a Mercer kernel,LK is compact and positive. Therefore, the fractional power operatorL β K is well defined for any β ≥ 0. We indicate its range space by\nFigure 1: L2ρX , HK , and L β K(L 2 ρX ) spaces, with 0 < β1 < 12 < β2.\nLβK(L2ρX ) := { f = ∞∑ i=1 aiΦi : ∑ i:ai 6=0 a2iλ −2β i <∞ } . (1)\nBy the Mercer’s theorem, we have that L 1 2\nK(L2ρX ) = HK , that is every function f ∈ HK can be written as L 1 2\nKg for some g ∈ L2ρX , with ‖f‖K = ‖g‖L2ρX . On the other hand, by definition of the orthonormal basis, L0K(L2ρX ) = L2ρX . Thus, the smaller β is, the bigger this space of the functions will be,1 see Fig. 1. This space has a key role in our analysis. In particular, we will assume that f `ρ ∈ LβK(L2ρX ) for β > 0, that is\n∃g ∈ L2ρX : f `ρ = L β K(g). (2)"
    }, {
      "heading" : "3 A Gentle Start: ASGD, Optimal Step Sizes, and the Perceptron",
      "text" : "We want to investigate the problem of training a predictor, f̄T , on the training set {xt, yt}Tt=1 in a stochastic way, using each sample only once, to have E`(f̄T ) converge to E`(f `ρ). For the square loss, `(x) = (1−x)2, the Averaged Stochastic Gradient Descent (ASGD) in Algorithm 1 has been proposed as a fast stochastic algorithm to train predictors [35]. ASGD simply goes over all the samples once, updates the predictor with the gradients of the losses, and returns the averaged solution. For ASGD with constant step size 0 < η ≤ 14 , it is immediate to show2 that\nE[E`(f̄T )] ≤ inf h∈HK E`(h) + ‖h‖2K (ηT )−1 + 4η. (3)\nThis result shows the link between step size and regularization: In expectation, the `-risk of the averaged predictor will be close to the `-risk of the best regularized function in HK . Moreover, the amount of regularization depends on the step size used. From (3), one might be tempted to choose η = O(T− 12 ). With this choice, when the number of samples goes to infinity, ASGD would converge to the performance of the best predictor in HK at a rate of O(T− 1 2 ), only if the infimum infh∈HK E`(h) is attained by a function inHK . Note that even with a universal kernel we only have E`(f `ρ) = infh∈HK E`(h) but there is no guarantee that the infimum is attained [26].\nOn the other hand, there is a vast literature examining the general case when (2) holds [11, 24, 34, 32, 7, 4, 33, 27, 16, 31, 29]. Under this assumption, this infimum is attained only when β ≥ 12 , yet it is possible to prove convergence for β > 0. In fact, when (2) holds it is known that minh∈HK [ E`(h) + ‖h‖2K (ηT )−1 ] − E`(f `ρ) = O((ηT )−2β) [12, Proposition 8.5]. Hence, it was observed in [33] that setting η = O(T− 2β 2β+1 )\nin (3), we obtain E[E`(f̄T )]− E`(f `ρ) = O ( T− 2β 2β+1 ) , that is the optimal rate [33, 27]. Hence, the setting η = O(T− 12 ) is optimal only when β = 12 , that is f `ρ ∈ HK . In all the other cases, the convergence rate of ASGD to the optimal `-risk is suboptimal. Unfortunately, β is typically unknown to the learner.\nOn the other hand, using the tools to design self-tuning algorithms, e.g. [1, 13], it may be possible to design an ASGD-like algorithm, able to self-tune its step size in a data-dependent way. Indeed, we would\n1The case that β < 1 implicitly assumes that HK is infinite dimensional. If HK has finite dimension, β is 0 or 1. See also the discussion in [27].\n2For completeness, the proof is in the Appendix.\nAlgorithm 1 Averaged SGD. Parameters: η > 0 Initialize: f1 = 0 ∈ HK for t = 1, 2, . . . do\nReceive input vector xt ∈ X Predict with ŷt = ft(xt) Update ft+1 = ft + ηyt`′(ytŷt)k(xt, ·)\nend for Return f̄T = 1T ∑T t=1 ft\nAlgorithm 2 The Kernel Perceptron. Parameters: None Initialize: f1 = 0 ∈ HK for t = 1, 2, . . . do\nReceive input vector xt ∈ X Predict with ŷt = sign(ft(xt)) Suffer loss 1(ŷt 6= yt) Update ft+1 = ft + yt1(ŷt 6= yt)k(xt, ·)\nend for\nlike an algorithm able to select the optimal step size in (3), that is\nE[E`(f̄T )] ≤ inf h∈HK E`(h) + min η>0 ‖h‖2K (ηT )−1 + 4η = inf h∈HK E`(h) + 4 ‖h‖K T− 1 2 . (4)\nIn the OCO setting, this would correspond to a regret bound of the form O(‖h‖K T 1 2 ). An algorithm that has this kind of guarantee is the Perceptron algorithm [22], see Algorithm 2. In fact, for the Perceptron it is possible to prove the following mistake bound [9]:\nNumber of Mistakes ≤ inf h∈HK T∑ t=1 `h(yth(xt)) + ‖h‖2K + ‖h‖K √√√√ T∑ t=1 `h(yth(xt)), (5)\nwhere `h is the hinge loss, `h(x) = max(1 − x, 0). The Perceptron algorithm is similar to SGD but its behavior is independent of the step size, hence, it can be thought as always using the optimal one. Unfortunately, we are not done yet: While (5) has the right form of the bound, it is not a regret bound, rather only a mistake bound, specific for binary classification. In fact, the performance of the competitor h is measured with a different loss (hinge loss) than the performance of the algorithm (misclassification loss). For this asymmetry, the convergence when β < 12 cannot be proved. Instead, we need an online algorithm whose regret bound scales asO(‖h‖K T 1 2 ), returns the averaged solution, and, thanks to the equality in (4), obtains a convergence rate which would depend on\nmin η>0 ‖h‖2K (ηT )−1 + η. (6)\nThe r.h.s of (6) has exactly the same form of the expression in (3), but with a minimum over η. Hence, we can expect it to always have the optimal rate of convergence. In the next section, we will present such algorithm."
    }, {
      "heading" : "4 PiSTOL: Parameter-free STOchastic Learning",
      "text" : "In this section we describe the PiSTOL algorithm. The pseudo-code is in Algorithm 3. The algorithm builds on recent advancement in unconstrained online learning [28, 18, 15]. It is very similar to a SGD algorithm [35], the main difference being the computation of the solution based on the past gradients, in line 4. Note that the calculation of ‖gt‖2K can be done incrementally, hence, the computational complexity is the same as ASGD in a RKHS, Algorithm 1. For the PiSTOL algorithm we have the following regret bound.3\n3All the proofs are in Appendix.\nAlgorithm 3 PiSTOL: Parameter-free STOchastic Learning. 1: Parameters: a, b, L > 0 2: Initialize: g0 = 0 ∈ HK , α0 = aL 3: for t = 1, 2, . . . do 4: Set ft = gt−1 bαt−1 exp ( ‖gt−1‖2K 2αt−1\n) 5: Receive input vector xt ∈ X 6: Adversarial setting: Suffer loss `t(ft(xt)) 7: Receive subgradient st ∈ ∂`t(ft(xt)) 8: Update gt = gt−1 − stk(xt, ·) and αt = αt−1 + a|st| ‖k(xt, ·)‖K 9: end for\n10: Statistical setting: Return f̄T = 1T ∑T t=1 ft\nTheorem 1. Assume that the sequence of xt satisfies ‖k(xt, ·)‖K ≤ 1 and the losses `t are convex and L-Lipschitz. Let a > 0 such that a ≥ 2.25L. Then, for any h ∈ HK , the following bound on the regret holds for the PiSTOL algorithm\nT∑ t=1 [`t(ft(xt))− `t(h(xt))] ≤‖h‖K √√√√2a(L+ T−1∑ t=1 |st| ) log ( ‖h‖K √ aLT b + 1 ) + bφ ( a−1L ) log (1 + T ) ,\nwhere φ(x) := x2 exp( x2 )(x+1)+2 1−x exp( x2 )−x\n( exp ( x 2 ) (x+ 1) + 2 ) .\nThis theorem shows that PiSTOL has the right dependency on ‖h‖K and T that was outlined in Sec. 3 and its regret bound is also optimal up to √ log log T terms [18]. Moreover, Theorem 1 improves on the results in [18, 15], obtaining an almost optimal regret that depends on the sum of the absolute values of the gradients, rather than on the time T . This is critical to obtain a tighter bound when the losses areH-smooth, as shown in the next Corollary.\nCorollary 1. Under the same assumptions of Theorem 1, if the losses `t are also H-smooth, then4\nT∑ t=1 [`t(ft(xt))− `t(h(xt))] = Õ max ‖h‖ 43K T 13 , ‖h‖K T 14 ( T∑ t=1 `t(h(xt)) + 1 ) 1 4   .\nThis bound shows that, if the cumulative loss of the competitor is small, the regret can grow slower than√ T . It is worse than the regret bounds for smooth losses in [9, 25] because when the cumulative loss of the competitor is equal to 0, the regret still grows as Õ ( ‖f‖ 4 3\nK T 1 3\n) instead of being constant. However, the\nPiSTOL algorithm does not require the prior knowledge of the norm of the competitor function h, as all the ones in [9, 25] do.\nIn the Appendix, we also show a variant of PiSTOL for linear kernels with almost optimal learning rate for each coordinate. Contrary to other similar algorithms, e.g. [13], it is a truly parameter-free one."
    }, {
      "heading" : "5 Convergence Results for PiSTOL",
      "text" : "In this section we will use the online-to-batch conversion to study the `-risk and the misclassification risk of the averaged solution of PiSTOL. We will also use the following definition: ρ has Tsybakov noise exponent\n4For brevity, the Õ notation hides polylogarithmic terms.\nq ≥ 0 [30] iff there exist cq > 0 such that\nPX({x ∈ X : −s ≤ fρ(x) ≤ s}) ≤ cqsq, ∀s ∈ [0, 1]. (7)\nSetting α = qq+1 ∈ [0, 1], and cα = cq + 1, condition (7) is equivalent [32, Lemma 6.1] to:\nPX(sign(f(x)) 6= fc(x)) ≤ cα(R(f)−R(fρ))α, ∀f ∈ L2ρX . (8)\nThese conditions allow for faster rates in relating the expected excess misclassification risk to the expected `-risk, as detailed in the following Lemma that is a special case of [3, Theorem 10].\nLemma 1. Let ` : R→ R+ be a convex loss function, twice differentiable at 0, with `′(0) < 0, `′′(0) > 0, and with the smallest zero in 1. Assume condition (8) is verified. Then for the averaged solution f̄T returned by PiSTOL it holds\nE[R(f̄T )]−R(fc) ≤ (\n32 cα C\n( E[E`(f̄T )]− E`(f `ρ) )) 12−α , C = min { −`′(0), (` ′(0))2\n`′′(0)\n} .\nThe results in Sec. 4 give regret bounds over arbitrary sequences. We now assume to have a sequence of training samples (xt, yt)Tt=1 IID from ρ. We want to train a predictor from this data, that minimizes the `-risk. To obtain such predictor we employ a so-called online-to-batch conversion [8]. For a convex loss `, we just need to run an online algorithm over the sequence of data (xt, yt)Tt=1, using the losses `t(x) = `(ytx), ∀t = 1, · · · , T . The online algorithm will generate a sequence of solutions ft and the online-to-batch conversion can be obtained with a simple averaging of all the solutions, f̄T = 1T ∑T t=1 ft, as for ASGD. The average regret bound of the online algorithm becomes a convergence guarantee for the averaged solution [8]. Hence, for the averaged solution of PiSTOL, we have the following Corollary that is immediate from Corollary 1 and the results in [8].\nCorollary 2. Assume that the samples (xt, yt)Tt=1 are IID from ρ, and `t(x) = `(ytx). Then, under the assumptions of Corollary 1, the averaged solution of PiSTOL satisfies\nE[E`(f̄T )] ≤ inf h∈HK\nE`(h) + Õ ( max { ‖h‖ 4 3\nK T − 23 , ‖h‖K T− 3 4\n( TE`(h) + 1 ) 1 4 }) .\nHence, we have a Õ(T− 23 ) convergence rate to the φ-risk of the best predictor in HK , if the best predictor has φ-risk equal to zero, and Õ(T− 12 ) otherwise. Contrary to similar results in literature, e.g. [25], we do not have to restrict the infimum over a ball of fixed radius in HK and our bounds depends on Õ(‖h‖K) rather than O(‖h‖ 2 K), e.g. [35]. The advantage of not restricting the competitor in a ball is clear: The performance is always close to the best function inHK , regardless of its norm. The logarithmic terms are exactly the price we pay for not knowing in advance the norm of the optimal solution. For binary classification using Lemma 1, we can also prove a Õ(T− 12(2−α) ) bound on the excess misclassification risk in the realizable setting, that is if f `ρ ∈ HK .\nIt would be possible to obtain similar results with other algorithms, as the one in [25], using a doublingtrick approach [9]. However, this would result most likely in an algorithm not useful in any practical application. Moreover, the doubling-trick itself would not be trivial, for example the one used in [28] achieves a suboptimal regret and requires to start from scratch the learning over two different variables, further reducing its applicability in any real-world application.\nAs anticipated in Sec. 3, we now show that the dependency on Õ(‖h‖K) rather than onO(‖h‖ 2 K) gives us the optimal rates of convergence in the general case that f `ρ ∈ LβK(L2ρX ), without the need to tune any parameter. This is our main result.\nTheorem 2. Assume that the samples (xt, yt)Tt=1 are IID from ρ, (2) holds for β ≤ 12 , and `t(x) = `(ytx). Then, under the assumptions of Corollary 1, the averaged solution of PiSTOL satisfies\n• If β ≤ 13 then E[E`(f̄T )]− E`(f `ρ) ≤ Õ ( max { (E`(f `ρ) + 1/T ) β 2β+1T− 2β 2β+1 , T− 2β β+1 }) .\n• If 13 < β ≤ 12 , then E[E`(f̄T )]− E`(f `ρ)\n≤ Õ ( max { (E`(f `ρ) + 1/T ) β 2β+1T− 2β 2β+1 , (E`(f `ρ) + 1/T ) 3β−1 4β T− 1 2 , T− 2β β+1 }) .\n10 1\n10 2\n10 3\n10 4\n10 5\n10 6\n10 7\n10 −3\n10 −2\n10 −1\n10 0\n10 1\nT\nB o u n d\nExcess ℓ-risk bound\nEℓ(fℓρ) = 0 Eℓ(fℓρ) = 0.1 Eℓ(fℓρ) = 1\nFigure 2: Upper bound on the excess `-risk of PiSTOL for β = 1 2 .\nThis theorem guarantees consistency w.r.t. the `-risk. We have that the rate of convergence to the optimal `-risk is Õ(T− 3β2β+1 ), if E`(f `ρ) = 0, and Õ(T− 2β 2β+1 ) otherwise. However, for any finite T the rate of convergence is Õ(T− 2ββ+1 ) for any T = O(E`(f `ρ)− β+1 2β ). In other words, we can expect a first regime at faster convergence, that saturates when the number of samples becomes big enough, see Fig. 2. This is particularly important because often in practical applications the features and the kernel are chosen to have good performance that is low optimal `-risk. Using Lemma 1, we have that the excess misclassification risk is Õ(T− 2β (2β+1)(2−α) ) if E`(f `ρ) 6= 0, and Õ(T− 2β\n(β+1)(2−α) ) if E`(f `ρ) = 0. It is also worth noting that, being the algorithm designed to work in the adversarial setting, we expect its performance to be robust to small deviations from the IID scenario.\nAlso, note that the guarantees of Corollary 2 and Theorem 2 hold simultaneously. Hence, the theoretical performance of PiSTOL is always better than both the ones of SGD with the step sizes tuned with the knowledge of β or with the agnostic choice η = O(T− 12 ). In the Appendix, we also show another convergence result assuming a different smoothness condition.\nRegarding the optimality of our results, lower bounds for the square loss are known [27] under assumption (2) and further assuming that the eigenvalues of LK have a polynomial decay, that is\n(λi)i∈N ∼ i−b, b ≥ 1. (9)\nCondition (9) can be interpreted as an effective dimension of the space. It always holds for b = 1 [27] and this is the condition we consider that is usually denoted as capacity independent, see the discussion in [33, 21]. In the capacity independent setting, the lower bound isO(T− 2β2β+1 ), that matches the asymptotic rates in Theorem 2, up to logarithmic terms. Even if we require the loss function to be Lipschitz and smooth, it is unlikely that different lower bounds can be proved in our setting. Note that the lower bounds are worst case w.r.t. E`(f `ρ), hence they do not cover the case E`(f `ρ) = 0, where we get even better rates. Hence, the optimal regret bound of PiSTOL in Theorem 1 translates to an optimal convergence rate for its averaged solution, up to logarithmic terms, establishing a novel link between these two areas."
    }, {
      "heading" : "6 Related Work",
      "text" : "The approach of stochastically minimizing the `-risk of the square loss in a RKHS has been pioneered by [24]. The rates were improved, but still suboptimal, in [34], with a general approach for locally Lipschitz loss functions in the origin. The optimal bounds, matching the ones we obtain for E`(f `ρ) 6= 0, were obtained for β > 0 in expectation by [33]. Their rates also hold for β > 12 , while our rates, as the ones in [27], saturate at β = 12 . In [29], high probability bounds were proved in the case that 1 2 ≤ β ≤ 1. Note that, while in the range β ≥ 12 , that implies fρ ∈ HK , it is possible to prove high probability bounds [29, 27, 4, 7], the range 0 < β < 12 considered in this paper is very tricky, see the discussion in [27]. In this range no\nhigh probability bounds are known without additional assumptions. All the previous approaches require the knowledge of β, while our algorithm is parameter-free. Also, we obtain faster rates for the excess `-risk, when E`(f `ρ) = 0. Another important difference is that we can use any smooth and Lipschitz loss, useful for example to generate sparse solutions, while the optimal results in [33, 29] are specific for the square loss.\nFor finite dimensional spaces and self-concordant losses, an optimal parameter-free stochastic algorithm has been proposed in [2]. However, the convergence result seems specific to finite dimension.\nThe guarantees obtained from worst-case online algorithms, for example [25], have typically optimal convergence only w.r.t. the performance of the best inHK , see the discussion in [33]. Instead, all the guarantees on the misclassification loss w.r.t. a convex `-risk of a competitor, e.g. the Perceptron’s guarantee, are inherently weaker than the presented ones. To see why, assume that the classifier returned by the algorithm after seeing T samples is fT , these bounds are of the form ofR(fT ) ≤ E`(h)+O(T− 1 2 (‖h‖2K +1)). For simplicity, assume the use of the hinge loss so that easy calculations show that f `ρ = fc and E`(f `ρ) = 2R(fc). Hence, even in the easy case that fc ∈ HK , we have R(fT ) ≤ 2R(fc) + O(T− 1 2 (‖fc‖2K + 1)), i.e. no convergence to the Bayes risk. In the batch setting, the same optimal rates were obtained by [4, 7] for the square loss, in high probability, for β > 12 . In [27], using an additional assumption on the infinity norm of the functions in HK , they give high probability bounds also in the range 0 < β ≤ 12 . The optimal tuning of the regularization parameter is achieved by cross-validation. Hence, we match the optimal rates of a batch algorithm, without the need to use validation methods.\nIn Sec. 3 we saw that the core idea to have the optimal rate was to have a classifier whose performance is close to the best regularized solution, where the regularizer is ‖h‖K . Changing the regularization term from the standard ‖h‖2K to ‖h‖ q K with q ≥ 1 is not new in the batch learning literature. It has been first proposed for classification by [5], and for regression by [16]. Note that, in both cases no computational methods to solve the optimization problem were proposed. Moreover, in [27] it was proved that all the regularizers of the form ‖h‖qK with q ≥ 1 gives optimal convergence rates bound for the square loss, given an appropriate setting of the regularization weight. In particular, [27, Corollary 6] proves that, using the square loss and under assumptions (2) and (9), the optimal weight for the regularizer ‖h‖qK is T − 2β+q(1−β)\n2β+2/b . This implies a very important consequence, not mentioned in that paper: In the the capacity independent setting, that is b = 1, if we use the regularizer ‖h‖K , the optimal regularization weight is T− 1 2 , independent of the exponent of the range space (1) where fρ belongs. Moreover, in the same paper it was argued that “From an algorithmic point of view however, q = 2 is currently the only feasible case, which in turn makes SVMs the method of choice”. Indeed, in this paper we give a parameter-free efficient procedure to train predictors with smooth losses, that implicitly uses the ‖h‖K regularizer. Thanks to this, the regularization parameter does not need to be set using prior knowledge of the problem."
    }, {
      "heading" : "7 Discussion",
      "text" : "Borrowing from OCO and statistical learning theory tools, we have presented the first parameter-free stochastic learning algorithm that achieves optimal rates of convergence w.r.t. the smoothness of the optimal predictor. In particular, the algorithm does not require any validation method for the model selection, rather it automatically self-tunes in an online and data-dependent way.\nEven if this is mainly a theoretical work, we believe that it might also have a big potential in the applied world. Hence, as a proof of concept on the potentiality of this method we have also run few preliminary experiments, to compare the performance of PiSTOL to an SVM using 5-folds cross-validation to select the regularization weight parameter. The experiments were repeated with 5 random shuffles, showing the average and standard deviations over three datasets.5 The latest version of LIBSVM was used to train the SVM [10]. We have that PiSTOL closely tracks the performance of the tuned SVM when a Gaussian kernel is used. Also, contrary to the common intuition, the stochastic approach of PiSTOL seems to have an advantage over the tuned SVM when the number of samples is small. Probably, cross-validation is a poor approximation of the generalization performance in that regime, while the small sample regime does not affect at all the analysis of PiSTOL. Note that in the case of News20, a linear kernel is used over the vectors of size 1355192. The finite dimensional case is not covered by our theorems, still we see that PiSTOL seems to converge at the same rate of SVM, just with a worse constant. It is important to note that the total time the 5-folds cross-validation plus the training with the selected parameter for the SVM on 58000 samples of SensIT Vehicle takes ∼ 6.5 hours, while our unoptimized Matlab implementation of PiSTOL less than 1 hour, ∼ 7 times faster. The gains in speed are similar on the other two datasets.\nThis is the first work we know of in this line of research of stochastic adaptive algorithms for statistical learning, hence many questions are still open. In particular, it is not clear if high probability bounds can be obtained, as the empirical results hint, without additional hypothesis. Also, we only proved convergence w.r.t. the `-risk, however for β ≥ 12 we know that f `ρ ∈ HK , hence it would be possible to prove the stronger convergence results on\n∥∥fT − f `ρ∥∥K , e.g. [29]. Probably this would require a major change in the proof techniques used. Finally, it is not clear if the regret bound in Theorem 1 can be improved to depend on the squared gradients. This would result in a Õ(T−1) bound for the excess `-risk for smooth losses when E`(f `ρ) = 0 and β = 12 .\n5Datasets available at http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/. The precise details to replicate the experiments are in the Appendix."
    }, {
      "heading" : "A Per-coordinate Variant of PiSTOL",
      "text" : "Recently a number of algorithms with a different step size for each coordinate have been proposed, e.g. [14, 13]. The motivation is to take advantage of the sparsity of the features and, at the same time, to have a slower decaying step size for rare features. However, till now this adaptation has considered only the gradients and not to the norm of the competitor. Here we close this gap.\nAs shown in [14], these kind of algorithms can be very easily designed and analyzed just running an independent copy of the algorithm on each coordinate. Hence, we have the following corollary.\nCorollary 3. Assume the kernelK is the linear one. Also, assume that the sequence of xt satisfies ‖xt‖∞ ≤ 1 and the losses `t are convex and L-Lipschitz. Let a > 0 such that a ≥ 2.25L, and b = 1d . Then, for any u ∈ Rd, running a different copy of Algorithm 3 for each coordinate, the following regret bound holds\nT∑ t=1 [ `t(w > t xt)− `t(u>xt) ] ≤‖u‖∞ d∑ i=1 √√√√2a(L+ T−1∑ t=1 |si,t| ) log ( d ‖u‖∞ √ aLT + 1 ) + φ ( a−1L ) log (1 + T ) ,\nwhere φ(x) := x2 exp( x2 )(x+1)+2 1−x exp( x2 )−x\n( exp ( x 2 ) (x+ 1) + 2 ) .\nUp to logarithmic terms, this regret bound is very similar to the one of AdaGrad [13], with two importance differences. Using our notation, AdaGrad depends ∑T−1 t=1 s 2 i,t rather than ∑T−1 t=1 |si,t|. In the case of Lipschitz losses and binary features, these two dependencies are essentially equivalent. The second and more important difference is that AdaGrad depends on ‖u‖2∞ instead of ‖u‖∞, or in alternative it assumes the knowledge of the (unknown) ‖u‖∞ to tune its step size."
    }, {
      "heading" : "B Convergence in L1ρX",
      "text" : "Define ‖f‖L1ρX := ∫ X |f(x)|dρX . We now use the the standard assumption on the behavior of the approximation error in L1ρX , see, e.g., [34].\nTheorem 3. Assume that the samples (xt, yt)Tt=1 are IID from ρ and `t(x) = `(ytx). If for some 0 < β ≤ 1 and C > 0, the pair (ρ,K) satisfies\ninf f∈HK ∥∥f − f `ρ∥∥L1ρX + γ ‖f‖2K ≤ Cγβ , ∀γ > 0 (10) then, under the assumptions of Theorem 1, the averaged solution of PiSTOL satisfies\nE[E`(f̄T )]− E`(f `ρ) ≤ Õ ( T− β β+1 ) .\nThis Theorem improves over the result in [34], where the worse bound O ( T − β 2(β+1) ) ,∀ > 0, was\nproved using the prior knowledge of β. See [26] for a discussion on the condition (10)."
    }, {
      "heading" : "C Details about the Empirical Results",
      "text" : "For the sake of the reproducibility of the experiments, we report here the exact details. The loss used by PiSTOL in all the experiments is a smoothed version of the hinge loss:\n`(x) =  0 x ≥ 1 (1− x)2 0 < x < 1 1− 2x x ≤ 0.\nFor the SVM we used the hinge loss. The parameters of PiSTOL were the same in all the experiments: a = 0.25, L = 2, β = √ 2aLT . The a9a dataset is composed by 32561 training samples and 16281 for testing, the dimension of the features is 123. The Gaussian kernel is\nK(x,x′) = exp ( −γ ‖x− x′‖22 ) ,\nwhere γ was fixed to 0.04, as done in [19]. The “C” parameter of the SVM was tuned with cross-validation over the range {2−1, 20, 21, 22, 23}. The SensIT Vehicle dataset is a 3-class dataset composed by 78823 training samples and 19705 for testing. A binary classification task was built using the third class versus the other two, to have a very balanced problem. For the amount of time taken by LIBSVM to train a model, we only used a maximum of 58000 training samples. The parameter γ in the Gaussian kernel is 0.125, again as in in [19]. The range of the “C” parameter of the SVM was {20, 21, 22, 23, 24, 25, 26}. The news20.binary dataset is composed by 19996 samples with dimension 1355191, and normalized to have L2 norm equal to 1. The test set was composed by 10000 samples drawn randomly from the training samples. The range of the “C” parameter of the SVM was {21, 22, 23, 24}."
    }, {
      "heading" : "D Proofs",
      "text" : ""
    }, {
      "heading" : "D.1 Additional Definitions",
      "text" : "Given a closed and convex function h : HK → [−∞,+∞], its Fenchel conjugate h∗ : HK → [−∞,+∞] is defined as h∗(g) = supf∈HK ( 〈f , g〉K − h(f) ) .\nD.2 Proof of (3) From [35], it is possible to extract the following inequality\nE[E`(f̄T )] ≤ inf h∈HK\n(1− 2η)−1 [ E`(h) + ‖h‖ 2 K\n2ηT ] ≤ inf h∈HK [ E`(h) + ‖h‖ 2 K 2ηT ] + ((1− 2η)−1 − 1)E`(0).\nUsing the elementary inequalities (1− 2η)−1 − 1 ≤ 4η, ∀0 < η ≤ 14 , we have\nE[E`(f̄T )] ≤ inf h∈HK\nE`(h) + ‖h‖ 2 K\nηT + 4η."
    }, {
      "heading" : "D.3 Proof of Theorem 1",
      "text" : "In this section we prove the regret bound in the adversarial setting. The key idea is of the proof is to design a time-varying potential function. Some of the ideas in the proof are derived from [18, 15].\nIn the proof of Theorem 1 we also use the following technical lemmas.\nLemma 2. Let a, b, c ∈ R, a, c ≥ 0. • if b > 0, then\nexp\n( 2a b+ b2\n2c\n) ≤ 1 + a b\nc + b2 2c\n( (a+ b)2\nc + 1\n) exp ( 2a b+ b2\n2c\n) .\n• if b ≤ 0, then exp\n( 2a b+ b2\n2c\n) ≤ 1 + a b\nc + b2 2c\n( a2 + b2\nc + 1\n) exp ( b2\n2c\n) .\nProof. Consider the function g(b) = exp ( (a+b)2\n2c\n) . Using a second order Taylor expansion around 0 we\nhave\ng(b) = exp\n( a2\n2c\n) + ab\nc exp\n( a2\n2c\n) + ( (a+ ξ)2\nc + 1\n) exp ( (a+ ξ)2\n2c\n) b2\n2c (11)\nfor some ξ between 0 and b. Note that r.h.s of (11) is a convex function w.r.t. ξ, so it is maximized when ξ = 0 or ξ = b. Hence, the first inequality is obtained using upper bounding ξ with b, and (a + ξ)2 with a2 + b2 in the second case.\nLemma 3. [15, Lemma 14] Define Ψ(g) = b exp ‖g‖ 2 K\n2α , for α, b > 0. Then\nΨ∗(f) ≤ ‖f‖K\n√ 2α log (√ α ‖f‖K b + 1 ) − b.\nLemma 4. For all δ, x1, . . . , xT ∈ R+, we have T∑ t=1 xt δ + ∑t i=1 xi ≤ ln (∑T t=1 xt δ + 1 ) .\nProof. Define vt = δ+ ∑t i=1 xi. The concavity of the logarithm implies ln b ≤ ln a+ b−aa for all a, b > 0. Hence we have T∑ t=1 xt δ + ∑t i=1 xi = T∑ t=1 xt vt = T∑ t=1 vt − vt−1 vt ≤ T∑ t=1 ln vt vt−1 = ln vT v0 = ln δ + ∑T t=1 at δ .\nWe are now ready to prove Theorem 1. Differently from the proof methods in [15], here the potential functions will depend explicitly on the sum of the past gradients, rather than simple on the time.\nProof of Theorem 1. Without loss of generality and for simplicity, the proof uses b = 1. For a time-varying function Ψ∗t : HK → R, let ∆t = Ψ∗t (‖gt‖K)−Ψ∗t−1(‖gt−1‖K). Also define gt = ∑t i=1 kt, with kt ∈ H.\nThe Fenchel-Young inequality states that Ψ(f)+Ψ∗(g) ≥ 〈f , g〉K for all f, g ∈ HK . Hence, it implies that, for any sequence of kt ∈ HK and any h ∈ HK , we have\nT∑ t=1 ∆t = Ψ ∗ T (‖gT ‖K)−Ψ∗0(‖g0‖K) ≥ 〈h , gT 〉K −ΨT (‖h‖K)−Ψ∗0(‖g0‖K)\n= −ΨT (‖h‖K) + T∑ t=1 〈h , kt〉K −Ψ∗0(‖g0‖K).\nHence, using the definition of gT , we have\nT∑ t=1 〈h− ft , kt〉K ≤ ΨT (‖h‖K) + Ψ∗0(‖g0‖K) + T∑ t=1 ( Ψ∗t (‖gt‖K)−Ψ∗t−1(‖gt−1‖K)− 〈ft , kt〉K ) .\nWe now use the notation in Algorithm 3, and set kt = −∂`t(ft)k(xt, ·), and Ψ∗t (x) = b exp( x 2 2αt ). Observe that, by the hypothesis on `t, we have ‖kt‖K ≤ L. Observe that, with the choice of αt, we have the following inequalities that will be used often in the proof:\n• ‖gt‖Kαt ≤ ‖∑ti=1 ki‖K αt ≤ 1a . • ‖gt−1‖K‖kt‖Kαt ≤ ‖kt‖K ‖∑t−1i=1 ki‖K αt ≤ ‖kt‖Ka ≤ La .\n• 2‖gt−1‖K‖kt‖K+‖kt‖ 2 K\n2αt ≤ ‖kt‖K ‖∑t−1i=1 ki‖K+‖kt‖K αt ≤ ‖kt‖K ‖∑ti=1 ki‖K αt ≤ ‖kt‖Ka ≤ La .\nWe have Ψ∗t (‖gt‖K)−Ψ∗t−1(‖gt−1‖K)− 〈ft , kt〉K = exp ( ‖gt‖2K 2αt ) − exp ( ‖gt−1‖2K 2αt−1 ) − 〈ft , kt〉K\n= exp\n( ‖gt−1‖2K\n2αt\n)[ exp ( 2 〈gt−1 , kt〉K + ‖kt‖ 2 K\n2αt\n) − (1 + 〈gt−1 , kt〉K\nαt−1 ) exp\n( a ‖kt‖K ‖gt−1‖ 2 K\n2αtαt−1\n)] (12)\nConsider the max of the r.h.s. of the last equality w.r.t. 〈gt−1 , kt〉K . Being a convex function of 〈gt−1 , kt〉K , the maximum is achieved at the border of the domain. Hence, 〈gt−1, kt〉 = ct ‖gt−1‖K ‖kt‖K where ct = 1 or −1. We will analyze the two case separately.\nCase positive: Consider the case that ct = 1. Considering only the expression in parenthesis in (12),\nwe have\nexp\n( 2 ‖gt−1‖K ‖kt‖K + ‖kt‖ 2 K\n2αt\n) − ( 1 + ‖gt−1‖K ‖kt‖K\nαt−1\n) exp ( a ‖kt‖K ‖gt−1‖ 2 K\n2αtαt−1\n)\n≤ 1 + ‖gt−1‖K ‖kt‖K αt\n− ( 1 + ‖gt−1‖K ‖kt‖K\nαt−1\n) exp ( a ‖kt‖K ‖gt−1‖ 2 K\n2αtαt−1\n)\n+ ‖kt‖2K\n2αt exp\n( 2 ‖gt−1‖K ‖kt‖K + ‖kt‖ 2 K\n2αt\n)( (‖gt−1‖K + ‖z‖K)2\nαt + 1\n) (13)\n≤ 1 + a ‖kt‖K L\na exp\n( L\na ) ‖gt−1‖2K 2α2t + ‖kt‖2K 2αt exp ( L a )( 2L a + 1 ) − exp ( a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 ) ,\nwhere in the first inequality we used the first statement of Lemma 2. We now use the fact that A := L a exp ( L a ) < 1 and the elementary inequality exp(x) ≥ x+ 1, to have\n1 + a ‖kt‖K L\na exp\n( L\na ) ‖gt−1‖2K 2α2t + ‖kt‖2K 2αt exp ( L a )( 2L a + 1 ) − exp ( a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 )\n≤ (A− 1) a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 + ‖kt‖2K 2αt exp\n( L\na\n)( 2L\na + 1\n) (14)\n≤ (A− 1) a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 + ‖kt‖K L 2αt exp\n( L\na\n)( 2L\na + 1\n) . (15)\nThis quantity is non-positive iff ‖gt−1‖ 2 K\nαt−1 ≥ A1−A ( 2L a + 1 ) .\nWe now consider the case of A1−A ( 2L a + 1 ) > ‖gt−1‖ 2 αt−1 ≥ ‖gt−1‖ 2 αt . In this case, from (14), we have\nf∗t (‖gt‖K)− f∗t−1(‖gt−1‖K)− 〈ft , kt〉K ≤ exp (\nA\n2(1−A)\n( 2L\na + 1\n)) exp ( L\na\n)( 2L\na + 1 ) ‖kt‖2K 2αt .\n(16) Case negative: Now consider the case that ct = −1. So we have\nexp\n( ‖gt−1‖2K\n2αt\n)[ exp ( −2 ‖gt−1‖K ‖kt‖K + ‖kt‖ 2 K\n2αt\n) + (‖gt−1‖K ‖kt‖K αt−1 − 1 ) exp ( a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 )]\n≤ exp ( ‖gt−1‖2K\n2αt\n)[ 1− ‖gt−1‖K ‖kt‖K\nαt + (‖gt−1‖K ‖kt‖K αt−1 − 1 ) exp ( a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 )\n+ ‖kt‖2K\n2αt exp\n( ‖kt‖2K\n2αt\n)( ‖gt−1‖2K + ‖z‖ 2 K\nαt + 1\n)] ,\nwhere in the inequality we used the second statement of Lemma 2. Considering again only the expression\nin the parenthesis we have\n1− ‖gt−1‖K ‖kt‖K αt + ‖kt‖2K 2αt exp\n( ‖kt‖2K\n2αt\n)( ‖gt−1‖2K + ‖kt‖ 2 K\nαt + 1 ) (‖gt−1‖K ‖kt‖K\nαt−1 − 1 ) exp ( a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 )\n≤ a ‖gt−1‖K ‖kt‖ 2 K αtαt−1 + ‖kt‖2K 2αt exp\n( L\n2a )(‖gt−1‖2K αt + L a + 1 )\n+ (‖gt−1‖K ‖kt‖K αt−1 − 1 )( exp ( a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 ) − 1 )\n≤ ‖gt−1‖ 2 K ‖kt‖ 2 K\n2αtαt−1 exp\n( L\n2a\n) + ‖kt‖2K\n2αt\n( exp ( L\n2a\n)( L\na + 1\n) + 2 ) + ( L a − 1 ) a ‖kt‖K ‖gt−1‖ 2 K\n2αtαt−1\n≤ ‖kt‖ 2 K\n2αt\n( exp ( L\n2a\n)( L\na + 1\n) + 2 ) + ( L\na exp\n( L\n2a\n) + L a − 1 ) a ‖kt‖K ‖gt−1‖ 2 K 2αtαt−1 . (17)\nWe have that this quantity is non-positive if ‖gt−1‖ 2 K αt−1 ≥ ‖kt‖Ka exp( L2a )( L a+1)+2\n1−La exp( L 2a )− L a\n. Hence we now consider\nthe case that ‖gt−1‖ 2 K αt−1 < ‖kt‖K a exp( L2a )( L a+1)+2\n1−La exp( L 2a )− L a\n.\nFrom (17) we have\nΨ∗t (‖gt‖K)−Ψ∗t−1(‖gt−1‖K)− 〈ft , kt〉K ≤ exp ( L\n2a\nexp ( L 2a ) ( L a + 1 ) + 2\n1− La exp ( L 2a ) − La\n) ‖kt‖2K\n2αt\n( exp ( L\n2a\n)( L\na + 1\n) + 2 ) (18)\nPutting together (16) and (19), we have\nΨ∗t (‖gt‖K)−Ψ∗t−1(‖gt−1‖K)− 〈ft , kt〉K ≤ a\nL φ\n( L\na ) ‖kt‖2 αt . (19)\nUsing the definition of φ(La ) and summing over time we have\nT∑ t=1 ( Ψ∗t (‖gt‖K)−Ψ∗t−1(‖gt−1‖K)− 〈ft , kt〉K ) ≤ a L φ ( L a ) T∑ t=1 ‖kt‖2K αt\n≤ φ ( L\na ) T∑ t=1 ‖kt‖K L+ ∑t i=1 ‖ki‖K ≤ φ ( L a ) log ( 1 + ∑t i=1 ‖ki‖K L ) ≤ φ ( L a ) log (1 + T ) , (20)\nwhere in the third inequality we used Lemma 4. Using (D.3), (20), and the definition of subgradient, we have\nT∑ t=1 `t(ft(xt))− T∑ t=1 `t(h(xt)) ≤ T∑ t=1 ∂`t(ft(xt)) (h(xt)− ft(xt)) = T∑ t=1 〈h− ft , kt〉K\n≤ ΨT (‖h‖K) + Ψ∗0(‖g0‖K) + T∑ t=1 ( Ψ∗t (‖gt‖K)−Ψ∗t−1(‖gt−1‖K)− ft(zt) ) ≤ ΨT (‖h‖K) + Ψ∗0(‖g0‖K) + φ ( L\na\n) log (1 + T ) .\nUsing Lemma 3 completes the proof."
    }, {
      "heading" : "D.4 Proof of Corollary 1",
      "text" : "We first state the technical results, used in the proofs.\nLemma 5. [25, Lemma 2.1] For an H-smooth function ` : R→ R+, we have (`′(x))2 ≤ 4Hf(x). Lemma 6. [12, Lemma 7.2] Let c1, c2, · · · , cl > 0 and s > q1 > q2 > · · · > ql−1 > 0. Then the equation\nxs − c1xq1 − c2xq2 − · · · − cl−1xql−1 − cl = 0 has a unique positive solution x∗. In addition,\nx∗ ≤ max { (lc1) 1 s−q1 , (lc2) 1 s−q2 , · · · , (lcl−1) 1 s−ql−1 , (lcl) 1 s } .\nLemma 7. Let a, b, c > 0 and 0 < α < 1. Then the inequality\nx− a(x+ b)α − c ≤ 0 implies x ≤ amax{(2a) α1−α , (2(b+ c))α}+ c. Proof. Denote by y = x+ b, so consider the function f(y) = y− ayα − b− c. Applying Lemma 6 we get that the h(y) = 0 has a unique positive solution y∗ and\ny∗ ≤ max { (2a) 1 1−α , 2(b+ c) } .\nMoreover, the inequality h(y) ≤ 0 is verified for y = 0, and limy→+∞ h(y) = +∞, so we have h(y) ≤ 0 implies y ≤ y∗. We also have\ny∗ = a(y∗)α − b− c ≤ amax { (2a) α 1−α , (2(b+ c))α } + b+ c.\nSubstituting back x we get the stated bound.\nProof of Corollary 1. Using Cauchy-Schwarz inequality and Lemma 5, we have\nL+ T−1∑ t=1 |st| ‖k(xt, ·)‖K ≤ √ T √√√√L2 + T−1∑ t=1 s2t ‖k(xt, ·)‖2K ≤ √ T √√√√L2 + 4H T−1∑ t=1 `t(ft(xt))\n≤ √ T √√√√L2 + 4H T∑ t=1 `t(ft(xt)).\nDenote by Loss = ∑T t=1 `t(ft(xt)) and Loss ∗ = ∑T t=1 `t(h(xt)). Plugging last inequality in Theorem 1, we obtain\nLoss− Loss∗\n≤ ( L2\n4H + Loss\n) 1 4\n‖f‖K T 1 4 √√√√2a√4H log(‖f‖K √aLT b + 1 ) + bφ ( L a ) log (1 + T ) .\nDenote by C = √ 2a √ 4H log ( ‖f‖K √ aLT b + 1 ) . Using Lemma 7 we get\nLoss− Loss∗ ≤ bφ ( L\na\n) log (1 + T )\n+ ‖h‖K T 1 4C max\n{ ‖h‖ 1 3\nK T 1 12 (2C) 1 3 , 2 1 4\n( Loss∗ + bφ ( L\na\n) log (1 + T ) + L2\n4H\n) 1 4 } ."
    }, {
      "heading" : "D.5 Proof of Lemma 1",
      "text" : "Proof. For any f ∈ L2ρX , define Xf = {x ∈ X : sign(f) 6= fc}. It is easy to verify that\nR(f)−R(fc) = ∫ Xf |fρ(x)|dρX (x)\n= ∫ Xf |fρ(x)|1(fρ(x) > )dρX (x) + ∫ Xf |fρ(x)|1(fρ(x) ≤ )dρX (x).\nUsing condition (8), we have\nR(f)−R(fc) ≤ 1 ∫ Xf |fρ(x)|2dρX (x) + ∫ Xf dρX (x) (21)\n≤ 1 ∫ Xf |fρ(x)|2dρX (x) + cα(R(f)−R(fc))α. (22)\nUsing Lemma 10.10 in [12] and proceeding as in the proof of Theorem 10.5 in [12], we have∫ Xf |fρ(x)|2dρX (x) ≤ 1 C ( E`(f)− E`(f `ρ) ) . (23)\nHence we have\nR(f)−R(fc) ≤ 1\nC\n( E`(f)− E`(f `ρ) ) + cα(R(f)−R(fc))α. (24)\nOptimizing over we get\nR(f)−R(fc) ≤ 2 √ cα C √ E`(f)− E`(f `ρ)(R(f)−R(fc)) α 2 , (25)\nthat is\nR(f)−R(fc) ≤ (\n4 cα C\n( E`(f)− E`(f `ρ) )) 12−α . (26)\nAn application of Jensen’s inequality concludes the proof."
    }, {
      "heading" : "D.6 Proof of Theorem 2",
      "text" : "We need the following technical results.\nLemma 8. [12, Lemma 10.7] Let p, q > 1 be such that 1p + 1 q = 1. Then\nab ≤ 1 q aqηq + 1 p bpη−q, ∀a, b, η > 0.\nLemma 9. Let a, b, p, q ≥ 0. Then\nmin x≥0\naxp + bx−q ≤ 2a qq+p b pq+p ,\nand the argmin is ( pa qb )− 1q+p .\nProof. Equating the first derivative to zero we have\npaxp−1 = qbx−q−1.\nThat is the optimal solution satisfies\nx =\n( pa\nqb\n)− 1q+p .\nSubstituting this expression into the min we have\na\n( pa\nqb\n)− pq+p + b ( pa\nqb\n) q q+p\n= a q q+p b p q+p\n(( p\nq\n)− pq+p + ( p\nq\n) q q+p ) ≤ 2a qq+p b pq+p .\nThe next lemma is the same of [12, Proposition 8.5], but it uses f `ρ instead of fρ.\nLemma 10. Let X ⊂ Rd be a compact domain and K a Mercer kernel such that f `ρ lies in the range of LβK , with 0 < β ≤ 12 , that is f `ρ = L β K(g) for some g ∈ L2ρX . Then\nmin f∈HK ∥∥f − f `ρ∥∥2L2ρX + γ ‖f‖2K ≤ γ2β ‖g‖2L2ρX . Proof. It is enough to use [12, Theorem 8.4] with H = L2ρX , s = 1, A = L 1 2 K and a = f ` ρ .\nThe next Lemma is needed for the proof of Lemma 12 that is a stronger version of [12, Corollary 10.14] because it needs only smoothness rather than a bound on the second derivative.\nLemma 11. [17, Lemma 1.2.3] Let f be continuous differentiable on a set Q ⊂ R, and its first derivative is H-Lipschitz on Q. Then\n|f(x)− f(y)− f ′(x)(y − x)| ≤ H 2 (y − x)2.\nLemma 12. Assume ` is H-smooth. Then, for any f ∈ L2ρX , we have\nE`(f)− E`(f `ρ) ≤ H\n2 ∥∥f − f `ρ∥∥2L2ρX . Proof. Denoting by gx(f(x)) = ∫ Y `(f(x), y)dρ(y|x), we have that\nE`(f)− E`(f `ρ) = ∫ X gx(f(x))− gx(f `ρ(x))dρX . (27)\nWe now use the Lemma 11 to have\ngx(f(x))− gx(f `ρ(x)) ≤ g′x(f `ρ(x))(f(x)− f `ρ(x)) + H\n2 (ŷ)(f(x)− f `ρ(x))2\n= H\n2 (f(x)− f `ρ(x))2, (28)\nwhere in the equality we have used the fact that f `ρ(x) is by definition the minimizer of the function g ′ x. Putting together (27) and (28) we have\nE`(f)− E`(f `ρ) ≤ H\n2 ∫ X (f(x)− f `ρ(x))2dρX .\nProof of Theorem 2. We will first get rid of the norm inside the logarithmic term. This will allow us to have a bound that depends only on norm of g. Let L(f) = ‖f‖K √ 2α log (√ α‖f‖K b + 1 ) + q(f). Denote by h∗ = arg minf∈HK L(f). Hence, we\nhave\n‖h∗‖K √ 2α √ α‖h∗‖K b√\nα‖h∗‖K b + 1\n≤ ‖h∗‖K √√√√2α √α‖h∗‖Kb√ α‖h∗‖K b + 1 ≤ L(h∗) ≤ L(0) = q(0). (29)\nSolving the quadratic inequality and using the elementary inequality √ a+ b ≤ √a+ b\n2 √ a , we have\n√ α ‖h∗‖K ≤ 2− 1 2 q(0) + b. (30)\nSo we have min f∈HK\nL(f) = min f∈HK ,‖f‖K≤ 2 − 1 2 q(0)+b√ α L(f) (31)\nWe now use this result in the regret bound of Theorem 1, to have\nmin h∈HK T∑ t=1 `t(h(xt)) + ‖h‖K √√√√2a(L+ T−1∑ t=1 |st| ) log ( ‖h‖K √ aLT b + 1 )\n≤ min h∈HK T∑ t=1 `t(h(xt)) + ‖h‖K √√√√2a(L+ T−1∑ t=1 |st| ) log ( 2− 1 2 `(0)T b + φ (a−1L) log (1 + T ) + 2 ) .\nReasoning as in the proof of Corollary 1, and denoting byC = 2 √ a √ H log ( 2−\n1 2 `(0)T b + φ (a −1L) log (1 + T ) + 2 ) and B = bφ ( L a ) log (1 + T ) + L 2 4H , we get\nT∑ t=1 `t(ft(xt)) ≤ min h∈HK T∑ t=1 `t(h(xt)) + bφ ( L a ) log (1 + T )\n+ ‖h‖K T 1 4C max ‖h‖ 13K T 112 (2C) 13 , 2 14 ( T∑ t=1 `t(h(xt)) +B ) 1 4  . Dividing everything by T , taking the expectation of the two sides and using Jensen’s inequality we have\nE[E`(f̄T )] ≤ E [ 1\nT T∑ t=1 E`(ft) ] ≤ min h∈HK E`(h) + b T φ ( L a ) log (1 + T )\n+ ‖h‖K T− 3 4C max\n{ ‖h‖ 1 3\nK T 1 12 (2C) 1 3 , 2 1 4\n( TE`(h) +B ) 1 4 } .\nWe now need to upper bound the terms in the max. Using Lemma 8, we have that, for any η, γ > 0\n2 1 4C ‖h‖K T− 3 4 (TE`(h) +B) 14\n≤ 1 2\n( η\n1 2 ‖h‖2K + η− 1 2C2T− 3 2 2 1 2 (TE`(h) +B) 12 ) ≤ 1\n2\n( η\n1 2 ‖h‖2K +\n1 2 γ−1η−1C4T−2 + γT−1(TE`(h) +B)\n) , (32)\nand 2 1 3C 4 3 ‖h‖ 4 3 K T − 23 ≤ 2\n3\n( η\n3 2 ‖h‖2K + η−3C4T−2\n) . (33)\nConsider first (33). Observe that from Lemma 12 and Lemma 10, we have\nmin h∈HK min η>0 E`(h) + 2 3 (η 3 2 ‖h‖2K + C4T−2η−3)\n= min h∈HK min η>0 E`(h)− E`(f `ρ) + E`(f `ρ) +\n2 3 (η 3 2 ‖h‖2K + C4T−2η−3)\n≤ min h∈HK min η>0\nH\n2 ∥∥h− f `ρ∥∥2L2ρX + E`(f `ρ) + 23(η 32 ‖h‖2K + C4T−2η−3) = min h∈HK min η>0 E`(f `ρ) + 2 3 C4T−2η−3 + H 2 (∥∥h− f `ρ∥∥2L2ρX + 43H η 32 ‖h‖2K )\n≤ min η>0 E`(f `ρ) +\n( 2\n3\n)2β ( H\n2 )1−2β η3β ‖g‖2L2ρX + 2 3 C4T−2η−3\n≤ E`(f `ρ) + 2 ( 2\n3\n) 3β β+1\n‖g‖ 2 β+1\nL2ρX\n( H\n2\n) 1−2β β+1 (\nC4T−2 ) β β+1 . (34)\nConsider now (32). Reasoning in a similar way we have\nmin h∈HK min η>0 min γ>0 E`(h) + 1 2\n( η\n1 2 ‖h‖2K +\n1 2 γ−1η−1C4T−2 + γT−1(TE`(h) +B) ) = min h∈HK min η>0 min γ>0 ( 1 + 1 2 γ )( E`(h)− E`(f `ρ) + 1 2 ( 1 + 12γ )η 12 ‖h‖2K ) + 1 4 γ−1η−1C4T−2\n+ 1\n2 γT−1\n( B + TE`(f `ρ) ) + E`(f `ρ)\n≤ min h∈HK min η>0 min γ>0\nH\n2\n( 1 + 1\n2 γ )(∥∥h− f `ρ∥∥2L2ρX + 1H (1 + 12γ)η 12 ‖h‖2K ) + 1 4 γ−1η−1C4T−2\n+ 1\n2 γT−1\n( B + TE`(f `ρ) ) + E`(f `ρ)\n≤ min η>0 min γ>0\n1 2 H1−2β\n( 1 + 1\n2 γ )1−2β ηβ ‖g‖2L2ρX + 1 4 γ−1η−1C4T−2 + 1 2 γT−1 ( B + TE`(f `ρ) ) + E`(f `ρ)\n≤ min γ>0\n( H ( 1 + 1\n2 γ\n)) 1−2β β+1\n‖g‖ 2 1+β\nL2ρX (4γ)\n− ββ+1 C 4β β+1T− 2β β+1 +\n1 2 γT−1\n( B + TE`(f `ρ) ) + E`(f `ρ) .\nWe now use the elementary inequality 1 + x ≤ max(2, 2x),∀x ≥ 0, to study separately\nmin γ>0\n(2H) 1−2β β+1 ‖g‖ 2 1+β\nL2ρX (4γ)\n− ββ+1 C 4β β+1T− 2β β+1 +\n1 2 γT−1\n( B + TE`(f `ρ) ) , (35)\nand min γ>0 (γH) 1−2β β+1 ‖g‖ 2 1+β L2ρX (4γ) − ββ+1 C 4β β+1T− 2β β+1 + 1 2 γT−1 ( B + TE`(f `ρ) ) . (36)\nFor (35), from Lemma 9, we have\nmin γ>0\n(2H) 1−2β β+1 ‖g‖ 2 1+β\nL2ρX (4γ)\n− ββ+1 C 4β β+1T− 2β β+1 +\n1 2 γT−1\n( B + TE`(f `ρ) ) ≤ 2 (2H) 1−2β 2β+1 ‖g‖ 2 2β+1\nL2ρX 4− β 2β+1C 4β 2β+1T− 2β 2β+1\n( 1\n2 T−1\n( B + TE`(f `ρ) )) β2β+1 . (37)\nOn the other hand, for (36), for β < 13 , we have that the minimum over γ is 0. For β > 1 3 , from Lemma 9, we have\nmin γ>0\n(γH) 1−2β β+1 ‖g‖ 2 1+β\nL2ρX (4γ)\n− ββ+1 C 4β β+1T− 2β β+1 +\n1 2 γT−1\n( B + TE`(f `ρ) ) = min\nγ>0 2− 2β β+1H 1−2β β+1 ‖g‖ 2 1+β L2ρX γ 1−3β β+1 C 4β β+1T− 2β β+1 + 1 2 γT−1\n( B + TE`(f `ρ) ) ≤ 2 12H 1−2β4β ‖g‖ 2 4β\nL2ρX CT− 1 2\n( 1\n2 T−1\n( B + TE`(f `ρ) )) 3β−14β . (38)\nPutting together (34), (37), and (38), we have the stated bound."
    }, {
      "heading" : "D.7 Proof of Theorem 3",
      "text" : "Proof. From the proof of Theorem 2, we have that\nT∑ t=1 `t(ft(xt)) ≤ inf h∈HK T∑ t=1 `t(h(xt)) + ‖h‖K √√√√2aLT log(2− 12 `(0)T b + φ (a−1L) log (1 + T ) + 2 ) + bφ ( a−1L ) log (1 + T ) .\nDividing everything by T , taking the expectation of the two sides and using Jensen’s inequality we have E[E`(f̄T )] ≤ E [ 1\nT T∑ t=1\nE`(ft) ]\n≤ inf h∈HK E`(h) + ‖h‖K T− 1 2 √√√√2aL log(2− 12 `(0)T b + φ (a−1L) log (1 + T ) + 2 )\n+ b T φ ( a−1L ) log (1 + T ) .\nDenote by D = √ 2aL log ( 2−\n1 2 `(0)T b + φ (a −1L) log (1 + T ) + 2\n) . Using the Lipschitzness of the loss,\nwe have E`(h)− E`(f `ρ) ≤ L ∥∥h− f `ρ∥∥L1ρX , so\ninf h∈HK\nE`(h) +D ‖h‖K T− 1 2\n≤ inf h∈HK min η>0 L (∥∥h− f `ρ∥∥L1ρX + η2L ‖h‖2K ) + E`(f `ρ) + 1 2 D2T−1η−1\n≤ min η>0 E`(f `ρ) + CL1−β2−βηβ +\n1 2 D2T−1η−1\n≤ E`(f `ρ) + C 1 1+β (2L) 1−β 1+βD 2β β+1T− β β+1 ,\nwhere in the last inequality we used Lemma 9."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "<lb>Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and<lb>more importance, thanks to their scalability. While various methods have been proposed to speed up their<lb>convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time<lb>assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in<lb>the practical world validation methods remain the only viable approach. In this paper, we propose a new<lb>kernel-based stochastic gradient descent algorithm that performs model selection while training, with no<lb>parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in<lb>online learning theory for unconstrained settings, to estimate over time the right regularization in a data-<lb>dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the<lb>target function, using the range space of the fractional integral operator associated with the kernel.",
    "creator" : "TeX"
  }
}