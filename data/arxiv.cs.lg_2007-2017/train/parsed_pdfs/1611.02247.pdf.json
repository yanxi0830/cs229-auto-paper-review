{
  "name" : "1611.02247.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT WITH AN OFF-POLICY CRITIC",
    "authors" : [ "Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Model-free reinforcement learning is a promising approach for solving arbitrary goal-directed sequential decision-making problems with only high-level reward signals and no supervision. It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016). Deep neural network parametrization minimizes the need for manual feature and policy engineering, and allows learning end-to-end policies mapping from high-dimensional inputs, such as images, directly to actions. However, such expressive parametrization also introduces a number of practical problems. Deep reinforcement learning algorithms tend to be sensitive to hyperparameter settings, often requiring extensive hyperparameter sweeps to find good values. Poor hyperparameter settings tend to produce unstable or non-convergent learning. Other of these algorithms tend to exhibit high sample complexity, often to the point of being impractical to run on real physical systems. Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.\nModel-free reinforcement learning consists of on- and off-policy methods. Policy gradient methods (Peters & Schaal, 2006; Schulman et al., 2015) are popular on-policy methods that directly maximize the cumulative future returns with respect to the policy. While these algorithms use unbi-\nar X\niv :1\n61 1.\n02 24\n7v 1\n[ cs\n.L G\n] 7\nN ov\nased gradient estimators of the true reinforcement learning objective, their estimators rely on Monte Carlo returns and have high variance. To cope with high variance gradient estimates and difficult optimization landscapes, a number of techniques have been proposed, including constraining the change in the policy at each gradient step (Kakade, 2001; Peters et al., 2010) and mixing valuebased back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al., 2015). However, these methods all tend to require very large numbers of samples to deal with the high variance when estimating gradients of high-dimensional neural network policies. The crux of the problem with policy gradient methods is that they can only effectively use on-policy samples, which means that they require collecting large amounts of on-policy experiences after each parameter update to the policy. This makes them very sample intensive. Off-policy methods, such as Q-learning (Watkins & Dayan, 1992; Sutton et al., 1999; Mnih et al., 2015; Gu et al., 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al., 2016), can instead use all samples, including off-policy samples, by adopting temporal difference learning with experience replay. Such methods are much more sample-efficient. However, convergence of these algorithms is in general not guaranteed with non-linear function approximators, and practical convergence and instability issues typically mean that extensive hyperparameter tuning is required to attain good results.\nIn order to make deep reinforcement learning practical as a tool for tackling real-world tasks, we must develop methods that are both data efficient and stable. In this paper, we propose Q-Prop, a step in this direction that combines the advantages of on-policy policy gradient methods with the efficiency of off-policy learning. Unlike prior approaches for off-policy learning, which either introduce bias (Sutton et al., 1999; Silver et al., 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al., 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al., 2016) which fit the value function on-policy, Q-Prop learns the action-value function off-policy. The core idea is to use the first-order Taylor expansion of the critic as a control variate, resulting in an analytical gradient term through the critic and a policy gradient term consisting of the residuals in advantage approximations. The method helps unify policy gradient and actor-critic methods: it can be seen as using the off-policy critic to reduce variance in policy gradient or using on-policy Monte Carlo returns to correct for bias in the critic gradient. We further provide theoretical analysis of the control variate, and derive two additional variants of Q-Prop. The method can be easily incorporated into any policy gradient algorithm. We show that Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE) (Schulman et al., 2015; 2016), and improved stability over deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016) across a repertoire of continuous control tasks."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "Reinforcement learning (RL) aims to learn a policy for an agent such that it behaves optimally according to a reward function. At a time step t and state st , the agent chooses an action at according to its policy π(at |st), the state of the agent and the environment changes to new state st+1 according to dynamics p(st+1|st ,at), the agent receives a reward r(st ,at), and the process continues. Let Rt denote a γ-discounted cumulative return from t for an infinite horizon problem, i.e Rt = ∑∞t ′=t γ\nt−t ′r(st ′ ,at ′). The goal of reinforcement learning is to maximize the expected return J(θ) = Eπθ [R0] with respect to the policy parameters θ . In this section, we review several standard techniques for performing this optimization, and in the next section, we will discuss our proposed Q-Prop algorithm that combines the strengths of these approaches to achieve efficient, unbiased RL."
    }, {
      "heading" : "2.1 POLICY GRADIENT METHODS",
      "text" : "Policy gradient methods1 apply direct gradient-based optimization to the reinforcement learning objective. This involves directly differentiating the J(θ) objective with respect to the policy parameters θ . The standard form, known as the REINFORCE algorithm (Williams, 1992), is shown below:\n∇θ J(θ) = Eπ [ ∞\n∑ t=0\n∇θ logπθ (at |st)γ tRt ] = Eπ [ ∞\n∑ t=0 γ t∇θ logπθ (at |st)(Rt −b(st))], (1)\n1“Policy gradient” technically refers to a more general class of methods (Sutton et al., 1999); however, to make it compatible with references in other work we use it to primarily refer REINFORCE (Williams, 1992).\nwhere b(st) is known as the baseline. For convenience of later derivations, Eq. 1 can also be written as below, where ρπ(s) = (1− γ)∑∞t=0 γ t p(st = s) is the normalized state visitation frequency,\n∇θ J(θ) = Est∼ρπ (·),at∼π(·|st )[∇θ logπθ (at |st)(Rt −b(st))]. (2) The gradient is estimated using Monte Carlo samples in practice and has very high variance. A proper choice of baseline is necessary to reduce the variance sufficiently such that learning becomes feasible. A common choice is to estimate the value function of the state Vπ(st) to use as the baseline, which provides an estimate of advantage function Aπ(st), a centered action-value function Qπ(st), each defined below,\nVπ(st) = Eπ [Rt ] = Eπθ (at |st )[Qπ(st ,at)] Qπ(st ,at) = r(st ,at)+ γEπ [Rt+1] = r(st ,at)+ γEp(st+1|st ,at )[Vπ(st+1)] Aπ(st ,at) = Qπ(st ,at)−Vπ(st).\n(3)\nQπ(st) summarizes the performance of each action from a given state, assuming it follows π thereafter, and Aπ(st ,at) provides a measure of how each action compares to the average performance at the state st , which is given by Vπ(st). Using Aπ(st ,at) centers the learning signal and reduces variance significantly.\nBesides high variance, the main problem with policy gradient is that it requires on-policy samples which makes it very sample intensive. To achieve similar sample efficiency as off-policy methods, it is naturally crucial to use off-policy data, but it is nontrivial. Prior attempts use importance sampling to use off-policy trajectories; however, these are known to be difficult scale to high-dimensional action spaces because of rapidly degenerating importance weights (Precup, 2000)."
    }, {
      "heading" : "2.2 OFF-POLICY ACTOR-CRITIC METHODS",
      "text" : "Actor-critic methods (Sutton et al., 1999) include a policy evaluation step, which uses temporal difference (TD) learning to fit a critic Qw for the current policy π(θ), and a policy improvement step which greedily optimizes the policy π against the critic estimate Qw. Significant gain in sample efficiency is achievable using off-policy TD learning for the critic, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), recently popularized by experience replay for training deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b).\nDeep deterministic policy gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016) is an instance of off-policy algorithms which achieves significant results on high-dimensional continuous control tasks. The updates for this method are given below, where πθ (at |st) = δ (at = µθ (st)) is a deterministic policy, β is arbitrary exploration distribution, and ρβ corresponds to sampling from a replay buffer:\nw = argmin w Est∼ρβ (·),at∼β (·|st )[(r(st ,at)+ γQ(st+1,µθ (st+1))−Qw(st ,at)) 2]\nθ = argmax θ\nEst∼ρβ (·)[Qw(st ,µθ (st))]. (4)\nWhen the critic and policy are parametrized with neural networks, full optimization is expensive, and instead stochastic gradient optimization is used. The gradient in the policy improvement phase is given below, which is generally a biased gradient of J(θ).\n∇θ J(θ)≈ Est∼ρβ (·)[∇aQw(st ,a)|a=µθ (st )∇θµθ (st)] (5) The crucial benefits of DDPG are that it does not rely on high variance REINFORCE gradients and is trainable on off-policy data. These properties make DDPG and other analogous off-policy methods significantly more sample-efficient than policy gradient methods (Lillicrap et al., 2016; Gu et al., 2016b; Duan et al., 2016). However, the use of a biased policy gradient estimator makes analyzing its convergence and stability properties difficult."
    }, {
      "heading" : "3 Q-PROP",
      "text" : "In this section, we derive the Q-Prop estimator for policy gradient. The key idea from this estimator comes from observing Equations 2 and 5 and noting that the former provides an unbiased, but\nhigh variance gradient, while the latter provides a deterministic, but biased gradient. By using the deterministic biased estimator as a particular form of control variate (Ross, 2006; Paisley et al., 2012) for the unbiased policy gradient estimator, we can effectively use both types of gradient information to construct a new estimator that is in general unbiased, and in practice exhibits improved sample efficiency through the inclusion of off-policy samples."
    }, {
      "heading" : "3.1 Q-PROP ESTIMATOR",
      "text" : "To derive the Q-Prop gradient estimator, we start by using the first-order Taylor expansion of an arbitrary function f (st ,at), f̄ (st ,at) = f (st , āt)+∇a f (st ,a)|a=āt (at − āt), as the control variate for the policy gradient estimator. We use Q̂(st ,at) = ∑tt ′=t γ\nt ′−tr(st ′ ,at ′) to denote Monte Carlo return from state st and action at , i.e. Eπ [Q̂(st ,at)] = r(st ,at) + γEp[Vπ(st+1)], and µθ (st) = Eπθ (at |st )[at ] to denote the expected action of a stochastic policy πθ . Full derivation is in Appendix A.\n∇θ J(θ) = Eρπ ,π [∇θ logπθ (at |st)(Q̂(st ,at)− f̄ (st ,at)]+Eρπ ,π [∇θ logπθ (at |st) f̄ (st ,at)] = Eρπ ,π [∇θ logπθ (at |st)(Q̂(st ,at)− f̄ (st ,at)]+Eρπ [∇a f (st ,a)|a=āt ∇θµθ (st)] (6)\nEq. 6 is general for arbitrary function f (st ,at) that is differentiable with respect to at at an arbitrary value of āt ; however, a sensible choice is to use the critic Qw for f and µθ (st) for āt to get,\n∇θ J(θ) = Eρπ ,π [∇θ logπθ (at |st)(Q̂(st ,at)− Q̄w(st ,at)]+Eρπ [∇aQw(st ,a)|a=µθ (st )∇θµθ (st)]. (7)\nFinally, since in practice we estimate advantages Â(st ,at), we write the Q-Prop estimator in terms of advantages to complete the basic derivation,\n∇θ J(θ) = Eρπ ,π [∇θ logπθ (at |st)(Â(st ,at)− Āw(st ,at)]+Eρπ [∇aQw(st ,a)|a=µθ (st )∇θµθ (st)] Ā(st ,at) = Q̄(st ,at)−Eπθ [Q̄(st ,at)] = ∇aQw(st ,a)|a=µθ (st )(at −µθ (st)).\n(8)\nEq. 8 comprises of an analytic gradient through the critic as in Eq. 5 and a residual REINFORCE gradient in Eq. 2. From the above derivation, Q-Prop is simply an unbiased policy gradient estimator with a special form of control variate. The important insight comes from the fact that Qw can be trained using off-policy data as in Eq. 4. Under this setting, Q-Prop is no longer just a policy gradient method, but more closely resembles an actor-critic method, except in policy improvement step it has an additional REINFORCE correction term that ensures that the gradient estimator is unbiased regardless of the parametrization, training method, and performance of the critic.\nIntuitively, if the critic Qw approximates Qπ well, it provides a reliable gradient, reduces the estimator variance, and improves the convergence rate. Interestingly, control variate analysis in the next section shows that this is not the only circumstance where Q-Prop helps reduce variance."
    }, {
      "heading" : "3.2 CONTROL VARIATE ANALYSIS AND ADAPTIVE Q-PROP",
      "text" : "For Q-Prop to be applied reliably, it is crucial to analyze how the variance of the estimator changes before and after the application of control variate. Following the prior work on control variate (Ross, 2006; Paisley et al., 2012), we first introduce η(st) to Eq. 8, a weighing variable that modulates the strength of control variate. This additional variable η(st) does not introduce bias to the estimator.\n∇θ J(θ) =Eρπ ,π [∇θ logπθ (at |st)(Â(st ,at)−η(st)Āw(st ,at)] +Eρπ [η(st)∇aQw(st ,a)|a=µθ (st )∇θµθ (st)]\n(9)\nA measure of the variance of this estimator is below, where m = 1...M indexes the dimension of θ ,\nVar∗ = Eρπ [∑ m Varat (∇θm logπθ (at |st)(Â(st ,at)−η(st)Ā(st ,at)))]. (10)\nIf we choose η(st) such that Var∗ < Var, where Var = Eρπ [∑m Varat (∇θm logπθ (at |st)Â(st ,at))] is the original estimator variance measure, then we have managed to reduce the variance. Directly analyzing the above variance measures is nontrivial, just like computing the optimal baseline is\ndifficult (Williams, 1992). In addition, it is often impractical get multiple action samples from the same state, which prohibits using naı̈ve Monte Carlo to estimate the expectations. Thus, we propose a surrogate variance measure, Var = Eρπ [Varat (Â(st ,at))]. A similar surrogate is also used by prior work on learning state-dependent baseline (Mnih & Gregor, 2014), and the benefit is that the measure becomes more tractable,\nVar∗ = Eρπ [Varat (Â(st ,at)−η(st)Ā(st ,at))] = Var+Eρπ [−2η(st)Covat (Â(st ,at), Ā(st ,at))+η(st)2Varat (Ā(st ,at))].\n(11)\nSince Eπ [Â(st ,at)] = Eπ [Ā(st ,at)] = 0, the terms can be simplified as below, Covat (Â, Ā) = Eπ [Â(st ,at)Ā(st ,at)]\nVarat (Ā) = Eπ [Ā(st ,at) 2] = ∇aQw(st ,a)|Ta=µθ (st )Σθ (st)∇aQw(st ,a)|a=µθ (st ),\n(12)\nwhere Σθ (st) is the covariance matrix of the stochastic policy πθ . The nice property of Eq. 11 is that Varat (Ā) is analytical and Covat (Â, Ā) can be estimated with single action sample. Using this estimate, we propose adaptive variants of Q-Prop that regulate the variance of the gradient estimate.\nAdaptive Q-Prop. The optimal state-dependent factor η(st) can be computed per state, according to η∗(st) = Covat (Â, Ā)/Varat (Ā). This provides maximum reduction in variance according to Eq. 11. Substituting η∗(st) into Eq. 11, we get Var∗ = Eρπ [(1−ρcorr(Â, Ā)2)Varat (Â)], where ρcorr is the correlation coefficient, which achieves guaranteed variance reduction if at any state Ā is correlated with Â. We call this the fully adaptive Q-Prop method. An important conclusion from this analysis is that, in adaptive Q-Prop, the critic Qw does not necessarily need to be approximating Qπ well to produce good results. Its Taylor expansion merely needs to be correlated with Â, positively or even negatively. This is in contrast with actor-critic methods, where performance is greatly dependent on the absolute accuracy of the critic’s approximation.\nConservative and Aggressive Q-Prop. In practice, the single-sample estimate of Covat (Â, Ā) has high variance itself, and we propose the following two practical implementations of adaptive Q-Prop: (1) η(st) = 1 if ˆCovat (Â, Ā)> 0 and η(st) = 0 if otherwise, and (2) η(st) = sign( ˆCovat (Â, Ā)). The first implementation, which we call conservative Q-Prop, can be thought of as a more conservative version of Q-Prop, which effectively disables the control variate for some samples of the states. This is sensible as if Â and Ā are negatively correlated, it is likely that the critic is very poor. The second variant can correspondingly be termed aggressive Q-Prop, since it makes more liberal use of the control variate.\nAlgorithm 1 Adaptive Q-Prop 1: Initialize w for critic Qw, θ for stochastic policy πθ , and replay buffer R← /0. 2: repeat 3: for e = 1, . . . ,E do . Collect E episodes of on-policy experience using πθ 4: s0,e ∼ p(s0) 5: for t = 0, . . . ,T −1 do 6: at,e ∼ πθ (·|st,e), st+1,e ∼ p(·|st,e,at,e), rt,e = r(st,e,at,e) 7: Add batch data B = {s0:T,1:E ,a0:T−1,1:E ,r0:T−1,1:E} to replay buffer R 8: Take E ·T gradient steps on Qw using R and πθ 9: Fit Vφ (st) using B 10: Compute Ât,e using GAE(λ ) and Āt,e using Eq. 7 11: Set ηt,e based on Section 3.2 12: Compute and center the learning signals lt,e = Ât,e−ηt,eĀt,e 13: Compute ∇θ J(θ)≈ 1ET ∑e ∑t ∇θ logπθ (at,e|st,e)lt,e +ηt,e∇aQw(st,e,a)|a=µθ (st,e)∇θµθ (st,e) 14: Take a gradient step on πθ using ∇θ J(θ), optionally with a trust-region constraint using B 15: until πθ converges."
    }, {
      "heading" : "3.3 Q-PROP ALGORITHM",
      "text" : "Pseudo-code for the adaptive Q-Prop algorithm is provided in Algorithm 1. It is a mixture of policy gradient and actor-critic. At each iteration, it first rolls out the stochastic policy to collect on-policy\nsamples, adds the batch to a replay buffer, takes a few gradient steps on the critic, computes Â and Ā, and finally applies a gradient step on the policy πθ . In our implementation, the critic Qw is fitted with off-policy TD learning using the same techniques as in DDPG (Lillicrap et al., 2016):\nw = argmin w Est∼ρβ (·),at∼β (·|st )[(r(st ,at)+ γEπ [Q ′(st+1,at+1)]−Qw(st ,at))2]. (13)\nVφ is fitted with the same technique in (Schulman et al., 2016). Generalized advantage estimation (GAE) (Schulman et al., 2016) is used to estimate Â. The policy update can be done by any method that utilizes the first-order gradient and possibly the on-policy batch data, which includes trust region policy optimization (TRPO) (Schulman et al., 2015). Importantly, this is just one possible implementation of Q-Prop, and in Appendix C we show a more general form that can interpolate between pure policy gradient and off-policy actor-critic.\nA limitation with Algorithm 1 is that computation time is significantly more than TRPO because it requires many gradient steps to train Qw at each iteration. Fortunately, training routines for Qw can be made asynchronous. Furthermore, in real-world applications, data collection speed is often the bottleneck, enabling sufficient time between policy iterations to fit Qw well with the updated policy πθ and the replay data."
    }, {
      "heading" : "4 RELATED WORK",
      "text" : "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Williams, 1992; Greensmith et al., 2004; Schulman et al., 2016). However, exploration of action-dependent control variates is relatively recent, with most work focusing instead on simpler baselining techniques (Ross, 2006). A subtle exception is compatible feature approximation (Sutton et al., 1999) which can be viewed as a control variate as explained in Appendix B. Another exception is doubly robust estimator in contextual bandits (Dudı́k et al., 2011), which uses a different control variate whose bias cannot be tractably corrected. Control variates were explored recently not in RL but for approximate inference in stochastic models (Paisley et al., 2012), and the closest related work in that domain is the MuProp algorithm (Gu et al., 2016a) which uses a mean-field network as a surrogate for backpropagating a deterministic gradient through stochastic discrete variables. MuProp is not directly applicable to model-free RL because the dynamics are unknown; however, it can be if the dynamics are learned as in model-based RL (Atkeson & Santamaria, 1997; Deisenroth & Rasmussen, 2011). This model-based Q-Prop is itself an interesting direction of research as it effectively corrects bias in model-based learning.\nPart of the benefit of Q-Prop is the ability to use off-policy data to improve on-policy policy gradient methods. Prior methods that combine off-policy data with policy gradients either introduce bias (Sutton et al., 1999; Silver et al., 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013). Q-Prop provides a new approach for using off-policy data to reduce variance, while remaining unbiased."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We evaluated Q-Prop and its variants on continuous control environments from the OpenAI Gym benchmark (Brockman et al., 2016) using the MuJoCo physics simulator (Todorov et al., 2012) as shown in Figure 1. Algorithms are identified by acronyms, followed by a number indicating batch\nsize, except for DDPG, which is a prior online actor-critic algorithm (Lillicrap et al., 2016). “c-” and “v-” denote conservative and aggressive Q-Prop variants as described in Section 3.2. “TR-” denotes trust-region policy optimization (Schulman et al., 2015), while “V-” denotes vanilla policy gradient. For example, “TR-c-Q-Prop-5000” means convervative Q-Prop with the trust-region policy update, and a batch size of 5000. “VPG” and “TRPO” are vanilla policy gradient and trust-region policy optimization respectively (Schulman et al., 2016; Duan et al., 2016). Unless otherwise stated, all policy gradient methods are implemented with GAE(λ = 0.97) (Schulman et al., 2016). Note that TRPOGAE is currently the state-of-the-art method on most of the OpenAI Gym benchmark tasks, though our experiments show that a well-tuned DDPG implementation sometimes achieves better results. Our algorithm implementations are built on top of the rllab TRPO and DDPG codes from Duan et al. (2016) and will be released upon publication. Policy and value function architectures and other training details including hyperparameter values are provided in Appendix D."
    }, {
      "heading" : "5.1 ADAPTIVE Q-PROP",
      "text" : "First, it is useful to identify how reliable each variant of Q-Prop is. In this section, we analyze standard Q-Prop and two adaptive variants, c-Q-Prop and a-Q-Prop, and demonstrate the stability of the method across different batch sizes. Figure 2a shows a comparison of Q-Prop variants with trust-region updates on the HalfCheetah-v1 domain, along with the best performing TRPO hyperparameters. The results are consistent with theory: conservative Q-Prop achieves much more stable performance than the standard and aggressive variants, and all Q-Prop variants significantly outperform TRPO in terms of sample efficiency, e.g. conservative Q-Prop reaches average reward of 4000 using about 10 times less samples than TRPO.\nFigure 2b shows the performance of conservative Q-Prop against TRPO across different batch sizes. Due to high variance in gradient estimates, TRPO typically requires very large batch sizes, e.g. 25000 steps or 25 episodes per update, to perform well. We show that our Q-Prop methods can learn even with just 1 episode per update, and achieves better sample efficiency with small batch sizes. This shows that Q-Prop significantly reduces the variance compared to the prior methods.\nAs we discussed in Section 1, stability is a significant challenge with state-of-the-art deep RL methods, and is very important for being able to reliably use deep RL for real world tasks. In the rest of the experiments, we will use conservative Q-Prop as the main Q-Prop implementation."
    }, {
      "heading" : "5.2 EVALUATION ACROSS ALGORITHMS",
      "text" : "In this section, we evaluate two versions of conservative Q-Prop, v-c-Q-Prop using vanilla policy gradient and TR-c-Q-Prop using trust-region updates, against other model-free algorithms on the HalfCheetah-v1 domain. Figure 3a shows that c-Q-Prop methods significantly outperform the best TRPO and VPG methods. Even Q-Prop with vanilla policy gradient outperforms TRPO, confirming the significant benefits from variance reduction. DDPG on the other hand exhibits inconsistent performances. With proper reward scaling, i.e. “DDPG-r0.1”, it outperforms other methods as well\nas the DDPG results reported in prior work (Duan et al., 2016; Amos et al., 2016). This illustrates the sensitivity of DDPG to hyperparameter settings, while Q-Prop exhibits more stable, monotonic learning behaviors when compared to DDPG. In the next section we show this improved stability allows Q-Prop to outperform DDPG in more complex domains."
    }, {
      "heading" : "5.3 EVALUATION ACROSS DOMAINS",
      "text" : "Lastly, we evaluate Q-Prop against TRPO and DDPG across multiple domains. While the gym environments are biased toward locomotion, we expect we can achieve similar performance on manipulation tasks such as those in Lillicrap et al. (2016). Table 1 summarizes the results, including the best attained average rewards and the steps to convergence. Q-Prop consistently outperform TRPO in terms of sample complexity and sometimes achieves higher rewards than DDPG in more complex domains. A particularly notable case is shown in Figure 3b, where Q-Prop substantially improves sample efficiency over TRPO on Humanoid-v1 domain, while DDPG cannot find a good solution.\nThe better performance on the more complex domains highlights the importance of stable deep RL algorithms: while costly hyperparameter sweeps may allow even less stable algorithms to perform well on simpler problems, more complex tasks might have such narrow regions of stable hyperparameters that discovering them becomes impractical."
    }, {
      "heading" : "6 DISCUSSION AND CONCLUSION",
      "text" : "We presented Q-Prop, a policy gradient algorithm that combines reliable, consistent, and potentially unbiased on-policy gradient estimation with a sample-efficient off-policy critic that acts as a control variate. The method provides a large improvement in sample efficiency compared to state-of-the-art policy gradient methods such as TRPO, while outperforming state-of-the-art actor-critic methods on more challenging tasks such as humanoid locomotion. We hope that techniques like these, which combine unbiased gradient estimation with sample-efficient variance reduction through off-policy critics, will eventually lead to deep reinforcement learning algorithms that are more stable and efficient, and therefore better suited for application to complex real-world learning tasks."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Rocky Duan for answering questions about rllab code, and Yutian Chen and Laurent Dinh for discussion on control variates. SG was funded by NSERC and a Google Focused Research Award."
    }, {
      "heading" : "A Q-PROP ESTIMATOR DERIVATION",
      "text" : "The full derivation of the Q-Prop estimator is shown in Eq. 14. We make use of the following property that is commonly used in baseline derivations:\nEpθ (x)[∇θ log pθ (x)] = ∫\nx ∇θ pθ (x) = ∇θ\n∫\nx p(x) = 0\nThis holds true when f (st ,at) is an arbitrary function differentiable with respect to at and f̄ is its first-order Taylor expansion around at = āt , i.e. f̄ (st ,at) = f (st , āt)+∇a f (st ,a)|a=āt (at − āt). Here, µθ (st) = Eπ [at ] is the mean of stochastic policy πθ . The derivation appears below:\n∇θ J(θ) = Eρπ ,π [∇θ logπθ (at |st)(Q̂(st ,at)− f̄ (st ,at)]+Eρπ ,π [∇θ logπθ (at |st) f̄ (st ,at)] g(θ) = Eρπ ,π [∇θ logπθ (at |st) f̄ (st ,at)]\n= Eρπ ,π [∇θ logπθ (at |st)( f (st , āt)+∇a f (st ,a)|a=āt (at − āt))] = Eρπ ,π [∇θ logπθ (at |st)∇a f (st ,a)|a=ātat ] = Eρπ [∫\nat\n∇θ πθ (at |st)∇a f (st ,a)|a=ātat ]\n= Eρπ [ ∇a f (st ,a)|a=āt ∫\nat\n∇θ πθ (at |st)at ]\n= Eρπ [∇a f (st ,a)|a=āt ∇θEπ [at ]] = Eρπ [∇a f (st ,a)|a=āt ∇θµθ (st)]\n∇θ J(θ) = Eρπ ,π [∇θ logπθ (at |st)(Q̂(st ,at)− f̄ (st ,at)]+g(θ) = Eρπ ,π [∇θ logπθ (at |st)(Q̂(st ,at)− f̄ (st ,at)]+Eρπ [∇a f (st ,a)|a=āt ∇θµθ (st)]\n(14)"
    }, {
      "heading" : "B CONNECTION BETWEEN Q-PROP AND COMPATIBLE FEATURE APPROXIMATION",
      "text" : "In this section we show that actor-critic with compatible feature approximation is a form of control variate. A critic Qw is compatible (Sutton et al., 1999) if it satisfies (1) Qw(st ,at) = wT ∇θ logπθ (at |st), i.e. ∇wQw(st ,at) = ∇θ logπθ (at |st), and (2) w is fit with objective w = argminw L(w) = argminwEρπ ,π [(Q̂(st ,at)−Qw(st ,at))2], that is fitting Qw on on-policy Monte Carlo returns. Condition (2) implies the following identity,\n∇wL = 2Eρπ ,π [∇θ logπθ (at |st)(Q̂(st ,at)−Qw(st ,at))] = 0. (15)\nIn compatible feature approximation, it directly uses Qw as control variate, rather than its Taylor expansion Q̄w as in Q-Prop. Using Eq. 15, the unbiased policy gradient is,\n∇θ J(θ) = Eρπ ,π [∇θ logπθ (at |st)Qw(st ,at)] = Eρπ ,π [(∇θ logπθ (at |st)∇θ logπθ (at |st)T )w] = Eρπ [I(θ ;st)w],\n(16)\nwhere I(θ ;st) = Eπθ [∇θ logπθ (at |st)∇θ logπθ (at |st)T ] is Fisher’s information matrix. Thus, variance reduction depends on ability to compute or estimate I(θ ;st) and w effectively."
    }, {
      "heading" : "C UNIFYING POLICY GRADIENT AND ACTOR-CRITIC",
      "text" : "Q-Prop closely ties together policy gradient and actor-critic algorithms. To analyze this point, we write a generalization of Eq. 9 below, introducing two additional variables α,ρCR:\n∇θ J(θ) ∝αEρπ ,π [∇θ logπθ (at |st)(Â(st ,at)−ηĀw(st ,at)] +ηEρCR [∇aQw(st ,a)|a=µθ (st )∇θµθ (st)]\n(17)\nEq. 17 enables more analysis where bias generally is introduced only when α 6= 1 or ρCR 6= ρπ . Importantly, Eq. 17 covers both policy gradient and deterministic actor-critic algorithm as its special cases. Standard policy gradient is recovered by η = 0, and deterministic actor-critic is recovered by α = 0 and ρCR = ρβ . This allows heuristic or automatic methods for dynamically changing these variables through the learning process for optimizing different metrics, e.g. sample efficiency, convergence speed, stability.\nTable 2 summarizes the various edge cases of Eq. 17. For example, since we derive our method from a control variates standpoint, Qw can be any function and the gradient remains unbiased. A natural\nchoice is to use off-policy temporal difference learning to learn the critic Qw corresponding to policy π . This enables effectively utilizing off-policy samples without introducing bias. An interesting alternative to this is to utilize model-based roll-outs to estimate the critic, which resembles MuProp in stochastic neural networks (Gu et al., 2016a). Unlike prior work on using fitted dynamics model to accelerate model-free learning (Gu et al., 2016b), this approach does not introduce bias to the gradient of the original objective."
    }, {
      "heading" : "D EXPERIMENT DETAILS",
      "text" : "Policy and value function architectures. The network architectures are largely based on the benchmark paper by Duan et al. (2016). For policy gradient methods, the stochastic policy πθ (at |st) = N (µθ (st),Σθ ) is a local Gaussian policy with a local state-dependent mean and a global covariance matrix. µθ (st) is a neural network with 3 hidden layers of sizes 100-50-25 and tanh nonlinearities at the first 2 layers, and Σθ is diagonal. For DDPG, the policy is deterministic and has the same architecture as µθ except that it has an additional tanh layer at the output. Vφ (st) for baselines and GAE is fit with the same technique by Schulman et al. (2016), a variant of linear regression on Monte Carlo returns with soft-update constraint. For Q-Prop and DDPG, Qw(s,a) is parametrized with a neural network with 2 hidden layers of size 100 and ReLU nonlinearity, where a is included after the first hidden layer.\nTraining details. This section describes parameters of the training algorithms and their hyperparameter search values in {}. The optimal performing hyperparameter results are reported. Policy gradient methods (VPG, TRPO, Q-Prop) used batch sizes of {1000, 5000, 25000} time steps, step sizes of {0.1, 0.01, 0.001} for the trust-region method, and base learning rates of {0.001, 0.0001} with Adam (Kingma & Ba, 2014) for vanilla policy gradient methods. For Q-Prop and DDPG, Qw is learned with the same technique as in DDPG (Lillicrap et al., 2016), using soft target networks with τ = 0.999, a replay buffer of size 106 steps, a mini-batch size of 64, and a base learning rate of {0.001, 0.0001} with Adam (Kingma & Ba, 2014). For Q-Prop we also tuned the relative ratio of gradient steps on the critic Qw against the number of steps on the policy, in the range {0.1, 0.5, 1.0}, where 0.1 corresponds to 100 critic updates for every policy update if the batch size is 1000. For DDPG, we swept the reward scaling using {0.01,0.1,1.0} as it is sensitive to this parameter."
    } ],
    "references" : [ {
      "title" : "Input convex neural networks",
      "author" : [ "Brandon Amos", "Lei Xu", "J Zico Kolter" ],
      "venue" : "arXiv preprint arXiv:1609.07152,",
      "citeRegEx" : "Amos et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Amos et al\\.",
      "year" : 2016
    }, {
      "title" : "A comparison of direct and model-based reinforcement learning",
      "author" : [ "Christopher G Atkeson", "Juan Carlos Santamaria" ],
      "venue" : "In In International Conference on Robotics and Automation. Citeseer,",
      "citeRegEx" : "Atkeson and Santamaria.,? \\Q1997\\E",
      "shortCiteRegEx" : "Atkeson and Santamaria.",
      "year" : 1997
    }, {
      "title" : "Pilco: A model-based and data-efficient approach to policy search",
      "author" : [ "Marc Deisenroth", "Carl E Rasmussen" ],
      "venue" : "In Proceedings of the 28th International Conference on machine learning",
      "citeRegEx" : "Deisenroth and Rasmussen.,? \\Q2011\\E",
      "shortCiteRegEx" : "Deisenroth and Rasmussen.",
      "year" : 2011
    }, {
      "title" : "Benchmarking deep reinforcement learning for continuous control",
      "author" : [ "Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Duan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2016
    }, {
      "title" : "Doubly robust policy evaluation and learning",
      "author" : [ "Miroslav Dudı́k", "John Langford", "Lihong Li" ],
      "venue" : "arXiv preprint arXiv:1103.4601,",
      "citeRegEx" : "Dudı́k et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudı́k et al\\.",
      "year" : 2011
    }, {
      "title" : "Variance reduction techniques for gradient estimates in reinforcement learning",
      "author" : [ "Evan Greensmith", "Peter L Bartlett", "Jonathan Baxter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Greensmith et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Greensmith et al\\.",
      "year" : 2004
    }, {
      "title" : "Muprop: Unbiased backpropagation for stochastic neural networks",
      "author" : [ "Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih" ],
      "venue" : "International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Gu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous deep q-learning with model-based acceleration",
      "author" : [ "Shixiang Gu", "Tim Lillicrap", "Ilya Sutskever", "Sergey Levine" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Gu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Double q-learning",
      "author" : [ "Hado V Hasselt" ],
      "venue" : "In Advances in Neural Information Processing Systems, pp. 2613–2621,",
      "citeRegEx" : "Hasselt.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hasselt.",
      "year" : 2010
    }, {
      "title" : "A natural policy gradient",
      "author" : [ "Sham Kakade" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kakade.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kakade.",
      "year" : 2001
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Deterministic policy gradient algorithms",
      "author" : [ "Guy Lever" ],
      "venue" : null,
      "citeRegEx" : "Lever.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lever.",
      "year" : 2014
    }, {
      "title" : "Guided policy search",
      "author" : [ "Sergey Levine", "Vladlen Koltun" ],
      "venue" : "In International Conference on Machine Learning (ICML), pp",
      "citeRegEx" : "Levine and Koltun.,? \\Q2013\\E",
      "shortCiteRegEx" : "Levine and Koltun.",
      "year" : 2013
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra" ],
      "venue" : "International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural variational inference and learning in belief networks",
      "author" : [ "Andriy Mnih", "Karol Gregor" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Mnih and Gregor.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih and Gregor.",
      "year" : 2014
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Safe and efficient offpolicy reinforcement learning",
      "author" : [ "Rémi Munos", "Tom Stepleton", "Anna Harutyunyan", "Marc G Bellemare" ],
      "venue" : "arXiv preprint arXiv:1606.02647,",
      "citeRegEx" : "Munos et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Munos et al\\.",
      "year" : 2016
    }, {
      "title" : "Variational bayesian inference with stochastic search",
      "author" : [ "John Paisley", "David Blei", "Michael Jordan" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Paisley et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Paisley et al\\.",
      "year" : 2012
    }, {
      "title" : "Policy gradient methods for robotics",
      "author" : [ "Jan Peters", "Stefan Schaal" ],
      "venue" : "In International Conference on Intelligent Robots and Systems (IROS),",
      "citeRegEx" : "Peters and Schaal.,? \\Q2006\\E",
      "shortCiteRegEx" : "Peters and Schaal.",
      "year" : 2006
    }, {
      "title" : "Relative entropy policy search",
      "author" : [ "Jan Peters", "Katharina Mülling", "Yasemin Altun" ],
      "venue" : "In AAAI. Atlanta,",
      "citeRegEx" : "Peters et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2010
    }, {
      "title" : "Eligibility traces for off-policy policy evaluation",
      "author" : [ "Doina Precup" ],
      "venue" : "Computer Science Department Faculty Publication Series, pp",
      "citeRegEx" : "Precup.,? \\Q2000\\E",
      "shortCiteRegEx" : "Precup.",
      "year" : 2000
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Highdimensional continuous control using generalized advantage estimation",
      "author" : [ "John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel" ],
      "venue" : "International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Schulman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2016
    }, {
      "title" : "Deterministic policy gradient algorithms",
      "author" : [ "David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Silver et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2014
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
      "author" : [ "Richard S Sutton" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Sutton.,? \\Q1990\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1990
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Mujoco: A physics engine for model-based control",
      "author" : [ "Emanuel Todorov", "Tom Erez", "Yuval Tassa" ],
      "venue" : "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
      "citeRegEx" : "Todorov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Todorov et al\\.",
      "year" : 2012
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "For policy gradient methods, the stochastic policy πθ (at |st) = N (μθ (st),Σθ ) is a local Gaussian policy with a local state-dependent mean and a global covariance matrix. μθ (st) is a neural network with 3 hidden layers of sizes 100-50-25 and tanh nonlinearities at the first 2 layers",
      "author" : [ "Duan" ],
      "venue" : null,
      "citeRegEx" : "Duan,? \\Q2016\\E",
      "shortCiteRegEx" : "Duan",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 293
    }, {
      "referenceID" : 22,
      "context" : "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 293
    }, {
      "referenceID" : 13,
      "context" : "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 293
    }, {
      "referenceID" : 25,
      "context" : "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 293
    }, {
      "referenceID" : 16,
      "context" : "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 293
    }, {
      "referenceID" : 8,
      "context" : "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.",
      "startOffset" : 85,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.",
      "startOffset" : 85,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.",
      "startOffset" : 85,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "Policy gradient methods (Peters & Schaal, 2006; Schulman et al., 2015) are popular on-policy methods that directly maximize the cumulative future returns with respect to the policy.",
      "startOffset" : 24,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "To cope with high variance gradient estimates and difficult optimization landscapes, a number of techniques have been proposed, including constraining the change in the policy at each gradient step (Kakade, 2001; Peters et al., 2010) and mixing valuebased back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al.",
      "startOffset" : 198,
      "endOffset" : 233
    }, {
      "referenceID" : 20,
      "context" : "To cope with high variance gradient estimates and difficult optimization landscapes, a number of techniques have been proposed, including constraining the change in the policy at each gradient step (Kakade, 2001; Peters et al., 2010) and mixing valuebased back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al.",
      "startOffset" : 198,
      "endOffset" : 233
    }, {
      "referenceID" : 22,
      "context" : ", 2010) and mixing valuebased back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 125
    }, {
      "referenceID" : 27,
      "context" : "Off-policy methods, such as Q-learning (Watkins & Dayan, 1992; Sutton et al., 1999; Mnih et al., 2015; Gu et al., 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al.",
      "startOffset" : 39,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "Off-policy methods, such as Q-learning (Watkins & Dayan, 1992; Sutton et al., 1999; Mnih et al., 2015; Gu et al., 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al.",
      "startOffset" : 39,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : ", 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al., 2016), can instead use all samples, including off-policy samples, by adopting temporal difference learning with experience replay.",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : ", 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al., 2016), can instead use all samples, including off-policy samples, by adopting temporal difference learning with experience replay.",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "Unlike prior approaches for off-policy learning, which either introduce bias (Sutton et al., 1999; Silver et al., 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 24,
      "context" : "Unlike prior approaches for off-policy learning, which either introduce bias (Sutton et al., 1999; Silver et al., 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : ", 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al., 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al.",
      "startOffset" : 29,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : ", 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al., 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al.",
      "startOffset" : 29,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : ", 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al., 2016) which fit the value function on-policy, Q-Prop learns the action-value function off-policy.",
      "startOffset" : 143,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "We show that Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE) (Schulman et al., 2015; 2016), and improved stability over deep deterministic policy gradient (DDPG) (Lillicrap et al.",
      "startOffset" : 157,
      "endOffset" : 186
    }, {
      "referenceID" : 13,
      "context" : ", 2015; 2016), and improved stability over deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016) across a repertoire of continuous control tasks.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 29,
      "context" : "The standard form, known as the REINFORCE algorithm (Williams, 1992), is shown below:",
      "startOffset" : 52,
      "endOffset" : 68
    }, {
      "referenceID" : 27,
      "context" : "1“Policy gradient” technically refers to a more general class of methods (Sutton et al., 1999); however, to make it compatible with references in other work we use it to primarily refer REINFORCE (Williams, 1992).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 29,
      "context" : ", 1999); however, to make it compatible with references in other work we use it to primarily refer REINFORCE (Williams, 1992).",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "Prior attempts use importance sampling to use off-policy trajectories; however, these are known to be difficult scale to high-dimensional action spaces because of rapidly degenerating importance weights (Precup, 2000).",
      "startOffset" : 203,
      "endOffset" : 217
    }, {
      "referenceID" : 27,
      "context" : "Actor-critic methods (Sutton et al., 1999) include a policy evaluation step, which uses temporal difference (TD) learning to fit a critic Qw for the current policy π(θ), and a policy improvement step which greedily optimizes the policy π against the critic estimate Qw.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 26,
      "context" : "Significant gain in sample efficiency is achievable using off-policy TD learning for the critic, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), recently popularized by experience replay for training deep Q networks (Mnih et al.",
      "startOffset" : 148,
      "endOffset" : 183
    }, {
      "referenceID" : 24,
      "context" : "Significant gain in sample efficiency is achievable using off-policy TD learning for the critic, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), recently popularized by experience replay for training deep Q networks (Mnih et al.",
      "startOffset" : 148,
      "endOffset" : 183
    }, {
      "referenceID" : 15,
      "context" : ", 2014), recently popularized by experience replay for training deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b).",
      "startOffset" : 80,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : ", 2014), recently popularized by experience replay for training deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b).",
      "startOffset" : 80,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "Deep deterministic policy gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016) is an instance of off-policy algorithms which achieves significant results on high-dimensional continuous control tasks.",
      "startOffset" : 42,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "Deep deterministic policy gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016) is an instance of off-policy algorithms which achieves significant results on high-dimensional continuous control tasks.",
      "startOffset" : 42,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "These properties make DDPG and other analogous off-policy methods significantly more sample-efficient than policy gradient methods (Lillicrap et al., 2016; Gu et al., 2016b; Duan et al., 2016).",
      "startOffset" : 131,
      "endOffset" : 192
    }, {
      "referenceID" : 3,
      "context" : "These properties make DDPG and other analogous off-policy methods significantly more sample-efficient than policy gradient methods (Lillicrap et al., 2016; Gu et al., 2016b; Duan et al., 2016).",
      "startOffset" : 131,
      "endOffset" : 192
    }, {
      "referenceID" : 18,
      "context" : "By using the deterministic biased estimator as a particular form of control variate (Ross, 2006; Paisley et al., 2012) for the unbiased policy gradient estimator, we can effectively use both types of gradient information to construct a new estimator that is in general unbiased, and in practice exhibits improved sample efficiency through the inclusion of off-policy samples.",
      "startOffset" : 84,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "Following the prior work on control variate (Ross, 2006; Paisley et al., 2012), we first introduce η(st) to Eq.",
      "startOffset" : 44,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "difficult (Williams, 1992).",
      "startOffset" : 10,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "In our implementation, the critic Qw is fitted with off-policy TD learning using the same techniques as in DDPG (Lillicrap et al., 2016): w = argmin w Est∼ρβ (·),at∼β (·|st )[(r(st ,at)+ γEπ [Q ′(st+1,at+1)]−Qw(st ,at))].",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 23,
      "context" : "(13) Vφ is fitted with the same technique in (Schulman et al., 2016).",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "Generalized advantage estimation (GAE) (Schulman et al., 2016) is used to estimate Â.",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "The policy update can be done by any method that utilizes the first-order gradient and possibly the on-policy batch data, which includes trust region policy optimization (TRPO) (Schulman et al., 2015).",
      "startOffset" : 177,
      "endOffset" : 200
    }, {
      "referenceID" : 29,
      "context" : "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Williams, 1992; Greensmith et al., 2004; Schulman et al., 2016).",
      "startOffset" : 105,
      "endOffset" : 169
    }, {
      "referenceID" : 5,
      "context" : "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Williams, 1992; Greensmith et al., 2004; Schulman et al., 2016).",
      "startOffset" : 105,
      "endOffset" : 169
    }, {
      "referenceID" : 23,
      "context" : "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Williams, 1992; Greensmith et al., 2004; Schulman et al., 2016).",
      "startOffset" : 105,
      "endOffset" : 169
    }, {
      "referenceID" : 27,
      "context" : "A subtle exception is compatible feature approximation (Sutton et al., 1999) which can be viewed as a control variate as explained in Appendix B.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "Another exception is doubly robust estimator in contextual bandits (Dudı́k et al., 2011), which uses a different control variate whose bias cannot be tractably corrected.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 18,
      "context" : "Control variates were explored recently not in RL but for approximate inference in stochastic models (Paisley et al., 2012), and the closest related work in that domain is the MuProp algorithm (Gu et al.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 27,
      "context" : "Prior methods that combine off-policy data with policy gradients either introduce bias (Sutton et al., 1999; Silver et al., 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).",
      "startOffset" : 87,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "Prior methods that combine off-policy data with policy gradients either introduce bias (Sutton et al., 1999; Silver et al., 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).",
      "startOffset" : 87,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : ", 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).",
      "startOffset" : 147,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "Figure 1: Illustrations of OpenAI Gym MuJoCo domains (Brockman et al., 2016; Duan et al., 2016): (a) Ant, (b) HalfCheetah, (c) Hopper, (d) Humanoid, (e) Reacher, (f) Swimmer, (g) Walker.",
      "startOffset" : 53,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : ", 2016) using the MuJoCo physics simulator (Todorov et al., 2012) as shown in Figure 1.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "size, except for DDPG, which is a prior online actor-critic algorithm (Lillicrap et al., 2016).",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 22,
      "context" : "“TR-” denotes trust-region policy optimization (Schulman et al., 2015), while “V-” denotes vanilla policy gradient.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : "“VPG” and “TRPO” are vanilla policy gradient and trust-region policy optimization respectively (Schulman et al., 2016; Duan et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "“VPG” and “TRPO” are vanilla policy gradient and trust-region policy optimization respectively (Schulman et al., 2016; Duan et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 23,
      "context" : "97) (Schulman et al., 2016).",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : ", 2016; Duan et al., 2016). Unless otherwise stated, all policy gradient methods are implemented with GAE(λ = 0.97) (Schulman et al., 2016). Note that TRPOGAE is currently the state-of-the-art method on most of the OpenAI Gym benchmark tasks, though our experiments show that a well-tuned DDPG implementation sometimes achieves better results. Our algorithm implementations are built on top of the rllab TRPO and DDPG codes from Duan et al. (2016) and will be released upon publication.",
      "startOffset" : 8,
      "endOffset" : 448
    }, {
      "referenceID" : 3,
      "context" : "as the DDPG results reported in prior work (Duan et al., 2016; Amos et al., 2016).",
      "startOffset" : 43,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "as the DDPG results reported in prior work (Duan et al., 2016; Amos et al., 2016).",
      "startOffset" : 43,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "While the gym environments are biased toward locomotion, we expect we can achieve similar performance on manipulation tasks such as those in Lillicrap et al. (2016). Table 1 summarizes the results, including the best attained average rewards and the steps to convergence.",
      "startOffset" : 141,
      "endOffset" : 165
    } ],
    "year" : 2016,
    "abstractText" : "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is the high sample complexity of such methods. Unbiased batch policy-gradient methods offer stable learning, but at the cost of high variance, which often requires large batches, while TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of unbiased policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and offpolicy methods. We analyze the connection between Q-Prop and existing modelfree algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym’s MuJoCo continuous control environments.",
    "creator" : "LaTeX with hyperref package"
  }
}