{
  "name" : "1706.04690.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP",
    "authors" : [ "Satyen Kale" ],
    "emails" : [ "satyenkale@google.com", "zkarnin@gmail.com", "Tengyuan.Liang@chicagobooth.edu", "dpal@yahoo-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 6.\n04 69\n0v 1\n[ cs\n.L G\n] 1\nOnline sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially."
    }, {
      "heading" : "1 Introduction",
      "text" : "In modern real-world sequential prediction problems, samples are typically high dimensional, and construction of the features may itself be a computationally intensive task. Therefore in sequential prediction, due to the computation and resource constraints, it is preferable to design algorithms that compute only a limited number of features for each new data example. One example of this situation, from [Cesa-Bianchi et al., 2011], is medical diagnosis of a disease, in which each feature is the result of a medical test on the patient. Since it is undesirable to subject a patient to a battery of medical tests, we would like to adaptively design diagnostic procedures that rely on only a few, highly informative tests.\nOnline sparse linear regression (OSLR) is a sequential prediction problem in which an algorithm is allowed to see only a small subset of coordinates of each feature vector. The problem is parameterized by 3 positive integers: d, the dimension of the feature vectors, k, the sparsity of the linear regressors we compare the algorithm’s performance to, and k0, a budget on the number of features that can be queried in each round by the algorithm. Generally we have k ≪ d and k0 ≥ k but not significantly larger (our algorithms need1 k0 = Õ(k)).\nIn the OSLR problem, the algorithm makes predictions over a sequence of T rounds. In each round t, nature chooses a feature vector xt ∈ Rd, the algorithm chooses a subset of {1, 2, . . . , d} of size at most k′ and observes the corresponding coordinates of the feature vector. It then makes a prediction ŷt ∈ R based on the observed features, observes the true label yt, and suffers loss (yt − ŷt)2. The goal of the learner is\n∗This work was done while the author was at Yahoo Research, New York. 1In this paper, we use the Õ(·) notation to suppress factors that are polylogarithmic in the natural parameters of the problem.\nto make the cumulative loss comparable to that of the best k-sparse linear predictor w in hindsight. The performance of the online learner is measured by the regret, which is defined as the difference between the two losses:\nRegretT = T∑\nt=1\n(yt − ŷt)2 − min w: ‖w‖0≤k\nT∑\nt=1\n(yt − 〈xt, w〉)2 .\nThe goal is to construct algorithms that enjoy regret that is sub-linear in T , the total number of rounds. A sub-linear regret implies that in the asymptotic sense, the average per-round loss of the algorithm approaches the average per-round loss of the best k-sparse linear predictor.\nSparse regression is in general a computationally hard problem. In particular, given k, x1, x2, . . . , xT and y1, y2, . . . , yT as inputs, the offline problem of finding a k-sparsew that minimizes the error ∑T t=1(yt−〈xt, w〉)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w∗ such that yt = 〈xt, w∗〉 for all t. Furthermore, the computational hardness is present even when the solution is required to be only Õ(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details. The hardness result was extended to online sparse regression by Foster et al. [2016]. They showed that for all δ > 0 there exists no polynomial-time algorithm with regret O(T 1−δ) unless NP ⊆ BPP .\nFoster et al. [2016] posed the open question of what additional assumptions can be made on the data to make the problem tractable. In this paper, we answer this open question by providing efficient algorithms with sublinear regret under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) [Candes and Tao, 2005]. It has been shown that if RIP holds and there exists a sparse linear predictor w∗ such that yt = 〈xt, w∗〉 + ηt where ηt is independent noise, the offline sparse linear regression problem admits computationally efficient algorithms, e.g., Candes and Tao [2007]. RIP and related Restricted Eigenvalue Condition [Bickel et al., 2009] have been widely used as a standard assumption for theoretical analysis in the compressive sensing and sparse regression literature, in the offline case. In the online setting, it is natural to ask whether sparse regression avoids the computational difficulty under an appropriate form of the RIP condition. In this paper, we answer this question in a positive way, both in the realizable setting and in the agnostic setting. As a by-product, we resolve the adaptive feature selection problem as the efficient algorithms we propose in this paper adaptively choose a different “sparse” subset of features to query at each round. This is closely related to attribute-efficient learning (see discussion in Section 1.2) and online model selection."
    }, {
      "heading" : "1.1 Summary of Results",
      "text" : "We design polynomial-time algorithms for online sparse linear regression for two models for the sequence (x1, y1), (x2, y2), . . . , (xT , yT ). The first model is called the realizable and the second is called agnostic. In both models, we assume that, after proper normalization, for all large enough t, the matrix Xt formed from the first t feature vectors x1, x2, . . . , xt satisfies the restricted isometry property. The two models differ in the assumptions on yt. The realizable model assumes that yt = 〈xt, w∗〉 + ηt where w∗ is k-sparse and ηt is an independent noise. In the agnostic model, yt can be arbitrary, and therefore, the regret bounds we obtain are worse than in the realizable setting. The models and corresponding algorithms are presented in Sections 2 and 3 respectively. Interestingly enough, the algorithms and their corresponding analyses are completely different in the realizable and agnostic case.\nOur algorithms allow for somewhat more flexibility than the problem definition: they are designed to work with a budget k0 on the number of features that can be queried that may be larger than the sparsity parameter k of the comparator. The regret bounds we derive improve with increasing values of k0. In the case when k0 ≈ k, the dependence on d in the regret bounds is polynomial, as can be expected in limited feedback settings (this is analogous to polynomial dependence on d in bandit settings). In the extreme case when k0 = d, i.e. we have access to all the features, the dependence on the dimension d in the regret bounds we prove is only logarithmic. The interpretation is that if we have full access to the features, but the goal is to compete with just k sparse linear regressors, then the number of data points that need to be seen to achieve good predictive accuracy has only logarithmic dependence on d. This is analogous to the (offline)\ncompressed sensing setting where the sample complexity bounds, under RIP, only depend logarithmically on d.\nA major building block in the solution for the realizable setting (Section 2) consists of identifying the best k-sparse linear predictor for the past data at any round in the prediction problem. This is done by solving a sparse regression problem on the observed data. The solution of this problem cannot be obtained by a simple application of say, the Dantzig selector [Candes and Tao, 2007] since we do not observe the data matrix X , but rather a subsample of its entries. Our algorithm is a variant of the Dantzig selector that incorporates random sampling into the optimization, and computes a near-optimal solution by solving a linear program. The resulting algorithm has a regret bound of Õ(logT ). This bound has optimal dependence on T , since even in the full information setting where all features are observed there is a lower bound of Ω(logT ) [Hazan and Kale, 2014].\nThe algorithm for the agnostic setting relies on the theory of submodular optimization. The analysis in [Boutsidis et al., 2015] shows that the RIP assumption implies that the set function defined as the minimum loss achievable by a linear regressor restricted to the set in question satisfies a property called weak supermodularity. Weak supermodularity is a relaxation of standard supermodularity that is still strong enough to show performance bounds for the standard greedy feature selection algorithm for solving the sparse regression problem. We then employ a technique developed by Streeter and Golovin [2008] to construct an online learning algorithm that mimics the greedy feature selection algorithm. The resulting algorithm has a regret bound of Õ(T 2/3). It is unclear if this bound has the optimal dependence on T : it is easy to prove a lower bound of Ω( √ T ) on the regret using standard arguments for the multiarmed bandit problem."
    }, {
      "heading" : "1.2 Related work",
      "text" : "A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers.\nWithout any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound. This question was answered in the negative by Foster et al. [2016], who showed that efficiency can only be obtained under additional assumptions on the data. This paper shows that the RIP assumption yields tractability in the online setting just as it does in the batch setting.\nIn the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al. [2016] (see also the references therein). In fact, these papers solve a more general problem where we observe a matrix Z rather than X that is an unbiased estimator of X . While we can use their results in a black-box manner, they are tailored for the setting where the variance of each Zij is constant and it is difficult to obtain the exact dependence on this variance in their bounds. In our setting, this variance can be linear in the dimension of the feature vectors, and hence we wish to control the dependence on the variance in the bounds. Thus, we use an algorithm that is similar to the one in Belloni et al. [2016], and provide an analysis for it (in the appendix). As an added bonus, our algorithm results in solving a linear program rather than a conic or general convex program, hence admits a solution that is more computationally efficient.\nIn the agnostic setting, the computationally efficient algorithm we propose is motivated from (online) supermodular optimization [Natarajan, 1995, Boutsidis et al., 2015, Streeter and Golovin, 2008]. The algorithm is computationally efficient and enjoys sublinear regret under an RIP-like condition, as we will show in Section 3. This result can be contrasted with the known computationally prohibitive algorithms for online sparse linear regression [Zolghadr et al., 2013, Foster et al., 2016], and the hardness result without RIP\n[Foster et al., 2015, 2016]."
    }, {
      "heading" : "1.3 Notation and Preliminaries",
      "text" : "For d ∈ N, we denote by [d] the set {1, 2, . . . , d}. For a vector in x ∈ Rd, denote by x(i) its i-th coordinate. For a subset S ⊆ [d], we use the notation RS to indicate the vector space spanned by the coordinate axes indexed by S (i.e. the set of all vectors w supported on the set S). For a vector x ∈ Rd, denote by x(S) ∈ Rd the projection of x on RS . That is, the coordinates of x(S) are\nx(S)(i) = { x(i) if i ∈ S, 0 if i 6∈ S, for i = 1, 2, . . . , d.\nLet 〈u, v〉 = ∑i u(i) · v(i) be the inner product of vectors u and v. For p ∈ [0,∞], the ℓp-norm of a vector x ∈ Rd is denoted by ‖x‖p. For p ∈ (0,∞), ‖x‖p = ( ∑ i |xi|p)1/p, ‖x‖∞ = maxi |xi|, and ‖x‖0 is the number of non-zero coordinates of x. The following definition will play a key role:\nDefinition 1 (Restricted Isometry Property Candes and Tao [2007]). Let ǫ ∈ (0, 1) and k ≥ 0. We say that a matrix X ∈ Rn×d satisfies restricted isometry property (RIP) with parameters (ǫ, k) if for any w ∈ Rd with ‖w‖0 ≤ k we have\n(1− ǫ) ‖w‖2 ≤ 1√ n ‖Xw‖2 ≤ (1 + ǫ) ‖w‖2 .\nOne can show that RIP holds with overwhelming probability if n = Ω(ǫ−2k log(ed/k)) and each row of the matrix is sampled independently from an isotropic sub-Gaussian distribution. In the realizable setting, the sub-Gaussian assumption can be relaxed to incorporate heavy tail distribution via the “small ball” analysis introduced in Mendelson [2014], since we only require one-sided lower isometry property."
    }, {
      "heading" : "1.4 Proper Online Sparse Linear Regression",
      "text" : "We introduce a variant of online sparse regression (OSLR), which we call proper online sparse linear regression (POSLR). The adjective “proper” is to indicate that the algorithm is required to output a weight vector in each round and its prediction is computed by taking an inner product with the feature vector.\nWe assume that there is an underlying sequence (x1, y1), (x2, y2), . . . , (xT , yT ) of labeled examples in R\nd × R. In each round t = 1, 2, . . . , T , the algorithm behaves according to the following protocol: 1. Choose a vector wt ∈ Rd such that ‖wt‖0 ≤ k.\n2. Choose St ⊆ [d] of size at most k0.\n3. Observe xt(St) and yt, and incur loss (yt − 〈xt, wt〉)2. Essentially, the algorithm makes the prediction ŷt := 〈xt, wt〉 in round t. The regret after T rounds of an algorithm with respect to w ∈ Rd is\nRegretT (w) =\nT∑\nt=1\n(yt − 〈xt, wt〉)2 − T∑\nt=1\n(yt − 〈xt, w〉)2 .\nThe regret after T rounds of an algorithm with respect to the best k-sparse linear regressor is defined as\nRegretT = max w: ‖w‖0≤k RegretT (w) .\nNote that any algorithm for POSLR gives rise to an algorithm for OSLR. Namely, if an algorithm for POSLR chooseswt and St, the corresponding algorithm for OSLR queries the coordinates St∪{i : wt(i) 6= 0}.\nThe algorithm for OSLR queries at most k0 + k coordinates and has the same regret as the algorithm for POSLR.\nAdditionally, POSLR allows parameters settings which do not have corresponding counterparts in OSLR. Namely, we can consider the sparse “full information” setting where k0 = d and k ≪ d.\nWe denote by Xt the t×d matrix of first t unlabeled samples i.e. rows of Xt are xT1 , xT2 , . . . , xTt . Similarly, we denote by Yt ∈ Rt the vector of first t labels y1, y2, . . . , yt. We use the shorthand notation X , Y for XT and YT respectively.\nIn order to get computationally efficient algorithms, we assume that that for all t ≥ t0, the matrix Xt satisfies the restricted isometry condition. The parameter t0 and RIP parameters k, ǫ will be specified later."
    }, {
      "heading" : "2 Realizable Model",
      "text" : "In this section we design an algorithm for POSLR for the realizable model. In this setting we assume that there is a vector w∗ ∈ Rd such that ‖w∗‖0 ≤ k and the sequence of labels y1, y2, . . . , yT is generated according to the linear model yt = 〈xt, w∗〉+ ηt , (1) where η1, η2, . . . , ηT are independent random variables fromN(0, σ\n2). We assume that the standard deviation σ, or an upper bound of it, is given to the algorithm as input. We assume that ‖w∗‖1 ≤ 1 and ‖xt‖∞ ≤ 1 for all t.\nFor convenience, we use η to denote the vector (η1, η2, . . . , ηT ) of noise variables."
    }, {
      "heading" : "2.1 Algorithm",
      "text" : "The algorithmmaintains an unbiased estimate X̂t of the matrixXt. The rows of X̂t are vectors x̂ T 1 , x̂ T 2 , . . . , x̂ T t which are unbiased estimates of xT1 , x T 2 , . . . , x T t . To construct the estimates, in each round t, the set St ⊆ [d] is chosen uniformly at random from the collection of all subsets of [d] of size k0. The estimate is\nx̂t = d\nk0 · xt(St). (2)\nTo compute the predictions of the algorithm, we consider the linear program\nminimize ‖w‖1 s.t. ∥∥∥∥ 1\nt X̂Tt\n( Yt − X̂tw ) + 1\nt D̂tw ∥∥∥∥ ∞\n≤ C √ d log(td/δ)\ntk0\n( σ + d\nk0\n) .\n(3)\nHere, C > 0 is a universal constant, and δ ∈ (0, 1) is the allowed failure probability. D̂t, defined in equation (5), is a diagonal matrix that offsets the bias on the diag(X̂Tt X̂t).\nThe linear program (3) is called the Dantzig selector. We denote its optimal solution by ŵt+1. (We define ŵ1 = 0.)\nBased on ŵt, we construct w̃t ∈ Rd. Let |ŵt(i1)| ≥ |ŵt(i2)| ≥ · · · ≥ |ŵt(id)| be the coordinates sorted according to the their absolute value, breaking ties according to their index. Let S̃t = {i1, i2, . . . , ik} be the top k coordinates. We define w̃t as\nw̃t = ŵt(S̃t). (4)\nThe actual prediction wt is either zero if t ≤ t0 or w̃s for some s ≤ t and it gets updated whenever t is a power of 2.\nThe algorithm queries at most k + k0 features each round, and the linear program can be solved in polynomial time using simplex method or interior point method. The algorithm solves the linear program only ⌈log2 T ⌉ times by using the same vector in the rounds 2s, . . . , 2s+1− 1. This lazy update improves both the computational aspects of the algorithm and the regret bound.\nAlgorithm 1 Dantzig Selector for POSLR\nRequire: T , σ, t0, k, k0 1: for t = 1, 2, . . . , T do 2: if t ≤ t0 then 3: Predict wt = 0 4: else if t is a power of 2 then 5: Let ŵt be the solution of linear program (3) 6: Compute w̃t according to (4) 7: Predict wt = w̃t 8: else 9: Predict wt = wt−1 10: end if 11: Let St ⊆ [d] be a random subset of size k0 12: Observe xt(St) and yt 13: Construct estimate x̂t according to (2) 14: Append x̂Tt to X̂t−1 to form X̂t ∈ Rt×d 15: end for"
    }, {
      "heading" : "2.2 Main Result",
      "text" : "The main result in this section provides a logarithmic regret bound under the following assumptions 2\n• The feature vectors have the property that for any t ≥ t0, the matrix Xt satisfies the RIP condition with (15 , 3k), with t0 = O(k log(d) log(T )).\n• The underlying POSLR online prediction problem has a sparsity budget of k and observation budget k0.\n• The model is realizable as defined in equation (1) with i.i.d unbiased Gaussian noise with standard deviation σ = O(1).\nTheorem 2. For any δ > 0, with probability at least 1− δ, Algorithm 1 satisfies\nRegretT = O ( k2 log(d/δ)(d/k0) 3 log(T ) ) .\nThe theorem asserts that an O(logT ) regret bound is efficiently achievable in the realizable setting. Furthermore when k0 = Ω(d) the regret scales as log(d) meaning that we do not necessarily require T ≥ d to obtain a meaningful result. We note that the complete expression for arbitrary t0, σ is given in (13) in the appendix.\nThe algorithm can be easily understood via the error-in-variable equation\nyt = 〈xt, w∗〉+ ηt , x̂t = xt + ξt.\nwith E[ξt] = E[x̂t − xt] = 0, where the expectation is taken over random sampling introduced by the algorithm when performing feature exploration. The learner observes yt as well as the “noisy” feature vector x̂t, and aims to recover w\n∗. As mentioned above, we (implicitly) need an unbiased estimator of XTt Xt. By taking X̂ T t X̂t it is easy to verify that the off-diagonal entries are indeed unbiased however this is not the case for the diagonal. To this end we define Dt ∈ Rd×d as the diagonal matrix compensating for the sampling bias on the diagonal elements of X̂Tt X̂t\nDt =\n( d\nk0 − 1\n) · diag ( XTt Xt )\n2A more precise statement with the exact dependence on the problem parameters can be found in the appendix.\nand the estimated bias from the observed data is\nD̂t =\n( 1− k0\nd\n) · diag ( X̂Tt X̂t ) . (5)\nTherefore, program (1) can be viewed as Dantzig selector with plug-in unbiased estimates for XTt Yt and XTt Xt using limited observed features."
    }, {
      "heading" : "2.3 Sketch of Proof",
      "text" : "The main building block in proving Theorem 2 is stated in Lemma 3. It proves that the sequence of solutions ŵt converges to the optimal response w\n∗ based on which the signal yt is created. More accurately, ignoring all second order terms, it shows that ‖ŵt−w∗‖1 ≤ O(1/ √ t). In Lemma 4 we show that the same applies for the sparse approximation wt of ŵt. Now, since ‖xt‖∞ ≤ 1 we get that the difference between our response 〈xt, wt〉 and the (almost) optimal response 〈xt, w∗〉 is bounded by 1/ √ t. Given this, a careful calculation of the difference of losses leads to a regret bound w.r.t. w∗. Specifically, an elementary analysis of the loss expression leads to the equality\nRegretT (w ∗) =\nT∑\nt=1\n2ηt 〈xt, w∗ − wt〉+ (〈xt, w∗ − wt〉)2\nA bound on both summands can clearly be expressed in terms of | 〈xt, w∗ − wt〉 | = O(1/ √ t). The right summand requires a martingale concentration bound and the left is trivial. For both we obtain a bound of O(log(T )).\nWe are now left with two technicalities. The first is that w∗ is not necessarily the empirically optimal response. To this end we provide, in Lemma 16 in the appendix, a constant (independent of T ) bound on the regret of w∗ compared to the empirical optimum. The second technicality is the fact that we do not solve for ŵt in every round, but in exponential gaps. This translates to an added factor of 2 to the bound ‖wt − w∗‖1 that affects only the constants in the O(·) terms. Lemma 3 (Estimation Rates). Assume that the matrix Xt ∈ Rt×d satisfies the RIP condition with (ǫ, 3k) for some ǫ < 1/5. Let ŵn+1 ∈ Rd be the optimal solution of program (3). With probability at least 1− δ,\n‖ŵt+1 − w∗‖2 ≤ C · √ d\nk0 · k log(d/δ) t\n( σ + d\nk0\n) ,\n‖ŵt+1 − w∗‖1 ≤ C · √ d\nk0\nk2 log(d/δ)\nt\n( σ + d\nk0\n) .\nHere C > 0 is some universal constant and σ is the standard deviation of the noise.\nNote the ŵt may not be sparse; it can have many non-zero coordinates that are small in absolute value. However, we take the top k coordinates of ŵt in absolute value. Thanks to the Lemma 4 below, we lose only a constant factor √ 3.\nLemma 4. Let ŵ ∈ Rd be an arbitrary vector and let w∗ ∈ Rd be a k-sparse vector. Let S̃ ⊆ [d] be the top k coordinates of ŵ in absolute value. Then,\n∥∥∥ŵ(S̃)− w∗ ∥∥∥ 2 ≤ √ 3 ‖ŵ − w∗‖2 ."
    }, {
      "heading" : "3 Agnostic Setting",
      "text" : "In this section we focus on the agnostic setting, where we don’t impose any distributional assumption on the sequence. In this setting, there is no “true” sparse model, but the learner — with limited access to features — is competing with the best k-sparse model defined using full information {(xt, yt)}Tt=1.\nAs before, we do assume that xt and yt are bounded. Without loss of generality, ‖xt‖∞ ≤ 1, and |yt| ≤ 1 for all t. Once again, without any regularity condition on the design matrix, Foster et al. [2016] have shown that achieving a sub-linear regret O(T 1−δ) is in general computationally hard, for any constant δ > 0 unless NP ⊆ BPP.\nWe give an efficient algorithm that achieves sub-linear regret under the assumption that the design matrix of any (sufficiently long) block of consecutive data points has bounded restricted condition number, which we define below:\nDefinition 5 (Restricted Condition Number). Let k ∈ N be a sparsity parameter. The restricted condition number for sparsity k of a matrix X ∈ Rn×d is defined as\nsup v,w: ‖v‖=‖w‖=1,\n‖v‖0,‖w‖0≤k\n‖Xv‖ ‖Xw‖ .\nIt is easy to see that if a matrixX satisfies RIP with parameters (ǫ, k), then its restricted condition number for sparsity k is at most 1+ǫ1−ǫ . Thus, having bounded restricted condition number is a weaker requirement than RIP.\nWe now define the Block Bounded Restricted Condition Number Property (BBRCNP):\nDefinition 6 (Block Bounded Restricted Condition Number Property). Let κ > 0 and k ∈ N. A sequence of feature vectors x1, x2, . . . , xT satisfies BBRCNP with parameters (κ,K) if there is a constant t0 such that for any sequence of consecutive time steps T with |T | ≥ t0, the restricted condition number for sparsity k of X, the design matrix of the feature vectors xt for t ∈ T , is at most κ.\nNote that in the random design setting where xt, for t ∈ [T ], are isotropic sub-Gaussian vectors, t0 = O(log T +k log d) suffices to satisfy BBRCNP with high probability, where the O(·) notation hides a constant depending on κ.\nWe assume in this section that the sequence of feature vectors satisfies BBRCNP with parameters (κ,K) for some K = O(k log(T )) to be defined in the course of the analysis."
    }, {
      "heading" : "3.1 Algorithm",
      "text" : "The algorithm in the agnostic setting is of distinct nature from that in the stochastic setting. Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine. Natarajan [1995], Boutsidis et al. [2015] cast the sparse linear regression as maximization of weakly supermodular function. We will introduce an algorithm that blends various ideas from referred literature, to attack the online sparse regression with limited features.\nFirst, let’s introduce the notion of a weakly supermodular function.\nDefinition 7. For parameters k ∈ N and α ≥ 1, a set function g : [d] → R is (k, α)-weakly supermodular if for any two sets S ⊆ T ⊆ [d] with |T | ≤ k, the following two inequalities hold:\n1. (monotonicity) g(T ) ≤ g(S), and 2. (approximately decreasing marginal gain)\ng(S)− g(T ) ≤ α ∑\ni∈T\\S\n[g(S)− g(S ∪ {i})].\nThe definition is slightly stronger than that in Boutsidis et al. [2015]. We will show that sparse linear regression can be viewed as weakly supermodular minimization in Definition 7 once the design matrix has bounded restricted condition number.\nNow we outline the algorithm (see Algorithm 2). We divide the rounds 1, 2, . . . , T into mini-batches of size B each (so there are T/B such batches). The b-th batch thus consists of the examples (xt, yt) for t ∈ Tb := {(b− 1)B+1, (b− 1)B+1, . . . , bB}. Within the b-th batch, our algorithm queries the same subset of features of size at most k0.\nThe algorithm consists of few key steps. First, one can show that under BBRCNP, as long as B is large enough, the loss within batch b defines a weakly supermodular set function\ngt(S) = 1\nB inf w∈RS\n∑\nt∈Tb\n(yt − 〈xt, w〉)2.\nTherefore, we can formulate the original online sparse regression problem into online weakly supermodular minimization problem. For the latter problem, we develop an online greedy algorithm along the lines of [Streeter and Golovin, 2008]. We employ k1 = O∗(k) budgeted experts algorithms [Amin et al., 2015], denoted BEXP, with budget parameter3 k0k1 . The precise characteristics of BEXP are given in Theorem 8 (adapted from Theorem 2 in [Amin et al., 2015]).\nTheorem 8. For the problem of prediction from expert advice, let there be d experts, and let k ∈ [d] be a budget parameter. In each prediction round t, the BEXP algorithm chooses an expert jt and a set of experts Ut containing jt of size at most k, obtains as feedback the losses of all the experts in Ut, suffers the loss of\nexpert jt, and guarantees an expected regret bound of 2 √ d log(d) k T over T prediction rounds.\nAt the beginning of each mini-batch b, the BEXP algorithms are run. Each BEXP algorithm outputs a set of coordinates of size k0k1 as well as a special coordinate in that set. The union of all of these sets is then used as the set of features to query throughout the subsequent mini-batch. Within the mini-batch, the algorithm runs the standard Vovk-Azoury-Warmuth algorithm for linear prediction with square loss restricted to set of special coordinates output by all the BEXP algorithms.\nAt the end of the mini-batch, every BEXP algorithm is provided carefully constructed losses for each coordinate that was output as feedback. These losses ensure that the set of special coordinates chosen by the BEXP algorithms mimic the greedy algorithm for weakly supermodular minimization."
    }, {
      "heading" : "3.2 Main Result",
      "text" : "In this section, we will show that Algorithm 2 achieves sublinear regret under BBRCNP.\nTheorem 9. Suppose the sequence of feature vectors satisfies BBRCNP with parameters (κ, k1 + k) for k1 = 1 3κ\n2k log(T ), and assume that T is large enough so that t0 ≤ ( k0Tκ2dk )1/3. Then if Algorithm 2 is run with parameters B = ( k0Tκ2dk ) 1/3 and k1 as specified above, its expected regret is at most Õ(( κ8dk4 k0 )1/3T 2/3).\nProof. The proof relies on a number of lemmas whose proofs can be found in the appendix. We begin with the connection between sparse linear regression, weakly supermodular function and RIP, formally stated in Lemma 10. This lemma is a direct consequence of Lemma 5 in [Boutsidis et al., 2015].\nLemma 10. Consider a sequence of examples (xt, yt) ∈ Rd × R for t = 1, 2, . . . , B, and let X be the design matrix for the sequence. Consider the set function associated with least squares optimization:\ng(S) = inf w∈RS\n1\nB\nB∑\nt=1\n(yt − 〈xt, w〉)2.\nSuppose the restricted condition number of X for sparsity k is bounded by κ. Then g(S) is (k, κ2)-weakly supermodular.\nEven though minimization of weakly supermodular functions is NP-hard, the greedy algorithm provides a good approximation, as shown in the next lemma.\n3We assume, for convenience, that k0 is divisible by k1.\nAlgorithm 2 Online Greedy Algorithm for POSLR\nRequire: Mini-batch size B, sparsity parameters k0 and k1 1: Set up k1 budgeted prediction algorithms BEXP\n(i) for i ∈ [k1], each using the coordinates in [d] as “experts” with a per-round budget of k0k1 .\n2: for b = 1, 2, . . . , T/B do 3: For each i ∈ [k1], obtain a coordinate j(i)b and subset of coordinates U (i) b from BEXP (i) such that\nj (i) b ∈ U (i) b .\n4: Define V (0) b = ∅ and for each i ∈ [k1] define V (i) b = {j (i′) b | i′ ≤ i}. 5: Set up the Vovk-Azoury-Warmuth (VAW) algorithm for predicting using the features in V (k1) b . 6: for t ∈ Tb do 7: Set St = ⋃ i∈[k1] U (i) b , obtain xt(St), and pass xt(V (k1) b ) to VAW. 8: Set wt to be the weight vector output by VAW. 9: Obtain the true label yt and pass it to VAW.\n10: end for 11: Define the function\ngb(S) = 1\nB inf w∈RS\n∑\nt∈Tb\n(yt − 〈xt, w〉)2. (6)\n12: For each j ∈ U (i)b , compute gb(V (i−1) b ∪ {j}) and pass it BEXP(i) as the loss for expert j. 13: end for\nLemma 11. Consider a (k, α)-weakly supermodular set function g(·). Let j∗ := argminj g({j}). Then, for any subset V of size at most k, we have\ng({j∗})− g(V ) ≤ ( 1− 1α|V | ) [g(∅)− g(V )].\nThe BEXP algorithms essentially implement the greedy algorithm in an online fashion. Using the properties of the BEXP algorithm, we have the following regret guarantee:\nLemma 12. Suppose the sequence of feature vectors satisfies BBRCNP with parameters (ǫ, k1 + k). Then for any set V of coordinates of size at most k, we have\nE\n  T/B∑\nb=1\ngb(V (k1) b )− gb(V )\n \n≤ T/B∑\nb=1\n( 1− 1κ2|V | )k1 [gb(∅)− gb(V )] + 2κ2k √ dk1 log(d)T k0B .\nFinally, within every mini-batch, the VAW algorithm guarantees the following regret bound, an immediate consequence of Theorem 11.8 in Cesa-Bianchi and Lugosi [2006]:\nLemma 13. Within every batch b, the VAW algorithm generates weight vectors wt for t ∈ Tb such that ∑\nt∈Tb\n(yt − 〈xt, wt〉)2 −Bgb(V (k1)b ) ≤ O(k1 log(B)).\nWe can now prove Theorem 9. Combining the bounds of lemma 12 and 13, we conclude that for any\nsubset of coordinates V of size at most k, we have\nE\n[ T∑\nt=1\n(yt − 〈xt, wt〉)2 ]\n(7)\n≤ T/B∑\nb=1\nBgb(V ) +B(1− 1κ2|V | )k1 [gb(∅)− gb(V )] (8)\n+O ( κ2k √ dk1 log(d)BT\nk0 +\nT B k1 log(B)\n) . (9)\nFinally, note that T/B∑\nb=1\nBgb(V ) ≤ inf w∈RV\nT∑\nt=1\n(yt − 〈xt, w〉)2,\nand T/B∑\nb=1\nB(1− 1κ2|V |)k1 [gb(∅)− gb(V )] ≤ T · exp(− k1κ2k ),\nbecause gb(∅) ≤ 1. Using these bounds in (9), and plugging in the specified values of B and k1, we get the stated regret bound."
    }, {
      "heading" : "4 Conclusions and Future Work",
      "text" : "In this paper, we gave computationally efficient algorithms for the online sparse linear regression problem under the assumption that the design matrices of the feature vectors satisfy RIP-type properties. Since the problem is hard without any assumptions, our work is the first one to show that assumptions that are similar to the ones used to sparse recovery in the batch setting yield tractability in the online setting as well.\nSeveral open questions remain in this line of work and will be the basis for future work. Is it possible to improve the regret bound in the agnostic setting? Can we give matching lower bounds on the regret in various settings? Is it possible to relax the RIP assumption on the design matrices and still have efficient algorithms? Some obvious weakenings of the RIP assumption we have made don’t yield tractability. For example, simply assuming that the final matrix XT satisfies RIP rather than every intermediate matrix Xt for large enough t is not sufficient; a simple tweak to the lower bound construction of Foster et al. [2016] shows this. This tweak consists of simply padding the construction with enough dummy examples which are well-conditioned enough to overcome the ill-conditioning of the original construction so that RIP is satisfied by XT . We note however that in the realizable setting, our analysis can be easily adapted to work under weaker conditions such as irrepresentability [Zhao and Yu, 2006, Javanmard and Montanari, 2013]."
    }, {
      "heading" : "A Proofs for Realizable Setting",
      "text" : "Proof of Lemma 3. Let ∆ := ŵ − w∗ be the difference between the true answer and solution to the optimization problem. Let S to be the support of w∗ and let Sc = [d] \\ S be the complements of S. Consider the permutation i1, . . . , id−k of S\nc for which |∆(ij)| ≥ |∆(ij+1)| for all j. That is, the permutation dictated by the magnitude of the entries of ∆ outside of S. We split Sc into subsets of size k according to this permutation: Define Sj , for j ≥ 1 as {i(j−1)k+1, . . . , ijk}. For convenience we also denote by S01 the set S ∪ S1.\nNow, consider the matrix XS01 ∈ Rt×|S01| whose columns are those of X with indices S01. The Restricted Isometry Property of X dictates that for any vector c ∈ RS01 ,\n(1− ǫ) ‖c‖2 ≤ 1√ n ‖XS01c‖2 ≤ (1 + ǫ) ‖c‖2 .\nLet V ⊆ Rt be the subspace of dimension |S01| that is the image of the linear operator XS01 , and let PV ∈ Rt×t be the projection matrix onto that subspace. We have, for any vector z ∈ Rt that\n(1− ǫ) ‖PV z‖ ≤ 1√ n ∥∥XTS01z ∥∥ ≤ (1 + ǫ) ‖PV z‖\nWe apply this to z = X∆ and conclude that\n‖PV X∆‖ ≤ 1√ t(1− ǫ) ∥∥XTS01X∆ ∥∥ (10)\nWe continue to lower bound the quantity of ‖PV X∆‖. We decompose PV X∆ as\nPV X∆ = PV X∆(S01) + ∑\nj≥2\nPV X∆(Sj) (11)\nNow, according to the definition of V we that there exist vectors {cj}j≥2 in R|S01| for which\nPV X∆(Sj) = XS01cj\nWe now invoke Lemma 1.1 from Candes and Tao [2005] stating that for any S′, S′′ with |S′|+ |S′′| ≤ 3k it holds that\n∀c, c′ 1 n 〈XS′c,XS′′c′〉 ≤ (2ǫ− ǫ2) ‖c‖2 ‖c′‖2\nWe apply this for S01, Sj, j ≥ 2 and conclude that\n‖PV X∆(Sj)‖22 = 〈PV X∆(Sj), X∆(Sj)〉 ≤ 2ǫt ‖cj‖2 · ‖∆(Sj)‖ ≤ 2ǫ √ t\n1− ǫ ‖PV X∆(Sj)‖2 · ‖∆(Sj)‖2 .\nDividing through by ‖PV X∆(Sj)‖2, we get\n‖PV X∆(Sj)‖ ≤ 2ǫ √ t\n1− ǫ ‖∆(Sj)‖ . (12)\nLet us now bound the sum ‖∆(Sj)‖. By the definition of Sj we know that any element i ∈ Sj has the property ∆(i) ≤ (1/k) ‖∆(Sj−1)‖1. Hence\n∑\nj≥2\n‖∆(Sj)‖ ≤ (1/ √ k) ∑\nj≥1\n‖∆(Sj)‖1 = (1/ √ k) ‖∆(Sc)‖1\nWe now combine this inequality with Equations (10), (11) and (12)\n1\nt\n∥∥XTS01X∆ ∥∥ ≥ 1− ǫ√\nt ‖PV X∆‖\n≥ 1− ǫ√ t ‖PV X∆(S01)‖ − 1− ǫ√ n\n∑\nj≥2\n‖PV X∆(Sj)‖\n≥ 1− ǫ√ t ‖X∆(S01)‖ − 2ǫ ∑\nj≥2\n‖∆(Sj)‖\n≥ 1− ǫ√ t ‖X∆(S01)‖ − 2ǫ√ k ‖∆(Sc)‖1\nThe third inequality holds since X∆(S01) ∈ V hence PV X∆(S01) = X∆(S01). We continue to bound the expression by claiming that ‖∆(S)‖1 ≥ ‖∆(Sc)‖1. This holds since in Sc, ŵSc = ∆(Sc) hence\n‖w∗‖1 = ‖ŵ −∆(Sc)−∆(S)‖1 ≤ ‖ŵ‖1 + (‖∆(S)‖1 − ‖∆(Sc)‖1) Now, the optimality of ŵ implies ‖ŵ‖1 ≤ ‖w∗‖1, hence indeed ‖∆(S)‖1 ≥ ‖∆(Sc)‖1.\n‖∆(Sc)‖1 ≤ ‖∆(S)‖1 ≤ √ k ‖∆(S)‖2 ≤ ‖∆(S01)‖2 ≤\n√ k\n(1− ǫ) √ t ‖X∆(S01)‖\nWe continue the chain of inequalities\n1\nt\n∥∥XTS01X∆ ∥∥ ≥ 1− ǫ√\nn ‖X∆(S01)‖ − 2ǫ√ k ‖∆(Sc)‖1\n≥ ‖X∆(S01)‖ ( 1− ǫ√\nn − 2ǫ√ k ·\n√ k\n(1 − ǫ)√n\n)\n= (1− ǫ)2 − 2ǫ (1− ǫ) √ t ‖X∆(S01)‖\nRearranging we conclude that\n‖∆(S01)‖ ≤ 1\n(1− ǫ) √ t ‖X∆(S01)‖ (RIP of X)\n≤ 1 ((1 − ǫ)2 − 2ǫ)t ∥∥XTS01X∆ ∥∥ ≤ √ 2k\n(1− 4ǫ)t ∥∥XTX∆ ∥∥ ∞\n(since for any z ∈ R2k, ‖z‖2 ≤ √ 2k ‖z‖∞)\n≤ C √ dk log(d/δ)\ntk0\n( σ + d\nk0 ‖w∗‖1\n) (Lemma 14 and ǫ < 1/5)\nfor some constant C. We continue our bound on ‖∆‖ by showing that ‖∆(Sc01)‖ ≤ ‖∆(S01)‖\n‖∆(Sc01)‖22 (i) ≤ ‖∆(Sc)‖21 · ∑\nj≥k+1\n1 j2 ≤ 1 k ‖∆(Sc)‖21 ≤ 1 k ‖∆(S)‖21 ≤ ‖∆(S)‖ 2 2 .\nInequality (i) holds due to the following: Let αi be the absolute value of the i’th largest (in absolute value) element of ∆(Sc). It obviously holds that αi ≤ ‖∆(Sc)‖1 /i. Now, according to the definition of S01 we have that ‖∆(Sc01)‖22 = ∑ j≥k+1 α 2 i and the inequality follows. Hence,\n‖∆(Sc01)‖2 ≤ ‖∆(S)‖2 ≤ ‖∆(S01)‖2 .\nWe conclude that\n‖∆‖2 ≤ √ 2 ‖∆(S01)‖2 ≤ C\n√ dk log(d/δ)\ntk0\n( σ + d\nk0 ‖w∗‖1\n)\nfor some universal constant C > 0. Since ‖∆(S)‖1 ≥ ‖∆(Sc)‖1 and |S| ≤ k we get that\n‖∆‖1 ≤ 2 ‖∆(S)‖1 ≤ 2 √ k ‖∆(S)‖2 ≤ 2 √ k ‖∆‖2\nand the claim follows.\nProof of Lemma 4. Let S be the support of w∗. We can decompose the square of the left hand side as\n∥∥∥ŵ(S̃)− w∗ ∥∥∥ 2\n2 =\n∑\ni∈S∩S̃\n(ŵ(i)− w∗(i))2 + ∑\ni∈S̃\\S\n(ŵ(i))2 + ∑\ni∈S\\S̃\n(w∗(i))2.\nWe upper bound the last sum on the right hand side as\n∑\ni∈S\\S̃\n(w∗(i))2 = ∑\ni∈S\\S̃\n[(ŵ(i)− w∗(i)) + (ŵ(i))]2\n≤ 2 ∑\ni∈S\\S̃\n(ŵ(i)− w∗(i))2 + (ŵ(i))2\n≤ 2 ∑\ni∈S\\S̃\n(ŵ(i)− w∗(i))2 + 2 ∑\ni∈S̃\\S\n(ŵ(i))2 ,\nwhere first inequality follows from the elementary inequality (a+ b)2 ≤ 2a2 + 2b2 and the second inequality is due to the fact that S̃ contains top k entries of ŵ in absolute value and |S \\ S̃| = |S̃ \\ S|. Hence,\n∥∥∥ŵ(S̃)− w∗ ∥∥∥ 2\n2 =\n∑\ni∈S∩S̃\n(ŵ(i)− w∗(i))2 + ∑\ni∈S̃\\S\n(ŵ(i))2 + ∑\ni∈S\\S̃\n(w∗(i))2\n≤ ∑\ni∈S∩S̃\n(ŵ(i)− w∗(i))2 + 2 ∑\ni∈S\\S̃\n(ŵ(i)− w∗(i))2 + 3 ∑\ni∈S̃\\S\n(ŵ(i))2\n≤ 2 ∑\ni∈S∩S̃\n(ŵ(i)− w∗(i))2 + 2 ∑\ni∈S\\S̃\n(ŵ(i)− w∗(i))2 + 3 ∑\ni∈S̃\\S\n(ŵ(i))2\n= 2 ∑\ni∈S\n(ŵ(i)− w∗(i))2 + 3 ∑\ni∈S̃\\S\n(ŵ(i))2\n≤ 3 d∑\ni=1\n(ŵ(i)− w∗(i))2\n= 3 ‖ŵ − w∗‖22 .\nTaking square root finishes the proof.\nLemma 14. There exists a universal constant C > 0 such that, with probability at least 1 − δ, the convex program (3) is feasible and its optimal solution ŵ satisfies\n∥∥∥∥ 1\nt XTt Xt(ŵ − w∗) ∥∥∥∥ ∞ ≤ C √ d log(d/δ) tk0 ( σ + d k0 ‖w∗‖1 ) .\nWe note that the above lemma is beyond simple triangle inequality on the feasibility constraints, as the left hand side depends on actual design matrix Xt which we do not observe, instead of X̂t.\nProof. To simplify notation, we drop subscript t. Namely, let X = Xt, X̂ = Xt and D̂ = D̂t, and also let η = (η1, η2, . . . , ηt) be the vector of noise variables.\nFirst, we show that w∗ satisfies the constraint of (3) with probability at least 1− δ. We upper bound ∥∥∥∥ 1 t X̂T (Y − X̂w∗) + 1 t D̂w∗ ∥∥∥∥ ∞ = ∥∥∥∥ [ 1 t X̂T (X − X̂) + 1 t D̂ ] w∗ + 1 t X̂Tη ∥∥∥∥ ∞\n≤ ∥∥∥∥ [ 1 t X̂T (X − X̂) + 1 t D̂ ] w∗ ∥∥∥∥ ∞ + 1 t ∥∥∥X̂T η ∥∥∥ ∞\nWe first bound the left summand. By Lemma 15, we have\n∥∥∥∥ [ 1 t X̂T (X − X̂) + 1 t D̂ ] w∗ ∥∥∥∥ ∞ ≤ ‖w∗‖1 · ∥∥∥∥ 1 t X̂T (X − X̂) + 1 t D̂ ∥∥∥∥ ∞\n≤ ‖w∗‖1 (∥∥∥∥ 1\nt XT (X̂ −X) ∥∥∥∥ ∞ + ∥∥∥∥ 1 t (X̂ −X)T (X̂ −X)− 1 t D̂ ∥∥∥∥ ∞ )\n≤ ‖w∗‖1 C · √ d3 log(d/δ)\ntk0 3 .\nFor the right summand, since η is vector of i.i.d Gaussians with variance σ2, with probability at least 1− δ,\n1\nt ∥∥∥X̂Tη ∥∥∥ ∞ ≤ Cσ t √ log(d/δ) ·max i∈[d] ∥∥∥X̂(i) ∥∥∥ 2\nwhere X̂(1), X̂(2), . . . , X̂(d) are the columns of X̂. Since the absolute value of the entries of X̂ is at most d/k0,\nwe have ∥∥∥X̂(i) ∥∥∥ 2 ≤ √ td/k0 and thus\n1\nt ∥∥∥X̂Tη ∥∥∥ ∞\n≤ Cσ √ d log(d/δ)\ntk0 .\nCombining the inequalities so far provides\n∥∥∥∥ 1 t X̂T (Y − X̂w∗) + 1 t D̂w∗ ∥∥∥∥ ∞ ≤ C √ d log(d/δ) tk0 ( σ + d k0 ‖w∗‖1 )\nand hence conclude the constraint of the optimization problem (3) is satisfied (at least) by w∗ and thus the optimization problem is feasible.\nNow consider the vector ∆ := ŵ − w∗, we have ∥∥∥∥ 1\nt XTX∆ ∥∥∥∥ ∞ ≤ ∥∥∥∥ 1 t (X̂T X̂ − D̂)∆ ∥∥∥∥ ∞ + ∥∥∥∥ 1 t (X̂T X̂ − D̂ −XTX)∆ ∥∥∥∥ ∞\n≤ ∥∥∥∥ 1\nt (X̂T X̂ − D̂)∆ ∥∥∥∥ ∞ + ∥∥∥∥ 1 t (X̂ −X)TX∆ ∥∥∥∥ ∞\n+ ∥∥∥∥ 1\nt XT (X̂ −X)∆ ∥∥∥∥ ∞ + ∥∥∥∥ ( 1 t (X̂ −X)T (X̂ −X)− 1 t D̂ ) ∆ ∥∥∥∥ ∞ .\nAccording to Lemma 15 we have\n∥∥∥∥ 1\nt XT (X̂ −X)∆ ∥∥∥∥ ∞ ≤ ∥∥∥∥ 1 t XT (X̂ −X) ∥∥∥∥ ∞ ‖∆‖1 ≤ C √ d log(d/δ) tk0 (‖w∗‖1 + ‖ŵ‖1) ≤ 2C √ d log(d/δ) tk0 · ‖w∗‖1\nwhere the last inequality is by the optimality of ŵ. The same argument provides an identical bound for∥∥∥ 1t (X̂ −X)TX∆ ∥∥∥ ∞ . The last summand can also be bounded by using Lemma 15 and the optimality of ŵ.\n∥∥∥∥ ( 1 t (X̂ −X)T (X̂ −X)− 1 t D̂ ) ∆ ∥∥∥∥ ∞ ≤ 2C √ d3 log(d/δ) tk0 3 · ‖w∗‖1\nFinally, according to the feasibility of ŵ and w∗ we may bound the first summand\n∥∥∥∥ ( 1 t X̂T X̂ − 1 t D̂ ) ∆ ∥∥∥∥ ∞ ≤ 2C √ d log(d/δ) tk0 ( σ + d k0 ‖w∗‖1 ) ,\nand reach the final bound.\nLemma 15. For any t ≥ t0, with probability at least 1− δ, the following two inequalities hold ∥∥∥∥ 1\nt (X̂t −Xt)T (X̂t −Xt)−\n1 t D̂t ∥∥∥∥ ∞ ≤ C √ d3 log(d/δ) tk0 3 ,\n∥∥∥∥ 1\nt XTt (X̂t −Xt) ∥∥∥∥ ∞ ≤ C √ d log(d/δ) tk0 ,\nwhere ‖·‖∞ denotes the maximum of the absolute values of the entries of a matrix.\nProof. Throughout we use that |xs(i)| ≤ 1 for all i ∈ [d] and all s ∈ [t], and (2) (x̂s(i) − xs(i))2 − 1tDii is unbiased with absolute value of at most (d/k0) 2 and variance of at most (d/k0) 3. For the first term, let’s bound [ 1\nt (X̂ −X)T (X̂ −X)− 1 t D̂\n]\nij\n= 1\nt\nt∑\ns=1\n(x̂s(i)− xs(i))(x̂s(j)− xs(j))− 1\nt D̂ij\nFor i = j, we have\nE [( (x̂s(i)− xs(i))2 − 1\nt Dii\n)2] ≤ E [ (x̂s(i)− xs(i))4 ] ≤ (d/k0)3\n(x̂s(i)− xs(i))2 − 1\nt Dii ≤ (d/k0)2, E\n[ (x̂s(i)− xs(i))2 − 1\nt Dii\n] = 0\nHence, by Bernstein’s inequality, for any v > 0,\nPr [∣∣∣∣∣ 1 t t∑\ns=1\n(x̂s(i)− xs(i))2 − 1\nt Dii ∣∣∣∣∣ > v ] ≤ 2 exp ( − v 2t (d/k0)3 + (d/k0)2v/3 ) .\nIt follows that for any δ > 0, with probability at least 1− δ it holds for all i ∈ [d] that, ∣∣∣∣∣ 1 t t∑\ns=1\n(x̂s(i)− xs(i))2 − 1\nt Dii ∣∣∣∣∣ ≤ O ( log(d/δ)d2 tk0 2 + √ log(d/δ)d3 tk0 3 ) .\nSimilarly we have 1t (D̂ii −Dii) ≤ O ( log(d/δ)d2 tk02 + √ log(d/δ)d3 tk03 ) .\nFor i 6= j we use an analogous argument, only now the variance term in Bernstein’s inequality is (d/k0)2 rather than (d/k0) 3, hence only reach a tighter bound.\nFor the second term, we again bound via Bernstein’s inequality as\n[ 1\nt XT (X̂ −X)\n]\nij\n= 1\nt\nt∑\ns=1\nxs(i)(x̂s(j)− xs(j)) ≤ O\n  √ d log(d/δ)\ntk0 +\nd log(d/δ)\ntk0\n \nThe claim now follows by noticing that for large enough t, the dominating terms are those that scale as 1/ √ t.\nProof of Theorem 2. By Lemma 3,\n‖wt+1 − w∗‖2 ≤ O\n  √ d\nk0\nk log(d/δ)\nt (σ +\nd\nk0 ‖w∗‖1)\n  .\nWe have\nRegretT (w ∗)− Regrett0(w∗) =\nT∑\nt=t0+1\n(yt − 〈xt, wt〉)2 − (yt − 〈xt, w∗〉)2\n= T∑\nt=t0+1\n(〈xt, w∗ − wt〉+ ηt)2 − η2t\n=\nT∑\nt=t0+1\n(〈xt, w∗ − wt〉+ 2ηt) 〈xt, w∗ − wt〉\n=\nT∑\nt=t0+1\n2ηt 〈xt, w∗ − wt〉+ (〈xt, w∗ − wt〉)2 ,\nwhere we used that yt = 〈xt, wt〉 + ηt. To bound the regret we require the upper bound, that occurs with probability of at least 1− δ,\n∀t ≥ t0 |〈xt, w∗ − wt〉| (i) ≤ ‖xt‖∞ √ ‖wt − w∗‖0 · ‖wt − w∗‖2 (ii) ≤ O  k · √ d\nk0\nlog(log(T )d/δ)\nt\n( σ + d\nk0\n)  .\nInequality (i) holds since 〈a, b〉 ≤ ‖a(S)‖2 · ‖b‖2 with S being the support of b and ‖a(S)‖2 ≤ ‖a‖∞ √ |S|. Inequality (ii) follows from Lemma 3 and Lemma 4, and a union bound over the ⌈log(T )⌉ many times the vector wt is updated. Now, for the left summand of the regret bound we have by Martingale concentration inequality that w.p. 1− δ\nT∑\nt=t0+1\n2ηt 〈xt, wt − w∗〉 ≤ O  σ √√√√log(1/δ) T∑\nt=t0+1\n〈xt, wt − w∗〉2  \n= O  σ √\nlog(1/δ) log(T )k2 · d log(d log(T )/δ) k0\n( σ + d\nk0\n)2   .\nThe right summand is bounded as\nT∑\nt=t0+1\n〈xt, w∗ − wt〉2 = O ( k2 · d log(d log(T )/δ)\nk0\n( σ + d\nk0\n)2 · log(T ) ) .\nClearly, the right summand dominates the left one.\nIt remains to bound the regret in first t0 rounds. Since wt = 0 for t ≤ t0, we have\nRegrett0(w ∗) =\nt0∑\nt=1\n2ηt 〈xt, w∗〉+ (〈xt, w∗〉)2 ≤ O ( σ √ t0 log(1/δ) + t0 ) .\nHere, we used that | 〈xt, w∗〉 | ≤ 1 since ‖xt‖∞ ≤ 1 and ‖w∗‖1 ≤ 1. We also used that ηt 〈xt, w∗〉 ∼ N(0, σ2 〈xt, w∗〉2) and η1 〈x1, w∗〉 , η2 〈x2, w∗〉 , . . . , ηt0 〈xt0 , w∗〉 are independent. Thus their sum is a Gaussian with variance at most σ2t0.\nCollecting all the terms along with Lemma 16, bounding the difference RegretT −RegretT (w∗), gives\nRegretT ≤ ( t0 + √ t0 log(1/δ) + k\n2 · d log(d log(T )/δ) k0\n( σ + d\nk0\n)2 · log(T ) ) (13)\nLemma 16. In the realizable case, w.p. at least 1−δ we have for any sequence of wt that RegretT −RegretT (w∗) = O(σ2k log(d/δ)).\nProof. It is an easy exercise to show that RegretT −RegretT (w∗) is equal to the regret on an algorithm that always plays w∗. We thus continue to bound the regret of w∗.\nLet ∆ ∈ Rd be the difference between w∗ and w̃, the empirical optimal solution for the sparse regression problem. The loss associated with w∗ is clearly ‖η‖2, where η is the noise term y = Xw∗ + η. The loss associated with w̃ is\n‖X(w∗ +∆)−Xw∗ − η‖2 = ‖η −X∆‖2 = ‖η −XS̃∆‖2\nwhere S̃ is the support of ∆, having a cardinality of at most 2k. The closed form solution for the least-squares problem dictates that\n‖η −XS̃∆‖2 ≥ ‖η −XS̃X † S̃ η‖2 = ‖η‖2 − ‖XS̃X † S̃ η‖2 .\nHere, A† is the pseudo inverse of a matrix A and XS is the matrix obtained from the columns of X whose indices are in S. It follows that the regret of w∗ is bounded by\n‖XS̃X † S̃ η‖2\nfor some subset S̃ of size at most 2k. To bound this quantity we use a high probability bound for ‖XSX†Sη‖2 for a fixed set S, and take a union bound over all possible sets of cardinality 2k. For a fixed set S we have that ‖XSX†Sη‖2/σ2 is distributed according to the χ22k distribution. The tail bounds of this distribution suggest that\nPr [ ‖XSX†Sη‖2 > 2kσ2 + 2σ2 √ 2kx+ 2σ2x ] ≤ exp(−x)\nmeaning that with probability at least 1− δ/d2k we have\n‖XSX†Sη‖2 < 2kσ2 + 2σ2 √ 2k · 2k · log(d/δ) + 2σ2 · 2k · log(d/δ) = O(σ2k log(d/δ))\nTaking a union bound over all possible subsets of size ≤ 2k we get that w.p. at least 1− δ the regret of w∗ is at most O(σ2k log(d/δ))."
    }, {
      "heading" : "B Proofs for Agnostic Setting",
      "text" : "We begin with an auxiliary lemma for Lemma 10, informally proving that for any matrix X̄ with BBRCNP (Definition 6) and vector y, the set function\ng(S) = inf w∈RS\n‖y − X̄w‖2\nis weakly supermodular. Its proof can be found in [Boutsidis et al., 2015], yet for completeness we provide it here as well.\nLemma 17. [Lemma 5 in [Boutsidis et al., 2015]] Let X̄ be a matrix whose columns have 2-norm at most 1 and y be a vector with ‖y‖∞ ≤ 1 of dimension matching the number of rows in X. the set function\ng(S) = inf w∈RS\n‖y −Xw‖2\nis α-weakly supermodular for sparsity k for α = maxS:|S|≤k 1/σmin(XS) 2, where XS is the submatrix of X obtained by choosing the columns indexed by S, and σmin(A) is the smallest singular value of A.\nProof. Firstly, the well known closed form solution for the least-squares problem informs us that\ng(S) = inf w∈RS\n‖y −Xw‖2,\n= yT [I − (XTS )†XTS ]y.\nWe use the notation A† for the pseudoinverse of a matrix A. That is, if the singular value decomposition of A is A = ∑ i σiuiv T i with σi > 0 then A † = ∑ i σ −1 i viu T i .\nLet us first estimate g(S) − g(T ), for sets S ⊂ T . For brevity, define HS as the projection matrix XSX † S projecting onto the column space of XS . Denote by ZT\\S the matrix whose columns are those of XT\\S projected away from the span of XS , and normalized. Namely, writing xi as the i’th column of X , ζi = ‖(I −HS)xi‖, zi = (I −HS)xi/ζi, and ZT\\S’s columns are {zi}i∈T\\S . Notice that the columns of ZT\\S and XS are orthogonal, hence according to the Pythagorean theorem it holds that\ng(S) = ‖y‖2 − ‖HSy‖2, g(T ) = ‖y‖2 − ‖HSy‖2 − ‖ZT\\SZ†T\\Sy‖2\nmeaning that g(S) − g(T ) = ‖ZT\\SZ†T\\Sy‖2. In particular, this implies that for any j /∈ S it holds that g(S)− g(S ∪ {j}) = (zTj y)2, since zj is a unit vector. Let us now decompose g(S)− g(T ).\ng(S)− g(T ) = ‖ZT\\SZ†T\\Sy‖2 = ‖(ZTT\\S)†ZTT\\Sy‖2 ≤ ‖(ZTT\\S)†‖2 · ‖ZTT\\Sy‖2\nThe norm used in the last inequality is the matrix operator norm. We now bound both factors of the product on the RHS separately. For the first factor, we claim that ‖(ZTT\\S)†‖ = ‖Z † T\\S‖ ≤ ‖X † T‖. To see this, consider a vector w ∈ R|T\\S|, for convenience denote its entries by {w(i)}i∈T\\S , and write zi = (xi − ∑\nj∈S αijxj)/ζi. We have\nZT\\Sw = ∑\ni∈T\\S\nziw(i) = ∑\ni∈T\\S\nxiw(i)/ζi − ∑\nj∈S\nxj ∑\ni∈T\\S\nw(i)αij/ζi = XTw ′\nfor the vector w′ ∈ R|T | defined as w′(i) = w(i)/ζi for i ∈ T \\ S and w′(j) = − ∑ i∈T\\S w(i)αij/ζi for j ∈ S. Since ζi ≤ ‖xi‖ ≤ 1 we must have ‖w′‖ ≥ ‖w‖. Consider now the unit vector w for which ‖ZT\\Sw‖ = ‖Z†T\\S‖−1, that is, the unit norm singular vector corresponding to the smallest non-zero singular value of ZT\\S . For this w, and its corresponding vector w ′, we have\n‖Z†T\\S‖−1 = ‖ZT\\Sw‖ = ‖XTw′‖ ≥ σmin(XT )‖w′‖ ≥ σmin(XT )‖w‖ = σmin(XT ).\nIt follows that ‖(ZTT\\S)†‖2 = ‖Z†T\\S‖2 ≤ 1/σmin(XT )2\nWe continue to bound the right factor of product.\n‖ZTT\\Sy‖2 = ∑\ni∈T\\S\n(zTi y) 2 =\n∑\ni∈T\\S\ng(S)− g(S ∪ {i}).\nBy combining the inequalities we obtained the required result:\ng(S)− g(T ) ≤ ( 1/σmin(XT ) 2 ) ∑\ni∈T\\S\ng(S)− g(S ∪ {i}).\nProof of Lemma 10. We would like to apply Lemma 17 on the design matrix X . The only catch is that the columns of X may not be bounded by 1 in norm. To remedy this, let j be the index of the column with the maximum norm and consider the matrix X̄ = 1‖Xj‖X instead (here, Xj is the j-th column of X ; note that Xj = Xej for the j-th standard basis vector ej). Now, for any subset S of coordinates,\ninf w∈RS ‖y − X̄w‖2 = inf w∈RS ‖y −Xw‖2.\nThus, we conclude that the set function of interest, g(S) = infw∈RS ‖y−Xw‖2, is α-weakly supermodular for sparsity k for α = maxS:|S|≤k ‖X̄†S‖22. For any subset of coordinates S of size at most k, let w be a unit norm right singular vector of X̄S corresponding to the smallest singular value, so that ‖X̄†S‖2 = 1‖X̄Sw‖ . But\n1 ‖X̄Sw‖ = ‖Xej‖ ‖Xw′‖ , where w ′ is the vector w extended to all coordinates by padding with zeros.\nSince the restricted condition number of X for sparsity k is bounded by κ we conclude that ‖Xej‖ ‖Xw′‖ ≤ κ.\nSince this bound holds for any subset S of size at most k, we conclude that α ≤ κ2.\nProof of Lemma 11. By the α-weak supermodularity of g, we have\ng(∅)− g(V ) ≤ α · ∑\nj∈V\n[g(∅)− g({j})]\n≤ α|V | · [(g(∅)− g(V ))− (g({j∗})− g(V ))].\nRearranging, we get the claimed bounds.\nThe following lemma gives a useful property of weakly supermodular functions.\nLemma 18. Let g(·) be a (k, α)-weakly supermodular set function and U be a subset with |U | < k. Then g′(S) := g(U ∪ S) is (k − |U |, α)-weakly supermodular. Proof. For any two subsets S ⊆ T with |T | ≤ k − |U |, we have\ng′(S)− g′(T ) = g(U ∪ S)− g(U ∪ T ) ≤ α ∑\nj∈(T∪U)\\(S∪U)\n[g(U ∪ S)− g(U ∪ S ∪ {j})]\n≤ α ∑\nj∈T\\S\n[g(U ∪ S)− g(U ∪ S ∪ {j})] = α ∑\nj∈T\\S\n[g′(S)− g′(S ∪ {j})].\nProof of Lemma 12. For i ∈ {0, 1, . . . , k1}, define the set function g(i)b as g (i) b (S) = gb(S ∪ V (i) b ).\nFirst, we analyze the performance of the BEXP algorithms. Fix any i ∈ [k1] and consider BEXP(i). Conceptually, for any j ∈ [d], the loss of expert j at the end of mini-batch b is gb(V (i−1)b ∪ j) (note that this loss is only evaluated for j ∈ U (i)b in the algorithm). To bound the regret, we need to bound the magnitude of the losses. Note that for any subset S, we have 0 ≤ gb(S) ≤ 1B ∑ t∈Tb\ny2t ≤ 1. Thus, the regret guarantee of BEXP (Theorem 8) implies that for any i ∈ [k1] and any j ∈ [d], we have\nE\n  T/B∑\nb=1\ngb(V (i−1) b ∪ {j (i) b })\n  ≤ T/B∑\nb=1\ngb(V (i−1) b ∪ {j}) + 2\n√ dk1 log(d)T\nk0B .\nThe expectation above is conditioned on the randomness in V (i−1) b , for b ∈ [T/B]. Rewriting the above inequality using the g(i−1) and g(i) functions, and using the fact that V (i−1) b ∪ {j (i) b } = V (i) b , we get\nE\n  T/B∑\nb=1\ng (i) b (∅)\n  ≤ T/B∑\nb=1\ng (i−1) b ({j}) + 2\n√ dk1 log(d)T\nk0B . (14)\nNext, since we assumed that the sequence of feature vectors satisfies BBRCNP with parameters (ǫ, k1+k), Lemma 10 implies that the set function gb defined in (6) is (k1 + k, κ 2)-weakly supermodular for κ = 1+ǫ1−ǫ . By Lemma 18, the set function g (i) b is (k, κ\n2)-weakly supermodular (since |V (i)b | ≤ k1). It is easy to check that the sum of weakly supermodular functions is also weakly supermodular (with\nthe same parameters), and hence ∑T/B\nb=1 g (i−1) b is also (k, κ 2)-weakly supermodular. Hence, by Lemma 11, if\nj∗ = argminj ∑T/B b=1 g (i−1) b ({j}), we have, for any subset V of size at most k,\nT/B∑\nb=1\ng (i−1) b ({j∗})− g (i−1) b (V ) ≤ (1− 1κ2|V | )[\nT/B∑\nb=1\ng (i−1) b (∅)− g (i−1) b (V )].\nSince gb(V ) ≥ gb(V ∪ V (i−1)b ) = g (i−1) b (V ), the above inequality implies that\nT/B∑\nb=1\ng (i−1) b ({j∗})− gb(V ) ≤ (1− 1κ2|V | )[\nT/B∑\nb=1\ng (i−1) b (∅)− gb(V )].\nCombining this bound with (14) for j = j∗, we get\nE\n  T/B∑\nb=1\ng (i) b (∅)− gb(V )   ≤ (1− 1κ2|V |)[ T/B∑\nb=1\ng (i−1) b (∅)− gb(V )] + 2\n√ dk1 log(d)T\nk0B .\nApplying this bound recursively for i ∈ [k1] and simplifying, we get\nE\n  T/B∑\nb=1\ng (k1) b (∅)− gb(V )   ≤ (1− 1κ2|V |)k1 [ T/B∑\nb=1\ng (0) b (∅)− gb(V )] + 2κ2|V |\n√ dk1 log(d)T\nk0B .\nUsing the definitions of g (k1) b and g (0) b , and the fact that |V | ≤ k, we get the claimed bound."
    } ],
    "references" : [ {
      "title" : "Budgeted prediction with expert advice",
      "author" : [ "Kareem Amin", "Satyen Kale", "Gerald Tesauro", "Deepak S. Turaga" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Amin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Amin et al\\.",
      "year" : 2015
    }, {
      "title" : "Linear and conic programming estimators in high dimensional errors-in-variables models",
      "author" : [ "Alexandre Belloni", "Mathieu Rosenbaum", "Alexandre B. Tsybakov" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Belloni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Belloni et al\\.",
      "year" : 2016
    }, {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "Peter J Bickel", "Ya’acov Ritov", "Alexandre B Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Bickel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel et al\\.",
      "year" : 2009
    }, {
      "title" : "Greedy minimization of weakly supermodular set functions",
      "author" : [ "Christos Boutsidis", "Edo Liberty", "Maxim Sviridenko" ],
      "venue" : "arXiv preprint arXiv:1502.06528,",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2015
    }, {
      "title" : "The Dantzig selector: statistical estimation when p is much larger than n",
      "author" : [ "Emmanuel Candes", "Terence Tao" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Candes and Tao.,? \\Q2007\\E",
      "shortCiteRegEx" : "Candes and Tao.",
      "year" : 2007
    }, {
      "title" : "Decoding by linear programming",
      "author" : [ "Emmanuel J Candes", "Terence Tao" ],
      "venue" : "IEEE transactions on information theory,",
      "citeRegEx" : "Candes and Tao.,? \\Q2005\\E",
      "shortCiteRegEx" : "Candes and Tao.",
      "year" : 2005
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Efficient learning with partially observed attributes",
      "author" : [ "Nicolò Cesa-Bianchi", "Shai Shalev-Shwartz", "Ohad Shamir" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Variable selection is hard",
      "author" : [ "Dean Foster", "Howard Karloff", "Justin Thaler" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Foster et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 2015
    }, {
      "title" : "Online sparse linear regression",
      "author" : [ "Dean Foster", "Satyen Kale", "Howard Karloff" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Foster et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 2016
    }, {
      "title" : "Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2014
    }, {
      "title" : "Linear regression with limited observation",
      "author" : [ "Elad Hazan", "Tomer Koren" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Hazan and Koren.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hazan and Koren.",
      "year" : 2012
    }, {
      "title" : "Model selection for high-dimensional regression under the generalized irrepresentability condition",
      "author" : [ "Adel Javanmard", "Andrea Montanari" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Javanmard and Montanari.,? \\Q2013\\E",
      "shortCiteRegEx" : "Javanmard and Montanari.",
      "year" : 2013
    }, {
      "title" : "Open problem: Efficient online sparse regression",
      "author" : [ "Satyen Kale" ],
      "venue" : "In COLT, pages 1299–1301,",
      "citeRegEx" : "Kale.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kale.",
      "year" : 2014
    }, {
      "title" : "Attribute efficient linear regression with distribution-dependent sampling",
      "author" : [ "Doron Kukliansky", "Ohad Shamir" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Kukliansky and Shamir.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kukliansky and Shamir.",
      "year" : 2015
    }, {
      "title" : "Learning without concentration",
      "author" : [ "Shahar Mendelson" ],
      "venue" : "In COLT, pages",
      "citeRegEx" : "Mendelson.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mendelson.",
      "year" : 2014
    }, {
      "title" : "Sparse approximate solutions to linear systems",
      "author" : [ "Balas Kausik Natarajan" ],
      "venue" : "SIAM journal on computing,",
      "citeRegEx" : "Natarajan.,? \\Q1995\\E",
      "shortCiteRegEx" : "Natarajan.",
      "year" : 1995
    }, {
      "title" : "Sparse recovery under matrix uncertainty",
      "author" : [ "Mathieu Rosenbaum", "Alexandre B. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Rosenbaum and Tsybakov.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rosenbaum and Tsybakov.",
      "year" : 2010
    }, {
      "title" : "An online algorithm for maximizing submodular functions",
      "author" : [ "Matthew J. Streeter", "Daniel Golovin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Streeter and Golovin.,? \\Q2008\\E",
      "shortCiteRegEx" : "Streeter and Golovin.",
      "year" : 2008
    }, {
      "title" : "On model selection consistency of lasso",
      "author" : [ "Peng Zhao", "Bin Yu" ],
      "venue" : "Journal of Machine learning research,",
      "citeRegEx" : "Zhao and Yu.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhao and Yu.",
      "year" : 2006
    }, {
      "title" : "Online learning with costly features and labels",
      "author" : [ "Navid Zolghadr", "Gábor Bartók", "Russell Greiner", "András György", "Csaba Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zolghadr et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zolghadr et al\\.",
      "year" : 2013
    }, {
      "title" : "X̄w‖ is weakly supermodular. Its proof can be found in [Boutsidis et al., 2015], yet for completeness we provide it here as well",
      "author" : [ "‖y" ],
      "venue" : null,
      "citeRegEx" : "−,? \\Q2015\\E",
      "shortCiteRegEx" : "−",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "One example of this situation, from [Cesa-Bianchi et al., 2011], is medical diagnosis of a disease, in which each feature is the result of a medical test on the patient.",
      "startOffset" : 36,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "In this paper, we answer this open question by providing efficient algorithms with sublinear regret under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) [Candes and Tao, 2005].",
      "startOffset" : 205,
      "endOffset" : 227
    }, {
      "referenceID" : 2,
      "context" : "RIP and related Restricted Eigenvalue Condition [Bickel et al., 2009] have been widely used as a standard assumption for theoretical analysis in the compressive sensing and sparse regression literature, in the offline case.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error ∑T t=1(yt−〈xt, w〉)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = 〈xt, w∗〉 for all t.",
      "startOffset" : 185,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error ∑T t=1(yt−〈xt, w〉)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = 〈xt, w∗〉 for all t. Furthermore, the computational hardness is present even when the solution is required to be only Õ(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details.",
      "startOffset" : 185,
      "endOffset" : 515
    }, {
      "referenceID" : 5,
      "context" : ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error ∑T t=1(yt−〈xt, w〉)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = 〈xt, w∗〉 for all t. Furthermore, the computational hardness is present even when the solution is required to be only Õ(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details. The hardness result was extended to online sparse regression by Foster et al. [2016]. They showed that for all δ > 0 there exists no polynomial-time algorithm with regret O(T ) unless NP ⊆ BPP .",
      "startOffset" : 185,
      "endOffset" : 613
    }, {
      "referenceID" : 5,
      "context" : ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error ∑T t=1(yt−〈xt, w〉)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = 〈xt, w∗〉 for all t. Furthermore, the computational hardness is present even when the solution is required to be only Õ(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details. The hardness result was extended to online sparse regression by Foster et al. [2016]. They showed that for all δ > 0 there exists no polynomial-time algorithm with regret O(T ) unless NP ⊆ BPP . Foster et al. [2016] posed the open question of what additional assumptions can be made on the data to make the problem tractable.",
      "startOffset" : 185,
      "endOffset" : 744
    }, {
      "referenceID" : 3,
      "context" : "In this paper, we answer this open question by providing efficient algorithms with sublinear regret under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) [Candes and Tao, 2005]. It has been shown that if RIP holds and there exists a sparse linear predictor w such that yt = 〈xt, w∗〉 + ηt where ηt is independent noise, the offline sparse linear regression problem admits computationally efficient algorithms, e.g., Candes and Tao [2007]. RIP and related Restricted Eigenvalue Condition [Bickel et al.",
      "startOffset" : 206,
      "endOffset" : 488
    }, {
      "referenceID" : 4,
      "context" : "The solution of this problem cannot be obtained by a simple application of say, the Dantzig selector [Candes and Tao, 2007] since we do not observe the data matrix X , but rather a subsample of its entries.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "This bound has optimal dependence on T , since even in the full information setting where all features are observed there is a lower bound of Ω(logT ) [Hazan and Kale, 2014].",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 3,
      "context" : "The analysis in [Boutsidis et al., 2015] shows that the RIP assumption implies that the set function defined as the minimum loss achievable by a linear regressor restricted to the set in question satisfies a property called weak supermodularity.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "The analysis in [Boutsidis et al., 2015] shows that the RIP assumption implies that the set function defined as the minimum loss achievable by a linear regressor restricted to the set in question satisfies a property called weak supermodularity. Weak supermodularity is a relaxation of standard supermodularity that is still strong enough to show performance bounds for the standard greedy feature selection algorithm for solving the sparse regression problem. We then employ a technique developed by Streeter and Golovin [2008] to construct an online learning algorithm that mimics the greedy feature selection algorithm.",
      "startOffset" : 17,
      "endOffset" : 529
    }, {
      "referenceID" : 3,
      "context" : "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al.",
      "startOffset" : 66,
      "endOffset" : 746
    }, {
      "referenceID" : 3,
      "context" : "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound.",
      "startOffset" : 66,
      "endOffset" : 768
    }, {
      "referenceID" : 3,
      "context" : "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound.",
      "startOffset" : 66,
      "endOffset" : 781
    }, {
      "referenceID" : 3,
      "context" : "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound. This question was answered in the negative by Foster et al. [2016], who showed that efficiency can only be obtained under additional assumptions on the data.",
      "startOffset" : 66,
      "endOffset" : 978
    }, {
      "referenceID" : 2,
      "context" : "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al.",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al.",
      "startOffset" : 112,
      "endOffset" : 197
    }, {
      "referenceID" : 1,
      "context" : "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al.",
      "startOffset" : 198,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al. [2016] (see also the references therein).",
      "startOffset" : 198,
      "endOffset" : 386
    }, {
      "referenceID" : 1,
      "context" : "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al. [2016] (see also the references therein). In fact, these papers solve a more general problem where we observe a matrix Z rather than X that is an unbiased estimator of X . While we can use their results in a black-box manner, they are tailored for the setting where the variance of each Zij is constant and it is difficult to obtain the exact dependence on this variance in their bounds. In our setting, this variance can be linear in the dimension of the feature vectors, and hence we wish to control the dependence on the variance in the bounds. Thus, we use an algorithm that is similar to the one in Belloni et al. [2016], and provide an analysis for it (in the appendix).",
      "startOffset" : 198,
      "endOffset" : 1005
    }, {
      "referenceID" : 4,
      "context" : "The following definition will play a key role: Definition 1 (Restricted Isometry Property Candes and Tao [2007]).",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "The following definition will play a key role: Definition 1 (Restricted Isometry Property Candes and Tao [2007]). Let ǫ ∈ (0, 1) and k ≥ 0. We say that a matrix X ∈ R satisfies restricted isometry property (RIP) with parameters (ǫ, k) if for any w ∈ R with ‖w‖0 ≤ k we have (1− ǫ) ‖w‖2 ≤ 1 √ n ‖Xw‖2 ≤ (1 + ǫ) ‖w‖2 . One can show that RIP holds with overwhelming probability if n = Ω(ǫk log(ed/k)) and each row of the matrix is sampled independently from an isotropic sub-Gaussian distribution. In the realizable setting, the sub-Gaussian assumption can be relaxed to incorporate heavy tail distribution via the “small ball” analysis introduced in Mendelson [2014], since we only require one-sided lower isometry property.",
      "startOffset" : 90,
      "endOffset" : 665
    }, {
      "referenceID" : 8,
      "context" : "Once again, without any regularity condition on the design matrix, Foster et al. [2016] have shown that achieving a sub-linear regret O(T ) is in general computationally hard, for any constant δ > 0 unless NP ⊆ BPP.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine.",
      "startOffset" : 133,
      "endOffset" : 323
    }, {
      "referenceID" : 3,
      "context" : "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine. Natarajan [1995], Boutsidis et al.",
      "startOffset" : 133,
      "endOffset" : 445
    }, {
      "referenceID" : 3,
      "context" : "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine. Natarajan [1995], Boutsidis et al. [2015] cast the sparse linear regression as maximization of weakly supermodular function.",
      "startOffset" : 133,
      "endOffset" : 470
    }, {
      "referenceID" : 3,
      "context" : "The definition is slightly stronger than that in Boutsidis et al. [2015]. We will show that sparse linear regression can be viewed as weakly supermodular minimization in Definition 7 once the design matrix has bounded restricted condition number.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "For the latter problem, we develop an online greedy algorithm along the lines of [Streeter and Golovin, 2008].",
      "startOffset" : 81,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "We employ k1 = O∗(k) budgeted experts algorithms [Amin et al., 2015], denoted BEXP, with budget parameter k0 k1 .",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "The precise characteristics of BEXP are given in Theorem 8 (adapted from Theorem 2 in [Amin et al., 2015]).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "This lemma is a direct consequence of Lemma 5 in [Boutsidis et al., 2015].",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "8 in Cesa-Bianchi and Lugosi [2006]: Lemma 13.",
      "startOffset" : 5,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "For example, simply assuming that the final matrix XT satisfies RIP rather than every intermediate matrix Xt for large enough t is not sufficient; a simple tweak to the lower bound construction of Foster et al. [2016] shows this.",
      "startOffset" : 197,
      "endOffset" : 218
    } ],
    "year" : 2017,
    "abstractText" : "Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.",
    "creator" : "LaTeX with hyperref package"
  }
}