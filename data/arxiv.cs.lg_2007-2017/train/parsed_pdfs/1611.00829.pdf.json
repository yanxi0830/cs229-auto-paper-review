{
  "name" : "1611.00829.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multidimensional Binary Search for Contextual Decision-Making",
    "authors" : [ "Ilan Lobel" ],
    "emails" : [ "ilobel@stern.nyu.edu", "renatoppl@google.com", "avladu@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n00 82\n9v 1\n[ cs\n.D S]\n2 N\nov 2\n01 6"
    }, {
      "heading" : "1 Introduction",
      "text" : "Binary search is one of the most basic primitives in algorithm design. The binary search problem consists in trying to guess an unknown real number θ ∈ [0, 1] given access to an oracle that replies for every guess xt if xt ≤ θ or xt > θ. After log(1/ǫ) guesses, the binary search algorithm is able to estimate θ within ǫ precision.\nWe study a multidimensional and online version of the binary search problem. The unknown quantity is a vector θ ∈ Rd with ‖θ‖2 ≤ 1 and in each iteration an adversary selects a direction ut ∈ Rd such that ‖ut‖2 = 1. At each iteration, the algorithm is asked to guess the value of the dot product θ⊤ut. After the algorithm makes a guess xt, it is revealed to the algorithm whether xt ≤ θ⊤ut or xt > θ⊤ut. The goal of the algorithm designer is to create an algorithm that makes as few mistakes as possible, where a mistake corresponds to a guess with an error larger than ǫ.\nThis problem has recently come up as a key building block in the design of online algorithms for contextual decision-making. In contextual decision-making, the direction ut corresponds to a context relevant for the period t decision and θ⊤ut corresponds to the optimal period t decision. Contextual decision-making is increasingly important in an economy where decisions are ever more customized and personalized. We now mention two applications:\nPersonalized Medicine [3]: Determining the right dosage of a drug for a given patient is a well-studied problem in the medical literature. For example, for certain anticoagulant drugs, the appropriate dosage can vary by up to a factor of 10 from individual to individual. Increasingly, doctors are using not only demographic information to decide dosage levels, but are also using higher-dimensional clinical and genetic data. Bastani and Bayati [3] propose a mathematical formulation for this problem and tackle it using tools from statistical learning and contextual bandits. At its core, the problem studied is a multidimensional binary search problem: each patient is associated with a vector of features ut which describes his or her demographic, genetic and clinical data. The algorithm outputs a recommended dosage xt and then observes only whether the dosage\nwas below or above the ideal level. If the ideal dosage is a linear function of the features with unknown coefficients θ then what the algorithm observes is whether θ⊤ut ≥ xt or θ⊤ut < xt. Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products. Instead of attempting to learn the market value of each product independently, it might be more sensible for the firm to build a pricing model based on features of each product. In internet advertising, for example, each impression sold by an ad exchange is unique in its combination of demographic and cookie data. While it is hopeless for the exchange to learn how to price each combination in isolation, it is reasonable for the firm to learn a model of the market value of its different products. In this setting, each product t is described by a vector ut of features. Assume the market value is a linear function with unknown coefficients θ. If the firm sets a price xt for this item it will learn that θ⊤ut ≥ xt if the product is sold and that θ⊤ut < xt otherwise. The goal in this setting is not minimizing guesses that are ǫ far from the θ⊤ut as in the personalized medicine setting, but to maximize revenue. Revenue, however, is a very asymmetric objective: if the price is above the market value we lose the sale and incur a large loss, while underpricing still leads to a sale where the loss in revenue is the difference θ⊤ut−xt. Nevertheless, Cohen et al [5] showed that an algorithm for the multidimensional binary search problem can be converted into an algorithm for the feature-based pricing problem in a black-box manner.\nThe first approach to this problem was due to Amin, Rostamizadeh and Syed [2] in the context of the pricing problem and is based on stochastic gradient descent. The stochastic gradient approach requires the features ut to be drawn from an unknown iid distribution, so that each feature can be used to obtain an unbiased estimator of a certain function. Subsequent approaches by Bastani and Bayati [3] and Qiang and Bayati [12] use techniques from statistical learning such as greedy least squares or LASSO. Javanmard and Nazerzadeh [8] apply a regularized maximum likelihood estimation approach and obtain an improved regret guarantee. One could also use a general purpose contextual bandit algorithm (such as Agarwal et al. [1]) to tackle the iid version of the multidimensional binary search problem, but such an algorithm would have regret that is polynomial in 1/ǫ instead of the logarithmic regret obtained by the specialized algorithms.\nAll the previously discussed work rely on assuming that the directions ut are sampled iid. The only approach that makes no assumptions about the directions ut is by Cohen et al [5]. They do so by tackling directly the multidimensional binary search problem with adversarial feature vectors ut and describe an algorithm with a bound of O(d\n2 log(d/ǫ)) on the number of mistakes. To obtain that bound, the paper builds on the ellipsoid method from convex optimization. The algorithm always maintains a knowledge set in the shape of an ellipsoid and then chooses an xt that cuts the ellipsoid through its center whenever there is significant uncertainty on the value of θ⊤ut. The algorithm than replaces the resulting half-ellipsoid with the smallest ellipsoid that encloses it, and proceeds to the next iteration.\nOur Contributions: Our paper significantly improves the regret bound on the multidimensional binary search problem, providing nearly matching upper and lower bounds for this problem. In Proposition 3.1, we construct a lower bound of Ω(d log(1/ǫ √ d)) via a reduction to d one-dimensional problems, which is significantly lower than the O(d2 log(d/ǫ)) regret bound from Cohen et al [5]. Under Cohen et al’s ellipsoid-based algorithm, a fraction 1 − e−1/2d of the volume is removed at each iteration. This fraction is significantly less than half because the step of replacing a half-ellipsoid with its own enclosing ellipsoid is expensive in the sense that it adds back to the knowledge set most of the volume removed in the latest cut. Thus, any ellipsoid-based method requires d steps in order to remove a constant fraction of the volume. Therefore, an algorithm that removes a constant fraction of the volume at each iteration has the potential to perform significantly\nbetter than an ellipsoid-based method and, thus, might close the gap between the upper and lower bounds. We can thus conjecture that an algorithm that selects xt in each iteration so as to create two potential knowledge sets of approximately equal volume would perform nearly optimally.\nCutting a convex set into two sets of approximately equal volume is not a difficult task. In a classical result, Grünbaum showed that cutting a set through its centroid generates two sets, each with at least a 1/e fraction of the original volume (see Theorem 5.1). Computing a centroid is a #Phard problem, but finding an approximate value of the centroid is sufficient for our purposes, and an approximation can be computed in polynomial time. An idea similar to this one was proposed by Bertsimas and Vempala [4], in a paper where they proposed a method for solving linear programs via an approximate Grünbaum theorem.\nHowever, removing constant fractions of the volume at each iteration is not sufficient for our purposes. Even if the knowledge set has tiny volume, we might not be able to guess the value of θ⊤u for some directions u with ǫ accuracy. To solve our problem, we need to ensure that the knowledge set becomes small along all possible directions. An algorithm that does not keep track of the width of the knowledge set along different directions might not perform well. Perhaps surprisingly, our conjecture that an algorithm that cuts through the centroid at each iteration would have nearoptimal regret is false. We show in Theorem 8.3 that such a centroid algorithm generates a worstcase regret of Ω(d2 log(1/ǫ √ d)). This occurs precisely because the centroid algorithm does not keep track of the different widths of the knowledge set. In an ellipsoid-based algorithm, keeping tracks of the widths of a knowledge set is a relatively easy task since they correspond to the eigenvalues of the matrix that represents the ellipsoid. Keeping track of widths is a more difficult task in an algorithm that does not rely on ellipsoids. This brings us to our key algorithmic idea: cylindrification.\nCylindrification is the technique we introduce of maintaining a set of directions along which the width of the knowledge set is small and expanding the set in those directions, thus converting the set into a high-dimensional cylinder. A cylindrified set when projected onto its subspace of small directions becomes a hypercube. When projected onto its subspace of large directions, a cylindrified set looks exactly like the original set’s projection onto the same subspace. Cylindrification reduces regret by significantly increasing the usefulness of each cut.\nOur main algorithm, the Projected Volume algorithm, maintains two objects at all times. It maintains a knowledge set (as the previous algorithms did), but it also maintains a set of orthogonal directions along which the knowledge set is small. At each iteration, it cylindrifies the knowledge set and then computes an approximate value of the centroid of the cylindrified set. It then chooses xt in order to cut through this approximate centroid. In Theorem 4.3, the main result of our paper, we prove that this algorithm has a near-optimal regret of O(d log(d/ǫ)).\nThe analysis of our algorithm relies on a series of results we prove about convex bodies. We first prove a directional version of Grünbaum’s theorem (Theorem 5.3), which states that the width of the two sets along any direction u after a cut through the centroid are at least 1/(d+1) of the width along u of the original set. We also prove that Grünbaum’s theorem is robust to approximations (Lemma 5.5) and projections (Lemma 7.1). We also prove that the process of cylindrification does not add too much volume to the set (Lemma 6.1). We then use these geometric results to prove that the volume of the knowledge set projected onto its large directions serves as a potential function and show that it decreases exponentially fast, proving our main result."
    }, {
      "heading" : "2 The Model",
      "text" : "We consider an infinite horizon game between a player and nature. The game begins with nature selecting a state θ from the d-dimensional unit ball centered at the origin. We label this ball K0,\ni.e., K0 = {θ ∈ Rd : ||θ||2 ≤ 1}. The player knows K0, but does not know the value of θ. 1 At every period t = 0, 1, 2, ..., nature selects a vector ut from the d-dimensional unit sphere, i.e., U = {u ∈ Rd : ||u||2 = 1}, that we refer to as the period t direction. At every period, after nature reveals ut, the player must choose an action xt ∈ R. The player’s goal is to choose a value of xt that is close to u ⊤ t θ. Formally, we try to minimize the number of mistakes we make, where a mistake occurs whenever |xt − u⊤t θ| > ǫ for a given ǫ > 0. We incur regret in period t whenever we make a mistake:\nrt =\n{\n0 if |xt − u⊤t θ| ≤ ǫ ; 1 if |xt − u⊤t θ| > ǫ .\nAt the end of each period, nature reports to the player whether xt ≤ u⊤t θ or xt > u⊤t θ. We note that we do not learn the regret rt in each period, only whether xt − u⊤t θ is positive. Our goal is to find a policy that minimizes our total regret, or equivalently, the total number of mistakes we make over an infinite time horizon, i.e., R = ∑ ∞\nt=1 rt."
    }, {
      "heading" : "3 Lower Bound",
      "text" : "We now construct a lower bound on the regret incurred by our algorithm. The lower bound is obtained via a straightforward reduction to d one-dimensional problems. Proposition 3.1. Any algorithm will generate regret of at least Ω(d log(1/ǫ √ d)). Proof. Assume nature selects θ from within a d-dimensional cube with sides of length 1/ √ d. This is a valid choice since the unit ball K0 contains such a cube. Let ei represent the vector with value 1 in coordinate i ∈ {1, ..., d} and value 0 in all other coordinates. Suppose nature selects directions that correspond to the vectors ei in round-robin fashion, i.e., ut = e(t mod d)+1. Because of the symmetry of the cube from which θ is selected, and the orthogonality of the directions ut, this problem is equivalent to d independent binary searches over one-dimensional intervals with length l = 1/ √ d. Our result follows since a one-dimensional binary search over an interval with length l up to precision ǫ incurs Ω(log(l/ǫ)) mistakes.\nWe note that the lower bound above applies even for the iid version of the multidimensional binary search problem, as nature could be given a distribution over d orthogonal direction vectors. Making the problem offline would also not lower the regret, as having advance knowledge of the direction vectors is useless in the instance above."
    }, {
      "heading" : "4 The Projected Volume Algorithm",
      "text" : "In this section, we describe the central idea for obtaining near-optimal regret. In the standard single-dimensional binary search algorithm, the error of the algorithm at any given iteration is proportional to the length of the interval. The length of the interval thus provides a clear measure in which to make progress. In the multi-dimensional case, there is no global measure of error, but only a measure of error for each direction. To make this precise, consider a knowledge set K ⊆ Rd corresponding to the set of values of θ that are compatible with what the algorithm has observed. Given a direction u (i.e., u is a unit vector), the error incurred by the algorithm to predict the dot product u⊤θ corresponds to the directional width of K along u:\nw(K,u) = max x,y∈K\nu⊤(x− y) . (4.1)\n1Although we assume for simplicity that K0 is a ball throughout our paper, we could have let K0 be an arbitrary convex body contained inside the unit ball.\nwhich is a measure that is particular for direction u. Since the algorithm does not know which directions it faces in future iterations, it must decrease some measure that implies progress in a more global sense. A natural such measure is the volume of K. However, measuring volume alone might be misleading. Consider that case where our current knowledge set is the thin rectangle represented in Figure 1.\nǫ\nFigure 1: Decreasing volume might not lead to progress with respect to width. Both horizontal and vertical cuts remove half the volume, but only the vertical cut makes progress towards our goal.\nCutting the knowledge set along either the red horizontal or the blue vertical line and keeping one the sides would decrease the volume by half. From the perspective of our problem, however, the red cut is useless since we already have a good estimate of the width along that direction. Meanwhile, the blue cut is very useful since it decreases the width along a direction that has still a lot of uncertainty to be resolved.\nMotivated by this observation we keep track of the volume of the knowledge set projected onto a subspace for which there is still a non-trivial amount of uncertainty. Precisely, our algorithm will be parametrized by a value δ > 0 which defines the notion of ‘small’. We maintain two objects:\n1. the knowledge set Kt ⊆ Rd which will consist of all vectors θ which are consistent with the observations of the algorithm so far.\n2. a set of orthonormal vectors St = {s1, . . . , snt} spanning a subspace Ut of dimensionality nt such that the knowledge set has small width along any of those directions and has large width along any direction perpendicular to them. Formally:\nUt = span(St) s.t. w(Kt, s) ≤ δ, ∀s ∈ St and w(Kt, u) > δ, for all u perpendicular to Ut , where span(·) denotes the span of a set of vectors. It will be useful to refer to Lt = {u|u⊤s = 0, ∀s ∈ St} as the subspace of large directions.\nOur plan will be to ignore a dimension once it becomes small enough and focus on bounding the volume of the projection of the knowledge set Kt onto the subspace of large directions Lt. To formalize this notion, let us define the notion of cylindrification of a set with respect to orthonormal vectors.\nDefinition 4.1 (Cylindrification). Given a set of orthonormal vectors S = {s1, . . . , sn}, let L = {u|u⊤s = 0;∀s ∈ S} be the subspace orthogonal to span(S) and ΠL(K) be the projection 2 of K onto L. Given a convex set K ⊆ Rd and a set of orthonormal vectors S = {s1, . . . , sn} we define:\nCyl(K,S) :=\n{\nx+ n ∑\ni=1\nyisi\n∣ ∣ ∣ ∣\nx ∈ ΠL(K) and min θ∈K θ⊤si ≤ yi ≤ max θ∈K θ⊤si\n}\n."
    }, {
      "heading" : "Or more concisely, but less intuitively:",
      "text" : "Cyl(K,S) = ΠL(K) + Πspan(s1)(K) + . . .+Πspan(sn)(K)\nwhere the sums applied to sets are Minkowski sums. 3\nInformally, the cylindrification operation is designed to create a set with the same projection onto the subspace of large directions, i.e., ΠLtCyl(Kt, St) = ΠLt(Kt), while regularizing the projection of the set onto the subspace of small directions: ΠStCyl(Kt, St) is a box.\n2Formally if {ℓ1, . . . , ℓk} is an orthonormal basis of L, then πL(x) = ∑ k\ni=1 ℓiℓ\n⊤\ni x and ΠL(K) = {πL(x)|x ∈ K}. 3By Minkowski sum between two sets, we mean A+B = {a+ b : a ∈ A, b ∈ B}.\nWe are now ready to present our algorithm, focusing on its geometric aspects and ignoring (for now) the question on how to efficiently compute each step. The algorithm is parametrized by a constant δ > 0. It starts with K0 being the ball of radius 1 and with S0 = ∅. In each iteration the algorithm receives a unit vector ut from nature. The algorithm then predicts xt using the centroid zt of Cyl(Kt, St), by setting xt = u ⊤ t zt. The definition of the centroid is given below:\nDefinition 4.2. The centroid z of a convex set K is defined as\nz = 1\nvol(K)\n∫\nx∈K x dx ,\nwhere vol(·) denotes the volume of a set. Upon learning if the estimate was too small or too large, we updateKt to Kt+1 = Kt∩{θ|θ⊤ut ≤ xt} or Kt+1 = Kt ∩ {θ|θ⊤ut ≥ xt}. The next step in our algorithm is to verify if there exists any direction v orthogonal to St such that w(Kt+1, v) ≤ δ. As long as such directions exists, we add them to St and call the resulting set St+1.\nOur main result in this paper is:\nTheorem 4.3. The Projected Volume algorithm has regret O(d log(d/ǫ)) for the multi-dimensional binary search problem.\nOur strategy for proving Theorem 4.3 is to use the volume of the projection of Kt onto the subspace of large directions as our potential function:\nΦt := vol(ΠLtKt) .\nIn each iteration, either the set of small directions remains the same or it grows. We first consider the case where the set of small directions remains the same, i.e., St+1 = St. In this case, we want to argue that the volume of the projection of Kt onto Lt decreases in that iteration. If St = ∅, then ΠLtKt = Kt and the volume decreases by at least a constant factor. This follows from Grünbaum’s Theorem, which we review in the next section. However, if St 6= ∅, then a decrease in the volume of Kt does not necessarily guarantee a decrease in the volume of the projection. For example, consider the example in Figure 2 where we cut through the center of a rectangular Kt. Even though the volume of Kt+1 is half the volume of Kt, the volume of the projection onto the x-axis doesn’t decrease as much. We will argue that the decrease in volume due to Grünbaum’s Theorem extends to projections (with a small loss) if the width along the cut direction is much larger than the width along the directions orthogonal to the projection subspace.\nWe now consider the case where we add a new direction to St. In this case, we will measure the volume in the next iteration as projected onto a subspace of smaller dimension than in period t. In general, the volume of a projection can be arbitrarily greater than then volume of the original set. We will use, however, the fact that the Kt is “large” along every direction of Lt to argue that adding a vector to St can blow up the potential by at most a factor of O(d\n2/δ). While this is a non-trivial volume increase, this can happen at most d times, leading to a volume increase by a factor of at most O(d2/δ)d. We can use this fact to obtain that the algorithm will take at most O(d log(d/δ)) steps before Lt becomes zero-dimensional.\nAn inquisitive reader might wonder if we truly need cylindrification to obtain near-optimal regret. We could consider an algorithm that simply chooses xt = u ⊤ t zt at each iteration, where zt is the centroid of Kt. We show in Theorem 8.3 that such an algorithm incurs regret of Ω(d 2 log(1/ǫ √ d)). Without cylindrification, nature might select directions such that most of the volume reduction corresponds to widths in directions along which the set is already small. Cutting at the centroid of the cylindrified set, instead of the centroid of the original set, is thus crucial to ensure we make progress in the large directions.\nThe Projected Volume algorithm as discussed above does not actually run in polynomial time since computing the centroid of a convex set is a #P-hard problem. Fortunately, we can turn Projected Volume into a polynomial time algorithm with a few tweaks, as we show in Theorem 9.4. The key step is to approximate the value of the centroid instead of relying on an exact computation. The polynomial time version of Projected Volume presented in Section 9 also contains a technique for efficiently finding small directions to add to the set St."
    }, {
      "heading" : "5 Convex Geometry Tools",
      "text" : "In this section, we begin to develop the technical machinery required by the plan outlined in the previous section. In the heart of the proof will be a statement relating the volume of a convex body and a volume of its cylindrification with respect to dimensions along which the body ‘small’. In order to obtain this result, we will require customized versions of Grünbaum’s Theorem. Let us start by revisiting the basic statement of the theorem:\nTheorem 5.1 (Grünbaum). Let K be a convex set, and let z be its centroid. Given an arbitrary nonzero vector u, let K+ = K ∩ {x|u⊤(x− z) ≥ 0}. Then,\n1 e · vol(K) ≤ vol(K+) ≤\n(\n1− 1 e\n)\n· vol(K) .\nIn other words, any hyperplane through the centroid splits the convex set in two parts, each of which having a constant fraction of the original volume. See Grünbaum [7] for the original proof of this theorem, or Nemirovski [11] for a more recent exposition. The first step in the proof of Grünbaum’s Theorem consists of applying Brunn’s Theorem, which is an immediate consequence of the Brunn-Minkowski inequality:\nTheorem 5.2 (Brunn). Given a convex set K, and let g(t) be the (d − 1)-dimensional volume of the section K(t) := K ∩ {x|x⊤e1 = t}. Then the function r(t) := g(t)1/(d−1) is concave in t over its support.\nWe will rely on Brunn’s Theorem to prove our customized versions of Grünbaum’s Theorem."
    }, {
      "heading" : "5.1 Directional Grünbaum Theorem",
      "text" : "We begin by proving a theorem which characterizes how much directional widths of a convex body can change after a cut through the centroid. In some sense, this can be seen as a version of Grünbaum’s Theorem bounding widths rather than volumes.\nTheorem 5.3 (Directional Grünbaum). If K is a convex body and z is its centroid, then for every unit vector u 6= 0, the set K+ = K ∩ {x|u⊤(x− z) ≥ 0} satisfies\n1\nd+ 1 · w(K, v) ≤ w(K+, v) ≤ w(K, v) ,\nfor all unit vectors v.\nThe first step will be to prove Theorem 5.3 when v is the direction of u itself. We prove this in the following lemma.\nLemma 5.4. Under the conditions of Theorem 5.3, w(K+, u) ≥ 1d+1 · w(K,u). We defer the proof of this lemma to Appendix A.1. We are now ready to prove the Directional Grünbaum Theorem: Proof of Theorem 5.3. By translating K we can assume without loss of generality that z = 0. Consider three cases:\n1. There exists a point x+v ∈ K+ ∩ argmaxx∈K v⊤x. In such case, we know by the previous lemma that\nw(K+, v) ≥ v⊤(x+v − z) ≥ 1\nd+ 1 w(K, v) .\n2. The second case is where there exists a point x−v ∈ K+ ∩ argminx∈K v⊤x. Then,\nw(K+, v) ≥ v⊤(z − x−v ) ≥ 1\nd+ 1 w(K, v) .\n3. In the remaining case, let x+v ∈ argmaxx∈K v⊤x and x−v ∈ argminx∈K v⊤x be such that u⊤x+v < 0 and u ⊤x−v < 0. Also, let xu = argmaxx∈K u ⊤x. In such a case, choose real\nnumbers λ+, λ− between zero and one such that:\nu⊤ ( xu + λ +(x+v − xu) ) = 0 and u⊤ ( xu + λ −(x−v − xu) ) = 0 .\nWe can bound λ+ and λ− as follows: 1\nd+ 1 w(K,u) ≤ u⊤xu = λ+ · u⊤(xu − x+v ) ≤ λ+ · w(K,u) .\nSo λ+ ≥ 1d+1 . By the same argument λ− ≥ 1d+1 . Now, the points, x̃+v = xu + λ+(x+v − xu) and x̃−v = xu + λ\n−(x−v − xu) are in K+, since they are convex combinations of points in K and their dot product with u is non-negative. Now:\nw(K+, v) ≥ v⊤(x̃+v − x̃−v ) = λ+v⊤(x+v − xu) + λ−v⊤(xu − x−v ) ≥ v⊤(x+v − x−v )\nd+ 1 =\nw(K, v)\nd+ 1 ."
    }, {
      "heading" : "5.2 Approximate Grünbaum Theorem",
      "text" : "We will use the Directional Grünbaum Theorem to give an approximate version of the standard volumetric Grünbaum Theorem. Essentially, we will argue that if we cut through a point sufficiently close to the centroid, then either side of the cut will still contains a constant fraction of the volume.\nLemma 5.5 (Approximate Grünbaum). Let K be a convex body, and let z be its centroid. For an arbitrary unit vector u, and scalar δ such that 0 ≤ δ ≤ w(K,u)/(d + 1)2, let Kδ+ = {x ∈ K|u⊤(x− z) ≥ δ}. Then,\nvol(Kδ+) ≥ 1\ne2 · vol(K) .\nThe proof of this lemma follows from a modification of the original proof for Grünbaum’s theorem, and it can be found in Appendix A.2."
    }, {
      "heading" : "6 Cylindrification",
      "text" : "Next we study how to relate the volume of a convex body to the volume of its projection onto a subspace.\nLemma 6.1 (Cylindrification). Let K ⊂ Rd be a convex body such that w(K,u) ≥ δ for every unit vector u, then for every (d− 1) dimensional subspace L:\nvol(ΠLK) ≤ d(d+ 1)\nδ · vol(K) .\nAs one of the ingredients of the proof, we will use John’s Theorem:\nTheorem 6.2 (John). If K ⊂ Rd is a bounded convex body, then there is a point z and an ellipsoid E centered at the origin such that:\nz + 1\nd E ⊆ K ⊆ z + E .\nIn particular, we will use the following consequence of John’s Theorem:\nLemma 6.3. If K ⊂ Rd is a convex body such that w(K,u) ≥ δ for every unit vector u, then K contains a ball of diameter δ/d.\nProof. Applying John’s theorem and translating K if necessary so that z = 0, there exists an ellipsoid E such that 1dE ⊆ K ⊆ E. Since the width of K in each direction is at least δ, the width of E must be at least δ in each direction. Since E is an ellipsoid, it must contain a ball of diameter δ. Thus, 1dE contains a ball of diameter δ d . Hence, K also contains such a ball.\nWe now prove our cylindrification lemma. Proof of Lemma 6.1. Our proof proceeds in two steps:\nStep 1: Squashing K. Assume without loss of generality that the (d−1)-dimensional subspace L is the space defined by the d−1 first coordinates. Represent by xL the projection of each x onto the d − 1 first components and define f : Rd−1 → R such that f(xL) is the length of the segment in the intersection of K and the line {(xL, y) : y ∈ R} (see the top of Figure 3). Formally:\nf(xL) =\n∫\ny∈R 1K(xL, y)dy .\nWe now argue that f is concave. Given xL ∈ Rd−1 let ax, bx be such that (xL, ax), (xL, bx) ∈ K and f(xL) = bx − ax. Let yL, ay, by be defined analogously. To see that f is concave, given 0 < tx, ty < 1 with tx + ty = 1 we have: (txxL + tyyL, txax + tyay) and (txxL + tyyL, txbx + tyby) are in K by convexity, so: f(txxL+ tyyL) ≤ (txbx+ tyby)− (txax+ tyay) = txf(xL)+ tyf(yL) , which allows us to define the squashed version of K (depicted in the bottom of Figure 3) as: K ′ = {(xL, h) : xL ∈ ΠLK, 0 ≤ h ≤ f(xL)} . By construction, vol(K ′) = vol(K).\nStep 2: Conification. We know by Lemma 6.3 that K contains a ball of diameter δ/d so there exists xL such that f(xL) ≥ h := δ/d. Define then the cone C to be the convex hull of {(xL, 0) : xL ∈ ΠLK} and (xL, h). Such cone is a subset of K ′, so vol(K) = vol(K ′) ≥ vol(C). Since the volume of a d-dimensional cone is given by the volume of the base times the height divided by d+ 1,\nvol(K) ≥ vol(C) = h d+ 1 · vol(ΠLK) = δ d(d + 1) · vol(ΠLK) ."
    }, {
      "heading" : "7 Analysis of the Projected Volume Algorithm",
      "text" : "We are now almost ready to analyze our algorithm. To do so, we first consider a version of Grünbaum’s Theorem which concerns cuts through the centroid of a cylindrified set. The set being cut is still the original set, but we focus on what happens to the volume of its projection onto the subspace of large directions. The proof of this lemma can be found in Appendix A.3.\nLemma 7.1 (Projected Grünbaum). Let K be a convex set contained in the ball of radius 1, and let S be a set of orthonormal vectors along which w(K, s) ≤ δ ≤ ǫ2 16d(d+1)2\n, for all s ∈ S. Let L be the subspace orthogonal to S, and let ΠL be the projection operator onto that subspace. If u is a direction along which w(Cyl(K,S), u) ≥ ǫ, z is the centroid of the cylindrified body Cyl(K,S), and K+ = {x ∈ K : u⊤(x− z) ≥ 0}, then:\nvol(ΠLK+) ≤ ( 1− 1 e2 ) · vol(ΠLK) ,\nwhere vol(·) corresponds to the (n− |S|)-dimensional volume on the subspace L. We now employ the tools we have developed to analyze the regret of the Projected Volume algorithm. As outlined in Section 4, we will keep in each iteration a convex set Kt of candidate θ vectors and we will keep a orthonormal basis St of directions for which Kt is small. If Lt is the subspace of directions that are orthogonal to St then our plan is to bound the potential Φt = vol(ΠLtKt). Notice that if Lt is empty, then St must be an orthonormal basis such that w(Kt, s) ≤ δ,∀s ∈ St. In particular, for every unit vector u and any two x, y ∈ Kt we must have:\nu⊤(x− y) = ∑\ns∈St\nu⊤s · s⊤(x− y) ≤ dδ .\nIf δ ≤ ǫ/d, then the algorithm will be done once Lt becomes empty. Our goal then is to bound how many iterations can we have where Lt is non-empty. First we provide a lower bound on the potential. We will use in this section the symbol γd to denote the volume of the d-dimensional unit ball. The following loose bound on γd will be sufficient for our needs: Ω(d\n−d) ≤ γd ≤ O(1). Lemma 7.2. If Lt is non-empty then Φt ≥ Ω( δd)2d.\nProof. Let KL = ΠLtKt and k be the dimension of L. Then w(KL, u) ≥ δ for all u ∈ L implies by Lemma 6.3 that KL contains a ball of radius δ k , so vol(KL) ≥ γk ( δ k )k . Since ( δ k )k ≥ ( δ d )d and γk ≥ Ω(1d)d we have that Φt ≥ Ω( δd)2d.\nNow we will give an upper bound on Φt as a function of t. Together with the previous lower bound, we will get a bound on the number of iterations that can happen before Lt becomes empty. The main ingredient will be a Grünbaum-type bound on the volume of the projection that is specifically tailored to our application. For this purpose, we use Lemma 7.1, which will specifically address the issue discussed in Figure 2. We are now ready for the proof of our main theorem: Proof of Theorem 4.3. Our goal is to bound the number of steps for which the algorithm guesses with at least ǫ error. Let Rt be the total regret after t steps. Let Nt be 1 if w(Cyl(Kt, Lt), ut) > ǫ and zero otherwise. Since ∣\n∣u⊤t (zt − θ) ∣ ∣ ≤ ǫ whenever w(Cyl(Kt, Lt), ut) ≤ ǫ, Rt ≤ ∑t τ=1 Nτ .\nLet Kt and Lt be the respective set and subspace after t iterations. Setting δ ≤ ǫ 2\n16d(d+1)2 we\ncan apply Lemma 7.1 directly to obtain that:\nvol(ΠLtKt+1) ≤ ( 1− 1 e2 )Nt vol(ΠLtKt) .\nIf Lt+1 = Lt, then vol(ΠLt+1Kt+1) = vol(ΠLtKt+1). If we add one new direction v ∈ Lt to S, then we replace KL = ΠLtKt by its projection on the subspace L ′ = {x ∈ Lt : v⊤x = 0}. Since\nw(Kt, u) ≥ δ,∀u ∈ Lt, then by Theorem 5.3 after we cut Kt we have w(Kt+1, u) ≥ δd+1 , so applying the Cylindrification Lemma (Lemma 6.1) we obtain:\nvol(ΠL′Kt+1) ≤ d(d+ 1)2\nδ vol(ΠLtKt+1) .\nIf we need to add r new directions to Lt the volume can blow up by at most ( d(d+1)2\nδ\n)r . In\nparticular, since the initial volume is bounded by O(1), then:\nΩ\n(\nδ\nd\n)2d ≤ Φt = vol(ΠLtKt) ≤ O(1) · ( d(d + 1)2\nδ\n)d\n· ( 1− 1 e2 )\n∑ t\nτ=1 Nτ\n,\nwhich means that:\nRt ≤ t ∑\nτ=1\nNτ ≤ O ( d log d\nδ\n)\n= O\n(\nd log d\nǫ\n)\n."
    }, {
      "heading" : "8 Why Cylindrification?",
      "text" : "At the heart of our algorithm lies the simple idea that we should cut a constant fraction of the volume at each iteration if we want to achieve a Õ(d log(1/ǫ)) regret bound. Our algorithm, however, is quite a bit more complex than that. It also keeps a set of ‘small’ directions St and it cuts through the center of a cylindrified version of the knowledge set Kt at each iteration. An inquisitive reader might wonder whether this additional complexity is really necessary. In this section we argue that cylindrification is actually necessary to obtain our near-optimal regret bound. We prove there exists an instance where the algorithm that only cuts through the center of the knowledge set (without cylindrifying it first) incurs Ω(d2 log(1/ǫ √ d)) regret.\nFormally, consider the algorithm that only keeps Kt and in each iteration guesses xt = u ⊤ t zt\nwhere zt = 1\nvol(Kt)\n∫\nKt xdx and updates Kt to K + t or K − t . We call this procedure the Centroid\nalgorithm. In order to construct an instance with Ω(d2 log(1/ǫ √ d)) regret for this algorithm, we first define the following set. Given s = (s1, . . . , sk) with si > 0 for all i, define:\n∆(s) = {x ∈ Rk+ : ∑ i xi si ≤ 1} = conv({0, s1e1, . . . , skek}), where conv(·) denotes the convex hull of a set of points. Lemma 8.1. The centroid of ∆(s) is given by sk+1 .\nWe now consider how the Centroid algorithm performs on a particular set, when nature selects a specific sequence of directions. The set we start from is the product between a (d−k)-dimensional hypercube and a k-dimensional set ∆(s), where only the kth entry of s is significantly larger than ǫ. We now argue that it nature might require us to take Ω(k log(1/ǫ)) into a similarly structured set with k replaced by k+1. Repeating this argument d times will lead to our negative conclusion on the performance of the Centroid algorithm.\nLemma 8.2. Let 1 ≤ k < d, s ∈ Rk with 0 ≤ si ≤ ǫ for i < k, 14 ≤ sk ≤ 1. If K = ∆(y)× [0, 1]d−k\nthen there is a sequence of Ω(k log(1/ǫ)) directions ut such that the Centroid algorithm incurs Ω(k log(1ǫ )) regret and by the end of the sequence, the knowledge set has the form: K ′ = ∆(s′)× [0, 1]d−k−1 where s′ ∈ Rk+1, 0 ≤ s′i ≤ ǫ for i < k + 1, 14 ≤ s′i ≤ 1.\nProof sketch. Starting from K, as a first step we select Ω(k log(1ǫ )) vectors in the direction ek which cause the side of the k-th side of the simplex ∆(s) to reduce to ǫ getting one unit of regret in each step. At the end of this step we are left with the situation illustrated in Figure 8.2. In step 2 we choose a direction slightly bent towards ek+1 to carve a (k+1)-dimensional simplex out of K. The resulting shape will be, as depicted in Figure 8.2, only partially what we want. In the third step we select more directions along ek+1 to remove the ‘leftover’ and keep only the part corresponding to the (k + 1)-dimensional simplex. A complete proof is provided in Appendix A.5.\nThis is the main ingredient necessary to show that the algorithm without cylindrification can incur Ω(d2 log(1/ǫ √ d)) regret.\nTheorem 8.3. The algorithm that always chooses xt = u ⊤ t zt where zt is the centroid of Kt can\nincur Ω(d2 log(1/ǫ √ d)) regret.\nProof. Start with the set K0 = [0, 1] d. Apply Lemma 8.2 for k = 1, 2, 3, . . . , d − 1. The total regret is ∑d−1\nk=1Ω(k log( 1 ǫ )) = Ω(d 2 log(1ǫ )). To construct a valid instance (one that fits within a\nball of radius 1), we replace our initial set with K0 = [ 0, 1/ √ d ]d , leading to an aggregate regret of Ω(d2 log(1/ǫ √ d)). We did not do our computations above using this scaled down instance instead\nof [0, 1]d in order to avoid carrying extra √ d terms."
    }, {
      "heading" : "9 Computation",
      "text" : "The Projected Volume algorithm described earlier, while yielding optimal regret with respect to the number of dimensions d, can’t be implemented as presented in polynomial time. The reason is that it requires implementing two steps, both of which involve solving highly nontrivial problems. The first is computing the centroid, which is known to be #P-hard [13]. The second is finding a direction along which a convex bodyK is “thin” (i.e. finding a unit vector u such that w(K,u) ≤ δ), for which we are not aware of a polynomial time algorithm.\nIn order to make these problems tractable, we relax the requirements of our algorithm. More specifically, we will show how our algorithm is robust, in the sense that using an approximate centroid, and finding approximately thin directions does not break the analysis.\nIn the following subsections, we show how to implement both of these steps. Then, we put them together into a polynomial time version of our algorithm."
    }, {
      "heading" : "9.1 Approximating the Centroid",
      "text" : "An approximation of the centroid sufficient for our purposes follows from a simple application of standard algorithms for sampling points from convex bodies (hit-and-run [9], ball-walk [10]). A\nsimilar application can be found in Bertsimas and Vempala [4], where the authors use approximate centroid computation in order to solve linear programs.\nOur application faces the same issues as in [4]. Namely, in order to efficiently sample from a convex body, one requires that the body is nearly isotropic. Although the body we start with is isotropic, after cutting or projecting this property is lost. Therefore we require maintaining a linear transformation under which the body ends up being in isotropic position. The many issues encountered when approximating the centroid are carefully handled in [4], so we will restate the following result which is implicit there (see Lemma 5 and Theorem 12):\nTheorem 9.1 ([4]). Given a d-dimensional convex body K, one can compute an approximation z′ to the centroid z of K in the sense that ‖z − z′‖ ≤ ρ in Õ(d4/ρ) steps of a random walk in K. Note that for hit-and-run sampling, one only requires determining the intersection between a line and the given convex body; in our case this only requires one iteration through the inequality constraints determining the body."
    }, {
      "heading" : "9.2 Finding Approximately Thin Directions",
      "text" : "Instead of exactly recovering directions u satisfying w(K,u) ≤ δ, we instead recover all the directions along which w(K,u) ≤ δα , and potentially some along which δα ≤ w(K,u) ≤ δ. We do this by computing an ellipsoidal approximation of K. Indeed, having access to an ellipsoid E such that E ⊆ K ⊆ αE , we can:\n1. find a direction u such that w(K,u) ≤ δ, by checking whether E has a direction u such that w(E, u) ≤ δ/α, or\n2. decide that w(K,u) ≥ δ/α for all u simply by showing that the smallest directional width of E is greater than or equal to δ/α.\nThis task can be performed simply by inspecting the eigenvalues of E. A natural notion for such an ellipsoid is the John ellipsoid. However, computing it is NP-hard. Instead, by relaxing the approximation factor, a polynomial time algorithm can be obtained. Such a result is provided in Grötschel et al [6], which we reproduce below for completeness (see Corollary 4.6.9).\nTheorem 9.2 ([6]). Given a convex body K containing a ball of radius r, and contained inside a ball of radius R, along with access to a separation oracle for K, one can compute an ellipsoid E such that E ⊆ K ⊆ √ d(d+ 1)E using dO(1) · log(R/r) oracle calls.\nThis immediately yields the following Corollary, which we will use in our algorithmic result.\nCorollary 9.3. Given a convex body K containing a ball of radius r, and contained inside a ball of radius R, along with a separation oracle for K, one can either find a direction u such that w(K,u) ≤ δ, or certify that w(K,u) ≥ δ/( √ d(d+ 1)) for all u using dO(1) · log(R/r) oracle calls."
    }, {
      "heading" : "9.3 Obtaining a Polynomial Time Algorithm",
      "text" : "The polynomial time version of our algorithm is very similar to the initial one. The differences that make computation tractable are:\n1. Instead of computing the centroid exactly, we compute the centroid to within distance ρ = (ǫ/d)O(1), via Theorem 9.1.\n2. Every iteration of the algorithm the set St is updated by repeatedly computing the ellipsoidal approximation described in Corollary 9.3, and adding the direction u corresponding to the smallest eigenvalue of the ellipsoid, if it certifies that w(K,u) ≤ δ. When no such direction is found, we know that w(K,u) ≥ δapprox := δ/ √ d(d+ 1) for all u.\nA complete description of the new algorithm, along with its analysis, can be found in Appendix B. Combining the results in this section, we obtain the following theorem:\nTheorem 9.4. There exists an algorithm that runs in time (d/ǫ)O(1) achieving regret O(d log(d/ǫ)) for the multi-dimensional binary search problem."
    }, {
      "heading" : "A Deferred Proofs",
      "text" : "A.1 Proof of Lemma 5.4\nSince width is invariant under rotations and translations we can assume, without loss of generality, that u = −e1. Also, since scaling the convex set along the direction of u also scales the corresponding coordinate of the centroid by the same factor, we can assume that the projection of K onto the e1 axis is [0, 1]. Using the notation from Theorem 5.2, we can write the first coordinate of the centroid z as\nz⊤e1 = 1\nvol(K)\n∫\nK x⊤e1 dx =\n∫\nK x ⊤e1 dx\n∫ K 1 dx = ∫ 1 0 t · r(t)d−1 dt ∫ 1 0 r(t) d−1 dt .\nOur goal is to show that z⊤e1 ≥ 1d+1 . We will do it in a sequence of two steps. To simplify notation, let us define V := vol(K).\nStep 1: linearize r. We prove that the linear function r̃ : [0, 1] → R given by\nr̃(t) = (V d)1/(d−1) · (1− t)\nsatisfies\n∫ 1\n0 t · r̃(t)d−1dt ≤\n∫ 1\n0 t · r(t)d−1dt and\n∫ 1\n0 r̃(t)d−1dt =\n∫ 1\n0 r(t)d−1dt = V .\nWe immediately see that the second condition is satisfied, simply by evaluating the integral. Next we show that r̃ satisfies the first condition.\nSince by definition, r is supported everywhere over [0, 1], it means that r(1) ≥ r̃(1) = 0, and therefore r(0) ≤ r̃(0) (since otherwise, by concavity, it would be the case that r(t) ≥ r̃(t) everywhere, and the second identity could not possibly hold). Again, using the concavity of r, this implies that there exists a point p ∈ [0, 1] such that r(t) ≤ r̃(t) for all t ∈ [0, p], and r(t) ≥ r̃(t) for all t ∈ [p, 1].\nHence, we can write\n∫ 1\n0 t ·\n( r(t)d−1 − r̃(t)d−1 ) dt =\n∫ p\n0 t ·\n( r(t)d−1 − r̃(t)d−1 ) dt+\n∫ 1\np t ·\n( r(t)d−1 − r̃(t)d−1 ) dt ,\nwhere all the coefficients of t from the first term are nonpositive, and all the coefficients of t from the second term are nonnegative. Therefore we can lower bound this integral by\n∫ p\n0 p· ( r(t)d−1 − r̃(t)d−1 ) dt+\n∫ 1\np p· ( r(t)d−1 − r̃(t)d−1 )\ndt = p· ( ∫ 1\n0 r(t)d−1dt−\n∫ 1\n0 r̃(t)d−1dt\n)\n= 0 ,\nwhich proves that the first condition also holds. Step 2: solve for the linear function. We can explicitly compute\n∫ 1\n0 t · r̃(t)d−1dt = V d ·\n∫ 1\n0 t · (1− t)d−1dt = V d · 1 d(d+ 1) = V d+ 1 .\nTherefore, combining the results from the two steps, we see that\n1\nd+ 1 = ∫ 1 0 t · r̃(t)d−1dt ∫ 1 0 r̃(t) d−1dt ≤ ∫ 1 0 t · r(t)d−1dt ∫ 1 0 r(t) d−1dt = z⊤e1 ,\nwhich yields the desired conclusion.\nA.2 Proof of Lemma 5.5\nSince our problem is invariant under rotations and translations, let us assume that u = e1, and z = 0. Furthermore, notice that our problem is invariant to scaling K along the direction of u. Therefore we can assume without loss of generality that [a, 1] is the projection of K onto the e1 axis. Then, in the notation of Lemma 5.2, we have:\nvol(K+) =\n∫ 1\n0 r(t)d−1dt , vol(Kδ+) =\n∫ 1\nδ r(t)d−1dt .\nFrom Theorem 5.1, we know that vol(K+) ≥ vol(K)/e. We will show that vol(Kδ+) ≥ vol(K+)/e, which yields the sought conclusion.\nFrom Theorem 5.3 we know that w(K,u)/(d+1) ≤ 1. Hence, using our bound on δ, we obtain δ ≤ 1/(d + 1). We are left to prove, using the fact that r is a nonnegative concave function, that:\n∫ 1\n1/(d+1) r(t)d−1dt ≥ 1 e · ∫ 1 0 r(t)d−1dt .\nTo see that this is true, it is enough to argue that the ratio between the two integrals is minimized when r is a linear function r(t) = c ·(t−1), for any constant c; in that case, an explicit computation of the integrals produces the desired bound.\nTo see that the ratio is minimized by a linear function, we proceed in two steps. First, consider the function r̃ obtained from r by replacing in on the [1/(d + 1), 1] interval with a linear function starting at r(1/(d + 1)) and ending at 0:\nr̃(t) =\n\n\n\nr(t), if t ∈ [\n0, 1d+1\n]\n,\nr (\n1 d+1\n) · dd+1 · (t− 1), if t ∈ [ 1 d+1 , 1 ] .\nNotice that this function is still concave, and its corresponding ratio of integrals can not be greater than the one for r (since the same value gets subtracted from both integrals when switching from r to r̃).\nNext, consider the function\nr̂(t) = r\n(\n1\nd+ 1\n)\n· d d+ 1 · (t− 1), t ∈ [0, 1] .\nSince r̃ is concave, it is upper bounded by r̂ everywhere on [0, 1/(d + 1)]. Therefore, the ratio of integrals corresponding to r̂ can only decrease, compared to the one for r̃.\nFinally, the result follows from evaluating the integrals for r(t) = t− 1.\nA.3 Proof of Lemma 7.1\nSince the problem is invariant under rotations and translations, we can assume without loss of generality that z = 0, S = {e1, . . . , ek} and L = span{ek+1, . . . , en}. For every vector x we will consider the projections of x onto the two corresponding subspaces, xS = (x1, . . . , xk) and xL = (xk+1, . . . , xn). For simplicity, will also use the notation KL := ΠLK.\nThe proof consists of four steps. Step 1: the direction u has a large component in L. Since w(Cyl(K,S), u) ≥ ǫ, and z = 0 is\nthe centroid of the cylinder, there must exist y ∈ Cyl(K,S) such that ∣ ∣u⊤y ∣ ∣ = ∣ ∣u⊤(y − z) ∣ ∣ ≥ ǫ2 . Therefore ∣\n∣u⊤S yS ∣ ∣+ ∣ ∣u⊤LyL ∣ ∣ ≥ ǫ2 . Since the width of Cyl(K,S) is at most δ along all small directions, we have ‖yS‖∞ ≤ δ. Therefore, by Cauchy-Schwarz,\n‖uL‖ ‖yL‖ ≥ ∣ ∣ ∣ u⊤LyL ∣ ∣ ∣ ≥ ǫ\n2 − kδ .\nNow, remember that since y ∈ Cyl(K,S), K is contained inside the unit ball, and all the small directions have length at most δ, it must be that ‖y‖ ≤ 1 + kδ. Since this implies the same upper bound on ‖yL‖, combining with the bound above we see that\n‖uL‖ ≥ ǫ/2− kδ 1 + kδ ≥ ǫ/2− ǫ 2/(16(d + 1)) 1 + ǫ2/(16(d + 1)) ≥ ǫ 4 .\nStep 2: lower bound the width of KL along the direction of uL. Let ûL = uL/ ‖uL‖ be the unit vector in the direction uL. We know by the last step that\nw(KL, uL) ≥ ∣ ∣ ∣ û⊤LyL ∣ ∣ ∣ ≥ ∣ ∣ ∣ u⊤LyL ∣ ∣ ∣ ≥ ǫ 2 − kδ ≥ ǫ 4 .\nStep 3: show that for all x ∈ K+, one has û⊤LxL ≥ −ǫ/(4(d + 1)2). If x ∈ K+, then u⊤LxL + u⊤S xS ≥ 0. Since ‖xS‖∞ ≤ δ, we have u⊤LxL ≥ −kδ. Hence\nû⊤LxL ≥ − kδ ‖uL‖ ≥ −4dδ ǫ ≥ − ǫ 4(d+ 1)2 ,\nwhere we used the fact that δ ≤ ǫ2/(16d(d + 1)2). Step 4: upper bound the volume of ΠLK+. From the previous step, we know that if x ∈ ΠL(K+), then xL ∈ {xL ∈ KL|û⊤LxL ≥ −ǫ/(4(d + 1))}. Therefore:\nvol(ΠLK+) ≤ vol(KL)− vol ({ xL ∈ KL ∣ ∣ ∣\n∣\n(−ûL)⊤xL ≥ ǫ\n4(d+ 1)2\n}) ≤ vol(KL) · ( 1− 1 e2 ) ,\nwhere the first inequality follows from the previous step, since\nΠLK+ ⊆ { xL ∈ KL ∣ ∣ ∣\n∣\n(−ûL)⊤xL ≤ ǫ\n4(d+ 1)2\n}\n.\nThe second inequality follows from Lemma 5.5, since in Step 2 we showed that in this proof, we meet the conditions of that lemma. We note that it is very important that z is the centroid of Cyl(K,S) and not the centroid of K, since the application of Lemma 5.5 relies on the fact the projection of z onto the subspace L is the centroid of KL.\nA.4 Proof of Lemma 8.1\nLet zi = 1\nvol(∆(s))\n∫\n∆(s) xidx be the i-th component of the centroid of ∆(s). So if s−i is the vector\nin Rk−1 obtained from s by removing the i-th component, then the intersection of ∆(s) with the hyperplane xi = a can be written as: {x| xi = a, x−i ∈ ∆( s−i1−a/si )}. Therefore, we can write the integral defining zi as:\nzi = 1\nvol(∆(s))\n∫ si\n0 xivol\n(\n∆\n(\ns−i 1− xisi\n))\ndxi = 1\nvol(∆(s))\n∫ si\n0 xivol (∆ (s−i)) ·\n(\n1− xi si\n)k−1\ndxi\nsince scaling each coordinate a constant factor scales the volume by this constant powered to the number of dimensions. Solving this integral, we get:\nzi = vol(∆(s−i)) vol(∆(s)) · s\n2 i\nk(k + 1) .\nWe can apply the same trick to compute the volume:\nvol(∆(S)) =\n∫ si\n0 vol (∆ (s−i)) ·\n(\n1− xi si\n)k−1\ndxi = vol(∆(s−i)) · si k .\nSubstituting the volume vol(∆(S)) in zi we get zi = si\nk+1 .\nA.5 Proof of Lemma 8.2\nWe break the sequence of directions chosen by nature in three parts. We will show that the first part alone has regret O(k log(1ǫ )) and the other two parts will be used to bring the knowledge set to the desired format. We won’t specify the exact value of θ. We only assume that θ is an arbitrary point in the final knowledge set produced.\nStep 1 : Nature picks Ω(k log(1ǫ )) vectors in the direction ek, choosing the K+ side. The knowledge set is initially ∆(s)×[0, 1]d−k with centroid at (\ns k+1 , 1 2 , . . . , 1 2\n)\n. The set obtained\nby cutting through this point using a hyperplane orthogonal to ek can be described as {\nx ∈ Rd : xk ≥ sk\nk + 1 ,\nk ∑\ni=1\nxi si\n≤ 1, 0 ≤ xi ≤ 1 } ,\nwhich is, up to translation, equal to the set ∆((1 − 1k+1)s) × [0, 1]d−k . By applying such cuts Ω(k log(1ǫ )) we are left with a set ∆(ŝ)× [0, 1]d−k where 0 ≤ ŝ ≤ ǫ. Since we assumed that θ is in the last knowledge set while sk ≥ 2ǫ kk+1 we must be incurring one unit of regret, so we must have incurred at least Ω(k log(1ǫ )) regret.\nStep 2 : Nature picks a single vector in the direction v = (\nk+1 2k · 1ŝ1 , . . . , k+1 2k · 1ŝk , 1, 0, . . . , 0\n)\n,\nchoosing the K− side. Since the centroid is z = ( ŝk+1 , 1 2 , . . . , 1 2) the half-space defining K− is given by: v\n⊤x ≤ v⊤z = 1, therefore K− is described by:\nK− =\n{\nx ∈ Rd : k ∑\ni=1\nxi ŝi k + 1 2k + xk+1 ≤ 1,\nk ∑\ni=1\nxi ŝi\n≤ 1, 0 ≤ xi ≤ 1 }\nTo understand the shape of K− it is useful to decompose it in two parts based on the value of xk+1. Let y = 1− 12 k+1k which is a quantity between 0 and 12 .\n• for x ∈ K− with xk+1 ≥ y the constraint ∑k i=1 xi ŝi ≤ 1 is implied by ∑ki=1 xiŝi k+1 2k + xk+1 ≤ 1,\nsince we can re-write the second constraint as: ∑k i=1 xi ŝi (1−y) ≤ 1−xk+1 ≤ 1−y. This means in particular that {x ∈ K− : xk+1 ≥ y} is equal, up to translation to ∆(ŝ, 1−y)× [0, 1]d−k−1.\n• For x ∈ K− with xk+1 ≤ y then the constraint ∑k i=1 xi ŝi k+1 2k +xk+1 ≤ 1 is implied by ∑k i=1 xi ŝi ≤ 1 since\n∑k i=1 xi ŝi (1−y) ≤ 1−y ≤ 1−xk+1. In particular, this means that {x ∈ K− : xk+1 ≤ y}\nis the set ∆(ŝ)× [0, y] × [0, 1]d−k−1. Step 3 : Nature picks r vectors in direction ek+1 choosing the K+ side, where r will be decided later. After Step 2, the set is a concatentation of ∆(ŝ)× [0, y]× [0, 1]d−k−1 and ∆(ŝ, 1−y)× [0, 1]d−k−1 as displayed in Figure 4. By cutting in the ek+1 direction, we will eventually be left only with the ∆(ŝ, 1 − y) × [0, 1]d−k−1 part of the set. Pick r to be the minimum value such that this happens. Since the volume of the sections along the xk+1 dimension are non-increasing, the set after the cut must keep at least half of the width along ek+1. Therefore, after r cuts, we must be left with ∆(s′)× [0, 1]d−k−1 where s′ ∈ Rk+1 and 14 ≤ 1−y 2 ≤ s′k+1 ≤ 1− y."
    }, {
      "heading" : "B Polynomial Time Algorithm",
      "text" : "Correctness. Correctness of this algorithm follows from a simple modification of our original analysis. In order to tolerate the fact that the centroid produced by the sampling scheme is only\napproximate, we need to resort to the Approximate Grünbaum Theorem (see Lemma 5.5) in order to track the decrease in volume, and also to an approximate version of the Directional Grünbaum Theorem (see Lemma B.1 below), in order to argue that directional widths still do not decrease faster than they are supposed to.\nLemma B.1 (Approximate Directional Grünbaum). Let K be a convex body with centroid z. Let z′ be an approximate centroid in the sense that ‖z − z′‖ ≤ ρ. Then for every vector u 6= 0, the set K+ = {x|u⊤(x− z′) ≥ 0} satisfies\n1\nd+ 1 · w(K, v) − ρ ·max\n(\n1, w(K, v)\nw(K,u)\n)\n≤ w(K+, v) ≤ w(K, v)\nfor any unit vector v.\nProof sketch. The analysis follows from minor modifications in the analysis of Theorem 5.3. First we modify Lemma 5.4 in order to show that\n1\nd+ 1 w(K,u) − ρ ≤ w(K+, u) ≤ w(K,u) .\nIndeed, since ‖z − z′‖ ≤ ρ, taking a cut perpendicular to u that passes through z′ instead of z changes the directional width along u by at most ρ. Therefore the bound above holds in the worst case. Second, we consider the three cases considered in the proof. In the first two cases, we have w(K+, v) ≥ 1d+1 · w(K, v) − ρ via the previous bound. In the third case, let λ+ and λ− defined similarly. Then we have 1d+1w(K,u) − ρ ≤ λ+ · w(K,u) and similarly for λ−. Therefore min{λ+, λ−} ≥ 1d+1 − ρ w(K,u) . Finally, this yields\nw(K+, v) ≥ w(K, v) · ( 1 d+ 1 − ρ w(K,u) ) ,\nand our conclusion follows.\nSimilarly, we require a robust version of projected Grünbaum, which we sketch below.\nLemma B.2 (Approximate Projected Grünbaum). Let K be a convex set contained in the ball of radius 1, and let S be a set of orthonormal vectors along which w(K, s) ≤ δ ≤ ǫ2 32d(d+1)2 , for all s ∈ S. Let L be the subspace orthogonal to S, and let ΠL be the projection operator onto that subspace. If u is a direction along which w(Cyl(K,S), u) ≥ ǫ, z is the centroid of the cylindrified body Cyl(K,S), z′ satisfies ‖z − z′‖ ≤ ρ := ǫ\n8(d+1)2 , and K+ = {x ∈ K : u⊤(x− z′) ≥ 0}, then:\nvol(ΠLK+) ≤ ( 1− 1 e2 ) · vol(ΠLK) ,\nwhere vol(·) corresponds to the (n− |S|)-dimensional volume on the subspace L.\nProof sketch. The proof follows the same steps as the proof of Lemma 7.1. Below we sketch the essential differences, and show how they affect the analysis.\nThe first two steps are identical, since they do not involve the perturbed centroid z′. For the third step, we proceed identically to show that\nû⊤LxL ≥ − 4dδ ǫ ≥ − ǫ 8(d+ 1)2 ,\nwhere we used δ ≤ ǫ2 32d(d+1)2 .\nFinally, for the fourth step we use the fact that if x ∈ ΠL(K+) then xL ∈ {xL ∈ KL|û⊤LxL ≥ −ǫ/(8(d + 1)2)− ρ}. Hence\nΠLK+ ⊆ { xL ∈ KL ∣ ∣ ∣\n∣\n(−ûL)⊤xL ≤ ǫ\n4(d+ 1)2\n}\n.\nand thus we obtain the same bound on vol(ΠLK+).\nPutting everything together, we can show that the algorithm using these approximation primitives yields the same regret asymptotically. The constants we will use throughout the our algorithm will be δapprox = δ/( √ d(d + 1)) = ǫ2/(16d1.5(d + 1)3), and ρ = δ2approx/(2(d + 1)). The two key results required for our robust analysis are:\n1. If the cardinality of S does not increase, then\nvol(ΠLtKt+1) ≤ ( 1− 1/e2 ) vol(ΠLtKt) .\nThis is given by the approximate projected Grünbaum theorem (Lemma B.2).\n2. When adding an extra direction to S, we know that w(Kt, u) ≥ δapprox, for all u ∈ Lt. Then by Lemma B.1 after we cut Kt we have that for any vector v ∈ Lt,\nw(Kt+1, v) ≥ w(Kt, v)\nd+ 1 − ρ ·max\n(\n1, w(Kt, u)\nw(Kt, v)\n)\n≥ δapprox d+ 1 − ρ · 1 δapprox ≥ δapprox 2(d+ 1) ,\nby our choice of ρ = δ2approx/(2(d + 1)). So applying the Cylindrification Lemma (Lemma 6.1) we obtain that the volume of the convex body projected onto the new subspace of large directions L′ is bounded by\nvol(ΠL′Kt+1) ≤ d(d+ 1)\nδapprox/(2(d + 1)) vol(ΠLtKt+1) =\n32d1.5(d+ 1)3\nδ vol(ΠLtKt+1) .\nThis follows just like before from Lemma 6.1. Our method of finding thin directions based on the approximate John ellipsoid (Corollary 9.3) guarantees that all directional widths in the large subspace L are at least δapprox. Therefore the blow up in volume is at most by a factor of (32d1.5(d+ 1)3)/δ.\nSince all the new bounds are within polynomial factors from the ones used in the analysis using exact centroids, by plugging in the old analysis, we easily obtain the same regret, up to constant factors.\nRunning time. For the running time analysis, note that the centroid approximation can be implemented using Õ(d4/ρ) = (d/ǫ)O(1) calls to the separation oracle for the convex body. Such a separation oracle needs to take into account both the linear inequalities added during each iteration, and the at most d projections. Such an oracle can be implemented by maximizing a linear functional over a set determined by the intersection between the initial unit ball and the linear constraints (whose number is bounded by the number of iterations of the algorithm Õ(d log(1/ǫ)); therefore this step can be implemented in polynomial time, and therefore all the centroid approximation steps require time (d/ǫ)O(1).\nThe routine for finding the thin directions will be called at least once every iteration, and will find a thin direction at most d times. Therefore this contributes dO(1) log(R/r) · log(1/ǫ) to the running\ntime, where r is a lower bound on the smallest ball contained in the body, while R is an upper bound. From the setup we have R = 1; also, since we are finished after Õ(d log(1/ǫ)) iterations, and each iteration shrinks the smallest directional width by at most a factor of dO(1), according to Lemma 6.3, we have that at all times the body will contain a ball of radius d−Ω(d). Therefore the running time contribution of the routine required for finding thin directions is dO(1) log(1/ǫ).\nAll the other steps require at most polynomial overhead, therefore the total running time is (d/ǫ)O(1)."
    } ],
    "references" : [ {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E Schapire" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Repeated contextual auctions with strategic buyers",
      "author" : [ "Kareem Amin", "Afshin Rostamizadeh", "Umar Syed" ],
      "venue" : "In Proceedings of NIPS, pages 622–630,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Online decision-making with high-dimensional covariates",
      "author" : [ "Hamsa Bastani", "Mohsen Bayati" ],
      "venue" : "Working paper, Stanford University,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Solving convex programs by random walks",
      "author" : [ "Dimitris Bertsimas", "Santosh Vempala" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Feature-based Dynamic Pricing",
      "author" : [ "Maxime C. Cohen", "Ilan Lobel", "Renato Paes Leme" ],
      "venue" : "In Proceedings of the 2016 ACM Conference on Economics and Computation, EC ’16,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Geometric algorithms and combinatorial optimization, volume 2",
      "author" : [ "Martin Grötschel", "László Lovász", "Alexander Schrijver" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Partitions of mass-distributions and of convex bodies by hyperplanes",
      "author" : [ "Branko Grünbaum" ],
      "venue" : "Pacific Journal of Mathematics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1960
    }, {
      "title" : "Dynamic pricing in high-dimensions",
      "author" : [ "Adel Javanmard", "Hamid Nazerzadeh" ],
      "venue" : "arXiv preprint arXiv:1609.07574,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Hit-and-run mixes fast",
      "author" : [ "László Lovász" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Faster mixing via average conductance",
      "author" : [ "László Lovász", "Ravi Kannan" ],
      "venue" : "In Proceedings of the thirty-first annual ACM symposium on Theory of computing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Efficient methods in convex programming",
      "author" : [ "Arkadi Nemirovski" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Dynamic pricing with demand covariates",
      "author" : [ "Sheng Qiang", "Mohsen Bayati" ],
      "venue" : "Available at SSRN 2765257,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The binary search problem consists in trying to guess an unknown real number θ ∈ [0, 1] given access to an oracle that replies for every guess xt if xt ≤ θ or xt > θ.",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "We now mention two applications: Personalized Medicine [3]: Determining the right dosage of a drug for a given patient is a well-studied problem in the medical literature.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "Bastani and Bayati [3] propose a mathematical formulation for this problem and tackle it using tools from statistical learning and contextual bandits.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products.",
      "startOffset" : 22,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products.",
      "startOffset" : 22,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products.",
      "startOffset" : 22,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products.",
      "startOffset" : 22,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "Nevertheless, Cohen et al [5] showed that an algorithm for the multidimensional binary search problem can be converted into an algorithm for the feature-based pricing problem in a black-box manner.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "The first approach to this problem was due to Amin, Rostamizadeh and Syed [2] in the context of the pricing problem and is based on stochastic gradient descent.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "Subsequent approaches by Bastani and Bayati [3] and Qiang and Bayati [12] use techniques from statistical learning such as greedy least squares or LASSO.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "Subsequent approaches by Bastani and Bayati [3] and Qiang and Bayati [12] use techniques from statistical learning such as greedy least squares or LASSO.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Javanmard and Nazerzadeh [8] apply a regularized maximum likelihood estimation approach and obtain an improved regret guarantee.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "[1]) to tackle the iid version of the multidimensional binary search problem, but such an algorithm would have regret that is polynomial in 1/ǫ instead of the logarithmic regret obtained by the specialized algorithms.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "The only approach that makes no assumptions about the directions ut is by Cohen et al [5].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "1, we construct a lower bound of Ω(d log(1/ǫ √ d)) via a reduction to d one-dimensional problems, which is significantly lower than the O(d2 log(d/ǫ)) regret bound from Cohen et al [5].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "An idea similar to this one was proposed by Bertsimas and Vempala [4], in a paper where they proposed a method for solving linear programs via an approximate Grünbaum theorem.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "See Grünbaum [7] for the original proof of this theorem, or Nemirovski [11] for a more recent exposition.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "See Grünbaum [7] for the original proof of this theorem, or Nemirovski [11] for a more recent exposition.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "If K = ∆(y)× [0, 1] then there is a sequence of Ω(k log(1/ǫ)) directions ut such that the Centroid algorithm incurs Ω(k log(1ǫ )) regret and by the end of the sequence, the knowledge set has the form: K ′ = ∆(s)× [0, 1] where s ∈ Rk+1, 0 ≤ s′i ≤ ǫ for i < k + 1, 1 4 ≤ s′i ≤ 1.",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "If K = ∆(y)× [0, 1] then there is a sequence of Ω(k log(1/ǫ)) directions ut such that the Centroid algorithm incurs Ω(k log(1ǫ )) regret and by the end of the sequence, the knowledge set has the form: K ′ = ∆(s)× [0, 1] where s ∈ Rk+1, 0 ≤ s′i ≤ ǫ for i < k + 1, 1 4 ≤ s′i ≤ 1.",
      "startOffset" : 213,
      "endOffset" : 219
    }, {
      "referenceID" : 0,
      "context" : "Start with the set K0 = [0, 1] d.",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "We did not do our computations above using this scaled down instance instead of [0, 1]d in order to avoid carrying extra √ d terms.",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "1 Approximating the Centroid An approximation of the centroid sufficient for our purposes follows from a simple application of standard algorithms for sampling points from convex bodies (hit-and-run [9], ball-walk [10]).",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "1 Approximating the Centroid An approximation of the centroid sufficient for our purposes follows from a simple application of standard algorithms for sampling points from convex bodies (hit-and-run [9], ball-walk [10]).",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 3,
      "context" : "similar application can be found in Bertsimas and Vempala [4], where the authors use approximate centroid computation in order to solve linear programs.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "Our application faces the same issues as in [4].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "The many issues encountered when approximating the centroid are carefully handled in [4], so we will restate the following result which is implicit there (see Lemma 5 and Theorem 12): Theorem 9.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "1 ([4]).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "Such a result is provided in Grötschel et al [6], which we reproduce below for completeness (see Corollary 4.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "2 ([6]).",
      "startOffset" : 3,
      "endOffset" : 6
    } ],
    "year" : 2016,
    "abstractText" : "We consider a multidimensional search problem that is motivated by questions in contextual decision-making, such as dynamic pricing and personalized medicine. Nature selects a state from a d-dimensional unit ball and then generates a sequence of d-dimensional directions. We are given access to the directions, but not access to the state. After receiving a direction, we have to guess the value of the dot product between the state and the direction. Our goal is to minimize the number of times when our guess is more than ǫ away from the true answer. We construct a polynomial time algorithm that we call Projected Volume achieving regret O(d log(d/ǫ)), which is optimal up to a log d factor. The algorithm combines a volume cutting strategy with a new geometric technique that we call cylindrification.",
    "creator" : "LaTeX with hyperref package"
  }
}