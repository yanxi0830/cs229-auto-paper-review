{
  "name" : "1606.08561.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Estimating the class prior and posterior from noisy positives and unlabeled data",
    "authors" : [ "Shantanu Jain", "Martha White", "Predrag Radivojac" ],
    "emails" : [ "predrag}@indiana.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Access to positive, negative and unlabeled examples is a standard assumption for most semisupervised binary classification techniques. In many domains, however, a sample from one of the classes (say, negatives) may not be available, leading to the setting of learning from positive and unlabeled data (Denis et al., 2005). Positive-unlabeled learning often emerges in sciences and commerce where an observation of a positive example (say, that a protein catalyzes reactions or that a Facebook user likes a particular product) is usually reliable. Here, however, the absence of a positive observation cannot be interpreted as a negative example. In molecular biology, for example, an attempt to label a data point as positive (say, that a protein is an enzyme) may be unsuccessful for a variety of experimental and biological reasons, whereas in social networks an explicit dislike of a product may not be possible. Both scenarios lead to a situation where negative examples cannot be actively collected.\nIf the class priors in the unlabeled data are known, positive-unlabeled learning can straightforwardly translate into learning of non-traditional classifiers (Elkan and Noto, 2008); i.e., classifiers that discriminate between labeled and unlabeled data. These non-traditional classifiers can then be used to compute the outputs of traditional classifiers; i.e., those that discriminate between positive and negative data (Jain et al., 2016). Under mild assumptions, even when the class priors are unknown, there exists a monotonic relationship between the outputs of these classifiers (Jain et al., 2016) and, hence, the models trained for information retrieval and ranking generally do not suffer when trained on labeled vs. unlabeled data. Unfortunately, the monotonic relationship is non-linear, suggesting that in the absence of class prior knowledge, a non-traditional classifier that learns posterior class distributions cannot be used to infer the posterior distributions of the traditional classifiers. Therefore, a fundamental yet ill-posed problem in these situations is that of learning class priors of positive and negative examples in unlabeled data.\nar X\niv :1\n60 6.\n08 56\n1v 1\n[ st\nat .M\nL ]\n2 8\nJu n\nClass prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Jain et al., 2016). Application of these algorithms to real data, however, is limited in that none of the proposed algorithms simultaneously deals with noise in the labels and practical estimation for high-dimensional data. Much of the theory on learning class priors rely on the assumption that either the distribution of positives is known or that the positive sample is clean. In practice, however, labeled data sets contain class-label noise, where an unspecified amount of negative examples contaminates the positive sample. This is a realistic scenario in experimental sciences where technological advances enabled generation of high-throughput data at a cost of occasional errors. One example for this comes from the studies of proteins using analytical chemistry technology; i.e., mass spectrometry. For example, in the process of peptide identification (Steen and Mann, 2004), bioinformatics methods are usually set to report results with specified false discovery rate thresholds (e.g., 1%). Unfortunately, statistical assumptions in these experiments are sometimes violated thereby leading to substantial noise in reported results, as in the case of identifying protein post-translational modifications. Similar amounts of noise might appear in social networks such as Facebook, where some users select ‘like’, even when they do not actually like a particular post. Further, the only approach that does consider similar such noise (Scott et al., 2013) requires density estimation, which is known to be problematic for high-dimensional data.\nIn this work, we propose the first classification algorithm, with class prior estimation, designed particularly for high-dimensional data with noise in the labeling of positive data. We first formalize the problem of class prior estimation from noisy positive and unlabeled data. We extend the existing identifiability theory for class prior estimation from positive-unlabeled data to this noise setting. We then show that we can practically estimate class priors and the posterior distributions by first transforming the input space to a univariate space, where density estimation is reliable. We prove that these transformations preserve class priors and show that they correspond to training a nontraditional classifier. We derive a parametric algorithm and a nonparametric algorithm to learn the class priors. Finally, we carry out experiments on synthetic and real-life data and provide evidence that the new approaches are sound and effective."
    }, {
      "heading" : "2 Problem formulation",
      "text" : "Consider a binary classification problem of mapping an input spaceX to an output spaceY = {0, 1}. Let f be the true distribution of inputs. It can be represented as the following mixture\nf(x) = αf1(x) + (1− α)f0(x), (1)\nwhere x ∈ X , y ∈ Y , fy are distributions over X for the positive (y = 1) and negative (y = 0) class, respectively; and α ∈ [0, 1) is the class prior or the proportion of the positive examples in f . We will refer to a sample from f as unlabeled data.\nLet now g be the distribution of inputs for the labeled data. Because the labeled sample contains some mislabeled examples, the corresponding distribution is also a mixture of f1 and a small proportion, say 1− β, of f0. That is,\ng(x) = βf1(x) + (1− β)f0(x), (2)\nwhere β ∈ (0, 1]. Observe that both mixtures have the same components but different mixing proportions. The simplest scenario is that the mixing components f0 and f1 correspond to the classconditional distributions p(x|Y = 0) and p(x|Y = 1), respectively. However, our approach also permits transformations of the input space X thus resulting in a more general setup. The objective of this work is to study the estimation of the class prior α = p(Y = 1) and propose practical algorithms for estimating α. The efficacy of this estimation is clearly tied to β, where as β gets smaller, the noise in the positive labels becomes larger. We will discuss identifiability of α and β and give a practical algorithm for estimating α (and β). We will then use these results to estimate the posterior distribution of the class variable, p(y|x), although the labeled set does not contain any negative examples."
    }, {
      "heading" : "3 Identifiability",
      "text" : "The class prior is identifiable if there is a unique class prior for a given pair (f, g). Much of the identifiability characterization in this section has already been considered as the case of asymmetric noise (Scott et al., 2013); see Section 7 on related work. We recreate these results here, with the aim to introduce required notation, to highlight several important results for later algorithm development and to include a few missing results needed for our approach. Though the proof techniques are themselves quite different and could be of interest, we include them in the appendix due to space.\nThere are typically two aspects to address with identifiability. First, one needs to determine if a problem is identifiable, and, second, if it is not, propose a canonical form that is identifiable. In this section we will see that class prior is not identifiable in general because f0 can be a mixture containing f1 and vice versa. To ensure identifiability, it is necessary to choose a canonical form that prefers a class prior that makes the two components as different as possible; this canonical form was independently introduced as the mutual irreducibility principle (Scott et al., 2013) or the max-canonical form (Jain et al., 2016).\nWe discuss identifiability in terms of measures. Let µ, ν, µ0 and µ1 be probability measures defined on some σ-algebra A on X , corresponding to f , g, f0 and f1, respectively. It follows that\nµ = αµ1 + (1− α)µ0 (3) ν = βµ1 + (1− β)µ0 (4)\nConsider a family of pairs of mixtures having the same components:\nF(Π) = {(µ, ν) : µ = αµ1 + (1− α)µ0, ν = βµ1 + (1− β)µ0, (µ0, µ1) ∈ Π, 0 ≤ α < β ≤ 1}, where Π is some set of pairs of probability measures defined on A. The family is parametrized by the quadruple (α, β, µ0, µ1). The condition β > α means that ν has a greater proportion of µ1 compared to µ. This is consistent with our assumption that the labeled sample mainly contains positives. The most general choice for Π is\nΠall = Pall × Pall \\ { (µ, µ) : µ ∈ Pall } ,\nwhere Pall is the set of all probability measures defined on A and { (µ, µ) : µ ∈ Pall }\nis the set of pairs that are equal. Removing equal pairs prevents µ and ν from being identical.\nWe now define the maximum proportion of a component λ1 in a mixture λ, which is used in the results below and to specify the criterion that enables identifiability; more specifically\naλ1λ = max { α : λ = αλ1 + (1− α)λ0, λ0 ∈ Pall, α ∈ [0, 1] } . (5)\nOf particular interest is the case when aλ1λ = 0, which should be read as “λ is not a mixture containing λ1”. We finally define the set all possible (α, β) that generate µ and ν when (µ0, µ1) varies in Π:\nA+(µ, ν,Π) = {(α, β) : µ = αµ1 + (1− α)µ0, ν = βµ1 + (1− β)µ0, (µ0, µ1) ∈ Π, 0 ≤ α < β ≤ 1}. If A+(µ, ν,Π) is a singleton set for all (µ, ν) ∈ F(Π), then F(Π) is identifiable in (α, β). First, we show that the most general choice for Π, Πall, leads to unidentifiability (Lemma 1). Fortunately, however, by choosing\nΠres = { (µ0, µ1) ∈ Πall : aµ1µ0 = 0, aµ0µ1 = 0 }\nas Π, we do obtain identifiability (Theorem 1). In words, Πres contains pairs of distributions, where each distribution in a pair cannot be expressed as a mixture containing the other. The proofs of the results below are in the Appendix.\nLemma 1 (Unidentifiability) Given a pair of mixtures (µ, ν) ∈ F(Πall), let parameters (α, β, µ0, µ1) generate (µ, ν) and α+ = aνµ, β + = aµν . It follows that\n1. There is a one-to-one relation between (µ0, µ1) and (α, β) and\nµ0 = βµ− αν β − α , µ1 = (1− α)ν − (1− β)µ β − α . (6)\n2. Both expressions on the right-hand side of Equation 6 are well defined probability measures if and only if α/β ≤ α+ and (1−β)/(1−α) ≤ β+.\n3. A+(µ, ν,Πall) = {(α, β) : α/β ≤ α+, (1−β)/(1−α) ≤ β+}. 4. F(Πall) is unidentifiable in (α, β); i.e., (α, β) is not uniquely determined from (µ, ν). 5. F(Πall) is unidentifiable in both α and β; i.e., Individually, α and β are not uniquely\ndetermined from (µ, ν).\nObserve that the definition of aλ1λ and µ 6= ν imply α+ < 1 and, consequently, any (α, β) ∈ A+(µ, ν,Πall) satisfies α < β, as expected.\nTheorem 1 (Identifiablity) Given (µ, ν) ∈ F(Πall), let α+ = aνµ and β+ = aµν . Let µ∗0 = (µ−α+ν)/(1−α+), µ∗1 = (ν−β +µ)/(1−β+) and\nα∗ = α +(1−β+)/(1−α+β+), β∗ = (1−β +)/(1−α+β+). (7)"
    }, {
      "heading" : "It follows that",
      "text" : "1. (α∗, β∗, µ∗0, µ ∗ 1) generate (µ, ν)\n2. (µ∗0, µ ∗ 1) ∈ Πres and consequently, α∗ = a µ∗1 µ , β∗ = a µ∗1 ν .\n3. F(Πres) contains all pairs of mixtures in F(Πall). 4. A+(µ, ν,Πres) = {(α∗, β∗)}. 5. F(Πres) is identifiable in (α, β); i.e., (α, β) is uniquely determined from (µ, ν).\nWe refer to the expressions of µ and ν as mixtures of components µ0 and µ1 as a max-canonical form when (µ0, µ1) is picked from Πres. This form enforces that µ1 is not a mixture containing µ0 and vice versa, which leads to µ0 and µ1 having maximum separation, while still generating µ and ν. Each pair of distributions in F(Πres) is represented in this form. Identifiability of F(Πres) in (α, β), precisely, A+(µ, ν,Πres) = {(α∗, β∗)} tells that (α∗, β∗) is the only pair of mixing proportions that can appear in a max-canonical form of µ and ν. Moreover, Theorem 1 statement 1 and Lemma 1 statement 1 imply that the max-canonical form is unique and completely specified by (α∗, β∗, µ∗0, µ ∗ 1), with α\n∗ < β∗ following from Equation 7. Thus, using F(Πres) to model the unlabeled and labeled data distributions makes estimation of not only α, the class prior, but also β, µ0, µ1 a well-posed problem. Moreover, due to Theorem 1 statement 3, there is no loss in the modeling capability by using F(Πres) instead of F(Πall). Overall, identifiability, absence of loss of modeling capability and maximum separation between µ0 and µ1 combine to justify estimating α∗ as the class prior."
    }, {
      "heading" : "4 Univariate Transformation",
      "text" : "The theory and algorithms for class prior estimation are agnostic to the dimensionality of the data; in practice, however, this dimensionality can have important consequences. Parametric Gaussian mixture models trained via expectation-maximization (EM) are known to strongly suffer from colinearity in high-dimensional data. Nonparametric (kernel) density estimation is also known to have curse-of-dimensionality issues, both in theory (Liu et al., 2007) and in practice (Scott, 2008).\nWe address the curse of dimensionality by transforming the data to a single dimension. The transformation τ : X → R, surprisingly, is simply an output of a non-traditional classifier trained to separate labeled sample, L, from unlabeled sample, U . The transform is similar to that in (Jain et al., 2016), except that it is not required to be calibrated like a posterior distribution; as shown below, a good ranking function is sufficient. First, however, we introduce notation and formalize the data generation steps (Figure 1).\nLet X be a random variable taking values in X , capturing the true distribution of inputs, µ, and Y be an unobserved random variable taking values in Y , giving the true class of the inputs. It follows that X|Y = 0 and X|Y = 1 are distributed according to µ0 and µ1, respectively. Let S be a selection random variable, whose value in S = {0, 1, 2} determines the sample to which an input x is added (Figure 1). When S = 1, x is added to the noisy labeled sample; when S = 0, x is added to the unlabeled sample; and when S = 2, x is not added to either of the samples. It follows that\nXu = X|S = 0 and X l = X|S = 1 are distributed according to µ and ν, respectively. We make the following assumptions which are consistent with the statements above:\np(y|S = 0) = p(y), (8) p(y = 1|S = 1) = β, (9) p(x|s, y) = p(x|y). (10)\nAssumptions 8 and 9 tell that the proportion of positives in the unlabeled sample and the labeled sample matches the true proportion in µ and ν, respectively. Assumption 10 tells that the distribution of the positive inputs (and the negative inputs) in both the unlabeled and the labeled samples is equal and unbiased. Lemma 2 gives the implications of these assumptions. Statement 3 is particularly interesting and perhaps counter-intuitive as it tells that with non-zero probability some inputs need to be dropped.\nLemma 2 Let X , Y and S be random variables taking values in X , Y and S, respectively, and Xu = X|S = 0 and X l = X|S = 1. For measures µ, ν, µ0, µ1, satisfying Equations 3 and 4 and µ1 6= µ0, let µ, µ0, µ1 give the distribution of X , X|Y = 0 and X|Y = 1, respectively. If X,Y and S satisfy assumptions 8, 9 and 10, then\n1. X is independent of S = 0; i.e., p(x|S = 0) = p(x) 2. Xu and X l are distributed according to µ and ν, respectively. 3. p(S = 2) 6= 0.\nThe proof is in the Appendix. Next, we highlight the conditions under which the score function τ preserves α∗. Observing that S serves as the pseudo class label for labeled vs. unlabeled classification as well, we first give an expression for the posterior:\nτp(x) = p(S = 1|x, S ∈ {0, 1}), ∀x ∈ X . (11)\nTheorem 2 (α∗-preserving transform) Let random variables X,Y, S,Xu, X l and measures µ, ν, µ0, µ1 be as defined in Lemma 2. Let τp be the posterior as defined in Equation 11 and τ = H ◦ τp, where H is a 1-to-1 function on [0, 1] and ◦ is the composition operator. Assume\n1. (µ0, µ1) ∈ Πres, 2. Xu, X l are continuous with densities f, g, respectively, 3. µτ , ντ , µτ1 are the measures corresponding to τ(Xu), τ(X l), τ(X1), respectively, 4. (α+, β+, α∗, β∗) = (aνµ, a µ ν , a µ1 µ , a µ1 ν ) and (α + τ , β + τ , α ∗ τ , β ∗ τ ) = (a ντ µτ , a µτ ντ , a µτ1 µτ , a µτ1 ντ ).\nThen (α+τ , β + τ , α ∗ τ , β ∗ τ ) = (α +, β+, α∗, β∗) and so τ is an α∗-preserving transformation.\nMoreover, τp can also be used to compute the true posterior probability:\np(Y = 1|x) = α ∗(1− α∗) β∗ − α∗\n( p(S = 0)\np(S = 1)\nτp(x) 1− τp(x) − 1− β ∗ 1− α∗ ) . (12)\nThe proof is in the Appendix. Theorem 2 shows that using any function τ , that can be expressed as a composition of a one-to-one function, H , defined on [0, 1], with τp preserves α∗. Trivially, τp itself is one such function. We emphasize, however, that α∗-preservation is not limited by the efficacy of the calibration algorithm; uncalibrated scoring that ranks inputs as τp(x) does, also preserves α∗. Theorem 2 further demonstrates how the true posterior, p(Y = 1|x), can be recovered from τp by plugging in estimates of τp, p(S=0)/p(S=1), α∗ and β∗ in Equation 12. The posterior probability τp can be estimated directly by using a probabilistic classifier or by calibrating a classifier’s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of α∗ and β∗."
    }, {
      "heading" : "5 Algorithms",
      "text" : "In this section, we derive a parametric and a nonparametric algorithm to estimate α∗ and β∗ from the unlabeled sample, U = {Xui }, and the noisy positive sample, L = {X li}. In theory, both approaches can handle multivariate samples; in practice, however, to circumvent the curse of dimensionality, we exploit the theory of α∗-preserving univariate transforms to transform the samples.\nParametric approach. The parametric approach is derived by modeling each sample as a two component Gaussian mixture, sharing the same components but having different mixing proportions:\nXui ∼ αN (u1,Σ1) + (1− α)N (u0,Σ0) X li ∼ βN (u1,Σ1) + (1− β)N (u0,Σ0)\nwhere u1, u0 ∈ Rd and Σ1,Σ0 ∈ Sd++, the set of all d×d positive definite matrices. The algorithm is an extension to the EM approach for Gaussian mixture models (GMMs) where, instead of estimating the parameters of a single mixture, the parameters of both mixtures (α, β, u0, u1,Σ0,Σ1) are estimated simultaneously by maximizing the combined likelihood over both U and L. This approach, that we refer to as a multi-sample GMM (MSGMM), exploits the constraint that the two mixtures share the same components. The update rules and their derivation are given in the Appendix.\nNonparametric approach. Our nonparametric strategy directly exploits the results of Lemma 1 and Theorem 1 stating the direct connection between (α+ = aνµ, β + = aµν ) and (α ∗, β∗). Therefore, for a two-component mixture sample, M , and a sample from one of the components, C, it only requires an algorithm to estimate the maximum proportion of C in M . For this purpose, we use the AlphaMax algorithm (Jain et al., 2016), briefly summarized in the Appendix. Specifically, our two-step approach for estimating α∗ and β∗ is as follows: (i) Estimate α+ and β+ as outputs of AlphaMax(U,L) and AlphaMax(L,U), respectively; (ii) Estimate (α∗, β∗) from the estimates of (α+, β+) by applying Equation 7. We refer to our nonparametric algorithm as AlphaMax-N."
    }, {
      "heading" : "6 Empirical investigation",
      "text" : "In this section we systematically evaluate the new algorithms in a controlled, synthetic setting as well as on a variety of data sets from the UCI Machine Learning Repository (Lichman, 2013).\nExperiments on synthetic data: We start by evaluating all algorithms in a univariate setting where both mixing proportions, α and β, are known. We generate unit-variance Gaussian and unit-scale Laplace distributed i.i.d. samples and explore impact on the accuracy of estimation of mixing proportion, size of the component sample, and separation and overlap between the mixing components. The class prior α was varied from {0.05, 0.25, 0.50, 0.75, 0.95} and the noise component β from {1.00, 0.95, 0.75}. The size of the labeled sample L was varied from {100, 1000}, whereas the size of the unlabeled sample U was fixed at 10000.\nExperiments on real-life data: We considered twelve real-life data sets from the UCI Machine Learning Repository. To adjust these data to our problems, categorical features were transformed into numerical using sparse binary representation, the regression data sets were transformed into classification based on mean of the target variable, and the multi-class classification problems were converted into binary problems by combining classes. In each data set, a subset of positive and negative examples was randomly selected to provide a labeled sample while the remaining data (without class labels) were used as unlabeled data. The size of the labeled sample was kept at 1000 (or 100 for small data sets) and the maximum size of unlabeled data was set 10000.\nAlgorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al., 2016). There are several versions of the Elkan-Noto estimator and each can use any underlying classifier. We used the e1 alternative estimator combined with the ensembles of 100 two-layer feed-forward neural networks, each with five hidden units. This same classifier was used as a class-prior preserving transformation that created an input to the AlphaMax algorithms. The algorithm proposed by du Plessis and Sugiyama (2014) minimizes the same objective as the e1 Elkan-Noto estimator and, thus, was not implemented. It is important to mention that neither Elkan-Noto nor AlphaMax algorithms were developed to handle noisy labeled samples. In addition, the theory behind the Elkan-Noto estimator restricts its use to class-conditional distributions with non-overlapping supports.\nEvaluation: All experiments were repeated 50 times to be able to draw conclusions with statistical significance. In real-life data, the labeled sample was created randomly by choosing an appropriate number of positive and negative examples to satisfy the condition for β and the size of the labeled sample, while the remaining data was used to determine α in the unlabeled sample. Therefore, the class prior in the unlabeled data varies with the selection of the noise parameter β. The mean absolute difference between the true and estimated class priors was used as a performance measure. The best performing algorithm on each data set was determined by multiple hypothesis testing using the P-value of 0.05 and Bonferroni correction.\nResults: The comprehensive results for synthetic data drawn from univariate Gaussian and Laplace distributions are shown in Appendix (Table 2). In these experiments no transformation was applied prior to running any of the algorithms. As expected, the results show excellent performance of the MSGMM model on the Gaussian data. These results significantly degrade on Laplace-distributed data, suggesting sensitivity to the underlying assumptions. On the other hand, AlphaMax-N was accurate over all data sets and also robust to noise. These results suggest that new parametric and nonparametric algorithms perform well in these controlled settings.\nTable 1 shows the results on twelve real data sets. Here, AlphaMax and AlphaMax-N algorithms demonstrate significant robustness to noise, although the parametric version MSGMM was competitive in some cases. On the other hand, the Elkan-Noto algorithm expectedly degrades with noise. Finally, we investigated the practical usefulness of the α∗-preserving transform. Table 3 (Appendix) shows the results of AlphaMax-N and MSGMM on the real data sets, with and without using the transform. Because of computational and numerical issues, we reduced the dimensionality by using principal component analysis (the original data caused matrix singularity issues for MSGMM and density estimation issues for AlphaMax-N). MSGMM deteriorates significantly without the transform, whereas AlphaMax-N preserves some signal for the class prior. AlphaMax-N with the transform, however, shows superior performance on most data sets."
    }, {
      "heading" : "7 Related work",
      "text" : "Class prior estimation in a semi-supervised setting including positive-unlabeled learning, has been extensively discussed previously; see Saerens et al. (2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein. Recently, a general setting for label noise has also been introduced, called the mutual contamination model. The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016). The setting of asymmetric label noise is a subset of this more general setting, treated under general conditions by Scott et al. (2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998). A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise. Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class. As the most related work, though Scott et al. (2013) did not explicitly treat the positive-unlabeled learning with noisy positives, their formulation can incorporate this setting by using π0 = α and β = 1 − π1. The theoretical and algorithmic treatment, however, is very different. Their focus is on identifiability and analyzing convergence rates and statistical properties, assuming access to some κ∗ function which can obtain proportions\nbetween samples. They do not explicitly address issues with high-dimensional data, nor focus on algorithms to obtain κ∗. In contrast, we focus primarily on the univariate transformation to handle high-dimensional data and practical algorithms for estimating α∗. Supervised learning used for class prior-preserving transformation provides a rich set of techniques to address high-dimensional data."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we developed a practical algorithm for classification of positive-unlabeled data with noise in the labeled data set. In particular, we focused on a strategy for high-dimensional data, providing a univariate transform that reduces the dimension of the data, preserves the class prior so that estimation in this reduced space remains valid and is then further useful for classification. This approach provides a simple algorithm that simultaneously improves estimation of the class prior and provides a resulting classifier. We derived a parametric and a nonparametric algorithm and then demonstrated performance on a wide variety of learning scenarios and data sets. To the best of our knowledge, this algorithm represents one of the first practical and easy-to-use approaches to learning with high-dimensional positive-unlabeled data with noise in the labels."
    }, {
      "heading" : "It follows that",
      "text" : "1. (α∗, β∗, µ∗0, µ ∗ 1) generate (µ, ν)\n2. (µ∗0, µ ∗ 1) ∈ Πres and consequently, α∗ = a µ∗1 µ , β∗ = a µ∗1 ν .\n3. F(Πres) contains all pairs of mixtures in F(Πall). 4. A+(µ, ν,Πres) = {(α∗, β∗)}. 5. F(Πres) is identifiable in (α, β); i.e., (α, β) is uniquely determined from (µ, ν).\nProof: Throughout the proof, we use an alternate characterization of aλ1λ given by Theorem 3 (statement 1) as\naλ1λ = inf R(λ, λ1),\nwhere R(λ, λ1) = {λ(A)/λ1(A) : A ∈ A, λ1(A) > 0}. Statement 1: First, we show that µ∗0, µ ∗ 1 are well defined probability measure. α+ 6= 1, β+ 6= 1: Suppose α+ = 1. It follows that µ = ν and consequently, from Equation 6, µ0 = µ1 = ν. However, µ0 6= µ1 because they are picked from Πall. Thus α+ 6= 1 by contradiction. Similarly β+ 6= 1. Thus the denominator in R.H.S of µ∗0 and µ∗1 is not 0. Probability measure: By definition α+ = inf R(µ, ν). Thus α+ ≤ µ(A)/ν(A) when ν(A) > 0. Consequently, µ(A) − α+ν(A) ≥ 0. The inequality is trivially true when ν(A) = 0. Thus µ(A) − α+ν(A) ≥ 0 for all A ∈ A. Hence µ∗0 is a probability measure. Similarly µ∗1 is also a probability measure. Second, we show that (α∗, β∗, µ∗0, µ ∗ 1) generate µ, ν. (α∗, β∗, µ∗0, µ ∗ 1) → (µ, ν): Observe that µ∗0, µ∗1 can also be expressed as µ∗0 = β∗µ−α∗ν β∗−α∗ and µ∗1 = (1−α∗)ν−(1−β∗)µ\nβ∗−α∗ . Moreover, after some algebraic manipulation of equations labeled 7, α∗/β∗ = α+ and (1−β∗)/(1−α∗) = β+ can be derived. Thus, from Lemma 1 (statements 1 and 3), (α∗, β∗, µ∗0, µ ∗ 1) generate µ, ν. Statement 2: a µ∗0 µ∗1 = 0 : Suppose, for some > 0 and\nν(A)− β+µ(A) ≥ (µ(A)− α+ν(A)) (for all A ∈ A) ⇒ (1 + α+)ν(A) ≥ ( + β+)µ(A) ⇒ ν(A)/µ(A) ≥ ( +β+)/(1+ α+) (when µ(A) > 0)\nThus ( +β+)/(1+ α+) is a lower bound toR(ν, µ) and consequently, β+ ≥ ( +β+)/(1+ α+). However, because α+β+ < 1, ( +β+)/(1+ α+) > ( α+β++β+)/(1+ α+) = β+. This is a contradiction. Thus for all > 0 there exists some A ∈ A such that ν(A ) − β+µ(A ) < (µ(A ) − α+ν(A )). We now divide the inequality on both sides by µ(A ) − α+ν(A ), observing that the divisor is strictly greater than 0 because ν(A )− β+µ(A ) ≥ 0 as µ∗1 is a probability measure. Thus\n(ν(A )−β+µ(A ))/(µ(A )−α+ν(A )) <\n⇒ µ∗1(A )/µ∗0(A ) < (1−α+)/(1−β+). Because the choice of is arbitrary and β+ 6= 1, any lower bound of R(µ∗1, µ∗0) cannot be greater than 0. Thus aµ ∗ 0\nµ∗1 = 0.\na µ∗1 µ∗0 = 0 : Suppose, for some > 0\nµ(A)− α+ν(A) ≥ (ν(A)− β+µ(A)) (for all A ∈ A) ⇒ (1 + β+)µ(A) ≥ ( + α+)ν(A) ⇒ µ(A)/ν(A) ≥ ( +α+)/(1+ β+) (when ν(A) > 0)\nThus ( +α+)/(1+ β+) is a lower bound toR(µ, ν) and consequently, α+ ≥ ( +α+)/(1+ β+). However, because α+β+ < 1, ( +α+)/(1+ β+) > ( β+α++α+)/(1+ β+) = α+. This is a contradiction. Thus for all > 0 there exists some A ∈ A such that µ(A ) − α+ν(A ) < (ν(A ) − β+µ(A )). We now divide the inequality on both sides by ν(A ) − β+µ(A ), observing that the divisor is strictly greater than 0 because µ(A )− α+ν(A ) ≥ 0 as µ∗0 is a probability measure. Thus\n(µ(A )−α+ν(A ))/(ν(A )−β+µ(A )) <\n⇒ µ∗0(A )/µ∗1(A ) < (1−β+)/(1−α+).\nBecause the choice of is arbitrary and α+ 6= 1, any lower bound of R(µ∗0, µ∗1) cannot be greater than 0. Thus aµ ∗ 1\nµ∗0 = 0.\nSummarizing, aµ ∗ 0\nµ∗1 = 0 and aµ\n∗ 1\nµ∗0 = 0; consequently, (µ∗0, µ ∗ 1) ∈ Πres. Because µ∗0 is not a mixture\ncontaining µ∗1 and µ = α ∗µ∗1 + (1 − α∗)µ∗0, α∗ = a µ∗1 µ from Theorem 3 (statement 2). Similarly, β∗ = a µ∗1 ν . This concludes the proof of statement 2. Statement 3: Because statements 1 and 2 are true for any (µ, ν) ∈ F(Πall), statement 3 is true. Statement 4: It follows from statements 1 and 2, that (α∗, β∗) ∈ A+(µ, ν,Πres). To show that A+(µ, ν,Πres) contains no other element, we give a proof by contradiction. Suppose (α, β) ∈ A+(µ, ν,Πres) and (α, β) 6= (α∗, β∗). Let (α, β, µ0, µ1) generate (µ, ν), for some (µ0, µ1) ∈ Πres. First, we show that α+ = α/β and β+ = (1−β)/(1−α): α+ = α/β: Suppose for some 0 < < (β−α)/β(1−β) and all A ∈ A where ν(A) > 0,\nµ(A)/ν(A) ≥ α/β +\n⇒ α+ (1− α) µ0(A)/µ1(A) β + (1− β)µ0(A)/µ1(A) ≥ α+ β β\n⇒ µ0(A)/µ1(A) ≥ β2 /(β−α− β(1−β)) ⇒ aµ1µ0 ≥ β 2 /(β−α− β(1−β)) > 0\nHowever, this is a contradiction because (µ0, µ1) ∈ Πres. Thus for every 0 < < (β−α)/β(1−β) there exists some A ∈ A with ν(A ) > 0 such that µ(A )/ν(A ) < α/β + . Thus α+ < α/β + , Because can be made arbitrarily small, α+ ≤ α/β. However, because A+(µ, ν,Πres) ⊆ A+(µ, ν,Πall), (α, β) also belongs to A+(µ, ν,Πall) and consequently α+ ≥ α/β from Lemma 1 (statement 3). Thus α+ = α/β. β+ = (1−β)/(1−α): The proof is similar to α+ = α/β —supposing ν(A)/µ(A) ≥ (1−β)/(1−α)+ for some > 0 and all A ∈ A with µ(A) > 0, reaching a contradiction and following the subsequent steps. α+ = α/β, β+ = (1−β)/(1−α) and Equation 7 implies α = α∗ and β = β∗, which contradicts our assumption. Hence (α∗, β∗) is the only element in A+(µ, ν,Πres), which proves statement 4. Statement 5: Statement 5 follows by observing that A+(µ, ν,Πres) is a singleton set.\nLemma 2 Let X , Y and S be random variables taking values in X , Y and S , respectively, and Xu = X|S = 0 and X l = X|S = 1. For measures µ, ν, µ0, µ1, satisfying Equations 3 and 4 and µ1 6= µ0, let µ, µ0, µ1 give the distribution of X , X|Y = 0 and X|Y = 1, respectively. If X,Y and S satisfy assumptions 8, 9 and 10, then\n1. X is independent of S = 0; i.e., p(x|S = 0) = p(x) 2. Xu and X l are distributed according to µ and ν, respectively. 3. p(S = 2) 6= 0.\nProof: Observe that\np(x|S = 0) = p(x, Y = 1|S = 0) + p(x, Y = 0|S = 0) = p(Y = 1|S = 0)p(x|Y = 1, s = 0) + p(Y = 0|S = 0)p(x|Y = 0, s = 0) = p(Y = 1)p(x|Y = 1) + p(Y = 0)p(x|Y = 0) (from assumptions 8 and 10) = p(x).\nThus X is independent of S = 0. This proves statement 1. From statement 1, Xu has the same distribution as X , which is µ. Now,\np(x|S = 1) = p(x, Y = 1|S = 1) + p(x, Y = 0|S = 1) = p(Y = 1|S = 1)p(x|Y = 1, s = 1) + p(Y = 0|S = 1)p(x|Y = 0, s = 1) = βp(x|Y = 1) + (1− β)p(x|Y = 0). (from assumptions 9 and 10)\nThus the distribution of X|S = 1 is βµ1 + (1− β)µ0, which is ν. This proves statement 2. Now,\np(S = 2|x) = 1− p(S = 0|x)− p(S = 1|x)\n= 1− p(S = 0)− p(x|S = 1) p(x) p(S = 1) (because S = 0 and X are independent)\nThe probability p(S = 2|x) is independent of x only if p(x|S=1)/p(x) is a constant with respect to x. Let p(x|S=1)/p(x) = c, where c is some constant. Integrating over x on both sides gives ∫ X p(x|S =\n1)dx = c ∫ X p(x)dx. Since both integrals are 1, it follows that c = 1. Thus p(x|S=1)/p(x) = 1, which implies µ = ν, i.e., the labeled and unlabeled samples have the same distribution. However, this implies µ1 = µ0, which contradicts the assumption. Therefore, S = 2 is not independent of X .\nTheorem 2 (α∗-preserving transform) Let random variables X,Y, S,Xu, X l and measures µ, ν, µ0, µ1 be as defined in Lemma 2. Let τp be the posterior as defined in Equation 11 and τ = H ◦ τp, where H is a 1-to-1 function on [0, 1] and ◦ is the composition operator. Assume\n1. (µ0, µ1) ∈ Πres, 2. Xu, X l are continuous with densities f, g, respectively, 3. µτ , ντ , µτ1 are the measures corresponding to τ(Xu), τ(X l), τ(X1), respectively, 4. (α+, β+, α∗, β∗) = (aνµ, a µ ν , a µ1 µ , a µ1 ν ) and (α + τ , β + τ , α ∗ τ , β ∗ τ ) = (a ντ µτ , a µτ ντ , a µτ1 µτ , a µτ1 ντ ).\nThen (α+τ , β + τ , α ∗ τ , β ∗ τ ) = (α +, β+, α∗, β∗) and so τ is an α∗-preserving transformation.\nMoreover, τp can also be used to compute the true posterior probability:\np(Y = 1|x) = α ∗(1− α∗) β∗ − α∗\n( p(S = 0)\np(S = 1)\nτp(x) 1− τp(x) − 1− β ∗ 1− α∗ ) . (12)\nProof: First we prove that (α+, β+) = (α+τ , β+τ ). To this end, we expand τp(x) as follows\nτp(x) = p(S = 1, x, S ∈ {0, 1})\np(x, S ∈ {0, 1})\n= p(S = 1, x)\np(x, S = 0) + p(x, S = 1)\n= p(x|S = 1)p(S = 1)\np(x|S = 0)p(S = 0) + p(x|S = 1)p(S = 1) (13)\n= p(S = 1)\np(x|S=0) p(x|S=1)p(S = 0) + p(S = 1)\n= p(S = 1)\nf(x) g(x)p(S = 0) + p(S = 1)\n; (14)\nthe last step is justified because f(x) = p(X = x|S = 0) and g(x) = p(X = x|S = 1). For oneto-one functions G1(t) = p(S=1) tp(S=0)+p(S=1) and G2(t) =\np(S=1) 1 t p(S=0)+p(S=1) defined on R+ ∪ {0,∞}, we apply Theorem 4 with\n• Xu, X l and H ◦G1 playing the role of X , X1 and G, respectively. Now, τ = H ◦ τp\n= H ◦G1 ◦ τd (from Equation 14) Because H and G1 are both one-to-one functions, H ◦ G1 is one-to-one as well. Thus all the conditions of Theorem 4 is satisfied and consequently, α+ = α+τ .\n• Xu, X l and H ◦G2 playing the role of X1, X and G, respectively. Now, τ = H ◦ τp\n= H ◦G2 ◦ τd (from Equation 14) Because H and G2 are both one-to-one functions, H ◦ G2 is one-to-one as well. Thus all the conditions of Theorem 4 is satisfied and consequently, β+ = β+τ .\nNow, from Theorem 1 statement 2 and Equation 7\nα∗ = α +(1−β+)/(1−α+β+)\nβ∗ = (1−β +)/(1−α+β+)\nα∗τ = α + τ (1−β + τ )/(1−α+τ β + τ ) β∗τ = (1−β + τ )/(1−α+τ β + τ ).\nand thus (α∗τ , β ∗ τ ) = (α ∗, β∗).\nNext we prove Equation 12. Let c = p(S=1)/p(S=0). Rearranging the terms of Equation 13,\nτ(x) 1− τ(x) = p(x|S = 1)p(S = 1) p(x|S = 0)p(S = 0)\n= p(S = 1)\np(S = 0)\n( p(x, Y = 1|S = 1) + p(x, Y = 1|Y = 0)\np(x)\n) (from Lemma 2 statement 1)\n= p(S = 1)\np(S = 0)\n( p(Y = 1|S = 1)p(x|Y = 1, S = 1)\np(x) + p(Y = 0|S = 1)p(x|Y = 0, S = 1)) p(x) ) = p(S = 1)\np(S = 0)\n( p(Y = 1|S = 1)p(x|Y = 1)\np(x) + p(Y = 0|S = 1)p(x|Y = 0) p(x) ) (from assumption 10)\n= 1\nc\n( p(Y = 1|S = 1)\np(Y = 1) p(Y = 1|x) + p(Y = 0|S = 1) p(Y = 0) p(Y = 0|x) ) = 1\nc\n( β∗\nα∗ p(Y = 1|x) + 1− β ∗ 1− α∗ (1− p(Y = 1|x)) )\n= 1\nc ( 1− β∗ 1− α∗ + ( β∗ α∗ − 1− β ∗ 1− α∗ ) p(Y = 1|x) ) .\nRearranging the terms,\np(Y = 1|x) = α ∗(1− α∗) β∗ − α∗\n( cτ(x)\n1− τ(x) − 1− β∗ 1− α∗\n) .\nTheorem 3 Let µ, µ1 and µ0 be three measures defined on A such that µ = αµ1 + (1 − α)µ0 for some α ∈ [0, 1]. Define\nR(µ, µ1) = {µ(A)/µ1(A) : A ∈ A, µ1(A) > 0}."
    }, {
      "heading" : "It follows that",
      "text" : "1. aµ1µ = inf R(µ, µ1).\n2. If aµ1µ0 = 0, then α = a µ1 µ .\nProof: The proof follows from Lemma 4 and Theorem 3 of Jain et al. (2016).\nTheorem 4 (Restatement of Theorem 9 in Jain et al. (2016)) LetX andX1 be random variables with densities f and f1 and measures µ and µ1 respectively. For R + = R+∪{0,∞} and an abstract space Xτ , given any one-to-one function G : R + → Xτ , define function τ : X → Xτ\nτ = G ◦ τd,\nwhere\nτd(x) = { f(x)/f1(x) if f1(x) > 0 ∞ if f1(x) = 0.\nLet µτ and µτ1 be the measures for the random variables τ(X), τ(X1) respectively for σ-algebra"
    }, {
      "heading" : "Aτ on Xτ . Then aµ1µ = aµτ1µτ .",
      "text" : ""
    }, {
      "heading" : "B MSGMM",
      "text" : "Let U = {Xui } and L = {X li} be the unlabeled sample and the noisy positive sample, respectively. The parametric approach is derived by modeling each sample as a two component Gaussian mixture, sharing the same components but having different mixing proportions:\nXui ∼ αN (u1,Σ1) + (1− α)N (u0,Σ0) X li ∼ βN (u1,Σ1) + (1− β)N (u0,Σ0)\nwhere u1, u0 ∈ Rd and Σ1,Σ0 ∈ Sd++, the set of all d× d positive definite matrices. The algorithm is an extension to the EM approach for Gaussian mixture models (GMMs) where, instead of estimating the parameters of a single mixture, the parameters of both mixtures (α, β, u0, u1,Σ0,Σ1) are estimated simultaneously by maximizing the combined likelihood over both U and L. This approach, that we refer to as a multi-sample GMM (MSGMM), exploits the constraint that the two mixtures share the same components. To derive the update equations, we introduce missing variables Wui ,W l j that give the true class of the ith and jth example in U and L, respectively. The variables Wui ,W l j are Bernoulli distributed; i.e., Wui ∼ Bernoulli(α) and W lj ∼ Bernoulli(β). For\nW = {Wui }|U |i=1, V = {W lj} |L| j=1\nthe quartet (U,L,W, V ) forms the observed and unobserved variables in the EM framework. The complete data log-likelihood, llC is given by,\nllC = ∑|U | i=1W u i log[αφ1(x u i )] + (1−Wui ) log[(1− α)φ0(xui )]\n+ ∑|L| i=1W l i log [ βφ1(x l i) ] + (1−W li ) log [ (1− β)φ0(xli) ] ,\nwhere φi is the density of N (ui,Σi). Our goal is to maximize E[llC ]. To do so we take the conditional expectation of llC with respect to W and V given U and L. For\nw̄ui = E[W u i |Xui = xui ] =\nαφ1(x u i )\nαφ1(xui ) + (1−α)φ0(x u i ) ,\nw̄li = E[W l i |X li = xli] =\nβφ1(x l i)\nβφ1(xli) + (1−β)φ0(x l i) ,\nwe obtain\nE[llC ] = |U |∑ i=1 w̄ui log[αφ1(x u i )] + (1− w̄ui ) log[(1− α)φ0(xui )]\n+ |L|∑ i=1 w̄li log [ βφ1(x l i) ] + (1−w̄li) log [ (1−β)φ0(xli) ]\nwhich up to constants, which are ignored in the optimization, can be explicitly written as\nE[llC ] = |U |∑ i=1 w̄ui [ logα− 1 2 log |Σ1|−(xui −u1)TΣ−11 (xui −u1) ]\n+ |U |∑ i=1 (1−w̄ui ) [ log(1−α)− 1 2 log |Σ0|−(xui −u0)TΣ−10 (xui −u0) ]\n+ |L|∑ i=1 w̄li [ log β− 1 2 log |Σ1|−(xli−u1)TΣ−11 (xli−u1) ]\n+ |L|∑ i=1 (1−w̄li) [ log(1−β)− 1 2 log |Σ0|−(xli−u0)TΣ−10 (xli−u0) ] .\nFinally, we get the parameter update equations by maximizing E[llC ] with respect to (α, β, u0, u1,Σ0,Σ1):\nα← 1/|U |∑|U |i=1 w̄ui β ← 1/|L|∑|L|j=1 w̄lj u1 ← ∑|U | i=1 w̄ u i x u i + ∑|L| j=1 w̄ l jx l j∑|U |\ni=1 w̄ u i + ∑|L| j=1 w̄ l j\nu0 ← ∑|U | i=1(1− w̄ui )xui + ∑|L| j=1(1− w̄lj)xlj∑|U |\ni=1(1− w̄ui ) + ∑|L| j=1(1− w̄lj)\nΣ1 ← ∑|U | i=1 w̄ u i (x u i − u0)(xui − u0)T + ∑|L| j=1 w̄ l j(x l j − u0)(xlj − u0)T∑|U |\ni=1 w̄ u i + ∑|L| j=1 w̄ l j\nΣ0 ← ∑|U | i=1(1− w̄ui )(xui − u0)(xui − u0)T + ∑|L| j=1(1− w̄lj)(xlj − u0)(xlj − u0)T∑|U |\ni=1(1− w̄ui ) + ∑|L| j=1(1− w̄lj)\nThe update rules reduce to the standard GMM when the labeled sample is not provided. Further generalization to more than two samples and/or mixing components is straightforward."
    }, {
      "heading" : "C AlphaMax",
      "text" : "For a mixture sample M and a component sample C AlphaMax(M,C) estimates the maximum proportion of C in M . AlphaMax is based on the constrained maximization of the log likelihood of samples M and C, derived using nonparametric estimates of their densities m and c, respectively. We list the main steps of AlphaMax below.\n1. Estimate c nonparameterically as ĉ using sample C. Obtain the weights, vi, and components κi from nonparametric density estimation of m as a k-component mixture, m̂(x) =∑k i=1 viκi(x), using M .\n2. Construct two density functions c̃(·|ω) and m̃(·|ω) from vi, κi and ĉ parameterized by a k-dimensional weight vector ω = [ωi], 0 ≤ ωi ≤ 1, which re-weights components κi:\nc̃(x|ω) = ∑k i=1 ωiviκi(x)∑k\ni=1 ωivi ,\nm̃(x|ω) = (\nk∑ i=1 ωivi\n) ĉ(x) + ( 1−\nk∑ i=1 ωivi\n) c̃(x|1− ω);\n3. Maximize the log likelihood of M and C constructed with m̃ and c̃ under the constraint∑k i=1 ωivi = r for many values of r equispaced in [0, 1].\nllr = max w.r.t. ω ∑ x∈M log m̃(x|ω) + ∑ x∈C log c̃(x|ω),\nsubject to ∑k i=1 ωivi = r,\n0 ≤ vi ≤ 1, i = 1, . . . , k.\n4. Estimate the maximum proportion of c in m, acm (minor abuse of notation 1), as the x-\ncoordinate of the elbow in the llr versus r graph.\nThe densities m̃(·|ω) and c̃(·|ω) are constructed to approximate m and c. The efficacy of the approximation depends on the value of ω; there exists ω such that m̃(·|ω) and c̃(·|ω) are good approximations provided ∑k i=1 ωivi ≤ acm, however, the approximation deteriorates progressively, even\nwith the optimum ω, as ∑k i=1 ωivi moves beyond a c m. This suggests that the graph of llr versus r should be approximately a flat line from 0 to acm and decrease progressively beyond a c m exposing an elbow at acm, which is detected in the last step. The pseudo code for elbow detection is provided in (Jain et al., 2016).\nOur implementation uses histograms as the nonparametric method to obtain m̂ and ĉ. The bin-width is chosen to cover the component sample’s (after the transformation) range and reveal the shape of its distribution, using the default option in Matlab’s histogram function. More bins with the same bin-width are subsequently added to cover the mixture sample’s range."
    }, {
      "heading" : "D Results for synthetic data",
      "text" : ""
    }, {
      "heading" : "E Results for multivariate AlphaMax-N and MSGMM",
      "text" : "To demonstrate the efficacy of class-prior preserving transform, we implemented the multivariate versions of AlphaMax-N and MSGMM and evaluated them on the twelve real data sets without applying the transform. There were significant stability and computational issues related to the highdimensional nature of the data sets. MSGMM was numerically instable because of singular/nearlysingular covariance matrix; AlphaMax-N became computationally demanding because the number of bins (for histogram based density estimation) grow exponentially with the dimension, resulting into a large parameter vector ω and consequently, a large optimization problem, even after removing the zero-count bins. This is expected, as density estimation for multivariate data is known to be problematic, which is one of the main reasons for introducing our transform. To make estimation feasible under these stability and computational issues, we used dimensionality reduction. Though not all data sets posed the same level of difficulty, to have a standard approach and permit effective density estimation, we used the top three principal components, obtained via principal component analysis on the z-score normalized data (mixture and component samples combined), as input to the two algorithms. We also attempted using top k principal components that preserve 75 percent of the total variance, however, for some of the data sets, the dimension was still too high.\nIn the same manner as in the univariate case, we used histograms in the multivariate implementation of AlphaMax-N. The bin-width for a dimension was selected to minimize the asymptotic mean integrated squared error (AMISE) with a normal reference rule, using the component sample, C. The formula for the bin-width of dimension k is given by:\nbk = 3.5σk|C|−1/(2+d), where d is the total number of dimensions, σk is the standard deviation of the kth dimension and |C| is the component sample size. Bins were added to cover the range of entire data, mixture and component combined and empty bins were removed to reduce the size of the optimization problem.\nTable 3 contains the results of AlphaMax-N and MSGMM on the real-life data sets, using the top three principal components under column headings AlphaMax-NM (M for multi-dimensional) and\n1acm is a minor abuse of notation; replacing the densities with the underlying measures makes the notation correct\nMSGMM, respectively. The results of AlphaMax-N and MSGMM-T with the class-prior preserving transform are also provided for comparison. Notice that, though AlphaMax-NM (without transform) performs well, AlphaMax-N (with transform) is significantly better in terms of estimation error, despite having a lower computational cost. Also notice the deterioration in the performance of MSGMM (without transform) compared to MSGMM-T (with transform). In conclusion, the results illustrate the usefulness of the class-prior preserving transform."
    } ],
    "references" : [ {
      "title" : "High breakdown mixture discriminant analysis",
      "author" : [ "S. Bashir", "E.M. Carter" ],
      "venue" : "J Multivar Anal,",
      "citeRegEx" : "Bashir and Carter.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bashir and Carter.",
      "year" : 2005
    }, {
      "title" : "Semi-supervised novelty detection",
      "author" : [ "G. Blanchard", "G. Lee", "C. Scott" ],
      "venue" : "J Mach Learn Res,",
      "citeRegEx" : "Blanchard et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Blanchard et al\\.",
      "year" : 2010
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T. Mitchell" ],
      "venue" : "In Proceedings of the 11th Annual Conference on Computational Learning Theory, COLT",
      "citeRegEx" : "Blum and Mitchell.,? \\Q1998\\E",
      "shortCiteRegEx" : "Blum and Mitchell.",
      "year" : 1998
    }, {
      "title" : "Robust supervised classification with mixture models: learning from data with uncertain labels",
      "author" : [ "C. Bouveyron", "S. Girard" ],
      "venue" : "Pattern Recognit,",
      "citeRegEx" : "Bouveyron and Girard.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bouveyron and Girard.",
      "year" : 2009
    }, {
      "title" : "Sample selection bias correction theory",
      "author" : [ "C. Cortes", "M. Mohri", "M. Riley", "A. Rostamizadeh" ],
      "venue" : "In Proceedings of the 19th International Conference on Algorithmic Learning Theory, ALT",
      "citeRegEx" : "Cortes et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning from positive and unlabeled examples",
      "author" : [ "F. Denis", "R. Gilleron", "F. Letouzey" ],
      "venue" : "Theor Comput Sci,",
      "citeRegEx" : "Denis et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Denis et al\\.",
      "year" : 2005
    }, {
      "title" : "Class prior estimation from positive and unlabeled data",
      "author" : [ "M.C. du Plessis", "M. Sugiyama" ],
      "venue" : "IEICE Transactions on Information and Systems,",
      "citeRegEx" : "Plessis and Sugiyama.,? \\Q2014\\E",
      "shortCiteRegEx" : "Plessis and Sugiyama.",
      "year" : 2014
    }, {
      "title" : "Learning classifiers from only positive and unlabeled data",
      "author" : [ "C. Elkan", "K. Noto" ],
      "venue" : "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Elkan and Noto.,? \\Q2008\\E",
      "shortCiteRegEx" : "Elkan and Noto.",
      "year" : 2008
    }, {
      "title" : "High-breakdown linear discriminant analysis",
      "author" : [ "D.M. Hawkins", "G.J. McLachlan" ],
      "venue" : "J Am Stat Assoc,",
      "citeRegEx" : "Hawkins and McLachlan.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hawkins and McLachlan.",
      "year" : 1997
    }, {
      "title" : "Nonparametric semi-supervised learning of class proportions",
      "author" : [ "S. Jain", "M. White", "M.W. Trosset", "P. Radivojac" ],
      "venue" : "arXiv preprint arXiv:1601.01944,",
      "citeRegEx" : "Jain et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2016
    }, {
      "title" : "A mutual contamination analysis of mixed membership and partial label models",
      "author" : [ "J. Katz-Samuels", "C. Scott" ],
      "venue" : "arXiv preprint arXiv:1602.06235,",
      "citeRegEx" : "Katz.Samuels and Scott.,? \\Q2016\\E",
      "shortCiteRegEx" : "Katz.Samuels and Scott.",
      "year" : 2016
    }, {
      "title" : "Estimating a kernel Fisher discriminant in the presence of label noise",
      "author" : [ "N.D. Lawrence", "B. Scholkopf" ],
      "venue" : "In Proceedings of the 18th International Conference on Machine Learning,",
      "citeRegEx" : "Lawrence and Scholkopf.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lawrence and Scholkopf.",
      "year" : 2001
    }, {
      "title" : "Sparse nonparametric density estimation in high dimensions using the rodeo",
      "author" : [ "H. Liu", "J.D. Lafferty", "L.A. Wasserman" ],
      "venue" : "In Proceedings of the 10th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Liu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2007
    }, {
      "title" : "Random classification noise defeats all convex potential boosters",
      "author" : [ "P.M. Long", "R.A. Servedio" ],
      "venue" : "Mach Learn,",
      "citeRegEx" : "Long and Servedio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Long and Servedio.",
      "year" : 2010
    }, {
      "title" : "Noise tolerance under risk minimization",
      "author" : [ "N. Manwani", "P.S. Sastry" ],
      "venue" : "IEEE T Cybern,",
      "citeRegEx" : "Manwani and Sastry.,? \\Q2013\\E",
      "shortCiteRegEx" : "Manwani and Sastry.",
      "year" : 2013
    }, {
      "title" : "Obtaining calibrated probabilities from boosting",
      "author" : [ "A. Niculescu-Mizil", "R. Caruana" ],
      "venue" : "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Niculescu.Mizil and Caruana.,? \\Q2005\\E",
      "shortCiteRegEx" : "Niculescu.Mizil and Caruana.",
      "year" : 2005
    }, {
      "title" : "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods, pages 61–74",
      "author" : [ "J.C. Platt" ],
      "venue" : null,
      "citeRegEx" : "Platt.,? \\Q1999\\E",
      "shortCiteRegEx" : "Platt.",
      "year" : 1999
    }, {
      "title" : "Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure",
      "author" : [ "M. Saerens", "P. Latinne", "C. Decaestecker" ],
      "venue" : "Neural Comput,",
      "citeRegEx" : "Saerens et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Saerens et al\\.",
      "year" : 2002
    }, {
      "title" : "Classification with asymmetric label noise: consistency and maximal denoising",
      "author" : [ "C. Scott", "G. Blanchard", "G. Handy" ],
      "venue" : "J Mach Learn Res W&CP,",
      "citeRegEx" : "Scott et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Scott et al\\.",
      "year" : 2013
    }, {
      "title" : "The curse of dimensionality and dimension reduction. Multivariate Density Estimation: Theory, Practice, and Visualization, pages",
      "author" : [ "D.W. Scott" ],
      "venue" : null,
      "citeRegEx" : "Scott.,? \\Q2008\\E",
      "shortCiteRegEx" : "Scott.",
      "year" : 2008
    }, {
      "title" : "The ABC’s (and XYZ’s) of peptide sequencing",
      "author" : [ "H. Steen", "M. Mann" ],
      "venue" : "Nat Rev Mol Cell Biol,",
      "citeRegEx" : "Steen and Mann.,? \\Q2004\\E",
      "shortCiteRegEx" : "Steen and Mann.",
      "year" : 2004
    }, {
      "title" : "Theorem 4 (Restatement of Theorem 9 in Jain et al. (2016)) LetX andX1 be random variables with densities f and f1 and measures μ and μ1",
      "author" : [ "Jain" ],
      "venue" : null,
      "citeRegEx" : "Jain,? \\Q2016\\E",
      "shortCiteRegEx" : "Jain",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "In many domains, however, a sample from one of the classes (say, negatives) may not be available, leading to the setting of learning from positive and unlabeled data (Denis et al., 2005).",
      "startOffset" : 166,
      "endOffset" : 186
    }, {
      "referenceID" : 7,
      "context" : "If the class priors in the unlabeled data are known, positive-unlabeled learning can straightforwardly translate into learning of non-traditional classifiers (Elkan and Noto, 2008); i.",
      "startOffset" : 158,
      "endOffset" : 180
    }, {
      "referenceID" : 9,
      "context" : ", those that discriminate between positive and negative data (Jain et al., 2016).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "Under mild assumptions, even when the class priors are unknown, there exists a monotonic relationship between the outputs of these classifiers (Jain et al., 2016) and, hence, the models trained for information retrieval and ranking generally do not suffer when trained on labeled vs.",
      "startOffset" : 143,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.",
      "startOffset" : 146,
      "endOffset" : 228
    }, {
      "referenceID" : 18,
      "context" : "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.",
      "startOffset" : 146,
      "endOffset" : 228
    }, {
      "referenceID" : 9,
      "context" : "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.",
      "startOffset" : 146,
      "endOffset" : 228
    }, {
      "referenceID" : 7,
      "context" : ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Jain et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Jain et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : "For example, in the process of peptide identification (Steen and Mann, 2004), bioinformatics methods are usually set to report results with specified false discovery rate thresholds (e.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "Further, the only approach that does consider similar such noise (Scott et al., 2013) requires density estimation, which is known to be problematic for high-dimensional data.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "Much of the identifiability characterization in this section has already been considered as the case of asymmetric noise (Scott et al., 2013); see Section 7 on related work.",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "To ensure identifiability, it is necessary to choose a canonical form that prefers a class prior that makes the two components as different as possible; this canonical form was independently introduced as the mutual irreducibility principle (Scott et al., 2013) or the max-canonical form (Jain et al.",
      "startOffset" : 241,
      "endOffset" : 261
    }, {
      "referenceID" : 9,
      "context" : ", 2013) or the max-canonical form (Jain et al., 2016).",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Nonparametric (kernel) density estimation is also known to have curse-of-dimensionality issues, both in theory (Liu et al., 2007) and in practice (Scott, 2008).",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : ", 2007) and in practice (Scott, 2008).",
      "startOffset" : 24,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "The transform is similar to that in (Jain et al., 2016), except that it is not required to be calibrated like a posterior distribution; as shown below, a good ranking function is sufficient.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 16,
      "context" : "The posterior probability τp can be estimated directly by using a probabilistic classifier or by calibrating a classifier’s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of α∗ and β∗.",
      "startOffset" : 130,
      "endOffset" : 178
    }, {
      "referenceID" : 15,
      "context" : "The posterior probability τp can be estimated directly by using a probabilistic classifier or by calibrating a classifier’s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of α∗ and β∗.",
      "startOffset" : 130,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "For this purpose, we use the AlphaMax algorithm (Jain et al., 2016), briefly summarized in the Appendix.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "The algorithm proposed by du Plessis and Sugiyama (2014) minimizes the same objective as the e1 Elkan-Noto estimator and, thus, was not implemented.",
      "startOffset" : 29,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016).",
      "startOffset" : 186,
      "endOffset" : 216
    }, {
      "referenceID" : 2,
      "context" : "(2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998).",
      "startOffset" : 84,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.",
      "startOffset" : 165,
      "endOffset" : 216
    }, {
      "referenceID" : 14,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.",
      "startOffset" : 165,
      "endOffset" : 216
    }, {
      "referenceID" : 8,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.",
      "startOffset" : 233,
      "endOffset" : 287
    }, {
      "referenceID" : 0,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.",
      "startOffset" : 233,
      "endOffset" : 287
    }, {
      "referenceID" : 11,
      "context" : "Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class.",
      "startOffset" : 95,
      "endOffset" : 153
    }, {
      "referenceID" : 3,
      "context" : "Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class.",
      "startOffset" : 95,
      "endOffset" : 153
    }, {
      "referenceID" : 5,
      "context" : "Class prior estimation in a semi-supervised setting including positive-unlabeled learning, has been extensively discussed previously; see Saerens et al. (2002); Cortes et al.",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "(2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "(2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al.",
      "startOffset" : 8,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al.",
      "startOffset" : 31,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein.",
      "startOffset" : 31,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein. Recently, a general setting for label noise has also been introduced, called the mutual contamination model. The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016). The setting of asymmetric label noise is a subset of this more general setting, treated under general conditions by Scott et al. (2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998).",
      "startOffset" : 31,
      "endOffset" : 583
    }, {
      "referenceID" : 0,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise. Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class. As the most related work, though Scott et al. (2013) did not explicitly treat the positive-unlabeled learning with noisy positives, their formulation can incorporate this setting by using π0 = α and β = 1 − π1.",
      "startOffset" : 263,
      "endOffset" : 611
    }, {
      "referenceID" : 9,
      "context" : "Proof: The proof follows from Lemma 4 and Theorem 3 of Jain et al. (2016).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Theorem 4 (Restatement of Theorem 9 in Jain et al. (2016)) LetX andX1 be random variables with densities f and f1 and measures μ and μ1 respectively.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "The pseudo code for elbow detection is provided in (Jain et al., 2016).",
      "startOffset" : 51,
      "endOffset" : 70
    } ],
    "year" : 2017,
    "abstractText" : "We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and both parametric and nonparametric algorithms proposed here constitutes an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.",
    "creator" : "LaTeX with hyperref package"
  }
}