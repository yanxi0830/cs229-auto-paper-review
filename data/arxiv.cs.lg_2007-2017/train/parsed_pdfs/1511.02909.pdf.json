{
  "name" : "1511.02909.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Azam Moosavi", "Razvan Stefanescu", "Adrian Sandu" ],
    "emails" : [ "rstefan@ncsu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "key words reduced order models, high-fidelity models, data fitting, machine learning, feasible region of parameters, local reduced order models."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many physical phenomena are described mathematically by partial differential equations (PDE), and, after applying suitable discretization schemes, are simulated on a computer. PDE-based models frequently require calibration and parameter tuning in order to provide realistic simulation results. Recent developments in the field of uncertainty quantification [52, 85, 35, 18] provide the necessary tools for validation of such models even in the context of variability and lack of knowledge on the input parameters. While uncertainty propagation techniques such as Markov chain [45] and perturbation methods [15, 16, 17] can measure the impact of uncertain parameters on some quantities of interest, they often become infeasible due to the large number of model realizations requirement. Similar difficulties are encountered when solving Bayesian inference problems since sampling from posterior distribution is required.\nFor large-scale simulations, the variational [79, 78, 21, 60, 61, 87] and ensemble [30, 96, 48, 43] based inference approaches are widely used in practice. Their efficiency decreases with increasing computational complexity of the underlying physical models. However, increasing model complexity is unavoidable as science fields progress. For example, finer space resolution of the underlying PDE models is one of the most important factors contributing to the one day/decade growth rate of the reliability of atmospheric weather predictions [13, 92].\nThe need for computational efficiency motivated the development of surrogate models such as response surfaces, low resolution, and reduced basis models, in order to facilitate optimization, inference, and uncertainty quantification.\nData fitting or response surface models [85] are constructed using only a data-driven angle. The underlying physics remains unknown and only the input-output behavior of the model is considered. Data fitting can use techniques such as regression, interpolation, radial basis function, Gaussian Processes, artificial neural networks and other supervised machine-learning methods. The latter techniques can automatically detect patterns in data, and one can use them to predict future data under uncertainty in a probabilistic framework [59]. While easy to implement due to the non-intrusive nature, the prediction abilities may suffer since the governing physics is not specifically accounted for.\nLow fidelity models attempt to reduce the computational burden of the high-fidelity models by neglecting some of the physical aspects (e.g., replacing Navier-Stokes and Large Eddy Simulations with inviscid Euler’s equations and Raynolds-Averaged NavierStokes [33, 76, 97], or decreasing the spatial resolution [20, 90]). The additional approximations, however, may considerably degrade the physical solution with only a modest decrease of the computational load.\nReduced basis [67, 9, 34, 65, 74, 24] and Proper Orthogonal Decomposition [49, 56, 42, 57, 58] are two of the popular reduced order modeling (ROM) strategies available in the literature. Data analysis is conducted to extract basis functions from experimental data or detailed simulations of high-dimensional systems (method of snapshots [81, 82, 83]), for subsequent use in Galerkin projections that yield low dimensional dynamical models. While these type of models are physics-based and therefore require intrusive implementations, they are usually faster and more robust than data fitting and low-fidelity models.\nRobustness of ROM in a parametric setting can be achieved by constructing a global basis [40, 68], but this strategy generates large dimensional bases that may lead to slow reduced order models. Local approaches have been designed for parametric or time domains generating local bases for both the state variables [70, 23] and non-linear terms [28, 66]. Here we address the robustness issue of POD reduced order models by charting the parametric domain with feasible regions of local reduced order models where the reduced solutions are accurate within an admissible prescribed threshold. Two essential ingredients are used to construct the parametric map: a database of reduced order models, and a data-fitting probabilistic model for the reduced order model errors. Then an incremental procedure uses the newly created probabilistic model to sample the parametric domain and generates a feasible region where a specific reduced order model provides accurate solutions within a prescribed tolerance. We then use a greedy approach to sweep the parameter domain and cover it with such feasible regions. This methodology is applied to the viscous 1D-Burgers model, and a parametric map for the viscosity parameter is generated for various error thresholds. Once the map is constructed there is no need to run the high-fidelity model again, since for each parameter value µ0 there exists a parameter µ, and the associated reduced order model (basis and reduced operators), in whose interval the current value µ0 falls; the corresponding reduced solution error is accurately estimated a-priori. The dimension k of the local basis is usually small since it depends only on one high-fidelity model trajectory.\nThe database of reduced order models and the high-fidelity trajectories are used to statistically model the reduced order approximation errors. We solve the resulting non-linear regression problems using Gaussian processes (GP) [84, 55] and artificial neural networks\n(ANN). Specifically, consider the reduced order model of dimension k constructed using the high-fidelity solution computed with parameter value µ. Let ε be the error of this reduced model when approximating the full solution at parameter configuration µ0. We use a GP or ANN approach to model the mapping {µ0, µ, k} → log(ε). Our approach is inspired by the multifidelity correction and ROMES methodologies available in the literature for estimation of surrogate models errors using a global basis. Multifidelity correction [2, 29, 33, 44] has been developed for low-fidelity models in the context of optimization. They simulate the input-output relation µ→ ε, where ε is the low-fidelity model errors. The ROMES method [25] introduces the concept of error indicators for reduced order models and generalizes the ’multifidelity correction’ framework by approximating the mapping ρ(µ) → log(ε). The error indicators ρ(µ) include rigorous error bounds and reduced order residual norms, while ε is the reduced order model error at µ using a reduced global basis. By estimating the log of the reduced order model error instead of the error itself the probabilistic map exhibits a lower variance as shown by our numerical experiments as well as those in [25].\nThe size of a feasible region directly depends on the location of parameter value µ within the parametric space. For overlapping feasible regions one can combine the available bases and the high-fidelity model trajectories in an efficient manner to generate more accurate reduced order models. Three different approaches are proposed here, i.e. bases interpolation, bases concatenation, and highfidelity model solution interpolation. Assuming a linear dependence we perform a Lagrangian interpolation of the bases in the matrix space [54], or interpolate their projections onto some local coordinate systems [54, 4]. Following the idea of the spanning ROM introduced in [93], for a new parameter not found in the ROM database, we create a projection basis either by concatenating two of the available bases that generated the higher accurate reduced order solutions for the new configurations, or by interpolating the associated high-fidelity solutions and then extracting the singular vectors.\nFinally, we address the issue of a-priori selection of the reduced basis dimension for a prescribed accuracy of the reduced solution. The standard approach is to analyze the spectrum of the snapshots matrix, and use the largest singular value removed from the expansion to estimate the accuracy level [91]. To take into account the error due to the integration in the reduced space, here we use data fitting models to approximate the mapping {µ, log(ε)} → k. Numerical results obtained using Gaussian processing and artificial neural networks are very promising.\nThe remainder of the paper is organized as follows. Section 2 reviews the reduced order modeling parametric framework. The problems solved herein are formulated in Section 3. Gaussian processes and artificial neural networks are reviewed in Section 4. The proposed techniques for generating reduced order bases for new parameter configurations are introduced in Section 5. Section 6 presents the details of the data fitting models, constructs the parametric map for the viscous 1D-Burgers model, and analyses the probabilistic model’s performance. Conclusions are drawn in Section 7."
    }, {
      "heading" : "2 Parametrized Reduced Order Modeling",
      "text" : "Proper Orthogonal Decomposition has been successfully applied in numerous applications such as compressible flow [72], computational fluid dynamics [50, 73, 98], and aerodynamics [12], to mention a few. It can be thought of as a Galerkin approximation in the spatial variable built from functions corresponding to the solution of the physical system at specified time instances. A system reduction strategy for Galerkin models of fluid flows leading to dynamic models of lower order based on a partition in slow, dominant, and fast modes, has been proposed in [64]. Closure models and stabilization strategies for POD of turbulent flows have been investigated in [77, 95].\nIn this paper we consider discrete inner products (Euclidian dot product), though continuous products may be employed as well. Generally, an atmospheric or oceanic computer model is described by the following semi–discrete dynamical system:\ndx(µ0, t)\ndt = F(x, t, µ0), x(µ0, 0) = x0 ∈ RNstate , µ0 ∈ P̃. (1)\nThe input-parameter µ0 typically characterizes the physical properties of the flow. For a given parameter configuration µ0 we select an ensemble of Nt time instances of the flow x µ0 t1 , ...,x µ0 tNt ∈ RNstate , where Nstate is the total number of discrete model variables, and Nt ∈ N, Nt > 0. The POD method chooses an orthonormal basis Uµ0 = [uµ 0\ni ]i=1,..,k ∈ RNstate×k, k > 0, such that the mean square error between x(µ0, ti) and the POD expansion x µ0 POD(ti) = Uµ0 x̃(µ 0, ti), x̃(µ0, ti) ∈ Rk, is minimized on average. The POD space dimension k Nstate is appropriately chosen to capture the dynamics of the flow. Algorithm 1 describes the reduced order basis construction procedure [88].\nAlgorithm 1 POD basis construction\n1: Compute the singular value decomposition for the snapshots matrix [xµ 0 t1 . . . x µ0 tNt ] = Ūµ0Σµ0 V̄ T µ0 , with the singular vectors matrix\nŪµ0 = [u µ0 i ]i=1,..,Nstate . 2: Using the singular-values λ1 ≥ λ2 ≥ ...λn ≥ 0 stored in the diagonal matrix Σ, define I(m) = ( ∑m i=1 λi)/( ∑Nstate i=1 λi). 3: Choose k, the dimension of the POD basis, such that k = minm{I(m) : I(m) ≥ γ} where 0 ≤ γ ≤ 1 is the percentage of total information captured by the reduced space X k = range(Uµ0). Usually γ = 0.99.\nNext, a Galerkin projection of the full model state and equations (2) onto the space X k spanned by the POD basis elements is used\nto obtain the reduced order model:\ndx̃(µ0, t)\ndt = UTµ0 F\n( Uµ0 x̃(µ 0, t), t, µ0 ) , x̃(µ0, 0) = UTµ0 x(0). (2)\nThe efficiency of the POD-Galerkin technique is limited to linear or bilinear terms, since the projected nonlinear terms at every discrete time step still depend on the number of variables of the full model. In case of polynomial nonlinearities the tensorial POD technique [88] can be employed to efficiently remove the dependence on the full dimension by manipulating the order of computions. A considerable reduction in complexity is achieved by the Discrete Empirical Interpolation Method (DEIM) [19, 86], a discrete variation of Empirical Interpolation Method [8], for any type of nonlinear terms.\nWhile being accurate for the given parameter configuration, the reduced model (2) loses accuracy when moving away from the initial setting. Several strategies have been proposed to derive a basis that spans the entire parameter space. These includes the reduced basis methods combined with the use of error estimates [74, 69, 68], global POD [89, 80], Krylov-based sampling methods [22, 94], and greedy techniques [36, 63]. The fundamental assumption used by these approaches is that a smooth low-dimensional global manifold characterizes the model solutions over the entire parameter domain. However, in order to ensure high accuracy of the reduced solution across the parameter space, the dimension of the reduced basis has to be increased in practice, leading to high computational costs. To alleviate this drawback we propose an alternative approach based on local parametric reduced order models."
    }, {
      "heading" : "3 Problem Description and Solution Methodology",
      "text" : "This work addresses the following problems in the construction of reduced order models: designing the parametric map, selecting the best two already existing bases for a new parameter configuration from accuracy point of view, and selecting the optimal dimension of the reduced basis. We formulate them in detail below."
    }, {
      "heading" : "3.1 Designing the parametric map",
      "text" : "Problem 1 (Efficient approximate ROMs). For an arbitrary parameter configuration µ0 ∈ P̃ construct a reduced order model (2) that provides an accurate and efficient approximation of the high-fidelity solution (1):\n‖x(µ0, ti)− U x̃(µ0, ti)‖2 < ε̄, i = 1, .., Nt, (3)\nfor some prescribed admissible error level ε̄ > 0. The snapshots used to generate the basis U can be obtained with any parametric configuration that belongs to P̃ .\nA simple solution is to solve the high-fidelity model for the specific configuration µ0, and then build the corresponding reduced order model. However, this approach is computationally expensive.\nOur methodology proposes to select a small countable subset I = {µj , j = 1, ..,M} ⊂ P̃, M > 0 and for each µj , a reduced order basis Uµj along with the reduced operators are constructed for j = 1, ..,M . We denote by U the set of bases Uµj , j = 1, ..,M . Then for each µ0 ∈ P̃ we can find a basis Uµj ∈ U and the suitable reduced order model such that its solution satisfies (3) for U = Uµj .\nThis strategy relies on the assumption that a reduced order basis and operators built for a specific parameter configuration µj ∈ P̃ can be used to design a reduced order model capable to accurately approximate the solution of the high-fidelity model (1) for all µ0 ∈ B(µj , rj)∩ P̃ , where B(µj , rj) is the closed ball of radius rj ≥ 0 centered at µj . Specifically, for µ0 ∈ B(µj , rj)∩ P̃ , the reduced order model is constructed by employing the basis Uµj and the reduced operators designed at µj , i.e.\ndx̃(µ0, µj , t)\ndt = UTµj F\n( Uµj x̃(µ 0, µj , t), t, µ 0 ) , x̃(µ0, 0) = UTµj x(0). (4)\nThen, by selecting a small radius rj , one can be able to obtain\n‖x(µ0, ti)− Uµj x̃(µ0, µj , ti)‖2 < ε̄, i = 1, .., Nt, (5)\nfor all µ0 ∈ B(µj , r) ∩ P̃ . The parametric map construction process ends as soon as the entire parameter domain P̃ is covered with a finite union of overlapping balls B(µj , rj), j = 1, ..,M , corresponding to different reduced order bases and local models\nP̃ ⊂ M⋃ j=1 B(µj , rj), (6)\nsuch that for each j = 1, 2, ..,M and ∀µ0 ∈ B(µj , r)∩ P̃ , the solution of the reduced order model (4) depending on the basis Uµj fulfils the accuracy condition (5)."
    }, {
      "heading" : "3.2 Selecting the best two already existing bases for a new parameter configuration",
      "text" : "This approach is inspired from the construction of local reduced order models where the time domain is split in multiple regions [70, 66]. In this way the reduced basis dimension is kept small allowing for fast on-line simulations. The cardinality of I depends inversely proportional with the prescribed level of accuracy ε̄. As the desired error threshold ε̄ decreases, the map changes since usually the radii rj are expected to become smaller, and more balls are required to cover the parametric domain, i.e. M is increased.\nThe construction of the parametric map (6) using the local reduced order models requires the following ingredients:\n1. The ability to probe the vicinity of µj ∈ P̃ and to efficiently estimate the level of error\nε = max i=1,..,Nt\n‖x(µ0, ti)− Uµj x̃(µ0, µj , ti)‖2, (7)\n2. The ability to find rj > 0 such that ε ≤ ε̄ for all µ0 ∈ B(µj , r)∩ P̃ . This can be theoretically achieved by assuming that the error ε is monotonically increasing with larger distances d(µ0, µj). However, this is not necessarily true and in practice this is obtained by sampling.\n3. The ability to identify the location of a new µ` (for the construction of a new local reduced order model) given the locations of the previous local parameters µj , j = 1, .., `− 1, so that\nB(µ`, r`) 6⊂ ( `−1⋃ i=1 B(µi, ri) ) , B(µ`, r`) ⋂( `−1⋃ i=1 B(µi, ri) ) 6= ∅. (8)\nThe implementation of the first ingredient does not employ a-posteriori error estimation formulas [63]. Inspired from the ’multifidelity correction’ and ROMES methodologies we construct probabilistic models to approximate the level of error ε in (7). Gaussian process and artificial neural networks are used to build the probabilistic functions to model the mapping (µ0, µj , k) → log(ε). Since the dimension of basis determines the level of error we include it among the input features. To design probabilistic models with reduced variances we look to approximate the logarithm of the error as suggested in [25].\nThe above machine learning techniques allow to sample the vicinity of µj and estimate the error for each sample parameter value. Based on these error estimates we construct the ball B(µj , r), or perhaps a larger set called a µj−feasible region, where the local reduced order model is accurate within the prescribed threshold ε̄.\nNext, a greedy algorithm is applied to identify the location of a new parametric configuration µ` (for the construction of a new basis) depending on the locations of the previous µi, i = 1, .., `− 1. Constraint (8) is imposed so the entire parametric domain P̃ satisfies (6) after the map construction is finished.\nFor the parametric area situated at the intersection of different feasible regions we can assign a new reduced order model based on the information required to construct the initial feasible regions. This is achieved by interpolation or concatenation of the underlying reduced bases or interpolation of the available high-fidelity solutions, as described in detail in Section 5."
    }, {
      "heading" : "3.2 Selecting the best two already existing bases for a new parameter configuration",
      "text" : "Since the error of the reduced order solution at a new parameter location µ0 is not necessarily smaller with the decrease of the distance d(µ0, µj), the following more general problem is posed.\nProblem 2 (Selection of best bases). For a new parameter configuration µ0 find the best available bases (among the existing ones) that provide the most accurate reduced order model solution.\nThe capability of the already proposed probability models can be used to estimate the error\nε = ‖x(µ0, ti)− Uµj x̃(µ0, µj , ti)‖2 , i = 1, .., Nt, (9)\nfor all available Uµj , j = 1, 2, .. in the database, and the bases that lead to the smallest estimated errors are selected. This approach is discussed in Section 6.3."
    }, {
      "heading" : "3.3 Selecting the dimension of the reduced basis",
      "text" : "Problem 3 (Optimal basis dimension). Find the optimal dimension of the basis Uµ for the parametric configuration µ such that the error is smaller than the prescribed threshold\n‖x(µ, ti)− Uµ x̃(µ, µ, ti)‖2 ≤ ε̄, i = 1, .., Nt. (10)\nBy optimal we understand the smallest dimension that enables the reduced order model solution to satisfy the error constraint (10). The basis dimension represents one of the most important characteristics of a reduced order model. The reduced manifold size directly affects both the on-line computational complexity of the reduced order model and its accuracy [51, 39, 31]. By increasing the size of the basis the projection error decreases and the accuracy of the reduced order model is enhanced. Consequently, the spectrum of the\nsnapshots matrix offers guidance regarding the choice of the reduced basis size when some prescribed reduced order model error is desired. However the accuracy depends also on integration errors in the case of unsteady models as stated in [41].\nWe seek to estimate the optimal size of the reduced order model by accounting for both the projection and integration errors. For this we use data fitting models to approximate the mapping {µ, log ε̄} → k, as explained in Section 6.5.\nFor all the problems addressed in this study a general probabilistic framework is introduced in Section 4, along with the description of the supervised machine learning techniques used to construct the discussed probabilistic models. For each problem we discuss the dataset, the features, as well as the accuracy and stability of the predictions of the associated data fitting models."
    }, {
      "heading" : "4 Supervised Machine Learning Techniques",
      "text" : "Consider a random vector z. Neural networks and Gaussian processes are used to build a probabilistic model φ : z → ŷ, where φ is a transformation function that learns through the input feature vector z ∈ Ω (the sample space) to estimate the deterministic output y ∈ R [59]. The estimator ŷ is expected to have a low variance. The features of z should be descriptive of the underlying problem at hand [11]. The accuracy and stability of estimations are assessed using the K-fold cross validation technique. The samples are split into K subsets (“folds”), where typically 3 ≤ K ≤ 10. The machine is trained on K − 1 sets and tested on the K-th set in a round-robin fashion [59]. Each fold induces a specific error quantified as the average of the absolute values of the differences between the predicted and the K-th set values.\nEfold = ∑N i=1 |ŷi − yi|\nN , VARfold =\n∑N i=1 (ŷi − Efold) 2\nN , (11a)\nwhere N is the number of test samples in the fold. The error is then averaged over all folds:\nE = ∑K\nfold=1 Efold K\n, VAR = ∑K fold=1 (Efold − E) 2\nK . (11b)\nThe variance of the prediction results (11a) accounts for the sensitivity of the model to the particular choice of data set. It quantifies the stability of the model in response to the new training samples. A smaller variance indicates more stable predictions, however, this sometimes translates into a larger bias of the model. Models with small variance and high bias make strong assumptions about the data and tend to underfit the truth, while models with high variance and low bias tend to overfit the truth [62] . The trade-off between bias and variance in learning algorithms is usually controlled via techniques such as regularization or bagging and boosting [11].\nIn what follows we briefly review the Gaussian process and Neural network techniques."
    }, {
      "heading" : "4.1 Gaussian process kernel method",
      "text" : "A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution [71]. A Gaussian process is fully described by its mean and covariance functions\nφ(z) ∼ gp ( m(z), k(zi, zj) ) , (12)\nwhere [71] m(z) = E [φ(z)] , k(zi, zj) = E [(φ(zi)−m(zi)) (φ(zj)−m(zj))] .\nIn this work we employ the commonly used squared-exponential-covariance Gaussian kernel with [71]:\nk(zi, zj) = σ 2 φ exp\n( − (zi − zj) 2\n2 `2\n) + σ2n δi,j , (13)\nwhere zi and zj are the pairs of data points in training or test samples, and δ is the Kronecker delta symbol. The model (13) has three hyper-parameters. The length-scale ` governs the correlation among data points. The signal variance σ2φ and the noise variance σ 2 n govern the precision of variance and noise, respectively. Consider a set of training data points z1, z2, · · · zn, and the corresponding noisy observations y1, y2, · · · yn,\nyi = φ(zi) + i, i ∼ N ( 0, σ2n ) , i = 1, . . . , n. (14)\nConsider also the set of test points z∗1, z ∗ 2, · · · , z∗m and the predictions ŷ1, ŷ2, · · · ŷm\nŷi = φ (z ∗ i ) , i = 1, . . . ,m. (15)\nFor a Gaussian prior the joint distribution of training outputs y and test outputs ŷ is:[ y ŷ ] ∼ N ([ m(z) m(z∗) ] , [ k(z, z) + σ2nI k(z, z ∗) k(z∗, z) k(z∗, z∗) ]) (16)"
    }, {
      "heading" : "4.2 Artificial Neural networks",
      "text" : "The predictive distribution represents the posterior after observing the data [11] and is given by: p (ŷ|z, y, z∗) ∼ N ( k(z∗, z) ( k(z, z) + σ2nI )−1 y , k(z∗, z∗)− k(z∗, z) ( k(z, z) + σ2nI )−1 k(z, z∗) ) . (17)\nThe prediction of Gaussian process will depend on the choice of the mean and covariance functions, and on their hyperparameters. The hyperparametrs can be inferred from the data by minimizing the marginal negative log-likelihood function θ = arg min L(θ), where\nL(θ) = − log p(y|z, θ) = 1 2 log |k (z, z) |+ 1 2 (y −m(z))T k (z, z)−1 (y −m(z)) + n 2 log (2π) ."
    }, {
      "heading" : "4.2 Artificial Neural networks",
      "text" : "The study of artificial neural networks (ANNs) begin in the 1910s in order to intimate human brain’s biological structure. Pioneering work was carried out by Rosenblatt, who proposed a three-layered network structure, the perceptron [37] . ANNs detect the pattern of data by discovering the input–output relationships. Applications include the approximation of functions, regression analysis, time series prediction, pattern recognition, and speech synthesis and recognition [46, 7].\nThe architecture of ANNs is schematically represented in Figure 1. ANNs consist of neurons and connections between the neurons (weights). Neurons are organized in layers, where at least three layers of neurons (an input layer, a hidden layer, and an output layer) are required for construction of a neural network. The input layer distributes input signals z1, z2, · · · , zn to the hidden layer. For a neural network with L hidden layers and m neurons in the hidden layer, let ŷj be the vector of outputs from layer `, b` the biases at layer `, and w`kj the weight connecting the neuron j to the kth input. Then the feed-forward operation is:\nx`+1j = ∑ k=1 w `+1 kj ŷ ` k + b `+1 j , ŷ 0 j = zj , j = 1, · · ·m\nŷ`+1j = φ ( x`+1j ) , ` = 0, 1, · · · , L− 1\nThe differentiable function φ is the transfer function and can be log-sigmoid, hyperbolic tangent sigmoid, or linear transfer function.\nThe training process of ANN adjusts the weights wi,j in order to reproduce the desired outputs when fed the given inputs. The training process via the back propagation algorithm [75] uses a gradient descent method to modify weights and thresholds such that the error between the desired output and the output signal of the network is minimized [32]. In supervised learning the network is provided with samples from which it discovers the relations of inputs and outputs. The output of the network is compared with the desired output,and the error is back-propagated through the network and the weights will be adjusted. This process is repeated during several epochs, until the network output is close to the desired output [38]. In unsupervised learning a model of the data distribution is formed in order to extract significant data features.The network extracts data features without being shown a set of inputs and outputs [46]."
    }, {
      "heading" : "5 Combining Available Information for Accurate ROMs at New Parametric Configurations",
      "text" : "The POD method produces an orthogonal basis that approximately spans the state solution space of the model for a specific parameter configuration. Moving away from the initial parametric configuration requires the construction of new bases since the initial reduced order model is not accurate anymore. However, if states depend continuously on parameters, the POD basis constructed for one parameter configuration can approximately span the solution space at different parametric settings in a local vicinity.\nSeveral methods to combine the available information to generate more accurate reduced order models for new parameter configurations have been proposed in the literature. One is the interpolation of the available reduced order bases Uµj computed for the parametric"
    }, {
      "heading" : "5.1 Basis interpolation",
      "text" : "configurations µj , j = 1, ..,M . The dependence of the bases on the parameters has been modeled with various linear and nonlinear spatially-dependent interpolants.\nHere we compare the performances of different strategies that involve Lagrange interpolation of bases in the matrix space and in the tangent space of the Grassmann manifold. In addition we propose to concatenate the available reduced bases followed by an orthogonalization process, and to interpolate the solutions of the high fidelity model as a mean to derive the reduced order basis for a new parameter configuration."
    }, {
      "heading" : "5.1 Basis interpolation",
      "text" : "Lagrange interpolation of bases Assuming the reduced manifold U : P̃ → RNstate×k poses a continuous and linear dependency with respect to the parametric space, and if M discrete bases Uµj = U(µj) have been already constructed for various parametric configurations µj , j = 1, 2, ..,M , then a basis corresponding to the new configuration µ0 can be obtained using Lagrange’s interpolation formula\nUµ0 = M∑ j=1 UµjLj(µ 0), Lj(µ) = ∏ i 6=j µ− µi µj − µi . (18)\nDrawbacks of this approach include the orthogonalization requirement for the resulting interpolated basis vectors, and the lack of linear variation in the angles between pairs of subspace planes [54] spanned by the reduced bases Uµj . Differential geometry results can be employed to alleviate these deficiencies.\nGrassmann manifold In the study proposed by Amsallem and Farhat [4] basis (matrix) interpolation in the tangent space of the Grassmann manifold at a careful selected point S representing a subspace spanned by one of the available reduced bases was performed. It has been shown that Grassmann manifold can be endowed with a differentiable structure [1, 27], i.e., at each point S of the manifold a tangent space exists. The mapping from the manifold to the tangent space at S is called the logarithmic mapping, while the backward projection is referred to as exponential mapping [10]. According to [4] the construction of a new subspace Sµ0 associated with a new parameter µ0 can be obtained by interpolating the known subspaces {Si}Mi=1 spanned by the already computed reduced bases Uµi , i = 1, ..,M . The steps required by this methodology [4] are described in the Algorithm2.\nAlgorithm 2 Interpolation in a tangent space to a Grassmann manifold algorithm [4] 1: Select a point Si0 of the manifold to represent the origin point for the interpolation spanned by the basis Uµi0 . 2: The tangent space TSi0 and the subspaces {Si} M i=1 are considered. Each point Si sufficiently close to Si0 is mapped to a matrix Γi\nrepresenting a point of TSi0 using the logarithm map LogSi0 [10]\n(I − Uµi0U T µi0 )Uµi(Uµi0Uµi) −1 = RiΛiQ T i (SVD factorization),\nΓi = Ri tan −1(Λi)Q T i .\n3: Each entry of the matrix Γ0 associated with the target parameter µ0 is computed by interpolating the corresponding entries of the matrices Γi ∈ RNstate×k associated with the parameter points µi, i = 1, ..,M − 1. A univariate or multivariate Lagrange interpolation may be chosen similar with the one introduced in (18). 4: The matrix Γ0 representing a point in the tangent space TSi0 is mapped to a subspace S 0 on the Grassmann manifold spanned by a\nmatrix Uµ0 using the exponential map [10]\nΓ0 = R0Λ0Q0 T\n(SVD factorization), Uµ0 = Uµi0Q 0 cos(Λ0) +R0 sin(Λ0).\nAmsallem and Cortial [3] proved that the subspace angle interpolation [54, 53] is identical to the interpolation in a tangent space to the Grassmann manifold of two reduced-order bases, thus the latter methodology can be viewed as a generalization of the former approach."
    }, {
      "heading" : "5.2 Basis concatenation",
      "text" : "Basis concatenation idea was introduced in [93] and emerged from the notion of a global basis [89, 80]. In the global strategy, the existent high-fidelity snapshots corresponding to various parameter configurations are collected in a single snapshot matrix and then a matrix factorization is performed to extract the most energetic singular vectors. This global basis is then used to build reduced order models for parameter values not available in the initial snapshots set."
    }, {
      "heading" : "5.3 Interpolation of high-fidelity model solutions",
      "text" : "Assuming xµ1 , xµ2 ∈ RNstate×Nt are the snapshots corresponding to two high-fidelity model trajectories, the following error estimate holds [91, Proposition 2.16]:\nx̄ = [xµ1xµ2 ] = ŪΛV̄ T , (SVD factorization) (19a)\n‖x̄− Ū(:, 1 : k) x̃‖F = Nt∑\ni=k+1\nλi = O(λk+1), (19b)\nwhere λi is the ith singular value of x̄ , Ū(:, 1 : k) are the first k singular vectors of Ū and x̃ ∈ Rk×2Nt . By ‖ · ‖F we refer to the Frobenius norm. The snapshot matrix typically stores correlated data and therefore contains linearly dependent columns. For rank deficient snapshot matrices, and in the case where the reduced order bases Uµ1 and Uµ2 corresponding to the trajectories µ1 and µ2 are available, we can construct a basis Û by simply concatenating columns of Uµ1 and Uµ2 such that the accuracy level in (19) is preserved.\nProposition 5.1. Consider the following SVD expansions of rank deficient snapshots matrices xµ1 , xµ2 ∈ RNstate×Nt\nxµj = Uµj Λj V T µj , j = 1, 2. (20)\nThere are positive integers k1, k2, and x̂ ∈ R(k1+k2)×2Nt , such that x̄ defined in (19) satisfies\n‖x̄− Û x̂‖F = O(λk+1), (21)\nwhere λk+1 is the (k + 1)-st singular value of snapshots matrix x̄, and Û = [Uµ1(:, 1 : k1) Uµ2(:, 1 : k2)] ∈ RNstate×(k1+k2).\nProof. Since xµ1 , xµ2 ∈ RNstate×Nt are rank deficient matrices, there exist at least two positive integers k1 and k2, such that the singular values associated with xµ1 and xµ2 satisfy\nλ1k1+1, λ 2 k2+1 ≤ λk+1 2 , ∀k = 0, .., Nt− 1. (22)\nNext, from [91, Proposition 2.16] and (20) we have the following estimates:\n‖xµj − Uµj (:, 1 : kj) x̃j‖F = Nt∑\ni=kj+1\nλji = O(λ j kj+1 ), (23)\nwhere λji is the i th singular value of xµj and x̃j ∈ Rkj×Nt , for j = 1, 2.\nBy denoting\nx̂ = [ x̃1 01 02 x̃2 ] ,\nwhere the null matrix 0j belongs to Rkj×Nt , j = 1, 2, we have\n‖x̄− Û x̂‖F = ‖[xµ1 xµ2 ]− [Uµ1(:, 1 : k1)x̃1 Uµ2(:, 1 : k2)x̃2]‖F ≤ (24) ‖x̄µ1 − Uµ1(:, 1 : k1)x̃1‖F + ‖x̄µ2 − Uµ2(:, 1 : k2)x̃2‖F ≤ O(λ1k1+1) +O(λ 2 k2+1) = O(λk+1). (25)\nIt is crucial for the proof that Uµ1 and Uµ2 are rank deficient since they have at least one null singular value. In practice usually k1 + k2 is larger than k thus more bases functions are required to form Û to achieve a similar level of precision as in (19) where Ū is built using a global singular value decomposition. This matrix factorization of x̄ is more costly than both singular value decompositions of matrices xµj , j = 1, 2, (23) combined for large space dimension Nstate. However the off-line stage of the concatenation method also includes the application of a Gramm-Schmidt-type algorithm to orthogonalize the overall set of vectors in Û .\nFor full rank matrices the precision is controlled by the spectra of snapshots matrices Uµ1 and Uµ2 but there is no guarantee that the concatenated basis Û can provide similar accuracy precision as Ū in (19) for all k = 1, 2, .., Nt.\nWhile the Lagrange interpolation of bases mixes the different energetic singular vectors in an order dictated by the singular values magnitude, this strategy concatenates the dominant singular vectors for each case and preserves their structure."
    }, {
      "heading" : "5.3 Interpolation of high-fidelity model solutions",
      "text" : "The method discussed herein assumes that the model solution dependents continuously on the parameters. Thus it is natural to consider constructing the basis for a new parameter configuration by interpolating the existent high-fidelity model solutions associated with\nvarious parameter settings, and then performing a SVD factorization of the interpolated results. For example, the Lagrange solution interpolant is given by\nxµ0 = M∑ j=1 xµjLj(µ 0), (26)\nwhere xµj ∈ RNstate×Nt is the model solution corresponding to parameter µj and the interpolation polynomials are defined in (18). A new basis is constructed from the interpolated model solution (26) for the new parametric configuration µ0. By integrating the corresponding reduced order model the output projected solution will present variations in comparison with the high-fidelity interpolation solution xµ0 thus the nonlinear dynamics of the model influence the final solution too.\nFrom computational point of view the complexity of the off-line stage of the solution interpolation method (26) is smaller than in the case of the bases concatenation and interpolation approaches. Only one singular value decomposition is required in contrast with the multiple factorizations needed in the latter two strategies where the involved matrices have the same size Nstate ×Nt. Having only Nt snapshots the size of the outcome basis should be smaller than in the case of basis concatenation approach.\nIf for each model solution xµj a singular value decomposition is available such that\nxµj ≈ Uµj x̃j, j = 1, 2, ..,M, (27)\nthen from (26) we get\nxµ0 ≈ M∑ j=1 Lj(µ 0)Uµj x̃j . (28)\nNow the basis Uµ0 associated with the new configuration is obtained following a matrix factorization, i.e..\nM∑ j=1 Lj(µ 0)Uµj x̃j = Uµ0Sµ0V T µ0 . (29)\nIn the basis interpolation method (18), the basis Uµ0 (denoted in (30) by Ūµ0 to differentiate it from the basis described in equation (29)) corresponding to the new parametric setting µ0 is derived from the following\nM∑ j=1 Lj(µ 0)Uµj = Ūµ0 S̄µ0 V̄ T µ0 . (30)\nWhile there is a close relationship between the philosophy behind the solution interpolation method and the bases interpolation strategy, the algebraic formulations of the bases Uµ0 (29) and Ūµ0 (30) are significantly different. Consequently an assumption of linear variation of the basis over the parametric interval does not imply that the high-fidelity solution varies linear over the parametric domain. The inverse proposition does not hold neither thus the choice of method depends on the model under study and its input."
    }, {
      "heading" : "6 Numerical Experiments",
      "text" : "We illustrate the application of the proposed machine learning methodologies to the construction of error models for the reduced order models solutions for a one-dimensional Burgers model and their subsequent utilizations. The model proposed herein is characterized by two scalar parameters, but the envisioned parametric map is designed to cover variation in the viscosity coefficient space only. For each of the problems introduced in Section 3 (constructing the parametric map, selecting the best two already existing bases for a new parameter configuration from accuracy point of view, and selecting the dimension of the reduced basis) we present in detail the proposed solution approaches and the corresponding numerical results.\nTo assess the performance of probabilistic models we employ various cross validation tests. The dimensions of the training and testing data sets are chosen empirically based on the number of samples. For artificial neural network models the number of hidden layers and neurons in each hidden layer vary for each type of problems under study. The squared-exponential-covariance kernel (13) is used for Gaussian process models."
    }, {
      "heading" : "6.1 One-dimensional Burgers’ equation",
      "text" : "Burgers’ equation is an important partial differential equation from fluid mechanics [14]. The evolution of the velocity u of a fluid evolves according to\n∂u ∂t + νu ∂u ∂x = µ\n∂2u ∂x2 , x ∈ [0, L], t ∈ (0, tf], (31)\nwith tf = 1 and L = 1. Here µ is the viscosity coefficient.The parameter ν has no physical significance and it is used to control the non-linear effect of the advection term. The model has homogeneous Dirichlet boundary conditions u(0, t) = u(L, t) = 0, t ∈ (0, tf]. A smooth initial condition is used, described by a seventh degree polynomial and shown in Figure 2."
    }, {
      "heading" : "6.2 Designing the parametric map",
      "text" : "The discretization uses a spatial mesh of Ns = 201 equidistant points on [0, L], with ∆x = L/(Ns − 1). A uniform temporal mesh withNt = 301 points covers the interval [0, tf], with ∆t = tf/(Nt−1). The discrete velocity vector is u(tj) ≈ [u(xi, tj)]i=1,2,..,Nstate ∈ RNstate , N = j = 1, 2, ..Nt, where Nstate = Ns − 2 (the known boundaries are removed). The semi-discrete version of the model (31) is:\nu′ = −νu Axu + µAxxu, (32)\nwhere u′ is the time derivative of u, andAx, Axx ∈ RNstate×Nstate are the central difference first-order and second-order space derivative operators, respectively, which take into account the boundary conditions too. The model is implemented in Matlab and the backward Euler method is employed for time discretization. The nonlinear algebraic systems are solved using Newton-Raphson method and the allowed number of Newton iterations per each time step is set to 50. The solution is considered accurate enough when the euclidian norm of the residual is less then 10−10.\nThe viscosity parameter space is set to the interval [0.01, 1]. Smaller values of µ correspond to sharper gradients in the solution, and lead to a dynamics that is more difficult to approximate by a reduced order model."
    }, {
      "heading" : "6.2 Designing the parametric map",
      "text" : "We seek to build a parameter map for the 1D-Burgers model for the viscosity domain consisting in the interval [0.01, 1]. The nonphysical parameter ν is set to 1. As discussed in Section 3, we take the following steps. First we construct probabilistic models to approximate the error of reduced order solution. Next, we identify “µ-feasible” intervals [d`, dr] in the parameter space such that local reduced order model depending only on the high-fidelity trajectory at µ is accurate within the prescribed threshold for any µ0 ∈ [d`, dr]. Finally, a greedy algorithm generates the parametric map by covering the parameter space with a union of µi feasible intervals.\n[0.01, 1] ⊂ M⋃ i=1 [ di`, d i r ] , (33)\nwhere each µi-feasible interval is characterized by an error threshold ε̄i (which can vary from one interval to another). This relaxation is suggested since for intervals associated with small parameters µ, it is difficult to achieve small reduced order models errors similar to those obtained for larger parametric configurations. One way to mantain the error thresholds constant is to start with a larger proposal.\nIn existing reduced basis methods a global reduced order model depending on multiple high-fidelity trajectories is constructed. In contrast, our approach decomposes the parameter space into smaller regions where the local reduced order model solutions are accurate within some tolerance levels. Since the local bases required for the construction of the local reduced order models depend on only a single full simulation, the size of the reduced manifolds is small, leading to lower on-line computational complexity."
    }, {
      "heading" : "6.2.1 Error estimation of ROM solutions",
      "text" : "We first construct probabilistic models φ : z→ ε̂, (34)\nwhere the input features z include a new viscosity parameter value µ0, a parameter value µ associated with the full model run that generated the basis Uµ, and the dimension of the reduced manifold. The target ε̂ is the estimated error of the reduced order model solution at µ0 using the basis Uµ and the corresponding reduced operators computed using the Frobenius norm\nε = ‖[x(µ0, ti)]i=1,..,Nt − Uµ[x̃(µ0, µ, ti)]i=1,..,Nt‖F . (35)"
    }, {
      "heading" : "6.2 Designing the parametric map",
      "text" : "The training data set includes equally distributed values of µ and µ0 over the entire interval, µ ∈ {0.1, 0.2, . . . , 0.9, 1} and µ0 ∈ {0.01, 0.02, . . . , 0.99, 1}, respectively, reduced basis dimensions spanning the interval [4, 5, . . . , 14, 15] and the reduced order model error ε. The entire training data set contains nearly 12, 000 samples, and for each sample a high-fidelity model solution is calculated. Figure 3(b) shows isocontours of the reduced order model error ε for viscosity parameter values µ0 and various POD basis dimensions. The design of the reduced order models relies on the high-fidelity trajectory for µ = 0.8. Since target values ε vary over a wide range (from 300 to 10−6) we consider the logarithms of the errors log(ε) to decrease the variance of the predicted results, i.e.\nφ : z→ l̂og(ε). (36)\nFigure 3(a) shows isocontours for the logarithms of the errors log(ε).\nWe construct two probabilistic models for estimating the ROM model errors, the first one uses a Gaussian process with a squaredexponential covariance kernel (13) and the second one uses a neural network with six hidden layers and hyperbolic tangent sigmoid activation function in each layer. Tables 1 and 2 show the averages and variances of errors in prediction provided by GP and ANN for different sample sizes. The results are obtained using a conventional validation with 80% of the whole data set involved in training process and the remaining 20% employed for testing. The misfit is computed using the same formulas presented in (11a) to evaluate the prediction errors of one-fold set in the K-fold cross validation approach. Table 2 shows the prediction errors of (34) computed via equation (11a) with y = ε and ŷ = ε̂, i.e. no data scaling; the predictions have a large variance and a low accuracy. Scaling the data and targeting log(ε) results using (36), reduce the variance of the predictions, and increase the accuracy, as shown in Table 1. The same formula (11a) with y = log(ε) and ŷ = l̂og(ε) was applied. In both cases ANN outperforms the GP. Moreover, as the number of data points grows, the accuracy increases and the variance decreases faster for ANN.\n6.2 Designing the parametric map\nFigures 4 and 5 show the corresponding histogram of the predicted models errors (36) and (34) using 100 and 1000 training samples for both ANN and GP models. As the number of training samples increase, the uncertainty in the prediction decreases. The histograms can also asses the validity of GP assumptions (16), (12), (14). The difference between the true and estimated values should behave as samples from the distribution N (0, σ2n) [25]. In our case they are hardly normally distributed and this indicates that the data set for the problems we are working with, are not from Gaussian distributions.\nScaling the data and targeting log ε errors clearly improve the performance of our probabilistic models. Consequently for the rest of the manuscript we will only use model (36). To asses the quality of the probabilistic models a five-fold cross-validation process is also used. The results computed using formula (11b) are shown in Table 3. Neural network outperforms the Gaussian process and estimates the errors more accurately. It also has less variance than the Gaussian process which indicates it has more stable predictions."
    }, {
      "heading" : "6.2 Designing the parametric map",
      "text" : "Figure 6 illustrates the average of errors in predictions over five different ANN and GP configurations. In each configuration, the machine is trained on random 80% split of data set and tested on the fixed selected test data shown in figure 6. Getting the average of predictions on different trained models decreases the bias in predictions."
    }, {
      "heading" : "6.2 Designing the parametric map",
      "text" : ""
    }, {
      "heading" : "6.2.2 Construction of a µ−feasible interval",
      "text" : "We saw in the previous subsection that probabilistic models can accurately estimate the error ε (35) associated with reduced order models. Thus we can employ them to establish a range of viscosity parameters around µ such that the reduced order solutions depending on Uµ satisfy some desired accuracy level. More precisely, starting from parameter µ, a fixed POD basis dimension and a tolerance error log(ε̄), we are searching for an interval [dl, dr] such that the estimated prediction l̂og(ε) of the true error log(ε) (35) meets the requirement\nl̂og(ε) < log(ε̄),∀µ0 ∈ [dl, dr]. (37)\nOur proposed strategy makes use of a simply incremental approach by sampling the vicinity of µ to account for the estimated errors l̂og(ε) forecasted by the probabilistic models defined before. A grid of new parameters µ0 is build around µ and the machines predict the errors outward of µ. Once the machines outputs are larger than the prescribed error log(ε̄), the previous µ0 satisfying the constrain (37) is set as dl, for µ0 < µ or dr for µ0 > µ.\nFigure 7 illustrates the range of parameters estimated by the neural network and Gaussian process against the true feasible interval and the results show good agreement. For this experiment we set µ = 0.7, dimension of POD=9 and error threshold ε̄ = 10−2. Values of µ0 = µ ± 0.001 · i, i = 1, 2, .. are passed to the probabilistic models. The average range of parameters obtained over five different configurations with neural network is [0.650, 0.780] while in the case of Gaussian process we obtained [0.655, 0.780]. In each configuration, we train the model with 80% random split of the data set and test it over the fixed test set of figure 7. For this design, the true range of parameters is [0.650, 0.785] underlying the predicting potential of machine learning models."
    }, {
      "heading" : "6.2 Designing the parametric map",
      "text" : ""
    }, {
      "heading" : "6.2.3 The parametric map as a reunion of µ−feasible intervals",
      "text" : "A reunion of different µk-feasible intervals can be designed to cover a general entire 1D-parametric domain [A,B]. We refer to this reunion as a parametric map and once such construction is available will allow for reduced order simulations with a-priori error quantification for any value of viscosity parameter µ0 ∈ [A,B].\nA greedy strategy is described in Algorithm 3 and its output is a collection of feasible intervals ∪nk=1[dkl , dkr ] ⊃ [A,B]. Each interval [dkl , d k r ] is associated with some accuracy threshold ε̄k. For small viscous parametric values we found out that designing µk−feasible intervals associated with higher precision levels (i.e. very small thresholds ε̄k) is impossible since the dynamics of parametric 1DBurgers model solutions changes dramatically with smaller viscosity parameters. In consequence we decided to let ε̄k vary along the parametric domain to accommodate the solution physical behaviour. Thus a small threshold ε̄0 will be initially set and as we will advance charting the parameter interval [A,B] from right to left, the threshold ε̄k will be increased.\nThe algorithm starts by selecting the first centered parameter µ0 responsible for basis generation. It can be set to µ0 = B but may take any value in the proximity of B, µ0 ≤ B. This choice depends on the variability of parametric solutions in this domain region and by electing µ0 to differ from the right edge of the domain, the number n of feasible intervals will be decreased.\nThe next step is to set the threshold ε̄0 along with the maximum permitted size of the initial feasible interval to be constructed. This is set to 2 · r0, thus r0 can be referred as the interval radius. Along with the radius, the parameter ∆s will decide the maximum number of probabilistic model calls employed for the construction of the µ0-feasible interval. While the radius is allowed to vary during the algorithm iterations, ∆s is kept constant. Finally the dimension of POD basis has to be selected together with three parameters β1, β2 and β3 responsible for changes in the threshold, radius and selecting a new parameter location µk encountered during the procedure.\nThe instructions between lines 5 and 20 generate the µk-feasible interval, for the case when the current centered parameter µk represents an interior point of [dkl , d k r ]. For situation when µk = d k l or µk = d k l , the threshold has to be increased (by setting ε̄k+1 = β1ε̄k at line 22), since the reduced order model solutions can not satisfy the desired precision according to the estimated probabilistic errors. At this stage the radius is decreased too since we reached a parameter region where model dynamics changes rapidly and a larger feasible interval is not possible. Once the new centered parameter µk+1 is proposed the algorithm checks if the following constrain is satisfied\n[dk+1l , d k+1 r ] ⋂( k⋃ i=1 [dil, d i r] ) 6= ∅. (38)\nThis is achieved by checking the estimated reduced order model solution error at dkl using the basis defined by the high-fidelity trajectory at µk+1 (see instruction 25). If the predicted error is smaller than the current threshold, assuming a monotonically increasing error with larger distances d(µ0, µk+1), the reduced order model solutions should satisfy the accuracy threshold for all µ0 ∈ [µk+1, dkl ]. In consequence the equation (38) will be satisfied for the current µk+1, if we set rk+1 = µk+1 − dkl (see instruction 28). In the case the error estimate is larger than the present threshold, the centered parameter µk+1 is updated to the middle point between old µk+1 and dkl . For the situation where the monotonic property of the error does not hold in practice, a simply safety net is used at instruction 12. The entire algorithm stops when µk ≤ A."
    }, {
      "heading" : "6.2 Designing the parametric map",
      "text" : "For our experiments we set A = 0.01, B = 1, ε̄0 = 1.e− 2, ∆s = 5.e− 3, r0 = 0.5, dim = 9, β1 = 1.2, β2 = 0.9 and β3 = 1.4. We initiate the algorithm by setting µ0 = 0.87, and the first feasible interval [0.7700, 1.0500] is obtained. Next the algorithm selects µ1 = 0.73 with the associated range of [0.6700, 0.8250] using the same initial threshold level. As we cover the parametric domain from right to left, i.e. selecting smaller and smaller parameters µk, the algorithm enlarges the current threshold ε̄k, otherwise the reduced order models would not satisfy the initial precision. We continue this process until we get the threshold 6.25 with µ32 = 0.021 and the corresponding feasible interval [0.00940, 0.039]. The obtained parametric map is depicted in Figure 8 where the associated threshold varies with the parameter change.\nAlgorithm 3 Generation of parametric map for reduced order models usage 1: Select µ0 as the right edge of the parameter interval, i.e. µ0 = B. 2: Set error threshold ε̂0, step size ∆s for selection of new parameter locations µ0, the maximum search radius r0, dimension of POD\nbasis dim and β1, β2 and β3. 3: Set k = 0. 4: DO 5: FOR i=1 to int( rk∆s ) 6: Set µo+ = µk + i∆s 7: IF φ(µo+, µk, dim) > log(ε̄k) THEN 8: Set dkr = µk + (i− 1)∆s. EXIT. 9: END IF\n10: END FOR 11: IF k > 0 THEN 12: IF dkr < d k−1 l THEN µk = µk+d k−1 l\n2 . GOTO 5. 13: END IF 14: END IF 15: FOR j = 1 to int( rk∆s ) 16: Set µo− = µk − j∆s 17: IF φ(µo−, µk, dim) > log(ε̄k) THEN 18: Set dkl = µk − (j − 1)∆s. EXIT. 19: END IF 20: END FOR 21: IF (i=1).OR.(j=1) THEN 22: Set ε̄k = β1 · ε̄k; rk = β2 · rk; GOTO 5. 23: ELSE µk+1 = µk − β3(j − 1)∆s; ε̄k+1 = ε̄k. 24: END IF 25: WHILE φ(dkl , µk+1, dim) > log(ε̄k+1) DO 26: µk+1 = µk+1+d k l\n2 . 27: END WHILE 28: Set rk+1 = µk+1 − dkl . 29: k = k + 1. 30: WHILE µk ≥ A THEN STOP."
    }, {
      "heading" : "6.3 Select the best already existing ROMs for a new parameter value µ0",
      "text" : ""
    }, {
      "heading" : "6.3 Select the best already existing ROMs for a new parameter value µ0",
      "text" : "An important and practical concern associated with the reduced order models is their lack of robustness with respect to parameter change. Here, we propose to solve another practical problem, i.e. giving a collection of reduced bases computed at various locations in the parameter space, find the one that proposes the most accurate reduced order solution for a new viscosity parameter µ0. We will rely on similar probabilistic models built in subsection 6.2.1. The input features for the GP and ANN are the new parameter µ0, a parameter µ whose corresponding trajectory is used as snapshots for generating basis Uµ and the dimension of the reduced manifold. The target random variable ŷ = ̂log ε is the estimated log of error of the reduced order model solution at µ0 using the basis Uµ. For our experiments, approximately 12, 000 samples were generated and include different values of POD basis dimensions from 4 to 15, new viscosity parameter µ0 ∈ [0.1, 1] and parameters µk ∈ [ 10−2, 1 ] equally distributed with interval 0.1 and 0.01, respectively and the corresponding log of the Frobenius norm of true ROM errors log(ε) (35). The data set constructed for this problem is similar to the one employed for designing the errors models. For that problem we fixed the parameter µ associated with the high-fidelity trajectory used for basis generation and computed the ROM errors corresponding to different µ0. Here we fixed µ0 and then computed the ROM errors using different bases Uµ.\nThe data set is randomly partitioned into 5 equal size sub-samples and a 5− fold cross-validation process is used to asses the quality of the probabilistic models. A neural network with 6 hidden layers and hyperbolic tangent sigmoid activation function in each layer is used while for the Gaussian process we have employed the squared-exponential-covariance kernel (13). Table 4 shows numerical results obtained with Neural networks and Gaussian process, over five folds using (11b). The Gaussian process outperforms the neural network and has less variance which indicates the GP is more accurate and stable for this specific problem than neural network.\nThe results in Figure 9 illustrate the errors in prediction of the reduced order models errors for two viscosity parameters µ0 = 0.35 and 0.65 and various bases represented along the y axis. The mean of the estimates in the case of the ANN are closer than the true errors in comparison with the output of the Gaussian Process for this particular example.\nMoreover we can notice that the estimation curves are crossing initially close to µ = 0.45. It suggests that one can choose the high-fidelity trajectory µ = 0.45 to construct a reduced order basis such that to obtain similar accuracy levels for both reduced order solutions computed at new viscosity parameters µ0 = 0.35 and 0.65. This reveals the non-monotonic property of the reduced order model error with respect to the distance d(µ0, µ)."
    }, {
      "heading" : "6.4 Combining Available Information for Accurate ROMs at New Parametric Configurations",
      "text" : ""
    }, {
      "heading" : "6.4 Combining Available Information for Accurate ROMs at New Parametric Configurations",
      "text" : "Experiments in subsection 6.3 revealed the potential of probabilistic models to select a hierarchy of reduced manifolds that produces higher accurate solutions. Figure 9 depicts the accuracy of reduced order models for µ0 = 0.35 and 0.65 using POD basis computed at various locations in the parameter interval. Assuming only 10 existing POD subspaces constructed for parameters equally distributed along the interval [0.1, 1], µ = 0.1, 0.2, 0.3, .., figure 9 shows that the most accurate reduced solutions for µ0 = 0.35 are produced using the bases computed at µ1 = 0.3 and µ2 = 0.4. Consequently, the numerical experiments described here focus on the construction of a POD basis for µ0 = 0.35 by combining the data available for µ1 = 0.3 and µ2 = 0.4.\nThe performances of the discussed methods (bases concatenation, Lagrange interpolation of bases in the matrix space and in the tangent space of the Grassmann manifold, Lagrange interpolation of high-fidelity solutions) are shown in the case of three main experiments: variation in the final time tf , in the non-linear advection coefficient ν and POD basis size. The first two experiments scale the time and space and modify the linear and nonlinear characteristics of the model. For example, in the case of a tiny small final time and advection coefficient, the diffusion linear part represents the main dynamical engine of the model thus it behaves linearly. The results are compared against reduced order models constructed using Uµ1 and Uµ2 , respectively.\nThe experiments make use of a space mesh of 201 points while 301 time steps are used. Figure 10 illustrates the Frobenius norm error between the high fidelity and reduced order model solutions for the final time tf = 0.01. Panel (a) presents the accuracy results as a function of the advection coefficient ν. Interpolating the high-fidelity solutions leads to the most accurate reduced order model. For large advection coefficients all of the methods suffer accuracy losses. Among the potential explanations we include the constant size of the POD basis and its linear dependence on the viscosity parameter assumed by all of the methods in various forms. Keeping the POD basis size constant represents a source of errors as seen in Figure 3 where the viscosity parameter is varied.\nBy interpolating the geometry characteristics of the reduced subspaces via subspace angle interpolation or Grassmann manifold approach (only one is shown in the figures since for interpolating two bases the methods are the same) we expect to obtain more accurate reduced order models. While Lagrangian interpolation of the bases is performed in both matrix space and tangent space of the Grassmann manifold (shown in cyan and green in Figure 10, the later approach performs better in this scenario confirming our expectations. The concatenation of bases using Gram-Schmidt was successful only for larger advection coefficients (red curve in Figure 10)(a) for a POD size set to 14.\nIncreasing the dimension of the basis enhances the so called Gram-Schmidt reduced order model solution accuracy for ν = 1 (see Figure 10(b)). For this case Lagrange interpolation in the matrix space shows better performances in comparison with the output of the Grassmann manifold approach.\nNext we increase the nonlinearity characteristics of the model by setting the final time to tf = 1 and Figure 11 illustrates the Frobenius norm errors as a function of the advection coefficient ν and POD dimension size. The errors produced by the reduced order model derived via Grassmann manifold method are similar with the ones obtained by the surrogate model relying on a POD basis computed via the Lagrange interpolation of the high-fidelity model solutions.\nThe Lagrange interpolation of bases in the matrix space is not successful as seen in both panels of figure 11. Increasing the POD size to 20, the Gram-Schmidt approach enhance the accuracy of the solution (see Figure 11(b))."
    }, {
      "heading" : "6.5 Optimal size of reduced order model",
      "text" : ""
    }, {
      "heading" : "6.5 Optimal size of reduced order model",
      "text" : "Here we propose two alternative approaches to select the reduced basis size that accounts for specified accuracy levels in the reduced order model solutions. These techniques employ construction of probabilistic models φ : z → ŷ via neural network and Gaussian process as stated in section 4. The input features z for this problem consist of the viscosity parameter µ ∈ [0.01, 1] and the log of the Frobenius norm of the error between the high-fidelity and reduced order models log(ε) (35). The searched output ŷ is the dimension of the reduced manifold d. For the training phase, the data set is generated using several runs of the 1D-Burgers model with various viscosity parameters µ, different basis sizes d and the log of the Frobenius norms of the discrepancies between the full and the projected reduced order solutions log(ε). The machines will learn the sizes of reduced order basis d associated with the parameter µ and the corresponding log(ε). Later it will be able to estimate the proper size of reduced basis by providing it the specific viscosity parameter µ and the desired error log(ε). The computational cost is low once the probabilistic models are constructed. The output indicates the dimension of the reduced manifold for which the ROM solution satisfies the corresponding error threshold. Thus we do not need to compute the entire spectrum of the snapshots matrix in advance which for large spatial discretization meshes translates into important computational costs reduction.\nFor our experiments, approximately 9000 samples were generated for different values of the viscosity parameter µ equally distributed within the interval [10−2, 1], various reduced basis dimensions from 4 to 15 and the corresponding log(ε). Figure 12 illustrates the contours of the log of reduced order model error, over the viscosity parameter domain and various POD sizes.\nA neural network with 5 hidden layers and hyperbolic tangent sigmoid activation function in each layer is used while for the Gaussian process we have used the squared-exponential-covariance kernel (13). Table 5 show the average and variance of error in GP and ANN estimations using different sample sizes. The ANN outperforms the GP and as the number of data points grows, the accuracy increases"
    }, {
      "heading" : "6.5 Optimal size of reduced order model",
      "text" : "and the variance decreases. The results are obtained using a conventional validation with 80% of the sample size dedicated for training data and the other 20% for the test data. The employed formula is described in equation (11a).\nFigures 13 and 14 shows the corresponding errors in estimation on 100 and 1000 training samples for both ANN and GP model. These histograms as stated before, can assess the validity of GP assumptions. The data set distribution shape is closer to a Gaussian profile than in the case of the data set distribution discussed in section 6.2.1 used for generation of reduced order model error probabilistic models.\nTo assess the accuracy of the probabilistic models, the data set is randomly partitioned into 5 equal size sub-samples, and 5− fold cross-validation test is implemented. The 5 results from the folds are averaged and they are presented in table 6. The neural network model correctly estimated the size of the reduced manifold in 87% cases. Gaussian process correctly estimates the POD size 53% of the times. The variance results shows that the GP model has more stable predictions indicating a higher bias in the outcomes.\nIn figure 15, we compare the output of our probabilistic approaches against the eigenvalues estimation on a set of randomly selected"
    }, {
      "heading" : "6.5 Optimal size of reduced order model",
      "text" : "test data. The eigenvalue estimation is the standard method for selecting the optimal reduced manifold dimension when a prescribed level of accuracy of the reduced solution is desired. Here the desired accuracy ε was set to 10−3. The mismatches between the predicted and true dimensions are depicted in the figure 15. The predicted values are the averages over five different models where each model of ANN and GP are trained on random 80% split of dataset and tested on the fixed selected 20% test data. We notice that the snapshots matrix spectrum underestimates the true size of the manifold as expected since the errors due to integration in the reduced space are not accounted. The neural network predictions were extremely accurate for most of the samples while the Gaussian process usually overestimated the reduced manifold dimensions."
    }, {
      "heading" : "7 Conclusions",
      "text" : "This work demonstrates the value of machine learning approaches to guide the construction of parametric space partitioning for the usage of efficient and accurate local reduced order models. While the current methodologies are defined in the sense of Voronoi tessellation [26] and rely on K-means algorithms [5, 6, 47], our approach delimitates sub-regions of the parametric space by making use of Gaussian Processing and Artificial Neural Networks models for the errors of reduced order models and parametric domain sampling. The employed machine learning models differ from the one proposed in [25] having more additional features such as reduced subspace dimension and are specially projected for accurate predictions of local reduced order models errors. For each sub-region, an associated reduced order basis and operators are constructed depending on a single representative high-fidelity trajectory and, the corresponding local reduced order models solutions have known precision levels. The novel methodology is applied for a 1D-Burgers model, and a parametric map covering the viscosity domain with parametric sub-intervals and associated errors thresholds is designed.\nOur numerical experiments revealed the non-monotonic property of the reduced order model error with respect to the distance between the current parametric configuration and the one used to generate the reduced subspace. Thus we proposed machine learning models for selecting a hierarchy of reduced bases producing the most accurate solutions for a new parameter configuration. Based on this hierarchy, three already existing methods involving bases interpolation and concatenation and high-fidelity model solutions interpolation are applied to enhance the quality of the associated reduced order model solutions. It has been shown that the assumption of linear variation of the basis over the parametric space leads to a different reduced basis formulation than if the linear variation hypothesis of the high-fidelity solution over the parametric domain is followed. Several experiments were performed by scaling the time and space and modifying the nonlinear characteristics of the model. In most cases, interpolating the already existing high-fidelity trajectories generated the most accurate reduced order models for a new viscous parameter revealing that the solution behavior over the parametric region under study can be linearly approximated. Lagrange interpolation of bases in the tangent space of the Grassmann manifold and concatenation of bases for larger reduced subspaces showed also good performances.\nFinally we addressed the problem of selecting the dimension of the reduced order model when its solution must satisfy a desired level of accuracy. Our approach based on learning better estimates the ROM basis dimension in comparison with the results obtained by truncating the spectrum of the snapshots matrix.\nA future goal is to decrease the computational complexity of the parametric map design procedure. Currently the training data required by the probabilistic models relies on many high-fidelity simulations. By employing fast a-posteriori error estimation results [63], this dependency will be much decreased. In addition we plan to incorporate residual norm and rigorous error bounds among the data fitting models features to enhance their prediction capabilities as remarked in [25]."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part and by the award NSF CCF 1218454 and by the Computational Science Laboratory at Virginia Tech."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Reduced order models are computationally inexpensive approximations that capture the important dynamical characteristics of large, high-fidelity computer models of physical systems. This paper applies machine learning techniques to improve the design of parametric reduced order models. Specifically, machine learning is used to develop feasible regions in the parameter space where the admissible target accuracy is achieved with a predefined reduced order basis, to construct parametric maps, to chose the best two already existing bases for a new parameter configuration from accuracy point of view and to pre-select the optimal dimension of the reduced basis such as to meet the desired accuracy. By combining available information using bases concatenation and interpolation as well as high-fidelity solutions interpolation we are able to build accurate reduced order models associated with new parameter settings. Promising numerical results with a viscous Burgers model illustrate the potential of machine learning approaches to help design better reduced order models. key words reduced order models, high-fidelity models, data fitting, machine learning, feasible region of parameters, local reduced order models.",
    "creator" : "LaTeX with hyperref package"
  }
}