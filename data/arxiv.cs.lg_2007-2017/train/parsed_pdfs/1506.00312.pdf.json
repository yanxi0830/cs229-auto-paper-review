{
  "name" : "1506.00312.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Copeland Dueling Bandits",
    "authors" : [ "Masrour Zoghi", "Zohar Karnin", "Shimon Whiteson", "Maarten de Rijke" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The dueling bandit problem [1] arises naturally in domains where feedback is more reliable when given as a pairwise preference (e.g., when it is provided by a human) and specifying real-valued feedback instead would be arbitrary or inefficient. Examples include ranker evaluation [2–4] in information retrieval, ad placement and recommender systems. As with other preference learning problems [5], feedback consists of a pairwise preference between a selected pair of arms, instead of scalar reward for a single selected arm, as in the K-armed bandit problem.\nMost existing algorithms for the dueling bandit problem require the existence of a Condorcet winner, which is an arm that beats every other arm with probability greater than 0.5. If such algorithms are applied when no Condorcet winner exists, no decision may be reached even after many comparisons. This is a key weakness limiting their practical applicability. For example, in industrial ranker evaluation [6], when many rankers must be compared, each comparison corresponds to a costly live experiment and thus the potential for failure if no Condorcet winner exists is unacceptable [7].\nThis risk is not merely theoretical. On the contrary, recent experiments on K-armed dueling bandit problems based on information retrieval datasets show that dueling bandit problems without Condorcet winners arise regularly in practice [8, Figure 1]. In addition, we show in Appendix C.1 that there are realistic situations in ranker evaluation in information retrieval in which the probability that the Condorcet assumption holds decreases rapidly as the number of arms grows. Since the K-armed dueling bandit methods mentioned above do not provide regret bounds in the absence of a Condorcet winner, applying them remains risky in practice. Indeed, we demonstrate empirically the danger of applying such algorithms to dueling bandit problems that do not have a Condorcet winner (cf. Appendix A).\nThe non-existence of the Condorcet winner has been investigated extensively in social choice theory, where numerous definitions have been proposed, without a clear contender for the most suitable resolution [9]. In the dueling bandit context, a few methods have been proposed to address this issue, e.g., SAVAGE [10], PBR [11] and RankEl [12], which use some of the notions proposed by social choice theorists, such as the Copeland score or the Borda score to measure the quality of each arm, hence determining what constitutes the best arm (or more generally the top-k arms). In this paper, we focus on finding Copeland winners, which are arms that beat the greatest number of other arms, because it is a natural, conceptually simple extension of the Condorcet winner.\nar X\niv :1\n50 6.\n00 31\n2v 1\n[ cs\n.L G\n] 1\nJ un\n2 01\nUnfortunately, the methods mentioned above come with bounds of the form O(K2 log T ). In this paper, we propose two new K-armed dueling bandit algorithms for the Copeland setting with significantly improved bounds.\nThe first algorithm, called Copeland Confidence Bound (CCB), is inspired by the recently proposed Relative Upper Confidence Bound method [13], but modified and extended to address the unique challenges that arise when no Condorcet winner exists. We prove anytime high-probability and expected regret bounds for CCB of the form O(K2 + K log T ). Furthermore, the denominator of this result has much better dependence on the “gaps” arising from the dueling bandit problem than most existing results (cf. Sections 3 and 5.1 for the details).\nHowever, a remaining weakness of CCB is the additiveO(K2) term in its regret bounds. In applications with large K, this term can dominate for any experiment of reasonable duration. For example, at Bing, 200 experiments are run concurrently on any given day [14], in which case the duration of the experiment needs to be longer than the age of the universe in nanoseconds before K log T becomes significant in comparison to K2.\nOur second algorithm, called Scalable Copeland Bandits (SCB), addresses this weakness by eliminating the O(K2) term, achieving an expected regret bound of the form O(K logK log T ). The price of SCB’s tighter regret bounds is that, when two suboptimal arms are close to evenly matched, it may waste comparisons trying to determine which one wins in expectation. By contrast, CCB can identify that this determination is unnecessary, yielding better performance unless there are very many arms. CCB and SCB are thus complementary algorithms for finding Copeland winners.\nOur main contributions are as follows: 1. We propose two new algorithms that address the dueling bandit problem in the absence of a Condorcet winner, one\ndesigned for problems with small numbers of arms and the other scaling well with the number of arms. 2. We provide regret bounds that bridge the gap between two groups of results: those of the form O(K log T ) that\nmake the Condorcet assumption, and those of the form O(K2 log T ) that do not make the Condorcet assumption. Our bounds are similar to those of the former but are as broadly applicable as the latter. Furthermore, the result for CCB has substantially better dependence on the gaps than the second group of results. In addition, Appendix A presents the results of an empirical evaluation of CCB and SCB using a real-life problem\narising from information retrieval (IR). The experimental results mirror the theoretical ones."
    }, {
      "heading" : "2 Problem Setting",
      "text" : "Let K ≥ 2. The K-armed dueling bandit problem [1] is a modification of the K-armed bandit problem [15]. The latter considers K arms {a1, . . . , aK} and at each time-step, an arm ai can be pulled, generating a reward drawn from an unknown stationary distribution with expected value µi. The K-armed dueling bandit problem is a variation in which, instead of pulling a single arm, we choose a pair (ai, aj) and receive one of them as the better choice, with the probability of ai being picked equal to an unknown constant pij and that of aj being picked equal to pji = 1− pij . A problem instance is fully specified by a preference matrix P = [pij ], whose ij entry is equal to pij .\nMost previous work assumes the existence of a Condorcet winner [10]: an arm, which without loss of generality we label a1, such that p1i > 12 for all i > 1. In such work, regret is defined relative to the Condorcet winner. However, Condorcet winners do not always exist [8, 13]. In this paper, we consider a formulation of the problem that does not assume the existence of a Condorcet winner.\nInstead, we consider the Copeland dueling bandit problem, which defines regret with respect to a Copeland winner, which is an arm with maximal Copeland score. The Copeland score of ai, denoted Cpld(ai), is the number of arms aj for which pij > 0.5. The normalized Copeland score, denoted cpld(ai), is simply Cpld(ai) K−1 . Without loss of generality, we assume that a1, . . . , aC are the Copeland winners, where C is the number of Copeland winners. We define regret as follows:\nDefinition 1. The regret incurred by comparing ai and aj is 2cpld(a1)− cpld(ai)− cpld(aj). Remark 2. Since our results (see §5) establish bounds on the number of queries to non-Copeland winners, they can also be applied to other notions of regret."
    }, {
      "heading" : "3 Related Work",
      "text" : "Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and\nMultiSBM [16], and mergeRUCB [17], all of which require the existence of a Condorcet winner, and often come with bounds of the form O(K log T ). However, as observed in [13] and Appendix C.1, real-world problems do not always have Condorcet winners.\nThere is another group of algorithms that do not assume the existence of a Condorcet winner, but have bounds of the form O(K2 log T ) in the Copeland setting: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) [10], Preference-Based Racing (PBR) [11] and Rank Elicitation (RankEl) [12]. All three of these algorithms are designed to solve more general or more difficult problems, and they solve the Copeland dueling bandit problem as a special case.\nThis work bridges the gap between these two groups by providing algorithms that are as broadly applicable as the second group but have regret bounds comparable to those of the first group. Furthermore, in the case of the results for CCB, rather than depending on the smallest gap between arms ai and aj , ∆min :=mini>j |pij − 0.5|, as in the case of many results in the Copeland setting,1 our regret bounds depend on a larger quantity that results in a substantially lower upper-bound, cf. §5.1.\nIn addition to the above, bounds have been proven for other notions of winners, including Borda [10–12], Random Walk [11, 18], and very recently von Neumann [19]. The dichotomy discussed also persists in the case of these results, which either rely on restrictive assumptions to obtain a linear dependence on K or are more broadly applicable, at the expense of a quadratic dependence on K. A natural question for future work is whether the improvements achieved in this paper in the case of the Copeland winner can be obtained in the case of these other notions as well.\nA related setting is that of partial monitoring games [20]. While a dueling bandit problem can be modeled as a partial monitoring problem, doing so yields weaker results. In [21], the authors present problem-dependent bounds from which a regret bound of the formO(K2 log T ) can be deduced for the dueling bandit problem, whereas our work achieves a linear dependence in K."
    }, {
      "heading" : "4 Method",
      "text" : "We now present two algorithms that find Copeland winners."
    }, {
      "heading" : "4.1 Copeland Confidence Bound (CCB)",
      "text" : "CCB (see Algorithm 1) is based on the principle of optimism followed by pessimism: it maintains optimistic and pessimistic estimates of the preference matrix, i.e., matrices U and L (Line 6). It uses U to choose an optimistic Copeland winner ac (Lines 7–9 and 11–12), i.e., an arm that has some chance of being a Copeland winner. Then, it uses L to choose an opponent ad (Line 13), i.e., an arm deemed likely to discredit the hypothesis that ac is indeed a Copeland winner.\nMore precisely, an optimistic estimate of the Copeland score of each arm ai is calculated using U (Line 7), and ac is selected from the set of top scorers, with preference given to those in a shortlist, Bt (Line 11). Theses are arms that have, roughly speaking, been optimistic winners throughout history. To maintain Bt, as soon as CCB discovers that the optimistic Copeland score of an arm is lower than the pessimistic Copeland score of another arm, it purges the former from Bt (Line 9B).\nThe mechanism for choosing the opponent ad is as follows. The matrices U and L define a confidence interval around pij for each i and j. In relation to ac, there are three types of arms: (1) arms aj s.t. the confidence region of pcj is strictly above 0.5, (2) arms aj s.t. the confidence region of pcj is strictly below 0.5, and (3) arms aj s.t. the confidence region of pcj contains 0.5. Note that an arm of type (1) or (2) at time t′ may become an arm of type (3) at time t > t′ even without queries to the corresponding pair as the size of the confidence intervals increases as time goes on.\nCCB always chooses ad from arms of type (3) because comparing ac and a type (3) arm is most informative about the Copeland score of ac. Among arms of type (3), CCB favors those that have confidently beaten arm ac in the past (Line 13), i.e., arms that in some round t′ < t were of type (2). Such arms are maintained in a shortlist of “formidable” opponents (Bit) that are likely to confirm that ai is not a Copeland winner; these arms are favored when selecting ad (Lines 10 and 13).\nThe sets Bit are what speeds up the elimination of non-Copeland winners, enabling regret bounds that scale asymptotically with K rather than K2. Specifically, for a non-Copeland winner ai, the set Bit will eventually contain LC+1\n1Cf. [10, Equation 9 in §4.1.1] and [11, Theorem 1].\nAlgorithm 1 Copeland Confidence Bound Input: A Copeland dueling bandit problem and an exploration parameter α > 12 .\n1: W = [wij ]← 0K×K // 2D array of wins: wij is the number of times ai beat aj 2: B1 = {a1, . . . , aK} // potential best arms 3: Bi1 = ∅ for each i = 1, . . . ,K // potential to beat ai 4: LC = K // estimated max losses of a Copeland winner 5: for t = 1, 2, . . . do 6: U := [uij ]= W W+WT + √ α ln t W+WT and L := [lij ]= WW+WT − √ α ln t W+WT\n, with uii= lii= 12 , ∀i 7: Cpld(ai) = # { k |uik ≥ 12 , k 6= i } and Cpld(ai) = # { k | lik ≥ 12 , k 6= i } 8: Ct = {ai |Cpld(ai) = maxj Cpld(aj)} 9: Set Bt ← Bt−1 and Bit ← Bit−1 and update as follows:\nA. Reset disproven hypotheses: If for any i and aj ∈ Bit we have lij > 0.5, reset Bt, LC and Bkt for all k (i.e. set them to their original values as in Lines 2–4 above). B. Remove non-Copeland winners: For each ai ∈ Bt, if Cpld(ai) < Cpld(aj) holds for any j, set Bt ← Bt \\ {ai}, and if |Bit| 6= LC + 1, then set Bit ← {ak|uik < 0.5}. However, if Bt = ∅, reset Bt, LC and Bkt for all k. C. Add Copeland winners: For any ai ∈ Ct with Cpld(ai) = Cpld(ai), set Bt ← Bt ∪ {ai}, Bit ← ∅ and LC ← K − 1 − Cpld(ai). For each j 6= i, if we have |Bjt | < LC + 1, set Bjt ←∅, and if |Bjt |> LC +1, randomly choose LC+1 elements of Bjt and remove the rest. 10: With probability 1/4, sample (c, d) uniformly from the set {(i, j) | aj ∈ Bit and 0.5 ∈ [lij , uij ]} (if it is nonempty) and skip to Line 14. 11: If Bt ∩ Ct 6= ∅, then with probability 2/3, set Ct ← Bt ∩ Ct. 12: Sample ac from Ct uniformly at random. 13: With probability 1/2, choose the set Bi to be either Bit or {a1, . . . , aK} and then set d ← arg max{j∈Bi | ljc≤0.5} ujc. If there is a tie, d is not allowed to be equal to c. 14: Compare arms ac and ad and increment wcd or wdc depending on which arm wins. 15: end for\nstrong opponents for ai (Line 4.1C), where LC is the number of losses of each Copeland winner. Since LC is typically small (cf.Appendix C.3), asymptotically this leads to a bound of only O(log T ) on the number of time-steps when ai is chosen as an optimistic Copeland winner, instead of a bound of O(K log T ), which a more naive algorithm would produce."
    }, {
      "heading" : "4.2 Scalable Copeland Bandits (SCB)",
      "text" : "SCB is designed to handle dueling bandit problems with large numbers of arms. It is based on an arm-identification algorithm, described in Algorithm 2, designed for a PAC setting, i.e., it finds an -Copeland winner with probability 1 − δ, although we are primarily interested in the case with = 0. Algorithm 2 relies on a reduction to a K-armed bandit problem where we have direct access to a noisy version of the Copeland score; the process of estimating the score of arm ai consists of comparing ai to a random arm aj until it becomes clear which arm beats the other. The sample complexity bound, which yields the regret bound, is achieved by combining a bound for K-armed bandits and\nAlgorithm 2 Approximate Copeland Bandit Solver Input: A Copeland dueling bandit problem with preference matrix P = [pij ], failure probability δ > 0, and approxi-\nmation parameter > 0. Also, define [K] := {1, . . . ,K}. 1: Define a random variable reward(i) for i ∈ [K] as the following procedure: pick a uniformly random j 6= i\nfrom [K]; query the pair (ai, aj) sufficiently many times in order to determine w.p. at least 1 − δ/K2 whether pij > 1/2; return 1 if pij > 0.5 and 0 otherwise. 2: Invoke Algorithm 4, where in each of its calls to reward(i), the feedback is determined by the above stochastic process.\nReturn: The same output returned by Algorithm 4.\na bound on the number of arms that can have a high Copeland score. Algorithm 2 calls a K-armed bandit algorithm as a subroutine. To this end, we use the KL-based arm-elimination algorithm (a slight modification of Algorithm 2 in [22]) described in Algorithm 4 in Appendix I. It implements an elimination tournament with confidence regions based on the KL-divergence between probability distributions.\nCombining this with the squaring trick, a modification of the doubling trick that reduces the number of partitions from log T to log log T , the SCB algorithm, described in Algorithm 3, repeatedly calls Algorithm 2 but forceterminates if an increasing threshold is reached. If it terminates early, then the identified arm is played against itself until the threshold is reached. Algorithm 3 Scalable Copeland Bandits Input: A Copeland dueling bandit problem with preference matrix P = [pij ]\n1: for all r = 1, 2, . . . do 2: Set T = 22 r\nand run Algorithm 2 with failure probability log(T )/T in order to find an exact Copeland winner ( = 0); force-terminate if it requires more than T queries.\n3: Let T0 be the number of queries used by invoking Algorithm 2, and let ai be the arm produced by it; query the pair (ai, ai) T − T0 times. 4: end for"
    }, {
      "heading" : "5 Theoretical Results",
      "text" : "In this section, we present regret bounds for both CCB and SCB. Assuming that the number of Copeland winners and the number of losses of each Copeland winner are bounded,2 CCB’s regret bound takes the form O(K2 + K log T ), while SCB’s is of the form O(K logK log T ). Note that these bounds are not directly comparable. When there are relatively few arms, CCB is expected to perform better. By contrast, when there are many arms SCB is expected to be superior. Appendix A provides empirical evidence to support these expectations.\nThroughout this section we impose the following condition on the preference matrix: A There are no ties, i.e., for all pairs (ai, aj) with i 6= j, we have pij 6= 0.5. This assumption is not very restrictive in practice. For example, in the ranker evaluation setting from information retrieval, each arm corresponds to a ranker, a complex and highly engineered system, so it is unlikely that two rankers are indistinguishable. Furthermore, some of the results we present in this section actually hold under even weaker assumptions. However, for the sake of clarity, we defer a discussion of these nuanced differences to Appendix E."
    }, {
      "heading" : "5.1 Copeland Confidence Bounds (CCB)",
      "text" : "To analyze Algorithm 1, consider a K-armed Copeland bandit problem with arms a1, . . . , aK and preference matrix P = [pij ], such that arms a1, . . . , aC are the Copeland winners, with C being the number of Copeland winners. Throughout this section, we assume that the parameter α in Algorithm 1 satisfies α>0.5, unless otherwise stated. We first define the relevant quantities:\nDefinition 3. Given the above setting we define:3 1. Li := {aj | pij < 0.5}, i.e., the arms to which ai loses, and LC := |L1|. 2. ∆ij := |pij − 0.5| and ∆min := mini 6=j ∆ij 3. Given i > C, define i∗ as the index of the (LC + 1)th largest element in the set {∆ij | pij < 0.5}. 4. Define ∆∗i to be ∆ii∗ if i > C and 0 otherwise. Moreover, let us set ∆ ∗ min := mini>C ∆ ∗ i . 5. Define ∆∗ij to be ∆ ∗ i + ∆ij if pij ≥ 0.5 and max{∆∗i ,∆ij} otherwise.4 6. ∆ := min {mini≤C<j ∆ij ,∆∗min}, where ∆∗min is defined as in item 4 above. 7. C(δ) := ( (4α− 1)K2/(2α− 1)δ ) 1 2α−1 where α is as in Algorithm 1. 8. Nδij(t) is the number of time-steps between timesC(δ) and t when ai was chosen as the optimistic Copeland winner\nand aj as the challenger. Also, N̂δij(t) is defined to be (4α ln t)/ ( ∆∗ij )2\nif i 6= j, 0 if i = j > C and t if i = j ≤ C. We also define N̂δ(t) := ∑ i 6=j N̂ δ ij(t) + 1.\n2See Appendix C.3 for experimental evidence that this is the case in practice. 3See Tables 2 and 3 for a summary of the definitions used in this paper. 4See Figures 7 and 8 for a pictorial explanation.\nUsing this notation, our expected regret bound for CCB takes the form: O ( K2+(C+LC)K lnT\n∆2\n) (1)\nThis result is proven in two steps. First, Proposition 4 bounds the number of comparisons involving non-Copeland winners, yielding a result of the form O(K2 lnT ). Second, Theorem 11 closes the gap between this bound and that of (1) by showing that, beyond a certain time horizon, CCB selects non-Copeland winning arms as the optimistic Copeland winner very infrequently.\nNote that we have ∆∗ij ≥ ∆ij for all pairs i 6= j. Thus, for simplicity, the analysis in this section can be read as if the bounds were given in terms of ∆ij . We use ∆∗ij instead because it gives tighter upper bounds. In particular, simply using the gaps ∆ij would replace the denominator of the expression in (1) with ∆2min, which leads to a substantially worse regret bound in practice. For instance, in the ranker evaluation application used in the experiments, this change would on average increase the regret bound by a factor that is of the order of tens of thousands. See Appendix C.4 for a more quantitative discussion of this point.\nWe can now state our first bound, proved in Appendix E under weaker assumptions.\nProposition 4. Given any δ > 0 and α > 0.5, if we apply CCB (Algorithm 1) to a dueling bandit problem satisfying Assumption A, the following holds with probability 1− δ: for any T > C(δ) and any pair of arms ai and aj , we have Nδij(T ) ≤ N̂δij(T ).\nOne can sum the inequalities in the last proposition over pairs (i, j) to get a regret bound of the formO(K2 log T ) for Algorithm 1. However, as Theorem 11 will show, we can use the properties of the sets Bit to obtain a tighter regret bound of the form O(K log T ). Before stating that theorem, we need a few definitions and lemmas. We begin by defining the key quantity:\nDefinition 5. Given a preference matrix P and δ > 0, then Tδ is the smallest integer satisfying Tδ ≥ C( δ2 )+8K2(LC+1)2 ln 6K 2 δ +K 2 ln 6Kδ + 32αK(LC+1) ∆2min lnTδ+N̂ δ 2(Tδ)+4K max i>C N̂ δ 2 i (Tδ).\nRemark 6. Tδ is poly(K, δ−1) and our regret bound below scales as log Tδ .\nThe following two lemmas are key to the proof of Theorem 11. Lemma 7 (proved in Appendix F) states that, with high probability by time Tδ , each set Bit contains LC + 1 arms aj , each of which beats ai (i.e., pij < 0.5). This fact then allows us to prove Lemma 8 (Appendix G), which states that, after time-step Tδ , the rate of suboptimal comparisons is O(K lnT ) rather than O(K2 lnT ).\nLemma 7. Given δ > 0, with probability 1− δ, each set BiTδ with i > C contains exactly LC + 1 elements with each element aj satisfying pij < 0.5. Moreover, for all t ∈ [Tδ, T ], we have Bit = BiTδ .\nLemma 8. Given a Copeland bandit problem satisfying Assumption A and any δ > 0, with probability 1 − δ the following holds: the number of time-steps between Tδ/2 and T when each non-Copeland winner ai can be chosen as optimistic Copeland winners (i.e., times when arm ac in Algorithm 1 satisfies c > C) is bounded by N̂ i :=\n2N̂ iB + 2 √ N̂ iB ln 2K δ , where N̂ i B := ∑ j∈BiTδ/2 N̂ δ/4 ij (T ).\nRemark 9. Due to Lemma 7, with high probability we have N̂ iB ≤ (LC+1) lnT(∆∗min)2 for each i > C and so the total number of times between Tδ and T when a non-Copeland winner is chosen as an optimistic Copeland winner is in O(KLC lnT ) for a fixed minimal gap ∆∗min. The only other way a suboptimal comparison can occur is if a Copeland winner is compared against a non-Copeland winner, and according to Proposition 4, the number of such occurrences is bounded by O(KC lnT ). Hence, the number of suboptimal comparisons is in O(K lnT ) assuming that C and LC are bounded. In Appendix C.3, we provide experimental evidence for this.\nWe now define the quantities needed to state the main theorem.\nDefinition 10. We define the following three quantities: A(1)δ := C(δ/4) + N̂ δ(Tδ/2), A (2) δ := ∑ i>C √ LC+1 ∆∗i\nln 2Kδ and A(3) := ∑ i≤C<j 1 (∆ij) 2 + 2 ∑ i>C LC+1\n(∆∗i ) 2 .\nTheorem 11. Given a Copeland bandit problem satisfying Assumption A and any δ > 0 and α > 0.5, with probability 1− δ, the regret accumulated by CCB is bounded by the following:\nA (1) δ +A (2) δ √ lnT +A(3) lnT ≤ A(1)δ +A (2) δ √ lnT + 2K(C + LC + 1)\n∆2 lnT.\nFor a general assessment of the above quantities, assuming that LC and C are both O(1), the above quantities in terms of K become A(1)δ = O(K2), A (2) δ = O(K log(K)), A(3) = O(K). Hence, the above bound boils down to the expression in (1). We now turn to the proof of the theorem.\nProof of Theorem 11. Let us consider the two disjoint time-intervals [1, Tδ/2] and (Tδ/2, T ]: [1,Tδ/2]: In this case, applying Proposition 4 to Tδ , we get that the number of time-steps when a non-Copeland\nwinner was compared against another arm is bounded by A(1)δ . As the maximum regret such a comparison can incur is 1, this deals with the first term in the above expression.\n(Tδ/2,T]: In this case, applying Lemma 8, we get the other two terms in the above regret bound. Now that we have the high probability regret bound given in Theorem 11, we can deduce the expected regret result claimed in (1) for α > 1, as a corollary by integrating δ over the interval [0, 1]."
    }, {
      "heading" : "5.2 Scalable Copeland Bandits",
      "text" : "We now turn to our regret result for SCB, which lowers the K2 dependence in the additive constant of CCB’s regret result to K logK. We begin by defining the relevant quantities:\nDefinition 12. Given a K-armed Copeland bandit problem and an arm ai, we define the following: 1. Recall that cpld(ai) := Cpld(ai)/(K − 1) is called the normalized Copeland score. 2. ai is an -Copeland-winner if 1− cpld(ai) ≤ (1− cpld(a1)) (1 + ). 3. ∆i := max{cpld(a1)− cpld(ai), 1/(K − 1)} and Hi := ∑ j 6=i\n1 ∆2ij , with H∞ := maxiHi.\n4. ∆ i = max {∆i, (1− cpld(a1))}.\nWe now state our main scalability result:\nTheorem 13. Given a Copeland bandit problem satisfying Assumption A, the expected regret of SCB (Algorithm 3) is bounded by O ( 1 K ∑K i=1 Hi(1−cpld(ai)) ∆2i ) log(T ), which in turn can be bounded by O ( K(LC+logK) log T ∆2min ) , where LC and ∆min are as in Definition 3.\nRecall that SCB is based on Algorithm 2, an arm-identification algorithm that identifies a Copeland winner with high probability. As a result, Theorem 13 is an immediate corollary of Lemma 14, obtained by using the well known squaring trick. As mentioned in Section 4.2, the squaring trick is a minor variation on the doubling trick that reduces the number of partitions from log T to log log T .\nLemma 14 is a result for finding an -approximate Copeland winner (see Definition 12.2). Note that, for the regret setting, we are only interested in the special case with = 0, i.e., the problem of identifying the best arm. Lemma 14. With probability 1− δ, Algorithm 2 finds an -approximate Copeland winner by time O ( 1\nK K∑ i=1 Hi(1− cpld(ai)) (∆ i) 2\n) log(1/δ) ≤ O ( H∞ ( log(K) + min { −2, LC })) log(1/δ).\nassuming5 δ = (KH∞)Ω(1). In particular when there is a Condorcet winner (cpld(a1) = 1, LC = 0) or more generally cpld(a1) = 1−O(1/K), LC = O(1), an exact solution is found with probability at least 1− δ by using an expected number of queries of at most O (H∞(LC + logK)) log(1/δ).\nIn the remainder of this section, we sketch the main ideas underlying the proof of Lemma 14, detailed in Appendix H. We first treat the simpler deterministic setting in which a single query suffices to determine which of a pair of arms beats the other. While a solution can easily be obtained using K(K − 1)/2 many queries, we aim for one with query complexity linear in K. The main ingredients of the proof are as follows: 1. cpld(ai) is the mean of a Bernoulli random variable defined as such: sample uniformly at random an index j from\nthe set {1, . . . ,K} \\ {i} and return 1 if ai beats aj and 0 otherwise. 5The exact expression requires replacing log(1/δ) with log(KH∞/δ).\n2. Applying a KL-divergence based arm-elimination algorithm (Algorithm 4) to the K-armed bandit arising from the above observation, we obtain a bound by dividing the arms into two groups: those with Copeland scores close to that of the Copeland winners, and the rest. For the former, we use the result from Lemma 15 to bound the number of such arms; for the latter, the resulting regret is dealt with using Lemma 16, which exploits the possible distribution of Copeland scores. Let us state the two key lemmas here:\nLemma 15. Let D ⊂ {a1, . . . , aK} be the set of arms for which cpld(ai) ≥ 1 − d/(K − 1), that is arms that are beaten by at most d arms. Then |D| ≤ 2d+ 1. Proof. Consider a fully connected directed graph, whose node set is D and the arc (ai, aj) is in the graph if arm ai beats arm aj . By the definition of cpld, the in-degree of any node i is upper bounded by d. Therefore, the total number of arcs in the graph is at most |D|d. Now, the full connectivity of the graph implies that the total number of arcs in the graph is exactly |D|(|D| − 1)/2. Thus, |D|(|D| − 1)/2 ≤ |D|d and the claim follows.\nLemma 16. The sum ∑ {i|cpld(ai)<1} 1 1−cpld(ai) is in O(K logK).\nProof. Follows from Lemma 15 via a careful partitioning of arms. Details are in Appendix H.\nGiven the structure of Algorithm 2, the stochastic case is similar to the deterministic case for the following reason: while the latter requires a single comparison between arms ai and aj to determine which arm beats the other, in the stochastic case, we need roughly log(K log(∆−1ij )/δ)\n∆2ij comparisons between the two arms to correctly answer the same\nquestion with probability at least 1− δ/K2."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In many applications that involve learning from human behavior, feedback is more reliable when provided in the form of pairwise preferences. In the dueling bandit problem, the goal is to use such pairwise feedback to find the most desirable choice from a set of options. Most existing work in this area assumes the existence of a Condorcet winner, i.e., an arm that beats all other arms with probability greater than 0.5. Even though these results have the advantage that the bounds they provide scale linearly in the number of arms, their main drawback is that in practice the Condorcet assumption is too restrictive. By contrast, other results that do not impose the Condorcet assumption achieve bounds that scale quadratically in the number of arms.\nIn this paper, we set out to solve a natural generalization of the problem, where instead of assuming the existence of a Condorcet winner, we seek to find a Copeland winner, which is guaranteed to exist. We proposed two algorithms to address this problem: one for small numbers of arms, called CCB; and a more scalable one, called SCB, that works better for problems with large numbers of arms. We provided theoretical results bounding the regret accumulated by each algorithm: these results improve substantially over existing results in the literature, by filling the gap that exists in the current results, namely the discrepancy between results that make the Condorcet assumption and are of the form O(K log T ) and the more general results that are of the form O(K2 log T ).\nMoreover, we have included empirical results on both a dueling bandit problem arising from a real-life application domain and a large-scale synthetic problem used to test the scalability of SCB. The results of these experiments show that CCB beats all existing Copeland dueling bandit algorithms, while SCB outperforms CCB on the large-scale problem.\nOne open question raised by our work is how to devise an algorithm that has the benefits of both CCB and SCB, i.e., the scalability of the latter together with the former’s better dependence on the gaps. At this point, it is not clear to us how this could be achieved.\nAnother interesting direction for future work is an extension of both CCB and SCB to problems with a continuous set of arms. Given the prevalence of cyclical preference relationships in practice, we hypothesize that the non-existence of a Condorcet winner is an even greater issue when dealing with an infinite number of arms. Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].\nFinally, it is also interesting to expand our results to handle scores other than the Copeland score, such as an - insensitive variant of the Copeland score (as in [12]), or completely different notions of winners, such as the Borda, the Random Walk or the von Neumann winners (see, e.g., [19, 31])."
    }, {
      "heading" : "A Experimental Results",
      "text" : "To evaluate our methods CCB and SCB, we apply them to three Copeland dueling bandit problems. The first is a 5-armed problem arising from ranker evaluation in the field of information retrieval (IR) [32]. The second is a 500- armed synthetic example created to test the scalability of SCB. The third is an example with a Condorcet winner which shows how CCB compares against RUCB when the condition required by RUCB is satisfied.\nAll three experiments follow the experimental approach in [3, 13] and use the given preference matrix to simulate comparisons between each pair of arms (ai, aj) by drawing samples from Bernoulli random variables with mean pij . We compare our two proposed algorithms against the state of the art K-armed dueling bandit algorithm, RUCB [13], and Copeland SAVAGE, PBR and RankEl. We include RUCB in order to verify our claim that K-armed dueling bandit algorithms that assume the existence of a Condorcet winner have linear regret if applied to a Copeland dueling bandit problem without a Condorcet winner. Note that in all our plots, the horizontal time axes use a log scale, while the vertical axes, which measure cumulative regret, use a linear scale.\nThe first experiment uses a 5-armed problem arising from ranker evaluation in the field of information retrieval (IR) [32], detailed in Appendix B. Figure 1 shows the regret accumulated by CCB, SCB, the Copeland variants of SAVAGE, PBR and RankEl, as well as RUCB on this problem. CCB outperforms all other algorithms in this 5-armed experiment.\nNote that three of the baseline algorithms under consideration here (i.e., SAVAGE, PBR and RankEl) require the horizon of the experiment as an input. Therefore, we ran independent experiments with varying horizons and recorded the accumulated regret: the markers on the curves corresponding to these algorithms represent these numbers. Consequently, the regret curves are not monotonically increasing. For instance, SAVAGE’s cumulative regret at time 2 × 107 is lower than at time 107 because the runs that produced the former number were not continuations of those that resulted in the latter, but rather completely independent. Furthermore, RUCB’s cumulative regret grows linearly, which is why the plot does not contain the entire curve.\nThe second experiment uses a 500-armed synthetic example created to test the scalability of SCB. In particular, we fix a preference matrix in which the three Copeland winners are in a cycle, each with a Copeland score of 498, and the other arms have Copeland scores ranging from 0 to 496.\nFigure 2, which depicts the results of this experiment, shows that when there are many arms, SCB can substantially outperform CCB. We omit SAVAGE, PBR and RankEl from this experiment because they scale poorly in the number of arms [10–12].\nThe reason for the sharp transition in the regret curves of CCB and SCB in the synthetic experiment is as follows. Because there are many arms, as long as one of the two arms being compared is not a Copeland winner, the comparison can result in substantial regret; since both algorithms choose the second arm in each round based on some criterion other than the Copeland score, even if the first chosen arm in a given time-step is a Copeland winner, the incurred regret may be as high as 0.5. The sudden transition in Figure 2 occurs when the algorithm becomes confident enough of its choice for the first arm to begin comparing it against itself, at which point it stops accumulating regret.\nThe third experiment is an example with a Condorcet winner designed to show how CCB compares against RUCB when the condition required by RUCB is satisfied. The regret plots for SAVAGE and SCB were excluded here since they both perform substantially worse than either RUCB or CCB, as expected. This example was extracted in the same fashion as the example used in the ranker evaluation experiment detailed in Appendix B, with the sole difference that this time we ensured that one of the rankers is a Condorcet winner. The results, depicted in Figure 3, show that CCB enjoys a slight advantage over RUCB in this case. We attribute this to the careful process of identifying and utilizing the weaknesses of non-Copeland winners, as carried out by lines 12 and 18 of Algorithm 1."
    }, {
      "heading" : "B Ranker Evaluation Details",
      "text" : "A ranker is a function that takes as input a user’s search query and ranks the documents in a collection according to their relevance to that query. Ranker evaluation aims to determine which among a set of rankers performs best. One effective way to achieve this is to use interleaved comparisons [33], which interleave the ranked lists of documents proposed by two rankers and present the resulting list to the user, whose subsequent click feedback is used to infer a noisy preference for one of the rankers. Given a set of K rankers, the problem of finding the best ranker can then be modeled as a K-armed dueling bandit problem, with each arm corresponding to a ranker.\nWe use interleaved comparisons to estimate the preference matrix for the full set of rankers included with the MSLR dataset6, from which we select 5 rankers such that a Condorcet winner does not exist. The MSLR dataset consists of relevance judgments provided by expert annotators assessing the relevance of a given document to a given query. Using this data set, we create a set of 136 rankers, each corresponding to a ranking feature provided in the data set, e.g., PageRank. The ranker evaluation task in this context corresponds to determining which single feature constitutes the best ranker [4].\nTo compare a pair of rankers, we use probabilistic interleave (PI) [34], a recently developed method for interleaved comparisons. To model the user’s click behavior on the resulting interleaved lists, we employ a probabilistic user model [34, 35] that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the MSLR dataset. Queries are sampled randomly and clicks are generated probabilistically by conditioning on these assessments in a way that resembles the behavior of an actual user [36]. Specifically, we employ an informational click model in our ranker evaluation experiments [37].\nThe informational click model simulates the behavior of users whose goal is to acquire knowledge about multiple facets of a topic, rather than seeking a specific page that contains all the information that they need. As such, in the informational click model, the user tends to continue examining documents even after encountering a highly relevant document. The informational click model is one of the three click models utilized in the ranker evaluation literature, along with the perfect and navigational click models [37]. It turns out that the full preference matrix of the feature vectors of the MSLR dataset has a Condorcet winner when the perfect or the navigational click-models are used. As we will see in Appendix C.1, using the informational click model that is no longer true.\n6http://research.microsoft.com/en-us/projects/mslr/default.aspx\nFollowing [3, 13], we first use the above approach to estimate the comparison probabilities pij for each pair of rankers and then use these probabilities to simulate comparisons between rankers. More specifically, we estimate the full preference matrix, called the informational preference matrix, by performing 400, 000 interleaved comparisons on each pair of the 136 feature rankers."
    }, {
      "heading" : "C Assumptions and Key Quantities",
      "text" : "In this section, we provide quantitative analysis of the various assumptions, definitions and quantities that were discussed in the main body of the paper.\nC.1 The Condorcet Assumption To test how stringent the Condorcet assumption is, we use the informational preference matrix described in Section B to estimate for each K = 1, . . . , 136 the probability PK that a given K-armed dueling bandit problem, obtained from considering K of our 136 feature rankers, would have a Condorcet winner by randomly selecting 10, 000 Karmed dueling bandit problems and counting the ones with Condorcet winners. As can be seen from Figure 4, as K grows the probability that the Condorcet assumption holds decreases rapidly. We hypothesize that this is because the informational click model explores more of the list of ranked documents than the navigational click model, which was used in [13], and so it is more likely to encounter non-transitivity phenomena of the sort described in [38].\nC.2 Other Notions of Winners As mentioned in Section 3, numerous other definitions of what constitutes the best arm have been proposed, some of which specialize to the Condorcet winner, when it exists. This latter property is desirable both in preference learning and social choice theory: the Condorcet winner is the choice that is preferred over all other choices, so if it exists, there is good reason to insist on selecting it. The Copeland winner, as discussed in this paper, and the von Neumann winner [19] satisfy this property, while the Borda (a.k.a. Sum of Expectations) and the Random Walk (a.k.a. PageRank)\nwinners [39] do not. The von Neumann winner is in fact defined as a distribution over arms such that playing it will maximize the probability to beat any fixed arm. The Borda winner is defined as the arm maximizing the score∑ j 6=i pij and can be interpreted as the arm that beats other arms by the most, rather than beating the most arms. The Random Walk winner is defined as the arm we are most likely to visit in some Markov Chain determined by the preference matrix. In this section, we provide some numerical evidence for the similarity of these notions in practice, based on the sampled preference matrices obtained from the ranker evaluation from IR, which was described in the last section. Table 1 lists the percentage of preference matrices for which pairs of winner overlapped. In the case of the von Neumann winner, which is defined as a probability distribution over the set of arms [19], we used the support of the distribution (i.e., the set of arms with non-zero probability) to define overlap with the other definitions.\nAs these numbers demonstrate, the Copeland and the von Neumann winners are very likely to overlap, as are the Borda and Random Walk winners, while the first two definitions are more likely to be incompatible with the latter two. Furthermore, in the case of %94.2 of the preference matrices, all Copeland winners were contained in the support of the von Neumann winner, suggesting that in practice the Copeland winner is a more restrictive notion of what constitutes a winner.\nC.3 The Quantities C and LC We also examine additional quantities relevant to our regret bounds: the number of Copeland winners, C; the number of losses of each Copeland winner, LC ; and the range of values in which these quantities fall. Using the above randomly chosen preference sub-matrices, we counted the number of times each possible value for C and LC was observed. The results are depicted in Figure 5: the area of the circle with coordinates (x, y) is proportional to the percentage of examples with K = x which satisfied C = y (in the top plot) or LC = y (in the bottom plot). As these plots show, the parameters C and LC are generally much lower than K.\nC.4 The Gap ∆ The regret bound for CCB, given in (1), depends on the gap ∆ defined in Definition 3.6, rather than the smallest gap ∆min as specified in Definition 3.2. The latter would result in a looser regret bound and Figure 6 quantifies this deterioration in the ranker evaluation example under consideration here. In particular, the plot depicts the average of the ratio between the two bounds (the one using ∆ and the one using ∆min) across the 10, 000 sampled preference matrices used in the analysis of the Condorcet winner for each K in the set {2, . . . , 135}. The average ratio decreases as the number of arms approaches 136 because, asK increases, the sampled preference matrices increasingly resemble the full preference matrix and so their gaps ∆ and ∆min approach those of the full 136-armed preference matrix as well. As it turns out, the ratio ∆2/∆2min for the full matrix is equal to 1, 419. Hence, the curve in Figure 6 approaches that number as the number of arms approaches 136."
    }, {
      "heading" : "D Background Material",
      "text" : "Maximal Azuma-Hoeffding Bound [40, §A.1.3]: Given random variables X1, . . . , XN with common range [0, 1] satisfying E[Xn|X1, . . . , Xn−1] = µ, define the partial sums Sn = X1 + · · ·+Xn. Then, for all a > 0, we have\nP (\nmax n≤N\nSn > nµ+ a ) ≤ e−2a2/N\nP (\nmin n≤N\nSn < nµ− a ) ≤ e−2a2/N\nHere, we will quote a useful Lemma that we will refer to repeatedly in our proofs:\nLemma 17 (Lemma 1 in [13]). Let P := [pij ] be the preference matrix of a K-armed dueling bandit problem with arms {a1, . . . , aK}. Then, for any dueling bandit algorithm and any α > 12 and δ > 0, we have\nP ( ∀ t > C(δ), i, j, pij ∈ [lij(t), uij(t)] ) > 1− δ."
    }, {
      "heading" : "E Proof of Proposition 4",
      "text" : "Before starting with the proof, let us point out the following two properties that can be derived from Assumption A in Section 5:\nP1 There are no ties involving a Copeland winner and a non-Copeland winner, i.e., for all pairs of arms (ai, aj) with i ≤ C < j, we have pij 6= 0.5.\nP2 Each non-Copeland winner has more losses than every Copeland winner, i.e., for every pair of arms (ai, aj), with i ≤ C < j, we have |Li| < |Lj |.\nEven though we have assumed in the statement of Proposition 4 that Assumption A holds, it turns out that the proof provided in this section holds as long as the above two properties hold.\nProposition 4 Applying CCB to a dueling bandit problem satisfying properties P1 and P2, we have the following bounds on the number of comparisons involving various arms for each T > C(δ): for each pair of arms ai and aj , such that either at least one of them is not a Copeland winner or pij 6= 0.5, with probability 1− δ we have\nNδij(T ) ≤ N̂δij(T ) :=  4α lnT( ∆∗ij )2 if i 6= j\n0 if i = j > C\n(2)\nProof of Proposition 4. We will prove these bounds by considering a number of cases separately:\n1. i ≤ C and pij 6= 0.5: First of all, since ai is a Copeland winner, this means that according to the definitions in Tables 2 and 3, ∆∗ij is simply equal to ∆ij ; secondly, assuming by way of contradiction that N δ ij(t) > 4α lnT ∆ij\n> 0, then we have τij > C(δ) and so by Lemma 17, we have with probability 1 − δ that the confidence interval [lij(τij), uij(τij)] contains the preference probability pij . But, in order for arm aj to have been chosen as the challenger to ai, we must also have 0.5 ∈ [lij(τij), uij(τij)]; to see this, let us consider the two possible cases:\n(a) If we have pij > 0.5, then having 0.5 /∈ [lij(τij), uij(τij)]\nimplies that we have lij(τij) > 0.5, which in turn implies\nuji(τij) = 1− lij(τij) < 0.5 = uii(τij),\nbut this is impossible since in that case ai would’ve been chosen as the challenger.\n(b) If we have pij < 0.5, then have 0.5 /∈ [lij(τij), uij(τij)]\nimplies that we have uij(τij) < 0.5, but this is impossible because it means that we had lji(τij) > 0.5, and CCB would’ve eliminated it from considerations in its second round.\nSo, in either case, we cannot have 0.5 /∈ [lij(τij), uij(τij)]. Therefore, at time τij , we must have had uij(τij) − lij(τij) > |pij − 0.5| =: ∆ij . From this, we can conclude the following, using the definition of uij and lij :\nuij(τij)− lij(τij) := 2 √\nα ln τij Nij(τij) ≥ ∆ij\n∴ 2 √ α ln τij Nδij(τij) ≥ ∆ij ∵ Nδij(τij) ≤ Nij(τij)\n∴ 2\n√ α lnT\nNδij(τij) ≥ ∆ij ∵ τij ≤ T\n∴ Nδij(τij) ≤ 4α lnT\n∆2ij ,\ngiving us the desired bound. The reader is referred to Figure 7 for an illustration of this argument.\n2. C < i: Let us deal with the two cases included in Inequality (2) separately:\n(a) i = j > C: In plain terms, this says that with probability 1 − δ no non-Copeland winner will be compared against itself after time C(δ). The reason for this is the following set of facts:\nai\n1 2 ai\naC aj\n∆∗ij\n∆∗iK\nhold. On the other hand, Lemma 17 states that with probability 1− δ we have uij(τij) ≥ pij . Putting these two together we get uij(τij) ≥ max{0.5, pij}. (3) On the other hand, we will show next that with probability 1 − δ, we have lij(τij) ≤ 0.5 − ∆∗i ; this is a consequence of the following facts: • Since ai was chosen as the optimistic Copeland winner, we can deduce that ai had no more that LC\noptimistic losses. • Let ak1 , . . . , akl be the l ≤ LC arms to which ai lost optimistically during time-step τij . Then, the\nsmallest pik with k /∈ {k1, . . . , kl}, must be less than to equal to the {LC + 1}th smallest element in the set {pik | k = 1, . . . ,K}. • This, in turn, is equal to the {LC + 1}th smallest element in the set {pik|pik < 0.5} (since this latter set of numbers are the smallest ones in the former set). But, this is equal to 0.5−∆∗i by definition. So, we have the desired bound on lij(τij) and combining this with Inequality (3), we have\nuij(τij)− lij(τij) ≥ max{0, pij − 0.5}+ ∆∗i = ∆∗ij ,\nwhere the last equality follows directly from the definition of ∆∗ij and the fact that pij > 0.5 −∆∗i . Now, repeating the same calculations as before, we can conclude that with probability 1− δ, we have\nNδij(τij) ≤ 4α lnT( ∆∗ij )2 .\nA pictorial depiction of the various steps in this part of the proof can be found in Figure 8.\nThe two boxes in the top row with red intervals represent arms to which a1 loses (i.e. p1j < 0.5), the number of which happens to be 2 in this example, which means that LC = 2. Now, by Definition 3.3, i∗ is the index with the index j with the (LC + 1)th (in this case 3rd) lowest pij , and since the three lowest pij in this example are piK , piC and pii∗ , this means that the column labeled as ai∗ is indeed labeled correctly. Given this, Definition 3.4 tells us that ∆∗i is the size of the gap shown in the block corresponding to pair (ai, ai∗). Moreover, by Definition 3.5, the gap ∆∗ij is defined using one of the following three cases: (1) if we have pij < pii∗ (as with the ones with red confidence intervals in the bottom row of plots), then we get ∆∗ij := ∆ij = 0.5− pij ; (2) if we have pii∗ < pij ≤ 0.5 (as in the plots in the 2nd, 3rd and 7th column of the bottom row), then we get ∆∗ij := ∆∗i ; (3) if we have 0.5 < pij (as in the 1st and 6th column in the bottom row), then we get ∆∗ij := ∆ij + ∆ ∗ i . The reasoning behind this trichotomy is as follows: in the case of arms aj in group (1), they are not going to be chosen to be played against ai as soon as top of the interval goes below 0.5, and by Lemma 17, we know that the bottom of the interval will be below pij . In the case of the arms in groups (2) and (3), the bottom of their interval needs to be below pii∗ because otherwise that would mean that neither arm ai∗ nor arms in group (1) were eligible to be included in the arg max expression in Line 13 of Algorithm 1, which can only happen if we have uij < 0.5 for j = i∗ as well as the arms in group (1), from which we can deduce that the optimistic Copeland score of ai must have been lower than K − 1−LC , and so ai could not have been chosen as an optimistic Copeland winner. Using the same argument, we can also see that the tops of the confidence intervals corresponding to arms in group (2) must be above 0.5, or else it would be impossible for ai to be chosen as an optimistic Copeland winner. Moreover, by Lemma 17, the intervals of the arms aj in group (3) must contain pij . 20"
    }, {
      "heading" : "F Proof of Lemma 7",
      "text" : "Let us begin with the following direct corollary of Proposition 4:\nCorollary 18. Given any δ > 0, any T > C(δ) and any sub-interval of length N̂δ(T ) := ∑ i 6=j N̂ δ ij(T ) + 1, with probability 1− δ, there is at least one time-step when there exists c ≤ C such that\nCpld(ac) = Cpld(ac) = Cpld(ac)\n≥ Cpld(aj) ∀ j, (4)\nProof. According to Proposition 4, with probability 1 − δ, there are at most ∑i 6=j N̂δij(T ) time-steps between C(δ) and T when Algorithm 1 did not compare a Copeland winner against itself: i.e. c and d in Algorithm 1 did not satisfy c = d ≤ C.\nIn other words, during this time-period, in any sub-interval of length N̂δ(T ) := ∑ i6=j N̂ δ ij(T ) + 1, there is at least\none time-step when a Copeland winner was compared against itself. During this time-step, we must have had\nCpld(ac) = Cpld(ac) = Cpld(ac)\n≥ Cpld(aj) ∀ j,\nwhere the first two equalities are due to the fact that in order for Algorithm 1 to set c = d, we must have 0.5 /∈ [lcj , ucj ] for each j 6= c, or else ac would not be played against itself; on the other hand, the last inequality is due to the fact that ac was chosen as an optimistic Copeland winner by Line 8 of Algorithm 1, so its optimistic Copeland score must have been greater than or equal to the optimistic Copeland score of the rest of the arms.\nLemma 19. If there exists an arm ai with i > C such that BiC(δ/2) contains an arm aj that loses to ai (i.e. pij > 0.5) or such that BiC(δ/2) contains fewer than LC + 1 arms, then the probability that by time-step T0 the sets Bit and Bt are not reset by Line 9.A of Algorithm 1 is less than δ/6, where we define\nT0 := C(δ/2) + N̂ δ/2(Tδ)\n+ 32αK(LC + 1) lnTδ\n∆2min\n+ 8K2(LC + 1) 2 ln\n6K2\nδ .\nProof. By Line 9.A of Algorithm 1, as soon as we have lij > 0.5, the set Bit will be emptied. In what follows, we will show that the probability that the number of time-steps before we have lij > 0.5 is greater than\n∆T := N̂δ/2(Tδ) +N\nwith\nN := 32αK(LC + 1) lnTδ\n∆2min + 8K2(LC + 1)\n2 ln 6K2\nδ\nis bounded by δ/6K2. This is done using the amount of exploration infused by Line 10 of Algorithm 1. To begin, let us note that by Corollary 18, there is a time-step before T0 := C(δ/2) + N̂δ/2(Tδ) when the condition of Line 9.C of Algorithm 1 is satisfied for some Copeland winner. At this point, if Bit contains fewer than LC + 1 elements, then it will be emptied; furthermore, for all k > C, the sets BkT0 will have at most LC + 1 elements and so the set\nSt := {(k, `)|a` ∈ Bkt and 0.5 ∈ [lk`, uk`]}\ncontains at mostK(LC+1) elements for all t ≥ T0. Moreover, if at time-step T1 := C(δ/2)+∆T we have aj ∈ BiT1 , then we can conclude that (i, j) ∈ St for all t ∈ [C(δ/2), T1], since, if at any time after C(δ/2) arm aj were to be\nremoved from Bit, it will never be added back because that can only happen through Line 9.B of Algorithm 1 and by Lemma 17 and the assumption of the lemma we have uij > pij > 0.5.\nWhat we can conclude from the observations in the last paragraph is that if at time-step T1 we still have aj ∈ BiT1 , then there are ∆T time-steps during which the probability of comparing arms ai and aj was at least 14K(LC+1) and yet no more than 4α lnTδ ∆2ij comparisons took place, since otherwise, we would have lij > 0.5 at some point before T1. Now, let Bijn denote the indicator random variable that is equal to 1 if arms ai and aj were chosen to be played against each other by Line 10 of Algorithm 1 during time-step T1 + n. Also, let X1, . . . , XN be iid Bernoulli random variables with mean 14K(LC+1) . Since B ij n and Xn are Bernoulli and we have E [ Bijn ] ≤ E[Xn] for each n, then we can conclude that\nP ( N∑ n=1 Bijn < s ) ≤ P ( N∑ n=1 Xn < s ) for all s.\nOn the other hand, we can use the Hoeffding bound to show that the right hand side of the above inequality is smaller than δ/6 if we set s = 4α lnTδ\n∆2ij :\nP ( N∑ n=1 Xn < 4α lnTδ ∆2ij ) ≤ P ( N∑ n=1 Xn < 4α lnTδ ∆2min )\n= P ( N∑ n=1 Xn < N 4K(LC + 1) − a ) ≤ e − 2a2 N\nwith a := −4α lnTδ ∆2min + N 4K(LC + 1)\n= e − 32α 2 ln2 Tδ ∆4 min N + 4α lnTδ K(LC+1)∆ 2 min − N 8K2(LC+1) 2\n≤ e 4α lnTδ K(LC+1)∆ 2 min − N 8K2(LC+1) 2\n= e− ln 6K 2/δ = δ/6K2.\nNow, if we take a union bound over all pairs of arms ai and aj satisfying the condition stated at the beginning of this scenario, we get that with probability δ/6 by time-step C(δ/2) + ∆T all such erroneous hypotheses are reset by Line 9.A of Algorithm 1, emptying the sets Bit.\nLemma 20. Let t1 ∈ [C(δ/2), Tδ) be such that for all i, j satisfying aj ∈ Bit1 we have pij < 0.5. Then, the following two statements hold with probability 1− 5δ/6:\n1. If the set Bt1 in Algorithm 1 contains at least one Copeland winner, then if we set t2 = t1 + nmax, where\nnmax := 2K max i>C\nN̂ δ/2 i (Tδ) +\nK2 ln(6K/δ)\n2 ,\nthen Bt2 is non-empty and contains no non-Copeland winners, i.e. for all ai ∈ Bt2 we have i ≤ C.\n2. If the set Bt1 in Algorithm 1 contains no Copeland winners, i.e. for all ai ∈ Bt1 , we have i > C, then within nmax time-steps the set Bt will be emptied by Line 9.B of Algorithm 1.\nTherefore, with probability 1 − 5δ/6, by time t1 + 2nmax all non-Copeland winners (i.e. arms ai with i > C) are eliminated from Bt.\nProof. We will consider the two cases in the following, conditioning on the conclusions of Lemma 17, Proposition 4 and Corollary 18, all simultaneously holding with 1− δ/2:\n1. Bt1 contains a Copeland winner (i.e. ac ∈ Bt1 for some c ≤ C): in this case, by Lemma 17, we know that the Copeland winner will forever remain in the set Bt because\nCpld(ac) ≥ max j Cpld(aj) ≥ max j Cpld(aj),\nthen Bt2 will indeed be empty. Moreover, in what follows, we will show that the probability that any non-Copeland winner in Bt is not eliminated by time t2 is less than δ/6. Let us assume by way of contradiction that there exists an arm ab with b > C such that ab is in Bt2 : we will show that the probability of this happening is less than δ/6K, and so, taking a union bound over non-Copeland winning arms, the probability that any non-Copeland winner is in Bt2 is seen to be smaller than δ/6. Now, to see that the probability of ab being in the set Bt2 is small, note that the fact that ab being in Bt2 implies that ab was in the set Bt for the entirety of the time interval [C(δ/2), t2] as we will show in the following. If ab is eliminated from Bt at some point between t1 and t2, it will not get added back into Bt because that can only take place if the set Bt is reset at some point and there are only two ways for that to happen:\n(a) By Line 9.A of Algorithm 1 in the case that for some pair (i, j) with aj ∈ Bit we have lij > 0.5; however, this is ruled out by our assumption that at time t1 we have pij < 0.5 and by Lemma 17, which stipulates that we have lij ≤ pij < 0.5.\n(b) By Line 9.B of Algorithm 1 in the case that all arms are eliminated from Bt, but this cannot happen by the fact mentioned above that ac will not not be removed from Bt.\nSo, as mentioned above, we indeed have that at each time-step between t1 and t2, the set Bt contains ab. Next, we will show that the probability of this happening is less than δ/6K. To do so, let us denote by Sb the time-steps when arm ab was in the set of optimistic Copeland winners, i.e.\nSb := { t ∈ (t1, t2] ∣∣ ab ∈ Ct } . We can use Corollary 18 above with T = Tδ to show that the size of the set Sb (which we denote by |Sb|) is bounded from below by t2 − t1 − ∑ i 6=j N̂ δ/2 ij (Tδ): this is because whenever any Copeland winner ac is played against itself, Equation (4) holds, and so if we were to have ab /∈ Ct during that time-step ab would have had to get eliminated from Bt because ab not being an optimistic Copeland winner would imply that\nCpld(ab) < Cpld(ac) = Cpld(ac).\nBut, we know from facts (a) and (b) above that ab remains in Bt for all t ∈ (t1, t2]. Therefore, as claimed, we have\n|Sb| ≥ t2 − t1 − ∑ i 6=j N δ/2 ij (Tδ) ≥ 2KN̂ δ/2 b (Tδ) + K2 ln(6K/δ) 2 =: nb, (5)\nwhere the last inequality is due to the definition of nmax := t2 − t1. On the other hand, Proposition 4 tells us that the number of time-steps between t1 and t2 when ab could have been chosen as an optimistic Copeland winner is bounded as\nN δ/2 b (Tδ) ≤ N̂ δ/2 b (Tδ). (6)\nFurthermore, given the fact that during each time-step t ∈ Sb we have ab ∈ Bt ∩ Ct, the probability of ab being chosen as an optimistic Copeland winner is at least 1/K because of the sampling procedure in Lines 14-17 of Algorithm 1. However, this is considerably higher than the ratio obtained by dividing the right-hand sides of Inequality (6) by that of Inequality (5). We will make this more precise in the following: for each t ∈ Sb, denote by µbt the probability that arm ab would be chosen as the optimistic Copeland winner by Algorithm 1, and let X b t be the Bernoulli random variable that returns 1 when arm ab is chosen as the optimistic Copeland winner or 0\notherwise. As pointed out above, we have that µbt ≥ 1K for all t ∈ Sb, which, together with the fact that |Sb| ≥ nb, implies that the random variable Xb := ∑ t∈Sb X b t satisfies\nP (Xb < x) ≤ P (Binom(nb, 1/K) < x). (7) This is both because the Bernoulli summands ofXb have higher means than the Bernoulli summands ofBinom(nb, 1/K) and because Xb is the sum of a larger number of Bernoulli variables, so Xb has more mass away from 0 than does Binom(nb, 1/K). So, we can bound the right-hand side of Inequality (7) by δ/6K with x = N̂ δ/2 b (Tδ) to get our desired result. But, this is a simple consequence of the Hoeffding bound, a more general form of which is quoted in Section D. More precisely, we have\nP ( Binom(nb, 1/K) < N̂ δ/2 b (Tδ) ) = P ( Binom(nb, 1/K) < nb K − a )\nwith a := nb K − N̂δ/2b (Tδ)\n< e−2a 2/nb = e\n−2(nbK −N̂ δ/2 b (Tδ)) 2\nnb\n= e−2nb/K 2+4N̂ δ/2 b (Tδ)/K−2N̂ δ/2 b (Tδ) 2/nb\n≤ e−2nb/K2+4N̂ δ/2 b (Tδ)/K = e− ln(6K/δ) = δ/6K\nUsing the union bound over the non-Copeland winning arms that were in Bt1 , of whom there is at most K − 1, we can conclude that with probability δ/6 they are all eliminated from Bt2 .\n2. Bt1 does not contain any Copeland winners: in this case, we can use the exact same argument as above to conclude that the probability that the set Bt is non-empty for all t ∈ (t1, t2] is less than δ/6 because as before the probability that each arm ab ∈ Bt1 is not eliminated within nb time-steps is smaller than δ/6K.\nLet us now state the following consequence of the previous lemmas: Lemma 7. Given δ > 0, the following fact holds with probability 1 − δ: for each i > C, the set BiTδ contains\nexactly LC + 1 elements with each element aj satisfying pij < 0.5. Moreover, for all t ∈ [Tδ, T ], we have Bit = BiTδ .\nProof. In the remainder of the proof, we will condition on the high probability event that the conclusions of Lemma 17, Corollary 18, Lemma 19 and Lemma 20 all hold simultaneously with probability 1− δ.\nCombining Lemma 20, we can conclude that by time-step T1 := T0+2nmax all non-Copeland winners are removed from BT1 , which also means by Line 9.B of Algorithm 1 that the corresponding sets BiT1 , with i > C are non-empty, and Lemma 19 tells us that these sets have at least LC + 1 elements aj each of which beats ai (i.e. pij < 0.5).\nNow, applying Corollary 18, we know that within N̂δ/2(Tδ) time-steps, Line 9.C of Algorithm 1 will be executed, at which point we will have LC = LC and so Bit will be reduced to LC + 1 elements. Moreover, by Lemma 17, for all t > T1 and aj ∈ Bit we have lij ≤ pij < 0.5 and so Bit will not be emptied by any of the provisions in Line 9 of Algorithm 1.\nNow, since by definition we have T δ ≥ T1 + N̂δ/2(Tδ), we have the desired result."
    }, {
      "heading" : "G Proof of Lemma 8",
      "text" : "Lemma 8 Given a Copeland bandit problem satisfying Assumption A and any δ > 0, with probability 1 − δ the following statement holds: the number of time-steps between Tδ/2 and T when each non-Copeland winning arm ai can be chosen as optimistic Copeland winners (i.e. time-steps when arm ac in Algorithm 1 satisfies c = i > C) is bounded by\nN̂ i := 2N̂ iB + 2 √ N̂ iB ln 2K\nδ ,\nwhere N̂ iB := ∑ j∈BiTδ/2 N̂ δ/4 ij (T ).\nProof. The idea of the argument is outlined in the following sequence of facts: 1. By Lemma 7, we know that with probability 1− δ/2, for each i > C and all times t > Tδ/2 the sets Bit will consist\nof exactly LC + 1 arms that beat the arm ai, and that Bit = BiTδ/2 . 2. Moreover, if at time t > Tδ/2 > C(δ/4), Algorithm 1 chooses a non-Copeland winner as an optimistic Copeland\nwinner (i.e. i > C), then with probability 1− δ/4 we know that\nCpld(ai) ≥ Cpld(a1) ≥ Cpld(a1) = K − 1− LC .\n3. This means that there could be at most LC arms aj that optimistically lose to ai (i.e. uij < 0.5) and so at least one arm ab ∈ Bit does satisfy uib ≥ 0.5 4. This, in turn, means that in Line 13 of Algorithm 1 with probability 0.5 the arm ad will be chosen from Bit. 5. By Proposition 4, we know that with probability 1− δ/4, in the time interval [Tδ/2, T ] each arm aj ∈ BiTδ/2 can be\ncompared against ai at most N̂ δ/4 ij (T ) many times.\nGiven that by Fact 3 above we need at least one arm aj ∈ Bit to satisfy uij ≥ 0.5 for Algorithm 1 to set (c, d) = (i, j), and that by Fact 4 arms from Bit have a higher probability of being chosen to be compared against ai, this means that arm ai will be chosen as optimistic Copeland winner roughly twice as many times we had (c, d) = (i, j) for some j ∈ BiTδ/2 . A high probability version of the claim in the last sentence together with Fact 5 would give us the bound on regret claimed by the theorem. In the remainder of this proof, we will show that indeed the number of times we have c = i is unlikely to be too many times higher than twice the number of times we get (c, d) = (i, j), where j ∈ BiTδ/2 . To do so, we will introduce the following notation: N i: the number of time-steps between Tδ/2 and T when arm ai was chosen as optimistic Copeland winner. Bin: the indicator random variable that is equal to 1 if Line 13 in Algorithm 1 decided to choose arm ad only from\nthe set Bitn and zero otherwise, where tn is the n th time-step after Tδ/2 when arm ai was chosen as optimistic Copeland winner. Note that Bi is simply a Bernoulli random variable mean 0.5.\nN iB: the number of time-steps between Tδ and T when arm ai was chosen as optimistic Copeland winner and that Line 13 in Algorithm 1 chose to pick an arm from BiTδ/2 to be played against ai. Note that this definition implies that we have\nN iB = Ni∑ n=1 Bin. (8)\nMoreover, by Fact 5 above, we know that with probability 1− δ/4 we have N iB ≤ N̂ iB := ∑\nj∈BiTδ/2\nN̂ δ/4 ij (T ). (9)\nNow, we will use the above high probability bound onN iB to put the following high probability bound onN i: with\nprobability 1− δ/2 we have N i ≤ N̂ i := 2N̂ iB + 2 √ N̂ iB ln 2K\nδ .\nTo do so, let us assume that the we have N i > N̂ i and consider the first N̂ i time-steps after Tδ/2 when arm ai was chosen as optimistic Copeland winner and note that by Equation (8) we have\nN̂i∑ n=1 Bin ≤ N iB\nand so by Inequality (9) with probability 1 − δ/4 the left-hand side of the last inequality is bounded by N̂ iB: let us denote this event with E . On the other hand, if we apply the Hoeffding bound (cf. Appendix D) to the variables Bi1, . . . , B i N̂i , we get\nP ( E ∧ N i > N̂ i ) ≤ P  N̂i∑ n=1 Bin < N̂ i B  = P\n N̂i∑ n=1 Bin < N̂ i/2− √ N̂ iB ln 2K δ \n≤ e −\nA2N̂ i B ( ln 2Kδ )2 A2N̂ i B + A2 √ N̂ iB ln\n2K δ (10)\nTo simplify the last expression in the last chain of inequalities, let us use the notation α := N̂ iB and β := ln 2K δ . Given this notation, we claim that the following inequality holds if we have α ≥ 4 and β ≥ 2 (which hold by the assumptions of the theorem):\nαβ2\nα+ √ αβ ≥ β. (11)\nTo see this, let us multiply both sides by the denominator of the left-hand side of the above inequality:\nαβ2 ≥ αβ +√αβ. (12)\nTo see why Inequality (12) holds, let us note that the restrictions imposed on α and β imply the following pair of inequalities, whose sum is equivalent to Inequality (12):\nαβ2 ≥ 2αβ + αβ2 ≥ 2√αβ2 = 2αβ2 ≥ 2αβ + 2√αβ2\nNow that we know that Inequality (11) holds, we can combine it with Inequality (10) to get\nP ( E ∧ N i > N̂ i ) ≤ e − ln 2K δ = δ\n2K .\nTaking a union over the non-Copeland winning arms, we get\nP (E ∧ ∀ i > C, N i > N̂ i) > 1− δ/2.\nSo, given the fact that we have P (E) < δ/4, we know that with probability 1−δ each non-Copeland winner is selected as optimistic Copeland winner between Tδ/2 and T no more than N̂ i times."
    }, {
      "heading" : "H A Scalable Solution to the Copeland Bandit Problem",
      "text" : "In this section, we prove Lemma 14, providing an analysis to the PAC solver of the Copeland winner identification algorithm.\nTo simplify the proof, we begin by solving a slightly easier variant of Lemma 14 where the queries are deterministic. Specifically, rather than having a query to the pair (ai, aj) be an outcome of a Bernoulli r.v. with an expected value of pij , we assume that such a query simply yields the answer to whether pij > 0.5. Clearly, a solution can be obtained using K(K−1)/2 many queries but we aim for a solution with query complexity linear in K. In this section we prove the following.\nLemma 21. Given K arms and a parameter , Algorithm 2 finds a (1 + )-approximate best arm with probability at least 1− δ, by using at most\nlog(K/δ) · O ( K log(K) + min { K\n2 ,K2(1− cpld(a1)) }) many queries. In particular, when there is a Condorcet winner (cpld(a1) = 1) or more generally cpld(a1) = 1 − O(1/K), an exact solution can be found with probability at least 1− δ by using at most\nO (K log(K) log(K/δ))\nmany queries.\nThe idea behind our algorithm is as follows. We provide an unbiased estimator of the normalized Copeland score of arm ai by picking an arm aj uniformly at random and querying the pair (ai, aj). This method allows us to apply proof techniques for the classic MAB problem. These techniques provide a bound on the number of queries dependent on the gaps between the different Copeland scores. Our result is obtained by noticing that there cannot be too many arms with a large Copeland score; the formal statement is given later in Lemma 15. If the Copeland winner has a large Copeland score, i.e., LC is small, then only a small number of arms can be close to optimal. Hence, the main argument of the proof is that the majority of arms can be eliminated quickly and only a handful of arms must be queried many times.\nAs stated above, our algorithm uses as a black box Algorithm 4, an approximate-best-arm identification algorithm for the classical MAB setup. Recall that here, each arm ai has an associated reward µi and the objective is to identify an arm with the (approximately) largest reward. Without loss of geenrality, we assume that µ1 is the maximal reward. The following lemma provides an analysis of Algorithm 4 that is tight for the case where µ1 is close to 1. In this case, it is exactly the set of near optimal arms that will be queried many times hence it is important to take into consideration that the random variables associated with near optimal arms have a variance of roughly 1 − µi, which can be quite small. This translates to savings in the number of queries to arm ai by a factor of 1 − µi compared to an algorithm that does not take the variances into account.\nLemma 22. Algorithm 4 requires as input an error parameter , failure probability δ and an oracle to k Bernoulli distributions. It outputs, with probability at least 1 − δ, a (1 + )-approximate best arm, that is an arm ai with corresponding expected reward of µ ≥ 1− (1− µ1)(1 + ) with µ1 being the maximum expected value among arms. The expected number of queries made by the algorithm is upper bounded by\nO (∑\ni (1− µi) log(K/(δ∆i )) (∆ i) 2\n) ,\nwith ∆ i = max {µ1 − µi, (1− µ1)}. Moreover, with probability at least 1 − δ, the number of times arm i will be queried is at most\nO (\n(1− µi) log(K/(δ∆i )) (∆ i) 2\n) .\nWe prove Lemma 22 in Appendix I. For convenience, we denote by µi the normalized Copeland score of arm ai and µ1 the maximal normalized Copeland score. To get an informative translation of the above expression to our setting, let A be the set of arms with normalized Copeland score in (1 − 2(1 − µ1), µ1] and let Ā be the set of the other arms. In our setting, this query complexity of Algorithm 4 is upper bounded by\nO 2|A| log(K/δ) (1− µ1) 2 + ∑ i∈Ā log(K/δ)(1− µi) (µ1 − µi)2  , (13) assuming7 δ < (1− µ1) .\nIt remains to provide an upper bound for the above expression given the structure of the normalized Copeland scores. In particular, we use the results of Lemma 15, repeated here for convenience.\nLemma 15. Let D ⊂ [K] be the set of arms for which cpld(ai) ≥ 1− d/(K − 1), that is arms that are beaten by at most d arms. Then |D| ≤ 2d+ 1.\nWe bound the left summand in (13):\n2|A| log(K/δ) (1− µ1) 2 ≤ (4(1− µ1)(K − 1) + 2) log(K/δ) (1− µ1) 2 = O\n( log(K/δ)K\n2\n) . (14)\nWe now bound the right summand in (13). Let i ∈ Ā. According to the definition of Ā it holds that (1 − µi) ≤ 2(µ1 − µi). Hence: ∑\ni∈Ā\nlog(K/δ)(1− µi) (µ1 − µi)2 ≤ ∑ i∈Ā 4 log(K/δ) 1− µi .\nLemma 23. We have ∑\ni: µi<1\n1\n1− µi = O(K log(K)).\nProof. Let Aτ be the set of arms for which 2τ ≤ 1 − µi < 2τ+1. According to Lemma 15, we have that |Aτ | ≤ 2τ+2(K − 1) + 1. Other than that, since 1 ≥ 1 − µi ≥ 1/(K − 1) for all i > C we have that Aτ = ∅ for any τ ≤ − log2(K − 1)− 1 and τ > 0. It follows that:\n∑ i>C 1 1− µi ≤ dlog2(K−1)e∑ `=0 |A`−log2(K−1)| 2`−log2(K−1) ≤ dlog2(K−1)e∑ `=0 22+` + 1 2`−log2(K−1)\n≤ (dlog2(K − 1)e+ 1) · 5(K − 1).\nFrom (13), (14) and Lemma 23, we conclude that the total number of queries is bounded by O ( log(K/δ) ( K log(K) + K\n2\n)) .\nIn order to prove Lemma 21, it remains to analyze the case where is extremely small. Specifically, when 2(1− µ1) takes a value smaller than 1/K then the algorithm becomes inefficient in the sense that it queries the same pair more than once. This can be avoided by taking the samples of j when querying the score of arm ai to be uniformly random without replacement. The same arguments hold but are more complex as now the arm pulls are not i.i.d. Nevertheless, the required concentration bounds still hold. The resulting argument is that the number of queries is Õ ( log(1/δ) ( K + K̄2 )) with ̄ = max{ , 1/ (√ K(1− µ1) ) }. Lemma 21 immediately follows.\nWe are now ready to analyze the stochastic setting.\n7The value of δ we require is 1/T . If the assumption does not follow in that case, the regret must be linear and all of the statements hold trivially.\nProof of Lemma 14. By querying arm ai we choose a random arm j 6= i and in fact query the pair (ai, aj) sufficiently many times in order to determine whether pij > 0.5 with probability at least 1−δ/K2. Standard concentration bounds show that achieving this requires querying the pair (ai, aj) at most O ( log(K/(∆ijδ))∆ −2 ij ) many times. It follows that a single query to arm ai in the deterministic case translates into an expected number of\nO ( log(KHi/δ)) Hi\nK − 1\n) = O ( log(KH∞/δ)H∞\nK ) many queries in the stochastic setting. The claim now follows from the bound on the expected number of queries given in Lemma 21."
    }, {
      "heading" : "I KL-based approximate best arm identification algorithm",
      "text" : "Algorithm 4 solves an approximate best arm identification problem using confidence bounds based on Chernoff’s inequality stated w.r.t the KL-divergence of two random variables. Recall that for two Bernoulli random variables with parameters p, q the KL-divergence from q to p is defined as d(p, q) = (1 − p) ln((1 − p)/(1 − q)) + p ln(p/q) with 0 ln(0) = 0. The building block of Algorithm 4 is the well known Chernoff bound stating that for a Bernoulli random variable with expected value q, the probability of the average of n i.i.d samples from it to be smaller (larger) than p, for p < q (p > q), is bounded by exp(−nd(p, q)).\nAlgorithm 4 KL-best arm identification Input: Access to oracle giving a noisy approximation of the reward of arm i for K arms, success probability δ > 0,\napproximation parameter > 0 1: for all i ∈ [K] do 2: T = 1 3: Si ← reward(i) 4: Ii ← [0, 1] 5: end for 6: B ← [K] 7: t← 2 8: while 1−maxi∈B min Ii1−maxi∈B max Ii > (1 + ) do 9: For all i ∈ B, Si ← Si + reward(i)\n10: For all i ∈ B, let Ii = {q ∈ [0, 1], t · d(Sit , q) ≤ ln(4tK/δ) + 2 ln ln(t)} 11: For all i ∈ B for which there exist some j ∈ B with max{q ∈ Ii} < min{q ∈ Ij}, remove i from B. 12: t← t+ 1 13: end while Return: arg maxi∈B min Ii.\nProof of Lemma 22. We use an immediate application of the Chernoff-Hoeffding bound\nLemma 24. Fix i ∈ [K]. Let Eit denote the event that at iteration t, µi /∈ Ii. We have that Pr[Eit ] ≤ 2 δ4tK · 1log(t)2 ≤ δ 2t log(t)2K .\nLet E denote the union, over all t, i of events Eit . That is, E denotes the event in which there exist some iteration t, and for some arm ai such that µi /∈ Ii. By the above lemma we get that\nPr[E] ≤ ∑ t ∑ i Pr[Eit ] ≤ K ∞∑ t=2\nδ\n2t log(t)2K ≤ δ\nIt follows that given that event E did not happen, the algorithm will never eliminate the top arm and furthermore, will output an (1 + )-approximate best arm. We proceed to analyze the total number of pulls per arm, while having a\nseparate analysis for (1+ )-approximate best arms and the other arms. We begin by stating an auxiliary lemma giving explicit bounds for the confidence regions.\nLemma 25. Assume that event E did not occur and let ρ ≥ 0. For a sufficiently large universal constant c we have for any t ≥ c log(tK/δ)(1−µi)ρ2 that max Ii < µi + ρ. Also, for t ≥ c log(tK/δ)(1−µi+ρ/2) ρ2 it holds that min Ii > µ− ρ.\nProof. We consider the Taylor series associated with f(x) = d(p + x, p). Since f(0) = f ′(0) = 0 it holds that for any x ≤ 1− p there exists some |x′| ≤ |x| with\nf(x) = x2f ′′(x′) = x2 (p+ x′)(1− p− x′) ≤ 2x2 1− p\nTo prove that max Ii < µi+ρ we apply the above observation for ρ ≤ 1−µi (otherwise µi+ρ > 1 and the claim is trivial) and reach the conclusion that for sufficiently large universal constant c it holds that\nt · d(µi + ρ/2, µi) > log(tK/δ) + 2 log log(tK/δ)\nt · d(µi + ρ/2, µi + ρ) > log(tK/δ) + 2 log log(tK/δ) The first inequality dictates that Si/t ≤ µi + ρ/2. The second inequality dictates that t · d(Si/t, µi + ρ) ≥ d(µi + ρ/2, µi + ρ) is too large in order for µi + ρ to be an element of Ii.\nThe bound for min Ii is analogous. Since now we have t ≥ c log(tK/δ)(1−µi+ρ/2)ρ2 , it holds that\nt · d(µi − ρ/2, µi) > log(tK/δ) + 2 log log(tK/δ)\nt · d(µi − ρ/2, µi − ρ) > log(tK/δ) + 2 log log(tK/δ) This means that first, Si/t ≥ µi − ρ/2 and second, that t · d(Si/t, µi − ρ) ≥ d(µi − ρ/2, µi − ρ) is too large in order for µi − ρ to be an element of Ii.\nLemma 26. Let i be a suboptimal arm, meaning one where µi ≤ 1− (1− µ1)(1 + ). Denote by ∆i its gap µ1 − µi. If event E does not occur then i is queried at most O ( log ( K δ∆i ) vi\n(∆i)2\n) many times, where vi = 1− µi\nProof. We first notice that as we are assuming that event E did not happen, it must be the case that arm 1 is never eliminated from B. Consider an iteration t such that\nt ≥ c log(tK/δ)vi (∆i)2\n(15)\nfor a sufficiently large c, then according to Lemma 25 it holds that max Ii < µi + ∆i/2. Now, since vi = 1 − µi ≥ 1−µ1+∆i/2 we have that for the same t it must be the case that min I1 > µ1−∆i/2. It follows that min I1 > max Ii and arm ai is eliminated at round t.\nLemma 27. Assume ≤ 1. If event E does not occur then for some sufficiently large universal constant c it holds that when t ≥ c log(tK/δ)(1−µ1) 2 the algorithm terminates.\nProof. Let i be an arbitrary arm. Since\nt ≥ c log(tK/δ) (1− µ1) 2 = c log(tK/δ)(1− µi) (1− µ1)(1− µi) 2\nwe get, according to Lemma 25 that\nmax Ii ≤ µi +\n3\n√ (1− µi)(1− µ1)\nIn order to bound √ (1− µi)(1− µ1) we consider the function f(x) = √ v(v + x). Notice that f(0) = v and f ′(x) = v 2 √ v(v+x) ≤ 12 for x ≥ 0. It follows that for positive x, √ v(v + x) ≤ v + x/2, meaning that\nmax Ii ≤ µi + ((1− µi) + ∆i/2)\n3 ≤ µ1 + (1− µ1) 3\nNow, since ≤ 1 we have\nt ≥ c log(tK/δ)(1− µ1) (1− µ1)2 2 ≥ (c/2) log(tK/δ)(1− µ1 + (1− µ1)) (1− µ1)2 2\nhence for sufficiently large c we can apply Lemma 25 and obtain\nmin I1 ≥ µ1 − (1− µ1)\n3\nIt follows that assuming ≤ 1, min I1 ≥ 1− ( 1−max\ni Ii\n) (1 + )\nmeaning that the algorithm will terminate at iteration t.\nThis concludes the proof of Lemma 22"
    } ],
    "references" : [ {
      "title" : "The K-armed dueling bandits problem",
      "author" : [ "Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims" ],
      "venue" : "Journal of Computer and System Sciences, 78(5),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "T. Joachims" ],
      "venue" : "KDD,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Beat the mean bandit",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "ICML,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval",
      "author" : [ "K. Hofmann", "S. Whiteson", "M. de Rijke" ],
      "venue" : "Information Retrieval,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Multileaved comparisons for fast online evaluation",
      "author" : [ "A. Schuth", "F. Sietsma", "S. Whiteson", "D. Lefortier", "M. de Rijke" ],
      "venue" : "In CIKM,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Toward predicting the outcome of an A/B experiment for search relevance",
      "author" : [ "L. Li", "J. Kim", "I. Zitouni" ],
      "venue" : "WSDM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Relative confidence sampling for efficient on-line ranker evaluation",
      "author" : [ "M. Zoghi", "S. Whiteson", "M. de Rijke", "R. Munos" ],
      "venue" : "In WSDM,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "A new monotonic, clone-independent, reversal symmetric, and Condorcet-consistent single-winner election method",
      "author" : [ "M. Schulze" ],
      "venue" : "Social Choice and Welfare, 36(2):267–303,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Generic exploration and k-armed voting bandits",
      "author" : [ "T. Urvoy", "F. Clerot", "R. Féraud", "S. Naamane" ],
      "venue" : "ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Top-k selection based on adaptive sampling of noisy preferences",
      "author" : [ "R. Busa-Fekete", "B. Szörényi", "P. Weng", "W. Cheng", "E. Hüllermeier" ],
      "venue" : "ICML,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "PAC rank elicitation through adaptive sampling of stochastic pairwise preferences",
      "author" : [ "R. Busa-Fekete", "B. Szörényi", "E. Hüllermeier" ],
      "venue" : "AAAI,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Relative upper confidence bound for the K-armed dueling bandits problem",
      "author" : [ "M. Zoghi", "S. Whiteson", "R. Munos", "M. de Rijke" ],
      "venue" : "In ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Online controlled experiments at large scale",
      "author" : [ "R. Kohavi", "A. Deng", "B. Frasca", "T. Walker", "Y. Xu", "N. Pohlmann" ],
      "venue" : "KDD,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : "Biometrika, pages 285–294,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1933
    }, {
      "title" : "Reducing dueling bandits to cardinal bandits",
      "author" : [ "N. Ailon", "Z. Karnin", "T. Joachims" ],
      "venue" : "ICML,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "MergeRUCB: A method for large-scale online ranker evaluation",
      "author" : [ "M. Zoghi", "S. Whiteson", "M. de Rijke" ],
      "venue" : "In WSDM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Iterative ranking from pair-wise comparisons",
      "author" : [ "S. Negahban", "S. Oh", "D. Shah" ],
      "venue" : "NIPS,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Contextual dueling bandits",
      "author" : [ "M. Dudı́k", "K. Hofmann", "R.E. Schapire", "A. Slivkins", "M. Zoghi" ],
      "venue" : "In COLT,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Discrete prediction games with arbitrary feedback and loss",
      "author" : [ "A. Piccolboni", "C. Schindelhauer" ],
      "venue" : "COLT,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "An adaptive algorithm for finite stochastic partial monitoring",
      "author" : [ "G. Bartók", "N. Zolghadr", "C. Szepesvári" ],
      "venue" : "ICML,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Kullback–leibler upper confidence bounds for optimal sequential allocation",
      "author" : [ "O. Cappé", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Multi-armed bandits in metric space",
      "author" : [ "R. Kleinberg", "A. Slivkins", "E. Upfa" ],
      "venue" : "STOC,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "X-armed bandits",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesvari" ],
      "venue" : "JMLR, 12,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Gaussian process optimization in the bandit setting: No regret and experimental design",
      "author" : [ "N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger" ],
      "venue" : "ICML,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Optimistic optimization of a deterministic function without the knowledge of its smoothness",
      "author" : [ "R. Munos" ],
      "venue" : "NIPS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Convergence rates of efficient global optimization algorithms",
      "author" : [ "A.D. Bull" ],
      "venue" : "JMLR, 12,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Exponential regret bounds for Gaussian process bandits with deterministic observations",
      "author" : [ "N. de Freitas", "A. Smola", "M. Zoghi" ],
      "venue" : "In ICML,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Stochastic simultaneous optimistic optimization",
      "author" : [ "M. Valko", "A. Carpentier", "R. Munos" ],
      "venue" : "ICML,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "ICML,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Axiomatic foundations for ranking systems",
      "author" : [ "A. Altman", "M. Tennenholtz" ],
      "venue" : "JAIR,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "C. Manning", "P. Raghavan", "H. Schütze" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "How does clickthrough data reflect retrieval quality",
      "author" : [ "F. Radlinski", "M. Kurup", "T. Joachims" ],
      "venue" : "In CIKM,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2008
    }, {
      "title" : "A probabilistic method for inferring preferences from clicks",
      "author" : [ "K. Hofmann", "S. Whiteson", "M. de Rijke" ],
      "venue" : "In CIKM",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2011
    }, {
      "title" : "An experimental comparison of click position-bias models",
      "author" : [ "N. Craswell", "O. Zoeter", "M. Taylor", "B. Ramsey" ],
      "venue" : "WSDM ’08, pages 87–94,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficient multiple-click models in web search",
      "author" : [ "F. Guo", "C. Liu", "Y. Wang" ],
      "venue" : "WSDM ’09, pages 124–131, New York, NY, USA,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Fidelity, soundness, and efficiency of interleaved comparison methods",
      "author" : [ "K. Hofmann", "S. Whiteson", "M. de Rijke" ],
      "venue" : "ACM Transactions on Information Systems,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2013
    }, {
      "title" : "Mathematical games: The paradox of the nontransitive dice and the elusive principle of indifference",
      "author" : [ "M. Gardner" ],
      "venue" : "Scientific American, 223:110–114,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "A survey of preference-based online learning with bandit algorithms",
      "author" : [ "R. Busa-Fekete", "E. Hüllermeier" ],
      "venue" : "Algorithmic Learning Theory, pages 18–39. Springer,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction The dueling bandit problem [1] arises naturally in domains where feedback is more reliable when given as a pairwise preference (e.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "Examples include ranker evaluation [2–4] in information retrieval, ad placement and recommender systems.",
      "startOffset" : 35,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "Examples include ranker evaluation [2–4] in information retrieval, ad placement and recommender systems.",
      "startOffset" : 35,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "Examples include ranker evaluation [2–4] in information retrieval, ad placement and recommender systems.",
      "startOffset" : 35,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "For example, in industrial ranker evaluation [6], when many rankers must be compared, each comparison corresponds to a costly live experiment and thus the potential for failure if no Condorcet winner exists is unacceptable [7].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "For example, in industrial ranker evaluation [6], when many rankers must be compared, each comparison corresponds to a costly live experiment and thus the potential for failure if no Condorcet winner exists is unacceptable [7].",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 7,
      "context" : "The non-existence of the Condorcet winner has been investigated extensively in social choice theory, where numerous definitions have been proposed, without a clear contender for the most suitable resolution [9].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : ", SAVAGE [10], PBR [11] and RankEl [12], which use some of the notions proposed by social choice theorists, such as the Copeland score or the Borda score to measure the quality of each arm, hence determining what constitutes the best arm (or more generally the top-k arms).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 9,
      "context" : ", SAVAGE [10], PBR [11] and RankEl [12], which use some of the notions proposed by social choice theorists, such as the Copeland score or the Borda score to measure the quality of each arm, hence determining what constitutes the best arm (or more generally the top-k arms).",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : ", SAVAGE [10], PBR [11] and RankEl [12], which use some of the notions proposed by social choice theorists, such as the Copeland score or the Borda score to measure the quality of each arm, hence determining what constitutes the best arm (or more generally the top-k arms).",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "The first algorithm, called Copeland Confidence Bound (CCB), is inspired by the recently proposed Relative Upper Confidence Bound method [13], but modified and extended to address the unique challenges that arise when no Condorcet winner exists.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "For example, at Bing, 200 experiments are run concurrently on any given day [14], in which case the duration of the experiment needs to be longer than the age of the universe in nanoseconds before K log T becomes significant in comparison to K.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "The K-armed dueling bandit problem [1] is a modification of the K-armed bandit problem [15].",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "The K-armed dueling bandit problem [1] is a modification of the K-armed bandit problem [15].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "Most previous work assumes the existence of a Condorcet winner [10]: an arm, which without loss of generality we label a1, such that p1i > 12 for all i > 1.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "However, Condorcet winners do not always exist [8, 13].",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "However, Condorcet winners do not always exist [8, 13].",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "3 Related Work Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "3 Related Work Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 6,
      "context" : "3 Related Work Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "3 Related Work Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 14,
      "context" : "MultiSBM [16], and mergeRUCB [17], all of which require the existence of a Condorcet winner, and often come with bounds of the form O(K log T ).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "MultiSBM [16], and mergeRUCB [17], all of which require the existence of a Condorcet winner, and often come with bounds of the form O(K log T ).",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "However, as observed in [13] and Appendix C.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "There is another group of algorithms that do not assume the existence of a Condorcet winner, but have bounds of the form O(K2 log T ) in the Copeland setting: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) [10], Preference-Based Racing (PBR) [11] and Rank Elicitation (RankEl) [12].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 9,
      "context" : "There is another group of algorithms that do not assume the existence of a Condorcet winner, but have bounds of the form O(K2 log T ) in the Copeland setting: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) [10], Preference-Based Racing (PBR) [11] and Rank Elicitation (RankEl) [12].",
      "startOffset" : 262,
      "endOffset" : 266
    }, {
      "referenceID" : 10,
      "context" : "There is another group of algorithms that do not assume the existence of a Condorcet winner, but have bounds of the form O(K2 log T ) in the Copeland setting: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) [10], Preference-Based Racing (PBR) [11] and Rank Elicitation (RankEl) [12].",
      "startOffset" : 297,
      "endOffset" : 301
    }, {
      "referenceID" : 8,
      "context" : "In addition to the above, bounds have been proven for other notions of winners, including Borda [10–12], Random Walk [11, 18], and very recently von Neumann [19].",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "In addition to the above, bounds have been proven for other notions of winners, including Borda [10–12], Random Walk [11, 18], and very recently von Neumann [19].",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "In addition to the above, bounds have been proven for other notions of winners, including Borda [10–12], Random Walk [11, 18], and very recently von Neumann [19].",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "In addition to the above, bounds have been proven for other notions of winners, including Borda [10–12], Random Walk [11, 18], and very recently von Neumann [19].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "In addition to the above, bounds have been proven for other notions of winners, including Borda [10–12], Random Walk [11, 18], and very recently von Neumann [19].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "In addition to the above, bounds have been proven for other notions of winners, including Borda [10–12], Random Walk [11, 18], and very recently von Neumann [19].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 18,
      "context" : "A related setting is that of partial monitoring games [20].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "In [21], the authors present problem-dependent bounds from which a regret bound of the formO(K2 log T ) can be deduced for the dueling bandit problem, whereas our work achieves a linear dependence in K.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "To this end, we use the KL-based arm-elimination algorithm (a slight modification of Algorithm 2 in [22]) described in Algorithm 4 in Appendix I.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Now that we have the high probability regret bound given in Theorem 11, we can deduce the expected regret result claimed in (1) for α > 1, as a corollary by integrating δ over the interval [0, 1].",
      "startOffset" : 189,
      "endOffset" : 195
    }, {
      "referenceID" : 21,
      "context" : "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 23,
      "context" : "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 24,
      "context" : "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 25,
      "context" : "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 26,
      "context" : "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 27,
      "context" : "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 28,
      "context" : "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23–29] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].",
      "startOffset" : 364,
      "endOffset" : 368
    }, {
      "referenceID" : 10,
      "context" : "Finally, it is also interesting to expand our results to handle scores other than the Copeland score, such as an insensitive variant of the Copeland score (as in [12]), or completely different notions of winners, such as the Borda, the Random Walk or the von Neumann winners (see, e.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 17,
      "context" : ", [19, 31]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 29,
      "context" : ", [19, 31]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "[1] Y.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] T.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Y.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] K.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7] L.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[8] M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[9] M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[10] T.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[11] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] W.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] N.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[19] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[20] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] G.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] O.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] N.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[27] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[28] N.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[29] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[30] Y.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[31] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[32] C.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[33] F.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[34] K.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[35] N.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[36] F.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[37] K.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[38] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[39] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[40] N.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "The first is a 5-armed problem arising from ranker evaluation in the field of information retrieval (IR) [32].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "All three experiments follow the experimental approach in [3, 13] and use the given preference matrix to simulate comparisons between each pair of arms (ai, aj) by drawing samples from Bernoulli random variables with mean pij .",
      "startOffset" : 58,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "All three experiments follow the experimental approach in [3, 13] and use the given preference matrix to simulate comparisons between each pair of arms (ai, aj) by drawing samples from Bernoulli random variables with mean pij .",
      "startOffset" : 58,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "We compare our two proposed algorithms against the state of the art K-armed dueling bandit algorithm, RUCB [13], and Copeland SAVAGE, PBR and RankEl.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "The first experiment uses a 5-armed problem arising from ranker evaluation in the field of information retrieval (IR) [32], detailed in Appendix B.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "We omit SAVAGE, PBR and RankEl from this experiment because they scale poorly in the number of arms [10–12].",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "We omit SAVAGE, PBR and RankEl from this experiment because they scale poorly in the number of arms [10–12].",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "We omit SAVAGE, PBR and RankEl from this experiment because they scale poorly in the number of arms [10–12].",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 31,
      "context" : "One effective way to achieve this is to use interleaved comparisons [33], which interleave the ranked lists of documents proposed by two rankers and present the resulting list to the user, whose subsequent click feedback is used to infer a noisy preference for one of the rankers.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "The ranker evaluation task in this context corresponds to determining which single feature constitutes the best ranker [4].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 32,
      "context" : "To compare a pair of rankers, we use probabilistic interleave (PI) [34], a recently developed method for interleaved comparisons.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 32,
      "context" : "To model the user’s click behavior on the resulting interleaved lists, we employ a probabilistic user model [34, 35] that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the MSLR dataset.",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 33,
      "context" : "To model the user’s click behavior on the resulting interleaved lists, we employ a probabilistic user model [34, 35] that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the MSLR dataset.",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 34,
      "context" : "Queries are sampled randomly and clicks are generated probabilistically by conditioning on these assessments in a way that resembles the behavior of an actual user [36].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 35,
      "context" : "Specifically, we employ an informational click model in our ranker evaluation experiments [37].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 35,
      "context" : "The informational click model is one of the three click models utilized in the ranker evaluation literature, along with the perfect and navigational click models [37].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "Following [3, 13], we first use the above approach to estimate the comparison probabilities pij for each pair of rankers and then use these probabilities to simulate comparisons between rankers.",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "Following [3, 13], we first use the above approach to estimate the comparison probabilities pij for each pair of rankers and then use these probabilities to simulate comparisons between rankers.",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "We hypothesize that this is because the informational click model explores more of the list of ranked documents than the navigational click model, which was used in [13], and so it is more likely to encounter non-transitivity phenomena of the sort described in [38].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 36,
      "context" : "We hypothesize that this is because the informational click model explores more of the list of ranked documents than the navigational click model, which was used in [13], and so it is more likely to encounter non-transitivity phenomena of the sort described in [38].",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 17,
      "context" : "The Copeland winner, as discussed in this paper, and the von Neumann winner [19] satisfy this property, while the Borda (a.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 37,
      "context" : "winners [39] do not.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "In the case of the von Neumann winner, which is defined as a probability distribution over the set of arms [19], we used the support of the distribution (i.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : ", XN with common range [0, 1] satisfying E[Xn|X1, .",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 11,
      "context" : "Here, we will quote a useful Lemma that we will refer to repeatedly in our proofs: Lemma 17 (Lemma 1 in [13]).",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 4 KL-best arm identification Input: Access to oracle giving a noisy approximation of the reward of arm i for K arms, success probability δ > 0, approximation parameter > 0 1: for all i ∈ [K] do 2: T = 1 3: Si ← reward(i) 4: Ii ← [0, 1] 5: end for 6: B ← [K] 7: t← 2 8: while 1−maxi∈B min Ii 1−maxi∈B max Ii > (1 + ) do 9: For all i ∈ B, Si ← Si + reward(i) 10: For all i ∈ B, let Ii = {q ∈ [0, 1], t · d(i t , q) ≤ ln(4tK/δ) + 2 ln ln(t)} 11: For all i ∈ B for which there exist some j ∈ B with max{q ∈ Ii} < min{q ∈ Ij}, remove i from B.",
      "startOffset" : 239,
      "endOffset" : 245
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 4 KL-best arm identification Input: Access to oracle giving a noisy approximation of the reward of arm i for K arms, success probability δ > 0, approximation parameter > 0 1: for all i ∈ [K] do 2: T = 1 3: Si ← reward(i) 4: Ii ← [0, 1] 5: end for 6: B ← [K] 7: t← 2 8: while 1−maxi∈B min Ii 1−maxi∈B max Ii > (1 + ) do 9: For all i ∈ B, Si ← Si + reward(i) 10: For all i ∈ B, let Ii = {q ∈ [0, 1], t · d(i t , q) ≤ ln(4tK/δ) + 2 ln ln(t)} 11: For all i ∈ B for which there exist some j ∈ B with max{q ∈ Ii} < min{q ∈ Ij}, remove i from B.",
      "startOffset" : 400,
      "endOffset" : 406
    } ],
    "year" : 2015,
    "abstractText" : "A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the formO(K log T ) but require restrictive assumptions, or offer bounds of the formO(K log T ) without requiring such assumptions. Our results offer the best of both worlds: O(K log T ) bounds without restrictive assumptions.",
    "creator" : "LaTeX with hyperref package"
  }
}