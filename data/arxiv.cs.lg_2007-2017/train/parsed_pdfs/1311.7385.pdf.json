{
  "name" : "1311.7385.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Algorithmic Identification of Probabilities",
    "authors" : [ "Paul M.B. Vitányi" ],
    "emails" : [ "paulv@cwi.nl.", "Nick.Chater@wbs.ac.uk." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 1.\n73 85\nv3 [\ncs .L\nG ]\n1 1\nJu l 2\n01 4\nThe problem is to identify a probability associated with a set of natural numbers, given an infinite data sequence of elements from the set. If the given sequence is drawn i.i.d. and the probability mass function involved (the target) belongs to a computably enumerable (c.e.) or co-computably enumerable (co-c.e.) set of computable probability mass functions, then there is an algorithm to almost surely identify the target in the limit. The technical tool is the strong law of large numbers. If the set is finite and the elements of the sequence are dependent while the sequence is typical in the sense of Martin-Löf for at least one measure belonging to a c.e. or co-c.e. set of computable measures, then there is an algorithm to identify in the limit a computable measure for which the sequence is typical (there may be more than one such measure). The technical tool is the theory of Kolmogorov complexity. We give the algorithms and consider the associated predictions.\nI. INTRODUCTION\nOne can associate the natural numbers with a lexicographic length-increasing ordering of finite strings over a finite alphabet. A natural number corresponds to the string of which it is the position in this order. Since a language is a set of sentences (finite strings over a finite alphabet), it can be viewed as the set of natural numbers. The learnability of a language under various computational assumptions is the subject of an immensely influential approach in [5] and especially [6], or the review [9]. But surely in the real world the chance of one sentence of a language being used is different from another one. For example, in general short sentences have a larger chance of turning up than very long sentences. Thus, the elements of a given language are distributed in a certain way. There arises the problem of identifying or approximating this distribution.\nVitányi is with the National Research Institute for Mathematics and Computer Science in the Netherlands (CWI) and the University of Amsterdam. Address: CWI, Science Park 123, 1098 XG, Amsterdam, The Netherlands. Email: paulv@cwi.nl.\nChater is with the Behavioural Science Group. Address: Warwick Business School, University of Warwick, Coventry, CV4 7AL, UK. Email: Nick.Chater@wbs.ac.uk. Chater was supported by ERC Advanced Grant “Cognitive and Social Foundations of Rationality.”\nDRAFT\nOur model is formulated as follows: we are given an infinite sequence of data consisting of elements drawn from the set (language) according to a certain probability, and the learner has to identify this probability. In general, however much data been encountered, there is no point at which the learner can announce a particular probability as correct with certainty. Weakening the learning model, the learner might learn to identify the correct probability in the limit. That is, perhaps the learner might make a sequence of guesses, finally locking on to correct probability and sticking to it forever—even though the learner can never know for sure that it has identified the correct probability successfully. We shall consider identification in the limit (following, for example, [6], [9], [16]). Since this is not enough we additionally restrict the type of probability.\nIn conventional statistics, probabilistic models are typically idealized as having continuous valued parameters; and hence there is an uncountable number of possible probabilities. In general it is impossible that a learner can make a sequence of guesses that precisely locks on to the correct values of continuous parameters. In the realm of algorithmic information theory, in particular in Solomonoff induction [18] and here, we reason as follows. The possible strategies of learners are computable in the sense of Turing [19], that is, they are computable functions. The set of these is discrete and thus countable. The hypotheses that can be learned are therefore countable, and in particular the set of probabilites from which the learner chooses must be computable.\nWe consider two cases. In case 1 the data are drawn independent identically distributed (i.i.d.) from a set of natural numbers according to a probability mass function in a co-c.e. set of computable probability mass functions. In case 2 the set is finite and the elements of the infinite sequence are dependent and the data sequence is typical for a measure from a co-c.e. subset of computable measures."
    }, {
      "heading" : "A. Preliminaries",
      "text" : "Let N denote the natural numbers, and R the real numbers. We say that we identify a function f in the limit if we have an algorithm which produces an infinite sequence f1, f2, . . . of functions and fi = f for all but finitely many i. This corresponds to the notion of “identification in the limit” in [6], [9], [16], [20]. In this notion at every step an object is produced and after a finite number of steps the target object is produced at every step. However, we do not know this finite number. It is as if you ask directions and the answer is “at the last intersection turn right,” but you do not know which intersection is last. In the sequel we often “dovetail” a computation. This is a technique that interleaves the steps of different computations ensuring progress of each individual computation. For example, we have computations\nc1, c2. Dovetailing them means first performing step 1 of c1, then performing step 2 of c1 followed by step 1 of c2, then performing step 3 of c1 followed by step 2 of c2, and so on."
    }, {
      "heading" : "B. Related work",
      "text" : "In [1] (citing previous more restricted work) a target probability mass function was identified in the limit when the data are drawn i.i.d. in the following setting. Let the target probability mass function p be an element of a list q1, q2, . . . subject to the following conditions: (i) every qi : N → R is a probability mass function; (ii) we exhibit a computable total function C(i, x, ǫ) = r such that qi(x) − r ≤ ǫ with r, ǫ > 0 are rational numbers. That is, there exists a rational number approximation for all probability mass functions in the list up to arbitrary precision, and we give a single algorithm which for each such function exhibits such an approximation. The technical means used are the law of the iterated logarithm and the Kolmogorov-Smirnov test. However, the list q1, q2, . . . can not contain all computable probability mass functions because of a diagonal argument, Lemma 1.\nIn [2] computability questions are apparently ignored. The Conclusion states “If the true density [and hence a probability mass function] is finitely complex [it is computable] then it is exactly discovered for all sufficiently large sample sizes.”. The tool that is used is estimation according to minq(L(q) + log(1/ ∏n\ni=1 q(Xi)). Here q is a probability mass function, L(q) is the length of its code and q(Xi)\nis the q-probability of the ith random variable Xi. To be able to minimize over the set of computable q’s, one has to know the L(q)’s. If the set of candidate distributions is countably infinite, then we can never know when the minimum is reached—hence at best we have then identification in the limit. If L(q) is identified with the Kolmogorov complexity K(q), as in Section IV of this reference, then it is incomputable as already observed by Kolmogorov in [12] (for the plain Kolmogorov complexity; the case of the prefix Kolmogorov complexity K(q) is the same). Computable L(q) (given q) cannot be computably enumerated; if they were this would constitute a computable enumeration of computable q’s which is impossible by Lemma 1. To obtain the minimum we require a computable enumeration of the L(q)’s in the estimation formula. The results hold (contrary to what is claimed in the Conclusion of [2] and other parts of the text) not for the set of computable probability mass functions since they are not c.e.. The sentence “you know but you don’t know you know” on the second page of [2] does not hold for an arbitrary computable mass probability.\nIn reaction to an earlier version of this paper with too large claims, in [4] it is shown that it is impossible to identify a computable measure in the limit given an infinite sequence of elements from its support\nwhich sequence is guarantied to be typical for some computable measure."
    }, {
      "heading" : "C. Results",
      "text" : "The set of halting algorithms for computable probabilities (or measures) is not c.e., Lemma 1 in Appendix A. This complicates the algorithms and analysis of the results. In Section II there is a computable probability mass function (the target) on a set of natural numbers. We are given an infinite sequence of elements of this set that are drawn i.i.d., and are asked to identify the target. An algorithm is presented which identifies the target in the limit almost surely provided the target is an element of a c.e. or co-c.e. set of halting algorithms for probability mass functions (Theorem 1). This underpins partially the result announced in [8]. The technical tool is the strong law of large numbers. In Section III the set of natural numbers is finite and the elements of the sequence are allowed to be dependent. We are given a guaranty that the sequence is typical (Definition 1) for at least one measure from a c.e. or co-c.e. set of halting algorithms for computable measures. There is an algorithm which identifies in the limit a computable measure for which the data sequence is typical (Theorem 2). The technical tool is the Martin-Löf theory of sequential tests [15] based on Kolmogorov complexity. In Section IV we consider the associated predictions, and in Section V we give conclusions. In Appendix A we review the used computability notions, in Appendix B we review notions of Kolmogorov complexity, in Appendix C we review the used measure and computability notions. We defer the proofs of the theorems to Appendix D."
    }, {
      "heading" : "II. COMPUTABLE PROBABILITY MASS FUNCTIONS AND I.I.D. DRAWING",
      "text" : "To approximate a probability in the i.i.d. setting is well-known and an easy example to illustrate our problem. One does this by an algorithm computing the probability p(a) in the limit for all a ∈ L ⊆ N almost surely given the infinite sequence x1, x2, . . . of data i.i.d. drawn from L according to p. Namely, for n = 1, 2, . . . for every a ∈ L occurring in x1, x2, . . . , xn set pn(a) equal to the frequency of occurrences of a in x1, x2, . . . , xn. Note that the different values of pn sum to precisely 1 for every n = 1, 2, . . . . The output is a sequence p1, p2, . . . of probability mass functions such that we have limn→∞ pn = p almost surely, by the strong law of large numbers (see Claim 1). The probability mass functions considered here consist of all probability mass functions on L—computable or not. The probability mass function p is represented by an approximation algorithm. In the limit p is reached almost surely.\nHere we deal only with computable probability mass functions. If p is computable then it can be represented by a halting algorithm which computes it as defined in Appendix A. Most known probability\nmass functions are computable provided their parameters are computable. In order that it is computable we only require that the probability mass function is finitely describable and there is a computable process producing it [19].\nOne issue is how short the code for p is, a second issue are the computability properties of the code for p, a third issue is how much of the data sequence is used in the learning process. The approximation of p results in a sequence of codes of probabilities p1, p2, . . . which are a list of the sample frequencies in an initial finite segment of the data sequence. The code length of this list grows to infinity as the length of the segment grows to infinity. The learning process uses all of the data sequence and the result is an encoding of the sample frequencies in the data sequence in the limit. This holds also if p is computable.\nTHEOREM 1: I.I.D. COMPUTABLE PROBABILITY IDENTIFICATION Let L be a set of natural numbers and p be a probability mass function on L which is an element of a c.e. or co-c.e. set of halting algorithms for computable probability mass functions. There is an algorithm identifying p in the limit almost surely from an infinite sequence x1, x2, . . . of elements of L drawn i.i.d. according to p. The code of p via an appropriate Turing machine is finite. The learning process uses only a finite initial segment of the data sequence and takes finite time. We do not know how large the finite items in the thorem are. We give an outline of the proof of Theorem 1. The proof itself is deferred to Appendix D. We start by extending the strong law of large numbers to probability mass functions on subsets of N . By assumption the target probability mass function p is a member of a c.e. or co-c.e. set of halting algorithms for computable probability mass functions listed as list A. If q is in list A and q = p, then for every ǫ > 0 we have p(a)− q(a) < ǫ for all a ∈ L. If q is in list A and q 6= p, then for some a ∈ L there is a constant δ > 0 such that |p(a) − q(a)| > δ. For every n = 1, 2, . . . we estimate p(a) for all a ∈ L by the number of occurrences of a in the n-length initial segment of the provided data sequence.\nLet #a(x1, . . . , xn) denote the number of times a = xi (1 ≤ i ≤ n). For qi = p almost surely limn→∞maxa∈L |#a(x1, . . . , xn)/n − qnk (a)| = 0, and for qi 6= p almost surely limn→∞maxa∈L |#a(x1, . . . , xn)/n − qni (a)| > 0. Hence we determine for each n = 1, 2, . . . the least index i (1 ≤ i ≤ n) in the list A for which |qi(a)−#a(x1, . . . , xn)/n| is minimal. This index is called in. Let qk = p with k least. Eventually the initial k-length segment of the list A is co-computably enumerated. Hence there is a finite n0 such that for all n ≥ n0 we have in = k, but we do not know how large n0 is. This means that p is identified in the limit.\nREMARK 1: Since the c.e. and co-c.e. sets strictly contain the computable sets, Theorem 1 is strictly\nstronger than the result in [1] referred to in Section I-B. It is more theoretical but strictly stronger than [2] that does not give identification in the limit for classes of computable functions.\nDefine the primitive recursive probability mass functions as the set of probability mass functions for which it is decidable that they are constructed from primitive recursive functions. Since this set is computable it is c.e.. The theorem shows that identification in the limit is possible for members of this set. Define the time-bounded probability mass functions for any fixed computable time bound as the set of elements for which it is decidable that they are probability mass functions satisfying this time bound. Since this set is computable it is c.e.. Again, the theorem shows that identification in the limit is possible for elements from this set.\nAnother example is as follows. Let L = {a1, a2, . . . , an} be a finite set. The primitive recursive functions f1, f2, . . . are c.e.. Hence the probability mass functions p1, p2, . . . on L defined by pi(aj) = fi(j)/ ∑n h=1 fi(h) are also c.e.. Let us call these probability mass functions simple. By Theorem 1 they can be identified in the limit. Following the proof of Theorem 1 in Appendix D, we give another example in Example 2. ✸"
    }, {
      "heading" : "III. COMPUTABLE MEASURES",
      "text" : "As far as the authors are aware, for general measures there exist neither an approximation as in Section II nor an analog of the strong law of large numbers. However, there is a notion of typicality of an infinite data sequence for a computable measure in the Martin-Löf theory of sequential tests [15] based on Kolmogorov complexity, and this is what we use.\nLet L ⊆ N be finite and µ be a measure on L∞ in a co-c.e. set of halting algorithms for computable measures. In this paper instead of the common notation µ(Γx) we use the simpler notation µ(x). We are given a sequence in L∞ which is typical (Definition 1) for µ. Thus, the constituent elements of the sequence are possibly dependent. The set of typical infinite sequences of a computable measure µ have µ-measure one, and each typical sequence passes all computable tests for µ-randomness in the sense of Martin-Löf. This probability model for L is more general than i.i.d. drawing according to a probability mass function. It includes stationary processes, ergodic processes, Markov processes of any order, and other models.\nTHEOREM 2: COMPUTABLE MEASURE IDENTIFICATION Let L be a finite set of natural numbers. We are given an infinite sequence of elements from L and this sequence is typical for one measure in a c.e. or co-c.e. set of halting algorithms for computable measures. There is an algorithm which identifies\na computable measure in the limit for which the sequence is typical. The code of this measure is an appropriate Turing machine and finite. The learning process uses only a finite initial segment of the data sequence. Let us explain the relation between Theorem 1 and Theorem 2. The set of infinite sequences of i.i.d. draws from a finite set L according to a probability mass function induces a measure on L∞. Such a measure is called an i.i.d. measure. The set of computable i.i.d. measures on L is a proper subset of the set of computable measures on L. An infinite sequence x1, x2, . . . drawn i.i.d. according to a computable probability mass function p on L is almost surely typical in the sense of Definition 1 for the induced computable i.i.d. measure µp, and every infinite sequence that is typical for µp is in the set of sequences almost surely drawn i.i.d. according to p. Hence Theorem 2 restricted to i.i.d. measures on finite sets implies Theorem 1 and vice versa.\nWe give an outline of the proof of Theorem 2. The proof itself is deferred to Appendix D. Lower semicomputable functions are defined in Appendix A. Let B be a list of a c.e. or co-c.e. set of halting algorithms for computable measures with each measure occurring infinitely many times. For a measure µ in the list B define σ(j) = log 1/µ(x1 . . . xj)−K(x1 . . . xj).\nBy (A.2), data sequence x1, x2, . . . is typical for µ iff supj σ(j) = σ with σ < ∞. By assumption there exists a measure in B for which the data sequence is typical. Let µh be such a measure Since algorithms for µh occurs infinitely often in the list B there is an algorithm µh′ in the list B with σh′ = σh and σh < h\n′. Therefore, there exists a measure µk in B for which the data sequence x1, x2, . . . is typical and σk < k with k least. If for every n := 1, 2, . . . we compute the least index i of µi in B such that µi(x1, . . . , xn) < i, then we identify in the limit a computable measure in B for which the provided data sequence is typical.\nREMARK 2: Let the underlying set L be finite. Define the primitive recursive measures as the set for which it is decidable that they are measures constructed from primitive recursive functions. Since this set is computable it is c.e.. The theorem shows that identification in the limit is possible for primitive recursive measures. Define the time-bounded measures for any fixed computable time bound as the set of elements for which it is decidable that they are measures satisfying this time bound. Since this set is computable it is c.e.. Again, the theorem shows that identification in the limit is possible for elements from this set.\nLet L be a finite set of cardinality l, and f1, f2, . . . be a c.e. of the primitive recursive functions. C.e. the strings x ∈ L∗ lexicographical length-increasing. Then every string can be viewed as the integer giving its position in this order. Define µi(ǫ) = fi(ǫ)/f(ǫ) = 1, and inductively for x ∈ L∗ and a ∈ L define µi(xa) = fi(xa)/ ∑ a∈L fi(xa). Then µi(x) = ∑\na∈L µi(xa) for all x ∈ L∗. Call the c.e. µ1, µ2, . . . the simple measures. The theorem shows that identification in the limit is possible for the set of simple measures. Following the proof of Theorem 2 in Appendix D we show another example in Example 3. ✸"
    }, {
      "heading" : "IV. PREDICTION",
      "text" : "In Section II the data are drawn i.i.d. according to a probability mass function p on the elements of L. Given p, we can predict the probability p(a|x1, . . . , xn) that the next draw results in an element a when the previous draws resulted in x1, . . . , xn. The resulting measure on L∞ is called an i.i.d. measure.\nFor general measures as in Section III, allowing dependent data, the situation is quite different. We can meet the so-called black swan phenomenon of [17]. Let us give a simple example. The data sequence is a, a, . . . is typical (Definition 1) for the measure µ1 defined by µ1(x) = 1 for every data sequence x consisting of a finite or infinite string of a’s and µ1(x) = 0 otherwise. But a, a, . . . is also typical for µ0 which gives probability µ0(x) = 12 for every string x either consisting of a finite or infinite string of a’s, or a fixed number n of a’s followed by a finite or infinite string of b’s, and 0 otherwise. Then, µ1 and µ0 can give different predictions given a sequence of a’s. But given a data sequence consisting initially of only a’s, a sensible algorithm will predict a as the most likely next symbol. However, if the initial data sequence consists of n symbols a, then for µ1 the next symbol will be a with probability 1, and for µ0 the next symbol is a with probability 12 and b with probability 1 2 . Therefore, while the i.i.d. case allows us to predict reliably, in the dependent case there is in general no reliable predictor for the next symbol. In [3] Blackwell and Dubin show that under certain conditions predictions of two measures merge asymptotically almost surely."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "Using an infinite sequence of elements from a set of natural numbers, algorithms are exhibited that identify in the limit the probability distribution associated with this set. This happens in two cases: (i) the target distribution is a probability mass function (i.i.d. measure) in a c.e. or co-c.e. set of computable probability mass functions (computable i.i.d. measures) and the elements of the sequence are drawn i.i.d. according to this probability (Theorem 1); (ii) the underlying set is finite and the infinite sequence is\npossibly dependent and typical for a computable measure in a c.e. or co-c.e. set of computable measures (Theorem 2).\nIn the i.i.d. case the target computable probability mass function is identified in the limit almost surely, in the dependent case the target computable measure is identified in the limit surely—it is one out of a set of satsfactory candidate computable measures. In the i.i.d. case we use the strong law of large numbers. For the dependent case we use typicality according to the theory developed by Martin-Löf in [15] embedded in theory of Kolmogorov complexity. The i.i.d. result is actually a corollary of the dependency result.\nIn both the i.i.d. setting and the dependent setting, eventually we guess an index of the target (or one target out of many possible targets in the measure case) and stick to this guess forever. This last guess is correct. However, we do not know when the guess becomes permanent. We use only a finite unknownlength initial segment of the data sequence. The target for which the guess is correct is described by a an appropriate Turing machine computing the probability mass function or measure, respectively.\nAPPENDIX"
    }, {
      "heading" : "A. Computability",
      "text" : "We can interpret a pair of integers such as (a, b) as rational a/b. A real function f with rational argument is lower semicomputable if it is defined by a rational-valued computable function φ(x, k) with x a rational number and k a nonnegative integer such that φ(x, k + 1) ≥ φ(x, k) for every k and limk→∞ φ(x, k) = f(x). This means that f can be computably approximated arbitrary close from below (see [14], p. 35). A function f is upper semicomputable if −f is semicomputable from below. If a real function is both lower semicomputable and upper semicomputable then it is computable. A function f is a semiprobability mass function if ∑\nx f(x) ≤ 1 and it is a probability mass function if ∑ x f(x) = 1.\nIt is customary to write p(x) for f(x) if the function involved is a semiprobability mass function.\nA set A ⊆ N is computable enumerable (c.e.) when we can compute a list a1, a2, . . . of which all elements are members of A. A c.e. set is also called recursively enumerable (r.e.). A co-c.e. set B ⊆ N is a set whose complement N \\ B is c.e.. If a set is both c.e. and co-c.e. then it is computable. The natural numbers above can be indexes.\nLet us explain the relation with identification in the limit. We explain this for the more complicated case of co-c.e. sets. The case for c.e. sets is similar. Consider a computable enumeration o1, o2, . . . of a set O of objects. A co-c.e. set S is a sublist C of o1, o2, . . . such that C = {oi : i ∈ S}. The members\nof C are the good objects and the members of O \\ C the bad objects. We computably enumerate the bad objects. We do not know in what order the bad objects are enumerated or repeated; however we do know that the remaining items are the good objects. These good objects with possible repetitions form a list A, a scattered sublist of the original computable enumeration of O. This list A is a co-c.e. set. It takes unknown time to enumerate each initial segment of A, but we are sure this happens eventually. Hence to identify the kth element in the list A while requiring the first 1, . . . , k − 1 elements requires identification in the limit.\nIt is known that the overwhelming majority of real numbers are not computable. If a real number a is lower semicomputable but not computable, then we can computably find nonnegative integers a1, a2, . . . and b1, b2, . . . such that an/bn ≤ an+1/bn+1 and limn→∞ an/bn = a. If a is the probability of success in a trial then this gives an example of a lower semicomputable probabity mass function which is not computable. Suppose we are concerned with all and only computable probability mass functions. There are countably many since there are only countably many computable functions. But can we computably enumerate them? The following lemma holds even if the functions are rational valued.\nLEMMA 1: (i) Let L ⊆ N and infinite. The computable probability mass functions on L are not c.e.. (ii) Let L ⊆ N , finite, and |L| ≥ 2. The computable measures on L are not c.e..\nProof: (i) Assume to the contrary that the lemma is false and the computable enumeration is\np1, p2, . . .. Compute a probability mass function p with p(a) 6= pi(ai) for ai ∈ L is the ith element of L As follows. If i is odd then p(ai) := fi(ai)+fi(ai)fi+1(ai+1) and p(ai+1) := fi+1(ai+1)−fi(ai)fi+1(ai+1), By construction p is a computable probability mass function but different from any pi in the enumeration p1, p2, . . ..\n(ii) Since L is finite the set L∗ is c.e.. Hence the set of cylinders in L∞ is c.e.. Therefore (ii) reduces\nto (i)."
    }, {
      "heading" : "B. Kolmogorov Complexity",
      "text" : "We need the theory of Kolmogorov complexity [14] (originally in [12] and the prefix version we use here in [13]). A prefix Turing machine is is a Turing machine with a one-way read-only input tape with an distinguished tape cell called the origin, a finite number of two-way read-write working tapes on which the computation takes place, an auxiliary tape on which the auxiliary string y ∈ {0, 1}∗ is written, and a one-way write-only output tape. At the start of the computation the input tape is infinitely inscribed from the origin onwards, and the input head is on the origin. The machine operates with a binary alphabet. If\nthe machine halts then the input head has scanned a segment of the input tape from the origin onwards. We call this initial segment the program.\nFor every auxiliary y ∈ {0, 1}∗, the set of programs is a prefix code: no program is a proper prefix of any other program. Consider a standard enumeration of all prefix Turing machines\nT1, T2, . . . .\nLet U denote a prefix Turing machine such that for every z, y ∈ {0, 1}∗ and i ≥ 1 we have U(i, z, y) = Ti(z, y). That is, for each finite binary program z, auxiliary y, and machine index i ≥ 1, we have that U ’s execution on inputs i and z, y results in the same output as that obtained by executing Ti on input z, y. We call such a U a universal prefix Turing machine.\nHowever, there are more ways a prefix Turing machine can simulate other prefix Turing machines. For example, let U ′ be such that U ′(i, zz, y) = Ti(z, y) for all i and z, y, and U ′(p) = 0 for p is not i, zz, y for some i and z, y. Then U ′ is universal also. To distinguish machines like U from other universal machines, Kolmogorov [12] called machines like U optimal.\nFix an optimal machine, say U . Define the conditional prefix Kolmogorov complexity K(x|y) for all x, y ∈ {0, 1}∗ by K(x|y) = minp{|p| : p ∈ {0, 1}∗ and U(p, y) = x}. For the same U , define the timebounded conditional prefix Kolmogorov complexity Kt(x|y) = minp{|p| : p ∈ {0, 1}∗ and U(p, y) = x in t steps}. To obtain the unconditional versions of the prefix Kolmogorov complexities set y = λ where λ is the empty word (the word with no letters). It can be shown that K(x|y) is incomputable [12]. Clearly Kt(x|y) is computable if t < ∞. Moreover, Kt′(x|y) ≤ Kt(x|y) for every t′ ≥ t, and limt→∞K t(x|y) = K(x|y)."
    }, {
      "heading" : "C. Measures, Semimeasures, and Computability",
      "text" : "Let L ⊆ N and finite. Given a finite sequence x = x1, x2, . . . , xn of elements of L, we consider the set of infinite sequences starting with x. The set of all such sequences is written as Γx, the cylinder of x. We associate a probability µ(Γx) with the event that an element of Γx occurs. Here we simplify the notation µ(Γx) and write µ(x). The transitive closure of the intersection, complement, and countable union of cylinders gives a set of subsets of L∞. The probabilities associated with these subsets are derived from\nthe probabilities of the cylinders in standard ways [10]. A semimeasure µ satisfies the following:\nµ(ǫ) ≤ 1 (A.1) µ(x) ≥ ∑\na∈L\nµ(xa),\nand if equality holds instead of each inequality we call µ a measure. Using the above notation, a semimeasure µ is lower semicomputable if it is defined by a rational-valued computable function φ(x, k) with x ∈ L∗ and k a nonnegative integer such that φ(x, k + 1) ≥ φ(x, k) for every k and limk→∞ φ(x, k) = µ(x). This means that µ can be computably approximated arbitrary close from below for each argument x ∈ L∗. Let x1, x2, . . . be an infinite sequence of elements of L. The sequence is typical for a computable measure µ if it passes all computable sequential tests (known and unknown alike) for randomness with respect to µ in the sense of Martin-Löf [15]. One of the highlights of the theory of Martin-Löf is that the sequence passes all these tests iff it passes a single universal test, [14] Corollary 4.5.2 on p 315, see also [15].\nDEFINITION 1: Let x1, x2, . . . be an infinite sequence of elements of L ⊆ N with L finite. The sequence is typical or random for a computable measure µ iff\nsup n {log 1 µ(x1 . . . xn) −K(x1 . . . xn)} < ∞. (A.2)\nThe set of infinite sequences that are typical with respect to a measure µ have µ-measure one. The theory and properties of such sequences for computable measures are extensively treated in [14] Chapter 4. There the term K(x1 . . . xn) in (A.2) is given as K(x1 . . . xn|µ). However, since µ is computable we have K(µ) < ∞ and therefore K(x1 . . . xn|µ) ≤ K(x1 . . . xn) +O(1).\nEXAMPLE 1: Let us elucidate by example the notion of typicality. Let µk be a measure defined by µk(x1 . . . xn) = 1/k for xi = a for every 1 ≤ i ≤ n and a fixed a ∈ {1, . . . , k}, and µk(x1 . . . xn) = 0 otherwise. Then K(a . . . a) (a sequence of n elements a) equals K(i, n) + O(1) = O(log n + log k). (A sequence of n elements a is described by n in O(log n) bits and a in O(log k) bits.) By (A.2) we have supn∈N {log 1/µk(a . . . a)−K(a . . . a)} < ∞. Therefore the infinite sequence aa . . . is typical for every µk. Similarly, the infinite sequence y1, y2, . . . is not typical for µk for yi ∈ {1, . . . , k} (i ≥ 1) and yi 6= yi+1 for some i. Namely, supn∈N {1/µk(y1y2 . . . yn)−K(y1y2 . . . yn)} = ∞. ♦ The example shows that an infinite sequence of data can be typical for more than one measure. Hence our\ntask is not to identify a single computable measure according to which the data sequence was generated as a typical sequence, but to identify a computable measure that could have generated the data sequence as a typical sequence."
    }, {
      "heading" : "D. Proofs of the Theorems",
      "text" : "Proof: OF THEOREM 1: I.I.D. COMPUTABLE PROBABILITY IDENTIFICATION. Let L ⊆ N , and X1,X2, . . . be a sequence of mutually independent random variables, each of which is a copy of a single random variable X with probability mass function P (X = a) = p(a) for a ∈ L. Without loss of generalty p(a) > 0 for all a ∈ L. Let #a(x1, x2, . . . , xn) denote the number of times xi = a (1 ≤ i ≤ n).\nCLAIM 1: If the outcomes of the random variables X1,X2, . . . are x1, x2, . . . , then almost surely for\nall a ∈ L we have lim n→∞ ( p(a)− #a(x1, x2, . . . , xn) n ) = 0. (A.3)\nProof: The strong law of large numbers (originally in [11]) states that if we perform the same\nexperiment a large number of times, then almost surely the number of successes divided by the number of trials goes to the expected value, provided the mean exists, see the theorem on top of page 260 in [7]. To determine the probability of an a ∈ L we consider the random variables Xa with just two outcomes {a, ā}. This Xa is a Bernoulli process (qa, 1− qa) where qa = p(a) is the probability of a and 1− qa = ∑ b∈L\\{a} p(b) is the probability of ā. If we set ā = min (L \\ {a}), then the mean µa of Xa is\nµa = aqa + ā(1− qa) ≤ max{a, ā} < ∞.\nThus, every a ∈ L incurs a random variable Xa with a finite mean. Therefore, (1/n) ∑n i=1(Xa)i converges almost surely to qa as n → ∞. The claim follows. Let A be a list of a c.e. or co-c.e. set of algorithms for the computable probability mass functions. If q ∈ A and q = p then for every ǫ > 0 and a ∈ L holds p(a)− q(a) < ǫ. By Claim 1, almost surely\nlim n→∞ max a∈L\n(\nqi(a)− #a(x1, x2, . . . , xn)\nn\n)\n= 0. (A.4)\nIf q ∈ A and q 6= p then there is an a ∈ L and a constant δ > 0 such that |p(a)− q(a)| > δ. Again by Claim 1, almost surely\nlim n→∞ max a∈L\n∣ ∣ ∣ ∣ qi(a)− #a(x1, x2, . . . , xn)\nn\n∣ ∣ ∣ ∣ > δ. (A.5)\nIn the proof of the strong law of large numbers it is shown that if we draw x1, x2, . . . i.i.d. from a set L ⊆ N according to a probability mass function p then almost surely the size of the fluctuations in going to the limit (A.4) satisfies |np(a)−#a(x1, x2, . . . , xn)|/ √ np(a)p(ā) < √ 2λ lg n for every λ > 1 and n is large enough for all a ∈ L, see [7] p. 204. Here lg denotes the natural logarithm. Since p(a)p(ā) ≤ 14 and λ = √ 2 suffices we obtain |p(a)−#a(x1, x2, . . . , xn)/n| < √ (lg n)/n for all but finitely many n.\nLet q ∈ A. For q 6= p there is an a ∈ L such that by (A.5) and the fluctuations in going to that limit we have |q(a)−#a(x1, x2, . . . , xn)/n| > δ− √ (lg n)/n for all but finitely many n. Since δ > 0 is constant, we have 2 √ (lg n)/n < δ for all but finitely many n. Hence |q(a)−#a(x1, x2, . . . , xn)/n| > √ (lg n)/n for all but finitely many n.\nLet A = q1, q2, . . . and p = qk with k least. We give the algorithm with as output a sequence of indexes i1, i2, . . . such that all but finitely many indexes are k. If L is infinite then the algorithm can only use a finite subset of it. Hence we need to define this finite subset and show that the remaining elements can be ignored. Let An = {a ∈ L : #a(x1, x2, . . . , xn) > 0}. In case a 6= An then |q(a) − #a(x1, x2, . . . , xn)/n| = qi(a). We disregard qi(a) < √\n(lg n)/n as follows. Let L = {a1, a2, . . .}. For each qi define the set Bi,n = {a1, . . . , am} with m least such that ∑∞ j=m+1 qi(aj) = 1− ∑m j=1 qi(aj) < √\n1/n. Therefore, if a ∈ L \\ Bi,n then qi(a) < √ 1/n. The sets An and Bi,n are finite for all n and\ni. Set Li,n = An ⋃ Bi,n. Then for every a ∈ L we have |qk(a) −#a(x1, x2, . . . , xn)/n| ≤ √ (lg n)/n for all but finitely many n. For i 6= k there is an a ∈ Lk,n but no a ∈ L \\ Lk,n such that |qi(a) − #a(x1, x2, . . . , xn)/n| > √ (lg n)/n for all but finitely many n. This leads to the following algorithm:\nfor n := 1, 2, . . .\nI := ∅; for i := 1, 2, . . . , n\nif maxa∈Li,n |qi(a)−#a(x1, x2, . . . , xn)/n| < √ (lg n)/n then I := I ⋃{i};\nin := min I\nWith probability 1 for every i < k for all but finitely many n we have i 6∈ I while k ∈ I for all but finitely many n. (Note that for every n = 1, 2, . . . the main term in the above algorithm is computable even if L is infinite.) The theorem is proven.\nEXAMPLE 2: We give an example of a list A of a co-c.e. set halting algorithms for computable probability mass functions. This set is large but does not contain all probability mass functions. A semiprobability mass function is a function for which the values sum to at most 1.\nFirst we obtain a computable co-enumeration of computable total functions which is not c.e.. Let f : N → N be a computable time-bound such as the Ackermann function, a total computable function growing faster than any primitive recursive function, and φ1, φ2, . . . a standard computable enumeration of all partial computable functions. Computably enumerate all φi such that φi(j) does not halt within f(j) steps for all i, j ≥ 1. Eliminate all those from φ1, φ2, . . . . The result is a subsequence of the original computable enumeration, a computable co-enumeration of total computable functions ψ1, ψ1, . . . which are time bounded by f .\nCLAIM 2: Given a computable co-enumeration of computable total functions, one can exhibit a computable total function φ(i, x, n) = qni (x) such that φ(i, x, n) ≤ φ(i, x, n+1) and limn→∞ qni (x) = qi(x) iff qi is a lower semicomputable semiprobability mass function.\nProof: Let ψ1, ψ1, . . . be as above. Computably change every ψ into an algorithm lower semicom-\nputing a semiprobability mass function q, see the proof of Theorem 4.3.1 in [14] (originally in [21], [13]). For every a ∈ L denote the nth approximation of q(a) in the lower semicomputation of q(a) by qn(a). Therefore we can compute\nQ = q1, q2, . . . , (A.6)\na list containing only algorithms which lower semicompute semiprobability mass total functions. Without loss of generality the function lower semicomputed by every algorithm in Q is over the alphabet L. Let L = {a1, a2, . . .}. The semiprobability mass functions q in list Q such that there is an n for which ∑n\ni=1 q n(an) < 1 − 1/n can be computably enumerated. The remaining elements in list Q are\nprobability mass functions and they are computably co-enumerated. The intersection of a two co-c.e. sets is co-c.e.. We show that the remaining lower semicomputable probability mass functions are computable. A probability mass function q in list Q can be computed as follows: for every ǫ > 0 let nǫ be least such that\n∑n j=1 q n(aj) ≥ 1 − ǫ for all n ≥ nǫ. Thus every probability mass function in list Q is computable and we have an algorithm to compute it. ♦\nProof: OF THEOREM 2 COMPUTABLE MEASURE IDENTIFICATION For the Kolmogorov complexity\nnotions see Appendix B. For the theory of semicomputable semimeasures, see Appendix C. In particular we use the criterion of Definition 1 to show that an infinite sequence is typical in Martin-Löf’s sense. The given data sequence x1, x2, . . . is, by assumption, typical for some computable measure µ and hence satisfies (A.2) with respect to µ. We stress that the data sequence is possibly µ-typical and µ′-typical for different computable measures µ and µ′. Therefore we cannot speak of the single true computable\nmeasure, but only of a computable measure for which the data is typical.\nLet B be a list of halting algorithms for a c.e. or co-c.e. set of computable measures such that each element occurs infinitely many times in the list.\nCLAIM 3: There is an algorithm with as input a list B = µ1, µ2, . . . and as output a sequence of indexes i1, i2, . . .. For every large enough n we have in = k with µk a computable measure for which the data sequence is typical.\nProof: Define for µ in B\nσ(j) = log 1/µ(x1 . . . xj)−K(x1 . . . xj).\nSince K is upper semicomputable and µ is computable, the function σ(j) is lower semicomputable for each j. Define the nth value in the lower semicomputation of σ(j) as σn(j). By (A.2), the data sequence x1, x2, . . . is typical for µ if supj≥1 σ(j) = σ < ∞ In this case, since µ is lower semicomputable, max1≤j≤n σ(n) ≤ σ for all n. In contrast, the data sequence is not typical for µ if σ(n) → ∞ with n → ∞ implying σn(n) → ∞ with n → ∞.\nBy assumption there exists a measure in B for which the data sequence is typical. Let µh be such a measure Since algorithms for µh occur infinitely often in the list B there is an algorithm µh′ in the list B with σh′ = σh and σh < h′. Therefore, there exists a measure µk in B for which the data sequence x1, x2, . . . is typical and σk < k with k least. The algorithm to determine k is as follows.\nfor n := 1, 2, . . .\nif i ≤ n is least such that max1≤j≤n σni (j) < i then output in = i else output in = 1.\nEventually max1≤j≤n σnk (j) < k for large enough n, and k is the least index of elements in B for which this holds. Hence there exists an n0 such that in = k for all n ≥ n0.\nFor large enough n we have by Claim 3 a test such that we can identify in the limit an index of a measure in B for which the provided data sequence is typical. Hence there is an n0 such that in = k for all n ≥ n0. We do not care what i1, . . . , in−1 are. This proves the theorem.\nEXAMPLE 3: We give an example of a list B of halting algorithms for a co-c.e. set of computable measures.\nCLAIM 4: Given a co-enumeration of computable total functions, one can exhibit a computable total\nfunction φ(i, x, n) = µni (x) such that φ(i, x, n) ≤ φ(i, x, n + 1) and limn→∞ µni (x) = µi(x).\nProof: To eliminate functions with undefined values, let ψ1, ψ1, . . . be a co-enumeration of total\nfunctions in a standard computable enumeration of all partial computable functions as in Example 2. Computably change every ψ into an algorithm lower semicomputing a semimeasure µ, similar to the method in the proof of Theorem 4.5.1 of [14] pp. 295–296 (originally in [21]). For every x ∈ L∗ denote the nth approximation of µ(x) in the lower semicomputation of µ(x) by µn(x). Therefore we can compute\nM = µ1, µ2, . . . , (A.7)\na list containing only algorithms which lower semicompute semimeasures. Without loss of generality the function lower semicomputed by every algorithm in M is over the alphabet L. Every function in the list will be in the list infinitely often, which follows simply from the fact that there are infinitely many algorithms which lower semicompute a given function. It is important to realize that, although the code of a computable measure may be in list M, it is there as an algorithm lower semicomputing the measure. By Claim 4 we can co-enumerate halting algorithms that lower semicompute semimeasures (A.7). Let µn(x) denote the nth lower semicomputation of µ(x) for a semimeasure µ. The semimeasures µ in list M such that there are x ∈ L∗ and n < ∞ such that either µn(ǫ) < 1 − 1/n or µn(x) − ∑a∈L µn(xa) < 1/n can be computably enumerated. The remaining elements in list M are wide set of computable measures (but not all) and they are co-c.e.. A lower semicomputable algorithm for a measure can be converted to a computable algorithm. To see this, let L = a1, a2, . . . , an. Let µ be a lower semicomputable semimeasure with ∑\na∈L µ(xa) = µ(x) for all x ∈ L∗ and µ(ǫ) = 1. Then, we can approximate all µ(x) to any degree of precision starting with µ(a1), µ(a2), . . . and determining µ(x) for all x of length n, for consecutive n = 1, 2, . . . . ♦"
    }, {
      "heading" : "ACKNOWLEDGEMENT",
      "text" : "We thank Laurent Bienvenu for pointing out an error in the an earlier version and elucidating comments. Drafts of this paper proceeded since 2012 in various states of correctness through arXiv:1208.5003 to arXiv:1311.7385."
    } ],
    "references" : [ {
      "title" : "Identifying languages from stochastic examples",
      "author" : [ "D. Angluin" ],
      "venue" : "Yale University, Dept. of Computer Science, Technical report, New Haven, Conn., USA",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Minimum complexity density estimation",
      "author" : [ "A.R. Barron", "T.M. Cover" ],
      "venue" : "IEEE Trans. Inform. Th., 4:37",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Merging of opinions with increasing information",
      "author" : [ "D Blackwell", "L Dubins" ],
      "venue" : "The Annals of Mathematical Statistics, 33:3",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "Limiting recursion",
      "author" : [ "E.M. Gold" ],
      "venue" : "J. Symb. Logic, 30",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Language identification in the limit",
      "author" : [ "E.M. Gold" ],
      "venue" : "Inform. Contr., 10",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "An Introduction to Probability",
      "author" : [ "W. Feller" ],
      "venue" : "Theory and Its Applications,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1968
    }, {
      "title" : "The probabilistic analysis of language acquisition: Theoretical",
      "author" : [ "A. Hsu", "N. Chater", "P.M.B. Vitányi" ],
      "venue" : "computational, and experimental analysis, Cognition,120",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Systems that Learn",
      "author" : [ "S. Jain", "D.N. Osherson", "J.S. Royer", "A. Sharma" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Grundbegriffe der Wahrscheinlichkeitsrechnung",
      "author" : [ "A.N. Kolmogorov" ],
      "venue" : "Springer-Verlag, Berlin",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1933
    }, {
      "title" : "Sur la loi forte des grandes nombres",
      "author" : [ "A.N. Kolmogorov" ],
      "venue" : "C. r. Acad. Sci. Paris, 191(1930), 910–912. See also A.N. Kolmogorov, Grundbegriffe der Wahrscheinlichkeitsrechnung, Springer-Verlag, Berlin, 1933. See also F.P. Cantelli, Sulla probabilitá come limite della frequenza, Rendiconti della R. Academia dei Lincei, Classe di scienze fisische matematiche e naturale, Serie 5a, 26",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1917
    }, {
      "title" : "Three approaches to the quantitative definition of information",
      "author" : [ "A.N. Kolmogorov" ],
      "venue" : "Problems Inform. Transmission, 1:1",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Laws of information conservation (non-growth) and aspects of the foundation of probability theory",
      "author" : [ "L.A. Levin" ],
      "venue" : "Problems Inform. Transmission, 10",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "An Introduction to Kolmogorov Complexity and Its Applications, Springer-Verlag",
      "author" : [ "M. Li", "P.M.B. Vitányi" ],
      "venue" : "New York,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "The definition of random sequences",
      "author" : [ "P. Martin-Löf" ],
      "venue" : "Inform. Control, 9:6",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "Formal models of language learning",
      "author" : [ "S. Pinker" ],
      "venue" : "Cognition 7",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "The Logic of Scientific Discovery",
      "author" : [ "K.R. Popper" ],
      "venue" : "Hutchinson, London",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "A formal theory of inductive inference",
      "author" : [ "R.J. Solomonoff" ],
      "venue" : "part 1 and part 2, Inform. Contr., 7:1–22, 224–254",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "On computable numbers",
      "author" : [ "A.M. Turing" ],
      "venue" : "with an application to the Entscheidungsproblem, Proc. London Mathematical Society 2, 42(1936), 230–265, ”Correction”, 43",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1937
    }, {
      "title" : "Learning recursive functions: a survey",
      "author" : [ "T. Zeugmann", "S. Zilles" ],
      "venue" : "Theoret. Comput. Sci., 397",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The learnability of a language under various computational assumptions is the subject of an immensely influential approach in [5] and especially [6], or the review [9].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "The learnability of a language under various computational assumptions is the subject of an immensely influential approach in [5] and especially [6], or the review [9].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : "The learnability of a language under various computational assumptions is the subject of an immensely influential approach in [5] and especially [6], or the review [9].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "We shall consider identification in the limit (following, for example, [6], [9], [16]).",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "We shall consider identification in the limit (following, for example, [6], [9], [16]).",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "We shall consider identification in the limit (following, for example, [6], [9], [16]).",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "In the realm of algorithmic information theory, in particular in Solomonoff induction [18] and here, we reason as follows.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "The possible strategies of learners are computable in the sense of Turing [19], that is, they are computable functions.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "This corresponds to the notion of “identification in the limit” in [6], [9], [16], [20].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "This corresponds to the notion of “identification in the limit” in [6], [9], [16], [20].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "This corresponds to the notion of “identification in the limit” in [6], [9], [16], [20].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "This corresponds to the notion of “identification in the limit” in [6], [9], [16], [20].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "In [1] (citing previous more restricted work) a target probability mass function was identified in the limit when the data are drawn i.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "In [2] computability questions are apparently ignored.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : "If L(q) is identified with the Kolmogorov complexity K(q), as in Section IV of this reference, then it is incomputable as already observed by Kolmogorov in [12] (for the plain Kolmogorov complexity; the",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "The results hold (contrary to what is claimed in the Conclusion of [2] and other parts of the text) not for the set of computable probability mass functions since they are not c.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "The sentence “you know but you don’t know you know” on the second page of [2] does not hold",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "This underpins partially the result announced in [8].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "The technical tool is the Martin-Löf theory of sequential tests [15] based on Kolmogorov complexity.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "In order that it is computable we only require that the probability mass function is finitely describable and there is a computable process producing it [19].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "stronger than the result in [1] referred to in Section I-B.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "It is more theoretical but strictly stronger than [2] that does not give identification in the limit for classes of computable functions.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "However, there is a notion of typicality of an infinite data sequence for a computable measure in the Martin-Löf theory of sequential tests [15] based on Kolmogorov complexity, and this is what we use.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "We can meet the so-called black swan phenomenon of [17].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "In [3] Blackwell and Dubin show that under certain conditions predictions of two measures merge asymptotically almost surely.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "For the dependent case we use typicality according to the theory developed by Martin-Löf in [15] embedded in theory of Kolmogorov complexity.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "This means that f can be computably approximated arbitrary close from below (see [14], p.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "We need the theory of Kolmogorov complexity [14] (originally in [12] and the prefix version we use here in [13]).",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "We need the theory of Kolmogorov complexity [14] (originally in [12] and the prefix version we use here in [13]).",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "We need the theory of Kolmogorov complexity [14] (originally in [12] and the prefix version we use here in [13]).",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "To distinguish machines like U from other universal machines, Kolmogorov [12] called machines like U optimal.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "It can be shown that K(x|y) is incomputable [12].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "the probabilities of the cylinders in standard ways [10].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "The sequence is typical for a computable measure μ if it passes all computable sequential tests (known and unknown alike) for randomness with respect to μ in the sense of Martin-Löf [15].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 12,
      "context" : "One of the highlights of the theory of Martin-Löf is that the sequence passes all these tests iff it passes a single universal test, [14] Corollary 4.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "2 on p 315, see also [15].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "The theory and properties of such sequences for computable measures are extensively treated in [14] Chapter 4.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "Proof: The strong law of large numbers (originally in [11]) states that if we perform the same experiment a large number of times, then almost surely the number of successes divided by the number of trials goes to the expected value, provided the mean exists, see the theorem on top of page 260 in [7].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Proof: The strong law of large numbers (originally in [11]) states that if we perform the same experiment a large number of times, then almost surely the number of successes divided by the number of trials goes to the expected value, provided the mean exists, see the theorem on top of page 260 in [7].",
      "startOffset" : 298,
      "endOffset" : 301
    }, {
      "referenceID" : 5,
      "context" : "np(a)p(ā) < √ 2λ lg n for every λ > 1 and n is large enough for all a ∈ L, see [7] p.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "1 in [14] (originally in [21], [13]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "1 in [14] (originally in [21], [13]).",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : "1 of [14] pp.",
      "startOffset" : 5,
      "endOffset" : 9
    } ],
    "year" : 2014,
    "abstractText" : "The problem is to identify a probability associated with a set of natural numbers, given an infinite data sequence of elements from the set. If the given sequence is drawn i.i.d. and the probability mass function involved (the target) belongs to a computably enumerable (c.e.) or co-computably enumerable (co-c.e.) set of computable probability mass functions, then there is an algorithm to almost surely identify the target in the limit. The technical tool is the strong law of large numbers. If the set is finite and the elements of the sequence are dependent while the sequence is typical in the sense of Martin-Löf for at least one measure belonging to a c.e. or co-c.e. set of computable measures, then there is an algorithm to identify in the limit a computable measure for which the sequence is typical (there may be more than one such measure). The technical tool is the theory of Kolmogorov complexity. We give the algorithms and consider the associated predictions.",
    "creator" : "LaTeX with hyperref package"
  }
}