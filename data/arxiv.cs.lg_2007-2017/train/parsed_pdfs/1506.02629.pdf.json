{
  "name" : "1506.02629.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalization in Adaptive Data Analysis and Holdout Reuse",
    "authors" : [ "Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment.\nWe also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach given in [DFH+14] is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. This, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches."
    }, {
      "heading" : "1 Introduction",
      "text" : "The goal of machine learning is to produce hypotheses or models that generalize well to the unseen instances of the problem. More generally, statistical data analysis is concerned with estimating properties of the underlying data distribution, rather than properties that are specific to the finite data set at hand. Indeed, a large body of theoretical and empirical research was developed for ensuring generalization in a variety of settings. In this work, it is commonly assumed that each analysis procedure (such as a learning algorithm) operates on a freshly sampled dataset – or if not, is validated on a freshly sampled holdout (or testing) set.\nUnfortunately, learning and inference can be more difficult in practice, where data samples are often reused. For example, a common practice is to perform feature selection on a dataset, and then use the features for some supervised learning task. When these two steps are performed on the same dataset, it is no\n∗Microsoft Research †IBM Almaden Research Center. Part of this work done while visiting the Simons Institute, UC Berkeley ‡Google Research §University of Toronto ¶Samsung Research America ‖Department of Computer and Information Science, University of Pennsylvania\nar X\niv :1\n50 6.\n02 62\n9v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\nlonger clear that the results obtained from the combined algorithm will generalize. Although not usually understood in these terms, “Freedman’s paradox” is an elegant demonstration of the powerful (negative) effect of adaptive analysis on the same data [Fre83]. In Freedman’s simulation, variables with significant t-statistic are selected and linear regression is performed on this adaptively chosen subset of variables, with famously misleading results: when the relationship between the dependent and explanatory variables is non-existent, the procedure overfits, erroneously declaring significant relationships.\nMost of machine learning practice does not rely on formal guarantees of generalization for learning algorithms. Instead a dataset is split randomly into two (or sometimes more) parts: the training set and the testing, or holdout, set. The training set is used for learning a predictor, and then the holdout set is used to estimate the accuracy of the predictor on the true distribution1. Because the predictor is independent of the holdout dataset, such an estimate is a valid estimate of the true prediction accuracy (formally, this allows one to construct a confidence interval for the prediction accuracy on the data distribution). However, in practice the holdout dataset is rarely used only once, and as a result the predictor may not be independent of the holdout set, resulting in overfitting to the holdout set [Reu03, RF08, CT10]. One well-known reason for such dependence is that the holdout data is used to test a large number of predictors and only the best one is reported. If the set of all tested hypotheses is known and independent of the holdout set, then it is easy to account for such multiple testing or use the more sophisticated approach of Ng [Ng97].\nHowever such static approaches do not apply if the estimates or hypotheses tested on the holdout are chosen adaptively: that is, if the choice of hypotheses depends on previous analyses performed on the dataset. One prominent example in which a holdout set is often adaptively reused is hyperparameter tuning (e.g.[DFN07]). Similarly, the holdout set in a machine learning competition, such as the famous ImageNet competition, is typically reused many times adaptively. Other examples include using the holdout set for feature selection, generation of base learners (in aggregation techniques such as boosting and bagging), checking a stopping condition, and analyst-in-the-loop decisions. See [Lan05] for a discussion of several subtle causes of overfitting.\nThe concrete practical problem we address is how to ensure that the holdout set can be reused to perform validation in the adaptive setting. Towards addressing this problem we also ask the more general question of how one can ensure that the final output of adaptive data analysis generalizes to the underlying data distribution. This line of research was recently initiated by the authors in [DFH+14], where we focused on the case of estimating expectations of functions from i.i.d. samples (these are also referred to as statistical queries). They show how to answer a large number of adaptively chosen statistical queries using techniques from differential privacy [DMNS06](see Sec. 1.3 and Sec. 2.2 for more details)."
    }, {
      "heading" : "1.1 Our Results",
      "text" : "We propose a simple and general formulation of the problem of preserving statistical validity in adaptive data analysis. We show that the connection between differentially private algorithms and generalization from [DFH+14] can be extended to this more general setting, and show that similar (but sometimes incomparable) guarantees can be obtained from algorithms whose outputs can be described by short strings. We then define a new notion, approximate max-information, that unifies these two basic techniques and gives a new perspective on the problem. In particular, we give an adaptive composition theorem for max-information, which gives a simple way to obtain generalization guarantees for analyses in which some of the procedures are differentially private and some have short description length outputs. We apply our techniques to the problem of reusing the holdout set for validation in the adaptive setting."
    }, {
      "heading" : "1.1.1 A Reusable Holdout",
      "text" : "We describe a simple and general method, together with two specific instantiations, for reusing a holdout set for validating results while provably avoiding overfitting to the holdout set. The analyst can perform any analysis on the training dataset, but can only access the holdout set via an algorithm that allows the\n1Additional averaging over different partitions is used in cross-validation.\nanalyst to validate her hypotheses against the holdout set. Crucially, our algorithm prevents overfitting to the holdout set even when the analysts hypotheses are chosen adaptively on the basis of the previous responses of our algorithm.\nOur first algorithm, referred to as Thresholdout, derives its guarantees from differential privacy and the results in [DFH+14, NS15]. For any function φ : X → [0, 1] given by the analyst, Thresholdout uses the holdout set to validate that φ does not overfit to the training set, that is, it checks that the mean value of φ evaluated on the training set is close to the mean value of φ evaluated on the distribution P from which the data was sampled. The standard approach to such validation would be to compute the mean value of φ on the holdout set. The use of the holdout set in Thresholdout differs from the standard use in that it exposes very little information about the mean of φ on the holdout set: if φ does not overfit to the training set, then the analyst receives only the confirmation of closeness, that is, just a single bit. On the other hand, if φ overfits then Thresholdout returns the mean value of φ on the training set perturbed by carefully calibrated noise.\nUsing results from [DFH+14, NS15] we show that for datasets consisting of i.i.d. samples these modifications provably prevent the analyst from constructing functions that overfit to the holdout set. This ensures correctness of Thresholdout’s responses. Naturally, the specific guarantees depend on the number of samples n in the holdout set. The number of queries that Thresholdout can answer is exponential in n as long as the number of times that the analyst overfits is at most quadratic in n.\nOur second algorithm SparseValidate is based on the idea that if most of the time the analysts procedures generate results that do not overfit, then validating them against the holdout set does not reveal much information about the holdout set. Specifically, the generalization guarantees of this method follow from the observation that the transcript of the interaction between a data analyst and the holdout set can be described concisely. More formally, this method allows the analyst to pick any Boolean function of a dataset ψ (described by an algorithm) and receive back its value on the holdout set. A simple example of such a function would be whether the accuracy of a predictor on the holdout set is at least a certain value α. (Unlike in the case of Thresholdout, here there is no need to assume that the function that measures the accuracy has a bounded range or even Lipschitz, making it qualitatively different from the kinds of results achievable subject to differential privacy). A more involved example of validation would be to run an algorithm on the holdout dataset to select an hypothesis and check if the hypothesis is similar to that obtained on the training set (for any desired notion of similarity). Such validation can be applied to other results of analysis; for example one could check if the variables selected on the holdout set have large overlap with those selected on the training set. An instantiation of the SparseValidate algorithm has already been applied to the problem of answering statistical (and more general) queries in the adaptive setting [BSSU15]. We describe the formal guarantees for SparseValidate in Section 4.2.\nIn Section 5 we describe a simple experiment on synthetic data that illustrates the danger of reusing a standard holdout set, and how this issue can be resolved by our reusable holdout. The design of this experiment is inspired by Freedman’s classical experiment, which demonstrated the dangers of performing variable selection and regression on the same data [Fre83]."
    }, {
      "heading" : "1.2 Generalization in Adaptive Data Analysis",
      "text" : "We view adaptive analysis on the same dataset as an execution of a sequence of steps A1 → A2 → · · · → Am. Each step is described by an algorithm Ai that takes as input a fixed dataset S = (x1, . . . , xn) drawn from some distribution D over Xn, which remains unchanged over the course of the analysis. Each algorithm Ai also takes as input the outputs of the previously run algorithms A1 through Ai−1 and produces a value in some range Yi. The dependence on previous outputs represents all the adaptive choices that are made at step i of data analysis. For example, depending on the previous outputs, Ai can run different types of analysis on S. We note that at this level of generality, the algorithms can represent the choices of the data analyst, and need not be explicitly specified. We assume that the analyst uses algorithms which individually are known to generalize when executed on a fresh dataset sampled independently from a distribution D. We formalize this by assuming that for every fixed value y1, . . . , yi−1 ∈ Y1 × · · · × Yi−1, with probability at least 1− βi over the choice of S according to distribution D, the output of Ai on inputs y1, . . . , yi−1 and S has a desired property relative to the data distribution D (for example has low generalization error). Note that in this\nassumption y1, . . . , yi−1 are fixed and independent of the choice of S, whereas the analyst will execute Ai on values Y1, . . . ,Yi−1, where Yj = Aj(S,Y1, . . . ,Yj−1). In other words, in the adaptive setup, the algorithm Ai can depend on the previous outputs, which depend on S, and thus the set S given to Ai is no longer an independently sampled dataset. Such dependence invalidates the generalization guarantees of individual procedures, potentially leading to overfitting.\nDifferential privacy: First, we spell out how the differential privacy based approach from [DFH+14] can be applied to this more general setting. Specifically, a simple corollary of results in [DFH+14] is that for a dataset consisting of i.i.d. samples any output of a differentially-private algorithm can be used in subsequent analysis while controlling the risk of overfitting, even beyond the setting of statistical queries studied in [DFH+14]. A key property of differential privacy in this context is that it composes adaptively: namely if each of the algorithms used by the analyst is differentially private, then the whole procedure will be differentially private (albeit with worse privacy parameters). Therefore, one way to avoid overfitting in the adaptive setting is to use algorithms that satisfy (sufficiently strong) guarantees of differential-privacy. In Section 2.2 we describe this result formally.\nDescription length: We then show how description length bounds can be applied in the context of guaranteeing generalization in the presence of adaptivity. If the total length of the outputs of algorithms A1, . . . ,Ai−1 can be described with k bits then there are at most 2k possible values of the input y1, . . . , yi−1 to Ai. For each of these individual inputs Ai generalizes with probability 1− βi. Taking a union bound over failure probabilities implies generalization with probability at least 1− 2kβi. Occam’s Razor famously implies that shorter hypotheses have lower generalization error. Our observation is that shorter hypotheses (and the results of analysis more generally) are also better in the adaptive setting since they reveal less about the dataset and lead to better generalization of subsequent analyses. Note that this result makes no assumptions about the data distribution D. We provide the formal details in Section 2.3.\nApproximate max-information: Our main technical contribution is the introduction and analysis of a new information-theoretic measure, which unifies the generalization arguments that come from both differential privacy and description length, and that quantifies how much information has been learned about the data by the analyst. Formally, for jointly distributed random variables (S,Y ), the max-information is the maximum of the logarithm of the factor by which uncertainty about S is reduced given the value of Y , namely I∞(S,Y ) . = log max P[S=S | Y =y]P[S=S] , where the maximum is taken over all S in the support of S and y in the support Y . Informally, β-approximate max-information requires that the logarithm above be bounded with probability at least 1− β over the choice of (S,Y ) (the actual definition is slightly weaker, see Definition 10 for details).In our use, S denotes a dataset drawn randomly from the distribution D and Y denotes the output of a (possibly randomized) algorithm on S. We prove that approximate max-information has the following properties\n• An upper bound on (approximate) max-information gives generalization guarantees.\n• Differentially private algorithms have low max-information for any distribution D over datasets. A stronger bound holds for approximate max-information on i.i.d. datasets. These bounds apply only to so-called pure differential privacy (the δ = 0 case).\n• Bounds on the description length of the output of an algorithm give bounds on the approximate max-information of the algorithm for any D.\n• Approximate max-information composes adaptively.\nWe remark that (pure) differential privacy and description length are otherwise incomparable – low description length is not a sufficient condition for differential privacy, since differential privacy precludes revealing even a small number of bits of information about any single individual in the data set. At the same time differential privacy does not constrain the description length of the output.\nComposition properties of approximate max-information imply that one can easily obtain generalization guarantees for adaptive sequences of algorithms, some of which are differentially private, and others of which have outputs with short description length. These properties also imply that differential privacy can be used to control generalization for any distribution D over datasets, which extends its generalization guarantees beyond the restriction to datasets drawn i.i.d. from a fixed distribution, as in [DFH+14]. Details of these results and additional discussion appear in Section 3 and supplemental material."
    }, {
      "heading" : "1.3 Related Work",
      "text" : "This work builds on [DFH+14] where we initiated the formal study of adaptivity in data analysis. The primary focus of [DFH+14] is the problem of answering adaptively chosen statistical queries. The main technique is a strong connection between differential privacy and generalization: differential privacy guarantees that the distribution of outputs does not depend too much on any one of the data samples, and thus, differential privacy gives a strong stability guarantee that behaves well under adaptive data analysis. The link between generalization and approximate differential privacy made in [DFH+14] has been subsequently strengthened, both qualitatively — by [BSSU15], who make the connection for a broader range of queries — and quantitatively, by [NS15] and [BSSU15], who give tighter quantitative bounds. These papers, among other results, give methods for accurately answering exponentially (in the dataset size) many adaptively chosen queries, but the algorithms for this task are not efficient. It turns out this is for fundamental reasons – Hardt and Ullman [HU14] and Steinke and Ullman [SU14] prove that, under cryptographic assumptions, no efficient algorithm can answer more than quadratically many adaptively chosen statistical queries in the worst case.\nDifferential privacy emerged from a line of work [DN03, DN04, BDMN05], culminating in the definition given by [DMNS06]. There is a very large body of work designing differentially private algorithms for various data analysis tasks, some of which we leverage in our applications. See [Dwo11] for a short survey and [DR14] for a textbook introduction to differential privacy.\nThe classical approach in theoretical machine learning to ensure that empirical estimates generalize to the underlying distribution is based on the various notions of complexity of the set of functions output by the algorithm, most notably the VC dimension(see e.g. [SSBD14] for a textbook introduction). If one has a sample of data large enough to guarantee generalization for all functions in some class of bounded complexity, then it does not matter whether the data analyst chooses functions in this class adaptively or non-adaptively. Our goal, in contrast, is to prove generalization bounds without making any assumptions about the class from which the analyst can choose query functions. In this case the adaptive setting is very different from the non-adaptive setting.\nAn important line of work [BE02, MNPR06, PRMN04, SSSSS10] establishes connections between the stability of a learning algorithm and its ability to generalize. Stability is a measure of how much the output of a learning algorithm is perturbed by changes to its input. It is known that certain stability notions are necessary and sufficient for generalization. Unfortunately, the stability notions considered in these prior works do not compose in the sense that running multiple stable algorithms sequentially and adaptively may result in a procedure that is not stable. The measure we introduce in this work (max information), like differential privacy, has the strength that it enjoys adaptive composition guarantees. This makes it amenable to reasoning about the generalization properties of adaptively applied sequences of algorithms, while having to analyze only the individual components of these algorithms. Connections between stability, empirical risk minimization and differential privacy in the context of learnability have been recently explored in [WLF15].\nFinally, inspired by our work, Blum and Hardt [BH15] showed how to reuse the holdout set to maintain an accurate leaderboard in a machine learning competition that allows the participants to submit adaptively chosen models in the process of the competition (such as those organized by Kaggle Inc.). Their analysis also provides an additional example of an application of the description-based technique we used to analyze SparseValidate."
    }, {
      "heading" : "2 Preliminaries and Basic Techniques",
      "text" : "In the discussion below log refers to binary logarithm and ln refers to the natural logarithm. For simplicity we restrict our random variables to finite domains (extension of the claims to continuous domains is straightforward using the standard formalism). For two random variables X and Y over the same domain X the max-divergence of X from Y is defined as\nD∞(X‖Y ) = log max x∈X P[X = x] P[Y = x] .\nδ-approximate max-divergence is defined as\nDδ∞(X‖Y ) = log maxO⊆X , P[X∈O]>δ P[X ∈ O]− δ P[Y ∈ O] .\nWe say that a real-valued function over datasets f : Xn → R has sensitivity c for all i ∈ [n] and x1, x2, . . . , xn, x ′ i ∈ X , f(x1, . . . , xi, . . . , xn)−f(x1, . . . , x′i, . . . , xn) ≤ c. We review McDiarmid’s concentration inequality for functions of low-sensitivity.\nLemma 1 (McDiarmid’s inequality). Let X1, X2, . . . ,Xn be independent random variables taking values in the set X . Further let f : Xn → R be a function of sensitivity c > 0. Then for all α > 0, and µ = E [f(X1, . . . ,Xn)],\nP [ f( ¯ X1, . . . ,Xn)− µ ≥ α ] ≤ exp ( −2α2 n · c2 ) .\nFor a function φ : X → R and a dataset S = (x1, . . . , xn), let ES [φ] . = 1n ∑n i=1 φ(xi). Note that if the range of φ is in some interval of length α then f(S) = ES [φ] has sensitivity α/n. For a distribution P over X and a function φ : X → R, let P[φ] .= Ex∼P [φ(x)]."
    }, {
      "heading" : "2.1 Differential Privacy",
      "text" : "On an intuitive level, differential privacy hides the data of any single individual. We are thus interested in pairs of datasets S, S′ that differ in a single element, in which case we say S and S′ are adjacent.\nDefinition 2. [DMNS06, DKM+06] A randomized algorithm A with domain Xn for n > 0 is (ε, δ)differentially private if for all pairs of datasets that differ in a single element S, S′ ∈ Xn: Dδ∞(A(S)‖A(S′)) ≤ log(eε). The case when δ = 0 is sometimes referred to as pure differential privacy, and in this case we may say simply that A is ε-differentially private.\nDifferential privacy is preserved under adaptive composition. Adaptive composition of algorithms is a sequential execution of algorithms on the same dataset in which an algorithm at step i can depend on the outputs of previous algorithms. More formally, let A1,A2, . . . ,Am be a sequence of algorithms. Each algorithm Ai outputs a value in some range Yi and takes as an input dataset in Xn as well as a value in Ȳi−1 . = Y1 × · · · × Yi−1. Adaptive composition of these algorithm is the algorithm that takes as an input a dataset S ∈ Xn and executes A1 → A2 → · · · → Am sequentially with the input to Ai being S and the outputs y1, . . . , yi−1 of A1, . . . ,Ai−1. Such composition captures the common practice in data analysis of using the outcomes of previous analyses (that is y1, . . . , yi−1) to select an algorithm that is executed on S.\nFor an algorithm that in addition to a dataset has other input we say that it is (ε, δ)-differentially private if it is (ε, δ)-differentially private for every setting of additional parameters. The basic property of adaptive composition of differentially private algorithms is the following result (e.g.[DL09]):\nTheorem 3. Let Ai : Xn × Y1 × · · · × Yi−1 → Yi be an (εi, δi)-differentially private algorithm for i ∈ [m]. Then the algorithm B : Xn → Ym obtained by composing Ai’s adaptively is ( ∑m i=1 εi, ∑m i=1 δi)-differentially private.\nA more sophisticated argument yields significant improvement when ε < 1 (e.g.[DR14])\nTheorem 4. For all ε, δ, δ′ ≥ 0, the adaptive composition of m arbitrary (ε, δ)-differentially private algorithms is (ε′,mδ + δ′)-differentially private, where\nε′ = √ 2m ln(1/δ′)ε+mε(eε − 1).\nAnother property of differential privacy important for our applications is preservation of its guarantee under post-processing (e.g.[DR14, Prop. 2.1]):\nLemma 5. If A is an ( , δ)-differentially private algorithm with domain Xn and range Y, and B is any, possibly randomized, algorithm with domain Y and range Y ′′, then the algorithm B ◦ A with domain Xn and range Y ′ is also ( , δ)-differentially private."
    }, {
      "heading" : "2.2 Generalization via Differential Privacy",
      "text" : "Generalization in special cases of our general adaptive analysis setting can be obtained directly from results in [DFH+14] and composition properties of differentially private algorithms. For the case of pure differentially private algorithms with general outputs over i.i.d. datasets, in [DFH+14] we prove the following result.\nTheorem 6. Let A be an ε-differentially private algorithm with range Y and let S be a random variable drawn from a distribution Pn over Xn. Let Y = A(S) be the corresponding output distribution. Assume that for each element y ∈ Y there is a subset R(y) ⊆ Xn so that maxy∈Y P[S ∈ R(y)] ≤ β. Then, for ε ≤ √ ln(1/β)\n2n we have P[S ∈ R(Y )] ≤ 3 √ β.\nAn immediate corollary of Thm. 6 together with Lemma 1 is that differentially-private algorithms that output low-sensitivity functions generalize.\nCorollary 7. Let A be an algorithm that outputs a c-sensitive function f : Xn → R. Let S be a random dataset chosen according to distribution Pn over Xn and let f = A(S). If A is τ/(cn)-differentially private then P[f(S)− Pn[f ] > τ ] ≤ 3 exp (−τ2/(c2n)).\nBy Theorem 3, pure differential privacy composes adaptively. Therefore, if in a sequence of algorithms A1,A2, . . . ,Am algorithm Ai is εi-differentially private for all i ≤ m− 1 then composition of the first i− 1 algorithms is ε′i−1-differentially private for ε ′ i−1 = (∑i−1 j=1 εj ) . Theorem 6 can be applied to preserve the generalization guarantees of the last algorithm Am (that does not need to be differentially private). For example, assume that for every fixed setting of ȳm−1, Am has the property that it outputs a hypothesis function h such that, P[ES [L(h)]− P [L(h)] ≥ τ ] ≤ e−nτ\n2/d, for some notion of dimension d and a real-valued loss function L. Generalization bounds of this type follow from uniform convergence arguments based on various notions of complexity of hypotheses classes such as VC dimension, covering numbers, fat-shattering dimension and Rademacher complexity (see [SSBD14] for examples). Note that, for different settings of ȳm−1, different sets of hypotheses and generalization techniques might be used. We define R(ȳm−1) be all datasets S for which Am(S, ȳm−1) outputs h such that ES [L(h)]− P [L(h)] ≥ τ . Now if ε′m−1 ≤ √ τ2/(2d), then even for the hypothesis output in the adaptive execution of Am on a random i.i.d. dataset S (denoted by h) we have P [ES [L(h)]− P[L(h)] > τ ] ≤ 3e−τ\n2n/(2d). For approximate (ε, δ)-differential privacy, strong preservation of generalization results are currently known only for algorithms that output a function over X of bounded range (for simplicity we use range [0, 1]) [DFH+14, NS15]. The following result was proved by Nissim and Stemmer [NS15] (a weaker statement is also given in [DFH+14, Thm. 10]).\nTheorem 8. Let A be an (ε, δ)-differentially private algorithm that outputs a function from X to [0, 1]. For a random variable S distributed according to Pn we let φ = A(S). Then for n ≥ 2 ln(8/δ)/ε2,\nP [|P[φ]− ES [φ]| > 13ε] ≤ 2δ\nε ln\n( 2\nε\n) .\nMany learning algorithms output a hypothesis function that aims to minimize some bounded loss function L as the final output. If algorithms used in all steps of the adaptive data analysis are differentially private and the last step (that is, Am) outputs a hypothesis h, then generalization bounds for the loss of h are implied directly by Theorem 8. We remark that this application is different from the example for pure differential privacy above since there we showed preservation of generalization guarantees of arbitrarily complex learning algorithm Am which need not be differentially private. In Section 4 we give an application of Theorem 8 to the reusable holdout problem."
    }, {
      "heading" : "2.3 Generalization via Description Length",
      "text" : "Let A : Xn → Y and B : Xn × Y → Y ′ be two algorithms. We now give a simple application of bounds on the size of Y (or, equivalently, the description length of A’s output) to preserving generalization of B. Here generalization can actually refer to any valid or desirable output of B for a given given dataset S and input y ∈ Y . Specifically we will use a set R(y) ⊆ Xn to denote all datasets for which the output of B on y and S is “bad” (e.g. overfits). Using a simple union bound we show that the probability (over a random choice of a dataset) of such bad outcome can be bounded.\nTheorem 9. Let A : Xn → Y be an algorithm and let S be a random dataset over Xn. Assume that R : Y → 2Xn is such that for every y ∈ Y, P[S ∈ R(y)] ≤ β. Then P[S ∈ R(A(S))] ≤ |Y| · β.\nProof.\nP[S ∈ R(A(S))] ≤ ∑ y∈Y P[S ∈ R(y)] ≤ |Y| · β.\nThe case of two algorithms implies the general case since description length composes (adaptively). Namely, let A1,A2, . . . ,Am be a sequence of algorithms such that each algorithm Ai outputs a value in some range Yi and takes as an input dataset in Xn as well as a value in Ȳi−1. Then for every i, we can view the execution of A1 through Ai−1 as the first algorithm Āi−1 with an output in Ȳi−1 and Ai as the second algorithm. Theorem 9 implies that if for every setting of ȳi−1 = y1, . . . , yi−1 ∈ Ȳi−1, R(ȳi−1) ⊆ Xn satisfies that P[S ∈ R(ȳi−1)] ≤ βi then\nP[S ∈ R(Āi−1(S))] ≤ |Ȳi−1| · βi = i−1∏ j=1 |Yj | · βi."
    }, {
      "heading" : "3 Max-Information",
      "text" : "Consider two algorithms A : Xn → Y and B : Xn×Y → Y ′ that are composed adaptively and assume that for every fixed input y ∈ Y , B generalizes for all but fraction β of datasets. Here we are speaking of generalization informally: our definitions will support any property of input y ∈ Y and dataset S. Intuitively, to preserve generalization of B we want to make sure that the output of A does not reveal too much information about the dataset S. We demonstrate that this intuition can be captured via a notion of max-information and its relaxation approximate max-information.\nFor two random variables X and Y we use X × Y to denote the random variable obtained by drawing X and Y independently from their probability distributions.\nDefinition 10. Let X and Y be jointly distributed random variables. The max-information between X and Y , denoted I∞(X;Y ), is the minimal value of k such that for every x in the support of X and y in the support of Y we have P[X = x | Y = y] ≤ 2k P[X = x]. Alternatively,I∞(X;Y ) = D∞((X,Y )‖X × Y ). The β-approximate max-information is defined as Iβ∞(X;Y ) = D β ∞((X,Y )‖X × Y ).\nIt follows immediately from Bayes’ rule that for all β ≥ 0, Iβ∞(X;Y ) = Iβ∞(Y ;X). Further, I∞(X;Y ) ≤ k if and only if for all x in the support of X, D∞(Y | X = x ‖ Y ) ≤ k. In our use (X,Y ) is going to be a joint distribution (S,A(S)), where S is a random n-element dataset and A is a (possibly randomized) algorithm taking a dataset as an input. If the output of an algorithm on any distribution S has low approximate max-information then we say that the algorithm has low max-information. More formally:\nDefinition 11. We say that an algorithm A has β-approximate max-information of k if for every distribution S over n-element datasets, Iβ∞(S;A(S)) ≤ k, where S is a dataset chosen randomly according to S. We denote this by Iβ∞(A, n) ≤ k.\nGeneralization via Max-information: An immediate corollary of our definition of approximate maxinformation is that it controls the probability of “bad events” that can happen as a result of the dependence of A(S) on S.\nTheorem 12. Let S be a random dataset in Xn and A be an algorithm with range Y such that for some β ≥ 0, k = Iβ∞(S;A(S)) = k. Then for any event O ⊆ Xn × Y,\nP[(S,A(S)) ∈ O] ≤ 2k · P[S ×A(S) ∈ O] + β.\nIn particular, P[(S,A(S)) ∈ O] ≤ 2k ·maxy∈Y P[(S, y) ∈ O] + β.\nWe remark that the classical notion of mutual information between S and A(S) would not suffice for ensuring that bad events happen with tiny probability. For example mutual information of k allows P[(S,A(S)) ∈ O] to be as high as k/(2 log(1/δ)), where δ = P[S ×A(S) ∈ O].\nComposition of Max-information: Approximate max-information satisfies the following adaptive composition property:\nLemma 13. Let A : Xn → Y be an algorithm such that Iβ1∞ (A, n) ≤ k1, and let B : Xn × Y → Z be an algorithm such that for every y ∈ Y, B(·, y) has β2-approximate max-information k2. Let C : Xn → Z be defined such that C(S) = B(S,A(S)). Then Iβ1+β2∞ (C, n) ≤ k1 + k2.\nProof. Let D be a distribution over Xn and S be a random dataset sampled from D. By hypothesis, Iβ1∞ (S;A(S)) ≤ k1. Expanding out the definition for all O ⊆ Xn × Y:\nP[(S,A(S)) ∈ O] ≤ 2k1 · P[S ×A(S) ∈ O] + β1 .\nWe also have for all Q ⊆ Xn ×Z and for all y ∈ Y:\nP[(S,B(S, y)) ∈ Q] ≤ 2k2 · P[S × B(S, y) ∈ Q] + β2 .\nFor every O ⊆ Xn × Y, define\nµ(O) = ( P[(S,A(S)) ∈ O]− 2k1 · P[S ×A(S) ∈ O] ) + .\nObserve that µ(O) ≤ β1 for all O ⊆ Xn × Y. For any event Q ⊆ Xn ×Z, we have:\nP[(S, C(S)) ∈ Q] = P[(S,B(S,A(S))) ∈ Q] =\n∑ S∈Xn,y∈Y P[(S,B(S, y)) ∈ Q] · P[S = S,A(S) = y]\n≤ ∑\nS∈Xn,y∈Y min\n(( 2k2 · P[S × B(S, y) ∈ Q] + β2 ) , 1 ) · P[S = S,A(S) = y]\n≤ ∑\nS∈Xn,y∈Y\n( min ( 2k2 · P[S × B(S, y) ∈ Q], 1 ) + β2 ) · P[S = S,A(S) = y]\n≤ ∑\nS∈Xn,y∈Y min\n( 2k2 · P[S × B(S, y) ∈ Q], 1 ) · P[S = S,A(S) = y] + β2\n≤ ∑\nS∈Xn,y∈Y min\n( 2k2 · P[S × B(S, y) ∈ Q], 1 ) · ( 2k1 · P[S = S] · P[A(S) = y] + µ(S, y) ) + β2\n≤ ∑\nS∈Xn,y∈Y min\n( 2k2 · P[S × B(S, y) ∈ Q], 1 ) · 2k1 · P[S = S] · P[A(S) = y] + ∑ S∈Xn,y∈Y µ(S, y) + β2\n≤ ∑\nS∈Xn,y∈Y min\n( 2k2 · P[S × B(S, y) ∈ Q], 1 ) · 2k1 · P[S = S] · P[A(S) = y] + β1 + β2\n≤ 2k1+k2 ·  ∑ S∈Xn,y∈Y P[S × B(S, y) ∈ Q] · P[S = S] · P[A(S) = y] + β1 + β2 = 2k1+k2 · P[S × B(S,A(S)) ∈ Q] + (β1 + β2) .\nApplying the definition of max-information, we see that equivalently, Iβ1+β2∞ (S; C(S)) ≤ k1 + k2, which is what we wanted.\nThis lemma can be iteratively applied, which immediately yields the following adaptive composition theorem for max-information:\nTheorem 14. Consider an arbitrary sequence of algorithms A1, . . . ,Ak with ranges Y1, . . . ,Yk such that for all i, Ai : Xn × Y1 × . . .× Yi−1 → Yi is such that Ai(·, y1, . . . , yi−1) has βi-approximate max-information ki for all choices of y1, . . . , yi−1 ∈ Y1 × . . .× Yi−1. Let the algorithm B : Xn → Yk be defined as follows: B(S):\n1. Let y1 = A1(S).\n2. For i = 2 to k: Let yi = Ai(S, y1, . . . , yi−1)\n3. Output yk Then B has ( ∑ i βi)-approximate max-information ( ∑ i ki)."
    }, {
      "heading" : "3.1 Bounds on Max-information",
      "text" : "We now show that the basic approaches based on description length and (pure) differential privacy are captured by approximate max-information."
    }, {
      "heading" : "3.1.1 Description Length",
      "text" : "Description length k gives the following bound on max-information.\nTheorem 15. Let A be a randomized algorithm taking as an input an n-element dataset and outputting a value in a finite set Y. Then for every β > 0, Iβ∞(A, n) ≤ log(|Y|/β).\nWe will use the following simple property of approximate divergence (e.g.[DR14]) in the proof. For a random variable X over X we denote by p(X) the probability distribution associated with X.\nLemma 16. Let X and Y be two random variables over the same domain X . If\nP x∼p(X) [ P[X = x] P[Y = x] ≥ 2k ] ≤ β\nthen Dβ∞(X‖Y ) ≤ k.\nProof Thm. 15. Let S be any random variable over n-element input datasets and let Y be the corresponding output distribution Y = A(S). We prove that for every β > 0, Iβ∞(S;Y ) ≤ log(|Y|/β).\nFor y ∈ Y we say that y is “bad” if exists S in the support of S such that\nP[Y = y | S = S] P[Y = y] ≥ |Y|/β.\nLet B denote the set of all “bad” y’s. From this definition we obtain that for a “bad” y, P[Y = y] ≤ β/|Y| and therefore P[Y ∈ B] ≤ β. Let B = Xn ×B. Then\nP[(S,Y ) ∈ B] = P[Y ∈ B] ≤ β.\nFor every (S, y) 6∈ B we have that\nP[S = S,Y = y] = P[Y = y | S = S] · P[S = S] ≤ |Y| β · P[Y = y] · P[S = S],\nand hence\nP (S,y)∼p(S,Y )\n[ P[S = S,Y = y]\nP[S = S] · P[Y = y] ≥ |Y| β\n] ≤ β.\nThis, by Lemma 16, gives that Iβ∞(S;Y ) ≤ log(|Y|/β).\nWe note that Thms. 12 and 15 give a slightly weaker form of Thm. 9. Defining eventO .= {(S, y) | S ∈ R(y)}, the assumptions of Thm. 9 imply that P[S ×A(S) ∈ O] ≤ β. For β′ = √ |Y|β, by Thm. 15, we have that Iβ ′ ∞(S;A(S)) ≤ log(|Y|/β′). Now applying, Thm. 12 gives that P[S ∈ R(A(S))] ≤ |Y|/β′ · β + β′ = 2 √ |Y|β."
    }, {
      "heading" : "3.1.2 Differential Privacy",
      "text" : "We now show that pure differential privacy implies a bound on max information. We start with a simple bound on max-information of differentially private algorithms that applies to all distributions over datasets. This bound applies to all distributions over datasets and implies that differential privacy based approach can be used beyond the i.i.d. setting in [DFH+14].\nTheorem 17. Let A be an -differentially private algorithm. Then I∞(A, n) ≤ log e · n.\nProof. Let S be any random variable over n-element input datasets for A and let Y be the corresponding output distribution Y = A(S). We will argue that I∞(Y ;S) ≤ log e · n, that I∞(S;Y ) ≤ log e · n follows immediately from the Bayes’ rule. Clearly, any two datasets S and S′ differ in at most n elements. Therefore, for every y we have P[Y = y | S = S] ≤ e n P[Y = y | S = S′] (this is a direct implication of Definition 2 referred to as group privacy [DR14]). Since there must exist a dataset S such that P[Y = y | S = S] ≤ P[Y = y] we can conclude that for every S and every y it holds that P[Y = y | S = S] ≤ e n P[Y = y]. This yields I∞(Y ;S) ≤ log e · n and concludes the proof.\nFinally, we prove a stronger bound on approximate max-information for datasets consisting of i.i.d. samples using the technique from [DFH+14]. This bound, together with Thm. 12, generalizes Thm. 6.\nTheorem 18. Let A be an ε-differentially private algorithm with range Y. For a distribution P over X , let S be a random variable drawn from Pn. Let Y = A(S) denote the random variable output by A on input S. Then for any β > 0, Iβ∞(S;A(S)) ≤ log e(ε2n/2 + ε √ n ln(2/β)/2).\nProof. Fix y ∈ Y. We first observe that by Jensen’s inequality,\nE S∼Pn\n[ln(P[Y = y | S = S])] ≤ ln (\nE S∼Pn\n[P[Y = y | S = S]] ) = ln(P[Y = y]).\nFurther, by definition of differential privacy, for two databases S, S′ that differ in a single element,\nP[Y = y | S = S] ≤ eε · P[Y = y | S = S′].\nNow consider the function g(S) = ln (\nP[Y =y | S=S] P[Y =y]\n) . By the properties above we have that E[g(S)] ≤\nln(P[Y = y])− ln(P[Y = y]) = 0 and |g(S)− g(S′)| ≤ ε. This, by McDiarmid’s inequality (Lemma 1), implies that for any t > 0,\nP[g(S) ≥ t] ≤ e−2t 2/(nε2). (1)\nFor an integer i ≥ 1, let ti . = ε2n/2 + ε √ n ln(2i/β)/2 and let\nBi . = {S | ti < g(S) ≤ ti+1 } .\nLet By . = {S |g(S) > t1 } = ⋃ i≥1 Bi.\nBy inequality (1), we have that for i ≥ 1, P[g(S) > ti] ≤ exp ( −2 ( ε √ n/2 + √ ln(2i/β)/2 )2) .\nBy Bayes’ rule, for every S ∈ Bi,\nP[S = S | Y = y] P[S = S] = P[Y = y | S = S] P[Y = y] = exp(g(S)) ≤ exp(ti+1).\nTherefore, P[S ∈ Bi | Y = y] = ∑ S∈Bi P[S = S | Y = y]\n≤ exp(ti+i) · ∑ S∈Bi P[S = S] ≤ exp(ti+i) · P[g(S) ≥ ti]\n= exp ( ε2n/2 + ε √ n ln(2i+1/β)/2− 2 ( ε √ n/2 + √ ln(2i/β)/2 )2) ≤ exp ( ε √ n/2 (√ ln(2i+1/β)− 2 √ ln(2i/β) ) − ln(2i/β)\n) < exp(− ln(2i/β)) = β/2i.\nAn immediate implication of this is that P[S ∈ By | Y = y] = ∑ i P[S ∈ Bi | Y = y] ≤ ∑ i≥1 β/2i ≤ β.\nLet B = {(S, y) | y ∈ Y, S ∈ By}. Then\nP[(S,Y ) ∈ B] = P[(S,Y ) ∈ BY ] ≤ β. (2)\nFor every (S, y) 6∈ B we have that\nP[S = S,Y = y] = P[S = S | Y = y] · P[Y = y] ≤ exp(t1) · P[S = S] · P[Y = y],\nand hence by eq.(2) we get that\nP (S,y)∼p(S,Y )\n[ P[S = S,Y = y]\nP[S = S] · P[Y = y] ≥ exp(t1)\n] ≤ β.\nThis, by Lemma 16, gives that Iβ∞(S;Y ) ≤ log(exp(t1)) = log e(ε2n/2 + ε √ n ln(2/β)/2).\nApplications: We give two simple examples of using the bounds on max-information obtained from differential privacy to preserve bounds on generalization error that follow from concentration of measure inequalities. Strong concentration of measure results are at the core of most generalization guarantees in machine learning. Let A be an algorithm that outputs a function f : Xn → R of sensitivity c and define the “bad event” Oτ is when the empirical estimate of f is more than τ away from the expectation of f(S) for S distributed according to some distribution D over Xn. Namely,\nOτ = {(S, f) : f(S)−D[f ] > τ} , (3)\nwhere D[f ] denotes ES∼D[f(S)]. By McDiarmid’s inequality (Lem. 1) we know that, if S is distributed according to Pn then supf :Xn→R P[(S, f) ∈ Oτ ] ≤ exp(−2τ2/(c2n)). The simpler bound in Thm. 17 implies following corollary.\nCorollary 19. Let A be an algorithm that outputs a c-sensitive function f : Xn → R. Let S be a random dataset chosen according to distribution Pn over Xn and let f = A(S). If for β ≥ 0 and τ > 0, Iβ∞(S;f) ≤ log e · τ2/c2, then P[f(S)− Pn[f ] > τ ] ≤ exp (−τ2/(c2n)) + β. In particular, if A is τ2/(c2n2)differentially private then P[f(S)− Pn[f ] > τ ] ≤ exp (−τ2/(c2n)).\nNote that for f(S) = ES [φ], where φ : X → [0, 1] this result requires ε = τ2. The stronger bound allows to preserve concentration of measure even when ε = τ/(cn) which corresponds to τ = ε when f(S) = ES [φ].\nCorollary 20. Let A be an algorithm that outputs a c-sensitive function f : Xn → R. Let S be a random dataset chosen according to distribution Pn over Xn and let f = A(S). If A is τ/(cn)-differentially private then P[f(S)− Pn[f ] > τ ] ≤ exp (−3τ2/(4c2n)).\nProof. We apply Theorem 18 with β = 2 exp (−τ2/(c2n)) to obtain that\nIβ∞(S,f) ≤ log e · (ε2n/2 + ε √ n ln(2/β)/2)) ≤ log e · (τ2/(c2n)/2 + τ2/(c2n)/ √ 2).\nApplying Thm. 12 to McDiarmid’s inequality we obtain that\nP[f(S)− Pn[f ] > τ ] ≤ exp ((1/2 + 1/ √\n2)τ2/(c2n)) · exp (−2τ2/(c2n)) + 2 exp (−τ2/(c2n)) ≤ exp (−3τ2/(4c2n)),\nwhere the last inequality holds when τ2/(c2n) is larger than a fixed constant."
    }, {
      "heading" : "4 Reusable Holdout",
      "text" : "We describe two simple algorithms that enable validation of analyst’s queries in the adaptive setting.\n4.1 Thresholdout\nOur first algorithm Thresholdout follows the approach in [DFH+14] where differentially private algorithms are used to answer adaptively chosen statistical queries. This approach can also be applied to any low-sensitivity functions 2 of the dataset but for simplicity we present the results for statistical queries. Here we address an easier problem in which the analyst’s queries only need to be answered when they overfit. Also, unlike in [DFH+14], the analyst has full access to the training set and the holdout algorithm only prevents overfitting to holdout dataset. As a result, unlike in the general query answering setting, our algorithm can efficiently validate an exponential in n number of queries as long as a relatively small number of them overfit.\nThresholdout is given access to the training dataset St and holdout dataset Sh and a budget limit B. It allows any query of the form φ : X → [0, 1] and its goal is to provide an estimate of P[φ]. To achieve this the algorithm gives an estimate of ESh [φ] in a way that prevents overfitting of functions generated by the analyst to the holdout set. In other words, responses of Thresholdout are designed to ensure that, with high probability, ESh [φ] is close to P[φ] and hence an estimate of ESh [φ] gives an estimate of the true expectation ESh [φ]. Given a function φ, Thresholdout first checks if the difference between the average value of φ on the training set St (or ESt [φ]) and the average value of φ on the holdout set Sh (or ESh [φ]) is below a certain threshold T + η. Here, T is a fixed number such as 0.01 and η is a Laplace noise variable whose standard deviation needs to be chosen depending on the desired guarantees (The Laplace distribution is a symmetric exponential distribution.) If the difference is below the threshold, then the algorithm returns ESt [φ]. If the difference is above the threshold, then the algorithm returns ESh [φ] + ξ for another Laplacian noise variable ξ. Each time the difference is above threshold the “overfitting” budget B is reduced by one. Once it is exhausted, Thresholdout stops answering queries. In Fig. 1 we provide the pseudocode of Thresholdout.\nWe now establish the formal generalization guarantees that Thresholdout enjoys. As the first step we state what privacy parameters are achieved by Thresholdout.\nLemma 21. Thresholdout satisfies (B/τn, 0)-differential privacy. Thresholdout also satisfies ( √\n8B ln(2/δ)/τn, δ)differential privacy for any δ > 0.\nProof. Thresholdout is an instantiation of a basic tool from differential privacy, the “Sparse Vector Algorithm” ([DR14, Algorithm 2]), together with the Laplace mechanism ([DR14, Defn. 3.3]). The sparse vector algorithm takes as input a sequence of c sensitivity 1/n queries3 (here c = B, the budget), and for each query, attempts\n2Guarantees based on pure differential privacy follow from the same analysis. Proving generalization guarantees for low-sensitivity queries based on approximate differential privacy requires a modification of Thresholdout using techniques in [BSSU15].\n3In fact, the theorems for the Sparse Vector algorithm in Dwork and Roth are stated for sensitivity 1 queries – we use them for sensitivity 1/n queries of the form ESh [φ], which results in all of the bounds being scaled down by a factor of n.\nto determine whether the value of the query, evaluated on the private dataset, is above a fixed threshold T or below it. In our instantiation, the holdout set Sh is the private data set, and each function φ corresponds to the following query evaluated on Sh: fφ(Sh) := |ESh [φ] − ESt [φ]|. (Note that the training set St is viewed as part of the definition of the query). Thresholdout then is equivalent to the following procedure: we run the sparse vector algorithm [DR14, Algorithm 2] with c = B, queries fφ for each function φ, and noise rate σ = 4 · τ . Whenever an above-threshold query is reported by the sparse vector algorithm, we release its value using the Laplace mechanism [DR14, Defn. 3.3] with noise rate σ = 2 · τ (this is what occurs every time Thresholdout answers by outputting ESh [φ] + ξ). By the privacy guarantee of the sparse vector algorithm ([DR14, Thm. 3.25]), the sparse vector portion of Thresholdout satisfies (B/(2τn), 0)-differential privacy, and\nsimultaneously satisfies (\n√ 2B ln(2/δ)\nτn , δ/2)-differential privacy. The Laplace mechanism portion of Thresholdout\nsatisfies (β/(2τn), 0)-differential privacy by [DR14, Thm. 3.6], and simultaneously satisfies (\n√ 2B ln(2/δ)\nτn , δ/2)differential privacy by [DR14, Thm. 3.6] and [DR14, Cor. 3.21]. Finally, the composition of two mechanisms, the first of which is ( 1, δ1)-differentially private, and the second of which is ( 2, δ2)-differentially private is itself ( 1 + 2, δ1 + δ2)-differentially private (Thm. 3). Adding the privacy parameters of the Sparse Vector portion of Thresholdout and the Laplace mechanism portion of Thresholdout yield the parameters of our theorem.\nWe note that tighter privacy parameters are possible (e.g. by invoking the parameters and guarantees of the algorithm “NumericSparse” ([DR14, Algorithm 3]), which already combines the Laplace addition step) – we chose simpler parameters for clarity.\nNote the seeming discrepancy between the guarantee provided by Thresholdout and generalization guarantees in Theorem 8 and Corollary 7: while Theorem 8 promises generalization bounds for functions that are generated by a differentially private algorithm, here we allow an arbitrary data analyst to generate query functions in any way she chooses, with access to the training set and differentially private estimates of the means of her functions on the holdout set. The connection comes from preservation of differential privacy guarantee under post-processing (Lem. 5).\nConsider the first guarantee of Lemma 21. In order to achieve generalization error τ via Corollary 7 (i.e. in order to guarantee that for every function φ we have: P [|P[φ]− ESh [φ]| > τ ] ≤ 6e−τ 2n) we need to set the budget so that we achieve (ε, 0)-differential privacy for ε ≤ τ . To this end we use the following budget function:\nbudget0(n, τ) = τ 2n. (4)\nWe can also make use of the second guarantee together with the results of Nissim and Stemmer [NS15] (Thm. 8). In order to achieve generalization error τ with probability 1 − β (i.e. in order to guarantee for every function φ we have: P [|P[φ]− ESh [φ]| > τ ] ≤ β), we can apply Thm. 8 by setting = τ/13 and δ = βτ26 ln (26/τ) . We can obtain these privacy parameters from Lemma 21 by setting the budget as follows:\nbudget1(n, τ, β) = Cτ4n2\nln(1/(τβ)) , (5)\nfor a fixed constant C ≥ 1/1400. We remark that a somewhat worse bound of budget1(n, τ, β) = τ 5n2\n512 ln(8/β)\nfollows by setting = τ/4 and δ = (β/8)4/τ in [DFH+14, Thm. 10]. Both settings lead to small generalization error and so we can pick whichever gives the larger bound. The first bound has grows only linearly with n but is simpler can be easily extended to other distributions over datasets and to low-sensitivity functions. The second bound features a quadratic dependence on n at the expense of a slightly worse dependence on τ .\nWe can now apply our main results to get a generalization bound for Thresholdout. (This is very similar to the accuracy guarantee for the sparse vector algorithm [DR14, Thm. 3.26], but the version we derive here states the probability of error per query rather than the probability of error in the worst case over all m queries):\nTheorem 22. Let T ≥ 0, τ > 0. Let Sh and St be data sets drawn i.i.d. from a distribution P. Consider an algorithm that is given access to St and adaptively chooses functions φ1, . . . , φm while interacting with Thresholdout which is given data sets Sh, St tolerance τ, and threshold T , and returns an answer ai on function φi : X → [0, 1]. If we set B = budget0(|Sh|, τ), then, for all i ∈ {1, . . . ,m} such that ai 6= ⊥, we have for all t > 0,\nP {|ai − P[φi]| > T + (t+ 1)τ} ≤ 6e−τ 2n + e−t/8 .\nIf we set B = budget1(|Sh|, τ, β) then for all i ∈ {1, . . . ,m} such that ai 6= ⊥, we have for all t > 0,\nP {|ai − P[φi]| > T + (t+ 1)τ} ≤ β + e−t/8 .\nProof. There are two cases, depending on whether Thresholdout answers query φ by returning ai = ESh [φ] + ξ (step (a)) or by returning ai = ESt [φ] (step (b)). First, consider queries whose answers are returned using step (a):\nThere are two sources of error we need to control: the deviation between ai and the average value of φi on the holdout set ESh [φi], and the deviation between the average value of φi on the holdout set and the expectation of φi on the underlying distribution, P[φi].\nThe first term, |ai − ESh [φi]| = ξ by definition of the algorithm, where ξ ∼ Lap(2 · τ). By properties of the Laplace distribution, we know that P[|ξ| ≥ t · τ ] = e−t/2. For the second term, we know that when we set B = budget0(|Sh|, τ), by Corollary 7 we have:\nP [|P[φi]− ESh [φi]| > τ ] ≤ 6e−τ 2n\nCombining these two bounds, we get:\nP[|ai − P[φi]| ≥ (t+ 1)τ ] ≤ P [|ai − ESh [φi]| ≥ tτ ] + P [|P[φi]− ESh [φi]| > τ ]\n≤ 6e−τ 2n + e−t/2\nSimilarly, if we set B = budget1(|Sh|, τ, β), we can bound the second term using Theorem 8:\nP [|P[φ]− ESh [φ]| > τ ] ≤ β\nAgain combining the two bounds, in this case we get:\nP[|ai − P[φi]| ≥ (t+ 1)τ ] ≤ P [|ai − ESh [φi]| ≥ tτ ] + P [|P[φi]− ESh [φi]| > τ ]\n≤ β + e−t/2\nNext, we consider the second case, those queries whose answers are returned using step (b): in this case, ai = ESt [φ]. This case is similar: there are two sources of error we need to bound. By definition of the algorithm, we have |ai − ESh [φi]| ≤ T + η, where η ∼ Lap(8 · τ). Hence, by properties of the Laplace distribution, we have:\nP[|ai − ESh [φi]| ≥ T + t · τ ] ≤ e−t/8.\nWe continue to have that: P [|P[φi]− ESh [φi]| > τ ] ≤ 6e−τ 2n\nwhen we set B = budget0(|Sh|, τ) and so in this case, by combining the two bounds, we obtain:\nP[|ai − P[φi]| ≥ T + (t+ 1)τ ] ≤ P [|ai − ESh [φi]| ≥ T + tτ ] + P [|P[φi]− ESh [φi]| > τ ]\n≤ 6e−τ 2n + e−t/8\nSimilarly, we continue to have that P [|P[φ]− ESh [φ]| > τ ] ≤ β\nwhen we set B = budget1(|Sh|, τ, β), and so in this case by combining the two bounds we obtain:\nP[|ai − P[φi]| ≥ T + (t+ 1)τ ] ≤ P [|ai − ESh [φi]| ≥ T + tτ ] + P [|P[φi]− ESh [φi]| > τ ]\n≤ β + e−t/8.\nThe theorem results from taking the maximum over the two cases.\nThe conclusion of the theorem shows that the generalization error of the algorithm is tightly concentrated around T + τ. We typically choose the parameters T and τ so that T + τ is reasonably small, e.g., 0.05. Increasing T relative to τ reduces the number of functions with above threshold response, but increases the generalization error. Decreasing T has the opposite effect and even T = 0 leads to a non-trivial guarantee.\n4.2 SparseValidate\nWe now present a general algorithm for validation on the holdout set that can validate many arbitrary queries as long as few of them fail the validation. The algorithm which we refer to as SparseValidate only reveals information about the holdout set when validation fails and therefore we use bounds based on description length to analyze its generalization guarantees.\nMore formally, our algorithm allows the analyst to pick any Boolean function of a dataset ψ (or even any algorithm that outputs a single bit) and provides back the value of ψ on the holdout set ψ(Sh). SparseValidate has a budget m for the total number of queries that can be asked and budget B for the number of queries that returned 1. Once either of the budgets is exhausted, no additional answers are given. We now give a general description of the guarantees of SparseValidate.\nTheorem 23. Let S denote a randomly chosen holdout set of size n. Let A be an algorithm that is given access to SparseValidate(m,B) and outputs queries ψ1, . . . , ψm such that each ψi is in some set Ψi of functions from Xn to {0, 1}. Assume that for every i ∈ [m] and ψi ∈ Ψi, P[ψi(S) = 1] ≤ βi. Let ψi be the random variable equal to the i’th query of A on S. Then P[ψi(S) = 1] ≤ `i · βi, where `i = ∑min{i−1,B} j=0 ( i j ) ≤ mB.\nProof. Let B denote the algorithm that represents view the interaction of A with SparseValidate(m,B) up until query i and outputs the all the i− 1 responses of SparseValidate(m,B) in this interaction. If there are B responses with value 1 in the interaction then all the responses after the last one are meaningless and can be assumed to be equal to 0. The number of binary strings of length i− 1 that contain at most B ones is exactly `i = ∑min{i−1,B} j=0 ( i j ) . Therefore we can assume that the output domain of B has size `i and we denote it by Y. Now, for y ∈ Y let R(y) be the set of datasets S such that ψi(S) = 1, where ψi is the function that A generates when the responses of SparseValidate(m,B) are y and the input holdout dataset is S (for now assume that A is deterministic). By the conditions of the theorem we have that for every y, P[S ∈ R(y)] ≤ βi. Applying Thm. 9 to B, we get that P[S ∈ R(B(S))] ≤ `iβi, which is exactly the claim. We note that to address the case when A is randomized (including dependent on the random choice of the training set) we can use the argument above for every fixing of all the random bits of A. From there we obtain that the claim holds when the probability is taken also over the randomness of A.\nWe remark that the proof can also be obtained via a more direct application of the union bound over all strings in Y. But the proof via Thm. 9 demonstrates the conceptual role that short description length plays in this application.\nIn this general formulation it is the analyst’s responsibility to use the budgets economically and pick query functions that do not fail validation often. At the same time, SparseValidate ensures that (for the appropriate values of the parameters) the analyst can think of the holdout set as a fresh sample for the purposes of validation. Hence the analyst can pick queries in such a way that failing the validation reliably indicates overfitting. To relate this algorithm to Thresholdout, consider the validation query function that is the indicator of the condition |ESh [φ]− ESt [φ]| > T̂ + η (note that this condition can be evaluated using an algorithm with access to Sh). This is precisely the condition that consumes the overfitting budget of Thresholdout. Now, as in Thresholdout, for every fixed φ, P[|ESh [φ]−P[φ]| > τ ] ≤ 2e−2τ 2n. If B ≤ τ2n/ lnm,\nthen we obtain that for every query φ generated by the analyst, we still have strong concentration of the mean on the holdout set around the expectation: P[|ESh [φ]−P [φ]| > τ ] ≤ 2e−τ 2n. This implies that if the condition |ESh [φ]−ESt [φ]| > T̂ +η holds, then with high probability also the condition |P [φ]−ESt [φ]| > T̂ +η− τ holds, indicating overfitting. One important distinction of Thresholdout from SparseValidate is that SparseValidate does not provide corrections in the case of overfitting. One way to remedy that is simply to use a version of SparseValidate that allows functions with values in {0, 1, . . . , L}. It is easy to see that for such functions we would obtain the bound of the form `i · Lmin{B,i−1}. To output a value in [0, 1] with precision τ , L = b1/τc would suffice. However, in many cases a more economical solution would be to have a separate dataset which is used just for obtaining the correct estimates. An examples of the application of SparseValidate for answering statistical and low-sensitivity queries that is based on our analysis can be found in [BSSU15].\nAn alternative view of this algorithm is as a general template for designing algorithms for answering some specific type of adaptively chosen queries. Generalization guarantees specific to the type of query can then be obtained from our general analysis. For example, an algorithm that fits a mixture of Gaussians model to the data could define the validation query to be an algorithm that fits the mixture model to the holdout and obtains a vector of parameters θh. The validation query then compares it with the vector of parameters θt obtained on the training set and outputs 1 if the parameter vectors are “not close” (indicating overfitting). Given guarantees of statistical validity of the parameter estimation method in the static setting one could then derive guarantees for adaptive validation via Thm. 23."
    }, {
      "heading" : "5 Experiments",
      "text" : "We describe a simple experiment on synthetic data that illustrates the danger of reusing a standard holdout set and how this issue can be resolved by our reusable holdout. In our experiment the analyst wants to build a classifier via the following common strategy. First the analyst finds a set of single attributes that are correlated with the class label. Then the analyst aggregates the correlated variables into a single model of higher accuracy (for example using boosting or bagging methods). More formally, the analyst is given a d-dimensional labeled data set S of size 2n and splits it randomly into a training set St and a holdout set Sh of equal size. We denote an element of S by a tuple (x, y) where x is a d-dimensional vector and y ∈ {−1, 1} is the corresponding class label. The analyst wishes to select variables to be included in her classifier. For various values of the number of variables to select k, she picks k variables with the largest absolute correlations with the label. However, she verifies the correlations (with the label) on the holdout set and uses only those variables whose correlation agrees in sign with the correlation on the training set and both correlations are larger than some threshold in absolute value. She then creates a simple linear threshold classifier on the selected variables using only the signs of the correlations of the selected variables. A final test evaluates the classification accuracy of the classifier on both the training set and the holdout set.\nFormally, the algorithm is used to build a linear threshold classifier:\n1. For each attribute i ∈ [d] compute the correlation with the label on the training and holdout sets: wti = ∑ (x,y)∈St xiy and w h i = ∑ (x,y)∈Sh xiy. Let\nW = { i | wti · whi > 0; |wti | ≥ 1/ √ n; |whi | ≥ 1/ √ n| }\nthat is the set of variables for which wti and w h i have the same sign and both are at least 1/\n√ n in\nabsolute value (this is the standard deviation of the correlation in our setting). Let Vk be the subset of variables in W with k largest values of |wti |.\n2. Construct the classifier f(x) = sgn (∑\ni∈Vk sgn(w t i) · xi\n) .\nIn the experiments we used an implementation of Thresholdout that differs somewhat from the algorithm we analyzed theoretically (given in Figure 1). Specifically, we set the parameters to be T = 0.04 and τ = 0.01. This is lower than the values necessary for the proof (and which are not intended for direct application) but suffices to prevent overfitting in our experiment. Second, we use Gaussian noise instead of Laplacian\nnoise as it has stronger concentration properties (in many differential privacy applications similar theoretical guarantees hold mechanisms based on Gaussian noise). No correlation between labels and data: In our first experiment, each attribute is drawn independently from the normal distribution N(0, 1) and we choose the class label y ∈ {−1, 1} uniformly at random so that there is no correlation between the data point and its label. We chose n = 10, 000, d = 10, 000 and varied the number of selected variables k. In this scenario no classifier can achieve true accuracy better than 50%. Nevertheless, reusing a standard holdout results in reported accuracy of over 63% for k = 500 on both the training set and the holdout set (the standard deviation of the error is less than 0.5%). The average and standard deviation of results obtained from 100 independent executions of the experiment are plotted in Figure 2 which also includes the accuracy of the classifier on another fresh data set of size n drawn from the same distribution. We then executed the same algorithm with our reusable holdout. The algorithm Thresholdout was invoked with T = 0.04 and τ = 0.01 explaining why the accuracy of the classifier reported by Thresholdout is off by up to 0.04 whenever the accuracy on the holdout set is within 0.04 of the accuracy on the training set. Thresholdout prevents the algorithm from overfitting to the holdout set and gives a valid estimate of classifier accuracy.\nHigh correlation between labels and some of the variables: In our second experiment, the class labels are correlated with some of the variables. As before the label is randomly chosen from {−1, 1} and each of the attributes is drawn from N(0, 1) aside from 20 attributes which are drawn from N(y · 0.06, 1) where y is the class label. We execute the same algorithm on this data with both the standard holdout and Thresholdout and plot the results in Figure 3. Our experiment shows that when using the reusable holdout, the algorithm still finds a good classifier while preventing overfitting. This illustrates that the reusable holdout simultaneously prevents overfitting and allows for the discovery of true statistical patterns.\nIn Figures 2 and 3, simulations that used Thresholdout for selecting the variables also show the accuracy on the holdout set as reported by Thresholdout. For comparison purposes, in Figure 4 we plot the actual accuracy of the generated classifier on the holdout set (the parameters of the simulation are identical to those used in Figures 2 and 3). It demonstrates that there is essentially no overfitting to the holdout set. Note that the advantage of the accuracy reported by Thresholdout is that it can be used to make further data dependent decisions while mitigating the risk of overfitting. Discussion of the results: Overfitting to the standard holdout set arises in our experiment because the analyst reuses the holdout after using it to measure the correlation of single attributes. We first note that neither cross-validation nor bootstrap resolve this issue. If we used either of these methods to validate the correlations, overfitting would still arise due to using the same data for training and validation (of the final classifier). It is tempting to recommend other solutions to the specific problem we used in our experiment. Indeed, a significant number of methods in the statistics and machine learning literature deal with inference for fixed two-step procedures where the first step is variable selection (see [HTF09] for examples). Our\nexperiment demonstrates that even in such simple and standard settings our method avoids overfitting without the need to use a specialized procedure – and, of course, extends more broadly. More importantly, the reusable holdout gives the analyst a general and principled method to perform multiple validation steps where previously the only known safe approach was to collect a fresh holdout set each time a function depends on the outcomes of previous validations."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this work, we give a unifying view of two techniques (differential privacy and description length bounds) which preserve the generalization guarantees of subsequent algorithms in adaptively chosen sequences of data analyses. Although these two techniques both imply low max-information – and hence can be composed together while preserving their guarantees – the kinds of guarantees that can be achieved by either alone are incomparable. This suggests that the problem of generalization guarantees under adaptivity is ripe for future study on two fronts. First, the existing theory is likely already strong enough to develop practical algorithms with rigorous generalization guarantees, of which Thresholdout is an example. However additional empirical work is needed to better understand when and how the theory should be applied in specific application scenarios. At the same time, new theory is also needed. As an example of a basic question we still do not know the answer to: even in the simple setting of adaptively reusing a holdout set for computing the expectations of boolean-valued predicates, is it possible to obtain stronger generalization guarantees (via any means) than those that are known to be achievable via differential privacy?"
    } ],
    "references" : [ {
      "title" : "Practical privacy: the SuLQ framework",
      "author" : [ "Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim" ],
      "venue" : "In PODS,",
      "citeRegEx" : "Blum et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 2005
    }, {
      "title" : "The ladder: A reliable leaderboard for machine learning competitions",
      "author" : [ "Avrim Blum", "Moritz Hardt" ],
      "venue" : "CoRR, abs/1502.04585,",
      "citeRegEx" : "Blum and Hardt.,? \\Q2015\\E",
      "shortCiteRegEx" : "Blum and Hardt.",
      "year" : 2015
    }, {
      "title" : "More general queries and less generalization error in adaptive data analysis",
      "author" : [ "Raef Bassily", "Adam Smith", "Thomas Steinke", "Jonathan Ullman" ],
      "venue" : "CoRR, abs/1503.04843,",
      "citeRegEx" : "Bassily et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bassily et al\\.",
      "year" : 2015
    }, {
      "title" : "On over-fitting in model selection and subsequent selection bias in performance evaluation",
      "author" : [ "Gavin C. Cawley", "Nicola L.C. Talbot" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Cawley and Talbot.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cawley and Talbot.",
      "year" : 2010
    }, {
      "title" : "Preserving statistical validity in adaptive data analysis",
      "author" : [ "Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth" ],
      "venue" : "CoRR, abs/1411.2664,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient multiple hyperparameter learning for log-linear models",
      "author" : [ "Chuong B. Do", "Chuan-Sheng Foo", "Andrew Y. Ng" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Do et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Do et al\\.",
      "year" : 2007
    }, {
      "title" : "Our data, ourselves: Privacy via distributed noise generation",
      "author" : [ "Cynthia Dwork", "Krishnaram Kenthapadi", "Frank McSherry", "Ilya Mironov", "Moni Naor" ],
      "venue" : "In EUROCRYPT,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Differential privacy and robust statistics",
      "author" : [ "C. Dwork", "J. Lei" ],
      "venue" : "In Proceedings of the 2009 International ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Dwork and Lei.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dwork and Lei.",
      "year" : 2009
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith" ],
      "venue" : "In Theory of Cryptography,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Revealing information while preserving privacy",
      "author" : [ "Irit Dinur", "Kobbi Nissim" ],
      "venue" : "In PODS,",
      "citeRegEx" : "Dinur and Nissim.,? \\Q2003\\E",
      "shortCiteRegEx" : "Dinur and Nissim.",
      "year" : 2003
    }, {
      "title" : "Privacy-preserving datamining on vertically partitioned databases",
      "author" : [ "Cynthia Dwork", "Kobbi Nissim" ],
      "venue" : "In CRYPTO,",
      "citeRegEx" : "Dwork and Nissim.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dwork and Nissim.",
      "year" : 2004
    }, {
      "title" : "The algorithmic foundations of differential privacy",
      "author" : [ "Cynthia Dwork", "Aaron Roth" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "Dwork and Roth.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dwork and Roth.",
      "year" : 2014
    }, {
      "title" : "A firm foundation for private data analysis",
      "author" : [ "Cynthia Dwork" ],
      "venue" : "CACM, 54(1):86–95,",
      "citeRegEx" : "Dwork.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dwork.",
      "year" : 2011
    }, {
      "title" : "A note on screening regression equations",
      "author" : [ "David A. Freedman" ],
      "venue" : "The American Statistician,",
      "citeRegEx" : "Freedman.,? \\Q1983\\E",
      "shortCiteRegEx" : "Freedman.",
      "year" : 1983
    }, {
      "title" : "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "author" : [ "T. Hastie", "R. Tibshirani", "J.H. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2009
    }, {
      "title" : "Preventing false discovery in interactive data analysis is hard",
      "author" : [ "Moritz Hardt", "Jonathan Ullman" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Hardt and Ullman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hardt and Ullman.",
      "year" : 2014
    }, {
      "title" : "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization",
      "author" : [ "Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Rifkin" ],
      "venue" : "Advances in Computational Mathematics,",
      "citeRegEx" : "Mukherjee et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Mukherjee et al\\.",
      "year" : 2006
    }, {
      "title" : "Preventing ”overfitting” of cross-validation data",
      "author" : [ "Andrew Y. Ng" ],
      "venue" : "In ICML, pages 245–253,",
      "citeRegEx" : "Ng.,? \\Q1997\\E",
      "shortCiteRegEx" : "Ng.",
      "year" : 1997
    }, {
      "title" : "On the generalization properties of differential privacy",
      "author" : [ "Kobbi Nissim", "Uri Stemmer" ],
      "venue" : "CoRR, abs/1504.05800,",
      "citeRegEx" : "Nissim and Stemmer.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nissim and Stemmer.",
      "year" : 2015
    }, {
      "title" : "General conditions for predictivity in learning",
      "author" : [ "Tomaso Poggio", "Ryan Rifkin", "Sayan Mukherjee", "Partha Niyogi" ],
      "venue" : "theory. Nature,",
      "citeRegEx" : "Poggio et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Poggio et al\\.",
      "year" : 2004
    }, {
      "title" : "Overfitting in making comparisons between variable selection methods",
      "author" : [ "Juha Reunanen" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Reunanen.,? \\Q2003\\E",
      "shortCiteRegEx" : "Reunanen.",
      "year" : 2003
    }, {
      "title" : "On the dangers of cross-validation. an experimental evaluation",
      "author" : [ "R. Bharat Rao", "Glenn Fung" ],
      "venue" : "In International Conference on Data Mining,",
      "citeRegEx" : "Rao and Fung.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rao and Fung.",
      "year" : 2008
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "Shai Shalev-Shwartz", "Shai Ben-David" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz and Ben.David.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Ben.David.",
      "year" : 2014
    }, {
      "title" : "Learnability, stability and uniform convergence",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2010
    }, {
      "title" : "Interactive fingerprinting codes and the hardness of preventing false discovery",
      "author" : [ "Thomas Steinke", "Jonathan Ullman" ],
      "venue" : "arXiv preprint arXiv:1410.1228,",
      "citeRegEx" : "Steinke and Ullman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Steinke and Ullman.",
      "year" : 2014
    }, {
      "title" : "Learning with differential privacy: Stability, learnability and the sufficiency and necessity of ERM principle",
      "author" : [ "Yu-Xiang Wang", "Jing Lei", "Stephen E. Fienberg" ],
      "venue" : "CoRR, abs/1502.06309,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in [DFH14], where we focused on the problem of estimating expectations of adaptively chosen functions. In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment. We also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach given in [DFH14] is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. This, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches.",
    "creator" : "LaTeX with hyperref package"
  }
}