{
  "name" : "1206.6435.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Rethinking Collapsed Variational Bayes Inference for LDA",
    "authors" : [ "Issei Sato", "Hiroshi Nakagawa" ],
    "emails" : [ "sato@r.dl.itc.u-tokyo.ac.jp", "n3@dl.itc.u-tokyo.ac.jp" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a well-known probabilistic latent variable model. It is used to model the co-occurrence of words by using latent variables called topics where a document is represented as a “bag of words” . It has a wide variety of applications in many fields. Originally, the variational Bayes (VB) inference was used for learning LDA. The collapsed variational Bayes (CVB) inference was developed as an alternative deterministic inference for LDA (Teh et al., 2007). The CVB inference is a variational inference improved by marginalizing out parameters as in a collapsed Gibbs sampler (Griffiths & Steyvers, 2004). (Sung et al., 2008) generalized the CVB inference for conjugate-exponential family models, called latent-space variational Bayes (LSVB) inference.\nSince the CVB inference requires intractable integrals, Teh et al. (Teh et al., 2007) used a second-order Taylor expansion to perform the integrals. Asuncion et al. (Asuncion et al., 2009; Asuncion, 2010) proposed another approximation that uses only the zeroorder information, called the CVB0 inference. The CVB0 inference does not have the drawbacks that\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nother inferences do: VB contains digamma functions which are computationally expensive, while CVB requires the maintenance of variance counts. In contrast, the stochastic nature of the collapsed Gibbs sampler causes it to converge more slowly than the deterministic algorithms. Asuncion et al.’s empirical results suggest that the CVB0 inference learns models that are as good as or better than those learned by the VB and CVB inferences and the collapsed Gibbs sampler in terms of perplexity. Furthermore, as shown in (Asuncion, 2010), when the asymmetric Dirichlet parameters are estimated over document-topic distribution, the predictive performance of the CVB0 inference clearly outperforms that of the CVB inference.\nWe have the question of why CVB0 outperformed CVB, even though the approximation of CVB is more accurate than that of CVB0. In this paper, we propose an interpretation of the CVB0 inference for LDA by using the α-divergence. Using the α-divergence helps\nclarify the properties of the CVB0 inference. We also experimentally show the performance of the subspecies of the CVB0 inference, which is derived with the αdivergence projection framework. Our analysis of the relationship between existing inference algorithms and α-divergence is summarized in Table 1, the meaning of which is revealed in later sections.\nThe remainder of this paper is organized as follows. Sections 3 and 4 review LDA and the CVB / CVB0 inference for LDA, respectively. Sections 5 and 6 explain α-divergence and its local projection, respectively. The key sections 7 and 8 describe local αdivergence projection for LDA and its connection to the CVB0 inference. Section 9 introduces other local projections inspired by the CVB0 inference. Section 10 evaluates algorithms in terms of document modeling. Section 11 concludes this paper."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "Suppose that we have N documents, V vocabularies, and T topics. w = {wd}Nd=1 denotes a set of documents and z = {zd}Nd=1 is a set of assigned topics. θd,t denotes the probability of topic t appearing in document d. ϕt,v denotes the probability of word v appearing in topic t.\nnd,t(z) denotes the number of observations of topic t in document d. nd denotes the total number of words in document d. nt,v(w, z) denotes the number of observations of word v assigned to topic t and nt,·(z) = ∑ v nt,v(w, z). For simplicity, we denote them by nd,t, nt,v and nt,·. The superscription “\\d, i” denotes the corresponding variables or counts with wd,i and zd,i excluded, e.g., w\n\\d,i = w\\{wd,i}, z\\d,i = z\\{zd,i}, and n\\d,it,v is the number of observations of word v assigned to topic t leaving out zd,i.\nE[x] denotes the expectation of x and V[x] = E[x2] − E[x]2 the variance. Multi(·) denotes the multinomial distribution. Dir(·) denotes the Dirichlet distribution."
    }, {
      "heading" : "3. Overview of LDA",
      "text" : "The following generative process is assumed with LDA. First, document-topic distribution θd and topic-word distribution ϕk are generated by\nθd ∼ Dir(γ), ϕt ∼ Dir(β), (1)\nwhere γ = (γ1, · · · , γT ) is a T -dimensional vector and β = (β1, · · · , βV ) is a V -dimensional vector.\nFor each document d, generate the i-th topic zd,i and\nword wd,i:\nzd,i ∼ Multi(θd), wd,i ∼ Multi(ϕzd,i). (2)\nWallach et al. (Wallach et al., 2009) explored the effects of choosing γ and β in LDA. They found in Markov chain Monte Carlo (MCMC) simulations that using asymmetric γ and symmetric β results in better predictive performance for held-out documents. Therefore, we use asymmetric γ = (γ1, · · · , γT ) and symmetric β = (β, · · · , β).\nThe assignment probability of topic t to the i-th word in document d given w\\d,i, z\\d,i, γ and β is\np(zd,i = t|wd,i = v,w\\d,i, z\\d,i,γ, β) ∝ p(wd,i = v,w\\d,i, z\\d,i, zd,i = t|γ, β), ∝ p(wd,i = v|zd,i = t,w\\d,i, z\\d,i|β)p(zd,i = t|z\\d,iγ),\n∝ n \\d,i t,v + β\nn \\d,i t,· + V β\n(n \\d,i d,t + γt). (3)\nThis is used for the collapsed Gibbs sampler."
    }, {
      "heading" : "4. CVB/CVB0 inference for LDA",
      "text" : "(Teh et al., 2007) proposed the CVB inference to LDA inspired by the collapsed Gibbs sampler and showed that the CVB-LDA outperformed the VB-LDA in terms of perplexity. They only introduced a variational posterior q(z) by marginalizing out θ and ϕ. The free energy of the CVB-LDA is given by\nFCV B [q(z)] = M∑ d=1 ∑ zd q(zd) log p(wd,zd|γ, β) q(zd) . (4)\nThus, the updates for q(z) are obtained by taking derivatives of FCV B [q(z)] with respect to {q(zd,i)} and equating to zero:\nq(zd,i = t)\n∝ expE[log p(wd,i = v,w\\d,i, z\\d,i, zd,i = t|γ, β)]q(z\\d,i),\n∝ exp { E [ log n \\d,i t,v + β\nn \\d,i t,· + V β\n(n \\d,i d,t + γt)\n]} ,\n∝ expE[log(n\\d,it,v + β)]\nexpE[log(n\\d,it,· + V β)] expE[log(n\\d,id,t + γt)]. (5)\nThis update equation for q(z) requires approximations to compute intractable expectation. By using the central limit theorem, the expectation should be closely approximated using Gaussian distributions\nwith means and variances, e.g.,\nE[nd,t] = nd∑ i=1 q(zd,i = t), (6)\nV[nd,t] = nd∑ i=1 q(zd,i = t)(1− q(zd,i = t)). (7)\nMoreover, using the second order Taylor expansion, we can approximately calculate\nq(zd,i = t) ∝ β + E[n\\d,it,wd,i ]\nV β + E[n\\d,it,· ] (γt + E[n\\d,id,t ])\nexp ( −\nV[n\\d,it,wd,i ]\n2(β + E[n\\d,it,wd,i ])2 +\nV[n\\d,it,· ]\n2(V β + E[n\\d,it,· ])2\n)\nexp ( −\nV[n\\d,id,t ]\n2(γt + E[n\\d,id,t ])2\n) , (8)\nwhere the superscription“\\d, i” denotes subtracting q(zd,i = t) and q(zd,i = t)(1− q(zd,i = t)).\n(Asuncion et al., 2009) showed the usefulness of an approximation using only zero-order information, called the CVB0 inference. The update using only zero-order information is given by\nq(zd,i = t) ∝ β + E[n\\d,it,wd,i ] V β + ∑\nv E[n \\d,i t,v ]\n(γt + E[n\\d,id,t ]). (9)\nWe derive this CVB0 inference by using α-divergence, which enables us to reveal the relationship among other inference algorithms.\n5. α-Divergence\nThis section reviews α-divergence. A readable introduction is provided in (Minka, 2005).\nLet our task be to approximate a complex probabilistic distribution p(x) where x = {x1, x2, · · · , xn}. We approximate p(x) as q(x), which is a simple probabilistic distribution, such as fully factorized distribution, i.e., q(x) = ∏n i=1 q(xi). A basic approach to obtaining q(x) is to minimize information divergence such as the Kullback-Leibler divergence:\nKL[p||q] = ∫ p(x) log p(x)\nq(x) +\n∫ (q(x)− p(x))dx,\n(10)\nwhere p(x) and q(x) do not need to be normalized. By using the KL-divergence, the estimation of q(x) is defined by the KL-projection of p(x) onto a family of q(x) as follows:\nq∗(x) = argmin q(x) KL[p(x)||q(x)]. (11)\nα-divergence is a generalization of the KL divergence (Amari, 1985; Trottini & Spezzaferri, 2002; Zhu & Rohwer, 1995), indexed by α ∈ (−∞,∞). The α parameter can be used in different ways by different authors. In this paper, we define α-divergence by the convention used in (Minka, 2005):\nDα[p||q] = ∫ αp(x) + (1− α)q(x)− p(x)αq(x)1−αdx\nα(1− α) ,\n(12)\nwhere p(x) and q(x) do not need to be normalized. If p = q, α-divergence is zero. Some special cases are\nD−1[p||q] = 1\n2\n∫ (q(x)− p(x))2\np(x) dx (13)\nlim α→0\nDα[p|q] = KL[q(x)||p(x)] (14) D0.5[p||q] = 2 ∫ ( √ q(x)− √ p(x))2)dx (15)\nlim α→1\nDα[p|q] = KL[p(x)||q(x)] (16)\nD2[p||q] = 1\n2\n∫ (p(x)− q(x))2\nq(x) dx. (17)\nThe case α = 0.5 is known as the Hellinger distance, and α = 2 is the χ2 distance. Since α = −1 swaps the position of p and q of the χ2 distance, we call the case α = −1 “the inverse χ2 distance”, which is the key divergence in this paper."
    }, {
      "heading" : "6. Local α-divergence projection",
      "text" : "In this section, we introduce a local divergence projection-based inference.\nSuppose that the approximate distribution q(x) is fully factorized. We derive the update q(xi) minimizing α-divergence as follows. Taking derivatives of α-divergence (12) with respect to q(xi) and equating them to zero, we obtain the following fixed point iteration equations:\nq(xi) ∝E [( p(x)\nq(x\\i) )α] 1α q(x\\i)\n(18)\nIn many cases, this update is intractable and thus we introduce an approximation for Eq. (18).\nSince Eq. (18) is\nq(xi) ∝E [( p(xi|x\\i) p(x\\i)\nq(x\\i) )α] 1α q(x\\i) , (19)\nwe replace p(x\\i) with q(x\\i), obtaining\nq(xi) ∝E [ p(xi|x\\i)α ] 1 α\nq(x\\i) . (20)\nIn the case α = 1, the update (20) is similar to belief propagation, and the factorized neighbors algorithm (Rosen-Zvi et al., 2005).\nThe update (20) means that it locally minimize αdivergence, i.e., for each i,\nq∗(xi) = argmin q(xi) Dα[p(xi|x\\i)q(x\\i)||q(x)]. (21)\nIn the case α = 1, i.e., KL divergence, this local projection-based inference is equal to the EP algorithm. We describe the connection of this α-divergence projection with the CVB0 inference in the next section."
    }, {
      "heading" : "7. CVB0 as α-divergence projection",
      "text" : "In this section, we derive the CVB0 inference by using the local α-divergence projection. First, we describe how the case α = 1, i.e. EP, cannot be applied for the collapsed LDA. Second, we derive a divergence projection applicable to the collapsed LDA and explain the relationship between this projection and the CVB0 inference.\nWe apply Eq. (21) with α = 1(EP) to the collapsed LDA. For each zd,i, we perform\nq∗(zd,i) = argmin q(zd,i) KL[p(zd,i|w, z\\d,i)q(z\\d,i)||q(z)].\n(22)\nThe update for q(zd,i) is\nq(zd,i = t) ∝E [ p(zd,i = t|wd,i = v,w\\d,i, z\\d,i) ] q(z\\d,i) ,\n∝E [ (n\n\\d,i d,t + γt)\nn \\d,i t,v + β\nn \\d,i t,· + V β ] q(z\\d,i) . (23)\nThe problem is that we cannot analytically execute this expectation. (Asuncion, 2010) derived Eq.(23) in a different way where he changed the CVB free energy by moving the logarithm out of the expectations, and pointed out the relationship between Eq.(23) and the CVB0 inference, which inspired this work. However, the intractable expectation in Eq.(23) was not executed. This intractability makes interpreting the CVB0 inference difficult.\nHere, we derive another approach by using the αdivergence projection. The key idea is to construct q(zd,i) by using the novel three parameters .\nWe define q(zd,i) as follows:\nq(zd,i = t) ∝ a(zd,i)b(zd,i)c(zd,i) (24) a(zd,i = t) = ñ \\d,i d,t + γt, (25) b(zd,i = t) = ñ \\d,i t,v + β, (26) c(zd,i = t) = 1\nñ \\d,i t,· + V β\n, (27)\nwhere we do not assume that ñ \\d,i d,i , ñ \\d,i t,v and ñ \\d,i t,· are expected counts, i.e., these are parameters of q(zd,i).\nWe also define\nq\\a(zd,i) = b(zd,i)c(zd,i), (28) q\\b(zd,i) = a(zd,i)c(zd,i) (29) q\\c(zd,i) = a(zd,i)b(zd,i). (30)\nSince our definition of α-divergence does not require normalization of the probabilistic distribution, we can introduce the following local projection:\na∗(zd,i = t) =\nargmin a(zd,i)\nDα[(n \\d,i d,t + γt)q \\ad,i(z)||a(zd,i)q\\ad,i(z)], (31)\nwhere q\\ad,i(z) = q\\a(zd,i)q(z \\d,i). Solving the above\noptimization (see Appendix A), we obtain\na∗(zd,i = t) =E [ (n \\d,i d,t + γt) α ] 1 α\nq(z\\d,i) . (32)\nAs in a(zd,i), we obtain b ∗(zd,i) and c ∗(zd,i) by locally minimizing the α-divergence:\nb∗(zd,i = t) =\nargmin b(zd,i)\nDα[(n \\d,i t,v + β)q \\bd,i(z)||b(zd,i)q\\bd,i(z)], (33)\nc∗(zd,i = t) =\nargmin c(zd,i)\nDα[ 1\n(n \\d,i t,· + V β)\nq\\cd,i(z)||c(zd,i)q\\cd,i(z)]. (34)\nThus, we have\nb∗(zd,i = t) = E [ (n \\d,i t,v + β) α ] 1 α\nq(z\\d,i) , (35)\nc∗(zd,i = t) = E\n[( 1\nn \\d,i t,· + V β )α] 1α q(z\\d,i) . (36)\nWhen we use α-divergence projection with α = 1 for\nestimating a(zd,i) and b(zd,i), we have a(α=1)(zd,i = t) =E [ n \\d,i d,t + γt ] q(z\\d,i) = E[n\\d,id,t ] + γt,\n(37) b(α=1)(zd,i = t) =E [ n \\d,i t,v + β ] q(z\\d,i) = E[n\\d,it,v ] + β.\n(38)\nWhen we use α-divergence projection with α = −1 for estimating c(zd,i), we have\nc(α=−1)(zd,i = t) =E ( 1 n \\d,i t,· + V β )−1−1 q(z\\d,i) ,\n=E [ n \\d,i t,· + V β ]−1 q(z\\d,i) , = 1\nE[n\\d,it,· ] + V β . (39)\nTherefore, we have the following update for q(zd,i)\nq(zd,i = t) ∝ a(α=1)(zd,i)b(α=1)(zd,i)c(α=−1)(zd,i),\n= (E[n\\d,id,t ] + γ) E[n\\d,it,v ] + β\nE[n\\d,it,· ] + V β . (40)\nAlthough the updates are performed in order, i.e., update a∗ given b and c, b∗ given a∗ and c, and c∗ given a∗ and b∗, this update is equal to the CVB0 update in Eq.(9)."
    }, {
      "heading" : "8. Discussion",
      "text" : "In this section, we explain why the CVB0 inference outperforms the CVB inference. To sum up this discussion, in the CVB0 inference, the “zero-forcing effect” works only with the nt,· estimation, while in the CVB inference it works with the q(z) estimation.\nThe previous section showed that the CVB0 inference is composed of the three projections with a mixture of α = 1 and α = −1:\nD1 = KL[(n \\d,i d,t (z) + γt)q \\ad,i(z)||q(z)], (41) D1 = KL[(n \\d,i t,v (z) + β)q \\bd,i(z)||q(z)], (42)\nD−1\n[ 1\n(n \\d,i t,· (z) + V β)\nq\\cd,i(z)||q(z) ] . (43)\nThis projection-based update with a different divergence measure reveals the properties of the CVB0 inference. Ideally, we use the (α = 1)-divergence projection, i.e., D1[p|q] = KL[p||q], but the integrals\nE[ 1 n \\d,i t,· +V β ] are not easy to evaluate. Instead, we use the inverse χ2 divergence D−1[p||q] for estimating c(zd,i).\nD−1[p||q] = 12 ∫ (q(x)−p(x))2 p(x) dx is known as a zeroforcing divergence (Minka, 2005) which emphasizes q to be small when p being small, i.e., p(x) = 0 forces q(x) = 0, which means that it avoids “false positive”. In our case (43), the zero-forcing effect on the nt,· estimation means that the emphasis in the estimation is on high-frequency topics or low-frequency topics tend to be estimated as zero in an entire corpus. We think that affecting n \\d,i t,· matters much less than affecting n \\d,i d,t and n \\d,i t,v throughout a whole corpus in LDA. We explain the zero-forcing effect of CVB0 in more detail in the next section.\nReturning to Eq.(20), i.e., q(xi) ∝ E [ p(xi|x\\i)α ] 1 α\nq(x\\i) ,\nwe describes the relationship between the CVB inference and α-divergence projection. First, we introduce the following theorem:\nTheorem 1 (Liapunov’s inequality) If x is a nonnegative random variable, and we have two real numbers α2 > α1, then\nE[xα2 ] 1 α2 ≥ E[xα1 ] 1 α1 . (44)\nand\nlim α→0\nE[xα] 1 α = expE[log(x)]. (45)\nUsing Eq.(20) and Theorem 1, we obtain\nq(xi) ∝ lim α→0\nE [ p(xi|x\\i)α ] 1 α\nq(x\\i) = exp(E[log p(xi|x\\i)])\n(46)\nThis is the variational inference minimizing KL[q||p].\nIn LDA, we have\nq(zd,i = t) ∝ lim α→0\nE [ p(zi,d|wd,i = v,w\\d,i, z\\d,i)α ] 1 α\nq(z\\d,i)\n= exp(E[log p(zd,i|wd,i = v,w\\d,i, z\\d,i)])\n∝ expE [ log n \\d,i d,t + γt\nn \\d,i d,· + ∑ t γt n \\d,i t,v + β n \\d,i t,· + V β ] q(z\\d,i) ,\n∝ exp(E[log(n\\d,id,t + γt)]) exp(E[log(n\\d,it,v + β)])\nexp(E[log(n\\d,it,· + V β)]) (47)\nThe update Eq.(47) is the same update as the CVB inference in Eq.(5). (α → 0)-divergence is also known to induce the zero-forcing effect."
    }, {
      "heading" : "9. Subspecies inspired by CVB0",
      "text" : "In this section, we consider other projection-based algorithms that help clarify the property of the zeroforcing effect in CVB0.\n9.1. CVB with (α = 1)-divergence\nFrom our view point, the CVB0 inference is composed of two different-type divergence projections: α = 1,−1. We consider using only α = 1 for the projections. To do this, we have to calculate the expectation given by\nc(α=1)(zd,i = t) = E\n[ 1\nn \\d,i t,· + V β ] q(z\\d,i) . (48)\nSince we cannot derive the analytical solution for this expectation, we propose two approximation methods. The first is a stochastic approximation called sample averaging given by\nc̃(α=1)(zd,i = t) = 1\nS S∑ s=1\n1\nn \\d,i t,· (z (s)) + V β , (49)\nwhere S denotes the number of samples and z(s) is the s-th samples generated from q(z). This method is accurate but not practical when S takes a large value. We use this approximation to investigate the accuracy of the next approximation.\nThe second is a deterministic approximation that uses the same approximation of CVB with the second-order Taylor expansion and Gaussian approximation given by\nĉ(α=1)(zd,i = t) = 1\nE[n\\d,it,· ] + V β +\nV[n\\d,it,· ]\n(E[n\\d,it,· ] + V β)3 .\n(50)\nAs shown in the experiments (Sec.10), we find that the second term of Eq.(50) is vanishingly small. ĉ(α=1) in Eq.(50) is calculated\nas 1 E[n\\d,it,· ]+V β\n( 1 +\nV[n\\d,it,· ] (E[n\\d,it,· ]+V β)2\n) . We find\nV[n\\d,it,· ] (E[n\\d,it,· ]+V β)2 = O(1/n) in many cases where n denotes the number of all words (tokens). For example, the variance takes the largest value when q(zd,i = t) = 1/2 for all d and i. In this case, E[nt] = n/2 and V[nt] = n(1 − 1/2)/2 = n/4. Therefore, we consider c(α=−1) is similar to c(α=1), which means that CVB0 is rarely affected by the zero-forcing effect."
    }, {
      "heading" : "9.2. Type-base CVB0 Inference",
      "text" : "We derive a type-based inference as an application of our framework. In a type-based inference, we only estimate the probabilistic distribution for each type in a document not each token; this is beneficial for computation cost and memory usage.\nWe exclude all counts of word v from document d, denoted by superscription “\\d, v”. The appearance probability of word v given w\n\\d,v d and z \\d,v d is\np(wd,∗ = v|w\\d,v, z\\d,v) = T∑\nt=1\nn \\d,v d,t + γt\nn \\d,v d,· + ∑ t γt n \\d,v t,v + β n \\d,v t,· + V β\n(51)\nMoreover, we have\np(zd,v = t|w\\d,v) ∝\nE\n[ n \\d,v d,t + γt\nn \\d,v d,· + ∑ t γt n \\d,v t,v + β n \\d,v t,· + V β ] p(z\\d,v|w\\d,v)\n(52)\nHere, we consider obtaining an approximation distribution q(zd,v). Instead of zd,i, we define q(z) factorized by using q(zd,v), i.e., q(zd,i) =∑V\nv=1 q(zd,v)δ(wd,i = v) and q(z) = ∏V v=1 q(zd,v) nd,v .\nThe update of q(zd,v) is obtained by\nq(zd,v = t) ∝ E [ (n \\d,v d,t + γt) n \\d,v t,v + β\nn \\d,v t,· + V β ] q(z\\d,v) ,\n(53)\nwhich is derived by minimizing the α-divergence as in q(zd,i).\nUsing the local α-divergence projection with α = 1 for n \\d,v d,t + γt and n\n\\d,v t,v + β, and α = −1 for 1n\\d,vt,· +V β , we\nhave\nq(zd,v = t) ∝ (E[n\\d,vd,t ] + γ) E[n\\d,vt,v ] + β\nE[n\\d,vt,· ] + V β . (54)\nWe call this update the type-based CVB0 (TCVB0) inference."
    }, {
      "heading" : "10. Experiments",
      "text" : "We compared CVB0 with its subspecies on document modeling in terms of perplexity to investigate the effect of α = −1. All results are averaged values from five experimental runs with random initialization. We set the number of iterations to 100 for each inference.\nWe use a fixed point equation for updating γ introduced in (Minka, 2000). We set β = 0.01 because\n(Asuncion et al., 2009) showed that CVB0-LDA with β = 0.01 worked well when compared with other setings (β = 0.01 was also used in (Griffiths & Steyvers, 2004)).\nIn this section’s figures, “CVB” indicates the second order approximation of the CVB inference.\n“CVB1s” indicates the stochastic approximation in Eq.(49) with S = 50. “CVB1d” indicates the deterministic approximation in Eq.(50).\nWe used four sets of text data with different properties. The first was ‘NIPS corpus (NIPS)” from which the number of documents was N = 1, 500 and the vocabulary size was V = 12, 245. The second was “The Wall Street Journal (WSJ)” from which we randomly chose N = 5, 000 (V = 38, 272) documents. The third was “Enron email corpus (Enron)” from which we randomly chose N = 5, 000 (V = 14, 758) documents. The fourth was “20 news group corpus (20ng)” from which we randomly chose N = 5, 000 (V = 13, 176). Stop words were eliminated.\nThe comparison metric we used for document modeling was the perplexity used by (Teh et al., 2007; 2008) that indicates the prediction performance for held-out words. We randomly split the words in a document into training words wtraind (80%) and test words w test d (20%).\nFigure 1 shows the experimental results. The bar graph indicates the results for test set perplexity interms of (T = 40, 80, 120) in each corpus. CVB0, CVB1s, CVB1d and TCVB0 outperformed CVB in terms of perplexity. Although we compared VB with others, we eliminated the VB results to clarify the differences of inference algorithms because CVB outperformed VB and the VB results change the scale of a bar-graph in some corpora.\nThe performances of CVB1s and CVB1d were similar to that of CVB0. Since the results of CVB1d were similar to those of CVB1s, the approximation used in CVB1d seemed to be accurate. When we analyzed\nV[nt,·] (E[nt,·]+V β)2 in Sec.9.1, the maximum value in all corpus when T = 120 was about 3.17e−4, which is negligible compared with 1. Therefore, as discussed in Sec.9.1, CVB0 was not affected by the zero-forcing effect. We believe this is the reason CVB0 worked better than CVB. Moreover, the performance of TCVB0 was similar to that of CVB0. Consequently, the TCVB0 inference was practical."
    }, {
      "heading" : "11. Conclusion",
      "text" : "In this paper, we reviewed existing inference algorithms of LDA in terms of the α-divergence projection. We showed that the CVB0 inference is composed of (α = 1,−1)- divergence projections and that α = −1 is similar to α = 1 in LDA, which means that CVB0 is not affected by the zero-forcing effect in LDA. Combining the marginalization of parameters and the heterogeneous α-divergence projection is useful because it is easy to apply to other topic models learned by the collapsed Gibbs sampler. Future work is to develop an online-update extension, such as that by (Hoffman et al., 2010; Sato et al., 2010; Wang et al., 2011). From the relationship between EP and assumed density filtering, we can extend the local α-divergence projection into an online algorithm, which leads to the online CVB0 inference. A convergence analysis is also important remaining work.\nA. Derivation for Eq.(32)\nTaking derivatives of\nDα[(n \\d,i d,t + γt)q \\ad,i(z)||a(zd,i)q\\ad,i(z)],\nwith respect to a(zd,i) and equating them to zero, 0 = ∑ z\\d,i q\\ad,i(z)− a(zd,i)−α ∑ z\\d,i (n \\d,i d,t + γt) αq\\ad,i(z),\nand we obtain the following fixed point iteration equations:\na(zd,i) =\n[∑ z\\d,i(n \\d,i d,t + γt)\nαq\\ad,i(z)∑ z\\d,i q \\ad,i(z)\n] 1 α\n,\n=\n[∑ z\\d,i(n \\d,i d,t + γt)\nαb(zd,i)c(zd,i)q(z \\d,i)∑\nz\\d,i b(zd,i)c(zd,i)q(z \\d,i)\n] 1 α\n,\n=\n[ b(zd,i)c(zd,i) ∑ z\\d,i(n \\d,i d,t + γt) αq(z\\d,i)\nb(zd,i)c(zd,i) ∑ z\\d,i q(z \\d,i)\n] 1 α\n.\n(55) Since ∑\nz\\d,i q(z \\d,i) = 1, we have\na(zd,i) = [∑ z\\d,i (n \\d,i d,t + γt) αq(z\\d,i) ] 1 α . (56)"
    } ],
    "references" : [ {
      "title" : "Approximate mean field for dirichlet-based",
      "author" : [ "Springer", "New York", "A. 1985. Asuncion" ],
      "venue" : null,
      "citeRegEx" : "Springer et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Springer et al\\.",
      "year" : 1985
    }, {
      "title" : "On smoothing and inference for topic models",
      "author" : [ "A. Asuncion", "M. Welling", "P. Smyth", "Y.W. Teh" ],
      "venue" : "In Proceedings of the International Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Asuncion et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Asuncion et al\\.",
      "year" : 2009
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T.L. Griffiths", "M. Steyvers" ],
      "venue" : "Proc Natl Acad Sci U S A,",
      "citeRegEx" : "Griffiths and Steyvers,? \\Q2004\\E",
      "shortCiteRegEx" : "Griffiths and Steyvers",
      "year" : 2004
    }, {
      "title" : "Online learning for latent dirichlet allocation",
      "author" : [ "Hoffman", "Matthew D", "Blei", "David M", "Bach", "Francis R" ],
      "venue" : "In Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2010
    }, {
      "title" : "Expectation-Propagation for the generative aspect model",
      "author" : [ "T. Minka", "J. Lafferty" ],
      "venue" : "In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (UAI)",
      "citeRegEx" : "Minka and Lafferty,? \\Q2002\\E",
      "shortCiteRegEx" : "Minka and Lafferty",
      "year" : 2002
    }, {
      "title" : "Divergence measures and message passing",
      "author" : [ "Minka", "Thomas" ],
      "venue" : "Technical report, Microsoft Research,",
      "citeRegEx" : "Minka and Thomas.,? \\Q2005\\E",
      "shortCiteRegEx" : "Minka and Thomas.",
      "year" : 2005
    }, {
      "title" : "Estimating a dirichlet distribution",
      "author" : [ "Minka", "Thomas P" ],
      "venue" : "Technical report, Microsoft,",
      "citeRegEx" : "Minka and P.,? \\Q2000\\E",
      "shortCiteRegEx" : "Minka and P.",
      "year" : 2000
    }, {
      "title" : "The dlr hierarchy of approximate inference",
      "author" : [ "Rosen-Zvi", "Michal", "Jordan", "Michael I", "Yuille", "Alan L" ],
      "venue" : "In Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Rosen.Zvi et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Rosen.Zvi et al\\.",
      "year" : 2005
    }, {
      "title" : "Deterministic single-pass algorithm for lda",
      "author" : [ "Sato", "Issei", "Kurihara", "Kenichi", "Nakagawa", "Hiroshi" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sato et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sato et al\\.",
      "year" : 2010
    }, {
      "title" : "Latent-space variational bayes",
      "author" : [ "Sung", "Jaemo", "Ghahramani", "Zoubin", "Bang", "Sung-Yang" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Sung et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2008
    }, {
      "title" : "A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation",
      "author" : [ "Teh", "Yee Whye", "Newman", "David", "Welling", "Max" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Teh et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2007
    }, {
      "title" : "Collapsed variational inference for hdp",
      "author" : [ "Teh", "Yee Whye", "Kurihara", "Kenichi", "Welling", "Max" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Teh et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2008
    }, {
      "title" : "A generalized predictive criterion for model selection",
      "author" : [ "M. Trottini", "F. Spezzaferri" ],
      "venue" : "In Canadian Journal of Statisticse,",
      "citeRegEx" : "Trottini and Spezzaferri,? \\Q2002\\E",
      "shortCiteRegEx" : "Trottini and Spezzaferri",
      "year" : 2002
    }, {
      "title" : "Rethinking lda: Why priors matter",
      "author" : [ "Wallach", "Hanna", "Mimno", "David", "McCallum", "Andrew" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Wallach et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wallach et al\\.",
      "year" : 2009
    }, {
      "title" : "Online variational inference for the hierarchical dirichlet process",
      "author" : [ "Wang", "Chong", "Paisley", "John William", "Blei", "David M" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Wang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2011
    }, {
      "title" : "Information geometric measurements of generalisation",
      "author" : [ "Zhu", "Huaiyu", "Rohwer", "Richard" ],
      "venue" : "Technical report, Aston University,",
      "citeRegEx" : "Zhu et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "The collapsed variational Bayes (CVB) inference was developed as an alternative deterministic inference for LDA (Teh et al., 2007).",
      "startOffset" : 112,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "(Sung et al., 2008) generalized the CVB inference for conjugate-exponential family models, called latent-space variational Bayes (LSVB) inference.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "(Teh et al., 2007) used a second-order Taylor expansion to perform the integrals.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "(Asuncion et al., 2009; Asuncion, 2010) proposed another approximation that uses only the zeroorder information, called the CVB0 inference.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "(Wallach et al., 2009) explored the effects of choosing γ and β in LDA.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "(Teh et al., 2007) proposed the CVB inference to LDA inspired by the collapsed Gibbs sampler and showed that the CVB-LDA outperformed the VB-LDA in terms of perplexity.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "(Asuncion et al., 2009) showed the usefulness of an approximation using only zero-order information, called the CVB0 inference.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "In the case α = 1, the update (20) is similar to belief propagation, and the factorized neighbors algorithm (Rosen-Zvi et al., 2005).",
      "startOffset" : 108,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "(Asuncion et al., 2009) showed that CVB0-LDA with β = 0.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "The comparison metric we used for document modeling was the perplexity used by (Teh et al., 2007; 2008) that indicates the prediction performance for held-out words.",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "Future work is to develop an online-update extension, such as that by (Hoffman et al., 2010; Sato et al., 2010; Wang et al., 2011).",
      "startOffset" : 70,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "Future work is to develop an online-update extension, such as that by (Hoffman et al., 2010; Sato et al., 2010; Wang et al., 2011).",
      "startOffset" : 70,
      "endOffset" : 130
    }, {
      "referenceID" : 14,
      "context" : "Future work is to develop an online-update extension, such as that by (Hoffman et al., 2010; Sato et al., 2010; Wang et al., 2011).",
      "startOffset" : 70,
      "endOffset" : 130
    } ],
    "year" : 2012,
    "abstractText" : "We propose a novel interpretation of the collapsed variational Bayes inference with a zero-order Taylor expansion approximation, called CVB0 inference, for latent Dirichlet allocation (LDA). We clarify the properties of the CVB0 inference by using the αdivergence. We show that the CVB0 inference is composed of two different divergence projections: α = 1 and −1. This interpretation will help shed light on CVB0 works.",
    "creator" : " TeX output 2012.05.21:0841"
  }
}