{
  "name" : "1704.06191.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Softmax GAN", "Min Lin" ],
    "emails" : [ "mavenlin@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Generative Adversarial Networks(GAN) [4] has achieved great success due to its ability to generate realistic samples. GAN is composed of one Discriminator and one Generator. The discriminator tries to distinguish real samples from generated samples, while the generator counterfeits real samples using information from the discriminator. GAN is unique from the many other generative models. Instead of explicitly sampling from a probability distribution, GAN uses a deep neural network as a direct generator that generates samples from random noises. GAN has been proved to work well on several realistic tasks, e.g. image inpainting, debluring and imitation learning.\nDespite its success in many applications, GAN is highly unstable in training. Careful selection of hyperparameters is often necessary to make the training process converge [11]. It is often believed that this instability is caused by unbalanced discriminator and generator training. As the discriminator utilizes a logistic loss, it saturates quickly and its gradient vanishes if the generated samples are easy to separate from the real ones. When the discriminator fails to provide gradient, the generator stops updating. Softmax GAN overcomes this problem by utilizing the softmax cross-entropy loss, whose gradient is always non-zero unless the softmaxed distribution matches the target distribution."
    }, {
      "heading" : "2 Related Works",
      "text" : "There are many works related to improving the stability of GAN training. DCGAN proposed by Radford et. al. [11] comes up with several empirical techniques that works well, including how to apply batch normalization, how the input should be normalized, and which activation function to use. Some more techniques are proposed by Salimans et. al. [13]. One of them is minibatch discrimination. The idea is to introduce a layer that operates across samples to introduce coordination between gradients from different samples in a minibatch. In this work, we achieve a similar effect using softmax across the samples. We argue that softmax is more natural and explanable and yet does not require extra parameters.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 4.\n06 19\n1v 1\n[ cs\n.L G\n] 2\n0 A\npr 2\n01 7\nNowozin et. al. [9] generalizes the GAN training loss from Jensen-Shannon divergence to any f-divergence function. Wasserstein distance is mentioned as a member of another class of probability metric in this paper but is not implemented. Under the f-GAN framework, training objectives with more stable gradients can be developed. For example, the Least Square GAN [8] uses l2 loss function as the objective, which achieves faster training and improves generation quality.\nArjovsky et. al. managed to use Wasserstein distance as the objective in their Wasserstein GAN (WGAN) [1] work. This new objective has non-zero gradients everywhere. The implementation is as simple as removing the sigmoid function in the objective and adding weight clipping to the discriminator network. WGAN is shown to be free of the many problems in the original GAN, such as mode collapse and unstable training process. A related work to WGAN is Loss-Sensitive GAN [10], whose objective is to minimize the loss for real data and maximize it for the fake ones. The common property of Least Square GAN, WGAN, Loss-Sensitive GAN and this work is the usage of objective functions with non-vanishing gradients."
    }, {
      "heading" : "3 Softmax GAN",
      "text" : "We denote the minibatch sampled from the training data and the generated data as B+ and B− respectively. B = B+ + B− is the union of B+ and B−. The output of the discriminator is represented by µθ(x) parameterized by θ. ZB = ∑ x∈B e\n−µθ(x) is the partition function of the softmax within batch B. We use x for samples from B+ and x′ for generated samples in B−. As in GAN, generated samples are not directly sampled from a distribution. Instead, they are generated directly from a random variable z with a trainable generator Gψ .\nWe softmax normalized the energy of the all data points within B, and use the cross-entropy loss for both the discriminator and the generator. The target of the discriminator is to assign the probability mass equally to all samples in B+, leaving samples in B− with zero probability.\ntD(x) = { 1 |B+| , if x ∈ B+ 0, if x ∈ B−\n(1)\nLD = − ∑ x∈B tD(x) ln e−µ θ(x) ZB\n= − ∑ x∈B+ 1 |B+| ln e−µ θ(x) ZB − ∑ x′∈B− 0 ln e−µ θ(x′) ZB\n= ∑ x∈B+ 1 |B+| µθ(x) + lnZB (2)\nFor generator, the target is to assign the probability mass equally to all the samples in B.\ntG(x) = 1\n|B| (3)\nLG = − ∑ x∈B tG(x) ln e−µ θ(x) ZB\n= − ∑ x∈B+ 1 |B| ln e−µ θ(x) ZB − ∑ x′∈B− 1 |B| ln e−µ θ(x′) ZB\n= ∑ x∈B+ 1 |B| µθ(x) + ∑ x′∈B− 1 |B| µθ(x′) + lnZB (4)"
    }, {
      "heading" : "4 Relationship to Importance Sampling",
      "text" : "It has been pointed out in the original GAN paper that GAN is similar to NCE [6] in that both of them use a binary logistic classification loss as the surrogate function for training of a generative model. GAN improves over NCE by using a trained generator for noise samples instead of fixing the noise distribution. In the same sense as GAN is related to NCE [5], this work can be seen as the Importance Sampling version of GAN [2]. We prove it as follows."
    }, {
      "heading" : "4.1 Discriminator",
      "text" : "We use ξφ(x) to represent energy function and Z = ∫ x e−ξ φ(x) is the partition function. The probability density function is then pφ(x) = e −ξφ(x)\nZ . With O as the observed training example, the maximum likelyhood estimation loss function is as follows\nJ = 1 |O| ∑ x∈O ξφ(x) + log ∫ x′ e−ξ φ(x′)dx (5)\n∇φJ = 1 |O| ∑ x∈O ∇φξφ(x)− Ex′∼pφ(x′)∇φξφ(x′) (6)\nAs it is usually difficult to sample from pφ, Importance Sampling instead introduces a known distribution q(x) to sample from, resulting in:\n∇φJ = 1 |O| ∑ x∈O ∇φξφ(x)− Ex′∼q(x′) pφ(x′) q(x′) ∇φξφ(x′) (7)\nIn the biased Importance Sampling [3], the above is converted to the following biased estimation which can be calculated without knowing pφ:\n∇̂φJ = 1 |B+| ∑ x∈B+ ∇φξφ(x)− 1 R ∑ x′∈Q r(x′)∇φξφ(x′) (8)\nwhere r(x′) = e −ξφ(x′) q(x′) , R = ∑ x′∈Q r(x ′). And Q is a batch of data sampled from q. At this point, we reparameterize e−ξ φ(x) = e−µ θ(x)q(x).\n∇̂θJ = 1 |B+| ∑ x∈B+ ∇θµθ(x)− 1∑ y∈Q e −µθ(y) ∑ x′∈Q e−µ θ(x′)∇θµθ(x′) (9)\nWithout loss of generality, we assume |B+| = |B−| and replace Q with B = B+ +B− in equation 9, namely q(x) = 12pdata(x) + 1 2pG(x), and compare the above with equation 2. It is easy to see that the above is the gradient of LD. In other words, the discriminator loss function in Softmax GAN is performing maximum likelihood on the observed real data with Importance Sampling to estimate the partition function.\nWith infinite number of real samples, the optimal solution is\ne−µ θ(x) = C pD pD+pG\n2\n(10)\nC is a constant."
    }, {
      "heading" : "4.2 Generator",
      "text" : "We substitute equation 10 into 4. The lhs of equation 4 gives\n− ∑ x∈B 1 |B| ln pD pD+pG 2 − lnC = KL(pD + pG 2 ‖pD) (11)\nThe gradient of the rhs can be seen as biased Importance Sampling as well,\n− ∑ x∈B pD pD+pG\n2 ∇pG pD+pG∑\nx∈B pD\npD+pG 2\n≈ −Ex∼pD ∇pG\npD + pG (12)\nwhich optimizes −Ex∼pD ln(pD + pG) = KL(pD‖ pD+pG 2 ) − Ex∼pD ln 2pD. After removing the constants, we get\nLG = KL( pD + pG\n2 ‖pD) +KL(pD‖ pD + pG 2 ) (13)\nThus optimizing the objective of the generator is equivalent to minimizing the Jensen-Shannon divergence between pD and pD+pG2 with Importance Sampling."
    }, {
      "heading" : "4.3 Importance Sampling’s link to NCE",
      "text" : "Note that Importance Sampling itself is strongly connected to NCE. As pointed out by [7] and [12], both Importance Sampling and NCE are training the underlying generative model with a classification surrogate. The difference is that in NCE, a binary classification task is defined between true and noise samples with a logistic loss, whereas Importance Sampling replaces the logistic loss with a multiclass softmax and cross-entropy loss. We show the relationship between NCE, Importance Sampling, GAN and this work in Figure 1. Softmax GAN is filling the table with the missing item."
    }, {
      "heading" : "4.4 Infinite batch size",
      "text" : "As pointed out by [3], biased Importance Sampling estimation converges to the real partition function when the number of samples in one batch goes to infinity. In practice, we found that setting |B+| = |B−| = 5 is enough for generating images that are visually realistic."
    }, {
      "heading" : "5 Experiments",
      "text" : "We run experiments on image generation with the celebA database. We show that although Softmax GAN is minimizing the Jensen Shannon divergence between the generated data and the real data, it is more stable than the original GAN, and is less prone to mode collapsing.\nWe implement Softmax GAN by modifying the loss function of the DCGAN code (https://github.com/carpedm20/DCGAN-tensorflow). As DCGAN is quite stable, we remove the empirical techniques applied in DCGAN and observe instability in the training. On the contrary, Softmax GAN is stable to these changes."
    }, {
      "heading" : "5.1 Stablized training",
      "text" : "We follow the WGAN paper, by removing the batch normalization layers and using a constant number of filters in the generator network. We compare the results from GAN and Softmax GAN. The results are shown in Figure 2."
    }, {
      "heading" : "5.2 Mode collapse",
      "text" : "When GAN training is not stable, the generator may generate samples similar to each other. This lack of diversity is called mode collapse because the generator can be seen as sampling only from some of the modes of the data distribution.\nIn the DCGAN paper, the pixels of the input images are normalized to [−1, 1). We remove this constraint and instead normalize the pixels to [0, 1). At the same time, we replace the leaky relu with relu, which makes the gradients more sparse (this is unfavorable for GAN training according to the DCGAN authors). Under this setting, the original GAN suffers from a significant degree of mode collape and low image qualities. In contrast, Softmax GAN is robust to this change. Examplars are show in Figure 3."
    }, {
      "heading" : "5.3 Balance of generator and discriminator",
      "text" : "It is claimed in [11] that manually balancing the number of iterations of the discriminator and generator is a bad idea. The WGAN paper however gives the discriminator phase more iterations to get a better discriminator for the training of the generator. We set the discriminator vs generator ratio to 5 : 1 and 1 : 5 and explore the effects of the this ratio on DCGAN and Softmax GAN. The results are in Figure 4 and 5 respectively."
    }, {
      "heading" : "6 Conclusions and future work",
      "text" : "We propose a variant of GAN which does softmax across samples in a minibatch, and uses crossentropy loss for both discriminator and generator. The target is to assign all probability to real data for discriminator and to assign probability equally to all samples for the generator. We proved that this objective approximately optimizes the JS-divergence using Importance Sampling. We futhur form the links between GAN, NCE, Importance Sampling and Softmax GAN. We demonstrate with experiments that Softmax GAN consistently gets good results when GAN fails at the removal of empirical techniques.\nIn our future work, we’ll perform a more systematic comparison between Softmax GAN and other GAN variants and verify whether it works on tasks other than image generation."
    } ],
    "references" : [ {
      "title" : "Quick training of probabilistic neural nets by importance sampling",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Adaptive importance sampling to accelerate training of a neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Jean-Sébastien Senécal" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "On distinguishability criteria for estimating generative models",
      "author" : [ "Ian J Goodfellow" ],
      "venue" : "arXiv preprint arXiv:1412.6515,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
      "author" : [ "Michael Gutmann", "Aapo Hyvärinen" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu" ],
      "venue" : "arXiv preprint arXiv:1602.02410,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Least squares generative adversarial networks",
      "author" : [ "Xudong Mao", "Qing Li", "Haoran Xie", "Raymond YK Lau", "Zhen Wang" ],
      "venue" : "arXiv preprint ArXiv:1611.04076,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "f-gan: Training generative neural samplers using variational divergence minimization",
      "author" : [ "Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Loss-sensitive generative adversarial networks on lipschitz densities",
      "author" : [ "Guo-Jun Qi" ],
      "venue" : "arXiv preprint arXiv:1701.06264,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2017
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "On word embeddings - part 2: Approximating the softmax",
      "author" : [ "Sebastian Ruder" ],
      "venue" : "http://sebastianruder.com/word-embeddings-softmax/index.html# similaritybetweennceandis",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "1 Introduction Generative Adversarial Networks(GAN) [4] has achieved great success due to its ability to generate realistic samples.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "Careful selection of hyperparameters is often necessary to make the training process converge [11].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "[11] comes up with several empirical techniques that works well, including how to apply batch normalization, how the input should be normalized, and which activation function to use.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[9] generalizes the GAN training loss from Jensen-Shannon divergence to any f-divergence function.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "For example, the Least Square GAN [8] uses l2 loss function as the objective, which achieves faster training and improves generation quality.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "A related work to WGAN is Loss-Sensitive GAN [10], whose objective is to minimize the loss for real data and maximize it for the fake ones.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "It has been pointed out in the original GAN paper that GAN is similar to NCE [6] in that both of them use a binary logistic classification loss as the surrogate function for training of a generative model.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "In the same sense as GAN is related to NCE [5], this work can be seen as the Importance Sampling version of GAN [2].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "In the same sense as GAN is related to NCE [5], this work can be seen as the Importance Sampling version of GAN [2].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "In the biased Importance Sampling [3], the above is converted to the following biased estimation which can be calculated without knowing p:",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "As pointed out by [7] and [12], both Importance Sampling and NCE are training the underlying generative model with a classification surrogate.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "As pointed out by [7] and [12], both Importance Sampling and NCE are training the underlying generative model with a classification surrogate.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "4 Infinite batch size As pointed out by [3], biased Importance Sampling estimation converges to the real partition function when the number of samples in one batch goes to infinity.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "3 Balance of generator and discriminator It is claimed in [11] that manually balancing the number of iterations of the discriminator and generator is a bad idea.",
      "startOffset" : 58,
      "endOffset" : 62
    } ],
    "year" : 2017,
    "abstractText" : "Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The key idea of Softmax GAN is to replace the classification loss in the original GAN with a softmax cross-entropy loss in the sample space of one single batch. In the adversarial learning of N real training samples and M generated samples, the target of discriminator training is to distribute all the probability mass to the real samples, each with probability 1 M , and distribute zero probability to generated data. In the generator training phase, the target is to assign equal probability to all data points in the batch, each with probability 1 M+N . While the original GAN is closely related to Noise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance Sampling version of GAN. We futher demonstrate with experiments that this simple change stabilizes GAN training.",
    "creator" : "LaTeX with hyperref package"
  }
}