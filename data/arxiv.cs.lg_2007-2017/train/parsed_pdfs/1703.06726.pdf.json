{
  "name" : "1703.06726.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the effect of pooling on the geometry of representations",
    "authors" : [ "Gary Bécigneul" ],
    "emails" : [ "GARY.BECIGNEUL@INF.ETHZ.CH" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n06 72\n6v 1\n[ cs\n.L G\n] 2\n0 M\nar 2\nto yield disentangled representations without us understanding why, the most striking examples being perhaps convolutional neural networks and the ventral stream of the visual cortex in humans and primates. As for the latter, it was conjectured that representations may be disentangled by being flattened progressively and at a local scale (DiCarlo and Cox, 2007). An attempt at a formalization of the role of invariance in learning representations was made recently, being referred to as I-theory (Anselmi et al., 2013b). In this framework and using the language of differential geometry, we show that pooling over a group of transformations of the input contracts the metric and reduces its curvature, and provide quantitative bounds, in the aim of moving towards a theoretical understanding on how to disentangle representations. Keywords: Differential Geometry, I-theory, deep learning, pooling, disentangle, representation, curvature, group orbit"
    }, {
      "heading" : "1. Introduction",
      "text" : "What does disentangling representations mean? In machine learning and neurosciences, representations being tangled has two principal interpretations, and they are intimately connected with each other. The first one is geometrical: consider two sheets of paper of different colors, place one of the two on top of the other, and crumple them together in a paper ball; now, it may look difficult to separate the two sheets with a third one: they are tangled, one color sheet representing one class of a classification problem. The second one is analytical: consider a dataset being parametrized by a set of coordinates {xi}i∈I , such as images parametrized by pixels, and a classification task between two classes of images. On the one hand, we cannot find a subset {xi}i∈J with J ⊂ I of this coordinate system such that a variation of these would not change the class of an element, while still spanning a reasonable amount of different images of this class. On the other hand, we are likely to be capable of finding a large amount of transformations preserving the class of any image of the dataset, without being expressible as linear transformations on this coordinate system, and this is another way to interpret representations or factors of variation as being tangled.\nWhy is disentangling representations important? On the physiological side, the brains of humans and primates alike have been observed to solve object recognition tasks by progressively disentangling their representations via the visual stream, from V1 to the IT cortex (DiCarlo and Cox, 2007; DiCarlo et al., 2012). On the side of deep learning, deep convolutional neural networks are also able to disentangle highly tangled representations, since a softmax − which, geometrically, performs essentially a linear separation − computed on the representation of their last hidden layer\nc© 2017 G. Bécigneul.\ncan yield very good accuracy (Krizhevsky et al., 2012). Conversely, disentangling representations might be sufficient to pre-solve practically any task relevant to the observed data (Bengio, 2013).\nHow can we design algorithms in order to move towards more disentangled representations? Although it was conjectured that the visual stream might disentangle representations by flattening them locally, thus inducing a decrease in the curvature globally (DiCarlo and Cox, 2007), the mechanisms underlying such a disentanglement, whether it be for the brain or deep learning architectures, remain very poorly understood (DiCarlo and Cox, 2007; Bengio, 2013). However, it is now of common belief that computing representations that are invariant with respect to irrelevant transformations of the input data can help. Indeed, on the one hand, deep convolutional networks have been noticed to naturally learn more invariant features with deeper layers (Goodfellow et al., 2009; Lenc and Vedaldi, 2015; Tensmeyer and Martinez, 2016). On the other hand, the V1 part of the brain similarly achieves invariance to translations and rotations via a “pinwheels” structure, which can be seen as a principal fiber bundle (Petitot, 2003; Poggio et al., 2012). Conversely, enforcing a higher degree of invariance with respect to not only translations, but also rotations, flips, and other groups of transformation has been shown to achieve state-of-the-art results in various machine learning tasks (Bruna and Mallat, 2013; Gens and Domingos, 2014; Oyallon and Mallat, 2015; Dieleman et al., 2016; Cohen and Welling, 2016a,b), and is believed to help in linearizing small diffeomorphisms (Mallat, 2016). To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and Bölcskei, 2015) as well as I-theory (Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016). In particular, I-theory permits to use the whole apparatus of kernel theory to build invariant features (Mroueh et al., 2015; Raj et al., 2016).\nOur work builds a bridge between the idea that disentangling is a result of (i) a local decrease in the curvature of the representations, and (ii) building representations that are invariant to nuisance deformations, by proving that pooling over such groups of transformations results in a local decreasing of the curvature.\nWe start by providing some background material, after which we introduce our formal framework and theorems, which we then discuss in the case of the non-commutative group generated by translations and rotations."
    }, {
      "heading" : "2. Some background material",
      "text" : ""
    }, {
      "heading" : "2.1. Groups and geometry",
      "text" : "A group is a set G together with a map · : G×G → G such that: (i) ∀g, g′, g′′ ∈ G, g · (g′ · g′′) = (g · g′) · g′′, (ii) ∃e ∈ G, ∀g ∈ G, g · e = e · g = g, (iii) ∀g ∈ G, ∃g−1 ∈ G : g · g−1 = g−1 · g = e, where e is called the identity element. We write gg′ instead of g · g′ for simplicity. If, moreover, gg′ = g′g for all g, g′ ∈ G, then G is said to be commutative or abelian.\nA subgroup of G is a set H ⊂ G such that for all h, h′ ∈ H , hh′ ∈ H and h−1 ∈ H . A subgroup H of a group G is said to be normal in G if for all g ∈ G, gH = Hg, or equivalently, for all g ∈ G and h ∈ H , ghg−1 ∈ H . If G is abelian, then all of its subgroups are normal in G.\nA Lie group is a group which is also a smooth manifold, and such that its product law and inverse map are smooth with respect to its manifold structure. A Lie group is said to be locally compact if each of its element possesses a compact neighborhood. On every locally compact Lie group, one can define a Haar measure, which is a left-invariant, non-trivial Lebesgue measure on its Borel algebra, and is uniquely defined up to a positive scaling constant. If this Haar measure is also right-invariant, then the group is said to be unimodular. This Haar measure is always finite on compact sets, and strictly positive on non-empty open sets. Examples of unimodular Lie groups include in particular all abelian groups, compact groups, semi-simple Lie groups and connected nilpotent Lie groups.\nA group G is said to be acting on a set X if we have a map · : G × X → X such that for all g, g′ ∈ G, for all x ∈ X, g · (g′ · x) = (gg′) · x and e · x = x. If this map is also smooth, then we say that G is smoothly acting on X. We write gx instead of g · x for simplicity. Then, the group orbit of x ∈ X under the action of G is defined by G · x = {gx | g ∈ G}, and the stabilizer of x by Gx = {g ∈ G | gx = x}. Note that Gx is always a subgroup of G, and that for all x, y ∈ X, we have either (G ·x)∩ (G ·y) = ∅, orG ·x = G ·y. Hence, we can writeX as the disjoint union of its group orbits, i.e. there exists a minimal subset X̃ ⊂ X such that X = ⊔x∈X̃G · x. The set of orbits ofX under the action ofG is writtenX/G, and is in one-to-one correspondence with X̃. Moreover, note that ifH is a subgroup ofG, thenH is naturally acting on G via (h, g) ∈ H×G 7→ hg ∈ G; if we further assume that H is normal in G, then one can define a canonical group structure on G/H , thus turning the canonical projection g ∈ G 7→ H · g into a group morphism.\nA diffeomorphism between two manifolds is a map that is smooth, bijective and has a smooth inverse. A group morphism between two groups G and G′ is a map ϕ : G → G′ such that for all g1, g2 ∈ G, ϕ(g1g2) = ϕ(g1)ϕ(g2). A group isomorphism is a bijective group morphism, and a Lie group isomorphism is a group isomorphism that is also a diffeomorphism.\nThe Lie algebra g of a Lie group G is its tangent space at e, and is endorsed with a bilinear map [·, ·] : g × g → g called its Lie bracket, and such that for all x, y, z ∈ g, [x, y] = −[y, x] and [x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0. Moreover, there is a bijection between g and left-invariant vector fields on G, defined by ξ ∈ g 7→ {g ∈ G 7→ deLg(ξ)}, where Lg(h) = gh is the left translation. Finally, the flow t 7→ φt of such a left-invariant vector fieldXξ is given by φt(g) = g exp(tξ), where exp : g → G is the exponential map on G.\nFor more on Lie groups, Lie algebras, Lie brackets and group representations, see Kirillov (2008), and for a rapid and clear presentation of the notions of sectional curvature and Riemannian curvature, see Andrews and Hopper (2010).\n2.2. I-theory\nI-theory aims at understanding how to compute a representation of an image I that is both unique and invariant under some deformations of a groupG, and how to build such representations in a hierarchical way (Poggio et al., 2012; Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).\nSuppose that we are given a Hilbert space X , typically L2(R2), representing the space of images. LetG be a locally compact group acting on X . Then, note that the group orbit G ·I constitutes such an invariant and unique representation of I , as G · I = G · (gI), for all g ∈ G, and since two group orbits intersecting each other are equal.\nBut how can we compare such group orbits? For an image I ∈ X , define the map ΘI : g ∈ G 7→ gI ∈ X and the probability distribution PI(A) = µG(Θ−1I (A)) for any borel set A of X , where µG is the Haar measure on G. For I, I\n′ ∈ X , write I ∼ I ′ is there exists g ∈ G such that I = gI ′. Then, one can prove that I ∼ I ′ if and only if PI = PI′ . Hence, we could compare G · I andG·I ′ by comparing PI and PI′ . However, computing PI can be difficult, so one must be looking for ways to approximate PI . If t ∈ S(L2(R2)), define P〈I,t〉 to be the distribution associated with the random variable g 7→ 〈gI, t〉. One can then prove that PI = PI′ if and only if P〈I,t〉 = P〈I′,t〉 for all t ∈ S(L2(R2)), and then provide a lower bound on the sufficient numberK of such templates tk, 1 6 k 6 K , drawn uniformly on S(L2(R2)), in order to recover the information of PI up to some error ε and with high probability 1− δ. Finally, each P〈I,tk〉 can be approximated by a histogram\nhkn(I) = 1 |G| ∑\ng∈G\nηn(〈gI, tk〉),\nif G is finite or\nhkn(I) = 1\nµG(G)\n∫\ng∈G ηn(〈gI, tk〉)dµG(g),\nif G is compact, where ηn are various non-negative and possibly non-linear functions, 1 6 n 6 N , such as sigmoid, ReLU, modulus, hyperbolic tangent or x 7→ |x|p, among others.\nIn the case where the group G is only partially observable (for instance if G is only locally compact but not bounded), one can define instead a “partially invariant representation”, replacing each hkn(I) by 1\nµG(G0)\n∫\ng∈G0\nηn(〈gI, tk〉)dµG(g),\nwhere G0 is a compact subset of G which can be observed in practice. Under some “localization condition” (see (Anselmi et al., 2013b)), it can be proved that this representation is invariant under deformations by elements of G0. When this localization condition is not met, we do not have any exact invariance a priori, but one might expect that the variation in directions defined by the symmetries of G0 is going to be reduced.\nFor instance, let G be the group R2 of translations in the plane, G0 = [−a, a]2 for some a > 0, η : x 7→ (σ(x))2 where σ is a point-wise non-linearity commonly used in neural networks, and tk ∈ S(L2(R2)) for 1 6 k 6 K . Then, note that the quantities\n√ |G0|hk(I) = √ ∑\ng∈G0\nη(〈gI, tk〉),\nfor 1 6 k 6 K are actually computed by a 1-layer convolutional neural network with filters (tk)16k6K , non-linearity σ and L 2-pooling. Moreover, the family ( √ |G0|hk(gI))g∈G is exactly\nthe output of this convolutional layer, thus describing a direct correspondence between pooling and locally averaging over a group of transformations.\nAnother correspondence can be made between this framework and deep learning architectures. Indeed, assume that during learning, the set of filters of a layer of a convolutional neural network becomes stable under the action of some unknown group G acting on the pixel space, and denote by σ the point-wise non-linearity computed by the network. Moreover, suppose that the convolutional layer and point-wise non-linearity are followed by an Lp-pooling, defined by Πpφ(I)(x) = ( ∫ y∈R2 |I(y)1[0,a]2(x − y)|p dy )1/p . Then, observe that the convolutional layer outputs the following feature maps:\n{Πpφ(σ(I ⋆ tk))}16k6K .\nBesides, if the groupG has a unitary representation, and if its action preserves R2, then for all g ∈ G and 1 6 k 6 K , we have\nΠpφ(σ(gI ⋆ tk)) = Π p φ(σ(g(I ⋆ g −1tk))) = gΠ p φ(σ(I ⋆ g −1tk)).\nThen, the following layer of the convolutional network is going to compute the sum across channels k of these different signals. However, if our set of filters tk can be written as G0 · t for some filter t and a subpart G0 of G, then this sum will be closely related to a histogram as in I-theory:\n∑\ng∈G0\nΠpφ(σ(I ⋆ gt)) = ∑\ng∈G0\ngΠpφ(σ(g −1I ⋆ tk)).\nIn other words, (local) group invariances are free to appear during learning among filters of a convolutional neural network, and will naturally be pooled over by the next layer. For more on this, see (Bruna et al., 2013; Mallat, 2016).\nFinally, let’s mention that this implicit pooling over symmetries can also be computed explicitly, and such group invariances across filters enforced, if we know the group in advance, as in G-CNNs and steerable CNNs (Cohen and Welling, 2016a,b)."
    }, {
      "heading" : "3. Main results: formal framework and theorems",
      "text" : "Let G be a finite-dimensional, locally compact and unimodular Lie group smoothly acting on R2. This defines an action (Lgf)(x) = f(g\n−1x) on L2(R2). Let G0 be a compact neighborhood of the identity element e in G, and assume that there exists λ > 0 such that for all g0 ∈ G0, supx∈R2 |Jg0(x)| 6 λ, where Jg is the determinant of the Jacobian matrix of g seen as a diffeomorphism of R2. We define Φ : L2(R2) → L2(R2), the averaging operator on G0, by\nΦ(f) = 1\nµG(G0)\n∫\ng∈G0\nLgf dµG(g).\nOur first result describes how the euclidean distance in L2(R2) between a function f and its translation by some g ∈ G0 is contracted by this locally averaging operator.\nTheorem 1. For all f ∈ L2(R2), for all g ∈ G,\n‖Φ(Lgf)−Φ(f)‖2 6 √ λmax ( 1, √\n‖Jg‖ ∞\n)\nµG((G0g)∆G0)\nµG(G0) ‖f‖2.\nProof: See Appendix A.\nThe symbol ∆ above is defined A∆B = (A ∪ B) \\ (A ∩ B) = (A \\ B) ∪ (B \\ A). Note that, as one could have expected, this result doesn’t depend on the scaling constant of the Haar measure. Intuitively, this result formalizes the idea that locally averaging with respect to some factors of variation, or coordinates, will reduce the variation with respect to those coordinates. The following drawings illustrate the intuition behind Theorem 1, where we pass from left to right by applying Φ.\nNote that the quantity µG((G0g)∆G0)\nµG(G0) , depending on the geometry of the group, is likely to de-\ncrease when we increase the size of G0: if G = R 2 is the translation group, G0 = [0, a] 2 for some a > 0, and gε is the translation by the vector (ε, ε), then µG is just the usual Lebesgue measure in R 2 and\nµG((G0gε)∆G0)\nµG(G0) ∼ ε→0 2 2aε a2 = 4ε √ µG(G0) .\nIndeed, locally averaging over a wider area will decrease the variation even more.\nAs images are handily represented by functions from the space of pixels R2 to either R or C, let us define our dataset X to be a finite-dimensional manifold embedded in a bigger space of functions Y . As for technical reasons we will need our functions to be L2, smooth, and with a gradient having a fast decay at infinity, we choose Y to be the set of functions f ∈ L2(R2) ∩ C∞(R2) such that |〈∇f(x), x〉| = Ox→∞( 1‖x‖1+ε ), for some fixed small ε > 0. Note that in practice, images are only non-zero on a compact domain, therefore these assumptions are not restrictive.\nFurther assume that for all f ∈ X , for all g ∈ G, Lgf ∈ X . Intuitively, X is our manifold of images, and G corresponds to the group of transformations that are not relevant to the task at hand. Recall that from I-theory, the orbit of an image f under G constitutes a good unique and invariant\nrepresentation. Here, we are interested in comparing G ·f and Φ(G ·f), i.e. before and after locally averaging.\nBut how can we compute a bound on the curvature of Φ(G · f)? It is well known that in a Lie group endorsed with a bi-invariant pseudo-Riemannian metric 〈·, ·〉, the Riemann curvature tensor is given by\nR(X,Y,Z,W ) = −1 4 〈[X,Y ], [Z,W ]〉,\nwhere X,Y,Z,W are left-invariant vector-fields, and hence if (X,Y ) forms an orthonormal basis of the plane they span, then the sectional curvature is given by\nκ(X ∧ Y ) = R(X,Y, Y,X) = 1 4 〈[X,Y ], [X,Y ]〉.\nTherefore, would we be able to define a Lie group structure and a bi-invariant pseudo-Riemannian\nmetric on Φ(G ·f), we could use this formula to compute its curvature. First, we are going to define a Lie group structure on G · f , which we will then transport on Φ(G · f). As a Lie group structure is made of a smooth manifold structure and a compatible group structure, we need to construct both. In order to obtain the group structure on the orbit, let’s assume that the stabilizer Gf is normal; a condition that is met for instance if G is abelian, or if this subgroup is trivial, meaning that f does not have internal symmetries corresponding to those of G, which is only a technical condition, as it can be enforced in practice by slightly deforming f , by breaking the relevant symmetries with a small noise. Besides, in order to obtain a smooth manifold structure on the orbits, we need to assume that Gf is an embedded Lie subgroup of G, which, from proposition B.0 (see appendix), is met automatically when this group admits a finite-dimensional representation.\nThen, from proposition B.1, there is one and only one manifold structure on the topological quotient space G/Gf turning the canonical projection π : G → G/Gf into a smooth submersion; moreover, the action of G on G/Gf is smooth, G/Gf is a Lie group, π is a Lie group morphism, the Lie algebra gf of Gf is an ideal of the Lie algebra g of G and the linear map from TeG/TeGf to TeGf (G/Gf ) induced by Teπ is a Lie algebra isomorphism from g/gf to the Lie algebra of G/Gf .\nFinally, we need a geometrical assumption on the orbits, insuring that G is warped on G · f in a way that is not “fractal”, i.e. that this orbit can be given a smooth manifold structure: assume that G · f is locally closed in X . Using this assumption and proposition B.2, the canonical map Θf : G/Gf → X defined by Θf (gGf ) = Lgf is a one-to-one immersion, whose image is the orbit G ·f , which is a submanifold of X ; moreover, Θf is a diffeomorphism fromG/Gf toG ·f . Further notice that Θf is G-equivariant, i.e. for all g, g ′ ∈ G,\nΘf (g(g ′Gf )) = Lgg ′f = LgLg′f = LgΘf (g ′Gf ).\nMoreover, we can define on G · f a group law by\n(Lg1f) · (Lg2f) := Lg1g2f,\nfor g1, g2 ∈ G. Indeed, let’s prove that this definition doesn’t depend on the choice of g1, g2. Assume that gi = aibi for ai ∈ G and bi ∈ Gf , i ∈ {1, 2}. Then, as Gf is normal in G, there exists\nb′1 ∈ Gf such that b1a2 = a2b′1. Then g1g2 = a1a2b′1b2 and hence Lg1g2f = La1a2f , and this group law is well-defined. Now that G · f is a group, observe that Θf is a group isomorphism from G/Gf to G · f . Indeed, it is bijective since it is a diffeomorphism, and it is a group morphism as\nΘf ((gGf )(g ′Gf )) = Θf ((gg ′)Gf ) = Lgg′f = (Lgf) · (Lg′f) = Θf (gGf ) ·Θf (g′Gf ).\nHence, G ·f is also a Lie group, since G/Gf is a Lie group andΘf : G/Gf → G ·f is a diffeomorphism. Moreover, Lie(G · f) is isomorphic to g/gf as a Lie algebra, since they are isomorphic as vector spaces (Θf being an immersion), and by the fact that the pushforward of a diffeomorphism always preserves the Lie bracket.\nNow that we have defined a Lie group structure on G · f , how can we obtain one on Φ(G · f)? Suppose that Φ is injective on G · f and on Lie(G · f). We can thus define a group law on Φ(G · f) by: ∀g1, g2 ∈ G/Gf , Φ(Lg1f) · Φ(Lg2f) := Φ(Lg1g2f). As the inverse function theorem tells us that Φ is a diffeomorphism from G · f onto its image, Φ(G · f) is now endorsed with a Lie group structure. However, in order to carry out the relevant calculations, we still need to define left-invariant vector-fields on our Lie group orbits.\nFor all ξ ∈ g, define the following left-invariant vector-fields respectively on G ·f and Φ(G ·f):\nXξ : Lgf 7→ d\ndt |t=0 (LgLexp(tξ)f),\nX̃ξ : Φ(Lgf) 7→ d\ndt |t=0 Φ(LgLexp(tξ)f).\nWe can now state the following theorem:\nTheorem 2. For all f ∈ X , for all ξ, ξ′ ∈ g,\n‖[X̃ξ , X̃ξ′ ]Φ(f)‖22 6 λ [ d\nds |s=0\nµG((G0 exp(s[ξ, ξ ′]))∆G0)\nµG(G0)\n]2‖f‖22.\nProof: See Appendix A.\nAs X is a manifold embedded in L2(R2), it inherits a Riemannian metric by projection of the usual inner-product of L2(R2) on the tangent bundle of X . Moreover, if we further assume that for all g ∈ G, |Jg| = 1, then this Riemannian metric is bi-invariant, and we can finally use the above formula on the Riemannian curvature, together with the previous inequality, to compute a bound on the curvature in a Lie group endorsed with an bi-invariant metric:"
    }, {
      "heading" : "Corollary.",
      "text" : "For all f ∈ X , for all ξ, ξ′ ∈ g,\n0 6 RΦ(f)(X̃ξ, X̃ξ′ , X̃ξ′ , X̃ξ) 6 [1\n2\nd\nds |s=0\nµG((G0 exp(s[ξ, ξ ′]))∆G0)\nµG(G0)\n]2‖f‖22.\nAnd if (X̃ξ , X̃ξ′) forms an orthonormal basis of the plane they span in Lie(Φ(G · f)) = Φ(Lie(G · f)), then:\n0 6 κΦ(f)(X̃ξ ∧ X̃ξ′) 6 [1\n2\nd\nds |s=0\nµG((G0 exp(s[ξ, ξ ′]))∆G0)\nµG(G0)\n]2‖f‖22.\nRemark. The sectional curvature of the basis (X̃ξ, X̃ξ′) at Φ(f) is also the Gaussian curvature of the two-dimensional surface swept out by small geodesics induced by linear combinations of X̃ξ(Φ(f)) and X̃ξ′(Φ(f)).\nAmong well-known finite-dimensional, locally compact and unimodular Lie group smoothly acting on R2, there are the group R2 of translations, the compact groups O(2) and SO(2), the euclidean group E(2), as well as transvections, or shears. Moreover, another class of suitable unimodular Lie groups is given by the one-dimensional flows of Hamiltonian systems, which, as deformations of images, could be interpreted as the smooth evolutions of the screen in a video over time, provided that these evolutions can be expressed as group actions on the pixel space.\nFinally, let’s see what Theorem 2 gives us in the case G = R2 ⋉ SO(2). Note that this group is not commutative, and its curvature form is not identically zero. Let θ ∈ (−π, π), a > 0, and G0 = [−θ, θ]× [0, a]2. A representation of this group is given by matrices of the form\ng(θ, x, y) =\n\n cos(θ) − sin(θ) x sin(θ) cos(θ) y\n0 0 1\n\n ,\nand a representation of its Lie algebra is given by\nξ(ζ, x, y) =\n\n 0 −ζ x ζ 0 y 0 0 0\n\n .\nThe Lie bracket is then given by\n[ξ(ζ, x, y), ξ(ζ ′, x′, y′)] = ξ(ζ, x, y)ξ(ζ ′, x′, y′)−ξ(ζ ′, x′, y′)ξ(ζ, x, y) = ξ(0, ζ ′y−ζy′, ζx′−ζ ′x).\nAs the exponential map on the group of translations is the identity map, and as the Haar measure on R 2 ⋉ SO(2) is just the product of the Haar measures on R2 and SO(2), we have\nµG((G0 exp(s[ξ(ζ, x, y), ξ(ζ ′, x′, y′]))∆G0) =\n2θµR2(([s(ζ ′y − ζy′), s(ζ ′y − ζy′) + a]× [s(ζx′ − ζ ′x), s(ζx′ − ζ ′x) + a])∆[0, a]2),\nand µG(G0) = 2θa 2. Therefore, when s → 0, we have\nµG((G0 exp(s[ξ(ζ, x, y), ξ(ζ ′, x′, y′]))∆G0) ∼\n2θ × 2(as(ζ ′y − ζy′) + as(ζx′ − ζ ′x)) = 4θas(ζ(x′ − y′)− ζ ′(x− y)),\nfrom what we deduce that\n[1\n2\nd\nds |s=0\nµG((G0 exp(s[ξ(ζ, x, y), ξ(ζ ′, x′, y′]))∆G0)\nµG(G0)\n]2 = (ζ(x′ − y′)− ζ ′(x− y))2 a2 .\nAs a consequence, if f ∈ X is an image in our dataset, of L2-norm equal to 1, and if we choose ξ(ζ, x, y) and ξ(ζ ′, x′, y′) such that the L2 functions X̃ξ(ζ,x,y)(Φ(f)) and X̃ξ(ζ′,x′,y′)(Φ(f)) are orthogonal in L2 and have L2-norm equal to 1, then the Gaussian curvature κ of the 2-dimensional surface swept out by these two vector fields around Φ(f), in the Lie group Φ(G ·f), is smaller than:\nκ 6 (ζ(x′ − y′)− ζ ′(x− y))2\na2 ."
    }, {
      "heading" : "4. Conclusion",
      "text" : "Being able to disentangle highly tangled representations is a very important and challenging problem in machine learning. In deep learning in particular, there exist successful algorithms that may disentangle highly tangled representations in some situtations, without us understanding why. Similarly, the ventral stream of the visual cortex in humans and primates seems to perform such a disentanglement of representations, but, again, the reasons behind this process are difficult to understand. It is believed that making representations invariant to some nuisance deformations, as well as locally flattening them, might help or even be an essential part of the disentangling process. As shown by our theorems, there is a connection between these two intuitions, in the sense that achieving a higher degree of invariance with respect to some group transformations will flatten the representations in directions of the tangent space corresponding to the Lie algebra generators of these transformations. Using our theorems, we showed that in the case of the group of positive affine isometries, a precise bound on the sectional curvature can be computed, with respect to the pooling parameters. We hope that this work will encourage the geometrical study of how representations evolve during learning, in function of the hyperparameters of the algorithm that is used on these representations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Simon Janin and Victor Godet for interesting conversations."
    }, {
      "heading" : "Appendix A. Proofs of Theorem 1, Theorem 2 and Lemma",
      "text" : "Theorem 1. For all f ∈ L2(R2), for all g ∈ G,\n‖Φ(Lgf)− Φ(f)‖2 6 √ λmax(1, √\n‖Jg‖ ∞ ) µG((G0g)∆G0) µG(G0) ‖f‖2."
    }, {
      "heading" : "Proof:",
      "text" : "We have\nµG(G0) 2‖Φ(Lgf)− Φ(f)‖22 =\n∫\nx∈R2\n(\n∫\ng′∈G0\nLg′gf(x)− Lg′f(x)dµG(g′) )2 dx,\nbut ∫\ng′∈G0\n(Lg′gf(x)− Lg′f(x))dµG(g′) = ∫\ng′∈G0\nLg′gf(x)dµG(g ′)−\n∫\ng′∈G0\nLg′f(x)dµG(g ′),\ni.e. setting g′′ = g′g and using the right-invariance of µG,\n∫\ng′∈G0\n(Lg′gf(x)− Lg′f(x))dµG(g′) = ∫\ng′′∈G0g Lg′′f(x)dµG(g\n′′)− ∫\ng′∈G0\nLg′f(x)dµG(g ′).\nAnd using ∫ A h− ∫ B h = ( ∫ A\\B h+ ∫ A∩B h)− ( ∫ B\\A h+ ∫ B∩A h) = ∫ A\\B h− ∫ B\\A h, we have\n∫\ng′∈G0\n(Lg′gf(x)−Lg′f(x))dµG(g′) = ∫\ng′∈G0g\\G0\nLg′f(x)dµG(g ′)−\n∫\ng′∈G0\\G0g Lg′f(x)dµG(g\n′).\nPlugging this in the first equation gives\nµG(G0)‖Φ(Lgf)− Φ(f)‖2 = ‖ ∫\ng′∈G0g\\G0\n(Lg′f)dµG(g ′)−\n∫\ng′∈G0\\G0g (Lg′f)dµG(g\n′)‖2,\ni.e. using a triangle inequality\nµG(G0)‖Φ(Lgf)− Φ(f)‖2 6 ‖ ∫\ng′∈G0g\\G0\n(Lg′f)dµG(g ′)‖2 + ‖\n∫\ng′∈G0\\G0g (Lg′f)dµG(g\n′)‖2.\nNow observe that by interverting the integrals using Fubini’s theorem,\n‖ ∫\ng′∈G0\\G0g (Lg′f)dµG(g\n′)‖2 = √ ∫\ng1∈G0\\G0g\n∫\ng2∈G0\\G0g\n(\n∫\nx∈R2 (Lg1f)(x)(Lg2f)(x)dx\n)\ndµG(g1)dµG(g2),\nand using a Cauchy-Schwarz inequality,\n‖ ∫\ng′∈G0\\G0g (Lg′f)dµG(g\n′)‖2 6 √ ∫\ng1∈G0\\G0g\n∫\ng2∈G0\\G0g ‖Lg1f‖2‖Lg2f‖2dµG(g1)dµG(g2).\nAs for all g′ ∈ G0 we have ‖Lg′f‖2 = ‖f √ |Jg′ |‖2 6 √ λ‖f‖2 with a change of variables, we have\n‖ ∫\ng′∈G0\\G0g (Lg′f)dµG(g\n′)‖2 6 √ λµG(G0 \\ (G0g))‖f‖2.\nFor the other term, note that by setting g′′ = g′g−1, we have\n‖ ∫\ng′∈G0g\\G0\n(Lg′f)dµG(g ′)‖2 = ‖\n∫\ng′′∈G0\\G0g−1 (Lg′′gf)dµG(g\n′′)‖2 = ‖ ∫\ng′′∈G0\\G0g−1 (Lg′′Lgf)dµG(g\n′′)‖2,\nand then similarly,\n‖ ∫\ng′′∈G0\\G0g−1 (Lg′′Lgf)dµG(g\n′′)‖2 = √ ∫\ng1∈G0\\G0g−1\n∫\ng2∈G0\\G0g−1 ‖Lg1Lgf‖2‖Lg2Lgf‖2dµG(g1)dµG(g2).\nAs for all g′ ∈ G0 we have ‖Lg′Lgf‖2 = ‖f √ |Jg′g|‖2 6 √ λ‖Jg‖∞‖f‖2, we have\n‖ ∫\ng′∈G0g\\G0\n(Lg′f)dµG(g ′)‖2 6\n√\nλ‖Jg‖ ∞ µG(G0 \\ (G0g−1))‖f‖2.\nTherefore\nµG(G0)‖Φ(Lgf)− Φ(f)‖2 6 √\nλ‖Jg‖ ∞ µG(G0 \\ (G0g−1))‖f‖2 +\n√ λµG(G0 \\ (G0g))‖f‖2,\nand the following fact concludes the proof:\nµG(G0 \\ (G0g−1)) + µG(G0 \\ (G0g)) = µG((G0g) \\G0) + µG(G0 \\ (G0g)) = µG((G0g)∆G0).\nTheorem 2. For all f ∈ X , for all ξ, ξ′ ∈ g,\n‖[X̃ξ , X̃ξ′ ]Φ(f)‖22 6 λ [ d\nds |s=0\nµG((G0 exp(s[ξ, ξ ′]))∆G0)\nµG(G0)\n]2‖f‖22."
    }, {
      "heading" : "Proof:",
      "text" : "As Φ realizes a diffeomorphism from G · f onto its image, and as Φ equals its differential from Lemma, we have that for all vector field X on G · f , Φ∗(X)(Φ(f)) = (dΦ)f (X(f)) = Φ(X(f)). Hence\n[X̃ξ, X̃ξ′ ]Φ(f) = [Φ(Xξ),Φ(Xξ′)]f\n= [Φ(Xξ) ◦Φ−1,Φ(Xξ′) ◦Φ−1]Φ(f) = [Φ∗(Xξ),Φ∗(Xξ′)]Φ(f)\n= Φ∗([Xξ ,Xξ′ ])(Φ(f))\n= Φ([Xξ ,Xξ′ ]f ).\nRecall that the Lie bracket of left-invariant vector fields is given by the opposite of the Lie bracket of their corresponding generators, hence in our case:\n[Xξ,Xξ′ ] = X−[ξ,ξ′] = −X[ξ,ξ′].\nTherefore,\n‖[X̃ξ , X̃ξ′ ]Φ(f)‖2 = ‖Φ([Xξ ,Xξ′ ]f )‖2 = ‖Φ(X[ξ,ξ′](f))‖2\n= ‖Φ(lim t→0\n1 t (Lexp(t[ξ,ξ′])f − f))‖2\n= ‖ lim t→0\n1\nt\n( Φ(Lexp(t[ξ,ξ′])f)− Φ(f) ) ‖2.\nFrom Theorem 1, we have\n‖Φ(Lexp(t[ξ,ξ′])f)− Φ(f)‖2 6 √ λmax(1, √\n‖Jexp(t[ξ,ξ′])‖ ∞ ) µG((G0 exp(t[ξ, ξ\n′]))∆G0)\nµG(G0) ‖f‖2.\nAs exp(t[ξ, ξ′]) → e when t → 0, its Jacobian goes to 1. Moreover, as f has a gradient with fast decay, we can take the limit out of the L2-norm, which concludes the proof."
    }, {
      "heading" : "Lemma.",
      "text" : "For all f ∈ X and ξ ∈ g,\nd\ndt |t=0 Φ(Lexp(tξ)f) = Φ\n( d\ndt |t=0 (Lexp(tξ)f)\n)\n."
    }, {
      "heading" : "Proof:",
      "text" : "For all x ∈ R2, ( d\ndt |t=0 Φ(Lexp(tξ)f)\n) (x) = d\ndt\n( 1\nµG(G0)\n∫\ng′∈G0\n(Lg′Lexp(tξ)f)(x)dµG0(g ′) )\n|t=0\n= 1\nµG(G0)\n∫\ng′∈G0\nd\ndt\n( f(exp(−tξ)g′−1x) )\n|t=0 dµG0(g\n′)\n= 1\nµG(G0)\n∫\ng′∈G0\nd(g′−1x)f ( − ξ(g′−1x) ) dµG0(g ′)\n= Φ ( d·f ( − ξ(·) )) (x) = Φ ( d\ndt |t=0 (Lexp(tξ)f)\n)\n(x)."
    }, {
      "heading" : "Appendix B. Supplementary material",
      "text" : "The next three propositions are taken from the publicly available french textbook Paulin (2014), in which they’re respectively numbered as E.7, 1.60, 1.62.\nProposition B.0 Let G be a Lie group and ρ : G → GL(V ) a finite-dimensional Lie group representation of G. Then for all v ∈ V , the map defined by g ∈ G 7→ ρ(g)v has constant rank, and the stabilizer Gv is an embedded Lie subgroup of G.\nProposition B.1. Let G be a Lie group, H be an embedded Lie subgroup of G, and π : G → G/H be the canonical projection. There exists one and only one smooth manifold structure on the topological quotient space G/H turning π into a smooth submersion. Moreover, the action of G on G/H is smooth, and if H is normal in G, then G/H is a Lie group, π is a Lie group morphism, the Lie algebra h of H is an ideal of the Lie algebra g of G and the linear map from TeG/TeH to TeH(G/H) induced by Teπ is a Lie algebra isomorphism from g/h to the Lie algebra of G/H .\nProposition B.2. LetM be a manifold together with a smooth action of a Lie group G, and x ∈ M ; (i) the canonical map Θx : G/Gx → M defined by Θx(gGx) = gx is a one-to-one immersion, whose image is the orbit G · x; (ii) the orbit G · x is a submanifold of M if and only if it is locally closed inM ; (iii) if G · x is locally closed, then Θx is a diffeomorphism from G/Gx to G · x."
    } ],
    "references" : [ {
      "title" : "The Ricci flow in Riemannian geometry: a complete proof of the differentiable 1/4-pinching sphere",
      "author" : [ "Ben Andrews", "Christopher Hopper" ],
      "venue" : null,
      "citeRegEx" : "Andrews and Hopper.,? \\Q2010\\E",
      "shortCiteRegEx" : "Andrews and Hopper.",
      "year" : 2010
    }, {
      "title" : "Representation learning in sensory cortex: a theory",
      "author" : [ "Fabio Anselmi", "Tomaso Poggio" ],
      "venue" : "Technical report, Center for Brains, Minds and Machines (CBMM),",
      "citeRegEx" : "Anselmi and Poggio.,? \\Q2014\\E",
      "shortCiteRegEx" : "Anselmi and Poggio.",
      "year" : 2014
    }, {
      "title" : "Magic materials: a theory of deep hierarchical architectures for learning sensory representations",
      "author" : [ "Fabio Anselmi", "Joel Z Leibo", "Lorenzo Rosasco", "Jim Mutch", "Andrea Tacchetti", "Tomaso Poggio" ],
      "venue" : "CBCL paper,",
      "citeRegEx" : "Anselmi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Anselmi et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised learning of invariant representations in hierarchical architectures",
      "author" : [ "Fabio Anselmi", "Joel Z Leibo", "Lorenzo Rosasco", "Jim Mutch", "Andrea Tacchetti", "Tomaso Poggio" ],
      "venue" : "arXiv preprint arXiv:1311.4158,",
      "citeRegEx" : "Anselmi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Anselmi et al\\.",
      "year" : 2013
    }, {
      "title" : "On invariance and selectivity in representation learning",
      "author" : [ "Fabio Anselmi", "Lorenzo Rosasco", "Tomaso Poggio" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "Anselmi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Anselmi et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding deep features with computer-generated imagery",
      "author" : [ "Mathieu Aubry", "Bryan C Russell" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Aubry and Russell.,? \\Q2015\\E",
      "shortCiteRegEx" : "Aubry and Russell.",
      "year" : 2015
    }, {
      "title" : "Deep learning of representations: Looking forward",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "In International Conference on Statistical Language and Speech Processing,",
      "citeRegEx" : "Bengio.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2013
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Yoshua Bengio", "Aaron Courville", "Pascal Vincent" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Invariant scattering convolution networks",
      "author" : [ "Joan Bruna", "Stéphane Mallat" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "Bruna and Mallat.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bruna and Mallat.",
      "year" : 2013
    }, {
      "title" : "Learning stable group invariant representations with convolutional networks",
      "author" : [ "Joan Bruna", "Arthur Szlam", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1301.3537,",
      "citeRegEx" : "Bruna et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bruna et al\\.",
      "year" : 2013
    }, {
      "title" : "Transformation properties of learned visual representations",
      "author" : [ "Taco S Cohen", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1412.7659,",
      "citeRegEx" : "Cohen and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cohen and Welling.",
      "year" : 2014
    }, {
      "title" : "Group equivariant convolutional networks",
      "author" : [ "Taco S Cohen", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1602.07576,",
      "citeRegEx" : "Cohen and Welling.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cohen and Welling.",
      "year" : 2016
    }, {
      "title" : "Steerable cnns",
      "author" : [ "Taco S Cohen", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1612.08498,",
      "citeRegEx" : "Cohen and Welling.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cohen and Welling.",
      "year" : 2016
    }, {
      "title" : "Untangling invariant object recognition",
      "author" : [ "James J DiCarlo", "David D Cox" ],
      "venue" : "Trends in cognitive sciences,",
      "citeRegEx" : "DiCarlo and Cox.,? \\Q2007\\E",
      "shortCiteRegEx" : "DiCarlo and Cox.",
      "year" : 2007
    }, {
      "title" : "How does the brain solve visual object recognition? Neuron",
      "author" : [ "James J DiCarlo", "Davide Zoccolan", "Nicole C Rust" ],
      "venue" : null,
      "citeRegEx" : "DiCarlo et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "DiCarlo et al\\.",
      "year" : 2012
    }, {
      "title" : "Exploiting cyclic symmetry in convolutional neural networks",
      "author" : [ "Sander Dieleman", "Jeffrey De Fauw", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1602.02660,",
      "citeRegEx" : "Dieleman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dieleman et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep symmetry networks",
      "author" : [ "Robert Gens", "Pedro M Domingos" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Gens and Domingos.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gens and Domingos.",
      "year" : 2014
    }, {
      "title" : "Measuring invariances in deep networks. In Advances in neural information processing",
      "author" : [ "Ian Goodfellow", "Honglak Lee", "Quoc V Le", "Andrew Saxe", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2009
    }, {
      "title" : "An introduction to Lie groups and Lie algebras, volume 113",
      "author" : [ "Alexander Kirillov" ],
      "venue" : null,
      "citeRegEx" : "Kirillov.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kirillov.",
      "year" : 2008
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Understanding image representations by measuring their equivariance and equivalence",
      "author" : [ "Karel Lenc", "Andrea Vedaldi" ],
      "venue" : "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "citeRegEx" : "Lenc and Vedaldi.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lenc and Vedaldi.",
      "year" : 2015
    }, {
      "title" : "Group invariant scattering",
      "author" : [ "Stéphane Mallat" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Mallat.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mallat.",
      "year" : 2012
    }, {
      "title" : "Understanding deep convolutional networks",
      "author" : [ "Stéphane Mallat" ],
      "venue" : "Phil. Trans. R. Soc. A,",
      "citeRegEx" : "Mallat.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mallat.",
      "year" : 2015
    }, {
      "title" : "Learning with group invariant features: A kernel perspective",
      "author" : [ "Youssef Mroueh", "Stephen Voinea", "Tomaso A Poggio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mroueh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mroueh et al\\.",
      "year" : 2015
    }, {
      "title" : "Differential geometry and statistics, volume 48",
      "author" : [ "Michael K Murray", "John W Rice" ],
      "venue" : null,
      "citeRegEx" : "Murray and Rice.,? \\Q1993\\E",
      "shortCiteRegEx" : "Murray and Rice.",
      "year" : 1993
    }, {
      "title" : "Deep roto-translation scattering for object classification",
      "author" : [ "Edouard Oyallon", "Stéphane Mallat" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Oyallon and Mallat.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oyallon and Mallat.",
      "year" : 2015
    }, {
      "title" : "Groupes et géométrie",
      "author" : [ "Frédéric Paulin" ],
      "venue" : "Notes de cours,",
      "citeRegEx" : "Paulin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Paulin.",
      "year" : 2014
    }, {
      "title" : "The neurogeometry of pinwheels as a sub-riemannian contact structure",
      "author" : [ "Jean Petitot" ],
      "venue" : "Journal of Physiology-Paris,",
      "citeRegEx" : "Petitot.,? \\Q2003\\E",
      "shortCiteRegEx" : "Petitot.",
      "year" : 2003
    }, {
      "title" : "Neurogéométrie de la vision",
      "author" : [ "Jean Petitot" ],
      "venue" : "Modeles mathématiques et physiques des architectures fonctionelles. Paris: Éd. École Polytech,",
      "citeRegEx" : "Petitot.,? \\Q2008\\E",
      "shortCiteRegEx" : "Petitot.",
      "year" : 2008
    }, {
      "title" : "The computational magic of the ventral stream: sketch of a theory (and why some deep architectures",
      "author" : [ "Tomaso Poggio", "JimMutch", "Joel Leibo", "Lorenzo Rosasco", "Andrea Tacchetti" ],
      "venue" : null,
      "citeRegEx" : "Poggio et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Poggio et al\\.",
      "year" : 2012
    }, {
      "title" : "Local group invariant representations via orbit embeddings",
      "author" : [ "Anant Raj", "Abhishek Kumar", "Youssef Mroueh", "P Thomas Fletcher" ],
      "venue" : null,
      "citeRegEx" : "Raj et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Raj et al\\.",
      "year" : 1988
    }, {
      "title" : "Ecole polytechnique, cmap phd thesis rigid-motion scattering for image classification",
      "author" : [ "Laurent Sifre", "Stphane Mallat" ],
      "venue" : null,
      "citeRegEx" : "Sifre and Mallat.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sifre and Mallat.",
      "year" : 2014
    }, {
      "title" : "comprehensive introduction to differential geometry",
      "author" : [ "Michael Spivak" ],
      "venue" : "vol. iv.[a]",
      "citeRegEx" : "Spivak.,? \\Q1981\\E",
      "shortCiteRegEx" : "Spivak.",
      "year" : 1981
    }, {
      "title" : "Improving invariance and equivariance properties of convolutional neural networks",
      "author" : [ "Christopher Tensmeyer", "Tony Martinez" ],
      "venue" : null,
      "citeRegEx" : "Tensmeyer and Martinez.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tensmeyer and Martinez.",
      "year" : 2016
    }, {
      "title" : "A mathematical theory of deep convolutional neural networks for feature extraction",
      "author" : [ "Thomas Wiatowski", "Helmut Bölcskei" ],
      "venue" : "arXiv preprint arXiv:1512.06293,",
      "citeRegEx" : "Wiatowski and Bölcskei.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wiatowski and Bölcskei.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "As for the latter, it was conjectured that representations may be disentangled by being flattened progressively and at a local scale (DiCarlo and Cox, 2007).",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 13,
      "context" : "Why is disentangling representations important? On the physiological side, the brains of humans and primates alike have been observed to solve object recognition tasks by progressively disentangling their representations via the visual stream, from V1 to the IT cortex (DiCarlo and Cox, 2007; DiCarlo et al., 2012).",
      "startOffset" : 269,
      "endOffset" : 314
    }, {
      "referenceID" : 14,
      "context" : "Why is disentangling representations important? On the physiological side, the brains of humans and primates alike have been observed to solve object recognition tasks by progressively disentangling their representations via the visual stream, from V1 to the IT cortex (DiCarlo and Cox, 2007; DiCarlo et al., 2012).",
      "startOffset" : 269,
      "endOffset" : 314
    }, {
      "referenceID" : 19,
      "context" : "can yield very good accuracy (Krizhevsky et al., 2012).",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "Conversely, disentangling representations might be sufficient to pre-solve practically any task relevant to the observed data (Bengio, 2013).",
      "startOffset" : 126,
      "endOffset" : 140
    }, {
      "referenceID" : 13,
      "context" : "How can we design algorithms in order to move towards more disentangled representations? Although it was conjectured that the visual stream might disentangle representations by flattening them locally, thus inducing a decrease in the curvature globally (DiCarlo and Cox, 2007), the mechanisms underlying such a disentanglement, whether it be for the brain or deep learning architectures, remain very poorly understood (DiCarlo and Cox, 2007; Bengio, 2013).",
      "startOffset" : 253,
      "endOffset" : 276
    }, {
      "referenceID" : 13,
      "context" : "How can we design algorithms in order to move towards more disentangled representations? Although it was conjectured that the visual stream might disentangle representations by flattening them locally, thus inducing a decrease in the curvature globally (DiCarlo and Cox, 2007), the mechanisms underlying such a disentanglement, whether it be for the brain or deep learning architectures, remain very poorly understood (DiCarlo and Cox, 2007; Bengio, 2013).",
      "startOffset" : 418,
      "endOffset" : 455
    }, {
      "referenceID" : 6,
      "context" : "How can we design algorithms in order to move towards more disentangled representations? Although it was conjectured that the visual stream might disentangle representations by flattening them locally, thus inducing a decrease in the curvature globally (DiCarlo and Cox, 2007), the mechanisms underlying such a disentanglement, whether it be for the brain or deep learning architectures, remain very poorly understood (DiCarlo and Cox, 2007; Bengio, 2013).",
      "startOffset" : 418,
      "endOffset" : 455
    }, {
      "referenceID" : 17,
      "context" : "Indeed, on the one hand, deep convolutional networks have been noticed to naturally learn more invariant features with deeper layers (Goodfellow et al., 2009; Lenc and Vedaldi, 2015; Tensmeyer and Martinez, 2016).",
      "startOffset" : 133,
      "endOffset" : 212
    }, {
      "referenceID" : 20,
      "context" : "Indeed, on the one hand, deep convolutional networks have been noticed to naturally learn more invariant features with deeper layers (Goodfellow et al., 2009; Lenc and Vedaldi, 2015; Tensmeyer and Martinez, 2016).",
      "startOffset" : 133,
      "endOffset" : 212
    }, {
      "referenceID" : 33,
      "context" : "Indeed, on the one hand, deep convolutional networks have been noticed to naturally learn more invariant features with deeper layers (Goodfellow et al., 2009; Lenc and Vedaldi, 2015; Tensmeyer and Martinez, 2016).",
      "startOffset" : 133,
      "endOffset" : 212
    }, {
      "referenceID" : 27,
      "context" : "On the other hand, the V1 part of the brain similarly achieves invariance to translations and rotations via a “pinwheels” structure, which can be seen as a principal fiber bundle (Petitot, 2003; Poggio et al., 2012).",
      "startOffset" : 179,
      "endOffset" : 215
    }, {
      "referenceID" : 29,
      "context" : "On the other hand, the V1 part of the brain similarly achieves invariance to translations and rotations via a “pinwheels” structure, which can be seen as a principal fiber bundle (Petitot, 2003; Poggio et al., 2012).",
      "startOffset" : 179,
      "endOffset" : 215
    }, {
      "referenceID" : 21,
      "context" : "To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and Bölcskei, 2015) as well as I-theory (Anselmi et al.",
      "startOffset" : 120,
      "endOffset" : 164
    }, {
      "referenceID" : 34,
      "context" : "To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and Bölcskei, 2015) as well as I-theory (Anselmi et al.",
      "startOffset" : 120,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and Bölcskei, 2015) as well as I-theory (Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).",
      "startOffset" : 185,
      "endOffset" : 258
    }, {
      "referenceID" : 4,
      "context" : "To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and Bölcskei, 2015) as well as I-theory (Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).",
      "startOffset" : 185,
      "endOffset" : 258
    }, {
      "referenceID" : 23,
      "context" : "In particular, I-theory permits to use the whole apparatus of kernel theory to build invariant features (Mroueh et al., 2015; Raj et al., 2016).",
      "startOffset" : 104,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "I-theory I-theory aims at understanding how to compute a representation of an image I that is both unique and invariant under some deformations of a groupG, and how to build such representations in a hierarchical way (Poggio et al., 2012; Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).",
      "startOffset" : 217,
      "endOffset" : 311
    }, {
      "referenceID" : 1,
      "context" : "I-theory I-theory aims at understanding how to compute a representation of an image I that is both unique and invariant under some deformations of a groupG, and how to build such representations in a hierarchical way (Poggio et al., 2012; Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).",
      "startOffset" : 217,
      "endOffset" : 311
    }, {
      "referenceID" : 4,
      "context" : "I-theory I-theory aims at understanding how to compute a representation of an image I that is both unique and invariant under some deformations of a groupG, and how to build such representations in a hierarchical way (Poggio et al., 2012; Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).",
      "startOffset" : 217,
      "endOffset" : 311
    }, {
      "referenceID" : 13,
      "context" : "For more on Lie groups, Lie algebras, Lie brackets and group representations, see Kirillov (2008), and for a rapid and clear presentation of the notions of sectional curvature and Riemannian curvature, see Andrews and Hopper (2010).",
      "startOffset" : 82,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "For more on Lie groups, Lie algebras, Lie brackets and group representations, see Kirillov (2008), and for a rapid and clear presentation of the notions of sectional curvature and Riemannian curvature, see Andrews and Hopper (2010). 2.",
      "startOffset" : 206,
      "endOffset" : 232
    }, {
      "referenceID" : 9,
      "context" : "For more on this, see (Bruna et al., 2013; Mallat, 2016).",
      "startOffset" : 22,
      "endOffset" : 56
    } ],
    "year" : 2017,
    "abstractText" : "In machine learning and neuroscience, certain computational structures and algorithms are known to yield disentangled representations without us understanding why, the most striking examples being perhaps convolutional neural networks and the ventral stream of the visual cortex in humans and primates. As for the latter, it was conjectured that representations may be disentangled by being flattened progressively and at a local scale (DiCarlo and Cox, 2007). An attempt at a formalization of the role of invariance in learning representations was made recently, being referred to as I-theory (Anselmi et al., 2013b). In this framework and using the language of differential geometry, we show that pooling over a group of transformations of the input contracts the metric and reduces its curvature, and provide quantitative bounds, in the aim of moving towards a theoretical understanding on how to disentangle representations.",
    "creator" : "LaTeX with hyperref package"
  }
}