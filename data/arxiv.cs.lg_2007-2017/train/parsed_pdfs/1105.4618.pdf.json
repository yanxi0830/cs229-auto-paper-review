{
  "name" : "1105.4618.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Haoyang Duan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We begin this report by describing the Probably Approximately Correct (PAC) model for learning a concept class, consisting of subsets of a domain, and a function class, consisting of functions from the domain to the unit interval. Two combinatorial parameters, the Vapnik-Chervonenkis (VC) dimension and its generalization, the Fat Shattering dimension of scale ǫ, are explained and a few examples of their calculations are given with proofs. We then explain Sauer’s Lemma, which involves the VC dimension and is used to prove the equivalence of a concept class being distributionfree PAC learnable and it having finite VC dimension.\nAs the main new result of our research, we explore the construction of a new function class, obtained by forming compositions with a continuous logic connective, a uniformly continuous function from the unit hypercube to the unit interval, from a collection of function classes. Vidyasagar had proved that such a composition function class has finite Fat Shattering dimension of all scales if the classes in the original collection do; however, no estimates of the dimension were known. Using results by Mendelson-Vershynin and Talagrand, we bound the Fat Shattering dimension of scale ǫ of this new function class in terms of the Fat Shattering dimensions of the collection’s classes.\nWe conclude this report by providing a few open questions and future research topics involving the PAC learning model.\nContents"
    }, {
      "heading" : "1 Introduction 2",
      "text" : ""
    }, {
      "heading" : "2 Brief Overview of Analysis and Measure Theory 3",
      "text" : ""
    }, {
      "heading" : "3 The Probably Approximately Correct Learning Model 7",
      "text" : ""
    }, {
      "heading" : "4 The Vapnik-Chervonenkis Dimension 11",
      "text" : "4.1 Sauer’s Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.2 Characterization of concept class distribution-free PAC learning . . . 14"
    }, {
      "heading" : "5 The Fat Shattering Dimension 15",
      "text" : "5.1 Sufficient condition for function class distribution-free PAC learning . 17"
    }, {
      "heading" : "6 The Fat Shattering Dimension of a Composition Function Class 19",
      "text" : "6.1 Construction in the context of concept classes . . . . . . . . . . . . . 19 6.2 Construction of new function class with continuous logic connective . 20 6.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 6.4 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
    }, {
      "heading" : "7 Open Questions 29",
      "text" : ""
    }, {
      "heading" : "8 Conclusion 31",
      "text" : "References 32"
    }, {
      "heading" : "1 Introduction",
      "text" : "In the area of statistical learning theory, the Probably Approximately Correct (PAC) learning model formalizes the notion of learning by using sample data points to produce valid hypotheses through algorithms. For instance, the following illustrates one learning problem which can be formalized in the PAC model. Given that there is a disease which affects certain people and out of 100 people in a hospital, 12 of them are sick with this disease. Is there a way to predict whether any given person in the hospital has the disease or not?\nThis report covers the PAC learning model applied to learning a collection of subsets C, called a concept class, of a domain X and more generally, a collection of functions F , called a function class, from X to the unit interval [0, 1]. The report involves mostly concepts from analysis and some concepts from probability theory, but only the completion of the first two years of undergraduate studies in mathematics are assumed from the readers.\nReport outline\nFirst, we give two definitions of PAC learning, one for a concept class C and the other for a function class F , and explore two combinatorial parameters, the VapnikChervonenkis (VC) dimension and the Fat Shattering dimension of scale ǫ, for C and F , respectively. Then, we explain Sauer’s Lemma, a theorem which involves the VC dimension of C and is used to prove that the finiteness of this dimension is a sufficient condition for C to be learnable.\nFinally, as the main new result of our research, given function classes F1, . . . ,Fk and a “continuous logic connective” (that is, a continuous function u : [0, 1]k → [0, 1]), we consider the construction of a new composition function class u(F1, . . . ,Fk), consisting of functions u(f1, . . . , fk) defined by u(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x)) for fi ∈ Fi. We then bound the Fat Shattering dimension of scale ǫ of this class in terms of a sum of the Fat Shattering dimensions of scale δ(ǫ, k) of F1, . . . ,Fk, where δ(ǫ, k) only depends on ǫ and k. There is a previously known analogous estimate for a composition of concept classes built using a usual connective of classical logic [18]. We deduce our new bound using results from Mendelson-Vershynin and Talagrand.\nBefore jumping into the PAC learning model, we provide some basic terminology and results from analysis and measure theory. From now on, any propositions or examples given with proofs, unless mentioned otherwise, are done by us and are independent of any sources."
    }, {
      "heading" : "2 Brief Overview of Analysis and Measure Theory",
      "text" : "This section lists some definitions and results in measure theory and analysis, found in standard textbooks, such as [6], [18], and [2], which are used in this report.\nProbability space\nDefinition 2.1. Let X be a set. A σ-algebra S is a non-empty collection of subsets of X such that the following are satisfied:\n1. If A ∈ S, then X \\ A ∈ S\n2. If Ai ∈ S for i ∈ N, then ⋃ i∈N Ai ∈ S\nIf S is a σ-algebra, then the pair (X,S) is called a measurable space.\nDefinition 2.2. Suppose (X,S) and (Y, T ) are two measurable spaces. A function f : X → Y is called measurable if f−1(T ) ∈ S for all T ∈ T .\nDefinition 2.3. Given a measurable space (X,S), a function µ : S → R+ = {r ∈ R : r ≥ 0} is a measure if the following hold:\n1. µ(∅) = 0\n2. If Ai ∈ S for all i ∈ N and Ai ∩Aj = ∅ whenever i 6= j, then\nµ\n(\n⋃ i∈N Ai\n)\n= ∑\ni∈N µ(Ai)\nThe triple (X,S, µ) is called a measure space. If in addition, µ satisfies µ(X) = 1, then µ is a probability measure and (X,S, µ) is called a probability space.\nGiven a probability space (X,S, µ), one can measure the difference between two subsets A,B ∈ S of X by looking at their symmetric difference A △ B, which is indeed in S:\nµ(A△B) = µ((A ∪B) \\ (A ∩ B)) = µ(((X \\ A) ∩ B) ∪ (A ∩ (X \\B))).\nMore generally, given two measurable functions f, g : X → [0, 1], one can look at the expected value of their absolute difference by integrating with respect to µ:\n∫\nX\n|f(x)− g(x)| dµ(x).\nThis report does not go into any details involving the Lebesgue integral but does assume that integration of measurable functions to the real numbers, which is a measure space, makes sense and is linear and order-preserving:\n∫\nX\n(rf(x) + r′g(x)) dµ(x) = r\n∫\nX\nf(x) dµ(x) + r′ ∫\nX\ng(x) dµ(x)\nand ∫\nX\nf(x) dµ(x) ≤ ∫\nX\ng(x) dµ(x),\nif f(x) ≤ g(x) for all x ∈ X . Validating hypotheses in the PAC learning model uses the idea of measuring the symmetric difference of two subsets of a probability space (X,S, µ) and calculating the expected value of the difference of f, g : X → [0, 1]. The structure of metric spaces arises naturally from these two notions.\nMetric spaces\nDefinition 2.4. Let M be a nonempty set. A function d : M ×M → R+ is a metric if the following hold for all m1, m2, m3 ∈ M :\n1. d(m1, m2) = 0 if and only if m1 = m2\n2. d(m1, m2) = d(m2, m1)\n3. d(m1, m2) ≤ d(m1, m3) + d(m3, m2)\nIn this case, the pair (M, d) is called a metric space.\nDefinition 2.5. Given a metric space (M, d), a metric sub-space of M (which is a metric space in its own right) is a nonempty subset M ′ ⊆ M equipped with the distance d|M′ , the restriction of d to M ′.\nThe structure of a metric space exists in every vector space equipped with a norm.\nDefinition 2.6. Suppose V is a vector space over R. A function ρ : V → R+ is a norm on V if for all v1, v2 ∈ V and for all r ∈ R,\n1. ρ(rv1) = |r|ρ(v1)\n2. ρ(v1 + v2) ≤ ρ(v1) + ρ(v2)\n3. ρ(v1) = 0 if and only if v1 = 0\nIf ρ is a norm on V , then (V, ρ) is called a normed vector space.\nProposition 2.7. Based on Definition 2.6, the function d : V × V → R+ defined by d(u, v) = ρ(u− v) is a metric on V , and d is called the metric induced by the norm ρ on V .\nThe following subsection provides a few examples of metric spaces which will be encountered in this report.\nExamples of metric spaces\nThe real numbers (R, ρ), with the absolute value norm ρ(r) = |r| for r ∈ R, is a normed vector space so R can be equipped with a metric structure.\nExample 2.8. The set R with distance d defined by d(r1, r2) = |r1−r2| for r1, r2 ∈ R is a metric space.\nThe unit interval [0, 1] is a subset of R, so it is a metric sub-space of (R, d), and this space will be used quite often in this report.\nGiven a probability space (X,S, µ), the set V of all bounded measurable functions from X to R is a vector space, with point-wise addition and scalar multiplication. The function ρ : V → R+ defined by\nρ(f) =\n√\n( ∫\nX\n(f(x))2dµ(x)\n)\nis a norm on V if any two functions f, g : X → R which agree on a subset of X with full measure, µ({x ∈ X : f(x) = g(x)}) = 1, are identified.1 The norm ρ is called the L2(µ) norm on V and we normally write ||f ||2 = ρ(f) for f ∈ V . As a result, V can be turned into a metric space.\nExample 2.9. Following the notations in the paragraph above, V is a metric space with distance d defined by\nd(f, g) = ||f − g||2 = √ ( ∫\nX\n(f(x)− g(x))2dµ(x) ) .\nWrite [0, 1]X for the set of all measurable functions from a probability space (X,S, µ) to [0, 1]. Then, it is a metric sub-space of V with distance induced by the L2(µ) norm on V , restricted of course to [0, 1]\nX . Given metric spaces (M1, d1), . . . , (Mk, dk), their product M1 × . . . ×Mk always\nhas a metric structure.\n1This identification can be done using an equivalence relation, so this report will not go into any details here.\nExample 2.10. If (M1, d1), . . . , (Mk, dk) are metric spaces, then their product M1 × . . .×Mk is a metric space with distance d2 defined by\nd2((m1, . . . , mk), (m ′ 1, . . . , m ′ k)) =\n√\n((d1(m1, m ′ 1)) 2 + . . .+ (dk(mk, m ′ k)) 2).\nThe distance d2 is normally referred to as the L2 product distance on M1× . . .×Mk.\nFrom Examples 2.8 and 2.10, the set [0, 1]k, which denotes the set-theoretic product [0, 1]× . . .× [0, 1] is then a metric space with distance d2 defined by\nd2((r1, . . . , rk), (r ′ 1, . . . , r ′ k)) =\n√\n(|r1 − r′1|2 + . . .+ |rk − r′k|2).\nAlso, following Examples 2.9 and 2.10, if F1, . . . ,Fk are sets of measurable functions from a probability space (X,S, µ) to the unit interval, then Fi ⊆ [0, 1]X for each i = 1, . . . , k. Therefore, the product F1 × . . . × Fk is a metric space with distance defined by\nd2((f1, . . . , fk), (f ′ 1, . . . , f ′ k)) =\n√\n((||f1 − f ′1||2)2 + . . .+ (||fk − f ′k||2)2)."
    }, {
      "heading" : "3 The Probably Approximately Correct Learning",
      "text" : "Model\nLet (X,S) be a measurable space. A concept class C of X is a subset of S and an element A ∈ C (a measurable subset of X) is called a concept. A function class F is a collection of measurable functions from X to the unit interval [0, 1]. Unless stated otherwise, from this section onwards, the following notations will be used:\n1. X = (X,S): a measurable space\n2. µ: a probability measure S → R+\n3. C: a concept class and F : a function class\n4. [0, 1]X : the set of all measurable functions f : X → [0, 1], instead of the customary notation of all functions from X to [0, 1].\nThis section provides the definitions of learning C and F in the Probably Approximately Correct (PAC) learning model, introduced in 1984 by Valiant.\nConcept class PAC learning involves producing a valid hypothesis for every concept A ∈ C by first drawing random points, forming a training sample, from X labeled with whether these points are contained in A. In other words, a labeled sample of m points x1, . . . , xm ∈ X for A consists of these points and the evaluations χA(x1), . . . , χA(xm) of the indicator function χA : X → {0, 1}, where\nχA(x) = 1 if and only if x ∈ A.\nOn the other hand, an unlabeled sample of points does not include these evaluations. The set of all labeled samples of m points can then be identified with (X ×{0, 1})m, and producing a hypothesis for A with a labeled sample is exactly the process of associating the sample to a concept H ∈ C (i.e. this process is a function from the set of all labeled samples to the concept class).\nHere is the precise definition of a concept class being learnable.\nDefinition 3.1 ([16]). A concept class C is distribution-free Probably Approximately Correct learnable if there exists an algorithm2 L : ∪m∈N(X × {0, 1})m → C with the following property: for every ǫ > 0, for every δ > 0, there exists a M ∈ N such that for every A ∈ C, for every probability measure µ, for every m ≥ M , for any x1, . . . , xm ∈ X, we have µ(Hm △ A) < ǫ with confidence at least 1 − δ, where Hm = L((x1, χA(x1)), . . . , (xm, χA(xm))).\nConfidence of at least 1−δ in the definition above, keeping to the same notations, simply means that the (product) measure of the set of allm-tuples (x1, . . . , xm) ∈ Xm,\n2In this report, a learning algorithm is simply defined to be a function.\nwhere µ(Hm △ A) < ǫ for Hm = L((x1, χA(x1)), . . . , (xm, χA(xm))), is at least 1 − δ. In other words, an equivalent statement to C is distribution-free PAC learnable is that for every ǫ, δ > 0, there exists M ∈ N such that for every A ∈ C, probability measure µ, and m ≥ M ,\nµm({(x1, . . . , xm) ∈ Xm : µ(Hm △ A) ≥ ǫ}) ≤ δ, 3\nfor Hm = L((x1, χA(x1)), . . . , (xm, χA(xm))). A concept class C is distribution-free learnable in the PAC learning model if a hypothesis H can always be constructed from an algorithm L for every concept A ∈ C, using any labeled sample for A, such that the measure of their symmetric difference H △ A is arbitrarily small with respect to every probability measure and with arbitrarily high confidence, as long as the sample size is large enough.\nEvery concept A ∈ C is a subset of X so A can be associated to its indicator function χA : X → {0, 1}. Even more generally, χA is a function from X to [0, 1]; in other words, every concept class C can be identified as a function class FC = {χA : X → [0, 1] : A ∈ C}, so it is natural to generalize Definition 3.1 for any function class F .\nDefinition 3.1 involves the symmetric difference of two concepts and its generalization to measurable functions f, g : X → [0, 1] is the expected value of their absolute difference Eµ(f, g), as seen in the previous section:\nEµ(f, g) =\n∫\nX\n|f(x)− g(x)| dµ(x).\nA simple exercise can show that if f, g ∈ [0, 1]X take values in {0, 1}, so they are indicator functions of two concepts A,B ⊆ X , then Eµ(f, g) coincide with the measure of their symmetric difference: Eµ(f, g) = µ(A△ B), where f = χA and g = χB.\nWith the generalization of the symmetric difference, distribution-free PAC learning for any function class can be defined. In the context of function class learning, a labeled sample of m points x1, . . . , xm ∈ X for a function f ∈ F consists of these points and the evaluations f(x1), . . . , f(xm). Then, the set of all labeled samples of m points can be identified with (X × [0, 1])m, and producing a hypothesis is the process of associating a labeled sample to a function H ∈ F (just as in concept class learning).\nDefinition 3.2 ([18]). A function class F is distribution-free Probably Approximately Correct learnable if there exists an algorithm L : ∪m∈N(X × [0, 1])m → F with the following property: for every ǫ > 0, for every δ > 0, there exists a M ∈ N such that for every f ∈ F , for every probability measure µ, for every m ≥ M , for any x1, . . . , xm ∈ X, we have Eµ(Hm, f) < ǫ with confidence at least 1 − δ, where Hm = L((x1, f(x1)), . . . , (xm, f(xm))).\n3The symbol µm denotes the product measure on Xm; the reader can refer to [6] for the details.\nBoth definitions of PAC learning contain the ǫ and δ parameters. The error parameter ǫ is used because the hypothesis is not required to have zero error - only an arbitrarily small error. The risk parameter δ exists because there is no guarantee that any collection of sufficiently large training points leads to a valid hypothesis; the learning algorithm is only expected to produce a valid hypothesis with the sample points with confidence at least 1− δ. Hence, the name “Probably (δ) Approximately (ǫ) Correct” is used [8].\nThe following example illustrates that the set of all axis-aligned rectangles in R2 is distribution-free PAC learnable. Both the statement and its proof can be found in Chapter 3 of [18] and Chapter 1 of [8].\nExample 3.3. In X = R2, the concept class C = {[a, b] × [c, d] : a, b, c, d ∈ R} is distribution-free PAC learnable.\nProof. Let ǫ, δ > 0. Given a concept A and any sample of m training points x1, . . . , xm ∈ X , define the hypothesis concept Hm to be the intersection of all rectangles containing only training points xi such that χA(xi) = 1. In other words, Hm is the smallest rectangle that contains only the sample points in A.\nLet µ be any probability measure, and in fact, Hm △ A = A \\Hm, which can be broken down into four sections T1, . . . , T4. If we can conclude that\nµ\n(\n4 ⋃\ni=1\nTi\n)\n< ǫ,\nwith confidence at least 1− δ, then the proof is complete. Consider the top section T1 and define T̃1 to be the rectangle along the top parts of A whose measure is exactly ǫ/4. The event T̃1 ⊆ T1, which is equivalent to µ(T1) ≥ ǫ/4, holds exactly when no points in the sample x1, . . . , xm fall in T̃1, and the probability of this event (which is the measure of all such m-tuples of (x1, . . . , xm) ∈ Xm where xi /∈ T̃1 for all i = 1, . . . , m) is\n( 1− ǫ 4 )m .\nSimilarly, the same holds for the other three sections T2, . . . , T4. Therefore, the probability that there exists at least one Ti such that µ(Ti) ≥ ǫ/4, where i ∈ {1, . . . , 4}, is at most\n4 ( 1− ǫ 4 )m .\nHence, as long as we pick m large enough that 4(1 − ǫ/4)m ≤ δ, with confidence (probability) at least 1− δ, µ(Ti) < ǫ/4 for every i = 1, . . . , 4 and thus,\nµ(Hm △ A) = µ ( 4 ⋃\ni=1\nTi\n)\n≤ µ(T1) + . . .+ µ(T4) < 4 ( ǫ\n4\n)\n= ǫ.\nPlease note that this argument, though very intuitive, actually requires the classical Glivenko-Cantelli theorem.\nIn summary, as long as m ≥ (4/ǫ) ln(4/δ), with confidence at least 1− δ, µ(Hm△ A) < ǫ. We note that this estimate of the sample size only depends on ǫ and δ, so C is indeed distribution-free PAC learnable.\nIn the next section, a fundamental theorem which characterizes concept class distribution-free PAC learning will be stated, and two more concept classes, one learnable and the other not,4 will be given. However, in order to state this theorem, the notion of shattering, which is essential in learning theory, must be introduced.\n4They are direct results of the theorem."
    }, {
      "heading" : "4 The Vapnik-Chervonenkis Dimension",
      "text" : "The Vapnik-Chervonenkis dimension is a combinatorial parameter which is defined using the notion of shattering, developed first in 1971 by Vapnik and Chervonenkis.\nDefinition 4.1 ([17]). Given any set X and a collection A of subsets of X, the collection A shatters a subset S ⊆ X if for every B ⊆ S, there exists A ∈ A such that\nA ∩ S = B.\nThere is an equivalent condition, which is sometimes easier to work with, to shattering, expressed in terms of characteristic functions of subsets of X .\nProposition 4.2. The collection A shatters a subset S = {x1, . . . , xn} ⊆ X if and only if for every e = (e1, . . . , en) ∈ {0, 1}n, there exists A ∈ A such that\nχA(xi) = ei,\nfor all i = 1, . . . , n.\nProof. Trivial.\nDefinition 4.3 ([17]). The Vapnik-Chervonenkis (VC) dimension of the collection A, denoted by VC(A), is defined to be the cardinality of the largest finite subset S ⊆ X shattered by A. If A shatters arbitrarily large finite subsets of X, then the VC dimension of A is defined to be ∞.\nThe VC dimension is defined for every collection A of subsets of any set X , so in particular, X = (X,S) can be a measurable space and A = C can be a concept class.\nThe following are a few examples of how to calculate VC dimensions in the context of X = Rn. In order to prove the VC dimension of a concept class C is d, we must provide a subset S ⊆ X with cardinality d which is shattered by C and prove that no subset with cardinality d+ 1 can be shattered by C.\nExample 4.4. If X = R, then the powerset of X has infinite VC dimension. More generally, for every infinite set X, VC(P(X)) = ∞.\nExample 4.5. In the space X = R, let C = {[a, b] : a, b ∈ R, a < b} be the collection of all closed intervals. Then, VC(C) = 2.\nProof. Consider the subset S = {1, 2} ⊆ R; C shatters S because\n[a, b] ∩ S =\n\n  \n   ∅ if a > 2 or b < 1 {1} if a ≤ 1, b < 2 {2} if a > 1, b ≥ 2 {1, 2} if a ≤ 1, b ≥ 2.\nOn the other hand, given any subset S = {x, y, z} ⊆ R with three distinct points, and assume the order to be x < y < z. Then, there are no closed interval in C containing x and z but not y.\nExample 4.6. Consider the space X = Rn. A hyperplane H~a,b is defined by a nonzero vector ~a = (a1, . . . , an) ∈ Rn and a scalar b ∈ R:\nH~a,b = {~x = (x1, . . . , xn) ∈ Rn : ~x · ~a = b} = {~x = (x1, . . . , xn) ∈ Rn : x1a1 + . . .+ xnan = b}.\nWrite C as the set of all hyperplanes: C = {H~a,b : ~a ∈ Rn \\ {~0}, b ∈ R}. Then VC(C) = n. Proof. Consider the subset S = {~e1, . . . , ~en} ⊆ Rn, where ~ei is the vector with 1 on the i-th component and 0 everywhere else. Suppose B ⊆ S and there are two cases to consider:\n1. If B = ∅, then let ~a = (1, 1, . . . , 1) ∈ Rn and the hyperplane H~a,−1 = {~x = (x1, . . . , xn) ∈ Rn : x1 + . . .+ xn = −1} is disjoint from S.\n2. If B 6= ∅, then set ~a = (a1, . . . , an) ∈ Rn \\ {~0}, where ai = χB(~ei). Then the hyperplane H~a,1 = {~x = (x1, . . . , xn) ∈ Rn : x1a1 + . . .+ xnan = 1} satisfies\nH~a,1 ∩ S = B.\nMoreover, no subset S = {~x1, . . . , ~xn, ~xn+1} ⊆ Rn with cardinality n + 1 can be shattered by C. At best, there exists a unique hyperplane H~a,b containing n of these points, say {~x1, . . . , ~xn}, so if ~xn+1 ∈ H~a,b, then there are no hyperplanes that include ~x1, . . . , ~xn, but not ~xn+1. Otherwise, if ~xn+1 /∈ H~a,b, then there are no hyperplanes that include ~x1, . . . , ~xn, ~xn+1.\nThe first example is trivial and the second is fairly well-known, seen in [8] and [10], but we believe the third, Example 4.6, is a new result.\nA very important concept related to shattering is the growth of all the possible subsets A ∩ S, for A ∈ C, as S ⊆ X increases in size. It is clear that this growth is always exponential if C has infinite VC dimension; Sauer’s Lemma explains the growth when VC(C) < ∞."
    }, {
      "heading" : "4.1 Sauer’s Lemma",
      "text" : "Given a concept class C of X , another way to express that C shatters a subset S ⊆ X , with cardinality n, is to consider the set of all A∩S, where A ∈ C. Following Chapter 4 of [18], C shatters S if and only if\n|{A ∩ S : A ∈ C}| = 2n.\nMore generally, for any subset S ⊆ X , define\nπ(S; C) = |{A ∩ S : A ∈ C}|\nand π(n; C) = max\n|S|=n π(S; C).\nThen, the VC dimension of C can now be expressed in terms of the growth of π(n; C) as n gets large.\nProposition 4.7. Given a concept class C, the following conditions are equivalent:\n1. VC(C) ≥ n;\n2. C shatters some subset S ⊆ X with cardinality n;\n3. π(n; C) = 2n.\nMoreover, the class C has infinite VC dimension if and only if π(n; C) = 2n for all n ∈ N. Conversely, C has finite VC dimension, say VC(C) ≤ d, if and only if π(n; C) < 2n for all n > d.\nProof. The proof follows from the fact that C shatters S if and only if π(S; C) = 2n.\nThe extremely interesting fact, as seen in the next theorem, is that if C has finite VC dimension d, then π(n; C) is bounded by a polynomial in n of degree d, for n ≥ d. This result, called Sauer’s Lemma, was first proven in 1972 by Sauer. In other words, as n gets large, π(n; C) is either always an exponential function with base 2 or eventually bounded by a polynomial function of a fixed degree.\nTheorem 4.8 (Sauer’s Lemma [12]). Suppose a concept class C has finite VC dimension d. Then\nπ(n; C) ≤ (en\nd\n)d\n,\nfor all n ≥ d ≥ 1.\nOf course, everything in this subsection, including Sauer’s Lemma, is true for any collection of subsets of any set but in the context of statistical learning theory, Sauer’s Lemma is particularly useful because it is used to prove the equivalence of a concept class having finite VC dimension and the class being distribution-free PAC learnable."
    }, {
      "heading" : "4.2 Characterization of concept class distribution-free PAC",
      "text" : "learning\nThe following is one of the main theorems concerning PAC learning, whose proof results from Vapnik and Chervonenkis’ paper [17] in 1971 and the 1989 paper [5] by Blumer et al..\nTheorem 4.9 ([17] and [5]). Let C be a concept class of a measurable space (X,S). The following are equivalent:\n1. C is distribution-free Probably Approximately Correct learnable.\n2. VC(C) < ∞.\nBoth directions of the proof require expressing the number of sample training points required for learning in terms of the VC dimension of C; Sauer’s Lemma is used to provide a sufficient number of points required for learning in the direction 2) ⇒ 1).\nUsing Theorem 4.9, one can more easily determine whether a given concept class is distribution-free PAC learnable.\nExample 4.10. Let X be any infinite set. Then the powerset P(X) is not distributionfree PAC learnable.\nExample 4.11. The set of all hyperplanes C = {H~a,b : ~a ∈ Rn \\ {~0}, b ∈ R}, as defined in Example 4.6, is distribution-free PAC learnable.\nBoth examples come directly from the calculations of their concept classes’ VC dimensions in Examples 4.4 and 4.6 and from Theorem 4.9.\nEvery concept class C can be viewed as a function class FC = {χA : X → [0, 1] : A ∈ C}, as seen in Section 3, so a natural question is whether the notion of shattering can be generalized. Indeed, the next section introduces the Fat Shattering dimension of scale ǫ, which is a generalization of the VC dimension."
    }, {
      "heading" : "5 The Fat Shattering Dimension",
      "text" : "Let ǫ > 0 from this section onwards. A combinatorial parameter which generalizes the Vapnik-Chervonenkis dimension is the Fat Shattering dimension of scale ǫ, defined first by Kearns and Schapire in 1994.\nThis dimension, assigned to function classes, involves the notion of ǫ-shattering, but similar to the notion of (regular) shattering, it can be defined for any collection of functions f : X → [0, 1], where X is any set, but for sake of this report, the following sections (still) assume X = (X,S) is a measurable space and the collection of functions is a function class F . Definition 5.1 ([7]). Let F be a function class. Given a subset S = {x1, . . . , xn} ⊆ X, the class F ǫ-shatters S, with witness c = (c1, . . . , cn) ∈ [0, 1]n, if for every e ∈ {0, 1}n, there exists f ∈ F such that\nf(xi) ≥ ci + ǫ for ei = 1, and f(xi) ≤ ci − ǫ for ei = 0.\nDefinition 5.2 ([7]). The Fat Shattering dimension of scale ǫ > 0 of F , denoted by fatǫ(F), is defined to be the cardinality of the largest finite subset of X that can be ǫ-shattered by F . If F can ǫ-shatter arbitrarily large finite subsets, then the Fat Shattering dimension of scale ǫ of F is defined to be ∞.\nWhen the function class F consists of only functions taking values in {0, 1}, then the Fat Shattering dimension of any scale ǫ ≤ 1/2 of F agrees with the VC dimension of the corresponding collection of subsets of X , induced by the (indicator) functions in F . Proposition 5.3. Suppose a function class F consists of only binary functions f : X → {0, 1}. For every f ∈ F , there exists a unique subset Af ⊆ X such that χAf = f . Moreover, write C = {Af : f ∈ F} and VC(C) = fatǫ(F) for all ǫ ≤ 0.5. Proof. The first statement, of the existence of a unique subset Af ⊆ X for every binary function f , is clear. Let ǫ ≤ 0.5. To show that VC(C) = fatǫ(F), it suffices to prove that C shatters S = {x1, . . . , xn} if and only if F ǫ-shatters S.\nThe equivalent condition to shattering as seen in Proposition 4.2 will be used. Suppose C shatters S and define c = (0.5, 0.5, . . . , 0.5) ∈ [0, 1]n. For every e ∈ {0, 1}n, there exists Af ∈ C, where f ∈ F , such that\nχAf (xi) = ei,\nfor all i = 1, . . . , n and thus,\nf(xi) = χAf (xi) = ei ≥ 0.5 + ǫ for ei = 1\nand f(xi) = χAf (xi) = ei ≤ 0.5− ǫ for ei = 0.\nConversely, suppose F ǫ-shatters S, with witness c = (c1, . . . , cn) ∈ [0, 1]n. Let e ∈ {0, 1}n and there exists f ∈ F such that\nf(xi) ≥ ci + ǫ for ei = 1, and f(xi) ≤ ci − ǫ for ei = 0,\nbut f is binary and ǫ is strictly positive, so f(xi) ≥ ci+ ǫ implies f(xi) = 1 for ei = 1 and f(xi) ≤ ci − ǫ implies f(xi) = 0 for ei = 0. As a result, consider Af ∈ C and\nχAf (xi) = f(xi) = ei\nfor all i = 1, . . . , n. Therefore, VC(C) = fatǫ(F).\nHere is an example of a commonly used function class which we proved, independent of any sources, to have infinite Fat Shattering dimension of scale ǫ.\nExample 5.4. Let X = R+ and let F be the set of all continuous functions f : X → [0, 1]. Then fatǫ(F) = ∞ for all 0 < ǫ ≤ 0.5.\nProof. Suppose 0 < ǫ ≤ 0.5, and consider a collection of continuous [0, 1]-valued functions defined as follows. Given e ∈ {0, 1}N, a countable binary sequence, define fe : X → [0, 1] by\nfe(x) =\n{\n1 if ei = 1\n0 if ei = 0,\nif x = i ∈ N. Otherwise, for x ∈ [m,m+ 1], with m ∈ N,\nfe(x) =\n\n \n  −(x−m) + 1 if em = 1, em+1 = 0 (x−m) if em = 0, em+1 = 1 em if em = em+1.\nFor each e ∈ {0, 1}N, fe is continuous because it is defined as a step function of lines which agree on the overlaps. Write F = {fe : e ∈ {0, 1}N} and F ⊆ F . To show that fatǫ(F) = ∞, it suffices to prove that fatǫ(F ) = ∞. Consider the subset S = {1, . . . , n} ⊆ X for any n ∈ N, and the collection F ǫ-shatters S with witness c = (0.5, 0.5, . . . , 0.5) ∈ [0, 1]n: for each e ∈ {0, 1}n, it can be extended to a countable binary sequence ẽ, where ẽi = ei for all i = 1, . . . , n and ẽi = 0 otherwise. Then, it is clear that\nfẽ(xi) = 1 ≥ ci + ǫ for ẽi = 1, and f(xi) = 0 ≤ ci − ǫ for ẽi = 0,\nwith xi = i ∈ S for i = 1, . . . , n.\nWith the generalization from a concept class to a function class, a natural question is whether the finiteness of the Fat Shattering dimension of all scales ǫ for a function\nclass F is equivalent to F being distribution-free PAC learnable. This question is addressed in the following subsection."
    }, {
      "heading" : "5.1 Sufficient condition for function class distribution-free",
      "text" : "PAC learning\nOne direction of Theorem 4.9 can be generalized and stated in terms of the Fat Shattering dimension of scale ǫ of a function class.\nTheorem 5.5 ([1] and [18]). Let F be a function class. If fatǫ(F) < ∞ for all ǫ > 0, then F is distribution-free PAC learnable.\nHowever, the converse to Theorem 5.5 is false. There exists a distribution-free PAC learnable function class with infinite Fat Shattering dimension of some scale ǫ.\nIn fact, for every concept class C with cardinality ℵ0 or 2ℵ0 , there is an associated function class FC defined as follows. Set up a bijection b : C → [0, 1/3] or to [0, 1/3]∩ Q, depending on the cardinality of C, and for every A ∈ C, define a function fA : X → [0, 1] by fA(x) = χA(x) + (−1)χA(x)b(A). Now, write FC = {fA : A ∈ C}. Note that FC can be thought of the collection of all indicator functions of A ∈ C, except that each “indicator” function fA has two unique identifying points b(A) and 1− b(A), instead of simply 0 and 1. The following proposition provides many counterexamples to Theorem 5.5, which are much simpler than the one found in [18].\nThe construction of the function class FC and the proposition below are developed from an idea of Example 2.10 in [11].\nProposition 5.6. Let C be a concept class. The associated function class FC = {fA : A ∈ C}, defined in the previous paragraph, is always distribution-free PAC learnable; this class has infinite Fat Shattering dimension of all scales ǫ < 1/6 if C has infinite VC dimension.\nProof. The function class FC is distribution-free PAC learnable because every function fA ∈ FC can be uniquely identified with just one point x0 ∈ X in any labeled sample: fA(x0) ∈ {b(A), 1− b(A)} uniquely determines A and thus, fA.\nFurthermore, suppose C has infinite VC dimension. Let n ∈ N be arbitrary and because VC(C) = ∞, there exists S = {x1, . . . , xn} such that C shatters S. Suppose ǫ < 1/6 and we claim that FC ǫ-shatters S with witness c = (0.5, . . . , 0.5) ∈ [0, 1]n. Indeed, let e ∈ {0, 1}n and there exists A ∈ C such that\nχA(xi) = ei,\nfor all i = 1, . . . , n, by Proposition 4.2. As a result,\nfA(xi) = 1− b(A) ≥ 0.5 + ǫ for ei = 1\nand fA(xi) = b(A) ≤ 0.5− ǫ for ei = 0.\nConsequently, FC has infinite Fat Shattering dimension of all scales ǫ < 1/6.\nThe next section explains the main result of our research: bounding the Fat Shattering dimension of scale ǫ of a composition function class which is built with a continuous logic connective."
    }, {
      "heading" : "6 The Fat Shattering Dimension of a Composition",
      "text" : "Function Class\nThe goals of this section are to construct a new function class from old ones by means of a continuous logic connective and to bound the Fat Shattering dimension of scale ǫ of the new function class in terms of the dimensions of the old ones. The following subsection provides this construction, which can be found in Chapter 4 of [18], in the context of concept classes using a connective of classical logic."
    }, {
      "heading" : "6.1 Construction in the context of concept classes",
      "text" : "Let C1, C2, . . . , Ck be concept classes, where k ≥ 2, and let u : {0, 1}k → {0, 1} be any function, commonly known as a connective of classical logic. A new collection of subsets of X arises from C1, . . . , Ck as follows.\nAs mentioned earlier in this report, every element A ∈ Ci can be identified as a binary function f : X → {0, 1}, namely its characteristic function f = χA, and vice versa. Now, for any k functions f1, . . . , fk : X → {0, 1}, where fi ∈ Ci with i = 1, . . . , k, consider a new function u(f1, . . . , fk) : X → {0, 1} defined by\nu(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x)).\nThe set of all possible u(f1, . . . , fk), denoted by u(C1, . . . , Ck), is given by\nu(C1, . . . , Ck) = {u(f1, . . . , fk) : fi ∈ Ci}.\nFor instance, when k = 2, we can consider the “Exclusive Or” connective ⊕ : {0, 1}2 → {0, 1} defined by\np⊕ q = (p ∧ ¬q) ∨ (¬p ∧ q),\nwhich corresponds to the symmetric difference operation. Then, our new concept class constructed from C1 and C2 is\n{A1 △ A2 : A1 ∈ C1, A2 ∈ C2}.\nThe next theorem states that if C1, C2, . . . , Ck all have finite VC dimension to start with, then regardless of u, the new collection u(C1, . . . , Ck) always has finite VC dimension.\nTheorem 6.1 ([18]). Let k ≥ 2. Suppose C1, . . . , Ck are concept classes, each viewed as a collection of binary functions, and u : {0, 1}k → {0, 1} is any function. If the VC\ndimension of Ci is finite for all i = 1, . . . , k. Then there exists a constant α = αk5, which depends only on k, such that\nVC(u(C1, . . . , Ck)) < dαk,\nwhere d = k\nmax i=1\nVC(Ci).\nThe proof of this theorem can be found in [18] and uses Sauer’s Lemma to bound the VC dimension of u(C1, . . . , Ck). The main objective of our project was to generalize this theorem for function classes, in terms of the Fat Shattering dimension of scale ǫ, but the connective of classical logic u would have to be replaced by a continuous logic connective, a continuous function u : [0, 1]k → [0, 1]."
    }, {
      "heading" : "6.2 Construction of new function class with continuous logic",
      "text" : "connective\nIn first-order logic, there are only two truth-values 0 or 1, so a connective is a function {0, 1}k → {0, 1} in the classical sense. However, in continuous logic, truth-values can be found anywhere in the unit interval [0, 1]. Therefore, we should consider a function u : [0, 1]k → [0, 1], which will transform function classes, and require that u be a continuous logic connective. In other words, u should be continuous from the (product) metric space [0, 1]k to the unit interval [19]; in fact, because u is continuous from a compact metric space to a metric space, it is automatically uniformly continuous.\nThe following provides the definition of a uniformly continuous function u from any metric space to another, but we must first qualify u with a modulus of uniform continuity.\nDefinition 6.2 (See e.g. [19]). A modulus of uniform continuity is any function δ : (0, 1] → (0, 1].\nDefinition 6.3 (See e.g. [19]). Let (M1, d1) and (M2, d2) be two metric spaces. A function u : M1 → M2 is uniformly continuous if there exists (a modulus of uniform continuity) δ : (0, 1] → (0, 1] such that for all ǫ ∈ (0, 1] and m1, m2 ∈ M1, if d1(m1, m2) < δ(ǫ), then d2(u(m1), u(m2)) < ǫ.\nSuch a δ is called a modulus of uniform continuity for u.\nIn particular, u : [0, 1]k → [0, 1], where [0, 1]k is equipped with the L2 product distance d2, is uniformly continuous with modulus of uniform continuity δ if for every\n5More specifically, α = αk is the smallest integer such that\nk < α\nlog(eα) .\nǫ ∈ (0, 1] and for every (r1, . . . , rk), (r′1, . . . , r′k) ∈ [0, 1]k,\nd2((r1, . . . , rk), (r ′ 1, . . . , r ′ k)) < δ(ǫ) ⇒ |u(r1, . . . , rk)− u(r′1, . . . , r′k)| < ǫ.\nGiven function classes F1, . . . ,Fk and a uniformly continuous function u : [0, 1]k → [0, 1], consider the new function class u(F1, . . . ,Fk) defined by\nu(F1, . . . ,Fk) = {u(f1, . . . , fk) : fi ∈ Fi},\nwhere u(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x)) for all x ∈ X , just as in Section 6.1 for concept classes, with fi ∈ Fi and i = 1, . . . , k. Our main result states that the Fat Shattering dimension of scale ǫ of u(F1, . . . ,Fk) is bounded by a sum of the Fat Shattering dimensions of scale δ(ǫ, k) of F1, . . . ,Fk, where δ(ǫ, k) is a function of the modulus of uniform continuity δ(ǫ) for u and k. It is a known result, seen in Chapter 5 of [18], that this new class u(F1, . . . ,Fk) has finite Fat Shattering dimension of all scales ǫ > 0 (and thus, it is distribution-free PAC learnable) if each of F1, . . . ,Fk has finite Fat Shattering dimension of all scales, but no bounds were known."
    }, {
      "heading" : "6.3 Main Result",
      "text" : "Fix k ≥ 2 and the following theorem is our main new result.\nTheorem 6.4. Let ǫ > 0, F1, . . . ,Fk be function classes of X, and u : [0, 1]k → [0, 1] be a uniformly continuous function with modulus of continuity δ(ǫ). Then\nfatǫ(u(F1, . . . ,Fk)) ≤ (\nK log(4c′k √ k/(δ(ǫ/(2c′))ǫ))\nK ′ log(2)\n)\nn ∑\ni=1\nfat c δ(ǫ/(2c ′))ǫ k √ k\n(Fi),\nwhere c, c′, K,K ′ are some absolute constants.\nExtracting the actual values of these absolute constants is not easy, and we hope to find them in future research. For this reason, comparing the bound in Theorem 6.4 with the existing estimate for the VC dimension of a composition concept class is difficult; however, in statistical learning theory, estimates for function class learning are generally much worse than estimates for concept class learning.\nIn order to prove Theorem 6.4, for clarity, we first introduce an auxiliary function φ : F1 × . . . × Fk → [0, 1]X , which is uniformly continuous from the metric space F1× . . .×Fk with the L2 product distance d̃2 to the metric space [0, 1]X with distance induced by the L2(µ) norm, and prove the following lemma.\nLemma 6.5. Let ǫ > 0, F1, . . . ,Fk be function classes of X, and φ : F1× . . .×Fk → [0, 1]X be uniformly continuous with some modulus of continuity δ(ǫ, k), a function\nof ǫ and k. Then\nfatc′ǫ(φ(F1 × . . .× Fk)) ≤ (\nK log(2 √ k/δ(ǫ, k))\nK ′ log(2)\n)\nk ∑\ni=1\nfat c δ(ǫ,k)√\nk\n(Fi),\nwhere c, c′, K,K ′ are some absolute constants and the symbol φ(F1× . . .×Fk) simply represents the image of φ.\nThen, we will relate the two uniformly continuous functions u and φ.\nLemma 6.6. Let ǫ > 0. If u : [0, 1]k → [0, 1] is uniformly continuous with modulus of continuity δ(ǫ), then the function φ : F1 × . . .× Fk → [0, 1]X defined by\nφ(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x))\nis also uniformly continuous with modulus of continuity δ(ǫ/2)ǫ 2k , and in fact, φ(F1 × . . .×Fk) = u(F1, . . . ,Fk)."
    }, {
      "heading" : "6.4 Proofs",
      "text" : "In order to prove Lemma 6.5, we first introduce the concept of an ǫ-covering number for any metric space, based on [9], and relate this number for a function class to its Fat Shattering dimension of scale ǫ by using results from Mendelson and Vershynin [9] and Talagrand [15].\nDefinition 6.7. Let ǫ > 0 and suppose (M, d) is a metric space. The ǫ-covering number, denoted by N(M, ǫ, d), of M is the minimal number N such that there exists elements m1, m2, . . . , mN ∈ M with the property that for all m ∈ M , there exists i ∈ {1, 2, . . . , N} for which\nd(m,mi) < ǫ.\nThe set {m1, m2, . . . , mN} is called a (minimal) ǫ-net of M . The following proposition relates the ǫ-covering number of a product of metric spaces, with the L2 product distance d 2, M1 × . . .×Mk to the ǫ√k -covering number of each space Mi.\nProposition 6.8. Let ǫ > 0 and suppose (M1, d1), . . . , (Mk, dk) are metric spaces, each with finite ǫ√\nk -covering numbers, Ni = N(Mi, ǫ√ k , di) for i = 1, . . . , k. Then\nN(M1 × . . .×Mk, ǫ, d2) ≤ k ∏\ni=1\nNi.\nProof. Let Ci = {ai1, . . . , aiNi} be a minimal ǫ√k -net for Mi with respect to distance di, where i = 1, . . . , k and suppose (a 1, . . . , ak) ∈ M1 × . . . × Mk. Then, for each\ni = 1, . . . , k, there exists aiji ∈ Ci, where 1 ≤ ji ≤ Ni such that di(ai, aiji) < ǫ√k . Hence,\nd2((a1, . . . , ak), (a1j1, . . . , a k jk )) =\n√\n(\n(d1(a1, a1j1)) 2 + . . .+ (dk(ak, akjk))\n2 )\n<\n√ √ √ √ ( (\nǫ√ k\n)2\n+ . . .+\n(\nǫ√ k\n)2 )\n= ǫ,\nwhere each (a1j1 , . . . , a k jk ) ∈ C1 × . . . × Ck, which has cardinality Πki=1Ni. Therefore, N(M1 × . . .×Mk, ǫ, d2) ≤ Πki=1Ni.\nAlso, if u : M1 → M2 is any uniformly continuous function with a modulus of uniform continuity δ(ǫ) from any metric space to another, then the image of a minimal δ(ǫ)-net of M1 under u becomes an ǫ-net for u(M1).\nProposition 6.9. Let ǫ > 0 and suppose (M1, d1) and (M2, d2) are two metric spaces. If a function u : M1 → M2 is uniformly continuous with a modulus of continuity δ(ǫ), then N(u(M1), ǫ, d2) ≤ N(M1, δ(ǫ), d1), where u(M1) denotes the image of u.\nProof. Suppose N = N(M1, δ(ǫ), d1) is the δ(ǫ)-covering number for M1 and let {m1, . . . , mN} be a δ(ǫ)-net for M1. Hence for every u(m) ∈ u(M1), where m ∈ M1, there exists i ∈ {1, . . . , N} such that\nd1(m,mi) < δ(ǫ),\nwhich implies d2(u(m), u(mi)) < ǫ as u is uniformly continuous. As a result, the set\n{u(m1), . . . , u(mN)}\nis an ǫ-net for u(M1), so\nN(u(M1), ǫ, d2) ≤ N(M1, δ(ǫ), d1).\nIn particular, we can view F1, . . . ,Fk as metric spaces, all with distances induced by the L2(µ) norm and suppose φ : F1 × . . .× Fk → [0, 1]X is uniformly continuous with modulus of continuity δ(ǫ, k). Then, by Proposition 6.8, if F1, . . . ,Fk all have finite δ(ǫ,k)√\nk -covering numbers, the metric space F1 × . . . × Fk, with the L2 product\nmetric d̃2, also has a finite δ(ǫ, k)-covering number: if we write N(Fi, δ(ǫ,k)√k , L2(µ)) as\nthe δ(ǫ,k)√ k -covering number for Fi, then,\nN(F1 × . . .× Fk, δ(ǫ, k), d̃2) ≤ k ∏\ni=1\nN(Fi, δ(ǫ, k)√\nk , L2(µ)).\nNow, by Proposition 6.9,\nN(φ(F1 × . . .× Fk), ǫ, L2(µ)) ≤ N(F1 × . . .× Fk, δ(ǫ, k), d̃2)\n≤ k ∏\ni=1\nN(Fi, δ(ǫ, k)√\nk , L2(µ)).\nIn other words, the ǫ-covering number for φ(F1×. . .×Fk) is bounded by a product of the δ(ǫ,k)√\nk -covering numbers of each Fi. To prove Lemma 6.5, we now state the main\ntheorem of a paper written by Mendelson and Vershynin, which relates the ǫ-covering number of a function class to its Fat Shattering dimension of scale ǫ.\nTheorem 6.10 ([9]). Let ǫ > 0 and let F be a function class. Then for every probability measure µ,\nN(F , ǫ, L2(µ)) ≤ ( 2\nǫ\n)Kfatcǫ(F)\nfor absolute constants c,K.\nAnd Talagrand provides the converse.\nTheorem 6.11 ([15]). Following the notations of Theorem 6.10, there exists a probability measure µ such that\nN(F , ǫ, L2(µ)) ≥ 2K ′fatc′ǫ(F),\nfor absolute constants c′, K ′.\nProof of Lemma 6.5. By Propositions 6.8 and 6.9,\nN(φ(F1 × . . .× Fk), ǫ, L2(µ)) ≤ k ∏\ni=1\nN(Fi, δ(ǫ, k)√\nk , L2(µ)),\nso\nlog(N(φ(F1 × . . .× Fk), ǫ, L2(µ))) ≤ k ∑\ni=1\nlog(N(Fi, δ(ǫ, k)√\nk , L2(µ))).\nBy Theorem 6.10,\nlogN(Fi, δ(ǫ, k)√\nk , L2(µ)) ≤ Kfatc δ(ǫ,k)√ k\n(Fi) log(2 √ k/δ(ǫ, k)),\nfor any probability measure µ where c,K are absolute constants. Moreover, by Theorem 6.11 for some probability measure µ and absolute constants c′, K ′,\nlog(N(φ(F1 × . . .× Fk), ǫ, L2(µ))) ≥ K ′fatc′ǫ(φ(F1 × . . .× Fk)) log(2)\nand altogether,\nfatc′ǫ(φ(F1 × . . .×Fk)) ≤ ∑k i=1Kfatc δ(ǫ,k)√ k\n(Fi) log(2 √ k/δ(ǫ, k))\nK ′ log(2)\n=\n( K log(2 √ k/δ(ǫ, k))\nK ′ log(2)\n)\nk ∑\ni=1\nfat c δ(ǫ,k)√\nk\n(Fi).\nNow, all that is left is to prove Lemma 6.6.\nProof of Lemma 6.6. Suppose u : [0, 1]k → [0, 1] is uniformly continuous with a modulus of continuity δ(ǫ), where [0, 1]k is a metric space with the L2 product distance d2. We claim that the function φ : F1 × . . .×Fk → [0, 1]X defined by\nφ(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x))\nis uniformly continuous with modulus of continuity δ(ǫ/2)ǫ 2k . Let ǫ > 0 and\n(f1, . . . , fk), (f ′ 1, . . . , f ′ k) ∈ F1 × . . .×Fk.\nSuppose\nd̃2((f1, . . . , fk), (f ′ 1, . . . , f ′ k)) =\n√\n((||f1 − f ′1||2)2 + . . .+ (||fk − f ′k||2)2)\n< δ(ǫ/2)ǫ\n2k =\n√\nδ(ǫ/2)2(ǫ/2)2\nk2 .\nHence, for each i = 1, . . . , k,\n||fi − f ′i ||2 = √ ( ∫\nX\n(fi(x)− f ′i(x))2 dµ(x) ) <\n√\nδ(ǫ/2)2(ǫ/2)2\nk2 .\nWrite Ai = {x ∈ X : |fi(x) − f ′i(x)| ≥ √ δ(ǫ/2)2 k } and we must have that µ(Ai) <\n(ǫ/2)2\nk , for each i = 1, . . . , k. Otherwise,\n∫\nX\n(fi(x)− f ′i(x))2 dµ(x) = ∫\nAi\n(fi(x)− f ′i(x))2 dµ(x) + ∫\nX\\Ai (fi(x)− f ′i(x))2 dµ(x)\n≥ ∫\nAi\n( √\nδ(ǫ/2)2\nk\n)2\ndµ(x) +\n∫\nX\\Ai (fi(x)− f ′i(x))2 dµ(x)\n= µ(Ai)\n( √\nδ(ǫ/2)2\nk\n)2\n+\n∫\nX\\Ai (fi(x)− f ′i(x))2 dµ(x)\n≥ (ǫ/2) 2\nk\nδ(ǫ/2)2\nk +\n∫\nX\\Ai (fi(x)− f ′i(x))2 dµ(x)\n≥ δ(ǫ/2) 2(ǫ/2)2\nk2 ,\nwhich is a contradiction. Now, write A = A1 ∪ . . . ∪ Ak and we have that X \\ A = {x ∈ X : |fi(x) − f ′i(x)| < √ δ(ǫ/2)2 k , for all i = 1, . . . , k}. Suppose x ∈ X \\ A and then\nd2((f1(x), . . . , fk(x)), (f ′ 1(x), . . . , f ′ k(x))) =\n√\n|f1(x)− f ′1(x)|2 + . . .+ |fk(x)− f ′k(x)|2\n<\n√\n(\nδ(ǫ/2)2\nk + . . .+\nδ(ǫ/2)2\nk\n)\n< δ(ǫ/2).\nConsequently, by the uniform continuity of u, for all x ∈ X \\ A,\n|u(f1(x), . . . , fk(x))− u(f ′1(x), . . . , f ′k(x))| < ǫ/2.\nFinally,\n||φ(f1, . . . , fk)− φ(f ′1, . . . , f ′k)||2 = √ ( ∫\nX\n(u(f1(x), . . . , fk(x))− u(f ′1(x), . . . , f ′k(x)))2 dµ(x) )\n≤ √ ( ∫\nX\\A (u(f1(x), . . . , fk(x))− u(f ′1(x), . . . , f ′k(x)))2 dµ(x)\n)\n+\n√\n( ∫\nA\n(u(f1(x), . . . , fk(x))− u(f ′1(x), . . . , f ′k(x)))2 dµ(x) )\n<\n√\n( ∫\nX\\A (ǫ/2)2 dµ(x)\n)\n+\n√\n( ∫\nA\n1 dµ(x)\n)\n≤ (ǫ/2) + (ǫ/2) = ǫ,\nas µ(A) ≤ ∑ki=1 µ(Ai) ≤ k ( (ǫ/2)2 k ) = (ǫ/2)2.\nNow we will prove our main theorem.\nProof of Theorem 6.4. By Lemma 6.6, if u : [0, 1]k → [0, 1] is uniformly continuous with modulus of continuity δ(ǫ), then φ : F1 × . . .× Fk → [0, 1]X defined by\nφ(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x))\nis also uniformly continuous with modulus of continuity δ(ǫ/2)ǫ 2k . Then, apply Lemma 6.5 with δ(ǫ, k) = δ(ǫ/2)ǫ 2k\nand with a simple change of variables c′ǫ′ → ǫ, Theorem 6.4 follows directly.\nAltogether, we can summarize the maps in this section in the following two diagrams (where i is the diagonal map):\nX i // Xk f1×...×fk // [0, 1]k u // [0, 1] ,\nwhile\nF1 × . . .× Fk φ // [0, 1]X .\nThis result is potentially useful because it allows us to construct new function classes using common continuous logic connectives and bound their Fat Shattering dimensions of scale ǫ. For instance, the function u : [0, 1]2 → [0, 1] defined by u(r1, r2) = r1 · r2 (multiplication) is uniformly continuous with a modulus of continuity δ(ǫ) = ǫ\n2 . Indeed, let ǫ > 0 and consider (r1, r2), (r ′ 1, r ′ 2) ∈ [0, 1]2. Suppose\nd2((r1, r2), (r ′ 1, r ′ 2)) < δ(ǫ) = ǫ 2 , so\n|r1 − r′1| < √ |r1 − r′1|2 + |r2 − r′2|2 < ǫ\n2\nand similarly, |r2 − r′2| < ǫ2 . Then,\n|u(r1, r2)− u(r′1, r′2)| = |r1r2 − r′1r′2| = |r1r2 − r1r′2 + r1r′2 − r′1r′2| = |r1(r2 − r′2) + r′2(r1 − r′1)| ≤ |r1(r2 − r′2)|+ |r′2(r1 − r′1)| ≤ |r2 − r′2|+ |r1 − r′1| < ǫ\n2 +\nǫ 2 = ǫ.\nAs a result, if F1 and F2 are two function classes with finite Fat Shattering dimensions of some scale ǫ, then the function class u(F1,F2) = F1F2 = {f1 · f2 : f1 ∈ F1, f2 ∈ F2}, defined by point-wise multiplication, also has finite Fat Shattering dimension of scale ǫ, up to some constant factor and Theorem 6.4 provides a precise bound.\nWe have made an interesting connection, which has not been explored much in the past, between continuous logic and PAC learning, and we plan to investigate this connection even further. For instance, the relationship of compositions of function classes and continuous logic may be interesting to study because compositions of uniformly continuous functions are again uniformly continuous. Furthermore, we can try to add some topological structures to concept classes to see how PAC learning can be affected. The next section provides a couple of other possible future research topics."
    }, {
      "heading" : "7 Open Questions",
      "text" : "The definitions of distribution-free PAC learning, for both concept and function classes, in Section 3, made no assumptions about probability measures, as a learning algorithm has to produce a valid hypothesis for any probability measure µ. If we fix a probability measure µ and ask whether a concept class, or a function class, is PAC learnable, then we are working in the context of fixed distribution PAC learning.\nDefinition 7.1 ([18]). Let µ be a probability measure. A function class F is Probably Approximately Correct learnable under µ if there exists an algorithm L : ∪m∈N(X × [0, 1])m → F with the following property: for every ǫ > 0, for every δ > 0, there exists a M ∈ N such that for every f ∈ F , for every m ≥ M , for any x1, . . . , xm ∈ X, we have Eµ(Hm, f) < ǫ with confidence at least 1− δ, where\nEµ(Hm, f) =\n∫\nX\n|f(x)− g(x)| dµ(x)\nand Hm = L((x1, f(x1)), . . . , (xm, f(xm))).\nWhen a function class F consists of only binary functions, i.e. F = C is a concept class, there is a theorem, proved by Benedek and Itai in 1991, which gives a characterization of fixed distribution PAC learnability.\nTheorem 7.2 ([4]). Fix a probability measure µ and consider a concept class C. The following are equivalent:\n1. C is Probably Approximately Correct learnable under µ.\n2. (Finite Metric Entropy condition) The ǫ-covering number of C when viewed as a metric space with distance d = µ( △ ) is finite for every ǫ > 0.\nHowever, there is no characterization for fixed distribution PAC learnability of a general function class. Talagrand had proved that a function class is a GlivenkoCantelli (GC) function class with regard to a single measure µ if and only if the class has no witness of irregularity, a property that involves shattering [13],[14]. Every GC function class is PAC learnable under µ [11], but the property of having no witness of irregularity is strictly stronger than PAC learnability. We would like to propose the following conjecture for a possible characterization.\nConjecture 7.3. Fix a probability measure µ and consider a function class F . Let ǫ > 0. The following are equivalent:\n1. The function class F is PAC learnable under µ to accuracy ǫ.6 6Being PAC learnable to accuracy ǫ means Definition 7.1 is satisfied, but only for this particular\nǫ.\n2. There exists M,N and γ > 0 such that for all functions f ∈ F , with probability at least γ, the set {g ∈ F : g|x̄N = f|x̄N } has an ǫ-covering number, with respect to the distance d = Eµ( , ), of at most M , where x̄N denotes a sample of N points.\nA very interesting research topic is to study this conjecture and either prove or disprove it. Also, by Proposition 5.6, the finiteness of the Fat Shattering dimension of all scales ǫ > 0 does not characterize function class PAC learning in the distributionfree case; consequently, another topic of research would be to come up with a new combinatorial parameter for a function class, related to the notion of shattering, which would characterize learning. This new parameter would have to solve the problem of unique identifications of functions, a problem that does not occur with concept classes.\nYet another possible research topic is to generalize the definitions of PAC learning and introduce observation noise, both in the fixed distribution and distribution-free cases. The paper [3] written by Bartlett et al. proves that the finiteness of the Fat Shattering dimension of all scales of a function class F is equivalent to F being distribution-free learnable under certain noise distributions. It would be interesting to generalize this result and/or apply it in the fixed distribution setting."
    }, {
      "heading" : "8 Conclusion",
      "text" : "This report introduces the definitions of Probably Approximately Correct learning for concept and function classes and defines the Vapnik-Chervonenkis dimension for concept classes and the Fat Shattering dimension of scale ǫ > 0 for function classes. Finiteness of the VC dimension characterizes concept class distribution-free PAC learning; however, the finiteness of the Fat Shattering dimension of all scales ǫ is still only sufficient for function class learning, and not necessary.\nGiven function classes F1, . . . ,Fk, one can construct a new class u(F1, . . . ,Fk) using a continuous function u : [0, 1]k → [0, 1], a continuous logic connective. The main new result of this report shows that the Fat Shattering dimension of scale ǫ of u(F1, . . . ,Fk) is bounded by a sum of the Fat Shattering dimensions of scale δ(ǫ, k) of classes F1, . . . ,Fk, up to some absolute constants. This result can be useful because it allows us to construct new function classes, which may be very natural objects, and bound their Fat Shattering dimensions."
    } ],
    "references" : [ {
      "title" : "Scale-Sensitive Dimensions",
      "author" : [ "N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler" ],
      "venue" : "Uniform Convergence, and Learnability. Journal of the ACM 44.4 ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Mathématiques: Topologie et Analyse",
      "author" : [ "G. Auliac", "J.Y. Caby" ],
      "venue" : "3rd Ed. Belgium: EdiScience",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Fat-Shattering and the Learnability of Real-Valued Functions",
      "author" : [ "P.L. Bartlett", "P.M. Long", "R.C. Williamson" ],
      "venue" : "Journal of Computer and System Sciences 52.3 ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Learnability with respect to Fixed Distributions",
      "author" : [ "G.M. Benedek", "A. Itai" ],
      "venue" : "Theoretical Computer Science 86.2 ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Learnability and the Vapnik-Chervonenkis Dimension",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M. Warmuth" ],
      "venue" : "Journal of the ACM 36.4 ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Measure Theory",
      "author" : [ "J.L. Doob" ],
      "venue" : "New York: Springer-Verlag",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Efficient Distribution-free Learning of Probabilistic Concepts",
      "author" : [ "M.J. Kearns", "R. Schapire" ],
      "venue" : "Journal of Computer System Sciences 48.3 ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "An Introduction to Computational Learning Theory",
      "author" : [ "M.J. Kearns", "U.V. Vazirani" ],
      "venue" : "Cambridge, Massachusetts: The MIT Press",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Entropy and the Combinatorial Dimension",
      "author" : [ "S. Mendelson", "R. Vershynin" ],
      "venue" : "Inventiones Mathematicae 152 ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Indexability",
      "author" : [ "V. Pestov" ],
      "venue" : "Concentration, and VC Theory. An invited paper, Proc. of the 3rd International Conf. on Similarity Search and Applications ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A Note on Sample Complexity of Learning Binary Output Neural Networks Under Fixed Input Distributions",
      "author" : [ "V. Pestov" ],
      "venue" : "Proc. 2010 Eleventh Brazilian Symposium on Neural Networks, IEEE Computer Society, Los Alamitos-Washington-Tokyo ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the Densities of Families of Sets",
      "author" : [ "N. Sauer" ],
      "venue" : "J. Combinatorial Theory 13 ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "The Glivenko-Cantelli Problem",
      "author" : [ "M. Talagrand" ],
      "venue" : "Annals of Probability 15 ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "The Glivenko-Cantelli Problem",
      "author" : [ "M. Talagrand" ],
      "venue" : "Ten Years Later. J. Theoret. Probab. 9 ",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Vapnik-Chervonenkis Type Conditions and Uniform Donsker Classes of Functions",
      "author" : [ "M. Talagrand" ],
      "venue" : "Annals of Probability 31.3 ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A Theory of the Learnable",
      "author" : [ "L.G. Valiant" ],
      "venue" : "Communications of the ACM 27.11 ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities",
      "author" : [ "V.N. Vapnik", "A.Y. Chervonenkis" ],
      "venue" : "Theory of Prob. and its Appl. 16.2 ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "A Theory of Learning and Generalization: With Applications to Neural Networks and Control Systems",
      "author" : [ "M. Vidyasagar" ],
      "venue" : "London: Springer-Verlag London Limited",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Model Theory for Metric Structures",
      "author" : [ "I.B. Yaacov", "A. Berenstein", "C.W. Henson", "A. Usvyatsov" ],
      "venue" : "London Math Society Lecture Note Series 350 ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Is there a way to predict whether any given person in the hospital has the disease or not? This report covers the PAC learning model applied to learning a collection of subsets C, called a concept class, of a domain X and more generally, a collection of functions F , called a function class, from X to the unit interval [0, 1].",
      "startOffset" : 321,
      "endOffset" : 327
    }, {
      "referenceID" : 0,
      "context" : ",Fk and a “continuous logic connective” (that is, a continuous function u : [0, 1] → [0, 1]), we consider the construction of a new composition function class u(F1, .",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : ",Fk and a “continuous logic connective” (that is, a continuous function u : [0, 1] → [0, 1]), we consider the construction of a new composition function class u(F1, .",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "There is a previously known analogous estimate for a composition of concept classes built using a usual connective of classical logic [18].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "This section lists some definitions and results in measure theory and analysis, found in standard textbooks, such as [6], [18], and [2], which are used in this report.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "This section lists some definitions and results in measure theory and analysis, found in standard textbooks, such as [6], [18], and [2], which are used in this report.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "This section lists some definitions and results in measure theory and analysis, found in standard textbooks, such as [6], [18], and [2], which are used in this report.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "More generally, given two measurable functions f, g : X → [0, 1], one can look at the expected value of their absolute difference by integrating with respect to μ:",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Validating hypotheses in the PAC learning model uses the idea of measuring the symmetric difference of two subsets of a probability space (X,S, μ) and calculating the expected value of the difference of f, g : X → [0, 1].",
      "startOffset" : 214,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : "The unit interval [0, 1] is a subset of R, so it is a metric sub-space of (R, d), and this space will be used quite often in this report.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Write [0, 1] for the set of all measurable functions from a probability space (X,S, μ) to [0, 1].",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "Write [0, 1] for the set of all measurable functions from a probability space (X,S, μ) to [0, 1].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Then, it is a metric sub-space of V with distance induced by the L2(μ) norm on V , restricted of course to [0, 1] X .",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "10, the set [0, 1], which denotes the set-theoretic product [0, 1]× .",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "10, the set [0, 1], which denotes the set-theoretic product [0, 1]× .",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "× [0, 1] is then a metric space with distance d defined by d((r1, .",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : ",Fk are sets of measurable functions from a probability space (X,S, μ) to the unit interval, then Fi ⊆ [0, 1] for each i = 1, .",
      "startOffset" : 103,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "A function class F is a collection of measurable functions from X to the unit interval [0, 1].",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] : the set of all measurable functions f : X → [0, 1], instead of the customary notation of all functions from X to [0, 1].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] : the set of all measurable functions f : X → [0, 1], instead of the customary notation of all functions from X to [0, 1].",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] : the set of all measurable functions f : X → [0, 1], instead of the customary notation of all functions from X to [0, 1].",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "1 ([16]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "Even more generally, χA is a function from X to [0, 1]; in other words, every concept class C can be identified as a function class FC = {χA : X → [0, 1] : A ∈ C}, so it is natural to generalize Definition 3.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Even more generally, χA is a function from X to [0, 1]; in other words, every concept class C can be identified as a function class FC = {χA : X → [0, 1] : A ∈ C}, so it is natural to generalize Definition 3.",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "1 involves the symmetric difference of two concepts and its generalization to measurable functions f, g : X → [0, 1] is the expected value of their absolute difference Eμ(f, g), as seen in the previous section:",
      "startOffset" : 110,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "A simple exercise can show that if f, g ∈ [0, 1] take values in {0, 1}, so they are indicator functions of two concepts A,B ⊆ X , then Eμ(f, g) coincide with the measure of their symmetric difference: Eμ(f, g) = μ(A△ B), where f = χA and g = χB.",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "Then, the set of all labeled samples of m points can be identified with (X × [0, 1]), and producing a hypothesis is the process of associating a labeled sample to a function H ∈ F (just as in concept class learning).",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "2 ([18]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "A function class F is distribution-free Probably Approximately Correct learnable if there exists an algorithm L : ∪m∈N(X × [0, 1]) → F with the following property: for every ǫ > 0, for every δ > 0, there exists a M ∈ N such that for every f ∈ F , for every probability measure μ, for every m ≥ M , for any x1, .",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "The symbol μ denotes the product measure on X; the reader can refer to [6] for the details.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Hence, the name “Probably (δ) Approximately (ǫ) Correct” is used [8].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Both the statement and its proof can be found in Chapter 3 of [18] and Chapter 1 of [8].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "Both the statement and its proof can be found in Chapter 3 of [18] and Chapter 1 of [8].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "1 ([17]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "3 ([17]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "The first example is trivial and the second is fairly well-known, seen in [8] and [10], but we believe the third, Example 4.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "The first example is trivial and the second is fairly well-known, seen in [8] and [10], but we believe the third, Example 4.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "Following Chapter 4 of [18], C shatters S if and only if |{A ∩ S : A ∈ C}| = 2.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "8 (Sauer’s Lemma [12]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "2 Characterization of concept class distribution-free PAC learning The following is one of the main theorems concerning PAC learning, whose proof results from Vapnik and Chervonenkis’ paper [17] in 1971 and the 1989 paper [5] by Blumer et al.",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "2 Characterization of concept class distribution-free PAC learning The following is one of the main theorems concerning PAC learning, whose proof results from Vapnik and Chervonenkis’ paper [17] in 1971 and the 1989 paper [5] by Blumer et al.",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 16,
      "context" : "9 ([17] and [5]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "9 ([17] and [5]).",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "Every concept class C can be viewed as a function class FC = {χA : X → [0, 1] : A ∈ C}, as seen in Section 3, so a natural question is whether the notion of shattering can be generalized.",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "This dimension, assigned to function classes, involves the notion of ǫ-shattering, but similar to the notion of (regular) shattering, it can be defined for any collection of functions f : X → [0, 1], where X is any set, but for sake of this report, the following sections (still) assume X = (X,S) is a measurable space and the collection of functions is a function class F .",
      "startOffset" : 192,
      "endOffset" : 198
    }, {
      "referenceID" : 6,
      "context" : "1 ([7]).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : ", cn) ∈ [0, 1], if for every e ∈ {0, 1}n, there exists f ∈ F such that f(xi) ≥ ci + ǫ for ei = 1, and f(xi) ≤ ci − ǫ for ei = 0.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 6,
      "context" : "2 ([7]).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "5) ∈ [0, 1].",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : ", cn) ∈ [0, 1].",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "Let X = R and let F be the set of all continuous functions f : X → [0, 1].",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "5, and consider a collection of continuous [0, 1]-valued functions defined as follows.",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "Given e ∈ {0, 1}N, a countable binary sequence, define fe : X → [0, 1] by fe(x) = {",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "5) ∈ [0, 1]: for each e ∈ {0, 1}n, it can be extended to a countable binary sequence ẽ, where ẽi = ei for all i = 1, .",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "5 ([1] and [18]).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 17,
      "context" : "5 ([1] and [18]).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "Set up a bijection b : C → [0, 1/3] or to [0, 1/3]∩ Q, depending on the cardinality of C, and for every A ∈ C, define a function fA : X → [0, 1] by fA(x) = χA(x) + (−1)Ab(A).",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : "5, which are much simpler than the one found in [18].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "10 in [11].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "5) ∈ [0, 1].",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 17,
      "context" : "The following subsection provides this construction, which can be found in Chapter 4 of [18], in the context of concept classes using a connective of classical logic.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "1 ([18]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "The proof of this theorem can be found in [18] and uses Sauer’s Lemma to bound the VC dimension of u(C1, .",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "The main objective of our project was to generalize this theorem for function classes, in terms of the Fat Shattering dimension of scale ǫ, but the connective of classical logic u would have to be replaced by a continuous logic connective, a continuous function u : [0, 1] → [0, 1].",
      "startOffset" : 266,
      "endOffset" : 272
    }, {
      "referenceID" : 0,
      "context" : "The main objective of our project was to generalize this theorem for function classes, in terms of the Fat Shattering dimension of scale ǫ, but the connective of classical logic u would have to be replaced by a continuous logic connective, a continuous function u : [0, 1] → [0, 1].",
      "startOffset" : 275,
      "endOffset" : 281
    }, {
      "referenceID" : 0,
      "context" : "However, in continuous logic, truth-values can be found anywhere in the unit interval [0, 1].",
      "startOffset" : 86,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "Therefore, we should consider a function u : [0, 1] → [0, 1], which will transform function classes, and require that u be a continuous logic connective.",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "Therefore, we should consider a function u : [0, 1] → [0, 1], which will transform function classes, and require that u be a continuous logic connective.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "In other words, u should be continuous from the (product) metric space [0, 1] to the unit interval [19]; in fact, because u is continuous from a compact metric space to a metric space, it is automatically uniformly continuous.",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "In other words, u should be continuous from the (product) metric space [0, 1] to the unit interval [19]; in fact, because u is continuous from a compact metric space to a metric space, it is automatically uniformly continuous.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "[19]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "In particular, u : [0, 1] → [0, 1], where [0, 1] is equipped with the L2 product distance d, is uniformly continuous with modulus of uniform continuity δ if for every More specifically, α = αk is the smallest integer such that",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "In particular, u : [0, 1] → [0, 1], where [0, 1] is equipped with the L2 product distance d, is uniformly continuous with modulus of uniform continuity δ if for every More specifically, α = αk is the smallest integer such that",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "In particular, u : [0, 1] → [0, 1], where [0, 1] is equipped with the L2 product distance d, is uniformly continuous with modulus of uniform continuity δ if for every More specifically, α = αk is the smallest integer such that",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : ", r′ k) ∈ [0, 1], d((r1, .",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : ",Fk and a uniformly continuous function u : [0, 1] → [0, 1], consider the new function class u(F1, .",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : ",Fk and a uniformly continuous function u : [0, 1] → [0, 1], consider the new function class u(F1, .",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "It is a known result, seen in Chapter 5 of [18], that this new class u(F1, .",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : ",Fk be function classes of X, and u : [0, 1] → [0, 1] be a uniformly continuous function with modulus of continuity δ(ǫ).",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : ",Fk be function classes of X, and u : [0, 1] → [0, 1] be a uniformly continuous function with modulus of continuity δ(ǫ).",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "× Fk → [0, 1] , which is uniformly continuous from the metric space F1× .",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "×Fk with the L2 product distance d̃ to the metric space [0, 1] with distance induced by the L2(μ) norm, and prove the following lemma.",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "×Fk → [0, 1] be uniformly continuous with some modulus of continuity δ(ǫ, k), a function",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "If u : [0, 1] → [0, 1] is uniformly continuous with modulus of continuity δ(ǫ), then the function φ : F1 × .",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "If u : [0, 1] → [0, 1] is uniformly continuous with modulus of continuity δ(ǫ), then the function φ : F1 × .",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "× Fk → [0, 1] defined by φ(f1, .",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "5, we first introduce the concept of an ǫ-covering number for any metric space, based on [9], and relate this number for a function class to its Fat Shattering dimension of scale ǫ by using results from Mendelson and Vershynin [9] and Talagrand [15].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "5, we first introduce the concept of an ǫ-covering number for any metric space, based on [9], and relate this number for a function class to its Fat Shattering dimension of scale ǫ by using results from Mendelson and Vershynin [9] and Talagrand [15].",
      "startOffset" : 227,
      "endOffset" : 230
    }, {
      "referenceID" : 14,
      "context" : "5, we first introduce the concept of an ǫ-covering number for any metric space, based on [9], and relate this number for a function class to its Fat Shattering dimension of scale ǫ by using results from Mendelson and Vershynin [9] and Talagrand [15].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 0,
      "context" : "× Fk → [0, 1] is uniformly continuous with modulus of continuity δ(ǫ, k).",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "10 ([9]).",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "11 ([15]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "Suppose u : [0, 1] → [0, 1] is uniformly continuous with a modulus of continuity δ(ǫ), where [0, 1] is a metric space with the L2 product distance d.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Suppose u : [0, 1] → [0, 1] is uniformly continuous with a modulus of continuity δ(ǫ), where [0, 1] is a metric space with the L2 product distance d.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Suppose u : [0, 1] → [0, 1] is uniformly continuous with a modulus of continuity δ(ǫ), where [0, 1] is a metric space with the L2 product distance d.",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "×Fk → [0, 1] defined by φ(f1, .",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "6, if u : [0, 1] → [0, 1] is uniformly continuous with modulus of continuity δ(ǫ), then φ : F1 × .",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "6, if u : [0, 1] → [0, 1] is uniformly continuous with modulus of continuity δ(ǫ), then φ : F1 × .",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "× Fk → [0, 1] defined by φ(f1, .",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "×fk // [0, 1] u // [0, 1] ,",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "×fk // [0, 1] u // [0, 1] ,",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "× Fk φ // [0, 1] .",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "For instance, the function u : [0, 1] → [0, 1] defined by u(r1, r2) = r1 · r2 (multiplication) is uniformly continuous with a modulus of continuity δ(ǫ) = ǫ 2 .",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "For instance, the function u : [0, 1] → [0, 1] defined by u(r1, r2) = r1 · r2 (multiplication) is uniformly continuous with a modulus of continuity δ(ǫ) = ǫ 2 .",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "Indeed, let ǫ > 0 and consider (r1, r2), (r ′ 1, r ′ 2) ∈ [0, 1].",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "1 ([18]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "A function class F is Probably Approximately Correct learnable under μ if there exists an algorithm L : ∪m∈N(X × [0, 1]) → F with the following property: for every ǫ > 0, for every δ > 0, there exists a M ∈ N such that for every f ∈ F , for every m ≥ M , for any x1, .",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "2 ([4]).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : "Talagrand had proved that a function class is a GlivenkoCantelli (GC) function class with regard to a single measure μ if and only if the class has no witness of irregularity, a property that involves shattering [13],[14].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "Talagrand had proved that a function class is a GlivenkoCantelli (GC) function class with regard to a single measure μ if and only if the class has no witness of irregularity, a property that involves shattering [13],[14].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 10,
      "context" : "Every GC function class is PAC learnable under μ [11], but the property of having no witness of irregularity is strictly stronger than PAC learnability.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "The paper [3] written by Bartlett et al.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : ",Fk) using a continuous function u : [0, 1] → [0, 1], a continuous logic connective.",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : ",Fk) using a continuous function u : [0, 1] → [0, 1], a continuous logic connective.",
      "startOffset" : 46,
      "endOffset" : 52
    } ],
    "year" : 2011,
    "abstractText" : "We begin this report by describing the Probably Approximately Correct (PAC) model for learning a concept class, consisting of subsets of a domain, and a function class, consisting of functions from the domain to the unit interval. Two combinatorial parameters, the Vapnik-Chervonenkis (VC) dimension and its generalization, the Fat Shattering dimension of scale ǫ, are explained and a few examples of their calculations are given with proofs. We then explain Sauer’s Lemma, which involves the VC dimension and is used to prove the equivalence of a concept class being distributionfree PAC learnable and it having finite VC dimension. As the main new result of our research, we explore the construction of a new function class, obtained by forming compositions with a continuous logic connective, a uniformly continuous function from the unit hypercube to the unit interval, from a collection of function classes. Vidyasagar had proved that such a composition function class has finite Fat Shattering dimension of all scales if the classes in the original collection do; however, no estimates of the dimension were known. Using results by Mendelson-Vershynin and Talagrand, we bound the Fat Shattering dimension of scale ǫ of this new function class in terms of the Fat Shattering dimensions of the collection’s classes. We conclude this report by providing a few open questions and future research topics involving the PAC learning model.",
    "creator" : "LaTeX with hyperref package"
  }
}