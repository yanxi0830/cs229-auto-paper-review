{
  "name" : "1312.6203.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Spectral Networks and Locally Connected Networks on Graphs",
    "authors" : [ "Joan Bruna", "Wojciech Zaremba" ],
    "emails" : [ "bruna@cims.nyu.edu", "woj.zaremba@gmail.com", "aszlam@ccny.cuny.edu", "yann@cs.nyu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Convolutional neural networks have been extremely succesful in machine learning problems where the coordinates of the underlying data representation have a grid structure (in 1,2 and 3 dimensions) and the data to be studied in those coordinates has translational equivariance/invariance w.r.t. the grid. On a regular grid, a CNN is able to exploit several structures that play nicely together to greatly reduce the number of parameters in the system:\n1. The translation structure, allowing the use of filters instead of generic linear maps. 2. The metric on the grid, allowing compactly supported filters. 3. The multiscale dyadic clustering of the grid, allowing subsampling.\nIf there are n coordinates on a grid of dimension d, a fully connected network would need O(n2) parameters for each feature map. Using arbitrary filters instead of generic fully connected layers reduces the complexity to O(n) parameters per feature map, as does using the metric structure by building a “locally connected” net [6, 11]. Using the two together gives O(1) parameters. Finally, using the multiscale dyadic clustering allows each succesive layer to use a factor of 2d less (spatial) coordinates per filter.\nIn this work, we will discuss constructions of deep neural networks on graphs other than regular grids, perhaps without any group structure. We will show that we are able to make use of (1) or (2) and (3) to get O(n) parameters per filter, and a factor less coordinates per level; and we will discuss the question of obtaining O(1) parameters per filter. These constructions allow efficient forward propagation and can be applied to datasets with very large number of coordinates.\nThe grid will be replaced by a weighted graph W (an m ×m symmetric and nonnegative matrix), with nodes indexed by a set Ω.\nar X\niv :1\n31 2.\n62 03\nv1 [\ncs .L\nG ]\n2 1"
    }, {
      "heading" : "1.1 Locality via W",
      "text" : "The notion of locality can be generalized easily in the context of a graph. Indeed, the weights in a graph determine a notion of locality. For example, a straightforward way to define neighborhoods on W is to set a threshold δ and take neighborhoods Nδ(j) = {i ∈ Ω : Wij > δ}. We can restrict attention to sparse “filters” with receptive fields given by these neighborhoods to get locally connected networks, reducing the number of parameters in a filter layer toO(n) if the neighborhoods are sparse."
    }, {
      "heading" : "1.2 Harmonic analysis on weighted graphs",
      "text" : "The combinatorial Laplacian L = D −W or graph Laplacian L = I − D−1/2WD−1/2 are generalizations of the Laplacian on the grid; and frequency and smoothness w.r.t. W are interrelated through these operators [1, 14]. For simplicity, here we use the combinatorial Laplacian. If f is an m vector, a natural definition of the smoothness functional ||∇f ||2W at a node i is\n||∇f ||2W (i) = ∑ j Wij [f(i)− f(j)]2,\nand ||∇f ||2W = ∑ i ∑ j Wij [f(i)− f(j)]2,\nWith this definition, the smoothest vector is a constant:\nv0 = arg min f∈Rm ||f ||=1\n||∇f ||2W = (1/ √ m)1m.\nEach succesive vi = arg min\nf∈Rm ||f ||=1 f⊥{v0,...,vi−1} ||∇f ||2W\nis an eigenvector of L, and the eigenvalues λi allow the smoothness of a vector f to be read off from the coefficients of f in [v0, ...vm−1]. Thus, just an in the case of the grid, where the eigenvectors of the Laplacian are the Fourier vectors, diagonal operators on the spectrum of the Laplacian modulate the smoothness of their operands. Moreover, using these diagonal operators reduces the number of parameters of a filter from m2 to m.\nThese three structures above are all tied together through Laplacian on the grid:\n1. Filters are multipliers on the eigenvalues of the Laplacian. 2. Functions that are smooth w.r.t. grid metric have coefficients with quick decay in the basis\nof eigenvectors. 3. The eigenvectors of the subsampled Laplacian are the low frequency eigenvectors of the\nLaplacian."
    }, {
      "heading" : "1.3 Multiresolution Analysis on Graphs",
      "text" : "CNNs reduce the size of the grid via pooling and subsampling layers. These layers are possible because of the natural multiscale clustering of the grid: they input all the feature maps over a cluster, and output a single feature for that cluster. On the grid, the dyadic clustering behaves nicely w.r.t. the metric and the Laplacian (and so with the translation structure). There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.r.t. Laplacian on the graph is still an open area of research. In this work we will use a naive agglomerative method."
    }, {
      "heading" : "1.4 Contributions",
      "text" : "Our contributions are summarized as follows:\n• we use robust/simple geometry to constructO(n) networks: we show that from a very weak geometric structure it is possible to obtain efficient architectures using O(n) parameters, that we validate on low-dimensional graph datasets.\n• We introduce a construction using O(1) parameters which we empirically verify, and we discuss its connections with a harmonic analysis problem on graphs."
    }, {
      "heading" : "2 Spatial and Spectral Constructions",
      "text" : "As above, let W be a weighted graph with index set denoted by Ω, and let V be the eigenvectors of the graph Laplacian, ordered by eigenvalue."
    }, {
      "heading" : "2.1 Spectral construction",
      "text" : "Given a weighted graph, we can try to generalize a convolutional net by operating on the spectrum of the weights (i.e. operating on the Laplacian). Suppose have a real valued nonlinearity h : R→ R; we can replace standard average pooling/subsampling by a dropping a set number of coefficients in V . That is, a layer of the network consists of\nyk+1 ← Dkh(UkFkyk) (2.1) with y0 = V Tx, and where x is the input into the network. Here\nFk =  Fk,1,1 ... Fk,1,fk−1... ... Fk,fk,1 ... Fk,fk,fk−1  , (2.2) where Fk,i,j is a dk × dk diagonal matrix with the spectral multipliers on the diagonal, fk is the number of filter maps on the kth level, and dk is the number of spectral values to keep. Uk is a block diagonal matrix with fk × fk blocks, each of which is the first dk columns of V , and Dk is a block diagonal matrix with fk × fk blocks, each of which is the first dk+1 rows of V T . This means that yk is each of the fk−1 filter outputs concatenated.\nIf the graph has an underlying group invariance this construction can discover it; the best example being the standard CNN; see 2.1.1. However, in many cases the graph does not have a group structure, or the group structure does not interact correctly with the Laplacian, and so we cannot think of each filter as passing a template across Ω and recording the correlation of the template with that location; Ω may not be homogenous in a way that allows this to make sense.\nThis construction can suffer from the fact that many reasonably nice graphs have meaningful eigenvectors only for the very top of the spectrum. Even when the individual high frequency eigenvectors are not meaningful, a cohort of high frequency eigenvectors may contain meaningful information. However this construction may not be able to access this information because it is nearly diagonal at the highest frequencies.\nFinally, it is not obvious how to do either the fprop or the backprop efficiently while applying the nonlinearity on the space side, as we have to make the expensive multiplications by Uk and Dk; and it is not obvious how to do standard nonlinearities on the spectral side. However, see 3.1"
    }, {
      "heading" : "2.1.1 Rediscovering standard CNN’s",
      "text" : "A simple, and in some sense universal, choice of weight matrix in this construction is the covariance of the data. Let X = (xk)k be the input data distribution, with xk ∈ Rn. If each coordinate j = 1 . . . n has the same variance,\nσ2j = E ( |x(j)− E(x(j))|2 ) ,\nthen diagonal operators on the Laplacian simply scale the principal components. While this may seem trivial, it is well known that the principal components of the set of images of a fixed size are (experimentally) the Fourier functions, organized by frequency. This can be explained by noticing that images are translation invariant, and hence the covariance operator Σ(j, j) = E ((x(j)− E(x(j)))(x(j′)− E(x(j′)))) satisfies Σ(j, j′) = Σ(j − j′), hence it is diagonalized by the Fourier basis. Moreover, since nearby pixels are more correlated than far away pixels, the principal components of the covariance are essentially ordered from low to high frequencies, which is consistent with the standard group structure of the Fourier basis. The upshot is that the the construction in 2.1 using the covariance as the data recovers a standard convolutional net without any prior knowledge.\n2.1.2 O(1) construction with smooth spectral multipliers\nIn the standard grid, we do not need a parameter for each Fourier function because the filters are compactly supported in space, but in this construction, each filter requires one parameter for each eigenvector on which it acts. Even if the filters were compactly supported in space in this construction, we still would not get less than O(n) parameters per filter because the spatial response would be different at each location.\nOne possibility for getting around this is to generalize the duality of the grid. On the Euclidian grid, the decay in the spatial domain is translated into smoothness in the Fourier domain, and viceversa. It results that a funtion x which is spatially localized has a smooth frequency response x̂ = V Tx. The eigenvectors of the Laplacian can be thought of as being arranged on a grid of the same size as the spatial coordinates.\nIn order to define a notion of smoothness, it is necessary to consider a topology in the domain of spectral coordinates. As previously discussed, on regular grids the topology is given by the notion of frequency, but this notion cannot be directly generalized to general graphs.\nA particularly simple and navie choice consists in choosing a 1 dimensional topology obtained by ordering the eigenvectors according to their eigenvalues, which will be tested in the experiments of section 4. In this setting, the diagonal of each filter Fk,i,j is parametrized by\ndiag(Fk,i,j) = αk,i,jK ,\nwhereK is a cubic spline kernel and αk,i,j are the spline coefficients. If one seeks to have filters with constant spatial support (ie, whose support is independent of the input size n), it follows that one can choose a sampling step ∆ = O(n) in the spectral domain, which results in a constant number δ = n∆−1 = O(1) of coefficients αk,i,j per filter.\nAlthough results from section 4 seem to indicate that the 1d topology given by the spectrum of the Laplacian is efficient at creating spatially localized filters, a fundamental question is how to define a dual graph capturing the geometry of spectral coordinates. A possible algorithmic stategy could be to consider an input distributionX = (xk)k consisting on spatially localized signals and to construct a dual graph Ŵ by measuring the similarity of in the spectral domain: X̂ = V TX . The similarity could be measured for instance with E((|x̂| − E(|x̂)|))T (|x̂| − E(|x̂|))."
    }, {
      "heading" : "2.2 Spatial construction (locally connected networks)",
      "text" : "Instead of generalizing a CNN by finding spectral multipliers, one can work entirely in space. We will need a multiscale clustering Ωk and neighborhoods at each scale, given by the rows of the matrices Wk (the rows and columns of Wk are indexed by the clusters in Ωk−1, where Ω0 = Ω. With these in hand, a layer of the net is\nxk+1 ← Lkh(Fkxk). (2.3)\nIn this construction, xk is dk−1 × fk−1 vector, where dk is the number of clusters at level k, fk is the number of filters at level k, and xk is the filter responses vertically concatenated. Furthermore,\nFk =  Fk,1,1 ... Fk,1,fk−1... ... Fk,fk,1 ... Fk,fk,fk−1  , (2.4) where Fk,i,j is a dk−1 × dk−1 sparse matrix with nonzero entries in the same locations as wk−1. h is again a componentwise nonlinearity, and Lk outputs the result of a pooling operation over each cluster in Ωk.\nIn the current code, to build Wk and Ωk we use the following construction: Ak(i, j) = ∑\ns∈Ωk(i) ∑ t∈Ωk(j) Wk−1(s, t), (2.5)\nand Wk = rownormalize(A), and Ωk is found as an covering for Wk."
    }, {
      "heading" : "3 Relationship with previous work",
      "text" : "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7]. A wavelet basis on a grid, in the language of neural networks, is a linear autoencoder with certain provable regularity properties (in particular, when encoding various classes of smooth functions, sparsity is gauranteed). The forward propagation in a classical wavelet transform strongly resembles the forward propagation in a neural network, except that there is only one filter map at each layer (and it is usually the same filter at each layer), and the output of each layer is kept, rather than just the output of the final layer. Classically, the filter is not learned, but constructed to facilitate the regularity proofs.\nIn the graph case, the goal is the same; except that the smoothness on the grid is replaced by smoothness on the graph. As in the classical case, most works have tried to construct the wavelets explicitly (that is, without learning), based on the graph, so that the corresponding autencoder has the correct sparsity properties. In this work, and the recent work [12], the “filters” are constrained by construction to have some of the regularity properties of wavelets, but are also trained so that they are appropriate for a task separate from (but perhaps related to) the smoothness on the graph. Whereas [12] still builds a (sparse) linear autoencoder that keeps the basic wavelet transform setup, this work focuses on nonlinear constructions; and in particular, tries to build analogues of CNN’s."
    }, {
      "heading" : "3.1 Multigrid",
      "text" : "We could improve both constructions, and to some extent unify them, with a multiscale clustering of the graph that plays nicely with the Laplacian. As mentioned before, in the case of the grid, the standard dyadic cubes have the property that the subsampling the Fourier functions on the grid to a coarser grid is the same as finding the Fourier functions on the coarser grid. This property would eliminate the annoying necessity of mapping the spectral construction to the finest grid at each layer to do the nonlinearity; and would allow us to interpret (via interpolation) the local filters at deeper layers in the spatial construction to be low frequency.\nThis kind of clustering is the underpinning of the multigrid method for solving discretized PDE’s (and linear systems in general) [13]. There have been several papers extending the multigrid method, and in particular, the multiscale clustering(s) associated to the multigrid method, in settings more general than regular grids, see for example [10, 9] for situations as in this paper, and see [13] for the algebraic multigrid method in general. In this work, for simplicity, we use a naive multiscale clustering on the space side construction that is not guaranteed to respect the original graph’s Laplacian, and no explicit spatial clustering in the spectral construction."
    }, {
      "heading" : "4 Numerical Experiments",
      "text" : "We show experiments on two variations of the MNIST data set. In the first, we subsample the normal 28× 28 grid to get 400 coordinates. These coordinates still have a 2d structure, but it is not possible to use standard convolutions. We then make a dataset by placing d = 4096 points on the 2d unit sphere to get the set S, and project random MNIST images onto a random rotation of S via bicubic interpolation. In all the experiments, we use Rectified Linear Units as nonlinearities. We train the models with cross-entropy loss, using a fixed learning rate of 0.1 with momentum 0.9."
    }, {
      "heading" : "4.1 Subsampled MNIST",
      "text" : "We apply the constructions from sections 2.1 and 2.2 to the subsampled MNIST dataset. Figure 4.1 shows examples of the resulting input signals, and figures 4.1, 4.1 show the hierarchical clustering constructed from the graph and some eigenfunctions of the graph Laplacian, respectively. The performance of various graph architectures is reported in Table 1. To serve as a baseline, we compute the standard Nearest Neighbor classifier, which performs slightly worse than in the full MNIST dataset (2.8%). A two-layer Fully Connected neural network reduces the error to 1.8%. The geometrical structure of the data can be exploited with the CNN graph architectures. Local Receptive Fields adapted to the graph structure outperform the fully connected network. In particular, two layers of filtering and max-pooling define a network which efficiently aggregates information to the final classifier. The spectral construction performs slightly worse on this dataset. We considered\na frequency cutoff of N/2 = 200. However, the frequency smoothing architecture described in section 2.1.2, which contains the smallest number of parameters, outperforms the regular spectral construction.\nThese results can be interpreted as follows. MNIST digits are characterized by localized oriented strokes, which require measurements with good spatial localization. Locally receptive fields are constructed to explicitly satisfy this constraint, whereas in the spectral construction the measurements are not enforced to become spatially localized. Adding the smoothness constraint on the spectrum of the filters improves classification results, since the filters are enforced to have better spatial localization.\nThis fact is illustrated in figure 4.1. We verify that Locally Receptive fields encode different templates across different spatial neighborhoods, since there is no global strucutre tying them together. On the other hand, spectral constructions have the capacity to generate local measurements that generalize across the graph. When the spectral multipliers are not constrained, the resulting filters tend to be spatially delocalized, as shown in panels (c)-(d). This corresponds to the fundamental limitation of Fourier analysis to encode local phenomena. However, we observe in panels (e)-(f) that a simple smoothing across the spectrum of the graph Laplacian restores some form of spatial localization and creates filters which generalize across different spatial positions, as should be expected for convolution operators."
    }, {
      "heading" : "4.2 MNIST on the sphere",
      "text" : "We test in this section the graph CNN constructions on another low-dimensional graph. In this case, we lift the MNIST digits to the sphere. The dataset is constructed as follows. We first sample 4096 random points S = {sj}j≤4096 from the unit sphere S2 ⊂ R3. We then consider a reference frame E = (e1, e2, e3) with ‖e1‖ = 1 , ‖e2‖ = 2 , ‖e3‖ = 3 and a random covariance operator Σ = (E + W )T (E + W ), where W is a Gaussian iid matrix with variance σ2 < 1. For each signal xi from the original MNIST dataset, we sample a covariance operator Σi from the former distribution and consider its PCA basis Ui. This basis defines a point of view and in-plane rotation which we use to project xi onto S. Figure 4.2 shows examples of the resulting projected digits. Since the digits ‘6’ and ‘9’ are equivalent modulo rotations, we remove the ‘9’ from the dataset.\nFigure 4.2 shows two eigenvectors of the graph Laplacian.\nWe first consider “mild” rotations with σ2 = 0.2. The effect of such rotations is however not negligible. Indeed, table 2 shows that the Nearest Neighbor classifer performs considerably worse than in the previous example. All the neural network architectures we considered significatively improve over this basic classifier. Furthermore, we observe that both convolutional constructions match the fully connected constructions with far less parameters (but in this case, do not improve its performance). Figure 4.2 displays the filters learnt using different constructions. Again, we verify that the smooth spectral construction consistently improves the performance, and learns spatially localized filters, even using the naive 1d organization of eigenvectors, which detect similar features across different locations of the graph (panels (e)-(f)).\nFinally, we consider the uniform rotation case, where now the basis Ui is a random basis of R3. In that case, the intra-class variability is much more severe, as seen by inspecting the performance of the Nearest neighbor classifier. All the previously described neural network architectures significantly improve over this classifier, although the performance is notably worse than in the mild rotation scenario. In this case, an efficient representation needs to be fully roto-translation invariant. Since\nthis is a non-commutative group, it is likely that deeper architectures perform better than the models considered here."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Using graph-based analogues of convolutional architectures can greatly reduce the number of parameters in a neural network without worsening (and often improving) the test error, while simultaneously giving a faster forward propagation. These methods can be scaled to data with a large number of coordinates that have a notion of locality.\nThere is much to be done here. We suspect with more careful training and deeper networks we can consistently improve on fully connected networks on “manifold like” graphs like the sampled sphere. Furthermore, we intend to apply these techniques to less artifical problems, for example, on netflix like recommendation problems where there is a biclustering of the data and coordinates. Finally, the fact that smoothness on the naive ordering of the eigenvectors leads to improved results and localized filters suggests that it may be possible to make “dual” constructions with O(1) parameters per filter in much more generality than the grid."
    } ],
    "references" : [ {
      "title" : "Diffusion wavelets",
      "author" : [ "R.R. Coifman", "M. Maggioni" ],
      "venue" : "Appl. Comp. Harm. Anal., 21(1):53–94, July",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Graph wavelets for spatial traffic analysis",
      "author" : [ "Mark Crovella", "Eric D. Kolaczyk" ],
      "venue" : "In INFOCOM,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Weighted graph cuts without eigenvectors 10  Table 3: Classification results on the MNIST-sphere dataset generated using uniformly random rotations, for different architectures method Parameters Error Nearest Neighbors  NA 80 4096-FC2048-FC512-9",
      "author" : [ "Inderjit S. Dhillon", "Yuqiang Guan", "Brian Kulis" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2048
    }, {
      "title" : "Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning",
      "author" : [ "Matan Gavish", "Boaz Nadler", "Ronald R. Coifman" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Emergence of complex-like cells in a temporal product network with local receptive fields",
      "author" : [ "Karol Gregor", "Yann LeCun" ],
      "venue" : "CoRR, abs/1006.0448,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Multiresolution signal processing for meshes",
      "author" : [ "I. Guskov", "W. Sweldens", "P. Schröder" ],
      "venue" : "Computer Graphics Proceedings (SIGGRAPH 99), pages 325–334,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Metis - unstructured graph partitioning and sparse matrix ordering system, version 2.0",
      "author" : [ "George Karypis", "Vipin Kumar" ],
      "venue" : "Technical report,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1995
    }, {
      "title" : "Efficient multilevel eigensolvers with applications to data analysis tasks",
      "author" : [ "D. Kushnir", "M. Galun", "A. Brandt" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(8):1377–1391,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast multiscale clustering and manifold identification",
      "author" : [ "Dan Kushnir", "Meirav Galun", "Achi Brandt" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Tiled convolutional neural networks",
      "author" : [ "Quoc V. Le", "Jiquan Ngiam", "Zhenghao Chen", "Daniel Chia", "Pang Wei Koh", "Andrew Y. Ng" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Wavelets on graphs via deep learning",
      "author" : [ "Raif M. Rustamov", "Leonidas Guibas" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Technical Report 149,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Using arbitrary filters instead of generic fully connected layers reduces the complexity to O(n) parameters per feature map, as does using the metric structure by building a “locally connected” net [6, 11].",
      "startOffset" : 198,
      "endOffset" : 205
    }, {
      "referenceID" : 9,
      "context" : "Using arbitrary filters instead of generic fully connected layers reduces the complexity to O(n) parameters per feature map, as does using the metric structure by building a “locally connected” net [6, 11].",
      "startOffset" : 198,
      "endOffset" : 205
    }, {
      "referenceID" : 11,
      "context" : "W are interrelated through these operators [1, 14].",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "In this work, and the recent work [12], the “filters” are constrained by construction to have some of the regularity properties of wavelets, but are also trained so that they are appropriate for a task separate from (but perhaps related to) the smoothness on the graph.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "Whereas [12] still builds a (sparse) linear autoencoder that keeps the basic wavelet transform setup, this work focuses on nonlinear constructions; and in particular, tries to build analogues of CNN’s.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "There have been several papers extending the multigrid method, and in particular, the multiscale clustering(s) associated to the multigrid method, in settings more general than regular grids, see for example [10, 9] for situations as in this paper, and see [13] for the algebraic multigrid method in general.",
      "startOffset" : 208,
      "endOffset" : 215
    }, {
      "referenceID" : 7,
      "context" : "There have been several papers extending the multigrid method, and in particular, the multiscale clustering(s) associated to the multigrid method, in settings more general than regular grids, see for example [10, 9] for situations as in this paper, and see [13] for the algebraic multigrid method in general.",
      "startOffset" : 208,
      "endOffset" : 215
    } ],
    "year" : 2017,
    "abstractText" : "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with O(1) parameters, resulting in efficient deep architectures.",
    "creator" : "LaTeX with hyperref package"
  }
}