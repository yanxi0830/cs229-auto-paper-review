{
  "name" : "1206.4658.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dirichlet Process with Mixed Random Measures: A Nonparametric Topic Model for Labeled Data",
    "authors" : [ "Dongwoo Kim", "Suin Kim" ],
    "emails" : [ "dw.kim@kaist.ac.kr", "suin.kim@kaist.ac.kr", "alice.oh@kaist.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Topic models such as latent dirichlet allocation (LDA) (Blei et al., 2003) have been extended to incorporate side information such as authorship (Rosen-Zvi et al., 2004), spatial or temporal coordinates (Wang & Grimson, 2007; Wang et al., 2008), and document labels (Ramage et al., 2009). Most of these models are parametric topic models, and they cannot be simply converted to nonparametric counterparts which generally have various advantages over parametric models. In the Bayesian nonparametric (BNP) literature\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\non Dirichlet processes (DP), modeling unknown densities with covariates has been often done with dependent Dirichlet Processes (DDP), but extending DDP for topic modeling requires more complex model settings and posterior inferences (Srebro & Roweis, 2005).\nIn this paper, we propose a novel nonparametric topic model, Dirichlet process with mixed random measures (DP-MRM) for documents with an arbitrary amount of discrete side information such as labels. DP-MRM can be seen as a nonparametric extension of LabeledLDA (L-LDA) (Ramage et al., 2009) in terms of defining topic distributions over labels. Recent research shows that incorporating label information into topic models has advantages for topic interpretation as well as other practical uses such as user profiling in social media (Ramage et al., 2010). However, L-LDA assumes that each label corresponds to a single multinomial (i.e., topic), and a document is only generated by the topics of the observed labels. Consequently, the model imposes an overly limiting restriction on the topics with which to represent the documents. While L-LDA models each label with a single multinomial, DP-MRM models each label with a random measure which is defined over the entire topic space.\nThere are several supervised topic models including sLDA (Blei & McAuliffe, 2007), discLDA (LacosteJulien et al., 2008), and medLDA (Zhu et al., 2009), that also model data with labels. There are two major differences between those models and DP-MRM. First, in the former models which are designed specifically for classification, each label acts as the supervisor for learning. In L-LDA and DP-MRM which are designed with the focus on understanding the meaning of each label in terms of the latent topics, each label actually is the label for one (in L-LDA) or a set of (DP-MRM) topic(s). Second, the former models are restricted to\nmodeling data with one label per document and cannot model documents with multiple labels. To illustrate this second point, we evaluate DP-MRM on data with single labels as well as multiple labels.\nAnother view of DP-MRM is that it is a more general case of the HDP (Teh et al., 2006). Modeling the corpus with our model using a single label for all documents would produce the same results as the HDP. Viewed this way, DP-MRM can be used instead of the HDP in many BNP models that are extensions of HDP. We show an example of this by incorporating the ddCRP (Blei & Frazier, 2011) into our model for the task of image segmentation as done in rddCRP (Ghosh et al., 2011).\nThe paper is organized as follows. In section 2, we describe DP-MRM along with the stick-breaking and Pólya urn perspectives. In section 3, we derive a sampling method for the latent variables based on Gibbs sampling. In section 4, we demonstrate our approach on labeled documents for single-labeled and multi-labeled corpora and compare the performance of our model by label prediction and heldout likelihood against LDA-SVM and L-LDA. In section 5, we present a modification of our model for image segmentation and compare the performance with nCuts (Shi & Malik, 2000) and rddCRP (Ghosh et al., 2011) quantitatively and qualitatively."
    }, {
      "heading" : "2. Dirichlet Process with Mixture of Random Measures",
      "text" : "In this section, we describe our model, Dirichlet process with mixed random measures (DP-MRM) model. We first review the generative process of L-LDA, and then we show how DP-MRM incorporates label information within the BNP framework based on Dirichlet Processes (DP). Lastly, we present the stick breaking process and the Pólya urn scheme for DP-MRM."
    }, {
      "heading" : "2.1. Model Definition",
      "text" : "L-LDA is a supervised version of LDA for modeling multi-labeled documents. The generative process of L-LDA starts with a definition of a document specific function label(j), which returns a set of observed label indices for document j. Then, for each document j, a multinomial distribution θj over topics is randomly sampled from a Dirichlet with parameter rjα, where rj is a K dimensional vector whose kth value is 1 if k ∈ label(j) and 0 if k /∈ label(j). Then, to generate the word i, a topic zji is chosen from this topic distribution, and a word, xji, is generated by randomly sampling from a topic-specific multinomial distribu-\ntion φzji . By using a document specific indicator vector rj , the model can specify the topic proportion of document θj over the |label(j)|−1 dimension simplex.\nWe now describe the generative process of Dirichlet process with mixed random measures. First, we define a DP distributed random measure G10, ..., G K 0 over K possible labels with a base distribution H as follows:\nH | β ≡ Dir(β) Gk0 | γk, H ∼ DP(γk, H), (1)\nwhere the base distribution H is assumed to be a symmetric Dirichlet distribution over the entire vocabulary dimension, and γk controls the variability of G k 0 . By defining one random measure per label, we place an infinite topic space for each label. For each document j, another DP distributed random measure Gj is defined with a mixture of labeled-random measure as follows:\nλj ∼ Dir(rjη) Gj |label(·), α, λj ∼ DP(α, ∑\n{k;label(j)}\nλjkG k 0) (2)\nwhere α is a concentration parameter, λjk is a mixing proportion of Gk0 , and η controls the sparsity of λj . DP-MRM uses a mixture of random measures,∑ k λjkG k 0 , as the base distribution of Gj , the document specific measure. For the mixing proportion λjk of each Gk0 , we sample λj from a symmetric Dirichlet prior parameterized by rj and η. Hence, with the observed labels label(j), rj selectively specifies the mixing proportions of Gk0 over the |label(j)| − 1 dimensional simplex.\nFor each word xji in document j, the probability of drawing a word xji is parameterized by a random variable θji drawn from Gj with some family of distribution F . It is typically assumed to be a multinomial distribution,\nθji | Gj ∼ Gj xji | θji ∼ F (θji) F (θji) ≡ Mult(θji), (3)\nwhich makes F to be conjugate to the base distribution H, and so it is possible to integrate out the factors θji.\nAs a result of the construction, the model chooses an appropriate number of topics for each label. Note that HDP can be viewed as a specialized instance of our model (Teh et al., 2006), where we assume there is a single ‘unknown’ label for all documents. Then the overall corpus is defined by a set of topics from the single ‘unknown’ label, Gunknown0 ∼DP(γ,H), and the random measure for document j is drawn from Gj ∼DP(α,Gunknown0 ). A similar idea of using a mixture of random measures was proposed in (Antoniak,\n1974), but our model extends that idea into a hierarchical construction for the grouped clustering problem."
    }, {
      "heading" : "2.2. Construction and Predictive Distribution",
      "text" : "We now describe two perspectives that are important for the inference algorithms for DP-MRM: the stick breaking process and the Pólya urn scheme.\nStick breaking process The stick breaking process is a constructive definition for generating a Dirichlet process (Sethuraman, 1991). Same as the model definition in the previous section, the stick breaking process can be divided into two level DPs. For the first level random measure Gk0 , we follow the general stick breaking process, which is given by the following conditional distributions:\nvkl ∼ Beta(1, γk) πkl = vkl l−1∏ d=1 (1− vkd)\nφkl ∼ H Gk0 = ∞∑ l=0 πkl δφkl , (4)\nwhere δ is a Dirac delta measure. A general stick breaking process can be seen as two independent sequences of deciding the stick length πl by samples from i.i.d. Beta trials and deciding the atom of the lth stick φl by i.i.d. samples from H.\nThe second level stick breaking construction is given by the following conditional distributions:\nλj ∼ Dir(rjη)\nwjt ∼ Beta(1, α) πjt = wjt t−1∏ d=1 (1− wjd) kjt ∼ Mult(λj) ψjt ∼ G kjt 0\nGj = ∞∑ t=0 πjtδψjt . (5)\nDeciding the length of each stick is the same as the general stick breaking process, but assigning atoms for\neach divided stick must be changed because there are K random measures for drawing ψjt. We introduce kjt as an indicator to G k 0 where atom ψjt is drawn.\nWe let θji denote the random variable drawn from Gj , ψjt the atom of Gj , and φ k l the atom of G k 0 . Note that each θji is associated with one ψjt (i.e., θji = ψjt), and each ψjt is associated with one φ k l , thus they form a shared structure across the corpus. Figure 1 visualize a sharing structure between first and second level DPs.\nPólya urn scheme A posterior perspective of the DP is the Pólya urn scheme which shows that draws from the DP are discrete and exhibit a clustering property. As Blackwell and MacQueen showed (Blackwell & MacQueen, 1973), our model can also be formed as a successive conditional distribution of θji given θj1, ..., θji−1.\nLet njt be the number of words for which factor θji corresponds to ψjt in document j, and mjkl be the number of ψjt such that ψjt = φ k l . Then the conditional distribution of θji given θj1, ..., θji−1, G 1 0, ..., G K 0 , and α, with Gj and λj marginalized out, is\nθji|θj1, ..., θji−1, α, η,G10, ..., GK0 (6) ∼ ∑ t njt i− 1 + α δψjt + α i− 1 + α ∑ k mjk· + rjkη mj·· + |rj |η Gk0 ,\nwhere |rj | is the number of 1’s in rj , and rjk is 1 if label k has been observed in document j. θji can be sampled from the first term of RHS or the second term of RHS. When θji is sampled from the first term, then it corresponds to one existing ψjt, and when it is sampled from the second term, we choose Gk0 to draw θji with probability proportional to mjk· + η. After that, we can marginalize out Gk0 to proceed further and get the conditional distribution\nψjt|ψ11, ..., ψjt−1, γk, Hk ∼ ∑ k m·kl m·k· + γk δφkl + γk m·k· + γk Hk. (7)"
    }, {
      "heading" : "3. Inference via Gibbs Sampling",
      "text" : "We propose a Gibbs sampler for DP-MRM, a Pólya urn scheme based on the marginalization of unknown dimensions (Escobar & West, 1995). For the collapsed Gibbs sampler, we marginalize out factors, θ, ψ, φ, mixing proportions, λ, and random probability measures, Gj , G k 0 . As a result, we only need to sample the index of each latent variable. Let tji be the index variable such that ψjt = θji, and kjt be the index variable such that ψjt ∼ Gk0 , and ljt be the index variable such that ψjt = φ kjt l . Let njt be the number of θji such that θji = ψjt, and let mjkl be the number of\nψjt such that ψjt = φ k l . We use fkl(xji) to denote the conditional density of x under mixture component l of random measure Gk0 , given all items except xji,\nfkl(xji) =\n∫ f(xji|φkl ) ∏ xj′i′∈xkl\nf(xj′i′ |φkl )h(φkl )dφkl∫ ∏ xj′i′∈xkl f(xj′i′ |φkl )h(φkl )dφkl ,\nwhere xkl = {xji; kjtji = k, ljtji = l}.\nSampling t : The conditional density of word xji being assigned to ψjt is\np(tji = t|t−ji, rest)\n=\n{ njt·\nnj··+α fkjtljt(xji) existing t\nα nj··+α\nΓ(xji) new t, (8)\nwhere Γ(xji) = ∑K k=1 mjk·+η mj··+Kη ∑L l=1 m·kl m·k·+γk\nfkl(xji) + γk\nm·k·+γk fklnew(xji).\nSampling k and l : When new t is sampled, we need to sample kjtnew and ljtnew . However, sampling k and l cannot be done independently because given l the probability of k is always zero except one. The joint conditional density of k and l is\np(kjt = k, ljt = l|k−jt, l−jt, rest) (9)\n∝ mjk· + η mj·· +Kη × m·kl m·k· + γk fkl(xji) existing l\np(kjt = k, ljt = lnew|k−jt, l−jt, rest) (10)\n∝ mjk· + η mj·· +Kη × γk m·k· + γk fklnew(xji) new l.\nSampling k and l of existing t changes the component memberships of all data items xjt = {xji; tji = t}, and this sampling can be done with the conditional distribution of k and l given xjt."
    }, {
      "heading" : "4. Application with Labeled Documents",
      "text" : "We measure the performance of DP-MRM with three experiments. First, we compare the label prediction performance of DP-MRM and LDA-SVM on singlelabeled documents. Then, we compare the label prediction performance of DP-MRM and L-LDA on multilabeled documents. Finally, we compare the predictive\nperformance of DP-MRM and L-LDA on heldout data. For the label prediction experiments, we take a semisupervised approach: divide the corpus into training and test sets, infer the posterior distribution of the training set with the observed labels (i.e. rjk = 1 only when k ∈ label(j)), and infer the posterior distribution of the test set with all possible K labels (i.e. rjk = 1 for all k).\nFor all evaluations, we run each model ten times with 5,000 iterations, the first 3,000 as burn-in and then using the samples thereafter with gaps of 100 iterations. For sampling the hyperparameters, we place Gamma(1,1) priors for γk, and α, and set β to 0.5."
    }, {
      "heading" : "4.1. Single-Labeled Documents",
      "text" : "DP-MRM was designed to model multi-labeled documents, but it assumes that a label generates multiple topics, so this flexible assumption allows DP-MRM to be used for modeling single-labeled documents as well. Note that L-LDA for single-labeled documents would assign every word in a document to a single topic, and the document would thus be modeled as a mixture of unigrams (i.e., naive Bayes).\nTo measure the classification performance, we trained our model with five comp subcategories of newsgroup documents (20NG)1. Table 1 shows the details of our datasets. 90% of the documents were used with the labels, and the remaining 10% of documents were used without the labels. We classified each of the test documents by the label with the most number of words assigned. As a baseline, we trained a multi-class SVM with the topic proportions inferred by LDA (Blei et al., 2003). MedLDA (Zhu et al., 2009), one of supervised topic model, also used for the comparision. The results, shown in Figure 2, display a significant improvement of our model over the LDA-SVM approach and MedLDA."
    }, {
      "heading" : "4.2. Multi-Labeled Documents",
      "text" : "We compared the performance of L-LDA and DPMRM using two multi-labeled corpora: the Ohsumed dataset2, which is a subset of the MEDLINE corpus consisting of medical journals, and RCV1-V2 dataset (Lewis et al., 2004), a corpus of Reuters news articles. We randomly sampled a subset of each corpus, and the detailed descriptions are shown in Table 1. Again, 90% of documents were used with the labels, and the rest 10% of documents were used without the labels.\nL-LDA provides a systematic way of naming the dis-\n1http://people.csail.mit.edu/jrennie/20Newsgroups/ 2http://ir.ohsu.edu/ohsumed/ohsumed.html\nTable 2. Topics discovered by L-LDA and DP-MRM for the Infant label of the Ohsumed dataset, a corpus of medical journal articles, and for the Corporate/Industrial label of the RCV news articles corpus. We show the top ten probability words for each topic. L-LDA discovers exactly one topic per label, but DP-MRM discovers several topics per label.\nInfant Corporate/Industrial L-LDA DP-MRM L-LDA DP-MRM\nchildren children colon tumor compan million oil shar ton airlin infect infect aeruginosa patient million profit pow compan million air month infant express leukemia percent percent ga bank percent carg patient month gene cell market half compan percent produc flight\nag ag type chemotherapi produc expect produc million export servic infant antibodi dna dose stat compan plant invest crop airport studi hiv mutat therapi bank billion operat stock wheat carri vaccin vaccin ha-ra receiv invest result refin market grain plan viru viru excret treatment plan market unit stat juli operat antibodi test urinari remiss billion shar million plan sugar aircraft\n0\n0.25\n0.50\n0.75\n1.00\nos.ms-windows.misc sys.ibm.pc.hardware sys.mac.hardware windows.x graphics\nSingle label prediction\nLDA50-SVM LDA100-SVM MedLDA-50 MedLDA-100 MDPM\nFigure 2. Accuracies of DP-MRM, MedLDA, and LDASVM on classification of 20NG. DP-MRM outperforms LDA-SVM and MedLDA on average.\ncovered topics, and thus increases the interpretability of them. However, the assumption that a document is generated from a subset of topics specified by the observed labels limits the expressiveness of the model. DP-MRM was designed to keep the benefits of L-LDA while increasing the expressiveness, and we can see the consequences of the design in the discovered topics shown in Table 2. The table shows one label from each corpus and the corresponding topics. DP-MRM discovered multiple topics for the labels ‘Infant’ and ‘Corporate/Industrial’, and these are more detailed topics than the single topics discovered by L-LDA.\nFor the classification of multi-labeled documents based on the posterior samples, we counted the number of words assigned to each measure Gk0 and classified as label k with various threshold cuts based on normalized counts. We scored each model based on Micro F1 and Macro F1 measures. Micro F1 accounts for the proportion of each class, so large classes affect its results, whereas macro F1 assigns equal weights to all classes. Table 3 shows the classification results with different cuts, and our model performs better than LLDA in terms of micro average, but in macro average,\nthere are inconsistencies between the different cuts. In general, DP-MRM shows more stable performance with respect to the cuts, whereas L-LDA shows variable results depending on the cut."
    }, {
      "heading" : "4.3. Predictive Performance",
      "text" : "To compare the model fit, we measure the predictive performance of our model and L-LDA with heldout likelihood of the test set. For each model, posterior sampling was done with 90% of the words in each document while the test set performance was evaluated on the remaining 10% of the words. Given S samples from the posterior, the test set likelihood for our model is computed as follows:\np(xtest) = ∏\nji∈xtest\n1\nS S∑ s=1 K∑ k=1 ∑ l θ (s) jklψ (s) klxji\nTable 3. Macro and micro F1 averages of L-LDA and DP-MRM for the two multi-label datasets. DP-MRM consistently performs better than L-LDA for micro F1, but not for macro F1.\nRCV Ohsumed Micro Average Macro Average Micro Average Macro Average\nCut DP-MRM L-LDA DP-MRM L-LDA DP-MRM L-LDA DP-MRM L-LDA 0.001 0.511 0.282 0.257 0.172 0.392 0.345 0.223 0.257 0.050 0.520 0.449 0.265 0.285 0.389 0.382 0.223 0.263 0.100 0.520 0.473 0.266 0.322 0.382 0.364 0.220 0.250 0.200 0.509 0.464 0.264 0.331 0.362 0.326 0.207 0.223 0.300 0.487 0.434 0.254 0.315 0.334 0.287 0.189 0.195 0.500 0.424 0.355 0.220 0.261 0.262 0.206 0.145 0.137\nθ (s) jkl = njkl· + α{m·kl/(m·k· + γk)} njk·· + α\nψ (s) klxij\n= n·klxij + β\nn·kl· +Wβ ,\nwhere njklx is the number of words x corresponding to φkl in document j, and W is the vocabulary size. The test set likelihood for L-LDA was computed as follows:\np(xtest) = ∏\nji∈xtest\n1\nS S∑ s=1 K∑ k=1 θ (s) jk ψ (s) kxji\nθ (s) jk =\nnjk· + α\nnj·· +Kα\nψ (s) kxji\n= n·kxji + β\nn·k· +Wβ ,\nwhere K is the total number of labels. Figure 3 shows the test set per-word log likelihood of both model with RCV dataset, our model performs better than L-LDA across ten folded dataset consistently."
    }, {
      "heading" : "5. Image Segmentation with ddCRP",
      "text" : "We describe an extension of DP-MRM, built by incorporating ddCRP, a nonparametric Bayesian prior that accounts for spatial dependencies, into DP-MRM. This illustrates the generality of DP-MRM that it may serve as a replacement for HDP for data with side information. We test this DP-MRM-ddCRP model on the task of image segmentation for multi-labeled images without manually segmented training data.\nImage segmentation is often done with manually segmented and labeled data (He et al., 2004; Gould et al., 2009). DP-MRM can also perform supervised segmentation, but such data are harder to obtain, whereas image collections with multiple labels and no segmentation are relatively easy to obtain (e.g., Picasa or Flickr). One recent paper has shown a Bayesian model for simultaneous image segmentation and annotation (Du et al., 2009) using a logistic stick-breaking process. While that model is specialized for image understand-\ning, DP-MRM is a general framework for modeling multi-labeled data including documents and images."
    }, {
      "heading" : "5.1. Incorporating ddCRP into DP-MRM",
      "text" : "The Chinese restaurant process (CRP) is an alternative formulation of the DP. CRP forms a clustering structure of customers by assigning each customer to an existing or a new table. ddCRP, however, forms a clustering structure of customers by linking customers, accounting for the distances between them; customers who are relatively close to each other are likely to be linked together than those who are far apart. Let ci be the assignment of customer i to the other customers, then the distribution of the customer assignment is\np(ci = i ′|c−i, f,D, α) ∝ { f(dii′) i 6= i′ α i = i′ , (11)\nwhere dii′ is the distance between customer i and i ′, and f(dii′) is a decay function of the distance which mediates how the distances affect the resulting distribution over the partitions. There are many possible ways of defining the decay function, and in this paper, we follow (Ghosh et al., 2011) and use a window decay function which measures the distance between superpixels as a hop distance between them.\nBased on the conditional distribution of assignments, the Pólya urn scheme for the combined model is:\nθji|θj1, ..., θji−1, α, η,G10, ..., GK0 (12)\n∼ i−1∑ i′ f(dii′) f isum + α δθji′ +\nα\nf isum + α ∑ k mjk· + rjkη mj·· + |rj |η Gk0 ,\nwhere f isum = ∑ i′ 6=i f(dii′). This equation is similar to Equation (6), but we modify the equation based on the window decay function.\nFor posterior inference, we modify the posterior sampling Equation (8) based on the customer assignment scheme, but the changes only affect the local sampling results (within the document level), and can be employed by the algorithm for ddCRP mixture in previ-\nous work. The sampling scheme based on link structure among customers enhances the rapid mixing of sampler. See (Blei & Frazier, 2011) for a more detailed explanation of posterior inference."
    }, {
      "heading" : "5.2. Image Segmentation with Multiple Labels",
      "text" : "For image segmentation, we use the eight scene categories in (Oliva & Torralba, 2001) which are fully segmented and labeled by human subjects and available from the LabelMe dataset (Russell et al., 2008). A widely used method for representing images for inference is a codebook of images (Fei-Fei & Perona, 2005). To generate the codebook, each image is first divided into approximately 1,000 superpixels using the normalized cut algorithm (Shi & Malik, 2000). Each superpixel is described via local texton histogram (Martin et al., 2004) and HSV color histogram. By using kmeans, we quantize these histograms into 128 bins, and superpixel i in image j is summarized via these codewords xji = {xtji, xcji} indicating its texture xtji and color xcji. The base distribution H should be defined as H ≡ Dir(ηt)⊗Dir(ηc) for image segmentation.\nFigure 4 shows some examples of the labeled objects from posterior samples where DP-MRM segments images into objects and labels each object. We note again that we do not give any pixel-level information for each object during the posterior inference, but our model can successfully segment images and label segments simultaneously. The results indicate that DP-MRM succeeds in inferring both the segments and the corresponding labels by capturing the co-occurrence patterns of superpixels and labels.\nFigure 6 shows some examples of the image segmen-\ntation results comparing the original images, human segmented images, and DP-MRM segmented images. Figure 5 shows the quantitative performance of the segmentation via Rand Index, comparing DP-MRM with rddCRP (Ghosh et al., 2011) and normalized cuts (nCuts) (Shi & Malik, 2000), varying the number of segments from two to ten. We also vary the number of segments for each image, denoted as nCuts(*), where the number of segments are given as the number of labeled objects in each image. The result shows DPMRM performs better than both rddCRP and nCuts."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we presented our new model, DP-MRM, in which the base distribution of DP is a mixture of random measures. The applications with multi-labeled documents and images are shown with label prediction and image segmentation experiments. The results show that DP-MRM for labeled data produces\ninterpretable topics with more flexibility than the Labeled LDA. One promising extension of our model is to incorporate prior knowledge of external sources or domain experts into a Bayesian nonparametric topic model. It is beyond the scope of this paper, but our model can use different βk for each base distribution Hk, therefore using the structualized prior βk from domain experts (Andrzejewski et al., 2009) can be easily incorporated into our model."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Tehcnology (2011-0026507)."
    } ],
    "references" : [ {
      "title" : "Incorporating domain knowledge into topic modeling via dirichlet forest priors",
      "author" : [ "D. Andrzejewski", "X. Zhu", "M. Craven" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Andrzejewski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Andrzejewski et al\\.",
      "year" : 2009
    }, {
      "title" : "Mixtures of dirichlet processes with applications to bayesian nonparametric problems",
      "author" : [ "C.E. Antoniak" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Antoniak,? \\Q1974\\E",
      "shortCiteRegEx" : "Antoniak",
      "year" : 1974
    }, {
      "title" : "Ferguson distributions via pólya urn schemes",
      "author" : [ "D. Blackwell", "J.B. MacQueen" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Blackwell and MacQueen,? \\Q1973\\E",
      "shortCiteRegEx" : "Blackwell and MacQueen",
      "year" : 1973
    }, {
      "title" : "Distance dependent chinese restaurant processes",
      "author" : [ "D. Blei", "P. Frazier" ],
      "venue" : "JMLR, 12:2461–2488,",
      "citeRegEx" : "Blei and Frazier,? \\Q2011\\E",
      "shortCiteRegEx" : "Blei and Frazier",
      "year" : 2011
    }, {
      "title" : "Supervised topic models",
      "author" : [ "D. Blei", "J. McAuliffe" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Blei and McAuliffe,? \\Q2007\\E",
      "shortCiteRegEx" : "Blei and McAuliffe",
      "year" : 2007
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D Blei", "A Ng", "M. Jordan" ],
      "venue" : "JMLR, pp",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "A bayesian model for simultaneous image clustering, annotation and object segmentation",
      "author" : [ "L. Du", "L. Ren", "D. Dunson", "L. Carin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Du et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2009
    }, {
      "title" : "Bayesian density estimation and inference using mixtures",
      "author" : [ "M.D. Escobar", "M. West" ],
      "venue" : "JASA, pp",
      "citeRegEx" : "Escobar and West,? \\Q1995\\E",
      "shortCiteRegEx" : "Escobar and West",
      "year" : 1995
    }, {
      "title" : "A bayesian hierarchical model for learning natural scene categories",
      "author" : [ "L. Fei-Fei", "P. Perona" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Fei.Fei and Perona,? \\Q2005\\E",
      "shortCiteRegEx" : "Fei.Fei and Perona",
      "year" : 2005
    }, {
      "title" : "Spatial distance dependent chinese restaurant processes for image segmentation",
      "author" : [ "S. Ghosh", "A.B. Ungureanu", "E.B. Sudderth", "D.M. Blei" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ghosh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2011
    }, {
      "title" : "Multiscale conditional random fields for image labeling",
      "author" : [ "X. He", "R.S. Zemel", "M.A. Carreira-Perpinán" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "He et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2004
    }, {
      "title" : "Disclda: Discriminative learning for dimensionality reduction and classification",
      "author" : [ "S. Lacoste-Julien", "F. Sha", "M.I. Jordan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lacoste.Julien et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lacoste.Julien et al\\.",
      "year" : 2008
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li" ],
      "venue" : "JMLR, 5:361–397,",
      "citeRegEx" : "Lewis et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning to detect natural image boundaries using local brightness, color, and texture",
      "author" : [ "D.R. Martin", "C.C. Fowlkes", "J. Malik" ],
      "venue" : "cues. TPAMI,",
      "citeRegEx" : "Martin et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2004
    }, {
      "title" : "Modeling the shape of the scene: A holistic representation of the spatial envelope",
      "author" : [ "A. Oliva", "A. Torralba" ],
      "venue" : null,
      "citeRegEx" : "Oliva and Torralba,? \\Q2001\\E",
      "shortCiteRegEx" : "Oliva and Torralba",
      "year" : 2001
    }, {
      "title" : "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora",
      "author" : [ "D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Ramage et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ramage et al\\.",
      "year" : 2009
    }, {
      "title" : "Characterizing microblogs with topic models",
      "author" : [ "D. Ramage", "S. Dumais", "D. Liebling" ],
      "venue" : "In ICWSM. The AAAI Press,",
      "citeRegEx" : "Ramage et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ramage et al\\.",
      "year" : 2010
    }, {
      "title" : "The author-topic model for authors and documents",
      "author" : [ "M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Rosen.Zvi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rosen.Zvi et al\\.",
      "year" : 2004
    }, {
      "title" : "Labelme: a database and web-based tool for image annotation",
      "author" : [ "B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman" ],
      "venue" : null,
      "citeRegEx" : "Russell et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Russell et al\\.",
      "year" : 2008
    }, {
      "title" : "A constructive definition of dirichlet priors",
      "author" : [ "J. Sethuraman" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Sethuraman,? \\Q1991\\E",
      "shortCiteRegEx" : "Sethuraman",
      "year" : 1991
    }, {
      "title" : "Normalized cuts and image",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "segmentation. TPAMI,",
      "citeRegEx" : "Shi and Malik,? \\Q2000\\E",
      "shortCiteRegEx" : "Shi and Malik",
      "year" : 2000
    }, {
      "title" : "Time-varying topic models using dependent dirichlet processes",
      "author" : [ "N. Srebro", "S. Roweis" ],
      "venue" : "UTML, TR# 2005,",
      "citeRegEx" : "Srebro and Roweis,? \\Q2005\\E",
      "shortCiteRegEx" : "Srebro and Roweis",
      "year" : 2005
    }, {
      "title" : "Hierarchical dirichlet processes",
      "author" : [ "Y. Teh", "M. Jordan", "M. Beal", "D. Blei" ],
      "venue" : null,
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "Continuous Time Dynamic Topic Models",
      "author" : [ "C. Wang", "D.M. Blei", "D. Heckerman" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Wang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2008
    }, {
      "title" : "Spatial latent dirichlet allocation",
      "author" : [ "X. Wang", "E. Grimson" ],
      "venue" : null,
      "citeRegEx" : "Wang and Grimson,? \\Q2007\\E",
      "shortCiteRegEx" : "Wang and Grimson",
      "year" : 2007
    }, {
      "title" : "Medlda: maximum margin supervised topic models for regression and classification",
      "author" : [ "J. Zhu", "A. Ahmed", "E.P. Xing" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Topic models such as latent dirichlet allocation (LDA) (Blei et al., 2003) have been extended to incorporate side information such as authorship (Rosen-Zvi et al.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : ", 2003) have been extended to incorporate side information such as authorship (Rosen-Zvi et al., 2004), spatial or temporal coordinates (Wang & Grimson, 2007; Wang et al.",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : ", 2004), spatial or temporal coordinates (Wang & Grimson, 2007; Wang et al., 2008), and document labels (Ramage et al.",
      "startOffset" : 41,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : ", 2008), and document labels (Ramage et al., 2009).",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "DP-MRM can be seen as a nonparametric extension of LabeledLDA (L-LDA) (Ramage et al., 2009) in terms of defining topic distributions over labels.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Recent research shows that incorporating label information into topic models has advantages for topic interpretation as well as other practical uses such as user profiling in social media (Ramage et al., 2010).",
      "startOffset" : 188,
      "endOffset" : 209
    }, {
      "referenceID" : 25,
      "context" : ", 2008), and medLDA (Zhu et al., 2009), that also model data with labels.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "Another view of DP-MRM is that it is a more general case of the HDP (Teh et al., 2006).",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "We show an example of this by incorporating the ddCRP (Blei & Frazier, 2011) into our model for the task of image segmentation as done in rddCRP (Ghosh et al., 2011).",
      "startOffset" : 145,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : "In section 5, we present a modification of our model for image segmentation and compare the performance with nCuts (Shi & Malik, 2000) and rddCRP (Ghosh et al., 2011) quantitatively and qualitatively.",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "Note that HDP can be viewed as a specialized instance of our model (Teh et al., 2006), where we assume there is a single ‘unknown’ label for all documents.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : "Stick breaking process The stick breaking process is a constructive definition for generating a Dirichlet process (Sethuraman, 1991).",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "As a baseline, we trained a multi-class SVM with the topic proportions inferred by LDA (Blei et al., 2003).",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 25,
      "context" : "MedLDA (Zhu et al., 2009), one of supervised topic model, also used for the comparision.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "We compared the performance of L-LDA and DPMRM using two multi-labeled corpora: the Ohsumed dataset, which is a subset of the MEDLINE corpus consisting of medical journals, and RCV1-V2 dataset (Lewis et al., 2004), a corpus of Reuters news articles.",
      "startOffset" : 193,
      "endOffset" : 213
    }, {
      "referenceID" : 10,
      "context" : "Image segmentation is often done with manually segmented and labeled data (He et al., 2004; Gould et al., 2009).",
      "startOffset" : 74,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "One recent paper has shown a Bayesian model for simultaneous image segmentation and annotation (Du et al., 2009) using a logistic stick-breaking process.",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "There are many possible ways of defining the decay function, and in this paper, we follow (Ghosh et al., 2011) and use a window decay function which measures the distance between superpixels as a hop distance between them.",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "For image segmentation, we use the eight scene categories in (Oliva & Torralba, 2001) which are fully segmented and labeled by human subjects and available from the LabelMe dataset (Russell et al., 2008).",
      "startOffset" : 181,
      "endOffset" : 203
    }, {
      "referenceID" : 13,
      "context" : "Each superpixel is described via local texton histogram (Martin et al., 2004) and HSV color histogram.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "Figure 5 shows the quantitative performance of the segmentation via Rand Index, comparing DP-MRM with rddCRP (Ghosh et al., 2011) and normalized cuts (nCuts) (Shi & Malik, 2000), varying the number of segments from two to ten.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "It is beyond the scope of this paper, but our model can use different βk for each base distribution Hk, therefore using the structualized prior βk from domain experts (Andrzejewski et al., 2009) can be easily incorporated into our model.",
      "startOffset" : 167,
      "endOffset" : 194
    } ],
    "year" : 2012,
    "abstractText" : "We describe a nonparametric topic model for labeled data. The model uses a mixture of random measures (MRM) as a base distribution of the Dirichlet process (DP) of the HDP framework, so we call it the DPMRM. To model labeled data, we define a DP distributed random measure for each label, and the resulting model generates an unbounded number of topics for each label. We apply DP-MRM on single-labeled and multi-labeled corpora of documents and compare the performance on label prediction with MedLDA, LDA-SVM, and Labeled-LDA. We further enhance the model by incorporating ddCRP and modeling multi-labeled images for image segmentation and object labeling, comparing the performance with nCuts and rddCRP.",
    "creator" : "LaTeX with hyperref package"
  }
}