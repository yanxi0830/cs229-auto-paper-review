{
  "name" : "1002.2780.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Collaborative Filtering in a Non-Uniform World:  Learning with the Weighted Trace Norm",
    "authors" : [ "Ruslan Salakhutdinov" ],
    "emails" : [ "rsalakhu@mit.edu", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 2.\n27 80\nv1 [\ncs .L\nG ]\n1 4\nFe b"
    }, {
      "heading" : "1. Introduction",
      "text" : "Trace-norm regularization is a popular approach for matrix completion and collaborative filtering, motivated both as a convex surrogate to the rank (Fazel et al., 2001; Candes & Tao, 2009) and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning (Srebro et al., 2005b; Bach, 2008; Abernethy et al., 2009; Salakhutdinov & Mnih, 2008).\nCurrent theoretical guarantees on using the tracenorm for matrix completion all assume a uniform sampling distribution over entries of the matrix (Srebro & Shraibman, 2005; Candes & Tao, 2009; Candes & Recht, 2009; Candes & Tao, 2009; Recht, 2009). In a collaborative filtering setting, where rows of the matrix represent e.g. users and columns represent e.g. movies, this corresponds to assuming all users are equally likely to rate movies and all movies are equally likely to be rated. This of course cannot be further from the truth, as in any actual collaborative filtering application, some users are much more active then others and some movies are rated by many people while others are much less likely to be rated.\nIn Section 3 we show, both analytically and through\nsimulations, that this is not a deficiency of the proof techniques used to establish the above guarantees. Indeed, a non-uniform sampling distribution can lead to a significant deterioration in prediction quality and an increase in the sample complexity. Under non-uniform sampling, as many as Ω(n4/3) samples might be needed for learning even a simple (e.g. orthogonal low rank) n×n matrix. This is in sharp contrast to the uniform sampling case, in which Õ(n) samples are enough. It is important to note that if the rank could be minimized directly, which is in general not computationally tractable, Õ(n) samples would be enough to learn a low-rank model even under an arbitrary non-uniform distribution.\nIn Section 4 we suggest a correction to the tracenorm regularizer, which we call the weighted tracenorm, that takes into account the sampling distribution. This correction is motivated by our analytic analysis and we discuss how it corrects the problems that the unweighted trace-norm has with non-uniform sampling. We then show how the weighted trace-norm indeed yields a significant improvement on the (highly non-uniformly sampled) Netflix dataset."
    }, {
      "heading" : "2. Complexity Control in terms of Matrix Factorizations",
      "text" : "Consider the problem of predicting the entries of some unknown target matrix Y ∈ Rn×m based on a random subset S of observed entries YS . For example, n and m may represent the number of users and the number of movies, and Y may represent a matrix of partially observed rating values. Predicting elements of Y can be done by finding a matrix X minimizing the training error, here measured as a squared error, and some measure c(X) of complexity. That is, minimizing either:\nmin X\n‖XS − YS‖2F + λc(X) (1)\nor: min\nc(X)≤C ‖XS − YS‖2F , (2)\nwhere YS , and similarly XS , denotes the matrix “masked” by S:\n(YS)i,j =\n{\nYi,j if (i, j) ∈ S 0 otherwise.\n(3)\nFor now we ignore possible repeated entries in S. We will also assume that n ≤ m without loss of generality. The two formulations (1) and (2) are equivalent up to some (unknown) correspondence between λ and C, and we will be referring to them interchangeably at our convenience."
    }, {
      "heading" : "2.1. Low Rank Factorization",
      "text" : "A basic measure of complexity is the rank of X , corresponding to the minimal dimensionality k such that X = U⊤V for some U ∈ Rk×n and V ∈ Rk×m. Directly constraining the rank of X forms one of the most popular approaches to collaborative filtering. Training such a model amounts to finding the best rank-k approximation to the observed target matrix Y under the given loss function. However, the rank is non-convex and hard to minimize. It is also not clear if a strict dimensionality constraint is most appropriate for measuring the complexity."
    }, {
      "heading" : "2.2. Trace-norm Regularization",
      "text" : "Lately, methods regularizing the norm of the factorization U⊤V , rather then its dimensionality, have been advocated and were shown to enjoy considerable empirical success (Rennie & Srebro, 2005; Salakhutdinov & Mnih, 2008). This is captured by measuring complexity in terms of the trace-norm of X , which can be defined equivalently either as the sum of the singular values of X , or as (Fazel et al., 2001):\n‖X‖tr = minX=U ′V 1 2 (‖U‖2F + ‖V ‖ 2 F). (4)\nNote that the dimensionality of U and V in (4) is not constrained. Beyond the modeling appeal of normbased, rather then dimension-based, regularization, the trace-norm is a convex function of X and so can be minimized by either local search or more sophisticated convex optimization techniques."
    }, {
      "heading" : "2.3. Scaling of the Trace-norm",
      "text" : "It will be useful for us to consider the scaling of the trace-norm with the size of the matrix X . This will\nallow us, for example, to understand the magnitude of the bound C we can expect to put on the trace-norm in the formulation (2).\nThe rank, as a measure of complexity, does not scale with the size of the matrix. That is, even very large matrices can have low rank. Viewing the rank as a complexity measure corresponding to the number of underlying factors, if data is explained by e.g. two factors, then no matter how many rows (“users”) and columns (“movies”) we consider, the data will still have rank two.\nThe trace-norm, however, does inherently scale with the size of the matrix. To see this, note that the tracenorm is the ℓ1 norm of the spectrum, while the Frobenius norm is the ℓ2 norm of the spectrum, yielding:\n‖X‖F ≤ ‖X‖tr ≤ ‖X‖F √ rank(X) ≤ n ‖X‖F , (5) where in the second inequality we used the fact that the number of non-zero singular values is equal to the rank. The Frobenius norm certainly increases with the size of the matrix, since the magnitude of each element does not decrease when we have more elements, and so the trace-norm will also increase. The above suggests measuring the trace-norm relative to the Frobenius norm. Without loss of generality, consider each target entry to be of roughly unit magnitude1, e.g. ±1, and so in order to fit Y each entry of X must also be of roughly unit magnitude. This suggests scaling the trace-norm by √ nm. More specifically, we study the trace-norm through the complexity measure:\ntc(X) = ‖X‖2tr nm , (6)\nwhich puts the trace-norm on a comparable scale to the rank. In particular, when each entry of X is, onaverage, of unit magnitude (i.e. has unit variance), in which case ‖X‖F = √ nm, we have:\n1 ≤ tc(X) ≤ rank(X) ≤ n. (7)\nTo further understand the trace-norm complexity control, consider “orthogonal” low-rank matrices U ∈ Rk×n and V ∈ Rk×m, such that Y = U⊤V and where the entries of U and V are i.i.d. N (0, 1/ √ k)2. The matrix Y is then of rank k, with each entry having zero mean and unit variance (magnitude). Its Frobenius norm is tightly concentrated at ‖Y ‖F = √ nm.\n1Any other constant magnitude will only result in some constant scaling\n2The important issue here is the orthogonality and the norm uniformity, not the randomness. But we find it easier to think of the orthogonality in terms of an i.i.d. random model.\nSince rows of U and V are orthogonal, this is essentially the singular value decomposition, with all k singular values being equal to √\nnm/k. We thus have tc(X) = k. And so at least in the orthogonal case, tc(X) = rank(X).\nAnother place where we can see that tc(X) plays a similar role to rank(X) is in the generalization and sample complexity guarantees that can be obtained for low-rank and low-trace-norm learning. Such learning guarantees were mostly discussed in the context of Lipschitz continuous loss functions (i.e. functions with a bounded first derivative), rather then the squared loss. The squared loss has a bounded second derivative rather then bounded first derivative and so requires somewhat different technical tools. Nevertheless, the main thrust of the results is still valid.\nFor Lipschitz continuous loss functions, if there is a low-rank matrix X∗ achieving low average error relative to Y (e.g. if Y = X∗ + noise), then by minimizing the training error subject to a rank constraint (a computationally intractable task), |S| = Õ(rank(X∗)(n + m)) samples are enough in order to guarantee learning a matrix X whose overall average error is close to that of X∗ (Srebro et al., 2005a). Similarly, if there is a low-trace-norm matrix X∗ achieving low average error, then minimizing the training error and the trace-norm (a convex optimization problem), |S| = Õ(tc(X∗)(n +m)) samples are enough in order to guarantee learning a matrix X whose overall average error is close to that of X∗ (Srebro & Shraibman, 2005). In these bounds tc(X) plays precisely the same role as the rank, up to logarithmic factors.\nWithout getting into the technical tools required to rigorously establish the above sample complexity guarantees, it is useful to understand them at a more abstract level. In order to understand the guarantees for low-rank learning, it is enough to consider the number of parameters in the rank-k factorization X = U⊤V . It is easy to see that the number of parameters in the factorization is roughly k(m + n) (perhaps a bit less due to rotational invariants). And so we would expect to be able to learn X when we have roughly this many samples, as is indeed confirmed by the rigorous sample complexity bounds.\nFor low-trace-norm learning, consider a sample S of size |S| ≤ Cn, for some constant C. Taking entries of Y to be of unit magnitude, we have ‖YS‖F = √\n|S| =√ Cn (Recall that YS is defined to be zero outside S).\nFrom (5) we therefore have: ‖YS‖tr ≤ √ Cn · √n =√\nCn and so tc(YS) ≤ C. That is, we can “shatter” any sample of size |S| ≤ Cn with tc(X) = C: no matter what the underlying matrix Y is, we can always\nperfectly fit the training data with a low trace-norm matrix X s.t. tc(X) ≤ C, without generalizing at all outside S. On the other hand, we must allow matrices with tc(X) = tc(X∗), otherwise we can’t hope to find X ∗, and so we can only constrain tc(X) ≤ C = tc(X∗). We therefore cannot expect to learn with less then ntc(X∗) samples. It turns out that this is essentially the largest random sample that can be shattered with tc(X) ≤ C = tc(X∗), and that if we have more then this many samples we can start learning. For our purposes here, we will mostly just make use of nonlearnability arguments of this form: if we can shatter a random sample of size |S| with a matrix X have the same complexity (e.g. trace-norm) as our target matrix X∗, we cannot hope to learn without a larger sample."
    }, {
      "heading" : "3. Trace-Norm Under a Non-Uniform Distribution",
      "text" : "In this section, we will analyze trace-norm regularized learning when the sampling distribution is not uniform. That is, when there is some, known or unknown, non-uniform distribution D over entries of the matrix Y (i.e. over index pairs (i, j)) and our sample S is sampled i.i.d. from D. Of course, if D concentrates on only a small subset of the matrix, we have no hope of recovering rows and columns of Y on which we have zero probability of seeing an observation. Instead, our objective here, as is typically the case in learning under an arbitrary distribution, is to get low average error with respect to the same distribution D. That is, we measure generalization performance in terms of the weighted sum-squared-error:\n‖X − Y ‖2D = E(i,j)∼D [ (Xij − Yij)2 ]\n= ∑\nij\nD(i, j)(Xij − Yij)2. (8)\nWe first point out that when using the rank for complexity control, i.e. when minimizing the training error subject to a low-rank constraint, non-uniformity does not pose a problem. The same generalization and learning guarantees that can be obtained in the uniform case, also hold under an arbitrary distribution D. In particular, if there is some low-rank X∗ such that ‖X∗ − Y ‖2D is small, then Õ(rank(X∗)(n + m)) samples are enough in order to learn (by minimizing training error subject to a rank constraint) a matrix X with ‖X − Y ‖2D almost as small as ‖X∗ − Y ‖ 2 D (Srebro et al., 2005a)3.\nHowever, the same does not hold when learning us-\n3Actually, this is shown only for Lipschitz continuous loss functions, and not for the squared-loss, but at the very least this holds if X is appropriately clipped. Since for-\ning the trace-norm. To see this, consider an orthogonal rank-k square n × n matrix, and a sampling distribution which is uniform over an nA × nA submatrix A, with nA = n\na (see Fig. 1). That is, the row (e.g. “user”) is selected uniformly among the first nA rows, and the column (e.g. “movie”) is selected uniformly among the first nA columns. We will use A to denote the subset of entries in the submatrix, i.e. A = {(i, j)|1 ≤ i, j ≤ nA}, rather then the matrix itself, and so we can say that D is uniform on A. For any sample S, we have:\ntc(YS) = ‖YS‖2tr n2 ≤ ‖YS‖ 2 F rank(YS) n2\n≤ |S|n a\nn2 = |S| n2−a , (9)\nwhere we again take the entries in Y to be of unit magnitude. In the second inequality above we use the fact that YS is zero outside of A, and so we can bound the rank of YS by the dimensionality nA = n a of A.\nSetting a < 1, we see that we can shatter any sample of size4 kn2−a = ω̃(n) with a matrix X for which tc(X) < k. When a ≤ 1/2, the total number of entries in A is less then n, and so Õ(n) observations are enough in order to memorize YA. But when 1/2 < a < 1, with Õ(n) observations, restricting to even tc(X) < 1, we can neither learn Y , since we can shatter YS , nor memorize it. For example, when a = 2/3 and so nA = n\n2/3, we need roughly n4/3 to start learning by constraining tc(X) to a constant — the same as we would need in order to memorize YA. This is a factor of n1/3 greater then the sample size needed to learn a matrix with constant tc(X) in the uniform case.\nThe above arguments establish that restricting the complexity to tc(X) < k might not lead to generalization with Õ(kn) samples in the non-uniform case. But does this mean that we cannot learn a rank-k ma-\nmal guarantees are not the focus of this paper, we rather view this statement only as an indicative statement without stating it rigorously.\n4Recall that f(n) = ω̃(g(n)) is the same as g(n) =\nõ(f(n)) and means that for all p we have g(n) log p g(n)\nf(n) → 0.\ntrix by minimizing the trace-norm using Õ(kn) samples when the sampling distribution is concentrated on a small submatrix? Of course this is not the case. Since the samples are uniform on a small submatrix, we can just think of the submatrix A as our entire space. The target matrix still has low rank, even when restricted to A, and we are back in the uniform sampling scenario. The only issue here is that tc(X) ≤ k, i.e. ‖X‖tr ≤ n √ k, is the right constraint in the uniform observation scenario. When samples are concentrated in nA, we actually need to restrict to a much smaller trace norm, ‖X‖tr ≤ na √ k, which will allow learning with Õ(kna) samples.\nIt is, however, easy to modify the above example and construct a sampling distribution under which Ω(n4/3) samples are required in order to learn even an “orthogonal” low-rank matrix, no matter what constraint is placed on the trace-norm. This is a significantly large sample complexity then Õ(kn), which is what we would expect, and what is required for learning by constraining the rank directly.\nTo do so, consider another submatrix B of size nB×nB with nB = n/2, such that the rows and columns of A and of B do not overlap (Fig. 1). Now, consider a sampling distribution D which is uniform over A with probability half, and uniform over B with probability half. Consider fitting a noisy matrix Y = X∗ + noise where X∗ is “orthogonal” rank-k. In order to fit on B, we need to allow a trace-norm of at least ‖X∗B‖tr = n 2 √ k, i.e. allow tc(X) = k/4. But as discussed above, with such a generous constraint on the trace-norm, we will be able to shatter S ⊂ A whenever |S ∩ A| = |S|/2 ≤ k/4n2−a. Since there is no overlap in rows and columns, and so values in the sub-matrices A and B are independent, shattering S ∩A means we cannot hope to learn in A. Setting a = 2/3 as before, it seems that with o(n4/3) samples, we cannot learn in both A and B: either we constrain to a trace-norm which is too low to fit X∗B (we under-fit on B), or we allow a trace-norm which is high enough to overfit YS∩A. Either way, we will make errors on at least half the mass of D.5\nFigure 2, left panel, precisely illustrates this phenomenon on a simulation experiment. For this synthetic example, we used nA = 300 and nB = 4700,\n5To make the above argument more precise, we should note that if we do allow high enough trace-norm to fit B, and |S| = o(n4/3), then the “cost” of overfitting YS∩A is negligible compared to the cost of fitting X∗B . For large enough n, we would be tempted to very slightly deteriorate the fit of X∗B in order to “free up” enough trace-norm and completely overfit YS∩A.\nwith an orthogonal rank-2 matrix X∗ and Y = X∗ + N (0, 1) (in case of repeated entries, the noise is independent for each appearance in the sample). The training sample size was also set to |S|=140,000. The three curves of Fig. 2 measure the excess (test) error ‖X −X∗‖2D = ‖X − Y ‖ 2 D − ‖Y −X∗‖ 2 D of the learned model, as well as the error contribution from A and from B, as a function of the constraint on tc(X), for the sampling distribution discussed above and a specific sample size. As can be seen, although it is possible to constrain tc(X) so as to achieve squarederror of less then 0.8 on B, this constraint is too lax forA and allows for over-fitting. Constraining tc(X) so as to avoid overfitting A (achieving almost zero excess test error), leads to a suboptimal fit on B.\nUntil now we discussed learning by constraining the trace-norm, i.e. using the formulation (2). It is also insightful to consider the penalty view (1), i.e. learning by minimizing\nmin X\n‖YS −XS‖2F + λ ‖X‖tr . (10)\nFirst observe that the characterization (4) allows us to decompose ‖X‖tr = ‖XA‖tr + ‖XB‖tr, where w.l.o.g. we take all columns of U and V outside A and B to be zero. Since we also have ‖YS −XS‖2F = ‖YA∩S −XA∩S‖2F + ‖YB∩S −XB∩S‖ 2 F, we can decompose the training objective (10) as:\n‖YS −XS‖2F + λ ‖X‖tr = (‖YA∩S −XA∩S‖2F + λ ‖XA‖tr) + (‖YB∩S −XB∩S‖2F + λ ‖XB‖tr) = (\n‖YA∩S −XA∩S‖2F + λnA √ tcA(XA) )\n+ ( ‖YB∩S −XB∩S‖2F + λnB √ tcB(XB) ) , (11)\nwhere tcA(XA) = ‖XA‖2tr /n2A (and similarly tcB(XB)) refers to the complexity measure tc(·) measured relative to the size of A (similarly B). We see that the training objective decomposes to a trace-norm regularized problem in A and a trace-norm regularized problem in B. Each one of these problems is a trace-norm regularized learning problem, under a uniform sampling distribution (in the corresponding submatrix) of a noisy low-rank “orthogonal” matrix, and can therefor be learned with Õ(knA) and Õ(knB) samples respectively. In other words, Õ(kn) samples should be enough to learn both inside A and inside B.\nHowever, the regularization tradeoff parameter λ compounds the two problems. When the objective is expressed in terms of tc(·), as in (11), the regularization tradeoff is scaled differently in each part of the training objective. With Õ(kn) samples, it is possible to learn in A with some setting of λ, and it is possible to learn in B with some other setting of λ, but from the discussion above we learn that no single value of λ will allow learning in both A and B. Either λ is too high yielding too strict regularization in B, so learning on B is not possible, perhaps since it is scaled by nB ≫ nA. Or λ is too small and does not provide enough regularization in A.\nReturning to our simulation experiment, the solid curves of Fig. 3 show the excess test error for the minimizer of the training objective (11), as a function of the regularization tradeoff parameter λ. Note that these are essentially the same curves as displayed in Fig. 2, except the path of regularized solutions is now parameterized by λ rather then by the bound on tc(X). Not surprisingly we see the same phenomena: different values of λ are required for optimal learning on A and on B. Forcing the same λ on both parts of the training objective (11) yields a deterioration in the generalization performance."
    }, {
      "heading" : "4. Weighted Trace Norm",
      "text" : "The decomposition (11) and the discussion in the previous section suggests weighting the trace-norm by the frequency of rows and columns. For a sampling distribution D, denote by p(i) the row marginal, i.e. the probability of observing row i, and similarly denote by q(j) the column marginal. We propose using the weighted version of the trace-norm as a regularizer:\n‖X‖tr(p,q) = ‖diag( √ p)Xdiag( √ q)‖tr\n= min X=U ′V\n1 2 ( ∑\ni\np(i) ‖Ui‖2 + ∑\nj\nq(j), ‖Vj‖2) (12)\nwhere diag( √ p) is a diagonal matrix with √\np(i) on its diagonal (similarly diag( √ q)). The corresponding normalized complexity measure is given by tcp,q(X) = ‖X‖2tr(p,q). Note that for a uniform distribution we have that tcp,q(X) = tc(X). Furthermore, it is easy to verify that for an “orthogonal” rank-k matrix X we have tcp,q(X) = k for any sampling distribution.\nEquipped with the weighted trace-norm as a regularizer, let us revisit the problematic sampling distribution studied in the previous Section. In order to fit the “orthogonal” rank-k X∗, we need a weighted trace-norm of ‖X∗‖tr(p,q) = √ tcp,q(X) = √ k. How large a sample S ∩ A can we now shatter using such a weighted trace-norm? We can shatter a sample if ‖YS∩A‖tr ≤ √ k. In order to calculate ‖YS∩A‖tr, recall that for (i, j) ∈ A we have p(i) = q(j) = 1/(2nA). We can now calculate: ‖YS∩A‖tr(p,q) = ∥ ∥ ∥ √ 1/(2nA)YS∩A √ 1/(2nA) ∥ ∥ ∥\ntr = ‖YS∩A‖tr /(2nA) ≤\n√ |S ∩ A|nA/(2nA) = √ |S|/(8nA). That is, we can shatter a sample of size up to |S| = 8knA < 8kn. The calculation for B is identical. It seems that now, with a fixed constraint on the weighted trace-norm, we have enough capacity to both fit X∗, and with Õ(kn) samples, avoid overfitting on A.\nReturning to the penalization view (2) we can again decompose the training objective:\nmin X\n‖YS −XS‖2F + λ ‖X‖tr(p,q) , (13)\nas:\n‖YS −XS‖2F + λ ‖X‖tr(p,q) = (‖YA∩S −XA∩S‖2F + λ ‖XA‖tr(p,q)) + (‖YB∩S −XB∩S‖2F + λ ‖XB‖tr(p,q)) = (\n‖YA∩S −XA∩S‖2F + λ/2 √ tcA(XA) )\n+ ( ‖YB∩S −XB∩S‖2F + λ/2 √ tcB(XB) ) (14)\navoiding the scaling by the block sizes which we encountered in (11).\nReturning to the synthetic experiments of Fig. 3, and comparing (11) with (14), we see that introducing the weighting corresponds to a relative change of nA/nB in the correspondence of the regularization tradeoff parameters used for A and for B. This corresponds to a shift of log nAnB in the log-domain used in the figure. Shifting the solid red (bottom) curve by this amount yields the dashed red (bottom) curve. The solid blue (top) curve and the dashed red (bottom) curve thus represent the excess error on B and on A when the weighted trace norm is used, i.e. the training objective (14) is minimized (except for an overall scaling in λ). The dashed black (middle) curve is the overall excess error when using this training objective. As can be seen, the weighting aligns the excess errors on A and on B much better, and yields a lower overall error. The weighted trace-norm achieves the lowest MSE of 0.4301 with corresponding λ = 0.11. This is compared to the lowest MSE of 0.4981 with λ = 0.80, achieved by the unweighted trace-norm. It is also interesting to observe that the weighted trace-norm outperforms its unweighted counterpart for a wide range of regularization parameters λ ∈ [0.01; 0.6]. This may also suggest that in practice, particularly when working with large and imbalanced datasets, it may be easier to search for regularization parameters using weighted trace-norm. Fig. 2, right panel, further shows the test error as a function on the constraint tcp,q(X).\nFinally, Fig. 3 also suggests that the optimal shift is actually smaller then nA/nB. We consider a smaller\nshift by using the partially-weighted trace-norm:\n‖X‖tr(p,q,α) = ∥ ∥ ∥ diag(pα/2)Xdiag(qα/2) ∥ ∥ ∥\ntr\n= min X=U⊤V\n1 2 ( ∑\ni\np(i)α ‖Ui‖2 + ∑\nj\nq(j)α ‖Vj‖2) (15)\nAnd he corresponding normalized complexity measure tcp,q,α(X) = ‖X‖2tr( pα\nn1−α , q\nα m1−α )."
    }, {
      "heading" : "5. Practical Implementation",
      "text" : "When dealing with large datasets, such as the Netflix data, the most practical way to fit trace-norm regularized models is through stochastic gradient descent (Salakhutdinov & Mnih, 2008; Koren, 2008).\nLet ni = ∑ j Sij and mj = ∑\ni Sij denote the number of observed ratings for user i and movie j respectively. The training objective (over the index pairs (i, j)) using partially-weighted trace-norm (Eq. 12) can be written as:\n∑\n{i,j}∈S\n(\n( Yij − U⊤i Vj )2 + (16)\n+ λ\n2\n(\np(i)α\nni ‖Ui‖2 +\nq(j)α\nmj ‖Vj‖2\n))\n,\nwhere U ∈ Rk×n and V ∈ Rk×m. We can optimize this objective using stochastic gradient descent by picking one training pair (i, j) at random at each iteration, and taking a step in the direction opposite the gradient of the term corresponding to the chosen (i, j).\nNote that even though the objective (16) as a function of U and V is non-convex, there are no nonglobal local minima if we set k to be large enough, i.e. k > min(n,m) (Burer & Monteiro, 2005). However, fitting orthogonal models in practice with very large values of k becomes computationally expensive. Instead, we consider truncated trace-norm minimization by restricting k to smaller values. In the next section we demonstrate that even when using truncated trace-norm, its weighted version significantly improves model’s prediction performance.\nIn all of our experiments, we also replace unknown row p(i) and column q(j) marginals in (16) by their empirical estimates p̂(i) = ni/|S| and q̂(j) = mj/|S|. This results in the following objective:\n∑\n{i,j}∈S\n(\n( Yij − U⊤i Vj )2 + (17)\n+ λ\n2|S|\n(\nnα−1i ‖Ui‖ 2 +mα−1j ‖Vj‖ 2\n))\n.\nSetting α = 1, corresponding to the weighted tracenorm (12), results in stochastic gradient updates that do not involve the row and column counts at all and are in some sense the simplest. Strangely, and likely originating as a “bug” in calculating the stochastic gradients by one of the participants, these are the actual SGD steps used by many practitioners on the Netflix dataset (Koren, 2008; Takács et al., 2009; Salakhutdinov & Mnih, 2008)."
    }, {
      "heading" : "6. Experimental results",
      "text" : "We evaluated various models on the Netflix dataset, which is the largest publicly available collaborative filtering dataset. The training set contains 100,480,507 ratings from 480,189 randomly-chosen, anonymous users on 17,770 movie titles. As part of the training data, Netflix also provides qualification set, containing 1,408,395 ratings. The pairs were selected from the most recent ratings for a subset of the users in the training dataset. Due to the special selection scheme, ratings from users with few ratings are overrepresented in the qualification set, relative to the training set. To avoid the issue of dealing with different training and test distributions, we also created our own validation and test sets, each containing 100,000 ratings that were randomly selected from the training set. As a baseline, Netflix provided the test score of its own system trained on the same data, which is 0.9514.\nThis dataset is interesting for several reasons. First, it is very large, and very sparse (98.8% sparse). Second, the dataset is very imbalanced with highly nonuniform samples. It includes users with over 10,000 ratings as well as users who rated fewer than 5 movies."
    }, {
      "heading" : "6.1. Results",
      "text" : "In our first experiment, for various values of α, we fit parameters U and V using stochastic gradient descent as in (17) with k = 30. Both U and V were randomly initialized for all models and regularization parameters λ were chosen by cross-validation.\nPerformance results of the weighted trace-norm regu-\nlarization for various values of α are shown in table 1. Observe that that the weighted trace-norm (α = 1) achieved a RMSE of 0.9105 on the Netflix qualification set, significantly outperforming its unweighted counterpart with α = 0, that achieved a RMSE of 0.9235. This large performance gap is striking. It clearly suggests that the weighting is quite important. Table 1 further reveals that the weighted trace-norm (α = 1) is not optimal. Surprisingly, partially weighted tracenorm with α = 0.9 achieved a RMSE of 0.9091, slightly outperforming the weighted matrix factorization. Performance results on the artificially created test set are similar to the results on the qualification set. Note also that the large gap in generalization performance between the test and the qualification sets is due to the Netflix’s special qualification selection scheme.\nIn our second experiment, we fitted much larger models with k = 100. As expected, the weighted tracenorm regularization (α = 1) attained a RMSE 0.9071, significantly improving upon the unweighted model’s RMSE of 0.9203. Again, this large performance gap strongly suggests that the weighting can yield significant performance boost, particularly when dealing with very imbalanced data, such as the Netflix dataset.\nIn all of our experiments, we also empirically observed that for a wide range of regularization parameters λ, optimizing the weighted trace-norm almost always yielded better predictions on both the test and the Netflix qualification sets than optimizing the unweighted trace-norm. This confirms our previous results on the synthetic experiment and strongly suggests that it may be far easier to search for regularization parameters using the weighted trace-norm."
    }, {
      "heading" : "7. Discussion",
      "text" : "In this paper we showed both analytically and empirically that under non-uniform sampling, trace-norm regularization can lead to significant performance deterioration and an increase in sample complexity. Motivated by our analytic analysis, we further suggested a corrected version of the trace-norm, called weighted trace-norm, that does take into account the nonuniform sampling distribution. Our results on both synthetic and highly imbalanced Netflix datasets further demonstrate that the weighted trace-norm yields significant improvements in prediction quality. It is interesting to note that setting α = 1 in the weighted trace-norm objective (12) implies that the frequent users (movies) get regularized much stronger than the rare users (movies). From Bayesian perspective, such regularization is quite unusual, since it effectively states that the effect of the prior becomes stronger as\nwe observe more data. Yet, our analysis and empirical results strongly suggest that in non-uniform setting, such “unorthodox” regularization is crucial for achieving good generalization performance.\nAlthough theoretical guarantees are not the focus of this work, we hope that the weighted tracenorm, and the discussions in Sections 3 and 4, will be helpful in deriving theoretical learning guarantees for non-uniform sampling distributions, both in the form of generalization error bounds as in (Srebro & Shraibman, 2005), and generalizing the compressed-sensing inspired work on recovery of noisy low-rank matrices as in (Candes & Plan, 2009; Recht, 2009)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "R.S. acknowledges the financial support from NSERC, Shell, and NTT Communication Sciences Laboratory."
    } ],
    "references" : [ {
      "title" : "A new approach to collaborative filtering: Operator estimation with spectral regularization",
      "author" : [ "J. Abernethy", "F. Bach", "T. Evgeniou", "J.P. Vert" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2009
    }, {
      "title" : "Consistency of trace norm minimization",
      "author" : [ "F. Bach" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bach,? \\Q2008\\E",
      "shortCiteRegEx" : "Bach",
      "year" : 2008
    }, {
      "title" : "Local minima and convergence in low-rank semidefinite programming",
      "author" : [ "S. Burer", "R.D.C. Monteiro" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Burer and Monteiro,? \\Q2005\\E",
      "shortCiteRegEx" : "Burer and Monteiro",
      "year" : 2005
    }, {
      "title" : "Matrix completion with noise",
      "author" : [ "E.J. Candes", "Y. Plan" ],
      "venue" : "Proceedings of the IEEE (to appear),",
      "citeRegEx" : "Candes and Plan,? \\Q2009\\E",
      "shortCiteRegEx" : "Candes and Plan",
      "year" : 2009
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E.J. Candes", "B. Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Candes and Recht,? \\Q2009\\E",
      "shortCiteRegEx" : "Candes and Recht",
      "year" : 2009
    }, {
      "title" : "The power of convex relaxation: Near-optimal matrix completion",
      "author" : [ "E.J. Candes", "T. Tao" ],
      "venue" : "IEEE Trans. Inform. Theory (to appear),",
      "citeRegEx" : "Candes and Tao,? \\Q2009\\E",
      "shortCiteRegEx" : "Candes and Tao",
      "year" : 2009
    }, {
      "title" : "A rank minimization heuristic with application to minimum order system approximation",
      "author" : [ "M. Fazel", "H. Hindi", "S.P. Boyd" ],
      "venue" : "In Proceedings American Control Conference,",
      "citeRegEx" : "Fazel et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Fazel et al\\.",
      "year" : 2001
    }, {
      "title" : "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
      "author" : [ "Koren", "Yehuda" ],
      "venue" : "In ACM SIGKDD,",
      "citeRegEx" : "Koren and Yehuda.,? \\Q2008\\E",
      "shortCiteRegEx" : "Koren and Yehuda.",
      "year" : 2008
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "B. Recht" ],
      "venue" : "preprint, available from author’s webpage,",
      "citeRegEx" : "Recht,? \\Q2009\\E",
      "shortCiteRegEx" : "Recht",
      "year" : 2009
    }, {
      "title" : "Fast maximum margin matrix factorization for collaborative prediction",
      "author" : [ "J.D.M. Rennie", "N. Srebro" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Rennie and Srebro,? \\Q2005\\E",
      "shortCiteRegEx" : "Rennie and Srebro",
      "year" : 2005
    }, {
      "title" : "Probabilistic matrix factorization",
      "author" : [ "Salakhutdinov", "Ruslan", "Mnih", "Andriy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2008
    }, {
      "title" : "Generalization error bounds for collaborative prediction with lowrank matrices",
      "author" : [ "N. Srebro", "N. Alon", "T. Jaakkola" ],
      "venue" : "In Advances In Neural Information Processing Systems",
      "citeRegEx" : "Srebro et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2005
    }, {
      "title" : "Maximum margin matrix factorization",
      "author" : [ "N. Srebro", "J. Rennie", "T. Jaakkola" ],
      "venue" : "In Advances In Neural Information Processing Systems",
      "citeRegEx" : "Srebro et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2005
    }, {
      "title" : "Scalable collaborative filtering approaches for large recommender systems",
      "author" : [ "Takács", "Gábor", "Pilászy", "István", "Németh", "Bottyán", "Tikk", "Domonkos" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Takács et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Takács et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Trace-norm regularization is a popular approach for matrix completion and collaborative filtering, motivated both as a convex surrogate to the rank (Fazel et al., 2001; Candes & Tao, 2009) and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning (Srebro et al.",
      "startOffset" : 148,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : ", 2001; Candes & Tao, 2009) and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning (Srebro et al., 2005b; Bach, 2008; Abernethy et al., 2009; Salakhutdinov & Mnih, 2008).",
      "startOffset" : 139,
      "endOffset" : 225
    }, {
      "referenceID" : 0,
      "context" : ", 2001; Candes & Tao, 2009) and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning (Srebro et al., 2005b; Bach, 2008; Abernethy et al., 2009; Salakhutdinov & Mnih, 2008).",
      "startOffset" : 139,
      "endOffset" : 225
    }, {
      "referenceID" : 8,
      "context" : "Current theoretical guarantees on using the tracenorm for matrix completion all assume a uniform sampling distribution over entries of the matrix (Srebro & Shraibman, 2005; Candes & Tao, 2009; Candes & Recht, 2009; Candes & Tao, 2009; Recht, 2009).",
      "startOffset" : 146,
      "endOffset" : 247
    }, {
      "referenceID" : 6,
      "context" : "This is captured by measuring complexity in terms of the trace-norm of X , which can be defined equivalently either as the sum of the singular values of X , or as (Fazel et al., 2001):",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 13,
      "context" : "Strangely, and likely originating as a “bug” in calculating the stochastic gradients by one of the participants, these are the actual SGD steps used by many practitioners on the Netflix dataset (Koren, 2008; Takács et al., 2009; Salakhutdinov & Mnih, 2008).",
      "startOffset" : 194,
      "endOffset" : 256
    }, {
      "referenceID" : 8,
      "context" : "Although theoretical guarantees are not the focus of this work, we hope that the weighted tracenorm, and the discussions in Sections 3 and 4, will be helpful in deriving theoretical learning guarantees for non-uniform sampling distributions, both in the form of generalization error bounds as in (Srebro & Shraibman, 2005), and generalizing the compressed-sensing inspired work on recovery of noisy low-rank matrices as in (Candes & Plan, 2009; Recht, 2009).",
      "startOffset" : 423,
      "endOffset" : 457
    } ],
    "year" : 2010,
    "abstractText" : "We show that matrix completion with tracenorm regularization can be significantly hurt when entries of the matrix are sampled nonuniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}