{
  "name" : "1605.06561.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DYNANEWTON Accelerating Newton’s Method for Machine Learning",
    "authors" : [ "Hadi Daneshmand", "Aurelien Lucchi", "Thomas Hofmann" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In machine learning, we often fix a function class with parameters x ∈ Rd, define a non-negative family of loss functions φz , a regularizer Ω, and then aim to minimize a regularized sample loss over training data S,\nfSν (x) := 1 |S| ∑ z∈S φzν (x), φ z ν (x) := φ z(x) + νΩ(x) . (1)\nJustified by theories like Tikhonov regularization or structural risk minimization, we know that we can control the expected risk of the minimizers x∗ν of f S ν , i.e. avoid overfitting, by choosing the regularization strength ν appropriately.\nIn this paper, we focus on Newton’s method for optimization, which obeys quadratic convergence towards an extremal point, when initialized sufficiently close to such a point [20]. However, despite this unmatched speed of convergence, Newton’s method has shortcomings for large-scale machine learning problems of the type described above: (i) Reaching the quadratic convergent regime through an initial damped phase [7] is where most of the computation is typically spent, unless one has access to an initial solution close-enough to the optimum. (ii) Being a batch algorithm, each Newton iteration involves a complete pass over the entire data set to compute the required gradient and Hessian matrix. (iii) The computation of the Newton update requires to solve a linear system of equations, which is a challenge, in particular, if the data dimensionality is high.\nThe strategy presented in this paper is as follows. Following the ideas of [8], we present a systematic way to dynamically subsample the data so as to match-up statistical and computational accuracy, leading to significant savings in the amount of overall computation. Moreover, employing this together with an adaptive control of the regularization strength, we define a continuation method [1],\nar X\niv :1\n60 5.\n06 56\n1v 1\n[ cs\n.L G\n] 2\n0 M\nay 2\nwhere we track solutions computed for problems with fewer data and with stronger regularization. We use previous solutions in order to compute the starting point for the next Newton iteration(s), possibly operating on a larger sample. An ideal sketch of the situation is shown in Figure 1. This is meant to address challenges (i) and (ii). Finally, in order to overcome challenge (iii), we also empirically investigate our strategy for a popular quasi-Newton method, namely BFGS, which computes approximations to the inverse Hessian through closed-form rank-one matrix updates."
    }, {
      "heading" : "2 Related work",
      "text" : "Adaptive data sub-sampling Recent results have shown that the finite sum structure of the empirical risk can be exploited to achieve linear convergence for strongly convex objectives [15, 10, 23]. This is accomplished by revisiting samples and storing their gradients in order to reduce the variance of future update directions. Although these methods achieve fast convergence on the empirical risk, they do not explicitly consider the expected risk, which has been separately studied in the literature on learning theory. It is usually analysed with the help of uniform convergence bounds that take the generic form [5]\nES [ sup x∈F ∣∣fS(x)−Ezφz(x)∣∣] ≤ H(n) , (2) where the expectation is over a random sample S of size n. Here H is a bound that depends on n, usually through a ratio n/d, where d is the capacity of F (e.g. VC dimension) [6].1\nThe recent work of [8] simultaneously exploits the properties of the empirical risk as well as the concentration bounds from learning theory to achieve fast convergence to the expected risk. This approach uses a dynamic sample size schedule that matches up optimization error and statistical accuracy. Although this approach was tailored specifically for variance-reduced stochastic gradient methods, we here show how a similar adaptive sample size strategy can be used in the context of non-stochastic approaches such as Newton’s method.\nRegularization paths and continuation methods There is a rich body of literature on numerical continuation methods, a reference work being [1]. The basic idea is to define a family of objectives, in the simplest case with a single parameter t ∈ [0;T ], and to optimize over a sequence of objectives ft with increasing t, such that the sequence approaches some desired final f = fT . The general motivation is that following the solution path x∗t = arg minx ft(x) may be computationally more efficient than optimizing the (typically) harder problem f directly.\n1For ease of exposition, we assume that all regularized risks fν can be governed by the same function H.\nFree energy based continuation methods, often for non-convex or integer problems, have been popularized in computer vision and machine learning under the name of deterministic annealing [21], a deterministic variant of simulated annealing [16]. Here the family of objectives is parametrized by the computational analogue of temperature. Similar techniques known as graduated optimization have also been proposed in computer vision [4] and in machine learning, a recent example being [13].\nIn machine learning, the model complexity is typically controlled through a regularizer. In structural risk minimization, choosing a good regularizer plays an important role in the bias-variance trade-off for model selection [22]. Another virtue of the regularization factor is its influence on the performance of the optimization procedure, as can be seen through continuation methods or regularization path techniques [12, 3]. This is formally described by defining a family of objectives functions ft := fνt with a decreasing sequence νt which progressively provide a better approximation of f . The typical goal pursued in this line of work is to combine optimization with model selection, although [12, Section 4.3] also report computational savings. Our use of a continuation method is purely motivated by computational complexity and justified by a rigorous analysis of the quadratic convergence regime of Newton’s method as described in detail in the next section."
    }, {
      "heading" : "3 Adaptive Newton Method",
      "text" : ""
    }, {
      "heading" : "3.1 Newton’s Method",
      "text" : "Assume that we have a µ strongly-convex function f : Rd → R, which we want to minimize over solutions x ∈ Rd. A Newton step defines the increment as\n4x = − [ ∇2f(x) ]−1∇f(x), x← x +4x (3) An equivalent way to define Newton increments without the need to invert the Hessian is implicitly as the solution of the linearized optimality condition\n∇f(x +4x) ≈ ∇f(x) +∇2f(x)4x != 0 (4)\nas can be verified by plugging in Eq. (3).\nNewton’s method converges to the optimal solution x∗ := arg min f(x) in a finite number of steps. The speed of convergence is characterized by two distinct phases that depend on the distance to x∗. The first phase is a damped phase with slow convergence while the second phase has quadratic convergence and is triggered when entering a region close to x∗.\nIn order to formally characterized this region of quadratic convergence, an important quantity is the Newton decrement function [7] defined as\nλf := √ ∇f> [∇2f ]−1∇f, λf : Rd → R≥0 . (5)\nWe will directly make use of this definition in conjunction with an additional requirement that we impose on f , namely that it is self-concordant [19]. Self-concordance allows for an elegant, affineinvariant characterization of the quadratic convergence region, as detailed in [7], leading to the sufficient condition that λf (x) ≤ η, where η ∈ (0; 14 ) is a constant whose exact value depends on control parameters for the line search. The self-concordance property restricts the set of functions f , yet it is known that in practice, a similar analysis often optimistically applies to a wider range of functions. For further details, we refer the reader to the discussion in [7, Section 9.6] and the work on logistic regression in [2].\nNote that strong convexity implies∇2f µI and thus [∇2f ]−1 1µI, so that immediately\nλf (x) ≤ ‖∇f(x)‖ 1 µ I = 1 √ µ ‖∇f(x)‖ =⇒ ‖∇f(x)‖ ! ≤ η√µ (6)\nand we arrive at a simple (sufficient) condition on the gradient norm at x.\nAlgorithm 1 Basic Newton continuation method: a priori (V1) and data-adaptive variant (V2). 1: given sample S, iterations T 2: V1: given sequence (µt,mt), V2: given starting point (µ0,m0) 3: x0 ← arg minx fµ0,m0(x;S) 4: for t = 1, . . . , T do 5: V1: do nothing, V2: compute µt and mt 6: compute Newton increment4x for ft(x) := fµt,mt(xt−1,S) 7: xt ← xt−1 +4x 8: end for"
    }, {
      "heading" : "3.2 Continuation Method",
      "text" : "We study the 2-parametric family of regularized empirical risk functions that are defined via smooth and convex, non-negative loss functions φz and relative to a full sample S∗ = (z1, . . . ,zN ). We make use of the definitions in Eq. (1) and we think of fSν as being indexed by ν as well as n = |S|, where S = (z1, . . . ,zn) consists of the first n samples of S∗. The canonical regularizer we consider is Ω(x) = 12‖x‖\n2 and we will sometimes utilize the commonly used heuristics of choosing µ ∝ 1/m to focus on a simpler 1-parametric family.\nWe want to implement the abstract procedure described in Algorithm 1, where we either pre-generate a sequence of problems ft or, alternatively, construct (µt,mt) greedily in a data-adaptive manner. The key condition for making this work as expected (by design) is that we are able to establish the following condition\nλft(x ∗ t−1) ≤ η or, more conservatively, ‖∇ft(x∗t−1)‖ ≤ η √ µt , (7)\nwhich will assure (under appropriate assumptions, e.g. self-concordance) that the minimizer of the previous optimization problem will provide a starting point that is within the quadratic convergence region of the subsequent optimization problem, yielding a proper ”hand-over” of the solution as illustrated in Figure 1."
    }, {
      "heading" : "3.3 Reducing Regularization Strength",
      "text" : "Let us first assume that we fix S and that µ := µt−1. We are seeking for the range of possible ν := µt ≤ µ for which we can guarantee the condition in Eq. (7). Lemma 1. Let Ω(·) = 12‖ · ‖ 2, x∗µ := arg minx fµ(x), and Bµ := η2 µ‖x∗µ‖2 . For any ν such that\nµ ≥ ν ≥ µ (\n1− 1 2\n(√ B2µ + 4Bµ −Bµ )) =⇒ λfν (x∗µ) ≤ η (8)\nProof. By the first order optimality condition for x∗µ, we have that∇f0(x∗µ) = −µx∗µ, hence ∇fν(x∗µ) = ∇f0(x∗µ) + νx∗µ = (ν − µ)x∗µ ⇐⇒ ‖∇fν(x∗µ)‖ = (µ− ν)‖x∗µ‖ . (9)\nBy definition we have\nλfν (x ∗ µ)= 1√ ν ‖∇fν(x∗µ)‖ ! ≤ η ⇐⇒ ‖x∗µ‖ ! ≤ η\n√ ν\n(µ− ν) ⇐⇒ ν2 − (2 +Bµ)µν + µ2\n! ≤ 0 (10)\nSolving the quadratic equation for ν yields the claim.\nCorollary 2. Assume that φ(0, z) ≤ Φ (∀z). Lemma 1 remains valid with B = η 2\n2Φ ≥ Bµ.\nProof. One can bound ‖x∗µ‖ easily through the following argument, exploiting non-negativity of φ\nΦ ≥ f0(0) = fµ(0) ≥ fµ(x∗µ) = f0(x∗µ) + µ\n2 ‖x∗µ‖2 ≥\nµ 2 ‖x∗µ‖2 =⇒ ‖x∗µ‖2 ≤ 2Φ µ . (11)\nNote that Corollary 2 gives an a priori rate guarantee, which only depends on the easily computable Φ. This is a strong argument in favor of a continuation method, yet may not result in the most efficient schedules for reducing µ. We will revisit this issue in Section 3.5."
    }, {
      "heading" : "3.4 Increasing Sample Size",
      "text" : "We now generalize our analysis to the case, where we also increase the sample size. The basic challenge is that we need to upper bound the gradient norm contributions coming from new data points. Intuitively this depends on the generalization capability of the current iterate.\nLemma 3. Assume f has Lipschitz continuous gradients with constant L. Let S be a given nsample, which we split into S0 = (z1, . . . ,zm) and S1 = (zm+1, . . . ,zn), where m ≤ n is arbitrary. Define x∗ := arg minx fS0(x). With high probability over S0 it holds that\nES1‖∇fS(x∗)‖2 ≤ 2L (n−m)\nn H(m) (12)\nProof. See Appendix.\nTheorem 4. Under the same conditions as Lemma 3 and for arbitrary ν ≤ µ and β ∈ (0;∞),\nES1‖∇fSν (x∗)‖2 ≤(1 + β) (ν − µ) 2 ‖x∗‖2 + (1 + β−1) ( n−m n )3 2LH(m)\nProof. See Appendix.\nThe theorem directly implies that we can construct a sequence of objective functions with sample size increasing at a geometric rate.\nCorollary 5. There exists an α∗ ∈ [0; 1) such that for all α ∈ [α∗; 1], with m/α ∈ Z, the following holds for ν := αµ and S = (z1, . . . ,zm/α),\n‖∇fSν (x∗)‖ ≤ η √ m, (13)\nwhere α∗ can be explicitly computed as the root of a third order polynomial.\nProof. See Appendix."
    }, {
      "heading" : "3.5 Data-Adaptive Algorithm",
      "text" : "The above analysis is largely data-independent and just involves a few constants: L, Φ, proportionality constants involving H and µ ∝ 1/m. As such, it leads to geometric rates that can be quite conservative. We will now show a data-adaptive strategy that – up to small approximation errors – maximally increases the sample size, and, equivalently, maximally decreases the regularization strength, within the desired range. First of all, note that we can easily compute the gradient on an increased sample. Denote S = (z1, . . . ,zn) and S0 = (z1, . . . ,zm), m ≤ n. Define x∗ = arg minx f S0 µ (x),\n∇fSν (x∗) = 1\nn n∑ k=m+1 ∇φzk(x∗) + ν ( 1− µm νn ) x∗ . (14)\nNote that if µ/ν = n/m, then the second term vanishes. As we need to compute the gradient anyway as part of the Newton iteration, we get it for free. We now could approximate\nλfSν (x ∗) ≤ 1√ ν ‖∇fν,n(x∗)‖ , (15)\nbut this results in bounds that are not very tight and hence underestimate the possible rate. We thus propose a tighter bound based on a Taylor approximation of the inverse Hessian at the previous iteration, something that we can compute with a little bit of extra computational effort.2 Let ν ≤ µ and define H := ∇2fS0µ (x∗)\n[H− (µ+ ν)I]−1 = H−1 + (µ− ν)H−2 + O(µ2) (16) 2Computing a Newton update is usually done via LU decomposition, which is cheaper.\nAlgorithm 2 Data-adaptive Newton’s method - DYNANEWTON 1: Given sample S and (µ0,m0), define S0 := S1:m0 2: x0 ← arg min fS0µ0 3: for t = 1, . . . , T do 4: Find the smallest α such that\nλfα(x ∗) ≤ η, where fα := fS\n′\nν , ν := αµ, S ′ := S1:n, n := mt−1/α. 5: (µt,mt)← (ν, n) 6: Compute Newton increment4x for fα at xt−1 7: xt ← xt−1 +4x 8: end for\nand thus with the additional assumption that H ≈ ∇2fSµ (x∗),[ λfSν (x ∗) ]2 ≈ ∇fSν (x∗)>H−1∇fSν (x∗) + (µ− ν)‖H−1∇fSν (x∗)‖2 . (17)\nFor instance, in the setting ν ∝ 1/n, it is straightforward to (numerically) find the maximal n – or equivalently minimal ν – such that the above approximation is < η2. In our experiments, we have found this approximation to be correct up to a few percent relative error.\nThe complete algorithm is summarized in Algorithm 2. In order to find an α that fulfils Eq. (22), we need to compute an estimate of ∥∥∇fSµ (x∗)∥∥. We solve this problem by first assuming that xt is a good approximation of the solution to ft and then computing the estimate from the samples (zm+1, . . . ,zm+k). We can avoid the first approximation at the cost of making 5 − 6 Newton iterations instead of just 1, but we have found this not to be necessary in any one of our experiments. Further experimental results are provided in the appendix."
    }, {
      "heading" : "4 Experiments",
      "text" : "Datasets We apply `2-regularized logistic regression on a set of 4 datasets of varying size and dimensionality summarized in Table 1. We use 90% of the data points as the training set and the remaining 10% as test set.\nComparison to standard baselines We compare DYNANEWTON (cf. Algorithm 2) to Newton’s method and – as a competitive SGD variant – to SAGA. We used a step size of ∼ 1L for SAGA as suggested in previous work [10, 14]. The results presented in Figure 2 show significant speeds-ups compared to Newton’s method. DYNANEWTON also outperforms SAGA and gets a very accurate solution after less than 6 epochs on all datasets. We also evaluated the solution quality on the expected risk and provide the complete results of this evaluation in the Appendix (see Figure 6). In order to relate convergence on the empirical and expected risks, we plotted a horizontal dotted line that shows the iteration at which DYNANEWTON reached convergence on the test set. This demonstrates that DYNANEWTON also achieves significant gains in terms of test error.\nAs the cost per iteration is typically higher for Newton’s method on a single machine, we also present a comparison in terms of running time in the Appendix (see Figure 5). Although the gains are more moderate when measuring time, it is worth pointing out that Newton is inherently much easier to parallelize as discussed in [9] and further gains are thus to be expected in a distributed setting.\nIncrement factor test We evaluate the influence of α on the convergence of the algorithm without data-adaptivity. As can be seen in Figure 4 a small value of α leads to faster convergence while it might also make the algorithm diverge if chosen too aggressively small. In contrast, the dataadaptive approach adapts the value of α and yields a stable convergence behavior on all datasets."
    }, {
      "heading" : "3. COVTYPE 4. SUSY",
      "text" : "We observed empirically that the data-adaptive method chooses a value of α ≈ 12 thus explaining why the non-adaptive approach with α = 12 achieves a similar - but slightly inferior - performance.\nImportance of the initialization point We investigate the role of the initialization point on the convergence of Newton’s method and DYNANEWTON. The results shown in Figure 4 demonstrate that bad initialization points (i.e. far away from the optimum) significantly slow down the convergence of Newton’s method as they require more steps to get inside the ball of quadratic convergence. By comparison, a poor initialization does not significantly affect DYNANEWTON."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We proposed a continuation method variant of Newton’s method that dynamically adapts the sample size and the regularization strength such that each subproblem provides a starting point that is within the quadratic convergence region of the subsequent optimization problem. We provided a theoretical analysis that characterizes the conditions required for achieving a proper hand-over and also devised a data-adaptive strategy of discretizing the solution path.\nOur empirical results demonstrate significant speed-ups across a wide range of datasets both in terms of empirical as well as expected risk. In particular the speed of convergence on the latter is quite\nremarkable, often getting close to optimal solutions in about 2 effective epochs. All results seem to be in good agreement with our theory and its predictions.\nIn the appendix, we provide further empirical evidence that shows that our results extend to nonexact Newton methods such as L-BFGS. This is of special interest for training large deep networks for which a distributed implementation of L-BFGS has been shown to provide significant speed-ups in terms of training time [9]. While our analysis does not provide any guarantees for this case, it seems that the proposed continuation method has merits beyond the results presented in this paper. A better understanding of the behavior of quasi-Newton methods is the topic of future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Aryan Mokhtari for helpful discussions on Newton’s method."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Proofs\nLemma 3\nProof. Using classical concentration inequality from statistical learning theory we get (see also [8]) with high probability over S0\nES1\n[ fS(x∗)−min\nx fS(x)\n] ≤ n−m\nn H(m) , (18)\nwhich bounds the suboptimality of the minimizer of the smaller m-sample on the larger n-sample. Furthermore, by virtue of smoothness:\n‖∇fS(x∗)‖2 ≤ 2L [ fS(x∗)−min\nx fS(x)\n] (19)\nPutting both inequalities together yields the claim.\nTheorem 4\nProof.\n∇fSν (x∗) = m n ∇fS00 (x∗) + n−m n ∇fS10 (x∗) + νx∗ (20)\n= −m n µx∗ + n−m n ∇fS1µ (x∗)− n−m n µx∗ + νx∗ = (ν − µ)x∗ + n−m n ∇fS1µ (x∗) = (ν − µ)x∗ + n−m n ∇fSµ (x∗) .\nHere we have repeatedly exploited the first order optimality condition of x∗, i.e. ∇fS1µ (x∗) = 0. Now, taking squared norms on both sides, we apply the generalized parallelogram identity ‖a + b‖2 ≤ (1 + β)‖a‖2 + (1 + β−1)‖b‖2 with β ∈ (0;∞).\n‖∇fSν (x∗)‖2 ≤ (1 + β) (ν − µ) 2 ‖x∗‖2 + (1 + β−1) ( n−m n )2 ∥∥∇fSµ (x∗)∥∥2 (21) Taking expectation with regard to S ′ on both sides and applying Lemma 3 concludes the proof.\nCorollary 5\nProof. For concreteness set β = 1. We want to chose α such that\nµ2‖x∗‖2(1− α)2 + 2LH(m) (1− α)3 ≤ µη 2\n2 (22)\nFor α = 1, the left hand side is 0, while the right hand side is strictly positive. As the derivative of the left hand side is finite at 1, we can solve for α < 1 (such that α ≥ 0) as claimed.\nA.2 Running time\nWe compare the running time of DYNANEWTON against other baselines in Figure 5. Although we see more moderate gains in comparison to SAGA in terms of computational time (especially in early iterations), it is worth pointing out that Newton is inherently much easier to parallelize as discussed in [9] and further gains are thus to be expected in a distributed setting.\nA.3 Expected risk\nWe present the results in terms of expected error in Figure 6. This largely confirms the results obtained on the training set. Although all methods achieve convergence after less than 3 epochs, DYNANEWTON achieves even faster convergence than Newton and SAGA."
    }, {
      "heading" : "3. COVTYPE 4. SUSY",
      "text" : "A.4 Approximation\nThe analysis we developed assumes that we reach x∗µ := arg minx fµ(x) before switching to ν. We empirically check the robustness to an approximate solution x̂µ obtained by performing a single Newton step instead of 6 steps, which guarantees convergence up to numerical precision (see Section 9.5 in [7]). As shown in Figure 7, the impact of the resulting approximate solution is almost negligible.\nA.5 BFGS\nOne shortcoming of Newton’s method is that it requires solving a linear equation system involving the Hessian matrix, which may be impractical for large and high-dimensional datasets. Approximate variants known as quasi-Newton methods [11] have thus been developed, such as the popular BFGS or its limited memory version L-BFGS [17]. Quasi-Newton methods such as BFGS do not require computing the Hessian matrix but instead construct a quadratic model of the objective function by successive evaluations of the gradient. There seems to be a gap between theoretically guaranteed convergence rates and the empirically observed effectiveness of BFGS, in particular on ill-conditioned problems [18] and for non-convex problems [9].\nWe used the inspiration provided by our continuation method approach to develop a variant of LBFGS that like DYNANEWTON, adaptively changes the sample size and the regularizer. We name this approach DYNALBFGS. The pseudo-code of this method is the same as Algorithm 2 except that the Newton increment is computed from the approximate L-BFGS Hessian matrix instead of the true Hessian. We evaluate the performance of DYNALBFGS for the task of `2-regularized logistic re-"
    }, {
      "heading" : "3. COVTYPE 4. SUSY",
      "text" : "gression on the two datasets described in the main paper. The results shown in Figure 8 demonstrate significant gains compared to L-BFGS. We also investigate the performance of DYNALBFGS on training a convolutional neural network consisting of two convolutional and pooling layers with one fully-connected layer. We include results on the standard MNIST dataset in Figure 9. Although our analysis does not extend to non-convex functions, we nevertheless still observe significant gains."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Newton’s method is a fundamental technique in optimization with quadratic con-<lb>vergence within a neighborhood around the optimum. However reaching this<lb>neighborhood is often slow and dominates the computational costs. We exploit<lb>two properties specific to empirical risk minimization problems to accelerate New-<lb>ton’s method, namely, subsampling training data and increasing strong convexity<lb>through regularization. We propose a novel continuation method, where we define<lb>a family of objectives over increasing sample sizes and with decreasing regular-<lb>ization strength. Solutions on this path are tracked such that the minimizer of the<lb>previous objective is guaranteed to be within the quadratic convergence region<lb>of the next objective to be optimized. Thereby every Newton iteration is guar-<lb>anteed to achieve super-linear contractions with regard to the chosen objective,<lb>which becomes a moving target. We provide a theoretical analysis that motivates<lb>our algorithm, called DYNANEWTON, and characterizes its speed of convergence.<lb>Experiments on a wide range of data sets and problems consistently confirm the<lb>predicted computational savings.",
    "creator" : "LaTeX with hyperref package"
  }
}