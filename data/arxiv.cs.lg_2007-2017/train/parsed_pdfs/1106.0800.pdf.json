{
  "name" : "1106.0800.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Reinforcement Learning for Gaussian Systems",
    "authors" : [ "Philipp Hennig" ],
    "emails" : [ "phennig@tuebingen.mpg.de" ],
    "sections" : [ {
      "heading" : "1 Introduction — optimal reinforcement learning",
      "text" : "Reinforcement learning is about doing two things at once: Optimising a function while learning about it. These two objectives must be balanced: Ignorance precludes efficient optimization; time spent hunting after irrelevant knowledge incurs unnecessary loss. This dilemma is famously known as the exploration exploitation tradeoff. Classic reinforcement learning often considers time cheap; the tradeoff then plays a subordinate role to the desire for learning a “correct” model or policy. Many classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration, such as “ -greedy” [1], or “Thompson sampling” [2]. However, at least since a thesis by Duff [3] it has been known that Bayesian inference allows optimal balance between exploration and exploitation. It requires integration over every possible future trajectory under the current belief about the system’s dynamics, all possible new data acquired along those trajectories, and their effect on decisions taken along the way. The trouble is that this amounts to optimization and integration over a tree of exponential cost in the size of the state space [4]. The situation is particularly dire for continuous space-times, where both depth and branching factor of the “tree” are uncountably infinite. Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9]. When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14]. But bound-based algorithms can not be extended to continuous spaces without making further assumptions [15], and doing so invalidates the strongest argument in their favour — that they are free of assumptions.\nIn a parallel development, recent work by Todorov [16], Kappen [17] and others has introduced an idea into reinforcement learning that has long been commonplace in other areas of machine learning: That structural assumptions, while restrictive, can greatly simplify inference problems. In particular, a recent paper by Simpkins et al. [18] showed that it is actually possible to solve for the exploration exploitation tradeoff locally, by constructing a linear approximation for the system using a Kalman filter. Simpkins and colleagues further assumed known dynamics, and a reward distribution known up to Brownian drift. Here, I will use their work as inspiration for a novel reinforcement\nar X\niv :1\n10 6.\n08 00\nv1 [\nst at\n.M L\n] 4\nJ un\nlearning paradigm that simultaneously seeks to learn and optimally control the dynamics and reward distributions of an unknown, nonlinear, time-varying system (note that most reinforcement learning algorithms are restricted to time-invariant systems). This framework uses nonparametric Gaussian process (GP) priors to track beliefs over (infinite-dimensional) nonlinear functions. It describes the global Bayes-optimal solution, in the sense that it assigns a value to every point in the problem space, and every possible state of knowledge. The main result is a novel, explicit statement about exploration and exploitation, in the form of an infinite-dimensional differential equation. This kind of description opens up new approaches to reinforcement learning. As an only initial example of such treatments, I present an approximate Ansatz that affords an explicit reinforcement learning algorithm (Section 4); tested in some simple but instructive experiments (Section 5).\nAn intuitive description of the paper’s results is this: Prior and corresponding choice of learning machinery (Section 2) allow explicit statements about the dynamics of the learning process (Section 3). The learning machine itself also provides an estimate of the dynamics of the unknown physical system. We combine both dynamics into a joint system, which can be optimally controlled. Doing so amounts to simultaneously controlling exploration (controlling the learning system) and exploitation (controlling the physical system).\nBecause large parts of the analysis relie on concepts from optimal control theory, it will use notation from that field. Readers more familiar with the reinforcement learning literature may wish to mentally replace coordinates x with states s, controls u with actions a, dynamics with transitions p(s′ | s, a) and utilities q with losses (negative rewards) −r. The latter is potentially confusing, so note that optimal control in this paper will attempt to minimize values, rather than to maximize them, as usual in reinforcement learning (these two options are, of course, equivalent)."
    }, {
      "heading" : "2 A class of learning problems",
      "text" : "Consider the task of optimally controlling an uncertain system whose states s ≡ (x, t) ∈ K = RD×R lie in a D + 1 dimensional Euclidean (thus Hilbert) phase space: A cost Q (negative cumulated reward) is acquired at (x, t) with rate dQ/dt = q(x, t), and the first learning problem is to learn this analytic function q. A second, independent learning problem concerns the dynamics of the system. Assume the dynamics separate into a free and a controlled term linear in the control:\ndx(t) = [f(x, t) + g(x, t)u(x, t)] dt (1)\nwhere u(x, t) is the control function we seek to optimize, and f, g are analytic functions. For the following analysis, we have to assume that either f or g are known, while the other may be uncertain (or, alternatively, that it is possible to obtain independent samples from both functions). A relaxation of this requirement is future work (see also Section 3). W.l.o.g., let f be uncertain and g known. Information about both q(x, t) and f(x, t) = [f1, . . . , fD] is acquired stochastically: A Poisson process of constant rate λ produces mutually independent samples\nyq(x, t) = q(x, t)+ q and yfd(x, t) = fd(x, t)+ fd where q ∼ N (0, σ2q ); fd ∼ N (0, σ2fd). (2)\nThe noise levels σq and σf are presumed known. We assume i.i.d. noise for notational simplicity; more complex Gaussian noise models are possible. Let our initial beliefs about q and f be given by Gaussian processes GPkq (q;µq,Σq); and independent Gaussian processes ∏D d GPkfd(fd;µfd,Σfd), respectively, with kernels kr, kf1, . . . , kfD over K, and mean / covariance functions µ / Σ. To ensure continuous trajectories, we also need to regularize the control. Following a customary approach in control, we introduce a quadratic control cost ρ(u(t)) = 12u(t)\nᵀR−1u(t) with control cost scaling matrix R. This matrix emerges as a result of using measurable quantities: Its units [R] = [x/t]/[v/x] relate the cost of changing location to the utility gained by doing so.\nThe overall task is to find the optimal discounted horizon value\nv(x, t) = min u ∫ ∞ t e−(τ−t)/γ [ q[χ[τ, u(χ, τ)], τ ] + 1 2 u(χ, τ)ᵀR−1u(χ, τ) ] dτ (3)\nwhere χ(τ) is the trajectory generated by the dynamics defined in Equation (1), using the control law (policy) u(x, t). Note the control-style definition of the discount factor γ > 0, which has the advantage of giving a meaningful unit, time, to γ.\nBefore beginning the analysis, consider the relative generality of this definition: We allow for a continuous phase space. Both rewards and dynamics may be uncertain, of rather general nonlinear form, and may change over time. Standard reinforcement learning assumptions — discrete space, time-invariance, known reward function — are a special case. The Poisson process governing the generation of samples is a somewhat ad-hoc choice. Some probability measure is required to make the flow of information measurable through time, and the Poisson process is in some sense the simplest such measure, assigning uniform probability density. The discrete time setting usually used in reinforcement learning is recovered by making the time steps small (see also the construction in the following Section). On the downside, we had to restrict the form of the dynamics. However, Eq. (1) still covers numerous physical systems studied in control, for example many mechanical systems, from classics like cart-and-pole to realistic models for helicopters [19]."
    }, {
      "heading" : "3 Optimal control for the learning process",
      "text" : "From a control-theoretic standpoint, the optimal solution to the exploration exploitation tradeoff is formed by the dual control [20] of a joint representation of both the physical system and the learning machine used to model it. In reinforcement learning, this representation has come to be known as a belief-augmented POMDP [3, 4], but is not usually construed as a control problem. This section constructs the Hamilton-Jacobi-Bellman (HJB) equation of the joint control problem for the system described in Sec. 2, and analytically solves the equation for the optimal control. This requires a description of the learning algorithm’s dynamics:\nAt time t = τ , let the system be at phase space-time sτ = (x(τ), τ) and have the Gaussian process belief GP(q;µτ (s),Στ (s, s′)) over the function q (all derivations in this section will focus on q, and we will drop the sub-script q from many quantities for readability. The forms for f , or g, are entirely analogous, with independent Gaussian processes for each dimension d = 1, . . . , D). This belief stems from a finite number N of samples y0 = [y1, . . . , yN ]\nᵀ ∈ RN collected at space-times S0 = [(x1, t1), . . . , (xN , tN )]\nᵀ ≡ [s1, . . . , sN ]ᵀ ∈ KN (note that t1 to tN need not be equally spaced, ordered, or < τ ). For arbitrary points s∗ = (x∗, t∗) ∈ K, the belief over q(s∗) is a Gaussian with mean function µτ , and co-variance function Στ [21]\nµτ (s ∗) = kᵀ(s∗,S0)[K(S0,S0) + σ 2 rI] −1y0\nΣτ (s ∗ i , s ∗ j ) = k(s ∗ i , s ∗ j )− k ᵀ(s∗i ,S0)[K(S0,S0) + σyI] −1k(S0, s ∗ j )\n(4)\nwhere K(S0,S0) is the Gram matrix with elements Kab = k(sa, sb). We will abbreviate K0 ≡ [K(S0,S0) + σ 2 yI] from here on. The co-vector k ᵀ(s∗,S0) has elements k ᵀ i = k(s\n∗, si) and will be shortened to k0. The core concern here is, how does this belief change as time moves from τ to τ + dt? If dt _ 0, the chance of acquiring a datapoint yτ in this time is λ dt. Marginalising over this Poisson stochasticity, we expect the mean after dt to be\nµτ+ dt = λdt (k0, kτ ) ( K0 ξτ ξᵀτ κτ )−1( y0 yτ ) +(1−λ−O(λ2 dt))dt ·k0K−10 y0 +O[(λdt)2] (5)\nwhere we have defined the map kτ = k(s∗, sτ ), the vector ξτ with elements ξτ,i = k(si, sτ ), and the scalar κτ = k(sτ , sτ ) + σ2y . Algebraic re-formulation yields\nµτ+ dt = k0K −1 0 y0 + λ(kt − k0 ᵀK−10 ξt)(κt − ξ ᵀ tK −1 0 ξt) −1(yt − ξᵀtK−10 y0) dt. (6)\nNote that ξᵀτK −1 0 y0 is the mean prediction at sτ and (κτ − ξ ᵀ τK −1 0 ξτ ) is the marginal variance there. Hence, (κτ − ξᵀτK−10 ξτ )−1/2(yτ − ξ ᵀ τK −1 0 y0) ∼ N (0, 1) and\n(κτ − ξᵀτK−10 ξτ )−1/2(yτ − ξ ᵀ τK −1 0 y0) dt = dω (7)\nwhere dω is the Wiener [22] measure. So the change to the mean is the stochastic rate\ndµ = (mτ+ dt −mτ ) = λ(kτ − k0ᵀK−10 ξτ )(κτ − ξ ᵀ τK −1 0 ξτ ) −1/2 dω ≡ λL dω (8)\n(where we have implicitly defined the innovation function L). A similar argument finds the change of the covariance function to be the deterministic rate\ndΣt = −λLLᵀ dt. (9)\nSo the dynamics of learning consist of a deterministic change to the covariance, and a stochastic change to the mean, itself a sample from a Wiener (Gaussian) process with covariance function LLᵀ. This separation is a fundamental characteristic of GPs (it is the nonparametric version of a more straightforward corresponding notion for finite-dimensional Gaussian beliefs, for data with known noise magnitude), L dω is known as the innovation process [e.g. 23]. Its significance here is that it allows a joint dynamic description of physical and learning system.\nWe introduce the belief-augmented spaceH containing states z(τ) ≡ [x(τ), τ, µτq (s), µτf1, . . . , µτfD, Στq (s, s ′),Στf1, . . . ,Σ τ fD]. Note that the means and covariances are functions, hence H is infinitedimensional. Evidently, z(τ) obeys the stochastic differential equation\ndz = (A(z) +B(z)u) dt+ C(z) dω (10)\nwith the free dynamics A(z), the controlled dynamics Bu, and the noise matrix C(z) given by A(z) = [ f(zx, zt) , 1 , 0 , 0 , . . . , 0 , −λLqLᵀq , −λLf1L ᵀ f1 , . . . , −λLfDL ᵀ fD ] (11)\nand B = [g(s∗), 0, 0, 0, . . . ] and C(z) = diag(0, 0, λLr, λLf1, . . . , λLfD, 0, . . . )\n(some readers may prefer an alternative rendering of Eq. (10) in Fokker-Planck form with drift A + Bu and diffusion 12CC\nᵀ). We do not actually know f , of course, but the belief provides an expected value (the mean of the corresponding GPs). Replacing q and f or g in the following with their expected values is an approximation, but does not make the method myopic, because we will optimize the dynamics of this expected value, not the value itself). The value (discounted cost to go) of any state s∗ = (x∗, t∗) under the control u satisfies the Hamilton-Jacobi-Bellman equation, which, for the discounted setting of Eq. (28), reads [18]\nγ−1v(z) = min u\n{ q(z) + 1\n2 uᵀR−1u+ (A(z) +B(z)u)ᵀ∇v + 1 2 tr [ C(z)ᵀ[∇2v]C(z) ]} (12)\nwhere ∇ is the (function space) gradient with respect to z, ∇2 the (function space) Hessian, tr the trace. Analytic minimisation over u bears\nu(z) = −RB(z)ᵀ∇v(z) (13) and results in the optimal Hamilton-Jacobi-Bellman equation\nγ−1v(z) = q +Aᵀ∇v − 1 2 [∇v]ᵀBRBᵀ∇v + 1 2\ntr [ Cᵀ[∇2v]C ] (14)\nA more explicit, novel form emerges upon re-inserting the definitions of Eq. (11) into Eq. (14):\nγ−1v(z)︸ ︷︷ ︸ dv/dt = q(zx, zt)︸ ︷︷ ︸ ∂v/∂t\n+ [ f(zx, zt)∇x +∇t ] v(z)︸ ︷︷ ︸\nfree drift\n− 1 2 [∇xv(z)]ᵀgᵀ(zx, zt)Rg(zx, zt)∇xv(z)︸ ︷︷ ︸ control benefit\n+ ∑\nc=q,f1,...,fD\n1 2 λ2 tr\n[ diag(Lfd) ᵀ∇2µfdv(z) diag(Lfd) ]\n︸ ︷︷ ︸ diffusion cost −λLcLᵀc∇Σcv(z)︸ ︷︷ ︸ exploration bonus (15)\nEquation (15) is the central result of this paper: For Gaussian process inference on nonlinear dynamic systems as defined in Section 2, optimal reinforcement learning, up to expectation, reduces to an infinite-dimensional quadratic differential equation, which can be interpreted as follows (labels in the equation, note the negative signs of “beneficial” terms): The total time derivative of the value comprises the immediate utility rate q; the effect of free drift through space-time and the benefit of optimal control; as well as a diffusion cost engendered by the curvature of the belief mean, and an exploration bonus caused by the increase in certainty. Note that the first line of the right hand side of this equation only depends on the phase space-time subspace of the augmented space, while the second line only depends on the belief part of the augmented space. I will call the first line exploitation terms, the second line exploration terms, for the following reason: If the first line dominates the right hand side of Equation (15) in absolute size, then the controller is governed by the physical sub-space — it is exploiting its knowledge to control the physical system. On the other hand, if the second line dominates the value function, learning is more important than exploitation — the algorithm explores the physical space to optimize knowledge. As far as I understand, this relationship is the first exact quantitative statement about reinforcement learning’s two objectives.\nSolving Equation (15) for v is nontrivial for two reasons: First, although the vector product notation may make the objects seem finite, the mean and covariance functions are of course infinitedimensional, and what looks like straightforward inner vector products are in fact integrals. For example, the exploration bonus for the reward, writ large, reads\nλLrL ᵀ r∇Σrv(z) = ∫∫ K λLr(s ∗ i )Lr(s ∗ j ) ∂v(z) ∂Σ(s∗i , s ∗ j ) ds∗i ds ∗ j . (16)\nFor general kernels k, these integrals may only be solved numerically. However, for at least one specific choice of kernel — square-exponentials — and parametric Ansatz, the required integrals can be solved in closed form. This analytic structure is so interesting, and the square-exponential kernel so widely used that, for the “numerical” part of the paper (Section 4), I restrict the choice of kernel to square exponentials. The necessary derivations are a marginal contribution of this paper. Technical and lengthy, they can be found in Appendix C.\nThe other problem, of course, is that Equation (15) is a nontrivial differential Equation. Section 4 presents one, initial attempt at a numerical solution that should not be mistaken for a definitive answer. Despite all this, Eq. (15) constitutes a useful gain for Bayesian reinforcement learning: It replaces the intractable definition of the value in terms of future trajectories with a deterministic differential equation. This opens up a new family of approaches to reinforcement learning, based on numerical analysis rather than sampling.\nDigression: relaxing some assumptions\nThis paper only applies to the specific problem class of Section 2. Any generalisations and extensions are future work, and I do not claim to solve them. But it is instructive to consider some easier extensions, and some harder ones: If time is discrete, Equation (14) turns into a stochastic difference equation, and a finite horizon formulation becomes feasible, albeit at linearly higher cost. On the other hand, it is far from straightforward to simultaneously learn both g and f , if only the actual transitions are observed, because the beliefs over the two functions become strongly dependent when conditioned on data, and factorizing approximations have to be used. A nonlinear effect of the control (i.e. replacing g(x, t)u with g(x, t, u)) certainly requires further regularising assumptions. Otherwise, that case poses a harder form of the exploration exploitation trade-off, because the control itself governs the fidelity of information about g (in the absence of additional knowledge, it would be necessary to try arbitrarily large controls, just to see what they do). On the question of learning the kernels for Gaussian process regression on q and f or g, it is clear that standard ways of inferring kernels [21, 24] can be used without complication, but that they are not “covered” by the notion of optimality of the learning as addressed here. Another, unrelated, interesting direction concerns the use of other probabilistic models with known dynamics for non-Gaussian systems."
    }, {
      "heading" : "4 Numerically solving the Hamilton-Jacobi-Bellman equation",
      "text" : "Solving Equation (14) is principally a problem of numerical analysis, and a battery of numerical methods may be considered. In this section, I report on one specific Ansatz, for which I break with the generality of the previous Sections and assume that the kernels k are given by square exponentials k(a, b) = kSE(a, b; θ, S) = θ2 exp(− 12 (a− b)\nᵀS−1(a− b)) with parameters θ, S. We find an approximate solution through a factorizing parametric Ansatz: Let the value of any point z ∈ H in the belief space be given through a set of parametersw and some nonlinear functionals φ, such that their contributions separate over phase space, mean, and covariance functions:\nv(z) = ∑\ne=x,Σr,µr,Σf ,µf\nφe(ze) ᵀwe with φe,we ∈ RNe (17)\nThis description is obviously restrictive, but it should be compared to the use of radial basis functions for function approximation, a similarly restrictive assumption widely used in reinforcement learning. The functionals φ have to be chosen such that they are conducive to the form of Eq. (15). If the\nkernels k of the beliefs are square exponentials, one convenient option is to choose\nφas(zs) = k(sz, sa; θa, Sa) (18)\nφbΣ(zΣ) = ∫∫ K [Σz(s ∗ i , s ∗ j )− k(s∗i , s∗j )]k(s∗i , sb; θb, Sb)k(s∗j , sb; θb, Sb) ds∗i ds∗j and (19)\nφcµ(zµ) = ∫ K 1 2 µ2z(s ∗)k(s∗, sc, θc, Sc) ds ∗ (20)\n(the subtracted term in the first integral serves numerical purposes only). With this choice, the integrals of Equation (15) can be solved analytically (Appendix C). The approximate Ansatz turns Equation (15) into an algebraic equation quadratic in wx and linear in all other we:\n1 2 wᵀxΨ(zx)wx − q(zx) + ∑ e=x,µq,Σq,µf ,Σf Ξe(ze)we = 0 (21)\nusing co-vectors Ξ and a matrix Ψ with elements (Dirac’s δ replaces the trace operation)\nΞxa(zs) = γ −1φas(zs)− f(zx)ᵀ∇xφas(zs)−∇tφas(zs)\nΞΣa (zΣ) = γ −1φaΣ(zΣ) + λ ∫∫ K L(s∗i )L(s ∗ j ) ∂φΣ(zΣ) ∂Σz(s∗i s ∗ j ) ds∗i ds ∗ j\nΞµa(zµ) = γ −1φaµ(zµ)−\nλ2\n2 ∫∫ K δ(s∗i − s∗j )L(s∗i )L(s∗j ) ∂2φaµ(zµ) ∂µz(s∗i )∂µz(s ∗ j ) ds∗i ds ∗ j\nΨ(z)k` = [∇xφks(z)]ᵀg(zx)Rg(zx)ᵀ[∇xφ`s(z)]\n(22)\nTo solve for w, we simply choose a sufficiently large number of evaluation points zeval to constrain the resulting system of quadratic equations, and then find the least-squares solution wopt by function minimisation, using standard methods, such as Levenberg-Marquardt [25]. A disadvantage of this approach is that is has a number of degrees of freedom Θ, such as the kernel parameters, and the number and locations xa of the feature functionals. The experiments (Section 5) suggest that it is nevertheless possible to get interesting results simply by choosing these parameters heuristically.\nAlgorithm 1: Approximate Bayes-Optimal Learning Controller for Gaussian Systems Data: Observations Sq ,Yq ,Sf ,Yf for utilities and dynamics. Scales γ,R, kq,kf ,hv , Noises ξq, ξf Result: Optimal Control u = −R[g(sτ )]ᵀ∇xφx(sτ )wx\n1 begin 2 [Θ, zeval] ^ GenerateNumericalBasis() ; // Heuristic. May be cached and reused 3 [Lq, Lf , q̂, f̂ ] ^ GPRegression(Sq, Yq, Sf , Yf) ; // Eq. (11) 4 [Ξ,Ψ] ^ ConstructLinearMaps(Lq, Lf , q̂, f̂ ,Θ, zeval) ; // Eq. (22), Appendix C 5 wopt ^ Minimize(‖Ξw − r(zeval)− 12wᵀΨw‖2) ; // standard problem"
    }, {
      "heading" : "5 Experiments",
      "text" : "I first apply the new method to a simple, one-dimensional environment, to demonstrate some aspects that are perhaps not obvious. This is then followed by a sample application, comparing to other algorithms."
    }, {
      "heading" : "5.1 Illustrative experiment using an artificial environment",
      "text" : "I constructed a simple example system in a one-dimensional state space by sampling f, q from the model described in Section 2, and setting g to the unit function, for simplicity. The state space was tiled regularly, in a bounded region, with 231 square exponential (“radial”) basis functions (Equation 39), initially all with weight wix = 0. For the information terms, only a single basis function was used for each term (i.e. one single φΣq , one single φµq , and equally for f , all with very large length scales S, covering the entire region of interest). We will see below that this does not imply a trivial structure for these terms. Five times the number of parameters, i.e. Neval = 1175 evaluation points zeval were\nsampled, at each time step, uniformly over the same region. It is not intuitively clear whether each ze should have its own belief (i.e. whether the points should cover the belief space as well as the phase space), but anecdotal evidence of the experiments suggests that it suffices to use the current beliefs for all evaluation points. A more comprehensive evaluation of such aspects will be the subject of a future paper. The discount factor was set to γ = 50s, the sampling rate at λ = 2/s, the control cost at 10m2/($s). Value and optimal control were evaluated at time steps of δt = 1/λ = 0.5s.\nFigure 1 shows the situation 50s after initialisation (The supplement contains a more revealing video of a second initialisation, see detailed comments in Appendix A). The most noteworthy aspect is the nontrivial structure of exploration and exploitation terms. Despite the simple parameterisation of the corresponding functionals, the innovation function L(x, t) differs for every point in the phase space, and induces a complex shape that depends on the value function virtually everywhere else. The system constantly balances exploration and exploitation. This is an important insight that casts doubt on the usefulness of simple, local exploration boni, used in many reinforcement learning algorithms.\nSecondly, note that the system’s trajectory does not necessarily follow what would be the optimal path under full information. The value estimate reflects this, by assigning low (good) value to regions behind the system’s trajectory. This amounts to a sense of “remorse”: If the learner would have known about these regions earlier, it would have strived to reach them. But this is not a sign of sub-optimality: Remember that the value is defined on the augmented space. The plots in Figure 1 are merely a slice through that space at some level set in the belief space. The video in the supplementary material can be construed as a flight through this space along one learning trajectory.\n5.2 Comparative experiment — the Furuta pendulum\nThe cart-and-pole system is an under-actuated problem widely studied in reinforcement learning. For variation, I test the algorithm on its cylindrical version, the pendulum on the rotating arm [26]. In this numerical simulation, the task is to swing up the pendulum from the lower resting point. To emphasise efficient exploration, I set the discount scale to only γ = 5s. I compare the average loss of a controller with access to the true f, g, q, but otherwise using Algorithm 1, to that of an -greedy TD(λ) learner with linear function approximation, Simpkins’ et al.’s [18] Kalman method and the Gaussian process learning controller (Fig. 2). None of these methods is free of assumptions; details on the setups can be found in the Appendix B. The GP method clearly outperforms the other two learners, which barely explore. Interestingly, none of the tested methods, not even the informed controller, achieve a stable controlled balance, although the GP learner does swing up the pendulum. This demonstrates a need for future research in more elaborate solution methods for the central result, Equation (15)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "I have presented a nontrivial class of reinforcement learning problems for which the optimal balance of exploration and exploitation can be expressed compactly as a differential equation. To my knowledge, this is the first analytical statement about this tradeoff. It decouples the problem of optimal reinforcement learning from the difficult prediction of future trajectories, replacing it with the problem of solving a differential equation, for which a considerable body of prior work is available. For some intuition into how such solutions might work, I have presented one specific approximation, using functionals to reduce the problem to finite least-squares parameter estimation.\nThe class of systems for which this method applies is not arbitrarily general, but provides reasonable descriptions for a wide range of physical systems. It even extends on some assumptions of classic reinforcement learning, e.g. by allowing dynamics and reward expectations to change over time. There are two arguments for “structured”, probabilistic, approaches like the one presented here. The first is the immense complexity of the general reinforcement learning problem, which makes it unlikely that efficient — in the probabilistic sense — yet universal reinforcement learning algorithms for realistic systems will be found in the foreseeable future. The other is the impressive collection of successes that control theory has historically had using structured models.\nThis work raises new questions, theoretical and applied, for future research. From the theoretical perspective, an intriguing query is how computational complexity interacts with prior assumptions: What is the class of reinforcement learning problems that can be solved in polynomial time? Another concern is the utility of approximate inference methods for expanding this class. Regarding the applied viewpoint, I have (superficially) studied only one approach to solving the Hamilton-JacobiBellman equation, and an empirical evaluation has identified some of its weaknesses. More options are available. Open research questions include the trade-off between performance and computational cost of such methods, and the viability of policy search methods in this context."
    }, {
      "heading" : "Acknowledgments",
      "text" : "I would like to thank Carl E. Rasmussen and Jan Peters for helpful discussions. This project was funded through a fellowship of the Max Planck Society.\nAppendix"
    }, {
      "heading" : "A Details on the supplementary animation",
      "text" : "As described in the paper, the environment for this experiment was sampled from the model: A square exponential kernel with time length-scale St = 10s and space length-scale Sx = 5m was used for both q and f (the latter is not shown in the plots). The discount factor was γ = 50s, the sampling rate λ = 2/s, the control cost 10m2/($s). The trajectory shown in Figure 1 of the paper and the one shown in the animation are both on the same environment, but generated from separate initialisations (because the data is stochastic, trajectories differ in each initialisation).\nThe six panels of the animation all show the same system, from different points of view. In each panel, time is on the abscissa, space on the ordinate. The third, the “movie-dimension”, shows the development throughout the learning and control process. In each panel, the current state of the system is shown as a big black diamond with green border. In the top left panel, the trajectory of the system is also denoted with black dots.\nThe movie shows two “episodes”. That is, the system is run from t = 0, x = 0 to t = 100 once, then placed back at t = 0, x = 0. The point of this somewhat artificial setup is to demonstrate the behaviour of the controller in regions of non-uniform prior knowledge. The following list describes the individual panels, and points out some interesting observations:\nTop Left: Loss Belief Mean belief over q as color, uncertainty (marginal variance) as transparency. This panel shows that the controller is doing a meaningful job of controlling the system, by avoiding (red) regions of high loss, and spending more time in (blue) regions of low loss.\nTop Middle: Exploitation Terms the exploitation terms, from Equation (15) in the main paper, are q(zx, zt) + [ f(zx, zt)∇x +∇t ] v(z)− 1\n2 [∇xv(z)]ᵀgᵀ(zx, zt)Qg(zx, zt)∇xv(z) (23)\nevaluated at every point in the plot. The structure of Equation (23) is reflected in the plot, which roughly traces the structure of the top left (q) plot, but with slightly broader structure, effected by the free and control dynamics\nTop Right: Exploration Terms the exploration terms, also from Equation (15), are + ∑\nc=q,f1,...,fD\n1 2 λ2 tr\n[ diag(Lfd) ᵀ∇2µfdv(z) diag(Lfd) ] − λLcLᵀc∇Σcv(z) (24)\nThe relationship between exploration and exploitation terms may be the most interesting aspect of the animation. First, note the nontrivial structure of both these panels. This structure is inherent from the HJB equation, and not created by the finite-dimensional approximation (the exploration terms are fitted with only four free parameters wµq, wΣq, wµf , wΣf , for the entire plot). In particular, it is striking that the exploration terms are not just a “tube” around the explored regions, but that they also depend on the curvature of the mean beliefs on q and f , leading to a comparably complex picture. This is an important insight, because the reinforcement learning literature has often presented the exploration-exploitation tradeoff as a kind of binary heuristic problem of pure exploration vs. pure exploitation (witness the widely used -greedy policy, which randomly switches between the two behaviours). The top middle and right panels show that exploration and exploitation depend in complex and not entirely local ways on the current beliefs about the system.\nBottom Left: Value Estimate This panel shows the current value estimate v(z) = ∑\nc=x,Σr,µr,Σf ,µf\nφc(zc) ᵀwc with φc,wc ∈ RNc (25)\nNote that the value under the prior (first frame) is entirely flat. This amounts to a boundary condition on the HJB equation. The panel also reveals some weaknesses of the parametric approach used here: Its structure is not sufficiently fine-grained everywhere to capture the full structure of the true value function. This also leads to non-optimal behaviour, e.g. towards the end of the second episode. See also the next panel.\nBottom Middle: Value Target The “value target” is given by the right hand side of the HJB equation, i.e. by the sum of exploration and exploitation terms. Note that these terms also depend on the parametric approximation, so they are not a bona fide target function — however, in an exact solution of the HJB equation, this panel would be identical to the “value estimate” panel. An interesting aspect of this panel is that it generally has finer structure than the value estimate. And interesting question for future research is whether using this function actually provides a better controller than the value estimate.\nBottom Right: Value Error This panel shows the square error between the left hand side of the HJB equation, and the right hand side of the equation (note that this is not exactly the same as the square difference between the bottom left and bottom right panels, but only up to a factor of 1/γ2, explaining the different scales of these plots. If this panel were uniform 0 everywhere in the augmented space (i.e. throughout the movie), an exact solution to the HJB equation would have been found. The experiment suggests that the approximation process works relatively well, although there are some regions of imprecise modelling. Interestingly, these regions tend to lie close to the current state of the controller (because the curvature of the mean functions is usually particularly high close to data points). It is tempting to try to correct for this in some ad-hoc way, e.g. by putting more weight on the modelling error close to the current state. But it is unclear whether doing so really improves performance. Good value estimates, and thus good control, often have more to do with specific regions of the phase space, which need not lie close to the current location. The second experiment, the Furuta pendulum, is an example of such a case, where good control depends on whether a small manifold in the phase space (the one leading to an upright balanced state) is captured well in the value function.\nOverall, the animation shows the system expressing relatively complex behaviour, both exploring and exploiting. E.g. around half way through the second episode, the system “decides” that the region it is currently exploring is, as it were, “not worth the effort” and jumps back, through a region of high loss, towards a region of certain low cost."
    }, {
      "heading" : "B Comparative experiments: Furuta pendulum",
      "text" : "In this simulation, the pendulum is modelled as two massive rods of masses m1 = 0.5kg (arm) and m2 = 2kg (pendulum) and lengths `1 = 0.5m, `2 = 0.25m. The exact forms of the dynamics can be found in Fantoni and Rogelio [19]. I chose the loss function to be given by\nq(x) = 3 · ( 1\n2 − exp\n( −1\n2 dᵀD−1d\n)) (26)\nwhere d = [dtip, θ̇1, θ̇2]ᵀ contains dtip, the absolute distance of the tip of the pendulum from the upright position, and the angular velocities of arm and pendulum. The scaling matrix is D = [1m, 25/s, 25/s]2. The advantage of this form of loss function is that it has roughly the right scale (-3,3) to be learned from a Gaussian process prior with zero mean and unit covariance function. Since this reward function does not contain θ1, the problem is spherically symmetric, and the phase space is 3-dimensional. To capture the rotational symmetry of the problem for the learning algorithms, I embed the 3D phase manifold in a 4-dimensional space by choosing the coordinate system x = [sin(θ2), cos(θ2), θ̇1, θ̇2]. The discount scale was set to γ = 5s, the sampling rate to λ = 50/s.\nEach of the methods in the comparison relies on a number of approximations with free parameters. In an effort to keep the results comparable, these parameters were chosen similarly for each method, where applicable. All methods parameterised the value function using the radial basis function features of Equation (22) in the main paper, their centres distributed over a regular grid with 232 loci, with widths [0.5, 0.5, 5, 10]. The Kalman and Gaussian process methods also require a tiling for the\nbelief space. To keep analogy with Equation (22) in the paper, the Kalman method sums over the current covariance measures. The Gaussian process learner uses the functionals of Equation (22) in the paper, with very large width. The corresponding concept for the TD learner is the parameter of the -greedy policy, which was set to 10%. The TD learner also needs a learning rate, which was set to decay like 1/n where n is the number of data points. The corresponding, albeit more powerful, concept for the Bayesian learners is the kernel similarity measure, which were square exponential kernels for both f and q, with width parameters chosen to roughly represent the actual length scales of these functions.\nIn each experiment, the pendulum starts from the lower resting position. None of the methods succeeded in balancing the pendulum well. Nevertheless, the Gaussian Process controller did learn to swing up the pendulum and enter a limit cycle with a slowly rotating pendulum in two out of five experiments, and to speed up the pendulum to fly roughly horizontally in the other three. These variations also lead to the relatively large variance noted in the Table in the paper’s Figure (2). The informed controller showed a more reproducible yet similar behaviour, always speeding up the pendulum to fly at half height. The other two learning methods did not explore enough to reach any interesting states. TD’s main problem is that it is inherently a discrete time method, so there is no concept of “guided” exploration over a finite time. The Kalman method seemed to suffer from the inadequacy of its linear model as a good description of this nonlinear system. (Note that, in the original paper by Simpkins et al. [18], this method was used only as a tracking controller).\nApart from the not overly surprising fact that the more elaborate learning method performed better than its competitors, the main takeaway of these experiments is that the approach of globally covering the phase space with basis functions of uniform precision is flawed. Intuitively, a good approximate value function for this problem requires high resolution around the upper balanced position, and along a manifold of trajectories that lead to it. It is also clear that the information required for such a focusing of the descriptive alphabet is contained in the mean and covariance beliefs used in this paper.\nThere are of course many other reinforcement learning methods to potentially compare to. However, many of them do not apply to continuous time systems. Others do not try to balance exploration and exploitation. This makes them uninteresting for this comparison, but interesting candidates for combination with this framework. One such method is that of Deisenroth and Rasmussen [27], which is a purely greedy method, but uses a Gaussian Process forward model for optimization."
    }, {
      "heading" : "C Mathematical appendix",
      "text" : "The main paper, solving a problem with several different conceptual dimensions, required rather dense notation. In this Appendix, more straightforwardly, the focus lies on solving a series of integrals, and notation should be as clean as possible. To this end, we will drop irrelevant indices, and also introduce some new, more compact notation. This may sometimes come at the cost of some variation between the notations in the two texts, but makes it much easier to parse the following derivations. In particular, we will not treat time as a special dimension of the phase space any more, and instead denote phase space-time coordinates by x ∈ K = RD, with a new definition for D as Dappendix = Dpaper + 1.\nC.1 Preliminaries\nThe derivations in this text, at their core, all rely on the Gaussian integral.∫ exp ( −1\n2 (x− a)ᵀC−1(x− a)\n) dx = √ (2π)D|C| (27)\nWe will use, throughout, kernels k : K ×K_ R in the square exponential (SE) class [21]:\nk(xi, xj ; θ, S) = θ 2 k exp\n( −1\n2 (xi − xj)S−1(xi − xj)\n) (28)\nwith a strength parameter θk and a scale S. We will adopt the notation from Rasmussen and Williams [21], where k(a, b) denotes a row vector ∈ R1×dim b and similarly k(a, b) ∈ Rdim a×dim b. The\nfollowing integrals will feature heavily in the derivations. They can all be derived straightforwardly from Equation (27), and hold for all a, b, c, d ∈ K∫\nk(x, a; θ, S) dx = θ2 √ (2π)D|S| = k ( a, a; θ 4 √ (2π)D|S|, I ) (29)∫\nk(x, a; θ1, S1)k(x, b; θ2, S2) dx = k (a, b;u2, S1 + S2) (30)∫ k(x, a; θ1, S1)k(x, b; θ1, S1)k(x, c; θ2, S2) d = k(a, b, 1, 2S1)k ( c, 1\n2 (a+ b);u3,\n1 2 S1 + S2 ) (31)\nNote that the third line is a special case (containing θ1, S1 twice). The general case is just as easy, but not needed here. The constants are\nu2 = θ1θ2 ( (2π)D|S1S2| |S1 + S2| )1/4 (32)\nu3 = θ 2 1θ2 ( (2π)D|S1S1S2| | 12S1 + S2| )1/4 (33)\nFinally, the notation in this Appendix will make frequent use of the summation convention: Indices showing up in at least two terms of a product should be understood as being summed over, unless they also feature only once on the other side of the equation.\nC.2 Square-exponential kernels on beliefs\nRecall that the experiments in the paper use the following parameterisation of the value function: v(z) = ∑\ni=x,Σr,µr,Σf ,µf\nφi(zi) ᵀwi with φi,wi ∈ RNi (34)\nwith the functions / functionals\nφax(zx) = k(xz, xa; θa, Sa) (35)\nφbΣ(zΣ) = ∫∫ K [Σz(x ∗ i , x ∗ j )− k(x∗i , x∗j )]k(x∗i , xb; θb, Sb)k(x∗j , xb; θb, Sb) dx∗i dx∗j and (36)\nφcµ(zµ) = ∫ K 1 2 µ2z(x ∗)k(x∗, xc, θc, Sc) dx ∗ (37)\nfor simplicity, we choose Sa = diag(s2) for all phase space basis functions (the dense generalisation is not difficult, but tedious). As mentioned in the paper, these choices allow analytic representation of the differential terms in the Hamilton-Jacobi-Bellman Equation, as detailed in the following paragraphs.\nC.2.1 Phase space gradients\nThe gradient’s elements with respect to the phase space dimensions is\n∂φax ∂xi = −xi − x a i s2i φax(x). (38)\nHence the phase space terms of the Hamilton Jacobi Bellman equation of the augmented system are (note the summations over i and a)\n[f(x), 1]∇xv(z) = −fi(x) xi − xai s2i k(x, xa; θa, Sa)wa (39)\nC.2.2 Terms in Σ(x∗i , x∗j )\nThe parametrized HJB equation contains two types of terms involving the covariance function Σ. The term in the direct time derivative of the value is exactly Equation (36). For its evaluation, we introduce a compact notation. Recall that the augmented state z consists of the current phase space-time x and a belief with mean and covariance functions. Both these functions are induced by the kernel and a dataset (Xz,Y z) ∈ (RD × R)Nz and may be evaluated at any point x∗ ∈ K. The following derivations will repeatedly contain certain objects: Kernel projections into the evaluation space, of the current location: kx ≡ k(x∗, x); and of the dataset kX ≡ k(x∗,X). Kernel values: between location and itself kxx ≡ k(x, x), between location and dataset kxX ≡ k(x,X) (and its transpose kXx), and the inverse of the Gram matrix K−1 ≡ [k(X,X) + σ2I]−1. With these notational shortcuts, we can evaluate Equation (36) to\nφbΣ(z)wb = ∫∫ K kX(x ∗ i ) ᵀK−1kX(x ∗ j )k(x ∗ i , xb; θb, Sb)k(x ∗ j , xb; θb, Sb) dx ∗ i dx ∗ jwb\n= k(xb,X;u2, Sz + Sb)K −1k(X, xb;u2, Sz + Sb)wb\n(40)\nwhere θz, Sz are the kernel parameters of the applicable GP belief over the phase space (i.e. the one for q, or any dimension of f or g).\nWe use the definition of the innovation term L from the paper, to find that the exploration bonus terms, evaluates to\n−λLLᵀ∇ΣφbΣ(z)wb = − ∫∫\nL(x∗i )L(x ∗ j )k(x ∗ i , xb; θb, Sb)k(x ∗ j , xb; θb, Sb) dx ∗ i dx ∗ j · wb\n= − λ kxx − kxXK−1kXx\n[∫ [kx(x ∗)− kX(x∗)K−1kXx]k(x∗; θb, Sb) dx∗ ]2 · wb\n= −λ [k(x, xb;u2, Sz + Sb)− k(xb,X;u2, Sz + Sb)K −1kXx] 2\nkxx − kxXK−1kXx · wb\n(41)\nC.2.3 Terms in µ(x∗)\nThe mean-belief term in the direct time derivative is given by Equation (37). This time, the notation becomes clearer when the elements ofX are explicitly mentioned in the summation, as Xi:\nφµ(z) cwc =\n1\n2\n∫ k(x∗, Xi)k(x ∗, Xj)(K −1Y )i(K −1Y )jk(x ∗, xc; θc, Sc) dx ∗ · wc\n= 1\n2 (K−1Y )ik(Xi, Xj ; 1, 2Sz)k(x\nc, 1\n2 (Xi +Xj), u3; Sz 2 + Sc)(K −1Y )jwc\n(42)\nThe computational cost of evaluating this term, assuming that K−1 is already available, is O(N2zNc), where Nc is the number of functional-parameter pairs used to model the effect of the mean. The diffusion cost term, taken from the paper, is (note that k(a, a, 1, C) = 1 ∀a,C)\n− λ 2\n2\n∫∫ δ(x∗i − x∗j )L(x∗i )L(x∗j )\n∂2φcµ(zµ)\n∂µz(x∗i )∂µz(x ∗ j )\ndx∗i dx ∗ j · wc\n= −λ2wc\n2[kxx − kxXK−1kXx]\n∫ [kxkx−2kxkXK−1kXx+kxXK−1kXkᵀXK −1kXx]k(x ∗, xc; θc, Sc)dx ∗\n= − λ 2wc\n2[kxx − kxXK−1kXx]\n[ k(x, xc;u3,\nSz 2 + Sc)\n− 2k(x,X; 1, 2Sz)k(xc − 1\n2 x,\n1 2 X;u3, Sz 2 + Sc)K −1kXx\n+ kxXK −1k(X,X; 1, 2Sz)k(xc −\n1 2 X, 1 2 X;u3, Sz 2 + Sc)K −1kXx\n] (43)\nthe summation of scalar and vector in the last line should be understood such that xc is subtracted from every element ofX ."
    } ],
    "references" : [ {
      "title" : "Reinforcement Learning",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : "Biometrika, 25:275–294",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1933
    }, {
      "title" : "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes",
      "author" : [ "M.O.G. Duff" ],
      "venue" : "PhD thesis, U of Massachusetts, Amherst",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "An analytic solution to discrete Bayesian reinforcement learning",
      "author" : [ "P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan" ],
      "venue" : "Proceedings of the 23rd International Conference on Machine Learning, pages 697–704",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Model based Bayesian exploration",
      "author" : [ "Richard Dearden", "Nir Friedman", "David Andre" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "A Bayesian framework for reinforcement learning",
      "author" : [ "Malcolm Strens" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Bayesian sparse sampling for on-line reward optimization",
      "author" : [ "T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans" ],
      "venue" : "International Conference on Machine Learning, pages 956–963",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A Bayesian sampling approach to exploration in reinforcement learning",
      "author" : [ "J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate" ],
      "venue" : "Uncertainty in Artificial Intelligence",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Near-Bayesian exploration in polynomial time",
      "author" : [ "J.Z. Kolter", "A.Y. Ng" ],
      "venue" : "Proceedings of the 26th International Conference on Machine Learning. Morgan Kaufmann",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics, 6(1):4–22",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning, 47(2):235–256",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "M. Kearns", "S. Singh" ],
      "venue" : "Machine Learning, 49 (2):209–232",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "R-max — a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "R.I. Brafman", "M. Tennenholtz" ],
      "venue" : "Journal of Machine Learning Research, 3:213–231",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "An analysis of model-based interval estimation for Markov decision processes",
      "author" : [ "A.L. Strehl", "M.L. Littman" ],
      "venue" : "Journal of Computer and System Sciences, 74(8):1309–1331",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "R. Kleinberg", "A. Slivkins", "E. Upfal" ],
      "venue" : "Proc. ACM Symposium on Theory of Computing, pages 681–690",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Linearly-solvable Markov decision problems",
      "author" : [ "E. Todorov" ],
      "venue" : "Advances in Neural Information Processing Systems, 19",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An introduction to stochastic control theory",
      "author" : [ "H.J. Kappen" ],
      "venue" : "path integrals and reinforcement learning. In 9th Granada seminar on Computational Physics: Computational and Mathematical Modeling of Cooperative Behavior in Neural Systems., pages 149–181",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Optimal trade-off between exploration and exploitation",
      "author" : [ "A. Simpkins", "R. De Callafon", "E. Todorov" ],
      "venue" : "American Control Conference, 2008, pages 33–38",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Non-linear Control for Underactuated Mechanical Systems",
      "author" : [ "I. Fantoni", "L. Rogelio" ],
      "venue" : "Springer",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Dual control theory",
      "author" : [ "A.A. Feldbaum" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1961
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : "MIT Press",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Differential space",
      "author" : [ "N. Wiener" ],
      "venue" : "Journal of Mathematical Physics, 2:131–174",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1923
    }, {
      "title" : "An innovations approach to least-squares estimation — part I: Linear filtering in additive white noise",
      "author" : [ "T. Kailath" ],
      "venue" : "IEEE Transactions on Automatic Control, 13(6):646–655",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Slice sampling covariance hyperparameters of latent Gaussian models",
      "author" : [ "I. Murray", "R.P. Adams" ],
      "venue" : "arXiv:1006.0868",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An algorithm for least-squares estimation of nonlinear parameters",
      "author" : [ "D.W. Marquardt" ],
      "venue" : "Journal of the Society for Industrial and Applied Mathematics, 11(2):431–441",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Swing-up control of inverted pendulum using pseudo-state feedback",
      "author" : [ "K. Furuta", "M. Yamakita", "S. Kobayashi" ],
      "venue" : "Journal of Systems and Control Engineering, 206(6):263–269",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "PILCO: A model-based and data-efficient approach to policy search",
      "author" : [ "M.P. Deisenroth", "C.E. Rasmussen" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Many classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration, such as “ -greedy” [1], or “Thompson sampling” [2].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "Many classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration, such as “ -greedy” [1], or “Thompson sampling” [2].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "However, at least since a thesis by Duff [3] it has been known that Bayesian inference allows optimal balance between exploration and exploitation.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "The trouble is that this amounts to optimization and integration over a tree of exponential cost in the size of the state space [4].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 11,
      "context" : "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 12,
      "context" : "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 13,
      "context" : "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 14,
      "context" : "But bound-based algorithms can not be extended to continuous spaces without making further assumptions [15], and doing so invalidates the strongest argument in their favour — that they are free of assumptions.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "In a parallel development, recent work by Todorov [16], Kappen [17] and others has introduced an idea into reinforcement learning that has long been commonplace in other areas of machine learning: That structural assumptions, while restrictive, can greatly simplify inference problems.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "In a parallel development, recent work by Todorov [16], Kappen [17] and others has introduced an idea into reinforcement learning that has long been commonplace in other areas of machine learning: That structural assumptions, while restrictive, can greatly simplify inference problems.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "[18] showed that it is actually possible to solve for the exploration exploitation tradeoff locally, by constructing a linear approximation for the system using a Kalman filter.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "(1) still covers numerous physical systems studied in control, for example many mechanical systems, from classics like cart-and-pole to realistic models for helicopters [19].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 19,
      "context" : "From a control-theoretic standpoint, the optimal solution to the exploration exploitation tradeoff is formed by the dual control [20] of a joint representation of both the physical system and the learning machine used to model it.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "In reinforcement learning, this representation has come to be known as a belief-augmented POMDP [3, 4], but is not usually construed as a control problem.",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "In reinforcement learning, this representation has come to be known as a belief-augmented POMDP [3, 4], but is not usually construed as a control problem.",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 20,
      "context" : "For arbitrary points s∗ = (x∗, t∗) ∈ K, the belief over q(s∗) is a Gaussian with mean function μτ , and co-variance function Στ [21]",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "where dω is the Wiener [22] measure.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "(28), reads [18]",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 20,
      "context" : "On the question of learning the kernels for Gaussian process regression on q and f or g, it is clear that standard ways of inferring kernels [21, 24] can be used without complication, but that they are not “covered” by the notion of optimality of the learning as addressed here.",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 23,
      "context" : "On the question of learning the kernels for Gaussian process regression on q and f or g, it is clear that standard ways of inferring kernels [21, 24] can be used without complication, but that they are not “covered” by the notion of optimality of the learning as addressed here.",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 24,
      "context" : "To solve for w, we simply choose a sufficiently large number of evaluation points zeval to constrain the resulting system of quadratic equations, and then find the least-squares solution wopt by function minimisation, using standard methods, such as Levenberg-Marquardt [25].",
      "startOffset" : 270,
      "endOffset" : 274
    }, {
      "referenceID" : 25,
      "context" : "For variation, I test the algorithm on its cylindrical version, the pendulum on the rotating arm [26].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "’s [18] Kalman method and the Gaussian process learning controller (Fig.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "The exact forms of the dynamics can be found in Fantoni and Rogelio [19].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : "[18], this method was used only as a tracking controller).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "One such method is that of Deisenroth and Rasmussen [27], which is a purely greedy method, but uses a Gaussian Process forward model for optimization.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 20,
      "context" : "We will use, throughout, kernels k : K ×K_ R in the square exponential (SE) class [21]:",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "We will adopt the notation from Rasmussen and Williams [21], where k(a, b) denotes a row vector ∈ R1×dim b and similarly k(a, b) ∈ R a×dim .",
      "startOffset" : 55,
      "endOffset" : 59
    } ],
    "year" : 2017,
    "abstractText" : "The exploration-exploitation tradeoff is among the central challenges of reinforcement learning. A hypothetical exact Bayesian learner would provide the optimal solution, but is intractable in general. I show that, however, in the specific case of Gaussian process inference, it is possible to make analytic statements about optimal learning of both rewards and transition dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics. The solution is described by an infinite-dimensional differential equation. For a first impression of how this result may be useful, I also provide an approximate reduction to a finite-dimensional problem, with a numeric solution.",
    "creator" : "TeX"
  }
}