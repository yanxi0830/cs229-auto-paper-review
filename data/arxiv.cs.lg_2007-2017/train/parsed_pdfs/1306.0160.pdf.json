{
  "name" : "1306.0160.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Phase Retrieval using Alternating Minimization",
    "authors" : [ "Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi" ],
    "emails" : [ "praneethn@utexas.edu", "prajain@microsoft.com", "sanghavi@mail.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Empirically, our algorithm performs similar to recently proposed convex techniques for this variant (which are based on “lifting” to a convex matrix problem) in sample complexity and robustness to noise. However, our algorithm is much more efficient and can scale to large problems. Analytically, we show geometric convergence to the solution, and sample complexity that is off by log factors from obvious lower bounds. We also establish close to optimal scaling for the case when the unknown vector is sparse. Our work represents the only known theoretical guarantee for alternating minimization for any variant of phase retrieval problems in the nonconvex setting."
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper we are interested in recovering a complex1 vector x∗ ∈ Cn from magnitudes of its linear measurements. That is, for ai ∈ Cn, if\nyi = |〈ai,x∗〉|, for i = 1, . . . ,m (1)\nthen the task is to recover x∗ using y and the measurement matrix A = [a1 a2 . . . am]. The above problem arises in many settings where it is harder / infeasible to record the phase of measurements, while recording the magnitudes is significantly easier. This problem, known as phase retrieval, is encountered in several applications in crystallography, optics, spectroscopy and tomography [33, 20]. Moreover, the problem is broadly studied in the following two settings:\n1Our results also cover the real case, i.e. where all quantities are real.\nar X\niv :1\n30 6.\n01 60\nv1 [\nst at\n.M L\n] 2\nJ un\n(i) The measurements in (1) correspond to the Fourier transform (the number of measurements here is equal to n) and there is some apriori information about the signal.\n(ii) The set of measurements y are overcomplete (i.e., m > n), while some apriori information about the signal may or may not be available.\nIn the first case, various types of apriori information about the underlying signal such as positivity, magnitude information on the signal [15], sparsity [39] and so on have been studied. In the second case, algorithms for various measurement schemes such as Fourier oversampling [34], multiple random illuminations [5, 43] and wavelet transform [9] have been suggested.\nBy and large, the most well known methods for solving this problem are the error reduction algorithms due to Gerchberg and Saxton [17] and Fienup [15], and variants thereof. These algorithms are alternating projection algorithms that iterate between the unknown phases of the measurements and the unknown underlying vector. Though the empirical performance of these algorithms has been well studied [15, 29, 30]. and they are used in many applications [31, 32], there are not many theoretical guarantees regarding their performance.\nMore recently, a line of work [8, 7, 43] has approached this problem from a different angle, based on the realization that recovering x∗ is equivalent to recovering the rank-one matrix x∗x∗T , i.e., its outer product. Inspired by the recent literature on trace norm relaxation of the rank constraint, they design SDPs to solve this problem. Refer Section 1.1 for more details.\nIn this paper we go back to the empirically more popular ideology of alternating minimization; we develop a new alternating minimization algorithm, for which we show that (a) empirically, it noticeably outperforms convex methods, and (b) analytically, a natural resampled version of this algorithm requires O(n log3 n) i.i.d. random Gaussian measurements to geometrically converge to the true vector. Our contribution:\n• The iterative part of our algorithm is implicit in previous work [17, 15, 43, 5]; the novelty in our algorithmic contribution is the initialization step which makes it more likely for the iterative procedure to succeed - see Figures 1 and 2.\n• Our analytical contribution is the first theoretical guarantee regarding the global convergence, and subsequent exact recovery of the signal, via alternating minimization for phase retrieval.\n• When the underlying vector is sparse, we design another algorithm that achieves a sample complexity of O ( (x∗min) −4 log n+ k ( log3 k + log 1 log log 1 )) and computational complexity\nof O (\n(x∗min) −4 kn log n+ k2 log2 1 log log 1 ) , where k is the sparsity and x∗min is the minimum\nnon-zero entry of x∗. This algorithm also runs over Cn and scales much better than SDP based methods.\nBesides being an empirically better algorithm for this problem, our work is also interesting in a broader sense: there are many problems in machine learning, signal procesing and numerical linear algebra, where the natural formulation of a problem is non-convex; examples include rank constrained problems, applications of EM algorithms etc., and alternating minimization has good empirical performance. However, the methods with the best (or only) analytical guarantees involve convex relaxations (e.g., by relaxing the rank constraint and penalizing the trace norm). In most\nof these settings, correctness of alternating minimization is an open question. We believe that our results in this paper are of interest, and may have implications, in this larger context.\nThe rest of the paper is organized as follows: In section 1.1, we briefly review related work. We clarify our notation in Section 2. We present our algorithm in Section 3 and the main results in Section 4. We present our results for the sparse case in Section 5. Finally, we present experimental results in Section 6."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Phase Retrieval via Non-Convex Procedures: Inspite of the huge amount of work it has attracted, phase retrieval has been a long standing open problem. Early work in this area focused on using holography to capture the phase information along with magnitude measurements [16, 26]. However, computational methods for reconstruction of the signal using only magnitude measurements received a lot of attention due to their applicability in resolving spurious noise, fringes, optical system aberrations and so on and difficulties in the implementation of interferometer setups [11]. Though such methods have been developed to solve this problem in various practical settings [10, 14, 31, 32], our theoretical understanding of this problem is still far from complete. Many papers [4, 18, 37] have focused on determining conditions under which (1) has a unique solution. However, the uniqueness results of these papers do not resolve the algorithmic question of how to find the solution to (1).\nSince the seminal work of Gerchberg and Saxton [17] and Fienup [15], many iterated projection algorithms have been developed targeted towards various applications [1, 13, 2]. [34] first suggested the use of multiple magnitude measurements to resolve the phase problem. This approach has been successfully used in many practical applications - see [11] and references there in. Following the empirical success of these algorithms, researchers were able to explain its success in some of the instances [44, 41] using Bregman’s theory of iterated projections onto convex sets [3]. However, many instances, such as the one we consider in this paper, are out of reach of this theory since they involve magnitude constraints which are non-convex. To the best of our knowledge, there are no theoretical results on the convergence of these approaches in a non-convex setting.\nPhase Retrieval via Convex Relaxation: An interesting recent approach for solving this problem formulates it as one of finding the rank-one solution to a system of linear matrix equations. The papers [8, 7] then take the approach of relaxing the rank constraint by a trace norm penalty, making the overall algorithm a convex program (called PhaseLift) over n × n matrices. Another recent line of work [43] takes a similar but different approach : it uses an SDP relaxation (called PhaseCut) that is inspired by the classical SDP relaxation for the max-cut problem. To date, these convex methods are the only ones with analytical guarantees on statistical performance [6, 43] (i.e. the number m of measurements required to recover x∗) – under an i.i.d. random Gaussian model on the measurement vectors ai. However, by “lifting” a vector problem to a matrix one, these methods lead to a much larger representation of the state space, and higher computational cost as a result.\nSparse Phase Retrieval: A special case of the phase retrieval problem which has received a lot of attention recently is when the underlying signal x∗ is known to be sparse. Though this problem is closely related to the compressed sensing problem, lack of phase information makes this harder. However, the `1 regularization approach of compressed sensing has been successfully used in this setting as well. In particular, if x∗ is sparse, then the corresponding lifted matrix x∗x∗T is also sparse. [39, 35, 28] use this observation to design `1 regularized SDP algorithms for phase\nretrieval of sparse vectors. For random Gaussian measurements, [28] shows that `1 regularized PhaseLift recovers x∗ correctly if the number of measurements is Ω(k2 log n). By the results of [36], this result is tight up to logarithmic factors for `1 and trace norm regularized SDP relaxations. [21, 38] develop algorithms for phase retrieval from Fourier magnitude measurements. However, achieving the optimal sample complexity of O ( k log nk ) is still open [12].\nAlternating Minimization (a.k.a. ALS): Alternating minimization has been successfully applied to many applications in the low-rank matrix setting. For example, clustering [25], sparse PCA [45], non-negative matrix factorization [24], signed network prediction [19] etc. However, despite empirical success, for most of the problems, there are no theoretical guarantees regarding its convergence except to a local minimum. The only exceptions are the results in [23, 22] which give provable guarantees for alternating minimization for the problems of matrix sensing and matrix completion."
    }, {
      "heading" : "2 Notation",
      "text" : "We use bold capital letters (A,B etc.) for matrices, bold small case letters (x,y etc.) for vectors and non-bold letters (α,U etc.) for scalars. For every complex vector w ∈ Cn, |w| ∈ Rn denotes its element-wise magnitude vector. wT and AT denote the Hermitian transpose of the vector w and the matrix A respectively. e1, e2, etc. denote the canonical basis vectors in Cn. z denotes the complex conjugate of the complex number z. In this paper we use the standard Gaussian (or normal) distribution over Cn. a is said to be distributed according to this distribution if a = a1+ia2, where a1 and a2 are independent and are distributed according to N (0, I). We also define Ph (z) def = z|z|\nfor every z ∈ C, and dist (w1,w2) def = √ 1− ∣∣∣ 〈w1,w2〉‖w1‖2‖w2‖2 ∣∣∣2 for every w1,w2 ∈ Cn. inally, we use the shorthand wlog for without loss of generality and whp for with high probability."
    }, {
      "heading" : "3 Algorithm",
      "text" : "In this section, we present our alternating minimization based algorithm for solving the phase retrieval problem. Let A ∈ Cn×m be the measurement matrix, with ai as its ith column; similarly let y be the vector of recorded magnitudes. Then,\ny = |ATx∗ |.\nRecall that, given y and A, the goal is to recover x∗. If we had access to the true phase c∗ of ATx∗ (i.e., c∗i = Ph (〈ai,x∗〉)) and m ≥ n, then our problem reduces to one of solving a system of linear equations:\nC∗y = ATx∗,\nwhere C∗ def = Diag(c∗) is the diagonal matrix of phases. Of course we do not know C∗, hence one approach to recovering x∗ is to solve:\nargmin C,x\n‖ATx−Cy‖2, (2)\nwhere x ∈ Cn and C ∈ Cm×m is a diagonal matrix with each diagonal entry of magnitude 1. Note that the above problem is not convex since C is restricted to be a diagonal phase matrix and hence, one cannot use standard convex optimization methods to solve it.\nAlgorithm 1 AltMinPhase\ninput A,y, t0 1: Initialize x0 ← top singular vector of ∑ i y 2 i aiai T\n2: for t = 0, · · · , t0 − 1 do 3: Ct+1 ← Diag ( Ph ( ATxt )) 4: xt+1 ← argminx∈Rn ∥∥ATx−Ct+1y∥∥ 2\n5: end for output xt0\nInstead, our algorithm uses the well-known alternating minimization: alternatingly update x and C so as to minimize (2). Note that given C, the vector x can be obtained by solving the following least squares problem: minx ‖ATx − Cy‖2. Since the number of measurements m is larger than the dimensionality n and since each entry of A is sampled from independent Gaussians, A is invertible with probability 1. Hence, the above least squares problem has a unique solution. On the other hand, given x, the optimal C is given by C = Diag(ATx).\nWhile the above algorithm is simple and intuitive, it is known that with bad initial points, the solution might not converge to x∗. In fact, this algorithm with a uniformly random initial point has been empirically evaluated for example in [43], where it performs worse than SDP based methods. Moreover, since the underlying problem is non-convex, standard analysis techniques fail to guarantee convergence to the global optimum, x∗. Hence, the key challenges here are: a) a good initialization step for this method, b) establishing this method’s convergence to x∗.\nWe address the first key challenge in our AltMinPhase algorithm (Algorithm 1) by initializing x as the largest singular vector of the matrix S = 1m ∑ i yiaiai\nT . Theorem 4.1 shows that when A is sampled from standard complex normal distribution, this initialization is accurate. In particular, if m ≥ C1n log3 n for large enough C1 > 0, then whp we have ‖x0 − x∗‖2 ≤ 1/100 (or any other constant).\nTheorem 4.2 addresses the second key challenge and shows that a variant of AltMinPhase (see Algorithm 2) actually converges to the global optimum x∗ at linear rate. See section 4 for a detailed analysis of our algorithm.\nWe would like to stress that not only does a natural variant of our proposed algorithm have rigorous theoretical guarantees, it also is effective practically as each of its iterations is fast, has a closed form solution and does not require SVD computation. AltMinPhase has similar statistical complexity to that of PhaseLift and PhaseCut while being much more efficient computationally. In particular, for accuracy , we only need to solve each least squares problem only up to accuracy O ( ). Now, since the measurement matrix A is sampled from Gaussian with m > Cn, it is well conditioned. Hence, using conjugate gradient method, each such step takes O ( mn log 1 ) time. When m = O (n) and we have geometric convergence, the total time taken by the algorithm is O ( n2 log2 1 ) . SDP based methods on the other hand require Ω(n3/ √ ) time. Moreover, our initialization step increases the likelihood of successful recovery as opposed to a random initialization (which has been considered so far in prior work). Refer Figure 1 for an empirical validation of these claims."
    }, {
      "heading" : "4 Main Results: Analysis",
      "text" : "In this section we describe the main contribution of this paper: provable statistical guarantees for the success of alternating minimization in solving the phase recovery problem. To this end, we consider the setting where each measurement vector ai is iid and is sampled from the standard complex normal distribution. We would like to stress that all the existing guarantees for phase recovery also use exactly the same setting [7, 6, 43]. Table 1 presents a comparison of the theoretical guarantees of Algorithm 2 as compared to PhaseLift and PhaseCut.\nOur proof for convergence of alternating minimization can be broken into two key results. We first show that if m ≥ Cn log3 n, then whp the initialization step used by AltMinPhase returns x0 which is at most a constant distance away from x∗. Furthermore, that constant can be controlled by using more samples (see Theorem 4.1).\nWe then show that if xt is a fixed vector such that dist ( xt,x∗ ) < c (small enough) and A is sampled independently of xt with m > Cn (C large enough) then whp xt+1 satisfies: dist ( xt+1,x∗ ) < 34dist ( xt,x∗ ) (see Theorem 4.2). Note that our analysis critically requires xt\nto be “fixed” and be independent of the sample matrix A. Hence, we cannot re-use the same A in each iteration; instead, we need to resample A in every iteration. Using these results, we prove the correctness of Algorithm 2, which is a natural resampled version of AltMinPhase.\nAlgorithm 2 AltMinPhase with Resampling\ninput A,y, 1: t0 ← c log 1 2: Partition y and (the corresponding columns of) A into t0 + 1 equal disjoint sets:\n(y0,A0), (y1,A1), · · · , (yt0 ,At0) 3: x0 ← top singular vector of ∑ l ( y0l )2 a0` ( a0` )T 4: for t = 0, · · · , t0 − 1 do 5: Ct+1 ← Diag ( Ph (( At+1 )T xt ))\n6: xt+1 ← argminx∈Rn ∥∥∥(At+1)T x−Ct+1yt+1∥∥∥ 2 7: end for\noutput xt0\nWe now present the two results mentioned above. All the supporting lemmas and proofs can be found in Appendix A. In the following theorems, wlog, we assume that ‖x∗‖2 = 1. Our first result guarantees a good initial vector.\nTheorem 4.1. There exists a constant C1 such that if m > C1 c2 n log3 n, then in Algorithm 2, with probability greater than 1− 4/n2 we have:\n‖x0 − x∗‖2 < c.\nThe second result proves geometric decay of error assuming a good initialization. Theorem 4.2. There exist constants c, ĉ and c̃ such that in iteration t of Algorithm 2, if dist ( xt,x∗ ) <\nc and the number of columns of At is greater than ĉ (\nlog 1η\n) n then, with probability more than 1−η,\nwe have:\ndist ( xt+1,x∗ ) < 3\n4 dist\n( xt,x∗ ) , and ‖xt+1 − x∗‖2 < c̃ dist ( xt,x∗ ) .\nProof. For simplicity of notation in the proof of the theorem, we will use A for At+1, C for Ct+1, x for xt, x+ for xt+1, and y for yt+1. Now consider the update in the (t+ 1)th iteration:\nx+ = argmin x̃∈Rn\n∥∥AT x̃−Cy∥∥ 2 = ( AAT )−1 ACy = ( AAT )−1 ADATx∗, (3)\nwhere D is a diagonal matrix with Dll def = Ph ( a` Tx · a`Tx∗ ) . Now (3) can be rewritten as:\nx+ = ( AAT )−1 ADATx∗ = x∗ + ( AAT )−1 A (D− I)ATx∗, (4)\nthat is, x+ can be viewed as a perturbation of x∗ and the goal is to bound the error term (the second term above). We break the proof into two main steps:\n1. ∃ a constant c1 such that |〈x∗,x+〉| ≥ 1− c1dist (x,x∗) (see Lemma A.2), and\n2. |〈z,x+〉| ≤ 59dist (x,x ∗), for all z s.t. zTx∗ = 0. (see Lemma A.4)\nAssuming the above two bounds and choosing c < 1100c1 , we can prove the theorem:\ndist ( x+,x∗ )2 <\n(25/81) · dist (x,x∗) (1− c1dist (x,x∗))2 ≤ 9 16 dist (x,x∗)2 ,\nproving the first part of the theorem. The second part follows easily from (3) and Lemma A.2.\nIntuition and key challenge: If we look at step 6 of Algorithm 2, we see that, for the measurements, we use magnitudes calculated from x∗ and phases calculated from x. Intuitively, this means that we are trying to push x+ towards x∗ (since we use its magnitudes) and x (since we use its phases) at the same time. The key intuition behind the success of this procedure is that the push towards x∗ is stronger than the push towards x, when x is close to x∗. The key lemma that captures this effect is stated below:\nLemma 4.3. Let w1 and w2 be two independent standard complex Gaussian random variables 2. Let U = |w1|w2 ( Ph ( 1 + √ 1−α2w2 α|w1| ) − 1 ) . Fix δ > 0. Then, there exists a constant γ > 0 such that if √ 1− α2 < γ, then: E [U ] ≤ (1 + δ) √ 1− α2.\nSee Appendix A for a proof of the above lemma and how we use it to prove Theorem 4.2. Combining Theorems 4.1 and 4.2, we have the following theorem establishing the correctness\nof Algorithm 2.\nTheorem 4.4. Suppose the measurement vectors in (1) are independent standard complex normal vectors. For every η > 0, there exists a constant c such that if m > cn ( log3 n+ log 1 log log 1 ) then, with probability greater than 1− η, Algorithm 2 outputs xt0 such that ‖xt0 − x∗‖2 < ."
    }, {
      "heading" : "5 Sparse Phase Retrieval",
      "text" : "In this section, we consider the case where x∗ is known to be sparse, with sparsity k. A natural and practical question to ask here is: can the sample and computational complexity of the recovery algorithm be improved when k n.\nRecently, [28] studied this problem for Gaussian A and showed that for `1 regularized PhaseLift, m = O(k2 log n) samples suffice for exact recovery of x∗. However, the computational complexity of this algorithm is still O(n3/ 2).\nIn this section, we provide a simple extension of our AltMinPhase algorithm that we call SparseAltMinPhase, for the case of sparse x∗. The main idea behind our algorithm is to first recover the support of x∗. Then, the problem reduces to phase retrieval of a k-dimensional signal. We then solve the reduced problem using Algorithm 2. The pseudocode for SparseAltMinPhase is presented in Algorithm 3. Table 2 provides a comparison of Algorithm 3 with `1-regularized PhaseLift in terms of sample complexity as well as computational complexity.\nThe following lemma shows that if the number of measurements is large enough, step 1 of SparseAltMinPhase recovers the support of x∗ correctly.\n2z is standard complex Gaussian if z = z1+iz2 where z1 and z2 are independent standard normal random variables.\nAlgorithm 3 SparseAltMinPhase\ninput A,y, k 1: S ← top-k argmaxj∈[n] ∑m i=1 |aijyi| {Pick indices of k largest absolute value inner product}\n2: Apply Algorithm 2 on AS ,yS and output the resulting vector with elements in S c set to zero.\nSample complexity Comp. complexity Algorithm 3 O ( k ( k log n+ log3 k + log 1 log log 1 )) O ( k2 ( kn log n+ log2 1 log log 1 )) `1-PhaseLift [28] O ( k2 log n ) O ( n3/ 2\n)\nTable 2: Comparison of Algorithm 3 with `1-PhaseLift when x ∗ min = Ω\n( 1/ √ k ) . Note that the\ncomplexity of Algorithm 3 is dominated by the support finding step. If k = O (1), Algorithm 3 runs in quasi-linear time.\nLemma 5.1. Suppose x∗ is k-sparse with support S and ‖x∗‖2 = 1. If ai are standard complex Gaussian random vectors and m > c\n(x∗min) 4 log\nn δ , then Algorithm 3 recovers S with probability greater\nthan 1− δ, where x∗min is the minimum non-zero entry of x∗.\nThe key step of our proof is to show that if j ∈ supp(x∗), then random variable Zij = ∑\ni |aijyi| has significantly higher mean than for the case when j /∈ supp(x∗). Now, by applying appropriate concentration bounds, we can ensure that minj∈supp(x∗) |Zij | > maxj /∈supp(x∗) |Zij | and hence our algorithm never picks up an element outside the true support set supp(x∗). See Appendix B for a detailed proof of the above lemma.\nThe correctness of Algorithm 3 now is a direct consequence of Lemma 5.1 and Theorem 4.4. For the special case where each non-zero value in x∗ is from {− 1√\nk , 1√ k }, we have the following\ncorollary:\nCorollary 5.2. Suppose x∗ is k-sparse with non-zero elements ± 1√ k . If the number of measurements m > c ( k2 log nδ + k log 2 k + k log 1 ) , then Algorithm 3 will recover x∗ up to accuracy with probability greater than 1− δ."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section, we present experimental evaluation of AltMinPhase (Algorithm 1) and compare its performance with the SDP based methods PhaseLift [7] and PhaseCut [43]. We also empirically demonstrate the advantage of our initialization procedure over random initialization (denoted by AltMin (random init)), which has thus far been considered in the literature [17, 15, 43, 5]. AltMin (random init) is the same as AltMinPhase except that step 1 of Algorithm 1 is replaced with:x0 ← Uniformly random vector from the unit sphere.\nIn the noiseless setting, a trial is said to succeed if the output x satisfies ‖x− x∗‖2 < 10−2. For a given dimension, we do a linear search for smallest m (number of samples) such that empirical success ratio over 20 runs is at least 0.8. We implemented our methods in Matlab, while we obtained the code for PhaseLift and PhaseCut from the authors of [35] and [43] respectively.\nWe now present results from our experiments in three different settings. Independent Random Gaussian Measurements: Each measurement vector ai is generated from the standard complex Gaussian distribution. This measurement scheme was first suggested\nby [7] and till date, this is the only scheme with theoretical guarantees. Multiple Random Illumination Filters: We now present our results for the setting where the measurements are obtained using multiple illumination filters; this setting was suggested by [5]. In particular, choose J vectors z(1), · · · , z(J) and compute the following discrete Fourier transforms:\nx̂(u) = DFT ( x∗ · ∗ z(u) ) ,\nwhere ·∗ denotes component-wise multiplication. Our measurements will then be the magnitudes of components of the vectors x̂(1), · · · , x̂(J). The above measurement scheme can be implemented by modulating the light beam or by the use of masks; see [5] for more details.\nFor this setting, we conduct a similar set of experiments as the previous setting. That is, we vary dimensionality of the true signal z(u) (generated from the Gaussian distribution)and then empirically determine measurement and computational cost of each algorithm. Figures 2 (a) and (b) present our experimental results for this measurement scheme. Here again, we make similar observations as the last setting. That is, the measurement complexity of AltMinPhase is similar to PhaseCut and PhaseLift, but AltMinPhase is orders of magnitude faster than PhaseLift and PhaseCut. Note that Figure 2 is on a log-scale.\nNoisy Phase Retrieval: Finally, we study our method in the following noisy measurement scheme:\nyi = |〈ai,x∗ + wi〉| for i = 1, . . . ,m, (5)\nwhere wi is the noise in the i-th measurement and is sampled from N (0, σ2). We fix n = 64 and m = 6n. We then vary the amount of noise added σ and measure the `2 error in recovery, i.e., ‖x − x∗‖2, where x is the recovered vector. Figure 2(c) compares the performance of various methods with varying amount of noise. We observe that our method outperforms PhaseLift and has similar recovery error as PhaseCut."
    }, {
      "heading" : "A Proofs for Section 4",
      "text" : "A.1 Proof of the Initialization Step Proof of Theorem 4.1. Recall that x0 is the top singular vector of S = 1n ∑\n` |a`Tx∗|2a`a`T . As a` are rotationally invariant random variables, wlog, we can assume that x∗ = e1 where e1 is the first canonical basis vector. Also note that E [ |〈a, e1〉|2aaT ] = D, where D is a diagonal matrix with D11 = Ea∼NC(0,1)[|a| 4] = 8 and Dii = Ea∼NC(0,1),b∼NC(0,1)[|a|\n2|b|2] = 1,∀i > 1. We break our proof of the theorem into two steps:\n(1): Show that, with probability > 1− 2 n2 : ‖S−D‖2 < c/4. (2): Use (1) to prove the theorem.\nProof of Step (2): We have 〈x0,Sx0〉 ≤ c/4+3 (( x0 )T\ne1\n)2 + ∑n\ni=2(x 0 i) 2 = c/4+2\n(( x0 )T\ne1\n)2 +\n1. On the other hand, since x0 is the top singular value of S, by using triangle inequality, we have 〈x0,Sx0〉 > 3− c/4. Hence, 〈x0, e1〉2 > 1− c/2. This yields ‖x0 − x∗‖22 = 2− 2〈x0, e1〉2 < c.\nProof of Step (1): We now complete our proof by proving (1). To this end, we use the following matrix concentration result from [40]:\nTheorem A.1 (Theorem 1.5 of [40]). Consider a finite sequence Xi of self-adjoint independent random matrices with dimensions n×n. Assume that E[Xi] = 0 and ‖Xi‖2 ≤ R,∀i, almost surely. Let σ2 := ‖ ∑ i E[Xi]‖2. Then the following holds ∀ν ≥ 0:\nP ( ‖ 1 m m∑ i=1 Xi‖2 ≥ ν ) ≤ 2n exp ( −m2ν2 σ2 +Rmν/3 ) .\nNote that Theorem A.1 assumes max` |a1`|2‖a`‖2 to be bounded, where a1` is the first component of a`. However, a` is a normal random variable and hence can be unbounded. We address this issue by observing that probability that Pr(‖a`‖2 ≥ 2n OR |a1`|2 ≥ 2 logm) ≤ 2 exp(−n/2) + 1m2 . Hence, for large enough n, ĉ and m > ĉn, w.p. 1− 3\nm2 ,\nmax ` |a1`|2‖a`‖2 ≤ 4n log(m). (6)\nNow, consider truncated random variable ã` s.t. ã` = a` if |a1`|2 ≤ 2 log(m)&‖a`‖2 ≤ 2n and ã` = 0 otherwise. Now, note that ã` is symmetric around origin and also E[ãi`ãj`] = 0,∀i 6= j. Also, E[|ãi`|2] ≤ 1. Hence, ‖E[|ã1`|2‖ã`‖2ã`ã†`]‖2 ≤ 4n log(m). Now, applying Theorem A.1 given above, we get (w.p. ≥ 1− 1/n2)\n‖ 1 m ∑ ` |ã1`|2ã`ã†` − E[|ã1`| 2ã`ã † `]‖2 ≤ 4n log(n) log(m)√ m .\nFurthermore, a` = ã` with probability larger than 1− 3m2 . Hence, w.p. ≥ 1− 4 n2 :\n‖S − E[|ã1` |2ã`ã † `]‖2 ≤ 4n log(n) log(m)√ m .\nNow, the remaining task is to show that ‖E[|ã1` |2ã`ã † `] − E[|a 1 ` |2a`a † `]‖2 ≤ 1 m . This follows easily by observing that E[ãi`ã j ` ] = 0 and by bounding E[|ã 1 ` |2|ãi`|2 − |a1` |2|ai`|2 ≤ 1/m by using a simple second and fourth moment calculations for the normal distribution.\nA.2 Proof of per step reduction in error\nIn all the lemmas in this section, δ is a small numerical constant (can be taken to be 0.01).\nLemma A.2. Assume the hypothesis of Theorem 4.2 and let x+ be as defined in (3). Then, there exists an absolute numerical constant c such that the following holds (w.p. ≥ 1 − η4 ):∥∥∥(AAT )−1A (D− I)ATx∗∥∥∥\n2 < cdist (x∗,x) .\nProof. Using (4) and the fact that ‖x∗‖2 = 1, x∗Tx+ = 1 + x∗T ( AAT )−1 A (D− I)ATx∗. That\nis, |x∗Tx+| ≥ 1−‖ (\n1 2mAA T )−1 ‖2‖ 1√2mA‖2‖ 1√2m (D− I)ATx∗‖2. Now, using standard bounds on\nthe singular values of Gaussian matrices [42] and assuming m > ĉ log 1ηn, we have (w.p. ≥ 1− η 4 ): ‖ (\n1 2mAA T )−1 ‖2 ≤ 1/(1 − 2/√ĉ)2 and ‖A‖2 ≤ 1 + 2/√ĉ. Note that both the quantities can be bounded by constants that are close to 1 by selecting a large enough ĉ. Also note that 12mAA T converges to I (the identity matrix), or equivalently 1mAA T converges to 2I since the elements of A are standard normal complex random variables and not standard normal real random variables. The key challenge now is to bound ∥∥(D− I)ATx∗∥∥ 2 by c √ mdist ( x∗,xt ) for a global constant\nc > 0. Note that since (4) is invariant with respect to ∥∥xt∥∥\n2 , we can assume that ∥∥xt∥∥ 2\n= 1. Note further that, since the distribution of A is rotationally invariant and is independent of x∗ and xt, wlog, we can assume that x∗ = e1 and x t = αe1 + √\n1− α2e2, where α = 〈xt,x∗〉. Hence, ∥∥(D− I)ATe1∥∥22 = ∑ml=1 |a1l|2 ∣∣∣Ph((αa1l +√1− α2a2l) a1l)− 1∣∣∣2 = ∑ml=1 U`, where Ul is given by,\nUl def = |a1l|2 ∣∣∣Ph((αa1l +√1− α2a2l) a1l)− 1∣∣∣2 . (7) Using Lemma A.3 finishes the proof.\nThe following lemma, Lemma A.3 shows that if U` are as defined in Lemma A.2 then, the sum of U`, 1 ≤ ` ≤ m concentrates well around E [U`] and also E [U`] ≤ c √ mdist ( x∗,xt ) . The proof of Lemma A.3 requires careful analysis as it provides tail bound and expectation bound of a random variable that is a product of correlated sub-exponential complex random variables.\nLemma A.3. Assume the hypothesis of Lemma A.2. Let U` be as defined in (7) and let each a1l, a2l, ∀1 ≤ l ≤ m be sampled from standard normal distribution for complex numbers. Then, with probability greater than 1− η4 , we have: ∑m l=1 Ul ≤ c2m(1− α2), for a global constant c > 0.\nProof of Lemma A.3. We first estimate P [Ul > t] so as to:\n1. Calculate E [Ul] and,\n2. Show that Ul is a subexponential random variable and use that fact to derive concentration bounds.\nNow, P [Ul > t] = ∫∞√\nt 2\np|a1l|(s)P [ Wl > √ t s ∣∣∣|a1l| = s] ds, where, Wl def = ∣∣∣Ph((αa1l +√1− α2a2l) a1l)− 1∣∣∣ .\nP [ Wl > √ t\ns ∣∣∣∣|a1l| = s] = P [∣∣∣Ph((αa1l +√1− α2a2l) a1l)− 1∣∣∣ > √ts ∣∣∣∣|a1l| = s]\n= P [∣∣∣∣∣Ph ( 1 + √ 1− α2a2l αa1l ) − 1 ∣∣∣∣∣ > √ t s ∣∣∣∣∣|a1l| = s ]\n(ζ1)\n≤ P [√ 1− α2 |a2l| α |a2l| > c √ t s ∣∣∣∣∣|a1l| = s ]\n= P [ |a2l| > cα √ t√\n1− α2 ] (ζ2)\n≤ exp ( 1− cα 2t\n1− α2\n) ,\nwhere (ζ1) follows from Lemma A.7 and (ζ2) follows from the fact that a2l is a sub-gaussian random variable. So we have:\nP [Ul > t] ≤ ∫ ∞ √ t\n2\nexp ( 1− cα 2t\n1− α2\n) ds = exp ( 1− cα 2t\n1− α2 )∫ ∞ √ t\n2\nse− s2 2 ds = exp ( 1− ct\n1− α2\n) .\n(8)\nUsing this, we have the following bound on the expected value of Ul: E [Ul] = ∫ ∞ 0 P [Ul > t] dt ≤ ∫ ∞ 0 exp ( 1− ct 1− α2 ) dt ≤ c ( 1− α2 ) . (9)\nFrom (8), we see that Ul is a subexponential random variable with parameter c ( 1− α2 ) . Using Proposition 5.16 from [42], we obtain:\nP [∣∣∣∣∣ m∑ l=1 Ul − E [Ul] ∣∣∣∣∣ > δm (1− α2) ] ≤ 2 exp ( −min ( cδ2m2 ( 1− α2 )2 (1− α2)2m , cδm ( 1− α2 ) 1− α2 )) ≤ 2 exp ( −cδ2m ) ≤ η\n4 .\nSo, with probability greater than 1− η4 , we have: m∑ l=1 Ul ≤ c2m(1− α2).\nThis proves the lemma.\nLemma A.4. Assume the hypothesis of Theorem 4.2 and let x+ be as defined in (3). Then, ∀z s.t. 〈z,x∗〉 = 0, the following holds (w.p. ≥ 1− η4e −n): |〈z,x+〉| ≤ 59dist (x ∗,x). Proof. Fix z such that 〈z,x∗〉 = 0. Since the distribution of A is rotationally invariant, wlog we can assume that: a) x∗ = e1, b) x = αe1 + √ 1− α2e2 where α ∈ R and α ≥ 0 and c)\nz = βe2 + √\n1− |β|2e3 for some β ∈ C. Note that we first prove the lemma for a fixed z and then using union bound, we obtain the result ∀z ∈ Cn. We have:∣∣〈z,x+〉∣∣ ≤ |β| |〈e2,x+〉|+√1− |β|2|〈e3,x+〉|. (10)\nNow,∣∣e2Tx+∣∣ = ∣∣∣e2T (AAT )−1A (D− I)ATe1∣∣∣ ≤ 1\n2m ∣∣∣∣∣e2T (( 1 2m AAT )−1 − I ) A (D− I)ATe1 ∣∣∣∣∣+ 12m ∣∣e2TA (D− I)ATe1∣∣ ≤ 1\n2m ∥∥∥∥∥ ( 1 2m AAT )−1 − I ∥∥∥∥∥ 2 ‖A‖2 ∥∥(D− I)ATe1∥∥2 + 12m ∣∣e2TA (D− I)ATe1∣∣ ,\n≤ 4c√ ĉ dist\n( xt,x∗ ) + 1\n2m ∣∣e2TA (D− I)ATe1∣∣ , (11) where the last inequality follows from the proof of Lemma A.2.\nSimilarly,∣∣e3Tx+∣∣ = ∣∣∣e3T (AAT )−1A (D− I)ATe1∣∣∣ ≤ 1\n2m ∣∣∣∣∣e3T (( 1 2m AAT )−1 − I ) A (D− I)ATe1 ∣∣∣∣∣+ 12m ∣∣e3TA (D− I)ATe1∣∣ ≤ 1\n2m ∥∥∥∥∥ ( 1 2m AAT )−1 − I ∥∥∥∥∥ 2 ‖A‖2 ∥∥(D− I)ATe1∥∥2 + 12m ∣∣e3TA (D− I)ATe1∣∣\n≤ 4c√ ĉ dist\n( xt,x∗ ) + 1\n2m ∣∣e3TA (D− I)ATe1∣∣ , (12) Again, the last inequality follows from the proof of Lemma A.2. The lemma now follows by using (10), (11), (12) along with Lemmas A.5 and A.6.\nLemma A.5. Assume the hypothesis of Theorem 4.2 and the notation therein. Then,∣∣e2TA (D− I)ATe1∣∣ ≤ 100 99 m √ 1− α2,\nwith probability greater than 1− η10e −n.\nProof. We have:\ne2 TA (D− I)ATe1 = m∑ l=1 a1la2l ( Ph (( αa1l + √ 1− α2a2l ) a1l ) − 1 )\n= m∑ l=1 |a1l| a′2l ( Ph ( α |a1l|+ √ 1− α2a′2l ) − 1 ) ,\nwhere a′2l def = a2lPh (a1l) is identically distributed to a2l and is independent of |a1l|. Define the random variable Ul as:\nUl def = |a1l| a′2l\n( Ph ( 1 +\n√ 1− α2a′2l α |a1l|\n) − 1 ) .\nSimilar to Lemma A.2, we will calculate P [Ul > t] to show that Ul is subexponential and use it to derive concentration bounds. However, using the above estimate to bound E [Ul] will result in a weak bound that we will not be able to use. Lemma 4.3 bounds E [Ul] using a different technique carefully.\nP [|Ul| > t] ≤ P [ |a1l| ∣∣a′2l∣∣ c√1− α2 |a′2l|α |a1l| > t ]\n= P [∣∣a′2l∣∣2 > cαt√\n1− α2\n] ≤ exp ( 1− cαt√\n1− α2\n) ,\nwhere the last step follows from the fact that a′2l is a subgaussian random variable and hence |a′2l| 2 is a subexponential random variable. Using Proposition 5.16 from [42], we obtain:\nP [∣∣∣∣∣ m∑ l=1 Ul − E [Ul] ∣∣∣∣∣ > δm√1− α2 ] ≤ 2 exp ( −min ( cδ2m2 ( 1− α2 ) (1− α2)m , cδm √ 1− α2√ 1− α2 )) ≤ 2 exp ( −cδ2m ) ≤ η\n10 exp (−n) .\nUsing Lemma 4.3, we obtain:∣∣e2TA (D− I)ATe1∣∣ = ∣∣∣∣∣ m∑ l=1 Ul ∣∣∣∣∣ ≤ (1 + δ)m√1− α2, with probability greater than 1− η10 exp(−n). This proves the lemma.\nProof of Lemma 4.3. Let w2 = |w2| eiθ. Then |w1| , |w2| and θ are all independent random variables. θ is a uniform random variable over [−π, π] and |w1| and |w2| are identically distributed with probability distribution function:\np(x) = x exp ( −x 2\n2\n) 1{x≥0}.\nWe have:\nE [U ] = E [ |w1| |w2| eiθ ( Ph ( 1 + √ 1− α2 |w2| e−iθ\nα |w1|\n) − 1 )]\n= E [ |w1| |w2|E [ eiθ ( Ph ( 1 + √ 1− α2 |w2| e−iθ\nα |w1|\n) − 1 )]∣∣∣∣∣|w1| , |w2| ]\nLet β def = √ 1−α2|w2| α|w1| . We will first calculate E\n[ eiθPh ( 1 + βe−iθ )∣∣|w1| , |w2|]. Note that the above expectation is taken only over the randomness in θ. For simplicity of notation, we will drop the conditioning variables, and calculate the above expectation in terms of β.\neiθPh ( 1 + βe−iθ ) = (cos θ + i sin θ) 1 + β cos θ − iβ sin θ[\n(1 + β cos θ)2 + β2 sin2 θ ] 1 2\n= cos θ + β + i sin θ\n(1 + β2 + 2β cos θ) 1 2\n.\nWe will first calculate the imaginary part of the above expectation: Im ( E [ eiθPh ( 1 + βe−iθ )]) = E [ sin θ\n(1 + β2 + 2β cos θ) 1 2\n] = 0, (13)\nwhere the last step follows because we are taking the expectation of an odd function. Focusing on the real part, we let:\nF (β) def = E\n[ cos θ + β\n(1 + β2 + 2β cos θ) 1 2\n]\n= 1\n2π ∫ π −π\ncos θ + β\n(1 + β2 + 2β cos θ) 1 2\ndθ.\nNote that F (β) : R → R and F (0) = 0. We will show that there is a small absolute numerical constant γ (depending on δ) such that:\n0 < β < γ ⇒ |F (β)| ≤ (1 2 + δ)β. (14)\nWe show this by calculating F ′(0) and using the continuity of F ′(β) at β = 0. We first calculate F ′(β) as follows:\nF ′(β) = 1\n2π ∫ π −π\n1\n(1 + β2 + 2β cos θ) 1 2 − (cos θ + β) (β + cos θ) (1 + β2 + 2β cos θ) 3 2 dθ\n= 1\n2π ∫ π −π\nsin2 θ\n(1 + β2 + 2β cos θ) 3 2\ndθ\nFrom the above, we see that F ′(0) = 12 and (14) then follows from the continuity of F ′(β) at β = 0. Getting back to the expected value of U , we have:\n|E [U ]| = ∣∣∣∣∣E [ |w1| |w2|F (√ 1− α2 |w2| α |w1| ) 1{√\n1−α2|w2| α|w1| <γ\n} ]\n+E [ |w1| |w2|F (√ 1− α2 |w2| α |w1| ) 1{√\n1−α2|w2| α|w1|\n≥γ } ]∣∣∣∣∣\n= ∣∣∣∣∣E [ |w1| |w2|F (√ 1− α2 |w2| α |w1| ) 1{√\n1−α2|w2| α|w1| <γ\n} ]∣∣∣∣∣\n+ ∣∣∣∣∣E [ |w1| |w2|F (√ 1− α2 |w2| α |w1| ) 1{√\n1−α2|w2| α|w1|\n≥γ } ]∣∣∣∣∣\n(ζ1) ≤ ( 1\n2 + δ\n) E [ |w1| |w2|\n√ 1− α2 |w2| α |w1|\n] + E [ |w1| |w2|1{√1−α2|w2| α|w1| ≥γ } ] ,\n=\n( 1\n2 + δ )(√ 1− α2 α ) E [ |w2|2 ] + E [ |w1| |w2|1{√1−α2|w2| α|w1| ≥γ } ] ,\n(ζ2) = (1 + 2δ) (√ 1− α2 α ) + E [ |w1| |w2|1{√1−α2|w2| α|w1| ≥γ } ] , (15)\nwhere (ζ1) follows from (14) and the fact that |F (β)| ≤ 1 for every β and (ζ2) follows from the fact that E [ |z2|2 ] = 2. We will now bound the second term in the above inequality. We start with the following integral: ∫ ∞ t s2e− s2 2 ds = − ∫ ∞ t sd ( e− s2 2\n) = te− t2 2 +\n∫ ∞ t e− s2 2 ds ≤ (t+ e)e− t2 c , (16)\nwhere c is some constant. The last step follows from standard bounds on the tail probabilities of gaussian random variables. We now bound the second term of (15) as follows:\nE [ |w1| |w2|1{√1−α2|w2| α|w1| ≥γ } ] = ∫ ∞ 0 t2e− t2 2 ∫ ∞ αt√ 1−α2 s2e− s2 2 dsdt\n(ζ1) ≤ ∫ ∞ 0 t2e− t2 2 ( αt√ 1− α2 + e ) e − α 2t2 c(1−α2)dt\n≤ ∫ ∞ 0 ( αt3√ 1− α2 + et2 ) e − t 2 c(1−α2)dt\n= α√\n1− α2 ∫ ∞ 0 t3e − t 2 c(1−α2)dt+ e ∫ ∞ 0 t2e − t 2 c(1−α2)dt\n(ζ2) ≤ c ( 1− α2 ) 3 2 (ζ3) ≤ δ √ 1− α2\nwhere (ζ1) follows from (16), (ζ2) follows from the formulae for second and third absolute moments of gaussian random variables and (ζ3) follows from the fact that 1 − α2 < δ. Plugging the above inequality in (15), we obtain:\n|E [U ]| ≤ (1 + 2δ) (√ 1− α2 α ) + δ √ 1− α2 ≤ (1 + 4δ) √ 1− α2,\nwhere we used the fact that α ≥ 1− δ2 . This proves the lemma.\nLemma A.6. Assume the hypothesis of Theorem 4.2 and the notation therein. Then,∣∣e3TA (D− I)ATe1∣∣ ≤ δm√1− α2, with probability greater than 1− η10e −n.\nProof. The proof of this lemma is very similar to that of Lemma A.5. We have:\ne3 TA (D− I)ATe1 = m∑ l=1 a1la3l ( Ph (( αa1l + a2l √ 1− α2a3l ) a1l ) − 1 )\n= m∑ l=1 |a1l| a′3l ( Ph ( α |a1l|+ a′2l √ 1− α2 ) − 1 ) ,\nwhere a′3l def = a3lPh (a1l) is identically distributed to a3l and is independent of |a1l| and a′2l. Define the random variable Ul as:\nUl def = |a1l| a′3l\n( Ph ( 1 + a′2l √\n1− α2 α |a1l|\n) − 1 ) .\nSince a′3l has mean zero and is independent of everything else, we have:\nE [Ul] = 0.\nSimilar to Lemma A.5, we will calculate P [Ul > t] to show that Ul is subexponential and use it to derive concentration bounds.\nP [|Ul| > t] ≤ P [ |a1l| ∣∣a′3l∣∣ c√1− α2 |a′2l|α |a1l| > t ]\n= P [∣∣a′2la′3l∣∣ > cαt√\n1− α2\n] ≤ exp ( 1− cαt√\n1− α2\n) ,\nwhere the last step follows from the fact that a′2l and a ′ 3l are independent subgaussian random variables and hence |a′2la′3l| is a subexponential random variable. Using Proposition 5.16 from [42], we obtain:\nP [∣∣∣∣∣ m∑ l=1 Ul − E [Ul] ∣∣∣∣∣ > δm√1− α2 ] ≤ 2 exp ( −min ( cδ2m2 ( 1− α2 ) (1− α2)m , cδm √ 1− α2√ 1− α2 )) ≤ 2 exp ( −cδ2m ) ≤ η\n10 exp (−n) .\nHence, we have:\n∣∣e3TA (D− I)ATe1∣∣ = ∣∣∣∣∣ m∑ l=1 Ul ∣∣∣∣∣ ≤ δm√1− α2, with probability greater than 1− η10 exp(−n). This proves the lemma.\nLemma A.7. For every w ∈ C, we have:\n|Ph (1 + w)− 1| ≤ 2 |w| .\nProof. The proof is straight forward:\n|Ph (1 + w)− 1| ≤ |Ph (1 + w)− (1 + w)|+ |w| = |1− |1 + w||+ |w| ≤ 2 |w| ."
    }, {
      "heading" : "B Proofs for Section 5",
      "text" : "Proof of Lemma 5.1. For every j ∈ [n] and i ∈ [m], consider the random variable Zij def = |aijyi|. We have the following:\n• if j ∈ S, then\nE [Zij ] = 2\nπ\n(√ 1− ( x∗j )2 + x∗j arcsinx ∗ j )\n≥ 2 π\n( 1− 5\n6\n( x∗j )2 − 1\n6\n( x∗j )4 + x∗j ( x∗j + 1\n6\n( x∗j )3))\n≥ 2 π + 1 6 (x∗min) 2 ,\nwhere the first step follows from Corollary 3.1 in [27] and the second step follows from the Taylor series expansions of √ 1− x2 and arcsin(x),\n• if j /∈ S, then E [Zij ] = E [|aij |]E [|yi|] = 2π and finally,\n• for every j ∈ [n], Zij is a sub-exponential random variable with parameter c = O(1) (since it is a product of two standard normal random variables).\nUsing the hypothesis of the theorem about m, we have: • for any j ∈ S, P [\n1 m ∑m i=1 Zij − ( 2 π + 1 12 (x ∗ min) 2 ) < 0 ] ≤ exp ( −c (x∗min) 4m ) ≤ δn−c, and\n• for any j /∈ S, P [\n1 m ∑m i=1 Zij − ( 2 π + 1 12 (x ∗ min) 2 ) > 0 ] ≤ exp ( −c (x∗min) 4m ) ≤ δn−c.\nApplying a union bound to the above, we see that with probability greater than 1 − δ, there is a separation in the values of 1m ∑m i=1 Zij for j ∈ S and j /∈ S. This proves the theorem."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Phase retrieval problems involve solving linear equations, but with missing sign (or phase, for<lb>complex numbers) information. Over the last two decades, a popular generic empirical approach<lb>to the many variants of this problem has been one of alternating minimization; i.e. alternating<lb>between estimating the missing phase information, and the candidate solution. In this paper, we<lb>show that a simple alternating minimization algorithm geometrically converges to the solution<lb>of one such problem – finding a vector x from y,A, where y = |Ax| and |z| denotes a vector<lb>of element-wise magnitudes of z – under the assumption that A is Gaussian.<lb>Empirically, our algorithm performs similar to recently proposed convex techniques for this<lb>variant (which are based on “lifting” to a convex matrix problem) in sample complexity and<lb>robustness to noise. However, our algorithm is much more efficient and can scale to large<lb>problems. Analytically, we show geometric convergence to the solution, and sample complexity<lb>that is off by log factors from obvious lower bounds. We also establish close to optimal scaling<lb>for the case when the unknown vector is sparse. Our work represents the only known theoretical<lb>guarantee for alternating minimization for any variant of phase retrieval problems in the non-<lb>convex setting.",
    "creator" : "LaTeX with hyperref package"
  }
}