{
  "name" : "1602.02355.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Hyperparameter optimization with approximate gradient",
    "authors" : [ "Fabian Pedregosa" ],
    "emails" : [ "F@BIANP.NET" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Most models in machine learning feature at least one hyperparameter to control for model complexity. Regularized models, for example, control the trade-off between a data fidelity term and a regularization term through one or several hyperparameters. Among its most well-known instances are the LASSO (Tibshirani, 1996), in which `1 regularization is added to a squared loss to encourage sparsity in the solutions, or `2-regularized logistic regression, in which squared `2 regularization (known as weight decay in the context of neural networks) is added to obtain solutions with small euclidean norm. Another class of hyperparam-\nCopyright 2016 by the author(s).\n10−2 10−1 100 101 102 103 104 105\nregularization parameter\ncr os\nsva\nlid at\nio n\nlo ss\nFigure 1: Hyperparameter Optimization with approximate gradient. The gradient of the cross-validation loss function with respect to hyperparameters is computed approximately. This noisy gradient is then used to estimate the optimal hyperparameters by gradient descent. A decreasing bound between the true gradient and the approximate gradient ensures that the method converges towards a stationary point.\neters are the kernel parameters in support vector machines. For example, the popular radial basis function (RBF) kernel depends on a “width” parameter, while polynomial kernels depend on a discrete hyperparameter specifying the degree. Hyperparameters can be broadly categorized into two groups: continuous hyperparameters, such as regularization parameters or the width of an RBF kernel and discrete hyperparameters, such as the degree of a polynomial. In this work we focus on continuous hyperparameters.\nThe problem of identifying the optimal set of hyperparameters is known as hyperparameter optimization. Hyperparameters cannot be estimated to minimize the same cost function as model parameters, since this would favor models with excessive complexity. For example, if regularization parameters were chosen to minimize the same loss as model parameters, then models with no regularization would always yield the smallest loss. For this reason, hy-\nar X\niv :1\n60 2.\n02 35\n5v 1\n[ st\nat .M\nL ]\n7 F\neb 2\n01 6\nperparameter optimization algorithms seek to optimize a criterion of model quality which is different from the cost function used to fit model parameters. This criterion can be a goodness of fit on unseen data, such as a cross-validation loss, or some criteria of model quality on the train set such as SURE (Stein, 1981), AIC/BIC (Liu and Yang, 2011) or Mallows Cp (Mallows, 1973), to name a few.\nChoosing the appropriate set of hyperparameters has often a dramatic influence in model accuracy and many hyperparameter optimization algorithms have been proposed in the literature. For example, in the widely used grid-search algorithm, the model is trained over a range of values for the hyperparameters and the value that gives the best performance on the cross-validation loss. This does not only scale poorly with the number of hyperparameters, but also involves fitting the full model for values of hyperparameters that are very unpromising. Random search (Bergstra et al., 2011) has been proven to yield a faster exploration of the hyperparameter space than grid search, specially in spaces with multiple hyperparameters. However, none of these methods make use of previous evaluations to make an informed decision of the next iterate. As such, convergence to a global minima can be very slow.\nIn recent years, sequential model-based optimization (SMBO) techniques have emerged as a powerful tool for hyperparameter optimization (see e.g. (Brochu et al., 2010) for an review on current methodologies). These techniques proceed by fitting a probabilistic model to the data and then using this model as an inexpensive proxy in order to determine the most promising location to evaluate next. This probabilistic model typically relies on a Gaussian process regressor but other approaches exist using trees (Bergstra et al., 2011) or ensemble methods (Lacoste et al., 2014). The model is build using only function evaluations, and for this reason SMBO is often considered as a black-box optimization method.\nA third family of methods, of which the method that we present can be seen as a particular instance, estimate the optimal hyperparameters using smooth optimization techniques such as gradient descent. We will refer to these methods as gradient-based hyperparameter optimization methods. These methods use local information about the cost function in order to compute the gradient of the cost function with respect to hyperparameters. However, computing the gradient with respect to hyperparameters has reveled to be a major bottleneck in this approach. For this reason we propose an algorithm that replaces the gradient with an approximation. More precisely, we make the following contributions:\n• We propose a gradient-based hyperparameter optimization algorithm that uses approximate gradient information rather than the true gradient.\n• We provide sufficient conditions for the convergence of this method to a stationary point.\n• We compare this approach against state-of-the art methods for the task of estimation of regularization and kernel parameter on two different models and three datasets.\nNotation We denote the gradient of a real-valued function by ∇. If this function has several input arguments, we denote∇i its gradient with respect to the i-th argument. Similarly, ∇2 denotes the Hessian and ∇2i,j denotes the second order differential with respect to variables i and j. For functions that are not real-valued, we denote its differential by D. We denote the projection operator onto a set D by PD. That is, PD(α) , arg minλ∈D ‖α− λ‖2, where ‖ · ‖ denotes the euclidean norm for vectors.\nThroughout the paper we take the convention of denoting real-valued functions with lowercase letters (such as f and g) and vector-valued functions with uppercase letters (such as X). Model parameters are denoted using lowercase Latin letters (such as x) while hyperparameters are denoted using Greek lowercase letters (such as λ)."
    }, {
      "heading" : "1.1. Problem setting",
      "text" : "As mentioned in the introduction, the goal of hyperparameter optimization is to choose the hyperparameters λ that optimizes some criteria, such as a cross-validation loss or a SURE/AIC/BIC criteria. We will denote this criteria by f : Rs → R, where s is the number of hyperparameters. In its simplest form, the hyperparameter optimization problem can be seen as the problem of minimizing the cost function f over a domain D ⊆ Rs. Some approaches, such as sequential model-based optimization, only require function evaluations of this cost function. However, in order to access the local information of this cost function, we will further inspect this function.\nThe cost function f depends on the model parameters, which we will denote by X(λ). These are commonly not available in closed form but rather defined implicitly as the minimizers of some cost function that we will denote h(·, λ) : Rp → R, where p is the number of model parameters. This makes the hyperparmater optimization problem can be naturally expressed as a nested or bi-level optimization problem:\narg min λ∈D\n{ f(λ) , g(X(λ), λ) } s.t. X(λ) ∈ arg min\nx∈Rp h(x, λ) ,\n(HO)\nwhere the minimization over h is commonly referred to as the inner optimization problem. A notable example of\nhyperparameter optimization problem is that of regularization parameter selection by cross-validation. For simplicity, we restrict the discussion to the case of simple or holdout cross-validation, where the dataset is split only once, although the methods presented here extend naturally to other cross-validation schemes. In this setting, the dataset is split in two: a train set (denoted Strain) and a test or holdout set (denoted Stest). In this case, the outer cost function is a goodness of fit or loss on the test set, while the inner one is a trade-off between a data fitting term on the train set and a penalty term. If the penalty term is a squared `2-norm, then the problem adopts the form:\narg min λ∈D\nloss(Stest, X(λ))\ns.t. X(λ) ∈ arg min x∈Rp\nloss(Strain, x) + eλ‖x‖2 . (1)\nThe trade-off in the inner optimization between the goodness of fit term and the penalty term is controlled through the hyperparamter λ. Higher values of λ bias the model parameters towards vectors with small euclidean norm, and the goal of the hyperparameter optimization problem is to find the right trade-off between these two terms. The parametrization of the regularization parameter by an exponential in Eq. (1) might seem unusual, but given that this regularization parameter is commonly optimized over a log-spaced grid, we will find this parametrization useful in later sections.\nTurning back to the general problem (HO), we will now describe an approach to compute the derivative of the cost function f with respect to hyperparameters. This approach, which we will refer to as implicit differentiation (Larsen et al., 1996; Bengio, 2000; Foo et al., 2008), relies on the observation that under some regularity conditions it is possible to replace the inner optimization problem by an implicit equation. For example, if h is smooth and verifies that all stationary points are global minima (as is the case for convex functions), then the values X(λ) are characterized by the implicit equation∇1h(X(λ), λ) = 0. Deriving the implicit equation with respect to λ leads to the equation ∇21,2h+∇21h ·DX = 0, which, assuming ∇21h invertible, characterizes the derivative of X . The chain rule, together with this equation, allows us to write the following formula for the gradient of f :\n∇f = ∇2g + (DX)T∇1g = ∇2g − ( ∇21,2h )T ( ∇21h )−1 ∇1g .\n(2)\nThis formula allows to compute the gradient of f given the following quantities: model parameters X(λ) (g and h are evaluated at (X(λ), λ)) and ( ∇21h )−1∇1g, which is usually computed as the solution to the linear system( ∇21h ) z = ∇1g for z. In the section that follows, we\npresent an algorithm that relaxes the condition of both knowledge of the exact model parameters and exact solution of the linear system."
    }, {
      "heading" : "2. HOAG: Hyperparameter optimization with approximate gradient",
      "text" : "As we have seen in the previous section, computing an exact gradient of f can be computationally demanding. In this section we present an algorithm that uses an approximation, rather than the true gradient, in order to estimate the optimal hyperparameters. This approach yields a tradeoff between speed and accuracy: a loose approximation can be computed faster but might result in slow convergence or the algorithm even diverge. At iteration k, this trade-off is balanced by the tolerance parameter εk. The sequence of tolerance parameters {ε1, ε2, . . .} will turn out to play a major role in the convergence of the algorithm, although the time being, we will treat it as free parameter. We now describe our main contribution, the HOAG algorithm:\nAlgorithm 1 (HOAG). At iteration k = 1, 2, . . . perform the following:\n(i) Solve the inner optimization problem up to tolerance εk. That is, find xk such that∥∥X(λk)− xk∥∥ ≤ εk .\n(ii) Solve the linear system ∇21h(λk, xk)qk = ∇1g(λk, xk) for qk up to tolerance εk. That is, find qk such that∥∥∥∇21h(λk, xk)qk −∇1g(λk, xk)∥∥∥ ≤ εk .\n(iii) Compute approximate gradient pk as\npk = ∇2g(λk, xk)−∇21,2h(λk, xk)T qk ,\n(iv) Update hyperparameters:\nλk = PD(λk − 1\nL pk) .\nThis algorithm consists of four steps. The first two steps of the algorithm compute the quantities used in Eq. (2) to compute the gradient of f . However, since these quantities are not computed to full accuracy, pk, computed in step (iii) is a noisy estimate of the gradient. This approximation is then used as a replacement of the true gradient in a projected gradient-descent (iv) iteration.\nThis procedure requires access to three quantities at iteration k: a εk-optimal solution to the inner optimization problem which can be computed with any solver, the first-order derivatives of g, (∇1g,∇2g), and an εk-optimal solution to a linear system involving ∇21h. In practice, this system is solved using a conjugate-gradient method, which only requires access to the matrix ∇21h through matrix-vector products. For example, in machine learning problems such as the ones introduced in Eq. (1), the quantity ∇21h corresponds to the Hessian of the inner optimization problem. Efficient schemes for multiplication by the Hessian can be derived for least squares, logistic regression (Lin et al., 2008) and other general loss functions (Pearlmutter, 1994)."
    }, {
      "heading" : "2.1. Related work",
      "text" : "There exists a large variety of hyperparameter optimization methods, and a full review of this literature would be outside the scope of this work. Below, we comment on the relationship between HOAG and some of the most closely related methods.\nRegarding gradient-based hyperparameter optimization methods we will distinguish two main approaches, implicit differentiation and iterative differentiation, depending on how the gradient with respect to hyperparameters is computed.\nImplicit differentiation. This approach consists in deriving an implicit equation for the gradient using the optimality conditions of the inner optimization problem (as we did in Eq. (2)). Originally motivated by the problem of setting the regularization parameter in the context of neural networks (Larsen et al., 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al., 2008). This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters.\nIterative differentiation. In this approach, the gradient with respect to hyperparameters is computed by differentiating each step of the inner optimization algorithm and then using the chain rule to aggregate the results. Since the gradient is computed after a finite number of steps of the inner optimization routine, the estimated gradient is naturally an approximation to the true gradient. This method was first proposed by Domke (2012) and later extended to the setting of stochastic gradient descent by Maclaurin et al. (2015). We note also that contrary to the implicit differen-\ntiation approach, this method can be applied to problems with non-smooth cost functions (Deledalle et al., 2014; Ochs et al.).\nHOAG, while belonging to the class of implicit differentiation methods, is related to iterative differentiation methods in that it allows the gradient with respect to hyperparameters to be computed approximately.\nFinally, we note that similar approaches have also been considered in the setting of sequential model-based optimization. Swersky et al. (2014) proposes an approach in which the inner optimization is “freezed” whenever the method decides that the current hyperparameter values are not promising. It does so by introducing a prior on training curves as a function of input hyperparameters. This approach however requires to make strong assumptions on the shape of the training curves which gradient-based methods do not make."
    }, {
      "heading" : "3. Analysis",
      "text" : "In this section we will prove that the summability of the tolerance sequence {εi}∞i=1 is sufficient to guarantee convergence of the iterates in HOAG. The analysis of this algorithm is inspired by the work of d’Aspremont (2008); Schmidt et al. (2011); Friedlander and Schmidt (2012) on inexact-gradient algorithms for convex optimization.\nWe will start this section by enumerating the regularity conditions that we assume for the hyperparameter optimization problem. The following conditions are assumed through the section:\n• (A1) L-smoothness. We assume that the first derivatives of g and the second derivatives of h are Lipschitz continuous functions.\n• (A2) Nonsingular Hessian. We assume that the matrix ∇21h, which corresponds to the Hessian of the inner optimization problem, is invertible at the values (X(λ), λ), λ ∈ D.\n• (A3) Convex compact domain. The domain under which the hyperparameters are optimized,D, is a convex non-empty and compact subset of Rs.\nThese assumptions are verified by many models of interest. For example, for the problem of estimation of regularization parameters of Eq. (1), it allows twice-differentiable loss functions such as logistic regression or least squares (assumption A1) and strongly convex penalties (A2), such as squared `2 regularization. Note that condition (A2) need not be verified on all its domain, only on the points (X(λ), λ), which would allow in principle to consider\nmodels that are defined through a non-convex cost functions. Assumption (A3) requires that the domain of the hyperparameters is a convex compact domain. In practice, hyperparameters are optimized over a s-dimensional interval, i.e., a domain of the form D = [a1, b1]× · · · [as, bs]. Our analysis however only require this domain to be convex and compact, a constraint that subsumes s-dimensional intervals.\nThe rest of the section is devoted to prove (under conditions) the convergence of HOAG. The proof is divided in two parts. First, we will prove that the difference between the true gradient and the approximate gradient is bounded by O(ε) (Theorem 3) and in a second part we will prove that if the sequence {εi}∞i=1 is summable, then this implies the convergence to a stationary point of f (Theorem 4). Because of space limitation, the proofs of this section can be found in the supplementary material. Theorem 1 (The gradient error is bounded). For sufficiently large k, the error in the gradient is bounded by a constant factor of εk. That is,∥∥∇f(λk)− pk∥∥ = O(εk) . This theorem gives a bound on the gradient from the sequence that bounds the inner optimization and the linear system solution. Is will be the key ingredient in order to show convergence to a stationary point, which is the main result of this section. This property sometimes referred to as global convergence (Nocedal and Wright, 2006):\nTheorem 2 (Global convergence). If the tolerance sequence is summable, that is, if {ε}ni=1 is positive and verifies\n∞∑ i=1 εi ≤ ∞ ,\nthen the sequence λk of iterates in the HOAG algorithm has limit λ∗ ∈ D, and this limit verifies the stationary point condition:\n∇f(λ∗)T (α− λ∗) ≥ 0 , ∀α ∈ D .\nIn particular, if λ∗ belongs to the interior of D it is verified that\n∇f(λ∗) = 0 .\nThis results gives sufficient conditions for the convergence of HOAG. The summability of the tolerance sequence suggest several natural candidates for this sequence, such as the quadratic sequence, εk = k−2 or the exponential sequence, εk = ρk, with 0 < ρ < 1. We will empirically evaluate different tolerance sequences on different problems and different datasets in the next section."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section we compare the empirical performance of HOAG. We start by discussing some implementation details such as the choice of step size. Then, we compare the convergence of different tolerance decrease strategies that were suggested by the theoretical analysis. In a third part, we compare the performance of HOAG against other hyperparameter optimization methods.\nAdaptive step size. Our algorithm relies on the knowledge of the Lipschitz constant L for the cost function f . However, in practice this is not known in advance. Furthermore, since the cost function is costly to evaluate, it is not feasible to perform backtracking line search. To overcome this we use a procedure in which the step size is corrected depending on the gain estimated from the previous step. In the experiments we use this technique although we do not have a formal analysis of the algorithm for this choice of step size.\nLet ∆k denote the distance between the current iterate and the past iterate, ∆k = ‖λk − λk−1‖. The L-smooth property of the function g, together with Lemma 3, implies that there exists a constant M > 0 such that the following inequality is verified:\ng(λk, xk) ≤ g(λk−1, xk−1) + Cεk+ εk−1(C +M)∆k − L∆2k ,\n(3)\nwhere C is the Lipschitz constant of g. This inequality can be derived from the properties of L-smooth functions, and the details can be found in the appendix. The procedure consists in decreasing the step (multiplication by α < 1) whenever the equation is not satisfied and to increase it (multiplication by β > 1) whenever the equation is satisfied to ensure that we are using a step size as large as possible. The constants that we used in the experiments are M = 1, α = 0.8, β = 1.2.\nInitialization. The previous sections tells us how to adjust the step size but relies on an initial value of this parameter. We have found that a reasonable initialization is to initalize it to L = ‖p1‖ so that the first update in HOAG is of magnitude at most 1 (it can be smaller due to the projection), where p1 is the approximate gradient on the first step. The initialization of the tolerance decrease sequence is set to ε1 = 0.1. We also limit the maximum precision to avoid numerical instabilities to 10−12, which is also the precision for “exact” methods, i.e., those that do not use a tolerance sequence. The initialization of regularization parameters is set to 0 and the width of an RBF kernel is initialized to − log(n feat), where n feat is the dimensionality of the dataset.\nAlthough HOAG can be applied more generally, in our ex-\nperiments we focus on two problems: `2-regularized logistic regression and kernel Ridge regression. We follow the setting described in Eq. (1), in which an initial dataset is partitioned into two sets, a train set Strain = {(bi, ai)}ni=1 and a test set Stest = {(b′i, a′i)}mi=1, where ai denotes the input features and bi the target variables.\nThe first problem consists in estimating the regularization parameter in the widely-used `2-regularized logistic regression model. In this case, the loss function of the inner optimization problem is the regularized logistic loss function. In the setting of classification, the validation loss or outer cost function is commonly the zero-one loss. However, this loss is non-smooth and so does not verify assumption (A1). To overcome this and following (Foo et al., 2008), we use the logistic loss as the validation loss. This yield a problem of the form:\narg min λ m∑ i=1 ψ(b′ia ′T i X(λ))\ns.t. X(λ) ∈ arg min x∈Rp n∑ i=1 ψ(bia T i x) + e λ‖x‖2 , (4)\nwhere ψ is the logistic loss, i.e., ψ(t) = log(1 + e−t). The second problem that we consider is that of kernel Ridge regression with an RBF kernel. In this setting, the problem contains two hyperparameters: the first hyperparameter (λ1) controls the width of the RBK kernel and the second hyperparameter (λ2) controls the amount of regularization. The inner optimization depends on the kernel through the kernel matrix, formed by computing the kernel of all pairwise input samples. We denote such matrix as K(γ)train, where the (i, j) entry is given by k(ai, aj , γ), where k is the RBF kernel function: k(ai, aj , γ) = exp(−γ‖ai − aj‖). Similarly, the outer optimization also depends on the kernel through the matrix K(γ)test, where its entries are the kernel product between features from the train set and features from the test set, that is, k(ai, a′j , γ). Denoting the full hyperparameter vector as λ = [λ1, λ2], the kernel matrix on the train set as, the full hyperparameter optimization problem takes the form\narg min λ ∥∥∥b−Ktest(eλ1)X(λ)∥∥∥2 s.t. ( Ktrain(e λ1) + eλ2I ) X(λ) = b ,\n(5)\nwhere for simplicity the inner optimization is already set as an implicit equation. Note that in this setting, and unlike in the logistic regression problem, the outer optimization function depends on the hyperparameters not only through the model parametersX(λ) but also through the kernel matrix.\nThe solver used for the inner optimization problem of the logistic regression problem is L-BFGS (Liu and Nocedal,\n1989), while for Ridge regression we used a linear conjugate descent method. In all cases, the domain for hyperparameters is the s-dimensional interval [−12, 12]s. Datasets. For the experiments, we use three different datasets. The first dataset that we use is the 20news dataset 1 which contains 18000 newsgroups posts on 20 topics, with the task of predicting the appropriate group of a post. The features we used are the tf-idf vectors obtained from the original dataset, and the groups were randomly split into two categories to obtain a binary classification problem.\nThe second dataset that we use is also a text categorization task denoted real-sim. This dataset contains 73218 UseNet articles from four discussion groups, for simulated auto racing, simulated aviation, real autos, real aviation. The binary classification task is to predict whether it belongs to the real-{autos, aviation} group or the simulated-{aviation, auto racing} group. This dataset was obtained from the libsvmtools project2.\nThe third dataset, which we denote the Parkinson dataset, is used by the kernel Ridge regression problem. This dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson’s disease(Tsanas et al., 2010). This dataset contains 5875 samples with 26 features and is publicly available from the UCI machine learning repository3.\nIn all cases, the dataset is randomly split in three equally sized parts: a train set, test set and a third validation set that we will use to measure the generalization performance of the different approaches."
    }, {
      "heading" : "4.1. Tolerance decrease sequence",
      "text" : "We report in Figure 2 the convergence of different tolerance decrease strategies. From Theorem 4, the sole condition on these sequences is that they are summable. Three notable examples of summable sequences are the quadratic, cubic and exponential sequences. Hence, we choose one representative of each of these strategies. More precisely, the decrease sequences that we choose are a quadratic decrease sequence of the form εk = 0.1 × k−2, a cubic one of the form εk = 0.1 × k−3 and an exponential of the form εk = 0.1 × (0.5k). The value taken as true minima of the hyperparameter optimization problem is computed by taken the minimum reached by 10 randomly initialized instances of HOAG with exponential decrease tolerance.\n1http://qwone.com/˜jason/20Newsgroups/ 2http://www.csie.ntu.edu.tw/˜cjlin/\nlibsvmtools/datasets/binary.html 3http://archive.ics.uci.edu/ml/\nThe plot shows the relative accuracy of the different variants as a function of time. It can be seen that non-exact methods feature a cheaper iteration cost. This makes them converge faster, especially on initial iterations. Note that because of how the Lipschitz constant L is computed, which can be seen as an a posteriori correction, the algorithm is not guaranteed to be a descent method. This can be appreciated in Figure 2, where the quadratic decrease sequence (and to some extent the cubit too) exhibits oscillations in the two first plots."
    }, {
      "heading" : "4.2. Comparison with other hyperparameter optimization methods",
      "text" : "We now compare against other hyperparameter optimization methods. The methods against which we compare are:\n• HOAG. The method we present in this paper, with an exponentially decreasing tolerance sequence. A Python implementation for this method will be made available upon acceptance of the manuscript.\n• Grid Search. This method consists simply in splitting the domain of the hyperparameter into an equallyspaced grid. We split the interval [−12, 12] into a grid of 10 values.\n• Random. This is the random search method (Bergstra and Bengio, 2012) samples the hyperparameters from a predefined distribution. We choose to samples from a uniform distribution in the interval [−12, 12].\n• SMBO. Sequential model-based optimization using Gaussian Process. We used the implementation found in the Python package BayesianOptimization4. As ini-\n4http://github.com/fmfn/ BayesianOptimization/\ntialization for this method, we choose 4 values equally spaced between−12 and 12. The acquisition function used is the expected improvement.\n• Iterdiff. This is the iterative differentiation approach from (Domke, 2012), using the same inneroptimization algorithm as HOAG. While the original implementation used to have a backtracking line search procedure to estimate the step size, we found that this performed worst than any of the alternatives. For this reason, we use the adaptive step size strategy presented in Section 4 (assuming a zero tolerance parameter ε).\nFor all methods, the number of iterations used in the inner optimization algorithm (L-BFGS or GD) is set to 100, which is the same used by the other methods and the default in the scikit-learn5 package.\nWe report in Figure 3 the results of comparing the accuracy of these methods as a function of time. Note that it is expected that the different methods have different starting points. This is because Grid Search and SMBO naturally start from a pre-defined grid that starts from the extremes of the interval, while random search simply chooses a random point from the domain. For HOAG and Iterdiff, we take the initialization λ1 = 0.\nIn the upper row of Figure 3 we can see the suboptimality of the different procedures as a function of time. We observe that HOAG and Iterdiff have similar behavior, although HOAG features a smaller cost per iteration. This can be explained because once HOAG has made a step it can use the previous solution of the inner optimization problem as a warm-start to compute the next gradient. This is not the case in Iterdiff since the computation of the gradient\n5http://scikit-learn.org\nrelies crucially on having sufficient iterations of the inner optimization algorithm.\nWe note that in the Parkinson dataset, solution is inside a region that is almost flat (the different cost functions can be seen in Figure 1 of the supplementary material). This can explain the difficulty of the methods to go beyond the 10−2 suboptimality level. In this case, SMBO, who starts by computing the cost function at the extremes of the domain converges instantly to this region, which explains its fast convergence, although it is unable to improve the initially reached suboptimality.\nSuboptimality plots are a standard way to compare the performance of different optimization methods. However, for the context of machine learning it can be argued that estimating hyperparameters up to a high precision is unimportant and that methods should be compared in terms of generalization performance. In the lower row of Figure 3, we display the test loss (g) on a validation set, that is, using a third set of samples {(b̃i, ãi)}ri=1 which is different from both the train and test set. This figure reveals two main effects. First, unsurprisingly, optimization beyond 10−2 of relative suboptimality is not reflected in this metric. Second, the fast (but noisy) early iterations of HOAG achieve the fastest convergence in two out of three datasets."
    }, {
      "heading" : "5. Discussion and future work",
      "text" : "In previous sections we have presented and discussed several aspects of the HOAG algorithm. Finally, we outline some future directions that we think are worthwhile exploring.\nGiven the success of recent stochastic optimization techniques (Schmidt et al., 2013; Johnson and Zhang, 2013) it seems natural to study a stochastic variant of this algorithm, that is, one in which the updates in the inner and outer optimization schemes have a cost that is independent of the number of samples. However, the dependency on the Hessian of the inner optimization (∇21h) in the implicit equation (2) makes this non-trivial.\nLittle is known of the structure of solutions for the hyperparameter optimization problem (HO). In fact, assumption (A3) is introduced almost exclusively in order to guarantee existence of solutions. At the same time recent progress on the setting of image restoration, which can be considered a subproblem of (HO), has given sufficient conditions on the input data for such solution to exist in an unbounded domain (De los Reyes et al., 2015). The characterization of solutions for the HO problem can potentially simplify the assumptions made in this paper.\nFor its practical importance we have focused in this paper on models with one or two hyperparameters. However, as\nnoted by several authors, the use of gradient-based hyperparameter optimization opens the door for the optimization of high-dimensional hyperparameter spaces. With the ever-growing size and complexity of modern datasets, this offers an exciting perspective for future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "I am in debt with Gabriel Peyré for numerous discussions, suggestions and pointers. I would also like to thank the developers of scikit-learn for discussions and for their continuous effort in producing high-quality tools, and to Justin Domke for posting the code of his Iterative differentiation method.\nThe author acknowledges financial support from the “Chaire Economie des Nouvelles Données”, under the auspices of Institut Louis Bachelier, Havas-Media and Université Paris-Dauphine."
    }, {
      "heading" : "6. Proofs",
      "text" : "Lemma 3 (The gradient error is bounded). For sufficiently large k, the error in the gradient is bounded by a constant factor of εk. That is,∥∥∇f(λk)− pk∥∥ = O(εk) . Proof. Before starting the proof, we introduce the following notation for convenience. We denote by Ak and Âk the Hessian of the inner optimization function evaluated at the model parameters and at the k-th iteration approximation, respectively. That is,\nAk = ∇21h(λk, X(λk)) and Âk = ∇21h(λk, xk) .\nIn a similar way we define bk, b̂k and Dk, D̂k:\nbk = ∇1g(λk, X(λk)) and b̂k = ∇1g(λk, xk) Dk = ∇21,2h(λk, X(λk)) and D̂k = ∇21,2h(λk, xk)\nNote that Âk, b̂k and D̂k are the quantities involved in the HOAG algorithm. On the other hand, the quantities Ak, bk and Dk are an “ideal” version of the former, computed when the tolerance is zero. It is not surprising though that the difference between Ak, bk, Dk and its hat counterpart will play a fundamental role in the proof.\nThe proof is structured in two parts. In part (i) we will prove that several sequences of interest are bounded, while in part (ii) we will use this to prove the main result.\nPart (i). We first note that that both ‖λ‖ and ∥∥X(λ)∥∥ are bounded and denote such bounds by ξ and η, respectively. ‖λ‖ is bounded as a direct consequence of assumption (A3). On the other hand, X(λ) is continuously differentiable as a result of the implicit function theorem. Since its domain is a bounded set, ‖X(λ)‖ is also bounded. We prove that the following sequences are bounded:\n• {‖xk‖}∞k=1. By the termination condition of the inner optimization problem we have that∥∥X(λk)− xk∥∥ ≤ εk. By the reverse triangular inequality we further have\nεk ≥ ∥∥X(λk)− xk∥∥ ≥ ∣∣∣∥∥X(λk)∥∥−‖xk‖∣∣∣\n=⇒ ‖xk‖ ≤ ∥∥X(λk)∥∥+ εk ≤ ζ + εk\nhence ‖xk‖ is a bounded sequence since {ε}∞k=1 defines a summable sequence (hence bounded).\n• {‖Ak‖}∞k=1, {‖Dk‖}∞k=1 and {‖bk‖}∞k=1. Assumption (A1) implies that there exists constants LE , Lg such that\n‖Ak −A0‖ ≤ LE ∥∥[λk, X(λk)]− [λ0, X(λ0)]∥∥\n≤ LE √ ξ2 + η2\n‖Dk −D0‖ ≤ LE ∥∥[λk, X(λk)]− [λ0, X(λ0)]∥∥\n≤ LE √ ξ2 + η2\n‖bk − b0‖ ≤ Lg ∥∥[λk, X(λk)]− [λ0, X(λ0)]∥∥\n≤ Lg √ ξ2 + η2\n,\nand so ‖Ak‖ ,‖Dk‖ and ‖bk‖ are all bounded sequences.\n• {‖A−1k ‖}∞k=1. By assumption (A2), A−1k exists and ‖A−1k ‖ < ∞ for all k. Since D is a compact set, the limit of the sequence verifies limk→∞ ‖A−1k ‖ < ∞, and hence is bounded.\nPart (ii). By the Lipschitz assumption on g and E, there exists finite numbers Lg and LE such that we have the following sequence of inequalities:\n‖Ak − Âk‖ ≤ LE‖X(λk)− xk‖ ≤ LEεk ‖bk − b̂k‖ ≤ Lg‖X(λk)− xk‖ ≤ Lgεk\nLet zk be the solution to the linear system of equations Akzk = bk and ẑ be the solution to Âkẑk = b̂k. Since by assumption Ak is invertible, zk = A−1k bk and by the Cauchy-Schwarz inequality, we have that\n‖zk‖ ≤ ‖A−1k ‖‖bk‖ ,\nand hence the sequence‖zk‖ is also bounded. By the summability condition of εk and the boundedness of ‖A−1k ‖ proved in part (i) of this proof, for all sufficiently large k it is verified that εk‖A−1k ‖Lg ≤ ρ < 1 and by classical results related to the sensitivity of linear systems (see e.g. (Higham, 2002, §7.1)) we have the following sequence of inequalities:\n‖zk − ẑk‖ ≤ εk\n1− εk‖A−1k ‖Lg\n( ‖A−1k ‖Lf +‖zk‖‖A−1k ‖Lg ) ≤ εk 1− ρ ( ‖A−1k ‖Lf +‖zk‖‖A−1k ‖Lg\n) = O(εk) (bound on all terms involved)\n(6) where the first inequality is derived from (Higham, 2002, §7.1) and the second one comes from the definition of ρ. This last equation provides a bound on the difference between solving the linear system at X(λ) and solving the\nlinear system at xk, assuming that the linear system is solved to full precision. However, in practice we do not attempt to solve the linear system to full precision but rather compute an approximation qk such that ‖Âkqk − b̂k‖ ≤ εk (step (ii) of HOAG) and we are interest in bounding the distance between qk and its noise-less version zk. We have the following sequence of inequalities:\n‖qk − zk‖ = ‖qk − ẑk + ẑk − zk‖ ≤ ‖qk − ẑk‖+ ‖ẑk − zk‖ (triangular inequality) ≤ ‖Â−1k Âk(qk − ẑk)‖+ ‖ẑk − zk‖ = ‖Â−1k (Âkqk − b̂k)‖+ ‖ẑk − zk‖ (definition of ẑk) ≤ ‖Â−1k ‖‖Âkqk − b̂k‖+ ‖ẑk − zk‖ (Cauchy-Schwarz) ≤ ‖Â−1k ‖εk + ‖ẑk − zk‖ (definition of qk) = O(εk) (Eq. (6)) .\nFinally, using this we can write that the difference between pk and the true gradient. Let ck, ĉk be defined as\nck = ∇2g(λk, X(λk)) and ĉk = ∇2g(λk, xk)\nThen it is verified that∥∥∇f(λk)− pk∥∥ = ‖ck −DT zk − ĉk − D̂Tk qk‖ ≤ ‖ck − ĉk‖+ ‖DTk zk − D̂Tk qk‖\n(triangular inequality)\n≤ ‖ck − ĉk‖+ ‖DTk zk − D̂Tk zk + D̂Tk zk − D̂Tk qk‖ (Add and remove DTk zk)\n≤ ‖ck − ĉk‖+ ‖DTk zk − D̂Tk zk‖+ ‖D̂Tk zk − D̂Tk qk‖ (triangular inequality)\n≤ ‖ck − ĉk‖+ ‖Dk − D̂k‖‖zk‖+ ‖D̂k‖‖zk − qk‖ (Cauchy-Schwartz)\n≤ Lgεk + LEεk‖zk‖+ ‖D̂k‖‖zk − qk‖ (Assumption (A1))\n≤ Lgεk + LEεk‖zk‖+ ‖D̂k‖O(εk) (previous inequality)\n= O(εk) (bound on ‖zk‖ and ‖D̂k‖)\nwhich completes the proof.\nTheorem 4 (Global convergence). The sequence λk of iterates in the HOAG algorithm has limit λ∗ ∈ D, and this limit verifies the stationary point condition:\n∇f(λ∗)T (α− λ∗) ≥ 0 , ∀α ∈ D .\nIn particular, if λ∗ belongs to the interior of D it is verified that\n∇f(λ∗) = 0 .\nProof. By assumption, f is L-smooth. This implies the following inequality for any pair of values α, β ∈ D:\nf(β) ≤ f(α) +∇f(α)T (β − α) + L 2 ‖β − α‖2 . (7)\nThis is a classical result on quadratic upper bounds for L-smooth functions (see e.g. (Nesterov, 2004, Lemma 1.2.3)). We will also make use of the following inequality concerning the projection PD, which stems from the fact that projections onto convex sets are firmly nonexpansive operators (see e.g. (Parikh and Boyd, 2013, §2.2)). Let η, ν ∈ Rs, then the following is verified:\n‖PD(η)− PD(ν)‖2 ≤ (η − ν)T (PD(η)− PD(ν))\nIn particular, for η = λk, ν = λk − 1Lpk, this reduces to\n‖λk − λk+1‖2 ≤ 1\nL pTk (λk − λk+1) (8)\nSetting now α = λk, β = λk+1 = PD(λk − 1Lpk) in Eq. (7), we have the following sequence of inequalities\nf(λk+1) ≤ f(λk)−∇f(λk)T (λk − λk+1)\n+ L\n2 ‖λk+1 − λk‖2\n= f(λk)− (∇f(λk)− pk + pk)T (∆λk − λk+1)\n+ L\n2 ‖λk+1 − λk‖2\n= f(λk)− (∇f(λk)− pk)T (λk − λk+1)\n− pTk (λk − λk+1) + L\n2 ‖λk+1 − λk‖2\n≤ f(λk)− (∇f(λk)− pk)T (λk − λk+1)\n− L 2 ‖λk+1 − λk‖2 (by Eq. (8)) ≤ f(λk) + ‖∇f(λk)− pk‖‖λk − λk+1‖\n− L 2 ‖λk+1 − λk‖2 (Cauchy-Schwartz)\n(9) By Lemma 3, ‖∇f(λk) − pk‖ = O(εk). Since D is bounded, we have that there exists M > 0 and such that for sufficiently large k\n‖∇f(λk)− pk‖‖λk − λk+1‖ < Mεk . (10)\nwhich applied to the previous inequality results in\nf(λk+1) ≤ f(λk) +M(εk)− L\n2 ‖λk+1 − λk‖2 ,\nor equivalently\n‖λk+1 − λk‖2 ≤ 2\nL\n( f(λk)− f(λk+1) +M(εk) ) .\nLet C be an lower bound on the function f . This bound exist and is finite because f has continuous derivatives and\nis defined on a compact set. Summing the last expression from k = m to k =∞ we obtain ∞∑ k=m ‖λk+1 − λk‖2 ≤ 2 L f(λm)− C +M ∞∑ k=m (εk)  . Since {εk}∞k=1 is a summable sequence we conclude that the right-hand side of this expression is finite. Hence, for the sum on the left-hand side to be finite we must have ‖λk+1 − λk‖2 → 0 as k → ∞. This implies that the limit of the sequence {λk}∞k=1 exists. We denote this limit by λ∗. Furthermore, we have the following sequence of equalities\n0 = lim k→∞\n{λk+1 − λk}\n= lim k→∞\n{PD(λk − 1\nL pk)− λk} (definition of λk+1)\n= lim k→∞\n{PD(λ∗ − 1\nL pk)− λ∗} (limit of λk)\n= PD(λ ∗ − 1\nL ∇f(λ∗))− λ∗ (Theorem 3) ,\nfrom where λ∗ verifies the fixed point equation\nλ∗ = PD(λ ∗ − 1\nL ∇f(λ∗)) . (11)\nIn the introduction we have formulated the projection as PD(α) = arg minλ∈D ‖α− λ‖2. However, it also admits the equivalent definition in terms of an unconstrained optimization problem:\nPD(α) = arg min λ∈Rs\nID(λ) + 1\n2 ‖α− λ‖2 ,\nwhere ID(λ) is the indicator function, which is 0 if λ ∈ D and +∞ otherwise. In light of this, the first order optimality conditions on Eq. (11) imply that\n0 ∈ ∂λ ( ID(λ) + 1\n2 ‖α− λ‖\n) ,\n=⇒ 1 L ∇f(λ∗) ∈ ∂ID(λ∗)\n(λ = λ∗, α = λ∗ − 1 L ∇f(λ∗))\nwhere ∂ denotes the subgradient. Since D is a convex set, ID is a convex function, hence its subgradient is a a monotone mapping (see e.g. (Rockafellar and Wets, §12.C)). By definition of monotonicity, it is verified that\n(v1−v0)T (x1−x0) ≥ 0 for all v0 ∈ ∂ID(x0), v1 ∈ ∂ID(x1)\nLet x1 = α be an arbitrary element ofD. Then 0 ∈ ∂ID(α) since the subgradient of the indicator function is either zero if α is in the interior, or it contains all positive numbers (including zero) if α is in the border. In any case, letting\nx1 = λ ∗ we have that 1L∇f(λ∗) ∈ ∂ID(λ∗) and so it must be verified\n1 L ∇f(λ∗)T (α−λ∗) ≥ 0 =⇒ ∇f(λ∗)T (α−λ∗) ≥ 0 .\nIf λ∗ belongs to the interior of D, then there exists a ball of radius λ around λ∗ contained within D. In particular, for every vector within this ball we have∇f(λ∗)T (α− λ∗) ≥ 0. Since this ball contains a basis of Rs, it must be verified that ∇f(λ∗) = 0, which concludes the proof."
    }, {
      "heading" : "7. Experiments",
      "text" : "Adaptive step size\nWe will now derive the inequality used by the adaptive step size procedure presented in the Experiments section. The derivation of this procedure uses the L-smooth assumption and Theorem 3. The L-smooth assumption implies that for all α, β in the domain, the following inequality is verified\nf(β) ≤ f(α) +∇f(α)T (β − α) + L 2 ‖β − α‖2 .\nSetting α = λk−1, β = λk in the above inequality, and using the bound ‖∇f(λk)−pk‖ < Mεk given by Theorem 3, we have that\nf(λk) ≤f(λk−1) + εk−1M∆k − L∆2k . (12)\nfor some constant M and where ∆k is defined as ‖λk − λk−1‖ (A more rigorous derivation of this inequality can be found in the supplementary material, Eq. (4)). Now, we do not have access to f(λk) as this depends on the exact model parameters. However, by by the definition of xk we have that\n∥∥xk −X(λ)∥∥ ≤ εk. Furthermore, g is Lipschitz continuous since this is a weaker condition than assumption (A1), hence there exists a constant C such that ‖g(λk, xk) − g(λk, X(λk))‖ ≤ Cεk. Since by definition f(λ) = g(λ,X(λ)), we can derive the inequalities g(λk, xk) − Cεk ≤ f(λk) and f(λk−1) ≤ g(λk−1, xk) + Cεk−1. Replacing this into Eq. (12) yields the following inequality:\ng(λk, xk) ≤ g(λk−1, xk−1) + Cεk+ εk−1(C +M)∆k − L∆2k .\n(13)\n10-10 10-8 10-6 10-4 10-2 100 102\n`2 regularization\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\n18000\nlo ss\no n h\no ld\n-o u t\nse t\nreal-sim dataset\n10-8 10-6 10-4 10-2 100 102\n`2 regularization\n500\n1000\n1500\n2000\n2500\n3000\nlo ss\no n h\no ld\n-o u t\nse t\n20news dataset\nRegularization10−15 10 −10 10−5 10\n0 105 10 10 Kernel w idth\n10−15\n10−10\n10−5\n100\n105\n1010\nh o ld -o u t lo ss 4 6 8 10 12 14 16 18 20 22\nParkinson dataset\nFigure 4: Cost functions of the different hyperparameter optimization methods. These are the cost functions (denoted f through the paper) as a function of the hyperparameters (λ) fo the three problems considered. In the two first images, the hyperparameter is the `2 regularization parameter and the objective function has a unique minima. The third features the cost function as a function of the kernel width and `2 regularization."
    } ],
    "references" : [ {
      "title" : "Gradient-based optimization of hyperparameters",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Bengio.,? \\Q2000\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2000
    }, {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "James Bergstra", "Yoshua Bengio" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bergstra and Bengio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bergstra and Bengio.",
      "year" : 2012
    }, {
      "title" : "Algorithms for hyper-parameter optimization",
      "author" : [ "James S. Bergstra", "Rémi Bardenet", "Yoshua Bengio", "Balázs Kégl" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2011
    }, {
      "title" : "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
      "author" : [ "Eric Brochu", "Vlad M Cora", "Nando De Freitas" ],
      "venue" : "arXiv preprint arXiv:1012.2599,",
      "citeRegEx" : "Brochu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Brochu et al\\.",
      "year" : 2010
    }, {
      "title" : "Bilevel approaches for learning of variational imaging models",
      "author" : [ "Luca Calatroni", "Cao Chung", "Juan Carlos De Los Reyes", "Carola-Bibiane Schönlieb", "Tuomo Valkonen" ],
      "venue" : "arXiv preprint arXiv:1505.02120,",
      "citeRegEx" : "Calatroni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Calatroni et al\\.",
      "year" : 2015
    }, {
      "title" : "Choosing multiple parameters for support vector machines",
      "author" : [ "Olivier Chapelle", "Vladimir Vapnik", "Olivier Bousquet", "Sayan Mukherjee" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2002
    }, {
      "title" : "Smooth optimization with approximate gradient",
      "author" : [ "Alexandre d’Aspremont" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "d.Aspremont.,? \\Q2008\\E",
      "shortCiteRegEx" : "d.Aspremont.",
      "year" : 2008
    }, {
      "title" : "Stein unbiased gradient estimator of the risk (SUGAR) for multiple parameter selection",
      "author" : [ "Charles-Alban Deledalle", "Samuel Vaiter", "Jalal Fadili", "Gabriel Peyré" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "Deledalle et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Deledalle et al\\.",
      "year" : 2014
    }, {
      "title" : "Generic methods for optimization-based modeling",
      "author" : [ "Justin Domke" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Domke.,? \\Q2012\\E",
      "shortCiteRegEx" : "Domke.",
      "year" : 2012
    }, {
      "title" : "Efficient multiple hyperparameter learning for log-linear models",
      "author" : [ "Chuan-Sheng Foo", "Chuong B. Do", "Andrew Y. Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Foo et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Foo et al\\.",
      "year" : 2008
    }, {
      "title" : "Hybrid deterministic-stochastic methods for data fitting",
      "author" : [ "Michael Friedlander", "Mark Schmidt" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Friedlander and Schmidt.,? \\Q2012\\E",
      "shortCiteRegEx" : "Friedlander and Schmidt.",
      "year" : 2012
    }, {
      "title" : "Accuracy and stability of numerical algorithms",
      "author" : [ "Nicholas J Higham" ],
      "venue" : null,
      "citeRegEx" : "Higham.,? \\Q2002\\E",
      "shortCiteRegEx" : "Higham.",
      "year" : 2002
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Johnson and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2013
    }, {
      "title" : "A bilevel optimization approach for parameter learning in variational models",
      "author" : [ "Karl Kunisch", "Thomas Pock" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "Kunisch and Pock.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kunisch and Pock.",
      "year" : 2013
    }, {
      "title" : "Sequential model-based ensemble optimization",
      "author" : [ "Alexandre Lacoste", "Hugo Larochelle", "François Laviolette", "Mario Marchand" ],
      "venue" : "arXiv preprint arXiv:1402.0796,",
      "citeRegEx" : "Lacoste et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lacoste et al\\.",
      "year" : 2014
    }, {
      "title" : "Design and regularization of neural networks: the optimal use of a validation set",
      "author" : [ "Jan Larsen", "Lars Kai Hansen", "Claus Svarer", "M Ohlsson" ],
      "venue" : "In Neural Networks for Signal Processing",
      "citeRegEx" : "Larsen et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 1996
    }, {
      "title" : "Adaptive regularization in neural network modeling",
      "author" : [ "Jan Larsen", "Claus Svarer", "Lars Nonboe Andersen", "Lars Kai Hansen" ],
      "venue" : "In Neural Networks: Tricks of the Trade. Springer,",
      "citeRegEx" : "Larsen et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 1998
    }, {
      "title" : "Trust region newton method for logistic regression",
      "author" : [ "Chih-Jen Lin", "Ruby C Weng", "S Sathiya Keerthi" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Lin et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2008
    }, {
      "title" : "On the limited memory bfgs method for large scale optimization",
      "author" : [ "Dong C Liu", "Jorge Nocedal" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Liu and Nocedal.,? \\Q1989\\E",
      "shortCiteRegEx" : "Liu and Nocedal.",
      "year" : 1989
    }, {
      "title" : "Parametric or nonparametric? a parametricness index for model selection",
      "author" : [ "Wei Liu", "Yuhong Yang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Liu and Yang.,? \\Q2011\\E",
      "shortCiteRegEx" : "Liu and Yang.",
      "year" : 2011
    }, {
      "title" : "Gradient-based hyperparameter optimization through reversible learning",
      "author" : [ "Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Maclaurin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maclaurin et al\\.",
      "year" : 2015
    }, {
      "title" : "Introductory lectures on convex optimization",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Nesterov.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2004
    }, {
      "title" : "Fast exact multiplication by the hessian",
      "author" : [ "Barak A Pearlmutter" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Pearlmutter.,? \\Q1994\\E",
      "shortCiteRegEx" : "Pearlmutter.",
      "year" : 1994
    }, {
      "title" : "Convergence rates of inexact proximal-gradient methods for convex optimization",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis R Bach" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2011
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2013
    }, {
      "title" : "Cross-validation optimization for large scale structured classification kernel methods",
      "author" : [ "Matthias W Seeger" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Seeger.,? \\Q2008\\E",
      "shortCiteRegEx" : "Seeger.",
      "year" : 2008
    }, {
      "title" : "Estimation of the mean of a multivariate normal distribution",
      "author" : [ "Charles M Stein" ],
      "venue" : "The annals of Statistics,",
      "citeRegEx" : "Stein.,? \\Q1981\\E",
      "shortCiteRegEx" : "Stein.",
      "year" : 1981
    }, {
      "title" : "Freeze-thaw bayesian optimization",
      "author" : [ "Kevin Swersky", "Jasper Snoek", "Ryan Prescott Adams" ],
      "venue" : "arXiv preprint arXiv:1406.3896,",
      "citeRegEx" : "Swersky et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Swersky et al\\.",
      "year" : 2014
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "Accurate telemonitoring of parkinson’s disease progression by noninvasive speech tests",
      "author" : [ "Athanasios Tsanas", "Max A Little", "Patrick E McSharry", "Lorraine O Ramig" ],
      "venue" : "Biomedical Engineering, IEEE Transactions on,",
      "citeRegEx" : "Tsanas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tsanas et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Among its most well-known instances are the LASSO (Tibshirani, 1996), in which `1 regularization is added to a squared loss to encourage sparsity in the solutions, or `2-regularized logistic regression, in which squared `2 regularization (known as weight decay in the context of neural networks) is added to obtain solutions with small euclidean norm.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : "This criterion can be a goodness of fit on unseen data, such as a cross-validation loss, or some criteria of model quality on the train set such as SURE (Stein, 1981), AIC/BIC (Liu and Yang, 2011) or Mallows Cp (Mallows, 1973), to name a few.",
      "startOffset" : 153,
      "endOffset" : 166
    }, {
      "referenceID" : 19,
      "context" : "This criterion can be a goodness of fit on unseen data, such as a cross-validation loss, or some criteria of model quality on the train set such as SURE (Stein, 1981), AIC/BIC (Liu and Yang, 2011) or Mallows Cp (Mallows, 1973), to name a few.",
      "startOffset" : 176,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Random search (Bergstra et al., 2011) has been proven to yield a faster exploration of the hyperparameter space than grid search, specially in spaces with multiple hyperparameters.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "(Brochu et al., 2010) for an review on current methodologies).",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "This probabilistic model typically relies on a Gaussian process regressor but other approaches exist using trees (Bergstra et al., 2011) or ensemble methods (Lacoste et al.",
      "startOffset" : 113,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : ", 2011) or ensemble methods (Lacoste et al., 2014).",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "This approach, which we will refer to as implicit differentiation (Larsen et al., 1996; Bengio, 2000; Foo et al., 2008), relies on the observation that under some regularity conditions it is possible to replace the inner optimization problem by an implicit equation.",
      "startOffset" : 66,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "This approach, which we will refer to as implicit differentiation (Larsen et al., 1996; Bengio, 2000; Foo et al., 2008), relies on the observation that under some regularity conditions it is possible to replace the inner optimization problem by an implicit equation.",
      "startOffset" : 66,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "This approach, which we will refer to as implicit differentiation (Larsen et al., 1996; Bengio, 2000; Foo et al., 2008), relies on the observation that under some regularity conditions it is possible to replace the inner optimization problem by an implicit equation.",
      "startOffset" : 66,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "Efficient schemes for multiplication by the Hessian can be derived for least squares, logistic regression (Lin et al., 2008) and other general loss functions (Pearlmutter, 1994).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : ", 2008) and other general loss functions (Pearlmutter, 1994).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "Originally motivated by the problem of setting the regularization parameter in the context of neural networks (Larsen et al., 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al.",
      "startOffset" : 110,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "Originally motivated by the problem of setting the regularization parameter in the context of neural networks (Larsen et al., 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al.",
      "startOffset" : 110,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : ", 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al.",
      "startOffset" : 97,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : ", 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al.",
      "startOffset" : 97,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : ", 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al., 2008).",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters.",
      "startOffset" : 88,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters.",
      "startOffset" : 88,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : ", 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al., 2008). This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters. Iterative differentiation. In this approach, the gradient with respect to hyperparameters is computed by differentiating each step of the inner optimization algorithm and then using the chain rule to aggregate the results. Since the gradient is computed after a finite number of steps of the inner optimization routine, the estimated gradient is naturally an approximation to the true gradient. This method was first proposed by Domke (2012) and later extended to the setting of stochastic gradient descent by Maclaurin et al.",
      "startOffset" : 14,
      "endOffset" : 946
    }, {
      "referenceID" : 0,
      "context" : ", 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al., 2008). This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters. Iterative differentiation. In this approach, the gradient with respect to hyperparameters is computed by differentiating each step of the inner optimization algorithm and then using the chain rule to aggregate the results. Since the gradient is computed after a finite number of steps of the inner optimization routine, the estimated gradient is naturally an approximation to the true gradient. This method was first proposed by Domke (2012) and later extended to the setting of stochastic gradient descent by Maclaurin et al. (2015). We note also that contrary to the implicit differentiation approach, this method can be applied to problems with non-smooth cost functions (Deledalle et al.",
      "startOffset" : 14,
      "endOffset" : 1038
    }, {
      "referenceID" : 27,
      "context" : "Swersky et al. (2014) proposes an approach in which the inner optimization is “freezed” whenever the method decides that the current hyperparameter values are not promising.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "The analysis of this algorithm is inspired by the work of d’Aspremont (2008); Schmidt et al.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "The analysis of this algorithm is inspired by the work of d’Aspremont (2008); Schmidt et al. (2011); Friedlander and Schmidt (2012) on inexact-gradient algorithms for convex optimization.",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "The analysis of this algorithm is inspired by the work of d’Aspremont (2008); Schmidt et al. (2011); Friedlander and Schmidt (2012) on inexact-gradient algorithms for convex optimization.",
      "startOffset" : 58,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "To overcome this and following (Foo et al., 2008), we use the logistic loss as the validation loss.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "The solver used for the inner optimization problem of the logistic regression problem is L-BFGS (Liu and Nocedal, 1989), while for Ridge regression we used a linear conjugate descent method.",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 29,
      "context" : "This dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson’s disease(Tsanas et al., 2010).",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : "This is the random search method (Bergstra and Bengio, 2012) samples the hyperparameters from a predefined distribution.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "This is the iterative differentiation approach from (Domke, 2012), using the same inneroptimization algorithm as HOAG.",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "Given the success of recent stochastic optimization techniques (Schmidt et al., 2013; Johnson and Zhang, 2013) it seems natural to study a stochastic variant of this algorithm, that is, one in which the updates in the inner and outer optimization schemes have a cost that is independent of the number of samples.",
      "startOffset" : 63,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "Given the success of recent stochastic optimization techniques (Schmidt et al., 2013; Johnson and Zhang, 2013) it seems natural to study a stochastic variant of this algorithm, that is, one in which the updates in the inner and outer optimization schemes have a cost that is independent of the number of samples.",
      "startOffset" : 63,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of `2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.",
    "creator" : "LaTeX with hyperref package"
  }
}