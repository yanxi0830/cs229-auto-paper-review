{
  "name" : "1205.3181.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multiple Identifications in Multi-Armed Bandits",
    "authors" : [ "Sébastien Bubeck" ],
    "emails" : [ "sbubeck@princeton.edu", "tengyaow@princeton.edu", "nviswana@princeton.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We are interested in the following situation: An agent faces K unknown distributions, and he is allowed to do n sequential evaluations of the form (i,X) where i ∈ {1, . . . , K} is chosen by the agent and X is a random variable drawn from the ith distribution and revealed to the agent. The goal of the agent after the n evaluations is to identify a subset of the distributions (or arms in the multi-armed bandit terminology) corresponding to some prespecified criterion. This setting was introduced in Bubeck et al. [2009], where the goal was to identify the distribution with maximal mean. Note that in this formulation of the problem the evaluation budget n is fixed. Another\nar X\niv :1\n20 5.\npossible formulation is the one of the PAC model studied in Even-Dar et al. [2002], Mannor and Tsitsiklis [2004] where there is an accuracy of ε and a probability of correctness δ that are prespecified, and one wants to minimize the number of evaluations to attain this prespecified accuracy and probability of correctness. This latter formulation has a long history which goes back to the seminal work Bechhofer [1954]. In this paper we focus on the fixed budget setting of Bubeck et al. [2009]. For this fixed budget problem, Audibert et al. [2010] proposed a new analysis and an optimal algorithm (up to a logarithmic factor). In particular this work introduced a notion of best arm identification complexity, and it was shown that this quantity, denoted H , characterizes the hardness of identifying the best distribution in a specific set of K distributions. Intuitively, it was shown that the number of evaluations n has to be Ω(H/ logK) to be able to find the best arm, and the algorithm SR (Successive Rejects) finds it with O(H log2K) evaluations. Furthermore in the latter paper the authors also suggested the open problem of generalizing the analysis and algorithms to the identification of the m distributions with the top m means. Our main contribution is to solve this open problem. We suggest a non-trivial extension of the complexity H , denoted H〈m〉, to the problem of identifying the top m distributions, and we introduce a new algorithm, called SAR (Successive Accepts and Rejects), that requires only Õ ( H〈m〉 ) 1 evaluations to find the topm arms. We also propose a numerical comparison between SAR, SR and uniform sampling for the problem of finding the m top arms. Interestingly the experiments show that SR performs badly for m > 1, which shows that the tradeoffs involved in this generalized problem are fundamentally different from the ones for the single best arm identification.\nAs a by-product of our new analysis we are also able to solve an open problem of Gabillon et al. [2011]. In this paper the authors studied the setting where the agent facesM distinct best arm identification problems. A multi-bandit identification complexity was introduced, that we denote H [M ]. On the contrary to the setting of single best arm identification, here the algorithm proposed in Gabillon et al. [2011] that needs of order of H [M ] evaluations to find the best arm in each bandit requires to know the complexity H [M ] to tune its parameters. Using our SAR machinery, we construct a parameter-free algorithm that identify the best arm in each bandit with Õ ( H [M ] ) 2 evaluations.\nBoth them-best arms identification and the multi-bandit best arm identification have numerous potential applications. We refer the interested reader to the previously cited papers for several examples."
    }, {
      "heading" : "2 Problem setup",
      "text" : "We adopt the terminology of multi-armed bandits. The agent faces K arms and he has a budget of n evaluations (or pulls). To each arm i ∈ {1, . . . , K} there is an associated probability distribution νi, supported3 on [0, 1]. These distributions are unknown to the agent. The sequential evaluations protocol goes as follows: at each round t = 1, . . . , n, the agent chooses an arm It, and observes\n1In the m-best arms identification problem we write un = Õ(vn) when un = O(vn) up to logarithmic factor in K 2In the multi-bandit best arm identification problem we write un = Õ(vn) when un = O(vn) up to logarithmic\nfactor in MK 3One can directly generalize the discussion to σ-subgaussian distributions.\na reward drawn from νIt independently from the past given It. In the m-best arms identification problem, at the end of the n evaluations, the agent selects m arms denoted J1, . . . , Jm. The objective of the agent is that the set {J1, . . . , Jm} corresponds to the set of arms with the m highest mean rewards.\nDenote by µ1, . . . , µK the mean of the arms. In the following we assume that µ1 > . . . > µK . The ordering assumption comes without loss of generality, and the assumption that the means are all distinct is made for sake of notation (the complexity measures are slightly different if there is an ambiguity for the top m means). We evaluate the performance of the agent’s strategy by the probability of misidentification, that is\nen = P ({J1, . . . , Jm} 6= {1, . . . ,m}) .\nFiner measures of performance can be proposed, such as the simple regret rn = ∑m\ni=1(µi−EµJi). However, as it was argued in Audibert et al. [2010], for a first order analysis it is enough to focus on the quantity en.\nIn the (single) best arm identification, Audibert et al. [2010] introduced the following complexity measures. Let ∆i = µ1 − µi for i 6= 1, ∆1 = µ1 − µ2,\nH1 = K∑ i=1 1 ∆2i and H2 = max i∈{1,...,K} i∆−2i .\nIt is easy to see that these two complexity measures are equivalent up to a logarithmic factor since we have (see Audibert et al. [2010])\nH2 ≤ H1 ≤ log(2K)H2. (1)\n[Theorem 4, Audibert et al. [2010]] shows that the complexity H1 represents the hardness of the best arm identification problem. However, as far as upper bounds are concerned, the quantity H2 proved to be a useful surrogate for H1. For the m-best arms identification problem we define the following gaps and the associated complexity measures:\n∆ 〈m〉 i = { µi − µm+1 if i ≤ m µm − µi if i > m ,\nH 〈m〉 1 = K∑ i=1 1( ∆ 〈m〉 i\n)2 , H 〈m〉 2 = max i∈{1,...,K} i ( ∆ 〈m〉 (i) )−2 ,\nwhere the notation (i) ∈ {1, . . . , K} is defined such that ∆〈m〉(1) ≤ . . . ≤ ∆ 〈m〉 (K). We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H 〈m〉 1 holds true for the m-best arms identification problem. In this paper we shall prove an upper bound on en that gets small when n = Õ ( H 〈m〉 2 ) (recall that by (1), Õ ( H 〈m〉 2 ) = Õ ( H 〈m〉 1 ) ). This result is\nderived in Section 3, where we introduce our key algorithmic contribution, the SAR (Successive Accepts and Rejects) algorithm. We also present experiments for this setting in Section 5.\nIn Section 4 we consider the framework of multi-bandit introduced in Gabillon et al. [2011], where the agent faces M distinct best arm identification problems. For sake of notation we assume that each problem m ∈ {1, . . . ,M} has the same number of arms K. We also restrict our attention to the single best arm identification within each problem, but we could deal with m-best arms identification within each problem. We denote by ν1(m), . . . , νK(m) the unknown distributions of the arms in problem m. We define similarly all the relevant quantities for each problem, that is µ1(m) > . . . > µK(m),∆1(m), . . . ,∆K(m), H1(m) and H2(m). Finally we denote by (i,m) the arm i in problem m. In the multi-bandit best arm identification, the forecaster performs n sequential evaluations of the form (It,mt) ∈ {1, . . . , K} × {1, . . . ,M}. At the end of the n evaluations, the agent selects one arm for each problem, denoted (J1, 1), . . . , (JM ,M). The objective of the agent is to find the arm with the highest mean reward in each problem, that is in this setting the probability of misidentification can be written as\nen = P(∃m ∈ {1, . . . ,M} : Jm 6= 1).\nFollowing Gabillon et al. [2011] we introduce the following complexity measure\nH [M ] 1 = M∑ m=1 H1(m).\nAgain we define a sort of weaker complexity measure by ordering the gaps. Let\n∆ [M ] 1 ≤ ∆ [M ] 2 ≤ · · · ≤ ∆ [M ] MK\nbe a rearrangement of {∆i(m) : 1 ≤ i ≤ K, 1 ≤ m ≤M} in ascending order, and let\nH [M ] 2 = max k∈{1,...,MK} k ( ∆ [M ] k )−2 .\nWe conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H [M ]1 holds true for the multi-bandit best arm identification problem. In this paper we shall prove an upper bound on en that gets small when n = Õ ( H [M ] 2 ) (recall that by (1),\nÕ ( H\n[M ] 2\n) = Õ ( H\n[M ] 1\n) ). This result, derived in Section 4, builds upon the SAR strategy intro-\nduced in Section 3. The improvement with respect to Gabillon et al. [2011] is that our strategy is parameter-free, while the theoretical Gap-E introduced in Gabillon et al. [2011] requires the knowledge of H [M ]1 to tune its parameter. Moreover the analysis of SAR is much simpler than the one of Gap-E.\nFor each arm i and all time rounds t ≥ 1, we denote by Ti(t) = ∑t\ns=1 1It=i the number of times arm i was pulled from rounds 1 to t, and by Xi,1, Xi,2, . . . , Xi,Ti,t the sequence of associated rewards. Introduce µ̂i,s = 1s ∑s t=1Xi,t the empirical mean of arm i after s evaluations. Denote by Xi,s(m) and µ̂i,s(m) the corresponding quantities in the multi-bandit problem.\n3 m-best arms identification In this section we describe and analyze a new algorithm, called SAR (Sucessive Accepts and Rejects), for the m-best arms identification problem, see Figure 1 for its precise description. The idea behind SAR is similar to the one for SR (Successive Rejects) that was designed for the (single) best arm identification problem, with the additional feature that SAR sometimes accepts an arm because it is confident enough that this arm is among the m top arms. Informally SAR proceeds as follows. First the algorithm divides the time (i.e., the n rounds) inK−1 phases. At the end of each phase, the algorithm either accepts the arm with the highest empirical mean or dismisses the arm with the lowest empirical mean, and in both cases the corresponding arm is deactivated. During the next phase, it pulls equally often each active arm. The key to decide whether to accept or reject during a certain phase k is to rely on estimates for the gaps ∆〈m〉i . More precisely, assume that the algorithm has already acceptedm−m(k) arms J1, . . . , Jm−m(k), i.e. there ism(k) arms left to find. Then, at the end of phase k, SAR computes for the m(k) empirical best arms (among the active arms) the distance (in terms of empirical mean) to the (m(k) + 1)th empirical best arm among the active arms. On the other hand for the active arms that are not among the m(k) empirical best arms, SAR computes the distance to the m(k)th empirical best arm. Finally SAR deactivates the arm ik that maximizes these empirical distances. If ik is currently the empirical best arm, then SAR accepts ik and sets m(k + 1) = m(k) − 1, Jm−m(k+1) = ik, and otherwise it simply rejects ik. The length of the phases are chosen similarly to what was done for the SR algorithm.\nTheorem 1 The probability of error of SAR in the m-best arms identification problem satisfies\nen ≤ 2K2 exp ( − n−K\n8log(K)H 〈m〉 2\n) .\nProof Consider the event ξ defined by\nξ = { ∀i ∈ {1, . . . , K}, k ∈ {1, . . . , K − 1}, ∣∣∣∣∣ 1nk nk∑ s=1 Xi,s − µi ∣∣∣∣∣ ≤ 14∆〈m〉(K+1−k) } .\nBy Hoeffding’s Inequality and an union bound, the probability of the complementary event ξ̄ can be bounded as follows\nP(ξ̄) ≤ K∑ i=1 K−1∑ k=1 P (∣∣∣∣∣ 1nk nk∑ s=1 Xi,s − µi ∣∣∣∣∣ > 14∆〈m〉(K+1−k) )\n≤ K∑ i=1 K−1∑ k=1 2 exp(−2nk(∆〈m〉(K+1−k)/4) 2)\n≤ 2K2 exp ( − n−K\n8log(K)H 〈m〉 2\n) ,\nwhere the last inequality comes from the fact that\nnk ( ∆ 〈m〉 (K+1−k) )2 ≥ n−K\nlog(K)(K + 1− k) (\n∆ 〈m〉 (K+1−k)\n)−2 ≥ n−K log(K)H\n〈m〉 2\n.\nThus, it suffices to show that on the event ξ, the algorithm does not make any error. We prove this by induction on k. Let k ≥ 1. Assume the algorithm makes no error in all previous k − 1 stages. Note that event ξ implies that at the end of stage k, all empirical means are within 1\n4 ∆ 〈m〉 (K+1−k) of\nthe respective true means. Let Ak = {a1, . . . , aK+1−k} be the the set of active arms during phase k. We order the ai’s such that µa1 > µa2 > · · · > µaK+1−k . To slightly lighten the notation we denote m′ = m(k) for the number of arms that are left to find in phase k. The assumption that no error occurs in the first k − 1 stages implies that\na1, a2, . . . , am′ ∈ {1, . . . ,m}, am′+1, . . . , aK+1−k ∈ {m+ 1, . . . , K}.\nIf an error is made at stage k, it can be one of the following two types:\n1. The algorithm accepts aj at stage k for some j ≥ m′ + 1.\n2. The algorithm rejects aj at stage k for some j ≤ m′.\nAgain to slightly shorten the notation we denote σ = σk for the bijection (from {1, . . . , K+1−k} to Ak) such that µ̂σ(1),nk ≥ µ̂σ(2),nk ≥ · · · ≥ µ̂σ(K+1−k),nk . Suppose Type 1 error occurs. Then\naj = σ(1) since if the algorithm accepts, it must accept the empirical best arm. Furthermore we also have µ̂aj ,nk − µ̂σ(m′+1),nk ≥ µ̂σ(m′),nk − µ̂σ(K+1−k),nk , (2) since otherwise the algorithm would rather reject arm σ(K + 1− k). The condition aj = σ(1) and the event ξ implies that\nµ̂aj ,nk ≥ µ̂a1,nk ⇒ µaj + 1 4 ∆ 〈m〉 (K+1−k) ≥ µa1 − 1 4 ∆ 〈m〉 (K+1−k)\n⇒ ∆〈m〉(K+1−k) > 1 2 ∆ 〈m〉 (K+1−k) ≥ µa1 − µaj ≥ µa1 − µm+1\nWe then look at the condition (2). In the event of ξ, for all i ≤ m′, we have\nµ̂ai,nk ≥ µai − 1 4 ∆ 〈m〉 (K+1−k) ≥ µam′ − 1 4 ∆ 〈m〉 (K+1−k) ≥ µm − 1 4 ∆ 〈m〉 (K+1−k).\nSo there are m + 1 arms in Ak (namely a1, a2, . . . , am′ , aj) whose empirical means are at least µm− 14∆ 〈m〉 (K+1−k), which means µ̂σ(m′+1),nk ≥ µm− 1 4 ∆ 〈m〉 (K+1−k).On the other hand, µ̂σ(K+1−k),nk ≤ µ̂aK+1−k,nk ≤ µaK+1−k + 14∆ 〈m〉 (K+1−k). Therefore, using those two observations and (2) we deduce(\nµaj + 1 4 ∆ 〈m〉 (K+1−k)\n) − ( µm − 1\n4 ∆ 〈m〉 (K+1−k) ) ≥ ( µm − 1\n4 ∆ 〈m〉 (K+1−k)\n) − ( µaK+1−k + 1\n4 ∆ 〈m〉 (K+1−k) ) ⇒ ∆〈m〉(K+1−k) ≥ 2µm − µaj − µaK+1−k > µm − µaK+1−k .\nThus so far we proved that if there is a Type 1 error, then\n∆ 〈m〉 (K+1−k) > max(µa1 − µm, µm − µaK+1−k).\nBut at stage k, only k − 1 arms have been accepted or rejected, thus ∆〈m〉(K+1−k) ≤ max(µa1 − µm, µm − µaK+1−k). By contradiction, we conclude that Type 1 error does not occur.\nSuppose Type 2 error occurs. The reasoning is symmetric to Type 1. In fact, if we rephrase the problem as finding the K −m worst arms instead of the m best arms, this is exactly the same as Type 1 error. Hence Type 2 error cannot occur as well. This completes the induction and consequently the proof of the theorem."
    }, {
      "heading" : "4 Multi-bandit best arm identification",
      "text" : "In this section we use the idea of SAR for multi-bandit best arm identification. Here at the end of each phase we estimate the gaps ∆i(m) within each problem, and we reject the arm with the largest such estimated gap. Moreover if a problem is left with only one active arm, then this arm is accepted and the problem is deactivated. The corresponding strategy is described precisely in Figure 2\nTheorem 2 The probability of error of SAR in the multi-bandit best arm identification problem satisfies\nen ≤ 2M2K2 exp ( − n−MK\n8log(MK)H [M ] 2\n) .\nProof Consider the event ξ defined by\nξ = { ∀ 1 ≤ i ≤ K, 1 ≤ m ≤M, 1 ≤ k ≤MK − 1∣∣∣∣∣ 1nk nk∑ s=1 Xi,s(m)− µi(m) ∣∣∣∣∣ ≤ 14∆(MK+1−k) } .\nFollowing the same reasoning than in the proof of Theorem 1, it suffices to show that in the event of ξ the algorithm makes no error. We do this by induction on the phase k of the algorithm. Let k ≥ 1. Assume the algorithm makes no error in all previous k − 1 stages. Then at phase k, for all active problem m, the arm (1,m) is still active. Moreover, as only k − 1 arms have been deactivated, one clearly has\nmax (i,m)∈Ak\n(µ1(m)− µi(m)) ≥ ∆(MK+1−k).\nSuppose the above maximum is achieved for the arm (i∗,m∗), so we have\nµ1(m ∗)− µi∗(m∗) ≥ ∆(MK+1−k). (3)\nAssume now that the algorithm makes an error at the end of phase k, i.e. some arm (1,m) is deactivated and it was not the last active arm in problem m. For this to happen, we necessarily have for some j ∈ {2, . . . , K} (e.g., j = hk(m)),\nµ̂j,nk(m)− µ̂1,nk(m) ≥ µ̂1,nk(m∗)− µ̂i∗,nk(m∗). (4)\nClearly on the event ξ one has\nµ̂j,nk(m)− µ̂1,nk(m) = µ̂j,nk(m)− µj(m) + µj(m)− µ1(m) + µ1(m)− µ̂1,nk(m)\n< 1\n2 ∆(MK+1−k).\nOn the other hand, using (3) and ξ, one has\nµ̂1,nk(m ∗)− µ̂i∗,nk(m∗)\n= µ̂1,nk(m ∗)− µ1(m∗) + µ1(m∗)− µi∗(m∗) + µi∗(m∗)− µ̂i∗,nk(m∗) ≥ 1 2 ∆(MK+1−k).\nTherefore, µ̂1,nk(m ∗)− µ̂i∗,nk(m∗) > µ̂j,nk(m)− µ̂1,nk(m), contradicting (4). This completes the induction and the proof."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011].\nWe compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification). The SR strategy is the plain Successive Rejects strategy of Audibert et al. [2010] which was designed to find the (single) best arm. We slightly improve it for m-best identification by running only K − m − 1 phases (while still using the full budget n) and then returning the last m surviving arms. Finally we consider the extension of UCB-E to the m-best arms identification problem, which is based on a similar idea than the extension Gap-E of Gabillon et al. [2011] for the multi-bandit best arm identification, see Figure 3 for the details. Note that this last algorithm requires to know the complexity H〈m〉1 . One could propose an adaptive version, using ideas described in Audibert et al. [2010], but for sake of simplicity we restrict our attention to the non-adaptive algorithm.\nIn our experiments we consider only Bernoulli distributions, and the optimal arm always has parameter 1/2. Each experiment corresponds to a different situation for the gaps, they are either clustered in few groups, or distributed according to an arithmetic or geometric progression. For each experiment we plot the probability of misidentification for each strategy, varying m between 2 and K − 1. The allocation budget for each experiment is chosen to be roughly equal to max1≤m≤K−1H 〈m〉 1 . We report our results in Figure 4. The parameters for the experiments are as follows:\n• Experiment 1: One group of bad arms, K = 20, µ2:20 = 0.4 (meaning for any j ∈ {2, . . . , 20}, µj = 0.4)\n• Experiment 2: Two groups of bad arms, K = 20, µ2:6 = 0.42, µ7:20 = 0.38.\n• Experiment 3: Geometric progression, K = 4, µi = 0.5− (0.37)i, i ∈ {2, 3, 4}.\n• Experiment 4: 6 arms divided in three groups, K = 6, µ2 = 0.42, µ3:4 = 0.4, µ5:6 = 0.35.\n• Experiment 5: Arithmetic progression, K = 15, µi = 0.5− 0.025i, i ∈ {2, . . . , 15}.\n• Experiment 6: Three groups of bad arms, K = 30, µ2:6 = 0.45, µ7:20 = 0.43, µ21:30 = 0.38.\nIt is interesting to note that SR performs badly for m-best arms identification when m > 1, as it has even worse performances than the naive uniform sampling in many cases. This shows that the tradeoffs involved in finding the single best arm and finding the top m arms are fundamentally different. As expected SAR always outperforms uniform sampling, and Gap-E has slightly better performances than SAR (but Gap-E requires an extra information to tune its parameter, and the adapative version comes with no provable guarantee)."
    } ],
    "references" : [ {
      "title" : "Best arm identification in multi-armed bandits",
      "author" : [ "J.-Y. Audibert", "S. Bubeck", "R. Munos" ],
      "venue" : "In Proceedings of the 23rd Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Audibert et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2010
    }, {
      "title" : "A single-sample multiple decision procedure for ranking means of normal populations with known variances",
      "author" : [ "R.E. Bechhofer" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "Bechhofer.,? \\Q1954\\E",
      "shortCiteRegEx" : "Bechhofer.",
      "year" : 1954
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "Pure exploration in finitely-armed and continuously-armed bandits",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2011
    }, {
      "title" : "Pac bounds for multi-armed bandit and markov decision processes",
      "author" : [ "E. Even-Dar", "S. Mannor", "Y. Mansour" ],
      "venue" : "In Proceedings of the Fifteenth Annual Conference on Computational Learning Theory (COLT),",
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "This setting was introduced in Bubeck et al. [2009], where the goal was to identify the distribution with maximal mean.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "possible formulation is the one of the PAC model studied in Even-Dar et al. [2002], Mannor and Tsitsiklis [2004] where there is an accuracy of ε and a probability of correctness δ that are prespecified, and one wants to minimize the number of evaluations to attain this prespecified accuracy and probability of correctness.",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "possible formulation is the one of the PAC model studied in Even-Dar et al. [2002], Mannor and Tsitsiklis [2004] where there is an accuracy of ε and a probability of correctness δ that are prespecified, and one wants to minimize the number of evaluations to attain this prespecified accuracy and probability of correctness.",
      "startOffset" : 60,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "This latter formulation has a long history which goes back to the seminal work Bechhofer [1954]. In this paper we focus on the fixed budget setting of Bubeck et al.",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "This latter formulation has a long history which goes back to the seminal work Bechhofer [1954]. In this paper we focus on the fixed budget setting of Bubeck et al. [2009]. For this fixed budget problem, Audibert et al.",
      "startOffset" : 79,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "For this fixed budget problem, Audibert et al. [2010] proposed a new analysis and an optimal algorithm (up to a logarithmic factor).",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "However, as it was argued in Audibert et al. [2010], for a first order analysis it is enough to focus on the quantity en.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "In the (single) best arm identification, Audibert et al. [2010] introduced the following complexity measures.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "It is easy to see that these two complexity measures are equivalent up to a logarithmic factor since we have (see Audibert et al. [2010]) H2 ≤ H1 ≤ log(2K)H2.",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "[Theorem 4, Audibert et al. [2010]] shows that the complexity H1 represents the hardness of the best arm identification problem.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H 〈m〉 1 holds true for the m-best arms identification problem.",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H [M ] 1 holds true for the multi-bandit best arm identification problem.",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H [M ] 1 holds true for the multi-bandit best arm identification problem. In this paper we shall prove an upper bound on en that gets small when n = Õ ( H [M ] 2 ) (recall that by (1), Õ ( H [M ] 2 ) = Õ ( H [M ] 1 ) ). This result, derived in Section 4, builds upon the SAR strategy introduced in Section 3. The improvement with respect to Gabillon et al. [2011] is that our strategy is parameter-free, while the theoretical Gap-E introduced in Gabillon et al.",
      "startOffset" : 56,
      "endOffset" : 464
    }, {
      "referenceID" : 0,
      "context" : "We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H [M ] 1 holds true for the multi-bandit best arm identification problem. In this paper we shall prove an upper bound on en that gets small when n = Õ ( H [M ] 2 ) (recall that by (1), Õ ( H [M ] 2 ) = Õ ( H [M ] 1 ) ). This result, derived in Section 4, builds upon the SAR strategy introduced in Section 3. The improvement with respect to Gabillon et al. [2011] is that our strategy is parameter-free, while the theoretical Gap-E introduced in Gabillon et al. [2011] requires the knowledge of H [M ] 1 to tune its parameter.",
      "startOffset" : 56,
      "endOffset" : 569
    }, {
      "referenceID" : 0,
      "context" : "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al.",
      "startOffset" : 53,
      "endOffset" : 376
    }, {
      "referenceID" : 0,
      "context" : "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification).",
      "startOffset" : 53,
      "endOffset" : 615
    }, {
      "referenceID" : 0,
      "context" : "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification). The SR strategy is the plain Successive Rejects strategy of Audibert et al. [2010] which was designed to find the (single) best arm.",
      "startOffset" : 53,
      "endOffset" : 772
    }, {
      "referenceID" : 0,
      "context" : "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification). The SR strategy is the plain Successive Rejects strategy of Audibert et al. [2010] which was designed to find the (single) best arm. We slightly improve it for m-best identification by running only K − m − 1 phases (while still using the full budget n) and then returning the last m surviving arms. Finally we consider the extension of UCB-E to the m-best arms identification problem, which is based on a similar idea than the extension Gap-E of Gabillon et al. [2011] for the multi-bandit best arm identification, see Figure 3 for the details.",
      "startOffset" : 53,
      "endOffset" : 1158
    }, {
      "referenceID" : 0,
      "context" : "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification). The SR strategy is the plain Successive Rejects strategy of Audibert et al. [2010] which was designed to find the (single) best arm. We slightly improve it for m-best identification by running only K − m − 1 phases (while still using the full budget n) and then returning the last m surviving arms. Finally we consider the extension of UCB-E to the m-best arms identification problem, which is based on a similar idea than the extension Gap-E of Gabillon et al. [2011] for the multi-bandit best arm identification, see Figure 3 for the details. Note that this last algorithm requires to know the complexity H 1 . One could propose an adaptive version, using ideas described in Audibert et al. [2010], but for sake of simplicity we restrict our attention to the non-adaptive algorithm.",
      "startOffset" : 53,
      "endOffset" : 1389
    } ],
    "year" : 2012,
    "abstractText" : "We study the problem of identifying the top m arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem.",
    "creator" : "LaTeX with hyperref package"
  }
}