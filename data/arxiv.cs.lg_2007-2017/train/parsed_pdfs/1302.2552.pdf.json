{
  "name" : "1302.2552.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "odalricambrym.maillard@gmail.com", "remi.munos@inria.fr", "daniil@ryabko.net" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 2.\n25 52\nv1 [\ncs .L\nG ]"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of selecting the right state-representation in an average-reward reinforcement learning problem. Each state-representation is defined by a model φj (to which corresponds a state space Sφj ) and we assume that the number J of available models is finite and that (at least) one model is a weakly-communicating Markov decision process (MDP). We do not make any assumption at all about the other models. This problem is considered in the general reinforcement learning setting, where an agent interacts with an unknown environment in a single stream of repeated observations, actions and rewards. There are no “resests,” thus all the learning has to be done online. Our goal is to construct an algorithm that performs almost as well as the algorithm that knows both which model is a MDP (knows the “true” model) and the characteristics of this MDP (the transition probabilities and rewards).\nConsider some examples that help motivate the problem. The first example is high-level feature selection. Suppose that the space of histories is huge, such as the space of video streams or that of game plays. In addition to these data, we also have some high-level features extracted from it, such as “there is a person present in the video” or “the adversary (in a game) is aggressive.” We know that most of the features are redundant, but we also know that some combination of some of the features describes the problem well and exhibits Markovian dynamics. Given a potentially large number of feature combinations of this kind, we want to find a policy whose average reward is as good as that of the best policy for the right combination of features. Another example is bounding the order of an MDP. The process is known to be k-order Markov, where k is unknown but un upper bound K >> k is given. The goal is to perform as well as if we knew k. Yet another example is selecting the right discretization. The environment is an MDP with a continuous state space. We have several candidate quantizations of the state space, one of which gives an MDP. Again, we would like to find a policy that is as good as the optimal policy for the right discretization. This example also opens\nthe way for extensions of the proposed approach: we would like to be able to treat an infinite set of possible discretization, none of which may be perfectly Markovian. The present work can be considered the first step in this direction.\nIt is important to note that we do not make any assumptions on the “wrong” models (those that do not have Markovian dynamics). Therefore, we are not able to test which model is Markovian in the classical statistical sense, since in order to do that we would need a viable alternative hypothesis (such as, the model is not Markov but is K-order Markov). In fact, the constructed algorithm never “knows” which model is the right one; it is “only” able to get the same average level of reward as if it knew.\nPrevious work. This work builds on previous work on learning average-reward MDPs. Namely, we use in our algorithm as a subroutine the algorithm UCRL2 of [6] that is designed to provide finite time bounds for undiscounted MDPs. Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP. The diameter D of a MDP is defined in [6] as the expected minimum time required to reach any state starting from any other state. A related result is reported in [3], which improves on constants related to the characteristics of the MDP.\nA similar approach has been considered in [10]; the difference is that in that work the probabilistic characteristics of each model are completely known, but the models are not assumed to be Markovian, and belong to a countably infinite (rather than finite) set.\nThe problem we address can be also viewed as a generalization of the bandit problem (see e.g. [9, 8, 1]): there are finitely many “arms”, corresponding to the policies used in each model, and one of the arms is the best, in the sense that the corresponding model is the “true” one. In the usual bandit setting, the rewards are assumed to be i.i.d. thus one can estimate the mean value of the arms while switching arbitrarily from one arm to the next (the quality of the estimate only depends on the number of pulls of each arm). However, in our setting, estimating the average-reward of a policy requires playing it many times consecutively. This can be seen as a bandit problem with dependent arms, with complex costs of switching between arms.\nContribution. We show that despite the fact that the true Markov model of states is unknown and that nothing is assumed on the wrong representations, it is still possible to derive a finite-time analysis of the regret for this problem. This is stated in Theorem 1; the bound on the regret that we obtain is of order T 2/3.\nThe intuition is that if the “true” model φ∗ is known, but its probabilistic properties are not, then we still know that there exists an optimal control policy that depends on the observed state sj∗,t only. Therefore, the optimal rate of rewards can be obtained by a clever exploration/exploitation strategy, such as UCRL2 algorithm [6]. Since we do not know in advance which model is a MDP, we need to explore them all, for a sufficiently long time in order to estimate the rate of rewards that one can get using a good policy in that model.\nOutline. In Section 2 we introduce the precise notion of model and set up the notations. Then we present the proposed algorithm in Section 3; it uses UCRL2 of [6] as a subroutine and selects the models φ according to a penalized empirical criterion. In Section 4 we discuss some directions for further development. Finally, Section 5 is devoted to the proof of Theorem 1."
    }, {
      "heading" : "2 Notation and definitions",
      "text" : "We consider a space of observations O, a space of actions A, and a space of rewards R (all assumed to be Polish). Moreover, we assume that A is of finite cardinality A def= |A| and that 0 ∈ R ⊂ [0, 1]. The set of histories up to time t for all t ∈ N∪{0} will be denoted by H<t def= O×(A×R×O)t−1, and we define the set of all possible histories by H def= ∞⋃\nt=1 H<t. Environments. For a Polish X , we Denote by P(X ) the set of probability distributions over X . Define an environment to be a mapping from the set of histories H to the set of functions that map any action a ∈ A to a probability distribution νa ∈ P(R × O) over the product space of rewards and observations.\nWe consider the problem of reinforcement learning when the learner interacts with some unknown environment e⋆. The interaction is sequential and goes as follows: first some h<1 = {o0} is generated according to ι, then at time step t > 0, the learner choses an action at ∈ A according to the current history h<t ∈ H<t. Then a couple of reward and observations (rt, ot) is drawn according to the distribution (e⋆(h<t))at ∈ P(R×O). Finally, h<t+1 is defined by the concatenation of h<t with (at, rt, ot). With these notations, at each time step t > 0, ot−1 is the last observation given to the learner before choosing an action, at is the action output at this step, and rt is the immediate reward received after playing at.\nState representation functions (models). Let S ⊂ N be some finite set; intuitively, this has to be considered as a set of states. A state representation function φ is a function from the set of histories H to S. For a state representation function φ, we will use the notation Sφ for its set of states, and st,φ := φ(h<t).\nIn the sequel, when we talk about a Markov decision process, it will be assumed to be weakly communicating, which means that for each pair of states u1, u2 there exists k ∈ N and a sequence of actions α1, .., αk ∈ A such that P (sk+1,φ = u2|s1,φ = u1, a1 = α1...ak = αk) > 0. Having that in mind, we introduce the following definition.\nDefinition 1 We say that an environment e with a state representation function φ is Markov, or, for short, that φ is a Markov model (of e), if the process (st,φ, at, rt), t ∈ N is a (weakly communicating) Markov decision process.\nFor example, consider a state-representation function φ that depends only on the last observation, and that partitions the observation space into finitely many cells. Then an environment is Markov with this representation function if the probability distribution on the next cells only depends on the last observed cell and action. Note that there may be many state-representation functions with which an environment e is Markov."
    }, {
      "heading" : "3 Main results",
      "text" : "Given a set Φ = {φj ; j 6 J} of J state-representation functions (models), one of which being a Markov model of the unknown environment e⋆, we want to construct a strategy that performs nearly as well as the best algorithm that knows which φj is Markov, and knows all the probabilistic characteristics (transition probabilities and rewards) of the MDP corresponding to this model. For that purpose we define the regret of any strategy at time T , like in [6, 3], as\n∆(T ) def = Tρ⋆ −\nT∑\nt=1\nrt ,\nwhere rt are the rewards received when following the proposed strategy and ρ⋆ is the average optimal value in the best Markov model, i.e., ρ⋆ = limT 1T E( ∑T t=1 rt(π\n⋆)) where rt(π⋆) are the rewards received when following the optimal policy for the best Markov model. Note that this definition makes sense since when the MDP is weakly communicating, the average optimal value of reward does not depend on the initial state. Also, one could replace Tρ∗ with the expected sum of rewards obtained in T steps (following the optimal policy) at the price of an additional O( √ T ) term.\nIn the next subsection, we describe an algorithm that achieves a sub-linear regret of order T 2/3."
    }, {
      "heading" : "3.1 Best Lower Bound (BLB) algorithm",
      "text" : "In this section, we introduce the Best-Lower-Bound (BLB) algorithm, described in Figure 1.\nThe algorithm works in stages of doubling length. Each stage consists in 2 phases: an exploration and an exploitation phase. In the exploration phase, BLB plays the UCRL2 algorithm on each model (φj)16j6J successively, as if each model φj was a Markov model, for a fixed number τi,1,J of rounds. The exploitation part consists in selecting first the model with highest lower bound, according to the empirical rewards obtained in the previous exploration phase. This model is initially selected for the same time as in the exploration phase, and then a test decides to either continue playing this model (if its performance during exploitation is still above the corresponding lower bound, i.e. if the rewards obtained are still at least as good as if it was playing the best model). If it does not pass the test, then another model (with second best lower-bound) is select and played, and so on. Until the exploitation phase (of fixed length τi,2) finishes and the next stage starts.\nThe length of stage i is fixed and defined to be τi def = 2i. Thus for a total time horizon T , the number of stages I(T ) before time T is I(T ) def = xlog2(T + 1)y. Each stage i (of length τi) is further decomposed into an exploration (length τi,1) and an exploitation (length τi,2) phases.\nExploration phase. All the models {φj}j6J are played one after another for the same amount of time τi,1,J def = τi,1 J . Each episode 1 6 j 6 J consists in running the UCRL2 algorithm using the model of states and transitions induced by the state-representation function φj . Note that UCRL2 does not require the horizon T in advance, but requires a parameter p in order to ensure a near optimal regret bound with probability higher than 1 − p. We define this parameter p to be δi(δ) in stage i, where\nδi(δ) def = (2i − (J−1 + 1)22i/3 + 4)−12−i+1δ . (1)\nThe average empirical reward received during each episode is written µ̂i,1(φj).\nExploitation phase. We use the empirical rewards µ̂i,1(φj) received in the previous exploration part of stage i together with a confidence bound in order to select the model to play. Moreover, a model φ is no longer run for a fixed period of time (as in the exploration part of stage i), but for a period τi,2(φ) that depends on some test; we first initialize J := {1, . . . , J} and then choose\nĵ def = argmax j∈J µ̂i,1(φj)− 2B(i, φj , δ) , (2)\nwhere we define\nB(i, φ, δ) def = 34f(τi − 1 + τi,1)|Sφ|\n√ A log(\nτi,1,J δi(δ) )\nτi,1,J , (3)\nwhere δ and the function f are parameters of the BLB algorithm. Then UCRL2 is played using the selected model φĵ for the parameter δi(δ). In parallel we test whether the average empirical reward we receive during this exploitation phase is high enough; at time t, if the length of the current episode is larger than τ1,i,J , we test if\nµ̂i,2,t(φĵ) > µ̂i,1(φĵ)− 2B(i, φĵ , δ). (4) If the test is positive, we keep playing UCRL2 using the same model. Now, if the test fails, then the model ĵ is discarded (until the end of stage i) i.e. we update J := J \\ {ĵ} and we select a new one according to (2). We repeat those steps until the total time τi,2 of the exploitation phase of stage i is over.\nRemark Note that the model selected for exploitation in (2) is the one that has the best lower bound. This is a pessimistic (or robust) selection strategy. We know that if the right model is selected, then with high probability, this model will be kept during the whole exploitation phase. If this is not the right model, then either the policy provides good rewards and we should keep playing it, or it does not, in which case it will not pass the test (4) and will be removed from the set of models that will be exploited in this phase."
    }, {
      "heading" : "3.2 Regret analysis",
      "text" : "Theorem 1 (Main result) Assume that a finite set of J state-representation functions Φ is given, and there exists at least one function φ⋆ ∈ Φ such that with φ⋆ as a state-representation function the environment is a Markov decision process. If there are several such models, let φ⋆ be the one with the highest average reward of the optimal policy of the corresponding MDP. Then the regret (with respect to the optimal policy corresponding to φ∗) of the BLB algorithm run with parameter δ, for any horizon T , with probability higher than 1− δ is bounded as follows\n∆(T ) 6 cf(T )S ( AJ log ( (Jδ)−1 ) log2(T ) )1/2 T 2/3 + c′DS ( A log(δ−1) log2(T )T )1/2 + c(f,D), (5)\nfor some numerical constants c, c′ and c(f,D). The parameter f(t) can be chosen to be any increasing function, for instance the choice f(t) := log2 t+ 1, gives c(f,D) 6 2\nD. The proof of this result is reported in Section 5.\nRemark. Importantly, the algorithm considered here does not know in advance the diameter D of the true model, nor the time horizon T . Due to this lack of knowledge, it uses a guess f(t) (e.g. log(t)) on this diameter, which result in the additional regret term c(f,D) and the additional factor f(T ); knowing D would enable to remove both of them, but this is a strong assumption. Choosing f(t) := log2 t + 1 gives a bound which is of order T\n2/3 in T but is exponential in D; taking f(t) := tε we get a bound of order T 2/3+ε in T but of polynomial order 1/ε in D."
    }, {
      "heading" : "4 Discussion and outlook",
      "text" : "Intuition. The main idea why this algorithm works is as follows. The “wrong” models are used during exploitation stages only as long as they are giving rewards that are higher than the rewards that could be obtained in the “true” model. All the models are explored sufficiently long so as to be able to estimate the optimal reward level in the true model, and to learn its policy. Thus, nothing has to be known about the “wrong” models. This is in stark contrast to the usual situation in mathematical statistics, where to be able to test a hypothesis about a model (e.g., that the data is generated by a certain model versus some alternative models), one has to make assumptions about alternative models. This has to be done in order to make sure that the Type II error is small (the power of the test is large): that this error is small has to be proven under the alternative. Here, although we are solving seemingly the same problem, the role of the Type II error is played by the rewards. As long as the rewards are high we do not care where the model we are using is correct or not. We only have to ensure that the true model passes the test.\nAssumptions. A crucial assumption made in this work is that the “true” model φ∗ belongs to a known finite set. While passing from a finite to a countably infinite set appears rather straightforward, getting rid of the assumption that this set contains the true model seems more difficult. What one would want to obtain in this setting is sub-linear regret with respect to the performance of the optimal policy in the best model; this, however, seems difficult without additional assumptions on the probabilistic characteristics of the models. Another approach not discussed here would be to try to build a good state representation function, as what is suggested for instance in [5]. Yet another interesting generalization in this direction would be to consider uncountable (possibly parametric but general) sets of models. This, however, would necessarily require some heavy assumptions on the set of models.\nRegret. The reader familiar with adversarial bandit literature will notice that our bound of order T 2/3 is worse than T 1/2 that usually appears in this context (see, for example [2]). The reason is that our notion of regret is different: in adversarial bandit literature, the regret is measured with respect to the best choice of the arm for the given fixed history. In contrast, we measure the regret with respect to the best policy (for knows the correct model and its parameters) that, in general, would obtain completely different (from what our algorithm would get) rewards and observations right from the beginning.\nEstimating the diameter? As previously mentioned, a possibly large additive constant c(f,D) appears in the regret since we do not known a bound on the diameter of the MDP in the “true” model, and use logT instead. Finding a way to properly address this problem by estimating online the diameter of the MDP is an interesting open question. Let us provide two intuitions concerning this problem. First, we notice that, as reported in [6], when we compute an optimistic model based on the empirical rewards and transitions of the true model, the span of the corresponding optimistic value function sp(V̂ +) is always smaller than the diameter D. This span increases as we get more rewards and transitions samples, which gives a natural empirical lower bound on D. However, it seems quite difficult to compute a tight empirical upper bound on D (or sp(V̂ +)). In [3], the authors derive a regret bound that scales with the span of the true value function sp(V ⋆), which is also less than D, and can be significantly smaller in some cases. However, since we do not have the property that sp(V̂ +) 6 sp(V ⋆), we need to introduce an explicit penalization in order to control the span of the computed optimistic models, and this requires assuming we know an upper bound B on sp(V ⋆) in order to guarantee a final regret bound scaling with B. Unfortunately this does not solve the estimation problem of D, which remains an open question."
    }, {
      "heading" : "5 Proof of Theorem 1",
      "text" : "In this section, we now detail the proof of Theorem 1. The proof is stated in several parts. First we remind a general confidence bound for the UCRL2 algorithm in the true model. Then we decompose the regret into the sum of the regret in each stage i. After analyzing the contribution to the regret in stage i, we then gather all stages and tune the length of each stage and episode in order to get the final regret bound."
    }, {
      "heading" : "5.1 Upper and Lower confidence bounds",
      "text" : "From the analysis of UCRL2 in [6], we have the property that with probability higher than 1 − δ′, the regret of UCRL2 when run for τ consecutive many steps from time t1 in the true model φ⋆ is upper bounded by\nρ⋆ − 1 τ\nt1+τ−1∑\nt=t1\nrt 6 34D|Sφ⋆ | √ A log( τδ′ )\nτ , (6)\nwhere D is the diameter of the MDP. What is interesting is that this diameter does not need to be known by the algorithm. Also by carefully looking at the proof of UCRL, it can be shown that the following bound is also valid with probability higher than 1− δ′:\n1\nτ\nt1+τ−1∑\nt=t1\nrt − ρ⋆ 6 34D|Sφ⋆ | √ A log( τδ′ )\nτ .\nWe now define the following quantity, for every model φ, episode length τ and δ′ ∈ (0, 1)\nBD(τ, φ, δ ′) def = 34D|Sφ|\n√ A log( τδ′ )\nτ . (7)"
    }, {
      "heading" : "5.2 Regret of stage i",
      "text" : "In this section we analyze the regret of the stage i, which we denote ∆i. Note that since each stage i 6 I is of length τi = 2i except the last one I that may stop before, we have\n∆(T ) =\nI(T )∑\ni=1\n∆i , (8)\nwhere I(T ) = xlog2(T+1)y. We further decompose∆i = ∆1,i+∆i,2 into the regret corresponding to the exploration stage ∆1,i and the regret corresponding to the exploitation stage ∆i,2.\nRecall that τi,1 is the total length of the exploration stage i and τi,2 is the total length of the exploitation stage i. Then for each model φ, we write τi,1,J def = τi,1 J the number of consecutive steps during which the UCRL2 algorithm is run with model φ in the exploration stage i, and τi,2(φ) the number of consecutive steps during which the UCRL2 algorithm is run with model φ in the exploitation stage i.\nGood and Bad models. Let us now introduce the two following sets of models, defined after the end of the exploration stage, i.e. at time ti.\nGi def= {φ ∈ Φ ; µ̂i,1(φ)− 2B(i, φ, δ) ≥ µ̂i,1(φ⋆)− 2B(i, φ⋆, δ)}\\{φ∗} , Bi def= {φ ∈ Φ ; µ̂i,1(φ)− 2B(i, φ, δ) < µ̂i,1(φ⋆)− 2B(i, φ⋆, δ)} .\nWith this definition, we have the decomposition Φ = Gi ∪ {φ⋆} ∪ Bi."
    }, {
      "heading" : "5.2.1 Regret in the exploration phase",
      "text" : "Since in the exploration stage i each model φ is run for τi,1,J many steps, the regret for each model φ 6= φ⋆ is bounded by τi,1,Jρ⋆. Now the regret for the true model is τi,1,J (ρ⋆ − µ̂1(φ⋆)), thus the total contribution to the regret in the exploration stage i is upper-bounded by\n∆i,1 6 τi,1,J(ρ ⋆ − µ̂1(φ⋆)) + (J − 1)τi,1,Jρ⋆ . (9)"
    }, {
      "heading" : "5.2.2 Regret in the exploitation phase",
      "text" : "By definition, all models in Gi ∪ {φ⋆} are selected before any model in Bi is selected. The good models. Let us consider some φ ∈ Gi and an event Ωi under which the exploitation phase does not reset. The test (equation (4)) starts after τi,1,J , thus, since there is not reset, either τi,2(φ) = τi,1,J in which case the contribution to the regret is bounded by τi,1,Jρ⋆ , or τi,2(φ) > τi,1,J , in which case the regret during the (τi,2(φ) − 1) steps (where the test was successful) is bounded by\n(τi,2(φ) − 1)(ρ⋆ − µ̂i,2,τi,2(φ)−1(φ)) 6 (τi,2(φ)− 1)(ρ⋆ − µ̂i,1(φ) + 2B(i, φ, δ)) 6 (τi,2(φ)− 1)(ρ⋆ − µ̂i,1(φ⋆) + 2B(i, φ⋆, δ)) ,\nand now since in the last step φ fails to pass the test, this adds a contribution to the regret at most ρ⋆.\nWe deduce that the total contribution to the regret of all the models φ ∈ Gi in the exploitation stages on the event Ωi is bounded by\n∆i,2(Gi) 6 ∑\nφ∈G\nmax{τi,1,Jρ⋆, (τi,2(φ) − 1)(ρ⋆ − µ̂i,1(φ⋆) + 2B(i, φ⋆, δ)) + ρ⋆} . (10)\nThe true model. First, let us note that since the total regret of the true model during the exploitation step i is given by\nτi,2(φ ⋆)(ρ⋆ − µ̂i,2,t(φ⋆)) ,\nthen the total regret of the exploration and exploitation stages in episode i on Ωi is bounded by\n∆i 6 τi,1,J (ρ ⋆ − µ̂1(φ ⋆)) + τi,1,J (J − 1)ρ ⋆ + τi,2(φ ⋆)(ρ⋆ − µ̂i,2,ti+τi,2(φ ⋆)) +\n∑\nφ∈Gi\nmax{τi,1,Jρ ⋆ , (τi,2(φ)− 1)(ρ ⋆ − µ̂i,1(φ ⋆) + 2B(i, φ⋆, δ)) + ρ⋆}+\n∑\nφ∈Bi\nτi,2(φ)ρ ⋆ .\nNow from the analysis provided in [6] we know that when we run the UCRL2 with the true model φ⋆ with parameter δi(δ), then there exists an event Ω1,i of probability at least 1− δi(δ) such that on this event ρ⋆ − µ̂i,1(φ⋆) 6 BD(τi,1,J , φ⋆, δi(δ)) , and similarly there exists an event Ω2,i of probability at least 1− δi(δ), such that on this event ρ⋆ − µ̂i,2,t(φ⋆) 6 BD(τi,2(φ⋆), φ⋆, δ1(δ)) . Now we show that, with high probability, the true model φ⋆ passes all the tests (equation (4)) until the end of the episode i, and thus equivalently, with high probability no model φ ∈ Bi is selected, so that ∑\nφ∈Bi\nτi,2(φ) = 0.\nFor the true model, after τ(φ⋆, t) > τi,1,J , there remains at most (τi,2−τi,1,J+1) possible timesteps where we do the test for the true model φ⋆. For each test we need to control µi,2,t(φ⋆), and the event corresponding to µ̂i,1(φ⋆) is shared by all the tests. Thus we deduce that with probability higher than 1− (τi,2− τi,1,J +2)δi(δ) we have simultaneously on all time step until the end of exploitation phase of stage i,\nµ̂i,2,t(φ ⋆)− µ̂i,1(φ⋆) = µ̂i,2,t(φ⋆)− ρ⋆ + ρ⋆ − µ̂i,1(φ⋆)\n> −BD(τ(φ⋆, t), φ⋆, δi(δ))−BD(τi,1,J , φ⋆, δi(δ)) > −2BD(τi,1,J , φ⋆, δi(δ)) .\nNow provided that f(ti) > D, then BD(τi,1,J , φ⋆, δi(δ)) 6 B(i, φ⋆, δ) , thus the true model passes all tests until the end of the exploitation part of stage i on an event Ω3,i of probability higher than 1 − (τi,2 − τi,1,J + 2)δi(δ). Since there is no reset, we can choose Ωi def= Ω3,i. Note that on this event, we thus have ∑\nφ∈Bi\nτi,2(φ) = 0.\nBy using a union bound over the events Ω1,i,Ω2,i and Ω3,i, then we deduce that with probability higher than 1− (τi,2 − τi,1,J + 4)δi(δ), ∆i 6 τi,1,JBD(τi,1,J , φ\n⋆, δi(δ))) + [τi,1,J(J − 1) + |Gi|]ρ⋆ + τi,2(φ⋆)BD(τi,2(φ⋆), φ⋆, δi(δ)) + ∑\nφ∈Gi\nmax{(τi,1,J − 1)ρ⋆, (τi,2(φ)− 1)(BD(τi,1,J , φ⋆, δi(δ)) + 2B(i, φ⋆, δ)} .\nNow using again the fact that f(ti) > D, and after some simplifications, we deduce that\n∆i 6 τi,1,JBD(τi,1,J , φ ⋆, δi(δ)) + τi,2(φ ⋆)BD(τi,2(φ ⋆), φ⋆, δi(δ))\n+ ∑\nφ∈Gi\n(τi,2(φ) − 1)3B(i, φ⋆, δ) + τi,1,J (J + |Gi| − 1)ρ⋆ .\nFinally, we use the fact that τBD(τ, φ⋆, δi(δ)) is increasing with τ to deduce the following rough bound that holds with probability higher than 1− (τi,2 − τi,1,J + 4)δi(δ)\n∆i 6 τi,2B(i, φ ⋆, δ) + τi,2BD(τi,2, φ ⋆, δi(δ)) + 2Jτi,1,Jρ ⋆ ,\nwhere we used the fact that τi,2 = τi,2(φ⋆) + ∑\nφ∈G\nτi,2(φ) ."
    }, {
      "heading" : "5.3 Tuning the parameters of each stage.",
      "text" : "We now conclude by tuning the parameters of each stage, i.e. the probabilities δi(δ) and the length τi, τi,1 and τi,2. The total length of stage i is by definition\nτi = τi,1 + τi,2 = τi,1,JJ + τi,2 ,\nwhere τi = 2i . So we set τi,1 def = τ 2/3 i and then we have τi,2 def = τi − τ2/3i and τi,1,J =\nτ 2/3 i\nJ . Now using these values and the definition of the bound B(i, φ⋆, δ), and BD(τi,2, φ⋆, δi(δ)), we deduce with probability higher than 1− (τi,2 − τi,1,J + 4)δi(δ) the following upper bound\n∆i 6 34f(ti)S\n√\nAJ log ( τ2/3i Jδi(δ) ) τ 2/3 i + 34DS √ A log ( τi δi(δ) ) τi + 2τ 2/3 i ρ ⋆ ,\nwith ti = 2i − 1 + 22i/3 and where we used the fact that ( J\nτ 2/3 i\n)1/2 τi,2 6 √ Jτ 2/3 i .\nWe now define δi(δ) such that δi(δ) def = (2i − (J−1 + 1)22i/3 + 4)−12−i+1δ . Since for the stages i ∈ I0 def= {i > 1; f(ti) < D}, the regret is bounded by ∆i 6 τiρ⋆, then the total cumulative regret of the algorithm is bounded with probability higher than 1 − δ (using the defition of the δi(δ)) by\n∆(T ) 6 ∑\ni/∈I0\n[34f(ti)S\n√ JA log\n(28i/3 Jδ ) + 2]22i/3 + 34DS\n√ A log\n(23i δ ) 2i + ∑\ni∈I0\n2iρ⋆ .\nwhere ti = 2i − 1 + 22i/3 6 T . We conclude by using the fact that since I(T ) 6 log2(T + 1), then with probability higher than 1− δ, the following bound on the regret holds ∆(T ) 6 cf(T )S ( AJ log(Jδ)−1 log2(T ) )1/2 T 2/3 + c′DS ( A log(δ−1) log2(T )T )1/2 + c(f,D) .\nfor some constant c, c′, and where c(f,D) = ∑\ni∈I0 2iρ⋆. Now for the special choice when f(T )\ndef =\nlog2(T+1), then i ∈ I0 means 2i+22i/3 < 2D+2, thus we must have i < D, and thus c(f, d) 6 2D."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research was partially supported by the French Ministry of Higher Education and Research, Nord- Pas-de-Calais Regional Council and FEDER through CPER 2007-2013, ANR projects EXPLO-RA (ANR-08-COSI-004) and Lampada (ANR-09-EMER-007), by the European Communitys Seventh Framework Programme (FP7/2007-2013) under grant agreement 231495 (project CompLACS), and by Pascal-2."
    } ],
    "references" : [ {
      "title" : "Finite time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Gambling in a rigged casino: The adversarial multi-armed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1995
    }, {
      "title" : "REGAl: a regularization based algorithm for reinforcement learning in weakly communicating mdps",
      "author" : [ "Peter L. Bartlett", "Ambuj Tewari" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "R-max - a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "Ronen I. Brafman", "Moshe Tennenholtz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Feature reinforcement learning: Part I: Unstructured MDPs",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Journal of Artificial General Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Thomas Jaksch", "Ronald Ortner", "Peter Auer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "Tze L. Lai", "Herbert Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1985
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "Herbert Robbins" ],
      "venue" : "Bulletin of the American Mathematics Society,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1952
    }, {
      "title" : "On the possibility of learning in reactive environments with arbitrary dependence",
      "author" : [ "Daniil Ryabko", "Marcus Hutter" ],
      "venue" : "Theoretical Compututer Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "PAC model-free reinforcement learning",
      "author" : [ "Alexander L. Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L. Littman" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Optimistic linear programming gives logarithmic regret for irreducible mdps",
      "author" : [ "Ambuj Tewari", "Peter L. Bartlett" ],
      "venue" : "In Proceedings of Neural Information Processing Systems Conference (NIPS),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Namely, we use in our algorithm as a subroutine the algorithm UCRL2 of [6] that is designed to provide finite time bounds for undiscounted MDPs.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "The diameter D of a MDP is defined in [6] as the expected minimum time required to reach any state starting from any other state.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "A related result is reported in [3], which improves on constants related to the characteristics of the MDP.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "A similar approach has been considered in [10]; the difference is that in that work the probabilistic characteristics of each model are completely known, but the models are not assumed to be Markovian, and belong to a countably infinite (rather than finite) set.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "[9, 8, 1]): there are finitely many “arms”, corresponding to the policies used in each model, and one of the arms is the best, in the sense that the corresponding model is the “true” one.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "[9, 8, 1]): there are finitely many “arms”, corresponding to the policies used in each model, and one of the arms is the best, in the sense that the corresponding model is the “true” one.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "[9, 8, 1]): there are finitely many “arms”, corresponding to the policies used in each model, and one of the arms is the best, in the sense that the corresponding model is the “true” one.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "Therefore, the optimal rate of rewards can be obtained by a clever exploration/exploitation strategy, such as UCRL2 algorithm [6].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Then we present the proposed algorithm in Section 3; it uses UCRL2 of [6] as a subroutine and selects the models φ according to a penalized empirical criterion.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Moreover, we assume that A is of finite cardinality A def = |A| and that 0 ∈ R ⊂ [0, 1].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "For that purpose we define the regret of any strategy at time T , like in [6, 3], as",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "For that purpose we define the regret of any strategy at time T , like in [6, 3], as",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Another approach not discussed here would be to try to build a good state representation function, as what is suggested for instance in [5].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "The reader familiar with adversarial bandit literature will notice that our bound of order T 2/3 is worse than T 1/2 that usually appears in this context (see, for example [2]).",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "First, we notice that, as reported in [6], when we compute an optimistic model based on the empirical rewards and transitions of the true model, the span of the corresponding optimistic value function sp(V̂ ) is always smaller than the diameter D.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "In [3], the authors derive a regret bound that scales with the span of the true value function sp(V ), which is also less than D, and can be significantly smaller in some cases.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "1 Upper and Lower confidence bounds From the analysis of UCRL2 in [6], we have the property that with probability higher than 1 − δ, the regret of UCRL2 when run for τ consecutive many steps from time t1 in the true model φ is upper bounded by ρ − 1 τ t1+τ−1 ∑",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Now from the analysis provided in [6] we know that when we run the UCRL2 with the true model φ with parameter δi(δ), then there exists an event Ω1,i of probability at least 1− δi(δ) such that on this event ρ − μ̂i,1(φ) 6 BD(τi,1,J , φ, δi(δ)) , and similarly there exists an event Ω2,i of probability at least 1− δi(δ), such that on this event ρ − μ̂i,2,t(φ) 6 BD(τi,2(φ), φ, δ1(δ)) .",
      "startOffset" : 34,
      "endOffset" : 37
    } ],
    "year" : 2013,
    "abstractText" : "The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a finite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T 2/3 where T is the horizon time.",
    "creator" : "LaTeX with hyperref package"
  }
}