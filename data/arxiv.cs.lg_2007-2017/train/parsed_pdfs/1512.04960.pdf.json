{
  "name" : "1512.04960.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Light Touch for Heavily Constrained SGD",
    "authors" : [ "Andrew Cotter", "Maya Gupta", "Jan Pfeifer" ],
    "emails" : [ "acotter@google.com", "mayagupta@google.com", "janpf@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n04 96\n0v 2\n[ cs\n.L G\n] 2\n4 O\nct 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many machine learning problems can benefit from the addition of constraints. For example, one can learn monotonic functions by adding appropriate constraints to ensure or encourage positive derivatives everywhere [e.g. Archer and Wang, 1993, Sill, 1998, Spouge et al., 2003, Daniels and Velikova, 2010, Gupta et al., 2016]. Submodular functions can often be learned from noisy examples by imposing constraints to ensure submodularity holds. Another example occurs when one wishes to guarantee that a classifier will correctly label certain “canonical” examples, which can be enforced by constraining the function values on those examples. See Qu and Hu [2011] for some other examples of constraints useful in machine learning.\nHowever, these practical uses of constraints in machine learning are impractical in that the number of constraints may be very large, and scale poorly with the number of features d or number of training samples n. In this paper we propose a new strategy for tackling such heavily-constrained problems, with guarantees and compelling convergence rates for large-scale convex problems.\nA standard approach for large-scale empirical risk minimization is projected stochastic gradient descent [e.g. Zinkevich, 2003, Nemirovski et al., 2009]. Each SGD iteration is computationally cheap, and the algorithm converges quickly to a solution good enough for machine learning needs. However, this algorithm requires a projection onto the feasible region after each stochastic gradient step, which can be prohibitively slow if there are many non-trivial constraints, and is not easy to parallelize. Recently, Frank-Wolfe-style algorithms [e.g. Hazan and Kale, 2012, Jaggi, 2013] have been proposed that remove the projection, but require a constrained linear optimization at each iteration.\nWe propose a new strategy for large-scale constrained optimization that, like Mahdavi et al. [2012], moves the constraints into the objective and finds an approximate solution of the resulting unconstrained problem, projecting the (potentially-infeasible) result onto the constraints only once, at the end. Their work focused on handling only one constraint, but as they noted, multiple constraints g1(x) ≤ 0, g2(x) ≤ 0, . . . , gm(x) ≤ 0 can be reduced to one constraint by replacing the m constraints with their maximum: maxi gi(x) ≤ 0. However, this still requires that all m\nThis version was also presented at the 29th Conference on Learning Theory (COLT 2016).\nconstraints be checked at every iteration. In this paper, we focus on the computational complexity as a function of the number of constraints m, and show that it is possible to achieve good convergence rates without checking constraints so often.\nThe key challenge to handling a large number of constraints is determining which constraints are active at the optimum of the constrained problem, which is likely to be only a small fraction of the total constraint set. For example, for linear inequality constraints on a d-dimensional problem, no more than d of the constraints will be active at the optimum, and furthermore, once the active constraints are known, the problem reduces to solving the unconstrained problem that results from projecting onto them, which is typically vastly easier.\nTo identify and focus on the important constraints, we propose learning a probability distribution over them constraints that concentrates on the most-violated, and sampling constraints from this evolving distribution at each iteration. We call this approach LightTouch because at each iteration only a few constraints are checked, and the solution is only nudged toward the feasible set. LightTouch is suitable for convex problems, but we also propose a variant, MidTouch, that enjoys a superior convergence rate on strongly convex problems. These two algorithms are introduced and analyzed in Section 3.\nOur proposed strategy removes the per-iteration m-dependence on the number of constraint evaluations. LightTouch and MidTouch do need more iterations to converge, but each iteration is faster, resulting in a net performance improvement. To be precise, we show that the total number of constraint checks required to achieve ǫ-suboptimality when optimizing a non-strongly convex objective decreases from O(m/ǫ2) to Õ((lnm)/ǫ2+m(lnm)3/2/ǫ3/2)—notice that the m-dependence of the dominant (in ǫ) term has decreased from m to lnm. For a λ-strongly convex objective, the dominant (again in ǫ) term in our bound on the number of constraint checks decreases from O(m/λ2ǫ) to Õ((lnm)/λ2ǫ), but like the non-strongly convex result this bound contains lower-order terms with worse m-dependencies. A more careful comparison of the performance of our algorithms can be found in Section 4.\nWhile they check fewer than m constraints per iteration, these algorithms do need to pay a O(m) per-iteration arithmetic cost. When each constraint is expensive to check, this cost can be neglected. However, when the constraints are simple to check (e.g. box constraints, or the lattice monotonicity constraints considered in our experiments), it can be partially addressed by transforming the problem into an equivalent one with fewer more costly constraints. This, as well as other practical considerations, are discussed in Section 5.\nExperiments on a large-scale real-world heavily-constrained ranking problem show that our proposed approach works well in practice. This problem was too large for a projected SGD implementation using an off-the-shelf quadratic programming solver to perform projections, but was amenable to an approach based on a fast approximate projection routine tailored to this particular constraint set. Measured in terms of runtime, however, LightTouch was still significantly faster. Each constraint in this problem is trivial, requiring only a single comparison operation to check, so the aforementioned O(m) arithmetic cost of LightTouch is a significant issue. Despite this, LightTouch was roughly as fast as the Mahdavi et al. [2012]-like algorithm FullTouch. In light of other experiments showing that LightTouch checks dramatically fewer constraints in total than FullTouch, we believe that LightTouch is well-suited to machine learning problems with many nontrivial constraints."
    }, {
      "heading" : "2 Heavily Constrained SGD",
      "text" : "Consider the constrained optimization problem:\nmin w∈W f (w) (1)\ns.t. gi (w) ≤ 0 ∀i ∈ {1, . . . ,m} ,\nwhere W ⊆ Rd is bounded, closed and convex, and f : W → R and all gi : W → R are convex (our notation is summarized in Table 1). We assume that W is a simple object, e.g. an ℓ2 ball, onto which it is inexpensive to project, and that the “trickier” aspects of the domain are specified via the constraints gi(w) ≤ 0. Notice that we\nconsider constraints written in terms of arbitrary convex functions, and are not restricted to e.g. only linear or quadratic constraints."
    }, {
      "heading" : "2.1 FullTouch: A Relaxation with a Feasible Minimizer",
      "text" : "We build on the approach of Mahdavi et al. [2012] to relax Equation 1. Defining g(w) = maxi gi(w) and introducing a Lagrange multiplier α yields the equivalent optimization problem:\nmax α≥0 min w∈W f (w) + αg (w) . (2)\nDirectly optimizing over w and α is problematic because the optimal value for α is infinite for any w that violates a constraint. Instead, we follow Mahdavi et al. [2012, Section 4.2] in relaxing the problem by adding an upper bound of γ on α, and using the fact that max0≤α≤γ αg(w) = γmax(0, g(w)).\nIn the following lemma, we show that, with the proper choice of γ, any minimizer of this relaxed objective is a feasible solution of Equation 1, indicating that using stochastic gradient descent (SGD) to minimize the relaxation (h(w) in the lemma below) will be effective.\nLemma 1. Suppose that f is Lf -Lipschitz, i.e. |f(w)− f(w′)| ≤ Lf ‖w − w′‖2 for all w,w′ ∈ W , and that there is a constant ρ > 0 such that if g(w) = 0 then ∥ ∥∇̌ ∥ ∥\n2 ≥ ρ for all ∇̌ ∈ ∂g(w), where ∂g(w) is the subdifferential of g(w).\nFor a parameter γ > 0, define: h (w) = f (w) + γmax {0, g (w)} .\nAlgorithm 1 (FullTouch) Minimizes f on W subject to the single constraint g(w) ≤ 0. For problems with m constraints gi(w) ≤ 0, let g(w) = maxi gi(w), in which case differentiatingmax{0, g(w)} (line 4) requires evaluating all m constraints. This algorithm—our starting point—is similar to those proposed by Mahdavi et al. [2012], and like their algorithms only contains a single projection, at the end, projecting the potentially-infeasible result vector w̄.\nHyperparameters: T , η 1 Initialize w(1) ∈ W arbitrarily 2 For t = 1 to T : 3 Sample ∆̌(t) // stochastic subgradient of f(w(t)) 4 Let ∆̌(t)w = ∆̌(t) + γ∇̌max{0, g(w(t))} 5 Update w(t+1) = Πw(w(t) − η∆̌(t)w ) // Πw projects its argument onto W w.r.t. ‖·‖2 6 Average w̄ = (\n∑T t=1 w (t))/T 7 Return Πg(w̄) // optional if small constraint violations are acceptable\nIf γ > Lf/ρ, then for any infeasible w (i.e. for which g(w) > 0):\nh (w) > h (Πg (w)) = f (Πg (w)) and ‖w −Πg (w)‖2 ≤ h (w)− h (Πg (w))\nγρ− Lf ,\nwhere Πg (w) is the projection of w onto the set {w ∈ W : g(w) ≤ 0} w.r.t. the Euclidean norm.\nProof. In Appendix C.\nThe strategy of applying SGD to h(w), detailed in Algorithm 1, which we call FullTouch, has the same “flavor” as the algorithms proposed by Mahdavi et al. [2012], and we use it as a baseline comparison point for our other algorithms.\nApplication of a standard SGD bound to FullTouch shows that it converges at a rate with no explicit dependence on the number of constraints m, measured in terms of the number of iterations required to achieve some desired suboptimality (see Appendix C.1), although the γ parameter can introduce an implicit d or m-dependence, depending on the constraints (discussed in Section 2.2). The main drawback of FullTouch is that each iteration is expensive, requiring the evaluation of all m constraints, since differentiation of g requires first identifying the most-violated. This is the key issue we tackle with the LightTouch algorithm proposed in Section 3."
    }, {
      "heading" : "2.2 Constraint-Dependence of γ",
      "text" : "The conditions on Lemma 1 were stated in terms of g, instead of the individual gis, because it is difficult to provide suitable conditions on the “component” constraints without accounting for their interactions.\nFor a point w where two or more constraints intersect, the subdifferential of g(w) consists of all convex combinations of subgradients of the intersecting constraints, with the consequence that even if each of the subgradients of the gi(w)s has norm at least ρ′, subgradients of g(w) will generally have norms smaller than ρ′. Exactly how much smaller depends on the particular constraints under consideration. We illustrate this phenomenon with the following examples, but note that, in practice, γ should be chosen experimentally for any particular problem, so the question of the d and m-dependence of γ is mostly of theoretical interest.\nBox Constraints Consider the m = 2d box constraints gi(w) = −wi − 1 and gi+d(w) = wi − 1, all of which have gradients of norm 1. At most d constraints can intersect (at a corner of the [−1, 1]d box), all of which are mutually orthogonal, so the norm of any convex combination of their gradients is lower bounded by that of their average, ρ = 1/ √ d. Hence, one should choose γ > √ dLf .\nAs in the above example, γ ∝ √\nmin(m, d) will suffice when the subgradients of intersecting constraints are at least orthogonal, and γ can be smaller if they always have positive inner products. However, if subgradients of intersecting constraints tend to point in opposing directions, then γ may need to be much larger, as in our next example:\nOrdering Constraints Suppose the m = d − 1 constraints order the components of w as w1 ≤ w2 ≤ · · · ≤ wd, for which gi(w) = (wi − wi+1)/ √ 2, gradients of which again have norm 1. All of these constraints may be active\nsimultaneously, in which case there is widespread cancellation in the average gradient (e1 − ed)/(m √ 2), where ei is the ith standard unit basis vector. The norm of this average gradient is ρ = 1/m, so we should choose γ > (d− 1)Lf . In light of this example, one begins to wonder if a suitable γ will necessarily exist—fortunately, the convexity of g enables us to prove a trivial bound as long as g(v) is strictly negative for some v ∈ W : Lemma 2. Suppose that there exists a v ∈ W for which g(v) < 0, and let Dw ≥ supw,w′∈W ‖w − w′‖2 bound the diameter of W . Then ρ = −g(v)/Dw satisfies the conditions of Lemma 1.\nProof. Let w ∈ W be a point for which g(w) = 0, and ∇̌ ∈ ∂g(w) an arbitrary subgradient. By convexity, g(v) ≥ g(w) + 〈 v − w, ∇̌ 〉 . The Cauchy-Schwarz inequality then gives that:\ng(v) ≥ −‖v − w‖2 ∥ ∥∇̌ ∥ ∥ 2 ,\nfrom which the claim follows immediately.\nLinear Constraints Consider the constraints Aw b, with each row of A having unit norm, bmin = mini bi > 0, and W being the ℓ2 ball of radius r. It follows from Lemma 2 that γ > (2r/bmin)Lf suffices. Notice that the earlier box constraint example satisfies these assumptions (with bmin = 1 and r = √ d).\nAs the above examples illustrate, subgradients of g will be large at the boundary if subgradients of the gis are large, and the constraints intersect at sufficiently shallow angles that, representing boundary subgradients of g as convex combinations of subgradients of the gis, the components reinforce each other, or at least do not cancel too much. This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al. [2015]."
    }, {
      "heading" : "3 A Light Touch",
      "text" : "This section presents the main contribution of this paper: an algorithm that stochastically samples a small subset of the m constraints at each SGD iteration, updates the parameters based on the subgradients of the sampled constraints, and carefully learns the distribution over the constraints to produce a net performance gain.\nWe first motivate the approach by considering an oracle, then explain the algorithm and present convergence results for the convex (Section 3.2) and strongly convex (Section 3.3) cases."
    }, {
      "heading" : "3.1 Wanted: An Oracle For the Most Violated Constraint",
      "text" : "Because FullTouch only needs to differentiate the most violated constraint at each iteration, it follows that if one had access to an oracle that identified the most-violated constraint, then the overall convergence rate (including the cost of each iteration) could only depend on m through γ. This motivates us to learn to predict the most-violated constraint, ideally at a significantly better than linear-in-m rate.\nTo this end, we further relax the problem of minimizing h(w) (defined in Lemma 1) by replacing γmax(0, g(w)) with maximization over a probability distribution (as in Clarkson et al. [2010]), yielding the equivalent convex-linear\nAlgorithm 2 (LightTouch) Minimizes f on W subject to the constraints gi(w) ≤ 0 for i ∈ {1, . . . ,m}. The algorithm learns an auxiliary probability distribution p (lines 9–13) estimating how likely it is that each constraint is the mostviolated. We assume that k ≤ m: if k > m, then the user is willing to check m constraints per iteration anyway, so FullTouch is the better choice. Like FullTouch, this algorithm finds a potentially-infeasible solution w̄ which is only projected onto the feasible region at the end. Notice that while the p-update checks only k constraints, it does require O(m) arithmetic operations. This issue is discussed further in Section 5.1.\nHyperparameters: T , η, k 1 Initialize w(1) ∈ W arbitrarily 2 Initialize p(1) ∈ ∆m to the uniform distribution 3 Initialize µ(1)j = max{0, gj(w(1))} // 0 if w(1) is feasible 4 For t = 1 to T : 5 Sample ∆̌(t) // stochastic subgradient of f(w(t)) 6 Sample i(t) ∼ p(t) 7 Let ∆̌(t)w = ∆̌(t) + γ∇̌max{0, gi(t)(w(t))} 8 Update w(t+1) = Πw(w(t) − η∆̌(t)w ) // Πw projects its argument onto W w.r.t. ‖·‖2 9 Sample S(t) ⊆ {1, . . . ,m} with ∣ ∣S(t) ∣ ∣ = k uniformly without replacement\n10 Let ∆̂(t)p = γµ(t) + (γm/k) ∑ j∈S(t) ej(max{0, gj(w(t))} − µ (t) j ) 11 Let µ(t+1)j = max{0, gj(w(t))} if j ∈ S(t), otherwise µ (t+1) j = µ (t) j 12 Update p̃(t+1) = exp(ln p(t) + η∆̂(t)p ) // element-wise exp and ln 13 Project p(t+1) = p̃(t+1)/ ∥ ∥p̃(t+1) ∥ ∥\n1\n14 Average w̄ = ( ∑T t=1 w (t))/T 15 Return Πg(w̄) // optional if small constraint violations are acceptable\noptimization problem:\nmax p∈∆m min w∈W h̃ (w, p) (3)\nwhere h̃ (w, p) = f (w) + γ\nm ∑\ni=1\npimax {0, gi (w)} .\nHere, ∆m is the m-dimensional simplex. We propose optimizing over w and p jointly, thereby learning the mostviolated constraint, represented by the multinoulli distribution p over constraint indices, at the same time as we optimize over w."
    }, {
      "heading" : "3.2 LightTouch: Stochastic Constraint Handling",
      "text" : "To optimize Equation 3, our proposed algorithm (Algorithm 2, LightTouch) iteratively samples stochastic gradients ∆̌ (t) w w.r.t. w and ∆̂ (t) p w.r.t. p of h̃(w, p), and then takes an SGD step on w and a multiplicative step on p:\nw(t+1) = Πw\n( w(t) − η∆̌(t)w )\nand p(t+1) = Πp ( exp ( ln p(t) + η∆̂(t)p )) ,\nwhere the exp and ln of the p-update are performed element-wise, Πw projects onto W w.r.t. the Euclidean norm, and Πp onto ∆m via normalization (i.e. dividing its parameter by its sum).\nThe key to getting a good convergence rate for this algorithm is to choose ∆̌w and ∆̂p such that they are both inexpensive to compute, and tend to have small norms. For ∆̌w, this can be accomplished straightforwardly, by sampling a constraint index i according to p, and taking:\n∆̌w = ∆̌ + γ∇̌max {0, gi (w)} ,\nwhere ∆̌ is a stochastic subgradient of f and ∇̌max(0, gi(w)) is a subgradient of max(0, gi(w)). Calculating each such ∆̌w requires differentiating only one constraint, and it is easy to verify that ∆̌w is a subgradient of h̃ w.r.t. w in expectation over ∆̌ and i. Taking Gf to be a bound on the norm of ∆̌ and Gg on the norms of subgradients of the gis shows that ∆̌w’s norm is bounded by Gf + γGg.\nFor ∆̂p, some care must be taken. Simply sampling a constraint index j uniformly and defining:\n∆̂p = γmej max {0, gj (w)} ,\nwhere ej is the jth m-dimensional standard unit basis vector, does produce a ∆̂p that in expectation is the gradient of h̃ w.r.t. p, but it has a norm bound proportional to m. Such potentially large stochastic gradients would result in the number of iterations required to achieve some target suboptimality being proportional to m2 in our final bound.\nA typical approach to reducing the variance (and hence the expected magnitude) of ∆̂p is minibatching: instead of sampling a single constraint index j at every iteration, we could instead sample a subset S of size |S| = k without replacement, and use:\n∆̂p = γm\nk\n∑ j∈S ej max {0, gj (w)} .\nThis is effective, but not enough, because reducing the variance by a factor of k via minibatching requires that we check k times more constraints. For this reason, in addition to minibatching, we center the stochastic gradients, as is done by the well-known SVRG algorithm [Johnson and Zhang, 2013], by storing a gradient estimate γµ with µ ∈ Rm, at each iteration sampling a set S of size |S| = k uniformly without replacement, and computing:\n∆̂p = γµ+ γm\nk\n∑ j∈S ej (max {0, gj(w)} − µj) . (4)\nWe then update the jth coordinate of µ to be µj = max {0, gj(w)} for every j ∈ S. The norms of the resulting stochastic gradients will be small if γµ is a good estimate of the gradient, i.e. µj ≈ max(0, gj(w)). The difference between µj and max(0, gj(w)) can be bounded in terms of how many consecutive iterations may have elapsed since µj was last updated. It turns out (see Lemma 4 in Appendix C.2) that this quantity can be bounded uniformly by O((m/k) ln(mT )) with high probability, which implies that if the gis are Lg-Lipschitz, then |gj(w)− µj | ≤ Lgη(Gf + γGg)O((m/k) ln(mT )), since at most O((m/k) ln(mT )) updates of magnitude η(Gf + γGg) may have occurred since µj was last updated. Choosing η ∝ 1/ √ T , as is standard, moves this portion\n(the “variance portion”) of the ∆̂p-dependence out of the dominant O(1/ √ T ) term and into a subordinate term in our final bound.\nThe remainder of the ∆̂p-dependence (the “mean portion”) depends on the norm of E[∆̂p] = γ ∑\nj ej max(0, gj(w)). It is here that our use of multiplicative p-updates becomes significant, because with such updates the relevant norm is the ℓ∞ norm, instead of e.g. the ℓ2 norm (as would be the case if we updated p using SGD), thus we can bound ∥ ∥\n∥E[∆̂p] ∥ ∥ ∥\n∞ with no explicit m-dependence.\nThe following theorem on the convergence rate of LightTouch is proved by applying a mirror descent bound for saddle point problems while bounding the stochastic gradient norms as described above.\nTheorem 1. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Dw ≥ max{1, ‖w − w′‖2} as a bound on the diameter of W (notice that we also choose Dw to be at least 1), Gf ≥ ∥ ∥∆̌(t) ∥ ∥\n2 and Gg ≥\n∥ ∥∇̌max(0, gi(w)) ∥ ∥\n2 as uniform upper bounds on the (stochastic) gradient magnitudes\nof f and the gis, respectively, for all i ∈ {1, . . . ,m} and w,w′ ∈ W . We also assume that all gis are Lg-Lipschitz w.r.t. ‖·‖2, i.e. |gi(w)− gi(w′)| ≤ Lg ‖w − w′‖2. Our result will be expressed in terms of a total iteration count Tǫ satisfying:\nTǫ = O\n(\n(lnm)D2w (Gf + γGg + γLgDw) 2 ln 1δ\nǫ2\n)\n.\nDefine:\nk =\n\n  \nm (1 + lnm) 3/4\n√ 1 + ln 1δ √ 1 + lnTǫ\nT 1/4 ǫ\n\n  \n.\nIf k ≤ m, then we optimize Equation 1 using Tǫ iterations of Algorithm 2 (LightTouch), basing the stochastic gradients w.r.t. p on k constraints at each iteration, and using the step size:\nη =\n√ 1 + lnmDw\n(Gf + γGg + γLgDw) √ Tǫ .\nIf k > m, then LightTouch would check more than m constraints per iteration anyway, so we instead use Tǫ iterations of Algorithm 1 (FullTouch) with the step size:\nη = Dw\n(Gf + γGg) √ Tǫ .\nIn either case, we perform Tǫ iterations, requiring a total of Cǫ “constraint checks” (evaluations or differentiations of a single gi):\nCǫ =Õ\n(\n(lnm)D2w (Gf + γGg + γLgDw) 2 ln 1δ\nǫ2\n+ m (lnm)\n3/2 D 3/2 w (Gf + γGg + γLgDw) 3/2 ( ln 1δ\n)5/4\nǫ3/2\n)\n.\nand with probability 1− δ:\nf (Πg (w̄))− f (w∗) ≤ h (w̄)− h (w∗) ≤ ǫ and ‖w̄ −Πg (w̄)‖2 ≤ ǫ\nγρ− Lf ,\nwhere w∗ ∈ {w ∈ W : ∀i.gi(w) ≤ 0} is an arbitrary constraint-satisfying reference vector.\nProof. In Appendix C.2.\nThe most important thing to notice about this theorem is that the dominant terms in the bounds on the number of iterations and number of constraint checks are roughly γ2 lnm times the usual 1/ǫ2 convergence rate for SGD on a non-strongly convex objective. The lower-order terms have a worse m-dependence, however, with the result that, as the desired suboptimality ǫ shrinks, the algorithm performs fewer constraint checks per iteration until ultimately (once ǫ is on the order of 1/m2) only a constant number are checked during each iteration."
    }, {
      "heading" : "3.3 MidTouch: Strong Convexity",
      "text" : "To this point, we have only required that the objective function f be convex. However, roughly the same approach also works when f is taken to be λ-strongly convex, although we have only succeeded in proving an in-expectation result, and the algorithm, Algorithm 3 (MidTouch), differs from LightTouch not only in that the w updates use a 1/λt step size, but also in being a two-phase algorithm, the first of which, like FullTouch, checks every constraint at each iteration, and the second of which, like LightTouch with k = 1, checks only two. The following theorem bounds the convergence rate if we perform T1 ≈ mτ2 iterations in the first phase and T2 ≈ τ3 in the second, where the parameter τ determines the total number of iterations performed:\nAlgorithm 3 (MidTouch) Minimizes a λ-strongly convex f on W subject to the constraints gi(w) ≤ 0 for i ∈ {1, . . . ,m}. The algorithm consists of two phases: the first T1 iterations proceed like FullTouch, with every constraint being checked; the final T2 iterations proceed like LightTouch, with only a constant number of constraints being checked during each iteration, and an auxiliary probability distribution p being learned along the way. Notice that while second-phase p-update checks only one constraint, it, like LightTouch, requires O(m) arithmetic operations. This issue is discussed further in Section 5.1.\nHyperparameters: T1, T2, η 1 // First phase 2 Initialize w(1) ∈ W arbitrarily 3 For t = 1 to T1: 4 Sample ∆̌(t) // stochastic subgradient of f(w(t)) 5 Let ∆̌(t)w = ∆̌(t) + γ∇̌max{0, g(w(t))} 6 Update w(t+1) = Πw(w(t) − (1/λt)∆̌(t)w ) // Πw projects its argument onto W w.r.t. ‖·‖2 7 // Second phase 8 Average w(T1+1) = (\n∑T1 t=1 w (t))/T1 // initialize second phase to result of first 9 Initialize p(T1+1) ∈ ∆m to the uniform distribution\n10 Initialize µ(T1+1)j = max{0, gj(w(T1+1))} 11 For t = T1 + 1 to T1 + T2: 12 Sample ∆̌(t) 13 Sample i(t) ∼ p(t) 14 Let ∆̌(t)w = ∆̌(t) + γ∇̌max{0, gi(t)(w(t))} 15 Update w(t+1) = Πw(w(t) − (1/λt)∆̌(t)w ) 16 Sample j(t) ∼ Unif{1, . . . ,m} 17 Let ∆̂(t)p = γµ(t) + γmej(t)(max{0, gj(t)(w(t))} − µ(t)j(t) ) 18 Let µ(t+1)k = µ (t) k if k 6= j(t), otherwise µ (t+1) j(t) = max{0, gj(t)(w(t))} 19 Update p̃(t+1) = exp(ln p(t) + η∆̂(t)p ) // element-wise exp and ln 20 Project p(t+1) = p̃(t+1)/ ∥ ∥p̃(t+1) ∥ ∥\n1\n21 Average w̄ = ( ∑T1+T2 t=T1+1 w(t))/T2 22 Return Πg(w̄) // optional if small constraint violations are acceptable\nTheorem 2. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Gf ≥ ∥ ∥∆̌(t) ∥ ∥\n2\nand Gg ≥ ∥ ∥∇̌max(0, gi(w)) ∥ ∥\n2 as uniform upper bounds on the (stochastic) gradient magnitudes of f and the gis,\nrespectively, for all i ∈ {1, . . . ,m}. We also assume that f is λ-strongly convex, and that all gis are Lg-Lipschitz w.r.t. ‖·‖2, i.e. |gi(w)− gi(w′)| ≤ Lg ‖w − w′‖2 for all w,w′ ∈ W . If we run Algorithm 3 (MidTouch) with the p-update step size η = λ/2γ2L2g for Tǫ1 iterations in the first phase and Tǫ2 in the second:\nTǫ1 =Õ\n(\nm (lnm) 2/3 (Gf + γGg + γLg) 4/3\nλ4/3ǫ2/3 +\nm2 (lnm) (Gf + γGg)\nλ √ ǫ\n)\n,\nTǫ2 =Õ\n(\n(lnm) (Gf + γGg + γLg) 2\nλ2ǫ +\nm3/2 (lnm) 3/2 (Gf + γGg) 3/2\nλ3/2ǫ3/4\n)\n,\nrequiring a total of Cǫ “constraint checks” (evaluations or differentiations of a single gi):\nCǫ =Õ\n(\n(lnm) (Gf + γGg + γLg) 2\nλ2ǫ +\nm 3/2 (lnm) 3/2 (Gf + γGg) 3/2\nλ3/2ǫ3/4\n+ m2 (lnm) 2/3 (Gf + γGg + γLg) 4/3\nλ4/3ǫ2/3 +\nm3 (lnm) (Gf + γGg)\nλ √ ǫ\n)\n,\nthen: E [\n‖Πg(w̄)− w∗‖22 ] ≤ E [ ‖w̄ − w∗‖22 ] ≤ ǫ,\nwhere w∗ = argmin{w∈W:∀i.gi(w)≤0} f(w) is the optimal constraint-satisfying reference vector.\nProof. In Appendix D.\nNotice that the above theorem bounds not the suboptimality of Πg(w̄), but rather its squared Euclidean distance from w∗, for which reason the denominator of the highest order term depends on λ2 rather than λ. Like Theorem 1 in the non-strongly convex case, the dominant terms above, both in terms of the total number of iterations and number of constraint checks, match the usual 1/ǫ convergence rate for unconstrained strongly-convex SGD with an additional γ2 lnm factor, while the lower-order terms have a worse m-dependence. As before, fewer constraint checks will be performed per iteration as ǫ shrinks, reaching a constant number (on average) once ǫ is on the order of 1/m6."
    }, {
      "heading" : "4 Theoretical Comparison",
      "text" : "Table 2 compares upper bounds on the convergence rates and per-iteration costs when applied to a convex (but not necessarily strongly convex) problem for LightTouch, FullTouch, projected SGD, the online Frank-Wolfe algorithm of Hazan and Kale [2012], and a Frank-Wolfe-like online algorithm for optimization over a polytope [Garber and Hazan, 2013]. The latter algorithm, which we refer to as LLO-FW, achieves convergence rates comparable to projected SGD, but uses a local linear oracle instead of a projection or full linear optimization. To simplify the presentation, the dependencies on Lg, Gf and Gg have been dropped—please refer to Theorems 1 and 2 and the cited references for the complete statements. Table 3 contains the same comparison (without online Frank-Wolfe) for λ-strongly convex problems.\nAt each iteration, all of these algorithms must find a stochastic subgradient of f . In addition, each iteration of LightTouch and MidTouch must perform O(m) arithmetic operations (for the m-dimensional vector operations used when updating p)—this issue will be discussed further in Section 5.1. However, projected SGD must project its iterate onto the constraints w.r.t. the Euclidean norm, online Frank-Wolfe must perform a linear optimization subject to the constraints, and LLO-FW must evaluate a local linear oracle, which amounts to essentially local linear optimization.\nLightTouch, MidTouch and FullTouch share the same γ-dependence, but the m-dependence of the convergence rate of LightTouch and MidTouch is logarithmically worse. The number of constraint evaluations, however, is better: in the non-strongly convex case, ignoring all but the m and ǫ dependencies, FullTouch will checkO(m/ǫ2) constraints, while LightTouch will check only Õ((lnm)/ǫ2+m/ǫ3/2), a significant improvement when ǫ is small. Hence, particularly for problems with many expensive-to-evaluate constraints, one would expect LightTouch to converge much more rapidly. Likewise, for λ-strongly convex optimization, the dominant (in ǫ) terms in the bounds on the number of constraint evaluations go as m/ǫ for FullTouch, and as (lnm)/ǫ for MidTouch, although the lower-order terms in the MidTouch bound are significantly more complex than in the non-strongly convex case (see Table 3 for full details).\nComparing with projected SGD, online Frank-Wolfe and LLO-FW is less straightforward, not only because we’re comparing upper bounds to upper bounds (with all of the uncertainty that this entails), but also because we must relate the value of γ to the cost of performing the required projection, constrained linear optimization or local linear oracle evaluation. We note, however, that for non-strongly convex optimization, the ǫ-dependence of the convergence rate bound is worse for online Frank-Wolfe (1/ǫ3) than for the other algorithms (1/ǫ2), and that unless the constraints have some special structure, performing a projection can be a very expensive operation.\nFor example, with general linear inequality constraints, each constraint check performed by LightTouch, MidTouch or FullTouch requires O(d) time, whereas each linear program optimized by online Frank-Wolfe could be solved in O(d2m) time [Nemirovski, 2004, Chapter 10.1], and each projection performed by SGD in O((dm)3/2) time [Goldfarb and Liu, 1991]. When the constraints are taken to be arbitrary convex functions, instead of linear functions, projections may be even more difficult.\nWe believe that in many cases γ2 will be roughly on the order of the dimension d, or number of constraints m, whichever is smaller, although it can be worse for difficult constraint sets (see Section 2.2). In practice, we have found that a surprisingly small γ—we use γ = 1 in our experiments (Section 6)—often suffices to result in convergence to a feasible solution. With this in mind, and in light of the fact that a fast projection, linear optimization, or local linear oracle evaluation may only be possible for particular constraint sets, we believe that our algorithms compare favorably with the alternatives.\nAlgorithm 4 (Practical LightTouch) Our proposed “practical” algorithm combining LightTouch and MidTouch, along with the changes discussed in Section 5.\nHyperparameters: T , ηw, ηp 1 Initialize w(1) ∈ W arbitrarily 2 Initialize p(1) ∈ ∆m to the uniform distribution 3 Initialize µ(1)j = max{0, gj(w(1))} // 0 if w(1) is feasible 4 For t = 1 to T : 5 Let η(t)w = ηw/t if f is strongly convex, ηw/ √ t otherwise 6 Set k(t)f , k (t) g and k (t) p as described in Section 5.2 7 Sample ∆̌(t)1 , . . . , ∆̌ (t)\nk (t) f\ni.i.d. // stochastic subgradients of f(w(t))\n8 Sample i(t)1 , . . . , i (t)\nk (t) g\n∼ p(t) i.i.d.\n9 Let ∆̌(t)w = (1/k (t) f )\n∑k (t) f\nj=1 ∆̌ (t) j + (γ/k (t) g ) ∑k(t)g j=1 ∇̌max{0, gi(t)j (w\n(t))} 10 Update w(t+1) = Πw(w(t) − η(t)w ∆̌(t)w ) // Πw projects its argument onto W w.r.t. ‖·‖2 11 Sample S(t) ⊆ {1, . . . ,m} with ∣ ∣S(t) ∣ ∣ = k (t) p uniformly without replacement 12 Let ∆̂(t)p = γµ(t) + (γm/k (t) p ) ∑ j∈S(t) ej(max{0, gj(w(t))} − µ (t) j ) 13 Let µ(t+1)j = max{0, gj(w(t))} if j ∈ S(t), otherwise µ (t+1) j = µ (t) j 14 Update p̃(t+1) = exp(ln p(t) + ηp∆̂ (t) p ) // element-wise exp and ln 15 Project p(t+1) = p̃(t+1)/ ∥ ∥p̃(t+1) ∥ ∥\n1\n16 Average w̄ = ( ∑T t=1 w (t))/T 17 Return Πg(w̄) // optional if small constraint violations are acceptable"
    }, {
      "heading" : "5 Practical Considerations",
      "text" : "Algorithms 2 and 3 were designed primarily to be easy to analyze, but in real world-applications we recommend making a few tweaks to improve performance. The first of these is trivial: using a decreasing w-update step size η (t) w = ηw/ √ t when optimizing a non-strongly convex objective, and η(t)w = ηw/t for a strongly-convex objective. In both cases we continue to use a constant p-update step size ηp. This change, as well as that described in Section 5.2, is included in Algorithm 4."
    }, {
      "heading" : "5.1 Constraint Aggregation",
      "text" : "A natural concern about Algorithms 2 and 3 is that O(m) arithmetic operations are performed per iteration, even when only a few constraints are checked. When each constraint is expensive, this is a minor issue, since this cost will be “drowned out” by that of checking the constraints. However, when the constraints are very cheap, and the O(m) arithmetic cost compares disfavorably with the cost of checking a handful of constraints, it can become a bottleneck.\nOur solution to this issue is simple: transform a problem with a large number of cheap constraints into one with a smaller number of more expensive constraints. To this end, we partition the constraint indices 1, . . . ,m into m̃ sets {Mi} of size at most ⌈m/m̃⌉, defining g̃i(w) = maxj∈Mi gj(w), and then apply LightTouch or MidTouch on the m̃ aggregated constraints g̃i(w) ≤ 0. This makes each constraint check ⌈m/m̃⌉ times more expensive, but reduces the dimension of p from m to m̃, shrinking the per-iteration arithmetic cost to O(m̃)."
    }, {
      "heading" : "5.2 Automatic Minibatching",
      "text" : "Because LightTouch takes a minibatch size k as a parameter, and the constants from which we derive the recommended choice of k (Theorem 1) are often unknown, a user is in the uncomfortable position of having to perform a parameter search not only over the step sizes ηw and ηp, but also the minibatch size. Furthermore, the fact that the theoreticallyrecommended k is a decreasing function of T indicates that it might be better to check more constraints in early iterations, and fewer in later ones. Likewise, MidTouch is structured as a two-phase algorithm, in which every iteration checks every constraint in the first phase, and only a constant number in the second, but it seems more sensible for the number of constraint checks to decrease gradually over time.\nIn addition, for both algorithms, it would be desirable to support separate minibatching of the loss and constraint stochastic subgradients (w.r.t. w), in which case there would be three minibatching parameters to determine: kf , kg and kp. This makes things even harder for the user, since now there are three additional parameters that must be specified.\nTo remove the need to specify any minibatch-size hyperparameters, and to enable the minibatch sizes to change from iteration-to-iteration, we propose a heuristic that will automatically determine the minibatch sizes k(t)f , k (t) g and k (t) p for each of the stochastic gradient components at each iteration. Intuitively, we want to choose minibatch sizes in such a way that the stochastic gradients are both cheap to compute and have low variance. Our proposed heuristic does this by trading-off the computational cost and “bound impact” of the overall stochastic gradient, where the “bound impact” is a variance-like quantity that approximates the impact that taking a step with particular minibatch sizes has on the relevant convergence rate bound.\nSuppose that we’re about to perform the tth iteration, and know that a single stochastic subgradient ∆̌ of f(w) (corresponding to the loss portion of ∆̌w) has variance (more properly, covariance matrix trace) v̄ (t) f and requires a computational investment of c̄(t)f units. Similarly, if we define ∆̌g by sampling i ∼ p and taking ∆̌g = γ∇̌max{0, gi(w)} (corresponding to the constraint portion of ∆̌w), then we can define variance and cost estimates of ∆̌g to be v̄ (t) g and c̄ (t) g , respectively. Likewise, we take v̄ (t) p and c̄ (t) p to be estimates of the variance and cost of a (non-minibatched version of) ∆̂p.\nIn all three cases, the variance and cost estimates are those of a single sample, implying that a stochastic subgradient of f(w) averaged over a minibatch of size k(t)f will have variance v̄ (t) f /k (t) f and require a computational investment of c̄ (t) f k (t) f , and likewise for the constraints and distribution. In the context of Algorithm 4, with minibatch sizes of k (t) f , k (t) g and k (t) p , we define the overall bound impact b and computational cost c of a single update as:\nb = η (t) w v̄ (t) f\nk (t) f\n+ η (t) w v̄ (t) g\nk (t) g\n+ ηpv̄\n(t) p\nk (t) p\nand c = c̄(t)f k (t) f + c̄ (t) g k (t) g + c̄ (t) p k (t) p .\nWe should emphasize that the above definition of b is merely a useful approximation of how these quantities truly affect our bounds.\nGiven the three variance and three cost estimates, we choose minibatch sizes in such a way as to minimize both the computational cost and bound impact of an update. Imagine that we are given a fixed computational budget c. Then our goal will be to choose the minibatch sizes in such a way that b is minimized for this budget, a problem that is easily solved in closed form:\n[\nk (t) f , k (t) g , k (t) p\n]\n∝\n\n\n√ √ √ √ η (t) w v̄ (t) f\nc̄ (t) f\n,\n√ √ √ √ η (t) w v̄ (t) g\nc̄ (t) g\n,\n√ √ √ √ ηpv̄ (t) p\nc̄ (t) p\n\n .\nWe propose choosing the proportionality constant (and thereby the cost budget c) in such a way that k(t)f = 2 (enabling us to calculate sample variances, as explained below), and round the two other sizes to the nearest integers, lowerbounding each so that k(t)g ≥ 2 and k(t)p ≥ 1.\nWhile the variances and costs are not truly known during optimization, they are easy to estimate from known quantities. For the costs c̄(t)f , c̄ (t) g and c̄ (t) p , we simply time how long each past stochastic gradient calculation has taken, and then average them to estimate the future costs. For the variances v̄(t)f and v̄ (t) g , we restrict ourselves to minibatch sizes k (t) f , k (t) g ≥ 2, calculate the sample variances v(t)f and v (t) g of the stochastic gradients at each iteration, and then average over all past iterations (either uniformly, or a weighted average placing more weight on recent iterations).\nFor v̄(t)p , the situation is a bit more complicated, since the p-updates are multiplicative (so we should use an ℓ∞ variance) and centered as in Equation 4. Upper-bounding the ℓ∞ norm with the ℓ2 norm and using the fact that the minibatch S(t) is independently sampled yields the following crude estimate:\nv(t)p = γ 2m2\n\n\n1\nk (t) p\n∑\ni∈S(t)\n( µi −max { 0, gi ( w(t) )})2\n\n ,\nWe again average v(t)p across past iterations to estimate v̄ (t) p ."
    }, {
      "heading" : "6 Experiments",
      "text" : "We validated the performance of our practical variant of LightTouch (Algorithm 4) on a YouTube ranking problem in the style of Joachims [2002], in which the task is to predict what a user will watch next, given that they have just viewed a certain video. In this setting, a user has just viewed video a, was presented with a list of candidate videos to watch next, and clicked on b+, with b− being the video immediately preceding b+ in the list (if b+ was the first list element, then the example is thrown out).\nWe used an anonymized proprietary dataset consisting of n = 612 587 training pairs of feature vectors (x+, x−), where x+ is a vector of 12 features summarizing the similarity between a and b+, and x− between a and b−.\nWe treat this as a standard pairwise ranking problem, for which the goal is to estimate a function f(Φ(x)) = 〈w,Φ(x)〉 such that f(Φ(x+)) > f(Φ(x−)) for as many examples as possible, subject to the appropriate regularization (or, in this case, constraints). Specifically, the (unconstrained) learning task is to minimize the average empirical hinge loss:\nmin w∈W\n1\nn\nn ∑\ni=1\n( max { 0, 1− 〈 w,Φ ( x+i ) − Φ ( x−i )〉}) .\nAll twelve of the features were designed to provide positive evidence—in other words, if any one increases (holding the others fixed), then we expect f(Φ(x)) to increase. We have found that using constraints to enforce this monotonicity property results in a better model in practice.\nWe define Φ(·) as in lattice regression using simplex interpolation [Garcia et al., 2012, Gupta et al., 2016], an approach which works well at combining a small number of informative features, and more importantly (for our purposes) enables one to force the learned function to be monotonic via linear inequality constraints on the parameters. For the resulting problem, the feature vectors have dimension d = 212 = 4096, we chose W to be defined by the box constraints −10 ≤ wi ≤ 10 in each of the 4096 dimensions, and the total number of monotonicity-enforcing linear inequality constraints is m = 24 576.\nEvery Φ(x) contains only d + 1 = 13 nonzeros and can be computed in O(d ln d) time. Hence, stochastic gradients of f are inexpensive to compute. Likewise, checking a monotonicity constraint only requires a single comparison between two parameter values, so although there are a large number of them, each constraint is very inexpensive to check."
    }, {
      "heading" : "6.1 Implementations",
      "text" : "We implemented all algorithms in C++. Before running our main experiments, we performed crude parameter searches on a power-of-four grid (i.e. . . . , 1/16, 1/4, 1, 4, 16, . . .). For each candidate value we performed roughly 10 000 iterations, and chose the parameter that appeared to result in the fastest convergence in terms of the objective function.\nLightTouch Our implementation of LightTouch includes all of the suggested changes of Section 5, including the constraint aggregation approach of Section 5.1, although we used no aggregation until our timing comparison (Section 6.3). For automatic minibatching, we took weighted averages of the variance estimates as v̄(t+1) ∝ v(t) + νv̄(t). We found that up-weighting recent estimates (taking ν < 1) resulted in a noticeable improvement, but that the precise value of ν mattered little (we used ν = 0.999). Based on the grid search described above, we chose γ = 1, ηw = 16 and ηp = 1/16.\nFullTouch Our FullTouch implementation differs from that in Algorithm 1 only in that we used a decreasing step size η (t) w = ηw/ √ t. As with LightTouch, we chose γ = 1 and ηw = 16 based on a grid search.\nProjectedSGD We implemented Euclidean projections onto lattice monotonicity constraints using IPOPT [Wächter and Biegler, 2006] to optimize the resulting sparse 4096-dimensional quadratic program. However, the use of a QP solver for projected SGD—a very heavyweight solution—resulted in an implementation that was too slow to experiment with, requiring nearly four minutes per projection (observe that our experiments each ran for millions of iterations).\nApproxSGD This is an approximate projected SGD implementation using the fast approximate update procedure described in Gupta et al. [2016], which is an active set method that, starting from the current iterate, moves along the boundary of the feasible region, adding constraints to the active set as they are encountered, until the desired step is exhausted (this is reminiscent of the local linear oracles considered by Garber and Hazan [2013]). This approach is particularly well-suited to this particular constraint set because (1) when checking constraints for possible inclusion in the active set, it exploits the sparsity of the stochastic gradients to only consider monotonicity constraints which could possibly be violated, and (2) projecting onto an intersection of active monotonicity constraints reduces to uniformly averaging every set of parameters that are “linked together” by active constraints. Like the other algorithms, we used step sizes of η(t)w = ηw/ √ t and chose ηw = 64 based on the grid search (recall that ηw = 16 was better for the other two algorithms).\nIn every experiment we repeatedly looped over a random permutation of the training set, and generated plots by averaging over 5 such runs (with the same 5 random permutations) for each algorithm."
    }, {
      "heading" : "6.2 Constraint-check Comparison",
      "text" : "In our first set of experiments, we compared the performance of LightTouch, FullTouch and ApproxSGD in terms of the number of stochastic subgradients of f drawn, and the number of constraints checked. Because LightTouch’s automatic minibatching fixes k(t)f = 2 (with the other two minibatch sizes being automatically determined), in these experiments we used minibatch sizes of 2 for FullTouch and ApproxSGD, guaranteeing that all three algorithms observe the same number of stochastic subgradients of f at each iteration.\nThe left-hand plot of Figure 1 shows that all three algorithms converge at roughly comparable per-iteration rates, with ApproxSGD having a slight advantage over FullTouch, which itself converges a bit more rapidly than LightTouch. The right-hand plot shows a striking difference, however—LightTouch reaches a near-optimal solution having checked more than 10× fewer constraints than FullTouch. Notice that we plot the suboptimalities of the projected iterates Πw(w\n(t)) rather than of the w(t)s themselves, in order to emulate the final projection (line 7 of Algorithm 1 and 17 of Algorithm 4), and guarantee that we only compare the average losses of feasible intermediate solutions.\nIn Figure 2, we explore how well our algorithms enforce feasibility, and how effective automatic minibatching is at choosing minibatch sizes. The left-hand plot shows that both FullTouch has converged to a nearly-feasible solution\nafter roughly 10 000 iterations, and LightTouch (unsurprisingly) takes more, perhaps 100 000 or so. In the right-hand plot, we see that, in line with our expectations (see Section 5.2), LightTouch’s automatic minibatching results in very few constraints being checked in late iterations.\n6.3 Timing Comparison\nOur final experiment compared the wall-clock runtimes of our implementations. Note that, because each monotonicity constraint can be checked with only a single comparison (compare with e.g. O(d) arithmetic operations for a dense linear inequality constraint), the O(m) arithmetic cost of maintaining and updating the probability distribution p over the constraints is significant. Hence, in terms of the constraint costs, this is nearly a worse-case problem for LightTouch. We experimented with power-of-4 constraint aggregate sizes (Section 5.1), and found that using m̃ = 96 aggregated constraints, each of size 256, worked best.\nFullTouch, without minibatching, draws a single stochastic subgradient of f and checks every constraint at each iteration. However, it would seem to be more efficient to use minibatching to look at more stochastic subgradients at each iteration, and therefore fewer constraints per stochastic subgradient of f . Hence, for FullTouch, we again searched over power-of-4 minibatch sizes, and found that 16 worked best.\nFor ApproxSGD, the situation is less clear-cut. On the one hand, increasing the minibatch size results in fewer approximate projections being performed per stochastic subgradient of f . On the other, averaging more stochastic\nsubgradients results in less sparsity, slowing down the approximate projection. We found that the latter consideration wins out—after searching again over power-of-4 minibatch sizes, we found that a minibatch size of 1 (i.e. no minibatching) worked best.\nFigure 3 contains the results of these experiments, showing that both FullTouch and LightTouch converge significantly faster than ApproxSGD. Interestingly, ApproxSGD is rather slow in early iterations (clipped off in plot), but accelerates in later iterations. We speculate that the reason for this behavior is that, close to convergence, the steps taken at each iteration are smaller, and therefore the active sets constructed during the approximate projection routine do not grow as large. FullTouch enjoys a small advantage over LightTouch until both algorithms are very close to convergence, but based on the results of Section 6.2, we believe that this advantage would reverse if there were more constraints, or if the constraints were more expensive to check."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have proposed an efficient strategy for large-scale heavily constrained optimization, building on the work of Mahdavi et al. [2012], and analyze its performance, demonstrating that, asymptotically, our approach requires many fewer constraint checks in order to converge.\nWe build on these theoretical results to propose a practical variant. The most significant of these improvements is based on the observation that our algorithm takes steps based on three separate stochastic gradients, and that trading off the variances of computational costs of these three components is beneficial. To this end, we propose a heuristic for dynamically choosing minibatch sizes in such a way as to encourage faster convergence at a lower computational cost.\nExperiments on a real-world 4096-dimensional machine learning problem with 24 576 constraints and 612 587 training examples—too large for a QP-based implementation of projected SGD—showed that our proposed method is effective. In particular, we find that, in practice, our technique checks fewer constraints per iteration than competing algorithms, and, as expected, checks ever fewer as optimization progresses."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Kevin Canini, Mahdi Milani Fard, Andrew Frigyik, Michael Friedlander and Seungil You for helpful discussions, and proofreading earlier drafts."
    }, {
      "heading" : "A Mirror Descent",
      "text" : "Mirror descent [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] is a meta-algorithm for stochastic optimization (more generally, online regret minimization) which performs gradient updates with respect to a meta-parameter, the distance generating function (d.g.f.). The two most widely-used d.g.f.s are the squared Euclidean norm and negative Shannon entropy, for which the resulting MD instantiations are stochastic gradient descent (SGD) and a multiplicative updating algorithm, respectively. These are precisely the two d.g.f.s which our constrained algorithm will use for the updates of w and p. We’ll here give a number of results which differ only slightly from “standard” ones, beginning with a statement of an online MD bound adapted from Srebro et al. [2011]:\nTheorem 3. Let ‖·‖ and ‖·‖∗ be a norm and its dual. Suppose that the distance generating function (d.g.f.) Ψ is 1-strongly convex w.r.t. ‖·‖. Let Ψ∗ be the convex conjugate of Ψ, and take BΨ(w|w′) = Ψ(w) − Ψ(w′) − 〈∇Ψ(w′), w − w′〉 to be the associated Bregman divergence. Take ft : W → R to be a sequence of convex functions on which we perform T iterations of mirror descent starting from w(1) ∈ W:\nw̃(t+1) = ∇Ψ∗ ( ∇Ψ ( w(t) ) − η∇̌ft ( w(t) )) ,\nw(t+1) = argmin w∈W BΨ\n( w ∣ ∣ ∣ w̃(t+1) ) ,\nwhere ∇̌ft(w(t)) ∈ ∂ft(w(t)) is a subgradient of ft at w(t). Then:\n1\nT\nT ∑\nt=1\n(\nft\n( w(t) ) − ft (w∗) ) ≤ BΨ ( w∗ ∣ ∣ w(1) )\nηT +\nη\n2T\nT ∑\nt=1\n∥ ∥ ∥∇̌ft ( w(t) )∥ ∥ ∥ 2\n∗ ,\nwhere w∗ ∈ W is an arbitrary reference vector.\nProof. This proof is essentially the same as that of Srebro et al. [2011, Lemma 2]. By convexity:\nη (\nft\n( w(t) ) − ft (w∗) ) ≤ 〈 η∇̌ft ( w(t) ) , w(t) − w∗ 〉\n≤ 〈 η∇̌ft ( w(t) ) , w(t) − w̃(t+1) 〉 + 〈 η∇̌ft ( w(t) ) , w̃(t+1) − w∗ 〉 ."
    }, {
      "heading" : "By Hölder’s inequality, 〈w′, w〉 ≤ ‖w′‖ ‖w‖∗. Also, Ψ(w) = supv(〈v, w〉−Ψ∗(v)) is maximized when∇Ψ∗(v) = w,",
      "text" : "so ∇Ψ(∇Ψ∗(v)) = v. These results combined with the definition of w̃(t+1) give:\nη (\nft\n( w(t) ) − ft (w∗) ) ≤ ∥ ∥ ∥η∇̌ft ( w(t) )∥ ∥ ∥\n∗\n∥ ∥ ∥w(t) − w̃(t+1) ∥ ∥ ∥\n+ 〈 ∇Ψ ( w(t) ) −∇Ψ ( w̃(t+1) ) , w̃(t+1) − w∗ 〉 .\nUsing Young’s inequality and the definition of the Bregman divergence:\nη (\nft\n( w(t) ) − ft (w∗) ) ≤ 1 2\n∥ ∥ ∥η∇̌ft ( w(t) )∥ ∥ ∥ 2\n∗ +\n1\n2\n∥ ∥ ∥w(t) − w̃(t+1) ∥ ∥ ∥ 2\n+BΨ\n( w∗ ∣ ∣ ∣ w(t) ) −BΨ ( w∗ ∣ ∣ ∣ w̃(t+1) ) −BΨ ( w̃(t+1) ∣ ∣ ∣ w(t) ) .\nApplying the 1-strong convexity of Ψ to cancel the ∥ ∥w(t) − w̃(t+1) ∥ ∥ 2 /2 and BΨ(w̃(t+1) | w(t)) terms:\nη (\nft\n( w(t) ) − ft (w∗) ) ≤ η 2\n2\n∥ ∥ ∥∇̌ft ( w(t) )∥ ∥ ∥ 2\n∗ +BΨ\n( w∗ ∣ ∣ ∣ w(t) ) −BΨ ( w∗ ∣ ∣ ∣ w̃(t+1) ) .\nSumming over t, using the nonnegativity of BΨ, and dividing through by ηT gives the claimed result.\nIt is straightforward to transform Theorem 3 into an in-expectation result for stochastic subgradients:\nCorollary 1. Take ft : W → R to be a sequence of convex functions, and F a filtration. Suppose that we perform T iterations of stochastic mirror descent starting from w(1) ∈ W , using the definitions of Theorem 3:\nw̃(t+1) = ∇Ψ∗ ( ∇Ψ ( w(t) ) − η∆̌(t) ) ,\nw(t+1) = argmin w∈W BΨ\n( w ∣ ∣ ∣ w̃(t+1) ) ,\nwhere ∆̌(t) is a stochastic subgradient of ft, i.e. E[∆̌(t) | Ft−1] ∈ ∂ft(w(t)), and ∆̌(t) is Ft-measurable. Then:\n1\nT\nT ∑\nt=1\nE\n[\nft\n( w(t) ) − ft (w∗) ] ≤ BΨ ( w∗ ∣ ∣ w(1) )\nηT +\nη\n2T\nT ∑\nt=1\nE\n[\n∥ ∥ ∥∆̌(t) ∥ ∥ ∥ 2\n∗\n]\n,\nwhere w∗ ∈ W is an arbitrary reference vector.\nProof. Define f̃t (w) = 〈 ∆̌(t), w 〉\n, and observe that applying the non-stochastic MD algorithm of Theorem 3 to the sequence of functions f̃t results in the same sequence of iterates w(t) as does applying the above stochastic MD update to the sequence of functions ft. Hence:\n1\nT\nT ∑\nt=1\n(\nf̃t\n( w(t) ) − f̃t (w∗) ) ≤ BΨ ( w∗ ∣ ∣ w(1) )\nηT +\nη\n2T\nT ∑\nt=1\n∥ ∥ ∥∆̌(t) ∥ ∥ ∥ 2\n∗ . (5)\nBy convexity, ft(w(t)) − ft(w∗) ≤ 〈 E[∆̌(t) | Ft−1], w(t) − w∗ 〉 , while f̃t(w(t)) − f̃t(w∗) = 〈 ∆̌(t), w(t) − w∗ 〉\nby definition. Taking expectations of both sides of Equation 5 and plugging in these inequalities yields the claimed result.\nWe next prove a high-probability analogue of the Corollary 1, based on a martingale bound of Dzhaparidze and van Zanten [2001]:\nCorollary 2. In addition to the assumptions of Corollary 1, suppose that, with probability 1 − δσ , σ satisfies the following uniformly for all t ∈ {1, . . . , T }:\n∥ ∥ ∥ E [ ∆̌(t) ∣ ∣ ∣ Ft−1 ] − ∆̌(t) ∥ ∥ ∥\n∗ ≤ σ.\nThen, with probability 1− δσ − δ, the above σ bound will hold, and:\n1\nT\nT ∑\nt=1\n(\nft\n( w(t) ) − ft (w∗) ) ≤ BΨ ( w∗ ∣ ∣ w(1) )\nηT +\nη\n2T\nT ∑\nt=1\n∥ ∥ ∥∆̌(t) ∥ ∥ ∥ 2\n∗ +\n√ 2R∗σ √\nln 1δ√ T + 2R∗σ ln 1 δ 3T ,\nwhere w∗ ∈ W is an arbitrary reference vector and R∗ ≥ supw∈W ‖w − w∗‖ bounds the radius of W centered on w∗.\nProof. Define f̃t (w) = 〈 ∆̌(t), w 〉\nas in the proof of Corollary 1, and observe that Equation 5 continues to apply. Define a sequence of random variables M0 = 0, Mt = Mt−1 + 〈 E[∆̌(t) | Ft−1]− ∆̌(t), w(t) − w∗ 〉\n, and notice that M forms a martingale w.r.t. the filtration F . From this definition, Hölder’s inequality gives that:\n|Mt −Mt−1| ≤ ∥ ∥ ∥E [ ∆̌(t) ∣ ∣ ∣ Ft−1 ] − ∆̌(t) ∥ ∥ ∥\n∗\n∥ ∥ ∥w(t) − w∗ ∥ ∥ ∥ ≤ R∗σ.\nthe above holding with probability 1 − δσ . Plugging a = R∗σ and L = TR2∗σ2 into the Bernstein-type martingale inequality of Dzhaparidze and van Zanten [2001, Theorem 3.3] gives:\nPr\n{\n1 T MT ≥ ǫ\n} ≤ δσ + exp ( − 3T ǫ 2\n6R2∗σ 2 + 2R∗σǫ\n)\n.\nSolving for ǫ using the quadratic formula and upper-bounding gives that, with probability 1− δσ − δ:\n1\nT\nT ∑\nt=1\n〈\nE\n[ ∆̌(t) ∣ ∣ ∣ Ft−1 ] − ∆̌(t), w(t) − w∗ 〉 ≤ √ 2R∗σ √\nln 1δ√ T + 2R∗σ ln 1 δ 3T .\nAs in the proof of Corollary 1, ft(w(t)) − ft(w∗) ≤ 〈 E[∆̌(t) | Ft−1], w(t) − w∗ 〉 , while f̃t(w(t)) − f̃t(w∗) = 〈\n∆̌(t), w(t) − w∗ 〉 by definition, which combined with Equation 5 yields the claimed result.\nAlgorithm 2 (LightTouch) jointly optimizes over two sets of parameters, for which the objective is convex in the first and linear (hence concave) in the second. The convergence rate will be determined from a saddle-point bound, which we derive from Corollary 2 by following Nemirovski et al. [2009], Rakhlin and Sridharan [2013], and simply applying it twice:\nCorollary 3. Let ‖·‖w and ‖·‖α be norms with duals ‖·‖w∗ and ‖·‖α∗. Suppose that Ψw and Ψα are 1-strongly convex w.r.t. ‖·‖w and ‖·‖α, have convex conjugates Ψ∗w and Ψ∗α, and associated Bregman divergences BΨw and BΨα , respectively.\nTake f : W ×A → R to be convex in its first parameter and concave in its second, let F be a filtration, and suppose that we perform T iterations of MD:\nw̃(t+1) = ∇Ψ∗w ( ∇Ψw ( w(t) ) − η∆̌(t)w ) ,\nw(t+1) = argmin w∈W BΨw\n( w ∣ ∣ ∣ w̃(t+1) ) ,\nα̃(t+1) = ∇Ψ∗α ( ∇Ψα ( α(t) ) + η∆̂(t)α ) ,\nα(t+1) = argmin α∈A BΨα\n( α ∣ ∣ ∣ α̃(t+1) ) ,\nwhere ∆̌(t)w is a stochastic subgradient of f(w(t), α(t)) w.r.t. its first parameter, and ∆̂ (t) α a stochastic supergradient w.r.t. its second, with both ∆̌(t)w and ∆̂ (t) α being Ft-measurable. We assume that, with probabilities 1−δσw and 1−δσα (respectively), σ2w and σ 2 α satisfy the following uniformly for all t ∈ {1, . . . , T }:\n∥ ∥ ∥ E [ ∆̌(t)w ∣ ∣ ∣ Ft−1 ] − ∆̌(t)w ∥ ∥ ∥\nw∗ ≤ σw and\n∥ ∥ ∥ E [ ∆̂(t)α ∣ ∣ ∣ Ft−1 ] − ∆̌(t)w ∥ ∥ ∥\nα∗ ≤ σα.\nUnder these conditions, with probability 1− δσw − δσα − 2δ, the above σw and σα bounds will hold, and:\n1\nT\nT ∑\nt=1\n( f ( w(t), α∗ ) − f ( w∗, α(t) ))\n≤BΨw ( w∗ ∣ ∣ w(1) ) +BΨα ( α∗ ∣ ∣ α(1) )\nηT +\nη\n2T\nT ∑\nt=1\n(\n∥ ∥ ∥∆̌(t)w ∥ ∥ ∥ 2 w∗ + ∥ ∥ ∥∆̂(t)α ∥ ∥ ∥ 2 α∗\n)\n+\n√ 2 (Rw∗σw +Rα∗σα) √\nln 1δ√ T + 2 (Rw∗σw +Rα∗σα) ln 1 δ 3T ,\nwhere w∗ ∈ W and α∗ ∈ A are arbitrary reference vectors, and Rw∗ ≥ ‖w − w∗‖w and Rα∗ ≥ ‖α− α∗‖α bound the radii of W and A centered on w∗ and α∗, respectively.\nProof. This is a convex-concave saddle-point problem, which we will optimize by playing two convex optimization algorithms against each other, as in Nemirovski et al. [2009], Rakhlin and Sridharan [2013]. By Corollary 2, with probability 1− δσw − δ and 1− δσα − δ, respectively:\n1\nT\nT ∑\nt=1\n( f ( w(t), α(t) ) − f ( w∗, α(t) ))\n≤BΨw ( w∗ ∣ ∣ w(1) )\nηT +\nη\n2T\nT ∑\nt=1\n∥ ∥ ∥∆̌(t)w ∥ ∥ ∥ 2\nw∗ +\n√ 2Rw∗σw √\nln 1δ√ T + 2Rw∗σw ln 1 δ 3T ,\n1\nT\nT ∑\nt=1\n( f ( w(t), α∗ ) − f ( w(t), α(t) ))\n≤BΨα ( α∗ ∣ ∣ α(1) )\nηT +\nη\n2T\nT ∑\nt=1\n∥ ∥ ∥∆̂(t)α ∥ ∥ ∥ 2\nα∗ +\n√ 2Rα∗σα √\nln 1δ√ T + 2Rα∗σα ln 1 δ 3T .\nAdding these two inequalities gives the claimed result."
    }, {
      "heading" : "B SGD for Strongly-Convex Functions",
      "text" : "For λ-strongly convex objective functions, we can achieve a faster convergence rate for SGD by using the step sizes ηt = 1/λt. Our eventual algorithm (Algorithm 3) for strongly-convex heavily-constrained optimization will proceed in two phases, with the second phase “picking up” where the first phase “left off”, for which reason we present a convergence rate, based on Shalev-Shwartz et al. [2011, Lemma 2], that effectively starts at iteration T0 by using the step sizes ηt = 1/λ(T0 + t):\nTheorem 4. Take ft : W → R to be a sequence of λ-strongly convex functions on which we perform T iterations of stochastic gradient descent starting from w(1) ∈ W:\nw(t+1) = Πw\n( w(t) − ηt∇̌ft ( w(t) )) ,\nwhere ∇̌ft ( w(t) ) ∈ ∂ft ( w(t) ) is a subgradient of ft at w(t), and ∥ ∥∇̌ft ( w(t) )∥ ∥ 2 ≤ G for all t. If we choose ηt = 1\nλ(T0+t) for some T0 ∈ N, then:\n1\nT\nT ∑\nt=1\n(\nft\n( w(t) ) − ft (w∗) ) ≤ G 2 (1 + lnT )\n2λT + λT0 2T\n∥ ∥ ∥w(1) − w∗ ∥ ∥ ∥ 2\n2 ,\nwhere w∗ ∈ W is an arbitrary reference vector and G ≥ ∥ ∥∇̌ft ( w(t) )∥ ∥\n2 bounds the subgradient norms for all t.\nProof. This is nothing but a small tweak to Shalev-Shwartz et al. [2011, Lemma 2]. Starting from Equations 10 and 11 of that proof:\nT ∑\nt=1\n(\nft\n( w(t) ) − ft (w∗) )\n≤G 2\n2\nT ∑\nt=1\nηt + T ∑\nt=1\n(\n1 2ηt − λ 2\n)\n∥ ∥ ∥ w(t) − w∗ ∥ ∥ ∥ 2\n2 −\nT ∑\nt=1\n1\n2ηt\n∥ ∥ ∥ w(t+1) − w∗ ∥ ∥ ∥ 2\n2 .\nTaking ηt = 1λ(T0+t) :\nT ∑\nt=1\n(\nft\n( w(t) ) − ft (w∗) )\n≤G 2\n2λ\n(\n1\nT0 + 1 +\n∫ T0+T\nt=T0+1\ndt\nt\n)\n+ λT0 2\n∥ ∥ ∥w(1) − w∗ ∥ ∥ ∥ 2 2 − λ (T0 + T ) 2 ∥ ∥ ∥w(T+1) − w∗ ∥ ∥ ∥ 2 2 .\nDividing through by T , simplifying and bounding yields the claimed result.\nAs we did Appendix A, we convert this into a result for stochastic subgradients:\nCorollary 4. Take ft : W → R to be a sequence of λ-strongly convex functions, and F a filtration. Suppose that we perform T iterations of stochastic gradient descent starting from w(1) ∈ W:\nw(t+1) = Πw\n( w(t) − ηt∆̌(t) ) ,\nwhere ∆̌(t) is a stochastic subgradient of ft, i.e. E[∆̌(t) | Ft−1] ∈ ∂ft(w(t)), and ∆̌(t) is Ft-measurable. If we choose ηt =\n1 λ(T0+t) for some T0 ∈ N, then:\n1\nT\nT ∑\nt=1\nE\n[\nft\n( w(t) ) − ft (w∗) ] ≤ G 2 (1 + lnT )\n2λT + λT0 2T\n∥ ∥ ∥w(1) − w∗ ∥ ∥ ∥ 2\n2 ,\nwhere w∗ ∈ W is an arbitrary reference vector and G ≥ ∥ ∥∆̌(t) ∥ ∥\n2 bounds the stochastic subgradient norms for all t.\nProof. Same proof technique as Corollary 1, but based on Theorem 4 rather than Theorem 3.\nWe now use this result to prove an in-expectation saddle point bound:\nCorollary 5. Let ‖·‖α and ‖·‖α∗ be a norm and its dual. Suppose that Ψα is 1-strongly convex w.r.t. ‖·‖α, and has convex conjugate Ψ∗α and associated Bregman divergence BΨα .\nTake f : W ×A → R to be λ-strongly convex in its first parameter and concave in its second, let F be a filtration, and suppose that we perform T iterations of SGD on w and MD on α:\nw(t+1) = Πw\n(\nw(t) − 1 λ (T0 + t) ∆̌(t)w\n)\n,\nα̃(t+1) = ∇Ψ∗α ( ∇Ψα ( α(t) ) + η∆̂(t)α ) ,\nα(t+1) = argmin α∈A BΨα\n( α ∣ ∣ ∣ α̃(t+1) ) ,\nwhere ∆̌(t)w is a stochastic subgradient of f(w(t), α(t)) w.r.t. its first parameter, and ∆̂ (t) α a stochastic supergradient w.r.t. its second, with both ∆̌(t)w and ∆̂ (t) α being Ft-measurable. Then:\n1\nT\nT ∑\nt=1\nE\n[ f ( w(t), α∗ ) − f ( w∗, α(t) )]\n≤G 2 w (1 + lnT )\n2λT + λT0 2T\n∥ ∥ ∥w(1) − w∗ ∥ ∥ ∥ 2\n2 +\nBΨα ( α∗ ∣ ∣ α(1) )\nηT +\nη\n2T\nT ∑\nt=1\nE\n[\n∥ ∥ ∥∆̂(t)α ∥ ∥ ∥ 2\nα∗\n]\n,\nwhere w∗ ∈ W and α∗ ∈ A are arbitrary reference vectors, and Gw ≥ ∥ ∥ ∥ ∆̌ (t) w ∥ ∥ ∥\n2 bounds the stochastic subgradient\nnorms w.r.t. w for all t.\nProof. As we did in the proof of Corollary 3, we will play two convex optimization algorithms against each other. By Corollaries 4 and 1:\n1\nT\nT ∑\nt=1\nE\n[ f ( w(t), α(t) ) − f ( w∗, α(t) )] ≤G 2 w (1 + lnT )\n2λT + λT0 2T\n∥ ∥ ∥ w(1) − w∗ ∥ ∥ ∥ 2\n2 ,\n1\nT\nT ∑\nt=1\nE\n[ f ( w(t), α∗ ) − f ( w(t), α(t) )] ≤BΨα ( α∗ ∣ ∣ α(1) )\nηT +\nη\n2T\nT ∑\nt=1\nE\n[\n∥ ∥ ∥∆̂(t)α ∥ ∥ ∥ 2\nα∗\n]\n,\nAdding these two inequalities gives the claimed result."
    }, {
      "heading" : "C Analyses of FullTouch and LightTouch",
      "text" : "We begin by proving that, if γ is sufficiently large, then optimizing the relaxed objective, and projecting the resulting solution, will bring us close to the optimum of the constrained objective.\nLemma 1. In the setting of Section 2, suppose that f is Lf -Lipschitz, i.e. |f(w)− f(w′)| ≤ Lf ‖w − w′‖2 for all w,w′ ∈ W , and that there is a constant ρ > 0 such that if g(w) = 0 then ∥ ∥∇̌ ∥ ∥\n2 ≥ ρ for all ∇̌ ∈ ∂g(w), where ∂g(w)\nis the subdifferential of g(w).\nFor a parameter γ > 0, define: h (w) = f (w) + γmax {0, g (w)} .\nIf γ > Lf/ρ, then for any infeasible w (i.e. for which g(w) > 0):\nh (w) > h (Πg (w)) = f (Πg (w)) and ‖w −Πg (w)‖2 ≤ h (w)− h (Πg (w))\nγρ− Lf ,\nwhere Πg (w) is the projection of w onto the set {w ∈ W : g(w) ≤ 0} w.r.t. the Euclidean norm.\nProof. Let w ∈ W be an arbitrary infeasible point. Because f is Lf -Lipschitz:\nf (w) ≥ f (Πg (w))− Lf ‖w −Πg (w)‖2 . (6)\nSince Πg(w) is the projection of w onto the constraints w.r.t. the Euclidean norm, we must have by the first order optimality conditions that there exists a ν ≥ 0 such that:\n0 ∈ ∂ ‖w −Πg (w)‖22 + ν∂g (Πg (w)) .\nThis implies that w − Πg(w) is a scalar multiple of some ∇̌ ∈ ∂g(Πg(w)). Because g is convex and Πg (w) is on the boundary, g(w) ≥ g(Πg(w)) + 〈 ∇̌, w −Πg(w) 〉 = 〈 ∇̌, w −Πg(w) 〉 , so:\ng(w) ≥ ρ ‖w −Πg(w)‖2 . (7)\nCombining the definition of h with Equations 6 and 7 yields:\nh (w) ≥ f (Πg (w)) + (γρ− Lf ) ‖w −Πg(w)‖2 .\nBoth claims follow immediately if γρ > Lf .\nC.1 Analysis of FullTouch\nWe’ll now use Lemma 1 and Corollary 2 to bound the convergence rate of SGD on the function h of Lemma 1 (this is FullTouch). Like the algorithm itself, the convergence rate is little different from that found by Mahdavi et al. [2012] (aside from the bound on ‖w̄ −Πg(w̄)‖2), and is included here only for completeness. Lemma 3. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Dw ≥ supw,w′∈W ‖w − w′‖2 as the diameter of W , Gf ≥ ∥ ∥∆̌(t) ∥ ∥ 2 and Gg ≥ ∥ ∥∇̌max(0, gi(w)) ∥ ∥ 2 as uniform upper bounds on the (stochastic) gradient magnitudes of f and the gis, respectively.\nIf we optimize Equation 1 using Algorithm 1 (FullTouch) with the step size:\nη = Dw\n(Gf + γGg) √ T ,\nthen with probability 1− δ:\nf (Πg (w̄))− f (w∗) ≤ h (w̄)− h (w∗) ≤ UF , and ‖w̄ −Πg (w̄)‖2 ≤ UF\nγρ− Lf ,\nwhere w∗ ∈ {w ∈ W : ∀i.gi(w) ≤ 0} is an arbitrary constraint-satisfying reference vector, and:\nUF ≤ ( 1 + 2 √ 2 ) Dw (Gf + γGg) √ 1 + ln 1\nδ\n√\n1 T +\n8DwGf ln 1 δ\n3T .\nProof. We choose Ψ(w) = ‖w‖22 /2, for which the mirror descent update rule is precisely SGD. Because Ψw is (half of) the squared Euclidean norm, it is trivially 1-strongly convex w.r.t. the Euclidean norm, so ‖·‖ = ‖·‖∗ = ‖·‖2. Furthermore, BΨ(w∗ | w(1)) ≤ D2w/2 and R∗ ≤ Dw. We may upper bound the 2-norm of our stochastic gradients as ∥ ∥ ∥∆̌ (t) w ∥ ∥ ∥\n2 ≤ Gf + γGg . Only the f -portion of the\nobjective is stochastic, so the error of the ∆̌(t)w s can be trivially upper bounded, with probability 1, with σ = 2Gf .\nHence, by Corollary 2 (takingFt to be e.g. the smallest σ-algebra making ∆̌(t), . . . , ∆̌(t) measurable), with probability 1− δ:\n1\nT\nT ∑\nt=1\n( h ( w(t) ) − h (w∗) ) ≤ D 2 w\n2ηT +\nη (Gf + γGg) 2\n2 +\n2 √ 2DwGf √\nln 1δ√ T + 8DwGf ln 1 δ 3T .\nPlugging in the definition of η, moving the average defining w̄ inside h by Jensen’s inequality, substituting f(w∗) = h(w∗) because w∗ satisfies the constraints, applying Lemma 1 and simplifying yields the claimed result.\nIn terms of the number of iterations required to achieve some desired level of suboptimality, this bound on UF may be expressed as:\nTheorem 5. Suppose that the conditions of Lemmas 1 and 3 apply, and that η is as defined in Lemma 3.\nIf we optimize Equation 1 using Tǫ iterations of Algorithm 1 (FullTouch):\nTǫ = O\n(\nD2w (Gf + γGg) 2 ln 1δ\nǫ2\n)\n,\nthen UF ≤ ǫ with probability 1 − δ. where w∗ ∈ {w ∈ W : ∀i.gi(w) ≤ 0} is an arbitrary constraint-satisfying reference vector.\nProof. Based on the bound of Lemma 3, define:\nx = √ T ,\nc = 8\n3 DwGf ln\n1 δ ,\nb = ( 1 + 2 √ 2 )\nDw (Gf + γGg)\n√\n1 + ln 1\nδ ,\na =− ǫ,\nand consider the polynomial 0 = ax2 + bx+ c. Roots of this polynomial are xs for which UF = ǫ, while for xs larger than any root we’ll have that UF ≤ ǫ. Hence, we can bound the T required to ensure ǫ-suboptimality by bounding the roots of this polynomial. By the Fujiwara bound [Wikipedia, 2015]:\nTǫ ≤ max { 4 (\n9 + 4 √ 2 )\nD2w (Gf + γGg) 2 ( 1 + ln 1δ )\nǫ2 , 16DwGf ln\n1 δ\n3ǫ\n}\n, (8)\ngiving the claimed result.\nC.2 Analysis of LightTouch\nBecause we use the reduced-variance algorithm of Johnson and Zhang [2013], and therefore update the remembered gradient µ one random coordinate at a time, we must first bound the maximum number of iterations over which a coordinate can go un-updated:\nLemma 4. Consider a process which maintains a sequence of vectors s(t) ∈ Nm for t ∈ {1, . . . , T }, where s(1) is initialized to zero and s(t+1) is derived from s(t) by independently sampling k = |St| ≤ m random indices St ⊆ {1, . . . ,m} uniformly without replacement, and then setting s(t+1)j = t for j ∈ St and s (t+1) j = s (t) j for j /∈ St. Then, with probability 1− δ: max t,j ( t− s(t)j ) ≤ 1 + 2m k ln ( 2mT δ ) .\nProof. This is closely related to the “coupon collector’s problem” [Wikipedia, 2014]. We will begin by partitioning time into contiguous size-n chunks, with 1, . . . , n forming the first chunk, n+ 1, . . . , 2n the second, and so on.\nWithin each chunk the probability that any particular index was never sampled is ((m − k)/m)n, so by the union bound the probability that any one of the m indices was never sampled is bounded by m((m− k)/m)n:\nm\n(\nm− k m\n)n\n≤ exp ( lnm+ n ln ( m− k m )) ≤ exp ( lnm− nk m ) .\nDefine n = ⌈(m/k) ln(2mT/δ)⌉, so:\nm\n(\nm− k m\n)n\n≤ exp ( lnm− ln ( 2mT\nδ\n))\n≤ δ 2T .\nThis shows that for this choice of n, the probability of there existing an index which is never sampled in some particular batch is bounded by δ/2T . By the union bound, the probability of any of ⌈T/n⌉ batches containing an index which is never sampled is bounded by (δ/2T )⌈T/n⌉ ≤ (δ/2n) + (δ/2T ) ≤ δ. If every index is sampled within every batch, then over the first n⌈T/n⌉ ≥ T steps, the most steps which could elapse over which a particular index is not sampled is 2n− 2 (if the index is sampled on the first step of one chunk, and the last step of the next chunk), which implies the claimed result.\nWe now combine this bound with Corollary 2 and make appropriate choices of the two d.g.f.s to yield a bound on the LightTouch convergence rate:\nLemma 5. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Dw ≥ supw,w′∈W max{1, ‖w − w′‖2} as a bound on the diameter of W (notice that we also choose Dw to be at least 1), Gf ≥ ∥ ∥∆̌(t) ∥ ∥\n2 and Gg ≥\n∥ ∥∇̌max(0, gi(w)) ∥ ∥\n2 as uniform upper bounds on the (stochastic) gradient\nmagnitudes of f and the gis, respectively, for all i ∈ {1, . . . ,m}. We also assume that all gis are Lg-Lipschitz w.r.t. ‖·‖2, i.e. |gi(w) − gi(w′)| ≤ Lg ‖w − w′‖2 for all w,w′ ∈ W . Define:\nk =\n\n  \nm (1 + lnm) 3/4\n√ 1 + ln 1δ √ 1 + lnT T 1/4   \n\n.\nIf k ≤ m and we optimize Equation 1 using Algorithm 2 (LightTouch), basing the stochastic gradients w.r.t. p on k constraints at each iteration, and using the step size:\nη =\n√ 1 + lnmDw\n(Gf + γGg + γLgDw) √ T ,\nthen it holds with probability 1− δ that:\nf (Πg (w̄))− f (w∗) ≤ h (w̄)− h (w∗) ≤ UL, and ‖w̄ −Πg (w̄)‖2 ≤ UL\nγρ− Lf ,\nwhere w∗ ∈ {w ∈ W : ∀i.gi(w) ≤ 0} is an arbitrary constraint-satisfying reference vector, and:\nUL ≤ 67 √ 1 + lnmDw (Gf + γGg + γLgDw)\n√\n1 + ln 1\nδ\n√\n1 T .\nIf k > m, then we should fall-back to using FullTouch, in which case the result of Lemma 3 will apply.\nProof. We choose Ψw(w) = ‖w‖22 /2 and Ψp(p) = ∑m\ni=1 pi ln pi to be the squared Euclidean norm divided by 2 and the negative Shannon entropy, respectively, which yields the updates of Algorithm 2. We assume that the ∆̌(t)s are random variables on some probability space (depending on the source of the stochastic gradients of f ), and likewise the its and jts on another, so Ft may be taken to be the product of the smallest σ-algebras which make ∆̌(1), . . . , ∆̌(t) and i1, j1, . . . , it, jt measurable, respectively, with conditional expectations being taken w.r.t. the product measure. Under the definitions of Corollary 3 (taking α = p), with probability 1− δσw − δσp − 2δ′:\n1\nT\nT ∑\nt=1\nh̃ ( w(t), p∗ ) − 1 T\nT ∑\nt=1\nh̃ ( w∗, p(t) )\n≤BΨw ( w∗ ∣ ∣ w(1) ) +BΨp ( p∗ ∣ ∣ p(1) )\nηT +\nη\n2T\nT ∑\nt=1\n(\n∥ ∥ ∥∆̌(t)w ∥ ∥ ∥ 2 w∗ + ∥ ∥ ∥∆̂(t)p ∥ ∥ ∥ 2 p∗\n)\n+\n√ 2 (Rw∗σw +Rp∗σp) √\nln 1δ′√ T + 4 (Rw∗σw +Rp∗σp) ln 1 δ′ 3T .\nAs in the proof of Lemma 3, Ψw is 1-strongly convex w.r.t. the Euclidean norm, so ‖·‖w = ‖·‖w∗ = ‖·‖2, BΨw(w∗ | w(1)) ≤ D2w/2 and Rw∗ ≤ Dw. Because Ψp is the negative entropy, which is 1-strongly convex w.r.t. the 1-norm (this is Pinsker’s inequality), ‖·‖p = ‖·‖1 and ‖·‖p∗ = ‖·‖∞, implying that Rp∗ = 1. Since p(1) is initialized to the uniform distribution, BΨp(p ∗ | p(1)) = DKL(p∗ | p(1)) ≤ lnm.\nThe stochastic gradient definitions of Algorithm 2 give that ∥ ∥ ∥∆̌ (t) w ∥ ∥ ∥\nw∗ ≤ Gf + γGg and σw ≤ 2(Gf + γGg) with\nprobability 1 = 1 − δσw by the triangle inequality, and h̃(w∗, p(t)) = f(w∗) because w∗ satisfies the constraints. All of these facts together give that, with probability 1− δσp − δ′:\n1\nT\nT ∑\nt=1\nh̃ ( w(t), p∗ ) − f (w∗)\n≤D 2 w + 2 lnm\n2ηT +\nη\n2T\nT ∑\nt=1\n(\n(Gf + γGg) 2 + ∥ ∥ ∥∆̂(t)p ∥ ∥ ∥ 2\n∞\n)\n+\n√ 2 (2Dw(Gf + γGg) + σp) √\nln 1δ′√ T + 4 (2Dw (Gf + γGg) + σp) ln 1 δ′ 3T .\nWe now move the average defining w̄ inside h̃ (which is convex in its first parameter) by Jensen’s inequality, and use the fact that there exists a p∗ such that h̃(w, p∗) = h(w) to apply Lemma 1:\nUL ≤ D2w + 2 lnm\n2ηT +\nη\n2T\nT ∑\nt=1\n(\n(Gf + γGg) 2 + ∥ ∥ ∥∆̂(t)p ∥ ∥ ∥ 2\n∞\n)\n(9)\n+\n√ 2 (2Dw(Gf + γGg) + σp) √\nln 1δ′√ T + 4 (2Dw (Gf + γGg) + σp) ln 1 δ′ 3T .\nBy the triangle inequality and the fact that (a+ b)2 ≤ 2a2 + 2b2: ∥\n∥ ∥∆̂(t)p\n∥ ∥ ∥ 2\n∞ ≤ 2\n∥ ∥ ∥E [ ∆̂(t)p ∣ ∣ ∣ Ft−1 ]∥ ∥ ∥ 2\n∞ + 2\n∥ ∥ ∥E [ ∆̂(t)p ∣ ∣ ∣ Ft−1 ] − ∆̂(t)p ∥ ∥ ∥ 2\n∞\n≤ 2γ2L2gD2w + 2 ∥ ∥ ∥E [ ∆̂(t)p ∣ ∣ ∣ Ft−1 ] − ∆̂(t)p ∥ ∥ ∥ 2 ∞ ≤ 2γ2L2gD2w + 2σ2p.\nSubstituting into Equation 9 and using the fact that a+ b ≤ (√a+ √ b)2:\nUL ≤ D2w + 2 lnm\n2ηT +\nη\n2\n( Gf + γGg + √ 2γLgDw )2 + ησ2p (10)\n+\n√ 2 (2Dw(Gf + γGg) + σp) √\nln 1δ′√ T + 4 (2Dw (Gf + γGg) + σp) ln 1 δ′ 3T .\nWe will now turn our attention to the problem of bounding σp. Notice that because we sample i.i.d. jts uniformly at every iteration, they form an instance of the process of Lemma 4 with µ(t)j = max(0, gj(w (s (t) j ))), showing that with probability 1− δσp: max t,j ( t− s(t)j ) ≤ 1 + 2m k ln ( 2mT\nδσp\n)\n. (11)\nBy the definition of ∆̂(t)p (Algorithm 2):\n∥ ∥ ∥E [ ∆̂(t)p ∣ ∣ ∣ Ft−1 ] − ∆̂(t)p ∥ ∥ ∥ 2\n∞\n=γ2\n∥ ∥ ∥ ∥ ∥ ∥   m ∑\nj=1\nej max { 0, gj ( w(t) )} − µ(t) \n− m k ∑\nj∈St\n( ej max { 0, gj ( w(t) )} − ejµ(t)j )\n∥ ∥ ∥ ∥ ∥ ∥ 2\n∞\n≤γ2 ( m− k k\n)2\nmax j\n( max {\n0, gj\n( w(t) )} − µ(t)j )2\n≤γ2 ( m− k k\n)2\nL2g\n∥ ∥\n∥w(t) − w(s (t) j )\n∥ ∥ ∥\n2\n2\n≤γ2 ( m− k k\n)2\nL2gη 2 (Gf + γGg)\n2 ( t− s(t)j )2\n≤γ2 ( m− k k\n)2\nL2gη 2 (Gf + γGg)\n2\n(\n1 + 2m\nk ln\n(\n2mT\nδσp\n))2\n≤6γ2 (m\nk\n)4\nL2gη 2 (Gf + γGg)\n2\n(\n1 + ln\n(\nmT\nδσp\n))2\nwhere in the second step we used the definition of the ∞-norm, in the third we used the Lipschitz continuity of the gis (and hence of their positive parts), in the fourth we bounded the distance between two iterates with the number of iterations times a bound on the total step size, and in the fifth we used Equation 11. This shows that we may define:\nσp = √ 6γ (m\nk\n)2\nLgη (Gf + γGg)\n(\n1 + ln\n(\nmT\nδσp\n))\n,\nand it will satisfy the conditions of Corollary 3. Notice that, due to the η factor, σp will be decreasing in T . Substituting the definitions of η and σp into Equation 10, choosing δσp = δ′ = δ/3 and using the assumption that Dw ≥ 1 gives\nthat with probability 1− δ:\nUL ≤ 2 ( 1 + √ 2 )√ 1 + ln 3 √ 1 + lnmDw (Gf + γGg + γLgDw) √ 1 + ln 1\nδ\n(\n1√ T\n)\n+\n( 2 √ 3 + 8\n3\n)\n(1 + ln 3) 3/2\n(m\nk\n)2\n(1 + lnm) 3/2\nDw (Gf + γGg)\n(\n1 + ln 1\nδ\n)3/2 ( 1 + lnT\nT\n)\n+ 2\n(\n3 + 2\n√\n2\n3\n)\n(1 + ln 3) 2 (m\nk\n)4\n(1 + lnm) 7/2\nDw (Gf + γGg)\n(\n1 + ln 1\nδ\n)2 (\n(1 + lnT ) 2\nT 3/2\n)\n.\nRounding up the constant terms:\nUL ≤ 7 √ 1 + lnmDw (Gf + γGg + γLgDw)\n√\n1 + ln 1\nδ\n(\n1√ T\n)\n+ 19 (m\nk\n)2\n(1 + lnm) 3/2 Dw (Gf + γGg)\n(\n1 + ln 1\nδ\n)3/2 ( 1 + ln T\nT\n)\n+ 41 (m\nk\n)4\n(1 + lnm) 7/2\nDw (Gf + γGg)\n(\n1 + ln 1\nδ\n)2 (\n(1 + lnT ) 2\nT 3/2\n)\n.\nSubstituting the definition of k, simplifying and bounding yields the claimed result.\nIn terms of the number of iterations required to achieve some desired level of suboptimality, this bound on UL and the bound of Lemma 3 on UF may be combined to yield the following:\nTheorem 1. Suppose that the conditions of Lemmas 1 and 5 apply. Our result will be expressed in terms of a total iteration count Tǫ satisfying:\nTǫ = O\n(\n(lnm)D2w (Gf + γGg + γLgDw) 2 ln 1δ\nǫ2\n)\n.\nDefine k in terms of Tǫ as in Lemma 5. If k ≤ m, then we optimize Equation 1 using Tǫ iterations of Algorithm 2 (LightTouch) with η as in Lemma 5. If k > m, then we use Tǫ iterations of Algorithm 1 (FullTouch) with η as in Lemma 3. In either case, we perform Tǫ iterations, requiring a total of Cǫ “constraint checks” (evaluations or differentiations of a single gi):\nCǫ =Õ\n(\n(lnm)D2w (Gf + γGg + γLgDw) 2 ln 1δ\nǫ2\n+ m (lnm)\n3/2 D 3/2 w (Gf + γGg + γLgDw) 3/2 ( ln 1δ\n)5/4\nǫ3/2\n)\n.\nand with probability 1− δ:\nf (Πg (w̄))− f (w∗) ≤ h (w̄)− h (w∗) ≤ ǫ and ‖w̄ −Πg (w̄)‖2 ≤ ǫ\nγρ− Lf ,\nwhere w∗ ∈ {w ∈ W : ∀i.gi(w) ≤ 0} is an arbitrary constraint-satisfying reference vector.\nProof. Regardless of the value of k, it follows from Lemmas 5 and 3 that:\nUL, UF ≤ 67 √ 1 + lnmDw (Gf + γGg + γLgDw)\n√\n1 + ln 1\nδ\n√\n1 T +\n8DwGf ln 1 δ\n3T .\nAs in the proof of Theorem 5, we define:\nx = √ T ,\nc = 8\n3 DwGf ln\n1 δ ,\nb =67 √ 1 + lnmDw (Gf + γGg + γLgDw)\n√\n1 + ln 1\nδ\n√\n1 T ,\na =− ǫ,\nand consider the polynomial 0 = ax2 + bx+ c. Any upper bound on all roots x = √ T of this polynomial will result in a lower-bound the values of T for which UL, UF ≤ ǫ with probability 1 − δ. By the Fujiwara bound [Wikipedia, 2015]:\nTǫ = max\n{\n(134) 2 (1 + lnm)D2w (Gf + γGg + γLgDw) 2 ( 1 + ln 1δ )\nǫ2 , 16DwGf ln\n1 δ\n3ǫ\n}\n,\ngiving the claimed bound on Tǫ. For Cǫ, we observe that we will perform no more than k + 1 constraint checks at each iteration (k + 1 by LightTouch if k ≤ m, and m+ 1 by FullTouch if k > m), and substitute the above bound on Tǫ into the definition of k, yielding:\n(k + 1)Tǫ ≤2Tǫ +m (1 + lnm) 3/4\n√\n1 + ln 1\nδ T\n3/4 ǫ\n√\n1 + lnTǫ\n≤max { 2 (134) 2 (1 + lnm)D2w (Gf + γGg + γLgDw) 2 ( 1 + ln 1δ )\nǫ2 , 32DwGf ln\n1 δ\n3ǫ\n}\n+max\n{\n(134) 3/2 m (1 + lnm) 3/2 D 3/2 w (Gf + γGg + γLgDw) 3/2 ( 1 + ln 1δ\n)5/4\nǫ3/2 ,\n(\n16\n3\n)3/4 m (1 + lnm) 3/4 D 3/4 w G 3/4 f ( 1 + ln 1δ )5/4\nǫ3/4\n\n\n\n√\n1 + lnTǫ.\ngiving the claimed result (notice the √ 1 + lnTǫ factor on the RHS, for which reason we have a Õ bound on Cǫ, instead of O)."
    }, {
      "heading" : "D Analysis of MidTouch",
      "text" : "We now move on to the analysis of our LightTouch variant for λ-strongly convex objectives, Algorithm 3 (MidTouch). While we were able to prove a high-probability bound for LightTouch, we were unable to do so for MidTouch, because the extra terms resulting from the use of a Bernstein-type martingale inequality were too large (since the other terms shrank as a result of the strong convexity assumption). Instead, we give an in-expectation result, and leave the proof of a corresponding high-probability bound to future work.\nOur first result is an analogue of Lemmas 3 and 5, and bounds the suboptimality achieved by MidTouch as a function of the iteration counts T1 and T2 of the two phases:\nLemma 6. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Gf ≥ ∥ ∥∆̌(t) ∥ ∥\n2\nand Gg ≥ ∥ ∥∇̌max(0, gi(w)) ∥ ∥\n2 as uniform upper bounds on the (stochastic) gradient magnitudes of f and the gis,\nrespectively, for all i ∈ {1, . . . ,m}. We also assume that f is λ-strongly convex, and that all gis are Lg-Lipschitz w.r.t. ‖·‖2, i.e. |gi(w) − gi(w′)| ≤ Lg ‖w − w′‖2 for all w,w′ ∈ W .\nIf we optimize Equation 1 using Algorithm 3 (MidTouch) with the p-update step size η = λ/2γ2L2g, then:\nE\n[ ‖Πg(w̄)− w∗‖22 ] ≤ E [ ‖w̄ − w∗‖22 ]\n≤ 2 (Gf + γGg)\n2 (2 + lnT1 + lnT2) + 8γ 2L2g lnm\nλ2T2 +\n3m4 (1 + lnm) 2 (Gf + γGg) 2\nλ2T 21 ,\nwhere w∗ = argmin{w∈W:∀i.gi(w)≤0} f(w) is the optimal constraint-satisfying reference vector.\nProof. As in the proof of Lemma 3, the first phase of Algorithm 3 is nothing but (strongly convex) SGD on the overall objective function h, so by Corollary 4:\n1\nT1\nT1 ∑\nt=1\nE\n[ h ( w(t) ) − h (w∗) ] ≤ G 2 w (1 + lnT1)\n2λT1 ,\nso by Jensen’s inequality:\nE\n[ h ( w(T1+1) ) − h (w∗) ] ≤ G 2 w (1 + lnT1)\n2λT1 . (12)\nFor the second phase, as in the proof of Lemma 5, we choose Ψp(p) = ∑m\ni=1 pi ln pi to be negative Shannon entropy, which yields the second-phase updates of Algorithm 3. By Corollary 5:\n1\nT2\nT2 ∑\nt=T1+1\nE\n[ h̃ ( w(t), p∗ ) − h̃ ( w∗, p(t) )]\n≤G 2 w (1 + lnT )\n2λT2 + λT1 2T2\n∥ ∥ ∥w(T1+1) − w∗ ∥ ∥ ∥ 2\n2 +\nBΨp ( p∗ ∣ ∣ p(T1+1) )\nηT2 +\nη\n2T2\nT2 ∑\nt=T1+1\nE\n[\n∥ ∥ ∥∆̂(t)p ∥ ∥ ∥ 2\np∗\n]\n.\nAs before, ‖·‖p = ‖·‖1, ‖·‖p∗ = ‖·‖∞, and BΨp(p∗ | p(T1+1)) = DKL(p∗ | p(T1+1)) ≤ lnm. Hence:\n1\nT2\nT2 ∑\nt=T1+1\nE\n[ h̃ ( w(t), p∗ ) − h̃ ( w∗, p(t) )]\n≤G 2 w (1 + lnT2)\n2λT2 + λT1 2T2\n∥ ∥ ∥w(T1+1) − w∗ ∥ ∥ ∥ 2\n2 +\nlnm ηT2 + η 2T2\nT2 ∑\nt=T1+1\nE\n[\n∥ ∥ ∥∆̂(t)p ∥ ∥ ∥ 2\n∞\n]\n.\nSince h is λ-strongly convex and w∗ is optimal, ∥ ∥w(T1+1) − w∗ ∥ ∥\n2 2 ≤ 2λ(h(w(T1+1))− h(w∗)). By Equation 12:\n1\nT2\nT2 ∑\nt=T1+1\nE\n[ h̃ ( w(t), p∗ ) − h̃ ( w∗, p(t) )]\n≤G 2 w (2 + lnT1 + lnT2)\n2λT2 +\nlnm ηT2 + η 2T2\nT2 ∑\nt=T1+1\nE\n[\n∥ ∥ ∥∆̂(t)p ∥ ∥ ∥ 2\n∞\n]\n.\nSince the (uncentered) second moment is equal to the mean plus the variance, and using the fact that h̃(w∗, p(t)) = f(w∗) since all constraints are satisfied at w∗:\n1\nT2\nT2 ∑\nt=T1+1\nE\n[ h̃ ( w(t), p∗ )] − f (w∗) (13)\n≤G 2 w (2 + lnT1 + lnT2)\n2λT2 +\nlnm ηT2 + η 2T2\nT2 ∑\nt=T1+1\n(\nE\n[∥\n∥ ∥∆̂(t)p\n∥ ∥ ∥\n∞\n])2 + ησ2p 2 ,\nwhere σ2p is the variance of ∥ ∥ ∥ ∆̂ (t) p ∥ ∥ ∥\n∞ . Next observe that:\n(\nE\n[∥\n∥ ∥∆̂(t)p\n∥ ∥ ∥\n∞\n])2\n=\n(\nE\n[\nmax j∈{1,...,m}\nγmax {\n0, gj\n( w(t) )}\n])2\n≤γ2L2gE [ ∥ ∥ ∥w(t) − w∗ ∥ ∥ ∥ 2\n2\n]\n≤ 2γ2L2g\nλ E\n[ h̃ ( w(t), p∗ ) − h̃ (w∗, p∗) ] ,\nthe first step using the fact that the gjs are Lg-Lipschitz and Jensen’s inequality. For the second step, we choose p∗ such that w∗, p∗ is a minimax optimal pair (recall that w∗ is optimal by assumption), and use the λ-strong convexity of h̃. Substituting into Equation 13 and using the fact that h̃(w∗, p∗) = f(w∗):\n(\n1− ηγ2L2g\nλ\n)(\n1\nT2\nT2 ∑\nt=T1+1\nE\n[ h̃ ( w(t), p∗ )] − f (w∗) ) ≤ G 2 w (2 + lnT1 + lnT2)\n2λT2 +\nlnm ηT2 + ησ2p 2 .\nSubstituting η = λ/2γ2L2g and using Jensen’s inequality:\nE\n[ h̃ (w̄, p∗) ] − f (w∗) ≤ G 2 w (2 + lnT1 + lnT2)\nλT2 +\n4γ2L2g lnm\nλT2 + λσ2p 2γ2L2g . (14)\nWe now follow the proof of Lemma 5 and bound σ2p. By the definition of ∆̂ (t) p (Algorithm 3):\nσ2p =E\n[\n∥ ∥ ∥ E [ ∆̂(t)p ∣ ∣ ∣ Ft−1 ] − ∆̂(t)p ∥ ∥ ∥ 2\n∞\n]\n=γ2E\n\n \n∥ ∥ ∥ ∥ ∥ ∥   m ∑\nj=1\nej max { 0, gj ( w(t) )} − µ(t)  −m ( ejt max { 0, gjt ( w(t) )} − ejtµ (t) jt )\n∥ ∥ ∥ ∥ ∥ ∥ 2\n∞\n\n \n≤γ2 (m− 1)2 E [\nmax j\n( max {\n0, gj\n( w(t) )} − µ(t)j )2\n]\n.\nThe indices j are sampled uniformly, so the maximum time maxj(t− s(t)j ) since we last sampled the same index is an instance of the coupon collector’s problem Wikipedia [2014]. Because the gjs are Lg-Lipschitz:\nσ2p ≤γ2 (m− 1)2 L2gE [\nmax j\n∥ ∥\n∥w(t) − w(s (t) j )\n∥ ∥ ∥\n2\n2\n]\n≤ γ2 (m− 1)2 L2gG2w\nλ2T 21 E\n[\nmax j\n( t− s(t)j )2\n]\n≤ γ2m4\n(\n1 + (lnm) 2 + π 2 /6 )\nL2gG 2 w\nλ2T 21\n≤ 3γ2m4 (1 + lnm)2 L2gG 2 w\nλ2T 21 ,\nthe second step because, between iteration s(t)j and iteration t we will perform t − s (t) j updates of magnitude at most Gw/λT1, and the third step because, as an instance of the coupon collector’s problem, maxj(t− s(t)j ) has expectation mHm ≤ m+m lnm (Hm is the mth harmonic number) and variance m2π2/6. Substituting into Equation 14:\nE\n[ h̃ (w̄, p∗) ] − f (w∗) ≤ G 2 w (2 + lnT1 + lnT2)\nλT2 +\n4γ2L2g lnm\nλT2 +\n3m4 (1 + lnm) 2 G2w\n2λT 21 .\nBy the λ-strong convexity of h̃:\nE\n[ ‖w̄ − w∗‖22 ] ≤ 2G 2 w (2 + lnT1 + lnT2)\nλ2T2 +\n8γ2L2g lnm\nλ2T2 +\n3m4 (1 + lnm) 2 G2w\nλ2T 21 .\nUsing the facts that ‖Πg(w̄)− w∗‖ ≤ ‖w̄ − w∗‖ because w∗ is feasible, and that Gw = Gf + γGg , completes the proof.\nWe now move on to the main result: a bound on the number of iterations (equivalently, the number of stochastic loss gradients) and constraint checks required to achieve ǫ-suboptimality:\nTheorem 2. Suppose that the conditions of Lemmas 1 and 6 apply, with the p-update step size η as defined in Lemma 6. If we run Algorithm 3 (MidTouch) for Tǫ1 iterations in the first phase and Tǫ2 in the second:\nTǫ1 =Õ\n(\nm (lnm) 2/3 (Gf + γGg + γLg) 4/3\nλ4/3ǫ2/3 +\nm2 (lnm) (Gf + γGg)\nλ √ ǫ\n)\n,\nTǫ2 =Õ\n(\n(lnm) (Gf + γGg + γLg) 2\nλ2ǫ +\nm3/2 (lnm) 3/2 (Gf + γGg) 3/2\nλ3/2ǫ3/4\n)\n,\nrequiring a total of Cǫ “constraint checks” (evaluations or differentiations of a single gi):\nCǫ =Õ\n(\n(lnm) (Gf + γGg + γLg) 2\nλ2ǫ +\nm3/2 (lnm) 3/2 (Gf + γGg) 3/2\nλ3/2ǫ3/4\n+ m2 (lnm) 2/3 (Gf + γGg + γLg) 4/3\nλ4/3ǫ2/3 +\nm3 (lnm) (Gf + γGg)\nλ √ ǫ\n)\n,\nthen: E [\n‖Πg(w̄)− w∗‖22 ] ≤ E [ ‖w̄ − w∗‖22 ] ≤ ǫ,\nwhere w∗ = argmin{w∈W:∀i.gi(w)≤0} f(w) is the optimal constraint-satisfying reference vector.\nProof. We begin by introducing a number τ ∈ R with τ ≥ 1 that will be used to define the iteration counts T1 and T2 as:\nT1 = ⌈ mτ2 ⌉\nand T2 = ⌈ τ3 ⌉ .\nBy Lemma 6, the above definitions imply that:\nE\n[ ‖Πg(w̄)− w∗‖22 ]\n≤ 2 (Gf + γGg)\n2 (4 + lnm+ 5 ln τ) + 8γ2L2g lnm\nλ2τ3 +\n3m4 (1 + lnm) 2 (Gf + γGg) 2\nλ2m2τ4\n≤ 10 (1 + lnm) (Gf + γGg + γLg) 2 (1 + ln τ)\nλ2τ3 +\n3m2 (1 + lnm) 2 (Gf + γGg) 2\nλ2τ4 .\nDefining ǫ = E [ ‖Πg(w̄)− w∗‖22 ] and rearranging:\nλ2ǫ\n(\nτ\n(1 + ln τ) 1/3\n)4\n≤ 10 (1 + lnm) (Gf + γGg + γLg)2 (\nτ\n(1 + ln τ) 1/3\n)\n+ 3m2 (1 + lnm) 2 (Gf + γGg) 2 .\nWe will now upper-bound all roots of the above equation with a quantity τǫ, for which all τ ≥ τǫ will result in ǫ-suboptimality. By the Fujiwara bound [Wikipedia, 2015], and including the constraint that τ ≥ 1:\nτǫ\n(1 + ln τǫ) 1/3\n≤max\n\n\n\n1, 2\n(\n10 (1 + lnm) (Gf + γGg + γLg) 2\nλ2ǫ\n)1/3\n,\n2\n(\n3m2 (1 + lnm) 2 (Gf + γGg) 2\n2λ2ǫ\n)1/4 \n\n\n.\nSubstituting the above bound on τǫ into the definitions of T1 and T2 gives the claimed magnitudes of these Tǫ1 and Tǫ2, and using the fact that the Cǫ = O(mTǫ1 + Tǫ2) gives the claimed bound on Cǫ."
    } ],
    "references" : [ {
      "title" : "Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems",
      "author" : [ "N.P. Archer", "S. Wang" ],
      "venue" : "Decision Sciences,",
      "citeRegEx" : "Archer and Wang.,? \\Q1993\\E",
      "shortCiteRegEx" : "Archer and Wang.",
      "year" : 1993
    }, {
      "title" : "Projection Algorithms and Monotone Operators",
      "author" : [ "H.H. Bauschke" ],
      "venue" : "Ph.D. Thesis, Simon Fraser University,",
      "citeRegEx" : "Bauschke.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bauschke.",
      "year" : 1996
    }, {
      "title" : "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "Oper. Res. Lett.,",
      "citeRegEx" : "Beck and Teboulle.,? \\Q2003\\E",
      "shortCiteRegEx" : "Beck and Teboulle.",
      "year" : 2003
    }, {
      "title" : "Sublinear optimization for machine learning",
      "author" : [ "K.L. Clarkson", "E. Hazan", "D.P. Woodruff" ],
      "venue" : "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Clarkson et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Clarkson et al\\.",
      "year" : 2010
    }, {
      "title" : "Monotone and partially monotone neural networks",
      "author" : [ "H. Daniels", "M. Velikova" ],
      "venue" : "IEEE Trans. Neural Networks,",
      "citeRegEx" : "Daniels and Velikova.,? \\Q2010\\E",
      "shortCiteRegEx" : "Daniels and Velikova.",
      "year" : 2010
    }, {
      "title" : "On Bernstein-type inequalities for martingales",
      "author" : [ "K. Dzhaparidze", "J.H. van Zanten" ],
      "venue" : "Stochastic Processes and their Applications,",
      "citeRegEx" : "Dzhaparidze and Zanten.,? \\Q2001\\E",
      "shortCiteRegEx" : "Dzhaparidze and Zanten.",
      "year" : 2001
    }, {
      "title" : "Playing non-linear games with linear oracles. In FOCS, pages 420–428",
      "author" : [ "D. Garber", "E. Hazan" ],
      "venue" : "IEEE Computer Society,",
      "citeRegEx" : "Garber and Hazan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Garber and Hazan.",
      "year" : 2013
    }, {
      "title" : "Optimized regression for efficient function evaluation",
      "author" : [ "E.K. Garcia", "R. Arora", "M. Gupta" ],
      "venue" : "IEEE Trans. Image Processing,",
      "citeRegEx" : "Garcia et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2012
    }, {
      "title" : "Monotonic calibrated interpolated look-up",
      "author" : [ "M.R. Gupta", "A. Cotter", "J. Pfeifer", "K. Voevodski", "K. Canini", "A. Mangylov", "W. Moczydlowski", "A. van Esbroeck" ],
      "venue" : "tables. JMLR,",
      "citeRegEx" : "Gupta et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2016
    }, {
      "title" : "Projection-free online learning",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "In ICML’12,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2012
    }, {
      "title" : "Revisiting Frank-Wolfe: Projection-free sparse convex optimization",
      "author" : [ "M. Jaggi" ],
      "venue" : "In ICML’13,",
      "citeRegEx" : "Jaggi.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jaggi.",
      "year" : 2013
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "T. Joachims" ],
      "venue" : "In KDD’02,",
      "citeRegEx" : "Joachims.,? \\Q2002\\E",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2002
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "In NIPS’13,",
      "citeRegEx" : "Johnson and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2013
    }, {
      "title" : "Stochastic gradient descent with only one projection",
      "author" : [ "M. Mahdavi", "T. Yang", "R. Jin", "S. Zhu", "J. Yi" ],
      "venue" : "In NIPS’12,",
      "citeRegEx" : "Mahdavi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mahdavi et al\\.",
      "year" : 2012
    }, {
      "title" : "Lecture notes: Interior point polynomial time methods in convex programming",
      "author" : [ "A. Nemirovski" ],
      "venue" : null,
      "citeRegEx" : "Nemirovski.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nemirovski.",
      "year" : 2004
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "Generalized constraint neural network regression model subject to linear priors",
      "author" : [ "Y. Qu", "B. Hu" ],
      "venue" : "IEEE Trans. on Neural Networks,",
      "citeRegEx" : "Qu and Hu.,? \\Q2011\\E",
      "shortCiteRegEx" : "Qu and Hu.",
      "year" : 2011
    }, {
      "title" : "Optimization, learning, and games with predictable sequences",
      "author" : [ "A. Rakhlin", "K. Sridharan" ],
      "venue" : "In NIPS’13,",
      "citeRegEx" : "Rakhlin and Sridharan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rakhlin and Sridharan.",
      "year" : 2013
    }, {
      "title" : "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Least squares isotonic regression in two dimensions",
      "author" : [ "J. Spouge", "H. Wan", "W.J. Wilbur" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "Spouge et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Spouge et al\\.",
      "year" : 2003
    }, {
      "title" : "On the universality of online mirror descent",
      "author" : [ "N. Srebro", "K. Sridharan", "A. Tewari" ],
      "venue" : "In NIPS’11,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2011
    }, {
      "title" : "On the implementation of a primal-dual interior point filter line search algorithm for large-scale nonlinear programming",
      "author" : [ "A. Wächter", "L.T. Biegler" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Wächter and Biegler.,? \\Q2006\\E",
      "shortCiteRegEx" : "Wächter and Biegler.",
      "year" : 2006
    }, {
      "title" : "Random Multi-Constraint Projection: Stochastic Gradient Methods for Convex Optimization with Many Constraints",
      "author" : [ "M. Wang", "Y. Chen", "J. Liu", "Y. Gu" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "In ICML’03,",
      "citeRegEx" : "Zinkevich.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Archer and Wang, 1993, Sill, 1998, Spouge et al., 2003, Daniels and Velikova, 2010, Gupta et al., 2016]. Submodular functions can often be learned from noisy examples by imposing constraints to ensure submodularity holds. Another example occurs when one wishes to guarantee that a classifier will correctly label certain “canonical” examples, which can be enforced by constraining the function values on those examples. See Qu and Hu [2011] for some other examples of constraints useful in machine learning.",
      "startOffset" : 0,
      "endOffset" : 441
    }, {
      "referenceID" : 0,
      "context" : "Archer and Wang, 1993, Sill, 1998, Spouge et al., 2003, Daniels and Velikova, 2010, Gupta et al., 2016]. Submodular functions can often be learned from noisy examples by imposing constraints to ensure submodularity holds. Another example occurs when one wishes to guarantee that a classifier will correctly label certain “canonical” examples, which can be enforced by constraining the function values on those examples. See Qu and Hu [2011] for some other examples of constraints useful in machine learning. However, these practical uses of constraints in machine learning are impractical in that the number of constraints may be very large, and scale poorly with the number of features d or number of training samples n. In this paper we propose a new strategy for tackling such heavily-constrained problems, with guarantees and compelling convergence rates for large-scale convex problems. A standard approach for large-scale empirical risk minimization is projected stochastic gradient descent [e.g. Zinkevich, 2003, Nemirovski et al., 2009]. Each SGD iteration is computationally cheap, and the algorithm converges quickly to a solution good enough for machine learning needs. However, this algorithm requires a projection onto the feasible region after each stochastic gradient step, which can be prohibitively slow if there are many non-trivial constraints, and is not easy to parallelize. Recently, Frank-Wolfe-style algorithms [e.g. Hazan and Kale, 2012, Jaggi, 2013] have been proposed that remove the projection, but require a constrained linear optimization at each iteration. We propose a new strategy for large-scale constrained optimization that, like Mahdavi et al. [2012], moves the constraints into the objective and finds an approximate solution of the resulting unconstrained problem, projecting the (potentially-infeasible) result onto the constraints only once, at the end.",
      "startOffset" : 0,
      "endOffset" : 1688
    }, {
      "referenceID" : 13,
      "context" : "Despite this, LightTouch was roughly as fast as the Mahdavi et al. [2012]-like algorithm FullTouch.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : "p μ Remembered gradient coordinates [Johnson and Zhang, 2013] k Minibatch size in LightTouch’s p-update w̄ Average iterate w̄ = ( ∑T t=1 w )/T consider constraints written in terms of arbitrary convex functions, and are not restricted to e.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "p μ Remembered gradient coordinates [Johnson and Zhang, 2013] k Minibatch size in LightTouch’s p-update w̄ Average iterate w̄ = ( ∑T t=1 w )/T consider constraints written in terms of arbitrary convex functions, and are not restricted to e.g. only linear or quadratic constraints. 2.1 FullTouch: A Relaxation with a Feasible Minimizer We build on the approach of Mahdavi et al. [2012] to relax Equation 1.",
      "startOffset" : 37,
      "endOffset" : 385
    }, {
      "referenceID" : 13,
      "context" : "This algorithm—our starting point—is similar to those proposed by Mahdavi et al. [2012], and like their algorithms only contains a single projection, at the end, projecting the potentially-infeasible result vector w̄.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "This algorithm—our starting point—is similar to those proposed by Mahdavi et al. [2012], and like their algorithms only contains a single projection, at the end, projecting the potentially-infeasible result vector w̄. Hyperparameters: T , η 1 Initialize w ∈ W arbitrarily 2 For t = 1 to T : 3 Sample ∆̌ // stochastic subgradient of f(w) 4 Let ∆̌ w = ∆̌ + γ∇̌max{0, g(w(t))} 5 Update w = Πw(w − η∆̌ w ) // Πw projects its argument onto W w.r.t. ‖·‖2 6 Average w̄ = ( ∑T t=1 w )/T 7 Return Πg(w̄) // optional if small constraint violations are acceptable If γ > Lf/ρ, then for any infeasible w (i.e. for which g(w) > 0): h (w) > h (Πg (w)) = f (Πg (w)) and ‖w −Πg (w)‖2 ≤ h (w)− h (Πg (w)) γρ− Lf , where Πg (w) is the projection of w onto the set {w ∈ W : g(w) ≤ 0} w.r.t. the Euclidean norm. Proof. In Appendix C. The strategy of applying SGD to h(w), detailed in Algorithm 1, which we call FullTouch, has the same “flavor” as the algorithms proposed by Mahdavi et al. [2012], and we use it as a baseline comparison point for our other algorithms.",
      "startOffset" : 66,
      "endOffset" : 976
    }, {
      "referenceID" : 1,
      "context" : "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al.",
      "startOffset" : 78,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al. [2015]. 3 A Light Touch This section presents the main contribution of this paper: an algorithm that stochastically samples a small subset of the m constraints at each SGD iteration, updates the parameters based on the subgradients of the sampled constraints, and carefully learns the distribution over the constraints to produce a net performance gain.",
      "startOffset" : 78,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al. [2015]. 3 A Light Touch This section presents the main contribution of this paper: an algorithm that stochastically samples a small subset of the m constraints at each SGD iteration, updates the parameters based on the subgradients of the sampled constraints, and carefully learns the distribution over the constraints to produce a net performance gain. We first motivate the approach by considering an oracle, then explain the algorithm and present convergence results for the convex (Section 3.2) and strongly convex (Section 3.3) cases. 3.1 Wanted: An Oracle For the Most Violated Constraint Because FullTouch only needs to differentiate the most violated constraint at each iteration, it follows that if one had access to an oracle that identified the most-violated constraint, then the overall convergence rate (including the cost of each iteration) could only depend on m through γ. This motivates us to learn to predict the most-violated constraint, ideally at a significantly better than linear-in-m rate. To this end, we further relax the problem of minimizing h(w) (defined in Lemma 1) by replacing γmax(0, g(w)) with maximization over a probability distribution (as in Clarkson et al. [2010]), yielding the equivalent convex-linear 5",
      "startOffset" : 78,
      "endOffset" : 1337
    }, {
      "referenceID" : 12,
      "context" : "For this reason, in addition to minibatching, we center the stochastic gradients, as is done by the well-known SVRG algorithm [Johnson and Zhang, 2013], by storing a gradient estimate γμ with μ ∈ R, at each iteration sampling a set S of size |S| = k uniformly without replacement, and computing: ∆̂p = γμ+ γm k ∑",
      "startOffset" : 126,
      "endOffset" : 151
    }, {
      "referenceID" : 6,
      "context" : "4 Theoretical Comparison Table 2 compares upper bounds on the convergence rates and per-iteration costs when applied to a convex (but not necessarily strongly convex) problem for LightTouch, FullTouch, projected SGD, the online Frank-Wolfe algorithm of Hazan and Kale [2012], and a Frank-Wolfe-like online algorithm for optimization over a polytope [Garber and Hazan, 2013].",
      "startOffset" : 349,
      "endOffset" : 373
    }, {
      "referenceID" : 6,
      "context" : "3], and Garber and Hazan [2013, Theorem 2]. Notice that because this table compares upper bounds to upper bounds, subsequent work may improve these bounds further. #Iterations to achieve #Constraint checks to achieve ǫ-suboptimality ǫ-suboptimality FullTouch γ D w ǫ2 mγD w ǫ2 LightTouch (lnm)γ D w ǫ2 (lnm)γD w ǫ2 + m(lnm) /2γ /2D3 w ǫ/2 Projected SGD D 2 w ǫ2 N/A (projection) Online Frank-Wolfe D 3 w ǫ3 N/A (linear optimization) LLO-FW dν D w ǫ2 N/A (local linear oracle) then: E [ ‖Πg(w̄)− w‖2 ] ≤ E [ ‖w̄ − w‖2 ] ≤ ǫ, where w∗ = argmin{w∈W:∀i.gi(w)≤0} f(w) is the optimal constraint-satisfying reference vector. Proof. In Appendix D. Notice that the above theorem bounds not the suboptimality of Πg(w̄), but rather its squared Euclidean distance from w∗, for which reason the denominator of the highest order term depends on λ rather than λ. Like Theorem 1 in the non-strongly convex case, the dominant terms above, both in terms of the total number of iterations and number of constraint checks, match the usual 1/ǫ convergence rate for unconstrained strongly-convex SGD with an additional γ lnm factor, while the lower-order terms have a worse m-dependence. As before, fewer constraint checks will be performed per iteration as ǫ shrinks, reaching a constant number (on average) once ǫ is on the order of 1/m. 4 Theoretical Comparison Table 2 compares upper bounds on the convergence rates and per-iteration costs when applied to a convex (but not necessarily strongly convex) problem for LightTouch, FullTouch, projected SGD, the online Frank-Wolfe algorithm of Hazan and Kale [2012], and a Frank-Wolfe-like online algorithm for optimization over a polytope [Garber and Hazan, 2013].",
      "startOffset" : 8,
      "endOffset" : 1593
    }, {
      "referenceID" : 11,
      "context" : "6 Experiments We validated the performance of our practical variant of LightTouch (Algorithm 4) on a YouTube ranking problem in the style of Joachims [2002], in which the task is to predict what a user will watch next, given that they have just viewed a certain video.",
      "startOffset" : 141,
      "endOffset" : 157
    }, {
      "referenceID" : 21,
      "context" : "ProjectedSGD We implemented Euclidean projections onto lattice monotonicity constraints using IPOPT [Wächter and Biegler, 2006] to optimize the resulting sparse 4096-dimensional quadratic program.",
      "startOffset" : 100,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "ApproxSGD This is an approximate projected SGD implementation using the fast approximate update procedure described in Gupta et al. [2016], which is an active set method that, starting from the current iterate, moves along the boundary of the feasible region, adding constraints to the active set as they are encountered, until the desired step is exhausted (this is reminiscent of the local linear oracles considered by Garber and Hazan [2013]).",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "[2016], which is an active set method that, starting from the current iterate, moves along the boundary of the feasible region, adding constraints to the active set as they are encountered, until the desired step is exhausted (this is reminiscent of the local linear oracles considered by Garber and Hazan [2013]).",
      "startOffset" : 289,
      "endOffset" : 313
    }, {
      "referenceID" : 13,
      "context" : "7 Conclusions We have proposed an efficient strategy for large-scale heavily constrained optimization, building on the work of Mahdavi et al. [2012], and analyze its performance, demonstrating that, asymptotically, our approach requires many fewer constraint checks in order to converge.",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "∥ α∗ 1− δσ Probability that σ bound holds 1− δσw Probability that σw bound holds 1− δσα Probability that σα bound holds A Mirror Descent Mirror descent [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] is a meta-algorithm for stochastic optimization (more generally, online regret minimization) which performs gradient updates with respect to a meta-parameter, the distance generating function (d.g.f.). The two most widely-used d.g.f.s are the squared Euclidean norm and negative Shannon entropy, for which the resulting MD instantiations are stochastic gradient descent (SGD) and a multiplicative updating algorithm, respectively. These are precisely the two d.g.f.s which our constrained algorithm will use for the updates of w and p. We’ll here give a number of results which differ only slightly from “standard” ones, beginning with a statement of an online MD bound adapted from Srebro et al. [2011]: Theorem 3.",
      "startOffset" : 181,
      "endOffset" : 910
    }, {
      "referenceID" : 14,
      "context" : "The convergence rate will be determined from a saddle-point bound, which we derive from Corollary 2 by following Nemirovski et al. [2009], Rakhlin and Sridharan [2013], and simply applying it twice: Corollary 3.",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "The convergence rate will be determined from a saddle-point bound, which we derive from Corollary 2 by following Nemirovski et al. [2009], Rakhlin and Sridharan [2013], and simply applying it twice: Corollary 3.",
      "startOffset" : 113,
      "endOffset" : 168
    }, {
      "referenceID" : 14,
      "context" : "This is a convex-concave saddle-point problem, which we will optimize by playing two convex optimization algorithms against each other, as in Nemirovski et al. [2009], Rakhlin and Sridharan [2013].",
      "startOffset" : 142,
      "endOffset" : 167
    }, {
      "referenceID" : 14,
      "context" : "This is a convex-concave saddle-point problem, which we will optimize by playing two convex optimization algorithms against each other, as in Nemirovski et al. [2009], Rakhlin and Sridharan [2013]. By Corollary 2, with probability 1− δσw − δ and 1− δσα − δ, respectively: 1 T T ∑",
      "startOffset" : 142,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "Like the algorithm itself, the convergence rate is little different from that found by Mahdavi et al. [2012] (aside from the bound on ‖w̄ −Πg(w̄)‖2), and is included here only for completeness.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : "2 Analysis of LightTouch Because we use the reduced-variance algorithm of Johnson and Zhang [2013], and therefore update the remembered gradient μ one random coordinate at a time, we must first bound the maximum number of iterations over which a coordinate can go un-updated: Lemma 4.",
      "startOffset" : 74,
      "endOffset" : 99
    } ],
    "year" : 2016,
    "abstractText" : "Minimizing empirical risk subject to a set of constraints can be a useful strategy for learning restricted classes of functions, such as monotonic functions, submodular functions, classifiers that guarantee a certain class label for some subset of examples, etc. However, these restrictions may result in a very large number of constraints. Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a compelling trade-off between per-iteration work and the number of iterations needed on problems with a large number of constraints.",
    "creator" : "LaTeX with hyperref package"
  }
}