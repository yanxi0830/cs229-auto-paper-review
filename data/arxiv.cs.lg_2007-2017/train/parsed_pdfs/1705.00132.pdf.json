{
  "name" : "1705.00132.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning against Expert Automata",
    "authors" : [ "Mehryar Mohri", "Scott Yang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We give a series of algorithms for this problem, including an automata-based algorithm extending weightedmajority and more efficient algorithms based on the notion of failure transitions. We further present efficient algorithms based on a compact approximation of the competitor automaton, in particular efficient n-gram models obtained by minimizing the Rényi divergence, and present an extensive study of the approximation properties of such models. We also extend our algorithms and results to the framework of sleeping experts. Finally, we describe the extension of our approximation methods to online convex optimization and a general mirror descent setting."
    }, {
      "heading" : "1 Introduction",
      "text" : "Online learning is a powerful and flexible model for sequential prediction. Online learning algorithms are typically computationally efficient and benefit from strong guarantees even when the learner has only limited information about her environment.\nWithin the online learning framework, the setting of prediction with expert advice has received widespread attention [Littlestone and Warmuth, 1994, Cesa-Bianchi and Lugosi, 2006, Cesa-Bianchi et al., 2007]. In this setting, the algorithm maintains a distribution over a set of experts, or selects an expert from an implicitly maintained distribution. At each round, the loss assigned to each expert is revealed. The algorithm incurs the expected loss over the experts and then updates her distribution on the set of experts. The objective of the learner is to minimize his expected regret, which is defined as the cumulative loss of the algorithm minus the cumulative loss of the best expert chosen in hindsight.\nHowever, this benchmark is only significant when the best expert is expected to perform well. When this is not the case, then the learner may still play poorly. As an example, it may be that no single baseball team has performed well over all seasons in the past few years. Instead, different teams may have dominated over different time periods. This has led to a definition of regret against the best sequence of experts with k shifts in the seminal work of Herbster and Warmuth [1998] on tracking the best expert. The authors showed that there exists an efficient on-line learning algorithm for this setting with favorable regret guarantees.\nThis work has subsequently been improved to account for broader expert classes [Gyorgy et al., 2012], to deal with unknown parameters [Monteleoni and Jaakkola, 2003], and has been further generalized [Cesa-Bianchi et al., 2012, Vovk, 1999]. Another approach for handling dynamic environments has consisted of designing algorithms that guarantee small regret over any subinterval during the course of play. This notion coined as adaptive regret by Hazan and Seshadhri [2009] has been subsequently strengthened and generalized [Daniely et al., 2015, Adamskiy et al., 2012]. Remarkably, it was shown by Adamskiy et al. [2012] that the algorithm designed by Herbster and Warmuth [1998] is also optimal for adaptive regret. Koolen and de Rooij [2013] described a Bayesian framework for online learning where the learner samples from a distribution of expert sequences and predicts according to the prediction of that expert sequence. They showed how the algorithms designed for k-shifting regret, e.g. [Herbster and Warmuth, 1998, Monteleoni and Jaakkola, 2003], can be interpreted as specific priors in this formulation. There has also been work deriving guarantees in the bandit setting when the losses are stochastic [Besbes et al., 2014, Wei et al., 2016]. ∗Courant Institute and Google Research †Courant Institute\nar X\niv :1\n70 5.\n00 13\n2v 1\n[ cs\n.L G\n] 2\n9 A\npr 2\n01 7\nThe general problem of online convex optimization in the presence of non-stationary environments has also been studied by many researchers. One perspective has been the design of algorithms that maintain a guarantee against sequences that do not vary too much [Mokhtari et al., 2016, Shahrampour and Jadbabaie, 2016, Jadbabaie et al., 2015, Besbes et al., 2015]. Another assumes that the learner has access to a dynamical model that is able to capture the benchmark sequence [Hall and Willett, 2013]. György and Szepesvári [2016] reinterpreted the framework of Hall and Willett [2013] to recover and extend the results of Herbster and Warmuth [1998].\nIn this paper, we significantly generalize the framework just described and consider prediction with expert advice in a setting where the learner’s cumulative loss is compared against that of sequences represented by an arbitrary weighted family of sequences. We model this family using a weighted finite automaton (WFA). This strictly generalizes the notion of k-shifting regret and extends it to the notion of regret against a WFA.\nMeasuring regret against an automaton is both natural and flexible. In fact, it may often be sensible to learn the set of competitor sequences using data before competing against it. For instance, the competitor automaton could be a language model trained over best sequences of baseball teams in the past. Moreover, the competitor automaton could be learned and reset incrementally. After each epoch, we could choose to learn a new competitor model and seek to perform well against that.\nWe show that not only it is possible to achieve favorable regret against a WFA but that there exist computationally efficient algorithms to achieve that. We give a series of algorithms for this problem. Our first algorithm (Section 6) is an automata-based algorithm extending weighted-majority and using automata operations such as composition and shortest-distance; its computational cost is exponentially better than that of a naı̈ve method.\nWe further present efficient algorithms based on a compact approximation of the competitor automaton (Section 7), in particular efficient n-gram models obtained by minimizing the Rényi divergence, and present an extensive study of the approximation properties of such models. We also show how existing algorithms for minimizing k-shifting regret can be recovered by learning a Maximum-Likelihood bigram language model over the k-shifting competitor automaton. To the best of our knowledge, this is the first instance of recovering the algorithms of Herbster and Warmuth [1998] by way of solely focusing on minimizing the k-shifting regret. Since approximating the competitor automaton is subject to a trade-off between computational efficiency and approximation accuracy, we also design a model selection algorithm adapted to this problem.\nWe further improve that algorithm by using the notion of failure transitions (ϕ-transitions) for a more compact and therefore more efficient automata representation. Here, we design a new algorithm (Section 9) that can convert any weighted finite automaton into a weighted finite automaton with ϕ-transitions (ϕ-WFA). We then extend the classical composition and shortest-distance algorithms for WFAs to the setting of ϕ-WFAs. The shortest-distance algorithm is designed by extending the probability semiring structure of the ϕ-WFA to that of a ring. We show that if the number of consecutive ϕ-transitions is not too large, then these algorithms have a computational complexity that is comparable to those for standard WFAs. At the same time, our conversion algorithm can dramatically reduce the size of a WFA.\nWe then extend the results above to the sleeping expert setting [Freund et al., 1997], where the learner may not have access to advice from every expert at each round (Section 11). Finally, we extend the ideas for prediction with expert advice to online convex optimization and a general mirror descent setting (Section 12). Here, we describe a related framework that parallels the previous discussion and also recovers existing algorithms for k-shifting regret."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "A weighted finite automaton (WFA) is a finite automaton whose transitions, initial, and final states additionally carry some weights. For our purpose, the weights belong to the probability semiring (R+ ∪ {+∞},+,×, 0, 1):1 we use multiplication to compute path weights by taking the product of the transition weights, and use addition to compute the weight of any string x by taking the sum of the weights of all accepted paths labeled with x. We assume that all automata are deterministic, so that for any state q and label a, there exists at most one transition leaving q labeled with a.\nFor a WFA A, we denote by QA its finite set of states, by IA ∈ QA its initial states, by FA ⊆ QA its final states, and by EA its finite set of transitions, which are elements of QA × Σ× R+ ×QA, where Σ is a finite alphabet. We also denote by λ : IA → R+ an initial weight function, and by ρ : FA → R+ a final weight function. This allows us to define a WFA as a 7-tuple: A = (Σ, QA, IA, FA, EA, λA, ρA).\n1For numerical stability, in practice, the computations should be performed in the log-semiring: (R ∪ {−∞,+∞},⊕log,+,+∞, 0), where ⊕log is defined by x⊕log y = − log(e−x + e−y), which is isomorphic to the probability semiring under the mapping w 7→ − log(w).\nGiven a transition e ∈ EA, we denote by w[e] ∈ R+ its weight. A path π in A is an element of E∗A with consecutive transitions. We denote by src[π] the source or origin of the path and by dest[π] its destination. The weight of a path in A is defined to be the product of the weights of its constituent transitions, thus the weight of path π = e1 . . . en is w[π] = ∏n i=1 w[ei].\nFor any string x ∈ Σ∗, let P (IA, x, FA) denote the set of paths accepted by A and labeled with x. A WFA A over an alphabet Σ defines a function, which we abusively also denote by A, mapping a string x ∈ Σ∗ to R+ ∪ {+∞} and defined as follows:\n∀x ∈ Σ∗, A(x) = ∑\nπ∈P (IA,x,FA)\nw[π]λA(src[π])ρA(dest[π]), (1)\nwhere, by convention, A(x) = 0 if P (IA, x, FA) = ∅. Thus, A(x) is the sum of the weights of all paths accepted by A and labeled with string x, multiplied by the weight of the initial and final states. In the probability semiring, the order of the terms in the sum does not matter, and the quantity is well defined [Mohri, 2009]. The size of a WFA is denoted by |A| and defined as the sum of the number of states and the number of transitions of A: |A| = |QA|+ |EA|.\nFor any state q ∈ Q, we use E[q] to denote the outgoing transitions of q, and more generally, E[Q′] = ∪q∈Q′E[q] to denote the set of transitions leaving states in Q′ ⊆ Q. Given a label a ∈ Σ, we also denote by E[q, a] the set of outgoing transitions of q with label a, and by E[Q′, a] the set of transitions leaving states in Q′ with label a. Similarly, we let ER[q] denote the incoming transitions of q and define, ER[Q′] = ∪q∈Q′ER[q], ER[q, a], and ER[Q′, a] analogously.\nFor any state q ∈ Q and any symbol a ∈ Σ, we denote by δ[q, a] the set of states reachable from q by reading label a. By overloading notation, this implies that δ[q, a] = dest[E[q, a]]. We also define δ[Q′, a] = ∪q∈Q′δ[q, a], δ[q] = ∪a∈Σδ[q, a] and δ[Q′] = ∪q∈Q′δ[q]. Similar to E[·] and ER[·], we also denote by δR[q, a] the set of states with transitions labeled by a that lead into state q, along with δR[Q′, a], δR[q], and δR[Q′] by analogy.\nGiven any automaton A, we denote by AR its reversal, that is the automaton derived from A by reverting the direction of each transition and by swapping initial and final state sets. Thus, the strings accepted by AR are the mirror images of those accepted by A. Moreover, the corresponding paths share the same weight."
    }, {
      "heading" : "3 Learning problem",
      "text" : "We consider the setting of prediction with expert advice, which can be seen as a sequential game over T rounds. Let Σ = {a1, a2, . . . , aN} be a set of N experts. At each round t, the learner specifies a probability distribution pt over the set of experts Σ, samples an expert it ∼ pt from the distribution, receives the loss vector lt ∈ [0, 1]N , and incurs the specific loss lt[it]. The learner’s goal is to minimize her regret, that is the difference between the cumulative expected loss of the learner’s algorithm and the best loss corresponding to some benchmark competitor class. The standard benchmark in this setting is that of static experts, leading to the following notion of (expected) regret:\nRegT (A,Σ) = max a∈Σ T∑ t=1 pt · lt − T∑ t=1 lt[a]. (2)\nHowever, static experts may be too weak a benchmark in many settings where the data is potentially non-stationary and where no single expert is accurate or effective during all time segments of the game. Thus, instead, we consider the regret of the learner against a set of sequences of experts represented by an automaton. Specifically, let C be a weighted finite automaton (WFA) over the semiring (R+,+,×, 0, 1) with the alphabet Σ. C accepts a (potentially infinite) set of sequences of experts of arbitrary length. For instance, C may be the set of expert sequences that use each expert once, or the set of sequences that use at most two experts. Let ST be the weighted finite automaton that accepts all sequences of length T and assigns a weight of one to every transition. Specifically, we define QST = {t}t∈[0,T ], IST = {0}, FST = {T}, and EST = {(t− 1, k, 1, t)}t∈[1,T ],k∈Σ. This automaton is illustrated in Figure 1. Notice that ST is acyclic.\nThen, CT = C ∩ ST , where ∩ denotes the intersection operation between automata, will define the set of expert sequences that our learning algorithm will compare against. Since CT accepts only a finite number of sequences, we can normalize the automaton CT so that the total weight over all expert sequences of length T is one. Let pCT be the probability distribution over sequences accepted by CT defined by: pCT (z T 1 ) =\nw(zT1 )∑ z̃T1 ∈CT w(z̃T1 ) . We now define the\nweighted regret of algorithm A against automaton C up to time T as:\nRegT (A,C) = max zT1 ∈CT T∑ t=1 pt · lt − T∑ t=1 lt[zt]− log ( pCT (z T 1 )|CT | ) , (3)\nwhere |CT | is the number of paths accepted by CT . This notion of regret uses sequences in CT = C ∩ ST as a benchmark, and penalizes the performance of an algorithm against different expert sequences based on the learner’s prior characterization of the importance of each sequence. |CT | is a normalization factor added so that if the automaton’s weight is uniform over all sequences, then the weighted regret is consistent with the standard notion of regret. To the best of our knowledge, previous work has not considered prediction with expert advice with a benchmark loss that differs over competitors.\nTo compare our work with previous results, we also define the unweighted regret of algorithm A against automaton C up to time T as:\nReg0T (A,C) = max zT1 ∈CT T∑ t=1 pt · lt − T∑ t=1 lt[zt]. (4)\nNote that the automaton C can still have arbitrary weights, although they play no role in this notion of regret. When C has uniform weights, then the unweighted regret and weighted regret are identical.\nNotice that if we associate with Σ the automaton accepting constant sequences and with uniform weights over all sequences, then this definition of regret is consistent with the expression RegT (A,Σ) in equation 2.\nAs a running example, the k-shifting expert problem can be cast as the automaton Ck-shift with uniform weights given by Figure 2. Then, the (weighted or unweighted) regret of an algorithm A against Ck-shift coincides with the definition of k-shifting regret studied by Herbster and Warmuth [1998]:\nCk-shift =\n{ z ∈ Σ∗ :\n∞∑ t=0 1zt+1 6=zt = k\n} ."
    }, {
      "heading" : "4 Naı̈ve algorithm",
      "text" : "A well-known algorithm for minimizing static regret in the prediction with expert advice setting is the weighted majority algorithm [Littlestone and Warmuth, 1994]. The algorithm maintains a distribution over experts that is proportional to the exponential weight of the cumulative loss of the experts: pt[i] ∝ e−η ∑t s=1 lt[i] for some η > 0.\nWe can extend this idea to path experts and design an algorithm that minimizes the unweighted and weighted regret. Specifically, suppose we enumerate the paths accepted by CT by {πj}Kj=1. At each round t, each path chooses an expert πj,t ∈ Σ, and can be attributed the loss lt[πj,t]. Thus, we can define a weight distribution over the experts using the formula: p̃t[j] ∝ e−η ∑t s=1 ls[πj,s]. We can then convert this into a distribution over experts using the formula:\npt[i] ∝ ∑K j=1 p̃t[j]1πj,t=i. We call this algorithm PATH-BASED WEIGHTED MAJORITY (PBWM). Its pseudocode is presented in Algorithm 1. The following guarantee holds for the regret of PBWM, which is proven in Appendix 14.\nTheorem 1 (Regret Bounds for Path-Based Weighted Majority). Let ṽ1,j = 1 for every j ∈ [K] be the initial path weights in the PBWM algorithm. Then, the unweighted regret of PBWM is bounded as follows:\nReg0T (PBWM,C) ≤ ηT + 1\nη log(|C ∩ ST |).\nAlternatively, let ṽ1,j = w[πj ]η for every j ∈ [K] be the initial path weights in the PBWM algorithm. Then, the weighted regret of PBWM is bounded as follows:\nRegT (PBWM,C) ≤ ηT + 1\nη log  K∑ j=1 w[πj ] η|C ∩ ST |η  . If we further assume that ∑K j=1 w[πj ] = 1, then the weighted regret is bounded as follows:\nRegT (PBWM,C) ≤ ηT + 1\nη log(|C ∩ ST |).\nIf we additionally assume that w is supported on more than a single path (i.e. | supp(w)| > 1), then there exists η∗ > 0 (decreasing as a function of T ) such that:\nRegT (PBWM,C) ≤ 2 √ THη∗(w)−Hη∗(w) + log(K).\nwhere Hη(w) = 11−η log (∑K j=1 w[πj ] η ) is the η-Rényi entropy.\nFor the weighted regret, note that since the η-Rényi entropy is non-negative and increasing in η [Van Erven and Harremos, 2014], the latter bound is at most\n2 √ TH0(w) + log(K) = 2 √ T log(| supp(w)|) + log(K).\nMoreover, if T is sufficiently large such that η∗ < 1 (e.g. T ≥ log(| supp(w)|), based on the proof of the theorem), and our path weights w are concentrated on only a subset of the paths A ⊂ [K] such that∑\nj∈AC w[πj ]∑ j∈A w[πj ] < (1− η∗),\nAlgorithm 1: PATH-BASED WEIGHTED MAJORITY(PBWM). Algorithm: PBWM(C ∩ ST , ṽ1), where ṽ1 ∈ RK+ is a vector of initial path weights. π ← C ∩ ST K ← |C ∩ ST | N ← |Σ| for j = 1 to K do\np̃1,j ← ṽ1,j∑K i=1 ṽ1,i\nfor j = 1 to N do p1,j ← ∑ i∈[1,K] : πi,1=j p̃1,i for t = 1 to T do it ←SAMPLE(pt) PLAY(it) RECEIVE(lt) for j = 1 to K do\nṽt+1,j ← ṽt,je−ηlt[πj,t] p̃t+1,j ← ṽt+1,j∑K i=1 ṽt+1,i\nfor j = 1 to N do pt+1,j ← ∑ i∈[1,K] : πi,t=j p̃t+1,i\nthen\nH∗η (w) = 1\n1− η∗ log  K∑ j=1 w[πj ] η∗  = 1 1− η∗ log ∑ j∈A w[πj ] η∗ + ∑ j∈Ac w[πj ] η∗  ≤ 1\n1− η∗ log ∑ j∈A w[πj ] η∗ + ∑j∈Ac w[πj ]η∗∑ j∈A w[πj ] η∗  ≤ 1\n1− η∗ log ∑ j∈A w[πj ] η∗ + ≤ 1 1− η∗ log(|A|) + .\nThus, the latter bound can be substantially tighter than the bound in terms of log(|C ∩ ST |)."
    }, {
      "heading" : "5 Automata operations",
      "text" : "While PBWM achieves a sublinear regret bound for both the weighted and unweighted regret, it can be computationally prohibitive. This is because the update at each iteration requires a summation over the number of paths in the competitor set, which can be exponential in the number of total rounds T . For instance, the total number of k-shifting experts is O (( T−1 k ) N(N − 1)k ) .\nTo design computationally efficient algorithms, we will exploit the properties of the competitor class automaton C. Note that any finite set of competitor paths can be represented by an automaton. But, different automata accepting the same set may lead to vastly different computational costs.\nOur algorithms make use of several common operations for weighted automata: composition, shortest-distance, and connection. In particular, we design an incremental version of the shortest-distance algorithm, which we call IncrementalShortestDistance (IncrSD)."
    }, {
      "heading" : "5.1 Composition/intersection",
      "text" : "Composition (or intersection) is an operation that combines two weighted automata into a new weighted automaton. The resulting automaton accepts the set of strings accepted by both input automata. The weight assigned to any string\nby the resulting composition is the product of the weights assigned by the input automata: A1 ◦A2(x) = A1(x)A2(x). A standard and efficient method for composing two weighted automata is to pair up matching transitions [Mohri, 2009]. States of A1 ◦A2 are identified with pairs of states of A1 and A2: Q ⊆ Q1 ×Q2, as are the set of initial and final states. Transitions are obtained by matching pairs of transitions from each weighted automaton and multiplying their weights following the result:(\nq1 a/w1−→ q′1, q2 a/w2−→ q′2 ) ⇒ (q1, q2) a/(w1w2)−→ (q′1, q′2).\nIn general, the space and time complexity of this composition is O(|A1||A2|), since in the worst case all transitions in A1 could be matched with transitions leaving A2. In our case, this would result in a complexity of O(|C||ST |) = O(|C|NT ). However, composition only actually constructs states and transitions that are reachable from the initial states I1 × I2. More generally, for an efficient implementation (in terms of data structure), the complexity is linear in the size of the output (disregarding data structure preprocessing).\nNow consider the incremental cost of creating states corresponding to q = (·, t+ 1) and the transitions reaching these states given that all states q = (·, s) with s ≤ t and their incoming transitions have been already created. The maximum number of transitions in this step is bounded by the total number of states reachable at time t multiplied by the maximum out-degree of any of these states: |QC,t|maxq∈QC,t |E[q]|, where QC,t = {q ∈ QC ×QST : q = (·, t)}. Thus, the total computational complexity of the composition of C with ST is in O (∑T t=1 |QC,t|maxq∈QC,t |E[q]| ) , where |E[q]| ≤ N and |QC,t| ≤ |C|. In general, we can expect both quantities to be much smaller."
    }, {
      "heading" : "5.2 Shortest-distance",
      "text" : "The second operation that we need is a single-source ‘shortest-distance’ computation over the semiring (R+ ∪ {+∞},+,×, 0, 1), that is, we wish to compute for any state q, α[q] defined by:\nα[q] = ∑\nπ∈P (IA,q)\nwA[π],\nwhere P (IA, q) is the set of paths from the initial states IA to q. For any acyclic automaton A, α can be computed in linear time using a general relaxation-based shortest-distance algorithm with a topological queue discipline [Mohri, 2009]. α can also be computed using other algorithms such as the Viterbi algorithm. In our context, this shortest-distance algorithm can be naturally modified to run in an incremental fashion and extended to the case of automata with failure transitions (Section 9). We provide the pseudocode of this subroutine as Algorithm 2, IncrementalShortestDistance (IncrSD). This will be a key ingredient for our computationally efficient learning algorithm."
    }, {
      "heading" : "5.3 Connection",
      "text" : "It is possible that some of the states created by composition will be non-accessible or non-coaccessible. A state q ∈ Q is non-accessible if there is no path from I to q. A state q ∈ Q is non-coaccessible if there is no path from q to F . Such states are called useless because they do not lie on accepting paths. Connection (or trimming) is a linear-time algorithm that removes these non-accessible states [Mohri, 2009]."
    }, {
      "heading" : "6 Automata-based algorithm",
      "text" : "We now have the tools to present Algorithm 3, AUTOMATAWEIGHTEDMAJORITY (AWM), an automata-based online learning algorithm. At the start of the algorithm, we compose the competitor class automaton C with ST . For computational efficiency, we apply a connection (or trimming) algorithm to prune useless states from the composed automaton. We then perform a shortest-distance computation backwards to compute β[q] the sum of the weights of all paths from q to the final states. At each time t, we use the current distribution pt to sample an expert it, receive the loss vector lt and incur loss lt[it]. We update the weights of the edges in C ∩ ST whose destination states have form (·, t) using these losses.\nWe then compute the flow of each edge e, flow[e], that is the sum of the weights of all paths that pass through e. In the semiring (R+,+,×, 0, 1) that we operate in, this flow can be computed as the product of three components:\nAlgorithm 2: INCREMENTALSHORTESTDISTANCE(IncrSD). Algorithm: INCRSD(C ∩ ST , t, α) CT ← C ∩ ST for each q ∈ QCT with q = (·, t) do\nα[q]← r[q]← 0 if t = 1 then\nfor each q ∈ ICT do α[q]← r[q]← 1\nQ ← ICT . else Q ← {q ∈ QCT : q = (·, t− 1)} while Q 6= ∅ do q ← HEAD(Q) DEQUEUE(Q) r̃ ← r[q] r[q]← 0 for each e ∈ ECT [q] do\nif α[dest[e]] 6= α[dest[e]] + (r̃w[e]) then α[dest[e]]← α[dest[e]] + (r̃w[e]) r[dest[e]]← r[dest[e]] + (r̃w[e]) if dest[e] /∈ Q then\nENQUEUE(Q,dest[e]) return α\nthe shortest-distance from the initial state to the source of e, α[src[e]], the weight of edge e, w[e], and the shortest distance from the destination state of e to the set of final states in C ∩ ST , β[dest[e]]. We then aggregate these flows by label (i.e. experts) and normalize them to get a distribution over the set of experts. Finally, since the weights of the transitions leading to states (·, t) have been modified, we update the shortest distances to their destination states using our incremental shortest-distance algorithm. The computational cost of this algorithm is linear in the number of these transitions. Note that other shortest-distances need not be modified.\nTheorem 2 guarantees that AWM performs the same update as PBWM and quantifies its computational cost.\nTheorem 2 (AUTOMATAWEIGHTEDMAJORITY Guarantee). The update made by AWM, pt+1 at round t, coincides with the update made by PBWM for unweighted regret when run with a normalized input C ∩ ST . Moreover, the computational complexity of AWM at each round t is O ( |E[QCT ,t]| ) and the total computational cost is\nO (∑T t=1 |E[QCT ,t]| ) .\nIf, after normalization, we multiplied the weight of every edge by η, then we would also recover the update as in PBWM for weighted regret.\nProof. If C ∩ ST is normalized as input to PBWM, then it coincides with the result of the normalization of wCT at the start of AWM. At any time t ∈ [T ], Flowt[ai] is maintained as ∑ e : dest[e]=(·,t),σ[e]=ai flow(e) that is the sum of the weights of all paths taking action ai ∈ Σ at time t. The weight of path π = e1e2 . . . eT is then:\nw[π] t∏ s=1 e−ηls[σ[es]] = w[π]e−η ∑t s=1 ls[σ[es]] = ṽt+1,j .\nAt the start of the algorithm, the normalization and shortest-distance computations take timeO (∑T t=1 |E[QCT ,t]| )\n. Computing the initial probability p1 takes time O(|Q1|). From time t = 1 to T , the loss update, flow computation, and flow normalization each take time O(|Qt|), and the incremental shortest-distance update takes time O (|E[QCT ,t]|). Thus, the per-iteration complexity is O (|E[QCT ,t]|), and the total computational complexity is O (∑T t=1 |E[QCT ,t]| ) .\nAlgorithm 3: AUTOMATAWEIGHTEDMAJORITY(AWM). Algorithm: AWM(C) CT ← C ∩ ST CT ← CONNECT(CT ) β ← SHORTESTDISTANCE((CT )R, F ) wCT [ICT ]← wCT [ICT ]/β[ICT ] α← 0 Flow0 ← 0 for each e ∈ ECT with dest[e] = (·, 1) do\nflow[e]← β[dest[e]] Flow0[σ[e]]← Flow0[σ[e]] + flow[e] Z0 ← Z0 + flow[e]\np1 ← Flow0/Z0 for t← 1 to T do\nit ←SAMPLE(pt) PLAY(it) RECEIVE(lt) Flowt ← 0 for each e ∈ ECT with dest[e] = (·, t) do\nw[e]← w[e]e−ηlt,σ[e] flow[e]← α[src[e]]w[e]β[dest[e]] Flowt[σ[e]]← Flowt[σ[e]] + flow[e] Zt ← Zt + flow[e].\npt+1 ← Flowt/Zt α← INCRSD(CT , t,α)\nNotice that the composition of the k-shifting experts competitor class with C given by Figure 2 results in Figure 3, so that the per-iteration complexity of AWM for the k-shifting automaton is O(N2k). This is substantially more efficient than the naı̈ve update, which was O (( T−1 k ) N(N − 1)k ) .\nIn general, the performance of AWM depends not only on the properties of the competitor class automaton but also on those of the resulting automaton after composition with ST . For instance, we could have also represented the k-shifting experts competitor class using the inefficient representation of Figure 3 (b), which would lead to a significantly higher per-iteration computational cost.\nIt should be noted that AWM can be seen as a generalization of the Expert Hidden Markov Model in [Koolen and de Rooij, 2013] to arbitrary losses. One major difference between their work and ours is the matter of framework and perspective. [Koolen and de Rooij, 2013] assume a Bayesian setting where the prior distribution over expert sequences is given and must be used. In our work, we assume the existence of a competitor automaton, but don’t necessarily need to sample from it to make a prediction. This will be crucial in the next section, where we design and use other automata to improve computational efficiency while preserving regret performance."
    }, {
      "heading" : "7 Approximation algorithms",
      "text" : "The algorithm described in Sections 6 was based on exploiting the properties of the competitor class automaton C. AWM was exact in that it performed the same computations as the naı̈ve algorithm, PBWM. If the resulting automaton after composition is still large, then the computational cost can still be prohibitive.\nIn this section, we take a different approach and no longer constrain ourselves to algorithms that perform precisely the same update as PBWM. Instead, we design algorithms to efficiently approximate the competitor set CT . An automaton that is compact and tightly approximates CT can lead to an algorithm that is efficient and achieves a favorable regret."
    }, {
      "heading" : "7.1 The effect of automata approximation on regret",
      "text" : "We first analyze the approximation cost of such an algorithm. For this, we will need to introduce the notion of∞-Rényi divergence [Rényi et al., 1961]. The∞-Rényi divergence between two distributions p and q over some set X is defined to be\nD∞(p||q) = sup x∈X log\n[ p(x)\nq(x)\n] . (5)\nTheorem 3 (Automata approximation cost). Assume that CT is normalized so that ∑ π∈CT wCT [π] = 1. If AWM is run with a weighted automaton Ĉ, then its unweighted regret is bounded as follows:\nReg0T (A,C) ≤ max π∈CT\nηT + 1\nη log\n( 1\nwCT [π]\n) + 1\nη D∞(pCT ||pĈT ),\nand its weighted regret is bounded as follows:\nRegT (A,C) ≤ ηT + 1\nη log ∑ π∈ĈT w ĈT [π]η|CT |η +D∞(pCT ||pĈT )\n≤ ηT + 1 η log(|CT |) + log ( |CT | |ĈT | ) .\nProof. The regret guarantee of AWM is based on the fact that it performs the same update as PBWM. By the proof of Theorem 1, the unweighted regret of the algorithm against π ∈ CT is bounded by:\nT∑ t=1 pt · lt − T∑ t=1 lt[zt] ≤ ηT + 1 η log\n( 1\nw ĈT [π]\n) .\nBy comparing the weight of the automaton used in the algorithm, ĈT , with the automaton used in the benchmark, CT , we obtain the bound:\nT∑ t=1 pt · lt − T∑ t=1 lt[zt] ≤ ηT + 1 η log ( 1 wCT [π] ) + 1 η log ( wCT [π] w ĈT [π] ) .\nThis allows us to bound the unweighted regret by:\nReg0T (A,C) ≤ max π∈CT\nηT + 1\nη log\n( 1\nwCT [π]\n) + 1\nη log\n( wCT [π]\nw ĈT [π]\n) .\nSimilarly, for any π ∈ CT , the weighted regret of the algorithm against π is bounded by:\nT∑ t=1 pt · lt − T∑ t=1 lt[zt]− log ( w ĈT (π)|CT | ) ≤ ηT + 1 η log ∑ π∈ĈT w ĈT [π]η|CT |η  ,\nso that\nT∑ t=1 pt · lt − T∑ t=1 lt[zt]− log ( wCT (π)|CT | ) ≤ ηT + 1\nη log ∑ π∈ĈT w ĈT [π]η|CT |η + log(wCT [π] w ĈT [π] ) .\nThis then implies that:\nRegT (A,C) ≤ ηT + 1\nη log ∑ π∈ĈT w ĈT [π]η|CT |η + log( sup π∈CT wCT [π] w ĈT [π] ) .\nFurthermore, as in the proof of Theorem 1, we can further bound the above quantity by:\nηT + 1\nη log ∑ π∈ĈT |CT |η |ĈT |η + log( sup π∈CT wCT [π] w ĈT [π] ) .\nTheorem 3 quantifies the extra cost of using a surrogate automaton during learning as 1ηD∞(pCT ||pĈT ) for unweighted regret and as D∞(pCT ||pĈT ) for weighted regret. This is tight, since it is always possible that the best path in hindsight in the regret definition may also be the one maximizing the log-ratio.\nThis suggests that using an approximate automaton Ĉ for which the size of ĈT is favorable and D∞(wCT ||wĈT ) is small would both lead to an efficient algorithm and a favorable regret guarantee.\nA consequence of this bound is a general algorithm. Given a family of automata C with a relatively small number of states and transitions, the algorithm can be formulated as solving the problem:\nmin ĈT∈C D∞(pCT ||pĈT ). (6)\nWhen the set of distributions associated to C is convex, this is a convex optimization problem, since q 7→ log(p/q) is a convex function and the supremum of convex functions is convex. The choice of the set C is subject to a trade-off: approximation accuracy versus computational efficiency of using elements in C. This raises a model selection question for which we discuss in detail a solution in Section 7.5: given a sequence of families (Cn)n∈N with growing complexity and computational cost, the problem consists of selecting the best n.\nFor general choices of C, this problem can be complex and relates to other standard automata learning problems [Pitt and Warmuth, 1993, Balle and Mohri, 2012, Hsu et al., 2012, Balle and Mohri, 2015]. In the following, we will consider the case where the family C of weighted automata is that of n-gram models, for which we can upper bound the computational complexity.\n7.2 n-gram models An n-gram language model is a Markovian model of order (n− 1) defined over Σ∗. By the chain rule of probability, pA[x] = ∏T t=1 pA[xt|x t−1 1 ]. For an n-gram model, conditioning is done only on a history of length (n − 1), thus\npA[x] = ∏T t=1 p[xt|x t−1 t−n+1]. An n-gram model can be compactly represented by a weighted automaton with states Q = Σn−1 encoding each possible history, and with one transition corresponding to each n-gram x1 · · ·xn ∈ Σn from state xn−11 to state x n 2 and labeled with xn. The natural automata representation for n-gram models are stochastic\nautomata, where the weights of the outgoing transitions for any state are non-negative and sum up to one. Any probabilistic weighted automaton can be converted into a stochastic automaton using the weight-pushing algorithm [Mohri, 2009], which takes linear time and consists of a shortest-distance computation combined with a reweighting of the transition weights, initial weights, and final weights in a way that pushes the weights towards the initial states.\nEach state in an n-gram model encodes a particular history, so that for stochastic automata, the outgoing transition weights are the conditional probabilities given that history. Since these probabilities define an n-gram model, an n-gram model can be interpreted as a product of simplices.\nSee Figure 4 for an example of an n-gram language model. Language models of this form are commonly used in language and speech processing [Jelinek and Mercer, 1980, Katz, 1987, Ney et al., 1994, Chen and Goodman, 1998, Allauzen et al., 2003]. In these applications, the models are typically smoothed since they are trained on a finite sample. In our case, smoothing will not be needed since we can train directly on CT , which we interpret as the full language.\nOne key advantage of n-gram models in this context is that the per-iteration complexity can be bounded in terms of the number of symbols. Since an n-gram model has at most |Σ|n−1 states, its per-iteration computational cost is in O ( |Σ|n ) as each state can take one of |Σ| possible transitions. For n small, this can be very advantageous compared to the original CT , since the maximum out-degree of states reached by strings of length t in the latter may be very large. In what follows, we denote by Pn the family of n-gram language models. Since Pn can be written as the product of simplices, it is therefore a convex set. If we denote by pCT the target distribution of sequences, then we can apply well-known techniques to learn a language model over the language CT .\n7.3 Maximum likelihood n-gram models A standard method for learning n-gram models is via maximum likelihood, which is equivalent to minimizing the relatively entropy between the target distribution pCT and the language model. This is defined by the optimization problem:\nmin pAn∈Pn D(pCT ||pAn), (7)\nwhere, for any two distributions p and q, D(p||q) = ∑ x p[x] log ( p[x] q[x] ) . Observe that, in general, the solution of this optimization problem is not guaranteed to provide an upper bound on the∞-Rényi divergence since the∞-Rényi divergence is an upper bound on the relative entropy2. However, as we shall see later (e.g., Theorem 4), in some cases, maximum likelihood solutions do benefit from favorable guarantees.\nMaximum likelihood n-gram solutions in particular are simple. For standard text, the weight of each transition is the frequency of appearance of the corresponding n-gram in the text. For a probabilistic CT , the weight can be similarly\n2The relative entropy also coincides with the 1-Rényi divergence\nobtained from the expected count of the n-gram in the paths of CT , where the expectation is taken over the probability distribution defined by CT and can be computed efficiently [Allauzen et al., 2003].\nMaximum likelihood n-gram models can further benefit from ϕ-conversion using the algorithms presented in Section 9. This can reduce the size of An and improve its computational efficiency without affecting its accuracy.\nAs an example, we can compute the bigram approximation to the k-shifting automaton. Remarkably, this will coincide with the FIXED-SHARE algorithm of Herbster and Warmuth [1998]. Thus, we can view and motivate the design of FIXED-SHARE as a bigram approximation of the desired competitor automaton, that is the family of k-shifting sequences.\nTheorem 4 (Bigram approximation of k-shifting automaton). Let CT be the k-shifting automaton for some k. Then, the bigram model A2 obtained using relative entropy satisfies:\npA2 [a1a2] = 1\nN\n( 1− k\n(T − 1)\n) 1a1=a2 +\nk\n(T − 1)(N − 1)N 1a1 6=a2 .\nMoreover, the approximation error is bounded as follows:\nsup π∈CT log\n( pCT [π]\npA2 [π]\n) ≤ − log ( 1− 2e− 112k ) .\nProof. Let a1, a2 ∈ Σ. Then\npA2 [a2|a1] = pA2 [a2|a1, a2 = a1] pA2 [a2 = a1] + pA2 [a2|a1, a2 6= a1] pA2 [a2 6= a1]\nConsider first the case where a2 = a1. Then pA2 [a2|a1, a2 = a1] = 1, and pA2 [a2 = a1] is the expected number of times that we see label a2 agreeing with label a1. Since pCT is uniform for the k-shifting automaton, the expected counts are pure counts, and the probability that we see two consecutive labels agreeing is 1 − kT−1 . Now, consider the case where a2 6= a1. By symmetry, pA2 [a2|a1, a2 6= a1] = 1N−1 , since a2 is equally likely to be any of the other N − 1 labels. Moreover, pA2 [a2 6= a1] = kT−1 .\nThus, the following holds:\npA2 [a2|a1] = 1 N − 1 k T − 1 1a1 6=a2 +\n( 1− k\nT − 1\n) 1a1=a2 .\nMoreover, by symmetry, we can write pA2 [a1] = 1 N , and therefore,\npA2 [a1a2] = pA2 [a2|a1]pA2 [a1] = k\nN(N − 1)(T − 1) 1a1 6=a2 + ( T − 1− k N(T − 1) ) 1a1=a2 .\nTo bound the approximation error in the regret, notice that for any string x accepted by CT :\nlog\n( pCT [x]\npA2 [x] ) = log ( 1\npA2 [x]\n) − log(|CT |\n(since the k-shifting automaton has uniform weights)\n= log\n( 1\npA2 [z = x|z ∈ CT ]pA2 [z ∈ CT ] + pA2 [z = x|z /∈ CT ]pA2 [z /∈ CT ] ) − log(|CT |)\n= log\n( 1\n1 |CT |pA2 [z ∈ CT ] + pA2 [z = x|z /∈ CT ]pA2 [z /∈ CT ]\n) − log(|CT |)\n(since pA2 is uniform on CT ) ≤ log (\n|CT | pA2 [z ∈ CT ]\n) − log(|CT |)\n= log\n( 1\npA2 [z ∈ CT ]\n) .\nThe probability that a string z is accepted by CT (under the distribution pA2) is equal to the probability that it admits exactly k shifts. Let ξt = 1{z shifts from t− 1 to t} be a random variable indicating whether there is a shift at the t-th symbol in sequence z. This is a Bernoulli random variable bounded by 1 with mean kT−1 and variance k T−1 (1− k T−1 ). Since each shift occurs with probability kT−1 , we can use Sanov’s theorem [Sanov, 1957] to write the following bound:\npA2 [z /∈ CT ] = pA2 [∣∣∣∣∣ T∑ t=2 ξt − k ∣∣∣∣∣ > 12 ] ≤ 2e−(T−1)u,\nwhere\nu = (T − 1) min { D ( k + (1/2)\nT − 1 || k T − 1\n) , D ( k − (1/2) T − 1 || k T − 1 )} ,\nand where D(p||q) is the binary entropy. We will now use the following technical lemma, which we prove in Appendix 14.\nLemma 1 (Binary entropy bound). The following inequalities hold for the binary entropy: −D ( k + (1/2)\nT − 1 || k T − 1\n) ≤ − 1\n12k(T − 1)\n−D ((\n1− 1 2k\n) k\nT − 1 || k T − 1\n) ≤ − 1 4k2\nk T−1 2 = − 1 8k(T − 1) .\nUsing this result, we can further bound the approximation error in the regret bound by:\nlog\n( 1\npA2 [z ∈ CT ]\n) ≤ log ( 1\n1− 2e− 112k\n) = − log ( 1− 2e− 112k ) .\nTo the best of our knowledge, this is the first framework that motivates the design of FIXED-SHARE with a focus on minimizing tracking regret. Other works that have recovered FIXED-SHARE (e.g. [Cesa-Bianchi et al., 2012, Koolen and de Rooij, 2013, György and Szepesvári, 2016]) have generally viewed the algorithm itself as the main focus.\nAs already pointed out, the bigram approximation of the k-shifting automaton has a per-iteration computational cost of O(|Σ|2) transitions. At each time t, every state of the form (·, t) corresponds to one of the labels in Σ, and admits a transition to a state in (·, t+ 1) corresponding to the same label as well as |Σ| − 1 transitions to states corresponding to\nAlgorithm 4: PROD-EG. Algorithm: PROD-EG(p1 ∈ (∆N )m, η) for t = 1, 2, . . . , T do\nPLAY(pt) RECEIVE(∇f(pt)) for j = 1, 2, . . . ,m do\nfor i = 1, 2, . . . , N do\npt+1,j(i) = pt,je −η ∂f ∂pj(i) (pt,j)\nother labels. In Section 9, we introduce the notion of ϕ-transition and design algorithms that can reduce the size of an automaton while preserving its language and weights. Using our new algorithm ϕ-CONVERT on A2 ∩ ST , we can significantly reduce the number of transitions so that there are only O(|Σ|) states and transitions reachable at every time. See Figure 5 for an illustration. The computational cost of using ϕ-AWM with this new ϕ-automaton coincides with the one described originally in [Herbster and Warmuth, 1998].\nOur derivation of FIXED-SHARE also allows us to naturally generalize the setting of standard k-shifting experts to k-shifting experts with non-uniform weights. Specifically, consider the case where CT is an automaton accepting up to k-shifts but where the shifts now occur with probability pCT [a2|a1, a1 6= a2] 6= 1N−11{a2 6=a1}. Since the bigram approximation will remain exact on CT , we recover the exact same guarantee as in Theorem 4.\nThe proof technique of Theorem 4 is illustrative because it reveals that the maximum likelihood n-gram model has low approximation error whenever (1) the model’s distribution is proportional to the distribution of CT on CT ’s support and (2) most of the model’s mass lies on the support of CT . When the automaton CT has uniform weights, then condition (1) is satisfied when the n-gram model is uniform on CT . This is true whenever all sequences in CT have the same set of n-gram counts, and every permutation of symbols over these counts is a sequence that lies in CT , which is the case for the k-shifting automaton. Condition (2) is satisfied when n is large enough, which necessarily exists since the distribution is exact for n = T . On the other hand, note that a unigram approximation would have satisfied condition (1) but not condition (2) for the k-shifting automaton.\n7.4 Minimum Rényi divergence n-gram models Unlike maximum likelihood n-gram models, which in general do not benefit from an approximation guarantee, in this section we discuss minimum Rényi divergence (or∞-Rényi divergence) n-gram models, which are directly obtained by solving Problem 6 over the family of n-grams models. This corresponds to the following optimization problem:\nmin pAn∈Pn D∞(pCT ||pAn) = min pAn∈Pn sup x∈CT log\n( pCT (x)\npAn(x)\n) . (8)\nThis is a convex optimization problem over a product of ∑n j=1 |Σ|n−j simplices. The problem can be solved using as an optimization algorithm an extension of the Exponentiated Gradient algorithm developed by Kivinen and Warmuth [1997], which we call PROD-EG. The pseudocode of the algorithm, which is based on a simple multiplicative update, is given in Algorithm 4. The following provides a general guarantee for the convergence of the algorithm. Its proof is provided in Appendix 14.\nTheorem 5 (PRODUCT-EXPONENTIATED GRADIENT (PROD-EG)). Let (∆N )m be the product of m (N − 1)-dimensional simplices, and let f : (∆N ) m → R be a loss function whose partial subgradients have absolute values all bounded by L. Let p1,j = 1N for j = 1, 2, . . . ,m. Then, PROD-EG benefits from the following guarantee:\nf\n( 1\nT T∑ t=1 pt\n) − f(p∗) ≤ 1\nηT m log(N) + η2L.\nFor the minimum Rényi divergence optimization problem (8), we can apply PROD-EG to the product of m =∑n j=1 |Σ|n−j simplices, each one corresponding to a conditional probability with a specific history. First, we remark that the subgradient of the maximum of a family of convex functions at a point can always be chosen from the\nsubgradient of the maximizing function at that point. Specifically, let {fα}α∈A be a family of convex functions, and let α(x) = argmaxα f(x). Then, it follows that\nmax α fα(x)−max α fα(y) ≥ fα(y)(x)− fα(y)(y) ≥ 〈∇fα(y)(y), x− y〉.\nLet π be the maximizing path of the minimum Rényi divergence objective. We will use the ∨ symbol to denote the maximum between two values, and the ∧ symbol the denote the minimum. We can then write\nlog\n( pCT (π)\np(π)\n) = pCT (π)− T∑ t=1 log ( 1 p(πt|πt−1t−n+1∨1) )\n= pCT (π)− T∑ t=1 ∑ zn∧t1 ∈Σn∧t 1πtt−n+1∨1=zn∧t1 log\n( 1\np(πt|πt−1t−n+1∨1)\n)\n= pCT (π)− T∑ t=1 ∑ zn∧t1 ∈Σn∧t 1πtt−n+1∨1=zn∧t1 log\n( 1\np(zn∧t|z(n∧t)−11 )\n)\n= pCT (π)− T∑ t=1 n∑ j=1 1j=n∧t ∑ zj1∈Σj 1πtt−n+1∨1=z j 1 log\n( 1\np(zj |zj−11 )\n)\n= pCT (π)− n∑ j=1 ∑ zj1∈Σj T∑ t=1 1j=n∧t1πtt−n+1∨1=z j 1 log\n( 1\np(zj |zj−11 )\n) ,\nso that its partial derivative with respect to p(zj |zj−11 ) is:\n∂d\n∂dp(zj |zj−11 ) log\n( pCT (π)\np(π)\n) = T∑ t=1 1j=n∧t1πtt−n+1∨1=z j 1 −1 p(zj |zj−11 ) .\nThus, by tuning PROD-EG with an adaptive learning rate\nηt ∝ 1√∑t\ns=1 ∥∥∥∇ log ( pCT (π(t))p(π(t))) )∥∥∥2∞ ,\nwhere π(t) = argmaxπ∈CT log ( pCT [π]\npt[π]\n) , we can derive the following guarantee for PROD-EG applied to the n-gram\napproximation problem.\nCorollary 1 (n-gram approximation guarantee). There exists an optimization algorithm outputting a sequence of conditional probabilities (pt)∞t=1 such that ( 1 T ∑T t=1 pt ) approximates the∞-Rényi optimal n-gram solution with the following guarantee:\nF\n( 1\nT T∑ t=1 pt\n) − F (p∗)\n≤\n√√√√√√√2Nn log(N) ∑T t=1 max j∈{1,...,n}\nzj1∈Σj\n∣∣∣∣ T∑ t=1 1j=n∧t1πtt−n+1∨1=z j 1\n1\npt(zj |zj−11 ) ∣∣∣∣ (N − 1)T 2 .\nEach iteration of PROD-EG admits a computational complexity that is linear in the dimension of the feature space. Since we have specified an n-gram model as the product of N\nn−1 N−1 simplices, the total per-iteration cost of solving the convex optimization problem is in O ( N(Nn−1) N−1 ) = O(Nn). Since the minimum Rényi divergence is not Lipschitz,\nthe maximizing ratio in the convergence guarantee may also become large when the choice of n is too small. In all cases, observe that this approximation problem can be solved offline.\nIf we restrict ourselves to CT with uniform weights and |Σ| = 2, then we can also provide an explicit solution for unigram automata. The solution is obtained from the paths with the smallest number of occurrences of each symbol, which can be straightforwardly found via a shortest-path algorithm in linear time.\nTheorem 6 (∞-Rényi divergence unigram solution for uniform weight competitors and |Σ| = 2). Assume that CT admits uniform weights over all paths and |Σ| = 2. For j ∈ {1, 2}, let n(aj) be the smallest number of occurrences of aj in a path in CT . For j 6= k, define\npk(aj) = max\n{ 1,\nn(aj) T−n(aj) } 1 + max { 1,\nn(aj) T−n(aj) } . Let k∗ be the solution of the following optimization problem:\nmax k∈{1,2} max j∈{1,2}\\{k}\nn(aj) log pk(aj) + [T − n(aj)] log (1− pk(aj)) .\nThen, pk∗ is the solution of the unigram∞-Rényi divergence optimization problem.\nProof. We seek a unigram distribution pA1,T that is a solution of:\nmin pA1∈P1 sup π∈CT log\n( pCT [π]\npA1 [π]\n) .\nSince CT admits uniform weights, pCT [π] = 1 |CT | , and since A1,T is a unigram automaton, log pA1 [π] can be expressed as follows: log pA1 [π] = nπ(a1) log p(a1) + [T − nπ(a1)] log (1− p(a1)) ,\nwhere p(aj) is the automaton’s weight on transitions labeled with aj and nπ(aj) is the count of aj in path π. Thus, the optimization problem is equivalent to the following problem:\n− max p(a1)∈[0,1] min π∈CT nπ(a1) log p(a1) + [T − nπ(a1)] log (1− p(a1)) .\nDenote the objective by F (p(a1), nπ(a1)). Then, the partial derivatives with respect to the label counts are given by\n∂F\n∂nπ(a1) = log p(a1)− log\n( 1− p(a1) ) .\nThus, ∂F∂nπ(a1) ≥ 0 if and only if p(a1) ≥ 1− p(a1). Furthermore, if p(a1) ≥ 1− p(a1), then the path π chosen in the optimization problem is the path with the minimal count of symbol a1. Similarly, if p(a2) ≥ 1− p(a2), then the path π chosen in the optimization problem is the path with minimal count of a2.\nSince we have either p(a1) ≥ p(a2) or vice versa (potentially both), we can write the optimization problem as:\n− max k∈{1,2} max p(aj)≥1−p(aj)\nj 6=k\nmin {nπ(aj)}j 6=k : π∈CT\nnπ(aj) log p(aj) + [T − nπ(aj)] log (1− p(aj)) .\nGiven k ∈ {1, 2}, let π(k) be the path that minimizes nπ(aj) over all π for j 6= k. Denote these counts nπ(k)(aj) by n(aj). Then we can rewrite the objective as:\n− max k=1,2,...,N max p(aj)≥1−p(aj)\nj 6=k\nn(aj) log p(aj) + [T − n(aj)] log (1− p(aj)) .\nDenote the objective for this new term by F̃k, which is a function of p(aj). The partial derivative of F̃k with respect to p(aj) is:\n∂F̃k ∂p(aj) = n(aj) p(aj) − T − n(aj) 1− p(aj) ,\nwhich is equal to 0 if and only if\np(aj) = n(aj)\nT − n(aj) (1− p(aj)) = max\n{ 1, n(aj)\nT − n(aj)\n} (1− p(aj)) .\nThe last equality follows from our assumption that p(aj) ≥ 1− p(aj). Now, let pk(aj) denote the probabilities that we have just computed. Then, we can write the optimization problem of F̃k as:\n− max k∈{1,2},j∈{1,2}\\{k} n(aj) log pk(aj) + [T − n(aj)] log (1− pk(aj)) .\nTheorem 6 shows that the solutions of the∞-Rényi divergence optimization are based on the n-gram counts of sequences in CT with “high entropy”. This can be very different from the maximum likelihood solutions, which are based on the average n-gram counts. For instance, suppose we are under the assumptions of Theorem 6, and specifically, assume that there are T sequences in CT . Assume that one of the sequences has ( 1 2 + γ ) T occurrences of a1 for some\nsmall γ > 0 and that the other T − 1 sequences have T − 1 occurrences of a1. Then, n(a1) = ( 1 2 + γ ) T , and the solution of the∞-Rényi divergence optimization problem is given by p∞(a1) = 1+2γ2 and p∞(a2) = 1−2γ\n2 . On the other hand, the maximum-likelihood solution would be p1(a1) = 1+ γT − 3 2T + 1 T 2 ≈ 1 and p1(a2) = 3 2T − γ T − 1 T 2 ≈ 0 for large T ."
    }, {
      "heading" : "7.5 Model selection",
      "text" : "The previous section introduced a method to learn a single n-gram model based on the competitor automaton CT . In practice, one seeks an n-gram model that balances the tradeoff between approximation error and computational cost.\nAssume that we have a maximum per-iteration computational budget B. We thus seek the most computationally efficient n-gram model within our budget that does not contribute to an increase in regret. Let pn be an n-gram model returned by PROD-EG. By Corollary 1, we can write F (pn)− GapTopt,n ≤ F (p ∗ n), where p ∗ n is the optimal n-gram model minimizing the objective and GapTopt,n is the expression quantifying the gap between PROD-EG and the optimal solution after running the algorithm for Topt iterations. Thus, if F (pn)− GapTopt,n > √ T for some n, then even the optimal n-gram model for this n will cause an increase in the regret. Let n∗ be the smallest n such that F (pn)−GapTopt,n ≤ √ T (or the smallest value that exceeds our budget). We can find this value in log(n∗) time using a two-stage process. In the first stage, we double n after every violation until we find an upper bound on n∗, which we denote by nmax. In the second stage, we perform a binary search within [1, nmax] to determine n∗. Each stage takes log(n∗) iterations, and each iteration is the cost of running PROD-EG for that specific value of n. Thus, the overall complexity of the algorithm is O (log(n∗)Cost(PROD-EG)), where Cost(PROD-EG) is the cost of running PROD-EG once. Algorithm 5 presents the pseudocode for this algorithm, n-GRAMSELECT."
    }, {
      "heading" : "8 Time-independent approximation of competitor automata",
      "text" : "In the previous section, we introduced the technique of approximating the automaton accepting competitor sequences of length T , CT . Intersecting C with ST for different T typically results in different approximation automata. Since each approximation requires solving a convex optimization problem, this can become computationally expensive.\nIn this section, we show how one can approximate the competitor set for different T using a single approximation. The key is to approximate the original automaton C directly. Specifically, assume first that C is a stochastic automaton (so that its outgoing transition weights at each state sum to 1). Let pC denote the distribution defined by C, and let P be a family of distributions over Σ∗ that we will use to approximate pC. Given, pA ∈ P , define for every x ∈ Σ∗\np̃A[x] =\n{ pA[x] pC[S|x|]\npA[S|x|] if pA[S|x|] > 0\n0 otherwise\nThus, p̃A[x] is a rescaling of pA based on the mass assigned by C to sequences of length equal to |x|. Note that p̃A may not necessarily be a distribution. Our algorithm consists of determining the best approximation to the competitor\nAlgorithm 5: n-GRAMSELECT. Algorithm: n-GRAMSELECT(CT , Topt, B) n← 1 pn ← 1|Σ| t← 0 while t ≤ Topt do\npn ← PROD-EG-UPDATE(pn) t← t+ 1 if F (pn)−Gapt,n > √ T and |Σ|n ≤ B then\nn← 2n t← 0 pn ← 1|Σ|\nnmax ← n. pn ← BINARYSEARCH([1, nmax], F (pn)−GapTopt,n ≤ √ T ) return pn\ndistribution pC within the family of rescaled distributions:\nmin pA∈P\nD∞(pC||p̃A). (9)\nNote that this is an implicit extension of the definition of∞-Rényi divergence, since p̃A is not a distribution. The design of this optimization problem is motivated by the following result, which guarantees that if p̃A is a good approximation of pC, then pA∩ST will be a good approximation of pCT for any T .\nTheorem 7. For any stochastic automata S and A, and for any T ≥ 1,\nD∞(pCT ||pA∩ST ) ≤ D∞(pC||p̃A).\nProof. Let x ∈ CT = C ∩ ST such that pCT [x] > 0. Since pCT [x] = pC[x] pC(ST ) , this implies that pC(ST ) ≥ pC[x] > 0. Thus, if pA(ST ) > 0, then\nlog\n( pCT [x]\npA∩ST [x]\n) = log ( pC[x]pA(ST )\npC(ST )pA[x]\n) = log ( pC[x]\np̃A[x]\n) .\nOn the other hand, if pA(ST ) = 0, then, by definition, p̃A[x] = 0, therefore the following inequality holds\nlog\n( pCT [x]\npA∩ST [x]\n) ≤ ∞ = log ( pC[x]\np̃A[x]\n) .\nThe result now follows by taking the maximum over x ∈ ST on the left-hand side and the maximum over x ∈ Σ∗ on the right-hand side.\nNote that, for n-gram approximations, pAn ∈ Pn, the condition pAn(ST ) = 1 always holds. Thus, the approximation optimization problem can be written as:\nmin pAn∈Pn D∞(pC||p̃A) = min pAn∈Pn sup x∈C log\n( pC[x]\npAn [x]pC[S|x|]\n) .\nAs in Section 7, this problem is the minimization of the supremum of a family of convex functions over the product of simplices. Thus, it is a convex optimization problem and can be solved using the PROD-EG algorithm.\nWe have thus far assumed that C is a stochastic automaton in this section. If the sum of the weights of all paths accepted by C is finite, we can apply weight-pushing to normalize the automaton to make it stochastic and then solve the approximation problem above.\nHowever, this property may not always hold. For example, the original k-shifting automaton shown in Figure 2 accepts an infinite number of paths (sequences of arbitrary length with k shifts). Since each transition has unit weight, each path also has unit weight, and the sum of the weight of all paths is infinite.\nHowever, we can still apply the approximation method in this section to the k-shifting automaton by rescaling the transitions weights of self-loops to be less than 1. Specifically, consider the automaton Ck-shift, whose states and transitions are exactly the same as those of the original automaton Ck-shift, except that transitions from taj to (t+1)ak for aj 6= ak now have weight N+1 , and self-loops now have weight 1− . To make the automaton stochastic, we also assign\nweight 1N to every initial state. Then, the weight of a path of length T accepted by Ck-shift, is (1− ) T−k−1\n(\nN−1 )k 1 N ,\nand the weight of all paths is equal to the following:∑ π∈Ck-shift, wCk-shift, [π] = ∑ T≥k+1 N ( T − 1 k ) (N − 1)k(1− )T−k−1 ( N − 1 )k 1 N\n= ∑\nT≥k+1\n( T − 1 k ) (1− )T−k−1 k\n≤ ∑\nT≥k+1\n( (T − 1)e\nk\n)k (1− )T−k−1 k\n< +∞.\nBy normalizing the weights of this automaton, we can convert it into a stochastic automaton, where pCk-shift, [π] ∝ (1− )|π|−k−1 (\nN − 1\n)k 1\nN .\nFigure 6 shows the weighted automaton Ck-shift, . To compare with the results in Section 7, we will now analyze the approximation error of a maximum-likelihoodbased bigram approximation.\nTheorem 8 (Bigram approximation of Ck-shift, ). The maximum-likelihood based bigram model for Ck-shift, is defined by\npA2 [z2|z1] =\n∑ T̃≥k+1(1− )T̃−k−1 ( 1z1 6=z2 k T̃−1 1 N−1 + 1z1=z2 ( 1− k T̃−1 )) ∑ T̃≥k+1(1− )T̃−k−1 .\nMoreover, for every T > k + 1, there exists ∈ (0, 1) such that D∞(pCT ||pA2) ≤ − log ( 1− 2e− 112k ) .\nProof. The maximum-likelihood n-gram automaton is derived from the expected counts of the original automaton. Thus, for any z1, z2 ∈ Σ,\npA2 [z2|z1] =\n∑ x∈Ck-shift(1− ) |x|−k−1 ( N−1 )k 1 N ∑|x| t=2 1z21=xtt−1∑\nx∈Σ∗(1− )|x|−k−1 ( N−1 )k 1 N ∑|x| t=2 1z1=xt−1\n=\n∑ x∈Ck-shift(1− ) |x|−k−1∑|x| t=2 1z21=xtt−1∑\nx∈Σ∗(1− )|x|−k−1 ∑|x| t=2 1z1=xt−1\n=\n∑ T̃≥k+1 ∑ x∈Ck-shift,T̃ (1− )T̃−k−1 ∑T̃ t=2 1z21=xtt−1∑\nT̃≥k+1 ∑ x∈Ck-shift,T̃ (1− )T̃−k−1 ∑T̃ t=2 1z1=xt−1\nNow, notice that for any T̃ ,\n∑ x∈Ck-shift,T̃ T̃∑ t=2 1z21=xtt−1\n= ∑\nx∈Ck-shift,T̃\nT̃∑ t=2 1z1=xt−1 ( 1z1 6=z2 k T̃ − 1 1 N − 1 + 1z1=z2 ( 1− k T̃ − 1 )) .\nThis allows us to rewrite the probability above as:\npA2 [z2|z1]\n=\n∑ T̃≥k+1(1− )T̃−k−1 ( 1z1 6=z2 k T̃−1 1 N−1 )∑ x∈Ck-shift,T̃ ∑T̃ t=2 1z1=xt−1∑\nT̃≥k+1(1− )T̃−k−1 ∑ x∈Ck-shift,T̃ ∑T̃ t=2 1z1=xt−1\n+\n∑ T̃≥k+1(1− )T̃−k−1 ( 1z1=z2 ( 1− k\nT̃−1 ))∑ x∈Ck-shift,T̃ ∑T̃ t=2 1z1=xt−1∑\nT̃≥k+1(1− )T̃−k−1 ∑ x∈Ck-shift,T̃ ∑T̃ t=2 1z1=xt−1\n=\n∑ T̃≥k+1(1− )T̃−k−1 ( 1z1 6=z2 k T̃−1 1 N−1 + 1z1=z2 ( 1− k T̃−1 )) ∑ T̃≥k+1(1− )T̃−k−1 .\nThus, pA2 [z2|z1] depends only on the condition z1 6= z2. Now, fix T > k + 1. Since for every x ∈ CT , x has k shifts and length T , pA2 is uniform over all sequences in CT . This allows us to bound the∞-Renyi divergence between pCT and pA2 by:\nsup x∈Ck-shift,T log\n( pCk-shift,T [x]\npA2 [x] ) = sup x∈Ck-shift,T log ( pCk-shift,T [x] pA2 [ξ = x|ξ ∈ CT ]pA2 [ξ ∈ CT ] + pA2 [ξ = x|ξ /∈ CT ]pA[ξ /∈ CT ] )\n≤ sup x∈Ck-shift,T log\n( 1 |CT |\n1 |CT |pA2 [ξ ∈ CT ]\n)\n= sup x∈Ck-shift,T log\n( 1\npA2 [ξ ∈ CT ]\n) .\nIf we now let (ξt)Tt=2 denote i.i.d. Bernoulli random variables with mean\np̄( ) =\n∑ T̃≥k+1(1− )T̃−k−1\nk T̃−1∑\nT̃≥k+1(1− )T̃−k−1 ,\nthen\npA2 [ξ /∈ CT ] = P [∣∣∣∣∣ T∑ t=2 ξt − k ∣∣∣∣∣ ≥ 1 ]\n≤ P [∣∣∣∣∣ T∑ t=2 ξt − p̄( )(T − 1) ∣∣∣∣∣ ≥ 12 ] + P [ |p̄( )(T − 1)− k| ≥ 1 2 ] .\nThus, if |p̄( )(T − 1) − k| < 12 , then pA2 [ξ /∈ CT ] can be bounded using the same concentration argument as in Theorem 4.\np̄( ) can be interpreted as the weighted average of k T̃−1 for T̃ ≥ k + 1, where the weight of k T̃−1 is (1− ) T̃−k−1. We want this average to be close to kT−1 for the specific choice of T > k + 1, which we obtain by appropriately tuning ∈ (0, 1).\nSince lim →0+ p̄( ) = 1, lim →1− p̄( ) = 0 and p̄( ) is continuous in on (0, 1), it follows by the intermediate value theorem that for any T > k + 1, there exists an ∗ such that p̄( ∗) = kT−1 .\nNote that in the proof of the above theorem, p̄( ) is monotonic in . Thus, one can find ′ such that ∣∣∣p̄( ′)− kT−1 ∣∣∣ ≤\n1 2(T−1) using binary search."
    }, {
      "heading" : "9 Failure transition algorithm",
      "text" : "The computational complexity of the AWM algorithm presented in Section 4 is based on the size of the composed automaton C ∩ ST , which itself is related to the original size of C. Similarly, if we were to apply AWM to an n-gram approximation, the computational complexity of the algorithm depends on the size of the approximating automaton. In this section, we introduce a technique to improve the computational cost of AWM by reducing the size of the automaton. To do this, we will use the notion of failure transition (or ϕ-transition). Failure transitions play a key role in the design of many efficient string-matching and automata algorithms, including egrep under UNIX [Aho and Corasick, 1975, Knuth et al., 1977, Crochemore, 1986, Mohri, 1997].\nLet ϕ be a symbol not in Σ. A ϕ-automaton is an automaton A with transition set EA ⊆ QA×Σ∪{ϕ}∪R+∪QA. Transitions of the forms e = (q, ϕ, w, q′) are called failure transitions and characterized by the semantic of “other”. If at state q there is no outgoing transition labeled with a given label a ∈ Σ and there is a ϕ-transition e leaving q, then the failure transition is taken instead without consuming the label, and the next state is determined using the transitions leaving q′. We assume that there is no ϕ-cycle in any of our ϕ-automata, and that there is at most one failure transition leaving any state. This implies that the number of consecutive failure transitions taken is bounded.\nMore formally, the transition function δ is defined as follows in the presence of ϕ-transitions. Recall that for state q ∈ QA and label a ∈ Σ, δ[q, a] is the set of states reachable from q by reading label a, and E[q, a] is the set of outgoing transitions from q with label a. For a ϕ-automaton, the set of states reachable from any state by reading label a is defined as follows:\nδ[q, a] =  {dest[e] : e ∈ E[q, a]} if E[q, a] 6= ∅ δ[δ[q, ϕ], a] if E[q, a] = ∅, E[q, ϕ] 6= ∅ ∅ otherwise.\nA failure transition can often replicate the role of multiple standard transitions when there is “symmetry” within an automaton, that is when there are many transitions leading to the same state from different states that consume the same set of labels. Figure 7 illustrates such cases.\nGiven any non-initial state q in an automaton, we would like to determine a set of parent states within ER[q] from which it is beneficial to introduce a ϕ-transition. We design a greedy solution in Algorithm 6, ϕ-SOURCESUBSET. For every q ∈ Q \\ I , having already computed Qk−1, ϕ-SOURCESUBSET greedily determines a candidate set Qk of k parent states based on which available parent state has the most label-weight overlap with the subset already chosen. It then selects the best candidate set from {Qk}|E R[q]| k=1 . ϕ-SOURCESUBSET is able to recover the ϕ-transition structure\nin Figure 7, and while it does not always produce the optimal subset of parent states, its computational cost is only O ( |δR[q]|2|ER[q]| ) for input state q.\n0 4 a b c 1 a b c\n2\na b c\n3\na b c\n0\n4\nφ\n1\nφ\n2\nφ\n3\nφ\n5\na b c\n(a) (b)\nWe now present an algorithm, ϕ-CONVERT, that converts a weighted automaton to a weighted ϕ-automaton without ever increasing the size of the automaton. Its pseudocode is provided in Algorithm 7, and it uses ϕ-SOURCESUBSET as a subroutine to select a candidate set of parent states for each state q. The algorithm then determines whether it should introduce a failure transition between these parent states and q based on whether the ϕ-transition will reduce the total number of transitions in the automaton. Theorem 9 provides an accompanying guarantee on efficiency.\nTheorem 9 (Correctness and performance of ϕ-CONVERT). ϕ-CONVERT returns a ϕ-automaton equivalent to the input automaton. The number of transitions in the ϕ-automaton output by the algorithm is never larger than the number of transitions in the input automaton. The running-time complexity of ϕ-CONVERT is O ( |QC| max\nq∈QC |δR[q]|2|ER[q]|\n) .\nProof. Let q be any state in C \\ IC, q′ ∈ δR[q], and e = (q′, a, w, q) ∈ ER[q] be such that we introduce a failure transition from q′ to q that replaces e. Then, any path that used to contain transition e will now contain the consecutive pair of transitions (q′, ϕ, 1, q̃) and (q̃, a, w, q), which is taken where a was previously taken (since the automaton is deterministic). The contributed weight of this pair is w and the substring consumed is a, which is the same as it was for e. Since q was arbitrary, the new automaton assigns the same weight to every string as the old automaton.\nMoreover, notice that whenever we consider introducing a failure transition, we remove |S∗| transitions from |Q∗| parent states while introducing |Q∗| ϕ-transitions from each of the parent states to a new state q̃, and |S∗| transitions from q̃ to q. Thus, the change in number of transitions is |S∗|+ |Q∗| − |S∗||Q∗|. This is precisely the condition that we check before introducing a failure transition.\nThe running-time complexity of ϕ-CONVERT is dominated by the subroutine ϕ-SOURCESUBSET. Since we must run it for every state in QC \\ IC, the total computational cost is O ( |QC|maxq∈QC |δR[q]|2|ER[q]| ) .\nFigure 8 shows the result of applying ϕ-CONVERT to the k-shifting automaton. Recall that the two main automata operations required for AWM are composition and shortest-distance. While these two algorithms are standard for weighted automata, it is not as clear how one can perform them over weighted ϕ-automata. We now extend both to ϕ-automata.\nFirst, we consider the composition of a weighted ϕ-automaton with ST . Algorithm 8 presents the pseudocode for our algorithm, ϕ-COMPOSITION.3 As with standard composition the output automaton is based on matching transitions of C and ST . However, when the algorithm considers a ϕ-transition in C, it checks if the label of the current transition considered in ST matches that of a transition leaving the source state of the ϕ-transition. In that case, the algorithm skips this pair of transitions, as the ϕ-transition is not taken in the presence of transitions with a desired label. If there is no such label, then the algorithm creates a new transition for the composed automaton, where the destination state in the ST component remains unchanged (since the automaton has not successfully “read” the label yet). Theorem 10 presents consistency guarantees that ϕ-composition behaves in an analogous way to standard composition.\nTheorem 10 (ϕ-COMPOSITION consistency guarantee). ϕ-COMPOSITION outputs a ϕ-automaton equivalent to C∩ST (or C ◦ ST ). The maximum number of transitions created by the algorithm is O (∑T t=1 |QC,t| maxq∈QC,t |E[q]| ) , where QC,t = {q ∈ QC ×QST : q = (·, t)}.\nProof. Given an automaton A, state q ∈ QA, and string x ∈ Σ∗, let δA[q, w] be the set of states reachable from q by reading string x.\nLet q ∈ CT . By construction, q = (q1, q2), where q1 ∈ C and q2 ∈ ST . We first show that for any string x ∈ Σ∗, δCT [q, x] = (δC[q1, x], δST [q2, x]).\nBy induction, suppose that |x| = 1. Let (q′1, q′2) ∈ (δC[q1, x], δST [q2, x]). Then ∃e2 ∈ EST such that e2 = (q2, x, w2, q ′ 2). Since C admits failure transitions, it is possible that ∃e1 ∈ EC such that e1 = (q1, x, w1, q′1) or not. If such an e1 does exist, then ϕ-COMPOSITION constructs the edge e = ((q1, q2), x, w1w2, (q′1, q ′ 2)) so that (q′1, q ′ 2) ∈ ECT . If such an e1 does not exist, then there must exist a sequence of failure transitions {ẽj}nj=1 leading\n3In the pseudocode, ] denotes the multiset union.\nAlgorithm 8: ϕ-COMPOSITION. Algorithm: ϕ-COMPOSITION(C, ST ) QCT ← IC × IST . Q ← IC × IST . while Q 6= ∅ do\nq = (q1, q2)← HEAD(Q) DEQUEUE(Q) if q ∈ IC × IST then\nICT ← ICT ∪ {q} λCT ← λC(q1)λST (q2) if q ∈ FC × FST then FCT ← FCT ∪ {q} ρCT ← ρC(q1)ρST (q2) for each (e1, e2) ∈ EC × EST do if σ(e1) = σ(e2) then\nif q̃ = (dest[e1],dest[e2]) /∈ QCT then QCT ← QCT ∪ {q̃} ENQUEUE(Q, q̃)\nECT ← ECT ] {(q, σ(e1), w[e1]w[e2], q̃)} else if σ[e1] = ϕ then\nif @e ∈ EC[q1] such that σ[e] = σ[e2] then if q̃ = (dest[e1], q2) /∈ QCT then\nQCT ← QCT ∪ {q̃} ENQUEUE(Q, q̃)\nECT ← ECT ] {(q, ϕ,w[e1], q̃)} CT ← (Σ, QCT , ICT , FCT , ECT , λCT , ρCT ) return CT\nto a state with a transition ẽn+1 labeled with x with destination q′1. Moreover, there cannot be any transition leaving {q1} ∪ {dest[ẽj ]}n−1j=1 labeled with x. In this case, ϕ-COMPOSITION would have constructed the transitions:\n{((q1, q2), ϕ, w[ẽ1], (dest[ẽ1], q2))} ∪ {((dest[ẽj ], q2), ϕ, w[ẽ1], (dest[ẽj+1], q2))}n−1j=1 ∪ {((dest[ẽn], q2), x, w[ẽn+1]w2, (q′1, q′2))},\nand there also would be no transitions labeled with x leaving {(q1, q2)} ∪ {dest[ẽj ], q2}nj=1. Thus, we would have (q′1, q′2) ∈ δCT [q, x]. Since (q1, q2), (q′1, q′2) and x are arbitrary, this implies that δCT [q, x] ⊇ (δC[q1, x], δST [q2, x]).\nTo show the other inclusion, assume that (q′1, q ′ 2) ∈ δCT [q, x]. Then, either there exists a transition of the form ((q1, q2), x, w, (q ′ 1, q ′ 2)) or not. If there does exist such a transition, then it must have been constructed from the transitions (q1, x, w1, q′1) and (q2, x, w2, q ′ 2) in EC and EST . Otherwise, the algorithm would have constructed failure transitions. Thus, if such a transition exists, then q′1 ∈ EC[q1, x] and q′2 ∈ EST [q2, x]. If such a transition does not exist, then (q′1, q ′ 2) is reached through a series of failure transitions. Thus, there must be a sequence of transitions:\n{((q1, q2), ϕ, w̃1, q̃1)} ∪ {(q̃j , ϕ, w̃j , q̃j+1)}nj=2 ∪ {(q̃n+1, x, w̃n+1, (q′1, q′2))},\nin which none of the states {(q1, q2)} ∪ {q̃j}nj=1 have outgoing transitions labeled with x. By construction, each of the states q̃j = (q̃j,1, q̃j,2) ∈ QC ×QST , and any failure transition in the output: ∀j ∈ [2, n], (q̃j , ϕ, w̃j , q̃j+1) must have been constructed from the transition (q̃j,1, ϕ, w̃j , q̃j+1,1) ∈ EC, and satisfy q̃j,2 = q̃j+1,2. Similarly, there must exist (q1, ϕ, w̃1, q̃1,1) ∈ EC and q̃1,2 = q2. Finally, the existence of (q̃n+1, x, w̃n+1, (q′1, q′2)) implies the existence of both (q̃n+1,1, x, w̃n+1,1, q ′ 1) ∈ EC and (q̃n+1,2, xw̃n+1,2, q′2) ∈ EST , where w̃n+1,1w̃n+1,2 = w̃n+1. In other words, EST contains a transition (q2, x, w̃n+1,2, q′2) andEC contains a sequence of failure transitions whose source states do not have outgoing edges labeled with x. But this means that the destination of this sequence of transitions (q′1, q ′ 2) is reachable from (q1, q2) by reading x, and so δCT [q, x] ⊆ (δC[q1, x], δST [q2, x]). Thus, δCT [q, x] = (δC[q1, x], δST [q2, x]) when |x| = 1.\nBy induction, assume that the statement is true for |x| = n − 1 and that now |x| = n. Then x = yz where |y| = n− 1 and |z| = 1. Thus, we have that\nδCT [q, x] = δCT [q, yz]\n= δCT [δCT [q, y], z]\n= (δC[δC[q1, y], z], δST [δST [q2, y], z])\n= (δC[q1, yz], δST [q2, yz])\n= (δC[q1, x], δST [q2, x]).\nThus, δCT [q, x] = (δC[q1, x], δST [q2, x]) for all x ∈ Σ∗. Notice that in the course of the proof above, we have constructed a one-to-one correspondence between the paths in ECT and the paths in EC and EST , where the weights of the paths in the former are exactly the product of the weights of the paths in the latter. Since the weight of any string is simply the sum of the weights of all accepting paths, the weight of any string in the output is equal to the sum of the products of the weights of matching paths in C and ST in the input.\nThe bound on the number of transitions in the output of composition follows from the same reasoning as for standard composition. Since the algorithm only constructs states and transitions that are reachable by paths in C ∩ ST , we may again consider the incremental cost of composing states corresponding to q = (·, t+ 1) given that we have composed all states of the form q = (·, t). The existence of failure states and transitions is accounted for in the definition of QC,t.\nThe second automaton operation that we need to extend to ϕ-automata is the shortest-distance computation. Specifically, in AWM, we applied Algorithm 2, INCRSD, as a subroutine, which was a shortest-distance computation for weighted automata defined over the probability semiring. In general, IncrSD cannot be directly applied to ϕautomata because it might sum over potentially ‘obsolete ϕ-transitions’. For example, if at a given state q, there is a transition labeled with a to q′ and a ϕ-transition whose destination state has a single outgoing transition also labeled with a to q′, the second path should not be considered.\nTo account for these types of situations and extend INCRSD in a computationally efficient way, we use the fact that the probability semiring (R+,+,×, 0, 1) admits a natural extension to a ring structure under the standard additive inverse −1. Specifically, upon encountering a transition e labeled with a leaving state q, we will check for ϕ-transitions with destination states that admit further transitions e′ labeled with a. Any such transition should not contribute any weight to the distance to dest[e′], since under the semantic of the ϕ-transition, this path should never be taken. To correctly account for these paths, we will preemptively subtract the weight of e′ from its destination state. When the algorithm processes the ϕ-transition directly, it will add this weight back so that the total contribution of this path is zero. The overhead of this extra computation is O(Nϕ(QCT ) max\nq∈QCT |E[q]|), where Nϕ(QCT ) is the maximum number\nof consecutive ϕ-transitions leaving states in QCT . The precise pseudocode is presented in Algorithm 9, ϕ-INCRSD. Theorem 11 provides a theoretical guarantee for the algorithm.\nTheorem 11 (ϕ-INCRSD guarantee). If ϕ-INCRSD is run with the topological order queue discipline and the shortest distances from source ICT to the set of states QC,t−1 = {q ∈ QCT : q = (·, t− 1)} are stored in α, then ϕ-INCRSD\nAlgorithm 9: ϕ-INCREMENTALSHORTESTDISTANCE (ϕ-IncrSD). Algorithm: ϕ-INCRSD(C ∩ ST , t, α) CT ← C ∩ ST for each q ∈ QCT with q = (·, t) do\nα[q]← r[q]← 0 if t = 1 then\nfor each q ∈ ICT do α[q]← r[q]← 1\nQ ← ICT . else Q ← {q ∈ QCT : q = (·, t− 1)} while Q 6= ∅ do q ← HEAD(Q) DEQUEUE(Q) r̃ ← r[q] r[q]← 0 for each e ∈ ECT [q] do\nif α[dest[e]] 6= α[dest[e]] + (r̃w[e]) then α[dest[e]]← α[dest[e]] + (r̃w[e]) r[dest[e]]← r[dest[e]] + (r̃w[e]) if dest[e] /∈ Q then ENQUEUE(Q,dest[e]) if σ[e] 6= ϕ then\nq̃ ← q wϕ ← 1 while ∃eϕ ∈ E[q̃] with σ[eϕ] = ϕ do\nwϕ ← wϕw[eϕ] if ∃e′ ∈ E[dest[eϕ]] with σ[e′] = σ[e] then\nα[dest[e′]]← α[dest[e′]]− (r̃wϕw[e′]) BREAK\nelse q̃ ← dest[eϕ]\nreturn α\ncomputes the shortest distances from source ICT to the set of states QC,t. The computational complexity of ϕ-INCRSD is O ( Nϕ(QC,t−1)|E[QC,t−1]| ) .\nProof. Assume that α stores the shortest distances from ICT to QC,t−1, and let q ∈ QC,t. Since we assume that α is correct up to QC,t−1 and every path from ICT to QC,t must pass through some state in QC,t−1, it follows that\nα[q] = ∑\nπ∈P [QC,t−1,q] π valid\nα[src[π]]w[π]\n= ∑\nπ∈P [QC,t−1]\nα[src[π]]w[π]− ∑\nπ∈P [QC,t−1,q] π invalid\nα[src[π]]w[π],\nwhere a valid π refers to a path that respects the semantic of the ϕ-transition. The first sum in the last expression is the value computed by the standard shortest-distance algorithm, INCRSD, ignoring the semantic of the ϕ-transition. It is also the value computed by the algorithm in the first “if” clause within the loop over ECT [q]. The second sum is the value computed and subtracted out from α[q] by the second “if” clause within the same loop, when we encounter a transition e such that σ[e] 6= ϕ. Thus, α[q] is computed correctly, and since q ∈ QC,t is arbitrary, ϕ-INCRSD returns α with the correct path weights for all states in QC,t.\nNotice that the algorithm loops over every state in QC,t−1, including states that are the destination of ϕ-transitions. For every state q, the algorithm loops over every transition e ∈ E[q] and possibly looks into Nϕ(q) consecutive\nϕ-transitions. If q′ is the destination of a ϕ-transition, then this will cost O(E[q′]). Thus, the total computational cost is O (Nϕ(QC,t−1)|E[QC,t−1]|).\nNotice that ϕ-INCRSD can be straightforwardly extended to a non-incremental form that computes the shortest path to every state given a source state. We call this algorithm ϕ-SHORTESTDISTANCE, which we will use to compute the analogue of β in AUTOMATAWEIGHTEDMAJORITY.\nWith these two new operations, ϕ-COMPOSITION and ϕ-INCREMENTALSHORTESTDISTANCE, we are now ready to present ϕ-AUTOMATAWEIGHTEDMAJORITY (ϕ-AWM), an extension to AWM that applies to ϕ-automata. Given an input automaton (not necessarily with ϕ-transitions), the algorithm first calls ϕ-CONVERT to determine whether it is beneficial to introduce ϕ-transitions. The algorithm then composes the output with ST to compute the set of sequences of length T that are accepted by C. The algorithm then uses the new shortest-distance algorithm to perform the same weighted majority update as before. Algorithm 10 presents the pseudocode for ϕ-AWM.\nAlgorithm 10: ϕ-AUTOMATAWEIGHTEDMAJORITY (ϕ-AWM). Algorithm: ϕ-AWM(C) C← ϕ-CONVERT(C) CT ← ϕ-COMPOSITION(C, ST ) CT ← CONNECT(CT ) β ← ϕ-SHORTESTDISTANCE((CT )R, F ) wCT [ICT ]← wCT [ICT ]/β[ICT ] α← 0 Flow0 ← 0 for each e ∈ ECT with dest[e] = (·, 1) do\nflow[e]← β[dest[e]] Flow0[σ[e]]← Flow0[σ[e]] + flow[e] Z0 ← Z0 + flow[e]\np1 ← Flow0/Z0 for t← 1 to T do\nit ←SAMPLE(pt) PLAY(it) RECEIVE(lt) Flowt ← 0 for each e ∈ ECT with dest[e] = (·, t) do\nw[e]← w[e] · e−ηlt[σ[e]] flow[e]← α[src[e]]w[e]β[dest[e]] Flowt[σ[e]]← Flowt[σ[e]] + flow[e] Zt ← Zt + flow[e].\npt+1 ← Flowt/Zt α← ϕ-INCRSD(CT , t,α)\nSince the update in ϕ-AWM is identical to the update in AWM, we immediately obtain the following guarantee for ϕ-AWM.\nTheorem 12 (ϕ-AUTOMATAWEIGHTEDMAJORITY Guarantee). If C∩ ST is normalized in the input to PBWM, then at each round t, ϕ-AWM performs the same weighted majority update as in PBWM for unweighted regret. Moreover, the computational complexity of ϕ-AWM at each round t is O (|Nϕ(QCT ,t−1)E[QCT ,t]|), so that the total computational cost is O (∑T t=1Nϕ(QCT ,t−1)|E[QCT ,t]| ) .\nFor the k-shifting automaton, the per-iteration computational complexity of ϕ-AWM is now O(Nk), since there is at most one consecutive ϕ-transition in the output of ϕ-Convert, and we now aggregate transitions at each time using failure transitions. This is a factor of N better than that of AWM, and only a factor of k worse than the specific algorithm of Herbster and Warmuth [1998]. Using a bigram approximation of the k-shifting automaton composed with ST and then introducing converting it into a ϕ-automaton, we obtain an algorithm that runs in O(N)."
    }, {
      "heading" : "10 Composition of multiple ϕ-automata",
      "text" : "In many cases, it may be more convenient and, in some instances, more efficient to define C as the composition (intersection) of multiple automata, each represented with failure transitions. While the composition algorithms discussed above are correct for composing an automaton with failure transitions with an automaton without failure transitions, they are not guaranteed to be accurate when composing two ϕ-automata. Specifically, in the process of matching transitions, the composition algorithm may produce multiple ϕ-paths between two states. See Figure 10 for an example.\nHaving redundant ϕ-paths is problematic because it can lead to incorrect weight computations. Specifically, we may associate extra paths to a given string due to multiple ϕ-paths. As a consequence, we may also assign extra weight to this string.\nTo avoid this situation, we introduce the concept of a ϕ-filter, which is a mechanism that can filter out all but one ϕ-path between any two states. As we shall see, this ϕ-filter can be implemented as composition with a particular finite-state transducer (FST).\nRecall that a finite-state transducer is a finite automaton in which each transition is augmented with an output label. We denote the input label of a transition e as i[e], the output label as o[e], and the label pair as (i[e] : o[e]). The output labels are concatenated along a path to form an output sequence, so that any accepting path has an associated output string. A finite automaton can be interpreted as a finite-state transducer whose input and output labels coincide for each transition. As with finite automata, finite-state transducers can also be augmented with weights, and as with weighted finite automata, weighted finite-state transducers can be composed efficiently and on-demand [Mohri, 2009].\nThe reason why redundant ϕ-paths are generated by standard composition algorithms is similar to the reason why redundant -paths are generated during composition of automata without failure transitions [Allauzen and Mohri, 2008].\nWhen the algorithm is in state q in WFA C and state q̃ in C̃, both of which contain outgoing ϕ-transitions, the algorithm may take any of the following steps: (1) move forward on a ϕ-transition in C while staying at q̃; (2) move forward on a ϕ-transition in C̃ while staying in C; or (3) move forward in both C and C̃.\nTo design a ϕ-filter, we first modify the two input automata C1 and C2 to distinguish between these two cases. In C1, for every ϕ-transition, we rename the label ϕ as ϕ2. Moreover, at the source and destination states of every ϕ-transition, we introduce new self-loop transitions labeled with ϕ1 and with weight 1. Thus, a transition labeled with ϕ2 will indicate a “move forward,” while a transition labeled with ϕ1 will indicate a “stay.” Similarly, in C2, we rename the ϕ labels as ϕ1, and we introduce self-loops labeled with ϕ2 and weight 1 at the source and destination states of every ϕ-transition. With these modifications, any ϕ-path resulting from the composition algorithm will include transitions of the form: (1) (ϕ2 : ϕ2); (2) (ϕ1 : ϕ1); or (3) (ϕ2 : ϕ1). This observation guides us in designing a ϕ-filter that will eliminate redundant paths.\nTheorem 13 (Correctness of ϕ-filter F). Let F be the finite-state transducer illustrated in Figure 12. Then, C1 ◦ F ◦ C2 is weighted transducer equivalent to C̃1 ◦ C̃2, where C̃i is the weighted transducer obtained from Ci by “removing” ϕ-transitions.\nProof. Notice first that there is at most one shortest ϕ-path that takes (ϕ2 : ϕ1) transitions first. Now, we claim that if a filter disallows the following subpaths:\n{(ϕ2 : ϕ2)(ϕ1 : ϕ1), (ϕ1 : ϕ1)(ϕ2 : ϕ2), (ϕ1 : ϕ1)(ϕ2 : ϕ1), (ϕ2 : ϕ2)(ϕ2 : ϕ1)},\nthen the filter will only contain ϕ-paths between states that are the shortest and contain (ϕ2 : ϕ1) transitions first. To see this, suppose that the filter allowed a path that wasn’t the shortest path or didn’t contain (ϕ2 : ϕ1) transitions first. If this path wasn’t the shortest path but contained all (ϕ2 : ϕ1) transitions first, then it must contain both transitions of the form (ϕ2 : ϕ2) and (ϕ1 : ϕ1) after (ϕ2 : ϕ1) transitions. But this implies that this path also contains consecutive transitions of the form (ϕ2 : ϕ2)(ϕ1 : ϕ1) or (ϕ1 : ϕ1)(ϕ2 : ϕ2), which are two of the disallowed subpaths. If the path didn’t contain (ϕ2 : ϕ1) transitions first, then it must begin with a transition labeled with (ϕ1 : ϕ1) or (ϕ2 : ϕ2). At some point, the path must include (ϕ2 : ϕ1), which would constitute one of the disallowed subpaths.\nFinally, we now show that a filter carrying the above properties can be represented as a finite-state transducer. Notice that the set of forbidden sequences can be represented by the transducer shown in Figure 11, and by the language:\nL = σ∗ ( (ϕ2 : ϕ2)(ϕ1 : ϕ1) + (ϕ1 : ϕ1)(ϕ2 : ϕ2)\n+ (ϕ1 : ϕ1)(ϕ2 : ϕ1) + (ϕ2 : ϕ2)(ϕ2 : ϕ1)\n) σ∗,\nwhere σ = {(a, a)}a∈Σ ∪ {(ϕ2 : ϕ1), (ϕ1 : ϕ1), (ϕ2 : ϕ2)}. Since L is a regular language, its complement, L̄, is also a regular language and can be represented as an automaton that is the result of determinimization and complementation of an automaton accepting L. This automaton is shown in Figure 12.\nFigure 11 illustrates the automaton accepting the language L in the proof of Theorem 13. We define our ϕ-filter F as the finite state transducer induced by the automaton obtained after determinization and complementation of the automaton in Figure 11. This new FST is illustrated in Figure 12.\nNotice that the composition of any two ϕ-automata and the ϕ-filter F, C1 ◦ F ◦ C2, will result in a finite-state transducer whose transitions have labels in {(a : a)}a∈Σ ∪ {(ϕ2 : ϕ2), (ϕ1 : ϕ1), (ϕ2 : ϕ1)}. Moreover, we identify all label pairs in {(ϕ2 : ϕ2), (ϕ1 : ϕ1), (ϕ2 : ϕ1)} using the same semantic of “other” as we did with ϕ. Thus, we can identify all label pairs in {(ϕ2 : ϕ2), (ϕ1 : ϕ1), (ϕ2 : ϕ1)} with the single pair (ϕ : ϕ) and treat the result of composition as simply a weighted finite automaton.\nWe are now ready to present a general ϕ-composition algorithm that is able to compose two ϕ-automata correctly. This composition algorithm will take as input three weighted finite-state transducers, two corresponding to the ϕ-automata, C1 and C2 and one corresponding to the ϕ-filter, F, and it will output C1 ◦ FC2. A straightforward implementation would be to recursively compose two transducers at a time (e.g. C1 ◦ FC2 = (C1 ◦ F) ◦ C2)). However, we can improve upon this by extending the 3-way composition technique presented in [Allauzen and Mohri, 2008] to our ϕ-automata setting. This algorithm, GENERAL-ϕ-COMPOSITION, is given as Algorithm 11. Instead of composing\ntwo automata at a time by matching transitions, this algorithm uses a lateral strategy to compose all three automata at once. Specifically, it first considers pairs of transitions in C1 × C2, and then it searches for matching transitions in F.\nAs in the original 3-way composition algorithm, we can pre-process the ϕ-filter F using perfect hashing in expected linear time O(|EF|) so that candidate transition sets E can be found in worst-case linear time O(|E|). In this way, the worst-case running-time of GENERAL-ϕ-COMPOSITION is in\nO ( |QC|max\nq∈C1 |EC1 [q]|max q∈C2 |EC2 [q]|+ |EC|\n) .\nThe algorithms described above are general. While the k-shifting automaton was used as a running example, the algorithms ϕ-Convert, ϕ-AWM, and n-GRAMSELECT can be applied to any weighted finite automaton. In practice, this automaton can even be learned from data and on-demand over a series of epochs. Given a set of experts, we can learn a language model over the sequences to determine which ones have performed well in the past. We can represent this language model using a WFA. We can then convert it into a ϕ-WFA or first an n-gram model and then a ϕ-WFA to learn against it in a computationally efficient way. As we learn using this ϕ-WFA, we can also simultaneously learn another language model over sequences that we can use in the next epoch."
    }, {
      "heading" : "11 Extension to sleeping experts",
      "text" : "In many real-world applications, it may be natural for some experts to abstain from making predictions on some of the rounds. For instance, in a bag-of-words model for document classification, the presence of a feature or subset of features in a document can be interpreted as an expert that is awake. This extension of standard prediction with expert advice is also known as the sleeping experts framework [Freund et al., 1997]. The experts are said to be asleep when they are inactive and awake when they are active and available to be selected. This framework is distinct from the\nAlgorithm 11: GENERAL-ϕ-COMPOSITION. Algorithm: GENERAL-ϕ-COMPOSITION(C1, C2, F) Q← IC1 × IF × IC2 Q ← IC1 × IF × IC2 while Q 6= ∅ do\nq = (q1, qF, q2)← HEAD(Q) DEQUEUE(Q) if q ∈ IC1 × IF × IC2 then\nI ← I ∪ {q} λ(q)← λC1(q1)λC2(q2)\nif q ∈ FC1 × FF × FC2 then F ← FCT ∪ {q} ρ[q]← ρC1 [q1]ρC2 [q2] for each (e1, e2) ∈ EC1 × EC2 do if σ[e1] = ϕ then\nσ[e1]← ϕ2. EC1 ← EC1 ∪ (q1, ϕ1, 1, q1) if (dest[e1], ϕ1, 1,dest[e1]) /∈ EC1 then\nEC1 ← EC1 ∪ (dest[e1], ϕ1, 1,dest[e1]) if σ[e2] = ϕ then\nσ[e2]← ϕ1 EC2 ← EC2 ∪ (q2, ϕ2, 1, q2) if (dest[e2], ϕ2, 1,dest[e2]) /∈ EC2 then\nEC2 ← EC2 ∪ (dest[e2], ϕ2, 1,dest[e2]) E ← {e ∈ EF : i[e] = σ[e1] ∧ o[e] = σ[e2]}. for each eF ∈ E do\nif q̃ = (dest[e1],dest[eF],dest[e2]) /∈ Q then Q← Q ∪ {q̃} ENQUEUE(Q, q̃) if σ[e1] ∈ {ϕ1, ϕ2} then s← ϕ else s← σ[e1]\nE ← E ] {(q, s, w[e1]w[e2], q̃)} C← (Σ, Q, I, F,E, λ, ρ) return C\npermutation-based definitions adopted in the studies in [Kleinberg et al., 2010, Kanade et al., 2009, Kanade and Steinke, 2014].\nFormally, at each round t, the adversary chooses an awake set At ⊆ Σ from which the learner is allowed to query an expert. The algorithm then (randomly) chooses an expert it from At, receives a loss vector lt ∈ [0, 1]|Σ| supported on At and incurs loss lt[it]. Since some experts may not be available in some rounds, it is not reasonable to compare the loss against that of the best static expert or sequence of experts. In [Freund et al., 1997], the comparison is made\nagainst the best fixed mixture of experts normalized at each round over the awake set: minu∈∆N ∑T t=1 ∑ i∈At uilt[i]∑ j∈At uj .\nWe extend the notion of sleeping experts to the path setting, so that instead of comparing against fixed mixtures over experts, we compare against fixed mixtures over the family of expert sequences. With some abuse of notation, let At also represent the automaton accepting all paths of length T whose t-th transition has label in At. Thus, we want to design an algorithm that performs well with respect to the following quantity:\nmin u∈∆|CT | T∑ t=1 ∑ π∈CT∩At uπlt[σt[π]]∑ π∈CT∩At uj .\nThis motivates the design of AWAKEPBWM, a path-based weighted majority algorithm that generalizes the algorithms in [Freund et al., 1997] to arbitrary families of expert sequences. Like PBWM, AWAKEPBWM maintains\na set of weights over all the paths in the input automaton. At each round t, the algorithm performs a weighted majority-type update. However, it normalizes the weights so that the total weight of the awake set remains unchanged. This prevents the algorithm from “overfitting” to experts that have been asleep for many rounds. The pseudocode of this algorithm is presented as Algorithm 12, and its accompanying guarantee is given in Theorem 14, whose proof is in Appendix 14.\nAlgorithm 12: AWAKEPATH-BASEDWEIGHTEDMAJORITY(AWAKEPBWM). Algorithm: AWAKEPBWM(C ∩ ST , ṽ1), where ṽ1 ∈ RK+ is a vector of initial path weights. π ← C ∩ ST K ← |C ∩ ST | N ← |Σ| for j = 1 to K do\np̃1,j ← ṽ1,j∑K i=1 ṽ1,i\nfor j = 1 to N do p1,j ← ∑ i∈[1,K] : πi,1=j p̃1,i for t = 1 to T do RECEIVE(At) for j = 1 to N do\npAtt,j ← pt,j∑ i∈At pt,i .\nit ←SAMPLE(pAtt ) PLAY(it) RECEIVE(lt) for j = 1 to K do\nṽt+1,j ← ṽt,je−ηlt[πj,t] p̃t+1,j ← ṽt+1,j∑K i=1 ṽt+1,i\nfor j = 1 to N do p†t+1,j ← ∑ i∈[1,K] : πi,t=j p̃t+1,i for j = 1 to N do pt+1,j ← p†t+1,j ∑ i∈At\npt,i∑ i∈At p†t+1,i\nTheorem 14 (Regret Bound for AWAKEPBWM). Let ṽ1,j = 1 for every j ∈ [K] be the initial path weights in the AWAKEPBWM algorithm. For each t ∈ [T ], let At ⊂ Σ denote the set of experts that are awake at time t. Then for any distribution u on Σ, AWAKEPBWM admits the following unweighted regret guarantee:\nT∑ t=1 u(At) E i∼pAtt [lt[i]]− ∑ i∈At uilt[πi,t] ≤ η 8 T∑ t=1 u(At) + 1 η log(K).\nAs with PBWM, the per-iteration computational complexity of AWAKEPBWM is in O(|CT |), the number of paths in consideration. We improve upon this by extending AWM to the sleeping expert setting and designing a new algorithm called AWAKEAWM. The key modification in AWAKEAWM is that during the flow computations, we intersect the set of available edges with At before performing the weight update. This intersection can be performed while preserving the favorable computational complexity of AWM, which depends on the states in CT reachable at times t times the maximum out-degree of any state. Algorithm 13 provides the pseudocode for AWAKEAWM. As in the non-sleeping expert setting, we can further improve the computational complexity by applying ϕ-conversion to arrive at a or n-gram approximation and then ϕ-conversion. All other improvements in the sleeping expert setting will similarly mirror those in the non-sleeping expert algorithms.\nAlgorithm 13: AWAKEAUTOMATAWEIGHTEDMAJORITY(AwakeAWM). Algorithm: AWAKEAWM(C) CT ← C ∩ ST CT ← CONNECT(CT ) β ← SHORTESTDISTANCE((CT )R, F ) wCT [ICT ]← wCT [ICT ]/β[ICT ] α← 0 Flow0 ← 0 for each e ∈ ECT with dest[e] = (·, 1) do\nflow[e]← β[dest[e]] Flow0[σ[e]]← Flow0[σ[e]] + flow[e] Z0 ← Z0 + flow[e]\np1 ← Flow0/Z0 for t← 1 to T do\nRECEIVE(At) PAtt ← ∑ j∈At pt[j] for each i ∈ At do pAtt [i]← pt[i]/P At t it ←SAMPLE(pAtt ) PLAY(it) RECEIVE(lt) Flowt ← 0 for each e ∈ ECT with dest[e] = (·, t) do\nif σ[e] ∈ At then w[e]← w[e] · e−ηlt[σ[e]] flow[e]← α[src[e]]w[e]β[dest[e]] Flowt[σ[e]]← Flowt[σ[e]] + flow[e] ZAtt ← 0 if σ[e] ∈ At then\nZAtt ← Z At t + flow[e]\npt+1 ← Flowt P At t\nZ At t\nα← INCRSD(CT , t,α)"
    }, {
      "heading" : "12 Extension to online convex optimization",
      "text" : "We now illustrate how the framework described in this paper can be extended to the general online convex optimization (OCO) setting. Online convex optimization is a sequential prediction game over a compact convex action space K. At each round t, the learner plays an action xt ∈ K and receives a convex loss function ft. The goal of the learner is to minimize the regret against the best static loss:\nT∑ t=1 ft(xt)−min z∈K T∑ t=1 ft(z).\nAs in the framework introduced above, we can generalize this notion of regret to one against families of sequences. Specifically, let CT ⊆ KT be a closed subset, let pCT be a distribution over CT , and and let uCT be the uniform distribution over CT . The uniform distribution is well-defined, since K being a compact set implies that KT is compact. Then we would like to compete against the following regret against pCT :\nRegT (A,CT ) = max zT1 ∈CT T∑ t=1 ft(xt)− ft(zt) + log ( pCT (z T 1 ) uCT ) . (10)\nIf pCT is uniform, then the last term vanishes.\nWhen CT = KT is the family of all sequences of length T and pCT is the uniform distribution, this problem has been studied in [Hall and Willett, 2013, György and Szepesvári, 2016]. In both works, the authors introduce a variant of mirror descent which applies a mapping after the standard mirror descent update, which is called DYNAMICMIRRORDESCENT in the first algorithm.\nSpecifically, if gt ∈ ∂ft(xt) is an element of the subgradient, and Dψ is the Bregman divergence induced by a mirror map ψ, then DYNAMICMIRRORDESCENT consists of the update rule:\nx̃t+1 ← argmin x∈K 〈gt, x〉+Dψ(x, xt)\nxt+1 ← Φt(x̃t+1).\nIn this algorithm, Φt is an arbitrary mapping that is specified by the learner at time t. Under certain assumptions on the loss functions, Ψ, and Φt, DYNAMICMIRRORDESCENT achieves the following regret guarantee against the competitor distribution pCT :\nTheorem 15 (DYNAMICMIRRORDESCENT regret against CT ). Suppose that the Φts chosen in DYNAMICMIRRORDESCENT are non-expansive under the Bregman divergence Dψ:\nDψ(Φt(x),Φt(y)) ≤ Dψ(x, y), ∀x, y ∈ K.\nFurthermore, assume that ft is uniformly L-Lipschitz in the norm ‖ · ‖ and that Ψ is 1-strongly convex in the same norm. Let x1 ∈ K be given and define Dmax = supz∈KDψ(z, x1). Then, DYNAMICMIRRORDESCENT achieves the following regret guarantee:\nRegT (A,CT ) ≤ Dmax η + η 2 T∑ t=1 ‖gt‖2 + max zT1 ∈CT { log ( pCT (z T 1 ) uCT )\n+ 2\nη T∑ t=1 ψ(zt+1)− ψ(Φt(zt))− 〈∇ψ(xt+1), zt+1 − Φt(zt)〉 } .\nThe proof of this result follows along similar lines as the original result by [Hall and Willett, 2013]. The major difference is that the authors in that work assume Ψ to be Lipschitz. This allows them to derive a slightly weaker but more interpretable bound. However, it is also an assumption that we specifically choose to avoid, since mirror descent algorithms including the EXPONENTIATED GRADIENT use mirror maps that are not Lipschitz. Hall and Willett [2013] also derive a bound for standard regret as opposed to regret against a distribution of sequences.\nThe first two terms in the regret bound are standard in online convex optimization, and the last term is the price of competing against arbitrary sequences. Note that György and Szepesvári [2016] present the same algorithm but with a different analysis and upper bound.\nProof. By standard properties of the Bregman divergence and convexity, we can compute\nT∑ t=1 ft(xt)− ft(zt) = T∑ t=1 ft(xt)− ft(zt) + ft(x̃t+1)− ft(x̃t+1)\n≤ 1 η 〈∇ψ(xt)−∇ψ(x̃t+1), x̃t+1 − zt〉+ ft(xt)− ft(x̃t+1)\n= 1\nη [Dψ(zt, xt)−Dψ(zt, x̃t+1)−Dψ(x̃t+1, xt)] + ft(xt)− ft(x̃t+1)\n= 1\nη\n[ Dψ(zt, xt)−Dψ(zt+1, xt+1) +Dψ(zt+1, xt+1 −Dψ(Φt(zt), xt+1)\n−Dψ(zt, x̃t+1) +Dψ(Φt(zt), xt+1)−Dψ(x̃t+1, xt) ]\n+ ft(xt)− ft(x̃t+1).\nSince Φt is assumed to be non-expansive and xt+1 = Φt(xt+1), it follows that−Dψ(zt, x̃t+1)+Dψ(Φt(zt), xt+1 ≤ 0.\nSince Ψ is 1-strongly convex with respect to ‖ · ‖, it follows that Dψ(x̃t+1, xt) ≥ 12‖x̃t+1 − xt‖ 2. Thus, we can\ncompute\n− 1 η Dψ(x̃t+1, xt) + ft(xt)− ft(x̃t+1)\n≤ − 1 2η ‖x̃t+1 − xt‖2 + ft(xt)− ft(x̃t+1)\n≤ − 1 2η ‖x̃t+1 − xt‖2 + ‖gt‖∗‖xt − x̃t+1‖\n≤ − 1 2η ‖x̃t+1 − xt‖2 + η 2 ‖gt‖2∗ + 1 2η ‖xt − x̃t+1‖2 = η\n2 ‖gt‖2∗.\nMoreover, we can also write\nDψ(zt+1, xt+1 −Dψ(Φt(zt), xt+1) = ψ(zt+1)− ψ(xt+1)− 〈∇ψ(xt+1), zt+1 − xt+1〉\n[ψ(Φt(zt))− ψ(xt+1)− 〈∇ψ(xt+1),Φt(zt)− xt+1〉] = ψ(zt+1)− ψ(Φt(zt))− 〈ψ(xt+1), zt+1 − Φt(zt)〉.\nCombining this inequality with the inequality above yields\nT∑ t=1 ft(xt)− ft(zt)\n≤ 1 η T∑ t=1 Dψ(zt, xt)−Dψ(zt+1, xt+1)\n+ T∑ t=1 ψ(zt+1)− ψ(Φt(zt))− 〈ψ(xt+1), zt+1 − Φt(zt)〉+ 1 η T∑ t=1 η 2 ‖gt‖2∗\n≤ 1 η Dψ(z1, x1) + T∑ t=1 ψ(zt+1)− ψ(Φt(zt))− 〈ψ(xt+1), zt+1 − Φt(zt)〉\n+ 1\nη T∑ t=1 η 2 ‖gt‖2∗.\nAdding in log ( pCT (z T 1 )\nuCT\n) to both sides and taking the max over zT1 ∈ CT completes the proof.\nBy restricting our competitor set to CT and adding the penalization term, it follows that DYNAMICMIRRORDESCENT achieves the following guarantee:\nmax zT1 ∈CT T∑ t=1 ft(xt)− ft(zt) + log ( pCT (z T 1 ) uCT )\n≤ Dmax η + η 2 T∑ t=1 ‖gt‖2 + max zT1 ∈CT\n{ log ( pCT (z T 1 )\nuCT\n)\n+ 2\nη T∑ t=1 ψ(zt+1)− ψ(Φt(zt))− 〈∇ψ(xt+1), zt+1 − Φt(zt)〉\n} .\nThis bound suggests that if we are able to find a sequence (Φt)Tt=1 that minimizes the last quantity, then we can tightly bound our regret. Now let F be a family of dynamic maps Φ that are non-expansive with respect to Dψ . Then we want\nto solve the following optimization problem:\nmin ΦT1 ∈FT max zT1 ∈CT\n{ log ( pCT (z T 1 )\nuCT\n)\n+ 2\nη T∑ t=1 ψ(zt+1)− ψ(Φt(zt))− 〈∇ψ(xt+1), zt+1 − Φt(zt)〉\n} . (11)\nWe can view this as the online convex optimization analogue of the automata approximation problem in Section 7, and we can use it in the same way to derive concrete online convex optimization algorithms that achieve good regret against more complex families of sequences.\nAs an illustrative example, we apply this to the k-shifting experts setting and show how a candidate solution to this problem recovers the FIXED-SHARE algorithm."
    }, {
      "heading" : "12.1 OCO derivation of FIXED-SHARE",
      "text" : "Suppose that we are again in the prediction with expert advice setting so thatK = ∆N and ft(x) = 〈lt, x〉. Assume that CT is the set of k-shifting experts and that pCT is the uniform distribution on CT . As for the weighted majority algorithm, let Ψ = ∑N i=1 xi log(xi) be the negative entropy so that Dψ(x, y) = ∑N i=1 xi log ( xi yi ) is the relative entropy. One way of ensuring that Φt is non-expansive is to define it to be a mixture with a fixed vector: Φt(x) = (1− αt)x+ αtwt for some wt ∈ ∆N and αt ∈ [0, 1]. By convexity of the relative entropy, it follows that for any x, y ∈ ∆N , Dψ(Φt(x),Φt(y)) ≤ Dψ(x, y).\nFor simplicity, we can assume that Φt = Φ. Then Problem 11 can be written as:\nmin w∈∆N ,α∈[0,1] max zT1 ∈CT\n{ 2\nη T∑ t=1 ψ(zt+1)− ψ((1− α)zt + αw)\n− 〈∇ψ((1− α)x̃t+1 + αw), zt+1 − (1− α)zt − αw〉 } .\nSince CT is symmetric across coordinates and we do not have a priori knowledge of of x̃t+1, a reasonable choice of w is the uniform distribution wi = 1N . We can also use the fact that the entropy function is convex to obtain the upper bound: −ψ((1 − α)zt + αw) ≤ −(1 − α)ψ(z) − αψ(w). Moreover, since zt is always only supported on a single coordinate, ψ(zt) = 0 for every t.\nThis reduces to the following optimization problem:\nmin α∈[0,1] max zT1 ∈CT\n{ 2\nη T∑ t=1 α log(N)\n− N∑ i=1 log ( (1− α)x̃t+1,i + α 1 N )[ zt+1,i − (1− α)zt,i − α 1 N ]} .\nWe can break the objective into three separate terms:\nA1 : 2\nη T∑ t=1 α log(N)\nA2 : − T∑ t=1 N∑ i=1 log ( (1− α)x̃t+1,i + α 1 N ) [zt+1,i − zt,i]\nA3 : − T∑ t=1 N∑ i=1 log ( (1− α)x̃t+1,i + α 1 N ) α [ zt,i − 1 N ]\nIt is straightforward to see that A1 = 2ηTα log(N). To bound A2, let it ∈ [N ] be the index such that zt,it = 1 and zt,i = 0 for all i 6= it. Then,\n− T∑ t=1 N∑ i=1 log ( (1− α)x̃t+1,i + α 1 N ) [zt+1,i − zt,i]\n= − ∑\nt:it+1 6=it\nN∑ i=1 log ( (1− α)x̃t+1,i + α 1 N ) [zt+1,i − zt,i]\n≤ − ∑\nt:it+1 6=it\nN∑ i=1 log ( α 1 N ) zt+1,i\n≤ −k log ( α N ) .\nTo bound A3, we can write\n− α T∑ t=1 N∑ i=1 log ( (1− α)x̃t+1,i + α 1 N )[ zt,i − 1 N ]\n= −α T∑ t=1 log ( (1− α)x̃t+1,it + α 1 N )[ 1− 1 N ] ≤ −αT log ( α 1\nN\n) .\nPutting the pieces together, the objective is bounded by\n2\nη\n( Tα log(N)− k log ( α N ) − αT log ( α N )) ,\nleading to the new optimization problem:\nmin α∈[0,1]\n2\nη\n( Tα log(N)− k log ( α N ) − αT log ( α N )) .\nNotice that α ∝ kT is a reasonable solution, as it bounds the regret by O ( k log ( NT k )) .\nMoreover, this choice of α approximately corresponds to FIXED-SHARE. Thus, we have again derived the FIXEDSHARE algorithm from first principles in consideration of only the k-shifting expert sequences. This is in contrast to previous work for DYNAMICMIRRORDESCENT (e.g. [György and Szepesvári, 2016]) which only showed that one could define Φt in a way that mimics the FIXED-SHARE algorithm."
    }, {
      "heading" : "13 Conclusion",
      "text" : "We studied a general framework of online learning against a competitor class represented by a weighted automaton and showed that sublinear regret guarantees can be achieved in this scenario.\nWe gave a series of algorithms for this problem, including an automata-based algorithm extending weighted-majority whose computational cost at round t depends on the total number of transitions leaving the states of the competitor automaton reachable at time t, which substantially improves upon a naı̈ve algorithm based on path updates. We used the notion of failure transitions to provide a compact representation of the competitor automaton or its intersection with the set of strings of length t, thereby resulting in significant efficiency improvements. This required the introduction of new failure-transition-based composition and shortest-distance algorithms that could be of independent interest.\nWe further gave an extensive study of algorithms based on a compact approximation of the competitor automata. We showed that the key quantity arising when using an approximate weighted automaton is the Rényi divergence of the original and approximate automata. We presented a specific study of approximations based on n-gram models by minimizing the Rényi divergence and studied the properties of maximum likelihood n-gram models. We pointed out\nthe efficiency benefits of such approximations and provides guarantees on the approximations and the regret. We also extended our algorithms and results to the framework of sleeping experts. We further described the extension of the approximation methods to online convex optimization and a general mirror descent setting.\nOur description of this general (weighted) regret minimization framework and the design of algorithms based on automata provides a unifying view of many similar problems and leads to general algorithmic solutions applicable to a wide variety of problems with different competitor class automata. In general, automata lead to a more general and cleaner analysis. An alternative approximation method consists of directly minimizing the competitor class automaton before intersection with the set of strings of length t. We have also studied that method, presented guarantees for its success, and illustrated the approach in a special case.\nNote that, instead of automata and regular languages, we could have considered more complex formal language families such as (probabilistic) context-free languages over expert sequences. However, more complex languages can be handled in a similar way since the intersection with ST would be a finite language. The method based on a direct approximation would require approximating a probabilistic context-free language using weighted automata, a problem that has been extensively studied in the past [Pereira and Wright, 1991, Nederhof, 2000, Mohri and Nederhof, 2001].\nFinally, all our results can be straightforwardly extended to the adversarial bandit scenario using standard surrogate losses based on importance weighting techniques."
    }, {
      "heading" : "14 Additional proofs",
      "text" : ""
    }, {
      "heading" : "14.1 Proof of Theorem 1",
      "text" : "Proof. We will use a standard potential function-based argument. Let Φt be the potential defined by: Φt = log (∑K\nj=1 ṽt,j\n) . Then\nΦt − Φt−1 = log\n(∑K j=1 e\n−ηlt[πj,t]ṽt−1,j∑K j=1 ṽt−1,j\n) = log ( E\nj∼p̃t\n[ e−ηlt[πj,t] ]) ≤ E j∼p̃t [(−η)lt[πj,t]] + η2 = E i∼pt [(−η)lt[i]] + η2.\nSumming over t results in the following upper bound:\nΦT − Φ1 ≤ T∑ t=1 E i∼pt [(−η)lt[i]] + η2T.\nWe can also straightforwardly derive a lower bound for the same quantity:\nΦT − Φ1 = log  K∑ j=1 ṽT,j − log  K∑ j=1 ṽ1,j  ≥ log (ṽT,j)− log  K∑ j=1 ṽ1,j  = −η\nT∑ t=1 lt[πj,t] + η log(ṽ1,j)− log  K∑ j=1 ṽ1,j  . Combining the two statements above yields\n−η T∑ t=1 lt[πj,t] + η log(ṽ1,j)− log  K∑ j=1 ṽ1,j  ≤ E i∼pt [(−η)lt[i]] + η2T,\nwhich can be rearranged as\nT∑ t=1 E i∼pt [lt[i]]− T∑ t=1 lt[πj,t] ≤ ηT + 1 η log ( 1 ṽ1,j ) + 1 η log  K∑ j=1 ṽ1,j  . If ṽ1,j = 1 for all j ∈ [1,K], then the first result can be computed in a straightforward manner. If ṽ1,j = w[πj ]η , then it follows that\nT∑ t=1 E i∼pt [lt[i]]− T∑ t=1 lt[πj,t] ≤ ηT + log ( 1 w[πj ] ) + 1 η log  K∑ j=1 w[πj ] η  , which implies that\nT∑ t=1 E i∼pt [lt[i]]− T∑ t=1 lt[πj,t] + log (w[πj ]|C ∩ ST |) ≤ ηT + 1 η log  K∑ j=1 w[πj ] η|C ∩ ST |η  . When ∑K j=1 w[πj ] = 1, the maximization of the last term can be written as follows in terms of the Lagrange function:\nmax w\n1 η log  K∑ j=1 w[πj ] η + λ K∑ j=1 w[πj ].\nThe partial derivatives of the objective with respect to wj must satisfy 1η 1∑K\nj=1 w η j\nηwη−1j + λ = 0, thus the solution\nverifies that wj = ( −λη ∑K j=1 w η j ) 1 η−1 , which is uniform in j. Thus, the maximizing value is wj = 1K , yielding the bound: ηT + 1η log(|C ∩ ST |). By interpreting w as a distribution, we can consider the Hη(w), the η-Rényi entropy of w. Recall that Hη(w) =\n1 1−η log (∑K j=1 w[πj ] ) . Thus, we can write that\nηT + 1\nη log  K∑ j=1 w[πj ] η|C ∩ ST |η  = ηT + 1− η η Hη(w) + log(|C ∩ ST |)\n= ηT + 1\nη Hη(w)−Hη(w) + log(K).\nSince η 7→ Hη(w) is a decreasing function (see e.g. [Van Erven and Harremos, 2014]), it follows that η 7→ η√ Hη(w) is an increasing function that increases at least linearly. Since we assume that w is supported on more than a single point, H0(w) > 0. Thus, for any T , there exists a unique η∗ such that η 7→ η\n∗√ Hη∗ (w) = 1√ T . Moreover, it is also the case that\nfor every η ≤ η∗, η√ Hη(w) ≤ 1√ T . Thus, we have that:\nηT + 1\nη log  K∑ j=1 w[πj ] η|C ∩ ST |η  ≤ inf η≤η∗ ηT + 1 η Hη(w)−Hη(w) + log(K)\n≤ 2 √ THη∗(w)−Hη∗(w) + log(K)."
    }, {
      "heading" : "14.2 Proof of Lemma 1",
      "text" : "Proof. We can compute that:\n−D ( k + (1/2)\nT − 1 || k T − 1 ) = −D ( k\nT − 1\n( 1 + 1\n2k ) || k T − 1 ) = ( 1 + 1\n2k\n) k\nT − 1 log\nk T−1( 1 + 12k ) k T−1 +\n( 1− ( 1 + 1\n2k\n) k\nT − 1\n) log\n1− kT−1 1− ( 1 + 12k ) k T−1\n= ( 1 + 1\n2k\n) k\nT − 1 log\n1\n1 + 12k +\n( 1− k\nT − 1 − 1 2k k T − 1\n) log ( 1 +\n1 2k k T−1\n1− kT−1 − 1 2k k T−1\n)\n≤ ( 1 + 1\n2k\n) k\nT − 1 − 12k\n1 + 14k +\n( 1− k\nT − 1 − 1 2k k T − 1\n) 1 2k k T−1\n1− kT−1 − 1 2k k T−1\n(using log(1 + x) ≥ x 1 + x2 and log(1 + x) < x)\n= 1\n2k\nk\nT − 1\n( 1−\n1 + 12k 1 + 14k\n) = − 18k2 k T−1\n1 + 14k = − 14k2 k T−1 2 + 14k ≤ − 1 12k(T − 1) .\nSimilarly, we can also write: −D ((\n1− 1 2k\n) k\nT − 1 || k T − 1 ) = ( 1− 1\n2k\n) k\nT − 1 log\nk T−1( 1− 12k ) k T−1 +\n( 1− ( 1− 1\n2k\n) k\nT − 1\n) log\n1− kT−1 1− ( 1− 12k ) k T−1\n= ( 1− 1\n2k\n) k\nT − 1 log\n1\n1− 12k +\n( 1− k\nT − 1 +\n1\n2k\nk\nT − 1\n) log ( 1−\n1 2k k T−1\n1− kT−1 + 1 2k k T−1\n)\n≤ ( 1\n2k − 1 8k2\n) k\nT − 1 +\n( 1− k\nT − 1 +\n1\n2k\nk\nT − 1 ) − 12k kT−1 1− kT−1 + 1 2k k T−1\n= − 1 4k2 k\nT−1 2 = − 1 8k(T − 1) ."
    }, {
      "heading" : "14.3 Proof of Theorem 5",
      "text" : "Proof. Consider the mirror map ψ : (∆N )m → R defined by ψ(p) = ∑m j=1 ∑N i=1 pj(i) log pj(i). This induces the Bregman divergence:\nBψ(p,q) = m∑ j=1 N∑ i=1 pj(i) log ( pj(i) qj(i) ) .\nSince each relative entropy is 1-strongly convex with respect to the l1 norm over a single simplex, the additivity of strong convexity implies that Bψ is 1-strongly convex with respect to the l1 norm defined over(∆N )m.\nThe update described in the theorem statement corresponds to the mirror descent update based on Bψ:\npt+1 = argmin p∈(∆N )m\n〈gt,p〉+Bψ(p,pt).\nwhere gt ∈ ∂(f(pt)) is an element of the subgradient of f at pt. Thus, the standard mirror descent regret bound (e.g. [Bubeck et al., 2015]) implies that\nT∑ t=1 f(pt)− f(p∗) ≤ 1 ηT sup p∈(∆N )m ψ(p)− ψ(p1) + η √ 2L.\nThe result now follows from the fact that ψ(p) ≤ 0 and ψ(p1) = m log(N)."
    }, {
      "heading" : "14.4 Proof of Theorem 14",
      "text" : "Proof. Given any p, q ∈ ∆K , let D(p||q) denote the relative entropy between p and q. Given At ⊆ Σ, let p(At) =∑ i∈{1,...,K} : πi,t∈At . We first claim that if for any path πj , πj,t /∈ At, then p̃t+1,j = p̃t,j . This is because πj,t /∈ At implies that\nwt+1,j = wt,j . Moreover, the normalization step guarantees that ∑ i/∈At p̃t+1,i = ∑ i/∈At p̃t,i.\nThen, for any distribution u ∈ ∆K , we can compute that\nD(u||p̃t)−D(u||p̃t+1) = K∑ i=1 ui log p̃t+1,i p̃t,i\n= ∑\ni∈{1,...,K} : πi,t∈At\nui log p̃t+1,i p̃t,i\n= ∑\ni∈{1,...,K} : πi,t∈At\nui log p̃Att+1,i\np̃Att,i\n= ∑\ni∈{1,...,K} : πi,t∈At\nui log p̃Att,i e −ηlt[πi,t] p̃Att,i ∑K j=1 p̃ At t,je −ηlt[πj,t]\n= ∑\ni∈{1,...,K} : πi,t∈At\nui(−ηlt[πi,t])− ∑\ni∈{1,...,K} : πi,t∈At\nui log K∑ j=1 p̃Att,je −ηlt[πj,t]\n= ∑\ni∈{1,...,K} : πi,t∈At\n−ηuilt[πi,t])− ∑\ni∈{1,...,K} : πi,t∈At\nui log E j∼p̃Att\n[ e−ηlt[πj,t] ] .\nBy Hoeffding’s inequality, we can bound the last term by:\n≤ ∑ i∈At −ηuilt[πi,t])− ∑ i∈At ui\n[ E\nj∼p̃Att [−ηlt[πj,t]] + η2 ] = −η\n∑ i∈{1,...,K} : πi,t∈At uilt[πi,t] + ηu(At) E j∼p̃Att [lt[πj,t]]− u(At)η2\n= −η ∑\ni∈{1,...,K} : πi,t∈At\nuilt[πi,t] + ηu(At) E i∼pAtt [lt[i]]− u(At)η2.\nAfter rearranging terms and summing over t, it follows that\nT∑ t=1 u(At) E i∼pAtt [lt[i]]− ∑\ni∈∈{1,...,K} : πi,t∈At\nuilt[πi,t] ≤ η T∑ t=1 u(At) + 1 η D(u||p̃1).\nFor the unweighted regret, initializing ṽ1 = 1K implies that D(u||p̃1) ≤ log(K), which completes the argument."
    } ],
    "references" : [ {
      "title" : "A closer look at adaptive regret",
      "author" : [ "Dmitry Adamskiy", "Wouter M Koolen", "Alexey Chernov", "Vladimir Vovk" ],
      "venue" : "In International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Adamskiy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Adamskiy et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient string matching: An aid to bibliographic search",
      "author" : [ "A.V. Aho", "M.J. Corasick" ],
      "venue" : "Communication of the Association for Computing Machinery,",
      "citeRegEx" : "Aho and Corasick.,? \\Q1975\\E",
      "shortCiteRegEx" : "Aho and Corasick.",
      "year" : 1975
    }, {
      "title" : "3-way composition of weighted finite-state transducers",
      "author" : [ "Cyril Allauzen", "Mehryar Mohri" ],
      "venue" : "In International Conference on Implementation and Application of Automata,",
      "citeRegEx" : "Allauzen and Mohri.,? \\Q2008\\E",
      "shortCiteRegEx" : "Allauzen and Mohri.",
      "year" : 2008
    }, {
      "title" : "Generalized algorithms for constructing statistical language models",
      "author" : [ "Cyril Allauzen", "Mehryar Mohri", "Brian Roark" ],
      "venue" : "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume",
      "citeRegEx" : "Allauzen et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Allauzen et al\\.",
      "year" : 2003
    }, {
      "title" : "Spectral learning of general weighted automata via constrained matrix completion",
      "author" : [ "Borja Balle", "Mehryar Mohri" ],
      "venue" : "In 26th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Balle and Mohri.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balle and Mohri.",
      "year" : 2012
    }, {
      "title" : "On the Rademacher complexity of weighted automata",
      "author" : [ "Borja Balle", "Mehryar Mohri" ],
      "venue" : "In International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Balle and Mohri.,? \\Q2015\\E",
      "shortCiteRegEx" : "Balle and Mohri.",
      "year" : 2015
    }, {
      "title" : "Stochastic multi-armed-bandit problem with non-stationary rewards",
      "author" : [ "Omar Besbes", "Yonatan Gur", "Assaf Zeevi" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Besbes et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Besbes et al\\.",
      "year" : 2014
    }, {
      "title" : "Non-stationary stochastic optimization",
      "author" : [ "Omar Besbes", "Yonatan Gur", "Assaf Zeevi" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Besbes et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Besbes et al\\.",
      "year" : 2015
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "Nicolò Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2007
    }, {
      "title" : "Mirror descent meets fixed share (and feels no regret)",
      "author" : [ "Nicolò Cesa-Bianchi", "Pierre Gaillard", "Gábor Lugosi", "Gilles Stoltz" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2012
    }, {
      "title" : "An empirical study of smoothing techniques for language modeling",
      "author" : [ "Stanley Chen", "Joshua Goodman" ],
      "venue" : "Technical Report,",
      "citeRegEx" : "Chen and Goodman.,? \\Q1998\\E",
      "shortCiteRegEx" : "Chen and Goodman.",
      "year" : 1998
    }, {
      "title" : "Transductions and repetitions",
      "author" : [ "Maxime Crochemore" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Crochemore.,? \\Q1986\\E",
      "shortCiteRegEx" : "Crochemore.",
      "year" : 1986
    }, {
      "title" : "Strongly adaptive online learning",
      "author" : [ "Amit Daniely", "Alon Gonen", "Shai Shalev-Shwartz" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Daniely et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2015
    }, {
      "title" : "Using and combining predictors that specialize",
      "author" : [ "Yoav Freund", "Robert E Schapire", "Yoram Singer", "Manfred K Warmuth" ],
      "venue" : "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Freund et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 1997
    }, {
      "title" : "Shifting regret, mirror descent, and matrices",
      "author" : [ "András György", "Csaba Szepesvári" ],
      "venue" : "In ICML,",
      "citeRegEx" : "György and Szepesvári.,? \\Q2016\\E",
      "shortCiteRegEx" : "György and Szepesvári.",
      "year" : 2016
    }, {
      "title" : "Efficient tracking of large classes of experts",
      "author" : [ "András Gyorgy", "Tamás Linder", "Gábor Lugosi" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Gyorgy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gyorgy et al\\.",
      "year" : 2012
    }, {
      "title" : "Online optimization in dynamic environments",
      "author" : [ "Eric C Hall", "Rebecca M Willett" ],
      "venue" : "arXiv preprint arXiv:1307.5944,",
      "citeRegEx" : "Hall and Willett.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hall and Willett.",
      "year" : 2013
    }, {
      "title" : "Efficient learning algorithms for changing environments",
      "author" : [ "Elad Hazan", "Comandur Seshadhri" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Hazan and Seshadhri.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hazan and Seshadhri.",
      "year" : 2009
    }, {
      "title" : "Tracking the best expert",
      "author" : [ "Mark Herbster", "Manfred K Warmuth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Herbster and Warmuth.,? \\Q1998\\E",
      "shortCiteRegEx" : "Herbster and Warmuth.",
      "year" : 1998
    }, {
      "title" : "A spectral algorithm for learning hidden markov models",
      "author" : [ "Daniel Hsu", "Sham M Kakade", "Tong Zhang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2012
    }, {
      "title" : "Online optimization: Competing with dynamic comparators",
      "author" : [ "Ali Jadbabaie", "Alexander Rakhlin", "Shahin Shahrampour", "Karthik Sridharan" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Jadbabaie et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jadbabaie et al\\.",
      "year" : 2015
    }, {
      "title" : "Interpolated estimation of markov source parameters from sparse data",
      "author" : [ "Frederick Jelinek", "Robert L. Mercer" ],
      "venue" : "In Proceedings of the Workshop on Pattern Recognition in Practice,",
      "citeRegEx" : "Jelinek and Mercer.,? \\Q1980\\E",
      "shortCiteRegEx" : "Jelinek and Mercer.",
      "year" : 1980
    }, {
      "title" : "Learning hurdles for sleeping experts",
      "author" : [ "Varun Kanade", "Thomas Steinke" ],
      "venue" : "ACM Transactions on Computation Theory (TOCT),",
      "citeRegEx" : "Kanade and Steinke.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kanade and Steinke.",
      "year" : 2014
    }, {
      "title" : "Sleeping experts and bandits with stochastic action availability and adversarial rewards",
      "author" : [ "Varun Kanade", "HB McMahan", "Brent Bryan" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Kanade et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kanade et al\\.",
      "year" : 2009
    }, {
      "title" : "Estimation of probabilities from sparse data for the language model component of a speech recogniser",
      "author" : [ "Slava M. Katz" ],
      "venue" : "IEEE Transactions on Acoustic, Speech, and Signal Processing,",
      "citeRegEx" : "Katz.,? \\Q1987\\E",
      "shortCiteRegEx" : "Katz.",
      "year" : 1987
    }, {
      "title" : "Exponentiated gradient versus gradient descent for linear predictors",
      "author" : [ "Jyrki Kivinen", "Manfred K Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Kivinen and Warmuth.,? \\Q1997\\E",
      "shortCiteRegEx" : "Kivinen and Warmuth.",
      "year" : 1997
    }, {
      "title" : "Regret bounds for sleeping experts and bandits",
      "author" : [ "Robert Kleinberg", "Alexandru Niculescu-Mizil", "Yogeshwer Sharma" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2010
    }, {
      "title" : "Fast pattern matching in strings",
      "author" : [ "D.E. Knuth", "J.H. Morris Jr.", "V.R. Pratt" ],
      "venue" : "SIAM Journal of Comput. Syst. Sci.,",
      "citeRegEx" : "Knuth et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Knuth et al\\.",
      "year" : 1977
    }, {
      "title" : "Universal codes from switching strategies",
      "author" : [ "Wouter M Koolen", "Steven de Rooij" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Koolen and Rooij.,? \\Q2013\\E",
      "shortCiteRegEx" : "Koolen and Rooij.",
      "year" : 2013
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "Nick Littlestone", "Manfred K Warmuth" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "Littlestone and Warmuth.,? \\Q1994\\E",
      "shortCiteRegEx" : "Littlestone and Warmuth.",
      "year" : 1994
    }, {
      "title" : "String-Matching with Automata",
      "author" : [ "Mehryar Mohri" ],
      "venue" : "Nordic Journal of Computing,",
      "citeRegEx" : "Mohri.,? \\Q1997\\E",
      "shortCiteRegEx" : "Mohri.",
      "year" : 1997
    }, {
      "title" : "Weighted automata algorithms. In Handbook of weighted automata, pages 213–254",
      "author" : [ "Mehryar Mohri" ],
      "venue" : null,
      "citeRegEx" : "Mohri.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mohri.",
      "year" : 2009
    }, {
      "title" : "Regular approximation of context-free grammars through transformation",
      "author" : [ "Mehryar Mohri", "Mark-Jan Nederhof" ],
      "venue" : "In Robustness in language and speech technology,",
      "citeRegEx" : "Mohri and Nederhof.,? \\Q2001\\E",
      "shortCiteRegEx" : "Mohri and Nederhof.",
      "year" : 2001
    }, {
      "title" : "Online optimization in dynamic environments: Improved regret rates for strongly convex problems",
      "author" : [ "Aryan Mokhtari", "Shahin Shahrampour", "Ali Jadbabaie", "Alejandro Ribeiro" ],
      "venue" : "In Decision and Control (CDC),",
      "citeRegEx" : "Mokhtari et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mokhtari et al\\.",
      "year" : 2016
    }, {
      "title" : "Online learning of non-stationary sequences",
      "author" : [ "Claire Monteleoni", "Tommi S Jaakkola" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Monteleoni and Jaakkola.,? \\Q2003\\E",
      "shortCiteRegEx" : "Monteleoni and Jaakkola.",
      "year" : 2003
    }, {
      "title" : "Practical experiments with regular approximation of context-free languages",
      "author" : [ "Mark-Jan Nederhof" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Nederhof.,? \\Q2000\\E",
      "shortCiteRegEx" : "Nederhof.",
      "year" : 2000
    }, {
      "title" : "On structuring probabilistic dependences in stochastic language modeling",
      "author" : [ "Hermann Ney", "Ute Essen", "Reinhard Kneser" ],
      "venue" : "Computer Speech and Language,",
      "citeRegEx" : "Ney et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Ney et al\\.",
      "year" : 1994
    }, {
      "title" : "Finite-state approximation of phrase structure grammars",
      "author" : [ "Fernando CN Pereira", "Rebecca N Wright" ],
      "venue" : "In Proceedings of the 29th annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Pereira and Wright.,? \\Q1991\\E",
      "shortCiteRegEx" : "Pereira and Wright.",
      "year" : 1991
    }, {
      "title" : "The minimum consistent DFA problem cannot be approximated within any polynomial",
      "author" : [ "Leonard Pitt", "Manfred K Warmuth" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Pitt and Warmuth.,? \\Q1993\\E",
      "shortCiteRegEx" : "Pitt and Warmuth.",
      "year" : 1993
    }, {
      "title" : "On measures of entropy and information",
      "author" : [ "Alfréd Rényi" ],
      "venue" : "In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. The Regents of the University of California,",
      "citeRegEx" : "Rényi,? \\Q1961\\E",
      "shortCiteRegEx" : "Rényi",
      "year" : 1961
    }, {
      "title" : "On the probability of large deviations of random magnitudes",
      "author" : [ "Ivan Nikolaevich Sanov" ],
      "venue" : "Matematicheskii Sbornik,",
      "citeRegEx" : "Sanov.,? \\Q1957\\E",
      "shortCiteRegEx" : "Sanov.",
      "year" : 1957
    }, {
      "title" : "Distributed online optimization in dynamic environments using mirror descent",
      "author" : [ "Shahin Shahrampour", "Ali Jadbabaie" ],
      "venue" : "arXiv preprint arXiv:1609.02845,",
      "citeRegEx" : "Shahrampour and Jadbabaie.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shahrampour and Jadbabaie.",
      "year" : 2016
    }, {
      "title" : "Rényi divergence and kullback-leibler divergence",
      "author" : [ "Tim Van Erven", "Peter Harremos" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Erven and Harremos.,? \\Q2014\\E",
      "shortCiteRegEx" : "Erven and Harremos.",
      "year" : 2014
    }, {
      "title" : "Derandomizing stochastic prediction strategies",
      "author" : [ "Vladimir Vovk" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Vovk.,? \\Q1999\\E",
      "shortCiteRegEx" : "Vovk.",
      "year" : 1999
    }, {
      "title" : "Tracking the best expert in non-stationary stochastic environments",
      "author" : [ "Chen-Yu Wei", "Yi-Te Hong", "Chi-Jen Lu" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Wei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "This work has subsequently been improved to account for broader expert classes [Gyorgy et al., 2012], to deal with unknown parameters [Monteleoni and Jaakkola, 2003], and has been further generalized [Cesa-Bianchi et al.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 35,
      "context" : ", 2012], to deal with unknown parameters [Monteleoni and Jaakkola, 2003], and has been further generalized [Cesa-Bianchi et al.",
      "startOffset" : 41,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "Within the online learning framework, the setting of prediction with expert advice has received widespread attention [Littlestone and Warmuth, 1994, Cesa-Bianchi and Lugosi, 2006, Cesa-Bianchi et al., 2007]. In this setting, the algorithm maintains a distribution over a set of experts, or selects an expert from an implicitly maintained distribution. At each round, the loss assigned to each expert is revealed. The algorithm incurs the expected loss over the experts and then updates her distribution on the set of experts. The objective of the learner is to minimize his expected regret, which is defined as the cumulative loss of the algorithm minus the cumulative loss of the best expert chosen in hindsight. However, this benchmark is only significant when the best expert is expected to perform well. When this is not the case, then the learner may still play poorly. As an example, it may be that no single baseball team has performed well over all seasons in the past few years. Instead, different teams may have dominated over different time periods. This has led to a definition of regret against the best sequence of experts with k shifts in the seminal work of Herbster and Warmuth [1998] on tracking the best expert.",
      "startOffset" : 149,
      "endOffset" : 1202
    }, {
      "referenceID" : 5,
      "context" : "Within the online learning framework, the setting of prediction with expert advice has received widespread attention [Littlestone and Warmuth, 1994, Cesa-Bianchi and Lugosi, 2006, Cesa-Bianchi et al., 2007]. In this setting, the algorithm maintains a distribution over a set of experts, or selects an expert from an implicitly maintained distribution. At each round, the loss assigned to each expert is revealed. The algorithm incurs the expected loss over the experts and then updates her distribution on the set of experts. The objective of the learner is to minimize his expected regret, which is defined as the cumulative loss of the algorithm minus the cumulative loss of the best expert chosen in hindsight. However, this benchmark is only significant when the best expert is expected to perform well. When this is not the case, then the learner may still play poorly. As an example, it may be that no single baseball team has performed well over all seasons in the past few years. Instead, different teams may have dominated over different time periods. This has led to a definition of regret against the best sequence of experts with k shifts in the seminal work of Herbster and Warmuth [1998] on tracking the best expert. The authors showed that there exists an efficient on-line learning algorithm for this setting with favorable regret guarantees. This work has subsequently been improved to account for broader expert classes [Gyorgy et al., 2012], to deal with unknown parameters [Monteleoni and Jaakkola, 2003], and has been further generalized [Cesa-Bianchi et al., 2012, Vovk, 1999]. Another approach for handling dynamic environments has consisted of designing algorithms that guarantee small regret over any subinterval during the course of play. This notion coined as adaptive regret by Hazan and Seshadhri [2009] has been subsequently strengthened and generalized [Daniely et al.",
      "startOffset" : 149,
      "endOffset" : 1833
    }, {
      "referenceID" : 0,
      "context" : ", 2015, Adamskiy et al., 2012]. Remarkably, it was shown by Adamskiy et al. [2012] that the algorithm designed by Herbster and Warmuth [1998] is also optimal for adaptive regret.",
      "startOffset" : 8,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : ", 2015, Adamskiy et al., 2012]. Remarkably, it was shown by Adamskiy et al. [2012] that the algorithm designed by Herbster and Warmuth [1998] is also optimal for adaptive regret.",
      "startOffset" : 8,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : ", 2015, Adamskiy et al., 2012]. Remarkably, it was shown by Adamskiy et al. [2012] that the algorithm designed by Herbster and Warmuth [1998] is also optimal for adaptive regret. Koolen and de Rooij [2013] described a Bayesian framework for online learning where the learner samples from a distribution of expert sequences and predicts according to the prediction of that expert sequence.",
      "startOffset" : 8,
      "endOffset" : 206
    }, {
      "referenceID" : 17,
      "context" : "Another assumes that the learner has access to a dynamical model that is able to capture the benchmark sequence [Hall and Willett, 2013].",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "We then extend the results above to the sleeping expert setting [Freund et al., 1997], where the learner may not have access to advice from every expert at each round (Section 11).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : ", 2015, Besbes et al., 2015]. Another assumes that the learner has access to a dynamical model that is able to capture the benchmark sequence [Hall and Willett, 2013]. György and Szepesvári [2016] reinterpreted the framework of Hall and Willett [2013] to recover and extend the results of Herbster and Warmuth [1998].",
      "startOffset" : 8,
      "endOffset" : 197
    }, {
      "referenceID" : 6,
      "context" : ", 2015, Besbes et al., 2015]. Another assumes that the learner has access to a dynamical model that is able to capture the benchmark sequence [Hall and Willett, 2013]. György and Szepesvári [2016] reinterpreted the framework of Hall and Willett [2013] to recover and extend the results of Herbster and Warmuth [1998].",
      "startOffset" : 8,
      "endOffset" : 252
    }, {
      "referenceID" : 6,
      "context" : ", 2015, Besbes et al., 2015]. Another assumes that the learner has access to a dynamical model that is able to capture the benchmark sequence [Hall and Willett, 2013]. György and Szepesvári [2016] reinterpreted the framework of Hall and Willett [2013] to recover and extend the results of Herbster and Warmuth [1998]. In this paper, we significantly generalize the framework just described and consider prediction with expert advice in a setting where the learner’s cumulative loss is compared against that of sequences represented by an arbitrary weighted family of sequences.",
      "startOffset" : 8,
      "endOffset" : 317
    }, {
      "referenceID" : 6,
      "context" : ", 2015, Besbes et al., 2015]. Another assumes that the learner has access to a dynamical model that is able to capture the benchmark sequence [Hall and Willett, 2013]. György and Szepesvári [2016] reinterpreted the framework of Hall and Willett [2013] to recover and extend the results of Herbster and Warmuth [1998]. In this paper, we significantly generalize the framework just described and consider prediction with expert advice in a setting where the learner’s cumulative loss is compared against that of sequences represented by an arbitrary weighted family of sequences. We model this family using a weighted finite automaton (WFA). This strictly generalizes the notion of k-shifting regret and extends it to the notion of regret against a WFA. Measuring regret against an automaton is both natural and flexible. In fact, it may often be sensible to learn the set of competitor sequences using data before competing against it. For instance, the competitor automaton could be a language model trained over best sequences of baseball teams in the past. Moreover, the competitor automaton could be learned and reset incrementally. After each epoch, we could choose to learn a new competitor model and seek to perform well against that. We show that not only it is possible to achieve favorable regret against a WFA but that there exist computationally efficient algorithms to achieve that. We give a series of algorithms for this problem. Our first algorithm (Section 6) is an automata-based algorithm extending weighted-majority and using automata operations such as composition and shortest-distance; its computational cost is exponentially better than that of a naı̈ve method. We further present efficient algorithms based on a compact approximation of the competitor automaton (Section 7), in particular efficient n-gram models obtained by minimizing the Rényi divergence, and present an extensive study of the approximation properties of such models. We also show how existing algorithms for minimizing k-shifting regret can be recovered by learning a Maximum-Likelihood bigram language model over the k-shifting competitor automaton. To the best of our knowledge, this is the first instance of recovering the algorithms of Herbster and Warmuth [1998] by way of solely focusing on minimizing the k-shifting regret.",
      "startOffset" : 8,
      "endOffset" : 2262
    }, {
      "referenceID" : 32,
      "context" : "In the probability semiring, the order of the terms in the sum does not matter, and the quantity is well defined [Mohri, 2009].",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "Then, the (weighted or unweighted) regret of an algorithm A against Ck-shift coincides with the definition of k-shifting regret studied by Herbster and Warmuth [1998]:",
      "startOffset" : 139,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : "4 Naı̈ve algorithm A well-known algorithm for minimizing static regret in the prediction with expert advice setting is the weighted majority algorithm [Littlestone and Warmuth, 1994].",
      "startOffset" : 151,
      "endOffset" : 182
    }, {
      "referenceID" : 32,
      "context" : "A standard and efficient method for composing two weighted automata is to pair up matching transitions [Mohri, 2009].",
      "startOffset" : 103,
      "endOffset" : 116
    }, {
      "referenceID" : 32,
      "context" : "For any acyclic automaton A, α can be computed in linear time using a general relaxation-based shortest-distance algorithm with a topological queue discipline [Mohri, 2009].",
      "startOffset" : 159,
      "endOffset" : 172
    }, {
      "referenceID" : 32,
      "context" : "Connection (or trimming) is a linear-time algorithm that removes these non-accessible states [Mohri, 2009].",
      "startOffset" : 93,
      "endOffset" : 106
    }, {
      "referenceID" : 32,
      "context" : "Any probabilistic weighted automaton can be converted into a stochastic automaton using the weight-pushing algorithm [Mohri, 2009], which takes linear time and consists of a shortest-distance computation combined with a reweighting of the transition weights, initial weights, and final weights in a way that pushes the weights towards the initial states.",
      "startOffset" : 117,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "obtained from the expected count of the n-gram in the paths of CT , where the expectation is taken over the probability distribution defined by CT and can be computed efficiently [Allauzen et al., 2003].",
      "startOffset" : 179,
      "endOffset" : 202
    }, {
      "referenceID" : 3,
      "context" : "obtained from the expected count of the n-gram in the paths of CT , where the expectation is taken over the probability distribution defined by CT and can be computed efficiently [Allauzen et al., 2003]. Maximum likelihood n-gram models can further benefit from φ-conversion using the algorithms presented in Section 9. This can reduce the size of An and improve its computational efficiency without affecting its accuracy. As an example, we can compute the bigram approximation to the k-shifting automaton. Remarkably, this will coincide with the FIXED-SHARE algorithm of Herbster and Warmuth [1998]. Thus, we can view and motivate the design of FIXED-SHARE as a bigram approximation of the desired competitor automaton, that is the family of k-shifting sequences.",
      "startOffset" : 180,
      "endOffset" : 601
    }, {
      "referenceID" : 41,
      "context" : "Since each shift occurs with probability k T−1 , we can use Sanov’s theorem [Sanov, 1957] to write the following bound:",
      "startOffset" : 76,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : "The computational cost of using φ-AWM with this new φ-automaton coincides with the one described originally in [Herbster and Warmuth, 1998].",
      "startOffset" : 111,
      "endOffset" : 139
    }, {
      "referenceID" : 26,
      "context" : "The problem can be solved using as an optimization algorithm an extension of the Exponentiated Gradient algorithm developed by Kivinen and Warmuth [1997], which we call PROD-EG.",
      "startOffset" : 127,
      "endOffset" : 154
    }, {
      "referenceID" : 19,
      "context" : "This is a factor of N better than that of AWM, and only a factor of k worse than the specific algorithm of Herbster and Warmuth [1998]. Using a bigram approximation of the k-shifting automaton composed with ST and then introducing converting it into a φ-automaton, we obtain an algorithm that runs in O(N).",
      "startOffset" : 107,
      "endOffset" : 135
    }, {
      "referenceID" : 32,
      "context" : "As with finite automata, finite-state transducers can also be augmented with weights, and as with weighted finite automata, weighted finite-state transducers can be composed efficiently and on-demand [Mohri, 2009].",
      "startOffset" : 200,
      "endOffset" : 213
    }, {
      "referenceID" : 2,
      "context" : "The reason why redundant φ-paths are generated by standard composition algorithms is similar to the reason why redundant -paths are generated during composition of automata without failure transitions [Allauzen and Mohri, 2008].",
      "startOffset" : 201,
      "endOffset" : 227
    }, {
      "referenceID" : 2,
      "context" : "However, we can improve upon this by extending the 3-way composition technique presented in [Allauzen and Mohri, 2008] to our φ-automata setting.",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "This extension of standard prediction with expert advice is also known as the sleeping experts framework [Freund et al., 1997].",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "In [Freund et al., 1997], the comparison is made against the best fixed mixture of experts normalized at each round over the awake set: minu∈∆N ∑T t=1 ∑ i∈At uilt[i] ∑ j∈At uj .",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "This motivates the design of AWAKEPBWM, a path-based weighted majority algorithm that generalizes the algorithms in [Freund et al., 1997] to arbitrary families of expert sequences.",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : "The proof of this result follows along similar lines as the original result by [Hall and Willett, 2013].",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "The proof of this result follows along similar lines as the original result by [Hall and Willett, 2013]. The major difference is that the authors in that work assume Ψ to be Lipschitz. This allows them to derive a slightly weaker but more interpretable bound. However, it is also an assumption that we specifically choose to avoid, since mirror descent algorithms including the EXPONENTIATED GRADIENT use mirror maps that are not Lipschitz. Hall and Willett [2013] also derive a bound for standard regret as opposed to regret against a distribution of sequences.",
      "startOffset" : 80,
      "endOffset" : 465
    }, {
      "referenceID" : 15,
      "context" : "Note that György and Szepesvári [2016] present the same algorithm but with a different analysis and upper bound.",
      "startOffset" : 10,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : "[György and Szepesvári, 2016]) which only showed that one could define Φt in a way that mimics the FIXED-SHARE algorithm.",
      "startOffset" : 0,
      "endOffset" : 29
    } ],
    "year" : 2017,
    "abstractText" : "We consider a general framework of online learning with expert advice where the regret is defined with respect to a competitor class defined by a weighted automaton over sequences of experts. Our framework covers several problems previously studied, in particular that of competing against k-shifting experts. We give a series of algorithms for this problem, including an automata-based algorithm extending weightedmajority and more efficient algorithms based on the notion of failure transitions. We further present efficient algorithms based on a compact approximation of the competitor automaton, in particular efficient n-gram models obtained by minimizing the Rényi divergence, and present an extensive study of the approximation properties of such models. We also extend our algorithms and results to the framework of sleeping experts. Finally, we describe the extension of our approximation methods to online convex optimization and a general mirror descent setting.",
    "creator" : "LaTeX with hyperref package"
  }
}