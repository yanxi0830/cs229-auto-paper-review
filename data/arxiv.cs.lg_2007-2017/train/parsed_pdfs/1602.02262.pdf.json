{
  "name" : "1602.02262.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recovery guarantee of weighted low-rank approximation via alternating minimization",
    "authors" : [ "Yuanzhi Li", "Yingyu Liang", "Andrej Risteski" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n02 26\n2v 2\n[ cs\n.L G\n] 8\nD ec\n2 01\nThe key technical challenge is that under non-binary deterministic weights, naı̈ve alternating steps will destroy the incoherence and spectral properties of the intermediate solutions, which are needed for making progress towards the ground truth. We show that the properties only need to hold in an average sense and can be achieved by the clipping step.\nWe further provide an alternating algorithm that uses a whitening step that keeps the properties via SDP and Rademacher rounding and thus requires weaker assumptions. This technique can potentially be applied in some other applications and is of independent interest."
    }, {
      "heading" : "1 Introduction",
      "text" : "Recovery of low-rank matrices has been a recurring theme in recent years in machine learning, signal processing, and numerical linear algebra, since in many applications, the data is a noisy observation of a low-rank ground truth matrix. Typically, the noise on different entries is not identically distributed, which naturally leads to a weighted low-rank approximation problem: given the noisy observation M, one tries to recover the ground truth by finding M̃ that minimizes ‖M − M̃‖2W = ∑ ij Wi,j(Mi,j − M̃i,j)2 where the weight matrix W is chosen according to prior knowledge about the noise. For example, the co-occurrence matrix for words in natural language processing applications [Pennington et al., 2014, Arora et al., 2016] is such that the noise is larger when the co-occurrence of two words is rarer. When doing low-rank approximation on the co-occurrence matrix to get word embeddings, it has been observed empirically that a simple weighting can lead to much better performance than the unweighted formulation (see, e.g., [Levy and Goldberg, 2014]). In biology applications, it is often the case that the variance of the noise is different for each entry of a data matrix, due to various reasons such as different properties of different measuring devices. A natural approach to recover the ground truth matrix is to solve a weighted low-rank approximation problem where the weights are inversely proportional to the variance in the entries [Gadian, 1982, Wentzell et al., 1997]. Even for collaborative filtering, which is typically modeled as a matrix completion problem that assigns weight 1 on sampled entries and 0 on non-sampled entries, one can achieve better results when allowing non-binary weights [Srebro and Jaakkola, 2003].\n∗Department of Computer Science, Princeton University. Email:yuanzhil@cs.princeton.edu †Department of Computer Science, Princeton University. Email: yingyul@cs.princeton.edu ‡Department of Computer Science, Princeton University. Email:risteski@cs.princeton.edu\nIn practice, the weighted low-rank approximation is typically solved by non-convex optimization heuristics. One of the most frequently used is alternating minimization, which sets M̃ to be the product of two low-rank matrices and alternates between updating the two matrices. Although it is a natural heuristic to employ and also an interesting theoretical question to study, to the best of our knowledge there is no guarantee for alternating minimization for weighted low-rank approximation. Moreover, general weighted low-rank approximation is NP-hard, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].\nA special case of weighted low-rank approximation is matrix completion, where the weights are binary. Most methods proposed for solving this problem rely on the assumptions that the observed entries are sampled uniformly at random, and additionally often the observations need to be re-sampled across different iterations of the algorithm. This is inherently infeasible for the more general weighted low-rank approximation, and thus their analysis is not portable to the more general problem. The few exceptions that work with deterministic weights are [Heiman et al., 2014, Lee and Shraibman, 2013, Bhojanapalli and Jain, 2014]. In this line of work the state-of-the-art is [Bhojanapalli and Jain, 2014], who proved recovery guarantees under the assumptions that the ground truth has a strong version of incoherence and the weight matrix has a sufficiently large spectral gap. However, their results still only work for binary weights, use a nuclear norm convex relaxation and do not consider noise on the observed entries.\nIn this paper, we provide the first theoretical guarantee for weighted low-rank approximation via alternating minimization, under assumptions generalizing those in [Bhojanapalli and Jain, 2014]. In particular, assuming that the ground truth has a strong version of incoherence and the weight matrix has a sufficiently large spectral gap, we show that the spectral norm of the difference between the recovered matrix and the ground truth matrix is bounded by the spectral norm of the weighted noise plus an additive error term that decreases exponentially with the number of rounds of alternating minimization, from either initialization by SVD or, more importantly, random initialization. We emphasize that the bounds hold without any assumption on the noise, which is particularly important for handling complicated noise models. Since uniform sampling can satisfy our assumptions, our guarantee naturally generalizes those in previous works on matrix completion. See Section 4.1 for a detailed comparison.\nThe guarantee is proved by showing that the distance between the intermediate solution and the ground truth is improved at each iteration, which in spirit is similar to the framework in previous works. However, the lack of randomness in the weights and the exclusion of re-sampling (i.e., using independent samples at each iteration) lead to several technical obstacles that need to be addressed. Our proof of the improvement is then significantly different (and more general) from previous ones. In particular, showing improvement after each step is only possible when the intermediate solution has some additional special properties in terms of incoherence and spectrum. Prior works ensure such properties by using re-sampling (and sometimes assumptions about the noise), which are not available in our setting. We address this by showing that the spectral property only needs to hold in an average sense, which can be achieved by a simple clipping step. This results in a very simple algorithm that almost matches the practical heuristics, and thus provides explanation for them and also suggests potential improvement of the heuristics.\nFurther results The above results build on the insight that the spectral property only need to hold in an average sense. However, we can even make sure that the spectral property holds at each step strictly by a whitening step. More precisely, the clipping step is replaced by a whitening step using SDP and Rademacher rounding, which ensures that the intermediate solutions are incoherent and have the desired spectral property (the smallest eigenvalues of some related matrices are bounded). The technique of maintaining the smallest eigenvalues may be applicable to some other non-convex problems, and thus is of independent interest. The details are presented in Appendix C.\nFurthermore, combining our insight that the spectral property only need to hold in an average sense with the framework in [Sun and Luo, 2015], one can show provable guarantees for the family of algorithms analyzed there, including stochastic gradient descent. We will demonstrate this by including the proof details for stochastic gradient descent in a future version."
    }, {
      "heading" : "2 Related work",
      "text" : "Being a common practical problem (e.g., [Lu et al., 1997, Srebro and Jaakkola, 2003, Li et al., 2010, Eriksson and van den Hengel, 2012]), multiple heuristics for non-convex optimization such as alternating minimization have been\ndeveloped, but they come with no guarantees. On the other hand, weighted low-rank approximation is NP-hard in the worst case, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].\nOn the theoretical side, the only result we know of is [Razenshteyn et al., 2016], who provide a fixed-parameter tractability result when additionally the weight matrix is low-rank. Namely, when the weight matrix has rank r, they provide an algorithm for outputting a matrix M̃ which approximates the optimization objective up to a 1 + ǫ multiplicative factor, and runs in time nO(k\n2r/ǫ). A special case of weighted low rank approximation is matrix completion, where the goal is to recover a lowrank matrix from a subset of the matrix entries and corresponds to the case when the weights are in {0, 1}. For this special case much more is known theoretically. It is known that matrix completion is NP-hard in the case when the k = 3 [Peeters, 1996]. Assuming that the matrix is incoherent and the observed entries are chosen uniformly at random, Candès and Recht [2009] showed that nuclear norm convex relation can recover an n×n rank-k matrix using m = O(n1.2k log(n)) entries. The sample size is improved to O(nk log(n)) in subsequent papers [Candès and Tao, 2010, Recht, 2011, Gross, 2011]. Candes and Plan [2010] relaxed the assumption to tolerate noise and showed the nuclear norm convex relaxation can lead to a solution such that the Frobenius norm of the error matrix is bounded by O( √ n3/m) times that of the noise matrix. However, all these results are for the restricted case with uniformly random binary weight matrices. The only relaxations to random sampling to the best of our knowledge are in [Heiman et al., 2014, Lee and Shraibman, 2013, Bhojanapalli and Jain, 2014]. In this line the state-of-the-art is [Bhojanapalli and Jain, 2014], where the support of the observation is a d-regular expander such that the weight matrix has a sufficiently large spectral gap. However, it only works for binary weights, and is for a nuclear norm convex relaxation and does not incorporate noise.\nRecently, there is an increasing interest in analyzing non-convex optimization techniques for matrix completion. In two seminal papers [Jain et al., 2013, Hardt, 2014], it was shown that with an appropriate SVD-based initialization, the alternating minimization algorithm (with a few modifications) recovers the ground-truth. These results are for random binary weight matrix and crucially rely on re-sampling (i.e., using independent samples at each iteration), which is inherently not possible for the setting studied in this paper. More recently, Sun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion without re-sampling. However, the result is still for random binary weights and has not considered noise. More detailed comparison of our result with prior work can be found in Section 4, and comments on whether their arguments can be applied in our setting can be found in Section 5.\nWe also mention [Negahban and Wainwright, 2012] who consider random sampling, but one that is not uniformly random across the entries. In particular, their sampling produces a rank-1 matrix. (Additionally, they require the ground truth matrix to have nice properties such as low-rankness and spikiness.) The rank-1 assumption on the weight matrix is typically not true for many applications that introduce the weights to battle the different noise across the different entries of the matrix.\nFinally, two related works are [Bhojanapalli et al., 2015a,b]. The former implements faster SVD decomposition via weighted low rank approximation. However, here the weights in the weighted low rank problem come from leverage scores, so have a very specific structure, specially designed for performing SVD decompositions. The latter concerns optimization of strongly convex functions f(V) when V is in the set of positive-definite matrices. It does this in a non-convex manner, by setting V = UU⊤ and using the entries of U as variables. Our work focus on the recovery of the ground truth under the generative model, rather than on the optimization."
    }, {
      "heading" : "3 Problem definition and assumptions",
      "text" : "For a matrix A, let Ai denote its i-th column, Aj denote its j-th row, and Ai,j denote the element in i-th row and j-th column. Let ⊙ denote the Hadamard product, i.e., C = A⊙B means Ci,j = Ai,jBi,j .\nLet M∗ ∈ Rn×n be a rank-k matrix. Given the observation M = M∗ +N where N is a noise matrix, we want to recover the ground truth M∗ by solving the weighted low-rank approximation problem for M and a non-negative weight matrix W:\nmin M̃∈Rk\n∥∥∥M̃−M ∥∥∥ 2\nW\nwhere Rk is the set of rank-k n by n matrices, and ‖A‖2W = ∑ i,j Wi,jA 2 i,j is the weighted Frobenius norm. Our goal is to specify conditions about M∗ and W, under which M∗ can be recovered up to small error by alternating minimization, i.e., set M̃ = XY⊤ where X and Y are n by k matrices, and then alternate between updating the two matrices. Ideally, the recovery error should be bounded by ‖W ⊙N‖2, since this allows selecting weights according to the noise to make the error bound small.\nAs mentioned before, the problem is NP-hard in general, so we will need to impose some conditions. We summarize our assumptions as follows, and then discuss their necessity and the connections to existing ones.\n(A1) Ground truth is incoherent: M∗ has SVD UΣV⊤, wheremaxni=1{||Ui||22, ||Vi||22} ≤ µkn . Additionally, assume σmax(Σ) = Θ(1). (See discussion below.) Denote its condition number as τ = σmax(Σ)/σmin(Σ).\n(A2) Weight matrix has a spectral gap: ||W −E||2 ≤ γn, where γ < 1 and E is the all-one matrix.\n(A3) Weight is not degenerate: Let Di = Diag(Wi), i.e., Di is a diagonal matrix whose diagonal entries are the i-th row of W. Then there are 0 < λ ≤ 1 ≤ λ:\nλI U⊤DiU λI, and λI V⊤DiV λI(∀i ∈ [n]).\nThe incoherence assumption on the ground truth matrix is standard in the context of matrix completion. It is known that this is necessarily required for recovering the ground truth matrix. The assumption that σmax(Σ) = Θ(1) is without loss of generality: one can estimate σmax(Σ) up to a constant factor, scale the data and apply our results. The full details are included in the appendix.\nThe spectrum assumption on the weight matrix is a natural generalization of the randomness assumption typically made in matrix completion scenario (e.g., [Candes and Plan, 2010, Jain et al., 2013, Hardt, 2014]). In that case, W is a matrix with d = Ω(logn)-nonzeros in each row chosen uniformly at random, which corresponds to γ = O (\n1√ d\n)\nin (A2). Our assumption is also a generalization of the one in [Bhojanapalli and Jain, 2014], which requires W to be d-regular expander-like (i.e., to have a spectral gap) but is concerned only with matrix completion where the entries of W can be 0 or 1 only.\nThe final assumption (A3) is a generalization of the assumption A2 in [Bhojanapalli and Jain, 2014] that, intuitively, requires the singular vectors to satisfy RIP (restricted isometry property). This is because when the weights are binary, U⊤DiU = ∑ j∈S(U\nk)(Uk)⊤ where S is the support of Wi, so after proper scaling the assumption is a strict weakening of theirs. They viewed it as a stronger version of incoherence, discussed the necessity and showed that it is implied by the strong incoherence property assumed in [Candès and Tao, 2010]. In the context of more general weights, the necessity of (A3) is even more clear, as elaborated below.\nNote that since (A2) does not require W to be random or d-regular, it does not a-priori exclude the degenerate case that W has one all-zero column. In that case, clearly one cannot hope to recover the corresponding column of M∗. So, we need to make a third, non-degeneracy assumption about W, saying that it is “correlated” with M∗. The assumption is actually quite weak in the sense that when W is chosen uniformly at random, this assumption is true automatically: in those cases, E[Di] = I and thus E[U⊤DiU] = I since U is orthogonal. A standard matrix concentration bound can then show that our assumption (A3) holds with high probability. Therefore, it is only needed when considering a deterministic W. Intuitively, this means that the weights should cover the singular vectors of M∗. This prevents the aforementioned degenerate case when Wi = 0 for some i, and also some other degenerate cases. For example, consider the case when N = 0, all rows of M∗ are the same vector with first Θ(logn) entries being zero and the rest being one, and in one row of M∗ the non-zeros entries all have zero weight. In this case, there is also no hope to recover M∗, which should be excluded by our assumption."
    }, {
      "heading" : "4 Algorithm and results",
      "text" : "We prove guarantees for the vanilla alternating minimization with a simple clipping step, from either SVD initialization or random initialization. The algorithm is specified in Algorithm 1. Overall, it follows the usual alternating\nAlgorithm 1 Main Algorithm (ALT) Input: Noisy observation M, weight matrix W, number of iterations T\n1: Initialize Y1 using either Y1 = SVDINITIAL(M,W) or Y1 = RANDINITIAL 2: for t = 1, 2, ..., T do 3: X̃t+1 = argminX∈Rn×k ∥∥M−XY⊤t ∥∥ W 4: Xt+1 = CLIP(X̃t+1) 5: Xt+1 = QR(Xt+1) 6: Ỹt+1 = argminY∈Rn×k ∥∥M−Xt+1Y⊤ ∥∥ W 7: Yt+1 = CLIP(Ỹt+1) 8: Yt+1 = QR(Yt+1) 9: end for\nOutput: M̃ = XT+1YT\nAlgorithm 2 Clipping (CLIP)\nInput: matrix X̃ Output: matrix X with\nX i = { X̃i if ‖X̃i‖22 ≤ ξ := 2µkn 0 otherwise.\nAlgorithm 3 SVD Initialization (SVDINITIAL) Input: observation M, weight W\n1: (X̃,Σ, Ỹ) = rank-k SVD(W ⊙M), i.e., the columns of Ỹ are the top k right singular vectors of W ⊙M 2: Y = CLIP(Ỹ), Y = QR(Y)\nOutput: Y\nAlgorithm 4 Random Initialization (RANDINITIAL)\n1: Let Y ∈ Rn×k generated as Yi,j = bi,j 1√n , where bi,j’s are independent uniform from {−1, 1} Output: Y\nminimization framework: it keeps two working matrices X and Y, and alternates between updating them. In an X update step, it first updates X to be the minimizer of the weighted low rank objective while fixing Y, which can be done efficiently since now the optimization is convex. Then it performs a “clipping” step which zeros out rows of the matrix with too large norm,1 and then make it orthogonal by QR-factorization.2 At the end, the algorithm computes a final solution M̃ from the two iterates.\nThe two iterates can be initialized by performing SVD on the weighted observation (Algorithm 3), which is a weighted version of SVD initialization typically used in matrix completion. Moreover, we show that the algorithm works with random initialization (Algorithm 4), which is a simple and widely used heuristic in practice but rarely understood well.\nWe are now ready to state our main results. Theorem 1 describes our guarantee for the algorithm with SVD initialization, and Theorem 3 is for random initialization.\n1The clipping step zeros out rows with square ℓ2 norm twice larger than the upper bound µk/n imposed by our incoherence assumption (A1). One can choose the threshold to be cµk/n where c ≥ 2 is a constant and can choose to shrink the row to have norm no greater than µk/n, and our analysis still holds. The current choices are only for ease of presentation.\n2The QR-factorization step is not necessary for our analysis. But since it is widely used in practice for numerical stability, we prefer to analyze the algorithm with QR.\nTheorem 1 (Main, SVD initialization). Suppose M∗,W satisfy assumptions (A1)-(A3) with\nγ = O ( min {√ n\nD1\nλ\nτµ3/2k2 ,\nλ\nτ3/2µk2\n}) ,\nwhere D1 = maxi∈[n] ‖Wi‖1. Then after O(log(1/ǫ)) rounds of Algorithm 1 with initialization from Algorithm 3 outputs a matrix M̃ that satisfies\n‖M̃−M∗‖2 ≤ O ( kτ\nλ\n) ‖W ⊙N‖2 + ǫ."
    }, {
      "heading" : "The running time is polynomial in n and log(1/ǫ).",
      "text" : "The theorem is stated in its full generality. To emphasize the dependence on the matrix size n, the rank k and the incoherence µ, we can consider a specific range of parameter values where the other parameters (the spectral bounds, condition number, D1/n) are constants. Also, these parameter values are typical in matrix completion, which facilitates our comparison in the next subsection.\nCorollary 2. Suppose λ, λ and τ are all constants, D1 = Θ(n), and T = O(log(1/ǫ)). Furthermore,\nγ = O\n( 1\nµ3/2k2\n) .\nThen Algorithm 1 with initialization from Algorithm 3 outputs a matrix M̃ that satisfies\n‖M̃−M∗‖2 ≤ O (k) ‖W ⊙N‖2 + ǫ.\nRemarks The theorem bounds the spectral norm of the error matrix by the spectral norm of the weighted noise plus an additive error term that decreases exponentially with the number of rounds of alternating minimization. We emphasize that our guarantee holds for any M∗,W satisfying our deterministic assumptions; the high success probability is with respect to the execution of the algorithm, not to the input. This ensures the freedom in choosing the weights to battle the noise. We also emphasize that the bounds hold without any assumption on the noise, which is particularly important here since weighted low rank is typically applied to complicated noise models.\nBounding the error by ‖W ⊙ N‖2 is particularly useful when the noise is not uniform across the entries: prior knowledge about the noise (e.g., the different variances of noise on different entries) can be taken into account by setting up a reasonable weight matrix3, such that ‖W ⊙ N‖2 can be significantly smaller than ‖N‖2. Also, in recovering the ground truth, a spectral norm bound is more preferred than a Frobenius norm bound, since typically the Frobenius norm is √ n larger than the spectral norm.\nFurthermore, when ‖W ⊙ N‖2 = 0 (as in matrix completion without noise), the ground truth is recovered in a geometric rate.\nFinally, in matrix completion with uniform random sampled observations, the term D1 concentrates around n, so D1 n disappears in this case.\nTheorem 3 (Main, random initialization). Suppose M∗,W satisfy assumptions (A1)-(A3) with\nγ = O ( min {√ n\nD1\nλ\nτµ2k5/2 ,\nλ\nτ3/2µ3/2k5/2\n}) ,\n‖W‖∞ = O (\nλn\nk2µ log2 n\n) ,\n3Note that W cannot be made arbitrarily small since it should satisfy our assumptions. Roughly speaking, W has spectral norm n and is flexible to take into account the prior knowledge about the noise. In particular, it can be set to the all one matrix, reducing to the unweighted case.\nwhere D1 = maxi∈[n] ‖Wi‖1. Then after O(log(1/ǫ)) rounds Algorithm 1 with initialization from Algorithm 3 outputs a matrix M̃ that with probability at least 1− 1n2 satisfies\n‖M̃−M∗‖2 ≤ O ( kτ\nλ\n) ‖W ⊙N‖2 + ǫ."
    }, {
      "heading" : "The running time is polynomial in n and log(1/ǫ).",
      "text" : "Remarks Compared to SVD initialization, we need slightly stronger assumptions for random initialization to work. There is an extra 1/(µ1/2k1/2) in the requirement of the spectral parameter γ. We note that the same error bound is obtained when using random initialization. Roughly speaking, this is because our analysis shows that the updates can make improvement under rather weak requirements that random initialization can satisfy, and after the first step the rest updates make the same progress as in the case using SVD initialization."
    }, {
      "heading" : "4.1 Comparison with prior work",
      "text" : "For the sake of completeness, we will give a more detailed comparison with representative prior work on matrix completion from Section 2, emphasizing the dependence on n, k and µ and regarding the other parameters as constants. We first note that when the m observed entries are sampled uniformly at random from an n by n matrix, the corresponding binary weight matrix will have a spectral gap γ = O( √ n m ) (see, e.g., [Feige and Ofek, 2005]). Converting the sample bounds in the prior work to the spectral gap, we see that in general our result has worse dependence on parameters like the rank than those by convex relaxations, but has slightly better dependence than those by alternating minimization. The comparison is summarized in Table 1.\nThe seminal paper [Candès and Recht, 2009] showed that a nuclear norm convex relaxation approach can recover the ground truth matrix usingm = O(n1.2k log2 n) entries chosen uniformly at random and without noise. The sample size was improved to O(nk log6 n) in [Candès and Tao, 2010] and then O(nk logn) in subsequent papers. Candes and Plan [2010] generalized the result to the case with noise: the same convex program using m = O(nk log6 n) entries recovers a matrix M̃ s.t. ‖M̃−M∗‖F ≤ (2 + 4 √ (2 + p)n/p)‖NΩ‖F where p = m/n2 and NΩ is the noise projected on the observed entries.\nKeshavan et al. [2009] showed that withm = O(nµk logn), one can recover a matrix M̃ such that ∥∥∥M∗ − M̃ ∥∥∥ F =\nO ( n2 √ k m ‖NΩ‖2 ) by an optimization over a Grassmanian manifold.\nBhojanapalli and Jain [2014] relaxed the assumption that the entries are randomly sampled. They showed that the nuclear norm relaxation recovers the ground truth, assuming that the support Ω of the observed matrix forms a\nd-regular expander graph (or alike), i.e., |Ω| = dn, σ1(Ω) = d and σ2(Ω) ≤ c √ d and d ≥ c2µ2k2. This would correspond to a parameter γ = O( 1µk ) for us. They did not consider the robustness to noise. Hardt [2014] showed that with an appropriate initialization alternating minimization recovers the ground truth approximately. Precisely, they assumed N satisfies: (1). µ(N) . σmin(M∗)2;(2). ‖N‖∞ ≤ µn‖M∗‖F . Then, he shows that log(nǫ logn) alternating minimization steps recover a matrix M̃ such that ‖M̃−M∗‖F ≤ ǫ‖M‖F provided that pn ≥ k(k + log(n/ǫ))µ × ( ‖M∗‖F+‖N‖F /ǫ\nσk\n)2 ( 1− σk+1σk )5 where σk is the k-th singular value of the ground-\ntruth matrix. The parameter γ corresponding to the case considered there would be roughly O( 1 k √ µ logn ). While their algorithm has a good tolerance to noise, N is assumed to have special structure for him that we do not assume in our setting.\nSun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion. They showed that by using m = O(nkmax{µ logn, µ2k6}) randomly sampled entries without noise, the ground truth can be recovered in a geometric rate. This corresponds to a spectral gap ofO ( 1\nmax{ √ kµ log n,µk3.5}\n) .\nOur result is more general and also handles noise. When specialized to their setting, we also have a geometric rate with a slightly better dependence on the rank k but a slightly worse dependence on the incoherence µ."
    }, {
      "heading" : "5 Proof sketch",
      "text" : "Before going into our analysis, we first discuss whether arguments in prior work can be applied. Most of the work on matrix completion uses convex optimization and thus their analysis is not applicable in our setting. There indeed exists some other work that analyzes non-convex optimization for matrix completion, and it is tempting to adopt their arguments. However, there exist fundamental difficulties in porting their arguments. All of them crucially rely on the randomness in sampling the observed entries. Keshavan et al. [2009] analyzed optimization over a Grassmanian manifold, which uses the fact that E[W ⊙ S] = S for any matrix S. In [Jain et al., 2013, Hardt, 2014], re-sampling of new observed entries in different iterations was used to get around the dependency of the iterates on the sample set, a common difficulty in analyzing alternating minimization. The subtlety and the drawback of re-sampling were discussed in detail in [Bhojanapalli and Jain, 2014, Candes et al., 2015, Sun and Luo, 2015]. We note that [Sun and Luo, 2015] only needs sampling before the algorithm starts and does not need re-sampling in different iterations, but still relies on the randomness in the sampled entries. In particular, in all the aforementioned work, the randomness guarantees that the iterates X,Y stay incoherent and have good spectrum properties. Given these, alternating minimization can make progress towards the ground truth in each iteration. Nevertheless, since we focus on deterministic weights, such randomness is inherently infeasible in our setting. In this case, after just one iteration, it is unclear if the iterates can have incoherence and good spectrum properties required to progress towards the ground truth, even under our current assumptions. The whole algorithm thus breaks down. To address this, we show that it is sufficient to ensure the spectral property in an average sense and then introduce our clipping step to achieve that, arriving at our current algorithm.\nHere for simplicity, we drop the subscription t in all iterates, and we only focus on important factors, dropping other factors and the big-O notation. We only consider the case when W ⊙ N = 0, so as to emphasize the main technical challenges.\nOn a high level, our analysis of the algorithm maintains potential functions distc(X,U) and distc(Y,V) between our working matrices X,Y and the ground truth U,V (recall that M∗ = UΣV⊤):\ndistc(X,U) = min Q∈Ok×k\n‖XQ−U‖2\nand distc(Y,V) = min\nQ∈Ok×k ‖YQ−V‖2,\nwhere Ok×k are the set of k × k rotation matrices. The key is to show that they decrease after each update step, so X and Y get closer to the ground truth.4 The strategy of maintaining certain potential function measuring the distance\n4Note that we also need a good initialization, which can be done by SVD. Since our analysis requires rather weak warm start, we are able to show that simple random initialization is also sufficient (at the cost of slightly worse bounds).\nbetween the iterates and the ground truth is also used in prior work [Bhojanapalli and Jain, 2014, Candes et al., 2015, Sun and Luo, 2015]. We will point out below the key technical difficulties that are not encountered in prior work and make our analysis substantially different. The complete proofs are provided in the appendix due to space limitation."
    }, {
      "heading" : "5.1 Update",
      "text" : "We would like to show that after an X update, the new matrix X̃ satisfies distc(X,U) ≤ distc(Y,V)/2+ c for some small c (similarly for a Y update).\nConsider the update step X̃ ← argminA∈Rn×k ∥∥M−AY⊤ ∥∥ W .\nBy setting the gradient to 0 and with some algebraic manipulation, we have X̃−UΣV⊤Y = G where\nGi := UiΣV⊤Y⊥Y ⊤ ⊥DiY(Y ⊤DiY) −1.\nwhere Di = Diag(Wi). Since X̃ is the value prior to performing QR decomposition, we want to show that X̃ is close to UiΣV⊤Y, i.e., the error term G on right hand side is small. In the ideal case when the error term is 0, then X̃ = UΣV⊤Y and thus distc(X,U) = 0, meaning that with one update X̃ already hits into the correct subspace. So we would like to show that it is small so that the iterate still makes progress. Let\nPi = V ⊤Y⊥Y ⊤ ⊥DiY and Oi = (Y ⊤DiY) −1,\nso that Gi = UiΣPiOi. Now the two challenges are to bound Pi and Oi. Let us first consider the simpler case of matrix completion, where the entries of the matrix are randomly sampled by probability p. Then Di is a random diagonal matrix with E[Di] = I and E[D2i ] = 1 pI. Furthermore, for n × k orthogonal matrices Y, Oi = (Y⊤DiY)−1 concentrates around I. Then in expectation, ||Pi|| is about ||V⊤Y⊥||/√p and ‖Oi‖ is about 1, so ‖Gi‖ is as small as µk||V⊤Y⊥||/(√pn) = µksin θ(V,Y)/(√pn). High probability can then be established by the trick of re-sampling.\nHowever, in our setting, we have to deal with two major technical obstacles due to deterministic weights.\n1. There is no expectation for Di. Since ‖Di‖2∞ can be as large as n 2 poly(logn) , ‖Pi‖ can potentially be as large as sin θ(Y,V) npoly(logn) , which is almost a factor n larger than the bound for random Di. This is clearly insufficient to show the progress.\n2. A priori the norm of Oi = (Y⊤DiY)−1 may be large. Especially, in the algorithmY is given by the alternating minimization steps and giving an upper bound on ‖Oi‖ at all steps seems hard.\nThe first issue For this, we exploit the incoherence of Y and the spectral property of the weight matrix. If Di is the identity matrix, then Pi = 0 which, intuitively, means that there are cancellations between negative part and positive parts. When W is expander-like, it will put roughly equal weights on the negative part and the positive part. If furthermore we have that Y is incoherent (i.e., the negative and positive parts are spread out), then W can mix the terms and lead to a cancellation similar to that when Di = I. More precisely, consider the (j, j′)-th element in Pi. Define a new vector x ∈ Rn such that\nxi = (Ṽj)i(Yj′ )i, where Ṽ = V⊤Y⊥Y⊤⊥ .\nThen we have the cancellation in the form of ∑ i xi = 0. When Di = I, we simply get (Pi)j,j′ = ∑\ni xi = 0. When Di 6= I, we have (Pi)j,j′ = ∑ s∈[n](Di)sx j,j′ s . Now mix over all i, we have\n∑ i∈[n] ((Pi)j,j′) 2 =\n ∑\ns∈[n] (Di)sxs\n  2\n= ‖Wx‖2\n= ‖(W −E)x‖2 (since Ex = 0) ≤ γ2n2‖x‖2\nwhere in the last step we use the expander-like property of W (Assumption (A2)) to gain the cancellation. Furthermore, if ‖Yj′‖∞ is small, by definition ‖x‖2 is also small, so we can get an upper bound on ∑ i∈[n] ‖Pi‖2F .\nThen the problem reduces to maintaining the incoherence of Y. This is taken care of by our clipping step (Algorithm 2), which sets to 0 the rows of Y that are too large. Of course, we have to show that this will not increase the distance of the clipped Y and V. The intuition is that we clip only when ‖Yi‖ ≥ 2µk/n. But ‖Vi‖ ≤ µk/n, so after clipping, Yi only gets closer to Vi.\nThe second issue This is the more difficult technical obstacle, i.e., ‖Oi‖ = ‖(Y⊤DiY)−1‖ can be large. Our key idea is that although individual ‖Oi‖ can indeed be large, this cannot be the case on average. We show that there can just be a few i’s such that ‖Oi‖ is large, and they will not contribute much to ‖G‖, so the update can make progress.\nTo be more formal, we wish to bound the number of indices i such that σmin ( Y⊤DiY ) ≤ λ4 . Consider an\narbitrary unit vector a. Then,\naY⊤DiYa = ∑\nj\naY⊤(Di)jYa = ∑\nj\n(Di)j〈a,Yj〉2.\nWe know that Y is close to V, so we rewrite the above using some algebraic manipulation as\n∑\nj\n(Di)j〈a, (Yj −Vj) +Vj〉2\n≥ 1 4\n∑\nj\n(Di)j〈a,Vj〉2 − 1\n3\n∑\nj\n(Di)j〈a,Yj −Vj〉2\nFor j’s such that Yj is close to Vj (denote these j’s as Sg), then the terms can be easily bounded since V⊤DiV ≥ λI by assumption. So we only need to consider j’s such that Yj is far from Vj . Since we have incoherence, we know that ‖Yj −Vj‖ is still bounded in the order of µk/n. So aY⊤DiYa can be small only when ∑ j 6∈Sg (Di)j is large.\nLet S denote those bad i’s. Let uS be the indicator vector for S and ug be the indicator vector for [n− Sg]. ∑\ni∈S\n∑ j 6∈Sg (Di)j = u ⊤ SWug\n≤ |S|(n − |Sg|) + γn √ |S|(n− |Sg|)\nwhere the last step is due to the spectral property of W. Therefore, there can be only a few i’s with large ∑\nj 6∈Sg (Di)j ."
    }, {
      "heading" : "5.2 Proofs of main results",
      "text" : "We only need to show that we can get an initialization close enough to the ground truth so that we can apply the above analysis for the update. For SVD initialization,\n[X,Σ,Y] = rank-k SVD(W ⊙M∗ +W ⊙N).\nSince ||W ⊙ N||2 ≤ δ can be regarded as small, the idea is to show that W ⊙M∗ is close to M∗ in spectral norm and then apply Wedin’s theorem [Wedin, 1972]. We show this by the spectral gap property of W and the incoherence property of U,V.\nFor random initialization, the proof is only a slight modification of that for SVD initialization, because the update requires rather mild conditions on the initialization such that even the random initialization is sufficient (with slightly worse parameters)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we presented the first recovery guarantee of weighted low-rank matrix approximation via alternating minimization. Our work generalized prior work on matrix completion, and revealed technical obstacles in analyzing alternating minimization, i.e., the incoherence and spectral properties of the intermediate iterates need to be preserved. We addressed the obstacles by a simple clipping step, which resulted in a very simple algorithm that almost matches the practical heuristics."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329."
    }, {
      "heading" : "A Preliminaries about subspace distance",
      "text" : "Before delving into the proofs, we will prove a few simple preliminaries about subspace angles/distances.\nDefinition (Distance, Principle angle). Denote the principle angle of Y, V ∈ Rn×k as θ(Y,V). Then for orthogonal matrix Y (i.e., Y⊤Y = I),\ntan θ(Y,V) = ‖Y⊤⊥V(Y⊤V)−1‖2.\nFor orthogonal matrices Y, V,\ncos θ(Y,V) = σmin(Y ⊤V), sin θ(Y,V) = ‖(I−YY⊤)V‖2 = ‖Y⊥Y⊤⊥V‖2 = ‖Y⊤⊥V‖2, distc(Y,V) = min\nQ∈Ok×k ‖YQ−V‖2\nwhere Ok×k is the set of k × k orthogonal matrices.\nLemma 4 (Equivalence of distance). Let Y, V ∈ Rn×k be two orthogonal matrices, then we have:\nsin θ(Y,V) ≤ distc(Y,V) ≤ sin θ(Y,V) + 1− cos θ(Y,V) cos θ(Y,V) ≤ 2tan θ(Y,V).\nProof of Lemma 4. Suppose Q∗ = argminQ∈Ok×k‖YQ−V‖2. Let’s write V = YQ∗ +R, then distc(Y,V) = ‖R‖2. We have\nsin θ(Y,V) = ‖(I−YY⊤)V‖2 = ‖Y⊥Y⊤⊥R‖2 ≤ ‖R‖2\nOn the other hand, suppose ADB⊤ = SVD(Y⊤V), we know that σmin(D) = σmin(Y⊤V) = cos θ(Y,V). Therefore, by A = Y⊤VBD−1, AB⊤ ∈ Ok×k we have:\ndistc(Y,V) ≤ ‖YAB⊤ −V‖2 = ‖YY⊤VBD−1B⊤ −V‖2 ≤ ‖YY⊤VBD−1B⊤ −YY⊤V‖2 + ‖YY⊤V −V‖2 ≤ ‖BD−1B⊤ − I‖2 + sin θ(Y,V) = ‖D−1 − I‖2 + sin θ(Y,V)\n= sin θ(Y,V) + 1− cos θ(Y,V) cos θ(Y,V) .\nFinally, sin θ(Y,V) ≤ tan θ(Y,V) and 1−cos θ(Y,V)cos θ(Y,V) ≤ tan θ(Y,V) can be verified by definition, so the last inequality follows.\nFor convenience in our proofs we will also use the following generalization of incoherence:\nDefinition (Generalized incoherence). For a matrix A ∈ Rn×k, the generalized incoherence ρ(A) is defined as:\nρ(A) = max i∈[n] {n k ‖Ai‖22 }\nWe call it generalized incoherence for obvious reasons: when A is an orthogonal matrix, then ρ(A) = µ(A)."
    }, {
      "heading" : "B Proofs for alternating minimization with clipping",
      "text" : "We will show in this section the results for our algorithm based on alternating minimization with a clipping step. The organization is as follows. In Section B.1 we will present the necessary lemmas for the initialization, in Section B.3 we show the decrease of the potential function after one update step, and in Section B.4 we will put everything together, and prove our main theorem.\nBefore starting with the proofs, we will make a remark which will simplify the exposition. Without loss of generality, we may assume that\nδ = ‖W ⊙N‖2 ≤ λσmin(M\n∗)\n200k (B.1)\nOtherwise, we can output the 0 matrix, and the guarantee of all our theorems would be satisfied vacuously."
    }, {
      "heading" : "B.1 SVD-based initialization",
      "text" : "We want to show that after initialization, the matrices X,Y are close to the ground truth matrix U,V. Observe that [X,Σ,Y] = SVD(W ⊙ M) = SVD(W ⊙ (M∗ + N)) = SVD(W ⊙ M∗ + W ⊙ N). By our assumptions we know that ||W ⊙N||2 ≤ δ which we are thinking of as small, so the idea is to show that W ⊙M∗ is close to M∗ in spectral norm, then by Wedin’s theorem [Wedin, 1972] we will have X,Y are close to U,V. We show that W⊙M∗ is close to M∗ by the spectral gap property of W and the incoherence property of U,V.\nLemma 5 (Spectral lemma). Let W be an (entry wise non-negative) matrix in Rn×n with a spectral gap, i.e. W = E+ γnJΣWK\n⊤, where J,K are n× n (column) orthogonal matrices, with ||ΣW||2 = 1, γ < 1. Furthermore, for every matrix H ∈ Rn×n such that H = AΣB⊤ (A,B not necessarily orthogonal, Σ ∈ Rk×k is diagonal) we have\n‖(W −E)⊙H‖2 ≤ γkσmax(Σ) √ ρ(A)ρ(B)\nwhere E is the all one matrix.\nProof of Lemma 5. We know that for any unit vectors x, y ∈ Rn,\nx⊤ ((W −E)⊙H) y = k∑\nr=1\nσrx T ( (W −E)⊙ArB⊤r ) y\n= γn\nk∑\nr=1\nσr(Ar ⊙ x)⊤JΣWK⊤(Br ⊙ y)\n≤ γn k∑\nr=1\nσr||Ar ⊙ x||2||JΣWK⊤||2||Br ⊙ y||2\n≤ γn k∑\nr=1\nσr||Ar ⊙ x||2||Br ⊙ y||2\n≤ γnσmax(Σ)\n√√√√ k∑\nr=1\n||Ar ⊙ x||22\n√√√√ k∑\nr=1\n||Br ⊙ y||22\n≤ γnσmax(Σ)\n√√√√ n∑\ni=1\nx2i ||Ai||22\n√√√√ n∑\ni=1\ny2i ||Bi||22\n≤ γnσmax(Σ) √√√√k n ρ(A) ( n∑\ni=1\nx2i )√√√√k n ρ(B) ( n∑\ni=1\ny2i\n)\n≤ γσmax(Σ)k √ ρ(A)ρ(B).\nThe lemma follows from the definition of the operator norm.\nThe spectral lemma can be used to prove the initialization condition, when combined with Wedin’s theorem.\nLemma 6 (Wedin’s Theorem [Wedin, 1972]). Let M∗, M̃ be two matrices whose singular values are σ1, ..., σn and σ̃1, ..., σ̃n, let U,V and X,Y be the first k singular vectors (left and right) of M∗, M̃ respectively. If ∃α > 0 such that maxnr=k+1 σ̃r ≤ minki=1 σi − α, then\nmax {sin θ(U,X), sin θ(V,Y)} ≤ ||M̃−M ∗||2\nα .\nLemma 7. Suppose M∗,W satisfy all the assumptions, then for (X,Σ,Y) = rank-k SVD(W ⊙M), we have\nmax{tan θ(X,U), tan θ(Y,V)} ≤ 4(γµk + δ) σmin(M∗)\nProof of Lemma 7. We know that\n‖W ⊙M−M∗‖2 ≤ ||W ⊙M∗ −M∗||2 + ||W ⊙N||2 ≤ γµkσmax(M∗) + δ.\nTherefore, by Weyl’s theorem,\nmax{σr(W ⊙M) : k + 1 ≤ r ≤ n} ≤ γµk + δ ≤ 1\n2 σmin(M\n∗).\nwhere the last inequality holds because of B.1 and the assumption on γ in the theorem statement. Now, by Wedin’s theorem with α = 12σmin(M ∗), for (X,Σ,Y) = rank-k SVD(W ⊙M),\nmax {sin θ(U,X), sin θ(V,Y)} ≤ 2(γµk + δ) σmin(M∗)\nSince γ and δ are small enough, so sinθ ≤ 1/2. In this case, we have tanθ ≤ 2sinθ, then the lemma follows.\nFinally, this gives us the following guarantee on the initialization:\nLemma 8 (SVD initialization). Suppose M∗,W satisfy all the assumptions.\ndistc(V,Y1) ≤ 8k∆1, ρ(Y1) ≤ 2µ\n1− k∆1\nwhere ∆1 = 8(γµk+δ) σmin(M∗) .\nProof of Lemma 8. First, consider Ỹ1. By Lemma 7 and 4, we get that\ndistc(Ỹ1,V) ≤ ∆1\nwhich means that ∃Q ∈ Ok×k, s.t. ‖Ỹ1Q−V‖2 ≤ ∆1\nhence\n‖Ỹ1Q−V‖F ≤ k∆1 ≤ 1\n4\nwhere the last inequality follows since γ and δ are small enough.\nNext, consider Y1. In the clipping step, if ‖Ỹi1‖ ≥ ξ = 2µkn , then ‖Ỹi1 −Vi‖ ≥ µk n , and ‖Y\ni\n1 −Vi‖ = ‖Vi‖ = µk n . Otherwise, Y i = Ỹi. So\n‖Y1Q−V‖F ≤ ‖Ỹ1Q−V‖F ≤ 1\n4 .\nFinally, we can argue that Y1 is close to V. Let’s assume that Y1 = Y1R−1, for an upper-triangular R.\nsin θ(V,Y1) = ‖V⊤⊥Y1‖2 = ‖V⊤⊥(Y1 −VQ−1)R−1‖2 ≤ ‖Y1Q−V‖2‖R−1‖2 ≤ 1\nσmin(Y1) ‖Y1Q−V‖F\nwhere the second inequality follows because the singular values of R and Y1 are the same. Note that\nσmin(Y1) ≥ σmin(V)− ‖Y1 −V‖F ≥ σmin(V)− k∆1 = 1− k∆1 ≥ 1\n2\nSo\nsin θ(V,Y1) ≤ 2‖Y1Q−V‖F ≤ 1\n2 .\nIn this case, we have tan θ(V,Y1) ≤ 2sin θ(V,Y1) and thus\ndistc(V,Y1) ≤ 2tan θ(V,Y1) ≤ 4sin θ(V,Y1) ≤ 8‖Y1Q−V‖2 ≤ 8‖Y1Q−V‖F ≤ 8k∆1.\nFor ρ(Y1), observe that Yi1 = Y i R−1, so\n‖Yi1‖ ≤ ‖Y i 1‖‖R−1‖2 ≤ ξ σmin(Y1) ≤ ξ 1− k∆1\nwhich leads to the bound."
    }, {
      "heading" : "B.2 Random initialization",
      "text" : "With respect to the random initialization, the lemma we will need is the following one:\nLemma 9 (Random initialization). Let Y be a random matrix in Rn×k generated as Yi,j = bi,j 1√n , where bi,j are independent, uniform {−1, 1} variables. Furthermore, let ‖W‖∞ ≤ λnk2µ log2 n . Then, with probability at least 1− 1n2 over the draw of Y,\n∀i, σmin ( Y⊤DiY ) ≥ 1\n4\nλ\nkµ .\nProof of Lemma 9. Notice that Y⊤DiY = ∑ j(Y j)⊤(Di)jYj , and each of the terms (Yj)⊤(Di)jYj is independent. Furthermore, it’s easy to see that E[(Yj)⊤(Di)j(Yj)] = 1n (Di)j , ∀j. By linearity of expectation it follows that E[ ∑\nj(Y j)⊤(Di)jYj ] = 1 n ∑ j(Di)j .\nNow, we claim ∑\nj(Di)j ≥ λnkµ . Indeed, by Assumption (A3) we have for any vector a ∈ Rn\na⊤V⊤DiVa = ∑\nj\n(Di)j〈Vj , a〉2 ≥ λ.\nOn the other hand, however, by incoherence of V, ∑ j(Di)j〈Vj , a〉2 ≤ ∑ j(Di)j µk n . Hence, ∑ j(Di)j ≥ λ nkµ . Putting things together, we get\nE[ ∑\nj\n(Yj)⊤(Di)jY j ] ≥ λ\nkµ\nDenote\nB := ‖(Yj)⊤(Di)jYj‖2 ≤ k\nn (Di)j ≤\nλ\nkµ log2 n\nwhere the first inequality follows from our sampling procedure, and the last inequality by the assumption that ‖W‖∞ ≤ λn k2µ log2 n .\nSince all the random variables (Yj)⊤(Di)jYj are independent, applying Matrix Chernoff we get that\nPr\n ∑\nj\n(Yj)⊤(Di)j(Y j) ≤ (1 − δ) λ\nkµ\n  ≤ n ( e−δ\n(1− δ)(1−δ) ) λ kµB ≤ n (\ne−δ (1 − δ)(1−δ) )log2 n\nPicking δ = 34 , and union bounding over all i, with probability at least 1− 1n2 , for all i,\nσmin ( Y⊤DiY ) ≥ 1\n4\nλ\nkµ\nas needed."
    }, {
      "heading" : "B.3 Update",
      "text" : "We now prove the two key technical lemmas (Lemma 10 and Lemma 11) and then use them to prove that the updates make progress towards the ground truth. We prove them for Yt and use them to show Xt improves, while completely analogous arguments also hold when switching the role of the two iterates. Note that we measure the distance between Yt and V by distc(Yt,V) = minQ∈Ok×k ‖YtQ − V‖ where Ok×k is the set of k × k orthogonal matrices. For simplicity of notations, in these two lemmas, we let Yo = YtQ∗ where Q∗ = argminQ∈Ok×k‖YtQ−V‖.\nWe first show that there can only be a few i’s such that the spectral property of Y⊤o DiYo can be bad, when Yo is close to V. Let (Di)j be the j-th diagonal entry in Di, that is, (Di)j = Wi,j .\nLemma 10. Let Yo be a (column) orthogonal matrix in Rn×k, and ǫ ∈ (0, 1). If ‖Yo − V‖2F ≤ ǫ 3λ2n 128µkD1 for\nD1 = maxi∈[n] ∑ j(Di)j , then\n∣∣{i ∈ [n] ∣∣σmin(Y⊤o DiYo) ≤ (1− ǫ)λ }∣∣ ≤ 1024µ 2k2γ2D1\nǫ4λ3 ‖V−Yo‖2F .\nProof of Lemma 10. For a value g > 0 which we will specify shortly, we call j ∈ [n] “good” if ‖Yjo −Vj‖2 ≤ g2. Denote the set of “good” j’s as Sg .\nThen for every unit vector a ∈ Rk,\na⊤Y⊤o DiYoa = ∑\nj∈[n] (Di)j〈a,Yjo〉2\n≥ ∑\nj∈Sg (Di)j〈a,Yjo〉2\n= ∑\nj∈Sg (Di)j\n( 〈a,Vj〉+ 〈a,Yjo −Vj〉 )2\n≥ (1− ǫ 4 ) ∑\nj∈Sg (Di)j〈a,Vj〉2 −\n4− ǫ ǫ\n∑ j∈Sg (Di)j〈a,Yjo −Vj〉2\n(Using the fact ∀x, y ∈ R : (x+ y)2 ≥ (1− ǫ0)x2 − 1− ǫ0 ǫ0 y2)\n≥ (1− ǫ 4 ) ∑\nj∈Sg (Di)j〈a,Vj〉2 −\n4− ǫ ǫ g2 ∑\nj∈[n] (Di)j\n≥ (1− ǫ 4 ) ∑\nj∈[n] (Di)j〈a,Vj〉2 −\nµk\nn\n∑\nj∈[n]−Sg\n(Di)j − 4− ǫ ǫ g2 ∑\nj∈[n] (Di)j\nBy Assumption (A3), we know that ∑\nj∈[n] (Di)j〈a,Vj〉2 = aTV⊤DiVa ≥ σmin(V⊤DiV) ≥ λ\nMoreover, recall D1 = maxi∈[n] ∑ j(Di)j , so when g 2 ≤ ǫ 2λ 16D1 ,\n4− ǫ ǫ g2 ∑\nj∈[n] (Di)j ≤\nǫλ\n4\nLet us consider now ∑\nj∈[n]−Sg (Di)j . Define:\nS =   i ∈ [n] ∣∣∣∣∣∣ µk n ∑\nj∈[n]−Sg\n(Di)j ≥ ǫλ\n4   \nThen it is sufficient to bound |S|. For Sg , observe that\n∑\nj\n‖Vj −Yjo‖22 = ‖V−Yo‖2F\nWhich implies that\n|[n]− Sg| = size ([n]− Sg) ≤ ‖V−Yo‖2F\ng2\nLet uS be the indicator vector of S, and ug be the indicator vector of [n]− Sg, we know that\nu⊤SWug = ∑\ni∈S\n∑\nj∈[n]−Sg\n(Di)j\n≥ ǫλn 4µk |S|\nOn the other hand,\nu⊤SWug = u ⊤ SEug + u ⊤ S (W −E)ug\n≤ |S||[n]− Sg|+ γn √ |S||[n]− Sg|\nPutting these two inequalities together, we have\n|[n]− Sg|+ γn √\n|[n]− Sg| |S| ≥ ǫλn 4µk\nWhich implies when |[n]− Sg| ≤ ǫλn8µk , we have:\n|S| ≤ 64µ 2k2γ2|[n]− Sg| ǫ2λ2 ≤ 64µ 2k2γ2‖V −Yo‖2F ǫ2λ2g2\nThen, setting g2 = ǫ 2λ\n16D1 , we have:\n∣∣{i ∈ [n] ∣∣σmin(Y⊤o DiYo) ≤ (1− ǫ)λ }∣∣ ≤ |S| ≤ 1024µ 2k2γ2D1\nǫ4λ3 ‖V−Yo‖2F\nwhich is what we need.\nLemma 11. Let Yo be a (column) orthogonal matrix in Rn×k. Then we have ∑\ni∈[n] ‖V⊤Y⊥Y⊤⊥DiYo‖22 ≤ γ2ρ(Yo)nk3‖Yo −V‖22\nProof of Lemma 11. We want to bound the spectral norm of V⊤Y⊥Y⊤⊥DiYo, for a fixed j ∈ [k], let Yj be the j-th column of Yo and Ṽj be the j-th column of Y⊥Y⊤⊥V.\nFor fixed j, j′ ∈ [k], consider a new vector xj,j′ ∈ Rn such that xj,j ′\ni = (Ṽj)i(Yj′ )i.\nNote that 〈Ṽj ,Yj′ 〉 = 0, which implies that ∑ i x j,j′\ni = 0. Let us consider V⊤j Y⊥Y ⊤ ⊥DiYj′ , we know that\nV⊤j Y⊥Y ⊤ ⊥DiYj′ =\n∑\ns∈[n] (Di)s(Ṽj)s(Yj′ )s\n= ∑\ns∈[n] (Di)sx\nj,j′ s\nWhich implies that\n∑\ni∈[n]\n ∑\ns∈[n] (Di)sx\nj,j′ s\n  2\n= ‖Wxj,j′‖22\n= ‖(W −E)xj,j′‖22 (since Exj,j ′ = 0)\n≤ γ2n2‖xj,j′‖22 Observe that\n‖xj,j′‖22 = ∑\ni∈[n] (xj,j\n′\ni ) 2\n= ∑\ni∈[n] (Ṽj)\n2 i (Yj′ ) 2 i\n≤ ρ(Yo)k n\n∑ i∈[n] (Ṽj) 2 i\n= ρ(Yo)k\nn ‖Ṽj‖22\n≤ ρ(Yo)k n ‖Y⊥Y⊤⊥V‖22 = ρ(Yo)k\nn ‖Y⊥Y⊤⊥(Yo −V)‖22\n≤ ρ(Yo)k n ‖Yo −V‖22.\nWhich implies\n∑\ni∈[n]\n ∑\ns∈[n] (Di)sx\nj,j′ s\n  2\n≤ γ2ρ(Yo)nk‖Yo −V‖22\nNow we are ready to bound V⊤Y⊥Y⊤⊥DiYo. Note that\n‖V⊤Y⊥Y⊤⊥DiYo‖22 ≤ ‖V⊤Y⊥Y⊤⊥DiYo‖2F ≤ ∑\nj,j′∈[k]\n( V⊤j Y⊥Y ⊤ ⊥DiYj′ )2\n= ∑\nj,j′∈[k]\n ∑\ns∈[n] (Di)sx\nj,j′ s\n  2\n.\nThis implies that\n∑ i∈[n] ‖V⊤Y⊥Y⊤⊥DiYo‖22 ≤ ∑ i∈[n] ∑ j,j′∈[k]\n ∑\ns∈[n] (Di)sx\nj,j′ s\n  2\n≤ γ2ρ(Yo)nk3‖Yo −V‖22.\nas needed.\nWe now use the two technical lemmas to prove the guarantees for the iterate after one update step.\nLemma 12 (Update, main). Let Y be a (column) orthogonal matrix in Rn×k, and dist2c(Y,V) ≤ min{ 12 , λ2n 384µk2D1 }\nfor D1 = maxi∈[n] ∑ j(Di)j .\nDefine X̃ ← argminX∈Rn×k ∥∥M−XY⊤ ∥∥ W . Let X a n× k matrix such that for each row:\nX i = { X̃i if ‖X̃i‖22 ≤ ξ = 2µkn 0 otherwise.\nSuppose X has QR decomposition X = XR. Then (1) ‖X−UΣV⊤Y‖2F ≤ ∆2u := ( 108ξµ2k3γ2D1 λ2 + 160γ 2µρ(Y)k4 λ2 ) distc(Y,V)2 + 160kλ2 ‖W ⊙N‖22. (2) If ∆u ≤ 18σmin(M∗), then\ndistc(U,X) ≤ 8\nσmin(M∗)− 2∆u ∆u and ρ(X) ≤\n4µ\nσmin(M∗)− 2∆u .\nProof of Lemma 12. (1) By KKT condition, we know that for orthogonal Y, the optimal X̃ satisfies ( W ⊙ [ M− X̃Y⊤ ]) Y = 0\nwhich implies that the i-th row X̃i of X̃ is given by\nX̃i = MiDiY ( Y⊤DiY )−1 = (M∗)iDiY ( Y⊤DiY )−1 +NiDiY ( Y⊤DiY )−1 .\nLet us consider the first term, by M∗ = UΣV⊤, we know that\n(M∗)iDiY ( Y⊤DiY )−1 = UiΣV⊤DiY ( Y⊤DiY )−1\n= UiΣV⊤(YY⊤ +Y⊥Y ⊤ ⊥)DiY ( Y⊤DiY )−1 = UiΣV⊤Y +UiΣV⊤Y⊥Y ⊤ ⊥DiY ( Y⊤DiY )−1\nwhich implies that\nX̃i −UiΣV⊤Y = UiΣV⊤Y⊥Y⊤⊥DiY ( Y⊤DiY )−1 +NiDiY ( Y⊤DiY )−1\nLet us consider set\nS1 = { i ∈ [n] ∣∣∣∣σmin(Y ⊤DiY) ≤ λ\n4\n}\nNow we have:\n∑\ni/∈S1\n∥∥∥X̃i −UiΣV⊤Y ∥∥∥ 2\n2 ≤ 16 λ2\n∑\ni/∈S1\n( 2‖UiΣV⊤Y⊥Y⊤⊥DiY‖22 + 2‖NiDiY‖22 )\n≤ 32µk‖Σ‖ 2 2\nnλ2\n∑\ni/∈S1\n‖V⊤Y⊥Y⊤⊥DiY‖22 + 32\nλ2\n∑ i∈[n] ‖NiDiY‖22\n≤ 32µk‖Σ‖ 2 2\nnλ2\n∑ i∈[n] ‖V⊤Y⊥Y⊤⊥DiY‖22 + 32 λ2 ‖(W ⊙N)Y‖2F\n≤ ∆g := 32γ2µρ(Y)k4\nλ2 distc(Y,V)2 +\n32k\nλ2 ‖(W ⊙N)‖22.\nwhere the last inequality is due to Lemma 11. Note that since ξ = 2µkn ≥ 2‖UiΣV⊤Y‖22, this implies ∣∣∣ { i ∈ [n]− S1 ∣∣∣‖X̃i‖22 ≥ ξ }∣∣∣ ≤ ∣∣∣∣ { i ∈ [n]− S1 ∣∣∣∣‖X̃i −UiΣV⊤Y‖22 ≥ ξ\n2 }∣∣∣∣ ≤ 2∆g ξ .\nLet S2 = { i ∈ [n]− S1 ∣∣∣‖X̃i‖22 ≥ ξ } , we have:\n∥∥X−UΣV⊤Y ∥∥2 F = n∑\ni=1\n∥∥∥Xi −UiΣV⊤Y ∥∥∥ 2\n2 (because ‖Xi‖22 ≤ ξ and ‖UiΣV⊤Y‖22 ≤ ξ)\n≤ ∑\ni∈S1∪S2 2ξ +\n∑\ni6∈S1∪S2\n∥∥∥X̃i −UiΣV⊤Y ∥∥∥ 2\n2\n≤ 2ξ(|S1|+ |S2|) + ∑\ni6∈S1∪S2\n∥∥∥X̃i −UiΣV⊤Y ∥∥∥ 2\n2\n≤ 2ξ|S1|+ 4∆g +∆g.\nBy Lemma 10, we know that |S1| ≤ 54µ 2k3γ2D1 λ2 ‖V−Y‖22. Further plugging in ∆g , we have ∥∥X−UΣV⊤Y ∥∥2 F\n≤ 2ξ 54µ 2k3γ2D1\nλ2 ‖V−Y‖22 +\n160γ2µρ(Y)k4\nλ2 ‖Y −V‖22 +\n160k\nλ2 ‖(W ⊙N)‖22\n=\n( 108ξµ2k3γ2D1\nλ2 +\n160γ2µρ(Y)k4\nλ2\n) ‖Y −V‖22 + 160k\nλ2 ‖(W ⊙N)‖22.\n(2) Denote B = ΣV⊤Y. Then,\nsin θ(U,X) = ‖U⊤⊥X‖2 = ‖U⊤⊥(X−UB)R−1‖2 ≤ ‖X−UB‖2‖R−1‖2 = 1\nσmin(X) ‖X−UB‖2\nSince ‖X−UB‖2 ≤ ∆u, we have\nσmin(X) ≥ σmin(UB)−∆u = σmin(ΣV⊤Y)−∆u ≥ σmin(M∗)cos θ(Y,V) −∆u.\nBy the assumption cos θ(Y,V) ≥ 1/2, so\nsin θ(U,X) ≤ 2 σmin(M∗)− 2∆u ∆u.\nWhen ∆u ≤ 18σmin(M∗), the right hand side is smaller than 1/3, so cos θ(U,X) ≥ 1/2, and thus tan θ(U,X) ≤ 2sin θ(U,X). Then the statement on distc(U,X) follows from distc(U,X) ≤ 2tan θ(U,X) ≤ 4sin θ(U,X).\nFinally, observe that Xi = X i R−1, so\n‖Xi‖2 ≤ ‖X i‖2‖R−1‖2 ≤\nξ\nσmin(X)\nwhich leads to the bound.\nB.4 Putting everything together: proofs of the main theorems\nFinally, in this section we put things together and prove the main theorems. We first proceed to the SVD-initialization based algorithm:\nTheorem 1. If M∗,W satisfy assumptions (A1)-(A3), and\nγ = O ( min {√ n\nD1\nλ\nτµ3/2k2 ,\nλ\nτ3/2µk2\n}) ,\nthen after O(log(1/ǫ)) rounds Algorithm 1 with initialization from Algorithm 3 outputs a matrix M̃ that satisfies\n||M̃−M∗||2 ≤ O ( kτ\nλ\n) ||W ⊙N||2 + ǫ."
    }, {
      "heading" : "The running time is polynomial in n and log(1/ǫ).",
      "text" : "Proof of Theorem 1. We first show by induction distc(Xt,U) ≤ 12t + 70 kλσmin(M∗)δ for t > 1, and distc(Yt,U) ≤ 1 2t + 70 k λσmin(M∗)\nδ for t ≥ 1. First, by Lemma 8, Y1 satisfies\ndistc(V,Y1) ≤ 8k∆1 = 64k(γµk + δ)\nσmin(M∗) .\nSince γ = O (\n1 τk2µ\n) , the base case follows. Now proceed to the inductive step and prove the statement for t + 1\nassuming it is true for t. Now we can apply Lemma 12. By taking the constants within the O(·) notation for γ sufficiently small and by the inductive hypothesis, we have\n( 108ξµ2k3γ2D1\nλ2 +\n160γ2µρ(Y1)k 4\nλ2\n) ≤ 1\n100 σ2min(M ∗)\nand\n∆u ≤ 1\n8 σmin(M\n∗).\nBy Lemma 12, we get\ndistc(U,Xt+1) ≤ 2\nσmin(M∗)− 2∆u ∆u ≤\n8\n3σmin(M∗) ∆u\n= 8\n3σmin(M∗)\n√( 108ξµ2k3γ2D1\nλ2 +\n160γ2µρ(Y1)k4\nλ2\n) dist2c(U,Xt) + 160k\nλ2 δ2\n≤ 8 3σmin(M∗)\n(√( 108ξµ2k3γ2D1\nλ2 +\n160γ2µρ(Y1)k4\nλ2\n) dist2c(Yt,V) + √ 160k\nλ2 δ2\n) (using √ a+ b ≤ √a+ √ b)\n≤ 1 2 distc(Yt,V) + 35\n√ k\nλσmin(M∗) δ\nso the statement also holds for t+ 1. This completes the proof for bounding distc(Xt,U) and distc(Yt,V). Given the bounds on distc(Xt,U) and distc(Yt,V), we are now ready to prove the theorem statement. For simplicity, let X denote XT+1 and Y denote YT , so the algorithm outputs M̃ = XY. By Lemma 12,\n‖X−UΣV⊤Y‖2F ≤ ∆2u := ( 108ξµ2k3γ2D1\nλ2 +\n160γ2µρ(Y)k4\nλ2\n) distc(Y,V)2 + 160k\nλ2 ‖W ⊙N‖22.\nPlugging the choice of γ and noting ξ = 2µkn and ρ(Y) = O(µ/σmin(M ∗)), we have\n‖X−UΣV⊤Y‖2F ≤ ∆2u = O ( distc(Y,V)2 ) + 160k\nλ2 ‖W⊙N‖22\nwhich leads to\n‖X−UΣV⊤Y‖F ≤ ∆u ≤ O (distc(Y,V)) + 16\n√ k λ ‖W ⊙N‖2.\nNow consider ‖M∗−M̃‖2 = ‖M∗−XY⊤‖2. By definition, we know that there exists Q such that Y = VQ+∆y where ‖∆y‖2 = O(distc(Y,V)). Also, let R = X−UΣV⊤Y.\nM̃−M∗ = [ UΣV⊤(VQ+∆y) +R ] (VQ+∆y) ⊤ −UΣV⊤\n= UΣQ∆⊤y +UΣV ⊤∆y(VQ+∆y) ⊤ +R(VQ+∆y) ⊤ = UΣQ∆⊤y +UΣV ⊤∆yY ⊤ +RY⊤.\nTherefore,\n‖M̃−M∗‖2 ≤ ‖UΣ‖2‖Q‖2‖∆y‖2 + ‖UΣV⊤‖2‖∆y‖2‖Y‖2 + ‖R‖2‖Y‖2 ≤ 2‖∆y‖2 + ‖R‖2\n≤ O (distc(Y,V)) + 16\n√ k λ ‖W ⊙N‖2.\nCombining this with the bound on distc(YT ,V), the theorem then follows.\nNext, we show the main theorem for random initialization:\nTheorem 3 (Main, random initialization). Suppose M∗,W satisfy assumptions (A1)-(A3) with\nγ = O ( min {√ n\nD1\nλ\nτµ2k5/2 ,\nλ\nτ3/2µ3/2k5/2\n}) ,\n‖W‖∞ = O (\nλn\nk2µ log2 n\n) ,\nwhere D1 = maxi∈[n] ‖Wi‖1. Then after O(log(1/ǫ)) rounds Algorithm 1 using initialization from Algorithm 4 outputs a matrix M̃ that with probability at least 1− 1/n2 satisfies\n‖M̃−M∗‖2 ≤ O ( kτ\nλ\n) ‖W ⊙N‖2 + ǫ.\nThe running time is polynomial in n and log(1/ǫ).\nProof of Theorem 3. Let Y be initialized using the random initialization algorithm 4. Consider applying the proof in Lemma 12, with S1 being modified to be\nS1 = { i ∈ [n] ∣∣∣∣σmin(Y ⊤DiY) ≤ λ\n4µk\n}\nBut with this modification, S1 = ∅, with high probability. Then the same calculation from Lemma 12 (which now doesn’t need to use Lemma 10 at all since S1 = ∅) gives\n∥∥X−UΣV⊤Y ∥∥2 F ≤ ∆gµk\nBut following part (2) of the same Lemma, we get that if ∆gµk < 18σmin(M ∗),\ndistc(U,X) ≤ 2\nσmin(M∗)− 2∆gµk ∆gµk\nSo, in order to argue by induction in 1 exactly as before, we only need to check that after the update step for X, distc(U,X) is small enough to apply Lemma 12 for later steps. Indeed, we have:\ndistc(U,X) ≤ 2\nσmin(M∗)− 2∆gµk ∆gµk ≤\n√ min { 1\n2 ,\nλ2n\n384µk2D1\n}\nNoticing that ∆g has a quadratic dependency on γ, we see that if\nγ = O ( min {√ n\nD1\nλσmin(M ∗)\nµ2k5/2 , λσ\n3/2 min(M ∗)\nµ3/2k5/2\n}) ,\nthe inequality is indeed satisfied. With that, the theorem statement follows."
    }, {
      "heading" : "B.5 Estimating σmax(M∗)",
      "text" : "Finally, we show that we can estimate σmax(M∗) up to a very good accuracy, so that we can apply our main theorems to matrices with arbitrary σmax(M∗). This is quite easy: the estimate of it is just ‖W ⊙ M‖2. Then, the following lemma holds:\nLemma 13. It γ = o( 1kµ ) and δ = ‖W ⊙N‖2 = o(σmax(M∗)) then ‖W ⊙M‖2 = (1± o(1))(σmax(M∗)) Proof. We proceed separately for the upper and lower bound.\nFor the upper bound, we have\n‖W ⊙M‖2 = ‖W ⊙M∗ +W ⊙N‖2 ≤ ‖W ⊙M∗‖2 + ‖W ⊙N‖2 ≤ ‖(W −E)⊙M∗‖2 + ‖E⊙M∗‖2 + ‖W ⊙N‖2 ≤ γkµσmax(M∗) + σmax(M∗) + δ ≤ (1 + o(1))σmax(M∗). (by Lemma 5)\nFor the lower bound, completely analogously we have\n‖W ⊙M‖2 = ‖W ⊙M∗ +W ⊙N‖2 ≥ ‖W ⊙M∗‖2 − ‖W ⊙N‖2 ≥ ‖E⊙M∗‖2 − ‖(W −E)⊙M∗‖2 − ‖W ⊙N‖2 ≥ σmax(M∗)− γkµσmax(M∗)− δ ≥ (1− o(1))σmax(M∗) (by Lemma 5)\nwhich finishes the proof.\nGiven this, the reduction to the case σmax(M∗) ≤ 1 is obvious: first, we scale the matrix M down by our estimate of σmax(M∗) and run our algorithm with, say, four times as many rounds. After this, we rescale the resulting matrix M̃ by our estimate of σmax(M∗), after which the claim of Theorems 1 and 3 follows.\nAlgorithm 5 Main Algorithm (ALT) Input: Noisy observation M, weight matrix W, rank k, number of iterations T\n1: (X1,Y1) = SVDINITIAL(M,W), d1 = 18k√log n + 64\n√ kδ\nλσmin(M∗)\n2: Y1 ← WHITENING(Y1,W, d1, λ, λ, µ, k) 3: for t = 1, 2, ..., T do 4: dt+1 = 1 2t+1 1 8k √ logn + 64 √ k λσmin(M∗) δ\n5: Xt+1 ← argminX∈Rn×k ∥∥∥M−XY⊤t ∥∥∥ W 6: X̃t+1 ← QR(Xt+1) 7: Xt+1 ← WHITENING(X̃t+1,W, dt+1, λ, λ, µ, k) 8: Yt+1 ← argminY∈Rn×k ∥∥M −Xt+1Y⊤ ∥∥ W\n9: Ỹt+1 ← QR(Yt+1) 10: Yt+1 ← WHITENING(Ỹt+1,W, dt+1, λ, λ, µ, k) 11: end for 12: Σ ← argminΣ‖W ⊙ (M −XT+1ΣY ⊤ T+1)‖2 Output: M̃ = XT+1ΣY ⊤ T+1\nAlgorithm 6 Whitening (WHITENING)\nInput: orthogonal matrix X̃ ∈ Rn×k, weight W, distance d, spectral barriers λ, λ, incoherency µ, rank k. 1: Solve the following convex programing on the matrices R ∈ Rn×k and {Ar ∈ Rk×k}nr=1:\n||R − X̃||2 ≤ d ||X̃⊤(R− X̃) + (R− X̃)⊤X̃||2 ≤ d2 ||X̃⊤⊥R||2 ≤ d (Rr)⊤Rr Ar, ∀r ∈ [n]\nTr(Ar) ≤ µk\nn , ∀r ∈ [n]\nn∑\nr=1\nAr = I\nλI n∑\nr=1\nWi,rAr λI, ∀i ∈ [n]\n2: ∀r ∈ [n],Xr ∼ Rademacher(Rr,Ar − (Rr)⊤Rr). 3: X = QR(X), X ∈ Rn×k whose rows are Xr.\nOutput: X. (may need O(log(1/α)) runs to succeed with probability 1− α; see text)"
    }, {
      "heading" : "C An alternative approach: alternating minimization with SDP whitening",
      "text" : "Our main results build on the insight that the spectral property only need to hold in an average sense. However, we can even make sure that the spectral property holds at each step in a strict sense by a whitening step using SDP and Rademacher rounding. This is presented a previous version of the paper, and we keep this result here since potentially it can be applied in some other applications where similar spectral properties are needed and is thus of independent interest.\nThe whitening step (see Algorithm 6) is a convex (actually semidefinite) relaxation followed by a randomized\nrounding procedure. We explain each of the constraints in the semidefinite program in turn. The first three constraints control the spectral distance between X and X̃. The next two constraints control the incoherency, and the rest are for the spectral ratio. The solution of the relaxation is then used to specify the mean and variance of a Rademacher (random) vector, from which the final output of the whitening step is drawn. Here a Rademacher vector is defined as:\nDefinition (Rademacher random vector). A random vector x ∈ Rk is a Rademacher random vector with mean µ and variance Σ 0 (denoted as x ∼ Rademacher(µ,Σ)), if x = µ+ Sσ where S is a k × k symmetric matrix such that S2 = Σ, σ ∈ Rk is a vector where each entry is i.i.d Rademacher random variable.\nWe use this type of random vector to ensure that if x ∼ Rademacher(µ,Σ), then E[x] = µ,E[xx⊤] = µµ⊤ +Σ. Since the desired properties of the output of whitening can be tested (see Lemma 17), we can repeat the whitening step O(log(1/α)) times to get high probability 1 − α. In the rest of the paper, we will just assume that it is repeated sufficiently many times (polynomial in n and log(1/ǫ)) so that Algorithm 5 succeeds with probability 1− 1/n.\nWe now present the analysis for this algorithm. The SVD initialization has been analyzed, so we focus on the update step and the whitening step.\nNote Since our algorithm will output matrix M̃ such that ||M̃−M∗||2 ≈ O ( k3/2 √ logn\nλσmin(M∗)\n) ||W⊙N||2 and ||M∗||2 =\n1, λ ≤ 1, therefore, without lose of generality we can assume that ||W ⊙ N||2 ≤ λσmin(M ∗)\nk3/2 √ logn , otherwise we can just output zero matrix."
    }, {
      "heading" : "C.1 Update",
      "text" : "We want to show that after every round of ALT, we move our current matrices X,Y closer to the optimum. We will show that X̃ ← argminX∈Rn×k ∥∥M−XY⊤ ∥∥ W\nis a noisy power method update: X̃ = M∗Y +G where ||G||2 is small.\nFor intuition, note that if ||G||2 = 0, that is, X̃ = M∗Y, then we know that tan θ(X̃,U) = 0, so within one step of update we will be already hit into the correct subspace. We will show when ||G||2 is small we still have that tan θ(X̃,U) is progressively decreasing. Then, in order to show ||G||2 is small, we need to make sure we start from a good Y as assumed in Lemma 16.\nFirst, we show that when G is small, then tan θ(X̃,U) is small.\nLemma 14 (Distance from OPT). Let M∗ = UΣVT ∈ Rn×n be the singular value decomposition of a rank-k matrix M∗, let Y ∈ Rn×k be an orthogonal matrix, X̃ = M∗Y +G, then we have\ntan θ(X̃,U) ≤ ||G||2 cos θ(Y,V)σmin(Σ)− ||G||2 .\nProof of Lemma 14. By definition,\ntan θ(X̃,U) = ||U⊤⊥X̃(U⊤X̃)−1||2 = ||U⊤⊥(M∗Y +G)(U⊤(M∗Y +G))−1||2 ≤ ||U⊤⊥G(ΣV⊤Y +U⊤G)−1||2 ≤ ||U⊤⊥G||2||(ΣV⊤Y +U⊤G)−1||2 ≤ ||U⊤⊥G||2||(V⊤Y)−1||2||(Σ+U⊤G(V⊤Y)−1)−1||2 ≤ ||U⊤⊥G||2 1\ncos θ(Y,V) σ−1min\n( Σ+U⊤G(V⊤Y)−1 ) .\nFor the last term, we have\nσmin(Σ+U ⊤G(V⊤Y)−1) ≥ σmin(Σ)− σmax ( U⊤G(V⊤Y)−1 ) ≥ σmin(Σ)− ||U⊤G||2 cos θ(Y,V) .\nTherefore,\ntan θ(X̃,U) ≤ ||U ⊤ ⊥G||2\ncos θ(Y,V) ( σmin(Σ)− ||U ⊤G||2 cos θ(Y,V) )\n= ||U⊤⊥G||2\ncos θ(Y,V)σmin(Σ)− ||U⊤G||2 ≤ ||G||2\ncos θ(Y,V)σmin(Σ)− ||G||2 completing the proof.\nNow we show that if Y has nice properties as stated in Lemma 16, then G is small. Recall the following notation: for a matrix A, let ρ(A) be defined as maxi{nk ||Ai||22}.\nLemma 15 (Bounding ||G||2). Let M∗ = UΣV⊤ ∈ Rn×n be the singular value decomposition of a rank-k matrix M∗, M = M∗ +N be the noisy observation, and let W,M∗ satisfy the conditions of Theorem 20. Let Y ∈ Rn×k be an orthogonal matrix. For X̃ = argminX||M−XY⊤||W we have X̃ = M∗Y +G where\n||G||2 ≤ max i∈[n]\n{ γk3/2 √ ρ(U)ρ(Y)\nσmin(Y⊤DiY) sin θ(Y,V) +\n√ k||W ⊙N||2 σmin(Y⊤DiY) } .\nProof of Lemma 15. By taking the derivatives of ||M − XY⊤||W w.r.t. X, we know that the optimal solution X̃ satisfies (W ⊙ [M − X̃Y⊤])Y = 0. Plugging in X̃ = M∗Y +G, we get\n(W ⊙ [M −M∗YY⊤])Y = (W ⊙ [GY⊤])Y.\nSince M = M∗ +N and I = YY⊤ +Y⊥Y⊤⊥ , the above equation is\n(W ⊙ [GY⊤])Y = (W ⊙ [M∗Y⊥Y⊤⊥ ])Y + (W ⊙N)Y.\nSo for any i ∈ [n] (recall that [·]i is the i-th row)\n[(W ⊙ [GY⊤])Y]i = [(W ⊙ [M∗Y⊥Y⊤⊥ ])Y]i + [(W ⊙N)Y]i. (C.1)\nNote that for every matrix S ∈ Rn×n, for Di = Diag(Wi) we have\n[W ⊙ S]i = SiDi.\nApplying this to (C.1) leads to\nGiY⊤DiY = (M ∗)iY⊥Y ⊤ ⊥DiY + [(W ⊙N)]iY.\nSince (M∗)iY⊥Y⊤⊥IY = 0,\nGiY⊤DiY = (M ∗)iY⊥Y ⊤ ⊥(Di − I)Y + [(W ⊙N)]iY.\nThis gives us\nGi = (M∗)iY⊥Y ⊤ ⊥(Di − I)Y(Y⊤DiY)−1 + [(W ⊙N)]iY(Y⊤DiY)−1. (C.2)\nNow we turn to bound the operator norm of G. By definition, it suffices to bound ||a⊤Gb||2 for any two unit vectors a ∈ Rn×1, b ∈ Rk×1 (note that for a scalar s, ||s||2 = |s|). By (C.2),\n||a⊤Gb||2 = ∥∥∥∥∥ n∑\ni=1\nai(M ∗)iY⊥Y ⊤ ⊥(Di − I)Y(Y⊤DiY)−1b+\nn∑\ni=1 ai[(W ⊙N)]iY(Y⊤DiY)−1b ∥∥∥∥∥ 2\n≤ ∥∥∥∥∥ n∑\ni=1\nai(M ∗)iY⊥Y ⊤ ⊥(Di − I)Y(Y⊤DiY)−1b ∥∥∥∥∥ 2︸ ︷︷ ︸\nT1\n+ ∥∥∥∥∥ n∑\ni=1 ai[(W ⊙N)]iY(Y⊤DiY)−1b ∥∥∥∥∥ 2︸ ︷︷ ︸\nT2\n.\nIn the following, we bound the two terms T 1 and T 2 respectively.\n(Bounding T 1) Let Q = ΣV⊤Y⊥Y⊤⊥ . We have\n(M∗)iY⊥Y ⊤ ⊥ = U iQ and ||Q||2 ≤ ||V⊤Y⊥||2 = sin θ(Y,V).\nAlso let B denote the matrix whose i-th column is Bi = (Y⊤DiY)−1b. Then T 1 becomes\nT 1 = ∥∥∥∥∥ n∑\ni=1\nai(M ∗)iY⊥Y ⊤ ⊥(Di − I)Y(Y⊤DiY)−1b ∥∥∥∥∥ 2\n= ∥∥∥∥∥ n∑\ni=1\naiU iQ(Di − I)YBi ∥∥∥∥∥ 2\n= ∥∥∥∥∥ n∑\ni=1\nk∑\nr=1\n(aiUi,r)Q r(Di − I)YBi ∥∥∥∥∥ 2\n= ∥∥∥∥∥ k∑\nr=1\nn∑\ni=1\n(aiUi,r)Q r(Di − I)YBi ∥∥∥∥∥ 2\n= ∥∥∥∥∥∥ k∑\nr=1\nn∑\ni,j=1\n(aiUi,r)(Wi,j − 1)Qr,jYjBi ∥∥∥∥∥∥ 2\nwhere the last equality is because Qr(Di − I)Y = ∑n\nj=1(Wi,j − 1)Qr,jYj . Now denote αi,r = aiUi,r and αr = (α1,r, ..., αn,r)⊤.\nT 1 = ∥∥∥∥∥∥ k∑\nr=1\nn∑\ni,j=1\n(aiUi,r)(Wi,j − 1)Qr,jYjBi ∥∥∥∥∥∥ 2\n= ∥∥∥∥∥ k∑\nr=1 α⊤r [(W −E)⊙ (B⊤Y⊤)]Qr ∥∥∥∥∥ 2\n≤ k∑\nr=1\n∥∥α⊤r [(W −E)⊙ (B⊤Y⊤)]Qr ∥∥ 2\n=\nk∑\nr=1\n‖αr‖2 ∥∥(W −E)⊙ (B⊤Y⊤) ∥∥ 2 ‖Qr‖2 .\nClearly, for ‖Qr‖2 we have ‖Qr‖2 ≤ ‖Q‖2 ≤ sin θ(Y,V).\nFor ‖αr‖2, we have\nk∑\nr=1\n||αr||2 ≤ √ k\n√√√√ k∑\nr=1\n||αr||22\n= √ k\n√√√√ k∑\nr=1\nn∑\ni=1\na2iU 2 i,r\n= √ k\n√√√√ n∑\ni=1\n( a2i ( k∑\nr=1\nU2i,r\n))\n≤ √ k √√√√kρ(U) n ( n∑\ni=1\na2i\n)\n= k\n√ ρ(U)\nn .\nFor ∥∥(W −E)⊙ (B⊤Y⊤) ∥∥ 2 , we can apply the spectral lemma (Lemma 5) to get\n∥∥(W −E)⊙ (B⊤Y⊤) ∥∥ 2 ≤ γk √ ρ(B⊤)ρ(Y).\nWe have ||Bi||2 = ||(Y⊤DiY)−1b||2 ≤ 1σmin(Y⊤DiY) , so\nρ(B⊤) ≤ max i∈[n]\n{ n/k\nσ2min(Y ⊤DiY)\n}\nand ∥∥(W −E)⊙ (B⊤Y⊤) ∥∥ 2 ≤ max\ni∈[n]\n{ γ √ knρ(Y)\nσmin(Y⊤DiY)\n} .\nPutting together, we have\nT 1 ≤ k √ ρ(U)\nn ×max i∈[n]\n{ γ √ knρ(Y)\nσmin(Y⊤DiY)\n} × sin θ(Y,V)\n≤ max i∈[n]\n{ γk3/2 √ ρ(Y)ρ(U)\nσmin(Y⊤DiY) sin θ(Y,V)\n} . (C.3)\n(Bounding T 2) Recall that B denote the matrix whose i-th column is Bi = (Y⊤DiY)−1b.\nT 2 = ∥∥∥∥∥ n∑\ni=1 ai[(W ⊙N)]iY(Y⊤DiY)−1b ∥∥∥∥∥ 2\n= ∥∥∥∥∥ n∑\ni=1 ai[(W ⊙N)]iYBi ∥∥∥∥∥ 2\n= ∥∥∥∥∥ n∑\ni=1\nai[(W ⊙N)]i k∑\nr=1\nYrBr,i ∥∥∥∥∥ 2\n= ∥∥∥∥∥ k∑\nr=1\nn∑\ni=1 ai[(W ⊙N)]iYrBr,i ∥∥∥∥∥ 2 .\nNow denote βi,r = aiBr,i and βr = (βr,1, βr,2, . . . , βr,n)⊤.\nT 2 = ∥∥∥∥∥ k∑\nr=1\nn∑\ni=1 ai[(W ⊙N)]iYrBr,i ∥∥∥∥∥ 2\n= ∥∥∥∥∥ k∑\nr=1\nn∑\ni=1 βi,r[(W ⊙N)]iYr ∥∥∥∥∥ 2\n= ∥∥∥∥∥ k∑\nr=1 β⊤r (W ⊙N)Yr ∥∥∥∥∥ 2\n≤ k∑\nr=1\n∥∥β⊤r (W ⊙N)Yr ∥∥ 2\n≤ k∑\nr=1\n‖βr‖2 ‖W ⊙N‖2 ‖Yr‖2 .\nWe have ‖Yr‖2 = 1. For ‖βr‖2, we have\nk∑\nr=1\n‖βr‖2 ≤ √ k\n√√√√ k∑\nr=1\n||βr||22\n= √ k\n√√√√ k∑\nr=1\nn∑\ni=1\n( a2iB 2 r,i )\n= √ k\n√√√√ n∑\ni=1\na2i\nk∑\nr=1\nB2r,i\n= √ k √√√√ n∑\ni=1\na2i ||Bi||22.\nWe have ||Bi||2 = ||(Y⊤DiY)−1b||2 ≤ 1σmin(Y⊤DiY) and ∑n i=1 a 2 i = 1, so\nk∑\nr=1\n‖βr‖2 ≤ √ k\n√√√√ n∑\ni=1\na2i ||Bi||22\n≤ max i∈[n]\n{ √ k\nσmin(Y⊤DiY)\n} .\nPutting together, we have\nT 2 ≤ max i∈[n]\n{ √ k\nσmin(Y⊤DiY) ‖W ⊙N‖2\n} . (C.4)\nThe lemma follows from combining (C.3) and (C.4).\nNow we have all the ingredients to prove the update lemma.\nLemma 16. Suppose M∗,W satisfy all the assumptions, column orthogonal matrix Y ∈ Rn×k is (5kµ)-incoherent, and for all i ∈ [n], Di = Diag(Wi) satisfies\n1 4 λI Y⊤DiY 4λI.\nThen X̃ ← argminX∈Rn×k ∥∥M−XY⊤ ∥∥ W satisfies\ntan θ(X̃,U) ≤ 1 16k √ logn tan θ(Y,V) + 16kδ λσmin(M∗) .\nProof of Lemma 16. By Lemma 14 and Lemma 15, we have\ntan θ(X̃,U) ≤ ||G||2 cos θ(Y,V)σmin(M∗)− ||G||2 , (C.5)\n||G||2 ≤ max i∈[n]\n{ γk3/2 √ ρ(U)ρ(Y)\nσmin(Y⊤DiY) sin θ(Y,V) +\n√ k||W ⊙N||2 σmin(Y⊤DiY) } . (C.6)\nBy the assumptions Y⊤DiY λ4 I, ρ(U) ≤ µ, ρ(Y) ≤ 5kµ, in (C.6) we have:\n||G||2 ≤ 4 √ 5γk2µ\nλ sin θ(Y,V) +\n4 √ kδ\nλ .\nPlugging this in (C.5), and noting that\nγ ≤ λ 128 √ 5k3µ √ logn , ||G||2 ≤ 1 4 σmin(M ∗), cos θ(Y,V) ≥ 1 2 ,\nwe get\ntan θ(X̃,U) ≤ 2 ||G||2 cos θ(Y,V)σmin(M∗) ≤ 1 16k √ logn tan θ(Y,V) + 16\n√ kδ\nλσmin(M∗)\nas needed."
    }, {
      "heading" : "C.2 Whitening",
      "text" : "What remains is to show that the whitening step can make sure that Y has good incoherency and Oi has the desired spectral property. Recall that the whitening step consists of a SDP relaxation and a new rounding scheme to fix Y whenever Y⊤DiY having very small singular values. Intuitively, we want to get through the SDP relaxation, an R close to V and Ar ≈ (Vr)⊤Vr ∈ Rk×k, so that we’d have the incoherency of R is close to µ(V) which is bounded by µ, and ∑n r=1Wi,rAr ≈ V⊤DiV λI. (Note one can not simply say when tan θ(Y,U) ≤ d, then Y⊤DiY is close to V⊤DiV. This is because ||Di||2 can be as large as npoly(logn) in our case, however, ||V⊤DiV||2 = O(1).) The key observation is that our randomized rounding outputs a n × k random matrix X such that E[Xr] = Rr (Xr is the i-th row of X), E[(Xr)⊤(Xr)] = Ar, with the variance of (Xr) bounded by Ar − (Rr)⊤Rr. Therefore,\nE[Xr ] = Rr, E[X⊤DiX] = n∑\nr=1\nWi,rAr λI\nThus, X is incoherent (Note ||Xr||22 = Tr[(Xr)⊤(Xr)]) and ||(X⊤DiX)−1||2 is small in expectation. we can apply matrix concentration bound on X to show that the above values actually concentrate on the expectation, thus the output matrix X = QR(X) will have the required properties.\nLemma 17 (Whitening). Suppose X̃ is µ-incoherent and satisfies tan θ(X̃,U) ≤ d2 where d ≤ 14k√logn . Then X ← WHITENING(X̃,W, d, λ, λ, µ, k) satisfies with high probability: (1). 14λI X ⊤ DiX 4λI; (2). X is (5kµ)-incoherent; (3). tan θ(X,U) ≤ 4dk√logn.\nAs a preliminary to showing whitening works, we need to introduce a new type of random variables and a new matrix concentration bound. Another natural distribution to use is a Gaussian random vector y ∼ N (µ,Σ). The advantage of a Rademacher vector x is that ‖x‖2 is always bounded, which facillitates proving concentration bounds.\nLemma 18 (Matrix Concentration). Let {xi}ni=1 be independent Rademacher random vectors in Rk with xi ∼ Rademacher(ai,∆i), let Tr(∆)max = maxi∈[n]{Tr(∆i)}, (||a||22)max = maxi∈[n]{||ai||22}, || ∑n i=1 ∆i|| ≤ ∆, then for every t ≥ 0,\nPr [∥∥∥∥∥ n∑\ni=1\nxix ⊤ i − E\n[ n∑\ni=1\nxix ⊤ i ]∥∥∥∥∥ ≥ t ] ≤ exp { − t 2\nc1 + c2t\n}\nwhere\nc1 = [2Tr(∆)max + (3 + k)(||a||22)max]∆, c2 = (k + 1)Tr(∆)max + 2 √ k(||a||22)max∆.\nProof of Lemma 18. Let Si ∈ Rk×k be a matrix such that S2i = ∆i, Note that\nE[xix ⊤ i ] = E[(ai + Siσ)(ai + Siσ) ⊤] = aia ⊤ i + SiE[σσ ⊤]S⊤i = aia ⊤ i +∆i\nWe first move the random variable to center at zero: consider yi = xi − ai, define Yi = xix⊤i − E [ xix ⊤ i ] = xix ⊤ i − (aia⊤i +∆i) = yiy⊤i + aiy⊤i + yia⊤i −∆i, we have: E[Yi] = 0. By yi and −yi being identically distributed, we obtain E[||yi||22yia⊤i ] = 0,E[〈ai, yi〉yiy⊤i ] = 0\nTherefore, using the fact that E[yi] = 0, and E[yiy⊤i ] = ∆i, we can calculate that\nE[Y2i ] = E[(yiy ⊤ i + aiy ⊤ i + yia ⊤ i −∆i)2]\n= E[||yi||22yiy⊤i + 〈ai, yi〉yiy⊤i + ||yi||22yia⊤i − yiy⊤i ∆i +||yi||22aiy⊤i + 〈ai, yi〉aiy⊤i + ||yi||22aia⊤i − aiy⊤i ∆i +〈ai, yi〉yiy⊤i + ||ai||22yiy⊤i + 〈ai, yi〉yia⊤i − yia⊤i ∆i −∆iyiy⊤i −∆iaiy⊤i −∆iyia⊤i +∆2i ] = E[||yi||22yiy⊤i ] + aia⊤i E[||yi||22] + ||ai||22E[yiy⊤i ] + E[〈ai, yi〉(aiy⊤i + yia⊤i )]−∆2i = E[||yi||22yiy⊤i ] +Tr(∆i)aia⊤i + ||ai||22∆i + aia⊤i ∆i +∆iaia⊤i −∆2i\nFurthermore,\nE[||yi||22yiy⊤i ] = E[(σ⊤∆iσ)Siσσ⊤S⊤i ] = SiE[(σ ⊤∆iσ)σσ ⊤]S⊤i\nOn the other hand, For u 6= v:\n(E[(σ⊤∆iσ)σσ ⊤])u,v = E\n[ ∑\np,q\nσp(∆i)p,qσqσuσv\n]\n= ∑\np,q\n(∆i)p,qE[σpσqσuσv]\n= 2(∆i)u,v\nFor u = v:\n(E[(σ⊤∆iσ)σσ ⊤])u,u = E\n[ ∑\np,q\nσp(∆i)p,qσqσuσu\n]\n= ∑\np,q\n(∆i)p,qE[σpσqσ 2 u]\n= ∑\np\n(∆i)p,p = Tr(∆i)\nTherefore, E[(σ⊤∆iσ)σσ ⊤] Tr(∆i)I+ 2∆i\nE[||yi||22yiy⊤i ] 2∆2i +Tr(∆i)∆i\nTherefore, by ∆2i Tr(∆i)∆i, aia⊤i ∆i +∆iaia⊤i 2||ai||22∆i, we obtain n∑\ni=1\nE[Y2i ] n∑\ni=1\n( ∆2i +Tr(∆i)∆i +Tr(∆i)aia ⊤ i + ||ai||22∆i + aia⊤i ∆i +∆iaia⊤i )\n[2Tr(∆)max + 3(||a||22)max]∆I+ n∑\ni=1\nTr(∆i)aia ⊤ i\n[2Tr(∆)max + 3(||a||22)max]∆I+ n∑\ni=1\n||ai||22Tr(∆i)I\n[2Tr(∆)max + 3(||a||22)max]∆I+ (||a||22)maxTr ( n∑\ni=1\n∆i\n) I\n[2Tr(∆)max + 3(||a||22)max]∆I+ k(||a||22)max∆I\nMoreover,\n||Yi||2 ≤ ||∆i||2 + ||yiy⊤i ||2 + ||aiy⊤i ||2 + ||yia⊤i ||2 = ||∆i||2 + 2||aiσ⊤S⊤i ||2 + ||SiσσS⊤i ||2 ≤ ||∆i||2 + 2 √ k(||a||22)max∆+ k||∆i||2\n≤ (k + 1)Tr(∆)max + 2 √ k(||a||22)max∆\nwhere the last inequality is due to ||σσ⊤||2 ≤ k. The lemma then follows by the matrix Bernstein inequality.\nNow we are ready to prove the lemma for the whitening step.\nLemma 17. Suppose M∗,N,W satisfy all assumptions, µ-incoherent column orthogonal matrix X̃ ∈ Rn×k is close to U: tan θ(X̃,U) ≤ d2 where d ≤ 14k√logn , then X ← WHITENING(X̃,W, d, λ, λ, µ, k) satisfies with high probability: (1). For all i ∈ [n], let Di = Diag(Wi), then 14λI X ⊤ DiX 4λI; (2). X is (5kµ)-incoherent; (3).\ntan θ(X,U) ≤ 4dk√logn.\nProof of Lemma 17. Firstly we need to show that there is a feasible solution to our SDP relaxation, and then we need to show that the output has the desired properties stated in the lemma.\n(Existence of a feasible solution) To be specific, we want to show that R = UQ, Ar = Q⊤(Ur)⊤UrQ is a feasible solution to the SDP for some orthogonal matrix Q ∈ Rk×k .\nClearly, by setting R and Ar as above, we automatically satisfy:\n(Rr)⊤Rr Ar\nTr(Ar) = ||Rr||22 = ||QUr||22 = ||Ur ||22 ≤ µk\nn n∑\nr=1\nAr =\nn∑\nr=1\nQ⊤(Ur)⊤UrQ = Q⊤U⊤UQ = I\nλI n∑\nr=1\nWi,rAr =\nn∑\nr=1\nWi,rQ ⊤(Ur)⊤UrQ = Q⊤ ( U⊤Diag(Wi)U ) Q λI\nSo we only need to show that there exists orthogonal Q that UQ satisfies the distance constraints:\n||X̃⊤⊥UQ||2 ≤ d, ||UQ− X̃||2 ≤ d,\n||X̃⊤(UQ − X̃) + (UQ − X̃)⊤X̃||2 ≤ d2.\nNote that sin θ(X̃,U) ≤ tan θ(X̃,U), so\n||X̃⊤⊥UQ||2 = ||X̃⊤⊥U||2 = sin θ(X̃,U) ≤ tan θ(X̃,U) ≤ d/2 ≤ d.\nMoreover, when tan θ(X̃,U) = d2 ≤ 12 ,\n1− cos θ(X̃,U) cos θ(X̃,U) ≤ sin θ(X̃,U),\nand thus by Lemma 4, distc(X̃,U) ≤ 2sin θ(X̃,U). By definition, there exits an orthogonal matrix Q such that\n||UQ − X̃||2 ≤ 2sin θ(X̃,U) ≤ d.\nFinally, since X̃ and UQ are orthogonal, we know that\nX̃⊤(UQ − X̃) + (UQ − X̃)⊤X̃ = −(UQ− X̃)⊤(UQ − X̃)\nwhich implies\n||X̃⊤(UQ − X̃) + (UQ − X̃)⊤X̃||2 = ||(UQ − X̃)⊤(UQ − X̃)||2 ≤ ||(UQ − X̃)⊤(UQ − X̃)||22 ≤ d2.\nThis shows that the solution to our SDP exists.\n(Desired properties) Now we show that the randomly rounded solution has the required properties with high probability. We first prove some nice properties of X, and then use them to prove the properties of X.\nClaim 19. X satisfies the following properties. (a). Orthogonality property.\nPr [ ||X⊤X− I||2 ≥ 1\n4\n] ≤ 1\n8 .\n(b). Spectral property.\nPr [ ∃i ∈ [n], ∥∥∥∥∥X ⊤DiX− n∑\nr=1\nWi,rAr ∥∥∥∥∥ 2 ≥ λ 2 ] ≤ 1 8 .\n(c). Distance property.\nPr [ ||X̃⊤⊥X||2 ≥ dk √ logn ] ≤ 1\n8 .\n(d). Incoherent property.\n∀r ∈ [n], (Xr)(Xr)⊤ ≤ µk(k + 1) n .\nProof of Claim 19. It is easy to verify that in expectation, the rounded solution satisfies the properties stated: (a). Orthogonality property.\nE[X⊤X] = n∑\nr=1\nE[(Xr)⊤Xr] = n∑\nr=1\n( (Rr)⊤Rr + SrE[σ ⊤σ]Sr ) = n∑\nr=1\nAr = I\nsince Xr = Rr + σSr where Sr is a PSD matrix with S2r = Ar − (Rr)⊤(Rr). (b). Spectral property.\nE[X⊤DiX] = n∑\nr=1\nE[Wi,r(X r)⊤(Xr)] =\nn∑\nr=1\nWi,rAr.\n(c). Distance property. E[X] = R, E[X̃⊤⊥X] = X̃ ⊤ ⊥R.\n(d). Incoherent property. E[(Xr)(Xr)⊤] = Tr(Ar).\nTherefore, we just need to show that the random variables in (a), (b), (c), and (d) concentrate around their expectation. First consider (a). We can apply the matrix concentration lemma (18), for which we need to bound ||∑nr=1∆r||2\nwhere∆r = Ar−(Rr)⊤(Rr). Note that ∑n r=1 Ar = I, it suffices to bound σmin (∑ r(R r)⊤(Rr) ) = σmin ( R⊤R ) . Since R = R+ X̃− X̃, we have\nσmin ( R⊤R ) = σmin ( X̃⊤X̃+ (R− X̃)⊤(R − X̃) + X̃⊤(R− X̃) + (R − X̃)⊤X̃ ) .\nThen by ||R− X̃||2 ≤ d, ||X̃⊤(R − X̃) + (R − X̃)⊤X̃||2 ≤ d2 and X̃⊤X̃ = I, we get\nσmin ( R⊤R ) ≥ 1− ||(R− X̃)⊤(R− X̃)||2 − ||X̃⊤(R− X̃) + (R − X̃)⊤X̃||2 ≥ 1− 2d2.\nTherefore, ∥∥∥∥∥ n∑\nr=1\n∆r ∥∥∥∥∥ 2 = ∥∥∥∥∥ n∑ r=1 Ar − n∑ r=1 (Rr)⊤(Rr) ∥∥∥∥∥ 2 = ∥∥I−R⊤R ∥∥ 2 ≤ 2d2.\nUsing the matrix concentration lemma (18) with ∆ = 2d2,Tr(∆)max ≤ µkn , (||a||22)max = µk n , t = 1/4, we obtain that when n is sufficiently large:\nPr [∥∥X⊤X− I ∥∥ 2 ≥ 1\n4\n] ≤ 1\n8 .\nNext consider (b). We can also apply the matrix concentration lemma (18) for each i ∈ [n] and then take the union bound. Here, ∆r = Wi,r ( Ar − (Rr)⊤(Rr) ) , so\n∥∥∥∥∥ n∑\nr=1\n∆r ∥∥∥∥∥ 2 ≤ ∥∥∥∥∥ n∑ r=1 Wi,rAr ∥∥∥∥∥ 2 ≤ λ.\nUsing the matrix concentration lemma (18) with ∆ = λ,Tr(∆)max = µk n ||W||∞, (||a||22)max = µk n ||W||∞, t = λ 2 , we obtain that for any i ∈ [n], when\nλ ≥ √\n32k2µ||W||∞ logn n λ,\nwe have\nPr [∥∥∥∥∥X ⊤DiX− n∑\nr=1\nWi,rAr ∥∥∥∥∥ 2 ≥ λ 2 ] ≤ 1 8n .\nTaking the union bound leads to the desired property. Now consider (c). By triangle inequality,\n||X̃⊤⊥X||2 ≤ ||X̃⊤⊥R||2 + ||X̃⊤⊥(X−R)||2.\nBy the SDP, ||X̃⊤⊥R||2 ≤ d, so it suffices to bound X̃⊤⊥(X−R) = ∑n r=1([X̃⊥] r)⊤(X−R)r. LetZr = ([X̃⊥]r)⊤(X−\nR)r, we have X̃⊤⊥(X−R) = ∑n\nr=1Zr. Furthermore, E[Zr] = 0 with ∥∥∥∥∥E[ n∑\nr=1\nZrZ ⊤ r ] ∥∥∥∥∥ 2 ≤ ∥∥∥∥∥E[ n∑ r=1 (X−R)r[(X−R)r]⊤] ∥∥∥∥∥ 2 = n∑ r=1\nTr(∆r) ≤ 3d2k, ∥∥∥∥∥E[ n∑\nr=1\nZ⊤r Zr] ∥∥∥∥∥ 2 ≤ ∥∥∥∥∥ n∑ r=1 ∆r ∥∥∥∥∥ 2 ≤ 3d2,\n||Zr||2 ≤ 2dk.\nBy Matrix Bernstein inequality, when n is sufficiently large,\nPr [ ||X̃⊤⊥(X−R)||2 ≥ 1\n2 dk\n√ logn ] ≤ 1\n8 .\nThe property then follows from the triangle inequality. Finally, consider (d). We know that Xr = Rr + σSr where Sr is a PSD matrix with S2r = Ar − (Rr)⊤(Rr). Therefore,\n(Xr)(Xr)⊤ = (Rr)(Rr)⊤ + σS2rσ ⊤ ≤ Tr(Ar) +Tr(Ar)||σ||22 ≤\nµk(k + 1)\nn .\nThis completes the proof of the claim.\nWe are now ready to prove the properties of X = QR(X), the final output of WHITENING. Assume none of the bad events in Claim 19 happen. First, by the spectral property (b) of X in the claim, we have that for any i ∈ [n],\nλ 2 I X⊤DiX 2λI.\nNote that ||X⊤X− I|| ≤ 14 , which implies that σ2max(X) ≤ 54 , σ2min(X) ≥ 34 . Therefore, for any i ∈ [n],\nX ⊤ DiX\n1\nσ2max(X) X⊤DiX\nλ 4 I\nand\nX ⊤ DiX\n1\nσ2min(X) X⊤DiX 4λI.\nNext, note that\nsin θ(X, X̃) = ||X̃⊤⊥X||2 ≤ 1 σmin(X) ||X̃⊤⊥X||2 ≤ 3 2 ||X̃⊤⊥X||2 ≤ 3 2 dk\n√ logn.\nSince sin θ(X̃,U) ≤ tan θ(X̃,U) ≤ d2 , we have\nsin θ(X,U) ≤ d 2 + 3 2 dk\n√ logn ≤ 2dk √ log n ≤ 1/2.\nWhen sinθ ≤ 1/2, tanθ ≤ 2sinθ. So\ntan θ(X,U) ≤ 2sin θ(X,U) ≤ 4dk √ logn.\nFinally, for incoherence, we know that ρ(X) ≤ µ(k+1), σmin(X) ≥ 34 . Then the output X satisfies ρ(X) ≤ 4ρ(X) ≤ 5µk.\nNote: since all the property of output X can be tested in polynomial time (for (3) we can test it using the input matrix X̃ because tan θ(X̃,U) ≤ d2 ), we can run the whitening algorithm for O(log(1/α)) times (using fresh randomness for the choice of X) and we will have success probability 1− α."
    }, {
      "heading" : "C.3 Final result",
      "text" : "Theorem 20. If M∗,W satisfy assumptions (A1)-(A3), and\n||W||∞ = O ( λ2n\nk2µλ logn\n) , γ = O ( λσmin(M ∗)\nk3µ √ logn\n) ,\nthen after O(log(1/ǫ)) rounds Algorithm 5 outputs a matrix M̃ that with probability ≥ 1− 1/n satisfies\n||M̃−M∗||2 ≤ O ( k3/2 √ logn\nλσmin(M∗)\n) ||W ⊙N||2 + ǫ."
    }, {
      "heading" : "The running time is polynomial in n and log(1/ǫ).",
      "text" : "The theorem is stated in its full generality. To emphasize the dependence on the matrix size n, the rank k and the incoherency µ, we can consider a specific range of parameter values where the other parameters (the lower/upper spectral bound, the condition number of M∗) are constants, which gives a corollary which is easier to parse. Also, these parameter values show that we can handle a wider range of parameters than the simple algorithm with the clipping as a whitening step.\nCorollary 21. Suppose λ, λ and σmin(M∗) are all constants, and T = O(log(1/ǫ)). Furthermore,\n‖W‖∞ = O (\nn\nk2µ logn\n) , γ = O ( 1\nk3µ √ logn\n) .\nThen with probability ≥ 1− 1/n,\n||M̃−M∗||2 ≤ O ( k3/2 √ logn ) ||W ⊙N||2 + ǫ.\nWe now consider proving the theorem. After proving these lemmas, the proof is rather immediate. Define the following two quantities:\nval = 4k √ logn, c =\n64 √ k\nλσmin(M∗) val =\n256k3/2 √ logn\nλσmin(M∗) .\nWe just need to show that tan θ(Xt,U) ≤ 12t + cδ for every t ≥ 1, and tan θ(Yt,U) ≤ 12t + cδ for every t > 1. We will prove it by induction.\n(a). After initialization, by Lemma 8 and 17, we have\ntan θ(Y1,V) ≤ 4kd1 √ logn = d1val = 1\n2 + cδ.\n(b). Suppose tan θ(Xt,U) and tan θ(Yt,V) ≤ 12t + cδ is true for t, and consider the iterates at step t+ 1. Since Yt is given by WHITENING, by Lemma 17, we know that 14λI Y ⊤ t DiYt 4λI and Yt is (5kµ)-incoherent.\nTherefore, applying Lemma 16 we have\ntan θ(X̃t+1,U) ≤ tan θ(Yt,V)\n4val +\n16 √ kδ\nλσmin(M∗)\n≤ 1 2t+2val +\n( c\n4val +\n16 √ k\nλσmin(M∗)\n) δ\n≤ 1 2t+2val + 32\n√ k\nλσmin(M∗) δ.\nNow, we know that tan θ(X̃t+1,U) ≤ dt+12 for dt+1 = 12t+1val + 64 √ k λσmin(M∗) δ. By Lemma 17,\ntan θ(Xt+1,U) ≤ dt+1val\n≤ ( 1\n2t+1val +\n64 √ k\nλσmin(M∗) δ\n) val\n≤ 1 2t+1 + cδ.\nUsing exactly the same argument we can show that tan θ(Yt+1,V) ≤ 12t+1 + cδ. Then the theorem follows by bounding ‖M∗ − M̃‖2 by tan θ(YT+1,V), tan θ(XT+1,U) using the triangle inequality and the spectral property of W. For simplicity, let X = XT+1 and Y = YT+1. By definition, we know that there exists Qx and Qy such that XQx = U + ∆x and XQy = V + ∆y where ‖∆x‖2 = O(tan θ(X,U)) and ‖∆y‖2 = O(tan θ(Y,V)).\n‖W ⊙ (M −XΣY⊤)‖2 ≤ ‖W ⊙ (M−XQxΣQyY⊤)‖2 ≤ ‖W ⊙ (M∗ +N−XQxΣQyY⊤)‖2\n≤ ‖W ⊙ (M∗ +−XQxΣQyY⊤)‖2 + ‖W ⊙N‖2 ≤ ‖W ⊙ (M∗ +−XQxΣQyY⊤)‖2 + ‖W ⊙N‖2.\nOn the other hand,\n‖W ⊙ (M−XΣY⊤)‖2 ≥ ‖W ⊙ (M∗ −XΣY⊤)‖2 − ‖W ⊙N‖2.\nTherefore,\n‖W ⊙ (M∗ −XΣY⊤)‖2 ≤ ‖W ⊙ (M∗ −XQxΣQyY⊤)‖2 + 2‖W⊙N‖2 = O(tan θ(X,U) + tan θ(Y,V)) +O(‖W ⊙N‖2).\nDefine ∆ = M∗ −XΣY⊤ and ∆′ = XQxΣQ⊤y Y⊤ −XΣY⊤, and note that the difference between the two is O(tan θ(X,U) + tan θ(Y,V)).\n‖∆‖2 ≤ ‖W ⊙∆‖2 + ‖(W −E)⊙∆‖2 ≤ ‖W ⊙∆‖2 + ‖(W −E)⊙∆′‖2 +O(tan θ(X,U) + tan θ(Y,V)).\nSo now it is sufficient to show that ‖(W − E) ⊙∆′‖2 ≤ c‖∆‖2 for a small c < 1/2. Now we apply Lemma 5. Let Z = QxΣQ⊤y −Σ.\n‖(W −E)⊙∆′‖2 = ‖(W −E)⊙ (XQxΣQ⊤y Y⊤ −XΣY⊤)‖2 = ‖(W −E)⊙ (XZY⊤)‖2 ≤ c‖Z‖2\nfor some small c < 1/2, since γ is small and X and Y are incoherent. Note that X and Y are projections, so ‖Z‖2 = ‖XZY⊤‖2, then\n‖(W −E)⊙∆′‖2 ≤ c‖∆‖2.\nCombining all things we have ‖∆‖2 = O(tan θ(X,U) + tan θ(Y,V)) + O(‖W ⊙ N‖2) = O(tan θ(X,U) + tan θ(Y,V)), which completes the proof."
    }, {
      "heading" : "D Empirical verification of the spectral gap property",
      "text" : "Experiments on the performance of the alternating minimization can be found in related work (e.g., [Lu et al., 1997, Srebro and Jaakkola, 2003]). Therefore, we focus on verifying the key assumption, i.e., the spectral gap property of the weight matrix (Assumption (A2)).\nHere we consider the application of computing word embeddings by factorizing the co-occurrence matrix between the words, which is one of the state-of-the-art techniques for mapping words to low-dimensional vectors (about 300 dimension) in natural language processing. There are many variants (e.g., [Levy and Goldberg, 2014, Pennington et al., 2014, Arora et al., 2016]); we consider the following simple approach. Let X be the co-occurrence matrix, where Xi,j is the number of times that word i and word j appear together within a window of small size (we use size 10 here) in the given corpus. Then the word embedding by weighted low rank problem is\nmin V\n∑\ni,j\nf(Xi,j)\n( log ( Xi,j\nX\n) − 〈Vi,Vj〉 )2\nwhere X = ∑\ni,j Xi,j , Vi’s are the vectors for the words, and f(x) = max{Xi,j , 100} for a large corpus and f(x) = max{Xi,j , 10} for a small corpus.\nWe focus on the weight matrix Wi,j = f(Xi,j). It has been observed that using Xi,j as weights is roughly the maximum likelihood estimator under certain probabilistic model and is better than using uniform weights. It has also been verified that using the truncated weight f(Xi,j) is better than using Xi,j . Our experiments suggest that f(Xi,j) is better partially due to the requirement that the weight matrix should have the spectral gap property for the algorithm to succeed.\nWe consider two large corpora (Wikipedia corpus [Wikimedia, 2012], about 3G tokens; a subset of Commoncrawl corpus [Buck et al., 2014], about 20G tokens). For each corpus, we pick the top n words (n = 500, 1000, . . . , 5000) and compute the spectral gap ‖W − E‖2 where W is the weight matrix corresponding to the words, and E is the\nall-one matrix. Note that a scaling of W does not affect the problem, so we enumerate different scaling of W (from 2−20 to 210) and plot the best spectral gap. We compare the two variants: with threshold (Wi,j = f(Xi,j)), and without threshold (Wi,j = Xi,j ).\nThe results are shown in Figure 1. Without threshold, there is almost no spectral gap. With threshold, there is a decent gap, though with the increase of the matrix size, the gap become smaller because larger vocabulary includes more uneven co-occurrence entries and thus more noise. This suggests that thresholding can make the weight matrix nicer for the algorithm, and thus leads to better performance."
    } ],
    "references" : [ {
      "title" : "A latent variable model approach to pmi-based word embeddings",
      "author" : [ "Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski" ],
      "venue" : "To appear in Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Arora et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2016
    }, {
      "title" : "Universal matrix completion",
      "author" : [ "Srinadh Bhojanapalli", "Prateek Jain" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Bhojanapalli and Jain.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bhojanapalli and Jain.",
      "year" : 2014
    }, {
      "title" : "Tighter low-rank approximation via sampling the leveraged element",
      "author" : [ "Srinadh Bhojanapalli", "Prateek Jain", "Sujay Sanghavi" ],
      "venue" : "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Bhojanapalli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bhojanapalli et al\\.",
      "year" : 2015
    }, {
      "title" : "Dropping convexity for faster semi-definite optimization",
      "author" : [ "Srinadh Bhojanapalli", "Anastasios Kyrillidis", "Sujay Sanghavi" ],
      "venue" : "arXiv preprint arXiv:1509.03917,",
      "citeRegEx" : "Bhojanapalli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bhojanapalli et al\\.",
      "year" : 2015
    }, {
      "title" : "N-gram counts and language models from the common crawl",
      "author" : [ "Christian Buck", "Kenneth Heafield", "Bas van Ooyen" ],
      "venue" : "In Proceedings of the Language Resources and Evaluation Conference,",
      "citeRegEx" : "Buck et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Buck et al\\.",
      "year" : 2014
    }, {
      "title" : "Matrix completion with noise",
      "author" : [ "Emmanuel J Candes", "Yaniv Plan" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Candes and Plan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Candes and Plan.",
      "year" : 2010
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "The power of convex relaxation: Near-optimal matrix completion",
      "author" : [ "Emmanuel J Candès", "Terence Tao" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candès and Tao.,? \\Q2010\\E",
      "shortCiteRegEx" : "Candès and Tao.",
      "year" : 2010
    }, {
      "title" : "Phase retrieval via wirtinger flow: Theory and algorithms",
      "author" : [ "Emmanuel J Candes", "Xiaodong Li", "Mahdi Soltanolkotabi" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candes et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Candes et al\\.",
      "year" : 1985
    }, {
      "title" : "Efficient computation of robust weighted low-rank matrix approximations using the l 1 norm",
      "author" : [ "Anders Eriksson", "Anton van den Hengel" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Eriksson and Hengel.,? \\Q2012\\E",
      "shortCiteRegEx" : "Eriksson and Hengel.",
      "year" : 2012
    }, {
      "title" : "Spectral techniques applied to sparse random graphs",
      "author" : [ "Uriel Feige", "Eran Ofek" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "Feige and Ofek.,? \\Q2005\\E",
      "shortCiteRegEx" : "Feige and Ofek.",
      "year" : 2005
    }, {
      "title" : "Nuclear magnetic resonance and its applications to living systems",
      "author" : [ "David G Gadian" ],
      "venue" : null,
      "citeRegEx" : "Gadian.,? \\Q1982\\E",
      "shortCiteRegEx" : "Gadian.",
      "year" : 1982
    }, {
      "title" : "Low-rank matrix approximation with weights or missing data is np-hard",
      "author" : [ "Nicolas Gillis", "François Glineur" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Gillis and Glineur.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gillis and Glineur.",
      "year" : 2011
    }, {
      "title" : "Recovering low-rank matrices from few coefficients in any basis",
      "author" : [ "David Gross" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Gross.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gross.",
      "year" : 2011
    }, {
      "title" : "Understanding alternating minimization for matrix completion",
      "author" : [ "Marcus Hardt" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Hardt.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hardt.",
      "year" : 2014
    }, {
      "title" : "Deterministic algorithms for matrix completion",
      "author" : [ "Eyal Heiman", "Gideon Schechtman", "Adi Shraibman" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "Heiman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Heiman et al\\.",
      "year" : 2014
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Jain et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2013
    }, {
      "title" : "Matrix completion from noisy entries",
      "author" : [ "Raghunandan Keshavan", "Andrea Montanari", "Sewoong Oh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Keshavan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Keshavan et al\\.",
      "year" : 2009
    }, {
      "title" : "Matrix completion from any given set of observations",
      "author" : [ "Troy Lee", "Adi Shraibman" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lee and Shraibman.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lee and Shraibman.",
      "year" : 2013
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Levy and Goldberg.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Improving one-class collaborative filtering by incorporating rich user information",
      "author" : [ "Yanen Li", "Jia Hu", "ChengXiang Zhai", "Ye Chen" ],
      "venue" : "In Proceedings of the 19th ACM international conference on Information and knowledge management,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Weighted low-rank approximation of general complex matrices and its application in the design of 2-d digital filters",
      "author" : [ "W-S Lu", "S-C Pei", "P-H Wang" ],
      "venue" : "Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on,",
      "citeRegEx" : "Lu et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 1997
    }, {
      "title" : "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise",
      "author" : [ "Sahand Negahban", "Martin J Wainwright" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Negahban and Wainwright.,? \\Q2012\\E",
      "shortCiteRegEx" : "Negahban and Wainwright.",
      "year" : 2012
    }, {
      "title" : "Orthogonal representations over finite fields and the chromatic number",
      "author" : [ "René Peeters" ],
      "venue" : "of graphs. Combinatorica,",
      "citeRegEx" : "Peeters.,? \\Q1996\\E",
      "shortCiteRegEx" : "Peeters.",
      "year" : 1996
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning" ],
      "venue" : "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Weighted low rank approximations with provable guarantees",
      "author" : [ "Ilya Razenshteyn", "Zhao Song", "David Woodruff" ],
      "venue" : "In Proceedings of the 48th Annual Symposium on the Theory of Computing,",
      "citeRegEx" : "Razenshteyn et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Razenshteyn et al\\.",
      "year" : 2016
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "Benjamin Recht" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Recht.,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht.",
      "year" : 2011
    }, {
      "title" : "Weighted low-rank approximations",
      "author" : [ "Nathan Srebro", "Tommi Jaakkola" ],
      "venue" : "In Proceedings of the 20th International Conference on Machine Learning",
      "citeRegEx" : "Srebro and Jaakkola.,? \\Q2003\\E",
      "shortCiteRegEx" : "Srebro and Jaakkola.",
      "year" : 2003
    }, {
      "title" : "Guaranteed matrix completion via nonconvex factorization",
      "author" : [ "Ruoyu Sun", "Zhi-Quan Luo" ],
      "venue" : "In IEEE 56th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Sun and Luo.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sun and Luo.",
      "year" : 2015
    }, {
      "title" : "Perturbation bounds in connection with singular value decomposition",
      "author" : [ "Per-Åke Wedin" ],
      "venue" : "BIT Numerical Mathematics,",
      "citeRegEx" : "Wedin.,? \\Q1972\\E",
      "shortCiteRegEx" : "Wedin.",
      "year" : 1972
    }, {
      "title" : "Maximum likelihood multivariate calibration",
      "author" : [ "Peter D Wentzell", "Darren T Andrews", "Bruce R Kowalski" ],
      "venue" : "Analytical chemistry,",
      "citeRegEx" : "Wentzell et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Wentzell et al\\.",
      "year" : 1997
    }, {
      "title" : "2016]); we consider the following simple approach",
      "author" : [ "Arora" ],
      "venue" : "Let X be the co-occurrence matrix,",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : ", [Levy and Goldberg, 2014]).",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "Even for collaborative filtering, which is typically modeled as a matrix completion problem that assigns weight 1 on sampled entries and 0 on non-sampled entries, one can achieve better results when allowing non-binary weights [Srebro and Jaakkola, 2003].",
      "startOffset" : 227,
      "endOffset" : 254
    }, {
      "referenceID" : 12,
      "context" : "Moreover, general weighted low-rank approximation is NP-hard, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].",
      "startOffset" : 108,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "In this line of work the state-of-the-art is [Bhojanapalli and Jain, 2014], who proved recovery guarantees under the assumptions that the ground truth has a strong version of incoherence and the weight matrix has a sufficiently large spectral gap.",
      "startOffset" : 45,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "In this paper, we provide the first theoretical guarantee for weighted low-rank approximation via alternating minimization, under assumptions generalizing those in [Bhojanapalli and Jain, 2014].",
      "startOffset" : 164,
      "endOffset" : 193
    }, {
      "referenceID" : 28,
      "context" : "Furthermore, combining our insight that the spectral property only need to hold in an average sense with the framework in [Sun and Luo, 2015], one can show provable guarantees for the family of algorithms analyzed there, including stochastic gradient descent.",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, weighted low-rank approximation is NP-hard in the worst case, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].",
      "startOffset" : 127,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "On the theoretical side, the only result we know of is [Razenshteyn et al., 2016], who provide a fixed-parameter tractability result when additionally the weight matrix is low-rank.",
      "startOffset" : 55,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : "It is known that matrix completion is NP-hard in the case when the k = 3 [Peeters, 1996].",
      "startOffset" : 73,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "In this line the state-of-the-art is [Bhojanapalli and Jain, 2014], where the support of the observation is a d-regular expander such that the weight matrix has a sufficiently large spectral gap.",
      "startOffset" : 37,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "We also mention [Negahban and Wainwright, 2012] who consider random sampling, but one that is not uniformly random across the entries.",
      "startOffset" : 16,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "Assuming that the matrix is incoherent and the observed entries are chosen uniformly at random, Candès and Recht [2009] showed that nuclear norm convex relation can recover an n×n rank-k matrix using m = O(nk log(n)) entries.",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "Candes and Plan [2010] relaxed the assumption to tolerate noise and showed the nuclear norm convex relaxation can lead to a solution such that the Frobenius norm of the error matrix is bounded by O( √ n3/m) times that of the noise matrix.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : ", 2014, Lee and Shraibman, 2013, Bhojanapalli and Jain, 2014]. In this line the state-of-the-art is [Bhojanapalli and Jain, 2014], where the support of the observation is a d-regular expander such that the weight matrix has a sufficiently large spectral gap. However, it only works for binary weights, and is for a nuclear norm convex relaxation and does not incorporate noise. Recently, there is an increasing interest in analyzing non-convex optimization techniques for matrix completion. In two seminal papers [Jain et al., 2013, Hardt, 2014], it was shown that with an appropriate SVD-based initialization, the alternating minimization algorithm (with a few modifications) recovers the ground-truth. These results are for random binary weight matrix and crucially rely on re-sampling (i.e., using independent samples at each iteration), which is inherently not possible for the setting studied in this paper. More recently, Sun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion without re-sampling.",
      "startOffset" : 33,
      "endOffset" : 947
    }, {
      "referenceID" : 1,
      "context" : "Our assumption is also a generalization of the one in [Bhojanapalli and Jain, 2014], which requires W to be d-regular expander-like (i.",
      "startOffset" : 54,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "The final assumption (A3) is a generalization of the assumption A2 in [Bhojanapalli and Jain, 2014] that, intuitively, requires the singular vectors to satisfy RIP (restricted isometry property).",
      "startOffset" : 70,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "They viewed it as a stronger version of incoherence, discussed the necessity and showed that it is implied by the strong incoherence property assumed in [Candès and Tao, 2010].",
      "startOffset" : 153,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "5} exact recovery ours (SVD init) real yes yes yes 1 μ3/2k2 ‖∆‖2 = O (k) ‖W ⊙N‖2 + ǫ ours (random init) real yes yes yes 1 μ2k5/2 ‖∆‖2 = O (k) ‖W ⊙N‖2 + ǫ Table 1: Comparison with related work on matrix completion: (1) [Candes and Plan, 2010]; (2) [Keshavan et al.",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 17,
      "context" : "5} exact recovery ours (SVD init) real yes yes yes 1 μ3/2k2 ‖∆‖2 = O (k) ‖W ⊙N‖2 + ǫ ours (random init) real yes yes yes 1 μ2k5/2 ‖∆‖2 = O (k) ‖W ⊙N‖2 + ǫ Table 1: Comparison with related work on matrix completion: (1) [Candes and Plan, 2010]; (2) [Keshavan et al., 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].",
      "startOffset" : 248,
      "endOffset" : 271
    }, {
      "referenceID" : 1,
      "context" : ", 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].",
      "startOffset" : 13,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : ", 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].",
      "startOffset" : 48,
      "endOffset" : 61
    }, {
      "referenceID" : 28,
      "context" : "(5) [Sun and Luo, 2015].",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : ", [Feige and Ofek, 2005]).",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "The seminal paper [Candès and Recht, 2009] showed that a nuclear norm convex relaxation approach can recover the ground truth matrix usingm = O(nk log n) entries chosen uniformly at random and without noise.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "The sample size was improved to O(nk log n) in [Candès and Tao, 2010] and then O(nk logn) in subsequent papers.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "Candes and Plan [2010] generalized the result to the case with noise: the same convex program using m = O(nk log n) entries recovers a matrix M̃ s.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "Candes and Plan [2010] generalized the result to the case with noise: the same convex program using m = O(nk log n) entries recovers a matrix M̃ s.t. ‖M̃−M∗‖F ≤ (2 + 4 √ (2 + p)n/p)‖NΩ‖F where p = m/n and NΩ is the noise projected on the observed entries. Keshavan et al. [2009] showed that withm = O(nμk logn), one can recover a matrix M̃ such that ∥∥M∗ − M̃ ∥∥ F = O ( n √ k m ‖NΩ‖2 ) by an optimization over a Grassmanian manifold.",
      "startOffset" : 0,
      "endOffset" : 279
    }, {
      "referenceID" : 1,
      "context" : "Bhojanapalli and Jain [2014] relaxed the assumption that the entries are randomly sampled.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Hardt [2014] showed that with an appropriate initialization alternating minimization recovers the ground truth approximately.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 14,
      "context" : "Hardt [2014] showed that with an appropriate initialization alternating minimization recovers the ground truth approximately. Precisely, they assumed N satisfies: (1). μ(N) . σmin(M∗)2;(2). ‖N‖∞ ≤ μ n‖M‖F . Then, he shows that log(nǫ logn) alternating minimization steps recover a matrix M̃ such that ‖M̃−M∗‖F ≤ ǫ‖M‖F provided that pn ≥ k(k + log(n/ǫ))μ × ( ‖M‖F+‖N‖F /ǫ σk )2 ( 1− σk+1 σk )5 where σk is the k-th singular value of the groundtruth matrix. The parameter γ corresponding to the case considered there would be roughly O( 1 k √ μ logn ). While their algorithm has a good tolerance to noise, N is assumed to have special structure for him that we do not assume in our setting. Sun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion.",
      "startOffset" : 0,
      "endOffset" : 708
    }, {
      "referenceID" : 28,
      "context" : "We note that [Sun and Luo, 2015] only needs sampling before the algorithm starts and does not need re-sampling in different iterations, but still relies on the randomness in the sampled entries.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "Keshavan et al. [2009] analyzed optimization over a Grassmanian manifold, which uses the fact that E[W ⊙ S] = S for any matrix S.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 29,
      "context" : "Since ||W ⊙ N||2 ≤ δ can be regarded as small, the idea is to show that W ⊙M∗ is close to M∗ in spectral norm and then apply Wedin’s theorem [Wedin, 1972].",
      "startOffset" : 141,
      "endOffset" : 154
    }, {
      "referenceID" : 29,
      "context" : "By our assumptions we know that ||W ⊙N||2 ≤ δ which we are thinking of as small, so the idea is to show that W ⊙M∗ is close to M∗ in spectral norm, then by Wedin’s theorem [Wedin, 1972] we will have X,Y are close to U,V.",
      "startOffset" : 172,
      "endOffset" : 185
    }, {
      "referenceID" : 29,
      "context" : "Lemma 6 (Wedin’s Theorem [Wedin, 1972]).",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "We consider two large corpora (Wikipedia corpus [Wikimedia, 2012], about 3G tokens; a subset of Commoncrawl corpus [Buck et al., 2014], about 20G tokens).",
      "startOffset" : 115,
      "endOffset" : 134
    } ],
    "year" : 2016,
    "abstractText" : "Many applications require recovering a ground truth low-rank matrix from noisy observations of the entries, which in practice is typically formulated as a weighted low-rank approximation problem and solved by non-convex optimization heuristics such as alternating minimization. In this paper, we provide provable recovery guarantee of weighted low-rank via a simple alternating minimization algorithm. In particular, for a natural class of matrices and weights and without any assumption on the noise, we bound the spectral norm of the difference between the recovered matrix and the ground truth, by the spectral norm of the weighted noise plus an additive error that decreases exponentially with the number of rounds of alternating minimization, from either initialization by SVD or, more importantly, random initialization. These provide the first theoretical results for weighted low-rank via alternating minimization with non-binary deterministic weights, significantly generalizing those for matrix completion, the special case with binary weights, since our assumptions are similar or weaker than those made in existing works. Furthermore, this is achieved by a very simple algorithm that improves the vanilla alternating minimization with a simple clipping step. The key technical challenge is that under non-binary deterministic weights, naı̈ve alternating steps will destroy the incoherence and spectral properties of the intermediate solutions, which are needed for making progress towards the ground truth. We show that the properties only need to hold in an average sense and can be achieved by the clipping step. We further provide an alternating algorithm that uses a whitening step that keeps the properties via SDP and Rademacher rounding and thus requires weaker assumptions. This technique can potentially be applied in some other applications and is of independent interest.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}