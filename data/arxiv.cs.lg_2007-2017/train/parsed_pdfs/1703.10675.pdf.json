{
  "name" : "1703.10675.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Applying Ricci Flow to Manifold Learning",
    "authors" : [ "Yangyang Lia", "Ruqian Lua" ],
    "emails" : [ "liyangyang12@mails.ucas.ac.cn", "rqlu@math.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Traditional manifold learning algorithms often bear an assumption that the local neighborhood of any point on embedded manifold is roughly equal to the tangent space at that point without considering the curvature. The curvature indifferent way of manifold processing often makes traditional dimension reduction poorly neighborhood preserving. To overcome this drawback we propose a new algorithm called RF-ML to perform an operation on the manifold with help of Ricci flow before reducing the dimension of manifold.\nKeywords: Manifold learning, Riemannian curvature, Ricci flow"
    }, {
      "heading" : "1. Introduction",
      "text" : "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al. Isomap aimed to preserve the geodesic distance between any two high dimensional data points, which can be viewed as an extension of Multidimensional Scaling (MDS) [12]. Local neighborhood preserving algorithms approximated manifolds with a union of locally linear patches. After the local patches are estimated with linear methods such as PCA [8], the global representation is obtained by aligning the local patches together. Traditional manifold learning algorithms often bear an assumption that the local patch of any point on embedded manifold\n∗Corresponding author Email addresses: liyangyang12@mails.ucas.ac.cn (Yangyang Li),\nrqlu@math.ac.cn (Ruqian Lu)\nPreprint submitted to Nuclear Physics B April 12, 2017\nar X\niv :1\n70 3.\n10 67\n5v 3\n[ cs\n.L G\n] 1\n1 A\npr 2\nis roughly equal to the tangent space at that point. Despite the success of manifold learning in many applications, there are still several problems remaining.\n• Locally short circuit problem: if the embedded manifold is highly curved, the Euclidean distance between any two points is obviously shorter than the intrinsic geodesic distance.\n• Intrinsic dimension estimation problem:since tangent spaces are simply taken as local patches, the intrinsic dimension of manifold cannot be determined by the latter, in particular in case of strongly varying curvature.\n• Curvature sensitivity problem: if the curvature of original manifold is especially high at some point, smaller patches are needed for representing the neighborhoods around this point. But one may not have as many data points as needed to produce enough small patches, especially when the data points are sparse.\nAmong the above mentioned three problems, the third one is critical. It is the background of the other two. To solve this problem is the main target of this paper."
    }, {
      "heading" : "1.1. Motivation",
      "text" : "In manifold learning one usually maps an irregularly curved manifold to a lower dimensional space directly. This is often unpractical since care is not taken of the varying geometric structure of the manifold at different points. Ricci flow is a very useful tool for evolving an irregular manifold and making it converging to a regular one (constant curvature manifold). In this paper we first analysis the intrinsic curvature of a manifold at each point and then use Ricci flow to regularize the metric and curvature of the manifold before reducing its dimension. Since Ricci flow preserves local structure of manifold and the Riemannian metric of the manifold after Ricci flow is uniform (see section 2), the local relationships among data points in the whole process of algorithm are preserved. In the current presentation we only consider open Riemannian manifolds with non-negative Ricci curvature. Manifold learning with negative Ricci curvature will be discussed in our next paper. We call this style of algorithm dynamic manifold learning. This paper is just a first attempt in this direction."
    }, {
      "heading" : "2. Basic Knowledge",
      "text" : "Given data set {x1, x2, · · · , xN} ⊂ RD, where N is the number of data points and D their dimension. We assume that {x1, x2, · · · , xN} ⊂ RD lie on a d-\ndimensional Riemannian manifold (M, g) embedded in the high dimensional Euclidean space RD (d D), where M is the manifold itself and g its Riemannian metric defined as the family of all inner products defined on all tangent spaces of M , RD called the ambient space of (M, g). The directional derivative defined on a Riemannian manifold is Riemannian connection, represented by ∇. The curvature of a d-dimensional Riemannian manifold is represented by a fourth-order tensor called the Riemannian curvature tensor Rm (X, Y, Z,W ). The trace of the Riemannian curvature tensor is a symmetric (0, 2)-tensor called Ricci curvature tensor Ric (Y, Z) = trRm (·, Y, ·, Z) [1]:\nRegarding Riemannian sub-manifold, the Riemannian curvature tensor is captured by the second fundamental formB (X, Y ) which is a bilinear and symmetric form defined on tangent vector fields X, Y . B (X, Y ) is used to measure the difference between the intrinsic Riemannian connection∇ onM and the Riemannian connection ∇̃ on M̃ , where M is embedded into M̃ . The relationship between ∇̃ and ∇ is shown by the Gauss formula [10]:\n∇̃XY = ∇XY +B (X, Y ) . (1)\nThe corresponding relationship between Rm (X, Y, Z,W ) and R̃m (X, Y, Z,W ) is shown by Gauss equation [10]:\nR̃m (X, Y, Z,W ) = Rm (X, Y, Z,W )−〈B (X,W ) , B (Y, Z)〉+〈B (X,Z) , B (Y,W )〉. (2) In this paper M̃ is RD, so R̃m (X, Y, Z,W ) = 0.\nRm (X, Y, Z,W ) = 〈B (X,W ) , B (Y, Z)〉 − 〈B (X,Z) , B (Y,W )〉. (3)\nUnder local coordinate system, the second fundamental form B can be represented by [10]: B (X, Y ) = ∑D α=d+1 h\nα (X, Y ) ξα, where ξα (α = d+ 1, · · · , D) is the normal vector field of M and hα (X, Y ) is the second fundamental form coefficient.\nRicci flow is defined by the following geometric evolution time dependent PDE [14] ∂gij\n∂t = −2Ricij , where gij = g (∂i, ∂j). The Ricci curvature Ric (g) is a\nLaplacian of the metric g, making Ricci flow equation a variation of the usual heat equation. In the time interval t ∈ I the Riemannian metric g (t) satisfies the metric equivalence condition e−2Ctg (0) ≤ g (t) ≤ e2Ctg (0) [14], where |Ric| ≤ C. So the relative geodesic distance between arbitrary two neighborhood points on M is consistent under the Ricci flow. In [11] researchers have worked out that the Riemannian manifold of dimension d ≥ 4 can be transformed to a sphere under\nthe spherical condition with the sectional curvature K satisfied maxK minK < 4 everywhere."
    }, {
      "heading" : "3. Algorithm",
      "text" : "In practice, it is difficult to analyze the global structure of a nonlinear manifold, especially when there is no observable explicit structure. In this paper we decompose the embedded manifold into a set of overlapping patches and apply Ricci flow to these overlapping patches independently of each other to avoid singular points. The global structure of deformed manifold under Ricci flow can be obtained from the deformed local patches with a suitable alignment. Our algorithm RF-ML is mainly divided into six steps:\n1. Find a local patch (neighborhood) Ui, i = 1, 2, · · · , N for each data point xi, i = 1, 2, · · · , N by using the K-nearest neighbor method.\n2. Construct a special local coordinate system on every point xi. In the neighborhood Ui we estimate the local patch information of xi with a covariance matrix Ci, Ci = ∑ xk∈Ui (xk − xi)\nT (xk − xi), where xi is the mean vector of the K-nearest data points. The first d eigenvectors (e1, e2, · · · , ed) with maximal eigenvalues of Ci form a local orthonormal coordinate system of xi on Ui. The last D − d eigenvectors (ed+1, · · · , eD) form a local orthonormal coordinate system of normal space.\n3. The intrinsic dimension d of local patches in this algorithm is determined by the number of the 95% principal components. In practice due to the different curvature of local patches, the local dimension di of each patch Ui may be in practice not all the same. We choose d = max{d1, d2, · · · , dN}. If d = D, stop the algorithm. The manifold’s dimension is not reducible. If d < D, continue the algorithm.\n4. Construct Ricci flow equations on local patches and then let the overlapping patches Ui, i = 1, · · · , N flow independently into local spherical patches Yi, i = 1, 2, · · · , N with constant positive Ricci curvature C. Details will be shown below.\n5. Align the discrete sphere patches Yi, i = 1, 2, · · · , N into a global subset P of a sphere with positive curvature C, where P ⊂ RD. pi ∈ P is the corresponding representation of xi. Details will be shown below.\n6. Reduce the dimensionality of the subset P using traditional manifold learning algorithms, where the distance metric between arbitrary two points on P is the metric of the sphere other than the Euclidean. The d-dimensional representations are {z1, z2, · · · , zN} ∈ Rd.\nStep 4) above aims to minimize the following energy function such that the Ricci curvature at every point converges to a constant curvature C.\nE (Ric) = ∫ ‖Ric− C‖2dM = N∑ i=1 |Ric (Xi)− C|2. (4)\nIn order to obtain the minimum solution of the curvature energy function, we calculate the solution step by step with the help of Ricci flow. The Ricci flow defined on the discrete data points {x1, x2, · · · , xN} ∈ RD is constructed as follows:\nSuppose the local coordinates of any point xj ∈ Ui under local coordinates 〈xi; e1, e2, · · · , ed, · · · , eD〉 are represented as ( x1j , x 2 j , · · · , xDj ) . The first d coordinates can be seen as the local natural parameters. A smooth representation of local patch Ui under the local coordinate system is described by:\nf (Ui) . = ( x1, x2, · · · , xd, fd+1 ( x1, x2, · · ·xd ) , · · · , fD ( x1, x2, · · · , xd )) , (5)\nwhere ( x1, · · · , xd ) is a coordinate chart at Ui. We use least square method to approximate the analytic functions fα, α = d + 1, · · · , D. In order to guarantee the curvature operator on each patch being satisfied the spherical conditions as presented in section 2, we choose second-order elliptic polynomial functions to approximate the structures of local patches. Under the local coordinates of Ui, the corresponding d tangent vector bases on xi are given by ∂f\n∂x1 (xi) , ∂f ∂x2 (xi) , · · · , ∂f∂xd (xi).\n∂f ∂xj (xi) =\n( 0, · · · , 1, · · · , 0, ∂f d+1\n∂xj (xi) , · · · ,\n∂fD ∂xj (xi)\n) . (6)\nThen the local Riemannian metric tensor is Gi = [gjk], gjk = 〈 ∂f∂xj (xi) , ∂f ∂xk (xi)〉. The second fundamental form coefficient is hαjk . = ∂ 2fα ∂xj∂xk . According to Gaussian equation and the definition of Ricci flow mentioned in section 2, the Ricci flow equation defined on Xi is:\n∂\n∂t\n( ∂f ∂xj · ( ∂f ∂xk )T) xi = −2 D∑ α=d+1 d∑ l=1 ( ∂2fα ∂xl∂xl · ∂ 2fα ∂xj∂xk − ∂ 2fα ∂xl∂xk · ∂ 2fα ∂xl∂xk ) xi .\n(7)\nIn order to solve the Ricci flow equation, we need to discretize the differential operators in each local patch Ui:\n∂f t+1 ∂xj = ∂f t ∂xj − 2∆t d∑ l=1 ( ∂2f t ∂xl∂xl ∂2f t ∂xj∂xj − ∂ 2f t ∂xl∂xj ∂2f t ∂xl∂xj ) · ( ∂f ∂xj )T † ,\ngt+1jk = δjk + ∂f t+1 ∂xj · ( ∂f t+1 ∂xk )T ,\nRt+1jk = d∑ l=1 ( ∂2f t+1 ∂xl∂xl ∂2f t+1 ∂xk∂xj − ∂ 2f t+1 ∂xl∂xj ∂2f t+1 ∂xl∂xk ) .\n(8)\nBy optimizing these update equations we have R∞jk → C, where C is a nonnegative constant.\nIn step 5) above, after the Ricci flow converges at each Xi, the overlapping local patches {U1, U2, · · · , UN} are mapped to a set of discrete local spherical patches {Y1, Y2, · · · , YN}. We denote the global coordinates of {Y1, Y2, · · · , YN} with {P1, P2, · · · , PN}. The relationship between {Y1, · · · , YN} and {P1, · · · , PN} is a set of global alignment maps. which shift the discrete local spherical patches to a global subset of a sphere. In each local patch Yi we have:\npij = P i + Tiyij + ε (i) j , i = 1, · · · , N, j = 1, · · · , K, (9)\nwhere Ti is the unit of orthogonal transformation and ε (i) j the reconstruction error, yij ∈ Yi, pij ∈ Pi. In order to obtain the optimized global affine transformation, we minimize the global reconstruction error matrix:\nmin N∑ i=1 ‖Pi ( I − 1 K eeT ) − TiYi‖2F , (10)\nMinimizing the above least square error is equal to solve the following eigenvalue problem:\nB = SWW TST , (11)\nwhere S = [S1, S2, · · · , SN ], PSi = Pi, W = diag (W11,W22, · · · ,WNN), Wii = ( I − 1 K eeT ) ( I − Y †i Yi ) and Y †i the generalized inverse matrix of Yi. Decomposing matrix B using SVD method, we obtain B = UΛU−1. The optimal\ndata set P that we finally need are distributed on a sphere with curvature C. So we need to give a set of constraints PiP Ti = 1 C2 , i = 1, 2, · · · , N . Under these constraints, we rewrite matrix B as: B = QRΛRTQT , where R is a diagonal matrix,\nRii = C · 2 √∑D j=1 U 2 ij , i = 2, · · · , D + 1, the rest Rkk = 1, k 6= 2, · · · , D + 1. The obtained 2nd to D + 1st columns of Q are the optimal data set P , which are distributed on a sphere with curvature C. An intuitive representation of algorithm RF-ML is shown in Figure 1."
    }, {
      "heading" : "4. Experiment",
      "text" : "We compare our method with traditional manifold learning algorithms (PCA, Isomap, LLE, LEP, Diffu-Map, LTSA) on four sets of three dimensional data from [13] including Swiss Roll, Sphere, Ellipsoid and Gaussian . The objective of this comparison is to map each data set to two dimensional space and then to analyze the neighborhood preserving ratio (NPR) [9] of different algorithms. The neighborhood preserving ratio (NPR) [9] is defined as follows:\nNB = 1\nKN N∑ i=1 |N (xi) ⋂ N (zi) |, (12)\nwhereN (xi) is the set of K-nearest sample subscripts of xi, andN (zi) is the set of K-nearest sample subscripts of zi. | · | represents the number of intersection points.\nTable 1 shows that for all but one dataset, the NPR of RF-ML is the best one among all algorithms. Using our algorithm we can analysis that Swiss Roll is a locally flat two-dimensional manifold. Its data structure is unchanged under Ricci flow. As for the Sphere dataset, its Gauss curvature is a unique constant everywhere. Regarding Ellipsoid dataset and Gaussian dataset, note that the Gauss curvatures at different points are not always the same. When using RF-ML to reduce their dimension, the NPR of Ellipsoid dataset is 87.95%, 34.28%, 66.58%, 12.49%, 73.33%, 58.90% respectively better over the other six traditional manifold learning algorithms. For Gaussian dataset, due to the characteristics of the dataset distribution the NPRs of PCA and LTSA are quite high. Compared with the six algorithms, the NPR of our method is not bad and also quite high. These results clearly demonstrate that our curvature aware algorithm RF-ML is more stable and the Ricci flow process preserves better the local structure of data points."
    }, {
      "heading" : "5. Conclusions and Future Work",
      "text" : "Whether the manifold structure is uncovered exactly or not impacts the learning results directly. Traditional manifold learning algorithms do not consider the varying curvature at different points of the manifold. Our method aims to excavate the power of Ricci flow and to use it to dynamically deform the local curvature to make the manifolds curvature uniform. The extensive experiments have shown that our method is more stable compared with other traditional manifold learning algorithms. One limitation of our algorithm is that RF-ML works only for manifolds with non-negative curvature. We will discuss the applicability of RF-ML algorithm to manifolds with negative Ricci curvature manifold in our next paper."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by National Key Research and Development Program of China under grant 2016YFB1000902; National Natural Science Foundation of China project No.61232015, No.61472412, No.61621003; Beijing Science and Technology Project: Machine Learning based Stomatology; Tsinghua-TencentAMSS Joint Project: WWW Knowledge Structure and its Application."
    } ],
    "references" : [ {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "S. Roweis", "L. Saul" ],
      "venue" : "Science, vol. 290",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "V",
      "author" : [ "J. Tenenbaum" ],
      "venue" : "de Silva and J. Langford, A global geometric framework for nonlinear dimension reduction Science, vol. 290",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Laplacian eigenmaps and spectral technique for embedding and clustering",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "NIPS",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data",
      "author" : [ "D.L. Donoho", "C.E. Grimes" ],
      "venue" : "Proceedings for the National Academy of Sciences of the United States of America, vol. 100",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Principal manifolds and nonlinear dimension reduction via local tangent space alignment",
      "author" : [ "Z. Zhang", "H. Zha" ],
      "venue" : "SIAMJ. Scientific Computing, vol. 26",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Niyaogi, Locality preserving projections",
      "author" : [ "P. Xiaofei He" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Principal component analysis",
      "author" : [ "I.T. Jolliffe" ],
      "venue" : "Springer-Verlag, vol. 69",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Dimensionality reduction of clustered data sets",
      "author" : [ "Guido Sanguinetti" ],
      "venue" : "IEEE TPAMI, vol",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Riemannian Manifolds",
      "author" : [ "J.M. Lee" ],
      "venue" : "Springer",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Riemannian manifolds of positive curvature",
      "author" : [ "S. Brendle", "R. Schoen" ],
      "venue" : "Proceedings of the International Congress of Mathematicians",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Multidimensional Scaling",
      "author" : [ "T. Cox", "M. Cox" ],
      "venue" : "Statistics for the Social and Behavioral Sciences",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Mani Matlab demo",
      "author" : [ "T. Wittman" ],
      "venue" : "http://www.math.umn.edu/∼wittman/mani/",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Recent developments on the Ricci flow",
      "author" : [ "H.D. Cao", "B. Chow" ],
      "venue" : "Bull. Amer. Math. Soc, vol. 36",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.",
      "startOffset" : 262,
      "endOffset" : 265
    }, {
      "referenceID" : 2,
      "context" : "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.",
      "startOffset" : 271,
      "endOffset" : 274
    }, {
      "referenceID" : 5,
      "context" : "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.",
      "startOffset" : 280,
      "endOffset" : 283
    }, {
      "referenceID" : 4,
      "context" : "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.",
      "startOffset" : 290,
      "endOffset" : 293
    }, {
      "referenceID" : 3,
      "context" : "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.",
      "startOffset" : 312,
      "endOffset" : 315
    }, {
      "referenceID" : 10,
      "context" : "Isomap aimed to preserve the geodesic distance between any two high dimensional data points, which can be viewed as an extension of Multidimensional Scaling (MDS) [12].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 6,
      "context" : "After the local patches are estimated with linear methods such as PCA [8], the global representation is obtained by aligning the local patches together.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "The relationship between ∇̃ and ∇ is shown by the Gauss formula [10]: ∇̃XY = ∇XY +B (X, Y ) .",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "The corresponding relationship between Rm (X, Y, Z,W ) and R̃m (X, Y, Z,W ) is shown by Gauss equation [10]:",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "Under local coordinate system, the second fundamental form B can be represented by [10]: B (X, Y ) = ∑D α=d+1 h α (X, Y ) ξα, where ξα (α = d+ 1, · · · , D) is the normal vector field of M and h (X, Y ) is the second fundamental form coefficient.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "Ricci flow is defined by the following geometric evolution time dependent PDE [14] ∂gij ∂t = −2Ricij , where gij = g (∂i, ∂j).",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "In the time interval t ∈ I the Riemannian metric g (t) satisfies the metric equivalence condition e−2Ctg (0) ≤ g (t) ≤ eg (0) [14], where |Ric| ≤ C.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "In [11] researchers have worked out that the Riemannian manifold of dimension d ≥ 4 can be transformed to a sphere under",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "Experiment We compare our method with traditional manifold learning algorithms (PCA, Isomap, LLE, LEP, Diffu-Map, LTSA) on four sets of three dimensional data from [13] including Swiss Roll, Sphere, Ellipsoid and Gaussian .",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "The objective of this comparison is to map each data set to two dimensional space and then to analyze the neighborhood preserving ratio (NPR) [9] of different algorithms.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : "The neighborhood preserving ratio (NPR) [9] is defined as follows:",
      "startOffset" : 40,
      "endOffset" : 43
    } ],
    "year" : 2017,
    "abstractText" : "Traditional manifold learning algorithms often bear an assumption that the local neighborhood of any point on embedded manifold is roughly equal to the tangent space at that point without considering the curvature. The curvature indifferent way of manifold processing often makes traditional dimension reduction poorly neighborhood preserving. To overcome this drawback we propose a new algorithm called RF-ML to perform an operation on the manifold with help of Ricci flow before reducing the dimension of manifold.",
    "creator" : "LaTeX with hyperref package"
  }
}